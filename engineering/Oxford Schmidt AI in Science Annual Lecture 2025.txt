Okay, my name is Jim Naismith.
I'm the head of the Mathematical, Physical, Engineering and Computing Life Science Division
here at Oxford, abbreviated as MPLS.
It's my pleasure to welcome you all to lecture, but first I should start by thanking the Schmidt,
and I want to read this correctly, the Schmidt Sciences for their very generous support
of the Schmidt AI and Science Fellowship Program here in the university.
It is hard to think of a more transformative investment than in AI at the moment.
All of us are very conscious that it's developing extraordinarily fast, and one of the things
that I think we're all excited about is how it's going to change all of science.
And so it's really great to see from 2022 that we have now recruited over, I think, 35 Schmidt
Fellows to the university, and the first cohort, which began in 22, we've already seen them
go on to great things.
So four of them have now entered academia, which, depending on your view of academia, may not
be that great.
But five have gone on to senior positions in industry.
And of course, that's a testament to my colleagues who have done such exceptional training and support
and mentoring, but also, of course, to the generosity of the Schmidt Sciences, which have provided
the resources to really make sure that those careers really go skyward.
And thinking of linking to that, the diversity of what we're seeing Schmidt Fellows do, from
birds to bees, microscopy to space to climate change, that's in many of the ways the most inspiring
thing, because there isn't a single area of science that won't be touched by this revolution.
And so congratulations to all of you who are Schmidt Fellows.
I know the competition is extraordinarily fierce, but I'll finish where I began in thanking the
Schmidt Sciences for their generous support of this program, which is doing so much to embeds
AI across this division and the university.
Thank you.
Thank you.
Thanks so much, Jim, for the vote of thanks.
It falls to me to introduce our speaker this evening, and I'm very glad she sent me a short
bio intro, which I still needed to print out, because my memory is not that good.
Her achievements are absolutely countless.
She's one of the pioneers across the world in cognitive science, and we are incredibly lucky
this evening to have Alison Gopnik to give our Schmidt annual seminar.
Alison is Professor of Psychology and a Philip Professor of Philosophy at the University of California
at Berkeley and a member of the Berkeley AI Research Group.
She received her BA from McGill University and her PhD, DPhil, from Oxford University.
So great to have you back.
She's a leader, an absolute world leader in cognitive science, particularly in the study
of learning and development.
She was a founder of the field of the theory of mind, an originator of the theory theory
of cognitive development, and the first to apply Bayesian models to children's learning.
And if you're like me, we all love a Bayesian.
She has received the APS Lifetime Achievement, Cattle and William James Awards, the SRCD Lifetime
Achievement Award, the APA Distinguished Scientific Contributions Award, the Bradford, Washburn and
Carl Sagan Awards for Science Communication, and the Rumorhart Prize for Theoretical Foundations
of Cognitive Science.
I need to take a breath now.
She's a member of the National Academy of Sciences in the U.S., the American Academy of
Arts and Sciences, and a Cognitive Science Society Fellow.
The American Association for Achievement of Advancement of Science, and a Guggenheim Fellow.
In 2022 through 23, she was president of the Association for Psychological Science.
She's the author of over 160 articles and many books, including best-selling and critically
acclaimed popular books, The Scientist in the Crib, The Philosophical Baby, and The Gardener
and the Carpenter.
She's written widely about cognitive science and psychology for The Wall Street Journal,
New York Times, Economist, and The Atlantic, amongst many other venues.
Her TED Talk has been viewed more than five and a half million times, and she's frequently
appeared on TV, radio, and podcasts, including Charlie Rose Show, Colbert Report, and The Ezra
Klein Show.
What a privilege we...
It's such a privilege this evening to have Alison Gopnik come and give this talk on understanding
how children, scientists, and AI systems learn.
Alison, the floor is yours.
Thank you so much.
APPLAUSE
Do you have a tiny ear?
Thank you so much for that introduction, and thank you all so much for being here.
I first came here 50 years ago, almost exactly 50 years ago, in 1975 to 1976 as a graduate
student, and that year was an anus mirabilis for me.
It was glorious.
It was a golden summer.
I sat in my bikini on the terrace of Wolfson College and went down the char well, drifting
my hands through punts and talked to everyone, and in fact, I can remember walking through
university parks and thinking, this is bliss to be alive at this moment.
And then, of course, this was the famous summer of 1976, and then, of course, it rained for
the next four years that I was at Oxford.
So I'm hoping that this glorious weather that we're having is actually Oxford welcoming me
back, although I suspect it's actually damn climate change.
So I want to start out by talking about three different stories you might tell about AI,
and three different stories that people have told about AI.
I think very often people understand things in terms of narratives and metaphors and stories.
And the first story, the one that I think is most prevalent both in popular imagination
and in the AI community itself is really a very ancient story, the story of the Golem.
And in the courts of preparing this talk, I went down a wonderful folklore rabbit hole.
It turns out that the story of the Golem is so ancient that there are zillions of different
versions of it, and it actually has its own number in the Encyclopedia of Folklore.
And the story is always the same.
What happens is that someone takes an inanimate object and gives it language, gives it thought,
gives it intelligence one way or another.
In this version, it's the rabbi of Prague who speaks special magic words to the Golem.
And then it becomes alive.
It thinks.
It becomes an agent.
It goes out in the world and makes things happen.
And you will be unsurprised to hear that it never ends well.
It always ends in tears, as we say in England.
But that's a very ancient picture and still a very compelling picture about the nature of
AI.
That what will happen or what AI has done is to produce particular kinds of agents, super-intelligent
agents or super-powerful agents, something like that, who will go out in the world and
in the best of cases cause utopia and in the worst of cases cause apocalyptic disaster.
Now, what I want to suggest is that that picture is not the right way to think about the AI systems
that we have now, nor indeed the right way to think about the way that AI systems could develop.
Instead, in terms of some of the systems that we have now, particularly the large models,
I want to suggest a different story, also an ancient story with many variations that has
its own number in the Encyclopedia of Folklore.
And this is the story of stone soup.
I don't know how many people here know this.
Stone soup or axe soup or nail soup.
And here's how the story goes.
What happens is that a group of travelers come to a small village and they're hungry and want
food and the villagers say, we have no food to share.
And the travelers say, that's fine, we're magicians and we're going to make stone soup.
So they take a big cauldron, they put some stones in the cauldron, it starts to bubble.
They say, this is going to be fantastic stone soup and the villagers are all very curious.
And they say, you know, this will be great, but it would be even better if we had a carrot
and onion to put in.
And one of the villagers goes and says, oh, I think I have carrot and onion somewhere and
comes and puts it in the soup.
And they say, oh, that's better.
You know, when we made it for the rich people, we put buttermilk and barley in the soup and
that made it even more delicious.
And another villager goes and gets buttermilk and barley and you can imagine how this works
out.
When the king had it, he actually put a chicken in and that made really wonderful soup.
And someone, yet another villager goes and gets a chicken and puts it in.
And so it goes through the entire village and of course in the end it's this wonderful,
delicious soup and the villagers say, this is amazing.
Here's this wonderful soup that was just made with stones.
Now when I gave this talk at NeurIPS, Ted Chiang, the wonderful science fiction writer I'll
talk about in a minute, rather mordantly said at the end of the story, well, let me give
you the AI version of the story first.
So the AI version of the story is this.
You had some tech executives and they came to the village of computer users and they
said, we have artificial general intelligence and we've just made it from gradient descent
and next token prediction and transformers.
And everyone said, really?
That's wonderful.
They said, yeah, but it would be even more intelligent if we had more data.
What we really need is more data.
Do you guys have any data?
And the villager said, well, we have all of the books we've ever written and all the pictures
that we've ever taken and all the journal articles that we've ever produced.
Sure.
We'll give you all of our data and the tech executive said, oh, that's good.
It's getting even more intelligent, but you know what would make it even more intelligent?
It still says stupid things.
If you did reinforcement learning with human feedback.
So if you guys would all tell it when it's saying something stupid or when it's saying something
intelligent and the users all said, well, there's all villages in Nigeria that would be happy
to do this.
So they said, see, it's getting even more intelligent, but you know, it still says stupid things a
lot of the time.
Maybe if you guys could think about prompt engineering and the users said, oh yeah, we put all of
our intelligence and thought into figuring out how to do the right kinds of prompts to actually
get the right results.
And then, of course, the visitors, the magic visitors say, see, we told you we got artificial
general intelligence just from a few algorithms.
So as I said, Ted Chiang's rather mordant response to this is, yeah, but at least in stone soup,
they don't sell the soup back to the villagers at an exorbitant price.
Now, this is intended, I think, in the original to be something of a debunking story.
And obviously it's a kind of debunking story here.
But it also, even in the original version, has an alternative moral.
And the alternative moral is when you combine all of the food from all of the different villagers,
you can get something that's well beyond what any of them could have individually.
So it's a moral about debunking the stone soup, but it's also a moral about the fact that you
can get something really new and really wonderful by combining what people already have.
And that's the story that I want to tell about large models in particular, about the contemporary
version of AI.
That actually what it's doing is putting together intelligences of lots of individual people
and information from lots of individual people, rather than being the sort of Gollum agent
that's going out and being an intelligent agent on its own.
But there's another question, that's, I think, where we are with AI now.
But there's a question about what kind of AI could we imagine having in the future?
So what kinds of things could we do that would really lead to something that was intelligent
in the same way that humans are intelligent?
And there, I think, actually a model comes from this wonderful book by Ted Chiang, The Life
Cycle of Software Objects.
And what Ted envisions in that novella, which I highly recommend, is a world in which AI agents
are like children, that people adopt them, they try to get them to learn, they become
more intelligent, eventually they become autonomous and go off and leave their adult parents.
And it's a really beautiful description of the way that intelligence works for humans,
and in particular, the way that intelligence works for human children.
And I think that's actually a better model of what we would need to have genuine artificial
intelligence than, say, either the Gollum or the Stone Soup model.
And this is not just a new idea.
In fact, in the very first, the famous Turing paper in 1950 where he proposes the Turing test,
the part that everyone remembers is the whole first part where he says, oh, let's use the
Turing test as a measure of whether we have an intelligent system.
But what people forget is halfway through, you know, and of course the Turing test is something
that current models seem to be able to pass, halfway through he says, you know, maybe that's
not the right idea.
Maybe instead of trying to design a program that thinks like an adult, maybe what we want
to do is produce one that simulates the child.
And then for the rest of the paper, he goes on to say why that would be a better method.
And of course, the reason is because the child is actually learning how to do the kinds of
things that an adult can do.
So simply programming in here's adult intelligence isn't really going to capture the most important
things about human intelligence, which are that we can be placed in a new world, a non-stationary
environment, an out-of-distribution setting, and we can learn about how that system works.
And it's children who are, as I'll argue in a bit, it's children who are actually the ones
who are doing that learning.
So I think Turing is right that looking at children is a better model for trying to think
about genuine intelligence than looking at the adult mind.
And over the past 30 years now, you know, in some ways with ideas that started 50 years
ago when I was in Oxford, what I've been doing is trying to, I and other cognitive scientists,
is trying to fulfill this project of what can we understand about how children learn so
much by looking at artificial systems that learn, and how can we use models, children
who are the best learners we know of in the universe, to design artificial systems that
could learn in similar kinds of ways.
So what can thinking about children teach us about AI?
So here's the first thing that it can teach us.
And I always put this slide up when I'm talking down in Silicon Valley, and there's always a sharp
intake of breath from the audience, and a whispered comment of, don't tell the VCs about this.
So what comes out of cognitive science is that there isn't any such thing as general intelligence,
whether you're thinking about it artificially or naturally.
It's not just that there are different kinds of cognitive capacities, different kinds of intelligences,
but those different intelligences actually play against each other, actually trade off against
one another, and our intention and conflict with each other.
And the intelligences that I'm going to talk about today, the first one which I'm not going
to talk about is the one that most commonly is talked about, the one that you often think
about when you're thinking about that Gollum picture of AI, or even when you're thinking
about normal adult cognition.
That's what in reinforcement learning they talk about is exploitation versus exploration.
That's the kind of intelligence that lets you go out into the world and maximize your utilities,
and get resources, and power, and do the things you want, and accomplish your goals, et cetera,
et cetera.
And I'm not going to talk about that kind of intelligence today.
Often that's seen as being what general intelligence is.
But there are all these other aspects, other cognitive capacities that are central to human intelligence,
that actually trade off against that exploitation capacity.
So one of them that I'm going to talk about in the sort of stone soup argument about large
models is cultural transmission.
So one of the things we do is to get information from other people, and figuring out how to both
teach other people and learn from other people, coordinate with other people, is a crucial
part of human intelligence.
But there are also these other three kinds of intelligence, which I think together are
the ones that characterize the amazing learning intelligence of children.
One of them is building models of the world around you.
Another is actually actively experimenting, exploring the world around you.
And both of those are things that people have pointed out are really, should be crucial
for artificial intelligence, and aren't integrated into something like the current large model
paradigm.
And both of those are in tension with exploitation.
So the point about model building and exploration is not to maximize your utilities.
It's to maximize the truth.
It's to figure out what the structure of your environment is like, independent of your utilities.
And then the last piece in the child's formula for intelligence, one which people have not
thought about, is care.
So the reason why children can learn as much as they do about the world is because they
have caregivers who are doing the exploitation part.
Caregivers who are actually giving them resources and putting them in the right kind of environment.
And what I'm going to suggest is that these three things together are the elements that
you would want if you really wanted to have a, as Turing suggested, a program that could
learn like a child.
And I'm going to give some examples of things that you could actually do computationally that
would give you those outcomes.
But before I get there, I want to talk a bit about the transmission piece.
So one of the things that cognitive scientists have talked about is the idea that human beings
live in a cultural niche.
That one of the things that we do really distinctively is to share information with each other.
And in particular, to get information from previous generations.
And it's because we're always building on the shoulders of the previous generation that
we can learn as much as we can.
And I think, not as a joke, that post-menopausal grandmothers are like the essential engine
of this kind of cultural transmission.
The fact that we have elders who know things and spend their time telling children about
them is a really essential piece of this.
And Boyd and people like Dick Boyd and Peter Richardson and Joe Henrich have argued that this
is really the secret sauce, oh I'm sorry, that this is really the thing that makes humans,
that makes humans special.
All the things that we think about, all the things that make this room completely different
from the Pleistocene are the result of this kind of cultural transmission.
And what I've been arguing is the right way to think about large models is to think of
them as cultural and social technologies that are involved in enabling this kind of cultural
transmission.
And this second piece is the piece that just came out in Science with Henry Farrell, who's
a political scientist, and James Evans, who's a sociologist, making this case.
So if you're interested in more of the argument, that's the place to look.
We also have some empirical evidence about it in the other piece.
All right.
What do I mean by a cultural and social technology?
Well, as long as there's been humans, we've had this capacity to get information from other
people.
But we've also designed technologies that make that capacity work more effectively.
That let us learn from more people across greater gulfs of space and time.
And let us learn more efficiently.
Let us put that information together in a form that we can access efficiently.
So if you think about language itself, language itself is kind of the ur-cultural technology
that allows us to make progress in a way that other animals can't, for example.
But if you think about pictures, right, pictures are another example of a system that we've developed
to communicate with other human beings and give them information.
And just a little side note, but one that I'm, a little side rant is when people talk about
vision models, for example.
They call them vision models as if what they were doing is getting the kind of input that
we get in our retina when we're looking around the room.
What they get is pictures.
And pictures are completely different from the kind of input that we get when we're looking
around the room.
They get the pictures that human beings have put on the web to communicate with other people.
And it's the pictures, not visual input, that are training those models.
And of course, as we get to things like writing and print, we have even more powerful ways
of moving information around.
And in parallel, we have to find ways of accessing, organizing, indexing that information.
So we have things like libraries and the beautiful old-fashioned card catalogues that you may not
even have them in the Bodleian anymore.
They used to have them in the Bodleian with their wonderful smell.
That's an example of a really remarkable information technology that lets you access the information
that's there in a library.
And of course, more recently, we have things like internet search and Wikipedia.
These are all incredibly important, powerful technologies that have changed the world,
that let us understand the world and learn in new ways.
But they're not themselves intelligent agents.
So it would be a category mistake, as we used to say in philosophy at Oxford, to ask whether
the UC Berkeley Library knows more or is more intelligent than I am.
There's more information in the library than I have in my head, but it's not the kind of thing.
It's not a golem.
It's not an intelligent agent.
We have other kind of similar technologies that we've developed that serve to coordinate
people's preferences rather than passing on information.
So a very old idea about markets, for example, is that what a market does is to take all of
these individual preferences and desires that a group of people have and turn them into one
efficient information processing symbol, which is the price.
So the price is a way of condensing information about hundreds, millions of people's preferences
and desires into a single unit.
And you can think about things like election devices in democracies or bureaucracies as being
examples of other kinds of institutions we have that serve this role of coordinating information
from lots of people.
And I think it's important to say that these cultural and social technologies have been
crucial for science, right?
So science emerged because we started to have things like originally correspondence but then
printed texts and nowadays things like learned journals and archive that enable scientists to
share information in an effective way.
And of course we've also had institutions like societies and research.
There's some lovely work on the economics of citations that argues that citations for science serve
the same role as prices for markets, right?
So citations are a way that you can summarize how influential is this person just with a single,
just with your single H index.
And of course as with markets, this is for better or for worse, right?
So sometimes markets and bureaucracies serve this important function but markets and bureaucracies
can also, and even elections can also distort the outcomes of, the outcomes and combinations of preferences.
And what I want to argue is that these are the kinds of models we should have for what LLMs are doing.
So what large models do is to give you a whole new set of tools that in a genuinely different revolutionary way enable you to do the same sort
of things that you could do with previous cultural and social technologies.
And that's part of the argument that's in that science paper.
I think the actual interesting change in technologies that really took place way before LLMs was that around the millennium,
this is a book that I would highly recommend, but I guess I can highly recommend it.
My husband actually wrote it, about the origins of, the origins of computer graphics and pixels.
And one of the things that Albie points out in this book is around the year 2000, kind of unheralded,
there was this great convergence in which all the previous cultural technologies became digital, became bits.
So that's the year of the first Pixar movie, it's the year of HDTV, it's the year of the PDF.
All of those, what that meant was that all of the previous technologies for getting information from other people,
now we're all in one format, namely the bit.
And when it's all in the digital format, it means it's instantaneously transmissible and infinitely reproducible.
So that means that all that information that all those cultural technologies were using before,
now suddenly is in this amazingly powerful new format.
And in a way it was just a matter of time before that fact started changing the way that we accessed that information.
And of course the large models crucially depend on the fact that all this information is available and available in digital form.
So, again, I want to emphasize that this is both good and bad.
So, cultural technologies are the things that have enabled us to do all the things that we can do.
But of course, as I said, they have negative consequences too.
And an example I really like about this is in the late 18th century, again, a very relevant period for science,
there were a lot of technological changes to print, which meant that suddenly, basically everybody could print a pamphlet,
which had not been true, say, in the Gutenberg age.
And that change had a lot to do with the spread of scientific ideas and enlightenment ideas more generally.
So it's no coincidence that in that is a picture of Benjamin Franklin and it's no coincidence that Benjamin Franklin was a printer,
as were a lot of the other people who were involved in the American Revolution and the Enlightenment.
What that meant was that Benjamin Franklin, literally overnight, could come and print a pamphlet that said something
about his interpretation of what was happening in the revolution, for example.
And it was things like Tom Paine's Common Sense being printed that actually led it to be circulated and be a new idea for everybody,
as opposed to just the elites.
On the other hand, so we all celebrate how wonderful printing was.
On the other hand, a long time ago, the historian Robert Darden actually went and read all the stuff
that had been printed in France during the same period in the Enlightenment.
Not just the good stuff, but all of it.
And you will be shocked to hear that the vast majority of it was essentially libel and softcore porn,
and often a combination of libel and softcore porn.
So that Let Them Eat Cake, for example, was actually a meme that was circulated in these pamphlets
along with stories about Marie Antoinette's lesbian orgies.
So from the very beginning, you have these capacities to spread information,
and to do all the awful things that we know that something like current social media or indeed current LLMs can do.
These kind of hallucinations were there from the very beginning.
And the way that we dealt with that is by developing norms and rules and regulations and laws that take something like print
and have things like newspapers or editors or fact checking or journalism schools that try to ensure that this new medium of print
is actually going to be more positive, more truthful than negative.
And I think it's fairly, it's interesting that if you think about the case of science, you see the same thing.
So in the case of science as well, what you see is that we have this constant process of developing new norms
that enable us to make scientific progress with whatever the new tools are that we're using in science.
And this isn't just, you know, a thing that's a scientific method that remains the same.
As the tools change, the kinds of methods and norms and regulations that we have change as well.
I mean, something that I think is worth pointing out is that, you know, Spearman and Pearson and those statistics,
that's after Einstein.
That's a really recent, that's a really recent development.
So lots of science was happening before we had ideas like statistical p-values.
Okay.
So the argument that I want to make is if we're thinking about large models and thinking about large models as tools for science,
the context in which we should be thinking of them is the context of all these other technologies
that we've had over human history that enable us to agglomerate information, access information.
Again, both for good and for ill.
And our job is to figure out what are the properties of these new systems
and what are the kinds of norms, rules, regulations, et cetera, that we need to make sure that overall
we're going to get the benefits rather than the crises from those systems.
But now I want to switch gears and talk about not the way that AI currently works,
but the way that AI could work in the future.
And the way that you could imagine having an artificial intelligence system that was learning
in the way that a child is learning, for example, which is not what the current large models are doing.
And as I said, there are three elements of this that I'm going to talk about.
And it's not a coincidence that these three elements are also important elements in the progress of science.
So the first one is actually model building and in particular building theories.
That's what scientists do.
And a very crucial part of building theories is building causal models in particular,
figuring out the causal structure of the world around us.
That's a central piece of science.
And way back when I was sitting in punts in Oxford 50 years ago,
an idea that I and other developmentalists had was that what children were doing
is very much like what scientists were doing in this regard.
The children were building theories.
This has sometimes been called the theory theory,
that children learned by building theories in very much the same way that scientists built theories.
The only trouble back 50 years ago was that when we talked to the philosophers of science about this,
they said, yeah, but we have no idea how scientists build theories, which was kind of disappointed.
But in fact, over the succeeding 50 years, there's been a remarkable set of work involving philosophers of science
and computer scientists that at least for this part of theory building,
which is about causal inference, has given us really beautiful rigorous computational accounts about how this is possible.
And the remarkable thing in, and again, what I've been doing for the past 30 years, 25 years,
is showing that this is the same principles apply to children's learning,
apply to how children learn about the causal structure of the world.
So the same principles that allow scientists to develop causal theories are rather remarkably principles
that you can see being used by even very, very young children.
And we have a recent review paper that I recommend in Nature that goes over this, by now,
quite large body of work showing that children are, in fact, able to do these kinds of causal inferences
in a rational way in very much the same way that we would like to see scientists making these inferences.
I won't go into all the details there.
And the theoretical approach that, the theoretical approach, as I mentioned before,
that we've used to explain this relies on this rather remarkable convergence of work in computer science
and philosophy of science around the arts, around now I guess around 20 years ago,
that formalized patterns of causal inference and causal model building,
showed how you could build causal models from patterns of data.
And it turns out that this is also a very good characterization of what children are doing.
And, of course, it's central to science.
What is it that makes these relations causal?
That's a philosophical question you could ask.
What do we mean when we say we're understanding something about causal structure?
And the idea that's become the dominant idea, again, both in computer science and the work of people like Judea Pearl
and in philosophy of science and the work of people like James Woodward and Clark Leemore,
is that causation is about intervention.
So what it means to have a causal relation is that if you intervened on the causal variable,
you would get some effect on the effect variable.
The technical term that I like to use for this is wiggle.
So if you wiggle X and then Y wiggles, that means that there's a causal relation between them.
There's a lot more sub-clauses as there always is in philosophy of science, but that's the central idea.
And that makes causation different from, say, association and prediction,
which are the central ideas behind, say, the kind of classic neural net algorithms, right?
So if you see X and Y, you might see that they're associated, you might predict Y given X,
but that's quite different from saying that if you had intervened on X, you would bring about Y.
A classic example is if you see that someone has yellow nicotine stained fingers back,
this is all showing my age, back in the olden days when people had yellow nicotine stained fingers,
you could predict that they would get lung cancer, but cleaning their fingers would not keep them from getting lung cancer,
whereas not smoking would, which means there's a predictive association between the fingers and the lung cancer,
but there's a causal relation between smoking and lung cancer.
So what we've, and this kind of causal learning, this kind of ability to look out in the world,
see relations among data and evidence in the world and make causal conclusions,
is actually something that turns out to be very hard for the current large models, for example.
So because the large models are all based on, in this stone soup way, on what humans know,
they're good at saying things like, well, what will happen if you drop a glass, it'll break.
What they're not so good at doing is taking a whole new set of data
and figuring out what the causal structure must be for that new set of data.
And that's something that children, we've discovered, are extremely good at.
In fact, interestingly, better than adults in many, many circumstances.
And we and others have shown that this is a real weakness in the current models.
And this is one paper and there are others that show this.
And essentially what we do is put kids in an online causal environment
and then put agents, both large models and various kinds of reinforcement learning agents,
in the same environment and see what kinds of causal inferences they can draw.
All right, how is this possible? How does it work, right?
We've shown that kids are amazingly good at doing this.
We've shown that normatively this is what scientists should be doing.
But at an algorithmic level, what is going on in those little downy heads
that's actually enabling to do it?
And the idea that lots of people had, including me back in the arts,
is that you could think of this as a Bayesian learning process.
I'm told that there are people who love Bayes here, which is good,
because I love Bayes too.
Although we'll get to a bit of a catch in a minute.
So the idea is the way that these systems like causal base nets work
is that you have a graphical model of the causal structure.
And if you have the right model, then you can predict the data and the evidence,
particularly the conditional dependencies among the variables.
And what that should mean is that from a Bayesian perspective,
if you have a model, a hypothesis, and you generate the data,
you should be able to go backwards, right?
That's all that Bayesian inference does is say, given this hypothesis,
here's the probability of this pattern of data.
So given this pattern of data, what's the probability of this hypothesis?
And the Bayesian idea is that you can do this kind of inversion
and figure out the answer.
And this was the approach that people took in the causal base net kind of,
and still take in the causal base net causal inference literature.
But there's a catch, as I said, and that catch is the search problem, right?
So it's easy to say, and in fact, what we do with the kids is we give them a pattern of data
and we say to them, here's two hypotheses about how this works.
This machine, this blicket could make it go, or this other blicket could make it go.
Which one is it? And the kids are normatively choosing the right one.
But of course, there's an incredibly large potential space of hypotheses
that they could be considering.
And the question of how you search through that space of potential hypotheses
is really challenging, right?
How is it that you could, how is it that you could do that?
And frankly, you know, for 20 years, I kept waiting for my brilliant,
30 friends to come up with a good way of solving that search problem.
I think all of computer scientists have been thinking about ways to get,
good ways to solve it, and we've tried sampling, and we tried MCMC,
and we tried all sorts of things.
And as someone once said, they all work in asymptopia, but, you know,
we're a far way away from asymptopia.
So I think we were in this kind of dilemma.
Now, if you actually looked at children, what you see is that children are constantly exploring.
So unlike a kind of classical Bayesian story, it's not like they're just sort of,
or unlike a large model, it's not like they're just sitting there waiting for the data
to waft over them and then pulling out the structure and the data.
They spend all of their time literally from the time they wake up until the time they fall asleep doing experiments,
actively intervening in the world.
When scientists, and of course, that's the gold standard for scientists about how to find out about causal structure,
is you don't just sit and do the statistical analysis.
You actually go out and do experiments.
When two-year-olds do it, we call it getting into everything as opposed to call it having an experimental research program.
And there's been a bunch of really beautiful work that's shown that when kids are getting into everything,
they do it in this remarkably systematic way and in a way that gives them just the kind of data and evidence
that they need to solve the problems that they're facing.
But it's rather remarkable that in this causal inference literature,
as well as, of course, in the AI literature in general,
there's not very much work on how this kind of active learning experimentation is possible,
either in philosophy of science or in computer science.
Active learning very, very rapidly becomes, like Bayesian search, rapidly becomes intractable.
So how is it that we actually can do this as scientists?
How is it that we can actually do this as children?
How is it that a computer system, an artificial system,
could decide what kinds of interventions should you perform to get the right kind of data about the world?
And I think it's important that this is the crucial thing, or at least one crucial thing,
that discriminates the current systems from what a truly intelligent system would be like.
Because, again, especially if you think about non-stationary environments,
and that's our evolutionary, our human ecological niche is the non-stationary environment.
That's what we're adapted for.
When things change, the only way you're going to get the right data
is by actually going out and doing things and interacting with this changing environment.
So the question that we've been asking over the past few years is,
could we design exploratory AI?
Could we understand something about the techniques that the kids are using,
and could we design AI agents that do something similar?
And maybe in the interest of time, I'll see how we're doing.
Okay, so maybe in the interest of time, I'll skip over a bit of this.
But, of course, one of the reasons why exploration has been relevant,
people have thought about it a lot, is because of this explore-exploit trade-off
that shows up in many, many, many different contexts within AI and within computer science.
So there's this very fundamental provable tension between your ability to look at many, many different kinds of solutions to a problem,
and your ability to just narrow in on the solution that will be most effective in the shortest run with the fewest use of resources.
You know, you can think about it as if there's a big box of potential solutions to a problem or hypotheses about the world.
And one thing you can do is just do a little cold search and very near to where you already are,
which will be a quick and dirty way of just hill climbing your way to a solution.
But there might be a much better solution that's much further out in the box.
And if you just do the little local hill climbing, you'll never get there.
But if you bounce randomly and wildly about in the box, then you're more likely to actually find that solution.
And I'll skip over a few slides that suggest that that's what children are designed to do.
That's what children are really, really good at.
But here is a slide that's relevant.
If you, sorry, so this is where I actually argue about all this so you can find out about it there.
But if you think about the features that make you a good explorer versus the features that make you a good exploiter,
they're really in tension with one another.
So things like randomness, variability, noisiness.
Who do you think sounds more like the person who's bouncing around that box in a random noisy way?
Your four-year-old are you, right?
That's what four-year-olds do.
And we have a bunch of arguments and a bunch of data that suggest that, in fact, preschoolers are better explorers,
wider, more high-temperature explorers than adults are.
Okay.
But having said that they're very good explorers, we're still facing this question about,
well, how do they do it?
What algorithms enable them to explore as effectively and widely as they do?
And this is where a very different tradition than the Bayesian tradition comes in,
which is the tradition of reinforcement learning.
And I have to say, as a boomer cognitive scientist, when I first saw the revival of reinforcement learning,
both in computer science and neuroscience, my thought was, oh, my God, what are we doing?
Are we going to have to go back to, you know, wearing bell bottoms next?
I thought we got rid of all that in the 50s.
But, of course, there's a brand new generation of work in reinforcement learning that has, you know,
revolutionized AI in lots of ways and been very influential also in neuroscience.
And one of the interesting things about reinforcement learning is that you could think of it
as being a kind of proto-causal learning.
Because reinforcement learning is a case where you actually are intervening on the world
and then you're seeing what the outcomes of your interventions are.
But it's a very narrow kind of causal learning.
It's restricted to the things that you do yourself and to utilities as outcomes.
It's restricted to the things you do yourself and the positive or negative reward that you get as a result.
Whereas causal learning enables you to just go out and work on the structure of the environment.
So is there a way that you could think about reinforcement learning that would actually speak to questions of causal learning?
And what we've been thinking about and working on most recently is the idea of intrinsically motivated reinforcement learning.
So now instead of having the reward be the cheese or the absence of the shock,
now the reward that you've got is an epistemic reward.
Now the reward you've got is something like information or novelty or something that will help you understand what's going on in the world.
And there's been a number of attempts to have these kind of intrinsically motivated rewards with reinforcement learning.
And the result of having this kind of intrinsic reward is that you get better exploration than you would
if you were just narrowly trying to accomplish a particular utility.
The intrinsic reward that we've been most interested in is a reward called empowerment.
And the idea behind empowerment, it's an information theoretic idea, but it's not just information game.
The idea is that what you want to do is maximize the mutual information between your actions and outcomes.
And also maximize the diversity of your actions.
What that means is what you really want is to find the things in the world that you can control or other people can control.
So what you want to find is the things in the world that if you wiggle them will cause something else to wiggle.
Now if you go back to thinking about that definition of causality, that interventionist definition of causality I started out with,
that's just what a causal relation is.
So what I've argued in this new paper that's in this, so this work was done a while ago.
And interestingly it was done in the context of trying to think about the evolution of intelligence in the first place.
And again I think this is an important moral thinking about current AI.
If you look at the evolution of intelligence when you start seeing brains is when you start seeing eyes and claws.
So it's when you've actually got animals that can move and can see and coordinate what they see and how they move
that you start getting things that look like brains.
And the thought is that empowerment is a way of describing this coordination between the actions that you perform
and the things that you sense, the things that you see in the world around you.
And the argument that I've made is that you could think about empowerment gain as intrinsically a form of causal learning and vice versa.
That if you gain empowerment, if you have more things in the world that you can wiggle to make other things wiggle,
you will have more information about the causal structure of the world.
And if you have more information about causal structure, automatically that means that you'll have a higher empowerment.
In fact, when you look at children sort of amazingly what you see is that they're far more driven by empowerment than anything else.
This is a beautiful set of experiments from the 70s.
It's interesting.
It was actually Carolyn Robey Collier back in the 70s was trying to see if you could operantly condition babies.
In other words, could you use reinforcement learning with babies?
And what she did was tie the baby's leg to a mobile and then she just saw what the babies did.
And what she discovered is three-month-olds will experiment with their leg to see what happens with the mobile.
And she showed that it isn't because they care about the mobile.
They won't treat the mobile as a reward.
What they care about, and she said this explicitly, is the relationship between the mobile and their actions.
And they'll do things like kick for a little while and then stop and try one leg and try another leg.
Have this kind of experimental research program.
And as a developmental psychologist, I am obligated to show cute baby videos.
This is my grandson, Kit, who's about a year old in this video.
And what he's doing is exploring a novel causal relation that had never existed before.
His grandfather is a musician who's playing in the background.
And you can see that what Kit is doing is figuring out that there's this relationship between hitting a piece of metal and getting a particular kind of pure tone.
And what he does is start out by actually doing the right thing with the mallet.
And then he stops and he tries the other end of the mallet.
And then he stops and he tries his fat little hand on it, which doesn't make any noise at all.
And then he goes back to the end of the mallet.
And then he tries doing it high up on the xylophone and then lower on the xylophone.
Someone who was watching this video recently said, as I'm a parent of a two-year-old,
and all I can think about is, oh, when is he going to stick this in his eye?
And that actually gets to the last point, which is that at one point his grandfather says,
kid, don't put that in your mouth.
And that gets to the, so here's the paper that's coming out in Philtrans,
but is already available on a preprint where we make this argument.
And we have a bunch of empirical evidence that shows that even very young children
are in fact showing this preference for empowerment and that's enabling their causal learning.
Here's another paper that's making the same point.
In this paper, we actually use Minecraft and we had both children and agents
with different kinds of intrinsic motivations exploring a Minecraft environment.
And something like an empowerment strategy both leads to the greatest exploration
and characterizes what the children are doing when we just say to them,
just play with Minecraft.
All right.
So the last point, which I'll go over quickly, but I do want to make this point, is this.
So, you know, the point about Kit's grandfather saying, don't put it in your mouth, Kit,
and the person in the audience saying, oh my God, he's going to stick it in his eye,
is that this kind of exploration and empowerment is only possible because care is taking place.
It's only possible because a mom or a grandmother or a grandfather is sitting there
and making sure both to give you the xylophone so that you can explore it
and to make sure that you're going to be safe when you do.
And one of my favorite results in all of psychology is this result from originally from Regina Sullivan,
who's a comparativist, and more recently Nim Tottenham showed this with human children.
So the oldest, most classic result in all of psychology, the basis for all of reinforcement learning,
is you put the rat in the maze, it goes down one arm in the maze, it gets shocked,
goes down the other arm, nothing happens.
It will never go down the arm where there was the shock.
It just stays away from it forever after, which is a great exploit strategy, right?
That will make sure you don't get shocked, but of course it's a terrible explore strategy
because maybe the environment is non-stationary, maybe sometimes there's cheese,
maybe there's only a shock every one out of ten times.
And the remarkable thing that Regina Sullivan discovered is that this is, of course,
true Psych 101 for adult rats, but juveniles are exactly the opposite.
So the juveniles will prefer to go down the arm of the maze that leads to the shock,
and anyone who's had a two-year-old or a teenager will recognize this is true for human children as well.
But in fact, Nim Tottenham showed, not with a shock but with an unpleasant noise,
that this was indeed true.
But there's an interesting catch.
They'll only do it, both the children and the rats, if they can smell the mother.
So if the mother is present, then the rats will explore the aversive stimulus.
But if the mother isn't, or indeed if the mother herself has been negatively conditioned,
so the mother is giving off an odor of terror, then they won't explore the other alternative.
So what seems to happen is that the presence of the mother is saying,
this is a safe context in which you can explore.
And in work that we're doing in my lab, we've shown that this is also true for human children.
And in fact, the kind of flip side of the learning and exploration piece
is that the smart animals also have larger caregiving investments.
So if you compare animals with different sizes of brains, the larger your brain,
the longer your childhood, the more investment there is from caregiving.
And yet, in spite of that, people haven't thought very much about caregiving.
It's been sort of strikingly absent in the psychological and philosophical
and political science and economic literatures.
And it's partly because it doesn't fit the standard models of politics and economic science.
It's also probably my co-author, Margaret Levy, and I have noticed,
is there anything in common about all the people who were writing these things about economics
and political science and moral psychology that might explain why they never talk about caregiving?
It's the most important thing in most lives, but it's just invisible in all of these disciplines.
And we finally decided it's because they're tall.
They're just very tall and they don't see the children around them.
So I'll skip over this a bit and just end by saying how does care function in science
and how does care function in AI?
So our project now is to have a social science of caregiving
and the cognitive science and a computational account of caregiving.
I can talk about that a bit more later.
But of course, when you think about science, science depends on care.
Ideally, that's what we want our funding agencies to do.
That's what is happening with the Schmidt Science Agency, right?
And we want our funding agencies, like our parents,
not to be dictating exactly what we're going to do,
but to be like that rat mom who sits there and says,
yes, it's safe, you can explore this risky alternative.
And of course, this speaks very deeply to those of us in America now
who are facing the bad mothers or lack of caregivers,
the destruction of the caregivers in our culture.
And the last thing is thinking about the idea of care in the context of AI
and the alignment problem.
Again, going back to the Ted Chiang story,
what happens in the Ted Chiang story is that these human caregivers
become attached to their AIs.
They love them.
They want to support them.
And yet, at the same time,
they know that they're going to have to grow up and become autonomous.
So, famously, one of the things that people worry about in AI
is this alignment problem.
How are we going to get AIs to have the same kinds of goals
that we humans do?
And one common way of doing this is what I think of as the Stepford Wife,
another ancient reference, this kind of Stepford Wife solution,
which is we're just going to get them to want exactly the same things that we do.
But of course, if you think about it,
if we actually had AIs that were autonomous enough to change their goals
and preferences in the light of a changing environment,
who had the same kind of intellectual capacities that our children do,
simply getting them to match our own preferences would not be the right way of doing it.
And in fact, every one of us who ever has a child faces the alignment problem.
What we do with every generation is try to say,
we know that you're going to have different goals and preferences
and utilities and knowledge, but we want them to be good ones.
We want the fact that Grandmom told you the stories about Peter Rabbit
to enable you to recognize some of the norms about what's a good thing to do
and what isn't a good thing to do.
Even though what you end up with might be different,
not just in terms of what you know,
but also in terms of your goal and utility structure than ours.
So I think if we are thinking about autonomous systems,
the right model for our relationship to those systems should be a model of care,
that our relation to an autonomous AI would be like the relation that we have to our children.
What we want to do is both support their exploration and knowledge
and constrain it in a way that would let it be positive rather than negative.
So let me end with my children, namely all the graduate students
who actually do all the exploration,
and even more my academic grandchildren who actually do all the exploratory work
while I sit there and write the grand proposals.
And also my mothers, the various foundations that have at least up till this year
have supported this work.
And let me stop and take some questions.
Awesome. Awesome. Thank you so much.
I haven't got the microphone. There is a roving microphone.
Please, if you'd like to ask a question, put your hand up,
and the microphone will come to you.
First hand I saw was Jen here and then...
Are there any women who have questions?
I've had this experience before that somehow it always ends up...
We'll start there, but I just want to encourage the women to ask questions.
Then they always come up at the end and ask all the good questions.
So I like to have this as a convention.
But do please start.
Firstly, thanks for the great talk and sorry for not being a woman.
You said in an earlier slide before talking about intrinsic curiosity and caregiving
about how reinforcement learning struggles in high dimensional environments.
That's right.
I was just curious to hear to what extent do you think it's a fundamentally difficult problem
versus we just haven't found the right algorithms or inductive biases or models, that sort of thing.
Yeah.
No, I mean, I do think that if you think about the explore exploit distinction,
that's a foundational provable tension, right?
So that suggests that it isn't just that we haven't got the right set of utilities worked out
or we haven't got the right way of searching through the utility space.
There's really something that's intrinsically intentioned between trying to maximize reward
and the kinds of things that you do when you're maximizing utility-based rewards
and what you do when you're trying to maximize epistemic rewards like exploration.
Now, of course, you know, the intrinsic rewards like having a system that's rewarded for empowerment,
that's an RL system.
It's using the same basic apparatus that RL systems use.
There's a lot of challenging technical problems.
I have to say after 10 years at Bayer, my conclusion is everything is intractable
except gradient descent.
That's essentially what we've learned from all these years.
But just because it's intractable now doesn't mean it'll be intractable forever.
So you could think about what I'm proposing as being,
yeah, we're going to use reinforcement learning.
We're just going to have a different reward structure.
Or you could think of it as it's going to be really different
from the kind of reinforcement learning that we're used to.
And a really interesting point, I think, is in reinforcement learning,
there's this contrast between model-free and model-based learning.
But the model-based one, maybe this is speaking to your point about priors and inductive biases,
have to get the model from somewhere else, right?
So they're always getting the information about what the model should be from something other.
And once you've got that, that makes reinforcement learning much more efficient.
But, of course, what the kids are doing is how could you actually construct the model
as a result of doing the things that you're doing in reinforcement learning?
And my thought and hope is that empowerment, because it's so closely related to causality,
sort of automatically would get you a causal model through the act of doing reinforcement learning.
At least that's the hope.
And we have a little bit of evidence that that might be true.
Yes?
I can shout.
I can shout.
Yeah, you can shout.
I can shout as well.
Oh, no, actually, there's people online.
Yeah, just because...
Oh, because there's people online, yeah.
Thank you for an amazing talk.
My question is more about nomenclature, and maybe it is a moot point,
but there is a lot of talk in the LLM world and people talking about agents
about whether things understand and how you can quantify whether something understands.
Do you think it is a moot point to even try to do this?
And the reason I'm asking is because every time people argue about whether something understands,
they don't define what understanding means.
Like, the Turing experiment is basically circumventing the whole trying to define and understand
by saying mimicry is enough, and then the Chinese room experiment saying,
well, that's not enough.
It's like, okay, but can you measure this?
And is, like, causal learning and the world building stuff something that can help us put a number
on whether something understands?
Yeah.
I think that's an excellent question.
And one of the other things that I've argued for and other people have argued for,
it's very strange because in the AI world, the way that you evaluate systems is through benchmarks.
And, of course, we'd never do that in developmental psychology, for instance.
So, it would never occur to us to say, is this child smarter than this other child?
Well, we're going to have a benchmark and we'll give them a bunch of outside of, you know,
some weird psychometric stuff.
And we'll give them a benchmark and see how well they do on it.
What we do is we have experiments with many, many control conditions.
And, you know, we have, it takes tremendous ingenuity to figure out when they do a particular thing,
why are they doing it?
What kind of underlying computations, what kind of underlying algorithms do they have?
And it's not obvious at all.
I mean, you know, you can get a four, and thinking about four, three and four-year-olds
really makes this vivid because you can have a four-year-old who says something that looks
like they've really understood something, and then you ask them the four questions down the road
and you realize, oh, no, they're just parroting what it is that they've heard before
or they're trying to do something else or they're answering it in a way that isn't the way that you thought.
That happens, that's like every day, every study that you do, you discover that with four-year-olds.
But then sometimes, I mean, in fact, mostly what you discover is that they know much more
than you would have thought too.
So, I showed the picture of our little blicket detector.
It's a little box that lights up when you put some things on it and not others.
If you ask the four-year-old, do you understand conditional probability, you get a blank look
and a beautiful story about birthdays and ponies, right?
But when you actually give them data and very carefully control the conditional probabilities
and then say, make the machine go, they make the machine go the right way.
So, I think you really need to have those kinds of methods that come from, here's an alien intelligence,
it's doing something that looks kind of like what, say, we grown-ups do,
but then it's doing these weird things too.
You have to use those kinds of methods to answer that question rather than just saying,
does it understand or not?
And again, I think something like causal inference is a nice example where it isn't,
can it understand?
It's, can it do the kinds of computations you'd have to do if you had a causal model, for example?
Is someone right behind you, I guess?
Yeah.
Hello.
Thank you very much for the talk.
I have, would you be able to elaborate a bit about the difference between predicting
and understanding causation?
Right.
Because if I can predict the future very well, and I can guess that by wiggling something,
then I can predict that the other thing would wiggle.
And if I'm really good at understanding causation, then I'm also very good at understanding, predicting.
So, could you elaborate a bit more on the difference?
Yeah.
So, if you think about, if you think about the difference between just doing prediction
and doing causal inference, again, I, you know, this is my, my mantra,
is think about the non-stationary environment, right?
So, we can send a rocket to the moon, right, without ever having seen any evidence at all
about rockets in the moon, right?
I mean, we've never seen any, we've never seen any correlation between sending up a rocket
and having it land on the moon or a correlation between sending it up and having it crash into
the moon, right?
We have no past data about that set of relations.
But when we get a causal model, a complicated causal model through physics of all the relations
between the rockets and the moon, we can do it just on one go.
We can do a kind of one-shot learning.
And we can even do things like, say, we know that if we, conversely,
if we know that the tides cause the, the moon causes the tides, even though we can't actually
ever intervene to change the moon, we know that if we did, what we mean when we say it causes the tides,
is that if we did, we could, we could bring about these new outcomes.
So, so the idea is that when you have a causal understanding, it's not just that you understand the
predictive structure of the current environment, but you can create new environments that are really different
from the environment that you're in.
And a really interesting thing that comes from the empowerment view that I've been thinking about a lot,
just think about Kit with the xylophone and his grandfather with the piano.
Those are really interesting human examples where we actually bring causal structure into the world.
That's what all of you engineers, I'm gesturing at the engineering world, do is take things
that have not had causal structure before, and now suddenly there's causal structure in the world
because we humans need to take advantage of causal structure in our artifacts, for example.
And I think in our social lives as well, one of the things that we do when we have norms is,
we introduce a kind of social causal structure into the world that wasn't there before.
And that means that those are some of the things that I think make causation
and causal understanding different from just prediction.
And there's a lot of interesting technical, philosophical, and computational work trying
to make that argument.
People like Judea and Jim have made those kinds of arguments.
Thank you.
Thank you very much indeed.
Do you think it's true that our education systems are tending to constrain
and reduce the amount of exploration that our children are turning into our students and our adults?
If so, what could be done to change that?
And could we also induce our AIs to be more exploratory?
Right.
And how?
Yeah.
That's a great question.
And actually, it was the question before about, you know, could we get RL to work better?
There's something called Good Heart's Law, which I really like, which is the idea
that when you maximize for a particular reward, what ends up, which you think is a sign of something else,
what ends up happening is that you just, you know, kind of reward hack onto that particular reward.
And I've had this depressing thought, which is that that's what grades do, right?
So what grades do is we want the grades to be a proxy for exploration and knowledge, but
of course, what we end up doing is training kids to reward hack the grades.
And that's gotten, at least in the United States, that's gotten worse rather than better.
I will be sentimental for a minute and say when I first arrived at Oxford in 1976, I remember
going to the head of the department and saying, what do I have to do now?
And she said, nothing.
You'll be here for the next five years and read a bunch of books and, you know, think
and do experiments and then write a thesis five years from now.
And we might pass you, we might not, but, you know, that's what you're going to do.
And I said, but what about my classes?
And she said, well, there aren't any classes.
You're a graduate student, right?
And it was shocking and it was also incredibly liberating.
And at one point I was talking to the head of the Chancellor of Oxford.
I think it was at Davos.
And I said, you know, what you really want in a university is to have a preschool.
Like the wonderful thing about Oxford was that it was being in preschool for a scientist
where people were taking care of you and they were feeding you really well and, you know,
you got lots of nice, you got lots of nice recess time running around and then you could
just sit and talk to your fellow students and figure things out.
And I was quite pleased because he was very positive about that.
He was not insulted by that comparison.
He was very pleased by it.
And, again, I mean, I think if we want AIs that are going to be more exploratory,
we're going to have to think about very different kinds of training regimes then.
I mean, supervised learning is essentially what we do with kids in school
and that has lots of advantages.
You can get lots of learning that way.
But it's exactly the thing that weighs against having general exploration.
Great.
It's on? Oh, sorry.
Thanks so much for such an excellent talk.
My question is almost a human-computer interaction question.
Yeah.
You've covered with the stone soup analogy and maybe in large models or social technologies
that these LLMs are best understood as these mimetic technologies,
more akin to, like, cultural institutions like libraries.
That seems to me in tension with the way maybe the commercial incentives or, like,
the current way that we are putting agentic faces on these models through something like
construction fine-tuning is going.
I can see that maybe there are some negative elements to that that you want to commercially motivate,
that you're building something intelligent and get a bunch of VC money.
But maybe also that's the right window into the latent space, as it were.
So I'm curious if that's a tension for you or if that's resolved or how you think about that.
Yeah.
That's a really good question.
And that's the five slides that I got rid of before I started the talk.
One of the things that I think is really fascinating is that if you look at, and this is a bit of a humanities pitch,
if you look at history, which people in AI, you know, like three years ago counts as being ancient history,
not like what they're doing in classical epigraphy down by the Thames,
if you look at these historical examples, one of the things that's interesting is that people have always used
fictional agents as a way of accomplishing cultural, as a way of accomplishing cultural transmission.
So if you think about, you know, the Greek gods, right, that's a way of telling you things that are important
about the values in your culture by having individuals who are these kind of fictive agents
who you tell stories about and interact with.
So I say sometimes, don't say that I said that AIs are gods because I think there are too many people
in the valley who already believe that.
But put it another way, they're only gods that don't exist like all the other gods, right?
That's their most notable property.
But if you, another wonderful rabbit hole is if you look at that 18th century print development,
that's the rise of the novel is happening at the same time.
And it turns out that the novel is very, very closely linked to these changes in printing technology.
So Samuel Richardson, who was the first, arguably the first English novelist who wrote things like Clarissa and Pamela
that were actually kind of great works with a bit of softcore porn in the edges, was a printer.
And so what happens when you have a novel?
So if I say, what would Elizabeth Bennett do, which is kind of one of my morals from,
especially when I was a 20 year old in Oxford, that's a way of saying,
here's a fictive agent who is conveying something important about the cultural background,
even though you don't think that Elizabeth Bennett or Zeus or Peter Rabbit,
my other favorite example, actually exists when grandmother,
when the postmenopausal grandmother, who's the engine of all of cultural transmission,
says to her grandchild, this is a story of Peter Rabbit, and he went into Mr. McGregor's garden,
and then he had to have chamomile tea instead of bread and milk and blackberries.
She's not telling him about the rabbit.
What she's telling him about is don't transgress these kinds of norms
because then here's what the negative outcomes are going to be.
So I think it's quite interesting that we so often use these kind of fictive agents
as a way of conveying some of the information that we get through cultural transmission.
And I think it's no coincidence that we have these artificial agent-like interfaces
that we use with something like, you know, where ChatGPG says,
oh, I'm so sorry, I just hallucinated that result.
In some ways, that's just a commercial trick, but I think it also may have something deeper
to say about the way that we design fictive agents as a way of interacting
with cultural technologies in general.
There's a beautiful quote that I also took out from Socrates about writing,
which he thought was a terrible idea.
And he says, here's why it's a terrible idea.
First of all, you're going to believe what you read in books.
You won't be able to interrogate them the way you can in a Socratic dialogue.
And it's a terrible idea because no one will remember all of Homer anymore,
both of which are true.
But even better, he has this lovely phrase where he says, you know,
they talk to you as if they actually had minds and ideas,
but then when you ask them questions, they just sort of sit there dumbly
and say the same things over and over again.
That's literally his description of writing, which sounds a lot like ChatGPT, right?
You know, it feels as if because it's using language that it's actually got a mind,
but then it just keeps saying the same things over and over again when you're interrogating it.
So I think that's another nice example even if you think about the origins of writing.
We have time for folks.
Thank you so, so much, Alison.
Huge round of applause again for amazing.
Thank you all for attending this most amazing evening and the seminar.
I'd just like to make a personal thanks to the local Schmidt team, Tanya, Ben, Wendy, Catherine.
Also to Jim for coming on behalf of Division and Lionel representing Rubin College.
I think some of us are off there for some food.
I hope it's not got too burnt in the oven.
But thank you all very, very much.
Schmidt team, it's a privilege to work with you.
Thank you so much for all the organization and help with this.
And Alison, thank you very much.
Thank you.
Thank you for welcoming me back to Austin.
