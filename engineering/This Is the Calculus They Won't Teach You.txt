Think of a stupidly big number. A Google. A Googleplex. 10 to the power of a Googleplex.
That number is so massive that it has no meaning in the real world, but it's still nothing compared
to infinity. Try to imagine an infinitely small number, also called an infinitesimal. 0.0000 on
and on. You need to attach more zeros than whatever massive number you just thought of
in order to get a true infinitesimal. To put the ridiculousness of those numbers and infinity
into perspective, the observable universe is about 93 billion light years in diameter.
If, for whatever reason, you wanted to express that monstrosity in terms of the smallest unit
we have for distance, plank length, you would get this number. This is the largest possible
number you would ever need to use to describe a length. And it still doesn't even come close
to a Google. And a Google doesn't even come close to infinity. As anyone living in a finite
world with a finite lifespan knows, infinity is a horrifying concept. I mean, who as a child did
not lie in bed filled with a slowly mounting terror while sinking into the idea of a universe
that goes on and on forever and ever? Asks Rudy Rucker in Infinity and the Mind.
We should have absolutely nothing to do with this. Us trying to play with infinity would be
like an ant trying to kill God. It's just too powerful of a concept to do anything with.
We should just erase it from our minds and live out our lives using numbers that actually
have meaning to us.
Hey, hey, wait, what are you doing? I'm doing calculus. Is that an infinity I see in there?
Yep, nothing too mind-blowing, actually. Why in the world did we as a species ever agree
to do math with infinity? Oh, come on, it's not that big of a deal.
What sick people would ever voluntarily summon this demon of an idea? There has to be something
wrong with this. How did calculus even begin?
If you've taken a calculus course before, you may have been taught in this order.
This makes absolutely no sense, right? I mean, integrals are the opposite of derivatives,
and to even define derivatives, you need limits, right? Well, this ordering is closer to how
the idea is developed historically. That being said, where on this map do you think the first
calculus concepts began? Here? Maybe here? Because it's here. And where on this timeline
did you think calculus began? Because it's around here. The story begins with the ancient
Greeks. What were they up to? Let's start with a tour of old math. This is math. This
is kind of math. And this is math, right? Well, this would look like nonsense since the Hindu-Arabic
numerals and math notation weren't used back then, and none of this exists yet. But this is math.
In ancient Greek philosophy circles, geometry is all the rage. With our math knowledge now,
we can find areas of shapes like this, and this, and... Oh god. Never fear, says a certain Archimedes.
If you cut up a circle into pizza slices, then rearrange those slices like this. It almost looks
like a parallelogram. If you make those slices smaller and smaller, you'll eventually get a
perfect rectangle, whose side lengths are the radius of a circle and half the circumference.
So since the circle has the same area as this rectangle, the area of a circle is its radius
times half its circumference. That's ridiculous! Those tiny pizza slices won't ever become a
rectangle. But what if you make them infinitely small? You can't do that! Infinity doesn't exist!
This was a convincing argument at the time. Let's travel 150 years before Archimedes was born to see
what the ancient Greeks thought about infinity. In order to go from point A to B, I need to get
halfway there first. Then I need to go halfway again, and so on. Wait, I'll need an infinite
amount of halves to get to point B, and I can't take an infinite amount of steps, therefore I cannot
walk to the other side of my room. Okay, if you really don't believe that the area of a circle
is equal to the area of this rectangle, then let's assume that this rectangle is smaller than the
circle. Because if the area of the rectangle isn't equal to the area of the circle, then it has to be
either less than or greater than the circle. So let's start with the assumption that the area of
the rectangle is less than the area of the circle. Now let's cut off the curvy parts of the rearranged
circle so we get this parallelogram. We know that this parallelogram is smaller than the circle because
we literally just cut off these pieces, but is the parallelogram bigger or smaller than the rectangle?
Well, we define the rectangle to have a height that is equal to the radius of the circle.
Meanwhile, since we cut off this extra part to make the parallelogram, the parallelogram's height
is less than the radius by a small amount. So the parallelogram is smaller than the rectangle,
which we assumed is smaller than the circle. Now let's cut this circle into tinier pizza slices,
and let's make a new parallelogram by cutting off the curvy parts again. Hang on, this parallelogram
is slightly taller than the other one, since the curvy part we're cutting off also got smaller.
If we keep cutting the circle into smaller pizza slices, then the curvy part that's left over will
keep getting smaller and smaller. There should be a point where the curvy difference becomes less than
whatever difference there is between the circle and the rectangle. So eventually, the parallelogram
should become bigger than the rectangle while still being smaller than the circle. But wait,
how is it even possible for the parallelogram to become bigger than the rectangle? The rectangle's
height is the radius of the circle. That's how we defined it. But the parallelogram's height
will always be the radius of the circle minus some really, really small number. We have two
conflicting conclusions from following two logical ways of thinking. So maybe that means the rectangle
isn't smaller than the circle after all. What if the rectangle is bigger than the circle? This time,
let's make a parallelogram by extending the sides of the slices until they become triangles.
The parallelogram is bigger than the rectangle because the height is a little bigger than the
radius. The parallelogram is bigger than the rectangle, which is bigger than the circle. If we cut
the circle into small slices again and repeat the parallelogram making process, we start getting
smaller parallelograms the more we cut the circle. Eventually, the parallelogram should be smaller
than the rectangle. But that conflicts with the fact that the rectangle's height is the radius of
the circle, while the parallelogram's height is always the radius plus some small number. So this
rectangle can't possibly be bigger than the circle. Well, if this special rectangle is neither smaller
than nor bigger than the circle, then it must be equal to the circle. And therefore, the area of a
circle is its radius times its circumference over two. All right, fine. I guess you win this time.
If you got lost there, it's okay. I did the first 20 times I looked at it as well. But the key parts
that you should appreciate about this argument are that one, it cuts something into tinier and tinier
pieces, then adds them back up again. That's what integration is. And two, even though it uses the
idea of infinity, it never really uses infinity. It's almost like Archimedes is very carefully trying
to tame a beast. There's a really nice lesson about math that you can take away from this proof.
Notice how Archimedes started with a hazy argument about how you can make a rectangle out of pizza
slices of a circle, and then created a rigorous proof for it after that. The big part of math is
about finding what people call a moral solution to a problem, which is the solution that should be
right, and then coming up with a formal proof to justify it. Archimedes wrote about this in a text
called The Method, and this method is still recognized today. Hey, here's even a quote from
one of Grant Sanderson's podcasts talking about this idea. The mathematical equivalent of you playing
with your ribbon for the undergrad result, where you first figure out what's true, and then when you
need to make it rigorous, that's a second step. But just into like, what do you want to be true?
That should be treated as a step worth highlighting and holding up as just as valuable as the rigorous
follow. Exactly. Exactly. And you know who did this? Archimedes. So I give an example.
It's the 800s, and you are a scholar in the Baghdad House of Wisdom. You're sporting a cool beard and
turban, and you just finished reading the works of Archimedes and other Greek philosophers. What a
wonderful time to contribute to the development of algebra, you think to yourself. Algebra? I thought
this video was about calculus. You're right. But the development of algebra was crucial to the
development of calculus, and it just so happens to have happened in the middle of our story. So let's
look at how this algebra thing is going. You know these? Now they're starting to get popular,
especially because Muhammad ibn Musa al-Kawarizmi is using them in his book Al-Jabre. But these aren't
popular yet. Instead, equations are written as sentences, and numbers are thought of as geometric
shapes. A square is a square, a cube is a cube, and uh... Ah. Yeah, they didn't really do these.
They also didn't do these, since you can't have negative area or volume. 800 years later,
these ideas caught on to a new group of mathematicians, who not only spread Hindu-Arabic
numerals, but also added their own notation. Francois Viette introduced the use of letters as
symbols for unknowns and constants, and Rene Descartes standardized the use of these letters
for variables and these for constants. Hey, math is now starting to look a little bit more like math,
but the best is soon to come. After he said, I think, therefore I am, the guy that looked like
a cartoon villain had the idea to combine geometry with algebra to make a coordinate system. This was
revolutionary. Look at what happens when you graph a quadratic. It's a parabola. To understand just how
crazy this is, the ancient Greeks had been cutting up cones to study different shapes for centuries.
Archimedes even wrote a whole book specifically about the parabola. They had been studying these
shapes without a standard way to easily manipulate them, but with Descartes' analytic geometry,
you have the power to analyze any curve as a family of equations. Another thing is, up until this
point, the idea that you could compare two variables in this way was ridiculous. This number represents
money, and this number represents cabbages. Those are two very different numbers. You cannot put them
together, but Descartes did it, and somehow it worked. Now we can visually represent relationships
between variables, and it's all thanks to Descartes and his method. Speaking of relationships,
hey Descartes, look, there's this Fermat guy who wants to talk to you. He says he developed
analytic geometry a decade before you did. He says he did what? And so began a fierce lifelong rivalry
between two huge names in math and physics. Armed with analytic geometry, this battle is where some
of the origins of the modern derivative lay, and where we can start talking about calculus again.
Now, the idea of an instantaneous rate of change goes back to at least Zeno, who, other than the
argument about not being able to get out of his room, also famously riddled that an arrow moving
through the air is really an infinite collection of moments where the arrow is unmoving in each.
Some practical uses of derivatives can be found in the 500s, where Indian mathematician Aryabhata
discovered that the rate of change of the sine function is the same as the cosine function,
and used this to approximate specific values of sine. But the derivative in the graphical sense
starts here. Descartes discovered a way to find the tangential slope by finding the radius of a
circle centered at where this line hits the x-axis. Fermat found the tangential slope by doing something
similar to solving this as h gets smaller and smaller, which you may be a bit more familiar with.
But more than just finding the derivative, Fermat also tackled some other problems that influenced
later calculus development, like using derivatives in physics, finding the maxima and minima of curves,
and finding a general integral power rule. Descartes and Fermat's work will pave the way for
others to make discoveries that will forever change the world.
Hey, let's check in on the development of integral calculus. What have they been up to?
While algebra was being developed in the 800s, some scholars in Baghdad were working on finding
the volume of certain shapes. Quick side note, if you've ever taken a calculus course before,
have you ever wondered why in the world people would ever need to find a volume of solids that
revolve around an axis, and why the AP test keep checking that we know this formula?
Oh, oh, I see. In 1600s Italy, Bonaventura Cavalieri performed integration with a controversial
method where he cut a 2D shape into infinitesimally small slices, then rearranged them. His student,
Evangelista Torricelli, later used his invisibles to find a shape with an infinite surface area and a
finite volume. But notice that, unlike Archimedes, Cavalieri and Torricelli play directly with
infinity, and mathematicians really didn't like that. Now hold on to that thought for later,
because calculus is about to change fundamentally. Ideas behind integral and differential calculus
have so far been developing separately over thousands of years, but we're about to see the
birth of a new calculus, and it starts with two stunning men. I'm talking about Galileo and
Kepler, of course. These guys actually came before Descartes and Fermat, but I bring them up now
because I think their work transitions better with Newton and Leibniz than the others, and the way
that they reflect the beginnings of mathematicians discovering the relationship between differentiation
and integration. A key reason why these two mark the beginning of this relationship is because,
get this, isolated behavior in the real world. A lot of the examples and mathematicians I brought
up so far have been doing what feels a lot like pure math, or math for the sake of doing math. These two
will do what feels a lot more like applied math, where they use math to model physics. Galileo Galilei isn't
called the father of modern science for nothing. When he asked the question, he made sure that he
could answer it by focusing on that one question only. One of the questions he asked at the time
was how objects fell, and he went about answering it by creating the most ideal system for measuring
the effect of gravity on an object. A ramp where he could time how long it takes for a ball to roll
down to different lengths using a water clock. When the ball starts moving, he opens a valve and lets
water flow into this bucket. When it reaches a checkpoint or stops, he closes the valve and sees how much
water has accumulated. I wonder if there's any pattern to how this ball falls. Wait a minute,
in one second, the ball moves one unit. Then in the second second, the ball moves three units. Then
in the third second, the ball moves five units. That's odd. Gravity follows some rule to do with
odd numbers. If you know a bit of physics, you might know that this is because gravity on Earth's
surface can be roughly modeled by a parabola, which follows this pattern. One unit traveled is
one squared units. One unit plus three units is two squared units. One unit plus three plus five
is three squared units, and so on. You may also start to notice how this is the first hint toward
discovering the connection between the integral and the derivative. The total distance that the
ball travels, which feels a lot like integration because we're adding up the distances it travels
in each second, can be calculated by a pattern in the rate at which the ball covers distance,
which feels like a derivative. Not only that, even the way that Galileo measures any of this
feels a lot like integration and differentiation. He first tracks how much water accumulates in a
bucket, then subtracts differences in how much water there is, to find the rate of change of
the ball. While Galileo was looking at gravity on Earth, Johannes Kepler studied gravity in the
solar system. He discovered three laws after so much trial and error. But we're only interested
in the second one, that the planets orbit in a way that for two time intervals, the area of these
pizza slices are the same. Huh, I wonder why that happens. Ah, what the f- It's because velocity is
constant while acceleration is radial. If an object keeps moving at the same speed, it sweeps out the
same area in the same time interval. If the angle at which an object moves changes, but the object
keeps moving at the same speed, it'll still sweep out the same area in the same time interval.
Who the hell are you? Isaac Newton, born on Christmas, had a father who died before he was
born, and a mother who left him for a rich second husband. Went to college poor, even though his
mother definitely could have supported him, and was understandably antisocial. Would end up on this list,
be simultaneously hated and loved by billions of physics and math students over the centuries,
and make everything about modern life possible. It's around 1666, and there's an epidemic in
England. Hey, what did you do over quarantine? I learned to play the guitar. I got good at
painting. I started putting together the ideas that would lead to my calculus in my world-changing
book, The Principia. Show off. After doing some light reading, Newton went on an intellectual rampage,
in the midst of discovering laws of gravity and inventing telescopes. One day, he must have gotten
pretty bored because he started tackling some casual puzzles. Hmm, I wonder if there's anything
special about a curve that has an equation that can express its area. Oh, huh. The height of the
curve is the same as the rate at which the curve's area increases. I also wonder if there's anything
special about the equation that gives the area of the curve. Oh, the area given between two bounds
is just the equation at the first bound, minus the equation at the second bound. Now, what did Newton
do with most of his findings? He kept them hidden, but they got leaked, and one day he started getting
annoying fan mail from a certain Gottfried Leibniz was a German lawyer who worked as a secretary to a
politician, so he traveled a lot on diplomatic missions. He got to meet a lot of people, and one
person he happened to meet was Christian Huygens, who was the mathematician of the time. Hey, you seem
like a cool guy. Wanna learn advanced math from me so you can publish the first articles on calculus
in 15 years and one day have a way longer Wikipedia page than your own boss? That sounds awesome.
Thanks, man. Hey, you should go talk to this Newton guy. I heard he's really smart. Yo, Isaac. I heard
about your contributions to math. Would you be so kind as to show me some of your work? That would
make my day. No. Come on, man. What if I show you this cool infinite series I found? That's amazing.
You're a mathematical mastermind. It's incredible to see that someone else is able to do something
I already know how to do in three different ways. The son of a... Leibniz discovered the fundamental
theorem of calculus after Newton, but he was the first to publish his calculus, and he packaged
it all in neat, understandable notation that we still use today. There are a few key differences
between Newton and Leibniz's calculus, though. For Leibniz, this d part in his dy dx notation represents
a differential, or an infinitesimal. You might be familiar with this explanation of imagining the d
as a tiny, tiny nudge in the direction of a variable, and dy dx as a ratio of tiny nudges.
That's what it is. Newton also played with infinitesimals, but he presented them in a
different way. He used what he called fluxions, which treat each variable as changing with time,
rather than directly comparing how one variable changes with another. This is kind of like treating
every variable like a parametric equation. So basically, Leibniz looked at infinitesimal nudges
in variables, while Newton looked at infinitesimal nudges in time. Notice, though, how both of them
explicitly used infinitesimals in their calculus. This was definitely frowned upon, and they both
knew it. Newton trivialized his work, since it was only good for finding solutions, not proving them.
Leibniz said that, philosophically speaking, I no more believe in infinitely small quantities than
in infinitely great ones. That is, in infinitesimals rather than infinituples. I consider them both
as fictions of the mind for succinct ways of speaking, appropriate to the calculus. To Newton
and Leibniz, their early calculus was just a thinking tool. It wasn't rigorous, it was kind
of hand-wavy. The question of what to do with infinitesimals and infinity won't be answered until
a bit later. But before we get into that, Leibniz did another thing to calculus that Newton could
never ever have done. Remember how he was a secretary who went on diplomatic missions? That made him
really good at dealing with people, and he used those people skills to popularize calculus. One
genius can only do so much, but a lot of geniuses working in the same field can do a lot more. And
this was honestly one of Leibniz's biggest contributions to the early study of calculus,
that he got the ideas to the people who are ready to work with them. You might be able to notice that
a key theme in how I presented the development of calculus is that it involved many pairs, both in
people and also in ideas, with the pairs of algebra and geometry, integral and differential calculus,
and math and physics. This pairness of calculus, to me, is telling of how powerful relationships
between ideas and people can be, even when they might not seem to work well together at first.
Galileo conducted experiments by creating an ideal environment, while Kepler only had data to try to use
to discover his laws. Descartes was pretty famous in his lifetime, Fermat was a lot more reserved,
Newton kept to himself, Leibniz was a people person. But despite the differences, they all contributed
to the development of pivotal ideas, and altogether, they were able to make calculus what it is.
Not only does this show how big ideas developed from the contributions of many people rather than
from a single prodigy, it also shows how much more far-reaching things can become when different
ways of thinking find a common ground, and can have free reign to influence each other.
So far I've mostly been talking about infinity as if it was a value,
but there's also another type of infinity that I'll quickly mention where you treat it like a
process that never ends. Enter the second half of calculus 2. Remember Zeno's paradox about movement?
An infinite series is when you add up a sequence of numbers, in this case 1 half plus 1 fourth plus
1 eighth and so on, to get a finite value. They are too much to get into now, and I know I'm skipping
a really big part of math, but for the purposes of this narrative of calculus, these are like tools
that kind of developed alongside the fundamental theorem, and helped Newton, Leibniz, and the
people after them make a lot of discoveries. Notably, Leonard Euler's use of infinite series
moved calculus away from the language of geometry, and more toward the language of functions,
and he even derived his famous number from them. This time period, just after Newton and Leibniz,
is also to blame for the convergence test that you may have had to memorize, and all the funny names
that you may be familiar with. Attention everybody, there is not a single infinite series whose sum has
been rigorously determined. In other words, the most important parts of mathematics stand without
foundation. Yeah, but what about the Zeno series? I'm pretty sure that it converges to 1. Yeah,
it converges to 1. But what does that even mean to add an infinite amount of terms? In the 1800s,
there was another revolution in calculus, and all of mathematics as a whole, after mathematicians
decided to revisit the foundations of math. Now is the time that we finally get to talking
about limits. I've been implicitly referencing them throughout this video, and mathematicians
since before Newton and Leibniz had been kind of using them this whole time. What Leibniz said about
thinking of infinitesimals as fictions of the mind is kind of an example of what a limit tries to do.
It's like a tool of thinking that gets you to express infinite numbers in a way that doesn't
actually summon infinite numbers. A limit is usually described as a value that some function
gets close to. So if we scroll to the right of this function forever, it approaches zero.
So how is this limit idea better than just saying, imagine that infinitesimals are real,
like Leibniz did? First off, let's look at what a function is. A function is defined by pairs of x and
f of x. Let's say we have a function, and we think that the value it approaches at a is l. If you aren't
convinced that a pairs with l, then I dare you to give me any range around l that you think only contains
values that don't pair with a. I guarantee that no matter what range you pick, I can give you a range
around a that only has values that pair with numbers inside your range, meaning that my a has
to pair with one number in your range. You can keep picking the smallest number you can think of, but I
guarantee that any range you come up with will always be matched by a range around a that proves
that your number isn't small enough for my value. It's clear that since you can't give me a range that
excludes my a, the limit of f at a has to be l. It doesn't really matter if you understood that
or not. The key to all of this is that now, when we do calculus, we aren't all agreeing to pretend
like we're playing with infinity. Writing lim means that we agree that if we wanted to, we could keep
on going, forever and ever, picking smaller and smaller or bigger and bigger numbers that slowly
push the function closer to the limit. We now have a language where the only thing you have to know
logically to prove anything is if one number is bigger or smaller than another number. It's very
similar in spirit to the initial problem I gave at the beginning of the video where Archimedes proved
the area of a circle, and it's beautiful. The attitude of the epsilon delta proof perfectly
complements the attitude of the mathematician who asks, how can I know this for certain? By answering,
if you're really up for it, I guarantee that you can ask that question over and over again until
you are satisfied. The epsilon delta limit is almost like an atom for calculus, and in that way, it
captures the whole idea of calculus in itself. By defining calculus with the language of limits,
we get to tame infinity. Archimedes would be proud. But of course, there's more to the story than that.
There's still the question of infinity itself. The limit was able to redefine calculus without
explicitly using infinity, but it didn't get rid of the idea of infinity. There are still debates today
about infinity's place in math in the real world, ranging from views that argue that calculus can
work fine with infinitesimals to views that infinity doesn't exist at all and that our current
foundation for math, set theory, is flawed because it directly plays with infinity. But that is an
incredible, surprisingly heated debate to save for another time.
I have long embraced the belief that every course should be built around a story,
a quest to answer certain burning questions. In writing this book, I sought to unearth the
questions that drove the historical development of calculus, says Dr. David Brissou about his book
Calculus Reordered, which is one of the books I used to research for this video. For me, the question
that took me down this quest had to do with infinity. Is infinity really this simple of an idea that we
could just say the derivative is like finding an infinitely small ratio between two changing variables?
It's easy to get desensitized to the idea of infinity when first learning calculus. Each time
a new subject is introduced, there's almost like a mental countdown to when the teacher says something
like, now imagine that we take this process on to infinity. What happened to that whole other side
of infinity that leads to paradoxes and is a massive headache to think about? And come to think of it,
why do we even have these limits again? I was surprised and felt almost validated when I learned
that the use of infinity in math was a problem even to the mathematicians who developed the ideas that
led to calculus. And after the initial shock from hearing that integration came before differentiation,
everything started to make more sense. Infinity only became this simple after we developed the
limit to trap it. But the idea of cutting something into infinitely many pieces was a thing before that,
and for most of the history of calculus, mathematicians struggled with trying to limit infinity to just
that one interpretation. The whole story of calculus is a story of how mathematicians over millennia
developed different ways of jousting with infinity to get simple results, leading up to the twist that
integration and differentiation are connected, and coming to a resolution with the development of
formal limits and the following shift from calculus to analysis. As Dr. Brassell says, the progression we
now use for teaching is appropriate for the student who wants to verify that calculus is logically sound.
However, that describes very few students in first-year calculus. By emphasizing the historical
progression of calculus, students have a context for understanding how these big ideas developed.
Now, this begs the question, what if we always teach introductory calculus like this?
I've talked with many, many engineers, and no engineer that I have ever met says that in their job,
they actually need to find derivatives or evaluate integrals. I taught at Penn State for many years,
and interviewed students. And most of them, when I asked them, what do you want to get from this
course? They said, I want to understand calculus. They've been told that this is important mathematics,
that this laid the groundwork for much of our modern era. They want to know why it's important
and try to understand what's important about it. A lot of students, they're pressed for time at college
or university. And so, and again, there have been some great studies on this that show that
when students get to a point where they're having difficulty maintaining this mastery of the ideas
behind calculus, they fall back on mastering the technique so that they can pass the next test.
And unfortunately, that's what happens to most students during their calculus course. And I think
part of the problem there is that so much of the way that we teach the calculus facilitates this. We
have a great many ideas, but we don't really spend the time to allow students to absorb these ideas.
We spend a lot of our time just explaining, well, here's how you apply this particular technique. And I
interviewed Penn State students, this was toward the end of the semester, and I asked them to describe
what goes on in a typical class. At this point, one of the most common responses I got, well, at the
beginning of the class, the professor tries to explain what's going on in the mathematics he's going to
present, and I know I can ignore that. And then he starts doing examples, and that's when I know I have
to start paying attention. So what I think the history is very important because it can help illuminate
the ideas that are behind the concepts of calculus. And it motivates the subject. I find students are
much more interested in the calculus if they know where the ideas are coming from, and also know how
people had to struggle to come to these ideas. I know that you mentioned the infinitesimals and these
tiny pieces. Yeah, that's something that philosophers wrestled with for hundreds of years. I think the
history of calculus is a great way of informing instructors. You know, here's a place where the
field as a whole stumbled and had difficulty for a long time. Expect your students to stumble and have
difficulty for a long time.
Calculus is notorious for being a difficult subject riddled with hazy ideas and limitless rules to
hastily memorize, but it doesn't have to be this way. Some mathematical pedagogists have considered
mixing calculus with its history to contextualize the concepts and leave a lasting impression on the
main ideas of calculus. There have been efforts to reform the way calculus is taught for decades.
Notably, there was a reform movement in the early 1990s, but support for it dwindled in the years
following. I bring this up because I want to emphasize that the current way that calculus is
taught may give many people a flawed impression on what calculus and math in general is. I know that
personally, I walked out of calculus only having a vague idea of how this supposedly revolutionary math
I just learned was important. I hope that if you were like me, this video was able to go beyond what
you might learn in the classroom and show the humanity of math, and I hope now you have a
different perspective on what calculus is.
Thanks for watching!
