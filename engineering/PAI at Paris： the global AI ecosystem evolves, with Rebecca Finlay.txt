Welcome back to the London Futurist Podcast. In this episode, our guest is Rebecca Findlay,
the CEO at Partnership on AI. Rebecca previously joined us in Episode 62 back in October 2023
in what was the run-up to the Global AI Safety Summit in Bletchley Park in the UK.
Well, times have moved on and earlier this month, Rebecca and the Partnership on AI participated
in the latest Global Summit in that same series held this time in Paris. This summit, breaking
with the previous naming, was called the Global AI Action Summit. We'll be hearing from Rebecca
how things have evolved since we last spoke and what the future may hold. Prior to joining
Partnership on AI, Rebecca founded the AI and Society program at global research organization
CIFAR, one of the first international multi-stakeholder initiatives on the impact of AI in society.
Rebecca's insights have been featured in books and media including the Financial Times,
The Guardian, Politico and Nature Machine Intelligence. She is a fellow of the American
Association for the Advancement of Sciences and sits on advisory bodies in Canada, France
and the US. Rebecca, welcome to the London Futurist Podcast.
Great to be with you. Thank you so much.
Thanks for joining us again, Rebecca.
Rebecca, I understand that you were part of the steering committee for the Paris AI Action
Summit, working with summit envoy Anne Boevereau. What did that involve and what was it like?
Rebecca. Well, it was a very interesting process because it was very consultative right from the
beginning. Regular meetings of this quite diverse group of entities. So you had individuals representing
the countries who had indicated they were going to be engaged in the summit. You had individuals
representing the international organizations like the UN, like the OECD, like UNESCO. And then you had a few
other individuals like myself who represented the broader civil society and business community. Right from the
very beginning, France had indicated that they really wanted to widen the aperture beyond safety. We can talk about the
role and function of a safety summit versus, in this case, what was called an action summit. But they intentionally wanted to bring in a number of other topics.
So there were these five thematic areas identified, each with its own envoy, leading a set of both consultations with the
steering committee and wider consultations with working groups and one-off meetings and different calls for ideas and
other ways. So those were public interest AI, open innovation, governance, trust and safety, and also future of work. So really covering, in some sense, many of the topics that we think of in the broader sense of what does it mean to advance responsible and safe artificial intelligence.
And so in a truly interactive way, the steering committee was presented with ideas and concepts and actions and approaches and provided an opportunity to give feedback all the way up to the declaration and the final days before the summit. Fascinating experience.
It must have been really interesting, Rebecca. And you say that the focus is different from Berkeley Park. And obviously the big difference, as you say, is the move away from safety and more towards other aspects of the AI debate.
Anne Bouvere, perhaps her most famous comment has been that AI safety is addressing science fiction, i.e. the possibility of very advanced AI taking over control from humans and doing away with us all.
She thinks that's science fiction. And there's this very interesting divide which has opened up in the AI community. I mean, it's been there forever, but it's becoming more and more obvious and more fierce.
And on the one hand, you've got Andrew Ng, you've got Yann LeCun who say that superintelligence is either never going to happen or it's many, many, many years off.
And you've got Geoff Hinton, Joshua Bengio, Stuart Russell, much Tegmark and others saying, no, it could happen much sooner.
And indeed, Sam Altman and Demesis Harbis, who run the two biggest AI labs, both say it could happen this decade.
And so those people say we should take the possibility of these extinction risks very seriously.
Was it very obvious that this was going to be a big debate through the process?
And how did you deal with that? How did you work with both sides of that debate?
I am fundamentally of the belief that we need to be looking for ways to bring these conversations together rather than to see them as in counterforce to each other.
And I do think it's the nature of research and scientific discovery that you're going to have very different perspectives on where the technology is going and how long it's going to take us to get there.
I actually think that's sort of a creative and natural tension in the development of AI as both a field of science and as a real world technological application that is interacting within markets and with economic structures and social structures.
So I think it's really fundamentally important that rather than looking at those as being in counter to each other, that we think about how that tension can advance the field.
There is no doubt that the AI action summit was intended to refocus the conversation beyond just safety.
And I think it is fundamentally important that there were a number of side events, including meetings of the safety institutes and otherwise follow up on the safety framework commitment that came out of Seoul, for example.
I think it was really important that I think it was really important that that moved forward and I participated in a number of those and believe strongly in that work.
And it was clear that the summit was going to be about a wider set of issues, right?
How do we now start to think about what does it mean to apply AI systems in the real world, for example, in the public interest as well?
It was interesting to be in Paris in that moment and to see that play out in real time.
Yeah, I can absolutely see why you have your job, Rebecca.
And I mean this in the most positive way possible.
You are a true politician.
But let me just try and see if I can push you off the fence.
Do you think super intelligence is coming within, say, 10 or 20 years and that it's an existential risk?
Or do you not think that?
I fundamentally do not know.
I am not trying to sit on the fence here.
I'm actually saying to you, I fundamentally do not know.
And in the work that I do, I find it really challenging because of all of the different definitions for what super intelligence or artificial general intelligence actually mean.
It's why I've been a real supporter of the state of AI safety science report led by Yoshua Bengio and the really phenomenal group of scientists that he brought together to do that work.
Because I feel as if we have this fundamental gap in AI science that other fields of science have over time been very intentional about developing.
And that is this way finding around scientific consensus, body of evidence.
What do we know?
What do we not know?
So one of the pieces of work that I think we have to attend to, and you can see it's going to help me to better understand where the science is going to exactly answer the question you've asked.
We need to have that on an ongoing basis.
So state of science report really fundamentally important.
But the question I think now is how do we build institutions around that to make sure that it is sustained.
One of the things that I heard in Paris was the commitment by the UK government to continue to support that report over the next year, which I think is crucial.
They've made a major contribution in putting that report into place and to supporting that process.
And now the question is, what is the role?
And I really don't know the answer to this question of the UN and the International Panel on AI and the commitment that came out of the Global Digital Compact back in September.
So let's talk more about the possible institutions that may be involved.
Can I share with you admiration for the clarity that Yoshua Bengi's report has brought, but it poses questions.
Who is going to follow this up further?
So will PAI be playing a role in that?
Is that part of what you consider your own mission to be?
A hundred percent.
So I was really delighted to announce the launch of a new independent advisory expert group this year.
We're calling it SAGE because we think it will be a wise expert group of individuals that can help to give guidance both to all of us at the Partnership on AI to the broader community as we think through, where are those gaps in knowledge?
Where are those true questions we need to be attending to?
And how do we build consensus and collective action on them?
At PAI, we're really committed to translating what we know into practice to ensure that we're both driving better practice, better policy and advancing shared understanding.
And the summits have a role to play in that, which we can talk a little bit about, but at the same time, really fundamentally supporting a global community that has a really solid knowledge base for determining where we go from here is crucially important.
And I think it's important for PAI to be part of that conversation.
And then also we have a role in advocating for this to happen and supporting efforts like the one that emerged out of the Bletchley Park process and now is going to move into the UN process in some way.
I think we're all united in admiration of Joshua Bengio and it's been very interesting to watch him over the last year, particularly become less the quiet member of the three godfathers of deep learning, along with Hinton and Lacoon and become kind of a co-equal voice with them.
I was at the IASEAI, I think that's right, the International Association for Safe and Ethical AI, the inaugural meeting of that, the OECD in Paris, just before the Paris summit.
And he was very clear and he's been very clear in all the various meetings.
I mean, he had hundreds of meetings in Paris the last couple of weeks that the breakpoint in making sure that AI doesn't become unsafe is not to have a genetic AI.
He wants scientific AI, which is like an Oracle.
You can have it as advanced as you like, but it sits in a box and it cannot affect the outside world.
He calls this scientific AI.
And he does not want us to be developing AIs which can affect the real world by taking over our laptops and going ahead and booking plane tickets and on.
And it just seems to me that this is fundamentally a problem because we are releasing those agents right now.
Companies are making agentic AI.
They are releasing them into the wild.
The reasoning AIs like ChatGPT 401 and 403 and so on, these are agents.
And so the horse, if not bolted, it's at least halfway out of the stable.
So how can we achieve what he wants, which is to make AI safe by not having AI agents, if we are going ahead and releasing those agents?
Well, agents as a technology has been around for a long time, interacting in different ways with machine learning systems.
So I think, again, for me, it's often the question about what are we talking about in what context and in what environment?
However, I do agree with the point that agents add a level of safety and security that we need to better understand when they are applied specifically to these large language model systems.
So in that regard, we've been really delighted to pull together a group focused specifically on thinking through agent safety and what that means in terms of fundamental guardrails that need to be in place to manage that technology.
This is work that builds on the work that we released.
And I think I spoke with you about the last time that I had the opportunity to be with you, which was our framework for safe foundation model deployment.
And that as a set of specific guardrails across the lifecycle of development and deployment of these large language models has become a foundation on which we can now start to ask some of those questions, which is what are the new risks that are presented by applying agents to these models?
And what does that mean in terms of safe development and deployment of those systems?
You might remember that that was some of the very earliest work that attended to both closed model releases through APIs, but also through open model releases as well.
And of course, that was one of the core takeaways from the AI Action Summit this last week.
The real shift that we are seeing in terms of a focus on what does it mean to develop these systems openly and what does a broader open innovation ecosystem mean for the long term development of AI technologies as well.
So I think that is another area that you can see a difference between the conversation that happened in the UK and took place in Paris.
We'll be right back after a quick break.
If you spend way too much time online, first of all, same.
And second, you need Promo Guy talk pills.
I'm Rawal and with Alex, we break down everything happening in tech, digital marketing and the Internet's latest chaos.
No boring corporate talk, just real discussions with real opinions.
So go on, press play, you know you want to.
So there's possibility of having guardrails for agentic AI safety.
I've got two questions about that.
I wonder if you can say a little bit more about what some of these guardrails are.
And then what's going to ensure that companies follow these principles rather than saying, hey, we know better.
We don't trust this walk health and safety nonsense.
We're going to do our own way.
Thank you very much.
Well, I can't say what the guardrails are because we are in the midst right now of initiating that work and developing that work.
That's part of this question about how we build a responsible field of practice.
Who needs to be together as we start to think through some of these questions?
And so that's some of the work that we're kicking off this year.
I think one of the interesting pieces of work that also needs to happen is really understanding how humans are interacting with these systems.
So we spend a lot of time focusing on the systems and the guardrails and the responsible practices therein.
And we need to better understand how, in fact, are humans interacting with these systems?
Where are we prone to overtrust these systems and how are they deployed therein as well?
The role of PAI is to bring organizations, companies, civil society, academics to the table early on with the goal of creating guidelines that have relevance for real world practice.
And that's how we believe we can contribute to a broader ecosystem that stretches all the way from regulation through to fundamental science to enact a better practice and responsibility in the space.
So you mentioned regulation related to the question of whether concerns about AI safety are science fiction or important things we should attend to now is the question of whether AI should be regulated.
And this applies both to near-term issues, which are sometimes called ethics, sometimes called responsible AI, which mean privacy, bias, transparency, and so on.
And also the longer-term issues, which are sometimes called safety, which are about alignment or control.
And the Americans made it very clear.
J.D. Vance made it abundantly clear that they are having no truck with regulation.
Regulation is the work of the devil.
And he was quite rude about Europe's tendency to regulate things.
The UK joined the US in being one of the only two companies that didn't sign the declaration that came out of the summit.
The UK's position is quite interesting.
It doesn't seem to want to be clear about why it didn't sign the declaration.
America made it very clear.
They're not signing because they think it's too much regulation.
A UK official said rather disingenuously, I thought, that they didn't know what the American position was and therefore they couldn't be slavishly following it.
But a Labour MP who was at the event was quoted by The Guardian as saying, well, the UK really has no choice other than to be downstream of the US.
Which is a nice way of saying we've got to be their poodle.
And, you know, we want to carry favour with the US by not signing up to a declaration which we think imposes too much regulation.
Do you know why the UK didn't sign the declaration?
Was it just slavishly following the American line or was it something else?
I do not know.
And I really think it's challenging to speculate in this moment.
And I have to say on the question of regulation versus broader assurance.
For us, there is a really fundamental piece that is missing in the assurance ecosystem that is holding companies back from being able to move forward in innovating and developing systems.
And this would be companies across all sectors.
And that is to better understand what are all the levers that they have available to them to put guardrails into place.
Those common standards and norms that can drive forward economic growth alongside regulations.
So one of the messages that I brought to the AI Action Summit was to call on the governments who were there to work very hard collectively to identify what does it mean to have a vibrant and effective assurance ecosystem in place.
Whether it's auditing or standards or certifications or disclosures, what should companies be understanding or sort of the rules of the road when they're developing this technology?
It's a really important piece when you think about all the companies who are experimenting and trying to figure out how do they make use of their data effectively?
How do they ensure that they don't break trust with their customers and their clients as they're developing AI within their systems?
To me, it's a crucial question that we need to be working together to do.
So if summits create opportunities to bring governments together so that we can advance some of that, what does it look like for international assurance norms that are interoperable to some extent?
I think that's a great opportunity.
We have the opportunity to have an event around that to catalyze some of that work.
I think it's an important piece of activity now for all of us to be focusing on as we come out of this summit and think about what's going to happen at the next summit in India.
Yeah.
Yeah.
Stuart Russell makes a very good point, I think, about regulation that the banking industry and the airline industry and the drug industry and the car industry are all quite heavily regulated.
There are lots of things you're not allowed to do if you're a banker or an airline company.
And we're all safer because of it.
And he thinks that it's really quite strange.
I don't know for sure whether he's right about this, but he knows more about it than I do.
He thinks it's very strange that a sandwich shop in San Francisco is more heavily regulated than somebody creating an advanced AI.
It's hard to believe that a sandwich can be as dangerous as an advanced AI possibly could.
So there's quite a strong argument that you do need regulation of advanced AI.
Of course, the trick is how to get that right and also how to make it harmonious globally, if not necessarily completely level.
The EU has got some regulations in place and California seems to be going some way towards mirroring a lot of EU regulation.
Federally, the US doesn't want to do it, but lots of states will.
So how do you see that evolving?
Do you think state by state America will adopt EU style regulations or do you think it will go in a completely different direction?
I think it's too early to really understand where regulation is going to go within countries and within sub jurisdictions of countries.
It's an important question that policymakers need to be asking themselves.
How do you unlock innovation through better protection of citizens or better guardrails being in place?
And each country and each jurisdiction is going to have, of course, a different answer to that question.
International harmonized technical standards can be really useful in companies that are thinking about how do you work across borders and across jurisdictions as well.
You know, this question of the role of auditing, you may have seen the launch of current AI, which I think is probably one of the most sustaining investments to come out of the AI Action Summit.
So this is a funding and de-risking mechanism that was announced at the summit, $400 million in an initial fund that has been set up in order to both support, I mentioned earlier, this focus on openness, but open data sets, open modeling, open tooling and safety, and also accountability and auditing.
So understanding better, what does it look like to have a really effective auditing mechanism in place?
What does that mean for the private sector, but what does that mean for civil society as well?
So it feels to me like there's a lot of these areas that are really tractable that we can move forward on and it's going to be a key focus for us in the coming year.
Is this connected with the theme of accountability?
I noticed you were part of an event with a whole bunch of other companies, Mozilla, IBM, Ada Lovelace Institute, A&O Sherman, Hugging Face, Singapore's IMDA and so on, to try and reach some harmonious views on accountability.
And it seems to be controversial because many software providers say, we can't be held accountable if somebody takes our software and builds a bad app on it.
And as a result of their incompetence, some third party down the line suffers harm.
And so they sometimes seem to want to deny that portion of responsibility.
Other people say, no, if somebody can use your platform and do bad things with it, you ought to have a share of accountability too.
So how did that meeting evolve? Did it reach some kind of consensus?
It's interesting that the space when we think about accountability, auditing and insurance is right now very fragmented.
Exactly as you say, there are different actors in the value chain, even understanding what the risks of those different actors are and what responsibilities they have to each other and what levels of disclosure need to be in place.
That's a whole area of work that we started last year when we released our value chain for open model development and deployment.
And we found it's been really useful across our partner community and better understanding where the potential risks and interventions lie.
So there's that piece of work that needs to continue to be done.
There's also a piece of work that says, how do different accountability functions operate together in order to support an ecosystem?
And that's what we heard when we brought together the community that you mentioned on the Thursday before the summit in Paris, that there are all of these initiatives that are standing up.
Good, high quality, effective in their own right, but they're not talking to each other and thinking about how they each can contribute to and hand off to each other across a variety.
And that's both nationally and internationally is really important.
So, for example, you mentioned Singapore, a huge commitment to thinking through what an assurance ecosystem all the way from their Safety Institute through to the Verify Foundation initiative that they have put into place to really better understand what it means to support that work.
An organization like the Ada Lovelace Institute based in the UK, which has done some of the really early work on understanding what is a true auditing ecosystem look like and need to contribute to the broader system as well.
Of course, the OECD, they have all of the work they've been doing around responsible business conduct and codes of practices.
So an opportunity through PAI's convening process to bring those conversations together to start to stand up and help to way find for countries, for governments, jurisdictions to better understand what they need to put into place to really advance that innovation in a resilient and sustainable way.
One of the other big discussions in the AI community in recent years, and it really came to the fore in Paris was whether and how Europe should step up and get involved in the industry.
At the moment, AI is really a duopoly in which America is a long way ahead.
China's quite a long way behind.
But then in third place is either possibly the UK or Canada.
And those third place nations are a very, very long way behind the first two.
At Paris, President Macron announced, I think about a hundred odd million euro investment in French AI.
And then the EU said that they were going to fund a sort of a CERN, CERN for AI, European CERN for AI.
It's not clear yet how that money would be spent.
It does look like Europe is starting to wake up and decide to play a prominent role.
Do you see that happening?
Is that an important part of your work?
It's interesting that you put it that way because it feels as if Europe's been attending to some of these questions around AI and AI regulation for quite some time.
Absolutely.
Europe has been playing a role in regulation, but it hasn't really been playing a role in developing the technology.
Mr. Isle to one side, and of course we've got DeepMind in London, but it's a Google owned company.
There are many, many British and European scientists involved in AI, but there's no big tech companies in Europe apart from Spotify.
Well, now you're touching on what was, of course, one of the major topics of conversation in Paris, which was DeepSeq.
The release of DeepSeq just the week before, and while not really knowing what actually was developed and how much it cost and what it took to release those models.
The notion that an open model could be released that was trained with much less compute with certain levels of capabilities really opened up the conversation about, well, what other jurisdictions need to be advancing and supporting the development of models like DeepSeq or other business models as well.
It does make one think about all of the amazing scientists in the EU and in the UK and the strong support for industrial applications in both of those regions.
I'm wondering, how do we start to incent business models that work well for those different places?
You've given a fairly positive account of what happened in Paris.
I wonder if we can tempt you to express any disappointments or frustrations or, let's say, things you'd like to see come more to the fore in the next summit in India, if that's where it's going to be, although I haven't seen a date set for it yet.
Yes. At the very last event in Paris, Prime Minister Modi announced that the next summit would take place in India, but did not provide a date or a definition or framing for what that summit is about.
You know, and this is a question that has been on my mind even coming out of the UK Safety Summit, which was, yes, much more focused in nature.
What is the role of these summits amidst a broader set of international activities that are taking place all the way from the Global Digital Compact and the announcements that were made by the UN in September through to things like the G7 Hiroshima process.
We had the OECD release the framework for that as a side event around the Paris summit through to the new network of safety and security institutes and the work that they're doing together globally as well.
And so when I think about organizations like PAI that are really thinking about the long term impact of these technologies and how we ensure that they are developed in the interests of people and society.
I think there is a question about how the summits as providing a moment in time and a specific opportunity to bring together a particular set of actors, all the way from governments through to industry, through to civil society and academia, what role they play in advancing this work.
And I want to make sure that as an organization, we're thinking about that and we're continuing to stay focused on that question as we explore opportunities to engage with the different summits moving forward.
I think that's a broader question for the community as well.
Certainly one of the conversations that I'm going to be having with our partners, with governments, is to think about how do we set this up for success?
I know that the announcement around current AI and public interest AI was a commitment that really came out of a reflection on where we had come to with the summits previously in the UK and in Korea and the focus on safety and this desire to say, is this the moment to stand up for funding mechanisms to really advance the benefits of AI more broadly?
So I think those are all open questions in my mind right now.
I'm sure you're thinking about them.
I'm sure they'll be part of some of the conversations you're going to be having with your guests in the coming months.
And I think it's something that is really going to be crucial in this moment as we advance this work together.
My last question will be about one of the themes that we haven't touched much on yet, which is the future of work.
Both Callum and I have the view that politicians aren't seriously considering what's going to happen when more and more people find themselves unexpectedly unable to get jobs
because AI has gained the ability to do what they thought was uniquely human.
How did that theme of the conversation go?
And maybe that's a topic that should be elevated in India or in other global forum.
A hundred percent.
I mean, there are two things we haven't really chatted a little bit about that I think were key takeaways from the summit and they're related.
One was the more global nature.
So about 80-ish countries, as I understand it, participated in different ways.
60 plus signed on to the final declaration.
So the question of how do we bring a broader set of voices into this conversation?
And clearly that is related to the future of work on all sorts of different levels.
We've done a lot of work looking at the way in which workers often in precarious roles who are situated outside of the global north are very much driving forward some of the data work
and the development of these technologies often not seen and not supported.
So how do you make sure that there are adequate processes in place as we're thinking about the development of this work moving forward?
And then I think there is a broader question about what the implications, particularly for generative AI, are going to be on even more categories of workers in all countries.
And how do we take that forward?
So, yes, there was a declaration, a statement on the future of work, which was released alongside the final declaration in Paris.
And so I'm happy that that was elevated to be part of the conversation.
And I'm looking forward to making sure that that continues to be part of the global dialogue moving forward as we figure out what those implications actually look like.
And, of course, we're already seeing the impact of AI in the workforce on all sorts of different functions, both in terms of, as we've seen from some of the research, driving increased productivity, but also risking and threatening jobs and roles as well.
I completely agree with David that policymakers are not taking the future of jobs seriously enough.
I coined the phrase, the economic singularity, because I think the break is going to be very different and much more serious than most people think.
And we are going to probably need a very different type of economy in the not too distant future.
And we're not thinking about that yet.
But my last question will be different, because since we last met, I've become a co-founder of and David an advisor to a company called Consium, which is addressing artificial consciousness.
That is the possibility that in the next few years or decades, we may create, we as species, may create machines which are conscious.
And this would be an extremely important event, and we would be well advised to be prepared for it.
I'm guessing this doesn't really come across the PIA radar very much.
Is it something that people are talking about or not?
It's an interesting question, again, leads me back, no surprise to wondering, what do we mean by consciousness?
And of course, not the question to raise at the end of a podcast.
Yeah, a bit unfair.
We should save this for the next one, perhaps.
But yeah, a whole set of really interesting questions there in around how we define consciousness.
Yeah.
And what does it mean?
Just as a sort of a door opener.
It's easy to get too worried about definitions on this.
When I say consciousness, I mean what we all understand consciousness to be.
We can't define it.
We can't explain it.
We can't describe exactly how it happens.
Although, actually, we're making some progress on that front.
But I just mean the experience of experience.
What it's like to be Rebecca.
The fact that you have sense inputs.
The fact that you have feelings.
The fact that you have an awareness of yourself.
It's that bundle of things, experience, which we have.
I have.
And I assume that you two have.
And we are pretty confident that machines don't have.
But they may in the future.
And if and when they do, it'll be very important.
Because they will become moral patients.
And we will have a duty of care to them on those sorts of issues.
I actually think this is the year that conscious AI or artificial consciousness
is going to enter mainstream debate.
But it's still around the edges at the moment.
Well, now I know why this is the London Futurist podcast.
I've totally got it.
That's great.
Fascinating questions.
You know, I had the opportunity to work at the Canadian Institute for Advanced Research.
And they had a fabulous program called Brain, Mind, and Consciousness.
So I refer you to those folks and the work that they are doing on exactly this question.
Much more highly qualified than I am to support you in that effort.
That's a lovely way to end that we've demonstrated that we are indeed the London Futurist podcast.
Rebecca, thank you very much indeed for joining us again.
My pleasure.
Thank you so much.
It's been a fascinating conversation.
And thanks for helping us to see progress and positivity, along with some challenges remaining.
I look forward to bringing you back on a future date when we can hopefully see things even more clearly.
Thank you.
