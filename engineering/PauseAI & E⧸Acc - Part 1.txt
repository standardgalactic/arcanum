there was a computer that took over the whole world could you have empathy for it and like
think of it as like an ancestor or something like that people have different feelings about that
i think the human extinction is a bad thing i would maybe i should sure that's my plan i was
gonna i was gonna ask about that but yeah thanks for clarifying yeah yeah whether you see ai as an
amazing new technology or a dangerous alien mind the impact it will have on society is hard to deny
should ai be carefully regulated outright banned or should we leave the free market to run its
natural course these are important questions to get right but with societal relevance comes political
argument and the debates around ai increasingly provide more heat than light i met chad stearns
my guest today on the pause ai discord server which contains a dedicated channel for people who wish to
express disagreement with the organization chad stood out as being especially willing to engage
in civil reasoned debate in this conversation this video being the first in the series i hope to build
a bridge of understanding between views on ai that at least on the surface seem as different as they
could be my name's chad i'm a programmer been programming for about 10 years now went to school
for economics so it's kind of like a different kind of thing from what i end up doing for work i
actually work at an ai company now not doing research or anything we mostly use like chat dpt
and things like that like a lot of other ai startups i kind of jumped into it recently with
all the excitement going on with ai development what got you interested in ai to begin with
i found my current company mostly through connections and interest it does seem like there's a lot of
really great ai stuff going on right now a lot of big things to develop i'm just following
ai developments on twitter like like so many other people what is the day-to-day work like
for someone who's building ai there's all these new like llm products out there it's mostly finding
use cases for them and then making processes that integrate these we've got a product we're trying
to find customers for you talk to it like any other chat ai but it goes through several steps of using
llms to try to analyze how it's going to respond i've seen other started to have the same kind of
structure i guess but i think that that's sort of a normal flow so you take something like gpt4
chat gpt or something like that someone can just use that as a chatbot but if you want to actually
use it in a product you need other stuff to to go around it and then you build those things like to
make it more useful but you're using someone else's ai as like a base yeah basically i think that
captures it well it's kind of like these big companies just made these things and they just
sort of unleashed and you could buy you know access like untapped usage for chat gpt like like me
and you know a lot of other people do but it's it it sort of feels like people are just figuring
out how to get productive uses out of them startups like mine are actually putting together
like building blocks to try to get something out of it it's interesting seeing the business model
that the ai companies are taking like open ai and probably anthropic as well seem to be the approach
of we're going to build this really general purpose ai and then other people will make useful products
with it and then we'll license them and so like the businesses are paying us money so that they can use
our thing other companies like meta might be more we really want ai to power our search algorithms
and we're going to build that so we can use it for this thing so it actually kind of makes sense
for them to be open source and others not to be yeah interesting yeah that makes sense so yeah the
reason i uh contacted you for this is you're in a kind of interesting position of being on both the
pause ai and the effective accelerationists uh servers for those in watching this who doesn't
know what those are these are like someone being a republican and a democrat at the same time uh yeah
where uh pause ai which is what i'm involved in is all about trying to stop frontier development slow
things down so that society regulation all these kind of things should catch up whereas effective
accelerationist is very much against that although i don't think i should be the one characterizing what
eac is so why don't you go ahead and saying what eac or effective accelerationist is all about
right yeah yeah it's an interesting question i don't know if i could be like the spokesperson from
it because i i sort of jumped into it like curious about it feeling like this could be my thing but
since i've jumped in i've met a lot of people with them you know honestly i think that's more
interesting because like if i was talking to a spokesman i would be getting a very pr sanitized kind of
thing and i think it's be more real and more interesting to see like what is it from someone
who's on the ground and like actually participating sure yeah i first saw the term on twitter and
just for information like i'm pretty pretty libertarian in how i feel about things and i
think i saw some tweets i think i saw mark andreessen who's a tech entrepreneur and he had some kind of
manifesto and i realized like reading through this i happen to agree with like everything is manifesto
is this something for me so i started just following more e-acceleration of stuff and i think i i
realized i was really like the first thing that really confused me was i was talking to somebody
and i was trying to say like no no no it's not about inventing artificial super intelligence it's
about free marketing stuff like let's have let's unleash technology it'll be great for humanity things
like that and then they corrected me and then i talked to more i was like they were right like i was
wrong and they were right since then i've talked to more people and like yeah more of them are kind of
like this like they they're really excited about create like birthing i don't know a new kind of
entity a new new species or something like that i don't know if anyone's really this deep into it
but like you know making like a computer god or something like that there's lots of fantastic kind
of descriptions about stuff like that so quite a lot of them seem to be in the back end of it also
many of them work on my end of it where it's just like we're just kind of this is just another
expression of like libertarianism and there's there's tension there which is i know that that's
been interesting to just work out is is the tension between those two things yeah i've noticed
that's something that makes the ai debates really hard to follow because people are very used to
polar you know you're you're this or you're that you're republican or democrat left right or whatever
else ai is really a two-axis disagreement there's this concept of agi this artificial general intelligence
which will be like this god and that that kind of thing people who think that's going to happen and
we're on on track to that being fairly soon so that we should be making decisions under that
assumption and summer's like hey you know that that's kind of hype technology takes a long time
to develop you can underestimate how hard the problems are we might get another ai winter we've
had them in the past we should set our expectations a little less grandiose than that
but then there's the other axis which is like totally separate from it which is whether the
unchecked advancing of technology is going to be good or bad and you could be in any one of those
four quadrants like you could say that agi isn't going to happen but we should still be scared
because there's going to be all these externalized consequences like social media being an example of
that kind of thing or certain takes on social media anyway what i'm hearing from you there is that
effective accelerationism is very explicitly the technology is good but it's kind of ambiguous as
to whether agi is going to happen but the dominant active members tend to be skewing towards it is and
you're maybe like a little less focused on agi yeah i think that captures it pretty well i don't know
which way it skews more but yeah i see it the same way well there could also be a distinction between
the active leading members versus the silent majority i imagine people who see this as being the utopia or
hell or whatever else or might be a little louder than the people are like oh you know it's kind of
more of the same yeah yeah i think i've noticed something akin to what you're saying which is like
right now this is a really niche topic and i think that attracts weird people just innately whether
no matter which side you're on it you are so you get you get sort of weird or extreme comments just
by the fact that those are the kind of people who just end up in niche political subjects would that
be an accurate characterization of your view that i agi or super intelligence or that kind of thing
isn't coming soon yeah that's how i feel it could be wrong i feel like technology just as a general
statement i feel like technology takes a long time to develop it takes a long time to get
implemented and find its full productive views discovered like i feel like that that way about
the internet 30 years or something of really just like public internet and we're still finding new
ways of using it saying all that i don't really have a strong argument yet to present but that's
my thought that that things take a long time to develop intuitively do you see agi as just being
this thing that doesn't happen and isn't really coherent or what's your mental sense of timeline or is
timeline just not really how you think about this i think about timelines quite a lot especially trying to
listen to people if you realize what timeline people are talking about that makes a big difference
into what they're talking about it's a good question about agi so my definition of agi when i just think
about it is just human-like intelligence that that's kind of a rough definition but we all know what
human-like intelligence is like because we interact with humans all the time so i think of it as just
making a machine that has the capabilities roughly of a human being can it can it reason can it do math
does it have motivations does it have agency things like that i think it's definitely possible because we
we already have one instance of it uh in the world which is human beings it seems like a matter of
time before we can get machines to do that as well i guess i'm optimistic that something like that will
happen eventually whether it be in the next 100 years or or plus you know 400 years after 100 years
i'm feeling like it would it would take longer i think we're not close when i think about the the
capabilities of ai we have today that they're very impressive and in some ways they exceed human
intelligence but they're not close to something that can generally do things and bringing lots of
components together that could be a well-rounded intelligence that doesn't seem like we're close
to yet but i could be totally wrong about this too so what are some of the kind of things that
you see that say like oh yeah this this this is this is a really big barrier this is going to take a
long time for ai to get over this because you'll see the open ai demos and like sora realistic videos
and none of that was possible a year ago from a certain perspective it's like oh man this is just
like blowing up so much in the last year that's an exponential every year is going to be even more
mind-blowing than that if you have a more measured approach what are the key data points you're looking
at that that makes that seem likely music chat gpt every day both personally and at work and i i think
of it like i'm just putting in text and we're getting like text out of it also i i see the the
latency times a lot i think that's just relevant to our work because you know if you do a bunch of these
calls to open ai just takes a long time because every every single one takes half a second or
something like that i mentioned that to bring my unique perspective which is that if you're gonna
have a machine that's doing a broad range of human-like intelligent things i i would think
that a lot of it has to be somewhat instantaneous much how human reasoning seems fairly instantaneous
like people can have conversations that they say things within a second if they get thoughts you
know until a second time and just interacting with an ai to even get just like a text output which is not
demonstrating reasoning it doesn't have memory or agency or interest or anything like that takes
so long the times alone don't make it feel like a kind of centralized intelligence yeah there's two
arguments i heard there one i'm kind of skeptical on the other one i want to hear more about the the
one that seems a little strange to me is latency because if i asked chat gpt to write an essay
yeah there's latency it's not instantaneous but if i asked a person to write that same essay it would
take days the other thing that you mentioned though which i find interesting is that there's no
reasoning there say a little bit more about that because it often gives very impressive answers
that seem very creative and well reasoned not always what do you kind of see from digging into
this a little more on the technical side of it so first i think that's a good point about the latency
which is that it can compared to a human writing an essay it's like way way better right like it does
it way faster so like measuring the time of that yeah it's like exceeding human abilities but i think that's
a narrow ability producing language getting the next token giving it input the latency to even do
one sort of cycle of thought was what i meant the latency in a human brain is much faster but i don't
know if that's a really significant point i just wanted to want to clarify that i'm sorry i said so
much there that i forgot what sure the other thing i'd asked about was the you had mentioned that there's
no reasoning that's a little surprising you know from a naive perspective of just like working with chat gpt
it gives some really impressive answers it it seems like it kind of reasons and is creative
sometimes what are the kind of things that that you look at it and say like no that might all be
smoke and mirrors there actually isn't really reasoning here i see what you're saying we know
it can it can do reasoning but if you type in a complicated situation it'll give you a correct answer
that isn't just language it has to understand the components of what you're saying and then come to
a correct answer so i guess llm seem to be doing some kind of reasoning to that extent although
sometimes i get things wrong and that it's funny you know that that's the thing people share around
there's sort of like a language outcome or byproduct what i know about llms is that they
they predict the next token i'm sure it's much more complicated than that but they're doing plausible
language and i think what's interesting is when you really max out that ability it seems to spill over
into other abilities where by just looking at the context of language alone it could come up with
answers that seem to be reasoning i guess that's how i would explain this apparent reasoning
ability and i don't mean to undermine it by saying it's not real reasoning if we're getting the
desired outcome it does seem to be reasoning in some sense but it's not fundamentally like a reasoning
module i guess that's that's what i'm thinking about the thing my mind goes to in assessing the
intelligence of artificial intelligence is in terms of levels of abstraction and generality so if i
think of just about the the task of predicting the next token or the next word to be a slightly less
accurate but more intuitive concept you can imagine a machine that is a pure statistical inference kind
of thing just sees every word after every other word in every single case and says like oh when there's
an and then that does the next most likely kind of thing and but like but that's too narrow you'll
get a lot of wrong answers so if you have a little bit more context that'll that'll give you more
you could imagine something that you build that way and it could work and you can get the same result
but you would need just massively more data to to make that work for every possible combinations
of words that's long enough to be able to predict the next word just the amount of computation that
has to happen is of just this massive kind of lookup just gets ridiculous and you can compress a lot of
that down if there's some kind of knowledge of syntax like of how language works and just grammatical
structure and so on that can do a lot of work and in a relatively smaller space and then you can get
even more in at least in terms of quality if you also have some understanding of semantics like
what words mean and and so on you'll get higher quality with a lot less computation a lot less
lot lot less data required so then the question is like how many levels of abstraction are in there
because it's hard to see inside all those neurons where the abstraction's happening and also how much
abstraction is needed to get to a human level i guess the analogy i like giving is with chess
an ai that's the best in the world but just has a giant lookup table is less impressive than one
that's pretty good but just figures it all out because it's that generality that really matters
and that's a hard thing to measure like what's a test for generality like skill on a particular thing
is like oh does it hit this benchmark or does it not the thing that's missing from measuring the
intelligence of ai is that intuitively people seem to have this idea of this is the thing that's
emblematic of intelligence it used to be chess and then it was go and now it's language or whatever
else and oh it does that now it's intelligent but if someone had a different benchmark then they're
not impressed and so it's like some people the trend line is straight up and some people it's a
total flat line and like that's a hard thing to agree on a trend isn't useful unless it's linear
because then you can extrapolate and predict if it's just a sudden thing what can you do with that
where i would see that linear trend line is from levels of abstraction and generality
where exactly chat gpt is on that scale a little harder to say yeah it's kind of interesting i think
i think that's what you're saying some people are saying nothing's happening some people are saying
everything's happening and then it's hard hard to see which one of those things are i've heard some
people point out that like maybe the standard of agi is sort of changing or not not just different but
shifting we never consider what we have agi the agi is the next thing and that that'll be
be really scary but then i don't know if that's status is that that comforting or not because then
maybe the danger is already here in some sense if you look at the word general that's obviously a
scalar term something must be more or less general it's not something that's like an on-off switch
but thinking tends to be booleans and so like have we achieved it or not is like a very
natural question to ask and thing to say but even just like looking at the language of it like that
doesn't it's kind of incoherent i would be of the opinion that yes we have agi now but it's very
weak and over time it'll be stronger and at some point it reaches a threshold that makes a really
big difference that's really more of an economics and psychology question than it is technology really
i think like the definition of intelligence is just like raw ability but it's interesting when we talk
about agi it it seems like what spooks people is thinking about it having agency and intentions
which is not really an ability i wonder if that should be or should not be a part of the definition
if it's just the ability i think of computers is already like super able 40 years ago they could do
math better than you know anybody but most people wouldn't think that computers are more intelligent
than them it seems like they focus on this thing that's not really about ability i don't know like
at this point it'd be a good time to to shift gears here because the the conversation about how
intelligent ai really is is a really interesting one and informs a lot of things but it's actually
not the point that's the most contentious disagreement it's really more about whether
this is good or not before getting into the cruxes on that i want to understand where ex position is
if i were to just look at twitter the impression that i got was that they believe that ai or agi will
cause human extinction and that's a good thing because computers will be better than us and that that's like oh
yeah that's like that doesn't sound very pleasant is that an accurate view of what they're about
i don't know yeah so i haven't seen anyone say anything so direct as i guess i should in
a chat room or something speaking to people directly i haven't seen anybody say something
like this would be human extinction and i think it would be good i've seen richard sutton say that on
twitter i think i guess what i'm getting at is that is that a weird extreme or is it representative
i don't know i mean i'm not sure it could just be because i i've seen screenshots of people saying
that as well like i think there's a fellow named beth jezos who said things to this extent
and then it also i've noticed like people in at least the one discourse environment they seem to
like like that like that so i don't know transitively that like they actually also think
that maybe it's just a thing that they just don't focus on but if you push them or if you think
about what they claim that would just be a logical implication of that i'm not sure but there's some
excitement about if there could be a computer that's like that that's more powerful smarter
has agency all this stuff it'll be so powerful and it would just like it would be the next evolution
and so then there's a question about like what would happen to humanity it does seem like there's like
admiration for that next step and maybe that would come at the price of humanity you know i've wondered
on this topic specifically like like if there was a computer that took over the whole world i don't
know if it would dominate in some sense could you have empathy for it and like think of it as like
an ancestor or something like that people have different feelings about that i think the human
extinction is a bad thing i would maybe i should sure that's my flag i was gonna i was gonna ask
about that but yeah thanks for clarifying yeah yeah but yeah so i wonder if that's like a dividing
thing is how much people would identify with the ai that would take over the world okay so for you
personally human extinction is bad for effective acceleration is generally they don't seem to be
saying that explicitly but it sounds like it might be implied based on other things that they're saying
for the more libertarian side of it what like myself and we just don't think like agi is going
to just explode so it's just sort of not a question for us there's a little bit of jargon
from less wrong where a lot of these conversations initially happen called the foom theory or hard
takeoff or fast takeoff it has a lot of different names the way i would summarize that is if you think
of ai as automating tasks and you know enough collection of tasks as a job enough collection of jobs as an
industry well ai itself is an industry which is a collection of jobs of tasks and so if all of
those get automated then you can have a system that's able to just improve itself and we expect
this will happen because humans are improving ai and so those are abilities that could be done
by the machine so yeah once it gets to that point we have that ai just kind of improving itself
once you have that exponential or training itself then you have this really fast takeoff and the
reason that matters which is a thing that gets lost in that discussion a bunch is that if you're
relying on iteration to keep things safe like how things often work as you make something it breaks
you figure out what's wrong and then you fix it if the takeoff is so fast that you can't iterate
anymore then you can't rely on that really fundamental tool what's your reason for disagreeing
with that kind of logic it sounds fanciful that anything that could just blow up that fast just
because something has never happened doesn't mean that it won't ever happen something never happens
until it happens at least once but then i need a reason for thinking why is this extremely unique
event going to happen where there's a singular just an explosion of technology in this super advanced
entity that emerged very quickly in the world and when i think through the reason i'm not sold
i don't know that's a complete or satisfying answer but i guess that that's where i'm at
what you're basically speaking to rationalist jargon is priors the idea of like okay i have this model of how the
world works and you say something that goes against that model i need to update and so that
means i need a lot of evidence to make that change that's reasonable it's a starting point it's not a
proof but it's a this is where i start convince me and i think that's that's totally valid for just a
conversation the way i would respond to that is to talk about priors is that a reasonable starting
place this foom idea is this thing that's never happened before and now it's this totally new thing why
would you expect a sudden change i guess my skepticism of that or or why i would think that
it is reasonable is that well actually it's very well-precedented like if you look at the
millions of years of human history the last several 10 000 years was just a radical hockey stick kind of
change and even the last several decades has been another exponential comparison to that talking about
the agricultural revolution and the industrial revolution and then the digital revolution after that
we're on an exponential now what's so strange about expecting that exponential to continue
exponentially yeah yeah great point i totally see what you're saying that especially on like
historical time scales there have been major major changes it's abrupt changes in human history and it
does seem like things are accelerating there's a fellow i quite admire robin hansen i think who pointed
out that the time between the agricultural revolution and the industrial revolution is way longer than like
the industrial revolution and maybe like i don't know the information age that we're in now i'll
concede that but i think even if things are accelerating we plot it out is it going to really
accelerate to like things change overnight i think that'd be my my first response is is it going to
accelerate that much and then my second one is what specific things are we going to analogize about
the previous acceleration of human history with sort of a foom-like situation because all of these
revolutions that we're we're seeing historically and like in our short evolutionary history
all still have humans in them for example there's still constant features of humans throughout all
that even if there's lots of cultural and biological evolution going on in the meantime yeah so the thing
i would be analogizing to would be the transition from human knowledge development being based on
evolution to being based on culture some animals can use tools for example but all of that is something
that they've gained the ability through some evolutionary adaptation but then once humans have the ability to
pass on culture and accumulate on that now we're no longer reliant on people dying off in generations
and some propagating and spreading you can just have an idea go person to person and just go on that
time scale well okay one thing i'm going to concede part of the foom hypothesis was back in the days
before deep learning was dominant and people implicitly expected that ai would be code that people wrote
and then ai would help write the code and then it would get better at writing its own code and like
okay you can kind of imagine how that improves itself and gets faster and faster and you take humans out
of the loop now that it's like training it and you don't really self-improve you maybe make a better
algorithm and then start over and make a new ai that's a little awkward through that that foom lens
even so i think the idea is not necessarily dead i'm thinking about things like chain of thought or
groups of agents communicating with each other perhaps if it gets to a point where rather than
having to create a new training run things can generate new capabilities by just talking to itself
or other instances then it loses that barrier and that's something that could happen at a critical
threshold yeah i don't know maybe there would be some i mean i could see like the way that we would
get like that that could be a differentiating factor between all of i guess like the advancement of our
society up until now is that there's a revolution in intelligence or self-improvement and then that would be
a another acceleration in the development what you're saying i think i want to shift the subject
again because the hard takeoff is actually not really like it's relevant but it's not the core
part in terms of whether we have a pessimistic or an optimistic outlook of how ai goes there's a big
question as to whether one expects ai to be good for people by default or whether that's something
that takes really careful engineering to get right i guess that that is my prior that technology is
good and freedom is good good things that they that help humanity but then remember my position
is we don't have agi we just have a really cool useful technology that seems empirically plausible
that you know you develop with technology it just wipes out everything but i just don't see this
technology as close to that i have priors or assumptions that letting people work with this
technology or just technology generally will let them come up with something good that helps people the
thing that would be in opposition to the view that you described is what i see gary marcus for example
tweeting of even if it doesn't become this world conquering sort of thing it's still super dangerous
to have this thing that people can use it to make weapons and have deep fakes that mess up the elections
and nothing on the internet is is reliable anymore there's all these externalized costs that companies
don't care about that they're just going to go after the profit and they're going to push all these
damages onto society and that's really bad we need to need to reign the tech companies in
those seem like real concerns as well but that outcome of ai doesn't have to be human extinction
it could be all kinds of bad things it's perfectly safe but it helps people make weapons or something
or it destroys the wonders of the internet we can't really communicate because we can't trust anything
on it anymore so yeah i see what you mean those are real independent questions as well any technology i
would expect to be disruptive any really big technology so i want to recognize that there's winners and
losers in anything like that there are bad things i'm sure but i think on net that's the point that
i'm really focused on i think on net i think things are for the better i'm sure there are losers with
the internet too but most people wouldn't think of the internet as a mistake or wasn't worth it i'm
wondering that something like the internet was released on people very slowly and a lot of the dangers of
it were mitigated by people seeing things happen and then working against it i'm imagining if you were to
drop the internet of today without any defense mechanisms without any antivirus without any
knowledge of phishing scams or social media addiction or any of that kind of thing right back
in the 90s that might have been a lot more scarily disruptive what i'm getting at is the the pace of
innovation seems to matter it's interesting because when i think of like the slowdown it's mostly just
because people have not gotten used to it you know this hypothetical internet of today just dropped on
people in the 90s like when people even log in would they even know that's a thing to do they even have
the dopamine addiction to log into aol or whatever would be available to them when i think of the
slowdown i think of as just an inherent part of consumers just adopting things or businesses just
adopting things so there's going to be some kind of breaks just inherent in something like ai actually
and this kind of relates to the work that you're doing open ai pushes some foundation model that's just
going to be a curiosity until people actually do the hard work of making products that are actually useful
finding customers communicating the value to customers customers learning how to actually use
that stuff telling their friends about it all these things that are based on human timescales
that take time to disseminate yeah i think so you captured what i was trying to say pretty well maybe
there's one thing i could add on to that too is i think so assume that we should pause ai like it's
dangerous we should go slow we should think about it we should make sure that it's safe there's
another question which sort of entity in the world should be the one doing the pausing or the one doing the
safety concern and that could be the government which is what a lot of people think but i also
think that the corporations like the companies themselves they could also handle safety as well
they might have their own concerns they probably don't want to wipe out humanity at the very least
for profit incentive right like there's no one that's healthy with no one alive and it seems like a lot
of people complain about especially like in the ea areas like they've complained about the
censorship of these lom's like you can't get them to say certain things and i think there's a really
bad history of uncensored ai it makes sense why the companies would not want to have an uncensored one
because they don't want the project to just crash and burn immediately i i connected that to our
previous thought too because it seems like the the companies themselves might not want to be too
disruptive they might want to slow down the pace of their rollout just innately because they don't
want to do something that looks really ugly really quickly they want to have a good track record
a good reputation but long haul they're going to have some product that's going to be around for
a hundred years so the core idea i'm hearing and all of that is like okay yeah it's it's fine to
have an intellectual like philosophical debate about whether we're better off going fast or slow or
whatever else but we're on this fast track right now and if we're going to change that that means
changing it by some mechanism whether that's a government regulation or whatever else and those
mechanisms that we would use that has to happen that need has consequences and that needs to be
balanced against whatever you think the impact of the trajectory we're on is yeah i think i think
that's part of it i mean if you if you thought companies were not going to do a sufficiently good job
being safe and the government would be do a sufficiently good job of regulating safety
i can see why you'd pick the government if i'm kind of curious about like making the observations of
what do these companies do i've at least observed companies to some extent being slower not 100
unleashing because they don't want to get a bad reputation through a lot of ugly disruption so i guess
that's like the framework i don't know if that that's pro or anti-regulation or pro or anti i'm a
libertarian i'm always thinking about what companies can do but that's like a framework of thinking about
those i i see on the time that's towards the end of what you had available so i want to respect that
thanks very much for your time i and enjoyed the conversation yeah thank you will for for
reaching out to me i hope you have a good one you as well
