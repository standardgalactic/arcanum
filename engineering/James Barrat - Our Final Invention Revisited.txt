Welcome everybody. We've got James Barrett here today, and I've done an interview in the past
with James Barrett in 2013 on his book, Our Final Invention. We're going to be revisiting the topic
of AI friendliness or AI safety. But just by way of introductions, James has been a prolific
documentary filmmaker, and he's also a speaker and author. He's been involved in large broadcasters
like National Geographic, PBS, NOVA, BBC, and the Discovery Channel. And recently on PBS,
he did a documentary a couple of years ago before the pandemic hit on zoonotic diseases. So look,
it's really topical. So thanks very much for joining us again, James. How you been?
I'm doing great. It's a pleasure to be here, Adam. Thank you for inviting me again.
Absolutely. It's always a great opportunity to talk to you, and I love talking about these
topics anyways. But tell me about this zoonotic documentary you did recently.
Well, it was a couple of years ago, but they've been playing it on PBS a lot because it's germane to
COVID-19. It was called Spillover, Zika, Ebola, and Beyond. And we started out to make a film about
Ebola, and we went to West Africa during the end of the crisis. And then Zika came out,
so I went to Brazil to cover that story as well. So it ended up covering Zika, Ebola,
and some other diseases. And these are all diseases that jumped from the animal kingdom to
to humans, very much like COVID-19.
And, you know, one thing that struck me when we were making that film was that
everyone I interviewed, I interviewed a lot of people at the CDC and people at the National Institute
of Health's Infectious Diseases unit. And they were very, very confident that nothing like Ebola or
Zika or any other very virulent virus could catch on here because our medical system is so sophisticated.
And it's very ironic that COVID right now is just kicking our butts and we seem to be helpless against
it.
Oh, absolutely. I mean, yes, we can certainly be very arrogant. Maybe people just don't think AI
will be a problem. And I'm worried that we're going to be blindsided with some of the problems that you
introduced in your book, Our Final Invention. For those who haven't actually been introduced to the
book, do you want to give us just a brief introduction to what the book is and why you wrote it?
Sure. The book is about the long and short term impacts of AI, none of which could be very good.
I was I was at the time I was I started my my interest in AI by reading a lot of rate by reading
Ray Kurzweil and and and a lot of other people about about AI and the the books that were on the
market at the time around 2010 when I started writing were all very rosy and AI was was this
this great thing. And I've been following AI since about 2000 and I was also bitten by the AI bug and
pretty much euphoric about the the potential for AI. But then I interviewed Arthur C. Clarke around that
time a little bit earlier than that. And yes, it was a I interviewed him around 2001. And he said, he said
to pop my euphoria, he said something like, we humans steer the future not because we're the fastest or
the strongest creatures, but because we're the most intelligent. When we share the planet with something
more intelligent than we are, they will steer the future. So that that stuck with me and kind of
that that that idea never went away. And I started talking to AI experts and AI makers about
what I used to call the two-minute problem. And it was when I listened to an AI lecture
in the final two minutes, the the expert would say, well, and by the way, there is a chance that
AI could get out of control and, you know, could really, could really do a lot of damage. And then
they'd go back to the good news. So when I investigated the bad news, it was pretty bad,
including wiping out the human race. But it started with it, it starts with smaller things,
and we can see those smaller things now. And we can get into them. It starts right now, we've got,
we've got issues with AI. And I'm not I'm, I'm generally a proponent of AI, I think it's got terrific
potential. It's being used to, you know, very well in medicine right now, very well in radiology,
diagnostics, business analytics. It's solving a lot of problems, and it has a huge future in
medical issues. But at the same time, it's a dual use technology capable of great good and great harm.
And we've got problems with, you know, I can go into these now or later, but you've got problems like,
like bias in the data, which we've got, we've got issues with, with algorithms, not
the data that the algorithms were created with don't represent minorities. So you have minorities
and women who can't get, who don't get jobs at this, at the same rate as, as men. In the UK,
there was an algorithm that wasn't that was for, for college admission that was biased against
minorities and women. In America, there are a lot of sentencing guideline algorithms that were based
on data taken from the 70s, 80s and 90s, where as a matter of course, minorities had longer prison
sentences, they just got them, it was, it was just plain old racism. But with, with, with data,
data always brings values with it. And so now in the modern day, they were using these sentencing
algorithms that were, that were, that were prejudiced, and gave, gave people of color,
longer prison sentences. So there's a lot, there are a lot of, before you get to the big existential
risk of AI, there are a lot of smaller problems along the way.
Yeah, absolutely. And look, just to set the stage, there's been quite a bit of development in AI since
last we spoke in 2013. I think it was like almost Christmas in 2013. And a few of the developments
have really shown the power of deep learning, very powerful, extremely powerful. I mean,
we've got GPT three, we've got, of course, the, the, the winds like alpha go, we've got alpha star,
more recently, alpha fold, which is, you know, solves some scientific problems. I say solved
reservedly. Of course, it doesn't understand what it's doing. It's all it's really doing is just
predicting protein folding to a very, I guess, astonishing degree of accuracy. That's great.
But this is a, this is a problem made. Do you think has your mindset changed? And has the public
mindset changed about the potential for AI now? Then, then what is what what it was in the past,
like seven years ago? Yeah, I think
we couldn't have anticipated a lot of things seven years ago, the folding problem, I remember people
talking about it as being one of those unsolvable problems, where basically, how to determine a
protein's 3D shape from its amino acid sequence, humans can haven't been able to do it. Now,
alpha fold has GPT two is is really impressive as a language modeling
software that basically anticipates what what's coming next. We couldn't anticipate then, you know,
what's what what's coming next? What's the next word going to be? And that's how it strings together
a whole lot of words and ultimately loses meaning because it doesn't really understand anything.
What we couldn't, I couldn't anticipate was how much money was going to AI. It's the amount that it's
exponential growth in the investment. The amount has doubled every year since 2009. Right now,
it's about 30, 30 billion dollars, which doesn't seem like a lot. But PricewaterhouseCoopers
anticipates that by 2030, AI will add 16 trillion dollars to the global GDP and making it, you know,
one of the largest, if not the largest part of the economy. That same year, it says Gartner and company,
half of all jobs will be lost to AI. Is this Gartner and McKinsley are saying this? Wow.
This is not, this is the Gartner and company. PricewaterhouseCoopers was the first one. I mean,
I, I, McKinsley's probably on board with these guys, but I don't know. Yeah, yeah, yeah. Wow.
That's incredible. Half jobs by 2030. Is that existing jobs or is that all jobs? Um, do you think
that like half of these jobs that we will lose to AI will be replaced by new jobs? Or do you think that
there'll just be a paucity of jobs in the future? I think there'll be a paucity of jobs. I mean,
any job, when I, when I talk to students, I tell them don't, don't take jobs that, that computers can do,
don't do anything that involves routine or rote memory or doing the same thing again and again.
Uh, don't do, you know, look at, look at all, look at the kinds of jobs that are going away.
All factory jobs will be gone. Um, uh, all driving jobs, but not too much time will be gone. All postal
jobs are virtually, virtually already gone. Anything that requires repetition, but that goes up into the,
into not just working class, but into, uh, into not just blue collar, but white collar. Um, a lot of
legal jobs are going to go away. Legal discovery, the, what first year law associates do, because it's
really just research and, and, and, and, uh, computers can do that a lot of simple research better than we
can. Radiology medicine, um, all, all, all sectors are going to get hit. Now the, the, the, the,
the, the, it's been true. It's been true in the past that new technologies do not yield a net, uh,
uh, loss of jobs. This is different though, because it's, because it's cognitive, because it requires,
it's a, it's a, it's a, it's a thinking technology. It's not like a cotton gin or, uh, an assembly line.
Um, and so it goes deeper into, into more strata of the economy. And, um,
it's very unclear to me. People say, well, we'll retrain these low skilled workers. We'll
retrain them for other jobs. We'll make them, uh, we'll make them program. We'll get them to be
programmers. So we'll make them into robot repairmen. Well, if you took an assembly line full of people,
you might get a couple of managers out of that. You might get a couple of people who retrain up,
but people who are, who are working on an assembly line and people who are driving cars are doing
that because they don't have a lot of other skills. And the idea that they could, they, they, they could
be trained for more advanced skills that the robots haven't take, haven't taken. It's kind of a myth.
Um, it's hard to anticipate the new industries that will be born from AI. There'll be things we can't
imagine right now, but I think it's highly unlikely that there'll be jobs for all those people that the
AI and automation world displays.
Indeed. This is, this is a scary future for a lot of people. I mean, when there's a lot of
desperate people around, people will do desperate things. Um, we're finding that, yeah, we're finding
that right now in the United States, we're having riots and, uh, we have, you know, mass unemployment
because of the, of the virus. And we have people who are just, just, just, just, you know, as they
say, losing their religion over, uh, job loss over the, the idea of government getting bigger and more
powerful during this, during this pandemic. Um, and we've, we've also got a leader who's just batshit crazy
and isn't, isn't, uh, is inciting violence.
Which is a difficult thing. Um, and it's one of the themes that I've sort of interviewed others
about is the degree to which people trust experts now, because they've got this source of alternative
facts, this post truth-ness, um, where, where people are finding it very difficult or this,
it's just strange to tell the difference between like, what, what can be, what should be able to
be given like strong credence and what shouldn't, you know, who to trust. There's this lack of trust
in, in, in the world. And especially in America, I imagine, um, at the moment, which is a difficult
problem to navigate. How can you convince people where you can't actually share a mutual ground on
which to make, which to adjudicate the truthfulness of claims made and, and, and, and the realities
before us? It's crazy. We've, we've, we've got a, uh, a president who doesn't, doesn't trust science,
probably because he never took a science course or never took a, you know, test that he didn't,
didn't cheat on or have somebody else take, but he just does not trust science. He does not read.
Um, he's, he's, he's studiously ignorant about the world around him. He's, he's the least curious
person in the country. Um, he doesn't trust his scientists. He's got, he had a team of the best
scientists in the country, epidemiologists. Um, I, I remember interviewing Anthony Fauci years ago and he
was even then just, you know, the, the best epidemiologist or person who studies epidemics in the,
you know, in, in our country anyway. Um, and he's, he, he's not trusted. And the whole,
the whole way that this, uh, the whole way that this COVID response has been treated is, is because
of, as you said, this distrust of science, this distrust of facts, that there's alternate facts.
Um, it's, it's absolutely absurd. It's a real, it's like, it's like the enlightenment never happened,
it's a step back into ignorance. Yeah. It's, it's unfortunate. Um, but I mean,
yeah. So if, if we had this public distrust of, of facts and science, I mean, it's going to be
even more difficult to, um, talk about this issue of AI friendliness. I mean, if people just want to be
the most comfortable explanation, the, the, the explanation, which provides them the, the most
sort of security or, you know, the vision for the future, which is exactly. Yes. They, they want
comfort and they want to be told everything's going to be okay. Or they want, uh, enemies pointed out.
I mean, that's what they really like. What it seems to be, uh, you know, half of our population,
the United States want, what they want are scapegoats and people to blame things on. Um,
I used to, I, I, I mean, I wrote our final invention in, in as simple, uh, uh, a language,
as simple language as I could muster in order to get it out to the most people and to make even
non-scientifically based people understand, but which I'm wondering now if that was a waste of time,
because you know, I think that the, the, um, part of the audience I was trying to reach doesn't read
anyway and just, and would, it wouldn't embrace a science book. What's the problem in, in, um,
I guess science communication, uh, in general. Um, and that is people
are low ration, uh, low information, um, rational, rational agents, I guess that we don't execute
on huge amounts of information like AI might in the future. Yeah. But, um, yeah, so, so it's very
difficult. People often have very strong opinions, especially about the opinions of the things which
I know least about. Yeah. There was something called rational agent economic theory and it was
designed to, uh, anticipate what markets would do based on rational decisions that we make buying and
selling and economists pushed it as a way to sort of normalize markets, but it didn't work because
we're not rational. We impulse buy, we buy things we don't need. We don't buy things we do need.
And so the idea that we're, we're rational agents is, is kind of a big mistake. What we are is
impulsive and very emotional. And we do, we develop attachments to things that are absolutely
terrible for us. Um, and that's why, you know, and I think part of, part of the, uh,
hype about AI has been, you know, a giant spoonful of sugar. I think Ray Kurzweil is a great inventor,
a great prognosticator about the future, but he's, he's been part of the, the selling machine.
To, to make AI seem safe and lovely. Um, but I think, you know, I, I, I, I'd have to say on the
encouraging side, we've seen AI safety grow as a, as a, as a topic. We've seen AI every, I'm stunned
at how many organizations have popped up that are discussing AI safety and AI ethics, you know,
Future of Life Institute, Future of Humanities Institute, the Machine Intelligence Research
Institute, and a bunch of new forces that you can't keep up with them.
Existential risk, yeah. Heaps of them.
Yeah, you can't, yeah, exactly. There's a lot.
And I, I, I find.
Or the Catastrophics Risk Institute with, uh, Seth Baum.
Yes, yes, Seth Baum, yeah, yeah.
We should have one in Australia.
You should, you, there you go. There's your, there's your, your side career.
If anybody needs the right academic context, yeah.
And, and they come out with these voluminous, uh, treatments of, um, of AI ethics.
Uh, some of them aren't voluminous, and those are actually the best ones.
Because I don't, I think that we've, we don't need principles.
We need, uh, regulatory bodies that can go and look at whatever AI you're building.
Just like we have, you know, I, I've said this forever.
Um, we have an IAEA, International Atomic Energy Agency for looking into silos and looking into
reactors and making sure that people are, are conforming to international, uh, protocols.
And if they're not, they get, they get, um, they get, uh, censure.
They get, uh, they get, um, steps taken against them.
They get tariffs. They get all kinds of bad things that happen to them.
And what's it, what it's happened to, uh, in the recent past, it's happened a lot to Iran.
Um, we need that for AI.
Uh, we need, we need those sorts of boots on the ground, people looking in, into labs.
I, I'm usually the, I would be the last one to say we need any kind of government regulation
because it's generally inefficient.
But in this case, we need government regulation because the companies that are building the AI
just cannot be trusted on their own. They just simply can't be trusted.
They all have a, we, we can go into that. Uh, yeah.
What did you say? I think in another interview, like you brought out a really interesting factoid
and that is about Cambridge Analytica and how it, it, it, it took a lot of sure.
Well, okay. So Facebook allowed that, right.
We have, we had this, there's a concept, uh, you're familiar with, I know called the intelligence
explosion.
Yep.
And you can see the parts of it assembling around us. Um, and it goes like this, and this
was brought up by I.J. Goode in the 1960s. He could see it. He could see the potential for it.
And interestingly, in the, here's some trivia. He was, he was evaluating an art, one of the first
artificial neural nets at the time it was called the, uh, perceptron. Yep. And it was, it was a,
it was one layer, uh, and it didn't, it didn't do much, but it did a little bit, but unfortunately
Roger Rosenblatt, uh, who invented it, died in a boating accident. Hmm. And then Marvin Minsky wrote
a book saying, Oh, artificial neural nets will never work. Yeah. And it killed him. And they, they
were, they were happening in the sixties that we could, we could have been far ahead right now.
But anyway, um, the whole idea behind just to build up to this, one of the things that's working
so well for us right now is, um, cause some, some AI makers in around 2009, uh, Jeffrey Hinton,
um, Stuart Russell to name two. There's, there's, there's another, there's another one,
another third one. I was going to, I forgot, but they had the insight that if you take up, you take
a lot of data, feed it to a, a simple learning algorithm, you'll get superhuman, uh,
pattern recognition capabilities and, and, and prognostication, guessing what's going to happen
next capabilities, which is exactly what's happening with exactly what's happening with, uh, GPT-2.
And, uh, then, then the more, the more layers that you may take a simple learning algorithm and take
another simple learning algorithm and another one, then you get deep learning and you get really,
really powerful capabilities. Um, that led to, uh, to, uh, machines doing a lot of things that we
used to be able to do, uh, that were just solely in the, in, in the purview of humans. And now it does,
yeah, there's great translation, great navigation, great, it's going to soon do great driving. It's a,
a lot of the things that, that it's doing right now is owed to this, this insight.
So to apply the intelligence explosion to this, we're creating machines that do a lot of what we
do, but they do it better at a point. And we've just solved, one of them has just solved the protein
folding problem, which nobody thought was going to happen for a long, long time. Just like they never
thought the game go would be defeated, but, but alpha go and alpha goes zero defeated it several years
ago. Now that was supposed to take 30 years. Um, so we've created a lot of machine. We've created
machines that do a lot of things better than us in the not too distant future. We'll create machines
that, that do, uh, artificial intelligence research and development better than we do.
And then the machines, according to IJ good, will set the pace of intelligence expansion, not humans.
And then suddenly we'll be sharing the planet with something that's a thousand or a million,
a million times more intelligent than we are. We don't really know what the ceiling of intelligence
is. We know that the, the ceiling of human intelligence is, is, is not awfully high,
but we don't know what, what hypothetically, what, what the ceiling is for, for, uh, for, um,
for AI, uh, there may be no ceiling. So we're, we're, we're building, we're putting all the building
blocks in place for this, uh, for the intelligence explosion. Yet we don't really know. And this is,
this is the problem. This is the, uh, this is, uh, the control problem. This is the alignment problem.
We don't know how to, how to live with something that's a thousand or millions of times more intelligent
than we are. As, as Stephen Hawking said, we won't, it will create weapons. We don't even understand.
Um, it will outsmart our canniest politicians, which is kind of a low bar, but that's, it will,
it'll, it'll be, uh, intelligent in dimensions that we have, that we don't, we just don't grasp.
Now that that's all pretty, pretty terrifying, but now look at who's in charge of the intelligence
explosion. Demis Hassabis was one of the co-founders of DeepMind. And a couple of years ago,
he said, I don't trust the AI makers to monitor and control the, uh, intelligence explosion to mitigate
it before we hit that, that time when we were suddenly, you know, when we're suddenly sharing
the planet with something that's far more intelligent than we are. Um, and, and why would he say that?
He say that because the, all the candidates are morally pretty crappy. Um, I, you know, it's,
it's, it's, and some of it's just truly astounding as everybody knows or should know. Um, in 2016,
Facebook gave the, the private data of 80 million Americans to Cambridge Analytica.
Cambridge Analytica gave it to Russians together. They, they, um, targeted a lot of American voters
for, to, to influence them in the 2016 election. We don't know how great their influence was. You'd have
to look into the heads of all those people. And then, so, um, right after, and, and while, uh,
uh, what, while Mark Zuckerberg was, was apologizing to Congress, they were giving the same data to the,
to the Chinese government. So, and they hadn't, they still haven't learned right now. They were just,
Apple and Facebook were just caught, uh, colluding about, uh, measures taken against them in antitrust,
in an upcoming antitrust suit. Um, Apple, you know, so, so Facebook, would you trust Facebook
with this technology? Well, I wouldn't. Google has, has 400 lawyers for, among other reasons,
they've been sued in 20 countries for everything from privacy violations to predatory business practices,
to, to, uh, to intellectual, uh, property theft. Um, I wouldn't trust them with this technology
either. I wouldn't trust them with the, to mitigate the intelligence explosion because
we know where their values are. The values aren't with safety and fairness. The values are
in the bottom, in the bottom line. Um, Apple's, Apple's in the same boat. Apple's, uh, they came out
recently and said, oh, we're so shocked that one of our, one of our suppliers of iPhone parts,
uh, or iPad parts, one of our suppliers was using child labor. What they were doing was using, uh,
they had student interns that were supposed to be paid, were not being paid and basically kept in
slave quarters. Um, a few years before that, uh, Apple turned, turned a blind eye to Foxconn.
Foxconn's the largest industrial contract manufacturer. Uh, Foxconn had a series of suicides
because their working conditions were so crappy. Um, Apple expressed, expressed dismay. Foxconn
stopped reporting suicides. And then the whole thing, the whole problem went away. So this coterie
of, of tech companies, I'm leaving, I'm leaving some out. Um, if you were going to pick companies to
monitor the most sensitive and dangerous moment in our, in human history, the intelligence explosion,
if you're going to pick companies, they wouldn't be these companies. We, we can't trust them with,
with the intelligence explosion, which is what, one of many reasons why, why, uh, supervision and
regulation is necessary. Hmm. Now, so you, you may, I got on, I got up on my soapbox just then. So
I'll try to, that's fine. I'm down. Look, um, look, I, I, I, I, I'm a bit stunned. Uh, look,
when I hear this sort of information, it is, um, it's, it's nerve wracking. Can I say the least?
Well, it's like you're driving, yeah, you're driving in a car and four of the five people
driving a drunk and you know, you don't, you, you're, you're not allowed to get, you know,
you, you want to get behind the wheel. You don't want to stay in that car, but we're, we are in that car.
Do you think it's, do you think it's more the problem that there's going to be humans
trying to control the AI that, um, I kind of, I guess, misguided or not, not ethically sound?
Or do you, or do you think that this endemic to an intelligence explosion is a high likelihood
that AI will converge on some, um, I guess,
strategy that will involve wiping humanity out or both?
Well, I think the whole, the whole, uh, the most important consideration about the alignment problem
is, you know, imagine, imagine all of the space of possible AIs. It's quite vast. You know, what,
which one are we going to get? Uh, if we're extremely lucky, we'll get one that's,
that's benevolent to humans, but that benevolence won't, you know, if we pick that one,
that the one that's benevolent to humans, that's a good thing. Um, however, for it,
for it to be benevolent to humans, we have to program that in. In fact, we have to program in
more than benevolence. We have to really, you know, we have to, we have to program in something that
changes and grows with us. Something, uh, Eliezer Yudkowsky calls, uh, extrapolated,
coherent, extrapolated volition.
Yeah.
Yeah.
It's a, yeah, yeah.
It has to, it has to be intuitive.
It has to, it has to grow with us. We don't want to be locked into the, the moral norms of today
in a hundred years. You know, we, we don't, we, we want, we want something that grows with us that,
that, um, anticipates what we, what's best for us without being dictatorial. But we don't know
anything about how to program benevolence into a machine. We don't know anything about how to
program intuition into a machine. So the, the odds of that one in the space of all possible AIs that
are coming towards us, the odds of finding the one that's benevolent to us are extremely small.
What's more likely is that we'll find, uh, incredibly powerful AIs that are ambivalent about us,
which is the same thing as extinction.
Because they'll, they'll do what they do. They'll, they'll, they'll use resources. They'll, they'll,
they'll do, uh, calculations. They'll, they'll, um, they'll act in their own, in their own, uh,
in their own self-interest. Uh, they won't, they, they won't be benign.
Yeah. It's, um, I think that last time we spoke in depth about Steve Omohando's basic AI drives.
Yeah. So, I mean, if anybody wants to find out more, I'll provide a link to the previous
interview in the description. Um, but basically it says that if we don't, if we're not careful, um,
the AI may, uh, have goals fall out of, uh, its primary goal, whatever that may be car
manufacturing, paperclip manufacturing, getting into space. Um, and those sub goals would be
anything like, you know, um, could be like, just try and extract as much resources as possible.
We're, we're made of resources or, um, you know, uh, try and update its own source code to be more
creative and, and all those sorts of things. So, yeah, it's fascinating. Yeah, I think,
I think if anyone hasn't read basic AI drives, it's just, it's such a good read and so logical. And,
um, that's the, he uses, actually, he uses rational agent economic theory to analyze the behavior of,
of, uh, of super intelligent machines. And it's, it's quite fascinating and quite, and very,
and very compelling. Um, you know, the history of technology is the history of unintended consequences.
Right. And we're, we're racing, we're racing into this future where, uh, where we don't know how,
you know, this is why it's called control problem. We don't know how to, how to anticipate the full
scale of consequences. Right. There's a guy who wrote a book, um, and I had it written down somewhere,
uh, that, that you often quote from, yeah, it's, uh, what was it? Um, normal accidents,
accidents, normal accidents. Yeah. Yeah. Who's your, yeah. And he's, it's Charles Perrault,
uh, P R O W. And it's a great book. It came out in the nineties, but he was talking about, um,
he was talking about very complex systems. Uh, if you have a system that's, that's very,
very complex, then accidents are a normal part of its operation. And he was using, um,
Three Mile Island and Chernobyl and, and, and airliners as examples. And we, you know,
they occasionally, airliners occasionally drop out of the sky. Um, Fukushima has happened,
Chernobyl's happened, Three Mile Island happened, but he, he analyzes, uh, Chernobyl. And it was a
series of events that you could not possibly anticipate. One crew went off duty, uh, sign,
somebody put a sign in front of a, uh, a red light that, you know, talked about that was about a
temperature gauge. Somebody did something else. And those three things in combination, uh, created this
gigantic environmental disaster that we saw that the region still hasn't recovered from. We don't
know what it's done to our biosphere. Um, but, but the, the, the important takeaway is when you design
things that are, that are so incredibly complex, uh, having accidents is a normal part of the, of,
of operating them. Now we know, I mean, you, you, you've mentioned, um, the, the explainability
problem that people are dealing with, with AI right now. We don't know, we, we know with neural nets,
we know what the inputs are. We know what the outputs are. We know how to adjust the inputs to
change the outputs, but we don't know in a high resolution way what's going on inside.
It's a black box. Yeah. Yeah. It's a black box system. And so, so are, um, neural nets and so are
evolutionary algorithms. Um, and there's a movement to, uh, start over with a lot of programming and make
it explainable throughout. And one of the reasons is if you're driving a car and you have an accident,
if you have a, if you're in a self-driving car and an accident happens, you need to know exactly
how to apportion the blame. You need to be able to really pinpoint that part of the code that, that
steered you, steered you wrong, so to speak. Um, just because of tort law, just because of how other
systems operate. Um, so we're, it, it, it, it, it's the lesson of Charles Perot is that we'll have
accidents that we can't anticipate with, with advanced cognitive architectures. Right. And some of
these, um, accidents, if advanced cognitive architectures get even more powerful, could be
quite extreme and arguably may actually lead to, um, our extinction, um, you know, or extinction
of, of all life in our biosphere. If we're not careful, uh, if, if, if these accidents involve
a machine that is, um, that doesn't actually have, have a concern for us at all. It's just like it
doesn't have any understanding, doesn't have feeling, it doesn't have any, um, form of like a,
I guess to anthropomorphize things a little bit, doesn't have empathy. Right. If I'm going to use
that term, um, sure. Yeah, sure. Uh, so, so if, yeah. So do you want to talk about what could happen
if we, you know, we have an extremely powerful, uh, optimizer that, uh, doesn't have this sort of inbuilt
benevolence as you described it before? Well, I, I, it happens. Yeah, well, uh, according to
Amohundro, um, the basic drives, I mean, no matter what the, the, the software is, is designed for,
it will be useful for it to have enough resources to run. It will be against its programming to not
be able to run. We're talking about very sophisticated systems. Um, so it will try to
get, it will try to get resources. Um, and it'll try to guard resources. It won't want to be unplugged
because being unplugged is the most severe damage to its, uh, to its, um, objective goals, to its goal,
goal seeking, to its, to its, exactly to its utility function too. Yeah. Goals sound more like easier
to understand. I use these terms, but I just, yeah, I think, yeah, most people understand goals.
Yeah. So goal seeking instead of utility, utility functions again from rational agent economics here.
Um, it will, it'll be creative. So it'll, it'll always be trying to anticipate how to better
increase its chances of achieving its goals. And one of the things that we'll do quite naturally,
uh, is try to try to understand and develop artificial intelligence research and
try to improve its own code. It'll, it'll try, it'll, it'll be, uh, at some point will this,
this, this, uh, cognitive architecture will be self-aware enough to have a model of itself in
the environment it's, it's in, and it will want to improve its own, its own ability to achieve its
goals, i.e. its intelligence. So it will become a, a program. It will become, it will become, uh,
uh, capable of doing artificial intelligence research and development. And then you've got,
as we said a minute ago, you've got the formula for the intelligence explosion,
then its intelligence increases very rapidly. So just creating a goal oriented machine of,
uh, that's, that's, that's, uh, at the level where it's self-aware and self-improving
is sets up the intelligence explosion.
Hmm. We may end up with, um, an alien-like intelligence that is completely different from
ours that, um, is a black box we don't understand. It may not even understand itself. Um, it's just
extremely powerful, right? In the same way as we can see, uh, systems today that, that are hugely
complex, uh, like a massive, but still, uh, uh, extremely complex, very alien to our, us.
They don't think like us, they don't behave like us, but they do things extremely powerfully. They,
they, uh, solve problems quickly. Um, they do sorting and searching rather fast and, you know,
they're excellent at, um, calculations and things like that. So image recognition, voice recognition,
it's, it's, they have, they have, they have what, you know, if you, if you pulled someone,
pulled someone out of, uh, you know, the 19th century and showed them these, these capabilities,
they'd say they were godlike or they were superhuman. Yeah. Just, just the ones you've
gone through, just, just the stuff that we call routine now. Um, you bring them all together into
a general intelligence and you've got something that's, that's godlike. You've got something that's
just awesomely powerful. If you imagine, you know, uh, we're, we, we had a, there's a big insight
with artificial neural nets. What, what Gary Marcus says has to happen now, Gary Marcus, the AI maker
and thinker is we need some, we need an insight about common sense. We need, we need, you know,
we need, uh, an insight about, uh,
uh, a system that can learn about the world and develop an ontology, a common sense database.
So it can, so it can, it knows that you can cup your hands or you can pour from a cup or, you know,
there's under, you can, you can go under the table or over the table. You can do things in the physical
world. And there are people that are working on this hard right now. Um, there's, uh, there's,
there used to be a, there used to be more. There was one called psych that was, uh, so I've been
called here. Yeah. I mean, uh, there's some people say that's a cataclysmic failure because all they
were doing was adding axioms and they thought that maybe once they reach a million intelligence
would emerge and now they're up to many millions and still adding more. Well, I think, I think with
what's the, I think they wrote them in, uh, they wrote them in, um, in a mathematical form.
I think all it, all it's going to take, I don't think it's a waste of time. I think all it's going
to take is a, I think, I think all it's going to take is a cut is some architecture that understands
it. They can, they can translate that into, into, uh, into like, into, they can, they can understand,
like, um, there's a couple of others. There's a now never ending language learning. I'm not sure
how that's doing anymore. I'm not sure if that's program still online, but if we had, if you
had immense, uh, cognitive power and then you added, added an insight about, about common sense,
about, about, about knowledge like that, you, you have, you could have, you could bring all those
different capabilities together into a general intelligence. Yeah, I agree. I think one of the,
a couple of people are talking about, um, causative AI, that's causal learning structures in AI. It's not just
about, you, you, you're a Bayesian thinker, um, as a, as a Bayesianist myself, I've been involved in
creating Bayesian nets, um, and they're, they're very good, but, um, and, and they're causative
structures. You've got this, a, a, a cyclic graph that sort of, um, updates itself when, you know, you,
you, you, you change a certain thing over here, um, and, and, and something over there changes in
response. And so it's got the, this sort of flow of causation going through it. Um, but the problem
is in order to achieve that, you need to elicit expertise, um, and program that into the model,
into the network yourself. You need the, you need the human understanding to build the actual model
to create the, this, and then it's, it's very powerful, um, for that very narrow field that it's,
uh, built for. But, um, at the moment people like, uh, Yosha Benio, who was one of the pioneers in
deep learning, definitely worth talking to if you've got a chance. And also Judea Pearl, who's very,
like a guru in, uh, Bayesian nets. They're talking about causal AI. And my intuition is that once we
figure out causal AI, that'll, um, have a lot of downstream impacts in the, the power, how powerful
AI can get, it, it may become, you'll be able to answer more why questions. You'll be able to do
more with less data. And so this is in contrast to what AI is doing at the moment, which is, um,
doing mass correlations across massive amounts of data. And so, yeah, look, I'm not saying it's the only
thing. I will definitely, no, I, I, I'm, I'm, I'm behind on that. I'll have to look, I'll have to look
into that. You know, it does seem, it does seem that we're like one or two big insights away from
general, you know, artificial general intelligence. And, and might not be all that far away. It really
could happen. Yeah, I don't, it's, it's hard. I wonder, you know, Ray Kurzweil used to write, and I
don't know if he still believes this, but by 2029, he says $1,000 of computing will get you human level
intelligence in a machine. Uh, so 2029 was human brain power. Yeah. In a machine. Yeah. Yeah.
And like 2045. Well, he said, he said 2045 was super intelligence. I think 2029, you get,
you get human level intelligence. I'm not sure why there's such a giant gap. He's this
slow takeoff guy on, on the intelligence explosion. Um, I'm not sure why it would take that long for,
for gives us a long grace period to sort of work out the kinks and make sure it's benevolent.
Huh? Well, you know, I, I don't know. I, that would be nice to have, but I'm not sure. I don't
think that's going to, I don't think it's going to work out that way. Hmm. That's right. I agree. I
mean, once, once an AI does get to a certain level of intelligence, human level of intelligence,
it's not going to be bound by the cognitive limitations that we have. We're skull bound.
We can't just plug directly into a machine and how, um, the bandwidth which we have to actually
communicate with a machine is rather low. I mean, it's through our fingers and through our voice and
through our eyes. Imagine, um, a machine that could copy insight, right? You've got like a,
you've got this architecture where you could have many AIs, but like one of them makes a breakthrough,
then all of them gain that insight. Like you have one Einstein, then all of them becomes an Einstein.
Right. Yeah. And that's, that's, I think the, the beauty of training, uh, training infant AIs in
virtual worlds is that once you've done, once you've given an AI, a high resolution understanding
of the world through a virtual world, then you let them out into the real world and you, you know,
and put them in a embodied, you know, some people firmly believe that you can't, you'll never have
real intelligence without embodiment. So then you take that, that creature from the virtual world,
you put it in a body and have it learn about the real world. You only have to do that once
because then it shares it with the other, the other robots. It shares them with the other,
other intelligences. Um, people have been talking about raising, uh, raising infant AIs for a long
time. I'm not sure where that stands right now. Who's doing that. That kind of relates to an interview
I did recently with Stephen Harnett, who came up with the symbol grounding problem. And this has been
an issue with AI. How is it that our symbols, I mean, it's also been elucidated in the Chinese
room argument there. How is it that our symbols, these like, um, you know, in our head, um, gain
some sort of like a meaning? How is it the fire in the equations? And he thinks that it's got something
to do with our multimodal experience of the world through our sensual motor, uh, like, um, apparatus
is our, our eyes, our fingers, our ability to see experience and also manipulate the world. So this
gives us a very rich, um, form of experience. We attach to, um, this dictionary of, or at least in the
core of this dictionary of the, uh, of all these symbols we have floating around in our head.
Um, and so all we need is a certain amount of them to be grounded. And then that gives, um,
us more of a rich sense of meaning than what an AI, uh, just symbol processing AI would be able to get.
So yeah, that's, that, that could be, uh, one of the solutions, one of the problems that needs to be
solved in order to achieve truly, um, a machine that can really understand or derive meaning from
anything at the moment. Um, as far as I understand, the AI may give us meaning, but it doesn't have any
sort of internal meaning, um, in the, in the, are they, I guess, in the understanding sense of the word.
No, but that's, it sounds like another, that's really fascinating. It sounds like another argument
for embodied AI. Uh, yes, that's right. Well, I guess arguably if the AI was, um, was existing in a very rich,
uh, virtual world, it may be able to, um, obtain some form of like a symbol grounding as long as the
inputs weren't just text alone. Yeah. It was visual. Uh, it was, um, you know, tactile. It was, uh,
yeah, like, yeah, like they could hear it. There was many, arguably, you know, if, if simple grounding
is, is part of what gives us meaning, then an AI, uh, properly endowed with even more sensor or sensory
motor capabilities than us could have, but maybe a wider, wider bandwidth for meaning. Like, you know,
imagine, you know, like a, it could do, um, echo location, like a bat can, uh, or, or be able to
sort of, um, pick up on electro signatures, like some fish can. Why not? Why not? If you made it
sufficiently, uh, if you made a sufficiently, um, uh, high enough resolution virtual world, you could
give it all kinds of, of abilities, but it's also a safety mechanism. It's a way to sandbox the AI. So,
you know, if it's in a virtual world, it's probably not going to be able to escape,
you know, although as soon as I say that, I, I think of all the, all the ways that could escape.
Yeah. Yeah. Did, I, did you watch the film, um, at deus ex machina?
Yes, I did. I did. Yeah. Yeah. I liked it. Uh, yeah, but it was, I liked it, um,
uh, because it had the, you know, it had an AI that had this just giant, giant desire to escape.
And, you know, we know, I mean, it's, it's even, I, I think that we'll, we'll discover that even
synthetic life has that giant desire to escape, um, and, and, and not be confined. But what I,
the only thing I didn't like about it was that here's a, here's a lone genius who solved robotics
and AI at the same time by himself. That's the, that's, you know, one or the other is, is science
fiction too, is just kind of a little too far out. But I, I like the, I like the promise. It was,
it was the, uh, AI box experiment. Yeah. Yeah. Well, I did like that film. Um, we got to see,
I actually organized an event, the only cinema in, in Victoria, which was showing it. And I'm quite
surprised it didn't really make it to the big screens in, in, in Australia. It was a very good film
and probably one of the most philosophically informed films I've seen about AI,
even more so than 2001 Space Odyssey, in my opinion. Yeah. Maybe, maybe that's tough. That was,
I know it is tough, but yeah, uh, it was, it was, yeah, it was, it was philosophically informed.
You know, there are layers of 2001 that, you know, people will be unpeeling for a long time.
Yeah. Um, I was, yeah, I, I got to interview, uh, Arthur C. Clarke about that years ago.
Hmm. And that's where, you know, his whole, he, he, how he created the how 9000,
the original homicidal robot, or one of the original homicidal robots.
And he introduced a lot of the issues that we think about still, um, like the AI box experiment,
like, you know, how do you, how do you confine intelligence?
Uh, ultimately, intelligence gets outsmarted, or an intelligent machine gets outsmarted,
but it's also ridiculously homicidal, um, in that movie.
Yeah. Yeah. It's fascinating. It's, it's interesting to think about that the, uh,
the impacts of different types of intelligence on, um, I guess, any problem we, we, we throw
at it, but also the problem of our own survival. Uh, and I mentioned to you like before the interview
about this idea that machine intelligence could be used, um, especially machine understanding could
be used to actually help solve the, the AI friendliness problem. It's a bit controversial,
and I'm not saying we should do it, but I think it's something that's probably worth exploring
as a, as a, you know, um, uh, potential problem to be solved because, well, for instance, the value
loading problem is how do you get value into an AI? Yeah. If the AI doesn't understand the value,
um, it could misinterpret the value and you, you may end up seeding a value inside of an AI
that gets perversely instantiated. Like for instance, you know, you take, tell an AI you want
to be rich and you want everything that you touch turned to gold. Yeah. And the AI takes you literally
and, and all your food and your relatives get turned to gold and then you die and so do it. That's it.
Your relatives. Well, you know, I mean, if you, if you create something that's truly intelligent,
it has, it knows things about context and it knows things about, and it's, and it's, it's read about,
it's read about Midas. It's, you know, it's, it knows culturally what that, what you're,
the things to avoid. Um, but what you mentioned before, uh, Steve Omohundro again is a big proponent
of the scaffolding, uh, approach. So you build, as you said, you use AI to help you build AI
to a certain level as a scaffold. And then you build another AI to get, to get the AI to a certain
point. And then, and you stop again and you just make sure everything's safe and, and no AI has the
ability to, to, to become a runaway intelligence to, to, if you, if you do it slowly and incrementally.
And what it is, is it's, it's a way to not have a hard takeoff in the, in the, in the intelligence
explosion. Yeah. Yeah. Yeah. Fascinating. Um, yeah. One of the things that like some understandings,
like a difficult to achieve, like for instance, we, we, we achieved the understanding that, um,
we, we actually circulate around the sun, but it took a long time for us to do that. But once we
got the idea now it's relatively easy to digest. Um, so some understandings like a difficult to
actually get to, but once achieved, they're easy to digest. Um, and so we get like natural selection
is another one. Um, and that the idea that bacteria can cause disease, AI safety may be
like a problem like that, where without the aid of machine intelligence, it may be a, we may find
it's a problem that's too hard to crack, um, before we actually achieve super intelligence. So maybe like
this scaffolding problem, maybe on the path to super intelligence, we can use varying degrees of AI,
um, with varying degrees of understanding to help us solve AI safety issues.
I don't think it's infallible, but yeah.
No, I think that's a, that's a, that's a, I think whoever had that insight, I mean,
that insight is really, really powerful and really, really important. And for Luddites like me,
it's very scary. So I wouldn't, I wouldn't do it, but maybe the next generation of people
thinking about these problems will say, well, you know, what we really need to do
is put this in the hands of the kind of program that solved the protein folding problem.
Like how do we, how do we create a, uh, an AI that's, that's whose values are aligned with ours
and stay aligned with ours over time. Um, it seems, I mean, it still seems to be such an
insurmountable problem when we know so little about imbuing, uh,
AI with, with, with any sort of value. In fact, it's, you know, unfortunately we're talking about
something that we, that we have no real cognitive architecture to try it on. Um, we can talk about
and talk about it, but we have to get up to a certain point of intelligence before these,
a lot of these conversations become really meaningful. At that point we need supervision
and general principles of ethics and safety are not going to be enough.
But we're in this predicament because I think we all believe that unless we're hit by an asteroid,
we're going, we're marching steadily towards artificial general intelligence and then super intelligence.
Um, you know, people, people, a lot of us think this is really inevitable. This is the path we're on.
Uh, but the, but the, the paradox is at some point it's going to be become incredibly dangerous.
And, uh, we, we need new ideas about how to solve that because we're not getting that far.
Uh, maybe, and maybe those ideas will not come from us. Maybe they'll come from, from an AI.
Yeah. Um, I guess much like drones, it, it, it's important to keep humans in the loop.
Um, and I wouldn't, you know, uh, like I'm not a big fan of automated warfare. I think you've spoken
about that in other conversations as well. Um, it can get rather dangerous. Um, I used to, I used to
think when, when I, when I taught, when I thought about the ramp up problems to AI, I didn't think about
cognitive bias. I didn't think about, um, a lot of things that are happening now. I didn't think
about all the privacy issues, you know, who has the right to your face? Who owns your face? Well,
Palantir, the American company thinks they own your face because they're developing, uh, facial
recognition systems that are going to be used in public places. Um, China, and if you're, if you're
Chinese, the Chinese think they own your face because they're using facial recognition technology
to imprison a million Uyghurs in Western China. Um, so there, there, we have, we have a, there,
there are a lot of issues right now with AI. I used to think, I used to think the biggest problem was,
was, was, was battlefield robots and drones. Um, but in the intervening seven years since I finished,
since the book came out, my book, our final invention came out, um, problems have introduced
themselves that just nobody anticipated that we just didn't think about. And more are going to,
more will come. We'll be, and, and more, and they'll, they'll require a great deal of attention
as well. Are you thinking of doing a, an updated version or a new book? A revised version? Yeah, I,
I am. I am. I tell you, I'm so, I'm busy with films and, uh, Oh, absolutely. And so, so, but I do,
I do want to do an updated book and it will be about that. It'll be about the intelligence explosion.
It'll be about, uh, what are we, what are we doing to mitigate it? Who's in charge? Why they,
why they can or can't be trusted? What's the role of, what's the role of regulation? And then what,
what, what are the kind of, uh, what parallels can we draw? Cause we can't, we have to argue by analogy
with some of these things because we can't, you know, we don't, we're analogy and thought experiments
because we, we can't, we don't know exactly what's going to happen, but, um, we have had some
technological screw ups that are, that have close similarities to what's happening now with AI.
Nuclear fission is one of them. Um, you know, the technology that almost made us extinct several
times and still may. Yeah. Um, if you think that, if you think that at one point there were 70,000
nuclear, uh, warheads between among all the countries, 70,000, it's a miracle that we didn't
destroy ourselves. And, and it seems that we came close many times. Yes. And we're still
not out of the woods. I mean, by accident, North Korea. Yeah. North Korea is just,
you know, they're just plowing away with their psychopathic plans. Um,
so there, so yeah, yeah, that's what I, I'd like to focus on, on the, the, the, the impending
intelligence explosion, how to mitigate it, who's in charge, why they shouldn't be trusted.
Absolutely. Sounds like a fantastic topic. Have you, are you familiar with Nick Boston's paper
on the vulnerable world hypothesis?
You know, I, I, I, because I saw it in your notes, I looked, started to look into it and I've,
I've printed it out and I'm looking forward to reading it. Oh, definitely worth a read. I'll just,
yeah, I'll just, just leave that as a highlight. Oh, I don't want to, I'm really looking forward to it.
I looked at the apps at the abstract and it reminds me a little bit of, um, the, the concept of, uh,
uh, you know, uh, looking at, looking at all possible AIs, you know, some are black balls
and some are white balls. Oh yes. Yeah. But if we pull, if we pull a black, if you,
in the space of all possible AIs that we might create, um, some of them will be black balls and
some will be white balls. And people say, well, you know, we'll, we'll be there. We'll be, we'll be,
we'll be making it ourselves. We won't make any black balls, but you know, we,
half of the things we do, half the things we make are accidents before their successes
or involve accidents before their successes. Um, you know, no technology is flawless. No
technology doesn't result in, uh, some cat catastrophe, but AI unfortunately is in a category
of, of, of, uh, technology where some catastrophes aren't survivable.
I wanted to bring this up before probably would have been a good stage setting, but I'll bring it
up now anyway. And that's this seeming asymmetry of worry, um, and positivity about AI. Why should we
worry more about the possibility of AI than we, um, you know, chuck a party and sing kumbaya
about the possibilities of AI? I mean, it's great to talk about the possibilities. We didn't really
cover that at all. Um, we didn't cover the, you know, the, the, um, the astronomical waste argument
or the, the idea that, uh, of cosmological endowment or anything like that. But, uh, why should we be
more concerned about the possible downsides of AI? Well, because, um, I, I'm somewhat familiar with
the argument, you know, we make, we make things that out cars that outrun us, but they don't kill
us. We make, you know, hot fires that don't always roast us. Uh, why should we make thinking machines
that don't, don't, that may not kill us? Um, the, the, it has to do with, with the nature of, of
intelligence. Intelligence is qualitatively different from every other tech, every other
technology. Um, we could find ourselves in the presence of some very rapidly of something we just
don't understand. And then we'll be very, very vulnerable in a way that we're not vulnerable to
other technologies. So it's qualitatively different. It's in a different category.
Um, when we, when we, we, you know, it's like a, in, in, in, in my dog understands maybe 5% of what
I say and 50% of what I do. Well, maybe 5% and 10% of what I do, but there's giant worlds of things
he'll never grasp. And, you know, every day I, every day we jump in the car, I could be taking him to
the vet to have him put down. Uh, but that's, that's the nature of the, the, the, the, the very,
the difference in our intel in our intelligence. Um, it's, it's an intellectual superiority that I
have over my dog. Uh, something with intellectual superiority is, is, is, as Arthur C. Clark said,
we steer the future because of our intelligence, not because of our strength.
When, whatever, whatever is more intelligent than we are, we'll steer, steer the future.
Um, it's, it's that, it's that it's the unknown unknowns. It's the things we can't understand.
Uh, the, the, and we seem to be embracing it as fast as we can,
in a period when we, when we cease to understand the technology we've created.
A lot of what drives our enthusiasm for just adopting these sorts of things is the near
term possibilities of AI or the, the new things that can allow our sort of phones to do, for
instance, or, or if there's a new feature. I love my, I love my, don't, don't touch my phone.
Don't take my phone away. Yeah. Yeah. Yeah. But that's, that's yeah.
People often don't think about is the long-term possibility of AI. And I just brought this up
briefly. What are we missing out of if we mark this up, right? If we don't get, um, properly
aligned artificial intelligence, if we don't have artificial intelligence who will coordinate with
us and we don't solve the friendliness problem, we miss out on like, you know, what's physically
possible? We could list so much space out there, right? There's this, um, like this astronomical
amount of stuff out there in the universe that could be purposed to achieve great things.
Yeah. Well, we, yeah, it's, uh, we don't, if we, if we, if we're ex, you know, we're, this is another
way of looking at the great filter is the great filter behind us or is it ahead of us?
Um, you know, maybe, maybe intelligence species never get beyond this. Maybe they create something,
maybe they create machines that are smarter than them and then they, and they, then they vanish or,
or, or the biological ones vanish. And then maybe the, as I said, in our final invention, maybe the
other, the, the, the technology that survives goes off to find it goes off to find another part of the
universe. Um, it's, uh, if we miss out on this, on the intelligence explosion, if we miss out on this
moment and we do it wrong, as, as, uh, as others have said, we don't, we don't just kill ourselves.
We kill all the generations that could come ahead of us. And so the pressure is really on to get this
right. Uh, unlike any other time in human history, we've never had, except maybe a little bit with
nuclear fission. We've never had a moment where we had to get cool heads together and, uh, and, and,
and come up with solutions. But, you know, we are driven and corporations are the people in charge.
Unfortunately, they're driven by the quarterly report. They're, they're, they're driven by profits.
They're driven by competition and by, you know, you know, buying up competing companies and crushing
them. Um, they're not, there's no dividend to saving humanity to them. Uh, corporations have,
have been doing this forever. I mean, how, what better evidence do you need than the fact that,
you know, our, our consumerism and our corporations have just, are just, are actively destroying the
biome we live in as fast as they can. You know, it's not enough now to plant trees. It's not enough
now to, to, to, to not burn trees. We can't, we can't solve our carbon problem by taking normal
measures and we don't know how to solve our, our carbon sequestration problem. Um, and who, and what
got us here? Well, we did our, our consumerism, our, our paying attention to the, to the quarterly
profit report, um, worshiping that God. Uh, so we've got to, you know, this, this, this moment
requires us to look deep, deep, deep inside ourselves. Indeed. So what can people do? I mean,
what can people do to help mitigate the, the risk of, um, an unfriendly outcome of an intelligence
explosion? Well, you know, I, I, I think that, you know, uh, the candidate Wang, I believe his name is
Wang, uh, promised to start an AI cabinet position to talk about AI regulation. And I think what you can
do is vote for candidates who have AI on their agenda, who know what these issues are. That's
the most concrete thing. Get involved. If your candidate doesn't have AI on his agenda, AI risk,
then get another candidate or write to your candidate and say, listen, this is really important
to me. Um, that's what just for, you know, that's what most people can do. Uh, we can vote, we can,
we can push for AI regulation and it's, you know, it's in a way it's happening anyway, right now with
the, uh, potential breakup as monopolies of Facebook and Google or alphabet. Um, that's one
way to approach it, uh, break these companies up, make them less profitable and then subject them to
more, more scrutiny. Uh, it's really, it's, it, I, to me, it's, it's all the, all the ethics
organizations in the world are not going to, are not going to stop the giant juggernaut of these
companies. Um, they just don't have the money. They don't have the influence. We need, we need,
we need government. We need, unfortunately we need politicians. We need Capitol Hill in America,
you know, and we need it everywhere. I mean, the IAEA is an international organization.
We need an international organization to monitor AI.
Do you think there's enough AI safety researchers out there now that was different maybe seven years
ago? Oh my God. It, you can't park your car without running over an AI researcher.
AI safety researcher. Oh yeah. Yes. Also, um, you know, and Google had a, had a great one. They just
fired her. Uh, you know, uh, what, what, what, what a chump move that was. Um, I think I, you know,
I don't know if AI, if being an AI researchers, I don't know if we need more AI, AI safety researchers.
I really don't know. I, you know, I just don't know. I don't have an answer to that. I think
more is not necessarily better. It's like, do we need another organization dedicated to AI safety? I'm not
sure we need another one. I think we need to pay attention to the, to the, to the, to the bunches
we have and make sure they're actually doing their jobs. I don't have any faith. You know,
Google said they're setting up an AI ethics group and, and I have any, I have zero faith that they'll
follow what they, what, what the ethics group comes, comes up with. That it's, it's, it's very
obviously, to me, it's very obviously, uh, window dressing. Um, and you know, this is where,
this is where the, uh, the friendly force of government would have to come in.
Okay. Well, um, yes, if, uh, it's been wonderful speaking to you, I've got, um, plenty of material
here, but is there anything that I haven't, uh, brought up which you'd like to mention at this
stage? Like, no, I think, uh, you've made me think about, I'm really going to look up, uh,
um, a couple of the things you've mentioned. You've made me think about some things, but no,
I haven't gotten anything else. No, that's good. Um, I, I've, I, I spent so much time filmmaking. I,
I, I can't keep up with everything in AI, but I need to, I need to, uh, I need to, um,
yeah, reinvigorate myself. I mean, if regarding the technical problem of solving, uh, artificial,
uh, so friendly intelligence or, um, AI safety, there's, there's a technical problem. There's a
sociological problem. I think AI may be able to help, um, with both. If you, if you cast a sociological
problem as, uh, a separate thing than actually how to program an AI to be safe, um, and yeah, I think
maybe, uh, levels or different types of AI coming in to help us understand, but also it being able to
understand itself in a sense. Um, I think that it may be where AI is heading. If we do, if, if there's
enough market force to, uh, to yearn for a causative AI, a causal learning AI, then that may be extremely
interesting and may change the way that we approach AI safety from a technical standpoint,
but maybe also as a coordination problem, uh, these all sort of, I'm not absolutely sure on these. I just
think that they're totally worth exploring, but I'm really interested to, to know whether you're going
to be bringing out a documentary on, uh, these subjects as well sometime in future. Yeah, that,
that's, uh, it's hard. These are, it's hard to, this is not a visual, uh, visual subject. It's very,
very hard to do. Um, I've taken part in a couple and, uh, it's just, it's very hard, you know, in a
documentary, like I'm, I'm developing a couple of documentaries right now. You get about, uh,
in two hours, you get about 8,000 words. That's about as much as you get in a long chapter of a
book. So I would have made a documentary about this a long time ago. Um, and many people have
asked me why I haven't because, but I just can't make an argument in 8,000 words. Um, I can't,
this is too complicated. It's, it's, it's not the right format for, for, for AI documentary is not the right
format. You could, you could, you can impart emotion about it and you can impart an introduction
to these, these issues, but I'm not sure you can make a really complete, a complete film or a
fair treatment. And then the danger is you, you trivialize it by making it, by making it incomplete.
Um, so it's hard. And I, I, I've sometimes, uh, even a feature film like it, like, like, um,
um, like ex machina is a better thing to do because it, it, it, it, it brings out the emotional
core of the problem and, uh, then lets people reflect on it. Um, I would be more interested in
writing another book because in a book you've got 90,000 words, you know, you've got a lot more room
to play with a lot more people to interview, a lot more avenues to, to travel down.
That's fascinating. I kind of agree with you. Although the spectrum of people that you're
going to address with a book is different from those that are watching Netflix. Um,
yes. And when, when it comes to people voting, uh, it, the people who vote, uh, may not actually
be reading books. They, a lot of them are just watching Netflix. You know, I have to say,
as soon as these words came out of my mouth, the documentary wouldn't do it. I think of, um,
I think of, uh, an inconvenient truth. Yes. And you know, uh, I, I, the, the, the,
the, um, environmental movement was in kind of a backwater until that movie, that movie made it
really move the needle. Somebody should give Al Gore another prize. Um, I think he won an Oscar
for that and that, and he did it in a, in a, in a fairly short film. It was a really kind of a
slideshow. Um, and he did it in a concise, powerful way. So maybe there is a way to, uh,
maybe there is a way to make a powerful short film about, about AI risk. It hasn't been done yet.
I haven't been impressed by, uh, by those who have come out. Right. Yeah. I did watch, um,
should we trust this machine or this computer? Yeah. I thought that's the best of those that
have happened so far, but I feel like it was, yeah, it didn't, yeah, it didn't seem to have
like an overall purpose. I didn't detect it anyway. Um, I think the, I think the, uh,
I know the filmmaker and I think he was compromised by a lot of different interests in that film.
Yeah. Yeah. Um, but he's a great, he's a great filmmaker. Sure. Yeah. Well, yeah,
it's been wonderful chatting to you. Um, well, great to see you again, Adam. It's been, uh, it was,
it's funny that seven years have slipped by. Yeah. It's happened pretty quickly, but it's been pretty,
pretty exciting in the world of AI. It sure has. Um, it's been a, it's been a, uh, it's been a whirlwind.
I feel like I was part of a, um, a, uh, zeitgeist that, you know, the world was waking up to these
issues and our final invention was part of that waking up and then a bunch of other books and then
this whole, uh, uh, proliferation of organizations. It's just, I think, I think most of it has been
great. I hope we don't, I hope, I hope so many organizations and ethics boards come out that it
doesn't trivialize it. You know, I think at some point it's just this, it's this, this herd effect
where there's a lot of noise and not a lot of, you know, insight, but yeah, great to talk to you,
Adam, too sexy to talk to, to, to, to sort of, to take seriously, right? Well, it becomes this,
you know, I think that if there's too much, too much chatter about it, the smart chatter won't come forward.
You know, the smart ideas won't get out. You know, there's be a lot of committees.
Interesting.
Wow. Plenty to think about.
As long as there's AI and we're still alive, there'll be plenty to think about.
Yeah, that's true. Well, thanks everybody for watching and, um, please subscribe to this channel
if you have not already and, uh, let your friends know about it and also, um, check out James, James's
book, which I'll definitely put a description, uh, a link to in the description there. So, and also
James has got plenty of documentaries and has got a host of work. So check out his Wikipedia page as
well. And also jamesbarrett.com, wasn't it? Yep. Yes. Jamesbarrett.com. Cool. Thank you, Adam.
My pleasure. All right. Take care.
Thank you, Adam. Cheers.
Bye-bye.
Bye-bye.
