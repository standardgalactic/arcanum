Thanks for the context, that's spot on.
So a bit more on that background.
So this was written while I was working at Google.
And by the time it got published, I was no longer at Google.
So that's why it shows up independent researcher.
So to me, this came about, this interest in both collective intelligence and active inference
came about while working in organizational effectiveness.
So basically trying to get the great big collective of engineers, product managers, finance people
and so on, that is Alphabet, to work more as an intelligent entity.
And in being a mathematician, I, of course, I decided to approach it in a mathematical
modeling fashion.
And that also coincided with me running into active inference.
And on the one hand, it fit really well with my other interests and backgrounds and Bayesian
modeling and so on.
But it was also, I would say it was a pretty early days in terms of active inference modeling
and in code.
So you're going to, if you dig into the paper, you're going to see it's that style of modeling
is pretty outdated.
So myself, I'll do this in a quite different way.
And for discussion today, I'll skip over some of the details that today I would try to do
in different ways.
But I think the results still hold and the message is still pretty, pretty spot on.
And as Pranav mentioned, it has led us, the three of us in pretty different directions,
which are also pretty, I think we're going to be pretty interesting to you guys.
So yeah, the main motivation in a nutshell is really that formal models of collective intelligence
have not been well formalized in terms of the relationship between the local scale interactions
between the agents that make up the collective and the behavior of the collective itself at
that higher state, higher level state space in which it's defined, in which we would expect
to observe intelligent behavior.
And the goal of bringing active inference in is that it's a pretty natural lingua franca
with respect to which we can interpret these behaviors and basically measure how, what's
going on in terms of both free energy reduction based in terms of their base and mechanics.
And, and yet to, in order to, to achieve that, we had to, to look at augmenting our, our agent
models with some say non-standard features.
So the, the, the most interesting really being that this, the particular way in which we, we
think about alterity about modeling the other, an agent modeling the other's behavior, which
is to, is quite particular to our particular interest in modeling collectives made up of,
of agents that are highly sophisticated where, where, where you expect, expect them to, to have,
to have, to have a theory of mind that refers to the, the, the, the, the, the, the, the, the
others behave, the alter's behavior in the, in the world and for these things to match
up and, and to also have like a self-referential beliefs and so on.
And the other part of this is really the, the concept of, of goal alignment, which is, is pretty
commonplace in, in, in collective intelligence and, and related literature, but it's, it's
kind of the, the, the way in which goals show up in most active inference modeling is, is
either exogenous or I would say top-down and here represent some avenues towards really endogenous
goal alignment in a specific sense.
And so what we, what we really wanted to do was to, to explore the effects of, of providing
active inference agents with specific cognitive capabilities.
So we, we hypothesize separately that theory of mind and goal alignment, these two capabilities
would, would, would show, would result in a, in a better performance of the collective
and, and also that their combination would be, would be even better.
And as you're going to see, it turns out that really this combination is, is crucial because
each of these two capabilities in and of itself, even in a very simple, very simplified world
model, they can, they have limitations that, that in cases are very hard to, to, to breach
having to do with self-preferentiality, for instance.
And what I wanted to, to say is also that this is an exploratory paper and we haven't really
put a lot of, we haven't really put any effort into, into turning it into a full-blown theory.
So, uh, take it, uh, with that, with that grain of salt, you're going to see a lot of like
heuristic results, but we do think that it's pretty, pretty, uh, illustrative.
So, uh, I'm just going to very quickly go through the, the, the, the model itself and the results
and pass it on to, to, um, to Jacob and, uh, and Brunov.
So, uh, if you've, if you were looking at active inference literature in, uh, around the end
of last decade, you might have run into this paper from McGregor at all.
It's not from the Fristen camp.
So the formalism is a bit different, uh, but we thought it was really nice for the, the,
the, uh, uh, formal clarity and the simplicity of the, of the framing.
So this is, uh, two agents that have one, uh, one common target position, which is the
shared target here in the bottom.
And, uh, they each have, uh, individual target positions.
This, this setup is so that we can explore this concept of goal alignment of an agent being
willing to, to, uh, preferentially pursue the shared goal as opposed to its own particular
goal, which might be easier and, uh, this, this setup is, uh, actually our, our simulation
was with 60 cells instead of 20, but it's this pretty much the, the, the, the general setup
and the positions of, of targets are also, uh, randomized between runs and so on.
But really the, the, the basic concept is that there is a kind of a chemical signal that corresponds
to this, this color, this color, this color, or this shade, uh, here, and that is the only,
uh, the main sensoria that our agent has.
We also posit that the, uh, our agent, therefore, it doesn't know its absolute position.
Uh, it only knows it's, uh, it's, uh, it's kind of like relative position.
So it understands the, the, the, the geometry of the, of the space where it's in, but it doesn't
know exactly where it is.
It doesn't know the, the, the position where it is, only the signals that it can pick up,
which is imperfect, imperfect, and it can also observe its, uh, its peers, uh, delta.
It's, it's, it's a distance from itself.
It's, uh, uh, signed distance from itself.
So that setup is sufficient to, to create some pretty, uh, pretty compelling behavior.
Um, what, uh, what we ended up having here is, uh, that as I, as I mentioned,
we implement these, uh, go straight to, to, to results, uh, we, we implement these, um, capabilities
of theory of mind and, uh, and goal alignment also in pretty straightforward ways.
The, the, this is the baseline.
The illustration here, A is for the baseline agent where essentially each, uh, agent has,
uh, just one, uh, internal belief state, uh, and one goal, one target state and, uh, or target
distribution that would be more accurate to say, to, to say, and just performs a very standard,
uh, very simple active inference loop in, in this one without any planning, without any kind
of, uh, uh, for prediction, just, uh, your one step forward, uh, action.
The, uh, setup B here is, uh, is with, um, theory of mind where, uh, our agents, uh, in addition to
having the self actualization loop, uh, from normal active inference, they actually have also a partner
actualization loop, which assumes that the partner wants the same, uh, the same sorts of things as,
as they, and have the same genetic model, uh, of the world as they do. And therefore they try to
infer what the partner is, is going to do as well. And so there's this, uh, this nice symmetry that
really, that really helps the agents extract information from the partner's, uh, actions.
And, uh, in addition to that, the, the, this is just for simplicity, just illustrating without the
theory of mind, but here there's, uh, the, just the assumption of, uh, with goal alignment,
it's just the assumption that the agent is pursuing some kind of either the, just the shared goal or
some combination of the, uh, some weighted combination of either the shared goal or the,
or the, uh, or their own particular goals. And so with the, with this setup, that's the,
the combination of the two with theory of mind angle alignment. Um, and, uh, here's, uh, uh,
one example run of, of what the, this, uh, this looks like. There's a, the setup is that
there's a strong agent that is very capable and just, you know, it has very strong, uh, ability to,
to, uh, to track the signals and to find its, uh, its target state. And there's an agent that,
that is pretty weak and whose behavior looks more like a, like a random walk. And it can also track
their, their beliefs, the distribution of their beliefs about the partner, about the partner. So
there's some discussion here about the, the potential, uh, the potential ways in which
these mechanisms interact, for instance, uh, when we endow the weak agent with a stronger theory of
mind, it's, it's better able to, to infer, uh, the, the location of, of its peer, which is what happens
here as the, the weak agent's beliefs about the strong agent in the bottom. They, uh, they start out
pretty, pretty fuzzy, but they get, uh, increasingly sharp here. And as the, basically as the, the strong
agent, uh, finds its preferred position, the weak agents, even though it doesn't really know where
it is, um, uh, it has a pretty strong ability to detect where the partner is. So, uh, the, the main
result is, as I mentioned, uh, before that, for this result in, uh, in really good, uh, individual,
just in terms of, or even of individual performance of distance from their target position, uh, in the
end, when you average this out, uh, you really need, uh, for the, uh, for the weak agents to, to, to get,
uh, anything anywhere close to the strong agent's performance, you actually need a combination of,
uh, theory of mind parameter that is strong enough and a goal alignment parameter that is strong
enough. And, uh, there's this, this interesting aspect of theory of mind, uh, being too strong
is also negative because it can lead to a blind meeting, the blind, uh, effect, especially if
the environment is ambiguous, uh, which can be the case when you have multiple targets as, as, uh, in
this setup and therefore the review observed this kind of like Goldilocks, Goldilocks effect with
respect to, to, to theory of mind, especially as it relates to, to alignment. Um, and, uh, finally,
when, how does this connect to, to collective intelligence? Uh, we, we modeled here the collective
level system free energy as, uh, basically, uh, there was like an illustration here that, that sort of
showed this, uh, that you can imagine, uh, a bunch of identical copies of this same two agent subsystem
that are asked to decode different, uh, pieces of a problem. So like a paralyzed, uh, a paralyzed
inference problem here, we're just looking at, uh, uh, the, the inference, the, the, the, the belief
inference part of active inference, not at the, that the active part, at least from the collective
perspective. So the, the setup here is that, uh, what we define as collective intelligence is the,
uh, the ability of this, uh, setup of, of M, uh, a large number of, of identical copies of this two
agent subsystem to aggregate into an effective, uh, an effective, uh, variation of free energy
minimization, uh, ensemble. And that's what we observed here as well, that, um, the, the system
free energy for, uh, for this collective, which as I mentioned, I think was like 60, uh, 60 peers,
60 pairs of, of peers. It's, uh, its ability to, to reduce free energy is, uh, empirically at least for
this, this, this, uh, study we, we found, uh, okay, but not great, uh, ability to, to get to, to,
to solutions, uh, with just theory of mind, uh, as well as with the baseline agents that don't have
anything, uh, with the, uh, when you introduce goal alignment, uh, you get that a lot better and
sort of intuitive and, but to actually, to get to, to results that, that look like they're going to
result in perfect, uh, in an exact solution to the, to the minimization problem, you really need
the combination of theory of mind and goal alignment. Uh, so this, uh, there's a bunch of
potential, uh, implications of this when I'm going to get into it, because I've already been, uh, uh,
too long-winded. I just wanted to say that, uh, from my, from my side, uh, where I, where I took this
has been, uh, first in direction of, of trying to use, uh, multi-scale active inference to,
to model, uh, the interaction of economic agents and the climate and nature risk, uh, which eventually
led me to, to leave Google and start this, uh, start a startup that, uh, that was focusing on this.
And, uh, more recently I've been, uh, looking at this from the perspective of, of, um,
what I call not, not AI safety, but everything safety. So risk minimization in, in, in collectives
more generally, uh, working right now with setups such as, um, fleets of autonomous vehicles,
uh, their safety, um, also the ability of, of collectives to, to the kinds of interventions
that you might want to do in collectives to, to, uh, minimize, uh, over-harvesting in, uh, fishery
productions or, and other tragedy of the common types, pro type problems. And, uh, more recently
in, uh, in the financial system. So I'm going to, to pause here and, uh, hand it over to, to Pranav
and then Jacob, uh, can, is my screen visible is my presentation. Uh, and I have a whole bunch
of slides, which we will not look at all of it, but if you're interested, here's the link to the
whole thing. Uh, I'll take about 10 to 12 minutes and Jacob, then you can jump in. Uh, so as I was
mentioning, I was, I'm in the business school and I care about thinking in terms of, uh, how does
this come about? Uh, so parallel to this information theoretic approach, uh, and using simulations,
uh, I was pursuing my PhD at Carnegie Mellon at that time under the same topic of collective
intelligence and trying to understand where it comes from. And the theory that we've been building
on and trying to, uh, falsify and validate through empirical data and simulation, multi-agent
simulation approaches was the big idea being that collective intelligence can be thought of
in terms of attention, memory, and reasoning at the collective level, regulating itself.
The better it is at regulating each other, these systems in response to the environmental
threats or the environment that they are in, the more, uh, intelligent we will, uh, end up having
the collective behave. So two big points that we take away is thinking of a systems problem requires
thinking in terms of social solutions and collective intelligence as a way of is a system solutions
that might not be obvious at the individual level. Uh, but how does that come about that one
perceptual shift or a framework shift that would be needed is to think in terms of dynamics or rather
specifically dynamics of emergence, uh, which is that the person is not in the collective, which is in
the physical sense, true, but in the cognitive sense, the person is not in the collective, but the
collective intelligence or the rules of engagement that leads to collective outcome are in the
other person. So these are the two things that I basically will be making a case for.
So in my world, uh, I've been talking to hospitals and software developers and things like that.
Uh, what you are starting to see is most hospitals are in an extremely fast-paced high uncertainty in
them. Right. There is variability in patient admissions. There is case complexities that keeps on
changing. Even if you have benchmark, uh, case complexities as you're diagnosing and as you're doing
procedures, the complexity is, uh, evolving and unfolding as we go. Technology is central. Uh, so
at this point it may not be fully, uh, you know, agentic algorithms doing it, but technology is central
in creating most interactions. And there is a whole bunch of people which are with different
specializations. So in any given hospital, there are about 25 different provider teams,
such as nurses and, you know, the cleaning crew, the doctors, different specialists within doctors,
all of those people, they have to be interacting and coordinating in a fast-paced environment when
the future is unknown. Okay. And this is not just a problem of hospitals. We see that as the pace of
technology has grown, we are entering a place where much of the work is happening in larger teams,
where there are many different specialists and they have to coordinate. So this interdependence,
this fast-pacedness and this dealing with uncertainty, uh, progressively. So is the crisis in the world
of management or organization, right? Software developers, physicians, scientific teams. We have
a whole bunch of evidence of this growing up. So a question that most teams or organizations are now
really pushing for is they're not concerned with their specific performance right now. It is not like
an assembly line. It is about sustaining performance in a changing environment, right? When their workload is
changing in unknown ways, uncertain ways, their knowledge interdependencies are changing, according
to that, as well as you have to maintain and retain members, right? The best people have to be maintained
by organizations or teams, because if they leave, they are also losing a lot of potential. So in short,
in the management world, we've been using the metaphor of teams or organizations or adaptive systems,
but we have really not embraced it. Over here, we cannot run away from that anymore, right? We cannot have
structures that are hierarchical in terms of the org charts that you see, but in terms of dynamic
networks of people interacting. So this is at the simplest level, highest level of abstraction.
It is a systems problem of dealing with fast-paced change as a collective. It requires a system
solutions. Local optimization by imposing hierarchies, which is much stabler over time,
does not work or tend to have significant negative effect. So the idea of collective intelligence,
from that perspective, is the ability of a collective to sustain performance as their
environment or the different tasks are changing over time. This mathematical equation is simply
illustrative. This is not how I calculate anything of those things. Now, back to the point. What is the
mechanism? How does this come about? Now, I'm restricting ourselves only to humans or human-like
things in which you have attention, memory, and reasoning as three cognitive features in people. So the
large setup, we have a dynamic environment, changing levels of workload, knowledge and dependencies,
different members having different goals. There's some form of interaction dynamic response. The big
pull here that we are saying is that there are collective attention, memory, and reasoning systems
which interact with each other, right? So this brings us to the second point, right? Is how do I get my
team to behave with collective intentions? And this is where I lean into the idea of using flocking as an
example. This is a very simplified example, and most of you know this. So I'll jump over it by
highlighting the key frame in which we can think about this, right? I often start my classes and
many of business talks with how are these words coordinating, right? Current management theory up
till very recently would say that this bird must be a genius right up top because that's the leader.
And we know that is not true. We did not know that was not true for a long time. So even top papers
were trying to figure out how is this communication happening until we actually push through the idea
of emergence, where we are starting to think in terms of not there is decentralized control,
there is no centralized control happening, right? There are some rules of interaction. Every individual
agent only has local perception, but they're interacting with their neighbors and neighbors,
not steady neighbors, but neighbors that could be changing themselves. They're interacting with their
local neighbors with specific rules in mind. And if those rules are being followed and well designed,
you have sustained behavior that emerges at the level of the collective, right? Which none of the
individual bird is trying to coordinate centrally or holistically. Again, very amenable to agent-based
modeling. So the insight here is once you think of the rules, the bird is not in the flock, right? Physically,
it is cognitively or from a perspective of collective intelligence, the flocking, which is the rules of
the flocking are in the bird. So how do we pull this into humans, right? The crisis of organizing as
perceived by individual us as people is really a problem of tetris. How do I make sure all of my work
gets done while being a good team? Right? So I want to try and be successful individually as well as the
collective has to thrive and be successful as a, you know, as a whole without negative and externalities
outside. So what are the rules that govern such behavior? In that push, right? The theorization
that we are building, and this is just a high level output of it is that individuals have skills, focus,
and goals, and this could be machine agents also. The way these coordinate, right? How the skills get
coordinated, that is your memory system. How your skills and knowledge gets coordinated is your memory
system. How your focus and time gets coordinated is your attention system, the collective's attention
system. And how the goals and outcomes that people are after, the motivations that they have, get
coordinated is the reasoning system. In a fast changing environment, each member is actually having
a changing set of focus goals and skills. So that dynamic coordination is that. And when all these three
of these systems are regulating each other at the level of the collective, then you would expect
collective emergence, uh, collective intelligence to emerge as a whole, right? The theory that I'll
not dive into, uh, deeply is all of these three systems, the collective memory, attention, and reasoning
are built up on metacognitive processes. That is how our meta memory functions, how our meta attention
functions, and how our meta reasoning functions. Uh, I've already gone over this, the details around this
transactive memory or collective memory has been around and studied well, studied for the past 20 to 25 years,
but attention and reasoning as collectors is something that we are starting to build more on. Ideas have been
floating around, but to coalesce them is the work that has been done. Just as flocking, like the average
size of the flock is an indicator of all the underlying processes working very well, we have emerging patterns
for each of these three systems. If there's a well-coordinated memory system, you will see specialization emerge.
If there is a well-coordinated attention system, you will see a bursty type of activity happening in these
teams, right? There's temporal convergence. There's a quick exchange of emails or quick exchange of work
being done. And then there is a period of silence and then a task comes in quick exchange. So both teams,
and if there's a good reasoning system, you see increasing levels of commitment by members towards the
collective. For those who are interested in systems dynamics things, this is a representation. This is a
multi-agent model. So this is only a representation, but it highlights two big things. There is an
efficiency aspect. That is how are we using our resources, memory and attention resources, and how
are we coordinating that as a collective? And there is a maintenance of the collective itself. It is the
collective tapping into the right opportunities and the right people to get into this. So higher level,
what regulation means here is memory, attention, and reasoning as to regulate itself in response to the
main threat that is in the environment. If the task demands high levels of resolving high levels of
knowledge and independence, you need to have a dominance of coordination that is driven by the
memory system. But as these things go up and down, different types of threats coming at different
levels, the response of the team should be able to regulate at how much decision, how many decisions are
being made by individuals with, you know, with the rules of coordinating by based on memory, attention on
reasoning respectively. We ran this into a simulation model. Again, what this is at the high level saying
is that from a purely efficiency perspective, that is memory and attention perspective, rules that combine
both memory and attention based coordination outperform any other individual rule or an uncoordinated rule.
Once you build in members also have motivations and they can regulate how much effort they are going to
put in into your work, into the work towards the team or the organization. Then even regulating the
reasoning system, right, which is memory, attention and reasoning, all three of them together will give
you the best outcome. So you cannot ignore any one of them is the thing. So again, the big message being
at the individual level, when each member acts in response to others, as a manager, you're working with
your employees, making sure that they're empowered, right, making sure that their skills are being
utilized and making sure that their time is being utilized in the best way possible leads to the best
outcomes. To test this, not the dynamics of it, but even the veracity of whether this idea is worth
pursuing at the high level, aggregate level, we tested this idea using open source data from open source
software teams. Again, I'm not going into the details, happy to chat about them. This is of interest.
The first thing you do is assess what is driving the behavior. So what does the environment look like?
So over a one and a half year period, what it seems like much of the performance or the quality of code
that is being put out by these software teams is dependent on their ability to deal with the level of
workload, the amount of bug queries and new features that they're developing, and not so much on the
complexity of the code itself, right? Like what kinds of different coding languages are required and
what kinds of different modules are being developed. And they have to be able to retain members. So once
we know that the level of workload and membership are the two driving factors that are harsh in this
dynamic environment, from the theory perspective, would expect the attention system and the reasoning
system to dominate the explanation, right? Teams that are able to have good attention and memory
systems will have higher collective intelligence. And that is what we find. So we go in, we operationalize
the burstiness or looking at the attention system, the level of specializations of how members are
contributing to different parts of the code base. And we find that. What is interesting here is,
while this is just an aggregation, once we dig into and look at how is attention the chart on the
right is affecting, you can see a very clear modulation of moderation. In fact, it is only in
the highest 25% of the sample where well developed attention system starts to matter. So this for me
points at the underlying nonlinearities that the effect won't show up till, you know, about 80% of
within the distribution of the sample. But when it when it is needed, it is absolutely without that all
performance stacks. So that's pretty much where I'm at. We've pushed this theorization to, you know,
more theory pieces on talking about the general architecture of collective human machine or social
cognition. And thinking about if we are developing AIs, not for, you know, assistance, but as coaches,
artificial social intelligence, this has been a big research work that we've worked with DARPA for the past
five years on how to develop artificial social intelligence. That's where we are pushing. All right.
I'll stop right here. Thank you. And I will hand it.
