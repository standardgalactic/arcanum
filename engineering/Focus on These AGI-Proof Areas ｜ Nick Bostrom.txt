AI changes the payoff functions of different types of human activities.
Learning, I think, over production, relationship over things, and then capital over labor.
Priest, prostitute, politician. There might be a demand specifically that the service be provided
by a human being as opposed to a machine. You think you want this fundamental novelty,
deep purpose, this world historic sense of meaning. What you actually want is a lot of pleasure,
social engagements, aesthetic experiences. We are seemingly approaching this critical
juncture in human history upon which the next billion years might depend. If you want real
purpose, knock yourself out now because it's never going to be more at stake than it is
right now and in the next few years. What was striking to me was how theological our entire
conversation today has been and how similar to the Christian afterlife. Once you have a super
intelligence explosion, you sort of zip fast forward to technological maturity. Human capital
is a depreciating asset in this world of advancing AI.
When I used GPT-3 for the first time, it filled me first with awe and then with dread because I could
see how close AI was in surpassing me in research, writing, philosophy that I trained so hard for.
If the Copernican revolution took away our special place in the universe and Darwin robbed us of
our special place in nature, then AI threatens to undermine the last pride of the human race,
our intelligence. Today, work gives many of us purpose and meaning, but AI is making more and
more of that work obsolete until one day, perhaps all human activity may be redundant. Nick Bostrom's
Deep Utopia is about that day and what to do about it. In this interview, you're going to learn where
you can invest your time, effort, and resources that is most AI-proof and what the good life looks
like in a world where AI has made you obsolete. My name is Jonathan Bee. I'm a fellow at the Cosmos
Institute researching philosophy and artificial intelligence. If you want to make sense of this
new age of AI we're entering, please subscribe. Without further ado, Nick Bostrom.
AI changes the payoff functions of different types of human activities. Certain activities are going to
be made obsolete a lot sooner and other activities seem to be a lot more robust. And so the three I want
to talk with you about are learning, I think, over production, relationship over things, and then
capital over labor. So by the first one, learning over production, what I was thinking about was in
your book you gave this thought experiment of a technology that can help us learn. And you describe
this as potentially one of the most difficult technologies to create because it requires of
you potentially reading and then changing billions if not trillions of synapses in our brains, right,
to be able to learn a new language or gain the experience of becoming a good father or a good mother.
So my thought is, anyone engaged in intellectual work, your production may be rendered obsolete. Maybe
in just a few years time, we can feed in your book, Deep Utopia. A podcast, an interview AI will
generate a better script than the one that we can have today. And then video AI is able to create
something a lot more beautiful than the conversation we're having. However, if I wanted to-
Even more beautiful.
Yes, even more beautiful. However, if I wanted to understand Professor Bostrom's work, I still have to do
the hard work of sitting there and reading it, it seems, for a much longer time. So that seems to be
one area of life that's going to be a lot more resistant to AI innovation.
Yeah. If you think about what it actually would require through some sort of neurotechnology to
sort of directly download skills. So first you would have to have the ability to read off what your
current synopsis, how they are configured, right? And as you said, you know, millions or billions or
trillions of them. Then you would need to interpret, you know, what all of those synopsis actually encode,
like currently, and then figure out how they would need to be changed in such a way that they now encode
this additional knowledge that you want to download, like without sort of messing up what's already there,
or changing your personality too much. And then you would have to physically change all of these.
So this definitely seems like a task for mature super-intelligence. But until that time, like if you
do want to have some sort of relatively fine-grained control over what goes on inside your brain,
for now, you know, the best method is the traditional one of, you know, reading and thinking and talking
and working around yourself and meditating, etc. I think that's actually quite optimistic,
because it means, at least according to Aristotle, in the beginning of his metaphysics, he says, you
know, all men desire to know that knowledge and the contemplative life is in some sense the most
human life for Aristotle, that that is the one that will be made obsolete at the end.
Yeah, I always find it a little bit funny when philosophers are like thinking, what's the best
and highest form of being a human? And the conclusion is being a philosopher. But, you know, maybe they are
right. Well, I've actually, for a long time, thought of philosophy as something that has a deadline.
In fact, this, I think, occurred to me in my late teens. It seemed to me that at some point, our current
philosophical efforts would be rendered obsolete, either by AIs that could, you know, become super
intelligent or maybe by humans developing cognitive enhancements of various forms that would make
subsequent generations much better at this. And so that rather than spending my time thinking about
the sort of eternal questions of philosophy, it seemed more useful to focus on that subset of the
questions in philosophy, where it might actually matter whether we get the answer now, as opposed to
in a few hundred years. Would you prefer to know the answers to those eternal questions?
Well, one is certainly curious.
But if we do end up in a trajectory where human lifespan becomes extremely long, then maybe rather than
sort of using up all these mysteries right away to immediately know the answers, maybe you would want
to spread it out a little bit so that you have sort of interesting things to learn and discover even,
you know, hundreds of years, thousands of years into the future.
You know, ignorance might become a scarce commodity. Maybe put some in the shelf for like the bottle of
champagne, you know, that's from a specific year that there are only small numbers of left. You might want to
save it for a special occasion. But yeah, there might be some other, I guess,
professions as well that are sort of relatively immune or variables that where it's not just
the specific knowledge and skills we have, but the fact that these things be done by a human is regarded as
significant in its own right. Right. Could you give some examples?
Well, I mean, either a priest, prostitute, politician,
where there might be a demand specifically that the service be provided by a human being,
as opposed to a machine, even if the machine could have all the same functional attributes.
Right. I want to move on to the second type of human activity that is somewhat AI resistant.
And that seems to be relationships over things. And what I mean by that is in your book, you gave a
thought experiment of let's say we're at technological maturity and we have a better version of an AI parent
in every domain. Right. So it's better at changing diapers, better at teaching, better at emotional support.
I still would be willing to bet that most people wouldn't want to completely outsource their role as a parent.
And this could be even more resistant to even technologically mature AI, because what's constitutive
to forming a relationship is that it requires, at least in our current view, two humans.
Yeah, I think relationships is one of the more plausible places where we might find purpose
that survives this transition to technological maturity. There is some value in honoring and
continuing this existing relationship between two people. And that even if you could sort of teleport in
a robotic surrogate that would be sort of functionally superior, it wouldn't be as good. Or maybe it
would be better in some respects, but it would also lose this value of sort of continuing the current
relationship. Even if the robot sort of appeared indistinguishable from the original parent, if you
imagine a thought experiment where nobody would actually notice any difference, you might still
think that the reality is that there is now this different person who is playing the parenting role.
And if you care about sort of truth in relationships, that might already be a disvalue. And so, yeah,
the existing human relationships, to the extent that they sort of partially consist of sort of
intrinsically valuing this connection to a particular other being would potentially be resistant.
What are things in a child's education that are potentially made more obsolete,
given the current wave of innovation in AI?
Well, potentially all of it. But we don't know how long that will take, right? So it makes sense to
hedge your bets a bit. You don't want to find yourself, you know, 18 or 20 going on the labor
market with no skills. And it turns out the whole AI transition has been delayed.
So, yeah, I would like want to make sure to hedge a bet, get the broad basis, some useful stuff,
but then also like have fun while it lasts. I think it would also be a shame to have wasted your childhood.
Perhaps we are too trigger happy in declaring that certain things are made obsolete, given technology.
And for example, so I went through the Chinese education system and it had perhaps the oddity
when compared to the Western education system, an extreme emphasis on math, like rapid arithmetic
computation, including memorization, memorization of the classics. And I think it's tempting to think
that, you know, even with the printing press, you know, memorization is no longer needed,
we have access to all these great books, but certainly with internet and with calculators in our
pockets that we don't need to know these things, even though machines can perform it better.
There's a lot of alpha, I think, to be had in perfecting these skills, because it's not just
that it cultivates your character, it enables you to see things that other people might not.
I would also say general judgment and especially in these kind of turbulent times with all like social
media, so many mimetic dynamics that people are now exposed. So having a kind of robust, I don't know,
cognitive immune system where you can sort of reflect yourself on what makes sense and what doesn't,
and not getting swept up in whatever latest fad or cult or memes that are sort of bombarding you.
I think that certainly is something I also want to see. I want to explore the third and last
potential activity that would be more resistant. Again, this is not in technological maturity,
but just on the way there. And that's capital over labor. Because one anecdote you brought up is how
shocks to the labor market throughout history have changed whether the disproportion of production going
to capital or labor has shifted. And so for example, post-Black Plague, one of the best times to be a
peasant in human history, because so much of the labor force was called that they had a lot more
bargaining or negotiating power. And so I think you hear these stories of them having like holidays
for like half the year or something like that, because they had so much more bargaining and negotiating
power. It seems like AI, again, on the way there, will do the reverse, right? It'll make labor
extremely abundant. And so practically, if I'm someone in law school or someone in medical school
training 10, 20 years before I can start making capital, and I'm building up my labor and building
up a skill, that trade-off suddenly seems a lot less attractive.
Yes, human capital is a depreciating asset in this world of advancing AI.
And so investments with very long payback times, especially if you're doing them only because of
the ultimate payoff of a higher salary 20 years into the future or something, it should be discounted
accordingly. I mean, you would have to have scenarios where AI development takes longer or where it gets
so regulated that they can't perform these particular jobs.
Let me read you a quote from your book.
Keynes predicted that by 2030, accumulated savings and technical progress would increase productivity
relative to his own time between fourfold and eightfold. And as a consequence,
the average working week would decrease to 15 hours. As we approach 2030, the first part of
Keynes prediction is on track to vindication. The second part of Keynes prediction, on the other hand,
would appear to be about to miss its mark. If trends are extrapolated, while it is true that working
hours have declined substantially, we are nowhere near the 15-hour work week that Keynes expected.
Why do you think this is?
Greed has triumphed over sloths. I think the reason is twofold. First, status competition. So we work hard
to afford a whole bunch of things going well beyond our basic material needs. So we don't just want a car.
We want a car that's nicer than the cars everybody else has, et cetera. And so that provides potentially
unlimited demand for more because the bar keeps racing. And then I think the other factor, I think,
is a kind of work ethics that is a fairly ingrained norm that it's sort of virtuous to be not just loafing on the sofa,
but to sort of exert yourself. It's interesting because most people think that we've yet to approach
this sort of post-scarcity world where all of our fundamental needs can just be met like that. But at least in
the first world, you know, North America, Western Europe, that seems to be far gone. Like most people
are able to sustain themselves just fine. And so it seems like it's, maybe it's less so greed that
has triumphed over sloth, but vanity. Because now the reason that people work, I do think it's mostly
this kind of social drive. Rousseau, I think, talks about this in his second discourse where he kind of
flips the view of the direction of scarcity. So in Rousseau's second discourse, in his state of nature,
before civilization is formed, there's actually a state of natural abundance.
Not because the productive capacities are actually increased, but because people don't have these
social vain desires, in Rousseau's perspective, vain desires that civilization has cultivated.
But it's actually in civilization that scarcity increases. Again, not because of the amount of
things that have diminished, but the amount of new desires that have been born.
Yeah, I mean, I think there is a form of scarcity that has been quite pervasive through human history
and prehistory, even aside from civilization. In that there were many things that were really needed
in some sense, but not available to most people or indeed any people say like quality health care. If
you were a hunter gatherer and, you know, you got sick, you know, maybe you could rub some leaf,
you know, but that there are a lot of conditions that couldn't be fixed that way. And occasionally there
would be periods of famine as well, you know, maybe that you get the cold season or something else.
And so I think it's kind of, to some extent, been endemic up until a couple of hundred years ago,
and in many parts of the world until more recently are still today, we've been in a sort of
approximately Malthusian condition. So yes, there has been various advances in productivity and all
kinds of technologies, you know, over thousands of years, but whenever the economy grew by 10%, the
human population also grew by 10% and average income still hovered around subsistence, you know,
with some fluctuations. And I think it was really only in special circumstances, like you mentioned
in the aftermath of the plague, you know, where there had been a great culling.
Or, you know, maybe people discovered some new, came to a new island or a resource where there were
no humans yet, like for a period of time, they could enjoy plenty. And then since the Industrial
Revolution, where economic growth has been so rapid that population growth, although it has been high,
hasn't been able to keep up, that you have been able to sort of have increasing average incomes.
So we talked about AI right now and the current trends we're seeing, but now I want to move
into the heart of your book, which is what you described as deep utopia, full technological maturity.
And as a brief overview for our listeners, this is where AI far exceeds human capacities in almost
all tasks, where we can simulate, right, virtual experiences to be indistinguishable from the
experiences we experience now, where the world is plastic, that anything materially that can be done
within the realms of physics, that we have that technology to do, but also that we ourselves are
plastic, right? So in this deep utopia, you described how, for obvious reasons, work other than the types of
work you described, priest, prostitute, and politician, other than jobs that constitutively require humans,
are already made redundant. But I think the most interesting claim here on redundancy
is that leisure or a lot of human leisure would also be made redundant as well. Why is that?
If you go through those sort of leisure activities one by one, you can for many like cross them out or
at least put a little question mark on top of them, whether they would still have a point in this
condition of technological maturity. So maybe some billionaire goes to the gym, you know, five
times a week because they want to, why? They want to be like fit and healthy and maybe they feel more
relaxed afterwards. But if you had this drug that would induce exactly the same effects without, you
know, having to go there and sweat and make that, then you could still go to the gym, but why? Like if you
could just pop the pill, you get exactly the same health effects, the same body, the same mental
clarity after, like I think many people would just pop the pill. We talked earlier about child rearing,
like maybe people would still want to do that, but a lot of the specific activities involved in child,
like changing the diapers, like doing this kind of, a lot of it is kind of individually things you might
be tempted to outsource. Um, and, um, so, and then you can like go through leisure activity by leisure
activity. And, and I think, uh, many of them would lose their original purpose at technological maturity.
And, and then if you did them, it would have to be for some other reason. Generally speaking, I think
you would have a kind of post-instrumental condition where to first approximation, that would be nothing you
would need to do in order for something else to happen, because that would be a shortcut to the
other thing. Uh, you could sort of press a button or, or like request your AI or robots to, to do that
thing. And you wouldn't have to exert effort yourself to get that outcome. And so all the things we do for
those instrumental reasons, uh, would drop out of the picture, maybe with some like class of exceptions,
but, and so I think this is the, the most interesting insight, um, in your book, which is even if we solve
all of the tremendously difficult problems around alignment, um, around all the other issues that you
described in your various other books, and we get to the maximally good state, right? Where politics,
society, and, and AI is purely working for us. It's still in some sense difficult to imagine what a good
life looks like because it's so, so different from, um, how we conceive of a good life today.
And so when we're, when so much of work and so much of leisure is made redundant, what is the best
type of life one can live in such a utopia? Well, it's actually quite challenging to really envisage a
great, like, because yeah, it would risk undermining a lot of the things that currently give meaning to
and you would feel almost at a loss. That's just kind of, do we just become these kind of amorphous,
you know, drugged, drugged out pleasure blobs or what, what, like what, and then it might seem quite
alienating and on unattractive. I think though that if you, if you push through that, I think there is a
there on the other side of something that actually would be very worthwhile.
So in your book, you described five, let's call it motes of defense, uh, for, for a life in deep
utopia. And I, I found this sort of way of framing it as already very interesting because you, you think
of in utopia, you're not defending things, right? But, but you are defending here because of how
difficult it's to imagine a good life here. Hedonic valence, experienced texture, autotelic activity,
artificial purpose and social cultural entanglement. These are, these will become the, in your view,
the pillar of the good life once you push through as you describe it. So can you give us an overview
of what, uh, what, what each of these means? The first one being hedonic quality. So this is basically
the observation that, um, we could have a lot of pleasure in utopia, pleasure in the wide sense,
not just kind of physical, but mental like enjoyment. Like you could actually immensely enjoy, um,
every hour and every day of life, um, to, to degrees that, that are like, at least would match the sort
of the peak current human experiences, perhaps go a lot beyond that. And, um, it's easy to sort of
dismiss this because it's like a philosophically boring point. I mean, it's kind of trivial that when
you have this advanced newer technology, you could do this. And, uh, and then we immediately jump to
sort of thinking, well, that's a kind of, um, degenerate existence. We think of junkies and it's like
not really, but, but like, actually it, I think is super important, maybe the most important. And this
on its own might make it very worthwhile to swap out the, uh, the current world for the, I mean,
I should say there is also a sort of minus one mode, which the book doesn't talk very much about,
but, but it's super important, which is just to get rid of the negatives that currently plague the
human condition, which is like immense and terrible. But, uh, here we are thinking about what
could you do more than just kind of getting rid. And so there, yeah, just this hedonic well-being
like that every day could actually be an immense delight. Like there, there are some people who are
like philosophical hedonists. I think pleasure is the only thing that matters and the absence of pain.
So for them, that's kind of case closed at this point, right? But there are other value systems that
maybe think pleasure is a good thing, but there are other good things. So let's see what we can add
to just mere hedonic well-being. And so then the second one is experienced texture. We observe that
not just can these utopians have great level of enjoyment, but rather than just being sort of
dazed out junkies that have a diffuse, confused sense of pleasure, they could like attach this pleasure
to valuable experiences. Like say the appreciation of beauty, um, or the, uh, uh, understanding of deep,
important truths. Um, so like pleasure in, in like learning and understanding the basic laws of physics,
learning about human nature, learning about, you know, and appreciating great art and, um,
natural beauty, uh, you know, place and like, that's, that's how they get their fun. It's not just
like the junkie, but it's like this kind of connoisseurship that is also exquisitely joyful,
you know, maybe appreciating the, uh, moral virtue and goodness in, in various people and historical
people and so forth. Um, that already seems to like make it, uh, more attractive. Um, and then,
then you can add some further things like, so it, it's not the case that you need to imagine these
utopians as mere passive recipients of these experiences of truth, beauty, and, uh, and, uh,
understanding of deep truths that they take enjoyment in. This, this would be this kind of
autothelic, uh, stuff. So they, they, they don't just sit there passively like observing great beauty
and feeling joy, but they could go around them like doing stuff. Um, artificial purpose is, um,
purpose that we, uh, uh, create, uh, in order that we are then able to engage in purposeful activities.
So you could set yourself some maybe arbitrary goal. And then once you have that goal, then
if, if you select that goal in a suitable way, you could then have instrumental reasons to
engage in various efforts to achieve it. So the key here is that the goal you set yourself has to be
sort of constitutedly such that it calls on you to make an effort rather than
for you to press the button to have the robot do it. Um, so you can bake into the goal that the goal
needs to be achieved by your own efforts. Otherwise like, so the goal is to achieve a certain thing with
your own efforts. Then if that, if you have that goal for whatever reason, then now you have purpose,
uh, because the only way you can achieve your goal is through your own efforts. So we, we can think of
this like the paradigm cases, kind of various forms of games where, you know, maybe you, you decide to
play a game of golf. There was previously no reason why the ball would have to go into a sequence of
holes. It's like a completely arbitrary goal, but you adopt that goal. And now you have, um, a reason
to try hard to hit the ball in exactly certain ways to achieve this goal. And you could generalize that.
You could have much more complex games with, you know, multiplayer, multi-modalities, like, you know,
extending maybe over years and, uh, that, that could then give you purposeful activity, not just activity.
And that, so those, those are the first four. And there's like one remaining one, which is,
there would be some natural purposes that could survive into technological maturity.
Um, but that would be sort of subtle purposes.
Like what, what, what natural purposes would survive in a, in this post-instrumental state?
Well, take something like, um, so if you currently have some value or goal that say,
uh, you, you value the continuation of a certain tradition, that might be something you just happen
to, that many people would right now have that as one of their values. And the tradition might be such
that it just isn't continuing on, unless humans are continuing to do it. It might be like constitutively
part of the tradition that humans are doing this thing every year in a certain way. Like imagine some
ritual or something like that. So then those would survive because that would be like, you could create
robots who would be doing this stuff, but it wouldn't count as continue or honoring your ancestors.
Like it might not count as you honoring your answer. Your, your value of honoring your own ancestors
might not be served by building a whole, like, like ensemble of, of robots who are like going around
and you know, paying visits to their grave or thinking about them. Like it might require you to do it
for that value to be achieved. Um, more broadly, I think various forms of, uh, social entanglements
where, I mean, we touched upon it a little bit earlier when we discussed parenting, where if, if there
is this existing relationship between a child and a parent, and part of what is valuable about that is
that it's these particular individuals who are relating in a certain way, then that could also
give you a natural purpose, even at technological maturity to continue to do certain things and
interact in various ways, uh, with your child. And, and, and this might at first sight seem
weak, uh, like compared to a lot of the reasons we have things to do today, there are like very stark,
tangible, immediate consequences if we fail to do them. So like maybe somebody has to go into work
every morning because otherwise they will lose their job and then they can't pay their rent and
then they will get thrown out on the street and then it will be cold. And this is like a very sort of
hard set of consequences. And a lot of the stuff we do today are motivated by these kind of hard, hard
consequences. And, and, uh, in, in utopia, a lot of that would go away. But, um, I think like just as, um,
um, you, you walk outside in, in the daytime, you, you, you, you see the sun, you don't see the stars,
they are much fainter than the sun, but they're still there. I see these subtle values as already
being there. Like there's a whole host of these more almost aesthetic reasons for doing things
that are blotted out from sight currently, because there are these kind of more screaming moral and
practical needs that we need to take care of in our lives. But if you imagine a scenario where all of
that went away, like the sun set, and then it would make sense for our evaluative pupils to dilate,
to take in more of the fainter light that comes from these subtler values.
Right. So what I, what I understand these five rings of defense is that the four, the first four
rings, a critic might say what's lacking from them is a kind of necessity, right? That, that I feel I need
to do X, Y, Z because it, it's from this external source that gives my life purpose. And it, it seems
like you're trying to rescue necessity in utopia by highlighting a subset of activities, like honoring
one's ancestors, that it is, it constitutively requires us humans, or maybe even stronger, you as an
individual to do. And that's how you rescue, uh, necessity. And so maybe analogy here is, you know,
LeBron playing in the NBA, bring back a, uh, uh, the championship to his hometown, Akron, Ohio.
There's nothing necessary about that, right? It's just, it's a set of rules that, that we've invented
for ourselves, but it doesn't make it any less meaningful for the people involved. Is that,
is that a good way to understand it? Yeah. Well, if he independently wants to do this,
or if there is an independent value that this is something that should happen, that like,
if, if, if, if it were a case that he just set himself this goal, because otherwise what should
I do all day long? And then like he convinced himself to pursue this, then, then it would be
an instance of the fourth artificial purpose. But if there's like some independent reason that was not
just created in order that somebody has something to do, then it would count as a natural purpose.
Right. And, and in this example of LeBron winning the championship for, for the, for Cleveland,
it would be something like the recognition and expectation of his family and friends and the
whole city that he grew up in. Would that be a good example of something that would make this
social cultural entanglement instead of artificial purpose? Yeah. So that, that would give him a real
reason to do this if, if people continue to want that to happen and if he cares about what other
people want or what they will think of him. So it seems like social cultural entanglement relies heavily
on, um, uh, the economy of recognition, right? What, what people desire, what people give honor to,
and what people give esteem to. Do you think that humans will start esteeming the recognition of
non-human agents? And maybe we're already starting to see the, the genesis of this where there are
these dating, dating bots that have already existed. And right now it's only appealing to people who are
struggling to form real human relationships. But I remember there was one dating bot that after they
changed the algorithm and the dating bot behaved nothing like what they previously behaved like,
the people were in tears as if a real family member had died. So it seems like if eventually we think
that we'd care as much about, uh, artificial recognition as, as human recognition, then a lot
of these social cultural entanglements would be threatened because presumably we can, uh, tell what,
with a robot, uh, what, what to, what to recognize or, or what to give esteem to. Um, yeah, that's right.
So you could have these, uh, future social, um, entanglements with, with, with various forms of digital
minds, uh, but, uh, you have a kind of legacy. So if, if now you care about certain people,
like you, you might not want, even if, if you had some brain technology that could sort of, uh, extirpate
that care and implant a different care, or like, so you may be like somebody who is kind of, uh, faith
of falling in love with the wrong person. And, but it, it might have been better if they had fallen in
love with somebody else. Uh, but once they are there, they are there. Um, and so we might have
these legacy purposes that come from current commitments that we care about and that we will
carry on to, uh, into, into technological maturity. I think it's not the only, I mean, so there certainly
are these social entanglements. I think there might also be, um, more broadly aesthetic reasons. Like
you might think it would just be a more beautiful way to live your life if you did certain things
yourself and upheld various things and kept doing certain things, even maybe avail yourself of a lot
of the conveniences, but it would just be, uh, so I think those could, and spiritual and religious,
um, reasons for doing things also could possibly survive. Right. I see. And so I want to go back to
the beginning, um, about pleasure, uh, because I think I want to defend your position a bit because there
are entire schools of very serious philosophies, uh, for example, the Epicureans who do see that
pleasure as the, the supreme good. And so that literally might be, might be enough. But even for
someone who we, we, we naturally consider to, to think of pleasure as this kind of secondary or
tertiary thing, someone like Aristotle, um, when you read him closely, uh, what the virtuous man is,
is, is, is someone who takes pleasure in doing the right thing, right? So it's about correctly aligning
one's pleasure. So, so I think that ends, that adds credence to your view that, that pleasure itself
might be worth, uh, jumping into the state if, if nothing else.
Yeah. I mean, in, in the second case, I guess, particularly if the pleasure was kind of coupled
with the right, that if you took pleasure in the right things, uh, which, which is like, yeah, that,
that could be like taking pleasure in, in contemplating the right things or in doing,
engaging in the right kinds of activities and so forth. So yeah, you can, I think in general,
it's nice if you, um, steer towards a future that will score high on, on a, on a wide range of
different values. And according to, uh, many different people's preferences, if, if we can do that
by just compromising slightly on any one of the values. So I think
because the future could be very big in terms of the resources available, um, that would be a bunch
of different values that it could be quite cheap to satisfy. And so we should make sure to satisfy all
of those values. There are certain values that are resource hungry. Um, like if you are a utilitarian,
for example, like you could always make more happy people. And so you just want more and more and more
resources, but, uh, if there are the values that just needs a little bit, and then they are sort of
almost maxed out, then it seems like let's, let's do that. And then, you know, the, the outer space,
maybe the utilitarians could have a bigger say about how we dispose of that.
What about values, uh, that could compete with each other? And I think this factors into this
pleasure case because in Christian theology, for example, a lot of the sins aren't loving the wrong
things, but they're liking the right things, but in the wrong degree, right? Right. So lust, gluttony
are all about liking good things, but into such an immense degree that you ignore some of the most,
uh, the more important things in life. So here's a concern, Professor Bostrom, you might be able to design
a perfect super drug. That's like Molly plus mushrooms, plus cocaine, all the pleasures of
all the drugs we have right now with zero of the side effects continuously. And there might be nothing
objectionable about that in itself, except for the fact that such an immense pleasure would detract us
from pursuing perhaps some of the, the, the less obviously good and interesting parts of our lives,
but are no less important for a flourishing life in the long run.
Yeah. Um, so I think that certainly would be the case today, um, um, with more mature technology,
you might imagine having the ability to, uh, create more fine-grained experiences that, um, well,
a, that mean, it could remove sort of the addictive potential and, uh, the, the sort of stupefying effect
of some intoxicants, the, the adverse, uh, you know, implications for your liver and blood pressure,
like all of these things. But, but also like psychologically, rather than just having it the
kind of monotone thumb pleasure, and that's the alternative to living a rich and engaged life
in deep relationships with other people, you could, you know, weave these things together in, in a more
intricate way. So, so that the pleasure comes from these, you know, virtuous and appropriate activities
and thoughts and experiences. Right. So, uh, to, to, to give an, uh, example that certainly will be
outdated. It's like, uh, ingesting a drug or, or going, undergoing biotechnical enhancements that gives
me immense pleasure when I brush my teeth, when I visit my friends, when I, uh, when I, when I go to bed
in the right time, right? It's about exactly like what Aristotle described, like interweaving pleasure
into what also would in itself make for a good life. Yeah, that seems right. Like some of the
specifics there might need to be like, you might not need to brush your teeth anymore. Right. Exactly.
That's why I said it would be outdated. Right. Uh, but yeah, so it would be focused more perhaps,
um, on what is intrinsic, like intrinsically valuable activities and experiences as opposed to these kind
of instrumental necessities. So right now there are all these things we have to do in our lives.
And so, yeah, let's take pleasure in them because then we do them more and it's like all works. Right.
But, uh, even in a condition where you didn't have to do all of these things, like you have,
you don't have to, you know, clean your house because you have a robot that cleans it. Your teeth
are like enhanced so they don't rot even if you don't brush them, et cetera, et cetera. Then in that scenario,
it seems like what we should be spending more of our time on are activities that are intrinsically
valuable. Um, right. And so then taking pleasure in those would seem to be appropriate. Yeah. And,
and that might be, uh, even better for someone concerned with virtue because we would be able
to program the virtuous, uh, uh, activities as more pleasurable, right? So for example, to respond to the
Christian critic that I just brought up, we might say, yes, we are going to make sex, sexual activity,
10 times more pleasurable, but we can also dial up, uh, reading the Bible or the contemplation of God,
to be even more pleasurable. So that pleasure naturally directs us, uh, to, to, to, uh, a good
life in and of itself. Yeah. Or, or the, the, the, the, the, the sexual pleasure likes being specifically
connected to the in wedlock, uh, a case, et cetera, et cetera. I mean, we have already a little,
if it's like the Ozempic, for example, I think, uh, is kind of, you know, reduces people's, uh,
vice of overeating, uh, too much. And so there are like these limited ways, uh, now there might
be sort of harder trade-offs. So there might be certain values that for example, maybe it would be
perhaps appropriate for, for these utopians occasionally to remember the earlier times
and the horrors and tragedies of history, and maybe feeling, uh, sad and mournful when, when they
kind of contemplate that, maybe they would have like, I don't know, you like imagine an annual
ceremony where you try to think of all the people who died before they ever got to, uh, experience
this. And, um, with some, some, some, some beautiful ceremony to other, the, the, right.
And, and then maybe you don't want to have like, I mean, certainly certain forms of pleasure would
seem to be inappropriate in, and maybe you would need to actually have sadness or maybe some kind
of bittersweet. I don't know. I did this, there's a huge design space here that, uh,
uh, hopefully that could sort of work out. Um, I see. Um, one thing that seems to be threatened
even with, uh, these five modes of defense, uh, is interestingness, right? Not living a boring life
because the challenge is you could be a mortal. You could potentially live forever, uh, until something
catastrophic happens to you. Um, is there any defense for a boring life given of how long we'll live?
Um, first of all, there is an even more basic distinction here between subjective boredom and
objective boringness. So the subjective boredom is just you feel bored. That certainly could be
abolished in a solved world, like through the same kind of neurotechnology that we have already discussed.
And, and we have to be careful that that doesn't sort of infect our intuitions about the objective
boringness or interestingness. Uh, constantly reminded that, that that's already a big chunk of what we
normally associate with things that are boring or interesting. All these people in utopia could
like be totally fascinated all the time and just be completely immersed and find this like every fiber
of their being is just like, wow, this is sort of cool and interesting and I want to dive in, right?
So that, that's already there. Like, um, but if we're talking about this objective, so the, the objective
notion of interestingness is, is a little bit harder to pin down, but it's some notion that certain
experiences or activities are sort of maybe such that they, it would be fitting to be interested
in them and others, it would be sort of unfit. So like just staring at the wall, maybe you think that
even if you could take a drug that would make that feel super interesting, that there is like some sort
of normative disvalue in that because it's not the kind of object that it would be appropriate to be
super, so, so then you can think objective interesting as well. So to the extent that it involves
something like complexity and richness and sophistication of your experiences or activities,
they could sort of just rack that up to 11.
There is a slightly different version of interestingness, objective interestingness, where it
calls for fundamental novelty,
um, where you might run out of that, um, that it's like, so for example, if you think it's not
nearly as interesting to learn about fundamental physics as it is to discover it for the first time,
then, you know, eventually we will have figured out what the, all the basic laws of nature are and the
fundamental truth and the big general concepts and then scientists will, uh, have to content themselves
with finding smaller and smaller truths, more local truths that are less profound and so that that's a
kind of form of interestingness that that you would eventually run out of or maybe you would set aside
little pockets of ignorance, um, and mysteries to, to sort of, as, as I alluded to earlier, um, if you think
about human lives currently, if you take this second notion of sort of fundamental novelty in, in an
individual person's life, I think a lot of that happens really early on and so, like, in the first
couple of years of life, think, think about, so you discover, well, like, you discover that there is a
world like that, that's a pretty fundamental discovery, right? And then, like, it contains
objects, uh, like, that continue to exist even when you look away, like, wow, that's like, just like a
jaw-dropping real, a cognitive revolution, right? And then you discover that there are other people in
the world, like, that's a, now, like, when you're grown up, what's like the most profound thing you
learn in a given year? Like, it's registered a lot lower on the sort of Richter scale of fundamental
novelty. And so, so we are already suffering huge diminishing returns within our current lifespan.
If you sort of look at the human, like, the planet Earth, as it were, from, from some alien comes here,
and, like, each, the average person's life, like, how much fundamental novelty is there? Like, they are
all, you know, maybe a few people are doing some interesting new things, but most people are just
doing the same, they have the same old thoughts, the same old fears, and the same hopes, and they hope,
you know, boy meets girls, yada, yada, yada, get the job, get the paycheck, get old,
you know, somebody passes away, you're sad for a while, and then you eat, and that's like,
from a certain lens, you might think that our current lives would be extremely boring already,
just because it's already pretty much been done, and only small details are different.
In fact, I want to generalize this, this might be a caricature of your view, but this was my kind
of takeaway of how you essentially are defending life in utopia, which is you're saying, look, you think
you want this fundamental novelty, you think you want deep purpose, this kind of world historic
sense of meaning, look at your life now, how many of us have those things, and yet so many of us live
great lives. What you actually want is a lot of pleasure, social engagements, aesthetic experiences,
it's kind of like, your argument is kind of like the softest position in the gorgeous, like, let's stop
talking about these highfalutin values with big fancy names like purpose, meaning, and interestingness,
and let's get the basics done. And the example you gave was Nietzsche, and you said something like,
you know, Nietzsche talks a big game about his higher men and the Napoleons of the world,
he lived like a bohemian reading and writing books in the Alps. Is that a fair categorization of your view?
Well, I think Nietzsche was one of those people who, like, would have a relatively more plausible
claim to interestingness in this later sense, like in that he, like, thought big original thoughts and
really dove into that. I think there is, if you have an axiology, a theory of value that is kind of
pluralistic, I think given that it's pretty plausible that there is some extra value in also having this
third kind of interestingness that's kind of globally, you know, registers in a significant
way. I'd say that even there though, like, if you just zoom out a little bit further, it's probably an
infinite universe out there and with a lot of other planets, with a lot of other civilizations that have
already thought the same thoughts and thought much better thoughts and created better. So, like,
depending on your scale, we might already be completely unable to realize any fundamental novelty in the
world. But if you do focus on this mesoscopic scale that is kind of either an individual life or like
the globe as it is now, with eight billion humans or so, and you think that that's where I want to make
the significant difference on that scale for this particular kind of, then I'd say that right now is
the golden era of purpose. Right now there are, like, immense stakes in the world. There are, like,
a lot of immediate moral urgent causes that where you could individually make a significant difference.
Plus, we are seemingly approaching this critical juncture in human history upon which, like, the next
billion years might depend. Like, this, if you want real purpose, like, knock yourself out now, like,
because it's never going to be, like, more at stake than it is right now and in the next few years.
And if you can't even bother to do this right now, like, then, I mean, how much value do you really
place on this kind of global purpose? Right. And so, is my reading of your view of human nature right,
which is, you're defending the life in deep utopia, not by rescuing this deep global sense of purpose,
meaning, interestingness, but by suggesting people actually don't need that or desire it.
Yeah, I think it's, like, one value of which we might have less in this old world, but I think we
could have a lot more of most of the other values, such that the net balance is, like, an enormous
positive. I see, I see. I mean, like, yeah, okay, so maybe if there weren't people starving,
you would be deprived of the purpose of feeding them, etc. But still, it's a trade I would be happy
to take. Right. And there's, like, something lost there, like, there's, like, something nice and
glorious in somebody going out of their way and feeding the starving. That's, like, a little plus
there, but there's also this huge negative. And if we could all just have enough to eat without that,
I think that would be better. And you can, I think, generalize that.
Right. When you looked back at history in the book, you said that the periods and the people
worth dramatizing or talking about are rarely the good periods or good lives that you would want to live.
Yeah, there's, like, a big difference. And this is, like, a fundamental thing to bear in mind when
forming some opinion about this kind of utopian problem. That you could evaluate a hypothesized
condition from two different perspectives. And what we, I think, often default to, if we're not
careful, is the external point of evaluation. Like, we look at this, like, future utopian condition
as if it were, like, a stage play. We sit in the audience and we'll look at this. And then we form some,
like, kind of thumbs up or thumbs down. But from that perspective, I think we will tend to overvalue
interestingness and drama. Like, if you go to the theater, you want stuff, you know, to happen.
You want, like, you know, there's, like, a king and he gets killed. And then the assassin flees. And
then they overtake him. And there's, like, or the movies we write or the novels, etc. So good stories
often have a lot of suffering in them. But there is a different than, which I think is the right way
to evaluate this. It's not, like, how good is utopia from the outside to look at, but how good is it
actually from the inside to inhabit, to live it. And there, I think, yeah, the stories that, you know,
are most interesting to read about are not necessarily the stories that are best to live out in your own
life. And we need to correct for that if we're actually trying to build a utopia that we would
be moving into. If it's not just a fiction or a screenplay, but, like, an actual plan for what we
want to spend the rest of our time in.
So you mentioned something like there are no wars in history that are worth it, no matter what kind of
great art is produced by them. But I wonder if on the off chance that that war or conflict or bad thing
creates these civilizational grounding pieces of art. Would you still will that away? So here's the
thought experiment. You know, ex ante, I think you would say no, no Trojan War, right? But ex post,
given that we know how fundamental the Trojan War was to establishing not only just Greek tragedy,
but Greek philosophy and Greek culture, would you, if you could wave your wand one way or the other,
would you say spare those lives in the Trojan War? I don't want my Iliad and Odyssey or even the
responses to that. Well, I don't feel entirely competent to make these judgments. I would think
that at some point enough is enough. That, I mean, we've had a lot of wars by now and we've had a lot
of people dying for various causes and suffering horrible fates. And, you know, maybe there's a certain
kinds of value that can exist in human style life. And, but if you think about this,
do we want like, okay, so maybe you want another few decades, another few hundred years, another few
thousand years, but like a hundred thousand, we need a million years more of these like two-legged
creatures running around here and killing one another and getting cancer and like having headaches
and stuff like at some point, I think we want to maybe unlock the next level and say, well, you know,
if there are values in some of these tragic and beautiful things, like it might not scale with
a number of instances of the tragic and beautiful, like having 10 tragedies doesn't create 10 times as
much beauty value as one, like, and, and so it's the kind of value that seems to saturate.
Right.
Um, whereas like the value of like a nice cup of tea, you know, the, the thousandth cup of tea might be,
you know, just as a taste, just as good as the first cup of tea. So eventually you've kind of ticked
all this, you've, you've had your big life, you're, you're the youth of humanity. You've had your adventures
and stuff, and then you settle down a little bit.
I mean, I think there might be different kinds of adventures that might be a lot more interesting
in many ways. It's just that maybe they involve less suffering.
So other than this, uh, global sense of purpose, meaning global sense of stakes,
global sense of novelty and interestingness, is there any fundamental set of human values that
will not be fulfilled in this utopia or, or be fulfilled worse than currently?
I think a lot of them have some connection with this, um, yeah, this sense, the purpose and meaning
that they seem to be particularly, uh, uh, threatened by the affordances of, of, uh, of a solved world.
Um, depending on, you know, your kind of more spiritual and religious views, there could be,
I guess, additional, uh, constraints there in terms of what could be achieved in, in a solved world.
And then this kind of fundamental interestingness of the form that requires novelty on, on a global
scale, not the cosmic scale, not on a sort of day to day scale, but on the sort of scale of planet
earth that, that also might, it might just be so many times you can discover, like you can discover
relativity theory once, the theory of evolution once, and then there might be, you know, 50 more
discoveries of that magnitude that you can make. And then after that, it starts to dwindle.
Right. So that would be another example.
And I think that's a very interesting insight on human nature, which is that humans are the type
of creatures such that one of our core values requires us to be in a fallen world or, or in an
imperfect world, right? Meaning this kind of global sense of scale, purpose, or novelty.
Our, our natures have maybe been conditioned on the existence of, uh, problems, uh,
in, in that throughout human history and prehistory and indeed all the way back to our great ape
ancestors and way earlier than that, there were like various forms of scarcity and things that needed
to be done all day long. You had to check for predators, you had to get food, you had to do,
you had to find, and so like, um, a lot of our psychology is kind of just assumes that there are these needs
and we see a little bit of how problems can arise, uh, when that is no longer the case today with
obesity, right? So we had, we have kind of psychologically, um, evolved in a way that assumes
that it's food is scarce and you need to try to find it and grab it when you can and stuff yourself
as much as possible because maybe tomorrow there won't be anything to eat. And we've removed that
constraint from our external world in, in, in, at least in, in wealthier countries. There's plenty,
like the fridge, you know, is full of food and now there's this mismatch between our environment and
our psychology. Um, as we move to technological maturity, that, that, that, that little crack
could open up much wider and that could be a huge mismatch between what our sort of evolved
psychological nature is and what the environment actually demands of us. And that's kind of what creates
this problem in the first place, that we would need to possibly like change ourselves quite
fundamentally to become, uh, suited for life in, in utopia. Right. So, so here's a, uh, proposal about
rescuing the sense of, uh, at least perceived global purpose, global novelty, global meaning, global stakes,
which is to wipe off our memories and then go into a VR simulation that is indistinguishable. Now the,
obviously the, the philosopher's objection is laid out in the experience machine, right? Where the,
the conclusion is that you wouldn't want to enter into such an, uh, such a machine, even if you
couldn't tell because it would be unfitting that, that there's something objectively bad about it,
even if the experiencer doesn't know. But correct me if I'm wrong, I believe there were subsequent
variations on this, uh, experience machine, uh, uh, thought experiment that said, but what if I told you
that, uh, you live in experience machine now, would you want to pull out? And I, and I believe the intuition
is that you wouldn't want to pull out. And so what people are really after isn't fittingness to
objective reality, but it's a kind of fit, a familiarity. So, so if that's the case, what's
wrong with wiping out our memories and pretending to live like Achilles, uh, in utopia for one life
and just keep on doing this to, to get that kind of global sense of, uh, uh, uh, meaning back?
First, it's not clear how much meaning you would get from that. So if there was no,
it, it would kind of fall, it seems to me into the artificial purpose category.
Um, but you wouldn't know, right? That's the key part.
No, you wouldn't know, but that's, that's also the case in utopia that, I mean, if, if you want
some like partial amnesia so that, that you forget about various things that would be easily arranged.
Um, I think like, I mean, I presume that you might want to edit the, like, so if, if, if edit the, uh,
the sort of, uh, version two point, like if you want to re revisit history, like there are a lot,
a lot of parts of it that I think you would want to omit from, from your sort of, uh, recapitulation
of it. Um, either because they are too horrible or because they are just kind of a bit dull.
Um, but, uh, certainly you could imagine creating virtual, uh, worlds that you could interact with
and inhabit and explore, um, in different configurations and with different variations.
So I just want to draw a distinction here because when I, when I, when I heard you say artificial
purpose, I thought what you meant was for example, going rock climbing. So, you know,
I could get helicoptered up to the cliff face, but I knowingly restrained my own set of available means,
uh, to climb this rock face knowingly, hence making it artificial. But if I wiped my memory
and entered into the life of Achilles, like phenomenologically, I wouldn't be able to know.
Right. And so, so it would lose the artificial side of that, uh, at least from, from the subject. No?
Yeah. So I think there is a continuum there. If you sort of, you could imagine the, the,
the rock climber, once they are halfway up the wall, um, today, that might not be a helicopter
that could reach them in time. They didn't have to climb the wall. That was kind of
artificial purpose. But once they are there, they really have no, uh, choice other than to
like do their utmost to keep doing it. Otherwise on, you know, on pain of death. Right. And similarly,
you could imagine utopians if they wanted to, could create little, um, holes in utopia, right? Where
the world is not solved, where there's like real need and constraint and stakes of various kinds. Um,
if, if you thought that was a added value to sort of being subject to these forms of risk.
Um, now you don't want to make too many holes or you just kind of destroy the utopia, right? Like
if you, you could like just destroy the whole utopia, you're back to scarcity and real need again.
But that, that would also mean giving up all the good things about it. But you might have little pockets,
like designed and, you know, maybe that would be real stakes, but maybe the stakes wouldn't be quite
like, um, a child dies from brain cancer type of stakes, which, but, but more like, well, if, if you
fail at this task, you will have a month of, uh, you know, being excluded from the, your normal fun
gadgets and friends that, and you have to like work hard for a month to sort of get back to where you
were or something like something more kind of human scale stakes, right? Right. I think the
interesting question about human nature of whether, you know, we think that the picture you painted in,
in deep utopia is an attractive one or not has to do with how important necessity is for, for a good
life. Right. And for, for someone on the opposite extreme, someone like the Unabomber, he thought that
today, even in our current technological society, there are not enough necessary primary actions that we
have to do. And so even now he wants to take us away from the current technological utopia that we
are in. And so I found that to be a very interesting, uh, uh, extreme opposite end of this, of this
experiment. Yeah. Although I think, uh, some of what he was thinking about were the side of contingent
psychological effects of living the current lives where there are various forms of psychological, I mean,
from, from, from overeating, but also like, like various kinds of malaise that can happen when, uh,
people live in this modern artificial, uh, like you get addicted to your social media feed. And if you
never can, can you really connect to any other, can you have real friends if you've never been in a
life or death situation where you like saw that they like were a true friend, even though it, they risked
their life. Like that might be a form of connect. There might be all sorts of ways in which, uh, the mismatch
between what we evolved to do and the current world creates psychological ailments that maybe they
are outweighed by, by the comforts and benefits. I think that's plus possibly the case, but still
there is this kind of psychological cost, probably some kinds of mental illnesses are more widespread
because we were not perfectly adjusted to the modern, but those things could be fixed. Uh, like you,
you, you, you, you wouldn't have to, uh, uh, uh, overeat or, or, or become feel socially alienated or,
uh, in, in other ways, like you might even get closer to nature in some ways, like rather than living
in sort of concrete square buildings, like you could imagine a technological maturity, you would be
actually living in some sort of savannah, like, you know, but minus the bugs perhaps, and mind like,
you know, the temperature would always be right. And right. Um, and so, so in many ways you could
have like some, like even like first you could adjust the psychological apparatus so that it
didn't have these negative symptoms. Second, to some extent you could also adjust the environment so
that it would in many ways more match what we were naturally, uh, designed to interact with.
What was striking to me was how theological our entire conversation today has been and how
similar the thought experiment you set up was, was to the Christian afterlife. So let me just give
you a few examples. So in your deep utopia, right? Plasticity means that the material world is not
really a concern. We are, but we also have our individual bodies, right? In the Christian afterlife,
individuality is preserved. So all kinds of social political issues are resolved. And just as the
saints in, in heaven are supposedly spend their time contemplating God, a lot of the activities that
you described are contemplation based. Uh, and also you delivered this book in a lecture format where
you kind of construct this world in six days, uh, six days of lectures, and then you rest on, on the
seventh. What do you make of the, the, the theological, uh, aesthetic of your work?
Yeah. I mean, the number of days is more, I thought it was kind of done on, on, I mean, it took
second of six years to do the six days. I felt like it was time for a wrap at that point. Uh, um, and, and,
but I think in general that there are, uh, uh, strong parallels between the, uh, thoughts
developed in, in, in religious, theological context of, because it's like, in some sense, the same
fundamental question, like what's the best possible future for a human being? If, if you abstract away
from various contingents, limitations and constraints, um, more generally, I think actually when you
think through the full and ultimate implications of the sort of standard physicalist worldview and
really think, uh, in, in many ways you get to considerations traditionally developed in, in a
theological context. I mean, we, we alluded to the simulation argument earlier, right?
There it's very striking. Uh, I mean, it starts from a different kind of, um,
assumption to begin with, but the end result is something at least structurally strikingly
similar to, to many religious and theological conception. And my, my most recent paper, um,
AI creation and the cosmic host is, is another example of how you start to think through about,
in, in this case, um, various ethical questions related to how we should relate to, uh, digital
minds and AI that we are building and, and the possibilities of levels of, uh, where again,
you sort of come up. And so it might be that there is like the, the philosopher, Derek Parfit,
who was a colleague at, at Oxford, uh, um, he had this metaphor in, in his work of, of a big mountain.
And he, he, he did work on metaethics and he, he had this view that different approaches to metaethics,
um, sort of consequentialism and deontology, they were kind of climbing the same mountain
from different sides. And that when you thought through each one to its kind of purest and clearest
form that they would sort of converge at the peak. And I think maybe there is like some similar
phenomenon where you have a big mountain where people have been climbing it from the theological side.
And if you climb, it's far enough, high enough from the naturalistic side, maybe you get to
a similar conclusion in the end. Thank you so much for a fascinating discussion.
Thank you, Jonathan. I enjoyed this.
