I
Sam is here. Thank you so much, Sam, for being here. We are super appreciative. Sam, of course,
is the co-founder and CEO of OpenAI. As I said earlier, one of the most consequential companies
of our time. In just the past few years, since the launch of ChatGPT, Sam and his team have brought
AI to the mainstream. He's fundamentally really altered how we work, communicate,
innovate, and now think about what our future is even going to be like. His company is now worth
about $157 billion. And I should say, OpenAI didn't just spark an arms race. It really redefined
the possibilities of technology. And as he pushes towards general or AI, I should say,
artificial general intelligence, we're forced to grapple with some very big questions about the
future of work, about the balance of power, about ethical questions, and so many other things. And
we're going to delve into all of that with Sam. So thank you, the man at the center of it all.
Thanks for having me.
Great to see you. So here's where we'll start. Two years ago, almost to this week, basically,
you launched ChatGPT. And I think it's fair to say that when you press the button, since then,
all hell has broken loose. You know.
It does feel like that.
It has changed the global conversation. It has changed fundraising priorities, the move of
resources. Other technology companies have shifted the way they're doing things. There have
been lawsuits. All of it. And so I'm very curious if you could just take us back two years ago,
literally, almost, as I said, to the day or to the week, and what you thought was going to happen
when you press the button.
So in the abstract, we always knew there would be some moment where, for whatever reason,
like, the world would go from not getting it to getting it. And that all of a sudden,
it would be clear that this technology was working. Like, internally, at that point,
seemed very obvious to us that language models were going to keep scaling. They were going to do
all these useful things. And why it happened at the launch of ChatGPT, I mean, I have some theories,
but why it happened then and not when we launched the GPT-3 and the API earlier, or why it didn't
happen until GPT-4 launched several months later, like, why that exact moment? I think there is some,
like, chanciness to when it actually catches fire.
We had observed, though, with the API and GPT-3 that, you know, GPT-3 was a little bit early,
and it didn't work for that many things. But in some sense, one of the killer use cases of it
was developers in the playground, which is where you could, like, test ideas quickly before you
implement the API. They loved just talking to the model. They would just sit there and, like,
talk to GPT-3 conversationally about whatever. And that was kind of the main thing people were using
it for. So we said, well, if that's what people want, we can make it much easier to
use. You don't have to sign up for a developer account and do all these other things. And we
can sort of tune it to be good at conversations. And so we said, okay, let's make this as a
product. Now, we'd actually been planning to launch it with GPT-4. We finished GPT-4 in, like,
August of 2022, but we held it for a while.
But did you expect all of this to happen?
Again, at some point, we thought there would be a big moment in the world. Did we know it was
going to be right then with the launch of ChatGPT? Certainly not.
So here we are now, two years later, and the next big question that we're all asking,
I can't even believe we're just asking it two years later, and you actually just wrote about it.
You said that it is possible that we will have superintelligence in a few thousand days from now.
Two years ago, did you think that? And when you say a few thousand days,
I can do various versions of math on what that means.
Well, I mean, there is wide uncertainty, and there's also different definitions of what
superintelligence is. I, yes, two years ago, we thought we were on a pretty steep curve.
I mean, we started OpenAI because we thought this was possible, and maybe not that far away.
And it has gone well, but we hoped it was going to go well. Like, we believed in deep learning as this
unbelievable new discovery of humanity, and we thought it could go here, and we had a duty to get
it here and to make it go well and to broadly share the benefits. But yeah, we did think we could get
here, and we still believe we can get here. It's not for sure. There's a ton of hard work, a ton of
research and engineering still to do. But I think it's possible. And I think it's possible, not super
far in the future, I expect that in 2025, we will have systems that people look at, even people who
are skeptical of current progress, and say, wow, that I did not expect that that does change what,
like what? Agents are the thing everyone is talking about, I think, for good reason. You know,
this idea that you can give an AI system a pretty complicated task, like a kind of task you'd give
to a very smart human that takes a while to go off and do and use a bunch of tools and create
something of value. That's the kind of thing I'd expect next year. And that's like a huge deal. We
talk about that like, oh, you know, this thing is going to happen. But that's like, if that works as
well as we hope it does, that can really transform things. Okay, so what are we supposed to think?
There's been a whole bunch of headlines in the past couple of weeks where people have talked actually
about maybe that the scaling laws of AI actually are slowing down. And you put out this, I don't know,
it was cryptic or it was very direct. You put out a tweet where you wrote, there is no wall.
I don't see how it could be any less cryptic than that. Like that was an attempt not to be cryptic at
all and say, hey, this is going to keep going. In fact, hopefully this will put it to rest at
least for a while. Yeah, I've always been struck by how much people love to speculate on, is there
a wall? Is scaling going to keep going? And like, rather than just like look at the curve of progress
and say, maybe I shouldn't bet an exponential against an exponential like that. In any case,
we have a bunch of new great stuff. So we're doing this kind of fun thing for the holidays. We're doing
12 days of open AI starting tomorrow, but we're going to launch. We'll either launch something
or do a demo every day for the next like 12 weekdays. And without saying too much about the
thing for tomorrow, it is continued progress. And give us a little bit more of a hint then
if it's going to start tomorrow, start today. I don't want to spoil your surprise. It's not
along the way. I'm not good with that. I want to know. One of the big questions about AI right now
though is, and this goes to the scaling question, is how much data and what it takes to actually
continue to scale? How much of this is about just pure processing power and therefore capital,
right? Versus data, where you get that data, this idea of synthetic data, digital data,
that we're going to recreate data on top of data. What does it actually take to scale at this point,
when people even talk about some of the limiting factors? I think that there's some question embedded
in that about which piece is going to get there. So the three key inputs, and there are others too,
but the three key ones are compute, data, and algorithms. And you can trade them off sometimes.
Like if you have much better data, you could use less compute. Or if you have a lot more compute,
you could use that to create synthetic data and make more data. For a while, there was incredible easy
gains in front of us with compute. More recently, we've had a lot of algorithmic gains. But kind of,
you want to push on all of those inputs at the same time. And at different times, there may be
differential returns to one or the other. But I kind of think of all three.
But for you, compute, I assume, is the biggie? And if you don't have the compute,
all the others don't matter? I mean, in some sense, algorithmic progress is the biggie,
the biggest one, you know, if you can, like, maybe you can double the size of a computer,
maybe over time with crazy amounts of capital, you can 10x the size of the computer.
Once in a while, these 1000x gains come along algorithmically. The transformer is like a good
recent example of that. That's rare. But that is the biggest gain when it happens.
So right now, though, there's an arms race, it feels like for processing power, right? There's an
arms race for fundraising for processing power. There's an arms race to build just and get more
compute. You have a relationship with Microsoft. Yeah. And you rely on them right now.
I think there's a race on all of these things. The compute one is like, the most fun, the most
theatrical, the biggest numbers, it gets the most attention. It is really important. I don't want to
dismiss it. But there's certainly also huge amounts of effort going into who can come up with the best
algorithmic ideas, who can secure new data sources. So I think it is on all three fronts.
You do have this relationship with Microsoft. Yeah.
It has been described as at times, it's been described as a tech bromance in the best way
possible. And then I was just reading Alex Heath, who wrote, it's clear that Microsoft and OpenAI are
disentangling. I don't think we're disentangling. Look, I will not pretend that there are no
misalignments or challenges. Obviously, there are some. But on the whole, I think it's been a tremendously
positive thing for both companies and excited to do like much, much more together.
But long term, do you think you need to have your own processing power that's yours
as opposed to relying on somebody else?
No. And in fact, I mean, I think we need to ensure that we get
enough compute of the kind we want that we can rely on and all of that. And there may be reasons
we have like some very crazy ideas about things we'd like to build that are, you know, like high risk,
high reward. But we certainly don't need to have like OpenAI get really good at building computer
like massive scale data centers. In fact, one of the things
and maybe this is a consequence of kind of just like how I grew up, but I like grew up as one of
those like startup generation founders of the generation where it was like, oh, before that period
in time, everybody had to build their own hardware and you had these colos and data centers. And it was
this thing you had to do and then all of a sudden AWS changed the scene. And so I've always like
thought that way. I'm open to us, you know, saying, okay, we are going to have to build stuff
ourselves or maybe vertical integration turns out to be more important over time. But it's really nice
to get to focus on like research and product and do what we're good at. But one of the reasons I ask is
it creates this sort of friend enemy frenemy situation, I imagine, where you have your product,
you know, I was actually using it to check out a couple things just yesterday.
How to do great. Happy to hear it. By the way, we're gonna have Sundar here later. The search
product is fascinating what you're doing. It is really cool. I have to say I
it has my favorite product we've launched. Thank you for saying that. It's my favorite product we've
launched in a long time. I use it so much and I'm so happy with it. It is like completely changed my
usage of the internet. But where I was going with this is you have this relationship where you have
Microsoft that's using your product in Copilot. You now have Apple using your product.
And then you have your own version of a sort of native product that you sell commercially yourself.
At some point, do the interests no longer become aligned? And I say that because there's been some
reporting including in the New York Times that suggested there has been frustrations about how
much processing power and access you're getting, different requirements of things and services that
folks at OpenAI are supposed to use that are Microsoft products that maybe they don't want to use.
I'm just trying to understand all that. There's definitely at various times like real compute
crunches. Again, our product has scaled like two years ago we had almost no business. Now we have
more than 300 million weekly active users. We have users sending more than a billion messages a day to
ChatGPT. We have a very large developer business. I think it's like 1.3 million US developers, many more
in the world. So we need lots of compute more than we projected. And that's just been like an unusual
thing in the history of business to scale that quickly. And there's been tension on that. I have
not heard people like upset about using Microsoft services though. You know, we have a big platform
business. We have a big first party business. Many other companies manage both of those things. And
we have like things that we're really good at. Microsoft has things they're really good at.
Again, there's not no tension. But on the whole, like, I think our incentives are pretty aligned.
It sounds like there's a little bit of tension.
For sure.
On the artificial general intelligence piece though, because you always said part of your deal with
them is that if you ever get there, right, that then the deal could technically be called off.
It sounds like you might be getting close.
We've also said that our intention is to treat AGI as a mile marker along the way.
We have left ourselves some flexibility because we don't know what will happen. But my guess is we
will hit AGI sooner than most people in the world think. And it will matter much less. And a lot of the
safety concerns that we and others expressed actually don't come at the AGI moment. It's like AGI can get
built. The world goes on mostly the same way. The economy moves faster. Things grow faster.
But then there is a long continuous continuation from sort of what we call AGI to what we call
super intelligence.
And that's when we should get worried. I mean, I remember when you even the past couple of years,
you were in Washington and other places telling us that we should be nervous.
Well, so first of all, even at the AGI moment, I think there are things to be nervous about.
I expect the economic disruption to take a little longer than people think, because there's a lot
of inertia in society, but then to be more intense than people think. So the first couple of years,
maybe not that much changes. And then maybe a lot changes in the economy. I'm not a believer in the
no work. I think we'll always find things to do. And with every major technological revolution,
there's a lot of job turnover. But I would bet we will have never seen it go this fast. So I think
there's things like that to worry about in the relatively near term. But not these super dangerous
things. That stuff, I think, is further out than a system that can, you know. Do you have any faith
that the government or somebody is going to figure out how to avoid that?
I have faith that researchers will figure out how to avoid that. I mean, there are going to be some...
Let me caveat that a little bit. I think there's a set of technical problems that the smartest people
in the world are going to work on. And, you know, I'm a little bit too optimistic by nature, but I assume
that they're going to figure that out. And we're working super hard. Others are working super hard.
I think there's a lot of work in front of us. But I also think we have this magic, not magic, we have
this incredible piece of science called deep learning that can help us solve these very hard
problems. I assume we'll get that right. The societal issues around not just AGI, not the thing that we
think will come in a few years that can do a bunch of jobs and create a lot of economic value, but like
true super intelligence, the system that is not just smarter than you or smarter than me, but smarter
than all of us put together, just unbelievable capability. Even if we can make that technically
safe, which I assume we'll figure out, we are going to have to have some faith in our governments. There
are going to have to be some policy issues around that. There is going to have to be global coordination
to a degree that I assume will rise to the occasion, but seems challenging.
Okay. So here's the safety question. And we've all read the headlines. You've been in the headlines
over the past year or two years. There've been so many people both inside OpenAI who've either left
or spoken out or other things where they say, these guys are not focused on safety enough,
right? You have to trust questions. Just speak to it. So we understand when people say that there's not
enough focus on safety. You think what? What is that? What is not happening that is supposed to be
happening? Well, I would point to our track record. I would say, you know, we've talked about, we got
this like big product out there. This is a very new technology moving very quickly. It started off as
something that we did not know how we were going to align very well. And it is now generally considered by most
of society to be acceptably safe and acceptably robust. Safety, that's always like a contract
negotiated by a whole bunch of stakeholders. It's maybe hard to like exactly define what it means
for child CPT to be safe. But the reason I ask is when we hear, you know, different people who have
left the organization and they've gone public, they take to Twitter and they say, these folks are not
focused on safety. Is that about resources on safety? Is that about processing power? Is that about
your attention? There are definitely people who think that ChatGPT is not sufficiently safe,
that it allows things that it shouldn't or doesn't do things that it should. Or there are people who
will push on parts of that. There are people who will say, you know, fine, ChatGPT, maybe that's like
a little bit safer than we thought it was going to be. But what's the plan for the next system? What are
you gonna do about that? And, and also there are people who would say like, putting out a system like
this at all is unsafe, because open AI accelerated a race in the world. And that gives us less time to
work on safety. And so the like, we believe in, you know, this is an opinionated stance, that this
idea of iterative deployment is really important. We got to put these systems out into the world,
society and the technology have to co evolve, you have to start while the stakes are lower, you have to
understand how people are going to use this and what it doesn't work for what it does. There are other
people who say, you know, sure, there's some benefits there, but it's not worth the costs.
Right. It's a different question. And maybe it goes back to the issue of processing power. And I was
going to mention, Elon Musk is building his own processing, you know, these huge processing power
labs effectively to power XAI. And one of the reasons I was asking whether you needed your own is because
it seems like everybody is getting their own. We were just talking before to Ken about Elon Musk.
Elon Musk has a sort of unusual relationship in your life over the years, both as a founder and
somebody who's suing the company now. How much do you see XAI as a competitor? We talk about Google,
obviously, all the time. And we talk about Anthropic, and some degree, Microsoft, and I don't know where you
think Amazon is. We'll talk to Jeff Bezos this afternoon. But how do you see that?
They're a competitor.
Do you think of them as one of your biggest competitors? I mean, it appears that they are
coming on super strong. Yeah, I assume they'll be a really serious competitor.
Did you expect that? Yes.
You did? Yes.
And what are they doing? And what are they doing? And how have they been able to literally spin up a
company that feels, I'm not claiming it's comparable to what you're doing, but getting super close,
super fast? Meaning, what is that? Well, a lot of the models at the frontier are all pretty close now,
but certainly tremendous respect for how quickly they built that data center.
Is that, by the way, do you fear that all this becomes commoditized as a result? I mean, if it seems
like, I'm not saying anybody can do it, but it just seems like, what is your advantage? And by the way,
what is your disadvantage? I think the technology itself, to the degree you believe deep learning
is just like, sort of like a law of physics, and that we discovered an important new piece of science,
then yes, that part will be done by lots of people. Everybody has their analogy for what AI is like,
you know, I saw Sundar is going to be here, he talks about it, like electricity, a whole bunch of others,
people talk about like the Industrial Revolution, some people talk about like the Renaissance.
The one I like is the transistor. There was a thing, a scientific discovery, that a few companies
discovered first, that transformed our society, that scaled unbelievably well. Like, you know, when people
talk about scaling laws, the best analogy for that, I think is Moore's law. And that came to be used by
a lot of companies and all of the world, like, look at all the things in this room with transistors. But
we don't think about that as a transistor device. And we don't think about Google as a transistor
company, even though they wouldn't be here without one. And I kind of think that's what's going to happen
with AI. There will be shockingly capable models, widely available, used for everything. And it will
be inconceivable to people in the future, that the devices, the products and services they use are not
really smart. And they want to call themselves AI companies or AI products. And in that sense, in some
sense, the AI itself, the kind of the reasoning engine nature of it will become commoditized. And
that's fine. That's good. Like science should be just diffused throughout society. And that's why we're
focused on building things like ChatGPT.
One of the other questions, and it is Elon related, is you are now moving towards having a for-profit
component part of your business. Right now it's a not-for-profit. And he, as everybody knows,
if you read the papers, is suing you over this and many other things.
How do you feel? I actually just want to actually ask you on a very personal level, forgetting about
even the merits of the case. As a guy who has spent a lot of time with him, and you guys founded this
thing together. How do you feel personally today about all of this?
This is tremendously sad. I grew up with Elon as like a mega hero. I thought what Elon was doing was
absolutely incredible for the world. And I'm still, of course, I mean, I have different feelings about
him now, but I'm still glad he exists. And not just because, no, I mean that genuinely, not just because I
think his like companies are awesome, which I do think. But because I think he like, at a time when
most of the world was not thinking very ambitiously, he pushed a lot of people, me included, to think
much more ambitiously. And I'm grateful is like the wrong kind of word, but I'm like thankful. I'm
positive about that. You know, we started OpenAI together. And then at some point, he like totally
lost faith in OpenAI and decided to go his own way. And that's fine, too. But I think of Elon as a
builder and someone who, like, you know, known thing about Elon, he really cares about being the guy.
But I think of him as someone that if he's not, that just competes in the market, and in technology,
and whatever else, and doesn't resort to lawfare. And, you know, whatever the stated
complaint is, what I believe is, he's just like, he's a competitor, and we're doing well. And that's
sad to see. I got to ask you this, there was an article in The Wall Street Journal, you were in it,
Jeff Bezos was in it, and some of the other people who are his main competitors were in it. And there was
speculation in the article that folks like yourself were worried about his influence, not just his
influence in the technology area, but given his close relationship now with the president elect.
I'm actually not. I, I believe that pretty strongly, I mean, it may turn out to be wrong,
but I believe pretty strongly that Elon will do the right thing. And that Americans,
it'd be profoundly un-American to use political power to the degree that Elon has it to hurt your
competitors and advantage your own businesses. And I don't think people would tolerate that. I don't
think Elon would do it. I, it would go, again, lots of things not to like about him, but it would go so
deeply against the values I believe he holds very dear to himself that I'm not that worried about it.
Okay. One last related question on that. And then I actually want to talk a different,
different tech question and about the company, which is one of the things that he alleged is
that he thinks that you guys are now so big that there's not competition and that you're trying to
prevent competition from happening by preventing potential funders of, for example, XAI or arguably
others from both funding you and funding them at the same time. Incorrect. What we said, which is very
standard and what we say is if you invest in us and want to invest in any of our competitors,
that's totally fine. We stop your information rights. You can still do it, but like is a very
standard term for other companies of our scale, you know, where it's going to stop like telling
you our research roadmap and everybody is like, that seems reasonable. But again, it's like, you want
to go invest in them, go invest. Let me ask you a different question just about the company. You did
start it as a not-for-profit. It was a research outfit originally. Now I think there's a view that
it has to shift to be some kind of profit oriented company. Does it have to? First of all,
I like to talk about the reason we started as a nonprofit because I think it sort of speaks to
the answer too. When we started, we did not have any idea that we were going to be a product company.
We also did not know that the amount of capital we needed would turn out to be so huge. If we did
know those things, we would have picked a different structure. It's hard to overstate because it wasn't
that long ago on the calendar, but it's hard to overstate how different 2016 was. This was years
before we had the research that led to language models. This was four and a half years before we
launched our first product. This was like six and a half years before we launched ChatGPT.
All we knew that we wanted to do was do some AI research and that we thought AGI and super
intelligence eventually would be this important thing to the world, and we wanted to like somehow
do something that would be good. At the time we were working on like writing papers, new RL algorithms,
new theories, a way to play video games, a robotic hand, and it was not clear that there would ever be
a product or revenue stream at all. And it wasn't clear that we were going to need one because it
wasn't clear we needed so much money. Right. It became clear after GPT-1 and some other work
that we were going to need to scale and we started and also Elon decided to stop funding us as a
non-profit and we couldn't get somebody else to do it, that we needed to find a way to make a capped
profit. Now we wanted to keep going with a lot of the things that we think are good about a non-profit,
and so we had the subsidiary capped profit that worked for a while and in some senses still does
kind of work and in some other senses is like straining the theory of what a non-profit controlled
org can be and the amount of capital we need at this next stage. So we are and have been for a while
looking at some changes. Nothing is decided. It is, as you can imagine, like very complicated to figure
this out. And the board is like hard at work on it. I don't, in any configuration, like the non-profit
doesn't go away. Like one thing that the board has looked at, for example, is a PBC that a non-profit
owns a huge chunk of and then figures out how to use that amount of wealth for the purpose of the
non-profit. There's other ideas too. Yeah. One of the things I've always been fascinated by is you never
took equity in the company. I think you think about that more than I do. I'm surprised. You get
paid $76,000, I looked. I believe you. And that's it. If the company does have one of these moments,
there's an expectation you will get some equity. There was a bunch of press about that and there's
been like some investor pressure. It is, look, it is weird that I didn't get equity. Did you want it?
You didn't want it? No, I didn't want it. If I could go back in time, I would have taken it just
some little bit, just to never have to answer this question.
And no matter how many times I try to explain to people, like, I am, I have the most interesting,
coolest job in the world. This is like my retirement dream way to spend my time after,
like what was a pretty good career. And people can work on like art projects and not get paid for
that. And no one thinks it's weird or whatever. It just does not come across. So I wish I had taken
some. I don't imagine I would work any harder or less hard. There would be like, I think something
clearer about the alignment I would have had with investors or whatever else. And it would have like,
it definitely would have been easier to raise money. Like there are plenty of investors who have not
invested because I didn't take equity. They say, really, that's interesting. They will not invest
because you don't have equity. It's come up a few times. Yeah.
So if it's, now the company is $157 billion, what do you think is a good number for yourself
if you, if it happens? Again, I think you think about it more. You got it.
We're going to run out of time. So I have to ask a couple other quick questions.
One is, and at full disclosure, we should say, it just so happens.
Can I say one more thing on that point before you go? I know where I'll be quick. Like I,
this is my childhood dream job. Like not every day, like not day to day. I would rather not like,
you know, be beating my head against the wall all the time, but getting to work on AGI and like getting
to like sit in the room with the smartest researchers in the world and go on this crazy adventure.
Like that is what I always wanted to do. That was like my literal childhood dream. And that is
all of the weirdness aside. I think it should at least be understandable that that is worth more
to me than any additional money. Fair enough. Thank you.
Let me ask you this and I would be remiss if I didn't mention it. It just so happens that the New
York Times company has a lawsuit against open AI and Microsoft over training on content. And there's
a lot of content creators and other folks in this room who make their living off of content. And
I'm just so curious, and I know you can't speak to the lawsuit itself, but for those folks who are in
this room who've written books or written articles or made movies or what have you that have had their
information trained on, whether it was on the open web or the closed web or on YouTube or what have you,
what have you, how we should feel. I think, I think we do need a new
deal, standard, protocol, whatever you want to call it, for how creators are going to get rewarded. I
very much believe in the right to learn or whatever you want to call it. And if an AI reads
something, a physics textbook and learn physics, it can use that for other things like a human can.
I think those parts of copyright law and fair use really need to keep applying. But I think there's
additional things that we're starting to explore and others are where, you know, a particular passion
of mine has always been, can we figure out how to do micropayments where if you generate a story in the
style of Andrew Ross Sorkin, you can opt into that for your name and likeness and style to be used and
get paid for it. There's many other ideas too. I think the discussion on fair use or not is at the
wrong level. And like, I, of course, we very much believe in you need one of these right to learn
approaches. But, but the part I really agree with is we need to find new economic models where creators
can have new revenue streams. On the New York Times, look, I don't believe in showing up in someone
else's house as a guest and being rude. But I will say, I think the New York Times is on the wrong
side of history in many ways. We will, we can discuss and debate that. And we'll do that, I think, in
court. Look forward to seeing you. My final question for you actually has almost nothing to do with
technology, but maybe it does, which is you, you have a very exciting thing that's happening in your life
personally in the next year, which you have your child. Yeah. And for all parents out there and
would-be parents who are thinking about AI and what this is going to do to our life and
what our, even what our own meaning is as people, if the machine actually can do things that we really
can't. What we're supposed to do, how we're supposed to think about it, what it does to our own dignity.
What do you think if it, when you have your child and you're, you're thinking and talking to that
child, what do you think you're going to be telling them about this new world we're about to enter?
There's nothing like, I mean, I'm sure having a kid will be even stronger, but just even like really
like getting ready to have a kid, there's nothing like, at least for me, there's been nothing that has
made AGI seem so irrelevant. And like the level of excitement I have about AGI, I thought was pretty
high, but it's so much higher about having a kid. It really like puts into perspective what actually
matters. And I think that is a, I think that's like a specific lens on the general phenomenon, which is
we have been developing incredible new technology for a long time. And each time it's happened, people
have had these conversations, you know, what does this mean? The industrial revolution comes along,
so machines take all of our jobs, what does this mean? Computer revolution comes along, computers take a
bunch of current jobs, what does it mean? And the answer, at least in terms of what it means to be
human, is not very much. The economy will grow. The kinds of jobs people will do will change.
And people will care way more and love their kids way more than they care about AGI and anything else
that any technology can deliver. The sort of the deep human drives are so powerful and have been here
for so long. Evolution, evolutionary drift is pretty slow. That, you know, I think in some sense,
my kids will grow up in a super different world. And in some other sense, it'll be exactly the same.
Sam Altman, everybody. Thank you so, so much for the conversation.
Thank you for a fabulous conversation. Thank you.
Thank you.
