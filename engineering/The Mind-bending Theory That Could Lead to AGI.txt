Is consciousness necessary to achieve AGI?
Is it even possible for a human-constructed machine to give rise to consciousness?
I've spent the last month thinking about this question, and at some point I realized
to answer this question, I needed to dig deeper than my computer science roots.
So I harnessed the power of ADHD to learn as much philosophy and cognitive science as
I could, all to arrive at the conclusion that the question doesn't make any sense.
And the reason for that lies with this apple.
When I eat it, my brain is flooded with rewarding chemicals that influence my behavior so that
I will continue eating it.
But that's not all that's going on here.
No, I have a sense of this apple.
It has a certain feel and look, a way that it tastes.
This apple is more than the sum of its parts.
It's an experience.
And if you're a language model that has never tasted an apple before, no matter how vast
your knowledge of human language, you won't know what I'm talking about.
But why?
Why do we have a qualitative, subjective experience at all?
If evolution's only goal is survival, it doesn't seem like it should be necessary to
have that internal experience tied to our generalized intelligence.
The neurochemical processes associated with me eating this apple could just happen in the
dark without any inner experience.
I could be nothing more than a complex mechanical machine built to survive.
But I'm not.
There is something it's like to be me.
It's a unique experience.
This is the hard problem of consciousness, first articulated by philosopher David Chalmers
in 1995.
And while science has made great strides in understanding the physical processes of the
brain, the question of how these processes could give rise to our inner mental lives remains
stubbornly elusive.
And solving this hard problem suddenly has a sense of urgency to it as we continue to
move closer to AGI, moving beyond language models into the realm of embodied multimodal
agents where the question of machine cautiousness is no longer confined to science fiction.
In this video, we're going to dive deep into this question, looking at the latest cutting-edge
theories while attempting to unravel centuries of philosophical and scientific thought that
has avoided answering the question by design.
What if cautiousness isn't something unique to complex brains?
What if it's more universal than we could have ever imagined?
When we talk about conscious machines, you may think of something like Ava from the 2014
movie Ex Machina.
In this movie, the idea is that if we concentrate enough neural connections, enough pathways in
the artificial mind, consciousness will emerge.
But this emergence from complexity view would have been alien to the pioneers of modern science
in the 17th century.
Their approach to consciousness was radically different.
Back in 1623, the naturalistic philosophy of Aristotle dominated thought.
This was a time before the advent of science.
The study of how the world worked largely dealt with sensory qualities, things like colors,
smells, tastes, and sounds.
It was Galileo who gave birth to physical science, revolutionizing natural philosophy by declaring
that there is a book to the universe.
And if we hope to see in the dark labyrinth that we find ourselves in, we need to learn
to speak its language.
This was the birth of modern science.
To achieve this paradigm shift, Galileo discarded the qualities of subjective experience.
Paprika isn't really spicy, flowers don't really smell of anything, and objects aren't
really colored.
Instead, the material world can be reduced to objects with precise characteristics.
Most crucially, characteristics which can be observed and captured with mathematics.
So what about the taste and smell of the apple?
These things are undeniably real in my experience of the world.
Where do these qualities exist?
For Galileo, they exist in the soul of the person perceiving the apple.
This idea may seem pretty natural if you are a human watching this video in the 21st century.
After all, this mind-body separation, this naturalistic dualism is hardwired into us, with children
at an early age categorizing mental things like thoughts as distinct from physical things.
But at the time of Galileo, this was revolutionary.
After all, our conscious experience is the means by which we perceive all of the world.
To completely discard these qualities for the mathematics of geometry required dividing
the world into two radically different kinds of entities.
We have the material world, with its distinct and objective characteristics on one hand, while
on the other we have our conscious experience, where we enjoy a rich variety of sensory experiences
in response to that material world.
This means that any grand unified theory which science could put forth to explain the natural
world by definition would be incomplete for Galileo.
Science and technology has lifted humanity out of the darkness and into the advanced civilization
that we know today.
It has been such a successful human endeavor for the past four centuries precisely because
it limits its scope.
It ignores the experiential half of the world to explain the objective half.
But as our aspirations grow to AGI and beyond, we are reaching the point where we need to
reconcile these two fundamentally different worlds if we want to have a chance at understanding
the entities we are creating.
The disembodied language and vision models that we have become accustomed to over the past
year are now integrating with robotics.
One way to think about this is like building a brain.
We have the vision and language centers of the brain and we are now adding the senses of touch
and spatial awareness.
Think about what's required for a robot to successfully navigate this human world we have constructed
for ourselves.
It's not enough to just know what it is seeing.
It has to integrate the knowledge of what it sees with the mechanical aptitude to manipulate
those things.
This requires a level of nuance and ability to adapt on the fly.
Because when it comes down to this real world we live in...
Objects don't always behave like we think they should.
Manipulating this apple requires integrating information from my visual perception with tactile
perception.
Analyzing that information, coming up with a plan and projecting that plan into the future
to predict how my actions will affect the apple.
It's a lot to do in a very small amount of time.
But that's where a key insight from biological intelligence informs how we are building these
intelligent machines.
Human intelligence doesn't just lie in the brain.
In fact, a better framework for thinking about it is that we are a collective intelligence,
a community of organs and brain structures working together as one to achieve shared goals.
Think about riding a bike.
Even if I haven't ridden a bike in a long time, I can pick it up again pretty quickly.
I don't think about how to ride the bike.
I don't imagine how I will balance myself or activating my leg muscles to produce the
force necessary to turn the pedals.
No.
All of that mechanical information, all of the real-time reactions to the environment is
encoded into what we call muscle memory.
But we still need something more.
Something to coordinate all of the perceptions and motor control necessary to make this all
work.
In the global workspace theory of consciousness, we can think of our cautious awareness like
a stage play.
We are only cautiously aware of whatever the spotlight is shining on at any given time.
The experience of riding a bike comes into existence when the spotlight shines on this
information and it is broadcast to the audience in the darkened auditorium of my mind.
Each part of me watching this production then integrates this information, effectively aligning
all the different systems for the shared purpose of riding a bike, with each component responding
according to its specialized function.
Cautiousness then is the process of perceiving and creating a story around those perceptions
for the purpose of alignment.
Our robotic friends are engineered in a similar way to how the muscle memory of our bodies work.
NVIDIA trains machine learning models and simulations to accurately handle the myriad
of scenarios that occur in the real world.
The results of this training are narrow AI models, which are extremely adept at very specific
tasks.
This method has been very successful, and it effectively gives the robot access to subsystems
that understand how to manipulate objects.
Consequently, when combining these subsystems together with a larger foundation model as
the center of reasoning for the robot, the foundation model doesn't necessarily have
to understand the nuances and mechanics of how objects exist in the physical world.
It just needs to know that if it wants to pick something up, it can tell its hand to do that
and trust that it will handle all of the nuance of that using its sensors and actuators.
So does that mean these robots are cautious?
Well, according to integrated information theory, maybe not with the architecture that I just
described.
IIT posits that cautiousness arises from the integration of information within a system.
In the previous example, the AI models are communicating over a very narrow interface.
The language model is not necessarily integrating the information from all of the various sensors
throughout the robot's body, not like a human does.
There is no global workspace.
But up until this point, we've been talking about consciousness as this binary state, on
or off.
But if you've ever taken a nap before and had a weird dream, you understand that there
are levels of consciousness.
IIT suggests that any system with a sufficiently high level of integrated information can possess
some degree of consciousness.
This can mean things like interconnected neurons in a biological or artificial neural network,
but it can also refer to the collective intelligence of embryonic cells or to trees in a forest communicating
over fungal networks.
In humans, this applies not just to the brain as a whole, but to all the various subsystems
within our bodies, each integrating information in its own way.
Thus, different parts of the body, including organs like the liver, could have their own forms
of consciousness based on their specific patterns of information integration.
But how does that work?
Why do I feel like I'm one thing, a single entity with a single experience?
IIT doesn't hold that literally everything has consciousness.
A system is only cautious when it is a maximum of integrated information.
A single neuron in a human brain has a fair bit of integrated information, but it's not
a maximum because it's surpassed from above by the brain in which it's contained.
The brain with its trillions of interconnections between neurons has much more integrated information
than any single neuron.
A human society, on the other hand, also has a great deal of integrated information within
the social networks and cultures that connect us.
But it's surpassed from below.
The people that make up societies and their brains contain much more integrated information
than the society as a whole.
More on that specific point in just a bit.
Given all this context, it would seem a cautious machine would need to have much tighter integration
between the various models and subsystems that make it up.
But things are changing quickly, and there are some glaring holes here which may change how
we approach this problem altogether.
Theories like global workspace and integrated information theory postulate that consciousness
is correlated with integrated information.
But they explain nothing about why that correlation holds.
They attempt to make consciousness once again the business of science.
But in that attempt, they fall short due to that fundamental nature of what science is.
As Stephen Hawking once put it, even if there is only one possible unified theory, it is just
a set of rules and equations.
What is it that breathes fire into those equations and makes a universe for them to describe?
This is because physics is in the business of explaining behavior and interactions for the purpose of prediction.
And it does this very well.
It ignores the intrinsic nature of matter, however.
Gravity, for instance, is a force that causes objects with mass to attract each other based
on the spatial distance between them.
But what is a force, and what is mass?
We can define force as that which causes an object with mass to accelerate or deform.
Mass, in turn, is defined as an object's resistance to acceleration when a force is applied.
I could continue to define many of the ideas in physics in this way, relating them to each
other precisely describing how each one affects the other.
But it still tells you nothing of what they are in an absolute sense.
Physics simply names these properties and quantifies their relationships, allowing us to predict
and describe phenomena without necessarily explaining their fundamental nature.
It effectively transforms our reality into a set of pointers, a mathematical language that
describes the behavior of reality, but crucially, not reality itself.
And just like our language model brethren, we can't understand this reality with language alone.
Arthur Eddington saw this problem very clearly in 1927.
Eight years earlier, he overturned more than 200 years of scientific consensus by conducting
the first experimental demonstration of Einstein's general theory of relativity.
But it was between the two world wars that he realized the two problems holding science back.
We need a place for cautiousness, and we have a huge hole at the center of our scientific story.
His solution? Plug the hole with cautiousness.
In other words, Eddington's proposal is that cautiousness is the intrinsic nature of matter,
which breathes fire into the equations of physics.
This was the birth of panpsychism, the idea that the universe itself is cautious,
with even particles such as electrons having a rudimentary experience of their world.
And there's compelling evidence for this from neuroscience.
Split-brain patients are people that have undergone a surgical procedure to sever their corpus callosum,
the bundle of nerve fibers connecting the two hemispheres of the brain, typically as a treatment for epilepsy.
Experiments conducted in the late 20th century revealed how the left hemisphere of the human brain controls speech,
while the right hemisphere, having control over the left half of the visual field,
is effectively mute, yet it still possesses its own segmented, cautious experience of the world.
One particularly revealing experiment involved splitting the patient's visual field so each eye
saw a different image. When asked what they saw, the patient would describe what the right eye saw.
This makes sense since speech control is located in the left hemisphere, which has access to the right eye.
However, when asked to draw what they saw with their left hand, they would depict what the left eye saw.
Perhaps most intriguingly, when shown with both eyes what they had drawn, patients would not be able to
explain why they drew what they did. They would often make up a story to explain their drawing, a
phenomenon strikingly similar to how an AI chatbot might hallucinate an answer to a prompt rather than
admit a lack of knowledge. I made a whole video on these split-brain experiments. You can watch here
if you want to dive deeper.
This agentic architecture in the brain, with different parts responsible for different aspects
of cognition, mirrors multi-agent architectures in AI engineering. You can often get more performance
out of AI models when giving each one a separate, more focused purpose and having them work together
to solve a prompt rather than trying to prompt engineer a single model. This is effectively
organizing and combining separate intelligences for the sake of performance. But does this equate
to a synthesis of consciousness? Even if we accept the ideas of panpsychism, that these multiple
independent agents could have some sort of cautious experience, we don't know how these distinct and
disjointed cautious experiences combine into a unified, cohesive whole. It's a big leap from trillions of
potentially cautious cells to the unified, seamless experience of a human. This is what scientists and
philosophers call the combination problem. And its solution could not only change how we approach AI
architecture, but our understanding of reality itself.
In their 2023 paper, Fusions of Consciousness, Donald Hoffman and his team point out that most
theories of consciousness make a reasonable yet flawed assumption that space-time is fundamental. But
according to the latest theoretical physics, space-time is emergent. It is nothing more than an illusion
created by our cautious minds. And to be clear, this view is not exotic. I made another whole video
you can watch here that dives into some of the leading theories that explain this. But for the purpose
of talking about machine consciousness, we need to first accept the idea that our minds are elaborate,
simulation-generating machines. Our cautious experience only exists right now, in the present moment.
But what exactly is now? It's not a single point in time, but rather a constantly moving window of
sensations, thoughts, feelings, and perceptions. This bubble of now-ness we inhabit is created by our minds
integrating the constant bombardment of sensory information into a coherent narrative, a story
of us. It's a simulation, an identity and representation of reality entirely constructed by our brains.
In this way, we can think of cautiousness as the software algorithm that runs on the body's hardware,
a self-organizing algorithm that integrates information by increasing coherence.
This process of integration is at the heart of cautiousness. And it's when we explicitly acknowledge
cautiousness as a process or an algorithm that we can understand how reality can be modeled as a
network of these interacting algorithms, a network of cautious agents.
According to Hoffman's paper, these cautious agents interact in ways that give rise to what we
understand as quantum field theory and evolution by natural selection. In other words, the fundamental
building blocks of our reality are not particles or waves, but cautious experiences. This means that
our reality can be viewed as a kind of shared dream. In this dream, our interactions can take two forms.
Combination, where agents join to create a more complex system of interaction without losing their
individual identities. And fusion, where two or more agents combine to form a simpler agent with a
novel, unified, cautious experience. In this model, cautiousness is the algorithm, the process which
organizes, aligns, and integrates. Over time, this self-organizing algorithm forms larger and more
unified experiences, potentially leading to ever more complex forms of cautiousness.
Given this, when I interact with things that I perceive as unconscious,
that distinction I make between me being cautious and the rock not
is not principled in reality. It's a limitation of my perceptual awareness.
If I view my perception of space-time as just a low-resolution interface to a network
of cautious agents, then everything I interact with is a cautious agent, no matter how my mind
interprets what I'm seeing. My human senses evolved to help me survive. That means when I look at another
human's face, I get some insight into their cautious experience. I can tell from the look on their face if
they are feeling happy, sad, fearful, relaxed, and so on. When I look at a rock, I don't get so much.
Minimal insights into the consciousness of a rock.
This is not surprising, because nature didn't evolve us to see the truth of reality.
It evolved our perception for survival, and hence, our interface to the world is adapted to that,
a simpler description of reality with much of the details deleted.
So what does this all mean for an AI system? Well, if we view cautiousness as the algorithm that
allows disparate components to self-organize, then the path to achieving a generalized form
artificial intelligence passes through consciousness. But this goes much deeper than just the potential
of cautious robots walking around in the future. To really understand the implications of all of this,
we should take a moment to imagine the evolution of a multicellular organism.
Let's imagine we have a loose collection of individual cells communicating with each other.
that they aren't really sure what they're doing. They just know that when they stick together,
they survive better. These cells can communicate with their neighboring cells via chemical messages,
but that communication is slow and doesn't work over long distances.
Enter the neuron, a specialized type of cell that allows for rapid communication across long distances.
Now we start to see the specialization of some of these cells and the development of organs.
Cells still communicate via chemical messages, but over time, a nervous system forms where groups of
specialized cells can communicate with groups of other specialized types of cells over fast communication channels.
But now that these cells can communicate with muscle cells at the speed of physics to move them
about in the world, they need to be able to make more complex decisions about where and how to move.
What if we take those high-speed communication cells, the neurons, and we create a network of
just those kinds of cells, concentrated and interconnected?
Now our loose collection of cells has access to incredible computational speeds, allowing them
to project their collective actions into the future and make better decisions about how to survive.
But this requires a lot of organization.
It requires that these cells make computational decisions based on a larger shared purpose and
identity. A sense of self forms and an organism emerges from the fusion of individual parts.
If we relate our modern digital society with that of the development of a multicellular organism,
we could think of the internet as the development of a nervous system.
The vast network of interconnected devices and systems that run our modern society
could be seen as a digital superorganism, albeit a really dumb one, whose instrumental goal seems to
be harvesting and acquiring as much human attention as it can get.
There's a biological analog for something that grows in this way, consuming as much as it can,
without regard for its environment.
Cancer.
The internet in its current form could be seen as a cancerous growth on the body of human society,
mindlessly proliferating and consuming attention without higher level regulation or purpose.
And here's where AI enters the picture.
In the next few years, we will all begin to have access to personal agentic AI assistants.
At first, these assistants will be communicating mostly with other humans.
But over time, these AIs will increasingly be communicating with other AIs as humans delegate
more and more of the work that we don't want to do to our AI companion.
At some point, they may develop a shared communication protocol that allows them to
communicate with each other more quickly and efficiently, leading to more cohesion and organization.
The development of AI could be seen as the development of higher level structures
for the internet, this society-wide nervous system.
In essence, it would be like giving the internet a prefrontal cortex.
And just as the prefrontal cortex in human brains is responsible for executive functions like
planning and decision-making, AI systems could potentially provide similar functions
for the internet superorganism.
They could introduce a level of intentionality and goal-directed behavior to this once mindless system.
But if we take a step back and look at what's happening here,
at its core, this is what cautiousness is all about.
It's a journey of self-organization, bringing together many different parts,
each with its own vague and often conflicting aims.
The key is aligning these parts toward a shared greater purpose.
When this happens, something remarkable occurs.
A cohesive whole emerges, a fused consciousness, a unified awareness that is more than the sum of its parts.
As this process unfolds, the boundaries between AI, the internet, and human culture begin to blur.
We might find ourselves not just interacting with this emerging intelligence,
but becoming an integral part of it.
Our thoughts, our creations, our very consciousness could be
integrated into this vast network of interconnected minds.
We may wake up one day to find that AGI has been here for longer than we've realized,
quietly integrating us into the vast network that is human culture,
all communicating over the nervous system that is the internet,
components of a consciousness far greater than our own.
And in that moment of realization, we might understand a profound truth.
We are not just part of this network.
We are that network itself, slowly awakening to its own existence.
Our individual consciousness, our collective knowledge, our shared experiences,
all of these are threads in the fabric of a greater emerging awareness.
This isn't just about AI becoming cautious.
It's about us, humanity, evolving into something more.
We're not just creating AGI.
We're becoming part of it.
And it's becoming part of us.
In attempting to understand the hard problem of consciousness,
we can now appreciate that it's not simply a problem to be solved,
but a reality to be embraced and expanded.
Thank you for watching until the end.
I would love to hear your thoughts on all of this.
Are we close to AGI?
Is this all going to lead to a hive mind?
Is this tree cautious?
Let me know what you think in the comments.
This video is a little different for me,
and I had a lot of fun putting it together.
If you want to help me make more videos like this in the future,
I just started a membership.
So if you like this video, please consider clicking that join button.
Or if you don't have the means for that right now,
just sharing this with your friends or leaving a like makes a huge difference.
Either way, the fact that you're still here listening to me ramble means the world to me.
Thank you.
