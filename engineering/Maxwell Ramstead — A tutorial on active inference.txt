so just I just want to express my gratitude to everyone who's come today thank you for coming
so my name is Maxwell Ramsted I am a postdoctoral fellow here at the Jewish General I work with
Lawrence and I work mainly on this approach called active inference that I'll be presenting today
so my talk appropriately is called a tutorial on active inference from the predictive brain to
sociocultural regimes of expectation and the talk basically is going to come in four parts
yeah so the first is basically a motivation of the problems that we're trying to address using
active inference so I'll present on the one hand the problems per se that active inference
addresses like the space of problems in which active inference lives and then I'll move on to
a few more specific motivations about why an approach like this might be interesting then
I'll turn to what's known in the literature as predictive processing which is basically this
active inference scheme applied to the brain then I'll move to active inference per se these two
sections will kind of come together I mean the the predictive processing part is just the application
of this framework to the brain so they're not all that distinct although you know if you get into the
the more technical debates there there are some distinctions but we can return to that and finally
if time allows I'll I'll discuss some of the multi-scale extensions of active inference so going beyond
models of brains and cells to like this kind of integrated view of systems of systems of systems that
are nested the ones within the others and the kind of move together thanks to this active inference
framework so first I guess a discussion of the motivation and problems so basically the problem I think that
active inference helps us solve most clearly is the problem of multi-scale systems so this is a picture of
McGill University in slightly warmer times so McGill University is a is a complex system it's a system that's composed of several different parts right some of which are living some of which are not
right so the faculty the student body the support staff right but also all those are living and some things that arguably are not right like the buildings right the rest of the physical infrastructure labs and so forth well if you zero in on one component of this the living ones right so say the student body in McGill is itself composed of bodies right and so forth
of the physical infrastructure infrastructure labs and so forth well if you zero in on one of the student body
of this the living ones right since then body is itself composed of bodies and bodies as we know are composed of cells and cells make up organ networks right that eventually make up organisms organisms themselves interact with social group and eventually make up off social networks one of which may be the most evil I posted on bots
eventually make up organisms. Organisms themselves interact in social groups and
eventually make up social networks, one of which may be the most evil I've posted
the image of right here. So systems within systems within systems, right? We are
systems within systems within systems. That's kind of the takeaway of this
slide. So if you look at the structure of the brain, the brain structure in a
sense recapitulates the structure of the environment in which it's encapsulated.
So if you look at the brain structure, the structure of the brain, this is from a
paper by Park and Friston. What you'll notice is basically a repeated
encapsulation of networks within networks within networks, right? So, you know,
dendritic formation at one level, you know, and if you zoom out a bit you have
networks of neurons and then if you zoom out even more you have brain regions
interacting with brain regions and so forth. And what we what we notice is that
there's a there's a a segregation in the brain, a kind of spatial temporal
segregation of the interactions between brain regions that in a sense corresponds
to and captures the regularities of the kind of statistical segregation that we see
out in the out in the world around us, right? Things change in relevant ways at
different scales, at different spatial and temporal scale. So this is something
that we'll return to. And so, you know, up until very recently this was lacking in
the literature, but suppose we wanted to study all these levels, you know, together in an
integrated fashion, you know, recognizing the interest and dignity of all these
levels, right? So suppose we wanted to construct a framework that was able to
address, you know, I've plotted here, this is from a paper that I wrote with
Karl Friston and Paul Badcock a while back, I've plotted spatial dimensions on the
y-axis and temporal dimensions on the x-axis. So what we would like is a theory that
can, in an integrated fashion, offer an explanation for the way that systems
behave from the subcellular level through to the cellular level, the level of
tissues, organs and organ networks, two organisms, all the way eventually to
speciation, the construction of specifically designed niches. And from a,
from a, this is from the spatial point of view, but from the temporal point of view,
we would also like an account that's able to account for the variety of temporal
scales that are involved in the phenomena of interest, right? So from mechanistic
processes occurring over milliseconds all the way to, you know, phylogeny and
adaptive radiation which span, you know, hundreds of thousands if not millions of
years. So, so that's one, that's the motivation of the problem. How would we, how
would we address all these different spatial and temporal scales in a
principled way? What kind of framework would be able to enable that? Okay, so that's the
problem. One motivation for thinking about active inference is particular, in
particular is a, it's an observation. It's that most self-organizing systems in
nature tend to dissipate, right? So from galaxies and stars to lightning bolts and
tornadoes, almost all, really like all, and I mean that in a very strong sense,
almost all systems that self-organize in nature self-organize to equilibrium. So
what does this mean if we had to unpack it in a kind of basic way? Well first, first we
should note that these self-organizing systems consume the gradients around
which they organize, right? So if for instance you think of a lightning bolt, a
lightning bolt self-organizes around a charge gradient, right? And in striking, the
lightning bolt consumes the gradient around which it's self-organized, effectively
leaving the entire system at, at equilibrium. So the same could be said for
tornadoes, although, although now it's not a charge gradient, it's a temperature
gradient in a weather system. But the, the main takeaway is that for almost all
systems in nature, self-organization serves to increase entropy. So in, in this
context, entropy is a measure of spread. You can think about it roughly as a
quantification of how many different configurations the system could be in,
right? So if you think of a gas versus say a crystal or, or water or a solid, in a
gas, the constituent particles could be in any number of different
configurations, right? Like this, this one that I'm pointing to at the end of my
finger, this little molecule of gas might be at the other end of the room in
like, you know, 20 minutes. There's, there's, there's less constraint as to
where the, the, the little particle will end up. Whereas if you think of a
crystal, well, the, the kind of regular latisse structure in a crystal means that
there are only ever local interactions and that you can predict with a high
degree of reliability, what any different, what any, any given particle, what
state it'll be in, right? So high entropy means a high number of available
configurations, low, low entropy means a low number of available configurations.
So, um, so back to this, uh, most systems in nature, right? Self-organized to
equilibrium. They, they dissipate, they increase entropy. Um, but other systems,
uh, self-organized and do not write a bunny rabbits and tigers. And, uh, you know, I
don't want to make assumptions about you, but I assume you as well. Don't self-organize
to, uh, equilibrium, right? We, we self-organize to this regime of states
that's in effect very far from equilibrium, right? Uh, I, I don't exist at room
temperature. My core body temperature is about 36 and a half degrees Celsius. I
imagine yours is roughly the same. Um, you know, so we, we, we managed to stay
far away from equilibrium. Uh, and the question is, well, how, how do we manage to
do this? Um, so here's going to be my first kind of technical excursus. I'm
going to, uh, present to you, uh, the state space formalism from dynamical
systems. Uh, and this will help us think a bit more formally about what I just
said. So a state space description, uh, is something borrowed from dynamical
systems theory. Uh, and the idea is to construct an abstract space that
represents all possible states of a system, right? So it's, it's literally that
we're, we're just constructing a state, uh, a space, right? So, uh, like a, a
mathematical object with dimensions that you can kind of move around in,
right? And this space is going to represent all the possible states that
our system can be in. And so how do we do this? Well, basically for every way
that the system can change, right? AKA for every variable in the system, what
we're going to do is plot one dimension in this space. So we're going to say,
okay, this, this variable corresponds to a dimension in the space that we're
constructing. So suppose you have a very simple physical system, uh, with a
temperature and a size, right? Then you might represent it this way, right? One
of the dimensions corresponds to temperature. The other dimension
corresponds to size. And so what you'll notice already from the setup is that a
point in this space here is already a complete description of the system that
we're interested in, right? Just so to recap the argument, every dimension in this
space corresponds to a variable in the system. Therefore, every point in the
system corresponds to assigning a value to every variable. So essentially a
position along the dimension, right? And so that might seem like a banal
observation, but it allows us to, to say interesting things about what you might
call the intrinsic geometry of a system in terms of its phase space. So for
example, you might want to say, well, in, in the whole space of possible states that
the system could be in, there's this particular region that has these
particular properties that when you're in this area, something interesting
happens, right? So as an example, uh, here again, I'm just using the same thing,
right? Plotting the temperature on the x-axis and size on the y-axis. Well, you
might say, well, most living things that exist, exist in this sub region of that
space, right? Most living things exist somewhere between zero and a hundred
roughly degrees Celsius. And most living things are somewhere between, uh, how did
I put it? Like 10 centimeters and a hundred meters. If you're looking at like
these very large colonies of, of cloning trees and stuff like, so this is the kind
of thing that you can say, uh, using the, uh, state space formalism. And one of the
interesting things, uh, is that you can do with this is describe trajectories over
the phase space, right? So this is sort of like, you know, literally drawing a
trajectory over this phase space and saying, okay, well, uh, the system starts
here and then some perturbation moves the system towards another state and then the
system has to cycle back. So for example, I wake up in the morning, I'm a bit hungry,
right? Uh, this, uh, is somehow related in an interesting way to my, uh, you know,
making coffee and breakfast and then I eat and then I'm satiated and then I move to
another area of the phase space that is me satiated, right? But, you know, I, I
consume, uh, sugar, uh, you know, as I, as I, for example, give this talk. So, uh, sure
enough, I'll be hungry later today. So then I'm back where I was. And then this, you
know, so the phase space description allows us to say something about the
dynamics of a system. It, it allows us to say something about the way that a
system states evolve over time. Um, so, uh, basically the reason I'm talking about
this is that active inference, uh, is a theory of provides a mechanics of or tells
us how living systems are able to do this, right? So we, we all exist as living
things in a bounded set of states, right? We, we don't, uh, self-organize to
dissipation like these things, right? We, uh, self-organize to a very well-defined
set of states. The question is, how do we, how do we do this? And active inference
provides a, uh, an answer to that question. Active inference tells us about how
things can stay away from, uh, basically, you know, states that they, they want to
avoid and stay in a regime of states that are compatible with their existence.
Okay. Second motivation, uh, which you might call the animal's perspective or the
Bayesian room. So if you really, really, really want to caricature the position, uh,
that organisms are in, basically, uh, organisms are in what you might call the
Bayesian room. Um, Chris Elias Smith has called this the animal's perspective. So
the, the idea of the Bayesian room is that organisms only ever have access to
their sensory input, right? Uh, and the sensory input isn't always very reliable.
Uh, the, for those of you who do neurophysiology will know that, uh, you know,
the sensory motor channels that we deal with are just intrinsically noisy. Uh, they
have to deal with movement and variance and there are varying trustworthiness as
well. Uh, that's kind of context dependent. So, uh, uh, you know, I spent a lot of time
in London last year and there's a lot of fog. So you learn, for example, not to trust your
eyes as much and to listen more than you would, for example, in Canada, when there, where
there's very little fog. So, uh, this kind of, uh, this kind of problem that the organism
has to solve is a reverse inference problem in the sense that it has access only to its
sensory input and, uh, it needs to determine what caused its sensory input. So, uh, you
know, you, you might here, I've got a, a Spock ear connected to a brain, right? A very simple
sensory system as, as, as occur in nature frequently, right? Uh, so suppose my Spock
ear system, here's a snap, a crack of some kind, right? Well, uh, the, what caused the
snap is, uh, you know, relevant, right? Sometimes for the survival of, uh, my, the Spock ear
organism in this case, uh, it, maybe it was just, uh, the wind moving, uh, you know, the
branches of trees and a twig snapped. Maybe it was a tiger, right? And the difference is
relevant, uh, cause the brain, uh, we mustn't forget that its main function is to coordinate
movement. Uh, in a, I, I forget what the, um, the animal is there. There's a, uh, there's
a very simple animal that lives in water and it has a nervous system for the first phase
of its life. It moves around. And in a metaphor for tenure, what it does when it reaches maturity
is it attaches to a rock and it digests its nervous system. It's the first thing it does
when it doesn't have to, when it doesn't have to generate, uh, movement, right? When you don't
have to coordinate movement anymore, you don't really need a nervous system. Um, so
yeah, uh, so out of multiple possibilities for action, the brain has to determine, well,
so what exactly caused the sensation? Was it a tiger or was it just a twig? And has to coordinate
action, you know, the, in an adaptive way as a function of, uh, its inferences. So this
is another motivation, right? Active inference is useful because it tells us how this happens.
Okay. That was the motivation introduction bit. Uh, we can now move to, uh, predictive
processing unless there are questions about that opening part. Are we good? Alright. Okay.
So predictive processing, uh, is a theory of how the, uh, brain works. And it's essentially
a, uh, uh, an instantiation of active inference applied to the dynamics of the brain. And it's,
uh, it's, it's a good starting point to understand active inference, um, in part because historically,
this is how, uh, the sequence, uh, kind of originated, right? Uh, these approaches were developed
in theoretical and computational neurosciences. And only recently, uh, through the work of,
uh, I mean, you know, our group at McGill and, and the work of others, uh, has been extended
to, to model social and cultural phenomena. Uh, so we'll start with the brain. Um, so,
you know, uh, neuroscience one-on-one, the traditional view of cognitive processing that you'll learn
in any neuroscience class, uh, is that the brain is essentially an aggregative bottom-up feature
detector. So we'll, we'll unpack this, uh, sequentially. Um, the brain is, uh, first of all, the feature
detector, meaning that, uh, the, the more, uh, sensory areas of the brain that are supposed to be like
lower down on the processing hierarchy, uh, what they do is essentially detect features, right? So
in the primary visual area, for example, you're sensitive to bars. And then as you, uh, ascend the
cortical hierarchy, you, uh, and this is the other keyword, uh, aggregate, right? These statistical
features together until you arrive at something like, uh, a complex percept. Um, so two things to
note about, uh, the traditional view. On the traditional view, the bottom-up signal is the
one that's driving the show, right? Uh, essentially what, what the brain is, uh, kind of cast as in this,
uh, on this view is a kind of passive collector of sensations, right? That, uh, get, uh, uh, combined,
aggregated together, um, and yield, you know, the, our world of experience, right? Um, another thing to
note is that, uh, on this view, the, the descending connections, right? That descend from so-called higher
areas, uh, like the prefrontal cortex back down to the sensory areas. These are seen as modulation or
feedback, um, right? So the, so on this view, the brain is a passive, uh, organ. It receives sensations,
it aggregates them, and the top-down stuff is, uh, just feedback. So there are a few weird things
about this, uh, for people who, uh, study the brain in more detail. One thing to note, for example, is
that roughly 80% of the brain's connections are doing this feedback thing, right? Are descending.
Um, so it, in a sense it would sort of be surprising that 80% of the brain's energy budget goes to
something that's just feedback, right? Uh, so, I mean, another, uh, for, uh, uh, someone trained in
philosophy like myself, another, uh, very important problem is like, where is meaning supposed to emerge
from this, right? So somehow I'm supposed to be aggregating geometric features of stimuli, and then
meaning is supposed to pop out of that somehow. Um, it's not clear how that's supposed to happen. Uh,
as is often the case in neuroscience, we, we, we give a nice little name, uh, to, to our ignorance,
essentially. We, it's called the binding problem in cognitive neuroscience. So how, how is this
information bound together to produce a meaningful stimulus? Uh, I mean, it, you know, what we think is
that this is maybe just the wrong way to look at the problem, that maybe no answer is forthcoming.
Um, so the predictive processing viewer, the active inference view, flips this on its head.
And says, okay, well maybe, maybe this top-down thing is what the brain is mainly engaged in.
Maybe that's the main activity that the brain is, uh, busying itself with. Um, so from this point of
view, uh, the, the main activity of the brain is essentially to produce these kind of top-down
predictions about what it should expect to sense next, right? And, uh, what flows up, right? So what's
traditionally, uh, thought of as the signal, right? Uh, is, uh, is what we call a prediction error.
Uh, so it's essentially the difference between, uh, what was predicted at a given moment and what
is actually sensed. I'll return to this in a second, uh, like in a lot of detail, but a few things to note,
uh, first, uh, I guess the most important is that really conceptually it, it flips the traditional view
of like how the brain works on its head. Uh, because what we're basically saying is, well, the, the top-down
activity is the main driver and essentially sensory, uh, information is feedback, right? Uh, the, the,
the feedback is not what we thought it was in a sense. Uh, so it's, it's sort of like moving from
a conception of the brain as a passive collector of data and a combiner of, uh, you know, geometric,
uh, or, or statistical information. Uh, moving from that to a view of the brain as a kind of query
machine, right? It's sort of, uh, perception is like a Google search, right? You, you, you ask a
question and you get an answer. This is so, you know, a visual saccade is as asking the world a question
essentially. So perception is an answer to that question. So it's a complete inversion of the received
view in that sense. So now we're going to get, uh, a bit more technical. So how does this work?
Um, it works via, uh, this broad framework, uh, that we, we can call generative modeling. Uh,
you're going to see this a lot, uh, in the following slides. The scheme is always basically the same.
You have two things, uh, two, it's, so when I say the active inference is simple, I, I, I mean it, uh,
you can essentially explain it with two circles and a line as I'll try to do now. Uh, so the two circles
here represent on the one hand data, right? The data that's available to, to us, uh, so observations,
things that are capable of registering observations or measurements on the one hand,
and, uh, on the other, the, uh, hidden or latent, uh, factors that actually cause the data that we're
observing, right? Uh, so in this graph causality flows from left to right, right? You have states in
the world that cause the blue arrow, the data that we're observing on, on the right here. Um,
and so, uh, what we want to do if we go back to this problem that I presented of, you know, the,
the brain having to, you know, solve this inference problem, right? Is it a tiger? Is it a twig? Uh,
so basically, uh, the brain, uh, the brain according to active inference at least what the brain is busy
doing is kind of reversing this arrow, right? Uh, so the, the arrow of causality goes from hidden
states to data, and what we want to do is move from the data that we have to an inference of what the
most likely, uh, uh, causal, uh, factors are that caused the data, right? Um, so this is, I say data in a
kind of broad sense because it's important to keep in mind the historical context in which these
methods were initially developed. Uh, my mentor at UCL, uh, Carl Fristen, uh, developed, uh, these
generative modeling techniques for fMRI initially. Um, so in fMRI, which you have is a bold signal,
and what you want to infer is, uh, the underlying neurobiological activity that caused the bold signal.
So, uh, just by show of hands, how many of you are familiar with neuroimaging and like more
technically? Okay, so a fair bit of you. Just as a refresher for those of you who don't know, um,
when you do fMRI, you're not directly measuring brain activity, right? Uh, these fancy heat maps of
the brain that you see are not direct measurements of brain activity. What we measure is, uh, is a proxy
for brain activity, right? So it's a, uh, it's, uh, basically a measure of, uh, this, this bold signal
stands for blood oxygenation level dependent signal. It, it essentially, uh, consists in put,
putting someone, you know, in a brain scanner, which is basically a big magnet. And then you measure,
uh, essentially a byproduct of oxygen consumption, uh, by neurons in the brain, right? So what you're,
you're measuring is an effect of, uh, the neural activity that you really find interesting.
And so, uh, what you would like to do then is from this signal, right? From these, uh,
from this data, you want to infer the most probable, uh, neurobiological activity that caused the data.
Uh, so this is the technique that was pioneered in the nineties, uh, by, uh, by Carl Friston. In the nineties,
they were using statistical parametric mapping, which I won't get into very much,
but essentially, uh, for, for reasons that I'll, we'll be able to see a bit later,
um, the, these techniques allow us to, to move from this data to a model of, uh, of the, essentially
the connectivity of, of the brain that the most likely connectivity that would have generated the data
that we're interested in explaining, right? Active inference and it's more familiar, uh, format for those
of you who, who work on it, uh, comes in at a second level. What happens when the data that we want to
explain is literally the sensory signals that the brain is receiving all the time. How does that change,
uh, the general scheme? Well, it changes it in the, what we're doing now is constructing, uh, a model,
not just of the underlying neurobiological activity that caused a signal that we measured,
but rather constructing a model of what's, what causes the sensory signals of an organism.
So, uh, you know, the, the, the major part of, uh, what causes our sensory experience is us acting in
the world in various ways, right? Think of, think only of the visual sacchades that you're performing
many times a second, right? You, we are, uh, we are as embodied agents are the main
causal driver of our phenomenology in a sense. So, uh, active inference per se is about characterizing
this, right? Uh, the, uh, it's about characterizing a model, uh, of how our sensory data were generated.
Um, so I'll return to that in a lot of detail later. I just want to quickly say that, uh, there's a new
kind of tier of active inference that, that's being developed right now. This is probably the least
explored most cutting edge application of this, this framework. What does it look like when the
data that we're trying to explain is, uh, outcomes of the diagnostic process,
right? So like the, the data that I'm trying to explain, for example, is, uh, a schizophrenia diagnosis
or a depression diagnosis, right? So then the model that we're writing is a model of the process that
generated the diagnostic classification per se. And for me as a philosopher, this is really interesting
because this is where it gets really meta, right? Like this is where we are part of the model in a
sense. We as clinicians and researchers really are represented in the model that generates, uh,
the data if the data is psychiatric diagnoses. Anyway, the, so the point of generative modeling,
the way that it's, it's been used, uh, for, for a few decades now is, is this kind of very simplified
pipeline where on the one hand you have an experimental setup that generates data that you want
to explain and on the other you have a computational setup that is effectively a model of the process
involved in your experimental setup. So then you get this kind of circular relation going where, um,
your experimental setup provides data, uh, you can essentially, uh, model, uh, or create models of the
most probable causal process, uh, to have generated your data. Uh, that in turn, uh, can be used to
refine your hypotheses, uh, and your methodology, right? That, that you, you're using in your experimental
setup. And so there's this kind of circular motion between modeling work and computational work, uh, that,
that we capitalize on essentially in the generative, uh, modeling paradigm. Um, okay. So what is a good model?
A good model is a model that generates, uh, uh, a small amount of error. So we'll see what that looks
like in the brain. Um, the way that, so remember I, I gave this kind of general framework to understand
what's going on like in the brain in terms of active inference. If we wanted to unpack it a bit more
at any layer, uh, of cortical activity or really of any neural activity, according to this framework,
what's, what's happening is, uh, that at layers above and at the same layer, any unit is receiving,
uh, predictions about what it should sense, right? And, uh, basically, uh, what's going on is a,
is a constant comparison between, uh, what, uh, the brain expects to perceive and what it actually does
perceive. And the discrepancy between these two signals is what gets shuffled up, uh, the hierarchy,
uh, in this kind of fashion, right? So, um, I mean, interestingly, this happens as early as the
retina. Uh, so, uh, I, I mean, I won't get into the, into the neurobiology too much, but the, there,
there's reason to think that this predictive architecture stuff, uh, happens everywhere at
every level that really there's no prediction free, uh, layer of, uh, cognitive activity.
Um, yeah, as early as the retina, like I was saying. So if, if, if we want to think about it
metaphorically, but we're sort of moving towards, uh, a formalism here, like what it means then to
minimize this error quantity is basically you have a cloud of data that you're trying to explain,
right? And you're constructing a model of how that data, uh, was, was generated, like what, what,
and therefore what you should expect, um, uh, of that data. So what you're essentially doing is
fitting a curve to a cloud of data and the prediction error per se is, is this distance here.
It's this distance between the value that the model predicted and, uh, the value that you're actually
perceiving. Um, so there's a, there's a story to tell about thermodynamics and information theory
that I'm going to skip over. Uh, but we can return to, uh, later if you're interested. Um,
so essentially according to active inference slash the predictive processing, there are two,
two ways to minimize this, this quantity, right? This, this discrepancy between what I expected
and what I perceived. Um, the first is to change your model. I think it's the most obvious, right?
Uh, if, uh, if there's a discrepancy between what you predicted and, uh, what you perceived,
the simplest way to reduce the discrepancy is just to change what you predicted. So we just change,
uh, the model, uh, and we'll see what that means a bit later. We adopt a different model and this
model generates less prediction error than the original one, right? Which means that it's better.
Uh, it means that it's more reliably, uh, you know, explaining the variance in your data.
Um, of course it's always possible to overfit. So, I mean, this is just a brief mathematical excursus,
but for those of you who have done statistics, you'll know that like, if you have n data points,
you can always arbitrarily construct, um, a polynomial function of degree n plus one that'll
go by all of the points that you're interested in, but capture none of the trends that you're trying
to explain, right? So I just ad hoc, you know, you probably can't tell, right? Because it's so
gracefully traced. Uh, but, uh, I ad hoc constructed a function here, which I assure you is a very
rigorous one and it passes by all of the data points as you'll notice. But what you'll also
notice is that there is a trend in the data and that this function isn't capturing it anymore.
So you might think that if the brain does something like that and overfits data, uh, then on the long
term, it's going to generate a lot of prediction error. And so, uh, you know, some disorders,
uh, you might think of delusional ideation and this kind of thing is, have been understood in
terms of overfitting. Um, so overall, you're better with, uh, a slightly, uh, simpler model
that generates some error, right? But that on average sticks to the, the trend and the data better.
So this also speaks to a more general, more, I guess, philosophical point here is that in this
framework, the error is your signal, right? When we say like minimizing prediction error, we don't
want zero prediction error because that would mean having zero signal. So one way to think about this
is your model has to be simpler than the reality that you're modeling, right? So, uh, think of a map,
right? A model is basically a map. Uh, a one-to-one scale map would be completely useless,
right? Like if I had a, you know, the, a map of this room would be like, you know, I, I wouldn't be
able to, uh, to look at it and it, it wouldn't, it wouldn't, uh, contain, uh, you know, information
in a way that, that, that, that could be used in a useful fashion, right? The model has to be simpler
than the data. This means that just by construction, there will always be prediction error, right? Just,
that's how it works. You need prediction error because it's your signal. Okay. So first way to
minimize prediction error is, uh, to change your model. Okay. Second way to minimize prediction
error is to change the world, right? So if, if there's this discrepancy, right? You know, you might
want to make your model more like the world, but you might also just want to make the world more like
your prediction. Um, and so in active inference slash predictive processing action itself is
understood as a form of self fulfilling prophecy in the following sense. Uh, you start off with a
prediction of action and you're not moving, right? So I don't know, like I, I don't have a prop handy.
I usually do. Uh, but suppose I had a glass of water handy and I wanted to reach for it. Well,
the way that it's unpacked in the predictive processing framework is I produce a prediction
of movement, but I'm not moving, right? So if so facto, if I'm not moving and I expect to move,
that induces a prediction error, right? Uh, and so the cool thing about using a prediction error
as your signal is that it updates over time in real time, right? So I, I basically initiate
movements that in real time, reduce this prediction error. So it can be used as a kind of
knowledge driven real time signal, uh, you know, that enables, uh, adaptive motor behavior.
Uh, so this is just what I said. You generate an action prediction, you're not moving,
therefore a prediction error is induced, uh, and that can be used to guide online motor behavior.
Okay. Uh, so now, uh, I'd ask you to buckle in really tightly because we're going to do some math.
Uh, so we're going to look at the generative models themselves. I've talked a lot about the models,
et cetera. Uh, so what are these models? Um, so remember the, the general flavor of this is we have
data and we're trying to, uh, infer the, the most probable latent states that cause the data that we're
trying to explain, right? So causality flows in this direction, inference flows in that direction.
Okay. Um, so how exactly do you quantify, uh, you know, how well a model explains your data?
So the way that you typically, uh, will do this is by constructing several alternative models,
right? The, that are each, each encode a different hypothesis about how the data might have been caused.
And then you evaluate, uh, how well that model accounts for the variance in your data. Um, so
if you want to compress that algorithm into like a quantity, what you get is this spooky variational
free energy thing, uh, which, you know, you might also just call the model evidence or a bound on the
model evidence. Basically this variational free energy thing is a measure of how much evidence is
provided by the data for a given model of the process, right? So it's important to stress this,
the free energy isn't an energy in the sense of thermodynamics, right? It's a, it's a measure of
how well your model explains the variance in your data. So I think that's an important takeaway,
you know, to, to, to like understand what is going on here, right? Uh, it, the, let me repeat,
the variational free energy is just basically a measure of how well your model explains, uh, the
data, the variance in your data. Uh, yes.
So you use, um, upper bound a lot in your articles as well, and I just, I don't understand it.
Okay. Well, so the, the model evidence itself, if, if you actually try to compute it,
usually for just for calculation reasons, you won't be able to do it. Uh, typically it's because
these always involve, um, a normalization term. You kind of have to divide by this term and this term
calculating it involves something over an infinite set of states. So basically you can't do it
analytically. And, um, so what you do instead is you construct this quantity of the variational free
energy, which, which is basically an upper bound on the model evidence. What that means is that like
the model evidence, um, the, the, your variational free energy basically tells you, you at least have
this much evidence, right? You might have a bit more, but you at least have that much evidence from
your model for, for your model, right? So, so just to, uh, rehearse this again, right? So always the same
formula, you have some data, you're trying to construct the most probable, uh, model of the process
that caused the data and you get at it through inference. Um, so, uh, so this is what the models
typically look like. And Lawrence once told me that just presenting these models is basically an act of
intimidation. Uh, this one is very consoling and comforting. Thank you. Uh, so, so, uh, rather than, uh,
the engage in this act of violence, I'll, I'll try to unpack a bit what, what these models are, right? Um, so
the most basic generative model is this thing here. And this thing here is just this thing here,
right? Everyone sees, right? You have your data, you have your, uh, most probable, uh, causal states
and a relation of inference between them, right? So here you have, it's just flipped. Here you have
your, your data, right? The most probable causal state that caused your data that you're trying to infer.
And now what we're doing is just adding a bit of mathematical niceties. Uh, mainly we're
parameterizing these relations, right? So this a thing is just a likelihood mapping. And basically
what it does is specify for every state, if this state was actually the case, right? What kind of data
would I expect to see? Right? So for example, if it is night, I would expect it to be dark outside.
This is the kind of thing that this a, uh, characterizes. So one important distinction
to make sense of these, uh, graphs is that the circles are either your data or the quantities that
you're trying to infer, right? And the squares are essentially, uh, parameters of the relations between
these circles, right? So they characterize the arrows between, um, so like I was saying, s is just
your state that you're trying to infer. O is your data. A is your likelihood mapping from your states
to your observations. Basically, like I said, assuming that this state is the case, what,
what is the probability of observing this or that? And D is just your prior beliefs over states. So like
independently of any observations that you might make, what, what do you think the state is? Right? So,
uh, just I'll, I'll run through some of the math, right? Uh, so if you see on the left hand side here,
s equals D is just saying, well, my guess about the state independent of any data is just my prior
about the state. Right? Pretty simple. Uh, this O equals AS thing. Again, it's just describing this
relation that the likelihood mapping has, right? Your, uh, your outcome just is a combination of
this mapping between, uh, your states and observations and the, your beliefs about states.
And, uh, here to the, uh, right, what you see in, uh, a mathematical form that I won't get into too much
is that your, the state that you predict, right? Your posterior estimate over states is equal to some
combination that I won't get into too much of your prior beliefs and, uh, what you learn from your
observations from the data, uh, and how that relates to states. Right? So these look very intimidating at
first, but like we can unpack them in a way that's, uh, okay. So these were the models up until that we
were using up until 2015 roughly. And, uh, in 2014, 15, 16, what happened, uh, was that we started considering
temporal depth and temporality. So here we're adding, and it looks a bit more complicated,
but we're really just stacking the first layer model, right? Does everyone see that? Like, see,
this thing here is just this, right? So, uh, basically the only thing that we've added here is this B matrix
and the B matrix specifies your beliefs about the way that states transition over time. Again,
independently of the observations that you're making, this is very clear from the, uh, equations here to,
uh, the left. So it just says your state at time t plus one is a combination of this B matrix, right?
That captures your beliefs about the way states transition and, uh, the state at time t, right?
Nothing exorbitant or a complicated. And again, the posterior thing is just telling you, okay, uh,
your, your, your posterior belief about, uh, a state, uh, is just equal to some combination of,
uh, your state and B in the past, your state and B in the future and what you're seeing in the present,
right? So again, it, it looks intimidating, but it's actually less complicated, uh, than it is.
Um, uh, I, I note Hesp et al 2020. Uh, this is, this presentation of the material is, uh, was developed
by, uh, one of my, uh, close collaborators and friend, friends, Casper Hesp at the university of
Amsterdam. Uh, this is from a, uh, a paper that we've pre-printed now called deeply felt affect,
uh, which, I mean, besides, uh, articulating emotional inference, uh, using, uh, the active
inference framework also presents a tutorial of, of the more formal, uh, kind of package that underwrites
active inference. So if you're interested, I can send you the paper. Okay. So, so, so far we've
talked about, uh, perception at from time, from moment to moment, right? This, this relation between
O and S mediated by A, uh, then if you consider, you know, the, the sequence of states, uh, what,
what you're doing is introducing beliefs about the way the world transitions, states evolve over
time independently of your observations. You'll notice that your prior here is effectively
disappeared from these steps here because your prior just feeds in your first kind of estimation
and then it kind of goes on. So you're really just using S2 as your new prior for the next time,
step and so on. Okay. Uh, but so far, uh, this, this could have just been a tutorial on reinforcement
learning, uh, active inference does something special, uh, which, uh, is this. So again, don't
panic. Uh, this is just what we've just seen. All I did here was add a circle, uh, or which is this
policy selection thing, this pie thing here. So in active inference, the way that, uh, action selection
is implemented is essentially by choosing, uh, transition matrices by choosing your beliefs
about the way that the world changes over time. So again, we have these B matrices that we just
discussed here, right? These beliefs about the way the states in the world transition.
But what this policy thing, this policy selector does up here is effectively choose a series of B
matrices, right? So to act is to have beliefs about the way you think the world should evolve
in, in this kind of self-confirming way that we discussed earlier. Um, and all the stuff at the top
does is just like put the variational free energy thing into, uh, into the loop. Uh, so this is definitely
beyond, uh, the level of, uh, mathematical discussion that I had in mind, uh, for today,
but essentially, uh, you have this G thing. G is just your expected free energy, right? So it's,
it's the amount of free energy that you expect for every possible action that you could take.
And essentially what this thing is doing is selecting the action, uh, that leads to the lowest
free energy, right? So again, it's just this, uh, action selection thing that leads to, uh,
you know, uh, prediction error minimization or that runs on prediction error minimization.
Um, what you'll notice is that PI again is, is a circle. It's one of these quantities that we have
to infer. So contrary to, uh, you know, more traditional schemes, uh, in motor control, uh,
where you just program a command and it's affected in active inference, your little agent is effectively
trying to infer, well, what am I doing? Right? No, but really like, uh, uh, yeah. Well,
on the basis of your prior beliefs, right? And on the data that you're receiving from your senses,
what is it that I must be doing? Right? Um, so, uh, this again contains a lot of weird math that I'm
not going to go into, but I just wanted to show you this cause it's cool. So here you have your
generative, uh, process, uh, here you have your generative process. So this is just a basically
a model of, uh, like, um, a model in the sense of we are modeling just the process that actually
generates the data that's sort of like invisible, uh, invisible to us. Right? So this is sort of like,
it's hidden to us, but like there is a process that generates our data and this is a Forney style
factor graph representation of the generative model that I just showed you. It's, it's all the same
terms, right? You got your B's, your D's and your A's and all that. I won't get into how you go from one,
from one to the other because it's a little complicated. I was just putting this slide up
because I wanted you to notice two things. Uh, well, it's the same thing, but it's, it has two elements
where the process that generates our data and the model of our data, like where they meet is at two
points. You have these, uh, you things up here, which are the actual actions that you're performing,
right? And this, these, uh, the O's, your, your, your observation. So basically the model,
the generative model and the generative process meet at action and observation. Like that's,
that's sort of the two connective points. Um, I mean, this is complicated and we can return to it.
Uh, then this is the last technical thing I wanted to show you, like where we're at now with the
modeling is that we, we introduce a new layer of states into the model that are about states at the
lower layer. Uh, so in our new work, we're working on metacognition, uh, emotional, uh, content,
um, and kind of self-control mindfulness meditation and this stuff. So what we did here,
um, is connect the layer of states at the top here, uh, with their own transition matrices and
everything to states at the lower level. So what we've begun doing is treating states, oops,
is treating states at the lower level here, uh, as observations. So like we're, we're treating the
results of inference as new data, right? That the system can, can then use, uh, to make inferences
about itself and basically how well it's doing. Uh, so it, I told you this paper is called deeply felt
affect because we use this kind of, uh, layered, uh, structure to account for basically beliefs about
oneself, right? And eventually self-control in this kind of thing.
So Max, if I may, for the sake of people who may get a little lost when the discussion is
too abstract and too formal, if you could think of a couple of concrete examples without technical
language, uh, going beyond something like, oh, here's how a monkey moves through a room to find
a high quality treat. Something more like, how would you help make sense of human emotions and moods,
for example, how, you know, checking in with oneself, you know, in phenomenology, how would you make all that
relevant? Well, so this bottom part of the model, you can make it do, uh, you can make it do whatever
you're interested in making it do, right? So you could have a model of, I don't know, uh, having a
conversation, right? So in this kind of model of having, it might be simpler to explain that bit just
with this. So, you know, the data that you have to explain might be, uh, you know, configurations,
spatial configurations, uh, you know, of expression in someone's face, uh, you know, the, the auditory, uh,
sensations that you're registering, right? So for each of these modalities, you have an A matrix
that specifies the way that the, you believe these sensations are typically related to the states
that caused them, right? Uh, and you have this policy selection thing that affects beliefs about
the way that states evolve, right? So, uh, in a conversation, for example, right, you would be
resolving that continuously. I would be inferring these lower level states, like what is Sam telling me now,
right? And I would be selecting a policy that, uh, you know, that facilitates this interaction that
ideally gets us like to some kind of coupled point where we're able to interact. So the, the point of
adding these additional layers is that now I can make inferences about my inferences. I can be like,
well, am I sure that Sam feels good today? You know, like he, he, he, I don't know, like, uh, he, uh,
he seems tired or something, uh, or, uh, I don't know, like, uh, yeah, it's, it's things to do with,
for example, confidence in your own judgments. Uh, how well am I doing? You know, how, how, how,
how do I feel about these inferences that I'm making this kind of thing? So inferences about
our inferences. And, uh, you know, the, the reason why this is important, I think, uh, just generally,
uh, for, for humans is that, uh, you know, uh, I'm, I'm telling this to you, but you, you know,
in particular, most of human thinking is thinking through other minds, right? Uh, it's thinking, uh, about,
right. Other minds. I don't want to take us too far away from your main narrative, but
the way you're describing it now, it seems like they're, um, maybe two things conflated or, or maybe
just one part of them being substantiated because you're saying, I'm talking to you and I'm thinking,
uh, gee, I wonder, um, uh, you know, how Maxwell's feeling today. I wonder if, um, um,
I'm thinking something about the nature of our conversation and that's a kind of self-consciousness,
if I'm thinking that. Right. I wonder if I'm making a good impression. Right. Um, so what it,
what it brings to mind is, is there a generic self-consciousness? Like you just have lots of
those thoughts because this version is just very specific. I just have one question. Right. I just
have one hypothesis, but in real life, these things come in bundles. Yeah. It comes with a whole,
and that's what we would then call an affective stance, let's say. Yeah, absolutely. I'm feeling
uneasy with you. So I'm engaging with lots of alternative hypotheses. Right. Lots of uncertainty
at that level. So then I'm getting clear as I'm talking, which is probably part of the point here.
But anyway, um, uh, so, so how would you model that? In other words, it's not simply one, uh, thing,
one bit of metacognition or one bit of reflection on the nature of interaction. It's a different,
I would describe it informally. Again, it's like a different stance or a different, uh, a different,
um, different kind. Well, one important thing is that we can entertain more than one different model.
Right. So, um, there's some interesting work, uh, by, um, Izumura, um, I think, who essentially
implements, um, like a simple, uh, and these are very simple toy systems for now. What we're doing now
is scaling them up to human systems, but, uh, it's basically a bird simulation and it's, it's,
it listens to eight different birds and it, it, it, it recognizes them essentially because it
entertains eight different models that might correspond to this or that. So the capacity to
simultaneously entertain different, uh, models might help to sharpen what I'm trying to say again. So
to me, it's different if I say, okay, I'm uncertain where I stand with Maxwell. Right. And so I have
to entertain many different models versus I'm just, uh, here's my, my hypothesis. In other words, having
one policy at that level is very different than having many policies. Right. So to model that, do we have
to have yet another level that's deciding. Oh, okay. I see your question. Right. Well, the thing to note about
this is that this top level of inference makes this bottom level accessible to the system, right? So
basically inferences at, at this bottom level just happen, right? So they, they, they guide action,
but in an implicit sense, whereas these states are literally about the states below. Uh, so attentional
states, for example, exist at this level, right? Uh, states about my sensory states. So that helps
because then if you think what happens when you practice mindfulness meditation and you stabilize
certain states. Right. So you're going from what my state of mind is like, where there's not one of
those red circles, there's like dozens of them and they're, they're all different hypotheses, but it's
actually happening to me and they're all interfering with each other. Right. Uh, which then has a
disruptive effect, let's say, on my attention state versus having one that is coherently. Right. Sort of
reinforcing a particular mode. Anyway, I, I guess I'm getting too far down into one. No, no, no, what you're
seeing is, is that's, that's really like, uh, I think, uh, an important point. I mean, and in, in,
in work that we haven't, that I'm not going to be presenting today, which is basically like where
we're at right now, uh, we've got a three layer system going to explain mindfulness meditation.
This is a work by, uh, Lars Sandved-Smith, um, in particular, uh, that we're doing with, uh, Karl
Fristin at UCL, where basically on top of this, we have an addition, yet an additional layer to make the
attentional states opaque to the system. So that's kind of what I'm getting. Yeah. The idea that you
could learn to manage your own. Yeah, precisely. So this is already there kind of implicitly,
right? Look, we have a B matrix that, that links these different things. So we already have beliefs
about the way that these higher order mental states are transitioning. What would it look like if we
implemented policy selection there? Because you can, it's a B matrix. So why not just like hook these
up to a higher level pie? You could. You could. And that's what we end up doing in the, the, anyway.
So I guess I will, we will be putting this on YouTube. Uh, yeah. Yeah, you can snip out all the
parts. Are the B matrices, you said you can treat them like policies? A policy is a sequence of B
matrices, right? Because that, that's what it is to, to select an action under this framework is to
change your beliefs about the way that states are supposed to evolve, right? It's an array of B
matrices. Yeah, it's a, well, it's, it, in, in the more. Why wouldn't you just use your B matrices?
Uh, well, because in, in the most simple case, it's just a series of B matrices. But what if you're
considering counterfactual depth, right? Like. Okay, so you just keep creating. Yeah, then, then you have
like trees of B matrices and so forth. That's how you can create a network. Yeah. There's feedbacks
between like different contexts and principles based on context, for example, like, um, maybe
what you're getting at. Well, let, let me try to just finish. I have about like 10 minutes left
of material. One small point, and oh, you'll come back to it. So, because for me, the, so it's, it's
appealing again that you get these hierarchical levels and it seems like you're getting more, uh,
getting things that can model self-reflection and all these different, you know, strategies for
managing oneself, et cetera, et cetera. However, the more layers you add, it seems to me you don't
necessarily have more data, do you? So. Exactly. The problem of like your, your complexities back
to the overfitting, whatever, having way more complexity to your model than you actually have
in most. I mean, that's a really good question. And we struggled really hard. Uh, you know,
last time I was in London, a lot of what we did was ask, well, do we really need a third level?
It can't we just do policy selection at the second level? But the key thing is this opacity thing
is that to make these states of inference accessible to the system, to, to, to allow the
system to use them as data for further inference, you need a higher order set of states. So that's
sort of, so if you give me 10 minutes, I'll be done. Cool. Uh, okay. So why is this framework Bayesian?
I mean, we've sort of seen it before. Uh, but basically Bayes, Bayes' rule is just a way to combine
prior probabilities, uh, with your likelihoods to get, uh, posterior probability. Um, so, uh,
yeah, Bayes is important. So I put on a little cramp. Uh, so your prior probability is just the
probability of some event, uh, before any evidence is taken into account. Uh, your likelihood is the
likelihood of, uh, some event, the, the probability of some event given some evidence. That should be
probability of some event given some evidence. So just to illustrate the difference, if Houdini magics
away an elephant on stage, well, you know, there's a high likelihood that the elephant is gone. You
can even go on stage and collect more data. The elephant clearly is gone, right? But we know that
there's a low prior probability that elephants just dematerialize, uh, because elephants are solid
objects and right, those don't typically dematerialize. So, uh, Bayes' rule is just a way to combine,
uh, these quantities optimally. Uh, so basically the, a cool thing about this is that you can, uh,
always use your posterior at one step. So the result of combining your prior and your likelihood
as your new prior at the next step. So there's this nice kind of bootstrapping thing that you can do.
And the way that this relates to what we've been talking about is basically, uh, the descending
connections, right? They carry your priors and, uh, your prediction error is basically always
integrating the data that you're, uh, that you're generating. So it's, it's essentially a likelihood.
And the Bayesian brain says, okay, well, this is what the brain essentially looks like, right?
You've got, uh, likelihoods flowing up in the form of unexplained prediction error and prior
probabilities flowing down, right? In the form of neural prediction. Um, these, these schemes are
typically hierarchical. I, I hinted at this earlier. Uh, the reason why they are is that as you, um,
mystically, as you go up the hierarchy, the things that are represented are more stable over time.
And, uh, as you go towards the more sensory and things change faster, right? So for those of you who
are familiar with Fourier analysis, basically you can take any image, uh, and decompose it into,
uh, frequency bands, right? So high, uh, spatial frequency and, uh, low spatial frequency information,
right? Which, what you'll notice. So high spatial frequency is to the right here,
low spatial frequency is to the left here. And what you'll notice is that in a conversation,
uh, high spec, high spatial frequency information changes much faster than low spatial frequency
information. I mean, unless there's someone like myself, you know, just moves around his face and,
like, makes funny expressions. Like typically most people like, you know, their lips move faster than
the rest of their face, right? So you might think that this is implemented in, uh, the hierarchies that
the brain, um, encodes. The, the hierarchies of, of, of information that the brain encodes. And, uh,
according to the, uh, predictive processing framework, this is precisely what we see.
Um, so I'll skip over that. All right. Um, so yeah, so you have hierarchies of information, uh, that encode
regularities that are time sensitive and you have prediction error minimization going across, uh, the,
this hierarchy and it essentially explains, um, the way that the brain reacts to, um, stimuli. Um,
All right. So that was predictive processing. Um, we can scale this up is what I submit to you now.
Uh, so, uh, I have this, uh, winter is coming, um, just to talk about my favorite example of active
inference. It's like, well, uh, when I get cold, I don't know about you, but I put on a parka,
especially like in Canada, it's, it's cold, right? So the, we can cast this as active inference as well,
I think. Um, so the, the point for the next two slides is, are, is basically going to be,
it's not just the brain that engages in active inference. It's every cell in your body,
every organ system in your body, uh, and effectively maybe, uh, social groups as well.
And this is what we, you know, uh, we, we work on. Um, okay. The last bit of math that I want to
introduce to you, I think is simpler than the rest. Uh, it's called a Markov blanket. Um, and what I
want to submit to you is that rocks, cells, organs, animals, social groups, basically,
uh, anything that exists at all, uh, has a Markov blanket. So a Markov blanket is just a way to use
statistics to answer what is traditionally a philosophical question, right? The philosophical
question being, what does it mean to exist? What does it mean to exist as a thing, right? What does
it mean to be a thing? So, I mean, if you ask a philosopher, they'll tell you like a story about
metaphysics and, you know, like a type token identity theory and whatever, uh, we've issued
all of this, uh, for something much more simple, uh, which is to say, okay, well, if we're interested
in a system, right? So suppose the system that we're interested in is the brain, right? Because
we've talked about it and we want to differentiate it from the environment in which it, in which it is
embedded. What we'll do essentially is introduce or define a third set of states that mediates the
causal relations between the system that we're interested in defining and its environment.
Um, so these are known as sensory and active states. Um, so these are metaphors, of course,
uh, the, the way that they're defined is by their connectivity, right? So sensory states cause
internal states, uh, but are not caused by internal states and active states caused but are not caused by
external states. Um, so, I mean, the point of doing this is to say, okay, to exist is to be endowed with
some degree of conditional independence relative to your environment, right? So if you consider this
mass of gas, right, here, it's not a system because it'll just dissipate, right? Like there's no robust
sense in which it's independent of its environment in any sense, like, you know, it's gone, right?
Uh, a Markov blanket is a way of saying, conditioned on the existence of this set of states, the sensory
and the active states, the internal states of the system are independent of the external states of the
world. Is that clear to everyone? Okay. Um, so active inference then is a story about how internal states,
which encode our model, right? And the active states, which are like our, our skeletal muscles
and so forth, change to minimize free energy. And the end effect is to allow this inference process to
happen, right? So the, uh, the inference here meaning that the, uh, free energy or prediction error
diminishes, right? So making the internal states more like the external states and vice versa. Uh, again,
this is just this story, right? Uh, it's just that as we've seen inference also occurs through action.
So basically like you have the state estimation bit going on here where we're kind of inferring what
should be going on in the world. You have the policy selection bit here, but this is just the story
that we told, right? But just now, uh, presented in terms of like the existence of the system.
Um, I'm going to skip through that. So, uh, so yeah, this is a, uh, a drawing by, uh, the famous physicist
Huygens, uh, and, uh, Yella Brineberg has it in one of his papers on this. Like an important point is
that like an advantage of using active inference over other frameworks is that it just, it comes from
physics, right? So you solve this inference problem, but using, uh, using a framework that basically says,
well, inference is sort of like a rock falling down a cliff. You're just falling to an, an energy
minimum, right? So the, the reason it's called variational free energy, um, is with, uh, is
analogous, is, is with analogy to, um, the thermodynamic quantity free energy, right? In thermodynamics,
free energy is the amount of energy left in a system that can perform work, right? In the information
theoretic context, the variational free energy is basically the amount of wiggle room you still
have on your parameters to get a better representational grip of the situation, right?
It's like, yeah, how, how much room is left on your parameters to do work, to do representational work?
Okay. Last thing basically that I'll be talking about today. Um, so here's our Markov blanketed system.
Again, uh, I submit to you that all of the components of this system are also systems,
right? This is the observation that we started with. Uh, I'm a system, I'm an organism, but I'm made
of networks of organs that are themselves systems and the organs themselves are systems of cells and so
forth. So every component of a Markov blanket is itself Markov blanketed. So in this, uh, 2015 paper, uh, by, uh,
Carl, uh, Friston, uh, which I think is the first in the literature to do this, uh, they effectively
connected two levels of description. So what they showed is that on the assumption that what you see
here first is, uh, uh, uh, I think seven, eight cells, um, eight cells that share a generative model
and over time reach a target configuration. So it's basically a little creature with a head and a tail.
Everyone sees that. And so what you have plotted here is basically the beliefs of each cell about
what kind of cell they are essentially. And so you have your free energy plotted here.
And so what you see is, uh, all of these cells share the same generative model.
So basically they're able to infer their place relative to other cells. So long as they're able
to communicate with other cells, because we all have the same expectations, right? We all expect
to sense the same kinds of things. So, uh, basically the, the little units start off and
their free energy spikes because they're trying to figure out what's going on. But then as they
communicate, the free energy starts to go down until it reaches a minimum value. And when it's
reached its minimum value here, uh, the simulation reaches its target configuration, right? Um, so that
for, for me, this was like an eye-opening moment where, uh, I kind of, you know, uh, with Carl realized
that, uh, you know, this is how you effectively connect levels of organization, right? Uh, units
at one level sharing a generative model are able to enact a target morphology. So from there we've, we
generalized, this is from our, uh, one of our physics of life reviews papers. Uh, we're, we're essentially just
telling the story of how any Markov blanketed system itself is composed of Markov blanketed systems.
So you have this kind of recursively nested, uh, systems of systems of systems of systems approach,
which you might think of as a, uh, a vertical stack of systems. I say vertical, uh, because there are
two dimensions to this system really. Uh, there's a vertical stack where like cells compose organs,
which compose organ networks and so on and so on. But there's also a horizontal stack where at any
scale of interest, uh, like the, the relevant actors like cells, for example, are, are also in
the process of niche construction. They're, they're constructing an environment for themselves and
they're effectively sharing a physical environment. Um, yeah. So this kind of gets us here,
right? Uh, where we started, uh, at least I, I hope I've made the point that, you know, via active
inference, there's at least the possibility for something like an integrated, uh, science of
culture, mind, and brain that takes all of these levels seriously, right? Um, and I'm just going
to skip through this right to the thank you slides. Um, yeah. So, uh, that's what I wanted to share today.
Thank you for your attention. Uh, thank you to my funders. Um, special thanks to the people listed,
particularly those of you who, uh, came and, uh, yeah. Thanks for your attention.
