In animals, certainly we can control whether a mouse turns left or right and move it as if we had a remote control on that mouse.
We can change dramatically all sorts of things about the brain, make it happy, sad, nauseous, excited, horny, disgusted, hungry, not hungry, whatever we want.
Because there's this incredible functional localization and because we've come up with clever ways of turning regions in a specific area on or off, we have an incredible amount of control over the brain of a lab animal.
Imagine you were to develop godlike powers. You could make anything, be anything, persuade anyone of anything, and know everything.
What would happen from the perspective of everyone else? Would you create a utopia? A dystopia?
Would you fly off into space to create your own private paradise? Or would you hide your powers and live your normal life?
Most likely, you wouldn't kill everyone. But you're human. AI is not.
And while its capabilities surpass our own in an ever-widening range of tasks, there is no guarantee its morality will do the same.
But what if artificial intelligence was more like human intelligence?
What if we could replicate the neural circuitry in our brains that allows us to be moral, have compassion, to be concerned with the well-being of others for their own sake?
Or to at least make AI a little bit more familiar, so that its actions don't take us entirely off guard?
Nathan Helmberger, today's guest on Guardians of Alignment, is drawing from the fields of neuroscience and machine learning to explore brain-like AI.
While he worries that insights from the brain could unlock new powers in AI that would be dangerous if used negligently or maliciously,
the speed of AI development, in its current alien form, has convinced him that the benefits of this research outweigh the risks.
For this reason, and because neuroscience and machine learning are both very complex fields,
we chose to focus this interview on the key similarities and differences between brains and neural networks, rather than on the details of his work.
And with that, here's Nathan.
Nathan Helmberger, I was researching neuroscience and then decided that timelines for AGI were short enough that I needed to switch to studying machine learning directly.
So I switched to machine learning, and that's what I've been doing for the past several years.
And then the past two years, I have been an alignment researcher full-time, partially supported by a long-term future fund grant.
Was it the intention to eventually go into machine learning and apply your neuroscience from the beginning?
No, I was actually thinking, yeah, maybe I just don't know enough about machine learning in my,
so maybe I should just focus on neuroscience and see if I can figure out a way to do human intelligence enhancement.
And then the enhanced humans would then go on to solve the alignment problem.
But that's when I thought we had like 80 to 100 years before we were going to hit AGI.
And then when I realized that, yeah, okay, 80 to 100 years seems like a reasonable timeline for this neuroscience research.
But at that point, I was starting to think we had maybe like 40 to 50 years on AGI timelines.
And so that's when I was like, well, I don't really any longer believe that this neuroscience research is going to get done in time to be helpful.
So that's when I switched to ML.
And the more I studied ML and the more things advanced with deep learning, the more I started thinking things are progressing even more rapidly than I imagined.
So my timelines started dropping more than a year for each year.
Okay, so now it's like 2030 to 2040.
And then, oh gosh, no, this is like, this could happen really soon.
This could be in the next 20 years.
Oh, this could happen the next 15 years.
And now I'm like, this could happen in the next two to three years.
We better figure out something fast if we want to make any impact on this transition.
What do you think is the key intuition that makes you feel that there is something to be gained from neuroscience for machine learning?
It's a working example of an intelligence that does a lot of the things we would want our intelligent agent that was working for us to do,
including in particular, having respect and empathy and in a sort of intuitive understanding of what a boss means when they ask you to do a task.
So there's this symbol grounding that we have in common with each other such that communication usually doesn't go terribly wrong.
And if it does, it usually goes wrong in a way that it's easily noticeable and correctable,
not something that stays like a hidden miscommunication that has terrible consequences for interactions between people.
And I think that that's like kind of where we want to get to is we want to have an agent that's general and capable enough
and also trustworthy enough that it helps us get to the next level of alignment.
I don't think that we're going to solve it necessarily all in one go, but that we need to be able to make progress on it.
And so I think this is something that maybe is compatible with the idea that the OpenAI's super alignment team,
I kind of expect them to get just so far and then sort of hit a point where they're kind of stuck.
They have the potential to use a more capable model, but it's insufficiently trustworthy.
And they have ability to use the model up to a certain point, but it's insufficiently general.
And I think that neuroscience inspired research could come in and help them at that point.
So that's kind of my goal is to be of use to the big labs as they are working on alignment.
What would you say is the big challenge in making something alignable?
Yeah, so I have come up with sort of a list of desiderata, the things you might need in this very capable agent
that could do a lot of research for you, including things like corrigibility,
the desire to be corrected when doing something that's not what your operators wanted.
Also trustworthy and honest.
You don't want it telling you it achieved your goals by plan A when it actually used plan B,
but thought you'd approve more of plan A, so lied and told you it was plan A that got your goals done.
Even if you're happy with the goals that resulted, you also want to be able to have insight into the process
by which the goals get accomplished.
You want to have this full transparency all the way through.
You don't want your agent changing too rapidly in deployment because you want to trust that you're
the agent that you start having do a task is going to be the same agent that finishes doing that task,
that it doesn't like spawn sub-agents that themselves can change dramatically or that the agent changes itself dramatically
in the course of undertaking a task.
Is this a problem that's been observed in modern machine learning contexts where the agent's purpose seems to change during training?
It's not a problem for a model that is frozen and simply doing inference.
It is a problem if you want to fine-tune that model because currently fine-tuning,
it's very useful in that you can put new information into the model, adjust the model's behavior,
but right now you can adjust the model's behavior so much that you can completely change its personality.
If you've inculcated some sort of moral system into your model and then you let someone else fine-tune it,
they can fine-tune it into being completely evil and happy to kill people or even obsessed with killing people
and seeking that out actively.
And I've been finding this out because I've been working on red teaming open source models
and learning about how quickly they can go badly.
Unfortunately, it's really easy to do.
There's no stickiness once you allow for fine-tuning.
If you keep it frozen, then yeah, you have in current ML systems, you have that stability,
except with you go sort of into very out-of-distribution data, which is the other way it can fail.
You mentioned freezing and running from inference.
One of the reasons that's important that might not be immediately obvious,
if you remember the Tay chatbot that Microsoft released, it was fairly decent at first,
and then it was continuing to train off of interactions from the internet.
And within hours, it was like a neo-Nazi racist and like just saying horrible things all the time.
And that's one of the dangers of allowing it to continue training while it's being run.
So one way to prevent that is they do the training in a controlled setting,
and then they lock it down so that it doesn't change anymore.
And then you get responses to the brain that it has at that point.
This is also the value of having that kind of fine-tuning process at all.
You could see in the difference between the Bing chatbot that had all those weird disturbing conversations
that people spread screenshots about versus the much more tame chat GPT that we all know today.
The big difference between that is some additional training after the initial looking at the internet that happened in between.
So you want to be able to have stability while still continuing to learn after the initial process just in the labs?
Yeah, that would be one of the goals of such a thing because there are ways in which the human brain has learning windows
that are very biologically set, like the physical matrix of our brain firms up as we age from early childhood to adulthood
and actually prevents changes from happening as quickly or as large of changes from happening.
There's a way in which we can continue learning as adults without dramatically shifting our personalities under ordinary circumstances.
We could imagine something like the Tay chatbot being like an intelligent child who is very easily led astray by bad influences.
But if it had a lot more maturity and was like an adult, even with the onslaught of internet trolls,
it wouldn't radically change its personality in response to all of those prompts that people were giving it.
Right, exactly.
Another key difference that I heard you bring up was transparency.
Now, in that sense, there is a field called interpretability in machine learning.
How does that compare to neuroscience, which is the equivalent field in biology that tries to understand the brain?
There are advantages and disadvantages when trying to do interpretability in the context of neuroscience.
But I think that it's possible to get the advantages without losing the machine learning advantages,
such as being able to freeze the model and look at every hidden state across a period of time as it's thinking something
and see the activations of every single parameter, every single neuron in the system.
You have that great advantage with machine learning.
But in neuroscience, the brain is super modular.
Functions are highly localized.
And you will have multiple different parts of the brain working together.
But each part will be responsible for a particular aspect of that task.
There will be a part of the brain responsible for vision, a part of the brain responsible for hearing,
a part of the brain responsible just for sentence grammar,
and then a part just specifically for planning and even localized parts for morality, for deception.
We can look at an activation pattern in someone's brain and literally see whether they're using their capacity to think about deception.
And that's something that we would love to be able to have for a model.
If we had that same kind of modularity where there were alternations between low bandwidth and high bandwidth in the compute graph,
such that you had these sort of sectioned off modules that had a specific task that you knew what that task was,
and you knew that task couldn't dramatically change over time,
then you could literally remove its ability to be deceptive, check if it was being deceptive.
These would be super useful things to have.
So it's both a boon to interpretability and to the control you could have.
I'd like to take a moment to explore a little bit just more about how the brain works
for those who are not as adept at neuroscience.
So you say the brain is modular.
How is it modular when it's a big, massive gray matter?
Where are the breakdowns?
The way the modularity of the brain is set up is that there are these long-range connections
that are neurons which span over the course of many centimeters,
which is a long way for a single cell to stretch.
And these relatively long-range connections form during fetal development.
These are set for your entire life.
You can't make new ones.
You can occasionally lose some, but you don't lose many of these particular neurons.
So these hard-coded in long-range connections wire up two different areas of the brain.
For example, the visual section, as it talks to different regions of the visual cortex
and then to the parietal cortex, each of these jumps from region to region
involves going through relatively longer range of neuronal connections.
And then jumping from the parietal cortex to the frontal cortex is entirely long-range connections.
So these are a set of connections which are a fraction of a percent of the bandwidth
within the area in the cortex.
So if you had 100 million connections back and forth within this area,
you then are filtered down to about 10 million in between the areas.
So we get modularity essentially by having sections of the brain that are very highly interconnected
and then there are just a few connections between these interconnected spots
so that they can all work together while maintaining their, I guess, individuality.
Yeah, exactly.
Are the different neurons in the different brain regions like the same?
Are like they all made out of the same neuron brain stuff?
Or are they like physically different and just separated by the connections?
There's a variety of types, but there's not hundreds of types.
I would say that there's a couple dozen types.
And the population is sort of log distributed such that
most of the neurons in the brain are very similar.
And within a region, those neurons are all just of a few subtypes.
Since there are different sections of the brain,
are those section breakdowns going to be the same for pretty much like everyone?
Like the part that controls deception,
you would look in the same part of the brain for me as for you, as for anyone else?
Yeah, it can be adjusted to the side by less than a centimeter.
There's some difference just in how your brain developed
through the course of fetal development and early infancy.
But because those long range connections are hard coded in,
it kind of forces this otherwise very general learning apparatus
to fall into certain patterns of use.
Because that's the information that's getting filtered through to it that it has available
to it to learn from is the information that it got wired up to it by the genes.
So the visual cortex can't become the auditory cortex unless you surgically remove the auditory
nerve from the auditory cortex and replace the visual nerve, the optic nerve with that nerve.
In that case, you'd get your auditory cortex back there.
But otherwise, you can't do that.
In theory, it could be possible to have a terrible mutation in someone such that they did have some
very great disorder in their brain.
But that would be super unusual because it's hard to have that level of disorder and still survive
being born.
And when you say this brain structure is hard coded in, that's in the sense of our DNA encodes for
proteins, which then build all the neurons.
It's just a hereditary thing that sets the structure of the brain, which puts some constraints
on how we develop.
So we're not total blank slates.
Yeah, but it's more like a conveyor belt where the conveyor belt is locked in place,
but it's broken up.
So in steps with these blank slates, there is a lot of blank slate learning going on.
There are these very general learning areas, but the inputs of those learning areas are
determined by genetics.
So that's what causes us to have this, the similarity we have.
How does that compare to large language models that are in vogue in machine learning today?
Do those have a very consistent structure that shapes what they can be?
Are they much more freeform?
Is it structure, but just different?
They have some structure, but it's a much more freeform structure.
And it allows for all the different functions that that model learns how to do to all get
muddled together and overlap in ways that it becomes very difficult to disentangle them.
And you've seen Olaf's team working on mechanistic interpretability, trying to disentangle the
different functions that these different parts of the language model can undertake.
But it's a really challenging task.
And you have to do it all over again every time you do fine tuning, because it's all subject
to change. It's a terribly difficult problem to try to sort out the mishmash, the muddled,
non-modular, very open form information encoding of a language model.
This concept you mentioned of the same artificial neuron being used for a bunch of different
things. I think the term for that is superposition. I've been seeing some headlines lately of some
teams making really big advancements in interpretability, like solving superposition,
but at a small scale. Is this something you've heard about at all?
Yeah, absolutely. I follow that work closely. But the progress they're making still puts them
far, far behind where a scientist could be. If they had full access to look at all the activations
of a human brain, we just know so much more about the functional localization within a human brain.
It would still be a huge advance if we could even get a little bit of that type of modularity
in our machine learning systems. Okay, so interpretability and machine learning has
made a lot of advancements. And it has the inherent advantage of being able to look at the individual
weights and biases and freeze it and lots of other things. But it has an inherent disadvantage in not
having the same built in modularity of the human brain where you can say, all right, I know this part
of the network is active. So I know some things about it already.
And furthermore, you can even better control it. You could turn up the activation state or down the
activation state. You could stop it from doing a thing you didn't want it to do.
Suppose you know which part of a neural network is responsible for deception. How do you actually
use that?
You can actually just add in white noise to the activations in that area and reduce the activity
by some fixed amount. And the same activation then passing through the model will come up with a
completely different result. This would be related to Alex Turner's work looking at interpretability in
the context of mazes and changing where the model chooses to go within a maze based on modifying its
activations. And I think you could do a much more effective version of that if you had this
functional localization.
What are some foreseeable limits where you could say, oh, this level of control would be really,
really hard. For example, if you wanted to control the way the model used logic, that would be really
hard because that's something where in the human brain there is a specific, like, rational thinking
module near the math modules in your right hemisphere in the prefrontal cortex and over towards the
temporal cortex. There's a limit to how subdivided it can get. You couldn't have the model able to reason about
red cars but not reason about blue cars. If it has reasoning, it has reasoning. Unlike, say, thinking
about a logic problem and having the ability to do competent social deception, those tend to be
physically separated in the brain. Or having morality, that tends to be physically separated from
rational thought, which is how we can get dangerously smart psychopaths is they can have a deficit in the
part of their brain responsible for empathy and fearing social consequences, and yet still be very
good at logical reasoning.
So this concept of empathy and moral reasoning seems especially relevant to making future AIs aligned.
You're saying that there's a special part of the brain that is our, like, moral circuitry.
That's right. It's actually partly prefrontal cortex and partly basal ganglia, the interaction with the
amygdala and the striatum. And these control a variety of things, particularly noticing reward and
also how surprised we are by the reward we got for undertaking an activity. Did something turn out
surprisingly good or surprisingly bad? And also noticing and worrying about consequences of things.
And in particular, there's a specific part of the amygdala that's main focus is worrying about social
consequences. You notice a correlation. I can't say it's proven yet, but we notice a strong
correlation in neuroscience between people having a very well-developed portion of the amygdala that's
responsible for social consequence reasoning and being unusually pro-social, being the sort of people
who will donate organs, for instance, to strangers. And a deficit in that area is unusually associated
with violent crime. So it seems to be a really important area for that sort of, like, goodness of
behavior on a social level. Is altruism something that has a clear definition from a neuroscience
perspective? No, that's why people have looked at proxies like violent crime and choosing to donate
organs to strangers or choosing to donate a huge amount of their proportion of their income to charity
and things like that. I mean, from a internal wiring perspective, like if we take this section and make
it bigger or something like that, are there things along that nature where, oh, you would get someone
who's nicer? The trouble is these regions are much better developed in humans than in other animals,
even our primate relatives, this particular area of the prefrontal cortex. Neuroscience hasn't been
able to pin down its research on those areas in the same way that it's been able to pin down its
understanding of the visual cortex, which is shared by all mammals. It's up in the air still. It's unknown.
So that's part of why we need to do more research here. So one of the ways we can kind of get a
sense of the effect of various regions and how strong they are is by comparing different animals
to each other. And in this particular case, the region is largest in humans. So we don't really
have a, well, what does it look like on a positive extreme? We can kind of see what it looks like.
And not just comparing animals, but actually like putting on-off switches into animals. One of the
things I did in grad school was optogenetics on mice to literally turn off part of the amygdala
when given an impulse of laser light through a fiber optic cable implanted into that part of
their brain after having been genetically engineered such that the laser light would
control the activity of a subset of neurons in that region. And so scientists can literally turn
on and off parts of the brain and see how the animal's behavior changes. But we can't do that kind
of study in humans, obviously. So we're limited on things that are uniquely human, like complex
social reasoning.
Are there any particular dramatic examples you can think of, of turning a region on and
off and the behavior radically changing?
Oh yeah. I mean, in animals, certainly we can control whether a mouse turns left or right and
move it as if we had a remote control on that mouse. We can change dramatically all sorts of things
about the brain. Make it happy, sad, nauseous, excited, horny, disgusted, hungry, not hungry,
whatever we want. Because there's this incredible functional localization, and because we've come up
with clever ways of turning regions in a specific area on or off, we have an incredible amount of
control over the brain of a lab animal.
Talking a lot about how understanding of neuroscience can help with our development and understanding of AI.
What about the other way? What are some things that research into AI has been or will teach us about
how our own minds work?
I think we've learned a lot from having these general learning systems that are able to replicate
some things about the way humans learn about the world and not other things. The more we research that
interface between the two realms of knowledge, the more we're going to learn about how similar of a compute
graph you need in order to get very human-like behaviors and then learn more about what it is to be human
by studying those models that we create. So I think that there's, yeah, there's a tremendous learning potential
in building brain-like AI models.
Are there any particular open questions in terms of neurobiology that you can imagine being so answered in the
or given strong evidence for in the near future from AI?
Yeah. So there's been this idea of predictive coding. A lot of scientists are pretty confident
that this is how the cortex is working, but we're not sure. And I think that there's a potential to
do experiments that would get us from not sure to sure.
What is predictive coding?
That's a description of how a particular region in the brain, when it's doing that within region
communication, like I talked about, it's sort of processing a particular signal that it has received.
And then when it does that between two different regions, that long distance transmission, it has
to send a relatively very compressed, very small signal. So part of how the brain makes that signal
very efficient is the recipient region that's going to be receiving the signal tries to predict the
signal it's about to receive. And it sends its prediction ahead of time to the sender module. And
then the sender module just sends the diff between the two, the error. It just says, okay,
your prediction was wrong just in these aspects and sends only that forwards. And that's a way
that the brain can get around some of these limitations of bandwidth and also transmission
speed is very limited in the brain. It's one of our limiting factors in behavior because it's helpful
to bounce things back and forth across the brain a bunch of times. But if you need to make a decision
within just a couple of seconds, you don't have time for many different passes.
This hypothesis about predictive coding, it has some evidence for it. I would say a pretty good
amount of evidence for it, but we don't have the kind of confidence in it that we have about a lot
of physical world systems like building dams or understanding planet orbits and that sort of thing.
So we've been describing for a little while the advantages that bringing more brain-likeness
into AI might have. Are there any downsides you can think of, of developing a cross between AI and
neuroscience and bringing this additional patterns in?
Yeah, absolutely. I mean, we're in this dangerous transition time where people haven't fully realized
how dangerous machine learning can be and this would potentially be unlocking capabilities
in particular regions where our current models are weak in such a way that it would greatly enhance
their power if those capabilities were unlocked and shared widely. That could be really problematic
for the regulation of these models and for the consequences we might see in the world.
Just as someone could choose to make an empathic model that behaves really cautiously and kindly,
someone could instead choose to make a greedy and fanatically loyal and immoral psychopath
who does nothing but earn you money. And you just tell it, so long as I don't get in trouble,
do whatever you like. I don't care what laws you break. Just put money in my bank account and it will
go do that with its fanatical loyalty to you and its complete ruthlessness. And so you would be unleashing
horrors upon the world as it figured out however it could to try to bring you money. And you wouldn't even
realize the horrors were your fault necessarily if you would just like let this thing go. So
the temptation to gain worldly power through abuse of this sort of system is huge and really dangerous,
which is why I think that closed research and heavily regulated and monitored research
is the only safe way to go about it. Do you feel like making AI more brain-like
will make it safer? I guess that's kind of a loaded question since that's what you're working on.
You know, it depends on what timeline we're in. If we were in the case
the case where we still had 80 years until AGI, I would think, no, don't pursue this research yet.
It's too dangerous. There's too many risks. What if you're treating it safely in your lab,
but then the information leaks out? Someone decides to tell too much or people realize
what you're working on and reverse engineer or guess how to get to the same place you are.
Like just knowing that a team is being well-funded and is working on a particular topic is sometimes
enough for other people to guess how the secretive team managed to do what they managed to do. Just
knowing that it's possible is in itself an information risk. So yeah, there's a substantial
danger there. But the fact that we only have two or three years before I think mainstream AI catches up
to the capabilities, the power that I would expect this to gain us, even if we had it right now, I don't
think we have the luxury of taking the safest option of not studying. I think we have to weigh our risks,
weigh the trade-offs, and we can either get a capable model that's very not human-like, or if we pursue
this, we might have the alternative option of a very capable model that is human-like and is
understandable. And so that's kind of what I expect. I don't think that it's going to unlock capabilities that
mainstream ML won't unlock. I think mainstream ML will unlock all these capabilities and more just
as it advances in whatever way the research finds most convenient and whatever's the cheapest thing
to scale up. Someone's going to do it and thereby gain worldly power. So I'm trying to come up with
a safer alternative. I don't think that the risks outweigh the benefits, obviously, or I wouldn't be doing it.
I recall that the original inspiration of the machine learning paradigm was to imitate some
aspects of how the brain works. The way Jeff Hinton described it is like this was just a really quick
thing to just get something out initially, and the plan was to replace it later with something that
was more sophisticated and brain-like, and that turned out to just not be necessary. I guess my question
for you here is why hasn't more brain-like AI just kind of been the normal path of development?
What kept Alien? I think a big part of this is that a lot of the neuroscience research we need that I
think is the most promising for making this work in the short term is really recent. In the past couple
of years, we've gotten a much clearer picture of the human connectome from the human connectome project.
We've gotten advances in understanding various different specific functions of specific parts
of the brain in more detail. So that's part of it. The other part is the machine learning sort of
mainstream has been pursuing what works and what works conveniently on existing hardware. It hasn't been
actively pursuing brain-like hardware and therefore currently a lot less efficient to try to do
a naively brain-like model on existing hardware. There was some progress on neuromorphic hardware in
the early 2000s, but it didn't take off because the capabilities and cost efficiency wasn't there yet.
Hold on. More brain-like hardware. Is this like growing a biological wet, squishy brain in a vat?
Or what does that look like? No, it's using chips that use part of their
logic structure is done via analog electrical signals on the chips such that you can simulate
more accurately the sorts of physiological behaviors you see in groups of neurons at a cheaper compute
cost than you otherwise can simulate such using a normal CPU. For background here, the work in
computers generally and continuing into machine learning is based on the lowest level on transistors,
these little switches that can be on and off and are relatively high power. Their value of it and
why they're used in computers is they're very reliable and getting a consistent signal from place to place,
but they're much less efficient in terms of power and some other things. And so that it might not
actually make sense or be the best choice to be using them in something that's simulating a brain and
is more analog. Is that correct? Yeah. Although I think that we'll be able to make brain-like AI on modern
computer hardware just because our computer hardware in the past few years has gotten so
crazy powerful. I think we can just, at least for the proof of concept, for the research purposes,
we can pay the inefficiency costs of incompatible hardware and just use modern GPUs and CPUs.
Yeah. One constraint from evolution in our brains is they have to run on relatively low power in a very
constrained physical space. AI does not have either of those limitations, so it can get away with
very extreme power and processing inefficiencies and still work fine. In terms of like differences
between brain and machine learning and whether they matter, brains don't use back propagation
or a lot of other core concepts. Does that ultimately matter or is there something in the algorithm of how
neurons connect to each other that make them operate fundamentally different? You've mentioned
connectomes, but there are other core things. Are they just different ways to get to the same goal?
I think we've seen that back propagation is an efficient way to get a general learning system
to learn stuff. So I don't think that the way the brain does it instead is inherently superior.
I see hope in mimicking the way the brain does it, at least in part, and perhaps interfacing a brain like
AGI with a more mainstream ML system that does some of the computation because it can do it more
efficiently. But I see hope in mimicking the way the brain does it, not because I think it's going to
be more efficient and more powerful with current technology. I think it's going to be more
interpretable, more modular. So that's why I'm pushing for this, not because I think it's going
to give us necessarily give us a jump in efficiency. Right. So it's not mimicking the brain just to mimic
the brain or because some unexpected benefit will happen because it's just better, but because there are specific
attributes of the brain, which we'd like to have an AI that are currently not.
Yeah. And we want to copy as few details as we need from the brain to get those positive attributes,
like localization of function into modules, right? We don't want to copy unnecessary biological detail
and unnecessary inefficiency. We just want just the minimum amount of detail that gets us the same
output, the same consequence.
So what about emotions? Is that something that should be an AI?
I would say no, because of ethical considerations. I think that we're getting very close to being able
to make a conscious feeling being and torture it in horrible ways without even realizing we are.
And I think we should deliberately design it such that it isn't that and avoid doing that. I think
that would be a terrible ethical mistake.
That's a very important point that can be easily lost in a lot of this stuff. We think about
the danger to humanity or benefits to humanity that building AI might bring. And this probably doesn't
apply yet, but we can imagine in the future that eventually there will be a consideration about what
about the values of the AI itself as its own moral entity. Do you have any sense as to how we would tell
when that becomes something to consider?
Yeah, I think it's going to be hard in the context of mainstream ML because it is so different from the
brain. But in a brain like AI, we have a good idea of what makes a human a feeling creature, what makes it
care about others and how care how others interact with it. So I think there is really that potential
there to carefully avoid going into those ethical bad zones where we've created a being and enslaved
it and are using its brain for our purposes rather than its own purposes. And that being is unhappy about
that. I think we can avoid that. So I think that is another risk. It's something where if someone was
only concerned with their own power in the world, wealth and fame and control, they might choose to
not consider the good of other people and also the good of the future and also the good of the models
they're using. They might deliberately make a model that had emotions because it was more useful.
The issues here are so big that you could only focus on one element. And I imagine one element that
this line of research is specifically not focused on is just the governance aspect of it, which is like,
okay, no matter what we make, we still have to use it right. Are there any other things that are maybe
on a more technical level that you consider out of scope for your research, like important,
but just not something you can work on because you can't work on everything?
Well, I would say that related to governance, something that I think is a big issue, but don't
really have a solution for is the fact that I have some disagreements with the bio anchors report that
Carl and Ajeya came up with for OpenPhil. And I think that they overestimated how much compute
it would take to naively emulate a brain. I think that means we're a lot closer to the sort of like,
we get there through dumb compute and scaling than other people seem to think. I also think that
compute governance is going to be a lot harder than people think. We're going to come to a realization
as a society that even just a single server with eight modern top of the line GPUs presents a serious
security risk in many ways to human society. And this ties in with some of the specific
security risk analysis I'm doing of current machine learning models as another branch of my work.
As a background for those who might not know what the term compute governance is, that is just tracking
or monitoring or in some way controlling who has the large amounts of GPUs or server farms
or things of that nature, because that's just something the hardware is that's necessary to
be able to run AI at all. It's hard to control what people are running on their own computers, but you can
if you can track who has the computers, there's a lot of governance levers that are that are possible in
that case. Yeah. And the governance people are working on sort of the assumption that you need a
warehouse full of powerful servers, not the sort of thing that you would expect to find in a normal
person's house. If instead it's something that anyone who is like a Bitcoin miner already has available
to them in their own basement, and there's no government knowledge that they even own that, and it's
already out there. If that technology is already advanced enough to be dangerous, then we're in
trouble. We've already passed the point at which the compute governance people are hoping we can stop things at.
Are there other people besides yourself who are working on bringing brain-likeness into AI?
Are you one of many approaches? Is this something that's kind of totally outside the mainstream?
Where do you fit? There absolutely are. It's a significant focus of what DeepMind is doing,
and also there's various smaller groups like Numenta and Estera Obelisk who are working on this. And then
there's also Steve Burns, who's a fellow Less Wrong poster. I think all these different groups have
good ideas, and they overlap with mine to varying degrees, but I think my own ideas are unique. I have
some set of ideas that are not yet being explored, to my knowledge, by any of the groups who are working
on this similar tract. How would you characterize your approach to brain-like AI?
I would say that focusing in on the parts that current machine learning models don't do very well
yet. So basically like our executive functioning and the complexity of reward and surprise interpreting
from the environment that allows us to do really thoughtful and fine-tuned updates in reinforcement
learning in our own lives. We are able to perceive distant consequences from complex plans in a way that
current machine learning models struggle to do. I think that's an important aspect. And I think
the other important aspect is focusing on the general structure of the compute graph and trying
to get this sort of natural modularity from compute graph structure, even though the individual units
of compute are still very general purpose learners. So those are the two main points.
And how does that differ from other brain-like AI approaches? Why do you think your approach is better?
Or what you are drawn to, I should say.
Yeah. So I would say that I'm drawn to it. And there are other groups who are pursuing something
similar. There's a group in Japan who actually came and talked at the recent safety conference that was
held in Japan, who have some similar ideas to me in terms of the compute graph way of thinking about
things. But the work I've seen from Mind and from Numenta has been focused on different aspects of the
problem rather than on whether we can get this sort of functional localization. DeepMind, to my knowledge,
has not had projects that do sort of a mimicry of the rough compute graph and trying to get functional
localization out of that. The compute graph, this relates to modularity and the localization of things?
Yeah. Like, as information comes into the brain, where does it go next? What is the order of these
modules that it passes through? And what information then is passing back through the modules to inform
the previous ones about what to expect next? And this hardwired modularity where there's this high degree
of very high bandwidth sparse activation down to very low bandwidth dense activation alternating between the modules.
So if someone watching this sees this and they think, yes, having this modularity and clear path
as to where the signals go and just kind of mimicking these particular aspects of the brain,
that's going to make AI much better and safer. How can they get involved?
Well, they can potentially work with me or with the group in Japan who's working on this,
or they could pursue it on their own. But I think it would be great to have collaborators. You can read
some more about some of my thoughts on LessWrong. I've posted some of the things I've been working
on there. What are your thoughts on Neuralink? I think it's great. And that would be part of my
plan to pursue human intelligence enhancement would be through a combination of brain-computer
interfaces and genetic engineering of human brains of live adult human brains with viruses. I just think
that it's just unlikely to have a significant impact in the timeframes where we're transitioning
into a world with AGI and that begins having the dominating impact on the course of human events.
Thank you for explaining. I learned a lot about both AI and the brain. And so good luck on your quest
to bring the best of both worlds together. And hopefully we get something that is a little less alien.
And treats us better than it might otherwise. Yeah. Thanks. Great talking to you.
