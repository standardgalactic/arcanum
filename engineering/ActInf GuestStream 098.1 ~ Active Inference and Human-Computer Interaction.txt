Hello, welcome. This is Active Inference guest stream number 98.1 on February 10, 2025. We
will be discussing Active Inference in Human-Computer Interaction with the authors of this very
interesting paper. So, thank you all for joining and a pass to the authors for their introduction
and presentation. Thanks again, looking forward to it.
I'm Roderick Murray-Smith, I'm Professor at the University of Glasgow and the PI of the
European Research Council project that's funded this work, the DEFI project. I'm John
Williamson, I'm a senior lecturer here at the University of Glasgow and also one of the
investigators on this project. I'm Sebastian Stein, I'm a research fellow here in Glasgow
and also one of the investigators on the project. Okay, so we published on Archive a review paper
on Active Inference and Human-Computer Interaction and then the friendly team at the Active Inference
Institute said, oh, would you like to give a talk on this? So we're very honoured to be included in the
in the Institute's presentation. So we're going to give an overview of it, but if you want to
see things in more detail, then at the bottom of the screen you can see a link to the archive
publication. Okay, let's get going.
So we'll cover a bit of motivation for the work, describe Active Inference and HCI, what the core
elements are. We'll give an example of a particular case study in Ordinal Selection which Sebastien will
present and that's work that we presented in Oxford at the Active Inference Workshop. We thought it would
be useful to have some concrete examples as part of the paper, as part of the discussion here, and then
some of the challenges for applying this work in the context of human-computer interaction.
So a little bit of background about where this is all based. We started a year ago the five-year
project called DEFI, Designing Interaction Freedom with Active Inference, and the idea was we wanted to
look at how we could better build the flexible interfaces of the future that would use richer
sensing machine learning in novel ways. And that has three main elements. Active inference is going to be
the framework for building things. We wanted to use an approach called Optimal Mechanism Design to
build the interaction mechanisms that could combine rich sensing machine learning tools and embed content
into computational structures. And we wanted these mechanisms to also have shared autonomy mechanisms
so that the amount of freedom the computer would have could be adapted. So what we'll be talking about
today is the first part of this activity on active inference. So there's a bit of background for the
motivation. You can think about, you know, how have humans gone about controlling the world around them?
You know, the very first tools would just be simple rocks and things where the human could use
their muscle power to pick up a rock and do something more than with their bare hands.
And as time went on, we had tools which would give us some more feedback about the world that the actual
this design of a chisel might be make it easier to feel the texture of the wood that you were working
with. And then further development got us to take external power and modulate that. So that could be
wind power or steam power. There's some external source of power which we could use to adapt our tools.
But recent years have moved towards the insertion of external computational power. And you can see the
and the framework that we're working with here and the grey boxes in this figure are where
computational power has been inserted. So the human can act on an interface, they can perceive the interface
and that computer that they're interacting with can also control the world. So this could be a
smart car and it could be tracking the road with its own cameras and augmenting the
control to the steering wheel. But it can also be looking at the human and trying to infer their
intent. It could be augmenting the display. So we have a closed loop interaction with the world and
the tool has got external computational power, algorithms, artificial intelligence.
So the argument would be that for less effort, the human could achieve the same or more performance.
And that's kind of a summary of what the AI was actually standing for artificial intelligence
rather than active inference. You can take one of these old Don Norman graphs where you're plotting
the amount of effort you have to put in against the performance. And usually
the lower line would be human alone. So usually you have to put a fair amount of effort before
you get any sort of performance out. I think artificial intelligence or algorithmic support can lower
that threshold so you can get something for far less. But it's always worth having a think about where
that gap is being filled from. You know, is it an earlier version of you that's somehow been learned?
Or is it other people like you, according to some measure? Or is it more malevolent input from
whichever service you're using, which is trying to nudge you the way they want you to be?
So anyway, it's worth thinking about that background to tool design. We're saying that humans are now
starting to need to use artificially intelligent tools or algorithmic tools. So these are going to be
the most complex things that we've used to date. And the tools are no longer going to be passive
extensions of ourselves. They're going to be active partners where they're starting to take on
certain tasks and interaction becomes more of a dance than a command and control one. So active
inference appears highly relevant to this element of active tools.
Yeah, also, as part of the context, there was, we think there's a bit of a crisis in HCI. So you've
got key researchers like Kornbeck and Roosevelt highlighting that we have concepts of interaction
that talk about design, but we fail to produce theories and concepts that have high determinacy
and adequate scope. So they can't really do anything in detail and they're not, or they're not broad enough
if they can do something in detail. We haven't in HCI been seeing the sort of ratcheting up of
performance that you see in subjects like machine learning and computer vision, where people are
sharing software and building on each other's work. And there are key questions about agency,
which is core to understanding how we are going to be relating to algorithmic support. But how do we
go about measuring that? How do we go about checking whether our agency is being taken away from us by
computers? So given that active inference is a closed loop computational theory for modeling agent
behavior, it seems very relevant to discussing these questions. So you've got this elegant unifying
theory. It allows us to incorporate machine learning, Bayesian inferencing, probabilistic programming,
and dynamic systems. We can work out from low level physical interactions to high level reasoning
decision tasks. So it seems like it's potentially going to be something that could support the next
generation of human computer systems. So three elements that are relevant for that is in human
computer interaction. You've got a lot of uncertainty. Humans are very variable. Their context is complex
and variable. So we need to have models that can cope with uncertainty. Active inference is completely
probabilistic and views agents as entities performing Bayesian inference. So that's quite appropriate.
Humans are also intelligent agents. They're inherently predictive. They're trying to understand the
world around them and act in a way that is based on predictive models. Active inference supports this
sort of analysis. So we have a predictive element. And we also want to balance the exploitation of the
knowledge that we have with exploration to try and reduce future uncertainty. And that's something which
active inference gives us support on as well, unifying the reasoning about perception and reasoning about
actions in a single framework. So let's get into active inference and HCI. So your traditional
active inference framework was often set in a more natural context rather than a design context. So you
might have an intelligent agent. In this case we have our human user, but it could be any other biological
agent and it's interacting with an environment. And it has action states that it can sense the environment
and make predictions about what it's going to sense given its actions on that environment.
So with active inference in a human-computer interaction context, there are several major
configurations here in M3. You've got an active inference model of the human interacting with some
system that could be anything. So when we're saying system we're talking about the computer and its
interface. You could have the computer and its interface represented by an active inference model
interacting with a user who's unknown. You could have the active inference model transducing
the information between the user and the computer.
So we can split things up into offline and online use. So the description approach is saying we've
got a model of a user operating under active inference principles to reason about how the user's going to
behave. You could have the system is just its environment and you could use that at design time
to simulate user behavior and optimize your designs or you could have it at evaluation time to interpret
observed interaction behaviors. Mutual interaction is where we're modeling both the human agent and the
computer agent. So we're looking at a model of the whole interaction loop and we have two interaction
agents working together. So here you see one agent has become the environment for the other. So the human
motor states are then perceived by the computer sensor states and the computer's action states
could be changing its display, making noises, sending vibrations, moving a robot arm.
And these are perceived by the perception states of the user.
Okay, so that's the major change from the usual presentation is we're designing the environment of
the one agent with another agent. Okay, in terms of online
options, we've got a user interacting directly with the active inference agent. This is a constructive
approach. And it can be predicting the user's behavior and acting appropriately. So the user is part
of the environment but might not be explicitly modeled in terms of theory of mind. We'll get to that in a
second. And the other one is the transduction one. So you can have an active inference agent lying
between a user and an existing system, so it's mediating their interaction, and neither of them
are explicitly assumed to be active inference agents. So the user and the system jointly form the
environment of the mediating agent.
And all of the models that we've been discussing so far could be augmented by explicitly incorporating an
active inference model of the partner within their generative form or forward model. So we've called
this a reflective approach. You're constraining an active inference system that incorporates an
active inference model of a user. So you see the embedding of another active inference model within
its system predictions. And this ties into work that's been going on in other areas of work and theory of
mind in terms of levels of mental modeling where you could basically have different levels of higher
order cognitive modeling of mutual theories of mind between humans and agents. So we're now going to go
back to the transduction work that we mentioned here and have a specific use case. So Sebastian, do you want
to take a look? Yeah, so this is a use case where we look at what's called noisy ordinal selection. So in this
case we were looking at using active inference as a transducer as Ron suggested and as one of the
options that we might want to use active inference for in human computer interaction.
And this is work that we presented at the active inference workshop last year in Oxford.
So here we're looking at a rather abstract human computer interaction setting, which you can think of as
as a two player game where one player, in this case, the user would think of a number and the other player in
this case, the interface attempts to to guess that number and presents one number in each interaction time step back to
the user where the user could then give feedback in the form of saying that the number that they're thinking of is either higher or
lower than the number presented. But that feedback is then corrupted by some amount of noise where
the feedback below might be flipped into the feedback higher and vice versa, potentially with different
rates. And that's a sort of scenario that's rather relevant for sort of naturally ordered symbols like
alphabets or sort of imposed orderings like we have in things like drop down menus. And the sort of channel
characteristics of having very noisy feedback is quite commonly seen in interfaces like brain computer
interfaces or other assistive technologies like electromiography, where we usually have rather low
low classification rates and have to deal with high levels of noise. But we have the possibility to have
rather noise free sort of feedback in the form of a display back to the user.
So here's one example of how an interaction might unfold over time. So time is on the x axis here and
the different targets presented to the user indicated by the agents trace and presented by the blue line here.
In this case, the user is thinking of the number four. And then the first time step all the way on the
left, the agent or the interface is presenting to the user the number 16. The user feedback is indicated by
the triangles pointing up or down indicating that the feedback received by the interface was either
lower and in the case of this first time step or higher, as in the case of the second time step.
What you see here is that I think in time step five, it is where the feedback given by the user is
the first corrupted feedback, at which point the agent flips effectively its exploration area to sort of
higher the higher numbers. And we'll see later why that is. And in particular here, the agent flips its
belief to be one where the user is responding with very high error rates. But we see that over time,
the system recovers and the agent can actually correctly infer that the user's target is at or near
the true target number four.
Okay, so there are some classic information theoretic approaches that we could apply here,
and which have been applied in previous work. But they are only optimal under a set of strong
assumptions. For example, Horstein's posterior matching algorithm is only applicable if we assume that
the noise is actually IID fixed and known in advance, and that the symbol set is arbitrarily large,
as well as the feedback channel being noise-free. So we were interested in seeing whether active
inference can help relax those assumptions. In order to do that, we translated the problem.
Yeah, if we can go on the next slide. We translated the problem into an active inference formulation,
where we represent the user's intended target, as well as the error rates as the latent state of the
system that we would like to infer. And the observations that the agent can make about the
system it's reasoning about the user are these binary feedback symbols, which correspond to their number being lower or higher than what the agent has displayed.
And the agent's action space is effectively the set of symbols that the user might want to
wants to select. In order to apply active inference, we need a probabilistic forward model. So
the system the agent interacts with, which in this case is the state transitions of this latent state
as a function of the agent's action in the previous state. So that's kind of answering the question,
how might the user's target and the noise vary over time? And we need a probabilistic forward model of how the
user feedback is generated. So answering the question, what symbol would I observe if the user intended to
communicate that the target they're thinking of is higher? And so in order to select the next section,
we need to inherently trade off between learning about the user's intent and learning about the environment
characteristics of the error rates of the channel. So commonly, we use Gaussian distributions to
represent the belief about the latent state in active inference agents, but Gaussian distributions are
not particularly helpful here. That is because a sequence of user interactions is usually compatible with
distant and mutually exclusive beliefs. So if we take an example where the agent displayed the number 20 and
the feedback received was that the user's number is lower than that. And that is compatible with
the user's target actually being lower and the channel having a rather low error rate. But it's also
compatible with a scenario where the user's target is actually higher and the channel has a higher error rate.
So in order to represent a potentially large number of these modes in our belief distribution,
we decided to implement a particle filter where the belief distribution is then a weighted combination
of point masses in this space of latent states. But the K-divergence can still be computed rather
efficiently and quickly just by using the multinomial distribution represented by the weights in this case.
So going back to the example that we saw in the beginning, I want to now look at how the agent
reasons about the latent state, in particular, the error rates and the user target. And we're going to
first look at the error rate, which in this case we assume to be symmetric. So it's equally likely to flip
a signal lower into one that's suggesting that the target is higher and vice versa. And here you can see
around the time step where the first errors occur and the agent belief about the error rate
is sort of changing from a lot of mass accumulated near the lower error rates to mass and the belief
distribution aggregating around high error rates. So I should have said before, the color in this case
represents the probability density within sort of local regions around these sort of discretized levels of
of error rate. And the true error rate in this case is 20% as indicated by the red dashed line.
And so over time we see that the agent's belief about the error rate converges to somewhere close to the
true error rate of the system. Next slide. Now looking at the marginal distribution about the user target,
we see that initially the agent has rather broadly spread out density, meaning that there's relatively
low evidence for specific targets over the first few time steps. But as the agent learns more and more
about the error rate with which the user is communicating, it's unable to identify the target correctly
towards interaction step 35 and onwards. So we did quite a comprehensive set of evaluation
experiments, which are detailed in the paper, but just to highlight a few here, I want to present first
what we've done in terms of simulating experiments and then present some of the key results.
So we're always assuming that the channel statistics are symmetric in this case, just to simplify the analysis.
And we're looking at scenarios where the user could select between one out of 32 different targets.
And we repeated simulations where the simulated user actually
imagines each of the 32 different targets 10 times and simulated for 500 interaction time steps,
where after each every 100 times steps, we reset the distribution over the user targets to be
uniform, which is simulating the behavior over five successive interaction episodes.
So one of the main findings is that even in the absence of knowing about the error rate of the
communication channel, the active inference agent can make decisions as well or better than
the Horstein algorithm that I've alluded to earlier. After the first 300 time steps,
when the agent has learned enough about the error rate to compensate. And we've also done some
experiments indicated on the right-hand side of this slide where we looked at the error rate actually
changing over time. And again, with the heat map displayed here, you can see that the agent's belief can
track the changes in the true error rate of the user. So just to summarize, we presented a concrete
example for using an active inference agent to do reliable one event selection and showed its
effectiveness in inferring both the control polarity and working with non-stationary channel
statistics. And we can bring in additional assumptions and
further domain knowledge in a setup that's more specific than this one.
Thank you.
Okay, so let's talk a bit about core elements of active inference in HCI.
So the key aspects of an active inference agent are listed below. We've already discussed
the idea of the agents and what entities are going to be modeled and implemented as
active inference agents, the users, the systems, both, neither. We need to look at the role of the
environment. Is it just a transmission medium between the agents? Is the user using the system
as an instrument to act on it? Or are they both fighting environmental flux operatively?
We want to tie in some discussion about Markov blankets. How do we separate agents from their environment
to work out the actions that each agent is capable of and be able to define boundaries between the
user and the system? Forward models are obviously key to active inference. So we have to be able to
create a generative forward model and that's often going to be an implementation step that's very
challenging and because we have to be able to predict future states, synthesize the observations that
we would see. We're going to look at how we predict and reason about the next best action because we need
to use the forward predictive models to estimate future states and predicate actions on the estimates.
We have to think about the preference prior structure because the agent's goals or desired are
going to be defined as a distribution rather than as a single reference value or control law.
So we're going to have to think about how we can shape these priors to align with the goal of
interaction design. So it's this change from trying to describe the natural world to trying to design and
create a system or a world that's going to be better for the humans involved. And then there's all the
practical implementation questions around building and training the forward model, implementing your rollout policy,
getting your inference mechanisms to work and perform Bayesian updates and the challenges associated with
that. So let's start with the environment. Here's a sort of cartoon of the basic ways in which
the environment can play a role. You might have the user in the system who is directly engaging with
the environment. Can they manage the variability in the environment cooperatively? So an example of that
could be that you might be trying to use a smart speaker and the users try to operate that to
change the mood and observable behavior of a party in their flood. But the user can also interact
directly with others in this party and they can bring their observations of that together to pick the
next best track. Similarly a central heating system might have an application for controlling the
temperature in a building but the user can also make changes like opening or closing windows.
We might have a transmission approach so that we were just describing the environment
in terms of what to what extent it affects the communication between the user and the system. So
if you have an environment with lots of background noise then any audio display by the system may be
difficult for the user to hear. Similarly bright sunlight on the screen might make the screen visible to the
user so we need to understand the impact of the environment on the communication between the user and the system.
We may have the user applying the system as some sort of intermediary instrument to act on or better
sense the environment. So it could be transducing user unobservable information from the environment
environment or it could be acting on the environment at the behest of the user. We haven't discussed multi-agent
environments but they would be a natural follow-on where we start to have multiple explicitly modelled agents
acting together. An interesting question in HCI is where the boundaries are and obviously the active
inference community has focused quite a lot on the notion of Markov blankets but HCI has often struggled
a bit with this so the very name human computer interaction suggests that we're assuming there's a
clear boundary between a human and the computer and people might often potentially naively associate that
with Clark's skin bag around the physical limitation of the human and we know that if you're going to
understand the system then finding the boundaries between that system and its environment seems to be an
essential for analysis of it. But these the Markov blankets that describe the limits of
one agent may change dynamically as we give and take control. That could be driven by technology so we might
give a user a tool or an assistive technology and we can transform what they can act and sense so that
you if you have somebody wearing augmented reality glasses or virtual reality then we can change how
they're interacting with the world we can let them see things that would otherwise be invisible and we may
be able to give people the ability to act in the world in a different way. So modern tools will have
context sensitive algorithmic support you from algorithms from AI and these could augment the
user cognitively or it might relieve them the need to attend to or or check on mundane tasks and so that
could be seen as taking or augmenting agency depending on how it's designed but basically you can imagine
that as we're taking over tasks from the human that the the states which previously were being controlled
by them are now being handed over to the agent to work on so we're seeing an adaptation of the
Markov blanket depending on how much support we're giving the human. So we've you know been evolving human
tools from rocks to AI as we mentioned earlier we're getting this trajectory of increasingly powerful effort
leverage doing things better doing more new things and so that means that we're increasing our
individual collective intelligence we're extending our natural selves with our tools
and although we started off with crude functional tools like hammers and axes we've moved to tools which
have controllers in them like thermostats and governor technologies based on a controller approach
and then each time you're giving some of your agency away for understood outcomes and the tools can take
on more and more of our tasks but the question is um are we at a point where we're potentially giving up
really important bits of ourselves okay so arguments have been made in the active interest community
that to maintain the integrity of the agent the blanket needs to change and there's several papers making
arguments about that that you need to adjust yourself as your as your environment changes in order to keep your
internal states stable um and we're looking at um the basic approach that we were showing earlier we
have the markov blanket with its sensory and actuation nodes internal and external states but i think one of
the interesting things that we could be pulling in from machine learning is the notion of attention
so that at different points in the interaction cycle the human may be actually only paying attention to a
very limited subset of the sensory states so we have an effectively different markov blanket even though
the human could potentially perceive things on the the states that they're ignoring
and this can of course be structured into a hierarchical setup
so this use of attention means that we could be tracking you know with our closely models we could
be going through interaction processes and looking at what the human pays attention to using the standard
bayesian methods to infer markov blankets at different points in time and see what is the human
ignoring at certain time points and what does that mean for system design other research within hci um
has been raising similar questions although not from an active inference perspective so there was a paper by
um sataniana and jones at mit that it runs a computer scientist in an hci context the others and
um an anthropologist and they were looking at intelligence as agency and looking at dyads of
co-fused agents and looking at the line of control shifting between them and where they say compose we
would say acting on the environment and when they say interpret we would think of that as sensing the
environment and a lot of their interest was in in different contexts when is it good to give up
agency on your control side and on your perception side and they were arguing that in certain cultures
one is more appropriate than the other but it's not consistent across cultures they were also
speculating about what happens and this your ai is taking over more and more control over the state
of things as you can see in the bottom right corner the sort of pacman figure where the green is
gradually eating up the blue of the human and so you can think about that relating to markov blankets
and saying well you know which states is it important for the human to continue to have control over
okay we mentioned forward models in active inference in a human computer interaction context we're going to
need to have forward models of human perception we're going to have to forward models the computer
sensing process we're also going to need to be able to predict the human motor control system and what
it's capable of and the reliability of the computer displays given the environment an interesting
observation is obviously that active inference focuses on forward models and it doesn't need
explicit inverse models which is a difference from other contexts
okay so um we want to look a bit at prediction so active inference agents aren't reacting directly
to the sensations they're right reacting to their beliefs about future states of the world and the
observations are there to correct the estimates of these bonus so we're trying to minimize our future
surprise um and you've got the the standards um approaches of expected free energy and potentially
breaking that down as information gain and the pragmatic value and the interesting question is how
humans are going to experience this so if a system is acting to gain information about them um they may
experience it as a proactive system this is giving them a personalized way to interact or it may be that they
find that the system is moving into the uncanny valley where they're feeling it distinctly creepy as it
tries to pick their brains about what sort of a user they are so it's going to be interesting to see how
that the implications of curiosity in the information gain term are perceived by users when they start to
interact with the system but we also think that the the whole approach of using surprise minimization as a
guiding principle in interaction design is going to give a give us a normal perspective on the design
process and may lead to some surprising outcomes in terms of final system design
okay preference players they're obviously another key element of active inference and researchers initially
have highlighted that most uh concepts of interaction don't tell us much about how intentions are
formed or affected by interaction and it's interesting to think about how active inference manages this how
the preference priors are going to be designed because obviously our system's behavior is going to be a
function of the preference priors that we give the system and what we assume about the user and there's
more recent work in active inference about adaptation of preference priors during interaction and it's going to be
interesting to be interesting to think about how that models but that seems like an area that's still
quite open for development and we'd be curious to hear how the community's evolving in that direction
okay um and there's you know a lot of people in human communication interaction have been used to using
objective reward functions and reinforcement learning and so there's interest in contrasting the active
inference approach um misdistribution of preference priors compared to classical control laws
okay challenges it's not going to be easy obviously there are huge computational challenges in active
inference with the sorts of complexity of system we're looking at we're going to have all the complexity of action rollouts we're going to have to
uh balance the performance of the final system with the real-time requirements of online applications
and it's going to be interesting to see how much we can continue to use sampling and research and how
much we have to move towards amortized algorithms where we're learning and mapping from offline behavior
even if we can't get real-time work going we do still think that offline analysis of these mutually interacting
active inference models can be interesting for the design side we're essentially giving you know
creating digital twins of our human and system and observing how they interact with each other
um another challenge we feel at the moment is that the software infrastructure for active
inference is still lagging a long way behind for example machine learning so having easy to share
standardized software libraries that people are familiar with and having the correct computational
abstractions to build reason about interfaces is not there yet so we need to develop workflows where
we can actually apply active inference to interaction in a predictable way
um as we just mentioned preference modeling is a new perspective for people in human computer interaction
in terms of using distributions so we've got these flexible frameworks but they're unfamiliar
to designers and the consequences of changes to these distributions will have significant effects
so how do we work with that how does it manage the uncertainty in the heterogeneity in human behavior
and in general with interfaces representing uncertainty has the potential to make interfaces more robust
because we know that we have that uncertainty there ignoring it is not an option
but if we create problematic interfaces users may struggle to understand what's going on so
creating the right metaphors mapping the uncertainty into usable feedback for the users is going to be a major challenge
prediction is obviously key for active inference and that can potentially mitigate latency in the interaction loop
but that predictive aspect of it could be unsettling for users it may make them the interface feel like it's in this uncanny valley where it's sort of human but not quite
um we were we've been speculating as if we have a pair of active inference agents
they're going to be trying to push the joint system into a mutually predictable region so that even if their predictive and generative models are not yet
completely capable over the entire range they're going to be if active inference is the right framework to think about things then the two agents are going to be pushing each other into a mutually
um predictable space so that could mitigate some of the challenges that we've just been outlining
um um in terms of preference we're going to have to develop how we elicitate and validate present preference priors for design purposes
how we can get generalized preferences from individual user preferences but potentially creating hierarchies of preferences
okay lots of the criticism of an active inference approach from the hci community would be similar to other
computational simulation approaches you've got the cost of complexity of developing models
the general difficulty of creating models which can represent the cognitive and perceptual quality
complexity of humans and especially the sensitivity of human behavior to the defined details of context
but you know there's something that's been around a challenge has been around for a long time and
thomas sheridan one of the people who's been working in this field since
the 60s has made the point that you know actually the process of modeling forces you to really think hard
about what you're considering and to ask and answer questions about what the essential
features of the structure and function are so in terms of changing the approach to an hci to doing science
forcing yourself to create good generous models is an important part of thinking about and understanding the
problem so the modeling that's required by active inference requires you to think about what are
really important features and if you can build generative models that validate against user behavior
you've got an acid test of whether you really understand that interactive system
but of course any adaptive intelligent system is going to have to deal with co-adaptation of users and
you're going to have to adapt for account for recursive theories of mind so we're facing really challenging
things because even humans sometimes struggle with predicting what they're um the people they're
interacting with are actually thinking and what's going on in their heads
okay so active inference faces the challenges but you've got that inherent capacity to model
those the user and the computer in an interaction loop and then try to approach the problems computationally
okay so in general we're seeing active inference as an example of the broader um area of simulation
intelligence in hci um if you haven't heard of the term simulation intelligence there's a very good
um archive paper on it it's about 80 pages long but uh um it goes through how you can bring different
types of simulation model into uh understanding the world and making better models of it and we see
active inference is very much an example of this you've got a number of different fields coming
together which are letting you build a computational human computer interaction approach and that can
include um interdisciplinary approaches you've got probabilistic programming and predictive models you've
got physics and form machine learning you need to take dynamics and control systems into account
and you've got the physiology and cognitive science of the human side of it so you're simulating the
the whole interaction look from perception cognition motor control sensors computation display
um and you're built doing that in a way that's computational computationally implementable and testable
and we had some previous work based on non-active inference approaches um which was published at the the wist
conference um over a couple of years and this is us simulating human uh motor control and perceptual
systems so there's a simulated um ocular model simulated muscle models and then that was using reinforcement
learning to perform certain tasks and it was the user in a box model that you could simulate we would like
to extend that work to include active inference for the future okay so one of the things that we're
arguing is an important part of this framework is that having active inference models of both the human
and the the system lets us model the intertwined action of human and system um and it's avoiding
the problems that were seen when people tried to bring classical control models in where you could exchange
building blocks in and out assuming nothing else would change because already in the 1960s
mature his colleagues and with their crossover model demonstrated that human pilots would adapt their
behavior to shape the properties of the closed loop so that if the aircraft properties changed they would
become different pilots they would behave differently so that the closed loop system had a consistent um
shape um and hall nagel joint cognitive systems approach highlighted the need to examine the joint
system rather than individual blocks and so he was viewing the human as an implicit model of the process they were
controlling so we can't understand interaction either from the user or the technology in isolation you have
to have this joint model and we think that the sort of active inference mutual interaction models are going to
be required to actually model concepts like agency or engagement and interaction so let's have a look at
our outlook um we moved into this area because generative modelling is at the core of active inference
and the massive advances that have been made recently in generative machine learning
uh make it possible to do the sort of analysis of the human perceptual system that would have been
impossible even 10 years ago and so the whole general we before we started working with
active inference we were using things like perceptual control theory and to build systems so we were
taking a closed loop approach of looking at um empowerment approaches you know polani's work
uh where you're modeling the channel capacity from the actuation to the sensing but these control
based approaches were only only become realistic if you can model human learning perception and that's
what machine learning has provided us with in recent years active inference is flexible preference
priors and continuously adaptive behavior behavior we think is going to be well placed to support ai tool
use and we've highlighted the implications for hci theory so having this whole loop model gives us the
potential to model agency and engagement although there are going to be challenges in getting there
an active inference gives us this probabilistic predicted unified approach to closed loop modeling that
we think could be quite a clean representation of the problems
we've highlighted that there are major computational challenges in getting things going especially in
real time if it's a good theory of behavior but not yet practical for real-time use could it be used as a
better user in a box model to help us in the design process offline and there are going to be interesting
questions about when to amortize you know the computation into some hard-coded non-linear network
but of course you know we've given you one example during this talk of an interactive application but
we want to branch out within our project and test it in a wider range of interactive applications because
that's how we think we're going to hit progress so that's us hitting the end and i'm going to just put up
one thing that we had good feedback from reviewers about was our one-page visualization of active
inference and you can see that developed in the actual archive paper itself but some people thought
that was a nice way to present it so we'll leave that up while you ask questions okay thank you
awesome all right uran want to begin with some reflections or questions uh yeah uh first of all
thanks very much uh for your talk it was really interesting uh i'd also like to echo what the reviewer
said uh this is a fantastic introduction to active inference if you do not know how it works um really
clear figures clear explanations uh if you're about to start reading say the par textbook i would
say have a look at this first and then go to the textbook it's a really just nice primer so uh thank
you for that like genuinely education and communication of how active inference works is
really hard so it's really nice to see good examples of how to communicate those ideas very clearly
um in terms of my questions i have loads uh daniel if there are questions from the chat definitely
interject and put those in as well or i will just monopolize all of the time there um but to start
with i think what i wanted to uh um is in terms of the alternative to active inference that you've
experienced before uh what were their key limitations that you think active inference is completely like
overriding and what are the limitations where active inference is also going to struggle
so you know as i may i did mention the things like perceptual control theory you know we're using
you know behaviors the control of perception so there's a lot of the the general ethos of active
inference already in there but they were not so behavior perceptual control theory wasn't explicitly
using a probabilistic approach um and it had you know with standards reward function so there are
lots of you know lots of different areas where people have got elements of it so the the active
or the empowerment approach where you're modeling the the channel capacity from your actuators to your
uh sensors that's essentially saying how much can i put into the world and predict what's going to come
what's going to come so that's relating to generative models so there are similar points there but active
inference has a nice coherent framework um we're still trying to understand ourselves what the relative
merits of um you know the energy and expected free energy and the energy of the future and so on all the
variants that could come in there as an actual cost function are so that it feels like there's something
interesting there with the preference priors giving you more flexibility but we don't you know we need
to see that making an improvement on a concrete case and understand the difficulty of how to specify
those parameters because when you're not just trying to describe a natural system but actually create
and design a system that's never been made before you have different possibilities and um it's it's
potentially taking active inference out of its comfort zone in terms of not just describing natural
things but saying is this a good design approach so i'm not sure i've answered your question but if
john or sebastian have other ideas to answer it better yeah we've done quite a lot of work into bayesian
models in hci um but that's usually about how we model the absorption of information for the
environment solves the perception problem um and one of the nice things about active inference is that
also includes the acting that part of its framework so it really adds to the whole interaction loop can be
unified in in that model i think that's one of the big advantages i think also the predictive aspect so
not always being about modeling what is happening now that is really important in human interactions
you have to deal with latency and the fact that people are also making mutual predictions most of
the models don't explicitly account for that and i think that's something which active inference does
very well and there's you know so there are some approaches like computational rationality where
people focus on every area will have its own focal points and in computational rationality there's a lot of
focus on what are the cognitive bounds or the physical bounds of the human how does the environment have
you know constraints and these would all be represented by the generative models um
an active inference also you know the community has focuses on more on say the markov blanket aspect
which is maybe underplayed in other areas and so that's interesting from an analytic perspective if
you're trying to do understand things of systems and say well what is the system and how does that
change over time and um how should we infer that so again how do we we can take the general ideas of
the markov blanket but actually making that work reliably doing the statistical analysis to be able to
actually say okay no we can actually infer this but actually how much data do we need to do that and how
reliable is it going to be seems to still be an open question but it's like that's an aspect that's
not been a focal point of other areas so i'll see anything more okay so fantastic um next question
um but maybe we could also talk about the the vignettes at the end of the paper in the appendix because i
think they were very instructive but i'll ask this question and then maybe you could talk about the vignettes
in relation to this question which is what do you see as a difference between computation
and cognition when it comes to these sorts of uh computer assistance for humans to use
i'm not sure i understand the question but if any of you understand it better or think you understand
then then how would you know so i'm not sure i understand i'm sorry sorry that's all right maybe
maybe i'll come for an example um so let's say i have a calculator that as a tool can help me compute
but i wouldn't necessarily say it's taking away it's not shifting that line of control that you spoke
about before okay whereas that yes we could extend the sort of this create a spectrum say where there's
a tool where let's say it's a large language model power chat bot where i could ask it to tell me
something or do something for me and that's going to maybe you know it takes the example of the smart
car so you know you might have your smart car in a full manual mode and then you're having to pay
attention to everything in the world and you know you're having to make sure that you stay in your lane
that you keep your speeds um you know appropriate for the other cars around you and safe and so on and
you have to be aware of the uh how slippy the road is that icy or wet or dry or sandy and if you move
into a semi-autonomous mode where the system starts to do lane following you are now relieved of the you
know the the computer can do the computation of tracking the the white lines in the road and
making sure that you're in a safe distance from both sides and safe distance from the car in front
so you're relieved of having to monitor those states in the world and you can potentially use
that spare capacity to do something else and think a bit more about where you're actually trying to go
or whether to change your route and so on so that would be an example of giving up one bit of agency
so that you could um focus on others we had the smart speaker example that could be combined with
the car so there the smart speaker could make decisions about what to play um now you could
have something like a control a dial where you could gradually move through different genres of content and
have much more control of what exactly was going to come up next but if for example you're trying to
control your car while also picking music you maybe need to give more autonomy to the music player and
just let it play something roughly in your direction rather than trying to micromanage it because you've
got to keep your eyes on the road so it's this divided attention you know where what are you paying
attention to at what time and what's a reasonable amount of attention to pay that it would be unsafe to
focus on the music system when you're supposed to be driving and so active inference could help
uh in predicting what you're doing what your loads are what your limitations are and if you're spending
too much time paying attention to the interface it needs to take control and take that away from you
so that you can focus on the road does that make sense yes yes
uh next i'm just going to go through my list um sort of in the order as we talked spoke about some
things and i'll also pull some stuff from the review paper as well um but uh yeah if you feel like
there's anything else that you didn't get a chance to speak about that you'd like to put in then by all
means to do so uh so my next question is um on the the uncanny valley aspect uh that you mentioned
before i think one of the vignettes was talking about uh like some of the like i think referred
to like the seal paro product um not necessarily by name but this idea of uh a robot that can anticipate
your emotions and your emotional state uh and try and behave in a way to make you feel better right
um and that's that's a product that has historically i think the designers of that said they started
with a dog and the reason they the dog was not successful as a robot product to help people feel
better was because people have very strong prior beliefs about what a dog is how it looks how it
feels how it sounds how it should behave and that immediately put it in the uncanny valley because it
sort of looks like that but it's not and then that difference between your observation and your priors
uh presumably gives you a very large uh free energy uh gap and that is uncomfortable um so um i think
it is very interesting to see how if you have if you sort of make your robot more uh predictive and
able to handle uncertainty how that would actually create an uncanny valley effect um in my mind it would
actually reduce that because it's able to better match your priors if it has a model of you does
that make sense yes indeed i think it's um it's going to be obviously as you you you know if we get to
the goal of you know fantastic model of everything then you know then hopefully things would be good
but the as you're moving towards that goal there's going to be mismatches where it's slightly wrong and
it's whether people find those glitches on the way to you know so if you had somebody who's willing to
stick with it no matter how creepy it was on the way then fine but it may be that some of the early
phases of that learning process make it you know it's got some elements of prediction but it's
this creepy enough that you just don't want to touch it it feels like uh maybe it wriggles the wrong way
and uh you just go oh my god what is this alien thing and i don't want to touch it and so it never
gets the rest of the information it needs to improve its model and it's the same with in general
with hci so we we were talking about this in in copenhagen um a week or two ago and there was a
question about are designed agents compatible with the free energy principle because you know they they
don't die and so on but any any app which is not good is going to get binned and any product any
robot that is too creepy is going to be switched off and ignored so um death for an agent is being
ignored and being thrown in the bin essentially so it's trying to do things to avoid that drop in its
ability to act in the world so it wants to if it's a if it's been designed to work well with the human
then it should you know the human's preferences should be key to it but it also wants to be able to
continue to work with the human in order to help the the human and if it does something which gets
it binned then it's never going to be able to improve the human so that's a very black hole in
its predictive horizon that if it thinks there's a chance that you could be binned for behaving this
way that's going to be something in wants to avoid so i guess it's the the question is will we hit
uncanny valleys when we see on the way to perfection when we're still making errors and we haven't quite
got the model or is it just already very weird if if humans therefore humans seeing a computational
object that seems to be getting them so well will that already you know be something that throws people
and they they don't you know it's partly also how quickly can the human adapt their model of the
world so there may be some humans who've never experienced a machine that could understand them
the way this one does and that's already hard for them to adjust to so um they just say no this is not
a world i want to be in where you know machines behave like this so that's i think we're going to see
different things from different cultures different communities depending on your upbringing what your
expectations are whether you can grasp a machine that seems to understand you in some ways better
than you do yourself because it may be able to measure aspects of your behavior and compare that to
huge numbers of other people and maybe it picks out some things that you're not aware of yet
which would be potentially scary for some people absolutely no there's definitely very strong ethical
considerations uh going into this uh especially considering what do you explicitly model and what
do you implicitly model and as you alluded to there the the group priors and then going for specific
priors on an individual user um certainly what comes to mind when you're talking about products and
going extinct the extinction pressure is uh one one tool that did not have to my knowledge at least
any predictive power but was very much largely a rule-based tool was the microsoft clippy assistant
when you were typing in microsoft word and goes oh it looks like you're trying to write a letter
which i think universally everybody found not necessarily creepy but just incredibly irritating
um so i think that's that's where i sort of see us moving away from is these very like simplistic
assistants with very limited perception we go to something that's more probabilistic more
sophisticated oh sorry clippy was the team you know the original work there was to be you know the
bayesian reasoning team and eric horvitz and co were doing the original work but i think the problem
there was it was taken away from the bayesian uh reasoning team and given to a product team who just
said bugger that we're going to clip all these things and make it much more straightforward and then
you know they didn't have the richness to actually cope with the complexity of the environment yeah okay
um could i ask a question about um
the in sort of the experiences your team has had in actually implementing active inference uh the noisional
the noisy ordinal task selection um experiments as well um so i know part of the challenges that you
raised was uh sort of the um not necessarily mature software toolboxes that we have for implementing
active inference so things like pymdp rx and fur these are probably the most advanced implementations
out there for active inference uh do you mind if i ask what your team was using
um and where you see the biggest challenges from the software implementation side
yeah so we we looked at pymdp and we did use it in some preliminary experiments but
at the moment or at least at the time we were looking at it it was very much limited to
discrete probability distributions and so as soon as you get into very high dimensional state spaces you
reach the challenge that you'll have a combinatorial explosion of discrete bins in that state space
and so it would become very very costly very quickly um and so we were looking at alternative
approaches and we don't use julia at all um so rx infer wasn't really something we were looking at in
in much detail um and so we just decided to start implementing um some of these things ourselves and
because we've been working with realistic inference tools before we thought we were sufficiently capable of
doing that um but then actually you know deciphering the active inference equations the expected free
energy and so on uh from the book was more challenging than we thought i think um and so
in part it was also a useful exercise to actually do the programming ourselves to see you know do things
actually work the way they should and now we're implementing the right thing here um yeah so the short
answer is we use our own implementations but very much with the idea that you know as we work through
more and more uh specific examples we end up with more robust implementations where we hopefully can
you know slot in categorical distributions and swap them out for particle filters or discussing
distributions and different inference algorithms and always depending on you know what's what's demanded
by the individual um application that we want to build
daniel i think you can probably speak more
what's in it we've been surprised at how challenging it is to do with things that we
think should be simple so you know so it's it's been taking us longer to get you know we're building
a basic mouse model and expanding we had control model predictive control based models of mouse movement
um in previous papers before we started using active entrance and we thought it would be relatively
straightforward to take the active entrance approach to that but it's taken us longer to get things going
there than we expected so yeah so implementing things in you know continuous space is uh is
is interesting and as soon as you get something concrete you start to go okay so what should the
preference um priors be on in this you know so is it the selection process isn't the movement process
should we have hierarchies of them you know so it becomes a it makes you it forces you to suddenly go
okay there's a lot more complexity here than we thought i mean initially i must admit most of my
experiences with the discrete models and uh pi mdp specifically um but i think daniel can probably
speak to more of the rx and fur side and the julia side and i think they are pretty advanced ways of
handling the continuous distributions particularly for the the drone flying examples that they have
um daniel i don't know if you want to add anything on that yeah um i really resonated that a lot of the
considerations of cognitive modeling independent of whatever implementation approach are like what
slice or what joints of nature do you really want to model so that was a cool constructivist approach
that you took with building up that biophysical realism um and then when it comes to implementing
active inference it's a it's a great learning experience to actually try to implement in limited
cases different equations and then that helps like pull back the curtain and realize like hmm we're not
really dealing with a collected data set and then training a generalized statistical model to interpolate
something about the patterns of that data set but rather explicitly stating certain distributions that
might have observability or non-observability or control and and so it's like pulling out those islands
or the icebergs that you want to model explicitly rather than it just it's like etched away or revealed from
where in another way of modeling you could have just collected the salient data set and then interpolated
not needed to specify anything specific just we have trace data on experiences that were rated positively or
negatively we trained a model and that's how we are continuing to classify these traces of data
versus it's like wow there can be a lot of steps and a lot of degrees of modeler freedom
in determining what which explicit distributions are relevant for that it's it's a separate question then
like which data set is relevant for it but but that's it comes up all the time in in modeling and
uh the frontiers of the software development are also quickly moving hopefully meeting up with these more
modeler challenge areas
the only thing i'd add on to that is uh coming back to i think something you said earlier rod on
it's an acid test whether you can create an agent that is successful in these environments part of
that is also actually not even necessarily if you know you've implemented the model correctly but
just being able to specify a generative model for a problem it really does force you to think and i
think that's one of the things that's been missing a lot from uh the more traditional sort of reinforcement
learning and deep learning approaches where it is just get a bunch of data chuck it at a neural network
and hope for the best and you sort of relying on the fact that well i don't know what a generative
model for this is i don't have any model of causality i don't have any explicit model of a user
i just throw some data and i get an output back out and i think it's really interesting from the active
inference point of view of getting explainability from structure and being able to think about a
problem sort of almost like reasoning from your armchair and saying i'm going to think about this
problem and say the world acts in a particular the world behaves in a particular way specify those
causal chains and then test that agent out in the real world afterwards
and i think the you know obviously the key thing is if you're using your deep learning to model how
a city works with traffic or how a hospital work or how a company works you're usually doing that in
order to do something else more efficiently and so as soon as you close the loop using these predictive
models you've changed the city you've changed the hospital and it's not going to it's going to be
moved to somewhere where you have no data before because you didn't have the ability to control it before
so the you know the active inference because it's a closed loop framework means it's going to then
have to adjust to that new world and that's part of the basic framework whereas a lot of the the
classical you know big data work was saying oh all we need to do is get lots of data we can build a
model but they were forgetting that as soon as they used that they moved the system that they were trying
to optimize to somewhere it had never been before and their model was then useless and so
i think that's the interesting but the challenge is okay we close the loop but you know like control
theory spent the decades looking at stability of closed loop systems and um you know we're going to
have to revisit a lot of that in active inference work is yes we're trying to change things but you know
can we guarantee that this that our hospital is going to be stable our city is going to be stable if we put
active entrance loops around it and i don't think we can guarantee that yet can we
lots of interesting um questions in the chat just read a few in this last section so
fraser patterson wrote some questions hopefully that we will hear more about in the coming years
so let us continue on some other questions thank you though fraser um jeff wrote how can incorporating
active inference in hci assist user trust
well i would say one aspect is interpretability so if you build explicit forward models and um
somehow communicate the belief that the agent holds uh that is you know the trigger for the agent taking
certain actions then that could help explain and thereby build trust right um this causal chain of
internal belief to agent action this is harder to disentangle if you look at things like large
language models obviously it now comes in where you know they make more explicit their sort of chain of
thoughts but i i would say that that is one maybe one aspect that can help with user trust so there are
the there's elements of it so there's some bits that would be on the design side so if we have
explicit models and we can look at a particular interaction context and analyze it then we can
say okay we understand why the system was behaving the way it was and we can decide whether that's
a good or bad thing we may still need to create a succinct metaphor for interaction for the user where
the predictions are fed in and that can communicate that succinctly to the user um in some settings or
if this was a you know so say you were controlling a tanker and an active inference navigation system
was giving you it could be saying my predictions about where you're going to be if you do this thing
or this and then you could relate to that another way it can potentially build trust is that because
as long as you're closing loops to uh you know maximize um your predictability and minimizing your
surprise it can potentially there might be two possible interpretations of the user's situation and
it could try and do something to quickly reduce the uncertainty of which they were in and then
have a more coherent uh uh feedback for them whereas in other situations the system would have to say
well if if this was your intention then this would be happening if this was you so an active inference
system could try and reduce that uncertainty more rapidly in order to give a more coherent um feedback
and control so that might be one way to build up trust that you can see uh okay it thinks i'm doing this
again it's about the mutual modeling aspect that we were talking about as the user starts to become
more confident that they understand the system it can work with that so if it understands that they
don't yet have the user doesn't seem to have a good model of themselves if the active inference agent
has an explicit model of the user's model of that that model is still quite uncertain it knows there's
not an awful lot of point in having a very deep prediction horizon because very soon the user is
going to get so diffuse about their understanding of what's going on that it maybe needs to spend more
effort in the early stages of improving the user's model of it so it's maybe it has to perform some tasks
not so much to achieve the user's goals but to help to reduce the uncertainty of the user's model of it
so there may be a stage where the system just automatically essentially trains the user
how capable it is and it's also then improving its model of the user and vice versa so you may find
it achieves trust by going through a process of reducing the uncertainty of the mutual models and then
they can do more together and the prediction horizons gradually get longer because they're able to
predict each other's behavior work as coherently yeah being able to explicitly disentangle pragmatic
and epistemic value and when are we seeking to provide preferred sensations like staying on the road for
the self-driving car or the songs that are preferred for a given listener and then epistemic value can provide
learning and updating it's not needed in every single situation just taking ad hoc excursions from blood
sugar homeostasis is probably deleterious but then again it would be in balance with maintaining it in
a healthy range to understand responsivity so it's like whether for biological systems or these more
synthetic informational systems being able to understand and have an explicit dial and this is
also one of the most challenging parts of modeling even after you've set up the pragmatic and epistemic
value terms their balance is like doing control theory on a seesaw because it contains a higher order
trade-off with preference satisfaction and with learning Aaron yeah i just want to come in with i think the
rx and fur team originally had an application where they were looking at hearing aids and parameter tuning and
optimization specifically to help users use hearing aids more because the situations where you might
be using that hearing aid would require very different parameters so if you're in train station
versus in your house your hearing aid would need to be tuned differently but that's a very difficult
procedure to do so they would have an active inference algorithm that would try and understand the
uncertainty of where the user is but deliberately choose odd parameter choices to learn about the context to
then choose good parameters values in order that the user has a good hearing experience
um so there you're sort of you i think they're implicitly getting that trust because the user
is then just using the product more as to your original point of if apps aren't useful they don't get
used and they die um so as a user trusts a product more or trusts a system more they are going to use it more
um i did want to come also onto the original question as well about improving user trust and the
uncertainty uh aspect of this i mean active inference has a very clear understand like explicit modeling
of uncertainty how do we communicate that to users in a sensible way you also mentioned the large language
model stuff and chain of thought i think you know they don't have an explicit under like model of uncertainty
but they do have a good job they do a good job of communicating whether what they're communicating
is actually valid or not is a separate question um but they give some english language that people can
understand right yeah there's a lot of work um on how to represent distributions interactively and you can
have things like audio models which are the properties of the sound can represent the distributions involved
you've got visual models where you can blur images but i think you're going to end up with and you've
got things like churn you know so um so churn off faces or churn above faces who can remember the
churn above radiation and share of faces where you could you know have multiple dimensions of a
face representing a high dimensional data but you could potentially add extra properties to it so i think
there's going to be lots of scope for creative metaphor design to represent uncertainty we've been
looking at this on and off over the years um so that's going to be something where there's going to
be loads of scope for creative creativity so we you could separate the um the algorithmic elements of
estimating your beliefs from the way you display them to the user and then try and optimize you
know so which ways are working how do you change it how do you if you're not if a particular approach
is not represent is not communicating its uncertainty well to the user then we need to learn better ways of
doing that so there's going to be an element of data-driven tuning based on how well people are
communicating and there's going to be an element of just creative leaps of the imagination and how do
we you know how do we take high dimensional uncertain data and represent them just to go back
some of these things have been around for a long time so you know the bayesian modelers at microsoft
had to think carefully about the wizards you know so if you're doing something mundane like a printer wizard
there may be an optimal question to ask next in terms of expected information gain but it may be
it may if you ask a series of questions which seem to be about a topic you know are you properly plugged
into the printer and then it suddenly leaps away and asks something about a completely different topic and
then comes back to wires and printing and you know printer wires and so on the user may lose trust in
it even if it was a statistically it was an optimal question to ask and because the user can't see the
relationship and thinks this thing doesn't know it's just guessing random things so there may be an
information theoretically optimal way to ask things but if you don't take into account the user's model
then and so if the user is an experienced user and trusts the system then that would be an optimal
thing to do if the user is a naive user in terms of this wizard which is often the case because things
should hopefully not fail all the time then you have to assume that they are a bit mistrustful about
products from microsoft and uh um that they then are willing to just quit using the wizard if it starts
to ask what they think of as random questions so this has been around for a long time and um it's going
to be an interesting question so one of the you know for example in terms of the the interaction between
the the human and the the computer the reason that we wanted to work on optimal interaction mechanisms
as a topic is that we think there's going to be a very limited set of core features on the
interaction which which can be combined in different ways with different sensors and we want to have
a framework that people can quickly recognize and it's the way that nowadays toddlers pick up a magazine
and start to swipe it thinking it's an ipad they they've very quickly grasped the basic ways of
interacting with a tablet and they're trying to generalize that around the world but we think
that you know a big part of making active inference and hci work well together is going to be picking
appropriate metaphors building appropriate interaction mechanisms so that even if somebody is using a new
sensor for the first time they can quickly manipulate it and see ah okay it seems to be mapping to a dial or a
slider and i'm hearing some formative feedback from the core mechanism that i'm familiar with
and i can quickly adjust my behavior to um work with the possible mappings that might be involved there
so it's going to there's going to be a co-creation of interaction metaphors at the same time as the
active inference and hci come together well maybe in closing um a room and then all the authors
how are you planning to take the work forward and or what would be on your wish list
slash issues or questions to raise up for people to explore more
okay well the work is is continuing forward and uh we're we're hiring exciting new uh phd students
some of them we're asking questions which you you weren't transmitting but we're sure that we'll
get a chance to hear that from them in person so hopefully they'll be pushing things forward
um but i think the um this also and you can see you've been voted in as a new speaker sometimes does
that um so i think the questions that we have for the community would you know be about things like um
the um the preference priors you know how are people working with that and tuning that and eliciting
that in other contexts and adapting them learning them from behavior because that's all going to be
super relevant for building systems that can interact well as a human and reasoning about you
know what should should the priors on the system just be about pleasing the human should we have other
constraints on them so what does that mean for something like an educational app which should know
more than the user about such a certain thing at the start of the process because the whole point
is to change the human so i think the you know we're we're excited to hear what's happening on
things like preference priors on the whole debate about alternatives to efe um for prediction because
and what happens when things have not yet converged because some of the assumptions in the core
um cost functions like efe are assuming that the system's already doing the right thing and what
happens when there's a mismatch at the start of that process so i think they're getting more input from
people who are experienced working with these things would be exciting for us as you have any
anything more you want to add in yeah i think we've still got some questions around how
the learning of the models happens so how do you learn a power model particularly how do you do
that during uh interaction without introducing instability um the questions about the representation
of the low level distributions the kind of things sebastian was talking about you can't always work with
uni model models even uh want to do things in continuous spaces how can we do that for the kind of
distributions we actually encounter in human computer interaction and questions about how we do the
the debugging of these kinds of models what are the diagnostics we can apply to see how well these
models are working um and track that you know we're going to build them and evaluate them something
which is going to be core to that i think for me one of the most interesting things is looking at
scaling so we can scale things up to higher dimensional user inputs but also scaling to
much higher uh dimensional degrees of freedom in terms of the system that i've been that's being
controlled and the time horizons over which we reason and uh that then brings up questions about
you know what do we amortize what do we actually explicitly model and roll out uh and whether or
not we bring in some concepts of you know hierarchical agents and so on so i think scaling for me and
everything around that uh sort of what's going to keep me busy for the next next one
i think just to just to add there as well i think the scale free part is particularly
particularly uh a point of focus right now so we've had the scale for active inference paper that um
came out last year um i think there's a paper that came out this year about the tree search
and trying to reduce the sheer competition or explosion of policy policies that you can have if
you have a planning horizon longer than x so i think we'll see a lot of uh work addressing those
issues over the next few years uh for me it's just a case of getting really clear and reproducible
software packages so that we can make this easy i think that's that's where that's where the work
will be over the next few years of just being able to say right create an agent let it go the thought
the thinking should be about how you set up your generative model and not really about whether
you're using effective free energy or expected free energy of the future or generalized free energy
i think that's largely a problem that should not necessarily that should not need to be solved by
say designers or human computer interaction specialists i'd like it ideally that the software is
you know sort of uh batteries included off you go you get active inference out of the box but then
it's coming up with a model that maps to your problem maps to the problem that you they are
trying to solve for a user um so hopefully uh we get there sooner rather than later yes and i think
also creating tools to you know harnesses to take these agents and and their works and you know run
simulations and analyze them in different contexts and visualize what's going on and look at
counterfactual situations and so on so i think if this is if we're doing it all right then we have
the tools to make the active inference work well but then we're trying to you know even if it's
understanding a natural system you might like to put some put it into a particular context and observe
what happens and doing the tracking of you know the behavior so the kind of digital twin element of
what's this so when we build active inference agents that we can actually monitor and analyze and say
okay we're now understand if we want to have interpretable systems then we we can get it into
a particular context and statistically understand what's going on around it and why is you know is
this down to is the behavior is the lack of you know it seems to be a sensible action there down to
an imperfect um generative model of the world is it down to too short a prediction horizon is it that
we're just aren't sensing the world well enough then that's that's just the limit of it's doing the
right thing given the information it has but so i think uh creating the tooling to understand and
analyze the models is going to be interesting as well well you've created really a wonderful style
sheet and almost a graphical taxonomy of different settings where that is coming into play
in cyber physical systems which have at least partially a fully modelable component and so that plus
open source software development plus community feedback and application it will be exciting to see
where we all go so thank you again to the authors and to run for raising the possibility of the stream
so thanks for inviting us see you all thank you thank you
so
you
you
you
you
you
you
you
you
you
you
you
you
