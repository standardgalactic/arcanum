For anyone watching this video, statistically speaking, you fit into one of two camps.
You're either in the 60% who are too young to have experienced what I'm about to describe,
otherwise you're part of the 40% and have your own variation of this story,
even if you watched it play out from a distance.
But regardless of which camp you sit in, we have to start this story with optimism.
Optimism is how we got to the predicament we find ourselves in today,
and the best way for me to describe that optimism is to start with my own experience.
I grew up in Limerick, a small city in the southwest of Ireland, and lived there until I was in my mid-twenties.
By the end of my time in Limerick, I'd become pretty disillusioned and restless.
I was living a purely musical life, spending most of my time managing the band that I was in,
if I wasn't performing, I was writing, or recording, or going around the country handing CDs into radio stations.
Yeah, CDs.
We had a distribution deal that put them in shops around the country,
not yet realizing that Napster and its successors were in the late stages of strangling that model to death.
But I did much more promotion than just that.
I spent a lot of time building and maintaining our website using Flash and ActionScript,
not yet realizing that platform dominance had made that endeavor almost completely pointless,
and also that my Flash and ActionScript skills were soon to be redundant too.
Even back then I knew that social media was important,
and so dedicated a lot of time tending to our MySpace page,
which was the place to promote your band at the time.
It was buggy, inundated with ads and pop-ups, and most annoyingly, it was a volcano of spam.
Scammers were free to pester you with impunity,
deleting random friend requests from people you didn't know was a daily chore.
I was also heavily involved in the general music scene in Limerick.
I helped put on shows, recorded other bands, and had a nice sound engineering gig too.
For a time, I liked my little niche.
I liked my circle of friends, and I liked working as a musician.
But I just had no money, and I couldn't figure out how to bootstrap our band to get to the next stage of professionalism.
With the odds feeling like they were so stacked against us,
we eventually went the way of most bands and agreed to split up.
The obvious next step for me was to move to London,
where I could make use of my design degree and get into software.
Money had always been the problem,
and my goal was to solve that problem to bankroll my next steps as a musician.
When I made the decision to leave Limerick, I was gone within a month,
and I remember that final month really well.
I soaked up as much atmosphere as possible,
saw as many shows as I could,
and visited everyone I knew.
I knew that this was it.
I wanted to remember this time clearly.
I didn't want faces to begin to fade from memory,
and of course, I didn't want to be forgotten.
So during my first year in London,
I'd go back to visit Limerick all the time,
to play solo shows, and even do sound engineering occasionally.
But over time, people began to stop thinking of me
as someone they could call if they needed something.
My friends formed new bands,
sound engineers replaced me,
and before I knew it, I was a stranger in my own hometown,
which was made even worse by the fact
that I was an even bigger stranger in my new adopted city.
It was a weird and sometimes oppressively lonely time.
I wasn't yet a very experienced designer,
so sat right at the bottom of any pecking order in any company I worked in.
No one knew me.
I auditioned to join a few bands, but couldn't find anyone I gelled with.
So for six months, the only six months in my life where I did this,
I decided to put music on hold and try some other things and meet some people.
And that's when I took a brief detour
and became interested in the world of amateur dramatics,
joining a nearby theatre group.
I'd always wondered whether my affinity for performing on the stage
would transfer to acting,
but unfortunately, I'd never find out.
Because once the theatre director discovered I could do sound engineering,
my fate as technical staff was sealed.
I'd do sound effects, audio cues,
little moments of piano playing,
slide whistles for when someone's trousers fell down,
you name it.
I worked in a few productions,
participated in competitions,
made one really good lifelong friend,
and attended a whole bunch of rap parties.
And along the way,
kind of developed a slightly flirty relationship
with an actress there who was also Irish.
I wasn't totally sure she was interested,
but it seemed like it was going that way.
At one particular party, when we were saying our goodbyes,
she asked me for my email address, which I gave her,
and the next morning I woke up to find an invitation to this thing called Facebook.
Now, when I got my first look at Facebook,
I wasn't all that impressed.
To my eyes, as a designer working on sophisticated web products,
it looked a bit crap.
What is it?
Another MySpace, but with nothing on it?
What I didn't realise was I had just become,
inadvertently, a very early adopter in the UK.
But of course, because I was only connected to one person on it,
there wasn't much to do.
I posted a message on her wall,
hey, I'm here, yay, and left it at that.
Then, a week or two later,
I got a curious email notification to inform me
that I had just been poked.
She had poked me.
So, I logged onto Facebook again
and saw indeed that I had been poked.
And seemingly, my only available option was to
poke her back?
I'm not sure if this is, like,
is this the right thing to do?
Feels a bit weird.
Okay, I poked her.
Unfortunately, this was the most active period
in our janky dalliance,
and neither of us messaged to the other again.
So, once more,
Facebook faded out of memory
and I moved on.
The end.
And then, one day, a few months later,
I got a new notification.
A guy I'd studied with in primary school
who I'd actually forgotten existed
wanted to be my friend.
I barely recognised him.
The chubby boy I knew
had become an adult man
with a beard and everything.
That's crazy.
Hey man, oh my god, how have you been?
What are you doing these days?
Hey Martin, it's great to see you.
I'm an electrical engineer.
I live in Berlin now.
Wow, it's been 15 years since I saw you.
I'm working as a designer in London.
Brilliant.
I always remember those mad comics
you used to make about our teachers.
What an interesting
and totally random thing to happen.
And then, about a week or two later,
it happened again.
This time, it was a guy
I had been very close with
when I studied in a brutal
all-boys secondary school.
We used to ditch PE class together
and talk about how he'd eventually
get out of Ireland
and become artists.
I started talking to him all the time
and found out that he was living in London too.
And then, the notifications started coming
in multiples each week.
People I'd studied with in our college.
People whose band I'd recorded.
A guy we toured with in the Czech Republic.
My old guitar teacher, etc.
It was such an amazing experience.
Hey, it's so great to see you again.
How have you been?
Is your band still going?
Can you send me your music?
What country are you in?
What job are you doing?
Whatever happened to Tom?
Whatever happened to Jill?
Oh, Tom and Jill are here.
Send them a friend request.
And what followed this
was what I can only describe
as two weeks of total insanity.
Fifteen or twenty people per day
wanting to be my friend.
Everyone I'd ever met
who remembered my name.
Within the space of one month,
I went from feeling totally adrift
and isolated
to suddenly being able to chat
with all my friends on a daily basis.
Yes, there had long been things
like chat rooms
and early social media sites
like 6degrees.com,
Makeout Club, Friendster,
Friends Reunited,
Myspace or Orkut.
But none of these services
had come close to what Facebook had achieved.
They had the right combination
of privacy settings
so our conversations weren't exposed
for everyone to see.
They'd engineered an experience
that made us feel safe
using our real names.
You got almost no spam
and most importantly,
the entire service was geared
towards fostering communication.
When you logged in,
your feed would be updated
with a chronological list
of what your friends
had been talking about
while you were away.
You just scrolled back
to see what you had missed.
Back then,
I'd come across
all kinds of fascinating conversations.
In particular,
since many of my friends
had totally different professions to me,
it was really instructive
to read discussions
they'd have with other professionals
in the same field.
Programmers,
civil servants,
artists,
academics,
historians,
you name it.
I remember during one election in the UK,
I really enjoyed frontline dispatches
from friends who were involved
as either volunteers or advisors.
It could sometimes be
so much more instructive
than your traditional newspaper reporting.
Of course,
there would be disputes,
but they take place
between people who knew each other
and you were much more likely
to moderate your responses
simply because
conversations were taking place
in front of people
you liked and respected.
There were a few people I knew
who felt the need
to constantly post their thoughts
and conversations
where I'd rather they didn't.
It could be annoying,
but it wasn't anything earth-shattering.
More than anything else,
my memory of this period of Facebook
is that it was kind of chaotic
in free form.
People used it in very different ways.
For example,
there was a period
where people would post
long think pieces or blogs
that would kick off
long discussion threads.
I remember once posting
an intentionally terrible screenplay
of a full episode of Lost.
I was always really disappointed
by the writing of that show
and thought writing my own version
would be an amusing way
to criticise it.
I spent about a week writing it.
My intended audience,
my network of connections
on Facebook.
When I started writing music again,
I'd post demos
to get reactions
from other musicians.
It was great.
I had a lot of fun
on Facebook back then
and would often spend
countless hours on it
talking with my closest friends,
especially late at night
on weekends
when I could grab a bottle of wine,
listen to some music
and forget whatever crappy job
I was working at the time.
It was my connection
to my hometown,
my three brothers,
my closest friends
and it was increasingly
becoming my connection
to my new life in London too.
Over the next two or three years
as I became a more
experienced designer,
I managed to transition
away from bad agencies
and began working
in software and games companies,
eventually landing in a company
that made games exclusively
on the Facebook platform.
It was now both my social life
and my working life.
And if that wasn't enough
to make it clear
how important this site
had become to me,
the first few conversations
I ever had with a girl
who would eventually
become my wife
took place on Facebook.
For me,
it was a transformative technology.
There's just no other way
to describe it.
The funny thing is,
during the earliest years
on the platform,
I never really paid
close attention
to what was happening
behind the scenes
and it would be years
before I'd start following
the Facebook story
more closely.
So,
when I decided
to make this video,
I went back to find out
as much about the early days
as I could.
And this is when I came
to fully appreciate,
for the first time,
the sheer vastness
of media coverage
that exists about Facebook,
which,
when you take it all together,
tells a fascinating story
right up to the present day.
The story of Facebook
from beginning to now
is the story
of numerous crucial moments
of the development
of the internet,
Rubicons that can't be uncrossed.
It's the story
of the transformation
of our behaviour
and the story
of how legal rules
built for the era
of TV and print
are struggling to keep up.
And it all starts
with an explosion
of positivity.
In early 2004,
Mark Zuckerberg launched
The Facebook,
a site exclusively
for students
of Harvard University.
When it proved
to be a hit there,
other universities
emailed the team
to ask if the service
could also be expanded
to include them.
A lot of the expansion work
was done by the co-founder,
Dustin Moskovitz,
as described by Mark
in a lecture he gave
at Harvard a little later.
Dustin's contribution
was nicely captured
in a CBS segment
about young entrepreneurs.
Dustin just launched
100 schools a couple
of days ago,
so he's passed out.
Dude, what's up?
Dude, you're on TV.
The Facebook's
initial purpose
was to offer
a convenient way
for people at the same
university to look
each other up.
You can take a person
and look them up
and you can get a picture
of how they'd like
to portray themselves
in addition to,
say, the courses
that they're taking,
sports teams,
clubs that they're a part of.
But I think that
there's a lot of times
where people will just
go out, meet someone,
say, okay,
I don't know that much
about this person
and they'll go back
to their room
and do what people
call Facebook them,
which is go on the Facebook
and look them up
just to kind of see
the snapshot
of what they have.
In order to make
its college users feel safe,
privacy was of
the highest importance.
And then we also give
people complete control
over what parts
of their profile
get shown.
We don't force anyone
to show anything
and we give people
granular control
over some of the
more sensitive stuff.
So right next to
the cell phone field,
there's another field
that's like,
who do you want
to show this to?
I mean, we care about it.
If people feel like
their information isn't private,
then that screws us
in the long term too.
But we're obviously
really sensitive
to people's privacy.
By the way,
just as an aside,
it's really interesting
to see how poorly
attended this lecture was.
That wouldn't last long.
In August 2004,
the venture capitalist
and co-founder of PayPal,
Peter Thiel,
invested $500,000
in the Facebook,
which bought him
a 10.2% stake
in the company
and a place on its board.
An investment
that was vital
for the early expansion
of the product.
Many former employees
and advisors
would later report
that Thiel wielded
significant influence
over Zuckerberg's thinking,
especially during the early days.
For example,
Thiel is often credited
for convincing Mark
of the importance
of having absolute control
over the company.
However,
in order to achieve this,
there was a problem to overcome.
You see,
while at college,
Mark had liberally handed out
large stakes
to a few of his co-founders,
including, famously,
Eduardo Saverin,
who held a 30% stake
in the company,
which he'd bought
for a fairly trivial investment.
To get around this,
Mark incorporated
a new company
that effectively purchased
the Facebook,
which allowed him
to redistribute the shares,
reducing Eduardo's stake
to less than 10%.
By doing this,
Mark consolidated
his position
as the majority shareholder,
with additional mechanisms
put in place
to ensure that his decisions
could not be overridden,
a situation that persists
right up to today.
Then,
another feature appeared.
There's this feature
called poking,
where you just go
to someone's profile
and you could poke the person.
And, like,
what does that do?
Nothing.
It sends them a message,
it's like,
you've been poked.
You know, like,
who cares?
You know what I mean?
It's like,
I thought about it
when I was drunk.
Oh,
so that's what that was about.
Anyway,
since the platform
was now rapidly growing
in registrations,
a collection of new investors
began bidding
for a stake in the company,
including the Washington Post.
However,
the winning investment
eventually came from
Jim Breyer of Accel Partners,
who offered $12.7 million
for a 10.7% stake
in the company.
By June,
the team moved operations
to Palo Alto,
one of the principal cities
of Silicon Valley.
Around this same time,
Facebook expanded
to serve high schools too.
An important thing
to note at this point
is Facebook's strict policy
that registrants
use their real identity.
This was enforced
by only accepting
email addresses
issued by colleges
or universities.
As we'll see later,
this policy of non-anonymity
is one of the core aspects
of their success.
One immediate benefit,
which I described earlier,
was that when you have
to use your real identity,
it acts as a disincentive
to behave badly.
Another immediate benefit
of this approach
was that they didn't need
to dedicate too many resources
on content moderation.
MySpace has
almost a third
of their staff
is monitoring the pictures
that get uploaded
for pornography.
We hardly ever
have any pornography uploaded.
I think that a lot
of the reason
is that people use
their real names
on Facebook.
When it came to hiring,
they took a pragmatic
dual approach.
On the one hand,
they began poaching
high-caliber developers
from other companies
by offering higher salaries,
primarily targeting
those who worked in Google,
one of the many tactics
that would make
the two competitors
mortal enemies.
However,
they also diversified
their teams
by focusing on
inexperienced raw talent too.
You know,
I started the company
when I was 19, right?
So I can't institutionally
believe that experience
is that important.
You know,
we invest in people
who we think
are just really talented,
even if they haven't
done that thing before.
Technically,
one of Facebook's
major advantages
was how fast
it could process information.
Other social networks
like MySpace
and Friendster
were suffering
from mounting
performance problems
as more users joined,
increasingly struggling
to cope
with the amount
of information
being shared
and consumed.
Facebook's innovation
was a system
for storing data
called the Social Graph,
which was in part
responsible for their
ability to scale rapidly.
It's worth describing
the Social Graph
in a basic sense.
You as a user
are a single node
in a network.
Other people
are also individual nodes too.
If you send them
a friend request
and they accept,
a connection is formed
that describes
the relationship
between both of you.
And it's the connection
description
that's crucial here.
Say you make a post
of some kind.
This post becomes
a new object
in the network.
Its connection to you
is that you're the publisher.
If your friend
comments on this post,
a new connection
is drawn
that describes
their relationship
to it as a commenter.
So, as you and your friends
engage with
and comment
on each other's content,
more and more nodes
and connections are drawn
that explain
the relationships
back and forth.
This means that
everything around you,
all the actions
you've taken
on anything you've seen,
and all the actions
taken by anyone else
that relates to you,
is stored as a massive
constellation of
crisscrossing
connection points.
This allowed the data
to be decentralised
so that information
most relevant to you
could be loaded quickly,
which made Facebook
far faster than its competitors,
who relied on more
traditional databases,
which needed to scan
through numerous
massive tables of data
for each user,
matching them against
other massive tables
for each type
of social interaction,
a process that became
more and more inefficient
as the platform scaled up.
The scaling issue
social networks tend to have
is that users consume
orders of magnitude
more data than they create,
which means the database
storing all this information
is under constant bombardment
by millions of computers
trying to fetch information,
which makes it run slower
and slower the larger
the overall population gets.
To make this run efficiently,
Facebook initially used
the open-source solution
Memcached,
which, like the name suggests,
caches versions of the database
on multiple different servers
distributed all over the world.
This way,
computers fetch information
from Memcached servers
rather than directly
from the database.
If the information
is missing from the cache,
the Memcached server
fetches it from the database
and stores the result.
As the years progressed,
Facebook would build
more sophisticated systems
on top of this
so that it could keep up
with the rapid growth in users.
This investment in scalability
would prove to be
one of the decisive advantages
that Facebook had
over Friendster and MySpace.
Moreover,
armed with real identities
and a rich database
of connections,
it was possible for Facebook
to make accurate
friend recommendations
that greatly sped up
the process of adoption.
Just as all of this
was beginning to really lift off,
in late 2005,
the influential author,
Tim O'Reilly,
published a now-famous article
titled
What is Web 2.0?
In this article,
he predicted
that the future powerholders
of the web
would be the platforms
who harnessed
what he called
the collective intelligence
of the internet.
In other words,
those who controlled
the most data.
The race is on
to own certain classes
of core data
location,
identity,
calendaring of public events,
product identifiers
and namespaces.
The winner will be the company
that first reaches critical mass
via user aggregation
and turns that aggregated data
into a system service.
He may not have known
at that exact moment
that he was describing Facebook
two years in the future.
In 2005,
Facebook released the ability
to upload photos
and also the ability
to tag people in them.
If someone tagged you in a photo,
it would automatically be linked
to your profile,
making them immediately accessible
to anyone you were friends with,
which helped to make the site
feel more alive and engaging.
They also introduced
events around this time too.
This feature allowed users
to organize social gatherings
or other kinds of public events.
In the early days,
it was used by almost everyone I knew.
Gigs, meetups,
house parties,
you name it.
It was a major aspect
of how Facebook felt
like the center of your social life.
Even now,
thinking back to how everyone
used to use events
makes me feel a little nostalgic.
Around this time,
they changed the name of the site
from the Facebook
to just Facebook
and bought the Facebook.com domain
for $200,000.
2006 was an enormous year.
In September,
they opened up registrations
to the whole world.
Now, anyone over the age of 13
who had a valid email address
could join.
But this new version of Facebook
that the public saw
was very different
because it included
a revolutionary design change,
something it's not hyperbolic to say
would go on to change the internet
and the wider world.
The newsfeed.
Prior to the newsfeed,
Facebook was a static site
where if you wanted to find out
what your friends or family
were up to,
you visited their wall
which lived on their profile page.
From there,
you'd post messages to them
and wait for a response.
If you didn't do that,
you missed out.
To find information,
you basically went to
one person's profile
and then another
and then another.
It was hugely inefficient.
We knew we could do better.
To solve this,
the newsfeed aggregated posts
and activities
from all of your friends' profiles
and collected them together
in one continuous chronological feed.
We wanted to deliver
10 million unique newspapers
every single day.
It was undoubtedly a major success
despite an initial uproar
from many of its users
who didn't expect
such a significant change.
They were used to
one-on-one discussions,
not one-to-everyone discussions.
And there were some other
unexpected consequences too.
Say for example,
you just went through a breakup
and changed your relationship
status to single.
Now, due to the newsfeed,
that life event
would suddenly be broadcast
to all your family and friends
as well as many of your partner's
family and friends too,
bringing what would be,
for many,
an enormous amount
of unwanted attention
to a particularly painful episode
in your life.
We woke up
to hundreds of outraged people.
Groups had formed
in the middle of the night.
Groups like
I hate Facebook
and Ruchi's the devil.
Amidst all that chaos
and all that noise,
we noticed something unusual.
Even though everyone
said they hated it,
engagement had doubled.
There were more page views
than there ever were before.
Aggregation feeds
might not have been a new idea
and there were already
somewhat comparable examples
on other social media sites
such as Flickr.
However,
Facebook's application
of this concept
to the way we interact
with our friends
was massive.
Especially when they began
applying sorting algorithms
later to determine
what content should appear
in our feeds.
In July 2006,
Mark was faced
with a crucial decision
when the search giant
Yahoo offered to buy
Facebook for $1 billion.
An offer which,
after some soul-searching,
he eventually turned down.
That was really stressful
because a lot of people
really thought
that we should sell the company.
So for a lot of the folks
who joined early on,
being able to sell a company
for a billion dollars
after a couple years,
that was like a home run.
And it is a home run.
The part that was painful
wasn't turning down the offer.
It was the fact that
after that,
huge amounts of the company quit
because they didn't believe
in what we were doing.
2007 is when Facebook
began to demonstrate
to the wider world
how much of a big deal
it was going to be.
This is the year
where we can look back
and see that Mark Zuckerberg
was, from a business perspective,
becoming a highly savvy
long-term strategist.
Case in point,
in April,
they launched
the Facebook platform.
This allowed third-party developers
to create applications
and games that ran on Facebook,
which turned out
to be an incredibly valuable
two-way relationship.
Anyone who has ever worked
in a startup can tell you
that the hardest part
of trying to grow
a digital business
is getting people
to even notice you exist.
In particular,
getting them to register.
No one likes
signing up to things
and in order to convince them,
you need to spend money
on marketing.
So if you were an indie developer,
you either attracted investors
to help you with things
like marketing
or you were almost
certainly screwed.
But now,
Facebook created a way
that software developers
could access users' data
from the Social Graph,
allowing you to sell your wares
to the largest set
of connected users
in the world.
For game makers,
this was enormous.
If people played
their game on Facebook,
their friends would get notified
about it in their news feeds.
Free advertising.
As I mentioned,
I worked in a gaming company
who thrived on this platform.
Our access to the Social Graph
was absolutely crucial
for our survival.
Facebook was no longer
just a site,
it was a platform
with endless different kinds
of applications and games
that would keep people
coming back
and staying longer.
By 2012,
Zynga,
the biggest name
in social gaming
on the platform
at the time,
made up 11%
of Facebook's revenue.
I'll never forget
this period in London
when there was investor money
sloshing around everywhere
looking for the next
big social gaming company.
I remember interviewing
at one of the darlings,
Playfish,
who'd just been bought by EA.
They were so in need
of people with excellent
Flash skills,
they made me an offer
on the spot.
And then I used that contract
to negotiate a better offer
with the company
that I eventually joined.
The second major change
in November of 2007
was Facebook Ads.
The significance of this move
is one I doubt I need
to expand upon all that much.
It was a system
that now also granted
advertisers access
to the rich data
contained in the social graph
so they could target
their adverts
with a level of precision
never seen before.
Age, gender, region,
interests,
previous purchases,
you name it.
Four months later,
they hired Sheryl Sandberg
to take the role
of Chief Operating Officer.
Sandberg had previously
been instrumental
in growing Google's
advertising business
and her movement
to Facebook
was a big coup.
Facebook Ads combined
with Sandberg's experience
is one of the major factors
that led to Facebook
becoming a tech giant.
According to the New York
Times reporters
Shira Frankel
and Cecilia Kang,
Sheryl was the perfect foil
for Zuckerberg.
She oversaw all the departments
that didn't interest him,
policy and communication,
legal,
human resources
and revenue creation.
Drawing on years
of public speaking training
and on political consultants
to curate her public persona,
she was the palatable
face of Facebook
to investors
and the public.
It was also in 2007
that Microsoft
invested in Facebook,
buying $240 million
in preferred stock,
thereby valuing Facebook
as a $15 billion company.
At the time,
this investment
was criticised
for being too cavalier.
Now,
it's clear that Microsoft
could see
what had been predicted
by Tim O'Reilly.
They understood
the ridiculous power
that comes with having
the world's largest database
of real user identities,
who,
through their usage
of the platform,
were constantly feeding
the system
with more and more data
about their interests
and behaviours.
Later in 2007,
Facebook introduced
Pages,
a feature that allowed
entities other than
personal profiles
such as artists,
brands and public figures
to create a presence
on the platform.
We'll be coming back
to this one later.
In 2008,
they launched
Facebook Connect,
which allowed users
to sign into third-party sites
using their Facebook
login details.
This integration
enabled other services
to utilise information
shared by Facebook
and vice versa.
For example,
Spotify used this integration
very effectively,
allowing users
to share their playlists
with friends on Facebook,
which in turn
drove more traffic
back to Spotify.
In return,
Facebook obtained
additional data
and was able to provide
an even richer experience
for its users.
Overall,
this technology
effectively made
any third-party sites
that supported it
an extension of Facebook.
Also in 2008,
they introduced
a new messaging feature
that provided a way
for its users
to speak privately.
Then,
in 2009,
they introduced
what would become
the defining symbol
of the platform,
the Like button.
The Like button
almost immediately
became a new indicator
of social capital,
letting you know
how much people
appreciated the stuff
you posted.
Shortly after,
Likes would also be used
by Facebook
as a vital metric
that would determine
what appeared
in people's newsfeeds.
By June 2009,
they announced publicly
that Facebook
was now being used
by 250 million people,
an incredible milestone.
Finally,
in 2010,
rounding out the core experience
we generally know today,
they introduced
Groups,
a feature that allowed users
to invite like-minded people
to groups
which could be
collaboratively managed
by its members.
This enabled people
to engage in group chats
which could be shielded
from view if needed
by making them private.
And that's it.
An incredible growth story
over a six-year period
where each step
widened the possibilities
that paved the way
for an even larger next step.
This is how they managed
to become vital,
not just to my private
and working life,
but also those
of countless others
around the world.
So,
I hope you enjoyed
my story about Facebook,
how it got started
and where it ended up.
And I expect for some of you
young whippersnappers out there,
a bit of an education, no?
So,
if you'll let me torture
this fake out
for just a few more seconds,
there are some great books
I want to recommend
which helped me a lot
when putting this video together.
The first,
and in my opinion,
the best is
Broken Code
by Jeff Horowitz
of the Wall Street Journal,
released at the end of 2023.
Kind of a strange name actually,
Broken Code,
sounds like vaguely negative.
Anyway,
then there's
An Ugly Truth
by Cecilia Kang
and Shira Frenkel,
which collects together
numerous accounts
from former employees.
Another is an account
by Roger McNamee
who knew Zuckerberg
during the early days.
It's okay.
Not essential reading,
but it has its moments.
That one's called
Zucked.
Gosh,
I should actually
probably read these.
Hold on,
let me just grab my phone,
Kindle,
my fondle.
What?
Ah, right.
Ah, yeah.
Right.
Okay.
Right.
You know what?
Let me just put that down.
This does actually
bring back some memories.
Just for the sake
of being thorough,
just a quick addendum
to this video.
Okay,
there was actually
something that bothered me
a little bit
in the early days.
Like,
no big deal.
I can be finicky
about these things.
Right.
Okay,
so Facebook was an experience
that changed an awful lot
during the first few years,
and although many of the changes
were positive,
there were others
that gave me pause.
For example,
I remember scrolling
through the news feed one day
and coming across
a very simple,
but also,
to me,
rather unusual statement.
Andy Jones likes Budweiser.
That's it,
an image of
the Budweiser brand
with a like
associated with
my friend Andy.
The funny thing was
that since I knew Andy
really well,
I was pretty certain
he wasn't much
of a Budweiser fan.
I mean,
you'll struggle to find
anyone in the UK
or Ireland who is.
The other funny thing
was that since Andy
was a work colleague,
he just happened
to be sitting across
from me at the time,
so I called him over.
Andy,
what do you make of this?
And I'll never forget
the look of confusion
and mild frustration
on his face
when he read the message.
I don't remember
his exact explanation
for what post he'd seen
that he'd liked,
but what I do remember
was his conclusion.
And now they're using me
to sell you Budweiser?
And he legitimately
felt embarrassed
by this association,
as would I.
This was a feature
called Social Ads,
initially launched in 2007.
The idea was simple.
Brands could post
interactive updates
that appeared in the
side panel or newsfeed,
and if you made a comment
or liked the message,
that engagement
would then be broadcast
to your friends' feeds.
This was a popular
method of advertising
because research data
showed that people
were much more likely
to engage with adverts
if they took the form
of a recommendation
from your friends.
As you can imagine,
this was not a popular move,
in large part because
the change had not been
rolled out with any transparency.
Since people were not asked
to give their consent
to be used as adverts,
many became a little
less trusting.
And bear in mind
that this was happening
during a time
where people were beginning
to wake up to a new reality
where their personal information
was being used
for the purpose of targeted
advertising across the internet.
To further erode trust,
there were numerous
privacy blunders,
which people were becoming
more and more used to
even as early as 2007.
One really bad early example
was uncovered by
the privacy activist
and blogger Chris Segoyan.
In a nutshell,
he discovered that
even if your privacy settings
were quite tight
so that your personal information
could only be seen by friends,
it was still possible
to learn deeply personal things
about you
using their advanced search feature.
Say, for example,
you searched for your neighbour,
John Smith,
and limited the search
to the town that you live in.
Boom!
There's John Smith.
Then you refine your search,
this time including the filter
Religion Buddhist.
Oh, John Smith's name
no longer pops up.
I wonder why.
Okay, let's instead
change the religion to Christian.
Ah, John's name appeared again.
I guess he must be Christian.
After a little bit
of additional research
on this exploit,
Segoyan eventually realised
that you could brute force
a lot of personal information
about large numbers of people,
including things like
the kind of groups
they were signed up to
as well as personal identifiers
or hobbies
they listed on their profile.
Using this information,
Segoyan demonstrated
that one could, say,
quickly isolate a group of people
in a university
who fit this criteria.
Christian men
interested in other men.
Muslim women
interested in other women.
Students who work
in local strip clubs.
Students under the age of 18
who self-identified
as being interested
in alcohol, etc.
It's not hard to see
how this information
could be put to very bad use.
When Segoyan published
this particular revelation,
it got picked up
by a few different news publications,
eventually leading to Facebook
tightening up
their privacy settings.
You can still see a comment
from one of the Facebook reps
announcing the fix
on the comments section
under Segoyan's
original blog post.
Another, even earlier,
privacy-related outcry
was caused by
the news feed itself.
Remember that example
I gave earlier
of changing your status
to single
and everyone being able
to see it?
Many people thought
that this was highly creepy
and there was a very large outcry.
So, why did they do it?
Well, let's return to that clip
I showed earlier
by Ruchi Sung-vi,
the then product manager
of the news feed.
Even though everyone
said they hated it,
engagement had doubled.
We may as well
stamp that phrase
into our memory banks right now.
Even though everyone
said they hated it,
engagement had doubled.
Now, apologies
for pointing out the obvious here,
but engagement
and fulfillment
are not the same thing.
If someone clicks on more stuff,
that doesn't necessarily
mean they're happier.
There are many themes
in this video
and this one
will run all the way through.
Nevertheless,
the outcry was so bad
that Zuckerberg published
what was fast becoming
a common feature of the site,
the semi-apology blog post.
Calm down.
Breathe.
We hear you.
People are concerned
that non-friends
can see too much about them.
We're listening to all your suggestions
about how to improve the product.
It's brand new
and still evolving.
And we agree,
stalking isn't cool,
but being able to know
what's going on
in your friends' lives is.
Okay, so if we return back to 2007,
believe it or not,
we've still not got
to the biggest privacy scandal
of that year.
Beacon.
Beacon was an advertising program
that third-party vendors
could sign up to,
which allowed your purchases
on their sites
to be repurposed
into an advert
targeted right back
at your friends on Facebook.
So what are the consequences of this?
When a Facebook user
bought his wife
a diamond ring online,
the surprise was ruined
because Beacon notified
all his friends
and his wife
about it on Facebook.
Oh dear,
cat's out of the bag.
This caused absolute uproar
because users were not informed
about the change
at any point
nor asked for their consent.
Beacon was opt-out.
This prompted
a severe online response
from users
and privacy advocates
as well as a lot
of critical news coverage.
Regardless,
Zuckerberg resisted
making changes to the feature,
justifying it
as a cool,
positive way
to share your tastes
with others.
Is there any concern
you're turning Facebook
into something
much more commercial?
I actually think
that this makes it
less commercial.
I mean,
what would you rather see?
A banner ad
from Bloomingdale's
or that one of your friends
bought a scarf?
Well,
I guess this shows
how difficult it is
for a company like yours
to make money
through advertising
and protect people's
real privacy
and their sense of privacy.
You know,
it might take some work
for us to get this
exactly right.
This is something
that we think
is going to be
a really good thing.
There have to be ads
either way
because we have
to make money.
I mean,
we have 400 employees
and, you know,
we have to support
all that
and make a profit.
No one bought this.
Soon after,
Zuckerberg then posted
another apology blog entry
on Facebook
titled
Thoughts About Beacon.
We've made a lot of mistakes
building this feature
but we've made even more
with how we've handled them.
We simply did a bad job
with this release
and I apologize for it.
Again,
many were not impressed
by this apology
especially because
there was no suggestion
Beacon was going to be discontinued.
It was merely a
bad job.
Then,
even worse,
a researcher named
Stefan Berto
published a blog post
revealing that Beacon
tracked users' activity
and sent it to Facebook
even when they weren't
signed into Facebook.
A serious privacy breach
even for 2007.
Eventually,
in an attempt
to put the whole fiasco
to bed,
Zuckerberg then agreed
to make Beacon opt-in
but by then
there was already
a class-action lawsuit
filed against them
which they eventually settled
for $9.5 million
in 2009
and then,
to coincide with this payout,
Facebook finally agreed
to discontinue Beacon entirely.
In February 2009,
Facebook made changes
to their Terms of Service.
These changes
appeared to say
that Facebook
could hold onto users' data
even after they deleted
their accounts
and that this information
could be used
for advertising purposes.
This change was reported on
by a now-defunct blog
called The Consumerist
and sparked enormous backlash
and media attention.
The pressure eventually led
to Facebook reverting back
to their previous Terms of Service.
But regardless,
its effect was to broadcast
what looked like
a presumption of permanent ownership
over users' personal data.
In a similar vein,
another increasingly unpopular feature
from a privacy point of view
was Find Friends,
a service that allowed people
to use their address book
of email contacts
to quickly locate their friends,
which was much more convenient
than searching for them manually.
However,
what users didn't realize
was that the addresses
they were uploading
could be stored
and used to build
what's referred to as a
shadow profile
for those not yet on the platform.
Worse still,
this data could potentially
be shared with advertisers too.
This meant that at the time,
Facebook could obtain
and benefit from your personal data
even if you actively chose
not to be on Facebook.
And there we have it.
Facebook's early privacy issues.
And until a few months later.
Another black eye for Facebook,
the social networking giant
has reportedly revealed
the sexual orientation
of some users to advertisers.
Okay,
the problem here
was that if you were viewing
a page or group
related to a specific
sexual orientation
and clicked on an ad
while there,
advertisers could potentially
deduce where you would come from
by looking at the referrer URL,
even if your profile
was set to private.
But that's not all.
And just last week,
we learned some of the most
popular apps on Facebook
were leaking users' information
to advertisers.
This one was a bigger deal.
The crux of the story was this.
If one of your friends
was playing a popular game
like Mafia Wars
or Farmville,
even if you never opted
to play the game yourself,
the games could still scrape
your personal information
and give it away
to fourth-party advertisers.
It was a problem reported
to have affected
tens of millions of users.
So why was this possible?
Well,
in 2010,
Facebook released a tool
called the Open Graph,
which granted outside developers
access to data available
from the social graph.
And in order to make sure
that the Open Graph was adopted,
it included quite the
personal information giveaway.
Usernames,
locations,
emails,
birthdays,
political persuasions,
relationship status,
and employment history,
as well as the activities,
posts,
and likes of one's friends.
The Open Graph
puts people
at the center of the web.
One former Facebook employee,
Sandy Parakelas,
warned at the time
that the Open Graph
could be misused
for mass data harvesting
by bad actors,
but claims that her objections
were ignored.
Just store this one away for later.
Open Graph.
We have not heard
the last of this.
It's like deja vu.
We're always talking about
Facebook and privacy.
Now,
aside from Beacon
and the Open Graph,
you might have noticed
that a few of the errors
I've just described
were unforced errors.
In fact,
unforced errors
basically came to typify
the platform during this time.
But why do they keep happening?
Well,
partly it's due
to the development ethos
that underpinned Zuckerberg's
now infamous catchphrase,
move fast and break things,
also known as
done is better than perfect.
The idea is this.
When trying to grow
in a highly competitive landscape,
it's better to get a feature
released as quickly as you can
so that you can collect data
on how it's being used.
From there,
you can use that data
to inform how you refine
the feature and make it better.
Or,
if it's underperforming,
kill it completely
and move on.
In this way,
when you fail,
you fail fast,
rather than wasting tons
of time and resources
polishing things
that are destined
to be dead on arrival.
However,
not all features
are created equal.
The consequence of a page
failing to load
is embarrassing for sure,
but fairly trivial
compared to the consequences
of mishandling
your users' private data.
However,
this distinction
does not appear
to have been treated
with enough importance
by Facebook
and there are
innumerable accounts
from former employees
indicating that the company
was a total mess
when it came to stability.
According to Jeff Horowitz,
Bonuses and promotions
were doled out to employees
based on how many features
they shipped.
Reviews incentivized employees
to complete products
within six months,
even if it meant
the finished product
was only minimally viable
and poorly documented.
Engineers
and data scientists
described living
with perpetual uncertainty
about where user data
was being collected
and stored.
According to Brian Boland,
a former vice president
in Facebook's
Advertising and Partnerships division,
building things
is way more fun
than making things
secure and safe.
Until there's a regulatory
or press fire,
you don't deal with it.
Now, of course,
Facebook stated at the time
that privacy
was of the utmost importance
to them.
But, to rattle off
an old cliche,
it's what you do,
not what you say
that indicates
where your real priorities lie.
And with that in mind,
let's begin building a profile
of Facebook's actual philosophy.
We've already included the phrase
even though everyone hated it,
engagement had doubled.
Now let's add,
done is better than perfect.
These are both aspects
of the same general drive
to grow as quickly as possible.
Bear in mind
that at the time,
Facebook was by no means
a guaranteed long-term.
success
and Zuckerberg
was fully aware of this.
Yes, it hit 250 million users,
but, as we'll see later,
there was an enormous amount
of growing left to do.
Friendster and MySpace
had already demonstrated
what happened
to complacent social media firms
and it was entirely possible
that a new upstart
could come along
and knock Facebook
out of the game.
This fear was even expressed
in an internal manifesto
handed out to employees
called the Red Book.
If we don't create
the thing that kills Facebook,
somebody else will.
To go back to Tim O'Reilly's
observation in his famous
What is Web 2.0 article,
the race is on to own
certain classes of core data,
location, identity,
calendaring of public events,
etc.
The key word here is race.
This was a race
and Zuckerberg understood
this better than most.
He was in this game to win
and the only way to win
was to become so big
and so dominant worldwide
that you couldn't really be touched.
As his co-founder,
Chris Hughes, put it,
From our earliest days,
Mark used the word
domination to describe
our ambitions.
And this leads us, neatly,
to the biggest privacy scandal
during Facebook's
formative years,
a scandal largely in response
to the rise of a new upstart.
Twitter
Founded in 2006,
Twitter began to explode
in 2007,
going from 20,000
to 60,000 tweets per day.
Its model was completely
different to Facebook
because everything you posted
was public.
This made it an attractive
platform for celebrities,
journalists,
and organizations
who could reach a wide audience
without requiring
a prior connection.
Tweets could go viral
to the world
in ways that Facebook
posts couldn't
and it was also more suited
to breaking news
and real-time coverage
of live events, too.
At this moment in time,
Twitter was a major competitor
that Facebook needed
to clear off the board.
So, in 2008,
Zuckerberg tried to acquire it.
However,
talks quickly broke down
due to disagreements
about the valuation of the company
and Twitter decided
to carve out their own path.
And, as Twitter
continued to skyrocket,
Zuckerberg began to realize
that the very thing
that had put Facebook
on the map,
being able to converse
with your circle of friends
out of view of the general public,
was becoming a limiter to growth.
If Facebook users
would instead post
their thoughts publicly,
they'd not only be able
to blow Twitter out of the water,
they'd also become
even more attractive
to advertisers
and third-party developers, too.
When people share more,
the world becomes
more open and connected.
And in a more open world,
many of the biggest problems
we face together
will become easier to solve.
So, in December 2009,
they introduced
the Privacy Transition Tool,
a pop-up that appeared
in front of all Facebook users,
which contained
a large and complex
set of privacy controls,
ostensibly created
to help users tweak
their privacy settings
more precisely.
And if they wanted
to skip all this detail,
they were encouraged
to press a button
that simply accepted
the recommended settings
and then they could
get on with their day.
But what was not obvious
about this was that
Facebook had changed
whatever defaults
people had previously set up.
Information that was supposed
to be only viewable
by friends
had automatically been changed
to a new setting,
viewable by everyone.
I remembered this moment
really well.
Some pop-up appeared
about some new widget
they'd created
where I could fiddle
with privacy settings.
That's nice.
Didn't really care too much
because I was already
reasonably pleased
with how it worked.
And it wasn't until
months later that I realised
my profile was now
basically public.
And I'll never forget
how irritated I was
when I finally investigated
these new settings in detail
and saw the new defaults.
I would never have chosen
those settings
had I been presented
with them clearly
and it was definitely
one of those key moments
for me where I began
to regard Facebook
as a suspicious
and untrustworthy company.
I'd argue that this
specific scandal
was the one that began
seriously altering
the general public's
impression of Facebook.
It was no longer
a cool site that made
a bunch of dumb mistakes
which could be forgiven.
It was now a site
that we all felt
could be relied upon
to intentionally deceive you
for their own benefit.
The move seemed
too transparently self-serving
for there to be
any other explanation.
We are here today
to urge Facebook's creator
and CEO, Mark Zuckerberg,
to revisit this new policy
as soon as possible.
We believe Facebook
should reverse its policy
to begin exploring guidelines
so users know up front
what's happening.
Do you feel like
it's a backlash
or that you feel like
you're violating
people's privacy?
You know,
there were real learning points
and turning points
along the way
in terms of building things.
If I knew what I knew now,
then I hope I wouldn't
have made those mistakes
but I can't go back
and change the past.
I can only do what we think
is the right thing going forward.
In 2011,
the FTC would ultimately issue
an eight-part complaint
against Facebook
culminating in an order
to cease these kinds of practices.
The resulting settlement
forced Facebook
to take a number
of different steps
to improve their protection
of users' personal data,
including submitting
to external audits.
At the time,
it appeared that these steps
might finally bring Facebook to heel.
But since all this
was generating so much bad press,
Facebook decided to go
on the offensive
by hiring the public relations firm
Burson Marsteller
who came up with
a pretty silly scheme.
Deflect attention
away from Facebook
by trying to make
other tech giants look bad.
To do this,
they contacted tech bloggers
and tried to convince them
to write negative stories
about Google.
Specifically,
by feeding them a weak story
about one of Google's features
which they said
was misusing users' private data.
And to sweeten the deal,
they suggested
that if the bloggers
did write these stories,
they'd help to get them
published in prestigious outlets
like The Washington Post
or Politico.
One of the bloggers
they emailed was
Chris Segoyan,
the guy who broke the story
about Facebook's search exploit.
Chris,
immediately sensing
that something suspicious
was going on,
emailed back.
Who's paying for this?
Not paying me,
but paying you.
To which they responded,
Thanks for the prompt reply.
I'm afraid I can't disclose
my client yet,
but all the information
included in this email
is publicly available.
Any interest in pursuing this?
Irritated,
Segoyan published
the email exchange
and soon after,
USA Today
finally linked it back
to Facebook,
causing them
a significant amount
of embarrassment
since the scheme
was a pretty ham-fisted one.
They really thought
it was okay
to tell a journalist,
don't worry,
I'll write your story for you,
no one will know.
What do they make you feel like
when you get this call
saying,
hey,
I'm going to let you
be a fake journalist
for a day?
So I was a little bit shocked
that Facebook's PR firm
acting on behalf of Facebook
thought that I would go along
with their pitch
and attack one of their competitors,
not for a legitimate
privacy problem,
but for something
that was really
not bothering anyone.
This is not the last time
Facebook will try
to do something like this.
Anyway,
this pretty much concludes
what I think of as
phase one of Facebook's
privacy misadventure,
which very much
sets the stage for phase two.
But for now,
let's move on.
So if we stop for a second
and look at our profile so far,
we've addressed Facebook's goal
of achieving massive growth
as fast as possible
and I've mentioned
their working practice mantra
that causes constant sloppiness.
These are major factors
in their decision making
for sure,
but there's another core factor
I've not yet included,
which it's time to address.
Corporate virtue.
I've worked in a lot of companies
over the years
and have witnessed
on quite a few occasions
how proposals for new directions
can be framed as
acts of virtue,
something altruistic,
not self-serving.
These proposals are often
accompanied by a
whipping up of employees
to get on board.
And sometimes,
when these proposals
get enough traction
and achieve enough
mass self-delusion,
they can give birth
to truly terrible decisions
that baffle observers
on the outside.
The need to be seen
to be virtuous
is especially true
for social media companies
who,
because they serve people,
can't justify their existence
on purely self-interested grounds.
They can't say,
for example,
our goal is to grow so fast
that we acquire the world's data
before anyone else does.
No,
they need to convince those
within and without
that there's a higher moral principle
underpinning their actions.
Facebook's statement of virtue,
their mission,
so to speak,
was at this time
to make the world
more open and connected,
a value consistently posited
as the ultimate good,
because when people are connected,
they're opened up
to the world of information
which ultimately empowers them.
It's not an insane thing
to say in the abstract.
Being connected to networks of friends
has clearly been beneficial
for hundreds of millions of people,
but when the idea of connection
is positioned as the ultimate good,
we begin to see problems.
Later in this video,
we'll see utterly catastrophic problems,
but for now,
we're still only talking about privacy.
We're still in the playpen, folks.
So you might be thinking,
okay,
but where's this internal mass self-delusion?
Well, helpfully,
an internal memo called The Ugly
written by Andrew Bosworth,
one of the founding members of Facebook,
lays that bare.
It was an attempt to pump up employees
who are beginning to feel
the weight of external criticism.
In the memo,
he reminds them of their core mission,
connecting people,
and then goes on to boldly state
that this mission is so important
it justifies many of the privacy scandals
we just looked at.
Maybe it costs a life
by exposing someone to bullies.
Maybe someone dies in a terrorist attack
coordinated on our tools,
and still we connect people.
The ugly truth
is that we believe
in connecting people so deeply
that anything that allows us
to connect people more often
is de facto good.
And that's why
all the work we do in growth
is justified.
All the questionable
contact and porting practices,
all the subtle language
that helps people
stay searchable by friends,
all of the work we do
to bring more communication in.
Anyway,
this statement of corporate virtue,
connecting people
is the ultimate good,
has to be added to our profile.
This one is the rocket fuel
because it sanctions
the first two statements.
And while we're on the topic
of corporate virtue,
things were about to get
a bit more complicated
when, in 2012,
Facebook became a public company,
their initial public offering
being one of the most
anticipated in history.
Our mission isn't
to be a public company.
Our mission is to make
the world more open
and connected.
From this point onwards,
when Mark made decisions,
there'd now be shareholders
expecting those decisions
to deliver returns.
Anyway,
the statements of virtue
just kept on coming.
Remember that internal manifesto
I mentioned a minute ago,
the one they handed out
to employees called
The Red Book?
Well,
it contained a pretty
prophetic statement.
Changing how people communicate
will always change the world.
Apart from being a bit silly
and self-aggrandizing,
this statement also contained
an obvious logical flaw.
You know,
change isn't always good.
And man,
was Facebook about to change?
Starting around 2011,
I began to notice
something quite concerning.
My closer friends
increasingly seemed
to be routinely ignoring
any of the things I posted,
especially when they were
think pieces,
ideas about larger topics
I was interested in
that I wanted to get feedback on.
And on the flip side,
I was becoming more
and more disappointed
by how others seemed
to be changing
what they shared too.
Posts were rarely
conversational anymore
and were increasingly
becoming one-liners,
trite jokes
and toothless observations,
memes, trivialities,
or just links
to YouTube videos
with no commentary.
At the time,
I interpreted this
as a general lack of interest
in actual conversation
and an increasing tendency
to be lazy.
This came to a head
in 2013
when I took a career break,
moving to Glasgow
to study a master's
and composition.
So, once again,
I found myself
in a new city
where I didn't know anyone
and once again,
I tried to take some refuge
in Facebook.
But this time,
it was totally different.
Almost no one
saw anything I posted
and I began to develop
a mild hang-up about it.
Sometimes I get
no response from anyone.
It wasn't until I asked
one of my friends
straight up,
are you not seeing
the stuff I'm posting anymore?
His response?
What stuff?
Incidentally,
while making this video,
I looked back over
some of my posts
after this period,
which didn't take long
because there are
only a handful of them
and noticed an interesting
one from 2018.
A link to a YouTube video
I just released,
a comic review
of the notation app
Sibelius.
On YouTube,
this was my first experience
of a properly viral hit
seen by hundreds of thousands
of people
within the first week
of its release.
On Facebook,
its total,
as of today,
is 13 likes
and 5 comments,
which was high for me.
All of the comments
bar one
are from immediate family members.
None of the hundred
or so composers
or music teachers
I'm connected to
on Facebook
saw it there.
All of them
saw it on YouTube.
I don't know
when it happened,
but Facebook made a decision
at some point
that Martin Keery
is as boring as hell.
Send him right to the back
of the queue every time.
But here's the thing,
I wasn't the only person
experiencing this.
My assumptions
that my friends
were getting sick of me
were largely wrong,
at least I hope so.
And my perception
that people were becoming lazier,
causing them to post
dumber stuff,
was also off the mark too.
So,
what was going on?
Well,
the main reason was
that the news feed
kept changing at a rapid pace.
Initially,
it was guided by
a simple algorithm
called EdgeRank,
which decided
what content
you should see.
Its purpose was
to prioritise posts
by the friends
it deemed
were most likely
to be important to you,
judged in large part
by how much
you engaged with them.
Over time, though,
EdgeRank was gradually
replaced with more
sophisticated machine
learning-driven algorithms,
which started to optimise
towards maximising engagement
and the amount of time
you spent on the platform.
The justification being
more time equals more good.
The introduction
of the like button
supercharged this situation.
People began to start
thinking of their posts
in terms of how
successful they were
and Facebook now
had a powerful metric
to help them determine
what posts should be boosted.
Now, since my posts
were only intended
for a few people
and therefore didn't
get much engagement
compared to more
widely appealing memes
and one-liners,
this changed my entire
feeling about what
Facebook was for.
It made me feel like
I was now kind of
in competition for attention.
The most successful thing
I ever posted on Facebook
was a really simple observation
about some workspace jargon
I noticed people
were beginning to use.
Those who saw this post
started chiming in
with examples of
workspace jargon
they found agonizing
and the post blew up.
For days,
people commented on it
and it was a nice feeling
for as long as it lasted
but what's the lesson here?
Think like an editorial team?
Think of things to say
that will draw people in?
That will get clicks?
What's the point?
I just want to talk to my friends.
It's too much work.
But there were plenty of people
who did get drawn
into this competition
and many of them
started learning
from the algorithm,
discovering new techniques
that tended to get
more engagement.
My absolute least favourite
of these,
and I know this is not
an original observation
is what I'm going to call
the Living the High Life post.
You and your friends
on a yacht
drinking champagne,
glamour shots
in exotic locations,
posts where you allude
to how successful you are.
Now, early on,
I was a little bit guilty
of this last one.
I definitely bragged
a little bit about things
I thought were impressive
until I read a blog post
titled
7 Ways to be Insufferable
on Facebook
by Tim Urban,
creator of
Wait But Why.
His main observation
was that there are fundamentally
two different kinds
of Facebook posts.
Those you post for other people
and those you post for you.
Let me give a few examples.
Heading to Hawaii
with the love of my life
for a month of relaxation,
yoga and shopping.
Need to be careful though,
10k can disappear like that.
What's the purpose of this post?
Well,
to craft an image of yourself
so that others are jealous of you.
Just got my degree.
Distinction, baby!
This second example
looked at through
Tim Urban's critical lens
was a bit of an eye-opener for me
because I recognised
that I was slightly guilty
of doing this.
Although I hadn't specifically posted
my degree results,
which were excellent,
I definitely posted
other achievement milestones
over the years.
Now, in one respect,
so what?
I think a legitimate
life achievement
should be celebrated
because they don't happen every day
and it's understandable
that you'd want to share them
with loved ones and friends
even if you are
kind of boasting a little bit.
But this blog
was now forcing me
to confront an awkward problem
I legitimately
had not considered until then.
Unintentional harm.
What if one of my friends
was having a hard time
and especially bad breakup,
flunking out of university,
losing their job,
a serious bout of depression,
etc.
And at their worst moment,
they suddenly see my post
about how well things
are supposedly going for me,
which acts as a punch
in the stomach for them.
And the worst part is,
I wouldn't know I'd done it.
The moment I started
pulling that thread,
I realised social media
had a lot more serious
consequences than
I'd first imagined.
We'll get back
to this topic later.
There were plenty of other
major social changes
happening too,
some of which were
natural evolutions
that can't be blamed
on Facebook.
For example,
that moment
when you get a friend request
from your mom.
I remember jumping on a chat
with my three brothers
to discuss how we were
going to handle this,
ending with us all
jointly realising that,
like,
we're hardly going to reject
a friend request from our mom.
It turns out that
most of the people we knew
were also coming to
the same conclusion
and bam,
the older generation
are all suddenly here,
all suddenly commenting
on our posts
and browsing with
mild concern
through the photos
we've been tagged in.
Photos of our nights out,
gigs we played
in crazy goth clubs
with naked people
dancing around.
Another thing that affected
Facebook usage
were the horror stories
we kept reading about,
where people got fired
from their jobs
because they didn't realise
the things they thought
were private
were actually public,
including posts
where they complained
about how much
they hated their boss.
A famous incident
of social media
foot-shooting
occurred when
college students
who'd appeared
in the movie
Borat
sued the filmmakers.
In the movie,
they'd been seen
guzzling boos
and making racist remarks,
which they argued
was an unfair portrayal
that caused them
significant harm,
including damage
to their chances
of seeking employment.
As part of
the filmmakers' defence,
they submitted images
posted publicly
on the plaintiff's
Facebook page,
which showed them
engaging in similar
kinds of boozy behaviour.
The lawsuit was
ultimately unsuccessful
for various reasons,
but it taught the world
a valuable lesson.
Curate your profiles
carefully.
It could be argued
that all people had to do
was edit their privacy
settings to be more strict,
but Facebook's privacy
settings were miserably
designed and hidden away
where you often
wouldn't go looking.
Bad design aside,
though,
after the privacy
transition tool
debacle in 2009,
we hadn't much faith
that private stuff
would remain private.
So we were all just
much more cautious
about what we shared,
especially given that
our extended families
and work colleagues
were all now in here
with us.
As for the Facebook team,
well,
the changes kept on coming.
In 2012,
to the surprise
of absolutely no one,
Facebook introduced ads
directly into the newsfeed.
These were called
sponsored stories.
Initially,
they didn't perform
very well
and annoyed advertisers
who weren't seeing
the returns they expected,
with some becoming
completely disillusioned,
stating that they were
fundamentally some of the
worst-performing ad units
on the web.
In order to fix this situation,
Facebook stepped up
their investment
in the burgeoning field
of machine learning.
One team,
led by a former
Microsoft AI specialist,
Joaquin Quinoneiro Candela,
started producing
some very powerful results
by taking sophisticated
user classifications,
for example,
men in their early 30s
who like Bon Jovi,
ice cream and rugby,
and combining them
with brute force
machine learning techniques
that started by simply
aiming everything at everyone
and continuously optimising
the results over time
to maximise conversions.
And as the money
started rolling in,
the team continuously
tested hundreds
of machine learning variants
to see which would
make the most money.
And these tweaks mattered.
A successful tweak
could mean additional
hundreds of millions
of dollars in revenue.
Then,
because these successes
were so significant,
they created a tool
called FB Learner Flow,
which allowed
unspecialised coders
around the company
to launch machine learning
experiments on other
parts of Facebook,
including the newsfeed.
But FB Learner Flow
was a black box,
with many engineers
having no clue
what the underlying
machine learning technology
was actually doing.
One former employee
referred to this as
giving rocket launchers
to 25-year-old engineers.
This would go on
to create a situation
where nobody really knew
why or how things
were appearing
in people's feeds
because the rules
that governed them
were total abstractions
only understood
by the machine learning
algorithms themselves,
setting the stage
for some major crises
down the line.
Getting back to 2012,
in May,
another change came
that had a dramatic effect
on news outlets
as well as innumerable
other businesses
who'd created
a Facebook presence
by creating a page
to promote their services.
The change was
the introduction of
promoted posts,
which allowed anyone
running a page to pay
in order to boost
their views.
Bear in mind that
Facebook had originally
built pages
to produce a win-win situation.
Facebook benefits
from diversity of content,
which keeps people
there for longer.
News outlets
and other businesses
benefit because,
if they do a good job,
they can grow their audience
at a rate that wasn't
possible before.
Once that benefit
was established,
many news outlets,
small, medium, and large,
started dedicating their efforts
and resources
into growing a Facebook presence,
becoming more and more
reliant on Facebook
as a source of traffic
and revenue.
They became committed.
But promoted posts
turned out to be
a massive rug pull.
Only two months
after it was launched,
the reach of organic posts,
meaning posts you don't pay for,
started rapidly declining,
leading to numerous accusations
that Facebook had
intentionally engineered
this decline
to force page owners
to pay in order
to maintain their reach.
The reality was a bit
more complicated than this,
though.
For example,
with more and more
news outlets
entering into the space,
the competition to appear
on users' feeds
was getting ever more fierce,
and those running pages
couldn't reasonably have
expected stable growth
forever under those conditions.
However,
the timing,
speed,
and scale
of the decline
of non-paid posts
was quite suspicious.
A TechCrunch article
from around this period
reported a 40% decline
of average non-paid posts
across the board
over the duration
of only four months,
as well as an unconvincing
response from Facebook
representatives
about why this was happening.
The massive reduction
inorganic traffic
sent untold numbers
of indie news outlets
scrambling to recalibrate
their budgets
or simply going out
of business.
And on the consumer side,
we saw less from the pages
we'd intentionally signed up to
and more promoted posts
we didn't care about,
as well as a slew
of annoying ads
that were often
bafflingly irrelevant.
Incidentally,
in 2011,
Facebook did something similar
with respect to gaming
by instituting a 30% tax
on all revenue
earned on the platform.
The social gaming company
I worked for at the time
couldn't recover from this tax
and eventually folded.
But it wasn't just indie games
and news outlets.
Celebrities,
musicians and public figures
who'd built up
a respectable audience
were also having
an increasingly hard time
actually reaching them
organically.
Moreover,
Facebook had its own
strange rules
that dictated
what people saw
and what went viral,
which changed constantly
according to criteria
no one understood.
This situation persists
even today.
And this leads us to
the trash.
Ever-increasing numbers
of clickbait articles
and spam,
much of which was now
being force-fed us
via our newsfeeds.
A problem made worse
as Facebook altered
its mechanics
to maximise the effects
of virality.
Removing the limits
for how many friends
you could make in one day.
Removing the limits
for how many places
you could cross-promote
the same article.
Moves that were gifts
to spammers
who could piggyback off posts
that had already gone viral
in the past
by simply copying
and reposting them
to as many people
in as many different spaces
as they wanted.
This meant not only
that we saw more
and more trash
but that we kept seeing
the same trash
over and over again.
Learn weird tricks
that will help you lose weight.
This weird trick
helps cure diabetes
in 30 days.
You'll find out
how these tasty pastries
contain a real ingredient
that reverses diabetes.
I do not know
how long this video
will be online.
Your total investment today
is now $19.99.
Listen,
don't even decide right now.
As social media
became more and more prevalent,
a new term began to be used
in research and media coverage.
Brain hacking.
The practice of introducing
mechanics that encourage
frequent use
which maximizes the potential
for capturing people's attention.
These included
automatic emails
that notified you constantly
when any type of thing happened.
Someone liking your post
for example.
Infinite scroll
which keeps loading
more content
so you never get the feeling
you've reached any kind
of checkpoint
where you can catch your breath.
Videos that play automatically
in your feed
which maximizes the chance
of catching your attention
and
my least favorite,
annoying red notification medallions
all over the place
that never go away.
Even if you check your notifications
and clear them
they still go red again
almost immediately
to tell you something.
Anything.
Just so you click on them
and lose a few more minutes
out of your day.
Facebook didn't invent
half of these techniques
of course
but they employed
basically all of them.
In combination
they made the experience
more exhausting
and unbelievably less fulfilling.
So all of this
taken together
the notifications,
the trash,
the adverts,
the insecurity about privacy,
the engagement patterns
and algorithm changes
that rewarded either trivial
or downright toxic posts.
It left me
and many others like me
to suddenly find
that there wasn't
very much for us
on Facebook anymore.
It had distanced me
from my friends
who didn't see anything
I posted
and vice versa.
My news feed
was full of random crap.
I couldn't express myself honestly
because I couldn't quite trust
that Facebook wouldn't
trick me into making it public
and even when I did
want to post something
the new engagement rules
that assigned a score
to my efforts
were pushing me
towards inane
one-linerism.
That moment
when I began my masters
in Glasgow
with the crazy darkness
at 3pm
not having made any friends yet
doing the long distance
relationship thing
with my now wife
who was still working
in London at the time
that's when I decided
that Facebook was no longer
a good place to be.
I preferred to just
deal with the darkness
and the loneliness
on my own
without Facebook
wearing me down further
with its relentless mission
to serve me nothing
but depressing
irrelevant garbage.
And so
I became one of the
zombie users of Facebook
logging in
just to check to see
if I had any messages
and then leaving real quick
and I wasn't the only one.
In 2013
according to an article
in The Information
the sharing of original
personal posts
had declined by 21%
and a year later
by a further 16%.
It was now much more
of a content consumption site
which prioritised
low quality videos
and also news.
And just one last thing
about change.
While I was putting
this video together
I decided to look back
over my old timeline
starting right at the beginning
and what I found was
really weird
and a little bit sad too.
Conversations that
no longer make any sense
because the people
I had them with
have since left the platform.
So it's often just
my responses.
It reads like I'm having
a mental breakdown
talking to myself
and this led me to
a chilling realisation.
For me
all I have to look forward to
is more of this.
Facebook is eventually
going to become a mortuary.
A reminder of all my friends
and family who've either
left or died
where the evidence
of our interactions
are then scrubbed away
like a Soviet-style
rewriting of history.
Oh look
another ad
deceased likes Budweiser.
That's not something
I want to experience
and rather than
leaving it up in zombie form
I've now decided
to cross the point of no return
and delete my account entirely.
That said
it was really 2013
where my story
with Facebook ended.
If this is where it all ended
the story would be
one of a wasted opportunity
and a major disappointment
but Facebook would roll on
without me
and instead
the story would go
from disappointment
to outright tragedy.
So let's begin this part
of the story
with a quick look
at how Facebook
was broadly perceived
by the public
in the early 2010s.
You see
even though more and more people
were becoming frustrated
with Facebook
many still viewed it
through a positive lens
especially during
the Arab Spring Uprising
where activists
in Tunisia and Egypt
used it to organize
protest events
and provide the public
with information
that contradicted
the narratives
provided by official state media.
The success of the uprisings
as well as the part
that social media
had to play in them
was widely reported
around the world.
One of the key organizers
of the Egyptian protests
Wael Ghonim
praised Facebook
for making the uprising possible.
You're giving Facebook
a lot of credit for this?
Yeah, for sure.
I want to meet Mark Zuckerberg
one day and thank him actually.
The idea that Facebook
could help people
overcome censorship
and produce positive change
made an impression
on a large number of people.
Although Facebook
had become something
that was viewed with skepticism
by civic-minded observers
the Arab Spring
was an encouraging sign
that convinced
some highly talented leaders
to overcome their skepticism
and join the company
applying their skills
to issues around security
privacy
protection of minors
counter-terrorism
and more.
In 2010
Zuckerberg was named
Times Person of the Year.
In 2012
Obama's re-election campaign
once again successfully demonstrated
the pioneering use of social media
especially Facebook
to reach voters.
Zuckerberg and Obama
were seen together often
around this time
most notably
during a town hall meeting
at Facebook's headquarters
in Palo Alto.
For sure
Obama's unique global popularity
aided the perception
that Facebook was a tool
for good
in the democratic process.
Part of what makes for
a healthy democracy
is when you've got citizens
who are informed
who are engaged
and what Facebook
allows us to do
is make sure
this isn't just
a one-way conversation.
Facebook's headquarters
were buzzing with experts
and researchers
looking to understand
whatever they could
about how the platform
affected society
and the ever-changing nature
of online behavior.
Much of this was done
specifically to spot
and avoid risks
and a lot of research
was published internally
for other employees
to read and act upon.
Unexpected issues
occurred all the time
big and small
and it was the job
of various different teams
to identify problems
and suggest solutions
before they got out of hand.
The only problem
was that within
the corporate structure
which was very much tied
to improvements
and core metrics
many of these
civic-minded employees
felt that they were
second-class citizens
compared to the teams
tasked with
achieving rapid growth.
It's much easier
to demonstrate
that you brought in
more users
than it is to demonstrate
that you prevented
something bad
from happening.
To illustrate
let's return to the issue
of trash content
a problem that was
continuously growing
throughout the early 2010s.
If you clicked on
a junk science article
you'd be recommended
not just more
from the same authors
but related pages
that also peddled
junk science.
Facebook was so heavy-handed
with these kinds of
recommendations
that many began
to game the system
by setting up
multiple pages
that all peddled
the same type of content
which had a multiplier effect
unbeknownst to Facebook
that sent overall views
for trash peddlers
into the stratosphere.
This was exacerbated
by the blatant plagiarism
I mentioned earlier.
If you'd no scruples
you could simply
find a page
with a large following
and start copying
and pasting
its most viral content
on your page
as well as viral content
from Twitter,
YouTube or Reddit.
Viral content
always has high potential
to go viral
again and again
so with Facebook
failing to adequately
enforce rules
against plagiarism
while simultaneously
supercharging
anything that went viral
you had the perfect
environment for trash.
Easy,
high rewards,
no punishment.
One of the best examples
of this were the infamous
Troll Farms
run in large part
by teenagers in Macedonia.
Through trial and error
these teenagers
had managed to figure out
how to game
Facebook's engagement metrics
and were making ad money
hand over fist
by simply reposting
whatever viral trash
they could lay their hands on.
They'd amassed
an enormous number of pages
with high followings
contributing to a situation
where the top 10 pages
for popular subjects
were often led
by Troll Farm creators
rather than legitimate ones.
Troll Farms overwhelmingly
dominated the top 20
most popular Christian pages
including
the most popular page of all
Be Happy,
Enjoy Life.
They also dominated
the top 20 pages
aimed at African Americans
including the top page
My Baby Daddy Ainge.
According to a report
by the data scientist
Jeff Allen
our platform has given
the largest voice
in the Christian American community
to a handful of bad actors
who based on their
media production practices
have never been to church.
Our platform has given
the largest voice
in the African American community
to a handful of bad actors
who based on their
media production practices
have never had an interaction
with an African American.
Adding more fuel to the fire
Facebook made additional
tweaks to their algorithm
giving more weight
to comments and emojis
like the angry face.
This meant that
if you saw content
that made you really angry
and you decided to comment on it
getting into a little argument
along the way
you were more likely
to see content
from that same source again
and since Facebook
didn't pay enough attention
to what was actually
taking place on their platform
users' anger appeared to them
to be simply
an improvement in engagement
an abstraction
on an engineer's
analytics dashboard.
Worse still
this produced a situation
where tribal polarizing content
which tended to get more likes
ended up drowning out
moderate thoughtful content.
Wa'el Ghanim
the Arab Spring organizer
we saw earlier
himself became very worried
about these problems
in later years.
If you increase the tone
of your posts
and become more exclusionary
against your opponents
you're going to get
more distribution.
So if I call my opponents' names
my tribe is happy
and celebrating.
This was the situation
at Facebook
when primaries
were taking place
in the run-up
to the 2016 US election.
More and more content
about either Trump
Bernie Sanders
or Hillary Clinton
started to go viral
and since trash content
tended to make people
argue more
highly partisan content
disinformation
or rage bait
tended to get more engagement
which in turn
made it more likely
to go viral
and therefore
more likely to be recycled
by nihilistic
Macedonian teenagers
half a world away.
One story had Hillary Clinton
dying mid-campaign
and being replaced
with a doppelganger.
Another saw the Pope
endorsing Trump for president.
To make matters even worse
there was another
more sinister way
that Macedonian teenagers
could make money
from their pages.
They could sell them
along with their massive
followings to third parties.
Third parties
whose identities
and motivations
were a mystery.
We'll come back
to this in a moment.
Other beneficiaries
of Facebook's
recommendation setup
were radical organisations
who learned
that they could draw
big audiences
by first luring them in
with milder
more acceptable
news pages
knowing that they'd
then be recommended
far more extreme pages
which they also owned.
An efficient mass funnel
for speedy radicalisation.
This technique
was exploited successfully
by all kinds of
extremist organisations
including
pre-Covid anti-vaxxer
groups and ISIS.
Highly populist politicians
like Narendra Modi
in India
and Duterte
in the Philippines
had also found
this new variant of Facebook
to be an excellent platform
for amplifying hateful rhetoric
in the lead up
to their elections.
It later became clear
that misinformation campaigns
had been run successfully
in elections in Turkey
and Indonesia too.
Recommendations aside though,
Facebook's targeted
advertising capabilities
also became more prevalent
in political campaigns.
In 2013,
a company called
Aggregate IQ
was set up
to help political campaigns
understand and target
specific voter segments
more effectively
through voter behaviour analysis,
demographic studies
and predictive modelling.
By integrating this data
with Facebook's
advertising systems,
they claimed to be able
to aim political messages
at highly specific
target audiences
much more effectively
than had been
previously possible.
From 2015 to 2016,
during the Brexit
referendum in Britain,
Aggregate IQ
began working with
four different
Vote Leave campaigns
with the official campaign
dedicating 40%
of its budget to them.
The Leave campaign's
most successful ads
spread incorrect assertions
about the cost of EU membership
as well as misleading statements
about immigration
and national sovereignty.
When Britain ultimately
voted to leave the EU,
Aggregate IQ
took as much credit
as they could.
On their website,
they proudly posted a quote
from the Leave campaign's
chief strategist,
Dominic Cummings.
Without a doubt,
the Vote Leave campaign
owes a great deal
of its success
to the work of Aggregate IQ.
We couldn't have done it
without them.
Anyway,
we've finally come to that point
in the video
I know you've all been waiting for,
where we discuss
the 2016 US election,
which produced arguably
the most famous
of Facebook's black eyes.
Things are momentarily
going to get a bit complicated here
because there were actually
four different
controversial factors at play
which are all related,
but also distinctly different.
In many ways,
this is the moment
when all of Facebook's chickens
came home to roost.
The first factor
is that Facebook
had been developing
advertising services
to sell to political campaigns,
which included
dedicated assistance
from Facebook employees.
An obvious move
given the amount of money
invested in advertising
during election years.
These services were offered
to both the Clinton
and Trump campaigns,
but whereas the Clinton campaign
largely turned down the offer,
the Trump campaign
took full advantage of it.
The second factor
was Facebook's evolving
attitude towards news.
You see,
in the years leading up
to the election,
Zuckerberg had been
increasingly insisting
that they were
a completely neutral platform.
This was largely
in response to accusations
from conservative politicians
and news outlets
that Facebook had
a left-leaning bias.
Zuckerberg's chumminess
with Obama,
as well as Sandberg's
involvement with the Clinton campaign,
only helped to fuel
these accusations.
With Trump ramping up
anger about media bias
throughout 2015 and 2016,
this pressure
became more pronounced.
But then,
in May 2016,
a scandal appeared
out of nowhere
in the form of a Gizmodo article
titled
Former Facebook Workers,
We Routinely Suppressed
Conservative News.
In the article,
an anonymous Facebook employee
described his frustrations
working on a content
curation team
who he said
regularly removed news
about conservative politicians
from Facebook's
Trending Topics section.
Given that the 2016 election
was only six months away,
this story generated
a massive outcry
of conservative anger
towards Facebook.
In response,
the Trending Topics team
were let go
and Zuckerberg cracked down
on all kinds of content curation.
Then,
much to the disappointment
of many Facebook employees,
influential conservatives
including Glenn Beck
and Tucker Carlson
were invited
to meet Zuckerberg
at Facebook's campus
so he could assure them
of their neutrality.
After this,
the whole company
was put on notice.
From now on,
automated trending content
was king.
This leads us to the third factor,
which I've already mentioned.
The algorithm changes
fueling the spread
of polarizing content,
which,
in combination with
now completely unmoderated news,
meant that Americans
were far more likely
to see content
that was trashy,
unsubstantiated,
crass,
or simply just untrue.
Hillary Clinton
had a secret child
out of wedlock.
Hillary Clinton
coordinated a domestic
terror attack
on the US,
etc.
And, of course,
this content would then
be copied and pasted
by troll farms,
including our pals,
the Macedonian Teenagers,
which gave the most
polarizing content
second,
third,
fourth,
and fifth lives.
It wasn't all just
alt-right fake news either.
Some fake news aimed
to bolster Hillary's chances.
All in all,
in the three months
leading up to the election,
fake news articles
favoring Trump
were shared
30 million times
on Facebook,
whereas those favoring Clinton
were shared
8 million times.
The number of people
who saw these shared posts
were in the multiple
hundreds of millions.
Later,
it was reported
that over half of
American adults
who saw fake news
said they believed
the stories at the time.
Groups also played a big part
in spreading misinformation too.
A friend of mine,
Kat,
who was studying misinformation
in the lead-up to the election,
joined multiple different
Trump groups
to keep an eye on them.
Soon after,
Facebook started recommending
those same groups
to her family and friends,
pointing out,
helpfully,
that she was in them.
And then there's
the fourth factor,
Russia.
Let's just give a little
backstory here.
In 2011,
protests against Vladimir Putin
were breaking out
all over the Russian Federation,
inspired in large part
by Alexei Navalny,
who used social media
to organise protest events.
This prompted Putin
to launch a counter-offensive
to disrupt the protesters'
ability to organise online
and interfere with
their communications in general.
An investment into
a new type of information
warfare,
where sowing confusion
was the main goal.
In addition to this,
he publicly accused
the then Secretary of State,
Hillary Clinton,
for having run
an influence operation
aimed at fuelling
the protests in Russia.
You know,
Russian voters deserve
a full investigation
of all credible reports
of electoral fraud
and manipulation.
Later on,
it was reported
that a US spy overheard
a Russian military
intelligence officer
stating that they were
going to get Clinton back
for what she had done.
Now,
when we think of the 2016 election
and Russia,
we think of hackers.
And that's certainly
not off the mark.
Traditional hacking
did play a significant role.
In early 2015,
Facebook's threat
intelligence team
discovered that
Russian hackers
were trying to gain access
to the accounts of figures
involved in the upcoming
US election.
Then,
a few months later,
US cybersecurity firms
published a report
claiming that hackers
with ties to Russia
had successfully hacked
the email accounts
of various senior figures
in the Democratic Convention.
Soon after,
a threat detection analyst
at Facebook
named Ned Morin
used the information
in these reports
to uncover
a Facebook page
called DC Leaks,
which he suspected
was linked
to the email hack.
And so,
he began to keep
a close eye
on DC Leaks
by monitoring
their activities,
including reading
their private instant messages.
From this,
he discovered
that the hackers
were contacting
US journalists
on Facebook
and leaking emails
directly to them
in order to embarrass
the Clinton campaign.
At this point,
Trump would then issue
one of the more astonishing
statements of the campaign.
It shows how little respect
they have
for our country
when they would hack
into a major party.
But it would be interesting
to see,
I will tell you this,
Russia,
if you're listening,
I hope you're able
to find
the 30,000 emails
that are missing.
I think you will probably
be rewarded
mightily by our press.
Once Morin
had confirmed
his findings,
he then sent reports
about the problem
up the chain of command
intended to eventually
arrive at the desk
of one of the then-leaders
of the company,
Elliot Schrage,
the VP of Global
Communications
and Public Policy.
Schrage is an interesting
individual.
Most accounts I've read
paint him as a
domineering company man.
If I press a like button
on a brand,
that could pop up
as a sponsored story.
I might not know about it.
I certainly wouldn't
necessarily agree to it.
Well, but when you,
when you press a like button
on a, on a brand,
you're saying,
I like this.
But I'm not saying
I advertise this.
I'm, you're asking
a profound question.
What's advertising?
On the Facebook system,
I'm affirmatively
communicating that
I'm associating myself
with whatever I'm liking.
Schrage will pop up
a few more times
in this video.
However, for now,
it will suffice to say
that the Russian hacking
report intended for him
did not appear to reach
either Zuckerberg or Sandberg.
It has been suggested
numerous times
that Zuckerberg's
strong resistance
to curating political content
acted as a disincentive
for top leaders
to bring this kind
of information to him.
Now, just in case
you didn't know,
I should probably mention
that Trump went on
to win that particular election.
But even before that happened,
stories were already emerging
about Facebook
due to the amount of lies
and polarizing content
people were experiencing
on the platform.
Soon after the election,
Zuckerberg was asked
about the fake news problem.
His answer made headlines
around the world.
You've been getting
a lot of pushback
from people who feel
that you distorted
the way that people
perceived the information
during the course
of the campaign
because you didn't filter
out enough fake stories
that might have been
published simply
to gain traction
and sell advertising.
You know, I've seen
some of the stories
that you're talking about
around this election.
I think the idea
that fake news
on Facebook
influenced the election
in any way,
I think,
is a pretty crazy idea.
Soon after this,
when Zuckerberg finally found out
about the Russian hacking
taking place on Facebook,
he was reported
to have said,
oh, f***,
how did we miss this?
It's not exactly clear
how he didn't know about it
and there's never been
a transparent explanation
as to where exactly
in the chain
the communication broke down.
Moreover,
it was reported
that Facebook dragged
their feet on the idea
of disclosing their findings
to the public
because they knew
how damning the story would be.
Now, when it comes
to Russian hackers
trying to gain access
to high-profile accounts
or trying to disseminate
hacked information,
this is the good old-fashioned stuff.
The kind of hacking
that Facebook were used to
and which,
once identified,
could be prevented.
However,
hacking was only
the tip of the iceberg.
Ned Morin
and his boss,
Alex Stamos,
who'd been trying to sound
the alarm about Russian meddling
for months,
both believed
there was much more going on
than just hacking.
Later,
they would tell journalists
that their fears
were largely dismissed
by Schrage,
who they believed
was taking direction
from Sandberg herself.
Everyone seemed
to want the story
to go away,
but it wouldn't die.
Finally,
an article published
by Time Magazine
in 2017
blew the story wide open,
confirming what Stamos
had been warning about
for months,
that a Russian-backed
campaign of disinformation
had been employed
on Twitter and Facebook
in order to influence
the outcome of the election.
This pressured both
US intelligence agencies
and Facebook
to work together
to uncover the extent
of the interference,
prompting Zuckerberg
and Sandberg
to give the thumbs up
to Morin and Stamos
to finally uncover
what had actually
taken place
on their platform,
and after a few weeks
of searching,
they got their answer.
The main culprits
were a group
known as the
Internet Research Agency,
which was run out
of St. Petersburg.
They weren't using
some hyper-sophisticated
form of hacking,
no,
they were using
the exact same tactics
as the Macedonian teenagers.
Troll farming,
building up numerous pages
with names like
Secured Borders,
all sharing the same content
in order to game
Facebook's engagement algorithms,
spreading highly polarising
content on key voter issues,
which drove people insane
and made them argue,
which Facebook then
misrecognised
as virtuous engagement
and boosted even further.
In order to siphon
as many voters
away from Clinton
as possible,
the pages favoured
not just Trump,
but also Bernie Sanders,
a rather clever tactic
to spread bitterness
among more left-leaning voters.
Worse still,
Russian troll farmers
were using Facebook's
own advertising system
to buy cheap page-like ads,
which,
as the name suggests,
encourages people
to like your page,
and once they do this,
they'll start seeing
more of your content
in their feeds,
an incredibly cheap way
of scaling up
so your disinformation articles
and memes
are much more likely
to go viral organically.
And to cap it all off,
when anything the Russian
troll farmers posted
went viral,
you could be sure
that other troll farmers
would plagiarise it
for ad revenue,
including
our dear old friends,
the Macedonian teenagers.
When this report
was delivered
to Facebook's leadership,
as well as
US intelligence agencies,
they were appalled.
It turned out
the Internet Research Agency
owned 120 different pages
and it posted
80,000 pieces of content,
reaching in total
126 million Americans.
The exact degree
to which the Russian
disinformation campaign
contributed to Trump's election
is unknown,
but when this report
was released to the public,
it set off an unholy
firestorm of bad PR
for the company.
It also prompted
a reasonably new question
in the minds of many people,
a question that has dogged
the company ever since.
Is Facebook a net bad
for the world?
Hearings on social media
manipulation began
in autumn 2017,
which was attended
by representatives
from Twitter,
YouTube and Facebook.
Separately,
Sheryl Sandberg
visited Washington
to discuss steps
Facebook could take
to prevent such incidents
in the future.
Then,
prepare yourselves
for dj vu here,
Elliot Schrage directed
a PR company
called
Definers Public Affairs
to help navigate
all the bad press,
who in turn
began publishing
news articles
to try and spread
the blame.
In particular,
towards Google again.
They also looked
for ways to discredit
George Soros
when he became critical
of Facebook too.
Just keep that in your head
a moment longer.
As for Ned Morin
and his boss Alex Stamos,
they were dispirited
throughout this entire ordeal.
They tried to raise the alarm
but had largely been ignored.
However,
they did have at least
one moment of light.
They discovered
that a coordinated Russian attack
was taking place
to disrupt France's
upcoming election,
but this time,
they were able to tip off
the French authorities
in advance.
This meant
that when the hackers
suddenly released
20,000 emails
from the Emmanuel Macron campaign
just two days
before the election,
security officials
were able to immediately
inform the public
where the hacks had come from,
which led to news outlets
advising voters
not to believe what they read.
This was an example
of where Facebook
could do good
as long as executives
like Schrage,
Zuckerberg,
and Sandberg
allowed them to.
But then,
in 2018,
just when we thought
this issue
was beginning to wind down,
unbelievably,
almost inconceivably,
it actually managed
to get worse.
That's right,
there's actually
a fifth factor.
Let's wind back
for a second
to understand
how it happened.
Remember earlier
I described
numerous scandals
related to the Open Graph
and how it granted
third-party developers
far too much access
to users' personal data?
Well,
in 2014,
an academic
named Alexander Kogan
accessed the Open Graph
to plug into an app
he had developed
called
This Is Your Digital Life.
The app was presented
as a personality quiz
and it collected
personal data
from users
who installed it.
Almost 300,000
Facebook users
participated in this quiz
from which Kogan
was able to extract
not just their data
but also that
of their friends,
ballooning the overall
data set
to almost 90 million people.
Kogan shared this data set
with a company
called
Cambridge Analytica
whose trade
was to create
what they called
psychographic profiles
of voters
in order to target them
with personalized
political messages.
Then we added
a unique extra layer
of data
about personality,
decision-making
and motivation.
This creates
an unparalleled
rich and detailed view
of voters
and the issues
they care about.
We call this
behavioral micro-targeting.
Our techniques
are working
across the globe
from local elections
to the U.S. 2016
presidential race.
Cambridge Analytica's
then-Vice President
Steve Bannon
eventually left his position
to become the chief executive
of Trump's 2016 election campaign.
He then hired
Cambridge Analytica
to come work for the campaign
and produce targeted advertising.
When the New York Times
and Observer in London
broke the story
that Facebook had allowed
users' private data
to be stolen
and that this data
was then utilized
by the Trump campaign
for political advertising,
the world's media
went into a frenzy
and it wasn't long
before politicians
were insisting
that Zuckerberg himself
testify before Congress
about this.
In the UK,
the scandal
had an interesting counterpart
when it was reported
that Cambridge Analytica
had extremely close ties
to Aggregate IQ
who, as I mentioned earlier,
had been utilized
by the Leave campaigns
for their targeted advertising.
According to the former
Cambridge Analytica employees
Brittany Kaiser
and Christopher Wiley,
the connection between
the two companies
was so close
that Cambridge Analytica
considered Aggregate IQ
to be one of their departments
and that they shared
the same tactics and technology.
This kicked off
a separate firestorm
in the UK
about the use of stolen data
to influence
the Brexit referendum.
For Sheryl Sandberg,
this was especially devastating
because, as the person
ultimately in charge
of public relations,
it was the second time
she'd failed to head off
a major scandal
in the space of a year.
And, you know,
we know we need to earn back trust.
However, this seems
slightly unfair.
Sheryl's more notable failure
was how she and Elliot Schrage
failed to notice
the Russian hacking
and disinformation campaign
despite Facebook employees
trying to tell them about it.
The Cambridge Analytica scandal,
however,
was a direct cause
of the Open Graph
which was very much
Mark's baby.
Elliot Schrage
would then go on
to do something
even more stupid.
When Sheryl went
to testify
during Congressional hearings,
the public relations firm
he had hired,
Definer's,
started digging up dirt
on the Senators
questioning Sandberg,
spreading the information
they'd obtained
to reporters
in an attempt
to portray the hearings
as politically motivated
and not based
on genuine public concern.
When this story broke,
it further destroyed
Facebook's credibility,
spelling the end
for Elliot Schrage,
who had by now
demonstrated
such consistently bad judgment
that he felt obliged
to offer his resignation.
This also damaged
Sandberg's reputation
which she would never
fully recover from.
The Cambridge Analytica
scandal produced
unprecedented public
ire towards Facebook.
The Delete Facebook
hashtag began
trending on Twitter
with high-profile
celebrities like Cher
announcing that they
deleted their accounts
with many others
following suit.
Amidst all of this,
Facebook's stock
also plummeted
by 10%.
And this takes us
to the big moment
where Zuckerberg
finally testified
before Congress
for the first time.
In order to prepare
for this,
Zuckerberg held mock hearings
with a team of litigators
who trained him
on his responses
and prepared him
to deal with tactics
such as grandstanding,
aggression,
and left-field questions
intended to throw him
off his talking points.
They also took
other precautions.
For example,
since Mark had a history
of sweating
during interviews,
his staff insisted
that the AC be turned
down low during hearings.
When the day finally arrived,
Mark's testimony
was calm and assured.
However,
most privacy advocates
felt that the questions
addressed to him
were weak.
Worse still,
some lawmakers displayed
a notable lack
of technical literacy.
So how many data categories
do you store?
Does Facebook store
on the categories
that you collect?
Senator,
can you clarify
what you mean
by data categories?
Well, there's some
past reports
that have been out there
that indicate
that Facebook collects
about 96 data categories.
So how many
does Facebook store
out of that?
Do you store any?
Senator,
I'm not actually sure
what that is referring to.
Could somebody call you up
and say,
I want to see
John Kennedy's file?
Absolutely not.
Could you,
if not,
not,
could you,
not would you do it,
could you do it?
In theory.
Do you have the right
to put my data,
a name on my data
and share it with somebody?
I do not believe
we have the right
to do that.
If I'm emailing
within WhatsApp,
does that ever
inform your advertisers?
No,
we don't see
any of the content
in WhatsApp.
It's fully encrypted.
Right,
but is there
some algorithm
that spits out
some information
to your ad platform
and then,
let's say I'm emailing
about Black Panther
within WhatsApp,
do I get a WhatsApp,
do I get a Black Panther
banner ad?
You said back then
that Facebook
would always be free.
Is that still
your objective?
Yes.
Well, if so,
how do you sustain
a business model
in which users
don't pay
for your service?
Senator,
we run ads.
I see.
Consequently,
this shifted
the media's focus
away from Facebook's
wrongdoing
and instead
on how clueless
the senators
appeared to be.
Congress wanted
to teach them
a thing or two,
but it turns out
they didn't know
a thing or one.
As a result
of Zuckerberg's performance,
Facebook's stock
recovered significantly,
increasing his net worth
by around $3 billion
in just 24 hours.
However,
the PR damage
still remains to this day,
with the name
Cambridge Analytica
serving as a byword
for Facebook's
cavalier attitude
to users' data.
Before we move on,
I want to mention
one funny twist
in this last scandal,
which is that
Cambridge Analytica's
supposed secret sauce,
their
psychographic profiling,
was a totally
useless service
and was heavily
criticised by many
experts in polling
data and psychology
due to the lack of
evidence and transparency
provided.
Even Alexander Kogan,
the researcher
who had shared the data
with Cambridge Analytica
in the first place
knew the approach
didn't work.
According to him,
their profiling data
was only right
one percent of the time.
Worse still,
by the time the 2016
election rolled around,
the stolen data
they'd acquired
was already two years old
and nowhere near as
detailed as the real-time
treasure trove
Facebook themselves
had access to.
And given that
Facebook already offered
highly calibrated
targeted advertising
to politicians,
the irony here
is that Trump's campaign
would have been far
better off ditching
Cambridge Analytica
entirely
and simply doubling
down on the much
more powerful targeted
advertising they were
already using Facebook
to produce.
Had they done that,
they would have wasted
a lot less money,
Facebook would have been
spared the scandal
to end all scandals
and users' personal data
would have been even
more effectively utilised
to sway the election.
From this point in the video
onwards,
the topics discussed
are at times
going to get much more dark.
And although I promise
to avoid explicit
or gruesome detail,
I still think it's wise
to issue a trigger warning
here.
In August 2013,
Facebook announced
a new service
called internet.org
which would bring
free internet access
to underserved communities
around the world,
particularly in developing countries.
They would do this
by partnering with
mobile operators
and other technology companies
across Latin America,
Asia and Africa,
providing a stripped-back
version of the internet
that could be served
at low bandwidths.
At the time,
it was reported
that this was the project
Zuckerberg was most excited about.
When people are connected,
we can accomplish
some pretty amazing things.
We can get access
to new jobs
and opportunities
and ideas.
But technology,
it has to serve
the whole of society.
Connectivity can't just be
a privilege for
some of the rich
and powerful.
It needs to be something
that everyone shares.
So, how did it work
in practice?
Well, when you accessed
internet.org,
you were provided
with a few free services.
Most importantly,
a simplified version
of Facebook
and Facebook Messenger,
as well as Wikipedia,
BBC News,
and the forecasting service
AccuWeather.
It also included
a few local services too,
depending on the region.
However,
internet.org
was not a fully free
internet service.
If you wanted to access
any site outside
of these core offerings,
you'd hit a paywall.
To give an example
of how all this
combined together,
let's take Indonesia.
At the time,
the free search engine
they provided in Indonesia
was Ask.com.
But,
if you tried to access
any of the listings
on Ask.com,
you'd be prompted
to pay for data.
Criticism of this idea
appeared almost immediately,
mainly focusing on
how internet.org
broke with the principle
of net neutrality,
which holds that
service providers
and governments
should treat
all internet traffic equally,
thus ensuring
a level playing field.
To give some specifics,
internet service providers
should not grant
higher speeds
to certain services,
which could stifle
fair competition,
by favouring
established players
over startups.
Secondly,
the entire internet
should always be accessible.
Making some of it paid,
again,
tips the scales
in favour of the powerful,
whose services
can be offered for free,
making it harder
for smaller entities
to compete.
Lastly,
internet service providers
should never limit
freedom of expression,
especially where
someone decides to communicate.
Such barriers to access
could be manipulated
by those wielding power
to discriminate
against voices
they don't agree with.
All in all,
many pointed out
that internet.org
was not free access
to the internet,
it was free access
to Facebook net,
a walled garden
where Facebook gets to decide
what people can have access to.
For this reason,
65 NGOs from around the world
published an open letter
criticising internet.org
as a poor attempt
to bring the developing world online.
Apart from the concerns
I've already mentioned,
it included strong opposition
to the idea of a two-tiered internet
that favoured wealthier citizens.
It also outlined
some security concerns
indicating the technology
opened the door
for users' activity
to be monitored
by their governments,
as well as grave concern
that many of its users
would not understand
privacy consent
due to a lack of education
on the topic
as well as high levels
of illiteracy
among the poorer populations.
Some call the effort
digital colonialism
due to the overwhelming priority
given to Western services
over local ones.
Now,
few believed this project
was motivated
purely by philanthropy.
The benefit to Facebook
of being installed
as the core communications platform
for potentially billions
of additional people
was obvious.
It was the growth tactic
to beat all growth tactics.
Just to remind you
one more time
of that Web 2.0 quote,
the race is on
to own certain classes
of core data,
location,
identity,
calendaring of public events,
etc.
Internet.org was set up
so that if anyone wanted
to sell a service
or blog about something
or message a friend,
their only real option
would be to do it on Facebook.
Thus,
Facebook creates an ecosystem
and therefore a dependency
on their service
which would persist
even if a properly open internet
came into existence.
Now,
some watching might feel
Facebook's initiative
is not being treated fairly here
and that these are
acceptable trade-offs
given the potential benefit
to those who would otherwise
have no internet at all.
That's a reasonable position
and I'll address it
in just a few minutes.
But for now,
let's look at some
of the practical problems
faced by internet.org
when it was launched.
The most obvious
was that many of the services
were only available in English.
Another,
more subtle issue
was that to newly online users,
Facebook was largely considered
to be the internet
which,
according to Mong Palatino
who studied the Philippines version,
misleads the public
and makes them think
that these sites and services
are the essential tools.
This would turn out
to be a big problem.
A little later,
Facebook rebranded
internet.org to Free Basics
which they said
better communicated
the service being provided.
Their number one target,
India.
In order to establish
Free Basics in India,
Mark engaged in a PR campaign
visiting local villages
and cities
and meeting with
various politicians
including the Prime Minister
Narendra Modi.
Here in India,
243 million people
are connected to the internet
and more than 100 million
are on Facebook already.
But, you know,
there's still more than
a billion people
in India
who can't connect
and don't have access
to the same opportunities
as everyone else.
However,
enthusiasts began to sour
on the idea
when they finally saw
what Free Basics
really looked like.
There were no other
social media apps
other than Facebook,
Bing rather than Google Search,
and access to only 36 sites
for free,
all of which were chosen
by Facebook.
This prompted fierce resistance
from privacy
and net neutrality advocates
around the country.
What Zuckerberg means
by internet for all
is essentially
Facebook for all
along with a few
non-profit services
thrown in to give it
the appearance of philanthropy.
Soon after,
net neutrality
became a national issue
with fierce protests
coming from around the country.
Ultimately,
despite significant lobbying
from Facebook,
the Telecom Regulatory Authority
of India sided
with the net neutrality advocates
rejecting Free Basics outright.
At the time,
this was seen
as a massive blow
for Facebook.
But just to make that blow
a little bit worse,
one of Facebook's board members,
Marc Andreessen,
decided to get really angry
about it on Twitter.
Anti-colonialism has been
economically catastrophic
for the Indian people
for decades.
Why stop now?
Very good.
I'm sure Zuckerberg and Sandberg
adored that particular contribution.
Especially how it likens
Facebook to colonialism.
Nice one.
As you can imagine,
this was reported everywhere.
He later apologized for it,
but who cares?
In 2017,
a damning report
from Global Voices
summarized Free Basics
as not being particularly useful.
They observed
that of all the versions
they tested,
none met the linguistic needs
of their users
and that it was sorely lacking
in sites for local services
or independent news sources.
But behind the scenes,
unbeknownst to most people,
worse things were happening.
In particular,
Facebook seemed to be
under-investing
in local moderation teams.
According to one former employee,
I can't recall anyone
at the company
directly questioning
Mark or Cheryl
about whether there were
safeguards in place
or raising something
that would qualify
as a concern or warning
for how Facebook would integrate
into non-American cultures.
This would eventually
open the door
for all kinds of problematic
content to be shared
with too few people
working at Facebook
knowing about it.
And all of this together,
the relentless drive
for growth and speed
with insufficient attention
paid to safeguarding
the site to protect its users,
set the stage
for a truly horrific event.
Facebook's relationship
with Myanmar began in 2010.
When it was launched,
it allowed users
to access its platforms
without incurring data charges,
which helped it rapidly
rise in popularity.
In 2013,
the government in Myanmar
under Thayn Sain
introduced some moderate reforms
to the country,
including allowing mobile operators
in for the first time,
which in turn prompted
a flood of cheap mobile phones
into the market.
This produced the perfect conditions
for free basics,
which was eventually introduced
in 2016.
To many people in Myanmar,
Facebook quickly became
the most trusted source of news,
so powerful
that a print magazine
was even created
for those not yet online
so they could read about
various popular Facebook posts
each week.
However,
beyond the technological changes
taking place,
ethnic tensions in Myanmar
had been escalating
over the same period,
with numerous leaders
trying to turn the country's
Buddhist majority
against its Muslim population,
particularly targeting
the Rohingya,
an ethnic minority
from the northern Rakhine state.
This involved spreading stories
about the Rohingya's
supposed misdeeds.
There were warnings
about new attacks
which the Rohingya were planning,
but which never happened.
All kinds of fake science stories
were spread around too.
For example,
a claim that Rohingya DNA
was distinctly different
from that of other human beings.
And this was coupled
with one of the most worrying
telltale signs
that precipitates
likening the Rohingya
to vermin.
And in addition to all of this,
a new problem.
All of this stuff
was spreading like wildfire
on Facebook.
Now,
there were a few reasons
why Facebook was so powerful
at spreading hate messages
in Myanmar.
The first
is that in comparison
to the heavily censored
state media,
Facebook was seen
by the population
as a source of legitimate news.
News that came from
the real people on the ground.
Another reason
was that a sizable portion
of the population,
having only just come online,
were not yet experienced enough
to recognize the tactics
and language
of online disinformation.
The gasoline,
as you can imagine,
was Facebook's algorithms
which unwittingly promoted
hate speech
and disinformation.
And lastly,
a crucial oversight
that allowed for the
unmitigated disaster
that was about to unfold,
a serious lack
of content moderation.
In fact,
as early as 2014,
Matt Schisler,
a doctoral student
at the University of Michigan,
had been trying to alert Facebook
to the situation in Myanmar,
eventually being promised
a direct line
to one of their working groups.
However,
this group would turn out
to not be particularly useful,
as we'll see.
There were numerous acts
of violence and outrages
from 2014 to 2017,
including a riot
triggered by a viral Facebook post
that falsely claimed
two Muslim men
were to blame
for the f*** of a Buddhist girl
in the city of Mandalay.
Many Rohingya were displaced
during the violence
that took place
as a consequence of that lie.
Later,
Schisler noticed
a dangerous post
beginning to spread
about an elderly man
who'd taken pity
on the displaced Rohingya
and who had been photographed
delivering rice to them.
The posts accused this man
of being a traitor,
prompting calls
for him to be punished.
Schisler later recalled
that when he reported this
to the Facebook working group,
it took weeks
to get a response.
And when that response came,
it was
that there was nothing
they could do
unless the man himself
reported the comments personally,
which was impossible
because he didn't have
a Facebook account
and Schisler had no way
to contact him.
In response to accusations
that they weren't doing enough,
Facebook developed sticker packs
with statements like
Think Before You Share,
which users could apply
to posts and comments.
Theoretically,
these would help Facebook moderators
spot extremely harmful content.
However,
it was reported
that these stickers
had an unintended side effect,
which was that the algorithm
counted them
as a sign of engagement.
So, when applied,
they actually amplified
hateful messages even further.
Around this time,
Schisler would discover
that Facebook had only
one moderator
working out of the Dublin office.
One mod
who only spoke Burmese
in a country of around
54 million people
and over 100 languages,
not to mention dialects.
By 2015,
the number of moderators
was increased to only four.
Again,
all Burmese speakers.
Facebook's automatic detection systems
regularly missed
extreme hate speech
due to problems
recognising Burmese script.
One such post,
translated to English,
was the nonsense statement,
I shouldn't have a rainbow
in Myanmar.
However,
the original post
in Burmese script
was nothing like that.
All the Muslims
that you see in Myanmar,
none of them
should be left alone.
By 2017,
with 15 million people
now on the platform,
the Myanmar security services
began a massive
social media campaign
to convince the public
that it was time
to wipe the Rohingya
from the map.
When it began,
it was reported
that Facebook
had almost no idea
this was happening.
The leader of Myanmar's military
at the time posted,
we openly declare
that absolutely
our country
has no Rohingya race.
What followed
was a systematic
campaign to
and burn the villages
of the Rohingya.
Official reports state
that around 24,000 Rohingya
were during the campaign,
including children,
with another 700,000
fleeing to dangerous
and dishevelled refugee camps
in Bangladesh.
A UN investigation
after the fact
announced that it believed
Facebook played a leading role
in the amplification
of hate speech.
A report from
Amnesty International
stated,
"...our investigations
have made it clear
that Facebook's
dangerous algorithms
which are hardwired
to drive engagement
and corporate profits
at all cost,
actively fanned
the flames of hate
and contributed
to mass violence."
As Mohamed Chawif,
a Rohingya activist,
put it,
"...the Rohingya
just dream of living
in the same way
as other people
in this world.
But you, Facebook,
you destroyed our dream."
Since then,
Facebook has been met
with similar criticism
for amplifying hate speech
in other countries,
most notably in Ethiopia.
To give one example,
according to an investigation
by Vice in 2020,
it was a central source
of disinformation
that led to ethnic violence
in the region of Orumia,
resulting in hundreds of deaths.
Warnings about tensions
in Ethiopia persist even today,
a situation exacerbated
due to the complexity
of the culture,
including around 80
different languages.
In 2021,
two posts appeared on Facebook
condemning a man named
Mehreg Amare,
a 60-year-old Tigrayan professor
living in northern Ethiopia.
The posts wrongfully accused him
of helping a paramilitary group
commit atrocities
during an armed conflict.
These posts also included
pictures of Amare's home,
with multiple people
commenting on the post
suggesting that he be
cleaned.
Trusted partners in Ethiopia
tasked with notifying Facebook
about things like this
stated that Facebook
refused to take the posts down
because it didn't want
to restrict free speech.
Soon after,
Amare was murdered
outside his home.
The posts that caused this
remained up on Facebook
for a further year,
and his family are now
suing Facebook
for $2 billion.
The case my clients
have made is that
not only do Facebook
allow such content
to be on the platform,
they prioritize it
and they make money
from such content.
I hold Facebook
directly responsible
for our father's
or my father's murder.
Since then,
the trusted partners
I just mentioned
have ended their
relationship with Facebook,
with other multiple
Ethiopian observers
reporting that they feel
there's just no point
bringing concerns like these
to the company
because they don't do anything.
Facebook actually has
a process called
Break the Glass,
which they use
in highly dangerous situations,
allowing them to take
significant measures
to prevent the spread
of inflammatory information.
However,
in Ethiopia,
according to Amnesty International,
there is evidence
suggesting that Zuckerberg
personally intervened
to stop mitigation measures
from being applied
because the measures
may have interfered
with the
meaningful social interaction metric.
There are many more
stories like this,
but for the sake of time,
I'll move on.
Facebook has since announced
a significant investment
in local moderators,
but will also need
to establish other safeguards
to spot dangerous trends
before they get out of hand.
At the time of writing,
the situation in Ethiopia
is still highly uncertain,
with news reports
about Facebook's presence
in the country
popping up regularly.
This is one
to keep an eye on.
The original critics
of Free Basics
who argued that it broke
the principle of net neutrality
could hardly have predicted
how bad the situation
was going to get,
where Facebook became
the sole source of news,
crowding out local sources.
This alone is validation
of their criticism,
but I want to add
one more thing to it.
One thing to bear in mind
about the idea of a company
who can become so powerful
that they dominate
conversation in general.
Facebook has made
innumerable terrible mistakes
throughout their history,
as we've seen
and as we're going to see.
And every single time
they've been exposed,
it was published
via an external source
outside of Facebook.
A privacy advocate report,
a tech blog,
a whistleblower publishing
on a different platform,
an open letter,
or most commonly,
a news report by
The Wall Street Journal,
The New York Times,
The Guardian,
BuzzFeed, etc.
A service like Free Basics,
if it can get enough traction
in a given country,
automatically disadvantages
external criticism of Facebook
by hiding it behind a paywall
and crowding it out
with their own free content.
If Facebook became
your sole source of information,
you'd never hear
any criticism of Facebook.
A situation so unacceptable,
I think Free Basics
should be rejected
wherever it's met.
OK, so to do this section justice,
we need to look back
over one last aspect
of the platform
I've not yet discussed.
Remember Tim Urban's blog,
which I mentioned earlier,
about the different kinds of posts
he felt were self-serving
and therefore insufferable?
Well, in that blog,
he proposed
five negative motivations
that lead people to post
what he considered to be
insufferable stuff.
These motivations were
image crafting,
trying to make others feel jealous,
craving attention
and narcissism.
But it's the fifth motivation
that slightly rubs me up
the wrong way.
Although I have to say,
this was written in 2013,
the article is obviously
tongue-in-cheek
and I don't think Tim
was intentionally being spiteful here.
The fifth motivation was
loneliness.
Seeing a lonely person
acting lonely on Facebook
makes me and everyone else sad.
So the person is essentially
spreading their sadness.
And that's a shitty thing to do.
This one caught my attention
because, well,
it's a bit mean.
Oh, lighten up, will you?
Your sadness is really
killing my buzz.
Pretty surefire way
to make a lonely person
feel more lonely.
Now, as I mentioned earlier
in this video,
sometimes,
when you see content
that's intentionally
jealousy-inducing,
examples of how great
other people's lives
appear to be,
it can have a destructive effect
on your sense of self-worth.
My wife,
who works as a psychotherapist,
have been noticing,
along with her colleagues,
an uptick in how social media
played a role
in people's negative sense
of themselves
and their accomplishments
as far back as 2010.
But at the time,
there were obvious positives too.
In 2007,
an influential study
investigated the potential
Facebook had
for developing social capital
among college students,
pointing out that
it could increase
one's network of peers,
where online relationships
eventually turn into
face-to-face relationships.
And this aspect of Facebook
has not gone away.
In May of this year,
I went to a reunion gig
for the band Cardiacs,
which took place in London.
After the show
and we were all hanging around
outside the venue,
I learned something
I thought was really sweet.
A few young fans
who couldn't afford
to stay overnight in London
have been offered beds
by some of the local organisers
who just wanted to help others
have their first
Cardiacs experience.
All of this was organised
on Facebook.
Apart from that,
ongoing research
leading right up
to the present day
suggests that
some neurodivergent people
can benefit from Facebook,
which,
due to its structured
and predictable
communication system,
helps to facilitate
positive social interactions,
in part because it removes
some of the complex social cues
involved in talking
to someone face-to-face.
However,
going back to the early years again,
the positive stories
were beginning to get outnumbered
by the negative ones.
For example,
how Facebook was being used
as a tool for bullying,
as well as other
concerning developments.
One well-known study
from 2013
found that heavy use of Facebook
was strongly correlating
with a decline in users'
sense of well-being.
Later,
another widely shared study
found a correlation
between time spent
on social media
and feelings of social isolation.
Both studies acknowledged
the benefits I just mentioned,
but also agreed
that something unhealthy
was going on
and that it seemed
to be getting worse.
When trying to understand
why this was happening,
I found it interesting
to read even earlier research,
back when we were all
trying to figure out
what social media was for
and how it could change people.
And I think it's important
to look at this for a second,
go back to first principles,
as it were.
To channel the sociologist,
Maris Rosenberg,
we all have something
called a self-concept,
which is our understanding
about how we appear
to those around us.
One of the core aspects
of our self-concept
is our identities,
which we construct
through a kind of
validation process,
a social dialectic.
It starts with
an identity announcement,
where we put the concept
of our identity
out into the world
by talking to someone,
wearing certain clothes,
attending a certain type
of social gathering,
etc.
Then, in return,
identity placement occurs,
where those around us
accept or reject
the identities
we're trying to project,
indicated through their actions,
expressions, or words.
Now, when we're talking
to someone face-to-face
in the real world,
we're aware that
the identities we're projecting
will need to be,
to some extent,
consistent with
how we look,
how we speak,
how confident we are,
as well as a ton
of other traits.
There's a performance
and appearance burden
needed to pull it off.
However, identities
we construct online
are not restricted
in this way.
We can withhold
unwanted information
about ourselves.
We can hide things
about our appearance
that we don't like
and we can role-play.
On Facebook,
which is all about
projecting our identities,
we can curate
exactly what others see,
allowing us an opportunity
to validate
a hoped-for possible self.
An ideal self.
And this is why
the like button
when it was introduced
in 2009
was so consequential.
Let's say you post
something that's
very important to you
and three people
comment on it positively.
Then, a person you know
posts something else
and it gets
five positive comments.
No biggie.
But if we add likes,
you got four
and they got 14.
Now it seems a little bit
like your identity announcement
was a little less validated
by your peers
than this other person.
But then,
Facebook decides,
that post with 14 likes
is rather popular.
Let's show it to more people.
Which means that
when you check back later,
you see the tally has changed.
You still have four,
but this other person
now has 59.
You've been obliterated.
A 2010 study
found that users
with low self-esteem
or narcissistic traits
tended to be more active
on Facebook,
tended to post more
self-promoting content
on Facebook
and were much more likely
to manipulate their photos
before posting them to Facebook.
Other studies ran experiments
to understand the nature
of social comparison
on Facebook.
In one experiment,
female participants
browsed the platform,
noting the attractiveness
of others
and reported feeling bad
about their own appearance
by comparison.
Another study noted
that when participants
were depressed,
they were more likely
to compare themselves
negatively against others,
leading to feelings of envy.
All of this seemed
to point to a big problem.
While Facebook seemed
to be building a refuge
for people with
mental health problems,
they also seemed
to be creating mechanics
that could make
those problems worse.
Then there's the second
major issue that was
beginning to cause concern.
Children.
In 2011,
Consumer Reports
released an estimate
that Facebook had
7.5 million users
under the age of 13,
a finding that alarmed
advocates of child protection.
In 2012,
Bill Price,
the chairman of
Common Sense Media,
a child advocacy group,
visited Facebook's
headquarters to meet
with Sheryl Sandberg
and Elliot Schrage.
Price's aim
was to try and partner
with Facebook
to help strengthen
protections for children.
However,
Sandberg and Schrage
used the meeting
to express anger
over a book
that Common Sense Media
had just published,
titled
Talking Back to Facebook,
which gave advice
to parents
on how to protect
their children
from the worst dangers
of social media.
Price later recalled
that when he suggested
tools aimed at improving
the safety
and mental health
of teenagers,
Schrage dismissed
him out of hand
while also making
the threat
that Facebook
could sever business
relationships
with any tech investors
on Common Sense Media's board.
Price later said,
I left the meeting
feeling pretty depressed.
It's sort of a typical story
of where ultimate power
corrupts people's perspectives.
And then,
in that same year,
Facebook acquired Instagram.
The acquisition of Instagram
was a big deal,
causing concern
among regulators
who worried about
the consolidation
of social media power
under one company.
But it was ultimately
approved by the FTC
a few months later.
The final price tag,
$1 billion,
raised quite a few eyebrows,
considering that Instagram
earned no revenue
and had only 13 employees.
With the benefit
of hindsight,
of course,
we can see that it was
an incredibly strategically
valuable asset,
given its emphasis
on imagery rather than text,
its rising popularity
among teens,
and its dominance
on mobile devices.
In its early years,
Instagram was
reasonably uncontroversial,
with Facebook generating
most of the bad headlines.
For example,
in 2017,
a 12-year-old girl
live-streamed herself
ending her own life
on Facebook.
A journalist for
The Guardian wrote
at the time
that when she first
saw this video,
she flagged it as
inappropriate content,
only to receive
the following response.
We've reviewed
the share you reported
for showing someone
injuring themselves
and found that it
doesn't violate
our community standards.
This video would stay up
for a further two weeks
before they finally bowed
to the pressure to remove it.
And although this isn't
related to children
or mental health,
I have to also mention here
the incident in 2019
where a white nationalist
in New Zealand
used Facebook
to live-stream his attack
on a mosque,
one of the worst
and most savage
mass shootings in history.
Now, as for Instagram,
its grace period
began to significantly
erode around 2017.
A survey conducted
at this time
found that over
one in five teenagers
aged between 12 and 20
experienced bullying
on Instagram.
Also in 2017,
a UK report
on the rise of anxiety
and depression
among teenagers
caused a stir
when the research
indicated that Instagram
was actually the most
negative of all
the social media platforms.
What was going on?
Well, again in 2017,
an event occurred
that provided
a very brutal answer.
Something that's shaped
how the platform
has been viewed
in the UK
up to the present day.
It began when a 14-year-old girl
named Molly Russell
who was very active
on Instagram
took her own life.
In the inquest
that followed,
her phone was analysed
revealing that
over the duration
of six months,
she had seen
over 2,000 pieces
of content related
to self-harm
or depression
on the platform.
Official reports
noted that some
of this content
was recommended to Molly
based on her activity
and not specifically
requested by her.
The psychologist
who had reviewed
the content Molly
had been looking at
reported not being able
to sleep for a few days
afterwards.
And when the coroner's
report finally arrived,
it contained something
that had never been
seen before.
A world first.
The inclusion of social media
in the official cause
of death.
Molly Rose Russell
died from an act
of self-harm
whilst suffering
from depression
and the negative effects
of online content.
Media reporting
on this event
heavily focused
on the tendency
of Instagram's
algorithms
to notice
when you viewed
dangerous content
and then recommend
even more of it
back to you.
Various studies emerged
that appeared
to back this up.
According to one,
we found that
among those
who were exposed
to self-harm content
on Instagram,
only 20.1%
indicated to have
intentionally searched
for this content.
Indeed,
the majority of users
seemed to make contact
with Instagram's
self-harm content
unintentionally.
Other research found
that hashtags
concerning self-injury
rose from 63,000
to over 110,000
throughout 2018.
Then, in 2019,
in response to the
Molly Russell case
in particular,
Instagram announced
that it would no longer
allow explicit images
of self-harm
on their platform.
However, at the same time,
a related form of content
was exploding on Instagram.
The glorification
and promotion
of what I'll just call
EDs,
which was being spread
through a variety
of popular hashtags
I won't name here.
Apart from highly disturbing
and bleak imagery
related to EDs,
participants in this
kind of content
would often publicly
set themselves
incredibly dangerous
target weights,
as the phenomenon
grew bigger,
ED coaches
began to emerge
ostensibly to help
others hit these targets,
encouraging them
to join private groups
where they could get
into even more
horrendous detail.
After heavy media
reporting on this topic
for years,
Instagram finally
began to crack down
on it.
But it wasn't
plain sailing.
Even by 2021,
journalists were still
able to demonstrate
by creating new accounts
and learning some
of the new hashtags
that it was still
relatively simple
to access this type
of content,
pointing out that
when you did get access,
Instagram's recommendations
would then kick in
to promote more
of it back to you.
Around this time,
another scandal occurred
when it was reported
that Instagram
was recommending terms
like appetite suppressants
and fasting
to those who had
searched for ED content
in the past.
Then Instagram released
augmented reality filters
that could be applied
to your selfies,
which in one case
showed you how you'd look
if you had plastic surgery.
Others applied lip filler
or other kinds of
beautification effects
to users' faces.
These caused a lot
of backlash,
with some pointing out
that they promoted
unrealistic body expectations
and were more likely
to make teenagers
feel insecure
about their appearance.
Internally,
Instagram began debating
whether or not
to ban these filters.
We now know
that this was objected to
by Andrew Bosworth,
the same guy who wrote
the ugly memo
we looked at earlier.
Remember that?
The one that argued
that breaching users'
private data
was actually an act of virtue?
Well, in this case,
he argued that by banning
the filters,
users would simply
move to other apps
which aren't as likely
to be as restrained.
Or, to put it another way,
leave it in,
we gotta grow.
Mercifully,
his opinion was ignored
and Instagram began banning
some of the most
controversial filters
in 2019.
At the heart of all of this
lies the problem
of negative social comparison,
something compounded
by Instagram's tendency
to promote fashion
and beauty content
as well as celebrity lifestyles.
Instagram's internal
research teams
were also digging
into the causes
of negative social comparison
at this time, too.
One document,
published in 2020,
which was later leaked,
highlighted a few
troubling areas.
For example,
it found that
one in ten people
felt negative social
comparison on Instagram
often or always,
with teenage women
being the most vulnerable group.
It also found problems
with the high degree
of celebrity content
that was being promoted
on people's feeds.
However,
it depended on the celebrity,
with Ariana Grande,
Kylie Jenner,
Katy Perry
and Jennifer Aniston
being among the names
that tended to cause
the strongest negative comparisons.
They also found
that like counts
significantly contributed
to negative comparisons,
especially when it came
to people in your social circle.
For example,
a celebrity's post
needed to achieve
10,000 likes
before it could cause
a negative social comparison,
whereas for a peer,
it only took 10 likes.
Now, before we move on,
I need to mention
one crucial thing
about a lot of this research.
Even though
there is an enormous number
of damning studies
on how social media
is negatively affecting
mental health,
so much so that
one might consider
this topic
an open and shut case,
in the last three years,
a few new studies
have emerged
that call a lot
of this previous research
into question.
One of the most respected
reports from the Oxford
Internet Institute
criticised previous research
for drawing conclusions
from unreliable methods,
like, for example,
questionnaires,
where respondents
can be influenced
depending on how
the questions are asked.
Another problem
they pointed out
was the lack
of longitudinal research
that tracks the impact
of social media
over years
or even decades.
One reason this is important
is that social media
changes so rapidly,
research from even
five years ago
might now be
hopelessly out of date.
We have no way of knowing.
Longitudinal research
would help to fix this,
letting us know
whether mental health issues
were persisting
over a long period of time.
As the lead researcher
on one paper,
Candice Odgers,
put it,
there doesn't seem
to be an evidence base
that would explain
the levels of panic
and consternation
around these issues.
Now, on the other hand,
there's also a lot of research
over the last three years
that still finds
significant negative correlations,
although all of them
do agree
that the total body
of research
is not strong enough.
Problem is,
we've mountains of correlation,
enough to make us
very concerned,
but not yet proof
of causation.
Now, as you can imagine,
Metta loves this research,
especially the one
by the Oxford Internet Institute
and they reference it constantly
with statements like
there is zero proof
our platforms do any harm.
However,
what they leave out
is that this research
has not been disproved either.
Moreover,
the lead researcher
of the study they really like,
Professor Andrew Plooski,
drafted an open letter
to Metta
signed by a large coalition
of other scholars
pleading with them
to share their data
so this issue
could be conclusively resolved.
And if Metta won't do it,
he called on legislators
to force their hand,
hardly a ringing endorsement.
At the time of writing,
Metta has not complied
with this request.
And to hammer this point home,
even the US Surgeon General,
Vivek Murthy,
got involved this year,
recently issuing
a public warning for parents
that included the sentence,
social media
has not been proved safe.
But if we set aside
the issues of mental health,
there are also a lot
of factual reports
of issues that just
can't be ignored,
like reports of a massive rise
in child grooming abuses
on Instagram during COVID,
or the ED or self-harm problems
I've already described.
And now,
imagine people's reaction
when in 2021,
BuzzFeed revealed leaked plans
for something called
Instagram Kids,
a version of Instagram
for children under the age of 13.
Media coverage was,
of course,
blistering.
Now,
we'll return to Instagram Kids
in a moment,
because there was
a more pressing problem.
Children were already
using Instagram,
and serious questions
were emerging
about the number
of underage accounts
that actually existed
on the platform.
One embarrassing revelation
occurred live
when Adam Masseri,
the head of Instagram,
was interviewing Jojo Siwa.
Jojo, are you there?
Can you hear me?
We're good.
We're here.
We're alive.
Now,
the offending part
of this interview
has almost been
completely scrubbed
from the internet,
but I was able
to eventually find
a part of it.
The moment when Jojo
blurted out an admission
that she'd been on the platform
since she was a child.
I know you're not supposed
to have Instagram until you're 13.
I did.
I had an account.
Many, many five-year-olds.
I don't want to hear that.
You didn't hear that.
Nobody else heard that.
And with all this groundwork
out of the way,
we finally reached the point
in this chapter
where we can discuss
the next worldwide scandal,
arguably the most damning
scandal of all.
To understand how
it all unfolded,
we need to momentarily
set Instagram to one side
and return to Facebook.
In 2019,
the data scientist
Frances Haugen
joined Facebook's
Civic Integrity Team,
the team whose job it was
to make the platform safe.
However,
it didn't take long
for Haugen to become disillusioned
due to what she described
as hostility
from upper management
towards recommendations
from her team.
One example,
she'd later recount,
was that instead of removing
97% of hate speech
as Facebook publicly claimed,
their system was instead
calibrated
to be highly permissive,
removing only around
5% of hate speech
in actuality.
She was also appalled
by the dysfunction
of something called
CrossCheck,
a manual review process
for potentially harmful content
posted by high-profile figures
such as politicians
or celebrities.
Haugen would later write
that CrossCheck
was seriously understaffed,
meaning that if a high-profile user
posted something appalling,
it would take up to three days
for it to be reviewed.
And since these posts
were allowed to remain public
until they were reviewed,
they could go viral,
reaching around 75%
of the audience
they were ever likely to get.
The actions of the
Brazilian football star Neymar
was a notable illustration
of how unfair the system was.
After a woman had accused him
of r*****,
he responded by posting
her DMs on Facebook,
which included naked pictures
of her.
Even though this was a clear
violation of Facebook's rules,
he was protected by CrossCheck,
which allowed his posts
to be seen by 56 million people,
provoking his fans,
to send a firestorm of abuse
towards his accuser.
Haugen eventually concluded
that for any civic-minded employee,
there were three options.
You could either ignore the truth
about how the platform worked,
you could quit,
or
do your best to solve the problem,
all the while knowing
there isn't enough
institutional investment
or will to actually fix it.
Her team created a detailed policy
for how Facebook should handle
disinformation or hate speech
spread by politicians
all over the world.
This was seen as a
particularly important policy
given that the 2020 US election
was on the horizon.
However,
the policy was rejected
by Zuckerberg,
who complained
that he could write a better proposal
over the weekend.
That proposal
ended up being
Facebook will not touch speech
by any politician
under any circumstances.
This new policy
would then be formally announced
by Elliot Schrage's replacement,
the former Deputy Prime Minister
of Great Britain,
Nick Clegg.
But it is not our role
to intervene
when politicians speak.
And that's why I want to be
really clear with you today.
We do not submit speech
by politicians
to our independent fact-checkers
and we generally allow it
on the platform
even when it would otherwise
breach our normal content rules.
So despite everything
that had happened in 2016,
Facebook would not fact-check
politicians' speech
in the run-up to the 2020 election,
As one Politico journalist put it,
While major news organisations
are strengthening fact-checking
and accountability,
Facebook is saying,
If you are a politician
who wishes to peddle in lies,
distortion,
and not-so-subtle racial appeals,
welcome to our platform.
We will not fact-check you.
You are automatically newsworthy.
You are automatically
exempt from scrutiny.
To give one example
of this in practice,
after violent protests
broke out in response
to the shooting
of George Floyd in 2020,
Donald Trump created a post
on Twitter and Facebook
that ended with the line,
When the looting starts,
the shooting starts.
Twitter took it down,
whereas Zuckerberg
personally made the call
to leave it up,
despite the fact
that it broke Facebook's rules
about inciting violence.
Not long after this,
just as the team
Haugen was a part of
had overseen
the exhausting mission
of keeping Facebook safe
during the 2020 US election,
it was announced
that they were being dissolved,
with their members
being scattered
across the company.
This prompted
the leader of her team,
who she highly respected,
to quit.
Right after this,
Haugen opened up LinkedIn
to start looking around
for other work
and discovered a message
from the journalist,
Jeff Horowitz,
who was wondering
if she'd be willing
to share information with him.
Over time,
as they got to know each other,
Haugen began contemplating
whether she should go public
with the information
she knew about Facebook.
The 2020 election
will be rigged
if mail-in voting
allowed to take place.
How are you handling
the President of the United States
to let him know
and let people
who are using your platform
that this is misinformation,
is it not?
I certainly think
that anyone who's saying
that the election
is going to be fraudulent,
I think that that's problematic
and I think additional context
needs to be added to that.
The final straw for Haugen
was just after
the 2020 election ended
when the Stop the Steal movement
unexpectedly began to explode,
aided by a technique
called super-inviting,
where a small number of users
were able to send out
enormous numbers of invites
to bring people
to Stop the Steal groups.
What's particularly damning
about this
is that according to Haugen,
Facebook had actually developed
really powerful protections
that would have stopped
this from happening.
Protections which I mentioned earlier
called Break the Glass measures.
To illustrate how effective
they were,
look at this data
demonstrating how
Break the Glass measures
obliterated QAnon groups
in the three months
leading up to the election.
So, what happened?
Why did Break the Glass measures
fail to prevent
the Stop the Steal movement?
Well, first, as I mentioned,
they disbanded
the Civic Integrity team
in the beginning of December,
the team who'd been heavily involved
in developing these protections
in the first place.
And second,
because the protections
caused a drop in revenue,
they were simply switched off.
before Christmas,
despite the fact that
Stop the Steal
was growing at this time
and despite the fact
that an enormous number
of Facebook employees
went on vacation over Christmas
which left the company vulnerable.
According to Haugen,
Facebook didn't switch back
on the Break the Glass measures
until 24 hours after
the January 6th riots had occurred.
What would have happened
had we intervened
on December 15th
or December 26th
or even January 1st?
Unquestionably,
the adversarial groups
would not have grown
as fast as they had.
It was then that Haugen
decided to go public
with what she knew.
But before she left the company,
she decided to collect
as much evidence as she could
by trawling through
Facebook's employee forum,
searching through
thousands of internal documents
and sharing them
with Jeff Horowitz.
Ultimately,
this led to a series of articles
published in the
Wall Street Journal,
all of them under
the umbrella title
The Facebook Files.
Now, in the past,
Facebook had always been able
to plausibly claim
that the harms they had caused
had occurred
due to technical mistakes
that were unintended
or social developments
they couldn't have foreseen.
But this mega-leak
was something new,
leading to a simple,
overarching allegation.
Facebook knew about the harms
they were bringing about.
Facebook willfully chose
not to fix the problems
that were causing these harms.
Facebook intentionally
misled the public
about the nature
and severity of those harms,
delivering reassurances
they knew not to be true.
The Facebook Files
was not one scandal,
it was around 12 different scandals.
But the most damning of all,
the one that caused
the most outrage
and demands for justice,
was the article titled
Facebook knows
Instagram is toxic
for teen girls.
The article references
a variety of leaked
internal studies
that suggested
Instagram was aware
of the harms they produced,
how they recommended
dangerous content
leading to depression,
EDs, self-harm
or ending one's own life,
and the kicker,
a revelation that one study
called the Mental Help Deep Dive
was cited in a presentation
given to Mark Zuckerberg himself.
Yet, according to the article,
the company withheld
this information from the public
and failed to act
on their own research.
Now, just as all this
was happening,
Facebook,
the parent company,
rebranded to Meta.
Our mission remains the same,
it's still about
bringing people together.
I'm not going to dwell
on the rebrand here,
except to say that
from this point onwards,
I'm going to be referring
to Meta as the decision-making
entity and not Facebook.
Anyway, the Facebook Files
kicked off the most
unbelievable storm of activity
amongst journalists,
lawmakers and legal experts
around the world,
who began looking back
over Meta's previous
statements about mental health,
many of which
bore very little comparison
to their own leaked studies.
New congressional hearings
were set up,
endless new articles
were published,
and almost all attorney generals
across the US
began legal proceedings.
Everyone else is way skinnier
and way prettier
and also sort of hurt
my own self-confidence
about my own body,
which led to me
practically starving myself
for like a year.
As a consequence,
Meta was then compelled
to share some of their
internal communications too.
And these internal communications
are something else.
From them,
we've learned even more
about Meta's attitude
towards the toxic content problem.
In particular,
a lawsuit by the Tennessee
Attorney General against Meta,
which was published
in its unredacted form
in January 2024,
contains a treasure trove
of internal emails.
This document is incredible.
First, we learned
that on March 8th, 2019,
the Director of Research
sent Sheryl Sandberg
a summary of their findings
about the damage
Instagram was doing,
recommending that Meta
invest to fix these problems
and also fund more research.
On April 8th,
this request was repeated
in an email sent to Sandberg,
Mosseri and Zuckerberg.
Eventually,
a member of Meta's
finance team responded,
stating that the recommended
investments would not be funded.
Adam Mosseri,
clearly demoralised by this,
wrote on the same day,
Unfortunately,
I don't see us funding this
from Instagram anytime soon.
He later wrote,
Wellbeing is the existential
question we face
and we lack a roadmap of work
that demonstrates
we care about wellbeing.
Nick Clegg chimed in too,
saying that Meta
needs to do more
and we're being held back
by a lack of investment
on the product side,
which means that we're not able
to make changes and innovations
at the pace required.
According to the document,
Mark Zuckerberg did not respond
to any of these emails.
Then,
as the Instagram story exploded,
things got crazy dysfunctional.
Zuckerberg,
under pressure to make
a public statement
about all of this,
decided to deflect the problem
in the strangest of ways.
He was in the middle
of a personal rebrand
at the time,
a rebrand intended
to present himself
as a fun-loving guy.
So instead of meeting
the issue head-on,
he instead pushed
the head of communications
to put out a joke statement
that referenced a video clip
of him riding a hydrofoil.
The idea of the joke
was to correct a reference
to his hydrofoil,
which the New York Times
had mistakenly called
an electric surfboard.
The post read,
Look,
it's one thing for journalists
to make false claims
about my work,
but it's crossing a line
to say I'm riding
an electric surfboard
when it's clearly a hydrofoil
and I'm pumping that thing
with my legs.
Nick Clegg's exasperation
about all of this
deserves to be read out in full.
Am I missing something here?
On the day a meta rep
is pulled apart
by US senators
on whether we care enough
about children on our services,
Zuckerberg is going to post
about surfboards?
Maybe I've lost my humour
about this whole thing,
but I really think
this would seem
to any casual observer
to be pretty tone-deaf
given the gravity
of the things we're accused of.
If I was him,
I wouldn't want to be asked,
While your company
was being accused
of aiding and abetting
teenage s***,
why was your only public
pronouncement a post
about surfing?
The Wall Street Journal's
reporting has dramatically
consolidated a wider narrative
that we're bad for kids,
which has been brewing
for some time.
It now makes regulations certain
and in my view,
makes launching Instagram kids
nigh impossible.
I've told Zuckerberg
and Sandberg this already.
Defending the hydrofoil post,
the head of communications
responded,
I'm really eager to just
do whatever he wants
at this point.
My spine has been
surgically removed.
The lawsuit also revealed
internal frustrations
with Zuckerberg,
who, due to his
majority shareholding,
was the only person
in the company
who could make big decisions.
As Adam Masseri put it,
Zuckerberg was
the only non-middle manager
at Meta.
Another executive
put it more bluntly,
referring to Meta as
an absolute dictatorship.
It's surreal.
If you've read
nothing else I've referenced
in this video,
read this lawsuit.
You'll find everything
linked in the description.
In October 2021,
just over a decade
after Mark Zuckerberg
was Times Person of the Year,
he appeared on the cover
once more.
This time,
the headline was
Delete Facebook.
Quite a reversal.
Anyway,
just as the Facebook files
was midway through its run,
Frances Haugen went public,
announcing that she was
the internal whistleblower
that had shared the documents
with the Wall Street Journal.
She then testified
before Congress
about everything
that she'd seen
while working at Facebook.
The documents
I have provided
to Congress
prove that Facebook
has repeatedly
misled the public
about what its own
research reveals
about the safety
of children
and its role
in spreading divisive
and extreme messages.
At the end of 2021,
Adam Masseri
was called to testify too.
By now,
the questions being put
to Masseri
and other Meta representatives
were broadly better,
more specific
and more informed.
In particular,
the chairman,
Richard Blumenthal,
as well as the senators,
Marsha Blackburn
and Amy Klobuchar,
were always sharp
and well-prepared.
However,
these hearings
were a common spectacle
at this point
and it was becoming clear
that an impasse
had been reached.
By now,
Meta's boilerplate defense
was that legislation
was needed
on an industry level,
which is right on paper,
and we'll get back to this.
The position of the lawmakers
was that in the absence
of legislation,
Meta needed to invest
much more
in making their platforms safer
while also being more transparent
with their data and research,
which is also right.
And due to this impasse,
hearings had generally become
a bit of a whipping exercise.
The person in the hot seat
looks penitent for a bit,
they get slammed
for things they've done
and said,
they announce a few new initiatives
that sound nice,
and then everyone goes home.
Inevitably,
this led to criticism
about the ineffectiveness
of lawmakers to legislate
and solve these problems.
And with all these new
leaked documents
now out in the open,
more and more articles
kept emerging
at a constant rate,
alleging all kinds of new damage
Facebook and Instagram
were causing.
It was a negative PR avalanche.
Advertisements for human trafficking
in the Gulf,
facilitating sex slavery
all over the world,
Mexican cartels
using the platform
to hire hitmen
and posting videos
of them murdering people.
Governments using the platform
to crack down
on pro-democracy protesters,
out-of-control images
of videos of abuse
and gore
on news feeds in India.
New leaked documents
indicating Meta
was aware
most of their automated
flagging systems
didn't work in Ethiopia,
agreeing to suppress
anti-government protesters
in Vietnam
in return for favourable treatment
from the Vietnamese government.
And on and on.
And as for Instagram,
the story just kept
getting worse too.
In November 2023,
more communications
were unsealed
indicating that Instagram
knew it had millions
of underage users
but only restricted
a fraction of them.
Within the company,
Meta's actual knowledge
that millions of Instagram
users are under
the age of 13
is an open secret
that is routinely documented
and zealously protected
from disclosure
to the public.
The outcry following
all these revelations
finally seemed to kick
Congress into life.
Throughout 2022
and 2023,
new legislation
with bipartisan support
began to appear,
including
the Kids Online Safety Act.
The purpose of this bill
is to force platforms
to change their design
to eliminate harmful content
or provide an opt-out
for algorithmic
recommendations altogether.
And to ensure this happens,
the bill also proposes
a duty of care
which makes platforms liable
when harm is caused
to minors.
This led to another
big moment when,
in January 2024,
Zuckerberg was again called
to testify before Congress
to answer for Meta's
failures on teen mental health.
The existing body
of scientific work
has not shown
a causal link
between using social media
and young people
having worse mental health outcomes.
However,
this time around,
the stakes were higher.
Not only was the Kids Online Safety Act
now a looming threat,
but the senators themselves
were armed
with much more damning material.
Meanwhile,
social media platforms
generated $11 billion
in revenue in 2022
from advertising
directed at children
and teenagers,
including nearly $2 billion
and ad profits
derived from users
age 12 and under.
Instagram also displayed
the following warning screen
to individuals
who were searching
for child abuse material.
These results
may contain images
of child abuse.
And then you gave users
two choices.
Get resources
or see results anyway.
Mr. Zuckerberg,
what the hell
were you thinking?
The basic science
behind that
is that
when people
are searching
for something
that is problematic,
it's often helpful
to help direct them
towards something
that could be helpful
for getting them
to get help.
I understand
get resources.
In what sane universe
is there a link
for see results anyway?
Kids' safety
is an area
where, quote,
we are investing heavily,
end quote.
We now know
that statement
was untrue.
We know it
from an internal email.
It's an email
written by Nick Clegg.
You know who he is.
He summarized
Facebook's problems.
He said, quote,
we are not on track
to succeed
for our core
well-being topics.
Nick Clegg
was asking you,
pleading for resources,
and you rejected
that request.
Here's a quote
from your own study.
Quote,
we make body image issues
worse for one
in three teen girls.
37% of teenage girls
between 13 and 15
were exposed
to unwanted nudity
in a week
on Instagram.
You knew about it.
Who did you fire?
Senator,
this is why
we're building all
these tools.
Who did you fire?
Senator,
I don't think that that's...
Who did you fire?
I'm not going to answer that.
There's families
of victims here today.
Have you apologized
to the victims?
Would you like
to apologize
for what you've done
to these good people?
You've all gone through.
It's terrible.
No one should have
to go through
the things
that your families
have suffered.
And this is why
we invest so much
and are going to continue
doing industry-leading efforts
to make sure
that no one
has to go through
the types of things
that your families
have had to suffer.
A month later,
in February 2024,
the Kids Online Safety Act
got the backing
of over 60 senators
assuring its passage
through the Senate.
At the time of writing,
it has yet to have
a companion bill
that will be voted on
in the House.
My bet is that
it ultimately won't see
the light of day,
but I might be wrong.
It's possible that
if there's enough
public pressure,
enough representatives
might vote it through.
So to finish off
this section,
let's address the question
of why legislation
is so hard.
Since this is a very
large topic,
I'm just going to touch
on one or two aspects.
The trouble with trying
to legislate to prevent
harms being caused
by search engines
or social media platforms
is that it's probably
impossible to do so
without accepting
negative trade-offs.
This goes all the way
back to the foundations
of the internet.
Now, I'm sorry
for another mini-story here,
but it's impossible
to understand
where we are
with legislation now
if you don't know
about this.
See, while the internet
was still in its infancy
in the early 90s,
there was an online
bulletin board
called Prodigy
who promoted themselves
as a family-friendly
platform and thus
used human moderators
to filter inappropriate
content.
However, a major problem
occurred when one
of Prodigy's users
posted a scathing
defamatory criticism
of the brokerage firm
Stratton Oakmont,
the same company
depicted in
The Wolf of Wall Street.
In response,
Stratton Oakmont
sued Prodigy
for hosting the
defamatory post
and won.
The reason the court
ruled in favour of Stratton
was because they were
applying the same
standards of liability
reserved for
traditional news publications.
In the court's opinion,
since Prodigy
actively moderated
its message boards,
it was acting more
like a publisher
than a simple
conduit of information
and was therefore
liable for the
defamatory post.
This was rightly seen
as a destructive precedent.
How was the internet
going to flourish
if companies could
be bullied in this way?
And how was the internet
ever going to be safe
if internet services
fearing liability
were too afraid to moderate?
The existing legislation
was not in line
with the technology.
That much was clear to many.
To fix this,
representatives Chris Cox
and Ron Wyden
introduced Section 230
as part of the
Communications Decency Act
in 1996.
Section 230
is the legislation
that made the internet
as we know it possible.
In essence,
it protects services
so they can actively
moderate content
posted by their users
without being held
liable for it.
This protection means
meta can't be held liable
even when illegal content
is posted on their platforms,
although they must take it down
once they become aware of it.
This is why those
who report on meta
who want to see them
reined in
tend to focus on stories
where they can demonstrate
intentional inaction.
Now, the Kids Online Safety Act
or COSA for short
doesn't seek to directly amend
Section 230,
but it certainly
butts up against it
in a few key ways.
While Section 230
is a clear legal shield
for platforms
so that they can't be held
liable for content
posted by their users,
COSA makes platforms
liable if harm
is caused to children,
which is very,
very plausibly
going to be done by content.
How can this work?
Well, the smart aspect of COSA
is that it seeks to avoid
unintended censorship
by allowing social media platforms
to choose the ways
they would protect kids.
One proposed approach
to comply with COSA
is to alter the platform
significantly
if the user is underage.
However, we don't know
how this would play out.
An accepted global standard
for digital age verification
does not exist.
Even if it did exist,
children can still trick
the platform
to be recognised
as an adult anyway.
What happens then?
It could be argued
that COSA might force
platforms to overcorrect,
censoring a broader range
of content for everyone,
although I must say
I find that
a little implausible.
Ultimately,
it's up to each individual
to decide for themselves
whether this is
the legislation they want
to help protect children
on social media platforms.
My only aim here
is to frame it
as transparently as I can.
I should also mention
that this is by no means
the only legislation
currently being proposed either.
There are too many
to try and explain
in this video,
so I'll just list them
for those interested
in researching for themselves.
And with that,
we've just about covered
all the important events
that led up to 2024.
All that's left is...
right now.
I'm aware the running time
is like insane,
but at least you know
I wasn't slacking off
over the last year.
And don't worry,
we're not going to limp
to the finish line here.
The stuff I'm going to talk about
in this last chapter
is quite something.
And by the way,
if you've liked what you've seen so far
and want to support my work,
please consider subscribing
to the video streaming service
Nebula,
where you'll find videos of mine
not available on YouTube.
There's a special link
in the description
that benefits my channel
if you use it.
Alternatively,
consider supporting me
on Patreon instead
and get access to
behind-the-scenes content
and other kinds of
special videos
and other stuff.
Support from these places
is how my channel gets funded.
Anyway,
with all that said,
let's finally bring this thing home.
Now,
after digesting
all this material,
this enormous
and convoluted history,
I can't help but wonder
what stage in the evolution
of social media
we're currently living in.
Is it still early days?
Are we still essentially
guinea pigs,
the generations whose
aggregate misfortune
will help to temper
a better and safer
social media in the future?
Or is what we're experiencing
now the early warning signs
of a far worse future existence,
one that needs to be averted?
Well,
to try and help us answer this,
let's look at Meta
right now.
At the time of writing,
Meta as a business
is in better shape,
in no small part
due to the rise of AI.
Because Meta
had a surplus of GPUs
as well as a long-time experience
in the field of machine learning,
they entered the AI race
really quickly.
Since then,
they've released
multiple versions
of their large language model,
LAMA,
which is open source,
in contrast to the closed models
of OpenAI and Google.
Think of LAMA as
Meta's answer to chat GPT.
But the current rosy outlook
is only a recent development.
Just back in 2022,
with the global economy souring,
Meta's stock dropped massively,
due in large part
to scepticism
over Zuckerberg's
metaverse strategy.
To tackle this,
Zuckerberg engaged
in what he called
the year of efficiency,
letting around 22%
of Meta's workforce go
over a very short time frame,
something most companies
of that size
are unable to achieve.
This drastic cost-cutting,
combined with their quick entrance
into the AI race,
incorporating AI features
into Facebook
before Google or Apple
could do the same,
caused Meta's stock to explode.
At the time of writing,
Meta is, once again,
the darling of the stock market,
resulting in a significant improvement
in how business publications
talk about them.
It's like this guy
reorganized an entire company
in three months.
It's unbelievable
what Zuckerberg did.
Regarding the metaverse
strategy itself,
I'm not going to dwell
on this too much
because the technology
requires a few more breakthroughs.
What I will say
is that I always find it
a little bit irritating
when publications refer
to Mark's vision
of a metaverse
because it's not his vision.
Setting aside the decades
of sci-fi novels
about this idea,
from a tech point of view,
it's much more
Microsoft's vision.
In 2015,
when I worked at Microsoft,
all of these ideas,
conferences and virtual spaces,
arranging apps
around your room,
a large virtual cityscape
where people can interact,
etc.,
were being discussed
like ad nauseum.
For a while,
I worked on a team
that built applications
for the HoloLens,
so I was right in the thick
of all this stuff.
Loads of people I knew
ended up in meta,
so when I saw these ideas
recycled by Zuckerberg
in 2021,
it felt a bit like deja vu,
and again,
when Apple recycled it
in 2023,
bringing absolutely nothing
original to the table
themselves.
With that in mind,
I did actually enjoy
Zuckerberg's takedown
of the Vision Pro.
It was cathartic
and also convincing.
I know that,
you know,
some fanboys get upset
whenever anyone dares
to question if Apple's
going to be the leader
in a new category.
And that takes me
to a different point,
Zuckerberg's personal rebrand,
which seems to have
kicked into a new gear.
Now,
for as long as he's been
in the public eye,
he has consistently
been the butt of the joke.
A lot of this began
with the movie
The Social Network,
which portrayed him
as a cold-hearted dork.
And even though
I loved that movie,
there are aspects
of the portrayal
which I have real
big problems with.
Some aspects
are just untrue.
In particular,
the idea that he had
no success with girlfriends
and that his obsession
with social media
was a coke to get over it.
In fact,
he met his now-wife,
Priscilla Chan,
while he was studying
at Harvard.
His personal gaffs
up until this point
are pretty much
never-ending.
Being sweaty
during high-profile interviews,
having a booster seat
during congressional hearings,
looking weird
on a hydrofoil,
having a generally
robotic delivery,
that incredibly creepy-looking
metaverse video.
I think it's time
for my workout.
Anyway,
as part of his
personal rebrand,
Mark has been
an increasingly visible figure
in the world of MMA.
He's done away
with the tightly cropped hairstyle,
allowing his hair
to grow out more naturally.
He's changed the kinds
of clothes he wears
to not look so stiff.
And crucially,
he's generally far more
natural in interviews
these days.
And he does a lot
of interviews,
especially on YouTube.
And his choice
of subject matter
and interviewer
has decidedly changed too.
In the past,
up until around 2019
and 2020,
it appears that
the strategy for communications
was to be seen
to be engaging
in big questions
about tech
and its effects
on society.
You can see this clearly
in his conversation
with Yuval Harari,
who really challenges him.
I know that Facebook
faces a lot of criticism
about kind of
encouraging people
to move
to these extremist groups.
I think also
it's something
that you can solve
if you put enough
energy into that.
What I worry more
is increasing inequality.
Mark gives a decent
account of himself here,
but ultimately,
he's always on the defensive
and regularly deflects
criticisms rather than
directly engaging with them.
And consequently,
the comments under the video
are almost entirely
hostile towards him.
But in the following year,
after the PR disaster
of the Facebook files,
which, for one thing,
forced him to drop
Instagram kids,
there's been a very clear
U-turn in their
communication strategy.
Now, the focus
is almost exclusively
on the topic
where he's strong,
cool tech.
And the choice of interviewers
is pretty clever too.
They are all,
to a person,
toothless.
Tech podcasters,
intellectual lightweights,
or sympathetic bros,
many of whom
see the opportunity
to interview Zuckerberg
as a giant boost
to their credibility.
Like you and I share,
we're tech optimists
and I think
almost all of this tech,
if not all of it,
ends up being a net benefit
for humanity in the long term.
The one thing that unifies
all of these more recent interviews
is that Zuckerberg is rarely,
if ever,
meaningfully challenged.
It's also useful for Meta
that the world has largely
identified a new villain
in Elon Musk
due to his consistently
ridiculous decisions
with respect to Twitter
as well as a variety
of other things.
But I'd be remiss
if I didn't mention
that Mark and Priscilla
pledged to give away
or sell 99%
of their Facebook shares
within their lifetime,
with the funds going
to a new LLC they set up
that would allocate money
to various charitable efforts.
This is by no means
the only charitable work
they've done,
but it's the most eye-catching.
It's almost like
Mark's intention
is to continue doing
what he's doing
for many years to come
and when he's finally gone,
he'll donate a ton of money
by way of apology
for all the damage he's done
and hopefully secure
a positive legacy.
Combined with all the good
business press,
the question is,
will this work?
Will the general public's
perception of Meta
and Zuckerberg
over the coming years
get better
as new generations
transition into adulthood?
Will they be able
to leave behind
the incredible legacy
of mistakes and damage?
More to the point,
will Meta eventually
become a company
people broadly agree
are a force for good?
Well, to answer the first point
about public perception,
never underestimate
Zuckerberg's problem-solving skills.
Long gone are the days
where Sheryl Sandberg
is wheeled out
as the public spokesperson
when the company
gets in trouble.
Sandberg's star fell
during the scandals
I've already covered.
She stepped down
as COO in 2022
and stepped down
from the board
in January 2024.
What's left
is the new comms strategy
I've just outlined.
Focus on the positive text stuff
and deflect everything else.
And I think,
at the time of writing,
that it's working.
Have you heard the term
Zuckassance?
Uh...
Zuckassance!
You haven't heard that?
It's Zuck's renaissance.
As for the question
of whether Meta
is beginning to turn it around,
it's going to be very interesting
to look back on this video
in five years
to see how things
actually play out.
But to my mind,
the answer is,
I'm very pessimistic.
Even though there hasn't been
a mega-scandal
in the last two years,
there's been a lot
of negative developments.
Developments which demonstrate
the same brutal drive
for growth and control
as well as new developments
that could link together
in dangerous new ways.
Starting with the smaller stuff,
let's talk about Instagore,
Instagram's current incarnation,
which at the time of writing
is recommending brutal,
shocking content
of people getting murdered
or mutilated,
animals and children
being severely injured,
fatal car crashes,
electrocution,
people falling off buildings,
the list goes on.
I'd heard people talking
about this new brutal trend
on Instagram,
but not being a regular user myself,
hadn't seen it.
So I turned to my Discord group
and asked whether any of them had,
and I really was not prepared
for what came back.
20 minutes later,
I had to shut off my computer
and try to go to sleep
with just horrible images
banging around inside my skull.
In particular,
things I saw happening
to small children
where I couldn't help
but imagine my own small children
in the same situation.
So why are people
uploading this kind of stuff?
Well,
some things don't change.
For numerous pages
that I've seen,
there's an accompanying lengthy text
promoting cars
or other products.
Many different accounts
of identical promotions
indicating they're owned
by the same people.
I probably wouldn't have mentioned this
if I hadn't experienced it.
The problem here
isn't just that the content
is horrible,
it's the relentlessness of it
that makes it so much worse.
A compound effect
where every 15 seconds
you're watching
another brutal injury,
another death,
another animal getting mistreated,
another child being mangled,
another death,
another injury.
And then Instagram interprets
your stunned,
caught-in-the-headlight's
lack of action as engagement
and just pumps out more of it.
After only two or three minutes of this,
you enter into a kind of
elevated sense of borderline panic.
It feels like your body
has kicked up your adrenaline levels
due to a sense of
imminent danger to your person.
It's horrible.
It affects you.
It's been widely reported
that moderators for meta
are at serious risk
of developing PTSD
and I can see why.
So what about data and privacy?
Well, meta is doing
terribly here too.
Let's take Europe.
In July 2023,
the EU Court of Justice
ruled that meta was in violation
of our data protection law,
GDPR,
by forcing users
to accept targeted advertising
as a condition of using
Facebook or Instagram.
GDPR states,
very clearly,
that users have
a fundamental right
to opt out
of having their data shared
for the purposes of advertising
and meta's actions
really give me the sense
that they've nothing
but contempt for it.
So what did they do
when that ruling
was handed down in July?
Well,
they finally created the option
to allow people to opt out
for a subscription fee
of 251 euro a year,
a tactic now referred to as
pay or okay.
In April of this year,
the European Data Protection Board
ruled that pay or okay
was not okay.
It now seems meta
has run out of options
and will be forced to provide
a proper, free opt-out mechanism soon,
although God knows
what new tricks
they'll cook up next
that EU legislators
won't be expecting.
Meta's relationship
to the European Union
is going to continue
to be very interesting,
especially now that
the Digital Services Act
has come into full force
in 2024.
This legislation requires
online services
to swiftly remove
illegal content
and other harmful
online activities.
They also require
much more transparency
about exactly how
online services moderate.
If successfully enforced,
this would tighten the screws
in such a way
that lack of compliance
would be much easier to prove.
And if it is proved,
the legislation now
gives the courts
the power to fine up
to as much as 6%
of Meta's annual global turnover.
That's not a fine
they could easily brush off.
Now, for my money,
the privacy-related story
to keep an eye on,
a scandal that's emerging
right now,
is how Meta is using
the stuff we post
in their platforms
to train their AI models.
In the US,
where protections
aren't yet in place,
if you'd like to opt out of this,
you are, frankly,
screwed.
They don't need to inform you
that your data's being used
to train their AI
and if you submit an objection,
they're not obliged
to act upon it.
In the EU,
where legislation prevents
them from doing this,
they pushed out a highly
convoluted notification
to its users,
which I think takes the award
for worst collection
of dark patterns
in Meta's history.
And that's a fiercely
competitive pool.
I tweeted about this in May
a few days after they rolled it out
and that tweet
certainly did the rounds.
Let me summarise.
See, I've worked
in growth teams for years
and I know how to design
and streamline an experience
to maximise traffic.
The rule is,
the simpler the experience,
the more people
who'll make it through
to the end.
With every additional step
you add,
large percentages of people
will drop out.
Now Meta did the opposite
of this.
It's hilariously awful.
Okay, so step one,
you get this notification
with the misdirecting
non sequitur.
We're planning new AI
features for you.
Learn how we use
your information.
Notice the absence here
of the terms private,
data or opt-out.
And once you click on that,
step two,
you get shown this notice
where they explain
the legal basis
they're using
for collecting your information.
Legitimate interest.
Let me quote the opinion
of the European Centre
for Digital Rights on this.
Instead of asking users
to consent,
opt-in,
Meta argues that
it has a legitimate interest
that overrides
the fundamental right
to data protection
and privacy
of European users.
Once their data
is in the system,
users seem to have
no option of ever
having it removed,
right to be forgotten.
The shameful bit here
is how hidden
the right to object link is,
placed towards the end
of the second paragraph.
Seems to me
there's a rather larger
and more intuitive space
down here
that they could have
used instead.
Ugly stuff.
Also, notice the line
that says
if your objection
is honoured,
it will be applied
going forwards.
Don't see that too often,
so let's object.
Which leads us to step three,
this big form you need to read.
It's only at this stage,
the stage where you are objecting,
that they inform you
about which of your content
they plan to use
to train their models.
They also state,
amazingly,
that they may choose
to ignore your objection.
So, step four,
you post your objection.
Bear in mind
that needing to fill in a form
is not a necessary friction.
They don't need it,
and I highly doubt
any humans are going to read it.
The one thing I am sure about
is that it will turn
more people away
because they couldn't
be bothered filling it in.
Step five,
once you've confirmed
your objection,
you're then told
you need to check your email
and grab a code they sent you.
I'd love to hear
their justification for this.
Step six,
you open the email they sent,
which contains a code
that's only valid for one hour.
So if you miss it,
you'll need to start again.
Anyway,
now you copy the code.
Step seven,
enter the code
and get a confirmation message.
And then,
comically,
in the space of around
eight seconds,
you receive an email
letting you know
whether or not
your objection
is going to be honoured.
Wow,
those meta folks
sure know how to work efficiently.
I should mention
that when I tweeted this,
some of the people
who responded to my tweet
told me that they got an error
when they objected,
which is just like
classic Facebook.
So let's get into AI.
Now,
when it comes to this subject,
I'm not a deep pessimist.
Yet.
I'm excited about
the potential of the technology,
even though,
like most others,
there are aspects
I find pretty horrible.
Like,
sorry to beat a dead horse here,
but as someone who spent
enormous portions of my youth
learning to draw
and learning different kinds of
styles and techniques
on top of that,
I feel really deeply
for the visual artists of today
who are particularly vulnerable
to the rise of generative AI
in a way I don't think
is true for musicians.
It sucks if you've been
using Instagram
to promote your work for years
and now find yourself
at no recourse
as that same work
is used to train programs
that are literally
in the process of replacing you.
But when discussing
Meta's AI strategy,
I want to avoid
leaning into hyperbolic pessimism
like your garden-variety
video essayist.
The only observation
I can make
with some confidence
is that,
as is always the case
with Meta,
they provide tools for free
which could be potentially
very beneficial
to the outside world
while also locking things down
in such a way
that they guarantee
their own long-term dominance.
So that's how
I'm going to frame this.
On the beneficial side
is Meta's choice
of open-sourcing
a significant number
of trained models,
making them available
to the public.
Apart from their flagship
large-language model,
LAMA,
you also have Detectron,
a computer vision system
that can detect objects
and segment them.
With enough reliability,
imagine how this could be
utilised in, say, healthcare,
tumour detection,
glaucoma detection,
assisted surgery
or any one of a thousand
other possibilities.
And those are just two.
The list goes on.
There are many more
useful models like this
which could serve
as incredibly powerful tools
to smaller businesses
and start-ups.
And it must also be said
that Meta has some form
when it comes to releasing
beneficial technologies
to the public,
like React and React Native
or PyTorch.
Where all this starts
getting hairy
is when you start thinking
about the wider strategy
being employed.
First, let's investigate
how they're open-sourcing
their AI models.
Taking Lama,
if you look at the terms
of the license,
it's clear that this model
is in fact only
partially open-source,
to put it charitably.
First, the data training
the large language model
is proprietary,
as is the code used
to do the training.
This prevents others
from forking the project
to create their own version,
a version that could, say,
be much more open,
so that you could interrogate it
to find out what sources
it used when answering
your questions.
That's not an open-source
approach, not even close.
And this also raises
significant suspicions
about where their training data
is coming from.
If it's been trained
in our conversations
on Facebook and Instagram,
then the data could never
be released because it would
be the biggest privacy breach
in history.
And just to be clear,
I'm not saying they should
definitely release everything
open-source,
but they just can't call it
open-source unless they do that.
But the most eye-opening
part of this license
is definitely the clause
that if you're a company
with over 700 million
monthly active users,
you can't use Lama for free
and must obtain
a commercial license instead.
Hmm, 700 million.
What's that about?
Well, Snapchat hit 750 million
in 2023,
TikTok and WeChat
passed 700 million
a few years before.
And this helps us
to understand the dominance part.
Making something free
to the end-user
and calling it open-source
is great for PR,
but it can also be
a pretty sharp-edged
business strategy too
because it kills competition.
Giving their AI services
away for free
suits Facebook's
advertising model
whilst also undermining
competitors in their weight class
like Google or OpenAI
who charge for access
to their AI services.
Making it free
also reduces the risk
of new tech giants
emerging to compete
in the future,
but it can also
strangle small businesses too
so that they never
get off the ground.
Over the last year,
I've spoken to a few
well-known AI startups
who've expressed
grave concern
that their entire business model
can be completely snuffed out
if Facebook decides
to roll out a similar feature
for free.
So, in summary,
open-sourcing their AI models
is a clear power play
to broaden their domination
as a tech superpower
by disadvantaging
other tech giants
while also making it unlikely
that new businesses
can grow big enough
to become properly competitive too.
And it's our data,
often given without
our knowledge or consent,
that's empowering them
to do this.
It is,
as I hope I've demonstrated
clearly throughout this video,
classic Mark Zuckerberg.
And then there's
the other side
of the AI coin.
And prepare yourselves
for another variation
on a classic theme here,
AI content is completely
out of control
on Meta's platforms.
In 2024,
an alarmingly creepy
new trend is kicked off
with vast amounts
of bizarre AI-generated images
flooding Facebook.
One example is a
double-plus-uncomfortable
series of images
featuring young children
from historically
disadvantaged countries
surrounded by
elaborate or impossible
structures made of food.
In particular,
the theme of
overabundance of food
just feels really ugly
and mean-spirited to me.
There are many others
that are just
overtly racist in theme,
which I'm obviously
not going to show.
Then there's
the endless series
of pictures of Jesus.
Shrimp Jesus,
crab Jesus,
Jesus performing
feats of heroism
or revealing himself
in unexpected places.
There are all kinds
of hyper-real,
cutesy images of babies.
Babies with kittens,
wide-eyed babies
with miniature kittens,
babies with Jesus.
Okay, okay,
this one's actually
kind of funny.
And there's all kinds
of other things.
There are images
of amazing stuff,
people celebrating
their 120th birthday,
amazing scenes
of various kinds.
Damn,
this woman's garter stitch
technique is off the charts.
We are quadruplets.
Today we turn 90.
We are waiting for
congratulations from you.
Uh, how's her arm
doing that?
What's going on over here?
This one is an amazing category.
Families sitting around
a table that contains
the preserved corpse
of someone's mother.
Okay, so I've kept
most of these examples light
and you might be thinking,
this all seems harmless
enough, Tantacruel.
Ah, Tantacruel,
get a grip.
Well, you see,
with this stuff
flooding Facebook,
an enormous number
of these images
are going viral.
In the case of some
of the Jesus imagery,
this is due to
devout Christians
around the world
feeling obliged
to comment on them
and resharing them further.
Problem is,
according to a Stanford
Internet Observatory report
in March 2024,
a significant number
of the pages
that blast out this content
are either spammers
trying to direct people
off-platform
to content farms
or just looking
to straight-up scam
the vulnerable
using the comments section
to obtain information
from them.
And there's evidence,
yet again,
of multiple pages
owned by the same people
acting together
to boost virality.
And of course,
because anyone can blast out
tons of AI-generated images
every day,
the compound effect
is that it feels like
the platform
is being taken over by it.
Once you've seen a few,
you get sucked
into the AI vortex.
This aspect has been reported
on by a few YouTubers already,
many of whom
fall into the category of...
crap,
focusing their attention
on what they see as
the real story here.
How stupid people are
that they believe
this stuff is real.
Thing is,
when it comes to scamming,
I don't think it's a stretch
to suggest
that one of the advantages
of using AI in this way
is that it acts as a filter,
much like the famous
Nigerian Prince scam
that helps to identify
the older,
the more gullible
or the less tech-savvy.
The easy marks.
But there's another
worrying reason
why these AI pages exist,
which is to grow them
to be as large as possible
in order to sell them,
almost always to
unknown third parties
who want those pages
for unknown purposes.
Like,
imagine there was an election
in a majority Christian country
with a population
that has historically
been vulnerable to disinformation,
like Ethiopia.
Well,
if you've managed
to create a significant following
among Ethiopian Christians
on Facebook
through the use of seemingly
harmless AI-generated
Jesus imagery,
there might be some folk
interested in the outcome
of that election
who'd be willing to take
that page off your hands.
The marketplace for buying
and selling accounts
is not hard to find.
I mean,
it's all over Reddit.
Have a Facebook page
with 50k followers to sell.
Please contact me,
just Serious Buyers.
Hi,
is the followers majority
in the USA?
Please PM me the details.
On swapped.co,
at the time of writing,
you can buy an Instagram account
with 1.4 million followers,
90% of whom are women,
25% of whom are in the US
for $5,000.
Many would feel safe
signing up to
Unicorns are Cute
until...
Congrats!
You're now a card-carrying member
of Alpha Battle Brotherhood,
Iron Fist Nation.
And this leads us nicely
to the thing
we've all been waiting for,
the beginning of AI
being deployed
to sway political discourse.
In May 2024,
Meta dismantled a network
of over 500 fake Facebook accounts
and 32 Instagram accounts
linked to the Israeli firm Stoic.
According to Meta,
the accounts presented
as being owned by US citizens,
but were all spouting
AI-generated comments
in various discussion threads,
praising Israel
and criticizing US campus protests
over the war in Gaza.
In this case,
Meta, to their credit,
seems to have been ready
for this kind of
inauthentic behavior.
Now, the thing we've all
been expecting to see
is, of course,
the use of AI-generated
disinformation
to influence the US elections
this year.
But because this approach
is so obvious,
Meta must have been
preparing for it for ages.
Sure enough,
in August 2024,
they announced
the removal of tons
of AI disinformation content
with the majority
of abuses coming from...
Oh, I wonder
which country it might be.
So if we're destined
to see yet another
major disaster
in this upcoming election,
I'd be surprised
if AI plays a big part in it.
In fact,
I'm now going to boldly suggest
that because Facebook
will be so focused
on not getting yet another
black eye this time around,
the social media story,
if there is one,
will happen over on Twitter.
But that doesn't mean
Facebook's doing great.
Apart from AI,
we can also rely
on garden-variety disinformation,
the type spread by humans.
Take, for example,
a story originated in Russia
about Olena Zelenska,
the wife of the Ukrainian president,
Vladimir Zelensky,
which falsely reported
her lavish spending
during a shopping spree,
a lifestyle they pointed out
made possible
due to US war aid.
Another video did the rounds
that claimed falsely
to show Hunter Biden
assaulting a woman in a hotel.
In August 2024,
two British men were jailed
for spreading false information
about immigrants
and asylum seekers,
which contributed
to violent anti-immigration riots
that lasted a week in the UK.
The riots themselves
were set off by the lie
spread around Facebook and Twitter
that a man who'd
three children in a violent spree
was himself an asylum seeker.
And in a move
that may elevate risk
a bit further,
Meta has just announced
their intention
to shut down support
for a service called
CrowdTangle,
a tool that enables
outside observers
to track the spread
of disinformation
on Meta's platforms.
CrowdTangle has been used
to prove embarrassing stories
about Meta
on numerous occasions.
The choice to shut
the service down
in the run-up to the US election
and replace it with a new tool
many feel is much more restrictive
is,
what can I say?
The decision has been met
with damning
and widespread criticism.
Beyond that,
the outcome of the US election
might be really consequential
for Facebook.
The initial choice
between Joe Biden and Trump
was a pretty uncomfortable
one for Zuckerberg.
Biden strongly supported
legislation to bring Meta
to heel,
whereas Trump has
a massive axe to grind.
He hates that Facebook
banned his account
and cracked down
on disinformation
after the January 6th riots,
and he even threatened
on Truth Social
and in his new book
that he'd imprison Zuckerberg
for life if he
interferes with the
upcoming election.
Trump has also expressed
strong support
for repealing Section 232,
which would be
a disaster for Facebook,
although it's highly unlikely
Trump would actually
see that through.
The addition of J.D. Vance
to the ticket
is a curveball
because he's the protg
of Peter Thiel.
Thiel himself
is a major donor for Trump,
which I'd imagine
bought him a degree of influence.
To my mind,
that could only really work
in Zuckerberg's favour,
but to what degree
is anyone's guess?
Probably not that much.
Kamala Harris
has similar sentiments
to Joe Biden,
although she's somewhat
less fervent about them.
Even so,
she's expressed support
for Section 230 reform
as well as antitrust legislation
and social media legislation too.
So,
one way or the other,
it's clear Zuckerberg
won't have too many friends
in Washington
for the next four years.
Next,
there's even more reason
to be concerned about children.
In 2023,
there was another massive finding
by Jeff Horowitz,
and I'm going to have to be
super careful about
how I phrase this here,
the revelation
that there's a collection
of grown men
who are now using Instagram
to satisfy their interest
in extremely young users.
Not only that,
but if you were inclined
to follow these men,
say it with me,
Instagram's recommendations
would kick into gear,
recommending you more
of the content
they were looking at.
A New York Times article
from February 2024
found even more evidence of this,
running a story
about the experience
of some parents
who reported receiving
vile comments
from grown men
on pictures they posted
that included their children,
with some men making offers
to pay to see more.
Now, with that,
we've tied up
just about everything,
and I had been planning
on ending the video here,
but I'm actually not quite done,
because there's one last,
really strange aspect
to this story
I think you need to be aware of,
because it's happening right now,
and it's often directly affecting us
in ways we don't see.
This was a rabbit hole
I inadvertently stumbled into
almost two years ago.
It started in early 2023
when I published a video
about dangerous online forums.
In that video,
I also briefly touched
on Section 230,
the legislation
we looked at earlier.
After I released that video,
in order to stay informed
about Section 230,
I made a few contacts
in the US
and also followed
a bunch of different legal experts
who specialize in matters
related to privacy,
free speech,
and tech regulation.
This allowed me
to keep track
of the latest developments
by reading relevant articles,
summaries,
and think pieces.
In particular,
I valued articles
that offered deep legal analysis,
and when searching for that stuff,
I eventually stumbled
across a group called
the Chamber of Progress.
Now,
the Chamber of Progress members
are staunch advocates
of Section 230,
and they believe
it should remain unchanged,
a perspective I thought
it important to understand,
because I didn't want
to be naive.
As I mentioned,
amending Section 230
would involve
a very complex balancing of harms,
and I wanted to understand
those trade-offs in detail.
So, with this in mind,
I'd occasionally read posts
by members of the Chamber of Progress,
especially when they were
discussing ongoing lawsuits
or legislation aimed at big tech.
Sometimes,
I found their explanations
very enlightening.
However,
I also noticed
something strange about them.
For example,
one of their members
tended to get in
vicious arguments on Twitter,
like all the time.
Also,
some of their takes
were just baffling to me.
Here's an example tweet
I found particularly odd.
People,
AI companies are scraping my posts
to train their models.
Also, people,
here's a public photo dump
of me when I was 19
because Twitter trends.
This struck me as
really childish
and mean-spirited,
seeming to suggest
that if you happen
to be naive about privacy,
you don't deserve privacy.
There was another time
when the Chamber of Progress
began posting criticism
of the EU AI Act,
the first sweeping standard
for AI regulation
in the world,
arguing that it was
a death blow to innovation.
Thing is,
like any legislation,
there's a balance
to be struck
between protecting people,
enforcing transparency,
preventing potential harms,
and yes,
not overly hampering innovation.
In general,
I expect civic-minded legal experts
providing commentary
on these things
to give an holistic overview,
to help others appreciate
whether those balances
are acceptable or not.
That said,
their articles did give me pause
from time to time.
They made me concerned
that perhaps the EU
had overstepped.
Nevertheless,
over time,
I began to see them
as a bit of a broken record,
ideological zealots
who you could always rely on
to trash any legislation
of big tech
with unwavering certainty.
I also got sick of their
constant bickering
on social media too,
so I quietly unfollowed
their members
and moved on.
Then,
about six months later
while making this video,
I put out a call on Twitter
to interview Section 230 experts
on the topic of
the Kids Online Safety Act,
and lo and behold,
one of its members
got in contact,
offering to speak to me
and wanting to know more
about what I was up to.
So I outlined the basic purpose
of my video
and received
no response.
Not even,
oh, I'm a bit busy,
just nothing,
which I found
a little bit weird.
They seemed really interested
at first.
Well, that got my
spider-sense tingling.
Until that point,
even though I'd followed
some of its members,
I was only aware
of the organization itself
in the most haziest of ways.
So I quickly logged on
to the Chamber of Progress website
to finally look
at what they actually do.
Chamber of Progress
is a new tech industry coalition
devoted to a progressive society,
economy,
workforce
and consumer climate.
We back public policies
that will build a fairer,
more inclusive country
in which all people benefit
from technological leaps.
Oh, that's reassuring.
Then I looked at
their partners section up here
and,
oh, they're funded by Meta
and Google
and Amazon
and Apple
and Midjourney.
Oh.
Then to verify,
I checked Meta's page
on their political engagement,
the candidates they fund,
how much they spend
on lobbying, etc.
And sure enough,
the Chamber of Progress
was listed there too.
And that's when I realized
that this entire time
I'd been a total sucker.
I'd read so many articles
on big tech,
often with so many different threads
to follow up on,
I hadn't scrutinized
stuff from this group
carefully enough.
I'd failed to ask
the most basic of questions
that you always need to ask
when encountering opinions
that don't quite add up.
How do these people get paid?
I thought I'd been reading
the opinions of
completely independent
legal scholars.
What really bothers me about this
is that there's a broad spectrum
of opinion out there
and it's already not
a level playing field,
but for Meta to pump millions
into one particular opinion,
the opinion that happens
to serve their interests
so that they're dramatically amplified,
strikes me as highly unprincipled.
I couldn't believe
they'd managed to manipulate me
from beyond their platforms.
To the outside observer,
the materials produced
by these organizations
can appear as grassroots opinions
from civic-minded citizens.
Like if the articles I'd read
had been accompanied
with the disclaimer,
Chamber of Progress
is a lobbying group
that receives funding from Meta,
it would have completely changed
how I regarded their work.
And this is the final part
of the new comms strategy
I mentioned earlier,
enormous spending on lobbyists
and influence groups
to try and shoot down legislation
or drive popular anger
about tech regulation,
often by creating PR campaigns
and even documentaries
that try to undermine legislators
in the eyes of the public.
In 2023,
Meta spent $19 million on lobbying,
but 2024 looks set to dwarf that.
Spending in the first half of 2024
is already way higher than 2023
and also miles higher
than any other tech giant.
The Chamber of Progress
is run by Adam Kovacevic.
As one tech journalist puts it,
Adam Kovacevic has become
a dialogue quote for reporters,
always happy to make
big tech's case
as the companies
stay above the fray.
For their money,
the big tech companies
get an organisation
willing to push their viewpoints
in the press,
sometimes with spotty disclosure.
What does he mean
by spotty disclosure?
Well, listen to how Kovacevic
is introduced in this news report.
He's the founder and CEO
of Chamber of Progress,
a centre-left
tech industry policy coalition
and it aims to ensure
all Americans benefit
from technological advances
and works to make sure
the tech industry
operates responsibly.
Hearing that,
I'd think Kovacevic
is holding big tech to account.
For another example,
in an article he published
on a site called CEPA,
the Chamber of Progress
is described as
a centre-left
tech industry policy coalition
promoting technology's
progressive future.
No mention of funding again.
But if you go to the
About section on the CEPA site
and select Supporters,
boom!
Hello, Meta!
Kovacevic pops up regularly
when you look up legislation
that seeks to rein in
big tech companies,
usually in the form
of an article on Medium
that trashes that legislation.
Sometimes his trashing
is justified,
sometimes it's really forced,
clutching at straws.
For example,
take the proposed
American Innovation
and Choice Online Act,
championed by a senator,
Amy Klobuchar,
which seeks to limit
tech giants' ability
to unfairly promote
their own services
over those by smaller competitors.
Specifically,
it seeks to prevent
the intentional
disadvantaging
of smaller businesses
by doing things like
forcing them to pay
in order to rank highly
in search results.
The Chamber of Progress's
response to this legislation
is that it would
break Amazon Prime.
That's the doomsday scenario,
forcing Amazon
to rethink how Prime works.
On all of this,
I'm joined by
Adam Kovacevic,
founder and CEO
of the Chamber of Progress.
Adam...
They're funded by Meta,
Amazon,
and a bunch of others.
You forgot to mention that.
Talk to us about
how significant this bill,
if passed, could be.
Zuck us on.
That would mean that
Amazon couldn't feature
Amazon Basics brand products.
It probably couldn't highlight
Amazon Prime
two-day shipping products.
Even if true,
would that not be
a price worth paying
for a properly competitive
playing field
where big tech
can no longer monopolize
absolutely everything?
Listen to Amy Klobuchar
outline the degree
to which these groups
are interfering
with the legislative process.
Let's talk about
what we're up against.
I'm talking about
some of the most powerful
companies in the world
with armies of lobbyists
and lawyers.
They are everywhere
in every corner
in this town
at every cocktail party
and all over this building.
I tell my colleagues
that they don't even know
sometimes when someone
is trying to influence them.
I tell them
they should ask the person
if they're being paid
by a tech company
or if they are on
a board of a tech company
or if they have
some affiliation
with one of the
big tech companies.
They are also lobbying
the American people
with astroturf campaigning
and other dishonest
PR tactics.
These companies
have been telling
anyone who will listen
that acting to protect
competition
in our digital markets
will sometimes
or somehow
seed our national security
or it will outlaw
Amazon Prime
claims that were disputed
by the Department of Justice
and Amazon's own lobbyists.
But they don't stop there
because providing
what appears to me
as incredibly one-sided
legal analysis
isn't their only approach.
They also run
fear-mongering ads
on Facebook too.
Your Amazon Prime subscription
is in danger
hoping I suppose
that they'll motivate
their target demographic
to complain to the representatives
to put them under pressure.
And then of course
there's the money.
I think this is actually
the best evidence
of just how big
and dominant
and bullying
these companies are.
Running ads in states
where people are
in tough races.
How obvious can it be?
Message received.
They also run
other kinds of
cringe PR campaigns.
My favourite is
their 2024 gem
Generate and Create,
a campaign that celebrates
woefully
how generative AI
is a force
for creative liberation
actually.
Opponents of fair use
are working to shut down
AI-created art
through legal
and legislative action.
We're fighting back.
And being paid by
MetaGoogle
and MidJourney
to do so.
How are they fighting back?
Well, by creating
a laughably produced
YouTube video
where they drag
three artists to the stage
to argue their case for them.
These sad sacks
then proceed to mumble
unconvincingly
about how Gen AI
is useful
for their workflow, kinda.
My favourite line
by Kovacevic is this.
Gen AI is a net plus
for creativity overall.
It's expanding access
to creative tools
for more and more people
and bypassing
a lot of the
traditional gatekeepers.
What gatekeeping
is he talking about?
The Salon de Paris?
Is he talking about, like,
the admissions process
for art colleges
or animation studios?
To whom it may concern,
it's been a dream of mine
to study at your institution.
I'm passionate,
creative,
and hardworking.
Enclosed is my
AI-generated portfolio.
Looking forward
to hearing back,
Timmy!
They even created
a rubbish website
to kick off their
Generate and Create campaign,
and it's filled
with hilarious cringe.
I really do love it
when legal professionals
weigh in on matters
to do with creativity.
It's so cute.
We didn't hire
a professional designer
or need to learn
Adobe Illustrator
to create our campaign mascot,
Arty Fish.
We created Arty
to Midjourney,
an AI tool
to expand creativity.
With AI,
anyone can instantly
create custom designs.
Well done, guys.
That is how image generation works.
Get a grip.
I don't even understand
why this is here.
Is this meant to be inspiring?
And also,
Arty Fish?
Please tell me the concept
was AI-generated too.
But the icing on the cake
was a tweet
one of their more
overzealous members put out,
clearly getting a bit
carried away
after one too many
doctrinaire Zoom calls.
It's a masterpiece of nonsense,
one of the most
condescending backwards pieces
of mumbo-jumbo
you'll ever read.
For context,
it's aimed at the artist community
on Twitter
and reads
Many of the artists on here
have demonstrated
their dedication
to keeping marginalised folk
including those with disabilities
out of their community
and marketplace.
Gen AI says
F*** that!
Those barriers come down now
and I'm here for it!
As you can imagine,
this caused absolute uproar
with an enormous number
of critical responses
coming from disabled artists
who pointed out
that far from being marginalised,
their ability to create art
provided a vital way
for them to earn a living
and interact with others online.
One of my favourite responses
was this
Double combo!
The diversity
and gatekeeper card
played at the same time.
This is why big tech
pays so much for this lobbying.
But that's not all.
Another purpose
of this ridiculous campaign
is to try and sell the idea
that artwork generated by AI
should be protected
under fair use.
Yeah, I know.
So in order to do that,
they have to try
and mesmerise the general public
into thinking that
if you tell a computer
to make you art,
you're an artist.
And even if you have
sympathies with that idea,
just remember,
the reason they're saying it
is not because they actually
care about creativity
or have the faintest clue
what they're even talking about
when it comes to creativity.
It's to try and manufacture
a broad consensus
as a step towards
protecting generative AI
from claims of copyright infringement.
Now, I'm not a big lover
of the copyright holders
of this world,
but I can tell you for sure
I'd rather a world
where those guys
are still holding the aces
rather than the entirety
of Earth's artwork
being subsumed
by the likes of Meta.
Another of their big pushes
is to advocate
that Section 230 be extended
to also cover generative AI,
meaning that tech firms
would be completely shielded
from anything the AI outputs.
And by the way,
the original authors
of Section 230,
Ron Wyden and Chris Cox,
do not share this opinion.
According to Cox,
Section 230 is about
protecting users and sites
for hosting and organizing
users' speech
and has nothing to do
with protecting companies
from the consequences
of their own actions
and products.
According to Wyden,
and it isn't
a particularly close call.
Now, you might agree
with some of the
Chamber of Progress's positions,
I agree with some
of their positions,
but I think it's
completely disgraceful
that Meta is willing
to actively pump them
with money,
which enables them
to hire more staff,
take out ads
and waste legislators' time
and our time.
It's fine for big tech
to hire third parties
for research or advertising
or legal representation,
but these relationships
have to be
completely transparent.
And in my opinion,
given the examples
I've just shown,
as well as tons of others
I haven't,
the transparency test
has not been passed.
Not even close.
And if you take
all this together,
that's why I can
finally conclude
that I have incredibly
low faith
that Meta is going
to turn things around.
Because there's seemingly
no end to their callousness,
no end to their incredible
lack of respect
for their users.
Every time there's
an opportunity to improve,
they choose the most
self-serving
and disappointing option,
which regularly makes
the world around them
demonstrably worse.
Bear in mind
that in January 2024,
Mark stated,
boldly,
that he welcomed regulation
and wanted it applied
at an industry level.
When I first heard him
say this,
I felt it was a good sign.
I felt he'd accepted
change was needed.
But for Meta
to then go and fight
regulation at every turn,
that's about as low
as it gets.
They've now gone
completely through
the looking glass.
They work for them,
not for us,
and their behaviour
is as objectionable
as it's ever been.
What else needs to be said?
Meta regularly ranks
in the top 10 lists
of the most hated
companies in the world.
And this is really troubling
because Zuckerberg
answers to no one.
Yes, he looks bad
if shares plummet,
but he can't be ousted
for bad performance.
He can't be ousted
by his own company
on any grounds,
making him,
to put it bluntly,
possibly the most
influential person
on the planet,
given his control
over how we communicate.
But he has consistently
demonstrated a pattern
of terrible judgment
when it comes to
negative effects on society.
Why?
Who knows?
It's hard to know
how much he's been
influenced by the
techno-libertarian ideas
of those like Peter Thiel,
but whatever the reason,
he seems to be both
ideologically or intellectually
incapable of living up
to his responsibility
for the damage
his platforms do.
He's a top-tier
developer and businessman,
but unfortunately
for all of us,
that's all he is.
As his one-time mentor,
Roger McNamee,
later wrote,
What I did not grasp
was that Zuck's ambition
had no limit.
I did not appreciate
that his focus on code
as the solution
to every problem
would blind him
to the human cost
of Facebook's
outsized success,
and I never imagined
that Zuck would craft
a culture in which
criticism and disagreement
apparently had no place.
His recent retreat
from engaging
in valuable social conversations,
coupled with his attempts
to actively hamper legislation,
have made it clear,
to me at least,
that he's given up
on the world
and that he sees his critics
as opponents
that need to be beaten.
So, what can be done?
Well, there are three angles.
The first and most obvious
that we've already spoken about
is legislation
that erects
appropriate safeguards.
First, forcing social media platforms
to openly share
their analytics and research
so they can be tracked
and held accountable
for their failings,
and establishing standards
they have no choice
but to follow.
But if that's not your thing,
the second option
is antitrust legislation,
which could seek
to split meta up
into smaller parts
because it's simply become
too big and too monopolistic,
just as was done
with Standard Oil in 1911
or with AT&T in 1982,
moves which we can now see
were massively beneficial
for fostering competition
and innovation in the US.
This would not solve
the problem of how
to regulate social media,
but it would weaken
Zuckerberg's iron grip,
including his ability
to pump money into lobbying,
his ability to set terms
favourable to himself,
and his ability
to stifle open competition.
At the very least,
it would help to diversify
the approaches being used
to make social media
safer and less intrusive.
And just so you know,
at the time of writing,
there are already
three new antitrust legislation bills
being actively discussed
in the US right now.
The third way
is to kill meta with our feet,
either by just closing
our accounts
or by moving to a competitor.
But obviously,
the choice of competitor
is important here
in order to avoid
repeating the same problems.
I don't think it's a stretch
to suggest that most of us
would be happy
to leave meta's platforms
if there was a proper alternative,
which had the potential
to achieve mass adoption
and which we believed
would do right by us
in the long term.
Now, there are many
alternative social media
platforms out there
run by rival tech companies
or by non-profits.
It's an enormous topic
which I could dedicate
an entire video to,
but for now,
I'll just summarize quickly.
Since for-profit companies
are clearly just too dangerous
to run a social media utility,
the ideal solution
would be a non-profit
like the Wikimedia Foundation.
However,
it would need to secure
enough funding
to be competitive
because money is needed
for it to be useful,
fun and well-moderated.
Apart from that,
it would also need a system
where its leaders
are either re-elected
every few years
or ousted
if they underperform.
Then there's
the experience itself.
Now,
many of the existing
non-profit alternatives
fall into the trap
of championing
technical solutions
to solve social problems
like federated social media
using open decentralized
technologies
such as ActivityPub,
things that sound great
to the tech savvy
but which means
nothing to regular people
who only notice
the usability tax
that makes them confusing
and annoying to use.
Regular users
should not need to understand
how a Fediverse works.
They should not need
to understand
what server they're on
and how they can post
to other servers.
It's not good enough.
In order to beat meta,
it's not enough
to be virtuous.
It's not enough
to have a cool
technical solution.
You have to be able
to provide an excellent
experience or there's
just no point.
Apart from that,
any social media firm,
no matter how virtuous,
would eventually face
awkward challenges
when it comes to free speech.
So it would need
a public charter
and an oversight board
when decisions need
to be made
that might curtail free speech.
A process made much simpler
when you don't have to worry
about a hit on ad revenue.
Gosh,
just thinking about it,
I'd love to be involved
in setting up something like this
or just joining an existing
promising effort
if there was enough
momentum behind it.
One can dream.
So the final thing I'll say
given everything we've looked at
over this terrifyingly long video
is to emphasise
the importance of trust.
Trust is the most precious
of assets
and meta's biggest problem
is that Mark Zuckerberg
has lost it.
Looking to the future,
we need to have solutions
that we trust
because it's impossible
to avoid mistakes.
We will never have social media
that's completely safe.
We will never have social media
where horrific abuses
or manipulation
or exploitation
doesn't happen.
But what we can have
is better people at the top,
motivated by the right things,
making the best they can
out of a bad situation.
And as I've just pointed out,
we do have options here.
You can force trust
by supporting
safeguarding legislation.
You can avoid the need
for so much trust
placed on one person
by promoting
antitrust legislation.
Or you can vote
with your feet
by closing your accounts
or by going somewhere
more worthy of your trust.
But whichever option you prefer,
if you agree with me
that something has to be done,
take whatever active steps
you can to make it happen.
