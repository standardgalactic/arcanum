Good morning, DEF CON. Thanks for coming out. I appreciate it. We have two speakers
this morning, Tom Cross and Greg Conti, and their talk is going to be on deception and
counter-deception. Please give them a good DEF CON welcome.
So, good morning, everyone. Thanks for making it out this morning. So we're excited about
this talk. I've been coming to DEF CON, you know, since it got started. And when, you
know, early DEF CON, you know, at that time, the Internet was just beginning to become publicly
available to consumers, right? And we were thinking about, you know, how is the Internet
going to affect society? Like, what is going to happen, right? And we were sort of imagining
a world that we now live in today. And I think it's interesting to look back at how, what
we thought would happen and question, you know, sort of, you know, where we've ended up, right?
I think people in this community could see both the, like, promise and peril of the Internet.
I remember going to lunch at DEF CON, going to a food court, and everyone's paying with
cash. And we're thinking, you know, in the future, we're probably going to pay electronically
for everything. And a little record will be kept every time we buy something. And like,
it will create this model of our behavior. And how will that get used against us, right?
But at the same time, I think people were genuinely excited about the idea that everyone will have
the world's knowledge at their fingertips, right? And maybe that will make people a little smarter,
might up level humanity. And so, as, you know, time has gone by, and we've actually got
all these capabilities, and everyone's using them. You know, there's certain facets of human
nature that have come roaring forth, and have a huge impact on how this stuff actually affects us.
I think the part of the problem is that people don't come to the Internet because they want to
get smarter. They come to the Internet because they want to be told that they're already smart.
They seek validation, right? And there's a lot of people who recognize this, and they're in what I
call the funhouse mirror business. So they present you a world in which you're a good person, in which
you should get what you want, and in which the people that you dislike, we're going to show you
those people in the most negative light possible over and over again. And this world view that is
centered around you and why you're special is highly compelling to people's egos. This is what we,
on some level, mean by deception. And we find deception not just in the, like, narratives that
appear on social media, but also at every level, like in the phishing emails that people are getting,
in the malware that's running on their computers. Ultimately, the Internet has become this massive
deception engine in which these false narratives are appearing at every level of abstraction,
and you can't trust anything that you see in the screen. I think people have recognized this.
They've recognized that the Internet is not giving us what we wanted. It's not making us smarter.
And in fact, the theme of this year's DEF CON is how do we engage with the Internet that isn't giving
us what we want? How do we make things better? So before I get into the overview of the talk,
let me introduce us. So my name's Tom. I've been coming to DEF CON for a long time. I've done a few
social media projects over the years. My current project is called Feeds here. It's a news reader
app for Mastodon. So it shows you the top links that have been posted on your feed in the past 24
hours and what people are saying about each link. And there's lots of InfoSec folks on Mastodon.
I've also, you know, generally had a career in InfoSec and I've spoken at a lot of cons on
information security topics often with this gentleman, Greg.
Hi, I'm Greg Conti. Thank you for joining me here on Arrakis for DEF CON 32.
My background is I was long term faculty at West Point where I ran their cybersecurity research and
education programs. Also worked at NSA twice and US Cyber Command twice. And then I developed and
taught the information operations course at Black Hat Training seven years ago and been running it
since there and in private sessions and also run the military strategy and tactics for cybersecurity
course for 10 years. Same deal. Thanks, Tom.
Oh, and Greg and I are teaching a class on adversarial thinking at DEF CON training after DEF CON is over.
If you guys want more Vegas. So what are we going to cover in this talk? The first thing we're going to do is we're
going to tap into some of Greg's deep expertise with military doctrine and thinking militaries have
been thinking about conflict for hundreds of years and deception in particular is an area where they
have identified maxims that teach you how to craft effective deceptions. So we're going to talk about
how to do deception effectively. And then we're going to flip the coin over and ask, well, if we understand
offense really well, what does that teach us about defense? Are there counter deception principles that
we can derive that tell us how to fight deception? And the deception and counter deception principles
we're going to cover, they're useful in any context where this could be occurring. It could be useful for
malware attribution or in a security operations context. But, you know, this year's DEF CON is about
engaging with the internet. And so we're going to spend the third part of the talk trying to apply
some of these counter deception maxims and ask what sort of capabilities, you know, is the internet
lacking that might be useful or might help us defend ourselves against deception that we're facing? So
why are we talking to you guys about this? Like, why is this relevant? I think, you know, hackers have
the ability to identify fuckery better than like, a lot of, you know, general people. So you guys have
a unique talent at that. And, you know, I also, you know, I mean, I listened to Cory Doctorow's talk last
year, which inspired this year's theme and his talk yesterday. It's he's engaged in a very important
discussion about like policy issues. And I think, you know, what I do is I write code and I break code.
And I want to talk about, you know, how I can apply those skills to engage with this problem as well.
So, you know, hopefully, you know, we add to the discussion a new dimension.
So, Greg, let's talk about deception.
So deception has been around for millennia. And the key idea is that it's the act of hiding the truth
to get yourself an advantage. And what you're trying to do is influence your target to make an
incorrect decision, right, or to take an action that you want or fail to take an action all to your
advantage. And we've seen examples from our five legged Trojan horse AI, generative AI delivers when you
need a five legged Trojan horse, to the Civil War, where people painted logs black, to create the
to try and fake cannons, to the Cuban Missile Crisis, hiding medium range ballistic missiles,
and concealing them in ships, to the Persian Gulf War, where the invading Iraqis were concerned about
an attack by the sea, by the Marines, and instead were surprised by an attack by land
from the opposite direction. We see it in the Russia-Ukraine conflict, the Ghost of Kyiv, which
was a mythical fighter pilot, ace fighter pilot fighting the Russian aggressors over the city,
which was later, you know, proved to be a deception operation. So we've seen this for millennia.
And when we think about the targets of deception, humans come first to mind. And, you know, in
information security, users, you know, things like phishing, typo squatting, domain mimicry,
spoofed login pages, but also experts, think malware analysts, false flags, fileless malware,
deceptive metadata, code injection, rotating command and control infrastructure. So specialists can
be targets for deception as well. But more than that, it's not just the humans, it's our code.
So our code, think malware detection systems. People try and deceive that through fileless malware,
polymorphic malware, rotating command and control infrastructure. And as we move into the era of AI,
AI is also a deception target. Think poisoning the training data of the AI,
or jailbreaking techniques to overcome safeguards that people put in place.
And deception can occur at all levels. It can occur at all levels of the network stack.
It can also occur in thinking more broadly at the tactical, operational and strategic level.
The tactical level, you're actively deceiving someone you're engaged in conflict with.
All the way up to strategic level, where you try to hide your basic, you know, national objectives,
your intentions, your strategies, and your capabilities, or put forth,
maybe to put forth an alternate reality that's more beneficial.
So humans process data through this idea of the DIKW hierarchy. We begin with data and by adding context,
we can create information, by adding meaning, we can create knowledge, and by adding insight,
we can create wisdom. Well, deception can poison our ability to, you know, by poisoning the data that we
can have incorrect information, knowledge, wisdom that we think is entirely accurate. So deception
poisons our ability to think. And it's a professional discipline deception. What we have here,
the orange document, is a declassified CIA document that includes the deception maxims that we're going to
be discussing. And also there are, there are manuals, like this is the Department of Defense Joint
Publication on Military Deception. It comes out every few years, updated. So this, the key takeaway
here is it's a professional discipline. And looking at some of the maxims, we're just going to run through
these pretty quick, and then we're going to flip them and show how you might counter. But one of the most
important is it's easier to maintain a pre-existing belief in your target than to force a change.
Right? So if someone believes the world is flat, you can help encourage them to continue thinking
the world is flat. But if you want to have them think the world is round, well, that's much harder.
So yeah, I really think Magruder's principle is sort of the golden rule of deception. It speaks to this,
like, fundamental aspect of human nature. You know, say there's a politician you hate. There might be
different answers in the room for who that person is, but everyone hates some politician, right?
And if I walk up to you and I say, well, you know that politician you hate? Did you hear about the
stupid thing they did this morning? Whatever I say next, whatever it is, you're going to believe it.
Because it's aligned with what you already think, and it feels good to be right. And so, if any of you
have read Asimov's Foundation series, he has a really good model, I think, of deception that he
presents in those books. There are these robots that can manipulate people's emotions. And they're
constantly explaining that they can't radically change people's worldview. And the way that they
function is by trying to understand what that person thinks and identify some specific belief that
they have that they can emphasize or reinforce just enough to get that person to take an action that
they might have otherwise not taken. And I think that's a really good representation of, like,
effective deception in a nutshell.
So, one of my favorites is the idea of exploiting the limits of human and machine sensing
and information processing. So, you think, we as humans have senses, right, that are limited and,
like, known parameters of what we can sense. And we have limitations in our ability to process
information. Machines do the same. They have sensors and they have information processing.
So, if you can exploit those limits, for example, think self-driving cars. There's been some really
interesting talks that you can exploit the sensing and processing of a self-driving car and make it
see things that aren't there. In a heist movie where there's usually a motion sensor and someone moves
really, really slowly to slip past the motion sensor, that kind of thing. And a variation of that is,
here's a picture of an inflatable tank from Ghost Army in World War II. It was a group of creatives,
and they were a formal deception unit. And it's their ability to create inflatable tanks and fake enemy
or fake troop emplacements was because the capabilities of the time to sense what was, you know,
sense the environment. These tanks looked real from a distance. Now they'd be, they'd obviously be not.
Then the third maxim is Jones's dilemma when the idea is that in this competitive information
environment of competing narratives, that if you're going to try and deceive someone and you have to,
ideally, have more false sources than real. So it's this battle of quantity and quality of competing
narratives. Another strategy is the idea that you want to carefully create a story,
that you don't just have single data points, but you're putting puzzle pieces out there for the
deception target to connect together that tell the story that you want, that you're trying to,
to portray. Another is the idea of carefully designed planned placement of deceptive material.
The idea is you want to make your target work for it. If you, if, think if you wanted to have a fake
diary in your room, right? If you leave it on their desk, that, that is an obvious place. And that
could be like your, um, that probably what you want to do is make it hard, have a lock on it,
have it hidden in a ventilation duct, have it written in code. By the time they find that and
they've unpacked all that, then they think this is absolutely got to be the real diary. And we see that
in malware as well, right? That's certainly a strategy. If you make them work for it,
people will be more likely to believe it. The flip side of this is an orgy of evidence where
things are, you know, many incriminating things are obviously found. Well, that should raise suspicions.
So yeah, Olympic Destroyer is a really good example of the use of, of some of these techniques in order
to fool malware analysts as to attribution. Doing attribution on IOCs is fraught with risk as with
respect to, uh, you know, the manufacturing of those IOCs. Um, in this case, uh, there was a rich
header in a binary, uh, that, uh, you know, captures information about the developer's environment. And
they, they literally copied a rich header from a Lazarus sample and put it in their, their sample.
Um, and so someone who knows rich headers and like, uh, has a collection of them and knows that they
could be used to, uh, uh, you know, attribute malware might find this and, you know, be excited
that they connected the dots, right? And so now they're emotionally invested in the idea that this
is true. Um, and, uh, you know, they, they go out with this narrative. It turns out that in fact,
in this case, the rich header didn't match the rest of the binary and, uh, the attribution was
completely deceptive. Um, so again, uh, uh, uh, you, you know, avoid, uh, if you're trying to protect
yourself against deception situations where you're emotionally invested in the consequences of your work.
So I particularly like this one, uh, maximum 6a, 6b, there are two sides of the same coin.
The first is ambiguity. So the idea is you want to increase doubt in the person's mind,
in your target's mind by providing many possible truths. Okay. Makes sense. They're, they,
they're uncertain. There's this cloud of uncertainty. The flip side of that is you wanted to decrease
doubt and focus the target on a particular, um, falsehood. So you want them instead of being
in a sea of doubt, you want them absolutely positively sure that they're right, uh, or that
they sure about this falsehood. They think that they, you know, that it's true. Another idea is that in,
in professionalized deception, that there's the idea of husbanding deception assets,
that there are always limited resources. You need to save them for the time and at the best time and
the best place to be effective. You can see that in cyber, in, uh, information security capabilities
all day, right? Uh, but imagine if you had the ability to place deceptive material on a web server,
you can't, if you use it, you're using that capability, uh, and he might not be able to use it
again. So you save these for the time and place you think will be most effective. Uh, Maxim 8 is the
idea of feedback. In a professional, uh, deception operation organization, the attackers are monitoring
their target audience looking for feedback that the deception has been believed. And they are also,
so they want that feedback loop. Uh, and they're also monitoring their friendly organization to see if
they're being deceived. All right. So now that we've talked a little bit about the principles of
effective deception, let's flip the coin over and talk about how you practice counter deception. Um,
so, uh, um, at a high level, I think there are four, um, general ways to counter deception.
One is through intelligence collection. So if you're actually monitoring your adversary and just watching
them create their deception, then you know exactly what they're doing and why they're doing it and
what the truth is. Right. Um, and U S cyber command has this defend a forward, uh, um, uh, concept,
which has to do with like, just going out and directly spying on the people that are committing
to acts so that you know exactly what they're up to. Um, another thing you could do, not, we don't
always have intelligence capabilities, um, is disruption. Um, if, uh, so we talked about husbanding assets,
if, uh, you know, someone has to go to a certain effort to inject, um, deceptive narratives, um,
you know, in places and, and you can take action to, to interfere with their capabilities, um, then
that prevents them from being successful. Um, a simple example is like if they have to build a botnet
in order to spread something online and you're able to take the botnet down, then you're interfering
with their deceptive capability. Um, another thing you can do is, is analytic. Sometimes that's your
only option. Um, you're looking at the information you're collecting and you're critically analyzing
it in order to figure out whether or not it's deceptive. Um, and a lot of our talk will focus
on that because that's often the only option that you've got. Um, another thing you can do is deterrent.
So if you can, if you can demonstrate, uh, that the deception will not be effective, um, perhaps your
adversary will not bother to do it. Um, so, uh, and let's talk a little bit about analytic processes.
Um, in Infosec we're really good at devil's advocacy, right? Um, and so this is the same thing.
It's about playing devil's advocate with respect to things that you are, that you are deciding
or that you believe. Um, and, uh, you have to have the discipline to do that. And that's one of
the hardest things about it. Um, but, uh, you know, if you have a belief or intelligence related conclusion,
and there's a set of, um, facts that underpin that belief, you can look at each of those facts and
ask yourself, how hard would it be for my adversary to simulate that fact? Am I, how many ways have I
measured that? Um, you, you know, like, is it possible to fake that? Uh, and, uh, you know,
even in malware attribution, you know, a rich header is easy to fake, uh, you know, maybe, uh, you know,
could they get access to a particular source IP address or how hard would that be, uh, for them to,
to simulate in order to fool me, right? Um, so let's talk about some of the deception maxims,
and I'm going to sort of flip them around and, um, and create counter deception maxims based on them.
And I kind of organized them into categories. So the first category, uh, is, um, maxims that
suggest when deception may be present. So your human intuition is both an asset and a liability
when you're dealing with deception. This is the context where it's an asset. You want to develop
a kind of spidey sense that tells you things aren't quite right here. It, this looks fishy.
And that might prompt you to dig deeper, um, and explore the hypothesis that what you're seeing
might be a deception. Um, so for example, we've talked about carefully sequencing, um, events to
tell a story. So it builds up in the target's mind. Um, so flip that over. What happens if there's
all kinds of evidence that gets disclosed all at once? If you find the diary sitting out on a table
where it shouldn't be, um, you know, that might be a sign to you that like, Hey, someone's trying to,
you know, manipulate me that the, this is, this is possibly deceptive, right? Um, so the, the,
the person operating in the deception must either do ambiguity or misdirection. And in each case,
we can try to counter that, right? So if they're engaged in the ambiguity deception,
there's going to be a lot of narratives that are available, right? And if you see multiple narratives,
then that might be a sign, uh, that deception is taking place, that some of these narratives
are simulated. Um, that was certainly the case with Olympic destroyer. There were a bunch of
organizations that had different attribution that they were publishing. Um, and, and that's because
the malware was this intentionally sort of misleading analysts into thinking that the, that, that
different narratives were correct. Um, misdirection, this in a misdirection deception,
the attacker is trying to get you to believe a particular thing, which is false. And so, um,
if you analyze it carefully, perhaps you can discover that it's not true.
Uh, the, the plus minus rule is this idea that nothing that is an imitation or a simulation can
be exactly the same as the real thing. Otherwise it is real. And so identifying those characteristics,
which are added or removed from the, uh, from the thing, it helps you identify that it is deceptive.
And again, you have to have the discipline to say, okay, this isn't right. It's incongruent.
And therefore I must accept that it is not real.
The place where your human intuition operates against you has to do with mental discipline.
Humans are really good at jumping to conclusions based on not necessarily enough evidence. We kind
of have to do that in order to function in the complicated civilization that we have. Um, but, um,
you know, that's how, uh, deceptive, deceptive operations get you. And so you kind of have to
have the discipline to slow down and not listen to sometimes to your intuition. Um, so this is
Magruder's principle is, is the biggest one here. Um, it's important to apply the same critical
analysis to facts that support your assumptions that you do to facts that challenge them. And that's
really hard for people to do. Um, you know, also with respect to carefully designed placement of
deceptive material, don't assume a fact is true just because you had to work hard to get it. Ask
yourself, could the adversary have simulated that? Um, I love this quote, uh, from a book on
professional counter-deception operations. Um, the vulnerable mind fits ambiguous information to its
own preconceptions and expectations. I think that's a really clear statement of, of vulnerability to
deception.
So humans are not that complicated, which is probably part of the problem, right? Psychologists
have been studying, uh, biases in humans for, for hundreds of years. And I have a list of examples
here. And if you're wondering, the prompt was, uh, humans swimming in a sea of piranha. That was the
prompt that gave us that. Um, but I wanted to highlight two. One is the idea of selection bias. And just one
instance of selection bias is where you can choose, um, from a variety of facts, truths to paint a
picture, but you don't share all the truths. You're just selecting certain truths. And I think we see
that in many news agencies today. A lot of what they say is true, but which ones they choose, uh, paint a
picture. So that's selection bias. Confirmation bias sounds a lot like Magruder's principle. The idea that,
um, we are, uh, are biased toward things that already, uh, reinforce our preconceived beliefs.
So the third set is, uh, maxims that suggest methods of preventing deception. Um, so, uh, the two
maxims that have to do with the limits of human and machine sensing and information processing, like,
speak to the need to be able to measure reality from multiple, uh, perspectives. Um, you know, and think
about this on a, in a technical sense, if you, if you're in a security operations function and you
have sensors out there that like, look at your network, um, you know, if there's only one kind
of a particular sensor, then that sensor could be, uh, its data source could be manipulated. It could be,
be presented with false facts. If you've got multiple ways of looking at the situation,
then it gets harder and harder for the person that's operating the deception fool all of the sensors
that you have. Um, with respect to husbanding deception assets, again, deception operations,
professional ones, um, involve effort and resources. And if you can disrupt and target
those capabilities, um, then you can undermine the ability for them to be successful. Um,
feedback is also interesting. A professional deception operation is going to be watching you
to determine how you behave. You may choose to behave as if the deception was effective because
you don't want the deceiver to know that you were not deceived. Uh, you may also choose, um, to, uh,
to show the adversary that you are debunked, that you are, um, aware of their deception, which is a
deterrent method. Um, one of the things that, um, you know, folks in DHS and CISA are talking about
is pre bunking. So we think that this deceptive narrative is going to be placed out there. So in
advance, we're going to go ahead and explain to everyone that it's false so that, you know, people just
don't bother trying to push that narrative in the first place. So in, in developing this set of
trustworthy sources of data, like when we're looking at things on the internet, it's really important
for us to, uh, you know, collect different places where we can get different points of view, uh, that
are credible and, uh, objective. Um, and so like curating collections of experts, I think is really
important. Uh, and, but also a weakness that we have is we have a tendency to believe
experts who, um, you know, are telling us what we already think. And so we have to
find objective criteria for evaluating who we want to listen to. Um, and so I, I think this is a real
challenge. Um, like even with, uh, with respect to experts, experts sometimes, uh, deceive us, right?
So let's say there's a set of facts, one, two, three, four, and five, and they add up to a conclusion,
which is 15, right? Um, so I, what I can do is I can present you some of those facts, one,
two, and three, and a conclusion, which is six. Each one of the facts I've presented to you is
absolutely true. And the conclusion I'm presenting to you is a natural consequence of the facts that
I've presented, right? So when you're listening to an expert, the challenge is that you depend upon
them, not just to tell you what the conclusions are, but also what the underlying facts are to educate
you about the issue. Um, and so you're, you're vulnerable to the fact that, that without knowing
more than they do about the subject, that you can't see the other facts that they're omitting
that may lead you to a different conclusion. Um, and so, uh, you know, I, I, one of the things that
people wring their hands about a little bit is journalistic objectivity. There's this idea that like
in the American civilization in particular, where we have two political parties, that what should happen
is that, uh, journalists should present both sides. And we know that like getting both of these
perspectives does not get us closer to the truth, right? Um, and so perhaps there's this role for
journalism as a professional resource that goes out and finds the other facts that are missing
and adds them into the equation to get the complete truth, right? Um, this is something that people have
been talking about for a very long time. This is not an internet problem. Uh, Walter Lippmann was writing
about this stuff in the 1920s. Um, and he had these interesting ideas about what journalism could be,
but then he also, uh, you know, he, he was, um, uh, he was concerned that it won't be that,
uh, because the business model of newspapers doesn't support this kind of work. Um, and, and if,
if the news media isn't going to go out and find the additional facts for me, my question is,
can my computers do that? Can I, can the internet give me the facts that I need in order to objectively
understand reality? And so that relates to the third part of our talk and our call to arms. Um,
you know, like hackers have an independent view of reality that's somewhat outside of mainstream
thinking. Um, and that enables you to see things with different eyes. Um, there are interesting
capabilities that I'm going to talk about some examples of them, uh, that might be useful to people
to help combat, uh, deception and disinformation. And I think also we're, we're tool makers and breakers.
Um, and so, uh, you know, I think we, we're not entirely cynical about the value of the, that new
tools can, can bring. Um, so I'm going to focus on four areas. One area, it's just something that
I need to mention. It's that model the adversary's capabilities and neutralize them. I've mentioned
that several times. Um, there's been tremendous work over the years here at Defcon in the misinformation
village. Um, and there's also this thing called a disarm framework, which is essentially a kill chain
for internet, uh, you know, botnets and deception operations. This is fantastic work. We love it.
Uh, and, um, you know, I want to endorse it here. There's lots of things that people can do in that
area. Um, because there's so much going on in that space, we're not going to delve deeper into it.
Uh, we're going to focus on some other stuff, uh, um, particularly building tools for information
triangulation. Um, you, you know, knowing the, like when questions have been raised about what we're
reading, uh, and, uh, identifying and collecting those, that network of experts that you can use to
determine whether or not something is real. So a really simple example I want to include because
it's a project that was, um, the first one wiki scanner was a project that was done by somebody
in this community. It was a, it was a small focused project and it had interesting consequences.
So it shows that like you can do some hacking over a few weekends and make something useful.
Um, so wiki scanner, um, looked for anonymous edits to Wikipedia and checked the, um, you know,
like who is information associated with the IP address and identified situations where people
were editing Wikipedia anonymously from the IP blocks assigned to large organizations. And so you
could kind of see what narratives were being pushed in Wikipedia by certain large orgs. Um, there were
certainly interesting results that came out of that analysis. Um, by the way, it's mothballed.
Somebody else made a thing called wiki watchdog. The source code is available for that,
but it's also mothballed. So like, this is a thing anyone in this room could pick up and,
you know, you know, do something with, um, immediately. And there are interesting results
to be found. Um, sticking to the topic of Wikipedia briefly, um, the idea behind Wikipedia
is that, um, with enough eyes, all bugs are shallow, right? So, um, you know, everyone's reading
this article and they're removing, uh, fake things from it. And so we get this objective truth.
Um, the problem with it is that when you visit a page, it might have been edited two seconds ago
by someone pushing an agenda and you can't really tell, right? So, um, I, I wrote a paper in 2006
in which I suggested that we highlight passages that are new so that when you're reading the article,
you can kind of tell which passages are less trustworthy because they haven't gone through
that editing process. Um, there were some academics that built a thing called wiki trust,
uh, which like implemented a very similar set of ideas and, uh, with some added analytics underneath
and, uh, it was usable for a while. So this is kind of an interesting thing. And this is also
mothballed. I, I believe in this set of ideas, uh, and there's work to be done in this area.
Uh, um, but I'm also, I like the high level concept that maybe my computer should tell me when I should
be careful about what I'm reading. Um, because, um, some objective, objective criteria suggests that,
uh, you know, this information might not be reliable, right? So I want to return to some of the, like,
core information resources that inspired the creation of the web. Um, because, uh, there's a
whole lot of ideas in here for things that we don't have today. Um, and it's a way of seeing the negative
space in the internet. Um, so the first resource here is a paper called As We May Think, uh, by Vannevar
Bush. So Vannevar Bush was the guy that ran all defense-related research during World War II. Um,
so he was an incredibly important guy. He wrote a paper in the 40s, um, which might be the most
important academic paper of the 20th century. Um, and it's, it's, it's about using microfiche
to create, um, to link documents together and create knowledge management systems. This guy's
got a camera on his head because he imagined he wanted to take screenshots of books he was reading.
And so you imagine having a camera in his head so he could screenshot, uh, documents, right? Um,
so it's, it's weird tech, um, but the ideas are super interesting. And he, um, you know,
had a lot of influence over to other people who are incredibly important. One is Douglas Engelbart.
Douglas Engelbart gave the mother of all demos in which, like, he demonstrated things like mice and
modems, um, uh, at Stanford University in the 60s. Um, he wrote a book about augmenting human
intelligence. And one of his sort of precepts is that computers will always augment human
intelligence better than they can replace it. And that is an idea that I think is, it makes sense
to remember these days, given everything that's going on with AI. And also I think social media
companies sometimes forget that the purpose of computers is to make us smarter. If you forget that,
you will make the wrong things. Um, so, uh, Ted Nelson is the guy who coined the term hypertext.
Um, he did, so the, it first appears in print in this book, which is called Computer Lib.
It's an extra large book that's hand written and illustrated in like a zine. Uh, it's a really
interesting, uh, document. Um, and you can buy a copy of it. Um, they've, they've got it printed again.
Um, a scholarly read through these three resources will reveal a whole lot of ideas that these people
had for things that the internet should do that the internet does not yet do. Um, late in Engelbart's
life, uh, um, he was sort of frustrated with the web and like felt like it didn't achieve its potential.
And Ted Nelson is still ranting about that on YouTube and you can find him, uh, doing so. Um,
so if we look at some of the stuff that these guys did, so, um, Xanadu is Nelson's project. Um,
hyperscope is something that the Douglas Engelbart Institute has been working on to try to implement
some of his ideas. There's a bunch of things in there that we don't have today. And it's interesting
to think, or maybe we do, but only in narrow, limited context, right? So it's interesting to
think about like, what would it take to make this a part of the internet? Um, uh, you know,
what, why don't we have it? Um, in the context of deception and counter deception, I'm particularly
interested in backlinks. All these guys wanted backlinks. So a web document can link out to another
document from a phrase. Um, in their systems also, when you're reading a document, you can see which
people link in and what phrases they link into. So you can see if someone built upon the ideas that
you're reading. Um, and you can also see if someone is criticizing the thing that you're reading.
Why don't we have that in the internet? Um, well, the answer is that it's a content moderation
problem. If anybody could, uh, annotate a webpage in any way that they wanted, then we'd have a bunch
of spam and abuse, right? Um, and so we need to think about the tools that we've built for managing
content moderation and see if we can couple them to, uh, back linking in order to create something
that's useful. There've been some interesting projects over the years that played around with
back linking either in narrow context or, you know, to varying degrees of success. Um, you know,
I think a key thing here is that all opinions are not created equal. Um, there are certain opinions.
Again, I've curated my experts whose opinions that matter to me. Those are the ones that I want to see.
So when I choose to follow somebody on social media, I'm indicating that, that what they say
is relevant to me, even if I don't agree with it, right? So I've selected that group. It would be
useful to me if I could see anytime I'm viewing a document, if those people have ever commented on it,
right? I I'd love to build that. The, a challenge that we face is that in order to build and innovate
on top of our social media systems, we need systems that are technically, financially, and culturally
open to innovation. Um, and unfortunately, many of the systems that we collectively use, like fail at
one or more of those criteria. And therefore it makes it difficult for people to create new things
that sit on top of them. And we need to create new things in order to make the internet better.
Um, so I've, I've talked about curating collections of experts. Uh, there are some interesting protocols
that exist that people have built over time, uh, for, um, for endorsing people. Like I really like
LinkedIn endorsements, not as a tool, but as a concept. Um, you know, you can say, I follow this
person and this is why, right? Um, uh, but unfortunately that's a closed system. Um, some of these things
out there aren't machine readable. Uh, you know, I, I think it would be cool if I, when I'm following somebody on,
on social media, if I could say why, and that endorsement sticks to them if they accept it.
Uh, and, uh, um, as long as I continue to follow them. And if we did that, then I could do this.
I could say, who do the people I follow follow who are endorsed for this particular thing, like law.
And I could get back a list of a bunch of people who are endorsed for this particular topic. And I
can ask, what are these people saying about this thing? Right? It gives me the ability to sort of like,
um, you know, identify the reputation of people on the internet and utilize that in constructive
ways to help me figure out what's real. Um, so I, I think there are lots of, there's lots of work to
be done in that space. There's lots of like open potential there. Uh, another area is LLMs. Um,
you know, LLMs have some built-in biases. There's challenges dealing with them. They hallucinate. So
it's not, um, there's lots of caveats to what I'm saying here. And I know what they are. Um,
but at the same time, it is a dispassionate computer, right? So can it go out and find those
missing facts for me that aren't included in a narrative by reading all the things, right? Uh,
and, and dispassionately present them to me. Um, that's a potentially interesting thought. Um,
and I think it's harder than it sounds. Um, but you know, there are other things we could do with LLMs. Uh,
I, uh, you know, one of the things is that, that I mentioned on the last page is that there are,
um, like if you're a university professor at an accredited university in a particular subject area,
um, your university website indicates that like this guy is the professor of this, right? But then
your social media profile, there's, it's not machine readable. Your university website is not
machine readable. I can't tell, uh, you know, um, automatically that you have this expertise. But an LLM
could go read every university website, find all the professors, find all their social media profiles,
and created a machine readable, uh, you know, system that allows me to, again, ask this question,
who knows lots about economics? Here they are, right? Um, and so I think that those are interesting
possibilities. LLMs are really good at turning human unstructured information into like very
structured information that's machine readable and we can do things with it. Greg?
Greg? So, so beyond these technical solutions, we, we have to think like how can we scale this,
um, so for humans, right? Uh, one is the idea of teaching media literacy in schools to try and have
a better informed consumers of information. Uh, media literacy now who I have highlighted here,
they have a nice depth of resources from K through 12, uh, for, for classroom instruction. And the,
the other idea is to emphasize critical thinking both in school and in our day-to-day lives.
And this is from, and I can't quite see it, Justin Wright, um, the idea there are certain questions we
ask, uh, that we ask and help us probe through and see through deception. One is who benefits,
right? Where did this originate? There's a, there are 48 questions here and they're very thought-provoking
and useful. Yeah. So, so we've, um, put a bunch of, uh, references in the, the slides, uh, you know,
which are like, look, look at this from a bunch of different perspectives. I'm sure that there are
ideas that you have that relate to maybe the principles that we've articulated that are different
from the ones that I've come up with. Um, and that's what I'm excited about and why I wanted to
talk to all of you about this. So, um, hopefully, uh, you know, somebody out there has been maybe
inspired to work on something. Uh, and, um, uh, you know, there's, there's lots more to read about
all of this stuff. Um, you know, these are our email addresses. Uh, you know, we'd love to hear
from you. Um, we're going to be hanging out outside after the, um, talk is over. So if you want to talk
to us or poke at some of the things we're saying, we'd love to talk to you more. Um, and, uh, again,
I really appreciate your time and attention this morning, uh, um, and the fact that you made it here
on the last day of, uh, of DEF CON for a morning session. Thank you.
