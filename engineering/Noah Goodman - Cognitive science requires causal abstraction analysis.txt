um great um so um i'm gonna get to causality uh after a few minutes i promise but um i want to
start with the story um and this story is the result of me kind of wrestling with what uh you
know the things that are happening in machine learning uh mean for me as a cognitive psychologist
in the field of cognitive psychology and the the feeling that a lot of the dialogue is much too
anchored on this exact moment of the technology rather than the the general things that are
happening okay so let's consider a thought experiment it's 2030. um the the very many labs
consortium has completed the the psych 50 project which is this intense five-year effort to collect
all of the experiments done in the last 50 years um they they did this by using large language models
to read all the method sections generate javascript code uh and items and then uh they uh uh crowdsource
replications because of all of the out of sort out of work white collar workers due to large language
models um at the same time gpt-8 was just released by the now properly named closed ai
and uh so these this consortium runs all the experiments from the psych 50 corpus the last
50 years of psychology on gpt-8 instructed to think like a human and discovers there's this amazing
convergence that the the the answers provided by the uh you know super language model agree with the
human uh the human population and and even better if you ask gpt-8 to think like a whole bunch of humans
you get a population of responses that reflect the the human population so of course the new york
times runs an article the end of psychology uh and so then the field uh has to respond to this um
how does the the field of psychology respond well add 10 more years so 10 years later there's a
retrospective volume where there's a nice historical analysis uh uh and it identifies three main uh viewpoints
that emerged over the those 10 years in psychology applicationism explanationism and mechanismism
so uh the applicationists um they really lean into the view that the primary value of
behavioral science is social impact and so when they discover that there are now ai surrogates that let
them predict uh how humans will respond in very complicated uh real world situations they think this is
great um and they just say that's all we need in order to design interventions and and use uh
psychology the way we always wanted uh including the uh you know uh now uh 2040 famous example uh where
they intervened on the beliefs of a small midwestern city to make them think that cricket protein was
actually just as good as other kinds uh um and so the result is that for applicationists theory takes
a back seat it isn't what's important uh in order to achieve outcomes okay then there's another group
uh of psychologists the explanationists and the explanationists uh they hold fast to the idea that
the goal of psychology is uh achieving an understanding of human behavior um they reject that gpt-8 itself
is an explanation because they don't understand you can't understand it it's too big and complicated
um and then they fracture a little bit there's one group that uh of explanationists that still
believes that models could be explanations they just need to be models that are transparent enough
for humans to understand um and so then uh these uh neo new basians maybe that's me and me and todd
uh you know we build some beautiful uh uh basian complex basian models that capture a small fragment
of what uh gpt-8 can do there's another group of explanationists who say no i can't understand your
your uh complicated basian models um only models uh only theories that can be expressed as uh clear
verbal theories are uh are okay for explaining behavior um and so overall this this uh group of psychologists
says to themselves well they implicitly say to themselves if explanations uh aren't there to increase
predictive or manipulative power um what is it they're good for well they're increasingly good
as a as a kind of uh artistic uh value this is you know what fire event said back in against method
anything goes and so it's okay it's defined by the community what counts as a satisfying explanation
and that's what we're doing um the most radical of these sub-communities is uh the reddit uh uh le5
site community that insists that the only theories that can be considered must be understandable
by school children and then there's a final group of psychologists um these are uh the mechanismists
and they actually endorse the idea that that gpt uh means that behavioral psychology is a completed
endeavor and so they they lean into cognitive neuroscience they double down and say great what we
need to do is figure out the circuits in between the input and the output of behavior that's what we're
doing um this goes pretty well uh until uh it's discovered that the uh internal states of gpa while
they correlate well with neural behavior right at a population level you can correlate them with
what's going on in neurons nobody can explain what gpa is doing in terms of understandable circuits anyhow
and so they they just collect a lot of data and hope that something will change at some point
okay so um is there hope um that was my pessimistic imagination of where we could find ourselves um i
don't think it's it's so bad um but i do think that we need to think deeply about the kinds of uh rules
of the game and the tools that we're using uh now in order to send ourselves down to a better uh a better
path to the future um so you know explanationism uh requires i think in order to make it useful uh uh
as as a science constructively specifying some space of acceptable explanations not in anything goes
but something that we think is uh sort of normatively uh normatively uh acceptable or justified or
something like that similarly mechanismism uh requires some kind of tools for finding causal
structure in very complicated systems complicated networks um you know contra my pessimist pessimism
in the story on the last slide um and so one uh possibility that um really this was brought to my
attention by uh thomas icard and a lot of what i'm going to say in the next three slides is uh thanks to
thomas and a bunch of great collaborators um you know maybe it's the case that simpler causal models
actually can in a very formal way serve as explanations of more complex ones and then what
we have to do is figure out how to how to uh describe that relation relationship in a precise way
so this is the idea of of causal abstraction um we want a mathematical theory of one when one
causal system is an abstraction uh faithful full abstraction of another um the good news is that
uh people smarter than me have been thinking about this for uh you know what's it been uh maybe five
or six or seven years now um and the basic idea alighting a lot of mathematical details and differences
is that if you have these two causal models you want there to be some kind of map from the variables of
the abstract model into the variables of the uh the low level high low i'll call them high level low
level model um that has the property that mapping back and forth between these uh these values commutes
with interventions that you can do at the high level and the low level um so i'm going to illustrate
that i'm not going to write down the math for you all but i'm going to walk through uh kind of the
idea in some simple more specific uh cases um so imagine that we have a simple algebra one plus two
times three and the output is nine and we train a neural net and it can do this and we say hooray it
knows arithmetic um then what we want to do is we want to understand whether it knows arithmetic in the
sense that it is carrying out the uh the algorithmic process of first adding one plus two and then
multiplying three to get the answer so we have a high level causal model do those uh arithmetic steps
and a low level causal model whatever is in this neural net um and the question is are they alignable
can we find the right kind of function that relates them uh in this way okay so what we want um hinges on
the the fact that causal systems um you know at least have to uh i want to avoid saying they're fully
characterized by their interventions after the talk this morning but at least interventions are
a pretty important part that that's a that's fair right maybe yeah okay um so um if we take the high
level causal model on the right if we reach in and change the intermediate variable three to something
else uh oops sorry we're trying to figure out where that intermediate variable might live on on the left
okay so um how would we know what does it mean to have such an alignment um well the idea is that um
we will have such an alignment between something in the low level model and something in the high
level model um if it's a case that the interventions on the high level model have a corresponding
intervention on the low level model that does the same thing uh uh changes the relationship between
input and output in the same way so let me illustrate that um you can imagine that we reach in um and we
change the value from three to five right so it goes from uh sorry five to three uh i can i can do
arithmetic on this um we change the value from five to three and now the answer on the output changes from
twenty to fifteen so we have this counterfactual output um this is the idea that the kind of
interventions we're interested in uh or that we're going to focus on at least are these interchange
interventions um this was an idea that um thomas and atticus geiger and chris potts um came up with
that i think is is super brilliant because it takes the very abstract notion of all of the interventions
and gives us gives us a really concrete set of cases to look at so the cases we're interested in
are the ones where we just record uh the value of the intermediate uh variable from uh one
case and we go and we oops don't have that animated we plug it in to uh to another case
and so the nice thing about these interchange interventions is that they unlike you know say
i could go in and say oh five i'll change it to six but that really depends on these numbers being
numbers interchange interventions are perfectly well defined for arbitrary models that have arbitrary
contents of the variables so here i can go in and i can say okay well if that variable is doing the
same thing it's playing the same role then if i record the intermediate uh activations when i put in
two plus three um and i splice them in over here when i had put in one plus two it will change the answer to
the you know to say 15. um and furthermore with a little bit of math to make this precise i can say
the uh the the extent to which i get the correct counterfactual answer from doing that that interchange
intervention is a measure of uh how close the alignment is to having worked um what we call the
iia or alignment score okay um so the basic picture here is that if i've hypothesized a high-level causal
model and i have a target low-level causal model i next have to come up with an alignment between the
variables in the high-level model and the low-level model and then i could do this swapping this
interchange intervention to see whether the variable over here plays the same causal role as the variable
over there parentheses caveats with respect to the inputs that we're using to test this which
we can talk about later um okay um that's straightforward except for the fact that when i
just look at this neural net i don't really have any idea where that that intermediate uh you know sum of
two variables should live right big pile of neural net activations i don't know what it does um so a lot
of the interesting work that's been happening in the collective of causal reasoning people at
stanford the last couple of years um and really spearheaded by atticus geiger who's a brilliant
student who just graduated has been finding ways to much more effectively and efficiently search for
and find the the populations in the low level that implement variables in a high-level model if they're
there of course um so the the main one that i want to tell you about today um hinges on the idea that
we first need to move from thinking in a localist way about the representations in the network to a
distributed way so what do i mean by that well rather than saying
one plus two variable i'm going to say this is a vector space
and i'm interested in whether there's some direction some subspace of that vector space
that represents that information um that's the classic uh kind of pdp connectionist idea that
you know important conceptual variables won't be necessarily represented in say single neurons
they'll be represented in some superposition of neurons some direction in the vector space
um okay so given that that's the case there's an interesting uh sort of uh fact from linear algebra
which is that um you can um find you you can basically find vectors by leaving the vectors
where they are and changing the basis um and so the the basic idea is what we're going to do to search
for a variable that could align is we're going to um pick out some dimensionality of the subspace
or relax that in a minute and then we will try to come up with a rotation that's the next slide
we will try to come up with a rotation such that when you do it um do the swap over say the first k
dimensions and then undo the swap um you have uh you have the right counterfactual behavior um
now the idea here is basically all we're actually doing is allowing ourselves to use the you know modern
tools of gradient based uh optimization in order to look for a subspace of all of the neurons that
represents that high-level variable we're not actually changing the the neural net or the high-level model
in any way okay and you guys should interrupt me if this is totally unclear um good so um good we go
forth and we do that um let me give you an example um so there's this uh very simple computation that's
been kicking around uh the psychology especially developmental psychology world for some decades
called hierarchical equality you might recognize it as one of gary marcus's favorite reasons that neural
nets can't do stuff um and hierarchical equality is is a really kind of cool but simple uh computation
it says you're going to have four inputs uh w x y and z and you first check whether the first two
inputs are the same and you get you know true or false you check whether the second two inputs are
the same true or false and then you check whether those two variables are the same right both true or
both false um so what you can do is you can train a neural net you can just say great i'm going to
generate a whole lot of data but it's consistent with that rule so you know one one two two yeah that's true
uh two one three three that's false because these two are different but these two are the same
and so on okay um so then you can train a neural net and that's generate data train the neural net
um you get a neural net um it turns out that this was not something that i expected but it turns out
that actually even pretty small neural nets not even big old transformers can solve this task perfectly
well um and can even generalize um but you have the question of whether those neural nets um are
carrying out that computation just by some kind of you know approximate embedded dynamical systemy cool
thing or whether they're actually implementing the symbolic computation right um this is a question that
i think had worried me for a long time when can we say a neural net actually implements a symbolic model
and before this line of work that sort of thomas showed me i had no way of answering that question
or thinking about it so we set out to use uh this causal abstraction uh method uh to analyze the neural
net that we trained on this task um and what we did is we explored um three different possible high
level models so the first one is the one that actually you know we use to generate the data
two intermediate things uh the second one is one where you generate one intermediate value
and then just have a single function to combine the remaining things together
um and um the the third one is the one that actually uh doesn't have any intermediate variables
it does sort of a copy and then combine the critical thing about these is just that they
are three different uh computation you know functionally equivalent ways to compute the same thing um
okay and then we went and we looked for uh for the the the variables and uh in the neural net and
asked you know are these causally represented is there a uh you know a distributed representation of
each of these variables that does the right thing when you swap out activations from other examples
um and the result um which is pretty was pretty surprising to me is that yes in fact um uh this
is varying the dimensionality of the subspace so how much room do you give this variable to
to use um and as long as you give the variable enough room to use you can find it exactly represented
so um in the details don't matter very much but basically um somewhere in layer one of this uh fully
connected network you actually find an exact representation of the two intermediate variables
being computed this is a really strong thing to be able to say right it's saying that this neural net
that if you look at it is just a it's a dynamical system it's a pile of weights in some kind of useful
mathematically precise sense is implementing the symbolic computation um it's also worth saying maybe two
things two other things which is that this isn't uh like there's many ways to implement that computation
uh that are not implemented in the neural net so those and and others um and also it's it's
distributed if you went and look started looking for a localist representation of those two variables
you wouldn't find it we sort of do two different versions of that on the on the bottom um and so you
know it's doing the symbolic thing in the distributed way i guess basically like jay mcclelland has been
saying for a really long time um but the only way to find it is by knowing the the causal relationships
that you're looking for okay um and yes thank you now now what did we settle on what does that mean
um okay um i want to then describe taking this the same basic idea um now we want to scale it up
like obviously what we'd like to do is we'd like to go analyze gpt8 um or at least a reasonably large
transformer um the first thing we have to do that i'll go through we'll just kind of skip over is we
have to extend the algorithm just a little bit to allow it to search more efficiently and the critical
thing here is we we don't want to have to determine the size of the of the representation ahead of
time so we do this thing where we sort of relax the boundaries so we can have uh learn filters and then
we can take derivatives and it can figure out how much space it wants um it's technically useful but
not that important um and then we have a new name we call it boundless distributed abstraction search
um and cool then we go and we say great um when we were doing this the um the the best open source
uh transformer that we could get our hands on was this one called uh alpaca and the biggest one we
could fit on our uh gpus with seven billion parameters which is still big enough to actually do
things right like you can do some things not by the way as many things as i was hoping it took us a
little while to find a task that it was like sufficiently good at to be worth analyzing um
but we found this task where we say uh now in kind of chat mode right please say yes only if it costs
between 350 and 550 dollars otherwise no you give a price for four dollars and ten cents uh and it says
yes okay so there's the task and then we can ask great how is the transformer carrying out that
extremely simple computation um and so the the high level model that we want to explore um you know
a kind of natural hypothesis is that what it's doing is it is sort of computing the intermediate
variables of comparing the price that we give it to the lower bound comparing it to the upper bound
and then you know combining those taking the conjunction to to figure out if it counts okay
okay um okay there are alternatives um there's like other ways you could do this computation uh
so like i kind of like this one as an alternative which is not crazy which is you um you compute the
midpoint of the of the interval and then you measure the absolute distance to the midpoint it's like a
totally fine way you could do this right and importantly it's the same input output function it only
differs in the mechanism and the causal kind of steps you go through okay um so this is a big old heat
map these these four correspond to looking for the causal representation of those four different causal
models um and the the basic finding here so the first thing to say which is annoying and requires a lot
of further thought is that the task performance alpaca is only gets 85 of these right even though it's very
simple and so kind of the ceiling counterfactual performance is around that so you know when we
see 0.85.88 somewhere in that range what we're seeing is like the best you could do at causally
representing these things um this is basically a heat map showing the layers of the alpaca model and
then the tokens of input and the number is the intervention accuracy score of looking for the
representations of the causal variables the high level causal variables somewhere in that layer
somewhere in that part of the the transformer um okay so the the first result is that it does look like
the transformer is representing this computation based on computing the boundaries and putting them
together and not something like my uh favorite algorithm the midpoint distance computation okay
um the second thing which i think is even more interesting is that those so now we now we have
this set of directions these representations which are some subspaces of the transformer activations and
we can ask okay are those still the causally relevant directions if we change the task so if we generalize
a little bit are they still the representations because it's possible that the transformer as soon as you
change anything it puts its information in some completely different place so for instance we can
take our basic task and we can add a random prefix that could in principle interfere in all sorts of
ways with the computation um and and the result is that um it's uh those representations are relatively
stable to this change to adding random prefixes um they're also uh um uh we can also uh sort of
generalize in a very minimal sense generalize the task from saying yes or no to say true or false
and you could imagine like you know transformers do all sorts of stuff um but it turns out those
representations are conserved the same representations are still how the transformer computes the answers to
this this slight twist um okay um what's the takeaway what are my thoughts well first of all some things
that we still need we need much better tools for finding the abstract computations here i posit them
and we go look to see if they align um but that is painful um we need much better math for thinking
about approximate abstractions um and thinking about like kind of intermediate interchange intervention
accuracy um and and then there's kind of deeper questions um like um when is it that a causal
abstraction feels like it yields a useful understanding of the low level system is it anyone does it have
to be simple does it have to be something um uh and then maybe just the last thing i'll mention is uh
the the you know the question that i brought up in my uh story earlier which is we still don't know
it's a hypothesis we have to explore but it could turn out that very complicated uh large language
model systems are just not coverable by a set of these kind of understandable causal abstractions
and so i think that's something we really need to think pretty hard about it thanks
um so i'm going to be honest i did not follow everything about that but i was very interested in
the math part especially about the alignment that you're talking about because you kind of defined
it as essentially some sort of mapping and i just want to ask whether you have an intuition as to is
this mapping or are you talking about problems oh um uh it's a commutative diagram okay um it is not
a homomorphism or an isomorphism because you expect to lose structure okay uh when you go from the low
level model to the high level model so i don't remember what that makes it do you know what that
makes it it can be a homomorphism of a particular structure to another structure where that's where
the algebraic operation whatever binary operation is um composition of interventions yeah
i guess one one thing i'll say which is not you know uh directly responding but um
we very broadly speaking are trying to figure out a more algebraic notion of this that involves
looking at interventional algebras and how they they interact and i think it's closer to the
question you're asking so you you don't really have a next answer as to what kind of alignment
is this but you just have an input in the first sort of alignment and you don't really know like
no no no i mean there's a very there's a very definite the papers you know kind of laid out there
is a very definite uh property that these mappings have to have it's a set of mappings that have a
certain community community property um which basically says that with respect to the interventions
you're interested in you can get the causal effects the predict causal the counterfactual effects
either at the low level or at the high level and they'll agree
so uh since we just had a question in the weeds i'll ask another um i like the idea about aligning
high level instructions and all other ones by aligning vectors in space but that seems to require that
there's a definition of what it means to have a vector representing something to the higher space
i can't imagine how that would work in recurrent problem maybe so even in a uh the time you have
to feed forward about what you could talk about it it's not obvious to me how you define direction
clearly but maybe there is one in the low level space or the high level space no i don't know oh the
high level space you don't have to have vectors so this is the beauty of interchange intervention
so the high level space you have some values of the variables you record a value you swap it in
for a different input um that might not be a vector itself the low level model because you know here i in
the yeah in the in these applications i'm assuming that it's a vector so that there's a notion of subspace
but the critical property is just that whatever it is it's a thing i can record from one input
and splice in when i'm processing another input okay um recurrent models are kind of hard for their
own reason as long as you have a particular time at which you think the variable is to be represented
everything is straightforward if you don't know when it's complicated
