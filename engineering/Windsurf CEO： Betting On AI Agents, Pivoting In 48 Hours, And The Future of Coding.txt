One of the things that I think is true for any startup is you have to keep proving yourself.
Every single insight that we have is a depreciating insight. You look at a company like
NVIDIA. If NVIDIA doesn't innovate in the next two years, AMD will be on their case. That's why
I'm completely okay with a lot of our insights being wrong. If we don't continually have insights
that we are executing on, we are just slowly dying. This notion of just a developer is probably
going to broaden out to what's called a builder. And I think everyone is going to be a builder.
I think software is going to be this very, very democratized thing.
Welcome back to another episode of The Light Cone. Today, we've got a real treat. We have
the co-founder and CEO of WindSurf, one of the people who literally brought Vibe Coding into
existence. Varun, thanks for joining us.
Thanks for having me, guys.
Where is WindSurf now? All of us intuitively know, well, we use it, but how big is it now?
Where is it?
Yeah. So the product has had well over a million developers kind of use the product. It has
hundreds of thousands of daily active users right now. It's being used for all sorts of things,
from modifying large code bases to building apps extremely quickly, zero to one. And we're super
excited to see where the technology is going.
Let's get to the brass tacks. How did you get started?
The company actually started four years ago. We didn't start as WindSurf. We actually started as
a company called ExaFunction. We were a GPU virtualization company at the time. Previously,
me and my co-founder had worked on autonomous vehicles and AR VR. And we believe deep learning
was going to transform many, many industries from financial services to defense to healthcare.
You were right.
Yeah. We might have timed it wrong though. Ultimately, we built a system to make it easier
to run these deep learning workloads. And similar to what VMware does for computers and CPUs,
we did that for GPUs. The middle of 2022 rolled around though. And what happened at the time was
we were managing upwards of 10,000 GPUs for a handful of companies. And we had made it to a couple
million in revenue. But the transformer became very popular with these models like Text DaVinci from
OpenAI. And we felt that that was going to fundamentally disrupt the business we had or
that small business that we had at the time, because we felt that everyone was going to run
these transformer type models. And in a world in which everyone was going to run one type of model
architecture, transformers, we thought if we were a GPU infrastructure provider, we would get
commoditized, right? If everyone's going to do the same thing, what is our alpha going to be? So at the
time we basically said, Hey, could we take our technology and wholesale pivot the company to do
something else? And that's scary. Yeah, that was that was a bet the company moment. We did it within
a weekend. So, you know, weekend, me and my co-founder had a conversation along the lines of,
I don't think this is going to work. We don't think we know how to scale this company. And at the time,
we were early adopters of GitHub Copilot. We told the rest of the company on Monday,
and everyone started working on Codium, which is the extension product, the Monday immediately after.
I'm just curious to like dig into this pivot story, because it's like pretty rare,
actually, to hear like the details of a pivot, especially a late stage pivot, like at the time
that you guys decided to pivot to Codium, how far along was the company? How many people?
One of the things about the company was, I guess, like we tried to embrace a lot of the YC
sort of analogies here of, you know, ramen profitability, all these other sort of key insights.
We were only a team of eight people at the time, even though we were making a couple million in
revenue. So we were kind of like free cash flow positive. It was the peak of zero interest rate at that
time. So the company was a year and a half old, we had raised somehow magically $28 million of cash
at the time. And I think the big point here in our minds was, it doesn't matter if we're doing kind
of well now, if we didn't know how to scale it, we kind of need to change things like really fast.
I guess the thing that's remarkable is when you started the company, you had this thesis where you
were betting that a lot of companies were going to build their own custom deep learning pipelines to
train birds, right? That was like the thing that was working. But in 2022, you saw the hockey stick
shift that suddenly there would be one model that would rule them all. So you were foreshadowing a
lot of the future. And there's a lot of it that came from that conviction. So I'm curious, what were
those signs? Because you had to be really embedded into it too. We're making already seven figures,
you could have raised a Series A, you're like, we're going to throw that all that away and burn it.
Yeah. So actually, even crazy enough, we had raised our Series A at that time.
Okay, you've raised it.
So whether or not we should have been able to is a different question. No, I think you're totally
right. I think one of the things that was happening at the time was we were working with, largely
speaking, these autonomous vehicle companies, because they had the largest sort of deep learning
workloads at the time. And we were sort of seeing, hey, that workload is growing and it's large.
But we were betting fundamentally that the other workloads, which were these other natural language
workloads, kind of like these workloads in these other industries like financial services,
healthcare would take off. But I think once we saw these, these models, these generative models
handle so many of the use cases, right? Maybe an example is in the past, you would train a BERT
model to actually go out and do sentiment classification. But very quickly, when we tried
even a bad version of GPT-3, like the very old version, we were like, this is going to kill
sentiment classification. There's no reason why anyone is going to train a very custom model anymore for this
task. I think we saw the writing on the wall that our hypothesis was just wrong, right? And that's
like one of those things, right? You go in with some thesis on where you believe the space is going
to go, but if your hypothesis is wrong and information on the ground changes, you have to change really
fast. So then what did you decide to do? So it's like, you decided, okay, we're going to pivot. And when
we work with founders, that's kind of stage one. So you're not half foot in, half foot out. So you had
that conviction, we need to try something out. How do you figure out what was going to be the next step?
I think we needed to pick something that actually everyone at the company was going to be excited
about. I think if we had picked something that was what we thought could be valuable,
but people were not excited about, ultimately we would fail. We'd fail immediately. We came
with an opinion in its stance of, we were early adopters of a product called GitHub Copilot.
We thought that that was the tip of the iceberg on where the technology could go. Obviously,
everyone at the company was a developer. Dev tool companies, generally speaking, usually don't do that
well. And in the past, they have not. But hey, when you have no other options, it's a very easy
decision, right? You're going to be a zero with a high probability anyways. You might as well pick
something that you think could be valuable and everyone's going to be motivated to work on.
Everyone's forgot this now, it feels like, because GitHub Copilot's in the background. But
at that particular moment, it felt inevitable that Copilot was going to win, right? It just
had everything, like the GitHub connection, Microsoft distribution.
Open AI. Open AI, yeah. It seemed like no one could compete. So how did you have the bravery to
be like, oh yeah, we can totally crush Copilot? Yeah. So this is where there's an irrational
optimism piece. I've said this before to the company, but I think startups require two distinct
beliefs and they actually run counter to each other. You need this irrational optimism because
if you don't have the optimism, you just won't do anything. You're just a pessimist and a skeptic and
those people don't really accomplish anything in life. And you need uncompromising realism,
which is that when the facts change, you actually change your mind, right? And that's a very hard
thing to do both because the thing that makes you succeed through irrational optimism is exactly
opposite to the things that allow you to be a very realistic company. So irrational optimism,
you basically said, hey, we know how to run and train models ourselves. We actually trained the
first autocomplete models ourselves and ran it on our product, gave it out for free. I don't think
we had the exact roadmap on where this was going, but we just felt there was a lot more to do here.
And if we couldn't do it, then I guess we'd die, but we might as well bet that we could do it.
Were your early versions better than GitHub Copilot at the time?
So our earliest version that we shipped out was materially worse than GitHub Copilot. The only
difference was it was free. We built a VS Code extension. After Pivot, within I think two months,
we had shipped the product and given it out to Hacker News, posted something on Hacker News.
And we built that out. It was missing a lot of key features. The model that we were running was
like an open source model that was not nearly good as the model that GitHub Copilot was running.
Very quickly then our training infrastructure got better. So we actually went out and trained our
own models based on the task. And then suddenly it actually got capabilities that even GitHub
Copilot did within two months. Basic capabilities. Now we'd find this hilarious that it was even
state of the art, but our model could actually fill in the middle of code. So when you're writing code,
you're not only just adding code at the end of your cursor, but you're filling it in between two parts
between two parts of a line. And that code is very incomplete and looks nothing like the training
data of these original models. So we trained our models to make it actually very, very capable for
that use case. And that actually allowed us to pull ahead in terms of the quality latency.
We were able to control a lot of details within a couple of months. So the beginning of 2023,
I'd say the autocomplete capabilities were much better than what Copilot had.
Was that a totally new capability for you guys? Because you guys have been building
GPU infrastructure. It sounds like you basically hacked together the first version by taking an
off the shelf open source model, sticking it into like a VS code extension, and just kind of like
wiring the two to talk to each other. But then right after that, you had to train your own coding
model from scratch. And you guys have been following the transformer stuff, but you hadn't built it.
In order to do that, I assume you had to download all of GitHub and train a whole model from scratch.
Like how did you all figure out how to do that in like only two months?
Yeah. It's a great question. So first of all, when we ran the model ourselves,
the reason why we were able to run it and provide it for free is we actually had our own inference
runtime at the time. And that obviously came from the fact that we were originally a GPU virtualization
company to start with. So that enabled us to actually ship the V0 with an open source product
quite quickly. Immediately after that, you're totally right. We had never trained a model like this in the past,
but I think we hired people that were smart, capable, and excited to win. So we needed to
figure it out. There's no other option, right? Otherwise you die. Makes the decision making
really, really simple. So yeah, we had to figure out how do you get a lot of data? How do you do
this at scale? How do you clean this data? How do you make it so that it's actually capable of
handling this case where code is very incomplete? And we shipped a model like very, very quickly after
that. Wow. And you did all of that with like eight-ish people in two months. Yeah, that's right.
And then right after that, because you were running your own models, you started getting
interesting customers, right? Yeah. So basically what happened,
the product was free at the time. So we ended up getting a lot of developers using the product
across all the IDs. So VS Code, JetBrains, Eclipse, Vim. Companies started reaching out because they
wanted to not only, I guess, run the product in a secure way, they also wanted to personalize it to all the
private data inside the company. So very quickly afterwards in the next coming months, companies
like Dell, JP Morgan Chase started to become customers of our product. Now these companies
have like tens of thousands of developers on the product internally. But we started actually
making a lot of focus of the company, making sure that the product works on these very large code bases.
Some of these companies have code bases that are well over a hundred million lines of code,
right? And making sure that the suggestions are fast is one thing, but making sure it's actually
personalized to the code base and the environment that they have was almost a requirement at the
time. You did that pivot, you built it in two months, then shipped it. And within a couple of
months, you got these big logos. Yeah. So, I mean, obviously these companies take some time to
time to close, but pilots were starting within a, within like a couple months or a quarter after that.
You know, obviously we had no salespeople at the company. So like the founding team was just
trying to run as many pilots as possible to see what would ultimately work. At what point did you
expand beyond just the VS code extension into supporting all these other IDEs?
That was actually very, very soon afterwards.
Well, how did you think about that? Like, you know, there's like one argument that you could
make, which is like, there's lots of VS code developers. You had a tiny team. You could have
made the argument that I was just focused on building a great experience for VS code. You'd
only captured a tiny percentage of the market of all possible VS, VS, VS code developers. And that,
that's not what you did. You expanded like horizontally very quickly and build extensions for all those
IDEs. So why I think maybe the fundamental reason that, that we thought was quite critical
is if we were going to work with companies, companies have developers that write in many
languages. Uh, like for instance, uh, like, you know, a company like JP Morgan Chase might have
over half of their developers writing in Java. And for those developers, they are going to use
Jebrance and IntelliJ. IntelliJ is over 70 to 80% of all Java developers in the world currently use
IntelliJ right now. So we would just need to turn away a lot of companies. Like a lot of companies
would not be able to use us as the de facto solution. Uh, like we'd be one of many solutions
inside the company. Um, so because of that, we made the decision, but luckily because we made
the decision early enough, it changed the architecture of how we built the product out,
which is to say, we are not like building a separate version of the product for every single
IDE. We have a lot of shared infrastructure that actually lives on a per editor basis.
So it's actually a very, very small amount of code that actually needs to get written
to make sure we can support as many IDs as possible. Um, so this is one of those things that an
early decision that we made ended up making it much easier to make this transition.
How about the transition from Codium to Windsor?
At the time we, you know, now, now we're probably going to middle of 2023. We start
working with some very large enterprises within the next year. Like the business has gotten well
over sort of eight figures in, in, in revenue, um, just from these enterprises using the product.
And we have this like free individual product. But I think one of the things about this industry
that we all kind of know is the space moves really, really fast. And, uh, we basically
are always making bets on things that are not working, right? Actually, most of the bets we
make in the company don't work. And I'm excited, uh, when I'm like happy when we're, when we're,
let's say only 50% of the things we're doing are actually working, because I think when,
if a hundred percent of the things we're doing, uh, are working, um, I think like, it's a very bad sign
for us because it's probably like one of, one of maybe three things. Um, the first thing it, it,
it is, is like, Hey, we're not trying hard enough. Uh, right. That's, that's probably what it means.
The second thing is we somehow have a lot of hubris, right? And the hubris is like,
we believe everything we do is right. Even despite the facts that are, that are sort of on the ground.
And then, um, the, the sort of third key pieces here is we're not actually testing our hypotheses
in a way that like tells us where the future is going. We're not actually at the frontier of what
the capabilities and technology, um, ultimately is. So we believed actually in the very beginning
of last year that agents were going to be extremely huge. And we had prototypes of this in the
beginning of last year and they just didn't work, but there were different pieces we were building
that we felt were going to be important to making agents work, which is understanding large code
bases, understanding the intent of developers, making edits on the code base really, really quickly.
And we had all these pieces. The thing we didn't have as a model that was capable of calling these
tools efficiently enough. And then obviously in the middle of last year, that completely changed with the
advent of like Sonnet, um, three, five. Right. And with that, we basically said, okay, we now have
these agentic capabilities, but the ceiling of what is we can show to our developers on VS code was limited.
We were not able to provide a good enough experience. And we thought what was going to happen is
developers would spend way more time, not writing software, but reviewing software that the AI was
going to put out. We're, I think a technology company at heart. And I, you know, I think we are, we are a product
company, but I think the product serves the technology, which is to say, we want to make the
product as good as possible to make it so that people can experience the technology. Right. And
we felt that we were, you know, with VS code, we were not able to do that. So the middle of last year,
we decided, Hey, like we need to actually go out and actually have our own ID out there. Um, so that's
what, that's what triggered actually creating windsurf. The way that you did that was you forked VS code
forward to VS code. Was that a whole new set of capabilities that you guys had to learn,
like basically how to like develop on this VS code code base that I'm sure is super complicated.
Yeah. We needed to figure that out. Uh, that was one, once again, another thing where, uh,
we ended up shipping windsurf out in, uh, within, uh, less than three months of starting the project.
Uh, that's when we, we shipped it out across all operating systems.
Wow. And what, what happened? Like was, did it like take off immediately or was it unnoticed for,
for a long time? It took off pretty quickly. I would say, um, I think the speed at which it took
off among early adopters was, was quite high. There were obviously some very rough edges,
and this is like one of those things where, you know, because of the rough edges, obviously people
started coming and leaving the platform fairly quickly. But what we saw was like,
as we improve the capabilities of the agent, as we improve the capabilities of the passive experience,
even the passive top experience has made massive leaps. Um, in the last like couple months,
we started realizing that not only were people talking about the product more and more, uh,
people were also staying on the product, uh, more and more at higher rate.
How many people worked on shipping windsurf and it was done in a period of one or two months?
A couple months. So yeah, like less than three months. Um, this was another, I wouldn't say
it's a bet the company moment because it's not a fundamentally different, uh, sort of paradigm
compared to like moving from a GPU virtualization product to, uh, to an, uh, like an AI code product.
But yeah, it was, uh, anyone that could work on it needed to kind of like drop what they were working
on in the past and work on it immediately. And at that time, how, how big, uh, were you guys?
The engineering team was probably still less than 25 people. Wow. This is crazy.
Interestingly, our company actually from an employee standpoint actually didn't have that few people.
We actually had a fairly large go-to-market team because it's in the AI space. One thing that's like
a little bit weird about our company compared to most of their companies is we have a fairly large
go-to-market team. Like we were selling our product to the largest fortune 500 companies.
It's very hard to do that purely by letting them swipe a credit card. Uh, you need a lot of support.
You need to make sure that the, that the technology is like getting adopted properly. Um, which is very
different than just give the people the product and see it grow, um, effectively. So from an
engineering standpoint, we've always run fairly lean, uh, but because of the market interest,
we've always had a lot of people on go-to-market actually. Who are like sort of the ideal people
to go into that function? Is it, you know, really good engineers who want to be forward deployed?
Yeah. We, so we have two components of it. We have account executives. So these are folks that
in general, we, we try to find people that are very curious and excited about the capabilities.
In fact, people that would use windsurf in their free time, because they're, they're providing the
product to leaders who also love software and technology, right? So if they're, if they're just
completely just unaware of the technology, they're not going to be helpful. And then we also have these
deployed engineer, uh, like sort of roles, uh, similar to what you said, uh, that get their hands really
dirty with the technology and make sure that our customers get the most value from the, from the
technology. I mean, the wild thing is, uh, because everyone uses windsurf,
it sounded like, um, you're having even these AEs who are non-technical become like just vibe
coding champions. Yeah.
One of our biggest users of windsurf at the company is a, is a non-technical person who leads,
uh, uh, like partnerships, uh, at the company. Uh, he's actually replaced buying a bunch of sales
tools, uh, inside the company. Uh, and, uh, this is one of those things where I think windsurf is
giving power back to the domain experts, right? In the past, what would happen in an organization is
he would need to talk to a product manager who would talk to an engineer who, and the engineer
would have a large backlog because they're, this clearly doesn't immediately make the product better,
right? So this has to be a lower priority, but now he is actually kind of, uh, empowered to actually go
and build these apps. Do you have any programming background at all? No. How interesting. None.
Cause that's definitely one of the controversies on Twitter at the moment. It's just like,
is vibe, can you actually vibe code unless you know some amount of coding already?
Yeah. One of the things we do have is if we do need to deploy one of these apps, we have a person
that actually focuses on making sure that these apps are secure and deployed, but the amount of
leverage that that person has is ridiculous. Instead of him going out and building all of these apps,
uh, the V zero could actually get built by, by people that are domain experts, but non-technical
inside the company. With the Codium launch, you went head on against like Microsoft and GitHub and
like, you know, these income, huge incumbents with the ID launch, you went sort of head on against
Cursor. It's like the hot startup of the moment. I'd like, um, I know again, how was,
how did you all think about that internally? This might be a weird thing about our company,
but our company just doesn't have like, uh, like morale is not really affected by what other companies
do. Right. Uh, it's not possible if, uh, like our company has gone through a lot of basically very
turbulent times. The fact that we needed to pivot at 10, uh, 10, 10 employees and just completely kill
our idea is like a normal thing for the company. And then second of all, kind of like the companies
that are relevant in our space has always been a fluctuating, uh, set of companies. Like, you know,
I, I really respect all the companies in our space, but yeah, co-pilot at, if you were to go to the
beginning of 2023, everyone would have thought GitHub co-pilot was the comp was the product that
everyone would be, would use. And there was no point building. And then the middle, uh, kind of
Devon came out and like, everyone was like, Hey, like Devon is going to solve everything. Right. Um, and, and I'm sure
they're doing good work now. Uh, but, and then after that, obviously cursor is doing a really great
job. So I think what really matters to us most is are we actually, do we have a good long-term
strategy and are we executing in a way where we're getting towards that long-term strategy
while being flexible with the details. Right. And as long as we're doing that, I think we'll,
we have a fighter's chance. Right. And that's like the way we've always operated.
Do you educate yourself at all on the competitors products?
Yes. Yeah. Yeah. I think we don't want to put our heads in the sand and kind of tell ourselves
our product is awesome. Um, and, uh, and, and just kind of, cause it's very easy to do that.
Um, especially given the fact that before we worked on Windsor, if the company was also growing
very, very quickly, uh, from like a revenue standpoint, what sort of opinions did you have
or what taste or opinions did you have for the, for IDE, um, that was a maybe different to cursor?
Yeah. Well, I'm actually just asking is cursors
a very well like product obviously. And so at a product level, why are you like, oh yeah,
like actually we want to build it this way. Yeah. No, I think it's a great question.
Uh, so maybe the first point is at the time, actually, when we started working on Windsor,
all the products were basically chat and this autocomplete, uh, capability. I think that's
basically what, uh, GitHub Coppell was what cursor was at the time. I think we took a very
opinionated stance that we thought agents were, were, uh, where the technology was actually going.
We were the first agentic editor that was out there. Um, and, uh, I think the biggest sort of
takeaway was we didn't believe in this kind of paradigm where everyone would be at mentioning
everything, right? This almost reminded us of like the anti pattern of what Google and
the search engines were before like Google's improved their product a lot, uh, which was
kind of like these landing pages that had like every distinct kind of like bucket of things you
could search for. But Google came out with this very clean search box. Even Google at the time,
you would need, you would get better answers if you wrote and, or, uh, like site link. Um, and now
it's gotten way better. Right. And I guess we had a belief that the, the software would get more and more
easy to build. Right. And we would build from that starting point when we saw all the other players
in the space, making their product so configurable in a way that we thought was, I think good for
users now for where the technology was, but something that wouldn't be unnecessary down the
line. So we invested in capabilities, like how do you deeply understand the code base to
understand the intent of the developer? How do you, how do you actually go out and, uh, and, and make
changes in a way that's like very quick, um, to the code base. So we took the approach of, hey,
instead of having this read only system where you tag everything, what happens if you could make
changes very quickly? And that's why like at the time we, we were kind of the first to do that.
Now, if you were to ask, like, is that a very obvious decision now? I think it's very obvious
now. It looks very obvious. And this is where like, one of the things that I think is true for any
startup is you have to keep proving yourself, right? Like every single insight that we have
is a depreciating insight. It is a very, very depreciating insight. Like technology, the reason why
companies win, um, at any given point is not like they had a tech, they had a tech insight,
like one year ago, right? Actually, if a company wins, um, other than the fact that they have,
you know, monopoly, uh, you know, it's, it's, uh, it's actually like a compounding tech advantage
that keeps sort of existing over and over again. And I think the example of this that I find most,
most exciting is you look at a company like NVIDIA. If NVIDIA doesn't innovate in the next two years,
AMD will be on their case, right? And NVIDIA will not be able to make 60,
70% gross margins at that point. Right. Um, even though it's like one of the largest companies
in existence, uh, right now by basically having good insights to start with, you're able to learn
from the market and maybe compound that advantage with time. And that's the only thing that, that is,
uh, that could be persistent. It sounds like a moat is, uh, you know, we think of it as a noun,
but it's actually a verb. Yeah. Something that could change with time. Right. I also think for us,
and I tell the company this, if we're not continuing to have insights and that's why I'm completely okay
with a lot of our insights being wrong. Um, if we don't continually have insights that we are
executing on, uh, we are just slowly dying. Um, that's like, what's actually happening.
I think the interesting thing is that it's easier now looking back and connecting the dots
on your journey, how a lot of these technology bets you took actually did end up compounding
what windsurf ended up becoming, right? Like it was happenstance and you being really good at GPU
deployment, VMware optimization ended up being the thing to be really good at being blazingly fast
autocomplete because it's faster than other products. Right. So that kind of compounded there.
There's the aspect of you building all these, uh, plugins for enterprises and being so good
at reading large code bases. And you did something that was contrarian. There was a lot of products
and we work with YC companies, a lot of, uh, code gen tools use, uh, vector databases because we work
with a lot of companies and that's was the standard approach how a lot of folks were building, but you
guys did something very different, right? Yeah. So one of the things that sort of, uh,
I think got really popular is this term rag got very popular. Yeah. You weren't anti-rag.
Yeah. I don't know if we're anti-rag. Rag obviously makes sense. You do want to retrieve
some stuff and based on the retrieval, you want to generate some stuff. So I guess the idea is,
is correct that, you know, everything is retrieval, augmented generation. But I think what people got
maybe a little too opinionated about was like the way rag is implemented. It has to be a vector
database that you got in search. I think a vector database is a tool in the toolkit, right? If you
were to think about what users ultimately want, they want great answers and they want, uh, great
agents. That's what they actually want. And how do you end up doing that? You need to make sure that
what's in the context is as relevant as possible. So what we ended up doing is having a series of
systems that enable us to pack the context with the most relevant snippets of code. And the way we
ultimately did that was it was a combination of keyword search, rag, abstract syntax free parsing.
And then on top of that, using, as you mentioned, all the GPU infrastructure we have to take large
chunks of the code base and in real time, re-rank it right. As the query is coming in. Right. And
we found that that is like the best way for us to find the best context for the user. And the kind
of the motivation for this is because people have kind of weird questions. They might have a question
for a large code base of upgrade all versions of, of, uh, of this API to this API. And if embedding
search only finds five of them, it's not of the 10, it's not a very useful, a useful feature at that
point. So we needed to make sure the precision recall was as high as possible, which meant that we
used a series of technologies to actually get to the, get to the, the best solution.
There's a bit of a thing going on with a lot of AI startups getting built, taking too much of a
intellectual shortcut with what's what works for the problem domain space. But you took it from first
principle, right? So you built like a way more complex system that did an AST parts in,
SyncStars3 and all this stuff, which is like, cool. Yeah. I think maybe one of the things that I,
that is potentially interesting to discuss there is, is, uh, we started off, a lot of the company
started off working on autonomous vehicles. And the reason why that's kind of important is these
are systems where you can't just YOLO these systems, which is to say you build the software and
then you kind of like let it run. Uh, you need really good evaluation. I think at the company,
we don't strive for complexity. We strive for what works. Uh, and then the question then is
like, why is the system so much more complex now? It was because we built really good evaluation
systems. Oh, interesting. How did the evals work? Yeah. So the evals for code are actually really
cool. Uh, basically the idea is code. You can leverage a property of code, which it can be run.
Right. Uh, and, uh, we not only have real time user data, uh, we can put that aside for now,
but we can take a lot of open source projects and find, I guess, commits in these open source projects,
uh, with tests attached to them. So you can imagine a lot of cool things we can do based on
that. You can take the intent of a commit, delete all the code that is not the unit test. Right.
And then you can see, Hey, are you able to retrieve the parts where the change needs to get made?
Do you have a good high level intent to make those changes? And then after making the changes,
does the test pass? You can do that task. You can mask the task. And by masking the task,
it's, it's more like a, more like the, the Google task. And what I mean by the Google task is
it's trying to predict your intent, which is to say, let's say you only put in a third of the
change, but you don't get the intent. Can you then fill out the rest to make the test pass?
So there's so many ways you can slice this and each of them, you can break it down into so much
granularity. You can be like, what is my retrieval accuracy? What is my intent accuracy? What is my,
um, what is my passing? What is my test passing accuracy? And you can do that. And then now you have
a hill to climb. And I think that's actually important before you add a lot of complexity
for any of these AI apps. I think you need to like make a rigorous hill that you can actually climb,
right? Otherwise you're just shooting in the dark, right? Why would we add the ASD parsing if it's
unnecessary? Actually, it's awesome if it was unnecessary. Um, right. I don't want to add a
lot of complex stuff to our code. In fact, I want the, I want the simplest code that ends up doing the,
uh, having the most impact. So the evals were actually really, really critical for us to make a lot
of these investments at the company. How much of the development that you do is like
basically driven by improving the scores on the evals versus like basically vibes based.
Like you guys are all using windsurf yourself. You're getting feedback from users all the time.
And then you have just like a sense that this thing is going to work better. And then the evals are just
sort of like a check that you didn't screw up something else. It's a little bit of both,
but obviously for some kinds of systems, I think evals are more important than by,
but like more easy than vibes, uh, just because for the system that basically takes a large chunk of the
code, chunks it up and passes it to hundreds of GPUs in parallel, giving you a result in one second.
It's very hard to have an intuition of like, is this way better? Because that's a very complex
sort of retrieval question. Um, but on the other hand, there are much easier things that from a
vibe perspective are valuable. What if we looked at the open files in a code base, this is actually a
harder thing to eval because when you're evaling, you don't know what the user is doing in real time.
This is one of those cases where having a product in market helps us a lot. Right. And we're able to take
a lot of user data on how people use the product to actually actively make the product much better.
So that's maybe starts with the vibes and then after that you can build evil. Right. Um, so it's,
it's a little bit of both basically. I think a lot of, uh, there's a lot of chatter on the internet
about vibes code is only for toy apps. Windsurf is actually being used for real production, large
code bases. Can you tell us about how the power users use it for like more hardcore engineering?
So this is an interesting thing where a lot of us at the company, I'm not saying that this is,
this is common, didn't get a tremendous value from ChatGPT in the way that probably a lot of
the rest of the world did. Um, and that's not because ChatGPT is not a useful product. I think
ChatGPT is an incredibly useful product. It's actually because a lot of them had already used
things like, like Stack Overflow at the time. And Stack Overflow is a worse version than ChatGPT for
the kinds of questions you want. But that was just a thing that they already knew how to use. Right.
Um, and so they were able to get away with not using or relying on chat nearly as much,
but basically what happened is very recently with agents, the agent is making larger and larger scale
changes, uh, with time. And I think what developers now at our company do is they have felt the hills
and valleys of this product, which is to say, if you don't provide enough intent, it actually goes
out and changes way more of the code than you actually need. Right. Um, and this is like a real
problem with, with, with the tool right now, but they understand the hills and values. And now the very
first time they have a task, they are putting it into Windsurf. They're not, their first thing is not to
actually go out and type in, in the actual editor. It's actually put the intent and actually, uh,
and actually make those changes. Um, and, uh, they're doing kind of very interesting things now,
like deploying our, our software, uh, to our servers actually now gets done with the workflows
that are entirely built, um, inside Windsurf. Uh, so just a lot of boilerplate and repetitive tasks have
been completely repeat, uh, like sort of eliminated inside our company. But the reason why this is
possible is kind of because we're able to operate over a code base that has many millions of lines of code,
uh, really, really effectively. So if you were to give some tips to the audience,
how should a user that uses Windsurf properly provide this intent so that the changes are
more surgical? Because what you're saying with the agents creating all these broad changes,
I've seen that happen, but how, how do you get those precise changes? What, what do you do? How do
you, how do you feed the system? How do you become a- Shouted it with all caps, right? Yeah. Um,
no. So I think this is one of those things where, uh, I think you kind of need to need to have a
little bit of faith in the system and let it kind of mess up a little bit, uh, which is, which is kind
of scary because I think a lot of people for the most part, they will write off these tools really
quickly. Obviously no one at our company would write off the tool because they're building the
tools themselves. I think people's expectations are very high and maybe that's like the, the main piece
of feedback I'd give, which is that, you know, our product actually for these larger and larger
changes, it might make 90% of the changes correctly, but if 10% is wrong, people will just write off
the entire tool. And I think at that point, actually probably the right thing to do is
either revert the change. We have an ability to actually revert the change or just keep going and
see where, see where it ultimately can go. Uh, and, and maybe the, the most important aspect is
commit your code, like as frequently as possible. I think that maybe that's like the big,
big sort of, uh, tip there, which is that, uh, you know, you don't want to get in a situation
where you've made 20 changes and on top of that made some changes yourselves and you can't like
revert it. And then you get like very frustrated at the end of it.
And one thing I wondered like in that vein is whether we need to change the way
Git works with this AI coding paradigm. Have you thought at all about whether doing Git commit
all the time is the right move or whether there needs to be like a deeper infrastructure change?
Yeah, I think we have. So, you know, one of the things that we always think of is in the future,
you're going to have many, many agents running in parallel on your code base that has some,
some trade-offs, right? If you have two agents that kind of modify the same piece of code at the same
time, it's hard to actually know what's going on. That's another thing is that like,
it's hard to have multiple branches checked out at the same time with different agents working
on them independently. That's right. Oh, the merge conflicts. Oh God.
Yeah. There's a lot of them, but Hey, like, you know, that's how, that's how real software
development works too. When you have a lot of engineers that operate on a code base,
they're all like kind of mucking around with the code base at the same time. So that's not a very unique
thing. I think Git is a great tool. I think it's maybe a question of like, how, how can you skin
Git to work in a way that is, that is, uh, that works for this, for this product surface. Like an
example is Git has these things called work trees, which is like, you can have many work trees and
versions of the repository all in your, all in the same directory. Um, and, and, you know, perhaps you
can have many of these agents working on different work trees, um, or instead of exposing the branch
concept to you, you actually can maintain a branch yourself that you can apply to the, to the main branch of the user,
um, kind of repeatedly. One of the things that we think about at the company in terms of why we
think our agent is really good is like, we try to have a unified timeline on everything that happened.
The unified timeline is not just what the developer did, but actually what the developer did in
addition to what the agent did. So actually our product, if you end up doing things in the editor,
if you end up doing things in the terminal, right, all of those things are captured and the intent is,
is actually kind of tracked in such a way where when you use the AI, the AI knows,
right. And in that situation. So in some ways we'd like this thing where the agent is not operating
on a completely different timeline, but it's like something that's kind of getting merged in,
um, you know, at a fairly high cadence. Um, so I think this is like an open problem. I don't,
I don't think we have like the exact right answer for this.
What are other things that you envision changing about one serve in the future? How, how is it going
to evolve?
There's probably a lot of people that think the vibe coding is kind of a, kind of a fad,
but I think that's going to get more and more capable with time. I think, you know, whenever I hear
someone saying, Hey, this is not going to work for this complex use case. It's, it's like, it feels
like a Luddite saying something. It's like, if you look at the way these, these, uh, these AIs have
gone better sort of year over year, it's, it's actually astonishing. It's astonishing. Like,
I'll give you an example of, of something that I held like kind of near and dear to my heart,
which is, you know, there's this math Olympiad called the Amy. And I used to do that, uh, do that
in, in high school. Um, and you know, I was very excited about, about how well I would do it. My high
score was somewhere close to 14 and, uh, and, uh, that's a very high score. But the crazy thing is
that was one of those things that I thought, Oh, wow. Like the AI systems, they're, they're not going
to get anywhere near, um, as good. And beginning of the year last year, it was probably like well
under five maybe. Um, and now they're, you know, the average that opening has put out is like, it's
getting 14 and a half to 15, right. Uh, for Oh four mini. Uh, so it's almost like you have to keep
projecting this out, right. It's going to get crazy. And basically every part of the software development
lifecycle, whether it be writing code, reviewing code, testing code, debugging code, designing code,
AI is going to be adding 10 times the amount of leverage, uh, very shortly. It's going to happen
like much, much more quickly than people imagine. Going back to your current engineering team,
I'm just curious, like if they have all this time freed up from, you know, not having to deal with
like version upgrades and, you know, boring boilerplate stuff, like what do they spend the extra time on?
One of the things about our company and probably every startup, uh, that, that is building in this
space is the ceiling of where the technology can go is so high. It's so high. So it's, it's actually
that, you know, if developers can spend less time doing boilerplate, they can spend more time testing
hypotheses for things that they're not sure work. Right. In some ways engineering becomes a lot more
of like a research kind of, uh, kind of culture, right. Where you're testing hypotheses really, really
quickly. Um, and that has some high cycle time attached to it, right. You need to go out and implement
things. You need to build the evaluation. You need to test it out with our users, but that's like the
things that actually make the product way better. Does that mean you're going to hire a different
type of engineer going forward? Are you looking for different things? Yeah. I think for engineers
that we hire, we want to look for people with a really high agency that are willing to be wrong,
uh, and bold, but you know, weirdly, I don't know if that's, that's changed for a startup, right?
Startups should never be hiring people that, uh, the reason why they're joining a company is to,
to very quickly write like boilerplate code. Right. Cause in some sense, and I don't want like,
you know, this is not the goal, but a startup can succeed even if they have extremely kind
of ugly code, right? That's not usually the reason why a startup fails. Sounds like my startup.
Yeah. Yeah, exactly. That's not usually the reason why a startup fails. The reason why a startup fails
is they didn't build a product that was differentially good, uh, for their users. That's why they
ultimately failed. This is all true, but also like in reality, you always need some sort of like
workhorses to just kind of get certain things done. I feel like in the old days, this was like
building Android apps. Um, it's like you do it as you'd hired someone to do it because there are very few
people who would just be willing to do it. Yeah. Like maybe in your vision for engineering, like
you don't need those people. Like that's not actually a useful skill to have in an organization
anymore. Cause the AI is just like your infinite workhorse. Is that fair? Yeah. Maybe the, the,
the sort of aspects of software that are really niche that are undesirable for a lot of people to
do except a handful of people, those things kind of get democratized a lot more, unless that has
like a lot of depth attached to it. Right. Uh, at least for the time being, um, yeah. If something is like,
Hey, like, you know, we need to change the system to use a new version. Um, and there was someone
that deeply always got in the weeds with version changes. I don't think you have people that are
just focused on that right inside companies. How about how you interview people? Yeah. I think,
I think we have a fairly rigorous and high technical bar and that has a, that's a combination
of, we give interviews that actually allow people to use the AI to kind of solve a problem
because we want to validate if people, I like kind of hate these tools or not. There are still
some developers that do. And obviously if you do, we're probably the wrong company to kind of work
at, but also the same time we do have sort of interviews in person where, uh, kind of onsite,
where, uh, we don't give them the AI and we want to see them think, right. It would be a bad thing if
ultimately when someone needs to write a nested for loop, they need to go to chat GPT. Right.
And, and I'm not like, that's fundamentally because, because, uh, it just, it just feels like
that is a good proxy for problem solving skills. And I think problem solving skills are just at a high
level still should go at a premium. That is, that is the valuable skill that humans have.
Yeah. A challenge that a lot of companies we've talked to have had that we've even had ourselves
is that windsurf has gotten so good that if you give people windsurf, it's difficult to even come
up with an interview question that windsurf can't just one shot where, you know, anyone can do it
because you literally just copy and paste the question into windsurf and hit enter. And so you're
not really evaluating anything. Yes. So I actually think that's true. And it's, it's, it's, you're totally
right. There's very few problems now that something like an 04 mini is not able to solve now. Um,
right. I mean, if you look at competitive programming, it's just, it's just in a league
of its own already at this point, the crazy thing is interviews by nature are going to be kind of
isolated problems right there by nature, because if the problem actually required so much understanding
to do, you wouldn't be able to explain the problem. So that's like perfect for the LLMs where
you give them an isolated problem where you can test and run code extremely quickly. So yeah,
you're totally right. Like, I think if you tell, if you only have algorithmic interviews
and you let people use the AI, I don't know, you're not really testing anything at that point.
Does that mean that you've gone away from just algorithmic questions and you ask
different, like much harder questions that are actually well-suited to being able, being able to
use an AI? Yeah. We, we have questions obviously where that are, that are both system designing plus
algorithms, algorithms related, but these are questions that are fairly open-ended, right? Uh, there may
not be a correct answer. There are, there are trade-offs that you can ultimately make. And I
think what we want to do is just see how people think, right? Given different trade-offs and
different constraints, right? Uh, and, and we're trying to validate for like intellectual curiosity,
right? And if someone ultimately says, I don't know why, why, why, that's totally fine. As long as
like they've gone to a depth that we feel, um, kind of shows, you know, kind of interest, um, interest
and like good problem solving skills, if that makes sense. Like you can tell when someone is curious,
right. And wants to learn things. It's like very obvious. The next thought after this,
which might be counterintuitive, you're at the forefront of building all these AI coding tools.
It hasn't affected at all your hiring plans. On the contrary, you actually need
way more engineers to execute. Tell us more about that. So I think, I think that just boils down to,
I think the problem has a very high ceiling, right? There's so many more things that we really want to
do. The mission of the company is to reduce the time it takes to build technology and apps by 99%. It's
going to take a lot of work to go out and do that. Now, granted each person in our company is way
more productive than they were a year ago. Uh, but I think for us to go out and accomplish that,
I think it's, you know, it's a Herculean task. We need to start targeting more of the development
experience, right? Right now we've helped a lot with the code writing process, um, and, and maybe the,
the, the navigation of code process, but we have not touched much on the, on the design process,
on the deploying process. The debugging process is fairly rudimentary right now. There's just so
many different pieces. If I was to look at it, like, you know, if you say you have a hundred units
of time, uh, you know, we have an ax we've cut off maybe like 40 or 50 in that, in that, in that time,
but there's just a lot more snippets that we need to cut out basically at this point.
It does feel like when I'm using Winsurf, like I am often the extremely slow bridge between different
pieces of technology, copy and pasting data back and forth from my, that's probably actually still a
large chunk of your time. All the pieces have gotten so fast that now it's like the glue between
them, but like I'm the glue, but I'm like much slower. Can I go off the reservation and ask a
weird question? Go for it. Okay. Uh, one of the things that, I mean, I think Pete on our team,
he just released a great essay about, you know, prompting and you should let users have access
to system prompts. The other thing that he came up with that we've been using all at YC internally is
a new agent infrastructure that has direct access, read access to our system of record,
our Postgres database. And in the process of using this, we're starting to realize like if CodeGen
gets a lot better, which it's, you know, I, based on this conversation, I think we can count on that
getting like 10 X, a hundred X better from here. Uh, what if instead of, you know, building package
software, there's like just in time software that the agent basically just builds for you as you need it.
Does that change the nature of software and SAS and you know, what happens to all of us and
Windsor if I don't know, I think this notion of just a developer is, is probably going to broaden
out to what's called a builder. And, uh, I think everyone is going to be a builder and they can
decide like, you know, how deep they want to go and build things. Uh, maybe, maybe our current version
of developers can go deep enough that they can build more complex things. Right. Um, in the, in the
shorter term, but yeah, I think software is going to be this very, very demo democratized thing.
Right. I imagine a future in which, you know, what actually happens when you ask an AI assistant,
Hey, build me something that like tracks the amount of calories I have. Why would you have a very
custom app that goes out and does this? It's probably something that takes like all the inputs
from your AR glasses and everything, and has a custom piece of software that kind of comes out
and like an app that is there. And like, it has tasks that go and tell you, uh, you know, are you on
track with, with all the calories you're, you're sort of consuming here. Um, and I think that's a
very, very custom piece of software that you have for yourself that you can keep tweaking. I can
imagine a future like that where effectively everyone is building, but people don't know what
they're building as software. They just, they're kind of just building, uh, just capabilities and
technology that they have for themselves. Do many people use windsurf who don't know how to write
code at all? It's actually a large, a large number of our users. Yeah. Interesting. Yeah. How did they end
up getting into windsurf? Like, do they work at some company where like some program or show them
how to use it? Like I, I, I tend to think of windsurf as targeting more like the professional
developer market. That's like using this as a new superpower versus the like non-technical user
market. That's doing like what Gary was talking about. We were shocked by this too, because we
were like, Hey, our product is an ID, but there's actually a non-trivial chunk of our developers that
have never opened the editor up and they just, you know, our agent is called cascade. Right. And, uh,
just to live in cascade, we have browser preview. So they just open up the browser
preview. They can click on things and, and sort of, uh, make changes. The benefit is because,
because we kind of understand the code when they come back to the repository and the code has
actually gotten like quite gnarly. We're actually able to pick up from where the developer left off
or the, the, the, the kind of builder left off and keep going from where they were. Um, I will say
we have not optimized tremendously for that use case, but it's actually kind of crazy how much is actually
happening there. Do you think in the longterm that this ends up being one product that targets like
both of these audiences, or do you think actually there's like different products for different
audiences? There's like a windsurf, which is like focused on like serious developers who want to see
the code and be in the details. And then there's maybe other products for, for folks who are totally
not tactical who don't, who don't even want to see the code. I don't know what the longterm is going to
look like something tells me it's going to become more unified. But one of the things that I will
just say is like, as a startup for us, even though we do have, you know, uh, like a good number of
people, there's a limit to what we can focus on internally as well. Uh, so for us, like we're not
going to be focused on how do we build the best possible experience for, uh, the developer as well
as build the experience where we have so many things for the non-developer. But I, I have to imagine
that this idea of building technology, if you, as you get better and better at understanding code,
um, you're going to be able to deliver a great experience for non-developers, um, as well.
But, uh, I don't know what the path dependence is. I assume like a bunch of companies in the space
will go from non-developers to then supporting an ability to edit the code. Right. And I think
we're starting to see this already where, you know, the, the lines are sort of getting blurred right now.
You probably care about it for your evals at least.
Yeah, no, you need to, you need to care about it for your evals that maybe that's like the hard part
for me that to, to imagine for the pure non-developer product, what is the hill you're climbing if
you're not like kind of understanding the code? How do you know your product is getting better and
better? Um, it's like an open question. Are you completely relying on the base models getting
better? Which is fine, but then you should imagine that your product is an extremely light
layer on top of the base model, which is a scary place to be, right? That means you're going to get
competed across all different axes. How do you think about that in general? I guess like the,
something we've talked about a lot on this podcast is just the GBT wrapper meme, um, has completely gone away.
I feel, um, though every big release from one of the labs sort of brings it back a little bit and
everyone's a little bit scared that like, you know, open AI is just going to eat everything.
Um, how do you think about that? I think the way we, I think about this is like, yeah,
the company, as I mentioned before, it's a moving goalpost, which is to say today,
if we're generating sort of 80, 90% of all committed software, uh, yeah, I think, I think when the new
model comes out, uh, we're going to need to up our game. We can't just be at the same, same stage.
Maybe we need to be generating 95% of all committed code. And I think our opportunity
is the gap between where the foundation model is and what the, what, what a hundred percent is.
Right. And as long as we can continue to deliver an experience where there is a gap between the
two, which I think there is, as long as there's any human loop at all, um, in the experience,
uh, there's a gap, uh, we'll be able to go out and build things, but that is a constantly transforming
sort of a goalpost for us. Right. So you can imagine when a new model comes out,
maybe the baseline on what the foundation model by itself provides has doubled the alpha we provide
on top of what the base model provides needs to double as well. It feels very like for me,
the reason why this is not the most concerning is let's suppose that you were to take the
foundation model and it's providing 90%, it's reducing the time it takes by 90%. That actually
means if we can deliver one or 2% more percentage points, that's a 20% gain on top of what the new
baseline is. Right. Um, like I guess 90, if 90 becomes 92 or 93, um, which, which, which is still very,
very valuable right at that point, because effectively the 90 becomes a new baseline,
uh, for everyone. So I think basically the way we sort of operate is how can we provide as much
additional value as possible? And as long as we have our eye on that, I think we're going to,
we're going to do fine. What advice would you have for our startups that are working in the like
AI coding space? We have a ton of them. What are, what are the opportunities that like you think
are going to be open to new startups? I've seen a lot of things that I think could be like,
particularly interesting. I don't think any of these technologies we've really adopted, but
there's so many different pieces of how people build software. And, uh, and I'm not going to
say niche, but there are so many different types of workloads out there. I've not really seen a lot
of startups in the space that are just like, we do this one thing really, really well. Um, like I'll
give you an example. Like we do these kinds of like Java migrations really, really well.
Crazily enough, if you look at this category, the amount that people spend on this is probably maybe
billions, if not tens of billions of dollars doing these migrations every year. It's a, it's a massive
category. Um, that's an example. Migrations from what to what? So an example, this is like actually
kind of crazy. JVM seven to eight or something like that. Yeah. Rails versions. Yeah. Even more
than that actually, it's like a lot of, a lot of companies write COBOL, have COBOL. And crazily enough,
most of the IRS software is written in COBOL. Apparently in the early 2000s, they tried to migrate from
COBOL to Java. I think it was a five plus billion dollar project. Uh, surprise, surprise. It didn't happen.
You think they could one shot it now? I don't, I don't know if they can one shot it.
I'm just kidding. But, uh, but no, but it's imagine, imagine if you could do those tasks
very well. It's such an economically valuable sort of task. I think we obviously don't have
the ability to focus on these kinds of things inside the company. That's a very exciting space.
If you could do a really good job there. The second key piece is there are so many things that
developers do that are also not making the product better, uh, but important like automatic resolution of
like, you know, kind of, uh, alerts and, and, uh, and bugs in the, in, in software. Um, that's also
a huge, huge amount of, amount of spend, um, out there. And I'd be curious to see like what a best
in class product in that category actually looks like. I'm sure someone that if they got truly in
the weeds on that, they could build an awesome product. Um, but, but I think I, I've not heard
of one that is like tremendously taken off. I think those are actually both really great insights.
And one thing I like about them is that there's like, it's not just an opportunity for like two startups,
like each one of those is like a bucket that could have like a hundred large companies.
And we actually do have a company from, from S21 called bloop that does these cobalt to Java
migrations with, with agents. That's awesome. Yeah. Yeah. It's a gnarly problem. It's a very
gnarly problem, but it's, if you were to talk to any company that has existed for over 30 years,
this is probably something that is costing them hundreds of millions a year.
So reflecting on this journey, I mean, we're all really thankful for you creating windsurf. It's a super
charging all of society right now. Uh, what would you say to the person who, you know,
basically the you from five years ago before you started this whole thing?
The biggest thing I would say is change your mind much, much faster than you believe is
reasonable, right? It's very easy to kind of fall in love with your ideas, um, over and over again.
And you do need to, otherwise you won't really do anything. Uh, but, but pivot as quickly as possible
and treat pivots as a badge of honor. Most people don't have the courage to change their mind on things.
And they would rather kind of fail doing the thing that they told everyone they were doing
then, uh, change their mind, uh, take a bold step and succeed.
Varun, thank you so much for joining us today. We'll catch you guys next time.
