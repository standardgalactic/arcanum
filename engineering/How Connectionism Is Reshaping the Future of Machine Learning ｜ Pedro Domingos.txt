I see a bunch of data. I'm going to postulate that they were generated by some process,
and I'm going to try to identify what that process was. This is actually what generative
learning is. So you come up with this model like a Bayesian network that says like, well,
this is how you generate patients and their symptoms. First, you decide which disease they
have, and then you decide which symptoms they have. What fever are they going to have based
on the fact that they have COVID, et cetera, et cetera. You generate the data. And this is why
it's called the generative model in opposition to discriminative models, which is what most
of machine learning is about, which is to just go say like, okay, I'm going to go forward from this
data and try to decide what you suffer from, or whether it's a cat or a dog, as opposed to like,
how do I generate a cat or a dog? So there's always been this thing in machine learning
between generative and discriminative. Discriminative has always dominated and really still does
because it's an easier problem and it works better. When you need somebody, you need somebody.
You want to hire people fast. The longer you wait, the more the market drifts, the more your
needs drift. For example, you just realized your business needed to hire somebody yesterday.
How can you find the right candidate fast? Just use Indeed. When it comes to hiring, Indeed is all you
need. Stop struggling to get your job posts seen on other job sites. Indeed's sponsored jobs help you
stand out and hire fast. With sponsored jobs, your job postings jump to the top of the page for your
relevant candidates so you can reach the people you want faster. And it makes a huge difference.
According to Indeed data, sponsored jobs posted directly on Indeed have 45% more applications
than non-sponsored jobs. One of the things I love about Indeed is that it makes hiring so fast.
I've searched for candidates before, and it can take months, particularly if you're just working
through word of mouth. In those days, I didn't have Indeed. Plus, with Indeed's sponsored jobs,
there's no monthly subscriptions, no long-term contracts, and you only pay for results.
How fast is Indeed? In the minute I've been talking to you, 23 hires were made on Indeed,
according to Indeed data worldwide. So there's no need to wait any longer. Speed up your hiring right
now with Indeed. And listeners on this show will get a $75 sponsored job credit to get their jobs
more visibility at Indeed.com slash Eye on AI. Just go to Indeed.com slash Eye on AI. That's Indeed,
I-N-D-E-D dot com slash Eye on AI. All run together, E-Y-E-O-N-A-I. Go right now and support our show by
saying you heard about Indeed on this podcast. Indeed.com slash Eye on AI. Terms and conditions
apply. Hiring indeed is all you need. The biggest surprise about the Xeep for me was how surprised
people were. There's really nothing particularly surprising about anything they did. Same with
what the others are doing. It's the natural progress. And a lot of it, honestly, is adding
more capabilities and tweaks and variations that are all useful and understandable, but on top of a core
that isn't changing that much. And honestly, I think we're converging, as is often the case, to a local
optimum. I personally am more interested in what the global optimum is than the local one. But in the
short term, that's where we are. Yeah. It's kind of what happened with supervised learning. It was,
everyone was amazed. And then a lot of people started focusing on optimizing and tweaking,
and it became stronger and stronger. Is that right?
When you say supervised learning, what do you mean exactly?
Yeah. I mean the labeled data pattern recognition systems that dominated neural nets before generative
AI? Oh, I see. Interesting. Well, I'll tell you a very short story. When I was in grad school,
this was in the early 90s, one of my colleagues of the same advisor, he left because he said there was
no future in machine learning because it was all, you know, the only thing that worked was supervised
learning and supervised learning. There was nothing new left to happen. Well, we can see how that turned
out, right? I don't think, I mean, I see what you're saying, but I was, supervised learning is such a broad
paradigm in machine learning that it's, there are certainly and have been local optimum within
supervised learning. It's hard to see supervised learning. I mean, I guess in the current context,
people talk about like, oh, we're running out of data and so on and so forth and we need to do other
things. And that's where the lot of work is. But the problem is that they're not, they're not getting
the most out of the data is that the supervised learning algorithms are not that good. And in fact,
the, the, the key idea in some ways of LLMs, maybe we'll talk about this is, is to take advantage of state that
is to, of, of data. The, the, the problem with super, it continues to be the case that the only thing that really
works in machine learning is supervised learning. You know, it's, yeah, that reinforcement learning and even
unsupervised learning, blah, blah, blah. The key innovation in, in, in LLMs in many ways is just, and it's not a thing that's new to LLMs, but it's what drives it.
Supervised is how to turn a massive amount of unsupervised that into supervised.
Yeah. So all, you know, under all of this stuff, there's still supervised learning and gone and moreover and, and related to deep seek, people are always talking about these days I've become, you know, honestly, a little skeptical and just like, oh, we have our latest, greatest reinforcement learning, blah, blah, blah. And when you, when you look at it up close, it's really just doing supervised learning, but it's sexier to call it reinforcement learning. So there you go.
Yeah. And so anyway, let's, let's get on with the connectionist. One thing that interests me is how, I mean, and I think I mentioned this in talking, this whole series, is that there are a lot of strategies or schools,
schools or tribes, as you call them that remain viable for, for progress and, and continued research.
I had on the podcast a couple of weeks ago,
I'm not sure how to pronounce it when he pronounced it. It doesn't sound like that.
Um, the, uh, the, uh, the guy who came up with, uh, the long storm, long, short-term memory, uh, networks, LSTM.
And he is continuing research and has now created a company to use his research and industrial applications.
And I thought that was really interesting.
Um, so yeah, let's talk about the connectionists, where they came from, where they're going and whether there are schools of, uh, of connectionists, uh, theory that are being, uh, neglected because the bright, shiny objects, uh, is capturing everyone's imagination.
Yeah, sure.
So go ahead. Uh, and Pedro go reintroduce yourself. I apologize for that, but, uh, yeah, go ahead.
I'm Pedro Dominguez. I'm a professor of computer science at the university of Washington.
Uh, I have been an AI researcher since the nineties. Uh, my specialty is machine learning.
Uh, I've worked in a lot of different areas of AI along the different paradigms and also in a lot on unifying them.
Uh, uh, I'm probably best known as the, as the, uh, author of a book called the master algorithm, which is an introduction to machine learning for a broad audience.
And more recently of 2040, uh, Silicon Valley satire, which is basically a spoof of AI hype and fear and, and, and the tech world.
Yeah. Uh, so tell us about connectionists, how they emerged, uh, and how they operate alongside, um, the other, uh, schools of thought that we've talked about, uh, and how they relate them to what's going on now.
Connectionism is one of the oldest paradigms in AI.
You could say it's the oldest after symbolic AI.
Uh, it started in the forties.
Um, which is a long time ago in, in, in AI terms.
And you could say it's really one of the two big ones along with symbolic AI.
Sometimes if one thing over there, I think of like these two big opposing groups and they are the symbolists and the connectionists.
And in many ways, the connectionists historically haven't find themselves in opposition to the symbolists.
Now, one interesting thing that has happened is, so the idea in connectionism is, uh, actually a very appealing one.
And that appeal I think is very important, which is okay.
We want to build intelligent machines.
Uh, what do we know that's intelligent?
It's humans.
Uh, and in particular the human brain.
So like good engineers, let's, when we'd be, when you're behind the competition, what you do is you reverse engineer it.
You open it up, right?
You open up the car, right?
Whatever your Toyota, you start by looking at your GM or, or Ford cars and go like, well, how is this thing built?
Right.
And then you copy it.
And then maybe eventually like Toyota did you improve it and so on.
But that's the idea.
Let let's reverse engineer the brain.
And so this started from the very beginning of the modern age of computer science.
People were, there was this paper that came out in the forties by McCulloch and pits, uh, on a model of neurons.
You could actually say that was the paper that started neural networks.
And, and a lot of people don't know is that a lot of the computing hardware today that has nothing to do with AI a lot.
All of it is really, uh, derived from that paper.
So in some sense, every computer as they used to call them in the fifties is an electronic brain.
Every, every circuit in a computer is, is, is imitating neurons.
A very important point to remember as a yes or like no starts to gobble up the rest of computer science.
Uh, but the big difference, uh, between those neurons and the ones in your brain is that those neurons didn't learn.
They didn't have weights.
They were really just logic gates, which is again, what computers are made of.
You know, it was a more, it was a generalization of, of the, of the logic gates with, we, we tend to use.
Uh, and then really the key name in this story was Frank Rosenblatt, who in the fifties created this thing called the perceptron.
That was the first, uh, neural network that learned, but it wasn't a neural network.
It was actually just a single neuron that learned with some pre-computed features that were fed into it.
And, uh, um, people got incredibly excited about this at the time.
The New York times had the front page story saying, uh, you know, human level intelligence is coming soon.
I, I can't help laughing at this because you know, it continues to happen today.
Uh, one day it will be true, but, but in the meantime, people should know this history.
And so for a while, uh, uh, it was very popular and you could even say that was probably the most popular approach to AI.
For a while.
And in fact, it's interesting that a lot of people from the other paradigms, they started out during neural networks.
Mm-hmm John Holland of evolutionary computing and Marvin Minsky of symbolic.
Yeah, Marvin Minsky's PhD thesis was on neural networks, which boggles the mind.
Uh, but then they all, uh, uh, drifted away from it.
Once that initial seductive idea was not really panning out very well.
And, and then, um, also very famously in 1969, Marvin Minsky, who was by now one of the leaders of the symbolic school and similar paper wrote a book called perceptrons.
Yeah.
Basically tearing down the claims that the perception was a great thing.
They showed mathematically a series of things that perceptron just couldn't do.
Mm-hmm .
And then interesting perceptrons just in neural networks just fell like a stone.
Yeah.
And then for the next 15 years, they were dead.
And in fact, they even reflected on machine learning in general, the consensus, you know, during the seventies, uh, and early eighties was that, you know, machine learning doesn't really work.
It's too hard.
Uh, you know, you gotta program things, you know, you gotta do knowledge engineering.
And that was the big thing.
It was symbolic AI and knowledge engineering and machine learning was nowhere.
Yeah.
And then in the eighties, they, they, uh, they came back.
Uh, and the key development was, was the, the, uh, back propagation algorithm.
Mm-hmm .
People knew, and Minsky and Papert said that like, if you could train networks with multiple layers, then all those limitations would be solved, but nobody knew how to.
And in their words, they couldn't see how it would ever be done.
Uh, and that prop itself has an interesting history, but it was in the eighties that it took off.
Right.
And then there was another moment of great excitement, which then again, died down again, because people thought like, oh, we're just gonna build deeper and deeper layers.
And pretty soon, you know, it'll be the brain, right?
They got over optimistic again.
There's always somebody in AI being over optimistic.
The only question is who, but then, you know, the problem with back prop is that it did, it only worked with one hidden layer, which wasn't much.
And so people gave up on it.
And by the end of the nineties, neural networks were kind of not dead, but like completely sidelined again.
And, and that's when, you know, early two thousands is when, uh, you know, people like Jeff Hinton, uh, uh, uh, Yann LeCun and Yosha Benjo started working on this.
Actually, they didn't start.
They never stopped working.
They were the diehards that never stopped working, but they, they started to figure out ways to learn deeper networks.
And in fact, the term deep learning partly it's a marketing term.
It's clever, right?
Whoa.
The learning is deep, but really technically just, uh, refers to the fact of having networks with many hidden layers, which indeed, once you're able to learn them can do amazing things.
And, and here we are today.
Yeah.
Can you just take a dogleg on, uh, back prop and explain, uh, where it came from and how, uh, Hinton applied it?
Yeah.
So first of all, what is the difficulty that back prop is solving?
It's actually one of the central problems in machine learning.
So in the master, I'm gonna talk about the five tribes and how each one of them has an algorithm, its own master algorithm that solves the key problem.
And my contention, of course, that you have to solve all of them.
So none of the algorithms is enough, but the key problem, and it is a central problem in machine learning that back prop solves is credit assignment.
Yeah.
I have a very big system with lots of layers, lots of neurons, a big mess of connections.
And it's supposed to look at an image and tell me if it's a cat or a dog and the image it's looking at is a dog, but it says it's a cat.
Right.
And so now the question is, who do I blame?
Maybe it should be called the blame assignment problem.
Like who, who should change, right?
In connectionism, all the knowledge is in the connections between the neurons.
Again, inspired by the human brain where, you know, as, as far as we know, uh, all the knowledge you learn is, is in the.
The strings of the connections between neurons, which connections should I change?
And if you think about it, this is not obvious at all, right?
There's a mistake at the end.
There's an image at the beginning, like, you know, who the hell knows what's going on.
But back prop, uh, technicalities aside, just gives a super simple answer to this.
And again, often machine learning, super simple answers are what gets you the farthest.
And the answer is this, what I'm going to do conceptually, not in practice, because that would be too inefficient, but conceptually is I'm going to tweak each weight in turn a little bit.
I'm going to take every weight of every neuron in every lane and say like, let me increase you a little bit.
You know, and there's usually an output function that is continuous that says like, what is, you know, your probability, let's say loosely speaking of being a cat.
Oh, when I tweak this, when I move this weight up, the probability goes up a little bit.
Good. So let me keep that.
When I tweak this other weight down, the probability goes up.
So let me tweak that.
Right.
If you do this for every single neuron over and over again, uh, over time, amazing things happen.
It will probably learn to classify the, the cat images really being a cat image and so on.
And, and, and, uh, you know, back prop is really just a computationally efficient way, a clever way, but you know, of a type that we know very well of trying to not be too wasteful.
Like you don't want to redo the whole computation.
Every time you do this, that would just be ridiculous, particularly with today's networks.
So it's really just an efficient way to do that.
What happened with back prop there again, uh, is one of those delicious iron is that the history of AI is full of is it took off.
There was this paper published in 1986 by David Romer Hart, Jeff Hinton and, uh, um, Chris Williams or somebody Williams, Robert Williams.
Uh, there was a third author, um, and it's often credit.
It was originally credit, let's say with, you know, that's when they discovered back prop.
The truth is that big back prop was discovered 20 times before that.
As it became more prominent.
I mean, I alone know three or four different people who invented back prop.
Uh, one of them was young.
Well, the grad student in France, he actually invented black pop on his own.
And I know, you know, uh, uh, uh, for example, there was a postdoc of my advisor, UC Irvine, who in, in like early eighties or even late nineties, he submitted the paper to the top AI conference with back prop.
And the paper was rejected and the reviewer said like, what are you doing?
Minsky and pepper already showed that.
Yeah.
And then there was this guy economist from Harvard who was, you know, had actually found that in, he published, you know, a paper or a thesis, I think about it in the seventies.
Turns out that, um, there was a paper by control theorists, uh, Bryson Ho in the sixties, before the preceptor's book was published.
The, uh, the, the back prop algorithm already existed.
Yeah.
And it's like, yeah, LeCun says that really, uh, the credit, uh, uh, for back prop should be given to Leibniz because it's really just the chain rule of calculus.
Yeah.
So there you go.
Yeah.
One question I have about back prop is I understand the, the theory, um, that, that you keep adjusting the weights, going back down through the weights, uh,
to get closer and closer to the, uh, target.
Uh, but is it done sequentially?
Like each neural pathway is adjusted and then the, the outputs assessed, or is it done, uh, incrementally through all the neurons?
And then the, the, I mean, how, how does that work?
It's done sequentially, but back through the layers.
And in fact, that's where the name comes from.
Yeah.
So the way things work in back prop is that you have your image of the cat and first you do a forward pass.
Right.
Sometimes also called the inference pass, which is basically just computing the output.
First of the neurons that I write on top of the image and then the next layer, all the way to the final output that says cat or dog.
And then comes the error back propagation phase where you measure the error.
Like, you know, your output was, you know, 0.7 for cat, but it should have been 0.9.
Right.
Or whatever it was 0.1 for catch at 0.9.
So now what I'm going to see is I can look at the last layer of neurons, starting with the very last neuron in that case.
And see like, if I tweak every one of your weights, uh, how does that change things?
But then the key is that when I go to the previous layer, actually don't start from scratch.
That would be wasteful.
I don't go all the way back to the output.
I say like, well, now I know how much the neurons, the weights in the last layer, how much difference each one of them makes.
Based on that, let me see how much difference each neuron in the previous layer makes.
So you just go back through the layers all the way to the initial one.
And that's why it's called back propagation because you're propagating the errors backward and then you're learning from that.
Yeah.
Uh, okay.
Uh, okay.
So we've got back prop.
Uh, Jeff and Ilya and Alex create AlexNet in 2012.
Well, which, uh, takes advantage of the larger data sets available for, and the greater compute power, uh, available.
And they have a breakthrough on the image net competition.
And that sets off the, uh, the new, uh, deep learning craze.
So where, where does it go from there then?
Well, sorry to just back up slightly, uh, deep learning in public perception exploded then, but already before that there had the first big progress of deep learning.
They cost people to stop paying attention.
In speech, uh, Jeff Hinton for the most part has always worked mainly in vision.
He also Benja was the language guy, which we'll probably get to.
Yeah.
And the Kuhn was also very much into vision.
into vision but but at one point you know because that's what they could do in some ways vision is
very expensive uh jeff and his students started working on speech and speech was working really
well uh you know um jeff had this intern jeff hinton had this uh um student uh natty jately
who he calls apparently you know larry page calls him the the 30 million dollar intern
and jeff says it really should be the billion dollar intern he did an internet google one
summer where he applied speech applied deep learning to the speech system that they have
it was already as you can imagine a highly engineered thing right and then one summer
he basically beat the state of the art that these i don't know hundreds of engineers have developed
so so um and speech was again it's a long-standing area of ai and it had plateaued for you know for
decades and then suddenly out comes deep learning it's like wow suddenly you know the this phenomenon
that you've seen in a whole bunch of other areas was first seen in speech right and then you know
lxnet itself has an interesting uh history but but you're correct uh they they um the vision community
has always been skeptical of um deep of neural networks because they didn't think of that as
serious vision they all just worked again it's hard to not say this with a smile but for the first 50
years all they were working was digit recognition yeah you know it was like how to recognize digits
better because again you know that that's what you could that's what they had the data for and
that's what wasn't too expensive and vision people were like like we really couldn't care less about
that that's not vision go away so and and like the pope of vision is is jitendra malik who's a
professor at berkeley and jeff hinton at one point uh got on the phone with him and said what would it
take to persuade you that you know nil networks are good for vision and then you know jitendra said
well you know there's this data set called pascal uh which was you know an important data set back
then but then you know hinton you know talked with his students came back and said like no pascal is too
small uh you know is there a bigger one and then and then you can say well there is this data set that
this person at stanford fifeli and his students have developed called image net that's actually pretty
large there's a million examples that doesn't sound large today but back then the likes of pascal and
other sets were like you know a few thousand which sounds ridiculous that anybody would try to solve
vision with a few thousand examples and so and they and the key thing in alex net was really that they
used gpus right without gpus it was like they couldn't have done it right but then but the gpus were
available then uh uh and then you know they they weren't the first ones to repurpose them from machine
learning but you know they they got this amazing success and then to answer your question what
happened was the vision folks initially uh uh were skeptical the people in machine learning were very
excited and the the folks are just like they're doing something wrong this this can't be right
right they just botched something and they're getting these results aren't real so they went back
and and and repeated the experiments and you know oh my god the results are real and then they started
applying it and this is sort of like you know the wavefront that continues today they started
applying it to other problems in vision not just you know uh um recognizing you know literally cats
from dogs those are the most frequent things on image net is different types of dogs but other tasks
envision it continued to work really well and then within the space of a few years vision went from
having no papers to speak of uh using deep learning to basically everything just uses deep learning
uh and and and and then of course this this got into the public consciousness and then the attitude
of the deep learning folks which you gotta um admire at one level but you know be a little skeptic i was
like okay we've solved vision what's next let's do language now right let's do machine translation
right and i remember talking with yoshua ben joseka i don't know like 2015 and he's saying like yeah we
haven't quite matched the statistical machine translation results yet statistical machine
transition you know systems at the point again at google and other places were very big and already
doing very well in many ways uh you know it was the death of the employment for translators and whatnot
which of course didn't happen you know and and that whole phenomenon continues today but
he said we're almost there within a couple years they weren't just almost there they had blown past all
of that right and these days that pattern continues with reasoning it's like yeah we solved vision we
solved language and now we need to solve reasoning the problem with this is that they haven't really
solved vision or language or any of those things they made a lot of progress but in the meantime there's
a lot you know again we are stuck in these you know local optima that we are going to need to get
out of and to their credit people like yan lecun right who again he's a long-standing uh uh neural
network vision guys is like no no no this isn't solved and it isn't i mean if you look at the video
understanding which is really the main problem in vision right the others are kind of like sub
problems of that it's very far from solved no one really knows how to be understanding and you know
the new generation and things like transforms or not people have applied it to that but you know it's
it's an improvement but um all of that continues to be installed as his language and as his reasoning
even more of course you know the latest generation is that this whole you know chat gpt etc and whatnot
which in some ways is more of a sociological phenomena than a technical one the big novelty
in chat gpt was that everybody you know was blown away by it and started using it in terms of research
content there is very little new in chat gpt you could say oh they scale things up some more but even
that is not true google had already you know they that google made the mistake of not releasing their
chatbot into you know in technical terms there wasn't anything significantly new in chat gpt over
you know over lambda for example google's chatbot at the time and of course a lot of in technical
innovation has happened since but it's interesting that in some ways in you know um there is all this
focus of research and industrial attention on a very very narrow front even within deep learning
well a lot of the other things including probably where the big leaps are going to come from are
being neglected yeah yeah can you go back a little bit uh because there was another flexion point with
the transformer algorithm uh and talk about uh and this is all within the the connectionist camp right
can you talk about the development of the transformer and on speech i just want to jump back a little bit
uh terry sanowski was doing something called net talk uh before uh jeff had come out with uh with his
vision stuff was net talk a step uh towards uh you know applying uh neural networks to speech
yeah so net talk uh back in the late 80s early 90s when somebody needed to come up with an example of
a neural network success it was net talk net talk was not speech recognition was speech synthesis
right because you gave it a text and it read it aloud and the thing that got people really excited
and again i think you can listen to this on youtube even now if you just you know search for net talk
is it started out producing noise and then as the network learned through successive generations of
backdrop it started it sounded at first like it was babbling like a baby this i think like that fact
that it was babbling like a baby is really what got people and then the babbling got more coherent and
finally it was actually doing a pretty good job of synthesizing speech which this does it sounds like
what is the big deal but at the time it was a big deal uh so you know so net talk you know was the
poster child of neural networks but the truth is there's a useful application that never went
anywhere it wasn't good enough reliable enough blah blah and and commercially speech recognition was
it wasn't as much more important right yeah and and and even harder right it's like you get this it's
much harder you know if you think about like it's not that hard to do a bad job of synthesizing speech
from text you know what the phonemes are and you string them together it'll sound very stilted but it's
not that speech recognition is a hard problem right there's like this noise wave coming at you
all garbled and like people like you know eat up syllables and you've got to turn that into text that
is really hard so you're right net talks it was what you know people talked about back in 1990 but you
know to circa 2005 10 what jeff and his students were doing was again something that the speech people
had to take seriously all right again you know i sometimes joke that machine the purpose of machine
learning conference is to is to publish application papers before they're good for the application
conferences you know you probably speech there and vision because of the because you know as tests of
the machine learning algorithms and then when they mature they go to their natural language whatever uh
conferences which brings us back to transformers and it's exactly you know if we go back to the point
where i said and then people said let's do language uh let's pick up the story there uh as i mentioned
the main guy of that very small group that had always been interested in language but but he even he
he did his phd on i think speech and then kind of didn't work on that for a while because again they
got stuck for reasons that we can go into um and then came back and his group started looking at
machine translation who is that i'm sorry who who was that so so the uh um attention right is the key
thing in transformers right and attention it's an interesting story um it was so yoshua banjo said to
his to a new postdoc of his kyung hyun cho and and and someone who was an intern he's he's the best
part of this more people don't know this transformers were not invented at google i mean the name was
and the tool architecture was but the key idea in transformers is attention i mean the transformers
paper is called attention is all you need because actually the main thing they did was take other
stuff out which is a valid contribution but attention was invented by an intern in yoshua
banjo's group he wasn't any you know great machine learning researcher or anything he was an intern who
didn't know anything and just started working on this and he came up with that idea so what was the
problem what was the solution right he's like to do machine translation the way they tried it initially
was like you would go through for example the translating french to english you would go through
the french text with what is called a recurrent neural network that processes things sequentially
over time and you would wind up with with the state of the neural network that captured the text
that you had read and then you would start from this state with and this was called the encoding part
right because they encoded french into the network and then there was the decoding part
where you turn that internal state which is a bunch of numbers that nobody understood and still
doesn't and turned it into english again sequentially by generating the text one word at a time and this
didn't work very well there was a bottleneck like like there was too much information lost and they
tended to remember the recent text meaning the last parts of the text but not the early one and so
yoshua said to them like you know you know we got to do something about this right and and what is the
attention idea in a way the attention idea is really just doing with neural networks what people have been
doing in the statistical approach to machine translation for decades there's always there
was always this phase called the alignment phase which has nothing to do with what people call
alignment today is this fact that for example when you're translating fresh to english word order is
often different yeah like the adjectives you know will come you know before or after right and so it's
not enough to say i'm gonna go through the text one at a time when i'm generating the next word let's
say the next word is the adjective like for example i saw a big cat right and then like where does the
big come from right you got to go find in the previous text the word that big is translating oh
look on sha okay right i already translated sha and all the car whatever right you get the idea right
and so what they did was this was they did the neural network version of the same process
where what this and this is what attention does is like now that i'm generating the word
you know like what is my next word is going to be well what what is it the translation of
right oh you know big you know that's gone as big as the translation of gone lesson and and and then
you can learn this using background right so that was that was their innovation very important one
right but they were just trying to solve a machine translation problem the paper is just about
machine translation and then the transformer guys same thing they they you know they refined you know
a bunch of stuff and basically to cut out the stuff that no longer was was longer useful and and and
then i mean i it's interesting because like i remember reading these papers at the time and each
case thinking this is a very cool idea i bet you can do a lot of other things with it besides translation
next paper and then what people found and this is partly you know the the beauty of machine learning
is like they took the same architecture and started you know doing more and more things with it and
keep and it kept working amazingly right then and here we are today right chat gpt the t in gpt is
transformed yeah yeah uh and then the guys at google there was a team uh that that as you say stripped
out everything uh and made a very simple architecture around attention right um i i'm i'm i'm i'm laughing
because it's anything but simple but i mean so it wasn't it wasn't one team there were several teams
maybe three that's why the paper has eight authors there are three different groups of people who you
know for one reason or another said like yeah let's let's try to use this for whatever right or or you
know they had their different concerns but they were all started using this new mechanism of
attention right the basic attention is just you know a couple of equations and again uh um we can
try to understand what it's doing in in multiple ways but and but they weren't making any headway
and then the key guy was actually noam shazir is one of the eight authors of the paper but he's really
the guy who invented transformers in that sense if you take attention for granted uh he i mean again
i've talked to several of these people they they just hacked a million things as often the case until
something worked there was no great insight there at least that they can yeah maybe there is but you
know but we don't know what it is yet uh probably but you know noam was the guy who who like figured
out how to make it work right and and then and then and then um it you know i was smiling because
he you know the transformer so the recurrent neural networks are are one of the problems is that they
work sequentially so they also have to be trained sequentially which makes it hard to paralyze them
and paralyzation is very important so that you can scale up to make things run faster and and the
and and you know initially you know from the the the montreal group from from from benjo's group um
they were just doing attention on top of a recurrent neural network and then what this guy said you
know very very important right and again there's a history in machine learning of these things happening
so like actually once you have attention you can take out the recurrence and now you can just train
in parallel and now you can train on much bigger corpora this is really the key uh thing that happened
in that paper right now this system that they have at the end of the day is anything have and have even
more so today is is anything but simple it's a huge pile of different stuff and hacks left and right and
and you know this actually slows down progress right is that it's there's nothing simple about the i mean
things tend to become simpler over time as you understand the better and i think that's the case
with transformers but they have all these different kinds of layers and mechanisms and whatnot which we
could go into but like a transformer the full architecture is a is a big pile of complication and
you know that that holds us back yeah uh how does uh i i don't mean to jump around but uh as i
mentioned uh sap uh hawk writer uh who had developed lstms uh much earlier how does that relate to
attention because it's it's it's it's it's uh looking uh back a certain number of tokens and and holding
them in memory and that to me sounds like attention right um so uh lstms were very hot in machine
learning for language at one point and that was the point that which people are already focusing on
language but attention had not been invented yet yeah and and and lstms are a type of recurrent neural
network the problem with recurrent neural network that really for example made yosha benio stop working
on what was what is called the vanishing gradients problem right you think about it when you do back
propagation the further back you go the more diffused the signal becomes it's like the the neurons at
the output layer yeah you are to blame you're clearly wrong right once you get to a neuron like you know
a hundred layers behind right it's like there's a million paths to that neuron like who knows if it's
to blame or not i guess like if you if you think of like of a drop of ink you know diffusing through you
know a you know a glass of water right as time passes it becomes spread over everything and if
this is the credit or blame that you have to assign right like a recurrent neural network over time
right even if it's just one layer because you use it over and over again it becomes you know an infinite
you know layer network and the signal just diffuses through it until it's like you know the the ink
all over the water you don't there's no you can't learn and there's nothing to learn everybody's
equally to blame or not to blame right and and lstms were a solution to this problem and the solution
to this problem the lst and you know people tried a bunch of things at the time neural turing machines
and blah blah blah but lstms again which were had invented been invented way before uh by sepp and his
advisor uh jürgen schmidhuber uh notorious in machine learning for for various uh reasons um their solution
was actually to do something that works a little bit like a computer it has these memory cells and
you can actually write stuff into the memory cell and then it stays there until you decide to access
the cell which is really like a von neumann computer that we all use works and then what they had was
these gates that controlled access to the memory there was like the forget gate etc and what it did was
it allowed you or didn't allow you to you know write into the memory read the memory etc etc
and again the key was that there was this wasn't digital right it was something that you could
gradually learn right which again is what attention does later it like it doesn't just say like oh i'm
going to pay attention to this it's paying attention to all the past you know to what is called the
context window and this is like well the back prop says pay more attention to this guy and then the
back process pay even more attention to this guy right and and lstm was honestly a more complicated
than hack here and more unwieldy way to accomplish this effect attention in many ways is a misleading
term right because it really doesn't have anything to do with psychology you know the psychological
you know uh version of that term in fact the irony is that kyung hun and and and dima bada now
dima is the intern who invented it they didn't call it attention uh yoshua banjo came at the end of
the paper and put attention everywhere and then they took it out again as they tell the story uh
except they left it in a bunch of places and then you know and then the rest is history so then you've
got uh the transformers um again this is all in uh in the connectionist uh uh stream and then there's
there's some things happening uh off to the side uh there's uh or maybe we're going to talk about this
uh next uh in the next episode but there's uh you know generative uh adversarial networks uh and uh
well let's talk about gans is that or or is that on the evolutionary uh uh algorithm end of things
no very good so so gans generative adversarial networks um were very popular for a few years
uh in in in in ai um and now it's kind of died down and and and it's shrewd of you to point out
their connection to evolutionary learning most people don't make that connection but i think it
is a very important connection uh and gans are actually how the term generative came so the term
generative from the beginnings uh was is the term from statistical uh uh machine learning and the idea
of a generative model is that it's a model that generates the data right it's like
i want a process by which you generate samples right and then so let me turn this the other
way around right this is actually the whole bayesian way of thinking is i see a bunch of data i'm going
to postulate that they were generated by some process and i'm going to try to identify what
that process was this is actually what generative learning is so you come up with this model like
a vision network this is like well this is how you generate patients and their symptoms first you
decide which disease they have and then you decide which symptoms they have what fever are they
going to have based on the fact that they have covet etc etc you generate the data and this is why
it's called the generative model in opposition to discriminative models which is what most of
machine learning is about which is to just go say like okay i'm going to go forward from this data
and try to decide you know what what you suffer from or whether it's a cat or a dog as opposed to like
how do i generate you know a cat or a dog right so there's always been this thing in machine learning
between generative and discriminative discriminative always has always dominated and and really still
does because it just work it's it's an easier problem and it works better it's in some sense less
satisfying because like what is the deep science here i'm just hacking to get the result i want to
know you know like for example physics right is about generative models right like here's how the
universe is generated so there's a certain type of person who really likes you know who thinks
generative models should win at the end of the day now you know young goodfellow who's the guy who
invented gans was this another student at the same time in yosho benjo's group who had a huge group
and the the so it's not entirely an accident that so many people uh came out of it and they were
actually trying to solve a different problem which was the problem there's a type of neural network
which again is a generative model right not trained by backprop and by the way these
were the ones that jeff hinton was always into jeff hinton even though he's a co-author on the
paper never liked backprop he's actually on record they're saying that backprop is not the future
he actually told me at one point the only reason i i publish papers on discriminative i
the only reason i do this community of learning is in order to publish the papers
because he was what was was generative neural networks that actually generate you know uh stuff
right uh but the problem with those new networks is that the inference is intractable and so
they were trying to come up with a way to make it tractable right there's a long history of trying
to do this in various quadrants of course including the bayesians and whatnot and and and generative
adversarial networks were actually a way to do this and you know without going into too much of the of
the history or the details what is the idea is that the adversarial network is a game between the
generative network that generates data and the discriminative network that tries to classify it
so actually have a generative model the classifier working against each other which is a brilliant
idea because the the the the idea is that the the the classifier is trying to distinguish between
real data and the data that was created by the generative network and so that forces the generative
network to get better otherwise in the beginning it's basically like you don't look anything like a cat
falls right and then it gets better but then that forces the discriminator to get better as well
so they get into this evolutionary arms race right it's co-evolution is the term in evolution right
it's like the predator and the prey that generated in the discriminator and this was like a genuinely
new idea in the context of neural networks in fact at the time you know yann lecun was going
around saying like this is the most important idea in neural networks of the last 20 years
right then it petered out because it turned out you know it's very unstable there's a lot that goes
wrong people never really to my disappointment at least you know you want to take that further into
the whole evolutionary realm of things because co-evolution is very powerful so maybe that's what
we need in machine learning but like the it was too hard to to you know to cut a long story short
so it kind of petered out but that's where this whole thing of generating deepfakes began
right he and in his paper had this few you know grainy images that were generated right they were
like i mean at the time this was actually very impressive right but then like if you think about
applying more power to this the images get bigger they get you know final resolution you
get from images to video and the whole you know deep x the industry was born so even though now
there are other techniques like you know uh diffusion and whatnot which we could also uh talk about
being used to generate these things so so guns are no longer the state of the art but the whole
like that term generative in the neural network context really took off there and this whole notion
which a lot of ai things gave you all but they're like you know how can i dazzle you with generating
text or generating images really started with with gains yeah uh where should we go from here we don't
have a lot of time left i wanted to i don't know if it if it needs a big chunk of time on its own but
then you have rich sutton working on reinforcement learning sort of coming at it from you know the
behavioral psychologist side uh and that to me is probably the most powerful idea uh and it's it was
certainly applied but it seems to have uh become uh the darling of of generative ai right now with uh you
know this deep seek is training with pure rl and can you talk about how rl fits into all of this
absolutely so reinforcement learning is actually a different so there's supervised learning and
supervised learning reinforcement learning it's a different type of learning that again has been
around for decades it is inspired by uh animal psychology in fact that's where the term comes from
like you know pavlovian experiments you reinforce behaviors by giving rewards and et cetera et cetera
the the the rich sutton has been the long-standing leader uh uh in that area and so what is the
basic idea in reinforcement learning it's it's it's important to start by understanding that and by the
way reinforcement learning like supervised learning can be applied with any of the paradigms but it has
always been most popular with neural networks yeah so with supervised learning you have a teacher
the teacher says aha right answer like you know yes this is a cat or like no this is a dog like
you label things with the right answer yeah which makes learning easier but is you know where where
does that data come from there are many domains where you don't have that if you look at how animals
learn right or like how we learn right you touch the stove and then you learn not to but the interesting
thing about enforcement learning is that i feel the burn when i touch the stove but i really shouldn't
even have moved my hand towards it so you need to to to uh um back up that signal to to where to you
know so supervised learning if it was just like at the last moment don't touch it that would be
supervised learning the whole idea in enforcement learning is they have this thing called delayed
rewards you do something and you only see the result later and then you need to propagate those
results back to where you should have taken the actions a famous example of this is alpha go yeah
right and indeed reinforcement learning goes back to the 50s and you know the paper that coined it to
machine learning had the precursor reinforcement learning to learn to play checkers and they learn
to play that at human level and the idea is this is like i play a whole game of checkers either
computer with you the human i don't know if my moves are good or bad the only thing i know it
is at the end of the day i won or i lost or we tied right so there's a very small piece of of
reward as it's called the word is like you won right you get a you know you get a reward right you
got to and reinforcement learning is a set of techniques or really it's just the problem
reinforcement learning is the problem of like how can i take those rewards and propagate them back to
where i need to make a decision so that i make the right move that many steps further will lead me to
the game right and this you know combined with with with neural networks uh for for vision applied to
the to the the go board was of course what what let you know uh um a deep mind um you know beat lisa
doll and etc etc now uh reinforcement learning as done by rich sutton and other people for a long time
was always very theoretical they wanted to they did these tiny little experiments that were you know
not convincing to anybody honestly and they tried to prove theorems what the deep mind folks did when
they came along that was very important they said like you know to hell with all that we're just going
to engineer the heck out of this and get it to win and they did right and so again for a while deep
reinforcement learning was very hot there were like many papers about it um they actually started by
doing it in in video games in atari and then and then went on to things like going chess that
unfortunately has petered out also because it didn't it didn't the deep minds agenda when they
came out was deep reinforcement learning and their ideas like we're going to start with these games and
then we're going to move to robots in simulation and final robots in the world and this is how we're
going to solve ai right there mr service was going around saying like where the apollo project ai we're
going to get there they don't say that anymore and so um these days reinforcement learning as you
pointed out is again popular but honestly those of us who know this history at this point have some
justified skepticism because we keep seeing this over and over again which is people come out saying
like oh look at this great success for reinforcement learning and then when you look at it more
carefully there was no great success what they were doing was effectively supervised learning or
could have been done by supervised learning in fact the main there's this thing called reinforcement
learning from human feedback that open air popularized that they say is very important
to make you know their chatbots work and whatnot there was this paper out of stanford that basically
showed effectively you're just doing supervised learning yeah so i would say the same with deep seek
until i see proof to the contrary i will assume and again the evidence is is points in that direction
at least as far as i can tell that they're not there's really no enforcement learning magic going on
the problem with reinforced so the appeal of reinforcement learning to the people who really want to
solve ai is that like clearly we do something like that there's even a correspondence to some of
the neuroscience with dopamine circuits and whatnot the problem is that people have disappointingly
despite so much effort they have never really truly managed to make it work in fact it boils down
to this if your rewards are delayed are it's learning from sparse delayed rewards if the rewards are
not very delayed they're not very sparse then you can just use supervised learning which is way
easier if the words really are delayed and sparse then it doesn't work so so far we haven't really
solved that problem so when you see claim in the meantime however in the last few years
reinforcement learning has just become the sexy term that you can throw around and you call your thing
reinforcement learning and it sounds more impressive even to yourself which seems to be also what had
happened with with deep seek yeah uh okay uh we have a couple of minutes left where should we go from
here uh uh well uh let me take you that question let me take that question the following way where should
the neural network community go from here right and and i think the higher the bit is as we alluded to
earlier what you've seen is broadly from the point of view if we have but even from the point of view
of machine learning and then deep learning what we're seeing is more and more people doing more
and more research about less and less right so so i think what where we really need to go from here is
we need to broaden the scope to all i mean by the way as an example uh there are papers now showing that
maybe lstms and recurrent networks aren't such a bad idea right a lot of things that seem dead in machine
learning or ai have a tendency to come back you know neural networks being one of them so i wouldn't
be surprised at all if one of these things has dethroned transformers by next year or or maybe it'll
stagnate for 10 years or who knows right but i think a lot of people should be doing a lot we should really
should let a thousand flowers bloom and right now it's basically one flower blooming or one petal of one
flower blooming and that's not very healthy yeah and that phenomenon because uh you we we've talked
about it in in you know during the supervised or the supervised learning vision phase when everyone
is focused on one narrow aspect and then they're forgetting about the basians or something that's
a matter of funding isn't it oh funding is a big part of it in fact minsky and papert publicly admitted
that the main reason they wrote that book that killed neural networks for a while was that you know
darpa was the big funder of ai research right so historically the biggest funder you know when we
finally get to ai we will have darpa to thank as much as anybody for having you know stuck with it
until you know the industry took over but they were getting jealous that you know neural networks were so
sexy and they were getting all the darpa funding and symbolic ai wasn't getting enough so they said like
we got to get rid of these guys and so they did so funding has always been an issue right i mean
particularly in in research and industry as well although the phenomenon is a little different
darpa for a long time after after that in fact when i became a researcher darpa did not fund much
machine learning research if any because the the consensus which in the field and among darpa
programs like like machine learning is always the time in the meantime machine learning was taking off
and darpa was still not waking up finally they woke up in the 2000s and started you know getting into
machine learning and again like the the funding the funders themselves whether in industry or in
academia or you know the funding agencies they're very prone to this um you know hurt behavior it's
like the vcs right vcs are supposed to be looking out for different things but they're all looking out
for the same things so you know you need to inject some you know in there's this you know um gradient
descent right there's like look going looking for the optimist what new networks do but there's this
method called simulated annealing where you actually inject noise which seems like a bad thing but the
noise actually helps you not go straight to the local minimum you know find find a deeper one we
need more simulated annealing in in ai research when you need somebody you need somebody you want to hire
people fast the longer you wait the more the market dress the more your needs drift for example you just
realized your business needed to hire somebody yesterday how can you find the right candidate
fast just use indeed when it comes to hiring indeed is all you need stop struggling to get your job
posts seen on other job sites indeed's sponsored jobs help you stand out and hire fast with sponsored
jobs your job postings jump to the top of the page for your relevant candidates so you can reach the
people you want faster and it makes a huge difference according to indeed data sponsored jobs posted
directly on indeed have 45 percent more applications than non-sponsored jobs one of the things i love
about indeed is that it makes hiring so fast i've searched for candidates before and it can take months
particularly if you're just working through word of mouth in those days i didn't have indeed plus with
indeed sponsored jobs there's no monthly subscriptions no long-term contracts and you only pay for results
how fast is indeed in the minute i've been talking to you 23 hires were made on indeed according to indeed
data worldwide so there's no need to wait any longer speed up your hiring right now with indeed and
listeners on this show will get a 75 sponsored job credit to get their jobs more visibility at indeed
dot com slash eye on ai just go to indeed dot com slash eye on ai that's indeed i-n-d-e-d dot com
dot com slash eye on ai all run together e-y-e-o-n-a-i go right now and support our show by saying you
heard about indeed on this podcast indeed dot com slash eye on ai terms and conditions apply hiring indeed is all
you need
you
