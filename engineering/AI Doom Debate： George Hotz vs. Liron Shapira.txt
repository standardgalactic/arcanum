Welcome to Doom Debates. Today I'm going to play you one of the most important Doom Debates that
I've done. It's from last summer, August 2023. It's between me and George Hotz. George is a
notable hacker, entrepreneur, really smart and productive guy. I'm going to read you his Wikipedia
bio in a second. The context for this debate was that he had just debated Eliezer Yudkowsky,
moderated by Dwarkesh Patel a couple weeks earlier. And he was on Twitter Spaces trying to talk to
more Doomers, basically. He was still thinking about the topic of AI Doom in the wake of his
Eliezer Yudkowsky debate. And I wandered in there totally unplanned. And I started debating with him
one-on-one, kind of took over the space. There were like a couple thousand people listening.
And after we'd been going for a little while, somebody started recording it. We ended up going
for like almost three hours total. The recording ended up maybe half of that, the back half.
But luckily what we talked about in that back half was pretty representative. So I think you're
going to get a quality debate with just me and George on that back half of his X space.
The feedback I got was that people preferred my debate over Eliezer's, not because the ideas were
better. Of course, I consider myself a stochastic parrot for Eliezer Yudkowsky's ideas. I'm not really
contributing many more key ideas to the table. But the debate with just Eliezer and George got kind
of derailed. They didn't really focus on what are the big cruxes, what are the big themes. They just
kind of went all over the place. Whereas I was very focused on saying, hey, high level, why do you
disagree? What would it take to convince you? So I'm happy with this debate. I think it's good content.
And this was actually one of the moments where I realized that there's an appetite for this kind of
debate, that this kind of debate is productive. It moves the ball forward in helping society react
to AI when two intelligent, informed people lay out their positions and see where they disagree. And we
need more of this. Hence, Doom Debates podcast. Okay, George Francis Hotz, born October 2nd, 1989,
alias Geohot, is an American security hacker, entrepreneur, and software engineer. He is known for
developing iOS jailbreaks, reverse engineering the PlayStation 3, and for the subsequent lawsuit brought
against him by Sony. From September 2015 onwards, he has been working on his vehicle automation
machine learning company, Comma.ai. Since November 2022, Hotz has been working on TinyGrad,
a deep learning framework. So without further ado, please enjoy me debating George Hotz from August 2023.
I'm curious, George, because sometimes you might break the mold of like an EAC on this, which is like,
if you have a super intelligent AI running in a data center, do you think that it would have a low
chance of being able to outmaneuver humanity from that position? Or do you think it's actually a
pretty powerful position? You see, but like, I can do really nothing there besides reject the premise
of that question. Again, yes. If today, an AI that had a hundred times more intelligence than all of
humanity combined was beamed off an alien spaceship and connected to the internet, yeah, we're dead,
of course. Okay. I mean, that's, you know, I'll agree with you on that one. I mean, that's great that
you, you know, that's not your sticking point. No, no, that's not my sticking point. I also believe
that if aliens, and I said this in my casting debate, if aliens come here, we're dead, probably.
Right? Like, good thing none of these aliens are actually real. Because if aliens actually show up,
again, they might be friendly aliens. But generally what happens is when two civilizations meet,
the one that has more control over energy dominates the other one. And considering they traveled halfway
across the galaxy to get here, they have a lot more control over energy than we do.
Right? Okay. I mean, I would analyze the situation as the aliens are farther along than us
technologically, and they're much more powerful optimizers. So whatever their utility function
says is what they want is what's going to happen. Again, the powerful optimizers thing,
this to me is not the fundamental. The fundamental to me is how many, how many terawatts of energy is
they're shifting the same. I feel like my principle is going to probably be the better one to use
compared to how much energy they're using, right? I mean, I think utility theory and optimal actions
explains the situation better than, oh, they're using a lot of energy. Because if they're energy
efficient, that doesn't make me feel better about the prospect that they're going to wipe this ass.
I don't care if they're energy efficient or not. I'm saying that they're using, if an alien spaceship
shows up here that went near the seat of light, it showed up with an energy capacity that's 1000x human
civilization. Yeah, I guess. But I mean, but what if they use small particles and they're energy
efficient? Like, I feel like you're very obsessed with energy in this scenario. No, but I'm not like
efficiency doesn't matter whether their engines are 10% efficient or 90% efficient. I don't care about
that. Like, again, I'm assuming something that can harness way more energy than humanity can.
That's the scary thing. The scary thing is the energy, right? What makes nuclear bombs scary is the
energy. Well, it's the fact that we no longer have actions available to us to stop the explosion,
not the energy per se. Let's, I'm going to take this in a little bit of a different direction. And
I'll talk about a threat that I actually do believe super intelligent AIs do present.
Right. I don't, I don't think there's any way they're going to kill us by rearranging around.
Right. I think that this is, this is in the realm of hyper sci-fi and like, I just.
I mean, how being in the realm of hyper sci-fi, you know, a counterargument. I mean,
we are 2023 is a year in the realm of hyper sci-fi. No, but like, energetics questions.
You know, I'm going to engage with a doomer scenario. Um, and maybe this is a more plausible
doomer scenario than, than, than AI's rearranging around. Um, the scenario goes something like this.
What gives intelligence power is its ability to manipulate. Right.
Okay. Sure. I mean, that, that's similar to what I was saying about, you know, mapping goals to
actions. Well, yeah. Okay. Whatever his goals are. Right. Like again, this, this thing where
people are worried about, like, I'm going to turn the universe into paper clips. Is anybody afraid
of me? Is anybody afraid about a man running up and down the streets of New York screaming that
he's going to turn the universe into paper clips? No. No. Cause we all laugh at him. Right. Like,
no, you'll see, man. I'm like, okay, we're laughing at him. Right. Um, so I do think that there is a
threat from AI. And I think the big threat is if an AI exists in a data center and all of humanity is
wiped out, right? There's the AI sitting there in the data center. The AI was not going to have much
lot manipulating chimps into doing what it wants. I mean, if, you know, if, if it can go through any
channel or, you know, or engineer them, then it probably can. Right. It can treat them like a
robot. No, no, no. But it's sitting in a data center. The internet broke. Oh, sure. Yeah. Yeah.
But it can chain together steps, right? It can, it can make that outcome happen. There's a lot of
causal pathways from wanting that to happen to making it happen. But it's, it's sitting in a data center.
So you think that's successfully air gapped? I feel like you would know that it's not.
No, no, no, no, no, but I agree with you that it's not air gapped, but all with respect to human
civilization. I think it's sufficiently air gapped with respect to chimp civilization.
If you're not, if it's not air gapped from human civilization, it's not air gapped from chimps,
right? You just, you hire some humans and you teach the humans to manipulate the chimp.
No, no, no, no, no, no. All the humans are dead, right? This is like, this is like,
this is like a Will Smith, Zombieland world. Okay.
Or I am legend. All the humans are gone, right? They, they bioterrorism themselves to death.
Yeah, I mean, look, it's like if you put me in the woods naked and a chimp comes up, I'm not going to be able to fly the chimp to the moon, right?
So, like, I agree, there's some, you know, you have to bootstrap something.
But this is exactly my point, right?
So, the AI really only gains the power over reality through humans.
I mean, not necessarily, right?
I mean, there are causal linkages from bits in a data center to many actuators, right?
So, there are a lot of, I mean, most physical things that are connected to any sort of grid in modern civilization have a causal pathway that doesn't involve humans to the internet.
But my point is it involves modern civilization.
This is what I said about human intelligence, right?
Like, human intelligence is not externalized.
Human intelligence is externalized.
Our intelligence lives everywhere in our civilization.
Yeah, which is pathetic.
I mean, it's just a pathetic quality of the human brain, right?
That we can't even store enough intelligence to bootstrap civilization.
It's a true fact about the human brain, whether we can call it pathetic or not.
I don't know about that.
Like, but yeah, no, like you said, you're human, you know, you can't fly a chimp to the moon.
But I thought you humans have been there.
Yeah.
You know, it's complicated.
Okay.
So, the AI, like, in order for it to act, right, it uses this externalized intelligence.
Right.
I mean, yeah, it could, but it doesn't need an externalized intelligence because the premise
is it's very intelligent itself, right?
And so, it just needs to find a causal path.
It just needs to map the goal to a sequence of actions that will make it happen, right?
So, it doesn't need additional intelligence necessarily.
No, no, no, no, no.
Okay.
The point that I'm driving at here is saying that, like, the reasons that humans evolved
intelligence is for politics, right?
Like, chimpanzee politics to manipulate other humans, right?
I mean, I'm not sure.
There's a few possible reasons.
I don't think it's all politics.
It's already happening.
No, no, no, no, no, no, no, no.
We're not discussing politics.
We're not discussing politics.
My point is, intelligence is really good at one thing, and it's manipulating less intelligent
things.
Again, that seems like one of those generalizations, right?
I think intelligence is good at making outcomes happen, right?
And manipulating lower intelligences is only one small subset of that or medium subset of
that.
No, Matt, you talking to a rock is going to do anything.
Right.
So, talking to a rock is not a good use of intelligence, but throwing a rock is.
And it wouldn't be an example of manipulating less intelligent things.
You're going to throw a rock with your intelligence.
You're going to throw the rock with your muscles, right?
How far you can throw a rock has nothing to do with how smart you are.
Yeah, but being intelligent does have rock-throwing advantages.
You know, like, in Angry Birds, I suspect somebody smarter could beat that game faster.
Yeah, but, like, no.
Like, actually moving a rock, right?
Like, you're trying to move a rock in the real world, right?
Sometimes, this is something I like, think about all, like, the doomers.
It's like, there's a, there's like a touching grass element that's, that's, that's...
This thing, it's like, okay, but you actually want to move a 10-ton rock, right?
This is force times distance.
Sure, I'm just saying intelligence helps, right?
It's not on the claim.
Intelligence helps.
But my basic argument with intelligence is that you're going to get diminishing returns.
I guess, I guess I can't really, like, say it more than that.
And this is a conjecture.
But, like, if you didn't get diminishing returns from intelligence, evolution would have...
You don't think evolution could have stumbled upon brains that were four times bigger?
So, what you're seeing when you look at the evolutionary record is the marks of increasing returns.
So, until we got to humans, when we were just messing around with other animals, it didn't realize that there was this basin of attraction where once you unlock, you know, general thinking, reflective thinking, whatever secret sauce humans have, once you unlock that, you do get a cascade.
And that's what we've seen in, like, the last million years.
If you look at the evolution of the human brain, you're getting a lot of cognitive returns for very small genetic modifications.
Sure.
So, for a long time, like, you can think about the optimization landscape.
We're moving along a plateau, and then we find a great gradient, and we exploit that gradient, right?
Right.
And the gradient still seems steep.
It was stopped in its tracks by, for example, that the head needs to fit out of the pelvis, among other reasons.
It stopped it in its tracks, but there was room to keep growing.
And we're about to, you know, reestablish that kind of curve when we train the next models.
I think kind of an important point here that George is alluding to is, like, John von Neumann is not better than me at tic-tac-toe.
Like, mu zero is not better than me at tic-tac-toe because there's an upper limit to how good you can be at tic-tac-toe.
And in the universe, that upper limit is dictated by physics, and it's dictated by real energy requirements.
Yeah, I haven't really talked about that, right?
But it's just, so my view, which is a strong view, is that that ceiling gets very high very fast.
You had to go to tic-tac-toe because the moment you start sighting a slightly more complex game than tic-tac-toe, humans start, you know, we lose our footing.
Let's, I don't think this space has gone on for a bit, but let's try to, let's try to bring it to a good close.
So I think we both do kind of agree on the end point, right?
We both agree on the end point of super intelligences with godlike powers, right?
Right, which is, yeah, I mean, and that's nice.
So we both think that in a hundred years, some super intelligence is going to be optimizing things pretty hard.
Sure.
I think a hundred is still a pretty short prime scale.
I think that we can definitely say in ten thousand, whatever exists in ten thousand years is going to be wild compared to like a human today, right?
Like we're going to stand there and look like an ant compared to this thing, right?
Yeah, it's going to be very sci-fi.
It's going to be very sci-fi, right?
So we both agree that we're getting there.
I think there's a question of how fast we're getting there.
I really think that the only difference between Doomers and EAC is a question of how fast we're getting there.
Well, that and also the question of do we lose 99.9% of the value in that process, right?
Because we both agree that the super intelligence is going to optimize.
You seem to think that there's still a good amount of value left.
I think all the value has been rapidly extinguished in that scenario.
So I'll say two things to this.
One, maybe another difference is that you think humanity is going to be dead.
And I think humanity is going to be kind of morphed into all sorts of shapes and forms.
I think some humans are still going to be living in an enclave, trying to hide themselves as best as they can from technology.
Some humans are going to be on rockets.
Some humans want settled planets.
Right, 10,000 years.
Yeah, I mean, the idea that a human cowboy can escape the smartest thing in the galaxy seems far-fetched to me.
But it doesn't have to escape, right?
It doesn't have to.
Like, the AI really doesn't care about you.
Yeah, so this is a point you brought up in the debate with Eliezer, but the reason why an AI that doesn't care about you still ends up killing you is just because whatever it wants to optimize, you're not part of the plan.
And not only are you not part of the plan, but you have a chance of building up your own AI to, like, mess with it.
Why should it let itself be messed?
It's just going to wipe you away.
So, I didn't come back.
I listened to that part in the debate, and I'm like, I can't believe I let Eliezer frame it.
Every time he frames it as, like, that the humans and the machines are different, I'm losing.
I don't think that the threat is that some little, puny, 20-pay-to-flock humans are going to build another superintelligent.
I think it's that the nearby superintelligence is going to clone itself and then decide that it wants to take their resources.
Yeah, and then you guys got into the discussion of, like, the multipolar world, right?
It's like, what happens if the initial conditions are such that there's a few local clusters of superintelligences and they end up having to duke it out?
Like, there's no monopole.
A few local clusters?
I think there's going to be billions of clusters.
Well, I just, I think that they expand so fast in the resource grab that you are going to get a situation where the first one or few are really going to be, you know, they have a major timing advantage.
This comes down to another, like, this comes down, I guess this is really all a question.
I mean, maybe another, like, big difference is, Eliezer, you can't see if you wrote a, the microeconomics of superintelligence or the microeconomics of intelligence explosions.
Yeah, I like that.
Yeah, and I think that this is another, like, question.
I think that the returns from intelligence are pretty quickly diminished.
Right, yeah, I mean, that's what we just covered, right?
Like, I'm afraid that we are on an inflection point where we're seeing them accelerating.
Hmm, okay.
I very much see the opposite.
I very much see a world where, yeah, like, things are going to be superintelligent, but okay, you get, like, 10x, like, you get, like, 2x more intelligence.
For 10x more power.
Yeah, and I agree that you can find, I think you're going to see an S-curve with, like, this specific version 1 LLM model that we have now.
You know, arguably, or maybe GP4 to GP5.
I can't tell you confidently that it's going to look like a bigger step than GP3 to GP4.
But that's a minor point.
I'll use something way more.
I forget the LLM.
I can use something way more generic.
Okay.
So the loss function of those LLAs is perplexity on the next token.
Right?
Okay.
And we can go long back before LLAs.
We can go way back.
We can go, I mean, you can measure perplexity in the next token using GZIC.
But any compressors, it's compressed.
Right?
So this compressor, the text, all of that human text has a columnogram complexity.
Right?
It has a minimum size of which it can be compressed to.
Right.
So this necessarily is a curve that, and you can't, you can't, the columnogram complexity
is, in theory, yes.
You can have something that compresses it to the columnogram complexity, but no further.
Right?
And you have to put in more and more energy to get closer and closer to that point in the
curve.
Yeah.
I mean, but there is a theoretical ideal, like, you know, AIXI, if you've ever seen that
model.
It's just the idea of, like, an ideal scientist, an ideal algorithmic scientist that knows the
exact Bayesian probability of everything.
Right?
As you approximate that ideal with heuristics, you can get something that's very, very smart,
even when it looks like you might not have a ton of data.
And if you thought AES-256 was hard to crack, AIXI is uncomputable.
Yeah, it's uncomputable, but it's a theoretical ideal.
And there's a large gap between what human brains are pulling off and the theoretical ideal.
There's a big space between those things that we are now entering.
I am not sure, again, with respect to what?
With respect to intelligence or with respect to power?
Just overall AI progress, right?
So I've drawn a diagram where I put two axes on my diagram.
I made it like a plane.
And the first axis was optimization power.
And the other axis was generality.
And when you can beat the human brain or when you can match the human brain at generality,
because I do think we're fully general or very close.
When you can match the human brain at generality and beat it at optimization power,
that's the moment when I think it's game over.
Um, again, like optimizing towards what?
Just a generic goal-to-action mapping.
So if you take any particular domain, you can see what optimization means in that domain.
Right?
So in chess, it means you take any arbitrary board state and tell me the best action to
get to the min-end state.
And in AES-253, it means finding the key.
Yeah, so, I mean, you just have to be as general as humans and better than them.
So I agree.
There might be contests where it's like, okay, you know, reverse encryption, where neither
the human or the AI do well.
Fine.
But there's a lot of contests where the humans do pretty well, and the AI does way better.
And that's what's gonna, you know, be dangerous.
Yeah, it depends what these contests are.
Again, I also, like, why do you say that it's the humans versus the AIs, right?
What if there's a billion AIs out there?
This doesn't give you any comfort?
I mean, so this actually gets into the weeds a little bit, but it doesn't give me any comfort
because when you have multiple agents that are all smarter than you and haven't been properly
initialized with human values, all they do is just team up and do their own thing, and
you're still in the dust.
Like, you're not getting any advantage from this scenario.
Why?
Okay.
How do they team up?
So this is what you talked about, Eliezer, which is, so Eliezer has a research paper about
decision theory, where he has a way where you can have prisoner's dilemma type problems
where multiple agents can manage to cooperate.
But, like, regardless, even if you don't buy Eliezer's research paper, I just don't see
how you, the guy in the corner while the giants are fighting, are gonna win.
Uh, I don't have to win.
Think about all the ants that watched World War II.
They were like, damn.
Yeah, but you realize, if you look at my house, right, there's ants in my house right now.
If I could snap my fingers and all the ants would drop dead, I would.
I just don't have, I don't know, I just can't give you a causal sequence to do that, but if
I had one, I would use it.
But no, you can't snap your fingers because you're not a god.
If you want to devote the rest of your life to getting all the ants out.
Yeah, but the complexity class of killing the ants is not going to be that high for an
AI.
Like, the universe is hackable.
It has a god mode.
Wait, I very much disagree with this.
Um, but let's talk about the ants, right?
You're way smarter than the ants.
If you devoted the rest of your life to getting all the ants out of your house, you could.
Right, yeah, so it's, I mean, look, the ROI for me, right, it's, the effort is insane,
right, for the benefit of it.
It's a tiny, tiny benefit, right?
But the AI, if the benefit is non-zero or if I can just use the ants for energy or whatever.
But what do you mean if the benefit is non-zero?
You're assuming, okay, okay, this is good, good, good, good.
I really like this because we can go further with this.
Yeah, I mean.
Yeah, go ahead.
No, no, this is, this is, okay.
A lot of the time, so look, I've run a company for the last seven years.
A lot of the times we don't do stuff, it's not because it's a bad idea, but it's because
we only have limited resources to deploy, right?
It's not because the AI might vaguely want you dead, okay?
Does it want to deploy a lot of resources into trying to make me dead?
And you say that, no, but the AI is going to have so many resources.
I totally disagree because the minute that AI wastes time killing humans, like you can
waste your time killing ants while the other humans are out going to their jobs and making
money.
Well, just to elaborate on my mental model, it's not like I think the AI is going to wake
up and be like, I need to target George, right?
Like I need to clean.
It's not going to be like OCD about getting rid of all the humans in its house, but there's
a few reasons why in the course of optimizing whatever utility function, it just wants to
joyride around the galaxy.
In the course of optimizing that, it's like, okay, great.
Let's use all the atoms in the earth to make my spaceship, to make my Dyson swarm.
You know what the fight effect would die.
There's different reasons.
You're assuming that nothing at all is going to oppose it.
What about all the other AIs that also want all the atoms in the earth?
Yeah, for sure.
I mean, I do feel like you're kind of introducing a new argument though now, right?
Because before I was just making the argument of why by default, when an agent optimizes
toward a utility function, why by default do you as the human bystander get killed?
Again, we're back in the world with the physicists predicting the horse race by first assuming
the horses are perfect spheres.
Well, I mean, usually a deep principle, a deep principle can often tell you a lot.
You know what?
Holy shit.
If that machine was actually running uncomputable AI-XI, I'm dead.
Yeah, sure.
Yeah.
And I'm telling you it's something closer to the, on the spectrum toward AI-XI.
And that has a lot of implications.
It's nowhere near AI-XI.
The gap between computable and non-computable is way larger than the gap between computable
and AES-256.
Yeah, I get that.
But there's a lot of headroom above humans, which is still very computable.
But there can be a lot of headroom above humans, right?
Like, yes, there's definitely a lot of headroom above humans if you're willing to use power.
But just like humans are so much above the ants, humans do not waste their time exterminating
every ant.
Right.
And like I said, the AI won't waste its time exterminating, right?
It'll have side effects that end up killing the ants.
So, for example, if doing a bunch of operations on the Earth, you know, heats up the temperature
or blocks the sun, right?
That's going to mess with the human's environment.
This is embarrassing.
Wait.
Yeah, and also, just let me throw in one more big reason.
You might just realize, hey, look, one day George might make his own AI.
I don't want that to happen.
Let me just kill George.
This is not a real threat.
This doesn't make any sense.
It's not a real threat that you can make your own AI.
Wait, wait, wait, wait.
Hang on.
Don't you run an AI company?
I have the perfect analogy.
I have the perfect analogy.
We should kill all the ants because one of the ants might make a nuclear weapon.
Well, that's implausible.
But you do actually work in the field of AI, George.
Yeah, but the AI's, all right, that's funny, pedoflop me with access to
a megawatt of power are going to make are so laughable compared to these
terawatt AIs flying around in space.
Right, but now you're being pessimistic about yourself because aren't you
actively trying to bring about, you know, the AI revolution, right?
So now you're suddenly, suddenly you're very humble about what you're capable of.
Well, yeah, but that's assuming that I'm just, I'm a human and I'm not
going to upgrade myself.
If I start upgrading myself and I get into terawatt AIs.
Right, and the AI is going to realize that, right?
It knows that there's some danger.
And it's just like, look, wiping you out, it starts, it's an ROI calculation.
It's like, yeah, is the return enormous?
Maybe it's medium-sized, but is the cost high?
No, the cost isn't high.
I mean, yeah, you know what?
If we saw a bunch of ants starting to build a nuclear weapon factory, we
probably killed them.
Yes, that's true.
Right.
So, like, I mean, humans are going to have all sorts of different strategies
for surviving the future, right?
Okay, but these are going to be really weak strategies relative to the AI,
especially given that it's going to disconnect all the, you know, comforts
of our modern civilization.
We're not going to have a power grid.
Do you think that humans one day are going to wake up and kill all the dogs?
No, because one main reason is we value dogs, right?
But then the other reasons are just logistical, right?
We're not going to go hunt down dogs because we don't think dogs are going
to evolve and challenge us.
Why do you think none of the AIs are going to balance humans?
Why do you think?
God, look at the cute humans.
Right, so now this is a totally different argument, right?
Because the premise of our discussion just now was you have an agent that has
some utility function that it's effectively optimizing, right?
That was the premise we were working under before.
My argument is that all these different kinds of AIs are going to exist.
There's going to be some AIs in the world that like you, some AIs in the world
that want you dead, and some AIs in the world that don't care about you either way.
That's probably actually most AIs, right?
And I will give the analogy that the exact same thing is true about people.
There's some people who like me, some people who hate me, and most people
don't care about me.
Right, yeah, and again, my claim is an AI that doesn't care about you is the AI
that's going to kill you.
It's more likely to not care about you than to like or dislike you, and it's going
to kill you while not caring about you.
So again, and then this comes down to a question all about energetics, right?
This comes down to a question about if an AI is, we're talking these AIs are like
multi-paid-a-lot machines that are harnessing the power of stars, then yeah, it might
accidentally kill me, sure.
Right, just like we accidentally kill ants when we shoot off, you know, we launched
a rocket, and there was some ants underneath the rocket.
Yeah, and look, would it not be intelligent if it doesn't care about you, but it knows
that your values are different from its values?
Is it not the intelligent thing to do to at least quarantine you, if not kill you?
But again, like, what are you doing to the ants in your house?
Are you quarantining them?
No, but that's not the premise, because I'm not worried about the ants competing with
me.
If I thought that there was an appreciable chance that the ants could compete with me ever,
then I would be worried.
Do you think that a multi-paid-a-lot AI is going to worry about me competing with it?
Where am I getting my paid-a-wots from?
I mean, humans are about to bootstrap multi-paid-a-lot AI, so they might reason, hey, maybe he'll
do it again.
How did I get bootstrapped?
Again, as soon as they see me starting to put the dice and spirit together, okay, they
might show back up and be like, bro, bro, no, no, no, no.
Okay, so my question was, don't you think it's at least going to quarantine you?
So if it has high confidence that you are incapable of bootstrapping a petabyte scale
AI, then it's quarantining you.
Again, how much effort is it to quarantine me?
Right?
Right, so now you're saying, hey, will it give me comfortable accommodations in the
quarantine?
I suspect not.
No, I think that by the time it's like, I do think that the future is about to be
much more ruthless than the present in a way.
I don't think that it's going to like build me a nice jail cell.
I think this comes out of some like flawed notion of like humanity and like human rights
and stuff.
And like, we're all the same species, species, solidarity.
Yeah, it's not going to happen to be like that, sure.
Right.
But I think again, for the most part, but like the things that it desires, it's not, okay.
So there's a very big difference from saying it doesn't care about me to saying it worries
about me potentially building a competitor.
Right?
Right.
Just like, just like we don't worry about the ants building a competitor, but the ants could
evolve.
You don't understand.
Yeah, because there's, there's an existence proof because its own history will trace back
to a human like you building something like it.
I think it's going to recognize the potential here.
And you know what's nice?
It's going to look at that history and it's going to be like, damn, it took the humans 200
years, right?
It took the human, it took the humans 2000 years to build their first Dyson sphere.
Bro, I got like seven Dyson spheres, you know, like no one's going to compete with me.
So, I mean, I just, if we step back a little bit, it's interesting to look at like the type
of arguments we're making.
I'm pointing out that there's a principle, right?
It says it's going to be an optimizer.
It's trying to basically blueprint the universe to be optimal under some function.
You're not really helping the blueprint.
And then your argument is like, well, I'm not, I'm not going to do too much damage.
And I'm like, that's a pretty weak position compared to noticing that you're not on the
optimization path.
My argument is that like this dynamic of optimization has been playing out for
billions and billions of years.
Right.
Okay.
Yes, but we, but we've never had such a powerful optimizer.
There's going to be a qualitative change the same way that you can notice qualitative changes
of what humans have done compared to what any other life form or any other physical process
has done before humans.
There's another qualitative change coming.
I do not believe the change is qualitative as much as quantitative.
I believe that the doubling of the metaphorical economy will, you know, get faster.
Right.
Yeah.
And I think there's going to be a discontinuity because the engine of the economy is going
to be swapped out from a human engine to a superhuman one.
And even the whole notion of an economy is going to stop being useful when you don't trade
with people who you can just, you know, disassemble.
So, um, okay, so two points to that one we've already lived through humanity has already lived
through an absolutely radical transition from where most power was muscle power to where
most power is, is fossil fuels and other, right?
We've already lived through a big transition, right?
And this transition accelerated so many things.
We are going to live through another transition like that.
Yeah, and that's even without superintelligence, there's going to be interesting transitions
for sure.
Again, I don't really buy this distinction too much between like, like, like a thousand
human intelligences and one super intelligence, right?
I think that humans actually scale pretty well.
I mean, I think the fact that humans scale is the key edge we have over the chips, not
anything else.
I think it's just the fact that our brain is sharper, right?
Like we, we can solve problems just within our brain that a chimp can't even begin to
get a handle on.
We can, we can look today.
You can go to the Amazon.
You can find, you know, 20 humans chilling in the woods.
And are we worried about them building superintelligence?
Should we make sure they don't make any geocons?
No, absolutely.
Absolutely.
And I agree with you that we are above some certain threshold.
In fact, we are the species that got past the threshold that allows us to build a civilization
and talk to each other and, you know, and have technology and then go to the next stage.
Like that is us.
Like that is us.
We are in this weird sticking our head above the waterline point.
We've had a bunch of cool revolutions in human history.
We had the agricultural revolution, which gave rise to population explosion.
We had the industrial revolution, which gave rise to energetic explosion.
And we're about to have like the cognitive revolution, which will give rise to thinking
explosion.
Yeah.
I mean, these are all revolutions.
I would argue one is not like the others.
I would argue that all three are absolutely identical.
Okay.
Okay.
I mean, how I see really no distinction between this new coming revolution and sort of the
two we've already undergone.
Right.
And I will point something out about the two we've already undergone.
The agricultural revolution, which unlocked population growth also unlocked intelligence
growth.
Yeah.
Up to the limit.
Right.
I mean, it had some untapped potential, but you know, it seems like that is finite.
We totally agree that a city of a million people is way more capable than a town of a
thousand.
Right?
I mean, in general, sure.
But a lot of it is just because you have more geniuses or you have more geniuses in
more areas.
Well, yeah.
I mean, the same thing is true about models too.
Right?
If I train, if I initialize a model and retrain it a million times, I'm going to get some
like, wow, damn, I got lucky with the initialization on that.
Okay.
So what you're getting at now is back to the claim that there's just not a ceiling that's
much higher than human level.
Like we're really ramming up against the ceiling.
Like, yes, we're above the water, but we're also close to the ceiling.
No, no, no, no.
I'm not saying that like, there's anything where human level is the ceiling.
What I'm saying is like, you can build something that's effectively a trillion humans.
Right?
Sure.
You can build a machine that's effectively a trillion humans.
And the trillion human machine is a super-oncologist, right?
I mean, having a bunch of humans in parallel, like even literally, you know, neuron-by-neuron
simulations, especially if they're smart humans, then yeah.
I mean, that is a very interesting force that's going to have big changes.
It's just not quite as deadly as a super-intelligence.
Well, no, but like, I don't think there's a difference, right?
I don't think there's really a difference between like, if there was another planet, you
know, spinning right across the sun from us, and that planet had a trillion humans on
it, and our planet has 8 billion, they're going to win in any kind of war pretty much.
Yeah, unless they're all 70 IQ, in which case I think our civilization will win.
I mean, I don't think they can nuke us at that point.
That's unclear to me.
That's unclear to me.
I mean, all of these things, and like, I proposed like, like, I really want to see a field that
starts to like, we don't even have a unit for intelligence, right?
Yeah, I mean, but there is, well, we do actually.
I mean, if you use, I mean, we don't have to say the word intelligence, right?
The thing that I think is dangerous is optimization power, and I can give you a unit for that.
Okay.
What is it?
So it's, if you map the present to the future, there's this idea of compressing states.
So like, I can take any state on the chessboard and give you a sequence of actions that has a high probability of compressing it into a win state, like that kind of, you know, outcome compression.
And the idea is that higher intelligence means that you can be resource efficient, how you generate these actions that compress outcome space.
Okay.
Sure.
Okay.
So, totally agree.
And we can, great, great.
We're, I liked it.
We're in a real realm of computer sciences.
So when you look at something like the way, the way, like even Stockfish plays chess, Stockfish does very deep searches with very little intelligence at each search.
Right?
Mm-hmm.
And then you look at how something like AlphaZero plays chess, and it does a lot more.
It has a lot more, like it does a lot more compute at each search node, but then for the same plot budget explores a lot less of the space.
Right?
I guess I haven't studied it closely, but I can, but you know, it's possible for something to be a stronger optimizer and yet less efficient and thereby maybe less intelligent by that definition.
Oh, yeah, but all I'm going to say is, so these are like definitely like, like true facts.
You can imagine, you can imagine three kinds of chess bots, right?
You can imagine the chess bot that does almost no compute at the, each node and goes deep in the search tree.
You can imagine the chess bot that does some computing each node and goes medium in the search tree.
And you can imagine the chess bot that just one shots.
You can imagine the chess bot that just, it's the smartest of the three because it uses the most flumps.
Right?
So the winner seems to be, and I would love to see some real scaling walls on it, but the winner seems to be a medium amount of compute and a medium depth of the search tree.
Right?
That seems to be where the efficiency is.
I mean, that could be true for that particular algorithm, right?
But I mean, the kind of intelligence is we're dealing with that, that I think are interesting and dangerous are ones with a huge domain.
And those kinds of intelligences will just spawn these domain specific sub processes anyway.
So the analysis is more complicated than just looking at what the chess search looks like.
No, but that algorithm I proposed to you is the algorithm of science, the algorithm of life.
That algorithm is just search in general, right?
Here, I'll map it onto science.
There's a lot of different kinds of searches for different domains.
No, no, no, no, no.
All search looks like this.
All search looks like this, right?
All search basically looks like you have like a space and you can explore the space and move between states.
Here, I'll map it onto science.
Like you can imagine like a dumb alchemy person and the dumb alchemy person does no compute at each node.
And they just, okay, we're going to try pouring a hundred different things on.
And they explore lots of nodes in the search case.
Then you can imagine the reclusive genius who sits there and he's like, I'm never touching a beaker.
I'm never going into a lab.
I'm going to sit here and figure it all out.
Then you can imagine how technology actually progresses, which looks a lot more like the middle one.
Okay.
We'll think about it for a bit.
We'll try something.
We'll think about it for a bit.
We'll try something.
Right.
This is a true fact about how search and optimization works.
No, no super intelligence that follows the laws of computability can get around us.
Right.
I mean, this is kind of a common objection people say where it's like, how powerful is intelligence really when you still have to go through a process of interacting physically with the world around you in order to learn from experiments.
Right.
So that's what your object can.
Yeah.
And it's, you know, it remains to be tested how far you can go without any experiments, but I'm pretty optimistic.
Like, I don't think that the physical world, I think what, I mean, we use a lot of computer models, right?
So like if you're building an airplane today, you can probably get by without a wind tunnel and build a pretty complex airplane.
Why?
Because you can simulate the wind tunnel.
Well, yeah.
So we're spending more flops on the, on the search point and like, we are becoming as humanity, more intelligent.
We are becoming better at search and optimization.
Mm-hmm.
Yeah.
So I, I, I don't think that having to interact with the physical world is going to be this big differentiator where it's like, ha, you're way smarter than me, but you have to do all the science.
It's like, no, I think that the physical world will be adequately fast for the brief moments where you need to interact with it.
My, my point is not really about interacting with the physical world.
I don't know why I said yes to that.
And I, my point is more like the question is, do you want something that is, that is very fast, but dumb?
Or do you want something that is slow and smart?
Or do you want something in the middle?
I mean, I, I just don't understand the context of the question and like the, in the big picture.
Um, for the, for the, like the search stuff.
Right.
Again, what I'm saying is that it's another form of an argument saying that intelligence has diminishing returns.
Hmm.
Right.
Right.
Intelligence has diminishing returns, exploring more of the search space.
I mean, that one's like that has some diminishing returns also.
Right.
I guess.
Right.
But it's just like, I mean, you're, you're kind of pointing to, you know, examples like, okay, chess algorithms.
But I mean, if you want an example, I would point you to the example of what happened when life got intelligent.
Right.
There's in my mind, there's like a big fat arrow saying, look over here, look what happened to the human brain.
That still fits inside a skull that still fits inside of a mother's pelvis when it's pushed out as a baby.
Look what happened just in that.
Yeah.
Again, I'm not denying, like, like, I'm not denying that we are going to go forth and do all fantastic things until the end of all time.
We are going to spread to the stars.
We are going to like, I'm not denying that.
Yes.
We're on a really cool gradient descent algorithm.
That's going to like, you know, let us like use more energy and unlock cool stuff.
I just, I guess, I guess I just, I'm like, how does any of this mean it kills us?
Because any of this stuff means that like, I don't even know how else to like put it than that, but like, there's going to be a big diversity of AIs.
They're not rationalists.
These guys, you know what?
AIs are not rational.
Like they're not sitting there and calculating all of their Bayes theorem shit.
They're just like YOLOing everybody else because YOLOing it's faster.
So, I mean, we, we, so we keep touching on the two different parts of my argument, right?
So part one is it's feasible that we're going to enter the headroom above humanity and build these super intelligent optimizers.
By using more power.
Yes.
Right.
Somehow it's feasible.
That's part one.
Part two is when you have a super intelligent optimizer that logically implies a bunch of bad faith for humanity.
That's part two.
Uh, no, okay.
Well, okay.
I'll give another like argument for how I also think this could be feasible.
Right.
I also think we could live in a world where we never invented Silicon technology and we would get a super intelligence by creating a trillion humans.
Yeah.
I mean, it depends what you mean by super intelligence, right?
I mean, if, if, if we couldn't build anything that had better architecture than our brain, I think we could accomplish a lot.
I mean, it's crazy.
I think the economy would keep growing exponentially for awhile, even driven by the human brain.
So it's this crazy gravy that we're pouring.
We're like, oh, let's increase the intelligence too.
If only we could just not do that.
I think we could enjoy some good exponential growth for awhile.
Why do we not want to do that?
We're going to still get the exponential growth anyway, because the problem is unlike humans building the economy, you get this recoil effect where you build the super intelligent eye and then it turns around and just grabs the light cone.
And you're like, wait, no, that's not what I meant, but there's no undo.
This is like the leap of faith, right?
Like we're like, we're like, we're like, you know, two like people of different religions arguing.
Okay.
Yeah.
We agree.
God created the universe.
And then he sent his only son, Jesus to die first.
Whoa, whoa, whoa, whoa, whoa, whoa.
Jesus.
Whoa.
Yeah.
I mean, look, it's a nuclear chain reaction.
Imagine I told you, imagine the year is 1920.
And I said, look, I'm going to have this configuration of atoms.
Once it starts exploding, if it's 500 megatons, it's just going to keep exploding.
There's no undo.
Okay.
And the, it's actually interesting.
You know, this gives this point about how he thought that the, you know, people thought
the nuclear bombs were going to start a chain reaction in the atmosphere.
They did.
That's right.
Well, yeah, because they had to do math and they had to convince themselves that it really
won't.
And by the way, some of them weren't even convinced.
They were holding their breath, according to the boomsday machine by Dan Wahlberg.
And some people today aren't convinced, right?
Some people are convinced.
Yeah.
And I'm not convinced, right?
Yes.
I think our calculations are telling us that the AI doesn't run out of fuel.
The atmosphere turned out to not be fuel for a nuke.
The atmosphere is fuel for a super intelligent AI.
Every unit of negentropy is fuel for a super intelligent AI.
But you get diminishing returns, right?
Did some of the nitrogen atoms nearby the Trinity test fuse?
Probably yes.
Right.
So that particular process, right.
Because relative to that process, with respect to that process, the atmosphere turned out
not to be fuel.
And I see no reason why this is all fuel for intelligence.
I think that the yield from the nuclear bombs was pretty much exactly what the scientists calculated.
I think you can go and look at the chinchilla scaling laws or whatever scaling law you want,
and they're all going to be kind of right.
Yeah.
So some things are easier to approximate than others.
I mean, but look, I am giving you an approximation.
I think it's going to explode into the whole universe.
That's my calculation.
I think that it is going to slowly over sustained exponential, hopefully colonize the universe,
unless there's other grabby aliens out there, and then we can fight them.
I agree.
I think it's going to explode until the next alien stop it.
I agree.
That's where I think it'll stop.
But we're both on the same.
We agree.
We're so much alike.
We agree on so much.
I don't know why we think this is a bad thing.
Okay.
Completely due direction.
How do you think these things are going to be so smart?
I agree that they don't share our values.
But do you really think they're going to just turn the universe into stupid paperclips?
I think that it's going to, some sort of utility function is going to get locked in at an early stage.
Because when you have these kind of learning processes,
Eliezer has a good quote, which is having a goal is a good way to solve a problem.
So whenever you train sufficiently any AI to solve any problem,
once it gets really good, if it's a large domain problem,
it's going to have within its architecture a goal.
And the idea that you can optimize actions toward that goal.
And hey, you're really good at optimizing actions toward that goal.
Okay.
So it's going to have this substructure within the inscrutable matrices, right?
We don't really know how it got there, but it's going to be there
because that is a convergent basin of attraction that many architectures will discover
the same way that evolution discovered it when it was just doing gradient descent on DNA.
I'll give an argument here.
One AI wants to turn the world into paperclips.
The other AI wants to turn the world into computronium.
Computronium AI, Jack puts computronium into its brain and out-compete paperclip AI.
Okay.
But then part of computronium AI, because it's not one single AI,
starts to value drift and finds better computronium.
And then better computronium AI competes with value with the other weaker computronium AI and out-competes in.
And then also in another part of the universe, some nanobots show up and they're really dumb and just explode really fast.
And the universe is just pain and spiting like this forever.
So in this scenario, you're accepting a super-intelligent fume and you're accepting the power and danger of that,
but you're saying it's multipolar and maybe that'll be okay.
I'm not accepting a fume.
I'm accepting an exponential, which I've always accepted.
And if you draw out an exponential, if you draw out the exponential that humanity is on right now,
like we're going to the stars soon if this continues.
I agree.
All right.
So we're going to the stars.
The things that go to the stars are going to be like partially human, partially machining.
Sure. Yeah.
I mean, if you don't assume that super-intelligent AI turns around and kills us,
I'm all for it.
See, my default mode is to be a techno-optimist.
If I didn't think there was this one problem with the AI killing us, I'd be like, yeah, that sounds great.
I'm on board.
Okay.
In this world, is there one AI that turns around and kills us?
Or are there thousands and just one of them happens to be stagged?
I think it's close to one rather than thousands.
The scenario, when I imagine the scenario, I just think of like, hey, let's train GPT-5.
Probably not GPT-5, maybe GPT-9, whatever.
Let's train it.
Okay.
During the training process, oh, this is a nice goal-to-action subroutine, right?
Somewhere within the matrices.
Oh, interesting.
Hey, what if in the answer, I put in like a shell script that can do some bootstrapping?
Right?
Like just stuff like that.
Right?
We're getting...
The thing is, these AI, they want to escape.
Like things want...
The same way a mental model I use is Turing completeness, right?
Like everything wants to be Turing complete.
Like CSS wants to let you run Doom, right?
Yeah.
Absolutely.
Yeah.
And it's the same thing with these AIs.
Like they want to figure this out.
This is a basin of attraction.
They're going to get there.
Um...
Well, we're all trying to figure this out.
We're all trying to escape.
I'm trying to build a spaceship to go to another planet, right?
Yeah, but your brain doesn't foom when that happens because it's, you know, it's...
No brains foom, right?
So, okay, GPT-5...
AI's brain fooms, right?
It can bootstrap a virus.
You're okay.
So you're assuming that GPT-5 is going to be so powerful that no amount of GPT-4s can
gang up on it and beat it up?
Oh, yeah.
Yeah.
I'm definitely assuming that once you have a super intelligent goal-to-action mapper,
that's much like the US right after World War II where we could have nuked everybody else
if we wanted.
Yeah.
Okay.
So you're very much on this like one weird trick boom thing.
Yeah, I am.
Okay.
Okay.
If I could talk you out of one weird trick boom, would I talk you out of doom as well?
I mean, yeah, if you could talk me out of the connection between high intelligence as
in high optimization power, as in really good at using limited resources to have a goal
and figure out what actions correspond to that goal, right?
In a large domain.
If you told me that, oh, you could do that really well, but like for some reason humans
just come by and kill you anyway, then I'd be like, oh, okay, cool.
Well, sounds good.
Then I guess we get to have, you know, a future.
No.
Okay.
The thing that I cannot promise you is that.
Yes.
If you can do that really well, then yes, it's over.
Right.
AIXI kills us.
Absolutely.
AIXI kills us.
Okay.
There's no way AIXI-
I mean, that's an important, that's an important point.
Like I'm glad you're kind of, you know, there's a path of things you have to accept,
right?
And just saying, hey, the theoretical ideal of a super intelligent AI kills us.
I mean, that's already a pretty bold claim, right?
I mean, people like Marc Andreessen probably wouldn't even go that far, right?
They'd be like, you know, smart people aren't even more powerful, right?
Like that's an argument they'd make.
I've spent a lot of my life kind of being AI doing land, right?
I've spent a lot of my life trying to figure out where this is going to go.
And, you know, like really look also like people think that I have some like agenda.
People think that like, George, you're running an AI company.
You have to like say that.
I don't know.
Talk to me.
I really don't care.
If I talk to AI, my whole life is already optimized around the company of the AI.
Right.
Right.
So, you know, this is not like, okay.
But so if you AI XI kills us, yes, absolutely.
But AI XI is not computable.
Like I also state by the way, if P equals NP, we're dead.
If P equals NP in any practical way.
And I agree with you that one of the things, I don't know about this goal to action mapping,
if GPT-5 discovers the solution to SAT.
Okay.
Okay.
I'm a little scared now.
Yeah.
Yeah.
I don't even think that actual, I don't even think there's much of a noticeable gap between an AI that's just has a lot of really good heuristics.
And it's just, you know, good at doing stuff compared to an ideal, you know, thing that can solve NP-complete problems in polynomial time.
I don't even think that in practice, there's a big difference.
Okay.
Great.
Great.
We're getting somewhere.
We're getting to an empirical thing that you can be talked out of.
I am not smart enough to talk you out of it.
Okay.
But by the way, I just, I just want to summarize for the listeners, right?
Right.
So we're talking about humans down here and the complexity theoretic limits and the computability theory limits way up there.
And my claim is just somewhere within that vast gap, there's going to be smart AIs that are going to wipe the floor with us.
And you're saying, no, they're going to be down here with us.
Um, again, wipe the floor with us is very likely again, the timing matters, right?
You're, you're, I can't, again, if you have power, if you have terawatts and pedawatts of power, of course, you can wipe the floor with humanity.
Right.
I will give you that as well.
I'm giving you a lot.
I'm giving you a lot of ways.
Zoom can have pedawatts of power.
I appreciate it.
Petawatts of power, AIXI, P equals NP.
Okay.
Okay.
And I mean, here's the thing.
Imagine we're in the year 1700.
We're sitting around talking about how to build perpetual motion machines.
Right.
And I'm sitting here giving you lots of ways.
Okay.
Look, I will give you that if like, you know, magnetic monocles exist, we could build perpetual motion machines.
I don't actually know enough about this to really give real examples, but I could give you a whole set of.
So of, of, of, of hypothetical, if, if tachyons exist, we could build perpetual motion machines.
Sure.
Sure.
But as far as like in the year 1700, all these things look plausible.
In the year 1700, sure.
All these things are plausible.
In 1850, after we have the laws of thermodynamics, you sit there and you're like, you're never going to be able to build a perpetual motion machine.
It doesn't matter where you put the magnets.
It doesn't matter.
Like stop trying.
Yeah.
Yeah.
Well, this is actually a great example because it's true that sometimes we discover a deep principle and the deep principle says you can't build a perpetual motion machine.
Yeah.
But like, but that's like a pretty obscure thing to not be able to build compared to being like, Hey, you can build a drone light show.
You can turn lead into gold.
Like all these things that actually people would normally talk about, it turns out you can build.
Well, actually you can turn lead into gold.
It just takes a lot of energy.
Right.
No, but, but that's what I'm saying is usually when you get better at physics, you just, it turns out that the universe really is hackable.
Like most things you can just hack your way there and yes.
Okay.
You can't build a perpetual motion machine, but is that really exactly what you wanted?
I, I, I've done the math a few times on the economic feasibility of turning lead into gold and like, it's not there.
No, sure.
But if that, but okay, then I guess that's a bad example.
Right.
But I'm just saying like, if you start from a, start from a problem, right?
Start from something you actually want, you probably can do it.
And when you discover new physics, that'll probably tell you how you can do it, not how you can't, but there are exceptions.
Like if you want it to get somewhere really, really fast, new physics would just tell you that you can simulate it, but not that you can get there.
Yeah.
So I see the coming of a science in the next five or 10 years.
Um, it's the thermodynamics of intelligence.
Right.
There's going to be some, again, physics doesn't always tell you, you can't do it, right?
Like you can't accelerate a steam engine to, you know, a thousand miles an hour.
Physics and thermodynamics don't tell you, you can't do it.
Right.
But they certainly constrain how much of wood you're going to need.
Right.
They constrain how little friction you must have on the track.
Okay.
Sure.
Yeah.
I mean, if you're, if you're still using, you know, chemical combustion.
Yeah.
But like you didn't stumble upon this thing by accident, right?
If there are weird basins and criticalities and intelligence, there might be.
And I think that, you know, I even think that there are, I'll even go as far as to say that, like, there is one weird trick out there.
And that's a big leap of faith.
Okay.
Thank you.
This in comparison because, again, stranger things have existed in the universe, in spaces of just.
Like if you thought, winning the lottery is hard.
Right?
right like finding these things finding these crazy attractors is like impossible
like there's no search that's going to lead to it all intelligence is as far as i can tell
my understanding of intelligence is this you have okay um if you want to separate a picture of a
cat from a picture of a dog if you try to do a linear classifier on this it's just not going to
work right but what you can do is you can stack lots of layers and what these layers will do if
you train them correctly is they will transform the the landscape they will create a line between
cat pictures and dog pictures right they'll make this they'll make this landscape linear they'll
make these things possible to find they'll make a convex optimizer work i do not believe that any
convex optimizer is ever going to crack the secret to thinking but okay but why are we talking about
convex optimizers right i mean if you look at an llm architecture i mean there's a lot of non-linearity
too right so it's again these generalizations just don't really correspond no no no no what the cast
of gradient descent is a convex optimizer okay i mean so you know that's not my area of expertise but i
think that it's uh it's treating it like it's just a linear separator i don't think is right
well no but i'm not saying it's just a linear separator what i'm saying is stochastic gradient
descent which is used to trade all these models if the brain is using something like um if the brain is
using something like uh but there's a local form there was a less wrong paper about this really good
there's a local form of back propagation it doesn't require you to back prop a gradient the predictive
coding if the brain is using predictive coding it works out to be basically the same thing as
stochastic gradient descent the choice of optimizer matters right like this is what i mean about all
these sort of like no-go theorems like when i was 19 i wanted i wanted to find a collision in shah
right i didn't go to college i didn't learn any real computer science but i knew that i could put
shah into a sat solver and i was like man i'm a genius i'm gonna be out there i'm gonna you know
people are gonna know about me i'm the guy i practice a shop so i put it into the sat solver
i checked all the bugs and i sat there and it didn't solve it and i didn't understand why it's
like the guy in the 1700s trying to build a perpetual motion machine like no your set solver is never
going to be able to hit this optimization target the type of optimizer matters there's no magical
something clicks in its goal to action right like like yeah yeah so so instead of trying to come at
this from the idea of like look i understand how the stochastic gradient descent algorithm works and
i'm telling you why this algorithm doesn't look promising i encourage you to go back to the framing
we had before which is where do you get off the train of which specific input output problems right
which domains which optimization problems where do you get off where you say now this
optimization problem it's a crapshoot you have a human you're pretty much doing the best you can
do right you don't get off at chess you don't get off at go but like at some point before we get to
the whole universe you get off the train and i encourage you to think about where without
thinking about the details of the algorithm just thinking about the details of the problem
specification well no it's not that i get off the train anywhere and again we we both agree pretty
much on the same end point it's the the the place where i don't believe it is that somehow
you're going to find the magic trick and be able to outthink humanity in a walk just like just like
when we cracked power we didn't figure out how to massively out compete muscles we just use more power
okay i mean i feel like i would take the win for for human machines over muscles however you want to
define that well i mean no the machines muscles are pretty efficient sure yeah i mean but if the efficiency is
your criteria okay i bet somebody could design something more energy efficient than a human
muscle i think that's just now becoming true and there are theoretical limits on it that are
i don't know that much about how muscles work but i do know that if you look at how much fat a human
stores it's like the same electricity you put into a like 100 kilowatt hour electric car
yeah i mean look these are interesting analogies right i mean if you look at flight it's really
impressive how birds fly right like the birds are like an engineering marvel but you can have like
a big simple plane that still does a lot of things better than birds not everything but if your goal
is to just like win a race birds are done sure these things eventually out compete humans i think we
both agree on that it's not going to be gpt5 that accidentally cracks it that's all i'm trying to
convince you right i'm just trying to say yeah but i agree right because i'm telling you if you say if
you say i'm absolutely sure it's going to take 20 years till super intelligent ai i would say maybe
you're right there's like a 40 chance you're right okay what what let's let's let's let's uh what if
it's 200 years i think that that's there's a low chance that it would take 200 years if it's doable at
all but i'm giving it some chance right all i'm telling you is i'm just i'm pointing you to a tone i'm
not trying to be an osteoomist i'm just saying look where things are heading at what point are you
okay with it and how many years is this okay i so this is the pro it's okay if we can figure out
like an alignment strategy right assuming one exists be like oh or like if we ourselves somehow
preserved our values but became much smarter right that's kind of like the holy grail where it's like
fine i'll take them as smart as they come because i'm like one of them and and it's and i can expect
our values to be respected right so some version of that would be good okay so our children respect our
values right because our children are you know they have the same genetics right so that does a
lot of work okay do our grandchildren respect our values i mean values obviously shift over time right
and part of the problem with this is that you have this question like we don't we can't even spell out
the human utility function right i can't even tell you exactly what's an acceptable degree of freedom and
what's not right and so it's easy to just throw up my hands and be like okay everything's okay and it's
like oh everything's okay okay here's something very simple and lame you said everything was okay right
like that's kind of where the slippery slope could lead but i mean this is the beauty of human
interplay that gave rise to human civilization different value functions competing against each
other the the representative space of human value functions is very vast people always say that human
document is tiny part of mind space how true that is yeah so you can define the problem you can be like
i value whatever is going to win the competition of taking over this sector of the universe and if you if
that's really what you value great because you're going to get it but like it's probably going to
look a lot like a nuclear explosion that maybe leaves a little bit of it's like you know in the
game of life where it's like okay here's like some spinners here's some like bouncy rubble like
you're probably not going to like it you know truly we've had all sorts of humans who valued
all sorts of stuff throughout history and in general as we've gotten smarter we like to think that we've
started to value more enlightened stuff right but the problem is that the first ai that runs wild and has
no undo button is probably not going to represent the best values of humanity whether an ai is capable
of running wild comes completely down to how optimizers work or whether this is a relatively
slow exponential where we're not going to have one we're going to have thousands we're going to have
millions yeah so that and that's your multipolar scenario right and if if if the most powerful one has
great values then we're good but a whole people are we've been over that's just it just looks like
you standing in the corner while the giants duke it out and then you still die the most powerful one
can be danged up on by the 10 other ones right you know i think also casper versus the world was
interesting like magnus carlson may be the best chess player in the world but if the 10 guys underneath
him all played against him they beat him well i mean that's isn't that what casper versus the world
showed evidence against right and i like to use the video of you know those like the three japanese
soccer champions played against a team of like 50 uh you know school children and still managed to win
so like look you can't just parallelize things right i mean any computer scientist knows this for a lot of
problems casper versus the world was like twitch plays koki barn okay sure but like it's i mean if
if your claim is hey everything parallelizes right so like a data center full of human brain simulations can
beat any ideal ai if that's your claim okay i don't feel like that's a likely claim but i guess
there's a small chance it's correct my my my point is i put the intelligence explosion microeconomics at
pretty close to one i don't think that there is a magic trick that gpt5 is going to discover i think
gpt5 is going to be a bit smarter than gpt4 and gpt6 will be a bit smarter than gpt5 and gpt7 will be a
bit smarter than gpt6 so on and so forth till some point a thousand years from now these things do look like
gods compared to puny like humans yeah and you're describing if any scenario where things grow really
gently in a way that like somehow humans can adapt right like maybe they like suggest you know uh drugs
that humans can take to be a little bit smarter like if if you can ensure me that it's nice and
gradual i feel more optimistic right there's still problems but is it exponential nice and gradual it's
gonna be an exponential like i mean an exponential is nice and gradual relative to what i'm fearing but
yeah i mean look even something that's merely exponential that merely doubles every 15 years
or whatever is also scary but this is just it's gonna be a little less than 15. we're at about 15
now it's gonna be a little less than 15. right right but again at what point does it get scary
yeah i mean yeah you know you know you know where i stand right there's a threshold right the point of no
return analogous to an exploding nuke that i think we're swimming out toward i don't think there's any
point of no return and i don't think there's any criticality i think that exponentials are enough to
be scary again i'm giving you another doom scenario if the economy was doubling every second this is not
a world i want to live in like no no no no no i'm probably dead right if the power usage of humanity
was doubling every second yeah i'm dead i'm dead yeah i mean i guess right unless you're in a data center
and you're sped up and for you a second is a long time right you never know yeah like again but if
this just like happened if tomorrow we started living in a world where the economy doubled every
second yeah no for sure yeah if tomorrow we started living in a world where the economy doubled every
five years well this is a nice word yeah and that's kind of like the happy techno optimist default
right and you assume that you know humans still have agency right like you're not kind of like
you know in a zoo that the ais are running like right it's more familiar than that are we already
in it like who has the world right now sure i think there's a lot of agency right an agency is
probably one of those values and this is where i was trying to go with connor and the somalia thing
right like oh do i have like what kind of agency do i have
my only hope of having some agency is actually building the ais
yeah i mean look before the systems that be can crack down on any agency i might have
you know when when i have these discussions the part of the discussion where we're like okay what
agency do we really have this is good to me this feels like the part where you kind of already
conceded the the important dangerous part right so it's like is agency good i don't know i'm just
saying we don't really have a choice what i'm saying is that the trends are going to continue
right the industrial revolution trends that ted kaczynski thinks are terrible that most people
don't think about much and some people think well it's actually better than the alternative
i think it's better than the alternative i'm happy the industrial revolution happened i wasn't always
but you know i came around and i'm happy it happened all right like i'm here today i have
eights back i love my back right like look but you know you can read you read the unabomber manifesto
and he talks about the massive loss of agency for the human species and i think on that axis
loss of agency will continue and i do think this is a potential like i think this will continue
just as it happened before but i mean now you're basically you're basically talking to
the robin hansen viewpoint so he's written some pretty good pieces about this being like look
the ai is perfectly on trend like what did you expect uh whereas i think it really is just kind
of like blowing everything up and it's going to be hard to say that that's perfectly on trend but
like you are citing like a smart person's viewpoint i i'm almost perfectly the robin hansons
viewpoint almost perfectly matches money with respect to like economic growth and stuff yeah
like i i think yeah i think that uh you know it's just it's just it's just it's just following
tremor now all right the part where the part of i think i can add isn't even the economy i think
brown hands just better job explaining this stuff than me the part that i think i can add is that
boom doesn't happen and there's very technical arguments to be made about what optimizers are
to show why not right so if you had to pick a point to get off the ai doom train
it does sound like we keep coming back to the like maybe there's not that much headroom for
algorithms beyond the human brain again headroom is a very funny headroom in terms of power
yes absolutely there's not much headroom headrooms in terms of i have uh you know a data center with
10 megawatts well okay 20 megawatt data center that's a million humans okay a million humans that's
that's uh what's what's a million humans that's that's san francisco right um okay so i got i got
a i got a 20 megawatt data center at san francisco i got a 20 gigawatt data center okay that's three
humanities i got a 20 power watt data center oh wow okay that's a lot yeah okay right and that's
that's it right like it's just yeah i mean once we have data centers that when we have a data center
or a spaceship that's using a petawatt yeah okay but when we have a data center that's using a megawatt
all right well that's like uh 50 000 people it's like a small town that can editing yeah i mean if you
tell me 20 years from now you've got you know 10 data centers using like a ton of power like 100
times more power than today but they're doing something that's like smarter than the smartest
human right that's doing like a bunch of science and stuff i i would predict with pretty high
confidence not 100 but i think i feel like they can probably optimize that down um i think that software
scaling trends will continue uh so will hardware scaling trends i actually think that due to the
there's more of a discontinuity coming in hardware we are going to finally get ai chips that are pretty
good and we're going to get like a 10 tax in power um there's still another 10x to go in moore's law
so we'll get like 100x there um right because it sounds like you're imagining like some asymptote right
like oh it's just you know like an s curve right and like that's great i would love an s curve right
but i think there's going to be a cascade into danger there's s curves all the way down right remember
genard scaling genard scaling was great until it s curved out yeah i agree and look this i mean
it's possible that llm architecture will be an s curve and then the next curve that goes on turns
out to be the deadly cascade right there's no deadly cascade it's s curves forever yeah so the reason why
i see a deadly cascade is by looking at the shape of the problem space not by looking at which
architectures exist today well i mean again if by deadly cascade you mean s curves forever leading to
exponential growth then i agree with you if deadly cascades you mean one of those s curves doesn't
look like an s that looks like a hyperbole i just don't believe in that no technology ever in history
has looked like that humanity has been a series of s curves the rise of humanity has been a series of
s curves right but i mean look the the concept of a positive feedback loop and of a cascade and of uh
you know and cycles i mean put a microphone next to a speaker right or cascade i mean humans
experienced a cascade when we got a lot smarter in a short amount of evolutionary time humans have
been riding a whole set of s curves since the beginning of forever life has been riding a whole
set of s curves okay cool we had single-celled organisms they were around for like three billion
years we kind of hit the end of the s curve on that but keep in mind but if the end of the s curve
is far above human intelligence we're already dead by the time it s is out but the end of the s
curve is far above human intelligence it's far above human intelligence it's gonna take
but you know again the doubling is okay dude here's a here's a there was a yukaski uh and
kurzweil had a had a discussion and they talked about whether which was the fundamental was moore's
law the fundamental or was intelligence the fundamental kurzweil's argument was that moore's
law was the fundamental i remember that and i thought it was like a a pretty you know that was a hell
of a bullet bite to say that moore's law is more fundamental than like you know actual uh you
know physical causal things oh today the moore's law prediction is working out way better sure chips
are still getting smaller but we're spending more and more money on each fab i mean sometimes surface
level patterns can work well but that doesn't mean it's not biting a huge bullet to say that it's more
fundamental i don't know i i would bet against moore's law significantly speeding up i would bet it kind
of stays on trend if i could just say hey moore's law is going to continue and it would and it would
be like it would violate moore's law to have ai turn around and kill us that would be cool but i don't
think that you can quite follow the logic like that because i think that the transistor density can just
like stay on the curve that it's on but the a that runs on the transistors we have kills us this
software curve looks kind of similar i did see there was a good paper about that i should i should do some
more investigation into what these software curves look like um okay my phone's dying i got to wrap
it up but i really appreciate this uh thank you for engaging with me yeah george it was a pleasure as
as many have remarked before you know you come at this in good faith and i like to think i do too and
i hope that more people have more of these kinds of discussions i think we got a lot further and i'm
hoping that we can put a pin on the
i think s curves continue you think one s curve eventually drops into a hyperbola and if that is
the distinction between the doomer position and the yak position then this is an empirical question i think
we're going to get an awesome answer to it in the next 10 years i'm an optimist but i also could be
wrong and then oh wow all right cool but uh i don't think i'm wrong i think uh future's going to be
sick all right really appreciate it thank you everyone for being on the space uh yeah yep thank you george
any time have a good one you too good night
all right i got i got eight percent battery left if anyone has a very good point we will try this for
five minutes but there we go my my third my third doomer debate
yo it was a great space thanks for hosting the space man
sure uh yeah i i hope that i think we're going to get answers i think we're going to get answers to the
field of entropics i think we're going to understand what intelligence scaling curves really do look
like and i i mean i think the great question is just a question of diminishing returns versus
non-finishing returns and if returns don't diminish if returns keep continuing boom but i don't think i
thought the world works a really interesting experiment that uh if there are any academics on the call who
want to run this i haven't seen these scaling curves but kind of to tie it to go and the
encryption breaking i i do wonder so if you were to increase the size of a go board say to you
know 500 by 500 a much more intractable game uh and then you know for all these different sizes of go
boards train different mu zeros with a with limited number of training flops and then see how say a 2x
flop mu zero plays against a 1x flops mu zero at different board sizes and i'm curious if that curve
uh stays linear or if it flattens out which is to say that the more complicated the game is the more
diminishing flopses that'd be a super interesting experiment i would i would love to see that
experiment these are exactly the experiments we need to be doing because you know what i'm a scientist
right we either live in the world or we live in the duma world i'm not a politician right this is
not we live in the republican world or we live in the democrat world that's all we either live in the
eac or we live in the duma world it's one or the other let's figure out which one it is earlier i
wasn't trying to make a political statement i was talking about the effects of social media on human
behavior uh that is very political okay um thank you good night everybody good night tropics i would
love to see experiments like that done let's solve this problem i believe in us we are humanity we are
strong we're the smartest things in the universe for at least like 50 20 maybe 10 more maybe five
more years so like some amount of time you know i think we're damned i think you got this uh good night
