Hello, everyone. I can see people are slowly joining us. I'll give it a few more minutes,
but I'm so happy you're here today. We've got Neil Kirk from Abertay University. He will
presenting cognitive biases in voice perception and all his work he's done with that. So I'm really
looking forward to that. In the meantime, feel free to let me know in the chat where you're
dialing in from today, where you're based, what kind of research you're doing, and what you're
interested in in this topic specifically, and just to give Neil an idea who's in our audience and what
kind of stuff people are researching. That's always interesting to know, and then we'll make a start in
just a few moments. I'll give people a bit more time. This is also the last webinar in our series
for the year. So the final one. And at the end, I'll announce a little bit of a plan for the next
year and what kind of topics we want to cover. And then I'll give you an opportunity as well
to give us some input about topics you would like to hear about. So if you have any ideas for that,
please bear with us and feel free to feed that back to us.
Okay, I think we'll get started for now. Just because I know there are quite a lot of people
who have never used Gorilla before, I'd like to do a quick introduction to what Gorilla is and how you can
use it for your own research. So to give you an overview, I'm going to present what's inside Gorilla.
So Gorilla is this all-in-one powerful and easy to use platform that allows researchers to create
surveys. It has feature parity, for example, with Qualtrics. And we also have a task building tool
which offers feature parity or more functionality compared to, for example, E-Prime, LabVance,
and then you link those elements together. You can build questionnaires, tasks, link those together
and build an experiment flow so that you can go from the surveys, questionnaires, to the tasks,
into a randomizer which could randomize you to different conditions for your participants,
and then on to further tasks which can be presented in a counterbalanced order. And then we have a few
advanced tools like a game builder, a multiplayer tool, a shop builder, and UX UI tools. So all of that
is Gorilla. And if you're interested in any of those in more detail, we do have webinars for those as
well. You can have a look on our Gorilla Presents page. Just to give you a bit more of an idea what
we can do with Gorilla, I've got a quick video here which I would like to share. So we support traditional
reaction time tasks. You could have any type of stimulus and any response. You get accuracy reaction
time measures as well as trial by trial randomization. Here you can see a relational reasoning task and
how that is set up with the spreadsheet so that we only create one display and we can draw in
different types of stimuli for this content we want to show. Here you can see a list of the different
components and then we also have our questionnaire builder tool here. This does all the complicated
things that experimental psychologists need. For example, a matrix survey tool and we can support
scoring and branching and creating sub-scales within one of the same questionnaire. But here you can see
the scoring. You can toggle that on for different items of your questionnaire and decide where to score.
And then what you do is you link those questionnaires and tasks together into what we call
an experiment tree. And this is also where you can implement some randomization as you can see here
in the image where we send participants down to different tasks. And we've seen very complex
experiment trees in the past. Some of them have loads of different nodes to it and some are quite small and
straightforward. All those are perfectly fine research designs. Now I mentioned advanced functionality like
eye tracking, mouse tracking, audio stimuli, microphone recordings, webcam recordings, drawing tasks and a
lot more. Today we're going to focus on audio mainly and I then also have a few features which we're going to
have a look after Neil's presentation. But for now I'm going to hand over to Neil for his presentation
and the work he's done with Gorilla. Thank you very much. So again, just give me a moment. I've not been
doing much teaching on Zoom these days. So let me just make sure I'm clicking all the right buttons.
No problem. I'm sure we're all very patient with you. So take your time to get set up.
Right. I'm going to share the screen. Is that just Zoom? That looks good. I can't see the Zoom.
Okay, perfect. Okay. So thank you very much. I'm really pleased to have been asked to do this
presentation. I'm going to talk about some of the work that I have been doing over the last couple of
years in voice perception, but particularly looking at particular types of cognitive biases.
And all the work I do is in Gorilla. So any papers that I link to or any preprints, you'll get links
to all my materials and stuff in there as well. So my work has generally involved investigating the
cognitive processes that are involved in speaking your mind. And what I mean by that is actually
minority, indigenous, non-standard and dialect language varieties, because they're often forgotten
about in psycholinguistic research. There's often a focus on a very small number of standard languages.
Fair enough, they have a high number of speakers, but we often forget about what it is to speak a
dialect or if you use a minority language. So all my work has kind of been involved in that domain.
In particular, I work with speakers of Scots, which is a Germanic variety. It's got a lot of overlap
with English. It's sometimes given the status of being bad English because people don't recognize
it as its own sort of legitimate variety. And if you're interested in any of my previous work,
I've got a couple of blog posts that I've written in the past that you might find interesting to read
about that stuff. But more recently, I've been interested in not just what's happening in the
sort of cognitive processes involved in speaking these, but how are these people perceived by other
listeners? And in doing work in that area, it started to get me more and more interested in voice
perception and speech perception work more generally. So if you think about it, human speech and its diversity
of accents, dialects, and languages allow us to extract a lot of rich social information from
a human voice. We could tell, if you imagine that you've answered the phone and you don't know
who's on the other end of the line, from the sound of their voice, you can pick out quite a lot of
features about who they might be. So you might be able to have a sense of their gender, their regional
origin, their social status, things like their emotional state, and even personality characteristics.
Listeners are actually quite consistent in what they extract from a voice and how they map onto
certain personality features. And there's different models of voice perception.
Some propose that there are a lot of similarity to face perception. You can think of the voice as like
an auditory face. And that we actually not only extract information about who the voice belongs
to, the identity of the speaker, but also lots of other social information as well.
Now, I'm just going to test, see how good we are at identifying voices. I'm hoping my sound works.
And so let's see if this will do what I want it to do. If you can just put in the chat, maybe Ioana,
you could let me know some of the responses because I can't actually see the chat on my screen just now.
So here's the first one. Love their country. Love the country. Is that working?
We can hear it. And Trump is coming up three times already in the chat.
Love the country. Yeah, well done. Good. Nice, easy one to start with.
Well done. So yeah, so you, just from the side of his voice, you are able to identify who that person is.
Okay. Speaker B, let's go. If I can get it to work.
Yeah. And it takes me a while. Yeah. And it takes me a while.
Yeah. And it takes me a while.
Yeah. Any suggestions? Adele. Adele. Yeah.
Yeah. Yeah. Two times. I sometimes get Gemma Collins, but yes, Adele is the right answer. Well done.
Yeah. Well done. Cool. Great. Okay. Okay. I want you to tell me who this person is.
Yeah. I just want to do something a little different. Yeah. I just want to do something a little different.
Okay. I'm deliberately misleading you. This is not a voice that you're familiar with, but what I want
people to do is just to, from hearing it, tell me who this person is. How do they, what is the picture
that you paint of them socially? Is it a man? Is it a woman? Roughly what age are they? What can you
sort of tell about them? I'll play it a few more times. Yeah. I just want to do something a little
different. Yeah. I just want to do something a little different. Yeah. I just want to do something a
little different. Any idea where they're from? Is there anything coming in? Woman, 40ish at the
moment, woman, thirties, Scottish middle-class woman, middle age, female, late twenties.
Yeah. I just want to do something a little different. So it is an unfamiliar speaker.
Yeah. I just want to do something a little different. I'm looking at, I've gone ahead in
my slides. But it's actually me. It's my voice changed using an AI system. So nobody seemed to get
that. So yeah, that just kind of shows you how much this technology has come on. I've never had
anybody guess that it's actually me. I'm not so sure I'm happy about the middle age, but I can take
the sort of middle class. I'll take that one. So leaving that aside, our voice is a key part of our
self-identity. But I think that a lot of people find is they don't actually like their own voice. So
again, I'm going to throw that to the chat. Who here likes the sound of their own voice?
Having to look back into recordings of myself, it's never an Android task of mine. So I definitely
identify with that. No one in the chat either. They hate it.
Okay. Okay. Again, it's very few and far between that I ever have somebody that says that they do
like the sound of their voice. I feel like I'm neutral on mine now. I think I've just heard it so
many times on recordings that I don't hate it anymore. But that's not to say that I actually
like it either. So what if I told you that you could actually take on a new voice and it could
become yours? That might be quite an intriguing prospect for a lot of us. So let me just delve into
some of the theory that will underpin this. So I'm going to talk about the self-prioritization
effects. So our cognitive system is highly biased towards information that relates to ourselves in
some way. So for example, I'm sure we've all experienced, you're in a crowded room, you hear
your name being spoken and you've honed in on it over all the other noise that you're kind of processing.
And that's because, again, it's a self-relevant cue and our attentional system has given it priority
and given it that focus. And what this does is, particularly in experimental tasks, we can see
that people are faster at processing self-relevant information and they're more accurate at identifying
the self-relevant stimuli as well. But what's interesting is that even stuff that is owned by us,
but even for a very brief amount of time, or even really arbitrary associations, can activate that
system and become part of our kind of self-processing bias. So just to demonstrate that, this was first
introduced in a paradigm by Sui et al. And what they did was assign shapes. So again, just arbitrary
kind of assignments of a circle, a triangle, and a square. And they assigned them as belonging to
either you, a friend or a stranger. And then what they did was a perceptual matching paradigm where
you saw a shape and you saw the label that it belonged to, and you had to see whether it belonged
to that person or not. So for example, if they had a square and a stranger, they would respond to say
that that was a match. If they had a triangle and you, they would respond to say that that was a
mismatch. And if it was a triangle and a, sorry, if it was you and a circle, that would be a match.
And essentially what this paradigm showed is that the participants were way faster and more accurate
at categorizing the shape that corresponded to the self. So even something, again, a very arbitrary
association, a very temporary one, this is such a powerful system that it just gets absorbed into
it. And you see this bias for this new, completely, essentially silly kind of association with you
in a shape. And this has also been shown for other types of more relevant stimuli, like faces and names
and stuff like that. Now, Payne et al investigated whether this self-prioritization
effect existed for externally produced voices. So as we know, like our voice is highly personable.
It tells a lot of information about who we are. So could we actually take on another voice as being
ours? And would it actually work for this? And what they did was, again, use a similar paradigm,
and they had three external voices. One was assigned as belonging to you, one was assigned as belonging to
a friend, and one was assigned as belonging to a stranger. And again, a perceptual matching task,
you heard the voice, you saw a label, and you had to see whether the two matched or were a mismatch.
Now, just to emphasize, if the labels were assigned as belonging to Tom, Dick, and Harry,
we might not be any particularly faster than any one of them, and compared to the others,
it might just be equally good, equally fast, equally accurate across the three names.
But if it's associated as belonging to us, like you, friend, and stranger, we might expect a pattern
where they're faster with the new self-associated new voice. So the hypothesis here was that listeners
will have faster reaction times and greater accuracy for this new self-voice compared to the friend or
stranger voices. And this is exactly what they found. As you can see on the graph, the self-voice had
faster response times, and it also corresponded to higher accuracy as well. So the hypothesis was
supported here. So really interesting. I should say that the materials for this featured in a Gorilla
Spotlight, and this is what inspired me to start doing this work. I thought it was such a cool study,
such a cool paradigm. And it was seeing this, and the fact that it had been made as Gorilla Open
materials, that I was like, oh, I'm just going to take that off the shelf, and I want to do something
with this, which I'll come to in a second. I'm getting ahead of myself again. Just excited. So in a
second experiment, they wanted to investigate whether having greater links with the self, with this
external voice would boost the self-prioritization effect. And how they investigated that was by
gender matching the voice that you got with your own gender, or they had gender mismatching voices.
And so the hypothesis here was that if greater similarity with this external voice and the actual
self, if there was more similarity, you should prioritize it more. And how this manifested was
essentially they found the same pattern of results in both the gender match condition and the non-gender
match condition. So this didn't actually appear to have any effect. It didn't matter if you were a
male getting a female voice or a male getting a male voice, you would prioritize them equally to the
same extent. So the hypothesis here was not supported. And I'm still really intrigued. I would
have expected that to have been the case. In a third experiment, they investigated whether the link with
self could be boosted by the act of greater ownership over the voice in the form of having the agency to
choose it. So there were still three voices in the experiment, but instead of one randomly being assigned
to the self, you actually picked which one would become self. And we see from the other kind of
self-biased literature that choice and ownership are factors that will boost self-prioritization
effects. So the hypothesis here was that there'd be a greater self-prioritization effect for the voice
when it was chosen compared to a group who did not have the choice. And this did show an effect,
although it manifested in a slightly unexpected way. So rather than this
those that chose the voice, rather than the self voice getting faster,
it seemed to actually deprioritize the other voices. So it kind of made them slower. So in relative terms,
the self has had greater prioritization, but through the kind of slowing down of the other voices. So
what happened in the choice condition was that the friend and the stranger became sort of equal to each
other. You normally have a self as faster than friend, as faster than stranger, but here the
friend got deprioritized. Whereas in the no choice condition, you saw that classic effect. So we can think
of it as self faster than friend and friend is equal to stranger when you had choice. And in the no choice
condition, you get the classic self as faster than friend and friend is faster than stranger. So the
hypothesis was supported here. So across three experiments, there was a clear self-prioritization
effect found for these newly self-associated voices. Gender matching did not seem to significantly
influence this prioritization, but actually having some choice, some agency and selecting and having a greater
sense of ownership over that voice did boost the effect.
So this is interesting because it shows us that a voice doesn't have to be self-generated to become
part of us. And I think this is good news if you are somebody that uses, say, a communication device.
If you are somebody that wants to play a video game where you're selecting the voice of your character,
it's good news for that. And it's also good news if you're a sea witch wanting to steal the voice of a
mermaid to commit a very elaborate romance scam. So good news in that regard.
As I mentioned, the Payne et al. paper was a bit of an open science dream. So they had all their
pre-registration, all their data and scripts available on the USF. They'd made their materials
available as guerrilla open materials. And the paper was an open access published paper as well. So it
just meant it was so easy to kind of come along and just be able to take what they had and start
sort of running with it. So this allowed me to come along. I was interested. I thought it was
such a cool paradigm. I was starting to get more interested in voice perception stuff.
Immediately, I had a sense of, OK, I want to do this with Scottish voices. I want to do sort of an
accent. I'll replicate the paradigm, but I just want to see if it generalizes as well. So essentially,
I took the materials from the guerrilla open materials, but I put new recordings in of three
Scottish men. So I did my pre-registration and essentially I replicated the results in that I
found a self-prioritization effect. They were faster with the self-voice compared to the other two. So
that was cool. It was, you know, I think replication is important. So it was nice to be able to take that
and just show the same results. But a question remained here, which is we can take this new voice
on. And obviously I've mentioned before, this system is a very powerful system, but how does
it compare to our actual own real voice? We can prioritize a new one, but would it overcome our
real voice essentially? And I thought this was particularly interesting, as we've demonstrated,
in that most of us don't like the sound of our own voice. So particularly in an external recording,
where we're not actually hearing it through the way we hear it in our skulls, through bone
conduction. There's always that kind of dissonance that happens when hearing a recording versus how
we sound in our own heads. In some other self-biased paradigms, sometimes that temporary association
that you're given in a task is a very, very powerful one and actually overrides our own sort of
information. But we saw from the Payne et al. study that this greater ownership increases
prioritization. And what do we have the greatest ownership of? Our own voice. So we wanted to
investigate whether being assigned a new voice could overcome the prioritization of your actual real voice.
So what we did here was we adapted to the paradigm so that we included participants' own voice this time.
So we did a phase one where we got recordings of our participants saying hello, and then we extracted those
recordings and then put them into a new version, invited them back for a second phase. A third of the participants were assigned to a version where their real voice,
was assigned as belonging to you. So there was a kind of consistency there. A third of the participants
were assigned as having their real voice assigned to being a friend. And the other group had their real
voice assigned to being a stranger. And for those two groups, the friend and stranger, they got a new voice as being the self-voice.
This all gets very complicated, all the self and real and own and new and all the rest. So I might have to emphasize these points a few times.
So we were interested, what will happen when the voice is signed, this new voice is assigned, but your own voice is in the paradigm.
So for the group who were given their own voice as belonging to them, I'm using the arrows to point to
where the real voice was in these conditions. Unsurprisingly, they were quicker with their own
real voice, probably actually the fastest we've seen in any study. So you actually seem to be
fastest when it is your own voice in the role assigned to you. So nothing too surprising here.
In the group whose own voice was assigned to friend, but they were given a new voice as the self,
we actually found that the real voice was faster than the stranger and it was actually faster than the new
self-associated voice. And likewise, we found the same thing happen in the group whose own voice was
assigned to stranger. It was faster than both the friend and the self voice. Essentially, in all conditions,
the participant's real voice was prioritized. So we know that the association of something to the
self is very powerful. In this case, it wasn't overcome by how their brain focused on their actual
voice, even when it was a recording and it might be something that they don't like the sound of. It was
still very, very powerful. So our brain is very biased towards the sound of our own voice.
And this, like I say, the strongest boost was for when it was actually your own voice was in
the self role. I may come on and say this in a second, but I think this has a lot of implications
for voice technology because with the systems that we have now for voice cloning, it might be really
effective to have your own voice being used in a system. The technology is there.
So again, in terms of open science, I've got a project page where you can access the pre-registrations,
you can access the data. I've made my code available. I've made the tasks available on
guerrilla open materials. And again, we actually published in the same journal as
Pain et al. So it's open access, British Journal of Psychology. So what was interesting, one of our
reviewers offered a suggestion for an analysis, and we ended up doing something slightly different,
but I think it has been quite insightful. They were interested in whether the bias for the own voice,
in terms of how quickly you responded to it, was affected in any way by how acoustically similar
the new voice was to the participant's real voice. So what we did here was we measured and mapped the
voices onto a voice space. So we take certain acoustical features out of it. I can't remember what
the formula is for doing this, but there is a formula that's out there. And what this does is you plot
the voice on an XY coordinate grid. And then what you can do is measure the distance, the straight
line distance between your actual, the real, the participant's real voice and the voice that they
were assigned to. So this was only for the participants who were given a new voice, rather
than the ones that had their own voice assigned to them. And essentially what we did was calculate
and run a correlation between the degree of acoustical distance between their voice and the new voice,
and how much of a bias in terms of how quickly they could respond to their real voice. Sorry,
this is complex. I need to remind myself how this works. And essentially we found a correlation
in that essentially the way I would interpret this is that they had a reduced bias for their own voice
when this new voice was more acoustically similar to their real voice. So in other words,
the more similar the external voice was to the real voice, the less of an advantage that they had.
So it kind of suggests that it's not something about our own voices like so super, got this special
position for it in our brain. It might just be that it's highly familiar. So something that has some
degree of similarity to it might actually disrupt that system a little bit more. So although I've said it
might be useful to have a voice, our own voice with technology, given that people don't like using
the sound of their own voice, they might not want to use their own voice in a, I don't know, in their
sat nav for example. But maybe a voice that's got a lot of similar features to yours
might be beneficial, but not give you that dissonance that comes from hearing your actual own voice.
So what other ways are there of making an external voice more similar to your own? Well, you know,
bringing it back to where I was interested in the first place, accent. I'm really interested in,
would I prioritise a voice that had a Scottish accent more than one that had an English accent,
for example? Now, accent is this real sort of strong indicator of who we are and where we're from. So it's
proposed that dedicated systems evolved to be able to extract social information from voices,
helping increase our survival. And language and dialect and accent are features that can rapidly
evolve over quite a sort of quickly evolving over quite a short time geographical location as well.
They're basically a very good indicator of who's in our in group and who's in our out group,
and who our coalition partners might be and things like that. And another aspect of that is they're
very unique to who we are and the group that we belong to, but it's actually quite hard to fake an
accent authentically to sort of infiltrate another group. So they actually are quite a strong signal
of our social identity. So I was interested in whether accent matching would have any effect on the SPE,
the self prioritisation effect. So I ran a version of the paradigm where I had Scottish and English
participants and they were either given all Scottish voices or all English voices. So the accents of the
voices either all matched their own or were a mismatch, much in the same way that Tay Natal had done for
the gender matching. So as I've said here, a Scottish participant with English voices would be the mismatched
accent condition. And essentially what I found was that in the matched accent condition,
condition, we found that same effect that had been observed previously. Self was fastest, followed by
friend, which had become deprioritised into being equal to stranger. But in the other condition, we saw
that typical pattern of self is faster than friend is faster than stranger. So basically accent matching
had the same effect as what choice had in the Pay Natal experiment three. So under that interpretation,
I think we can see that actually having an accent of a voice that matches your own boosts the effect.
What I think is really interesting about this is
this didn't affect the participant's subjective identification with the voice. So when I asked the
Scottish participant, how strongly did you identify with the voice? And it was English. It didn't matter
whether it was English or Scottish. It was all the same. Nothing came out there. Nothing also came out if I
looked at the actual Scottish and English participants separately. It was just the same effect. It wasn't
stronger in one of these groups. I think this is happening at a much sort of deeper processing level
than it is something particularly subjective and that the participants were aware of.
So we see that both accent matching and choice boost this effect. So of course,
my next question was, why not both? Let's see what happens when you do accent matching and you've
offered that out of the three voices that they're getting, they can pick the voice that they're going
to get. Now, this starts to get quite complicated. So what I've done here is just measure the distance
between the self voice and the friend voice. As I've shown before, the friend kind of gets
the distance between them increases with greater prioritization. So essentially, bigger bars on
this next graph are a sign of greater prioritization. So we can see that from experiment one, the matched
accent condition has a greater bias for the self compared to the friend. When we look at experiment two,
the red bar here is basically choice and accent matching. It's a really strong boost. So I think these two
things are additive and can really considerably increase the prioritization of the new voice that
you're given. I wanted to have it finished today, but we've got a third experiment that I'm not going
to talk about. So I'm hoping to have the preprint of this out next week. So just if you follow me on
LinkedIn or Lou Sky or anything, I'll post about this when it's available. But the third experiment's
quite cool as well. So yeah, again, this is good news. I've seen actually quite a few of these good
news stories. They're actually very poignant stories about people who don't have the ability to speak,
maybe using a communication device, who've now been given a customized voice. So in one of these cases,
it was he wanted an accent of his late father. And it sounds as if his father lived in a particular
place that had a real mix of accents. So they're able to create this voice for him and he was able
to choose that voice for himself. And the effects of, you see the reactions to having this new voice,
it's incredibly emotional. And so this self-prioritization effect, I think, actually can
give us some insights into not just the subjective alignment with the voice, but actually how do we
prioritize it more? How do we sort of attune our brain to it more? If we're in the absence of using
a voice that is ours, if we're choosing an external one, I think there are ways that we can boost this
effect. So yeah, I think this research and these findings have got a real insight into all types of
voice technology. So just to give a bit of a summary, we can take on an external voice and activate this
self-prioritization effect. It's a very powerful thing. However, it does seem to be strongest for our
own voice. But we have shown that the SPE can be boosted via greater ownership in the form of choice
and similarity, whether that's in the form of acoustical features or accent. So maybe we could
use AI to customize an ideal voice for ourselves to use in these technologies. Again, I'm thinking
about in a video game, would I play differently if it was a voice that was either mine or one I'd
customized ideally for myself? I don't know, if I had to slay a townsperson, I might not want to do
it anymore if it's me saying it. Or likewise, it might be more distressing or more real to have pain
inflicted on me if it's my voice that's kind of crying out in pain. So I just want to finish up by,
I'm just going to touch upon some other work that I've done investigating how we perceive these AI
voices. I'm really interested in the development of this technology. As I mentioned, accent, dialect,
language, they're really, there's a lot of information that we can extract about an individual
based on how they sound. And some of these things come from their accent and dialectal features.
And these are hard to fake. But with this technology, as I demonstrated earlier, I can break
the system now. If you all thought I was a middle-aged, middle-class female, I've kind of got you, you
know, there's positives and negatives, I think, to this. So I was interested in how
well we could perceive, whether we could tell the difference between a natural recording versus one
that has been enhanced to give the speaker a new identity. Because although I'm talking about some
positive applications for communication devices for entertainment, there are also implications
for things like cyber fraud and security. So I just want to plug a preprint that I've had
out for a couple of weeks. And I'll just give a very quick run through of some of the main findings
here. So as I say, I was interested in how well people could tell natural human from AI enhanced voices.
And I used the feature on 11 labs to change the voice of the speaker and give it a new identity.
And I was interested in whether for less represented dialects, so like the regional dialect from my
city, you're not used to hearing technology use it. It's likely not featured as heavily in the training
for these systems as like standard English has. I noticed sometimes that I could hear little artifacts
in the recordings, and it would sound a bit mechanical. So maybe you can detect it that way.
But also, maybe your prior experience with technology means that you don't associate it
as being able to represent your dialect. So I've used this, you might have seen this video online
of the guys trying to speak in Scottish accents to activate a lift, and it just gets increasingly,
they get increasingly exasperated because it can't understand them. But I do think it underscores a
serious point, which is that certain communities are kind of excluded from voice technology systems,
whether that's voice recognition or speech recognition, or these new production side of
things. So again, I run my experiment on Gorilla. The participants heard the speaker and they just had
to say whether it was human or AI. I'm interested in how well they can do this. But in particular,
I'm interested in what kind of response bias do they have? When they're unsure, do they have a tendency
to move in one direction or the other? And how I'm holding for this is whether they have more of a
liberal response or a conservative response to indicating that the voice is AI. So in this case,
a liberal response would be to detect that the signal is there and say that it's AI, more conservative
would be to sort of say that it's human. And this gives me a score for each participant between
minus one and one as to where their bias lies. So in my first experiment, I had mostly Scottish
participants, their overall bias was 0.22. So it's above zero. So it indicates that there is a bias
towards responding human in general. But in this group of participants, it was essentially completely
manifested in the regional dialect compared to Scottish standard English. So when they heard
a recording that had been AI enhanced, and it was speaking the dialect, they were
way more likely to say it was human than they were when they had a standard Scottish English recording.
So to try and interpret that further, my sort of feeling here was, I think this is this whole thing
of we don't expect technology to be able to do these dialects. So the more familiar we are with the
dialect, almost the more that we are taken in by the fact that it's that we think that AI is actually
human, because we understand that really well, but we don't think technology can be that good at it.
So what I did as a follow up study was get some English participants, recruited them on Prolific,
and sorry, I should have put those statistics up. They again had an overall human categorization bias
that was in a similar ballpark. It wasn't significantly different overall, but how it manifested in the
across the two dialects was different. So it was stronger in this group in the standard variety,
and essentially that didn't actually come out significant. It's a much smaller effect size.
So in the Scottish participants, their familiarity with the dialect meant that they were more likely
to believe it was human. In the English participants, there was a similar bias overall, but
it didn't manifest differently. So again, I think the interpretation here is that basically,
the more familiar you are with the dialect, the more you believe that it might not be able to be
spoken by or used by an AI system. So just to, sorry, I'm going back again. To finish up, what does
this mean in practical terms? Well, I do think the consequences of the types of errors you made in
this task could be, have different consequences essentially. So if I'm answering the phone and it's
actually an AI voice and I'm being scammed, but I think it's a human, I think that's got worse consequences
than if it's actually a human. And I say that they're AI, I might just cause a little bit of a confusion or
embarrassment or a misunderstanding. But if I actually believe in AI voice to be a real person, I might be more likely to
to be taken in by, I don't know, a phishing scam or something like that. So despite the consequences
of misidentifying AI, we're still seem to have this stronger bias for thinking it's human. So I think in general
for now, I think we need to start getting more skeptical and start sort of seeing if we can nudge this bias.
So in general, my participants in both groups weren't that good at telling humans from AI overall.
So that has some good news for using AI voices for greater accessibility, for greater language
representation. But as I mentioned, it's got some quite serious implications for cyber security and
fraud and manipulation. So I've just been awarded some funding by the Scottish Institute for Policing
Research to try and do this. So I'm going to try and see what I can do to protect us all against
misuse of AI voices. But yeah, sorry, I think I've overrun time, but that's me done. So thank you very much.
That's no problem at all, Neil. This was really fascinating research. And I particularly enjoyed
the impact you listed out for the studies you've run and how that can actually be really beneficial for
for people who are looking for their own voice and who are not able to speak, to identify and how
that contributes to their real identity. And that's fascinating. I had never thought about that.
I saw recently a video about one of these cases that I reported it showed up on Instagram,
and I actually started crying. I understand. I'm not that emotional a person. And it was just,
it really made me feel like, wow, this stuff has these insights. And yeah, I just, I felt just so,
sort of, yeah, it was so cool to be doing stuff that can have this kind of real world,
and they can have these benefits. And again, it can also have benefits in a more entertainment context as
well. So I think there's a real, these psychological insights on these sort of cognitive biases can,
can have real practical implications for the development of this technology. So.
Definitely. Yeah. We've had plenty of people in the chat saying this was a fascinating talk and
how important this topic is. And thank you for your presentation. So yeah. Thank you, everyone.
Thank you, Neil, for being here and presenting this really cool work you've done. And good luck for
the funding and the research you're going to do with that.
Yeah. Thank you. It's great to get such a positive response. So yeah,
I'm really excited about what comes next with this.
Yeah. I'm sure loads of people will follow you and follow your updates about your research. So
that was really great. Thank you so much. Have a good winter break. And I hope to see loads of
you again in the new year for our new webinars. And in case you're interested in them. And yeah,
happy researching for everyone. Thank you.
