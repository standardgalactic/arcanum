Testing on YouTube.
Testing on YouTube.
YouTube is
working. How's it going? Open room, XYZ, NLP prompter, and
MS.
Alright, now let's test on X.
Testing on X.
Testing on X.
Is it working on X?
Test?
Alright, works good.
Let's quickly swap over here. Swap to this.
Swap back.
And I think we're ready to go.
Alright.
Alright, welcome everybody to another Hoopost stream.
How's it going, Prateek?
How's it going, Prateek?
Today we're going to be...
The stream is called Diffusion Versus Autoregressive.
This entire thumbnail was generated by the image generator that's built into OpenAI's
GPT.
But I thought it was interesting that it misspelled autoregressive here.
Put three S's.
Kind of interesting, right?
Kind of interesting, right?
It got confused with the number of S's.
But pretty amazing that it can actually generate such cohesive text.
So we're going to be looking at this paper right here.
This is a paper that came out relatively recently here, 21 July 2025, called Diffusion Beats Autoregressive
in Data-Constrained Settings out of Cardi-Mellon University.
This is a paper that came out relatively recently here, 21 July 2025, called Diffusion Beats Autoregressive
in Data-Constrained Settings out of Cardi-Mellon University.
How's it going, Julien and Ed?
And MLS.
It's kind of interesting, right?
It got confused with the number of S's.
But pretty amazing that it can actually generate such cohesive text.
So we're going to be looking at this paper right here.
This is a paper that came out relatively recently here, 21 July 2025, called Diffusion Beats Autoregressive
in Data-Constrained Settings out of Cardi-Mellon University.
How's it going, Julien and Ed?
And MLOps and GenAI Decoded.
3Blue1Brown, he recently posted a video on this.
Yeah, actually, I have that pulled up.
I watched it this morning.
This is, you guys should definitely watch this channel, 3Blue1Brown.
Very, very good channel.
But this is like a guest video by this other channel called Welsh Labs, which is similar in nature.
It's basically just like math and engineering concepts explained in very nice kind of visual ways.
The Welsh Labs video actually has a very good video for the MLA, which is the Deep Seek Multi-Latent Attention.
So highly recommend both of those channels, 3Blue1Brown and Welsh Labs video.
But we are going to actually look at that for a second.
But let's get back to this.
So this paper is basically comparing two different, the two dominant model architectures, which is basically just the structure of the computation that happens inside a model, right?
And there's kind of two camps here.
There's what's called autoregressive models and then there's what's called diffusion models.
And people might be familiar with transformers, but transformers exist in both of these paradigms, right?
You have diffusion transformers and you have, obviously, transformers are used in autoregressive.
So this is kind of more high level than that, right?
So let's get into this.
How's it going?
87GN.
Autoregressive models have long dominated the landscape of large language models, driving progress across a wide range of tasks.
Recently, diffusion-based language models have emerged as a promising alternative.
So this is something that is pretty crazy and most people don't know, but there are diffusion-based language models.
And for example, Gemini Diffusion is an example of that.
These are kind of a novelty.
So these aren't something that people use every day.
You know, nobody's really using text diffusion models in their everyday workflows, but we are starting to see people kind of test this out as a possible idea, right?
So this is kind of more of an experimental or kind of research model.
But they are very cool to look at, right?
So you can actually see that in this case, you give it a prompt, which is this math question, and then it kind of diffuses the answer out of text space, which is a very strange kind of, you can see how it's kind of hard to see because they do it quite quickly here.
But in this little animation, you see how the text starts off as noise, and then it kind of just diffuses out.
So you can actually see the tokens are kind of shifting.
So the tokens start off as kind of noisy random tokens.
So you can see it starts off as like this random string of twos, and then it ends up on just the number two or this word square.
It starts off with power, exponent power denominator, then square, right?
So each individual token is kind of taking this little trajectory to get to the final token for the final output, right?
So it's kind of very similar to the core idea of diffusion model, which is basically kind of reversing time and going from a pure noise to a point in this high dimensional space that represents a coherent semantic concept like a cat, or in this case, the answer to this question.
All right, we systematically study mass diffusion models in data-constrained settings where training involves repeated passes over limited data, right?
So this isn't even just true in data-constrained settings.
I would say in pretty much all of machine learning, usually you're not training on your data set once.
You're training it on it multiple times, and this is called epochs, right?
So you might train for 100 epochs.
You might train for 10 epochs.
With some of these kind of super large pre-training runs where people are training on the entire internet, the idea of doing many, many epochs is kind of not necessarily possible, right?
Just because you have so many tokens in your data set that you'll just go through them once.
But historically, when the data sets were small, and especially if you're doing something in, for example, robotics, where you have a very small data set, you're going to have a lot of epochs over that data set.
The stream started great.
All right.
Diffusion models make better use of repeated data, achieving lower validation laws and superior downstream performance.
We interpret this advantage as implicit data augmentation.
Mass diffusion exposes the model to diverse distribution of token orderings and prediction tasks, unlike AR's fixed left-to-right factorization.
This is basically the key finding in this paper, right?
Is that inside the training process of a diffusion model, there is kind of this built-in data augmentation, which ends up making them much more robust to repeated epochs or repeatedly seeing the same data point over and over again.
Because even if you're feeding the same data point in, you're kind of getting a different path from noise to image, right?
So even if you keep feeding the same image of the cat in to this diffusion model during training, you're always starting from a different noise, right?
So it's like you're going from a different noise to this cat.
And then maybe one epoch later, when you see the cat again, you start from a different noise, right?
So there's this almost like built-in augmentation, and that's why these diffusion models perform better or beat the autoregressive models in a data-constrained setting.
That's basically the key finding.
So if you don't have a lot of time, you can basically turn off the stream right now because that's it.
All right.
We find new scaling laws for diffusion models and derive a closed-form expression for these critical compute thresholds at which diffusion begins to outperform AR.
So scaling laws, you might have heard that term before, and I have a bunch of different papers pulled up for scaling laws, right?
Scaling laws is almost like an entire niche now, where here's, for example, a paper called Scaling Data-Constrained Language Models.
Here's another one, Training Compute Optimal Large Language Models.
This is the chinchilla paper, as it's, if you've ever heard someone talk about chinchilla scaling laws or chinchilla optimal, this is what they're talking about, is basically this specific scaling law paper.
You have this one here by Apple Distillation Scaling Laws.
And with scaling law papers and scaling law research, what it is, what the point of scaling law research is, is trying to find a pattern or a law, I guess.
Law is kind of a strong term for it, and I'll kind of go into my explanation as to why I feel like laws or the use of the word law for scaling law feels a little bit too strong.
What they do in these papers is they'll train multiple different sized models for different amount of times, maybe on different sized data sets, right?
And then they'll compare the test laws.
So they'll say, okay, here's different amount of models or different sized models trained on different sizes of data.
In this one here, they'll have maybe the relative size of the student model versus the teacher model.
So all these scaling law papers are basically these hyperparameter sweeps over the core parts of the model, core properties of the model, right?
So the model has some depth to it, right?
Which is how many of these transformer blocks or if you want to think of it in like a fully connected neural net, right?
Like how many of these layers of neurons do I have in this model?
So models usually have some notion of width, right?
Which is like how big is the kind of vectors that I'm using inside this model, which are a way of thinking of like how big is the embedding space in which I'm kind of creating these concepts and multiplying these vectors if you're doing the attention, right?
Like what is the width of those vectors, which is another way of saying like how much capacity is there inside this high dimensional space for me to kind of like create this embedding space that can kind of group together semantically related things, right?
So that's kind of the notion of width.
You also have the notion of the data set size, right?
Like what is the size of the data that I'm going to be feeding into this model?
You also have the training time, right?
How many tokens am I going to give to this model?
And then even those two there, right?
The data set size and the training, and this is what this paper kind of explores a lot, is this idea of the, what do they call it here?
They have a specific term for this.
R D star.
So R D star is a learned constant that characterizes the number of epochs after which training more epochs result in a significantly diminished return.
So it's almost like the amount of knowledge that's inside, the amount of knowledge that you can extract out of a data set per epoch.
I think they have a better definition of this.
Let me see if I can find it.
They introduced this term here.
Here, R D star represents the half-life of data reuse, right?
So whenever you're giving that same data to the model again, right, performing multiple epochs, you're reusing that data, right?
So they introduced this concept of this R D star where repeating data beyond this R D star epochs will result in diminishing returns, right?
So you can't just keep getting more information out of the data every time you feed it.
If that was the case, then, you know, we would already be at ASI, right?
But the reality is that even if you just keep feeding the internet to the same model over and over and over, at some point, it's kind of extracted all the juice that there is, all the intelligence that there is in that.
Open room XYZ, do you think there's a lack of data ever really present in reality as you can almost always generate more data, basically?
No, I don't think the answer to that question is very nuanced, right?
Because I don't think there's a limit to data, right?
I think that you can always generate more data.
There's no limit to the number of chairs or paperclips because you could theoretically just create infinite paperclips and then have a robot that goes into a distant star system and then consumes the entire star system to create more paperclips.
But there is a limit to the amount of paperclips, and that limit is basically like what can you, the amount of iron, I guess.
I don't even know if paperclips are made out of iron, but how much ore can you gather?
How much energy do you have available to convert this raw ore into paperclips?
And it's the same with data, right?
Is that there is no limit to the amount of data available, right?
You could just keep generating more and more data, but generating data costs energy, and there's a limit to the amount of energy you have, right?
Just because we live on a resource-scarce planet.
And then there's also a limit of kind of computation where at some point, even if you're generating infinite amount of data, you would need even more computation to generate more data, right?
So then you almost are limited by time, right?
Where it's like, okay, there's a limited amount of time.
Each time you generate data, you have to take some amount of time, some amount of computation.
So at some point, you get limited by time.
But yeah, there is no data ceiling.
If you have infinite energy and infinite time, you could generate infinite data.
But we don't have infinite time, and we don't have infinite energy.
Okay.
Let's go back up to the top here.
All right.
And there are different takes on this, right?
Because I feel like as we've evolved, right, as we've gone from the 2000s to the 2010s, now to the 2020s, we've hit different walls.
And sometimes we've been data constrained.
Sometimes we've been compute constrained, right?
So if you think about even the 80s and 90s when people like Jan LeCun were doing some of their elementary work, right, the beginnings of kind of deep learning, they were pretty much compute constrained.
They were also kind of data constrained, but they were very much compute constrained, right?
Because it's just the computers at the time were not capable of doing thousands and thousands and millions and millions of gradient computations and steps on a giant model, right?
It's just like you didn't even have the computers that were capable of pushing gradients to the scale required to get the intelligence that we have now.
So that was definitely – they were also data constrained because the data sets were pretty small.
But once you started to get to the 2010s, right, okay, now you get the internet.
Now you start being less data constrained, right?
You're still a little bit compute constrained.
And then one of the big unlocks in the 2010s was using GPUs, right, to do this training.
And when you unlock the GPUs, what did you really do?
Well, you kind of created a step function in the amount of compute that you could use to do this learning approach.
So once we kind of got rid of that compute bottleneck by using GPUs, then suddenly we could also increase the data.
And now we started to get that ImageNet moment where we had, okay, we increased the data a lot.
And now we increased the compute a lot by using the GPU.
And now suddenly we get to the next plateau.
And right now in the 2020s, some people think we're data constrained, right?
There's the famous Ilya Satzkever where he says that the internet is kind of like the fossil fuel of data, right?
There's a limited amount of internet data.
And now that we've trained and gathered all the internet data, we've run out of it.
So we're data constrained, right?
Some people think that, hey, we're actually compute constrained, right?
And even in the most wealthy of frontier labs like OpenAI and Google, right, like they don't have infinite compute either, right?
They only have maybe $100 million to spend on this training run.
And they can't actually spend a billion dollars on this training run because there's not even a company which will offer them $1 billion worth of GPUs, right?
So we're always in this kind of combination of data constrained and also compute constrained.
But in this paper, what they really want to drive home is that if you're in a paradigm where you're data constrained,
then diffusion models are definitely the way to go.
And that's what their scaling laws here show.
And we don't even need to trust them on this.
We can also kind of empirically get to the same conclusion where, for example, in my world, so I kind of do a lot of robotics stuff.
And in robotics, right, we have these pretty much, I would say everybody uses these now, but these are called vision language action models, right?
Which are also kind of a language model.
But the vision language action models that everybody uses, a core part of that is this action diffusion transformer.
This is the action part, right?
So this VLM, this is an autoregressive model.
So this is actually a combination of autoregressive and diffusion.
But this diffusion transformer is critical, right?
You actually, this is a very important part of the success of vision language action models in robotics.
And why would that be the case, right?
Like, why is it that the current best solution for robotics, a core part of that is diffusion?
Well, maybe it's because one of the huge problems in robotics is that it's data constrained, right?
Is that there isn't a data set at the size of the internet for robotics, which means that robotics as a kind of field is very much in this data constraint.
So, therefore, it makes sense that the go-to approach is a diffusion model, right?
So that's a nice little kind of empirical affirmation of the hypothesis here that anywhere where you're data constrained, you're going to see diffusion models as the kind of standard as opposed to autoregressive models.
How's it going, Andre?
Question here.
There is a limit to how much information, informative data you can generate.
Infinite Minecraft worlds won't teach you much about the real one.
Yeah, that's another kind of important notion there, too, where, and this is something that isn't very, like, theoretically, very, there isn't a strong theoretical understanding of this.
But there's a notion of kind of data quality, right?
Where one data point is not the same as another data point, right?
One picture, maybe in image space, it's a little bit less confusing.
But the important paper there is the textbooks are all you need paper, which feels like a long time ago at this point.
But in that paper, right, they basically created these synthetic textbooks, and they showed how training on those synthetic textbooks was led to better results than training on just kind of like generic internet slop, right?
So what that paper tells you and gives you an intuition on is that, okay, well, this much smaller data set of synthetic textbooks is somehow you can get more juice out of that than this giant pile of internet slop.
So, therefore, you should be able to take a paragraph of text and get some score for, like, hey, how useful, how high quality is this data, right?
And then if you kind of use that to filter a bunch of generated data, then you can get a bunch of high quality generated data.
And that's kind of what this whole reinforcement learning kind of explosion is over the past couple years is people realizing that, hey, you can actually generate data and then filter it down if you have something like math or code or a video game where you can basically say, okay, in a video game, or I always use the example of Go because Go is, like, the perfect example for reinforcement learning.
But in Go, you can have these synthetic games, right, where the AI just plays itself, and you can always say, okay, well, I can filter the moves to just the moves where the final six, the model ended up winning.
So I can take all these generated games, and I can filter them down to just the games where the model won, and now I have a high quality synthetic data set that I can use to push gradients into my model.
And this is a way to basically generate infinite amount of high quality Go data.
Right?
Imad Moustak now saying he's building an 8B model for medical and already outperforms GPT-4 in med.
Yeah, I heard that.
I was listening to that.
So Imad Moustak was, he's like this, like, hedge fund guy who basically, he's known for bankrolling stable diffusion.
So he created the corporate entity that eventually released a bunch of stable diffusion models, and then eventually they ran out of money.
And then he left the company, and then all the talent left, and now you have some of the talent in Black Forest Labs.
I think stable diffusion still exists as a corporate entity.
There's still kind of people there.
I think they're focused on, like, audio models or something weird like that.
But this guy, Imad Moustak, he now goes on the podcast circuit.
And then yesterday he was on the Peter Diamantis or something like that podcast.
And he was talking about, and I cringed at that take, honestly.
Whenever he was like, we're building, he has like this, like, British accent.
So the thing with British accents, guys, is that they make people sound smarter than they are.
There's so many people on the kind of general YouTube influencer sphere that because they have British accents, they sound smart.
But then when you actually think about what they're saying, it actually isn't smart at all.
And I feel like Imad Moustak is, like, the perfect example of this, where he was just sitting there in his little British accent.
And he said, we now have an $8 billion parameter model for medical that outperforms GPT-4.
I'm like, no.
No, you don't.
I haven't done the test, but I can almost guarantee you that if I had side-by-side your 8B parameter model and then GPT-4,
and I asked it a bunch of medical questions, GPT-4 would beat the shit out of your 8B model.
So, like, I don't know what the fuck your engineers told you,
but there's literally zero chance that an 8B model beats GPT-4 at fucking medical questions, right?
What probably, yeah, they overfitted on some data set.
That's literally probably what happened, is that he paid for some engineer to take an 8B model, probably a Quen,
and then they just overfit to shit on some medical data set and then just asked it questions from a benchmark
that is basically extremely close to the data set that they overfit on, and it got a higher score than GPT-4.
But that doesn't mean that it's actually better at medical questions.
That just means that it's overfit on that specific niche, right?
He is more Bangladeshi than British.
Yeah, I don't really know his origin story,
but I know that the British accent doesn't necessarily need to come from England.
That's happened to me before with, for example, people from Singapore
also have a kind of, like, British-sounding accent,
but they might not have ever even been to England.
And also, I know that a lot of rich kids in kind of countries like India or Bangladesh
that have kind of a British legacy to them,
there's basically these kind of, like, British private schools.
So if you're, like, a rich kid in Bangladesh,
you go to, like, the British private school,
and therefore the English that you learn is British.
So you actually learn British English,
and you sound like you're from England, but you're actually from Bangladesh.
So I think it's kind of that situation.
I think that soon there will be a time to generate a new data set
that will be terabytes big, so it'll be impossible to overfit on them.
Yeah, I mean, that's also why,
that's also another advantage of large data sets,
is that you don't, they have so much variance,
so much variety in them that it's much harder to overfit.
And maybe let's, we keep using this term overfitting,
but what is overfitting?
Overfitting is whenever your model kind of starts to memorize
or fit too strongly to the distribution of the data that you're giving it, right?
So this is how you would determine whether your model is underfitting or overfitting
if you were looking at a loss curve, right?
So this is like a loss curve, right?
Error is the same thing as the loss, right?
So over time, as your model gets smarter and smarter
with each batch of data that you feed it,
the loss goes down, which means that it's giving you the right answer, right?
Or in this case, it's producing the right tokens,
predicting the right next token of each individual batch, right?
And you can see the training always goes down.
So no matter what, if you keep feeding the same data to the model,
it's going to get better and better and better at knowing what the next token is
for that specific data set.
But at some point, it's going to kind of diverge like this, right?
And why does it diverge?
It diverges because hopefully, if you set it up correctly,
your test data set is very different from your training data set, right?
And at some point, your model is starting to rewire itself internally
so that it basically works very well for that training data set
just so it can keep getting even lower score and keep reducing that loss
because that's what it wants to do.
But it's no longer actually getting better at what you want it to get better,
which is kind of developing this kind of more loose model
that is better at fitting points that aren't necessarily inside that training distribution.
And that's when you see this divergence, right?
So this is another example that people sometimes use for that,
but I think this one's kind of harder to understand.
But like here's a model, right?
In this case, you have some data and you're fitting a model to this data.
The model is just a straight line, right?
So in this case, the straight line has some kind of closeness to the data,
but it's not, it doesn't fit it quite well, right?
So this is underfit, right?
In this case, now your model is this kind of curve
and that fits the data much better, right?
And it probably has a kind of a better representation.
So if I were to sample something in my test set that was out here,
this model would give you a better answer than this one.
But then you can get to the point of overfitting,
where the fit of this model, which is this squiggly line to these x's,
is better than here, right?
So if you actually took the loss for each,
the difference between this x and this red line for each of these points,
this one fits it better, right?
But the problem is that this red line doesn't actually represent
the underlying distribution of this data,
which is really the whole point of the model, right?
The whole point of the model is to give you a representation of that data set
that you can use to extrapolate to other points, right?
And at this point, you can't really use this to extrapolate at all, right?
Because it's so overfit to the specific data point,
or these specific data points.
What would you expect from a language model that would have 24 trillion parameters
and would have been trained on 100 trillion tokens with high variability?
It's going to be really fucking smart.
And maybe, so let's talk about scaling loss for a second,
because I think this is a nice little transition here, right?
But here we're talking about kind of fitting lines to data points, right?
And one thing that kind of always bothers me with these scaling law papers
is that they call these laws, but they're not laws, they're models, right?
And for example, right here, you have all these data points,
and then you have this line that fits these data points, right?
And there's kind of an implicit assumption here that, okay,
because this line fits these data points here,
then whenever we go out here,
it's also going to accurately predict what these data points would have done
if you would have done this, right?
So if each of these is, if these are 100,
so these points right here are small little models,
100 million parameter models,
and then these points up here are 10 billion parameter models, right?
And oh, look at that.
It's a very clear relationship here between the number of training flops, right?
How much compute you're spending on training,
and the size of the model.
And they're like, okay, well, that means that if we trained
a 100 billion parameter model on this many training flops,
then we would get this point.
But that's not actually true at all, right?
What you're, what you're, this is a kind of an extrapolation,
and this is the one paper that does this correctly.
So this is the Apple paper, Distillation Scaling Laws,
and they actually do this correctly here.
So if you actually look at their figure one,
which is the most important figure of the paper, right?
Every, if this is the cheat code to papers,
is that the abstract is the most important paragraph in the paper,
and the figure one is the most important figure in the paper.
So if you really want to get the summary of a paper in 30 seconds,
all you have to do is read the abstract and look at the first figure,
and that'll give you like 80% of the paper.
But in the figure one of this paper, you notice here how in their chart,
they have this solid line equals interpolation, dotted line equals extrapolation, right?
So you can see here that the solid line only exists between these two data points.
So they are self-aware that, hey, anything beyond this,
this dotted line is basically just made up.
We don't actually know, right?
And these dotted lines right here, these are all dotted,
which means they're all extrapolations, right?
So in this paper, they're at least kind of self-aware of the fact that,
hey, this relationship that we found,
this model that we've, or this line that we fit to our data points is just,
you can only really assume it is high,
it has high validity inside this range, right?
Within.
Interpolating, you can probably say it's pretty accurate,
but as soon as you start extrapolating, you know,
you don't actually know that for sure.
And that's why I feel like the use of the word law here and scaling law is kind of not correct
because these aren't laws, right?
These aren't like scaling laws.
These are, you're really only doing this in a very narrow kind of regime.
And then you're kind of assuming that that relationship would continue
if you did it outside of that, right?
But that could not be the case.
It could not be like that at all, right?
You could imagine a scenario where it kind of goes like this and it's curved
and then it flattens out, or maybe it's curved in the other way, right?
So this notion of extrapolating out from outside of what you found
and then calling that a law just kind of seems a little bit outrageous to me.
Okay.
Let's go back here.
All right.
So what are we looking at in the figure one in this paper?
Pareto frontier of validation loss versus flops
for auto-aggressive and mass diffusion models under data constraint settings.
Okay.
Each point represents a model trained until convergence.
We report the best validation loss achieved among all models
using less than or equal to the flops shown on the x-axis.
Okay.
So it's the same chart as this, basically.
It's amount of...
amount of training
and the loss that you get.
So lower is a smarter model, right?
So you can see here that as you train the model for longer,
you get a smarter model, right?
And the key point in this paper is that at some point, right,
these auto-aggressive models
quickly extract all the intelligence that they can,
but these diffusion models are capable of kind of going beyond that.
And it all basically comes down to this idea of the implicit data augmentation
that exists in a diffusion model.
And this is 100 million unique data, 50 million unique data, right?
Okay.
Let's go here into the end of the introduction section where they list...
So another high-value section of papers is at the very end of the introduction.
Usually the beginning of the introduction section is kind of not super useful.
It's kind of pretty generic.
But at the end of the introduction section,
usually they give you basically a little summary of the paper,
or at least the best parts of the paper.
So the abstract is kind of like the full summary of the paper in one paragraph.
And then at the end of the introduction,
you get kind of the more nuanced kind of item-by-item summary of the paper.
Okay.
Diffusion models surpass auto-aggressive models given sufficient compute.
Okay.
Beyond a critical compute threshold,
diffusion models continue improving and ultimately achieve better performance.
Diffusion models benefit far more from repeated data.
Diffusion models have a much higher effective epoch count, right?
Which is how many times you're repeating.
And this is like almost 100x more, right?
So like the RD for auto-aggressive models,
which is this kind of metric that they come up with
that is basically how much intelligence can you squeeze out of each individual data point, right?
For an auto-aggressive model, it's about 15.
For diffusion models, it's about 500.
Critical compute point follows a power law with data set size.
We found that the amount of compute required for diffusion models to outperform auto-aggressive models scales as a power law with a number of unique tokens.
This yields a closed form expression that predicts when diffusion models become the favoring modeling choice for any data set size.
And kind of like I was saying before, I just don't like these power laws or these expression models that predict when diffusion...
Like, these are just models, guys.
Like, you know, it's kind of like the chinchilla.
Anytime somebody says the chinchilla optimal, it's like...
Like, okay, so you're...
This paper, this chinchilla paper, right?
These chinchilla optimal curves here are only for one specific model trained on one specific data set
within a very specific range of model sizes and a very specific range of training flops, right?
So it's like the conclusions you can draw are thus limited to that specific narrow domain, right?
And even here, right?
So they say, like, oh, look at this.
It's going to go up.
It's like, do you actually know that?
You don't actually know that, right?
You don't actually know that this dotted line continues here.
This is all extrapolation here.
It could be the case that it kind of flattens out, right?
So that's another kind of important point that I want to stress in this stream is that, like, these scaling laws, these equations where you can just plug in, like, this one here.
Oh, you plug in the size of your model and your data set, and this will tell you exactly the amount of training that you want to do.
It's like, no, it's going to give you an estimate, but, like, there is no actual, you know, this isn't, like, the speed of light or some of the more actual hard, like, laws that exist in our universe, right?
These are just kind of intuitions that people build through these kind of effectively hyperparameter sweeps.
All right.
There was one sentence here that I thought was important.
Yeah, right here.
Okay.
Say this name.
Wenningoff?
AR models can still benefit from data reuse.
Training up to four epochs on repeated data achieves performance nearly on par with training on fresh data, suggesting an effective strategy for improving data efficiency.
In contrast, computer vision has long embraced multi-epoch training along with aggressive data augmentation, such as random cropping, flipping, and color jittering, to expand effective data set size and improve generalization, particularly for discriminative tasks like classification and detection.
Okay, so what they're talking about here is that computer vision, which was one of the really kind of the big success stories of modern deep learning, right, specifically convolutional neural nets for things like classification, which is what ImageNet was, right?
People figured out that, hey, actually, you can do this thing called data set augmentation, where we only have 100 cat images and 100 dog images.
And if you train on the same 100 cat images and 100 dog images for 100 epochs, right, you're going to kind of overfit to those cat images and dog images.
But if you take those same cat images and dog images, and then every time you do an epoch, you augment them.
So basically, you might do a flip.
You might give the same cat image an upside down.
And from the point of classification, it's still a cat image.
So even if you flip it upside down, it's still a cat image.
Even if you blur it, it's still a cat image.
Even if you increase the contrast or change the exposure, it's still a cat image, right?
So these are all transformations of the input data that still have the same label, right?
So when people were doing these supervised learning for training these classification models, this type of data augmentation in images worked very, very well, right?
So there's a rich history of using dataset augmentation to allow for really high epoch training in computer vision, right?
But you can't really do that in text, right?
So there isn't the same notion of kind of invariant transformations that you can apply to text that give you the ability to reuse the same data points over and over and over again, right?
So you can come up with them, right?
There's some ways to come up with kind of data augmentation strategies that work for text.
But really, the text world, right, which is where LLMs come from, doesn't really have a rich history of kind of dataset augmentation like computer vision, right?
Because computer vision has these very obvious kind of ways of augmenting.
All right, you guys are popping off in the chat.
Let me see if there's any questions here.
Do you think this may be a reason for OpenAI to not silently abandon the GPT 4.5 model?
So OpenAI abandoned the GPT 4.5 model, one, because it was expensive, right?
So like if nobody really uses it that much and you have a cloud instance and it has to be a big cloud instance that's hosting this huge ass 4.5 model and nobody's using it, you're just burning money, right?
So part of the reason that OpenAI stopped hosting this 4.5 model is that they just weren't making money on it, right?
Not enough people were using it.
They were burning a bunch of money on inference and they were like, hey, like this isn't really making us any money.
Nobody cares.
Let's get rid of it.
But there's also another reason why they got rid of the GPT 4.5 project.
And this is a little bit more of a conspiracy theory.
But we do know that a lot of these Chinese AI labs, like for example, DeepSeek, they distill from the American frontier labs, right?
So basically, you can shortcut your way to a better model without paying the compute price by distilling from somebody else who's paid the compute price, right?
So part of why DeepSeek succeeded is because Gemini and GPT or Gemini, OpenAI, all these big companies paid for these huge pre-training runs.
And then DeepSeek just was able to get basically use those pre-trained foundation models to generate data sets.
And then they train on those data sets.
And that's what distillation is.
It's when you use a teacher model to basically create a data set for a student model.
And that allows you to kind of shortcut.
You'll get to the – the student will be able to get to the point of the teacher in less time, right?
So less compute.
And I think part of the reason that OpenAI got rid of the GPT 4.5 is because they paid a huge amount of money for that model, right?
You've got to think about where did GPT 4.5 come from.
GPT 4.5 was basically an extrapolation of the scaling laws that went from GPT 2 to GPT 3 to GPT 4, right?
So from GPT 2 to GPT 4 to GPT 4.5, people were following these scaling laws of, hey, if you just keep increasing pre-training on a bigger amount of data, you'll get a smarter model.
So the amount of compute that went into GPT 4.5 was probably significantly larger than the amount of compute that went into GPT 3.5 and GPT 2, right?
So if you can then get a bunch of answers from GPT 4.5 and fine-tune on that, you can now shortcut all of that, right?
So now OpenAI's compute budget, you're getting basically free compute out of that, right?
Because you're distilling from their model.
So I think probably what they realized at OpenAI is that like, hey, none of the human users are actually using this model much at all.
It actually seems to be a bunch of weird like power users that are basically just asking thousands and thousands of these weird questions to our GPT 4.5 model.
And then they eventually realized, oh, wait a second.
Those are fucking people distilling from our model.
That's what they're doing.
They're distilling from our model.
And we don't want to just give them free.
We just don't want to give them all this free compute that we did, right?
Like we spent all this money and all this compute to get this GPT 4.5 model.
And now all these people are just distilling from our model for free.
He's like, they're like, fuck that.
We're not going to let them do that.
So we're going to actually close out this model so that we don't just give them all that free kind of next level pre-training that we did.
So that's my conspiracy theory is that they realized that everyone was kind of starting to distill from them and that there was more people distilling than there was even people using it.
And then they were like, okay, so not only are we losing money on the inference here, but we're also basically giving away the answer to our competitors.
So let's just get rid of it, right?
And I think this is actually where the kind of game theoretic dynamics around distillation kind of leads to a future where frontier models never actually expose the latest model to you, right?
So you're never actually talking to the most powerful model that they have.
You're always talking to something that's been distilled from or something that kind of is a little bit less than what they have.
And they're using their most powerful model to distill the models that you serve and that your users use every day, right?
So distillation and the effectiveness of distillation from a game theoretic perspective leads to a strategy where the frontier labs don't give you their most powerful model because they don't want you to distill from that.
That's my conspiracy theory.
Did you guys like that one?
Uh, open room.
Data augmentation would be cool if you pipe it through a diffusion model to generate new images as, as, as though into the mix of different lauras.
Okay.
Let's talk about diffusion models.
And, uh, again, I'm going to just show this video here because I just feel like this is just such a beautiful animation, but this is an explanation of diffusion models, right?
And you start from noise and you diffuse out the noise.
So really what the model is actually predicting is the noise, right?
The model is saying, okay, this is what the noise is.
And then it removes that, right?
So in training, you're actually going backwards and then in inference, you go the other way, right?
So it's like training and inference are almost like flipped mirrored versions of each other.
But, uh, it's high, it's difficult to understand or have an intuition about what that means in kind of a high dimensional space like images.
But in this two dimensional space, like this spiral, it's a much more intuitive visual explanation.
So imagine that each of these points is an image, right?
So this is a two dimensional space where each yellow point is a valid image.
So this might be an image of a cat.
This might be an image of a dog.
This might be an image of a trailer or a car.
And then everything that's not inside this spiral is basically garbage, right?
So like here's an image of a cat and then everything here is just a bunch of weird garbled crap that isn't actually an image that you want, right?
So what your diffusion model should do is it should be able to take an arbitrary point in the space, which is just going to look like garbled crap, like noise.
And then it should be able to bring you to a point on this curve.
And that's basically what diffusion models are doing, except they're doing it in a much higher dimensional space, right?
They're starting from some random point in this super high dimensional space, which looks like noise to us.
And then they're able to take you to the closest point that isn't noise, right?
That actually looks like a cat, right?
And this is where the width of the model matters, right?
Because the width of the model defines kind of the dimensionality of that space, which determines how much space there is to hold all these complicated concepts, right?
So if you have a very wide model versus a very narrow model, the very narrow model has a limited amount of space to put all of images in, right?
Versus the wider models have more space to put all of images in.
And the space of all possible images is a huge, very complicated space.
So, okay, let's keep going here.
All right, so you got your diffusion model.
It takes you from the noise to the image step by step.
Here's your kind of two-dimensional simple version of that.
You have a point, which is one single image.
You start from, okay, here he's just kind of talking about, okay, so this is what a diffusion model is doing, right?
It's like, in training, you start with the image and you add a bunch of noise, right?
And every time you add noise, you're kind of moving around until you finally end up here and you're just in the middle of nowhere.
It all looks like noise, right?
But because you've now done that, you now know the end point and the start point.
And when you're doing inference, what you want to do is actually the opposite.
This will be your start point and you want to get to an end point.
So you see how basically the way that diffusion model training is formalized, you basically get this flip where it's like the start point becomes the end point.
And now the end point becomes a start point, right?
All right.
So if you take a bunch of images and then add a bunch of noise, right, you get this weird distribution of points that all look like noise.
And then what a diffusion model does is it reverses time and brings you back to the image distribution, right?
So beautiful.
I love this animation.
Boom.
Boom.
Okay.
Now that you have that in your head, think about this.
Okay.
So this was what you started from, right?
If I add a different noise, I would get a bunch of different points, right?
So whenever in this paper, they talk about this idea of diffusion models have training involves repeated passes, diffusion models make better use.
Where is it?
Implicit data augmentation.
Why is there implicit data augmentation?
Because you can always basically just keep redoing this, right?
If you give different noise and you start with a different random seed, you're going to end up with a different distribution of random noise points here, right?
But you always know what the curve is.
So you can always go back to the true images.
But you can just keep adding noise and noise and noise and every single time you add noise, it's different, right?
Which means that you have effectively infinite trajectories.
You have kind of this infinite, super nice kind of data augmentation where here's a trajectory from this cat to this noise and this cat to a different noise and this cat to a different noise.
So from that one data point of here's a cat image, you have basically infinite training data of here's the cat image to this weird point over here and then here's the same cat image to this weird point over here and then here's the same cat image to this weird point over here.
So it's like the data augmentation is built in to the diffusion model.
Okay.
I thought these were cool too, right?
These animations are just so sick.
Boom.
And then here's the field, right?
So whenever people talk about flow matching, right, where in the flow matching objective, which is kind of like the evolution of diffusion models, now what the model is really kind of predicting is this field, right, which is just a function that you can evaluate at any point in space, right?
So this is a two-dimensional space.
So a two-dimensional field means that at any point here I can define this vector.
And what that vector does is it points you from this random point in the space, which is just going to look like noise, to a point in the space, which is going to actually be part of your data distribution, which is in this case images, right?
So it basically points you from noise to images.
But the tricky part of diffusion models is that this field is conditioned on time.
So it's a time-conditioned field.
So the field changes as a function of time.
So depending on where you are in this process, right, maybe time step T equals zero is right here, right?
This is time step T equals zero.
This is time step T equals one, right?
So the field that your flow matching diffusion model is giving you is time varying.
It's a different field here than it is here than it is here, right?
And actually they have a nice animation for that as well.
Here it is.
So this is the field changing over time.
So you can see initially it just kind of points you to the center.
And then once you get to the – there's a very interesting kind of point here, right?
So as soon as it goes from like 0.5 up to 0.6, right, it goes – or it goes through that 0.4.
There's this kind of almost like phase change if you want to use that terminology where suddenly it goes from pointing generically towards the center of your distribution towards pointing specifically to an area, a very small area of your distribution.
So you can see here initially, right, your field just points you to the center.
But then once you cross this weird threshold at 0.4, it points you to a more specific narrow place.
So this is the time varying field that a diffusion model is producing.
Let's keep going here.
Diffusion models yield better downstream performance.
I think we already talked about that.
Let's go a little bit into here.
I think most people are familiar with this at this point, right, where – maybe let's kind of like glaze over this a little bit.
But autoregressive models, which are kind of your basic transformer – and like I said, diffusion models also use transformers.
So transformer isn't necessarily specific to one of these two families.
But in your classic autoregressive transformer, you're predicting the next token conditioned on previous tokens.
So you're basically saying for my sequence of tokens, right, so I have some input sequence of tokens, I predict the next token, X, J, given all previous tokens.
So that's what this little parallel bar means.
It basically means I'm trying to predict the probability of this token, right, and this model, what the autoregressive transformer model does is it gives you this distribution of probability.
So it's basically saying, okay, for all the possible tokens in my vocabulary, right, all possible next words, what is the probability of the word being the?
What is the probability of the word being attention?
What is the probability of the word being mechanism, right?
And for each of those little words, I'm going to pick a probability.
So all the model is doing is it's saying this is the probability that the next word is the given this previous part of the sentence, right?
Diffusion, and in this case they're talking about mass diffusion, it's more like, hey, the entire thing, right?
So it's basically saying given this, what they call it corrupted sequence, but another way of kind of thinking about a corrupted sequence is just the noise, right?
So this is like a corrupted image.
So what is the, it's a little, there's so much nuance here that I'm trying to be precise with my language, but it's like I can't help but not be precise with my language.
But it's like given this corrupted image, what is the direction that will take you to a less corrupted image?
That's basically what the diffusion model is doing, right?
It's saying here's the full corrupted image.
Give me this specific token, right?
And in text space, you can see how right here, right?
So like it's, look at this word specifically, the word square here.
It starts off from pure noise, and you can see it starts off denominator, exponent, power, and then it ends up a square, right?
So it's basically going denominator, which is just noise.
Then the next word is exponent, which is a little bit closer to what you want, then power, and even closer, and then eventually whatever the final word was, right?
So it's like it's starting from noise and then ending up at the final place, but in text space, which is kind of crazy to think about, right?
I could just watch that animation all day.
Look at that.
So cool.
Causal attention, full self-attention for mass diffusion.
Okay, so what they're, okay, let's talk about masks for a second here.
Let's talk about attention mask.
So whenever you're doing autoregressive models, right, the one secret, so the dirty secret about the attention mechanism is that it's actually a set-to-set algorithm, which means that there's nothing about the attention mechanism that forces you to only pay attention to the words before, right?
So the attention mechanism just says every single word in this sequence, the Statue of Liberty, blah, blah, blah, and the Statue of Liberty, blah, blah, blah, can pay attention to, which is just a fancy way of, or kind of an anthropomorphic way of saying that we're taking the vector that represents the word liberty and the vector that represents the word high, and we're kind of multiplying them together and then getting this, if the words are similar, you're going to get a high agreement.
If the words are not similar, you're not going to get agreement there, right?
So that's what the attention term means there, right?
It's like words that are semantically related or have an important kind of relationship to each other are going to have this high attention, right?
But there's nothing in the attention mechanism that makes it specifically only pay attention to the words before, right?
So if you look here, there's nothing that's forcing you to only pay attention or only condition on the words before.
We enforce that by using this mask.
So we basically hard code this into autoregressive transformers where we say, okay, you can't pay attention to any of the words after the next token that you're trying to predict, right?
So why do we do that?
The reason we do that is because we need to use this.
Whenever we're doing this in inference, you don't have that there, right?
So whenever you're performing inference, you're only predicting one word at a time, which means that you can't say, okay, well, this word here, positional, can pay attention to the word fixed because you're not even there.
You're here, right?
You don't have the future.
You can only pay attention to the past, right?
Versus diffusion isn't like that at all, right?
In diffusion, you have the whole thing available, right?
The whole image is available for you to pay attention to.
It's just that it's all noise, right?
So this over here is paying attention to this over here versus if we were doing this autoregressively, you'd be kind of rastering through this left to right and you would have to predict this token right here before you know what this token is, right?
Versus in diffusion models, you start off the whole chunk is instantiated and it's just that a lot of it is noise and then you're iteratively removing that noise, right?
But you can get so much more signal out of that because think about it with this image here, right?
So whenever you get here and you're trying to basically get the token that represents the cat eye and I'm aware that what I'm trying to explain here is a very high level explanation that doesn't actually formally, it's not exactly what's going on, right?
Because for example, one thing, this is usually not done in image space, it's usually done in a latent space, right?
You're kind of with the DDIM and DDPM, there's like different, when are you adding the noise?
So there's a lot of nuance and complication here, but the way I'm explaining this is I'm trying to make it simpler so that it's easier to understand.
But understand that the simple thing that I'm explaining is not actually one-to-one mapped with reality, but when you're trying to predict this cat eye, right, you can pay attention to this other cat eye, right?
So diffusion allows you to say, okay, well, both of these eyes are kind of coming out of the noise at the same time, which means that I can make sure that both of these eyes are the same color, right?
Versus in an autoregressive kind of way, if you were kind of like rastering, you just have to pick this first eye color, and then you get the second eye color, right?
Because the second eye kind of comes after the first eye left to right, right?
So it's like you're kind of like forcing yourself into a specific area, and then you're forced to extrapolate the tokens based on the previous tokens.
Versus in diffusion, you have kind of more freedom to kind of diffuse everything together, right, if that makes any sense.
Diffusion is more general, less bias built into the system.
Yeah, less bias built into the system.
And that's, so if diffusion models are cooler, if diffusion models work better and they have less bias, like why the fuck aren't we using diffusion models, right?
Like you could pose that question.
Well, the reason is that autoregressive models and this kind of left to right kind of going down the elements of the sequence one by one, it's a bias, right?
It's a bias.
It's something that you kind of baked into the model that didn't have to be there.
But it just so happens that what we're trying to model is language and kind of a reasoning chain that happens in language, right?
So language, which is this kind of like symbolic kind of manipulation, has this implicit kind of like sequential time bias in it, right?
And that comes from the fact that our four-dimensional reality, right, which is three dimensions of time and one dimension of three dimensions of space and one dimension of time has time in it, right?
So like the bias of the fact that we have time means that language itself that developed from humans that are existing in space time has time kind of baked into it, which means that all logical reasoning has this kind of implicit kind of, if I do X, then X equals this and then this equals this and then this equals this and then this equals this, right?
So there's kind of this implicit kind of like next, you need to get to here, to then get to here, to then get to here, to then get to here, to then get to here, right?
That bias is built into our reality, which means it's built into our language, which means that assuming it, whenever you're trying to model that language, which is what language models are doing, is actually a good implicit bias, right?
So autoregressive is the standard because it just so happens that it's a very good inductive bias for language, which is why we were able to get language models using autoregressive models.
But maybe that's starting to flip, right?
What this paper's starting to kind of suggest is that maybe diffusion models are just better and maybe we can actually start using diffusion models to generate text, right?
And why do diffusion models work well in images?
Well, there isn't really a notion.
Why do you need the notion of time and kind of sequential, this sequential bias when generating an image, right?
That kind of seems stupid, right?
Like this whole vision transformer, kind of I'm going to take the image and turn it into like this sequence of visual tokens that go from left to right, top to down.
Like that's completely arbitrary, right?
Like why left to right, top to down, right?
Why are we turning this into a sequence?
It's better off to just not have that.
And that's part of it too.
I think a huge part of it is the kind of implicit data augmentation, which we've talked about.
But I think a lot of it is that space doesn't have this kind of like sequence bias.
So images are much closer to kind of space than they are to time.
So that's why diffusion works really well for things like images rather than language, whereas language works better with autoregressive because it's more time-like.
Does any of that make sense or am I just kind of saying nonsense here, guys?
Is it data efficient to train model using diffusion since the main advantage of AR is that it's very data efficient?
So it comes down to like what do you mean by efficient, right?
Because there's the efficiency from the data set perspective or data set reuse, which is what this paper is arguing for, right?
This paper is basically saying that given the same data set, you can repeatedly train on it for many, many epochs better on a diffusion model than an autoregressive model.
And I think that's, for example, what this chart here is showing.
So in this chart here, let's go to this one here.
Where is it?
I thought I, did I pass it already?
Hmm, I thought I, oh, here it is, okay.
There it is.
Okay.
So this is a contour plot, which basically shows you the loss.
So here on the x-axis you have epochs.
So this means you trained on the same data set for longer.
So this means you trained on the same data set 1,000 times.
And this means you trained on the same data set once, right?
And then down here, this is parameters.
So this is the very small model.
And then this is a very big model.
So up here you have a very small model that was trained on the same data set thousands of times.
And up here you have a very big model that was trained on the data set once, right?
And these contours are a loss, right?
So you can basically see that here the loss is high and then it gets lower and lower and lower and lower and lower.
So these are your smartest models.
And then as soon as you start going above this area here, the loss goes back up, right?
So now the model is actually getting stupider and stupider and stupider.
So if you train a big ass model on the same data thousands and thousands of times, it's just, you know, you can see how you go this way.
You start training more and more on the same data.
It gets dumber, right?
Versus diffusion models don't have that property at all, right?
So the same plot here for diffusion models, you can see the loss goes down, down, down, down, down, and just seemingly keeps going down, right?
You can just basically, if you keep making the model bigger and you keep increasing the epochs, that kind of implicit data set augmentation allows you to just kind of infinitely keep training and you can just keep decreasing the loss.
Okay, but back to Ed's question here, right, which is efficiency, right?
So this is efficiency at the notion, a data set efficiency at the notion of like epochs.
Like how much juice can you get per epoch?
But there's also the notion of efficiency from a compute paradigm, right?
So whenever you're training these models, there's actually some code that defines this math, right?
So like this math, this loss objective, right, is actually some code that is being executed on some amount of GPUs, right?
And part of why we use autoregressive language models is because mapping the inference and training steps of an autoregressive pipeline into a GPU is actually very efficient.
There's a very efficient way to express that computation on a medium of a bunch of GPUs connected together via NVLink, right?
So that might not be the case for diffusion, right?
So like if you try to train a diffusion model in parallel over a thousand GPUs with a different objective, you're not going to get the same amount of computational efficiency, right?
And there's a lot of nuance there.
There's all kinds of ways that people have made diffusion models faster, but it's not equivalent, right?
So like when you're in math world and you're just looking at like this versus this, it's very easy to say, oh, well, this is a better objective than this.
So therefore, we should always use this.
But then once you get into the real world, you know, you're going to talk to someone who actually writes the code and they're going to say, okay, well, I can write code that does this.
And it's going to be 10 times easier to do that in parallel than the code that does this, right?
So the reality of which models people use and which architectures people use, it's not coming top down from the math, right?
It's not like here's what the theory says is the best possible thing and then we train using that.
It's also kind of bottom up from the actual people that are writing the GPU kernels, right?
Where it's like, okay, well, there's constraints from both sides.
Because of BERT style encoder only masks part of the sentence means we only use 5% of our data to learn.
So you can, masking is a very high level general concept.
You can have masking in an autoregressive objective as well, right?
You can imagine a situation where rather than conditioning and predicting the next, predicting the next token condition on all previous tokens, you can mask out some of these tokens, right?
So you could take some of these tokens and zero them out, right?
And that would be a type of data augmentation that you could do with an autoregressive learning process, right?
Where you're like, okay, well, rather than giving it all the previous tokens, I'm going to give it some of the previous tokens and I'm going to zero out some of the, it's not actually zero out, but maybe you put noise in there, right?
And now you have a form of data augmentation that works for an autoregressive objective.
But, yeah, so masking is a very high level concept.
You can apply it to pretty much anything.
And there's lots of different types of masking.
How's it going?
A little KM.
In some ways, but when you write, you as well delete and edit.
So maybe diffusion is better as an idea, refining things as opposed to writing without the delete key.
Yeah, that's another good way of kind of explaining it, too, where it's like, when you're doing everything kind of in this sequence of like, predict the next token, then the next token, then the next token, then it's like, all previous tokens that you've kind of generated are kind of fixed.
Now, when you're predicting this word, unless, it's like, all previous ones of these are fixed.
It's like, you can't change the past anymore, which is, it all comes down to space and time.
It's like, it's kind of trippy to think about it, but it's like, time has that as well, right?
There's that, time has this kind of like, constant movement forward and you can't change the past.
Versus, and there's a constraining element to that, right?
So like, and actually, here's an even trippier thought, is that what, I don't know if people have done this, and I'm sure people might be able to point me to some papers where they've seen this, but can we do diffusion in the text space for reasoning chains, right?
Which is kind of what we're seeing here, right?
Where it's like, rather than creating a reasoning chain, right?
Which is just a long sequence of tokens that get to a final answer, right?
Which is how all of these reasoning models work.
Rather than kind of sequentially getting to that part, can you kind of diffuse it to it, right?
Can you diffuse reasoning chains, right?
Which is kind of like, you can solve that as a human, right?
So like, if you have a math problem as a human, right?
Like, you can basically do it left to right, kind of full autoregressive, where you're like, okay, well, I'm going to do step one, then step two, then step three, then step four.
But you can also kind of do it in a more kind of diffusion kind of way, right?
Where you're saying, okay, well, I kind of know that I'm going to need this step, six, and I kind of know that I need this step one, and then therefore, I'm going to write that one first, then step six.
And then now that I have step one and six, I can kind of get a little bit better answer in four.
Now that I know this four, I can kind of change the six a little bit.
So like, you can kind of diffuse out a reasoning chain just as a human, right?
Where you don't have this implicit kind of left to right autoregressive bias.
So that's something crazy to think about.
And then if we take this paper at face value and we say, okay, well, diffusion models are capable of learning from a smaller amount of data because they can use more epochs.
Then we go back to this idea of distillation where, hey, if we can distill from very smart models, then maybe we can distill from small data sets, right?
So, hey, maybe if OpenAI is going to give you GPT 4.5 via API, maybe you don't need to collect a data set of 10,000 reasoning traces from GPT 4.5 and know the exact token distribution.
So maybe you can just take a small data set from a very smart model and distill using a kind of a diffusion objective, a text-based diffusion model that is capable of getting there faster, right?
So, like, there's, I'm kind of having a hard time explaining this, but it feels like there's a, there's somewhere, there's a Venn diagram somewhere in the center there where you combine this idea of diffusion text models and distillation to get, to kind of like, they combine in a good way.
Does that make sense?
LLMs spend too much time connecting words instead of chunks, envisioning the solution.
You can think in a way that if you hide the answer of a math problem, you can solve the problem, but if you hide the question, you may solve the inverse problem that fits the answer.
That's a nice little conclusion there, open room.
Josh Phillips, feels like we're just going to loop back into thermodynamics and entropy theory with all this, the further we go.
Yeah, there is a, there is a very strong parallel there, right, between entropy and time and kind of reversing entropy and reversing time, you know?
It's, it's, it's very similar ideas, right?
Kind of like how the concepts of entropy and thermodynamics and the concepts of entropy and information theory, right?
There's like, there's a reason that whenever they came up with entropy for information theory, they kind of used that same word entropy, right?
They could have come up with a different word, but they, they were like, hey, wait, this is a lot like thermodynamics.
So, I think it's no coincidence that the same kind of entropy concept exists in all these different scientific fields because there's like a kind of like a deeper, a deeper kind of element to that, you know?
And then you start getting into the even crazier ideas, like our whole reality is probably a simulation in the sense that what quantum mechanics and kind of the weirdness of quantum mechanics is telling you, right?
Is that really at the, at the base layer of our reality is not this kind of materialist understanding of like, hey, there's these things and then they're interacting.
It's like, no, actually there's information and the information is, is interacting with more information.
So, it's like the lowest layer of our reality is information and then computation on that information, which is basically, I know I kind of circle jerk this guy all the time, but Stephen Wolfram, that's his kind of core idea where it's like really reality is kind of information and then computation that is happening on that information.
And experiments like the, and experiments like the double slit experiment and all the kind of like variants of that experiment basically confirm that that's actually what's happening.
It's like information is at the bottom, right?
So, what is the second law of thermodynamics in information space?
Where does that get you, right?
If you combine those two ideas.
I like this Google account.
Hierarchical concept diffusion mapping.
That's a badass paper name.
You should write that.
You can think in a way, if you hide the answer, but math, okay, I think I already answered that one.
Afik Farouk Naneh.
Also remember why we do predict tokens by token is that because we had no option predicting multiple tokens or features before diffusion and GAN era.
Yeah, that kind of goes to kind of what I was getting before where it's like the theory doesn't drive the implementation necessarily, right?
Where it's not like we come up with these ideas from a theory side and then our implementations are based entirely from the theory.
It's like the implementations and what people use in practice also comes from the other side as well.
It comes from the design of hardware like the GPUs, right?
So like we use specific architectures because they happen to work well with these weird blobs of silicon that Jensen Huang and his team designed to render pixels in video games in the 90s, right?
So like it's bottom up and it's also top down, right?
So those two things are intention.
Okay.
Let's go a little bit deeper in here.
Is there anything else we can look at this?
We fit scaling laws tailored to data constraint regimes.
We looked at that figure.
Let's look at this figure as well.
This one here.
How about this one?
Predicted validation laws for AR models left and diffusion models right under compute optimal settings extrapolated to larger compute budgets.
And this is the part that kind of skews me out where it's like I don't know if you can necessarily do that, right?
I think it's kind of against the against general good scientific practice to extrapolate beyond your data, right?
That's why I like this paper, the fact that they admit that, hey, this is interpolation and this is extrapolation versus a lot of these scaling law papers.
They're just like, no, it all is like this is fine, you know, like we're going to extrapolate and it's totally fine to extrapolate.
It's like, yeah, maybe not, you know, dotted lines indicate the hypothetical case where repeated data is valuable as new data for AR.
This holds up to about four epochs for diffusion up to 100 epochs, which is that's a huge it's like to like orders of magnitude more.
All right, but okay, you got one epoch, two epochs, four epochs, eight epochs up to 64 epochs.
This is AR validation loss over training likes, right?
So validation loss means the loss of your model on a holdout data set, right?
So it's different from the data that you're pushing in gradients, right?
So it goes back to this right here, right?
Here they call it test and training, but you can also call it training and validation, right?
Here's one where it's called validation, but basically the validation is supposed to be different from your training so that whenever you evaluate on your validation data, you're actually supposed to get a more realistic score versus if you're constantly, if you're just testing on your training data, you're just, you're going to be diluting yourself, right?
You're going to think you're doing better and better and better, but really you're just overfitting harder and harder and harder and harder, which goes back to our emod mustock kind of shit.
Talk where he was like, oh, our 8B parameter is performing better than GPT-4 on medical data.
It's like, no, dude, what you're doing is you're just evaluating on the, on the training data set.
It's like you just overfit this 8B model to medical questions.
And now anytime you ask it medical questions is extremely overfit, but go ahead and ask it what one plus one equals.
It might not even be able to get that.
Emad, he mad.
He mad.
He mad.
He mad.
Uh, we could combine diffusion with autoregressive models.
I don't know what the combination would look like.
Autoregressive models for adding noise, finding the framework structure, using diffusion models to then envision the underlying principle truth of the system.
We're getting pretty, pretty next level here.
Okay.
Here's the diffusion models.
You can see how this flattens out.
So as you get more and more epochs, you start to get to this point of diminishing returns.
And eventually this would turn upwards, right?
So here they actually, you see how here they make it go flat?
It probably would go up, right?
It would probably look more like this, where it would start going up at the end.
So even in this paper, the extrapolation is likely incorrect, right?
If you start doing, uh, 80 epochs, a hundred epochs, 1000 epochs, it would start to curve upwards and the model would actually get stupider.
Versus with these diffusion models, you don't get as much of that because you have this implicit, uh, data set augmentation.
And then here's the kind of, uh, chinchilla scaling law, kind of diffusion scale or whatever they call it, distillation scaling law.
Here's the diffusion scaling law, which is basically this, this notion of a critical compute frontier, the compute required for diffusion to match autoregressive at a given token count.
So you plug in the size of your model, the amount of tokens that you're going to train on, and it'll give you this nice little plot here where you can say, okay, if I have 10 to the second data points and I want to train, uh, 10 to the 22nd compute flops, I should probably use a diffusion model.
If I, uh, have a ton of unique data and I don't want to train on it a million times, then I should probably use an autoregressive model.
So this is kind of the, the, the, what they call the critical compute frontiers right here.
Right.
And that determines your choice.
And like I said, empirically, this actually kind of matches what we see, right?
Where if you look at images, right?
Images are actually images is kind of like an in-between space because there's a huge amount of image data, but you're kind of compute constrained because images are kind of more compute heavy.
So therefore you're kind of like here, right?
I guess maybe it doesn't fit, but at least it fits for robotics for robots.
You definitely have a very small amount of data, right?
Like think about like, uh, for my project, my tap bot, it's like, we're talking on like the order of like a thousand data points, right?
It's like, I can collect 10 trajectories.
I can maybe collect a hundred trajectories, but I'm not going to be able to collect a thousand trajectories of my robot, right?
So I'm, I'm, I'm on this side of the unique data curve.
So if that's the case, then a diffusion model would work much better because I have a very small data set, which is exactly what we see in robotics, right?
That's why everybody's using diffusion models because your data sets are very small versus text.
It's the other way around, right?
You have basically infinite text.
There's so much text on the internet.
You can just keep generating reasoning traces.
So there's a huge amount of data.
So maybe that's why we see autoregressive as the dominant paradigm in text.
Uh, isn't that the reason Zach is collecting infinity stones using his money?
What?
Oh, you're talking about Zuck.
Zuck purchasing a bunch of researchers.
I think it's just cause he has a bunch of money, you know, he's not afraid to use it.
It's kind of, it's kind of like, you know, I feel like the 2020s kind of the one, if you want to go into that kind of intrapreneurial kind of startup space, right?
Like, we do see an advantage towards having kind of this super opinionated kind of like authoritarian style or people call it like kind of founder mode, right?
Where like, think about what Elon did where he was like, okay, I'm just going to fucking purchase this warehouse in like wherever it is, Tennessee.
And I'm going to put a hundred thousand GPUs in there and I'm going to rent out all these like, uh, trucks that basically burn natural gas to power the GP.
Like you were, you would have never gotten that type of, you couldn't do that if you were kind of ruling by committee like Google does, right?
Where Google kind of, they're, they don't have a singular founder who has that kind of authoritative power.
So they're kind of forced to do things differently versus meta and XAI because they have one person who can basically veto anybody else.
Uh, Elon in the case of XAI and Zuck in the case of meta, they can do these kinds of crazy strategies.
Like, Hey, I'm just going to literally get the top a hundred researchers and just pay them a billion dollars.
And now I have all the top researchers, right?
But now the question is, okay, what's more important GPUs or researchers, right?
Because Elon was like, okay, I don't give a shit about the researchers.
I'm going to pay a ton of money to get all the GPUs, right?
Zuck is kind of doing the opposite approach where, Hey, I already have kind of enough GPUs.
I'm just going to pay a shit ton of money to get the researchers.
And I actually kind of, I think Elon has a slightly better approach there.
I think that you can take kind of average researchers and give them a huge GPU budget and they're probably going to end up in a better place than if you take the best researchers and give them kind of a medium GPU budget.
I think if Zuckerberg would have taken all those pay packages and all those, uh, all those salaries and bonuses that he gave out to these researchers and instead just hired a bunch of generic kind of like intel.
There's like millions of these kids, right?
That are good at math, good at programming, you know, a lot of them are desperate for jobs too, because a lot of them are here.
They went to university in the U S but they need to maintain employment because maybe they're from China or India.
Right.
And they need to basically get a job here.
You know, you could get a thousand of these people that are incredibly smart for basically dirt cheap because they're in a very shitty situation where they need to get a job in order to stay in the U S or else they basically go back to their home country.
So, yeah, if I was in Zuckerberg's position, I would have just spent all that money on making more GPU data centers and then just hired a bunch of just generically smart people and then gotten them to work on it, which is kind of what Elon did.
Right.
I don't think he like specifically poached super high level people.
Maybe you poach a couple high level people just so you kind of have someone leading the engineering, but you don't need like a lot of them.
Right. And, and meta keeps making this mistake again and again, right.
Or it's like, you know, for example, what did they pay for Yann LeCun?
Right. What did they pay for these like celebrity researchers to come there?
It's like, did, is that really, did that really lead them to a better outcome?
I don't know.
If your goal is to make AGI on RTX 5090s, how would you go about doing that?
That's kind of what the, I've talked about these guys before, but control T prime intellect.
That's what these guys are doing, right?
These guys are basically, they took that DiLoco paper, right?
That Google paper where they basically gave you a basic algorithm for distributed training across compute that is spread out geographically.
And they basically built the entire startup around that.
So they're like, okay, we're going to create a platform to basically train on the compute that is available out there.
But there's a critical flaw in this, right?
And the critical flaw is that in 2022, this was a very good idea, right?
Because here's the NVIDIA data center compute GPUs, data center compute GPU chart.
Yeah.
Look at this.
Okay.
So why, why is prime intellect secretly fucked?
They're secretly fucked because of this chart right here, right?
Where in 2021, right?
The amount of GPUs in data centers and the amount of GPUs that people had for gaming was a little bit more even, right?
So if you want to think about the total amount of compute that is out there in people's GPUs versus the total amount of compute that is in data centers is comparable, right?
So actually in that situation, you know, 100,000 5090s is somewhat comparable to what you can get in a single data center.
But that's going to be less and less the case, right?
Every year that goes by, NVIDIA doesn't give a shit about these gaming GPUs anymore.
So, you know, every single year that goes by, the relative size of the compute available in a decentralized world versus the compute available in these kind of centralized data centers is going to keep diverging, right?
So at some point, the amount of compute available in a single Google data center is like 100 times bigger than the amount of compute that is spread out over the whole world, right?
So that's the whole point is that this only works in the current situation where we have this like weird lag where everybody has these GPUs that were made for gaming and there's a bunch of them.
So you can actually, if you can figure out how to train on all of them, then you can actually train something that's pretty good.
But every year, the relative size of that compute versus the relative size of just the amount of compute in a data center gets worse and worse and worse and worse.
And at some point, it's like there's no amount of fancy algorithms or interesting things that you can do with the compute mesh available in the world that even comes close to one giant like GB200 data center where everything is connected, right?
So that's kind of the problem with their approach is that decentralized doesn't work if NVIDIA doesn't care about consumer GPUs anymore.
Okay.
Damn, we got distracted.
I keep getting distracted, but I feel like people are into the distractions here.
Little cam, appreciate the explanation.
Yeah, I mean, it gets, it gets more nuanced, right?
Because, right, this strategy could still work if, if there, if you kind of have this kind of like spot instance.
So like in AWS, right, there's this notion of kind of like a spot instance, right?
Where like when you use AWS, you can say, okay, I want to rent this machine and it's, it's my machine.
It's always going to be on, nobody else can use it and I'm paying for every single hour that it's on, right?
But there's this notion of spot instances where it's like, hey, I don't want to have my own machine.
And sometimes you as AWS, like someone's renting out this machine in this chunk of time and then somebody else is renting it in this chunk of time.
And in between those, there's a little two hours where no one's using it and I can use it in those two hours, right?
So there is a possible future where in all these giant data centers, there's enough kind of spot compute, right?
Where there's enough kind of like in and out, kind of like weird little chunks of time where people aren't using this and then people aren't using the one over here and then people aren't using the one over here.
The open AI employees go on vacation.
So for like a week, the, the big training cluster isn't being used.
So there is a potential future where the, the amount of compute that is available in these kind of like little spot instances here and there combined with the amount of compute that's available in kind of gaming GPUs is enough that it's comparable to a big foundation lab data center.
But I just don't, I just don't feel like that's going to happen.
I feel like it's much more likely that they're never going to give you that spot can be right.
Like XAI is going to build this giant ass data center and they're not going to let anybody else use it.
And at some point that, that giant ass data center is going to have more compute than everybody's 50, 90s combined 10 times over, you know?
All right.
Discussion.
Why do diffusion models perform auto outperform autoregressive models in low data regimes?
We hypothesize that the key advantage stems from the use of random masking and diffusion models, which serves as a form of data augmentation, right?
This kind of idea of noise, which we keep talking about, right?
Where you start off here and you're trying to predict the, the, the, basically you're like whenever you're going here, right?
It's like you're predicting each individual token based on all the other tokens, but the other tokens are noised, right?
So like, it's like, there's this, you're almost trying to predict this one and you don't have the full picture.
And that is a type of data augmentation because every single time you do that, it's random, right?
So like you're trying to predict this token here and, and every time you create an example to train on, you're using a random number to, to seed the noise.
So the noise is always different every time.
So it's like, there's a data augmentation kind of built in to diffusion models.
Unlike AR, which are trained on a single fixed left to right, mass diffusion models are exposed to a wide variety of token prediction tasks and orderings during training.
This broader distribution over prediction tasks encourages better generalization and is more effective.
Analogous to image-based tasks where techniques like random cropping or color jittering boost generalization.
By learning to denoise across many corruptions, the model extracts richer signal per example over time resulting in improved data efficiency.
I feel like the, we already talked about this, but to me, it's like, there's this, there's this red light just flashing in my head that is saying, okay, there is some idea that exists at the combination of diffusion models in text space and distillation of reasoning models.
So reasoning models with very long reasoning chains, distilling from those using a text-based diffusion model.
There's some, there's something there.
I don't, I don't know how to describe it, but there's just something there that I feel like could lead to magic.
For practitioners, our takeaway is simple.
If you are compute constrained, use autoregressive models.
If you are data constrained, use diffusion models.
XAI will not let their employee leave the office after they build the cluster.
Maybe.
I don't, I just, I don't know why, like, for example, the, the, the whole like hundred million dollar researchers, it's like.
Like, where does this all go?
So this is kind of like a crazy idea that I had where, you know, you think about our society, right?
And we're already kind of starting to, to diverge and split, right?
There's basically, as, as AI gets better and better and better, right?
You're, you're, there's a huge difference between like this one human who is extremely more productive with AI and the human who doesn't use AI and, and is less productive, right?
And over time, that's going to get more and more extreme, right?
I think I might've, let me talk about this, right?
So where does that lead?
Over time, the companies will not look like, uh, hey, here's a 10,000 engineers, right?
We're already starting to see, they just fire more and more people.
They fire people, right?
So we already see the engineering teams get smaller and smaller and smaller.
And now we're seeing almost these like super teams where you basically have these very small engineering teams where everybody's getting paid a ton of money.
But those, each of those individual engineers, they're kind of, you know, they're, they're living in a weird world, right?
Where they basically have to spend all day inside this computational world.
Maybe they start, they're doing Adderall all the time, you know, they're taking supplements because they're just constantly working and constantly working.
I think that's going to keep getting more and more and more extreme to the point where maybe eventually the company XAI basically just has like 10 employees.
It's like Elon Musk and then like nine engineers who are basically on IV drip Adderall sitting there.
And they have like a bunch of AIs all hooked up into their neural links, right?
And they're, what are we even talking about there?
At that point, we're almost talking about these Dune navigators, right?
Where in the sci-fi world of Dune, it's like you have basically these people that live in these pods that are basically, they're not even human anymore, right?
They like turn into these like slugs.
But what if that happens to humanity as well, right?
Where like the engineers in these companies kind of, they don't even, they're not even human anymore, right?
They have like 10 different neural links in them.
They're on these like weird like drug concoctions that allow them to just permanently work and 95% of humanity is just kind of normies who are living around, like living a very human life, eating food, you know, like going to the park, you know, running around.
And then 99% of the work is being done by 1% of the humanity that has basically thrown away their humanity and become these weird kind of like beasts that basically just sit there and have extremely high level math and programming knowledge and are kind of augmented by this like exocortex of AI agents that basically, I don't know, kind of a crazy concept, but we're already starting to see it, right?
And I think the Zuckerberg like $100 million researchers is like the beginning of that, right?
Where it's like, do you really want to be that $100 million researcher?
I don't know if you do, right?
Because if someone's paying you a 100 mil salary, like you're going to be working 997.
You're working every single day of the week, right?
You're grinding.
And at some point, if you spend all day, every day coding and you do that for years and years and years and are you even human anymore, right?
Like when's the last time you touched grass?
You know?
Yeah, like the XAI, like they're like, oh, I basically work all day.
And I'm like, okay, at some point you're like losing your humanity, right?
So you're starting to become this creature, right?
And then think about 10 years from now when you start having neural links, right?
You're not going to get paid $100 million if you don't have at least a neural link that allows you to program significantly faster, right?
And I think that gets even more intense, right?
Eventually, you just have a researcher that's getting paid a billion dollars, but they're not even human.
Like they're not even using that billion dollars for anything.
Like you don't even understand what they're thinking about anymore.
They're becoming this kind of creature, you know?
Humanity splits.
Humanity splits.
All right.
I'm getting very wild here.
Dude, this last name.
How do you say this?
Nikolas Möninghoff?
Möninghoff?
i'm trying to say i have no idea what i'm saying though is this german is this dutch what the fuck
is this okay uh is there anything else interesting here
28 june 2025 scaling data constrained language models from hugging face harvard
extrapolating this trend suggests that training data set size may soon be limited by the amount
of text data available on the internet we investigate scaling language models and data
constrained regimes find that with constrained data for a fixed compute training up to four epochs
yields negligible changes to the loss so you can train four times on the internet
but again this also like kind of there's an implicit assumption here that you're not doing
data augmentation because if you had some form of data augmentation which i know is hard to do
in the text world right because you don't have the same kind of invariant transforms that you have in
the image world but if you had some form of data augmentation i'm sure you could crank this up to
more oh this was a random paper that i just threw in here so i i saw this this morning so i haven't
even like scrolled through this but i thought it might be interesting while ai systems demonstrate
exponentially improving capabilities the pace of ai research itself remains linearly bounded by human
cognitive capacity creating an increasingly severe development bottleneck right if we're bounded by human
capacity isn't that an argument for creating a cast of humans that are post-human right like if you're
limited by your ai researchers and their cognitive capabilities like why not take those ai researchers and
start you know like turning them not human so that they can be faster they can work 24 hours a day they
don't need to sleep anymore they're on some weird like glp1 uh like adderall that hasn't been discovered
yet that basically removes the need for sleep they don't feel pleasure anymore and they don't feel pain
anymore but hey who cares they're uh they're increasing your uh ai research speed
uh the first demonstration of artificial intelligence for ai research is the critical domain of neural
architecture discovery okay so this is neural architecture search so this is where you
you search to try to find the best architecture right and what they're talking about here is that
most of this neural architecture search is currently fundamentally limited to exploring human defined
spaces so so it's kind of more like a hyper parameter sweep kind of right you're just saying okay
uh this hyper parameter yeah here's four possible values for it and this hyper parameter here's uh it's
within this range and within this range and within this range right so you're like defining this space
that is constraining your search process and then your search process you can run it in this space and
it'll find the best architecture within that space but ideally you don't want to constrain that space
so right so for example you could do neural architecture space in the space of diffusion models but then there
might be some other type of architecture that isn't a diffusion model that you didn't explore and this
is kind of where uh if you guys listen to the recent podcast with uh demis and lex he talks about this
where uh demis he's very excited in this combination of kind of using uh language models to basically
more efficiently search through a large space so part of the problem with search in these human defined
spaces which is how neural architecture search has usually worked right now is that uh this if you if you
loosen the constraints of the space it becomes too big of a space and then search is too slow
because it's just too too many possible combinations but if you use a language model to search to basically
say okay well here's this giant search space and then the language model will will basically more
efficiently search through that space because it's it can get rid of kind of the dead branches right it's
it's kind of like finding little it's it's it's more efficiently searching rather than kind of like a brute
force search uh asi arch can conduct end-to-end scientific research in challenging domains
autonomously hypothesizing architectural concepts implementing them and training them empirically so
this is kind of like the directed search if you almost want to say think of it like that
20 000 gpu hours like alpha goes move 37 that revealed unexpected strategic insights or ai discoveries
ai discovered architectures demonstrate emergent design principles that systematically surpass human
design baselines and illuminate previously unknown pathways for architectural innovation
crucially we established the first empirical scaling law for scientific discovery itself which
probably also garbage you know probably also not a law i like my pet peeve is people using
the word law to describe something that is just very constrained
emergence of mentats
uh why is data augmentation hard in the text world just build question answer question
rewrite text yeah i'm not saying that there isn't ways of doing data set augmentation in the text world
there's lots of different ways of doing data augmentation for text and for autoregressive models they just
don't they're not as simple and as efficient as uh the kind of like noise-based augmentation that you
get for free out of diffusion models
uh maybe to pay 100 million for the phd researchers is to rise the stock value of stock owners yeah a lot
of that you're you're correct open room a lot a lot of the giant like public salaries is more kind of
like uh it's more kind of like illusion crafting you know it's more like creating this facade so that
because a lot of what you're doing with these giant companies is not necessarily like
actually trying to do something it's just like creating the illusion that you're there's progress and that
there's growth so it's more like you know that's where hiring like the jan lacun is like actually a great
idea because it's like you don't actually care that he's going to do something produce work and value for you
what you really care about is that he goes out there and he does podcasts and then he associates your brand of the
meta ai brand with kind of intelligent people so a lot of these research researchers getting paid these huge salaries is more
just to associate the researchers and their brand with meta ai so that the stock price keeps going rather than actually what is the most
efficient way of exploring the search space of possible language models to create state-of-the-art
language models like i said if you're actually more interested in that than maybe just like a thousand
you know like smart people that you pay absolutely nothing because there's a lot of smart people in the world that
are desperate for work you know that would be a more efficient way of searching through the space of
models than just getting one human who's like a little bit more uh a little bit uh fancier and has
a little bit better credentials but it's like i don't know i think this anti-bitter lesson you can also apply
the bitter lesson to human organizations deep seek seems to take this approach yeah i mean deep seek does do
something i think they are kind of pretty high level talent right because it's like basically they
they get to pick from the cream of the crop of like the chinese academic institutions so like
they're not they're not just using randos random people they're using extremely it's very competitive
to get into deep seek from what i understand you know sook wants to explore the search space of
dunking on altman really what we need is a fight we need sam altman zuckerberg and uh
and elon all to get into a cage and fight each other who else would we put in that cage
put gary marcus in there just for it you know
put in uh demis you know i could see demis being like secretly like super good at fighting you know
like it doesn't make any sense he just goes in there he kills all of them in like one move
all right
shaolin
all right what else do we got distillation scaling laws is there anything important here
student cross entropy
knowledge distillation loss there's that uh
kale divergence right there's that kale divergence from the reinforcement learning coming back
is equivalent to optimizing the callback liebler divergence between the teacher and student
predictions right
the problem with this type of distillation is that you need
these you need the log its
right
distillation if you have the log its versus distillation if you don't have the log its is like
way less efficient right because when you have the log its that means that you have
the full probability distribution over all possible tokens for the next token right versus if you're just
using the gpt 4.5 api you only get the actual token that it was right so it's like you don't have the full
log its you just get the actual token so it's less efficient to distill if you don't actually have
access to the log its than if you do have access to the log its which is why i feel like distilling with
something like diffusion which apparently has a much better data efficiency seems like a good idea
uh have you covered the flow matching and ddim
i think i did some of those papers for sure you probably can go back and dig into
uh some of the
historical streams to get that but if you're interested in
diffusion models i would 100% recommend this video right here that just came out this morning
how do ai videos actually work
this is a top tier video
but it's like classic kind of three blue one brown style animations where they like explain
all of the different diffusion models they talk about ddim how it's different from ddpm
uh it's very well done video they talk about clip flow matching so if you're interested in that
like honestly like don't watch my videos watch these videos this is gonna this is gonna get you
100% faster to the knowledge that you want
how much can we really push distillation if we train 1000 diffusion models and distill the knowledge
into one diffusion model of the same size with a lot of compute
you it's not so you can't
one way to think about like what a model is is a model is kind of a compression of the data set that
it was trained on right it's like a compressed version of the data right so if you take a teacher
model and you distill a student from it
the student is a compressed version of the teacher which is a compressed version of the data
so really the student is kind of just a compressed version of the data
but if you distill 1000 students from the same teacher
you're not getting extra stuff there right it's like every single student is going to be kind of like
the teacher so it's not like there's a benefit to distilling a thousand different students from
the same teacher right generally when you get into ensembles like that it's more about better
exploration right so having a lot of different models is is good whenever you get into this notion of
kind of exploring search spaces right so if you're doing some kind of reinforcement learning process
usually right you want to be able to efficiently explore so you need to add some kind of randomness
there so you can basically say okay well uh anytime i'm playing a game i'm creating a synthetic
game of go and i have the the go ai playing itself right i'm not just going to have them play
the next move every single time maybe 30 of the time i'll have them play a random move right and
in that way i'll explore more of the tree right but just using randomness as exploration is less
efficient than if you have a more directed form of exploration right so you can imagine having
a go game between two ais and then 50 of the time uh the move is not necessarily what the
super strong ai wants to do but it's like some other move uh that comes from a different ai that
has a different kind of uh next token distribution so that gives you a kind of a a more it gives you
the kind of randomness and the exploration that you need in order to kind of efficiently search the
space of all possible go moves but it's a more directed form of that search right it's so that's where
i can see kind of ensembles of models and and where having uh a diversity or a higher variance of
models is more useful right if you're kind of searching the space you don't want all your
models to basically always predict the same thing you want kind of some models to give you a slightly
different output so that you can kind of search a bigger part of the space that's where kind of
i see there's a benefit to having slightly different models but if you take
if you take a transformer and then distill it on something and then you take a
uh diffusion model and distill it on that thing it's like they're just going to end up at the
same place right distillation doesn't give you anything new it just basically gets you to where
that other thing is which is kind of why i keep describing it as kind of a shortcut where it's like
if someone's if gp if open ai spent like a hundred million dollars uh doing the pre-training for gpt 4.5
and someone distills from that gpt 4.5 they're going to get to something that's very close to gpt
4.5 but maybe with 10 million dollars right because they could just basically distill from that so it's
like distillation is just a shortcut one thousand teachers into one student now now that's more
interesting right where hey let's take grok 4 and let's take open ai 03 and then distill from both of
those into a student now that student is not going to be the same as grok 4 it's not going to be the
same as 03 and it might actually be better than both of them right so that's super interesting too
right distilling from kind of a mixture of different uh teachers
look at this thumbnail look at those three s's
all right guys i think i have to log out here in a second
let's do a quick summary all right so today's stream was called diffusion versus auto regressive
and the central part of this stream was this paper here diffusion beats auto regressive and data
constrained settings carnegie melton paper where they basically it's kind of like a scaling law paper
right similar to the chinchilla scaling law paper similar to the distillation scaling law paper where
they're basically showing you uh that if you take a bunch of models some of them auto regressive
some of them diffusion and you train them on the same data set for one epoch more epochs 100 epochs
right which is how many times you're training on the same data set there seems to be this pattern
that diffusion models uh can extract more juice from that data than auto regressive models and
really it all boils down to this uh bias that exists in diffusion models which is that they have this
uh implicit data augmentation that comes from the fact that they're a core part of the learning
objective in diffusion models is this noise right where you're kind of adding noise starting from noise
and that implicit data augmentation ends up allowing diffusion models to train for significantly more epochs
which has all kinds of interesting implications but like all other scaling law papers you know
it kind of depends you know really all they're showing you is for this specific model in this
specific training data set with this specific uh with or size of flops and like so like all scaling
law papers they're really it's really not you shouldn't take this a hundred percent at face value right
it's just a it's just an intuition that you maybe want to have but it could be the case that if you
extrapolate beyond that the it it breaks and it becomes something else right like it could be the
case that if you actually kept extrapolating this maybe the blue line would suddenly go down and it
would go way below the orange line right but we don't know so like all scientific studies
use it as an intuition piece feel free to interpolate but don't extrapolate too much
all right that's all i got thanks for hanging out guys thank you ed open room perpetual learner
aries josh phillips nlp prompter google account macamill rafael lil km afik farouk non
well some scrolling up sorry if i miss you i'm just sometimes i miss julian 87 uh ed 7 7 0
who else pratik everybody else thanks for hanging out hope you guys have a good weekend
and and yeah go out there train a diffusion model
