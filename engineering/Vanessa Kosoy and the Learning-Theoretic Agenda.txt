the current path is leading us to a disaster companies like openai and other leading labs
they haven't come up with any convincing solutions to the hard parts of the problem
and they don't even have the tools to do that because of lack of theoretical understanding
what the companies are doing is basically just developing things in trial and error
that's kind of a band-aid method which is to say it it works until it doesn't work discussing the
future of ai tends to get philosophical what does it mean to have goals or understanding is power
seeking a default consequence of wanting things or a human quirk resulting from our unique
evolutionary history what motivates kindness framing questions in this way makes them accessible
allowing everyone to participate in the conversation but this lack of precision also makes such questions
intractable as disagreements become a clash of intuitions today's guest on guardians of
alignment is vanessa kosoi who works for the machine intelligence research institute or miri
on building mathematical theories for safe ai this focus on understanding from first principles puts
her work in stark contrast to the move fast and break things experimental approach of leading ai labs
in this interview and elsewhere vanessa defends the value of a more theory-based approach and explains
what it means to explore machine learning as a basic science i've always been an autodidact so i kind
of just tend to learn things by myself when i was little i thought i will be a theoretical physicist i
actually have a bachelor's in math but after completing bachelor's instead of going to academia i decided to
go for a career in the industry instead and software so i had a long career in the software
industry specifically in algorithm engineering mostly computer vision various roles algorithm engineer
team leader r d manager i also had my own startup i was a consultant and then something like 10 years ago
i got exposed to this whole topic of existential risks from ai and started thinking that well that's
actually seems kind of important i started pivoting to that and initially it was just me doing
research in my free time then it was supported by miri and then i also got more recently also got
support from the long-term future fund which enabled me to go full-time so what was that uh process like
you were working just a self-directed uh way i presume and then and then later you gained support from
miri and and other sources uh how did how did that come about i just started reading some of the stuff
miri were writing and people unless wrong were writing and i had my own ideas about that i started
working on my own ideas and writing posts unless wrong mostly about that after that i got invited to
some workshops some events and eventually miri said that just well it seems like you're doing some nice work
here so maybe we will also pay you for it and i was great because that also enabled me to do more
of that and spend less time doing other things so yeah so a little background for the audience here
less wrong is a blog a very popular blog it's initially on rationality but also on ai related
things and then miri is the machine intelligence research institute i like to describe them as the
people working on alignment before it was cool tell me a little bit more about miri as a as an institution
miri were more or less the first to talk about existential risks from artificial intelligence
elianzer utkowski started talking about this in the year 2000 and initially miri was just like
utkowski and then like over the years they managed to get some funding and and then get you know other
researchers on board and they were thinking about this question of how do we make artificial intelligence
safe and how do we even approach the question what kind of mathematical theory we can come up with
to solve this question that's even before the deep learning revolution started and way before the whole
hype with large language models in the recent years so they have most of their time have been committed to
trying to come up with some foundational mathematical theory that will help us with air alignment more
recently they pivoted more to outreach and trying to influence policy because of their beliefs that
the timelines are really short and we don't really have time for developing this theory
unfortunately have you been a part of that pivot in terms of now doing more outreach are you still
continuing to do the the theoretical research as before and it's just miri's doing less of that
yeah my outlook is different i'm more conservative you could say i think that the timelines are not
wide as short as a lot of the people in the areas community believe and i think that if the effort to
in the policy channels to regulate ai development and the delay ai development to stop the really
dangerous kind of ai from coming about will succeed then it's only buying us time and then like the
question is buying our time for what and i think the theoretical foundations is definitely still the
most important thing among the things we shouldn't be doing with the time that we have or with the time
that we will succeed to buy by some policy plan one way or another i think that practically in any world
creating this foundational theory is the key and that's what i'm doing that's definitely where my
personal skills and advantages lie in working on math and not on policy you mentioned uh timelines
intuitively and it's no it's impossible to really predict these things with any kind of precision but
just in terms of like what motivates you what you kind of see as your timeline to for when this stuff
needs to be solved well i mean some people think that you know agi is coming in in 10 years or even
less i think that it's a little extreme but when things need to be solved the sooner the better right
like you know if we have the solution in in five years then we're in a better place than if we only
have the solution in 10 years which is still better than we only have the solution in 20 years and so on
realistically my personal opinion is still the chances are that there are still multiple decades
before we actually arrive at the sort of ai that poses existential risks so that kind of
gives us a little more time but not infinite time describe that realization what made it click for you that
this is something that you want to work on it actually started as more like a curiosity because it
actually started with me discovering completely randomly you could say some papers on agi you
know not not even on air alignment or risk or anything like that but just some paper by jrgen schmidhuber
and marcus hutter with some ideas about agi and i always was a moth nerd so it seemed really cool
that there are those mathematical frameworks for thinking about agi i started reading about that
and eventually also found less wrong where some people were also discussing this kind of thing then
i i well on the one hand i was reading more and more things that eliezer kowski wrote on the topic
and people unless wrong wrote on the topic but i also started thinking about the mathematical models
eventually it clicked to me that when you're thinking about the actual math then it makes
perfect sense that there there's no reason there's no mathematical reason why ai would have to you know
care about humans or care about anything at all aligned with what we as humans care about and on the
other hand it's also seemed obvious that it could be much more capable than us it's pretty clear
intuitively i think but but for me i like to understand everything through math so for me when
i thought that you could actually put that into a mathematical model then it really clicked for me
that this is the real thing this is something we should really be concerned about this idea that
there's no reason to assume that ai will necessarily be nice that sounds a lot like the orthogonality
thesis that uh bostrom wrote about that yeah you can have intelligence on one hand and how nice it is
on another thing and they don't have to go together or any set of values can go with any level of
intelligence is that essentially what your uh the insight was yeah yeah that's exactly the orthogonality
thesis i mean it's not in hindsight it seems just like an obvious thing but for me it was necessary to
see that you could actually think of it in terms of mathematical objects like you know values could
be formalized as a utility function and then an agent could be formalized as some kind of a optimizer
some kind of a bias optimal policy or whatever for for this utility function you could actually put
rigorous meaning behind each of those terms and see that it actually all makes sense it's not just
some kind of a philosophical hand-waving trick what in your view is the fundamental reason that makes
the problem of ai alignment hard i think the problem is hard i think that what makes it hard is
well first of all that we're aiming at a very narrow target because human values are very complicated and
specific we care about a lot of very detailed things you know love friendship beauty in in you know our
own subjective understanding of it sex all of those things are kind of human things that exist at because
of some complicated accidents of evolution the way it happened on some very particular planet
it's a very particular point in history of a particular universe this set of values is a very
very narrow fraction of the huge space of possible values of huge space of possible minds that you could
imagine so most minds are definitely not anything that would be nice to us but by our standards worse than
that there's also this phenomenon which nate soras formulated nicely in one of his relatively recent
posts where he wrote that there is a basin of attraction around agent capabilities but there is no basin of
attraction about agent alignment so what it means is that if you apply enough optimization pressure even
with brute force techniques you will eventually generate highly capable agents and an example of that is
evolution right evolution right evolution is a very in in some ways very primitive brute force algorithm
that eventually may created the human brain which is a much more sophisticated algorithm if you put
enough brute force optimization into looking for things that are successful in open world environments
eventually you will come across intelligent agent and and this is even before you put in recursive
self-improvement in the equation which makes it even stronger as this kind of basing of attraction where
you converge to whereas nothing of the sword is true about being aligned to human values in particular
it's very plausible that we could by just kind of blind or semi-blind trial and error arrive at creating highly
capable agents much before we understand enough to actually make those agents aligned that sounds like going against the
other ostrom popularized idea of instrumental convergence that things like survival will be wanted by almost any
system optimizing hard enough there's this notion of instrumental convergent goals which are certain goals that
most intelligent agents will pursue because they help them to achieve their terminal goals whatever their terminal
goals are and these are things like survival like gaining more resources becoming more intelligent and so on and so forth
but human values are not that if we manage to construct an AI that survives and gains a lot of resources
that's nice for the AI I guess but that doesn't help us in terms of alignment to our values at all
that's a very different kind of thing does the issue of modern AI and being trained on human generated data and
existing in human society not help I think it help but it kind of leaves a lot of questions one of the
questions is okay you learn from human generated data but how do you generalize from there because it's
really not clear what conditions are needed to get a good generalization especially when the concept
you're learning is something extremely complicated the harder the complexity of the concept you're learning the
more data points you need to learn it what we're doing with the so-called large language models which
are the hype in recent years is trying to imitate humans which is I mean nice it could even lead to
something good with some probability not a very high probability but the problem with it is that in order to use
that you need to generalize far outside the training distribution here we actually need to look at what what the goal is
the problem is that it is technically possible to create super intelligent AI which will be dangerous
to solve this problem it's not enough to create some kind of AI which would not be dangerous because
otherwise I could you know write an algorithm that doesn't do anything that's not dangerous machine
accomplished we need to be able to create AIs that are sufficiently powerful to serve as defense
systems against those potential dangerous AIs those have to be systems that have superhuman capabilities
and building sophisticated models of the world and building complex long-term plans based on that and
that's something that is far outside the training distribution of a large language model or anything
that's based on human imitation it is extremely unclear whether we can actually rely on the algorithms we
have to generalize that far out of the training distribution without completely losing all of their alignment
properties LLMs are fundamentally imitative which doesn't seem particularly dangerous in itself but
it also limits what it can do and so we can't really expect that that's just going to stop here
eventually there might be something like reinforcement learning added onto it maybe not necessarily that
algorithm but something that can be as creative as alpha zero is in go and finding a really creative move
that no one's seen before so we need to be prepared for things that are very much more powerful because
those are going to be useful and the economics that led to building LLMs are going to lead to building bigger things
is that roughly what you're getting at with that preparedness sense
yeah yeah that sounds pretty much on the spot either it will be reinforcements learning or well I don't want to
speculate too much what is needed in order to make AI more powerful because it's not good information to
have out there fair enough moving on a little bit more to the the things that you uh actually work on
one idea I've seen as adjacent to this is the term agent foundations and also the learning
theoretic agenda what are those things agent foundations is is kind of this abstract idea that says
we need to create a foundational mathematical theory which explains what agents are you know
like what what does it mean mathematically for an algorithm to be an agent what types of agents are
possible you know what what capabilities they can or or cannot have and so on the learning
theoretic agenda is more specific than that in the sense that it's like a very specific program that is
trying to achieve this goal specifically by tools that build on statistical and computational learning
theory algorithmic information theory control theory this kind of thing so this is basically kind of the
program I created to answer this challenge of coming up with those agent foundations okay so agent
foundations is like the the question of how do minds work that encompasses AI and learning
theoretic agendas like how do we design algorithms that like push this in a good direction is that
right I wouldn't put it that way I would just say that like agent foundations is just trying to
understand how minds work and people have been trying to do it in various ways you know like miri have
historically had all sorts of proof theoretic models that try to to approach this and then there's
garbrand's logical induction and there's various ideas under this very broad umbrella whereas the learning
theoretic agenda is a very specific approach it's kind of this approach that starts with AIXI and kind
of classical reinforcement learning theory as the starting points and then kind of looks what are the
missing ingredients from that in order to have a foundational theory of agents and start building
towards those ingredients towards those missing ingredients with ideas such as infrobationism and
infrobation physicalism and metacognitive agents and so on the kind of agents and minds that you're
talking about here is this tied to the frontier large language models or is it more broad to AI or any kind
of thinking entity when I say agent I mean something very broad much broader than existing AIs or even
just AI certainly like including humans or you know potential aliens or whatever so for me an agent
agent is a system that has particular goals and it is learning sophisticated models of the world
in which is in which it is embedded and uses those models to build long-term plans in order to achieve
its goals so this is kind of the informal description of what I mean by an agent and then like the whole
goal of this program is to go from this to a completely formal mathematical definition and study all the
implication of this definition so not even beyond llms it's even broader than machine learning what's
the reason for that approach given how dominant machine learning is why not focus on the things
that seem to be the most widely used yeah so first of all let's do some order in the terminology I would
distinguish between AI machine learning and deep learning AI is kind of this thing that people started
thinking about since the 1950s about how to build thinking systems without really having good
understanding of what what does it even mean but just some kind of intuitive notion that there is such
a thing as thinking and we should be able to replicate it in a machine machine learning is a more specific
approach to this that kind of emerged well I don't want to point the finger exactly when but probably
something like the the 80s machine learning talks about specifically this idea that the central element
of thinking is learning and learning means that you're interacting with some unknown environment and
you need to create a model of this environment so you need to like take the data that you see
and use it to create a model and that's analogous to how scientists do experiments gather data and then
like build theories based on this data this kind of general idea is called machine learning or more
accurate would be to call it just learning the machine part comes from trying to come up with ways
to actually implement this inside a machine this is a field that has a lot of mathematical theory the
mathematical theory behind machine learning is what's known as statistical and computational learning
theory and that's actually the foundation of the learning theoretic agenda that's why it's called
learning theoretic there has been a hypothesis that this kind of notion of learning captures most of
the important bits of what we mean by thinking and I think that this hypothesis have been extremely
well supported by recent developments in technology and this is something that I completely endorse and
this is basically the basis of my whole research program so there's no contradiction here because
learning is still a very general things humans also do learning aliens would also have to do
learning deep learning is a more specific set of algorithms to how to actually accomplish learning
efficiently in a machine and and that's what started deep learning revolution around 2010 although the
the algorithms existed in some form for decades before that but it took a while to to get the details
right and also to have the right hardware to to run them so deep learning it's unfortunate feature is
that we don't understand it mathematically so a lot of people are trying to understand it but we don't
have a good theory of why it actually works that's why it's not the focus of my research program because
I'm trying to come up with some mathematical understanding I definitely have a hope that eventually people
will crack this kind of mystery of how deep learning works and then it will be possible to integrate the
theory I'm building with that other theory even if we had this theory then it still seems really
important to think in the broadest possible generality because well first of all we don't know that the
algorithms that exist today will be the algorithms that bring about AGI but also because the broadest
generality is just the correct level of abstraction to think about the problem to get at those concepts of
what does it mean even for a system to be aligned because there are some philosophical problems
that need to be solved here and they are in a way specific to some very particular algorithms and
also there is the fact that I actually want this theory to also include humans in some ways because
I might want to use this theory to formalize things like value learning right how do I design an AI system
that learns values from humans seeing the wikipedia level or just browsing the internet description of machine
learning and deep learning it's very easy to use them interchangeably like I think I've seen the
description as deep learning is just the idea that you add multiple layers of neurons and so because
there's multiple layers it's deep yeah so let me try to clarify the distinction machine learning
talks about taking data and building models from it the type of model you're building can be very different
before deep learning we had algorithms such as support vector machines polynomial regression is also a very
simple type of machine learning fitting a model to data various methods used in statistics can be
regarded as sort of machine learning so there is some space of models or space of hypothesis and
you're trying to use the data in the optimal way to infer what the correct hypothesis is or have some
probability distribution of her hypothesis if you're like doing a bayesian approach or whatever but different
types of hypothesis classes lead to very different results in terms of the power of the algorithms but
also in terms of what do we know to say about how to learn those hypothesis classes and what do we know to
prove mathematically under which conditions we can actually learn them for example for support vector
machines the mathematical theory is basically solved like for kernel there's like kernel methods that build on
top of that and that's also has very solid mathematical theory deep learning is a particular type of
learning algorithm which uses those artificial neural network architectures it's not just multiple
layers there's a lot of details that matter for example the fact that the activation functions are
rather it turns out to be pretty important or what kind of regular realization matter you use in training
you know like dropouts is basically what kind of started the deep learning revolutions essentially
if you're working with sequences then we have transformers which is a very specific network
architecture so there is actually a lot of very specific details that people came up with over the
years mostly with the process of trial and error to just see what works we don't have a good theory for
why those specific things work well we don't even understand the space of models those things are actually
learning because you can prove theoretically that if you take a neural network and you just let it learn
another neural network then in some situations that will be infeasible but for real world problems
neural networks succeed to learn a lot of the time and this is ostensibly because the real world has some
particular properties that make it learnable or there's some particular underlying hypothesis class that the neural
networks are learning and which captures a lot of real world phenomena but we don't even have a
mathematic description of what this underlying hypothesis class is we have some results for some
very simple cases like two-layer or three-layer neural network or some other simplifying assumptions but
we're not close yet to having the full answer deep learning assumes certain things about how the world is
in terms of what it can pick up and it happens to work fairly well but it's not really clear what
it's assuming yeah that's exactly right so we have different no-go theorems which say that for arbitrary
data even if the data is fully realizable so even if the data is such that the neural network can
perfectly express an exactly correct model even if these cases in general the problem is infeasible in
general gradient descent will not converge to the right model and also no other algorithm will converge because
the problem will just be intractable there are some properties that the world has and
since this deep learning is successful in such a big variety of very different cases it feels like those
properties should have some simple mathematical description you know it's not like some properties
that are extremely specific to tags or to audio or to images it's some properties that are extremely general
and hold across a wide range of different modalities and problems those are properties that i can just
speculate that those are could be for example properties that have to do with compositionality
which have to do with how the real world is often well described as being made of parts and how things
can be decoupled according to different spatial scales or different temporal scales on which the dynamics is
happening but we don't have a theory that actually explains it to get a little more concrete about
this you mentioned relu as one of the examples of a thing that just works as i understand it relu is
basically like taking the output and it'd be kind of being like a graph where it's flat on one side and a
diagonal line past zero so just get some input and run it through this function to change the output in this
way whereas before it would use a sigmoid which was kind of more like a smooth line and for some reason
relu works better and the the sense i'm getting from your uh explanation or at least in the direction
of why is that this impacts what kinds of things the neural network is able to understand and so it
expanded it or changed it in a direction that's more matching with reality but it's all these things
or come up with a throw stuff at the wall and see what sticks kind of way and just measure the results
without really understanding why relu is better than sigmoid i mean that's more or less right we
we have to just be careful that when we say what the neural network can understand it's a pretty
complicated concept because it's not just what the neural network can express with some set of ways but
it's what the neural network can actually learn for a process of gradient descent so it has to do not
just with the space of functions that the neural network can describe but with the entire lost
landscape that is kind of created in this kind of space of weights when we look at a particular data
set when you're describing a gradient descent and lost landscape the analogy i hear throw around a lot
is like a ball rolling down a hill there's a constant gravity force and you want it to get down to sea
level but often it doesn't because it finds some local minima or just like a hole or something where
any direction it goes is up and so it's not going to roll anymore it's not just what the neural
network can learn and express but also how these design choices shape the landscape so that down
always gets you to sea level yeah that's that's a pretty much a pretty good explanation gradient
descent is something that we have good mathematical theory for how gradient descent converge to the global
minimum for convex functions but the loss for neural networks is non-convex but it still happens to be
such that it works people have gained some insights about why it works but we still don't have the full
answer for that okay so if the the landscape is really bumpy then you kind of don't expect it to get
to the base but it does somehow and that kind of framing raises a lot of just immediate questions of
unpredictability like if you don't know how this what this landscape looks like then where's that
thing going to end up you mentioned ixie at one point what is that ixie is this idea by marcus hutter
which is supposed to be a mathematical model of the perfect agent the way it works is there is a prior
which is the solomon of prior which for those who don't know what that is it's basically some way to
mathematically formalize the concept of occam's razor occam's razor is this idea that simple
hypothesis should be considered a priori more likely than more complicated hypothesis and this is
really at the basis of all rational reasoning what what hutter did is he took solomon of prior
which is a very clever way to mathematically formalize this notion of occam's razor and say
well let's consider an agent that's living in a universe sample from from the solomon of prior
and this agent has some particular reward function that it's maximizing and let's assume it's just
acting in a base optimal way so it's just following the policy that will lead it to maximize its expected
utility according to this prior and let's call this axi which is a really cool idea only it has
a bunch of problems with it starting from the minor in quotes problem that it's uncomputable there's
not an algorithm that exists to implement it even in theory i think i heard it uh ixie explained once
is imagining the entire universe described as a bunch of bit ones and zeros and it's at the start
all of them could either be a one or a zero then you get a little bit of data and now you've locked in a
few of those numbers and now you've with each one you've cut the space of all things it could be in
half as you keep learning you get more and more certain it's actually a little more nuanced than
that because well just the fact you have a lot of possibilities doesn't mean that it's uncomputable
because maybe the exact thing is uncomputable but you could still imagine that there is some clever
algorithm that approximate this bayesian inference process so for example if you look at classical
reinforcement learning theory then there are things like algorithms for learning in arbitrary mark of
decision process with n states so mark of decision process with n states it still has an exponentially
large space of possible ways it could be and we still have actually efficient algorithms that converge
to to the right thing out of this exponentially large thing by exploiting some properties of the
problem the thing with axi is that its prior is such that even individual hypothesis in the prior
are already arbitrarily computationally expensive because in its prior it considers every possible
program so every possible program that you can write on a universal turing machine is a possible
hypothesis for how the world works and some of those programs are extremely expensive computationally
some of those programs don't even halt they just enter infinite loops and you can't even know which
because this is the healthy problem right this is why the ai excites a setting where it's it's kind
of a non-starter for doing something computable not to mention computationally tractable the minor
problem of uncomputability aside a perfect algorithm what does that mean does that make it safe if it were
somehow magically computed no it doesn't make it safe in any way it's perfect in the sense of it's the most
powerful algorithm you could imagine again under some assumptions i mean there are other problems
with this it assumes that the outside world is simpler than the agent itself there are like multiple
problems with this but if you can put all those problems aside then you could argue that this is
the best possible agent and in this sense it's perfect it's very very very not safe like in order for
for it to be safe we would need to somehow plug the right utility function into it and that would still be
a very non-trivial problem to to understand how to do that what kind of algorithms do you look for
i'm imagining that there will be something that i call the frugal universal prior which is some kind of
prior that we can mathematically define which will simultaneously be rich enough to capture a very big
variety of phenomena and on the other hand we'll have some clever algorithm that can actually allow
efficient learning for this prior using for example some compositionality properties of the hypothesis
of this prior or something of death but even knowing this prior there's a lot of other conceptual
problems that you also need to deal with like what i call the problem of privilege where the formalization
of occam's razor it kind of privileges the observer and you need to understand how to deal with that
then there's the problem of realizability where you cannot actually have a hypothesis which gives you a
precise description of the universe but only some kind of approximate or partial description and you
need to understand how to deal with that fact then there's also the fact that you want your utility
function to be not just a function of your observations but also some parameters that you cannot
directly observe and you want to be able to deal with that somehow you also want to be able to prove
some frequencies guarantees for this algorithm so to know how much data this algorithm actually needs
to know to learn particular facts and have like a good theory of that so there's like a whole range
of different questions that come up when studying this axi like models studying axi like models that is
what you're working on you could yeah if you wanted to put it in one sentence i guess what what are some
interesting problems that you're interested in solving i've seen newcomb's problem floating around
and stuff adjacent to this newcomb's problem is a problem that in the other utkowski wrote about a
lot as something that's very confusing for classical accounts of rationality you have two boxes that you
need to choose from one box has a thousand dollars the other box has either nothing or a million dollars and
you can either choose the first box or you could take the money that's in both boxes and normally
taking the money that's in both boxes is always strictly superior to just taking the one box except
that in this spot experiment there is some entity called omega which can predict what you do and so
it only puts the one million dollar in the other box if it knows that you will only take
that box and won't try to take the thousand dollar box as well so only if you're the type of agent that
would predictably for omega take only one box only in this case you will get out of the room with one
million dollars whereas in the other case you will only have one thousand dollars so arguably it's better
to take one box instead of two boxes as opposed to what many classical accounts of rationality would
say this is one example of an interesting thought experiment for me this thought experiments falls
under as a special case of the problem of non-realizability where like you need to deal with environments
that they're so complex you're not able to come up with a full description of the environment that you can
actually simulate because in this example the environments contains this agent omega which simulates you
and this means that you cannot simulate it because otherwise it would create this kind of circle
paradox i've actually also showed that my theory of dealing with non-realizability which i call
information is actually leads to optimal behavior in this kind of newcombe-like problem scenarios and
the newcombe problem like the reason for looking into that is not because we expect to be faced with
the omega offering us boxes at some point but because it's just a visible way of thinking about like how
do i deal with things when i i can't know what's going on and and also like yeah it might be easy to
say yeah well i'll just take one box because i'll get more that way but then when you really dive into
what's a coherent non-hand wavy reason as to why then there's some interesting insights that can come
potentially come up from that have there been any discoveries that that you found through exploring these
kinds of things i would say that informationism itself is an interesting discovery that some
theoretical account of agents that can reason about complicated worlds that are much too complicated
for the agent to simulate now i explain it usually i kind of motivate it by stating the problem of
non-realizability but the way i actually arrived at thinking about this is through thinking about
so-called logical uncertainty the reason people started thinking about it was because of so-called
updateless decision theory which came from thinking about newcom-type paradoxes so it all comes from
that line of reasoning even though after the fact you can motivate it by some much more general abstract
motivation what's the connection between these decision theory type questions and making a safer ai
the idea is creating general mathematical theory of agents the way it's going to help us with making
ai safer there are several reasons the most obvious reasons is that having this theory we hopefully will
be able to come up with rigorous models of what does it mean for a system to be an aligned agent and
having this rigorous definition we'll be able to come up with some algorithms for which we can prove those
algorithms are actually safe agents or at least we could have some conjecture that says that this given
model of such and such conjectures we think that those algorithms are safe agents like in cryptography
we have some conjectures which have very strong evidential support we could have at least some
semi-formal arguments because now when people are debating whether a particular design is safe or not
safe it's all kind of boils down to those hand-waving philosophical arguments which don't have any
really solid ground whereas this gives us tools for much more precise crisp thinking about this kind
of question it hypothetically also gives us much more power to leverage empirical research because maybe
we will be able to to take the empirical research we have plug it into the mathematical theory and get
some answers about how we expect those results to actually extrapolate to various regimes where we
haven't tested it would this line of research that you're working on eventually be usable in evaluating
things like large language models or deep learning based systems to be able to say with greater
certainty the extent to which they're safe or unsafe i think that there are multiple paths to impact so
there is a path to impact where we will eventually come up with a theory of deep learning if not like
a fully foolproof theory then at least some strong conjectures about how deep learning works that can
interface with the theory of agents i'm building and then we could use this composite theory to prove
things or at least to have like strong arguments about properties of systems building deep learning there
could be a different path to impact where we use this theory to come up with a completely new type
of algorithm for building the eye which are not deep learned but for which we have a good theoretical
understanding there's also some first possibility that we won't have a good theory but we could at least
reason by analogy similarly to how many deep learning algorithms are designed by analogy to some
algorithms for which we have mathematical theory like deep key learning is analogous to simple key
learning for which we have mathematical theory so we could also imagine a world in which we have
some kind of idealist toy model algorithms for which we have some rigorous arguments why they are
aligned and then we have some more heuristic algorithms which we cannot directly prove things about but
which are arguably analogous to those toy models so i heard three paths to impact there one was potentially
building a different form of ai from the ground up that's verifiable right from the beginning and does
the same things as deep learning based ai but in a more rigorous sort of way a second one is evaluating
or better understanding deep learning or whatever is state of the art and then a third in between the
two is having a simpler form of ai that analogizes to the state of the art things that you can use to
understand the other one better yeah that sounds about right i'd like to focus a bit on using
foundational research to understand other things like deep learning getting at this theory based
approach bringing in like a total opposite counterpoint one could argue no you should just look at the
things that are being used and collect data about it and then build your theory by finding patterns in
the data when the theories are wrong as a result of more data then update it then why work on theory
in advance the biggest reason is that you cannot really reliably extrapolate from empirical research
without having an underlying theory right because you might for example take some measurements and find
some trend but then there is some phase transition later on that you don't see in the trend but which
happens and behavior changes to a completely different regime and because you don't have a theoretical
explanation you won't notice or people will just switch to using completely different algorithms which
behave in completely different ways so you might have those empirical models that you have of existing
geis but those empirical models are very myopic they're always looking one step ahead and then
you don't see the cliff three steps ahead of you updating those theoretical empirical models on new
things that happen it might just be not quick enough like eventually you fall off the cliff
and then it's too late to say oh actually that trend line was wrong but this is already too late
luckily we are in a domain where we have tools to do research even without empirical data or of course
we should use the empirical data we have but we're not necessarily bottleneck on empirical data
because the thing we're studying is algorithms and algorithms are mathematical objects so they
can be studied mathematically and this is very different from just studying some phenomenon like you
know just some phenomenon of physics where if you don't have the data then there's no way to generate
the data without having it whereas here this really should all boil down to math well or more precisely
it should boil down to math plus whatever properties the real world phenomena have that we want to
assume in our mathematical theory here it is like something that we need empirical input for but on the
other hand we already have a really good understanding of physics so given the knowledge of physics and other
scientific domains that we have it's very plausible that we just have enough information to answer all the
questions purely for mathematical inquiry even if we had no empirical data at all which is not to say
we shouldn't also do empirical data to kind of supercharge this research but we're not limited to that so
it's not a choice between theory versus experiment we should be using both you're focused on the theory side
and arguably there's not enough work on that and that's where the bottleneck is not on just getting more
data yeah yeah i think we should definitely be doing both ideally there needs to be synergy where
experiments produce new phenomena for theorists to explain and theory inspires the experiments the
theorists should be telling the experimentalist which questions what kind of experiments are the most
interesting and we should have this kind of synergy but i think that in the current landscape
definitely in the alignment the theory side is the side is currently left behind and that's where i
think we should put the marginal efforts in do you see that uh synergy existing now like is open ai
asking murray for feedback on their experiments or is there any kind of connection or they just people
siloed off from each other i think it doesn't exist now almost at all i mean okay no to be
fair it exists in some areas and doesn't it exists much less in other areas for example there's
people working on singular learning theory there's this new oracle tmeos i think that they are much
more interfaced with experimental work which is good the kind of research that miri is doing is and
and the kind of research i'm doing is much less interfaced with experimental work i have some plans for
creating an experimental group working in closed loop with me on those questions as part of my big
long-term plans but i i still haven't gotten around to to doing that if you could change anyone's mind or
set political agendas or business kind of things what would you like to see happen to have more of an
interface well i think first of all we just need more theorists because to have an interface we need
something to interface with so we just need more theorists for one thing i think that that this is
in practice that's where the bottleneck is now once this progress in theory gets sufficiently paced there
will be a bunch of questions i mean there are already questions that i would like to see experiments on
but the more this thing picks up the more such questions we have i think that the main bottleneck
is just in having more people working on this theory what would change from an external perspective
if there was a lot more theory work i imagine a skeptic could argue here that like hey the open ai and
these other companies are making these really awesome breakthroughs in a in a largely experimental
kind of pace it's it's working great if it ain't broke don't fix it what's broken in in your view
i think that the current path is leading us to a disaster i think that companies like open ai
and other leading labs are wildly overconfident about their abilities to solve problems as they come
along i think that they haven't come up with any convincing solutions to the hard parts of the
problem and they don't even have the tools to do that because of lack of theoretical understanding
we don't even have models that are precise enough to have a solution in which we could really be
confident we need to be very precise about the arguments we make which convince us that the
solution is good and we don't even have the tools to reach this type of precision what the companies are
doing is basically just developing things in trial and error if we see any problems then we'll just tweak
the thing until the problem goes away and that's kind of a band-aid method which is to say it it
works until it doesn't work it fixes problems on a superficial level but eventually there will come a
point where either the problem will not be caught in time and the results will be catastrophic or the
problems will be caught in time but then nobody will have any idea what to do in order to fix it and
eventually someone is going to do the catastrophic thing anyway the only thing which makes me perhaps
less pessimistic than other people in miri for example is because i think we still have more time
i don't think they're quite as close to agi and i think that a lot of things can change during this
time which is again not to say they will change we might burn all this time and still end up with a
catastrophe what's an example of an existing problem that only has superficial solutions the real
problem we're concerned about is not really an existing problem right the main thing we're
concerned about is the future ai systems which will be much more powerful than existing ai systems
will bring about extinction of the human race or a catastrophe on a similar level that's not an
existing problem for the simple reason that the ai systems we have today are not capable of learning
a model of the world that's so sophisticated that it enables you to do this this type of actions
but we see that even the problems that we have the companies kind of struggle with all the things
that happen with large language models were the infamous jail breaks where they're trying to make
them well behave in various ways you know not telling the users offensive and for like dangerous
information for example and the users easily find jail breaks to work around that or tell false answers
but again that's for me that's not the real problem that's just an analogy i mean they're kind of
struggling with this very simple much easier problems now which is not to say they won't solve
them trial and error will get you eventually the reason trial and error is not the solution for
existential risk is because once you're everyone are dead the trial is over there's no more trial
the problems we have now they're still struggling with them because they don't have like
principle tools to solve them but eventually they will trial and error their way through and will
fetch them somehow or at least they solve them well enough for it to be economic but once you reach
the point where failures are global catastrophes trial and error is no longer an acceptable method of fixing
the problem obviously we don't get to see lots of test data of the world ending but i would imagine
there'd be some precursor issues that are smaller but hint at what's to come do you see the challenges
with uh hallucination or not being able to control what the ai says as those kind of precursors are
totally irrelevant or there's there's something else or is this the kind of thing you're looking at
there just won't be any precursor issues it's kind of a difficult question because they're really very
important bits that are still missing from existing di systems to produce the kind of risks so we can
point at examples where systems are kind of misgeneralizing there's a lot of famous examples some
program that wins tetris and quotes by pausing the game forever or that wins some boat threat game by
racing the boats in infinite circles doing various weird unintended behaviors because the metric that the
algorithm is maximizing is not actually what the users intended so you could call those precursors
but i feel like it's not exactly capturing the magnitude of the problem because those are still
toy settings there are no kind of open world systems that are acting in the open physical world
the goals that they're trying to solve are much simpler than human values there's not really operating
domains where there are really complex ethical considerations maybe large language models are
starting to approach this because they enter domains where there are like some at least morally not
completely trivial issues that come up but on the other hand large language models are not really doing
things that are strongly superhuman they're very superhuman in the sense that they have like a very
large breadth of knowledge compared to humans but not in our senses so it's it's kind of hard i mean
there are things that are sort of analogous but it's not really strongly analogous but then again our
motivation to be concerned about this risk doesn't come from looking at all that you know like alias
rutkowski starting talking about those things before deep learning was a thing at all yeah i guess the
reason i was asking about it is in the places where the stuff gets debated and polarized one of the
common objections is that oh there's no evidence behind this this is all just storytelling is there
evidence of the danger or does it really come from looking at the math the problem is what do you call
evidence because that's really a very complicated question the things that would be like obvious
evidence would be things like ais completely going out of control breaking out of the box you know hacking
the computer copying themselves to their computers out of light manipulating human operators and so
on but this kind of thing this is a sort of canary that you only expect to see very very very close to
the point where it's already too late it's not possible to say that we will only rely on this type
of evidence to resolve the debate other type of evidence some people can say that evolution is sort
of evidence how a machine learning algorithm can produce something which is completely unaligned with
the original algorithm other people show you reinforcement learning algorithms doing not what the
designer intended but for every argument like that you could have a counter argument which says yeah
but this example is not really similar we cannot really project from there to existential risk because
there are some disanalogies and yeah there will always be some disanalogies because until you have
ais in the real world that are very close to being an existential risk you won't have anything that's
precisely analogous to something presenting an existential risk so we have no choice but to
reason from first principles or from math or by some more complicated more multi-dimensional analysis
we just have no choice the universe doesn't owe it to us to have a very easy empirical way of testing where
those concerns are real or not one of the things i'm hoping for is that the theory will also bring
about strong arguments for the dangers or if the theory will tell us no everything is fine then we
can also all relax but the lack of theory is part of the reason why we don't have foolproof completely
solid arguments in in one direction or the other direction the challenge with finding evidence is that
anything you can point to that exists now could be interpreted in multiple ways having solid theory
would lend some credence to to one interpretation over another yeah absolutely like if if you have
a theory that says that a particular type of misgeneralization is universal across most possible
machine learning systems and we also see this type of misgeneralization happening in real
machine learning systems then it would be much harder to dismiss it and say oh yeah here we have
this problem but we'll do this and that and that will solve it easily one thing that's still bugging me
about the issue of evidence not being available now is the analogy my mind immediately goes to
is climate change you know you could say that like oh the idea of large swaths of the world being
uninhabitable and you know all sorts of other things is just this this elaborate story and all that's never
happened before you know why why would you believe that you know humans have never made the earth
uninhabitable but then you can look at a bunch of things that exist already of like small-scale
disasters the graphs of co2 versus temperature and all that sort of thing and point to say like hey
the really bad stuff hasn't happened yet but there is a lot of evidence what makes ai different from that
yeah i think that climate change is a great analogy because the big difference between climate change
and ai is that in climate change we have really good theory like in climate change we have physics
right and we have like planetary science which is on a very very solid foundations and we have computer
simulations it's still not trivial there are some chaotic phenomena which are hard to simulate or
predict so not everything is completely trivial but still we have some very very strong theoretical
foundation for understanding how those things work and what are the mechanisms and this theory is
telling us that there's still big uncertainty intervals around how much exactly degrees of
warming we're going to get with such and such amount of co2 but we still have a fairly solid prediction
there whereas with ai we don't have this like the analogous situation if you want to imagine like
a climate change ai style then it would be something like not having a theory which explains why
co2 leads to warming having some empirical correlation between temperature and co2 and then people
could argue ad infinitum correlation is not causation maybe the warming is caused by something
completely different maybe maybe if we do some unrelated thing it will stop the warming which is not
actually true we would be in the dark and with the ai we're currently in the dark what is happening
currently with your work at mary there are multiple problems i'm looking at hopefully we'll publish very
soon a paper on imprecise linear bandits which is related to informationism that i mentioned before
which is like a theory of agents that reason about complicated worlds so that's analyzing this theory in
some very simple special case in which i succeeded to get some precise bounds for how much data an
algorithm would need to learn particular things after that i'm starting to look into the theory of
learning state representations in reinforcement learning which is currently another big piece
missing from the theory which is kind of how your algorithms should learn about which features of
the world are actually important to focus about there's also in parallel i have a collaborator
gergie such who works on using my theory of information physicalism to create a new interpretation of quantum
mechanics he has some really interesting results there it's kind of a test case which demonstrates how this
framework of thinking about agents allows you to solve all sorts of philosophical confusions
in this case it's like confusions that have to do with interpretation of quantum mechanics
other people like in miri are working on other stuff scott gerberman has some project about some new type
of imprecise probability some new way of representing beliefs that have some nice compositionality
properties caspar osterheld from carnegie milan and abram damsky had a paper recently about some new type of
frequencies guarantees for algorithms that are making decisions based on something that's similar to a
prediction market so yeah a lot of interesting things are happening are there any other questions that
i have been on your mind that i did not ask that you feel would be really helpful and
someone seeing this getting a sense of what you're about here not a question exactly but i also have
a more concrete approach to how to actually solve alignment how to actually design an aligned
agent which i call physicalist super limitation is a kind of variation on the theme of value learning
but it draws from the framework of information physicalism which comes from the learning theoretic
agenda and from some ideas in in algorithmic information theory to come up with some interesting
at this point kind of semi-formal approach to how you could have an ai that learns human values
in a robust way it deals with a lot of problems that other approaches to value learning have
like how do you determine where are the boundaries of the agent how do you take into account things
which are the internal the agent which is the human in this case what is a human how do you locate
this human in space how do you take into account things which are not just behavior but also internal
thought processes of the human in inferring the human's values how do you prevent perverse incentives
such as the eye somehow changing or manipulating the humans to change the human values
or how do you avoid the inner alignment problem it answers a range of concerns that other approaches
have this sounds reminiscent of inverse reinforcement learning inverse reinforcement learning is this
idea that we should look at behaviors of humans in fear what those humans are trying to do and then we
can do this thing we as an ai so i actually have presentations in which i explain physical
supplementation as inverse reinforcement learning on steroids right it's taking this basic idea but
implementing it in ways that solve a lot of the deep problems that more simplistic approaches have
one problem that simplistic approaches have is that they model humans as perfect agents that follow
a perfect policy given perfect knowledge of the environment which is wildly unrealistic instead
i model humans as learning agents that kind of just learn things as they go along and they also might
even do that imperfectly another thing is the issue of boundaries what is a human exactly where do you
put the boundaries around the human is there just some particular input and output port which the human
uses and you consider everything that goes through the sports to be the human but then how do you deal
with various discrepancies between what goes into the sport and what the human actually intended to do
or various possibilities like the eye actually hijacking this channel and so on in my approach
instead the way human is formalized is that a human is a particular computation that the universe is
running and this is something i can actually formalize using information physicalism it has particular
properties which make it agentic so the agent detects which computations the universe is running
among them depends which computations are agents and amongst those agents it selects which agent is
its user by looking at the causal relationship and this way it homes onto the the boundary of the agent
the first thing is because we're talking about the computation that this human is running which is
human reasoning regarded as a computation we're also automatically looking at internals and like
internal thought processes and not just things that are expressed as external behaviors so we have
like potentially much more information there what would be the best way for someone to get involved and
what would they want to learn in advance one thing they could immediately start doing is just reading
up on stuff people did in agent foundations and in the learning theoretic agenda until now i have this recent post
learning theoretic agenda status 2023 which summarizes a lot of the things also have a reading list post
where i recommend some background reading for people who want to get into the field more concretely in terms of
careers tab so it's already too late for apply but now i'm running a track in months which is a training program for
researchers who want to get into ai safety so i have a track focused on the learning theoretic agenda hopefully there will be
another such track next year actually i have a fantasy of having an internship program which would bring
people to israel to work with me on this currently because of the war this thing has been postponed
but hopefully eventually things will settle down and i will revive this project these are currently
the main ways to get involved thank you for very much for that description wish you the best in developing
this theory and gaining more interest so that mismatch between evidence and theory starts to get corrected
and the researchers know what they're doing rather than stumbling in the dark thank you for having me
