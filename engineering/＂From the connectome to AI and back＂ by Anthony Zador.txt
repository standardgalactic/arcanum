Thank you.
Um, want to, I would love it if you ask questions, I would love to have a conversation.
Um, I've given, I've sort of taken my standard talk and, but you know, this is a small group.
So, you know, I have no need to get through every slide.
I'm not really going to, I'm going to talk about one project, but most of the talk is
going to be focused on sort of my overall approach and perspective.
Uh, now for, you know, I'm used to talking to neuroscientists and physicists, you guys
are an even more diverse group than, than my lab, which intellectually diverse, which
is, um, sort of, uh, saying something.
Anyway, um, my lab is, has these days two parts and experimental part and a theory part based
sort of focused on neuro AI.
Um, I'm not really, I'm not going to talk at all really about the experimental side.
I'll just say a couple of words, which is that for the better part of 10, 15 years, a lot
of my lab has been developing a suite of tools to convert the problem of neural circuitry into
a problem of DNA sequencing.
And the core idea there was that by converting the problem of circuitry into DNA sequencing,
we could leverage the incredible advances in DNA sequencing technology.
And basically I've made the argument that the only technology that has the scale to potentially
map out a connectome, not just once, but repeatedly would be DNA sequencing.
And so, um, that work is still ongoing in my lab.
And I hope that by the end of the end of this talk, you'll get some idea of why I find that
an interesting and important approach without, uh, I'm not going to talk about the technology.
I'm happy to talk about it, um, at any time.
So, you know, when I was a kid, I, I watched probably too much science fiction and, um, you know,
that's ultimately part of what motivated me to, um, to, to get into neuroscience and, um, AI.
Um, these are depictions, um, from the past, I don't know, 75 years of what robots and AI would
potentially look like in the future.
And so I've arrayed the, um, the pictures along a timeline for when the movie or TV show came out and, um,
in red is the proposed time at which we would, we would see that particular, um, so that particular robot.
So, um, uh, I'll call your attention, for example, to this one, um, this is lost in space.
Very few people recognize the original lost in space, but, uh, the famous words danger,
will Robinson come from, from here.
And, you know, I'll just point out that this show, which came out in the early sixties as
a low rent, uh, competitor for star Trek.
Um, it was terrible, terrible show, but it did have a robot and that was set in 1997.
So, you know, we've got, um, this one, which is, um, uh, Westworld.
That's the old Brenner, the original.
And, uh, it was set, it was, um, released, I think in the late sixties, early seventies.
And the prospective future was 1983, you know, um, uh, Terminator was set in a future of 1997,
parts of it, um, and so on.
So basically the bottom line is that we've been wildly optimistic and I was disappointed.
So, you know, I was, I was thinking that by the time I got to, to graduate school, you
know, in the late eighties, we would have robots.
And I was like, you know, what's going on here?
We don't have them.
Why not?
What's going on?
So why aren't we there?
And how do we get there safely?
Um, so it dawned on me that, um, you know, the, the most obvious way to get there would
be if someone handed us a robot, we could reverse engineer it.
Um, and no one was handing us a working robot, but they were handing us life and brains.
I was pretty sure that brains had a lot to do with, um, what caused us to be intelligent.
And I wasn't the first person to have this thought.
Uh, this is famously a quote from Richard Feynman, who, um, on his board on the day he died was,
uh, written what I cannot create.
I do not understand.
And, you know, that has been taken by some, uh, to be the idea that we're not really going
to understand the brain until we can build one.
Um, he also wrote know how to solve every problem that has been solved.
So, uh, one of the teaching points of this for me has been that if ever I go on a trip,
or even if I just leave my, my, my office, I make sure to have smart things on my board
just in case, you know, uh, cause that's what I'm going to be remembered by.
So I have to, you know, erase any, any thing that's wrong.
So, uh, you know, the, the whole vision of neuro AI is to, uh, reverse engineer the brain.
And, uh, we, we believe those of us who do know AI that this is a virtuous circle that,
um, building a brain, uh, helps us understand what's important in brains and understanding
what's how brains work helps us to build better brains.
So how would we know, um, if we succeeded in building, uh, an AI?
Well, the, the most famous and traditional, um, sort of test for that is the so-called
Turing test.
He called it, uh, so Turing of, Alan Turing, of course, was one of the fathers of mother,
modern computing, and he proposed the, what is now known as the Turing test, what he called
the imitation game.
And basically the idea was you have a machine and a human, and they're competing, trying to
convince this person that, um, that, that they are a human.
And for a long time, so, so that was the idea.
If we could, if, if an AI could get to that point, then, um, you know, we, we could just
say that we've built an intelligent machine.
Um, and that raises the question, right?
Does modern AI pass the Turing test?
I asked this of ChatGPT a little while ago, a couple months ago, gave this long answer,
but the bottom line is that by any reasonable measure, ChatGPT passes the Turing test.
Now we could, some people might quibble with that, but I, you know, I, and if you, if you
actually were to subject the actual ChatGPT to a Turing test, it would fail, but that's partly
because it's been programmed to, um, number one, tell you repeatedly that it's an AI.
Uh, if the goal were to modify it slightly to pass the Turing test, I think it would.
Does that mean that we have successfully built an intelligent machine?
And I think the answer is no.
Um, there are some pretty clear, um, cases where we can see that AI is still not perfect.
This is a beautiful image and I think it, it reflects both the strengths and weaknesses of,
of AI.
It's, it's really just a wonderful image.
You see the, uh, the unicorn's, uh, horn going through the man's head and there's just
something wrong with that in, in just the right way.
Um, so AI fails, but in seemingly unexpected ways.
And so the question is, can we make sense of that?
So, you know, this is, this is an old slide or actually it's a slide of an old failure,
but it illustrates the, the core problem of, um, all AI systems, which is they fail
on edge cases.
And that's as a result of the way they're trained.
So this is a classic, um, uh, failure.
A cow on a grassy background is correctly identified as a cow,
but a cow on a sandy background is called a camel.
And why is that?
Well, of course, because the AI is, um, uh, over rotating, um, on the sand in the background.
All right.
Now this is a fixable problem.
Uh, this paper is, I think from 2017.
And I, I very much doubt that, um, uh, a system today would make exactly this mistake.
Um, but the problem is that fixing mistakes like this, um, really amounts to a case of whack-a-mole,
right?
So we can fix this either by showing like the most obvious is to show the AI lots of pictures of,
uh, cows on sandy backgrounds and labeling them, um, cows and not camels.
And it'll get the idea eventually.
Uh, a sort of better approach is to, um, trick the AI into, um, emphasizing the background less.
That's a somewhat more general approach.
The first approach is what, um, Andreas Kolias, uh, describes as fixing the problem
of out of distribution, uh, data by making sure that no data are out of distribution.
So you just basically increase the size of the data set infinitely and then nothing, uh, will fool it.
That, that doesn't seem like the right, um, approach to me.
Um, here's another example of a self-driving Tesla crashing into a private jet, uh, due to the smart
summon feature, um, Cold Spring Harbor gets a fair amount of its money from philanthropic
private donors, um, high net worth individuals.
So this usually sort of causes them to perk up and recognize the importance of the problems that we're,
we're raising, um, sort of, and it's a less funny example of that is, uh, autopilot, um,
where Tesla crashed into overturned truck.
Um, so, you know, the problem with out of distribution data is that you're always going to have this long tail,
um, so a couple of years ago, Nanda de Fritas, who is at DeepMind, wrote, uh, I think this is a tweet
from about 2022. It's all about scale now. The game is over. It's about making these models bigger,
safer, computer efficient, faster sampling, smarter memory, et cetera. And, you know, indeed, language
models have increased dramatically in size over the last few years. But the question that I would raise
is whether that scaling is sufficient as a path to improvement. And, you know, here, here you see,
um, that, you know, you can scale the original plane, which flew a few feet off the ground up to 120,000,
about 120,000 feet, but you'll never get, um, planes that go to the moon because that's just not,
I'll never get there. So all of this dances around what is sort of the encapsulation of,
or the, the fundamental motivation of everything that my lab does in neuro AI,
which is beautifully stated as Moravec's paradox. Moravec was a roboticist. This quote is almost 40
years old, but it has stood the test of time and stands as an inspiration. He wrote,
encoded in the large, highly evolved sensory motor portions of the human brain is a billion years of
experience about the nature of the world and how to survive in it. The deliberate process we call
reasoning is I believe the thinnest veneer of human thought effective only because it is supported by
this much older and much more powerful. They're usually unconscious sensory motor knowledge. We are
all prodigious Olympians and perceptual motor areas. So good. We make the difficult look easy abstract
thought there was a new trick, perhaps less than a hundred thousand years old. We have not yet
mastered it. It is not all that intrinsically difficult. It just seems so when we do it.
So basically what, uh, Moravec says is that the things, you know, traditionally we're used to thinking
of, um, the things that humans do, the things that are uniquely human as being the most interesting and
important, right? That puts humans at the, at the center here. So chess and go and math and conversation,
like those are traditionally thought to be the pinnacle of these systems. And, you know, there's
planning and higher reasoning that some animals do. And then this sort of trivial business of interacting
with the world that all animals do. That's kind of the definition. It turns out that that's actually the
wrong way to think about it. This interacting with the world business, this sensory motor knowledge is
the hard part. And the stuff that is uniquely human is I should make this spell just a lot thinner because
that stuff is easy once you get the other stuff. So it's all about decentering humans just in the same
way that Kepler, uh, and Copernicus decentered the earth as the center of the universe. Uh, so Moravec's
paradox explains why, you know, uh, Google, uh, you know, alpha go defeated the world champion in go
almost eight years ago now, but we still don't have robots washing our dishes. We don't have, uh, Rosie
washing our dishes. So the usual counter to this when I bring it up is, um, you know, sure. Like the,
I would say that 99% of AI engineers reject this idea. They say, um, you know, sure brains have served
as the inspiration for, um, for humans. I mean, uh, brains have served as the inspiration for AI,
but once you have that basic inspiration, it's time to move on, right? We don't study feathers to build
better planes. And my argument to that is, well, if our goal were to actually mimic what birds do,
if our goal were to achieve artificial general flight, which is the analog of
so-called artificial general intelligence, then actually we probably would study birds because
a jet fighter can go fast in the same way that, you know, a computer can multiply many numbers and
are more numbers than a human can. But, uh, you know, no, no artificial flight system can
capture a fish from the water or hover in front of a flower. Okay. So, uh, that, that is sort of the,
the vision neuro AI can neuroscience has been the inspiration for most major advances
in AI and can continue to be. So I'm going to touch upon three areas that I think are particularly fruitful
for drawing, um, uh, neuroscience and then dive a little bit into one project in one of them. So
the first really tough problem is the so-called alignment problem. Um, this comes in many forms.
It's something that keeps lots of AI researchers up at night. Um, it's the AI safety problem. Uh, one
particularly vivid encapsulation of it, articulation of it is this thought experiment from Nick Bostrom
from about 20 years ago, the so-called AI paperclip maximizer. Suppose we have an AI whose only goal
is to make as many paper clips as possible. The AI will realize quickly that it would be much better
if there were no humans because humans might decide to switch it off. Uh, if humans were to do so, there
would be fewer paper clips. Also human bodies contain a lot of atoms that could be turned into paper clips.
The future that this AI would be trying to gear toward would be one in which there were a lot of paper clips,
but no humans. So I think that this is, uh, in large part, a problem because of the way we train AIs.
Um, we, we train AIs to maximize the single goal. And that's really not what, um, biological organisms
are programmed to do or have evolved to do. Um, you know, famously the, um, these, these, um, you know,
animals solve the four Fs feeding, fleeing, fighting, and mating. And they, um, you know, the ability to
balance these ultimately is much harder than I think than we think, right? We take it for granted.
This is, you know, Moravex paradox strikes again. We take it for granted that animals can do this
effortlessly, right? That, uh, you know, an animal can be eating one moment and then fleeing the next.
Um, I, I would say that, and, and, and it, they do so, um, sort of in, in ways that are hardwired.
There's not time to learn over the course of a lifetime to do these things. And I would say that
in humans, this gives rise to, um, you know, complex morality and it's that complex morality
ultimately that we're interested in, in, in alignment. Um, so one approach that we're taking
to this is to, um, domesticate our AIs, right? Some people think that it's not possible to align
AIs with our goals, but you know, we, we humans have repeatedly shown that you can hijack, um,
um, evolution even, and get an, an organism that, you know, the wolf on the left, which is pursuing
its own goals and, and create a domesticated version of that, like the one on the right
in just a few generations. So I think the, the idea is if we can figure out sort of what has happened to
the neural circuits that, um, uh, allows, you know, not only, uh, that it takes this, this,
this organism on the left that has satisfied the four Fs and just kind of rebalanced them in a way
that, that serves our goals and aligns to our interests. So that's all I'll have to say about
that right now. It's an ongoing, uh, project in the lab, still relatively early stages, but we're
pretty excited about it. The second, and this is probably the lowest hanging fruit for, um, for, for
neuro AI, which is the energy problem, uh, is, you know, it's, it's hard to know exactly how to make
the quantification, but it would not be unreasonable to say that brains are at least more than a thousand
times more energy efficient than, than GPUs. Um, you know, it is a fact that the human brain uses about 20
watts. Um, it's not quite clear what to compare that to, but, uh, for example, serving up a large
language model might be one thing you'd compare it to, and then you get a number like 10,000 watts.
Um, certainly you could compare it to training an AI model, which is one of the big costs of, uh,
LLMs. It's about a hundred million dollars, which is an amount of electricity that is comparable to
what it takes to run a small city or medium-sized city for a week. Um, training children to speak
is somewhat less expensive than that. Um, you can, you can do it for well under one million.
Um, so we know, roughly speaking, what we know an awful lot about neural circuits and how single
neurons behave. We know why neural circuits, uh, biological neural circuits are so energy efficient,
and it basically has to do with, um, sparse activity. Basically a neuron in, in the cortex,
in the human cortex spikes, maybe once every 10 seconds. And most of the energy in a neuron goes to
restoring ion gradients after a spike. So if you can minimize the amount of spiking and still get,
you know, do the computation you need to do, then, uh, you can save tons of energy. Um, another,
another, um, sort of reason is that not only, um, do neurons fire rarely, but they fire stochastically.
In particular, synaptic release is highly stochastic. So something like 95% of the time that an action
potential invades the presynaptic terminal, there's no release of neurotransmitter. And that, that,
that willingness to just throw away, uh, information instead of insisting that every bit that you
generate be passed with high fidelity to wherever you think it needs to go. You're the, the brain's
willingness to operate in this highly stochastic regime, I think is, uh, potentially an even
greater source of, of energy savings, but we don't know how to compute in that regime. We understand
the hardware, but we don't understand how you would build machines that can work in that regime.
Um, okay. So now I'm going to turn to the, the third, um,
yeah, and one more comment about the, the, the efficiency of machines versus brains is that
another, the really big contributors is that the machines that we typically build are, are digital
arranger analog. You know. Yeah. It has two well-spaced values that are very far from each other.
Zero and one are really big voltage gap from each other to give you, you know, noise,
you know you're you're you're resistant to noise but because the two that the one and the zero are
so far from each other it takes a lot of energy to move from zero to one you know analog circuitry
has lots and lots of values packed very very closely together yeah it's a really energy
efficient representation but not very noise friendly so that that's a really interesting
question i guess um so there are certainly circuits that are purely analog like the retina
okay uh the retina until you get to the ganglion cells and i i have every expectation that that
the analog nature of that circuit is a is a huge energy energy savings but the what i find
particularly interesting is that in the cns the central nervous system um energy or i mean uh
information is transmitted in what at first blush seems to be a digital mode right so you generate
natural potential well but i i think it's a hybrid right and i guess i would say that so so you know
the way that a cortical neuron operates is that it gets a bunch of inputs it integrates them in analog
then it either does or does not generate an action potential and that process is um like the propagation
of action potentials is actually extremely reliable uh and digital and then what happens is that at the
other end that's where the stochasticity comes in so there's you know there's an analog process and
then there's a weirdly stochastic digital process you send a zero or one but most of the time you throw
what's that i would add one more process to that which is that even though spikes they're there they
either happen or they don't and that seems like a binary process there's information as far as you
know most people believe being encoded in the time delta between absolutely 100 100 that's analog um
um sure it is analog um but yeah it's a continuous valued signal with noise which in some regimes so
this is something we think about a lot and i'd love to i almost gave that talk but the project isn't quite
um so yeah i think that's a really interesting set of points and so i i i completely agree that
yeah you know there are places where i would i think about the signal as analog there are places
where i think about as digital i think indeed it's useful to think about the timing i think that the
the fact that um spikes um spike timing contains information is you know highly uh overlooked even
though there's like a you know i i when i came of age in computational neuroscience um you know spike
timing versus spike rate was was one of the central questions that people were asking and it uh i would
yeah anyway it's it's a whole other discussion that i i'd love to have offline so um yeah i agree i i
think understanding the sort of trade-off between analog digital and noise and just sort of you know
coming up with a new paradigm for computing with real neuron-like elements is absolutely key
so like i say this is this is one of the projects that i'm very excited about uh i'm doing it in
collaboration with um uh a neuro ai fellow here actually two of them um christian pele and timo winderlich
who together developed an algorithm for doing gradient descent on spiking neural networks called event
prop and so we're sort of pushing that a little bit more anyway let me let me go on but i i point
well taken uh let me go on to this what i would call the learning problem um and this is this is sort
of what i've been thinking a lot about for the last few years um so ai relies uh heavily um relies on
learning so or what what people in artificial neural networks call learning and here's one version of the
one version of the basic paradigm this is super supervised learning you um somebody said something
in the chat um oh okay i'll look that up uh it's just a link to the the work you just mentioned so
people can check oh okay yeah um well then i don't need to look it up i've read it um so yeah ai relies
on learning um so i'm sure you're all familiar with the classic supervised learning paradigm you take
a bunch of images labeled images um and then use gradient descent and you squeeze the information
buried in the pixels into the weights of an artificial neural network um
so one key fact here is that neural networks typically start um tabula rasa blank slates they're
they're initialized to a bunch of um random values and because of that uh they you need a lot of of
examples to train them humans do not rely mostly on unsupervised unsupervised learning labeling objects
um every second for a year would be something like uh ten to the seven training examples which is
really too small a number to get any traction um so that you know people have been aware of this for
literally decades um the usual answer is uh unsupervised learning so you know wonderful quote from
jan the kun if intelligence is the cake the bulk of the cake is unsupervised learning the icing on the
cake is supervised learning and the cherry is reinforcement learning right that that's his vision so most
most of what we do he feels is unsupervised learning which is now rebranded as self-supervised learning
um but i would say even that's not enough so you know when animals are born
uh like a squirrel can jump from tree to tree and after a few weeks of birth um a colt can stand
within seconds of birth a spider can hunt basically at birth right no one teaches a spider how to hunt
no one teaches the spider how to spin a web you've got termite mounds if you take a termite and
a termite egg and stick it in the ground um eventually you'll get a lot of termites and
eventually they'll make a mound no one taught them how to do it they didn't learn that was somehow
built into them birds of a particular species will make uh characteristic um uh nests beavers seem to be
just predisposed to build dams this is a wonderful video i highly recommend downloading this
fun video um about justin beaver the rescue beaver some woman found a beaver pup and brought it home
and raised it in her house and um it did a lot of damage in the house trying to build dams you can see
that it tried to gnaw at her at her doors and grab the lacrosse stick and eventually in the bottom right
it tried to try to build a dam in the bathtub um it's people as far as i know don't really study
beavers in the lab but hopi huckstra uh at harvard has has a research program uh studying um
paramiscus and there are these two species of paramiscus which are kind of mice one of which builds
um a long escape tunnel so a long tunnel with one entrance and a uh an exit pretty far away and the
other one builds a a uh uh uh tunnel with just a burrow that just has a a short little stubby entrance
like that and what's interesting is that if you do cross-processing experiments so if you take a deer
mouse and um pup or and and give it to um old field mouse foster parents the adult fostered deer mouse
will still produce the stubby tunnel and vice versa indicating that its predisposition to build a
particularly particular type of uh burrow is built in hardwired um in some cases this is a pretty
i'm going to skip over this for now so a lot of behavior is largely innate uh and these are just
more examples so learning is overrated that's sort of the summary of this part of the talk shouldn't
focus on learning should focus on um you know the brain comes mostly pre-packaged and with all due
respect to the young who is you know a frenchman who appreciates his food this is really the right
picture right you build your cake out of a package of of betty crocker so uh ai summary so far ai relies
on learning natural intelligence relies on innate strategies and so i'm going to talk for the
remaining few minutes about some work that i've done with um originally with alex koulikoff um
on how you turn innate structure into a wiring diagram so innate behaviors are encoded in our genes
um right this is dna here somehow produces a wiring diagram so we can ask the question how complicated
is the program that generates a brain um you know so we can quantify that the um wiring diagram right
you can just represent it as a list of connections and the complexity of that um of a random matrix of
size w can be quantified in in bits so why don't we go and do that for some bit some
simple organisms um with c elegans well study model organism 302 neurons 7 000 synapses
um it takes about 60 000 bits using the formula on the left which just assumes a um straightforward cost
for uh figure for listing all the connections and you can see that uh those 60 000 bits would fit very
comfortably into the c elegans um genome with room to spare so there's really no no conundrum no
challenge here uh it is easy to understand how the c elegans wiring diagram could be hard-coded in its
genome uh so yes every connection could in principle be explicitly specified in the genome
now what about a human brain here we run into some trouble uh the human genome is 10 to the 9 bits
and the uh connectome under some simplifying assumptions could be maybe of order 10 to the 15 bits
so um you know that comes from basically 10 to the 11 um neurons in the human brain and each neuron
connecting to between one and 10 000 other neurons so if you have to make a file that enable that that
encoded all those possible connections it would be of order 10 to the 15. so even if every single
nucleotide in the human genome were devoted to specifying the wiring diagram there still wouldn't
be enough so you know a missing factor of 10 to the 6 at least what's going on here well is it really a
mystery we we know that this is not really a mystery what it is is that the wiring is not a list of
connections the wiring is specified by a more efficient set of rules and so what i was describing
on the left were the limits of what it would take to specify the wiring diagram of a random network
but if we knew that the wiring diagram were had structure we could write a program that could be far
more efficiently specify the wiring diagram so you know if i wanted to wire up a brain whose uh connectivity
matrix was the one on the right it would be you know three lines of code whatever it'd be connect to
your four nearest neighbors and so the conundrum is that the real brain is somewhere between a particular
random network you know that is uh specified and the particular structured one here and so that has
led us to start thinking hard about like what kinds of rules are used in the cortex thing and can we
compress the wiring diagram uh and and is there any utility to that uh yes so here's a digression on rules
and programs um this is basically a a pointer to the idea of kolmogorov complexity so you could have
some very simple lists like this one right uh two two two two two two two no matter how many twos there
are it's a very simple program for i equals one two whatever a hundred thousand print two end right some
lists are complex seem complex but um are actually quite simple and you know here are the
digits of e and you know i can write down a program so the digits of v never repeat they're not as
simple as the one up there but there's a simple um program that i could use to to generate those digits
um but some lists in fact the vast majority of lists and the vast majority of wiring diagrams
are seem complex and really are and so you know if i give you this particular string of digits
the shortest possible program is just that list itself there's nothing better that you can do
and this whole notion is the notion of komagorov complexity which is defined as the length of the
shortest program in bits that generates a particular sequence so our contention is that the description
length or komagorov complexity of the wiring diagram of actual brains is maybe a lot closer to e
and not so much like um this random list that i had here it's clearly sadly not as in most cases not
you know maybe the retina is a little bit like all twos really easy to understand but we think that
there are rules there that are that encode the wiring diagram um that that may not be obvious but
nonetheless have a shorter description length than just the set of all connections all right so
um here's here's the problem right we that that um we face is that there's a genome here which is
small or at least the number of bits to describe the genome is small compared to the number of bits
bits um uh in the wiring diagram so yeah so let me tell you about the algorithm that we developed here
um so let's say that this is this is work with Alex Kulikov it's um yeah there it is it's published um so
this size a little yeah sorry genomic bottle bottleneck algorithm our goal is to compress the weight matrix
w while still maintaining performance compression will be performed via a genome okay and this was
published last year um the graduate student who just finished up sergey chuvev a um an undergrad
deviantia who is now doing her graduate work at work at georgia and Alex uh kulikov who's my colleague
here at full screen harper so here's the basic setup here's a network that solves um the mnist digit
recognition problem so you give it um a bunch of pixels a digit and you pass it through this um uh
mlp multi-layer perceptron and it is supposed to give the correct label a two or a seven or whatever
so anything any information that we learned about the ensemble of pixels corresponding to digits is
contained in the weight matrix w so here's the basic idea we have a weight matrix w that um solves the
problem and we want to compress that into a different representation g which we'll call the genome
problem such that when we uncompress it over the course of development it produces an approximation w star
that is close to w where close is defined not necessarily as mean square distance between the
individual weight elements but rather in terms of the performance of w star on this data set so this is
the the goal that we set ourselves right our goal is to compress the weight matrix w while still into a
weight matrix w star that's an approximation of w while still maintaining performance uh this is a lossy
compression problem so w star is not exactly w but it's close in some sense uh in fact in the sense of
performance and their compression the compressed version is stored in what we'll call the genome so
uh one way to write this down is that we define a loss function on g that has um two terms one is just that it
contains as much as possible about um uh the original w so this is uh the uh error on um of of w star
on um the the test data before we've trained w star at all and then there's this other term which is the
entropy of g the size of g so we would like the smallest network possible that gives us the best
performance possible and those are sort of two terms that trade off one another um and this was inspired by
tali tishpi's information bottleneck and it's related in some ways to a lot of other approaches
um such as weight pruning but we sort of take it from a different angle um
um okay so how do we do this um so let's let's think about the original network which we'll call
the p network for phenotypic network um it has a bunch of inputs x sub i and a bunch of weights wij
to produce outputs okay so the x sub i here are let's say pixels in mnist um what we're gonna do
is compress that using a genome which is itself an mlp and this genome gets as its input two um terms one
is the label of a particular neuron in the network x sub i in binary okay so if there are imagine there
were eight units here then it would take three bits to specify every unit right so that would be
you know this would be zero one zero and that would be the uh input here and then if there were eight
units here let's say this one is unit one one zero then there would be a binary label here so the input
to this network is a pair of binary labels for a particular element wij and that now represents a large
training set we ask the network uh to learn to predict each one of these elements wij from this uh big w
in the p network so the uh input yeah okay all this i just said that yeah um okay so then what we do
is um we go through a cycle we we start with this um this approximation okay w initial uh which is our
estimate from the decompress network then we train that to good performance and then we compress it and
we keep doing this going around around and around until we find networks that are both compressible
and performant and the key point there is that there are many many networks that potentially solve
any particular problems such as mness that's the nature of over parameterized artificial neural networks
and um what we what our algorithm does is it finds the one of the subset of those networks
that not only solves the problem but can actually be compressed by this algorithm so we have to go
around this cycle uh a bunch of times in order to find one of these these networks so one way to think
about this is it's um uh it's not real evolution it's lamarckian evolution which by the way is a much
better algorithm i believe than darwinian evolution it just can't be biologically implemented so we've gone
ahead and implemented lamarckian evolution uh and i would say that just as a side note um darwinian
evolution is incredibly inefficient and it only works because it's uh acted on something like i once
did a back of the envelope calculation that says that there have been maybe 10 to the 35 individuals
individuals individual animals that have given their lives so that our genetic sequence could be as
clever as as it is and we're giving our lives uh to improve the genetic sequences of our descendants
uh okay so yeah it works um if we train um mnist on a particular network uh if we train mnist the
standard way we get performance of about 0.95 we can compress it down by a factor of over 300 and lose
almost no performance we can compress it down by a factor of a thousand even and lose only a little bit
of performance so this approach works with mnist it also works with um cfar pen which is a somewhat
more challenging task we can get something like a hundred fold um compression um and uh this is a
complicated slide that just goes to say that one outcome of this is that for cfar and interestingly
for cfar but not mnist it actually enables transfer learning and that basically suggests that it is it is
extracted from the um the weight something those things that are important for performance uh so
basically it's applied sort of occam's razor and discovered what's potentially important so it can
generalize to other tasks such as um a variant of cfar um and i'll also say that we've applied it in a fun
setting which is reinforcement learning and not only do we get good compression but we get really good
transfer learning here in fact we get something that i would say goes beyond transfer learning we get
networks that are initialized so that they can learn which is actually quite a challenge so this is a
simulated physics environment called mujoko where um it's a physics engine and it you've got sort of
approximations of um friction and and torque and whatnot you specify the um forces applied to all
the joints on these and if you just try to take a standard agent like the one shown here it ends up
learning very bad policies that don't seem reasonable for walking um not shown here but if we um
um initialize um one of our networks and then train it you see that it it it actually immediately knows
to walk and i guess it's not shown here but uh with further training it learns to walk and run better
so this suggests that uh it's not just that they learn faster but that they learn the right thing
uh so let me just give a few closing thoughts um we need to go beyond the turing test to an embodied
turing test to sort of address this uh idea that i was raising about morvex paradox um and our goal
would be to achieve mouse level ai because if we could achieve mouse level ai we'd be 99 of the way
there um so why are humans so successful here's a little speculation is that learning well no you you'll
not be surprised that i don't think that it's because we're so good at learning um i think humans are
particularly good at learning but maybe not any better than other apes maybe not better than other
animals like crows and a couple other animals um i don't think that's the key i think that um and i
think part of the evidence for that is that humans modern humans have been around for over a hundred
thousand years and for most of our time on the earth we were kind of um a footnote like if you
if you took a survey of life on the planet 50 000 years ago uh you would not be impressed by by humans
they were our overall population size was a i think a couple hundred thousand in fact we'd almost died out
70 000 years ago we went through a population bottleneck um we only started sort of dominating
the earth as we like to say recently like in the last 10 000 years and so what what happened um and i
think that the answer uh is not that we're better at learning not that we're smarter the one thing that
we have though is that uh we can accumulate information over generations through cultural evolution
and that's enabled by language so language was the big insight was the big advance um that enabled
oral transmission and then was accelerated once we got written transmission and cultural evolution
breaks the genomic bottleneck um but even so all human behavior uh builds on 500 million years of animal
behavior so here's the summary animal ai relies mainly on learning animals rely on innate structure
uh innate structure derives from the genome which gets converted into a wiring diagram and wiring is
compressed through a genomic bottleneck and here's my collaboratome um for this project uh really what
i talked about was work done in collaboration with alex gulikov sergey and deviantia um and i'd like
to thank my fundasome as well so thank you for your attention
