Hello, everyone. Welcome to Bold Conjectures with Parish Chopra. Today, I'm with Conor Lee,
CEO of Conjecture AI, where they're trying to build controllable AI systems that don't cause
the doom of humanity. He was ex-head of Illyutia AI, a non-profit research lab that gave us
wonderful open source models like GPT-J. Conor is known for his views on the massive risk AI
systems pose to humanity. In fact, Conor has been on this podcast before. Three years back when GPT-3
was super new, we had a conversation about how it is a fire alarm and we need to act before it's too
late. And three years, a lot of time in the world of AI. Now we have GPT-4, Anthropic, Opus, and the
progress just seems to be accelerating. So what does it all mean? Let's take an update from Conor.
So welcome again to the podcast, Conor. Yeah, thank you so much for having me back. It's been a while.
Yeah, so we talked on this podcast three years back. How has the world changed in terms of AI risk
ever since then? Well, unfortunately, I must report definitely not for the better. So to my great
chagrin, it seems that many of my predictions turned out to be not only correct, but too
optimistic. I think things went even worse than I expected them to go. I was a bit naive three years
ago about some of the economic and political realities. I think we're going to talk about
some of those a bit later in the podcast as well through a kind of a different lens.
Yeah. Is that if I could summarize the state of the world right now, it would be no one has a plan.
There are no adults in the room. No one knows what they're doing. And we're just accelerating as fast
as possible towards doom, straight on, no breaks, no lube. That's how I would describe our current state.
Yeah. I mean, you sound much more worried than you were three years back when I didn't
sense this kind of worry, but it seems like a very strong statement.
I mean, it's exactly what's happening, right? We have for-profit corporations building smarter and
smarter agents with zero oversight, server input, zero democratic oversight in any way. They all admit
themselves even that they do not know how to control their systems. They do not know how they
work. They do not know what they're capable of until they build them. There's no plan on the
government side. There's no plan on the economic side of how to deal with anything from, you know,
even prosaic arms, you know, like we have like deep fake massive, you know, disinformation and like
identity theft and so on. No one has a plan for that. That works. There is massive job automation looming
on the horizon. No one has a plan. How we deal with that. There's a usual thing where people are like,
you know, you bring up job automation and people are like, Oh, don't worry. You know,
if we work less, it'll be great because then we can just relax and do art. And I'm like, okay,
who pays for that? Who grows your food? By default, all the resources will either go to the owners of
the AIs or the AIs themselves. They're not going to go to the general population unless you have some
kind of, you know, massive government, you know, backed redistribution and which government is
currently working on doing that. I don't see anyone currently working on that. And then the existential
risks that, you know, we're building systems that are, you know, every single year that are like,
massively smarter three years ago at GPT three, you know, a thing that's like, you know, could write
some cute little kind of incoherent, you know, paragraphs or whatever. Now people get upset that,
oh, my cloud opens, uh, it, it, it, it didn't quite finish my, you know, deployment of my NCCL,
you know, distributed programming bug in one shot. Oh, wow. So interesting. It's, it's like,
I'll have to be frank here. It's like, it's a joke. Like, like, you know, just this morning,
I think there's also this announcement of this new robotic startup using like AI models,
like humanoid robotics and so on. Like, it's not even, it's like, not like at this point,
I'm just kind of like, I stopped being even fun of it. Um, there's a, there's a great,
so there's a fable when I think it was his office fable of the frog and scorpion. And the way the
story goes is a frog wants to cross the river. There's a scorpion who also wants to cross the river.
And the scorpion asked the frog if the frog could bring him across the river. And the frog says,
no, you're a scorpion. You're going to sting me. And then the scorpion says, no, that doesn't make
any sense. If I stung you, well, then we would both, you know, drown. So I've not, I'm not going
to do that. So the frog thinks, okay, yeah, it makes sense to me. So frog takes a scorpion,
they go halfway across the river and scorpion stings them. And the frog is like,
why would you do this? Now we're both going to drown. And the scorpion said,
lol, lmao. I think that's how the story goes.
Yeah. I think it's in the nature of scorpion to do so. Uh, and in fact, that whole thing is just so,
so much in parallel to what's happening, wherein people just can't stop themselves from just
eliminating them. It's kind of funny, like that, the Devon model that came across, um,
and nobody would have guessed five to 10 years ago, software engineers will be the first to
eliminate their job. Why would you do that? Uh, it just doesn't make any sense to me.
Uh, before we dive into that corner, I wanted to, uh, uh, ensure we're on the same page for a number
of audiences who have not a lot of background. How would you convince them that AI actually poses
a grave risk and what risks would you broadly classify as?
If you build systems that are smarter than all humans, better at politics, business, science,
and everything else, and we don't know how to control them, which we don't currently know how
to do, what do you think happens? That's it. Like, just think about it. What happens?
Um, in that sense, I wanted to go into this question a little later, but since you mentioned it,
there are parallels between how people react to, uh, climate change and the risk with climate change.
And this, uh, do you notice them too? I mean, both are a little bit abstract risks. Uh, you can't
feel them in your gut. Um, but has there been like, uh, discussion around this? Is there similarity?
Of course there's many similarities. They're both large catastrophic risks that are pushing against
a very strong incentive gradient. There's a lot of vested interests that benefit from not addressing
climate change the same way that many interested interests were interested in not addressing, uh,
AI risk. If it was in everyone's interest to address these risks, they were legible, understandable,
cheap to solve, but then we would solve them. I mean, hopefully, um, the climate change is a good
example of a risk, which through very, very hard fighting after many, many decades is like start,
you know, kind of now at a position where it has like pretty good global consensus to various degrees.
But even then, like, are we on top of climate change? Do you feel like our governments have
this under control and we don't have to worry? I mean, not really. So in a sense, climate change is
an easier problem than AI risk. Not that it's an easy problem by any means, but it's a problem which
we have global consensus on is goes over decades before the, you know, worst harm. They're already in
harms today, but the worst harms we still have a couple of years or decades. And even then we're
not on the ball. Now take a problem, which is, uh, a hundred times, a thousand times faster,
you know, that unfolds over years, not over decades. That is even harder, you know, the lies and even
more abstract, weird, you know, technical things that are not quite so understandable. Even so, I kind
of disagree that the AI safety problem is that abstract. I think a lot of people present it as this
weird, esoteric thing. Well, I think it's actually not that weird and esoteric. It's just like,
what do you think happens? You know, you make things that are super smart and that want to make
profit and, you know, whatever. What do you think happens if you don't do this properly? Like, like,
of course, you know, if you have chimps and they create homo sapiens and like, what's the worst that
could happen? Like the worst that could happen is pretty bad. Like, of course, um, so yeah.
Yeah. On that, I mean, I notice sometimes people have this, um, baseless optimism because
when we talk about job loss through AI, some people say that that's always happened. People
always find a new job. Similarly with climate change. I, I know some of my friends have faith
in technology that will end up. We've always solved problems. Uh, what do you have to say
about that? I'm, I'm also asking in terms of at two levels, one at a practical level,
but also at the level of human nature, have you thought about it? Where does this come from?
This naive faith and optimism that we'll just figure things out. So there's nothing to worry
about. Oh, of course. Like, this is a very common reaction, especially among like tech people and
stuff like that. Um, it's always much nicer to just pretend the problem doesn't exist or it's
someone else's problem. Like, of course, like, I think this is kind of trivially true. It's a kind of
like one of the most obvious COVID mechanisms. Just say, don't worry, it'll be okay. Like,
it's the obvious psychological defense mechanism. And it's also one of the obvious, um, responsibility
deflection mechanisms where, um, you don't want to be responsible for this, right? You want to do
your thing. You want to make money. You want to, I don't know, build your family, whatever you want
to do. Right. Um, and now if someone comes to you and they're like, Hey, there's actually this
monumentally large problem, then that's kind of also like asking for your time. And then you'll be
defensive about this. And I understand, and I think this is a good heuristic. So generally,
if you see someone come to you and say like, here's a monumentally massive thing, you have to
dedicate all your resources to it right now, you probably shouldn't listen to them. This is a
generally good practice. Um, this person is, you know, unlike unlikely to be very calibrated unless
there's more to it. So if this was the only thing we have, it's just like some guy is saying,
this is the case, but there's no evidence. There's no consensus. There's no first principles
thinking there's nothing. If there was nothing, then yeah, you shouldn't take this seriously.
But like, this is just not the case, like laughably. So like, look, look around you, bro.
Like a lot of my, um, so when I look at these, like optimistic things, a lot of it, I think boils
down to the quote, I think it was often Sinclair who said, um, it is very, it is very hard to get a
man to understand something when his salary depends on him, not understanding it. So realizing the,
the fact that you work at an oil company and that you don't think climate change is that serious are
not independent variables. This is not how humans work. Humans are not perfectly rational beings
that, you know, evaluate evidence completely disconnected from the social context. If you
work at a soil job, there's both, there's two fat, at least two factors here. The first factor is
you're unlike, if you're working there, you don't want to feel like a bad person,
you know, like, you know, oh, you know, your colleagues, they're all nice people. You know,
I'm sure the people who you work with, you and your oil company, I'm sure they're wonderful people
and you like your colleagues and so on, or maybe you don't, whatever, but like, you're a good person,
right? So now this guy telling you that actually maybe you're a bad person because you're doing a bad
thing. Well, nah, he must be wrong. And the second factor is you don't get hired for a job like this.
If you think climate change is a big problem, if you think climate change is destroying the planet,
you don't, you don't accept, but also you don't get offered a job as an oil exec. So the fact that
oil execs are unusually laissez-faire about this is not an independent variable. This is not like,
these things are deeply entangled. I think with tech, it's the same thing. The fact that a bunch
of engineers at OpenAI are pretty chill about the AGI risk thing is not a coincidence. It's not like,
there's a selective pressure towards this kind of thing. So ultimately you can't just always outsource.
Just think about what happens. Like just sit down for five minutes, say we succeed at building these
things. We can argue about, you know, will we build these things? When will they be built? Sure,
we can argue about all that later, but just assume we succeeded building these things, which a lot of
experts think we will in the next couple of years, and a lot of experts agree that we don't know how
to control them. Then what happens? True. So tech is a very, I mean, AI researchers,
that community is very interesting in the sense that some very smart people deny the risk. So on one
hand, we have someone like Jeffrey Hinton, who quit his job in Google to raise alarms about AI. But on the
other hand, we have someone like Yakun, who's head of AI at Facebook, who actively denies there is a
risk. And both are presumably smart at top of the game in terms of their research capabilities. So how
do you explain this dissonant view and completely polar polarizing view from the very similar kind of
people? I think this is a very important thing to understand. Smartness isn't everything. Just because
someone is smart doesn't mean they're right. It doesn't get someone smart doesn't mean they're
telling the truth. Just because they're smart doesn't mean they're rational. It's actually
there's a lot of there's been some funny studies from back in the day where people with very high
IQ actually were less rational because they were better at coming up with rationalizations of things
they wanted to believe that were so smart that no one could debunk them. There's a thing I kind of
quite often when I'm talking to really smart people is they will believe whatever justifies
whatever they want to do and they will justify it using super complex, super brilliant arguments
that are really hard to untangle. And so you can use your own intelligence to troll yourself.
And this is actually quite common that people will use their intelligence to troll themselves.
Rationality is a skill. It's actually hard to
evaluate. And it's an important thing to understand about rationality, about making good decisions,
about seeing the world for what it is, is that it's not a purely intellectual skill. So much of what
it means to actually evaluate systems objectively is emotional. It's about knowing your own emotions,
your own biases, your own how you relate to these things, how you can control and how you can select
for these things. And it's also a deeply social process. Science and rationality are deeply social
things. There is no way. Can we go deeper into it? I'm very curious about your definition of
rationality, especially also how does emotion and social skills come into the play?
So it might be useful for me to introduce the ontology I kind of use when I think about this
kind of thing is when I think about a mind, I like to split it into four things. This is just my personal
way of thinking about it, not saying this is the one true way. I just find it a useful way to think
about intelligence or a mind. I split into four parts. The first part is its world model or its
knowledge. So this is what it knows about the world, what questions it could answer, what things it could
simulate, what knowledge it has collected, et cetera, et cetera. The second thing is its epistemology.
This is how it updates its world model. What procedures does it do? Given a new piece of
information, what updates does it apply to the world model? How does it reason about this?
The third thing is the decision theory or rationality. Decision theory is given a world state,
a piece of information and certain things you want to accomplish, how do you decide what to do?
And the fourth is your values or your aesthetic. And these are things that are completely disconnected
from everything else. They're just what you like and it can be anything. So in humans,
they usually cluster around, you know, fun, happiness, you know, family, friends, et cetera,
or, you know, sex, drugs, whatever. But it can theoretically be anything, you know, some people
like classical music, some people like heavy metal. That's what it is. So the values are what you spend
resources on. And the other things are about gaining resources to spend on your values, whatever your values
are. So rationality is what I'm calling decision theory here is if the more rational they are,
the better you are at not making decisions, which are predictably stupid. So let's say I give you two
options. Either I give you an apple or you give, I give you an apple and you give me $2. The rational
choice hypothetically is you pick the apple because why would you give me $2? You don't have to give me $2.
Like there's no reason to do that. So, um, this is a very technical definition of rationality. Um,
so what I'm not talking about is like, you know, commander Spock, you know, like this is the like
straw man version of rationality, you know, uh, who talks really monotone and, you know, says some
stupid numbers or whatever. That's not what I'm talking about. For me, rationality is much closer to
not being stupid than it is to being smart. Being smart is far more about like your world model,
your epistemologist, one being rational is just about not being predictably stupid.
Like if you don't know a piece of information, like maybe if you take the apple, they'll give
me $2, you know, I'll, you know, shoot you afterwards, but I can tell you about that or
something, or I'll poison the apple or something. If you didn't know about that, that's not a mistake
per se, because you didn't know about it, but if you didn't know about it and you'd make the
decision anyways, well, then you're irrational to some degree important. It's this things get more
complicated once it gets into social things. So unfortunately, once you deal with social
scenarios, things get even more complicated because now irrationality often becomes a good strategy
because you're the, the actions of other people depend on your actions and your mind. So
if you perceive me as someone who is fanatically loyal to you, you know, just completely delusionally
loyal to you, do anything you want, whatever, even if it doesn't make sense for me,
well, you might promote me to, you know, high commander, you know, it seems like a good right
hand man for me, the dictator or whatever. So if you want to be the right hand man, the dictator,
it can be useful to damage your own, your own rationality, your own epistemology and decision
theory in order to rise the ranks as an example. So things get very messy, very suddenly when you're
dealing with social situations where other people are thinking about your mind and like who you are as a
person, this is very hard to reason about. There's no easy way to do it. There's no simple trick. Oh,
here's how you solve all social problems and we'll go. It's hard. Right. Um, is it fair to say, um,
like, I mean, ultimately we're bundles of evolved biases and pleasing a local group who we feel affiliated
to as one of those biases. And I, so I dislike the bias framing personally. I think biases are a thing,
but I think they are vastly overhyped, especially in like online high IQ, autistic nerd circles.
A lot of biases quote unquote exist for actually very good reasons. Like wanting to please your tribe
might be a bias, but it's there for a good reason. It's because in fact, often pleasing your tribe in
many situations is a good strategy. It might involve you compromising and other things such
as compromising on your epistemology, on your rationality in various ways. But in a sense,
this can be a good strategy. It can also not be of course. And so biases are real. I'm just saying it's
actually tricky. Like if there's no, aha, I have a list of biases and I simply won't do those.
Unfortunately, it's not that simple. Okay. So getting back to the question of, let's say,
very smart people doing polar opposite things. Let's say Jeffrey Hinton and Yakun.
Am I understanding correct that they are applying their rationality towards different goals and
ends, or maybe they hold different values, even though they might be equally smart?
Yeah. And they can also just have, be less rational or more rational. I mean, look,
if I told you CEO of, you know, general oil says climate change, no big deal is your update. Wow.
The CEO of general oil, he's pretty smart. So maybe, maybe this whole climate change isn't real or is
your response? Well, duh, he's the CEO of general oil. Like what? Like, duh, I'm sure he's smart and he's
using his smarts to prevent us from lowering his stock price.
Yeah. But again, I mean, I say, let's say Jeffrey Hinton would have had a very high paying job at
Google and prestigious job also. There was something in his character to quit it
and start going around talking about this AI risk. So obviously it's very possible to explain
partially people's behavior, but there's something else that's driving some of the people on the
frontier and other things. Humans are not simple. Like this is like, um, incentives,
you know, people do have personalities. It's very fashionable to think of, you know, humans as purely
products of their environment, blank slate, you know, just incentive gradients and whatever.
But this isn't actually true. It is to a large degree, to a larger degree than many people think.
And there are some people who are fully explainable by their environment. Like there are people who,
you know, I have in the past said unkind things about, um, uh, revolving corpses, um, and they are
fully explainable by their environment. And now maybe it feels like them internally. They have some kind
of personality, they have some core, they whatever, and, and they might, right. But I don't know what's in
other people's brains. Like, I don't know what's inside your soul. I don't know what's in your mind.
How could I? So the question is far more, can I predict your behavior from a given set of variables?
And for some people, the set of variables you need to predict them includes a lot of internal state and
their family history and whatever. For other people, nah, you just look at their job title
in this predictor behavior. Okay. Got it. So I'm going to ask you to dissect the long reply
you made to Rune recently. Uh, uh, I think it was fascinating. I've read it a number of times.
Um, I think he said something like this, it's okay to watch and wonder about the dance of gods
and the clash of Titan, but it's not good to fret about the outcome. Political culture encourages us
to think that generalized anxiety is equivalent to civic duty. So can you break this down and maybe also
explain who Rune is and what's your interpretation of this tweet was and generally what you make of it?
Yeah. So I'd like to start with the content warning. Um, so, so far we've been, um, engaging
on object level questions. We've been talking about things in very clear, modern, rational terms. Um,
you know, whether you agree with me or not, dear listener, I've been trying to communicate things
that are like actual claims about actual things, or at least are about my opinions. Um, where we're
about to go is kind of a bit of a different territory, a different kind of epistemological theory.
So Rune is a Twitter, um, user who works at open AI and he has a great sense of humor. He's a really
great writer. He writes really funny, weird, quippy, crazy stuff. Um, he's a really interesting guy.
Like I like reading his stuff a lot. I think he has a really fun personality. I think he's very smart
and, uh, says a lot of funny things. Um, and he also uses a lot of mystical language and mystical
thinking and so on in his brand of humor and thinking, which I think is, is fun and cool and
interesting. And so recently Rune wrote the thing that Paras just, um, read out. And this kind of
sparked me to wanting to reply to this. And as you can see, it's written, you know,
the dance of gods and so on in a mystical language. So the, what we're going to go now
is into what I would call mystical language or mysticism. So if you don't mind Paras,
I'd like to take like just like five or 10 minutes to maybe talk about what is mysticism
if my perspective, would that be, would that be great?
Great. So, um, so my response to this tweet, which you're going to get to in a second is a,
is very dripping with a certain in non-rational language. It's less about science and biases
and so on and far more about spirits and gods and magic and so on. And in our modern culture,
it's usually, these things are very frowned upon or see it as old superstitions from a previous era,
an unenlightened era before we understood, you know, the modern incentive method. And
to first approximation, that's correct. To second approximation, that's not quite correct.
Um, so when I talk about mysticism, what I really mean is metaphors, the using of, um,
um, irrational language. The point is to eliminate a concept, to enumerate a concept. I'm like, okay,
I want to make this claim. I make this claim and maybe I provide some evidence or whatever. And then
we combine these, these various pieces using logical rules to get to some kind of conclusion.
Of course, in practice, it's never that perfect, but hypothetically, if I was in an academic
context, I'm supposed to make arguments that are logically coherent, that follow a certain structure,
that don't contradict themselves. And that result, you know, that follow to some conclusion
that we can evaluate and think about. Mysticism for me is loosening those constructs.
It's loose. It's going from, you know, like the most extreme form of rationality is like formal
proofs, formal logic, like mathematics. Then there's the intermediate kind of like, you know, science,
uh, academia, which is like, not quite as rigorous as mathematics. It's actually a lot less rigorous
than people think it is, but there's still rigor there. Like you're still trying to use rational,
reasonable, logical thought. And then there's a spectrum that goes even further towards the
mystical where it becomes more about metaphors. It becomes more about vibes, feelings, emotions,
stories. It no longer becomes about statements so much as stories. So when, when in a fable, you know,
you talk about a frog and a scorpion, you don't literally mean there was literally a frog literally
at a river with literally a scorpion, and they were literally talking. This is obviously not what you
mean. So, um, so mystic language is more like fiction writing and fiction, um, is cape is actually
very capable at discussing and pointing at useful concepts that are very, very, very hard to talk about
in formal language, especially emotions, relationships between people, how people think
about themselves, um, and stuff like this. It's quite hard to elucidate, you know,
fervor is love or hate or whatever using just a math proof. It's possible as many mathematicians
will tell you, but it's not easy and it's not legible to most people because the natural way for
humans to communicate is actually mystical. It's actually normal for people to communicate through
religious metaphors and storytelling more so than modern rational thought. This is actually a
relatively recent invention. It, it, the oldest civilization, like artifacts of civilization,
we know like, um, Gobekli Tepe and, um, Turkey is a temple. It's not a granary. It's not a, it's not a
thing. It's a temple, like humanity started with religion. And my, I think the reason this is still
useful is that there are just some things that have not been digested yet. Like for me, rationality
is the most digested, powerful, refined version of mystic thought. It's so powerful. It's so refined
that there's no ambiguity. Hypothetically, we know I say a thing and like, we're like, yep. Okay,
fully understood. There's no misunderstanding hypothetically, et cetera. And so mystic thought
allows you to, and mystic language allows you to talk about more things, but things become more
open to confusion, misinterpretation, wishful thinking, et cetera. So it's a trade-off. So where
we're going now is talking about things which don't have good rational, simple, like compact words.
I think they can, I think, you know, after years and decades of digesting this stuff,
of coming up with new ways of talking about this, better theories of psychology, better theories of
sociology and so on, we could develop rational methods for talking about these things,
but we don't really have that right now. So I result to metaphors. So for the rational,
you know, audience here, don't take the things I, you know, I said to rune or the rune says to me
as literally true, try to think of them as metaphors and think of them, what they might
be metaphors or what are they pointing to in your concept space? So right over, I just want to
have that little disclaimer here is that I understand that this is not literally true.
I'm not retarded. I'm not, I'm not crazy. I'm not schizophrenic. Like I understand that
there aren't literally spirits and there's not literally magic.
Duh, everything is mechanistic, but it can be useful to use this language to talk about these kinds of
things. Yeah. So, I mean, all this, I mean, this of course makes it so much more interesting because
there's just some concepts that you simply can't explain efficiently, unless you end up using
metaphors. So concepts like God, underworld, spirits, a lot of things, I think they take on this
character, which is just super hard to communicate. And maybe it'll diffuse the whole meaning if you try to
put them into literal, rational words.
Yes, it can be very difficult. I think it's possible, but it's hard. So like, it's often useful to use
these like mystical languages, say like underworld, spirits, God, and whatever, to conjure a set of
properties, like a big, you know, multi-dimensional embedding of a bunch of stuff. And then it can be
very helpful to combine these concepts in different ways to create intersections, to narrow in on like,
look, I'm pointing at something. And it can be frustrating because people might say,
well, you're not literally pointing at something. And like, yes, I'm pointing in a direction,
look there. So like a lot of mystical language is not about, here's the solution, write it down. It's,
hey, you see this direction I'm pointing at, there's something there, keep looking. I can't,
I would love to show it to you precisely, but I can't. So you have to go looking yourself.
So a lot of what I'm trying to do here is not to say, here's literally solution that applies
literally to you in your literal life. It's more like, Hey, if this, here's a, I'm going to point
out a little teeny corner of your brain that you may have not considered before, start poking around
there and see what you find. So this is how I think about this. So to return to the room to it.
Yeah. So what is the dance of gods he's talking about? And why does he recommend not
fretting about the outcome and what outcome possibly it must be meaning by this?
So I of course don't know exactly what Brune thinks in his head, but my interpretation of
what Brune is saying, I hope he thinks I'm not misinterpreting him here. The dance of gods is
the idea that there are structures. There are things that are larger than individual humans
that are the primary drivers of what happens in our world. In a physical sense, this is nonsense.
You know, of course, everything's made out of people to some degree, you know, good people and
their machines and their livestock. But if you rephrase this even slightly, it actually becomes
pretty obvious what these gods are. They're stuff like nations, corporations, you know, is where,
where is Google? Where is Google located? Where's the physical? Sure. You can point to their office,
but it's not Google. When Google sues the EU, who is Google and who is the EU? Sure. They have lawyers.
You can point to the lawyers, but they're not the EU. So who is this EU and what's he made out of?
And so these are what, you know, interchangeably can be called egregores, gods, emergent, again,
agentic structure, whatever you want to call it. Basically, there are ways in which you can view
reality where it is more, it is useful to speak about groups or emergent gods as themselves agentic
entities. One of my favorite examples, which is like a bit mystical tinged of this, is from,
I think it was one of Yuval Harari's books, where he talks about how the Egyptians believed
in the god Sobek, the crocodile god, that he controlled the flooding of the Nile. Now, of course,
in some sense, this is not literally true, but I forgot which century and which pharaoh it was,
but one of the pharaohs basically got, who was, you know, the story went that he was the
incarnation of Sobek and he collected, you know, this massive amount of workers who over like 40
years built this incredibly complicated, huge dam system, which allowed them to control the flooding.
So in a sense, who controls the flooding of the Nile now? The answer, Sobek is not insane. It's not
insane to say that Sobek does in fact control the flooding of the Nile in this situation. Now,
Sobek is not a physical thing. There's not actually a guy with a crocodile head who is like waving his
hand and making the Nile go up and down. But in a non-trivial sense, if you told someone, if you
went to this land and you asked, who is controlling the Nile? And they say, Sobek, this is a useful
answer. This is a sensible way to make sense of this. Similarly, when, you know, you say,
who developed GPT-4? And you answer OpenAI, this is a useful answer to me. Even so, OpenAI is not a guy
somewhere on a computer who developed GPT-4. It's still useful to say, you know, OpenAI.
So we can think of corporations, nations, groups of peoples as gods. This is the closest concept
we have from our ancestral environment. The same way we would call Sobek a god, we can call OpenAI
a god or the United States of America a god and so on. And as we know from our histories and our
mythologies, gods are very fickle and strange creatures who don't really, they have a lot of
human traits, but never quite. They're always odd. They have weird demands and they act sometimes in
bizarre ways. Sometimes they're extremely good and sometimes they're extremely evil. Sometimes they
just do shit that doesn't make any goddamn sense. And this is points to a real phenomenon and how
bureaucracies groups can act in ways that are very bizarre, like no normal person would act this way,
but they can also accomplish great things. They have great power. They can cause incredible things
to happen in the world that an individual human can't cause, at least not without becoming a god
or a part of a god. So another way to think about what a god is, is that you could kind of think about
human minds as computers that are running software. If you think of gods as computer programs that are
running distributed on many human minds, shards of gods run on many minds, a shard of the, you know,
the United States government runs in my mind, you know, because I believe in them to some degree,
and I act in accordance to them. A shard of, you know, my company conjecture runs in my mind,
because I act in the, in the name of this god, this corporation, this entity, and so on.
So you can think of them as distributed computer programs that are running on society. And so when
he talks, when Rune talks about the dance of the gods, he's talking about how these computer
programs are interacting, how these groups, these entities are fighting or allying or mutating or
otherwise doing things. And if you look at the news and you like, listen with this in mind,
this framing in mind, you will hear that they, to a massive degree, talk about gods, not about people.
They talk about, and often we talk about people. They only talk to people as avatars of gods,
you know, Joe Biden, the president of the United States of America, the avatar of the god of the United
States and so on. So massive amounts of global events are basically fully in the hands of gods.
They're not in the hands of people. Now people are parts of gods and people can affect gods,
but they're, it's useful to think of them as two different things, as two different layers of
abstraction, so to speak. Does that make sense? Yes. Um, this relates to, I mean, a couple of points.
One is that with gods, uh, I think an automatically a sense of being powerless, uh, comes, that's a
connotation that the word God implies. And I think that relates to what you said, that SF is where you
go if you want to sell every last crap of your mind, body, and soul, which is related to giving up
agency in front of gods. Um, is that what you meant by that? We shouldn't necessarily feel, uh,
powerless in front of gods that we can impact how they behave, what they behave.
So gods are made of people, or at least they run on people. And the way a god is shaped and formed
depends on the people it is running on and the way that people interact with it. There is a kind of
helplessness here where some gods are extremely powerful and many of them are predatory. Um,
this is so in Dawking's book, the selfish gene introduces cause of the selfish gene.
We talked about that through evolution, um, the actual selfish bit is a, is a piece of DNA
is that DNA is trying to get itself copied. Now, this is not because DNA has a mind. It's because
if you're a piece of DNA, that if it's in an organism causes more of that organism to exist,
well, then there's going to be more of you. So if you look into the world and look at the DNA in the
world, you should expect to find DNA that is good at copying itself. There's not a value judgment.
It's just a thing that happens. And Dawkins in the same book also introduced this concept of a meme,
which is, you know, nowadays is a word for a joke on the internet, but what it mean originally is the,
is the, like the psychological or informational equivalent of a gene. It's a piece of information
that copies itself, a piece of information that will keep itself, you know, and it kind of makes
sense, right? If you are introduced to a new culture, you've never been before, you don't
know anything about them. And you look at what art is the most well-known, it's probably going to be
good. You know, like on average, if you sample the art randomly, some art will be vastly overrepresented,
either because it has some historical consequence or because it's really easy to remember,
or it's very cheap, or it's very cool. It's very fun. It's very good, whatever.
There will be some reason that this is the most widely spread art. It's not random. It's not like
art is like evenly uniformly distributed, that every art is equally well represented. This is obviously
not how it works. And it makes sense because there's competition, but like in nature, you know,
nature, right into the claw, the best strategies for application are necessarily nice. It's not
necessarily being the best at something. It can also mean being the most predatory. So
if you have, for an example of this is evangelicalism or evangelical religions in general, not
specifically Christianity, but just any form of Christian, any form of religion, which is,
which has a very strong component of spreading itself.
Right. If you have a religion, which says we're the best religion and maybe it is,
but don't tell anyone about it. What happens to this, to this religion in one generation?
Yeah. Well, it's gone. It could have been as great as it wants. If you didn't tell anyone,
it doesn't matter. Now let's say you have a religion. Let's say it's a pretty shit religion.
It's like really bad. It hurts people, whatever, but it fervently like forces people, you know,
at gunpoint to spread it to as many people as possible. Well, that one might stick around.
Yeah. Makes sense. And so obviously there's a huge trade-off between all these different things.
Um, but yeah, so it's very tempting as an individual, you see you're, you're kind of at sea,
you're kind of like, there's all these like gods vying for your brain space. There's all these entities.
So sometimes I use words like psychic or spiritual to talk about what really what I mean is mimetic
is that you are being constantly, everything is trying to get your attention because the things
that get your attention are the things that survive. You know, those are the things that will be produced.
If some meme can convince you to convince your friends of it, well, that meme will stick around. So
there is an evolutionary arms race the same way that we have an evolutionary arm race with like
viruses and bacteria is that of course viruses and bacteria, they want to, they want to use all
our nice cells to reproduce. Similarly memes want to use our nice brains in order to reproduce. So we
are constantly in an evolutionary arms race against these things. They're trying to take control of us.
They're trying to make these things. This is a small caveat here. I mean, these memes or gods,
they obviously have to be at least somewhat beneficial for you to give your attention,
at least in the short term. So you'll only adopt something even if a religion was really,
really bad in the long term. But you would only pay heed to a god if that god offers you some solace,
some benefit or lures you into something that you find beneficial. Otherwise, you know,
you wouldn't give that god your mind space. Hypothetically, yes. But in practice, man,
do I spend a lot of time on Twitter. So this is correct. You are correct in general. The problem
is, is that the mind is not perfectly shielded. Like we don't have also like the, like the mind is not
some, like our immune systems are themselves mimetic. And, um, it's the same thing is like,
well, we would never allow a virus into our cells if it was not beneficial. So the viruses must give
us some kind of benefit. Well, the answer is no. Sometimes your immune system just fucks up.
Like sometimes you just make a mistake. It can be a useful way to get past your mimetic immune system
to offer you a short term benefit. There's a very common strategy, especially in memes to do this,
but it's not necessary. And there it's not necessary. There are ways to infiltrate your
mind without giving you anything beneficial. This is possible, not saying it's the best strategy or
it's the most common, but it is possible. I mean, does any example come into your mind
where in something which is, I can think of many examples. Yeah. Uh, here, here's a, here's a really
stark one. Uh, anorexia. So anorexia can be spread mimetically. Um, there's a lot of studies on this
where if young girls are exposed to people talking about anorexia or showing anorexia,
they can develop anorexia at much, much larger levels than they would otherwise. There's, you
know, like for example, I was like South Korea and Japan, the word, like the concept of anorexia
didn't really exist. And then psychologists start to introduce these terms and suddenly the wreck is
shot up. And now some people will say, well, anorexia always existed, but wasn't diagnosed,
but this doesn't match up because anorexia in particular is so bad that people often die or have
to go to the hospital because of malnutrition. So you can't hide it.
So this is a very specific example of a very virulent medic virus, which gives no benefit.
It just makes them miserable. It's a horrible experience. I don't know if you've had anyone,
your family or someone anorexia, it's awful. It's a terrible experience for people. They
know benefit from it. It hurts them severely. It can kill people. Um, this is unusually clean example.
Um, there are many examples that are on the spectrum from this unusually clean example.
Got it. Um, so, I mean, you also said, um, you watch many good hearted tech folks, uh, practically
give away agency. Is that related to what you're talking about because their minds are infiltrated
by some gods or memes that aren't necessarily, uh, on their side?
Absolutely. There's always a large, as I say, it's a predatory world. You know, we're swimming in a,
you know, a swamp, a primordial ooze of, you know, evolutionary pressure of viruses. There's, um,
it's surprising how bad our norms around epistemic hygiene are about mimetic hygiene.
Like people just like turn on the news or like scroll through Twitter for hours per day and like,
yeah, this is fine. Like the way you think about it is like putting your hands into like dirty slime,
like wash your hands. Like, what are you doing? Like, if you just, if you just dunk your hand
into the infected slime, yeah, it's going to be bad for you, you know, and like wash your hands,
God damn. Um, and people truly don't wash their hands to a surprising large degree. There is
surprisingly. So we talked a bit earlier about how people feel this helplessness. I don't think a lot
of this helplessness is that people don't have these terms to talk about it. So even before we had
germ theory, this was how we were dealing with physical illnesses as well, is we didn't have a
concept really that like drinking things, some things were bad or that not washing your hands
before or during surgery is bad. Like when doctors first said we should stop, start washing our hands,
before doing surgery, surgery, mind you, he was run out of medicine. He lost his job. He was vilified.
He was like, lost literally everything. It's a great story. You should forgot his name,
unfortunately, but yeah, everyone was like, like hated this guy. He destroyed his entire career.
I'm not sure if he committed suicide, but like, at least was close to it. Um, so truly awful story.
And now from our perspective, we're like, what the, like, what, like, of course you wash your hands
before you do surgery. What the hell are you thinking about? And I think we're at a similar phase
right now where we're in the pre hand washing era of like information consumption. I think part of
this is because for a very long time there wasn't that much slime, you know, like we didn't have,
before we had radio, before we had television, before we had the internet, the amount of like
negative medic information you could be exposed to is relatively low. So the pressure toward developing
memetic resistance was relatively low. Um, it still existed, but it was relatively low the same way how
before we had cities, there weren't really pandemics. There was much less evolutionary pressure on
viruses and bacteria to develop pandemic level replication without cities. And then after like,
there's a great, um, one of my favorite, like, you know, mysteries quote unquote is when the Spanish
came to America, you know, they brought smallpox with them and ravaged the North American continent.
Why didn't the Spanish get sick with mysterious North American diseases? Like, why didn't they bring,
you know, America pox back to Spain? And the usual explanation for this is, is that the
there weren't very dense as many dense cities with bad sanitation in North America. There were some
cities, but they were smaller and there weren't as many of them. And general sanitation was not
quite as bad as in European cities. So the viruses and the bacteria that developed in Europe were just
a million times more virulent and, you know, evolved in a sense than the ones that were common in North
America at the time. And there's a similar thing going on with the internet right now, where the internet is
a cesspool of mimetic evolution. It's a spawning ground, a primordial ooze from which like, you know,
we have like this extends competition of like more and more mentally ill, you know, teenagers coming up
with more and more deranged memes and more and more deranged culture, war issues, more and more,
whatever to have people's minds as quickly as possible. Like this is kind of where we're at right
now. And so when talking about these people that you're talking about, these like good hearted nerds,
um, three years ago, uh, when we talked last time, you know, I was more optimistic about this.
Um, cause I hadn't yet witnessed in its full horror, how much, like, I kind of thought like what you
were talking about earlier, like, well, but you know, Lacoon is a smart man. So why is he doing this?
I used to believe a similar thing. Cause I was like, well, if people are smart, they'll be able to
resist memes and like, you know, think for themselves. Right. Yeah. Right. But there's a saying
where like people think cults are for stupid people and those people themselves are probably
vulnerable to cults. Um, cults are not just for stupid people. Um, like cults affect very smart
people all the time. This is a very common phenomenon. And it's one that I think people
are so uncomfortable with this fact. Like most people in the common world are so uncomfortable
with the fact that many of the worst cults attract very intelligent and very academic people that they
don't even think about what this should teach us, like what we can learn from this side.
I'm fascinated by like, I'm chilling equal and like other extreme, like elite cults,
which are deranged. Like you read their literature and whatever, and it's insane. It's complete madness.
Yet it attracted top intelligentsia, top scientists, top engineers to their ranks.
And this should teach us something. It should say like, what's going on here. And then to pull from
the extreme example, back to the concrete example, how many of us have heard, you know, it's become
a meme in the Western sphere, you know, of like, I was just following orders, but let's be honest.
We all know someone who said something like that before, even for ourselves, you know, you work for
a big company, kind of do something, you know, maybe not quite so comfortable with, but I mean, it's your
boss's fault, he makes a decision. So you can let it happen. Right? Right. You know, sure, you're
building AGI, which, you know, might kill everybody, you don't really have a plan. But look, you're just
an engineer. You're just following orders. It's not your job to figure out how to solve it. You just
build technology. And this is very common. So is that what Rune meant by don't fret about the outcome?
So I think, I think this is the uncharitable interpretation, which is the one I'm thinking.
I'm not instantly charitable, just for fun. I think charity has its point. And sometimes you
shouldn't be charitable. Sometimes you should be, sometimes you shouldn't be.
I think the charitable interpretation is that he's saying, look, bro, if you're just some guy,
fuck it. Like, you can't do anything. You're too stupid. You're too weak. Don't even try. And
this is a harsh message, but there is some truth to it. And this is something I struggle with as well.
I don't have a good answer to this question is that, man, the world is actually complicated.
Like, like, fuck me. Like the world is actually hard, right? It's like, I could, I could, I could tell
some oil, you know, grad student be like, bro, like climate change. And he will answer like, okay,
well, what, what do I do? You know, I have 95 IQ and, uh, a full, a family that I have to feed.
And also my mom is dying of cancer. Like, well, what, what do I do? And this is actually a good
question. Like, I want to have sympathy here. So there's a, there's a cynical snarky version
of this where you're like, people are bad. Like people have no moral consciousness. They have no
spine. They're cowards. They will just do what's locally good, best for them, whatever. And I mean,
this is true, but it's not, it's not compassionate. It's not the proper compassion that I think should
be had here. The proper compassion that should be had here is that the world is fucking hard.
And being human is fucking hard. Taking care of your family is fucking hard. Taking care of your
mental health is fucking hard. There's no, like, we're all in this together, right? It's a human
condition. This is, it is actually a struggle and sure we could be like, ha ha ha. Look me living in
a first world country, you know, from, you know, loving middle-class background. I'm going to look
down on you because you, you know, cause you're like a struggling, you know, single mom, you know,
who can't, you know, has no time to think about serious issues. And that's just not productive.
Like what the, like, what the fuck are you doing? Like, like, shut the fuck up. Like it's,
it's easy to fall to cynicism. It's easy to be like, well, you know, no one's going to do anything.
Everyone's bad, blah, blah, blah. I understand. But this is not productive. We have to simply see
that it is actually hard. Fighting gods, building gods is hard. It takes resources. It takes luck.
It takes, you know, being healthy. You know, if you're just sick, like, well, damn,
like, well, you do like, fuck, you know, it's just, so I have a lot of compassion and I truly,
truly have compassion for people. And I do think it is true. And this is something I struggle with.
So in my personal relationships, a pattern that I often struggle with is that sometimes people will
like very normal, nice people, just normal people living their normal lives. You know,
people I meet at a party or whatever, they'll want my opinion on something. They want to know,
like, what's really going on. They want that. Like Connor, like tell me how it is, or they want
me to psychoanalyze them or something. Cause like often people get a feeling that I'm good at reading
people or whatever. And they want me to like psychoanalyze and they want me to talk to them
about spirituality. They want me to tell them what's wrong with them. And I usually don't, um,
partially because this is not my responsibility and like, I shouldn't do this. And also it's because
it will just hurt them often. Like I can tell, like, um, but what happens quite often with my mom
is like, you know, I'll have a call with my mom. I'll tell her how I'm doing and so on. And she'll
be like, ah, she really wants to know what I'm doing and like really what's going on in the world.
And, you know, I'll tell her what I'm doing and whatever. And she gets so scared, um, you know,
about the world, about politics, about AI and so on. And almost always it concludes with just like,
but you're okay, honey. Right. I said, yes, mom. Okay. Don't worry about it. And
I think this is actually compassionate and like, I love my mom and she loves me more than anything
in the world. Um, but she can't do what I do. Um, she has responsibilities to the rest of the family.
She doesn't have the same kind of personality traits, the same set of education that I necessarily
do to be able to run a company, to run political campaigning and so on. And I do not think it is a
good thing, both. I don't, I think it's unfair to these people to look out on them, but also at the
same time, what's the point of making them upset? Like, what's the point in like making them fret and
scared and frightened. I can make them aware of all the towering Titans, all the monsters and the devils
that are around us, but does it help? So I think what Rune is saying here, I think is actually
something compassionate. And I want to give Rune credit for this. I think this comes from a place
of love. I don't think Rune is a bad person. I think what he's saying here is coming from a place
of love. I think what he's saying is that if you're all just worried about all these demons that you can
do nothing about, you're just going to be scared. You're just going to be upset and it's not going to
help fight the demons. So like, don't fret about it. Enjoy your life. Be good to your neighbors.
Be good to your family, you know, do what you can. And I do think I do respect this and I struggle
myself with this dichotomy of on the one hand. Yeah. I, you know, I want my mom to just take care
of herself, take care of her family, you know, back home, um, be happy and like, just don't worry
about the politics. Like I'll, I'll handle it. But on the other hand, I'm also like, God
fucking damn it. Someone has to handle politics. Yeah. Yeah. And, um, on that note, I mean,
keeping almost everyone, uh, apart who can't really influence, uh, the course of AI, but there
are people who have a huge amount of influence. Rune, I'm sure has influence and open AI is
probably 400, 500 people. Uh, what do you have to say about them? Like what moral responsibility
they have? And, and it find it, I mean, of course, everyone notices this, let's say Sam
Altman goes around talking about dangers of AI yet he and his group of people are the same
ones building it. And that to me is cognitively dissonant. So what's, what's happening there?
Yeah. So there's a reason that I yelled at Rune and not my mom. Let me put it that way. Um,
yeah, the reason I wrote this long Twitter post calling out Rune rather scathingly, um, is because
like, bro, like this is co like ruin is obviously not my mom. Like, you know, my mom obviously can't
do what Rune can do. Obviously Rune is extremely intelligent. He's extremely capable. He's charming.
Like he's in a good spot. He has capital, both social and otherwise like, so it's in my aesthetics
too. Sometimes I'll meet someone like this. And when I meet someone like this, I like to hit them
with the equivalent, the verbal equivalent of a bamboo rod. So like in Buddhism, there's this thing
where you, uh, the student, when the student gets distracted, you hit their hands with a bamboo rod
because it doesn't really cause permanent damage, but it hurts like hell. I know if you've ever done,
I've ever had, oh God, it hurts so bad. Um, and so the, the point is to draw attention to something
to be like, wake up, like, like what the fuck are you talking about? Wake up, wake up, wake up,
wake up. And this was kind of like this post that wrote was not really meant for other people. It was
for Rune. It was like, I saw like a man who has great potential where I think is sleeping and I want
to hit him and be like, wake up in practice. This never works. Um, sorry, all you all Buddhist
masters out there and so on, or like people who hope, but in practice, this doesn't work. Um,
hitting people and making them wake up. It doesn't work. Um, there's reasons why it doesn't work. It
usually works temporarily. And usually just go back to sleep because their social environment is
optimized to put them back to sleep. Usually you need to hit people with a rod and then also move
them out of their social context into a different social context, which reinforces reflection or
you need to do what I call bootstrapping, which is a whole other topic I hope to write more about
in the future ever since this post. And even before that, I was thinking a lot about this
question of, okay, but what if you can't move social context and you want to keep awake, what do
you do? And the process I call is bootstrapping, which is this idea of what are the things you need
to do to not fall back asleep and keep going? And what is the minimum set of things? What's the smallest
commitment that if you do this or like something in this category, you will continue to improve
and you'll reach escape velocity, kind of like spiritual escape velocity. You'll start reflecting,
you'll get better and better and better. But it has to come from within, right? It has to be initiated
by the person. Yes. Ultimately, if you don't want to, whatever, like ultimately to go to the underworld,
you have to want to go to the underworld. Otherwise you can always return back.
It sounds like you need to escalate your commitment so that you ultimately come out of the situation
you are in. There needs to be something you're fighting for, or there needs to be something you
need to, you want to protect. The most common way, other than accidental or forced entry into the
world, the call to adventure is usually not an active thing, like on the part of the adventurer,
per se. It's usually that the valley is threatened. It's usually you are protecting. It's usually
the adventurer wants to save the valley. It wants to protect other people. And to do that,
they have to go to the underworld. This is how things usually go. Having something to protect
is a very powerful, um, way to motivate and to reason about yourself and keep yourself accountable.
Um, I think it is in a sense, a more healthy driver than the will, than the raw will to power.
I think the raw will to power is predictably disruptible, but if you just want power for yourself,
there's like no way to not become a villain. If you want to protect and you need power,
the will to power to get there. I think this is how heroes are made is, you know, the hero is the,
is the intersection between the will to protect and the will to power. Um, because you need both.
Otherwise, you know, you just, you're just, uh, you're just another demi-god, you know, ascending for
his own selfish needs. Um, and then a new predator in the field.
I think everyone has a self image in which they are doing whatever they're doing for the good.
And I'm sure OpenAI has a narrative where they're trying to create a lot of value in the world and
they don't actively see themselves as, uh, someone who could be doing evil things, right?
Exactly. And so this is exactly why I gave the warning about mystic talking and mystic language
is that a story isn't sufficient to be a hero. Like you're not the good guy because you follow
a narrative structure. This is simply insufficient. There is no way to take, I think I'm a good person
and therefore concluding you are a good person. This step doesn't exist. And you can take the, the,
the reason mysticism is dangerous is that it allows anything. The, you know, the, in the proof system,
in the mass system, you know, if you construct it correctly, it should only allow you to make correct
proofs, like incorrect ones are just, you know, just, you know, at least in a constructive system,
blah, blah, blah. You can't even make wrong ones. Like it's just not allowed. Or, you know,
they get assigned false or whatever. This is not the case outside of mathematics. I can, you can give me
literally anything right now. I promise you, you can give me anything and I can justify it.
I can come up with an argument that justifies anything. Any sufficiently smart person can do
this. Like anyone who is sufficient, has a sufficient high verbal IQ and has read enough
fiction and a little bit of psychoanalysis can justify literally anything in any context.
And the conclusion, so one conclusion you can draw from this is that, well, I guess morality is
completely arbitrary. I'll just do whatever the fuck I want. Fuck you. This is one thing you can draw from
this. Another thing you can draw from this is that, huh? Wow. Morality is actually hard.
And I can't actually justify, I can't actually guarantee that I am good just by thinking it.
This is just not sufficient. And then you have to think about larger things. You have to think
about your social system. You have to think about your own incentives. You have to think about yourself
as a, from third person. You have to think about not just, you know, who do I think I am,
but like in practice, what is the entity that inhabits my body? What is it? What is it doing? And where
is the steering? Because by default, the humans are really good at convincing themselves that,
you know, whatever they're doing is good. And it's very, very hard to evaluate this in any objective
sense, any fungible, even non-trivial. I'm not even saying the one true goodness. I'm saying just like,
not obviously stupid. Even this is really hard. And if you're in a bad social situation or an irreflexive
system, if you don't have control over your emotions, if you don't eat well, sleep well,
have family and friends, if you don't have these things, it's almost impossible.
Like it's almost impossible. It's really hard. It's really, really hard. It's possible, but it's
really hard and it takes effort and takes energy and it takes under a will to power to a certain degree.
Like you have to be willing to fight gods, right? Like, um, another blogger called Zvi, um,
wrote a little bit about that post I wrote. And one of the things he said is that like,
at the end of the post is that like, um, Rune is kind of saying is that, well, look,
the world is run by Cthulhu. Cthulhu is some powerful God, you know, we're, we're fucked,
you know, just live with it. And, uh, from his perspective, what I'm saying is, um, to a certain
degree, like, yeah, the world is run by Cthulhu and you have to punch him.
And this is insane. You know, punching Cthulhu is insane. Cthulhu is much stronger than you.
You're a human, but this is the only way out. Like either Cthulhu eats you or you find some
fucking way to punch him. I don't, I'm not saying I know how to punch him, but like, you need to find
out something that someone needs to, you know, if you don't do it, someone needs to punch Cthulhu.
And this is kind of a lot of how I think about these kinds of things. I'm not saying, look,
I am the Messiah. Here's the things you do in this order. And then everything turns out fine.
Not what I'm saying. I'm saying is, holy shit, Cthulhu is running the planet. Um, no one seems to
currently have a plan how to build a Cthulhu killer weapon, uh, or what to do with the corpse afterwards.
Um, we should probably figure that out like right now.
Yeah. And, and apparently a lot of smart people are sort of infected by these mind viruses, gods,
which of course, so there is a deeper, not even acknowledged. There is this thing going around.
Oh yeah. Like one of the biggest, like the biggest quote unquote spiritual skill or magical skill,
like the most important one is reflection. The ability to reason about your own thoughts.
This is much harder than people think it is, especially because your emotions will actively
hinder you in gaining access to your own mind. It's actually hard. Like people think,
many people think that we have like privileged access to our own minds that, you know, whatever
you think you think is what you think. That's not true. Yeah. Um, what do you think you think
is often not what you think, um, you know, say that three times fast. Um, and, um, this is actually
tricky. Like you are like the human condition is you are a faulty algorithm running on faulty hardware
with incomplete access to itself. That is supposed to reason about itself to improve itself while
surrounded in adversarial environment of constant programs made by much stronger programmers than you
who are trying to hack you. This is the human condition. This is the underworld. The underworld
is the thing below consensus narrative reality. It's the murmurs, the, the, the, the sludge,
the, the creatures, the, the, the worms, you know, everything that underlie the infrastructure,
the, the pillars upon the psychic pillars upon which everything stands. And once you actually look down,
you'd be like, oh shit, there's a bunch of shit down there. And a lot of that shit's in your head.
A lot of is like trauma and like weird, you know, desires that you may be not reflexive upon
the lack of education. You just never learned how to think about your own thoughts. You just never
practiced. Uh, it's just like, there's so much shit. Like, yeah, in a sense, if it was only gods,
that's already bad enough, but there's also a bunch of other shit. There's like, so
this psycho fauna, you know, like sites, you know, like psychic creatures,
gods are one of them, but there's other ones as well. There's various kinds of demons that are
in your head and they can come from all kinds of trauma or, you know, physical injury or whatever,
you know, can cause demons, um, to appear in your head that you have to deal with, or that will
inhibit you in some ways. And it's truly very hard to deal with this and there's no general purpose
solution. So, and like, and so there's one more thing I want to bring up before we move on to any
other topic, which is there's no easy way to talk about this. This is the hardest, maybe the hardest
part of this to talk about. I didn't talk about it in my tweet because it was too hard to talk about.
I'm going to try and I'm probably going to fail. There's a deeper sense where there is nothing in
our brain, but God's is that separating you from the other things doesn't actually make sense. You
isn't really a thing. Um, there is various patterns that claim to be Paras chakra. And these patterns
are very different depending on the time of day, which person you're around, what social context
you're in, whether you've eaten recently or not, um, whether you got hit on the head or not,
and many other things. The soul is not only in your brain. It's very distributed.
The thing that you would call the soul, like the person is distributed to your, your friends,
your family, your social networks, your tools, your writing, your, the art you have hanging in your
in your house. It's all part of you in some sense. And in a sense, you are a very big thing.
Like, and there's some things that are less you, you know, the, the, the paintings on your wall are
less you than the brain in your head is you, but it is you a little bit. It's a little bit you. And I
am a little bit you because we've known each other and we've talked not a lot. I don't know you
particularly well, but there's a little bit of Paras chakra in my head, a little bit. And there's a
little bit of me in you a little bit. And so this is very messy. So it can be useful to think about
just like, okay, I am a fortress and these gods are trying to enter my fortress, but this is actually
an awesome venture. It's more like you're also a spirit. You're also a God. You're also this thing
spread across psychic reality. And you're mixing with all these other things to various degrees.
And you don't have full control over the mixing. Like, you know, you can meet someone and they'll
just like save stuff to you and it enters your brain. And now you think differently. So it's very
hard to reason. And to think about how do you build yourself? How do you create this God, this entity,
this hero, whatever. It's not just, you decide one day, yep. I'm a good guy now. Hell yeah. This is
what I'm going to do now. That easy. You're made of many parts. That's almost like one God that you
identify with as self trying to control of all other gods. And that really happens. Exactly. You
got it. Like, this is a good way of thinking about it. This is like the Buddhist way of thinking about
it. So the self is just another illusion. It's just the punchline I've talked about in the post.
It's the punchline of like going all the way into the underworld. The full shamanic enlightenment
is there are no spirits. There never were any spirits, but like, of course not. Like the world
is mechanistic. There's no magic. There's no spirits. There's just people and words and language and
stuff. Like it's not, it's all mechanistic. It's all atoms deep down. That doesn't mean it can't be
useful to talk about them, but fundamentally there is no mystery here. It's nothing magical. It's a,
it's understandable. It has, it's a hard thing to understand. It's one of the complex parts of reality,
but it's just reality. Everything adds up to normality. And so the takeaway of like,
good in my claim, if you take my spirituality here, it shouldn't be like, you come out of this
and you're completely destabilized. Whoa, maybe soul is everywhere, dude. When you should come out of
this is like, Oh yeah. Huh? And then, you know, you just like, you're still, you like, it should
change who you are. You were always like this. And the whole being like, like the Zen position
might be like, you were always enlightened. Like you've always been enlightened. You just didn't
notice. Like, it's like, it's just how it is. I don't quite agree with the Zen position on here,
but I think it's a nice aesthetic. Um, and it's, it's similar here. It's like, look, it's not like
if I inform you about the existence of gods, it's not like they didn't exist previously.
If I inform you that you're this psychic cloud, it doesn't mean you weren't a psychic cloud before.
It's just, you were listening to this one story about you being oneself in your brain.
This one God, this one, you know, mono mono theistic, like even more mono theistic than
mono theistic God. It's in your head is not only is the only God you worship. It's the one that is
you. That's like a whole privilege position. And it's a very useful position. It's a very useful
for people to think of themselves as one entity because it's useful coordination and stuff like
this. I, but it isn't quite true. Yeah. And this is a acknowledgement of this view does help you
become more charitable and compassionate and it sort of, um, diffuses the boundaries between good
and evil because then these things are also diffuse. If people are, people are much more porous than
they would imagine themselves to be. Then, uh, they're very porous. And the thing I also want to
think that I, I, I think is important. I, an interpretation of this that I don't like is that
people are like, wow, I'm like everything dude. And the whole universe is one and therefore
everything is love. And I'm like, no, no, no, you went too far. Go back. Like, no, no, no, no, no,
hold on, hold on. Like some things are less you than other things. Some things are more you. Um,
this was called continuous agency. And like, for example, um, I might say some words to you
that convince you to go do something. You know, you go grab me a glass of water or something.
Are you part of my body? Like, not like we wouldn't say that, but technically my brain
outputs some neuron signals, which caused a chain of events that caused a glass of water to come to
my hand. So in a sense, I could think of you as part of my body, but doesn't really make sense.
Right? Like that's not how, not how we would use those words. So you're like less me, but there is
a causality there. Like there is a causality between like, and this is what becomes especially important,
for example, with corporations. So corporations, when well constructed to a large degree can be
thought of extended bodies, extended body minds of you, um, especially for like startups,
like smaller groups of people where you have like a strong founding figure or a small, small group of
strong founders with a lot of support. Um, you can think of it as a entity that is acting
in a coordinated way. This is how people often treat CEOs implicitly. Like when they say Sam Altman,
you know, did this in the name of open AI. And, but like, did he, or was it this massive process
of dozens of engineers and support staff and personal assistants and operations and all these
things that through a long convoluted chain resulted in this thing happening where Sam was involved in
this chip in this chain, of course, um, of course he does. But can you say, oh, he did this.
It's a spectrum. And I think this is a useful way to think about causality. And it's also a good way to
think about blame and credit is there's very tempting to say, like, you know, the guy who pulled the
trigger, he gets all the money or whatever, but is this actually a good way to think about how
things work? Like we have an intuition that like, no, we should probably get the guy in charge and
we should put him into prison. But even then does that make any sense? So like recently I've been
playing, um, this video game called cyberpunk 2077. It's a really nice video game. I like it a lot.
And it's a cyberpunk world. So it's like dystopia, like corporations control the world, blah, blah, blah.
And as part of the plot, one of the characters is like, you know, uh, you know, these evil corporate
guys, you know, we're, we have to like find the CEO and we're gonna like, you know, like kill him.
We have to defeat him, whatever. Right. And me as a player, I'm just like, no, that doesn't help.
Even if you find the CEO of the evil mega corp, and even if you kill him, it doesn't matter.
He just gets replaced the next day. Like, what's the point? The board will just select a new CEO.
Who's just as evil, like killing the CEO doesn't help. And so this is like a very,
this kind of interpretation of this kind of things. Like, it's not used, like, you know,
I think that's happened to me multiple times is a well-meaning journalist will come to me
and ask me a question. Like, is some ultimate nice. Is he a good guy? Like, does he care about
safety or is Demis, you know, is he a good guy or a bad guy or whatever? And my answer to this question is
it shouldn't matter. It shouldn't depend on whether some tech CEO is a nice guy or a bad guy
that whether or not the future will go well, this is a bad system. Like we should just not have a
system which allows this to be the case. So I don't care if Sam Altman is a nice guy or a bad guy.
I don't care if Demis is a nice guy. I've met both of them. A lot of positive things to say about them,
some negative things, but also a lot of positive, whatever. But ultimately I don't care.
There should be no one who makes these decisions. The causality of whether AGI gets created,
of how the future goes, shouldn't depend, like there should be no dependency node on
this tech guy is a nice guy.
And so this is where I think this thinking is very important is that I think Rune is completely
correct. AGI is being built by gods. It is not being built by people. And this is really important
to understand. The entities that are building AGI, ruling the world that are making these decisions
are not people. They don't follow people rules. They're gods. But that doesn't mean they don't
follow rules. Gods are part of nature. They're mechanical. They can be understood. They can be
modified. They can be fought. They can be hunted. This is all possible. But if you approach them
as if they were people, if you use the methods you use on individuals, on gods, you will completely
fail. Like, for example, a common strategy is appeasement. A lot, a lot of times people,
good hearted people will join a company that they don't agree with because they want to, you know,
change it from the inside. You know, they want to, they want to get on its good side. So when it
matters, they can put in a good word and steer it in a good direction. So this does work with people
except sociopaths, but like gaining the loyalty of a person, ingratiating yourself with them.
So that in that moment of crisis, they come to you for aid or for advice. That is a viable strategy
for people. It is not a viable strategy for gods. This is not how gods work. And you will
consistently be disappointed over and over and over again if you naively approach gods as if they
were people. They're not. You have to use completely different methods in order to hunt.
Have you found ways to communicate or reason with gods? Like, I know this is the biggest problem,
but yeah, how do you do so? Many, many, many ways. I mean,
we have a very complicated one called the legal system. That's a whole protocol for the god language.
This is legal in itself. Of course. Well, it's, it's implemented by various gods and has various
godlike properties, but like legals, our legal system is one of the most fascinating attempts
at codifying and like writing down actual god code and god language and god structure and god and spells,
like how to affect gods. Like it's, it's so retarded, right? I, I signed some piece of paper
in Delaware and now there's a new god. Like, you know, it's true, but it works and people believe
you. I could just like sign a piece of paper, Delaware, and it'd be like, okay, I am now X Inc.
And people just believe me. They're like, yeah, you're X Inc. Cool. You're a company. Good job.
Nice. Like it works. You just cast the magic spell. You signed the stupid piece of paper.
And if you don't do it, it doesn't work. If you don't sign the stupid piece of paper in Delaware,
well, people don't care. Then you're not really a corporation. Like you're just like some guy
pretending to be a corporation. But if you sign a piece of paper in Delaware, well, that counts.
So the legal system is a, and the, and government and so on is a fascinating attempt to control gods,
to shape gods. This is what regulation to a large degree is about. A large amount of regulation
is not about controlling people. It's about controlling gods. It's about controlling how
gods are allowed to interact. Also how they're allowed to interact with people, how they're allowed to
interact with each other, you know, legal personhood. Like we literally give gods personhood
in, in legal context. And this is actually good. This is actually based. Um, this is actually a good
way. I think this is good. And like, we shouldn't be reasoning on this level. So yeah, you can,
you can interface with gods through the legal system. For example, you can interface with them
through the spiritual, social marketing realm. You can interface with them through narrative.
You can interface with them through, um, you know, the commercial realm,
obviously like that's what most gods are doing is commerce. Um, most of them are trying to make
money in various ways. That's what most of them do. And so most gods are very receptive to money.
And if you have a lot of money, you can make gods pay attention to you and do a lot of things you
want them to do. Um, and vice versa. So gods are not a magic, weird out there thing. It's daily life.
It's a question of how do you interact with the bureaucracy? How do you interact with a nation?
How do you interact with a company is the same question as how do I interact with the God?
And the answer is, it's not, not that weird. Like we're used to, we do this every day. We constantly
interact with various gods. And sometimes it's really frustrating that a lot of gods are not super
aligned with our, you know, human worlds. So now the more interesting question is, all right, cool.
We have some basic interaction protocols or whatever, but this is obviously insufficient.
What's the advanced stuff? How do you hunt gods? How do you build new gods?
Now this is the interesting stuff. And you will see as we, when we map this into normal language
is mapped into how do you build a company? How do you win a political campaign? How do you, um, create
viral marketing? These are the same questions as how do you do advanced things with gods?
How do you build gods? How do you control gods? How do you shape gods? How do you kill gods?
All of these things are fundamentally the question of how do you get large groups of people to do or
believe something in various ways? And there's many ways to do that from marketing and campaigning,
politics, um, and to, you know, money, you know, building good products, narratives, fiction.
There are many ways.
Got it. Uh, so Connor, since we are at the end of the time, I want to talk about conjecture AI.
Uh, you mentioned somewhere you want to blaze a path through an underworld to build,
underworld to build infrastructure. Is that what you're doing? Are you trying to build infrastructure?
Uh, unfortunately, that is more of a side gig at the moment. Um, I have unfortunately many,
many things I need to do. Um, so conjecture itself is more focused. It is a, it is a company,
it is a R and D company. We are working on developing AI systems that are useful, commercially viable. Um,
so we can get the benefits of AI while being controllable and understandable. And in particular,
that we can know what they know and what they don't know. And we, they don't accidentally become
more powerful. Um, one of the things that most worries me about the current path of AI is that
we have no idea how smart AI systems are. We have no idea what G before can actually do in the limit.
We don't know what cloud three can do. There was that funny post where someone was like,
wow, look, Claude seems to have noticed it's in a test. Isn't that funny? Didn't expect that.
So the problem isn't that result like, okay, whatever. The problem is the not predicted.
If they were like, Hey, we, we trained our system such that we predict it will be able to know this.
And then it did. I'm like, okay, fair enough. That's all right. But no, it's complete black magic.
It's complete dark arts. No one knows what's going on inside of these things. And this is a big problem.
So even if we wanted to prevent super intelligence from coming in existence,
we currently can't because it will happen accidentally. So forget all the people who
want to do this on purpose. Even the people who don't want to do it on purpose currently cannot
prevent it from happening accidentally. There is some number N such that GPT N becomes super intelligent.
And we don't know what that N is. Maybe it's, maybe it's five, maybe it's six, maybe it's seven.
Who knows? I don't think it's eight, but we don't know. Right. It could be five. It could be four
for all we know. And we just didn't notice. I don't know. So we're building systems and developing
novel Beyonce or the art methods, um, that allow us to have much more control over what AI systems
actually learn and what they can actually do and not do, um, still R and D phase there. And the
separate thing of like blazing infrastructure through the underworld is kind of thing I alluded
to a bit earlier about bootstrapping. Um, currently a bit of a side gig. Um, but unfortunately it's
like when I talk about the AI problem, it's tempting to only talk about the technical problem,
but the true question of how do we build a good future for everybody is not a technical problem.
It's also a technical problem. It wasn't only a technical problem. It's a technical problem.
It's a social problem. It's a political problem. And to some degree, it's a spiritual problem.
Yeah. And I think all these problems need to be solved. I'm willingly or not working on all of
them kind of in parallel. I hope there will be more people, um, you know, who join me and others to try
to solve these problems and work on these issues in our France. Cause I think we need to solve every
single one. If we skip even one of them, then the future is going to be that great, but it is possible.
Yeah. On that note, um, uh, thank you so much. Uh, I think it's wonderful to have people like you
think about these problems and then sort of go into great depths to tease apart all the nuances.
I'm sure a lot of people will find this wonderful and I'll keep an eye on, uh, hopefully a lot of
contributions from your side that have prevent us from very obvious risks that are just out on the
horizon. So thank you Connor for your time. It was wonderful chatting with you. Thanks so much.
