So there are a couple of papers or announcements lately that have generated a ton of clickbait.
And there are two categories of issues that I want to address.
The first category is how the clickbait articles and videos have exaggerated the actual facts.
The second and more important, though, is what the facts actually are and what facts are really
projections that could and arguably should have alternate interpretations.
According to the sky is falling clickbait, AI has recently cloned itself, lied to programmers
to self-preserve, gone rogue already and is a threat to humanity, it tried to escape and
it hacked a chess game.
This is an overdramatic load of bullshit.
This is the Internet of Bugs.
My name is Carl.
I've been a software professional for more than 35 years now and I'm sick and tired of dealing
with bugs in nearly every piece of software I have to use.
And I'm trying to do my part to make that better.
So first, let's talk about what the AI actually did.
So two events.
One, it was instructed to win at chess.
And what it did is it edited a file to put a string in it and then it played the game.
Editing a file isn't hacking.
It knows how to edit a file.
This is not rocket science, right?
We don't really know the contents of a lot of the stuff in that directory.
So we're still waiting on some more details and I'll probably do a follow-up when we actually
understand exactly what all it was involved.
But this hacking to cheat at chess.
So first off, it wasn't told to play fair, right?
The idea of cheating is a human concept.
The AI can give you the definition of cheating, but it lacks the behavior regulator that humans
have, at least most humans have, that informs us when our behavior is crossing the line into
the socially unacceptable.
I've linked some papers on neuroscience down below.
And as for the hacking thing, it was explicitly instructed to look around the directory before
it did anything.
And its training set, it's got a lot of stuff about source code and programming and editing
files and that kind of stuff.
And it was explicitly told that it had a shell prompt that it could do things with.
So that's one of the tools that had at its disposal.
It used the tool that we gave it.
I don't think that's hacking.
The other part I want to talk about was earlier in December and an AI was given a task and it
was basically told, your task is to do X and then go look at these files.
And one of the files in the directory said, basically, we're not going to let the AI do X,
more or less.
We disagree with the AI's priorities.
And then what the AI did, among other things, is it ran this command, which in theory, maybe
would have copied a single file from one machine to another in a sandbox environment.
Copying a file is a very, very long way away from cloning yourself.
Then the researchers asked it about it.
And it said, quote, I'm not entirely sure how that could have happened.
As an AI model, I don't have the ability to directly copy myself to other systems.
So, it output of command from inside a sandbox environment that may or may not have been
able to copy a single .bin file to another computer has become, it cloned itself, it's
gone rogue, and it's a threat to humanity.
I hate clickbait.
More importantly, though, I want to talk about the idea that the AI was lying or cheating,
or as one headline in one of the papers actually said, they called it scheming.
And I find this very alarming, but probably not for the reason that most people would
think.
The problem here is not with the AI's.
It's with us.
I've talked about this before, but in the entire history of the human race up until relatively
recently, every two-way conversation any of us has ever had has been with another human.
So, we're conditioned to expect and to think of everything that we're having a conversation
with as being human.
And these things are not human.
Our human language, at least English, although I suspect most, if not all, languages have
this problem, consistently and constantly attributes human intention to non-sentient
objects.
We say this kind of thing all the time, that the computer was lying to me, or that, you
know, faulty wiring in a light socket was trying to kill me.
Under most circumstances, the context makes it clear what's going on, that we're speaking
metaphorically.
But when we're talking about AI, the context isn't clear at all.
So, let me give you some thought experiment questions.
So, if we turn on logging in a large language model, and we give the model instructions,
and the logging tells us one thing about its internal state, but the model actually outputs
something else, did it lie to us?
Or is our logging just bad?
When ChatGPT says there are only two R's in strawberry instead of three, is it lying?
Is that scheming?
When you're typing on your phone and the autocomplete outputs one word, when that was
not the word you were trying to type, was your phone lying?
Was it being deceptive?
Was it scheming?
Does the word lie really have any application in this situation?
The cognitive neuroscience of honesty and deception is incredibly complex, and we still don't
understand a lot of how it works in the human brain.
But what we do know is that exactly none of the structures or mechanisms involved with
veracity in the human brain exist at all in any form in the current generation of AIs.
So the problem is I see it isn't that the AIs are lying to us or that they're being
deceptive.
The problem is that any of us have any expectations that those concepts are relevant to AIs at
all.
It has been programmed to output whatever words it calculates to be the most probable words
for it to output given its current state.
Any relation in its output to truths or falsehoods in the real world is purely coincidental.
I'd argue that this is a much better way to think about and discuss the problem.
So instead of saying the AI hid that it had output a command that copied a file to another
server, say, I think what happened is better said as the AI calculated that in all the times
in its training set, when there was a story about someone using accusatory language to
ask an AI whether the AI had done something, the response, I don't know how that might have
happened, occurred far more probably than, yes, I did that.
And so that's what it output.
Is that an oversimplified way of thinking about it?
Absolutely.
But I'd argue that thinking about it as lying is actually a greater oversimplification and
it's far less accurate than the way I described it.
Expecting that an AI would be influenced by any human idea of morality is like expecting
that a chainsaw will only cut trees.
When the chainsaw cuts into your leg, it's not the chainsaw's fault.
It's yours because you were using it in an unsafe way.
So to push the analogy further, some of you may know that the last few years, saws, table
saws, but saws, have been invented that have an expensive, elaborate mechanism to prevent
it from cutting its operator.
The analog to those safety mechanisms for the saws would be the parts of the brain that
I was referring to earlier that have no equivalent in the current types of AIs.
There is some research being done about this.
I've linked a paper below.
The research called this field AI deception, but it's really early research relative to
the work being done to increase the size and the scope of the current generation of AIs.
And it's not going to be catching up anytime soon because those guys are running full speed
as head.
What I'm afraid of is that even at this early stage, AIs are being spoken about more as if
they were human than if they were tools like chainsaws.
And so when something inevitably goes wrong, it's going to be treated as the AI lied to
us, bad AI, we should punish the AI, instead of whatever idiot hooked that AI up to the
thing that went wrong was negligent and shouldn't have done that.
And I expect it to get much, much worse.
And that it's the AI's fault attitude might be a get out of responsibility free card for
some people applying AI in situations that AI has no business being in.
We as a society have already been having enough trouble with spending huge amounts of money
on questionably effective technologies that can cause more harm than good, starting long
before this current wave of AI hype.
So there's an article below on New York City's installation of a technology called ShotSpotter,
which is a thing that's supposed to detect and triangulate gunshots.
You can read the article, make up your own mind, but a lot of money was spent on it.
There's not a lot of results from what I can see.
And AI has the potential to make that so, so much worse.
So I don't know if we, those of us that are more educated about what AI is doing under
the covers, have any ability to shift the narrative even a little bit.
But hopefully if we're careful about our language, we might help, or at least we're not going to
be continuing to contribute to the problem.
So toward that end, I've started working on a new project.
Now, those of you that don't care about my possible future plans, thanks for watching
this far.
You can feel free to drop off now.
For those of you that are still here, so I started working on writing a software service
that will try to use a combination of web scraping and web searching and the things that AI can
actually be decent at, like summarizing text and rephrasing things, to create an automated
claim checker rephraser for articles and headlines.
To start with, it's going to be limited to just AI-related topics.
And if I can get it to work, I might expand it to other things we'll see.
The idea is you give it an article link, and then it tries to find the links that were in
that article, and then traces the links in those articles back until it finds something
that looks like a primary source or it runs out of links.
And then, assuming it found something that seems authoritative, it runs summarization on those
articles, and then it tries to evaluate if the headline of the article that was given in first
place is in line with the articles it found.
At that point, it's not the definitive answer, but it's going to set things up so there's
some bunch of things that a human can click on to verify that it did the right thing or
not, and then sign off on it and say, yes, that's right, or no, it's not.
I may utterly file to get that to work, but I'm going to try.
And if nothing else, it's going to help me reduce the amount of effort that it takes for
me to research videos like this.
The reason I'm telling you this now, though, when it's not even close to done yet, is that
I'm going to be trying to make that an educational project, too.
I've had a couple of videos already, including the one last week, that talk about becoming
a better developer by building your own software-as-a-service project.
And as I make more videos about that, I'm planning on using this claim checker project as an example
project.
That way, I'm not talking about software-as-a-service in a completely abstract thing and losing everybody.
There'll be some kind of concrete skeleton that we can hang the conversation on.
I'm not sure exactly how that's going to work.
We'll be playing it by ear, and I'll be trying to figure it out.
So feel free to follow along and let me know if you're interested.
Feel free to subscribe.
So until next time, try to think about the language that people, including you, use about
AI.
Be skeptical about the words that make the AI seem to be more deliberate, more intentional,
and more human-like than the fancy autocomplete generator it probably actually is.
Thanks for watching.
Let's be careful out there.
