Intelligence is very specifically
your ability to handle novelty,
to deal with situations you've not seen before
and come up on the fly with models
that make sense in the context of that situation.
And this is actually something
that you see very little of in LLMs.
If you ask them to solve problems
that are significantly different
from anything they've seen in their trainer,
they will fail.
The Abstraction Reasoning Corpus
for Artificial General Intelligence,
or RKGI for short,
you can think of it as a kind of IQ test
that can be taken by humans,
it's actually very easy for humans,
or AI agents.
Every task that you see,
every task you get is novel.
It's different from any other task in the dataset,
it's also different from anything you may find online.
RKGI is designed to be resistant to memorization,
and all the other benchmarks can be hacked
like memory alone.
When I've spoken to AI researchers,
I've gone through ARK challenges together with them,
and they are trying to look at their introspection.
So they're saying, I'm looking at this problem,
and I know it's got something to do with color,
I know it's got something to do with counting,
and then they run the program in their mind,
and they say, one, two, three,
no, that doesn't work, that doesn't work.
I think introspection is very effective
when it comes to getting some idea
of how your mind handles system two thinking.
I think it's not very effective for system one,
because system one is inherently not something
you have direct access to.
It happens like unconsciously, instantly,
in parts of your brain that you're not
directly observing via your unconsciousness.
But system two is not like that.
System two is very deliberate.
It's very slow, very low bandwidth.
It's very introspectable.
But what's not mentioned here is...
Fran√ßois Chollet, it's an honor to have you on the show.
Honestly, this means so much to me.
You're my hero, so thank you so much.
It's my pleasure to be here.
And I would say you shouldn't have heroes.
I shouldn't?
No.
Why not?
It makes for a disappointing experience.
Um, not for me.
Okay.
Yeah, not for me.
Hopefully I can live up to the expectations.
Definitely, I'm sure you will.
Um, Francois, I mean, you've been critical of the idea of scale is all you need in AI.
Can you tell me about that?
Sure.
So, yeah, so this idea that scale is all you need is something that comes from the observation
of scaling laws when training deep neural networks, which is, so scaling laws are this
relationship between the performance you see in deep learning models.
So, typically, LLMs, and how much data and compute went into training them.
And it's this sort of like logarithmic scaling of LLM performance as a function of training
compute.
Typically, that's how it's formulated.
Uh, many people are extrapolating from that that, well, there's no limit to, uh, how much
performance we can get out of these models.
All we need, uh, is to scale up the compute by a few orders of magnitude, right?
And eventually we get, uh, much beyond a human level performance, purely via scaling compute,
with no change, uh, in architecture, with no change in training paradigm.
And, well, the major flaw here is the way you measure performance.
Uh, in this case, performance is measured via, uh, exam-style benchmarks, uh, which are effectively,
uh, memorization games.
So, you're effectively, uh, measuring how good the LLM is at memorizing the answers to the questions,
uh, that you're going to test it on.
Not necessarily the exact answers, but maybe, uh, the sort of like, uh, program templates
that you need to apply to, uh, arrive at the answers.
And if you're measuring, uh, something that's fundamentally driven by memory, then it makes
sense that as you increase the amount of memory in the system, like the number of parameters,
the amount of trained data, uh, and compute is really just a proxy for that, you see a
higher performance.
Because, you know, of course, if you can memorize more, uh, uh, you're going to do better at
your memory game.
Um, and, uh, my take is that this performance increase that you're observing, it is actually
orthogonal to intelligence.
You are not really measuring intelligence because your benchmark can be hacked purely by preparing
for it, by memorizing things in advance.
Uh, if you want to, uh, benchmark intelligence, you need a different kind of game, a game that
you cannot prepare for, um, something like ARK, for instance.
And I think if you look at, uh, performance in ARK over time or the function of compute,
you don't see this relationship.
In fact, the highest performing models on ARK, uh, today, uh, did not require tons of compute,
uh, and some program search approaches actually did not require, uh, uh, any, um, training
time compute because they were not trained at all.
They, they do require some, uh, inference time compute, but it's not, uh, uh, very large amounts.
So you've, you said that, um, language models are interpolative databases and I've, I've spoken
with, um, Sabaro the other day and he calls them, um, approximate retrieval systems.
And many people say to me, Tim, this is ridiculous that of course, they're not databases, they do
extrapolation.
Um, but, um, I think as an intuition pump around memorization, that, that is what they
do.
And you wrote a Substack blog about this as well.
Yes.
Memorization is what they do.
I think, uh, the part where people get stuck is that when, when they hear memorization, they
think the LLMs are just memorizing, uh, answers to questions.
They are just memorizing content, right?
And of course they do memorize a lot of content, a lot of, uh, knowledge and factories and so on,
so that's not primarily what they do.
What they're primarily memorizing is functions, programs, and these programs do generalize
to some extent.
And, uh, there can be, well, free generalization.
And, um, when you query LLM, you are basically, uh, querying, uh, a point in program space.
So you can think of the LLM as a manifold where, uh, each point, uh, encodes a program.
Um, and, uh, of course you can, you know, uh, uh, interpolate across these manifolds to
compose programs or combine programs via interpolation like this, which means that you have an infinite
number of possible programs to choose from.
And what happens with LLMs is you are, uh, training them, training these very, uh, very rich, very flexible
models to predict the next token, right?
Um, and if you had, uh, infinite, uh, uh, memory capacity, what you could do is, of course, just
learn a kind of lookup table, right?
But in practice, uh, the LLM only has, uh, some billions of parameters, so it cannot just
learn a lookup table, uh, for every sequence, uh, in the string data, it has to compress.
And so what's actually learning is, uh, predictive functions that take, uh, uh, and, and they take
the form of vector functions, of course, because the LLM is a curve.
So the only thing you can encode with a curve is a bunch of vector functions, um, and so
you're learning these vector functions that take as input elements, uh, uh, of, uh, of,
uh, the, the, the entry sequence and, and output elements, uh, uh, of what comes after that.
Uh, like for instance, let's say the LLM comes across, um, the works of Shakespeare for the
first time, uh, but the LLM has already learned a model of the
English language.
Well, now the, the, the, the, the text that it's looking at is slightly different, but
it's still the English language.
Um, so it is possible to model it by raising a lot of, uh, uh, functions that came from,
uh, learning to model English in general.
Um, and it becomes much easier to model Shakespeare by just learning a sort of style transfer function
that will go from the model you have to this, uh, Shakespeare-sounding text, and it's kind
of like how you will end up, um, with things like the ability to do, uh, textual, uh, style
transfer with, uh, with an LLM, right?
It's because it turns out that, uh, it is more compressive, uh, to learn style independently
from content.
Um, and, uh, based, based on, on the same kind of model, uh, the LLM is going to learn
millions, uh, of, uh, of, uh, uh, independent, uh, uh, predictive functions like this.
And it can, of course, combine them via interpolation because they're all vector functions.
They're not like, uh, discrete programs like you might imagine, uh, a Python program, for
instance.
Yeah.
They're actually vector functions.
Because when you, when you say program, I think a lot of people think of a program as
being something with conditional logic and with an LLM.
That's not what they are.
They're...
Yeah.
It's, it's almost like in an input sensitive way, um, you see this kind of traversal through
the model and it's like a mapping.
Yeah.
So it's, it feels more like a...
It's an input to output mapping and that mapping is continuous and it is, it is implemented
via a curve.
But, but we can describe that as a program.
Yes.
Of course.
Yes.
They're functions.
Yes.
And they're, and you said they were compositional.
Yes.
Because, uh, these functions are vector functions.
You can, uh, sum them, for instance, uh, you can, uh, interpolate between them to produce
new functions.
I, I love this kaleidoscope hypothesis.
So can, can you, you know, dramatically introduce the kaleidoscope hypothesis?
Sure.
So everyone knows where the kaleidoscope is, right?
It's like this, uh, cardboard tube with a few bits of colored glass in it.
Um, and this, uh, this is just like few bits of, uh, uh, original information get, uh, mirrored
and repeated and transformed and they create, uh, this tremendous richness of complex patterns.
You know, it's, it's beautiful.
And the kaleidoscope hypothesis is this idea that, uh, the world in general and any domain
in particular follows the same structure that it appears on the surface to be, uh, extremely
rich and complex and, uh, infinitely novel with every passing moment.
And in reality, it is made from the repetition and composition of just a few atoms of meaning.
Um, and a big part of intelligence is the process of mining your experience of the world to identify
the bits that are repeated, um, and to extract them, extract these unique atoms of meaning.
And, uh, when we extract them, we call them abstractions.
And then, uh, as we build, uh, sort of like inner banks, uh, of such abstractions, then we can, uh, reuse them
to make sense of novel situations, of situations that appear to be extremely unique and novel on the surface,
but actually they can be interpreted by composing together these, uh, uh, reusable abstractions.
That's the fundamental idea behind intelligence.
Intelligence is a cognitive mechanism that you use to adapt to novelty, to make sense of situations
you've never seen before, and it works by creating models on the fly of the new situation
by combining together existing building blocks, abstract building blocks, which were mined, uh, from your past experience.
And there are, there are two key tricks here.
One trick is the synthesis trick, whereby you take these building blocks and quickly assemble them, uh, to form a program,
a model that matches the current task or the current situation that you're facing.
And there's synthesis, and there's abstraction generation, which is the reverse process,
in which you're looking at, uh, the information you've, you've got available about the world,
like your, your experience, your perception, also the models that you've created to respond to it.
And you're gonna turn that, distill it into reusable abstractions, which you then store, uh, in your memory,
so that you can use it, uh, the next time around.
So, synthesis and abstraction generation.
And together, they form intelligence in my model, at least, in my architecture of AGI.
So you've been prominent in the, in the AI space for many, many years now.
What experiences or insights led you to develop such a clear perspective of intelligence so early in your career?
Right.
So if you read, uh, some of my old blog posts or the, the first edition of, uh, my deep learning book,
you see that I started talking about how deep learning could do system one very well, but could not do system two.
And I started talking about the need for, uh, program synthesis, um, roughly in mid 20, 2016.
I mean, I started writing, writing by it a lot, uh, in 2017, but in practice, I started forming these ideas, uh, in 2016.
And there are several things, uh, that led me to it.
I think one of the big, uh, catalyst events was working on, uh, automated theorem proving using deep learning,
uh, with, uh, Christian Segedi.
Um, and the, the key idea was, you know, theorem proving is very, very akin to program synthesis.
Um, you, you, you're basically doing, uh, tree search, uh, with, uh, operators taken from a DSL.
And the key idea was, uh, to use a deep learning model to guide the search process.
And so I tried to do it, uh, for, uh, pretty long time, you know, trying and showing lots of different ideas.
And, uh, everything I was trying basically failed.
I mean, it was doing much better than random, but if you analyzed how it was performing
and how, uh, how it was, uh, uh, producing that, that ability to perform better than random,
it was just doing, uh, shallow pattern recognition, right?
It was not really doing any kind of system reasoning.
And it seemed like, uh, like a huge obstacle that I was just not able to overcome
by tweaking the architecture or the truing data or anything else.
Um, there was this pattern recognition shortcut available,
and this shortcut would be taken every single time.
You could not learn generalizable, uh, discrete programs, uh, via deep learning.
And that came as, um, as a big insight to me because, you know, before that point,
uh, I was, you know, like everybody else in the field.
I was under the assumption that deep learning models were a very general computing substrate,
that you could train deep learning models to perform, uh, any kind of computation
that, you know, they were, they were Turing complete, um, uh, that they were Turing complete.
And, um, around the same time, you know, 2015, 2016,
there were a lot of, uh, similar ideas floating around,
like the concept of neural Turing machine.
For instance, people thought, and I thought, uh, this was a very promising direction
that deep learning could ultimately, uh, replace, uh, uh, handwritten software, you know?
Um, so I, I subscribed to, to these ideas very early on,
but then, uh, in these experiments trying to get, uh, neural networks to do math,
I realized that actually they were fundamentally limited,
uh, that they were, uh, a pattern recognition engine,
and that if you wanted to do, uh, system two thinking,
you needed something else, you needed program synthesis.
So that's when I had this, uh, this realization,
I started talking about it.
Um, but in general, you know, I've been thinking about intelligence
and, and how to create it for, uh, quite a long time.
Like my first sort of like, uh, uh, AGI architecture,
uh, something I, I developed in, uh, back in 2010, uh, summer 2010.
So, uh, and, and the reason I developed it is because I was already thinking about it
for, for a few years before that.
So I've been, I've been, uh, uh, in the field for quite a long year.
Quick meditation on the shortcut rule, because I think this gets to the core of it.
Yes.
Deep learning.
I mean, basically like we're projecting into a Euclidean space,
a Euclidean space and the only semantic metric is the Euclidean distance.
And, you know, so, so these models learn a spectrum of spurious correlations
and perhaps more spurious than not spurious.
Sure.
So in general, um, the reason they, they're doing this is because spurious, spurious correlations
are always available to explain something, no matter what you're looking at.
There's always some element of noise, which you can wrongly interpret as being meaningful.
And it's also because, uh, deep learning models, they're, they're curves, um, meaning that
there are, um, continuous differentiable surfaces in a higher dimensional space.
Uh, and, and we are, we are fitting the parameters of these curves via stochastic gradient descent.
And a curve is, you can represent many things with a curve, but it's a very bad substrate
to represent any sort of discrete computation.
You can do it.
You can embed discrete processing on the curve, but it's just not a very good idea, right?
Um, it's not easy to fit generalizable discrete programs in this format.
And this is why you end up with things like the fact that it's tremendously difficult to
get, uh, a deep neural network to learn how to sort a list or to learn how to add, uh, two
sequences of, uh, digits, for instance.
Even LLMs, state-of-the-art LLMs, they have a very hard time doing it.
It's like they've been trained on millions of examples of adding digits.
But still, they are only achieving something like 70% accuracy on new digits.
So they've memorized a program to do it.
But because this program, uh, is a vector function, is embedded on a curve.
It is not a very good program.
It is not very accurate.
And you see this, uh, time and time again with any sort of, uh, algorithmic type processing.
And just for those of you at home, a piecewise linear function is still a curve.
Yeah.
People might get confused by that because they think of a curve as being this, this smooth thing.
But, um, they have to look at the Wikipedia definition of curve.
You're absolutely right.
It's still, it's still a curve.
You mentioned the neural, uh, Turing machine, which, um, actually isn't a Turing machine, of course,
but it, it behaves a little bit like one.
What, what do you see, um, is the gap there, you know, with neural networks not being Turing machine?
Fundamentally, I think, uh, fitting, uh, parametric curves ground descent is a good fit for, uh, what I call value-centric abstraction,
which is the idea that you're going to compare things via a continuous distance function,
which leads to the idea that you're going to embed things.
And by things, I mean, like, instances of something, like, could be images, could be, uh, discrete concepts, could be words, right?
Um, that's, that's, that's going to lead to this idea that you're going to embed them, uh, in, uh, uh, on a manifold.
So, a space where two things that are similar end up close together and different, uh, dimensions of variation on your manifold are semantically meaningful.
Um, you can do this with curves, with curves, because, um, the, the sort of flag continues.
It naturally leads you to compare things via, via continuous distance.
But that's a very bad fit, uh, for any kind of, uh, type 2 abstraction, like what I call program-centric abstraction,
where you're actually interested in graphs, and you're not interested in comparing graphs via a distance function.
You're interested in, uh, comparing when, uh, two graphs are exactly identical to each other,
or more precisely, when a graph appears to be a subcomponent of a larger graph.
Um, so, for instance, as a software engineer, if I'm refactoring some code, if I want to compress my code,
uh, by expressing, uh, uh, multiple functions as just one function,
I am not interested in how close the functions feel on the perceptual level.
I'm interested in whether they are implementing the exact, uh, program or, uh, in, uh, in, uh,
maybe in different forms, maybe I need to inject some abstraction in there.
Um, and this is a comparison that you have to do in a very, uh,
uh, explicit step-by-step way.
You cannot just look at two pieces of code and instantly say,
without having to think about it, oh, yeah, they look similar, you know.
And how would you describe that capability?
It's like a kind of epistemic risk rather than an aleateric risk.
Or, or verification might be a better way of describing it.
Yeah, verification, step-by-step verification is a good way of describing it.
And, um, you know, uh, I just said, it's definitely not like this sort of like perceptual, uh, continuous distance style comparison.
And that's true.
But I think it can also be guided by perception.
It's like doing this, uh, step-by-step exact comparison is very costly.
It requires, you know, all of your attention, uh, expanded over some length of time.
Uh, so you are not going to want to do it, uh, kind of, you know, in a brute force-like way over many different possible candidate functions.
You want to use your intuition to identify just a small number of options.
And, and these options you're going to try to verify exactly.
So I do think we have, um, the ability to do approximate, uh, distance comparisons between, uh, discrete objects.
But the, the key thing to keep in mind is that, uh, these, uh, these, uh, fast, uh, comparisons are not exact.
Right?
They're approximate.
So they might be wrong.
And I think you get, um, the, the same type of, uh, outputs from an LLM if you're trying to use it for programming.
Um, they, they will often give you things that feel right, uh, but aren't exactly right.
And in general, I think that's the thing to keep in mind when using deep learning or when you're using LLMs is that they're very good at giving you things that are directionally accurate.
but not actually accurate.
So if you want to use them well, you need, uh, uh, uh, post-facto verification step.
So, um, watching your children grow up, how has it influenced your thinking on intelligence and learning?
One thing you notice when you watch, uh, children grow up is the fact that constructivism, uh, is, uh, entirely right.
That, uh, you, uh, they learn things, uh, in a very active manner.
They try things out.
Uh, and from these expenses, these very deliberate expenses, they extract new skills, which then they, they, they reinvest, uh, in, uh, in, uh, new goals.
And in general, you know, you see pretty clearly that learning, learning in general, but especially in children is structured in whether we describe it as a series of feedback loops, where the child will, uh, notice something interesting, come up with an idea, set that as a goal.
Like imagine you're, you're, you're there on the floor, crawling, then you notice something that looks, uh, intriguing.
So you're like, Hey, I'm gonna grab it.
Right?
So that's your goal.
And now you're entering this sort of feedback loop where you're trying to, uh, reach that goal.
You're doing something towards it.
Then you get some feedback and you're evaluating, right?
You have this sort of like, uh, plan, action, feedback, back to plan, uh, loop.
And, uh, if you reach the goal, then in the process, you will have learned something and you will be able to reinvest, uh, that, that new skill, uh, in, uh, in your next endeavor.
Um, and the way, the way they set goals is always grounded in the things they already know about.
And you start not knowing much, like when, when you're born, you're animated by just a few reflexes.
Um, but when, when you start forming these goals, they always come from, uh, from this layer that you've already mastered.
And you're, you're building your own mind kind of like layer by layer.
Like at first, for instance, um, one of your most important sensory motor affordances is your mouth.
Because you have the sucking reflex, which is extremely important.
It's something that you're born with.
It's not something that's acquired.
It's extremely important because it's high feed, right?
Um, and you also have the, the things like the Palma grasp reflex for grabbing things.
Uh, but you cannot really use it yet because, uh, you, you are not in full control of your limbs.
So you cannot really like grasp, grasp things.
Um, but, um, when you start being more in control of your limbs, you will want to grasp things.
And the reason, the first thing that you, that you try to do after you, you, you grasp a thing
is you bring it to your mouth to suck it.
Because you set this goal, um, because it sounded interesting, uh, with respect to the things you already know how to do,
with the things you already find to be interesting, right?
And once you know how to grab things, you're gonna add that to your, to your, to your world,
so sort of like inner world, and you're gonna build the next layer on top of those things.
So next thing, uh, you, you're learning to crawl, for instance.
Why do you crawl?
Why, why, why are you trying to move forward?
Because you saw an object that seemed interesting that you want to grab.
So you are learning to crawl, to grab something.
You are learning to grab, to put it in your mouth.
And you're not learning to put things in your mouth because it's already something that's, that's hard-coded.
Um, so you're, you're sort of like constructing yourself in this sort of like layer-wise, uh, fashion.
So basically everything, everything you know, everything you think about,
is built upon, uh, lower, lower level primitives, which are built upon lower level primitives and so on.
And ultimately it comes back to, uh, these extremely basic sensorimotor affordances that, uh, newborn children have.
I do believe we construct, especially, you know, young children, they construct their thoughts based on their sensorimotor experiences in the world.
You, you have to, you, you cannot think in a vacuum.
You have to construct thought, to construct thoughts out of something.
Uh, and that something is extracted from your experience, right?
And the younger you are, of course, the more, uh, grounded, uh, your thoughts are.
They, they, they, they relate more directly to the things you're experiencing and doing in the world.
As you get, you know, older, your thoughts will get increasingly abstract, increasingly disconnected from physicality.
But they are ultimately, you know, built upon the physical layer.
It's just that, uh, the, the tower of layers, uh, has gotten so tall that you, you, you cannot see the ground anymore, but it's, it's still connected.
So children see the kaleidoscope and the kaleidoscope is created from abstractions in the universe.
And then children over time derive abstractions from the kaleidoscope and reason over them.
Yeah, they, they notice, um, bits in, in their experience, uh, or their own actions that appear to be reusable.
Uh, that appear to be useful to make sense of, uh, novel situations.
And as you go, you're building up these, uh, vast libraries of reusable bits.
And having access to them makes you really effective in making sense of new situations.
And, and you said constructivist, which is quite interesting.
So do, do you think children construct different abstractions?
Or do you think there's a kind of attractor towards representing the abstractions which the universe came up with?
I mean, do different people come up with different models?
Uh, to some extent, probably yes.
Uh, but because these models, they, they're ultimately extracted from the same kind of experiences
and they're extracted via the same kind of process, they will end up being very similar, I would think.
I mean, you, you do, you do definitely see that different children follow slightly different developmental trajectories,
but ultimately they are all somewhat parallel.
They are, they are all roughly following the same stages, maybe with different timing, you know.
So another interesting thing you've said is, you know, language models have near zero intelligence.
And I just wondered if it's near zero, which part of it is not zero?
Sure.
Yeah.
And, you know, people, people think that it's a very provocative statement because they're using LLMs all the time.
They find them very useful.
Uh, they seem to make sense.
They seem very human-like.
And so I'm like, hey, they have near zero intelligence.
And that sounds kind of shocking.
But the, the key is to understand that, um, intelligence is a separate concept from skill,
from behavior, uh, that you can always be skilled at something without necessarily being, uh, intelligent.
Uh, and intelligence is very specifically your ability to handle novelty, to deal with situations
you've not seen before, uh, and come up, uh, on the fly with models that make sense
in the context of that situation.
And this is actually something that you see very little of in LLMs.
Uh, if you ask them, uh, to solve problems that are significantly different from anything
they've seen in their training, they will fail.
Um, so that said, if you define intelligence in this way and you come up with a way to benchmark it, uh, uh, like,
RKGI for instance, and you try LLMs, like all the state-of-the-art LLMs on it, uh, they don't have zero performance.
Right?
Um, and so this is where the non-zero, uh, uh, part of my statement comes from.
So that said, you, it's not entirely clear whether that non-zero performance, that ability to adapt
to novel problems, uh, is actual intelligence or whether it's a flaw of the benchmark.
Maybe the benchmark was not actually producing, uh, entirely novel problems.
Maybe there was very significant overlap between this or that question and something that the LLMs
has seen its training data.
It's very difficult to control for that because the LLM has just memorized so much.
It has seen, you know, pretty much the entire internet plus, uh, tons of, uh, uh, data annotations
that were created specifically, uh, uh, for, for, for that LLM.
Um, and we don't know fundamentally what's in the training data.
So it's kind of, it's kind of difficult to tell, but it does seem to me that LLMs, uh,
are actually capable of some degree of recombination of what they know to adapt to something that
they've genuinely not quite seen before.
Uh, it's just that the, um, degree of this recombination, their generalization power is
very weak.
It's very low.
Yeah.
This gets to the core of it because a lot of people argue that this combinatorial creativity
or this cone of extrapolation does constitute novel model building.
And I interpreted what you said as, you know, if we zoom out and think of the training process
as well, that, that obviously is model building.
Yes, obviously.
It's just, uh, gradient descent, like fitting a curve to a data set via gradient descent is
model building.
Uh, the major flaw there is that it's very inefficient, uh, model building.
Uh, it requires to get a good model, you need a dense sampling of pretty much everything, uh,
uh, the model is going to have to deal with at this time.
So the model is effectively only displaying weak generalization.
It can adapt to things it has not seen before, but only if they remain, uh, very, very close
to things it has actually seen before.
And, um, where intelligence comes into play is the ability to adapt to things that are
way out of the, of the distribution, because the real world is not a distribution, right?
Uh, every day is new, every day is different, but you have to deal with it anyway.
Critics will say, and, and I can, I can empathize.
I mean, I, I use Claude Sonnet all of the time for my coding.
I, I, I, I'm paying for about, I don't know, 2000 requests a month on, on cursor.
So I'm using it a lot and it appears clairvoyant in many cases.
And they would argue, I'm sure that, well, because it's trained on so much stuff, the convex
hole is, you know, enough to capture any novelty we, we might need.
Therefore, what's the problem?
Sure.
That's something I hear a lot.
Uh, this idea that, yeah, maybe novelty is overrated.
I just need to train on everything.
This idea that, yes, there, there can exist a dense sampling of everything you might ever
want to do, everything you might ever want to know.
So, I mean, I disagree with that because imagine you were, you were training at a LAMS,
uh, 10 years ago and you're trying to use them now.
Uh, they're not going to know about, uh, the programming languages that you're using.
They're not going to know, uh, about all the, all the libraries and so on.
They're certainly going to seem, uh, much less, uh, intelligent just because, uh, there's
this, uh, gap in your knowledge.
The world is changing all the time.
And you could say, well, but what if you just retrain the model, uh, on, uh, freshly
scrapped, uh, data, uh, every single day?
I mean, sure, uh, you, you can do that and this will, uh, address, uh, some of these problems.
But still, it's likely that at some point you will come up with, uh, problems that are
actually novel, problems that don't have a solution, uh, on the internet.
Uh, and that's where you need intelligence, right?
And I'm actually quite confident that, uh, at some point in the future, maybe in the near
future, uh, we'll be able to create a system that can actually, uh, address, uh, this, uh,
this, uh, issue of novelty that can actually take what it knows and recombine it, uh, in,
in truly original ways to address completely new problems.
Once we have a system like this, uh, we can start, uh, developing new science, for instance.
Like, um, one of the things you cannot do with LLMs today, uh, is develop, uh, new science,
right?
Because the best they can do is, uh, speak, speak back to you some interpolation of something
they've read, uh, online, right?
Um, they're not, they're not gonna, uh, set you on the way to some grand discovery.
Again, the devil's advocate on that.
I agree that the creativity and the reasoning comes from the prompter.
And because we anthropomorphize the models and we, we miscredit the role of, of the human,
but still inside that addressable space in the LLM with a human supervisor, I'm sure we can, um,
creatively explore the convex hull of what is known, perhaps not create new things.
Sure.
Uh, you can do that.
And that's a process, you know, as, as you said, to be driven by, uh, you, the human,
because you, you are gonna be the judge of what's interesting versus what's nonsense.
And without this sort of external verification, uh, it's, it's difficult to make good use of LLMs.
In general, you know, that's, that's, I think that should be the thing you always keep in mind
when using LLMs is that they are very good at making, uh, useful suggestions,
but you should never blindly trust the suggestions they make.
Uh, especially if it's, uh, something like code, right?
You should always, uh, use it as a starting point, but verify, like make sure it's actually correct.
LLMs are very good at putting you, uh, in the right direction, but they're not very good at putting exactly correct answers.
And that's, that's why if we look at all of the successful implementations of LLMs or applications,
they always have a human supervisor in the loop.
Yes.
Or it could also be, uh, an external verifier.
Like sometimes, uh, the verification process is something that you can delegate to a symbolic system.
So, so now is a great segue for intelligence.
Now, um, fans of the show will know Yannick and I have already made about eight hours of content
on your measure of intelligence paper back in the day.
We, we poured through it and it's, it's fascinating.
But could you just briefly introduce it now just to give a refresher?
Sure.
So my definition of intelligence is, uh, skill acquisition efficiency.
So it's this idea that intelligence is separate from skill.
So if you have a benchmark that just measures, uh, the skill of an AI at something,
it is not a benchmark of intelligence.
It is always possible to score high without actually displaying any intelligence whatsoever.
Uh, if you want to actually measure intelligence, you have to look at how efficiently the system, uh,
acquires new skills given, uh, a limited amount of data.
So you have to control in particular for the data that the system has access to.
Um, and, uh, which usually takes two forms, you know, it can take the form of priors,
like the information that, uh, the system only has access to before it's looking at, uh, your benchmark.
And then experience, which is the amount of information that the system will extract from the task,
uh, the benchmark that, uh, you're, you're giving to it.
Um, and so if you control for priors, you control for experience and you measure skill,
uh, then you have some measure of skill acquisition efficiency,
the information efficiency of, uh, uh, the acquisition of high performance on a novel task.
Um, and that's, uh, uh, something that I've tried to turn into a concrete benchmark
and that was the ArcGGI dataset.
Just a quick point on that.
Is one of the potential issues with the measure of intelligence is that it's non-computable
because we can't represent the domain of all possible tasks?
Sure.
Um, so my, in, in, in the paper, I had this, uh, formalization, uh, of, uh, my measure of intelligence.
Um, and it is non-computable, um, its purpose is not to be used as a, a practical tool.
Like you're not going to actually want to run this equation, uh, on, on a system and get a number out of it.
Uh, it is a formalism, uh, that's useful to think about the problem of intelligence precisely, right?
It's, uh, it's a cognitive device.
It's not a practical device.
For, of course, so that there was this wonderful figure, which will show up on the screen now,
which is, you, you, you described the intelligence system as being a thing which produces skill programs
while adapting to novelty.
But one, one thing I was wondering though, is you're, you're talking about it as a kind of meta-learning prior
and do humans come with the meta-learning prior baked in, or is that something we also learn?
And should it be the same for AI systems?
Yeah.
So that's, that's a very important question.
Um, so intelligence is, uh, it's not skill.
It's a kind of meta-skill.
It is the skills through which you acquire new skills.
And is this meta-skill also something that is acquired through experience, or is it something that you're born with?
That's, uh, that's, uh, that's, uh, that comes, uh, hard-coded in your brain, uh, so by evolution presumably.
Um, I think the answer is that it is both.
I think you are born intelligent.
So you are born with, uh, this, uh, skill acquisition mechanism.
But this skill acquisition mechanism does not, uh, operate in a vacuum.
It actually needs, uh, so it, it, it, it, it's, it's composed of two bits, right?
There's the synthesis engine, which, um, take, takes a look at, uh, a new situation, a new task, and we'll try to, uh, combine, uh, existing parts, existing abstractions, uh, into a model for that task, for that domain.
Um, and there's the, uh, the abstraction engine bit, which looks at the models that we have produced so far, looks basically at the, the, the information available, and we'll try, uh, to produce reusable abstractions to be, to be added back, uh, to the library that's going to be, uh, used by the synthesis engine the next time around.
And, um, the, this library, of course, is acquired through experience.
And the better your library of abstraction, uh, becomes, uh, the more effective you are, uh, at synthesis, the more effective you are at acquiring, uh, new skills efficiently.
Uh, so I believe that this, uh, sort of, like, macro-level architecture of intelligence is something that you are born with.
But as you use it throughout your lifetime, you are getting better at it.
You are polishing it.
So you're not acquiring intelligence, uh, as a skill from scratch.
But you are polishing it.
Uh, another mechanism through which I think you're, you're, you're polishing it is that, um, the synthesis mechanism is probably incorporating learned components.
So that synthesis is itself, uh, synthesis from existing abstractions is itself a skill.
And you are getting better at it as you use it.
So I think for instance, uh, 15 year old is going to get better.
It's going to be better at skill acquisition than a 10 year old.
Um, this is really interesting because in a way you're combining rationalism, nativism with, with empiricism, because I think you're saying that there is the creation of de novo skill programs that are not just compositions of the fundamental ones.
But the, the broader question as well is we do this library learning.
So children develop, um, they, they finesse, they refine, they build these abstractions.
And surely there must be some trade off with complexification because you don't want the library to be too big.
No.
Right.
Because then you can't do search with it anymore.
So is there some kind of, um, pruning or does it converge on a certain side?
Is, is that the reason why our cognitive development seems to kind of plateau at, at a certain point?
Um, that's quite possible.
Um, you know, that, that, that, that's, that's actually a very, very deep question.
It's also very practical, I think, uh, to building an AGI.
So your AGI is going to have this library for usable, uh, primitives.
Do you want to expand the size of this library indefinitely?
Or do you want to cap it, uh, at some number?
Like you want at most one million programs in it or something like that.
So clearly our ability to efficiently acquire new skills, our intelligence, uh, does not, uh, improve over our lifetime in a, in an unbounded fashion.
Uh, it seems to peak, uh, relatively early on.
I think there's actually, uh, a trade off here, which is that your, uh, raw brain power, like for instance, the, the, the amount of, uh, information that you can integrate in your mind at any given point, uh, kind of, kind of trends down as you age, uh, inevitably.
Um, but the quality of the abstractions that you, uh, uh, work with and also your intuition for how to combine them.
So the, the learned components of the synthesis engine, uh, they do get polished over time.
Uh, they do get better over time.
So you have this, uh, kind of factor that makes you smarter and this factor that makes you dumber.
Um, uh, you know, empirically I think intelligence probably peaks, uh, in your early twenties.
That's when, that's when, uh, you're the most, the most efficient in acquiring new skills.
Um, but then again, you know, it depends.
Uh, I think, uh, uh, higher level, uh, cognition, uh, peaks probably in your early twenties.
But there are, uh, things that you should be learning, uh, earlier than that, right?
Anything.
So, you know, I mentioned like cognition is built layer by layer.
Each layer is built on top of the previous one.
Uh, the lower layers in the stack, they crystallize, they, they're set in stone relatively, uh, uh, early, uh, before 15 typically.
So if you want to acquire any kind of skill that deals with, uh, low-level sensoriometer primitives,
like you want to get really good at playing an instrument, you want to get really good at singing,
you want to acquire a native accent in some language, you should do it before you're 15 typically.
Yes.
I mean, on, on the, um, the abstractions, you could argue that, that it's, it's kind of limited by a computational bound,
or you could argue that it's just converging towards universal abstractions.
But I wanted to comment on what you just said.
Personally, I think knowledge is very important.
So I've, I've spent years doing this thing with Keith Duggar, who's one of the smartest people I know in the world.
He did his PhD at MIT and he's taught me how to be smart.
Just the way he thinks about things.
He has, I've reprogrammed my brain and I'd much rather be like this than go back to my early 20s.
Acquired better abstractions.
Much better abstractions.
And, but then again, I can give counter examples.
I've spoken with, um, uh, I don't want to mention any names, but, but sometimes professors who lean too much on their knowledge and not their fluid intelligence,
they can seem quite entrenched.
And so too much knowledge and not enough fluid intelligence can be a bad thing as well.
There seems to be some kind of optimal balance.
Yeah.
So it depends whether, um, you're relying on, it depends on whether you believe you already have the answers to the questions or whether you believe you, you have, uh, templates that you can use to get the answers.
Um, gaining better templates for problem solving or even for, for generic learning, um, that, that makes you more intelligent.
That's one of the, uh, points of education.
Like if you learn math, you learn physics, you learn programming.
Now you have all this, uh, these meta level templates for problem solving that make you more effective at problem solving, that even make you more effective at learning.
I think at 20, I was much more effective, uh, both in the, in the, in the, the methods I was using, um, in, in my approach at language learning than I would have been at, uh, 12.
Even though at 12, I had, uh, you know, more, more brain plasticity and more, more memory.
It was easier to retain things, uh, but I did not have the right, uh, tool sets, pretty much.
And that tool set is very much required.
Um, if you think you already have all the answers, then you're not going to be, uh, looking to create anything new or looking for new information.
And maybe that's the, the pitfall that, uh, uh, some intellectuals kind of fall into.
Uh, they, they think they've got everything figured out, so they don't need to, to search any further.
Uh, but instead if you're just, uh, carefully collecting and curating, um, ways to solve problems or like interesting ideas,
and you're not, not quite sure how you're going to use them yet, but they, they sound, uh, useful, they sound intriguing.
Um, and then you're faced with something new.
You're going to look into your library, look for, for the, the best sort of, like, uh, thing to connect it to.
Uh, that's how you get insights.
Like, if you're, uh, if you keep all these things in mind and then you come across something new,
instead of ignoring it because you already know everything or you think you know everything,
you're going to try to connect it with, uh, this sort of, like, uh, uh, things in your mind that are waiting for the click, you know?
Uh, and then that's how you get, uh, uh, big Eureka moments, you know?
Yes, the templates become activated, but I can give an example actually with your measure of intelligence paper.
I spent weeks studying that paper.
Um, I, I, I read it so carefully and so deeply.
And I remember there were a lot of ideas in it that I struggled with.
And now I could read it.
I could just flick through it and I just got it.
And actually it's the same with many other papers because you learn these abstractions.
And on MLSD we've always focused on the abstractions, but maybe there's a cost to that because I'm just,
a cognitive pathway in my brain is just lighting up and then, and I understand it, but maybe there's something else I'm missing.
Sure.
I think, you know, by, uh, sort of like abstracting away, uh, the details, you're able to focus on the bigger picture,
the, the third or the fourth time that you're reading it.
Um, and, and then you kind of find something new at a higher level.
Yeah.
You know, you don't get stuck in the details.
So at the end of the measure of intelligence paper, it was from 99, right?
You, you introduced the arc challenge, the abstraction and reasoning, uh, corpus.
Can you, can you bring that in?
Sure.
So yeah, it's from, from 2019, uh, the abstraction and reasoning corpus, uh, it's, uh, a dataset,
a benchmark that tries to capture, uh, the measure of intelligence that I outlined in a paper.
So, um, it's basically an IQ test for machines, but it's also intended to be easy for humans.
It's a set of tasks, uh, that are reasoning tasks.
So each task, you get, uh, a couple, like typically two to four demonstration examples,
which are, uh, the combination of, uh, an input image and an output image.
And the input image is, uh, basically a grid of colors.
They are pretty small grids, uh, typically like from five, five by five to 30 by 30,
30 by 30 is the largest.
Uh, and so you're seeing some patterns in this input grid.
And then, uh, you're told that it maps to a certain output grid with some other pattern.
And so your job is to figure out what is the transformation?
What is the program that goes from input to output?
And you get, uh, a few pairs, input, output pairs like this to learn this program on the fly.
And then you are given a brand new input grid.
And you must show that you've understood the program by producing yourself the corresponding output grid.
And, um, it's pretty easy for humans.
Uh, for instance, the, the, so the, the dataset is split into different subsets.
There, there's a public training subset, which is generally easier.
It's intended to demonstrate, uh, the sort of core knowledge priors that the, the, the tasks, uh, are built upon.
So core knowledge is, is another important concept here.
Um, I mentioned the, the, the, the, the grids feature patterns.
Well, these patterns must be referring to something, you know.
Um, and, um, in, in, in, in order to build anything, you need, you need building blocks.
So these building blocks are core knowledge, which are, uh, sort of like,
sort of like these knowledge priors that all humans are expected to have mastered by age roughly four.
So there are going to be things like objectness, like what is an object, basic geometry,
like, you know, symmetries, rotations, and so on, but basic topology, like things being connected.
Um, uh, uh, agentness as well, like goal and goal directedness.
So, uh, just, uh, uh, these very simple core knowledge systems.
And everything, uh, uh, in the RKGI tasks is built upon these, uh, atoms of knowledge, right?
Um, and, uh, so the, the training subset is just intended to demonstrate what core knowledge looks like
in case you want to apply a machine learning approach.
And instead of, you know, hard coding core knowledge, you want to, uh, learning from, uh, from the data.
Then there's a, uh, public validation subset, which is intended to be as difficult, uh, as the private, uh, private test set.
Uh, so it's intended for you to test your solutions and see what score you get.
Uh, and then there's the, the private test set, which is what we actually evaluate, uh, uh, the, the competition on, uh, on Kaggle.
Uh, and, uh, it's pretty easy for humans because we had the private test set, uh, verified by two people.
Uh, uh, and, uh, each one scored 97 to 98%.
So there are only 100 tasks in the private test set.
So it means they actually solved, uh, with no prior exposure, uh, 97 to 98 tasks out of 100.
And together they get to 100, right?
So the tasks that, uh, uh, each did not solve, uh, actually had the, uh, no, no overlap.
Um, so that shows that if you're a smart human, you should be able to do pretty much every, every, every task in the data set.
Uh, and, uh, it, it turns out, uh, this data set is tremendously difficult for AI systems.
Um, and so I released this in, uh, 2019.
Today, the state of the art, uh, was actually achieved, uh, uh, earlier this morning.
It's, uh, 46%, right?
Yes.
Nice one, Jack and team.
Yes.
Mohamed, uh, Jack and, and Michael.
Congratulations, guys.
Yeah.
Congrats.
Um, so, and, uh, so, oh, by the way, uh, there's actually, uh, an approach that's not public, but that has a proof of existence.
Uh, which should do, uh, 49% at least.
49% is, uh, what you get if you merely ensemble every entry that was made in the 2020 iteration of the competition.
Wow.
Why has nobody done that then?
Um, well, it's not exactly, uh, apples to apples, right?
Because we are talking about hundreds of submissions.
Uh, each submission was using, uh, some slightly different tweak on brute force program search.
Uh, but you have hundreds of them.
And each one was consuming some number of hours of compute.
So, even if you had all the notebooks for all these, uh, uh, submissions, uh, and you put them into one, uh, mega notebook, it would actually take too long to run it in the competition, right?
So, in a way, you are, uh, by assembling the submissions, you are, in a way, um, scaling up brute force program search, uh, to more compute, uh, and, and, and you're getting, uh, better results.
You know, in the limits, uh, if you had infinite compute, you should be able to solve ARC purely via brute force program search.
Right?
Um, it is definitely possible to produce, uh, domain-specific languages that describe, uh, ARC transformations in a relatively concise manner,
in a manner so concise that you would never need more than, like, 40, uh, different transformations to express, uh, a solution program.
Um, and, uh, and you're gonna have, like, you know, 200 primitives in your DSL.
Um, well, uh, just, uh, uh, uh, finding, uh, every possible, uh, program that's, uh, 40 operations deep, uh, out of, uh, a DSL of 200.
If you had, uh, infinite compute, you could definitely do that, right?
Well, there's an interesting discussion point on that.
I think I raised this with Ryan and Jack, which is that even if you did have an infinite amount of computation,
it's, there's, there's still a selection problem because you, you could select based on complexity, for example.
Selection is comparatively easy because you can simply, so, uh, for, for, let's say you have infinite compute.
So for each program you get, well, technically you get an infinite number of matches, right?
Uh, but let's say realistically you get like 10, uh, you can simply pick, uh, the simplest one.
Uh, the simplest one, like the shortest one.
But is the simplest one a good heuristic?
Uh, empirically, yeah, it seems to be.
Occam's razor, it seems to work in practice.
Because the other potential weakness is, um, I mean, you mentioned Elizabeth Spelke,
and folks at home, you should read, uh, she's from Harvard, she's a professor of psychology.
And, uh, you know, she came up with those, with those core knowledge priors.
But I think you're coming at this very much from the psychology school of thought,
which is that we should, um, understand the psychology of the human mind and build AI around that.
Is that fair?
Yeah.
So I'm, uh, a little bit cautious about the idea that AI should try to emulate human cognition.
I think we don't really understand enough about the human mind, uh, for that understanding to be a useful guide when it comes to creating AI.
So I have my own ideas about how, uh, how to, how intelligence might work and, and how to create some software version of it.
But it's only partially derived from, you know, introspection and looking at people.
Interesting.
And the reason I said it might be a potential weakness is let's say we select the lowest complexity program,
we have an infinite amount of computation.
We, we do the program synthesis.
And then we assume that because all of the generalization space would be in the kind of, um, compositional closure of the priors that we start with, then it will work.
Yes.
But, but that is an assumption.
Sure.
But it's a reasonable assumption.
You could also train a system to judge, uh, whether a given program is likely to generalize or not.
It will use length, uh, uh, on, on the DSL as, uh, one of its features, but not the only feature.
One of the other really important things about the ARC challenge is task diversity.
And the reason we need task diversity, I think if I understand correctly, there are about 900, uh, tasks in, in the original, um, ARC challenge.
Now you spoke about developer aware generalization.
What is it and why is it so important?
Right.
So developer aware generalization, uh, is deciding that, well, if generalization is the ability to adapt to things that are different from the things you've experienced before, um, then it kind of matters what a frame of reference you're taking.
Are you taking the frame of reference of, uh, the agent?
Does it matter if this agent is able to adapt to things that it has not in person experienced before?
Or do you take the frame of reference of the developer of the agent?
Uh, are you trying to get the agent to adapt to things that the developer of the system could not have anticipated?
Um, and I think the, the correct frame of reference is the frame of the developer.
Because otherwise, uh, what you end up with is the developer is going to, uh, build into the system, either via, uh, hard coding or via pre-training, um, the right kind of, uh, models and data so that the agent is going to be capable of performing very well.
But without actually, uh, demonstrating any kind of generalization, just by leveraging the prior knowledge, uh, that, uh, that, that is built, uh, into it.
The current, um, ARC benchmark, I just wondered if you could comment on, on its weaknesses, but just to cite a couple of examples.
Melanie put, Melanie Mitchell put a piece out saying that it should be a, a moving benchmark and Dilip George put an interesting piece out saying that it might be perceptually entangled in, in a way that, that we might not want.
So what are your reflections on the potential weaknesses of it?
Sure.
Uh, I mean, ARC-HDI is a first attempt at capturing my measure of intelligence.
Uh, it's pretty crude attempt because of course, you know, I'm, I'm, uh, technically limited in what I can produce.
And, uh, it, it has of course, uh, pretty, pretty strong limitations.
So I think the first limitation is that, um, it might be falling short of its goals in terms of how much diversity there is into it and, uh, uh, how much, uh, novelty.
So some tasks in, uh, uh, uh, version one of ARC-HDI, because by the way, that's going to be version two as well.
So some tasks are actually, uh, very close to each other.
There's some redundancy.
Um, and they might also be very close to things that exist online, some of them, and which might be actually one of the reasons why you see, uh, LLMs, uh, uh, uh, able to solve some percentage of ARC.
Or maybe they're actually doing it because they've seen similar things, uh, in their training data.
So I think that's the main flaw.
Um, and, um, so, yeah, so Melanie Mitchell mentioned, you know, a benchmark like this should be a moving benchmark.
I actually completely agree.
Uh, I think ultimately, uh, to measure intelligence, you're going to want not, uh, a static data set.
You're going to want a task generation process, um, and you're going to ask it for a new task.
It's going to, it's going to be capable of giving you something that's very unique, very different, uh, handcrafted just for you.
It's going to give it to you and then, um, it might try, for instance, to measure, uh, how data efficient you are in solving the task.
So it's, it's first going to give you maybe one or two examples, uh, going to challenge you to figure it out.
Um, and if you cannot, then maybe it can give you a couple more and then a couple more.
And that way, so the reason why something like this would be interesting is that you can start benchmarking, um, approaches that are very low intelligence.
Like for instance, uh, curve fitting via gradient descent, uh, technically curve fitting via gradient descent is a kind of program synthesis.
So you should be able to apply it on arc.
Uh, the main reason why you cannot is because for each, uh, task you only have a couple examples and, and the space is not interpretive.
So it doesn't really work, curve fitting doesn't really work.
But if for each task you had one thousand examples, for instance, it could be conceivable that you could fit a curve, uh, that will generalize to, to novel inputs.
Um, well, if you have this, uh, dynamic tasks generation and example generation system, then you can start benchmarking, uh, uh, techniques like this.
And it will be interesting because then you can start, uh, grading, uh, on the same scale, um,
uh, fitting a transformer for a gradient descent versus program search, uh, brute force program search, heuristic program search, deep learning guided program search, and so on.
And then you can start seeing, um, very concretely what it means, uh, to be more intelligent, what it means to be more, uh, uh, data efficient in your ability to produce generalization.
And the other thing that you can start creating, uh, uh, when you have, uh, uh, uh, this, this sort of dynamic, uh, benchmark generation processes,
you can start grading how much generalization power, uh, different systems have.
Uh, so you can, uh, you can measure how data efficient, uh, uh, your, your synthesis, your model synthesis processes,
but also how much, uh, generalization power the output model has.
Because you can, uh, challenge the test taker with different inputs that will be more or less difficult.
So you, you start at the lowest level by demonstrating a task with very few examples.
And let's say for instance, very, uh, very simple, uh, test inputs.
And, uh, as you go further, you're gonna add more examples to kind of refine the constraints of the problem.
But you're also gonna, uh, send the test taker, uh, much more difficult, uh, examples of the problem, uh, to kind of test how far it can generalize.
Uh, or how complex, uh, uh, the models it can produce can be.
I love this idea of a, of a generative arc.
And I could see...
Ultimately arc will be a generative benchmark.
Yes.
And, and I guess that is similar to the way things work in, in the world.
So there's a generative function of the universe.
It produces the kaleidoscope and we go backwards from the kaleidoscope to the generative function.
But knowing this is the thing, like we, in this intelligence process,
we need to know what the priors are.
And the priors must be either fundamental or deducible from the fundamental priors that were there in the first place.
Yes.
That's right.
And, you know, I think the big pitfall, uh, to avoid here is...
And that's actually the reason why, um, I did not, uh, release arc 1 as a generative benchmark.
This was, by the way, uh, the first direction, uh, I investigated when I was, uh, trying to come up with the thing that eventually became arc.
Um, I was thinking that I would, I would create a program synthesis benchmark where, um, the, the, the test examples would be created by some kind of master program.
Um, and, uh, I investigated many different directions, uh, things like, uh, cellular automata and so on.
Like for instance, you're, you're given the output of cellular automata and you need to reverse engineer, uh, the rules that produce it, that sort of thing.
Um, and ultimately, so I did not go with that for several reasons.
So one reason is that I wanted the tasks to be easy, intuitive for humans.
And, uh, that's actually difficult to achieve in this way.
Uh, I also wanted to avoid formalizing, uh, too much, uh, of the core knowledge because, uh, any, uh, um, formal formulation of core knowledge might be losing something,
might be missing something important that you cannot really put into, uh, into words, but that is there.
And, uh, also because, and that's very important, if you just write down one master program and let it generate, uh, your dataset,
then the complexity of the tasks in your dataset is fundamentally limited by the complexity of the master program.
And so as someone trying to solve the benchmark, the only thing I have to do is reverse engineer the master program.
And then I can use it, for instance, to generate, uh, infinitely many tasks that it can fit, uh, could fit a curve to.
Uh, or I just, uh, hard code the system that already understand, already understands how this, uh, uh, uh, master generative function behaves
and can anticipate it, right?
So I can hack the benchmark.
Um, and that's why ultimately I ended up with this model where every task, uh, in Arc 1 is actually handcrafted, uh, by me in this case.
And I, I think, you know, that's, that's touching on, uh, on something that, uh, um, is, is, is subtle, but very important,
which is that I'm a, I'm a big believer in the idea that the solution to the problem of intelligence
must be co-evolved with, uh, the challenge, the benchmark.
Like the benchmark should be, uh, a tool that points, um, researchers in the right direction, that's, that is asking the right questions.
But to ask these questions, uh, that is, uh, in itself, in itself, that is a complex problem.
So, I think if you, if you were capable of coming up with a master program that generates, uh, a test of intelligence that is, uh, rich enough, complex enough, novel enough, interesting enough to be true test of intelligence.
Uh, coming up with that program is as hard as coming up with AGI.
It is in fact the same kind of thing.
You, you basically need AGI to create, to create, uh, uh, the challenge that, uh, AGI is a solution to, right?
How explainable should these programs be?
I mean, as an example, you could explain to me the reason why you got a coffee this morning or something like that.
And I would understand that AGI presumably would be able to build models for things that we don't understand, like, um, economics or financial markets or something like that.
It would be an inscrutable, uh, mess.
So how could that work?
Well, yeah.
So, uh, AGI would be capable of approaching a new problem, a new task, a new domain, and, uh, very quickly and very efficiently from very tall data, coming up with a model of that thing.
And, uh, that model should be predictive.
So it should be able to anticipate the evolution of, uh, of the system it's looking at in the future.
Um, I think it should also be, uh, causal.
So you should, you should be able to use it, uh, to plan towards goals.
Like you can imagine, like, I have this model of the economy, for instance.
I want to get it, uh, towards this state.
Here are the, uh, interventions I can make that will actually, uh, causally lead, uh, to, to the desired state.
So it should be a, uh, a predictive model, a causal model that you can use to sort of, like, simulate the behavior of, uh, of, uh, of the system.
Um, and I think that actually makes it inherently interpretable.
Uh, you don't need to explain how the model works.
You can, you can just show it in action.
So one example is, let's say we are looking at Arc.
We're not looking at the economy anymore.
We're looking at a task, uh, in ArcGi.
Um, currently most of the, uh, program synthesis approaches, they are looking for, uh, input to output transformation programs.
And if you're not reading the contents of the program, then one way you can interpret them is just running them, uh, on a test input and seeing, uh, what you get.
I think, um, the kind of model that, uh, an actual, uh, AGI would produce in this case, they would not just be input to output transformations.
They would explain the contents of the task.
So there would be programs that you could use, for instance, um, to produce new instances of the task, right?
Or even to go from output to input, uh, when, when applicable instead of just going from input to output.
Uh, and such a kind of program is extremely interpretable because you can just, uh, uh, ask for new examples and then look at them, right?
Okay. So, so I can imagine there might be some kind of mediated interface, which does encapsulation, you know, and we understand the interface, but maybe we should think about this the other way.
So when I've spoken to AI researchers, I've gone through Arc, um, challenges together with them and they are trying to look at their introspection.
So they're saying, I'm looking at this problem and I, I know it's got something to do with color.
I know it's got something to do with counting.
And, and then they run the program in their mind and they say one, two, three, no, that doesn't work.
That doesn't work.
And, and then they try and formalize that into some kind of an approach.
Do you think that the way we introspect is a useful way to build a solution for the Arc challenge?
I think so.
I think introspection is very effective when it comes to, uh, getting some idea of how, uh, your mind handles system two, uh, thinking.
I think it's not very effective for system one because system one is inherently not something you have direct access to.
It happens like unconsciously, instantly, uh, in, uh, in parts of your brain that you're not directly observing, uh, via, via, via your unconsciousness.
But system two is not like that.
That system two is very deliberate.
Uh, it's very slow, very low bandwidths.
There's only, you know, a few things happening at any, any given time.
Uh, and it's, it's very introspectable.
So I think, you know, what, what you're describing, this idea that you're looking at a new task, you're trying to describe it, uh, via a set of properties in your mind.
Uh, and then you're coming up with, uh, a small number of different hypotheses about, uh, what could be some programs that match, uh, these, uh, these descriptive constraints.
And then you're trying to execute them in your mind to check that your intuition is correct.
I mean, that's kind of called, uh, system two thinking, right?
Um, I think that's basically how, uh, program synthesis works in the brain.
But what's not, uh, mentioned here is all the system one parts that, that are in support of this system two thinking.
I'm really a big believer in the fact that no cognitive process in the human mind is pure system one or pure system two.
Everything is a mix of both.
So even when you're doing things that seem to be extremely reasoning heavy, like solving arc or doing math or playing chess or something,
um, there's actually a ton of pattern cognition and intuition going on.
You're just not noticing it, right?
Um, and it takes the form, for instance, um, the fact that, um, you're only looking at maybe two to four different possible hypotheses for your arc task.
In reality, the space of potential programs, uh, is, is immense.
There's like hundreds of thousands of possible programs you could be looking at, but no, you're only looking at like two or three.
Uh, and what's doing this reduction is, uh, your intuition, right?
Uh, or pattern cognition, it is system one.
And I think that the reverse is also true.
Even when, uh, you're looking at cognitive processes that seem to be extremely system one, uh, like perception, for instance, um, there's quite a bit of system two elements.
Uh, when, uh, uh, I think perception, for instance, is very, very compositional.
It's not pure input to output, uh, matching the way deep learning model would do it.
There's actually quite a bit of, uh, generalizations, yeah, composition that happens.
And that is actually system two.
I really agree that there's some strange entanglement between the two systems.
Yes.
I mean, there was one task where color certainly had something to do with it.
Select, you know, you can almost visualize it as a SQL query, you know, um, group by the colors, uh, select counts, order and descending order, skip one, take three.
You know, that, that kind of thing.
And it's similar to abduction in the sense that there's this perceptual, um, inference happening to this set of hypotheses.
And, and then at some point I'm doing some post hoc verification, which really does seem like system two, but that, but the whole thing seems to work together in a symphony.
Yes.
Yes.
And they, they, they are so intermingled that maybe saying, um, that we're looking at system one plus system two, uh, or system one versus system.
Maybe that's, uh, the wrong framing.
Maybe what we are looking, we're looking for is actually a different kind, uh, of data structure, ups, or substrates that underlies cognition.
That is inherently both system one and system two.
Um, but yeah, what you're doing in your mind, as you describe is basically program synthesis, but that program synthesis is very, very heavily guided, uh, by perceptual primitives.
And just by intuition about what you feel like, uh, what you feel might be the, the, the, the, the correct solution.
So when we implement, um, programming synthesis in, in a computer, I mean, we could just do a naive, greedy, brute force search.
And then we have this combinatorial explosion.
Tell me about that.
Right.
Um, the primary obstacle that you run into if you're doing program synthesis.
So program synthesis that at a very high level, it's, you have, uh, uh, a language.
So typically it's, it's domain specific because that's a shortcut.
So it's not like a language like Python.
It's a language that's a little bit more specialized than that.
Um, and you have a bunch of functions in this language and you use them to create programs.
A program is basically just a composition of these functions, uh, into something.
Look, like in the, in the case of Arc, it's typically going to be a program that takes as input, um, uh, an input grid and produces the corresponding output grid.
And, um, the way you do program synthesis that you try a bunch of compositions of these functions.
And for each, each one, uh, each program, you're going to run it in practice.
So run it, uh, on the, on the target input, look at the corresponding output and check whether that output is the output you expected.
Uh, and you, you do that across all the examples that you have available, uh, across all, all the programs that you can come up to.
And then you look at which are the programs that, uh, actually match, actually produce the correct outputs across all the examples, right?
Uh, and maybe you have one, one such program that's a match.
Maybe you have 10 and then you, you must make a selection.
You must try to guess which one is more likely to generalize and typically it's going to be the shorter one.
But the huge, uh, bottleneck that you face is that, um, the size of program space, like the number of programs you have to look at,
grows combinatorially with, uh, the number of building blocks in your DSL, but also with the size of the program.
So if you are looking for, uh, programs that involve, for instance, 40 different function cores, um, you're looking at a very, very large space.
So you could not possibly, uh, iterate over every individual element of that space.
Uh, so that's the combinatorial explosion bottleneck.
And, uh, humans clearly do not suffer from this problem.
Like you, you describe this, uh, introspection process when you're looking at an hard task.
And you're only executing a very small number of programs, step by step.
And you're only really executing them to verify that they're actually correct.
Uh, you apparently rely on, uh, an extremely powerful kind of intuition, um, that is not entirely reliable,
which is why you still have to perform this verification step.
It does not give you the exact right answer, kind of like an LLM.
Um, I believe what LLMs are doing is actually the same kind of, uh, cognitive process.
It's, it's, it's better matching, right?
It's intuition.
Uh, so you still have to verify, but it's directionally correct.
It's doing a really, really good job, uh, at sifting through, pretty much, this, uh, almost infinite space of programs
and reducing it to just a few possibilities.
And, uh, I think that's actually, uh, the really hard part, uh, in cognition as this reduction process.
So there are some interesting approaches to work.
So I spoke to, um, Jack Cole and Ryan Greenblatt, and then there's, there's the dream coder, um, type approach.
Maybe we should start with dream coder.
Cause you know, um, Tannenbaum's group at MIT, um, you know, Kevin Ellis was the author of the dream coder paper.
And he's actually, um, working with, uh, Zena Tavares building a lab called basis.
And I spoke with them the other day and they are very much focused on the arc challenge.
And they're, they're implementing a lot of MIT's work, um, on the arc challenge, which is, which is really cool.
But I guess like the, the elephant in the room is that dream coder and please introduce what that is.
It's a really elegant, beautiful approach to arc, but unfortunately it doesn't work very well yet.
Right.
So it's been a while since I read the paper, but my recollection of dream coder is that it's a program synthesis technique that tries to create a bank of reusable primitives.
Uh, that is, that is actually, uh, developing kind of like as, uh, it gets used, uh, to, to solve new tasks.
And I think that's a fundamentally right idea.
And it's probably the only system in which I've seen, I've seen this idea in action, this idea of abstraction generation,
that you're going to use your experience and your, your problem solving experience to try to abstract away, uh, functions that you're going to put, uh, in your DSL for, for, uh, reuse later.
Uh, also remember it had this, uh, uh, wake sleep, uh, cycle.
So I think that was to, um, train.
Uh, so the, the, the, the, the synthesis component that they had, uh, leveraged deep learning and they were training the deep learning model, uh, via the, the wake sleep, uh, setting.
Can you, can you correct me?
Yeah.
So they, they had a, um, a neural network generative model for programs.
And then they had a sleep phase where they would retrain the generative model and something called an abstraction, um, sleep,
where they would kind of combine together programs that work very well and discard ones that weren't being used very well.
You know, that, that kind of thing.
Yeah.
Yeah.
That's, that's what I usually call an abstraction generation.
Yes.
Okay.
I see, uh, intelligence as having two critical components, uh, synthesis, where you're taking your existing building blocks.
And assembling them, composing them together to create a program that matches the situation at hand.
Right.
And then there's abstraction generation where you're looking back on the models you generated, uh, and, uh, or just your, your, your, the data you got about the world.
And you're trying to mine it, uh, to extract reusable building blocks that you're sending to your memory, uh, where you can reuse them the next time around.
And yeah.
And dreamcoder was actually trying to implement, uh, these two components, which I think is really, uh, uh, the right direction.
So it's very promising.
So what about Jack Cole?
What, what, what, what do you think of his solution?
Um, and that's the minds AI group on, on the leaderboard.
Right.
So what they are doing is basically they are doing, uh, an LLM.
So it's a, it's an encoder decoder model.
I think it's based on, uh, on T5, on the T5 architecture.
Uh, they are pre-training on a large, uh, code and math, uh, dataset because apparently it helps, which, you know, on its own, it's an interesting finding.
Um, and, uh, then they are further fine-tuning it on millions of generated arc-like tasks.
So they are producing, programmatically, uh, lots of tasks that look like arc tasks and they are fine-tuning, uh, the model on it.
When, when I say fine-tuning, so they are basically for each task, uh, they are tokenizing, uh, the task description.
They are, uh, reducing it to a sequence of tokens.
So that's, that's actually pretty easy, uh, feeding that into the LLM and they're expecting to produce, uh, the output grid in tokenized form.
And then they're decoding that back out.
Um, and, um, so just the setup I, I described on its own, as it turns out, does not perform very well.
It does like a few percent, but they added a really powerful twist, which is that, uh, they are doing test-time fine-tuning.
So they're, um, taking their, their pre-trained, uh, LLM and, uh, at inference time on each new task, they're producing a fine-tuned version of the LLM.
So they are doing that by, uh, producing a variance, uh, of the task, uh, by applying, uh, a bunch of randomized hard-coded transformations, basically.
Um, and they're turning that into, uh, sort of like mini-trained dataset.
They're fine-tuning the LLM on that trained dataset, and then they're, uh, applying that fine-tuned model, uh, on the test input and, and producing a test output.
Um, and if you think about it, so just this, uh, test-time fine-tuning trick is actually getting their model from a very, very low performance,
like small, small percentage of tasks sold, um, to, uh, you know, over, over 40%, uh, which is very impressive.
So if you zoom out by a lot, I think what they are doing is not that different, uh, from program search.
It's basically, uh, uh, at a different point on the spectrum.
So you can think of program search as a, as a spectrum with two axes.
One axis is like the richness and complexity of your DSL, of your bank of reusable building blocks.
And the other axis is the richness and complexity of the ways that you recombine, uh, these building blocks.
And, um, discrete program search typically is going to operate over a very, very small DSL, right?
A DSL with maybe 100 to 500, uh, primitive functions in it.
Uh, but it's going to recombine them in very complex ways, uh, to get programs that may have depths 20, for instance.
Um, and what, uh, Jack Cole is doing is basically turning his LLM into a, a database, uh, of reusable vector functions.
And it has millions of it.
So it's very, very broad, very large, uh, DSL in a way.
And then this time fine tuning is using, uh, gradient descent to recombine these primitives into a new program.
Um, and by the way, the fact that you have this huge, uh, uh, performance jump from not using test time finding to using test time finding really highlights empirically the fact that recombination,
program search is a critical component of intelligence.
If you're just doing, uh, static inference, you're not doing any, any sort of recombination.
Or, uh, if you're doing it, it must be, um, uh, some form of in-context learning.
So basically, uh, using, uh, a memorized recombination program.
Um, if, uh, if, uh, if, uh, if you're only doing static inference, you basically do not display much intelligence at all.
Uh, if you're doing, uh, recombination via test time fine tuning, then you are starting to implement the synthesis component, uh, of intelligence that I described.
And the problem is that gradient descent is a very weak, very data inefficient way of doing synthesis.
It is, in fact, a wrong paradigm.
Uh, and so what you get is that, uh, the resultant programs, uh, have a, a very shallow, uh, depth of recombination, right?
So on the, on the programs in this spectrum, um, the Mind's AI solution is, uh, this point where, uh, they're really maxing out on the richness of the DSL axis,
but they're very, very low on the depth of recombination axis.
Whereas, uh, discrete program search, as it's usually implemented, is, uh, on the complete other side of the spectrum,
where you have a very, very small, very concise DSL, but very sophisticated recombination, right?
Um, and intuitively, my guess is that what makes human intelligence, uh, uh, special is that it's not at either end of the spectrum.
It's somewhere in between.
You have access to, uh, a very large, very rich, uh, bank of abstractions of, uh, of ideas and patterns of thought.
Uh, but you're also capable of recombining them on the fly, uh, to a very meaningful degree.
You're not doing, uh, test and fighting in your brain when you, when you're coming up with novel ideas.
You're not doing gradient descent at all.
You are doing some form, uh, of discrete program search, uh, but you're doing it on top of this very, very rich, uh, bank of primitives.
And that enables you to solve any arc problem pretty much within seconds.
I remember reading your deep learning with Python book many years ago, and you were talking about the perils of fine tuning.
You have to have the learning rate quite low because you might damage those representations in, in the base model.
And when I spoke with Jack, he said that, um, I'm not sure how much of it I should say publicly, but he encoded the, the fine tuning in a kind of language which would reinforce the existing manifold of, of, of the model.
So, you know, he was kind of like saying, I want to use it as a foundation model by transforming the descriptions in a way that, that reinforces it.
And, um, and also the, the active inference thing, it's not active inference from a Christonian point of view, but the test time inference that is moving away from what you said earlier,
which is that it's not a retrieval system.
I'm actually now generating new compositions as, as part of the inference process.
That's correct.
It's not just a retrieval system.
When you, when you're just doing static inference, uh, with LLM, you're just prompting it to get, getting back some, some results.
Um, that's pure retrieval.
Uh, and there's very little recombination happening.
Any recombination, if it happens, must go through, uh, one of these, uh, pre-learned, uh, recombination programs.
Like, you know, some people say that, um, in context learning, uh, is leveraging some kind of, uh, hard-coded gradient descent algorithm that's latent in the LLM.
So maybe that's happening, but whatever is happening, clearly, empirically, uh, we can see that it doesn't work very well.
It doesn't adapt to novelty to very meaningful extent.
But, um, if you add test time fine tuning, then you are actually starting to do real recombination, right?
You're not just, uh, reapplying, uh, the, the programs stored in the LLM.
You are trying to, to modify them, to recombine them into something that's, uh, custom to the task at hand.
That's the process of intelligence, right?
Uh, my, I, I think, you know, directionally, this is the right idea.
Uh, the only issue I have with it is that, uh, gradient descent is just a terrible way to do recombination.
I mean, it is, it is a program synthesis algorithm, of course, right?
Uh, it's, it's just the, the wrong approach.
So in which case, I mean, I had this discussion with Jack when I interviewed him, but while I accepted that it's a general method, of course, it's still, um, domain specific in the sense that you have to come up with a prompting technique in order to fine tune the language model and so on.
But, but it could in principle be applied to, um, you know, fairly broad domains of, of problems.
But you would agree though, that it goes against the spirit of your measure of intelligence.
So there are, there are elements of the approach that are not quite in line with the spirits of the, of the competition.
I think in particular, the idea that, um, is going to pre-train this LLM on millions of generated arc tasks.
So this kind of makes me think of an attempt to anticipate what might be, uh, in the, in the test, uh, dataset, in the private test set.
Um, try, trying to generate as many tasks as possible and hope for collisions between, uh, what you generated and what's actually going to be, uh, in the test set.
So that, of course, is trying to hack, uh, the benchmark via memorization.
Um, it is not what we intended, uh, but you know, ultimately it is up to us, uh, the creators of the benchmark to make sure that it cannot actually be hacked via memorization,
that it is a resistant memorization.
If we did a bad job with that, because it's actually possible to anticipate what's in the, the private test set, then that's on us.
So in practice, by the way, I think we did a decent job because, uh, that, so if, if you're not doing test time fine tuning, right,
you're only getting a very low, uh, uh, accuracy on the test set.
So it kind of shows that, yes, the test set is actually decently novel, right?
I think this is also shown by the fact that, um, the best, uh, LLMs right now, if you're just doing, uh, direct prompting,
they're doing, uh, so the best one is, uh, cloud 3.5 is doing 21%, right?
So it kind of implies that, uh, about 80% of the data set is decently novel, right?
Even if you, if you, uh, use as your frame of reference the, uh, entirety of the internet pretty much.
Uh, so that's actually a good sign.
Uh, but I think, you know, in, uh, in, uh, Jack Cole's approach also, uh, the, the overall approach
is in the, in the spirit, uh, uh, of what I had in mind, because what it's doing is a form of program synthesis.
It's just that it's gathering, um, via, via learning.
It's gathering this enormous DSL, right?
And then it's doing very, very shallow combination and doing it with gradient descent,
which I think is, is not what you should be doing, but it ends up working, right?
So why not?
I agree with that.
So, so actually in spirit, it's the right approach, but it's bottlenecked by stochastic gradient descent on,
on a, on a large language model.
But, um, this is just an interesting segue though.
So again, in your deep learning with Python book, I think around chapter four,
it's very pedagogical for folks who want to learn about machine learning.
You spoke about the leakage problem.
So, you know, the reason why we have a training set and we have a validation set and a test set is
we don't want information to leak between the sets and it can happen inadvertently.
So for example, every time someone gets a new score on, on the, the arc challenge, it's tested on the private set.
And that's information and people then modify their approach.
And it's as if they've seen something in the private set when, when they haven't seen it directly.
That's correct.
And what they've seen is that, uh, this approach that they've tested performs better.
So now they've learned something about the, the contents, uh, of the, of the private test set.
And yeah, like many folks, even, you know, folks who are, uh, uh, machine learning experts,
they have this, um, misconception that you can only overfit if you are directly training on something.
If, if you're, if you're using this training data, that's not the case.
So for instance, uh, some years ago, people were doing, uh, neural architecture search to find, uh, new covenant architectures,
um, that would perform well on ImageNet.
They all used, uh, ImageNet, uh, as, as their reference.
And what they were doing is that they were mining this enormous space of possible architectures
and selecting the ones, uh, that ended up performing well when trained, uh, on, uh, on, uh, on ImageNet.
Uh, and what you ended up with was an architecture that was, at the architecture level, uh, overfit
to the, the ImageNet, uh, evaluation set, right?
Um, in, in general, if you have any sort of process that extracts, uh, information,
even, even just a few bits of information from your evaluation data set
and is re-injecting this information back into your model,
even if it's not an automated process, even if it's, if it's just you looking at the results
and then tweaking the approach by hand, uh, you are starting gradually to overfit
to, to, to, to what you're, you're testing on.
And, uh, ultimately, this would happen, uh, with the, the, the, uh, private test set of ArcGIS.
It's just that because the, the only bit of information you get
each time you submit something is your total score,
you're really not extracting many bits of information, right?
Um, but eventually, because, uh, each participant can make three submissions a day,
um, and there are many participants, uh, eventually you would start, uh, overfitting,
um, which is part of the reason why, uh, we're gonna release, uh, uh, version two of the data set.
And by the way, with version two data set, we're gonna, uh, do something that is pretty important
and should have been done earlier probably, which is that, um, we are gonna,
we're gonna have two private test sets, right?
That's gonna be the one, uh, that we evaluate on when you submit
and, and for which you see the score.
That's gonna be the publicly double score.
But then we're also gonna have an extra private one,
which we are only gonna, uh, evaluate your, your solution on at the end of the competition.
So that you're gonna proceed through the competition by only getting the feedback signal that
here's how, how well you perform on the first, uh, private test set, right?
But at the end, we're gonna swap that out with the new one.
And then you're gonna hope that your model will generalize to it.
Hope being the operative word.
Yeah.
Yeah.
I mean, now might be a good time to talk about, um, our friend Ryan Greenblatt from Redwood, uh, research.
So I, I interviewed him that he's a very, very smart guy.
I, I, I enjoyed talking with him and he, um, did a kind of, uh, you know,
let's generate loads and loads of candidate programs with, with an LLM and then, um, validate them in a kind of,
he didn't want to call it a neuro symbolic framework, which I thought was curious.
But what, what do you think about his approach?
Yeah.
I think that, uh, directionally that's the right approach.
You know, we, we kind of described how, uh, uh, when you are solving an odd task, you are, uh, generating a small number of hypotheses.
Then there are programs.
And then you are actually executing them in your mind, uh, to verify whether, whether they're correct or not.
Right.
Uh, it's the, it's the same kind of process where you're using a big intuition machine to produce candidate programs.
And these candidate programs, you're hoping that they're, uh, more or less right, but you're not sure.
Right.
So you still have to verify them, uh, via, uh, via a system two type, uh, process, uh, which, you know, in, in, uh, in this case,
that's going to be, uh, uh, a code interpreter.
Uh, in your case, you're actually literally going to be, uh, executing your programs in your, in your head.
Um, I think that's, that's basically, uh, again, the same type of, uh, uh, program search,
uh, approach that we are seeing, uh, uh, among the folks that are doing brute force program search,
or, uh, the mindset approach is just a different point, uh, on the programs in the spectrum.
But it's the same kind of thing.
Right.
Uh, and in general, you know, I think the, the research direction that is the most promising to me is combining, uh, deep learning with discrete program search.
Maybe not quite, uh, what Ryan Greenblatt is doing.
But the idea that you're going to use, uh, uh, a deep learning model to guide program search,
that it, it has to look at fewer candidate programs or sub programs, that is absolutely the right idea.
Right.
So I'm not surprised that this is getting good results.
And I do expect we are going to keep seeing, uh, even better results, uh, from variants of this approach.
So one thing I would change, um, is instead of generating end-to-end Python programs,
uh, and then, uh, just having a binary check, is it correct or not?
Um, I think it might be more interesting.
Um, it might be a better use of the LLM to generate, uh, modifiable graphs built on top of, uh, an arc-specific DSL.
And then instead of just checking whether the program is correct or not,
you might want to do, uh, local discrete search around your candidate programs.
Basically use your candidate programs as, uh, seed points, like starting points, uh, for discrete search
to, uh, reduce the amount of work that, uh, the discrete program search process has to do.
Um, and, you know, in general use, uh, I keep repeating this, but you should use LLM as a way to get you, uh, in the right direction,
but you should never trust it to land, uh, in the exact right spot.
You should assume that where you land is probably close to the solution, which is not exactly the solution.
So you're still going to have some, some amount of manual work to do to, to, to go from, uh, the points, like for instance,
the candidate programs that the LLM produced to the actual solution.
And that work has to be done by a system two type process.
Yeah, I discussed this with him and he still is of the mind that they are doing emergent reasoning,
and given enough scale that the divergence between aleateric risk and epistemic risk will tend towards zero,
which of course we don't agree with.
But I agree with you that wouldn't it be interesting if it's quite stateless, the system at the moment,
wouldn't it be interesting if there was some kind of program library and maybe retrieval augmented generation into the library?
He does have some interesting properties to the solution, which maybe you might want to comment on.
He's using, um, vision.
He's doing some interesting prompting.
He's using self, uh, reflection.
He's got like a candidate evaluation, uh, methodology.
What, what do you think about the overall thing?
Sure.
Um, I think it's promising.
And, uh, yeah, you know, I, I think we're going to, we're going to keep seeing variants of this that are going to perform well.
And this is, this is the reason why we introduced, uh, the public track in the challenge.
You know, we, we kept hearing from folks saying, Hey, I'm sure, uh, GPT 4.0 can, can, can do this.
We were like, well, maybe, uh, let's try it.
Um, and of course you cannot, uh, enter the private competition with GPT 4.0
because it would, it would involve sending, uh, the private, uh, task data, uh, to the OpenAI server.
So it will no longer be private.
So that's not possible.
So what we did is that we introduced an alternative test set, right?
Which we call, uh, semi-private.
So it's, it's private in the sense that we're not publishing it, but it's also not quite private
because it is being sent, uh, to OpenAI servers and, or Anthropic servers and so on.
Um, and, um, we, we did this because we want people like Kryon Greenblad to show up
and come up with some, uh, sophisticated, uh, chain of thought pipeline and, and prove us wrong, if possible.
And just before we leave this bit, are you aware of any other interesting approaches, which perhaps aren't in the public domain, but you know about?
So I am aware of various people making claims, uh, about, uh, about their solutions to Arc.
Uh, but I'm not aware of, uh, specific details.
They tend to be very secretive people.
Uh, and ultimately I only trust what I see.
We have two tracks.
We have the private track on Kaggle with a lot of money on the line.
Uh, we have the public track where you can use any set of the art LLM you want.
Um, if you, if you have something, you, you should, you should submit it to one, to one of the two tracks.
If it's self-contained, then just go for the money.
Uh, if, if it uses an LLM API, then use the public track.
But if it's not on the leaderboard, I'm probably not going to be, I'm not going to believe you.
Are the organizers worried that if someone did reach human level performance, that it would be worth more than a million dollars if they sold it somewhere else?
Um, sure.
Maybe I, I doubt that's what's going to happen though, but maybe.
Interesting.
And, and also, um, just on the economics of it, this is quite an open source approach, but what do you think the incentives are?
Because if, if I already had a really good solution, if I was Jack Cole, I mean, I would, it's worth me spending six months on it because there's a good chance I might win.
Uh, if I have nothing, then maybe I'll just have a quick look and see if there's anything, but I won't invest much time.
Um, versus start up a lab and put the money into that and just hire good people to work on it.
So, of course, there's a, there's a big money prize, but you know, we don't expect that people are going to show up and sort of arc because they want the money specifically.
Uh, the amount of money is not high enough that this is going to happen.
Instead, uh, the money that we're putting on the line is just a signal to, uh, indicate that this challenge matters and we are serious about it and we think it's important.
But ultimately the real value that there is, uh, in, uh, in submitting a solution and winning is, I would say, a reputational value.
It's like you become, uh, the first person to crack this open challenge that's been, that's been open since, uh, since 2019.
Uh, and presumably your solution is a big step forward towards AGI.
Um, a lot of people are talking about arc right now.
Uh, if you were to solve it, uh, you would definitely make headlines, right?
Uh, it would be a big deal.
So, for instance, you mentioned starting a lab.
Well, uh, it would be a great opportunity to start a lab around your solution and then raise a bunch of money, right?
And you could do that, uh, just, just on the momentum generated by your, your winning entry.
Could you comment on, uh, you know, I had Sabaro Kambahati on recently and he's got this LLM modulo architecture, which is really interesting.
You know, basically you have this newer symbolic, you know, LLM generating ideas, critics.
What do you think about that general idea?
Yeah, I think that's, uh, generally the right approach.
Like you should not blindly trust the output of an LLM.
Instead, you should use it as, uh, an intuitive suggestion engine.
It will give you good candidates, but you should never just blindly believe that these, these candidates are, are exactly the correct solution that you're looking for.
You should verify.
And this is why LLM modulo some external verifier is so powerful.
It's because you are cutting through, uh, the, the combinator explosion problem that, that would come with, uh, trying to iteratively trying every possible solution.
Uh, but you're also not, uh, limited by the fact that LLMs are terrible at system two, right?
Uh, because you, you still have this last mile verification, and that's going to be done by true system two.
Uh, uh, solution.
The architecture was really interesting because it was bi-directional as well.
So the outputs, you know, like the, the verifiers might give you yes, no, maybe, or some additional information.
And then the LLMs could be fine tuned and so on.
But, but my read on it though, is that it brutalizes it a little bit because the verifiers, of course, are very domain specific.
And that seems to be slightly different to some of the solutions to the arc challenge.
Yeah.
Uh, it will tend to be domain specific.
And also, uh, it's, it's not always the case that you're operating in a domain where there can be an external verifier.
Right.
Uh, sometimes there can be, I think in particular this is true with program synthesis from input-output pairs.
So in particular, this is true for arc, in fact, um, because you know, you know what output you have to expect given certain inputs and what you're producing can be, uh, you're processing programs.
So they can actually be executed.
They can be verified.
Uh, for many other programs, you have no such guarantees.
Right.
So moving on a tiny bit, um, agency.
Yes.
Now I think of agency as being defined as a virtual partition of a system that has self-causation and intentionality allowing for the control of the future.
And I assume that it's a necessary condition for intelligence.
And I, I know you don't because we spoke about this the other day, but what do you think is the relationship between agency and intelligence?
Mm-hmm.
Right.
So, you know, many people kind of treat, uh, agency embodiment intelligence as almost intentionable concepts.
Um, I like to separate them out, uh, in, in my own, uh, model of the mind.
Um, and the way I see it, intelligence is a tool that is used by an agent to accomplish goals.
Um, but it is, it is related to, but it is separate from, uh, your sensory motor space, for instance.
Um, or, uh, your ability to set goals.
And I think you can even separate it out from your world model.
So, I don't know if you're an RTS player, maybe.
Yes.
As in Command and Conquer, Warcraft.
Warcraft.
Right.
Yes.
Warcraft.
Warcraft.
Exactly.
Uh, so all these games, uh, are, are RTS games.
And in RTS game, well, uh, you have, you have, you know, units moving around and you can, uh,
give them commands, um, and you have a mini-map as well.
So, imagine that you're selecting a unit and you're right-clicking somewhere on the mini-map
to tell the, the unit to go there.
Well, um, you can think of the mini-map as being a world model.
Like, it's a simplified representation of the actual world, uh, of the game that captures,
uh, key elements of structure, um, like where things are typically and where you are.
And when you're right-clicking the mini-map, you are specifying a goal.
And, well, in this, in this metaphor, uh, intelligence is going to be the path-finding algorithm.
It's taking in this world model, taking in this goal, which are externally provided,
and figuring out what is the correct sequence of actions, uh, for the agent to reach the goal, right?
It's, it's about, uh, intelligence is about navigating, uh, intelligence is about navigating, um,
future situation space.
It's, it's about path-finding in future situation space.
Um, and, uh, in, in, in this metaphor, you can see that intelligence is a tool.
It is not the agent.
The agent is made of many things, including, uh, a goal-setting mechanism.
You know, in this metaphor, it's, it's played by you.
You are setting the goal.
Uh, it's made of a world model, which enables the agent to represent what the goal means,
uh, and maybe simulate, uh, planning.
Uh, it's also going to be, uh, uh, including a sensory motor space, like an action space, uh,
and, uh, and, uh, uh, that can, uh, that can receive, uh, uh, uh, sensory feedback as well.
Um, but the agent is the combination of all these things, and they're all separate from intelligence.
Intelligence is basically just a way to take in information and turn it into an actionable model, uh, something that you can use for planning.
Right.
It's, uh, it's a way to convert, uh, uh, information about the world into, um, a model that can navigate, uh, possible evolutions of the world.
I agree with everything you've just said.
I think the tension is after speaking with people like Carl Friston, you know, when we think about the physics of intelligence and, you know, this epic particle system we live in with function dynamics and behavior and so on, um, the agency and the intelligence, it's not explicit.
The world model isn't explicit.
So there seems to be something else going on, which is why in many cases, I think of agency and intelligence as being virtual properties rather than explicit physical properties.
That's not to say that we couldn't build an AI where everything is explicit because that would be useful.
We could, we could build it in computers, but there's always the tension of whether we think of the world as this complex simulation of low level particles and nested agents.
I have cells which are agents and my heart is an agent and I'm an agent or whether it's explicit.
All right.
Well, I think in the first AGI that we're going to build, uh, these different components are going to be, uh, explicitly separated out in software because that's simply the, the easiest, uh, way to get there.
At least it's my take on it.
Uh, the architecture is going to be explicit.
Yes.
So you, you actually spoke about functional dynamics the other day, which was music to my ears, obviously being a fan of, of the Fristonian worldview.
What's your take on that?
So to be, to be honest with you, this is actually, uh, something I've been thinking about, but I do not have that crisp ideas about it yet, but it is my general intuition as to how the human mind performs program synthesis.
So I think, um, there are, there are two scales, two levels at which the mind, um, changes itself.
There's, uh, the long-term scale, which, which has to do with, uh, abstraction mining, like abstraction generation and memory formation.
It's, um, uh, it has to do with neuroplasticity as well.
You are basically changing connections in your brain, uh, to store reusable programs.
Your, your, your formalism of intelligence focuses a lot on internal representation.
So this idea of in our minds, we, we have a, we have a world model and so on.
And when I read some of your blog posts from, from years ago, you're talking a lot about, um, this externalist tradition, which is that a lot of cognition happens outside of the brain.
How do you reconcile those two worldviews?
Right.
Um, well, I'm a, I'm a big believer that most of our cognition is, uh, externalized, uh, as you say, like, uh, when, when we are talking to each other, for instance, we are using, uh, words that we did not invent.
We are using, uh, mental images, ideas that we just, uh, read about somewhere and so on.
Um, and if we had to develop all these things on our own, you know, we would need extremely long lives, uh, to start being intellectually proactive.
So, um, I don't think there's really, uh, any, any contradiction between the two views.
Like the idea that sure, like humans, uh, as individuals are intelligent, uh, you possess intelligence, I possess intelligence.
Uh, we can use it sort of like, uh, in isolation on our own.
Uh, uh, and, uh, we can, uh, extract from our environment, from our lived experiences.
We can extract, um, reusable bits, uh, which we can make, we use to make sense of normal situations.
That's the process of intelligence.
We process it, uh, as individuals.
Uh, but also, um, we, we are able to communicate, right?
We are not, we are not just individuals.
We are also a society.
So, uh, these ideas, these, uh, reusable abstractions, we can, uh, extract them, uh, from our own.
Uh, from our brains, we can, uh, put them, uh, out there in the world, share them with others.
Like, we can write books, for instance.
Uh, we can, uh, type up computer programs that can be, uh, not even just executed by other brains, but even by computers, right?
Um, and, um, this process is just, uh, the creation of culture.
And then, uh, once culture is out there, you can download it into your brain, and that's, uh, education.
And as you're doing it, uh, you are sort of, like, artificially, uh, filling up your bank of reusable abstractions.
Uh, and it's a huge shortcut, you know?
Uh, it's, it's almost like downloading, uh, skills, like, in, in the matrix.
Uh, it's, it's a little bit of that, like, uh, learning about physics, learning about math.
Uh, you are downloading these, um, very rich, uh, reusable mental templates, uh, like, really mental building blocks.
And then you, you can, in your own brain, you can recombine them.
Uh, you can reapply them on new problems.
It makes you, uh, more intelligent, like, literally more intelligent.
It makes you, uh, more efficient at skill acquisition, more efficient at, uh, problem solving and so on.
Yeah, beautifully articulated.
I mean, there's a couple of great books I've read on this, you know, The Language Game and, um, also Max Bennett's book on intelligence,
basically talking about this, um, the plasticity of mimetic information sharing, um, you know, allowing us to stand on the shoulders of giants.
I, I, I think there's a, there's a, uh, an interesting, uh, angle to the question you asked.
I know, I know if, if you were aware of it, but what I've described there is this idea that humans are the source of, uh, abstraction.
Uh, human, individual human brains use their lived experience to extract abstractions.
And then they're, uh, externalizing them via language, typically, not, not exclusively, but most of the time.
Uh, and then other brains can download these abstractions and kind of make them their own, which is a huge shortcut.
Because, uh, you're, you, you don't have to experience everything on your own, uh, to start leveraging these abstractions.
Um, but in this model, abstraction generation and abstraction recombination to form new models is always happening inside brains.
Right?
The only part that's externalized is the memory.
It's that you're, uh, uh, moving the abstractions, the reusable building blocks out of these individual brains,
uh, putting them in, in books and so on, uh, and then, and then downloading them back.
But to be useful, they need to be internalized in your brain.
Uh, uh, question then is, could, uh, abstraction generation or recombination actually happen outside brains as well?
Uh, not necessarily, uh, in the context of creating an AGI because, you know, that's exactly what, what an AGI would be.
It would be, uh, this, uh, recombination and abstraction process, this synthesis and abstraction process, uh, encoded in software form.
But do we have today, like, external processes that, that, uh, implement this?
Well, I think we sort of do.
I think science in particular, uh, is doing a form of synthesis, uh, that is, that is driven by humans, but, uh, it is not happening inside human brains.
Like, we have the ability to, uh, do recombinative search over, uh, spaces that actually cannot fit inside human brains.
I think you said, uh, in a lot of the things that we invent, like, uh, when, when you create a better computer, for instance, uh, you are doing some kind of recombinative search over a space of possible devices,
but you are not really able to hold a, a full model of the device inside your own brain.
Instead, the model is distributed, uh, across some, some number of externalized artifacts.
Um, and I do believe that human civilization is implementing this, uh, highly distributed, uh, synthesis part of the, of the process of intelligence.
It is implemented it externally across, uh, many different brains manipulating externalized symbols and artifacts.
And this is what's underpinning a lot of our civilization because the systems we, we've been creating, we've been inventing,
are, uh, so complex that no one can really, uh, understand them in full.
So you cannot run, uh, this, uh, this invention process inside brains anymore.
Instead you are using brains to, uh, drive a much bigger externalized process.
So I think cognition is externalized not just in the sense that, uh, we have the, we have the power to, uh, uh, write down and then, uh, read, uh, ideas, abstractions,
and then reuse them inside our brains.
We're actually running, uh, intelligence outside our brains as well.
I completely agree.
And you've written about this, about how intelligence is collective situated, um, and, and externalized.
Yes.
But there's always the question of, yeah, many of the, you know, like science, for example, is, is, is a kind of collective intelligence, which supervenes on us and languages as well.
But do things like mimesis happen outside of, um, biology?
I mean, certainly it happens in, in the world, you know, the selfish gene that happens with, with genetics, but you could argue that a kind of mimesis actually happens just in any open physical system with certain patterns of functional dynamics and so on.
So, um, you know, the, the, the real question I think with this externalized cognition is where do the abstractions come from?
Perhaps our brains are just very efficient at building the map from the territory.
And it's, it's just a slightly better way of doing what already happens naturally externally.
Yeah.
Um, I think to a large extent, uh, the way we've externalized cognition is, uh, not as efficient as the way we've implemented cognition in our, in our brains.
Um, these externalized cognitive processes, they, you know, so intelligence is, is a kind of search process, right, over a space of possible combinations of a thing.
Um, I think right now this search process is to a large extent externalized when you're looking at technology, when you're looking at science.
Um, but it's not externalized in a very smart way.
I think we are roughly implementing, uh, brute force search.
I see it a lot, especially in deep learning research.
Um, the way the deep learning community as a whole is finding new things is by trying everything else and eventually hitting the thing that works, you know.
Um, and I believe, uh, uh, individual humans are actually much, if they, if they had enough brain power to actually model these things, uh, in their own brains,
uh, they would be much more effective at finding, finding the right solution.
Interesting.
I mean, Ryan Greenblatt's view was emblematic, emblematic of some of the X-Risk folks in that he was arguing that he can be in a hermetically sealed chamber or be a brain in a vat.
And it's a pure intelligence.
He would still be able to reason and solve tasks and, and, and so on.
And the counter view is that physicality and embodiment is really important.
I mean, when I asked Murray Shanahan this, I said, what's the reason why we need to have physically embodied robots?
And he said, well, these robots are interacting with the real world.
They're understanding the intricate causal relationships between things.
And that helps them build models more efficiently, but perhaps in service of just learning about the abstractions, which already exist in the physical world.
Yes.
To exercise intelligence, uh, it needs to be operating on something like you think out of something, uh, about something like you, you need to have some concrete environment and goals in that environment that you want to accomplish and actions that you can take.
So it's about something, it cannot be about nothing, but it's also, uh, made of something.
You are, uh, making your plans to reach your goals based out of existing components, existing, uh, subroutines.
Uh, if you have nothing at all, uh, you, not only you have nothing to be intelligent about, but your, uh, intelligence has nothing to recombine.
Right.
Um, and that's why embodiment is important.
I mean, in, in humans, you know, I mentioned this idea that cognition is built layer by layer, each new layer, which is a little bit more abstract than the one before it.
It is built, uh, in terms of the components that came before.
And if you, uh, dig deep enough, if you, uh, unfold your mind layer by layer, at the very bottom, uh, you will find things like, uh,
the circling reflex, for instance, uh, it's like, it starts, everything starts with, uh, your mouth.
Uh, and then, um, you, you start having things like, uh, grabbing objects to put them in your mouth and then things like crawling on the floor so that you can reach objects.
So you can, you can grab them and put them in your mouth and so on.
And at some point when you start putting objects in your mouth, uh, but the new things you're learning are still expressed in terms of this sort of like concept and skill hierarchy.
Right.
And, and, uh, when, when you end up doing abstract math, well, you are using building blocks that eventually resolve to these extremely primitive, uh, uh, sensory motor, uh, subroutines.
Right.
So, yeah, embodiment is important, but at the same time, uh, I think the kind of body and, and sensory motor afford and space that you have is very much, uh, plug and play.
If you have a true AGI, uh, you could, you could basically, if, if you have an AGI, you could plug any environment, uh, any sensory motor space, uh, any DSL as well, uh, into it.
Um, and it would start being intelligent about it, you know, uh, so in that sense, like embodiment is important, uh, but what kind of embodiment might not, might not necessarily be important.
Um, and you know, uh, uh, uh, another thing that's really important is goal setting, by the way, which is distinct from embodiment is also distinct from intelligence.
If you're just a brain in jar, uh, uh, with, with nothing to think about, well, uh, you're not going to be very intelligent, but also you're not really going to be doing
anything because you have nothing to do, you have no goal, uh, uh, to drive your thoughts.
Um, and I think it's, and this is especially true if you, if you're looking at, uh, children, the way you learn anything is by setting goals and accomplishing them.
You cannot really, uh, build, uh, good, uh, good, good world models, uh, passively, purely by, you know, uh, observing what's going on around you, uh, with no goals of your own.
That's not how it works. Uh, goal setting is a critical component, uh, of any, any intelligent agent.
I completely agree. I think the only unresolved tension in my mind is that there are many manifestations of intelligence,
and it is possible for us to build an abstract explicit version which would run on computers, essentially.
It doesn't necessarily need to mimic the type of intelligence we have in the real world.
Yeah, I think so. And I think it will probably have, uh, uh, at least in its first few iterations,
it will probably have significant architectural similarity with the way intelligence is implemented in, in people.
But, um, ultimately, you know, it, it might, it might drift away towards, uh, towards, uh, entirely new types of intelligence.
Now, you've said that language is the operating system of the mind. What do you mean by that?
Right. So, what, what's an operating system, right? It's not the same thing as a computer.
Um, it is something that makes your computer more usable, uh, and more useful.
It empowers, uh, computing for some user. Um, well, it, it empowers some user, uh, to, to, to best leverage, uh, the capabilities of their computer.
I think language plays a similar role for the mind. I think language is distinct from the mind.
Like it's, it's, it's a separate thing from, from intelligence, for instance, or even from a world model.
But it is a tool, um, that you as an agent, uh, is leveraging to make your mind, to make your thinking more useful.
Right. So I believe language and thinking are separate things. Language is, is, is a tool for thinking.
And what do you use it for? Well, I think one way is that you can use language to, uh, make your thoughts, uh, introspectable.
Your, your thoughts are there. They're like programs in your brain, uh, which you can, uh, execute to get their outputs.
Um, but you cannot really look at them, uh, by writing them down, uh, in, in, in words.
I don't mean like literally writing them down, but just expressing them, uh, as words, uh, suddenly you can start, uh, reflecting on them.
You can start looking at them. You can start comparing them. And, uh, critically, you can start indexing them as well.
Uh, I believe, uh, one of the rules of language is to enable you to, uh, do indexing and retrieval over your own, uh, ideas and memories.
If you did not have language, uh, then to retrieve memories, you would have to rely on, uh, external stimuli, right?
Like, you know, uh, Proust is eating a madeleine and it's reminding him of, uh, of, uh, uh, uh, uh, a specific time and place.
And, um, um, if, uh, Proust did not have language, then every time he, he, he, he, he, every time he needs to, uh, think about that particular time and place,
he would have to eat the madeleine. This would be his only access point to that memory, right? This external stimuli.
Uh, if he has language, then he can use language to try to, uh, query, uh, his own world model and retrieve, uh, the memories that he wants.
So it's, uh, it's, uh, a way to express what you want to retrieve, uh, inside your own mind.
Uh, it's also a way to compose together more complex thoughts.
If you cannot, uh, reflect on thoughts, if you cannot kind of like materialize them and, and, and look at them and, and modify them in your mind,
then I think you're also quite limited in the, in the complexities of the thoughts you can, you can formulate.
Uh, this is, this is a very, very simple program analogy, by the way.
If you have a computer, you can actually use it to write programs.
You do not need an operating system, right?
You can just write an assembly code.
Um, why not?
But you are severely limited, uh, in, in, in terms of the, the complexity of the software you can produce.
If you have, uh, an operating system and you, and, and, and, and you have, uh, uh, you know, high-level programming languages and so on,
then, uh, these are tools that you can use as a programmer, uh, to, to develop much more complex software.
And your intelligence as a programmer, your programmability has not changed.
It's just your tools that have gotten better.
And suddenly you are much more capable as you were before, right?
So I think intelligence is using language as a similar kind of tool.
Yeah, we have this information architecture of mediated abstractions at, um, there's almost like concentric circles, um, of, of complexity.
And in the language game that they spoke about, you know, scissors are a physical tool and, and language are the memetic equivalent of scissors.
And of course we can compose these tools together and use them in different circumstances.
But moving to consciousness a tiny bit, I mean, you suggested that consciousness emerges gradually in, in children.
How does this, you know, inform your, your views of, of machine consciousness?
Right.
So, I mean, to start with, I am not that interested in the idea of machine consciousness.
I'm specifically interested in intelligence and related, uh, aspects of cognition.
I think consciousness is a separate problem.
Clearly, you know, it, it, it has some relationship with intelligence.
Uh, you, you, you see it for instance in the fact that, well, anytime you, you use, uh, system two thinking, you are aware of what you're doing.
Consciousness is involved.
So clearly there is a relationship between consciousness, consciousness and system two.
Uh, the nature of this relationship is not entirely clear to me.
And I also do not pretend that I understand consciousness very well.
And honestly, I don't believe that anyone does.
So, I'm, I'm always very suspicious when I, when I hear people who have a very, very, uh, detailed and, and precise and categorical ideas about, about consciousness.
So, that, you know, I, I do believe that it's plausible that machine consciousness is possible, uh, in principle.
I also believe that, um, we don't have anything, uh, that resembles machine consciousness today.
Uh, we're probably pretty far from it.
Um, for, for, for a system to be conscious, you know, it would need, at the very least, it would need to be, uh, much more sophisticated than a sort of like input to it with mapping that you see in deep, deep learning, deep learning models.
And also in LLMS, um, at the very least you would expect the system to have some kind of permanent state, um, that gets, uh, influenced by, uh, external stimuli, but that is not just fully set, uh, by external stimuli.
It has some kind of, uh, consistency and continuity through time.
Uh, it can influence its own future states.
It is not purely a reactive, right?
I think consciousness is, is, uh, in opposition to, uh, purely a reactive type systems like deep learning models or insects maybe.
Um, and I don't think we have any, any system that looks like this today.
I also think consciousness requires the ability to, uh, introspect quite a bit, like this sort of, like, uh, self-consistent state of the system, uh, that is maintained across time.
It should have some way to represent and influence itself.
It should be self-driving in a way.
Um, and we don't have anything, uh, like that today.
But in principle, you know, maybe, maybe it's possible to, uh, build it.
And so you mentioned, uh, this, uh, this thing I mentioned on Twitter, like this idea that, um, uh, babies are not born conscious, which apparently is extremely controversial.
So maybe I can, I can say a little bit more about that.
Um, so first of all, you know, we have no real way of, uh, assisting with 100% certainty whether anyone is conscious at any stage of their life.
At any stage of development, right?
It's, it's basically a guess.
Um, it seems to me that, uh, babies, uh, in the womb are very unlikely to be conscious.
Because, uh, they're basically, uh, they're basically fully asleep all the time.
Like they're, uh, asleep, you know, they're, they're in, in one of, uh, two possible, uh, sleep states.
Like 95% of the time there's, uh, deep sleep where they're just, you know, inert.
And there's active sleep, uh, where they're moving around, you know, and, and, you know, the mother can, can feel them, uh, move around.
And when, when they're moving on, they're not actually awake.
They're actually asleep.
It's just active sleep.
And the remaining 5% is not, uh, wakefulness.
It's just transitions, uh, between deep sleep and active sleep.
And the reason they are just sleeping all the time is that, uh, they're being sedated, right?
Uh, the womb is very, uh, low oxygen pressure environment, um, and it's sedating them.
And also, uh, the placenta and the baby itself are producing, uh, uh, anesthetic products.
Basically, the, the, the placenta, uh, is actually producing anesthetics.
And so that's keeping the, the baby, like, in this, uh, dreamless sleep, uh, pretty much.
Which doesn't mean, by the way, that their brain is not learning.
Their brain is not, like, just disconnected and doing nothing.
Uh, they are actually learning, but they are, and they are learning in this very passive way.
You know, they're just computing statistics about, uh, what's going on in the environment,
which, which, you know, what brains do whether you're awake or you're asleep.
Um, but yeah, I, I believe that babies in the womb are not conscious.
And when they're born, they start at, uh, at consciousness level zero, pretty much.
Um, and as they start being awake and they start experiencing the world, uh, then consciousness
starts to light up.
But it is not this sort of, like, instant switch where, where they go from, uh, uh, being unconscious
to being fully conscious.
It happens gradually.
So you start at zero.
And by the way, you can have to start at zero even, uh, after you wake up.
Because, uh, when you're born, you have nothing to be conscious of, you know?
Like, um, pretty much everything, not just actions, but even perception is something that
you have to learn through experience.
When you're born, uh, you cannot even really see because you have not learned to see, you know?
Uh, you have not trained, uh, your visual cortex, right?
So you, you can see maybe like blobs of flight.
Uh, you cannot, you do not have a model of yourself, of your own sensorimotor affordances.
Uh, you have maybe a, a very crude, a proto model that you developed by moving around in
the womb and having your, your, your brain kind of, kind of like, uh, map, uh, what's going
on and what, you know, and correlations kind of like in, in, in your sensorimotor space.
But it's not really, um, a model.
It's not sophisticated model, if anything.
So you have nothing to be conscious of.
You have no world model, no model of yourself, uh, no real, uh, incoming perceptual stream
because you have not learned to take control of your, of your sensorimotor affordances just
yet.
So you start at zero and then as you build up these models, uh, your world model, your model
yourself and so on, uh, you start gradually, bit by bit, uh, being more conscious.
And, uh, at some points you, you reach a level where you can be said to be fully conscious
the way maybe like a dog might be fully conscious.
And I think it happens pretty fast.
It happens probably significantly earlier than the first clear external signs of consciousness.
Uh, I think around one month old-ish.
Now, the babies are probably, uh, conscious to, uh, at the same level as, you know, most,
most mammals, I suppose.
Um, but that's still not adult level consciousness, right?
Um, and, um, I think adult level consciousness is something that children only start experiencing
around age two to three.
It doesn't mean that they, they were not conscious the whole time.
Like, again, they're, they're conscious pretty much starting on day one.
It's just to, to very small amount, right?
Um, and, uh, so consciousness is something that you have to build up over time.
At least that, that's my theory.
And, um, there are some sort of, like, uh, indications that, uh, this is not entirely
made up, basically.
Um, one example is, uh, if you try to observe, uh, attentional blink, uh, try to measure it in, in children,
you will see that, basically, up until age, uh, three, they have, uh, significantly slower
attentional blink, uh, than adults.
Uh, and they're gonna pass, um, uh, the events around them into, uh, uh, fewer, fewer events.
So, they, they can have, uh, more coarse-grained resolution, uh, of time in, in the world.
Um, and I think that's actually, uh, that, that's tied, that's tied to this idea of, uh, level, uh, of consciousness.
I also have this, this very, uh, probably controversial idea that, well, so you, you reach, uh, adult-level consciousness
around, like, age two to three, roughly.
Um, but then you don't stop there.
You actually keep getting more and more, uh, conscious over time.
And, um, your consciousness level probably peaks around age, like, nine to ten.
And then, then it goes in reverse.
You get, uh, less and less conscious with every, every passing year.
But not to a very, uh, significant extent.
So that, uh, the, the difference in degree of consciousness between, um, I don't know, a, a 19-year-old
and a 10-year-old and a three-year-old is actually very, very minor, but it is still there.
And I think this, uh, plays into some things like, for instance, our subjective perception of time.
I think the more conscious you are, the, the, the higher your, your level of consciousness,
the slower, uh, your perception of time.
Because your perception of time is highly dependent on, uh, how many, um, things you can notice
in, uh, any, uh, any time span.
So, one way you, you could conceptualize your degree of consciousness is you can imagine
consciousness is a kind of, like, nexus, uh, in your world model.
It's a focus point from which, um, uh, from which span, like, a bunch of connections to other things.
Uh, connections that encode this, this, uh, focus point and give it meaning.
And these connections, they can be, they can be, uh, they can be fewer of them or more of them,
and they can be more or less deep, right?
And the deeper the connections, the more you have, the more conscious you are.
Uh, and, and there's also this, uh, this, uh, uh, temporal component where, uh, if, if you're highly conscious,
then even in, uh, one signal, you might be noticing many things and drawing many connections
between these things, uh, and, and, and things you know.
Uh, that's, that's a higher level of consciousness.
On the other hand, uh, if you're not, if you're noticing very few things, if you have a very coarse-grained
perception of reality, uh, that is evolving and, and you're only noticing few things, uh, in, uh,
in, uh, in, uh, any time span, then, uh, you are, you have a, a faster perception of time.
Like, things just, you know, pass in a blink.
Um, and that, that's, that's a lower level of consciousness.
Like, uh, if you drink a lot of booze, uh, you have reduced consciousness, right?
And things will actually seem to move faster and you will notice fewer things.
And, and, uh, the depth of connections that you establish between things is, is less.
Um, I think something like, you know, if you're, if you're, uh, a one-year-old toddler,
uh, you have, uh, a much slower, uh, attentional blink, your perception of time is likely very, very fast.
And, you know, we have this idea that children perceive time, uh, slower.
I think that's true, but it, it really depends on your age.
I think if you're one, time is super fast because again, you're, you're, you're at this lower level of consciousness.
If you're three, it's basically adult level.
But if you're 10, it's actually pretty slow, right?
Or if you're seven, it's, it's slow as well.
It actually gets slower and slower and slower until, until it peaks around age like nine, 10.
Then it starts getting faster again because you're less and less conscious at the time.
I remember being very bored when I was a child.
I've not felt bored in as long as I can remember.
And, um, I interviewed professor Mark songs recently.
He's got a great book called the hidden spring.
And his basic idea is that consciousness is prediction errors.
So the more, uh, you know, like you're conscious when you first learn how to drive.
So the more things become automated, the less conscious we are.
And then maybe time goes faster in many ways as, as we grow up.
But this idea of being more or less conscious is, is really interesting.
As you said, it's like a dimmer switch.
Yes.
But on the machine sentience thing, I remember you came on the show to talk about the Chinese room argument.
And you said understanding is a virtual property of functional dynamics in the system.
And presumably you would also argue that consciousness is a virtual property of functional dynamics in the system.
I think so. I think it is not strongly tied to substrate.
So in principle, you should be able to implement consciousness, uh, using the right functional dynamics in silicon.
Yes.
I don't think we have it or that we're close to having it.
But in principle, I don't see a problem with that.
Yes.
And we'll leave the hard problem of consciousness to one side.
By the way, Mark Psalms was quite dismissive about the hard problem of consciousness, you know,
which is that there is something it is like to be conscious.
Well, I think there is.
Oh, go on.
There is.
Yeah.
Like some people dismiss, yeah, some people dismiss the problem of consciousness saying,
yeah, no, like something like consciousness is what it feels to be an information processing system
or things like that.
It really means nothing.
It's just pushing, uh, the problem back to where you can better control it with words,
but it's not reusing the problem.
Uh, there is clearly such a thing as qualia and you are experiencing them right now.
So you cannot deny that they exist.
Uh, and we have no way to explain or even describe what they are.
Like you can describe many things about consciousness, but the, the subjective experience is not reducible
to, to these explanations.
There is something.
And we don't know what that is.
And you think we, we have it and animals have it, but.
Yes.
Animals have it.
I mean, not all animals.
Uh, and I, again, like I believe in this idea of degrees of consciousness, um, and, and, uh, animals probably have it to less extent than we do.
It might, it might not be a huge difference by the way, but it's probably less.
Yeah.
Do you think the earth could be conscious to some degree?
No, I don't think so.
I think, um, non-animal systems typically lack the basic prerequisites that I would want to see in a system to even start entertaining the notion that it might be conscious.
Like, for instance, the ability to maintain, um, this, uh, self-influenced, self-consistent, uh, inner state across time, uh, that's influenced by perception,
but that, that is also capable of, uh, um, driving itself, pretty much influencing its own future state, uh, that's, uh, uh, capable of, uh, representing itself, introspecting and so on.
Uh, I don't think you see that in non-biological systems today.
Do you think the collective of all Americans could be seen as a conscious being?
No.
Why not?
Again, because it lacks, uh, these basic prerequisites.
So it needs to be a physical form of connectedness to the surroundings.
It couldn't, there couldn't be a virtual version distributed over many agents.
You, no, you, you, you could definitely imagine a distributed version.
It's just that I'm not seeing the collective of all Americans, for instance, uh, implementing this, uh, self-influenced, uh, self-consistent, uh, state that's capable of, uh, representing itself and the world, uh, and so on.
And even then, you know, even if you have these things, uh, in a software system, for instance, it's not automatically conscious.
It's just that it starts being plausible that it might be conscious if you also see, uh, uh, signs like, uh, pretty clear signs that it might be.
So what, what might be such a sign?
Well, uh, it's difficult.
And I, I, I don't think that you, you're ever going to see, uh, a proof of consciousness, a proof of consciousness that works a hundred percent of the time.
I think it's always kind of a guess.
I guess, but typically, you know, I think it's highly likely that system is conscious if it has all these prerequisites and it is capable of expressing, uh, statements about its own inner state.
Um, that cannot be, uh, purely a product of repeating something the system has heard, you know?
Like if you ask an LLM about, uh, how it feels and so on, it will answer something, but it's really just, uh, rehashing something it has read.
So what I would want to see is, uh, the system is making statements about how it feels, and there seems to be a strong correlation
between the behavior of the system and what it is telling me.
And what it is telling me is unlike anything that the, the system has seen elsewhere before.
Like, I don't know, I'm, I'm, uh, uh, holding my, my two-year-old and trying to, uh, console them because, um, they're crying.
And I'm like, hey, you shouldn't cry, uh, stop crying.
And, um, they're like, but I want to cry.
That's, that's how I feel like, well, there's a pretty strong correlation between where, what, what the child is doing
and what they're saying about themselves, so you can believe them.
Uh, and they've never heard anyone saying, I want to cry.
It's, it's, it's, they're really expressing something they could, they could not have picked up from anywhere else, you know?
So, in this situation, it's just highly plausible.
It is not proof of anything, but it's highly plausible that they, they, in fact, do have, uh, some awareness of their, their own mental states
and they're expressing something about them and they are actually conscious.
They are experiencing qualia, you know?
So, Francois, you've been very critical of, um, singularitarianism and, and Dumaism.
What do you think is the driving force of these extreme views?
Well, you know, I think they're, they're good stories.
Like stories about the end of the world, this idea that we are living, uh, in the end times and maybe that we have a role to play in it.
Um, these are, these are good stories, which is why you find them a lot, uh, in fiction.
Like in science fiction, for instance, you find them a lot in religion as well.
Uh, and they're not new.
They've been, they've been around for thousands of years.
Uh, so I think that's the primary driving force.
It's just that they are, they are good as memes.
They are good stories.
People want to believe them, uh, and, and they're also very easy to retain and propagate.
And that's, that's really the main thing.
Uh, you know, everyone is just craving meaning, uh, uh, to organize their lives around,
which is why, uh, cults are still, are still a problem, uh, in, in our day and age.
Uh, and that's just an instance of that, I think.
Do you think there's a bit of a messiah complex as well?
Oh, absolutely.
Yeah.
Absolutely.
You know, you, you see it a lot in the, in the San Francisco Bay Area.
Um, there, there are people who are, who have kind of latched onto this idea of building a GI.
Um, and while using it to sort of like picture themselves as messiahs, as you say.
Personally, I see, uh, creating a GI as a scientific problem, not, not a religious quest.
Um, you know, and this, this is often, um, kind of, uh, merging together with, uh, the idea of eternal life, by the way.
Uh, which is of course very natural because, uh, uh, the, the story in, uh, most religions is always about, uh, uh, this, uh, this combination of, um, anyway.
Um, but yeah, it's, it's, it's, uh, kind of merging as well with this idea of, uh, eternal life, right?
Right?
That if you create a GI, uh, uh, it will, it will make you live forever, pretty much.
So it's, it's this very religious idea, right?
Um, and it has become this religious quest, uh, to get there first, uh, in, in, uh, whoever gets there first will become, uh, as gods, right?
So I'm not really subscribing to any of that.
I think building a GI is a scientific problem.
And, uh, once you build a GI, it's basically just going to be a very useful and valuable tool.
It is going to be, you know, as, as I, as I mentioned, a path-finding algorithm in future situation space.
It's going to be a piece of software that takes in information about the problem and is capable of very efficiently synthesizing a model of that problem,
uh, which you can use to make decisions about the problem.
Uh, so it's a valuable tool, but it, it does not, uh, turn you into God.
And certainly you can use it in scientific research and maybe you can use it in longevity research,
but it does not automatically, uh, make you immortal because it is not omnipotent.
I think if you start having very powerful ways to turn information into actionable models,
your bottleneck quickly starts becoming the information that you have.
So for instance, if you have, uh, an AGR that can, uh, do physics, it can quickly synthesize new, uh, physics theories.
Um, the, the thing is, uh, human scientists today, they're already very, very good at that.
They're in fact too good.
They're so good that their ability to synthesize plausible new theories far exceeds our ability to collect, uh, experimental data to validate them.
That's what, that's what you see with string theory, for instance.
Um, and that's, that's a pretty stark illustration of the fact that if you are too smart, then, uh, you start, you start running kind of like free, uh, uh, of information.
Um, and that's, that starts not being very useful anymore, right?
Uh, applied intelligence is grounded, uh, in, uh, in experimental data.
And if you are very intelligent, then experimental data becomes a bottleneck.
So it's not like you're going to see a runaway intelligence explosion.
Is there anything that would make you change your mind?
I mean, again, I had this discussion with Greenblatt and I try and avoid having X-risk discussions when, when I'm actually debating.
And a lot of it hinges on agency.
So I said, because I don't think systems are agential or will be, I don't see the problem because a lot of the, the mythos around this, you know, the Bostromium ideas around instrumental convergence and orthogonality.
It's all goals.
It's all agency based.
So no agency, no problem.
Presumably you agree, but you know, maybe if there was agency, would you think there was a problem?
Yeah, no, I think intelligence is separate from agency.
agency is separate from goal setting.
If you just have intelligence in isolation, then again, you have a way to turn information into actionable models.
Uh, but it is not self-directed.
It is not, uh, uh, able to set its own goals or anything like that.
Goal setting has to be an add-on, uh, an external component you plug into it.
Now you could imagine that, well, what if you combine this HGI with an autonomous goal setting system, with a value system?
Uh, you turn all of that into an agent and then you, you give it access to, uh, the nuclear codes, for instance, something like that.
Uh, is that dangerous?
Well, yes, but you've, you've kind of engineered that danger in a very deliberate fashion, right?
Uh, I think once, once we have HGI, uh, we'll have, uh, plenty of time to kind of, uh, anticipate this, this kind of potential risk.
So I do believe, you know, uh, HGI will be a powerful technology.
Uh, so this is exactly what makes it valuable and useful.
Um, anything powerful is also potentially risky.
But we are very much going to be the ones in control because HGI on its own cannot set goals until you actually create, uh, uh, uh, an autonomous goal setting mechanism.
But why would you do that, you know?
So the, the difficult part, the dangerous part is not the intelligence bit.
It's more like the, the, the goal setting, um, and action space bits.
Uh, and if you want to create something very dangerous that creates, that, uh, sets its own goals and takes action in the real world, you do not actually need, uh, very high intelligence to do so.
You can already, uh, do so with very crude techniques, right?
So the thing is, um, existential risk, I mean, it's a legitimate form of inquiry and especially nuclear risk, for example.
And I know many of these folks, they're not just solely focused on AI existential risk.
They're, they're looking at other risks as well.
But how do you view the incentives?
I mean, you could be really cynical and just say, oh, effective altruism and open philanthropy.
They're throwing lots of money at this.
And what they actually want is power and control.
How do you, how do you kind of think about this?
Well, there's, there's definitely a little bit of that.
I also think a lot of the true believers, they're just buying to it because they want to believe.
And it's, it's, it's, again, it's very parallel to religious ideas, uh, in many ways.
So I don't, I don't think it's, it's very rational, you know?
Um, so that said, you know, once we have EGI, because today we don't, and I don't think we're particularly close to it,
but once we have it, then we can start thinking about, uh, the risks that are involved.
I don't think you're, you're going to see, you know, um, the day, the day you just, uh, start trying the program,
it becomes, uh, self-aware and takes control of, uh, uh, your lab and so on.
I don't think you're going to see anything like that.
Uh, again, intelligence, EGI is just a piece of software that can turn data into models.
It's up to you to use it in certain way, right?
I mean, like an abstract way to think about this is framing it as safetyism and governance in general.
So if we take away the hyperbolic X risk and we talk about, um, you know, misinformation and things like that.
Sure.
What do you think about that?
Um, I mean, maybe I should be more specific.
I mean, uh, you know, deep fakes and misinformation and infringement of copyright and so on.
Do you think that we should strongly regulate this or would it harm innovation if we did?
I think there are definitely harms that can be caused, uh, by current technology, uh, by current and near term, uh, uses, uh, of AI.
And yes, I think some form of regulation might be useful to protect the public against some of these harms.
I also think that, um, the, the regulation proposals that I've seen so far are not really satisfactory.
They are more, uh, uh, leaning towards harming, uh, innovation, um, uh, than protecting the public.
I think ultimately they will, they're more, they're more likely to, uh, end up concentrating power, uh, in the AI space, uh, than, than just protecting the public.
Um, so I think regulating AI is difficult and just relying on existing non AI regulation to product people, uh, might be the better course of action.
Uh, given that introducing a new AI specific regulation, um, is, you know, it's, it's, it's, it's difficult problem.
And I, I don't think based on what I've seen so far, I don't think we're going to do a very good job at it.
Francois Cholet, it's been a, an honor and a pleasure. Thank you so much.
It's my pleasure. Thanks. Thanks so much for having me.
Amazing.
Thank you.
