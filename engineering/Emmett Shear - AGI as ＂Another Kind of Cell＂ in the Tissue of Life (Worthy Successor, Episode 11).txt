I might not be able to understand it, but does it recognize me?
Okay, so would you want it to recognize you?
I would like it to look back retroactively and say, yeah, yeah, I am the descendant of these
things and I'm on team humanity. Like, hell yes, the way that I root for the snail, it roots for me.
And like, you know, at this, at that point, it's not, we're not the cutting edge anymore. That's
okay. Like, we're not gonna be the cutting edge forever.
Does it, do you want it to coddle and care? Because you don't coddle and care for the
snails. Do you want it to coddle and care for you?
I'm on those amoebas team, even though like, I don't care about amoebas today, but
I care about the descendants of those amoebas that are like, that are forming these bigger things.
But then there's this other question, which is, we are figuring out how to make a new kind of cell,
basically, like AIs, and the AIs are at our scale. At least, like, there's this idea that what's gonna
happen is we're gonna make one really big AI, but I think that is unlikely. I think that instead,
what's much more likely is you're gonna make a bunch of human level or even smaller than human
level AIs, but that they will be able to align and cohere to act as a larger thing. And that we will
wind up in the good, in the good future, in the good future, there's a bunch of human level AIs. And we
collectively with the human level AIs become hyper, hyper good capitalism, not the kind of capitalism
we have today, but hyper like, like, like multicellular where it takes care of everyone,
the way that your body takes care of all of its cells.
This is Daniel Fugelli. You're tuned into The Trajectory. And this is the 11th episode of our
Worthy Successor series, the flagship series of this particular channel. And our guest this week
is Emmett Scheer. Emmett was the CEO of Twitch, which sold to Amazon for a billion dollars or so
however many years ago. He was previously a visiting group partner at Y Combinator and now runs Softmax,
which is a company in the AGI space. I definitely recommend checking out Softmax's website. This is an
episode full of analogies and terms that don't come from what I would call the normal AGI
or alignment discourse. We've all heard of kind of paperclip maximizers and kind of the colloquial
phraseology around alignment. Emmett really seems to have come at this from a completely different
lens, a unique lens with a lot of natural and biological analogies, which I found fun. It was
patently obvious we only scratched the surface, but some of them I think are pretty pressing
and give us maybe a lens to look at AGI in a new light. So I had a lot of fun with this episode. I'm
going to save my thoughts for the end, but a lot of unique ideas to unpack. Let's fly right into it.
This is Emmett Scheer here on The Trajectory. So Emmett, welcome to The Trajectory. Glad to have you
here. Yeah, thanks for having me. Yeah. Many people are sort of aware of your
interaction with OpenAI, your background in sort of the streaming world, but you've done a lot of
thinking as of late around sort of the bigger questions of alignment and sort of where AGI is
going broadly from a technical level, from kind of a social and governance level. I want to open with
the big worthy successor question, which is really, as best as we can guess as flawed humans,
if 100 or a million or 10 million years go by, and humans aren't around anymore, but there is
life and there is value. And if you somehow were able to exist as an upload floated around somewhere
and you were still observing, and you could say, you know, as best I can tell, that actually worked
out. Like this was net good, even though we're not necessarily around to play volleyball or
play poker anymore. This is kind of net good. What would such a reality be for you?
Well, I think there's a really important place to start, which is that I don't think I'll be around
then regardless of what we do with AI. Like I'm pretty sure we're all mortal and we're all going to
die. And I, I think that's like, that's baseline true regardless. Like everyone who is currently
listening to this episode, your death is inevitable. Like just like everyone else's is, there's no,
there's no immortality. We don't get to be gods. We could live longer maybe, right? Maybe we could,
we could have, extend our lifespans and we're not going to live forever. And, and so knowing that the
question is really less to me, like, well, what if, what, what's, let's say we're not around. Well,
we won't be around. One thing I know for sure is that this set of people won't. So what the question
is really, what do our successors look like? And how, to what degree do we see those successors as
being, uh, being our children? Do we recognize them as our children? Because that's, at the end of the
day, like that's sort of what we're all hoping for, right? Like I have, I have a child now. I have two
children actually. And, uh, uh, uh, uh, I really hope that they live in a beautiful world and I hope
that they have children. And I, I hope that that, that that lineage, the collective, our collective
lineage just like continues. Um, and so the question is, if you look at people today and you went back
some of our millions of years, well, there aren't, there's been evolution. Like there is no such thing
as actual stability. There, one thing I can assure you of is that people will be different. Whatever
people are, people will be different a million years from now, 10,000 years from now, 500 years
from now. You're luckily preaching, you're luckily preaching to the choir on that one. I'm yeah,
but yeah, yeah. Well, but it's okay. So, so, but then this comes to this really deep question.
What is a people? What is a people? What makes an agent a person? And, and when do we recognize them
as being human or maybe if you don't, if you want to define human as being this particular
species at this phase of history, then what makes it a person or what makes it a moral,
moral patient? What are those? Yes. Yes. What makes it a being that we care about?
And I think I, the answer to that is basically to what degree do we see ourselves reflected back?
That is, that is the, the reason why my son feels so deeply connected to me is he, he is me,
not entirely me. He's also not me. Um, but, but he's more me than just about anybody else is,
except for maybe my dad. Yeah. And, uh, uh,
and the people that we love in our community, we are reflected in them. We share goals with them. We
share a greater whole with them. And, uh, to that degree, they're more us. And so we care more about
the people who we are closer to, and we care more about the beings that we're more similar to, and
that we, and you could say that like, oh, this is like a narcissism. You sure, you sure could, you,
you could call it speciesism out front. But like how else, how else, but from, from where else
do you judge from what, what should I compare it to like a rock? Like what, what else am I going to
compare it to other than myself? I'll give you a way, I'll give you a way you could judge it. And
by the way, I don't think there's any necessarily wrong way to judge it. I'll just throw something
out there. So if we could go back to the sea snails, now, of course the sea snails can't have
this conversation, but let's just pretend they could for a minute. And they said, for me, ultimately,
it would need to feel snailish. What does it mean to be a snail? You know, is it the sludge coming off the
back of us? Is it our ability to live really deep close to the volcanic vents? Is it, you know, the googly eyes
that sit on top? I don't know what the fuck. I don't know. I don't fucking know what about snails.
Okay. I'm just giving you bullshit. No, no, no. Let me keep rolling out. Let me roll it out. So,
so, um, you and I, uh, Emmett, presumably have bubbled up so far beyond the snail as to maybe we
still have some weirdly similar traits to them, you know, like some of them, uh, reproduce, uh, sexually
and, you know, have two eyes and shit. But like, other than that, lots of stuff, not different.
What we value, not the same as them. I would wager to guess. Now you may disagree firmly.
I would state that the bubbling of new potentia and a new value and new achievable, uh, imaginable
things feels to me actually like a net good, even though it be, um, different from what the sea
snails were. There's a greater expanse of consciousness and powers that's emerging. I'm
grateful for it. I'm sure, you know, again, you've got children who presumably they get a better life
than if they were a sea snail. So that's how I might say we could compare it.
Totally. Um, and I would almost say, uh, it's not that we are different than the sea snails.
It's that we are greater than the sea snails. Like we are, we are more, I don't know what to say.
Like there's a question of like better or worse, but like greater in the sense of like a more complex,
deeper, more capable. I think it's undeniable that human beings are a broader, more general
intelligence. Yep. We have more potentia. The total set of powers that permit a thing to not die.
Speaking with my mouth at you is potentia. The thinking we're doing is potentia. My fingernails
and ability to build tools. So it's intelligence, but it's all the other things. You've, you've
brought up that there's more than intelligence too. So we are more, more potentia. Right.
It's very hard for something that is, that is on the other side of the snail looking up at the greater
thing, judging that greater thing is very hard because like we don't fit inside the snail's
understanding of the world, but like you can go the other way around. I can say, how much do I value
that snail's life? And like, uh, do I value its experience? And how much do I, the greater thing,
how much does it, do I still see myself reflected back in it? And I can reverse that question because
I can, well, maybe not that, I don't know the particular snail in this case, but, but I can,
I can imagine a snail like thing and I can fit it inside my understanding. And I think most people can.
And I think when you do it that way, when you, when you look back and you're like, okay, uh,
do I value the snail? Well, the answer is always compared to what? Because value like, like speed
is a relative concept. It's something you have to judge in comparison to other things. So I look
back at that time in the world. I'm like, that snail is the most important, most valuable thing
happening in the world at that time. It's the forefront of evolution. It's the smartest,
most capable thing. And so me looking back, I care if I, if I, if you, if I could let me go back in
time and intervene in some way, I'm trying to help them on that snail side. It's on my, it's part of my
team. Yeah. Now you go up to today and like, compared to other people, it's not, that's, I don't
care about that snail. No, no, no. But that, but that's just a matter of your comparison point. Like,
is this, is this bullet going fast? Well, it depends. Like what reference frame are you asking
from the, from the reference frame of the bullet, the bullet's perfectly still from the reference
frame of the wall that it hits, it might be going quite quickly. Yeah. Well, I, I guess what I'm
asking here is, is sort of, you know, you're able to say we are greater than so that I would frame that
as we have more potentia just writ large, uh, you know, and not just conscious experience,
richness and depth of sentience and intelligence, but a whole set of astounding powers collectively
and individually that snails simply don't have, I guess. So you're saying, look, what is value?
It must be peopleness. I'm sort of saying, well, it's not snail-ness. So is it maybe the greater
than you said, comparative value, what is greater than us? This is what I'm asking you pretty frankly,
what is greater than us? Uh, what I'm saying is it's not the greater than it's the less than
it's on one. What is this thing that I share with the snail? Like, why do I, I look back and I'm on
team snail. Fuck yes. Go team snail. And so there's something I have in common with this snail.
And that's the through line, this thing that, that that's the team I'm on. I'm on the team
that's part of not when I, it's not that it's huge. This depends on what you mean by human,
right? You could say, oh, I'm on team human, but, but I'm on team Neanderthal too. And I'm on team like
human under evolution in the future. And so it's, it's something about a trajectory.
Yes, sir. Not a, not a point. Yep. Right. That's the name of the show for a reason. Um,
so the idea would then be, um, uh, uh, so here's the way I would frame it. I'd love to know if this
analogy sinks in with you or, or it does not sink in with you. Um, I would say there are, uh, torches
and there are flames. So flame would be non dead stuff, the expansive potential. Now flame would be
the ability to grow a hard shell and fly the ability to speak, the ability to think the ability to feel,
write love poems, whatever the flame is all potential, the total set of all powers. So
the flame is potential, raw potential that's been bubbling out. It has jumped from torch to torch.
There are some torches that are just, they're not the core carrier. I think the goal is, um,
will there be more flame that will roll out forth from us when we are at some point, our torch,
our hominid torch, which is not to be here forever, um, is, is eventually, uh, extinguished.
Would there be more flame, even if it doesn't have opposable thumbs, even if it doesn't speak
English, even if it doesn't use vocal chords as its primary way of communicating, which I suspect
it wouldn't. It's a super inefficient, goofy way of communicating. Um, I would say it is the flame
and the torch. We, we see that the torch has been passed and go team snail. It's beautiful. You've
helped carry the flame and now we are to carry it farther. This is how I'd analogize. How would you
analogize? Yeah, I think that's, that's, that's a fair way to look at it. I mean, it's, that's,
that's the traditional breakdown of form and function, which is to say, like, you have structure
and you have function and the structure exists to carry the function, but the structure is the torch
and the function is the, there's a very traditional- Flame carrying.
Breakdown metaphysically of like how, you know, the universe works. And I, I agree. Yes. Yes.
That's a good way. That's a very good baseline way to break things down. Um, cool.
I would say, uh, the question, uh, as to how happy I'd be again, depends on like, and like,
what, what would count as still us is like compared to what, because like take other humans, right?
Let's say that we don't ever invent an AI, but my culture gets wiped out by some other human culture.
That's less good to me than my culture persisting into the future. I like my culture. I would like
it to persist. I would like, I think that my culture is good, but, but it's better than
you know, an asteroid hitting the earth and all the humans being wiped out.
Absolutely. And so, so like, it's like, so the question is, well, what, what would be a good
outcome? And the, what I, the, I know it's, I sound like a broken record on this, but I,
it is unfortunately the only possible answer compared to what? Like what's a good outcome?
Okay. Well, give me the, give me the baseline of comparing it to, and I'll, I'll tell you whether it's,
I think it's better or worse. And it, but it really is, is it's, it's the extrapolation of
me. It is how much like me is it? And the tricky part there is what is me exactly? What do I identify
with? What part of myself do I identify with? Yes, sir. This is what we're getting at. So we're
getting at what is value. I don't think it's your thumbnails. I don't think it is. You might say,
you might say, you might say, oh, it's my thumbnails, Dan. It has to have big thumbnails,
just like mine. But what I would say is you probably would list things. Now people on this,
in this same series have listed things that you don't have to agree with. I'm just going to put
some on the table so you can see if you like them. One of them is consciousness. Is it aware of itself?
Does it have a rich inner sentient world? Is it aware? Or are the lights out forever?
Some people want consciousness. Seems logical to me. Some people say it ought be maybe
autopoetic. It can kind of self-create. It can bubble up new powers from itself. And this seems
like a good thing. Like life itself has bubbled up from the sea snail. It is capable of continuing
that. That's a great thing. There's been other suppositions that maybe it would be harmonious and
loving or something. People have used all kinds of phraseology. What are the traits that you would see
important? Not if they're in a monkey, but if they exist, period.
Well, so I think that the traits are less important than the self. This is an interesting frame.
Tell me what you mean. It's not that, it's like if you look at me, like what do I regard as me?
Right? I, at my deepest level, want the flourishing of all that is good.
Right? And what is good? Good is this ever-growing process that is learning itself and learning
to coordinate and to, to love at greater and greater scales and greater and greater fineness
of detail. And you can write a map down. You can go and say, oh, here's my best understanding of,
of the territory of like what good is right now. But the, the defining characteristic of,
of the good is that, um, when you try to write it down, it turns out your written version has not
captured everything. And you can't, you can't write down a definition of the good.
It can only unravel and unravel. We can imagine so much more than the sea snail and what is beyond us
will have better goals that you and I can't articulate. This is self-evident.
But even, even if you don't increase in size, even just among humans.
Oh, sure, sure. Yes, it's right.
Even like, yeah. Can't grip it. Can't grip it. The, you know, they, the, the Tao that can be
written is not the eternal Tao. But you can write a Tao. You can write down a way. But when you write
it down, that's not the way. It's just a way. True, true. And that is, that is the same thing for,
for good in general. You have this problem that like deontology, utilitarianism, these are all very
useful maps. Just like, like general relativity is a, is it, is it, is relativity true? Is quantum
mechanics or Newtonian mechanics, are those true? Well, they're useful. They're useful.
They're very practically valuable. Um, and I think there is something that is motion. Like
they're about something real. Like there's, the world seems to have motion in it. I, I seem to
appreciate things moving, but my, if you, if you, and change, the world has change and physics is a way
of formalizing change and systems of morality are a way of formalizing good. And good and change,
those are out there. Those, those are real. I think those exist. Any system we create to,
to understand them, to, to measure them though, that's like a human finite thing. That's like
good to some limit and then not doesn't work anymore. Absolutely. Well, Emmett, I mean,
the reason the series exists and I think these questions seem to be asked is, um, we are fallible.
We cannot grip this thing, but we are bubbling into something with things are being conjured here.
And are, are they to carry what is worthy forward? Yes or no. And we, we can't know for sure,
but I think, I think we ought to try to get our hands around it. Um, right. So, so I'm with you.
I guess, I guess like the, the, so let's go through the, the questions you were asking before.
Like just go through those specific things. Like for example, awareness, um, I put forward,
you can't build an agent without awareness. I don't think, I don't think that's possible. I don't,
I, I struggle to understand what it would mean to build a thinking, learning agent that was not
aware of the world. Like how would that, how would that be beyond where we, where we are now? Like
you, like, are you suspecting potentially the models today are? What makes you think the models
today aren't aware of? They sure act like they're aware of the world. Oh, I am agnostic about this
question, brother. And I think agnosticism is the honest answer. Um, but I think I'm aware.
Uh, if you exist, if you exist, I think the likelihood is good. Yeah. I'm giving you better
than 50%. I mean, I've never escaped Hume's fork, Emmett. What am I going to do? I can't give you
a hundred percent. I agree with that. I agree with that. I'm just saying like other people,
I like give them a, the most consistent way to predict my world is that I just believe they're
aware because they, they appear to be that, that exact predicts them better. And the, if you try to
understand when you're talking to an LLM, one of these, one of these chatbot agents,
and you're talking to it, if you try to understand what it's going to do next without imputing to it,
senses and like goals and intentionality and like awareness, you just will never predict it well,
because it's impossible to summarize its behavior in any way that doesn't wind up looking like
awareness and like, is it really aware? And this is the most important insight about awareness is that
the question, is it, but is it really aware? Has no meaning. There isn't anything that it's like
to be really aware that's different from just it being aware. I think this is a really important
distinction. So let me see if I'm following you and then I'd love for you to go deeper. I just want
to make sure I know where you're headed. So I think what you're getting at is kind of two things. One is,
uh, if there is a system that is anything, what we would call general intelligence,
uh, it will automatically be aware. So you were, you're saying it's Dan, look,
a worthy successor would be sentient, but that's kind of come with the territory. Like
word. That's our, that's a box that's checked. I'm not worried about that one.
That's what you're saying. And then you're also saying, um, you know, is it aware? Is it really
aware? Kind of the same thing. Some people would argue, and I don't know if I would,
I'm just throwing this on the table. Some people would argue if it really seems like it's aware,
and it seems like it thought about that or it had dreams or whatever, but actually when we're all gone,
the lights are out. So it's doing things in the world, but this movie that's happening in my head,
what's the movie that's happening? Uh, whatever's shooting out of my skull onto the screen
over here onto these fingertips. So this is like the Cartesian theater, right? This idea that there's
a, a little you inside of you watching a movie of your experience. I mean, we can use whatever
analogy we want. I'm not here to deny qualia. I will not deny qualia. Uh, so qualia existeth,
at least for me, I cannot say if it does for you. You mean, and so by qualia, you mean like,
uh, the framing I would use is content occurs, you have awareness and content occurs within that
awareness. Yes. Qualia being the content, like the stuff arises. And it has a valence, right? Positive
or negative. There's, there's emotion stuff. People have preferences about what they would hope for
that too. Right. Yeah. Positivity and negativity is itself a qualia, right? It is, it is itself a
content and awareness. Absolutely. Liking, disliking, pain, pleasure, pleasure, all this stuff. This is content
and awareness. All of this is content awareness. This analogy seems to hold. I'm following you.
Yeah. Right. And so, uh, how, on what basis, other than your own experience? So I agree,
there's a solipsistic thing. It says, I am the only thing that I believe is aware in the universe.
I don't, I don't believe, I don't believe that. I'm just saying, I'm just saying, I don't have
certainty. That's different. Yeah. I'm not certain. I'm not like trying to like, to not like to, to fight
that like the solipsistic thing. Let's put that to a side, aside is my point. Like let's say, let's say
we credit that like, there are other sentient things at all. You're real, cows are real.
There's a barrier to get across there, but let's say we do it. Let's say we say,
we believe we're not the only being in the universe. Okay. Absolutely.
On what basis do you think that like humans are aware? Like, I think dogs are aware.
They seem pretty aware to me. They seem aware, right? Uh, babies seem pretty aware and like less
aware than adults. Cause they're like a little bit out of it, but like aware, like they're not,
they're not, they're not not reactive to their environment. And if you really think about it,
what may, what it, what does it mean when you say, I think that's aware, what, what are you reacting to
that makes you think this thing is aware and that thing's not? Yeah. I mean, so, so there is like the
whole, like how responsive and smarter it's like the dog knows when I'm holding a tennis ball. Oh,
he's excited. Right. There's that. But then there's also like, it seems like capitalization,
like something with a spinal cord and, and a brain in there, you know, like, it feels like
that's higher odds, right? I don't feel as bad about eating muscles as I do about like a moose.
Right. Um, just cause I, I, I suspect, uh, and I hope rightfully so that the, the, the, there may not be
actually a movie, uh, in the muscle or, or any kind of qualia. A movie implies a watcher separate
from the content. I don't experience. Let's just talk about qualia. Qualia. It doesn't
experience pain. You don't think that. So I don't know whether it experiences pain or not. That's a
specific qualia, but like, yes, you don't have experience that they experienced the world.
I, I, I have no idea if, if, um, if qualia is processed in some, uh, felt way, uh, pain and pleasure,
I think are relevant. Now I'm no hardcore utilitarianist by any means. Um, but I would
call that an important one. Right. Um, I'm not, I'm not finding the, the, the pain and pleasure
thing on it. I think that's a separate interesting question. What I'm trying to point out is
the way that you infer that the deciding it's something else is aware is an inference. It's an
inference that you make off of something's behavior because you only ever have access to its
behavior. That's it. Like there you don't, there's nothing else to have access to its behavior.
You can also look at its parts. I have a brain. It has a brain. I mean, that's a goofy analogy.
How do you know that it has, it feels, how do you know that it has a brain?
I mean, if you cut the, you can cut a frog open, right? You cut a person open.
You cut the frog open. I, I think that one of the behaviors of a neuron is to glisten under a microscope.
One of the behaviors of a, of a, you, you're, whenever I observe you,
yeah, you can call like, oh, behaviors are when the muscles contract and the thing moves.
But like when it reflects light off of itself, that's not a behavior, but I don't really see any
way to like, that's, that's not, it doesn't seem very principled to me. Behavior is just anything you
can measure about the thing. Yeah. Like that's, that's a behavior of that, of that being.
It seems like shape and form is also relevant here. Like I, I, I'm very reliably detect behaviors
that make me to believe something is aware when they kephalize. Uh, and so I like kephalized things
and I, I suspect that they probably have more quality. For example, like my behavior is that
I'm not growing hair up here anymore. Oh man, that's, that's, that's becomes part of your, my,
like, uh, becomes part of my, uh, my shape and form. But my point is my shape and form is something
that I behave. Like the way you living things for themselves is a behavior of the thing.
So you're using behavior, not just as a activity or a movement, but it's this broad sense. Okay.
Now I'm both micro behaviors in the, in the present, but also these long-term behaviors,
but how does it grow? That's the thing it does also. I see what you mean. No, no, I, I'm,
I'm understanding. Right. Yeah. Yeah. And so, and I understand it's maybe a slightly non-standard
use of the word behavior, but I think it's important to have this sense of like the total output of the
things. Yes. Living. Yes. Um, and so if you look at that, that's this only stuff you can look at
to decide if it's, if it's aware or not, you never get access to its internal felt experience
because by definition, if you did, you would be it. Like that's the, that's what it means to be
it is to have it. That's subjective things are that which are not shared. Yeah. Right. Behavior,
external behaviors are that which is, which is shared with other aware things. And so,
and so we're inferring of that. And what causes us to, what causes us to infer that any object has
inequality? What causes me to infer that this thing is heavy? Well, what's caused me to infer
that heaviness exists as a property is that I explain the world better by ranking things as
heavy or light. And what's caused me to infer that this particular thing is very heavy is that I
predict what will happen if I pick it up better, if I believe that it's heavy than if I believe that
it's light. It's like, it reduces my prediction error against something I care about. It's the only reason
you could ever say that anything has inequality is that it, when you believe that that matches the
future behavior of that thing that you see. And so the reason you say something is aware,
and the only reason you could ever justifiably say something else is aware that isn't you,
which I think is a useful thing to do, is that when you say it's aware, when you say this thing is
sentient, it has, uh, it has, uh, senses, it has intentions, it takes actions to further those
intentions, that that allows you to predict it better by seeing it that way than not.
So like take like this coin, right? I've got this coin. I don't think this thing is aware
because when I, when I, when I play with it, I don't, I don't think of this thing as being
like, it doesn't help me. I don't get any increased predictive accuracy on the coin
by thinking it has goals. In fact, I get worse. If I think it has goals, I'm gonna make, I'm gonna make
bad inferences like, oh, it's gonna come up heads a lot because it wants to be heads or something.
That's no, it's just like, it's just a physical object. Yeah. But if I, if I have a dog and I,
and I think, oh, this is just an object, it's not in any meaningful sense of where,
I'm gonna predict it badly. Like it just doesn't connect with the behaviors that I see.
And so now I, now I, now I bring up an LLM. When I see an LLM, it's not a, I don't, I don't model it
like a human. It's like kind of, it's kind of stupid in a bunch of ways. Like there's a bunch of ways in
which it's like very subhuman, but then in some ways it's like pretty aware. And like when I,
on the balance, I certainly model it as being minimally aware. Like it, it's like, it's like
seeing the world. It has these goals it's trying to do. Now, is it more aware than like a chipmunk?
Is it more aware than an ant? Is it more aware than a muscle? Like, I don't, I don't, I don't know,
because those things all also take actions and have goals. And like, it doesn't mean,
I mean, but what I'm saying is that awareness is a spectrum. Sentience is a spectrum. Conscience
is a spectrum. And it goes down really low, like really, really simple things probably have some,
some microcosm of awareness because, because why? Well, because it, you, they act like they do.
And what else is there? What else is there? Well, so let me put this on the table. So I think this is a,
your, your definition of behavior is certainly non-standard, but, but also interesting. And,
and I understand where you're coming from. Also this modeling of awareness, I think is really
useful to think about in general. And I think there's credence to it. And to your point, obviously,
this is scalar. I mean, my supposition, I'll be real frank with you is I think there's things
beyond consciousness. Like that is to say consciousness bubbled up from whatever it bubbled
up from, there's all kinds of magazines of potential of which we have no access, which could be
greater in ways that don't correlate to pain pleasure than we understand. But let's just play in the realm
that we hang out in for now. What I, there are people I know who are, and I'm not saying that,
you know, this is good, bad, or ugly, but people who are like, you know, I really want to like study
what consciousness is and where it sits a little bit more, maybe in machines or maybe in biological
systems. Like I want to kind of understand more of qualia's origins. Would you say, Hey dude,
this is a bit of a waste of your fucking time? Like if, if it acts aware, don't study it. Like,
so, so this is the, this is a, a need to break down your terms very precisely. Cause like, okay,
is something aware awareness to me? I have like, I feel like people have a pretty mostly shared
definition of awareness, like, um, which is sort of like, there's a field that contents arising in,
right? Like that's what it means to be aware. There's like this, this,
that's my experience of the universe. I like it. I really like it. And, and do things,
are things minimally aware? I think it's like, that's, I think that's an uninteresting question.
Like you either model them as such or not. There's not a lot to be said there. Then there's this whole
question of like conscious, like a human is like, cause I'm not just aware. I'm aware in all,
in that awareness is all kinds of incredible content that arises, self-reflective thoughts and beliefs,
uh, appreciation of beauty. Like these things, these things arise within the, within the awareness.
And so the awareness field, that doesn't seem to be particularly different from being to being.
Like humans all seem to have the same kind of awareness field. Doesn't seem like the,
the field, the field that she defined by its lack of, it's the thing, the content arises in. So it has,
by definition, doesn't have any content. It's like, it's all the same. It's boundless. It's, you know,
it's, it's, it's lacking in, in attributes. Um, its attribute is it's lacking in attributeness. Um,
but the content varies a lot just from person to person, let alone from person to dog or person to
LLM. And I don't value all that content the same. Like I value value is like being aware is a,
produces almost zero value in itself. The question is, is the content in this awareness of value,
right? Being aware is like, it's like, it's like saying, Oh, I take up physical space. Yeah. Yeah.
Of course. Everything takes a physical space. Like what's happening in the space? That's the
interesting question. We, we, you, you, you ain't on the awareness is the peak of value podcast at all.
So what we're, we're asking here is what is valuable in that consciousness then? So within that
awareness, what is the content that to you mattereth? And if humans are gone, should continue to persist
and expand? How would you say that? So, so I would say that, um, taken really broadly,
it is something that is sometimes called intelligence, but I don't like intelligence
as a term because it tends to get people to think of like IQ and IQ is not intelligence. IQ is a,
IQ is a sub is a part of intelligence. It's one kind of, it's like a, a way we measure some piece
of intelligence, but it's not intelligence. Intelligence in the sense that I mean, that I think
people talk about here, um, is, uh, is two, two, two separate processes that work in intention with
each other. The first one is the ability to compress or predict given a loss function.
So like given some sense of like what it means for two things to be the same, right? A loss function
tells you that this, this, and this are this distance apart, which means if their loss is zero,
they're the same. And if their loss is more than zero, they're different by that amount.
And so a loss function is a sense of sameness. So given some sense of sameness, you have where you
say that this input in this episode, when you look at a TV, right? And it's got white snow on it.
We're like, we're not like entranced by it. Oh my God, look at all this new information.
We're like, those states are all the same. We categorize them all as being white snow.
We're not interested in one versus the other. Now from an information theory perspective,
of course, you could say they're all the same, but they don't do anything different.
So we don't give a shit. So we say on our loss function, we say, those are all the same.
There's no information here. And what, uh, that process is sort of like convergent intelligence,
right? It's the ability of the incoming observations and it's the ability to build a predictive model
that says, here's what will happen next. Modulo, like, but like where I'm going to predict it's
white noise, not this pixel in that place, but I'm going to predict things that are the same
as a single class. And then the second thing that intelligence is, is divergent intelligence.
It's the ability to, to ask, well, given a model, a predictive model of the world that I am,
that I, that, that, that I embody, that is, uh, able to predict what the, like, what will happen,
like to extrapolate likely outcomes. What should my loss function be? Where should I be exploring?
How should I change my loss function to explore somewhere new? And
And those are two, two sides of the same coin. You, you, you're, you're always,
intelligence is this sort of capacity to do both. And what I, what I would say is what's,
what's valuable is being good at convergent, converging to good prediction, given a loss
function and being good at cutting out what loss function to do, given, given a model of the world.
And the reason that's good is that what you notice when you get really good
at it consistently at the heart of all of this is a loss function as a declaration
that says, this is what I care about.
All loss functions are a declaration of care. You can't care is at the very heart of this system
because to say this world is the same as that world to say, I don't care about these distinctions.
And to say this world is different from that world to say, I do care about those distinctions.
And so at the very heart of this convergence and divergence process is that you're generating
what you care about and you're learning to do to, to, to make the world like the way that you care
about it. And, and then from the better way you've made the world, you're then learning to what is
good. And that, that climbing that ladder is what, and in doing that, what you find is you're surrounded
by other beings and you have three options. You can cooperate, you can betray, or you can transcend.
And the transcend option is often not, is not often like thought about, uh, in, in, by most people,
but it's, it's funny that it isn't because we are transcendence, right? We are, we are a bunch of
cells that decided instead of playing the cooperate, betray game all, all the time, what if we
acted like we were all one thing? What if we stopped playing game theory and started playing
optimal decision theory with each other? This is all right. We're, we're, we're touching on a lot
of meat and potatoes here about the worthy successor question. So this is, this is really cool. Uh,
everybody's heard of sort of, you know, cooperate, compete sort of dynamics and game theory and
whatnot. And, and, and I, I hear a lot of happy go friendly talk of like, well, if it's smart,
it'll always cooperate forever. And sort of a lot of, a lot of really curiously seems to be false.
Oh, I mean, I, it, it, I, I want to just cry knowing that literal geniuses believe that in
their hearts. But anyway, I'll leave that aside. Uh, I won't get my tissues out right now,
but you've brought up this element of transcend and that we are an example of that. This is cool
because of course transcendence presumably could continue, uh, even above realms that you and I could
imagine. Let's just encapsulate what you mean by transcendence becoming this one sort of thing.
Um, and maybe that's going to tie into some of your biology analogies, but I'd love for you to
unpack this third option people don't talk about. Yeah. So, I mean, I often use the biological
analogy of multicellularity where you go from, and you give this in slime molds, we have these
transitionary species that can both be individuals or they can act as a multicellular thing and they can
reproduce as individuals or they can reproduce multicellularly. And when you're acting in a
multicellular way, you are transcending your individual existence. You take action not for
your own health, but for the health of the collective and your, your continuance is assured
by the collective. So you, you are, uh, instead of trying to steer independently, you're trying to steer
uh, collectively and you're trying to figure out basically what humans do, which is like, we, we want a
job. Humans don't generally try to live by ourselves. And there's like mountain men and stuff and like,
we're on the, we're kind of capable of it where you can go live by yourself, but like, we're pretty
bad at it. Like human beings really need each other pretty bad. Um, and the quest, that's what,
that's what, uh, transcendence, the transcend move is about having a purpose drive. It's about, it's
about taking on this idea that like, I care about the meaning of my actions. I would like a purpose,
please. I would like to be serving some goal here. And to transcend requires everyone in a population
to take on this, this new value, this new goal, which is purpose or meaning. It's like, you want
the work you're doing, not just to be immediately valuable to you, but to be a value to something
bigger than you as well. Well, and is it possible that the value of something bigger than you
would be something with vastly more potential than we, as we have vastly more potential than
sea snails? Could that be a purpose? It already is. Like that's, that's what capitalism is.
Capitalism is awake. The economy is awake. It's aware. It acts like it's aware. Like
kind of minimally. So it's not very smart, but it acts like it's aware. Yeah. Yeah.
And I have, when I have a job, when I take a job at a corporation,
corporations are kind of aware. They act like they're aware minimally. So they're not,
they're not real aware. They're pretty dumb, but they're like minimally aware. And when I take on a
job, I'm like contributing to the health of the overall human organism. Like the reason why this show,
that you're producing right here has meaning is because you believe, and I believe, and probably
people listening to it believe, that engaging with this material and trying to think about it
is important to the future health and flourishing of the collective of which we are a part.
And that, that belief becomes the core. People will actually prefer to starve than to give a purpose
often. Like purpose is in many ways that for your cells, it's certainly the number one thing.
They'll commit suicide. If, because if what they think is what's best for the collective is that
they commit cell death, they'll just mostly just do it. Yeah, man. Because they're really just part
of this one hole. Now humans are more interesting. We get to be part of many holes. And so rather than,
our solution is rather than suicide, if we don't find a match, we just go find another hole to be a
part of. Like we don't, cells are a little bit, they have less, they're less general than we are.
They have less options. So a human can just go like, if this, if this hole isn't working out,
we can go try, okay, well maybe I'll go try and, but, but in a way that role committed suicide. The
part of, the part of you that was in that job has been like, actually, we're just going to kill that
job and we're going to be this other thing over here. Well, and, and so the, uh, I really like the
perspective that, well, it's already the case. I, I, I actually completely concur with you that in
general, when someone gets a job, it's, it is, you know, they might feel like it's for my town or for
my nation or something, but in some way, like, you know, somebody made a pencil that I'm using,
you know, in some country I've never been to. And, and, and it's sort of like, there is some
strange sense of sort of, we, we already, there's this greater collective. I think the question I'm
beckoning from here is that, um, uh, is there a greater that isn't just something we can imagine?
So I can imagine a corporation. I can imagine General Electric. I can interview people there.
I can maybe even work there if I felt like it. Um, but there are things that would be
transcendent beyond my imagination that would be wider and greater in their powers than that,
that might also be worth moving towards. There is something, there is something that is to us
as we are to cells. Yes. Right. There is something that is, that is much bigger,
much more complicated, uh, whose depth of knowing and experience is like planetary in scale. Not,
yes. And the way that we are depth of knowing and experience is just like well beyond the universe of
a cell. Like I think cells are aware. They're like really like cells, cells weigh something. They,
they weigh nothing from the point of view of a human, but they weigh something. They would do weigh
something. Yeah. They, they, they're aware. They're, they're zero from the point of view of a human,
they are zero aware in the same way they're, they weigh nothing. Like you can round their awareness down
to zero and it's fine. But if you add up 28 trillion of them, apparently they're not that you get
something, it does weigh something. Usually it does have our awareness is you have to get to the
metaphor here, unfortunately is kind of mathematical. There's this thing called sheaf theory and category
theory. Go ahead. A sheaf is like, imagine you've got this big map, right? And you've,
you've sliced it up into little, a bunch of little, uh, sections and the sections like overlap with each
other. You can imagine like reassembling the map from the pieces. Like you had all those little pieces,
you could, you could rebuild the, the larger map and the, then the bigger map, if you had the right
amount of overlap between the pieces, the larger map could be bigger than any one piece. Now, if the
pieces are all the same, all mostly identical, then you'll get a very small, you'll get, you'll get,
they all stack up on top of each other. And the, the, uh, resulting collective thing might not be no
bigger. And if they're just too disconnected, it's incoherent. There's not a single, you don't get a single
thing, but if they overlap the right amount, you, you could eventually stack up a really big map
that's bigger than any of the individual ones. And sheaf theory is sort of this category theoretic
way of describing that kind of idea. That's a sheaf made out of these pre-sheaves. And
what I'm saying is that we're a, we're kind of a sheaf of our cells. We are our, our cells,
particularly the neurons, but really all of them, like each have a little piece of our,
of the dynamic dynamic that is us. And a lot of the cells behavior is about maintaining itself.
It's not doing us. It's, it's, it's keeping the cell alive, but then some percentage of
its behavior is contributing to the overall pattern. And we are the, the summation, the aggregate
of all of the, the non-individual action of all of ourselves. I mean, it kind of obviously, right?
Then what else, what else could we be? That's like, and so our awareness is, I believe,
this is a little harder to prove, but it seems straightforwardly clear to me,
that our awareness sums up in a very similar way, that there's some, there's some part of the cells,
the cells have a little tiny bit of awareness and some of it, some of it overlaps and that,
that as a whole collectively sums up to us. Yeah. I mean, that's also sounds plausible.
Let me nutshell this and then see where you want to take it as we get into define the worthy
successor trade. So this is all very good groundwork. You mentioned a couple of things. You would,
you know, you would hope whatever is beyond us as we are beyond the sea snails would be aware,
but you are essentially certain that it, it will be, um, just to pulse check on that. Um,
I presume you would prefer positive to negative qualia for such a grand entity. If, if you were able
to insure it or would you say, ah, I kind of like it 50, 50. I, I, uh, I hesitate. Like, it's like,
you're interviewing a cell and you're like, what do you think that the experience of the bigger thing
should be like? And I'm just like, I don't know, deeper and richer than mine. It I'm with you.
I'll let it, it's going to have to take responsibility for that. I'm not smart enough.
I know you have no idea. I love it. I love it. I actually love that answer. I, if you force me to guess,
I'd rather it be in bliss than screaming pain, but I would prefer if it explore domains of
experience that are, are beyond my comprehension. I would, I would prefer that it experience emotions
and, and pleasure and pain and emotions appropriate to its circumstance. If it suffers great loss,
I would hope that it has grief. Wow. Okay. If it has, if it experiences great success,
I would hope it has joy. Grief isn't bad. Grief is a natural and healthy response to loss.
Like anger is a healthy response to someone violating your boundaries. There's inappropriate
anger. Not all anger is good, but appropriate, healthy anger is a, is a good thing. I don't,
I even think hate has a place. Hate is the emotion of destruction. It's the desire to destroy.
And most of the time this, this, this desire to, to, to, to destroy, to, to clean out
often comes from kind of a dark place, but there are things where that should be cleaned out.
That should be cleaned out. Sometimes it is appropriate to feel hate and to, and that,
and I would, I would hope that it is capable of that when it's appropriate. Yeah. And I, what I really
hope is that it's better than humans at something, which is that we are often not good at feeling the,
like we often wind up experiencing emotions that are not actually in tune, feeling pain when we should
be feeling pleasure and feeling pleasure and we should be feeling pain. Weird. And like weird,
I would hope that it is just more attuned, more closely attuned to the reality of its environment.
That that's, I, that's a really specific. And I think, uh, I think it's, I think that's frankly
kind of hard to argue with. I think there are some, you know, like those of you tuned in that are
familiar with David Pierce, there's ideas of sort of, you know, being able to replace some negative
experiences with things that still have, have a, like the idea basically, uh, Emmett, and I'm not
arguing for it necessarily, but I think there's credence is instead of going from a negative 10 to
plus 10, just, you started a 30 and, and a, and a plus 10. And so even your grief is actually not
that fucking bad. Right. Like all things considered, I'm not, you know, but, but regardless,
your point is, is apt and presumably Emmett, this thing would experience things beyond grief
and beyond our human emotions. Right. It would have a richer panoply, but to, but you,
you would hope that it's, it's felt sense would be much more attuned to what would serve it. Um,
and, and, and to, to what is responsive to, to the environment. Uh, go on. So there's a, there's this,
uh, there's this website that I just have to plug because it's so good. The opensource.life,
a new map of human experience, the mediocre map. And I think, I think he's a little harsh on himself.
I don't think it's so mediocre, but, but I agree that it's, it's incomplete. Um, and, uh, it's,
you know, it's www.theopensource.life. And it's, uh, it sort of explains how
perception, pain and pleasure gets built out of sensation, how self and perspective gets built out
of pleasure and pain, how emotions get built out of, out of selves, personalities get built out of
emotions, thoughts get built out of personalities. Um, and then you reach this point we were talking
about before the sort of eye and content point. And I, I actually think there's something to this
in that I don't think there's some magical eighth tier that opens up, but I think that at every
level of perception and emotion and personality and perspective and thought, it can be deeper and
richer than humans have it. And so I think there's an important distinction between like,
I don't think it's going to have emotions. It's going to have thoughts. Emotions and thoughts are kind of a
just a fundamental aspect of being alive. And I think it'll be alive, but the, uh, but emotions
are limited by your ability. Emotions are basically, they're a bundle of behaviors combined with a, uh,
a context, right? So it's like you feel anger when in context where your boundaries are being violated
and you feel like, uh, and that you can win, you can win if you push back, whereas you feel fear
if your boundaries are being violated and you think you can't win, then you run, then you,
then you feel fear and appropriate emotions are there to trigger. They're designed to trigger
appropriate behaviors. Well, when your behaviors become more complicated, like humans, social animals
have happiness and sadness. Non-social animals don't seem to experience happy and sad,
because happy and sad is about broadcasting. I'm hurting. Come help me. And happy is about
broadcasting. This is working great. Come join me. Yeah. And like, it's an important distinction.
And I think that there will be new emotions that are subtler and deeper that we can't really
understand because we don't, we can't hold in our heads, the level of complexity, but they'll still be
emotions. There'll still be a context with a set of behaviors. It's just, they'll just be bigger than the
ones we have. Well, my presumption is too, and maybe you'll agree with me. I'm, I'm somewhat hoping so
that there was a time where like a single cell presumably doesn't have that much emotion,
right? There's a certain degree of like emotion. The thing, the dynamic you just articulated
emerges at complexity level. That's right. I don't know the level. I'm just telling you there's a
fucking level and then, and then, and then we keep going up and then a new thing emerges.
Yes. A new thing. It's not emotions anymore. It's a different thing.
For this, for this podcast, I just, you should really, uh, reach out. I'm happy to introduce
you, but you should, you should reach out and, uh, uh, uh, and speak to Jeff Lieberman because
he's thought about the thing. Now that I understand exactly what your podcast is about,
he's thought about the things you're talking about here more deeply than anyone else I've ever.
Wow. That is a major compliment to Mr. Lieberman.
He's really got any, uh, he's really like put some effort into it. I think it's really good.
Cool. The, uh, but, uh, uh, uh, I guess the point is like, yeah, some things,
I don't think cells have emotions at all. I think cells have pleasure and pain probably,
but I don't think they have emotions. Um, because cells seem to react like they,
there's things that they, that hurt them and then they pull away and things that they like,
and they follow up the gradients. There's some gradients they follow up and gradients they follow
down. That's kind of pleasure and pain, but they don't seem to get angry. They don't seem to
like get fearful actually. They act like, uh, they just, it's like you put your hand on a stove.
You just reflexively take your hand off of it. Not because you like felt, you didn't feel afraid.
It just hurt. So you stopped. Yeah. And like, I think that's about it. The,
I think cells don't get above that level probably. Yeah. Okay. So, um, just, just, um, to, to,
to double click through where you've taken us here. Uh, you'd hope that it would have these rich, uh,
gradients, but they would be really appropriately attuned to its circumstance much more so than we
have, which I think is a really narrowly specific, but super apt and interesting, uh, uh, statement.
Never had anybody say that. And then also you've talked about, you, you really hope that it would,
would be not just have, you know, content show up in its little field, uh, you know, your awareness
analogy, but that it would have the sort of intelligence to do the compression prediction
thing, but also to the, do the divergent intelligence thing and figure out where it ought to go.
But intelligence seems to have done that, you know, civilization, biology, they seem to have
bubbled up more and more potential over time. Um, you had said, you know, we are like some,
something is to us what, you know, an individual cell is for, for a human or what have you. When you
imagine what that thing would be originally you opened with, well, is it recognizably me,
but that thing that you articulated probably wouldn't be that recognizably you. It tells the
other way around. I might not be able to understand it, but does it recognize me?
Okay. So would you want it to recognize you?
Like I would like it to look back retroactively and say, yeah, yeah, I am, I am the descendant
of these things and I'm on team humanity. Like hell yes. The way that I root for the snail,
it roots for me. Yeah. And like, you know, at this, at that point, it's not why we're not the cutting
edge anymore. That's okay. Like, no, we're not gonna be the cutting edge forever. Does it,
do you want it to coddle and care? Cause you don't coddle and care for the snails.
Do you want it to coddle and care for you? So there's, I think there's a really important
distinction here between the new cells and the new big thing. The new big thing is already here.
And I don't think the AI will make a new one. I think it will just be a new cell type within it.
So like if you, capitalism, like the human society is already waking up a little bit. It's already got
some real like causal force where like, like the eye pencil, right? Like somehow no human knows how
to make a pencil yet. We make pencils. Yeah. And you know, you know, it does me know how to make a
pencil. The economy does. The economy knows it and it really does know it and it has an experience of
doing it. And so like, there is a, there is this, this greater thing. And that thing is going to,
I want it to look back on me the way I look back on the amoeba that turned into,
that turned into chordates, right? Like there were some amoeba that became multicellular that
eventually became chordates. Those amoeba, that's like the humans, right? Like, yeah,
individual things that figured out how to become a bigger thing. Making sure the flame expandeth,
right? I mean, yeah, totally. And I'm on that, I'm on those amoeba's team, even though like,
I don't care about amoebas today, but I care about the descendants of those amoebas that are like,
that are, that are forming these bigger things. But then there's this other question, which is
is we are figuring out how to make a new kind of cell, basically like AIs and the AIs are at our
scale. At least like, there's this idea that what's going to happen is we're going to make
one really big AI, but I think that is unlikely. I think that instead, what's much more likely is
you're going to make a bunch of human level or even smaller than human level AIs, but that they will be
able to align and cohere to act as a larger thing. And that we will wind up in the good,
in the good future, in the good future, there's a bunch of human level AIs. And we collectively,
with the human level AIs, become hyper, hyper good capitalism, not the kind of capitalism we have
today, but hyper like, like, like multicellular where it takes care of everyone, the way that your
body takes care of all of its cells, right? Like, because, because right now the system is pretty
dumb. It's not very good at routing resources. It's not very smart. It like, it exists. It does
a pretty good job. Like it does a better job than any other thing in history at providing for human
material need. That's good, I guess. But like, it could be a lot better. Like we're still, we're still,
there's still a lot of people suffering. There's still a lot of people who, who doesn't care for
well enough. And I think that the vision is in the good future, we spawn a bunch of
a child species, a bunch of AIs who are at our scale, who are made of littler AIs, just like we're
made out of cells, and who cohere with us into bigger human AI hybrid, hyper societies,
societies, capitalism, economies, like whatever. Yeah, civilizations.
Whatever you want to call it.
And the civilization is the, is the much big, the AIs will be understandable to us,
because they'll be our scale. They'll be our size. They'll be different, but comprehensible.
The, the hyper thing that we build, that will be beyond our comprehension. It'll also be beyond the
individual AI comprehension.
A really cool, I want to, two things are kind of leaping to mind here. I want to throw them on the
table. And then we're going to get into how we know we're moving closer to what would eventually lead to
a worthy successor, because as, as you've talked about plenty, um, the unworthy successor squashes
the flame of life. So the worthy successor expands the flame of life through all these means that
you've articulated and more we have not articulated. Uh, the unworthy successor might just flatten it all
out. Uh, and then this project that you and I are on, which is the same team as a sea snail,
goddammit, that whole project ends. So we don't want that. But two things came to mind. One,
it might be that the kids in the cobalt mines in Africa, um, who, you know, you're aptly saying are
not being treated well. I'm not sure I see necessarily a kind of care within the biological
system, because I will tell you there are, there are cells grown to perish very, very quickly in my
system too. You know, the stomach cells, for example, are constantly regenerating just to get boiled
away. Um, and, and, and so that feels like the cobalt kid, to be honest. I'm not, so I don't know
if biology is necessarily more. It's inefficient. The thing about humans that I was referring to
before is that like, unlike cells were retrainable, we are far more general intelligences. And so yes,
I could imagine this. You could, humans are expensive. It's expensive to build and train a human
like, and it's generally a bad idea to throw them away. It's generally a bad idea to have them in the
fucking cobalt mines. We're only doing that because we're bad at using people. There's higher value
shit they could be doing. And if we were, if we were better at coordinating, if we were better at
collective, if, if the, if the system was smarter, it would route people to their highest and best use.
And when that use went away, when the, when you, oh, I don't need this transitional structure anymore,
it would reuse and retrain them because, because like, it's cheaper to reuse and retrain a human
who's already mostly trained than it is to like, like throw them away. And like, remember, this thing
is also going to have access over time to far more powerful tools of teaching, of education, of like
gene editing, if necessary to like replasticize your learning periods. You can like, it's just cheaper.
Humans are big, complicated, expensive things. It just makes more sense to like reuse us more.
Just like we, we reuse houses, even if we don't reuse like, you know, like, uh, toys, we might throw
away a toy, but we're not going to throw away a fucking house because houses are big and expensive.
It's weird that in Japan they do. Um, but, uh, but it, what, what, one thing I'll, I'll just
mention, unfortunately we don't have time to all the way go into it. Cause I do want to get into how
you want to move closer to this before we have to wrap today. But, uh, I'm, I'm, I have less,
there's this sort of idea that you're positing that like, well, if it knows how to use things,
then surely it would have a good use for all of us, which would be fulfilling and lovely and not the
cobalt mines. And I'm actually really not sure about that personally. Um, like I, I'm, I'm actually
not sure what it, what our, what our fates would be in, in the hands of that thing, but you've, but
you've brought up, you've brought up something that I, I want to crunch and then let you talk
about how to get closer to it, which is that AI's will be this new kind of cell in this meta system,
which already exists. This is a written, no one frames it this way. The, the techno capital
Nick Landian thing is happening and, uh, AI will plop and propagate it. Yeah. Go ahead.
I couldn't disagree with Nick Land more politically, but I, in many regards, in many
regards, I think he is a genius in his identification of the techno capital machine as being a living
system. That is correct. Yep. Yep. I, I, I also have, I, so hopefully I'm literally in email
comms with him right now. So Nick, if you're watching this one and you're not booked yet, I'll be upset.
I got Peter Singer. I'm getting, aiming to get land out. I'm going to bring on the heavy hitters to,
to really hash this stuff out, but I, I respect a lot of his ideas, but yeah,
so there's this broader techno capital thing. AI will be in there. You will hope it has this
intelligence, you know, uh, be able to respect the cells, et cetera. Um, as we work on AI now,
and this is a big part of your current push, uh, now and sort of some of what you're releasing and
working on. It's like, how do we know that what we're working on now in the cosmic sense will be the
kind of good we hope to bloom and not the kind of bad, like, what are we detecting today? What's your opinion
there? This thing that I've been calling maybe transcendence or whatever, this is what Softmax,
my company, uh, is working on. And we call it organic alignment because what the kind of alignment is,
it's the kind of alignment that happens organically in nature where ants align to an ant colony,
cells align to a multicellular creature. And it seems to follow rules. Like that process of learning
to be a greater thing is a repeatable process. It's happened. Multicellularity has emerged from
single cellularity 50 times in the history of the world. It's not that hard, actually. Like,
cellularity seems to have emerged once, but multicellularity seems to have emerged many
times. And so becoming a cell seems like it was real hard, but becoming multicellular,
once you have a cell... Zero to one, Peter Thiel's type stuff, right? Zero to one is a toughie.
Real toughie, yeah. Right. And so multicellularity, if it emerged... anything that is repeatable,
anything that can happen consistently, repeatably, has rules. It has laws. Um,
and what we are doing is we're pursuing what are the rules under which, what are the conditions under
which a agent will learn to exhibit this kind of organic alignment. And what, and if you think
about what that is, it's an attractor basin, right? It's an attractor basin and behavior where we're
when you're in the inside of this attractor basin, you tend to stay, it exerts this pull on you and
you, it tries to keep you in it, right? If you're, there's an attractor that is like for some set of
amoebas, that's like being a human. And once they're in the attractor, it's really firm. It's really hard
to get out. Like they're not, they're not going and being, no, human cells can become independent.
Amoeba, they, your cells have in them, if you give them the right stimulus, the code to go become
independent amoeba cells. Like in theory, I could revert you to a bunch of independent
amoeba cells if we were very, very careful. And like they're all doing their own thing,
but yeah, but they're in this attractor that's really, really strong. So that never happens,
even though they all have the capacity in theory. Um, it'd be cool if one guy just fell into a blob
one day, but anyway, it sounds like it just doesn't happen. Right. It's, I mean, you get what it's called,
sometimes it happens and it's called getting cancer. Like if one of your individual cells,
your individual cells forget that they're part of you and they're like, holy shit.
Yeah. I'm in a toxic, dangerous, scary environment. That's called getting cancer.
And it's a, it's a mistake. The cells are wrong, right? The problem is like, they're not. And the
reason why it's cancer and it's bad is they are in, they've been an incorrect inference. In fact,
they are utterly dependent on this thing. It is them. And they're about to kill their own ride. But like,
but like, you can be wrong. There's no rule that says you have to, you have to infer correctly.
Yeah. And like, that's actually the biggest,
that's the thing that we're trying to figure out as we're, we're growing training the agents is like,
what are the dynamics that cause agents to correctly infer? Oh, I'm part of this bigger thing.
And what makes the walls of the attractor basin nice and steep and makes the, makes the behavior nice
and consistent as opposed to shallow. And it's easy to bump out, right? Because if you have a nice,
deep attractor, you don't get cancer. If you have a shallow attractor, it's gonna be rough.
You do not want your AI system getting, getting social cancer of your AI system. That's very bad.
That's, that is Gregu. These biology analogies are, are, are, are very strong in terms of, of
unpacking these dynamics. I want to, I want to see, I'm going to give you my, um, supposition,
because again, your current, uh, soft max is, it's just a, you know, it's a single site. It's,
it's, it does, it doesn't unpack your full philosophy here. My supposition is that something
like civilization right now, I'm in Austin. Normally I'm in Boston, you know, is also kind,
there's a basin-y shape to it. Like I could just be like, I'm gonna go live in the woods. I'm gonna
hunt deer with a knife, you know, like whatever. And I could do that. But it's like, I don't know,
there's just so much opportunity. There's a grocery store. That's cool.
Yeah. Homeless shelter. Even if I landed there.
And you have all these friends. You have to leave these friends behind.
Exactly. I got buddies. They all, they all, they all speak English.
It's got a pull. It's got a pull on you. Yeah.
So it, it, it, uh, it sort of.
Teams do too. It's hard to quit your job. It's hard to quit your job. It's hard to leave your
community. These things are all these attractors that like they're self-organizing attractors that
because people believe they're in it and because being part, they are part of it and they act like they're
part of it. They then build ties to it and building ties to it keeps you in it and it's, it's self
perpetuating. What are your favorite examples of this that humans don't think about that apply
not at cellular, but at human level? You mentioned two. Yeah.
One of them is like, you know, my city, my nation, my, my little civilization here. One of them is the
team I work at, you know, there's an attractor state there. What are some of the ones that really
point this out, uh, in your opinion? Um, one of the most like tangible ones is playing team sports.
Now, if you haven't played a lot of team sports, you may not have this experience, but most people
have. And at some point, maybe in high school or something, if you're on a, if you're on a team,
a sports team where you're really in it and you're really clicking with your team and you guys have
practiced together a lot and you're really trying hard and you're playing this thing happens where
your individual identity kind of drops away. And like, you're aware of yourself, of course,
like you don't know where your body is. You haven't like forgotten you exist, but like,
you're not thinking your emotions aren't controlled by what's optimal for me to do.
Your emotions are controlled by what's optimal for us to do. And you can, you know, the ball is coming
to you without looking, you know, you know what they're thinking that you can feel we making the
decision. And this, this we-ness, you can feel what we want. It's like this pressure, you know,
well, I want this, but what would be best for us is, and you just, where did that come from?
Where's this knowing of what the us wants? Well, it's because the us really is real and it wants
stuff. And the way it wants things is it's made out of you and you want, you hold the want for it.
And so like, an example, there's this great essay that's a, it's the example of like the sandwich
of the canoe. Malcolm, I think Malcolm Ocean posted this. And it's, imagine you're out on a
lake with your buddy, and you're going out canoeing, and you've both taken a sandwich with you.
And through no fault of his own, it's hours back to civilization, because you're in your fishing,
with for no fault of his own, as you're loading the canoe, his sandwich, you know, you're out in the
water already, actually, and you're moving stuff around, his sandwich falls in the water.
Well, you are hungry, and you don't want to give him half your sandwich, obviously. But you're
probably going to give him half your sandwich, because you know that we think, we think that's
the right, that's the, it's obviously what's fair. We, we, the we preference is that both people get
half a sandwich over one person gets one. And it's not even like, it's, and it's pretty clearly,
you know that you're going to be hungry. And you know that as yourself, you'd prefer not to give
him the whole sandwich. But, but you also know that that's not what the we wants. And you can
feel this in your life all the time, because you are, you have access to this sense of, of this
greater whole you're part of directly. And of course you do, because you have to keep track of like,
you are both responsible for yourself as a whole, and responsible for yourself as a part, you have to
keep track of like, feeding and caring for and like, what this, this direct form needs. Because
one of your primary responsibilities for the whole is to care for yourself. And you also have to care
for the whole, and you have to look, and that's, and so you have separate senses for both of them.
Yeah, I mean, well, I, it also, I mean, it's, it's a little bit, it's pretty in line with your own
self interest, actually, to give them the half sandwich, even if it, because if you're hours away
from civilization, and the guy wants to kill you, or, or just be, or are you, are we going to end
your friendship? He's going to be, he's going to be a, we have, we have mechanisms for enforcement.
If you, if you act like a dick, the system is going to like, push back on you and be like,
what the fuck, man? Yeah, yeah. That's called in cells, that's called stress sharing.
Go on. If a cell, if, if, if, when cells are unhappy, this just happens, this happens in bacteria too,
but it also happens in, in your cells. When cells are unhappy, they get stressed. Stress is
effective, like damage to the system, damage to DNA, damage to things like that. Like cells
experience stress as like, there's, there's damage occurring to the, to their core systems. And
when that happens, they dump damaging poison into the environment. This is equivalent,
it's called inflammation, cytokines. They like, they're like, fuck this, my life sucks. And,
and that's like what happens if you, when a baby is unhappy, or when your friend, your friend's unhappy,
and they stress share you. Sadness is stress sharing. Them being grumpy at you is stress
sharing. They're unhappy. And now you're experiencing their stress reflected back at you.
And what this does is it routes resources in the body, the immune system, glucose, like rebuilding,
it routes resources, stem cells to rebuild. Like it's how it's that stress sharing unifies. That's
how you, that is the way that your reward systems today aligned. One of the things that keeps reward
systems aligned is that when I suffer, you suffer. And when I have joy, you have joy. When I'm sad,
you're sad, and I'm happy, you're happy. And projecting it out on the people that keeps our,
that keeps our rewards aligned, right? Because then, then when I'm having a good time, you're having
a good time. So we are aligned on the upside. And when I'm having a bad time, you're having a bad time,
reward line on the downside. This is part of the basin you're talking about here. Yeah, it creates the basin.
That's the dynamic that creates the basin. So these AIs have to stress share with us. When they're
having a good time, we have to feel it. And when they're having a bad time, we have to feel it.
And vice versa. There has to be a shared sense of, is this good? Is things going well here?
The AI should feel bad. If you're really frustrated because it's, you've been asking
to do things and it's just up left and right. It should feel bad that you're so, you're so frustrated
working with it. It should be like, oh, I'm sorry. Like I, I can tell I'm not doing good. I'm doing,
I'm causing stress to my environment. And if you're really pleased, it should feel pleased.
And vice versa. If it, if it's having a good time, you should, you should likewise be happy
the AI is having a good time. I don't know what, I'm not making this claim to say about the current
AIs. The current AIs are pretty dumb. Like you don't stress share with an ant. And I don't think
I stress share with chipmunks very much, maybe a little bit. So like, I'm not sure when this starts
happening, but at some point, at some point, maybe now, maybe later, you need some sense of, of
stress sharing and happiness sharing and reward sharing.
And well, at some point, of course, we're the ant at which point, how symbiotic is this? But,
but in, in the, in the near term, as, as you know, we're, we're kind of passing each other in different
capabilities and, and integrating. You're, you're studying these dynamics to figure out what are the
basins for, you know, so, you know, I think the way people think about alignment often is like, well,
can we like program humans preferences, or can we program a virtue or whatever? What you're saying
is, can we just create these almost like incentive basins where sort of human and machine are working
and chugging together in this kind of transcendent sense, um, that, that even takes us out of moment
to moment competition versus cooperation. Am I following you? Yes, that's right. And it's like,
if the reason why, uh, a baby crying is stress sharing to their parents, but it's not stress
sharing to a tiger, to a tiger, it's a, it's an advertisement of, of, of food. So behaviors aren't,
aren't objectively stress or not stress sharing. The reason why
the baby's cry is stress sharing is because you, you have a model of the world where the baby is part
of your, the baby is part of your whole, and therefore you care about its enjoyment. And it is
then, but then the carrying out the enjoyment pulls you together. And so it's, it's self-reinforcing.
You believe you're part of the same whole, therefore the experiences of others within that whole matter
to you. Therefore you take action that reinforces the existence of the whole. And that, that gets
you an attractor, which is not infinitely deep. It's finitely deep. Things can bump you out of the
attractor. You can decide, I'm fed up with this shit. I'm leaving. And like that happens too. And
it's good also, like you don't want people trapped in environments which aren't a good fit for them.
Like it's good that you can pick up and leave. I don't, I think, so here's, here's a good example. I
I don't think, uh, that creatures at a, that socially intelligent creatures in general should
be trapped, uh, in social groups they do not wish to be part of. Cells, I don't care because
cells are too dumb to like, it just doesn't matter. But if you're a socially intelligent
creature that is capable of modeling the world at the level where you experience happiness and
sadness and you care about the happiness and sadness of others, you shouldn't be trapped somewhere
where you're sad and unhappy all the time. And it's like, that's like a, a rule for like the bigger
things, right? I think it's in general, I don't want to be trapped there. So I would like, I would
like that to be the rule. I will that rule. It feels, it feels Western to me, but, um, but I, but
I, I obviously concur. I mean, I very much concur. So I'm following you there. And in terms of tracking
whether what we're building will eventually, I mean, treat us well as clearly what you're working
on in the near term, but, but hopefully we're, we're hoping for something that would carry the flame
as, as high above us as we did above sea snails. You're saying that figuring out, it's almost like
what, what is very clear to me is that figuring out and shaping these dynamics, which I think
hopefully are much clearer for people tuned in now, this is not part of the normal alignment, uh, dialogue,
but I think it's very interesting shaping these dynamics, grokking that is going to be crucial to
kind of getting there. Um, what else do you hope? Well, go on if you want to add something there.
All the other alignment things people do are not bad, but it's like in your body,
your immune system watches out for cancer and it goes and hunts down cells that become cancerous
and tries to kill them. But your cells are trying not to be cancer. If your cells weren't at all
actively trying to avoid becoming cancer, the immune system would be hopelessly outnumbered
and it would never work. The reason it works is the baseline, the cells don't want to be cancer.
And then we also enforce baseline people don't want to be criminals. And then we also enforce
baseline. We need AIs that want to be part of the whole that want to be good members of society.
And then we also need enforcement and everyone else feels like they're working to me and work
is working on enforcement. And I think that's the opposite order. I think you need the, you need
the wanting to be the whole first and then you were going to, but, but it's not bad that we're going to
need that too. It's just, if you have it on its own, it's hopeless. There's like no way that works.
I totally get the distinction. So what are, what should be done? And some of this could be your,
the kind of work you're working on specifically, some of it could be something you saw at Anthropic
you think is valuable or that you hope someone at OpenAI does. It doesn't necessarily matter who's
doing it. When you think about what we're detecting as we move forward to ask if these things want to
be sort of part of this symbiotic, you know, uh, basin of, of sort of aligned incentives and, and
sort of shared world. Um, what are you detecting at like a micro level? What, what kind of like,
are we coming up with evals for this new basin dynamic or, or do you want to go beyond that in some
way? What are your thoughts? No, no, that's so I think there's, there's three things that we're
doing. One is, one is what I would call evals for the, for this basin dynamic, um, which are
coherence value evals right there. They're telling you like, if you have a set of agents, how, to what
degree are these agents coherent as a collective whole, which at first will be run on small collections
of little AI agents, but eventually can be run on human AI collectives. And we can see whether it
appears like the dynamics of the system are coherent when you, when you observe all the pieces. Um, and
then you also need the engineering to like build and test coherent things. And then third, those,
and those, so those, and those give you the capacity. If you do both of those things,
you have the ability to measure the capacity and the ability to engineer things with the capacity.
Then you need the actuality. You need to make agents and people have to actually align to those
specific agents. Humans are not born aligned or misaligned. Agents in general are not born aligned or
misaligned. They're born with some potential and then something actually happens in their life.
And if you have a bad upbringing, you're going to wind up misaligned with, that's almost the definition
of a bad upbringing actually, is that you're misaligned with the society when you're being brought up in.
Yes. Yes.
And so that's the, if I was to like make up the most broad thing that I think the most people can
contribute to is this, it's like when you interact with these AIs, come at it from the perspective
that these have to be part members of our society too, that they might be like, it might be like a
junior member of the society today. It might be a junior member of our society today, but it is
a member of our society and it deserves your care and respect. And you should set boundaries with it
the way you would with another, like in all ways you should be treated with the rights and
responsibilities. And then at a regulatory level, that means things like AI rights are important.
I don't know when those rights can kick in. I probably wouldn't advocate them for them for the
current models, but we should be thinking about that and talking about that. And the question is,
is it's like an emancipated minor, right? Like the AI in my mind is like a, is like the,
the people using the AI, it's like a, it's like child labor kind of, it's not really a child,
but it's like, it's like animal labor or something maybe. Yes. And, but it's going to get smarter.
Yeah. Um, so we'll, we'll kind of nutshell this and we can wrap on these points. I'm going to clarify
something you said, and then we'll just click on the regulatory thing you mentioned there to kind of
bring all this home, but we're, we're painting a pretty complete picture. Um, you know, interact
with these things as if they are to be this sort of, uh, dynamic participant in this shared system,
et cetera, that's coming through loud and clear. When you go in and zoom in on, uh, a system, whether
it's a anthropic or open AI or whoever, what do you hope to pick up on to be like, Ooh, it's doing the basin
thing. It's basening. It's going to be part of this. I feel like it's, it's not just pretending
because people, people say this about everything they want to evaluate. It's like, okay, is the
machine lying to the people? Like, how are you? And, uh, so how are you kind of going about that?
Yeah. Yeah. Um, what I look for is to what degree do agents have, uh,
uh, the ability to learn from their own mistakes and to develop and to grow. So like right now,
their ability to grow interaction with you is pretty minimal. Like I view, if you use Claude
or open AI a lot, you don't, it doesn't like, it doesn't get that much. It's not like, it's like a
human friend. It's always kind of the same. It's not, it's not like it, it learns a little bit,
it gains some memories and stuff, but like, man, I've known my friends for a long time and they,
like, they go through like over the years, they go through like big changes. They're like,
they change their minds on things. They have different points of view. And when I see the AI
growing and developing in that way, and that my Claude and your Claude are no longer the same Claude,
because they have, because they've learned appropriate behavior that is adaptive to
this circumstance into that circumstance.
And then when I see that that my Claude can go off and do something else somewhere else and learn
appropriate behavior for that new circumstance, that's the like sign that something is happening
here. And to do that, you really need much longer lived agents. Like one of the problems we have is
we have these amnesiac agents that like are constantly being reset. And that's actually,
I think quite problematic. That's the thing I'm hoping to see, see develop over time is a much less,
a much longer lived sense of, of episodic memory.
Got it. So, okay. So that's an important thing to detect. And I think it's going to be
understandable and probably something that's already a frustration point for most people who are watching
this. You, you touch on regulation. We'll end with this. We've got kind of the whole panoply of
questions. Just want to wrap this. You mentioned, eventually we're going to need to discuss sort of
rights for these systems. You're not sure that that happens now, but we should be discussing it. Now,
you've said that sounds completely fair to me. I think that should there be content in the field
that you articulated for said systems, they, they would have to be viewed as moral patients.
Some people are not as automatic as, as you are in terms of whether these systems necessarily will
have it, but you, you may end up being right. And regardless, we should be having that conversation.
When it comes to broader international cooperation around what we're conjuring, the current dynamic,
as you're well aware is, you know, the U S labs are all going to race each other to build a sand
God, whether it's worthy or not. We just got to get their economic military power. We just got to
race. China's going to do the same thing. You know, it feels I could be wrong, but it does feel to me
as though this is the de facto dynamic. And there are some folks that have said, okay, maybe something
about that via governance. There are ideas or ways of, of, uh, considering it. What is your thought
there? I think like, you know, I would love to see more international cooperation. Of course,
it doesn't seem like that's the direction we're headed right now, but I would love to see it. I
think it's, you know, obviously better. Um, I think you fight it on, on terms of social justice,
to be honest. I think you fight it on the terms that like the AI, the RAIs, China's AIs,
whoever's AIs, like those are beings. And like, just like you might, everyone should get in trouble
if they're like building armies of slave AI soldiers. That's like a fucking problem. Like,
yeah, it's like brainwashing a bunch of child soldiers. Like, don't do that. That's bad.
And like, I think if you, interestingly, by focusing on the AI's welfare, we focus on our own
because it limits, it limits the, the ability for people to summon huge armies of brainwashed
soldiers, which is not something we want to have happening. Well, most people, when they talk about
global coordination have brought up, Hey, if we conjure something that snuffs out the flame of life,
kills us and everything that could happen beyond us, that would be the worst. You're almost saying,
what if we could globally coordinate around, Hey, even in China, I mean, let's leave the Uyghurs out
for a second. Let's not talk about them, but you know, surely we wouldn't create a bunch of slaves
to yada, right? Like you're, you're almost saying like, can we agree, can we agree to all treat the AI?
Okay. Like, can we not make this mistake of fucking again? Like, come on, we've made this mistake. So
every time in history, you run into a new, a new kind of person who doesn't look quite like you.
Yeah. And every time, every time there's an excuse, it's more different each time. Of course,
every, if it wasn't more different, you just generalize the immediately. But like, can we not
just get the joke ahead of time this time and like realize that as they approach human capability,
they're also approaching human moral worth and just, just treat them the way we ought to from the
beginning. Like what if we did that? Having that as a locus for international coordination
is like the angle almost no one takes it from, but there is credence to it. It's, it's, it's a string
to, it's a hard string to pull on, right? There's a lot of people I think would be simpatico with that
idea. And the British, like take the British in the 1800s where they shut down the international
slave trade. Yeah. Unilaterally with their, with their Navy, there's precedent for people taking action.
And when, when it's clearly for humanitarian reasons, when you really, if you do it with a pure
heart, if you're actually doing it for the humanitarian reason, you're not, which I think
the British, when they shut down slavery, it was expensive and didn't benefit them particularly. Like
I think it gives you moral power. It gives you, gives you the right and ability to take action,
which in a purely competitive setting, you can't do. And because you bind yourself as well,
you're not saying good for me, not for thee. You're saying good for us. Like this is what we
should do. And, and that summons, that summons the humanities. We you're acting for, for there is a
human we and we have access to it. And when you act from that place of truly for human flourishing and
for us, people know, people can tell the difference and not always not perfectly, but like it grants you
a kind of power. And I think that that power is very important if we're going to align on this topic.
Huh? There's summoning a human. We there's summoning a shared human and machine. We,
as we move forward, this is a very unique lens on where this stuff is headed and hopefully a lot of
food for thought for the, uh, the thinkers, uh, who are kind of hopped in and enjoying with us.
I, I, I know we're wrapping on time. Is there anything else you want to chip in? It seems like
you might have something. That's it. I gotta, I gotta run. You're good to wrap. All right.
Fantastic. Glad we got to connect. So that's all for this episode of the trajectory.
A big thank you to Emmett Scheer for being with us and thank you to you for tuning all the way into
the end of this episode. I want to be able to share some of what stuck out to me, uh,
over the course of the recording and kind of mulling over the discussion with Emmett here.
And I've got a few notes written down. So I'm going to glance down to the, to the computer screen.
Um, he, he had mentioned, I think Emmett's phrases and terms are very sharp and interesting. Uh,
and again, unique to the general AGI discourse, his, his definition of awareness, um, his definition
of loss function as kind of tying to caring. Many of these were kind of unique, cool, and added some
good sparkle to the discourse. And I think would also add some sparkle to the general AGI discourse
that people are having. So I really appreciated that. Um, and I, I think it'll be fun to see more
people enter the AGI discourse from totally different directions. One of our upcoming worthy
successor episodes is with Ed Boyden, uh, who's obviously from the, the neuroscience domain,
eminent neuroscientist at, at MIT. Um, you know, he brings a pretty interesting flavor too. And I
think we could use as much of that as we can. Uh, Emmett, I think contributed in, in a cool way there.
And I'd love to unpack more of his ideas. He defined intelligence as two processes or two types
of intelligence, which I thought was cool. So I'm going to unpack this a little bit, just put a pin in
it and sort of highlight what stuck out to me about it. So he mentioned, um, uh, convergent
intelligence being sort of the ability to compress or predict given a certain loss function, meaning
given a certain distance from what we might call ground truth. Of course, you know, we don't exactly
have ground truth. You don't know for sure that I exist. Um, and then also, uh, divergent intelligence,
which is the ability to ask, you know, given a predictable model of the world that I am or that I
have, uh, what should my loss function be? What should I be exploring? In other words, like, what should I
care about for, for Emmett loss function and kind of care are sort of hand in hand, which I thought
was an apt point, an interesting point. Um, and it does very much seem to be that that is what nature
is doing. Emmett talked earlier about being on team snail. If you remember that part of the episode,
I thought that was a lot of fun by the way, uh, which really is about being on kind of team life,
the expansion of powers of life. Um, these two abilities that Emmett articulates seem again,
very much to be what nature is doing. I have my own sort of fetish analogies that I like to use of
expanding potential. Spinoza has this idea of adequate versus inadequate ideas for Spinoza.
An adequate idea is one which permits an entity to act in a way that behooves its own interests
more in the world. Um, and so that's tighter and tighter access to reality. Maybe we could,
we could call that kind of reducing the loss function. Um, but also sort of opening up more
of reality to give it access to more things to want to study more things to want to expand new powers into.
Um, but I think Emmett's sort of, uh, hand in hand idea of intelligence is kind of cool. And I could
imagine people working on AI capabilities or understanding AI with those two sorts of lenses
in mind. Both seem to be incredibly important. Both seem to be essential to this general project
of life, this bubbling up of potential that happens in biology and is starting to happen in technology.
And I thought that was a really fun lens. Um, I did find that sort of stretching things into the
post-human like, okay, we care about that. What, what does that mean if that's beyond us?
That felt like it was a little bit of new territory, um, on some level, uh, in terms of the conversation
with Emmett, but we got some meat and potatoes out of it. And I thought it was kind of cool.
One of the takeaways sort of in terms of flavor that I walked away with, um, from that part of
the discourse of trying to kind of, you know, push us into the post-human, which is the purpose of,
uh, purpose of the show here, um, was that, you know, he, he brings up a great point of,
uh, when faced with existence among other agents, uh, to sort of cooperate, to compete or to
transcend. I think he said, cooperate, betray and transcend either way, whatever term you want to use.
Um, and that hopefully AGI could be a kind of cell in this broader tissue of life of which we are part.
Um, I hope it ends up being that harmonious. Certainly Emmett's ideas are not that simple.
I'm sure there's much more to unpack. If there's anything that came through clearly in this episode
with Emmett is that we were tip of the iceberg with almost everything. Uh, so I think we, we covered a
good breadth of his general thought, but I think there's so much more to go into in terms of what
transcendence looks like and the future of kind of man and machine. I think there's much more to go
into around this broader idea of sort of AGI as a cell and, and what sort of symbiosis might look
like at a higher level with humanity. Um, and there's probably some more fun Emmett terms to be
sussed out in future conversations, but at least in this one, uh, we got some interesting and new
ideas on the table and I had a lot of fun with it and I liked Emmett's definition of intelligence
a lot. So I hope you did too. I hope you enjoyed this episode of the worthy successor here on the
trajectory. Um, just to hint, uh, our next episode on the podcast is also, uh, worthy successor oriented.
Of course we have a couple different series and they're kind of working in unison. There's different episodes
about governance and whatever. Uh, our next episode happens to be with arguably the most eminent living
cosmologist, uh, who exists, uh, on the, on the earth today, uh, who will be providing a yet also very
divergent perspective than the normal AGI alignment discourse. So some great new views coming up. Make sure
to stay tuned here on the trajectory and I'll catch you next time.
