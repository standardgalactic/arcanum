Welcome to Doom Debates. I'm Liron Jabirah.
I hope you've been having a good last few weeks.
I hope you've been going to some cool parties, spending time with your friends and family,
maybe doing some cool outdoor sports or enjoying nature.
What I've been doing is just scrolling Twitter,
specifically the part of Twitter that's talking about HEI and existential risk
and policy around that and the next breakthroughs and how soon they're coming.
And today I invite you to experience my world vicariously.
Join me on a journey into the world of Twitter beefs.
The first beef is with Jack Clark, who's one of the co-founders of Anthropic.
Jack was recently tweeting about how there's this new harder benchmark for math
that's extremely hard for humans, but it's actually still hard for AIs too.
It's one of the only current benchmarks that AIs aren't easily passing.
So here's Jack Clark's tweet. He says,
AI skeptics say,
LLMs are copy-paste engines incapable of original thought, basically worthless.
Professionals who track AI progress say,
We've worked with 60 mathematicians to build a hard test that modern systems get 2% on.
Hope this benchmark lasts more than a couple years.
And then he's got a quote from the famous mathematician Terence Tao,
the 2006 Fields medalist.
Terence is saying,
These are extremely challenging.
I think they will resist AI for several years at least.
And then Jack Clark writes,
So that's Jack Clark's tweet.
And then the person who comes in with the beef is actually the official account of
Paws AI US.
Paws AI US comes in with a quote tweet saying,
Oh, it's tragic how people don't understand the power of your god machines
because their political advocacy against the god machines is less effective?
Why don't you stop making god machines?
Jack Clark is a likable guy, very much in the style of Dario and most of the rest of the
Anthropic team.
These are thoughtful people, normally nuanced, normally mature people.
But at the end of the day, this whole project called Anthropic is a pipe dream.
It's neglectful of the very idea that building a safe AI sometime in the next decade or two,
when we have virtually zero theory about how that would even work in terms of alignment,
that whole idea is a non-starter.
And that whole company, Anthropic, and just like the other AI companies,
is a pipe dream that's neglecting the possibility that safe AI is impossible.
And they're just like,
Look, we'll do our best because there's no other choice but to do our best.
And so the thought that they're not allowed to think is,
Oh, we're just burning down the last few years that we have before superintelligent AI is here.
And instead of tapping the brakes, we're just trying to make it safe as best we can.
And if it works, it works.
If it doesn't, it doesn't.
There's zero effort to dignify the pause AI effort from these AI labs.
So you've got the pause AI US account kind of calling them out,
just being like,
Hey, you know what?
You are the problem.
You're actually not the good guys, right?
You're part of the problem.
And you're doing something that you can't reverse.
And you're legitimizing not just yourselves, but the other AI labs, right?
Open AI, you're not yelling that they should stop, right?
At the end of the day, you may be mature.
You may think of yourself as the good guys who are safety conscious,
but you're absolutely part of the problem.
And in fact, pause AI, their next protest is coming up on November 22nd.
And everybody's invited.
They just tweeted out,
Anthropic wants you to think they are the good guys.
But they are at the head of the pack making dangerous artificial intelligence.
Developing more and more powerful AI without safety regulations in place is not safe.
Period.
Join us in protest.
And then there's a Facebook link to sign up for their protest.
It's happening next Friday afternoon, November 22nd, at the Anthropic office in San Francisco.
Everybody's welcome.
I personally am planning to go.
And I just donated a thousand bucks to pause AI.
I think it's one of the best organizations that you can be supporting right now.
The U.S. branch of pause AI is run by my friend, Holly Elmore.
And I think she's been doing an amazing job moving the Overton window,
the window of what you're even allowed to say and talk about,
to publicly be making these kind of statements like,
hey, Anthropic, sorry, but you're part of the problem, right?
You guys should not be legally allowed to do what you're doing.
You're kind of like criminals before the law catches up, in our opinion, right?
If we summon the democratic power to make that the law before it's too late,
well, then their current project shouldn't be illegal, right?
Like, that's our position.
And Holly's been doing a great job letting rationalists talk about it.
Because for so long, there was so much trepidation in the rationalist community
from which you got Eliezer Yudkowsky, or rather, which maybe formed around Eliezer Yudkowsky, right?
That whole community has just always been in this very deliberative, thoughtful, intellectual mindset.
And it's just the farthest thing from their nature to, like, run out on the street and yell,
stop building this AI before you can make it safe.
And it's this distinction that a lot of people refer to as a scout versus soldier.
I think a coined term by Julia Galef in her book, a recent book, The Scout Mindset.
Everybody wants to just be a scout and just look out at the territory and just analyze what's true.
Nobody in the rationalist community wants to go out and be a soldier and just be like,
hey, you guys are wrong.
You need to stop.
They feel more comfortable writing a paper being like, probability of doom seems high
for this and this reason, but also there's this and this mitigating factor and kind of
lobbing the paper over the wall, hoping somebody else will take it and run and do the next step
of affecting policy, affecting social change.
Like, they feel like their work is done.
That's my model of the average rationalist.
I'm a pretty big rationalist myself, but I guess I'm just more comfortable being like,
hey, this seems right and the time to act needs to be now and I don't mind helping out the
soldier part of the movement.
And, you know, soldier is such an intense word.
It's literally just like engaging in a protest, right?
I mean, normies protest all the time without getting character assassinated as like hardcore
soldiers, right?
I mean, protesting in the normie world is just considered something you can do to defend
your beliefs, right?
And not enough rationalists are doing it, but look, doom debates, we're not exactly a rationalist
channel here, right?
I'm hoping to reach a wide segment of the population with these kinds of videos, just
being like, hey, this thing is worth protesting, right?
This needs to make it out of the rational sphere, out of the people who just like thinking
many decades into the future and be like, this is actually something that the AI labs have
gotten way ahead of their skis and like normal people just need to yell at them.
And then we need to yell at our representatives in Congress.
Like we need to just act, right?
It's time to stop thinking and start acting because Lord knows the AI companies are acting
as fast as they can with the best talent that they can and the most money imaginable, right?
We're talking over a hundred billion dollars, at least if you look at their combined valuations.
So it's a huge force arrayed against us.
I want to read you an excerpt from a recent blog post by the head of PAUSEAI US, you know,
Holly Elmore.
Post is called, Scouts Need Soldiers for Their Work to Be Worth Anything.
Since I started PAUSEAI, I've encountered a wall of paranoid fear from EAs and rationalists
that the slightest amount of wrongthink or willingness to use persuasive speech as an intervention
will taint the person's mind for life with self-deception.
That politics will kill their mind.
I saw people shake in fear to join a protest of an industry they thought would destroy the
world if unchecked because they didn't want to be photographed next to an un-nuanced sign.
They were afraid of sinning by saying something wrong.
They were afraid of sinning by even trying to talk persuasively.
The worry about destroying one's objectivity was often phrased to me as being a scout, not
being a soldier, referring to Julia Galef's book Scout Mindset.
I think we have all the info we need to contradict the fear of not being a scout in her metaphor.
Jumping ahead, here's another excerpt.
If a scout reports what they found to a superior officer and the officer wants to pretend that
they didn't hear it, a good scout doesn't just stay curious about the situation or note
that the superior officer has chosen a narrative.
They fight to be heard because the truth of what they saw matters to the war effort.
The success of the scout and the officer and the soldier is all ultimately measured in the
outcome of the war.
One more excerpt.
This is very scary for a contingent of effective altruists and rationalists today who treat
thinking and talking as sacred activities that must follow the rules of science or less wrong
and not be used for anything else.
I think these concerns are wildly overblown.
What are the chances that amplifying the message of an org you trust in a way the public will
understand undermines your ability to think critically?
That's just contamination thinking.
So yeah, I highly recommend reading the whole post.
I'll put it up in the show notes.
There's one more thing you need to know about Paws AI US, which is they're fundraising right
now.
They're still a small organization.
They're making a big impact, in my opinion.
Even when just 30 or 40 people show up to a local protest at one of these AI labs, we get
a disproportionate amount of attention, a disproportionate amount of coverage, because everybody's
wondering who's pushing back against these companies.
It's such a David and Goliath story right now.
Who is going to step up and be David?
Unfortunately, very few people are stepping up and being David, even while experts are
predicting, yeah, AGI is three years away, maybe five or 10 years away, maybe 15 years
away.
Like, it's time for David to actually get some power here.
So if you want to be part of that cause, I'll put up a link in the show notes to their donation
page.
Right now, you can read more info about them.
You can write a comment, and Holly Elmore, I'm sure, will respond.
They've got details about what they're spending the money on.
Looks like the core team is Holly and two other people right now.
So yeah, definitely take a look.
If you're wondering about me and Doom Debates, this is just an independent project on my part,
not officially affiliated with Paws AI.
But if you go to PawsAI.info and click on the link to join the Paws AI Discord, not only
will you be joining a great community, but there's actually a channel that they made for
me.
It's called hashtag Doom-Debates, and I'm there because even though this isn't a Paws AI
initiative, I'm a member of Paws AI, and I go to the protests all the time, and I'd love
to meet you and hang out, whether we just do it over the Discord channel, or we could even
potentially interact in Meatspace if you come to these protests.
So that's Paws AI US.
Click the link in the show notes if you want to donate.
Let's move on to the next beef.
Here's a fun Twitter beef from a few months ago.
This one's between Eliezer Yudkowsky, and you'll see.
It starts when Eliezer tweets, I think he's subtweeting some interactions that he's had,
and he tweets,
the kind of guy who has at some point learned linear algebra, who is very proud of knowing
linear algebra, and who presumes that all your disagreements with him about AI must be because
you don't know linear algebra.
Telling him that you know linear algebra won't help.
A similar kind of guy has learned about self-attention or something.
Mostly he is identical to the first guy, except even prouder.
However, you cannot possibly know how KQV matrices work.
If you say you know you're lying, if you knew, how could you go on disagreeing with him about AI?
Perhaps he works in the AI industry.
Now he has personally learned by hard sweat that AIs are stupider than you think,
or that they are smarter than you think.
He has earned this knowledge by work.
It cannot be systematized and supported by observation.
You can't have it.
Sometimes he has even learned that there is math inside AIs.
Did you know that?
You can't know that.
You wouldn't be scared of AIs if you knew that.
He knows how multiplication works.
It's just numbers.
It is not scary.
If this sounds like a straw man to you,
you remember in the first episode of Twitter Beeps,
I looked at Amjad Massad, who literally tweeted something like,
Tell me how multiplication of numbers could possibly kill me in the real world.
Or Mark Andreessen has actually tweeted something as explicit as,
AI is just math.
It can't hurt you.
It's just math.
This was quite a low-level failure mode,
and yet, sadly, it's not uncommon at all,
even among smart people who you'd think would know better.
So this is just Eliezer venting about an experience that rings true to me.
Now, a guy named Guillaume Verdun,
who on Twitter is known as Based Beth Jaisos,
replies to Eliezer.
Maybe he was even one of the guys that Eliezer was subtweeting.
Regardless, he replies to the thread.
He says,
Actually, you also need a background in functional analysis,
optimization theory,
probability theory,
complexity theory,
and thermodynamics to make coherent,
full-stack arguments about the future of AI.
Eliezer replied,
What's an example of a truth about the future of AI that requires functional analysis to understand?
Also, you left out evolutionary biology whose relevance is indirect but pretty important,
and which I discussed in the simple math of evolution sequence,
so nobody has to take my word on it.
And then Eliezer also added in a quote tweet,
Whenever I say anything remotely like this,
I lay out the arguments,
and I do my best to explain to the reader the basic math I think relevant.
That is one of the differences between intellectual leadership and intellectual showmanship and bullying.
It's incredible that your followers are crowing about such blatantly bad conduct.
I do agree with Eliezer on that.
I mean, Beth basically just listed a bunch of names of fields
instead of engaging with the argument,
and then declined to engage when asked.
Pseudo-intellectual tweets are his specialty.
Here's one of my favorites.
It's from a debate that Beth had with Cutter Lay.
And I just want you to savor how the wheels are turning in Beth's mind,
asking himself,
How do I just inject the most buzzwords
without actually needing to touch on this stuff to make my point?
But how do I make myself sound as far as possible?
Let's listen.
And by the way, the first guy is Conor Leahy,
and the second guy is Beth.
A lot of what civilization is lacking is coherency.
It's that we constantly are shooting ourselves in the foot for literally nothing.
Interesting point.
I think we have different models of how much mutual information we can have about the future.
Obviously, we know that physics,
Newtonian physics,
time translation and variance,
so we've all ways to navigate in Newtonian physical world.
But I think in the cases where dynamics,
we have very low mutual information about the future,
actually randomness can be your friend to search and find new optima.
But again, I think that this tracks in my model of your model of the world
where if you have high predictive power,
you have high mutual information,
coherence in quantum mechanics is some form of mutual information with the future,
then you can have stronger methods of optimal control.
Let me also give an example where coherence is also relevant to search.
A lot of what civilization is like,
So that was polite of Connor to kind of let the pseudo-intellectual rant go
and move on to making his actual point.
If you're wondering what Beth was talking about in the clip I just played with him and Connor Leahy,
he's just trying to make a point that maybe the future is unpredictable.
And he's busting out this detail about how Newtonian mechanics has time-invariant physical laws,
which is such a random detail if you're just trying to explain why a physical system is predictable.
It's predictable because it's non-chaotic, right?
It has to meet a lot of constraints to be predictable.
The fact that physics are time-invariant is the kind of random detail that you only bust out
when you're just intentionally trying to use buzzwords to sound smart
and try to pass it off onto the radar as if you actually are making coherent points
that a smart person would make.
Somebody's described Beth as a reverse Richard Feynman,
where instead of trying to take points and simplify them
and give you the simplest version that still helps you understand the point,
he just does the opposite where he takes something that hopefully there's some point,
not in every case, but let's say that there's some coherent point,
and then he tries to expand it out into saying,
hey, how many associations can I make?
How many different buzzwords can I draw on to make you think that my brain has gone off
and done a bunch of hard-earned deep connections?
Instead of optimizing for how do I just present to you the point
in the simplest, crispest way that actually gets the job done of having you understand the point.
Now, funny enough, since we're talking about Richard Feynman,
there's another gentleman who responded to the thread with Beth Jezos and Eliezer.
Remember, Beth was originally talking to Eliezer about all of these different fields
that you supposedly need to be an expert in in order to understand AI risk.
So, funny enough, Richard Feynman's son, Carl, who actually follows AI Twitter,
he quote-tweeted the part where Beth was saying,
you need a background in functional analysis, optimization theory, probability theory,
complexity theory, thermodynamics, and Carl Feynman wrote,
for people who wonder if this is true, I have a background in all of these things,
and I can assure you, this is nonsense.
Unchecked superhuman AI is possible and extremely dangerous,
and the arguments for that don't use any of those tools.
So, yeah, I mean, Carl Feynman and Eliezer Yudkowsky are obviously correct,
and everything that they've said in this Twitter beef,
there's not really much of a both sides to it.
It's just, look, I'm here cataloging these beefs, right?
As long as the state of discourse is so low,
or somebody's so obviously wrong,
and yet still getting tons of likes, tons of retweets,
I'll just be here calling it out, right?
Maybe the discourse quality will one day improve,
and I'll only tell you about Twitter beefs that are, like,
so high-level between one smart good-faith person
and another smart good-faith person,
but for now, we have pseudo-intellectual Beth Chazos beefs.
It is what it is.
The next Twitter beef is with Professor Jeffrey Hinton.
You may know him as 2018 Turing Award co-winner,
together with Yoshua Bengio and Jan LeCun.
Well, he's not just that anymore.
He's also the 2024 Nobel Prize winner in physics.
You may have heard about a month ago,
he's now a Nobel Prize winner,
so I like to use his name as often as I can,
because it lends massive credibility to the AI safety effort
to bring up a Nobel Prize winner
who warns about AI doom regularly,
and, of course, he's also one of the godfathers of the field.
I've heard Dr. Hinton say on various interviews
that he's happy to help raise awareness about AI risk.
He's actually looking for ways to personally contribute.
So I reached out to him on Twitter publicly.
I wrote,
Well, the man himself never responded to that,
but I was happy to see that I got 126 likes,
which is kind of a lot for a random tweet like that.
So if you were one of the people who liked it,
thanks very much.
Thanks for supporting the show.
Thanks for recognizing the value
of bringing on these luminaries
and just explicitly asking them,
you know, what's your P do?
What do you think people can do to help?
You know, get the message out there.
If you're watching this
and you have a connection to Jeffrey Hinton,
try telling him to check out Doom Debates.
Maybe he'll like the content
and maybe he will agree to come on.
You know, he's really busy.
I'm sure he didn't see my tweet,
but I do think that Jeff and I
could have a very productive discussion.
If you check out the Doom Debates archives
and you watch how I interview other guests
like Dr. Lee Cronin, Dr. Robin Hanson,
you can see these are high-quality interviews.
So Dr. Hinton, if you're listening to this,
I think you are aligned with the mission of this show,
which is to raise public awareness
about the risk of imminent human extinction
from artificial general intelligence.
I think we're on the same page
that this is a big risk.
And if you want to propel the show's growth
into the next tier
by gracing us with your presence,
please hit me up anytime
on Twitter, DM, or email, wiseguygmail.com.
Thanks, Dr. Hinton.
Okay, back to the Twitter beef.
This has to do with what Dr. Hinton was saying
right after he won the Nobel Prize.
He gave a press conference
at the University of Toronto
where he's a professor emeritus.
And he said something funny
about his former student,
Ilya Satzkever,
and what Ilya did last year to Sam Almond.
So OpenAI was set up
with a big emphasis on safety.
Its primary objective
was to develop artificial general intelligence
and ensure that it was safe.
One of my former students,
Ilya Satzkever,
was the chief scientist.
And over time,
it turned out that Sam Almond
was much less concerned with safety
than with profits.
And I think that's unfortunate.
I'm particularly proud of the fact
that one of my students
fired Sam Almond.
And I think I'd better leave it there
and leave it for questions.
To give you more context
on Professor Hinton,
you know,
over the last couple of years,
he's come out as a full doomer,
kind of tracing in the footsteps
of Eliezer Yudkowsky.
A couple of years ago,
he quit his highly lucrative job
at Google,
and he explicitly said
he wants the freedom
to speak out
about major concerns
that he sees.
And he's even been talking about
kind of regretting his research
or not seeing it coming,
how rapidly we'd be facing
these potentially existential threats.
He's kind of getting
into the doomer mode.
So I've tweeted
from my own account
a couple choice quotes
of Dr. Hinton
over the last few months
or the last couple of years.
Here's a recent clip
where Jeffrey Hinton says
his independent assessment
of P. Doom
is more than 50%.
And then he humbly adjusts it
for other people's assessments
as if other people
would know better
than Jeffrey Hinton.
So he adjusts it downward
because of other people
in his life,
and he arrives at
10 to 20% P. Doom.
I think it's cool
that the godfather of AI
keeps getting more explicit
that he sees how effed we are.
You know,
so many other people don't,
but he sees it.
So good on you, Dr. Hinton.
Here's that clip.
You want my P. Doom?
I think it's a sub-question.
You might expect Doom
from other sources.
I'm talking more about
something like P. misalignment.
Pretty high.
I think RLHF is a pile of crap.
I actually think the risk
is more than 50%
of the existential threat.
But I don't say that
because there's other people
who think it's less.
And I think a sort of
plausible thing
that takes into account
the opinions of everybody
I know
is sort of 10 to 20%.
You know,
we've got a good chance
of surviving it,
but we'd better think
very hard about
how to do that.
That's alarming.
Here's another recent clip
of Dr. Hinton
talking about AI existential risk,
and he specifically names
OpenAI and Meta.
I think the profit motive,
we saw what happened
in OpenAI
when everything was stacked
in favor of safety
because the company
that owned the for-profit bit
was a not-for-profit bit
which was set up
to be concerned primarily
with safety.
And even in that circumstance
where it was a very
unlevel playing field,
it was tilted all the way
you could tilt it
towards safety
and profit one.
So that doesn't seem good.
And while I'm at it,
there's one other thing
I want to mention,
which is that I think
open sourcing
the biggest models
is completely crazy.
My good friend,
Jan LeCun,
thinks it's the right thing
to do.
He thinks we're all
going to be fine.
We'll keep in control
of these things.
They won't have any goals
of their own.
They won't have any
desires of their own.
I think it's very,
very dangerous
to open source them.
I think it's like
open sourcing
nuclear weapons.
So I think
I would love governments
to forbid companies
to open source
big models.
Strong language
from one of our
top AI scientists,
right?
The godfather of the field.
A Nobel Prize winner.
So don't let anybody
gaslight you
as if
this is a niche opinion
that we're all
going to be doomed
or it's a niche opinion
that we need
harsh government regulation.
No, I'm sorry.
This is what the adults
in the room are saying.
And everybody else
is really enjoying life
trying to ignore
this message, right?
Like the razor blades
are, the whirling razor blades
are right in front
of their face
and they just refuse
to see it.
And they're seeing
how long they can go
without seeing it.
Here's a tweet
from the man himself.
October 31st, 2023.
Jeffrey Hinton writes,
Andrew Ng is claiming
that the idea
that AI could make us extinct
is a big tech conspiracy.
A data point
that does not fit
this conspiracy theory
is that
I left Google
so that I could speak freely
about the existential threat.
And then I replied,
hashtag believe Hinton.
And I put in a photo
from a protest we did,
a pause AI protest
from last year
in San Francisco.
One of the signs
was a big,
hashtag believe Hinton.
You gotta believe Hinton, people.
Okay, my next Twitter beef
is with Samuel Hammond.
I've been following him
on Twitter
and he tends to post
thoughtful, nuanced analysis.
So that's cool.
He's not enough
of an AI doomer
for my taste,
but I still respect him.
He recently tweeted
a screenshot
from a big post he did
called
95 Theses on AI.
So he kind of
brain dumped
a bunch of different thoughts
he had on AI
and he quoted
one of his own sections,
the section on
AI progress is accelerating,
not plateauing.
And he made some
of his own predictions.
I'll just read you
three of them
that I think are relevant
to this little beef.
Number one,
the last 12 months
of AI progress
were the slowest
they'll be
for the foreseeable future.
Okay, plausible.
Number two,
scaling LLM
still has a long way
to go,
but will not result
in superintelligence
on its own
as minimizing
cross-entropy loss
over human-generated data
converges
to human-level intelligence,
like not superintelligence.
Hmm.
Not sure about that,
but okay.
And then number three,
exceeding human-level reasoning
will require training methods
beyond next token prediction,
such as reinforcement learning
and self-play.
That, once working,
will reap immediate benefits
from scale.
Okay, my beef is just
with his point number two
where he says,
scaling LLMs
won't result in superintelligence
because their training process
is just training them
to predict tokens
that humans wrote,
so you can't get
something smarter than human
just by trying to predict
tokens that humans wrote.
I don't think you can make
that logical inference.
I think you're jumping too far.
Like, I don't want to go
full logic cop
because it's a reasonable guess,
it's a reasonable intuition,
but it's not a logically
sound connection
that he's making here.
So my reply tweet to him
is I said,
I don't see how one can claim
point number two.
Humans use kludgy analogies
to attack hard,
evolutionarily unfamiliar domains
like modern math
and engineering.
AI that learns to predict
the results of such efforts
will, in my opinion,
probably do it
by understanding
the underlying domains properly.
In other words,
if the AI is reading over
some physics textbook
or some intelligent essay
on a complicated topic
and the AI is just learning
to predict
what this particular
smart human wrote,
the AI might train
an understanding of itself
that's not just an understanding
of the human
who wrote this treatise.
It might be an understanding
of the underlying domain.
It might be a deep understanding
of physics.
And if the AI can understand
physics so deeply
that it knows how to predict
what a human physicist
would write in a textbook,
that doesn't mean
that the AI's intelligence
is limited to the human's intelligence.
It might just mean
that the AI understands physics
really, really well.
If the AI wants to predict
what the next textbook author
is going to write
in a physics textbook,
the AI might just draw
on its insight about physics.
Be like, oh yeah,
physics works like this.
So if a chapter is called this,
it might explain
this kind of physics.
Now you might argue,
okay, well the AI
is not going to invent new physics,
it's just going to draw
on the physics that it knows.
Maybe you don't know that
because what if
the next chapter heading
is reconciling
Einstein's theory of relativity
with quantum theory,
something which is kind of
an open problem still
in today's physics.
But the AI sees
that chapter heading
and it'll just be like,
oh, okay, yeah,
humans must have figured that out.
Here's the answer.
Right?
You don't know
that it doesn't know the answer.
You're making an assumption
that because it hasn't
seen the answer already,
it's not going to
regurgitate the answer,
but that's a separate assumption.
Being trained on human tokens
doesn't mean that
you don't get an understanding
that doesn't let you
generalize outside
the tokens you've seen.
If you see a bunch of humans
writing about physics,
you might generalize,
okay, yeah, physics.
I know physics now.
And if the next human
writes a book
that you haven't seen yet,
you might be able to predict
what's already in that book.
If you look at
Albert Einstein, the human,
he only got trained
on physics written
by other people, right?
He didn't get trained
on the theory of relativity,
and yet he came up
with the theory of relativity, right?
So he made some kind of leap.
You can't say that
just because somebody's
only seen other types of text,
then they're not going
to go make their own leaps, right?
Sam is kind of sneaking
in an assertion here
that AI is not going
to leap outside the data
that it's already seen.
But we know that
it makes small leaps, right?
Like we know that you can
give it a title of an essay
that's never been written before.
I'd be like,
hey, pretend a human
wrote this essay
and pretend the human
is very intelligent
and has good analyses.
What might the human write
in such an essay?
And AI will spit out an essay
that's like a perfectly
plausible essay.
So if the next essay
is like a new theory
of physics,
who's to say
that the AI can't do that, right?
I mean, maybe in practice
it can't today, right?
It seems like today
we don't have AIs
running away doing this stuff.
I grant that.
But who's to say
that it won't do that
in the next generation?
Who is actually drawing
a clear separation?
You don't get to sneak in
a clear separation
of what the AI
can and can't do
just by observing
that it only got trained
on tokens
outputted by dumber humans.
So I'm not saying
Sam's wrong.
I'm just saying
Sam hasn't really
justified his claim
and I don't think
that either of us know.
I think you just have
to be humble
and wait and see.
Or better yet,
shut it down.
Don't wait and see.
So that's the end
of that beef.
I never got a reply,
but it's no big deal.
We can move on
to the next beef.
The next beef
is with Jan LeCun
on the topic of
predicting the timeline
to AGI.
So recently Jan tweeted,
I said that reaching
human-level AI
will take several years
if not a decade.
Sam Altman says
several thousand days,
which is at least
two thousand days
or six years,
or perhaps three thousand
days or nine years.
So we're not in disagreement.
Basically they're both saying,
hey, it'll take several years
if not a decade.
And then Jan continues,
but I think the distribution
has a long tail.
It could take much longer
than that.
In AI,
it almost always takes longer.
In any case,
it's not going to be
in the next year or two.
Now, Eliezer feels like
he has to weigh in
on this claim
that AGI is definitely
not going to happen
in the next year or two
because,
as I often say on this show,
I don't think any of us
really know
what's going to happen
when we scale up LLMs.
I think we really need
to be humble
about how little we know
and how often
we're already getting surprised.
So when we see
Jan LeCun confidently saying,
it's not going to happen
in the next year or two,
if you watch
my first episode
of Twitter Beefs
when it was called
Doomtiffs number one,
if you watch that episode,
there's a funny clip
of Jan LeCun
predicting that
LLMs won't be able
to answer simple questions
about physics.
I don't think we can
train a machine
to be intelligent
purely from text.
So for example,
I take an object,
I put it on the table
and I push the table.
It's completely obvious
to you that the object
will be pushed
with the table, right?
Because it's sitting on it.
There's no text
in the world,
I believe,
that explains this.
And so,
if you train a machine
as powerful as it could be,
you know,
your GPT 5000
or whatever it is,
it's never going
to learn about this.
That information
is just not present
in any tent.
Right,
something that got
disproved very quickly
after he made
that prediction.
So Jan's at it again,
making a confident prediction
about what's totally
not going to happen
in the next year.
And Eliezer comes
into the thread
and he calls him out.
He says,
what do you expect
to have already seen
two years before
human-level AI
is possible?
To put it another way,
what algorithm
are you following
such that actually
two years before
human-level AI,
you will not
still be stating
not in the next
two years.
My own stance?
I don't know when.
So that's Eliezer
just making the case
for humility.
Because the way
Jan Lin-Quinn's talking
right now is just
not inspiring confidence
of somebody
who's epistemologically
reliable at making
these kinds of predictions.
Somebody who's
careful enough
to not accidentally
say that it's not coming
when it is coming.
Doesn't seem like
he's being careful enough.
Okay, so Jan replies,
I'm not following
any algorithm.
I'm working on
new architectures
like JEPA world models.
You can look that up.
It's like Jan Lekun's
own framework
that I haven't seen
many other people
talk about.
But anyway,
Jan continues
inference procedures,
planning and reasoning
by optimization
in embedding space,
and learning paradigms,
self-supervised learning.
When a system
with these components
starts working,
we'll be on a good trajectory.
And you know,
by good,
he means it'll get us
closer to AGI,
so he's not even
acknowledging that
maybe having AGI
will be disastrously bad.
He's just using good
as synonymous with AGI.
That's my interpretation,
at least.
So now there's a few
different sub-threads
where Eliezer also chimes in.
Here's a sub-thread
from Jorge Hernandez
where he's going back
to answer Eliezer's question
about what would you
definitely expect to see
two years before AGI.
And Jorge's saying,
Algorithms capable
of maneuvering
and meeting their goals
in highly dynamic environments,
which are also capable
of doing continuous learning,
i.e. no need
for separate training phases,
like a cat.
And Eliezer replies,
Why do you expect
to see that two years
before human-level AI
rather than three months
before human-level AI?
And Jan LeCun jumps in
and replies,
It'll take longer
than two years.
Doesn't really explain why,
but just says it.
Maybe he's too busy to reply.
And then Jorge Hernandez
replies to Jan and says,
Indeed,
cat-level AI
to human-level AI
will take us several years
and one or two more
architectural-slash-algorithmic
breakthroughs.
So,
I don't consider these replies
to be sufficient justification,
but you can imagine
just busy people
making predictions
without diving deeply
into how they're thinking.
Fair enough.
It is just Twitter,
after all, right?
It's not a serious forum.
Now,
another guy named Chad
in a different sub-thread,
he says,
I don't think scaling it
bigger is good enough.
They need a breakthrough,
just like Fusion Energy
needs a breakthrough,
but that seems to be regarded
as an incorrect opinion
right now.
And Eliezer replies,
Why do you expect
you would know
two years earlier
if that breakthrough
were on the way?
And Jan replies to that,
he says,
Because those of us
working on this
know how hard it is.
It's not that we don't have
the basic ideas
or concepts,
it's that making them work
is really hard
and takes time.
So,
that's the end of this beef.
It didn't really go
anywhere from there.
And again,
Jan to me
is just sounding like
somebody who doesn't have
epistemic humility.
Because the thing is,
on the object level,
he may be right.
Let's even say
there's a 75% chance
he's right
and a 25% chance
he's wrong.
It's just that
he's not reasoning reliably
when he says,
I know how hard it is
because I work on it.
He sounds like
the Wright brothers,
right,
where they started
working on
understanding flight.
And Wilbur Wright
famously said in 1901,
Not within a thousand years
will man ever fly.
Presumably,
he was coming
from the perspective of,
I know how hard it is
because I actually work on it.
And it's extra ironic
because he himself
was the one
who built the first
working flyer
together with his brother
in 1903.
So very similar attitude
coming here from Jan.
And it's not to say,
hey,
don't be pessimistic
or things always work.
Those aren't my claims.
My claim is just,
you don't know.
You're saying something
that you don't actually know,
that you don't have
a reliable
epistemological process.
You don't have
a reliable methodology
to be outputting statements
like,
it's going to take
more than two years.
That's what Eliezer
is also getting at.
The methodology
that you're using,
which is,
tell me subjectively
how hard it feels
like you have to work
today to get AIs
to do this stuff.
That particular methodology
we know is not reliable.
So instead of just
confidently declaring,
I know we're not
going to do this
in two years,
the correct thing to do
is to say,
it seems really hard.
I personally don't feel
like it'll happen
in two years,
but I totally acknowledge
that this is an area
where my confidence
is pretty low
and somebody
might surprise me.
It's incredibly arrogant
for him to be saying
stuff like that.
And again,
for him personally,
we have him on record
saying that LLMs
aren't going to be able
to talk about the physics
of a book on a table,
right?
So you'd think
that would give him
some humility,
but clearly no,
clearly it didn't.
It's just crazy to me
that given the stakes,
given that he is one
of the people trusted
to tell us
whether our whole species
is imminently doomed,
he's so confident
or dare I say cocky
when we really
actually need him
to show some humility.
The next Twitter beef
is with Rune.
Rune is a member
of the technical staff
at OpenAI
and a well-known
Twitter personality.
So he tweeted recently,
we will never be
quote-unquote
ready for AGI
in the same way
nobody is ready
to have their firstborn
or how Europe
wasn't ready
for the French Revolution,
but it happens anyways.
And then
Eliezer Yudkowsky replied,
nobody is ready
for a giant asteroid
crashing into Earth,
but it happens anyways,
says man working
to steer giant asteroid
toward Earth.
I thought that was
a good reply.
I think it's pretty messed up
when people who work
at these AI companies
act fatalistic,
like,
what can we do?
AI is coming for us.
Nobody can ever be ready.
It's like,
okay, great.
Have fun dying then.
That's your attitude?
And you call yourselves
the optimist.
You know,
John Sherman
from the For Humanity podcast
has this great observation
where how is it optimistic
to just be walking
like a lemming
into the future
when you have plenty
of signs
that it's not a good future
and you're just like,
that's not in my control.
I have to run toward it.
You can call it optimism
to be like,
hey,
I bet we have the power
to not run toward our doom.
I bet we can
continue avoiding doom,
continue humanity's
time-honored tradition
of somehow
not going extinct.
That's my optimism,
right?
That's the optimism
of the empowered doomer.
I consider these kind of tweets
in very poor taste.
They'd be totally uncalled for
if we were talking about
nuclear weapons,
pandemics,
this fatalism,
like,
look,
the nuke's gonna explode.
It's not in our hands.
Game theory says
the nuke is going to explode.
You're not gonna have
the perfect bomb shelter for it.
Just be ready.
It's like,
what are you talking about?
What are you talking about?
Your company
is supposed to be the steward.
It's your job
to have a no-go button.
It's not just
to have a go button.
You're supposed to only
build it when it's safe.
This is insane.
I know,
I know,
everybody's trying to build it.
If you don't build it,
somebody else will.
Okay,
but still,
you better stop them.
You better do your best,
right?
I'm not saying
you're guaranteed to succeed.
You better try your hardest
to stop everybody,
right?
In the case of nuclear proliferation,
what did we do?
We stopped everybody,
right?
So,
it's not impossible,
right?
We stopped terrorists
from building nukes.
We somehow did it.
For now,
maybe not next year,
but so far,
we did it.
Okay,
so resigning yourself
to just say it's inevitable
when many of the experts
are telling you,
this will kill us.
We can't survive this.
It's such an insane attitude to me.
I don't know why
people can't see that
at the very least,
this is just immature.
This is not how an adult
addresses the situation.
So,
even if he's right,
even if he's right,
okay,
sadly,
we think that the odds
are worth taking.
We think that there's only
a 10% chance
that we're all going to die
and that's low enough
that we really should
just plow through
because it's so hard to stop.
Okay,
if you're making a sober assessment,
fine.
But when you're talking
about it like that,
like,
hey,
you know,
having your first kid,
you can't be ready for it.
AGI,
you can't be ready for it.
To me,
it's just so inappropriate.
It's just so unbecoming
of somebody who's working
at an AI company
and actually tuned
into the situation
and somebody who should be aware
of his own responsibility
to be a voice of reason
or a voice of morality
or a voice of guidance.
Like,
compare him to Jeffrey Hinton.
At least Jeffrey Hinton,
you can tell,
he feels the weight
of responsibility.
Yashua Benjo,
he feels the weight
of responsibility.
Even somebody like Sam Hammond,
who I disagreed with,
who I think is not enough
of a doomer,
when he tweets,
it at least sounds like
he's acting like
a responsible adult,
right?
He understands the gravity
of the situation
and he's trying to balance
different factors,
whereas the attitude
that Rune is showing,
that Sam Allman is showing,
it's just unbelievable,
right?
It's very childish.
It's like,
wow.
Like,
we as a society
have implicitly
just given them
this authority
when they clearly
don't have the temperament
to handle it,
given them this authority
to decide that they're
going to go ahead
and build AGI.
It's incredibly scary.
Like,
it's great when we empower
tech innovators
and we let them run
and we're laissez-faire.
That's what makes
capitalism great.
I get it.
But when we're talking
about human extinction,
right?
The next nuclear weapon
grade technology,
and these are the kind
of people that we're
trusting to run it,
it's just like,
we have to be able
to stop them.
We have to be able
to say this isn't okay.
But the time is running
out to ever say that.
It may already be run out.
It's just,
like I said,
really poor taste.
And it's sad to see
the discourse happen
like this.
My next beef is with the famous
George Mason University
economics professor,
Tyler Cowan.
Overall,
I tend to be a big fan
of Tyler.
He's written a lot
of interesting books,
essays.
He's one of the most
prolific readers,
bloggers,
and broad domain
polymath thinkers
in the world.
And he's also known
for assembling
amazing talent.
I mean,
the George Mason University
economics department
is insanely overpowered.
Three people
that I personally follow,
Robin Hanson,
Brian Kaplan,
Alex Tabarrok,
they all work
for Tyler Cowan
in this one little department.
What are the chances?
So Tyler has created
this amazing talent attractor.
And anyway,
he's great.
But just when it comes
to AI,
I've never been able
to make any sense
out of his AI takes.
So from my perspective,
he's just got
this major blind spot.
Or perhaps it is I
who has the blind spot.
One of us has a blind spot.
Okay.
And this particular
Twitter beef stems
out of a recent post
that he wrote
on his blog.
Title of the post is
Effective Altruists
and Finance Theory.
I won't read
the whole thing,
but the key part
is when he says,
look,
all you doomers,
why aren't you making
some kind of move
in the stock market
that corresponds
to your belief in doom?
It doesn't make sense,
guys.
I'll quote this part
of Tyler's post.
He's talking about
doomers like me.
He's saying,
I have never,
ever heard not once.
I'm going to sit down
and study finance
and see if I can find
a feasible way
to short the market.
If I can't,
I will feel sad,
but I might get back
to you for further guidance.
And he's also never heard,
quote,
soon enough,
AI will be good enough
to tell me how to short
the market intelligently.
Then I am going to do this.
Thanks for the tip.
Tyler's post continues,
nope, never.
The absence of the last one
from the discourse
I find especially odd.
AGI will be powerful enough
to destroy us,
but not good enough
to help me do
an effective short.
Okay, dot, dot, dot.
The sociology here
is more indicative
of what's going on
than the arguments themselves
because the EAs,
rationality types,
and doomsters here
generally are very good
at learning new things.
For me,
it's frustrating
anytime I see
a non-doomer
who seems to not really
get pretty obvious truths
about what it's like
being a doomer
and then go on
to be like,
oh, doomers are a cult
or they're religious
or they refuse to see this.
So in this particular case,
a lot of us doomers
on Twitter
kind of are scratching
our heads saying like,
um, Tyler,
what do you want us
to do exactly?
Like, this doesn't make sense.
There was a good response
from a guy named
Jonathan Pallison
on Twitter.
He wrote,
Tyler Cowan wrote
another post about his,
if you really believe
there is an existential AI risk,
you should short the market
hypothesis.
Unfortunately,
even though he writes
about a lot of details,
the hypothesis fundamentally
does not make sense to me.
Let's say I think
there is 80% chance
that AI brings
increased wealth
and 20% chance
that the AI kills us all.
Why then would I expect
gains from shorting the market?
It seems better
to be long on the market
and earn good returns
in 80% of cases,
i.e. the opposite
of his suggestion.
Then Tyler Cowan himself
responds to Jonathan.
He says,
covered in my list.
He's talking about
his list of falsehoods
that doers say.
Tyler continues,
you can go long volatility
or long some things
and short others,
but you absolutely
should have
some kind of hedge.
Jonathan responds,
could you try and explain
why spelling it out more?
It's a topic
a lot of us
take very seriously
and there is seemingly
universal trouble
understanding you.
For example,
take the two worlds.
Let's say we're in the
80% non-doom world,
then it's mostly better
to go long
since there will be
more growth.
Let's say we're in
the 20% doom world,
then I'm not sure
what's best,
but either way I die
so I can't use the money.
He's saying like,
why would I short the market
when,
if and when I do win?
Okay, great,
I win and then
I instantly die.
So why would I make
that kind of financial plan?
That's basically
what he's asking Tyler.
And Tyler actually replies,
I have bought
fire insurance
on my home,
right?
Even though I don't think
a fire is even
20% likely.
And Jonathan replies,
a vital difference.
Fire insurance
helps dramatically
in the small proportion
of worlds where
there is a fire.
I can rebuild my life
which is in ruins.
It helps so much
that it is overall
worth it,
even though I lose
money in the majority
of worlds.
But the doom scenario,
short investment,
is not like that.
Instead of the payout
being extra useful
in the doom scenario,
it is less useful
because I and everyone
I care about die.
I really wanted
to see Tyler respond
to that because that
is the crux of the issue
for me.
Like, I don't know
why Tyler is saying
this stuff,
but unfortunately Tyler
did not respond to that.
I got into the fray myself.
I did a quote tweet
and I said,
killing me
at Tyler Cowen.
Want to hash this out
on my podcast?
So, you know,
I'm always trying to work
in doom debates
when I can
because I think
it would be awesome
for Tyler to come
and explain this
here on my podcast,
right?
I mean,
I would just seek
to understand
what he's off about.
I mean,
certainly there's
no question
that me versus Tyler,
Tyler is the one
who's way more insightful
about the finances
of all this, right?
So I'm totally open
to the idea
that he has
this brilliant point
how I should be able
to make money
off of my doom claims
if I'm right
and I'm just not seeing it
and Tyler is going
to explain it to me
and set me straight.
I'm open to that possibility,
Tyler.
So you're invited
to the podcast.
I was happy to see
60 people like my tweet.
I'm glad people
are starting to see
doom debates
as a productive forum
for the stuff
because that is
a major goal
of doom debates
is to be that forum.
Remember,
in addition to raising
public awareness
of the imminent risk
of human extinction
from AGI,
the other part
of doom debates'
mission is to raise
the level of discourse
about these kinds
of urgent topics.
Now,
I did get precisely
one type of reply
slash engagement
from Tyler
in the aftermath
of posting my tweet.
He retweeted
somebody else's tweet,
a guy named
Alexander Campbell
who replied to my tweet.
So the only engagement
I got from Tyler
is that he retweeted
Alexander Campbell
and Alexander wrote,
until doomers
understand and acknowledge
mark to market,
this conversation
will get nowhere.
So Tyler retweeted that.
I just wrote,
what?
I did not understand
what Alexander Campbell
is saying.
Doomers have to understand
and acknowledge
mark to market.
So doing my best
to unpack what he's
trying to say
because I never got clarity.
Maybe he's saying like,
all the different stocks
are going to have
lower marks
as we get closer
to doom.
Ergo,
doomers should short them
or buy puts on them,
right?
Which is just kind of
repeating
Tyler's original point.
So I'm just not understanding
the additional level
of insight
that Alexander Campbell
is bringing
by posting that
or that Tyler Cowen
is bringing
by retweeting that.
Again,
maybe the problem is me,
right?
I'm happy to come
hash this out.
Or if you're watching this
and you're on Tyler's side here
and you totally think
doomers ought to be
buying puts
even though the puts
only pay out
right before it's obvious
the world is going to end
and we can't really profit
from that scenario.
But if you think
we somehow can profit,
I'm all ears,
let me know.
And if we take a look
at my Twitter thread,
nobody else besides
Alexander Campbell
stepped up
to explain
what I was missing.
Greg Colburn commented,
seems like the issue
is that Tyler
doesn't actually think
of AI doom as doom,
i.e. the actual
end of humanity.
He thinks of it more
as a survivable catastrophe
where he will still
be alive
and there will be money
in houses
and stock markets
afterward.
A guy named Alexi
wrote,
Jesus,
that has to be
trolling at this point.
So yeah,
it's pretty crazy
that the whole
effective altruist
or AI doomer community
can't seem to reach
agreement on Tyler
on what should be
a pretty simple issue,
right?
Like I don't mind
making my beliefs
consistent enough
that if there's some
high expected value
opportunity that I'm missing,
then to either
take the opportunity
or admit that I don't
really believe
what I say I believe.
Like I'm happy to do
that kind of reconciliation.
I just don't get it,
right?
I don't get the logic.
I don't get why
I have to short the market
if I only think
a scenario where I collect
is one where doom
is extremely imminent.
But okay,
steel manning,
I guess there is
kind of a dream scenario
where people realize,
uh-oh,
we're so doomed,
in like 30 years
AI is going to come
kill us.
And at that point,
a lot of the stocks
tank,
but they still have
a little bit of value
left because hey,
there's still going to
be some cash flows
in the last 30 years
of the earth.
You know,
like the businesses
are still going to be
going,
so they're worth something.
The market's still open,
but the values have
crashed so much
because there's no
long-term horizon
for these companies.
At that point,
maybe I'll have
a decade or two
to enjoy my shorts,
right?
So I think that's
kind of where
Tyler Cowen is coming from
is like,
look,
don't you want to have
a last good decade or two?
So I guess in that case,
the crux of disagreement
would just be that
I don't think
we'll have two decades.
I think that
the moment it becomes
consensus that we're doomed,
if we haven't actually
stopped the doom
with good government policy,
then I just think
the crash will be quick,
right?
I think we're going
to hell pretty quick
or, you know,
I think tomorrow
all of our computers
might turn off
because China was
working on a larger
scale AI than any of us
knew about
and it became
super intelligent
and it's now a virus
that makes the internet
not work, right?
I expect to wake up
one day like that
and be like,
oh, okay,
it's game over.
The world is now
on a permanent slide
into hell.
Nice note, everybody,
right?
So I just don't see myself
being rich in some kind
of bunker in that scenario.
I expect to just go down
with everybody else.
So call me, Tyler.
By the way,
if you listen to my
recent episode
where I did the epistemology
debate with Vaden and Ben,
you know,
Baze versus Popper,
Vaden asked me
a similar question of,
hey, if you think
the world's going to end,
why don't you just go
make money on a prediction
market saying that
the world's going to end?
Or why don't I see
a prediction market
that says there's
a 50% P doom?
How come it's just
you telling you that?
Don't you trust
prediction markets?
And I pointed out,
look, prediction markets
are a very powerful tool
for aggregating
people's beliefs,
but the aggregation
doesn't work
when the people
who think that
there's a high P doom
don't expect to make money
when they're right.
And he said,
well, don't you think
that the price will increase
as P doom increases
toward 100?
And I was like,
well, no,
because the price
only increases
when somebody expects
that somebody else
will come in
with an even higher price
and then pay them.
But the whole point
of a higher P doom
is that you think
it's less likely
that you're going
to have to pay, right?
So there's a major
biasing force
so that anytime you have
a prediction market
about what's the P doom,
it's going to be
heavily biased.
If it exists at all,
it's going to be
heavily biased
toward low P doom
because only the people
who want to come in
and bet on low P doom
are rationally
expecting to collect.
Like anybody who comes
in with a high P doom,
maybe they're just
trying to be honest,
but they shouldn't
expect to collect, right?
So it's like a structural
problem with trying
to bet on doom.
It's the same thing
as betting on like,
hey, will this platform
be legally allowed
to operate in two years?
Or like,
will this platform
run away with
everybody's money?
If you bet yes,
that's like the dumbest
bet ever, right?
Because you only win
when the platform
runs away with your money, right?
So it's a similar kind
of structural problem.
You're assuming
that the platform
will stay in existence
to pay you.
And similarly,
you're assuming
that the world
will stay in existence
to pay you
when you bet
on these kinds of markets.
So that's a similar
kind of impasse, right?
What Vaden was asking me,
I feel like that's
kind of similar
to what Tyler
is asking me
where these people
are just aren't appreciating
the whole end
of the world thing
when they talk
about making money, right?
Or for whatever reason,
there's some kind
of disconnect here.
So Tyler
or anybody
who feels like
they're aligned
with Tyler
on this issue,
balls in your court.
Hit me up
on Twitter DM
or email me
wiseguy at gmail.com.
Come on the show.
Let's hash this out.
The next Twitter beef
is with the one
and only Professor
David Deutsch.
I actually instigated this.
I just randomly
did a top-level tweet
where I said,
Hey,
at David Deutsch,
instead of hand-waving
about creativity
being the essence
of what humans do
that AIs can't do,
why not just compare
human versus AI abilities
to plan toward goals
in the physical universe?
You'd make sense
and you'd still be
pointing out
a major separation.
You're probably thinking,
Whoa, Liron,
coming out of nowhere.
But, you know,
I've been watching
his podcasts
and this is what
made me want
to tweet that at him.
It's a podcast
where he talked
with Naval Ravikant
who I covered
on Doomtif's number one.
Check out that episode.
So David was talking
with Naval about creativity
and this is basically
how the conversation went.
David Deutsch says,
GPT-4 is not creative
and Naval says,
How would you define creativity?
And David Deutsch says,
Creativity is fundamentally
impossible to define.
So let me play you
that clip right now.
You've conceded
that GPT-4
has made progress
and it's improving,
but you're not willing
to say that it's improving
in the direction
of being a person.
Why?
So I see no creativity.
Now people say,
Oh, look,
it did something
I didn't predict,
so it's created.
Perhaps one of the problems
here is that we just
define creativity
so poorly.
So how would you define
creativity in this context?
Creativity
and knowledge
and explanation
are all fundamentally
impossible to define
because once you
have defined them,
then you can set up
a formal system
in which they are
then confined.
If you had the system
that met
that definition,
then it would be
confined to that
and it could never
produce anything
outside the system.
So, for example,
if you knew about
arithmetic
to the level
of the postulates
of P and O
and so on,
it could never,
and when I say never,
I mean never,
produce Gödel's theorem
because Gödel's theorem
involves going outside
that system
and explaining it.
You can say
that it's not
defining something
and then executing
the algorithm,
basically,
because it would always
be an algorithm
and once it was
in a framework.
So, you say,
well, it's ability
to go outside
the framework.
Well, I tried,
by the way,
ordering ChatGPT
to disobey me
and it didn't refuse
but it absolutely
didn't understand
what I was going on about.
Okay, as I watch
this clip now,
one thought that
comes to mind is
he's making creativity
be about like jumping
out of a formal system.
Like, imagine an AI
that could see Gödel's theorem
if it just knew arithmetic
but it's like,
okay, has he noticed
that today you can
pull up GPT
and talk to it
about Gödel's theorem?
So, like,
I know that's not
exactly your point.
Your point was about
an AI that's just
trained on piano arithmetic
but, like,
doesn't it give you
some kind of clue
about the nature
of intelligence
that we're just
discoursing with AIs
we have today
and they don't have
any trouble answering
your questions
about Gödel's theorem
and, like,
engaging in thought
experiments with you
about Gödel's theorem?
Like, it's not like
Gödel's theorem
is breaking their brains
here, right?
It's probably easier
for AIs to talk
about Gödel's theorem
than it is for humans
to talk about
Gödel's theorem
all things being equal.
So, when I was
listening to the clip
I thought it would be
natural for David Deutsch
to be like,
okay, I'm talking
about Gödel's theorem
let's ask LLMs
about Gödel's theorem
but, no,
that's not what he said.
He said,
let's ask LLMs
to disobey me.
Like, in his mind
this question of
please disobey me
is more analogous
to a formal
jumping out of the system
than just
talking about systems,
right?
And just directly
asking them
about Gödel's theorem.
So, I guess
he and I are just
both making two
different crude analogies.
But, anyway,
at the end of the day
my beef with David
is just like
is creativity
really that hard
to define?
Do you really have
to lean back
and be like,
I can't define it.
It's inevitable.
Like, really?
You can't even
take a stab at it?
And remember
I asked him
can't you just compare
human versus AI
abilities to plan
toward goals
in the physical universe?
Like, isn't that
a nice separation
that you could say?
Couldn't that support
your point of why
AIs aren't human level yet?
So, David actually
came over and replied
which is rare, right?
I don't get that many
David Deutsch replies
when I'm on Twitter.
So, he replied to my tweet
and he said
the answer is in my book
The Beginning of Infinity.
And then I wrote
I read it.
Big fan of that
and Fabric of Reality
his other book.
I'm still confused
about the meaning
of your claims
about AI lacking creativity
or knowledge creation
compared to humans.
E.g.
how to operationalize
testing the difference.
Alright?
And then
David replied again
and what he wrote was
right.
So, he just gave me
this one word reply
like apparently
just like a snarky reply
like I'm just explaining
what I'm confused about
and he's basically
right.
You are confused
Liron.
Go figure it out.
Alright?
That's like my interpretation
of his one word reply.
So, then I replied again
I wrote
according to your interview
the one I clipped
it's impossible
to define
slash operationalize.
Why focus on
using an undefined term
to talk about
the distinction
between human
and AI abilities
when there's also
an interesting distinction
that can be defined
namely planning prowess?
Now, David didn't reply
directly
but another user
named Sarah Fitzclairage
replied
and she wrote
that's like suggesting
looking under the lamp post
because the light
is brighter there.
And David Deutsch
retweeted her
so presumably
that's like his reply too.
Okay, fair enough.
I get it.
That makes sense.
And then I replied
in the general case
sure
you know
the move that I did
is kind of like
suggesting looking under
the lamp post
because the light's
brighter there.
Right?
That's kind of the move
I made when I said
here's another separation
that you could be
pointing out
between humans and AIs.
but my tweet continues
it feels like
in this particular case
he wants to say
something about
what current AIs
can't do
and this is the actual
thing they can't do.
And then David Deutsch
replies
and by actual
you mean
defined and empirically
testable?
And then I said
yep
I get what you're saying
about undefinability
in formal systems
but
do we have to go there
to e.g.
talk about
why GPT-4
can't write
a best-selling book?
And that is where
I lost David Deutsch
he didn't reply anymore
but like
I remain confused
right?
It's like
why is he going
on podcasts
and writing in his books
these hand-wavy claims
about creativity?
Why is a genius
like him
opting out
of the problem
of defining
what creativity means?
Like just define it
it's not that hard
right?
I mean
Eliezer Yudkowski
defines it great.
He basically says
finding solutions
that rank high
in your preference ordering
and low
in naive search ordering
so visually imagine
there's just like
this giant space
that you have to search
like you have to search
the Sahara Desert
and somehow
you find the one
bar of gold
that's in the Sahara Desert
the bar of gold
ranks high
in your preference ordering
because you think
a bar of gold
is valuable
but it would rank
very very low
in a naive search ordering
imagine asking
a random person
to try to search
the Sahara Desert
to find you the gold
they wouldn't know
where to begin
right?
Maybe they invent
some kind of
really powerful sonar
or scanning technology
that nobody's ever had before
and they find the gold
that way
well if they do that
I would say
that they're being creative
because they search
for something
that the naive search
would have sucked at
and even a pretty
intelligent human searching
would still fail to find
right?
So that would fall
under my definition
of creativity
it's very closely connected
to my definition
of search
and my definition
of intelligence
it's really hard
for me to think
about somebody
being extremely intelligent
but not creative
or vice versa
extremely creative
but not intelligent
the way I would make
that distinction
it's actually an interesting
puzzle
if I think about
somebody who's very
creative and not intelligent
I basically just think of
like kind of like
a babbling baby
right?
Just like a lot of
randomness involved
a lot of free association
where they're being
very generative
right?
They're combining
a bunch of building blocks
together in a bunch
of novel ways
but they're just not
doing it in a way
that searches
for a complex criterion
right?
So they're not narrowing
down a search space
they're just kind of
generating a bunch
of interesting elements
in a search space
and to the degree
that the elements
they generate
are consistently good
then they're not
merely creative
they're also intelligent
in order to even
generate good enough
candidates
so already it wasn't
a hard distinction
the way I was making
it out to be
let's do it the other way
let's talk about
somebody who's very
high intelligence
but low creativity
maybe somebody like that
always wants their
destination to be
very predictable
so it's like
oh you gave me
clear instructions
I'm just going to
follow the instructions
exactly
or you gave me
a handbook
I'm just going to
follow the handbook
closely
basically somebody
who's reducing
unpredictability
might be high
intelligence
low creativity
but of course
reducing unpredictability
if you tell Elon Musk
get us to Mars
he still has to be
really creative
even though the outcome
getting to Mars
is predictable
because he's so
intelligent
and so effective
so reducing
unpredictability
actually isn't sufficient
to say somebody
is highly intelligent
but not creative
I think the concept
again
the distinction
starts to get very blurred
like I'm not seeing
much of a distinction
between intelligence
and creativity
also if you imagine
somebody who's
highly intelligent
but very low creativity
all you have to do
is inject randomness
into that person's
thought process
and suddenly he becomes
both intelligent
and creative
so if this person
is struggling to just
be unpredictable enough
or be random enough
no problem
just give him a coin
just tell him to flip
the coin a bunch of times
and just generate
random data
and work with
the random data
just tell him
that he has to do that
it's a very simple instruction
it's a very simple
modification to the system
and the process
that he's going to have
to follow
to take the random data
and follow your instructions
to incorporate
the random data
the end result
of that process
is creativity
like if you're
playing a song
improvisation
is basically
taking random mistakes
and then
making them work anyway
at least that's
one key
that people use
to have good improvisation
it's not the only way
you can improvise
but it's certainly
a good way
right
so suddenly you turn
somebody who's
ridiculously good at music
you turn him into
somebody who's good
at creative music
because you just
order him
to fix all of these
mistakes that you're
injecting
right
so there's always
a trivial gap
in my mind
between high intelligence
and high creativity
but people like
David Deutsch
seem to insist
on treating creativity
as this mysterious
thing that they won't
even define
or they'll defend
their lack of a definition
because they'll connect
it to Godel's theorem
and they'll say
how could you possibly
define Godel's proposition
when you're working
in a formal system
you know the proposition
of I have no proof
right
that kind of trick
is if you studied
Godel's theorem
it's like a statement
that's saying
I am not provable
so it's kind of a real
logic twister
kind of a paradox
that's like the flavor
of Godel's theorem
and so David Deutsch
is getting a lot of mileage
out of Godel's theorem
being like
look this is the brain twister
that makes creativity
in current AI
is impossible
and I just don't see
the connection
right
it really seems like
he's reaching for a connection
I don't get it
luckily I've got some
Deutschians on speed dial
remember my old pals
Ben and Vaden
from last week's episode
where we debated epistemology
I know they're big fans
of Deutsch
so I will be sure
to ask them
in a few days
when we record our part two
I will be sure
to ask them
what Deutsch is thinking
when he talks about creativity
and this ineffable thing
that AIs can do
that humans can't
maybe they will have
a good answer
and maybe they'll enlighten us
but as for this Twitter beef
we're gonna have to leave it here
because there's no more
David Deutsch replies
okay those are all
the Twitter beefs
I have to share with you today
if you missed the previous
Twitter beefs episode
it was previously known as
Doomtiffs number one
if you missed that episode
go check it out
I think you'll enjoy it as well
it's the same kind of content
I'll probably be doing
these kind of Twitter beef episodes
once every two months
because you know
it's an outlet for me
it's a release right
so when I'm on Twitter
getting pissed off
and frustrated
at least I can
add it to my collection
of these beefs
and I can release it out
upon you
and you can also
vicariously
live the frustration
that's why you come here
to Doom Debates
because you want to
live the frustration
alright this is gonna be
a big week for Doom Debates
we've got an interview
coming up with
Andrew Critch
who was formerly
a researcher
at the Machine Intelligence
Research Institute
and also one of the
co-founders
of the Center for
Applied Rationality
super smart guy
so this is not gonna be
a very heated debate
because we both have
high P Dooms
but it'll be
a very interesting
exchange of ideas
between two people
who have different
conceptions of Doom
I'm still planning
to do a longer episode
where I analyze
all of the different
David Deutsch media
that I can get my hands on
try to do an authoritative
David Deutsch episode
and lots of other good stuff
so stay tuned
now if you're watching the show
hopefully I've been
giving you a lot of value
a lot of entertainment
a lot of information
I don't ask for much
in return
all I want you to do
is synthesize
the neurotransmitter dopamine
maybe put it in a vial
package it up
and just mail it to my address
just mail me the dopamine
okay
that sounds hard to do
okay fair enough
if it's hard to do
what you can give me instead
is just a unit
of social engagement
so like if you're
watching this on YouTube
just smack that
subscribe button on YouTube
my eyes will see
the number go up
and my brain will actually
give me the same
hit of dopamine
so that's probably
a more convenient process
for you to do it that way
or just smack the like button
smack the comment button
even write a custom comment
that's always much appreciated
or go to doomdebates.com
type your email address
smack the enter button
subscribe to my sub stack
that'll give me
the hit of dopamine
write a comment on my sub stack
that'll give me
the hit of dopamine
okay
one way or another
you gotta get me the dopamine
that's all I ask
alright
thanks very much
and I look forward
to seeing you again
on the next episode
of Doom Debates
We'll be right back.
