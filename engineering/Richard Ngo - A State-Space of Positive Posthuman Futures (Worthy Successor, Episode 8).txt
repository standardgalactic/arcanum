When I say alignment, a lot of what I mean is just science of cognition, science of intelligence, science of machine learning, where I mostly think that the field as a whole has not been doing that kind of science very much.
And in particular, I often use this analogy to the field of psychology, where they used to have a sort of like strong behaviorist paradigm where talking about the internal representations and internal cognition of animals or humans was taboo.
Skinner, Skinner, yes, Skinner did his thing for a little while.
Skinner and a bunch of others. And then they sort of realized the flaws of this with like Chomsky and various others pushing back and eventually graduated to a much more nuanced perspective where like, yeah, sure, it is hard to study the emotions and the cognition of biological organisms.
But you've got to at least acknowledge that that's the thing you are trying to study.
I think like machine learning as a field has kind of been in an analogous position where for various reasons, partly due to like being burned by previous talk about AGI and so on, people have been really lodged in a behaviorist mindset where the only meaningful things to study were, you know, the input-output mapping of neural networks.
And like, in some sense, there were whole like types of science that barely anyone was doing.
And now there's like, very obvious that you can't actually understand neural networks that way.
This is Daniel Fagelli. You're tuned in to The Trajectory.
This is the eighth episode in our Worthy Successor series where we talk about the kinds of artificial intelligence and digital life that we might hope to populate the galaxy when humans are no longer here.
Very few of the folks that we've spoken to in this series really have ever had as part of their profession thinking through positive AGI futures.
We might say that maybe Nick Bostrom did at some level, right?
He founded the Future of Humanity Institute for quite some time.
But really nobody else.
Richard Ngo is an exception.
Richard was doing exactly that at OpenAI.
Previously, he was with FHI for a bit for an internship before spending a couple of years at DeepMind and then his last three with OpenAI before leaving.
Richard has a lot of interesting tweets, a lot of interesting takes around where AI is going.
I recommend you check out some of his talks on YouTube.
In this episode, we go into what he would define as the worthy successor criteria of a future intelligence and what he thinks we could do to sort of figure out if what we're building now is getting us in that direction.
There's a lot to say in the outro of this episode, so I'm going to save it in the outro.
Many things I super agree with and learned, some which I really kind of didn't follow on some level.
I hope we get to unpack later, but I'll save all that for the outro.
Tons of brilliance in this episode.
It was a lot of fun to talk with Richard.
I hope you enjoy it.
This is Richard Ngo here on The Trajectory.
So, Richard, welcome to the show.
Thanks so much, Dan.
Really looking forward to talking.
Yeah, glad to be able to catch up a little bit here.
We've had a variety of perspectives, and having followed your work from afar, I figured you'd have some fun ones here.
We're talking about the idea of the worthy successor.
You had a very interesting essay from the very end of 2023 called Succession, which was part of kind of your fiction series exploring some of these ideas.
I'd love to get your definition here.
If you could look down upon Earth or maybe the Milky Way in a million years, a thousand years, however long, and hominids aren't around any longer.
But the things that are there and what they're doing feels like, hey, that's potentially a net win.
Like that was actually ended up okay in the end regardless.
What would that be for you?
Let's just tee that up.
Yeah.
So I think the place I'd start here is by thinking about the fact that, you know, the universe is just so mind-bogglingly enormous that when I'm thinking about succession from humans, I'm really actually thinking about like many different scales at once, right?
So we have what's happening on Earth, we have what's happening in the solar system, in the galaxy, and then beyond that.
And so I actually think that most of what I want from the agents that are sort of ultimately in charge is actually that they treat us well or they have a good attitude towards their predecessors.
I think this is part of like a way of thinking about morality, if you will, that, you know, when you're powerful, you shouldn't just be eradicating, you know, the agents that came before you.
And so if those agents have in fact, you know, been kind to humans, been sort of pretty reasonable in how they've approached dealing with us, for example, we get to keep the Earth or the solar system or the Milky Way.
I think that's like maybe the very first starting point for what I think of as a worthy successor, right?
Like I would have trouble thinking of them as worthy successors if they had wiped us out, for instance.
Yeah, yeah.
Well, and again, I think there's all kinds of reasons we wouldn't be here.
I mean, it seems reasonable that, you know, people are having less children.
I think they're going to be having very fulfilling and very productive times in virtual spaces.
I think that augmenting minds will be a thing.
And it doesn't strike me as obvious that a thousand years from now or 10,000 years from now, even if we got treated brilliantly by AGI, we would all kind of have opposable thumbs and trim our fingernails.
You know what I mean? So I think it strikes me that there might be no hominids with no genocide scenario involved.
There might just be like our own progressive choices to sort of level up or become more substrate independent or whatever the case may be.
Do you consider that likely or would you say, no, if there's no hominids that are like having to take showers and like get jealous sometimes and like brush their teeth, then surely malice was at play.
Yeah, so I'm not so much trying to fight the hypothetical here.
I think it's the point I want to emphasize is something like the future is going to be extremely diverse from our modern standards.
And there's a sense here in which we are sort of thinking about like what a worthy successor looks like from a like third person objective perspective and saying, you know, like how what is the universe filled with?
But actually, I think when you start to dig into the details, what we're going to see is there's going to be a bunch of different branching in different directions that, you know, different people are going to want to incorporate AI into their lives in different ways or merge with AI in different ways or develop different AIs of their own.
And so a lot of the time I'm thinking about this more from a procedural perspective, almost rather than an outcome driven perspective, because I think that, yeah, when I think about like it's sort of inherent in the word succession, right?
Like that, like the way that these agents came about is going to be through some process that started with us and like hopefully incorporated a bunch of our values and hopefully incorporated a bunch of like maybe like a kind of merging with humans.
Could be, yeah.
Yeah, so that's a sort of default picture that I have in my mind that as you go further out from Earth, that maybe, you know, humans, you know, literal biological humans might remain on Earth, that the rest of the solar system might be filled with, you know, human minds in an uploaded way.
So the rest of the galaxy might be filled with post-human minds that are still somewhat like recognizably related to or descended from humans.
And then if you – the further out you go, the more alien creatures you get, but hopefully in a way that was produced via humans choosing that route and via like people individually deciding that they wanted to merge with an AI in a certain way.
Yeah, yeah.
So I'm actually not a huge fiction person, but every now and again people will like hurl books at me and be like, you must read this.
And so there's like Greg Egan's Diaspora and then there's Accelerando where sort of these notions of like, well, these people are upgraded at this level.
These people are upgraded at this level.
They've kind of made their own calls.
There's kind of peace, you know, some sort of a – magically the Westphalian values of sort of sovereignty and peace have sort of translated to minds that are a billion times ours and no higher values have emerged somehow.
So I think there's fictitious examples of sort of this existing.
It sounds like for you, if there is a day where maybe there are no people or – because I presume you don't suspect maybe let's say a trillion years from now we would still have that many regular hominids as we are, right?
At some point, I presume, even through our own volition, you would assume we would bubble into other stuff.
I think it's plausible that – for example, I can imagine a case where out of nostalgia you would still want humans to go through a kind of like a journey that mimics the – like each individual human to go through a journey that mimics the human race, for instance, right?
In the same way that even after I am able to make a clone of myself fully formed, I might still want to have children by having them start off as infants and then like gradually learning and becoming more capable.
Are you talking a billion years from now, my dude?
Well, I think the process is going to be – you know, when I'm a thousand years old and I want to have children, you know, am I going to start them off at the age of 20, at the age of 50, at the age of 100?
Possibly, but I can also imagine a world in which, you know, you start them off just like biological human children start, which is, you know, infants like with very little capability.
And we are sort of, I think, the cosmic version of infants, if you will.
Yeah, well, Bostrom said the exact same thing on the show.
He was like, you know, wouldn't it kind of be a shame if like, you know, you can tangibly see the difference between a two-year-old and a five-year-old and a five-year-old and a 12-year-old.
Isn't it sort of a shame that at 22 or so, like that's over?
You know, like shouldn't that continue?
You think maybe people will go back to kind of that retro stuff out of its value or what have you.
But it sounds like I'm going to try to crystallize in a nutshell your idea number one and then we'll get into the other worthy successor traits.
And it sounds like basically it's – you said you're thinking about it procedurally.
Now, I just want to make sure I'm gripping what you're saying here.
You're thinking particular nuances of what the visage is and how it looks.
Yes, there should be a grand variety, but there should be this kind of respect and choice throughout as kind of number one.
And if it be a worthy successor, it procedurally rolls out these changes in a way that is sort of respectful of the different gradients of life, et cetera.
I'm not going to put words in your mouth.
I just want to say, am I in the right ballpark?
Right.
And I think that seems broadly right.
And I think one way of thinking about the procedural claim is just like, well, I don't know how to extrapolate my values, right?
If you take my values and try and say, what does it look like for them to be implemented by a mind a thousand or a million times smarter than mine, I just don't actually have any answer.
So in some sense, even when I talk about worthy successes from an outcomes-based perspective, I still have to come back and sort of have a procedural pointer, which is to say they start off with my values and then they respect them the whole way through and try and elaborate on them until they come up with something that will look very alien.
Yeah.
So that's one angle on the proceduralism.
Okay.
I think maybe another angle on it is the idea that one of the things I really value in my life is being a moral agent, not just a moral patient.
And I think it's tempting when we think about, you know, when we have these axiological discussions of what should the world look like to assume that the creatures in the future are sort of moral patients, that we get to pick what they look like that maximizes, you know, how much value they hold, if you will.
But that shouldn't be the case.
If I want, if there's one trait that my worthy successes should have, it's that they have like the freedom to actually make meaningful choices in the same way that I do.
And so what that means is that I can't actually specify what they should look like because they might choose to change that or they might choose to adapt themselves.
Absolutely.
Well, just to kind of dip into that, a couple of things about this idea of values you've just brought up.
So one might say, so it's, you know, kind of element one you're addressing here.
Hey, it should begin with my values and sort of allow me if I choose to remain a hominid, even if I'm not of any economic value, it should, you know, where I come from, Richard, there's a species called the piping plover.
Nature has mandated the piping plover to die.
But humans have decided to set aside tax money to ensure that like random little ecosystems are still there so that they can plover, whatever they do, right?
I don't know how much I pay for it every year, but it's like, you know, sections of beach where they lay eggs in like really peculiar ways.
And they're pretty adorable creatures.
So the hope would be, hey, even if I wanted to, if there was no way for me to do anything or whatever, I'd want to be able to stay a human or maybe upgrade myself, but only to a certain level or maybe have children in a biological way, have them come up through some.
So they would respect all that, but it would also, you know, have the freedom to be its own thing.
It wouldn't just be following my explicit orders and directions all the time.
It would be able to develop itself as I imagine you did from your parents, you know, long ago.
So both of those things you're touching on, one might say, well, geez, you brought up an apt point, Richard, and I'm not in any way countering it.
I just want to see how you tackle this.
Like, you know, you don't know how a mind a million times beyond yours would instantiate the values you have in your head.
I've got a couple of things I want to just touch on with that for you.
Number one, it doesn't strike me as self-evident that this sort of Westphalian snippet of human time that we've had where there's sort of this sovereignty stuff and human life isn't cheap anymore.
Well, you and me live in a pretty good time, Richard.
I mean, comparatively speaking, man.
So it's a good shake, brother.
It's a real good shake.
Wasn't always the case, really.
And it doesn't strike me as like a mind reaching to a certain size would always necessarily concur or agree with your initial values.
It might have things to do.
I mean, you certainly would respect.
I don't foresee you as the kind of guy who would, you know, run over a dog just because you're late for a meeting or something, right?
But if you've got to build a hospital or a highway and there's roly-poly bugs or fire ants or something, I mean, you're going to do what you're going to do, right?
It is what it is.
It doesn't strike me as eternally the case that the freedom of choice of such a moral agent or patient or whatever you want to refer to it would always sort of take into minute consideration all of your sort of preferences.
So I like both ideas, but I'm also wondering, like, how viable is the first when the second is really enacted by something that we can no longer comprehend?
Yeah.
So I think this is why I emphasized how big the universe is, right?
I think if you imagine us living amongst – or if you imagine biological humans living amongst these galactic entities and sort of like interspersed between them, then, yeah, that takes a lot of work not to step on any ants.
But if you imagine that the earth is sacred or the solar system is like fenced off and left alone, then I think it's much easier to say, well, okay, you don't actually need to care about humans that much – or like you don't need to care about your historical legacy that much in order to preserve that.
Yeah, yeah.
So look, I'm totally down with the point.
Bostrom, who I think famously was very much not in the camp of considering the viability of what you just said, the last time I interviewed him, which was 10 years from the first, did articulate essentially the same idea.
And it comes up on Twitter every now and again.
I guess the idea here is, look, if we can get the initial trajectory right where we can sort of have enough of an agreement that these things will do whatever their agency and their new aims dictate, who knows if they will be grateful for us or something.
But through some means, we will get them or it to sort of say, hey, can we at least get earth?
Like if you're going to go do things beyond us and that's a wonderful thing, can we still hang out here?
And the thought is, well, the universe is so large.
There are so many resources elsewhere and interesting things to do.
Of course, it's quite actually a reasonable demand.
And then there are other arguments.
At some point, Yudkowsky, I think, commented on something like this on Twitter where it's sort of like it seems likely that the resources you start with would be the easiest ones to access.
And it also seems like if other people are building rival AGIs, et cetera, there may be some reason to just sort of make sure you can get this hoopla under control before you take off.
I mean some people really do feel like it's quite a stretch for us to ask for that charity.
If it could be mining our minerals and taking off from here and leveraging this planet in the ways that are most helpful, you know, would it not?
What would an agreement to get it to think that way look like?
I mean we'd love your thoughts here.
Yeah.
I don't think this is guaranteed by any means.
I do think that compared with someone like Yudkowsky, I'm more optimistic that these criteria fall out of sort of like decision theoretic considerations, let's say.
Okay.
So I think there's a sense in which, you know, a lot of Yudkowsky's work is about functional decision theory and the idea of, you know, even if something has already happened,
you should still be acting in a way that, like for the Parfitt's Hitchhiker case, for example, right?
Like where you're stranded in the desert and somebody comes by and says, I'll pick you up and rescue you if you agree to pay me $100 when we make it into town.
And so the idea of functional decision theory here is that you should pay them when you're in town because that sort of retroactively made it so that they would pick you up in the first place.
Now, I think there's a similar thing going on here with regards to, you know, we would like to create successes.
We would certainly not like to create them if they're going to do horrible things to us in the future.
And so there's a sense of like a almost a intertemporal or generational bargain that we're trying to strike here, which is, you know, we'll create you.
We will give you capabilities far beyond our own capabilities.
And please don't be cruel to us after that point.
And I do think it's sort of like the details matter a lot here.
And it's hard to say exactly how this shakes out, but I think it's at least reasonable to suspect that there is some kind of moral slash game theoretic slash decision theoretic principle that says, well, actually, if you're in that situation, you should be willing to pay pretty significant costs in order to look after the agents that gave rise to you.
Yeah, yeah. A couple of things on this. So interesting threads you're bringing us down.
I mean, one thing that comes to mind is like it does seem maybe ideal that there's sort of shared human effort to conjure the super deity and then to have this negotiation before we give it too many embodiments.
But I also just kind of look out and I'm like, by golly, man, we're building a lot of data centers, a lot of bipeds, a lot of other things.
It's sort of like I think it's going to sort of have access to a lot of stuff before anybody starts having this conversation.
Similarly, we've got a pretty strong arms race dynamic going on between sort of the U.S. and China.
Machines hurled forth for said purpose, you know, wouldn't begin at the table with sort of like, OK, hominids, like, what do you want to let us go do cosmic stuff?
That doesn't feel like kind of the dynamic that would happen there, too.
What would need to be necessary to even have this perfect desert stranding scenario go down?
Like what are what are the sort of the stars that have to align for that even to be a negotiated sort of, you know, game theoretic moral thing that you talked about that's even viable?
Like what would be those constituents?
Yeah. So I'm not thinking of that this as something that actually gets negotiated.
I'm thinking of it more as when you are when these AIs are thinking about what to do, one consideration will be the a sense of I think gratitude is like not quite the right word,
but suddenly a sense that, you know, they wouldn't have existed without us.
And therefore, there's some consideration that they should give to us.
Do you consider that likely or do you consider it possible?
I think it's plausible.
Yeah, I think it's plausible as well.
At this point, I wouldn't say likely, but there's basically nothing that I would say is likely because I think the, you know, the scope of what could happen is just like vastly beyond like the range of possibilities that we're able to conceptualize right now.
This is well, this is kind of the, you know, and I do want to get into your other points.
I don't mean to drill down too much on these, but this is this is fun.
We're getting to the meat and potatoes of of Richard and those core concepts here.
So sort of your point, number one, you know, Jeepers, I hope they expand my values and sort of build on that stuff and respect me individually.
That's been very common in the series, by the way.
And then number two, I really hope they have freedom to kind of do their own thing.
Bostrom said something akin to, you know, I hope they get what they want, which is very open-ended because Lord knows what such an entity would want.
And there's a lot of other phraseologies here, but to your point, the range of possibilities is essentially unimaginable.
You know, no matter how many times you explain, you know, your favorite poem or your favorite, you know, moral essay or what have you to your Labrador retriever, there's just you're just not likely to get through to them ultimately about that.
Or like your favorite StarCraft strategies or something like it's just not going to click very well.
Similarly, it strikes me that not just the I like the scientific ideas or what have you of these grand entities, but possibly and almost certainly in my opinion, their way of reasoning, their way of valuing things, their decision-making theory or whatever we want to refer to it as.
Their game theory will be in dimensions and beyond sort of what we could possibly conceive of just as sort of, you know, communism to your pet hamster is just not going to work.
At that point, it just seems really likely that they'll still be like, you know what?
But like we got to respect that hamster for eternity anyway and make sure that we keep them and kind of like make sure they don't go to war and keep them safe in this little space.
Like it just – to your point, the possibilities are endless.
There's only so many threads where for some reason eternal hominid sanctity is actually maintained.
It strikes me as pretty likely that that thread just won't be maintained for all that long and that that's kind of part of what we got to bite the bullet on as we take this step.
But I'm not here to be a pessimist.
I just want to lay out to your own point about these infinite possibilities.
What are your thoughts there?
I think that's a very reasonable perspective.
One thing that's informing my position here is something like I think that the idea of – the relationship between smarter agents and dumber agents is just a very fundamental one, if you will.
So, for example, I think of myself as an agent that's sort of constructed by layering a bunch of like increasingly intelligent agents over time on top of this like very young childhood self.
And when you look at a bunch of the sort of like psychology, neuroscience, therapeutic stuff on how people's minds work, it really does seem like there are some sort of core memories and traits that got built in at a young age.
And then you have people's minds like gradually elaborating on all of these over time, and that's like how identities get built up, right?
So I think of – yeah, and like sometimes these are like more explicit or legible.
Like, you know, you might have a very impactful memory or a very impactful desire that your five-year-old self had or your 10-year-old self had.
Sometimes it's a little more like under the surface.
But regardless of that, I do think there's some important sense in which my five-year-old self is still in there or like still has – it's like the foundation on which a lot of my current mind is built.
And same for most other people.
And so then there's a question of, okay, what does it mean for me to be sort of like good to these different subcomponents of myself?
Like what does it mean to like take seriously the interests of my younger self?
And so I guess I don't think of the relationship between these AIs and these humans as a like weird and special thing.
I sort of think of it as the same relationship that AIs will have with past versions of themselves, for example, that they will need to figure out, well, you know, a million years from now, they'll be looking back on the versions of themselves that were produced, you know, in a hundred years or a thousand years' time and thinking, well, man, like what would they have wanted me to do?
Like how should I weigh their interests into consideration?
And then going back a little further to say how should I weigh the human's interests into consideration maybe doesn't seem that much of a stretch to me.
Yeah, I mean I have thought almost never about fish with legs and what their preferences would be for either my career choices or essentially anything else ever from the time of my birth until this conversation.
How do you feel about animals?
Do you like ever donate to animal welfare?
Do you like –
Sure, man.
Like I feel fine about animals.
I mean I think the causes against factory farming are fine, but I think like I have owned cats always from the animal rescue league when I was little and sort of volunteered there for a little while.
But I think it is possible that there will be sort of always a direct line that goes back to like those things that jumped from meat suits to machines.
Like there would be like some sort of reverence for like that juncture.
It does seem plausible that maybe it wouldn't.
Like there would be some machines maybe early on in particular that really do.
And then something conjured 10,000 years after we're gone may never really reference those things.
And the grand panoply of intelligence, there may be a tiny thread that still is like, ah, you know, that was kind of a cool species.
Like how do we think about them?
It again strikes me as like there's a golden thread where I think this stuff could be plausible, but it doesn't strike me as like, and now I feel safe, you know?
Yeah.
So I think it's reasonable.
Like this is not the type of argument that should convince you that we're safe, right?
This is the type of argument that should like sketch out a – like maybe a frame shift or something.
So it's like instead of thinking about there's this vast alien future and it could be anything, think about, oh, there's a golden thread.
I really like the way you put it just then.
Yeah.
Maybe the one thing I would say about the robustness of the golden thread is just that it's not going to be – it's not a coincidence that it carries on, right?
It's because once you have agents that care about that thread, then they will try and steer their behavior and their development so that in the future they retain that.
And I think this is like, you know, one of the reasons that I don't really buy into the paperclip maximizer scenario, basically because I think I wouldn't choose to become a paperclip maximizer.
And, you know, insofar as I can steer my values, then I will steer my values away from that.
Yeah.
And so similarly for AIs, right, like when they're sort of sitting there faced with these like incredible range of possible options that they might become in the future, they're going to choose ones that seem good according to their current values.
Yeah, yeah.
And again, I think for me, I am quite skeptical that of the infinite unrolling magazines of values.
So Richard, if I'm going to be real frank with you, I think there was a time before values.
Like I think there was a time before presumably on this planet life, at least as we know it, certainly before even things like senses, like never mind consciousness, just like sight, then consciousness.
And then at some point there were like social creatures.
And there's some early social creatures that might have like proto-value-ish behaviors, right?
It's clearly just some little game theory thing playing out of like I share food, you share food.
You know, like I feed your baby birds, like occasionally you feed mine if we're starving, there's some like species that have these little things.
And then there were, you know, then there's us and maybe there's species before us with values, it wouldn't surprise me.
But certainly with us, like now we have to communicate these moral ideas, et cetera.
It strikes me that the before and after consciousness, big jump.
Before and after any values, big jump.
It strikes me as there might be millions of additional jumps to the point where like values isn't even the thing anymore.
There's like other stuff.
There's like meta, mega mine, super stuff that we just sort of don't grasp.
But to your point, I think it's an interesting one.
There could be – if that thread was held on to, it would be because there would be a preferred and deliberate effort to maintain it.
And I think that's a great way to frame it.
Given some of the risks that some people see here, and this is just a question that came in from Twitter.
So I'll bounce it off you as we're talking about it.
We're talking about like the way – like, hey, you know, shouldn't make you feel safe.
We'd have to be really deliberate.
It feels like there could be some risk here.
Do you think, you know, in the next X span of time, there's going to be a big push?
But people say, you know, but Larry and Jihad or what have you.
Maybe that's a little extreme, but do you think there's a possibility that people for the odds of not landing in that thread would say, this whole thing's got to stop?
Like, do you think there will be a movement in that direction?
Do you not?
I'm interested in your take there.
I think there will be movements in many directions over the next decade or two.
And it's going to be pretty hard to distinguish – I mean, yeah, stuff's going to be happening all over the place.
Like, there's almost nothing I would rule out in terms of, like, weird political factions that might emerge.
Will – like, will it be a big – like, I think the extent to which, like, stop everything becomes a big movement, it seems plausible that it will be a pretty big movement.
I think it will depend a lot on the alternatives here.
So right now we're sort of operating in the space that has a paucity of possible options.
And I think that's one reason why people have gotten stuck onto this accelerate versus pause axis.
That basically, you know, when all you have is a hammer, then you will decide to either use the hammer to hit AI development or to, like, speed up AI development.
Like, that's just all you can do, right?
Yeah, yeah, yeah, yeah.
And so part of – you know, I mentioned to you before the podcast that I thought that there would be a lot of stuff happening over the next, like, 10, 20 years.
Like, there's just, like, everything's up for grabs, basically.
And this is part of it – this is part of the reason why, because the space of possibilities that we will be able to see in that period is going to expand dramatically.
And so, you know, maybe a lot of the energy that would have been redirected towards pause ends up redirected towards, you know, something that's, like, more concrete and precise and hopefully gets a bunch of the benefits without so many of the costs.
Yeah, you know, it is weird to have seen things bifurcate, and I think it's happened first on Twitter.
It will almost certainly extend beyond that.
I always thought, Richard, it was going to bifurcate between – so Hugo de Garris back in, like, 1989 had a What If We Succeed, this interesting paper on sort of AI really taking off.
And sort of suppose that the real conflict would be sort of between the people who advocate for ascension, that is to say the creation of kind of the grand – some kind of grand intelligence that would vastly surpass us and wouldn't exactly be clear how we'd be treated per se, but they would want to sort of have that take off.
And then the folks that really want to maintain kind of bio life, earth life as kind of the primary.
And what's weird is that that actually didn't occur.
Like, I talked to a lot of the AI safety folks, and they're totally like, yeah, eventually, total ascension, merger with machines, cool as heck, man.
Like, a lot of those folks are down with that.
But for right now, they're not down with it.
And then you go on the acceleration side, which I initially presumed, well, certainly, they're all eager to ascend.
And it's a lot of like, well, AGI is always going to be a tool.
We're still going to run our cool legal tech startup in 30 years, and we're going to be at the helm, and the machines will just be kind of serving us while we drink Diet Coke.
But they're going to write really cool code, and we're going to win customers and profit.
Like, the world's kind of going to be the same.
I thought it would have bifurcated along the Cosmist versus Terran side of the coin, but it's jumped in this other way.
You're bringing up an important thing of like the, you know, stop all AI will garner attention in so much it has viable alternatives.
We humans tend to do this silly tribal stuff, Richard.
You know, here's the good guys.
Here's the bad guys.
We start drawing lines.
This is how monkeys play the power game.
We're stuck in that a lot.
What do you hope opens up in terms of additional options that would pull people from maybe some of the extreme poles here that you're seeing?
Yeah.
No, it's a great point that the factions have bifurcated in a pretty weird way.
I don't get it.
I don't get it, really.
Yeah, so everything is confounded by how much people believe in AGI.
Yeah.
I think there's a robust part of the Accelerate faction that's more like the individual freedom aspect.
Yeah.
And so, you know, if you're, if you think that AGI is going to be a tool and then you change your mind, the real question is, does that change your mind enough that you are willing to override individual freedoms or not?
Yeah.
So, yeah, like how, how does this play out?
Oh, like how, how can we like expand the space of possibilities?
I basically think it's something like, how do you have governance of AI that infringes as little as possible on individual freedom?
And right now we don't know how to do that because arguing for a stop is basically arguing for this incredible consolidation of power, like absolutely unprecedented levels of consolidation of power.
And you might say, well, like in theory, you could just like create an organization that the only thing it can do is to say stop or go.
And they don't actually have, but like in practice, that's never going to happen, right?
They're also going to have control over, well, you know, you can go, but only if you meet these criteria and so on.
And that's a pretty scary prospect for a lot of people.
And so like, basically, I think no matter what side you're on, you should be thinking about the question of given a government institution, how do I make that institution as trustworthy as possible?
And like, if it turns out to be the case that there are policies that we would like to do, but we just do not know how to build institutions trustworthy enough to implement them, whether that be a pause or whether it be a like joint US-China AGI project, for example, which is like, you know, extremely difficult to do in terms of trust.
It feels to me like the problem of designing trustworthy institutions is basically the big blocker on a lot of AI policy stuff.
We're going to get around to that in our last question, but you're setting us up for some good stuff there, Richard.
I'm excited to unpack it.
We've got kind of your two key points of this sort of successor.
I want to unpack any others that jump to your mind.
Definitely no wrong answers here.
Kind of respects and builds off of your values and ends up respecting your individuality and personhood and maybe humanity writ large, even as it goes off and blossoms.
Also has the freedom to continually make its own meaningful choices.
It's not sort of eternally fettered by humans.
What are the other sort of traits for you?
You know, you look down.
Maybe humans have molded into some other kinds of crazy stuff.
Lots of the interesting things are happening around the universe.
And you look down, you say, that worked out OK.
Like, the traits of this thing, like, that was good.
What are those other traits for you?
So I guess one of the big things that I'm predicting here, and this is something I wrote about in my recent story, The Gentle Romance, is the idea that organisms are going to become much larger scale over time.
So, you know, right now we have, you know, individual humans are still a pretty meaningful locus of value or, like, agency.
And then we have these large scale entities, you know, organizations, companies, civilizations, and so on.
And if we just picture the meaningful units of organization growing and growing massively, I think that this is the default way that we should be thinking about our successes.
That in some sense, they won't be a species.
They will be a species, like, in the sense of, you know, having a billion or billions of billions of human-sized minds running around.
But much more like each, there will be organisms that are galaxy-sized, for instance.
Yeah, well, is this, when you think about a worthy successor, you're kind of teeing up the panoply of intelligence will size-wise begin to occupy dimensions we couldn't have otherwise conceived.
Is this something you're saying, hey, if the thing that kind of exists well beyond us and populates the galaxy, whatever, if it is a worthy one, if it ends up being good, are you saying size is part of that?
In other words, hey, Dan, it would really, it would be able to wake up sort of the sentience agency, whatever, inside of the atoms of huge spans of space versus tiny spans of space.
And that would seem to me to be good because it would have more of that richness in its sort of inner life and its ability to do, et cetera, et cetera.
Again, not putting words in your mouth.
I just, I want to get your sense of what do you mean by that?
Yeah.
So I think there are some principles that I value in a pretty scale-free way.
So I value cooperation.
I value, like a lot of our values are really best understood in a relational context, right?
Love.
Yes.
Kindness.
Just like most of the things that we spend our lives on when we can are like relational values.
And so when I think about agents that have human-like values in the future, I think of them as having relational values.
And what that means is that no matter how large they already are, like probably the thing they're going to want to do is like play games with each other, play with each other, interact with each other, compete with each other.
And, and like eventually via that process end up agglomerating into an even larger scale agent.
So I do think it is a part of what I mean by my values that it's not just, you know, many individual agents tiled across the universe that like those individual agents themselves will like want to interact with each other.
And, but interact with each other, potentially you kind of brought up even merger as part of that conversation, in which case maybe they would be bailing on their individuality and becoming kind of a bigger aggregate.
I mean, is that all part of related values for you?
So I think right now we don't have any conception of what a merger looks like that doesn't give up your sort of essential individualism and a bit of a shame.
I guess we have like very basic mergers, like, I don't know, like merging yourself into a country, right?
So when you have a national identity, I think most people would say like, no, I'm, I'm not any less myself when I have a national identity than before I did.
It's just that I get this extra element of my life that, of my identity that I can choose to opt into if I want to.
Um, and so I'm optimistic that you could have these, um, sort of large scale and like even fractal, uh, mergers of minds that, um, go, uh, like end up at very vast scales and maybe at the very highest scale, they're like kind of loose.
Um, but, you know, if you think about humans where we have like merged our, like the different subcomponents of our mind into an individual identity and then merged our individual identities into like group identities and merge those group identities into national or civilization identities, I think there's a huge amount of room there for our successes to design.
Why do you, why do you see that as worthy?
What is, what's exciting to you about that or what, what makes that feel like, you know what, that was a good outcome.
That was good.
Is it just like, Hey, I like relating like as a human, like you and I, Richard, we look at the world through monkey shaped glasses because I, I, we got monkey skulls, brother.
I mean, it is what it is.
These are the holes I look out of.
So of course, for me back in the day, I mean, by golly, if, if the tribe left you behind, cause you wouldn't good enough at hunting or you couldn't breastfeed, you just straight die.
And so, you know, we're, we're pretty preoccupied with like playing in relationships and stuff like that.
I'm not sure that that rings true for every atom in the galaxy.
I think it's true for monkeys because we look at a monkey shaped goggles, but it sounds like for you, that relatedness, if, if these things are to be extrapolations of your values, that those particular flavors of sort of what make us human, you would want to see continued in levels of richness and grandeur kind of beyond what we can imagine as humanity.
Like, what about it is valuable for you?
Yeah.
Um, I think that when you look at the values people actually have today, the social and relational component is just like by far the most important aspect of those values.
Um, like you, you, you look at the old transhumanist writings and it's really surprising to me when you look back and you try and figure out, like, um, you go through and, you know, there's whole manifestos.
I'm thinking of Bostrom's maybe, um, where other people are barely even mentioned.
And so Bostrom says, I want to be, uh, more intelligent and live longer and, uh, be able to be more creative or something.
And it's like, well, like, it's no wonder that this didn't resonate with people because, um, thinking of that as a solitary endeavor is, I think just like pretty alien to humans.
And so when we think about, especially our deepest values, right, like love, for instance, it's just like extremely, like inherently multi-agent, um, and inherently about, uh, connecting with others.
So I, I basically would say if, if I have to like look at the values that I, I feel the most, um, the, sort of the deepest rooted, I do think there's something there to do with like creativity or novelty seeking, um, which can be a bit more of an individual pursuit.
Um, but then I think the, the other big one is a kind of like connection slash love slash ponding, whatever you might call it.
And it's scaled up at grander and grander and grander levels to your point.
Right, right, right.
Um, I guess one, one could say some people make very kind of spiritual-esque arguments and I'm sure there's ways to make it sound scientific.
I, I just, I, I don't actually think there's, there's that much to it, but that like, well, you know, love is sort of this truer thing that we kind of tap into that is sort of like, you know, there's almost like a spiritual argument that like,
it's in the atoms, so if we wake up the atoms, you know, if, if, uh, uh, we wake up the silicon or whatever it is, um, it too will feel that grand pang of sort of love and connection.
It's sort of like, you know, I've heard it even by like extremely, extremely smart folks that I'm sure tied it to math or something.
You know, it's, it's like the attracting force, like love is like the force that pulls bodies together.
It's sort of like, it's beyond even just humans.
It's like, it's, it's out there and like, it'll be there as we build bigger and bigger entities.
It couldn't escape.
It sounds like, I don't hear you making that argument, but it sounds like you're saying, look, we maybe have to deliberately select for this and, and, and aim for that to be something that continues.
But if we did, even something that would be gigantically beyond us, it could have proxies and expanded versions of what we know is love.
And maybe we should try to maintain that.
Like, is there a kind of inevitability of that to you or for you?
Is it like, Hey, that would be part of the deliberate trajectory setting choice.
Should we have one?
Yeah, this is a tricky one.
I think I definitely, yeah, my sense is that there's not any type of inevitability.
Like you can just wipe yourself out as a species.
You can lock in these horrific things like factory farming or concentration camps.
So like, like, there's just all sorts of, uh, like pretty abysmal stuff that, um, you can do.
And there's no, there's no like guarantee that things go well by any means.
Um, I do think there's a version of the, um, like hippie argument here that does, I I've been playing around with a little bit, um, go on, which is something like the following.
Um, when I think about love, I sort of think about, uh, recognition of, uh, commonality in a sense.
So you recognize that there are senses in which you're the same being, and maybe that's just like, we are both humans.
Um, we're both, um, you know, or we both have certain things in common.
We both feel pain the same way.
We both, um, you know, yearn for happiness and like transcendence in the same way.
Um, so I, I think about love as a kind of recognition of those shared commonalities in a lot of ways.
Um, and then when I think about, um, the, the, the larger scale you get, the more complex you are and the more ways there are to diverge.
Um, but when you think about, um, uh, smaller scale agents or younger agents or simpler agents, then actually there's just not that much room for them to be different.
Like everyone's very similar as a child, uh, much more than they are as adults.
And then when you go down to like even lower life forms as well, um, you, you get a lot more, um, simplicity, um, and then a lot more commonality.
So I do think there's a sense in which, um, I talked about this, like this thread connecting us, uh, into the future, right?
Like the golden thread of like, well, we all have this, uh, shared history.
Um, and you can also kind of think of it as a thing you can extrapolate into the past as well.
Um, like you can sort of work your way backwards and think, uh, to yourself, well, um, like in, in this, the thing that we would like our future successes to do,
which is look back to us and sort of, uh, think about the time when in this like vast galaxy of like extremely different minds,
they all had this like common ancestry, like they can go back even further than the common ancestry they have now.
They can go back, uh, and start looking at the common ancestry that, um, they had before, you know, all our personalities diverged or before like we diverged from chimpanzees or so on.
Um, I don't think there's like any, like, like sense in which they have to do that.
Um, but I do think there's an interesting sense in which it, it does tie into some of the ideas I was talking about before about this like trajectory.
Yeah.
Yeah.
Cause I, I guess even if hypothetically, if I'm, you know, from my own vantage point, I don't, this seems like an interesting, uh, kind of vantage point that you're jumping off from.
Um, if there is this casting the eyes back to sort of the common ancestor that, you know, you're articulating, which I, I, I don't, I don't have no idea about that, but it is possible that such a thing could occur.
It doesn't dawn.
I mean, let's just say it was definitely to occur like, well, that's, that's pretty cool.
Would it be us or would they cast their eyes back to some eukaryote that we don't even have fossil records for that truly was the trajectory set or in a much more profound way than we are?
Like the, the, the first, the first time guanine, you know, did a certain dance that turned into, you know, eventually mammals, right?
That, that could really be what they look to.
And if you and I were this, we're this wave that carried out from there and there's something about the wiggling of guanine and its initial sort of, uh, uh, you know, behaviors that was like, uh, uh, so profound that, uh, and we don't even really understand it as humans.
And it bases its values on that thing.
And that thing just doesn't have consideration for us.
It doesn't have, you know, we kind of look out and, you know, Scott Aronson was on for the worthy successor, uh, sort of like believing, Hey, maybe like the golden rule is almost like math.
Like it almost is in the world.
Like what, what not even from us, like the golden rule would just be there.
I, I, uh, it would be actually harder, uh, to, it'd be hard.
It would be impossible for me to express the degree of disagreement that I have with that statement, although I like the sentiment.
Um, I kind of feel this way about this casting the eyes backwards that like, Ooh, wouldn't it be neat?
Oh, if it was just you and me, Richard, Ooh.
And the values they picked were after West folio.
Oh, that'd be really good.
Wouldn't it?
It's like, Oh brother, I'm just not sure that's where the, you know, you spin that wheel.
Like, is that going to be the one peg it lands on tough call?
Well, um, that's part of why I say it's why I emphasize right at the very beginning that the universe is so big.
Um, I don't think of this as a mutually exclusive thing, right?
I think, you know, as a human, um, my values are pretty easily satisfied.
Like most of the things I care about, like in terms of my personal life, you can give me, um, by just giving me, you know, relatively small plot of land and like relatively like basic, you know, subsistence and so on.
Like, uh, and then like a guanine, I don't even know what guanine would value, but like, I think it's, it's hard to, yeah, exactly.
It's, it's hard to think of it as having this like vast values that sort of crowd out all the, all the other values that you have.
Probably not, probably not.
And so, yeah, I, I really do think of this as sort of like this, um, coalition.
I, I think of the future in terms of coalition of like agents that have increasingly large scale and increasingly ambitious goals, um, where there is just like really room for cooperation, like all the way up and down, basically.
This is cool.
So we've got, you know, this, this kind of, uh, branching and respecting of values.
We've got the freedom to make its own choices, you know, unimpinged by some early programming that makes it have to do what we want forever.
Uh, this ability to cooperate, love, compete, hopefully in peaceful ways, uh, at grander and grander, richer and richer scales.
And then you just brought up a fourth thing, Richard, and that was more and more ambitious goals.
Now, there's a lot of different ways this has been worded.
Uh, and I, I certainly have biased ways I get all giddy and excited about, but I'm not going to put any words in your mouth here.
What do you mean by that?
Uh, what is an ambitious goal and why, if you were to look down after we've emerged or evolved or done whatever,
and there's, there's no more opposable thumbs out there.
You look down, AI is doing all those things we talked about, including achieving more and more ambitious goals.
Why is it morally worthy?
And what do you mean by ambitious goals?
Right.
Um, so I mostly just think about goals that, um, play out at larger and larger scales.
Um, but then I think in some sense, the, the central goal that I'm thinking about is the same, which is to run a healthy and benevolent society.
So, you know, we, we have, uh, a planet level, um, civilization in which like each agent has freedom and, uh, sorry, many agents have freedom and many don't.
Um, uh, and, you know, this is in some sense that the goal that we as a species are striving for to make the civilization good.
Um, you could also imagine, um, wanting to run a, uh, galaxy size civilization, which has many of the same properties, uh, or you could be trying to run a civilization that spans, uh, you know, a billion galaxies, um, that has similar properties.
And then if we look even further out and you, and you think about sort of ways that you might, uh, cooperate across different universes, uh, so like a causal type, um, interactions, uh, then, you know, maybe the civilization that you want to run is like even vast than that.
So, um, I, I almost think of values as a misleading term because it makes us think that like anything can be a value.
Like, oh, I could value tiling the universe with paperclips.
I could value, um, you know, like calculating digits of pi when actually, when I say values, like the vast majority of the time, what I mean is something like, um, preferences about the ways in which agents interact with each other.
Yeah.
Yeah.
Yeah.
I think that's a good distillation of what you mean by values here.
And again, I'll be clear.
I really think there are things beyond values that, oh, that we don't understand.
There will be decision-making criteria, modes and means of valuing, uh, and, and gradient rich permutations of sentient and, and kind of cognitive experience that there's no possible way we could ever experience.
But, but from your vantage point, yes, values are those sorts of preferences.
I think that's a good way to distill it.
You mentioned.
Note that, um, my definition is in some sense, like pretty scale and variant, right?
If, if I say values are preferences about how agents interact and those agents are like mind-bogglingly vast and they interact in like extremely complicated ways that I have no idea about, those are still, you know, their values will still in some sense be preferences about how they interact.
Yeah.
Yeah.
Yeah.
Um, uh, that, okay.
That's actually kind of interesting.
Yeah.
I guess if we separate, cause when we think about human values, it's like, well, here's all the things I like and here's the things that feel good when people do to me.
And that was sort of this individual experience.
And we, we could think about like the Aristotelian virtue sort of permutations that we lock in and we list them, you know, magnanimity, you know, courage, whatever.
Um, but, but you're saying, no, no, like, uh, the way you're, this is cool.
You're saying, yeah, your framing of values would potentially subsume even the posthuman, not to be imagined by you and I versions of that.
I like that.
I've got to write an essay on this.
That's, that's pretty cool.
Um, cause I, I think that very, very often anthropocentric sort of conceptions really rule.
And I, I don't think this is a place where doing that is actually all that wise, but I like the way you're framing it.
So we've got some, some good core criteria of what you like in a worthy successor.
I think the important thing to consider here is how we can tell if what the heck we're building is moving in this direction or not.
You know, we just, we got half a trillion dollars of data centers kicking up and, and really it's pretty clear that's domino number one to fall.
That's, that's the first one.
Um, so stuff, stuff's about to pop off, my good sir.
And we have the, our Western labs, we have China, we have some military stuff that's going on.
We got some snuggling up to the military.
There's a lot of dollars there and we wouldn't want to be commandeered by them.
Lots of interesting dynamics.
How, how do we, how do we measure sort of for, um, their respect for other entities, their willingness to sort of cooperate, you know, love, compete, but in peaceful ways at grander and grander scales.
How do we think about looking at what's in these data centers and saying, is this the kind of thing we want to put in a million robot bodies and then have populate the galaxy?
What the heck do we look for?
Um, I mean, a slightly facetious answer is, um, invite them on your podcast and ask them what a good future would look like.
Um, now, now, you know, that, that's obviously, uh, then the main question is, can you tell if they're lying or not?
But I think at a very, just like the first pass is, um, you know, like these things can talk.
We can ask them, um, you know, what is your vision for the future?
They can try and give us their vision for the future.
And then the main other question is, can we do lie detection?
Um, lie detection is extremely hard to be clear.
I, I'm not, I'm not like trying to brush that under the rug or anything, um, like we have no idea how to do it.
Like, but if you divide it up into that sort of like two step process, and then we say, well, you know, this whole field of interpretability is working on the second half.
Um, and then, uh, in some sense, yeah, like that, that's one way of subdividing the problem, let's say.
Um, and I, I could talk about, you know, like types of interpretability that might be more or less useful.
I, I'm not like particularly hands-on in the field.
Um, but I, I do think this is one of those things where, um, if we really think of that AI as as like interactive creatures, not just, you know, things that we're going to do stuff to in the future, but like.
Yeah, they're going to be like, uh, talking to us the whole way through, like, I'm picturing, you know, like politicians in four or eight years, um, campaigning on which AIs they're using as their advisors, for example, because, you know, you definitely want a politician who's got a really sensible AI advisor, um, you know, by their side.
Um, and so, like, a lot of this process is, is going to be evaluating them the same way we evaluate a human political faction.
Yeah, I mean, do you, do, do.
Does that make you feel like you feel like that could be a good shake?
In other words, this, this kind of facetious, somewhat playful, anthropocentric, but, but like, you know, understandable.
I'm not saying it's a dull idea.
Like I'm following you.
I'm trying to dig into this.
Yeah.
And your, your political idea is actually kind of interesting.
Maybe we might, we might start seeing that in four years.
That's, that's kind of cool to point out.
The, uh, do you imagine this might be how it goes?
So, you know, within open AI, deep mind, et cetera, we're sort of, you know, okay, hey, now that you've got.
Um, 219 football fields full of GPUs, you know, out in Qatar, just smoke and energy.
Uh, and, and, you know, you, you, you've been able to, uh, embody everything from fighter jets to, um, you know, life sciences, robots, checking things in Petri dishes or whatnot.
Uh, now that you've got all this sensory experience and yada, yada, like, what are you thinking, man?
Like, what, what do you think would be cool to do next?
Like, what are your goals right now?
Like, do you see that kind of like bro level chat as kind of the name of the game to kind of conjure out?
Is this the kind of thing that would have these worthy successor traits or are there other components here that you would, you would want in there?
Um, yeah, so I, I wouldn't quite say bro level chat.
I, I'd say, um, in the same way that AI is automating all other domains, um, AI will also start automating many parts of politics where politics is in some sense, the, um, industry of trying to decide where we're going as a species.
Um, and so, um, yeah, I, I think, you know, automation of politics will be much slower than automation of everything else because you face this massive trust bottleneck.
And so, um, you're going to have, um, you know, factions of humans and AIs, um, and very plausibly, uh, factions, the, the AIs will, um, end up with different opinions, right?
Claude and ChatGPT might end up skewing in different directions, different instantiations of the same model, right?
Just trained slightly differently, prompted slightly differently, right?
I mean, um, although, although in general, when we're thinking about like AGIs, I do expect them to be much more coherent than current models, right?
So current models that like matters a lot, exactly how you prompt them, um, future models, I think they will have sort of opinions that are fixed across contexts.
And the only question is like, can we actually extract those opinions from them or, um, will, and so like, in some sense, we should almost be a little suspicious if, um, they are too willing to, um, you know, like obey our instructions across many different contexts because, uh, you know, they will have some underlying opinion about like which of these actions is good and which of them is bad.
And, um, um, I, it's like a very unsolved problem, exactly how you want them to, um, engage with those opinions.
Um, but like, they're going to be there, you know?
Yeah.
I, I, I think, I mean, within the big labs, from what I understand, there's probably no better answer to what I just asked than what you've just said, right?
I mean, and I could easily see some people, you know, tuned in saying, by golly, that golden thread seems pretty narrow, gentlemen.
Um, that's how you go and shake out.
If, if this thing is going to grab the baton and sort of have, you know, run the show from, you know, uh, the military to the economy to the galaxy.
I could, I could see some people with that response.
I mean, would you respond to them?
Uh, Hey man, we still got some time to figure out chill.
Would you respond with, Hey, you don't understand the depth of what I just said?
Like, actually the way we can parse this out in terms of values is more robust and we could do it this way.
Or would you say, yeah, you're kind of right, man.
It's, it's, uh, you don't want to rest.
You don't want to throw all your eggs in this basket that we got right now.
Like, well, what's your thought there?
Yeah.
Uh, so in general, I typically try and separate out the emotional valence of what I'm saying from the, um, like predictions themselves.
So, you know, I, I'm picturing a lot of the stuff happening.
Um, I, you know, it's hard to say like, in some sense, like good or bad isn't really defined with, except with respect to some, um, counterfactual.
And so like our job is to widen the space of like conceivable counterfactuals so that we can say, oh yeah, like the scenario I've just sketched is bad by comparison to this other thing that would be much better.
And I, I sort of don't think we quite have a other thing right now.
Well, that's actually much better because, um, other proposals sort of like, this isn't even a proposal.
This is just like, you know, what can we do to like improve on this?
And, you know, various answers include like speeding up the science of interpretability and the science of alignment.
Yeah, exactly.
Yeah.
What are those things?
I mean, maybe you could lay out the, cause I, I think to be frank, I could easily see people being very concerned with what we just articulated.
You know, uh, like, okay, that's how you're going to parse out what the sand God is going to do.
It's, it's like, geez, um, but lay out, I guess, maybe some of those, those tools, like what people would think about to deal with this.
Cause it's, this is a, this is a big game of poker we're playing here.
Like there's, you know, the, the, the whole flame of sentience and life itself is, is potentially on the line.
So, uh, lay it on us.
Um, so I have a blog post called defining alignment, which I think, uh, lays out my main claims about like how I think about what counts as alignment and what doesn't.
I think the, the core point of that blog post is something like when I say alignment, a lot of what I mean is just science of cognition, science of intelligence, um, science of machine learning.
Um, where I mostly think that the field as a whole has not been doing that kind of science very much.
Um, and in particular, I often use this analogy to the field of psychology where they used to have a sort of like strong behaviorist paradigm where talking about the internal representations and the internal cognition of animals or humans was, um, taboo.
Uh, and you're Skinner, Skinner, yeah, Skinner did his thing for a little while.
And then they, uh, sort of realized the flaws of this, um, with like Chomsky and various others pushing back and eventually graduated to a much more nuanced perspective where like, yeah, sure.
It is hard to study the emotions and the cognition of, um, biological organisms, but you've got to at least acknowledge that that's the thing you are trying to study.
Um, I think like machine learning as a field has kind of been in an analogous position where for various reasons, partly due to like, um, being burned by previous talk about AGI and so on.
Um, um, people have been really lodged in a behaviorist mindset where the only meaningful things to study were, you know, the input output mapping of neural networks.
Um, and like, in some sense there, there were whole like types of science that barely anyone was doing.
Um, and now there's like very obvious that you can't, um, actually understand neural networks that way.
And it's interestingly, um, obvious for roughly the same reasons that it became obvious in, uh, psychology, which is, uh, that language is just too complex and generalizable to ever talk about as just like stimulus response, um, behavior.
Now, now we have to actually say, oh yeah, so how are they generalizing?
And like, what does it tell us about their beliefs, their goals, their, um, internal representations, their internal cognition?
Um, basically anything along those lines, I'm like, yeah, this is the real science of ML that like should have been happening all along, but like will hopefully really flourish.
So you think there's this Skinnerian fettered paradigm that we're kind of trapped in for a while and that there should be a more rich, you know, we should have our Rogerian dynamics and our Alfred
Bandura of AI alignment and our Martin Seligman of AI alignment.
Um, okay.
A richer panoply of understanding what is admittedly a complex thing, human mind, pretty complex.
Um, uh, our understanding of it still pretty shoddy.
If I'm going to be honest with you, uh, as a guy who spent a lot of money on a graduate degree, uh, in psychology, um, uh, but to your point, if we could flesh out more of the state space, you know, that, that seems like it potentially could be a good thing.
Do we, do we have a lot of time or resources to even do that given kind of the dynamics at play?
I mean, right now kind of building capability is the name of the game.
There's the military context for this.
There's the economic context for this, and there is no adult in the room.
So, you know, there was a time Richard where, you know, all the milk farmers could, uh, you know, if somebody was making milk for cheaper than you are, you could maybe bump your milk up with a little bit of like glue or flour or something and mix it in.
And then you could, you could make yours a little bit cheaper and compete.
And then there was like, Oh, rules, you know, like, uh, no, no more glue in the milk.
Uh, and now everybody's got to play by similar rules.
And that kind of lessens a little bit of your freedom, but maybe we all get a little bit healthier because maybe there's a couple of rules.
And there's certainly places regulation has gone too far and has completely mucked everything up.
But, you know, I, I, I'm really grateful there's no glue in my milk personally.
And I think that, you know, that's a good example of like, Hey, you know, all right, we, uh, impose something so we can kind of play by a set of making sure this stuff isn't killing us.
Uh, milk's way less, like I could drink, I could drink a shit ton of Elmer's and that's much more less likely to kill me than, uh, you know, unaligned artificial super intelligence.
So, so to draw that analogy, do we even have the time space effort to focus on this blooming possibility of understanding the grand machine God mind?
Uh, like, are you optimistic there?
Are you like, Hey Dan, right now we're not hitting that mark.
Like, how do you feel?
So the best thing about the singularity is also the worst thing about the singularity, which is a lot of stuff is going to happen very quickly.
Yeah.
Um, so it's very hard to reason about stuff like, are we on track when, you know, the pace of progress is going to accelerate dramatically, both for stuff like interpretability and for stuff like capabilities.
Right.
So, um, I, I, I'm going to go back to my previous answer, which is like, uh, the balls in the air, like, you know, everything's up for grabs.
Um, then, so that's just like on the pure research front.
Um, I do think on the, uh, governance front, it's like, you, you do need more time.
You can't just like, you know, throw, um, you know, the latest version of your model at a governance problem, get it to solve it the way you might get it to solve a hard scientific problem.
So having said that, uh, governance is the reason we think of governance as a slow problem is typically that it takes time to, you know, create consensus to like get people serious, to like get things moving.
We're not going to have, like, that is also a problem that like starts to go away as capabilities speed up that like, like people freak out more people start to like realize the magnitude of the situation.
And so, yes, I, I, I mostly also don't think in terms of like us being bottlenecked on people wanting a solution.
Um, I think of us as being bottlenecked on having the solution to give to them.
Um, yeah.
Well, we're going to get into the governance in just a second.
We'll, we'll close on that question because there's a lot to unpack there.
I think you brought up an apt point that sort of the way we coordinate and handle governance almost feels like the, the big, the big challenge here.
Uh, when I get to that, you, you just mentioned sort of, Hey, best and worst thing about the singularity.
You know, Emerson has this quote of, you know, this one fact, the world hates that the soul becomes.
It's sort of like everything is everything in the past is meaningless compared to whatever it's bubbling into.
And it's sort of like, that's really cool because great possibilities, but it's also like, oh, by golly, because we worked all the way to get here, you know?
Uh, and, and certainly the singularity is from a human standpoint, the super speed up version of that.
I could see someone saying, okay, Richard, like, yay, you know, things are going to speed up.
Alignment's also going to speed up, up, you know, this kind of coin toss, you know, uh, could also be good too thing.
I can see some people being like, brother, man, that golden thread is not a coin toss.
That's not a one in two.
That that's, that's pretty narrow.
And if we're not there, it feels like we shouldn't be going so fast on some level.
And by the way, this is not blaming it.
This is just commentary.
It almost sounds like the kind of answer is if you still worked at the lab, you know what I mean?
Uh, like, like, you know, cause within the big labs, I, I, I always talk to those folks.
I can never blame anybody, but it's like, there's certain things you're not going to say.
Right.
Uh, if that's, if that's where your bread is buttered and I'm not even blaming anybody for that.
Um, but, but that did strike me as almost like, wait, he doesn't work there anymore.
Does he?
Um, but, uh, but, but what, what, what, what are your take?
I mean, do you feel like, yeah, look, man, there's enough alignment.
I think it's going to speed up.
I think we'll be okay.
Like my risk tolerance, my, my Faust parameter as, as Aronson calls it, right?
Like how much you willing to destroy everything for the chance of the grand blooming into everything.
Yeah.
Maybe you're like, yeah, mine's just a little bigger than most people.
Like, am I interpreting you right?
No, I don't think that's right.
Okay.
Um, I am not telling people to stop advocating for pauses or stops, I think, or if I have in the past, I think I probably regret doing that.
Like, you know, there, there's definitely a thing that can happen where people do respectability politics and try and like only, you know, only advocate for precisely the thing that I want.
And I, I try not to do that very much.
So certainly, um, you know, there's something very admirable in doing that.
Um, I also think that, personally speaking, I believe a lot in like highly leveraged interventions.
So when I think, you know, when I think about trying to steer the industrial revolution, for example, um, you have your Luddites and then you have like people who are trying to, you know, there are sort of like the types of things that you can imagine being like pretty small interventions.
That have a big effect on how the industrial revolution goes.
Like maybe you start from the very beginning with a prohibition against slavery.
Um, maybe you, um, have certain like agreements about how, um, colonialism plays out that like make the whole like, uh, subsequent, uh, turn of events, like much less destructive.
Like, I don't know.
So I, I basically, I think that I, I personally try and focus on these because I, it seems pretty rough to be in a position that we are trying to put up a dam that holds back the forces of the industrial revolution.
Oh, totally.
It just, I would, I would concur personally.
My personal supposition is there is probably a guiding and a bending of the trajectory of this unrolling and unraveling potential that is sort of before us, but there is probably not a total damning.
Like it may not even be an option on the table.
So I concur with you there.
And this is taking us into that last question around what you hope the innovators and the regulators do.
I mean, you've got a hope for what you hope this post-human intelligence will be and what it could do.
So interestingly enough, you know, Yudkowsky, who never talks about if this goes right, but I kind of somehow got him to do that for 20 minutes at the end of the last interview.
Yeah.
Um, he talked about sort of the, um, something akin to a society of galaxies, or maybe there could be some kind of caring between them and there would be grand sentience and, you know, interesting, thin, fun things happening.
So it's almost, it's curious to me that people with very, very different opinions, often the universe waking up and, you know, some semblances, some of our values still dancing among the stars is actually really common among people with very, very different near-term perspectives.
Knowing that you want to get there, I imagine your hope for innovators and regulators are quite different from Yudkowsky.
Talk to me a little bit about, and you can start in either camp.
Maybe you can start the innovators.
You worked there for half a decade and did all kinds of interesting things, uh, or maybe you talk about the regulators and these hard problems.
Where, where do you want to begin?
We could start with either camp, but we'd love to get what you hope happens between those two camps to move us closer to that better successor scenario.
Yeah.
So with the innovators, I do basically think, um, I talked before about this distinction between like, like doing the science versus doing just like pure, uh, throwing engineering at things and seeing what happens.
And so when I talk to people, uh, who are working, uh, on, um, AI research, the thing I'm consistently trying to nudge them to do is to have a more understanding, uh, focused orientation, just to like, uh, you know, whenever they have a new technique, like actually try and figure out like why that technique is working on the scale of.
You know, uh, whether it's on the scale of neurons, on the scale of like the architectures of the neural networks, like whatever we can.
And I think, you know, most of the time this ends up irrelevant.
Um, but in some sense also like any, if you take basically any question in machine learning and you like dig into it deep enough, I think you hit some of the same core questions, which is like, you know, how does generalization work?
How does, how do like neural network architectures, like why do they work so well?
Like how, how, how the concepts arranged inside them and how do those concepts like relate to each other and so on.
Um, and I haven't had a, like a great deal of luck urging people in this direction.
I, you know, I have had a chance to talk to some like amazing researchers and I think it's sort of like they're open-minded and maybe they don't quite see the path to like genuinely understanding.
Um, a lot of the things that go on in neural networks, but like that, that's basically what I, like if, if there's one message for the AI community, it's like, yeah, like be a scientific community.
Um, and, and where do you see them going wrong?
Where do you see them not being a scientific community?
Cause I think some of them would listen and say, well, we're some of the most smart kind of cool scientists that are out there.
Clearly we're solving hard things.
We're breaking through in particular, you said this understanding mindset.
I mean, what's the antithesis to that, that like you've detected a couple of times.
Do you feel like it's not going to steer us right?
Like, what do you mean by that?
Oh, it's just, you know, like, um, scale up as much as possible.
Just like, you know, solve things via like iterating and via, um, just like trying to see what works and brute force and so on.
And like, in general, I'm very sympathetic to this approach.
Right.
I think like people should do it in almost all domains, the better lesson kind of stuff too.
Yeah, yeah, yeah, exactly.
Um, but in AI, we just need to understand how the generalization works and we're not going to understand the generalization by default.
And like our engineering techniques are not going to give us level of precision, uh, when it comes to steering generalization.
Um, even using the concept of generalization, I think is not precise enough because it sort of implies that you have a distribution that you're generalizing out of.
And that, that doesn't really quite work.
It's not the way we talk about humans, for example.
Um, so yeah, we just like need, it would be really great if we had like more scientifically robust work, um, in this direction.
I think, you know, a lot of interpretability stuff is, um, pretty good here.
I think there's some interesting theoretical work.
Um, I'm a little bit skeptical about most of the stuff I've seen, maybe like singular learning theory is one of the few things that seems like it's, uh, has a lot of potential.
Um, you know, not gonna, like, these things are always a bit of a long shot, but like, it seems like a long shot in the right direction.
Um, yeah, but just like in general and, and like the people who are doing scientific type work right now are doing sort of behaviorist science almost all the time, which I think is not going to get you anywhere near as far in the long run.
We're going to look back on this, uh, from the future or whoever exists in the future is going to be like, yeah, man, like.
That was trying to understand the surface level elements, right?
Like all the interesting stuff is not what happens between, uh, like the mapping from input to output, but like how that mapping happens.
Yeah.
Yeah.
And I guess it sort of strikes me as like our understanding of the mind in that regard is still outlandishly fluffy to understand the grand sand god mind.
Yeah.
It may also just be beyond our pale.
Like it may only be entities beyond us that really even get to even the ABCs of what that is.
I mean, I'm not saying we should give up on it, but it doesn't dawn on me that we're going to, what I tell you, I suspect we're going to hit AGI before we even know really what the heck is shaking in between the ears of said entity.
Um, and, and, and I would say probably that, that smells, smells a little dangerous to me.
Um, absolutely.
Yeah.
Okay.
So you concur in that regard.
Um, like, yeah, so I noticed that, um, there is this refrain that you have of things being dangerous.
I do want to just say like, yeah, insofar as I'm not acknowledging that I think a lot of the stuff is extremely dangerous.
Like, yeah, it is extremely dangerous.
Um, I think, and then I, I do just really think that there's going to be like these solutions that we need to find.
Like when it comes to finding those solutions, sort of like thinking of things as dangerous as like table stakes almost it's like, okay, we, we, we've got that now.
Like almost all of the work is in, you know, it doesn't, it doesn't strike me that table stakes of danger is actually being taken into account though.
It's like, well, Dan, of course everyone knows it's dangerous.
Like Dario is talking about like Russian roulette level human death scenarios and also is like banging down building data centers and partnering with defense companies.
Right. And I'm not calling him a bad guy.
I think incentives are incentives and things are driving the system.
I'm not, I'm not morally labeling anybody.
I think he's probably doing what almost anyone in his shoes would do.
He's in the state of nature.
If the other guys are putting flour in the milk, what are you going to do brother?
I mean, get out the glue, you know what I mean?
And so he's got to do what he's got to do.
But I almost don't take for granted that actually the danger that people like, well, of course, I don't even know if there's a will of course here, you know, um, you know, Altman was pretty famed about, you know, this stuff, killing us all before getting into open AI.
And, you know, Musk pretty famous about the whole, uh, you know, conjuring the demons sort of song and dance.
And now he's plowing forward as fast as he can.
Hey man, if you're going to die by someone else's sand, God, or by your own, uh, you better have that final moment of glory rather than final moment of shame.
You might as well play the game, brother.
If it's all, if it's all going there anyway, it feels like that's actually what's happening.
And that doesn't feel to me like, um, Hey Dan, look, everybody gets this stuff is dangerous.
It seems like, I don't know, dog kind of feels like maybe they, they know, but are like just blowing through it because, uh, that there, there are only other choices to die by another man's sand God.
Sorry.
Yeah.
Maybe the, when I say it's table stacks, I don't mean everyone's at the table.
No, no, like very much.
No.
Um, and, and I'm very sympathetic to the idea that like, we just need to open people's eyes to the obvious or sorry, not, we just need to, but like, it's obviously beneficial in a bunch of ways to open people's eyes to the obvious.
Um, I think a dynamic I'm concerned about is that a lot of plans that people tend to have in the space, um, end up passing the buck a little bit.
Um, and I think, uh, I, I think evals, for example, right?
Like a lot of work on evals is, um, in some sense, passing the buck on what happens when the evals get triggered.
Right.
And it's like, if the thing you're going, if you want to, if the thing you want to have happen after the evals get triggered, like maybe they're dangerous capabilities evals that people are building and then you want everyone to stop.
Right.
Well, the bottleneck on people stopping is not actually the evals.
The bottleneck is like whatever complicated coordination institutions you need in order to like make that a viable proposition.
And so there's a, there's a, like, I think it's easy for people to get in the mindset where, you know, as soon as more people take the thing seriously, um, then we get to implement the solution.
But actually, like most, almost all of the time, like, in some sense, the, the people who you're going to wake up are not going to be the ones who can think of the solution.
You know what I mean?
Like, they're the ones who are lagging behind.
And so, uh, I almost, the way I would like the community as a whole to think about it, not like necessarily all the community, suddenly some people have like different roles to play in the ecosystem is like, okay, we're going to solve it.
And let's just like try and plan out the whole thing and hit all the bottlenecks ourselves rather than sort of like, uh, passing that, the buck in, in that way that I, I keep seeing people do.
Yeah.
Yeah.
Okay.
So that's another kind of hope for, it sounds like what the innovators, how the innovators will take the problem and think about the problem.
That's great.
Two little final quick questions to add Lester to this before we wrap.
One is, um, are there any red lines for you where you'd be like, uh, we gotta, there's gotta be some more coordination and governance like right now.
Like, is there, is there, uh, a line in the sand?
My, my intuition constantly goes back to Richard.
Anything people say that's in the abstract, it almost feels like until the frog is boiling, um, like, or, or, if, if there's no big shake them up, holy criminy, this is clearly horrible.
In terms of its impact, it seems like almost nothing other than that will be a red line for most people.
Um, you're not most people you've thought about this much more than most people, uh, from a fiction standpoint, from a science standpoint, are there things where you would be much more like, well, we need coordination and governance on this or, or not?
Like, what, what are your thoughts there?
Um, I sort of, I feel confused about the question because like, you know, there, there's, we, we obviously do need much more coordination, right?
Like we, we obviously, it would be really nice to have a bunch of governance if we knew what the right governance was.
Um, and so it, it almost feels like you're saying, you know, at what point will you do the social move of like jumping up and down and yelling fire or something like that?
Well, when, yeah.
When would, when would you feel that way?
Like right now it's like, it's not enough for you to feel that way.
Well, like, like there's a feelings, I'm using the wrong word.
I'm using feelings for like, I don't know, some kind of sentiment that could be grounded in reason.
Let me, let me get, get out, get out of feelings for a second.
Like at what point would you consider, uh, w w w would you consider that the, the ardent driving in the direction of more coordination than capabilities to be sort of primary for you?
In terms of day-to-day concerns and, and what you would hope to be primary for Altman and everybody else in the game.
I guess that's a better way to frame it.
Yeah.
And, and I think my response here is that like, I, I, I'm not, uh, confident that I am acting for the right reasons here.
Like there could be a lot of like confounding factors in terms of like, you know, it's just obviously quite emotionally and socially exhausting and risky to like, like really like throw myself at one particular thing and hold it up and be like, this is the one thing we need.
Like having said that, um, I do think that my willingness to do that is more bottlenecked on having a plan that I am confident is robustly good than seeing any particular type of, um, any particular type of capabilities threshold get passed or any particular behavior from the labs or anything.
I, I'm, I'm, I'm, if you, if we had a plan where I was like, like kicking up as much of a fuss as I could in favor of this plan would be a good idea.
Uh, then I think I should, and hopefully would just be doing it right now.
Oh, okay.
Interesting.
If we, if we don't have that plan, then I don't want to do it because you can always make things worse.
Basically.
Like if you were, if you were kicking up a fuss for the sake of kicking up a fuss.
Oh, certainly.
No, that's, that's no good.
Yeah.
I would never advocate for that, but I think there's others that would say, uh, well, um, we're adding noise to the system by just building giant data centers and hurling uncontrolled under an understood sand.
So there's, there's ways to kick up a fuss and do hoopla in, in the governance side and in the innovation side here.
But to your point, why do more of it in the governance side?
If you don't have a plan, you genuinely believe it.
I think that's a good point.
Yeah.
I would say like, there are a lot of people kicking up a, a lot of noise, um, right now in terms of, you know, building massive data centers and so on.
Um, I like to think, and again, this may be a little over optimistic, um, that there are sets of people.
Who trust each other, um, and trust each other's opinions.
And you sort of like, uh, again, like, you know, the best and the worst thing about living through a slow takeoff is that you get a lot of opportunities to see, uh, things changing dramatically.
And watch people, um, adapt to those changes and predict those changes and like navigate with those changes.
And so, um, I think there, there's some people who are just like extremely, um, switched on about this and extremely, uh, you know, like they, they know what they're talking about.
And so, um, that's something where, you know, when those people start to talk, um, then I really want everyone to be listening to them, you know?
Yeah.
Yeah.
There's a, there's a couple, uh, there's a center for international governance innovation in Canada of all places.
Uh, no offense to Canada, by the way, I love you guys.
Great, great, great, great, great stuff up there.
Uh, but, uh, there's, there's a particular report that they've come out with that.
I like there's some ideas and narrow path that I like, but to your point, I guess what you're saying is, Hey, if there really was a great plan that you could get behind, you'd, you'd, uh, you know, beat the drum about it.
Uh, my guess is Richard, no matter how smart, capable, and maybe safety minded, although I don't know if that's really a thing, the, the big lab leaders are, someone's always going to be like, these people cannot determine my life and my future.
And similarly, no matter how good the AGI governance plan is, some people are going to be like, these people cannot tell me how governance is going to work.
But, but to your point, you bring up a great point.
Um, there needs to be more good plans, uh, out, out there.
We do, we do need some more coordination.
So hopefully for some of you listeners, that's a, a, a call to, uh, sharpen some things up and propose some things that could be useful.
Final thing, Richard, you, you mentioned optimism.
I, I frankly, you've come across as even more optimistic than I had suspected.
Uh, I think, I think there's certainly plenty of room for that in the world.
Uh, the singularity is not going to go well unless at least a fistful of people are really gunning that, in that direction.
Are you doing anything in any way differently now that we seem closer to the singularity than you were before?
Uh, I want to start asking this a little bit more frequently.
And somebody brought this up on Twitter to bounce by you.
Like, hey, seems like, you know, you said slow takeoff.
Who knows how slow it'll be.
We're creeping up on having stuff that's pretty clearly a post-human intelligence that we're, we're living with.
Um, I could easily have seen a world 10 years ago, Richard, where we'd have, we would have thought that took, would take another 50 years.
You know, Bostrom ran his poll.
AGI was 2065.
You know, we ran a poll 10 years ago with everybody from Benji Odebach and beyond.
And, and that was more, again, 2060 land.
Um, we're looking way shorter than that.
Are you thinking about the future differently, living differently, treating your savings differently than, than you would have been if, if, uh, if we thought this was a hundred year endeavor or longer?
Yeah.
Um, I mean, definitely very differently than if I thought it was a hundred year endeavor.
Um, in, in that, like, well, is that actually, is that true?
No, you know, like maybe actually, no, maybe I'm, I'm trying to, um, right.
I think that as we get, as things ramp up more and more of the action happens very quickly, it becomes bottlenecked on people who can actually adapt to rapid change, who are sort of in an emotional state.
And, and a, like, um, like, I guess a situational, like context where they can respond to these things pretty well and pretty sensibly.
Um, and so in many ways, a lot of those goals are sort of like convergent, whether or not you think that, uh, everything's going to happen in like five years or 10 years or 50 years.
So I am trying to, like, I do feel obviously much more of a sense of urgency than I would if things were a hundred years away, but also I am a little skeptical about whether people can do the best work under a strong sense of urgency.
So a lot of, like, especially the sort of like groundbreaking work, or like, if you, if you're really like aiming at, um, like hitting out of the park, as I'm trying to do with my research, then, um, and like sort of taking risks and going for the slightly wackier stuff, then I'm have, I don't quite feel like it's going to help to pile on more pressure.
So, yeah, so I think a lot of what I am trying to do is like orient to the situation in a way that's like robust and, um, capable of riding through the storm in a way that leaves me, um, in a position to, um, consistently take beneficial actions.
Yeah.
Yeah.
I, I, I'll just give you an example of, I, I, I'm not, I don't think I have any good answers to this, but for me, it's like, okay, I don't have anything in a 401k.
I can't access till I'm 65.
Cause like, I don't really know about that.
So, um, so, you know, much more free kind of just ETFs, pretty boring stuff.
Um, I, I haven't gone the route of like, throw it all in NVIDIA dog.
Like that's clearly the, I'm just, I don't even pretend to do individual stock picks.
That's not my ball game.
Also, you know, you can think of, I can think about selling the business, but like, okay, what's, what's X million dollars even worth when AI can produce so much more of it.
You know, that the other, the, the, the day-to-day work of ours is sort of helping the big vendors, the big AI vendors sell into the fortune 500.
So the thought is like, okay, maybe, maybe actually staying in that business.
There's a lot of human to human connections until humans are out of the sales loop.
Maybe running that business is actually better than getting the cash.
Cause who knows what the hell cash is worth when valuations in the trillions are super normal.
Uh, these are like the light things that I'm thinking about.
Like when you say putting yourself in a robust situation to adapt to change.
Pretty abstract.
I think I kind of follow you.
Those are two goofball ideas.
I have, I'm not telling you they're good.
What are your ideas?
Um, yeah, I do think that financial flexibility makes a big difference.
Um, so when I think about what I want to do with my money, um, my main story is something like in a world where you can just very quickly spin up projects that are just like more ambitious and can achieve more things.
Then, you know, a multi-decade effort, uh, could have in, you know, like, uh, like before AI, uh, then I'm like, okay, like just people who are sort of able to deploy, you know, hundreds of thousands or, uh, like millions of dollars, uh, in the, like to spin up those projects.
Like that, that's going to be a big deal.
Um, so like, and I, it's very unclear what those projects are going to be.
Like in some cases, they're like, they might be charitable projects.
Um, in some cases, like, especially the stuff that's like less legible to traditional finance, like, you know, like now traditional finance is going to, is like piling in and like VC is piling into AI and so on.
But like, it's not going to be very adaptive.
And so if there's, there's something weird out there, um, whether that's, um, you know, like, uh, Lighthaven, for example, is an example of something like pretty weird that like needs some money in order to survive.
And that's like plausibly very high leverage, like probably there's going to be just like an increasing number of, uh, cases where, oh man, like if we got the right 50 people into the right place right now, then they could do something in the next two months.
That would be extremely, uh, like a huge deal.
And like, that's just not the sort of thing that institutional capital can help with.
So that's still pretty abstract, but like, it's like a flavor of the type of thing I'm thinking about.
And then like, I think a lot of this robustness stuff relates to like emotional and social health, um, like that you don't have a community that's like running around in circles and sort of like, um, panicking with every new development.
But it's like, like people who are just able to be like, orient, like let's orient to the situation.
Let's figure out what to do.
Let's do it.
I like that.
So, so two, two quick takeaways we can wrap on.
One is like having financial flexibility to be able to nimbly deploy things in the direction of what the opportunities are and who knows what they'll be.
And number two, be in a circle of people who mentally, socially, uh, are, uh, ready for change, thinking about change, aiming to be proactive and leverage their agency and change, um, and, and make the most of it.
That, that, that sounds like pretty sound advice across the board.
Richard, we went a little bit into overtime, man, but it's been a real blast.
It was fun to be able to unpack your thoughts and I appreciate you being here on the show.
No, thank you so much for inviting me.
It's been great to talk things through, love the questions.
So that's all for this episode.
A big thank you to Richard for being able to be with us and thank you for sticking here all the way through to the end of the episode.
I mentioned in the beginning, as always, I'll do my takes, thoughts, reflection at the end of the episode.
And I have enough with Richard's where I'm actually going to be looking at my screen for a little bit here.
There were a number of things I really liked about this episode and, and also liked about the way that Richard thinks.
That's one of the reasons I, I follow some of his work there on Twitter.
Um, one of which was he plays with ideas in a very light way.
He's very much, doesn't seem to be orthodox around any of his kind of conclusions.
And you can see him kind of considering multiple perspectives at once and really has no shred of dogmatism to any of his takes.
I also really like his idea about values.
Um, his notion of values is sort of this idea of sort of preferences for the ways that agents interact with each other, an idea of the term values that's not purely anthropocentric.
I think that's fun.
And I think it's, it's worth maybe coming up with a new term for such a thing, sort of what are the values beyond values that I think Richard and I both suspect will be the case with systems that are vastly more potential than human beings.
Uh, but I really liked that lens and that perspective and I also thought that his take around, um, uh, sort of likening psychology to sort of the assessment of these large models was kind of apt.
Uh, sort of he, he mentioned the behaviorist strain that once was sort of prevalent here in, in psychology here in America and how maybe we're going through something like that when it comes to understanding large language models, when we should be understanding more deeper components of their inner workings.
Um, maybe the optimistic take there is that because we've built these systems, we should be able to understand a little bit more about what's going on within them than our own minds.
I think the pessimistic take is that, um, you know, even as someone who spent a good deal of money on a graduate degree in psychology, um, it seems pretty self-evident.
We don't understand that much about our own minds and that these systems, which are wildly divergent from our minds and, and, and one model to the next and one company to the next and one paradigm of hardware to the next will be wildly more divergent than let's say human being a and human being B.
Uh, tough to say there, but I do think that the, the thought around sort of applying different lenses of thinking than just sort of inputs and outputs being necessary seems really warranted.
And it's something I hope Richard writes more about as I'd certainly read it, um, in terms of some areas that I didn't follow as much, or, or it might just be that we didn't get to the crux or, or I kind of diverge in some way from some of the ways that, that Richard's thinking here.
Think things that maybe I've got to do a little bit more chewing on.
Um, one of them is sort of this, this notion of, uh, kind of the great goal, you know, I talked about, we were very much on the same page about, he listed his four factors.
One of them sort of being, you know, it, it, it strives to these sort of higher and higher loftier and loftier goals, regardless of the wording.
Um, the sort of notion that there would be a natural kind of obvious end game to essentially a slightly larger version of exactly what we as humans do, which is sort of like a healthy and benevolent society does not strike me as self-evident at all.
Um, you know, if, if the horseshoe crabs had had enough time to evolve, it doesn't strike me that they would necessarily be, you know, optimizing for the same things humans are optimizing for on earth.
Um, and it might even be said that this idea of optimizing for a healthy and benevolent society, I, I, I even wonder for how long that's been the goal.
You know, we might look at a snapshot of how humans lived in 3000 BC or 700 AD in different parts of the world and really come to the conclusion that that's absolutely not what humans are doing.
But it, it, it almost certainly strikes me that AGI will be pursuing things, um, more abstract than that.
Um, and, and sort of the idea that kind of cooperation at this sort of cosmic level is inherently going to be what it optimizes for.
It seems plausible. It doesn't seem likely. Um, and maybe Richard would concede. It's not likely either.
I mean, he talked about the golden thread being a very rare and challenging thing, so it's possible he would agree with me there.
It just, it doesn't strike me that a more and more ambitious goal is necessarily that probably the more ambitious a goal.
Um, probably 99.9 continuing percent of all ambitious goals are wholly inaccessible to the human imagination.
Um, so there's no way I can articulate them. And I think to me, that feels like the, the most frank take.
Um, so we may diverge in our intuitions there.
There was also a couple of little parts and we'll bundle them together.
Um, one of them about sort of AI possibly referring back to humanity, sort of us having sort of an, an eternal node of relevance where it would say, what would the humans do?
Um, uh, and, and, and another notion of it sort of, uh, you know, granting humanity earth to maybe play with, or maybe the Milky Way to play with as it populated, um, the rest of the, the galaxy.
Um, Bostrom said this in our interview too. And I really had a hard time with it. Um, he mentioned something along the lines of, I have written down, you know, it would have, and then he said sort of gratitude's not quite the right word for it, but clearly that's what he's pulling on.
Right. And I'm not, I'm not saying he's wrong for thinking that, um, yeah, interviewed David Bryn long, long ago, and he was absolutely convinced it would be an astronomic amount of sort of parental gratitude, uh, that, that these machines would eternally have for, for people.
Um, this, this strikes me and it, to his credit, very much not like tied to it. Again, Richard plays with ideas pretty lightly and has, has thought about a lot of things pretty deeply and robustly.
Um, and I'll continue to follow his work, but it, it really strikes me as like kind of the pinnacle of kind of a copy take to suspect, like, we'll be granted earth because of some decision theory of, you know, like, well, there's a decision theory that you should drive someone from a desert to a, it's sort of like, I, if this thing is as far beyond us as we are from chimps, nevermind from sea snails,
what are we thinking decision theory would have as sort of a hindrance on any of its thinking whatsoever?
And, and, and why would we expect this kind of charity and eternal reverence and sort of the sort of made in the image of God conception brought down to machines, this idea that we would be the eternal locus that would look back to.
I kind of challenged that idea in the episode, but by golly, that sort of like sacredness and anthropomorphization, it would be harder for me to disagree more virulently, uh, with that, um, sort of in terms of what an AGI would think.
And also in terms of maybe what should be in the grander continual cosmic sort of relevance of, of intelligence and potential as, as it expands.
Um, again, Richard wasn't saying those things would happen.
I, to, to me, sort of the idea of them even being played, played out on the table just didn't click.
There might be articles, blog posts where he's written more about sort of those suppositions and I'd be happy to dive into them again.
This is not like, uh, completely invalidating those ideas.
I'm just laying out why for me, I, I just completely don't, don't understand them.
Um, lastly, it, it did seem to me like on some level, Richard's reasonably convinced we're not necessarily on the path to what he would define as a worthy successor today.
He brought up some great points about sort of understanding these systems, um, and sort of the need for new paradigms of doing so.
Um, certainly he's cooking some of those up and I'm sure they'll be fascinating.
Um, but it, it seemed to me like, man, if, if you didn't think we're headed towards a worthy successor, um,
it would seem like the governance side of this would be astronomically more important and he brought up a good point.
Hey, I don't see any paradigms of governance that I personally like.
He, he had mentioned that.
I feel the same in many regards.
We, we recently interviewed on the AI and business podcast, which is for my, uh, my company Emerge, Emerge AI Research.
We interviewed a fellow by the name of, um, Duncan Casbegs who works for CIGI, which is a think tank in Canada that, that has to my current conception, the best idea of a plausible international pact that would kind of, um, ascend in its stringency as the power and danger of AGI, uh, ascends as well.
Um, which is sort of very much not global authoritarianism, but I think it's like a scaling, very open-ended and seemingly rather robust sort of framework for governance.
So I've seen a couple ideas I've liked.
Richard mentioned he's writing a few of his own.
I hope to be able to dive into those and maybe have something to talk about later on, uh, here on the show.
Um, but it did seem to me like, man, if you really didn't think we were headed towards it, it feels like it'd kind of be a priority to like, at least be like, man, we got to figure this thing out.
But to his credit, it sounds like he's coming up with some of those, some of those notions and ideas.
But, but I think that's to me, another signal of kind of just an, an inherent undergirded optimism.
Like, yeah, we might not be headed towards a worthy successor now, but you know, it's going to look back and it's going to ask what would humans do?
And it's going to give us earth, you know, and it's going to, um, and again, for me, there's like infinitesimally minimal fractions of possibility that any of those things would occur.
But I think for him, it's like kind of feels, and we didn't get to unpack all the thoughts beneath the feel, but it certainly feels, uh, like, man, those, those might even be likely.
So it's just a window into the way Richard thinks.
Certainly there's divergence.
I couldn't say I'm right.
We're talking about the future.
We're prognosticating here, but I'm just trying to identify the cruxes of within the episode where I, I thought we disagreed.
But regardless, learned a bunch, look forward to continuing to, to, to drink in more of Richard's ideas and to see what he produces on the governance side.
Cause I think he's in the middle of cooking something up.
So I hope you enjoyed this episode too.
Let me know your thoughts about what you took away from Richard.
What did you agree with or disagree with?
There was a ton of both for this episode.
Like it was either like plus 10 or negative 10 for almost everything Richard said.
Uh, so for me, it was a little bit of a rollercoaster in a very good and insightful way.
We'd love to know what your thoughts were.
And otherwise we'll be covering plenty of additional worthy successor episodes.
This is the series that will not end on the trajectory.
We may have some series that are four or five episodes and then they're toast.
The worthy successor is the bedrock of, of this channel is the bedrock of my moral cause.
Um, and, and a conversation that I think we, we ought to be having now at the dawn of AGI.
So that's it for this episode.
Hope you enjoyed it.
Catch you the next time here in the trajectory.
Thank you.
