The way in which the data estate has changed in the last 10 years has been pretty radical.
The way in which people manage and store their data has changed dramatically.
However, the way in which we manage that estate hasn't changed almost at all.
And that's really where Monte Carlo, that's the reason that we were created,
is to help organizations that now need to manage this very complex system.
Think of it almost as like a car manufacturing, right?
You have all these different pieces that you need to put together.
How do you know that the car is safe to drive?
To put this in simple words, most organizations just build the car, drive it at 120 miles per hour,
and then they learn that the door flies out or something, right?
That's really how we've been treating reliability of data.
What Monte Carlo does is connect to your systems across your data estate and learn what your data looks like.
Help you, first and foremost, detect issues.
We can talk about what sort of issues can come up and what does that look like.
But first and foremost, allow data and AI teams to be the first to know about data and AI issues.
And then the second is help them triage and resolve those issues.
What does the future hold for business?
Ask nine experts and get 10 answers.
Bull market, bear market, rates rising or falling, inflation going up or down.
Can somebody please invent a crystal ball?
Until then, over 40,000 enterprises have future-proofed their business with NetSuite by Oracle,
the number one cloud ERP, bringing accounting, financial management, inventory, HR into one fluid platform.
With one unified business management suite, there's one source of truth,
giving you the visibility and control you need to make quick decisions.
With real-time insights and forecasting, you're peering into the future with actionable data.
If I were a larger organization, this is the product I'd use.
Whether your company is earning millions or even hundreds of millions,
NetSuite helps you respond to immediate challenges and seize your biggest opportunities.
Speaking of opportunities, download the CFO's Guide to AI and Machine Learning at netsuite.com slash ionai.
That's netsuite, N-E-T-S-U-I-T-E dot com slash ionai, E-Y-E-O-N-A-I all run together
to get the CFO's Guide to AI and Machine Learning.
The guide is free to you at netsuite.com slash ionai, netsuite.com slash ionai.
Thanks for having me, Craig.
I'm excited to chat with you.
My name is Barb Moses.
I'm the co-founder and CEO of a company called Monte Carlo.
We're the creator and leader of a category called data and AI observability.
We started the company about five years ago.
We're honored to partner and serve more than 400 enterprises for various industries.
So these think about companies in gaming, media, retail, fintech, technology, financial services, etc.
Each of them have lots of data teams, whether it's data analysts, data engineers, machine learning engineers, AI teams, and they're all working to create data products.
A data product can be a report that an executive is looking at.
It can be a specific dashboard they're using to drive decisions.
It can be an internal product, or it can be an external product that a customer is using, or it can be a generative AI application.
In each of those instances, data teams are responsible to make sure that those data products are reliable and based on trusted, accurate data.
That's what Monte Carlo does.
We help organizations use data by making sure that that data is of high quality and high trust.
Yeah, I had a guy on not too long ago, and they had a product called Data Refinery.
And I know that there are a lot of other companies doing similar things because the data that we're talking about for training or fine-tuning AI models is largely unstructured.
And organizations have just tons of unstructured data in various forms, and these guys take that unstructured data and then transform it into something that's machine-readable.
Do you guys do that as well?
We, no.
So we don't do that.
What we would do in that situation, let me take a little bit of a step back first.
You know, I think in the last sort of decade or so, most of the work was done with structured data.
And so monitoring or observing the reliability of structured data is something that is critical, and we do that very well.
And, you know, just as an example for the things that you want to look at, you want to make sure that the data, you know, the field level is accurate.
You want to make sure that the data arrives on time, et cetera.
You can go into more details into what that means.
I think now there's sort of a rise with unstructured data, to your point.
I think companies or sort of enterprises are earlier in their journey for that.
And one of the, you know, important things that folks are doing right now is trying to understand how to monitor and observe the quality of unstructured data.
That's a very difficult problem.
One of the things where we sort of, where we can help is it is common for, we've seen instances where our customers have structured, unstructured data, perhaps like in the example that you suggested,
and then used observability to make sure that that data is accurate.
So maybe let me give a practical example to bring this to life a little bit.
You know, we work with a Fortune 500 insurance company.
And in insurance companies, there's a lot of support tickets or support conversations that happen.
And so, you know, support conversations with end users and support service teams are recorded.
And those are basically unstructured data.
And so, what this company did was used LLMs to actually score or grade those conversations on a scale from zero to 10,
where 10 was, you know, the sentiment was very positive of the conversation, and zero, the sentiment was very negative.
And so that allowed them to have a view of, you know, here are the positive conversations, here are the negative conversations,
and sort of do an analysis of that, where Monte Carlo, or where data and AI observability helps in this case is actually observing the output of the LLMs.
So imagine that, you know, you got a score of 13 for a conversation.
What does that mean?
Or a score of negative two.
How do you interpret that?
When the scale is zero to 10, those outlier values don't mean much.
And so actually having observability to make sure that the outputs make sense in this case,
how you can start to unpack, what does it mean to monitor unstructured data?
Does that example sort of make sense?
Yeah.
Help make it real?
And first of all, why Monte Carlo is, did you use Monte Carlo Tree Search or something like that?
Great question.
We actually do not.
It goes back, it traces back to sort of the origins of starting the company.
You know, we're data people and like to work with, you know, like proud of ourselves to be working with the best data teams in the world.
And so we were looking for a name that would resonate with data teams and, you know, be perhaps a bit catchy.
And, you know, I think we have some Monte Carlo fans at our company in terms of, you know, the Formula One racing.
Oh, that Monte Carlo, yeah.
Yeah.
Yeah.
And so those are some of the fun origins, if you will.
So it's both, you know, something that data team, data and AI teams know and find familiar, but also sort of a nod to the team.
Yeah, yeah.
And so this, it's a platform, is it a SaaS, is it a service or is it a tool that people subscribe and then can use?
How does it work?
Good question.
So by and large, it's a SaaS platform.
And, you know, the most common way in which organizations work with Monte Carlo is observing their data estate.
And so what do I mean by that?
You know, the way that organizations today process, ingest, generate, create data is through a plethora of systems.
And if you think about, you know, folks migrating to the cloud, you know, a very common stack, if you will, could include, you know, sort of upsource systems, or sorry, upstream source systems.
You might have a data lake house, data warehouse, like Snowflake, Databricks, AWS, Azure, GCP.
You probably have orchestration solutions, like DBT, Airflow, and then you probably have, you know, either an AI model, or you might have a BI solution, like Looker or Tableau, so that you can imagine sort of this pretty complex data estate.
Now, you know, the whole sort of premise of Monte Carlo is, you know, the way in which the data estate has changed in the last 10 years has been pretty radical.
The way in which people manage and store their data has changed dramatically.
Like, you know, those solutions that I just mentioned really grew in popularity in the last 5 to 10 years.
However, the way in which we manage that estate hasn't changed almost at all.
And that's really where Monte Carlo, that's the reason that we were created, is to help organizations that now need to manage this very complex system.
Think of it almost as like a car manufacturing, right?
Right.
You have all these different pieces that you need to put together.
How do you know that the car is safe to drive?
Yes.
You know, most, to put this in simple words, you know, most organizations just build the car, drive it at 120 miles per hour, and then they learn that, like, something, you know, the door,
you know, flies out or something, right?
That's really how we've been treating reliability of data.
What Monte Carlo does is connect to your systems across your data state, learn what your data looks like, help you, first and foremost, detect issues.
We can talk about what sort of issues can come up and what does that look like.
But first and foremost, allow data and AI teams to be the first to know about data and AI issues.
And then the second is help them triage and resolve those issues.
Because the reality is that even if you're sort of inundated with alerts about things breaking, if you don't have a good strategy for how to do root cause, for how to do triage and resolution, your team is going to be wasting a lot of time on that.
And so we think about sort of, you know, both of these things as two sides of the coin.
Both of them are very hard to do well.
And, yeah, does that make sense in terms of what we do?
How do we go into more depth?
Yeah.
Yeah.
No, and do you use, and is this primarily looking at data for AI models or is it looking at any enterprise data?
Yeah, great question.
So, you know, I think before sort of this AI craze, if you will, most organizations were really thinking about, you know, their data estate and not so much about AI.
I think today you can't think about one versus the other.
And so I think any efforts to distinguish between the two are probably a mistake.
And I'll explain why I think, you know, if you think about this like two years ago or so, I think there was really sort of this, and there still is sort of competition in terms of, you know, what's going to be the best, latest and greatest LLM, right?
And what's going to be the most important sort of foundation model in the reality?
And you're seeing this, you know, with OpenAI and Meta and Anthropic and, you know, with DeepSeek, we sort of appended the whole thing.
And, you know, for folks who don't know, DeepSeek is a company that's sort of allowed to run these models at significantly lower costs, like orders of magnitude lower and basically sort of, you know, put into question the whole reason of, you know, how to build these things at scale.
And I think what we are seeing is that every single company has access to the latest and greatest model.
And so, you know, every single organization can, you know, use OpenAI or Anthropic or any other solution.
And so in a sense, that doesn't actually present itself as a competitive advantage.
Because if everyone can do it, like everyone has access to, you know, the, you know, the, whatever the best 2,000 PhDs can create.
And then, so the question is, what is my competitive advantage or what is my moat?
And I think what we are seeing in working with companies is that the moat is really the data that's powering those models.
Because, and I'll just give a particular example, Intuit is a company that has a financial, sort of AI-driven financial assistant, which basically, you know, takes, is based on RAG pipelines.
And, you know, takes information about, you know, what kind of car Craig has and where does he live and all this information about Craig.
And then surfaces back to you recommendations, tax advice recommendations.
And so if, you know, the quality of that AI product is only as good as the quality of the data that it relies on.
And so if I have the wrong information about you, Craig, I might be surfacing the wrong information.
And that is terrible.
I can talk about, I can talk about some disasters that companies have had by relying on wrong data.
So actually, I think UnitedHealthcare in 2024 denied some claims based on wrong data.
Yeah.
And so, you know, there's significant sort of societal and brand, if you will, implications for getting the data wrong.
And so I think what we're seeing is that a lot of organizations are now rushing to get their data right, sort of be AI-ready, if you will, so that the data that's powering their AI applications can be reliable and trusted.
We really sort of more than, it's actually, you know, just, I'll share another anecdote.
We recently did a survey with data leaders and we found that a hundred percent of them feel pressure to build AI applications.
And 90, I think 96 or 97% of them actually do, which is great, you know, I guess folks are up for the challenge.
However, only two-thirds of them actually think their data is AI-ready or that their data can be trusted.
And so there's this tension where people really want to build with AI, are putting the resources behind that, but don't have the foundational data or the systems to serve that data in a way that makes their AI products reliable and trusted.
And because of that, I think we're seeing more and more companies working hard to make sure that their data is actually accurate and reliable and talk a little bit about the reasons for why data goes wrong.
I think that's like one of the key things that folks are trying to wrap their mind around, because it's actually not, it's not straightforward to try to understand how to observe your data and AI estate.
But let me just pause there.
Yeah, well, yeah, I want to hear that.
Is your platform or your system, is it a generative AI platform?
I mean, do you have a conversational interface that then, you know, looks at the data and can answer, or is it more of an IT solution than that?
Yeah, I mean, I would say, so it's a, that's a great question.
So we definitely have AI features, if you will.
So as an example, you know, we, when we start working with a data team, the first thing that they want to do is understand what their data looks like and understand what they should monitor.
And, you know, what they should monitor could be, for example, like, let's say if we're working with the sports team or a baseball team, and they're collecting information about, you know, player stats, for example.
And so they want to set up monitors about, you know, the speed at which the ball is thrown, or any sort of statistics about the players, as an example.
And so actually, like, coming up with the list of monitors for that is quite hard.
And so what we do is we use AI to come up with a recommendation for which monitors you should have.
And so we actually analyze sort of the data, profile the data, and suggest that, make recommendations, basically.
That's one example.
Another example is, you know, we allow data teams to basically write in natural language what sort of monitors they want.
We use AI to help them create those.
We also use AI to help them basically understand correlations between different things that go wrong.
So what I mean by that, if someone has sort of a, you know, detects sort of a data issue or an alert, they want to understand the root cause.
The root cause can be actually an amalgamation of a few different things.
So we think of the things that can go wrong as the data, the systems, the code, and the models.
So the data, you sort of mentioned, you know, we have both structured data and unstructured data.
And, you know, an issue in the data could be, like, just a problematic data source that sent a bunch of nulls, for example.
Systems, you know, it's your model APIs, your orchestrations.
We sort of talked about that before, and the problem can be, like, just, you know, a job failed, as an example.
So that's data, systems, code.
Code is basically sort of, you know, what engineers write to transform the data.
And you could have an issue with the code that happens all the time.
You know, if someone makes a change of the code, has a mistake in the code.
And then the fourth is the models.
And so, you know, you could have an outdated version of a model.
The model might not be served on time.
Or there might be inconsistency in the results of the model.
You could have the same prompt sharing a different result each time.
Or maybe the prompt is poorly written and doesn't have all the contextual information to provide a good answer.
And so, you know, when there's a data issue, the root cause can be traced to a failure in the data, the systems, the code, or the models.
And oftentimes, it's actually all of the above together at once.
And when a data team needs to decipher what, you know, what went wrong and why, that's where observability can help.
And so, we use AI to help explain what went wrong and why.
So, we help correlate between the issues.
You know, there's an orchestration job that failed.
There's also a schema change that happened at the same time.
There's a pull request that's related to those.
And there's a problem with a third-party data source.
And those three things sort of resulted in this model failure or in this report with the wrong data.
So, we use AI for that as well.
So, yeah.
You mentioned monitors a few times.
I'm not a data person.
So, what do you mean by monitors?
Yeah, great question.
So, monitors can be monitors of all of, you know, there's a few types of monitors.
Maybe the most basic type of monitor could be, you know, I receive data from Craig every morning at 8 a.m.
And I didn't receive data this morning.
I don't know why.
You know, in a sort of real-life example, this might be, I receive information from LinkedIn every morning at 8 a.m.
And this morning at 8 a.m., I didn't get information.
The API failed.
So, you can set up a monitor that automatically tells you, let me know if the data did not arrive at 8 a.m.
Another example of a monitor can be, you know, I typically get from LinkedIn at 8 a.m., a thousand rows in my table, a thousand rows of data.
And if I'm getting only 20 rows or if I'm getting 20,000 rows, I need to know because something's weird.
And so, you might set up a monitor that tells you, you know, maybe the volume of the data here is abnormally low or abnormally high.
And I'll just give a final example.
You know, let's say I'm tracking, you know, I'm tracking, you know, what kind of toppings I have in a pizza.
And suddenly, I get chocolate.
What sort of a topping is that?
And so, you know, I might want to set up a monitor that says pizza toppings can only include these five values.
And if I get anything else like that, then please let me know that must be an error.
I cannot have chocolate on my pizza.
That would be a disaster.
So, those are types of monitors.
Does that make sense?
Happy to give more examples.
Sure.
Yeah.
No, does data observability also go into measuring the bias of the data set in different ways?
That's a wonderful question.
You know, that's something that we're asking ourselves.
We haven't built that at Monte Carlo today.
That's not, it's definitely something that I think is going to be very, very important for a variety of reasons.
We haven't built it to date, but perhaps in the future.
Yeah.
So, it's a SaaS offering and enterprise connects to your platform.
And then does the platform automatically scan your systems and identify data sources?
Or do you have to list them?
Great question.
So, we would typically want or would have the data team point us to their data warehouse or their data lake, their data orchestration, and sort of, you know, the various solutions in their data state.
And those can be many and varied.
And, you know, we would typically want the data or AI team to direct us to that, and we would make the connection.
The other thing that we would typically recommend is to start with a particular use case that you know matters.
So, for example, you know, you don't want to monitor, or I'll explain why, historically, data and AI teams have done a pretty good job of hoarding data.
And so, there's a lot of stuff out there.
Now, you know, whenever you put monitoring or something on observability or something, there's a risk of alert fatigue.
And so, if there's some data set that I really don't care about, perhaps I don't need monitor for it.
But if there's a data set, you know, that's being used every single day by the product teams or by my customers, something that's externally facing, you know, for example, we work with regulated industries where, you know, think about like banks or trading platforms.
You know, you can't afford a stock number, to be wrong, for, you know, for a very long time or even for a short amount of time.
And so, that is data you need to monitor very, very carefully.
You know, on the other hand, if there's data that you're using maybe once a quarter, maybe it's okay, you know, that the frequency of monitoring is less than the severity of the monitoring.
So, you know, we like to work with data teams on understanding what's the most important data that you have.
What's the data that's really, you know, you're relying on it.
It's mission critical to your business.
Customers are using that data.
You know, this could be financial data, it could be marketing data, it could be customer platform data.
All this, you know, more and more organizations are identifying these critical use cases.
And so, we recommend, you know, to start with those or to have a strong sense of visibility and monitoring into those for sure to start out with.
Yeah, and you've written a book about data.
What's the focus of the book?
Is it this observability question or is it broader than that?
Yeah, it's a good question.
So, yeah, we write a book about sort of data quality and data observability.
I think there's sort of an interplay between data quality and data observability.
I think, you know, historically, the problem with data quality has been around for a very long time.
You know, it's been around 20, 30 years.
I do think the meaning of it has changed dramatically today because 20, 30 years ago, you really could get away with bad data.
Very few people, very few organizations were actually using data.
And if they were using data, it was very intermittently in small pockets of the organization, you know, maybe just to, you know, report numbers to the street once a quarter, for example.
But today's world, if you think about the types of companies, you know, just take an example.
Pilot is a company that has sort of manages truck stops and fuel information and needs real time on information on, you know, where are trucks?
What are the, you know, what's the fuel data, et cetera?
And so in those instances, you can't afford to look at the data, the quality of the data only once a quarter.
And so more and more companies are using data in a way that's very different than what it was five to 10 years ago.
And so the definition of data quality needs to change as well.
And so we really think of data observability as the next evolution of data quality.
Data quality often stops at detection of issues.
Data observability helps you take it from detection to resolution and triage or mediation of issues.
Data is fascinating because since I've been paying attention, which is only since about 2017, there's just so much data that's being created in every organization.
As you said, people didn't think about it a lot before because it wasn't considered that valuable.
But now that you have AI that feeds on data, people are starting to store it and analyze it.
And how many different kinds of storage, you know, S3 buckets and, you know, relational databases and these, all these,
is that sort of surface expanding with increasing numbers of data storage solutions and things like that?
And can you connect to all of them?
Yeah, you're actually spot on.
And so one of the things that we've seen happen in recent years is this explosion of these technologies.
And so oftentimes companies will have multiple from each of those categories that you mentioned.
For different purposes, you know, one can be for analytical data, one can be for real-time data, one can be for data science and AI use cases.
So there's actually, you know, legitimate reasons for why you would want to have different ways to store and process data.
Now, as there's been this explosion of these solutions, it's really hard to make sure that the data is accurate and reliable in each one of those.
And so you're spot on, you know, the idea of data and AI observability or what Monte Carlo does is connect to each of those and help teams make sure that the data in each of those is accurate, reliable.
And by that, I mean, all of the data is there.
The data is actually the right data.
It's arriving on time.
The volume of the data is what you expected.
The connections between the data are the right data.
Its output is the right output.
So there's sort of a lot that goes into making sure the data is trusted, sort of a variety of different techniques to make sure it's trusted.
But you're right.
It's a lot harder for organizations to do it now that data is spread across a very different number of integrations.
So, you know, Monte Carlo, we have over 50 or so integrations that we connect to where our team would deploy Monte Carlo.
Within 24 hours or so, we'd start seeing lineage.
So lineage means that we automatically parse the connections between the tables and the fields to understand, you know, which, how is data flowing?
And once we have that lineage, which is basically like a, sort of think of it as like a graph of connections or like a tree of connections, then we can start understanding, okay, if there's an issue with this particular data set, what are all the different areas that are impacted?
And if there's a problem here, what is the upstream root cause of those?
And so the first thing we do is sort of have that picture of lineage.
And then after that, work with the teams to help make sure that there's the, you know, the accurate sort of monitors on that data.
Do you, have you expanded into, so this is kind of DevOps, right?
Great question.
It's actually, it's, you're spot on, it's very much, we did not invent this, we're, you know, trying not to reinvent the wheel.
We're taking what works for DevOps, which is exactly right, observability for software engineering, and that's super well understood concept.
We've applied it to data and AI teams, so you're totally right, it's all of these concepts are concepts that are in DevOps, we are adapting them for data and AI, and our belief is that just like every single engineering team in the world has something to make sure that their software solutions and applications are reliable and trusted.
So there's a concept of uptime, right, like is my application up 99.999% of the time, we believe that in the same way, every single data and AI team needs to make sure that their data products, data and AI products are reliable, are trusted, are powered by the right data, et cetera.
Does observe, is observability, I mean, I imagine it is related to explainability, and there's, you know, a lot of work on explainability because a lot of enterprises are prohibited for regulatory reasons or whatever from using AI if they can't explain its outputs, or the logic behind the outputs, do you move into that space at all?
That's a wonderful question, because explainability is so important, and when I think about most data leaders, data and AI leaders that I speak with, their number one, you know, sort of mission for the year is often something like modernize our architecture and make data more easily accessible to more people in the organization.
And that's very hard to do if the data is not explainable, and so, you know, I hear that come up a lot, sort of the question of explainability, and that's something that a lot of data teams are spending time on.
We specifically don't focus on explainability.
I think it's something that our platform can certainly help with explainability in terms of understanding, again, what are the, you know, if there's a particular table that you're looking at, what are all the different sources of that table?
What is this table feeding?
What is this table feeding?
And so, we can help with explainability, but it's not our focus.
I will say that driving explainability of data when the data is wrong is probably the worst thing you can do.
So, and that is sort of a mistake that we do see folks do is, you know, they'll try to drive adoption of data, but when the data is inaccurate, people will default to not using it or not trusting it.
And then there's, you know, the brand of the data and AI team is tarnished, right?
Because they're basically supplying data to users who don't trust the data and there's finger pointing and they'll complain and say, hey, the data looks wrong.
And, you know, my dashboard is blank or, you know, the model output doesn't make sense.
And in all of these cases, the finger is always pointed towards the data or AI team.
And so, that is something that's quite destructive in an organization.
So, we will typically help with first making sure that the data can be used, is trusted, is reliable, and then also, you know, organizations will take that a step further and make sure that it's explainable, if that makes sense.
Yeah.
Are you, you know, you created this category of data observability.
How do you see the principles of data reliability and trust evolving to address the challenges of, you know, a probabilistic technology like generative AI, where you have hallucinations and you're trying to maintain transparency in the inputs so that
people can see if there's a problem in the outputs, or even if not explainability, at least be able to identify if there's a problem where it originated?
Yeah.
A hundred percent.
And maybe this is a good time to talk a little bit about what folks are doing today.
I think, especially with, you mentioned hallucinations and other things, you know, most organizations in those instances are defaulting to sort of human evaluations, where, you know, you can have like an end user, like thumbs up or thumbs down or sort of score a certain response.
I think those types of evaluations are 100% a critical part of making sure that your model is performing as you expected, and mostly that it meets the user's needs and expectations, if that makes sense.
I am finding that it's a necessary but not sufficient solution.
And I say that because it's very hard to do things like gaining visibility into the problems that we talked about across data systems, code and models in a way that's manual and relying on human intervention.
And especially when there's both structured and unstructured data, sort of having automation to do that is a lot easier.
It's also, you know, the advantage of having sort of automated or AI-driven observability is that you can actually both monitor and troubleshoot changes across your entire AI value chain.
And so oftentimes an end user will only interact with the output, you know, of a certain prompt, certain model, but they don't actually have visibility to everything upstream.
And oftentimes the problems do occur upstream.
And so having, you know, some way to monitor in a more holistic way is very important.
And again, you know, you sort of mentioned, I think detecting the issues or understanding that the data, that the, you know, that the output looks a little off is maybe only half of it.
The other half that I think is going to be really important is being able to tie, you know, the quality of the output from a model back to the root cause that you need to address.
And that could be because there's something wrong with the model or it could be something wrong with the data itself.
You know, it could be that your, you know, the training data is stale.
There's so many other reasons for why there might be hallucination.
You really need to have this full visibility into your data systems code and models to make sure that you minimize those as much as possible.
Yeah.
Yeah.
Yeah.
And the, how is this presented to users?
Is it a graphical interface with charts and alerts and that sort of thing?
Or is it a text interface that's, you know, giving insights into the data?
I mean, how, how, what does it look like for someone using the SaaS product?
Yeah.
Um, great question.
So the most common, um, workflow, if you will, um, is for a data engineer to receive an alert, oftentimes through a solution like Slack.
Um, or it can be a solution, um, like, uh, GRR ServiceNow, uh, or PragerDuty, where basically they're, they're receiving an alert.
Hey, something's wrong.
So they might configure it such that if it's a, you know, SEV0, the utmost priority or utmost criticality type of alert, they'll go directly to PragerDuty.
Um, and the first sort of information that they get is what type of alert this is.
So for example, the alert might be, you know, there's a particular data set that you expect to arrive at 8 AM.
It's now 8 01 and the data is in here, for example, or the alert that you might receive might be, you know, um, we suddenly got a field that says chocolate for your pizza toppings.
Um, something off, come take a look at it because this is weird.
So the first interaction is actually getting that alert.
And then typically what folks would do, you know, they would start to triage it.
So they would click into the alert and then they would go into the platform.
And then that platform and the platform, what you can see is basically sort of an investigation page with all the various information in one place.
So you can see, you know, this is what, um, you can basically double click into the specific alert.
So you can see this is what the data looked like historically.
And today this is what we're detecting that's off.
So for example, you know, every single day, the data has arrived at 8 AM and just this morning, it doesn't.
But then if you look, you know, a week ago, you can actually identify that, you know, once a month, the data is off, for example.
Um, and so in that instance, you might say, oh, okay, this is just seasonality.
And then our model actually can adjust and learn to that and say, once a month, there's a dip and the data arrives at 9 AM, for example.
Um, and so basically the idea is through the platform, you have various information.
You can profile the data.
You can, um, have different cuts of the data to understand what it looks like.
And that allows you to better understand, you know, what, what sort of their, their root cause.
So we might surface information to you about, you know, um, uh, anything that's sort of upstream of that that may have changed.
So basically correlated changes that happened around the same time.
And so, you know, the team member might, um, uh, you know, learn that there's a specific problem and then they would go ahead and fix that problem.
So that's sort of the most common workflow.
There's another workflow, which you mentioned sort of around dashboards.
A dashboard can give you a more holistic view and it can give you that at the data product level.
So what do I mean by that?
You can actually construct a data product in, in the, in the platform and say, these are all the assets that contribute to a particular data product.
Like for example, I'll, I'll use something domain based.
Um, you know, this is a data product that helps our marketing team set up a campaign, um, a particular campaign that they're running right now.
And so I need to monitor this entire data product and I want to look at the health of the data product overall.
And so a dashboard might tell you, you know, the data has been fresh, the data has been up to date, the jobs have been running correctly.
Um, you know, the SLAs are tight, sort of all the, all the different things that kind of the, the data product level to give a sense of uptime of the data product, if that makes sense.
So that sort of gives it a higher, a higher level overview.
And then for an executive, you can actually see a summarized view of that.
So you can say, you know, my organization has more than a hundred data products.
How many of them is, are green, you know?
Okay.
80% of them are green.
Great.
Can we get to 90% of them to be in next quarter?
What does that look like?
And so data executives are starting to sort of manage, you know, the health of their data products and have the sort of visibility and also measure a success or the health of their team in working through the incidents.
So one of the most important measurements is time to detection.
How quickly do we know about issues and then time to resolution?
How quickly have we resolved issues?
And so data teams often measure themselves on that.
You know, for example, it takes us, you know, an hour on average to, to know about issues.
And it takes us, we've improved in the last year from 20 hours down to three hours to resolve data issue.
That operational muscle is very important for customers to build.
And, you know, just starting by having a baseline is amazing for, for many teams because they haven't been measuring this and they've been hit by, you know, their, their customers or their CEO or whoever it is to make sure that the data is trusted.
And they don't have a way to communicate about how they're improving the data.
So hopefully that gives you a sense of, you know, various workflows.
Yeah. So I imagine, I'm guessing there's sort of two phases to using Monte Carlo.
One, you, you, you first want to, uh, get a handle on, on the health of your data.
So you, you identify all the data sources and you point, uh, Monte Carlo to them.
And then it presumably doesn't give a report on the, on, uh, issues that it finds in the data.
And then you spend some time correcting those.
And then the other phase would just be the monitoring phase where you've got this, the, the, the, the data connected to Monte Carlo.
And it's just watching your, your daily, uh, flow and identifying problems is, is that a fair assessment?
Yes. Yeah.
And how long does it take to, to get the, to guys on?
Well, obviously it depends on how much data you have.
But, uh, when, when you're working with a large organization is, is ensuring that you have, that your data sets are healthy.
Is that a process that takes, uh, weeks or months or, you know, for a, for a fortune 500 organization?
Great question.
Um, so, uh, the, you know, as a company, we've invested a lot in trying to make it as fast as possible for our customers.
Um, you know, we know that, um, or I think that the days of, you know, signing up for a SaaS solution and then waiting for nine to 12 months until you see value.
I don't think, I don't think that works anymore.
I don't think, I don't think you can pull that off today, um, as a vendor.
And so as a company, we've invested a lot in being very easy to work with.
And that means being very easy to get started with also, you know, from a legal and commercial perspective.
Um, and so our model makes very easy for organization to start, but also from a product and deployment perspective.
So we invest a lot of time, we have a full engineering team dedicated just to making sure enterprises can get up and running very quickly.
Because if you're not deployed, you can't see any value.
So our goal and, you know, our, our team is incentivized only to make sure that the customer is deployed as fast as possible.
You can start using the solution.
And so we, we invest a lot of resources in that.
And so in the simplest instance, you know, for a small organization, that could be a matter of, you know, a couple of hours to just get integrated.
Um, and start seeing value very quickly, maybe even within a day, right?
That would sort of be on, on one end.
I think for larger organizations, oftentimes, you know, they have security requirements and DevOps requirements and various permissions and authorizations that they need to go through.
And so our job is to help shepherd them through that, to help them navigate that in the best way possible.
And, you know, we typically try to do everything we can to make it as easy as possible, you know, to hold ourselves to the highest standards across all those areas and make it easy for enterprises to work with us in terms of the certifications that we have.
And, um, you know, our, our, our documented processes and policies to do that.
Um, and so for an enterprise, you know, it's not, again, it's not uncommon to, um, spend a couple hours, a couple of days to get connected.
And then within a week or so, you can already start, um, rolling this out to your team.
So again, you know, most of the, um, most of the work from a product development perspective on our end has been to reduce the amount of time on that.
Um, there is a component from the, the customer sort of enterprise side.
And I will just mention, you know, there's also one of the things, you know, sort of mentioned, you know, this is a new category.
And so, um, it's not like data teams have been doing this for the, in the past, uh, rather they are, um, developing a new muscle.
And so there's actually a fair amount of thought that needs to go into what is our strategy.
I talked a little bit about it, but what's the data that we care about?
What are the integrations that we should start with?
What's the data that's most important?
What's our alert strategy?
Who should handle the alerts?
Who takes the pager duty alert?
What happens when they do that?
Who should they go to?
Um, and so all of that stuff is, is things that we help operationalize.
And so we have, you know, really strong customer success and, and partners that we work with to make sure that customers are, are really, really successful in that strategy.
So I don't want to estimate, underestimate the importance of that part.
Yeah.
Um, and you were saying this is new, uh, and, and we're in this.
And we're in this period of history where data is exploding and the importance of data is, is coming to the fore.
How, what do people do without a solution like this?
Great question.
So, um, prior, you know, I want to say 10 years ago, they were just off the hook.
Like, I'm just going to be honest.
Nobody cared.
Right.
Um, because the stakes weren't that high for data, you know, um, at the most basic level companies had to count the number of users that they had or the number of customers that they had.
And honestly, companies still struggle to do that today.
So I'm not sure that, you know, we've made a ton of progress there, but, um, uh, that was what was 10 years ago.
Five years ago when we started the company, most, you know, cheap data and AI, or at the time was just data officers would tell me something like, I will manually look at every single report before it hits my CEO.
Yeah.
And before I look at it, I have my four people on my team look at it manually.
And so really like five, six years ago, it would just be eyes on glass.
Like you would basically have like, you know, how many sets of eyes could manually look at the data?
That's what people did.
I want to say a couple of years later, people started writing, you know, um, writing SQL code to create monitors.
So basically say, please look at this data set and make sure that X, Y, Z criteria is being fulfilled on this timeline.
And so people would manually do that.
You know, if you look at the biggest, you know, financial institutions today, you still deploy a lot of people who, um, will do a lot of manual work.
And so we'll manually look at this data.
I think that still exists.
And I think to a certain degree that's never going to go away.
Some of it won't go away, right?
You're always going to have, I think some human loop or some intuition.
Um, but in today's world, every single or very many users in the organization use data.
Um, AI is being surfaced to externally.
Um, you know, I think, um, the various companies are sort of deploying agents where an agent can have a conversation with a customer.
And if that agent is promising that customer something, it better promise something that the company can actually deliver on.
There's been instances.
I think there was someone who bought a car for $1 basically by tricking the agent into selling them.
They had this, I don't remember the brand, um, uh, of the car, but basically, you know, they, they, you know, you have to make sure that your agents are not selling and selling, uh, cars for $1, um, as an example.
Um, and so, uh, you know, I, I just think the, the world in which we are today is one in which the solutions of, you know, manually looking at the data or manually writing tests.
It just doesn't cut it anyway, but historically that's what folks have been doing.
Is there anything I haven't, uh, asked that you want to talk about?
I mean, what I can see that, um, that this space must be expanding very rapidly.
So you're, you're positioned well, but are, is there anything that we haven't talked about that you think listeners should hear?
I think we covered a ton.
I mean, I think you had great questions.
I'll, you know, I'll just end by saying I'm super excited for, you know, the next five years.
I think if you look at like the last 10 years, the last five years, we've, we've made a lot of progress.
I think we're going to make even, even more progress.
And you know, the, the thing, you know, where, where I find my passion, where I wake up every single day is I can't imagine a world where data isn't the most important data and AI is the most important part of our lives.
Um, I'm very excited about that future.
And I think that future would be way better if the data was actually the AI solutions were based on data that's reliable and trusted and accurate.
Um, and so we're really excited for that version of the world in which we want to live in.
Yeah, absolutely.
Um, okay.
Bar, uh, well, that, that's fascinating.
I mean, these are all these worlds that, that are connected to what I spend most of my time paying attention to and that I know very little about.
Um, uh, yeah.
And if somebody wants to explore, uh, Monte Carlo, what, what's the URL to get started with?
Monte Carlo data.com.
Okay.
I'll just repeat that Monte Carlo data.com.
And you're also welcome to reach out to me on LinkedIn.
If you'd like to connect.
What does the future hold for business?
Ask nine experts and get 10 answers.
Bull market, bear market, rates rising or falling, inflation going up or down.
Can somebody please invent a crystal ball?
Until then over 40,000 enterprises have future proofed their business with NetSuite by Oracle, the number one cloud ERP, bringing accounting, financial management, inventory, HR into one fluid platform.
With one unified business management suite, there's one source of truth, giving you the visibility and control you need to make quick decisions.
With real time insights and forecasting, you're peering into the future with actionable data.
If I were a larger organization, this is the product I'd use.
Whether your company is earning millions or even hundreds of millions, NetSuite helps you respond to immediate challenges and seize your biggest opportunities.
Speaking of opportunities, download the CFO's guide to AI and machine learning at netsuite.com slash ionai.
That's netsuite, N-E-T-S-U-I-T-E dot com slash ionai, E-Y-E-O-N-A-I all run together to get the CFO's guide to AI and machine learning.
The guide is free to you at netsuite.com slash ionai, netsuite.com slash ionai.
