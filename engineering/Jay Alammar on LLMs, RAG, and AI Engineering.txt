A lot of the most exciting things that AI is able to do, even more recently, are not generative in nature.
These are things like semantic search and re-ranking.
And these are, let's say, a lot of the more stable, highly reliable use cases that, you know, businesses and enterprises can do.
So I'm interested in that side.
I learn in public.
I'm here, so this is, I'm coming into my third year, where I get to see how people sort of build world-class large language models.
I get to build with them and then teach the developer community and enterprise customers how to problem solve with language models, what the use cases are.
Jay Alomar, renowned for his talent in visualizing complex AI concepts.
Yes, he was the guy who wrote the famous Illustrated Transform article back in the day.
He now brings his expertise to the forefront of AI development at Cohere.
I first got into Transformers because I read Jay Alomar's blog and now I get to work with him. He knows who I am. That is so cool.
In this episode, you'll discover why retrieval augmented generation could be the key to more reliable, context-aware AI,
real-world examples of how businesses are using this technology to gain a competitive edge,
and the ethical implications of these advancements and how the industry is addressing them.
Jay now advises enterprises and the developer community on practical applications of large language models.
We'll dig into how this transition from an educator to industry insider at Cohere has broadened his perspectives on AI.
Jay walks us through Cohere's mission and at the end of the show, he tells us about his exciting new book on natural language processing.
Enjoy the show.
So what really excites me about Cohere is you're the only LLM company that's actually helping people with the last mile.
True. It's a bunch of different things.
So language models have been this incredible, I'd say dramatic advancement in the ability of software to do things that software is basically not able to do five years ago.
And then very quickly that people started wanting to use language models for factual generation, for example, asking questions,
which is why a lot of this focus on RAG is one of the main things at Cohere for on retrieval augmented generation.
Retrieval augmented generation is the most useful application of language models today and ones that most sought after, I would say.
Okay. So it is the idea of augmenting a language model with some more information.
So it can answer questions more truthfully and have, let's say, access to a data source that can supply it with relevant information.
So the model and its generations can be more factual and more truthful and let's say grounded into a specific data set that is relevant for you.
And so that's the case where the model, instead of answering right away, it does a search step in the beginning.
That system does a search step in the beginning that gets some context and then the question is presented.
That's the simplest form of it, but we have a roadmap that takes this to the next level and the next level.
On the, let's say, technical side, that includes investing in retrieval.
So the search that comes before the generation.
So that extends to embedding models, re-ranking models.
Re-rankers are the fastest way to inject the intelligence of a language model into an existing search system.
They work in a very simple way.
It's one API call after an existing search system.
It just takes, let's say, the top 100 results that that search system produced.
It just changes the order.
It says, hmm, this result, number 30, is actually the most relevant to this query.
And so I'm going to put it at number one.
And that vastly improves every existing search system by just having that one API call at the end.
And then the generation models are also sort of tuned for citations, for paying attention to what you give it in the context and answering questions from that context.
And because these models are, let's say, general problem-solving tools, people need additional tools to build on top of them.
And so we support them with notebooks, code examples.
But most recently, one of the most recent things we've deployed is the Cohere toolkit,
which is basically the playground that we've been building and polishing for the last two years.
But it's highly optimized for tool use, for multi-step rag or multi-step tool use,
where the model can continue to ask questions and search the web or search a data source until it finally finds that answer for, you know,
a more complex task that people ask of language models rather than just, here's a question, give me an answer.
Or here's a request, give me back the text.
So it's very exciting to give people a way to sort of build a production ready.
There's a web application there.
People can just download and throw onto Docker and equip it with tools and then build something on the cutting edge of what's possible with LLMs today.
I liken this to the Apple Vision Pro, which is that you have to build the platform first,
and then you need a lot of developer imagination for that innovation to happen.
And right now, what are the most interesting uses of tools that you've seen?
So I love to think about the evolution of how people use LLMs.
It was, okay, you have this model that can write or that can chat.
And then how do you get around this problem of hallucination?
And so retrieval augmented generation is maybe the top use case now that a lot of enterprises and large companies sort of need to tackle when they chat with their own data.
And I really love how tool use sort of expands that and tool use is the natural extension or abstraction of it.
Where with RAG, you can, like we mentioned, the model can ask follow-up questions.
And so that's multi-step RAG.
And that extends very nicely to the idea of tool use where, yes, you can use a search engine.
Yes, you can use, let's say, searching of a PostgreSQL database or Notion.
But if you can search Notion, why can't you even post to Notion or do things that are a little bit more advanced and sophisticated
and just on the edge of what's possible with software today?
And so the first natural extension of tool use beyond RAG is to say, okay, retrieve information from these three different sources.
You have these three data sets.
For questions about HR, reach out to ask or search within, let's say, Notion.
For questions about, let's say, world knowledge, go search the web.
And so identify, giving a model, let's say, three or four sources and the knowledge of where to retrieve
or what source to use to answer what kinds of questions.
And so that's, let's say, the beginning.
And then that opens up the door for things like, okay, the model can generate code as well,
and then it can run that code within a Python environment, which then opens up things like visualization
and, let's say, data analysis.
And we have some examples like that we're running people through in the program today.
We're starting to see some very sophisticated kind of architectures of language models coming out.
They might be agentic, as you say, sophisticated forms of tool use, dynamically fine-tuning domain-specific language models,
having a mixture of deployment.
So, you know, mixture of SaaS and on-prem and even on mobile devices and so on.
So it feels to me that it's a bit of a myth that all of this comes for free.
There's a lot of serious engineering that needs to go into this.
Yeah.
What are you guys doing around kind of like helping people bridge that gap?
Yeah, especially when you think about the enterprise.
You know, it's very easy for you to see a cherry-picked example on Twitter that, you know, works six out of ten times.
But for companies to put these applications into production, they need to have a really good understanding of what is reliable behavior.
And so that's where we sort of advocate for what are the best use cases to start with, for example.
And so that's one area where, for example, re-ranking is the most reliable and robust first use case for a language model
that you can throw out and, you know, have in production within a weekend and sort of have expected good results
and dramatic uplift very quickly.
Investing in search.
So not thinking of language models just as, let's say, chat or conversational agents.
Think about generation beyond chat, which is, let's say, summarization, extraction.
And then think about beyond generation, which are these maybe underestimated applications that build on top of embeddings,
like semantic search and, like, classification.
And so having a, thinking about language models and their broad set of capabilities beyond just conversational agents is a good, let's say,
way to think about and prioritize the use cases that you go with and then you keep building and building on top of these capabilities
and how you evaluate them and how the team sort of internally in each company knows how to build and deploy and evaluate these use cases.
Jay, what do you do at Cohere?
I learn in public.
I'm here. So this is coming into my third year where I get to see how people sort of build world-class large language models.
I get to build with them and then teach the developer community and enterprise customers how to problem solve with language models, what the use cases are.
And so I go to developer conferences and speak there, I write guides and articles and code examples where just following this fascination of where LLMs are going to go next and what new things are they making possible.
We have 80 folks coming here today in Toronto.
Who are you speaking to?
So I'm speaking to, in general, builders, people who are building with large language models.
I try to make my talks accessible to everybody to have a good sort of sense.
But then I go a little bit more to the cutting edge when we talk about the latest retrieval augmented generation systems.
So there's a simple RAG template of search and then generate.
But our incredible RAG team sort of has a roadmap of how do you improve these systems?
You know, what are the failure points of them?
Do they fail on the generation side or on the retrieval side?
So we talk about a few advanced retrieval augmented generation steps that can improve existing RAG systems.
These can include things like query rewriting.
And so what is the right query to send to search instead of sending a whole paragraph that a user wrote, for example, what are the right keywords to use there?
So that's one area that dramatically continues to improve a RAG system, as well as asking follow-on questions, which is multi-hop RAG or multi-step RAG, that the system is able to do that.
As well as the thinking about data sources as tools where you define to a language model that these types of questions should be directed to this data source, which is more likely called routing now.
So which source is able to, the model is able to source that information from, and then how that extends to the next, to beyond RAG, which is tool use, which are the real, the next paradigm of what is going to be possible to do with software and language models that will continue to be, let's say, seems to be,
going to allow models to do things that are dramatically better and more ambitious than what is possible just with language models without tools.
So Jay, what are your reflections, you know, since the advent of large language models?
Two years ago, I would go to some of these events, and I would tell people why language models are interesting.
I don't need to do that anymore. People have already have an interaction with a large language model of some sort.
And they don't even have to be developers. I think everyone has now touched the power of LMS, which is a drastic, so let's say, adaptation for the technology.
And so the things that we need to raise awareness for have changed with time.
And so in the beginning, we had two or three years ago, my talks were, this is what a language model does, this is why they're cool.
Now we have to educate them on other things, which are to say, you know, maybe you should be thinking about retrieval augmented generation instead of asking the model questions and relying on the world knowledge sort of encoded into the model's parameters,
as well as advocating for a lot of other use cases that are more reliable for builders to build with AI.
Things like embeddings, like search, like re-ranking, like generation beyond just conversation and chat, things like summarization, extraction.
And so my conversation with a lot of developers are like, these are the various sets of new capabilities that language models enable you to do.
And so think of them as primitives, new primitives to think about on how to problem solve rather than thinking about this one black box text in text out model, just, you know, breaking them down to these capabilities.
And then you compose these pipelines or more intelligent systems that build on top of them.
You see an industry, these two things sort of emerging as probably two different jobs, the machine learning engineer and the AI engineer,
where one you're maybe problem solving on the model training level and having access to that model.
And on the other, maybe you're working on, let's say, chaining multiple LLM APIs together, maybe fine tuning an existing model.
And so it really depends on where you are in that value chain.
We're seeing a lot more, it's a lot more accessible for people to be AI engineers because then you can come to it from any software engineering background or from data science.
And you can build actual tools there and get very quick results there.
So if you're coming from that standpoint, the tools that you have are prompt engineering and possibly fine tuning as a second one.
Well, if you're an MLE and you have access to the model, then it's about how much compute you have.
Are you going to be able to, you know, continue training a base model or are you just doing the follow on, let's say, post training fine tuning?
Are you, for example, doing the instruction tuning of the model or the, let's say, preference tuning RLHF or DPO or sort of.
So, yeah, that will definitely depend on where you come at it from.
And I would see the majority of people would have access to tackle it just from the prompt, maybe a little bit of fine tuning.
Interesting.
So the space is evolving all the time.
So a few years ago, you needed millions and millions and millions of dollars and you needed to presumably have a data acquisition and processing pipeline.
That's where a lot of the work goes.
And, you know, in the olden days, you need, you needed to pay for an API.
Cohere have completely changed that.
You've now released a model.
So people, if they pay the commercial license, they can actually use the model.
They can create derivative models.
They can fine tune models.
They can build out these application architectures where they do some of the, you know, the prompt engineering, prompt injection and presumably fine tune models on domain specific data that they have locally.
They might want to run some of it on premises, some on mobile, maybe some in the cloud.
What is Cohere doing to kind of help people build that end to end story?
It is really the vision is to give people the flexibility.
Some people, some companies, for example, they want to focus on their own problem and they want an LLM provider that deals with the deployment for them.
And so that's a class of people who want to build, who want to do what new capabilities these models are enabling software to do without necessarily building up a machine learning team internally.
And let's say hiring a squad of machine learning and data engineers, for example.
So that's one category of companies.
And it's a spectrum.
It's just like software adoption.
You see a lot of companies that hire their own software teams, others that rely on external software or others that use software as a service.
And for the industry, thinking about AI as just an extension of software is a useful lens to have.
And so for the various deployment options of AI that Cohere thinks about, the Cohere platform is, let's say, the easiest way to just get an API key and start building and interacting with the model through the playground and through Coral.
And then you can take it to the next level and do the SDK and sort of play around with the code a little bit.
But then when you're talking about enterprise and large companies, they really care about things like data privacy.
And so that's where the private deployments come into and come into hand and where the model can actually go to the data instead of the other way around.
And so Cohere's focus here is on multiple things.
But one of the key aspects is being sort of on all the major clouds.
So wherever your data lives, the models can can can go there and you can build these these systems very easily.
So you're not, let's say, linked to one one cloud or another.
And that includes, let's say, multiple formats of how these private deployments.
So they can be in your own, let's say, virtual private cloud or it can be through services like Amazon Bedrock or Oracle's generative AI service.
And so giving people the flexibility of wherever you are and how you want to sort of build.
And then then the next level of it is absolutely the research, let's say, release of the models, which are the most, let's say, advanced users of these systems that have the capabilities that have the people who sort of know how to maybe evaluate these models or extend them.
They have access to the research weights to be able to evaluate that use case and take things to the next level.
The industry still needs to learn a lot of things along the way.
And it's definitely getting easier in some aspects and, let's say, more difficult in other aspects.
So now you have these models that can solve a lot of arbitrary problems that you can throw at them at a high and increasingly better rate with time.
And also they start to show you potential of tackling things that more and more were not possible with software before.
Just like how, you know, maybe summarization, we didn't have as good summarization systems five years ago or 10 years ago as we do now or chat or these other models.
But I'm a very strong advocate of bringing in the best practices of software engineering into building with LLMs.
So whatever LLM backed applications or pipelines or agentic things that you're doing, it's always good to think about unit tests, about software testing, about regression, about assertions in the flow.
There are some ideas there from different frameworks, for example, that inject that into the flow of LLM backed sort of applications.
And so more and more, yeah, we should never, let's say, stray too far from these best practices of software engineering and proper testing and sort of ensuring the reliability of a system.
Yeah, it's so interesting because, I mean, we've always spoken a good game about unit testing and software engineering best practices, but you know what it's like in reality.
And then I guess like the two components of this is, first of all, there's an argument that we don't need to hire data scientists anymore because there are foundation models.
You know, so let's say I'm working in a bank or something and I want to check receipts and I would have had a computer vision engineer come in and take a vision model and fine tune it and build out this whole predictive architecture.
And now we can just use a foundation model, right? We can compose together all these things, even things like recommendation engines, presumably there will be foundation models for that as well in the future.
And the other lens is that software is becoming a little bit like neuroscience.
Now it was before we had serverless architectures and we had, you know, like if you were sophisticated, you could build like an agential software system using the actor model or something like that.
But now this is real, it's here. And it feels like we're building these inscrutable living systems and we're kind of putting probes in and we're observing things.
We know no one person really understands how it all works. Is that something to be excited about or what do you think?
That's a good question.
So I'm driven most about my curiosity in this field.
That's really what pulled me very close into AI and machine learning and what continues to sort of drive me about this new class of tool, basically, that is able to do things that were not used to inanimate objects sort of being able to.
And then what new opportunities that sort of opens up for people on the individual level, on the company level, on the economy level and hopefully on the species level as well.
And so that's a little bit of my guiding antenna.
And along the way, there's a lot of things that we need to be conscious of.
Part of it is some of our biases, for example, that go into it.
You know, the first thing a lot of people thought about when they started to see this piece of software that they can chat with is to assign it some level of personhood and trust it.
You know, you still find people who, when they encounter a new language model, they ask it, what company developed you?
What data were you trained on?
And sort of applying this, let's say, cognitive bias of this is a generator of text that is coherent.
It might be analogous to a human mind.
And so, you know, being conscious of, let's say, responsible and ethical AI and the various, let's say, important ideas there and how to think about these models is something that needs a lot more awareness in the general public.
People closer to NLP sometimes have a little bit more exposure to that.
But since these technologies are touching the general public now, I think a lot of this awareness needs to be to be out there a lot more.
So I would say, yeah, that would be maybe the first reaction I would get to that line of reasoning.
Yeah, I think part of this is the inscrutability.
So we can build explainability technology, but it's kind of, you know, you can't explain these things.
It's a bit of a post hoc confabulation, but you can't explain human brains either.
So I have a theory of mind and that's how I figure out what you're doing.
But it's almost certainly wrong.
And in the same way that it's wrong about language models, I think there might be a new kind of generational gap.
You know, when computers came out, you know, my grandmother wouldn't have understood what on earth was going on.
And in a way I have confidence for the future because I think while we don't understand, you know, young kids growing up with this technology, they will learn.
I mean, some of it's just the kind of the naive novelty effect.
Right, because, you know, when you start to see some of the failure modes, you can't, you know, I think that illusion erodes.
But I do have hope for the future that we will kind of learn how to think about this technology and integrate it.
Yeah, yeah, yeah, absolutely.
And you start to see it like as more people in the hacker community, in the tech community sort of really care about reliable behavior of these models.
You start to see these frameworks, these ideas, these guardrails tools, for example, or LLM assertions in the flows of these pipelines.
So, yes, the incredible amount of life that's in this, you know, AI hacking ecosystem and how people are sort of figuring out not just business models and ideas and types of applications,
but also best practices of these that's commendable and any work in interpretability continues to be, you know, fascinating and needed alongside all of those practical tools.
You start to quickly see the potential of where these type of technologies, you know, are taking the software industry.
And so the sooner people start to build the right concepts, the right understanding, the right, let's say, primitives to problem solve with,
the better position they'd be in to use these in their existing, let's say, jobs or projects that they're working on and sort of be,
it's kind of like the people who invested very early in mobile applications when, you know, before smartphones sort of exploded or in the cloud as well.
So that trend is very clear that there's dramatic potential for disruption of existing standards, let's say, in the market.
And so the sooner you invest in that, the better you will be equipped to capturing the next wave of opportunities and possibilities.
What advice would you give to someone starting out in machine learning?
Share what you learn publicly, whatever format that you were comfortable with, be it Twitter, create videos, create, you know, Snap or TikTok videos, whatever format that you like.
Share what you learn. Don't let impostor syndrome sort of stop you from from sharing what you learn.
Whatever you were learning, whatever you've just learned last month is worthy of sharing because when you started, it was intimidating to you.
You didn't know what that was. And a lot of people are still in that mind frame now.
So don't don't give in to the to the curse of knowledge of I know it, it must be easy.
No, just feel free, share what you learn, even if you only share a curated list of what the websites that you found useful.
That is good content. Just share what you learn.
What worries you most about the use of AI in society?
There's a lot of attention to, you know, what areas we should pay attention to.
A lot of them are still kind of invisible or they don't get as much.
So there's a lot of recommendation systems right now, let's say, or machine learning.
So they're not necessarily generative AI, but they do control a lot of the information that flows into people's heads.
So these are the recommendations or the recommenders on social media.
These are systems that control a lot of the flow of information.
But you can take that to other recommendation systems in maybe any app that you use.
These are not necessarily, yeah, these generative AI of the next generation, but they're super impactful on, you know, you have people consuming content for hours every day.
And a lot of that is decided by these algorithms that grabs my attention in terms of AI safety.
What got me started really to switch into machine learning is the release of TensorFlow as an open source.
So I took that as an opportunity to pick things up.
So open source machine learning has been a big thing.
That time was the rise of deep learning.
So these deeper and deeper models, that was the, you know, the big rage at the time.
So deep learning for a while was starting to work and starting to make software do things that are absolutely tremendous.
And then from that, the rise of language models really took it to the next level.
And, you know, I wrote the Illustrated Transformer at the time.
It was a paper.
It was a new form of attention, like a third one.
There were a couple of pretty popular attention models at the time.
But then with time, the importance of the paper and the model kept increasing.
So BERT comes out, GPT comes out, other models comes out and they get bigger and bigger.
And then they go and eat computer vision and they go other places.
So, yeah, like the rise of Transformers and language models and just how scale gives them capabilities that are a little surprising.
I think that's one of the things that I find fascinating and I think surprised a lot of people in the ecosystem.
With this explosion in large language models, a lot of people are just starting to come across these.
These are not necessarily people in the field or like researchers or even software engineers.
And it's easy for people who are coming into that to misunderstand what this is.
And when people when a lot of people who are not close to the field here in artificial intelligence, they think, OK, this is software in the form of a human mind.
And the chat interface maybe also leads them on to think that.
And so they try to say, OK, tell me about yourself as a kind of model and how were you trained and what companies sort of trained you.
And there are a lot of things that we ascribe to these models when we are sort of first coming in that we we find our common failure modes that people should be.
We should raise awareness of how to think about these models, how they were trained and what they are good at and what they're not good at and what things are cherry picked that make a really amazing screenshot on social media and what things are reliable behavior that you can actually build into a product.
A lot of people learn that the hard way. They see something, they say, OK, the model can do this.
Let me build a product that does that. And then it's very difficult for them to get that reliably in 99 percent of the cases or 95 percent of the of the use cases.
So we try to raise awareness on on these things, but also to have people not focus specifically on just the generative parts.
A lot of the most amazing things that I can do and the recent developments are also on the representation side and what you can do with these with these representations, things like semantic search classification.
These are both reliable, but also they can give you like say specific specifically in business applications and real world applications give you reliable performance and improve existing systems very quickly.
So, yeah, when people think about chat models, it's good to take a step back and look at the whole view of what I can do and have that lens of reliability.
And whenever you come across an impressive demo, just ask because we keep falling into that.
This is not the first deep learning frenzy like we've seen we've been promised self-driving cars by three years ago, like maybe six different car companies said they would have self-driving cars by 2020.
This was back in 2016 when, you know, one of the previous deep learning frenzies happened.
So let's let's learn from the past and have sort of some some realistic timelines for for different use cases, some use cases.
Some use cases, the models are ready now, others later, but it's it's one of the it's an observation that people should build that in as as this technology develops and have that discerning eye of asking, is this cherry picked or is this reliable behavior?
Can you kind of sketch out the continuum between that old school thinking of semantic search and, you know, nowadays people are saying, oh, just have a large context window and do everything in the language model?
Yeah, that's a great point. So, yes, the the that reference is to sentence birth.
Yeah, that sort of was was developed by Niels.
And when people want to build with LLMs, there are multiple approaches you can build at the very top of the stack and where you're just a consumer of a commercial LLM, you just send prompts and get text back.
That's that's one way of doing it. But then you can also dig deeper and become a builder of systems that use these.
And if you want to be a builder with LLMs, one of the biggest things I advise people to do is have a sense of what embeddings are and what things you can do with them, because that's a lot closer to engineering than prompt engineering is.
You can get a lot of these. Again, this is reliable behaviors and semantic search is one of the best cases.
So even for the future of generation, semantic search is going to be very important as you talk about retrieval augmented generation and then search augmented generation.
So embeddings empower semantic search in, let's say, two ways.
So there's this idea of dense retrieval, which is where you do you you have your text archive, you have the embeddings of each text in your in your data archive.
When you get a query, you embed that and then you just find the nearest neighbors.
And that's been dramatically improved the last three, four years since BERT.
So BERT doesn't really do very BERT standard, let's say, not not sentence BERT.
But it focuses on token representations and token embeddings, but using it for retrieval requires additional training, which is what the sentence BERT papers brought, which is this contrastive training step of saying these two are these two sentences are similar.
And then these two sentences are not similar.
Yeah.
And then when you think about that objective, one question you might ask is, is a question and its answer all always semantically similar or can they be different?
And they can indeed be different.
And that's why if you're focused on text embeddings for search, you want to train additionally on data like like that.
So you're a query can fall into the embedding space point that is close to its answer.
So these are optimizations that came into embeddings that improve semantic search using embedding or let's say this first family of semantic search using dense retrieval is this.
So the other concept is this idea of re-rankers, which are usually maybe it's one of the easiest ways to plug in the intelligence of a large language model in any pipeline.
You have a search system, you know, every app that you build will most likely have a search component of it, a search engine to search.
So you don't necessarily have to be building the next web search engine.
And the easiest way to plug in a large language model in that is to say, OK, these are my search results that my current system provided.
These are the top 100 results.
You pass them to a re-ranker that just readjust them, the order of them in relevance to the search query.
And so, yeah, that's I would say these are two very easy ways to plug into plug the intelligence of language models beyond just, you know, generation steps.
And you can build, let's say, real systems with that and Cohere aims to make it easier to do both through an embedding model and a re-ranking model and re-ranking endpoint and models sort of specifically focused on these in multiple languages.
Now, if you want to do generation, generation can solve some problems and some use cases, but not all.
They're both tools in your toolbox and nothing sort of replaces necessarily everything.
If you have a long context window, let's say you want to ask questions about the documentation of a piece of software.
Is it really efficient if you have a thousand questions that you keep sending the entire documentation text to the model for it to sort of process it every time you ask a question?
That seems to be, you know, a lot of wasted compute because you're just sending the same thing over and over again.
But you can easily replace now by, let's say, a retrieval augmentation where you store it, the embeddings are there, they capture the text and the meaning of the information in it.
And then you can use it to retrieve the text whenever you need it, whenever you need that sort of detailed look at it, then you can present it to the model in that stage again.
So, yeah, if you want to take a step back from generation and build these tools in your toolbox, this is one great way that I advocate for thinking about LLMs.
For builders, I advocate that if you want to start building with LLMs, build a semantic search engine, because having that mental dexterity of what you can do with embeddings and what semantic search is able to give you just opens so many doors for you down the line,
where you're not just sending prompts and changing the text here and telling the model that it's the best poet in the world and trying to get better poetry out of it.
Can you talk about actually your role as an educator in this field?
What motivates you to put so much effort into creating all these educational resources?
And you can talk about the LLM University as well, but it's not just that.
You've been doing this for years now. Why do you think it's important?
I learn so much more when I try to explain something.
I can read a paper, but if I don't try to explain it, I will only get, you know, maybe 10% really or 20%, for example.
But if I have to explain it in detail, you know, part of your brain is like, you know, I don't want to put this out there and be wrong.
Let me make sure. Let me double check. Let me, you know, triple check this sentence that I said.
And that sort of, you know, when I identified something as this is an important idea, I need to have a really good sense of it.
The writing and education is has one result of it is just me learning more deeply on this.
And then it just comes out of there's a sense of gratitude to the people who have helped me understand these things.
So the incredible blog posts of other people who have, you know, explained things just, you know, childhood heroes like Carl Sagan and Feynman and people who've explained these very complex ideas to the general public in very compassionate ways that are, you know, don't expect you to be.
Very advanced in that field, but also people in the field.
So learning machine learning myself, like things like people like Andre Carpathie's blog was incredible.
Like the first time I understood like text generation via RNNs was one of Andre Carpathie's unreasonable effectiveness of RNNs, I believe was the blog post.
Andrew Trask had an incredible one was like neural networks in 11 lines of code.
And if you can condense that whole concept in 11 lines of Python, that that's really when neural networks sort of Chris Ola has had also, you know, a bunch of really good blog posts on LSTMs.
And then my now colleague Luis Serrano has incredible videos explaining things on YouTube.
Networks were, let's say, hard to understand. I'm not going to say convoluted.
His explanation of them, I thought, was extremely helpful, which is why, yeah, something like LLM University is something that is really special for me because I get to collaborate on it with him and with another incredible creator, Mior Amr, who created another visual book about it's a visual introduction to deep learning.
So these are some of my favorite machine learning educators out there who have like a real deep sense of explaining things and exposition and like making things simple and but also visual communication as well.
And LLMU is like the collaboration of the three of us to say, OK, you want to learn about large language models.
Here are the main concepts that you need to do in a very accessible visual way.
And we're available also on the Cohere Discord to answer any questions.
So if you're going through the materials and you have any questions, we're happy to sort of answer those questions.
And it's just the release is just the start. We're continuing to create more content for it.
Luis Serrano is a man. He is. He's amazing.
He was an inspiration for me, actually. And I can remember many of his videos in particular, actually, the he did one on collaborative filtering, which everything you just said, like amazing visualizations.
And I've always aspired to make content like that. But I think it just requires a certain kind of, you know, I mean, he's really, really good at it.
But similarly, you did something interesting because there are people like, you know, like Josh Starmer, StatQuest and Luis Serrano.
And they kind of like make really, really structured pedagogical content.
And what you did with the Illustrator Transformer is you kind of did that.
But you're talking, I don't know, I guess you're going into more depth.
The depth that's in there is the depth that I wanted to understand it in.
That's that's one one component, because I can just explain the architecture, but not go into, you know, some of the other details.
But I wanted to wrap my head around around a lot of the other main concepts.
Some of the things actually came from the authors themselves.
So after I had a draft, I sent it to the authors.
I'm like, OK, let's you know, what do you what do you think about this?
Are there any corrections, things that and they actually gave me some really good feedback, like focused.
So I had to write some new sections.
So, you know, I didn't have the section on the residual connections, for example.
I think that that came after some feedback from from some of the authors, but also some corrections to a lot of the which is another life hack that I.
So as I advocate for people to write and explain papers, you know, send that material to the authors.
Their emails are right there in the paper and you'd be doing them a favor of publicizing their work and you will get really good feedback from.
Yeah. Yeah.
So, yeah, my goals were to understand it and to let's say to to at a really close depth and at the time was also working at Udacity.
So, you know, I had to explain it and sort of to the level of code and having people train models that are kind of like that.
And so that forced me to really understand it all the way to what a student who is coming up and wanting to actually develop something like this.
Know the top 10, 15 concepts.
It's not everything, but it's like it will get you on the path.
So, you know, much of your work in explaining AI relies on visuals and graphics and interactive elements.
And, you know, so first of all, why do you think it's so important and how can visual and interactives help build intuition in a way that just text or math formulas cannot?
Yeah. So I think it does a few things.
It communicates a lot of information quickly.
Like we have insane bandwidth for for visual understanding that, you know, text maybe not isn't able to give that give us that same thing as at a glance.
So the amount of bandwidth that you can put into a very well designed conceptual figure, I find has been sort of really helpful to people.
But it also has these other effects of it can be scanned very quickly.
A lot of people when they are in the beginning, they they're deciding to read your article.
They haven't really committed to reading it yet.
What I find I do is I would scan the article to see, OK, what are the the headlines?
What are the images?
And seeing the images, the list of images seems to be like the first reading that somebody does.
And if it just gives them some idea, it would build a little bit more confidence to go deeper and deeper.
And so I find that also to be entertaining as I'm reading something, the monotony of the text.
I read, you know, one or two paragraphs and then you give me a visual.
It seems like this reward sort of system that you're you're receiving the information on multiple channels and it's sort of breaking your your your system sort of and your focus into reading text and then the visuals and the text and the visuals.
So they're hard to do and I have to iterate a lot on them.
So every image you see is usually the sixth or seventh sort of iteration of that image.
So it's also a thinking process because usually the first image I create out of it is wrong, is not enough, is has some issues.
And only after I do that, am I able to connect some other idea to it.
And then so, yeah, a lot of the times you would want to arrive at the final thing.
But no, the creating the images is part of my sort of thinking process of understanding a paper or understanding a concept.
So I'm really looking forward to a future where every concept that you want to understand is you can find really good good explanations on it that that are closer to your learning style.
So when I write, I have to think about different audiences, somebody who is an expert, somebody who is a not an expert.
I wrote a paper on on just the broad communication of machine learning ideas.
And I have a graph in there where I'm like thinking about the general public, the specialist and the super specialist, where a machine learning paper is really taking a super specialist up a little bit into the frontier.
That's all its job. But if you're writing exposition, you want to get the general public all the way to the frontier.
But how do you do that in 500 words? It's, you know, it's nearly impossible.
So I think of, OK, maybe we break it down into different levels of abstraction.
So the general audience can maybe understand the first third.
And then there's a mental sort of step after that where the specialist can continue to read.
But whatever your background is, you come away understanding a little bit deeper.
You're you're you're in. But then you're not presented with hopefully with any jargon that you weren't expecting before or walls of formulas that would intimidate you if you're not.
Or, you know, lots of blocks of code that you have to parse if you're not a not a programmer.
So I try to delay the complexity as much as possible and sort of have all of these audience in mind in that process.
How the hell do you keep up with all of this stuff? You know, what are your go to communities and so on?
It's it's definitely difficult. Machine learning advances very quickly.
And right now, the last, you know, several months, it's been there's so much more focus on it from every circle in technology, in business and just the general public.
But just curation, just like really focused curation and, you know, selecting specific fields to to really focus on.
I consume a lot of Twitter specifically, but only the people that I follow.
So not not the recommendations.
And I curate the people that I follow, you know, very closely for things that I that I look for.
And a lot of it is, yeah, for for machine learning, for NLP specifically as well.
So, you know, even catching up with NLP alone is is is an incredible task.
So, you know, I've had the the opportunity to not focus as much on on reinforcement learning or computer vision or these other things.
But then, yeah, now NLP is eating the world and large language models are going everywhere.
So, yeah, just being brutal and using those signals as voting mechanisms.
If I see a paper come up on my timeline three, four or five times, you know, that's some social proof of the people who in the past have provided good signals.
And so that's been a useful method for me so far.
What's coming next? I mean, what what do you see is the main limitations and transformers and what might replace them?
So there does seem to be some sort of at least maybe convergence in terms of just architecture research that there hasn't been, you know, major jumps in how the architectures are in the last year or two.
Let's say there's a lot more that can be done on the engineering side.
So people can build so many more.
We haven't even begun to exploit the kinds of architectures of systems, not models that you can build with these things.
So if you have a generation model that does this one generation step and then uses a search step and then another prompt that does something.
So the universe of applications that can be done with the existing systems is just beginning.
We're just beginning to touch the surface of what what that is like.
So I will I do see a lot of engineering catching up in products and from product business and let's say business model that catches up with that development in models.
But then, yeah, the next things are like what other where can we get data once you get all of the Internet data?
How do you train even better models?
And for that ideas like multimodality are is exciting embodiment and, you know, social interactions, how these models can sort of get some information from from social interactions that they have in the world.
And, you know, could could some random person in their garage just either replace a transformer or maybe even decompose a train transformer into a sort of neuro symbolic architecture or something which runs on the laptop?
So, I mean, there's a lot of this quantization that is making these models smaller and smaller.
And fine tuning is another way that you can get some some of the behavior of a very large model in a smaller model.
You see a lot of hardware and chip manufacturers thinking specifically about the transformer architecture and how it's sort of that can work better on their on their chips.
And even on the edge wasn't there, let's say, a Arduino Lama.
So, yeah, the hacker community seems to be doing, let's say, incredible work at shrinking these things down, deploying them at on the edge and more and more sort of devices.
That's that's very interesting to see.
So this article you wrote was back in the days of, you know, Vaswani in 2017, he wrote the Illustrator Transformer.
It was for translation. It was an encoder decoder.
And in the intervening years, what has stood the test of time?
So that's a really good segue into this.
So, yes, it was an encoder decoder.
Now to explain it, the format of language models that is most dominantly used now is the text generation model.
And so that's at least a good starting point to explain those to people.
And so the focus is not on a decoder encoder decoder model, but only on a text generation decoder model.
So that simplifies it. And whatever you learn here will translate into other things.
So that's one way that sort of explains that.
And then there were these ideas, there were a lot of different improvements suggested on the architecture in the previous years.
But a few other things that really stood the test of time are things like positional encoding.
So things like rope, positional encoding.
And there are things around the efficiency of the attention network and attention layer.
So things like grouped query attention and how that works.
And a few others, let's say smaller differences.
But these would tend to be maybe some of the three major areas that have changed in the last six or seven years.
Interesting. Are there any high level insights?
If you were building, let's say you were rebuilding GPT-2 from scratch or something like that.
How many layers do we need?
What kind of insights could people have about the architecture?
I think there are a lot of insights.
There are some on the architecture.
A lot of it has not really changed.
But there's a lot more insights on the data and how the data should be curated and the training phases.
I think that's where a lot of the focus and how the improvement on these models can really be taken to the next level.
And so these three steps of training.
So language modeling is only the first of these three steps.
And then you have the supervised fine tuning or instruction tuning.
Then you have the preference or RLHF and the more higher quality data that you have across these various steps.
That's the better.
So these tend to play a much bigger role than let's say the architecture itself, which on the whole stays very close to where it was five or seven years ago.
Jay, tell me about the book that you're writing.
Yes.
So hands-on large language models is coming out from O'Reilly and my co-author and myself, Martin Houghton Dorst, we write about transformer language models, LLMs in general, how to use them for applications.
That's, let's say, a big chunk of the book.
If you're a software engineer or data scientist or somebody who wants to use LLMs, we talk about that extensively across various use cases.
And then we also go into how these models work under the hood, how they're built.
And it includes one thing that is, let's say, an updated version of the illustrated transformer.
It's this blog post that, you know, millions of people have read on my blog, but that was about seven years ago.
So it was very interesting to revisit the transformer and see how we would explain it now and what changed in this time period.
So look out for it.
It's going to be August 2024 is the hope to get the print version.
We hope you like it and we'd love any feedback on it.
When you're writing a book like this, how do you do the kind of fundamental research?
So one thing that's helpful here is that we're thinking about applications and applications are informed by the use cases that we see in industry and the use cases that people build with and the use cases that you see in the developer ecosystem.
And so that's what informs, let's say, the major, major part of it, because we're writing a book for, you know, the for a large cross section of people, developers, data scientists, data scientists and other LLM builders.
And so we don't need to go very deep into sort of what's the cutting edge or the or the theoretical, but we can focus on what's applied, what has a lot of use cases, what has been sort of tried time and time again and sort of grown in adoption in actual industry.
And so that made the selection choice of use cases a lot easier.
Awesome.
So, Jay, you've already released several chapters of this book.
Where can people find it?
Yes.
The O'Reilly platform has this early release program.
I think about five or six chapters of the book are already in the early release on the O'Reilly platform.
So you can go to O'Reilly, sign up there and get access to that book and more and more chapters will continue to roll out until our release date in September.
And what's the title of the book?
Hands on large language models and the animal that you get, because that's a fascinating aspect of writing for O'Reilly, our animal is the kangaroo.
So you don't get to choose it.
There's a very secret method that O'Reilly uses to choose that.
We're very happy with the kangaroo ending up on the cover of our book.
Amazing. Jay, thank you so much.
Thank you so much.
Thank you so much.
Thank you.
Thank you.
