about creating a structure which allows for more focused discussion. To get actually a good nuance,
you need focus. A good debate needs moderation. Artificial intelligence is on track to radically
transform the world, perhaps within a few years. Whether that changes for the better depends in
part on whether AI researchers can find scalable ways to make the goals pursued by their systems
aligned with human values. Each of the top AI labs has their own approach to AI safety.
OpenAI uses scalable oversight, training AI on what behaviors are acceptable or prohibited
with human evaluation, supplemented with increasing AI assistance. Anthropic uses constitutional AI,
a principles-driven training process loosely reminiscent of Asimov's laws of robotics.
FETA's plan is to aggressively avoid planning altogether and mock anyone who disagrees.
But these are not the only options. Many independent researchers around the world,
mostly volunteers and a few paid by grants from charity organizations, are tirelessly working
on the alignment problem, with a tremendous diversity of approaches fitting to a field so
early in development. My goal with Guardians of Alignment is to shine a spotlight on some of
these researchers and their work, to facilitate communication and help them gain the recognition
and support they deserve. Today's guest, Kabir Kumar, has taken a far more ambitious and comprehensive
approach to this same goal. His website, AIplans.com, serves as a repository for a comprehensive array
of alignment plans. It is also a place where you can get involved by joining the conversation regarding
the strengths and weaknesses of these plans, and which is most likely to succeed. His hope is that,
one day, hopefully soon, AI labs will be incentivized to value long-term safety enough to draw from the
best ideas available, and AI plans will be a key resource in determining what those are.
I'm Kabir Kumar, so I am the founder of AIplans.com. We are attacking the no-retrust problem. The idea being
that once you boot up a superintelligence, if it's not aligned with you or your values and so forth,
then you don't get to go, oh, that doesn't work, let's try again. If we did get to try again,
we could sort of tackle it almost like every other technology. Our situation would be much better than
it is right now. So that's my main focus with AI plans on attacking that problem.
What got you initially interested in the subject of AI safety?
So previously, I was in DevOps and doing like startups and stuff. So in September-ish,
like a lot of people, I think I saw ChatGPT and I thought, oh my God, the bloggers were right. So
about like five, six years ago, I'd come across like Lesterong for like HMR, all this stuff. And I've
dismissed it because I thought no one is going to find a serious problem, a serious important problem,
and be smart enough to be the first people to spot it and try to solve it with a blog and some
private research. And at that, a disorganized blog. I was wrong. Some people did seem to try that
and think that this would be a good idea, which clearly it wasn't.
How do you feel about the state of organization of the AI safety research today?
So it almost kind of gives me hope because if it was very well organized, it was very well organized,
very well focused, and we were in the state where now, I would be much more concerned. But the fact
that it is so poorly organized and so poorly focused gives me a lot of hope that if we improve this,
and there is a lot of room to improve, then we can see a lot of improvement in alignment itself.
You mentioned the state we're in now. How would you characterize that?
So I think it's made of multiple things. So when I say we're in now, I'm not talking about the
alignment problem itself, more of a state of the AI safety field.
Right? Just how many people there are trying to solve a problem? How they are trying to solve a
problem? What is the way that they are organized? What is the way funding is organized? So on and so
forth. So first of all, the thing that's most important in any field is the funding. Funding
is primarily through donations. Donations or grants rather from open philanthropy, from EA, and for some
other miscellaneous donors. There's a total, I think, I think an estimate last time was a hundred
million dollars in AI safety, which is very silly. I think there was a graph that showed that actually,
there's more spent annually on backlogs than there is on AI safety.
Now, does that count the things like the super alignment team at open AI and the work that
their companies are putting into it? Or is that something where it's hard to really?
Yeah. I would estimate, even if you took that into account, it would still be a very small amount
compared to essentially every other industry. For example, Anthropic is one of the, and not just in
terms of money, but also in terms of time, right? For example, Anthropic is one of the safer orgs or
more safety focused orgs. And they bragged about how they had spent over 150 hours doing safety testing.
There are more hours than that in a week. One of the safety focused orgs has been bragging about how
it's spent less hours than there are in a week on safety. That is like one example of the current state.
Okay. Detecting a note of a cynicism regarding the top, the leaders of the field and solving this
safety problem. Am I reading that in or is that something?
Yeah. In a way it's not particularly their fault. So currently in AI, the liability is really not clear.
If you've been harmed by an AI, it's unclear right now who is actually liable, right? For example,
suppose you were the victim of a deepfake scam. Who is at fault? Is it the company who made the
voice-cloning technology? If part of a deepfake scam was that someone was able to take someone's, say,
social media posts or someone's texts and stuff and figure out what their patterns of speech were,
and they were able to use that to scam you and not just voice cloning, then is there a joint
liability between the scammer, the voice-cloning technology, and the person who made whatever
was used to match the speech patterns? And it's unclear. Even with the EUAI Act and the bipartisan
bill by Senator Hawley and Bloomfall, it's still unclear. And what this means in terms of 4D companies
is that so in almost every other industry, because there are liability laws, making a product which
is not safe is not profitable, right? If you want to build a bridge, if you want to build a car,
if you want to build pretty much anything, right, that's going to be used by people,
and that's going to have some safety concerns. Better safety is more profitable. So you actually
spend money on that. And every company spends money on that. It's not just a theater for volunteering
and for just talking, which is essentially what it is now. So right now, if an AI company is focusing
on safety, they either do that just out of the goodness of the hearts. And if they are doing it,
it's likely to be something which is not very profitable for them, right? Well, it should be
profitable. It should be profitable to make your product safer. And that's not what we currently have,
which is why, one of the reasons why we're in a safe way. Given the kind of state of the incentives,
it's not totally fair to criticize them for not doing enough because like, it's amazing that they're
doing anything at all. In a way. Yeah. I think there's still responsibility to be held for your
actions. Yeah. I don't say things like, ah, bad company doing bad thing. I don't think it's anything
like simple as that. I don't know. So zooming out a little bit from the larger companies,
uh, how would you kind of characterize the way, uh, AI alignment and safety research is being done,
not just at open AI and anthropic and, you know, D1, but like in everyone who's involved.
So I'm not in alignment research myself. So my view of the field is going to be flawed. And I'm
obviously very new to, to solving as well. So it's not going to be as in-depth as say David Ad or
something, right. Or someone at Mary, but in my small view, it's a lot of people just working on
problems that they find interesting. There's not a lot of ways for people to find out how,
what they're doing is wrong or right, or basically get fast and effective feedback. The sort of closest
things that, that people have is if they are part of a research organization, then they get, get feedback
from their colleagues, right. But obviously if you're working with someone for a long period of time,
then you're likely to share some biases. So maybe you might get some feedback at the next conference
at the next meetup, or you can just talk about your research and get feedback that way,
or you can maybe post it on alignment forum or post it on that's wrong. Right. But the opposite
issue of that is a lot of times, if you're an independent researcher, your research, your post,
it's not attractive at first glance, you've not sort of thumb nailed it and click baited it,
or you're not already well known in the field, then you'll maybe get three, four,
five, if you're lucky comments. And what often happens is your post will just be downvoted and
you just don't even know why. So what can I do except maybe try some other random thing,
or maybe we're going to keep going and just make this thing better because that's what I know what to do.
I'm hearing a big problem of there just not being enough feedback. And this is something that's,
that's make, you know, one of many things, if there wasn't enough already, making it really
hard to make, to make progress as an alignment researcher. And I think that goes pretty clearly
what you're working on. Tell me a little bit about more about your website.
Yes. So it's AI plans.com. So the end goal of this is to accelerate alignment research. What I mean
by this tackling, if you know we try this problem is, is that what is going to be any user can submit
a plan on the site and plans are ranked from top and bottom based on a score of strengths minus
vulnerabilities. Pretty much every single plan has more vulnerabilities than strengths. So every time
there's a new plan posted, that new plan, because it has no vulnerabilities submitted to it yet,
it will start out at the top. What you as a user can do, you can only submit plans, but you can also submit
feedback on plans in the form of strengths or vulnerabilities. Users can vote on the strengths
that you submitted and also on the vulnerabilities. And you'll have two, two bits of karma. You'll have
a karma for strength and you'll have a karma for vulnerability. Users who have higher karma,
their votes will have higher weight. So if an alignment researcher signs up to the site
and a mod verifies that they are alignment researcher, they've been in work and so on and so forth,
then they will start out with 50 karma and strength and 50 karma and vulnerability.
The idea being that an alignment researcher is more able to recognize a good critical vulnerability,
a good strength and so on and so forth. They will upvote the strengths and vulnerabilities
and eventually people who consistently post high quality strengths, high quality vulnerabilities,
they will also gain high karma. And the reason that if you post a plan on-air plans,
you will get more feedback. It will be that because plans start out at the top, plans will also have
a coefficient which will impact the votes. So the higher up a plan is, the more karma you will get
for commenting on that plan. So suppose you commented on the top plan and has a coefficient of 1.5.
So whatever vote someone is giving you, that vote gets magnified by that 1.5. So you have more incentive
to gain karma by giving submissions to the highest ranking plans. And then if you're an alignment researcher
and you have submitted a plan at the top and it's gone down the rankings because people have voted on it,
and you can actually see, hey, this vulnerability is not right. Or I have a solution for this. Or they go, oh,
yeah, I can see this vulnerability has some points, but I can make a new version of my plan
where it solves this. Then what you can do, you can iterate on that. So you can select that vulnerability,
you can select multiple vulnerabilities and so on and so forth and resubmit them and resubmit your whole
plan and you can add in some new information. And it starts out again at the top and you can iterate,
iterate and keep going. And what it lets you do is improve your plan and improve your research really
fast. This is a lot more than just the comment section on the alignment forum or less wrong. A
couple of just general summaries of some of the distinctions that I'm seeing here. One is you're
optimizing visibility for the lack of critiques that it's found. And you're trying to spread out the
feedback. So it's not everyone commenting on the thing that's the most commented on because that's
the most visible because it has the most comments while everything else just completely gets ignored.
And then you're also setting that up to really push for iteration in terms of translating the feedback
into let's change the plan and so that you can get higher up again. Basically try being very mindful
of the incentives. You're incentivizing getting people to look at plans and making the plans better
as opposed to what's going to what's going to be the article that gets the most people to read it so
they spend the most time on the site. Exactly, exactly. Making the incentive structure based around
having a good plan rather than just having a plan which looks good. What distinguishes a plan that that
looks good from a plan that is good from the perspective of people who are judging it based on
what they think. So it's not just about perhaps looks good is too much of a mistake. So that looks
attractive, right? For example, interpretability is something that's very attractive. It goes,
ah, if we could just understand what's going on, then you can solve it. Then it'd be way easier.
It's an attractive idea, but I think it's a bit too attractive. A lot of people are going to go,
psychedelic dog pictures that you get from it. And there's not enough people working on other areas
in the field, which are also important and could be more tractable if they just got past some
hurdles. Do you have any particular examples of certain plans that you've seen since you've
looked at a bunch of them that seems like, wow, this really deserves more attention?
Yeah, so you actually had someone on here, Beric, working on the Ares Project. I'm very
excited about it. I'd like to see it go further. I would like to see if the scales, having
interpretability is good. Something which is inherently interpretable, like Bres Project,
I think would be very good. Also, Nathan Helmberger's plan, I think, has a lot of potential. I think
I'm excited to see where that goes. I'm slightly concerned about that because it relies on perfect
or near-perfect air gapping, which concerns me. But for the audience, air gapping, what is that?
Ah, sorry. Air gapping is the idea you can essentially have perfect security, so perfectly
isolated from attackers. I'm probably the best person to talk about air gapping.
I'll add a little bit of color to it. Years ago, there was this big debate on... Eliezer Yudkowsky
proposed the AI box experiment, where he would have an instant message chat with someone and is
kind of imagining that you're trying to keep a person, the AI in the box, and they're trying to
convince you to say, I let Eliezer out of the box, but willingly, not just tricked to writing those words.
Yeah, yeah, yeah. But you can't say how. And you got them to do that. There was this big debate about
it, whether that would actually happen with a super-intelligent AI or whether you could close
it off close enough. But then in recent times, it's seeming like that's totally irrelevant
because OpenAI just connects ChatGPT to the internet. They have plugins. You can call
functions off of it. There's AutoGPT and the idea that we're going to box this stuff. No,
we're connecting it to the internet immediately. Yeah. I think people were also overly impressed
by that Eliezer experiment, by the way, or Eliezer experiment. Yeah, yeah. There was a lot of stuff
that people debated about that. But it was a big topic at the time, and now it just seems irrelevant.
Yeah. Security in general is pretty bad. We lock our doors with locks, which you can look up any
YouTube video on lockpicking and find out how to pick them. Most doors you can just open with a hook.
Like most, pretty much all our security systems are pretty terrible. What we actually rely on in
terms of security is surveillance and consequences and incentive structures. And that's what actually
stops us from not having stuff broken into over time. But, and also the fact that most people are
pretty nice and pretty law-abiding. And so don't actually want to break into stuff and steal stuff.
Yeah. First layer of defense is you're actually safe and you just want a security blanket. Second
layer of security, most people who are dangerous just aren't really trying that hard. And third layer
of security is punishment after the fact. I would say surveillance and punishment or
information gathering and punishment. Because actually making good security is really hard.
Especially keeping good security that stays good as time goes on and people find holes in your
security. That's really hard. And so this is, is relevant because to, to your site, because like,
this is the kind of thing that you would use as a, as a general kind of criticism for plans that like,
oh, it relies on this thing that isn't really reliable. Then that's, that's a problem if it's just assumed.
Yeah. So like we recently, um, held a Criticathon for 11 days. In the first two days, we made a list of
vulnerabilities, a list of ways that an alignment plan can fill. And one of them was that it relies,
was that it's impractical and impractical has subcategories. One of which was, relies on perfect
air-gapping. But you might even argue that perfect air-gapping is an unsolved problem. Essentially,
there was a broad list of eight different categories, which had, each of which has its own
subcategories. We were actually quite surprised at how well that applied to pretty much every single
plan. There was basically no plan, which was not caught by at least three or four of those vulnerabilities.
You described a few more of those general vulnerabilities that you apply to things. I got
the, the sense, like one of your processes is that the criticisms that are leveled on plans, like,
yeah, you can just make a criticism that's unique to that and ad hoc, but there, there seems to be a
kind of push towards having a standardized, uh, set of criticisms that apply. So this was more for
just the Criticathon. So how the Criticathon was structured was that the first two days were spent
getting this list on. The next two days, which actually ended up becoming three days because people
just had, had fun and wanted to keep doing it, was just going through plans and assigning a max of
five vulnerabilities to a plan. So each partisan could assign a max of five of these vulnerabilities
to a plan. For example, one vulnerability was it has unsolved technical problems. It relies on an
effectively flawless simulation. How to make an effectively flawless simulation is an unsolved
technical problem and the plan which relies on that has that vulnerability. A flawless simulation?
An effectively flawless simulation. What, what is, what does that mean? So for example, if what it
relies on is that, oh, we will have a, a model of an AGI in a simulation and we'll see how it behaves
within that simulation. And if it behaves badly in simulation, then we'll shut it down. You'll be
relying on your simulation corresponding very, very well with reality. Such a simulation has not been made
yet. And how to make one has many, many problems. So you're talking like, if someone's assuming that
someone can like program the matrix or something like that, then like that, that's probably not
going to happen before AGI or, or is it something more subtle than that? Kind of is the, it's a broad
stroke, but yeah. So if you rely on some kind of simulation, which is very, very accurate and has no flaws and
has the whole data distribution, then that is a big unsolved problem of how to do that. And relying on
that is a big weakness. Another wonderful example is if it's heavily relied on an untested and unvalidated
theory being exactly correct, right? Like suppose they have a theory about, oh, I think this thing
is going to be like this and they don't have strong backing of that theory being true. And they don't have
strong evidence or arguments on support to support that theory being true. The whole plan relies on that.
And that's a pretty big vulnerability as well. There's also the issues of something not scaling.
This was a list which was made in the first two days. Then in the next few days, what people did,
we just assigned a vulnerability to a plan. So for example, they might have said, okay, this plan has
vulnerability for BI, it has for CI and for DII. And you can just add that very quickly, unless you add,
unless you add a lot of vulnerabilities to plans very quickly.
I think that the perfect air gapping and some other things, some of them seem like they make
a lot of sense. Some of the other ones, someone might look at it and say, I'm not sure that's
really a fallacy or a flaw, or this is debatable.
Yeah, I think absolutely.
And how do you deal with the question of subjectivity in terms of what's the flaw?
So I think there's definitely a lot of nuance to be held. And I do not think that this thing by
itself, you should go, okay, this thing has this. Therefore, that's it. End of story. Because
that's why we had the rest of the days of critique one, right? So something that we
did after we would assign a vulnerability, the next two days, people split off into pairs,
the pair would pick a specific vulnerability within a specific plan. And one would argue for it,
one would put four arguments for it, the other would put forth arguments against it, then next day,
they would swap. This actually ended up making very good detailed and nuanced arguments,
and criticisms of plans. Because even if perhaps you weren't experienced in alignment or in AI and
stuff, because you got to see someone else who might have been no more than you, making the same
sort of points for the stance, which you would then debate for the next day, you get to then use those.
And also you get to sort of self-reflect on your own ideas as well. I expected it to go well,
and it went even better than I thought it would go. And then we had after that, each person in a pair
would rotate. So then you get to practice the same stance again, for new partner at the time,
yeah, and they're making some really high quality criticisms of plans in a very short amount of
time. And again, there was very much because it was this whole structure of, you know, going back and
forth. You got a lot of nuance, right, which I think is needed. And I don't think you can really
have a good vulnerability without going into that. I think any vulnerability with just saying,
just quoting this is insufficient.
A common theme I'm seeing in this and in the, in the plans in general is it's, it's not about
saying, you know, which being the ultimate judge in terms of which plans are good and which ones are
bad and trying to really like cut things down immediately. It's more about facilitating discussion
so that what's seems better or worse emerges as part of the process.
Exactly. Yeah. It's about creating a structure which allows for more focused discussion. To get
actually a good nuance, you need focus. A good debate needs moderation. If you aren't sure what
is the way that a plan can fail. Suppose you see an alignment plan and it's a bunch of theoretical
maths and computer science and you go, I have no idea where to start with this. You can use this list.
You can just go, Hey, does it meet one of these? Like, can I just use this to find a hole in that?
And then you can inspect that hole. You can inspect it. See, okay, is this actually applying?
Is this true? If it does apply, how does it apply? What might be the reason for it not applying?
And then you can use that to find out more. And because you're focusing on one specific part,
even as a complete beginner, you can get a lot of nuances. Like we have people in the critical form
who hadn't really done alignment stuff. We had data analysis. We had pediatricians. We had a whole
broad strokes of people. We had an alignment researcher as well. And obviously he was
very, very good at it because he's an alignment researcher, but still that focus allows for
high quality work to be produced. I'm interested in this, this topic of like
what like facilitates a discussion. And yeah, you've mentioned moderation and some structure also
focus as being something like, and, and with those like lists, like giving people clear starting points
so that they're not, they're not lost. My first impression of like some of these critiques was,
it was like, Oh, this, this is, this is too confident. Like it's not, you know, including all
the nuance and where, where it might not apply. Uh, but really it's a, a starting point for
discussion, not an ending point. And sometimes it's actually helpful to just stick your neck out
and say, this is my opinion and then get pushed back and then find the nuance afterwards than it
is to try to start with the perfect criticism. That's like totally applicable. Yeah, absolutely.
And I think like maybe even before you say it, you can just think, is this true? Could this plan
be relying on perfect day gapping? Could that be a problem? Is that a possibility?
It's really a list of possibilities. And then you can use that yourself to look at a plan and go,
could some of these apply? If you look at a lock and you have in front of you like a list of ways
that locks can be picked, the exact, the exact way that specific lock can be picked might not be
an exactly, exactly one of those things on the list, but you can look at that list for an idea,
right? You can look at lists and say, oh, are the hinges on the outside?
Oh, they are. Okay. Maybe they're not so like, so easy to pop out, but maybe that offers me to do
something to the hinges, right? You can say, oh, can I, can I get at a latch? Oh, I can,
I can actually see the latch right there. So maybe I can hook something there.
In this example, a hinge on the outside of a door in a lockpicking context, that's not automatically,
oh, this, this door is, is flawed from a lockpicking perspective, but it's, it's certainly
a thing that I should pay attention to if I've just encountered a door and I'm trying to assess it.
Exactly. Yeah. Yeah. Yeah. It's a vulnerability. It's a possible vulnerability. It might not be,
maybe for some reason, this one, this particular one isn't, but it's the thing to look out for as you
said. Are there any like highlights that came up for you in terms of a particularly productive
conversations that have emerged in some of these plans? I think conversation in general has been,
has been very good. I think what's really been the best thing I've seen when the focus can happen
and when the back and forth can happen. One of the ways that those could go wrong was if the
conversation would diverge from the focus, like if they, if they said, does this plan rely on human
ethics? And when I was debating for what I was debating against, and then they started talking about
does alignment on general, in general rely on human ethics. Then I, then I would go and go,
okay, remember folks talking about this specific plan, where in this specific plan does it say
whether or not it, you know, relies on human ethics. So some, for example, something else that they would
do, um, in section two, when we, uh, you know, picking, picking vulnerabilities in plans was not just
saying, oh, this plan is this vulnerability. You'd quote, which part of a plan has the vulnerability,
right? So you say, say on a page two, a section two, or page two, section three, so on and so forth,
and then you quote, this text is the one that has this vulnerability. Because if you go into the whole
field of alignment, this alignment itself requires human ethics on all that stuff. It's very,
very easy to get sidetracked and not actually get the good work done. But when that focus is there,
then you can get that good work. One thing I was struck by and seeing the site is just one source of
value for it is even more basic than that, of just having this thing where you can just scroll
through and see all these different plans, uh, out there and just kind of get a sense quickly of
just what's been done. And they're like, they have the titles there so I can say, oh, this catches my
attention. Like, I want to know more about this thing. Yeah, yeah, yeah, absolutely. So that's actually
one of them, how the site's actually being used right now. Um, so right now the whole voting system
is actually not set up because we need to integrate, integrate our database with the
Eigenkarma network. Um, so we're going to be integrating with the Eigenkarma network to allow
the whole weighted voting thing, because with Eigenkarma, um, they've solved so many problems
that we would need to solve for our voting system. They've solved circular voting and stock purpose
and all that stuff. So it's better if you just make a fork of that and, and change it for our needs
rather than making anything of our own from scratch. So that's still have to be done. But yeah,
in terms of the size of at the moment, it's just, as I said, it's a useful resource. Uh, multiple
researchers have just told me that they've found papers on there for the research. I think currently
it's about 180 plans. I suppose like I was a alignment researcher and I have a, my own like lab
and I'm writing some paper and came up with some novel approach and I wanted to get featured on, uh,
AI plans. How would, how would I do that? You can just, uh, log in or register as user and you've got
an account and then you can just submit a plan yourself. If you submitted a paper on archive,
we have a feature where you can just type, put in a link for the archive paper and get the archive
details. And it will just fill in the details for you. And then you just embed the PDF code
and you can just submit that and post it. Yeah. So this is how the site is looking at the moment.
We're updating constantly. So you can see the abstract and you can click here and to go to
post, right. You can scroll down and you can see the PDF and you can see, oh, cool. This is
this one, why some researchers at Berkeley. And then you can also go down and see, okay,
there's no vulnerabilities yet, but you could add one if you'd like, and you can add, add feedback
as either vulnerability or the strength. And currently plans are ranked from top to bottom
based on the least good size to the most good size. So the plans at the bottom are actually the
ones which have the most criticisms submitted. So see people saying this only sort of ad point or
it requires an important amount of data. You can also use the search bar. Suppose you're interested in
interpretability. So you can see, okay, cool. There's, you know, these things to do with
interpretability or if you're interested in courageability, you can have it in social support.
So if you wanted to revise a plan for vulnerabilities, what, what would that look like?
So suppose you, suppose you're one of these researchers on here and then you can scroll down
here and you can click this button here, revise a plan for vulnerability. And then it will submit
it. Then you can submit a new plan with the details already filled in, right? And this would,
and you will get to know the problem saying that it'll log the post permanently and clone a new one
to revise it. Suppose I had like a plan that I'm, I'm submitting and it's very similar to something
that exists already. Not the same. Uh, you know, it's, it's, it's enough to, to be worth its own
post, but there's a, just a very large amount of overlap. Yeah. So actually we've currently not
already seen that with each kind of research paper kind of does go in its own direction. But in terms
of, if there is just a lot of text, which is the same thing, what I would expect for that is
for those plans to basically be treated like any other really. So if you, so if that plan has the
same vulnerabilities and stuff as another plan, then it will just get those vulnerabilities added to
it. And if it has those same, same strengths that have the same strengths added to it. If it's a clone,
obviously that's a different story. I'm thinking more along the lines of, you know, people working
independently, uh, sometimes come across the same idea. So I think that's, that's actually a good thing.
It's good to have more variations on the same idea to see which variation has more vulnerabilities
and more strengths. Those should both be on a side. And these are organized by like,
I noticed you typing earlier at one point of, uh, searching by interpretability and in a way,
that's kind of things that are similar. Uh, so if you're interested in a topic,
you could see a whole group of things in some ways. Yeah. So you can just type in any word really,
because it's just, it's just a search bar really. Um, you can just say ethics maybe.
Right. And you can see, you know, here's the plans of that ethics in title or ethics in,
in the abstract. Right. So currently it's scanning through the abstract and title.
We're going to be changing this to be scanning through the post as well.
What's some of the development direction that you see this, the site going it's, it's useful now
already as a repository. We can kind of see it growing in the direction of being a, a source of
feedback and debate and discussion. What's your, what's your long-term plan for alignment?
Yeah. So my long-term plan for this is firstly accelerating alignment, alignment research.
Again, shift the incentive structure. Essentially what it says is going to become a leaderboard,
right? It's going to be a leaderboard of the best plans and the worst plans.
And no highly skilled ML researcher is going to be infused about working for a company whose plan is
on the bottom of a leaderboard. What I'm actually working with the creator of AISafety.careers,
or being able to have sort of a logo of each sort of organization next to the plan. Or we might,
we might not, we just have it somewhere in the corner. What this will do is it, suppose a company
likes or a lab like Redwood Research has more plans near the top. What I would like to have,
what I think would happen is we'll see if a shift in the flow of talent, right? Someone who might have
gone to work for say Meta, sees, oh, Meta's plan is actually all the way on the bottom of the leaderboard
and Redwood Research is actually at the top. Maybe I'll give them a, give them a try to see if they,
if their jobs are going to be decent and they'll go, you have more people working at Redwood Research
than you do at Meta. So your hope is to make performing well on a site like this, have some
prestige attached to it to change some of the culture, you know, around AISafety.
Yeah, that's one of them. Yes. And also in terms of policy, one of the difficulties in AI policy at
the moment is you come off as a Luddite. If you say, hey, we should ban this AI staff, right? You're
saying, but it's so useful, right? Why would we ban it? You know, what else could we even do? There's
nothing else which I can just ask a question and it can tell me 10 different versions of the answer.
What other technology can I use to get 10 different high, highly detailed medical answers
in two minutes, right? There's nothing else which do that. But with this, obviously there's 180
different ways. If you can just say, hey, here are the plans which are on the bottom, which have
50 plus vulnerabilities to them and they've not been improved, right? They've tried to improve them,
and they've actually gone to the bottom again. Then you can say, here is a list of reasons which
are posted by researchers and upworded by researchers, right? Which is showing that
there's a lot of things wrong with this. Let's ban the worst ones, like we do in any industry,
like a specialist is banned because it is one of the worst building materials you can build with.
It causes a lot of harm.
So creating like an easily accessible information source for advocacy groups in holding the AI labs
to the fire and or holding them accountable by saying like, you know, if they're not really,
if they don't have a plan at all, obviously that's bad. Or if they're proposing something that has tons
of holes in it to get it, I think showing that there's alternatives, showing there's alternatives
that are better options helps policy makers a lot. It helps them go, okay, I can actually make
something which is not, you know, making me look like a Luddite to my voters. And I can actually just
go, okay, here's the bad architecture. We will not have that. And here's a good architecture. We'll have
that instead. Or here's the less bad architecture. We'll have that instead. That's something which can
be done more easily than just, hey, no one train any AI ever.
Right. So if a politician's like, you know, wants to enforce some kind of safety rule and then open AI
says, yes, we got that. We have scalable oversight and RLHF and that sort of thing. And then Anthropic
is like, yes, we have constitutional AI and these, these rules, like we're doing that.
And then the politicians is in this place of either saying that's no good, you know, go home,
like no AI for you or okay, I guess that's good. But if they can, if they can say like, hey,
there's a lot of other options that you're, you're not considering here. That also kind of brings up
this, this kind of a, you know, broader question. I think that's a real challenge and a need in
alignment since so much seems to be happening in the larger community of like, you know,
independent researchers or small outfits and so on. And, and also just the fact that it's a new
field and very divergent, you know, it's not probably not mature enough to converge fully yet.
You might have some great idea and a great team and, and whatever else, but if you're not on one
of the few frontier companies that has access to all of the compute and they're not, and open AI is not
listening to you, then it almost like doesn't matter because you're just won't get implemented
in the, in the models that are big enough to matter.
Yeah. That's very much an issue. With this, not only can you as a researcher post on here and you
can almost get kind of prestige if your planet is near the top, because you know, each, each one has the
office name there. And if you stay near the top, then that is impressive. It's impressive if you can,
if you have a plan which survives the critique and, and has actually strengths as well.
So the reason that, so originally it was just, um, ranked by least criticized, the most criticized,
why we changed that was actually during the critique form, we found that there are some plans,
which even if they have a lot of vulnerabilities, they have a strength, which is by itself worth
highlighting, like say, forgotten framework that has a lot of vulnerabilities, lots and lots of
vulnerabilities, but it has the strength of the heuristics, which are very valuable and they
should be highlighted. So we've sort of finding out that that exists is why I've moved from, uh,
just from, uh, just critiques to strengths-mind vulnerabilities.
This kind of comes back to the, like evaluating the criticisms, uh, themselves. Some of the things I
saw listed on there, uh, was like only solves inner alignment, only solves outer alignment. How do you
distinguish between a, just a limitation of scope that someone just, you know, wants to focus on
something solvable and someone else will deal with the other thing versus like a fly and that like, this
just won't work because it kind of seems that this problem is big enough that it might not just be
one plan that solves it all. Absolutely. Um, so I think if a plan, say, solves the outer alignment
well or solves inner alignment well, that is a very good strength. That is a very powerful strength.
And so I would expect that to be completed as a strength and then, because then that will be
uploaded. Even if it has some downvotes or it goes down, down rank because of those vulnerabilities,
if it has a strength, like, okay, this actually tackles the inner alignment very well,
or this tackles the outer alignment very well, then it would still go up in ranking because it
has a strength. So you've mentioned, uh, alignment researchers using this as, as a source of getting
feedback and, and, and trying to make their plans as good as possible. Also from the perspective of an
audience, trying to understand what's out there and to, um, discuss it in a way that, in a focused sort
of way. What about, uh, policy makers and people who are interested in, in AI governance?
How might someone like that use the site?
So some company or lab saying that we've got really good safety or we care a lot about safety.
Suppose someone has been harmed by AI and they want to make a case that the company did not do
enough to make their AI safe. That's where someone like this could be used. They can make the case
that the company use an architecture, which has been shown to have lots of flaws, to have lots of
vulnerabilities. And the company chose to use that architecture instead of using a more robust,
more robust architecture. Like you can say that, okay, my landlord is responsible for my lung
conditions because they chose to keep asbestos as insulation rather than swapping it out for a
better installation, which would not, um, give me lung cancer. For example, I'm unsure if asbestos
actually gives a lung cancer or not, but it damages lungs in some way. With that, you can actually,
you know, make case because there is an alternative option. They could have chosen that, but they
chose to keep using this.
Yeah. It wouldn't necessarily have to just be like one or the other, but just like there was this,
this, this hole in their system and they could have supplemented it or, you know, whether it's
using it instead of, or whether it's an addition to, and they just didn't address this known problem.
Yeah. And just, just the fact that it is a known problem that is publicly on a list, but Hey, this
thing is a known problem. And there are alternatives available. They chose to put out a product,
even knowing that there is a known problem within it. And there are other options. I think that by
itself would be very powerful. It will be powerful, not only for folks, um, seeking compensation,
seeking liability. It also be powerful in terms of lobbying. So suppose a company gets charged of say
reckless endangerment, then it can be made case that you are using something which is known
to be harmful. It's just known to be very flawed. You are using power lines, which have been shown to be
unstable. You are using a form of steering, which has been shown to be faulty, right? If a car company
puts out cars, which have brakes, which are known to just randomly stop working, then they can be
held liable for that. They are expected to know the state of their brakes, and that can be assessed
in comparison to other brakes. And just being able to compare plans quickly is very valuable.
If someone comes, comes to your, uh, sees this, they're really interested in your site,
they want to get involved in some way. Uh, what are some, what are some ways people get involved?
Yeah. So one of the ways is the critique one that we're going to be holding, um, on the 15th.
So that's actually a very good way to just get sort of started in reading plans in alignment and
actually understanding alignment again, because of that focus, because it's such a broad field,
it's can be very easy to get lost within it and go, okay, I'm going to, I'm going to learn this.
And then, oh, wait, I need to learn that. Oh, I need to learn this. Anyway, I need to learn this.
Oh, I need to learn the philosophy of this. Oh, I need to learn the maths of decision theory. Oh,
I need to learn maths of complexity theory. And these are entire fields, which multiple geniuses
have dedicated lives to no matter how smart you are. Like as you, you're not going to focus
on one of those things with, you're not going to focus on all of them. You're just not going to be
able to do that. It's better to focus on one specific thing and be good in that and be good
in a way that's useful. And something like the critique form helps you start get started on that
or just submitting a critique on a plan. That's also quite simple. You just say, I think this plan
has this vulnerability for this or submitting a strength. You can say, I think this plan is good
because of this, or if you're an independent researcher or you have an idea for alignment,
just submitting that as well as a plan, it's also an option as well.
Is the critique-a-thon or things like it something that's going to be happening again?
Yeah. So it's going to be held either every month or every other month. So I would expect it to be
held again at the latest in December, possibly October.
You're seeing this later. There'll very likely be another chance to get involved in a focused sort
of way. And even if there's nothing going on, just going onto the site and leaving some critiques
and trying your best to be a part of that discussion is the way people can get involved at any time.
By October, I'm expecting that we will have the voting system fully set up.
And we're also going to be making a sort of tutorial, kind of like Metaculous has,
where you can kind of get started. And in the tutorial, you can sort of earn points.
Timed multiple choice quizzes are actually pretty useful for getting some basis of how
of how much person expertise is. We're going to be setting it so that if you have less than 50 karma,
you can actually use the time multiple time and multiple choice quiz to get a bit of karma based
on how well you do in that course. And it's also a good benchmark for getting an idea of what topics
you need to learn, because you can actually see, oh, I got this question right, I got this question
wrong. And this question wrong was on this topic. So I can go and learn that a bit, learn a bit more
about alignment. Thank you very much for your time. I wish the best with the site. And I hope
people watching this go and engage there. This is a small way that you can get involved in alignment
just directly by participating in the discussion. There's enough focus here that you don't necessarily
have to be intimidated by the fact that like, oh, I don't know this field super well, you know,
you can still get involved. Beyond involvement, this is also just a way to get informed. Because
one thing I've kind of remember feeling in just hearing all the news of AI and all the capabilities
coming out, and you know, the kind of doom posts that Eliezer puts out, you kind of get this feeling
like nothing is happening in alignment, and like nothing's going on. And yeah, there isn't enough,
you know, given the kind of timelines that are involved, and you know, there's there's some
chaoticness. But there's also a lot of creativity. And there's a lot of people who are passionate and
and working on this sort of thing. And just seeing that and seeing the diversity of ideas, too,
that it's not just like one thing that's really hard. There's like a lot of different approaches
is a good way to it's really stimulating.
It's also something that's going to be useful is once we have more posts, especially with the
coming upcoming critique of one, if you can see what kind of problems are the most common,
what are the strongest sort of vulnerability which fans tend to have, if you're research,
you can save a lot of time, I'm going to make sure that from the start, I'm not going to have this
problem. Or if you're interested in solving stuff and building stuff, you can go, okay, this is a
problem which keeps coming up. I want to see if I can just tackle that problem. I want to see if I can
solve that problem. And then that makes a whole bunch of other plans better, because that problem
is not solved, right? Like suppose there's a technical problem, like a, you know, making a
very good simulation, right? That's very hard. And that's somehow a lot of plans are relying on that.
And you say, actually, I'm going to make progress on that. I'm going to tackle that and see, okay,
specifically, what about a simulation is hard, and I'm going to make that easier. Or you suppose
there's some other problem which plans rely on, and that's really hard, like perfect air gapping
or something like that. And you say, I want to tackle that. This site, you know, lays the
foundation for, but I can imagine becoming more significant later on is the idea of meta-analysis.
There's like people producing like, you know, base level research and base level like ideas. But then
some of the best ideas happen from seeing lots of different things and seeing the patterns between
them and seeing the commonalities and just the general, the inspiration that comes from that.
And in order to get that kind of inspiration, you just need to get a lot of the lower levels
sort of things fed in. And this would be a way of just kind of priming that creativity pump and
seeing all the different things out there. And hopefully that leads to new insights and inspirations
and ways of looking at the problem that just isn't available when everyone's siloed in their own
individual. Yeah. Yeah. Like popping the bubbles and getting all the work mixed into one thing and
focus has already been proven to be very powerful. Just somehow no one had just thought to make a list
of alignment plans before. So just all by itself, it's good for that. But obviously, as it evolves,
it's going to be more and more a useful resource that you can look at and take inspiration from and
make really good progress on the problem. Thanks very much for your time.
Thanks for having me. It was great being here.
