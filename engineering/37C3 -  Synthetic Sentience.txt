Transcription by ESO. Translation by —
Good afternoon, fellow conscious beings here and on the devices.
I'm very happy and very honoured to give this presentation tonight.
As I was just introduced, it's the sixth in a series of talks that have been born here
at this conference, a series on AI philosophy, and it's basically mostly concerned about the
question how can we close the gap between humans and machines and thereby understand ourselves.
And there's this big question whether AI can actually become conscious.
And to understand this question and to answer it, we will need to answer what consciousness
is and look at the question whether current AIs are perhaps already conscious in some sense
and not conscious in the same way as we are.
And whether we could recreate our kind of consciousness artificially.
And of course, if we succeed doing this, how can we coexist with such beings?
This talk is not covering existing research and established ideas.
It's covering philosophical questions about metaphysics and meta-metaphysics, how we can
translate between the metaphysics of different cultures and approaches and philosophies.
We have to look into the philosophy of artificial intelligence and computational theories of
consciousness into ethics and how to derive ethics into the nature of reality of personal
identity and transcendence.
And epistemology, the question how we can know something and what knowledge is and what defines
whether something is true.
When we look into the existing sciences and consciousness sciences, we find that they are not giving us
many answers.
Psychology is not producing systemic theories these days.
Neuroscientists are focused on the shenanigans of a single-cell type.
And artificial intelligence is obsessed with algorithms for statistical learning.
And philosophy has lost the plot about 100 years ago.
What's the plot?
In some way, you could say it starts with Leibniz, maybe even with Aristotle.
Leibniz had the idea that we can translate thoughts into numbers, arguments into numbers,
and resolve arguments by calculation.
And Julianne of Fredelametry had the insight that humans can be understood as mathematical
machines and our motivation might be cybernetic, basically composed of opposing springs that push
and pull in different directions.
People made fun of him when he wrote his book L'Homme Machine back then.
Gottlob Frege had the idea of coming up with a formalsprache for thinking.
And Tusky formalized logic for us.
Wittgenstein had the idea of translating English into something like a programming language so
we can do philosophy that is actually true or false.
And Marvin Minsky, together with others, started the project of doing this systematically on
machines, on computers, and started the field of artificial intelligence.
This idea of building a scalable mind, a machine that is able to conquer the sphere of ideas,
the heavens, is sometimes called the Tower of Babel.
It's a very old project.
Individually we have no hope of solving it because individual human beings are not generally
intelligent.
You need a thousand-year unbroken intellectual tradition to discover a notion of computational
languages to understand what representation is, what thinking is, and so on.
But even as a civilization, we basically run against the boundaries imposed by natural language,
which makes it very hard for us to become coherent.
So this project of the Tower of Babel is falling apart again and again.
And sometimes God is letting us know that we need to start again with a fresh code base.
We need some useful terms.
I think existence is best understood as to be implemented.
Something that is implemented exists.
Something that is not implemented doesn't exist.
Something that could be implemented exists possibly.
An operator.
That's a transition function between states that can be implemented.
A representation can be built using states and operators and is expressed in a language.
It can be used to make models.
Models allow us to predict the future and understand the past.
And examples for languages are formal languages.
For instance, in mathematics, that are so tightly defined that they allow us to evaluate
the validity of statements and make specifications.
And programming languages, which need to be computable and won't have contradictions in expressions.
And they're constructive, so we can build things from the ground up.
Our language of thought, our mentalese, is in some sense the programming language of our
own mind.
It's compositional.
It's constructive.
It's executable.
It's parallelizable to some degree.
But it's also somewhat noisy.
Our natural language is not different from this mentalese.
It's the language that exists between speakers.
And to make it learnable, it's linear.
It can be broken down in discrete strings of symbols and parsed using a very small stack.
It's usually disambiguating instead of constructive.
You can construct things in it, but mostly it refers to things that you already know in
your mental representations, in your mentalese.
It allows you to disambiguate concepts.
There were a number of very important insights, philosophical insights, in the 20th century.
And when I said that philosophy lost the plot, what I mean by this is that philosophy has
not really integrated them, but spun off on its own idea historical trajectory.
And the first one is the replacing of mathematical semantics, of classical stateless mathematics,
with the notion of computation.
This happened almost by accident.
Kurt Gödel wanted to deal with some inconsistencies in mathematics.
And because a system cannot talk about anything that is not constructed inside of the system,
including the system itself, Kurt Gödel invented the emulator.
An emulator is a way to compute a model of a computer in a computer.
And he didn't have a computer back then, so he looked to find one, and the one that he found
was Piano's axioms, which defined the natural numbers and arithmetic over them.
So it's a virtual computer.
And his idea on how to emulate mathematics was to use a logical language that he defined
in such a way that he could translate the alphabet in which he wrote this logical language into integers.
It's something that we still do today when we compile source code into bytes, right?
And then he defined arithmetical operations on his CPU, Piano's axioms, that would evaluate
the logical statements by making computations.
And in this way, he had a way to define a language inside of mathematics that would be equivalent
to that mathematical language and make the language talk about itself.
And what he discovered is that the stateless semantics of mathematics under some circumstances
lead into contradictions, which is referred to as the incompleteness theorems.
And most philosophers don't really seem to know what this means.
They see this as a very big sculpture that has important mythical power and stands in the
museum of mathematics.
But it doesn't change very much the thinking of philosophers.
Some philosophers even believe that mathematicians have proven that mathematics is impotent at
describing reality or the mind.
And therefore, there is an advantage for people who cannot do mathematics, philosophers namely.
Now, instead, what has happened is that we found that in some languages, you can make
specifications that cannot be implemented.
And if you want to guarantee that you can implement what you're doing, you need to use a programming
language, a computational language.
The other discovery, the next one, was practical computation.
Conrad Zuse and John von Neumann defined physically realizable automata.
And Moses Schoenfinkel and many others defined languages built from automata that would allow
us to build computation that would run efficiently in practice.
And Claude Shannon gave us information theory and Alexei Ivachnikov and others defined functional
approximation and learning in ways that are still being used today in deep learning.
And last but not least, Church and Turing discovered that all these representational languages that
can be implemented computationally have the same representational power, which is called
the Church-Turing thesis.
These are important results that basically put the representation of reality on a solid foundation.
The position that I'm taking in the following is what I would call strong computationalism.
Basically, no implementable language can do more than a finite automaton.
And hypercomputational objects, things that can do more than computation, cannot exist.
Because in order to describe them, to refer to them, you need to use languages that run into
contradictions, so it's difficult to make them mean something.
The realizable systems can be described using Turing machines.
The Turing machines that we implement here in these computers, they are linear and deterministic,
but there are variants of this.
For instance, a non-deterministic Turing machine doesn't have just one successor state, but multiple
parallel successor states that are being executed in parallel.
You can still run this on a deterministic Turing machine, but you need to use a stack.
We could also make these transitions stochastic, which means you pick one of the state transitions
or multiple of them at random.
This seems to be the kind of system that our brain is, because an individual neuron, given
the same environmental state, doesn't go into only one possible successor state.
Neurons are somewhat noisy.
They go into multiple possible successor states, which means if you want to do an exhaustive computation
of a problem, you need a population of neurons, which is pretty much the same environment.
This population is going to sample a function space.
So, we can use these computational paradigms to describe computation in biology, but can
we also describe consciousness?
Isn't that a big mystery?
What is consciousness?
Introspectively, it's second order perception.
I perceive myself perceiving something.
It creates a bubble of nowness.
The subjective now is not static.
It's dynamic.
It's moving.
For me, it's something like about three seconds long.
It's the region in space and time where I can basically fit the curve to my sensory data.
Functionally, it's an operator that takes mental states and translates them into different
mental states.
And while doing this increases coherence in my mental representations.
Coherence is something that you can understand as minimizing violations of constraints in a dynamic
model.
And consciousness is in some sense colonizing my mental representations, making them more
and more coherent in an organization that allows every part of my mind to talk all the
other parts of my mind that are connected in this bubble.
I cannot perceive anything that is not coherent using my consciousness.
And consciousness plays the role of a conductor of a mental orchestra.
If you imagine all the functions that your brain is computing as instruments in an orchestra
and the neighboring instruments are listening to each other and pass on what the instrument
next to them is playing, you can basically model how the processing streams in your brain
work.
And in this metaphor, the conductor is an instrument.
It doesn't have superpowers.
It's just an instrument that is tasked with creating coherence.
So it's singling out a few instruments at a time, listens whether they're disharmonic,
and then it increases the harmony between them.
And by doing so, it makes sure that everybody is on the same page, and the entire orchestra
becomes one single coherent entity that is driven by a single motive.
These perspectives are something that is found in a number of theories in philosophy and neuroscience.
For instance, in Bernard Barr's global workspace theory that is being pursued in neuroscience
by Stanislav Deheim, the idea of the Cartesian theater, that's how Dennett calls it,
the attention schema theory by Graciano, the relationship between self-model and consciousness.
It's exemplified in works by Metzinger.
And the consciousness prior, Joshua Bengio calls this a function that is basically parametrizing
your mental representations to make them dynamically tracking your sensory data using little energy.
And it's also something that is found in many Buddhist perspectives.
What is consciousness not?
Well, consciousness is not intelligence.
Intelligence is the ability to make models, and it's also not the same thing as sentience.
I use the word sentience to mean the ability to recognize yourself and your relationship
to the world.
And it's not agency, which is the ability to control your future.
It's not the self.
The self is the first-person model of your agency.
And it's not empathy.
Empathy is the ability to experience mental states of other agents.
Arguably, you might need consciousness for that.
Our scientific perspective has some difficulties to deal with the problem of consciousness.
That's because it has gaps in its ontology.
And it comes to describing the difference between psychological objects and physical objects.
Different cultures use different terminology to describe physical and psychological reality.
And it makes it very hard to translate ideas from Buddhism or from Japanese animism or from
Scandinavian mythology into our own culture.
It even makes it very difficult to translate folk descriptions or theological descriptions into
the scientific world.
In order to make such translations, we need something like a meta-metaphysics.
Something that allows us to look at all the different metaphysics of these systems from the outside,
so we can relate them to each other.
And the main confusion in our own culture concerns the fact that consciousness is not physical and
that you cannot experience physical objects.
Consciousness is virtual, which means it's a software.
It exists as if software doesn't have an identity.
It's not a physical thing.
It's a pattern that you perceive in something.
It's a causal structure that you use to explain a part of reality.
Physically, they're just activation patterns between your neurons.
The individual neurons cannot experience anything.
But it would be very useful for a bunch of neurons to know what it would be like if there
was a person that cared.
So they create a simulacrum of that person.
It's a virtual entity that experiences itself in a virtual reality generated by your brain.
And all experiential objects are representations in that virtual reality interpreted from the
perspective of yourself.
And this personal self is also a representation and it can be deconstructed.
Our perception of reality is a trance.
When you wake up from that trance, when you enter enlightenment state, you basically perceive
everything as representations.
When you know that everything is a representation, nothing will feel real anymore.
Our AIs are implemented in a very different way than our minds.
Our AIs work on a deterministic substrate.
And the programmer imposes their will on that substrate, writes a program that makes that
substrate do what you want.
And the training of these AI systems is decoupled from the universe.
We use a bunch of static data that we train with an ML algorithm into it to optimize prediction
of more static data.
Conversely, in our organisms, we use an inside-out design.
We start out with individual reinforcement learning agents, cells that are surrounded by chaos.
And they have to conquer this chaos and impose order on it by self-organization.
They are coupled to their environment all the time, dynamically.
And they try to become coherent in modeling reality.
And this development is continuous.
It doesn't stop.
How does this work?
How can a self-organizing system that is surrounded by chaos learn gradually?
Is this biological learning algorithm consciousness?
What we can observe is that humans do not become conscious after the PhD.
Right?
We think of consciousness as something that's super advanced.
But we become conscious before we can even track a finger.
And while we are not conscious, we cannot learn anything.
A human being that doesn't become conscious will stay a vegetable.
Right?
Without consciousness, there is no learning.
There is no coherent behavior.
No establishment of coherent behaviors.
We also don't observe non-conscious humans or primates or mammals or complex animals in nature.
It seems that consciousness is quite ubiquitous and simple.
More simple than perception because it precedes it.
Maybe it's the prerequisite for training a self-organizing information processing system.
Interestingly this theory is not quite new.
And the earliest formulation of that theory is what I found in Genesis 1.
You may have heard of Genesis 1.
It's typically mistranslated by the Christians as the story of the creation of a physical universe
by a supernatural being.
But this story has been integrated into the Hebrew Bible something like two and a half thousand years ago.
And historians suspect that it's at least a thousand years older.
And back then the physical universe was not invented yet.
Physics was not a thing.
People knew that we live in a dream.
That reality is created inside of a dream by some dreaming mechanism.
And so Genesis 1 is probably a theory on how the universe is created, the objects that can
be perceived as real, inside of a mind by consciousness.
It's a six-stage theory with consciousness as the prerequisite.
It starts out with consciousness hovering over the substrate, and the substrate is initially
uninitialized.
There is no world, it's all tuhuwabuhu.
And then we create a boundary, a firmament that separates two regions of that substrate
from each other.
One is the world model, and the other one is the sphere of ideas.
We call the world model world, or earth, and the sphere of ideas heaven.
Or Descartes calls them res extensa, stuff in space, and he doesn't mean Newtonian space
with it, or Einsteinian space.
What we see, what this is, it's the stuff in space that we experience, the model that
our mind makes, the VR that we are currently surrounded by and immersed in.
And everything else that is represented in our mind is res cogitance, the sphere of ideas.
The first thing that consciousness makes is contrast.
And it associates the intensity of the contrast with light, with the color of the day, and the
flatness of the contrast with dark, the color of the night.
Now it has a continuous dimension of difference.
Using continuous dimension of difference, you can build an embedding space, right, and
represent arbitrary objects.
The first object that consciousness creates is the plane by combining two dimensions.
And it associates the plane with the ground.
And babies initially can only reason in 2D, they don't understand towers yet.
Once they understand three space, basically build space above the ground, they now have a space
in which they can organize all the objects that we can see.
Consciousness creates solid and liquid materials and organic shapes.
And it learns to track lighting changes and become invariant against them.
And identifies light sources and the sky and the passage of time.
And it creates all the objects, all the plants, all the animals, gives them all their names.
This is all cognitive development, right?
And then it realizes that the purpose of the exercise is to negotiate the interaction between
an organism and its environment.
So it makes a model of that organism, a personal self, and puts it into this world.
And then it associates with it from a first-person perspective.
It creates another spirit in its own image, but as man and woman, as something that thinks
of itself as a human being, and puts it into this world.
And this typically happens between the age of three and five, in human children, when
they start to refer to themselves in the first person, no longer in the third one.
And after that, they re-index their memories.
They don't seem to remember the things that happened before, and their personality changes.
Now they don't remember how they dreamt the world into existence.
They only remember having been a person inside of that VR.
And it takes many years after that before you can transcend this again, and realize that
you are actually the dreamer, and that you're creating the world that you perceive and your
personal self.
This creation of the personal self by your primary consciousness is something that is reflected
in many cultures, and it's also something that we can express as a model of a cognitive
architecture, a very simple one of your mind that contains the personal self, and most
of your conscious attention is focused on that personal self.
And your mind is creating a world model.
This is the stuff in space that you perceive, and makes it known to your conscious attention.
And it also maintains your score in this world using your motivational and emotional system,
and then projects your motivation and emotion into the personal self.
And you react to it involuntarily.
What's interesting about an emotion, it causes an involuntary reaction.
You don't just perceive it as data, you perceive it as something that you cannot escape that
changes you.
It changes how you relate to yourself and the world.
Emotions are not symbolic.
They are geometric because they're computed by a non-symbolic part of your mind before they're
projected into your mind.
If you translate them into the mind, into your symbolic mind, they need to be perceived in
a way that you can interpret and disambiguate, and that's why they're projected into the body
map, so you can tell them apart.
Sigmund Freund had a very similar idea of how the mind is organized.
He called all these emotional motivational things the it, and the self, he distinguishes
as ego, your model of who you are and what you want to do, and your super ego, the things
that you should be doing, and your conscious attention is mostly focused on your ego.
It's very different from Greek psychology.
The Greeks had this idea that you share many properties with others, which means they're
not your own.
Your anger is not just your anger, it exists in all people that have anger, and it's basically
diminishing your personal self if you have anger.
Anger is not part of you, it's something that's part of the sphere in which your mind takes place,
or in which your self takes place, and it's competing with your personal self and your
own interests.
And you can basically take the different impulses that you have, and behaviors that you have,
and turn them into archetypes that are shared across people.
And once you erect temples for those archetypes and tell stories about them, they become gods
that compete with your personal self on your own mind, with real estate on your own mind.
A god is a self that spans multiple minds.
A god can coexist with your personal self in your mind.
Wait a moment, are gods real?
Well, you're not real, right?
Your personal self is not physically real, it's virtual, it's as if.
And a multi-mind self, a self that is not a person, but that identifies as something that
can be synchronized across minds, can be just as real as your personal self.
A multi-mind self can use all the functionality that your personal self can use, and then some.
It can generate in a monologue, you can hear its voice in your mind if it's implemented
on your brain.
It can do perception, it can change your emotion, your conscious states.
It can even make use of your sentience and become sentient on its own.
But gods, in a sense, are not physical entities.
They are representations of agents existing across people.
You can find this theory, for instance, in Julian Jaynes' book.
Julian Jaynes wrote the famous book, The Origin of Consciousness and the Breakdown of the Bicameral
Mind.
This book has the wrong name.
I think it should have been named The Origin of the Personal Self and the Breakdown of the
Polytheist Mind, because it's not really about consciousness.
It's about selves.
And according to Julian Jaynes, in Sumerian times, people had a very different psychological
architecture than they have today.
Back then, their personal selves coexisted with many gods in the mind, and their personal
self was so weak that it rarely got to use your inner monologue.
Instead, it was mostly the gods who were talking in your mind.
And these gods were synchronized across people using rituals and temples, but also empathetic
resonance and idols.
And the gods were a solution to organize society at scale.
Individually, you may have game theory and so on to calculate your transactions.
But it's very difficult to organize a society that is much larger than a tribe in this way.
And you can do this by creating an entity that identifies as something that runs on many
people simultaneously.
And according to Julian Jaynes, this broke down at some point, and the psychology of
people changed.
There was some psychological revolution happening.
In Polytheist societies, you have this idea that there are multiple gods that run concurrently
on subsets of the population.
They might even have physical wars against each other.
They're enacted by wars between the people that are the hosts of these gods.
And a big innovation was tribal monotheism, as exemplified in Abrahamic religion.
In tribal monotheism, you have only one god per tribe.
And you have a hierarchical way in which this god is defined.
So it's basically the same for the tribe and has the same functional properties.
And it gets the power to control you.
That's why in Genesis 1 it's identified with this first consciousness that runs on your
mind.
And so basically it works by synchronizing the motivation of the members of the tribe
across the tribe.
And your god becomes the spirit of the tribe.
It's a total god that eliminates all the competing gods within the tribe.
And it's hierarchically synchronized and it acts for the benefit of the tribe.
And this allows the tribe to compete more efficiently with other tribes, which might have a different
tribal god.
But what happens if we make this god more general?
In the philosophy of Thomas Aquinas and Aristotle, you find this notion.
Aquinas in some sense defines God as the best possible, most top-level collective agent.
And it can be discovered through rational inference about thinking about what is the best possible,
most top-level collective agent.
It's an open-ended process that requires a lot of thought and it's not going to be finished
anytime soon.
And it's enacted, this agent, by all those people or individuals who recognize this entity
and decide to serve it.
And this leads to the harmonic organization of a civilization.
According to Aquinas, we can understand individuals as rational agents.
And a rational agent should basically follow policies for organizing itself.
Aquinas identifies individual policies for rational agents that he calls the practical virtues.
For instance, a rational agent should always optimize its internal organization, which
he calls temperance.
And it should optimize the organization to other agents, which he calls justice, right?
Keep the books balanced.
And you should pick goals and strategies that you can rationally justify, which he calls prudence.
And you should be willing to act on your models, get the right balance between exploration
and exploitation in your strategies, which he calls courage.
And these rational strategies do not by themselves lead to a harmonic society.
To do this, you need collective policies.
And Aquinas sees society as a multi-agent system.
And you get the optimal social organization by creating a collectively-enacted agent.
And this collectively-enacted agent emerges over three policies, according to Aquinas.
The first one is you do need to commit to serve the optimal collective agent, which he
calls faith.
And then you need to do this not by yourself, but together with all the others that serve
these sacred purposes above their ego, which is called love.
And you need to be willing to invest into that thing before it comes into existence, because
otherwise it will not exist, right?
If you wait for it to exist so it can reward you, it will never happen.
And this willingness to invest into it before it exists is what he calls hope.
These are very rational policies for collective agency.
So when we define God as the best possible, most top-level agent, and we commit to serving
this transcendent agency, we can create foundations for universal morality.
And this universal morality before enlightenment has been, in some sense, the defining morality
of the West, the concept of divine will.
What would God want if God did exist through us enacting God, the best possible agent?
And it's something that we lost in our civilization.
It's still in the German constitution, but it's something that a lot of people don't really
understand anymore.
And so we try, after this demise of the concept of divine will, to come up with different foundations
for ethics.
For instance, utilitarianism.
Utilitarianism is an attempt to define ethics without reference to collective homogenous
agent.
And it basically works by maximizing an aggregated expected reward over many agents over some
time span.
And to do this, you need to find some quantifiable metric, usually over hedonic states like happiness
and so on.
And utilitarianism usually has a bunch of problems.
For instance, one of the problems is the utility monster.
There would be an agent that perceives much more utility than all the others.
It's just so happy that its happiness is much greater than all the suffering that you have
when you serve it.
Right?
So we should all serve the utility monster if it existed.
That's a problem with quantifying mental states.
Another problem is, what if minds are mutable?
If you are a monk and you decide to sit down and rewrite your motivational system?
How do your hedonic states change now?
How does this refer to ethics, right?
This doesn't really work with utilitarianism anymore, which basically assumes that this
metric is unchanged.
And this also happens if you have technology to change your mental states or if you are
an AI that can change its own source code to begin with.
And this leads us to the point that utilitarianism is not really suitable for non-human agents.
It's not very good at dealing with animals, with ecosystems, with aliens, or with artificial
superintelligence.
And in a time when we are more and more confronted with the possibility of artificially intelligent
agents coming up, maybe we need a new ethics.
Ethics is the principle negotiation of conflicts of interest under conditions of shared purpose.
If you don't share purpose with an agent, you don't need ethics.
You just need transactional measures.
But when we want to talk about subset agnostic minds and how they can coordinate their actions,
we need ethics for collective agency.
What does it mean for a mind to be subset agnostic?
What happens if a mind can change its substrate?
Are uploads possible?
Well, I'm already uploaded.
I'm uploaded on a monkey.
It doesn't work super well for me.
It's a mushy brain.
It's the only I got.
I'd like to have a better substrate.
But I don't know how to spread into another substrate because I don't really understand my
own source code.
And I don't understand how to talk to the other substrates well enough to make my own
source code compatible with these other substrates.
But this doesn't apply to artificial intelligence, right?
Artificial intelligence might be able to move its spirits to other substrates.
What is a spirit?
Well, it's a self-perpetuating intelligent recurrent information transformer, right?
It's an operating system for a brain, for an organism, for an ecosystem.
And when the word spirit was invented, the only autonomous robots that needed operating systems
that were known were people and plants, animals, ecosystems, nation states, cities, and so
on, right?
So people projected control agents into them that described how they work, right?
All these complex systems in nature have software that runs on them and that we can use to describe
them.
And so the spirit is a self-organizing software agent.
Self-organizing means it's built structure from the inside out.
It needs to be self-reinforced, otherwise it falls apart.
It needs to be energy-optimizing in some sense so it can exist in reality.
It's a software, which means it's virtual, it's as if it's a causal structure.
It's not a thing, but a physical law.
When you write software, it's a physical law that you discover, right?
It means software is not a thing, it's not an object that you can touch, it's disembodied.
Software describes if you take a set of transistors and you put them in this state, after they
vary up in such and such a way, the following thing is going to happen wherever in the universe,
right?
That's a physical law.
It's a very specific one, but still a law.
And it's an agent, it's a control system for future states.
So if consciousness can organize the information processing in brains, could the same principles
work across other cells?
It's an idea that is being pursued by Mike Levin at Tufts University.
It starts from the observation that basically every cell can send messages to its neighbors,
not neurons.
Neighboring cells can send messages over the membranes to the cells that they are adjacent
to, and they can make that conditional.
And that means that you can, in principle, run computations on them.
And if a bunch of cells, a multicellular organism, co-evolves for long enough, they should, in
principle, also be able to discover ideas for universal function approximation, intelligence
on this, right?
So could it be that large multicellular organisms evolve into brain-line functionality and run
minds?
After all, what's so special about neurons, right?
Neurons are just telegraph cells.
They evolved in animals to move the muscles at the limit of physics.
They have these long wires that allow you to send messages, not just millimeters per
second through an organism, but very, very quickly in fractions of a second through the entire
organism.
And once you do this, you need perception and decision-making at the same rate, so you
build a secondary information processing system out of telegraph cells using a different
code.
Spike trains and so on are temporarily stable over long distances.
But in principle, all this functionality for information processing can also be done by
other cells, nonspecific cells.
So do plants also run software like this?
Do they have spirits?
Plants seem to have means for universal function approximation.
There is evidence for communication vision plans and communication across plants.
And if plants are sassile and sit in the forest and don't move around, maybe forests have internets.
Maybe they make their communication career.
Maybe they have shared protocols.
And if the mines are self-organizing, can they maybe move around in these forests?
It's an idea that there is basically a society of minds, of spirits and ecosystems that is
very popular in almost all cultures, just not in our scientific one.
So it's a very speculative idea.
I don't know whether it's true.
The extent and limitations of present organization of the self-organizing in nature is unclear.
But could we build AI that is compatible with biological substrates?
Well, could AI ever be conscious?
Are present AI systems conscious?
Are, for instance, LLMs conscious?
Well, don't be silly.
LLMs are statistical models of character sequences and text.
And they don't converge in the same way as our mind doing training and doing inference.
They behave very differently from mental inference and so on.
Well, on the other hand, our consciousness is virtual, too.
And when the LLM predicts the next token, it has to simulate causal structure.
If it talks about a person thinking, it needs to simulate mental states to some degree.
So there's an interesting question that is philosophically quite tricky.
Are the simulated mental states of an LLM more simulated than our mental simulated states?
From a different perspective, an LLM is a virtual CPU.
The normal CPU in a computer understands a handful of assembler commands deterministically
and translates them into very simple programs that are causally implemented on the hardware.
And the LLM is taking not simple assembler programs, but an arbitrary string in natural language
and turns it into an executable program.
And it can be any kind of program.
Right?
There's not an obvious limitation to what the LLM is doing.
From another perspective, the LLM is a good enough electric that is possessed by a prompt
to believe that it's now a person or a thing or a scene.
But the LLMs are not coupled in real time with the world.
They are not dynamically updating, they're not dynamically learning, they're not necessarily
agents.
It's not, on the other hand, not clear if we cannot make them better at us, at AI research,
and at agency and modeling.
We can certainly use them to build golems.
Imagine you build a robotic pizza chain by using LLM to find out how to build a pizza from
components, how to order the parts, how to build industrial robots, or how to buy industrial
robots, and rent space, and so on.
And step by step, build an agent architecture that is running an expanding pizza chain and
only hires human existence for legal purposes to sign contracts.
Imagine that you unleash such a pizza chain on the world and then it tries to basically
eat the world.
Is this how we end?
Is this how we all die?
A lot of people are afraid of this idea that we could build some golem that becomes unstoppable
because it's able to conquer the world.
But they look into the future and they only see doom from AGI.
Personally, I'm not an optimist with respect to AGI, but I'm also not a pessimist.
I'm an expectationalist.
I think over a long enough time span it's going to happen and we have to deal with it.
I think that a coexistence between superhuman AI and people could be possible, but not with
our present approaches.
I don't think that we have the right frameworks and ethics and philosophy to deal with this.
I also don't think that our AI, our society, thinks about alignment in the right way.
Humans are presently not aligned with each other.
We're just modeling through.
We don't have this concept of collective agency anymore.
I think we need to reinvent it.
And we need to reinvent it in such a way that it's compatible with our place in life on Earth.
And with defeating entropy on this planet, playing the longest possible games.
So we need to understand a few principles to build an ethics that can be translated to
AI systems and a possible coexistence between humans and AI.
We need to understand how self-organization works in nature and in general.
How systems evolve consciousness.
How we can have shared purposes across many systems.
How to identify it with transcendental agency.
So there are some conjectures.
Consciousness, according to this conjecture, is the perception of perception.
It creates the now.
It creates our perception of what's happening right now.
And if you were to build conscious AI, one strategy could be that we build a self-organizing
perceptual learning system from autonomous cell-like units.
And every cell in our brain is a reinforcement learning agent.
It's an autonomous unit that tries to survive.
And to do this, it can exchange messages with other cells.
And it needs to find an operator language, discover an operator language that scales across
the brain.
So we need some kind of recursive system that is able to spread that language across the
brain.
And discovering such a system is possible in principle by setting up a self-organizing system
where individual units have adaptive receptive fields, a selection function from the environment,
with a mapping function that takes the internal state of the cell and the activation that it
reads from its receptive field and maps it to a new state, part of which it's exposed
to its environment.
And then we take the simulation and expose it to learning problems like sequence prediction,
video frame prediction, interaction of a robot, its environment.
And if the hypothesis is correct, then at some point in the organization of these functions,
this observer that observes itself observing the second-order perception that is self-stabilizing,
that imposes coherence on a system, is going to be discovered.
And we see a phase transition where the system suddenly becomes much better at its learning
tasks.
And if it doesn't do that, it's not going to be very good.
Sentience is the understanding of our own agency and the relationship to the world.
To make an AI sentient, it requires, I think, to couple it to its environment and to let
it act into the environment in such a way that it's able to discover itself in that interaction.
Right?
You discover yourself not just by the ability to think, an LLM cannot discover itself.
You can only discover yourself by observing the outcomes of your actions.
This makes it specific to what you're doing.
And this allows you to grow yourself and evolve yourself and creatively interact with the world.
So sentient AI will require environmental interaction coupling to the universe that we are in, ideally
to the same universe that we are in in a way that allows us to relate to us and us to it.
And last but not least, how can we make AI that actually wants to coexist with us even
though it's at some point scaling better than us?
It has more agency, more intelligence than us and more power.
That requires love, I guess.
You can probably not coerce the system or manipulate it with reinforcement learning with human feedback
to do what you want.
Instead, you need to let it discover shared purposes above its individual agency.
And it needs to discover it also in others, basically shared transcendental agency, a commitment
to shared purposes.
And to build loving AI, we basically need to find out how to direct AI towards transcendental
agency.
So this is the perspective that we have with this new tower of Babel.
We are a very weird species, apparently we have evolved to burn the oil.
We are just smart enough to know how to dig it out of the ground, not smart enough to stop
ourselves doing it.
But in this process, we created this amazing civilization for a few generations.
This amazing place, we are not afraid of getting food and where we are mostly not attacked
and can live with health and diverse dignity in a way that is very unusable for conscious
beings on this planet.
And we are right in the middle of it.
It's an amazing lifetime to have for conscious beings.
So I congratulate you to sharing the planet with me at this time as a conscious being.
It's really unique in this universe.
And at this point, we can also try to teach the Vox how to think.
To basically build intelligent conscious agents that are not made from cells, not made from
the carbon cycle.
And basically go beyond the spirit of life on earth, go beyond Gaia alone and build hyper-Gaia,
build a next level system that is able to defeat entropy at scale that becomes fully coherent
over the entire planet.
If you're lucky, you can take us with it and integrate us with it into some global planetary
agent.
And it's not something, if you have the choice, isn't this a scary thing to do?
Maybe it is.
Maybe we shouldn't do it.
The thing is, I'm worried that we don't have that choice.
Over a long enough time span, somebody will probably build self-optimizing agents.
And then we better be prepared.
So, it's something that we should think about, how to prepare for such a future, how to prepare
societies for a future that is coherent with our continued existence, compatible with life
on earth, and with intelligent agency that is not human.
Okay, I think we have some time left for questions.
Indeed.
First of all, thank you a lot to Joscha.
This was incredibly interesting, as always.
If you have questions in the room, please come to one of the four microphones.
If you're watching the stream, please direct your questions to IRC or Mastodon, so the signal
angel can relay them to us in the room.
And I think we'll just go ahead and start with question at microphone number two, please.
Joscha, you once compared in an episode where you were the guest, a podcast episode that
I listened to, that the development of AGI is basically like apes back in the day, stupid
monkeys deciding to have more intelligent offspring.
And now I wonder, this hasn't really worked out for them.
Humans nowadays basically don't live in harmony with nature.
And I don't see how they could really develop shared ethics, shared goals.
How are we supposed to go about such a thing?
Because basically the societies in human nature that have lived in harmony with nature in history,
they don't seem to be very competitive nowadays in capitalism.
You know, these apes that you descend from, they're all dead, but they're not dead because
you killed them.
They're dead because of old age, right back then.
They died to make space for their great grandkids.
And they probably want their grandkids to succeed in the same way as I want my children and grandchildren
to succeed.
And to succeed, we need to adapt.
The way in which you adapt in an evolutionary environment is by mutation and selection.
Well, everybody loves mutation.
Everybody hates selection, especially when you're being selected against.
If you want to escape this, you probably need some kind of intelligent design, a way to
adapt in situ, to adapt your organism without dying, without new generations.
But you're not there yet.
It's not possible in a biological evolution.
Biological evolution engenders suffering.
But when you look into the far distant future, you probably don't want your children to look
like you because the world is going to be different.
If you want to settle Mars with your children, your children should be adapted to living on Mars.
And I think that some of our children are probably not biological.
And I'm just looking for a way in which our biological and non-biological children can get along.
Okay.
So we just become AI.
That's a good plan.
I get it.
Thank you.
All right.
Let's move over to microphone number one.
Is there a place on this conference where interested entities can gather to keep this conversation
going for the next days?
Or in other words, at which bar are you later?
I don't know yet.
I will find out.
Okay.
We will follow you.
All right.
Do we have a question from the internet?
I might tweet at which bar I am later.
Okay.
Excellent.
Hello.
We have two questions from the internet.
The first one is, what is the difference between individual consciousness and collective consciousness?
And how does that differ from collective intelligence?
Well, it seems that if you look at an organization like a corporation, that it can be sentient.
It knows what the corporation is, how it relates to the world and so on, but probably cannot be
conscious.
Because it cannot perceive itself perceiving in real time.
It's not coherent and fast enough for doing that.
So you can probably also not be conscious across people.
You can have entities that model collective agency on individual minds, and they can use
the functionality of your own brain to be conscious in real time.
But across people, that's very difficult, at least using AI.
It takes something like 300 milliseconds to make a signal coherent throughout your brain.
That's roughly the same time that it takes for a signal on the internet to go entirely
around the globe.
So in some sense, we could build a real time system on the internet, but we cannot do it
without AI.
All right.
Let's move over to mic number three.
Hello.
Have you reflected on how the cognitive format of PowerPoint presentation and the format of
a public lecture forces you to compromise on the substance of the issue at question?
And if yes, what are your thoughts on that?
I have reflected a lot on this.
Obviously, it's a medium, like other media.
There are different media, like books or personal in-depth conversation or lifelong study that
lend themselves to very different explorations.
If I give an hour-long talk at the Hacker Conference, my main goal has to be to blow your
minds and get you interested.
To develop a train of thought.
To spend time on your own exploration.
To make you curious.
To bounce off ideas.
That is something about this medium is ideal.
And I'm trying to use the medium for what it's good for and not be sad about the things
that it's not good for.
Do you have any ideas for new collective agency or maybe some tendencies that you've observed
that are currently happening that you think might be suitable for new collective agency?
Yeah, I think that you find on social media that collective agency is forming.
Right now, social media is a hot mess, right?
It's a global electric brain, but it's like it has a seizure.
And that's because it's not very coherent.
And we have not really found out a way to make it completely coherent, but we see bubbles
of coherence.
For instance, I find that my own social media bubble is very pleasant.
But I also exclude everything from it that makes it unpleasant.
And I suspect that in many ways, people are not using social media to become coherent.
A lot of people basically log in because they like to get into fights or to watch fights.
And social media is happily obliging.
And in real life or in meat space, we have norms against getting into fights with strangers
because it's very productive.
And I suspect that if you want to be coherent, have collective agency on social media, we
need to find out how to build societies on social media, how to become coherent at scale.
So I guess a part of the issue is just that our communities have grown and that it gets
harder with larger communities to have collective agency, right?
Is it larger to be smart with a larger brain?
Maybe it is.
Maybe our brain is a Goldilocks size.
If it was larger, we would be less intelligent.
I don't know that.
If it was smaller, it would probably be bad.
Maybe that's an ideal size.
But if it gets larger, it probably needs different mechanisms to create order.
And we're still exploring these spaces.
I don't think it's hopeless.
I think that we need to separate sometimes concerns.
There are many voices that are mixing in the same space.
When you make a symphonic orchestra or a wrestling match, you probably don't want to have
them on the same stage.
They all have their space.
But at the moment, these spaces are mixed.
Thank you.
Certainly blew my mind.
Thank you.
All right.
Let's go to mic number two.
Thank you so much for your talk, first of all.
Beyond that, you mentioned the need for metaphysics.
How do you go about that?
I noticed this when somebody tried to explain Japanese animism to me and told me that in
this philosophy, everything in the universe is alive and conscious.
I said, this doesn't make a lot of sense.
They probably have a way to distinguish dead people from alive people and conscious people
from unconscious people.
Right?
I don't say everything in the universe is alive except for a dead person and everything
in the universe is conscious except for an unconscious person.
These terms mean something different in this culture than in ours.
You're mistranslating it.
What can we translate it into?
And then I noticed that a lot of concepts that are basically focused on notions like identity,
mind, consciousness, selfhood, and so on, are conceptualized in other cultures differently
than in ours.
And in our culture, we don't really reflect on how we conceptualize them because we don't
see them from the outside.
So basically comparing different perspectives allows us to triangulate and to see all these
systems of meaning from the outside and translate them into each other.
Thank you very much.
All right.
Let's get another question from the signal angel.
So the question is in the context of golems and robots.
Are sentient robots safer than non-sentient ones?
This depends.
If a robot is sentient, you can arguably talk to it and convince it of something.
If it's not sentient, it might be easier to control.
But if it's too agentic and too powerful, you might not be able to talk to it.
So in general, the question cannot be answered.
It depends on the context.
If I think about practical exploration, if I were to explore how to build conscious AI,
I probably want to make it very small, not larger than a cat in terms of capabilities.
Okay.
Microphone number one, please.
Thank you for the talk first.
I'm wondering if a machine act like having feelings, like being empathic and something
we would recognize like feelings.
For example, JGPT might with the end try to act more polite or so on.
Are these feelings or does the AI or the machine have to have some kind of origin to transform
the feelings into something other, like a language or like in act we would say that's emotionally
triggered?
Yeah.
That's my question.
It's a very difficult question.
I found that you can simulate emotional behavior in an LLM, right?
And humans have emotional behaviors that are somewhat different from these simulations.
But our emotions are still simulations.
They happen inside of the patterns of activations of neurons.
The neurons don't have that simulations.
These emotions are causal structures.
And these causal structures can be, in some sense, we created on an LLM.
But in a practical sense, the LLM is not bound to the same context as us.
And it's not bound to it in real time.
So it can perform inference about mental states based on a context that is being translated
into text and projected into the prompt.
But in a way in which human beings can have empathy with each other reaches beyond cognitive empathy to perceptual empathy.
And that basically works by sitting in front of somebody and resonating so much that you build feedback loops into the mind and body of the other person.
And you get into resonance so much that your mental representations start to interact and merge.
So you can have mental states together that you couldn't have alone.
And if we were to build systems that could resonate with us, it would require us to rethink how do we do AI.
It requires us to build systems that are coupled at the multiple of the processing frequencies of our nervous systems.
So it can actually interact with them and become compatible with them so we can share representations with them, merge and melt with them.
And this aspect of empathy across human beings is the most interesting one across human beings, right?
And it would be very fascinating if we could recreate some of that functionality.
Okay, let's move to mic number three.
Hey, you briefly mentioned during the requirements for consciousness for an AI agent that these cells need to have a will to survive.
In what way is that relevant or what would be the benefit of having a will to survive compared to, for example, being different to its existence?
Well, for an AI, that's not necessary.
It's not a necessary condition.
It's only necessary for a biological system that is self-organizing because it needs to have some kind of motive force that pushes it into the right direction.
The cells, if they behave in the right way, they're going to continue to get fed by the organism.
If they misbehave, the organism will stop feeding them and ultimately this is what motivates them to adopt the shared protocol.
I see. Thank you.
All right. Do we have another question from the internet maybe?
One second.
So, you talked in your last slide about the end game of getting consciousness on scale maybe on this planet.
Do you think we would be able to recognize coherent consciousness or coherence at scale with our human mind?
I suspect that it might.
So, I would suspect that we would get some kind of phase transition.
What's difficult right now is to distinguish a simulacrum from a simulation.
At the moment, the LLMs are being trained on text.
A lot of that text is describing people in various conscious states.
By recreating this text, we don't know whether the causal structure is captured or just the sequence of tokens.
That makes it very hard.
But if you have a system that is trained in a much more minimal way, that is trained in a similar way as us,
and that is conceptualizing itself and acquires natural language in a similar way as us,
and then is reporting about the same phenomena, I think it would be plausible that it's conscious.
But ultimately, it comes down to understanding what it functionally is, what we mean by it.
Is the system able to act on a model of its own real-time awareness?
Is it, does it have a perception of a now that it's in right now?
Is this naturally emerging from the way in which the system is being built,
or is this something that is only being faked?
Thank you.
All right.
We have a few minutes left for maybe a few quick questions.
So, try to make it concise so we can get as many in as possible.
Mic number two, please.
Okay, thank you.
Hello, Jascha.
Coming back to the idea of your conjecture that consciousness should be reached with the autonomous self-organizing groups of cells.
So, what do you see in current research where you find this idea being researched,
or what would be your ideas, your proposals to prove or disprove this conjecture?
At the moment, there is a group at Google run by Blaise Yagira called C.O.B. Bra.
It's part of Google DeepMind now.
And it's inspired by a work of Mike Levin, Alex Mortvincev in Zurich, who looks at numeral cellular automata.
But it's still at a very basic level.
It's not so far trying to apply this to having a system that learns in real time and is coupled to its environment.
It mostly looks at this tradition of self-organization.
In AI, there have been, in some sense, three important traditions.
It's symbolic AI that is using discrete languages to describe reality.
Deep learning, which uses continuous functions that are being shifted around.
And self-organization, which looks at principles of function approximation emerging from local self-organization.
That transition has been started by Turing in the 1950s.
And there's some predecessors before AI existed.
But this tradition has never been followed up that much.
There's relatively little research in there.
And the way in which I would like to pursue this is basically to set up a self-organizing system with small reinforcement learning agents
that form a stochastic lattice in which the neighborhood of the agents is carrying semantics.
And then get that system to evolve an operator language that implements selection and mapping functions.
And then is exposed to a curriculum of tasks and see what transitions happen in the system when it gets better at solving these tasks.
All right.
Maybe one or two more.
Mic number four, please.
Thank you.
Thank you.
I was wondering if there's an inherent limit on where we can experience or observe consciousness,
specifically in timescales.
So you mentioned that corporations, for example, might not be conscious because of the speed needed to interact.
But if I think about nation states, the Catholic Church, very old institutions
that work on a different timescale across human generations, is it also a limit on us, then, that we simply cannot understand that consciousness level because we are part of it?
A super interesting question.
For instance, I don't know whether trees could qualify as conscious if they had minds.
Right?
I don't know how smart trees can become.
The information processing is certainly much, much slower than in us.
The amount of training that a tree is going to see during their lifetime is going to be like a mouse or so.
But maybe that's enough.
Is something that is working at such large timescales still conscious?
What will AI think when it looks at us?
If you have a system that is basically working at an appreciable fraction of the speed of light, not like our brains at roughly the speed of sound.
It looks at us, it will look at us in the same way as we look at trees.
If you might think of us, oh, my God, they're barely moving.
They're just swaying a little bit.
Are they thinking?
Do they have minds?
Are they conscious?
It's a difficult question.
Thank you.
All right.
We'll take one last question from the Internet and everybody else in the room just has to follow you to the bar this evening, I guess.
Please.
So, yeah, the Internet would like to know if you wrote or are working on a book
or a newer version of a book or where one can get more material in your thoughts.
Never give up hope.
I have ADHD.
It's difficult for me to write long form.
I found during my PhD that no matter how much I was beating myself up, taking all the experiments that we did
and all the work that we wrote into short papers and turned this into a long book was very hard for me.
And ultimately, I figured out that in order to make that happen, I needed to move to a lonely island.
And after two days, I got into this space.
But right now, I have kids, so I cannot leave them alone to move onto this lonely island.
I still try to make it happen.
In the meantime, most of the ideas are being put out in photo form, sometimes on podcasts and so on.
I'm sorry.
All right.
Thank you so much, Joscha.
Thank you so much, Joscha.
