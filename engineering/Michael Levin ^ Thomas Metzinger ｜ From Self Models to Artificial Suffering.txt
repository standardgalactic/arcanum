Good evening everyone and welcome to the Great Minds series. Tonight we have the honour of
learning from two brilliant thinkers whose work has not only advanced the field of consciousness
but has reshaped our understanding of the mind itself. Our first guest, Professor Thomas Metzinger
and joining him is Professor Michael Levin. Thank you both for being with us tonight.
So maybe to start, I've been saying for a long time that selves do not exist and I come at
this of course from a very different angle than Michael does. I come at it from the consciousness
angle. I've been in this consciousness thing for a long time and I think the first thing
one has to understand is that consciousness is not one problem but many. So there's a
question say of global integration, there's a question if phenomenal atoms exist, smallest
units of experience. And I thought of all these many problems, the most difficult one
is the subjectivity of the target phenomenon that as many people claim, it is always tied
to first person, to a first person perspective. I think that's false. But the question was,
what is a first person perspective? And how could it evolve in natural systems, right? So one
concept for instance, for me, Michael talks about epistemic agents, I have recently begun
talking about epistemic agent models, that is systems who model themselves as a knowing self. But I've done all this under the assumption,
that no such things in the world exists. And now there are basically two people, two kinds of people who disagree. Some people say, yes, selves do exist, and they talk about selves. And then there's Michael, who even spells self with a capital S. And that is maybe a good point for us to jump off.
I think if Michael's very interesting and very ambitious project gets through. One of the most important things is the principle of parsimony that we don't unnecessarily introduce entities that we don't need for explanation. So I've always thought, with some qualifications, you can explain the phenomenology of selfhood, and every
everything else without assuming the existence of self. But I see that in Michael's writing, self plays a big role, self with a capital S. And maybe you could explain this a little bit, Michael, what you mean by this, and what role it plays, and so forth.
Well, okay, a few kind of a few background things that I wanted to say, first of all, you know, my main bread and butter, at least at this point is not consciousness research, right? So typically, I mean, I have some thoughts about it. And I think I'm working on some things along these lines, but I do not have a special new theory of consciousness that I've been pushing or anything like that. So the vast majority of things that I've been writing about are much more practical,
practical, conventional third person accounts of cognition. So what I what I do, you know, have strong things to say about are what I think about different types of systems and how we interact with their cognitive properties.
You know, consciousness is a different problem to which we have not done that we have not added anything yet. So that's the first thing. So anything I say on this on the consciousness aspect of it is very unbaked at this point is what I'm saying.
The second thing I want to say is that, you know, you mentioned something quite interesting, which is the how do these things evolve? And I just want to leave my cards on the table at the beginning that I think cognition and consciousness are a superset of living things.
In other words, in other words, I think they were here before life and they do not fit within the what I think is a smaller set of things that we currently call life without getting hung up on the definitions.
I think that things that we call life are things that are good at scaling up these very simple cognitive properties. Life is very good at enlarging what I call the cognitive light cone.
And so and so, you know, it's more advanced versions, at least for now, those two things track each other nicely.
But I think as we go to the bottom, quote unquote, the bottom of the spectrum of cognition, I think we we come upon things which are already on that spectrum, but would not be called alive.
Called alive. And so so what I see the job of us when we when we talk about evolution is not to show how it shows up for the first time, because I think by the time you get to biological evolution, it's already here.
I think. Right. I think our job is to show how does it scale and how do we understand it, interact with it and things like that.
And I guess the the the the last thing I would say about parsimony is I like parsimony as much as the next person.
But I think we need to be careful that our job, I think I think our job is not simply to explain.
Our job is to to to discover and invent, which means that I can imagine scenarios in which we pick the most parsimonious model, but that model does not facilitate the next round of discovery.
In fact, we've made it we've made it, you know, we've made it so parsimonious that it's it's very good for what we have now, but it doesn't immediately show us what the next right what the next step is.
So so I'm not super tied to parsimony, although although, of course, you know, we don't want to multiply ideas unnecessarily.
But but I just want to be clear that that I like models that help us in this in this journey of discovery and invention, not simply explain the things that that we've had before.
Yeah, that is actually one thing where the two of us really strongly agree.
I think the value of a still metaphorical description really consists in its heuristic fecundity.
So will it help us to actually generate novel prediction or control phenomena?
And I mean, good philosophy could even do that, even if it uses somewhat fuzzy high level concepts, it can inspire people.
That's a criterion does something enable progress.
But when when when you spell like when you say there's say in a large nested system causal structure, there is a new functional hole and you call this self with a capital S.
So what is the fecundity?
Yeah, I will give you what I think of as a definition with what I mean by it.
But I also think we should talk about what we expect that word to do for us, because it's entirely possible that that the thing that you are deconstructing here is not, you know, that we may well agree on that.
Right. So we have to we have to talk about what it is that we are.
What exactly do we mean when when we say that that they're ourselves or not?
So here's here's how I have used it for me, I've used it as a term that emphasizes first person perspectives that include valence and attention and decision making of systems that are the owners of goals, preferences and memories that don't belong to their parts.
And I can give you some examples, but the point is that a coherent self is an emergent whole that is kind of it's implemented by an interlocked triad of three things in option space with or a problem space within which it operates a cognitive light cone that demarcates the size of the goals that it can pursue.
Okay, so what is the size of the largest goal that this thing can pursue in a set of cognitive processes that allow the system to navigate that space with some degree of competency.
I see selves as systems that fundamentally live forward in time, that is, they are attached to not so they so they may have memory, they're not attached to the the past interpretation of whatever memory traces exist in their body.
But but they're fundamentally a decision making system that has to be creative in deciding what do my memories mean now.
So and when we can we can we can get into get into that whole thing. So there, there, there are collection of parts that have in some way, found alignment or bought into the same story about themselves and the outside world.
And there there's there's there's there's there's some other stuff about how they interpret and and their own memory and grants and things like this. But that's basically what I'm going for. So this is, you know, the what what I've just described is, is not necessarily have all the features of the things that other people mean when they say selves and so it may well be so I don't know, you can tell me if we're if we're if we're in agreement or not.
Yeah, maybe we shouldn't get hung up in these more unimportant details. I don't know if you should maybe explain your concept of a of that cognitive light cone to the audience briefly.
But before you do, I'll ask you a question right away. So for a very complex system like the two of us, for instance, couldn't one distinguish between an epistemic light cone and a pragmatic light cone?
So in the say the epistemic surface and maybe the computational surface to the world that I can use an active inference so that I can think or cogitate about things far distant in the future as a human being.
But these are very obviously things that I can never interact with causally. So there is something I have a pragmatic cone in this embodiment, but I do also have an epistemic cone which may be much larger. Does does that fit into your conception of the light cone?
Good point. Let me let me describe what I have in mind and you can tell me which of these you think you think I mean.
What I don't mean are the things that I don't mean anything like a like a radius of a sensory motor boundary. So I don't mean the thing, you know, the James Webb telescope has an enormous sensory reach, but but that's not what I mean.
And I don't mean the things that are practically achievable by you. In fact, that's what I think is something that's interesting about humans is that maybe uniquely we have a cognitive light cone that is for sure bigger than our lifespan.
So maybe maybe maybe that's unique. What what I mean by the cognitive light cone are the type of the the size in some space, but let's say in the 3d space, we can we can collapse it to space and time.
The size of goals that you can represent in a in a in a whole in a goal in a goal directed action loop, the action loop is this simple cybernetic thing where you sort of measure your current state, you have some record of the state you're trying to get to, and then you try to minimize it.
The error and you try to get there. So so let's just let's just think about different different types of creatures that have cognitive light cone. So so if if if you have a dog,
clearly it has it has some memory and some predictive power and so on. But I don't think and this is an empirical question, but I don't think you can get your dog to care about what's going to happen three months from now in the next town over right it cannot it cannot
never mind the practicalities of whether we can achieve those goals, but it cannot represent those goals, right, the goals that it has are much smaller both both in space and time.
So if you have a bacterium, the cognitive light cone is even smaller.
So so I don't know, maybe 20 minutes forward and back in terms of in terms of the time axis and and it's you know, it's it's it's measuring and it cares about the sugar concentration in a very small space.
Right, right. The reason that and so and so if you have a human, you can represent goals that are enormous, you can work towards the world peace and you can think about the financial markets 100 years from now and whatnot.
These are right so so the scale of the goals that you can represent as a goal in some homeostatic process and the reason that that these things matter.
So now now here we get to this issue of fecundity. So with this, so in my lab, what we try to do is we try to take some of these philosophical ideas and embody their implications in practical inventions of various types.
So so here's how something like this ends up being being very useful, the story of cancer.
So one story of cancer, which is different than the conventional story of cancer, is that cancer is fundamentally a dissociative identity disorder of the cellular collective intelligence.
That basically what happens is that evolution has taken in individual cells, you know, unicellular organisms with a very small cognitive light cone, they only care about a local their own local physiological states and things like this.
And then through some very interesting mechanisms, which we've studied, they assemble into larger systems with much larger cognitive light cones, for example, the ability to pursue specific anatomical outcomes in morpheus space.
So they build nice organs, you know, livers and fingers and things like this, which individual cells cannot, no individual cell knows what a finger is or how many you're supposed to have, but the embryonic tissues absolutely knows.
And so the reason for thinking about it this way, is that when you have a breakdown of this, this cognitive glue, these mechanisms that that bind cells together to give them a larger light cone, by the way, projected into a different space.
When you have that breakdown, what happens is that it's a it's a failure mode of that process, individual cells disconnect, they start to pursue local tiny goals.
And as far as they're concerned, the rest of the body then is just environment, right?
So so what this what what this crazy kind of way of thinking about it suggests is actually a very specific therapeutic approach that when you have cells that have done that, you don't necessarily need to kill them, nor do you necessarily need to repair the hardware that may have led to this, for example, a nasty mutation like KRS and things like that.
But what you could do is force it to reconnect to the collective.
And we have ways of doing that.
And so in animal models and our moving moving towards towards human tissues, if you if you take those cells and you and you force them into an appropriate bioelectric circuit with their neighbors, they basically again become part of this collective which can remember things that individual cells can't do.
And then they begin to work on the right thing, even though they're bearing some of those mutations, so you can overwrite some of these hardware defects with this kind of this kind of process.
So so I think, you know, that's just an example.
I think I think it matters to ask the question, what are the boundaries of the system specifically, not just with respect to energy, as lots of people do, or even information, but actually to the size of their goals.
Right, when you have a system, what are the size of the goals of this thing is going to and then yeah, and then that that that helps you invent new ways to interact with it.
I get that. But then it becomes very important to be conceptually precise, if it's the goal to, to think about what goal directiveness actually means.
So from a philosopher's perspective, it could mean what we call practical intentionality.
So there is this whole thing intentionality, Brentano 1874.
Today, one would say there is theoretical intentionality, some system directed as a set of truth conditions, and as practical intentionality, a system directed as a set of satisfaction conditions.
But all of this, I think, doesn't license talk about goals, you know, of course, there are all these things like functional set points on attractors on different levels of granularity.
There are these, I found very intriguing ideas of idea of you, I don't know how you put it this editing a homeostatic pattern to influence the system.
But all of these terms do not carry any intrinsic normativity in them.
These are not goals. So if I wanted to be a very radical, hard headed scientist, I would say, there are no goals in this universe.
So what we have is systems, for instance, systems with smaller or larger light cones that become successful by hallucinating goals in a certain way, by internally representing, say, functional set points or something like that preferences in a specific way.
For instance, in a way that makes them unable to see that they are actually representing it and make it become, as philosophers would say, transparent, turn it into a real goal.
That's what I mean by hallucinating a goal, right?
Like, they predict satisfaction conditions or some, say, functional set point, but in a way that makes it a goal for them.
But still, there are no goals in reality, you know, there is no intrinsic normativity.
We can speak of the system has preferences, it's directed at goals and so on and so forth.
But that's actually very loose talk.
And I think, how to say, one would get more heuristic fecundity if one would think, how can I say the same thing on multiple nested levels without using the term goal?
And let me add a bit to this story.
It'll take me a minute or two.
So, I have thought a lot about this very, very special subcase of a phenomenal self model, like conscious systems, who have a global model of themselves with global properties, a meta system representation, if you will, which is transparent to them.
And then they cannot recognize this as a representation, so they get the phenomenology of identification and selfhood, and they get glued to the content of their own self model and have to act it out.
Now, I think, it is the structure of our own self model that also influences our theoretical intuitions, if we look for good new metaphors, or good new theoretical approaches.
And one thing is, I mean, we are both, I see we are both influenced by Dan a lot.
And one thing Dan really liked in my work is that I said, we actually have a phenomenal model of the intentionality relationship.
This is not only something philosophers talk about, you know, agent goal relationships or subject object relationships, the human brain actually constructs a model.
We have a model, for instance, for instance, in motor control of the S, us and an effector being directed at the target state, or in precision control, in consciously guided attention.
We actually have a model of ourselves focusing our attention, for instance, on a perceptual object.
This relationship is actually part of our own conscious experience.
This was successful, it was a good user surface, it created the idea that we have a first person perspective, but it may also, how to say, if we start thinking theoretically, as we're doing now, this structure that has evolved in our brains, in our conscious experience, it may limit us.
So what we see is actually elements or aspects of our own self model.
And then we try to discover this and say, there is goal directedness, say, even below a cellular level, or there is agency, not only on whole organism levels, but there is agency somehow in a biofilm or something like that.
But that is actually that way of thinking is infected.
By the way, our brains, the user surface they have created, by the way, our brains model reality.
So I totally understand what you're doing.
And I think it's very charming.
But whenever you say goal, or agent, or individual, or something like that, or goal directedness, I think you're importing our phenomenology there, which is probably very idiosyncratic.
Or that's what you're trying to find.
I'll stop here.
Did you get the point?
I do.
And for sure, my point is not that any of these things are going to be human like I, I agree with you that we have to fight hard, not to import our expectations of what these things feel like into these other unconventional models.
I will say I will say this, I am an engineer, I'm not a philosopher.
And so so maybe there are subtleties here that that are missing.
But again, I don't, I don't buy the existence of some, some kind of, in fact, almost anything but but certainly goals that are true goals as opposed to apparent goals.
So the thing that you're talking about, I'm not saying there are some kind of true goals that are different from what an observer observes, either in themselves or in a set in another system models it as a goal seeking system and interacts with them that way.
I don't know what it would mean for there to be something else over and above this that was a true goal of which these other things are just metaphorical.
And so so and and I'm sure this is this is a controversial, you know, stance and philosophy, but I don't think there are metaphors.
And then there are the real thing that we could some, you know, someday get hold of.
I think all we have is metaphor, certainly.
And certainly in science, I think everything people ask me this all the time.
Do you mean that metaphorically or real?
I don't know what you mean by by real?
Of course, it's a metaphor, right?
So so all I mean, when I say goals, I don't mean a human level or or or a human like, you know, a pre percept of what is it like to have a goal?
I don't mean any of that.
What I mean is that there is a particular framework that is useful in having a rich interaction with that system.
As an engineer, when we build out either when we build things or when we, you know, as a bio engineer, when you try to control biology, what you really have to know is how much autonomous action can I expect?
Right.
Because because because if you don't know that, you know, if someone comes to our house to repair the heating system and they don't have a view of the of the thermostat as having little tiny goals and doing things, you know, I don't want to hear about Maxwell's equations through the wires of the of the house.
They need to understand that you can do a lot if you understand what the system is trying to achieve and under what conditions it fails.
Similarly, not so good if they sort of try to, you know, appeal to to the thermostats, you know, moral intuitions.
That's not going to work either. You have to get it right.
Right. And so what I like as an engineer is the task of saying, OK, as an observer, I'm going to make a I'm going to make a an empirical hypothesis.
Here's the problem space I think you're working in. Here's the goal I think you have here are the competencies I think you might have to to achieve it.
And then we do experiments and then we see how has that enriched our interaction, whether that be with a cell or with a device or with a friend or whatever.
Somewhere along that that spectrum, you try you try to get it right.
And and, you know, Josh Bongard and I have been developing this this this thing called polycomputing.
And the idea is simply that any physical event, if we try to ask, is it computing and what is it computing?
Fundamentally, what you have to say is from the perspective of which observer that is the exact same event might might be interpreted and might have a compute computational value and information content that would be extremely different for different observers.
And that's all there is. You know, I'm not I'm not sure I don't know of any reason to think there's some kind of ground truth to the matter as to what it is.
And so so I you know, again, I don't you know, by by using the word goal, I don't I don't mean to invoke these other things that I think don't don't necessarily exist.
I just mean this this observer dependent functional interface that lets us have a richer relationship than if we didn't know that the system was a goal directed system.
So why not take a very sober and straightforward, say, control theoretic notion of goal and say this is all there is and we're going to spell this out on different scales.
That's the project. You could do that. You could do that.
So the problem, the problem with picking one level is that you're making an assumption and it's you know, it's an it's empirically testable.
You're making an assumption that that particular level will give you everything you want.
And of course, there are people who don't even like that. There are people who will say, no, no, no, you take the you should take the chemistry level.
Right. And and and especially so so let's say right in the bio in our community, let's say the regenerate medicine community.
There are people who will say you forget this control theory, cybernetic stuff chemistry.
We like we like models in terms of chemistry.
And they're not really reductionist because if you say to them, oh, you know, so quantum foam, right?
Ultimately, you want to talk about quantum foam.
They say, no, no, not chemistry.
So, you know, no, no, nobody wants to really like like follow that that through.
No, but nobody wants to talk about quantum foam, but they'll they'll pick they'll pick a level and they'll say, no, no.
Chemistry is the right level.
And then someone else may say chemistry is too low.
No, but but cybernetics and control theory will will do what you want.
That may be and that may be enough for some things.
But I think it's it's a this is not a choice that you can make philosophically.
I think this is a an empirical question for any given system.
What is the right level?
Because it depends on the epistemic interests.
Yeah, well, well, and in some of our work, what we're finding is so so one of the things we do is we import.
Or we try to import tools and approaches and concepts from behavioral neuroscience into other non brainy things and see how that works out for us.
That's the that's the research project.
So so one of the things we find is that actually importing not only active inference, but all kinds of stuff, you know, in terms of stress and anxiety and perceptual by stability and visual illusions and false beliefs and all of these kinds of things.
In some cases, those actually give you quite a lot more control of and a richer interaction with the system than you could have had if you stuck with control theory.
So I'm not saying control theory is bad.
I'm sure there's plenty of places where that will be sufficient, and maybe there's even places where just the chemical story is sufficient.
But I don't think we can assume that that's going to take us all the way through.
And I think because, you know, if if if we if we can agree that that interactions with with humans can't be dealt with by by control by the standard tools of control theory, that human came very smoothly and gradually from a single cell.
So somewhere in there, right, there has to be the scaling process.
So, yeah, nothing about control theory, but I don't see why it would have to be the best level.
So just a footnote.
I mean, again, I see you've read quite a lot of Dennett and there's this concept of the intentional stance.
There's this edited collection 1987, but the point and the move that Dan was making there was to be an instrumentalist, as we say about intentionality.
So not be a realist and say intentional properties are something that we find in the world and brains are somewhere else.
And then we have to explain them.
But to say the whole intentionalist vocabulary is an instrument we use and if it works, we can use it on different levels.
But we make no assumptions about what is the ground of reality or if intentionality is some irreducible property.
And what there's something I really like about stuff I've read from you is maybe we should get to your idea of scale free cognition.
But you won't like why I like it.
I'm getting accused all my life by other people that I pour out the baby with the bathwater.
And what I like is that you're really doing this when you say scale free cognition.
I think that's a bit too much.
And so what what we have is levels of description.
And analytically, a level of description is constituted by a logical subject and a set of predicates.
So a logical subject could be a biofilm or brain or what have you.
And a set of predicates are, you know, where we ascribe properties to that logical subject.
Now, I read you as actually saying there is a there are many, many more levels of description that are relevant.
And there is a continuity there.
But you're not but you're packaging it as if you say there is no level of description.
And you're not saying cognition is scale free.
You're just saying, boy, there are so many more scales than what we've ever thought about.
Right.
But yeah, you cannot work without scales or without levels of description.
Is that right?
This is correct.
And I think I'm following in the terminology and I can see how it could be confusing.
But I'm following terminology that in some sciences, you say that there are certain principles that are scale free, not because there is no scale, but because you find the same thing at multiple scales.
It isn't inherently tied to one scale.
And that's what I'm saying.
I agree with you.
I'm not saying there are no scales.
What I'm saying is we should untie our hands about certain important dynamics that we are used to finding at one particular scale and not just scale, but in fact, in space.
So so people talk about behavior and embodiment and these kinds of things.
And they specifically mean, you know, three dimensional space or moving and perceiving in space.
I want to untether all of that.
And I want to show you how that right in other problem spaces at other scales you can take.
And that's an empirical question.
But I think the evidence is good now that you can take some of the same concepts and apply them to to to things that we never would think of as something something like like us, you know, medium sized object moving at medium speed.
It's through 3D space.
And I want to one other thing about I wanted to get your thoughts on on, you know, then it's instrumentalism and all that.
I like it a lot because I think that what we're fundamentally indexing into is the tools and the approaches.
The reason I say these things is is because the tools and approaches of cognitive behavior science seem to be working at these other in these other, you know, in these other areas.
But my question is, why do we only apply this like when you were saying, you know, it's it's not something real we find in the in the physical world.
It's it's it's this kind of like, you know, an instrumentalist approach.
I'm why do we not apply that to everything like not not just not just these questions of cognition and whatnot, you know, my my molecular biology colleagues will say, well, attributing memories to cells or beliefs to cells is metaphorical, but pathways, you know what we really have our pathways.
I don't see this at all. I think I think that all of the things that we are talking about, these models, the models, right, which is all we have are exactly the same.
You treat it as a pathway because you have a set of tools that have gotten you to, you know, to a certain point where you can treat it as this.
It might be a pathway. You might think it's a Turing machine.
You might but but all of these are just formal models.
We do we ever connect to the actual thing?
I feel like I feel like we're we should be instrumentalists about all of it.
Well, it looks like we if we ever know reality at all, we know it under a representation.
And that can mean many, many things. It can mean under a description like pathway or something.
But it can also mean under a model created by our brain right now.
Like think about the interoceptive self model we have or something like that.
We know certain functional aspects of our own organism under this model, which has nothing to do with language or doesn't have syntactic properties or anything like this.
So there's a multi multiplicity of representational formats under which we know reality.
And where it gets interesting, of course, is when you ask yourself who is knowing and what is knowing, because there's, of course, also a representation of, I guess, in your world that would be info taxes or something like that.
I don't know if that's fitting or appropriate here, but I think there's one thing for a system.
Do something like info taxes and control the process by having an internal model of what is going on.
Right. Agreed. Yeah, yeah. I agree with that. Those are to me, those are different steps along the and not steps, but the different different capabilities across the cognitive spectrum.
I think there's an interesting class of systems of functional holes, which do what they do by actually using a self model, by using a generative model of themselves.
And I was just to give you an example, I was very enthusiastic about your Spongard Starfish because that robot allowed me to illustrate and to explain also to a larger audience what I've been by a self model and a self model that evolves in an individual system.
Finally, finally, finally, in a really consistent way, in a way that makes it intuitive for people, something that starts with random movements of effectors, and then calibrates an internal body model and just ends up pretty quickly, actually, after a short, steep learning curve, with the capacity to control a body as a whole.
So this system certainly doesn't have consciousness that starfish, we could either we could even do videos of it, which we could play to the audience if we wanted to it wanted to do that.
This system is definitely unconscious, but it controls global properties of itself body shape motion with an internal model of itself.
Now, what is the cut off point when it becomes so to speak computationally interesting to control yourself with under a representation of yourself as a whole.
So you will know much more about this than I do, there is model based control and there's model free control.
And there will be self model based control of a whole organism or entity and self model free control if I am right.
Does this scale up and down in any way or am I wrong?
Would one say that even very primitive self controlling entities must, you know, Conant and Ashby must instantiate some kind of self model?
Just we don't see it in these.
Do you have any thoughts on this?
I do. And I think this bleeds into some other stuff that I wanted to talk about, which was your awesome paper on artificial suffering and epistemic indeterminacy.
I think that we have to be very careful because I think that all of this is defined from the perspective of some observer, which may be the system itself for significant systems.
By saying that something does or doesn't have a self model, I think that's really hard.
And as you know, and especially I, you know, and I think two observers can can disagree about whether that's the case.
And, you know, Josh's other stuff with more on morphological computation, right, is is really apropos here, because what you end up what you end up having is systems that function as though they absolutely have a self model.
But but but there is no, at least, you know, that anybody who has has found, there is no explicit symbolic representation of it that that anybody can see, because the material is doing a lot of the work.
It's not the classic, you know, if you do if you do the classic thing where you write code and you say, okay, I'm going to represent some angles and some things here that, you know, you can see what the what the model is, but morphological computation and some of the stuff that we study show you that you can get there without anything that we as frankly, very inexperienced observers of these things, I think we're not very good at noticing these things would not recognize as a formal self model.
So I think we have to be really careful with that. And the other thing is that the kinds of things that we are interested here, which is which which is cognition, and then and then consciousness and so on.
I no longer believe that these things show up because of the material and the algorithm.
I mean, of course, the material and the algorithm is important. But I think there is an another component here, which means that we have to be extremely humble about these things and what what we think is there. Basically, I see in a variety of very minimal systems, emerging, not just emerging complexity, that's easy emerging complexity is very easy to get.
But I see what what I see is emergent cognition, you know, sometimes very minimal, sometimes more significant.
And we can talk about where I think it's coming from. But the bottom line is that I don't believe it is defined in any way that we yet understand by the accounting of the physical parts of the system or by the algorithm that we thought we we, you know, sort of implemented here.
So, so that's, that's, that's my point is that is that I think there's an exam, there's an there's a kind of emergence of cognition.
That we could talk about that makes it very difficult to have to be able to say anything unique or privileged about whether something does or does not have an internal model.
Yeah, so there is, I mean, this umbrella term of cognition, of course, there's a risk of acuity there too.
I mean, if we use it across scales, I mean, what I would offer is something does cognition.
If the representationalist level of description, whatever it means in that case is still heuristically, if it helps us to predict and control like that, if so, that's the naturalized version of intentionality, basically the representationalist level of description, and then there's a can of worms of, I don't know,
a general predictive processing, what format what dynamics and everything.
But as long as we can still say, the system achieves what it achieves by generating representations.
And this is helpful for us, then we could still say there's cognition there.
What is a difficult question is when you say emergent cognition, because there are these, I mean, there are technical discussions about notions of emergence in philosophy, but just, there's a very benign version that many scientists will use that you have new system properties,
properties that are suddenly properties of the system as a whole, which maybe we couldn't predict from the past, right, which are genuinely novel or sudden, but there's also strong emergence, which say the emergent properties say cognition, the new emergent property actually exerts downward causation, it has a causal force of itself.
I would hold, I would hold, if that is possible, the scientific image of reality is gone.
And this could be a universe full of miracles.
So the question is, if we say cognition emerges, do we want a benign, modest concept of emergent?
Let's go for the scary one.
Let's go for the scary one.
Okay, you go, you do.
You go ahead.
Yeah, okay.
I mean, look, this is, these are, these are very new, new, I ways of ways of thinking, at least on my part that I'm going to try to try to say so, so no, no doubt that there are many cans of worms here.
Okay, I have no doubt this is not fully worked out.
And it's not going to be, I'm not going to consider it fully worked out until we use it to actually achieve new things in the lab.
That's what I like as, you know, the, the, the, the, the evaluation of these ideas, but but let me let me throw out some things.
I don't want a universe full of miracles in the sense that I don't like mysterious views that we can't, you know, we can't know anything about what's going on.
However, I don't think that's the same as saying that there are causally important, causally potent patterns that ingress into the physical world that are not themselves determined by anything in physics.
And what I mean by that now, now the, the, the less controversial version of this is, is a kind of Platonist mathematics, right, is that we know there are truths of number theory and certain, you know, patterns that you get from, from, from very simple formulas and complex numbers of fractals with, you know, all kinds of shapes.
If we ask where does the, where do those things come from, you're not going to get an answer that has anything to do with physics.
The, those, I mean, this is, I know there are people who disagree with this too, but this is my, my amateur view of, of, of what mathematics does.
I, I think they're discovered, they're not invented.
And I think that a lot of these things, if not all of them would not be different if the constants at the big bang were, were, were tweaked and we had a completely different universe and different physics.
I think those things would still be the case.
So I prefer a mathematics first view.
Some people obviously don't.
And I think that these things do have causal power in the real world.
I think evolution, I think what evolution and living things are amazing at doing is exploiting these free gifts that, that do not have to be directly engineered or constructed.
They, they appear from wherever it is that the, that the truth of mathematics come from.
And we can talk about the properties of prime numbers that, that tell you why cicadas come at the, you know, the 17, every 17 years and things like this.
And we can talk about other patterns.
We can talk about laws of computation that, that allow evolution to evolve, to discover an ion channel.
So a voltage gated current conductance, right?
A voltage gated ion channel.
And immediately, I mean, that's, that's a transistor.
And immediately with a couple of those, you end up with a truth table.
Now you didn't have to evolve the truth table.
You get that for free as you get many things, you know, the fact that if you know two angles of a triangle, you don't have to look for the third.
You know what it is in flat space.
You know what it is.
So, so I, I, now, now the reason that I, I don't think this is, this is a mysterian view in that, you know, the mathematicians, they, they have what they call a map of mathematics, which is like, they, they believe that that space.
At least some of them believe that that space is structured and that what we can do is have a, a rational research program to map out that space.
It is not the same research program as mapping out the physical universe, but nevertheless, it isn't random.
It is not just a bunch of random stuff that pops up from time to time.
There is a structure, some kind of a metric to that space.
And that more interestingly, that, that by, by discovering one thing in that space, it now helps you discover some other things around it that literally, right.
And, and, and, and how we do this is this is, this is one reason we make xenobots and anthrobots and, and all these synthetic beings, because, because what I'm very interested in is a question of, okay, when you, when you have a, you have a creature normally, when you ask, why does it have a certain shape?
Why does it have certain behaviors? The standard answer is evolution, right?
So, so for years, you know, eons of selection, you've selected for a specific thing.
Well, we can make, um, I know where the history doesn't play a role, right?
There's no, there's no history. Yeah.
So you get it. So, so, so now the question is where did these properties come from now?
Um, much like with certain properties of networks and things like that, some people will just say, it's just something that holds, you know, this is just the real, it's just the fact that holds.
Well, that to me is a much more mysterious position. I don't like things that just randomly hold. I, I, I would prefer to think that there is a space of things that hold.
And every time you find one that helps you push forward a little bit and find some others around it.
And so that I, I think these things that ingress into the physical world are not random stuff that every once in a while we find some kind of thing that seems emergent and surprises us.
I think we can do better than that. I think, I think there's actually a space of these things that contain mathematics, but also I will conjecture here.
Um, I think probably contain kinds of minds and that what we're doing when we make these physical things like Xenobots and, and, and seesaws and, you know, simple machines and whatnot, what we're really making are pointers.
We're making pointers into that space pointers in the sort of computer science, you know, um, definition.
Um, that, and, and, and, and that, and this is, this is why I, I, you know, one reason I really liked your, um, your, your, your paper on, on the, the artificial suffering paper is that I, I think that is what requires us to be very, very careful about, uh, knowing, knowing what we have once we've built something.
You know, I, I've seen so many people who say, well, well, you know, I, I, I wrote this thing. I wrote the code for this. I know what it does.
No, I, I don't think that just because you built it using the laws of chemistry or engineering, or you wrote the code, I don't think you necessarily know what it does.
And in our latest work, and there's more coming, um, on very simple sorting algorithms.
These are, these are very simple, minimal, deterministic, fully transparent algorithms that people have been studying.
We found novel, um, novel behaviors that, for example, um, delayed gratification and things like this, that do come straight, you know, do come straight from cognitive science that no one ever thought these things could do.
And no one had ever looked because they assumed, right?
So, so anyway, so that's, so that's kind of my point is that, um, I, I don't want to be mysterious about it, but I don't think the physical facts fix all the causally important facts.
Yeah.
Yeah.
So the, of course the Platonism, that's really deep, deep water.
Maybe we should postpone this to, to another conversation, but would you agree?
I mean, the pointer to the space, the pointer is itself a representational device.
And what makes these things, these discoveries causally effective is properties of the representations of these, say, mathematical structures and not they themselves, you know?
Uh, I mean, so, so granted, these are extremely deep waters and probably far deeper than, than, you know, I can, I can go in, in the, on the philosophy side, but, but no, I, I, I actually think both sides are critical.
I think that, that the physical things, uh, are very important because they determine which, um, causal influences from that space are going to, uh, ingress, but the, but, but they by themselves do not have all of the information nor all of the, um, uh, uh, predictive, uh, you know, power that you need to know what you're going to get.
When you make that pointer, you know, something about what you're going to get.
You know, if you make a very simple, if you make a mechanical clock, you are not going to pull down a high level intelligence, you know, you know that much, but when you start making, but, but you don't actually know exactly what you're going to get.
And we are surprised all the time.
And I don't think it's because it's random and unknowable.
I think it's because we have just begun this, uh, this, this, this scientific enterprise.
And, and if we take seriously the idea that it's a space that, that we can explore, then we have a research program and that's, that's exactly what we're doing.
So before we go to the artificial suffering, let me ask, get that, get at your point from a different angle.
Do you think there are radically, radically unpredictable events, for instance, say in transitions from simple forms of delay gratification to more complex forms of delay gratification.
Are there things which surprise us now, transitions that surprise us now and look unpredictable to us now, or will they also in the future always remain, have this quality of having been and still be unpredictable, the transition itself?
Yeah, I think that, um, I think there will always be some new things that are unpredictable, but I think that window is going to move.
And if we do our job, uh, correctly over, you know, continuing over, over, um, uh, the, the time of future research, the things that are surprising to us now will become less surprising.
Right. I mean, I, I, I do think that a lot of, and I know this is also very like emergence is a very, very deep, uh, waters, but, uh, I, I do think that a large part of emergence is, has to be looked at from the perspective of an observer and the, and the surprise level of the observer.
So for example, um, um, um, the cellular automaton and the game of life, right. If you, if the, the, the first time I read the, the, the three rules of the game of life, Conway's life, I did not immediately say, oh yeah, I see, of course gliders, you know, moving at 45 degrees.
I get it. I, this was to me, to me, I looked at that. And in fact, in the book, the book that I read, I think Poundstone or somebody, it, it said, look, the, the, these simple rules have this incredible emergence, uh, emergent richness of this world.
You got some gliders and beehives and all this stuff. I, I, I don't know, you know, when, when von Neumann saw those rules, he might've said immediately, oh yes, of course, that's not emergent gliders aren't emergent.
They're obviously entailed by these rules. I can see it. I can see it right now. And so the question of what's emergent, what's surprising, uh, right.
Is, is I, I think observe a relative and I hope we get better at, uh, at, at predicting these things. I don't think they're fundamentally unpredictable, but that's the question.
I, I don't think they're fundamentally unpredictable, but I don't think they are predictable, uh, in the way that, that a lot of people are, are looking at this now.
You know, I, I think we're gonna, we're gonna need a lot more work on this. I don't think we're good at this at all for these kinds of things.
Let me tell you something that happened to me three days ago. Oh, this friend came by and I said, look at these flatworms and they make one without head and they make one with two heads.
And you know that you can cut them up to 250 times and, and, and, and show, and see this tadpole with eyes on its tail and spinal cord.
And I just showed this to a friend who was visiting and he, and he said, uh, these people need to be locked up.
You know, and had this absolutely not a philosopher or scientist, absolutely strong reaction of disgust.
And they're treading on dangerous ground. This is something where science shouldn't go.
And I guess you must've encountered this so many times. And I came to this, um, artificial suffering paper.
I wrote from a very different angle because I was in the European union's high level expert group on artificial intelligence,
supposed to do the ethical guidelines and collided head on with the industrial lobby.
And, uh, there were 52 experts and there were only two people who said there's a problem with inadvertently creating artificial consciousness,
not intelligence, but synthetic phenomenology without even having a theory of what we're doing.
And they all said, just, this is bizarre. This is science fiction.
And the only other person, uh, in the whole group who saw the risk, um, connected, uh, to, um, synthetic phenomenology was Jan Tallinn,
the inventor of Skype and everybody else said, you guys are, you're just living in a dream world.
Our robots crash all the time. This doesn't work. This is never going to happen.
Yeah. Um, no, if we do, let's put it like that.
Would you agree if anything, um, like the story you are trying to tell,
or you did the, the, the windows you're trying to open for all of us in your theorizing.
And if anything like the free energy principle and what Carl has been saying is true,
there's an elephant in the room and that elephant is that life might not be worth living.
Uh, and that there might be just in sentient systems.
There might be just much more, um, suffering than joy.
Um, that we, we are these anti-entropic systems that fight this anti, this uphill battle.
But in the human case, as you have written, this toxic information has entered our self models telling us we will die.
And we all know we will lose this battle and we will disintegrate in the end.
But on the other hand, we have this, what the Buddhists call Bhavatthana, uh, the craving for existence.
In a certain sense, I think, which your work actually adds aspects to is we are a craving of existence, you know, on, on, on many scales.
And it's, it's very clear that as soon as sentience and consciousness enter, enters the picture, this will create a lot of suffering.
Right.
And we haven't fully understood why we suffer so much as human beings.
Um, there's a long tradition, there's Buddhism and everything.
Um, and there's an extremely high risk that we would multiply this on cascades of not artificial, but as I say, post-biotic systems, systems for which this distinction between artificial and natural,
this distinction between artificial and natural doesn't make any sense because it's not exclusive and exhaustive so that we trigger a new scale level in the evolution of cognition,
which brings up a lot of suffering with it before we have even understood what the root causes of our own suffering are.
And if anything can be done about it so that it is super, super, um, dangerous to play with these things and try these out.
Because as we all know, in the history of science, sometimes things happen earlier than everybody has thought.
And sometimes you, you said you're an engineer.
Sometimes you create something without actually having the theory for it.
You just create the, it's true, right?
Sure.
Of course.
So what are your thoughts on this?
Should we be?
Yeah.
Do we have a responsibility?
Yeah.
Yeah.
Yeah.
Absolutely.
I mean, critical questions.
Okay.
So what I, what I don't feel competent to give a kind of authoritative answer on is the problem of existentialism and whether it's better to exist and to die versus being nothing.
I can tell you my, I'll, I'll, I'll give you my personal belief, but that's all it is.
Everything else I was saying before was driven by some, some degree of experience experiment and things like that.
This is just, you know, this is just my, my, my personal belief.
I don't have any way of, of, um, uh, you know, having an argument, some kind of a knock down argument for it.
Uh, I, I have a, uh, overall, I have a, uh, kind of a positive and optimistic view of all of this.
I think that there, there are, and, and I'll tell you how this gets into my dilemma as well.
Um, I, I, I think that to the extent that we can push forward, uh, knowledge and, and, you know, wisdom and understanding, we are going to, uh, get better at exactly the problem that, that you've said now.
And, uh, I, I, I think, um, I think we need to, I think we need to push through a, a, a painful part of, um, of our development as a species.
Maybe, uh, maybe other, maybe there are, you know, other intelligences in the universe that do it too.
But for us, uh, I, I think we are going to push past this to the point where we can a understand, um, the, the, the, the, the causes and the implementation and the meaning of, of joy and suffering.
And we are going to be able to do much, much better than we're doing now.
And I think it's worth it.
I, I don't, I don't think that this is an argument that any of this is an argument for nihilism or that it's better off to have an empty universe with nothing in it.
Uh, I, I re, I reject those things.
And, and, and here, so, so now let's, let's come back to, um, the person who, uh, who, who visited you before.
Yes.
Yes.
So, okay.
So, so I receive, I'm, I'm not even a clinician.
Okay.
I do basic science, but I get every day I get, uh, let's just, just for the purposes of ratios, I'll, I'll get one email from somebody saying, oh my God, uh, what you're doing is scary and terrible and you need to stop immediately.
And then I get 10 emails from people that say, my kid has a spinal cord defect.
Uh, I've got cancer.
Somebody else I love has, you know, some other kind of issue.
What do we do?
And why are you scientists sitting on your butt?
And we are having this horrible, um, experience.
The, the bottom line in my understanding is that we have now, uh, an incredible amount of, um, of, of biomedical suffering in the world.
People, people, uh, you know, and, and the people who are the most against this are the young, healthy people typically who don't, or, or, or, or they just don't have the imagination to, to ask, um, when, when something's wrong with my child or the person I love.
And I run to the hospital and pray like hell that they figured out what to do with this.
Where does that actually come from?
Right.
It doesn't, it doesn't, you know, the, the, the, the only place that knowledge comes from, uh, is, is from, uh, from doing experimental work in biology.
And right through, through dealing with, with, with, with people about, uh, on these issues for a long time.
I've only met one person ever whom I honestly believed, you know, uh, she, you know, um, a strict vegan, blah, blah, blah.
Right. So, so no, you know, no issues of, of meat industry and whatever.
And I believed her when she told me that she would not bring her kid to the doctor if they were sick.
I believed her, I think she was a monster.
And I think the rest of us, uh, understand that we have a responsibility to people that are, um, you know, there's, there's a concept in, in, in, in Buddhism of, um, uh, an inauspicious birth.
Right.
And, and I, I think at the moment, I think pretty much all births are inauspicious births for the following reason.
We now, contrary to what a lot of people think is that, look, uh, we have this optimal way, you know, about 80 year lifespan is optimal in our, our form.
Like this is pretty optimal.
So you scientists don't screw it up.
I think this is, this is ridiculous.
What we have now was not tuned for us by, uh, by a process that had, right.
A process that had, right.
That had our welfare or any of the things we believe we, we value in mind.
We were, we were, uh, basically shaped by random cosmic rays and some other aspects of, of, of evolution and, and so on.
This is where evolution left us.
It is not because this is how we're supposed to be.
Right.
Of course.
And so, so all the people who have, uh, you know, childhood cancers, and then later on, uh, you know, uh, they have, uh, the stigmatism and lower back pain and eventually, you know, degeneration and death.
Uh, all of these are random things that we are now subject to.
And it is, I, I view it as, as our responsibility in the community of bioengineers and, and developmental biologists to allow people, what I call freedom of embodiment.
You should not have to live your life in whatever body you were handed out by the, by a pattern of cosmic rays that, that happened to hit your egg cell when, you know, when you were gestated.
Uh, we should, we should have control over the kind of embodiment we have.
And that only happens if we have an understanding of what, you know, how bodies come to be, how minds come to me and all, and all of that.
Right.
So, so, okay.
So, so that's, so that's my answer to people who are, who are freaked out about flatworm experiments, but, but this goes, but, but there's a, there's a, there's a problem here, which is.
What you put your finger on, which is, which is this, uh, I, I take the issue that you raised in that paper very seriously.
I think, I think you're absolutely right to worry about this.
And, and I think we need to worry about it.
The problem is, and this is something that I, I wrestle with a lot.
The problem is that to the extent that we are, we are, um, working to figure out what is it about, uh, living things about cells, like an egg, you know, we all come from a, from a quiescent egg cell, a little blob of chemistry and physics.
And then eventually here we are.
So, so to the extent that we figure out how that works and what's going on there, it becomes easier to create artificial, uh, constructs.
Right.
To that are going to have exactly the problem that, that, that you're talking about.
And unfortunately it, it turns out that understanding how to relieve the suffering that, that I was just talking about all the, all the biomedical stuff.
I think the path to that goes right, right centrally through the issue of understanding what is it about biology that makes agents that matter in a moral, in a moral sense.
And, and, uh, we, you know, what, what I haven't done, I started at one point and I haven't done it and I still wrestle back and forth with it is, um, I think we're in a pretty good place now.
And, and I'm not the only one there's, there's at least one other person and maybe more who could actually write down what is, what is it that, that you would have to engineer into your system to, to, to reach that point.
I think from the biology end, it didn't come from the computer science or the AI, and it came from the biology, from trying to understand the biology.
And I think we're to the point now where we can pretty much write down a lot of that and right.
And I, I, I don't want to do it, but, and, and, you know, but, but at the same time, I think that it's not going to be very long before people who understand the biomedical literature that is being developed, who are going to catch on to all of this.
Right. This is like, this, this is coming. And so I think that it is absolutely an issue. And I think it is, you know, uh, in scale, not, not in, not in principle.
Um, but because, because we make living things that matter all the time and, and, you know, we, we know pigs can suffer and yet there's the meat industry and so on.
Uh, but, but in scale and the number of these agents that will be created for all kinds of purposes, uh, this is, this is potentially very problematic.
And that's why, uh, we really have to work on, um, developing a better ethical framework of recognizing other minds and relating to other minds and completely, um, uh, uh, diverse and divergent implementations than what we're used to.
Yeah. I'm not so optimistic. I mean, I have demanded this moratorium until 2050, but I would never dream that anybody even thinks about it seriously because the incentive landscape of the investors that drive this kind of research is very different.
It's not guided by any moral incentives. So all of this will be done. Um, many things in what you said.
So I think there's a very straightforward good to work by, uh, criterion for what counts as a moral object.
It has to be conscious and it has to be able to suffer. Uh, we don't have a theory of suffering, but I have some good ideas.
So any system that has a phenomenally transparent self model where it represents its own preference landscape in it, uh, and which can have thwarted preferences,
preferences, which are transparently embedded into that self model will experience them as its own suffering.
And then you can say things about, you know, loss of control, uh, as embedded into a conscious self model.
You can say something about the rate of prediction error, uh, in time, you know, that creates a surprise, uh, that the rate of prediction error, um, increases so fast.
So you can have some ideas what suffering is, but I think that's also what you meant.
We re if we want to go about this seriously, we need a theory of suffering that is grounded empirically grounded, but how are we going to do an evidence-based theory of suffering without creating a lot of suffering in the process and testing it out?
Yeah.
That's the problem.
That's the problem.
And what counts?
I mean, that's a philosophical point, you know, um, utilitarianism.
I mean, your argument was, is of course, very convincing.
It's actually the argument from the medical need and the biomedical need for this research.
It's actually a scandal that's not much more money and funding goes into exactly that kind of research.
You know, that's, that's all granted, but the individual animal, given that it has a conscious self model, the individual animal, uh, newly created system that you sacrifice in that research.
That has no interest, uh, in this human child being saved or so.
And it's a relationship between you as an individual and those individuals you sacrifice.
Even if on a population level, you could say, this is doing this kind of research and sacrificing animals, artificial systems or post-biotic systems is in the interest of everybody in a longer time window.
The individuals will not agree.
The individuals that get sacrificed.
And how does one get around this?
Do you have any ideas?
Have you thought about this?
I, I have, and, and I don't, I don't disagree with you.
Um, uh, although, um, just, just to, just to say one thing, I think, I think what, what you're saying about a theory of suffering and so on is critical.
We must have it, but I also don't think it's sufficient because I don't think anybody is going to argue that pigs, uh, don't match that criterion.
And yet it is not sufficient.
We have all the, you know, all the, the convincing, uh, arguments that you want for in that case.
And it really, it really hasn't moved the needle that much.
And so, so I think it's necessary, but not sufficient, but the argument, um, uh, the, the, the, the issue of, of looking at it from the perspective of the individual animal.
Yes.
So the way, the way that I propose to, for people to view it is as the following, there's a trolley problem here.
And there is, there is no option of not having your hand on the switch.
I don't believe it.
I think each and every one of us is, is making a choice.
There is no not making a choice.
You are going to make a choice.
So, so what I recommend everybody is to pay a visit to, you know, you don't, you don't necessarily get let in inside, but go into the front lobby of, of a pediatric oncology clinic.
So, so you're standing there here, come here, come the children with cancer.
And now you need to ask yourself, uh, yes, the planaria will not agree.
They don't care about the kids, but you are the one making that choice.
They're not able to make the choice.
They, they can't, they can't do it, but you can.
And so now you have to make a choice.
There is no saying, well, not my fault, not my problem.
I'm taking my hands off the wheel.
I don't believe that exists.
And certainly I don't believe that exists for those of us in the community who actually have the ability to contribute to that work.
If they choose to, you know, if they choose to do it.
So the scientists, uh, you are going to make a decision.
You are going to either contribute to that, uh, to, to a cure, or you are, uh, complicit in deciding that you are not going to do it.
Fair enough.
I can, I can, I can understand if somebody says to me, I have thought about it and I am, uh, I am, uh, literally, uh, uh, I taking, taking the side of the, of the flatworms and the tadpoles over the, over the, the human kids with cancer.
But that's a caricature.
It's not.
Nobody, but no one does this.
No, it's not the flatworms and the tadpoles.
Those are pretty sure.
I'm pretty sure they don't have phenomenal self models.
I, I, I, I am not, I don't know that I agree.
And in fact, and in fact, uh, yes, yes, no, I'm not.
My, my goal is not to try to weasel out by, by, um, focusing on so-called lower life forms.
That's not, that's not my argument.
My, my argument is that, uh, and, and by the way, I'm not sure that like earlier you said, you know, X and Y is not conscious.
So I'm not sure.
I don't think it's a binary thing.
I think, I think it's a continuum.
Sure.
And I don't, and I don't, uh, I, I do not discount any of the systems that I work with as not having something that makes it trivial.
Uh, in fact, in fact, down to the, you know, down to the, to the vegans who eat plants.
Uh, I, I, I don't think that's a, you know, I don't think that's an easy, uh, answer there either, by the way.
Oh, that's very interesting.
I'm a vegetarian on ethical grounds for 48 years, by the way.
Better than not, but still, but still you've got.
Sure.
Sure.
You create all kinds of indirect, uh, uh, suffering.
So, um, let me get at this from another angle.
So I have an intuition, the intuition that is, is that the interesting point actually falls out from you is that moral objecthood might be a graded property, you know, across the spectrum.
Yes.
And then even in that ethical realm, there are no binary yes and no answers, but I'll, I'll tell you a story.
So I'm in Brussels, uh, in this, uh, AI thing.
And there's this, uh, highly decorated lawyer coming and say, I really don't understand your thing about synthetic phenomenology.
Can you explain this to me?
I said, I said, yeah, I think we shouldn't recklessly risk the creation of suffering, conscious suffering on postbiotic systems.
And then the guy says, yeah, but listen, we punish our children and we torture and punish animals when we want to domesticate or train them.
And that's an established cultural practice.
And if we could make the learning curve of some of these artificial systems steeper by endowing them with conscious experience and punishing them and torturing them, uh, we have a cultural tradition.
Why shouldn't we do that?
And I just thought, oh boy, oh boy.
I see how you take, um, but isn't there actually a point, uh, that we say, yeah, we, we deliberately make other conscious creatures suffer if it's in our interest.
And why shouldn't we deliberately create conscious AI because we have this idea if it could become more efficient, it could speed up progress if we can torture it a little bit, punish it a little bit.
And it really hurts.
Yeah.
No, I mean, look, uh, yes, I, I, that's, that's not a, that's not, uh, the way to go, but, but, but, um, I, it as, as, as optimistic and, uh, positive as I do.
As I generally am about the universe, I do think there is one true tragedy and, and the true tragedy is that it is impossible to not have your hands on that, on that lever.
That, that, that's the tragedy is that, is that as far as I know, there is no way it would, wouldn't it be nice to be able to say, I don't know how to make this decision.
I don't know how to prioritize the suffering of this creature versus that creature.
And so I'm going to literally do nothing.
And my, you know, whatever you want to call it, my, my, my sort of karmic ledger there is going to be neutral and I'm not, I'm just not doing it.
I'm not doing either, either thing.
And, and the true tragedy for all of us agents that are complex enough to understand the choice is that I don't believe that's possible.
All of us have to, we have to pick and, and, and the best we can do.
And that's, you know, that's horrible, but, but there it is.
And the best that we can do is pick as best as we can.
So I, you know, I don't, I don't, I don't claim to have a solution to, to, to the philosophy of ethics, all, all in all, but I think that's where we are.
I think, I think all of us have to, have to be very conscious, conscious and deliberate about what choice we're making, because I don't believe there is no avoiding making that choice.
You know, I think you, a sense you have heard about this, there is a classical philosophical answer to your question, which is, one, you move the lever, you do whatever needs to be done in the world, in the most rational and evidence-based you are, based way you are capable of, including in the most compassionate way you are capable of, which includes
also, maybe improving your own compassion and training it and looking what the conditions are for it.
But then, like, you make the choice, because as you say, you have to make a choice as long as you're embodied in this world.
But you do it out of a quality of choices awareness.
So, you do it without egoic attachment, you do it because you are thrown into this embodiment, into this situation, and you move the lever.
But you try to do it in an additional mental state, which is actually choiceless. Pure awareness, observing without an observer.
This answer has been given, it has been given, for instance, in the Bhagavad Gita, but many, of course, sages in Eastern traditions have spoken about something like this.
Like, acting compassionately, as skillfully as you can, under these limited circumstances, but in a detached way.
So, does this ring any bells in you?
Yeah, it rings many bells. I've heard this, and I'm sympathetic to it.
It's beyond my pay grade to tell anyone how to achieve it.
There are plenty of people who have, you know, tried to guide others to that.
I think we all have our journey in that direction.
And there's, you know, there's not, there's not much I can add to it.
Much, much smarter people than me have.
Okay, now, going back concretely to your own discipline.
I mean, if you had to say what people really shouldn't do, would you have, could you produce a list of, say, red lines,
but the kind of research that you are trying to foster, or even only develop right now?
What shouldn't one do?
Absolutely. Okay, so the first thing I'll say is that any such list is not going to be optimal for the same reason that you said earlier,
which is that we cannot foresee in advance what, you know, what any particular research program is going to give rise to.
So, for sure, we have to know right away that it's not as if we're going to do this, and we're going to do it right,
and then everything will be will be great. I mean, that's naive, that's not going to happen.
We have lists, we have very long lists of things that we that like that.
These lists exist as guidelines for the research industry.
So the kinds of things that somebody that does when they go fishing, you know,
we need a month of writing protocols and reviews by ethicists and, and, and veterinarians and whatever before we can even begin.
So, so there is, there is an enormous structure around this for the, for the biomedical research community.
And then all of us have our own personal things that, that we think are not sufficiently impactful to reduce overall suffering that we are not going to do them.
So, so, so all of us researchers have various, you know, various lists.
However, I'm, I want to be careful, there's a, there's a free rider problem here, which is that.
Many of us will have some kind of biomedical need, we will, we will run to the hospital, we'll get treated, we will say, I am so glad I got treated, or my kid, you know, is better.
And I'm also glad I wasn't the one that had to do the experiments that led to this.
Right. So this is, this is a problem.
So I don't want anybody feeling too good, you know, that they've avoided some of these experiments that they themselves didn't want to do.
It's for the same reason that people, you know, who could, they don't want to hunt, but, but they go to the grocery store and they get their steak.
Like, you just have to think through where these things are coming from.
And, and this is part of what I mean as, as the tragedy that, that we have right now, which is that you cannot take your hands off the wheel.
Unless you're really willing to just, I'm, I'm just not willing to help anybody.
And I've made that this or myself, and I've made that decision.
All right. I don't have a knockdown argument against it, but I think it's insane.
How should we think about say Californian tech investors who want to drive this with financial incentives?
This development in AI and which will eventually stumble into synthetic phenomenology.
Do we as researchers have a responsibility to stop them or change the incentive landscape or not take certain rewards?
I think the problem is even worse than that, in the sense that I don't think it's just the incentive landscape.
If it was, if it was just the financial motive, that's one set of problems that we could try to deal with.
But I think it's worse than that, because I actually think that the much more what I consider the much more noble and positive vision of relieving human suffering.
That track goes right through the the discoveries that you need to do exactly this.
So even if so, this is this is my point.
And then then I don't know what to do about the investors who will absolutely stumble into it at some point.
I don't mean I don't mean we should ignore that.
But even if we had some kind of magic wand to make that go away, we would still have the much more, I think, difficult, morally difficult decision to make, which is that if we want to continue to relieve biomedical suffering, we are going to inevitably have to discover these very things that are problematic.
And, you know, it's not just a matter of somebody stumbling into it because they wanted a better, you know, GPT or whatever.
It's that understanding what it is about cells and tissues that give them larger ability to to understand the environment around them and to have preferences and like understanding all of that is exactly the road to what you're talking about.
So I don't I honestly don't see a way around it other than developing the same kinds of and obviously that's, as I said, with the pigs and everything else, that's extremely problematic.
But trying to develop ethical guidelines around these things in the way that we develop them for animals in the food industry, for companion animals, for, you know, other other other beings of diminished capacity and so on.
We already have like I think that aspect of it and again, I'm not I'm not a professional ethicist or anything like that, but I think we already have that problem just not in not, you know, not as scalable, but we already have this problem.
We deal with with other beings all the time who are who are not, you know, not not not quite like us and even and even and even with us.
Right. It's it's how easy is it throughout human history and to find examples of of this kind of like, well, they're a little bit different from us, so their suffering isn't quite as, you know, not as problematic.
Right. Humans are very good at that kind of thing that that to me, that is what we need to fight is because the right and to develop, as you said, to develop a mature theory of suffering and apply it then not just to these novel agents, but to everything and everyone that we deal with.
You know, I'm actually learning something that will alleviate my future suffering from Michael today, that you can actually ask for two minute breaks.
Never occurred to me. So I will try to pick up that thread.
So there is this concept of ethics washing like green washing, which means that large companies, Google or even political institutions install ethical debates, cultivate them in the public with panels, committees, even publications in order to postpone, distract from the real ethical issues.
So there's this concept of ethics.
So there's this concept of ethics that's called the pacing gap, which is the temporal gap between the implementation of a technology and how long the political institutions take to regulate this is often four years or more.
And there are people who exploit that time window.
Now, here's the question I'm battling with.
You say we just need this ethical debate. Yes.
Yes. And we have on the fly, we have to come up with some good rules and regulations to do all this.
But the thing is that we know in advance, for instance, if we know we cause suffering to pigs and we have a theory of suffering for pigs and all that, society as a whole will not care.
They will treat pigs very badly knowingly.
We know that in advance.
Yes.
So the same thing as if we now start to work on the ethics of synthetic phenomenology and postbiotic systems, we know, given the global or the American cultural context, the technology will be deliberately abused by some people.
And we know compassion will generally be low.
We know that.
Agreed. Agreed.
And we have the same problem with the traditional high level intelligence systems, which are human children.
We have the exact same scenario, right?
Well, what I mean is that even when I mean, I'm agreeing with your point that even when we have utter certainty that if anything matters and can, you know, in a moral sense that that human children do, we still have the scenario where some number I mean, statistics, but some number of them are raised in environments that are just absolutely atrocious.
Right. So we already have this.
It's not just the pigs and it's not just the the synthetic beings.
It's it's it's all of us.
That's why that's why I'm saying that this this this project of figuring out what to do about other minds and suffering and basically anyone who is not you.
Right. The other minds problem is a is fundamental.
And it's not just about AI and it's not just about sin bio.
It's about it's about all all sentient beings, which, you know, going back to the tradition that you were talking about.
But let's turn a question another way.
So in this new book, The Elephant and the Blind, I have asked the question, if we could create a machine that reliably just stays in a state of pure awareness, no preferences, no preference frustration, just pure awareness for as long as it exists, like a meditation machine or something like this.
Would there be anything wrong with creating such a machine from an ethical point of view?
An ethical point of view, I don't think, you know, because you wouldn't create suffering, you would have created synthetic phenomenology, but you would be certain you create no suffering.
And I was struck sometimes when I read papers by you, there is these short light that goes on where, for instance, you talk about the idea of a system that is liberated from the global self and
abides in some kind, you call it even a neuronic void.
Do you think there would be a way to, how to say, to focus research on suffering free phenomenology, like without having to test a theory of suffering and then inevitably creating suffering, but
circumventing this, creating the first non-suffering conscious systems in the history of this planet?
Is that possible?
Overall, what I'm going to say mostly is I don't know.
I think the deepest questions about this, I think currently we do not.
And so I couldn't possibly give you a, you know, a defensible answer.
But my, and by the way, this is one of the reasons why the Nirvanic stuff was written by actually my co-authors on that paper, right?
So I'm not the expert on that stuff.
We collaborate closely because I'm very interested in it.
And I think that that body of thought has a lot to offer here.
So, so I, you know, I collaborate with a number of people in the Buddhist tradition and so on, as I think that's something we need to, those are tools that we need to use, but I'm not, I'm not the expert on it.
I think that overall I'm optimistic in that much like with everything else, while we can't predict everything without trying it, we can often predict a lot of things without trying it.
And much like with conventional bodies, you know, I could come up with all kinds of scenarios.
And if I were to say to you, so I, you know, let's, you know, what if we do this, what, how would that be?
And you'd say, Oh, I don't do that.
That's terror.
I don't want that at all.
And so, right.
So, so we have, we have some ability to predict some of this.
And I think as we get better at these things, we'll have more ability, but I'm under no illusions that we will be able to avoid it entirely.
We're going to screw up.
We're going to make mistakes just like we have with medicine, you know, throughout the, you know, throughout the ages, we are going to mess up.
So, you know, I don't know, there's a, there's a bigger, there's a bigger picture here too, which is like, I did a, I did a poll once on, on Twitter.
And this, these, these were my followers who are already kind of, I think, skewed towards the technological end, but I asked them, I said, okay, just imagine you're, you're, you're a primitive, you know, orally hominid.
And you've just, you, you, you're, you're walking back to your camp and, and you, and you, you, you have this like vision of fire of what it is, but, but also everything else that comes after that.
So, so, you know, metals and swords and landing on the moon and, and, and, you know, artificial hearts and space shuttle and stem cells, you know, spinal cord transplant, like all of that stuff.
It just, you know, you suddenly see it all.
Do you keep going and show them the fire that you've made, or you put it out and you stay in your, in your cave and, you know, and then that's that.
Six percent of, of, of, of the people who read my stuff, six percent thought that you should put it out and, and, and, you know, and stay in the, stay in the rain.
And that's that.
So, you know, I, I, I think, I think what's inevitable is that we're going to make some mistakes.
And I think we do have some, some tools and some traditions that may help, but we are going to make mistakes.
And the real question is going to be, do we keep going or, or, or not?
And I don't, I mean, I'm not.
I think that's not the real question because humanity will keep going.
Somebody in China will keep going.
Even if you were going to shut down all your, all your labs.
I agree with that.
I agree with that.
It's like an unstoppable dynamics in a certain way.
The question is rather for me, if we think about the project of synthetic phenomenology and these scales,
it's very clear that our suffering has a lot to do with our form of embodiment and metabolism and, you know, homeostatic failure, allostatic failure and stuff like this.
Could there be systems who realize phenomenology and intelligence, but are uncoupled from this evolutionary thread in a way that they don't suffer from their embodiment?
Is that in principle possible?
Or does this vision you have of scale-free cognition imply that any future version of cognition or phenomenology will necessarily involve suffering?
Necessarily?
Necessarily?
Because that's just how it works.
Entropy and all.
Yeah.
I mean, the real answer is above my pay grade because we can talk about liberation from desire and from suffering and these ideas that are present in the, right, in the kind of Buddhist traditions.
I don't have the background to say anything intelligent about that, although there are many people I know who would to under conventional under conventional sort of modus operandi.
I don't think so.
I think to the extent that you are a viable cognitive system, you have goals, and if you don't measure whether those goals are being met, then, you know, I'm not sure.
Like, I don't think you can have a cognitive system without some goal directedness.
But what happens, you know, in these liberation states, I have no idea.
I mean, I don't know.
Could one have an architecture that does go frustration and everything and intelligent behavior but has a layer that is not attached because there is not the phenomenology of ownership and identification that is brought about by a transparent self-model.
For instance, a system that had a completely opaque self-model where the system would realize this is all the virtual governor.
This is all virtual.
It is a representation and that would be part of its phenomenology.
Right.
Yeah.
I don't know.
I mean, on the one hand, if we take seriously the idea that some humans have in the past become liberated in that way, then that should be reproducible in some other, some other.
We don't know that.
We don't.
We don't know that.
Yes.
I mean, we don't know that.
This is folklore.
It's fairy tales.
We have no way of knowing.
Correct.
We don't, which is why, which is why I'm saying, I mean, I don't have an answer to this.
Right.
I don't know whether that's true or not.
I guess, you know, it, one of the, one of the difficulties I have with that view in general, and I've asked people this is if, if you do have sufficient amount of, you know, detachment or decoupling or something, what is it that, how do you choose your goals then?
You know, what, what, what, what do you do?
And I've asked this, like, if ever you just imagine if everybody on the planet, uh, acquired that state, assuming that's possible, you know, they sort of detached.
What do they do after that?
Like that, that's it.
Do we still, do we still go to Mars?
Do we, do we solve other problems or what?
Well, if you look at these empirical evidence, we have a simple thing is that Buddhist monks and nuns do not have children.
So for instance, procreation would stop very likely.
I think that's a plausible prediction.
Yeah.
Possibly, you know, there's the Fermi paradox on, and one of the possible solutions I have thought about this is that civilizations at a certain advanced stage, just stop to procreate for some reason or other, because they change their own architecture.
Or because they come to a philosophical conclusion that we cannot really imagine that non-existence is better.
Yeah.
And that that solves the Fermi paradox on, but if everybody would become a Buddhist practitioner, mankind would cease to exist.
That's pretty obvious, right?
What we are handling in Buddhism is the craving.
How intense the craving is, is to reduce the craving rather than not reproduce and not continue doing what we're doing.
But we're doing it with compassion and being more present and thoughtful.
Yeah.
Yeah.
But we're not reducing into not reproducing or anything like that, or sitting, not doing anything.
Yeah.
Do you think there is something like compassionate child rearing or compassionate, skillful procreation?
Or is it actually, that's deep water, I know, it's also a strange question, but is it unethical to risk that your child will not profit from the fact that you're a Buddhist and will actually suffer?
Yeah.
It's a very interesting question, isn't it?
Yeah.
The only other is so right.
So I don't pretend to have any anything to say about the one way or the other about that question.
But there is something else, which is that I don't really believe that we make other minds in any circumstance.
We don't make them.
What we do, what I think now we do is we create embodiments through which various kinds, the same, the same way that the, you know, the laws of geometry and math ingress into the physical world.
My gut feeling is that this is, this is how minds come to be.
So we make embodiments, they become inhabited by, by minds in the same way that the laws of mathematics become inhabited in physical machines.
And what, what, what, what sort of, those mysterious minds are platonic entities?
Well, yeah, the way that, I mean, yeah, the way that the way that the mysterious triangles are platonic entities, there's plenty of, there's plenty of them.
It's just that the traditional, I think the traditional view is that these things have to be kind of low agency things like, like, you know, rules about triangles and things like that.
But I, I, I think it's, it's the properties that define the, the, the, the surprising non-physical properties that we encounter when we make machines, when we encounter them all the time, I think are of the same kind generally as the surprising cognitive properties that we find when we make certain other kinds of systems.
So from that perspective, right, from that perspective, one could ask whether, whether what you're really doing is providing or, or, or refusing to provide a physical experience for certain potential minds, let's, let's put it that way.
Yes.
And right, and what, and what the ethics of that is, right?
So, so, so my point is also that, right, if you suddenly decided, you know what, that's it, no more, no more life, no more, that, you know, that's it, off it goes.
I'm not sure that, again, I think we're back to this idea that I don't think you can take your hand off the, off the lever.
I think if you did that, that would be a definite choice in, in refusing to provide an embodiment, which, you know, I don't know, what's the, what's the purpose, right?
Is there a purpose to the embodiment?
Like, why are we here and, and suffering?
I mean, I'm not going to try to answer that, but, but if there is, then refusing to provide any kind of vehicle is also an issue.
Yeah, I would term your point, the potentiality argument against antinatalism.
So if one really becomes radical about this, and I have learned that I don't want to talk about it in the public, but if people, there are different philosophical versions of antinatalism.
But if people say we shouldn't procreate, and end this line of embodiments radically, well, of course, makes a decision about a large space of possibilities, you know, one loses future potential.
And it's almost certain that one doesn't know how large that space of possibility is that one shuts off by not procreating, that could be a type of argument that it is unethical to, I say, to preclude a very large number of possible embodiments without even knowing how large that possibility is.
I don't know, I don't know, I don't have a position on this, this is too difficult for me, but of course, but don't you think, maybe as a last question, don't you think that there is in this whole development, in evolution, in science,
in the evolution of culture, in the evolution of self-deception, in all of these processes, that there is a force in it, that we probably couldn't even resist, if we wanted to, in any way.
Does this have a momentum of its own?
Yeah, it probably does.
Again, these are deep, deep waters, we don't know anything, but it probably does.
But for whatever it's worth, my personal feeling on this is that we are floundering around at the early stage of our development, kind of like a baby, you know, that everything is, you know, you feel wet, you feel hungry, you don't know what's going on.
But we are at that early stage where I think that I'm optimistic about the path forward.
I think there are some fundamental problems.
I don't think we can get through it without a lot of the suffering that we have now.
But I think ultimately the force that you're talking about, the momentum is to push through that towards more intelligence in the universe, not less, more, more, better, more, more joy, more discovery, more, you know, experience of life as a positive thing.
I think that's, that's, that's the future that we should, we should aim for.
I'm definitely more on the pessimistic side and on the, I think it's more important to reduce suffering than to increase joy.
But given this funding, resources for research are limited, how would we best allocate resources?
Where do you think would we get the biggest bang for the buck in all of this?
Yeah.
I, I, I think that the, the biggest area that stands to contribute to all this is the field of diverse intelligence research.
All right.
Right.
Right.
And, and really, really trying to break down.
I mean, I think we as, as humans and also have being a product of, of certain educational systems,
we have a lot of built-in firmware that makes it hard for us to recognize minds that are not like ours.
We are very people, there is so much certainty out there.
People say, well, this, this thing can't that, and this doesn't, you know, and they don't,
just, just, just the idea that first of all, to, to, to make the, make people understand that first of all, you don't know.
And second of all, there are efforts that, that are out there that can help us to figure this out and that you can't just sit back and have assumptions about what anything is.
We have to have a mature science of it and we have to understand, you know, part of, part of making ethical choices is understanding what you're choosing between.
And I think, right.
And I think a huge number of people are making decisions in this space and they have no idea what they're talking about.
And, and, and, you know, and, and they're not informed in the way that you, you need to be informed to even begin to make, make some kind of choice here.
So, so I, I think putting this into diverse intelligence research and is, is, is, I think that's the best, that's the best shot we have right now.
Yeah.
You probably get the idea.
I would have, I would like to see really good work on a theory of suffering under the condition of achieving epistemic progress without creating more suffering in the process by having to do a lot of very ugly testing, you know?
Yeah.
Yeah.
And, and, and I, I don't disagree with that at all.
And I think that is a, that is a fabulous sector of the diverse intelligence research that should get much more, much more attention for sure.
All the audience is wishing the thanks for a brilliant conversation today.
It's organic and it's very refreshing to learn from both of you today.
Yeah.
Thank you so much.
I appreciate it.
And thanks Thomas.
We'll, we'll be in touch.
Thank you for joining us.
Have a good day.
Bye-bye.
Bye-bye.
Bye-bye.
