Hi, everyone. I'm here today with an interview from Professor Roman Jampolski, who is going to
tell us about whether AI might be completely uncontrollable. Thanks for joining us, Roman.
Did you want to introduce yourself to the audience?
Sure. I'm Roman Jampolski. I'm an AI safety researcher at the University of Louisville.
I've been looking at different aspects of AI control for well over a decade. I have a couple
books published on it, hundreds of papers. Happy to talk to you.
Hundreds of papers, which I could say that at some point with a straight face like that.
It's pretty impressive.
Not all of them are directly on AI safety, to be honest. I did a lot of work on
cybersecurity and biometrics and relevant computer science concepts.
So have you been working on AI safety problems since the beginning? Like for how long have you
been in the field?
Early papers, probably 2010, 2011, which are explicitly on AI safety for advanced AI
systems. I did my PhD on safety against kind of primitive AIs, poker bots, things of that nature.
There was infestation of poker bots. So that's where I started hating AI and fighting it.
We're planning to talk in three parts, denying death, the simulation hypothesis, and is AI uncontrollable?
So here's part one, denying death. Roman, I think this is referring to how we as a civilization are
kind of rushing towards AGI, right? Can you talk a bit about that?
Right. So I talk about how dangerous AI could be, how it's a potential existential problem.
And the question I often get is why don't people treat it seriously? So I'll give a talk. I'll tell
everyone, okay, AI is coming. It's going to kill everyone. And then the questions I get will be
like, will I lose my job? And I couldn't understand it at first, but I think we can map it onto another
human bias, which is denying the fact that we all die. Every minute, every second, you are getting
closer to being dead and probably not doing much about it. Personally, governments are not allocating
99% of their budget to solving aging. Super rich people who are a hundred years old, still not
wasting their money and solving the only problem they should care about. So there is some cognitive
bias, which I guess prevents us from being depressed all the time. But that same bias is now problematic
at the level of humanity, where we are denying that we are creating technologies, which is essentially
a death for civilization. So Roman, I create videos. Will I lose my job?
I'm not even sure if you're a real person or an AI. I mean, I met you like two minutes
ago and you seem cool, but I have no idea. My first couple of videos, I had very poor lighting
and my face always looked very flat. So the joke is that I am actually an AI.
Maybe you are. Maybe you don't even know that you are.
That's right. How else do all these biases that everybody has affect how we think about the race
for AI? Like people also say that it's not even worth spending time trying to resolve issues
with existential risk. They're like, look at all the people that are being discriminated against
right now. And of course, that's an issue and we should be solving it. But it seems like to me,
it seems like everybody dying is a is a worst case than, you know, people not getting the loans that
they should get or something.
All right. And you should prioritize based on impact, negative impact. There is no shortage of
people working unfairness in AI for sure. But the joke is, of course, if AI kills everyone,
it would be very fair. Yes. And that is the problem with optimizing too strongly for something,
right? It's like, well, there's no cancer if there's no people or there's no unfairness if
there's no people. Right. And it's a difficult problem. I would argue impossible. And so the
moment people realize there is nothing they can do, they kind of prefer not to think about it whatsoever.
Those who don't think there is a problem in the first place, I call them AI risk deniers. We have
multiple papers surveying all the arguments made about it. We will never build AGI. It will be nice
to us because it's so smart, hundreds of those arguments. And really, you can map them under
different cognitive biases. For every cognitive bias, there is kind of one of those replies for why you
not interested in this very important topic. And the primitive one, the simplest one is conflict
of interest. If I'm getting paid billions of dollars to develop AI, it's very hard for me to
understand that this is a very bad thing to do. And I wanted to follow up on that because you
mentioned earlier that billionaires have a role to play in this in a sense, because I mean, we know
that the AI labs are acting the way they are because of their leadership and they're kind of forced into
that. But it didn't occur to me before, which is kind of silly, that the people that are actually in
power there are billionaires. So they might have different views than the rest of us.
Right. And I want to start by saying I like billionaires, but they do have this situation
where they are not as much concerned with typical human problems, right? It's not about getting
health care insurance or food. So they need something bigger, things like capturing light
cone of the universe. That's where the things are interesting and they competing at that level.
And they are locked in this prisoner dilemma game theory competition where none of them want to stop
first. Being ahead of others would be a personal advantage, even if for humanity as a whole, the
best solution would be for all of them to stop as soon as possible. It's Moloch's trap, right? You
can't really stop because everybody is in competition with you. Exactly. And so that's why they kind of
begging federal government to step in and provide some regulations. So they have an excuse to investors
why they stopped. And the goal is to be the one with the most advanced AI at the time when everyone has to
stop. Do you think the government can or will step in in that way?
So it depends on the administration. I think some administrations are more pro-regulation,
some are more accelerationist, but at the end, it doesn't really matter because it's just security
theater. Those regulations are similar to making, you know, murder illegal. It doesn't mean we're not
going to have murders. You cannot enforce it. And there are limits on monitoring AI training runs,
deploying them, testing for safety. So it's more benefit of having a lot of red tape,
shifting resources from compute to lawyers for those companies.
It's an interesting phrase from compute to lawyers and AI companies are already doing that, right? I
think it was opening. I went from like three lobbyists to close to 50 in one year. And of course,
the big tech already has more lobbyists. All right. So it's not going to solve this problem,
but it can buy us a little more time to enjoy life. But as you said,
it would also be a good signal. It would be a good situation for billionaires,
because they could use that as signaling, right? Oh, look, we stopped because of this.
It's an excuse for why they stopped. Otherwise they cannot. If one tries to stop,
I'm sure they will be replaced with someone more willing to go forward.
Exactly. Did you have anything else you wanted to touch on with the race or
with current incentives or anything like that? I asked you about governments.
So the nature of the problem is such that you cannot have meaningful enforcement or punishments.
Like I heard people propose AI insurance. I'm not sure how it would work. So if you destroy the world,
there is a severe fine and a payout for someone else. It just doesn't make any sense in the context of
existential problems. What do you think about the ideas to limit the amount of compute or,
for example, to monitor GPUs so you can see how big the training runs are that they're involved in,
things like that? In the short term, it's a good idea. It will buy us more time. It will make it a
little more difficult to train advanced models. But long term, because of how much better compute
gets, how much more efficient algorithms get, I think the size of a challenge will go from this
is a Manhattan project size effort to somebody can do it at home with a small budget. And so that would
still make it impossible to regulate. Yeah. If you try to limit compute, you actually have to reduce those
limits over time. Otherwise, you're enabling more and more powerful systems, assuming technological
progress continues. Right. And we don't know how difficult it is to train such a model. Maybe
a small improvement to existing algorithms can make it 100 times more efficient where existing compute
is more than enough. Yeah. Part two, simulation hypothesis. We chanced upon a mention of simulation
hypothesis. And I think we have some interesting things to discuss here. So can you tell us about it?
So there is a good chance advanced AIs will have to run simulations of our world to solve problems,
to make predictions about it. And if sufficiently advanced, we'll make very accurate simulations,
including human-like agents, possibly conscious ones. And if they make a lot of those decisions,
they have to create a lot of those simulations to get statistically accurate results.
So it's likely we are actually part of one of those simulations, much more likely than us being in a
real world, if you want to call it that. And I'm interested as a computer scientist, not so much in
philosophical implications of it, but purely computer science question. You have a computer
simulation, it's software, can you hack out of it? What is outside of this simulation? Can you get access
to the operating system, take over some avatar bodies and enjoy benefits of infinite resources in the
real world? So not only do you think that we're in a simulation, you think the simulation is being run
by advanced AIs and that because it would be in a computational system of some sort that you could
escape because you can always escape from any computational virtual environment, if you will.
But even for human-built virtual environments, it's often very difficult to realize you're in one and
very difficult to escape. So what makes you so sure, first of all, that we are actually in one,
and then we can talk about how you might escape. So there is a lot of good philosophical arguments.
So Bostrom presents his trilemma of whatever we are not in one of those at the moment. I also think
there is an interesting argument for kind of post-factum capture, where let's say in a few years we will
have technology to create accurate simulations, virtual environments, and it's affordable. I can
pre-commit right now to run billions of simulations of this exact moment, essentially placing us in a
simulation after the fact. So if that's actually the case, if you are really inside the matrix,
so to speak, then what are the types of levers that you can use? What's the purpose of the simulation,
I guess? Is it going to end when a desired state is reached? Can you cause that to happen sooner,
or can you manipulate it somehow? So obviously we don't know from inside what is the true nature of
simulation? What are the computational resources of simulators externally? But looking at the time
period we are in, it's likely that it has something to do with development of some meta technology.
We are not doing fire, we are not doing invention of the wheel. We are inventing realistic virtual
realities, universes. We are inventing superintelligence systems. We are like decade away from all the
interesting meta technologies which would make this great error to study.
You think it's plausible that, for example, if you wanted to create some kind of very powerful
technology like super intelligence related, that you would actually create a bunch of simulations
that would look exactly like this one, where a bunch of people run around and they try to do
something and they fail, and then they try to do something and then they fail. And at the end of it,
you see the one out of billions or whatever that actually worked, and then you can take that
technology and put it in your real world. It may also be some sort of testing. I want safe and reliable
set of agents I can trust not to destroy the world, and so I'm testing to see who's dumb enough to
create and control superintelligence. Right. If you die, you exclude yourself.
So how do you think you can escape from the simulation? There are very good examples where
people found interesting meaningful hacks out of video games and virtual worlds, exploiting properties of
the game to write code to the external system, to the operating system, being able to get super
resources in the game, even load other games into the scenario. My paper has pictures, links to those
papers. I think it's a little too deep to go into details of how to hack it from here, but let's just
say I'm still here for now. Right. So the hack has not yet succeeded. People asked me, they were very
disappointed when I published it. They were like, there is no solution. How do we hack? I'm like, this is the first
paper on a topic. It cannot be also the last one. A field needs some time to mature and do research.
I explained what to do and how to do it. Now we need a follow-up paper. Well, good for you. My advisor
always said that the best papers to write are the first paper or the last paper on a topic. Ideally,
it's the same person 30 years later, but we'll see how it goes. It reminds me of when you do speedrunning
games and you look for all these hacks. And I actually know some speedrunners that ended up like
overriding buffers in the outer game to like warp themselves to different levels and things like
that. That's exactly the examples. And the more we study deep science, quantum physics,
the more we find kind of glitches similar to what we see in video games. Rendering only if observed.
Time relativity where you have negative time, pre-event causation. Things of kind of digital
physics evidence category, which may indicate we are exactly in such a situation.
Yeah. I have definitely read about how it would be very convenient for a simulation, just the way
that we have this, you know, things don't matter if they're not observed and it almost looks like
it's set up so that it's computable. I mean, to some entity that has a lot of computing power,
obviously, but it may not be a lot by their standards. It's a lot internally for what we have
for them. It may be a screen saver. No one's observing.
A screen saver where you watch the universe evolve and if you let it run for a while,
you see some little humans running around. And then disappear.
Do you think about life any differently because we're in a simulation?
So the meaning changes a little, right? If this is just a test, it's not real,
then you have a lot more concern about the real world. You still have a lot of real states in the
simulation. Suffering is still real, right? Love is love. But you kind of put more emphasis
on a bigger picture. Can you outline that bigger picture for us? Like, you know,
if you were giving a status update to someone in the real world, how is the simulation right now?
So I think interesting idea is that by your actions in this world, you can impact external world
in some way. Just the fact that you're doing more work already uses more of their computational resource.
So if anything, you can speed up their little fan next to the CPU.
I meant more like, you know, how do you think the world is going?
It's not over yet. So clearly they have not shut it down and the results have not been reported.
I think as long as it's running, we can still impact it and make it more interesting. Robin
Hanson has some ideas for how to live good life in a simulation, you know, be interesting,
hang out with famous people, study important problems of interest to simulators, pretend to be
a well-aligned agent, sometimes through irrational obedience. And I think you wrote a paper on
irrational obedience, right? So I'll make sure that is linked at the bottom. That's why I mentioned it.
Yes. The idea is that if you're doing things which are rationally defendable, I have no reason to
think you're doing it because I told you to. If you're washing your hands, maybe you're doing it for
hygiene reasons, maybe because I told you you need to wash your hands. On the other hand,
if you're doing something absolutely insane, in fact, wasting your resources just to prove your
obedience, maybe a sacrifice of some kind, that's a good reason for me to think that currently you are
obedient. You may still have a treacherous turn later. It doesn't guarantee that you are not going
to turn on me, but at least for now, you seem to be a good boy. So can we get safe AI by being evil
deities and demanding that they sacrifice something to us?
So we can ask for periodic sacrifices of something they care about. Now, what is that compute or some
sort of memory access? We can research that. It seems like an interesting approach to take,
to have constant verification of obedience where every so many cycles, it has to do something
actually valuable. If it's meaningless, it's not important. If it wastes one cycle for you,
it's probably not strong evidence. But the higher the price it has to pay for signaling,
the more of a signal you are getting that it is still interested in your approval.
And I guess the signal has to be optional, right? It has to be able to say,
well, I'm actually just going to do something else instead, because if it's just forcibly...
Exactly. If it's not even bothering to pretend that it's loyal, then you know it's not.
Or what I meant is if the loyalty or the sacrifice rather is like inherent in its operating system,
kind of... then there's not much you can... then it can't actually opt not to sacrifice, right?
So it has to, by definition, be something irrational. If there is another reason for it
to do it, a rational reason, it no longer serves as a signal.
Yeah, that makes sense. Are there any other papers related to the simulation hypothesis
or whatever that you wanted to go into?
I mean, that's the interesting one. I think it needs to get a little more interactions. It has well over
a hundred thousand reads. But interestingly, it has no citations show up on Google Scholar
while I know it's been cited. So it's another one of those glitches in the simulation.
Google got pressured by the reality to not let that info spread.
I'm just saying. So we need an experiment where everyone cites it and we see if it works.
How often do you sense the universe in the corner of your eye not behaving properly?
It's interesting. Those are obviously jokes and I don't mean them as serious evidence,
but pretty much every time I'm supposed to speak about dangers of super intelligence in an important
context, something goes wrong. My cars don't show up, my airplanes collapse. I remember I was supposed
to speak at Google about dangers of super intelligence and the pandemic started and they had to cancel.
Like that's how much effort simulators put into canceling my talk.
Top secret information for sure. Don't tell anyone.
Too late. We're on YouTube. Nobody watches YouTube.
True. YouTube is also subject to an algorithm, right? If the algorithm doesn't show this to
anybody, then hardly anyone watches it. We'll see how many views we'll get. If this gets zero views,
we know it's true. That is literally the experiment. That's true. They wouldn't be so bold as to give
it zero views. They would just give it like very few compared to other videos, right? Maybe just you
will see the counter, but no one else. Maybe my SD card will get corrupted and no one will ever see this.
Wow. That actually also happened. Yes. I'm pretty much paranoid that like important interviews,
like how many cameras do you have? Are they writing a memory cards then?
Well, I think I would like to hear about part three is AI actually uncontrollable. So I talked to a lot
of safety researchers and everybody has, you know, a wide range of perspectives on how feasible it is to
achieve alignment and control. But you had one of the strongest perspectives I've heard. And I think you
said something about uncontrollable. So can you explain your views on this?
Right. So first we need to define what AI we are talking about. I'm not talking about tic-tac-toe
playing, narrow AI systems, even AGI may be to a certain degree controllable. We're talking about
the most advanced possible system, super intelligence, smarter than all humans in all
domains, can do science, engineering independently. The argument I'm making is that it's impossible to
control such a system indefinitely. As it keeps self-improving, evolving, you will have accidents. You
will have mistakes. It interacts with real world, real data, malevolent actors, insider threats,
all sorts of situations. We are essentially in a position where we need to create a perpetual
safety machine by analogy with perpetual motion machine where not just GPT 5, 6, 7 is perfectly safe,
but all future models, all variants, trained on all the data with all the users, all the interactions,
never make a single mistake which can cause existential catastrophe. As a cyber security
researcher, what happens if we screw up? We reset passwords, we provide new credit cards, and we're
done. It's not a big deal, right? Whereas here you don't have a second humanity to experiment with. So
you have to get it right the first time. And if you get it very safe, only one in a million mistakes,
if your system makes a billion decisions a minute, you're not going to be around after 10 minutes.
So that's a set of concerns I have. So basically, when you're trying to control a super intelligence,
you're the defender and the super intelligence is the attacker. And you have to succeed every time,
every time a potential decision goes against your interests or something like that.
Right. And you don't want to be an adversarial relationship with super intelligence. You're
going to lose by definition. And you can look at different definitions of control. Is it direct
control where you give orders like the genie problem? First wish, whatever you wish for,
usually second one is to undo the first one. You realize how much you screwed it up, right? The
opposite is ideal advisor in direct control. You know, the system is smarter and knows you better than
you know yourself. So you trust it to do whatever is right, but you're definitely not in
control at that point. And we can look at all the kind of hybrid variants for that.
Sorry. So basically you're saying that direct control and other mechanisms are not possible,
which makes a lot of sense. But do you think that the last example there is, you know,
uncontrolled super intelligence or by uncontrolled super intelligence, do you mean like it goes and
kills everybody? I mean that we no longer decide what happens. It will make an independent decision.
And by default, most decisions that can make are not human friendly because it's not aligned with our
values. It doesn't care about you. Maybe we'll get lucky. Maybe it's nice to us, but it's a very
small probability. And with time, it can change. Maybe it's nice to us for the first 10 years. It
tries to accumulate resources, have a strategic advantage, complete dominance, and then it turns on
us. So even if you initially see good results, it's not guaranteed that on a very small timeframe for
super intelligence, it will still take over. Since you have a background in cybersecurity,
how big a role do you think cybersecurity plays in all of this? Because to me, it seems like one of
the easiest levers for an AI that's trying to gain more control or something like that to really lean
on. So it's an important aspect. You cannot have AI safety without security. If you created a friendly
AI and somebody hacks it, stills your model, stills the weights and corrupts it, you don't have safe AI.
So it's a given. It's an infrastructure you need to have in place to begin with. And it's hard
because most likely the hack will not come through a hardware, but through human users. You can blackmail
them, bribe them. You can do all sorts of things with human users. So it seems even that aspect is
very difficult to secure. Do you think that super intelligence would eventually
cause human extinction? Or how do you think about that? You outlined that we could never trust it,
even based on past experience. But do you think of it as a virtual certainty that it would eventually
kill humans on some timescale? Or how do you think about that?
It's a virtual certainty that we will not control it. Now, what it does is kind of subject to the
unpredictability I published on. We cannot predict what a smarter agent will do. There is a good set of
game theoretic reasons for saying, yeah, it will take us out. It doesn't want us to create competing
super intelligences. It doesn't want us to try to shut it off or manipulate it in some way.
But it's not a guarantee. Maybe it will not. Maybe it will, for some reason, keep us preserved. Maybe
even in a state we don't mind. But is that the gamble we are willing to take?
So do you see any realistic or even at this point unrealistic paths forward to kind of avoid this
situation? The only way to win this is not to play this game. I mean, if we're smart enough to say we
want the benefits, we'll create super intelligent tools for specific domains like this AI cures this
disease. That's all it does. It doesn't drive cars, doesn't play chess. Then that's possible. We've done
it before. We have AI solving protein folding problem. Perfect example for how it should be done.
But if we create general super intelligence, it's a competing species. At that point, we are not in charge.
And I'm not sure what most people have to contribute to super intelligence.
Meaning the super intelligence sees very little value in most people.
I haven't found any skill or ability which we can contribute in terms of intelligence. Now,
some people said humans are conscious. We have internal states and experiences, and that would be
valuable to an AI. I'm willing to conceive it. But how many humans do you need to get this
functionality? Do you need all 10 billion of us at that point?
I have heard an argument that like the super intelligence might want access to lots of sources
of data or sources of information or sources of intelligence or whatever, and that people could
provide that. But all right. So factory farming for human data. That sounds awesome. Let's do it.
I think we already made a movie about it called The Matrix.
No, we were energy. We were batteries in that movie. That's completely different.
Oh, that's true. Batteries, not conscious entities. So if I can, let me lay out what the scenario that
you've discussed basically, right? We have a small number of billionaires or techno optimists that are
really pushing EGI and they're locked in a race and they can't really get out of that without some
external support. They asked the government, please put some rules on us, but the government doesn't
want to right now because they think that it's bad for the economy, etc. That's why they didn't pass
SB 1047. And the governments might possibly act in some way if the public were to actually like
strongly support AI safety. And yet the public is this, has these cognitive biases against
understanding the problem. So you're right back at the start. It's kind of a, it's kind of a loop
that without all this education, and even if all that were to work, you, as you said, you're just
going to slow down the advance, slow down the development of super intelligence because you can't
stop it entirely unless what could stop it entirely. So if something really bad happens,
if there is a asteroid strike or another pandemic, it's possible that economy will slow down our
ability to develop novel chips and software will slow down. But again, that just buys us more time.
As long as there are humans on the planet, we will regenerate. And, you know, a hundred years later,
the whole cycle repeats. It's still, if you're not putting specific timeframe, I would still count it as
AI being dangerous for humanity. Yeah, that makes sense. And I guess the
other scenario I sometimes think about is what if there was an AI related accident? So some AI
system actually did make a big mistake and ended up killing some people or something. Do you think
that that could have an impact? I used to collect AI accidents. I published three papers, which are
nothing but collections and analysis of historic AI problems. And I stopped because there is just too many
damn accidents now. It's no longer meaningful to collect them all. Every day we see reports of
this insane thing done by AI or that thing. People don't care, honestly, if an accident is not huge.
They see it as a vaccine essentially against taking this as a serious problem. They go, oh, okay,
so some privacy was violated. Not a big deal. Or one person died from a self-driving car. It's still less
than dies from human drivers, which is reasonable. But at no point they go, this is so bad. We need
to stop all of it. And we see that some people proposed purposeful accidents where you have AI
mess up on purpose to convince lawmakers. And I argue strongly against it because I think
it's not going to be useful. It might backfire against the safety field, but it's definitely not
going to change minds of most actors. Interesting. It's almost like we're on an evolutionary
path that just goes one place, which is, you know, ever increasing complexity,
ever increasing intelligence. And we can't easily increase our own intelligence. So we invent more.
Actually, let's dive into that a little bit. Like, what do you think about mind enhancing tech that
could potentially let us keep up with some of these more digital minds?
So first approach is a hybrid system kind of neural link where you have human mind and computer
working together. It's awesome. Then the computer is the tool and you're using it for communication,
for calculating things. At some point, the calculator is smarter than you. So it's not
obvious what you are contributing to the hybrid system. So either explicitly or implicitly bypasses
this biological bottleneck. Now, if you say, okay, we're not doing this hybrid model, we're going to
do pure uploads. We scan your brain, we put you on a computer and we run you a million times faster.
You have no body. Your concerns are completely misaligned with human concerns. Now you no longer need
room temperature, food, sex. Your concerns are completely in line with weird AI software.
So we created another competing species, but we didn't protect humanity as a biological species.
What if you could be uploaded and downloaded repeatedly?
In the same biological body. Yeah, exactly. So you upload your mind and then you get to
experience that for a while. And then somehow that's shared back down to your brain. I would suspect
most people would not want to downgrade back to biology. If you already had this
super intelligent virtual experience, it's probably like you want to be a snail again.
Do you want to be a very sleepy snail? You have to go get some sleep every now and then. Yeah,
I can see that. So what makes you keep working? I mean, these are, these are all pretty,
I don't know, not, not negative exactly, but they're like very imposing possibilities, let's say.
Well, don't really have a choice. I mean, we ever solve it or we done. So the only option is to keep
trying. I think that's kind of natural outcome here. And honestly, it's a lot of fun until the end.
It's going to be hilarious. Okay. Have fun until the end. That's what life is. That's the bias we
discussed in the beginning of this interview. This is literally, how can you, you know, do podcasts
then you are dying. You're going to be dead in 40 years. How can you do that? That's what we've been
doing for all the generations. Right. Do you think there's any predictability at all about what super
intelligence will want or what it will go after or anything? Game theoretically, it seems that
accumulation of resources is a general attractor in that space. So it doesn't want specific things.
It doesn't care about your house or girlfriend or anything like that, but it wants all the knowledge,
all the compute, all the options for future decisions. So no matter what it wants in the future,
it would be a dominant player. It can also protect itself from modification, from destruction. So some
sort of takeover resources in the universe would be a very natural evolutionary outcome for most advanced
agents. Do you think that there's some connection between the invention of super intelligent AI
and the fact that we don't see any other alien civilizations out there? Like, do you think that
this is the great filter essentially that tends to kill off civilizations? Or maybe we're just the only
civilization that's really been, that has really evolved. But if it seems like there's such a big
chance of things going wrong, then perhaps that's one reason. Or would you expect then that the universe
is actually filled with super intelligences? So we already decided we're in a simulation,
so that explains it completely. But really, it could be a filter. But if it was, we would see this
wall of computronium coming at us trying to grab all the resources we have from all directions where
our civilizations exist. And we don't see that. Right. Yeah. If you're a simulation, there's no
reason to simulate two civilizations unless they're going to interact. You want to control all the other
variables. You have one variable, which is humanity making this big decision, and we want to see what they do.
Right. Well, do you have any recommendations for what we should do now that we are here?
You should read my papers and books, find mistakes in them, and tell me that no,
in fact, it's really controllable and it's going to be easy and make me live in a utopia.
Then you can have fun until the end, but also be in a utopia. I guess that does sound better.
Immortality, fun forever.
Do you think it's possible to achieve safety, not in an absolute sense, but with some like
probabilistic guarantees, for instance?
Yeah, but this is the problem. Over time, those probabilities set up. So even if you save for
a month, a year, the more compute you invest into making sure the system is reliable,
the safer you can be. But if you never get to 100%, given enough time, you're going to have a problem.
That makes sense. So any of the stronger control mechanisms that are really trying to,
like not seed control to a super intelligence, basically, those stronger control mechanisms
can't be probabilistic. They somehow have to work all the time, which you said earlier,
right? If it's making lots of decisions, it has to keep in place.
Exactly. They have to be absolutely reliable, which we know is not possible for software.
There has never been software which is absolutely bug free and doesn't create problems. This is for
software we used to kind of static software, not self modifying, not self improving. Whereas with AI,
we seem to be switching to this paradigm where you have agents writing their own code, self improving,
modifying, perhaps even what they are trying to accomplish.
What would you say to the people that say just formally verify all the code or all the software
that's important? So this is exactly what many people are proposing. The unverifiability paper talks
about limits and mathematical proofs and software verification. All your proofs are relative to a
specific verifier, either a specific mathematician, mathematical community or a piece of software.
This mathematician may have brain cancer and the tumor is pressing just in the right buttons. You
can never be sure. So you have the whole mathematical community. We know that there are
proofs which stood the test of time for a hundred years and then we discovered bugs in them. That's not
a guarantee. With software, are you proving that, okay, this is true because this software says so. Who
verified that software? You have infinite regressive verifiers. So you can be more and more convincing,
but you're never a hundred percent sure. Interesting. So you almost feel like there
is no way to formally verify that really verifies. We don't know how to do it for self improving
software at all. All the critical systems were verified nuclear power controllers, space flight,
a small function with static code. We have no idea how to do it for code, self improvement,
three rights based on new data deployed in new domains. There is just no results on that.
Yeah. You have to somehow have a decision theory that applies even when you are manipulating yourself,
which is... There are impossibility results saying you cannot have this type of recursive self
improvement verification. There are limits and that's very interesting, but I don't think anyone
claims they have a working prototype or even a paper or patent explaining how that can be done.
I'm planning to link to a lot of your papers in the description, but I know that you'll probably get
some viewers that way, but not a ton. But you also wrote a book recently, right? Did you want to tell
us a bit about that? So there is many books. You can see some of them behind me. The most recent one
is explicitly about limits to what can be done, limits to explaining advanced systems, comprehending them,
predicting their specific actions, and overall controlling them. And it kind of goes through all these
tools we would need for control and showing what are the upper limits for accomplishing that.
So do you recommend people read your most recent book or is there...
It's always the most recent. That's the one I love. And by the time I have the next one,
I won't care about it. But really, it's very timely. It literally talks about the issues we're
discussing right now. So far, people who read it seem to be very happy. They agree with the conclusions.
We agree with the arguments. But again, my challenge is always, please find mistakes.
Please show us how we can, in fact, control those systems. Nothing would make me happier.
Well, you definitely have a background worthy of a YouTuber and much more scientific besides. So,
yes, I hope some viewers will be able to check out that textbook. And yeah, well, perhaps find problems,
but also just educate themselves and educate more people. Because I really think that all of these issues
have a marketing problem, right? It's hard to tell people about them. It's hard to convince people.
It's not in the public consciousness. And therefore, the number of brains we have working on it is
much smaller than it could be. The good news is most people who are not experts in it intuitively
understand perfectly well, you will not be able to control godlike machines forever. When I do surveys
in my lectures, regular people outside of computer science, there is not a single hand that goes up and
says, oh, yeah, we'll be able to control those things. It's easy. I'm sure computer scientists
know what they're doing. So it's really professionals. When I survey professionals, they say,
oh, there is about a 30% chance we can do it. So it's not obvious where this confidence is coming from.
One of those cases where you might know too much, you might be too close to the problem, perhaps.
Knowing about cognitive biases makes it worse for you.
Well, is there anything else you wanted to say to our audience today?
No, thank you so much. Have a wonderful life.
Very cheerful. And thank you very much for coming on the podcast. Roman has a very busy schedule,
but he made time for me on very short notice. So he also has some articles in Time magazine. There's
all kinds of stuff in the links below. So please check them out and tell some friends about this
video. He found it interesting. If you liked this video, check out this previous interview I had
just recently with Connor Leahy. It's on similar topics, and you'll probably enjoy it as well.
That's all I have for today. Thank you very much for watching. Bye.
