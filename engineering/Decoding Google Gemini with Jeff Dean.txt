Welcome back to Google DeepMind, the podcast, with me, your host, Professor Hannah Fry.
In this episode, we get to speak to one of the most legendary figures in the world of computer
science, Jeff Dean. He was there in the late 1990s writing the code that would turn Google
from a small startup to the multinational company it is today. Jeff spearheaded TensorFlow
one of the programming tools responsible for the democratization of machine learning.
He's the one who pushed the boundaries of artificial intelligence in the direction of
large-scale models. And if that wasn't enough, he also co-founded Google's AI research project,
Google Brain, and was one of the earliest pioneers of a new neural network architecture
called Transformers. Not sure that one's going to catch on. I think this is why people joke that
Jeff Dean's resume just lists the things that he hasn't done. It's shorter that way.
More recently, Jeff, as Google's chief scientist, has occupied one of the most important seats
around the table as the two great AI arms of Alphabet have merged, DeepMind and Google Brain.
His latest baby, which he co-parents, is known as Gemini, which takes large language models well
beyond language alone. Gemini is a multimodal model which can understand text, code, audio,
images, and video. It is AI through and through, and almost certainly the direction that Google
Search itself is heading in. Jeff, thank you so much for joining me.
Jeff Dean's resume.
Thank you for having me. It's a delight to be here.
So, okay, 25 years, quarter of a century at Google. I want to know a little bit of what
it was like back in the early days, right? Like in the 90s when you first joined, when Google
wasn't the sort of slick organization that it is now. Was it all a lot of, you know, laptops
with stickers on it and sort of coding in flip-flops?
Sadly, it was pre-laptops.
Pre-laptops?
Well, mostly. Yeah, we all had those giant CRT-based monitors. It was pre-LCD monitors.
So, uh, they took up a lot of desk space.
Not very possible.
My desk was like a door on two sawhorses. Uh, that's, that was the, and you could adjust
it yourself by like getting out of the desk and standing up with your back to like get
it to the next higher setting.
Really?
Yeah.
Amazing.
And when I started, we were in the small office area, actually not, you know, maybe three
times as big as this room. Uh, the whole of Google.
The whole of Google on University Avenue in Palo Alto above what's now a T-Mobile like
cell phone store. Um, and, uh, you know, the really fun and exciting thing in those days
was, uh, you know, we were a small company, but we could see that people were using our
service more and more because, you know, we were providing good quality, high quality
search.
And you could see your traffic growing, you know, day over day, week over week.
So we'd always be trying to not melt on Tuesday at noon, which was kind of the peak traffic
hour of the week.
And that would require, you know, we deploy more computers quickly, that we optimize our
code to make it run faster, that we come up with new and interesting innovations for
next month's index that make it, you know, able to serve more users with the same hardware.
I can imagine being very exciting. Was there a moment when you guys realized, was there
a moment when you were like, ah, okay, this is really going to be big?
I mean, I think you could see that from the very earliest days when I was joining, I joined
the company because our traffic was growing really fast and we felt like, you know, by focusing
on returning really high quality search results and doing that quickly, um, and giving users
what they want, we actually, you know, want to get people off our site as quickly as possible
to the information they need, uh, that, that was a, you know, a winning proposition and
users seem to like our service.
And so it seemed reasonably promising, I would say from even the early days.
There's quite a big gap between reasonably promising though, and sort of what it ended
up being.
Has it been a surprise to you, like to, to all of you?
I mean, I think there's been a bunch of things that, that we've branched out into that were,
you know, obviously hard to anticipate autonomous vehicles.
Like it's hard to sort of fathom that when you're working on a search engine.
Um, but you know, I think the sort of gradual broadening out of our portfolio products to
other kinds of information makes a lot of sense.
So going from public web pages to, you know, helping users organize their own email with Gmail,
things like that.
Those are sort of natural evolutions of things that solve real problems people have and they
get you in the state of, okay, well now we don't just have one product.
We have like a, a handful of products that people use fairly regularly.
And I sort of wonder looking back through all that time, do you think that Google was always
a search company or do you think it was an AI company sort of pretending to be a search company?
Yeah.
I mean, I think a lot of the, uh, problems we wanted to tackle as a company really were
sort of ones that would require AI to really truly solve.
And so along the way and, you know, a long period of time, 25 years, you know, we've been
progressively tackling some of those hard AI problems and making progress on them and then using
the new techniques that are now starting to work in the midst of search and the midst of all of our
other products.
Do you think that Google will always be a search company or do you think,
is it even a search company now?
Is it changing?
Well, one of the things I really like about Google is our mission is still incredibly relevant,
even, you know, 25 years later, you know, organize the world's information and make
it universally accessible and useful.
And I feel like Gemini is really helping us push in the direction of understanding,
you know, lots of different kinds of information. So text, textual data, you know,
software code, which is kind of texty in nature, but, but very structured in certain ways,
but also all the other kinds of modalities of input that humans are sort of fluent in,
you know, we're naturally, you know, we read stuff, but we also see stuff with our eyes and hear stuff
with our ears. And you want models to be able to sort of take in information in its many forms
and also produce information in, you know, text form or maybe generate audio so that you can have
a conversation with the model or produce imagery if that's appropriate or, you know, annotate text
with graphs or things like that. So we're really trying to make a single model that can take in all
those modalities and produce all those modalities and use that capability when it makes sense.
Can you remember when you first came across neural networks?
Oh, yeah, actually. So neural networks have had an interesting history, like, you know,
AI is a quite old discipline. And the early phases of AI were about how do we define rules about how
things work? So that was like the 50s, 60s, 70s to some extent. And then neural networks kind of came
along in the, you know, 70s and had a wave of excitement in the late 80s and early 90s.
And I was actually an undergrad in 1990 at University of Minnesota. And I was taking a class in parallel
processing, which is this idea of how do you take problems and break them down into pieces that can be
done on different computers? And then kind of in conjunction, all those computers work together
to solve a single problem. I guess this is like the point where the computing power wasn't quite
what we have today. It's like, how do you make computers work as a team? Right. Yeah. And at that
time, neural networks was a sort of particular approach to machine learning and AI that involved kind
of very kind of crude approximations of how we think real human or other brains work with neurons.
So that's why they're called neural networks is they're made up of artificial neurons. And
artificial neurons have connections to other neurons below them. And then they look at kind of the signals
that come up from those artificial neurons. And they decide, you know, how interested are they in
that particular pattern of signals? And then they decide, should they be excited enough to send a
signal further up the neural network? And so that's one artificial neuron. And a neural network
is made up of lots of layers of lots of these neurons. And so the higher layer neurons build on the
representations of the lower neurons. And so if you're, for example, building a neural network for image
problems, the lowest neurons and the lowest layer might learn features like, oh, it's a splotch of
red or green, or there's an edge at this orientation. And then the next level up might learn, oh, it's an edge
with yellow on one side. And then higher up, it might be, oh, it looks like a nose or ears or a face.
And so by building these layered learned abstractions, these systems can actually
develop very powerful sort of pattern recognition capabilities. And that's what people were excited
about, about neural networks in kind of 1985, 1990. But we're talking teeny,
teeny, tiny. Teeny networks, yeah. So they could not recognize like faces and cars and things. They could
recognize like little patterns in artificially generated patterns.
Yeah, like you have a grid and you can recognize maybe a cross.
Yeah, or a handwritten digit. Is it a seven or an eight?
That's fancy.
Yeah, that was fancy. But that was kind of what they could do at that time.
But people were excited because they could solve those kinds of problems that other systems,
based on sort of purely logically specified rules of what a seven means,
weren't actually able to do very well in a way that generalized to like all kinds of messy handwritten
sevens. So I was kind of intrigued by that after my two lectures on neural nets. And I decided I would
you know, do a senior thesis, honors thesis on parallel training of neural networks. Because I felt like,
we just need more compute. What if we use the 32 processor machine in the department and made a,
you know, a bigger system that we could train bigger neural nets.
So that was what I spent, you know, a couple of months or three months on.
Did it work?
Yeah, yeah. So, so anyway, I was very excited. I was like,
oh, 32 processors is going to cause neural nets to really, really hum and sing.
Take them.
Turns out I was wrong. Naive undergrad me.
We needed about a million times as much processing power to get them to really start to work well on
real problems that you might sort of care about.
Yeah.
But then thanks to kind of 20 years of progress of Moore's law and, you know,
much faster CPUs and computational devices and stuff, we actually then started to have
practical systems that had a million times as much compute as even our fancy 32 processor machine.
And so I started to kind of get interested in neural nets again when Andrew Ng, who's a Stanford faculty member,
was consulting at Google one day a week and I bumped into him in one of our many micro kitchens.
I'm like, oh, what are you doing at Google? He's like, oh, I haven't really figured it out yet
because I just started consulting here. But some of my students at Stanford are getting good results on neural networks.
I'm like, oh, really? Why don't we train really, really big neural networks?
So that was the genesis of our work on neural networks at Google.
And then we formed a small team called the Google Brain Team to start looking at,
you know, how could we train very large neural networks using Google's computational resources?
And so we sort of built this software infrastructure that enabled us to take a neural network description
and then break it down into pieces that would be done on different computers, different members of this parallel team,
and then communicate amongst themselves in ways that they needed to do in order to sort of
tackle the overall problem of how do you train a single neural network on 2000 computers?
And so that was kind of the earliest software we built for really scaling up neural network training.
And it enabled us to train models that were 50 to 100 times larger than sort of existing neural networks.
2011, right?
Yeah, this is like early 2012.
So this is like before the big breakthroughs in image recognition. This is like way back.
And in many ways, you were doing then the same thing that you were doing previously of just like
kind of stitching computers together.
It's like my undergrad thesis.
Exactly, but again.
I'm like, hey, we could do that again, but at big scale.
And yet this time?
This time it actually worked because the computers were faster and we used a lot more of them.
Did it feel like a bit of a gamble though back in 2011?
Oh, yeah. The system that we built for training these neural networks and trying different ways
of breaking them apart, I actually named it Dist Belief because it's partly because people didn't
think it was really going to work and also because it was a distributed system that could build these.
One of the things we wanted to train was belief networks in addition to neural networks.
Oh, I love it. I love it.
So, Dist Belief.
Dist Belief. Amazing.
So while this was going on in stateside, this side of the Atlantic was the beginnings of DeepMind.
Yes.
And I know that you were the person tasked with coming over and checking them out,
right? Can you tell me about that story?
Yeah. So, actually, Geoffrey Hinton, who's a very well-known machine learning researcher,
spent a summer at Google in 2011, I think.
And we couldn't figure out how to classify him, so he got classified as an intern,
which is a little funny.
Most senior intern in the entire history of the world.
And so he and I were working together and then somehow we found out about DeepMind.
I think Geoffrey had known a little bit about sort of the formation of the company and some other
people also said, oh, yeah, there's this company over in the UK doing interesting.
And it was teeny tiny at this time.
Yeah, like probably 40 or 50 people or something. And so we decided as a company that we would go
check them out as a potential acquisition. And so I was in California. Geoffrey was back in Toronto
where he's a faculty member at that time. And Geoff has a bad back, so he can't actually fly
commercially because he can't sit down. He can only lay down or stand up. And airlines don't like it
when you stand up during takeoff. So we had to figure out a solution, which was to get a medical
bed in a private plane. And so we, a bunch of us, took off at California, flew to Toronto,
scooped Geoffrey up from the tarmac, put him in the medical bed, and then we all flew to the UK
and landed in some, not one of the main airports. It was on the edge of town. And we all got in a big
van and we trooped off to visit DeepMind, which was near Russell Square, I think. And we were really
tired from flying the previous night, but then we got like a 13 straight 20 minute lectures in a row of
all the different things they were doing. What, from the team at DeepMind?
Yeah, from the team.
Oh, wow.
So we saw some work, some…
While jet lagged.
While jet lagged.
It's like something from a sitcom.
Yeah, yeah, exactly. And then, so then we got some presentations on, you know, some of the
Atari work they were doing, which sort of was published later on how do you use reinforcement
learning to learn to play old Atari 2600 games. So games like Breakout or Pong or,
you know, various other ones, which was quite interesting.
And then…
Because you guys hadn't been doing the reinforcement learning at that time.
Right. We've mostly been focusing on how do you scale up large-scale supervised and unsupervised
learning.
And…
And the reinforcement learning is much more motivated by rewards.
Yeah. So I think all of these techniques are really useful and they're often useful in combination.
So reinforcement learning, you should think of as you have some agent operating in an environment,
and at every step, there's a bunch of different moves you could make or actions you could take.
So in the game of Go, for example, you know, you could play a stone in any of, you know,
a whole bunch of different positions. In Atari, you could move your joystick up, down, or left or right,
or you could push the left or right button. And often in these situations, you don't get an immediate reward.
So in Go, you play a stone and you don't know if that's a good idea or not until kind of the whole
process of the rest of the game plays out. And one of the interesting things about reinforcement
learning is it's able to take kind of long sequences of actions and then attribute rewards or
negative rewards to the sequence of actions that you took in proportion to, you know, how unexpected it
was based on, you know, when you made that move. Did you think it was a good idea? And then you won,
so maybe you should increase your idea that that was a good idea a little bit. Or maybe you lost and
you should decrease your sense that that was a good idea a little bit. And so that's kind of the main
idea behind reinforcement learning. And it's a quite effective technique, especially in environments
where it's very unclear immediately whether that was a good idea.
Yeah, absolutely.
In contrast, supervised learning is where you have an input and you have kind of a ground truth output.
So the classic example is you have a bunch of images and each image has been labeled with one
of a cat, a bunch of categories. So like there's an image car, there's another image ostrich, another
image, you know, pomegranate. If you have a rich set of categories.
Yeah, exactly. Tell me, when you were here at DeepMind and you decided that you would do the
acquisition, was Demis nervous?
I don't know if he was nervous. I mean, I think that I said, well, I've seen all these nice
presentations, but can I look at a little bit of the code? And so, because I wanted to make sure there
was like real code behind it and see like what the, you know, coding standards were like, that people
actually write comments and that kind of thing. So Demis kind of like was a little unsure about
this. I said, oh, it doesn't have to be like super secret code. Just pick some little bit of code and
show it to me. And so I went into an office with one of the engineers and we kind of sat down for 10
minutes. And like, I said, okay, what does this code do? And like, oh, okay, that thing, what does
that do? And where can you show me the implementation of that? And I came out satisfied.
It was neat and tidy.
It was reasonably neat and tidy. I mean, for a small company trying to move quickly,
it was kind of researchy code. But it was, you know, clearly interesting and well documented.
I've heard that when you do your code, you put a little thing, which is LGTM.
Oh, yeah. It looks good to me. Yeah. I use that in real life too. No, not just for code reviews.
Okay. So in these presentations then, can you remember what your impression was?
Yeah. I mean, it seemed like they were doing really interesting work,
particularly in the reinforcement learning side. We were focused on scaling. So we were training
models that were much, much bigger than the ones DeepMind was playing with at the time.
But they were learning to use reinforcement learning to sort of solve kind of gameplay,
which is a nice clean environment for reinforcement learning. But it seemed like the combination of
reinforcement learning plus a lot of the scaling work that we had been working on would be a really
good one. Because it's like, I guess, you're sort of approaching a problem from two different
directions, like really tiny with reinforcement learning, really small like toy models and building
up. And then you're kind of at this very, very big scale with this sort of rich understanding,
understanding in inverted commas. But then it's sort of putting the two together where things become
really powerful. Yeah. Yeah, indeed. And that was kind of a lot of the motivation behind the combination
we did last year of the kind of legacy DeepMind and legacy Brain and other parts of Google research.
We decided we would just combine the units together and form Google DeepMind.
Yeah. And Gemini as the... And Gemini, which actually predated the idea of combining,
but really was like, hey, we should really all work together on these problems because we're all
kind of sniffing around the same kind of general direction of trying to train really high quality,
large scale multimodal models. And it doesn't make sense to fragment our ideas and not work together
and fragment our compute resources and so on. We should really just put all this together,
build a combined team to go after this problem. And that's what we did.
So why Gemini?
I actually named it.
Did you? Yeah.
I mean, you're after the disbelief.
I do like naming things. It's fun.
Yeah, disbelief.
So Gemini relates to twins. And I felt it was a good name for the twins
of legacy DeepMind and legacy Brain kind of coming together to really start working together on
sort of an ambitious multimodal project.
I guess also Gemini and just thinking of the space missions.
Yeah.
It's like a precursor to Apollo.
Yeah. A good thing about a name that has multiple meanings is that was another reason to pick the
name. It's sort of the precursor to ambitious space program progress.
So I want to come on to the multimodal stuff.
Okay.
Just before I do, I guess one of the big reasons why this big change has happened in the sort of
public consciousness of chatbots and large language models is in part because of some work that came
out of Google Brain with Transformers. If you'll sort of forgive the pun.
Yeah.
Can you tell us a little bit about that Transformer work and how transformative it's been?
Sure.
Yeah.
So it turns out a lot of the problems you want to deal with in language and in a bunch of other domains
are problems of sequences.
So if you think about, you know, autocomplete in Gmail, you're typing a sentence and can the system
help you, you know, by finishing your sentence or your thought for you, a lot of that relies on
seeing part of a sequence and then predicting the rest of it. And essentially that's what these large
language models are trained to do. They're trained to take in data, you know, one word or one piece of
a word at a time and then predict what is the next thing that will follow.
Like fancy autocomplete.
Like fancy autocomplete.
Yeah. It turns out to be useful. You can model a lot of different problems this way as well. So like
translation, you can model as taking in the English version of a sentence and then training the model
to then output the French version of the sentence when you have enough English-French sentence pairs to
sort of train as like a sequence. You can also use this in healthcare settings. Like if you're trying
to predict, you know, a patient in front of you is reporting these symptoms and they have these lab
test results and in the past they've had these things, you know, you can model that whole thing
as a sequence and then you can predict, you know, what are the likely diagnoses that would make sense
if you have other de-identified data that you can train on that has also kind of been organized in
these sequences. And the way you can do that is you just hide the rest of the sequence and you force
the model to try to predict what happens next. It's quite an interesting thing that it's so applicable to,
you know, language translation, healthcare settings, DNA sequences, all kinds of things.
But that it's about the bit that you're paying attention to at any point in time?
Yeah, so the models that were successful prior to the transformer architecture were
what are called recurrent models where they have some internal state and every time they see a word
they do some processing to update their internal state and then they go on to the next word and
then they do that again. So now they have, they move their state forward a bit and update the state
with respect to the next word that they just saw. And so you can kind of imagine this as like a 12 word
sentence. You're doing that updating of the state 12 times, but every step is dependent on the previous
one. And so that means it's actually quite hard to get it to run fast because you have this what's
called a sequential dependency where step seven depends on step six, step six depends on step five and so on.
So one of the things a collection of researchers within Google research
did is they came up with a pretty interesting idea which is
instead of just having a single state that we update at every word,
and then when we're trying to predict a new word, let's process all the words all at once
and let's remember the state that we get when we're processing every word.
And then when we're trying to predict a new word, let's pay attention to all of the previous states
and figure out how to learn to pay attention to the important parts. So that's the learned attention
mechanism in transformer in order to predict the next word. And for some words, you know, you might need
to pay attention to the previous word a lot. For some contexts, it's very important to pay attention
kind of a little bit to a lot of the words in the context. But the important thing about that is it
really can be done in parallel. You can take in a thousand words, up, compute the state for each one of them
in parallel, and then that makes it sort of 10 to 100 times more efficient in terms of scaling and,
you know, performance than the previous recurrent models. And so that was why that was such a big
advance. But then I guess there's other things that seem to emerge from this. I mean, sort of a
conceptual understanding or maybe sort of abstraction that's possible just through sequencing and language
alone. I mean, was that a surprise? Yeah, I mean, I think some of the earliest work we did on language
modeling in the Google Brain team was really about modeling words not as their surface form of like
H-E-L-L-O or C-O-W, but really about a high dimensional vector that represents kind of the
way in which that word is used. You know, we're used to thinking in two and three dimensions as humans,
but when you have a hundred dimensions or a thousand dimensions, there's a lot of room in a thousand
dimensional space. But when you have things that are nearby and you've trained the model in such a way
that, you know, cow and sheep and goat and pig are all near each other and they're very far apart
from espresso machines. Although milk could be in between the two. Milk would probably be near the cow,
but kind of in between the two. Yeah, it would probably be kind of on that hundred dimensional
line in hundred dimensional space. So this is kind of why these models have surprisingly powerful
capabilities, I think, is because they're representing things with so many high dimensions that they can
actually really latch on to many different facets of a word or a sentence or a paragraph simultaneously
because there's so much room in their representation. It's sort of extracted the grounding that we
ourselves have given language, I guess. Yeah, I mean we, when we hear a word, we don't just think of the
surface form of the word. We think cow, oh, that triggers a bunch of other things like milk or espresso
machine or, you know, milking and calf and bull. And one of the things we found with those early word
representations was that directions had meaning. So if you think about, you know, present tenses of verbs
like walk, you would go in the same direction in this hundred dimensional space
to get from walk to walked as you would go get from run to ran as you would go from, you know, read to read.
Wow. So it actually understands, I keep using that word and I don't mean it, but there is like some
representation of tenses within the structure of these. Yeah. And it's just emerged from the training
process. It didn't, it's not something we told it to do. It's just the training algorithm we used
and the fact that language has lots of ways in which, you know, particular, you know, forms are used,
cause that to merge. And you could also, for example, change from male or female versions to words
and vice versa. So cow to bull is the same as direction as queen to king or man to woman, woman to man
and so on. Amazing. But this is still, this is just with language that we're talking about.
Yeah. So, okay. Tell me, tell me, tell me how does the multimodal aspect of this change? What, what,
what does it, how does it make it different? Yeah. I, because it, it's, you're still representing
the input data in these high dimensional spaces. And it's really a matter of how do you get from the
the pixels of an image, say, into something where ideally you'd like, uh, the multimodal model to
have the same kind of thing that we have. When we see a cow, that triggers kind of similar activations
in our brain to reading the word cow to, you know, uh, hearing a cow move, right?
Um, and you kind of want to train models so that they have that joint meaning and representation,
regardless of the way they arrived at that input data. Um, so if they see a video of a cow walking
through a field, you know, that should trigger a whole bunch of things that are related to that
in the model based on the activations that the model has built over, you know, typically these
are very deep layered models. And so the lowest layers typically have very simple representations.
And then the deeper, the higher layers in the model build on those representations and build sort of
more interesting and complex, uh, you know, combinations of features and, and representations of,
of be it words or images or. So when you're saying multimodal from the ground up,
right, which is kind of a big phrase that you hear about generally, it's, it's not that you've
got like the word section over here, pixel section over here, and you're translating between one and
the other. Right. But like in the model itself, those representations. Yeah. Very early in the models.
Does that make it harder at the beginning when you're setting it up? Does it make it more difficult to do?
Um, yeah, I mean, I think figuring out how to integrate different modalities into the model and,
you know, how should you train a multimodal model is more complex than a simpler pure language or pure
character based model. Um, but you get a lot of benefits from it, uh, in that you get sometimes
cross-modal transfer. We're now seeing visual stuff about cows, um, actually helps inform the language.
You know, maybe you'd seen a bunch of descriptions of, of cows in meadows or in something, but now it's,
um, suddenly has seen images of that and videos of that. And it's actually able to bring those
representations together in a way that makes, uh, kind of similar things trigger inside the,
the model, regardless of whether you saw the word cow or the kind of the, the image of cow.
Give me an example of the type of situation you, you see this being useful in, in the future.
Well, I think it, it's already useful, which is good. Um, I mean, as one example, you want to be able
to take in an image of kind of a, you know, handwritten whiteboard worked out math problem
and say, did the student get this problem, right? Right. And so now you need to really bring in the
multimodal capabilities in one example. You need to actually, you know, do handwriting recognition,
understand from that, okay, it's a physics problem that someone's written on the board
and it's got like maybe a picture of a skier going down a slope and one of the early Gemini tech
reports. We had this good example of a, you know, a student who'd worked out a problem on a, on a
whiteboard and, uh, you, you could actually ask Gemini, you know, did the student get this problem
right? If not, where did they go wrong? And can you explain how to solve the problem correctly? And
it was actually able to tell that, you know, the student had incorrectly applied, you know, the,
the formula for a skier going down a frictionless slope and instead they used the hypotenuse instead
of the height and it said, oh no, no, actually you should have used this and here's the problem worked
out. And it did all that and recognized all the handwriting and the fact that this was a physics
problem, this kind of physics knowledge that the model already had sort of was the right thing to
apply. I mean, that's a really neat way that you could use the existing model of Gemini in the
existing model of education, I guess. Totally. Yeah. But, but I suppose actually these are not
kind of isolated systems from one another. So in some ways, do you think that these multimodal models
will change the way that we have to do education full stop? I mean, I think the potential for using
AI tools to help, uh, education, uh, is really amazing. And we're sort of just at the beginning of
this journey as a society, I think, you know, we know, for example, that educational outcomes of
students who get one-on-one tutoring from another person are two standard deviations better than
students who are only, who have a traditional classroom setting of a teacher and, you know,
30 or so students. Um, so how could we get everyone to the point where they feel like they have the
benefit of an educational tutor that's one-on-one understands what they know, understands what they
don't know, can help them learn in the way that they learn best. Um, that is the potential of AI in
education. And I think really we're not that far away from something where you could point, uh, a Gemini
model or a future Gemini model at something, some piece of material and say, can you help me learn this?
Right. Take the chapter six in your biology textbook or something. And, you know, it's got a bunch of
images. It's got, you know, a bunch of text. Uh, maybe it's got a lecture video that you watched as
well. Um, and then you can actually say, I really don't understand this thing. Can you help me understand
it? It can ask you questions. It can, you know, you can ask it questions that you can answer the
questions. It can assess, are you right or wrong? And really guides you in your learning journey. And,
you know, because it's individualized and we should be able to get that to, you know,
many, many people around the world, uh, in not just English and, you know, languages spoken by,
you know, uh, you know, hundreds and hundreds of languages all around the world.
I mean, I, so I, I take what you said about the, the lots of different languages and trying to make
these as broadly available as possible, but is there a danger of creating a bit of a two-tier system here
where, where on the one hand, people who have access to these tools, um, as you described,
get far better outcomes, you know, accelerate their own learning and, and, and productivity.
And then anybody who is not fortunate enough to have access to the tools, uh, really struggles.
I mean, is that, is that something that concerns you?
Yeah. I mean, I think there is definitely a risk of creating two-tier systems.
I, I think what we should strive to do is make these technologies as broadly accessible,
universally accessible if we can, uh, for everyone and really try to lean into the strengths of what
that will do for society and to make it, you know, affordable or free, uh, for people to take advantage
of the capabilities for education, for, you know, healthcare, I think is another area that is,
you know, hugely, you know, huge potential for, for AI to really make a big difference in, in healthcare
accessibility. Go back to Gemini if, if we can. Sure. Okay. So I guess if you started off with
Google search, factuality must have been absolutely at the cornerstone of everything that you cared
about. Yeah. But Gemini, I mean, you work with it all the time. I imagine you've seen it say some
quite outlandish things. Yeah. How are you sort of squaring that circle in your head of like
releasing perhaps some of the need for absolute factuality at all times?
Yeah. It's actually a tricky balance as a company because we are sort of from our origins,
a search based company. And as you say, providing accurate factual information is kind of the pinnacle
of a search engine experience. Um, and I think we actually had built interesting large language
models internally that, you know, people enjoyed conversing with, uh, you know, actually some of
them were, uh, available internally during the pandemic. So people were all at home and you could
actually see internal usage spike during lunchtime because people were like having conversations with
their, their virtual chat bot. Cause you know, who else are you going to talk to when you're home alone or
whatever. Um, but the, you know, these models are trained to predict plausible next tokens essentially.
So a token, you can think of it as a word or a piece of a word. And so when you predict plausible next
tokens, that's a different thing than that is absolute truth, right? It's a probabilistically
plausible sentence and that's different than a fact. And I think one of the things we, we realized over time
was these models can actually be quite useful even if they're not a hundred percent factual. And so I
think realizing there's all these other use cases, or can you summarize this slide deck in five bullets?
And yes, you could argue about, uh, is that fifth bullet exactly right, but it's still pretty useful to get
4.5 bullets that are factually accurate about the slide deck. And, you know, we'd, we're striving to make
it five factually accurate bullets, but even without that, I think the utility of these models is
actually quite high. Was it an uncomfortable realization? Cause of course other labs did push
out their models earlier. Yeah. Yeah. Do you think that, that, that you guys had an abundance of
caution because of this factual issue? I mean, I think we had a number of different concerns,
factuality being one of them, you know, toxicity and bias in the way, uh, you know, the model is trained
and the outputs that it can produce, uh, is an area where we want to make, uh, you know, the model less
biased in a lot of ways. Um, and so there were a whole, uh, number of areas where we wanted to sort
of be, be relatively cautious before releasing things to the general public. Um, and I think we've,
we've gotten a lot of those issues kind of sorted out enough that we think the, the products we put
out in the space are, are useful, even though they're obviously room for improvement in things
like factuality, uh, and in bias, uh, or in other areas. Um, so I think that's taken a little bit of
an adjustment for people is, you know, strive for the best you can be, but also realize that by not
releasing something, you're sort of holding back something that could be useful for a lot of people,
even with its sort of foibles. But then with those foibles then, so, so in which direction do we go
from here? Do you think that, I mean, it sort of seems to me that there's been this, this real shift,
uh, in, in the way that computing happens as it were, you know, you get a calculator,
you put in the same sum twice, you get the same answer twice. Whereas, whereas we're now in an era
of probabilistic computing. And so I wonder whether the, the, is it that the public has to come to terms
with that and sort of accept that we're in an era where things are much more human-like and that
they can make mistakes, or is it something that you think is fixable? I think it's some of both,
right? I mean, I think there's a bunch of tech, uh, sort of technical approaches to some of these
problems that will make the factuality area issues better. One, one instance is if you think about the
data the model is trained on, like trillions of tokens of text and other data that are then mixed
together in this giant soup of, you know, billions and billions of parameters, um, I like to think of
that as like, you've seen a lot of stuff, but you don't recall it very well. Uh, whereas if you take
information that is in the, one of the things we've been pushing on in Gemini is having a long context
window. So when you have a long bit of space where you can put a lot of direct information that
you're trying to summarize or manipulate or compare, uh, in various ways or extract information from,
that information in the context window, the model actually has a much clearer view of, right? It's
got like the actual text and the representations of that text not tangled together with everything else
it seems. So this context window is sort of the bit that the, the, the model can see as
important at that moment. Yeah. It can sort of reason about that in more fidelity than other
things that it's seen in its training process. So it can take like five PDFs of scientific articles
and then you can ask questions about it. Like, can you, can you please tell me common themes across
these articles? And then it's actually able to, to do that because it has, you know, its own representation
of all the contents of those articles. And that's one of the reasons we've been pushing a lot on
on very long context windows for Gemini models as we think that's a really useful capability for
factuality, for, you know, video summarization, for all kinds of things. Um, is there a limit though
to the context window? Can you just push and push and push and push until it's sort of an infinite
component? Uh, that is an excellent question. Um, currently
the computational aspects of the attention process are quite expensive. So the longer you try to make
it, the more expensive it gets, um, expensive in terms of time, but also computer time and
eventually money time and money and compute and all kinds of things. Um, but we think it may be
possible to sort of come up with algorithmic improvements that, that enable you to go beyond
the kind of 2 million token, uh, context window, which is what we have now. I mean, a million tokens
is quite a lot. Uh, a million tokens is about 600 pages of text. Um, so, you know, that's like up most
books, you know, 20 articles, uh, it's an hour of video. How about on the other side? Because you said
it was a little bit of both that, that perhaps people have to adjust their expectations. So I think
these models are, are tools and people need to understand the capabilities of their tools, but also
some of the ways in which, you know, you probably don't want to use the tool. Um, so, I mean, I think
it's a bit of an educational process for people. Um, you know, don't just trust every fact that comes
out of a language model, uh, right off the bat, you know, you need to apply a, a bit of, of scrutiny to
that. Um, sort of like, you know, I think we've taught people these days that if you see something
online that doesn't necessarily make it true. I think a similar degree of skepticism for some kinds
of things from language models is probably also appropriate. Uh, that skepticism may decrease over
time as the models improve. Uh, but it's good to take it with a healthy dose dose of, you know, oh,
that might not actually be true. Um, aside from the context windows, are there ways that you can
yourself when you're sort of writing in prompts sort of minimize the risk of ending up with something
that's, that's a complete hallucination? So one technique that, uh, Google researchers kind of came up
with is what's called chain of thought prompting. So in the same way, um, if you just give the model a
sort of interesting math problem and you say, okay, what's the answer? Um, it's, you know, it may get
it correct, but it may not. And if instead you say, here's an interesting math problem, can you please
show your work step by step? So if you remember back to your fourth grade math teacher, he or she was
probably saying you should really show your work step by step and then get to the final answer and
then write the final answer down. And that's partly because that helps you get through that, you know,
multi-step thinking process of how do you actually go from, you know, what's being asked to, okay,
I need to calculate this, calculate this based on that and so on, and finally get to the answer. And
it turns out that not only makes the model's output more interpretable because it kind of tells you
what steps it's going through, um, but it also makes it more likely to get the correct answer.
What if it's not a math problem though? Yeah. I mean, even in non sort of crisply
defined right answer things, um, domains, this approach kind of works and it, there's a bit of
subtlety and, and I think people need to actually learn how to use these models and the way in which
you prompt them is actually a, you know, a big differentiator in how high quality the output is.
Like if you say, summarize this, um, that might lead to one outcome. If you say,
please summarize this and give me five bullet points that, uh, you know, highlight the major
important pieces of the article and, uh, identify two cons that the author wrote down. You know,
if you say that, that's a much clearer set of instructions to what the model should do than
just summarize this. So when we put these things together then, so sort of breaking down step-by-step
processes, but also understanding more context, uh, and the multimodal stuff too, are we moving towards
a situation where these kinds of multimodal models will understand us as individuals and our preferences?
Yeah. I mean, I think what you really want, I think is a sort of very personal version of Gemini
for you that understands, you know, what it is you're trying to do right now, but also understands
the context in which you're trying to do that. Uh, I'm vegetarian. So if I'm asking Gemini about
restaurant recommendations in London and it knew that I was a vegetarian, it would recommend different
things than if I was not. Um, and I think a general model that is serving the needs of every person
the same is not going to be as good as one that actually understands a lot about you and your
context. Uh, you know, there's some kinds of queries you might like to ask a model, um, that you can't
quite do today with Gemini, but you could imagine wanting to do, you know, can you take the pictures
I took on the hike last week and make a illustrated, uh, storybook for, you know, my kid's bedtime
tonight, you know, and it would know where those pictures from on your hike were and, you know,
how to make an illustrated storybook that would appeal to your, to your child. It would maybe know
how old your child was to make it age appropriate, you know, so I think you can't do that now
but that could be something useful. People would want, you'd want people to opt into that. I think the
more information you want the model to know, uh, and have in context, um, I think the more you want to
sort of have people understand what, what is happening. Um, one of the things I think we'll be able to do
is not train a version of the model on that data, but just have the right information available in
context in order to sort of call upon it when generating responses. And I think that would be
pretty nice. So as in, you've got like this sort of, this general structure that you can almost
imprint your own context onto, but then that's kind of private for you. That's right. Nice. Yeah,
that seems like it'd be pretty good. Yeah, that would be pretty good. Are we limited here to just
audio and visual and, you know, things that you can see on a screen, language, whatever, or do we
ever expect that these kind of assistants will come out of our computer server? Yeah, I mean, I think
there's actually a lot of different kinds of new modalities of data that aren't sort of strictly
human modalities that we want these models to understand. So, you know, lots of temperature readings
across the earth to help with weather prediction or genetic sequences or, you know, lidar data for
autonomous vehicles or robotics applications. And then in a, you know, one setting, you want these
models to perhaps be able to help with real world robotics applications. You know, be able to talk to
a robotic device, give it sort of instructions in plain language, you know, can you please go into
the kitchen and, you know, wipe the counter down and discard, you know, recycle the soda can I left on
the counter and then bring me a bag of pistachios or something, right? Like, robots have traditionally
not been able to understand language like that, but I think we're on the cusp of enabling that kind of
capability. And then being able to have robots do, you know, 50 or 100 useful tasks, even in messy
environments like this room, rather than kind of the traditional setting in which robots have
already been deployed in the world, which is sort of very controlled environments like, you know,
factory assembly line kind of things where they go from there to there. And it's a very predictable thing.
We've been talking here as assistants, you know, these things as being sort of augmenting human
abilities in that way. And I can see it in medical settings and education settings. But is there more
that the multimodal aspect of this offers us in terms of, I don't know, like how we understand the
world? Yeah, I mean, I think what these models can do now is often kind of do a few steps of reasoning
to get from what you asked it to do in order to sort of accomplish something. And I think as these models
improve in capability, you'll be able to sort of get models to work with you to do much more complex
tasks. And, you know, it's sort of a difference between can you order a bunch of chairs at the
chair rental place versus plan me a conference? Right? The latter is much higher level, much more complex,
you know, the right model would kind of ask you a bunch of follow-up questions because there's
ambiguity in there. You know, how many people are coming? You know, what's it about? Just like a
human. What country are you in? What country are you in? Where do you want to have it? When? And then
we kind of set off and actually be able to accomplish a lot of the kind of things that you might want
done in order to do that high level goal. But then if you have this sort of conceptual
links or these conceptual links, I'm going back to Cal here, right? And it understands pictures
and it understands, I don't know, I guess gravity having seen videos on the internet.
It probably watched like introductory lectures on physics.
Right? Oh, wow. Okay. So it understands it from that perspective.
Yeah. And also seeing a bunch of things falling.
Okay. So then could you go in one day and say, draw me the blueprint for a really efficient airplane?
Yeah. I mean, I think one of the things these models
need to be partnered with is some exploratory process. And that exploratory process can come in
the form of, you know, maybe it doesn't need to give you an answer in 200 milliseconds. Maybe you'd be
happy with your airplane tomorrow. Right? And so I think at that point, then you have a lot more freedom.
And how would you design systems to be able to efficiently do things like that, where they can go
off and try a few experiments in maybe in a simulator that they have access to, or maybe they create a
simulator for, you know, basic fluid dynamics or something. And they try to, you know, try a bunch of
designs. Maybe they have some ideas about what airplane shapes, you know, make sense, having seen a bunch of
existing airplanes. And so then they can kind of try to accomplish what it is you asked. Hopefully,
they first asked you, well, what characteristics do you want your airplane to have?
It was a paper airplane all along.
Yeah. Paper airplane. Yeah. It's important to know if it's paper, like that reduces the cost a lot.
So I think those kinds of things will come eventually. It's a little hard to tell exactly
when those capabilities, you know, that's a pretty complicated sort of integration of what
you want the reasoning in the model to do, the knowledge it needs, what you're asking it to do,
and how you're asking it. But, you know, we're already seeing pretty big advances in capabilities
of these models over, you know, five year, 10 year periods. And so over a five or 10 year period,
that might be possible. It might even be sooner than that for, you know, can you help me design
an airplane with these characteristics? But I guess these are like the early,
early precursors to what we might hope Apollo would be. Yeah, exactly. That's why it's Gemini.
That's why it's Gemini. Amazing. Jeff, thank you so much for joining me.
It's a pleasure to be here. Thank you for having me.
In a lot of ways, I think Jeff's whole story is one about scale. For Google search,
it was about how do you get more of the web, more users, faster queries. For neural networks,
it was about more computing power, more machines. And in the recent era of machine learning,
it's been about more and more and more data. But something emerges from all of that,
a genuine conceptual model of the world, one that is capable of abstraction and has already
a proven ability to enhance human productivity. And it's telling that Jeff isn't finished there.
There is more to come, more sensors, more modes. And when combined with the reinforcement learning
tools that were born in this building, maybe also more progress on the path to AGI.
If you've enjoyed this episode, please make sure that you subscribe to our podcast. And if you have
any feedback or you want to suggest a guest that you'd like to hear from, then why not leave us a
comment on YouTube. Until next time.
