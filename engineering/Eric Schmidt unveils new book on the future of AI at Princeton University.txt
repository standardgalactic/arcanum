I'm going to start off just by thanking all of you for being here this afternoon on behalf
of Princeton.
Wow, that was auspicious.
I am delighted to welcome Eric Schmidt, a proud alumnus from the great Princeton class
of 1976.
Back with you.
Eric's historic leadership in the technology industry has given him exceptional insight
into what some are calling the age of artificial intelligence and the opportunities and challenges
that AI presents for humanity.
We are especially fortunate that he has chosen to share these insights in several important
books over the past decade.
Today Eric will offer reflections from his most recent work, a collaborative effort with the
late Henry Kissinger and Craig Mundy, former chief research and strategy officer at Microsoft
titled, Genesis, Artificial Intelligence, Hope, and the Human Spirit.
The book was published just yesterday, but already reviewers are hailing it as a profound
exploration of the ways that AI will test our understanding of what it means to be human
and raise new questions about knowledge, power, and the possibilities for human progress.
Thank you, Eric, for being with us today to engage in this important discussion.
Thank you as well for all that you and Wendy are doing to foster innovation and creativity
at Princeton and to help this university meet the most critical challenges of our time.
It's now my great pleasure to turn things over to the university provost, Jen Rexford, a member
of the great Princeton class of 1991 and an extraordinary scholar and teacher in Princeton's computer science
department to offer some words of introduction and engage in conversation with Eric.
All right.
Jen.
Great.
Thank you.
I'm delighted to join Chris in welcoming back Eric Schmidt, a former member of our board
of trustees and a member of the class of 76.
He's an accomplished technologist, entrepreneur, philanthropist.
As CEO and chairman of Google from 2001 to 2011, he worked alongside co-founders Sergey Brin and
Larry Page to transform the company from a small Silicon Valley startup to the global tech
giant that it is today.
But to go back to the beginning, I think it's fair to say that Eric's technology career began
at Princeton, where he came here planning to major in architecture and ended up majoring
in electrical engineering.
I'll have some questions for him about that in a moment.
He went on to do his graduate work at Berkeley and then began his professional career at
this legendary Palo Alto Research Center that was part of Xerox and also had positions at
Sun Microsystems and Novell.
Today Eric remains deeply involved in technology and particularly AI as an investor, a philanthropist,
an author and a public servant.
Along with his spouse, Wendy, Eric has established a number of organizations, including the Schmidt Family
Foundation and Schmidt Futures that invest in talented people and bold ideas to address
many systemic global challenges.
He's also, as Chris mentioned, a loyal friend to Princeton, who's generously supported the
computer science department as well as interdisciplinary research initiatives across campus.
I've personally benefited a lot from Eric's wisdom and guidance over the years, and I deeply
value his partnership as Princeton navigates new challenges around the age of AI.
We're thrilled to have you back on campus, Eric, and look forward to discussing your book
with you.
Without further ado, I'd love to have you join me in giving a warm welcome to Eric Schmidt.
Eric Schmidt Schmidt, Thank you.
Can I take a moment to thank President Chris and Jen.
When I was here, there was no computer science.
Computer science is now the largest major.
Good job.
And Princeton is now rated better than Harvard and Yale in almost everything.
Good.
Fantastic job.
So when you came to Princeton in the 70s, what did you see that inspired you to study electrical
engineering?
Who or what drove you in that direction?
It was interesting that I was young, obviously, and I didn't really know what I was doing.
And I met, you know, they have this alumni recruiter people that you go visit in order to apply.
And I talked to this guy.
It just sounded sort of cool.
So I said, sure.
And then they let me in, which would never occur today given my lack of achievement at the time.
So when I showed up, I wanted to do architecture.
And I visited the architecture school.
And they told me that all they did was work.
And they clearly all they did was work.
And I thought, there's something wrong with this.
And then I realized that I was really a computer person, but there was no computer science.
And so Brian Kernahan, who's a faculty member here, was giving a talk in what was at the time called Engineering 217.
And it was like unbelievable about the origins of computing.
And I said, that's who I am.
So the most interesting thing and why I'm so grateful to Princeton.
I then met this guy, Jeff Ullman, who's now retired from Stanford.
But he was a brilliant faculty member.
And Jeff was very connected to Bell Labs.
And by the way, Jeff had a rule that he wouldn't call you by your first name until after you had passed your quals.
But he called me by my first name because I wasn't a graduate student.
You know, this is the rigor that we were under.
And we had a computer here called the 36091.
There were only three in the world.
I now own the control panel for the Princeton 36091, which I'll loan you if you want to see something from ancient past.
And the important thing was that it was clear what I wanted to work on.
And the university, strangely, decided me to do what I wanted to work on.
So, for example, I have an electrical engineering degree, but I didn't take any of the electrical engineering classes.
And the reason I mention that is that the university saw that making me happy was good for me.
And they built me as a computer scientist.
And by the time I graduated from undergraduate at Princeton, I had a graduate level of understanding of computer science.
Because there were no other classes I was interested in.
Which has also resulted in me knowing nothing besides computer science for the rest of my life.
Which is a mistake, which I've tried to address.
But the key point I want to make is that it was the judgment of the faculty.
You know, a very young man, in my case, that allowed me that opportunity.
And I'm very, very grateful of that.
Because Jeff was a consultant at Bell Labs, I came and I worked for Stu Feldman.
Right?
Who is here and who was a brilliant astrophysics student and graduate here who is in Bell Labs.
And they were building Unix.
And that's when the luck starts.
When you get at the origin of something that's going to explode, don't quit.
Write it.
Write it.
Right?
And we didn't know what we were doing.
But they certainly did.
I was just a young person.
Stu and I went back to see the building.
Right?
And the old rooms that we worked in, they looked like attics today.
But this is how origins start.
Right?
For each of you, that origin story is really important.
Don't give up that opportunity.
They don't come along very often and they occur when you're very young.
So in thinking about the book, how did you and your co-authors decide to write this book?
And what did you each bring to it?
Well, I think everybody here knows who, I guess I should assume that maybe not everybody,
I guess everybody in Princeton knows who Henry Kissinger is.
A lot of people in your generation don't.
But certainly one of the most historic figures in the 20th century.
And opened up China, you know, negotiating Richard Nixon, all that kind of stuff.
And he and I became friends 15 years ago when I met him at a conference.
And he said, why can't I get all this bad stuff off of Google about me?
Typical Henry kind of comment.
And I said, you can't.
He said, why not?
And I said, okay.
I invited him to Google where he then announced to the entire employee body that Google was a threat to modern civilization.
Which everyone at Google loved.
Okay.
To be accused of being a threat from Henry Kissinger was great.
And so that's how their friendship.
And at one point we started talking about technology.
And he had been studying the impact of technology for decades.
He had studied when he was at Harvard post-war.
He had studied Kant.
He was very interested in the relationship of humans and the space around them.
How we perceive things and how technology changes it.
There's a story that when he was in the government he actually tried to get the government to give him a computer.
A computer terminal.
It would have been a time sharing terminal at the time.
And they refused because they thought it might be too dangerous in his hands.
So he'd always been interested in it.
Now he had trouble understanding math and algorithms.
He couldn't type.
He could hardly see.
But his brain was so strong that we collectively and Craig and I did it.
Got him to understand what was going to happen.
The ideas around Genesis which are fundamentally not about technology.
And I've told you guys privately.
I really think this university should embrace these ideas and questions.
Because I don't know the answers.
And so it's your turn.
And my standard answer is the graduate students will figure this out.
Right.
Because they have to write PhDs.
Write a PhD on the 20 or 30 questions that are in this book.
And at least you get a PhD out of Princeton.
And at least you'll do something new.
Right.
Because all the normal PhDs have been written.
This is the arrival of a new intelligence that rivals human intelligence is a very big deal.
For ethics.
For society.
For child rearing.
For economics.
Right.
Simple example.
Alan is here.
You can speak for the economic profession.
If I told you that these technologies are going to double productivity every year for three years.
And therefore GDP will go up by 20% in five years.
Or per person or whatever.
I believe you would say we don't have models for that kind of growth.
Right.
For whatever reason.
And it sure looks like this is a discontinuous change.
This is not a continuous change.
So one of the things I really loved in the book was the discussion of polymaths.
You know, the importance they play in making major leaps forward.
And the idea of AI being a polymath in your pocket.
If you will.
I was wondering if you could say a few words about that and why it's so important.
Well, first place.
Because I don't want people to think that I'm crazy.
Which I may be.
Let's establish a belief system.
So there's what I call the Stanford Cisco School.
Which says this will occur in two cranks.
Which is about 18 months.
Which is three years.
I think they're wrong.
I think they're off by a factor of two.
In other words, I think it's six years.
And if that doesn't scare you, then let me tell you what happens in six years.
The current foundation models are now testing at 80 or 90% of the graduate level knowledge in every field.
Now there's nobody, not even at the brilliant people here, who can do all of that in every field at the same time.
Those foundation models will be connected in various clever ways.
Shirley is working on a physics one, which is incredibly interesting.
I mean, I can give you examples in chemistry that we funded and so forth and so on.
When those things get integrated together, you have a polymath that's driven by a human.
Okay?
Then the next thing that happens is the agentic revolution.
Which is essentially the development of agents that use these things to solve problems.
Which I can describe and you can certainly describe.
Think of them as an LLM that tries alternatives and then evaluates them and then learns from what works and what doesn't.
It's like a travel agent that's really stupid, but eventually they learn to be a good travel agent.
But it's applicable.
Then the next thing is you take that system and you train it for curiosity and power.
Learn something new and aggregate power.
And power doesn't mean physical power.
It means influence, dominance of an area and so forth.
And then run that thing.
What is that?
What is that?
That is a polymath that we as humans could never build.
And in my telling of the story that polymath will be available to each and every one of you and every citizen in the United States.
And in fact, every citizen of the world.
That is a very big deal.
I mean, my standard joke on this is that I spent my whole career managing software people.
And these things can write software almost as good as humans.
And they soon will be able to do it maybe better, but certainly as well.
So each and every one of you who's not a computer scientist and programmer, well, you can have your own programmer.
And they'll do exactly what you want, unlike my programmers.
Right?
Makes sense.
Indeed.
So when you think about Princeton's motto about being in the nation's service and the service of humanity,
where do you see the opportunities for AI to help humanity?
Well, it's always more fun to talk about the negatives, but let's talk about the positives first.
These digital systems are capable of doing very complicated tree pruning where they can help you with drug discovery.
You have a precision medicine initiative.
AI will be at the basis of those decision making, looking at all the information, trying to come up with stuff that works.
So you should expect an enormous sort of a Cambrian explosion of medical solutions to problems that have bedeviled us.
Climate change is a huge energy program here at Princeton, which I've been a little bit involved with.
You're not going to get to, in fact, some of the modeling that was done at Princeton is the best in the country.
And I know that because I endorsed it to the government.
You're not going to solve climate without new energy systems.
New energy systems require AI.
What do they need?
What that specifically means?
New materials, fission, fusion, new transmission, so forth and so on.
That's going to be AI developed.
Education.
The single best thing, and I don't need to tell the academics here, the single best thing you can do for the world is get it more educated.
Why do we not have an AI system that just educates everyone in their own language in the way that they learn?
Strikes me as a learnable proposition.
Think about global health care.
We have pretty health care in the United States, although we complain about it all the time.
Most countries have a nurse practitioner who is doing his or her best to service a large population in a little village.
Why don't they have an app that tells them how to do it better?
Just think about the compounding of these things.
Let's go into chemistry, physics, and so forth.
Most problems in science look to me to be multi-scale prediction problems.
So the systems that we're building are very good at multi-scale prediction problems.
And the reason is because large language models, roughly speaking, predict the next word.
There's not much difference between predicting the next word and predicting the next part of a protein sequence.
Algorithmically, they're very similar.
So, Demis, who just won the Nobel Prize, proved this by their protein modeling in AlphaFold, which is one of the great gifts, you know, to biology and well-deserved.
So, there are probably many other benefits, but what do most people care about?
Health care, education, safety, quality of their lives.
We can really materially improve it globally.
So, you said a moment ago it's, you know, easier to talk about the bad than the good.
So, maybe we could talk a little bit about the bad.
I mean, in terms of, you've hinted at this a little already, but if you could say a little bit about what worries you.
Well, a lot worries me.
There are many different layers of concerns about this.
The simplest way to say it is that when I'm in Silicon Valley, it feels like it's everything, everywhere, all at once.
There's so much money, there's so many people in your generation who are trying new ideas to solve the problem.
I can assure you the humans in the rest of the world, all the normal people, because you all are not normal.
Sorry to say, you're special in some way.
The normal people are not ready.
Their governments are not ready.
The government processes are not ready.
The doctrines are not ready.
They're not ready for the arrival of this.
And I can give lots of examples of not being ready.
But a simple example, I'll make some up right now.
You have a son or a daughter, and their best friend is not human.
Their best friend is a digital thing, whatever you want to call it.
What are the rules?
Is it okay that that child is the equivalent of Mark Zuckerberg, the surrogate parent who gets to decide what your kid learns and doesn't learn?
In fact, in the book, and Henry was very, very clear about this, he's very, very concerned that people like me make these decisions.
He wants a collection of people, which I think is actually represented by the disciplines of Princeton, to come to some consensus on how to roll these things out.
They're playing with the way people think is really powerful.
So if you think about state-sponsored misinformation, that's trivial compared to having your best friend be state-sponsored,
and the sort of daily interaction and shape someone's identity, their cultural values.
We talk in the book about something called duxa, which is a Greek term around the embedded structures of life.
And these are not laws, but they're cultural norms.
They're assumptions that we have of good behavior, and they differ by culture.
What happens to them?
So in the case where AI is built by one country, hopefully the US, what happens to all the other cultures?
Do we just roll through them?
They're already accusing us of hegemony by destroying their cultures by virtue of companies like Google, which is relatively innocuous,
because you have to ask Google something.
Imagine if Google instead is telling you something.
Think about the power that that would give to a company like Google.
So if you think about human traits like, you know, creativity, empathy, how do you see AI complementing or even adopting some of these traits?
I think a lot of it depends on how these things are trained.
The models, when they come out raw, don't necessarily have any belief systems.
And so technically what happens is you do pre-training and then you do a series of fine tuning,
which ultimately results in a technique called RLHF, which is human feedback to try to help it make better.
So a lot of decisions are made in that pipeline.
And so you could have a polite LLM.
You could have a nasty LLM and so forth.
And it's important to say that we don't understand the structure of how this is going to look out.
And I'll give you a simple example.
Someone like Sam Altman will talk about a trillion dollar data center.
And almost all of that money, by the way, goes into literally the electrical bills.
So if it's a trillion dollar data center, how many will there be?
Will there be a couple maybe in the U.S.? A couple in China?
Very few places can do a trillion dollars around that.
Now, why would you spend a trillion dollars to have something which is the world's polymath,
that can invent things that no one else can,
that can solve every economic, physical, and science problem?
Might be worth a trillion dollars.
I mean, the U.S. weapon programs cost a trillion dollars.
This is probably a more powerful weapon than the biggest weapons programs the U.S. sponsors.
It makes sense.
So if you think of it as that model, then it's going to be easy to regulate
because we're going to know exactly where they are because they're going to be huge.
There's an alternative view, right,
where there will be diffusion of these techniques into much smaller models that are disparate.
And then you have a huge, huge problem of essentially proliferation.
There's a long literature around nuclear proliferation and so forth.
A story here that only a few of you will remember.
In my undergraduate thesis, I wrote my thesis on language design,
and there was an undergraduate who wrote his thesis on how to design the trigger thing for a nuclear bomb.
And when the physicist, who was his supervisor, realized what he had invented,
he immediately caused it to be classified.
So the university had a problem because you're required to submit your undergraduate thesis to the public.
So this is a great conundrum to the leadership at the time.
This was under President Bowen.
And it was ultimately decided that if you wrote an undergraduate thesis that was so important,
it was classified, we would just give you the undergraduate degree,
which I thought was a good accommodation.
And he went on to be a great physicist.
So these are tricky, right?
Right.
And I worry a lot about the proliferation issue because we know evil exists in the world,
and we know that these systems are asymmetrically powerful.
So if you take the position that everyone will have access to a polymath,
and further, and this is an important assumption,
that access to that will be relatively distributed,
then we're in a very different structure of the world.
So you talked about trillion-dollar data centers.
It sort of begs the question of the role of academic institutions.
And you also talked about smaller models, and you could say, well, gee,
academic institutions should build small models or do theoretical work.
What do you think is the answer here?
I mean, obviously, academic institutions shouldn't compete with industry,
but we need some ability to contribute.
What do you recommend?
Well, so the industry answer is that the super-smart,
represented here, people will all become essentially consultants
or under contract in some way with these institutions,
under their intellectual property laws and without doing open source.
And that's sort of the way the industry views it.
Indeed, I was privileged to be the person who launched the Princeton,
Google, such collaboration, which by all accounts has been incredibly successful.
And I remember with the governor, who I think is still the governor.
Yes.
Maybe not for much longer, but in any case, he,
so he and I did the announcement a few years ago.
And what I said at the time is that had this existed,
I would spend all my time in this building,
because I would do whatever it took to get a computer
that didn't make me stay up all night.
Because way back when, when I was an undergraduate,
I could, I would sleep during the day and work at night,
because the computer was faster.
People of my generation understand that.
It makes no sense to anybody else.
But literally, I would sleep during the day,
and I don't think I went to class,
but somehow they gave me a degree anyway.
But it was, it worked out okay for everybody.
So one way to think about it is, is there a better model?
So first, the government needs to provide a lot more research
access to these things.
This is a new tool that will improve America
and American research and American universities.
A whole bunch of us have worked to try to get that.
There is a program called Nair, which I endorsed as part
of my role in National Security Commission for the Congress.
And there's a good chance that will come through.
Even if it doesn't come through,
we need to find a way to get $100 million, $200 million,
$300 million into Princeton for that, for that resource.
And I'm going to help.
We'll find other people who can do it and so forth.
We're very committed to solving this problem.
Part of it is I say to myself,
if I were a computer science faculty that, you know,
worked for you, for example, what would I be frustrated with?
That.
I'm not going to be frustrated with my students.
I'm not going to be frustrated with working here.
Salary is fine.
I'm busy doing my thing.
I don't have what I need, right?
And it's roughly analogous to, when you look at physicists,
if they work in accelerators, for example,
they literally will move for 20 years to be next to the accelerator.
Or they'll go to CERN for two or three years
because they have to be physically next to it.
Right.
We don't have that in computer science.
And I should say, as an aside,
that we also have new economics in our industry.
You all are familiar with the cost of these things.
I'll give you some numbers.
A typical GPU fully loaded is about $50,000.
100,000 GPUs is roughly then $5 billion, something like that.
200,000 GPUs is roughly $10 billion.
So Elon just built impressively a 200,000 GPU data center in Tennessee.
And that, you depreciate it over two to three years.
So that tells you that in over three years,
he has to come up with $3.5 billion of depreciation against his revenue.
So how much revenue does he need?
$10 billion a year.
How much revenue does Twitter have?
Not $10 billion a year, right?
There are plenty of examples that are worse.
Places like Microsoft and Google have so much capital
that they can actually afford this.
So part of the reason I'm mentioning this
is that the entire competitive landscape is being changed,
both for universities but also for companies
because of this extraordinary capital cost.
And in the last month, there's some evidence
that the scaling laws are beginning to come off the horrendously fast.
The scaling laws are, with more power, more data,
and so forth, you get emergent behavior.
And the rate of improvement seems to be slowing.
No one knows quite why.
It's not clear that's true.
We'll find out in the next month or two.
One theory is that we've essentially sucked in all
of the world's available public information.
And most of the models are converging on that.
What will happen with agents when agents arrive in the next year
or two is agents are much more complicated
and much more expensive.
So don't worry, the capital needs,
and particularly the electricity needs, will continue.
How do you fund it?
Right?
At some point, the investors in these AI startups
are going to want a return.
They're going to want to see some kind of revenue.
And it's always true.
And every generation has to learn that lesson, the hardware.
Now, maybe it'll happen.
Maybe it won't.
So you had talked a little bit about the role of government.
I'm curious your thoughts about the role of government
and regulation.
And also, you know, technology is moving so quickly.
How can government, which moves slowly, I mean,
really hope to regulate effectively?
I can give you a couple in the spirit of being brief,
because these conversations take days in regulation.
Europe is doing it the wrong way.
Europe wrote something called the AI-EU Act.
You can read it.
It's really quite depressing.
And it basically requires safety ahead of everything else.
And the most interesting requirement it has
is that the system be able to explain what it does.
And today, the technology is called the explainability problem.
They can't explain it.
Maybe we will.
People think they can work on that pretty soon.
Maybe that will happen here.
In China, they have a laissez-faire rule about regulation
as long as you follow all their other rules,
which are not laissez-faire.
And so the Chinese conception is that as long as the big companies
are building these models and as long as they're not talking
about Tiananmen Square and those sorts of things, it's fine.
It's obviously very serious.
I'm just trying to be punchy about it.
In the U.S., the U.S. and the U.K. and France and Korea,
I worked with a set of people to do trust and safety models,
including a Biden Executive Order Act, which I helped shape.
It sure looks like President Trump,
one of his first things is going to be to cancel that.
So I don't think we know.
But I think a fair statement is that in the U.S.,
whatever regulations around trust and safety are going to occur
are going to be very different and much later than I thought.
Now, if you're an entrepreneur trying to do crazy stuff,
this is good news.
If you're a person who worries about the dangers of these tools,
it's not good news.
So what do you think about sort of underexplored problems in AI
or if you were a grad student or an undergrad senior thesis student now,
like what would you work on?
Well, there's so many.
You have a graduate student here named Declan who we met with
who was explaining to me that the relationship between brain patterns
and neuroscience and these neural networks is actually much closer
and there might be a limit of what the computers can do compared to human brains.
In other words, I present this as though the humans will be limited
and the computers will be exceptional.
But what if, and he's sitting right here,
and you can speculate on your PhD research,
what if that limit is also in place for AI?
I want to know that answer.
I'm very, very interested in agents.
I think that one of the things about research
is you want to pick stuff that's not already going to get solved.
So for example, I'll pick a simple one.
Solar and battery improvements in climate change.
There are so many people working on that.
Those are important problems and so forth,
but you're not going to make a de novo new discovery in there
unless you're like super brilliant, much smarter than I am.
And maybe not even then.
But in the concept of agents, how does the agent work?
How do they talk to each other?
What is the language?
How do you orchestrate them?
If you have a bunch of agents that are in parallel
and they're all waiting for signals, which one do you call first?
There are many, many scheduling and organizational questions
which have a lot of implications in that.
I think that's probably the next, the agents,
the general answer to your question is agents.
And the specific problem is how do you orchestrate them and so forth.
There is research here at Princeton going on in transformer architectures
and the fundamental algorithmic limits.
If somebody could come along with the next big thing after transformers,
that would be like a big deal.
I don't exactly know how that would work.
And then finally, I gave you a presi of today we're language to language,
then we're anything to anything in terms of foundation models.
And then we're in agent loops where there's RL, reinforcement learning,
which is looking at scenarios.
There's an enormous jump in complexity because you, as computer scientists,
we think in terms of scalability and complexity.
The complexity in the agents of how do they go to the equivalent of tree search
to look at all their different choices to figure out what they're going to do.
That's a really important and really hard problem.
I would work there.
Cool.
So we're going to open it up to questions in a moment.
I just want one final question for me.
You know, what's your greatest hope for AI?
And what's the legacy you hope to lead in this space?
Well, first place, at my age, and it's disturbing that I will be returning
to my 50th anniversary in two years.
I haven't quite mentally adopted that fact, but I guess we have to deal with it,
all of us at some point.
I am a believer that exceptional talent changes the world.
So now we have a situation where we have tools that will allow exceptional talent
to scale at a level that is nonlinear.
And it goes something like this.
The talent goes like this, the scientists go like this, and so forth.
And at some point, my industry believes that there will actually be
AI-generated scientists.
So you have the human scientists with AI, and then you have the AI scientists,
and we believe collectively that the slope gets even higher.
That's why society is not ready.
And a whole new generation of people will come along and make an enormous amount of money.
Right?
A new generation of entrepreneurs, people that I don't know yet, but they'll be even richer,
even more successful, even more famous than Elon and Steve Jobs and so forth.
Because that acceleration creates a scale possibility that is inconceivably large.
So if you look at my career, I started off here working in the computer center,
which at the time was an 87 prospect.
This is before your time.
And today it's used for other things.
And since I lived there, I knew it really well.
As an aside, I decided the Kochs cost five cents.
And I decided that I was going to raise the price of the Koch machine to 10 cents.
And I decided to boycott it.
And then they raised it to 20 cents, and I realized that my boycott had no impact on it.
But I think the key thing to do here is figure out a way to get yourself into the middle of these curves that go like this.
I've been most successful, thanks to Princeton, that I was at the beginning of each of these revolutions.
That's where you want to be.
Because when they grow, it's like unbelievable.
So chemistry, science, math, all of them are going to be transformed to this.
So in that sense, you're the best generation to take on, right?
It's your turn now.
And it's their turn to ask questions as well.
So I'm going to open it up to questions now.
And we really want students in particular to ask questions.
So I'm going to ask that you wait for a microphone so that we can hear you
and introduce yourself before you ask your question.
Hi, thank you for the talk.
My name is Chris Catalano.
I'm a fourth year graduate student in molecular biology.
I really appreciated everything you had to say to AI, and I share many of the same dreams and optimisms
and hope for the future of technology and AI.
However, seeing the state of the country and the world the way they are right now,
and having observed what social media has done to a lot of people.
You talk about, for example, people having access to experts or, you know, a polymath in every field at their fingertips,
every person having this.
But it seems to me more likely that people will use it for videos that make them laugh or angry,
information that's only going to confirm their biases.
So maybe one day we'll get to that point where society is ready to use the technology that you speak of.
However, it seems to me the transition is going to be very destabilizing.
How does society navigate that transition, that power that suddenly people will have,
and what happens if we can't?
So this is what the book is about.
And one of the things that's worth saying is that none of us thought when we invented social media
that we would become a threat to democracy.
It wasn't on the list of attributes.
And these are the unintended effects of technologies that touch humans.
My simple answer is this thing is largely made by technical people
who didn't take any of those classes because they were too busy.
So they were literally surprised.
With Henry, I would say, I would make these, you know, statements about optimism and so forth.
And he said, the only problem with you, Eric, is that everything you're saying is inconsistent
with any historical record.
And I said, well, I don't, I didn't take history classes.
I didn't have time.
So I think that this time around the reason we wrote the book was to get everybody
to start thinking about these questions.
So the misinformation one is easy.
The social media companies have chosen not to police it because they haven't been required to
and they make more money because of it, right?
That's a regulatory problem.
I do think that there's a real danger here that these systems can optimize around a single individual
peculiarities, find like-minded people and then drive them insane.
We have these horrific stories, both men and women.
I'll use a stereotypical man.
He's got a tough life.
Nobody likes him.
Doesn't have a job.
Lives at home, whatever.
And he's online and he finds a group of people who he agrees with
and he gets radicalized and or commits suicide.
It's just not good for society.
It's not obvious to me how this will get regulated at all.
So I think this is something for us to think about.
Just to be completely depressing, this is going to happen very fast regardless of what you
and I agree to because there's too much money, too much scale, too many competitors.
Thank you so much for being with us, Mr. Schmidt.
I'm a senior studying economics.
You mentioned scaling laws and whether they're slowing down.
I know there's been a lot of discussion recently about if pre-training scaling is slowing and if the chain of thought, reasoning, if that's a new dimension to scale.
So my question is, do you think we'll be able to scale on both of those axes and what's going to happen in one to two months that should give us a better idea?
That's a better idea.
Thank you.
Yeah.
These are early.
An excellent question.
These are early fears in the industry because the whole model was around the emergence at each layer.
And these models take about 18 months to make.
And so when you start, the way it works is you spend six months screwing around, trying different algorithms.
This is where the university connections make a difference.
And then you start the model.
And they're typically trained in what are called mixture of extra expert models.
So they divisionalize the training.
And sometimes the mixture of experts doesn't work in the sense it doesn't come together.
There's something called a loss function and so forth, which they optimize for.
There are people who believe that this is simply nervousness because of the amounts of money at stake and so forth.
There are people who think there's a fundamental slowdown.
The reason I said what I said is that it doesn't, to some degree, it doesn't matter because the game will then be played one level higher.
So in computer science, there's always a game one level higher from where you are.
So you get the operating system.
Then you have a language fight.
Then you get the language fight.
Then you get the platform fight.
Then you get the apps fight.
Then you get the higher level apps.
And that's okay.
And there's plenty of money to be made at each of these levels.
I would bet that this alleged slowdown may not matter very much because the game will be played at the agent level, which uses whatever foundation models are available.
And most of the agents are not particularly complicated.
They have language input, language output, and they do something.
So that's my bet.
Up at the top.
Yes.
Yes, ma'am.
Thank you so much for the talk.
So actually, I'm a PhD in history.
So my question will be very simple.
Do you have any advice for students like me who are coming from humanities backgrounds in this world, which is kind of to be dominated by tech knowledge?
Thank you.
So this is, sorry, you said history?
Yep.
So the first thing I would do is I would take Google's Notebook LM, and I would take everything that you're doing in your history research, and I would throw it in.
Notebook LM essentially aggregates all sources of information, and then you can start doing queries.
And it has a really fun thing called podcasts where it makes up a synthetic podcast that's very entertaining.
But for you as a researcher, I would throw everything into my notebook, and then I would use that to get deeper insight.
That's the first thing I would do.
The second thing I would do is find somebody who's trying to build a history language model.
In other words, something which can understand the history and also predict things.
And I would align myself, in your case, as an expert, as a historian, with that work.
I think those two paths are likely to produce really interesting results for a discipline that hasn't changed very much.
At the top.
Hi, Eric.
I'm Rodrigo.
I'm a math major, sophomore here at Princeton.
I do know Rodrigo very well.
It's a pleasure to have you here at Princeton.
Thank you.
You are an incredible partner.
We worked together last summer, and you are the example that I use of why Princeton undergraduates are going to take over the entire world.
Oh, I'm truly honored right now.
Thank you so much.
I guess my question is, assuming that we have these very powerful systems that are even better than the most well-renowned mathematicians, physicists,
how do you think having these systems is going to change academia and the way that normally institutions like Princeton conduct academic research?
So, for example, would you see more people entering PhD programs or a completely transformation, completely new transformation on how PhD programs are, how they work and how they develop?
Would you see this academic change in institutions in the next few years?
So, again, the book talks about this at great length.
And I talk a lot about sort of what the computer can do.
But a better way to understand the question is how will humans interact with these powerful systems, right?
It's sort of roughly what you're asking.
It makes sense to me that in all the fields at Princeton, everyone will be using these systems all day, right?
So a mathematician will learn about math, and then they'll come up with conjectures, and then you'll have something which will just prove them.
And then they'll give you some proofs and say come up with the conjectures and vice versa.
In other words, the way the education will work is you'll have to learn with the computer.
And that's my guess.
So the teaching will ultimately, the presumption for you as an undergraduate, and my guess is when you enter Princeton, because Princeton people are so good,
is they'll presume you already know, even in high school, how to use these systems to get some basic learning and knowledge and drive them.
In math, and you're much, you're a real mathematician, I'm just making it up.
I'm assuming that the eventual path for math is that you'll have computers that generate conjectures and computers that generate proofs.
And occasionally the two will come up with something which is extraordinary.
But that will ultimately have mathematicians guiding it, at least for 10 years.
And frankly, when you get your PhD from here in math, you're probably what you'll end up doing.
Unless you come to work for me, which is a separate discussion.
Up here.
Hi, Eric. My name is Connor.
I just want to thank you for coming and speaking with us.
I'm a junior in the ECE department.
And, you know, I'm trying to kind of reconcile having these like all powerful AI models for like my future career in a technical industry.
So I just really wanted to ask you, like, what would be your advice, you know, to students studying technical subjects, getting ready to work with AI, but still be able to kind of work on these interesting problems themselves as well?
So let me give you a non-computer science answer than a computer science answer.
The non-computer science answer is learn Python and learn how to use the Python interfaces that are in these foundation models to do interesting things.
I started at another company, which is very small, and they're doing demos.
And what they did is they just used the APIs in Python from the big guys, and they just pay credits for it.
And they're able to do image classification, you know, various bisections, segmentation of information, generation of new images, that kind of stuff.
It's so easy now to build a prototype, especially if you're using Python, and especially if it's not going to be some huge scalable system.
So one way to think about it is that from a university perspective and a research perspective, a demo is good enough.
So I would imagine that every single, let's go back to our historian and her problem.
The most obvious thing for her to do is to take the historical test that she's studying and then have the images get generated on behalf of that and pay a small fee.
And then take historical images and have the systems essentially dissect, classify, and otherwise analyze the work, which she would have to do by hand.
So these tools are incredibly powerful there.
From a computer perspective, that's not going to be good enough.
And then you're going to have to design for scale, which means you're going to have to have real programming languages and you're going to have real architecture and you have real security and things like that.
And that's why we have a computer science department here.
Sir, really appreciate you coming.
I'm a second year PhD student studying military innovations and acquisitions for the Army.
Military innovation?
In which group?
I'm here on a downing to do my PhD through security studies.
Excellent.
Yes, sir.
So one of the many things wrong with the system that you and General Milley brought up in your article was the lack of VC doing things and stuff within the procurement cycle.
So I guess in a two part question, one, how do we get these exquisite technologies to scale that venture is producing?
And then are we creating the correct demand signals for venture to do the type of risky innovations that are required for national security?
So the problem you're describing is .
And I know this because I tried to solve it for a decade working for the Defense Department and I was the head of the Defense Innovation Board and worked for national security and all of that kind of stuff.
And I indeed spent quite a bit of time with President Obama and President Trump on this.
And I basically said to them, not even you as commander in chief can fix this problem.
And the core issue has to do with the system was designed post war in the 70s, especially after the failures of Vietnam.
And it has it's a zero trust environment where there's no particular incentive for moving quickly.
And there's a great incentive for doing very, very expensive problems that are very interesting, but nobody uses.
So they violate every tenant of technology business planning.
So I proposed, for example, of why don't you just work with a vendor to design the product, how we work in our industry.
And I was told it's a great idea, but it's illegal.
So I said, well, why don't we change the law?
Can't change the law.
OK, so having spent a decade working and I really actually like the people a lot.
They're trapped in a horrific system.
I have started to work on this on my own.
There's a whole bunch of startups in the in the tech sector that are doing the right thing.
And the simple rule would be attributable systems that are inexpensive.
They're based on consumer products.
They're done at scale and they are networked together.
A simple example, the F-35 of the underlying military networks that they use are not powerful enough to take the incredible information that an F-35 takes.
Right? They just they're just not powerful enough.
That's true of every base.
So it's just it's just a misarchitecture of how our national security should work.
So General Milley and I have written a series of papers on this and he and I sort of sort of agree.
Sort of strange to agree with, you know, the perfect general.
He looks like it.
He sounds like it and so forth.
And the great thing about it is he's a great Prestonian.
So we have a perfect alignment of interest.
And so Mark and I believe that the way this will be solved is by a complete rethinking of how war is done.
And you can see this in Ukraine.
So having spent lots of time there, the soldiers are sitting there at the desk and they're they're doing autonomous attacks using autonomy and remote, remote direction.
So my advice to you is to try to figure out you're not going to be able to solve the political problem, which is sort of where the money is.
But you can come up with an architecture for national security.
And the way to think about it is let's assume you had a great country and zero military.
What would you build?
The first thing you would build is not a battleship or an aircraft carrier or an airplane with people in it.
The first thing you would do is build autonomous defensive systems and offensive systems.
So if you think of it that way, you start from autonomy and a new networked architecture.
And by the way, this is really hard, right, because the network has to be secure.
The autonomy has to work.
And you have to deal with the fact that your competitor in this in the particular case in Ukraine and Russia, they're doing the same thing.
To me, that's the most interesting question.
The last couple of questions.
Hi, my name is Joey.
I'm a junior studying computer science.
Thanks so much for coming here and giving this talk.
I actually had a follow up about like considering research directions.
As an undergrad who's very interested in doing research and doing research currently in large language models and interested in doing grad school, I think I resonated a lot with your point about like finding yourself at the beginning of a new wave.
Do you feel like large language models are an area that is getting saturated?
Or and do you feel like perhaps you mentioned the Cambrian explosion, right?
And the Cambrian explosion is an example of where increasing the number of ways that organisms could sense the world resulted in an explosion in biodiversity.
So I wonder if like you feel like the next area of research would be somewhere in the space of spatial intelligence or vision language models, like combining other forms of perception with natural language.
Or do you feel like there's still so much more that we can develop in the field of language?
Remember that large language models have only been around for two or three years.
So it's crazy that we would all already be discussed, you know, that this is saturated.
I still think we're at the beginning.
I think that large language models change over time.
So your research area is preserved.
My strong advice to you is stay in the field and get your PhD, right?
And the reason is that at your age with everything that's going to happen, there's so much ahead of you that you don't see, right?
But I have the benefit of having watched it.
When I was here, the computer that I used, which I shared with the entire campus, was a hundred million times slower than the phone in your pocket or purse.
Pretty extraordinary, right?
Thank God to the physicists and the people for making that possible.
It resulted in my being able to donate money to Princeton, my family going to college, you know, et cetera, et cetera.
And having a nice house and all of that.
On the technical aspects, there are so many unexplained problems in large language models, broadly speaking.
Brittleness, predictability, explainability.
There are so many deep research issues that you can get plenty of PhDs just in solving those problems.
And I think your idea about integrating sensor models and vision models is a new one.
You should do that, right?
And at Princeton, as an undergraduate, you can do it.
Yes, sir.
Thank you.
HAP is my name.
It's an honor to listen to your talk.
My research is about creating technology for low-resource African languages.
And you already know the current technology is kind of centered to high-resource languages, especially English.
So how do we scale or how do we increase the, you know, inclusivity of low-resource languages?
So there's a belief in the industry that you can take a high-resource language, English,
and then you can essentially fine-tune it into the language structure of the languages that you're talking about.
If that's true, then that's a good solution.
If it's not true, can you as a researcher figure out how to make it true?
Because otherwise, we're going to have a problem where American hegemony takes over the world.
I'll give you a more general point to make the argument.
If you believe, as I do, that the U.S. and China are going to be the incredible competition in the next 20 years in this area,
it's going to then leave most of the other countries behind.
And these are proud countries with their own cultures, their own languages.
How do we propose to accommodate them?
Now, the typical American will say, well, what's wrong to speak English?
Okay, which strikes me as sort of dumb, right?
Wouldn't it be better if we actually accommodated their cultural mores, their languages, and so forth?
I think language is key to an aspect of culture.
That's why what you're doing is so important.
But I think the technical answer is it should be possible to essentially do fine-tuning
around these low-resource languages, because language has a common structure.
We learned this a long time at Google.
We were able to do translation of 100 languages by 100 languages pretty easy,
and indeed computer language to computer language as well.
The last question.
Hi.
Thanks.
I'm Jabari.
I'm a first-year grad.
Thank you for the really nice talk.
I heard last year that you were invited to Netanyahu's AI Advisory Forum.
And I wanted to ask, given Google's role in Project Nimbus and minimum estimates of up to 200,000 dead,
if you had met with Netanyahu or the Board of AI Advisors in the past year to discuss maybe killing less children?
I agree with the presumption in your question.
So we actually agree.
And I did not have any such meetings of any kind.
So zero meetings.
Great.
Thank you so much, all of you, for being here.
Can I just say one more time?
Oh, please.
It is, for those of you who are, well, I guess everyone here is either faculty or staff or going to be graduating,
it is a privilege to be here.
I knew it when the day I came in, and I still feel it today.
Thank you very much.
Thank you.
Thank you.
Thank you, John.
That was really good.
Thank you.
Yeah, good.
Thank you, Chris.
Thank you.
Thank you.
