Hey everyone. Hello. Thank you all for coming. My name is Mahesh and I'm on the Applied AI team
at Anthropic. I'm really excited to see a very full room and very excited that you chose me
over OpenAI. Thanks very much. So today we're going to be talking about MCP, Model Context Protocol.
This is more of a talk than a workshop, but I'll do my best to keep it interactive. If you want to
ask questions, feel free to do so and I'll do my best to answer them. Today we're going to talk
about the philosophy behind MCP and why we at Anthropic thought that it was an important thing
to launch and build. We're going to talk about some of the early traction of MCP in the last couple
of months and then some of the patterns that allow MCP to be adopted for AI applications, for agents,
and then the roadmap and where we're going from here.
Cool. So our motivation behind MCP was the core concept that models are only as good as the context
we provide to them. This is a pretty obvious thing to us now, but I think a year ago when most AI
assistance or applications were chatbots, you would bring in the context to these chatbots by
copy pasting or by typing or kind of pasting context from other systems that you're using.
But over the past few months, in the past year, we've seen these evolve into systems where the model
actually has hooks into your data and your context, which makes it more powerful and more personalized.
And so we saw the opportunity to launch MCP, which is an open protocol that enables seamless
integration between AI apps and agents and your tools and data sources. The way to think about MCP
is by first thinking about the protocols and systems that preceded it. APIs became a thing that
a while ago to standardize how web apps interact between the front end and the back end. It's a
kind of protocol or layer in between them that allows them to translate requests from the back end
to the front end and vice versa. And this allows the front end to get access to things like servers
and databases and services. LSP came later, and that standardizes how IDEs interact with language
specific tools. LSP is a big part of our inspiration and it's called Language Server Protocol and allows
an IDE that's LSP compatible to go and talk to and figure out the right ways to interact with different
features of coding languages. You could build a Go LSP server once and any IDE that is LSP compatible can
hook into all the things about Go when you're coding in Go. So that's where MCP was born.
MCP standardizes how AI applications interact with external systems and does so in three primary ways
and three interfaces that are part of the protocol, which are prompts, tools, and resources.
So here is the lay of the land before MCP that Anthropic was seeing. We spent a lot of time with
customers and people trying to use our API to build these agents and AI applications. And what we were
seeing is across the industry, but also even inside of the companies that we were speaking to, there was a
ton of fragmentation about how to build AI systems in the right way. One team would kind of create this
AI app that hooks into their context with this custom implementation that has its own custom prompt logic
with different ways of bringing in tools and data and then different ways of federating access to those
tools and data to the agents. And if different teams inside of a company are doing this, you can imagine that
the entire industry is probably doing this as well. The world with MCP is a world of standardized AI
development. You can see in the left box, which is the world of an MCP client, and there's some client
examples here like our own first party applications, recently applications like Cursor and Windsurf,
agents like Goose, which was launched by Block, all of those are MCP clients. And there's now a standard
interface for any of those client applications to connect to any MCP server with zero additional work.
An MCP server on the right side is a wrapper or a way of federating access to various systems and tools
that are relevant to the AI application. So it could be a database to query and fetch data and to give
the LM access to databases and records. It could be a CRM like Salesforce, where you want to read and
write to something that is hosted on a remote server, but you want the LLM to have access to it. It could
even be things on your local laptop or your local system, like version control and Git, where you want
the LM to be able to connect to the APIs that run on your computer itself.
So we can talk about the value that we've seen for different parts of the ecosystem over the past
few months. The value for application developers is once your client is MCP compatible, you can connect
it to any server with zero additional work. If you're a tool or API provider or someone that wants to give
LLMs access to the data that matters, you can build your MCP server once and see adoption of it
everywhere across all of these different AI applications. And just a quick aside, the way I
like to frame this is before MCP, we saw a lot of the n times m problem, where there were a ton of
different permutations for how these folks interact with each other, how client applications talk to
servers. And MCP aims to flatten that and be the layer in between the application developers and the tool and
API developers that want to give LLMs access to these data. For end users, obviously, this leads
to more powerful and context rich AI applications. If you've seen any of the demos on Twitter with
Cursor and Windsurf, even our own first party applications, you've seen that these systems are
kind of context rich, and they actually know things about you and can go and take action in the real
world. And for enterprises, there's now a clear way to separate concerns between different teams
that are building different things on the roadmap. You might imagine that one team that owns the data
infrastructure layer has a vector DB or a rag system that they want to give access to to other teams
building AI ops. In a pre-MCP world, what we saw was every single individual team would build their own
different way of accessing that vector database and deal with the prompting and the actual chunking
logic that goes behind all of this. But with MCP, an enterprise can have a team that actually owns the
vector DB interface and turns it into an MCP server. They can own and maintain and improve that, publish a
set of APIs, they can document it, and then all of the other teams inside their company can now build these
AI apps in a centralized way where they're moving a lot faster without needing to go and talk to
that team every time that they need access to it or need a way to get that data. And so you can kind
of imagine this is like the world with microservices as well, where different people, different teams
can own their specific service, and the entire company and the roadmap can move a lot faster.
Well, so let's talk about adoption. This is something that's been really exciting over the past couple
of months. It kind of comes up in almost every anthropic conversation with people that we work
with and a lot of our customers. The slide covers a few different personas, but we can start with the
applications and the IDEs. This has been really exciting recently, and it provides this really nice
way for people that are coding in an IDE to provide context to that IDE while they're working, and the
agents inside those IDEs go and talk to these external systems like GitHub, like documentation
sites, etc. We've also seen a lot of development on the server side. I think to date, there are
something like 1100 community built servers that folks have built and published open source. There are
also a bunch of servers built by companies themselves. I just built one as an example. There are folks like
and a bunch of others that have published official integrations for ways to hook into their systems.
There's also a ton of adoption on the open source side as well. So people that are actually contributing
to the core protocol and the infrastructure layer around it. So those are
a bit about what it actually means to build with MCP and some of the core concepts that are part of the protocol itself.
Here's kind of the view of the world of how to actually build with MCP. So on the left side,
you have the MCP client that invokes tools that queries for resources and interpolates prompts
and kind of fills prompts with useful context for the model. On the server side, the server builder
exposes each of these things. They expose the tools, the resources, and the prompts in a way that's
consumable by any client that connects to it. So it's like each of these components. A tool is maybe the
most intuitive and the thing that's developed the most over the past few months. A tool is model
controlled. And what that means is the server will expose tools to the client application and the model
within the client application, the LLM, can actually choose when the best time to invoke those tools is.
So if you use Cloud4Desktop or any of these agent systems that are MCP compatible, usually the way this
works is you'll interpolate various tools into the prompt. You'll give descriptions about how those
tools are used as part of the server definition and the model inside the application will choose
when the best time to invoke those tools are. And these tools are kind of the possibilities are kind
of endless. I mean, it's read tools to retrieve data. It's write tools to go and send data to
applications or kind of take actions in various systems. It's tools to update databases, to write files on your
files on your local file system. It's kind of anything. Now we get to resources. Resources
are data exposed to the application and they're application controlled. What that means is the
server could define or create images. It could create text files, JSON. Maybe it's keeping track of,
you know, the actions that you've taken with the server within a JSON file and it exposes that to the
application. And then it's up to the application how to actually use that resource. Resources provide
this rich interface for applications and servers to interact that go just beyond you talking to a
chatbot using text. So some of some of the use cases we've seen for this are files where the server
either surfaces a static resource or a static file or a dynamic resource where the client application can
send the server some information about the user, about the file system that they're working in,
and the server can interpolate that into this more complex data structure and send that back to the
client application. Inside Cloud for Desktop, resources manifest as attachments. So we let people,
when they're interacting with the server, go and click into our UI and then select a resource and it gets
attached to the chat and optionally sent to the model for whatever the user is working on. But
resources could also be automatically attached. You could have the model decide, hey, I see that
there's this list of resources. This one is super relevant to the task we're working on right now. Let
me automatically attach this to the chat or send it to the model and then proceed from there.
And finally, prompts. Prompts are user controlled. We like to think of them as the tools that the user
invokes as opposed to something that the model invokes. These are predefined templates for common
interactions that you might have with the specific server. A really good manifestation of this I've
seen is in the IDE called Z where you have the concept of slash commands where you're talking to
the LLM, to the agent, and you say, hey, I'm working on this PR. Can you go and summarize the work that
I've done so far? And you just type slash GHPR, you give it the PR ID, and it actually will
interpolate this longer prompt that's predefined by Z inside of the MTP server, and it gets sent to the
LLM, and you generate this really nice full data structure or full prompt that you can then send
to the LLM itself. A few other common use cases that we've seen are different teams have these
standardized ways of, let's say, doing document Q&A. Maybe they have formatting rules, they have,
you know, inside of a transcript, they'll have different speakers and different ways they want
the data to be presented. They can service that or surface that inside the server as a prompt,
and then the user can choose when it makes the most sense to invoke.
Cool. I'll pause there. Any questions so far about these various things and how they all fit together?
Yeah, I think we, a big part of MCP, sorry, the question is why aren't resources modeled in the
same way as tools? Why couldn't they have just been tools? A big part of the thinking behind MCP broadly
is it's not just about making the model better, it's about actually defining the ways that the
application itself can kind of interact with the server in these richer ways. And so tools are
typically model controlled, and we wanted to create a clean separation between what's model controlled
and application controlled. So you could actually imagine an application that's MCP compatible decides
when it wants to put a resource into context. Maybe that's based on predefined rules. Maybe that's
based on it makes an LLM call and makes that decision. But we wanted to create a clean separation for
the client builder and the server builder for what should be invoked by the model and what should be
invoked by the application. I saw you go first. Yeah, Glasses.
Yeah, the question is, are tools the right way to expose, let's say, a vector database to model?
The answer is kind of up to you. We think that these are really good to use when it's kind of
ambiguous when a tool should be invoked. So maybe the LLM sometimes should go and call a vector db. Maybe
sometimes it already has the information in context, and sometimes it needs to go talk to, maybe it needs
to go ask the user for more information before it does a search. So that's probably how we think about it.
If it's predetermined, then you probably don't need to use a tool. You just always call that vector db.
Sorry, sorry.
So the most things the MCP is able to do, the more important authorization and authentication becomes.
How are we modeling with these right now? We just inject them as part of the one of the parameters.
I'm going to get to that one later because it's very relevant and we have a lot to say.
So I may have missed this on-depth, but so if you've gone down the route of using the
agency framework which did tool calling, if you progress, would you just wrap the MCP with a tool
if you had a system solution?
Yeah, I think that it sounds like the broader question is how does MCP fit in with agent frameworks?
Cool. Yeah, I mean, the answer is they kind of complement each other. Actually,
LandGraph just this week released a bunch of connectors for, or I think they're called adapters,
for LandGraph agents to connect to MCP. So if you already have a system built inside LandGraph or
another agent framework, if it has this connector to MCP servers, you can expose those servers to the
agent without having to change your system itself as long as that adapter is installed. So we don't think
MCP is going to replace agent frameworks. We just think it makes it a lot easier to hook into servers,
tool prompts, and resources in a standardized way.
Okay, so by cancel method, the framework was a tool, the tool to call MCP, so going forward,
many of the tools would just be a wrapper for an MCP forward?
Yeah, the framework could call it tool, and that tool could be exposed to that framework from an MCP server,
if the adapter exists. Does that make sense? Yeah. Cool. I'll take one more if there are. Yeah.
So the question is kind of does MCP replace an agent framework and why still use one? I don't think it
replaces them. I think parts of it, it might replace the parts related to bringing context into the agent
and calling tools and invoking these things. But a lot of the agent framework's value, I think, is in the
knowledge management and the agentic loop and how the agent actually responds to the data that's brought
in by tools. And so I would think that there is still a lot of value in something where the agent
framework defines how the LLM is running in the loop and how it actually decides when to invoke the
tools and reach out to other systems. But I don't think MCP as a protocol itself fully replaces it.
MCP is more focused on being the standard layer to bring that context to the agent or to the agent
framework. Yeah. I don't know if that's the most clear answer, but that that's the one that we've
at least seen so far that might change as MCP involves. Cool. Sorry, I saw one more,
which I'll take, and then I will move on if that exists. Yeah.
Merkett-
Merkett-
Merkett-
Merkett-
Merkett-
yeah so the question is why do resources and prompts exist and why isn't this all
baked into tools because you can serve a lot of the same context via tools themselves so i think we
touched on this a little bit but there's actually a lot more protocol capabilities built around
resources and prompts than what i'm talking about here so part of your question was aren't resources
and prompts static can't they just be served as static data as part of a tool in reality resources
and prompts in mcp can also be dynamic they can be interpolated with context that's coming in from
from the user or from the application and then the the server can return a dynamic or kind of
customized resource or customized prompt based on the task at hand another kind of really valuable
thing we've seen is resource notifications where the client can actually subscribe to a resource
and anytime that resource gets updated by the server with new information with new context the server can
actually notify the client and tell the client hey you need to go update the state of your system or
surface new information to the user but the the broader answer to your question is yes you can do a
lot of things with just tools but mcp isn't just about giving the model more context it's about giving the
application richer ways to interact with that the various capabilities the server wants to provide so
it's not just you want to give a standard way to invoke tools it's also if i'm a server builder and i want
there to be a standard way for people to talk to my application uh maybe that's a prompt maybe i uh
you know i have a a prompt that's like a five-step plan for how someone should invoke my server and i
want the client applications or the users to have access to that that's a different paradigm because
it's me giving the user access to something as opposed to me giving the tool access to something
and so i kind of tried to write this out as model controlled application controlled and user
controlled the point of mcp is to give more control to each of these different parts of the system as
opposed to only just the model itself yeah i hope that that kind of makes sense
all right um let's see what this actually looks like the the wi-fi is a bit weird this works cool so
what we're looking at is cloud for desktop which is an mcp client um let me try to pause as this goes
through so cloud for desktop which is on the left side an mcp client and on the right side i'm working
inside of a github application um let's say i'm a repo maintainer for the anthropic python sdk i need to
get some work done what i'm doing here is i give the cloud for desktop app the url of the the repo i'm
working in and i say can you go and pull in the issues from this github repo and help me triage
them or help suggest the ones that that sound most important to you the model claude automatically
decides to invoke the list issues tool which it thinks is the most relevant here and actually pulls
calls that and and pulls these into context and start summarizing it for me you'll also notice that
i told it to triage them so it's automatically using what it knows about me from my previous
interactions with claude maybe other things in this chat or in this project to uh kind of intelligently
decide here are the top five that sound most important to you based on what i know about you
and so that's where the interplay between just giving models tools and actually the application
itself having other context about who i am what i'm working on the types of ways i like to interact
with it um and then those things interplay with each other the next thing i do is can you i ask it
can you triage the top three highest priority issues and add them to my asana project um i don't
give it the name of the asana project but claude knows that it needs to go and find that information
autonomously so i've also installed an asana server and has that has like 30 tools it first decides to
use list workspaces then search projects it finds the project and then it starts invoking tools to
start adding these as as tasks inside asana so this might be a pretty common application that you like
to use but the the things i want to call out are one i didn't build the asana server or the github
server these were built by the community each of them are just a couple hundred lines of code um
primarily it's a way of surfacing tools to the server and so uh it's not a ton of additional logic to
build i would expect they could be built in an hour um and they're all kind of playing together with
claude for desktop being the central interface it's really powerful to have these various tools
that someone else built for systems that i care about all interplaying on this application that i
like to use every single day claude uh claude for desktop kind of becomes the central dashboard for how
i bring in context from my life and i actually like run my day-to-day um and so inside anthropic we've
been using things like this a ton to go and reach out to you know our arkit repos to uh even make prs
or to bring in context from prs and mtp is the standard layer across all of this
cool and so just to close that out um here's windsurf and it's an example with using different
servers but it's windsurf's own application layer for connecting to mcp they have their own kind of ui
inside of their agent uh their own way of talking to the mcp tools other applications don't even call
them mcp tools for example boost calls them extensions um it's really up to the application
builder how to actually bring this context into the application the point is that there's a standard way
to do this across all of these applications
awesome so so far we've talked about um how to bring context in and how mcp brings context into
a lot of ai applications that you might already be familiar with but the thing that we are most excited
about and starting to see signs of is that mtp will be the foundational protocol for agents broadly
and there's a few reasons for this one is the the actual protocol features and the capabilities
that we're going to talk about in just a second um but it's also the the fact that these agent
systems are becoming better that the models themselves are becoming better and they use the
data you can bring to them uh in increasingly effective ways and so we think that there's some
really nice tailwinds here um and and let's talk about how or why we think that this is going to be
the case so um you might be familiar with the the blog that we put out my friends barry and eric put
out a couple months ago called building effective agents and one of the core things in the blog that
one of the first ideas that was introduced is this idea of an augmented llm it's an llm uh in the
traditional way that it takes inputs it takes outputs um and it kind of uses its intelligence
to decide on some actions but the augmentation piece are those arrows that you see going to things
like retrieval systems to tools and to memory so those are the things that allow the llm to query
and write data to various systems it allows the llm to go and invoke tools and respond to the results of
those tools in intelligent ways and it allows the the llm to actually have some kind of state such
that every interaction with it isn't a brand new fresh start it actually kind of keeps track of the
progress it's made as it goes on and so mcp fits in as basically that entire bottom layer mcp can
federate and make it easier for these llms to talk to retrieval systems to invoke tools to bring in memory
and it does so in a standardized way it means that you don't need to pre-build um all of these
capabilities into the agent when you're actually building it it means that agents can expand after
they've been programmed even after they've been initialized and they're starting to run to start
discovering different capabilities uh and different interactions with the world even if they weren't
programmed or built in to start um and and the the core thing in the blog or one of the simpler ideas
in the blog is agent systems at its core aren't that complicated they are this augmented llm uh concept
running in a loop where the augmented llm goes and does a task it kind of works towards some kind of
goal it uh invokes a tool looks at the response and then does that again and again and again until it's
done with the task and so where mcp fits in is it gives the llm the augmented llm these capabilities
in an open way what that means is even if you as an agent builder don't know everything that the agent
needs to do from the time at the time that you're building it uh that's okay the the agent can go and
discover these things uh as it's interacting with the system and as it's interacting with the real world
you can let the users of the agent go and customize this and bring in their own context in their own
ways that they want the agent to touch their data and you as the agent builder can focus on the core
loop you can focus on context management you can focus on how it actually uses the memory what kind
of model it uses the agent can be very focused on the actual interaction with the llm at its core
so i want to talk about a little bit about what this actually looks like in practice
um let me switch over to screen sharing my screen
cool so to talk about this um we're going to be talking about this framework this open source
framework called mcp agent that was built by our friends at last mile ai i'm just using it
as a really clean and simple example of how we've seen some of these agent systems uh kind of play in
with mtp so i'm switching over to my code editor make this bigger and what you see here is a pretty
simple application um the entire thing is maybe 80 lines of code um and i'm defining a a set of agents
inside of this type of the the overall task that i want this agent to achieve is defined in this uh this
task.md and basically i wanted to go and do research about quantum computing uh i wanted to
give me a research report about quantum computing's impact on cyber security and i tell it a few things
i want i want to go look at the internet synthesize that information and then give that back to me in
this nicely formatted file and so what mcp agent the framework lets us do is define these different
sub-agents the first one i'm defining is what's called a research agent where i give it the task that
it's an expert web researcher um its its role is to you know go look on the internet to go visit some
nice urls and to give that data back in a nice and structured way in my file system and you'll see on
the bottom is i've given it access to a few different mtp servers i've given access to brave for searching
the web i've given it a fetch tool to actually go and pull in data from the internet and i've given
access to my file system i did not build any of those mtp servers um and i'm just telling it the
name and it's going to go and invoke them and install them and making sure make sure that the
agent actually has access to them the next one similarly is a fact checker agent it's going to
go and verify the information that's coming in from the research agent it's using the same tools brave
fetch and file system and these are just mtp servers that i'm giving it access to and finally
there's the research report writer agent and that actually synthesizes all the data looks at all the
references and the fact checking and then produces a report for me in this nice format this time i'm
only giving it the file system and fetch tools or servers i don't need it to go look at the internet
i just needed to process all the data uh that it has here um and it knows what servers each of them
have access to and then once i pick it off the first thing it's going to do is go and form a plan
a plan is just a series of steps for how it should go and interact with all these systems and the
various steps you should take until it can call the task done so as an example the the first step it's
going to go and look at authority sources on quantum computing um and it's going to invoke the searcher
agent in in various different ways it knows it creates this plan based on the context about the
agent's task about the servers it has access to uh and so on the next step is maybe it goes and
verifies that information uh by focusing on the fact checker agent specifically and then finally
it intends to use the writer agent to go and synthesize all of this the kind of uh core piece
of this is mcp becomes this abstraction layer where the agent builder can really just focus on the task
specifically and the way that the agent should interact with the systems around it as opposed
to the agent builder having to focus on the actual servers themselves or the tools or the data it just
gives uh it kind of declares this in this really nice declarative way of this is what your task is
supposed to be and here are the servers or tools that you have available to you to go and accomplish that
task and so just to close out that part of demo i'm just going to kick this off
um and what's going to be going on the background is it's going to start doing some research uh it's
invoking the search tool uh the search agent and it's going to invoke the fact checking agent and
you'll start to see these outputs uh appear on the left side of the screen um and so this is a pretty
simple demo but i think it's a very powerful thing for agent builders because you can now focus
specifically on the agent loop and on the actual core capabilities of the agent itself and the tasks that
the the sub agents are working on as opposed to on the server capabilities and the ways to provide
context to those agents the other really nice piece of this which is obvious is we didn't write those
servers uh someone else in the community built them maybe uh the the most authoritative you know
source of research papers on quantum computing wrote them but all we're doing is telling our agents
to go and interface with them in a specific way and so you start to see the the outputs form the it looks
like the searcher agent put a bunch of sources in here um it's already started to draft the uh the
actual final report and it's going to continue to iterate in the background
cool
definitely yeah um so the question is have we seen agent systems uh also working for proprietary data
uh the really nice thing about mtp again is that it's open and so you can actually run mtp servers uh
on inside your own vpc um you can run it on top of uh on your your employees uh individual systems
and laptops themselves uh so the answer is definitely no yeah
yeah
yeah so uh the question is what does it mean to separate uh the agent itself and now the capabilities
uh that other folks uh kind of give to it
i i think the answer kind of varies um some of the ways that we've seen to improve agent systems
are uh you know what kind of model do you use is it actually the right model for the specific task if
you're building a coding agent or probably you should use cloud um and there's also things like context
management or knowledge management how do you store the the context and summarize it or compress that
context as the context window gets larger there's orchestration systems like if you're using multi-agent
agent are they in series are they in parallel and so there's a lot more that you can focus on based
on your task uh in that sense as well as uh the interface itself like how is the surface to the
user and the separation is then maybe you build a bunch of your own mcp servers for your agent that are
really really customized to what you want to do but when you want to expand the context to what the rest of
the world is also working on or the systems that exist in the rest of the world that's where mcp fits
in like you don't need to go and figure out how to hook into those systems that's all pre-built for you
uh let's do yeah that anyway
uh
so are people using mcp in this kind of way like only like one and for the
Yes, there is a slide that we'll get to, which is exactly that.
No, you're good.
No, that's great.
Really good questions.
Let's I'm going to do this side of the room because I didn't.
Yeah, not a ton of this is specific to last mile I think it's a really great framework.
It's called mcp-agent, and specifically what they worked on is they saw these things come out.
One was the agents framework.
There's really simple ways to think about agents, and they saw mcp, which is there are really simple ways to think about bringing context to agents.
And so they built this framework which allows you to implement the various workflows that were defined in the agents blog post using mcp and using these really nice declarative frameworks.
So what's specific to mcp agent, the framework is these these different components or building blocks for building agents.
So one is the concept of an agent.
An agent, as we've talked about, is an augmented LLM running in a loop.
So when you invoke an agent, you give it a task, you give it tools that it has access to, and the framework takes care of running that in a loop.
It takes care of the LLM that's under the hood and all of those interactions.
And then using these building blocks, you go a layer above, and you hook those agents together in different ways that are more authentic.
And those are described in the paper.
But one of the things in the in the blog post was this orchestrator workflow example.
So that's what I've implemented here, which is I've initialized an orchestrator agent, which is the one in charge of planning and keeping track of everything.
And then I give it to give it access to these various sub agents using all these nice things that are part of integration.
That being said, it's open source like it's not that I'm blessing.
This is the right way to do it necessarily, but it's a really simple and elegant way of doing it.
So the question is, how do resources and prompts fit in in this case?
The answer is they don't.
This example was more focused on the agentic loop and giving tools to them.
I would say resources and prompts come in more where the user is within the loop.
So you might imagine, instead of me just kicking this off as a Python script, I have this nice UI where I'm talking to the agent, and then it goes and does some asynchronous work in the background.
And it's a chat interface like what you might see with Claude.
In that case, the chat interface, the application could, you know, take this plan that I just showed you and surface this to me as a resource.
The application could have this nice UI on the side that says here's the first step, the second step, the third step.
And it's getting that as the server surfaces it to surface is the plan to it as this kind of formula.
Prompts could come in if there's a few examples, but you could say a slash command to summarize all of the steps that have occurred already.
You could say slash summarize, and there's a predefined prompt inside of the server that says here's the right way to give the user a summary.
Here's what you should provide to the LLM when you go and invoke the summarization.
So the answer to your question is, it doesn't fit in here, but there are ways it could.
Okay, I'll take, like, two more.
Let's go with you.
Does this introduce any, like, new workflows as it relates to, like, evaluations in this, if you're servicing a bunch of different tools,
is there any sort of, like, way into evaluations to understand if it's choosing the right tool, or
I think the answer, the question is, how did this fit into evaluations?
In particular, evals related to assessing tool calls and that's being done the right way.
So I think largely it should be the same as it is right now.
There is potential to have MCP be even a standard layer inside evals themselves.
I probably need to think this through, but you can imagine that there's an MCP server that surfaces, you know, the same five tools, and you give that server to one set of evals.
You also, let's say you have one eval system running somewhere to eval, like, these five different use cases.
You have a different eval system.
The MCP server could be the standard way to surface the tools that are relevant to your company to both of them.
But largely, I think it's similar to how it's been done already.
Yeah.
And the way?
I'll get to that.
I'll get to that.
Yeah.
What is it?
I think it's kind of a separate sort of .
Because a lot of the stuff that you put on the side side,
how would you say that stuff like authentication, or all of that stuff,
.
Yeah, I can address part of this.
So the question is, what is the separation between a lot of the logic that you need to implement in these systems?
Where should it sit?
Should it sit with the client or the server?
And the specific examples are things like retry logic, authentication.
I'll get to off in a bit, but on things like retry logic, I think my personal opinion, and I think this remains to be seen how it shakes out, is a lot of that should happen on the server side.
The server is closer to the end application and to the end system that's actually running somewhere, and therefore the server should have more control over the interactions with that system.
So the second principle is ideally MCP supports clients that have never seen a server before.
They don't know anything about that server before the first time it's connected, and therefore they shouldn't have to know the right ways to do retries.
They shouldn't have to know how to do logging in the exact way that the server wants and things like that.
So the server is ideally closer to the end application, and it's the one that's the end of service, and it's the one that's implementing a lot of that business logic.
It depends.
I don't have a really strong opinion to take on on where the agent frameworks themselves go.
I could see one counter argument being that you don't always want the server builders to have to deal with that logic either.
Like maybe the server builders want to just focus on exposing their API's and like letting all the agents do the work.
And yeah, I want to say I don't have a really strong take on that.
Yeah.
Is there a best practice?
Yeah, that's a really good question.
I'm glad you asked that. So a lot of the questions that we get. Sorry, the question here is, is there a best practice or a limit to the number of servers that you expose to an LM?
In practice, the models of today, I think, are good up to like 50 or 100 tools like Claude is good up to a couple hundred in my experience.
But beyond that, I think the the question becomes, how do you search through or expose tools in the right way without overwhelming the context, especially if you have thousands?
And I think there are a few different ways like one of the ones that's exciting is a tool to search tools.
And so you can imagine a tool abstraction that implements rag over tools.
It implements fuzzy search or keyword search based on, you know, the entire library of tools that's available.
That's one way. We've also seen like hierarchical systems of tools.
So maybe you have a group of tools that's, you know, finance tools.
You have like read data, then you have a group of tools that's for writing data, and you can progressively expose those groups of tools based on the current task at hand, as opposed to putting them all in the system, for example.
So there are a few ways to do it. I don't think everyone's landed on one way.
But the answer is there's technically no limit if you implement it the right way.
I don't know if you're going to get into this, but you're like a methodology or best practice of like, I have a step like first to find those servers, then find like promises and resources.
Like, are you going to kind of walk through like a step by step process?
Do you have that document?
Yeah, I'm not going to go through it yet, but we do have that documented.
So the question is like, what are the right steps to approach building an MCP server?
What's the order of operations?
We actually have this entire docs page that's like, how do you build an MCP server using Claude or using LLMs?
All the servers that we launched with in November, I think there were like 15 of them.
I wrote all of those in like 45 minutes each with Claude.
And so it's like really easy to approach it.
And I think tools are typically the best way for people to start rocking what a server is, and then going to prompts and resources from there.
Yeah, definitely.
I'll share links later.
Yeah, in the red.
I guess that takes a question a minute, but scanning back our own systems.
At what point is the servers where it's kind of generic .
Yeah.
The question is, if a lot of these servers are simple, can LLMs just generate them automatically?
The answer is yes.
If you guys have heard of Klein, which is one of the most popular IDEs that's open source.
It has like 30 K stars on GitHub.
They actually have an MCP auto generator tool inside the app.
You can just say, hey, I want to start talking to GitLab.
Can you make me a server and just auto generates on the fly?
Yeah, that being said, I think that that works for like the simpler servers, like the ones that are closer to just exposing an API.
But there are more complex things that you want to do.
You want to have logging or logic for data transformations.
But the answer is, yeah, for the more simple ones.
I think that's a pretty normal workflow.
Yeah, so the question is, are we talking to the actual owners of the services and the data?
Answer is yes.
A lot of them, a lot of the servers actually are official and public already.
So if I just scroll through official integrations, these are like real companies like Cloudflare and Stripe that have already built official versions of these.
We're also talking to bigger folks, but I can't speak to that yet.
They might also host the servers remotely.
Yes.
Yeah.
Like they'll build it and then they also maybe provide the infrastructure to expose it.
Yeah.
In the back.
You're asking about versioning as it relates to the protocol or to servers.
Yeah.
So the question is, how do we do best practices for versioning?
All these servers are so far, a lot of them are TypeScript packages on NPM or on PIP.
Therefore, they also have version package versions associated with them.
And so there shouldn't generally be code breaking changes.
There should be a pretty clear upgrade path.
But yeah, I don't think we actually have best practices just yet for what to do when a server itself changes for something like.
I mean, generally, I think it might break the workflow, but I don't think it breaks the application.
If the server changes since as long as the client and server are both following the MCP protocol, the the tools that are available might change over time or they might evolve.
But the model can still invoke those in intelligent ways for resource and prompts.
They might break users workflows if those resources and prompt changes, but like they'll still work as long as they're being exposed as part of the MCP protocol with the right list tools, call tools, list resources, etc.
I don't know if that answers your question, though.
I was thinking more on the fuzzy side of your school chain.
Right.
I think using versioning of the packages themselves makes sense for that.
And then I'm going to talk a little bit about a registry and having a registry MCP registry labor on top of all of this will also help a lot with that.
Okay, I'll take one more and then continue.
Yeah, question is, how are we thinking about distribution and extension system?
I'll get there too.
Yeah.
Cool.
Let's let's keep going.
So.
So we've talked about one way to build effective agents and I showed how to do that using the MCP agent framework.
Now I want to talk about the actual protocol capabilities that relate to agents and building agent systems with the caveat that these are capabilities in the protocol, but it's still early days for how people are using these.
And so I think a lot of this is going to evolve, but these are some early ideas.
So one of the most powerful things that's underutilized about MCP is this paradigm called sampling.
Sampling allows an MCP server to request completions, aka LLM inference calls from the client instead of the server itself having to go and implement interaction with an LLM or to go host an LLM or call cloud.
So what this actually means is, you know, in typical applications, the one that we've talked about so far.
It's a client where you talk to it and then it goes and invokes server to have some kind of capability to get user inputs and then decide.
Hey, I actually don't have enough input from the user.
Let me go ask it for more information or let me go formulate a question that I need to ask the user to give me more information.
And so there's a lot of use cases where you actually want the server to have access to intelligence.
And so sampling allows you to federate these requests by letting the client own all interactions with the LLM.
They can, the client can own hosting the LLM if it's open source, they can own, you know, what kind of models that's actually using under the hood.
And the server can request inference using a whole bunch of different parameters.
So things like model preferences, maybe the server says, hey, I actually really want, you know, specifically this version of Claude.
Or I want a big model or a small model.
Do your best to get me one of those.
The server obviously will pass through a system prompt and a task prompt to the client.
And then things like temperature, max tokens, it can request.
The client doesn't have to listen to any of this.
The client can say, hey, this looks like a malicious call.
Like, I'm just not going to do it.
And the client has full control over things like privacy, over the cost parameters.
Maybe it wants to limit the server to, you know, a certain number of requests.
But the point is, this is a really nice interaction because one of the design principles, as we talked about, is oftentimes these servers are going to be something where the client has never seen them before.
It knows nothing about them, yet it still needs to have some way for that server to request intelligence.
And so we're going to talk about how this builds up a little bit to agents.
But just putting this out there is something you should definitely explore, because I think it's a bit underutilized thus far.
One of the other kind of building blocks of this is the idea of composability.
So I think someone over there asked about composability, which is a client and a server is a logical separation.
It's not a physical separation.
And so what that means is any application or API or agent can be both an MCP client and an MCP server.
So if you look at this very simple diagram, let's say I'm the user talking to Cloud for desktop on the very left side, and that's where the LLM lives.
And then I go and make a call to an agent.
I say, hey, can you go find me this information?
I ask a research agent to go do that work.
And that research agent is an MCP server, but it's also an MCP client.
That research agent can go and invoke other servers.
Maybe it decides it wants to call, you know, the file system server, the fetch server, the web search server, and it goes and makes those calls and then brings the data back.
Does something with that data and then brings it back to the user.
So there's this idea of of chaining and of these interactions kind of hopping from the user to a client server combination to the next client server combination and so on.
So this allows you to build these really nice, complicated or complex architectures of different layers of LLM systems where each of them specializes in a particular task that's particularly relevant as well.
Any questions about composability? I'll touch on agents as well.
Yeah, so question is, how do you deal with compounding errors if the system itself is complex and multilayered?
I think the answer is the same as it is for complex hierarchical like agent systems as well.
I don't think MCP necessarily makes that more or less difficult.
But in particular, in particular, I think it's up to each successive layer of the the agent system to deal with information or controlling data as it's structured.
So like to be more specific, you know, the third node there, the kind of the middle client server node should collect data and fan in data from all of the other ones that just reached out to, and it should make sure it's up to par or meets whatever data structure days on spec.
It needs to before passing that data to the system right before it.
I don't think that's special to MCP I think that is true for all these like multi node systems.
It's just this provides like a nice face between each of them.
Does that answer your question?
Yeah.
Cool.
So I saw their hands.
let me try to spell yourself.
yeah the question is um why are these and why do they have to be mcp servers as opposed to just a
regular http server um the answer in this case for composability and like the layered approach
is that each of these can basically be agents um like in in the system that you're kind of talking
about here uh it's i i think that there's there are there are reasons for a bunch of protocol
capabilities like resource notifications like server to client communication the server
requesting uh more information from the client that are built into the mcp protocol itself
so that each of these interactions are more powerful than just data passing between different
nodes like let's say each of these are agents like the first agent can ask the next agent for
you know a specific set of data it goes and does does a bunch of asynchronous work talks to the real
world brings it back and then sends that back to the first client which um that might be multi-step
it might take multiple interactions between each of those two nodes um and that's a more complex
interaction that's captured within the mcp protocol that not might not be captured if it were just
regular http servers
i think that the point i'm trying to make is that uh each of these so you're asking like if uh the
google api or the file system things were just apis like regular non-mcp servers but mc making it an
mcp server in this at least in this case allows you to capture those as uh agents as in like they're
more intelligent than just you know exposing data to the lm it's like the each of them has autonomy um
you can give a task to the second server and it can go and make a bunch of decisions for how to pull in
richer data um you could in theory just make them regular apis but you lose out on like
these being independent autonomous agents each node in that system in the way it interacts with
the the task it's working on yeah so in terms of controlling low-ups and rate limits is that just
handled by the first or is there a more global way of handling that yeah um it kind of depends
on on the builders but i i do think it's federated uh because the llm is at the application layer
um and so that has control over uh how random rate limits work or how it should actually interact
with the llm it doesn't have to be that way like in theory if the server builder like the first node
wanted to own the interaction with the specific llm maybe it's running open source on that specific
server it could be the one that controls the llm interaction but in the example i'm giving here
the llm lives at the very base layer and at the application there and it's the one that's controlling
rate limits and control flow and things like that uh if it wants user input it does have to go all
the way back yeah and mcp does allow you to pass those interactions all the way back and then all
the way back forward yeah um i'm gonna go on this side first yeah um do you have to collect the
primary the guy zooming out a little bit but if there's a discrepancy it's just flat
uh yeah the question is how do you elect a primary how do you make decisions in network
uh the answer is it's kind of up to you i'm not opining on like network systems themselves or how uh you
know these these like logic requirement it's not a requirement it's not part of the protocol itself
it's just that mtp enables this architecture to exist okay so um i think the idea so the question is
how do you do observability how do you know the the other systems that are being invoked uh from a
technical perspective there's no specific reason that the application or the user layer would know
about those servers um in theory for example like the the first client application or the first mtp
server you see there uh is kind of a black box it makes the decisions about if it wants to go invoke
other sub-agents or other services and i think that's just how like the internet layer like apis work
today like you don't exactly know always what's going on behind the hood the protocol doesn't
opine on how observability should work or enforcing that you need to know the interactions that's really
up to the builders and the ecosystem itself
it's building on that like debugging sounds
like it's like uh it's not even not even posibility even without the possibility it's like
uh how do you guys have best practices on this where now we don't even know like we're calling a server
it as creative by somebody else uh yeah you're right if you call an api i don't know but if i call
a strike api i don't know exactly what that it is based on the interface or how to describe the dots
um but the mcp server if it's more university like a wrapper on the apis that already exists
uh yeah so the question is how do you actually make mcp servers debuggable um especially if it's
more than just a wrapper around an api and it's actually doing more complex things the answer is
that uh the protocol itself doesn't enforce like specific observability and interactions it's kind of
like incentive alignment for the server builder to expose useful data to the client um the mcp does
of course have ways for you to pass metadata um between the the client and the server and so if
you build a good server that has good debugging and and actually provides that data back to the client
you're more likely to be useful and actually like have a good ux um but the protocol itself doesn't
kind of enforce that if that's kind of what you're asking um which i think is the same answer
for apis like people will use your api if it's ergonomic and it's good and it makes sense and
provides you debugging and logs um so we think servers should do that i think we do have best
practices um i don't know off the top of my head but i can follow up with that so i guess somebody
asks you know something was wrong yeah was it the reason what was the resource i was given uh
uh was there yeah it sounds like those kind of things that they are still developing like best
practices or companies that's right like the best practices are still emerging these are
yeah i think the answer is we will get there like either anthropic or like mcp builders themselves or
the community will start to converge on best practices but i agree with you that there needs
to be best practices about how to debug and stuff this is more of an observation to other
architectural patterns that you have you have to have a trace and you have
it's only when you need that microservice that's because they have reasoning that you're not turned
over to so i think there are patterns that they're analogous to here it's just like finally
we're bringing in hey we also want this service to now reason that's that's exactly right yeah just
comment on like this is very similar to microservices except this time we're bringing in intelligence but
there are patterns that exist that we should be drawing from yeah um
you expect that to happen in natural language or um
um
yeah the question is um let's say that the client wants some amount of control or influence over
the server itself or the tool call like uh limit the number of web pages you go and look like look
at how do you do that so yeah one suggestion is by doing that via the prompt like that's an obvious
one that you can do uh one thing we're thinking about is something called like tool annotations
um these extra parameters or metadata that you can surface uh in addition to the regular tool call or
specifying the tool name to influence something like can you limit the number of tools uh or limit
equals five that's something that the server builder and the tool builder inside that server would have to
expose uh to be invoked by the the client but we're thinking about at least in the protocol a
standard uh a couple of standard fields that could could help with this so one example that comes to
mind is maybe the server builder exposes a tool annotation that's read versus write and so the
client actually can now know hey is this tool going to take action or is it only just like read only
um and i think the opposite vice versa of that is what you're talking about where uh is there a way
for the server to expose more parameters for how to control its behavior yeah
yeah so question on like on devx and how to actually you know look at the logs and actually
respond to them so one um shout out is we have something called inspector in our repo and inspector
lets you go look at logs and actually make sure that the connections to servers are making sense
so definitely check that out i think your question is uh could you build a server that
debugs servers i'm pretty sure that exists and i've seen it where it goes and looks at the standard
io logs and goes and make changes to to make that work i've seen servers that go and set up the desktop
config to make this work so yeah the answer is definitely you can have loops here i'll take the
last one uh and then i'll come back to these at the end
uh
yeah the question is around governance and security and who makes the decisions about
what a client gets access to um i think a lot of that should be controlled by the server builder
um we're we're going to talk about auth very shortly but that's a really big part of it like uh there
should be a default way in the protocol to there is a default way in the protocol to do authorization
authentication um and that should be a control layer to the end application that the server is
connecting to um and yeah i think that's the design principle is like you could have not malicious
clients but clients that want to ask you for all the information and it's the server builder's
responsibility to control that flow yeah i'm going to keep going and then i'll make sure to get back to
questions in just a second um so i think we basically have covered this but the combination
of sampling and composability um i think is really exciting for a world with agents um specifically
where if i'm an end user talking to my application and chatbot uh i can just go talk to that and it's
a orchestrator agent that orchestrator agent is a server and i can reach out to it from my cloud for
desktop but it's also an mcp client and it goes and talks to an analysis agent that's an mcp server
a coding agent another mcp server and a research agent as well and the this is composability and
sampling comes in where i am talking to claude from claude for desktop and the each of these agents and
servers here are federating those sampling requests through the layers to get back to my application
which actually controls the interaction with claude um so you get these really nice hard well they
will exist they don't exist yet but you will get these really nice hierarchical systems of agents
and sometimes these agents are going to live you know on the public web or they won't be built by you
but you'll have this way to connect with them while still getting the privacy and security and control
that you actually want when you're building these systems um so in a sec we're about to talk about
uh what's next and registry and discovery um but this is kind of the vision that i personally really
want to see and i think we're going to get there um of this like connectivity layer while they're still
being guarantees about who has control over the specific interactions in each of these okay i'll get
to questions in in a sec i'm just going to keep going so we've talked about a few things we've talked
about how people are using mcp today um we've talked about how it fits in with agents um there's a lot of
really exciting things that a lot of you have already asked about uh that are on the road map
and coming very soon so one uh is remote servers and auth so um let me pause this to say what's going
on so first um this is inspector uh this is the application i was just talking about where um it lets
you you know install a server and then see all the kinds of various interactions uh inspector already
actually has auth support so we added auth to the protocol about uh two three weeks ago we then
added to inspector it's about to land in all the sdks uh so you should go and check for that as soon
as it's available but basically what we're doing here is we provide a url to an mcp server for slack
um this is happening over ssc which uh as opposed to standard io ssc is the the best way to do remote
servers and so i just give it the link which is on the left side of the screen there and then i hit connect
and what happens now is that the server is orchestrating the handoff between
the server and slack it's doing the actual authentication flow and the way it's doing that
is uh the protocol now supports oop 2.0 and the server deals with the handshake where it's going out
to the slack server getting a callback url giving it to the client the client opens that in chrome
the user goes through the flow and clicks yeah this sounds good allow uh and then the server holds the
actual uh oauth token itself um and then the server federates the interactions between the user and and
the slack application by giving the client a session token for all future interactions um so the highlight
here and i think this is the number one thing we've heard since day one of launch is this will enable
remotely hosted servers this means servers that live on a public url um and can be discoverable by
people through mechanisms i'll talk about in a sec but you don't have to mess with standard io you can
have the server fully control those interactions those requests and they're all happening remotely
the the agent and the llm can live on a completely different system than wherever the server is running
uh maybe the server is an agent if you bring in that composability piece we just talked about
um but this i think is going to be a big like explosion in the number of servers that you see
because it removes the devx friction it removes the fact that you as a user even need to know what mcp
is you don't even need to know you know how to host it or how to build it it's just there it
exists like a website exists and you just go visit that website so any questions on remote server
actually because i know a lot of people are interested in this
yeah i think the question is um does our support of oath also allow for it sounds like scope change or
like again starting off with basic permissions but allowing people to request elevated permissions
and for those to be respected through the survey vertical yeah um like elevating from basic to
advanced permissions i think in the first version of it it does not support it out of the box but we
are definitely interested in evolving our support for off
so uh the question is uh isn't it a bad thing that the server holds the actual token i think if you
think about the design principle of the server being the one that actually is closest to the end application
of slack or wherever you want the data to exist like let's say slack itself uh builds a public mcp
server and decide decides the way that people should opt into it i think slack will want to control the
actual interaction between that server and the slack application um and then the the way that i think the
fundamental reason for this is clients and servers don't know anything about each other before they
start interacting um and so giving the server more control over how the interaction with the final
application exists um i think is what allows there to be a separation does that kind of make sense
uh yes uh you should be judicious about what servers you connect to i think that's true for all web apps
today as well
for what servers they have access to bs trust of servers is going to be increasingly important
um which we'll talk about with the registry in just a second
yeah the question is how does this fit in with restful apis and does it interact with the server
uh mcp is particularly good when there's uh i don't know data transformations or some kind of logic that you want to have on top of just the the interaction over rest um maybe that means
there are certain things that are better for lms than they would be for just a regular old client application that's talking to a server
uh maybe that's the way that the data is formatted maybe that's the amount of context you give back to the model
um you get a request uh you get something back from a server and you say hey claude like these are the five things you need to pay attention to
this is how you should handle this interaction after this the server is controlling all that logic and surfacing here
uh restful is going to still exist forever and that's going to be more for those stateless interactions
we're just going back and forth you just want the data itself yeah
no
no
mcp one of the things that's like
progressions on our tools here
um
how do we kind of think about
regressions or
emails
yeah um the question is how do we think about regressions as servers change as tool description
change how do we do evals um so a couple of things one is we're going to talk about the
registry in just a sec but i that's probably something we talked about with versioning where
you can pin a registry and as it changes you should test that new behavior um i think that this doesn't
change too much about the evils evals ecosystem around tools um you might imagine like a lot of the
customers that we work with we help them go and build these frameworks around how their agent
talks to tools and that's you know what's the right way what when should you be triggering a tool
call how do you handle the response these are pre-existing evals that exist or should exist
i think mcp makes it easier for people to build these systems around tool calls but that doesn't
change anything about how robust these evals need to be um but it does make it easier right because
you could at least the way i think about it is like i have my mtp server 1.0 my builder my developer
publishes 1.1 and then i just run 1.1 against the exact same evals framework and it provides this
really nice like diff i guess um yeah i don't think it changes too much about the needs of building evals
themselves yeah just the ergonomics um it's in the draft spec it's in there's an open pr in the sdks
so it's like i would say days away yeah uh it is an inspector though it's like fully implemented in
there so check it out cool i want to go to registry because a lot of questions about registry so
a huge huge thing that we've seen over the past two months is there's no centralized way to discover
and pull in uh mcp servers you've probably seen the the servers repo that we launched uh it's kind
of a mess there there are like a bunch that we launched there are a bunch that our partners
launched uh and then like 1 000 that the community launched and then a whole bunch of different
ecosystems have spun up around this um which is pretty fragmented and part of the reason is like
we didn't think it would grow this fast uh meant that we weren't quite ready to to do that but what
we are working on is an official mcp registry api this is a unified and hosted metadata service uh owned
by the mcp team itself but built in the the open um that that means the schema is in the open the actual
uh development of this is completely in the open uh but it lives on an api that we're owning just for
the sake of there being something posted and what it allows you to do is have this layer above the
various package systems that already exists where mcp servers already exist and are deployed these are
things like npm uh pi pi we've started to see other ones uh develop as well around java and rust and go
but the the point is a lot of the problems that we've been talking today talking about today like
uh you know how do you discover what the protocol for an mcp server is is it standard io is it sse
does it live locally on a file that i need to go and build and install or does it live at a url
uh who built it are they trusted uh was this verified by you know if shopify has an official mcp
server did shopify bless this server um and so a lot of these problems i think are going to be solved
with a registry um and we're going to work to make it as easy as possible for folks to port over the
entire ecosystem that already exists for mcp servers uh but the point is this is coming uh it's going
to be great and we're very excited about it because i think a huge problem right now is
discoverability and people don't know how to find mcp servers and people don't know how to publish them
uh and where to put them uh so we're very very excited about this and the last thing i'll touch
on is is versioning uh which a lot of people are asking about but you can imagine that this has its
own versioning where there's this log of hey what's changed between this and this like maybe the apis
themselves didn't change but i added a new tool or i added a new tool description or changed it
and this allows you to capture that within this central ecosystem or metadata service
when um soon i i it's under development we're actually working with uh block for example like
they're one of the open source folks that we work pretty closely with on mcp um but it's coming
uh there's a spec and i've i've read it yeah so question is can companies host their own registry
yeah we think of it i think kind of like artifactory where there's a public one there's there's an open
registry you can still obviously do your own um the nice artifact of this as well is uh there are
ecosystems like cursor or like yes code uh where you could hook into if you have an existing application
and marketplace that you work with you just hook into the api as like a second set of servers but
we are not going to align on what the ui for that necessarily looks like we're just providing the data
is there a path to uh putting something in the registry that's not even an fpm or five-time module
yes yeah that's a great point because yeah not all of these need to live on npm i think
uh yeah the answer is yes basically we can just let you put in a url as long as it's
like trusted and you provide more data oh sorry yeah
yeah
when you say execution um do you mean like how to actually surface these tools and
like let them be built or like can you say more about that
yeah i mean it could actually just be docker we like work really closely with docker themselves and
they have a an exact mirror of that repository the servers repo but it's all docker images and
they've done the whole build system uh so it literally could just be docker um there's also a
world where it's entirely remote servers like maybe you self-host and you don't want anyone
to deal with building uh so you just publish it at a url as well yeah
payments and permission batteries so i haven't thought about payments yet um it's not something
working about right now that's what i'm working about right now permission boundaries um what do
you mean by that does that mean like who gets to install or like look at one of these servers
yeah it's a good question i think we've touched on this a bit and this sounds a little bit separate from
the registry api or like maybe parallel um honestly i think best practices are still emerging but
that's the real answer like people are still figuring out the right way to do data governance around this
um so yeah i don't really have like a like the authoritative answer on this just yet
so
So I think our philosophy or like the principle, maybe generally about open source is we built
it, we launched it, and we want our products to be the world's best MCP clients, but they're
not going to be the world's only MCP clients.
And we are totally fine with that. We have and are talking to other foundational model
providers, can't comment on like who, but the point is, this is open and we intend for
it to be open. And if that creates more competition, that's probably good. And I think it's good
for users and it's good for developers. So I think there will be periods of time where
Claude and our first party services and APIs are the best. That might always not always
be the case. And I think that's, that's fine. And that's a good thing as well. But yeah,
we'll, we'll talk to other model companies if they're down.
There is no specific advantage relating to MCP that requires you to use Claude with MCP.
Claude is just better for many reasons, but like that's, I mean, it's true. But that's, that's more
about like Claude is just really good at tool use and agentic work. And that's not about
something fundamental with MCP itself, at least for now.
Um, I mentioned like, um, I'm just wondering if they have a
kind of
question. Yeah. Um, the question is like, how do we think about servers being more proactive or
initiating connections to the client? So, uh, there's a lot that we're thinking about here for
server initiated actions. So the simplest one that we can think about that already is supported is
server initiated notifications when a resource changes or the server is maintaining a file or a
log list and it wants to tell the client, Hey, I just made an update to this or a new, a new resources
available. Um, when it comes to sampling, there isn't something in the protocol just yet for the
server initiating sampling from scratch, um, where maybe it makes some decisions on its own and it's,
it reaches out. Uh, that's, that is something we're going to build, um, where the server will say,
Hey, I actually like completely unrelated, like you didn't ask me any questions, but I want to
start this interaction with you. Um, and it reaches out to the client and the client is ready to
receive those messages.
The, the server reaching out to the client would happen if like the system itself decides it needs
something like deterministically, maybe it not, not even predefined. It could be event driven. It could be
like, it just got a request from a user from some other system. It got an API request and it
initiates the client thing. Uh, also, if you think about composability, the server could in theory
also be a client and have its own LLM that it controls. Uh, so that's another reason why it could
initiate connections. Yeah.
Yeah.
Yeah.
So if you're working for both standardized out and SSC. Like, there will be, like,
a, like, like, running this API, like either you could have one that does standard out
all the API from your local server or interface that are on the SSC. Do you have any guidelines
yeah the question is guidelines uh between standard io and ssc the answer is like mcp is
transport agnostic um so the actual like behavior and the interactions between the client and server
don't matter about uh you know the fundamental nature of like the underlying transport that
being said the divide that we've seen so far is local or in memory communication happens over
standard io and remote is going to happen over ssc um and i think that's the pattern that makes
most sense but uh again it's it's transport agnostic if you want to build your own transports
uh and support them with mtp you can easily do that
uh
yeah question is does the the model have to be in the loop to interact with the server the answer is
no uh the server exposes a standard set of apis or um that's probably the wrong word to use but
a standard set of functions so call tools list tools call resources list resources um the the
client application can call those deterministically um
and just gives you a path and then the other
oh interesting are you talking about like server-to-server communication perhaps as well
uh the combination between the four tools you have to do them all
mm-hmm ics yes
i just want to say like well this is the
that we just want to have a fire so the second one is like okay give me the list
i don't think there's a built-in way in the protocol to do this today um a lot of the interactions do
have to go back to the client before it allows the tools to talk to each other and the main reason
for that is servers don't really know that other servers exist for the most part uh that being
said i'm pretty sure it's possible like it's pretty flexible so i think you could make that happen it's
just not like a first-class thing that we see work well i mean that fits quite well so um
mine is telling the model that comes to the call means of this idea of chasing
moving
uh
in
chases it sounds like this is a bet that like this function or like chases we must be both
but it has those you just mentioned um with like this code approach i generally call that
let's say you know very like um what's your opinion on that kind of fix how does this code
generation
to be quite honest i don't know or like i don't have a strong opinion on this um but yeah happy to chat
with you after if that makes sense okay i'm gonna keep going um i want to talk about real quick why
registry is is amazing um besides the reasons of ergonomics and verification all that stuff we've
talked about but for agents specifically an mcp server registry allows you to make agents self-evolving
what that means is you can dynamically discover new capabilities new data on the fly without
having to know anything about those from the time that that agent was initialized or programmed in
the first place so if you're a user and you have this general coding agent that knows exactly how
you work and knows the systems that you usually already have access to and it has a control flow
that really works well for you you say can you go check my grafana logs i think something's wrong
with them and can you go fix this bug let's say the agent wasn't programmed to know that the grafana
server existed so it's going to go talk to our registry it's going to do a search for an official
verified grafana server that has access to the right apis and then it's going to install or invoke
that server maybe it lives on uh remote over sse and then go and do the actual querying and go and
fix fix the bug um so this pretty simple example but the point is as barry mentioned in his talk a
couple days ago at this conference agents are going to become self-evolving by letting them discover and
choose their own tools and that makes that augmented llm system that we've talked about even more
powerful because you don't have to pre-package these you don't have to pre-define these the agent
itself will go out and look for them and make itself better it gives itself context um and i just
want to close that loop because i think that's going to be really powerful and i'm really excited for
that
my client now installed and essentially
uh
yes the question is how do you enforce control over arbitrary servers arbitrary access i think the
artifact or example is a really common one like you'll you can self-host registries and federate
which ones are approved or not approved you could also instead of using let's say if we had
a search api you could have a white list of specific servers and allow there to be a tool in between
where that agent has to go through that tool and the tool filters which servers it has access to
there's also the concept of verification like we'll in the registry figure out how to do this but
allowing there to be an official shopify server an official grafana server which of course helps with
this just a little bit but largely it will follow similar things to like artifactory and enterprise
tools as they exist today
this this is the future yeah not something that i currently am using but i don't think it's very far away
i trust agents to do it correctly from a functional perspective i don't trust
yet like the the servers themselves because there isn't like a great registry and all that kind of
stuff but the claw or the again models are good enough already at like deciding which tools to use
among hundreds uh so i do trust that part of it cool um we're getting close to time so i'm gonna keep
going there's another complement to server discovery uh that's different from a registry and that is
the concept of a dot well known um on the top here this is not a real url but let's say shopify had
a dot well known mcp dot json and that provided this this nice interface for you know here's shopify
we have an mcp endpoint that you should know about uh it has the resources and tools capabilities and you
auth with it uh using oauth 2. and what that means is if i'm a user and i talk to my agent and i say hey
help me go manage my store on shopify.com
and so this is a really nice complement to the registry where the registry is focused on discovery
and verification and the ability for people to find tools from scratch but if you also want
to go with top-down approach where you know you want to talk to shopify or you uh have an agent
that's going and looking around on the internet um it can go and check this uh dot well known as a
verified way of hey these tools do exist and uh this is how you use them and that's really powerful
and a specific thing that i'm particularly excited about is there's a really nice complement to
computer use anthropic release computer use model in october uh or just our regular model is a
computer use model and what it allows you to do is go and click around in these systems and these uis
that it's never seen before that don't have apis that it can go and interact with but what if you
could have that plus mcp.json there's a predefined way for that agent to go and call the apis that are
surfaced by shopify.com but for the long tail where that doesn't work it can use computer use it can
click around on the ui it can go log in it can go interact with buttons and i think the world where
those coexist inside one agent is the future and i think that's something we're thinking about
um i'm sure other people are thinking about it as well and i think mcp is going to be a big part of
that cool i'm going to keep going and i'll take questions at the end uh actually this is the last
slide but uh besides everything that we've talked about today uh there's a lot more things that we're
thinking about in the medium term um this is roughly in order of uh how much we're thinking
about it right now but uh there's a big discussion this is a bit granular about stateful versus
stateless connections right now mcp servers are somewhat stateful they hold state around the
connection between the server and client a lot of folks are interested in these more short-lived
connections where the client can disconnect from an mcp server go offline for a little bit come back later
and continue the conversation or the request uh in the same way without having to re-provide data
so we're working on this um idea around maybe that there's a bifurcation between the more basic
capabilities where it's the client asking the server for things versus uh capabilities where the server
is asking clients for things and i think this is going to be really elegant but you can imagine for
more advanced capabilities like sampling or server to client notifications they use something like sse
which requires there to be a long-lived connection but for short-lived things where it's just say hey
can you help me invoke this tool uh maybe that's a more short-lived like http or a regular request that
doesn't require a long-lived connection streaming big one we're thinking about is um how do we stream
data and actually have like chunk multiple chunks of data arrive at the client from the server over time
uh how to uh support that first class in the protocol name spacing uh which is also somewhat
relevant to agents and registries as we've been talking about but right now tools if you install
10 servers they have tools of the same name there is conflict uh often and like there isn't a great
way right now to separate that other than like appending the server plus the tool name before you
service it i think the registry is going to help a lot with this but we also want to uh kind of allow
this to exist first class in the protocol uh and maybe even allow people to create these like logical
groups of different tools that are pre-packaged into a really nice like package of finance tools that
are specific to these finance services that people care about uh and finally i think someone asked about
this over there but uh proactive server behavior uh elicitation where the server is either event
driven or has some kind of deterministic system where it decides it needs to go and ask the user for
more information or notify them about something uh we're just trying to figure out better patterns for
that existing in the protocol cool that's my talk uh my name is mehesh uh you can reach out
um linkedin i don't really use twitter but i felt compelled to put it on there i'm not going to
respond to you on twitter though um but yeah thanks so much for listening this is really great
