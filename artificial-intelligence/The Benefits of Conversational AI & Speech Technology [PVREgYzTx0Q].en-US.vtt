WEBVTT
Kind: captions
Language: en-US

00:00:30.040 --> 00:01:44.480
CRAIG
Why don't we go ahead,&nbsp;&nbsp;

00:01:44.480 --> 00:01:49.680
Trevor, and you can give us your background&nbsp;
and then we'll talk about speechmatics?

00:01:49.680 --> 00:01:52.480
TREVOR
Yeah, sure. So&nbsp;&nbsp;

00:01:53.040 --> 00:01:58.120
it's great to be speaking to you today. Thank you&nbsp;
so much for the time. I guess my background is,&nbsp;&nbsp;

00:01:59.120 --> 00:02:05.280
basically, I loved space as a kid. You know, I&nbsp;
was the guy with the space posters up all over&nbsp;&nbsp;

00:02:05.280 --> 00:02:10.440
my bedroom wall. So I ended up doing&nbsp;
a PhD in Computational Astrophysics,&nbsp;&nbsp;

00:02:10.440 --> 00:02:16.640
simulating the early universe. But as I got nearer&nbsp;
to the end of my PhD, I kind of realized that&nbsp;&nbsp;

00:02:16.640 --> 00:02:22.040
about five to ten people in the world would care&nbsp;
about what research I was doing. Because I was&nbsp;&nbsp;

00:02:22.040 --> 00:02:27.760
simulating the early universe, it's probably never&nbsp;
observable to prove the simulations I was running,&nbsp;&nbsp;

00:02:27.760 --> 00:02:34.300
let alone in my lifetime. So that's why I went&nbsp;
into AI looking for a bit more real-world impact.

00:02:34.300 --> 00:02:37.600
CRAIG
When was that? When did you make the switch?

00:02:37.600 --> 00:02:42.080
TREVOR
It was back in 2012. So I finished my PhD at&nbsp;&nbsp;

00:02:42.080 --> 00:02:50.800
the University of Edinburgh in 2012 and randomly&nbsp;
got introduced to the cofounders of DeepMind back&nbsp;&nbsp;

00:02:50.800 --> 00:02:56.720
when the company was about 15 people or so. And&nbsp;
I joined them as the first product manager there.

00:02:56.720 --> 00:02:57.320
CRAIG
Wow,&nbsp;&nbsp;

00:02:57.320 --> 00:03:04.040
that's wonderful. And Edinburgh, then you were&nbsp;
at Geoff Hinton's alma mater, is that right?

00:03:04.040 --> 00:03:06.640
TREVOR
So I was based up at the Royal Observatory.&nbsp;&nbsp;

00:03:07.720 --> 00:03:14.520
I had to cycle up the hill every day, which kept&nbsp;
me nice and fit. I like to say that I really&nbsp;&nbsp;

00:03:14.520 --> 00:03:20.240
should have befriended somebody in the computer&nbsp;
science department because having somebody that&nbsp;&nbsp;

00:03:20.240 --> 00:03:25.200
really knew how to code would have got my PhD&nbsp;
done a lot faster than me doing it by myself.

00:03:25.200 --> 00:03:29.440
CRAIG
Yeah. And Geoff, then that year–&nbsp;&nbsp;

00:03:31.040 --> 00:03:40.120
Ilya, and Alex Krizhevsky, were making the big&nbsp;
breakthrough. Were you aware of that at the time?

00:03:40.120 --> 00:03:45.560
TREVOR
I joined DeepMind around&nbsp;&nbsp;

00:03:46.080 --> 00:03:53.440
the 12th of April and I think it was literally&nbsp;
a week either side of that AlexNet came out on&nbsp;&nbsp;

00:03:53.440 --> 00:04:03.540
ImageNet and took the world by storm. So yeah, it&nbsp;
was good timing for me to jump into the AI domain.

00:04:03.540 --> 00:04:04.360
CRAIG
Yeah. And&nbsp;&nbsp;

00:04:04.360 --> 00:04:07.580
who did you meet a DeepMind that took you there?

00:04:07.580 --> 00:04:11.880
TREVOR
Yeah. So I initially met with Mustafa Suleyman,&nbsp;&nbsp;

00:04:11.880 --> 00:04:18.200
who was one of the co-founders there, and spent&nbsp;
some time with him and with a guy called Ben&nbsp;&nbsp;

00:04:18.200 --> 00:04:25.560
Coppin, who was the first, sort of, applied&nbsp;
AI person and engineering manager. And then,&nbsp;&nbsp;

00:04:25.560 --> 00:04:33.080
interviewed with Demis Hassabis, Shane Legg, and&nbsp;
a few of the other senior leaders there. Yeah,&nbsp;&nbsp;

00:04:33.080 --> 00:04:37.900
I was grilled for a few hours and&nbsp;
then an offer was made. It was great.

00:04:37.900 --> 00:04:41.160
CRAIG
Wow, that's really fantastic. I mean, what&nbsp;&nbsp;

00:04:41.160 --> 00:04:49.000
an incredible group of people. And I'm surprised&nbsp;
that you left. So how did you get to Speechmatics?

00:04:49.000 --> 00:04:55.000
TREVOR
Yeah, so I spent almost a decade at DeepMind,&nbsp;&nbsp;

00:04:55.000 --> 00:05:01.960
always on the applied side. So in the very early&nbsp;
days, we were looking for a variety of different&nbsp;&nbsp;

00:05:01.960 --> 00:05:08.360
applications. We ended up working on an iOS&nbsp;
game. As Demiss's background is heavily in games,&nbsp;&nbsp;

00:05:08.360 --> 00:05:12.560
I spent most of my interview with him talking&nbsp;
about Theme Park and how many hours of my youth&nbsp;&nbsp;

00:05:12.560 --> 00:05:20.400
I'd wasted on that. We were also working on image&nbsp;
recognition for fashion back in the day, looking&nbsp;&nbsp;

00:05:20.400 --> 00:05:28.840
at shape, pattern, color type aspects from the&nbsp;
image. And then when we were acquired by Google&nbsp;&nbsp;

00:05:28.840 --> 00:05:35.560
in 2014, I ended up working on a whole range of&nbsp;
applications including YouTube recommendations,&nbsp;&nbsp;

00:05:35.560 --> 00:05:46.880
Google Ads, self-driving car efforts, wearables,&nbsp;
activity recognition. Then around 2015, Deep Mind&nbsp;&nbsp;

00:05:46.880 --> 00:05:52.960
decided to start up its healthcare project.&nbsp;
So I helped sort of co-start that and spent,&nbsp;&nbsp;

00:05:52.960 --> 00:05:57.800
sort of, four or five years working on&nbsp;
a range of applications in healthcare,&nbsp;&nbsp;

00:05:57.800 --> 00:06:04.040
including in ophthalmology with Moorfields&nbsp;
Hospital. In fact, when I heard your podcast,&nbsp;&nbsp;

00:06:04.040 --> 00:06:13.240
Eye on AI, we had that same play in our project&nbsp;
there. But then I moved and transitioned to&nbsp;&nbsp;

00:06:13.240 --> 00:06:18.000
DeepMind Science and ended up working with&nbsp;
the AlphaFold team and exploring sort of the&nbsp;&nbsp;

00:06:18.000 --> 00:06:24.720
early commercialization efforts of that which&nbsp;
morphed into Isomorphic Labs. But essentially,&nbsp;&nbsp;

00:06:24.720 --> 00:06:32.920
I kind of felt like Isomorphic Labs was going to&nbsp;
be another quite research-focused effort. And I'd&nbsp;&nbsp;

00:06:32.920 --> 00:06:39.760
spent a lot of time working on incredible projects&nbsp;
with amazing algorithms, incredible people,&nbsp;&nbsp;

00:06:39.760 --> 00:06:47.040
just absolutely fantastic minds. But I always&nbsp;
felt like we had not quite made that leap from the&nbsp;&nbsp;

00:06:47.040 --> 00:06:54.400
research into having that real-world impact. I had&nbsp;
an opportunity to start my own startup so I left&nbsp;&nbsp;

00:06:54.400 --> 00:07:02.840
DeepMind to do that. I spent a couple of years&nbsp;
there working on applications in supply chain, in&nbsp;&nbsp;

00:07:02.840 --> 00:07:09.160
manufacturing, and in E-commerce was particularly&nbsp;
focused on the application of these algorithms to&nbsp;&nbsp;

00:07:09.160 --> 00:07:17.520
real-world problems. But then, the Speechmatics&nbsp;
offer came around and to me, speech is clearly&nbsp;&nbsp;

00:07:17.520 --> 00:07:25.560
going to be a core part of any future AGI stack.&nbsp;
Speechmatics has the best-in-class technology so&nbsp;&nbsp;

00:07:25.560 --> 00:07:31.840
there's a huge opportunity here for us to really&nbsp;
define, what does it mean to have speech in AGI.

00:07:31.840 --> 00:07:34.800
CRAIG
Yeah, that's fascinating.&nbsp;&nbsp;

00:07:37.160 --> 00:07:43.080
I've had an Oriol Vinyals on the podcast. If&nbsp;
you were working on AlphaFold, I'm sure you&nbsp;&nbsp;

00:07:43.080 --> 00:07:53.600
know him. Speechmatics, you are focused on speech&nbsp;
recognition. Is that right? Not speech generation?

00:07:53.600 --> 00:07:56.000
TREVOR
That's right. We&nbsp;&nbsp;

00:07:56.000 --> 00:08:04.760
have a mission to understand every voice. So the&nbsp;
research here is very focused on, how do you get&nbsp;&nbsp;

00:08:04.760 --> 00:08:15.680
the most out of audio coming in. And there is a&nbsp;
lot that goes into recognizing texts from speech.&nbsp;&nbsp;

00:08:15.680 --> 00:08:21.520
But obviously, there's, just a huge amount of&nbsp;
other information in that audio domain as well,&nbsp;&nbsp;

00:08:21.520 --> 00:08:26.280
as well as just the words that were said.&nbsp;
Equally, I think there's a fallacy that,&nbsp;&nbsp;

00:08:26.840 --> 00:08:30.440
speech solved. I like to say that there's two&nbsp;
people in the world, there's people that think&nbsp;&nbsp;

00:08:30.440 --> 00:08:38.240
that speech is solved and there's people that&nbsp;
have used Siri, Alexa, or Google Now. So we're&nbsp;&nbsp;

00:08:38.240 --> 00:08:47.680
still a long way from really being able to have a&nbsp;
speech Turing test, a way for us to have a Turing&nbsp;&nbsp;

00:08:47.680 --> 00:08:54.740
test via voices where we can't tell whether&nbsp;
it's a computer or a human on the other end.

00:08:54.740 --> 00:08:56.160
CRAIG
Listening,&nbsp;&nbsp;

00:08:56.160 --> 00:09:01.160
you mean, or a human on the other&nbsp;
end, or a computer speaking.

00:09:01.160 --> 00:09:03.040
TREVOR
Yeah

00:09:03.040 --> 00:09:05.480
CRAIG
You mentioned Siri,&nbsp;&nbsp;

00:09:05.480 --> 00:09:16.640
and I actually built an LLM-based app&nbsp;
for iOS and you're tied into the iOS&nbsp;&nbsp;

00:09:16.640 --> 00:09:26.040
speech-to-text and it's terrible– it's not&nbsp;
terrible but there’s very little nuance.

00:09:27.720 --> 00:09:29.640
TREVOR
So the&nbsp;&nbsp;

00:09:29.640 --> 00:09:37.680
built-in iOS dictation and a lot of the other&nbsp;
providers out there, everyone's generally good&nbsp;&nbsp;

00:09:37.680 --> 00:09:46.240
at recognizing voices like mine and yours. With a&nbsp;
clear English accent, in a nice quiet background,&nbsp;&nbsp;

00:09:46.240 --> 00:09:53.680
with not a lot of other speakers going on. Now,&nbsp;
we know that that's not the case for a huge amount&nbsp;&nbsp;

00:09:53.680 --> 00:10:00.920
of people in the world. There's 1000s of languages&nbsp;
spoken in the world. There's millions and billions&nbsp;&nbsp;

00:10:00.920 --> 00:10:07.920
of non-English speakers who don't have the same&nbsp;
experience through speech recognition. And then,&nbsp;&nbsp;

00:10:07.920 --> 00:10:12.960
you don't have to go further than the UK or the&nbsp;
US to realize there's a huge range of different&nbsp;&nbsp;

00:10:12.960 --> 00:10:20.800
accents, dialects, and ways of people speaking. So&nbsp;
really, it's about how do you get to the point for&nbsp;&nbsp;

00:10:20.800 --> 00:10:28.120
the speech recognition to be able to understand&nbsp;
every voice and not just the standard voice. And&nbsp;&nbsp;

00:10:28.120 --> 00:10:35.280
you also want to be able to do that in real-world&nbsp;
scenarios. So if you think about, as you're&nbsp;&nbsp;

00:10:35.280 --> 00:10:44.560
sitting in a cafe, if you're having a conversation&nbsp;
with people on WhatsApp or any text service while&nbsp;&nbsp;

00:10:44.560 --> 00:10:49.880
you're sitting in the cafe, it's send a note,&nbsp;
receive a note, send a note, receiver a note,&nbsp;&nbsp;

00:10:49.880 --> 00:10:57.720
it's a very structured conversation. So you can&nbsp;
imagine lots of AI tools being helpful in that&nbsp;&nbsp;

00:10:57.720 --> 00:11:02.920
scenario. But if you imagine then bringing your&nbsp;
friends into that cafe and having a conversation&nbsp;&nbsp;

00:11:02.920 --> 00:11:08.920
with them, it's much more dynamic, it's much more&nbsp;
fluid. You can imagine closing your eyes and still&nbsp;&nbsp;

00:11:08.920 --> 00:11:12.880
having the same conversation, recognizing&nbsp;
speakers from other tables but ignoring&nbsp;&nbsp;

00:11:12.880 --> 00:11:18.800
them. If you dropped Siri, Alexa, or a lot of&nbsp;
services into that it would just completely fall&nbsp;&nbsp;

00:11:18.800 --> 00:11:28.120
over and fail. So the challenge becomes, how do&nbsp;
you understand, recognize, and be responsive in&nbsp;&nbsp;

00:11:28.120 --> 00:11:35.760
an environment like that in a way that is human.&nbsp;
So for that you need to be able to, in my eyes,&nbsp;&nbsp;

00:11:35.760 --> 00:11:43.080
understand what was said, who said it, how it&nbsp;
was said. So we're still not there with all&nbsp;&nbsp;

00:11:43.080 --> 00:11:47.500
three of those things. So that's what we here&nbsp;
at Speechmatics are really focusing on doing.

00:11:47.500 --> 00:11:48.440
CRAIG
Yeah.&nbsp;&nbsp;

00:11:48.440 --> 00:11:54.440
And I'm really interested in the research&nbsp;
and science behind it but for application,&nbsp;&nbsp;

00:11:56.680 --> 00:12:03.880
where do you see the largest application?&nbsp;
Is it in transcription or dictation?

00:12:03.880 --> 00:12:10.640
TREVOR
So we already provide services to&nbsp;&nbsp;

00:12:10.640 --> 00:12:16.600
hundreds of different customers. So Speechmatics&nbsp;
has been going for just over a decade, although&nbsp;&nbsp;

00:12:16.600 --> 00:12:21.480
the founder, who was working on recurrent&nbsp;
neural networks back in the 1980s, for speech&nbsp;&nbsp;

00:12:21.480 --> 00:12:28.680
recognition, would say it's been going for a lot&nbsp;
longer. We've got hundreds of large enterprise&nbsp;&nbsp;

00:12:28.680 --> 00:12:35.360
customers that are transcribing millions of hours&nbsp;
every month. Speechmatics is a very horizontal&nbsp;&nbsp;

00:12:35.360 --> 00:12:41.520
offering, we go out to a lot of different&nbsp;
industries, a lot of different use cases. Some of&nbsp;&nbsp;

00:12:41.520 --> 00:12:50.040
our biggest industries include the media, things&nbsp;
like live captioning on sports games, those types&nbsp;&nbsp;

00:12:50.040 --> 00:12:56.880
of things. We've got a lot of customers in call&nbsp;
center applications, that side of things. We've&nbsp;&nbsp;

00:12:56.880 --> 00:13:05.280
got a lot of customers in edtech, making their&nbsp;
lectures and other things accessible for a broader&nbsp;&nbsp;

00:13:05.280 --> 00:13:11.960
audience. And then we've got government defense&nbsp;
and a lot of other areas that we can go into. So&nbsp;&nbsp;

00:13:11.960 --> 00:13:20.960
I think there's a huge number of use cases for&nbsp;
ASR. But I do think that as AI has progressed,&nbsp;&nbsp;

00:13:20.960 --> 00:13:27.600
and particularly the impact of large language&nbsp;
models, means that suddenly you can take a large&nbsp;&nbsp;

00:13:27.600 --> 00:13:35.440
amount of an unprocessed raw transcript in text&nbsp;
form and very quickly and easily pull insights,&nbsp;&nbsp;

00:13:35.440 --> 00:13:46.640
value, actionable information from these raw large&nbsp;
text files. So suddenly, there's this wealth of&nbsp;&nbsp;

00:13:46.640 --> 00:13:51.840
information that can come from turning your audio&nbsp;
into texts and then using large language models&nbsp;&nbsp;

00:13:51.840 --> 00:13:58.360
to do something valuable with it. So we're seeing&nbsp;
just a massive increase in the number of use cases&nbsp;&nbsp;

00:13:58.360 --> 00:14:05.300
and the number of customers that really want to&nbsp;
leverage ASR for a huge array of new domains.

00:14:05.300 --> 00:14:07.720
CRAIG
Yeah. So talk&nbsp;&nbsp;

00:14:07.720 --> 00:14:18.080
a little bit about the science behind that.&nbsp;
And what advances has Speechmatics made to&nbsp;&nbsp;

00:14:18.080 --> 00:14:26.220
make your system stronger, or superior,&nbsp;
whichever– superlative, you want to use?

00:14:26.220 --> 00:14:29.200
TREVOR
Yeah, so our favorite&nbsp;&nbsp;

00:14:29.200 --> 00:14:38.000
metric is really focused in on that high accuracy,&nbsp;
and more recently, low latency in particular as&nbsp;&nbsp;

00:14:38.000 --> 00:14:45.720
well. So the company has always built the system&nbsp;
to be focused, on how do we achieve the highest&nbsp;&nbsp;

00:14:45.720 --> 00:14:54.000
accuracy. And that is for our core service but&nbsp;
it's also for any additional new language that we&nbsp;&nbsp;

00:14:54.000 --> 00:15:01.480
build. So we offer across 50+ languages now but we&nbsp;
don't just release any language, we only release a&nbsp;&nbsp;

00:15:01.480 --> 00:15:08.120
language once we've got to a certain accuracy&nbsp;
that we're happy. Now, the way we get there&nbsp;&nbsp;

00:15:08.120 --> 00:15:15.160
is quite different from the way a lot of others&nbsp;
go. So a lot of our competitors build end-to-end&nbsp;&nbsp;

00:15:15.160 --> 00:15:21.920
models where they build a huge data set, pump&nbsp;
it through one large single end-to-end model,&nbsp;&nbsp;

00:15:21.920 --> 00:15:28.560
and get the output out. That enables them to&nbsp;
provide one single model that's multilingual but&nbsp;&nbsp;

00:15:28.560 --> 00:15:34.040
there's no free lunch in machine learning. So any&nbsp;
breath that you add there will take away from the&nbsp;&nbsp;

00:15:34.040 --> 00:15:41.760
depth that you get. So you see a decrease, a quite&nbsp;
rapid decrease, in accuracy as you get out to the&nbsp;&nbsp;

00:15:43.360 --> 00:15:48.880
lower range of languages where there's less data,&nbsp;
less representation of that language in the data&nbsp;&nbsp;

00:15:48.880 --> 00:16:00.800
set. What we do is, we utilize multiple techniques&nbsp;
to get the best out of both worlds. So we train a&nbsp;&nbsp;

00:16:00.800 --> 00:16:08.840
very large self-supervised, what we call body,&nbsp;
which is unlabeled data, millions of hours,&nbsp;&nbsp;

00:16:08.840 --> 00:16:16.880
it’s a bit like an end-to-end model but it's&nbsp;
the raw audio that goes in. A lot of the noises&nbsp;&nbsp;

00:16:16.880 --> 00:16:23.920
and sounds that we make when we speak is similar&nbsp;
across languages so it learns a representation of&nbsp;&nbsp;

00:16:23.920 --> 00:16:30.640
the audio data. We then have a separate body that&nbsp;
we call the acoustic body. That's where we train&nbsp;&nbsp;

00:16:30.640 --> 00:16:37.200
on labeled data, but it uses the self-supervised&nbsp;
learning body as a feed into it. So it uses the&nbsp;&nbsp;

00:16:37.200 --> 00:16:42.800
embeddings from self-supervised to help improve&nbsp;
it. This enables us to get very high accuracy on&nbsp;&nbsp;

00:16:42.800 --> 00:16:48.840
a low amount of labeled data, which is why we&nbsp;
get such high accuracy. It's because even in&nbsp;&nbsp;

00:16:48.840 --> 00:16:55.000
English where there is a large amount of labeled&nbsp;
data, it also means we get high accuracies on the&nbsp;&nbsp;

00:16:55.000 --> 00:17:00.680
less well-represented data in that. So we get&nbsp;
better at accents, we get better at dialects,&nbsp;&nbsp;

00:17:00.680 --> 00:17:06.440
we get better at localizations. Then for&nbsp;
languages where there's a lot less labeled&nbsp;&nbsp;

00:17:06.440 --> 00:17:14.400
data– labeling audio data is very expensive. It&nbsp;
means that we can get to very high accuracies on,&nbsp;&nbsp;

00:17:14.400 --> 00:17:20.500
like 100x less data than our competitors. So we&nbsp;
get that best of both worlds in our single model.

00:17:20.500 --> 00:17:24.880
CRAIG
Yeah. On languages with&nbsp;&nbsp;

00:17:24.880 --> 00:17:37.800
less data available, I've spoken to people that&nbsp;
work on indigenous languages, languages that are&nbsp;&nbsp;

00:17:37.800 --> 00:17:47.960
disappearing. Do you augment the supervised&nbsp;
learning, the label data for those cases?

00:17:47.960 --> 00:17:52.360
TREVOR
So we don't do&nbsp;&nbsp;

00:17:52.360 --> 00:18:02.280
indigenous languages at the moment. One of the&nbsp;
reasons is because when you use this low amount&nbsp;&nbsp;

00:18:02.280 --> 00:18:11.080
of labeled data, you want the data to then be&nbsp;
as accurately labeled as physically possible.&nbsp;&nbsp;

00:18:11.080 --> 00:18:20.960
So we use a lot of third-party providers to&nbsp;
provide very high-quality transcripts of the&nbsp;&nbsp;

00:18:20.960 --> 00:18:28.160
low amounts of data and that is a lot harder&nbsp;
to do for some of the less well-known. We've&nbsp;&nbsp;

00:18:28.160 --> 00:18:36.440
got out to about 50 languages and we're continuing&nbsp;
to try and expand that. We hope to get another 10,&nbsp;&nbsp;

00:18:36.440 --> 00:18:44.120
20 by the end of this year, for instance. But&nbsp;
we tend to still be focused on the languages&nbsp;&nbsp;

00:18:44.120 --> 00:18:50.080
where there is also some commercial value at&nbsp;
the moment. But our system could very easily&nbsp;&nbsp;

00:18:50.080 --> 00:18:57.140
be used to help save some of these languages if&nbsp;
the right data was available for us to train on.

00:18:57.140 --> 00:18:57.840
CRAIG
Yeah.&nbsp;&nbsp;

00:18:57.840 --> 00:19:07.360
And the unsupervised model, is it looking at&nbsp;
phonemes or different parts of speech? I've&nbsp;&nbsp;

00:19:07.360 --> 00:19:13.280
talked to people that are building&nbsp;
music generation software and they&nbsp;&nbsp;

00:19:13.280 --> 00:19:18.360
focus on individual notes of individual&nbsp;
instruments and then build up from there.

00:19:18.360 --> 00:19:20.760
TREVOR
That’s right. That&nbsp;&nbsp;

00:19:20.760 --> 00:19:27.840
is very much focused on the phoneme level of&nbsp;
understanding at the current time. So the team&nbsp;&nbsp;

00:19:27.840 --> 00:19:34.400
has worked on that since, I think 2018 was the&nbsp;
first time we started really playing around with&nbsp;&nbsp;

00:19:34.400 --> 00:19:42.160
transformers for doing that type of methodology.&nbsp;
So we built a lot of useful structures and ways&nbsp;&nbsp;

00:19:42.160 --> 00:19:48.160
of augmenting the data such that transformers&nbsp;
work really well on it. One of the things we&nbsp;&nbsp;

00:19:48.160 --> 00:19:56.040
are quite excited about for the future though,&nbsp;
is multiscale representation. So currently,&nbsp;&nbsp;

00:19:56.040 --> 00:20:06.600
we sort of break the data down into phoneme-type&nbsp;
temporal cuts. But obviously, what we're saying&nbsp;&nbsp;

00:20:07.400 --> 00:20:12.600
has some impact on the word level, in terms&nbsp;
of recognizing what words have been said,&nbsp;&nbsp;

00:20:12.600 --> 00:20:21.320
but it's also very useful to know longer&nbsp;
timescales, what's going on there. And&nbsp;&nbsp;

00:20:21.320 --> 00:20:26.280
this is essentially how large language models&nbsp;
have got so good as well. We want to explore&nbsp;&nbsp;

00:20:26.280 --> 00:20:31.800
those different timescales, get different&nbsp;
representations of those multiscale timescales,&nbsp;&nbsp;

00:20:31.800 --> 00:20:38.960
and see whether that will also greatly improve&nbsp;
not just the recognition of words from what&nbsp;&nbsp;

00:20:38.960 --> 00:20:46.320
was being said but also potentially understand&nbsp;
more of the other audio aspects like intonation,&nbsp;&nbsp;

00:20:46.320 --> 00:20:54.960
emotion, sentiment, or hopefully, the thing on&nbsp;
my bucket list for this is understanding sarcasm.

00:20:54.960 --> 00:20:57.720
CRAIG
That's interesting.&nbsp;&nbsp;

00:21:00.720 --> 00:21:08.120
On that note, you were referred to being the&nbsp;
speech component for AGI at some point. Is&nbsp;&nbsp;

00:21:08.120 --> 00:21:18.960
that the end goal: that the Speechmatics&nbsp;
system becomes so sensitive to things like&nbsp;&nbsp;

00:21:18.960 --> 00:21:28.520
sarcasm or emotion that it will plug into&nbsp;
whatever AGI system eventually emerges?

00:21:29.520 --> 00:21:33.320
TREVOR
So I think for the last decade, Speechmatics has&nbsp;&nbsp;

00:21:33.320 --> 00:21:41.400
been really focused in on, how do we understand&nbsp;
the most from that audio coming in, and we'll&nbsp;&nbsp;

00:21:41.400 --> 00:21:48.200
continue to do so. We've been playing around&nbsp;
with large language models to provide sentiment,&nbsp;&nbsp;

00:21:48.200 --> 00:21:56.000
topic detection, summarization, or chapterisation,&nbsp;
and things like this. But I think one of the most&nbsp;&nbsp;

00:21:56.000 --> 00:22:02.280
exciting use cases for large language models as&nbsp;
we've seen from the takeoff of ChatGPT, Claude,&nbsp;&nbsp;

00:22:02.280 --> 00:22:12.520
or others, is this chatbot-type persona. And also,&nbsp;
over the last couple of years, we've seen massive&nbsp;&nbsp;

00:22:12.520 --> 00:22:19.960
generational leaps in text-to-speech synthesis.&nbsp;
We've got to the point where we can now synthesize&nbsp;&nbsp;

00:22:19.960 --> 00:22:26.720
a lot of different voices, make them much more&nbsp;
realistic, not the monotone that we're used to&nbsp;&nbsp;

00:22:26.720 --> 00:22:35.320
with some of the more historical synthesis voices.&nbsp;
So we got to the point where you could argue that&nbsp;&nbsp;

00:22:35.320 --> 00:22:41.320
we passed the Turing test on a keyboard-based&nbsp;
system playing with a large language model. But&nbsp;&nbsp;

00:22:41.320 --> 00:22:50.240
what if we put audio in? So ASR, combined with an&nbsp;
LLM, combined with text-to-speech: you get this&nbsp;&nbsp;

00:22:50.240 --> 00:22:58.440
audio-in, audio-out type agent that could provide&nbsp;
you this seamless, responsive-type interaction.&nbsp;&nbsp;

00:22:59.560 --> 00:23:05.440
So here is Speechmatics we've been building&nbsp;
something that we've called Flow, which is&nbsp;&nbsp;

00:23:05.440 --> 00:23:14.040
this audio-in, audio-out conversational AI tool.&nbsp;
Now, a lot of people have plugged together ASR,&nbsp;&nbsp;

00:23:14.040 --> 00:23:21.920
LLM, and text-to-speech but what you find is those&nbsp;
systems are often quite stinted. It's often quite&nbsp;&nbsp;

00:23:21.920 --> 00:23:28.920
slow. If you've ever played with the voice aspect&nbsp;
of ChatGPT, it still takes a long time to respond,&nbsp;&nbsp;

00:23:28.920 --> 00:23:37.200
you're still having that WhatsApp-based structure&nbsp;
conversation where it's waiting for one to go and&nbsp;&nbsp;

00:23:37.200 --> 00:23:43.240
all that side of things. Where you want to get&nbsp;
to is much more of that– from the movie Her,&nbsp;&nbsp;

00:23:43.240 --> 00:23:48.560
that really seamless, flowing conversation&nbsp;
that you can have with another human. And&nbsp;&nbsp;

00:23:48.560 --> 00:23:56.160
that requires low latency responses, it requires&nbsp;
being able to interrupt, and it requires being&nbsp;&nbsp;

00:23:56.160 --> 00:24:04.160
able to understand not just text, but sneezes,&nbsp;
coughs, size, all that type of stuff. And it&nbsp;&nbsp;

00:24:04.160 --> 00:24:14.040
also requires understanding who is having the&nbsp;
conversation. So Flow uses our best-in-class ASR.&nbsp;&nbsp;

00:24:14.040 --> 00:24:20.680
It includes our diarization, which means that it&nbsp;
can recognize who it’s speaking to. And we built a&nbsp;&nbsp;

00:24:20.680 --> 00:24:26.560
conversational engine part that enables it to have&nbsp;
this very fast, rapid interaction with the large&nbsp;&nbsp;

00:24:26.560 --> 00:24:32.880
language model and text-to-speech component to&nbsp;
enable a truly responsive type of conversational&nbsp;&nbsp;

00:24:32.880 --> 00:24:39.400
assistant. So yeah, we're really excited to be&nbsp;
hopefully launching this on Wednesday this week,&nbsp;&nbsp;

00:24:39.400 --> 00:24:45.680
so in July, and bringing it to some of our&nbsp;
early adopters and customers very soon.

00:24:45.680 --> 00:24:51.880
CRAIG
And the&nbsp;&nbsp;

00:24:51.880 --> 00:25:00.080
knowledge base and the reasoner, do you&nbsp;
have your own LLM in there? I'm sorry if&nbsp;&nbsp;

00:25:00.080 --> 00:25:10.800
you just said that. Or, are you making an&nbsp;
API call to ChatGPT, Claude, or something?

00:26:06.080 --> 00:26:49.880
TREVOR
So we've built&nbsp;&nbsp;

00:26:49.880 --> 00:26:58.480
it to be LLM agnostic. I think what we've&nbsp;
seen in the LLM space is rapid development&nbsp;&nbsp;

00:26:58.480 --> 00:27:06.000
from a lot of different players. I imagine lots of&nbsp;
customers have a preference as to which one they&nbsp;&nbsp;

00:27:06.000 --> 00:27:15.080
would like to use. So what we've done is we've&nbsp;
brought Llama 3 into our service, we can self-host&nbsp;&nbsp;

00:27:15.080 --> 00:27:21.200
it ourselves. So that can adhere to any privacy,&nbsp;
security, compliance concerns that anybody may&nbsp;&nbsp;

00:27:21.200 --> 00:27:30.520
have. But you can easily plug that into GPT,&nbsp;
Claude, Cohere, or Gemini, whichever one you would&nbsp;&nbsp;

00:27:30.520 --> 00:27:38.560
prefer to use. We don't plan on doing our own&nbsp;
LLM. I think there's a lot of resources and other&nbsp;&nbsp;

00:27:38.560 --> 00:27:46.480
people building bigger and better LLMs. Our domain&nbsp;
expertise is really understanding that audio and&nbsp;&nbsp;

00:27:46.480 --> 00:27:52.040
that speech. And I think that's the opportunity&nbsp;
for Speechmatics: is that these LLMs are only&nbsp;&nbsp;

00:27:52.040 --> 00:27:57.560
going to be as good as what they can hear. We want&nbsp;
to make sure they can hear really, really well.

00:27:57.560 --> 00:28:02.280
CRAIG
Yeah. I want to talk more about Flow&nbsp;&nbsp;

00:28:02.280 --> 00:28:19.760
but on the research side, can you handle multiple&nbsp;
voices simultaneously and parse them? I had a guy&nbsp;&nbsp;

00:28:19.760 --> 00:28:29.480
from Google on, a year or so ago, who was working&nbsp;
on bird calls. They were working on– when you're&nbsp;&nbsp;

00:28:29.480 --> 00:28:42.800
listening to a cacophony of birds in the jungle&nbsp;
or deep forest, pulling apart that sound and&nbsp;&nbsp;

00:28:42.800 --> 00:28:52.360
identifying individual streams. I also am sort&nbsp;
of a backyard birdwatcher. I have the Cornell&nbsp;&nbsp;

00:28:52.360 --> 00:29:00.360
Ornithology Lab app; I don't know if you've&nbsp;
ever used it. It's really remarkable. If there&nbsp;&nbsp;

00:29:00.360 --> 00:29:10.680
are 10 birds, it will list all 10; so it's able to&nbsp;
differentiate. Can you do that with human speech?

00:29:10.680 --> 00:29:13.280
TREVOR
So I would say there's a&nbsp;&nbsp;

00:29:13.280 --> 00:29:22.480
spectrum. We definitely are able to understand&nbsp;
different voices. So if I got out Flow now,&nbsp;&nbsp;

00:29:22.480 --> 00:29:27.800
it would recognize my voice, it would recognize&nbsp;
your voice, it would be able to say who said what.&nbsp;&nbsp;

00:29:28.560 --> 00:29:33.800
And in a busy environment, it would still&nbsp;
recognize our voices despite an array of other&nbsp;&nbsp;

00:29:33.800 --> 00:29:40.800
people around us. And then there's– so you get&nbsp;
to the noisy environment and then there's really&nbsp;&nbsp;

00:29:40.800 --> 00:29:46.480
strong crosstalk. Maybe in some US political&nbsp;
situation where there's just lots of people&nbsp;&nbsp;

00:29:46.480 --> 00:29:53.240
shouting at each other, it's hard for a human to&nbsp;
recognize all of that going on. So that's still&nbsp;&nbsp;

00:29:53.240 --> 00:29:59.120
a research project for us to get to in terms&nbsp;
of, how do you truly pull apart the words that&nbsp;&nbsp;

00:29:59.120 --> 00:30:05.680
were said by all the different voices at that&nbsp;
one time. So we haven't got to that level of&nbsp;&nbsp;

00:30:05.680 --> 00:30:14.560
true pulling apart really deep crosstalk. But&nbsp;
in a call center, with an irate customer where&nbsp;&nbsp;

00:30:14.560 --> 00:30:20.480
they're over talking over the employee that's&nbsp;
trying to calm them down, we're able to do that.&nbsp;&nbsp;

00:30:22.520 --> 00:30:27.280
There's a research trajectory on that front.&nbsp;
But I would say that we have the best in class&nbsp;&nbsp;

00:30:28.120 --> 00:30:36.080
diarization for speech recognition. And I would&nbsp;
also argue that is a really critical step in&nbsp;&nbsp;

00:30:36.080 --> 00:30:42.720
having an assistant that can actually help you&nbsp;
in real-world scenarios. So if I was walking&nbsp;&nbsp;

00:30:42.720 --> 00:30:49.360
down the street talking to ChatGPT and somebody&nbsp;
else walked past, it would pick up what they were&nbsp;&nbsp;

00:30:49.360 --> 00:30:54.600
saying and it would think that that was what&nbsp;
I was saying as well, whereas with diarization&nbsp;&nbsp;

00:30:54.600 --> 00:31:01.120
it knows to listen to only me. You can also then&nbsp;
use that to set up things like, [only respond to&nbsp;&nbsp;

00:31:01.120 --> 00:31:05.840
me]. So you've got an assistant, it’s hearing the&nbsp;
whole conversation that's going on, but it'll only&nbsp;&nbsp;

00:31:05.840 --> 00:31:10.400
take an action or respond to a question if I say&nbsp;
it rather than if anybody else in the room says.

00:31:10.400 --> 00:31:12.560
CRAIG
Actually,&nbsp;&nbsp;

00:31:14.320 --> 00:31:27.880
I wear hearing aids. That's one of the challenges&nbsp;
of hearing aids; in a party or something you can't&nbsp;&nbsp;

00:31:27.880 --> 00:31:36.800
differentiate between voices very well. Are there&nbsp;
applications like that for hearing augmentation?

00:31:36.800 --> 00:31:40.640
TREVOR
I think that's a very good application&nbsp;&nbsp;

00:31:40.640 --> 00:31:47.480
for us to think about for the future. I know that&nbsp;
our founder Tony Robinson also wears a hearing&nbsp;&nbsp;

00:31:47.480 --> 00:31:58.560
aid so it's certainly a strong area of focus for&nbsp;
him. One of the opportunities here is, currently&nbsp;&nbsp;

00:31:58.560 --> 00:32:05.760
we built this first incarnation of Flow to be very&nbsp;
focused on the mobile device space. I think that's&nbsp;&nbsp;

00:32:05.760 --> 00:32:10.960
a great place for people to test it, explore&nbsp;
it, find what works for them. But I'd be really&nbsp;&nbsp;

00:32:10.960 --> 00:32:19.880
excited about thinking about putting it into&nbsp;
something with multi-directional microphones. So&nbsp;&nbsp;

00:32:19.880 --> 00:32:26.160
then you can start really getting an extra layer&nbsp;
of information out from the diarization, which I&nbsp;&nbsp;

00:32:26.160 --> 00:32:32.880
imagine will work very well in a hearing aid case,&nbsp;
where it would listen primarily to people that&nbsp;&nbsp;

00:32:32.880 --> 00:32:39.260
maybe you're looking at and maybe tone down some&nbsp;
of the stuff that is happening in your periphery.

00:32:39.260 --> 00:32:41.200
CRAIG
Yeah. So&nbsp;&nbsp;

00:32:41.200 --> 00:32:53.400
Flow is being released as an API or smartphone&nbsp;
app? How are people going to interact with it?

00:32:53.400 --> 00:32:58.560
TREVOR
So we're very similar to our ASR in that, we&nbsp;&nbsp;

00:32:58.560 --> 00:33:05.520
provide an API such that people can build speech&nbsp;
recognition into any product that they're building&nbsp;&nbsp;

00:33:05.520 --> 00:33:11.520
today. We have an online portal on our website,&nbsp;
that way people can go in and play with it and&nbsp;&nbsp;

00:33:11.520 --> 00:33:18.280
test it themselves but we don't do a vertically&nbsp;
integrated product ourselves. We instead enable&nbsp;&nbsp;

00:33:18.280 --> 00:33:24.440
others to build that into whatever products they&nbsp;
are taking to their customers. I think that that's&nbsp;&nbsp;

00:33:24.440 --> 00:33:33.160
ultimately what we want to do here; we want to&nbsp;
say, we want to enable any product, any technology&nbsp;&nbsp;

00:33:33.160 --> 00:33:40.720
to be seamlessly integrated with the thing that&nbsp;
comes most natural to us, like our voice. So,&nbsp;&nbsp;

00:33:40.720 --> 00:33:48.400
how do we enable any product to suddenly become&nbsp;
keyboard free, mouse free, touchpad free,&nbsp;&nbsp;

00:33:48.400 --> 00:33:55.480
and suddenly become usable using your voice?&nbsp;
I know that a lot of people don't use Siri&nbsp;&nbsp;

00:33:55.480 --> 00:34:01.400
because it's frustrating sometimes when it doesn't&nbsp;
recognize what you say. So it's kind of surprising&nbsp;&nbsp;

00:34:01.400 --> 00:34:08.160
that we don't have more products today that are&nbsp;
leveraging our voices. We think Flows are our&nbsp;&nbsp;

00:34:09.120 --> 00:34:14.360
inflection point where we get to the point where&nbsp;
technology can be used with our voices. So we're&nbsp;&nbsp;

00:34:14.360 --> 00:34:21.840
going to release Flow as a conversational AI API&nbsp;
so that anybody and everyone can integrate their&nbsp;&nbsp;

00:34:21.840 --> 00:34:29.360
products and enable their product to be interfaced&nbsp;
with through their customers' voices. Now, we're&nbsp;&nbsp;

00:34:29.360 --> 00:34:35.880
also going to release a, sort of, app version&nbsp;
of it. But that's mostly to sort of showcase the&nbsp;&nbsp;

00:34:35.880 --> 00:34:41.280
technology and enable people to understand what&nbsp;
is now possible, rather than having to plug and&nbsp;&nbsp;

00:34:41.280 --> 00:34:46.560
play an API just to be able to test it. I think&nbsp;
it's a little bit like, you know, OpenAI have&nbsp;&nbsp;

00:34:46.560 --> 00:34:52.200
released ChatGPT for everyone to use and play&nbsp;
with but ultimately, they're selling an API to&nbsp;&nbsp;

00:34:52.200 --> 00:34:57.240
enterprise customers to enable them to integrate&nbsp;
large language models into their products.

00:34:57.240 --> 00:34:57.760
CRAIG&nbsp;

00:34:57.760 --> 00:35:11.040
Yeah. And does iOS allow the microphone&nbsp;
to be taken over by another ASR?

00:35:11.040 --> 00:35:15.560
TREVOR
So you can build&nbsp;&nbsp;

00:35:15.560 --> 00:35:27.280
into an app a different ASR. You still have to use&nbsp;
the multiple microphones that are built in. But in&nbsp;&nbsp;

00:35:27.280 --> 00:35:35.800
an app setting, you're allowed to use different&nbsp;
things other than the iOS dictation. Obviously,&nbsp;&nbsp;

00:35:35.800 --> 00:35:42.560
other people would build on top of what we're&nbsp;
offering to provide actions. So if Apple were&nbsp;&nbsp;

00:35:42.560 --> 00:35:52.120
to use our service, the conversational AI service,&nbsp;
we would become the voice interface for Siri. Then&nbsp;&nbsp;

00:35:52.120 --> 00:35:57.800
Siri would still take all the action calls&nbsp;
to tell you, what is the weather today,&nbsp;&nbsp;

00:35:57.800 --> 00:36:01.840
set a timer for five minutes, and all the&nbsp;
standard things that you do with Siri.

00:36:01.840 --> 00:36:05.320
CRAIG
Yeah. Are you talking to Apple?

00:36:05.320 --> 00:36:07.040
TREVOR
Well,&nbsp;&nbsp;

00:36:07.040 --> 00:36:13.280
I think we should be. Hopefully after this&nbsp;
podcast they'll hear it speak and get in touch.

00:36:13.280 --> 00:36:17.480
CRAIG
Yeah. And since this is– I do&nbsp;&nbsp;

00:36:17.480 --> 00:36:27.560
video but this also goes out in audio format,&nbsp;
can you give us a demo while we're recording?

00:36:27.560 --> 00:36:30.360
TREVOR
Yeah. Hopefully,&nbsp;&nbsp;

00:36:30.360 --> 00:36:34.106
my team has finished doing a load balance test.

00:36:34.106 --> 00:36:36.720
CRAIG
Okay. If not that’s okay.

00:36:36.720 --> 00:36:38.760
TREVOR
I'm just getting&nbsp;&nbsp;

00:36:38.760 --> 00:36:47.987
it open here. So I will just press&nbsp;
play on here and we can get started.

00:36:47.987 --> 00:36:49.160
(Demo begins)
Hey, Humphrey,&nbsp;&nbsp;

00:36:49.160 --> 00:36:54.779
can you respond to me and only to me, please?

00:36:54.779 --> 00:36:56.320
HUMPHREY (Flow) 33:45
Oh, sorry about that. Yeah,&nbsp;&nbsp;

00:36:56.320 --> 00:37:01.160
no problem. I'll make sure to respond only&nbsp;
to you from now on. How's it going there?

00:37:01.160 --> 00:37:03.600
TREVOR
That's enough for now.

00:37:04.720 --> 00:37:09.800
What I've done here is I've put it into passive&nbsp;
mode. So it's still listening to what we're&nbsp;&nbsp;

00:37:09.800 --> 00:37:15.160
saying, it's still transcribing what's being said&nbsp;
but it will no longer respond to me. But you saw&nbsp;&nbsp;

00:37:15.160 --> 00:37:21.720
from that first interaction how quick and how&nbsp;
seamlessly responded to my first interaction?

00:37:21.720 --> 00:37:23.440
CRAIG
Yeah. And it&nbsp;&nbsp;

00:37:23.440 --> 00:37:29.420
won't respond if I say, hey Humphrey, how&nbsp;
are you today? It's not going to respond?

00:37:29.420 --> 00:37:32.880
TREVOR
Well, I think it may have woken&nbsp;&nbsp;

00:37:32.880 --> 00:37:43.945
up from that. But if I now said it strongly:&nbsp;
so hey Humphrey, are you listening to me?

00:37:43.945 --> 00:37:44.940
HUMPHREY (Flow)
Yeah, I'm listening.

00:37:44.940 --> 00:37:46.160
TREVOR
Great. So there&nbsp;&nbsp;

00:37:46.160 --> 00:37:53.400
are multiple speakers in this conversation&nbsp;
we're having, please can you only respond to&nbsp;&nbsp;

00:37:53.400 --> 00:37:57.440
my voice and not to the other person&nbsp;
in the conversation? Is that okay?

00:37:57.440 --> 00:37:58.640
HUMPHREY (Flow)&nbsp;

00:37:58.640 --> 00:38:01.600
Yeah, no problem. I'll only&nbsp;
respond to you from now on.

00:38:01.600 --> 00:38:03.320
CRAIG
Yeah. So hey,&nbsp;&nbsp;

00:38:03.320 --> 00:38:13.800
Humphrey. What's the weather&nbsp;
like in New York today?

00:38:13.800 --> 00:38:15.400
TREVOR
Could you&nbsp;&nbsp;

00:38:15.400 --> 00:38:21.360
potentially tell Humphrey a little bit more about&nbsp;
yourself and maybe a bit about this podcast?

00:38:21.360 --> 00:38:22.920
CRAIG
Yeah. So I'm&nbsp;&nbsp;

00:38:22.920 --> 00:38:30.240
a former New York Times correspondent. I got&nbsp;
interested in AI after meeting Geoff Hinton&nbsp;&nbsp;

00:38:30.240 --> 00:38:32.840
for a story that I wrote (coughs)– (Humphrey&nbsp;
initiates response)– and I, (coughs) I ended&nbsp;&nbsp;

00:38:32.840 --> 00:38:40.320
up starting the podcast so that I would have&nbsp;
the opportunity to speak to researchers without&nbsp;&nbsp;

00:38:40.320 --> 00:38:44.234
having to promise that I was going to write&nbsp;
an article. (Humphrey initiates response)–

00:38:44.234 --> 00:38:46.760
TREVOR
Okay, great.&nbsp;

00:38:46.760 --> 00:38:51.840
So Humphrey tried to interrupt you a couple&nbsp;
of times there. I think the cough may have&nbsp;&nbsp;

00:38:51.840 --> 00:38:59.505
set it off. But if I said, [Humphrey, who&nbsp;
is it that I'm speaking to at the moment?]

00:38:59.505 --> 00:39:01.600
HUMPHREY (Flow)
Ah, yeah. I'm Humphrey. Nice to meet you.

00:39:01.600 --> 00:39:06.465
TREVOR
And who else is in the conversation with us?

00:39:06.465 --> 00:39:07.760
HUMPHREY (Flow)
There's you and then&nbsp;&nbsp;

00:39:07.760 --> 00:39:12.580
there's two other speakers. Let's call them&nbsp;
the second speaker and the third speaker.

00:39:12.580 --> 00:39:17.120
TREVOR
That's great. And let me just pull up something.&nbsp;&nbsp;

00:39:17.680 --> 00:39:25.065
One of the speakers comes from a particular&nbsp;
journal. Could you tell me which journal that is?

00:39:25.065 --> 00:39:26.160
HUMPHREY (Flow)
The second speaker,&nbsp;&nbsp;

00:39:26.160 --> 00:39:28.780
I think is a former New York Times correspondent.

00:39:28.780 --> 00:39:32.000
TREVOR
So the system&nbsp;&nbsp;

00:39:32.000 --> 00:39:38.960
can remember what's been said and keep it back&nbsp;
and still assigns it to a speaker. So we could&nbsp;&nbsp;

00:39:38.960 --> 00:39:45.480
also do this where we tell it our names and then&nbsp;
it remembers that I'm Trevor and you're Craig,&nbsp;&nbsp;

00:39:45.480 --> 00:39:50.600
rather than speaker one and speaker&nbsp;
two. I think one of the potentially&nbsp;&nbsp;

00:39:50.600 --> 00:39:57.520
powerful demos I'd love to show you, if we&nbsp;
just have two minutes here is, there was a&nbsp;&nbsp;

00:39:57.520 --> 00:40:05.400
recent article about McDonald's using IBM to do&nbsp;
drive-thru recognition and the demo didn't go&nbsp;&nbsp;

00:40:05.400 --> 00:40:11.680
so well. So I haven't programmed this in any way&nbsp;
for this. I'm just going to prompt it to find my&nbsp;&nbsp;

00:40:11.680 --> 00:40:16.400
voice to be a McDonald's assistant and then we'll&nbsp;
see how it does. Is that alright if I try that?

00:40:16.400 --> 00:40:19.800
CRAIG
Yeah, sounds great.

00:40:19.800 --> 00:40:23.080
TREVOR
Hey Humphrey,&nbsp;&nbsp;

00:40:23.080 --> 00:40:27.320
I'd like you to pretend to be a McDonald's&nbsp;
drive-thru agent. Is that all right?

00:40:27.320 --> 00:40:29.080
HUMPHREY (Flow)&nbsp;

00:40:29.080 --> 00:40:34.040
Ah, yeah. That's alright mate. I'll do&nbsp;
my best to take your order and all that.&nbsp;

00:40:34.040 --> 00:40:37.000
Welcome to McDonald's. Can&nbsp;
I take your order today?

00:40:37.000 --> 00:40:40.040
TREVOR
Yeah. Could I get a Big Mac meal,&nbsp;&nbsp;

00:40:40.040 --> 00:40:51.905
large, with a Coke? Can I also get six chicken&nbsp;
nugs and can I also get a bacon McFlurry, please?

00:40:51.905 --> 00:40:53.720
HUMPHREY (Flow)
Okay. So to confirm, you'd like a&nbsp;&nbsp;

00:40:53.720 --> 00:40:59.280
Big Mac meal, large size, with a Coke, six-piece&nbsp;
Chicken McNuggets, and a bacon McFlurry, right?

00:40:59.280 --> 00:41:00.280
TREVOR
Yeah,&nbsp;&nbsp;

00:41:00.840 --> 00:41:05.825
that'd be great, please. Do you&nbsp;
definitely do bacon McFlurries?

00:41:05.825 --> 00:41:06.800
HUMPHREY (Flow)
Ah yeah, we do the&nbsp;&nbsp;

00:41:06.800 --> 00:41:11.120
bacon McFlurry. It's a popular one. I'll just&nbsp;
put that all together for you. Would you like&nbsp;&nbsp;

00:41:11.120 --> 00:41:14.720
to try our mobile payment and pickup&nbsp;
or would you like to pay the window?

00:41:14.720 --> 00:41:18.505
TREVOR
I’ll pay at the window. Thanks very much.

00:41:18.505 --> 00:41:19.600
HUMPHREY (Flow)
No problem at all.&nbsp;&nbsp;

00:41:19.600 --> 00:41:25.960
Your total comes out to be– let me just&nbsp;
check;It’ll be £11.99. I'll have everything&nbsp;&nbsp;

00:41:25.960 --> 00:41:29.620
ready for you at the next window. Please&nbsp;
drive up and we'll get you all fixed up.

00:41:29.620 --> 00:41:30.280
TREVOR
Thanks&nbsp;&nbsp;

00:41:30.280 --> 00:41:32.435
very much, Humphrey.
(Demo concludes)

00:41:32.435 --> 00:41:34.280
CRAIG
That's interesting.&nbsp;&nbsp;

00:41:34.280 --> 00:41:41.420
So a couple of questions, first, is&nbsp;
it only responding with the wake word?

00:41:41.420 --> 00:41:44.640
TREVOR
So you can set it up in multiple&nbsp;&nbsp;

00:41:44.640 --> 00:41:51.720
different ways. You can set it up to only respond&nbsp;
when you say the wake word, Humphrey, which isn't&nbsp;&nbsp;

00:41:51.720 --> 00:41:57.440
the persona that we've currently given it. You can&nbsp;
give it any persona you'd like or you can set it&nbsp;&nbsp;

00:41:57.440 --> 00:42:06.040
to a much more lively mode where it's always on,&nbsp;
always responding. Or, you can even set it to not&nbsp;&nbsp;

00:42:06.040 --> 00:42:13.080
have a wake word, but try and work out when is the&nbsp;
right time to engage in the conversation. This to&nbsp;&nbsp;

00:42:13.080 --> 00:42:17.840
me– that's where the power of large language&nbsp;
models really comes in. You suddenly have this&nbsp;&nbsp;

00:42:17.840 --> 00:42:22.920
generalizable tool that can try and work out, when&nbsp;
is the right time to interact in the conversation?

00:42:22.920 --> 00:42:25.440
CRAIG
Yeah. On that point,&nbsp;&nbsp;

00:42:26.600 --> 00:42:34.360
do you know the company Tenyx, T-e-n-y-x? I&nbsp;
had them on the podcast, but they're working,&nbsp;&nbsp;

00:42:34.360 --> 00:42:41.800
actually, with fast food places on this kind&nbsp;
of solution. One of the problems that they&nbsp;&nbsp;

00:42:41.800 --> 00:42:48.560
have focused on is endpoint prediction: when&nbsp;
does the model know that you finish speaking&nbsp;&nbsp;

00:42:48.560 --> 00:42:52.920
and that you're not just pausing? Is that&nbsp;
something that you guys have spent time on?

00:42:52.920 --> 00:42:56.840
TREVOR
Yeah, so when we did the first version of this,&nbsp;&nbsp;

00:42:57.720 --> 00:43:05.800
we tried to look into the endpoint prediction&nbsp;
in the ASR model. We already do that fairly&nbsp;&nbsp;

00:43:05.800 --> 00:43:12.360
well for including punctuation in our ASR.&nbsp;
But what we found was that when you get to&nbsp;&nbsp;

00:43:12.360 --> 00:43:20.960
the conversational AI element, it creates this&nbsp;
unnatural latency when you finish speaking. So&nbsp;&nbsp;

00:43:20.960 --> 00:43:26.920
what we actually found was that instead, if you're&nbsp;
finding the right way to put the information from&nbsp;&nbsp;

00:43:26.920 --> 00:43:32.880
the ASR into the LLM as quickly as possible&nbsp;
and then enabling a few different approaches,&nbsp;&nbsp;

00:43:32.880 --> 00:43:37.360
so understanding when is the right time&nbsp;
to respond, that gives you this much&nbsp;&nbsp;

00:43:37.360 --> 00:43:45.920
more seamless interaction. Then the other really&nbsp;
critical point is interruption. So with ChatGPT,&nbsp;&nbsp;

00:43:45.920 --> 00:43:50.440
if you're speaking to, when it's speaking back,&nbsp;
you can't interrupt. You have to touch the screen&nbsp;&nbsp;

00:43:50.440 --> 00:43:56.080
to interrupt it– sorry, I don't know if you&nbsp;
can hear the ice cream truck in the background.&nbsp;&nbsp;

00:43:58.520 --> 00:44:03.200
Whereas with ours, what we built in is this&nbsp;
ability to interrupt it with your voice.&nbsp;&nbsp;

00:44:03.200 --> 00:44:10.000
So if it does accidentally start before you&nbsp;
finished, if you're a slow talker or if you&nbsp;&nbsp;

00:44:10.000 --> 00:44:15.200
pause for intonation and it does try and respond,&nbsp;
if you carry on speaking, it will just naturally&nbsp;&nbsp;

00:44:15.200 --> 00:44:21.000
stop. It will continue to listen to the whole&nbsp;
conversation, and it will merge that whole section&nbsp;&nbsp;

00:44:21.000 --> 00:44:26.080
together. So it’ll understand that it's not a&nbsp;
completely different request between the break.

00:44:26.080 --> 00:44:27.240
CRAIG
Yeah.&nbsp;&nbsp;

00:44:27.240 --> 00:44:41.520
And this is kind of funny because as I mentioned,&nbsp;
I build something; it's also model agnostic. I&nbsp;&nbsp;

00:44:41.520 --> 00:44:49.200
drive a lot and I'm always curious about what's&nbsp;
around me. So I wanted something that would give&nbsp;&nbsp;

00:44:49.200 --> 00:44:57.280
me the history of the places I'm driving past.&nbsp;
It's got a wake word and, I mean, it's just&nbsp;&nbsp;

00:44:57.280 --> 00:45:08.640
for me right now. Right now it's using GPT-4o,&nbsp;
which hallucinates but for my purposes– you know&nbsp;&nbsp;

00:45:08.640 --> 00:45:17.640
when it’s hallucinating; it's pretty good.&nbsp;
Would this do this? If you asked Humphrey,&nbsp;&nbsp;

00:45:17.640 --> 00:45:23.640
what's the history of Chappaqua, New York,&nbsp;
for example, would it in its knowledge base,&nbsp;&nbsp;

00:45:24.560 --> 00:45:33.400
presuming the Llama 2’s training data&nbsp;
includes that, would it find that and respond?

00:45:33.400 --> 00:45:38.240
TREVOR
So I think some of the most fascinating examples&nbsp;&nbsp;

00:45:38.240 --> 00:45:43.240
of Speechmatics employees trying it have been&nbsp;
exactly the scenario you've been talking about,&nbsp;&nbsp;

00:45:43.240 --> 00:45:49.840
where people have been driving in the car, they're&nbsp;
bored of the radio, they just want to have a play&nbsp;&nbsp;

00:45:49.840 --> 00:45:54.320
around and they turn it on. And they end up having&nbsp;
a conversation about something that they're going&nbsp;&nbsp;

00:45:54.320 --> 00:46:01.280
past at the moment. So yeah, Llama 3 appears to&nbsp;
be pretty well tacked up to the history of the&nbsp;&nbsp;

00:46:01.280 --> 00:46:07.160
Cambridge area, which is where we're based, in&nbsp;
Cambridge and London. It does a pretty good job&nbsp;&nbsp;

00:46:07.160 --> 00:46:13.680
of that. Obviously, it has the same issues as&nbsp;
all large language models and sometimes if it&nbsp;&nbsp;

00:46:13.680 --> 00:46:19.680
doesn't know, it will just make something up. But&nbsp;
I've also had some fairly in-depth conversations&nbsp;&nbsp;

00:46:19.680 --> 00:46:29.360
with it about Monte Carlo methods, hidden Markov&nbsp;
models, and recurrent neural networks and it does&nbsp;&nbsp;

00:46:29.360 --> 00:46:35.800
a pretty good job. There's clearly a lot of texts&nbsp;
about that out on the internet at the moment. The&nbsp;&nbsp;

00:46:35.800 --> 00:46:43.240
great thing is that the voice it gives you back&nbsp;
makes it feel like an engaging experience. It&nbsp;&nbsp;

00:46:43.240 --> 00:46:50.120
makes it feel like you're not just reading a wall&nbsp;
of generated text but instead, you're having this&nbsp;&nbsp;

00:46:50.120 --> 00:46:56.640
deeper conversation and you're actually learning&nbsp;
through engagement and interaction rather than&nbsp;&nbsp;

00:46:57.600 --> 00:47:00.940
by having to scroll through a lot of bullet&nbsp;
points that come out to you on the screen.

00:47:00.940 --> 00:47:04.400
CRAIG
Yeah. So this is being&nbsp;&nbsp;

00:47:04.400 --> 00:47:11.800
released as API to be built into other products,&nbsp;
but you are releasing the app that you were just&nbsp;&nbsp;

00:47:11.800 --> 00:47:13.880
demonstrating?
 
TREVOR&nbsp;

00:47:13.880 --> 00:47:22.120
So in two days time we're having our launch&nbsp;
event. So we'll be doing a demo up on stage,&nbsp;&nbsp;

00:47:22.120 --> 00:47:29.440
in front of an audience of, I think it's 50,&nbsp;
60 people. I think it’s also going to join our&nbsp;&nbsp;

00:47:29.440 --> 00:47:36.920
panel. So if that goes well expect Flow to join&nbsp;
a few other panels as well; maybe in the future&nbsp;&nbsp;

00:47:36.920 --> 00:47:44.080
it could come on this podcast and you can do a&nbsp;
pod with them. Yeah, hopefully then launching&nbsp;&nbsp;

00:47:44.080 --> 00:47:49.000
live. If people want to find out more I'd highly&nbsp;
recommend them to go to speechmatics.com/flow.&nbsp;&nbsp;

00:47:52.160 --> 00:47:57.400
There you can sign up to our waitlist and we can&nbsp;
give you comps and information about how to trade&nbsp;&nbsp;

00:47:57.400 --> 00:48:02.520
out more. But we're very excited to think about&nbsp;
how to get this into the hands of more people&nbsp;&nbsp;

00:48:02.520 --> 00:48:08.260
so that even more interesting use cases than&nbsp;
I can dream of can come up and come to us.

00:48:08.260 --> 00:48:10.680
CRAIG
Yeah. I'm just curious,&nbsp;&nbsp;

00:48:10.680 --> 00:48:19.240
why not release it as a B2C app? You are in&nbsp;
effect but why not promoted as a B2C app?

00:48:19.240 --> 00:48:20.040
TREVOR
So I think there's a&nbsp;&nbsp;

00:48:20.040 --> 00:48:29.640
lot of interesting examples of exploring large&nbsp;
language models through this interaction but&nbsp;&nbsp;

00:48:29.640 --> 00:48:36.360
I think what people want the most is a problem&nbsp;
of there's, a use case of theirs to be solved.&nbsp;&nbsp;

00:48:37.120 --> 00:48:42.760
There's an awful lot of incredible products&nbsp;
already out there in the market that I think&nbsp;&nbsp;

00:48:42.760 --> 00:48:49.040
could be massively improved if we were able to&nbsp;
interact with them through our voice. I think&nbsp;&nbsp;

00:48:49.680 --> 00:48:58.120
we've probably reached peak chatbot-based apps on&nbsp;
our phones, we've all probably got a bit bored of&nbsp;&nbsp;

00:48:58.120 --> 00:49:02.920
having the same conversation with LLMs. I think&nbsp;
the most powerful use cases will come through&nbsp;&nbsp;

00:49:02.920 --> 00:49:08.160
when we integrate these with products that&nbsp;
we use every day, day in and day out but we&nbsp;&nbsp;

00:49:08.160 --> 00:49:12.560
no longer have to get out, type something in,&nbsp;
or touch the screen, and work our way through&nbsp;&nbsp;

00:49:12.560 --> 00:49:19.480
a UI but instead can just utilize natural&nbsp;
language to give our technology a command&nbsp;&nbsp;

00:49:19.480 --> 00:49:24.880
and it understands us, recognizes the context,&nbsp;
and takes an action based on what we've said.

00:49:24.880 --> 00:49:26.560
CRAIG
Yeah. Do you&nbsp;&nbsp;

00:49:26.560 --> 00:49:39.080
think that Siri working with OpenAI, I hope&nbsp;
eventually with you, that it will become the&nbsp;&nbsp;

00:49:39.080 --> 00:49:49.800
killer iPhone app that will take actions, answer&nbsp;
questions about history, and everything like that?

00:49:49.800 --> 00:49:51.720
TREVOR
I think the main&nbsp;&nbsp;

00:49:51.720 --> 00:49:59.960
limitation of Siri, aside from not being able to&nbsp;
understand every voice, sounding a bit robotic,&nbsp;&nbsp;

00:49:59.960 --> 00:50:06.720
was that it just how structured it was. So it&nbsp;
always felt limited in the way you could leverage&nbsp;&nbsp;

00:50:06.720 --> 00:50:12.120
it to take actions. So that's why everyone just&nbsp;
uses it to set a timer for all their chickens,&nbsp;&nbsp;

00:50:14.240 --> 00:50:19.880
because that's the useful use case. I think&nbsp;
large language models give you the ability&nbsp;&nbsp;

00:50:19.880 --> 00:50:27.400
to understand more complex natural language and&nbsp;
then understand what actions need to be taken.&nbsp;&nbsp;

00:50:28.040 --> 00:50:34.680
So I think Siri plus GPT is definitely a&nbsp;
massive step forward. But I would still&nbsp;&nbsp;

00:50:34.680 --> 00:50:41.680
sort of say that garbage in garbage out. You&nbsp;
really need to understand what is being said,&nbsp;&nbsp;

00:50:41.680 --> 00:50:46.520
who is it being said by, and how is it&nbsp;
being said to really be able to get to that&nbsp;&nbsp;

00:50:46.520 --> 00:50:54.720
killer app. Which is why I truly believe that&nbsp;
speech is a core part of any future AGI stack.

00:50:54.720 --> 00:50:56.640
CRAIG
On the&nbsp;&nbsp;

00:50:56.640 --> 00:51:03.040
various languages, what's the most obscure&nbsp;
language that you've trained it on so far?

00:51:03.040 --> 00:51:04.520
TREVOR
Oh, I don't know;&nbsp;&nbsp;

00:51:06.960 --> 00:51:15.480
there's a few I guess. I guess we're&nbsp;
going into Tagalog at the moment,&nbsp;&nbsp;

00:51:15.480 --> 00:51:21.720
I think is one of the ones we're exploring.&nbsp;
You've put me on the spot here. I can't think&nbsp;&nbsp;

00:51:21.720 --> 00:51:32.880
of which one is the most obscure. I’d probably be&nbsp;
offending somebody if I said too many languages.

00:51:32.880 --> 00:51:33.560
CRAIG
That's right.&nbsp;&nbsp;

00:51:33.560 --> 00:51:49.560
I guess obscure is the wrong word.&nbsp;
You do Icelandic, Latvian, Burmese…

00:51:49.560 --> 00:51:54.480
TREVOR
So we have just&nbsp;&nbsp;

00:51:54.480 --> 00:52:00.800
finished covering off all the languages that&nbsp;
are required for the EU. So we just finished&nbsp;&nbsp;

00:52:00.800 --> 00:52:08.120
Maltese and an improvement on Irish,&nbsp;
which means that we're covered for some&nbsp;&nbsp;

00:52:08.120 --> 00:52:15.920
EU regulation that's coming in very soon for&nbsp;
that. We've recently released Hebrew, Persian,&nbsp;&nbsp;

00:52:15.920 --> 00:52:22.960
and we've just done a big uplift on Arabic. One of&nbsp;
the things I would say with Arabic is obviously,&nbsp;&nbsp;

00:52:22.960 --> 00:52:28.160
there's a huge range of different languages&nbsp;
within Arabic. But what we've been able to&nbsp;&nbsp;

00:52:28.160 --> 00:52:36.360
do because of our combination of self-supervised&nbsp;
with the acoustic model with the label data, is we&nbsp;&nbsp;

00:52:36.360 --> 00:52:43.920
work a global Arabic model. So it can understand&nbsp;
all the different languages and dialects within&nbsp;&nbsp;

00:52:43.920 --> 00:52:51.640
Arabic rather than having to choose which one you&nbsp;
want to focus in on. We're particularly excited&nbsp;&nbsp;

00:52:51.640 --> 00:52:58.120
to be releasing those and watch this space open&nbsp;
more. You can always go on to the Speechmatics&nbsp;&nbsp;

00:52:58.120 --> 00:53:02.980
website and check out our list of 50 languages if&nbsp;
there's something you know you want to explore.

00:53:02.980 --> 00:53:07.560
CRAIG
Yeah. And that's speechmatics.com. Is that right?

00:53:07.560 --> 00:53:08.520
TREVOR
That's right.

00:53:08.520 --> 00:53:09.200
CRAIG
On the&nbsp;&nbsp;

00:53:12.480 --> 00:53:26.640
pulling apart voices in a crowd, where are&nbsp;
you on that? You mentioned national security&nbsp;&nbsp;

00:53:26.640 --> 00:53:33.040
applications; that seems like&nbsp;
it would be a huge benefit.

00:53:34.400 --> 00:53:39.160
TREVOR
So we offer diarization,&nbsp;&nbsp;

00:53:39.160 --> 00:53:47.360
which is the term for understanding different&nbsp;
people speaking, on top of our ASR as a standard&nbsp;&nbsp;

00:53:47.360 --> 00:53:54.160
feature for anyone to use. The scientific method&nbsp;
we use for doing that is a form of a clustering&nbsp;&nbsp;

00:53:54.160 --> 00:54:01.160
technique where we sample and we try and cluster&nbsp;
on some t-SNE plot, different distribution of the&nbsp;&nbsp;

00:54:01.160 --> 00:54:08.040
speakers. So if you have two different speakers&nbsp;
like in this conversation, then obviously, you&nbsp;&nbsp;

00:54:08.040 --> 00:54:14.160
get quite a clear separation as long as we don't&nbsp;
sound too similar. So it's then generally quite&nbsp;&nbsp;

00:54:14.160 --> 00:54:21.920
accurate in terms of recognizing which speaker is&nbsp;
the one that's been talking. As you increase the&nbsp;&nbsp;

00:54:21.920 --> 00:54:31.560
number of speakers, you get some more and more&nbsp;
overlay in the cluster space. So we go up to,&nbsp;&nbsp;

00:54:31.560 --> 00:54:36.080
you can do as many as you like, but I think&nbsp;
if you went more than 20 speakers you start&nbsp;&nbsp;

00:54:36.080 --> 00:54:41.600
to see quite a significant degradation in&nbsp;
terms of recognizing which speaker it is.&nbsp;&nbsp;

00:54:44.560 --> 00:54:50.880
It's a big part of our next research project&nbsp;
as well, alongside the multiscale: is that&nbsp;&nbsp;

00:54:50.880 --> 00:54:56.320
we're also exploring how to get ever-improving&nbsp;
diarization results for more and more speakers.

00:54:56.320 --> 00:55:00.440
CRAIG
Yeah. And how big is the team at Speechmatics?

00:55:01.760 --> 00:55:02.545
TREVOR
So the company&nbsp;&nbsp;

00:55:02.545 --> 00:55:08.040
has about 120 people. There's about 60&nbsp;
people on the engineering side and of them,&nbsp;&nbsp;

00:55:08.040 --> 00:55:15.960
there's probably 20 also machine learning&nbsp;
researchers and engineers. So as a company,&nbsp;&nbsp;

00:55:15.960 --> 00:55:24.000
we're quite sort of heavy on the research side.&nbsp;
I would consider us a bit of a bit of an applied&nbsp;&nbsp;

00:55:24.000 --> 00:55:29.360
research lab where we're always doing fundamental&nbsp;
research, exploring other fundamental research,&nbsp;&nbsp;

00:55:29.360 --> 00:55:33.840
and thinking about how do we bring this&nbsp;
into the speech domain. That's enabled&nbsp;&nbsp;

00:55:33.840 --> 00:55:43.600
us to provide multiple revolutions in ASR from&nbsp;
the early days, where everybody was using Kaldi,&nbsp;&nbsp;

00:55:43.600 --> 00:55:50.200
all the way through to using transformers today.&nbsp;
And I expect us to continue to explore and invest&nbsp;&nbsp;

00:55:50.200 --> 00:55:58.160
in ways that we bring some of the latest and&nbsp;
greatest AI breakthroughs into the speech domain.

