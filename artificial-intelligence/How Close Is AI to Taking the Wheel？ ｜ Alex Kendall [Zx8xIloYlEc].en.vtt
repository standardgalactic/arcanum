WEBVTT
Kind: captions
Language: en

00:00:00.070 --> 00:00:05.710
The world model for me is a model that can
understand the state of the world and predict

00:00:05.710 --> 00:00:09.390
how it's going to change, given an action
you put into it.

00:00:09.390 --> 00:00:13.740
So, mathematically speaking, it's a function
that takes your current state, your current

00:00:13.740 --> 00:00:15.549
action, and predicts the next state.

00:00:15.549 --> 00:00:16.549
Essentially a simulator.

00:00:16.549 --> 00:00:21.760
It's a model that can allow you to understand
how the world will evolve, given different

00:00:21.760 --> 00:00:24.789
things you might want to do or interact with
that world.

00:00:24.789 --> 00:00:30.000
The result of this is a world model that can
simulate the future and, if you want to, you

00:00:30.000 --> 00:00:35.000
can take that state and decode it back to
video so you can produce the video output

00:00:35.000 --> 00:00:36.390
of what's actually going to happen.

00:00:36.390 --> 00:00:37.390
But you might not want to.

00:00:37.390 --> 00:00:41.640
If you want to keep this real time and efficient,
you can just stay in your embedding space

00:00:41.640 --> 00:00:43.410
and use it to drive your car.

00:00:43.410 --> 00:00:50.080
Hi, I wanted to jump in and give a shout out
to our sponsor, netsuite by Oracle.

00:00:50.080 --> 00:00:56.530
I'm a journalist and getting a single source
of truth is nearly impossible.

00:00:56.530 --> 00:01:02.670
If you're a business owner, having a single
source of truth is critical to running your

00:01:02.670 --> 00:01:03.969
operations.

00:01:03.969 --> 00:01:11.570
If this is you, you should know these three
numbers 36,000, 25, 1.

00:01:11.570 --> 00:01:18.460
36,000 because that's the number of businesses
that have upgraded to NetSuite by Oracle.

00:01:18.460 --> 00:01:24.410
Netsuite is the number one cloud financial
system streamlining accounting, financial

00:01:24.410 --> 00:01:26.990
management, inventory, hr.

00:01:26.990 --> 00:01:28.290
And more.

00:01:28.290 --> 00:01:32.409
25 because NetSuite turns 25 this year.

00:01:32.409 --> 00:01:38.500
That's 25 years of helping businesses do more
with less, close their books in days, not

00:01:38.500 --> 00:01:41.409
weeks, and drive down costs.

00:01:41.409 --> 00:01:47.200
One because your business is one of a kind,
so you get a customized solution for all of

00:01:47.200 --> 00:01:52.619
your KPIs in one efficient system with one
source of truth.

00:01:52.619 --> 00:02:00.690
Change risk, get reliable, forecast and improve
margins Everything you need all in one place.

00:02:00.690 --> 00:02:07.649
As I said, I'm not the most organized person
in the world, and there's real power to having

00:02:07.649 --> 00:02:12.660
all of the information in one place to make
better decisions.

00:02:12.660 --> 00:02:20.000
This is an unprecedented offer by NetSuite
to make that possible Right now.

00:02:20.000 --> 00:02:25.900
Download NetSuite's popular KPI checklist,
designed to give you consistently excellent

00:02:25.900 --> 00:02:33.070
performance, absolutely free at netsuite.com.

00:02:33.070 --> 00:02:39.340
That's I on AI, e-y-e-o-n-a-i all run together.

00:02:39.340 --> 00:02:46.730
Go to netsuitecom I on AI to get your own
KPI checklist.

00:02:46.730 --> 00:02:52.610
They support us, so let's support them.

00:02:52.610 --> 00:02:56.459
I'm Craig Smith and this is I on AI.

00:02:56.459 --> 00:03:04.020
This week, I spoke with Alex Kendall, CEO
of Wave AI, to understand Wave's innovative

00:03:04.020 --> 00:03:09.970
approach to autonomous vehicles using a world
model called Gaia 1.

00:03:09.970 --> 00:03:16.000
Alex explains the advantages of world models,
which we've explored before on this podcast

00:03:16.000 --> 00:03:21.580
with Jan LeCun, and how they can be used in
AI agents.

00:03:21.580 --> 00:03:27.920
The discussion offers a unique view on the
progress, promise and obstacles in developing

00:03:27.920 --> 00:03:31.400
AI to act in the physical world.

00:03:31.400 --> 00:03:36.030
I hope you find the conversation as fascinating
as I did.

00:03:36.030 --> 00:03:37.879
I'm an engineer at heart.

00:03:37.879 --> 00:03:44.760
I've loved building things ever since I was
growing up and did so in the back garden,

00:03:44.760 --> 00:03:48.769
but got the chance to work on a bunch of robotics
in my childhood in New Zealand.

00:03:48.769 --> 00:03:52.989
Whether it's building drones to chase some
sheep around the field that I grew up in,

00:03:52.989 --> 00:03:59.409
or other projects I did at university, One
way or another they ended up taking me to

00:03:59.409 --> 00:04:00.920
the University of Cambridge.

00:04:00.920 --> 00:04:06.530
I spent many years there doing a PhD in research
fellowship in computer vision.

00:04:06.530 --> 00:04:11.420
I'm fortunate enough to publish some of the
first work that applied deep learning to scene

00:04:11.420 --> 00:04:20.880
understanding algorithms like semantic segmentation,
depth motion, and other forms of scene understanding.

00:04:20.880 --> 00:04:26.640
I think that work really inspired some of
the ideas that I've had to be able to build

00:04:26.640 --> 00:04:29.360
machines that can make decisions for themselves.

00:04:29.360 --> 00:04:35.740
Interestingly enough, a lot of my PhD work
stops short of understanding the future or

00:04:35.740 --> 00:04:40.980
doing future prediction, which is, I think,
one of the big topics we've been able to address

00:04:40.980 --> 00:04:45.700
with world models and Gaia, which I'm looking
forward to talking about.

00:04:45.700 --> 00:04:49.260
Yeah Well, that's fascinating.

00:04:49.260 --> 00:04:54.580
As I said, I just had Yann Lecun on the podcast
talking about world models.

00:04:54.580 --> 00:04:56.530
He mentioned Gaia.

00:04:56.530 --> 00:05:01.650
Where do I start?

00:05:01.650 --> 00:05:09.350
I'm interested in world models as an alternative
to large language models.

00:05:09.350 --> 00:05:13.900
Gaia won, your model.

00:05:13.900 --> 00:05:23.400
Yan says it's a little different from his
JEPA architecture that he's using to research

00:05:23.400 --> 00:05:24.590
world models.

00:05:24.590 --> 00:05:30.440
Can you tell us a little bit about Gaia winning?

00:05:30.440 --> 00:05:32.120
Then I have a lot of questions.

00:05:32.120 --> 00:05:42.740
I'm interested in marrying this tech with
robotics, because the big challenge in robotics,

00:05:42.740 --> 00:05:52.770
beyond the hardware, is building an AI brain
that can plan and make decisions and that

00:05:52.770 --> 00:05:54.310
sort of thing.

00:05:54.310 --> 00:06:01.470
There's a lot of talk right now about LLMs
being able to play that role, but I also think

00:06:01.470 --> 00:06:14.349
there are a lot of problems because of the
hallucinations or the fact that large language

00:06:14.349 --> 00:06:19.560
models don't have a very concrete underlying
model of the world.

00:06:19.560 --> 00:06:29.860
Why don't you talk about Gaia, how that came
about, what it is both generally, and then

00:06:29.860 --> 00:06:33.330
how it's built and will go from there?

00:06:33.330 --> 00:06:37.910
Well, taking a step back, maybe some background.

00:06:37.910 --> 00:06:45.229
So I lead Wave, an autonomous driving company,
and how? we've set off on a different path

00:06:45.229 --> 00:06:50.930
to build autonomous driving systems that have
the onboard intelligence to drive different

00:06:50.930 --> 00:06:54.139
vehicles in new places, including places they
haven't been to before, and understand the

00:06:54.139 --> 00:07:00.610
complexity and the long tail of situations
that you see on our roads.

00:07:00.610 --> 00:07:04.889
And this is a, you know, taking an AI approach
to autonomous driving is, you know, quite

00:07:04.889 --> 00:07:09.229
contrarian and different to how people usually
look at this problem.

00:07:09.229 --> 00:07:14.620
When we started six years ago, in 2017, we
set off to build an end-to-end neural net

00:07:14.620 --> 00:07:16.330
that could learn to drive.

00:07:16.330 --> 00:07:21.580
You know, take the data as input and output,
emotion plan to control a vehicle.

00:07:21.580 --> 00:07:27.210
And this end-to-end AI approach I guess you
know why a world model is interesting here.

00:07:27.210 --> 00:07:32.520
So, for me, it is a model that can understand
the state of the world and predict how it's

00:07:32.520 --> 00:07:36.910
going to change, given an action that you
know you put into it.

00:07:36.910 --> 00:07:40.970
So, mathematically speaking, it's a function
that takes your current state, your current

00:07:40.970 --> 00:07:43.400
action, and predicts the next state.

00:07:43.400 --> 00:07:47.000
So it's essentially a simulator.

00:07:47.000 --> 00:07:51.940
It's a model that can allow you to understand
how the world will evolve, given different

00:07:51.940 --> 00:07:55.069
things you might want to do or interact with
that world.

00:07:55.069 --> 00:07:56.979
Why is this important for self-driving?

00:07:56.979 --> 00:08:00.669
Well, the first thing you might look at when
you're building an end-to-end neural network.

00:08:00.669 --> 00:08:03.379
To drive a car is to build something that's
autoregressive.

00:08:03.379 --> 00:08:08.180
Build something that creates a function that
takes your input state and produces the motion

00:08:08.180 --> 00:08:09.330
plan that you should drive with.

00:08:09.330 --> 00:08:11.909
And that's kind of what people did in large
language models.

00:08:11.909 --> 00:08:14.669
And the problem with self-driving is it's
a safety-critical application.

00:08:14.669 --> 00:08:18.449
If you make the wrong decision, you're not
just going to put out some hallucinated text,

00:08:18.449 --> 00:08:23.400
but you know it's life and death decisions
of driving on our roads.

00:08:23.400 --> 00:08:27.099
So for that reason, it's really important
that you are aware of the implication of your

00:08:27.099 --> 00:08:31.400
decision and you can understand the dynamics
of the world.

00:08:31.400 --> 00:08:36.520
So that's really what motivated us to start
off with world models as a concept, and in

00:08:36.520 --> 00:08:42.750
2018, we actually published a blog with one
of the first examples of doing this On an

00:08:42.750 --> 00:08:43.750
autonomous vehicle.

00:08:43.750 --> 00:08:47.459
We published a model-based reinforcement learning
system where actually we're only on a quiet

00:08:47.459 --> 00:08:52.050
country road, but we learned to drive a car
with a world model.

00:08:52.050 --> 00:08:56.481
So this system had never driven an actual
car, it only learned in its imagination in

00:08:56.481 --> 00:08:57.481
a world model.

00:08:57.481 --> 00:09:01.630
But it used this model that had trained of
the dynamics of the world to be able to learn

00:09:01.630 --> 00:09:06.050
to operate this car and drive it down a quiet
country road and I guess over the last six

00:09:06.050 --> 00:09:10.160
years I can talk more about Gaia, but over
the last six years we have scaled that approach

00:09:10.160 --> 00:09:14.589
to the point it is today where, with the latest
and greatest in generative AI, we can now

00:09:14.589 --> 00:09:21.279
understand the full, diverse, rich, dynamic
urban scenes that we operate in today, like

00:09:21.279 --> 00:09:22.899
Central London.

00:09:22.899 --> 00:09:25.970
In building the world model.

00:09:25.970 --> 00:09:35.029
I mean, the autonomous driving systems to
date are helping me out here, but they're

00:09:35.029 --> 00:09:45.770
primarily reinforcement learning systems that
are taking in data from various sensors and

00:09:45.770 --> 00:09:54.010
have trained on what the best approach is
in any particular situation.

00:09:54.010 --> 00:10:00.370
Is that right, or what is the current state
of autonomous driving systems?

00:10:00.370 --> 00:10:05.720
Well, we're having an AI conversation and,
fundamentally, autonomous driving is a high

00:10:05.720 --> 00:10:06.720
problem.

00:10:06.720 --> 00:10:09.460
It's a problem of complex, high-dimensional
decision-making, and so you'd assume that

00:10:09.460 --> 00:10:14.339
you're going to use a data-driven method like
reinforcement learning to do it, but actually

00:10:14.339 --> 00:10:15.339
that's not the case.

00:10:15.339 --> 00:10:19.170
If you look at all of the large autonomous
driving efforts out there today, outside of

00:10:19.170 --> 00:10:22.899
Wave, primarily the approach is a traditional
robotics one.

00:10:22.899 --> 00:10:27.160
It's one of, yes, you use deep learning for
perception, but once you have the state of

00:10:27.160 --> 00:10:35.110
the world, it's very much a hand-coded optimization
approach to produce a motion plan that's aided

00:10:35.110 --> 00:10:40.540
by a set of infrastructure like HD map high-definition
maps that tell the car where and how to behave.

00:10:40.540 --> 00:10:47.370
So it's actually not an AI approach, and what
we've done is, I think, the first time an

00:10:47.370 --> 00:10:52.889
AI system has actually driven on the roads
at this level of scale, so that's not how

00:10:52.889 --> 00:10:56.260
the industry has traditionally approached
things.

00:10:56.260 --> 00:11:00.250
When you bring in an AI approach, of course,
the challenges there are how do you understand

00:11:00.250 --> 00:11:06.040
what it's doing, how do you make sure it's
safe and making the right decisions, and so

00:11:06.040 --> 00:11:11.260
that's brought up some of these challenges
that led us down the road of world models.

00:11:11.260 --> 00:11:12.560
So well, that's interesting.

00:11:12.560 --> 00:11:15.800
I didn't know that about autonomous driving
systems.

00:11:15.800 --> 00:11:23.649
So there they have all these sensors, that
data is coming into a central decision maker,

00:11:23.649 --> 00:11:30.350
and you're saying that decision maker is a
traditional control system and not a probabilistic

00:11:30.350 --> 00:11:31.709
AI system.

00:11:31.709 --> 00:11:33.730
Probably speaking, yes.

00:11:33.730 --> 00:11:39.459
A lot of the systems running around in San
Francisco today, for example, are of that

00:11:39.459 --> 00:11:40.459
approach.

00:11:40.459 --> 00:11:45.350
Now, more and more machine learning has been
used throughout over year on year in these

00:11:45.350 --> 00:11:46.350
systems.

00:11:46.350 --> 00:11:52.240
But it's not an end-to-end neural net, it's
not a large transformer that decides the whole

00:11:52.240 --> 00:11:56.050
decision making, and that's the step that
we've taken to replace that entire stack with

00:11:56.050 --> 00:12:02.490
one big neural network that learns how to
drive end-to-end.

00:12:02.490 --> 00:12:09.620
And I've seen a lot written recently about
using the reasoning power of large language

00:12:09.620 --> 00:12:22.261
models to play that role, to decide on actions,
and then with some other piece of software,

00:12:22.261 --> 00:12:28.639
to translate that action, to execute on that
plan.

00:12:28.639 --> 00:12:37.459
And can you talk about, well, first of all,
for Gaia, the world model.

00:12:37.459 --> 00:12:48.330
So there's a world model that's building a
state of the world in its I guess its weights,

00:12:48.330 --> 00:12:58.959
and then a reinforcement learning model that
learns to act on those on that state of the

00:12:58.959 --> 00:12:59.959
world.

00:12:59.959 --> 00:13:00.959
Is that right?

00:13:00.959 --> 00:13:03.070
Maybe you can describe the architecture a
little bit.

00:13:03.070 --> 00:13:04.070
Yeah.

00:13:04.070 --> 00:13:07.162
Let me jump into some of those details.

00:13:07.162 --> 00:13:11.740
We've got our research paper online that talks
about them in great depth.

00:13:11.740 --> 00:13:18.589
But one of the interesting things for me is
that if you look at the three big major trends

00:13:18.589 --> 00:13:23.040
that we've seen in large language models this
year I mean at the start of the year it was

00:13:23.040 --> 00:13:24.040
all about scale.

00:13:24.040 --> 00:13:28.160
Everyone was talking about how many parameters,
how much data, how much compute are these

00:13:28.160 --> 00:13:29.329
models trained on.

00:13:29.329 --> 00:13:32.760
In the middle of the year, it became about
multimodality.

00:13:32.760 --> 00:13:37.360
We pushed scale to some degree and now it's
about how do we understand across different

00:13:37.360 --> 00:13:43.470
modes, and a lot of image tech systems came
out, for example.

00:13:43.470 --> 00:13:46.920
And then, more recently, it's about synthetic
data.

00:13:46.920 --> 00:13:52.140
The benefits of synthetic data are clear:
you can control the bias in your training

00:13:52.140 --> 00:13:57.470
data, you can ensure that the training data
is equally sampled across the things you care

00:13:57.470 --> 00:14:03.720
about, or you can control the distribution
of your training data, and you can often get

00:14:03.720 --> 00:14:09.709
information that is harder to understand from
noisy real data alone.

00:14:09.709 --> 00:14:14.670
And those three trends that have really driven
the state of the art and say large language

00:14:14.670 --> 00:14:15.670
models.

00:14:15.670 --> 00:14:20.009
The interesting thing is we've seen the exact
same thing play out in robotics.

00:14:20.009 --> 00:14:25.350
So for us at Wave, we've been pushing the
scale of our neural network that drives the

00:14:25.350 --> 00:14:30.540
car, and in the next year our roadmap is going
to be pushing this in terms of parameters,

00:14:30.540 --> 00:14:35.660
data and compute by 100x further, two orders
of magnitude further, and so the results,

00:14:35.660 --> 00:14:37.900
the emergent behavior we're seeing, come out
as just remarkable.

00:14:37.900 --> 00:14:43.390
We believe the car to nudge its way through
crowds of pedestrians, to do complicated,

00:14:43.390 --> 00:14:47.670
unprotected turns, to predict the behavior
of other agents cutting in or moving around

00:14:47.670 --> 00:14:48.670
our vehicle.

00:14:48.670 --> 00:14:51.209
All of this kind of thing emerges at that
level of scale.

00:14:51.209 --> 00:14:56.870
The second trend on multi-modality that's
where I think it's really important to be

00:14:56.870 --> 00:15:01.620
able to learn to understand between different
modes, because ultimately, if you're training

00:15:01.620 --> 00:15:05.140
a self-driving car just off the video data
it has, it's going to be intelligent, but

00:15:05.140 --> 00:15:09.300
perhaps it will be more intelligent if it
doesn't only have that video data but also

00:15:09.300 --> 00:15:14.699
text and other information sources it has
when you and I learned to drive.

00:15:14.699 --> 00:15:20.660
I learned to drive when I was 16.

00:15:20.660 --> 00:15:25.430
And maybe my mom and dad probably had 20 or
30 hours in the car with me, maybe not that

00:15:25.430 --> 00:15:26.430
long.

00:15:26.430 --> 00:15:29.009
Maybe I learned something like five or 10
hours, I think.

00:15:29.009 --> 00:15:33.170
I was a fast learner but that kind of lengthened
time to learn how to drive.

00:15:33.170 --> 00:15:36.110
But it wasn't just that that allowed me to
drive.

00:15:36.110 --> 00:15:40.660
It was probably the 15 or 16 years of experience
I'd had of learning how the world works, learning

00:15:40.660 --> 00:15:44.350
what objects are, how things might behave
on the roads, and it was that observation

00:15:44.350 --> 00:15:49.800
that gave me the intelligence to drive, I
think that's seeing the same as true in robotics.

00:15:49.800 --> 00:15:54.360
We can train our autonomous driving system
now, not just on the video data of a driver

00:15:54.360 --> 00:15:58.430
but also internet video and text.

00:15:58.430 --> 00:16:03.042
We can literally feed it the PDF document
of the highway road code that the government

00:16:03.042 --> 00:16:06.550
writes and give it that as further context
to understand.

00:16:06.550 --> 00:16:11.990
And so I think multimodality is becoming really
important to bring together different sources

00:16:11.990 --> 00:16:15.000
of information and improve the intelligence
of your system.

00:16:15.000 --> 00:16:20.470
But then, secondly, with text specifically,
I think the future of how we interact with

00:16:20.470 --> 00:16:22.899
robots is going to be through language.

00:16:22.899 --> 00:16:26.230
We are going to be talking to our robots,
interacting with them.

00:16:26.230 --> 00:16:29.110
There's a reason why you and I have evolved
languages.

00:16:29.110 --> 00:16:32.880
The way we communicate is because it's the
most efficient way to get information across

00:16:32.880 --> 00:16:40.440
that we understand, and so, for that reason,
maybe not most efficient, but most natural

00:16:40.440 --> 00:16:44.149
way, and so, for that reason, I think the
accessibility of robotics will be greatly

00:16:44.149 --> 00:16:46.954
improved by us being able to just literally
conversely, you should be able to be in your

00:16:46.954 --> 00:16:51.370
self-driving car and say take the next left,
take the next right, drop me off here or I'm

00:16:51.370 --> 00:16:52.370
worried about this.

00:16:52.370 --> 00:16:53.370
Why are you doing that?

00:16:53.370 --> 00:16:55.699
And you should be able to build a sense of
trust through it.

00:16:55.699 --> 00:16:57.350
And we've done exactly that at Wave.

00:16:57.350 --> 00:17:02.290
We've produced a system called Lingo, which
is a first, a vision, language, action foundation

00:17:02.290 --> 00:17:08.250
model that combines those modalities of video,
of action and robotics and language that allows

00:17:08.250 --> 00:17:14.190
us to talk to our autonomous vehicle and ask
it why and what it's doing.

00:17:14.190 --> 00:17:15.720
And then, finally, the third trend, on synthetic
data.

00:17:15.720 --> 00:17:17.170
This is where Gaia comes in.

00:17:17.170 --> 00:17:19.400
Gaia is our world model.

00:17:19.400 --> 00:17:24.780
Not only is it a system that allows AI to
understand the implications of the decisions

00:17:24.780 --> 00:17:29.230
it's making, but also produces synthetic data.

00:17:29.230 --> 00:17:35.169
Generative AI is very good at recombining
data in new ways, and so we have lots of experiences

00:17:35.169 --> 00:17:36.549
of foggy scenes on the car.

00:17:36.549 --> 00:17:41.140
We have lots of experiences of jaywalking
scenarios, but we have very few foggy jaywalking

00:17:41.140 --> 00:17:42.140
scenarios.

00:17:42.140 --> 00:17:46.360
And Gaia can not only allow us to understand
how the world's going to evolve, but it can

00:17:46.360 --> 00:17:49.640
create new examples we haven't seen before.

00:17:49.640 --> 00:17:52.420
And again, we can do that by connecting vision,
language and action.

00:17:52.420 --> 00:17:55.370
We can prompt it and say, literally, give
it a prompt and say I want an example of a

00:17:55.370 --> 00:17:57.050
jaywalking pedestrian in the fog.

00:17:57.050 --> 00:18:04.390
Or we can take a scene that exists in the
real world and change it and ask it to recreate

00:18:04.390 --> 00:18:06.600
it with new features and things like that.

00:18:06.600 --> 00:18:10.549
I don't really get into the architecture of
Gaia and I'm happy to, but I just thought

00:18:10.549 --> 00:18:16.950
those three trends have been really powerful
for us in AI and exactly applicable to robotics

00:18:16.950 --> 00:18:18.620
as well.

00:18:18.620 --> 00:18:23.110
Yeah, so you can tell from my questioning.

00:18:23.110 --> 00:18:32.480
I'm a journalist, not a practitioner, so my
knowledge is fairly surface.

00:18:32.480 --> 00:18:36.840
But I understand large language models to
a degree.

00:18:36.840 --> 00:18:43.380
I understand the transformer algorithm and
the large language models are predicting the

00:18:43.380 --> 00:18:51.240
next token in a sequence and because of the
volume of training data it does a very credible

00:18:51.240 --> 00:18:56.380
job most of the time.

00:18:56.380 --> 00:19:06.409
I understand Jens' JEPA architecture, the
Joint Embedding Predictive Architecture, to

00:19:06.409 --> 00:19:07.799
a degree.

00:19:07.799 --> 00:19:13.530
He says your model is something different,
so can you kind of walk us through it at a

00:19:13.530 --> 00:19:23.250
very high level of how the model is trained
and what it's doing?

00:19:23.250 --> 00:19:28.980
It's predicting the next state of the world,
whether it's video or text or whatever.

00:19:28.980 --> 00:19:34.640
Is it doing that with a transformer algorithm?

00:19:34.640 --> 00:19:35.700
How is it doing it?

00:19:35.700 --> 00:19:42.630
I'm just sure the audience would also like
to hear.

00:19:42.630 --> 00:19:45.840
Absolutely so.

00:19:45.840 --> 00:19:49.909
I mean Jan and I share a lot of common beliefs
around these systems.

00:19:49.909 --> 00:19:55.450
We think that to go beyond the autoregressive
nature of large language models, of just predicting

00:19:55.450 --> 00:19:59.351
the next word, and getting to systems that
can understand and be safety critical, we

00:19:59.351 --> 00:20:00.820
need to have world models.

00:20:00.820 --> 00:20:02.650
We share the vision that these should be unsupervised.

00:20:02.650 --> 00:20:09.830
They should be able to be trained through
self supervision, through whether it signals

00:20:09.830 --> 00:20:13.840
contrast of learning or building energy spaces
or things like this.

00:20:13.840 --> 00:20:17.950
I think we share a lot in common there.

00:20:17.950 --> 00:20:23.950
I mean, jepa is a great architectural approach.

00:20:23.950 --> 00:20:32.110
I think there's a lot in common in these systems
in terms of there's some representation space.

00:20:32.110 --> 00:20:36.560
You want to train it with unsupervised learning,
and so for our approach, in particular with

00:20:36.560 --> 00:20:43.371
Gaia, what we first do is we tokenize up the
different inputs, whether it's images, action

00:20:43.371 --> 00:20:48.669
or language, and essentially, well, today
it's a large transformer, but you could use

00:20:48.669 --> 00:20:57.380
whatever your favorite flavor of neural network
or, let's say, machine learning system that

00:20:57.380 --> 00:21:02.080
you want to use, but essentially you take
those inputs and you learn this dynamics,

00:21:02.080 --> 00:21:07.539
this ability to take current state, current
action and predict next state, and then the

00:21:07.539 --> 00:21:12.990
result of this is a world model that can simulate
the future and, if you want to, you can take

00:21:12.990 --> 00:21:18.279
that state and decode it back to video so
you can produce the video output of what's

00:21:18.279 --> 00:21:19.279
actually going to happen.

00:21:19.279 --> 00:21:20.279
But you might not want to.

00:21:20.279 --> 00:21:24.270
If you want to keep this real, time and efficient,
you can just stay in your embedding space

00:21:24.270 --> 00:21:29.770
and use it to drive your car, and right now
we're working on Gaia 2 and Gaia Drive, and

00:21:29.770 --> 00:21:38.210
these are systems that very much are going
to see Gaia embedded in the vehicle, able

00:21:38.210 --> 00:21:42.890
to actually increase the intelligence, understanding
and improve the safety criticality of our

00:21:42.890 --> 00:21:45.460
system in a production setting.

00:21:45.460 --> 00:21:52.550
So that's really the guts of the architecture
Today it's a transformer that's able to predict

00:21:52.550 --> 00:21:54.410
future states.

00:21:54.410 --> 00:22:02.799
And again, forgive me, I'm sure you'll cringe
at my repeating this back to you in sort of

00:22:02.799 --> 00:22:14.029
super layman's language but you're encoding
the data coming in or tokenizing it and turning

00:22:14.029 --> 00:22:16.350
it into embeddings in.

00:22:16.350 --> 00:22:30.539
I guess you call it a feature space or embedding
dimension, and you're making predictions based

00:22:30.539 --> 00:22:32.970
on that at that level.

00:22:32.970 --> 00:22:48.340
So there and then to see the video, you decode
it into pixels but in that space you can predict

00:22:48.340 --> 00:22:49.340
the.

00:22:49.340 --> 00:22:50.340
Is it?

00:22:50.340 --> 00:22:55.250
How specific are those representations?

00:22:55.250 --> 00:22:59.730
When I said embedding space, I guess I meant
representation space.

00:22:59.730 --> 00:23:05.251
Yeah, One of the other big challenges of self-driving,
compared to large language models, is the

00:23:05.251 --> 00:23:07.400
amount of data you have as input is enormous.

00:23:07.400 --> 00:23:10.500
I mean, take our latest vehicle, for example.

00:23:10.500 --> 00:23:12.220
It's six or seven cameras.

00:23:12.220 --> 00:23:18.350
They have eight megapixels each, and then
you care about multiple frames over time and

00:23:18.350 --> 00:23:22.610
not to mention, like if you consider an imaging
radar as well or choose the sensors you want

00:23:22.610 --> 00:23:34.940
to use, just in cameras alone, the data eight
megapixels, your RGB values there, so 24 million

00:23:34.940 --> 00:23:36.340
bytes alone from those.

00:23:36.340 --> 00:23:42.650
Multiply that by the six cameras, then you've
got about 120 million bytes, and then multiply

00:23:42.650 --> 00:23:44.150
that by multiple time frames.

00:23:44.150 --> 00:23:49.670
I mean you're talking about gigabytes of data
there alone.

00:23:49.670 --> 00:23:58.030
And that data what it does is it makes it
you can't have an embedding space for your

00:23:58.030 --> 00:23:59.340
dealing with gigabytes of data.

00:23:59.340 --> 00:24:03.950
It's just too much to process, and so to make
these models practical, you need to be able

00:24:03.950 --> 00:24:08.120
to take that extraordinary amount of input,
data, all those videos, and compress them

00:24:08.120 --> 00:24:11.050
into an embedding space that you can reason
about.

00:24:11.050 --> 00:24:15.390
So I guess the question is how many factors
do you think you care about for a driving

00:24:15.390 --> 00:24:16.390
scene?

00:24:16.390 --> 00:24:17.390
You can list them out.

00:24:17.390 --> 00:24:21.159
You care about the positions of the cars in
front of you, the pedestrians, the cyclists,

00:24:21.159 --> 00:24:25.620
the direction they're facing, the way they're
going to move, the weather conditions, the

00:24:25.620 --> 00:24:27.580
traffic light, all these kinds of factors.

00:24:27.580 --> 00:24:30.320
Now, interestingly, if you go down the path
of trying to list them out by hand, you end

00:24:30.320 --> 00:24:35.669
up in the AV 1.0, the classical robotics approach
to autonomy, and that doesn't scale because

00:24:35.669 --> 00:24:40.460
it's very hard to enumerate all those factors
and to reason about them a priori.

00:24:40.460 --> 00:24:45.840
So we can do it as a thought exercise, but
I wouldn't advocate for that approach.

00:24:45.840 --> 00:24:49.730
But the point is that it's not hundreds and
millions of numbers.

00:24:49.730 --> 00:24:54.620
There's probably a much smaller set of things
that you care about there, and so we want

00:24:54.620 --> 00:24:58.620
to learn that you don't care about the pixels
in the sky, except you want to know the weather,

00:24:58.620 --> 00:25:00.700
but you don't really care about all those
things.

00:25:00.700 --> 00:25:03.710
There's a lot of redundant information in
this signal.

00:25:03.710 --> 00:25:05.850
Compare that to large language models.

00:25:05.850 --> 00:25:08.820
You have sentences of text that are really
high signal to noise ratio.

00:25:08.820 --> 00:25:13.760
Text is precise at saying this means that
and impacts this right.

00:25:13.760 --> 00:25:18.440
It's a direct description of what you care
about, compared to videos where most of the

00:25:18.440 --> 00:25:21.010
pixels in an image you don't care about are
Clouds.

00:25:21.010 --> 00:25:26.070
Second story of a building you don't care
about when you're driving.

00:25:26.070 --> 00:25:31.790
So the point is that you want to take this
data and embed it in a very efficient space,

00:25:31.790 --> 00:25:35.649
and the way we do that is through end to end
learning about what do we care about for driving,

00:25:35.649 --> 00:25:42.930
what actually is going to impact how the world's
going to evolve, and that's what we look to

00:25:42.930 --> 00:25:43.930
learn.

00:25:43.930 --> 00:25:47.290
So we look to build a transformer and a self
supervised learning approach that learns and

00:25:47.290 --> 00:25:51.221
embedding, that is really efficient, is as
small and compressed as possible, but has

00:25:51.221 --> 00:25:56.940
the information that we need to understand
the safety critical natures of the scenarios

00:25:56.940 --> 00:25:58.360
that we're driving through.

00:25:58.360 --> 00:26:05.649
So that's the primary task of that embedding
model and that learning model of the scene

00:26:05.649 --> 00:26:06.740
representation.

00:26:06.740 --> 00:26:16.380
Yeah, and then, right now that you know I've
seen some remarkable videos that you've done

00:26:16.380 --> 00:26:23.409
and maybe I can, you know, splice one into
this podcast.

00:26:23.409 --> 00:26:26.299
But they're awesome when you drive the car.

00:26:26.299 --> 00:26:31.940
I go out most weeks and when you see it learn
new behaviors week on week, like just yesterday

00:26:31.940 --> 00:26:36.169
I was when I went for a drive down a part
of London and Notting Hill and we went through

00:26:36.169 --> 00:26:37.940
Portobello Road Market.

00:26:37.940 --> 00:26:42.720
It's this crazy area with loads of pedestrians
all on the road.

00:26:42.720 --> 00:26:46.440
I've never seen our car sort of nudge its
way through a crowd of pedestrians but it

00:26:46.440 --> 00:26:51.309
did so really safely, in a way that you know
if you just stopped and waited for the pedestrians

00:26:51.309 --> 00:26:52.950
to clear, you'd be stuck there for an hour.

00:26:52.950 --> 00:26:55.539
So I have these kinds of new behaviors over
time.

00:26:55.539 --> 00:27:00.500
There's tons of videos that we have online
of this stuff, but it's pretty amazing seeing

00:27:00.500 --> 00:27:04.370
AI operate in the physical world like this.

00:27:04.370 --> 00:27:08.030
Yeah, the videos.

00:27:08.030 --> 00:27:20.010
So you're then decoding the representation
space into pixels in a video.

00:27:20.010 --> 00:27:26.770
That's useful, I presume, for creating training
data.

00:27:26.770 --> 00:27:36.630
But when you're actually driving in real time,
how is that state of the world being translated

00:27:36.630 --> 00:27:39.020
into action?

00:27:39.020 --> 00:27:47.370
And that's where I think you said there's
an RL engine or agent that's learning over

00:27:47.370 --> 00:27:49.650
time how to act on that.

00:27:49.650 --> 00:27:51.519
Can you talk about that part?

00:27:51.519 --> 00:27:52.519
Yeah, absolutely.

00:27:52.519 --> 00:27:59.720
I mean, one of the amazing things about Gaia
properties is that you can create photorealistic

00:27:59.720 --> 00:28:01.910
and diverse scenes that are controllable by
text.

00:28:01.910 --> 00:28:08.130
You can modify these environments and you
can also create multimodal futures.

00:28:08.130 --> 00:28:13.440
I think the really powerful thing is when
you only observe the past, you know you can't

00:28:13.440 --> 00:28:16.910
predict how the future is going to unfold
in a driving scene, and the fact that Gaia

00:28:16.910 --> 00:28:23.399
can generate diverse, multimodal, plausible
futures is a really important factor.

00:28:23.399 --> 00:28:30.220
But, yeah, in order to actually control the
car.

00:28:30.220 --> 00:28:36.510
So this is what we call Gaia Drive.

00:28:36.510 --> 00:28:41.779
It's not just decoding to images, but also
well, there's many ways that you can go about

00:28:41.779 --> 00:28:45.450
thinking about incorporating Gaia into a driving
system.

00:28:45.450 --> 00:28:51.700
For example, you could generate future data
and use that synthetic data to actually just

00:28:51.700 --> 00:28:52.950
train a system.

00:28:52.950 --> 00:28:57.669
You could use it to predict the future and
use the information it learns about predicting

00:28:57.669 --> 00:29:01.220
the future to improve the driving representation.

00:29:01.220 --> 00:29:03.230
Or you could actually bring it into full on.

00:29:03.230 --> 00:29:07.350
You know, model based reinforcement learning
or model predictive control or some kind of

00:29:07.350 --> 00:29:08.610
learning simulator.

00:29:08.610 --> 00:29:14.010
What that means is that, let's say you're
driving, you're at a green light and you want

00:29:14.010 --> 00:29:16.570
to decide whether you drive through the intersection.

00:29:16.570 --> 00:29:23.630
What the system can do is it can run its world
on a run Gaia for a few seconds ahead and

00:29:23.630 --> 00:29:24.630
see what might happen.

00:29:24.630 --> 00:29:28.309
Maybe it runs it a few times and sees how
various different things happen and then it

00:29:28.309 --> 00:29:32.590
can make a decision based on how it thinks
the future is going to play out.

00:29:32.590 --> 00:29:37.570
We do that in our brains and in our hippocampus
we have mechanisms that are, you know, most

00:29:37.570 --> 00:29:41.890
famously referred to as sort of thinking fast
and thinking slow.

00:29:41.890 --> 00:29:45.510
Thinking fast, the reactive decision making
you don't really plan ahead, you just do,

00:29:45.510 --> 00:29:51.059
whereas thinking slowly you take a step back
and use a reason for what might happen.

00:29:51.059 --> 00:29:52.059
Should I do this?

00:29:52.059 --> 00:29:53.059
And you sort of play chess.

00:29:53.059 --> 00:29:57.620
A few steps forward and we can do the same
thing in robotics.

00:29:57.620 --> 00:30:01.070
Robotics has typically had, you know, two
levels of control.

00:30:01.070 --> 00:30:06.840
There's a low level control that runs over
100 times a second that controls parts of

00:30:06.840 --> 00:30:10.240
a robot, and you have a high level control
that operates typically around 10 times a

00:30:10.240 --> 00:30:11.240
second.

00:30:11.240 --> 00:30:13.450
That is the high level reasoning.

00:30:13.450 --> 00:30:16.950
But actually what these kinds of models that
you do is maybe move one more level up, abstract

00:30:16.950 --> 00:30:19.210
and have a three tiered system.

00:30:19.210 --> 00:30:23.220
You can have a thinking slow thing which can
involve a large language model to interact

00:30:23.220 --> 00:30:27.130
and to reason and to plan could involve a
world model to actually understand the implications

00:30:27.130 --> 00:30:28.460
of these decisions.

00:30:28.460 --> 00:30:32.080
And that might happen at anything from one
hertz, you know, one time a second to maybe

00:30:32.080 --> 00:30:33.880
even one time every 10 seconds.

00:30:33.880 --> 00:30:35.340
It's quite an infrequent high level.

00:30:35.340 --> 00:30:40.740
If you think about when you're driving, that
sort of high level kind of topological task

00:30:40.740 --> 00:30:43.580
planning you do can happen at the highest
level.

00:30:43.580 --> 00:30:47.649
Then that middle tier, you know you're designing
the motion plan that you want to follow to

00:30:47.649 --> 00:30:50.230
ensure you don't hit things that you follow
problems.

00:30:50.230 --> 00:30:56.169
That's more reactive and it runs it at that
kind of 10 times a second and the 100 times

00:30:56.169 --> 00:31:01.539
a second is sort of the minute changes and
breaks and steering to make sure that you

00:31:01.539 --> 00:31:04.039
actually achieve that plan that you've set
up.

00:31:04.039 --> 00:31:10.799
And that three tier system I think is as I
think world models can play a really great

00:31:10.799 --> 00:31:17.279
role in that top one a new level of higher
level reasoning into robotics.

00:31:17.279 --> 00:31:26.789
And the advantages of that over agents built
purely off of large language models is that

00:31:26.789 --> 00:31:28.110
they're there.

00:31:28.110 --> 00:31:37.299
They require less computation, I would imagine,
but also 

00:31:37.299 --> 00:31:43.880
their decisions or their predictions are grounded
more firmly in reality, where, as a large

00:31:43.880 --> 00:31:54.770
language model, even if it's tokenizing pixels
in a video, it's only predicting one token

00:31:54.770 --> 00:31:56.289
ahead as it goes along.

00:31:56.289 --> 00:31:57.289
Is that right?

00:31:57.289 --> 00:32:01.080
Yeah, look, I think large language models
and world models can be complimentary, right.

00:32:01.080 --> 00:32:05.679
Like large language models give you the ability
to understand well, give you a text interface.

00:32:05.679 --> 00:32:09.929
They really interact through language, but
they give you the ability to learn a really

00:32:09.929 --> 00:32:15.190
incredible understanding of the world through
internet scale text and world models.

00:32:15.190 --> 00:32:18.659
The advantages of world models is they give
you the ability to understand the implication

00:32:18.659 --> 00:32:19.659
of your decision.

00:32:19.659 --> 00:32:25.150
Whether you are making a driving task decision,
whether you are outputting sentences and text,

00:32:25.150 --> 00:32:26.150
you know.

00:32:26.150 --> 00:32:31.110
Whatever it may be, it allows you to understand,
okay, what is the implication of that decision

00:32:31.110 --> 00:32:33.279
and the environment I'm operating in.

00:32:33.279 --> 00:32:35.169
Is that a good thing or not?

00:32:35.169 --> 00:32:39.490
And that understanding can help you do things
in a safety critical environment.

00:32:39.490 --> 00:32:44.000
So I think world models become really important
in an application like self driving, maybe

00:32:44.000 --> 00:32:48.360
not so much like an internet search problem,
but when you have safety critical applications

00:32:48.360 --> 00:32:53.830
whether that's in medical correspondence,
of language models or self driving for our

00:32:53.830 --> 00:32:58.290
application that's when world models give
you the ability to do that.

00:32:58.290 --> 00:33:02.220
The other advantage I described, though, is
not just at runtime, but also at training

00:33:02.220 --> 00:33:03.330
time.

00:33:03.330 --> 00:33:06.520
World models give you the ability to learn
more efficiently.

00:33:06.520 --> 00:33:10.269
We do this in our brains as well as people.

00:33:10.269 --> 00:33:12.139
We daydream or nightdream.

00:33:12.139 --> 00:33:18.019
When we dream, we actually go through a process
that solidifies our actions and lets us replay

00:33:18.019 --> 00:33:20.620
experiences to learn to do them better next
time.

00:33:20.620 --> 00:33:27.120
Yeah, like if you're learning to play tennis
and you hit a ball once you know you're not

00:33:27.120 --> 00:33:31.720
going to hit a ball with every single permutation
of the angle that your racket might be, to

00:33:31.720 --> 00:33:32.720
learn how it might go.

00:33:32.720 --> 00:33:36.380
You might only hit it a few times, but from
that you need to learn the general way of

00:33:36.380 --> 00:33:40.700
how to hit a ball, to get it in the right
part of the court to be able to play tennis

00:33:40.700 --> 00:33:43.279
and what we do is from the ways that we hit
the ball.

00:33:43.279 --> 00:33:49.710
We actually, you know, replay this in our
internal world model many times when we daydream

00:33:49.710 --> 00:33:52.789
to be able to learn how to do that more effectively.

00:33:52.789 --> 00:33:55.470
And the same is true with machine learning.

00:33:55.470 --> 00:33:58.770
When you have a world model, you can get more
out of your training data.

00:33:58.770 --> 00:34:05.380
You can replay, recombine, reconfigure and
use that to understand your training data

00:34:05.380 --> 00:34:13.669
and learn a much more performant, effective
or safe policy or decision making system from

00:34:13.669 --> 00:34:17.840
your training data because of the fact that
you can learn and recombine those experiences

00:34:17.840 --> 00:34:18.840
in new ways.

00:34:18.840 --> 00:34:24.970
So world models are really powerful with training
and inference or testing time.

00:34:24.970 --> 00:34:36.669
So and you're, you have this system, not only
creating training data, but acting as an agent

00:34:36.669 --> 00:34:38.230
to drive a car.

00:34:38.230 --> 00:34:41.980
How many, how developed is that system?

00:34:41.980 --> 00:34:49.940
How far is it from commercialization, for
example, how many cars do you have it in and

00:34:49.940 --> 00:34:54.950
how many road miles have you logged, and that
sort of thing.

00:34:54.950 --> 00:34:55.981
Yeah, we've been.

00:34:55.981 --> 00:35:01.161
We've been spending the last six years building
a different approach to autonomy, and we were

00:35:01.161 --> 00:35:05.609
at today as we've been able to demonstrate
that it can do a lot of things that have been

00:35:05.609 --> 00:35:06.609
blocking the industry for many years.

00:35:06.609 --> 00:35:12.700
It can drive on the kind of equipment that's
on modern vehicles today: single, GPU, computer,

00:35:12.700 --> 00:35:15.100
surround cameras, maybe a forward facing radar.

00:35:15.100 --> 00:35:19.890
It can drive in different places never been
to before and it can drive different vehicle

00:35:19.890 --> 00:35:20.890
types.

00:35:20.890 --> 00:35:25.920
So we are very excited to be in the process
of commercializing this technology now, to

00:35:25.920 --> 00:35:33.040
see it deployed across the world's most innovative
fleets and vehicle manufacturers and to see

00:35:33.040 --> 00:35:42.869
this deployed in a way that can utilize value
quickly and accelerate the growth from assisted

00:35:42.869 --> 00:35:44.869
autonomy through to full autonomy.

00:35:44.869 --> 00:35:45.869
And so we are.

00:35:45.869 --> 00:35:48.660
You know we are in that process right now.

00:35:48.660 --> 00:35:54.869
So far, we've been excited to partner with
some of the UK's largest fleets like DPD,

00:35:54.869 --> 00:36:00.500
ASDA and Ocado Group and these are large fleets
that do things like grocery delivery here

00:36:00.500 --> 00:36:04.760
in the UK and those partnerships have been
wonderful.

00:36:04.760 --> 00:36:09.210
We've been delivering groceries throughout
this year with our partners, as, for example,

00:36:09.210 --> 00:36:14.160
in London, at showing some of the value that
this autonomy, this autonomy technology, can

00:36:14.160 --> 00:36:16.040
bring to society.

00:36:16.040 --> 00:36:17.660
Yeah, just on that.

00:36:17.660 --> 00:36:21.520
I imagine you still have a safety driver in
the car.

00:36:21.520 --> 00:36:23.589
How is the regulation?

00:36:23.589 --> 00:36:32.369
I don't want to get lost in that regulation
discussion, but how are you managing to operate?

00:36:32.369 --> 00:36:42.930
As you know, crews in California run into
all kinds of trouble, but how is the regulatory

00:36:42.930 --> 00:36:47.960
environment allowing you to operate fleets,
for example?

00:36:47.960 --> 00:36:51.329
Yeah, that's a really important question.

00:36:51.329 --> 00:36:56.050
So today we operate with safety operators,
but we have a two-pronged approach here.

00:36:56.050 --> 00:37:02.160
The first is that we want to see the ability
to build value and see deployment of the system

00:37:02.160 --> 00:37:09.580
still as a driver assistance system or with
safety operators, as you say and there's extraordinary

00:37:09.580 --> 00:37:14.190
value that can be brought there, whether it's
the helping, support, safety or improving

00:37:14.190 --> 00:37:15.710
the efficiency of operating vehicles.

00:37:15.710 --> 00:37:19.790
There's a big opportunity there that we can
see today through driver assistance.

00:37:19.790 --> 00:37:25.780
But then, on the other hand, of course, ultimately
we want to get to level four autonomous driving

00:37:25.780 --> 00:37:31.690
at scale and we've been really excited about
the work that we've done with regulators around

00:37:31.690 --> 00:37:36.160
the world, but most specifically here in the
UK.

00:37:36.160 --> 00:37:41.420
We've had a number of UK ministers for rides
to show them the technology firsthand.

00:37:41.420 --> 00:37:46.590
We've sponsored the UK's parliamentary working
group on autonomous driving technology and

00:37:46.590 --> 00:37:54.609
we've helped support bringing legislation
to be considered to make this technology legal.

00:37:54.609 --> 00:38:02.730
And we've been offered, and are working on,
a 1.9 million pound grant with the government

00:38:02.730 --> 00:38:08.109
to help understand and put forward a safety
framework for AI systems.

00:38:08.109 --> 00:38:11.599
There's many more activities that we've been
doing in the space, but we're deeply engaged

00:38:11.599 --> 00:38:18.270
with regulators and believe that empowering
regulators to understand AI technology and

00:38:18.270 --> 00:38:25.589
to yeah, empowering them to understand it
and to be thoughtful about how best to manage

00:38:25.589 --> 00:38:27.870
those risks is the best way forward.

00:38:27.870 --> 00:38:32.890
So we've really looked into our part on that
front and have been excited about the results

00:38:32.890 --> 00:38:35.780
of the traction I should say.

00:38:35.780 --> 00:38:39.000
Yeah, can you?

00:38:39.000 --> 00:38:48.290
I have questions about the compute intensity
of this and the amount of training data.

00:38:48.290 --> 00:38:58.920
But setting that aside for a minute, mercedes-benz
has, I think, a level four autonomy in Germany,

00:38:58.920 --> 00:39:12.290
if I'm not mistaken, and the regulations there
allow them to drive hands free on certain

00:39:12.290 --> 00:39:14.190
roads and that sort of thing.

00:39:14.190 --> 00:39:23.880
How is that system working and how does that
compare to a guy of one?

00:39:23.880 --> 00:39:29.570
And yeah, answer that first and then on.

00:39:29.570 --> 00:39:37.500
So my understanding is that I haven't seen
a level four system from Mercedes-Benz or

00:39:37.500 --> 00:39:44.790
other automotive manufacturers outside of
the trials we've seen in China or in some

00:39:44.790 --> 00:39:49.270
parts of the US with some of the technology
companies.

00:39:49.270 --> 00:39:55.599
I have seen a limited level three system,
which is where the vehicle does take control

00:39:55.599 --> 00:40:00.000
and liability of the vehicle for certain scenarios
and I believe Mercedes have a product that

00:40:00.000 --> 00:40:07.099
allows you to do this at low speeds on a highway
in rush hour traffic.

00:40:07.099 --> 00:40:14.869
But in general, the automotive industry is
very we get to see as an automotive industry.

00:40:14.869 --> 00:40:23.000
We get to see technology at scale which can
do productionize, which can do driving any

00:40:23.000 --> 00:40:29.000
vehicle anywhere, whether it's not just highway
at low speeds, but urban, suburban, different

00:40:29.000 --> 00:40:32.970
cities around the world, and that's very much
what we're interested in solving.

00:40:32.970 --> 00:40:39.950
We want to build an AI driver, an artificial
intelligence system that has the onboard intelligence

00:40:39.950 --> 00:40:45.599
to drive vehicles through all of the kinds
of scenarios that we expect in our daily lives

00:40:45.599 --> 00:40:49.140
and the commutes and the travel that we do.

00:40:49.140 --> 00:40:55.359
We want to build this kind of technology to
help assist people, to free up their time,

00:40:55.359 --> 00:40:59.850
to make it safer on the roads, to give them
a more sustainable drive.

00:40:59.850 --> 00:41:01.700
All of these kinds of benefits.

00:41:01.700 --> 00:41:06.650
We want to ensure this technology can be brought
out quickly and broadly.

00:41:06.650 --> 00:41:17.310
We think that this is a unique opportunity
in space and something that of course the

00:41:17.310 --> 00:41:18.310
world.

00:41:18.310 --> 00:41:24.640
Yeah, I think it's a really important next
evolution that we need to be able to deliver,

00:41:24.640 --> 00:41:30.190
to lift up, quite frankly, the safety and
performance of all of the cars that we're

00:41:30.190 --> 00:41:31.890
putting on the road today.

00:41:31.890 --> 00:41:34.230
Yeah, and for training this system.

00:41:34.230 --> 00:41:44.400
I mean, one of the challenges, as I understand
it, of other autonomous vehicle control systems

00:41:44.400 --> 00:41:51.460
is they depend very heavily on supervised
learning and you have to label enormous amounts

00:41:51.460 --> 00:42:02.930
of data so that the system recognizes corner
cases and there's this very long tail of those

00:42:02.930 --> 00:42:05.010
cases.

00:42:05.010 --> 00:42:18.150
And, for example, you can train a car to drive
in California, but if you take it to Norway

00:42:18.150 --> 00:42:30.520
or something with a vastly different climate,
it's going to run into trouble if the snow

00:42:30.520 --> 00:42:36.859
hasn't been labeled into the system and that
sort of thing.

00:42:36.859 --> 00:42:38.369
How much?

00:42:38.369 --> 00:42:50.119
How do you train Gaia and can it generalize
across environments in real time, or do you

00:42:50.119 --> 00:42:57.619
need to train it in advance for every kind
of environment?

00:42:57.619 --> 00:43:01.710
The interesting thing is that when you are
driving, you're generalizing all the time.

00:43:01.710 --> 00:43:04.990
You will never see the same thing twice on
the road.

00:43:04.990 --> 00:43:09.470
Every time you go driving, the weather is
going to be slightly different than you've

00:43:09.470 --> 00:43:10.470
ever seen before.

00:43:10.470 --> 00:43:13.990
You can have cars and other agents around
you in different locations.

00:43:13.990 --> 00:43:20.550
So even if you're on the same road that you've
driven you're commuted every day of your life

00:43:20.550 --> 00:43:24.990
that the specific things that you go through
will be different in some way from what you've

00:43:24.990 --> 00:43:26.050
seen before.

00:43:26.050 --> 00:43:30.170
So I guess the first point to make is that
autonomous driving is all about being able

00:43:30.170 --> 00:43:31.170
to generalize.

00:43:31.170 --> 00:43:33.861
We do it every time we operate these vehicles
on the road.

00:43:33.861 --> 00:43:35.880
The question is how far can you generalize?

00:43:35.880 --> 00:43:38.360
It's one thing to generalize driving on the
same road every day.

00:43:38.360 --> 00:43:42.670
It's another to drive on one road in the UK
and then be able to drive in the US, where

00:43:42.670 --> 00:43:49.869
you have new things like four-way stop signs,
right turn at red, other local driving cultures,

00:43:49.869 --> 00:43:53.869
and so we want to build a system that can
effortlessly generalize to new environments

00:43:53.869 --> 00:43:58.210
to allow us to bring it to everyone around
the world.

00:43:58.210 --> 00:44:02.000
And how we've done that is that our system
is trained through unsupervised learning,

00:44:02.000 --> 00:44:07.400
so it doesn't need boxes drawn around objects
or things labeled in the scene for us to learn

00:44:07.400 --> 00:44:08.400
how to understand the scene.

00:44:08.400 --> 00:44:09.400
It's all unsupervised.

00:44:09.400 --> 00:44:13.511
We watch driving data and learn from that
how the world works, learn how to predict

00:44:13.511 --> 00:44:17.849
the future, train models like Gaia and these
kinds of things.

00:44:17.849 --> 00:44:23.319
So it's all through unsupervised learning,
and the key thing is that it becomes more

00:44:23.319 --> 00:44:25.560
efficient the more data you get.

00:44:25.560 --> 00:44:31.030
So we might need a certain amount of data
to train in the UK, for us to train a system

00:44:31.030 --> 00:44:34.920
and for us to generalize a system to another
environment like the US.

00:44:34.920 --> 00:44:39.770
We will need a fraction of that data because
a lot of the experience is shared.

00:44:39.770 --> 00:44:43.170
Right, the same rules of physics apply in
both countries.

00:44:43.170 --> 00:44:46.510
People tend to behave in a similar way, although
there are some differences.

00:44:46.510 --> 00:44:48.160
You need a little bit more data.

00:44:48.160 --> 00:44:54.329
We found that going from a passenger vehicle
to a van, for example, generalizing to a different

00:44:54.329 --> 00:45:03.240
vehicle, going from a Jaguar I-Pace passenger
vehicle to a 3.5 tonne delivery van, that

00:45:03.240 --> 00:45:08.440
took us about 2% to 3% of the training data
to be able to achieve similar level of performance

00:45:08.440 --> 00:45:11.940
on the van, and so generalization with N-Twin
neural networks can be very efficient.

00:45:11.940 --> 00:45:16.970
It's like large language models learning to
generalize from English to German, to French,

00:45:16.970 --> 00:45:23.510
to Mandarin, to other languages, and so those
are some of the things that we think about

00:45:23.510 --> 00:45:30.069
when we look to scale our technology and generalize
it to new driving scenarios.

00:45:30.069 --> 00:45:42.670
And what about the computer required to train
the models and then to operate the models?

00:45:42.670 --> 00:45:53.940
And do the models operate through a connection
to the cloud, or is there a GPU running in

00:45:53.940 --> 00:45:55.210
the vehicle?

00:45:55.210 --> 00:45:58.779
Yeah, so we don't train.

00:45:58.779 --> 00:46:01.369
The models live in the car.

00:46:01.369 --> 00:46:05.880
They are safety assured, validated, before
they're deployed on the car.

00:46:05.880 --> 00:46:10.720
Rather, we take the experience we get driving
all the time and we feed that back to the

00:46:10.720 --> 00:46:15.480
cloud in order to improve the new models that
we'll be validating and deploying.

00:46:15.480 --> 00:46:21.260
So it's all trained at scale and we've been
partnered for many years with our great friends

00:46:21.260 --> 00:46:24.559
Microsoft, and we work with Azure to be able
to train at scale.

00:46:24.559 --> 00:46:31.380
Azure has been able to provide us with extraordinary
compute power but, more importantly, the innovation

00:46:31.380 --> 00:46:37.290
that you need to be able to train these kinds
of models.

00:46:37.290 --> 00:46:43.990
The tough thing with training video scale
foundation models, like we do, is the data

00:46:43.990 --> 00:46:44.990
requirements.

00:46:44.990 --> 00:46:46.910
I talked about the size of the input set into
these models.

00:46:46.910 --> 00:46:48.570
It's truly ginormous.

00:46:48.570 --> 00:46:53.589
And to train on this kind of data, you can't
have it all sitting on the local cloud.

00:46:53.589 --> 00:46:57.520
It's tens of petabytes, maybe hundreds of
petabytes, since you need to be able to stream

00:46:57.520 --> 00:46:59.130
it to your GPUs.

00:46:59.130 --> 00:47:03.940
This means you need a different set of infrastructure
from when you're training a large language

00:47:03.940 --> 00:47:04.940
model.

00:47:04.940 --> 00:47:06.000
You can't have your data stored locally on
the GPUs.

00:47:06.000 --> 00:47:12.130
You need to be able to stream it from storage
that you can do fairly random access across,

00:47:12.130 --> 00:47:17.109
and that's quite a hard infrastructure challenge.

00:47:17.109 --> 00:47:20.114
So we've been thrilled to be able to do a
lot of pioneering work in this space and we've

00:47:20.114 --> 00:47:22.320
been able to do a lot of work in the space
with Microsoft to make it possible to train

00:47:22.320 --> 00:47:25.950
models at this scale.

00:47:25.950 --> 00:47:43.320
And is that, in order to get the video into
a, do you vectorize it in order to tokenize

00:47:43.320 --> 00:47:47.700
it, or how does that work?

00:47:47.700 --> 00:47:53.660
Yeah, we take the video data or the experience
for all kinds of data and, yep, it's tokenized,

00:47:53.660 --> 00:47:59.339
it's fed into the transformers at scale and
that's how it's trained and it's not just

00:47:59.339 --> 00:48:05.890
video data, it's also the navigation prompt,
it's the other robot sensors like GPS, wheel

00:48:05.890 --> 00:48:08.340
speeds, things like this.

00:48:08.340 --> 00:48:13.450
All these kinds of input data we use and feed
into the AI.

00:48:13.450 --> 00:48:15.119
Yeah, I mean.

00:48:15.119 --> 00:48:24.300
The reason I'm asking about the computer requirements
is that these large language models are amazing.

00:48:24.300 --> 00:48:34.250
They're people that are scaling them 10 times
or more from what we've seen so far.

00:48:34.250 --> 00:48:42.569
But there's a limit in the availability of
GPUs for the time being and for the foreseeable

00:48:42.569 --> 00:48:48.390
not the foreseeable future, but for at least
a few years.

00:48:48.390 --> 00:48:59.140
And then, because of that constraint, there's
a limit on the amount of inference.

00:48:59.140 --> 00:49:08.599
Anyone, a customer, can use the model for
these rate limits.

00:49:08.599 --> 00:49:13.859
Does a world model face those same constraints?

00:49:13.859 --> 00:49:18.790
Sorry, Craig, can you clarify your question?

00:49:18.790 --> 00:49:20.280
Constraints around rate limits?

00:49:20.280 --> 00:49:31.490
Yeah, just on the availability of GPUs both
for training and for inference, because it

00:49:31.490 --> 00:49:39.020
costs an enormous amount of money to train
an LLM, but it also costs an enormous amount

00:49:39.020 --> 00:49:47.339
of money on the inference side and people
are accessing these models through APIs and

00:49:47.339 --> 00:49:55.230
because of the costs and the compute constraints
they can only process so many requests a minute

00:49:55.230 --> 00:50:01.950
or so many tokens a minute and that is really
limiting the enterprise scale applications.

00:50:01.950 --> 00:50:08.540
So does that same kind of problem run through
a world model?

00:50:08.540 --> 00:50:15.760
Or is a world model less fundamentally less
compute intensive and so you can get around

00:50:15.760 --> 00:50:16.809
those problems?

00:50:16.809 --> 00:50:22.170
Well, the great thing about our space is that
at inference time, most of the compute runs

00:50:22.170 --> 00:50:29.770
on the car and so, assuming you've got vehicle
assets to deploy on that are there and fleets

00:50:29.770 --> 00:50:36.260
and are generating value, then you're in a
great spot because you can run all the inferences

00:50:36.260 --> 00:50:39.340
you need on the vehicles themselves.

00:50:39.340 --> 00:50:41.490
So our challenge is really training time.

00:50:41.490 --> 00:50:46.300
Yes, there's some inference costs, but it's
more manageable because it's a hybrid-edge

00:50:46.300 --> 00:50:52.750
cloud model and primarily you need to have
most of it on the car because the car should

00:50:52.750 --> 00:50:57.200
be able to have all the intelligence it needs
to be safe and make the kind of decisions

00:50:57.200 --> 00:51:01.540
it needs to operate in an environment on board
the vehicle.

00:51:01.540 --> 00:51:03.260
So training time really matters.

00:51:03.260 --> 00:51:08.720
And there, yeah, I mean we are hungry for
all of the compute and data storage that we

00:51:08.720 --> 00:51:14.069
can get to be able to power these models,
and I feel fortunate to be writing Moore's

00:51:14.069 --> 00:51:18.630
Law and other year-on-year improvement curves
that just bring down the cost and increase

00:51:18.630 --> 00:51:22.910
the availability of increasing orders of magnitudes
of these systems.

00:51:22.910 --> 00:51:29.960
But no, we certainly do have an appetite to
lift what we've got another 100x to next year

00:51:29.960 --> 00:51:35.790
in terms of scale of data and compute and
parameters in this model.

00:51:35.790 --> 00:51:39.800
Training computers is a really, really important
factor for us.

00:51:39.800 --> 00:51:42.380
Okay, then quickly, two questions.

00:51:42.380 --> 00:51:46.440
Is Gaia open source?

00:51:46.440 --> 00:51:56.160
And I've been writing a lot about why we don't
see humanoid robots the way everyone wants

00:51:56.160 --> 00:52:02.250
to see, and one of the you know they're the
hardware problems, but the other problem is

00:52:02.250 --> 00:52:07.000
having a reliable AI brain.

00:52:07.000 --> 00:52:17.520
Could this model be applied to other forms
of embodied AI robots?

00:52:17.520 --> 00:52:18.520
That's an awesome question.

00:52:18.520 --> 00:52:23.260
I'm really bullish on humanoid robotics being
a part of the future, the technology we're

00:52:23.260 --> 00:52:24.260
building.

00:52:24.260 --> 00:52:28.450
We want to see this kind of embodied AI system
empower all kinds of robots, whether it's

00:52:28.450 --> 00:52:32.059
manufacturing domestic robots or self-driving
cars.

00:52:32.059 --> 00:52:33.059
But I agree with you.

00:52:33.059 --> 00:52:37.220
I think self-driving cars will be the first
big application of embodied AI at scale, because

00:52:37.220 --> 00:52:41.560
there is data, there are hardware platforms,
there is a business case and so it can be

00:52:41.560 --> 00:52:42.560
built.

00:52:42.560 --> 00:52:48.580
Today, I think getting the data and the hardware
platforms for humanoid robotics is is harder,

00:52:48.580 --> 00:52:55.339
but I hope that the scale of embodied AI we
can build through self-driving can make that

00:52:55.339 --> 00:53:00.250
easier by taking that technology and adapting
it to humanoid robotics in the future, and

00:53:00.250 --> 00:53:03.130
I would love for Wave to be part of that.

00:53:03.130 --> 00:53:08.369
Once we've got our self-driving embodied AI
systems to scale.

00:53:08.369 --> 00:53:09.910
That's really the path we see.

00:53:09.910 --> 00:53:16.260
So I agree with you on that one, but I'm very
bullish on in 10 years time, ai not just being

00:53:16.260 --> 00:53:24.859
chatbots and co-pilots but being all kinds
of physical embodied AI systems in the world

00:53:24.859 --> 00:53:26.060
that we live in.

00:53:26.060 --> 00:53:28.510
Okay, and then the question on open source.

00:53:28.510 --> 00:53:30.910
Is Gaia open source?

00:53:30.910 --> 00:53:36.000
So, gaia, we have written an extensive research
paper around it, and we're very much fans

00:53:36.000 --> 00:53:41.270
of openly engaging with the AI community and
sharing ideas.

00:53:41.270 --> 00:53:46.819
We haven't open sourced the model itself for
now, and that's something that we continue

00:53:46.819 --> 00:53:52.130
to iterate on and develop internally and to
use to deploy on our fleets with our partners.

00:53:52.130 --> 00:53:58.990
Hi, I wanted to jump in and give a shout out
to our sponsor, netsuite by Oracle.

00:53:58.990 --> 00:54:05.440
I'm a journalist and getting a single source
of truth is nearly impossible.

00:54:05.440 --> 00:54:11.579
If you're a business owner, having a single
source of truth is critical to running your

00:54:11.579 --> 00:54:12.869
operations.

00:54:12.869 --> 00:54:20.140
If this is you, you should know these three
numbers 36,000, 25,1.

00:54:20.140 --> 00:54:27.359
36,000, because that's the number of businesses
that have upgraded to NetSuite by Oracle.

00:54:27.359 --> 00:54:33.300
Netsuite is the number one cloud financial
system streamlining accounting, financial

00:54:33.300 --> 00:54:37.180
management, inventory, hr and more.

00:54:37.180 --> 00:54:41.309
25, because NetSuite turns 25 this year.

00:54:41.309 --> 00:54:47.400
That's 25 years of helping businesses do more
with less, close their books in days, not

00:54:47.400 --> 00:54:50.300
weeks, and drive down costs.

00:54:50.300 --> 00:54:56.099
One because your business is one of a kind,
so you get a customized solution for all of

00:54:56.099 --> 00:55:03.599
your KPIs in one efficient system with one
source of truth: Manage risk, get reliable

00:55:03.599 --> 00:55:09.590
forecast and improve margins Everything you
need all in one place.

00:55:09.590 --> 00:55:16.540
As I said, I'm not the most organized person
in the world and there's real power to having

00:55:16.540 --> 00:55:21.569
all of the information in one place to make
better decisions.

00:55:21.569 --> 00:55:28.900
This is an unprecedented offer by NetSuite
to make that possible Right now.

00:55:28.900 --> 00:55:34.799
Download NetSuite's popular KPI checklist,
designed to give you consistently excellent

00:55:34.799 --> 00:55:39.600
performance, absolutely free, at netsuitecom.

00:55:39.600 --> 00:55:41.970
Slash I on AI.

00:55:41.970 --> 00:55:48.240
That's Eye on AI E-Y-E-O-N-A-I all run together.

00:55:48.240 --> 00:55:55.630
Go to netsuite.com, slash I on AI, to get
your own KPI checklist.

00:55:55.630 --> 00:56:01.750
They support us, so let's support them.

00:56:01.750 --> 00:56:02.800
That's it for this episode.

00:56:02.800 --> 00:56:06.030
I want to thank Alex for his time.

00:56:06.030 --> 00:56:11.599
If you want to learn more about the conversation
we had today, you can find a transcript on

00:56:11.599 --> 00:56:12.760
our website.

00:56:12.760 --> 00:56:20.180
I'm on AI . That's E-Y-E-O-N-A-I Remember.

00:56:20.180 --> 00:56:31.740
The singularity may not be near, but AI is
changing your world, so pay attention.

