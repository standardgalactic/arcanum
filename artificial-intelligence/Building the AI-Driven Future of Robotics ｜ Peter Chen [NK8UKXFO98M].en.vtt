WEBVTT
Kind: captions
Language: en

00:00:00.080 --> 00:00:02.760
 
Learning a world model of other humans, other

00:00:02.760 --> 00:00:03.760
human drivers.

00:00:03.760 --> 00:00:10.080
pedestrian cyclist behaviour, gives them a
much better ability to build a driving AI,

00:00:10.080 --> 00:00:11.730
so it absolutely makes sense.

00:00:11.730 --> 00:00:16.080
The same idea of understanding the environment
through a lot of data so that you can better

00:00:16.080 --> 00:00:21.980
anticipate what actions you should take also
makes sense in our robotics world, like, in

00:00:21.980 --> 00:00:30.030
particular, robotic manipulation. In robotics,
inference cost and speed are very important. In robotics,

00:00:30.030 --> 00:00:32.960
you need your robots to be acting all the
time.

00:00:32.960 --> 00:00:38.980
Like that means that you have to really optimise
your AI models to give output very quickly

00:00:38.980 --> 00:00:41.250
and so the robots can take actions continuously.

00:00:41.250 --> 00:00:45.010
 
AI might be the most important new computer

00:00:45.010 --> 00:00:46.629
technology ever.

00:00:46.629 --> 00:00:51.899
It's storming every industry and literally
billions of dollars are being invested.

00:00:51.899 --> 00:00:53.310
So buckle up.

00:00:53.310 --> 00:00:57.730
The problem is that AI needs a lot of speed
and processing power.

00:00:57.730 --> 00:01:02.199
So how do you compete without costs spiralling
out of control?

00:01:02.199 --> 00:01:08.900
It's time to upgrade to the next generation
of the cloud: Oracle Cloud Infrastructure,

00:01:08.900 --> 00:01:10.460
or OCI.

00:01:10.460 --> 00:01:18.710
Oci is a single platform for your infrastructure,
database application development, and AI needs.

00:01:18.710 --> 00:01:25.460
Oci has four to eight times the bandwidth
of other clouds, and offers one consistent

00:01:25.460 --> 00:01:28.570
price instead of variable regional pricing.

00:01:28.570 --> 00:01:34.329
And, of course, nobody does data better than
Oracle.

00:01:34.329 --> 00:01:41.170
So now you can train your AI models at twice
the speed and less than half the cost of other

00:01:41.170 --> 00:01:42.170
clouds.

00:01:42.170 --> 00:01:49.979
If you want to do more and spend less, like
Uber, 8x8, and Databricks Mosaic, take a free

00:01:49.979 --> 00:01:56.619
test drive of OCI at oracle.com/eyeonai 

00:01:56.619 --> 00:02:01.770
That's E-Y-E-O-N-A-I all run together.

00:02:01.770 --> 00:02:06.380
Oracle.com/eyeonai

00:02:06.380 --> 00:02:11.630
That's oracle.com/eyeonai

00:02:11.630 --> 00:02:17.170
Hi, I'm Craig Smith and this is Eye on AI.

00:02:17.170 --> 00:02:23.780
In today's episode, we delve deep into the
world of AI and robotics with Peter Chen,

00:02:23.780 --> 00:02:31.090
co-founder and CEO of Covariant, an industrial
AI robotics company.

00:02:31.090 --> 00:02:36.700
Peter talks about building a universal foundation
model that can operate different kinds of

00:02:36.700 --> 00:02:40.500
industrial robots across three continents.

00:02:40.500 --> 00:02:46.390
We also discuss the role of world models in
predicting the outcomes of actions and their

00:02:46.390 --> 00:02:51.680
potential to generalise across various robotic
applications.

00:02:51.680 --> 00:02:57.200
Whether it's navigating the complexities of
warehouse automation or envisioning the future

00:02:57.200 --> 00:03:04.459
of robotics, Peter's insights are as transformative
as the technology he's helping to create.

00:03:04.459 --> 00:03:08.800
I hope you find the conversation as engrossing
as I did.

00:03:08.800 --> 00:03:10.460
 
Craig, it's great to be here.

00:03:10.460 --> 00:03:12.010
Thank you so much for having me.

00:03:12.010 --> 00:03:13.010
My name is Peter Chen.

00:03:13.010 --> 00:03:17.370
I'm one of the co-founders, as well as CEO,
of Covariant, a company that is focused on

00:03:17.370 --> 00:03:20.010
building foundation models for robotics.

00:03:20.010 --> 00:03:22.510
A bit of a personal history about myself.

00:03:22.510 --> 00:03:24.150
I was born and raised in China.

00:03:24.150 --> 00:03:30.410
I got into programming and computer science
at a very young age.

00:03:30.410 --> 00:03:35.560
It always fascinates me how much intelligence
you can get by programming software, which

00:03:35.560 --> 00:03:42.000
is basically instructing a computer to act
intelligently, but there's always something

00:03:42.000 --> 00:03:44.530
left.

00:03:44.530 --> 00:03:47.840
What if the computers can learn from data?

00:03:47.840 --> 00:03:52.580
You have to instruct every single bit of the
intelligence rule.

00:03:52.580 --> 00:03:55.090
That's what got me into PhD.

00:03:55.090 --> 00:04:03.940
After my undergrad at UC Berkeley, I started
my PhD in AI with Professor Pieter Abbeel at

00:04:03.940 --> 00:04:10.250
UC Berkeley, focusing on two areas that have
both become extremely hot today.

00:04:10.250 --> 00:04:16.180
One area is reinforcement learning, the art
of having machine learning models produce

00:04:16.180 --> 00:04:19.959
actions. And some of these actions lead to
good consequences.

00:04:19.959 --> 00:04:22.380
Some of these actions lead to bad consequences.

00:04:22.380 --> 00:04:27.330
How can you have models learn from their own
mistakes and successes?

00:04:27.330 --> 00:04:29.620
That's reinforcement learning.

00:04:29.620 --> 00:04:33.430
Another area of my PhD focus was generative
models.

00:04:33.430 --> 00:04:41.710
My PhD advisor and I co-created and co-taught
the first graduate level generative AI class

00:04:41.710 --> 00:04:48.620
at Berkeley back in 2017, 2018, a long time
ago.

00:04:48.620 --> 00:04:56.680
Obviously, we have seen how those fields have
really taken off in the last couple of years.

00:04:56.680 --> 00:05:03.180
Those ideas that were once very academic,
cutting edge and unproven have become a much

00:05:03.180 --> 00:05:07.990
more commonplace concept that people interact
with.

00:05:07.990 --> 00:05:13.520
If you go to chatGPT, it is both a generative
model and a model that is aligned by reinforcement

00:05:13.520 --> 00:05:14.520
learning.

00:05:14.520 --> 00:05:19.449
It's amazing to see that arc of transformation
of something that was cutting edge, unproven

00:05:19.449 --> 00:05:25.190
ideas, to something that now is everyday and
is still continuing to accelerate AI's

00:05:25.190 --> 00:05:26.310
development.

00:05:26.310 --> 00:05:30.760
Another bit of my personal background was
I also spent time early on at OpenAI.

00:05:30.760 --> 00:05:34.250
I joined OpenAI fairly early.

00:05:34.250 --> 00:05:38.370
When I started working out at OpenAI, openai
didn't even have an office.

00:05:38.370 --> 00:05:43.490
We were working out of Greg Brockman's apartment
at the time.

00:05:43.490 --> 00:05:51.060
Obviously, OpenAI has done amazingly well
and has really powered a lot of the AI revolution

00:05:51.060 --> 00:05:53.440
that we are seeing today.

00:05:53.440 --> 00:05:59.720
Maybe the thing that I want to call out that's
super interesting to me, especially as I reflect

00:05:59.720 --> 00:06:06.930
back, is that some of the core philosophies
that OpenAI started out with are really the

00:06:06.930 --> 00:06:12.869
same set of philosophies that are powering
the success of OpenAI that we are seeing today.

00:06:12.869 --> 00:06:17.900
If I were to summarise the early research
philosophies at OpenAI in its first one or

00:06:17.900 --> 00:06:20.800
two years of existence as a company,

00:06:20.800 --> 00:06:27.120
it was this belief of a foundation model,
scaling up big models on large, diverse datasets,

00:06:27.120 --> 00:06:33.190
this belief in generative models, using generative
models to absorb a lot of unlabeled data,

00:06:33.190 --> 00:06:34.760
unstructured data.

00:06:34.760 --> 00:06:40.990
Third is reinforcement learning, like the
ability to teach agents or models, the ability

00:06:40.990 --> 00:06:43.160
to take actions in the world.

00:06:43.160 --> 00:06:48.099
Those philosophies heavily influence me and
heavily influence what we do at Covariant

00:06:48.099 --> 00:06:49.780
also.

00:06:49.780 --> 00:06:53.979
It's the same driving force of what powers
OpenAI's success today.

00:06:53.979 --> 00:06:58.770
Fast forward to how we founded Covariant.

00:06:58.770 --> 00:07:06.030
A couple of the founders at Covariant left
OpenAI in late 2017 to start Covariant.

00:07:06.030 --> 00:07:13.759
We started Covariant really with much of the
same thesis of what powers the current success

00:07:13.759 --> 00:07:15.440
of the large language model.

00:07:15.440 --> 00:07:17.680
We believe in a single model.

00:07:17.680 --> 00:07:20.430
We believe in a single large model.

00:07:20.430 --> 00:07:25.430
That is a foundation model, which means it's
a model that is trained on multiple types

00:07:25.430 --> 00:07:26.430
of tasks.

00:07:26.430 --> 00:07:31.940
It can leverage the transfer learnings across
multiple kinds of tasks and have emergent

00:07:31.940 --> 00:07:33.160
behaviour.

00:07:33.160 --> 00:07:39.710
It generalised to new tasks better, but also
performed better at any specific task than

00:07:39.710 --> 00:07:43.740
a bespoke model that is only trained on that
task.

00:07:43.740 --> 00:07:49.630
And we had an incredibly strong conviction
that this foundation model for robotics has

00:07:49.630 --> 00:07:56.650
to be the way to go to solve robotics problems
Because, like obviously, we have seen the

00:07:56.650 --> 00:08:01.250
success of this foundation model approach
for language. But the reason that it makes

00:08:01.250 --> 00:08:07.470
even more sense for robotics is that there's
only one physical world, unlike in the language.

00:08:07.470 --> 00:08:13.520
Like where you're trying to compress the whole
world of human knowledge, which includes many

00:08:13.520 --> 00:08:18.830
things that have nothing in common with each
other, like what is the soil composition on

00:08:18.830 --> 00:08:23.460
the moon versus how do you play chess, okay,
like both of these are knowledge that you

00:08:23.460 --> 00:08:28.099
can find on the internet, but they have absolutely
nothing in common with each other and you're

00:08:28.099 --> 00:08:31.120
trying to compress all these things into one
model.

00:08:31.120 --> 00:08:36.370
But if you think about building a foundation
model for robotics like the robot could have

00:08:36.370 --> 00:08:41.279
different bodies and the robots might be doing
different things, like it might be interacting

00:08:41.279 --> 00:08:43.279
in different kinds of environments.

00:08:43.279 --> 00:08:49.649
However, like all of these robots live and
operate in the same physical world, and so

00:08:49.649 --> 00:08:54.560
it makes a lot of sense to build one foundation
model that can learn from all of these different

00:08:54.560 --> 00:09:00.700
robot experiences and really understand physics
and understand how you control robots to move

00:09:00.700 --> 00:09:02.120
in the world around us.

00:09:02.120 --> 00:09:05.940
So that was a little bit of the founding story
of Covariant. And fast forward to today.

00:09:05.940 --> 00:09:12.000
Covariant has commercialised the first robotic
foundation model that's ever built. Like, so

00:09:12.000 --> 00:09:18.570
essentially one single model that is powering
robots working in production in customer environments

00:09:18.570 --> 00:09:24.230
in three different continents, dozens of different
robot hardware bodies.

00:09:24.230 --> 00:09:28.880
That same single foundation model is powering
and solving problems in a lot of different

00:09:28.880 --> 00:09:29.880
industries.

00:09:29.880 --> 00:09:35.589
We're starting out from warehouses, but our
long-term goal is to build this foundation

00:09:35.589 --> 00:09:41.269
model that can solve robotic manipulation
problems in general, like across multiple

00:09:41.269 --> 00:09:42.810
other industries.

00:09:42.810 --> 00:09:48.110
So that's a quick introduction about myself,
where I come from, like the philosophies and

00:09:48.110 --> 00:09:52.570
the technical ideas that have heavily influenced
us and where we have taken it so far.

00:09:52.570 --> 00:09:55.670
 
Yeah, I have a couple of questions.

00:09:55.670 --> 00:10:06.970
One, when you were doing the first course on
generative AI at Berkeley, was that in response

00:10:06.970 --> 00:10:13.020
to the development of the transformer algorithm,
or did you integrate the transformer algorithm

00:10:13.020 --> 00:10:21.040
into that course, Because that has really
accelerated - it is today the core of generative

00:10:21.040 --> 00:10:22.040
AI.

00:10:22.040 --> 00:10:28.560
 
Yeah, I would say Transformers was not a key

00:10:28.560 --> 00:10:30.620
focus of that course.

00:10:30.620 --> 00:10:37.440
So when we think about generative AI, you
can think about it as, like the two major

00:10:37.440 --> 00:10:38.790
components of it.

00:10:38.790 --> 00:10:42.480
So one major component is what is the model architecture?

00:10:42.480 --> 00:10:46.250
So how is your brain structured?

00:10:46.250 --> 00:10:51.630
So, obviously, a better brain structure can
allow you to learn better, and so a transformer

00:10:51.630 --> 00:10:57.779
is a really flexible brain structure that
can allow you to absorb a lot of knowledge,

00:10:57.779 --> 00:11:00.600
patterns from the data.

00:11:00.600 --> 00:11:03.529
There's another side of how you train a generative
model.

00:11:03.529 --> 00:11:05.910
That is like, how do you teach it?

00:11:05.910 --> 00:11:10.540
So, even if you have a very flexible brain
structure, like, how do you actually give

00:11:10.540 --> 00:11:11.540
that learning?

00:11:11.540 --> 00:11:14.620
Like, think of it as the curriculum teaching methods.

00:11:14.620 --> 00:11:18.870
So in this case, it would be the statistical
models that you use, like different versions

00:11:18.870 --> 00:11:24.500
of it would be - the most popular one now, obviously
it's diffusion autoregressive model of next

00:11:24.500 --> 00:11:30.240
token prediction, and then, like a little
bit earlier ago, like there will be GANs,

00:11:30.240 --> 00:11:32.106
VAEs [Variational Autoencoders]

00:11:32.106 --> 00:11:35.615
and these different models that are like
the statistical representation,

00:11:35.615 --> 00:11:38.866
like the statistical representations that you impose on the world,

00:11:38.866 --> 00:11:42.671
like which you can see as like methods of how you teach the models.

00:11:42.671 --> 00:11:49.141
So I would say, like the initial class, focus
more on things like what are the different models that you can use,

00:11:49.141 --> 00:11:54.037
as opposed to a more
focus on things like what is the underlying brain structure?

00:11:54.037 --> 00:11:58.623
Like, but you can like, swap and plug in with
these kinds of things.

00:11:58.623 --> 00:12:06.029
So, for example, the idea of diffusion like
you can implement diffusion with both convolutional new nets.

00:12:06.029 --> 00:12:09.750
You could implement it with a transformer-based architecture and like obviously

00:12:09.750 --> 00:12:14.450
currently the most popular one, like stable
diffusion is implemented with a combination

00:12:14.450 --> 00:12:17.649
of convolutional neural nets as well as transformers.

00:12:17.649 --> 00:12:19.630
You can really take the best of both worlds.

00:12:19.630 --> 00:12:23.610
 
Yeah, the current -

00:12:23.610 --> 00:12:31.430
I mean there's been a lot of talk, at least
in the literature, in the last few months

00:12:31.430 --> 00:12:45.690
about using large models, large language models
or pre-trained transformer models, as agents.

00:12:45.690 --> 00:12:55.910
And then, more recently you and I spoke about
this the other day - Yann LeCun and Alex Kendall

00:12:55.910 --> 00:13:08.350
at a company called Wayve.AI are working with
world models and, you know, that learn

00:13:08.350 --> 00:13:17.440
causality directly from sensory inputs, not
through the filter of language, and that world

00:13:17.440 --> 00:13:33.400
model idea speaks to me because it seems much
closer to how the brain learns, initially.

00:13:33.400 --> 00:13:36.209
In the Covariant foundational model,

00:13:36.209 --> 00:13:41.930
can you talk about the architecture and how
it works?

00:13:41.930 --> 00:13:51.470
I know that it's certainly proprietary, but - and then talk about these new developments with

00:13:51.470 --> 00:13:59.500
large language models and world models and
whether you're integrating those ideas or

00:13:59.500 --> 00:14:01.560
whether you see promise in them?

00:14:01.560 --> 00:14:03.459
 
Yeah.

00:14:03.459 --> 00:14:06.310
So those are really good questions.

00:14:06.310 --> 00:14:12.160
Maybe we would tackle them separately, like
the idea of world models and then also the

00:14:12.160 --> 00:14:14.720
idea of agents in the language world.

00:14:14.720 --> 00:14:21.470
So, first of all, like this idea of learning
a world model for any kind of what we call

00:14:21.470 --> 00:14:29.240
embodied agents makes a lot of sense, right.
Like, so, like if you, if the goal of an agent

00:14:29.240 --> 00:14:36.440
is to understand the physical world and take
actions in it and your actions have consequences,

00:14:36.440 --> 00:14:40.790
then you should have understanding of what
those consequences are right, as opposed to

00:14:40.790 --> 00:14:44.899
just blindly trying things and say, oh yeah,
pushing button A is better than pushing button

00:14:44.899 --> 00:14:46.460
B. Well, that's it.

00:14:46.460 --> 00:14:51.100
I mean, that kind of works if you have a lot
of data, but that's kind of like a very naive

00:14:51.100 --> 00:14:52.100
understanding of the world.

00:14:52.100 --> 00:14:56.620
Like, if I push a lot on button A, it tends
to give me better outcome than pushing button

00:14:56.620 --> 00:14:59.079
B. Okay, then I do pushing button A more.

00:14:59.079 --> 00:15:03.130
That's kind of like not having a very sophisticated
understanding of the world and it's also not

00:15:03.130 --> 00:15:04.180
very generalizable.

00:15:04.180 --> 00:15:08.300
Like what if I like, instead of presenting
button A and B to you, I give you a keyboard

00:15:08.300 --> 00:15:12.880
and you need to type in, like passkey, and
it does different things.

00:15:12.880 --> 00:15:16.730
You really cannot take any of the learnings
that you get from pushing button A is better,

00:15:16.730 --> 00:15:21.149
like if now you suddenly have a different
way of interacting in the world, and so the

00:15:21.149 --> 00:15:28.339
idea, like the general idea of building a
world model, is can we build agents that really

00:15:28.339 --> 00:15:37.000
understand the environment and have the ability
to anticipate what are the consequences of

00:15:37.000 --> 00:15:40.170
the actions that it takes?

00:15:40.170 --> 00:15:47.190
And this idea has many different incarnations,
like with how, what Alex and the team is doing

00:15:47.190 --> 00:15:48.370
at Wayve.

00:15:48.370 --> 00:15:53.300
A lot of it is about anticipating other agents'
behaviour.

00:15:53.300 --> 00:15:59.399
Right, like if you drive a car, like slowly
edging into pedestrians, like most often

00:15:59.399 --> 00:16:05.120
people would try to step away from the car
if they notice the car is approaching them.

00:16:05.120 --> 00:16:09.160
And if they don't step away, like that means
maybe they didn't notice that the car is approaching

00:16:09.160 --> 00:16:10.160
them.

00:16:10.160 --> 00:16:16.950
So there's like some interesting interaction
that you can learn by anticipating other agents'

00:16:16.950 --> 00:16:21.350
behaviour in that physical world, which is
absolutely the most core problem to solve

00:16:21.350 --> 00:16:26.569
in self driving, is like this kind of multi
agent interaction and the behaviour that you

00:16:26.569 --> 00:16:27.720
need to generate from there.

00:16:27.720 --> 00:16:33.930
And so by learning a world model of other humans,
other human drivers, pedestrian, cyclist behaviour,

00:16:33.930 --> 00:16:39.810
gives them a much better ability to build
a driving AI.

00:16:39.810 --> 00:16:41.569
So it absolutely makes sense.

00:16:41.569 --> 00:16:47.009
And the same idea of understanding the environment
through a lot of data so that you can better

00:16:47.009 --> 00:16:53.040
anticipate, like, what actions you should
take also makes sense in our robotics world,

00:16:53.040 --> 00:16:55.019
in particular, robotic manipulation.

00:16:55.019 --> 00:17:03.170
So in our case, like the world that you're
learning is not another human being's -  what's

00:17:03.170 --> 00:17:07.929
in another human being's mind, but it's more
like what happens to physics, like if I pick

00:17:07.929 --> 00:17:12.299
things up in different ways, like what is
more stable, what is less stable, like if

00:17:12.299 --> 00:17:18.040
I throw things away in a certain manner, like
where would it land if I want to carefully

00:17:18.040 --> 00:17:23.251
put things together, and so like again, like
you have two ways that you can build this

00:17:23.251 --> 00:17:24.251
AI.

00:17:24.251 --> 00:17:28.190
Like one way they can build this AI is, I just
do a lot of blind, random trials and I see

00:17:28.190 --> 00:17:31.210
what happens to work and I just keep doing
that.

00:17:31.210 --> 00:17:35.640
Or you could actually learn a sophisticated
understanding of okay, like if I pick things

00:17:35.640 --> 00:17:39.800
up this way, this is a pretty stable way of
grasping a certain item.

00:17:39.800 --> 00:17:44.190
If I pick things up another way, oh wow, this
is like a very unstable, very precarious way

00:17:44.190 --> 00:17:50.870
of picking up an item, but I can, by anticipating
what would happen in the physical world, like

00:17:50.870 --> 00:17:55.280
it gives the AI a much better ability to act
in it.

00:17:55.280 --> 00:18:01.390
So we are a strong believer in this idea of
a world model like, essentially, this AI that

00:18:01.390 --> 00:18:07.120
can learn about the environment and also anticipate
what would happen to it.

00:18:07.120 --> 00:18:13.470
And then another thing, that it's not just
more like you said, it's not just a more sensible

00:18:13.470 --> 00:18:19.940
way for an intelligent entity to learn, because
that's more like how humans would learn.

00:18:19.940 --> 00:18:24.500
You don't just randomly try, you anticipate,
like how things are, what would be the consequence

00:18:24.500 --> 00:18:25.830
of the action that you take.

00:18:25.830 --> 00:18:30.980
But, in addition to that, like those and I'm
really amazing property about world models that

00:18:30.980 --> 00:18:37.500
we believe is under talked about and which
is this idea that if you formulate the right

00:18:37.500 --> 00:18:45.799
world model that unified all robotic applications,
right, so like you could have like, for example,

00:18:45.799 --> 00:18:52.810
like if you think about like a robot that
is folding laundry, as opposed to like versus

00:18:52.810 --> 00:18:59.080
a robot that is packing a customer order in
a warehouse, like at a service level, like

00:18:59.080 --> 00:19:01.830
there's kind of nothing in common with these
two robots.

00:19:01.830 --> 00:19:02.830
Like robots.

00:19:02.830 --> 00:19:07.250
Like one robot is trying to carefully think
about, okay, like how do I pick up a deformable

00:19:07.250 --> 00:19:11.789
piece of a t- shirt and how do I flatten it
and how do I fold it?

00:19:11.789 --> 00:19:14.360
Another robot in a warehouse would be thinking
about it.

00:19:14.360 --> 00:19:18.980
I need to pick up the item and find where
the buckle is, and I need to scan the buckle.

00:19:18.980 --> 00:19:21.730
Like from a policy or from an action perspective.

00:19:21.730 --> 00:19:27.080
There's really nothing in common between these
two robots, but what is in common about them

00:19:27.080 --> 00:19:30.500
is there's only one physical world that's
powering them right.

00:19:30.500 --> 00:19:35.150
Like so, if you're learning a role model that
understands, if I interact with the world

00:19:35.150 --> 00:19:40.539
in a certain way, what would happen in the
next few seconds physically.

00:19:40.539 --> 00:19:46.039
Like that concept is universal, right, and
so like what this is like, what makes this

00:19:46.039 --> 00:19:47.780
role model idea so powerful?

00:19:47.780 --> 00:19:55.350
Like is it gives you this formulation, or
quite like an interface that is the same across

00:19:55.350 --> 00:19:59.580
all robots, like, no matter what kind of tasks
they are doing, no matter what kinds of hardware

00:19:59.580 --> 00:20:00.970
they are using.

00:20:00.970 --> 00:20:05.460
It's the same, it's the same role model because
there's only one physical world.

00:20:05.460 --> 00:20:11.549
Like now, suddenly you find a way to really
scale up the data that can go into training

00:20:11.549 --> 00:20:12.870
robotic foundation models.

00:20:12.870 --> 00:20:15.990
Like, like for all of these foundation models,
like one of the like.

00:20:15.990 --> 00:20:20.020
Obviously you need the right model, you need
the right algorithm, but you also need a lot

00:20:20.020 --> 00:20:21.909
of data of the right kind, right like.

00:20:21.909 --> 00:20:28.370
So the role model actually opens up the possibility
of training a large foundation model that's

00:20:28.370 --> 00:20:33.460
learning on a lot of data, because now you
can pull the data from many different robots

00:20:33.460 --> 00:20:35.200
together like and like.

00:20:35.200 --> 00:20:39.490
It doesn't matter what kind of tasks they
are doing and the environment that they're

00:20:39.490 --> 00:20:44.620
interacting with, there's the same set of
physical principles behind them and that's

00:20:44.620 --> 00:20:47.120
what the model can learn.

00:20:47.120 --> 00:20:51.720
Craig Smith: 20:48
And the initial training.

00:20:51.720 --> 00:20:53.370
That's sort of the ongoing training.

00:20:53.370 --> 00:20:59.630
But the initial training you can simply train
from video.

00:20:59.630 --> 00:21:13.400
Is that right that shows the laws of physics
or you know causality and that sort of thing,

00:21:13.400 --> 00:21:17.110
and then, yeah, then there's ongoing training.

00:21:17.110 --> 00:21:19.240
Is the network computers learning?

00:21:19.240 --> 00:21:23.120
 
Yeah, so we believe video is a super important

00:21:23.120 --> 00:21:24.570
format for this.

00:21:24.570 --> 00:21:30.490
Like, how can you like there's just so much
data that's encoded in video, but what we

00:21:30.490 --> 00:21:33.409
have found is that pure video is also not
sufficient.

00:21:33.409 --> 00:21:38.049
Like, for example, like, if you just go to
YouTube and just watch a whole bunch of videos,

00:21:38.049 --> 00:21:42.960
you only get a very partial view of the world,
like.

00:21:42.960 --> 00:21:43.960
So they are like that.

00:21:43.960 --> 00:21:51.020
Let me just point out like two things that
are missing in in like, the first thing that

00:21:51.020 --> 00:21:57.010
is missing is, in a lot of the cases, you
don't really know what are the actions that

00:21:57.010 --> 00:22:02.890
are taken, like because you're just passively
observing things and so you don't really know,

00:22:02.890 --> 00:22:05.740
like, what are the actions that are taken.

00:22:05.740 --> 00:22:12.580
And then the second thing is, like, in a lot
of these videos out there, you also don't

00:22:12.580 --> 00:22:15.890
have a very detailed.

00:22:15.890 --> 00:22:21.090
It also lacks the very small details that
are really important to robotics like.

00:22:21.090 --> 00:22:27.450
So, for example, like what is the actual velocity
of a certain value down to a very fine degree

00:22:27.450 --> 00:22:28.720
of precision, like?

00:22:28.720 --> 00:22:33.550
That information is kind of hard to infer
from a video like, but if you are controlling

00:22:33.550 --> 00:22:38.659
a full robot system like, you actually can
get it, maybe from the motor encoder, but

00:22:38.659 --> 00:22:41.510
you can get the information in a much more
precise way.

00:22:41.510 --> 00:22:45.330
In a lot of those, in a lot of robotics cases,
you do need a pretty precise understanding

00:22:45.330 --> 00:22:48.650
of the world that video doesn't fully communicate
like.

00:22:48.650 --> 00:22:53.980
So both the lack of understanding of what
are the actions that were taken in videos,

00:22:53.980 --> 00:22:59.830
as well as the position that is required,
makes this kind of what we call videos in

00:22:59.830 --> 00:23:08.110
the wild a useful source of data, but it's
definitely not a sufficient set of data.

00:23:08.110 --> 00:23:12.380
 
And then, where do you get it?

00:23:12.380 --> 00:23:26.350
Is that why that data is supplemented with
data from robots operating in different scenarios?

00:23:26.350 --> 00:23:28.630
Or are there other kinds of data?

00:23:28.630 --> 00:23:30.630
Can you use synthetic data, for example?

00:23:30.630 --> 00:23:31.740
 
Yeah.

00:23:31.740 --> 00:23:39.610
So let me maybe answer the broader question
like so, when we think about how do you build

00:23:39.610 --> 00:23:45.830
a robotic foundation model, like a truly universal
AI that can be powering any robot hardware

00:23:45.830 --> 00:23:50.010
to do any arbitrary things to a very high
level of autonomy?

00:23:50.010 --> 00:23:55.900
We believe the data recipe for those three
pillars, like the first pillar, is what we

00:23:55.900 --> 00:23:56.900
talk about.

00:23:56.900 --> 00:24:01.940
Like essentially, data on the Internet, like
video data, image data on the Internet.

00:24:01.940 --> 00:24:08.130
Second thing, that second thing about it is
synthetic data, like generated data that are

00:24:08.130 --> 00:24:09.130
not made.

00:24:09.130 --> 00:24:13.820
They may not look exactly like the real world,
but they contain useful structure about the

00:24:13.820 --> 00:24:20.460
world that can teach the AI and you can get
lots of interesting combinations of known

00:24:20.460 --> 00:24:24.779
factors or variations through simulation,
through this kind of synthetic data.

00:24:24.779 --> 00:24:25.779
Like.

00:24:25.779 --> 00:24:27.160
We believe that's very important.

00:24:27.160 --> 00:24:32.480
But these two things like this kind of Data
in the wild on the internet and synthetic

00:24:32.480 --> 00:24:37.149
data are both very useful learning sources,
but in our experience they are not sufficient.

00:24:37.149 --> 00:24:42.610
Like they still lack the kind of actual Detail
interaction with the world, like understanding

00:24:42.610 --> 00:24:48.020
cause and effects, understanding them to a
very high degree of fidelity, like those are

00:24:48.020 --> 00:24:51.169
not present in these two datasets that we
talked about.

00:24:51.169 --> 00:24:56.090
Like to give an example of like where the
synthetic data breakdown, it's very difficult

00:24:56.090 --> 00:25:01.520
to simulate contact and anything that deforms,
and so those are the kind of places that like

00:25:01.520 --> 00:25:06.480
okay, I can use in the way simulated data
to Simulate something that's rigid or maybe

00:25:06.480 --> 00:25:11.530
doesn't involve a lot of contact, but as soon
as you involve that like your simulations,

00:25:11.530 --> 00:25:14.620
quality or the precision of it would quickly
decrease.

00:25:14.620 --> 00:25:19.190
So, at the end of the day, like we, what we
have found is that the third bucket of the

00:25:19.190 --> 00:25:27.050
data that you need is robots Interacting with
objects in the real world at scale like so.

00:25:27.050 --> 00:25:32.649
These are like the three data buckets that
Go into training, a robotic foundation, models

00:25:32.649 --> 00:25:38.809
like so they don't on the internet, on the,
in the wild, synthetic data and then large

00:25:38.809 --> 00:25:46.000
volume of robotics data's interacting with
the real world in production and, and this

00:25:46.000 --> 00:25:49.000
is really the core focus of covariant.

00:25:49.000 --> 00:25:56.100
Ever since we started the company, like, we
strongly believe in the idea that, in order

00:25:56.100 --> 00:26:01.730
to build the best robot AI, you need to have
the most amount of robotics data that are

00:26:01.730 --> 00:26:08.130
the highest quality like, which is why we
really focus on one Solving customer problems,

00:26:08.130 --> 00:26:13.770
like making sure we build a technology that
is not just Interesting lab demo but it's

00:26:13.770 --> 00:26:19.770
something that actually works reliably 24-7
in an industrial environment and the robots

00:26:19.770 --> 00:26:26.169
are so reliable, so autonomous and deliver
as such a level of throughput that Our customers

00:26:26.169 --> 00:26:28.570
facilities just completely depend on them.

00:26:28.570 --> 00:26:34.410
And once you have that, like, you have Robots
out there that are generating data at an incredible

00:26:34.410 --> 00:26:40.610
rate because, like, we're deploying these
type robots into Industrial warehouse facilities

00:26:40.610 --> 00:26:42.340
that process amazing volume.

00:26:42.340 --> 00:26:48.659
Like so, once you actually make these systems
generate commercial value, you can collect

00:26:48.659 --> 00:26:54.470
Tremendous amount of data while you Generate
value for your customers.

00:26:54.470 --> 00:26:59.600
So that's being very customer value focused,
being very production deployment focused,

00:26:59.600 --> 00:27:04.870
is like one thing that we have really focused
on as a company.

00:27:04.870 --> 00:27:08.590
The second thing that we really focus on as
a company is collecting the right kind of

00:27:08.590 --> 00:27:09.590
data.

00:27:09.590 --> 00:27:10.590
Like.

00:27:10.590 --> 00:27:13.279
So it's not just about, oh, get the robots
out there and then, like, as they get used

00:27:13.279 --> 00:27:15.940
24-7, a lot of data gets generated.

00:27:15.940 --> 00:27:20.149
We also spend a lot of time thinking about,
like, what is exactly the right kind of data

00:27:20.149 --> 00:27:22.529
that you need to collect from the fleet of
robots.

00:27:22.529 --> 00:27:28.730
I'm out there so that you can actually enable
learning, and there is lots of deep research,

00:27:28.730 --> 00:27:33.110
thinking and iterations that go into it, and
it's still something that we are obviously

00:27:33.110 --> 00:27:34.690
very actively iterating on.

00:27:34.690 --> 00:27:40.269
 
Yeah, and the architecture of your, of your

00:27:40.269 --> 00:27:44.299
world, of your foundation model is?

00:27:44.299 --> 00:27:49.630
Is it?

00:27:49.630 --> 00:28:01.340
Is it like a JEPA architecture that Yanlacoon
talks about, where the model is encoding the

00:28:01.340 --> 00:28:08.370
data into a higher representation space and
then operating in that space to make predictions?

00:28:08.370 --> 00:28:20.539
Or or or is it More along the lines of, you
know, a generative, pre-trained transformer

00:28:20.539 --> 00:28:27.539
model, where Everything's being tokenized
and you're predicting the next in a series?

00:28:27.539 --> 00:28:34.710
Well, I guess that wouldn't be a world model,
but when you combine these things, yeah, there

00:28:34.710 --> 00:28:35.710
are.

00:28:35.710 --> 00:28:39.890
Peter Chen: 28:37
There are definitely different schools of

00:28:39.890 --> 00:28:42.960
thoughts on how you represent this type of
thing.

00:28:42.960 --> 00:28:51.470
Like one, one way to do it is like you can
say like there's some explicit Layton representation

00:28:51.470 --> 00:28:56.190
of the world that I learned and is in that
some kind of latent description of the world

00:28:56.190 --> 00:28:59.980
that I learned to predict, I learned causality.

00:28:59.980 --> 00:29:06.150
There's another version of the world like,
which is, if you think about A large language

00:29:06.150 --> 00:29:08.640
model like that, just a transformer.

00:29:08.640 --> 00:29:11.620
Looking at all the previous words and I predict
the next word.

00:29:11.620 --> 00:29:17.100
Like there's never an explicit Representation
of a latent structure somewhere right.

00:29:17.100 --> 00:29:22.779
Like there's no saying, oh, I encode all of
my previous words into like some latent space

00:29:22.779 --> 00:29:27.720
and I decode the word From it, but instead
let you just have this large structure that

00:29:27.720 --> 00:29:32.460
looks at everything that you have seen in
an auto, aggressively predicting the next

00:29:32.460 --> 00:29:35.130
one.

00:29:35.130 --> 00:29:39.330
I wouldn't say I don't.

00:29:39.330 --> 00:29:44.740
I think the jury is still out in terms of
which one will be a more likely successful

00:29:44.740 --> 00:29:46.039
structure, and we can probably draw some Biological
inspiration, or maybe draw some inspiration

00:29:46.039 --> 00:29:47.039
into the next word.

00:29:47.039 --> 00:29:52.630
And we can probably draw some biological inspiration
or maybe draw some inspiration from how we

00:29:52.630 --> 00:29:56.580
work, like and you say well, like it seems
more efficient to operate in some kind of

00:29:56.580 --> 00:30:01.929
latent space, like because, like then, you're
operating in this, your reasoning about the

00:30:01.929 --> 00:30:06.330
world in a more abstract way, as opposed to
looking at every single pixels and then trying

00:30:06.330 --> 00:30:10.720
to think about, like, what should the next
pixel be?

00:30:10.720 --> 00:30:12.350
I would say like.

00:30:12.350 --> 00:30:18.919
Both are avenues that we look at, but we don't
believe there's like one clear winner at this

00:30:18.919 --> 00:30:19.919
moment.

00:30:19.919 --> 00:30:26.000
Craig Smith: 30:21
But you, you have a foundation model in operation.

00:30:26.000 --> 00:30:30.390
So what's the, what's the, the architecture?

00:30:30.390 --> 00:30:39.730
Can you describe how that foundation model
works, whether it's Transformer based, yeah,

00:30:39.730 --> 00:30:40.730
just.

00:30:40.730 --> 00:30:44.909
Peter Chen: 30:43
So I think there are really two questions

00:30:44.909 --> 00:30:50.179
in there, like one is like what does well,
like what does role model look like?

00:30:50.179 --> 00:30:54.390
And like what are the other parts of foundation
models like in the specific architectures

00:30:54.390 --> 00:30:56.870
that I use there?

00:30:56.870 --> 00:31:03.830
So the specific role model that we have, I
would say it's more similar to the latent

00:31:03.830 --> 00:31:08.310
type of representation, like there's a more
compact, more abstract representation of the

00:31:08.310 --> 00:31:16.809
world that's operating on and and we believe
like that likely would continue to be the

00:31:16.809 --> 00:31:17.809
case.

00:31:17.809 --> 00:31:23.289
Like just because, like operating in pure
pixels of images and videos, it's a pretty

00:31:23.289 --> 00:31:25.269
wasteful representation to look at.

00:31:25.269 --> 00:31:29.840
Like when you think about, oh, if I don't
grab something in a stable way and it drops,

00:31:29.840 --> 00:31:34.100
like you in your head, you don't try to predict
where each pixel would go right, like you

00:31:34.100 --> 00:31:37.889
have this high level notion of oh, something
would drop.

00:31:37.889 --> 00:31:41.010
So we think, likely , that it will be more
successful.

00:31:41.010 --> 00:31:43.280
But we are pretty open minded about it.

00:31:43.280 --> 00:31:48.230
In terms of the second question of what are
the specific model architectures that are

00:31:48.230 --> 00:31:57.730
used in Our foundation models is a pretty
wide set of architectures.

00:31:57.730 --> 00:32:03.490
Is not a pure transformer based architecture,
like so it's not all attention blocks throughout.

00:32:03.490 --> 00:32:11.909
Like it's a combination of, it's a combination
of convolution attention and, in some places,

00:32:11.909 --> 00:32:17.720
more structured type of attention, like graphical
neural nets, like for specific places that

00:32:17.720 --> 00:32:19.620
make sense.

00:32:19.620 --> 00:32:30.289
So I would say, like the key insight there
is in robotics, inference, cost and speed

00:32:30.289 --> 00:32:31.289
are very important.

00:32:31.289 --> 00:32:38.649
Right, because, unlike in a maybe a different
World like, where it's okay for my like next

00:32:38.649 --> 00:32:43.190
sentence prediction to be a little bit slow,
in robotics, like you need your robots to

00:32:43.190 --> 00:32:49.019
be acting all the time, like that means like
you have to really optimised your models to

00:32:49.019 --> 00:32:54.740
give output very quickly and so the robots
can take actions continuously.

00:32:54.740 --> 00:33:01.150
And because of that, we spend a lot of work
on not just blindly following the biggest,

00:33:01.150 --> 00:33:06.169
most expressive architecture, but really trying
to use the domain understanding that we have

00:33:06.169 --> 00:33:12.740
about robotics and really optimising the architecture
to be more latency sensitive, like to be more

00:33:12.740 --> 00:33:14.480
compute, budget sensitive.

00:33:14.480 --> 00:33:26.910
Craig Smith: 33:20
And you have to adapt this model to whatever

00:33:26.910 --> 00:33:37.780
hardware it's running on, and we talked before
about the hardware constraints in robotics.

00:33:37.780 --> 00:33:43.310
Does the world model your foundation model?

00:33:43.310 --> 00:33:48.409
Does it for each specific robot that it's
controlling?

00:33:48.409 --> 00:34:02.840
The robot has a goal or a policy, and does
the model have to take into account the hardware

00:34:02.840 --> 00:34:03.840
configuration?

00:34:03.840 --> 00:34:04.840
Certainly, yeah.

00:34:04.840 --> 00:34:10.221
Peter Chen: 34:07
So think about in the large language model

00:34:10.221 --> 00:34:20.770
world it's very common to use system prompts
to configure the character, the tone or the

00:34:20.770 --> 00:34:25.629
styles of a language agent.

00:34:25.629 --> 00:34:33.020
In our world you would think of it as the
equivalent of prompting to basically instruct

00:34:33.020 --> 00:34:38.031
a robotic foundation model to know, well,
what kind of hardware am I using and what

00:34:38.031 --> 00:34:42.980
are the things that I can do with my current
hardware body.

00:34:42.980 --> 00:34:49.730
So think of it as one base model, but then
on top of that you add configurations or prompting

00:34:49.730 --> 00:34:52.169
that actually instruct the model on.

00:34:52.169 --> 00:34:58.270
Okay, you're now using this kind of hardware
body and this is what you should do with it.

00:34:58.270 --> 00:35:08.000
Craig Smith: 35:01
You've been working with this foundation model

00:35:08.000 --> 00:35:13.210
as a world model since the founding of covariant.

00:35:13.210 --> 00:35:22.500
Has the research moved significantly from
when you started?

00:35:22.500 --> 00:35:24.170
From when you started?

00:35:24.170 --> 00:35:26.660
Oh, certainly yeah.

00:35:26.660 --> 00:35:35.130
I'm reading a lot now about LLM-based agents
and all of this world model.

00:35:35.130 --> 00:35:39.440
I've followed Coons research and it's really
progressed a lot.

00:35:39.440 --> 00:35:40.440
Certainly.

00:35:40.440 --> 00:35:45.359
Peter Chen: 35:40
Yeah, there are many things that are accelerating

00:35:45.359 --> 00:35:47.010
at a very fast rate.

00:35:47.010 --> 00:35:50.849
One core thing is computers.

00:35:50.849 --> 00:35:54.930
When we started six years ago, the amount
of computers that you could have access to

00:35:54.930 --> 00:35:59.700
is very different from the amount of computers
I can have access to today.

00:35:59.700 --> 00:36:07.080
As computer availability goes up, you can
train bigger models that have more expressivity.

00:36:07.080 --> 00:36:10.450
And then the second thing is data scale.

00:36:10.450 --> 00:36:14.420
When we started as a company, we had no production
customers.

00:36:14.420 --> 00:36:20.770
Now we have robots running autonomously 24-7
on three different continents, so the data

00:36:20.770 --> 00:36:24.000
that we can generate is just at a completely
different scale.

00:36:24.000 --> 00:36:30.280
And then the last one is that the field has
also moved a lot very quickly.

00:36:30.280 --> 00:36:36.079
So, if you think about a lot of the really
scaled up transformer architectures, how do

00:36:36.079 --> 00:36:37.920
you do large scale training?

00:36:37.920 --> 00:36:41.610
How do you do image generation by diffusion?

00:36:41.610 --> 00:36:46.530
A lot of things have happened in the last
couple years that also enable us to build

00:36:46.530 --> 00:36:51.569
more sophisticated foundation models and world
models, because I think the key thing to recognize

00:36:51.569 --> 00:36:57.440
is that a lot of these ideas that we are talking
about like whether it's foundation model or

00:36:57.440 --> 00:37:03.710
world models, they have many different levels
of potential expressivity.

00:37:03.710 --> 00:37:12.510
So, for example, the most rudimentary form
of world model might only be able to allow

00:37:12.510 --> 00:37:18.480
you to predict whether I have successfully
grasped a certain item or not.

00:37:18.480 --> 00:37:22.200
That is also a world model, but it's just
a world model that's restricted to understanding

00:37:22.200 --> 00:37:25.440
whether I have successfully grasped an item.

00:37:25.440 --> 00:37:30.660
A much more expressive form of world model
could be well, if I have a cylindrical object

00:37:30.660 --> 00:37:35.130
in front of me and if I push it a little bit,
it would roll around, and if it's on a slanted

00:37:35.130 --> 00:37:39.470
surface in my rollback, that is a much more
sophisticated kind of world model compared

00:37:39.470 --> 00:37:43.810
to a world model that is only tailor specific
to one small use case.

00:37:43.810 --> 00:37:49.100
So I would say, because of the three forces
that have happened, like the more compute

00:37:49.100 --> 00:37:55.890
that is available, the more data that is being
generated by our production fleet and the

00:37:55.890 --> 00:38:03.060
AI field's advances have together allow us
to build progressively more powerful foundation

00:38:03.060 --> 00:38:06.619
models, and part of that would be the world
model.

00:38:06.619 --> 00:38:12.910
And these progressively more powerful models
can allow existing applications to perform

00:38:12.910 --> 00:38:18.300
better but also open up the possibilities
for newer types of applications.

00:38:18.300 --> 00:38:23.150
So I would say, in the world of building foundation
models for robotics, we are seeing a very

00:38:23.150 --> 00:38:26.599
similar trend to what we are seeing in the
large language model world.

00:38:26.599 --> 00:38:30.550
So you look at the difference between GPT-2
to GPT-3 to GPT-4.

00:38:30.550 --> 00:38:37.890
There are remarkable differences, like as
you scale up compute data techniques and you

00:38:37.890 --> 00:38:43.829
get greater capabilities out of it, even though
you can argue they are all the same idea of

00:38:43.829 --> 00:38:46.450
transformer plus next token prediction.

00:38:46.450 --> 00:38:52.400
But as you do those things, you get qualitatively
different results and it enables orders of

00:38:52.400 --> 00:38:54.359
magnitude more applications.

00:38:54.359 --> 00:39:01.079
Craig Smith: 38:55
Yeah, covariance robots, even though they're

00:39:01.079 --> 00:39:07.710
different form factors, operating controlled
environments, relatively controlled environments,

00:39:07.710 --> 00:39:21.349
and the degree of randomness or unpredictable
ability is within a fairly narrow margin.

00:39:21.349 --> 00:39:30.180
How long do you think before a world model
controlling a robot or a foundation model

00:39:30.180 --> 00:39:38.000
wouldn't only be a world model that can really
operate in a real-world environment, unstructured,

00:39:38.000 --> 00:39:39.000
uncontrolled?

00:39:39.000 --> 00:39:43.380
Peter Chen: 39:40
It's a really good question.

00:39:43.380 --> 00:39:49.190
I wouldn't call the current environment fully
structured, like the robots, like where the

00:39:49.190 --> 00:39:50.440
robots are operating in.

00:39:50.440 --> 00:39:55.869
If you think about all the objects that we
see and manipulate in our day-to-day life,

00:39:55.869 --> 00:39:59.720
they go through a warehouse at some point.

00:39:59.720 --> 00:40:05.880
If the covariant brain-powered, what we call
our foundation model, the covariant brain-powered

00:40:05.880 --> 00:40:11.260
robots operating in all warehouses, it's building
a pretty sophisticated understanding of how

00:40:11.260 --> 00:40:17.750
you manipulate things, even in the fully unstructured
world.

00:40:17.750 --> 00:40:25.150
I think maybe the question is getting more
at when we can have robots that move around

00:40:25.150 --> 00:40:33.180
and are fully in the wild, as opposed to in
confined space in industrial processes that

00:40:33.180 --> 00:40:35.770
are high volume?

00:40:35.770 --> 00:40:42.690
I think that mostly is going to become a hardware
question as opposed to an AI question.

00:40:42.690 --> 00:40:47.800
I actually believe hardware is going to be
the long pole in the tent, as opposed to whether

00:40:47.800 --> 00:40:57.310
you can build the AI layer that can navigate
in a less structured world freely.

00:40:57.310 --> 00:41:04.480
There are a lot of companies and efforts working
on this, like humanoid robots, maybe humanoid

00:41:04.480 --> 00:41:09.820
with wheels, maybe humanoid with legs or maybe
different kinds of form factors that actually

00:41:09.820 --> 00:41:14.099
allow you to navigate freely in space and
also do useful things with it.

00:41:14.099 --> 00:41:18.700
I think there are a lot of interesting hardware
problems to be solved there.

00:41:18.700 --> 00:41:26.900
Craig Smith: 41:19
Yeah, on the warehouse, for example, on the

00:41:26.900 --> 00:41:33.670
hardware side, we were talking about `Wave
AI.

00:41:33.670 --> 00:41:40.839
The car is a robot and it's well refined.

00:41:40.839 --> 00:41:50.190
How long do you think before world models
can be applied to autonomous vehicles with

00:41:50.190 --> 00:42:00.440
such success and stability that we can use
them on the roads today?

00:42:00.440 --> 00:42:02.840
Or is that really a regulatory issue?

00:42:02.840 --> 00:42:05.810
Peter Chen: 42:03
I think this question is much better answered

00:42:05.810 --> 00:42:08.310
by the self-driving car experts.

00:42:08.310 --> 00:42:14.160
Craig Smith: 42:11
Let me ask a warehouse question.

00:42:14.160 --> 00:42:17.430
I know we're coming up to the end of the hour.

00:42:17.430 --> 00:42:26.520
I remember when I first met you guys and we
were talking about the advantage of automated

00:42:26.520 --> 00:42:28.880
warehouses.

00:42:28.880 --> 00:42:34.710
One thing that fascinated me is, as you said,
they operate 24-7.

00:42:34.710 --> 00:42:38.020
They can operate in low light environments.

00:42:38.020 --> 00:42:43.250
They don't need the air quality that humans
need.

00:42:43.250 --> 00:42:49.230
I mean, do you have a vision for and, as you
said, almost every object that we touch has

00:42:49.230 --> 00:42:56.089
come through a warehouse do you have a vision
for warehouses of the future that maybe will

00:42:56.089 --> 00:43:08.730
be these vast underground spaces where robots
are working tirelessly in the dark and a human

00:43:08.730 --> 00:43:16.829
only has to go down periodically when there's
some glitch or some hardware problem?

00:43:16.829 --> 00:43:25.140
Peter Chen: 43:22
I think it's going to be more of a continuum.

00:43:25.140 --> 00:43:28.000
It's not going to be fully light-soul.

00:43:28.000 --> 00:43:29.079
What is the extreme of that?

00:43:29.079 --> 00:43:34.079
The extreme of that is, you can think about
the whole moon being colonised by robots.

00:43:34.079 --> 00:43:39.190
There's no human being on it, but there's
a space factory there that keeps pumping out

00:43:39.190 --> 00:43:43.640
goods and they get sent to the earth autonomously.

00:43:43.640 --> 00:43:45.660
That's one end of the extreme.

00:43:45.660 --> 00:43:48.830
It truly lights out no human touch point.

00:43:48.830 --> 00:43:50.580
I think that's pretty far away.

00:43:50.580 --> 00:43:58.550
Ultimately we will get there, but what we
believe is more gradual adoption and you can

00:43:58.550 --> 00:44:03.530
already start adopting this type of technology
today.

00:44:03.530 --> 00:44:11.400
For a long time this form of technology would
be adopted in the form of human augmentation.

00:44:11.400 --> 00:44:15.950
I have to stand there and pump out 500 units
of goods a day.

00:44:15.950 --> 00:44:23.890
I can now supervise 10 robots that are producing
5,000, 6,000, 7,000 units of goods.

00:44:23.890 --> 00:44:33.010
Today I have a much more engaging job of overseeing
10 robots and looking at where they get stuck.

00:44:33.010 --> 00:44:40.850
How can I arrange the upstream goods coming
in, organise it in a way that allows me to

00:44:40.850 --> 00:44:50.370
get more output out of this fleet of robots,
and figuring out how can I unblock the robots

00:44:50.370 --> 00:44:54.809
as quickly as possible as opposed to just
doing the same motion again and again like

00:44:54.809 --> 00:44:57.440
8 hours a day.

00:44:57.440 --> 00:45:03.230
This form of augmentation we see as a way
to solve the labour challenges that our customers

00:45:03.230 --> 00:45:04.230
have.

00:45:04.230 --> 00:45:09.849
It allows them to do much more with a much
smaller pool of labour, as well as have a

00:45:09.849 --> 00:45:17.010
much better human experience for people that
are working in this vitally important societal

00:45:17.010 --> 00:45:18.310
infrastructure.

00:45:18.310 --> 00:45:22.690
Now you actually have a job that is a lot
more fun, engaging and also way more productive

00:45:22.690 --> 00:45:23.720
than before.

00:45:23.720 --> 00:45:30.530
Over time, as the technology becomes better,
you can see that ratio chips improving.

00:45:30.530 --> 00:45:34.470
Maybe now it's like one person overseeing
a fleet of 10 robots.

00:45:34.470 --> 00:45:36.400
In the future it would be 50, 100.

00:45:36.400 --> 00:45:40.250
At some point you would have a whole factory
of robots and maybe just one person working

00:45:40.250 --> 00:45:42.150
around in it.

00:45:42.150 --> 00:45:47.160
Maybe at some point in the future we would
get to that oh, there's like 2 billion space

00:45:47.160 --> 00:45:50.110
factories on the moon and no one needs to
touch that.

00:45:50.110 --> 00:45:52.910
Craig Smith: 45:51
Yeah, okay.

00:45:52.910 --> 00:46:01.490
Well, let me ask one more question, because
when we first were talking during the pandemic,

00:46:01.490 --> 00:46:12.750
there was a lot I don't know whether it was
you or one of your partners I had asked whether

00:46:12.750 --> 00:46:17.790
robots could operate a chicken processing
factory.

00:46:17.790 --> 00:46:29.530
I mean, certainly there is some automation
there, but there was so much death from COVID

00:46:29.530 --> 00:46:40.450
on these production lines, but the hardware
wasn't versatile enough at that point to be

00:46:40.450 --> 00:46:48.980
able to deal with something as soft and floppy
as a chicken carcass.

00:46:48.980 --> 00:46:50.490
Is that?

00:46:50.490 --> 00:46:52.109
Are we on the road to that?

00:46:52.109 --> 00:46:57.849
And then my final, final question is do you
feel you talked about a continuum?

00:46:57.849 --> 00:47:08.190
Are we close to a step change in robotics
or are we still on just a pretty steady incline?

00:47:08.190 --> 00:47:12.160
Peter Chen: 47:10
So I don't actually have context on the chicken

00:47:12.160 --> 00:47:17.109
processing question, so I would maybe not
answer that specifically.

00:47:17.109 --> 00:47:26.290
But on the idea of making progress on manipulating
more dexterous objects, like deformable objects,

00:47:26.290 --> 00:47:29.300
in a more dexterous way, the answer is definitely
yes.

00:47:29.300 --> 00:47:35.210
So when we look at our systems and the capabilities
that they have, it's definitely understanding

00:47:35.210 --> 00:47:39.440
the physical world in a more and more nuanced
way and in a more and more expressive way,

00:47:39.440 --> 00:47:42.190
and so, like we're on a way there.

00:47:42.190 --> 00:47:53.619
And then, in terms of the questions of whether
we see a step change in robotics, I will say

00:47:53.619 --> 00:47:56.230
the most honest answer is we don't fully know.

00:47:56.230 --> 00:47:59.869
Like there are some forces that are working
for it and then there are some forces that

00:47:59.869 --> 00:48:01.040
are working against it.

00:48:01.040 --> 00:48:06.069
So let me tell you the forces that are working
for the huge inflection acceleration, the

00:48:06.069 --> 00:48:11.750
forces that are working for it is we are getting
to the point that you can really scale up

00:48:11.750 --> 00:48:18.579
compute and data to train really large robotic
foundation models, and we have seen this kind

00:48:18.579 --> 00:48:25.200
of step change, like this phase transition
of capabilities in the language world as you

00:48:25.200 --> 00:48:32.349
scale to a certain size of model, compute
and data, and we expect the same thing to

00:48:32.349 --> 00:48:38.150
happen, right, like we expect, like the foundation
models that power these robots to get significantly

00:48:38.150 --> 00:48:40.290
smarter, to get significantly more general.

00:48:40.290 --> 00:48:45.010
Like I mean, I cannot comment on what would
happen outside of covariant, but at least

00:48:45.010 --> 00:48:46.770
at covariant, like we are seeing that coming.

00:48:46.770 --> 00:48:54.840
Like how do you scale up data compute model
size significantly to get a much smarter model?

00:48:54.840 --> 00:49:02.650
The forces that are working against it is
the adoption still needs to go through hardware

00:49:02.650 --> 00:49:07.890
and it still needs to go through an enterprise
adoption process Like this is not like chat

00:49:07.890 --> 00:49:12.119
GBT, that you can like largely turn on the
faucet and or maybe not turn on the faucet

00:49:12.119 --> 00:49:17.360
but like go to a website and then you can
use it Like so like the adoption process would

00:49:17.360 --> 00:49:24.000
be slower, but I would say in terms of the
capability leapfrog, like we think we are

00:49:24.000 --> 00:49:27.070
very close to a very clear phase transition.

00:49:27.070 --> 00:49:31.150
Craig Smith: 49:28
AI might be the most important new computer

00:49:31.150 --> 00:49:32.770
technology ever.

00:49:32.770 --> 00:49:38.100
It's storming every industry and literally
billions of dollars are being invested, so

00:49:38.100 --> 00:49:39.440
buckle up.

00:49:39.440 --> 00:49:43.859
The problem is that AI needs a lot of speed
and processing power.

00:49:43.859 --> 00:49:48.349
So how do you compete without cost spiralling
out of control?

00:49:48.349 --> 00:49:55.030
It's time to upgrade to the next generation
of the cloud Oracle Cloud Infrastructure,

00:49:55.030 --> 00:49:56.599
or OCI.

00:49:56.599 --> 00:50:04.850
OCI is a single platform for your infrastructure,
database application development and AI needs.

00:50:04.850 --> 00:50:11.599
Oci has four to eight times the bandwidth
of other clouds, and offers one consistent

00:50:11.599 --> 00:50:14.700
price instead of variable regional pricing.

00:50:14.700 --> 00:50:20.460
And, of course, nobody does data better than
Oracle.

00:50:20.460 --> 00:50:27.290
So now you can train your AI models at twice
the speed and less than half the cost of other

00:50:27.290 --> 00:50:28.290
clouds.

00:50:28.290 --> 00:50:35.599
If you want to do more and spend less, like
Uber, 8x8, and Databricks Mosaic,

00:50:35.599 --> 00:50:42.619
take a free test drive of OCI at oracle.com/eyeonai

00:50:42.619 --> 00:50:52.380
That's E-Y-E-O-N-A-I all run together with
Oracle.com/eyeonai

00:50:52.380 --> 00:50:58.760
That's oracle.com/eyeonai

00:50:58.760 --> 00:50:59.880
That's it for this episode.

00:50:59.880 --> 00:51:03.280
I want to thank Peter for his time.

00:51:03.280 --> 00:51:11.450
If you want to read a transcript of the conversation
today, you can find one on our website eye-on.ai

00:51:11.450 --> 00:51:21.520
And remember the Singularity may not be near,
but AI is already changing your world, so

00:51:21.520 --> 00:51:26.589
pay attention.

