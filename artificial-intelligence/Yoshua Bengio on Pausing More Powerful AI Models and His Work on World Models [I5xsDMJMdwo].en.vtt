WEBVTT
Kind: captions
Language: en

00:00:00.149 --> 00:00:07.890
CRAIG: Hi, I'm Craig Smith and this is Eye
on AI. This week I talk to Yoshua Bengio,

00:00:07.890 --> 00:00:13.830
one of the founders of deep learning about
augmenting large language models with world

00:00:13.830 --> 00:00:22.289
models and inference machines that would give
AI systems the ability to reason on reality.

00:00:22.289 --> 00:00:28.609
We also talked about the famous pause letter
of which Yoshua was perhaps the most prominent signer.

00:00:28.609 --> 00:00:33.590
 
CRAIG: We are sponsored this week by NetSuite,

00:00:33.590 --> 00:00:40.690
Oracle's cloud-based enterprise resource planning
software to help any business manage their

00:00:40.690 --> 00:00:46.699
financials, operations, and customer relationships
in a single platform.

00:00:46.699 --> 00:00:53.690
For the first time in NetSuite's, 22
years as the number one cloud financial system,

00:00:53.690 --> 00:01:01.330
you can defer payments on a full implementation
of NetSuite for six months. No payment. 

00:01:01.330 --> 00:01:08.370
No interest for six months. Take advantage of
this special financing offer at www.netsuite.com/eyeonai.

00:01:08.370 --> 00:01:13.560
It's an unprecedented offer.
CRAIG: Hey, how are you?

00:01:13.560 --> 00:01:22.110
YOSHUA: Very good. 

00:01:22.110 --> 00:01:49.590
CRAIG: so I know you don't

00:01:49.590 --> 00:01:57.990
have much time. I'm going to jump right into
it. First of all, on, on the letter specifically,

00:01:57.990 --> 00:02:05.200
the letter has triggered this FTC complaint.
It was made by the Center for Artificial Intelligence

00:02:05.200 --> 00:02:12.400
and Digital Policy. I don't know much about
them, but it was a complaint to the United

00:02:12.400 --> 00:02:23.620
States FTC, asking them to stop OpenAI from
releasing any more powerful model. It follows

00:02:23.620 --> 00:02:27.849
on the letter.
The first question I have is your name

00:02:27.849 --> 00:02:35.989
to me at least stood out among the signatories
because you are known for being very measured

00:02:35.989 --> 00:02:48.930
in your comments and very reasoned in your
view of the risks of AI and your skepticism

00:02:48.930 --> 00:02:59.570
about the advent of AGI or human level intelligence
anytime in the near future. ChatGPT and GPT-4's

00:02:59.570 --> 00:03:08.069
capabilities took even people within the
community by surprise, but certainly the general

00:03:08.069 --> 00:03:14.129
public. So why did you sign the letter? Many
of your colleagues, Geoff Hinton and Yann

00:03:14.129 --> 00:03:20.480
LeCun, I could go on and on, did not sign
it. The people who did sign it, the prominent

00:03:20.480 --> 00:03:31.950
names who signed it Gary Marcus, Max Tegmark,
these guys are known to be very vocal in their

00:03:31.950 --> 00:03:36.630
concerns much more so than you. So that's
the first question.

00:03:36.630 --> 00:03:46.299
YOSHUA: I've been speaking for many years
about the long-term danger to society of having

00:03:46.299 --> 00:03:55.900
very powerful tools at our disposal. It's
like any technology in the sense the more

00:03:55.900 --> 00:04:00.200
powerful they are, the more useful they can
be, but also the more dangerous they can be

00:04:00.200 --> 00:04:04.370
if they're misused.
YOSHUA: And I had the impression for many

00:04:04.370 --> 00:04:12.730
years that the way our society's organized
in each country, but globally in general,

00:04:12.730 --> 00:04:20.030
is not adequate to face the challenges that
very powerful technologies bring, and in particular

00:04:20.030 --> 00:04:25.371
AI, but I'm thinking of biotechnology in the
future, which is something great that I

00:04:25.371 --> 00:04:29.130
think can be incredibly useful for humanity
as well.

00:04:29.130 --> 00:04:38.970
YOSHUA: But, but the more powerful it is,
 the more wisdom we need in the

00:04:38.970 --> 00:04:44.440
way we deal with this. And right now, for
example, the system of competition between

00:04:44.440 --> 00:04:53.090
companies as we are seeing now accelerating
with these large language models has benefits,

00:04:53.090 --> 00:05:00.020
potentially. This has, you know, driven innovation
 in some ways in the last century.

00:05:00.020 --> 00:05:07.850
YOSHUA: But also, it means that companies
are a bit in a haste and may not take the

00:05:07.850 --> 00:05:17.940
precautions that would otherwise be warranted.
So, in the short term, I think we need to

00:05:17.940 --> 00:05:26.690
accelerate the countermeasures
to reduce risks, and that's regulation. Very

00:05:26.690 --> 00:05:33.980
simple. We do that for many other areas. You
know, almost every industrial area is highly

00:05:33.980 --> 00:05:36.410
regulated when it touches the public, you know,

00:05:36.410 --> 00:05:46.260
whether it's airplanes or chemistry
or drugs or, They're all heavily regulated,

00:05:46.260 --> 00:05:54.170
not so much computing and, and AI within that.
And because these things are becoming very

00:05:54.170 --> 00:06:00.700
powerful, I think it's urgent we kind of
really change gear here. Now why did I sign?

00:06:00.700 --> 00:06:06.320
Like why now? Like if maybe last year I wouldn't
have signed this letter, it's because we now

00:06:06.320 --> 00:06:14.630
reached that threshold. The threshold is the Turing test, meaning that we have systems that

00:06:14.630 --> 00:06:21.840
we can dialogue with, and we can't be sure
if this is coming from a machine or a human.

00:06:21.840 --> 00:06:26.479
And that could be exploited in, in highly
dangerous ways that can threaten democracy.

00:06:26.479 --> 00:06:33.310
and I care about democracy.
CRAIG: Yeah. An interesting aside is the potential

00:06:33.310 --> 00:06:40.690
that AI has for enhancing democracy.
YOSHUA: absolutely like any other thing that

00:06:40.690 --> 00:06:48.980
AI can do. It could be very bad or very good,
but right now, where are the investments going?

00:06:48.980 --> 00:06:52.860
How much investment is there in like AI tools
for democracy?

00:06:52.860 --> 00:06:58.759
YOSHUA: Not so much because it's not clear
where the profit is going to be. And

00:06:58.759 --> 00:07:03.410
it's true for other areas of social importance
in healthcare. There is of course, quite a

00:07:03.410 --> 00:07:08.569
lot, quite a bit, but it's, it's minuscule
compared to what is going on in, you know,

00:07:08.569 --> 00:07:14.310
what is invested by large companies who
care about advertising recommendations, search

00:07:14.310 --> 00:07:20.229
engines, and, and so on.
CRAIG: You said that there's a need to accelerate

00:07:20.229 --> 00:07:27.210
regulation, that I think everybody agrees
with. But the letter didn't call simply for

00:07:27.210 --> 00:07:35.220
accelerated regulation. It called for this
pause. Was that meant as many people believe,

00:07:35.220 --> 00:07:41.840
more to get attention because  the
reality - although I saw Chelsea Finn on Twitter

00:07:41.840 --> 00:07:48.760
saying she was going to stop work on anything
that pointedtoward AGI -

00:07:48.760 --> 00:07:57.169
but is that, is that pause, was it
meant to be realistic or is it simply a way

00:07:57.169 --> 00:08:04.669
to emphasize the urgency?
YOSHUA: the chances that a request like this,

00:08:04.669 --> 00:08:11.030
you know, be followed by, by these companies
was small. And, I, I, I don't, I don't have

00:08:11.030 --> 00:08:18.680
high hopes, but the important thing indeed
is to say that we need to coordinate and the,

00:08:18.680 --> 00:08:23.550
the short term coordination that can happen
is between companies, maybe a few companies

00:08:23.550 --> 00:08:31.620
decide well, let's take a pause to improve
our, you know, testing documentation, you

00:08:31.620 --> 00:08:40.899
know, ethics studies around what we're doing.
But really, at the end of the day, it

00:08:40.899 --> 00:08:44.350
has to be in the hands of government that's
going to make sure that every company's going

00:08:44.350 --> 00:08:50.120
to do it. And even, you know, more difficult,
but also, but, but essential is that at the

00:08:50.120 --> 00:08:53.980
end of the day, it has to be international.
It has to be international treaties

00:08:53.980 --> 00:09:01.269
where the main countries that can do these
things agree just like we've done in the past

00:09:01.269 --> 00:09:08.870
for many international treaties where there
are risks that could be global. From technology.

00:09:08.870 --> 00:09:14.500
CRAIG: And then on this FTC complaint, and
I know that you haven't read it, so I can't

00:09:14.500 --> 00:09:22.209
really expect you to, to comment on it in
detail but since the letter was released,

00:09:22.209 --> 00:09:28.550
is there a plan for presenting governments with legislative

00:09:28.550 --> 00:09:35.149
language or regulatory language from Future
of Life Institute or, or

00:09:35.149 --> 00:09:39.360
YOSHUA: I'm not representing
the Future of Life Institute. Yeah, no, I

00:09:39.360 --> 00:09:45.950
understand. and I didn't even write that letter.
I was sent it and asked if I would,

00:09:45.950 --> 00:09:50.800
I would sign it and I said yes.
YOSHUA: I proposed some changes. It didn't

00:09:50.800 --> 00:09:56.060
happen, but I decided to sign anyways.
CRAIG: Well, let me put it this way. Is there

00:09:56.060 --> 00:10:02.980
a movement among people that sign the letter,
support the letter? I mean particularly the,

00:10:02.980 --> 00:10:11.910
the most prominent people to pursue regulation
with their independent governments. Is there

00:10:11.910 --> 00:10:17.779
any coordination at.
YOSHUA: There is not necessarily among the

00:10:17.779 --> 00:10:22.740
people who signed. I don't think it's organized
that way, but there are lots of groups of

00:10:22.740 --> 00:10:27.450
people who have been working on this in the
last few years. for your information, the

00:10:27.450 --> 00:10:31.329
Canadian government is moving forward, a legislation
that's probably going to be the first in the

00:10:31.329 --> 00:10:37.360
world to be voted around AI probably this
spring.

00:10:37.360 --> 00:10:43.980
YOSHUA: Europe has been working on this for
several years. it should come in 2023. So,

00:10:43.980 --> 00:10:51.760
loads and loads of scholars, scientists and,
you know, government policy, public policy

00:10:51.760 --> 00:10:59.410
people have been working on this for years.
 I've been involved, in the creation and,

00:10:59.410 --> 00:11:07.160
and as for two years as the co-chair of a
working group of the Global Partnership on

00:11:07.160 --> 00:11:11.541
AI that's a new organization has been created,
by France and Canada, which now has about,

00:11:11.541 --> 00:11:19.380
has about 30 countries. it, it has connections
to OECD and it's all about

00:11:19.380 --> 00:11:24.940
international coordination around AI. And
that working group was the working group on

00:11:24.940 --> 00:11:30.899
responsible AI. And a lot of the things we
discussed for a few years now are precisely

00:11:30.899 --> 00:11:37.250
these things.
And in fact, I just witnessed an exchange

00:11:37.250 --> 00:11:48.899
about very kind of precise suggestions regarding,
following up on the letter, so regarding watermarking

00:11:48.899 --> 00:11:58.240
for example, and mandatory display of the
origin of the content. Is it human or if it's

00:11:58.240 --> 00:12:04.160
images, like was it really recorded or is
it generated by machines, right.

00:12:04.160 --> 00:12:11.240
YOSHUA: so, these are two different things
and they're connected. So first of all, watermarking

00:12:11.240 --> 00:12:18.010
just means that the way that the words are
generated, or the pixels are generated is

00:12:18.010 --> 00:12:22.820
not going to look any different to a human.
But a machine with the right program that

00:12:22.820 --> 00:12:27.470
could be provided by the company that, that
does these things like OpenAI for example

00:12:27.470 --> 00:12:32.170
will be able to very, very easily say this
with very high confidence -

00:12:32.170 --> 00:12:41.150
like 99.999%. Not, not some like guess,
but like really be sure- this was generated

00:12:41.150 --> 00:12:47.050
by the, you know, GPT 4.0. Or, or something
of that.

00:12:47.050 --> 00:12:51.680
CRAIG: Yeah. Let me just jump in though. I'm,
I'm fairly familiar with, with the regulatory

00:12:51.680 --> 00:13:00.699
efforts going on. but the letter didn't call
the headline wasn't you know, speed up regulation.

00:13:00.699 --> 00:13:05.160
CRAIG: It was to pause development.
YOSHUA: No, but look, don't read the headline.

00:13:05.160 --> 00:13:08.050
Read the letter.
CRAIG: Well, this is the problem. This is

00:13:08.050 --> 00:13:16.921
my problem with the letter as, as someone
who's peripherally involved, is that I read

00:13:16.921 --> 00:13:23.920
the letter. My brother, who immediately sent
me a text, of course, didn't read the letter.

00:13:23.920 --> 00:13:28.850
He read the headline.
CRAIG: And the FTC complaint is not about

00:13:28.850 --> 00:13:37.490
speeding up regulation. It's about preventing
OpenAI from releasing more powerful models

00:13:37.490 --> 00:13:45.079
for the time being. So, I mean, do you
support that kind of action that is coming

00:13:45.079 --> 00:13:49.160
out of the letter?
YOSHUA: I agree with the pause.

00:13:49.160 --> 00:13:54.981
YOSHUA: However, I think, I don't think it
should be just OpenAI. And I don't think it

00:13:54.981 --> 00:14:00.910
should be only the United States. So, it's
going to take more time, but, but I, I really

00:14:00.910 --> 00:14:05.970
think that it has to move to the international
arena as quickly as possible. And by the way,

00:14:05.970 --> 00:14:11.560
China is also like probably ahead of the US
in terms of regulation of AI for different

00:14:11.560 --> 00:14:15.529
reasons.
But, but in a sense, they are as concerned

00:14:15.529 --> 00:14:22.840
by the destabilizing effects that these systems
can have on public opinion as democracies

00:14:22.840 --> 00:14:27.449
are for, for different reasons. They don't
want to lose power. We want to make sure that

00:14:27.449 --> 00:14:33.699
the, you know, democratic debate takes place
as it should. And so maybe there's an

00:14:33.699 --> 00:14:41.120
opportunity for an agreement even though it comes

00:14:41.120 --> 00:14:45.300
from different angles.
CRAIG: Yeah. Okay. I don't want to get

00:14:45.300 --> 00:14:54.600
too, too hung up on, on the letter.
I prefer talking about the research

00:14:54.600 --> 00:15:00.630
and the, and the developments. I mean, it
has been impressive what's come out of the

00:15:00.630 --> 00:15:08.399
scaling of transformer models.
The only constraint at this point

00:15:08.399 --> 00:15:16.150
seems to be money and, and electric power.
Some expertise and, and expertise. That's

00:15:16.150 --> 00:15:23.630
right. have but, but there are things missing
in the model, and that's what I've been focused

00:15:23.630 --> 00:15:27.220
on in talking to people.
YOSHUA: That's what I focused my research

00:15:27.220 --> 00:15:29.529
on.
CRAIG: Yes. So that's what I wanted to talk

00:15:29.529 --> 00:15:33.060
about.
So, on the drawbacks there's of course

00:15:33.060 --> 00:15:40.190
the hallucination problem that OpenAI is working
on with reinforcement learning with human feedback.

00:15:40.190 --> 00:15:47.060
CRAIG: I had a very interesting conversation
with Yann LeCun about his view that you have

00:15:47.060 --> 00:15:51.459
to build a world model that,
YOSHUA: I agree on that.

00:15:51.459 --> 00:15:59.350
CRAIG: Yeah, because these large language
models, they're, grounding in reality is what's

00:15:59.350 --> 00:16:04.500
contained in text. And there's, there's,
YOSHUA: that's not the only reason why we

00:16:04.500 --> 00:16:10.769
need a world model.
YOSHUA: there are other reasons. one reason

00:16:10.769 --> 00:16:21.529
I focus on is because.
humans separate knowledge of the world

00:16:21.529 --> 00:16:30.450
from how to take decisions. And in fact, scientists
do that as well and engineers do that. Just

00:16:30.450 --> 00:16:38.649
to understand that separation, consider a
Go playing system like AlphaGo. The knowledge

00:16:38.649 --> 00:16:45.540
that's needed here really is the rules of
the game, like how you make points and so

00:16:45.540 --> 00:16:47.279
on.
YOSHUA: And, and the fact that you want to

00:16:47.279 --> 00:16:51.770
have as many points, I mean, you want to win
and person who has more points, wins.

00:16:51.770 --> 00:16:56.670
YOSHUA: Once you have that, you don't need
to interact with the rest of the world. So,

00:16:56.670 --> 00:17:03.670
like the world model here would be easy to
get because it's very small. But the machinery

00:17:03.670 --> 00:17:09.380
to take that and turn that into good decisions,
which we call inference in machine learning.

00:17:09.380 --> 00:17:13.179
That's very complex.
YOSHUA: In fact, doing exact inference here,

00:17:13.179 --> 00:17:19.970
like finding the exact right move, the best
move is intractable. It's exponentially hard.

00:17:19.970 --> 00:17:24.640
And that's why we need really, really large
neural nets to do the inference. So AlphaGo

00:17:24.640 --> 00:17:30.789
is just doing inference. I mean, it's learning
to do inference by checking itself with the

00:17:30.789 --> 00:17:32.400
world model.
YOSHUA: It's checking itself because it's

00:17:32.400 --> 00:17:38.419
playing against itself and obeying the rules
in a sense. Right? And right now, if you look

00:17:38.419 --> 00:17:45.730
at something like large language models, there
is no separation between the knowledge we

00:17:45.730 --> 00:17:50.260
are extracting from the world, like the rules
of Go and how we should answer questions.

00:17:50.260 --> 00:17:56.900
YOSHUA: It's the same system that somehow
implicitly has the knowledge and is doing

00:17:56.900 --> 00:18:03.830
the actions like, you know, answering questions.
And let me give you another example here.

00:18:03.830 --> 00:18:11.870
I'm going to illustrate why it could be useful
to, to do that to have that separation. I

00:18:11.870 --> 00:18:17.740
had only one car accident in my life, and
I never drove down a cliff or something.

00:18:17.740 --> 00:18:26.080
YOSHUA: Probably would've died. Instead, I
had lots of thoughts imagining what would

00:18:26.080 --> 00:18:32.190
happen if I, you know, drive over a cliff
or what would happen if I suddenly brake on

00:18:32.190 --> 00:18:40.831
the highway and the person behind me hits
me. But I don't need to try it because I have

00:18:40.831 --> 00:18:46.620
a world model where I can like simulate these
kinds of things at a very abstract level.

00:18:46.620 --> 00:18:52.630
YOSHUA: And so, there are lots of good reasons
why you want that separation. Like the policy

00:18:52.630 --> 00:19:00.669
of how I drive. And my knowledge of how things
like causal knowledge of, you know, what,

00:19:00.669 --> 00:19:04.740
what happens if I do this? What's
going to be the cause-and-effect chain that

00:19:04.740 --> 00:19:13.539
can happen? And science is all like this.
People, like scientists come up with theories,

00:19:13.539 --> 00:19:19.669
they're like, like world models.
YOSHUA: And, once they kind of, you

00:19:19.669 --> 00:19:24.090
know, want to test a theory or, or believe
a theory, then they, you do engineering. Okay,

00:19:24.090 --> 00:19:28.660
so now let's solve for the bridge that's
not going to fall, which is not the same thing

00:19:28.660 --> 00:19:33.260
as like the laws of physics and mechanics
that are involved in the world model,

00:19:33.260 --> 00:19:35.870
right? So, there's a difference between the
two.

00:19:35.870 --> 00:19:41.390
YOSHUA: But right now, in our large neural
nets, trained end to end, which I contributed

00:19:41.390 --> 00:19:48.190
to in many ways. We are not doing that. We,
we have a single big neural net that embodies

00:19:48.190 --> 00:19:56.700
everything somehow. And that's detrimental
in many ways. I think

00:19:56.700 --> 00:20:01.610
it's one of the main causes of overfitting.
For example, because there is no place for

00:20:01.610 --> 00:20:04.690
like a real-world model there's no explicit

00:20:04.690 --> 00:20:12.790
place for reasoning, reasoning causally,
or reasoning, you know, more generally. Of

00:20:12.790 --> 00:20:19.289
course, if you look at ChatGPT, it seems to
kind of reason, but if you, if it has to reason

00:20:19.289 --> 00:20:23.850
over too many steps, it, it tends to get it
wrong. It's not just hallucinating, it's like

00:20:23.850 --> 00:20:29.190
getting it wrong.
YOSHUA: I did a little experiment. I asked

00:20:29.190 --> 00:20:35.780
him to do, additions or multiplication of,
of numbers and, you know, if it's just like

00:20:35.780 --> 00:20:39.720
one-digit numbers, it works very well. If
you have three-digit numbers, it

00:20:39.720 --> 00:20:45.440
can't do it usually. I mean, it, it claims,
it does it, and, and it gives

00:20:45.440 --> 00:20:53.720
you wrong answers or, or wrong explanations.
YOSHUA: which is interesting because to do

00:20:53.720 --> 00:20:57.740
this, you, you have many steps and you can
tell it, oh, do it step by step. Show me how

00:20:57.740 --> 00:21:03.100
you do it, right? Like people have been doing,
and it still gets it wrong. So, it, it's very

00:21:03.100 --> 00:21:08.250
weak in terms of reasoning. So, what, what
reasoning is like reason really, like you

00:21:08.250 --> 00:21:12.029
separate the knowledge from how you use it.

00:21:12.029 --> 00:21:17.420
That reasoning is using the knowledge, combining
pieces of knowledge in sort of a rational,

00:21:17.420 --> 00:21:24.549
coherent way in order to go from A to B. That's
reasoning like planning. Planning is a special

00:21:24.549 --> 00:21:29.580
case of reasoning.
CRAIG: So, in, in your research, how do you,

00:21:29.580 --> 00:21:35.359
at a very high level, how do
you build a world model? I mean, I'm

00:21:35.359 --> 00:21:43.880
sure you know, Yann''s joint embedding predictive
architecture. Yeah.

00:21:43.880 --> 00:21:56.590
YOSHUA: So, we've come up with a new framework
for training inference machines. and they,

00:21:56.590 --> 00:22:05.200
and it can also be used to. To help train
the world model. I don t know what kind of

00:22:05.200 --> 00:22:07.870
people you have in your audience.
YOSHUA: I don t know how much technical I

00:22:07.870 --> 00:22:14.250
can go here.
CRAIG: Go technical because people, their

00:22:14.250 --> 00:22:20.830
eyes glaze over if it's too technical,
but I'd rather have it in for those that that

00:22:20.830 --> 00:22:24.330
follow it.
YOSHUA: So, so we've come up in the last couple

00:22:24.330 --> 00:22:29.940
of years with something called generative
flow networks or GFlowNets. And what they

00:22:29.940 --> 00:22:35.450
do is they learn to do probabilistic inference.
YOSHUA: So, they are neural nets, which could

00:22:35.450 --> 00:22:41.040
be very big potentially, that learn to do
reasoning, essentially through a sequence

00:22:41.040 --> 00:22:46.820
of steps. And after that sequence of steps,
they get a kind of reward. So, they're connected

00:22:46.820 --> 00:22:54.010
to reinforcement learning. If the steps are
coherent with the world knowledge, so the

00:22:54.010 --> 00:23:00.360
world knowledge is given by some other neural
net or some other piece of code if you want,

00:23:00.360 --> 00:23:06.059
that says whether the result makes sense.
YOSHUA: So, you have two things to train.

00:23:06.059 --> 00:23:14.120
You've got the world model that embodies knowledge
and can check whether things are coherent.

00:23:14.120 --> 00:23:18.380
Let me give you an example from, say, mathematics,
that people can understand the, 

00:23:18.380 --> 00:23:25.460
You know more or less what a theorem and
a proof is about? Yep. Right? Yep. So, the

00:23:25.460 --> 00:23:30.490
world model here is just checking that the
proof is correct in the sense that each step

00:23:30.490 --> 00:23:36.031
is logical and coherent, and it uses things,
facts, that are known. Like, you know, if

00:23:36.031 --> 00:23:45.570
you add X plus Y is Z equals to Y plus X.
So, so you're allowed to do that transformation

00:23:45.570 --> 00:23:52.679
and the inference machine is proposing solutions
to a problem like, prove this theory and it

00:23:52.679 --> 00:23:55.230
needs to,  usually through a sequence of steps

00:23:55.230 --> 00:24:00.679
where it uses knowledge and if it messes up,
like it makes up, it hallucinates things along

00:24:00.679 --> 00:24:08.820
the way like ChatGPT does, the reward is
going to be very bad. And so, it learns

00:24:08.820 --> 00:24:16.610
to be coherent.
YOSHUA: And that's not something that is explicitly

00:24:16.610 --> 00:24:21.430
ingrained in current large language models.
They get it because they've seen

00:24:21.430 --> 00:24:26.559
so much text. And humans tend to be coherent,
but not always. And you know, there are lots

00:24:26.559 --> 00:24:31.120
of subtleties here. For example, who's speaking,
maybe I'm speaking not the truth, because

00:24:31.120 --> 00:24:33.940
I'm trying to convince you.
YOSHUA: And so, there are lots of subtleties

00:24:33.940 --> 00:24:38.790
in language that means that it's not always
like the truth that you're seeing right? And

00:24:38.790 --> 00:24:44.120
so, we need machines that understand the notion
of truth in a more fundamental way, I think.

00:24:44.120 --> 00:24:53.529
By the way, this is something that we are
doing with neural nets, but the idea behind

00:24:53.529 --> 00:25:01.490
reasoning and logic as a building block of
AI are from the early days of AI, like, it's

00:25:01.490 --> 00:25:04.620
like classical AI, symbolic AI.
YOSHUA: You know, that's what Gary Marcus

00:25:04.620 --> 00:25:11.910
has been saying. I think that the way that
these guys have been proposing it is not going

00:25:11.910 --> 00:25:23.529
to work. But what can work is to train a neural
net to behave in a coherent - to behave rationally

00:25:23.529 --> 00:25:28.429
as in manipulate pieces of truth together
to come up with proposals for things that

00:25:28.429 --> 00:25:32.450
make sense.
CRAIG: I understand there's the inference

00:25:32.450 --> 00:25:37.660
model, the world model, and then the language
model and the language model...

00:25:37.660 --> 00:25:41.180
YOSHUA: Yes, exactly. The language model is
yet another thing in the brain. The language

00:25:41.180 --> 00:25:45.180
part is quite distinct from -  That's right -
the reasoning part and the math part and,

00:25:45.180 --> 00:25:48.659
and, and you know, the knowledge about action and so on.

00:25:48.659 --> 00:25:53.289
CRAIG: Right. And that's kind of the problem
right now, the language model, it produces

00:25:53.289 --> 00:26:01.300
coherent language. It doesn't really know
whether something is true or not. Yes. the,

00:26:01.300 --> 00:26:08.980
and, in your structure, the world
model would provide that ground truth. and,

00:26:08.980 --> 00:26:14.010
and the inference model would execute reasoning and then the

00:26:14.010 --> 00:26:17.309
language model would express,
YOSHUA: it would, it would do more than execute

00:26:17.309 --> 00:26:23.559
reasoning, because reasoning you have to understand
reasoning is very hard because there are many

00:26:23.559 --> 00:26:30.480
paths you could follow. Think about proving
a theorem. Is it easy? No. Because how, you

00:26:30.480 --> 00:26:32.770
know, which pieces of knowledge do I combine
in what order?

00:26:32.770 --> 00:26:37.591
YOSHUA: There's an exponential number of possibilities.
That's why you need generative models. So, you

00:26:37.591 --> 00:26:42.870
need actually models that are very similar
to what we use for large language models.

00:26:42.870 --> 00:26:47.720
But instead of generating words and imitating
what humans have said, they generate sequences

00:26:47.720 --> 00:26:52.990
of like pieces of, I mean, they, they pick
pieces of knowledge or they, they pick pieces

00:26:52.990 --> 00:26:59.110
of solutions, and then they get rewarded
if, if they do it right.

00:26:59.110 --> 00:27:06.730
YOSHUA: Right. And by the way, the inference
machine is also important to train the model

00:27:06.730 --> 00:27:14.870
itself. So, if, if there are things that are
not given an input, so let, let me give you

00:27:14.870 --> 00:27:19.650
an example. You see an image and you're trying
to come up with an explanation for what's

00:27:19.650 --> 00:27:22.870
in the image. And maybe the image doesn't
come with labels.

00:27:22.870 --> 00:27:27.170
YOSHUA: So, you're trying to do improvised
learning. Like humans look at images and they

00:27:27.170 --> 00:27:33.600
make sense of them. So there, there are well
known principles in, in machine learning to

00:27:33.600 --> 00:27:38.690
deal with that situation where you don't observe
the full story. You have to, they're called

00:27:38.690 --> 00:27:44.480
latent variables, like the Geoff Hinton calls
them hidden variables. The model needs to

00:27:44.480 --> 00:27:51.039
come up with a, like a story that explains
the data. The inference machine is the thing

00:27:51.039 --> 00:27:56.620
that comes up with the story, and then the
world model checks whether the story is consistent

00:27:56.620 --> 00:28:01.100
with the image, and that's how the inference
machine gets better.

00:28:01.100 --> 00:28:06.130
YOSHUA: It's trying to make stories that are
consistent with the model, but when it does

00:28:06.130 --> 00:28:10.279
that, it helps to train the world model as
well, because it, now that you have a story

00:28:10.279 --> 00:28:15.010
and an image, you can see, you can build a
model that makes the right links between them.

00:28:15.010 --> 00:28:21.010
so, a lot of the work that Geoff and others
and, and you know, I did in the last few,

00:28:21.010 --> 00:28:24.840
in the early decades, especially of deep learning
with probabilistic models like Boltzmann machines

00:28:24.840 --> 00:28:30.010
and so on, were based on these ideas, 
but I think that now we can use these large

00:28:30.010 --> 00:28:34.900
neural nets, like the large language models,
but not for language, for reasoning.

00:28:34.900 --> 00:28:39.630
CRAIG: On the world model side though, it
sounds, when you say hearkening back to the

00:28:39.630 --> 00:28:44.460
early days of AI, it sounds like a rule-based
system where you load it up with

00:28:44.460 --> 00:28:51.330
YOSHUA: No, because we are going to learn
the rules as well, and we are going to learn

00:28:51.330 --> 00:28:54.260
them in a probabilistic way.
YOSHUA: So, so here's another aspect that's

00:28:54.260 --> 00:29:03.720
missing from classical AI. Given any finite
amount of data, you can't be a hundred percent

00:29:03.720 --> 00:29:08.970
sure of what is the right world model that
explains the data because there are, may be

00:29:08.970 --> 00:29:13.280
multiple interpretations. It's like in science
you have usually different theories that are

00:29:13.280 --> 00:29:17.000
compatible with the data.
YOSHUA: And what scientists do is try to come

00:29:17.000 --> 00:29:22.910
up with different theories, diverse set of
theories. That's important because if we just

00:29:22.910 --> 00:29:28.429
go for the first theory we find, it might
be completely wrong, even though it fits the

00:29:28.429 --> 00:29:36.279
data. It's, but what if we know that we have
two competing theories? We can be safer. Like

00:29:36.279 --> 00:29:39.659
maybe sometimes they agree and then we should
go that way, and sometimes they disagree,

00:29:39.659 --> 00:29:43.549
and then we should be careful.
YOSHUA: So, one of the problems with things

00:29:43.549 --> 00:29:48.751
like ChatGPT right now and, and many of these
models, it's not just this one, is that they

00:29:48.751 --> 00:29:54.990
are often confidently wrong. It's not just
that they hallucinate. They're sure of what

00:29:54.990 --> 00:30:01.210
they're saying. And of course, that's bad,
right? That can lead to catastrophic outcomes.

00:30:01.210 --> 00:30:05.120
But, and, and I guess there are some humans
who were like that too.

00:30:05.120 --> 00:30:09.880
YOSHUA: But I guess for humans is maybe a
lot is tied to their ego. But when, you know,

00:30:09.880 --> 00:30:17.549
in a, in good circumstances, people have a
good sense that some belief that comes to

00:30:17.549 --> 00:30:22.410
them isn't completely sure and they're actually
good when you ask them to bet. If they have

00:30:22.410 --> 00:30:27.820
to bet money on something, somehow, they'll
exploit the fact that they're not sure to

00:30:27.820 --> 00:30:32.330
bet more or less money.
YOSHUA: Okay? So, our brain is doing that

00:30:32.330 --> 00:30:37.990
calculation. It's not something we verbalize,
but we kind of sense it and then we act accordingly,

00:30:37.990 --> 00:30:43.240
and that is very important. because coming
back the connection to the world model is

00:30:43.240 --> 00:30:49.480
that really what we need to do is not to have
one world model, but to have a distribution

00:30:49.480 --> 00:30:52.809
over world models.
YOSHUA: So, it's not like we have a neural

00:30:52.809 --> 00:30:58.510
net that is the world model. I think that's
the wrong picture. the right picture is we

00:30:58.510 --> 00:31:07.750
have a neural net that generates the
theory that corresponds to the world model,

00:31:07.750 --> 00:31:11.200
just again, the same kind on that I've been
talking about. But now, instead of coming

00:31:11.200 --> 00:31:15.559
up with an explanation for an image, it comes
up with a general theory that explains a lot

00:31:15.559 --> 00:31:18.030
of data.
YOSHUA: And the theory could be something

00:31:18.030 --> 00:31:22.380
that can be verbalized the same way that scientists
come up with theories. So, you see, we are

00:31:22.380 --> 00:31:27.150
talking about things that are related to classical
AI, but in classical AI, not only, there was

00:31:27.150 --> 00:31:30.639
no uncertainty, but it was all handcrafted,
like the theory is a set of rules and facts

00:31:30.639 --> 00:31:35.460
that somebody puts down.
YOSHUA: And you know, maybe that can be part

00:31:35.460 --> 00:31:39.279
of a solution of something, but really many
of these statements, they're not like a hundred

00:31:39.279 --> 00:31:44.070
percent sure and so people are starting to
play with probability and so on. But it, we

00:31:44.070 --> 00:31:49.080
never got it quite right. And the reason we
never got it quite right is that getting it

00:31:49.080 --> 00:31:57.750
right is computationally - it looks computationally
intractable, but it turns out it can be approximated

00:31:57.750 --> 00:32:03.299
by these very large neural nets. They do a
good job at that. So, we can have large neural

00:32:03.299 --> 00:32:10.940
nets that just, just like we do, spit out
theories or, or pieces of theories that are

00:32:10.940 --> 00:32:18.010
relevant to what we're currently seeing. rather
than have one theory that's, you know, hard

00:32:18.010 --> 00:32:21.210
coded.
either by humans or even by a machine.

00:32:21.210 --> 00:32:26.690
And that's like one list that we don't change
and then we trust thereon - that doesn't make

00:32:26.690 --> 00:32:32.340
sense. We keep revising our views and our
views are uncertain. So, we have a distribution

00:32:32.340 --> 00:32:37.710
of our theories, and we don't say, oh, there
are these 10 theories and this, you know,

00:32:37.710 --> 00:32:41.350
this one is 0.8 and this one is 0.6.
YOSHUA: That's not how it works. And the reason

00:32:41.350 --> 00:32:47.210
is there's an exponential number of theories,
so we don't explicitly emulate them. Instead,

00:32:47.210 --> 00:32:51.840
we generate pieces of theories according to
their Bayseian probability. And there's a

00:32:51.840 --> 00:32:57.429
lot of evidence from cognitive science that
people do that. It's, it's happening

00:32:57.429 --> 00:33:00.760
unconsciously because that generative model
is something that's system one.

00:33:00.760 --> 00:33:07.280
YOSHUA: It's, it's behind the scenes. Right.
You don't control it. You just see your thoughts

00:33:07.280 --> 00:33:11.860
coming out.
CRAIG: I can imagine the inference model.

00:33:11.860 --> 00:33:17.789
I mean, there, there's a lot of inference
models out there now. How do you train the

00:33:17.789 --> 00:33:21.450
world model? Is it,
YOSHUA: okay, well that's, that's the cool

00:33:21.450 --> 00:33:23.650
thing.
YOSHUA: If you have the inference machine.

00:33:23.650 --> 00:33:29.929
CRAIG: Oh, sorry. I'll just finish the question - 
because there is a certain amount of reasoning

00:33:29.929 --> 00:33:36.490
in large language models, as you found in
your own research. And so, and that's contained

00:33:36.490 --> 00:33:41.169
in language, but if you want to build a real-world
model, you want to get beyond language.

00:33:41.169 --> 00:33:47.779
CRAIG: So, do you do it using video? I mean,
how do you do that? Right.

00:33:47.779 --> 00:33:53.029
YOSHUA: The, the, the world knowledge. I mean,
the world model could be about all kinds of

00:33:53.029 --> 00:34:01.220
things, images, concepts in the world. I,
I like to focus on the abstractions of the

00:34:01.220 --> 00:34:06.700
kind that we can verbalize, but you, you can
also, and Yann is sort of more focused on

00:34:06.700 --> 00:34:13.340
the perception and action part.
YOSHUA: but the principal, I think

00:34:13.340 --> 00:34:21.300
could be applied to both and the principal
is the there is no world model. You only have

00:34:21.300 --> 00:34:26.300
an inference machine because - So now what
I'm going to say is a little bit mind twisting.

00:34:26.300 --> 00:34:31.480
You only have an inference machine. And what
that inference machine does is it, it, it

00:34:31.480 --> 00:34:36.720
can generate the world model, or the piece
that you need at the moment.

00:34:36.720 --> 00:34:44.379
YOSHUA: So, and when you have such a piece,
you can also evaluate how your regular inference

00:34:44.379 --> 00:34:50.879
is doing. Like, you know, am I answering the
questions, right? So, if I see an image about,

00:34:50.879 --> 00:35:01.109
you know, dogs chasing cats, a piece of my
theory of about how the world works may come

00:35:01.109 --> 00:35:07.220
to me about things I know about things I think
I know, I think are true about cats and

00:35:07.220 --> 00:35:17.520
dogs. And that will serve to provide a reward
for having decided that I need to run out

00:35:17.520 --> 00:35:24.650
quickly to stop them because maybe the cat
is in danger or maybe the dog's in danger.

00:35:24.650 --> 00:35:30.150
so, what I'm trying to say is we don't actually
need a separate world. I mean, I'm saying

00:35:30.150 --> 00:35:32.589
the opposite of what I said earlier, that's
why I'm saying it's mind twisting.

00:35:32.589 --> 00:35:36.460
YOSHUA: So, so now that we've established,
we want a world model and an inference machine,

00:35:36.460 --> 00:35:41.070
I'm going to say that instead of a world model
thinking of like a separate neural net, because

00:35:41.070 --> 00:35:45.329
there are many possibilities of what the right
world model should be, you have actually just

00:35:45.329 --> 00:35:50.250
another generative neural net that generates
just the pieces of world model that you need

00:35:50.250 --> 00:35:55.460
on the fly.
YOSHUA: It's probabilistic. So maybe one day

00:35:55.460 --> 00:35:58.970
you view things one way, and the other day
you view things a different way because really

00:35:58.970 --> 00:36:06.640
there's uncertainty. and, and that serves
as a sort of reward to drive your normal inference.

00:36:06.640 --> 00:36:09.560
Like, what should I do? how should I plan?
And so, on

00:36:09.560 --> 00:36:16.500
CRAIG: these snippets of, of, of reality,
of the world model that are being generated,

00:36:16.500 --> 00:36:20.670
how do you ensure that they're, they reflect
reality?

00:36:20.670 --> 00:36:25.810
YOSHUA: Because you can, once you generate
a piece of theory, you can also like confront

00:36:25.810 --> 00:36:34.510
it to the observations. So, what a theory
gives you is a way to quantify, how well it

00:36:34.510 --> 00:36:43.119
matches any piece of data. So, like in science,
so if I have a theory that says force equals

00:36:43.119 --> 00:36:47.940
mass times acceleration, and I observe force
mass and acceleration, I can just compute

00:36:47.940 --> 00:36:53.540
how well it matches.
YOSHUA: So, a theory gives us a checkable

00:36:53.540 --> 00:37:01.010
quantity about how well it fits the data.
A theory is like a likelihood function, right?

00:37:01.010 --> 00:37:08.261
So, it, it, it, it's something or an energy
function, in terms of what Yann likes, it's

00:37:08.261 --> 00:37:15.460
a quantity that we can measure. The output
of a theory is a measure of how any particular

00:37:15.460 --> 00:37:20.700
data is consistent with the theory.
YOSHUA: So that's what a theory does. Its

00:37:20.700 --> 00:37:27.440
output is how well it fits with the data.

00:37:27.440 --> 00:37:33.560
CRAIG: is this active research and if so,
where are you in it?

00:37:33.560 --> 00:37:37.490
YOSHUA: yeah, it's very active. It's a good
part of what I do in my group. One of the

00:37:37.490 --> 00:37:41.000
first papers that goes in, in that direction
came out last summer.

00:37:41.000 --> 00:37:45.000
YOSHUA: It's called Bayesian Structure Learning
with Generative Flow Networks. And in, in

00:37:45.000 --> 00:37:52.710
this paper, the theory is a causal theory.
So, what the, the probabilistic inference

00:37:52.710 --> 00:38:01.119
machine does is it generates a graph that
says this variable is the cause of this one.

00:38:01.119 --> 00:38:05.690
This variable is the cause of that one and
so on.

00:38:05.690 --> 00:38:10.710
YOSHUA: and that's in theory of the observed
data that we are seeing. And once you have

00:38:10.710 --> 00:38:16.770
that, you have a way to numerically then evaluate
how it's consistent with whatever data you

00:38:16.770 --> 00:38:22.690
have at hand. And that becomes a reward for
the inference machine that generates the theory.

00:38:22.690 --> 00:38:25.710
And next time it's going to generate one that
fits better.

00:38:25.710 --> 00:38:31.540
YOSHUA: But it's not like in usual reinforcement
learning where you're trying to find the theory

00:38:31.540 --> 00:38:37.010
that best fits the data, that would be back
to maximum likelihood training. What, what

00:38:37.010 --> 00:38:41.410
the GFlowNet is different from reinforcement
learning. Standard reinforcement learning

00:38:41.410 --> 00:38:47.990
is that instead of looking for generating
the actions that give you the best reward,

00:38:47.990 --> 00:38:55.160
it's sampling actions or, you know, like theories
with some probability.

00:38:55.160 --> 00:39:01.220
YOSHUA: And it's trying to do it such that
this probability matches the reward so that

00:39:01.220 --> 00:39:07.550
it's proportional to the reward. So then if
you have, first of all, if you have two theories

00:39:07.550 --> 00:39:11.550
that are about equally well fitting the data,
you have a 50 50 chance of, of sampling them.

00:39:11.550 --> 00:39:15.599
If you have a million theories that are equally
fitting the data you, you can sample all of

00:39:15.599 --> 00:39:19.300
them with standard reinforcement running.
YOSHUA: It will pick one theory and be happy.

00:39:19.300 --> 00:39:24.800
Right. So, if you want to be Bayesian, which
is like mathematically the right thing to

00:39:24.800 --> 00:39:31.810
do if you could, but it's hard. You want to
have all the theory somehow. And if a theory

00:39:31.810 --> 00:39:38.000
fits the data twice as good, maybe you want
to sample it twice more often. And that's,

00:39:38.000 --> 00:39:42.900
that's the Bayseian way of thinking about how
you should be rational in your decision making.

00:39:42.900 --> 00:39:46.680
YOSHUA: Then you should consider all the theories
and average their decisions to decide what

00:39:46.680 --> 00:39:51.000
to do next.
CRAIG: Okay. Great. Thanks. Bye-bye I really

00:39:51.000 --> 00:39:54.329
appreciate you doing this.
YOSHUA: Hello to Beauregard!

00:39:54.329 --> 00:39:59.800
CRAIG: He'll be delighted I've got him studying
machine learning online. That's it for this

00:39:59.800 --> 00:40:05.800
episode. I want to thank our sponsor, NetSuite
by Oracle. If you want to give them a try

00:40:05.800 --> 00:40:10.500
for a limited time, you can get.
CRAIG: Full implementation with no payment

00:40:10.500 --> 00:40:23.940
and no interest for six months. Just go to
www.netsuite.com/eyeonai. Be sure to type

00:40:23.940 --> 00:40:40.859
the EYEONAI so they know the sponsorship is
working. If you want a transcript of this

00:40:40.859 --> 00:40:51.870
show, you can find one on our website, eye-on.ai.
CRAIG: And remember, the Singularity may not

00:40:51.870 --> 00:40:57.619
be near, but AI is changing your world, so
pay attention.

