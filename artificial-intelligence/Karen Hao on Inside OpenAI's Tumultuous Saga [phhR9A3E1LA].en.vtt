WEBVTT
Kind: captions
Language: en

00:00:00.080 --> 00:00:02.880
Karen Hao: 0:00
What does this actually achieve? I guess,

00:00:02.880 --> 00:00:10.809
like the narrative that OpenAI tried to say
was, by doing this, we will be able to continue

00:00:10.809 --> 00:00:15.929
developing AI for the betterment of humanity.
That's why it's called OpenAI, and I think

00:00:15.929 --> 00:00:22.199
that's fiction. Microsoft has invested tens
of billions I mean on paper, like what's been

00:00:22.199 --> 00:00:28.140
announced is they've invested over 12 billion
in OpenAI. Microsoft's fate is tied to this

00:00:28.140 --> 00:00:35.040
company significantly. I suspect that this
is not the end of the drama. I don't actually

00:00:35.040 --> 00:00:38.920
think that this resolution is going to be
okay, great, like everything is back to normal.

00:00:38.920 --> 00:00:43.820
Craig Smith: 0:39
This episode is sponsored by ISS, a leading

00:00:43.820 --> 00:00:50.829
global provider of video intelligence and
data awareness solutions. Founded in 1996

00:00:50.829 --> 00:00:58.489
and headquartered in Woodbridge, New Jersey,
ISS offers a robust portfolio of AI-powered,

00:00:58.489 --> 00:01:06.130
high trust video analytics for streamlining
security, safety and business operations within

00:01:06.130 --> 00:01:12.140
a wide range of vertical markets. So what
do you want to know about your environment?

00:01:12.140 --> 00:01:28.170
To learn more about ISS' video intelligence
solutions, visit issvscom. That's issvs.com

00:01:28.170 --> 00:01:36.101
they support us, so let's support them. Hi,
I'm Craig Smith and this is my AI. In this

00:01:36.101 --> 00:01:42.909
episode, I talked to Karen Hao, a fellow journalist
who is now a contributing writer at the Atlantic.

00:01:42.909 --> 00:01:49.600
She was previously writing for the Wall Street
Journal in Hong Kong and before that for MIT

00:01:49.600 --> 00:01:56.409
Technology Review. She's the journalist with
probably the best insight into open AI and

00:01:56.409 --> 00:02:03.490
we talked about the events of the past week.
Karen knows the players, knows open AI's history

00:02:03.490 --> 00:02:09.770
and has some unique insights into what happened
and what we're likely to see in the future.

00:02:09.770 --> 00:02:16.050
Beyond open AI, we talked about the likelihood
of artificial general intelligence happening

00:02:16.050 --> 00:02:22.900
in the near future, as well as the existential
risk that many AI researchers are concerned

00:02:22.900 --> 00:02:28.640
about. I hope you find the conversation as
fascinating as I did.

00:02:28.640 --> 00:02:31.090
Karen Hao: 2:28
My name is Karen Hao. I am currently a contributing

00:02:31.090 --> 00:02:35.989
writer to the Atlantic and also writing a
book about the AI industry through the lens

00:02:35.989 --> 00:02:46.620
of open AI's rise and impacts around the world.
I became a tech journalist very much by accident.

00:02:46.620 --> 00:02:53.000
I had studied engineering in college and then
I worked in Silicon Valley at a startup and

00:02:53.000 --> 00:03:00.900
in the first year of working there I saw very
rapidly the ills of Silicon Valley, if you

00:03:00.900 --> 00:03:09.250
will. The startup I was working for, the CEO,
was fired, so actually very relevant to today's

00:03:09.250 --> 00:03:19.150
weekend's events. The CEO was fired by the
board and I became very disenchanted with

00:03:19.150 --> 00:03:23.730
the progression of events that followed around
both leading up to and after the firing of

00:03:23.730 --> 00:03:31.550
the CEO. So I was looking for other opportunities
and I wasn't really convinced that I would

00:03:31.550 --> 00:03:38.700
be able to find something different within
the Valley. So I had this vague inkling at

00:03:38.700 --> 00:03:45.019
the time that I would enjoy journalism, in
part because I had always enjoyed writing

00:03:45.019 --> 00:03:56.220
and I was particularly interested at the time
in climate change and how do you incentivize

00:03:56.220 --> 00:04:01.260
mass groups of people to change their minds
and change their behaviours and originally

00:04:01.260 --> 00:04:04.740
I thought tech was the means to do that. Then
I started thinking maybe actually journalism

00:04:04.740 --> 00:04:08.640
is the means to do that, like you need to
really build public opinion and public support

00:04:08.640 --> 00:04:14.439
for the science around something before people
are going to act on it. So I started working

00:04:14.439 --> 00:04:20.919
as an environmental reporter for my first
job, but when I was looking for it was an

00:04:20.919 --> 00:04:26.860
internship and when I was looking for full-time
opportunities it was very difficult for me

00:04:26.860 --> 00:04:30.970
to get hired as an environmental reporter
because I didn't really have that kind of

00:04:30.970 --> 00:04:38.039
background. But consistently I was asked would
you be on our tech desk instead, because of

00:04:38.039 --> 00:04:43.690
the background that I had, and so I ended
up becoming a tech reporter and then I became

00:04:43.690 --> 00:04:50.720
an AI reporter, also not because I chose it,
but because it was a job that was available.

00:04:50.720 --> 00:04:59.229
And then it became sort of the perfect match
for me because I was really really fascinated

00:04:59.229 --> 00:05:05.770
by AI technologies and I actually had a lot
of friends from college that had gone into

00:05:05.770 --> 00:05:13.220
AI research, sort of on the other side. So
I was able to kind of quickly embed myself

00:05:13.220 --> 00:05:19.880
in the community and I realised that it was
this microcosm of exploring all of the narratives

00:05:19.880 --> 00:05:25.980
that we have about technology the promise
of it, the power, the potential, the societal

00:05:25.980 --> 00:05:34.380
impact, the sort of moneyed interests that
are involved, the egos that are involved,

00:05:34.380 --> 00:05:38.819
and so I ended up reporting on that for now
more than five years.

00:05:38.819 --> 00:05:42.400
Craig Smith: 5:39
Yeah, and you were at MIT Tech Review, correct?

00:05:42.400 --> 00:05:45.539
Karen Hao: 5:43
I was at MIT Technology Review and then I

00:05:45.539 --> 00:05:53.960
went to the Wall Street Journal and then I
joined contributing writer at the Atlantic

00:05:53.960 --> 00:05:59.520
and while I was at MIT Tech Review I guess
this gets to your other question of when did

00:05:59.520 --> 00:06:07.410
I embed in open AI While I was at MIT Tech
Review, our focus was really on trying to

00:06:07.410 --> 00:06:14.569
cover, like, the bleeding edge of AI research.
So, whereas you know, the journal takes a

00:06:14.569 --> 00:06:19.539
very different stance, it's like the journal
covered technologies that are starting to

00:06:19.539 --> 00:06:24.581
be commercialised, that are starting to have
some kind of business potential. Mit Tech

00:06:24.581 --> 00:06:30.400
Review was like if it had business potential,
it was ready too late, right? So it was always

00:06:30.400 --> 00:06:37.970
about what we try to call the trends before
they happen? And because of that, we started

00:06:37.970 --> 00:06:46.280
covering open AI very quickly after they were
founded. They were founded at the end of 2015,

00:06:46.280 --> 00:06:50.570
and we probably started covering their research
in 2017, because that's when they started

00:06:50.570 --> 00:07:00.509
producing some stuff that was starting to
push the boundaries. And then in 2019, I can't

00:07:00.509 --> 00:07:06.520
really remember like how this came up, but
I basically had a discussion with my editor

00:07:06.520 --> 00:07:10.349
at the time where I was like I think Open
AI is sort of just a really interesting lab

00:07:10.349 --> 00:07:14.550
and there hasn't really been that much coverage
of it Like we've covered its research but

00:07:14.550 --> 00:07:21.550
we haven't really covered its people and they
were starting to become just prominent enough

00:07:21.550 --> 00:07:27.229
within the tech world that it felt like it
was a worthwhile thing to do. And my editor

00:07:27.229 --> 00:07:32.870
at the time said you should just profile them.
And so I reached out to the company. They

00:07:32.870 --> 00:07:37.449
already knew me pretty well because I had
been covering their research and I said, hey,

00:07:37.449 --> 00:07:41.199
you've never had a profile done before. I
think I would be the best person to do it.

00:07:41.199 --> 00:07:46.250
I think MIT Tech Review would be the best
publication. And let me come to the office

00:07:46.250 --> 00:07:50.490
for three days and sit in on some meetings,
chat with researchers, chat with executives.

00:07:50.490 --> 00:07:56.419
So that's what I did. I ended up flying to
San Francisco from Boston and then I spent

00:07:56.419 --> 00:08:04.370
the three days there, and this was at the
end of 2019. So this was a really, really

00:08:04.370 --> 00:08:09.970
interesting period of time in the company's
history, because I went there a month after

00:08:09.970 --> 00:08:18.389
Microsoft invested a billion dollars into
the research lab. So in that year of 2019,

00:08:18.389 --> 00:08:27.270
the GPT-2 announcement happened, which GPT-2
was. As listeners may remember, it was a few

00:08:27.270 --> 00:08:33.169
generations before chat, gpt and initially,
openai took the stance of not releasing the

00:08:33.169 --> 00:08:37.360
model, but announcing to the world that they
had developed it. So that happened at the

00:08:37.360 --> 00:08:41.540
start of the year and then it was a really
big controversial decision because people

00:08:41.540 --> 00:08:46.370
thought like why would you announce it but
then not release it? That's really odd. And

00:08:46.370 --> 00:08:52.211
then the capped profit arm was created within
this nonprofit entity. Sam Altman joined as

00:08:52.211 --> 00:08:56.520
a CEO and then the billion dollar investment
happened. So it was a rapid succession of

00:08:56.520 --> 00:09:03.950
changes that made clear that the company which
was a nonprofit was quickly evolving into

00:09:03.950 --> 00:09:12.550
sort of a company and that it was sort of
positioning itself to become bigger and bigger

00:09:12.550 --> 00:09:14.670
and more influential.
Craig Smith: 9:14

00:09:14.670 --> 00:09:25.880
Yeah, yeah. And that GPT-2 debacle is how
I regarded it was sort of the first glimpse

00:09:25.880 --> 00:09:35.740
of what was to come, because they said they
developed it but it was too dangerous to release

00:09:35.740 --> 00:09:42.320
and it was so everyone. Of course that got
everybody excited like what the hell is this

00:09:42.320 --> 00:09:51.290
thing? And then there were limited people,
who were invited to review it, the creation

00:09:51.290 --> 00:09:57.709
of I mean, in your article, in the Atlantic
article, and it's really what I wanted to

00:09:57.709 --> 00:10:06.680
talk to you about. You have a line and I think
it's what a lot of people are thinking and

00:10:06.680 --> 00:10:15.970
concerned about and I'm gonna try and find
it here if I can find where I opened it. I

00:10:15.970 --> 00:10:24.700
don't have it open, but you have a line about
how this most important and powerful technology

00:10:24.700 --> 00:10:35.910
mankind has ever developed is controlled by
half a dozen people who are fighting among

00:10:35.910 --> 00:10:44.690
themselves. And that's kind of frightening.
And the whole sort of fiction of the nonprofit

00:10:44.690 --> 00:10:55.389
with a profit, even if it's a cap profit,
subsidiary. I wanted to ask if that is fiction

00:10:55.389 --> 00:11:00.120
as well? I mean, it's all the same people.
It's like you and I have a nonprofit, oh,

00:11:00.120 --> 00:11:08.779
and you and I also have a profit arm. It's
not like we put on one hat and we're a nonprofit,

00:11:08.779 --> 00:11:17.720
we take it off and we're a profit seeking
company. So is there anything real? Is that

00:11:17.720 --> 00:11:26.350
just a fig leaf from your point of view? And
what do you think about this kind of technology

00:11:26.350 --> 00:11:34.019
being in the hands of such a few people, and
people who evidently can't necessarily agree?

00:11:34.019 --> 00:11:39.589
Karen Hao: 11:35
So I think the nonprofit for profit arm, I

00:11:39.589 --> 00:11:46.360
mean it's interesting. The people that designed
that structure were Sam Altman, Greg Brockman

00:11:46.360 --> 00:11:53.880
and Ilya Satskever, who ended up becoming
the main characters of the weekend. I can't

00:11:53.880 --> 00:12:00.330
personally speak for what Sam believed when
he designed this because I never spoke to

00:12:00.330 --> 00:12:05.730
him about it, but I spoke with Greg extensively
about it during the time that I was embedded

00:12:05.730 --> 00:12:13.980
within the company and he genuinely thought
that this was the solution to raise. They

00:12:13.980 --> 00:12:19.949
were trying to solve a problem. They realised
that AI development, the type of AI development

00:12:19.949 --> 00:12:25.990
that they wanted to pursue, would be very,
very expensive and they needed to raise more

00:12:25.990 --> 00:12:34.100
money than a nonprofit could help them raise.
And they tried to. I remember Greg said this

00:12:34.100 --> 00:12:40.330
to me during that time that I was embedded.
He was like we did actually try because, like

00:12:40.330 --> 00:12:44.460
this notion of having this nonprofit was very,
very like near and dear to us. So we didn't

00:12:44.460 --> 00:12:50.449
wanna just immediately go like let's scrap
it and move for a for-profit. So this like

00:12:50.449 --> 00:12:56.490
at least for Brockman he genuinely thought
that this was like a really clever solution

00:12:56.490 --> 00:13:01.589
that they'd come up with to solve this problem
of needing the money but also staying a nonprofit.

00:13:01.589 --> 00:13:08.880
But the thing is. It's like what is, what
does this actually achieve? I guess, like,

00:13:08.880 --> 00:13:18.250
maybe the fiction is that this, the narrative,
that opening I tried to say was by doing this,

00:13:18.250 --> 00:13:25.670
we will be able to continue developing AI
for the betterment of humanity and with the

00:13:25.670 --> 00:13:32.139
participation of humanity. This was a really
big part of their early days. Messaging as

00:13:32.139 --> 00:13:34.839
well was like they were going to be open,
they were going to be transparent. That's

00:13:34.839 --> 00:13:44.560
why it's called open AI. I think that's fiction.
The non-profit for-profit is solving the specific

00:13:44.560 --> 00:13:49.170
problem that they wanted to solve. I think
that is, you know, like they were genuinely

00:13:49.170 --> 00:13:56.570
trying to solve this very particular problem.
But does it actually get us more open, more

00:13:56.570 --> 00:14:03.800
transparent, participatory AI development?
No, not at all. What it actually does is just

00:14:03.800 --> 00:14:09.970
entrenched the power of the people that designed
this thing. And ironically, I mean what we

00:14:09.970 --> 00:14:18.370
saw this weekend was that the non-profit for-profit
did end up working as designed, in that the

00:14:18.370 --> 00:14:29.209
board did in fact, do what their job was to
vote out Sam for like not aligning with the

00:14:29.209 --> 00:14:36.980
mission, supposedly. But then the reaction
that we see from Sam, from Greg and ultimately

00:14:36.980 --> 00:14:45.089
from Ilya, when Ilya flipped, suggests that
they're not actually here for this mechanism

00:14:45.089 --> 00:14:54.699
to be used against them, right. But if the
mechanism were designed with like true sincerity

00:14:54.699 --> 00:15:02.029
of maybe one day I'd actually ask Greg, I'd
ask like would you ever consider firing yourself

00:15:02.029 --> 00:15:08.870
if you felt that you were no longer up for
the job, which I could have even phrased it

00:15:08.870 --> 00:15:14.149
as, would you be open to the board firing
you or the board firing the CEO if they evaluate?

00:15:14.149 --> 00:15:18.440
And at the time he said like I would be open
to it, but clearly like they weren't actually

00:15:18.440 --> 00:15:25.899
open to it, right. So that's the fiction that
I think kind of became very plainly displayed

00:15:25.899 --> 00:15:27.270
this week?
Craig Smith: 15:26

00:15:27.270 --> 00:15:31.149
Yeah, and the board is the board of the nonprofit,
is that right?

00:15:31.149 --> 00:15:34.880
Karen Hao: 15:31
The board? Yes, the board is part of the nonprofit

00:15:34.880 --> 00:15:41.470
and the nonprofit governs the capped profit,
and that's why the board was able to exercise

00:15:41.470 --> 00:15:47.970
the power that had been bestowed upon them
with this legal structure to vote out Sam.

00:15:47.970 --> 00:15:52.380
Craig Smith: 15:48
Yeah, it'll be interesting to see if that

00:15:52.380 --> 00:16:02.440
nonprofit profit structure survives, because
it doesn't seem to make a lot of sense. I

00:16:02.440 --> 00:16:11.319
mean, going back to GPT2, that was the thing
that upset a lot of people Open AI. Open AI

00:16:11.319 --> 00:16:20.019
was supposed to be kind of an answer to big
tech, to Google specifically, and that they

00:16:20.019 --> 00:16:24.509
are going to be open source, they're going
to share all their research. It's not going

00:16:24.509 --> 00:16:34.740
to be controlled by a for profit entity, and
maybe that was just naive, that when anyone

00:16:34.740 --> 00:16:44.750
develops anything that has such profit potential,
they're not. The logic is that they need to

00:16:44.750 --> 00:16:51.300
raise funds and investors need to have some
profit participation, otherwise they won't

00:16:51.300 --> 00:17:00.170
invest. I mean, ultimately, this kind of technology
is not going to survive under a nonprofit

00:17:00.170 --> 00:17:10.069
umbrella, I think. But more specifically,
I talk and I know you do too to Yan Likun,

00:17:10.069 --> 00:17:23.429
who I have enormous respect for. I mean, obviously
he's a genius, but I mean in terms of his

00:17:23.429 --> 00:17:33.490
opinions on things like open source versus
proprietary research, and do you think that

00:17:33.490 --> 00:17:41.380
this kind of tech should be open source, regardless
of the dangers of open sourcing? Incredibly

00:17:41.380 --> 00:17:49.650
powerful technology, but simply to avoid this
sort of thing, that then you have the broader

00:17:49.650 --> 00:18:01.080
research community working on it, refining
it and then some sort of a licence structure

00:18:01.080 --> 00:18:06.710
that allows people to use it, whether it's
for research or commercial use.

00:18:06.710 --> 00:18:09.970
Karen Hao: 18:07
Yeah, I think it's a great question. To be

00:18:09.970 --> 00:18:15.010
honest, I haven't fully made up my mind about
whether to fully open source technologies

00:18:15.010 --> 00:18:21.289
like these, but certainly we need more transparency
than we need now. I think that is very, very

00:18:21.289 --> 00:18:28.150
clear. And also, what's interesting, I will
say that Yann has been a big advocate of open

00:18:28.150 --> 00:18:33.500
source or of transparency, but Meta's Llama
2 model does not actually technically fit

00:18:33.500 --> 00:18:40.010
the definition of open source. They open sourced
the model weights but by definition, they

00:18:40.010 --> 00:18:44.730
would also need to open source the data in
order for people to audit it, to understand

00:18:44.730 --> 00:18:49.460
how it works. And Meta has refused to release
any information about the data that it was

00:18:49.460 --> 00:19:01.300
trained on, and this is something that I think
could easily become like a very low stakes

00:19:01.300 --> 00:19:08.450
accountability measure. Is releasing the data?
Just saying what's in the data already is

00:19:08.450 --> 00:19:12.770
a huge step forward and you haven't trained
the model, like the data is not the model.

00:19:12.770 --> 00:19:17.330
So if you're worried, if we were to buy into
the idea that open sourcing the model could

00:19:17.330 --> 00:19:26.130
have dangerous potential, open sourcing the
data would not. But the fact that we don't

00:19:26.130 --> 00:19:31.210
have any understanding whatsoever of what
is being used to train these models, I think

00:19:31.210 --> 00:19:37.961
is a very telling sign of why actually these
companies that say that they can't open source

00:19:37.961 --> 00:19:41.600
the technologies was the true motivation behind
their arguments.

00:19:41.600 --> 00:19:47.039
Craig Smith: 19:43
And do you think that's because they're afraid

00:19:47.039 --> 00:19:49.860
of liability? Absolutely yeah.
Karen Hao: 19:49

00:19:49.860 --> 00:19:57.740
Absolutely. I think they're afraid of liability,
of reputation damage, because a lot of the

00:19:57.740 --> 00:20:03.929
content that is put into these systems is
not actually vetted very well, and that's

00:20:03.929 --> 00:20:11.760
precisely why open sourcing would create safer
systems, because if you force companies to

00:20:11.760 --> 00:20:18.820
open source, they would have to significantly
do more work to clean up the data sets, which

00:20:18.820 --> 00:20:24.600
would actually result in better products.
And if you have many more scientists within

00:20:24.600 --> 00:20:29.909
the community, many more other people within
the community going through like more eyes

00:20:29.909 --> 00:20:35.980
on these data sets, they will naturally just
become better. And I think it would also be

00:20:35.980 --> 00:20:42.390
a forcing function to then get to a place
where, like some of the companies are now

00:20:42.390 --> 00:20:45.929
doing this, where they're striking data deals,
where they actually purchase the data from

00:20:45.929 --> 00:20:53.990
a media company or from Shutterstock or whatever
it is. That came very late in the stage of

00:20:53.990 --> 00:20:59.780
the AI development that we're in. All of the
original models were not developed with these

00:20:59.780 --> 00:21:05.780
data deals, and now the companies can continue
to profit off of the data that wasn't paid

00:21:05.780 --> 00:21:13.511
for and that wasn't. They weren't being transparent
about it. But if we had more of this transparency,

00:21:13.511 --> 00:21:17.520
it would be a forcing function to accelerate
this trend, which I think is a really good

00:21:17.520 --> 00:21:24.190
one. There should be payments for data and
potentially even dividend payments to the

00:21:24.190 --> 00:21:28.630
data providers, data creators, over time.
Craig Smith: 21:27

00:21:28.630 --> 00:21:34.820
Yeah, yeah, that's. That's an interesting
idea that you know. I know you know the same

00:21:34.820 --> 00:21:43.601
people. I knew that Don Song at Berkeley has
been working on this idea of, you know, using

00:21:43.601 --> 00:21:52.549
the blockchain to secure your data and then
Be able to sell it and have kind of this lifelong

00:21:52.549 --> 00:22:00.289
income stream coming from it, which sounds
great to me. Now that I'm close to retirement,

00:22:00.289 --> 00:22:10.470
it would be nice to have an income stream
off the data that every company Used of mine.

00:22:10.470 --> 00:22:20.929
Yeah, so when do you think I mean Sam and
Greg? Or back at open AI? I, Ilya, feel bad

00:22:20.929 --> 00:22:31.740
for myself. You know I've interviewed such
a Deep soul. You know he clearly didn't mean

00:22:31.740 --> 00:22:40.250
to cause this, this Global ruckus. Helen toner,
I feel bad for her. She's been raked over

00:22:40.250 --> 00:22:46.220
the coals, people have made fun of her research
and where do you think this is going to go?

00:22:46.220 --> 00:22:51.730
I, I, I can't imagine that. I think they're.
They're what three people on the new board

00:22:51.730 --> 00:23:02.860
that again, with this technology, as critical
as it is, I would assume at the very least,

00:23:02.860 --> 00:23:10.049
Microsoft will have a seat on the board. Where
do you think it's? It's going to go in open

00:23:10.049 --> 00:23:18.289
AI's case and then we can talk about sort
of government regulation. I would think that

00:23:18.289 --> 00:23:24.470
Regulators would be looking at this and saying
you know, we can't have a bunch of you know

00:23:24.470 --> 00:23:30.010
30-somethings in Silicon Valley like. Yeah,
yielding the future of the world.

00:23:30.010 --> 00:23:40.279
Karen Hao: 23:30
So yeah, I Think for open AI's case, I suspect

00:23:40.279 --> 00:23:47.100
that this is not the end of the drama. I don't
actually think this resolution is going to

00:23:47.100 --> 00:23:51.799
be okay, great, like everything is back to
normal, Sam's installed, all happy and peaceful

00:23:51.799 --> 00:23:56.730
and all you know. Like the piece that I wrote
in the Atlantic talks about how there's all

00:23:56.730 --> 00:23:59.980
these different factions with the company,
different ideologies. They all are kind of

00:23:59.980 --> 00:24:04.470
in this power struggle and I really do think
that the more powerful a technology is, the

00:24:04.470 --> 00:24:11.720
more you end up with a game of throne style
power struggle, because people think, very,

00:24:11.720 --> 00:24:17.140
believe very strongly in their ideology for
how AI should be developed and it's both like

00:24:17.140 --> 00:24:22.799
this belief that's like a true, genuine belief
and also, of course, like there is elements

00:24:22.799 --> 00:24:29.730
of desire for power, desire for control, and
We've seen open AI go through different waves

00:24:29.730 --> 00:24:34.240
of drama before. So this is just the third
way. If you, I guess you could call it like

00:24:34.240 --> 00:24:38.970
the Elon Musk leaving. Open AI was the first
wave and then the anthropic open AI split

00:24:38.970 --> 00:24:43.669
was the second wave and now this is the third
wave. There's definitely something else that's

00:24:43.669 --> 00:24:51.640
gonna come, but in terms of what this means,
I guess for the course of AI development.

00:24:51.640 --> 00:24:57.840
I suspect that Sam is Certainly going to be
a lot wiser about selecting carefully, he's

00:24:57.840 --> 00:25:03.000
choosing his board members and trying to make
sure that he entrenched his power again. So

00:25:03.000 --> 00:25:12.020
if that is the case, then his specific ethos
and his sort of habit around rapid commercialization,

00:25:12.020 --> 00:25:18.980
rapid growth, is going to now be like the
main driving seat of the organisation and

00:25:18.980 --> 00:25:24.611
that is going to continue. We're going to
see way more proliferation of products, way

00:25:24.611 --> 00:25:30.370
more downstream companies building on top
of it and, unfortunately, I think we will

00:25:30.370 --> 00:25:44.919
see way more Ripple effects, negative ripple
effects as well as speed overtakes. You know

00:25:44.919 --> 00:25:52.240
Certain types of trust and safety concerns,
for example, and so I think that that's probably

00:25:52.240 --> 00:25:58.990
the most likely scenario, but also it's really
hard to know. It's really hard to know because

00:25:58.990 --> 00:26:03.361
I don't know if you saw there was like that,
that that letter that was circulating, open

00:26:03.361 --> 00:26:08.360
letter to the board from former employees.
So it's like is that going to be yet another

00:26:08.360 --> 00:26:18.630
episode in this particular weekend saga, or
Are we, is it buried for now and we don't

00:26:18.630 --> 00:26:21.150
see something else for another one to two
years?

00:26:21.150 --> 00:26:26.880
Craig Smith: 26:22
Yeah, yeah. And then you've got Microsoft

00:26:26.880 --> 00:26:34.840
and you know Satya's is public, they all smile
and everything's fine. But you can imagine.

00:26:34.840 --> 00:26:40.980
Karen Hao: 26:36
I've been sweating like crazy. Microsoft has

00:26:40.980 --> 00:26:47.529
invested Tons of billions I mean on paper,
like what's been announced is they've invested

00:26:47.529 --> 00:26:53.250
like something like over 12 billion In opening
up. But it's way more than that in the sense

00:26:53.250 --> 00:26:59.630
that when you look at their Investor, like
their latest investor statement, they say

00:26:59.630 --> 00:27:06.130
that they're planning on laying down more
than 50 billion dollars in new data centres

00:27:06.130 --> 00:27:13.460
for next year, and Not all of that is to our
opening AI, but they're laying that down because

00:27:13.460 --> 00:27:19.710
they're selling to Azure their cloud compute
customers. This idea of the Microsoft open

00:27:19.710 --> 00:27:27.000
AI partnership and this is why Microsoft stock
has been doing so well as it is Bank. It banks

00:27:27.000 --> 00:27:33.690
on this partnership. So, like you know, when
the news came and like, the Microsoft stock

00:27:33.690 --> 00:27:37.861
immediately started dropping and now that,
like, things are back to normal, Microsoft

00:27:37.861 --> 00:27:44.039
stock is increasing. Microsoft's fate is tied
to this company Significantly, significantly.

00:27:44.039 --> 00:27:54.460
Craig Smith: 27:45
So, yeah, yeah, although a brilliant move,

00:27:54.460 --> 00:27:59.870
because at one point it looked and this is
well you know all over Twitter and commentators

00:27:59.870 --> 00:28:07.740
who say but that he in effect had acquired
open AI or was on the cusp of a AI, without

00:28:07.740 --> 00:28:15.000
having any, any regulatory interference or
even having to pay a premium, actually paying

00:28:15.000 --> 00:28:23.350
a discount. So, yeah, adjust. In the data
centres this is something I've Been talking

00:28:23.350 --> 00:28:33.090
to people about. This technology is is so
much promise for enterprise, but because of

00:28:33.090 --> 00:28:42.780
the constraint in in available compute, which,
which goes all the way back to, you know,

00:28:42.780 --> 00:28:52.380
silicone starts at the foundry, but but then
through to to Nvidia and and their limited

00:28:52.380 --> 00:29:06.399
supply, you can't actually build and deploy
a heavy use enterprise application Using GPT

00:29:06.399 --> 00:29:15.769
for through an API, just the, the pipe through
which you're sending your tokens Is too narrow.

00:29:15.769 --> 00:29:23.670
Is this investment by Microsoft intended to
ease that? What do you think about that? That

00:29:23.670 --> 00:29:27.960
constraint, how long will that last?
Karen Hao: 29:27

00:29:27.960 --> 00:29:33.380
um, I definitely think that. Yeah, I do think
that Microsoft's investments are meant to

00:29:33.380 --> 00:29:40.570
try and facilitate all of their customer base
to transition to an AI forward business. I

00:29:40.570 --> 00:29:46.080
suppose, because I mean, every, every company
that I've been talking to these days, regardless

00:29:46.080 --> 00:29:51.960
of what industry they're in, is suddenly Alert
to the idea that they need some kind of AI

00:29:51.960 --> 00:30:02.179
strategy. And all of the tech giants Microsoft,
google as AWS, amazon are all trying to capture

00:30:02.179 --> 00:30:08.000
that new Market and they're trying to build
out their infrastructure to also facilitate

00:30:08.000 --> 00:30:18.299
that integration with these, these business
customers and I Mean it's I personally, I

00:30:18.299 --> 00:30:23.360
I think there's sort of two interesting things
that I'm personally watching for. One is how

00:30:23.360 --> 00:30:28.530
much of this talk is going to actually convert
into implementation, because a lot of the

00:30:28.530 --> 00:30:32.510
companies I talk to say they need the AI strategy,
but they're actually not sure what that means

00:30:32.510 --> 00:30:38.860
and whether or not it would ultimately be
valuable for their business. It is valuable

00:30:38.860 --> 00:30:43.640
right now for their business to talk about
it, but will it actually be valuable later

00:30:43.640 --> 00:30:51.080
to implement it? That's one thing to look
out for, then, I guess the second thing is

00:30:51.080 --> 00:31:01.480
whether or not these companies, these Cloud
providers that are jockeying for Market Share,

00:31:01.480 --> 00:31:08.669
are even able to acquire the resources necessary
to continue laying down the data centres to

00:31:08.669 --> 00:31:13.169
keep up with this kind of demand. I think
those are two things that could potentially

00:31:13.169 --> 00:31:22.279
end up limiting AI adoption or bottlenecking
adoption, but it's difficult to tell right

00:31:22.279 --> 00:31:27.630
now what that will actually look like in five
years time maybe.

00:31:27.630 --> 00:31:33.029
Craig Smith: 31:28
Yeah. Have you heard anything that you haven't

00:31:33.029 --> 00:31:41.210
published or that you have published about
this idea of Sam Altman starting a chip company

00:31:41.210 --> 00:31:45.880
or open AI? Starting a chip company to compete
with Nvidia?

00:31:45.880 --> 00:31:52.529
Karen Hao: 31:48
Only what's reported. My understanding is

00:31:52.529 --> 00:31:58.710
that this is not a new idea for him, that
it was something that he'd always been interested

00:31:58.710 --> 00:32:08.630
in but had never, maybe not taken seriously
before I'm not sure. But then it became much

00:32:08.630 --> 00:32:15.769
more real and viable, potentially and potentially
a smart business decision to be able to actually

00:32:15.769 --> 00:32:23.550
have that. But the thing is I don't know that.
People fully understand sometimes that it

00:32:23.550 --> 00:32:29.380
doesn't matter how many chip companies we
have. We only have one real chip manufacturer,

00:32:29.380 --> 00:32:43.130
which is the SMC. You're not going to get,
I mean, samsung as well and a couple other

00:32:43.130 --> 00:32:49.620
companies that are able to produce these chips,
but TSMC is the most consistent provider and

00:32:49.620 --> 00:32:53.610
everyone wants to use them. No matter how
many chip companies you have. There's still

00:32:53.610 --> 00:33:01.299
that bottleneck. I'm not really sure what
Sam's plan was with that, whether he was trying

00:33:01.299 --> 00:33:06.269
to create his own chip company to get around
the waiting list for Nvidia, or whether he

00:33:06.269 --> 00:33:10.530
was doing it for something else, like maybe
for optimising, like trying to get to the

00:33:10.530 --> 00:33:17.309
next stage of AI development, maybe by trying
to optimise how model training works by going

00:33:17.309 --> 00:33:20.310
to the hardware level. I'm not really sure.
Craig Smith: 33:20

00:33:20.310 --> 00:33:25.549
Yeah yeah. A lot of people don't understand
that when people talk about chip companies,

00:33:25.549 --> 00:33:36.000
they're not actually manufacturing the silicone
chips, they're designing chips. I love the

00:33:36.000 --> 00:33:49.490
bit in the article about Ilya chanting the
AGI. I'm a big AGI sceptic. I agree with Yanlacun

00:33:49.490 --> 00:34:01.920
again. I mean a lot of his ideas really resonate
with me that the language models are not the

00:34:01.920 --> 00:34:14.659
way to AGI. I mean they'll certainly advance
to something. As a journalist, a very well-informed

00:34:14.659 --> 00:34:24.280
journalist, what's your feeling about that?
Do you think I get comments all the time on

00:34:24.280 --> 00:34:31.379
various things that I post AGI we're going
to reach super intelligence sometime next

00:34:31.379 --> 00:34:37.690
year. It's like really, yeah, what's your
sense of that?

00:34:37.690 --> 00:34:42.679
Karen Hao: 34:38
My feeling is that we don't have any agreed

00:34:42.679 --> 00:34:49.190
definition of AGI. Agi could be here if you
define it based on what we have, or it could

00:34:49.190 --> 00:34:55.550
be 100 years away if you define it totally
differently. For the people who are saying

00:34:55.550 --> 00:35:01.310
super intelligence might be here soon, even
scientifically, we don't have an agreed upon

00:35:01.310 --> 00:35:07.920
definition of intelligence. I'm not talking
about AI. I'm talking about biology, psychology

00:35:07.920 --> 00:35:14.131
and neuroscience. There's no agreed upon definition
of intelligence. I'm sure that the people

00:35:14.131 --> 00:35:21.690
that are saying these things totally agree
with them. It's just like you get to define

00:35:21.690 --> 00:35:27.599
yourself what the goal is and where to go.
I think this is the fundamental problem of

00:35:27.599 --> 00:35:34.859
the AI industry as a whole, as illustrated
this weekend by OpenAI, is that by setting

00:35:34.859 --> 00:35:39.940
a goal towards something that is completely
undefined, you just get to do whatever you

00:35:39.940 --> 00:35:47.970
want. I'd say that it's under the banner of
a thing that sounds really nice and magical,

00:35:47.970 --> 00:36:01.240
even and de facto good, but ultimately the
AGI is actually just a rhetorical tool to

00:36:01.240 --> 00:36:05.010
continue advancing towards whatever you want
to advance towards.

00:36:05.010 --> 00:36:10.980
Craig Smith: 36:05
Yeah, although I think we all have an idea

00:36:10.980 --> 00:36:20.900
of people who are paying attention to the
space, of what it would look and feel like.

00:36:20.900 --> 00:36:32.480
I really like Yanlacoon's world model research
because it's grounded in language. You layer

00:36:32.480 --> 00:36:45.200
on top of that. Ilia is a student of Jeff
Hinton's. Jeff is now beating the existential

00:36:45.200 --> 00:36:56.450
risk gong. What do you think about that? Because,
again, I lean more towards Yan and his view

00:36:56.450 --> 00:37:08.160
that it's certainly their risks, but this
existential risk is a bridge too far.

00:37:08.160 --> 00:37:13.650
Karen Hao: 37:09
So I've talked with Hinton about this. Actually,

00:37:13.650 --> 00:37:20.190
what actually changed his mind about this
thing? Because he changed his mind. He's sort

00:37:20.190 --> 00:37:28.660
of relatively recently, and it was specifically
that he realised that the definition again

00:37:28.660 --> 00:37:33.079
this goes back to definitions the definition
that he was using for superintelligence before

00:37:33.079 --> 00:37:39.890
was potentially the wrong benchmark and that
he should actually just be observing the ability

00:37:39.890 --> 00:37:46.140
of these technologies, that we have to engage
with the real world and influence people and

00:37:46.140 --> 00:37:52.069
cause like real world phenomenon, and that
we had already reached a point where it was

00:37:52.069 --> 00:38:00.339
causing like mass real world phenomenon it
was like and large scale influence, you know

00:38:00.339 --> 00:38:07.810
and that, whereas humans are very lossy in
our ability to transfer knowledge, that digital

00:38:07.810 --> 00:38:12.680
intelligence, as he was calling it, digital
intelligence is not like you could have multiple

00:38:12.680 --> 00:38:18.220
models that immediately combine their knowledge.
I'm saying all this in quotes because I think

00:38:18.220 --> 00:38:24.609
it's sort of important to emphasise that there
are lots of debates around, like the use of

00:38:24.609 --> 00:38:30.520
these terminology, but that the, that digital
models would be able to combine instantly,

00:38:30.520 --> 00:38:36.430
transfer knowledge instantly, and then that
is how you would reach superintelligence.

00:38:36.430 --> 00:38:45.020
I am extremely sceptical of these claims as
well. I think that Hinton believes what he

00:38:45.020 --> 00:38:51.099
believes and has a very logical path for what
he believes. I also think that ultimately,

00:38:51.099 --> 00:39:00.660
like who likes me it's sort of like you would
have to have it. We don't have very good techniques

00:39:00.660 --> 00:39:06.390
right now for developing advanced capabilities
without massive data centres. So it doesn't

00:39:06.390 --> 00:39:11.660
to me it doesn't make sense that we should
fear like 100 models suddenly combining into

00:39:11.660 --> 00:39:17.589
one. Who's training those 100 models? Like
these models are exorbitantly expensive. Dario

00:39:17.589 --> 00:39:23.960
mode, a CEO of Anthropics, said publicly on
stage earlier this year that currently the

00:39:23.960 --> 00:39:30.900
industry is training models that are around
$100 million of cost. Then it's going to be

00:39:30.900 --> 00:39:35.089
a billion dollars of cost and he could see
in two years there is a reaching $10 billion

00:39:35.089 --> 00:39:42.880
of cost. I mean there is not. Like are we
going to train $110 billion models and then

00:39:42.880 --> 00:39:47.010
worry about them combining into superintelligence?
I and like, where are we getting the data

00:39:47.010 --> 00:39:55.940
from for this? From, I think it immediately
hits the real world limitations. But you know,

00:39:55.940 --> 00:39:59.920
like the scientists that have been working
on these things for a long time, they have

00:39:59.920 --> 00:40:06.740
a very I think they sometimes have tunnel
vision about the things that they research

00:40:06.740 --> 00:40:13.589
and they're not necessarily spending a lot
of time out in the world like they're. They're

00:40:13.589 --> 00:40:18.290
like in their lab and like thinking about
these things from a mathematical, theoretical

00:40:18.290 --> 00:40:21.740
perspective. And if you were to think about
it from that perspective, then certainly I

00:40:21.740 --> 00:40:27.540
think you would start to get to some alarming
conclusions. But yeah, that's sort of my view

00:40:27.540 --> 00:40:28.760
on it.
Craig Smith: 40:28

00:40:28.760 --> 00:40:35.540
This episode is sponsored by ISS, a leading
global provider of video intelligence and

00:40:35.540 --> 00:40:42.589
data awareness solutions. Founded in 1996
and headquartered in Woodbridge, New Jersey,

00:40:42.589 --> 00:40:51.020
it offers a robust portfolio of AI powered,
high trust video analytics for streamlining

00:40:51.020 --> 00:40:58.550
security, safety and business operations within
a wide range of vertical markets. So what

00:40:58.550 --> 00:41:05.260
do you want to know about your environment?
To learn more about ISS video intelligence

00:41:05.260 --> 00:41:22.210
solutions, visit issvscom. That's issvs.com
they support us, so let's support them. That's

00:41:22.210 --> 00:41:27.600
it for this episode. I want to thank Karen
for her time. If you want to learn more about

00:41:27.600 --> 00:41:37.800
what we talked about today, you can find a
transcript on our website IonAI, that's e-y-e-onai.

00:41:37.800 --> 00:41:46.130
And in the meantime, remember the singularity
may not be near, but AI is changing your world,

00:41:46.130 --> 00:41:51.049
so pay attention.

