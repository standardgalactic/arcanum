WEBVTT
Kind: captions
Language: en

00:00:21.960 --> 00:00:22.600
PEDRO:&nbsp;

00:01:06.280 --> 00:01:11.960
Hi, I'm Pedro Domingos. I'm a professor of&nbsp;
computer science at the University of Washington&nbsp;&nbsp;

00:01:11.960 --> 00:01:20.200
in Seattle. I am an expert on AI and in particular&nbsp;
machine learning. I'm originally from Portugal,&nbsp;&nbsp;

00:01:20.200 --> 00:01:26.040
got my PhD at UC Irvine back when hardly anybody&nbsp;
else was doing machine learning. This was in the&nbsp;&nbsp;

00:01:26.040 --> 00:01:32.800
80s. I've done a whole bunch of things in this&nbsp;
field since. I've written a couple of books,&nbsp;&nbsp;

00:01:32.800 --> 00:01:38.600
one called the Master Algorithm, introduced&nbsp;
machine learning to a broad audience,&nbsp;&nbsp;

00:01:38.600 --> 00:01:44.360
turned into a surprise bestseller, and most&nbsp;
recently, 2040: A Silicon Valley Satire which,&nbsp;&nbsp;

00:01:44.360 --> 00:01:50.520
as the name implies, is a satire of AI in the tech&nbsp;
industry and the politics and culture around it.

00:01:50.520 --> 00:01:54.920
CRAIG:
Yeah, I enjoyed the book. It reminded&nbsp;&nbsp;

00:01:54.920 --> 00:02:02.920
me a little bit– when I was in high school,&nbsp;
there was a book by a writer named Tom Robbins,&nbsp;&nbsp;

00:02:02.920 --> 00:02:11.520
Even Cowgirls Get the Blues. It's kind of a crazy&nbsp;
satire with a lot of wild characters, but with an&nbsp;&nbsp;

00:02:11.520 --> 00:02:24.040
underlying philosophical theme. I have a lot of&nbsp;
questions about the book, for listeners it's, at&nbsp;&nbsp;

00:02:24.040 --> 00:02:33.560
a very high level about what happens when AI has&nbsp;
advanced to the point that we have an AI candidate&nbsp;&nbsp;

00:02:33.560 --> 00:02:43.080
for president and all of the crazy things that&nbsp;
devolve from that. But without going into the plot&nbsp;&nbsp;

00:02:43.080 --> 00:02:54.760
so much, the book is largely around the theme of&nbsp;
control,lack of control, and the danger of losing&nbsp;&nbsp;

00:02:54.760 --> 00:03:08.080
control of AI. I understand this is a satire, but&nbsp;
for two years now that's been in the wind and I,&nbsp;&nbsp;

00:03:08.080 --> 00:03:17.000
frankly, think it's exaggerated, those fears,&nbsp;
and I think regulation is moving along quickly,&nbsp;&nbsp;

00:03:17.000 --> 00:03:22.640
maybe because of the noise made by the&nbsp;
research community, but it is moving along&nbsp;&nbsp;

00:03:22.640 --> 00:03:35.480
quickly. It's satire, is it poking fun at those&nbsp;
concerns or, are you raising those concerns?

00:03:35.480 --> 00:03:37.760
PEDRO:
Well, like all satire,&nbsp;&nbsp;

00:03:37.760 --> 00:03:44.920
it has an underlying serious intent. So 2040&nbsp;
is not about 2040 any more than Animal Farm is&nbsp;&nbsp;

00:03:44.920 --> 00:03:50.880
about animals or farming. It's about today; it's&nbsp;
about 2024. Like a lot of classic science fiction,&nbsp;&nbsp;

00:03:50.880 --> 00:03:56.960
what it does is it extrapolates current trends to&nbsp;
the point of absurdity. The thing that's kind of&nbsp;&nbsp;

00:03:56.960 --> 00:04:04.280
funny and maybe alarming in some ways is that a&nbsp;
lot of this stuff is not that far away. In fact,&nbsp;&nbsp;

00:04:04.280 --> 00:04:08.120
a lot of people's reaction to the book is,&nbsp;
[yeah, this seems so far out and over the top,&nbsp;&nbsp;

00:04:08.120 --> 00:04:14.640
and yet it's oddly realistic. Like, it could&nbsp;
happen any time]. In fact, I conceived of this&nbsp;&nbsp;

00:04:14.640 --> 00:04:20.120
book a few years ago but then, I decided I really&nbsp;
need to publish this before it becomes outdated,&nbsp;&nbsp;

00:04:20.120 --> 00:04:28.000
because so much of the stuff in it was coming&nbsp;
true. So it's a short, quick read but it actually&nbsp;&nbsp;

00:04:28.000 --> 00:04:32.880
covers a lot of things. So every element of&nbsp;
the book is there for a reason. It's there&nbsp;&nbsp;

00:04:32.880 --> 00:04:37.840
to illustrate something about today's world&nbsp;
of tech, AI, and whatnot. And as you say,&nbsp;&nbsp;

00:04:37.840 --> 00:04:46.240
one of the key elements is what I think are the&nbsp;
misguided fears about AI versus the real ones. So&nbsp;&nbsp;

00:04:46.240 --> 00:04:51.200
part of what the novel tries to illustrate with&nbsp;
PresiBot, who's this AI candidate for president,&nbsp;&nbsp;

00:04:51.200 --> 00:04:56.080
just think ChatGPT runs for president, that's&nbsp;
basically what it really is– and you can already&nbsp;&nbsp;

00:04:56.080 --> 00:05:04.240
see the jokes coming one after another, is that&nbsp;
the real danger is not Terminator or some evil AI&nbsp;&nbsp;

00:05:04.240 --> 00:05:09.920
taking over and exterminating us, the real danger&nbsp;
is that the AI is imperfect. The AI screws up,&nbsp;&nbsp;

00:05:09.920 --> 00:05:14.040
it hallucinates, it has bugs, it needs&nbsp;
fixed. The even bigger danger is that&nbsp;&nbsp;

00:05:14.040 --> 00:05:20.520
the humans developing and trying to control&nbsp;
the AI are flawed humans, like we all are,&nbsp;&nbsp;

00:05:20.520 --> 00:05:27.920
and they screw up. They get into conflicts and&nbsp;
disasters happen. Without giving any spoilers,&nbsp;&nbsp;

00:05:27.920 --> 00:05:32.800
those disasters are the result of the interaction&nbsp;
of the flaws of the AI with the flaws and the&nbsp;&nbsp;

00:05:32.800 --> 00:05:36.920
intentions of the various humans, which I think&nbsp;
is the real problem, and of course, what is always&nbsp;&nbsp;

00:05:36.920 --> 00:05:41.800
the real problem with technology it’s just that on&nbsp;
AI it might be on a bigger scale than ever before.

00:05:41.800 --> 00:05:43.000
CRAIG:
Yeah,&nbsp;&nbsp;

00:05:43.000 --> 00:05:51.840
but where do you lie on that safety debate&nbsp;
between Geoff Hinton and I don't know–

00:05:51.840 --> 00:05:52.940
PEDRO:
Yann LeCun.

00:05:52.940 --> 00:05:54.240
CRAIG:
Yann LeCun,&nbsp;&nbsp;

00:05:54.240 --> 00:06:01.520
yeah. They're the principles in much of the&nbsp;
debate. Where do you lie on that spectrum?

00:06:01.520 --> 00:06:04.400
PEDRO:
I'm very much in agreement with Yann&nbsp;&nbsp;

00:06:04.400 --> 00:06:10.480
on most of this and in disagreement with Geoff. I&nbsp;
have a lot of respect for Geoff as a researcher,&nbsp;&nbsp;

00:06:10.480 --> 00:06:16.360
but as someone to take advice from about real&nbsp;
world issues, he's not the first person I would&nbsp;&nbsp;

00:06:16.360 --> 00:06:22.520
turn to. In fact, he's very honest about it. He&nbsp;
says he never actually worried about any of these&nbsp;&nbsp;

00:06:22.520 --> 00:06:30.960
AI alarmist scenarios until recently. It was only&nbsp;
when ChatGPT came out that he started thinking,&nbsp;&nbsp;

00:06:30.960 --> 00:06:36.560
[Oh wow, superintelligence is almost here.&nbsp;
And then, [What is it going to do to us?]&nbsp;&nbsp;

00:06:36.560 --> 00:06:41.960
I think he and a lot of other people like him,&nbsp;
they're making two mistakes, at least. One is&nbsp;&nbsp;

00:06:41.960 --> 00:06:46.840
that superintelligence is not almost here by&nbsp;
any means. I think in the years since then,&nbsp;&nbsp;

00:06:46.840 --> 00:06:53.120
it's become increasingly clear that this AI still&nbsp;
has a long way to go, number one. But number two,&nbsp;&nbsp;

00:06:53.120 --> 00:06:58.680
and maybe less obvious but in the longer&nbsp;
term, more important, superintelligence is&nbsp;&nbsp;

00:06:58.680 --> 00:07:03.480
no extinction danger to humanity. This&nbsp;
is a natural mistake that people make&nbsp;&nbsp;

00:07:03.480 --> 00:07:09.240
by analogizing between AI and humans. The only&nbsp;
intelligence that we know is human intelligence,&nbsp;&nbsp;

00:07:09.240 --> 00:07:15.680
animal intelligence. So once we see a computer&nbsp;
behaving in ways that seem intelligent, we can't&nbsp;&nbsp;

00:07:15.680 --> 00:07:22.080
help but project onto the computer all sorts of&nbsp;
human characteristics that it just doesn't have,&nbsp;&nbsp;

00:07:22.080 --> 00:07:29.520
like ambition, the will to power, emotions, and&nbsp;
consciousness etc, etc. None of this is there.&nbsp;&nbsp;

00:07:30.080 --> 00:07:35.200
And it won't be there unless we put it there.&nbsp;
In particular, I think what we can, should,&nbsp;&nbsp;

00:07:35.200 --> 00:07:41.720
and will do is we're going to develop AI, remember&nbsp;
AI is under our control, we're going to develop it&nbsp;&nbsp;

00:07:41.720 --> 00:07:46.680
as an extension of our intelligence. It's not some&nbsp;
other creature that is going to come and attack&nbsp;&nbsp;

00:07:46.680 --> 00:07:53.640
us. That is just a fantastical sci-fi scenario&nbsp;
that unfortunately, Hollywood has promoted heavily&nbsp;&nbsp;

00:07:53.640 --> 00:07:59.800
because it makes for successful movies. People&nbsp;
think of Terminator when they think of AI. If&nbsp;&nbsp;

00:07:59.800 --> 00:08:05.580
after reading the book, what they think of instead&nbsp;
is PresiBot, then it has already achieved its aim.

00:08:05.580 --> 00:08:10.520
CRAIG:
Yeah. And the whole Terminator scenario–&nbsp;&nbsp;

00:08:11.240 --> 00:08:20.160
because people talk to me about this all the time,&nbsp;
they're talking about embodied AI and they're&nbsp;&nbsp;

00:08:20.160 --> 00:08:27.520
really talking about advanced robotics, which have&nbsp;
a really long way to go. There's some incredible&nbsp;&nbsp;

00:08:27.520 --> 00:08:33.160
things happening and things have moved very&nbsp;
swiftly in the last year or two but it's going&nbsp;&nbsp;

00:08:33.160 --> 00:08:41.800
to be a long time before you have a robot that can&nbsp;
navigate the the messiness of the world, let alone&nbsp;&nbsp;

00:08:42.360 --> 00:08:50.560
take action in the world in a way that would&nbsp;
be threatening to humans; that's my view. The&nbsp;&nbsp;

00:08:52.440 --> 00:09:00.680
question of superintelligence, there's a lot of&nbsp;
confusion, particularly in the public space about&nbsp;&nbsp;

00:09:00.680 --> 00:09:12.760
that because, there is a lot of superintelligence&nbsp;
already in AI. It's not this monolithic thing that&nbsp;&nbsp;

00:09:12.760 --> 00:09:21.760
we're going to reach someday, I think, where&nbsp;
suddenly you wake up one morning and there's&nbsp;&nbsp;

00:09:21.760 --> 00:09:29.360
this AI system that's infected the internet and&nbsp;
it's smarter than humans. I hear this from AI&nbsp;&nbsp;

00:09:29.360 --> 00:09:36.360
researchers in the alarmist camp. You know, “It'll&nbsp;
be smarter than you. You won't be able to stop it&nbsp;&nbsp;

00:09:36.360 --> 00:09:43.560
because it'll be smarter than you!” But there is&nbsp;
a lot of superintelligence already. I would argue&nbsp;&nbsp;

00:09:43.560 --> 00:09:59.120
that, for example, ChatGPT within the bounds of&nbsp;
its hallucinatory tendencies is superintelligent.&nbsp;&nbsp;

00:09:59.120 --> 00:10:12.920
It has access to more information than I do. It's&nbsp;
really a question of how reliable it is. But let's&nbsp;&nbsp;

00:10:12.920 --> 00:10:24.640
move on from that; you're very critical of the&nbsp;
Silicon Valley culture in the book. Can you talk&nbsp;&nbsp;

00:10:24.640 --> 00:10:36.160
about that a little bit? I'm on the East Coast.&nbsp;
I'm not steeped in Silicon Valley culture but it&nbsp;&nbsp;

00:10:36.160 --> 00:10:46.120
amazes me that you've got this concentration of&nbsp;
billionaires in and around San Francisco that are&nbsp;&nbsp;

00:10:46.120 --> 00:10:58.720
creating superhuman technology, even if it's not&nbsp;
consistent or across all domains. Yet they live in&nbsp;&nbsp;

00:10:58.720 --> 00:11:05.840
this– I'll probably get in trouble, but I lived in&nbsp;
the Bay Area for a while, but this real cesspool&nbsp;&nbsp;

00:11:05.840 --> 00:11:18.480
of crime, homelessness, and social dysfunction.&nbsp;
And I keep on thinking, you know, I should email&nbsp;&nbsp;

00:11:18.480 --> 00:11:24.400
Elon Musk and Jeff Bezos. I mean, between them,&nbsp;
they could solve the homelessness problem like&nbsp;&nbsp;

00:11:24.400 --> 00:11:34.120
that, with a fraction of their wealth. Can&nbsp;
you talk a little bit about that dysfunction?

00:11:34.120 --> 00:11:37.200
PEDRO:
So 2040, as the subtitle says,&nbsp;&nbsp;

00:11:37.200 --> 00:11:42.520
is very much a satire of Silicon Valley and the&nbsp;
culture of Silicon Valley around tech and around&nbsp;&nbsp;

00:11:42.520 --> 00:11:49.240
other things. It has several elements. One of&nbsp;
them is what you're referring to, which I'm always&nbsp;&nbsp;

00:11:49.240 --> 00:11:57.360
struck by when I go there these days. There's such&nbsp;
a contrast between the utopian high tech side and&nbsp;&nbsp;

00:11:57.360 --> 00:12:03.640
the dystopian. I mean, in San Francisco you see&nbsp;
the self-driving cars go by on Market Street,&nbsp;&nbsp;

00:12:03.640 --> 00:12:09.920
or you're in one. Then behind them, you see these&nbsp;
homeless people injecting and lying sprawling on&nbsp;&nbsp;

00:12:09.920 --> 00:12:18.400
the ground and piles of trash and poop. It makes&nbsp;
Neuromancer look like a children's story. It's&nbsp;&nbsp;

00:12:18.400 --> 00:12:24.280
like, how could this happen? And again, part of&nbsp;
the goal of a satire, and this one in particular&nbsp;&nbsp;

00:12:24.280 --> 00:12:31.600
is to say, if we let this go on, where is this&nbsp;
going to end and how do we stop it? So one of the&nbsp;&nbsp;

00:12:31.600 --> 00:12:37.440
elements of the satire is– so there's this giant&nbsp;
company called Happinet, which is the ultimate&nbsp;&nbsp;

00:12:37.440 --> 00:12:43.680
everything app, which exists in China and every US&nbsp;
tech billionaire wishes that’s what they had. It's&nbsp;&nbsp;

00:12:43.680 --> 00:12:48.320
an app that does everything for you. And there's&nbsp;
the headquarters, which is this super high tower,&nbsp;&nbsp;

00:12:48.320 --> 00:12:54.040
which is, of course, a caricature of the&nbsp;
Salesforce Tower, soaring high above the streets&nbsp;&nbsp;

00:12:54.040 --> 00:13:00.200
in San Francisco. The CEO of this company looks&nbsp;
down on the masses and he says they're just pixels&nbsp;&nbsp;

00:13:00.200 --> 00:13:08.880
and I can see the whole picture. Underground,&nbsp;
under San Francisco, there's a vast data center.&nbsp;&nbsp;

00:13:08.880 --> 00:13:14.760
It's there to minimize latency and blah, blah.&nbsp;
But then Ethan, the protagonist– Ethan is the&nbsp;&nbsp;

00:13:14.760 --> 00:13:21.400
CEO of this KumbAI startup that created PresiBot.&nbsp;
And at least in the beginning, he's the guy who&nbsp;&nbsp;

00:13:21.400 --> 00:13:28.560
nominally controls it, then other things happen.&nbsp;
At one point he discovers that this underground&nbsp;&nbsp;

00:13:28.560 --> 00:13:37.240
data center is full of homeless people squatting&nbsp;
there, because life is better there. It's not as&nbsp;&nbsp;

00:13:37.240 --> 00:13:43.760
cold and it's more sheltered. And there are all&nbsp;
these robots so that the surveillance cameras&nbsp;&nbsp;

00:13:43.760 --> 00:13:51.800
will not notice them. So again, there's multiple&nbsp;
layers of commentary in this but of course part of&nbsp;&nbsp;

00:13:51.800 --> 00:13:58.520
the question is, how did we wind up here? And Dave&nbsp;
Neufeld, who's the CEO of this company, of course&nbsp;&nbsp;

00:13:58.520 --> 00:14:04.600
he's inspired by the Elon Musks, Mark Zuckerbergs,&nbsp;
Jeff Bezos and whatnot of this world. You say&nbsp;&nbsp;

00:14:04.600 --> 00:14:10.000
that they could solve the homelessness problem&nbsp;
with a stroke of the pen. Actually, it's much&nbsp;&nbsp;

00:14:10.000 --> 00:14:18.160
more complicated than that. And again, as much as&nbsp;
it's a short book, I try to not oversimplify. The&nbsp;&nbsp;

00:14:18.960 --> 00:14:23.760
homelessness problem in San Francisco and other&nbsp;
places does not exist because of lack of money to&nbsp;&nbsp;

00:14:23.760 --> 00:14:30.160
solve it at all. Even without the billionaires,&nbsp;
we have put billions and billions of dollars in.&nbsp;&nbsp;

00:14:30.160 --> 00:14:35.480
The problem only gets worse. It exists because,&nbsp;
again, we could go into that but it's more of a&nbsp;&nbsp;

00:14:35.480 --> 00:14:43.560
political problem than a technical problem. Part&nbsp;
of what I spoof very much in this book is the&nbsp;&nbsp;

00:14:43.560 --> 00:14:48.880
technical people's naive notion that all these&nbsp;
problems have tech solutions. The first chapter&nbsp;&nbsp;

00:14:48.880 --> 00:14:55.720
is called “Optimize America!” because that's the&nbsp;
slogan of the PresiBot campaign. Then they very&nbsp;&nbsp;

00:14:55.720 --> 00:15:00.500
quickly discover that things are a lot more&nbsp;
gnarly than that, if you will, and they are.

00:15:00.500 --> 00:15:02.960
CRAIG:
Yeah. And when&nbsp;&nbsp;

00:15:02.960 --> 00:15:16.000
I said they could solve it, I wrote a lot, when I&nbsp;
was at the Times, about the Housing First policy.&nbsp;&nbsp;

00:15:16.000 --> 00:15:24.640
I don't know if you've read about it. This guy&nbsp;
Sam Tsemberis, who developed that during the&nbsp;&nbsp;

00:15:24.640 --> 00:15:32.280
Carter administration in New York and it's been&nbsp;
implemented in some places very successfully.&nbsp;&nbsp;

00:15:32.280 --> 00:15:40.120
The thing that really prevents it from being&nbsp;
implemented in larger cities is coordination&nbsp;&nbsp;

00:15:40.120 --> 00:15:49.720
between and among institutions. You just can't&nbsp;
get everyone on the same page and as a result,&nbsp;&nbsp;

00:15:49.720 --> 00:15:54.440
you end up with a fragmented system, and each&nbsp;
one is trying to address a different aspect&nbsp;&nbsp;

00:15:54.440 --> 00:16:05.880
of the problem. But anyway, that's for another&nbsp;
conversation. I mean, my view is if they would&nbsp;&nbsp;

00:16:05.880 --> 00:16:15.840
build housing for the homeless that would be&nbsp;
a leap toward a solution. And certainly the&nbsp;&nbsp;

00:16:15.840 --> 00:16:23.440
tech billionaires have that money. You can't&nbsp;
get it politically because the politics are&nbsp;&nbsp;

00:16:23.440 --> 00:16:33.720
so divisive that you can't come to a consensus.&nbsp;
Yeah, and Ethan lives in this tiny little flat,&nbsp;&nbsp;

00:16:33.720 --> 00:16:42.080
which I thought was funny. That's more of a New&nbsp;
York trope to me than San Francisco. I mean,&nbsp;&nbsp;

00:16:42.080 --> 00:16:47.000
San Francisco apartments are actually&nbsp;
larger than New York apartments.

00:16:49.520 --> 00:16:53.280
PEDRO:
By the way, that was actually inspired by reality.&nbsp;&nbsp;

00:16:53.280 --> 00:16:59.440
In particular, the apartment that he lives in&nbsp;
was inspired by a piece in 1843, The Economist&nbsp;&nbsp;

00:16:59.440 --> 00:17:05.920
Lifestyle magazine, where they talked about and&nbsp;
they showed various ways in which San Francisco&nbsp;&nbsp;

00:17:05.920 --> 00:17:11.280
is so expensive now that it's becoming like&nbsp;
Manhattan. And of course, it's also a tech Mecca.&nbsp;&nbsp;

00:17:11.280 --> 00:17:15.880
So people are coming up with all of these ways to&nbsp;
make ever smaller apartments, have all the things&nbsp;&nbsp;

00:17:15.880 --> 00:17:21.040
in them that you need, including beds that rise&nbsp;
into the ceiling and so on and so forth. And of&nbsp;&nbsp;

00:17:21.040 --> 00:17:26.280
course, that's what I'm making fun of in Ethan's&nbsp;
apartment. Again, it's completely real, that part.

00:17:26.280 --> 00:17:27.360
CRAIG:
Yeah.&nbsp;&nbsp;

00:17:30.880 --> 00:17:52.000
The idea of technology as a decision maker&nbsp;
is something that interests me. With agentic,&nbsp;&nbsp;

00:17:52.000 --> 00:17:55.020
which is a word I would&nbsp;
never have used a year ago–

00:17:55.020 --> 00:17:57.180
PEDRO:
No one would have used it a year ago.

00:17:57.180 --> 00:18:01.520
CRAIG:
That's right, models and with these reasoning&nbsp;&nbsp;

00:18:01.520 --> 00:18:11.640
models with Q*, Strawberry, or GPT-5 or whatever&nbsp;
you want to call it, that next generation. The&nbsp;&nbsp;

00:18:11.640 --> 00:18:24.400
idea of models that can make decisions and take&nbsp;
actions is becoming closer to reality. To me,&nbsp;&nbsp;

00:18:24.400 --> 00:18:39.000
that's a very positive thing. In the book it's&nbsp;
sort of the door to all the craziness but how&nbsp;&nbsp;

00:18:39.000 --> 00:18:44.960
do you feel about that? And I have a question&nbsp;
too, I had a conversation the other day about&nbsp;&nbsp;

00:18:44.960 --> 00:18:55.080
truth and AI is a truth engine, which was&nbsp;
fascinating, but how do you feel about the&nbsp;&nbsp;

00:18:55.080 --> 00:19:10.640
potential for agentic reasoning AI models to make&nbsp;
decisions? Maybe not autonomously, but certainly&nbsp;&nbsp;

00:19:10.640 --> 00:19:20.600
to aid in making decisions with humans to avoid&nbsp;
some of the foibles that humans are prone to.

00:19:20.600 --> 00:19:23.840
PEDRO:
AIs make decisions. That's what they've&nbsp;&nbsp;

00:19:23.840 --> 00:19:29.320
done since the beginning. And there's a whole&nbsp;
theory which you study in AI 101 about how to&nbsp;&nbsp;

00:19:29.320 --> 00:19:34.920
make decisions optimally and how to do search for&nbsp;
the best decisions, and etc. This is what classic&nbsp;&nbsp;

00:19:34.920 --> 00:19:40.280
AI is about. If you went to an AI conference&nbsp;
in 1980, it was all about automated reasoning,&nbsp;&nbsp;

00:19:40.280 --> 00:19:44.800
decision making, and planning and whatnot. Some&nbsp;
of that is not forgotten, unfortunately, but this&nbsp;&nbsp;

00:19:44.800 --> 00:19:52.080
is what AI does. So AI isn't really AI until&nbsp;
it's making decisions about something. Now the&nbsp;&nbsp;

00:19:52.680 --> 00:19:58.880
interesting questions are number one, decisions in&nbsp;
service of what goals? This is one of the crucial&nbsp;&nbsp;

00:19:58.880 --> 00:20:04.360
things: the difference between AI and a computer&nbsp;
program is that the computer program at some level&nbsp;&nbsp;

00:20:04.360 --> 00:20:09.640
doesn't make any decisions. It just does what&nbsp;
you told it to do in one situation or another.&nbsp;&nbsp;

00:20:09.640 --> 00:20:14.360
The difference with AI is that it does make&nbsp;
its own decisions but they are guided by the&nbsp;&nbsp;

00:20:14.360 --> 00:20:22.880
objectives that you put in. An AI system can never&nbsp;
do anything that is not by design subserving that&nbsp;&nbsp;

00:20:22.880 --> 00:20:28.840
objective. So this is unintuitive and I understand&nbsp;
people's fears but you could actually have a&nbsp;&nbsp;

00:20:28.840 --> 00:20:34.160
superintelligence that is vastly more intelligent&nbsp;
than us but you don't have to fear it because&nbsp;&nbsp;

00:20:34.160 --> 00:20:39.560
it's at our service. The intelligence is just&nbsp;
doing our bidding. Now there's a second part,&nbsp;&nbsp;

00:20:39.560 --> 00:20:44.280
which is, what happens when the AI starts&nbsp;
taking actions in the real world, say,&nbsp;&nbsp;

00:20:44.280 --> 00:20:50.400
a robot versus ChatGPT. Now of course, there are&nbsp;
many risks that weren't there before. But again,&nbsp;&nbsp;

00:20:50.400 --> 00:20:54.880
the bigger risk is that the AI will do damage.&nbsp;
A self-driving car might run somebody over,&nbsp;&nbsp;

00:20:54.880 --> 00:20:59.280
right? The self-driving car is deciding&nbsp;
whether to go left, right, stop, or accelerate,&nbsp;&nbsp;

00:20:59.280 --> 00:21:03.600
and it can make mistakes. So those are&nbsp;
the things that I think we need to fear;&nbsp;&nbsp;

00:21:04.280 --> 00:21:09.520
and I think we see some of that. It's interesting&nbsp;
seeing how this is shifting, even in just the last&nbsp;&nbsp;

00:21:09.520 --> 00:21:16.760
couple of years. We have a tendency, and the book&nbsp;
very much illustrates that, to prematurely put&nbsp;&nbsp;

00:21:16.760 --> 00:21:23.080
faith in AI as being more capable than it really&nbsp;
is. Because again, the demos are very impressive,&nbsp;&nbsp;

00:21:23.080 --> 00:21:27.040
it looks very intelligent; we're like, “Oh wow,&nbsp;
this AI is going to do all these things”. And then&nbsp;&nbsp;

00:21:27.040 --> 00:21:33.520
it blows up in your face, or you can jailbreak&nbsp;
it and manipulate it. So the problem is not the&nbsp;&nbsp;

00:21:33.520 --> 00:21:38.080
AI manipulating the humans, it's very much the&nbsp;
humans manipulating the AI. I think even from&nbsp;&nbsp;

00:21:38.080 --> 00:21:43.880
just a year ago to now by interacting with things&nbsp;
like ChatGPT people are coming to see, which I&nbsp;&nbsp;

00:21:43.880 --> 00:21:49.100
think is very healthy, AI as it really is and what&nbsp;
the real worries are versus the imaginary ones.

00:21:49.100 --> 00:21:55.680
CRAIG:
Yeah. There’s a couple of things I wanted&nbsp;&nbsp;

00:21:55.680 --> 00:22:07.200
to ask you that are sort of straying from the book&nbsp;
a little. I've talked to people who believe that&nbsp;&nbsp;

00:22:07.200 --> 00:22:18.160
AI will be an arbiter of truth. I don't believe&nbsp;
that at all, because truth is so subjective. I&nbsp;&nbsp;

00:22:18.160 --> 00:22:28.760
think the best that could happen is with a very&nbsp;
carefully constrained model, you could produce&nbsp;&nbsp;

00:22:28.760 --> 00:22:39.040
evidence based consensus on any issue based on&nbsp;
the training data. How do you feel about that,&nbsp;&nbsp;

00:22:39.040 --> 00:22:53.880
about using AI to settle questions where the&nbsp;
truth is unclear or hidden from a human observer?

00:22:53.880 --> 00:22:58.040
PEDRO:
I wouldn't use the expression AI as an arbiter&nbsp;&nbsp;

00:22:58.040 --> 00:23:07.280
of truth because arbiter is a very dangerous word.&nbsp;
I don't think AI can or will, or should have the&nbsp;&nbsp;

00:23:07.280 --> 00:23:15.000
final say on what is the truth. Having said that,&nbsp;
AI can and should, and I hope will be a fantastic&nbsp;&nbsp;

00:23:15.000 --> 00:23:22.760
tool to help us get closer to the truth. Very much&nbsp;
so, because it can gather and process evidence on&nbsp;&nbsp;

00:23:22.760 --> 00:23:31.760
a larger scale than any human can. Again, in 2040&nbsp;
there are two versions of PresiBot: the first&nbsp;&nbsp;

00:23:31.760 --> 00:23:38.040
version is ChatGPT, the second version, and more&nbsp;
interesting one, is a crowdsourced AI that in real&nbsp;&nbsp;

00:23:38.040 --> 00:23:43.560
time takes input from people and decides what is&nbsp;
true, what to do, where to go, and whatnot. This,&nbsp;&nbsp;

00:23:43.560 --> 00:23:49.600
I think is the real role of AI, is to increase&nbsp;
our collective intelligence and get us closer&nbsp;&nbsp;

00:23:49.600 --> 00:23:54.960
to the truth. Now you can never get to the truth&nbsp;
because, this is one of the basic lessons in AI,&nbsp;&nbsp;

00:23:54.960 --> 00:23:59.800
it doesn't matter how much computing power or&nbsp;
how much intelligence you have. Even just to&nbsp;&nbsp;

00:23:59.800 --> 00:24:06.120
play the game of chess optimally, you need&nbsp;
a computer bigger than the universe. Again,&nbsp;&nbsp;

00:24:06.120 --> 00:24:09.840
this is part of people's failure of imagination.&nbsp;
It's like, “Oh, it's more intelligent than us,&nbsp;&nbsp;

00:24:09.840 --> 00:24:15.760
therefore it's infinitely intelligent.” This is a&nbsp;
complete mistake between our intelligence and God,&nbsp;&nbsp;

00:24:16.360 --> 00:24:21.960
there's a vast, vast, vast gap. And yes, AI&nbsp;
will go beyond us, but AI is not going to do&nbsp;&nbsp;

00:24:21.960 --> 00:24:27.000
miracles. Now, unfortunately, what is happening&nbsp;
right now is that AI, instead of getting us&nbsp;&nbsp;

00:24:27.000 --> 00:24:31.680
closer to the truth, is actually arguably&nbsp;
getting us farther from the truth. Because,&nbsp;&nbsp;

00:24:31.680 --> 00:24:36.720
what happens with all the generative AI is that&nbsp;
it makes stuff up. That's what generative means,&nbsp;&nbsp;

00:24:36.720 --> 00:24:42.600
it means making stuff up. So what it's creating&nbsp;
is these deep fakes; and even like the problem&nbsp;&nbsp;

00:24:42.600 --> 00:24:50.080
with ChatGPT, which you alluded to, that it's&nbsp;
unreliable, ChatGPT, these LLMs literally have&nbsp;&nbsp;

00:24:50.080 --> 00:24:55.880
no notion of what is the truth. They just generate&nbsp;
text that makes people happy and that is similar&nbsp;&nbsp;

00:24:55.880 --> 00:25:00.880
to what the text that people have generated. That&nbsp;
in fact does not get us closer to the truth. It's&nbsp;&nbsp;

00:25:00.880 --> 00:25:09.760
actually completely ungrounded, and not completely&nbsp;
because that language, at least some of it comes&nbsp;&nbsp;

00:25:09.760 --> 00:25:16.120
from, say, good reporting and so on. But most of&nbsp;
it doesn't and ChatGPT can't tell the difference.

00:25:16.120 --> 00:25:26.280
CRAIG:
Yeah. And right now it's purely probabilistic.&nbsp;&nbsp;

00:25:26.280 --> 00:25:48.560
So if there isn't a probability curve that centers&nbsp;
on a consensus about an issue it'll come up with&nbsp;&nbsp;

00:25:48.560 --> 00:26:02.000
its own based on whatever distribution is in the&nbsp;
training data. If you had clean enough training&nbsp;&nbsp;

00:26:02.000 --> 00:26:14.120
data, presumably that covered enough of the&nbsp;
world or intellectual thought, it would give&nbsp;&nbsp;

00:26:14.120 --> 00:26:25.640
the consensus on any issue. I mean, you were&nbsp;
talking before about the second iteration of&nbsp;&nbsp;

00:26:25.640 --> 00:26:37.000
PresiBot which is crowdsourced. The problem with&nbsp;
the training data that is being used in a lot of&nbsp;&nbsp;

00:26:37.000 --> 00:26:45.600
these models, it's effectively crowdsourced,&nbsp;
but the crowd is unreliable because there's&nbsp;&nbsp;

00:26:46.720 --> 00:27:01.040
probably more unintelligence within the crowd&nbsp;
than there is intelligence for the AI to draw&nbsp;&nbsp;

00:27:01.040 --> 00:27:13.720
on. But I'm fascinated by the idea that you could,&nbsp;
either through careful curation of training data,&nbsp;&nbsp;

00:27:13.720 --> 00:27:26.000
come up with a system that could reliably answer&nbsp;
questions that are facing humans or governments&nbsp;&nbsp;

00:27:26.000 --> 00:27:34.880
in real time. PresiBot and PresiBot 2 aside,&nbsp;
is that something that you think is possible?

00:27:34.880 --> 00:27:37.840
PEDRO:
It's possible. And here's the big&nbsp;&nbsp;

00:27:37.840 --> 00:27:46.320
difference, at least one big difference between&nbsp;
1.0 and 2.0 is that PresiBot 1.0, like ChatGPT,&nbsp;&nbsp;

00:27:46.320 --> 00:27:52.920
it is learned from past data and that data by&nbsp;
definition is stale not only because time has&nbsp;&nbsp;

00:27:52.920 --> 00:27:58.920
passed but because you are not, ChatGPT,&nbsp;
is now answering something that it hasn't&nbsp;&nbsp;

00:27:58.920 --> 00:28:04.480
answered before. It's a new dialogue, PrisiBot&nbsp;
is in a new situation. Then what it has to do,&nbsp;&nbsp;

00:28:04.480 --> 00:28:12.000
and this is really the crucial point, is that it&nbsp;
has to generalize to the new situation from the&nbsp;&nbsp;

00:28:12.000 --> 00:28:18.920
ones that are in the text. And the more different&nbsp;
they are, the worse it gets. This is also true of&nbsp;&nbsp;

00:28:18.920 --> 00:28:23.320
humans but humans are much better at generalizing&nbsp;
from their experience to new situations than the&nbsp;&nbsp;

00:28:23.320 --> 00:28:27.600
best machine learning algorithms that we have&nbsp;
today. Hopefully we will get better algorithms.&nbsp;&nbsp;

00:28:27.600 --> 00:28:37.480
That's what I work on for a living, if you will.&nbsp;
You're right, it's also crowdsourcing in the sense&nbsp;&nbsp;

00:28:37.480 --> 00:28:43.520
that it came from people but this is crowdsourcing&nbsp;
in real time. Meaning, when there's a situation,&nbsp;&nbsp;

00:28:43.520 --> 00:28:49.160
what PresiBot 2.0 does, and actually technically&nbsp;
today this is possible, it listens to millions of&nbsp;&nbsp;

00:28:49.160 --> 00:28:54.280
people saying what they know, what they believe,&nbsp;
what should happen, and making suggestions;&nbsp;&nbsp;

00:28:54.280 --> 00:29:00.440
there are several examples of this in the book. So&nbsp;
this can actually work much, much better because&nbsp;&nbsp;

00:29:00.440 --> 00:29:06.240
it's taking input from our human intelligences&nbsp;
in the moment about the actual situation,&nbsp;&nbsp;

00:29:06.240 --> 00:29:11.160
about doing some very far fetched thing that&nbsp;
will be unreliable and prone to hallucination and&nbsp;&nbsp;

00:29:11.160 --> 00:29:17.640
whatnot. I think there's a lot to do to perfect&nbsp;
that. But this is very much a possibility and a&nbsp;&nbsp;

00:29:17.640 --> 00:29:23.200
very good one that is being very neglected right&nbsp;
now. And it's not just at the level of a president&nbsp;&nbsp;

00:29:23.200 --> 00:29:28.600
of the United States, the level of a company.&nbsp;
Again, the company that created PresiBot was&nbsp;&nbsp;

00:29:28.600 --> 00:29:34.160
created as a demo to then tell CEOs to companies.&nbsp;
If you think about the job of a company or the&nbsp;&nbsp;

00:29:34.160 --> 00:29:39.800
CEO of a company, a lot of the decisions are very&nbsp;
mundane, but it's screwing up all the time because&nbsp;&nbsp;

00:29:39.800 --> 00:29:43.680
it needs to do this for the customer, it needs&nbsp;
this knowledge that actually somebody else has,&nbsp;&nbsp;

00:29:43.680 --> 00:29:47.640
but I don't even know that it has, and I'm&nbsp;
handing it off from one customer service rep&nbsp;&nbsp;

00:29:47.640 --> 00:29:53.600
to another to the chat bot, there's this hell&nbsp;
that people live in and AI can overcome that.

00:29:53.600 --> 00:29:58.040
CRAIG:
Yeah. Do you think that&nbsp;&nbsp;

00:29:59.080 --> 00:30:09.680
the risks that AI governance, and we've seen as&nbsp;
I said, a lot of activity, the Europeans are,&nbsp;&nbsp;

00:30:09.680 --> 00:30:21.080
are working hard to protect the consumers&nbsp;
and the US is relying more on existing&nbsp;&nbsp;

00:30:21.080 --> 00:30:28.120
institutions. Do you think that the governance&nbsp;
is keeping up? One of the themes in the book is&nbsp;&nbsp;

00:30:28.120 --> 00:30:43.800
that the race for innovation in Silicon&nbsp;
Valley is throwing us blindly into danger.&nbsp;&nbsp;

00:30:43.800 --> 00:30:55.120
But I kind of think that the regulation is&nbsp;
a pretty fast follower. What do you think?

00:30:55.120 --> 00:31:00.360
PEDRO:
Where to start? One of the central&nbsp;&nbsp;

00:31:00.360 --> 00:31:07.480
plot elements in 2040 is PresiBot’s panic button.&nbsp;
The panic button is the kill switch of the AI, and&nbsp;&nbsp;

00:31:07.480 --> 00:31:11.440
this is a real thing. People have proposed having&nbsp;
kill switches, there are research papers on having&nbsp;&nbsp;

00:31:11.440 --> 00:31:16.080
kill switches, I would not be surprised at all if&nbsp;
a bunch of regulations in various countries demand&nbsp;&nbsp;

00:31:16.080 --> 00:31:23.200
kill switches. So Presibot has a kill switch which&nbsp;
is in the hands of Ethan, the CEO of the company,&nbsp;&nbsp;

00:31:23.200 --> 00:31:31.240
then Ethan loses the kill switch and then the fun&nbsp;
begins. Again, there is a fun element to that,&nbsp;&nbsp;

00:31:31.240 --> 00:31:35.680
of course, of the chaos that ensues and the&nbsp;
people fighting over it and so on. But this&nbsp;&nbsp;

00:31:35.680 --> 00:31:40.880
is making a real point, the point is, these&nbsp;
measures that you put in place to make AI safe,&nbsp;&nbsp;

00:31:40.880 --> 00:31:47.160
if you're not careful, make it less safe. Kill&nbsp;
switches are a great example of that. Kill switch&nbsp;&nbsp;

00:31:47.160 --> 00:31:51.480
is like the ultimate guarantee against Terminator,&nbsp;
but actually it's nothing of the kind. It's just&nbsp;&nbsp;

00:31:51.480 --> 00:31:59.320
another vector by which chaos can come in. Let&nbsp;
me put it this way: the real problem in AI is&nbsp;&nbsp;

00:31:59.320 --> 00:32:08.560
who controls it. The kill switch in a way is the&nbsp;
embodiment of that in 2040. Then the realization&nbsp;&nbsp;

00:32:08.560 --> 00:32:13.800
that he has is, instead of having people fight&nbsp;
over the kill switch, I should give everybody one&nbsp;&nbsp;

00:32:13.800 --> 00:32:20.400
because this is a democracy. So the kill switch&nbsp;
originally is an app that Arvind, the CTO, and his&nbsp;&nbsp;

00:32:20.400 --> 00:32:27.120
friend coded up quickly that literally has a red&nbsp;
button that you push or that you can use to talk&nbsp;&nbsp;

00:32:28.360 --> 00:32:34.280
through PresiBot. What they do, because they're&nbsp;
very short of time, they call it the PresiApp,&nbsp;&nbsp;

00:32:34.280 --> 00:32:38.880
they create PresiApp and anybody can push the&nbsp;
button and talk into PresiApp. Then of course&nbsp;&nbsp;

00:32:38.880 --> 00:32:44.800
the interesting question becomes, when there's 20&nbsp;
or 100 million people talking into PresiBot at the&nbsp;&nbsp;

00:32:44.800 --> 00:32:53.640
same time, what does the AI do? And you alluded&nbsp;
to, it will reflect the consensus view. Well,&nbsp;&nbsp;

00:32:53.640 --> 00:32:57.840
maybe when there's a consensus but often&nbsp;
the problem is that there is no consensus.

00:32:57.840 --> 00:32:58.520
CRAIG:
Yeah, or&nbsp;&nbsp;

00:32:58.520 --> 00:33:05.240
that the consensus is misguided as we saw,&nbsp;
I don't know how you feel– with Brexit.

00:33:07.240 --> 00:33:08.280
PEDRO:
Actually,&nbsp;&nbsp;

00:33:08.280 --> 00:33:14.200
Brexit is a great example. So Brexit actually won&nbsp;
by a fairly slim margin. If the bunch of people&nbsp;&nbsp;

00:33:14.200 --> 00:33:18.000
who didn't bother to vote, particularly the young&nbsp;
people, had bothered to vote, it would have gone&nbsp;&nbsp;

00:33:18.000 --> 00:33:25.840
in a different direction. Today there's a great&nbsp;
majority, 60 something percent, last that I saw,&nbsp;&nbsp;

00:33:25.840 --> 00:33:32.440
or something in that range, of British people who&nbsp;
regret Brexit. I would say that one of the primary&nbsp;&nbsp;

00:33:32.440 --> 00:33:38.360
roles of AI, and again this is what we do in&nbsp;
machine learning, is to try to see ahead. Try to&nbsp;&nbsp;

00:33:38.360 --> 00:33:44.960
guide us, not decide for us, but say, [Look, this&nbsp;
is what you want and you think that Brexit will&nbsp;&nbsp;

00:33:44.960 --> 00:33:52.840
give it to you. Here's maybe why it won't– “We&nbsp;
want to deregulate Britain and be free from all&nbsp;&nbsp;

00:33:52.840 --> 00:33:58.440
the European Union red tape.”] We knew that wasn't&nbsp;
going to happen. [We want to stem immigration and&nbsp;&nbsp;

00:33:58.440 --> 00:34:03.320
whatnot], there's less immigration from Europe,&nbsp;
there's more from other sides. So that's why&nbsp;&nbsp;

00:34:03.320 --> 00:34:09.280
it has turned out badly. But good AI could&nbsp;
actually have seen this and among other things,&nbsp;&nbsp;

00:34:09.280 --> 00:34:15.440
alerted people to this. Another thing that I think&nbsp;
should happen, and again, the book alludes to,&nbsp;&nbsp;

00:34:15.440 --> 00:34:21.880
is people often don't have time for politics even&nbsp;
though it's very important. But you could deputize&nbsp;&nbsp;

00:34:21.880 --> 00:34:28.720
your model to represent you. Your model, with your&nbsp;
authorization, based on whatever data you want,&nbsp;&nbsp;

00:34:28.720 --> 00:34:34.240
is a better representative of you for the decision&nbsp;
making than your member of Parliament or your&nbsp;&nbsp;

00:34:34.240 --> 00:34:40.280
member of Congress. For two reasons: one is that&nbsp;
it doesn't have the cognitive limitations that the&nbsp;&nbsp;

00:34:40.280 --> 00:34:45.560
members of Congress do. But even more important,&nbsp;
it doesn't have conflicts of interest. So you can&nbsp;&nbsp;

00:34:45.560 --> 00:34:51.760
have, instead of 400 people deciding for the 300&nbsp;
million of us, what you can have is 300 million&nbsp;&nbsp;

00:34:51.760 --> 00:34:57.760
models that will have a much more interesting&nbsp;
argument going back and forth about any issue that&nbsp;&nbsp;

00:34:57.760 --> 00:35:03.220
you care to decide about. I think we're going&nbsp;
to look back on what we have today as barbarism.

00:35:03.220 --> 00:35:06.800
CRAIG:
Yeah, I agree. And that's actually to&nbsp;&nbsp;

00:35:06.800 --> 00:35:15.600
the idea of a truth engine. Right now, the models,&nbsp;
you can easily manipulate them through prompts to&nbsp;&nbsp;

00:35:15.600 --> 00:35:24.480
say anything you want. But if you had a hundred&nbsp;
flowers blooming, to use the Maoist phrase,&nbsp;&nbsp;

00:35:24.480 --> 00:35:35.720
and all of these AIs debating to the point of&nbsp;
convergence, maybe that's the closest you could&nbsp;&nbsp;

00:35:35.720 --> 00:35:49.760
get to optimal decision making or the truth. I saw&nbsp;
a survey recently about how public perception of&nbsp;&nbsp;

00:35:49.760 --> 00:35:57.640
AI is at an all time low. I mean, in the short&nbsp;
time that people have been thinking about it,&nbsp;&nbsp;

00:35:59.560 --> 00:36:08.080
because their experience of it has been&nbsp;
frustrating and glitchy and layered on&nbsp;&nbsp;

00:36:08.080 --> 00:36:26.200
top is all of the alarmist media. In the way that&nbsp;
social media insinuated itself into society, in a&nbsp;&nbsp;

00:36:26.200 --> 00:36:35.200
way that a lot of people didn't understand while&nbsp;
it was happening, and now everyone recognizes&nbsp;&nbsp;

00:36:37.240 --> 00:36:47.760
the evils, frankly, of social media, the way it's&nbsp;
managed today; how do you see AI integrating into&nbsp;&nbsp;

00:36:47.760 --> 00:36:56.560
society, because I see the same thing? I mean,&nbsp;
you think about it, a lot of the people I talk to&nbsp;&nbsp;

00:36:56.560 --> 00:37:10.360
think about it. But I go camping upstate with some&nbsp;
buddies who are construction workers or that sort&nbsp;&nbsp;

00:37:10.360 --> 00:37:17.400
of thing, and they have no clue. And frankly, they&nbsp;
don't want to know but I keep telling them this&nbsp;&nbsp;

00:37:17.400 --> 00:37:25.360
stuff, it's going to impact your life or at least&nbsp;
your children's lives. So, are you optimistic&nbsp;&nbsp;

00:37:25.360 --> 00:37:34.240
about that integration? Certainly in the book it&nbsp;
doesn't paint a very optimistic picture. What do&nbsp;&nbsp;

00:37:34.240 --> 00:37:41.760
you think has to happen for it to integrate&nbsp;
in a healthier way than social media did?

00:37:41.760 --> 00:37:44.120
PEDRO:
I think the first thing that people should&nbsp;&nbsp;

00:37:44.120 --> 00:37:52.160
realize is that AI has been part of their lives&nbsp;
for decades now. The most important application of&nbsp;&nbsp;

00:37:52.160 --> 00:37:57.720
AI in the world today, by far, is not chatbots.&nbsp;
Chatbots, for the most part are entertainment&nbsp;&nbsp;

00:37:57.720 --> 00:38:02.920
or they do mundane things like generate vacuous&nbsp;
text or text that doesn't need to have a very&nbsp;&nbsp;

00:38:02.920 --> 00:38:08.920
high standard, etc. The most important application&nbsp;
of AI today is things like recommendation systems.&nbsp;&nbsp;

00:38:08.920 --> 00:38:16.320
It's making choices for you from the vastness of&nbsp;
the infosphere, it's giving you search results,&nbsp;&nbsp;

00:38:16.320 --> 00:38:21.360
choosing movies and books and music and&nbsp;
whatnot for you to consume, its Amazon product&nbsp;&nbsp;

00:38:21.360 --> 00:38:27.800
recommendations, the tweets that you see, the&nbsp;
posts on on social media, they are all selected&nbsp;&nbsp;

00:38:27.800 --> 00:38:34.080
for you by AI. So I'm not surprised that people&nbsp;
have this negative view of AI right now because&nbsp;&nbsp;

00:38:34.080 --> 00:38:38.280
most people have only recently become aware&nbsp;
of AI. And what they see, as you say, is,&nbsp;&nbsp;

00:38:38.280 --> 00:38:43.400
on the one hand, the crap that it generates, on&nbsp;
the other hand, all the alarmist fears. I think&nbsp;&nbsp;

00:38:43.400 --> 00:38:48.120
the antidote for this, and I think it will happen&nbsp;
but we have to make it happen because as you say,&nbsp;&nbsp;

00:38:48.120 --> 00:38:54.760
there are multiple forces pushing in the other&nbsp;
direction, people have to get to know AI better.&nbsp;&nbsp;

00:38:54.760 --> 00:38:59.960
They are in some ways getting to know AI better.&nbsp;
For example, the incompetence AI actually runs&nbsp;&nbsp;

00:38:59.960 --> 00:39:05.600
counter to the Terminator fears. Like, [oh, we&nbsp;
need a moratorium of AI before a disaster happens&nbsp;&nbsp;

00:39:05.600 --> 00:39:12.400
now], is ridiculous. People are like, what?&nbsp;
ChatGPT? But we tend to veer between those two&nbsp;&nbsp;

00:39:12.400 --> 00:39:18.080
poles; so we have to keep making a better. That's&nbsp;
for us, the engineers and the researchers. But&nbsp;&nbsp;

00:39:18.080 --> 00:39:26.840
we also have to, in some ways, enlist people&nbsp;
to use AI for their benefit. This is how AI&nbsp;&nbsp;

00:39:26.840 --> 00:39:31.840
can, should, and will permeate society. It’s not,&nbsp;
[We have disclosed AIs that belong to these large&nbsp;&nbsp;

00:39:31.840 --> 00:39:36.480
companies that do a bunch of stuff under the&nbsp;
hood.] It's like a car shows up at your door&nbsp;&nbsp;

00:39:36.480 --> 00:39:42.800
with no steering wheel and says, get in, I know&nbsp;
where you want to go. Wouldn't you rather drive&nbsp;&nbsp;

00:39:42.800 --> 00:39:48.200
the car yourself? Well, in order to drive the car&nbsp;
yourself, you need two things. You need to demand&nbsp;&nbsp;

00:39:48.200 --> 00:39:52.480
that the car have a steering wheel and pedals&nbsp;
because they're there, they're just not exposed&nbsp;&nbsp;

00:39:52.480 --> 00:39:57.720
to you. So that's number one. Then number two, you&nbsp;
have to learn to drive. So this is what I want to&nbsp;&nbsp;

00:39:57.720 --> 00:40:03.440
encourage people to do, is to demand a car with&nbsp;
a steering wheel. And number two: learn to drive.

00:40:03.440 --> 00:40:05.480
CRAIG:
Although,&nbsp;&nbsp;

00:40:06.520 --> 00:40:21.000
certain things will simply go away as obsolete or&nbsp;
interesting. When I hear people complain about AI,&nbsp;&nbsp;

00:40:21.000 --> 00:40:27.320
“oh, I'm never going to use AI”, “I didn't&nbsp;
ask for it!”; you hear that all the time.&nbsp;&nbsp;

00:40:30.440 --> 00:40:38.400
I always say, [Well, when was the last time you&nbsp;
used a paper map to get somewhere?] People don't&nbsp;&nbsp;

00:40:38.400 --> 00:40:48.080
really understand that that's AI. In a way&nbsp;
that paper maps are pretty much obsolete now,&nbsp;&nbsp;

00:40:48.080 --> 00:40:54.680
maybe steering wheels and accelerator and&nbsp;
brake pedals will be obsolete, I don't know.&nbsp;&nbsp;

00:40:57.440 --> 00:41:04.760
As much as I enjoy driving, I can imagine a&nbsp;
day where driving is a little bit like people&nbsp;&nbsp;

00:41:04.760 --> 00:41:14.480
that drive stick shift cars today. It's kind of&nbsp;
a choice that's unnecessary, but they enjoy it.

00:41:14.480 --> 00:41:18.280
PEDRO:
Just to clarify, I wasn't literally talking about&nbsp;&nbsp;

00:41:18.280 --> 00:41:24.180
self-driving cars. I understand the confusion, I&nbsp;
was using cars as a metaphor for control of AI.

00:41:24.180 --> 00:41:25.160
CRAIG:
I see.

00:41:26.240 --> 00:41:28.120
PEDRO:
I talk about this in my previous book,&nbsp;&nbsp;

00:41:28.120 --> 00:41:33.160
The Master Algorithm. AIs have the equivalent&nbsp;
of a steering wheel, which is the objective&nbsp;&nbsp;

00:41:33.160 --> 00:41:38.080
function. It's the metric that they're trying&nbsp;
to optimize. It's the means by which we control&nbsp;&nbsp;

00:41:38.080 --> 00:41:43.520
AI. I think the discussion around AI should be&nbsp;
focused on what the metrics should be. Should&nbsp;&nbsp;

00:41:43.520 --> 00:41:48.760
those metrics just be set by the company? Should&nbsp;
there be a component that is mandated by law to&nbsp;&nbsp;

00:41:48.760 --> 00:41:54.560
be for the social good? There should definitely,&nbsp;
I think, be a major component, the biggest one,&nbsp;&nbsp;

00:41:54.560 --> 00:41:58.720
which is under the control of you, the user.&nbsp;
So this is what I metaphorically mean by the&nbsp;&nbsp;

00:41:58.720 --> 00:42:11.480
steering wheel of the AI. It's very good that&nbsp;
people are now aware of AI as AI. I think it's&nbsp;&nbsp;

00:42:11.480 --> 00:42:24.040
incumbent on us, the experts to tell people&nbsp;
about what AI really is. At the end of the day,&nbsp;&nbsp;

00:42:24.040 --> 00:42:30.840
to quote Karl Marx, it's all about who has the&nbsp;
power. AI will change society in the direction of&nbsp;&nbsp;

00:42:30.840 --> 00:42:39.360
the people who use it better. So, you never asked&nbsp;
for AI but that is a very dangerous proposition.&nbsp;&nbsp;

00:42:39.360 --> 00:42:43.960
It's like Henry Ford said, “If I had asked people&nbsp;
what they wanted, they would have asked for faster&nbsp;&nbsp;

00:42:45.800 --> 00:42:52.920
horses.” Imagine trying to do anything with horses&nbsp;
today, competing with someone who has a car. So&nbsp;&nbsp;

00:42:52.920 --> 00:42:58.760
we all need to start thinking about how to use&nbsp;
AI in our jobs to automate some of the tasks,&nbsp;&nbsp;

00:42:58.760 --> 00:43:02.920
but not all, because that's usually what&nbsp;
happens with AI, in our private lives.&nbsp;&nbsp;

00:43:02.920 --> 00:43:07.640
One of the biggest roles of AI today is in&nbsp;
recommending matches. There's people who were&nbsp;&nbsp;

00:43:07.640 --> 00:43:14.200
born because they had matched parents. Two, as a&nbsp;
citizen, how can you use AI to inform you better,&nbsp;&nbsp;

00:43:14.200 --> 00:43:20.400
to represent you in a lot of decision processes,&nbsp;
etc. Honestly, the people who do that will have&nbsp;&nbsp;

00:43:20.400 --> 00:43:24.980
power and the ones who don't do that will not&nbsp;
have power. So which one do you want to be?

00:43:24.980 --> 00:43:27.120
CRAIG:
Yeah, you're right.&nbsp;&nbsp;

00:43:29.680 --> 00:43:35.180
If you were to sum up, what are&nbsp;
the three messages in your book?

00:43:35.180 --> 00:43:37.760
PEDRO:
I would say that the first&nbsp;&nbsp;

00:43:37.760 --> 00:43:45.880
message is that AI is PrisiBot, not Terminator.&nbsp;
When you think of AI think of an imperfect human&nbsp;&nbsp;

00:43:45.880 --> 00:43:51.920
creation that is being debugged, that is a work in&nbsp;
progress. When you look at an AI, see through the&nbsp;&nbsp;

00:43:51.920 --> 00:43:59.760
AI to the people controlling it. AI is like mirror&nbsp;
shades, you don't see the eyes behind it. You got&nbsp;&nbsp;

00:43:59.760 --> 00:44:03.960
to think about the people staring at you on the&nbsp;
other side of the AI. That I think is the first&nbsp;&nbsp;

00:44:03.960 --> 00:44:08.800
message and if people just get that one message,&nbsp;
that's already great. The second one, which&nbsp;&nbsp;

00:44:08.800 --> 00:44:14.920
comes on the heels of that, is what PresiBot 2.0&nbsp;
illustrates. If you now think of I as an imperfect&nbsp;&nbsp;

00:44:14.920 --> 00:44:20.440
artifact that is a collective creation of a bunch&nbsp;
of people, what do you want it to be? What do you&nbsp;&nbsp;

00:44:20.440 --> 00:44:26.280
want to do with it? Let us start perfecting&nbsp;
democracy using AI. That's what PresiBot 2.0&nbsp;&nbsp;

00:44:26.280 --> 00:44:30.800
on a good day, of course, some things do go wrong&nbsp;
because there's still pitfalls, is a better form&nbsp;&nbsp;

00:44:30.800 --> 00:44:36.320
of democracy. It's overcoming this trade off&nbsp;
between representative and direct democracy. We&nbsp;&nbsp;

00:44:36.320 --> 00:44:42.920
need to do that because autocracies are using AI&nbsp;
to perfect themselves. The third message I would&nbsp;&nbsp;

00:44:42.920 --> 00:44:49.380
say is more to do, not with AI in particular but&nbsp;
with the tech world and with politics and what&nbsp;&nbsp;

00:44:49.380 --> 00:44:55.320
not, which is there's you see this so much today,&nbsp;
and again, it’s one of my motivations for writing&nbsp;&nbsp;

00:44:55.320 --> 00:45:00.920
the book, is that the tech people start out with&nbsp;
a very naive notion that this technology is going&nbsp;&nbsp;

00:45:00.920 --> 00:45:09.760
to make the world better. And it's super naive, it&nbsp;
is. But then what happens, and the last five years&nbsp;&nbsp;

00:45:09.760 --> 00:45:14.880
AI has just been hit with this, is that as soon as&nbsp;
the technology starts to make a difference in the&nbsp;&nbsp;

00:45:14.880 --> 00:45:21.200
real world, everybody scrambles for control of it.&nbsp;
For example, there are a lot of people trying to&nbsp;&nbsp;

00:45:21.200 --> 00:45:26.960
make, and this honestly chills my blood, trying&nbsp;
to make machine learning systems learn models&nbsp;&nbsp;

00:45:27.520 --> 00:45:33.960
of what the world should be instead of what it&nbsp;
is like. People at Google do this right now.&nbsp;&nbsp;

00:45:33.960 --> 00:45:40.520
They will alter, for example, the gender ratios&nbsp;
of professions and whatnot to make it look like&nbsp;&nbsp;

00:45:40.520 --> 00:45:45.280
what the world should be, to make them, I'm&nbsp;
making up the example but it's a real one,&nbsp;&nbsp;

00:45:45.280 --> 00:45:51.120
like 50% of programmers are women. I'm like, no,&nbsp;
the goal of the machine learning is to tell the&nbsp;&nbsp;

00:45:51.120 --> 00:45:55.640
truth, going back to your earlier question. Then&nbsp;
on top of that, we can have whatever decision&nbsp;&nbsp;

00:45:55.640 --> 00:46:01.560
processes and objectives we want, but do not&nbsp;
create an Orwellian world on the back of AI.&nbsp;&nbsp;

00:46:02.600 --> 00:46:09.840
So it's very important that we be aware that this&nbsp;
is going on. Right now, AI's creating a world for&nbsp;&nbsp;

00:46:09.840 --> 00:46:15.360
you that makes these choices and creates&nbsp;
an imaginary world that is different from&nbsp;&nbsp;

00:46:15.360 --> 00:46:23.640
the real one. It's like 1984 but taken to a whole&nbsp;
different level. Again, if all that the book does&nbsp;&nbsp;

00:46:23.640 --> 00:46:28.640
is make people aware of this and make them start&nbsp;
thinking about it, that's that's already great.

00:46:28.640 --> 00:46:33.760
CRAIG:
Yeah. What are you working on now that the&nbsp;&nbsp;

00:46:33.760 --> 00:46:44.840
book's out, or on its way out? Are you still doing&nbsp;
research? Are you going to become a novelist?

00:46:44.840 --> 00:46:46.040
PEDRO:
I'm sadly not&nbsp;&nbsp;

00:46:46.040 --> 00:46:50.160
going to become a novelist. This was more a fun&nbsp;
thing that I did on the side and also because I&nbsp;&nbsp;

00:46:50.160 --> 00:46:58.240
thought it was worth it. But it was really, almost&nbsp;
like a hobby, I'm not a novelist. I know a lot of&nbsp;&nbsp;

00:46:58.240 --> 00:47:03.360
artists that I have a lot of respect for. I'm more&nbsp;
of a scientist. I have more of an analytical mind&nbsp;&nbsp;

00:47:03.360 --> 00:47:08.760
as opposed to a synthetic one. Part of what made&nbsp;
me write this book was that in an earlier time,&nbsp;&nbsp;

00:47:08.760 --> 00:47:12.680
I actually learned to write science fiction.&nbsp;
I went to this famous thing called the Clarion&nbsp;&nbsp;

00:47:12.680 --> 00:47:18.680
West Writers Workshop that a lot of the biggest&nbsp;
names in science fiction have come from. I know&nbsp;&nbsp;

00:47:18.680 --> 00:47:23.000
the basics of how to write fiction. But at&nbsp;
the end of the day, I'm a scientist and my&nbsp;&nbsp;

00:47:23.000 --> 00:47:27.920
main occupation in the last several years has&nbsp;
been doing research. This is what I'm spending&nbsp;&nbsp;

00:47:27.920 --> 00:47:33.160
most of my time on. And going forward, this is&nbsp;
what I'm going to spend even more of my time on.&nbsp;&nbsp;

00:47:33.160 --> 00:47:38.480
I intend to write more books; I have lots of&nbsp;
ideas for novels to write, but you’ve got to&nbsp;&nbsp;

00:47:38.480 --> 00:47:43.960
prioritize. The books that I'm going to write&nbsp;
after this are going to be nonfiction ones. The&nbsp;&nbsp;

00:47:43.960 --> 00:47:49.360
main thing that I'm working on right now, and have&nbsp;
been, is better learning algorithms, is better AI,&nbsp;&nbsp;

00:47:49.360 --> 00:47:55.880
precisely AI that doesn't hallucinate. It's AI&nbsp;
that's reliable. It's AI that generalizes further&nbsp;&nbsp;

00:47:55.880 --> 00:48:02.200
than the current AI can because maybe you're using&nbsp;
some of the things that people do. In particular,&nbsp;&nbsp;

00:48:02.200 --> 00:48:06.200
going back to my previous book, The Master&nbsp;
Algorithm, I believe that to really solve&nbsp;&nbsp;

00:48:06.200 --> 00:48:11.480
the AI problem, you really need to combine&nbsp;
the key ideas from the different paradigms,&nbsp;&nbsp;

00:48:11.480 --> 00:48:16.680
the two main ones being symbolic AI which can&nbsp;
reason and neural networks which can do other&nbsp;&nbsp;

00:48:16.680 --> 00:48:22.280
things but not reason. You really need a very&nbsp;
deep integration of them and I'm making very&nbsp;&nbsp;

00:48:22.280 --> 00:48:30.600
good progress in that so we'll see where it goes.&nbsp;
Also, a lot of this was motivated by this dream,&nbsp;&nbsp;

00:48:30.600 --> 00:48:36.040
if you will, that goes back 20 years that I&nbsp;
have of this crowdsourced AI, of this collective&nbsp;&nbsp;

00:48:36.040 --> 00:48:42.520
intelligence, of mass collaboration supported by&nbsp;
AI. In order to do that, something like GPT is&nbsp;&nbsp;

00:48:42.520 --> 00:48:49.960
not enough. We have technology to go way beyond&nbsp;
where we are today, but we do not quite have the&nbsp;&nbsp;

00:48:49.960 --> 00:48:54.600
technology to go all the way, precisely because&nbsp;
people will contradict each other and there will&nbsp;&nbsp;

00:48:54.600 --> 00:48:59.800
be a lot of crap, basically. And the AI needs to&nbsp;
know how to deal with that. Symbolic AI doesn't&nbsp;&nbsp;

00:48:59.800 --> 00:49:03.440
know how to do that, but then the neural&nbsp;
AI doesn't know how to reason with that.&nbsp;&nbsp;

00:49:03.440 --> 00:49:09.460
So we do need progress on a scientific level&nbsp;
to do this. But I think we're getting there.

00:49:09.460 --> 00:49:11.240
CRAIG:
Yeah. And don't&nbsp;&nbsp;

00:49:11.240 --> 00:49:19.120
you think that with all the hints coming out of&nbsp;
OpenAI that that's going to be their next model,&nbsp;&nbsp;

00:49:19.120 --> 00:49:24.240
at least a step toward that,&nbsp;
that reasoning neural network?

00:49:24.240 --> 00:49:26.720
PEDRO:
Great question. So I don't know what Q*&nbsp;&nbsp;

00:49:28.040 --> 00:49:33.280
or Strawberry are. But knowing a bunch of people&nbsp;
at OpenAI, including some of the people working&nbsp;&nbsp;

00:49:33.280 --> 00:49:39.160
on this, I think that very much trying to add&nbsp;
reasoning capabilities is the key direction to go,&nbsp;&nbsp;

00:49:39.160 --> 00:49:44.600
and making the AI more reliable. They understand&nbsp;
this very well and they're working on it. Now,&nbsp;&nbsp;

00:49:44.600 --> 00:49:50.440
what I see them doing, I don't think is going to&nbsp;
get them there. They have a particular type of&nbsp;&nbsp;

00:49:50.440 --> 00:49:55.600
person in the company, very much the hacker type.&nbsp;
Again, this is what I'm making fun of in 2040,&nbsp;&nbsp;

00:49:55.600 --> 00:50:01.000
is the hacker's thinking they're about to&nbsp;
solve AI by hacking. And hacking has a place,&nbsp;&nbsp;

00:50:01.000 --> 00:50:07.400
the scaling and whatnot, it's not trivial&nbsp;
but I don't think you're going to get to this&nbsp;&nbsp;

00:50:07.400 --> 00:50:12.080
kind of AI, AGI, superintelligence, or human&nbsp;
level AI, whatever you're going to call it,&nbsp;&nbsp;

00:50:12.080 --> 00:50:16.680
just by tweaking transformers. The funny&nbsp;
thing is that Sam Altman also thinks that,&nbsp;&nbsp;

00:50:16.680 --> 00:50:23.840
he just doesn't say that these days. But two&nbsp;
years ago– it's on the record of him saying [yeah,&nbsp;&nbsp;

00:50:23.840 --> 00:50:27.120
I don't think transformers are going to do&nbsp;
it.] But of course, now he needs to sell what&nbsp;&nbsp;

00:50:27.120 --> 00:50:36.480
he has. So I don't think that, for all their good&nbsp;
intentions, OpenAI is the most likely player to&nbsp;&nbsp;

00:50:36.480 --> 00:50:40.840
get to this. They have some people. DeepMind&nbsp;
has more as a research lab. I think DeepMind&nbsp;&nbsp;

00:50:40.840 --> 00:50:47.640
is vastly better than OpenAI and not even close.&nbsp;
But also, I think this is very neglected today:&nbsp;&nbsp;

00:50:47.640 --> 00:50:53.440
the great majority of AI research today still&nbsp;
happens at universities. The Googles, the Metas,&nbsp;&nbsp;

00:50:53.440 --> 00:50:58.280
and the Microsofts get all the publicity but if&nbsp;
you just look by the amount of research and in&nbsp;&nbsp;

00:50:58.280 --> 00:51:02.520
particular you look at the research that is less&nbsp;
short term, it's overwhelmingly coming from the&nbsp;&nbsp;

00:51:02.520 --> 00:51:06.520
universities. If I had to guess where this is&nbsp;
going to come from, for these various reasons,&nbsp;&nbsp;

00:51:06.520 --> 00:51:10.260
it's probably not going to be one of those&nbsp;
labs; it's going to be some grad student.

00:51:10.260 --> 00:51:10.920
CRAIG:
Yeah. Do&nbsp;&nbsp;

00:51:10.920 --> 00:51:26.560
you follow Rich Sutton's work? He's&nbsp;
involved with John Carmack, the coder&nbsp;&nbsp;

00:51:27.640 --> 00:51:34.200
from the gaming industry. He’s got a&nbsp;
startup called Keen. I'm just curious&nbsp;&nbsp;

00:51:34.200 --> 00:51:42.320
whether you followed that. It's sort of trying&nbsp;
to get to AGI through reinforcement learning.

00:51:42.320 --> 00:51:44.480
PEDRO:
So I have known Rich for&nbsp;&nbsp;

00:51:44.480 --> 00:51:50.360
a long time. He's an interesting character. He’s,&nbsp;
of course, the most important person in the world&nbsp;&nbsp;

00:51:50.360 --> 00:51:56.640
in reinforcement learning. When I was writing The&nbsp;
Master Algorithm, I actually asked a bunch of my&nbsp;&nbsp;

00:51:56.640 --> 00:51:59.920
colleagues, so do you believe in this notion of&nbsp;
a master algorithm? The master algorithm is one&nbsp;&nbsp;

00:51:59.920 --> 00:52:07.680
learning algorithm that can learn anything. And&nbsp;
there was a spectrum of opinion. People say [No,&nbsp;&nbsp;

00:52:07.680 --> 00:52:13.680
that's never going to happen], to various ideas.&nbsp;
But the two people who most strongly believed in&nbsp;&nbsp;

00:52:13.680 --> 00:52:17.080
the notion of the master algorithm&nbsp;
were Geoff Hinton and Rich Sutton.

00:52:17.080 --> 00:52:18.240
CRAIG:
Oh!

00:52:18.240 --> 00:52:20.040
PEDRO:
And this is not a coincidence.&nbsp;&nbsp;

00:52:20.040 --> 00:52:25.080
They are the leaders of their schools of thought.&nbsp;
Of course, their idea of what the master algorithm&nbsp;&nbsp;

00:52:25.080 --> 00:52:29.720
is was very different. The master algorithm for&nbsp;
the Rich is reinforcement learning. In fact,&nbsp;&nbsp;

00:52:29.720 --> 00:52:36.000
in an early discussion that I had with him years&nbsp;
ago now, I was pointing out the thing about&nbsp;&nbsp;

00:52:36.000 --> 00:52:40.320
reinforcement learning is that it's very intuitive&nbsp;
but it keeps not working. This has been the case&nbsp;&nbsp;

00:52:40.320 --> 00:52:46.040
even now. DeepMind came to prominence with deep&nbsp;
reinforcement learning. But that actually hasn't&nbsp;&nbsp;

00:52:46.040 --> 00:52:52.920
gone anywhere, unfortunately. I wish it had. So&nbsp;
kind of quizzed Rich about some of this. And he&nbsp;&nbsp;

00:52:52.920 --> 00:52:57.080
said like, [Well, reinforcement learning doesn't&nbsp;
have to be that, it can be something else…]&nbsp;&nbsp;

00:52:57.880 --> 00:53:03.300
And I'm like, sure, so what is reinforcement&nbsp;
learning? And Rich says, “It's whatever works”.

00:53:03.300 --> 00:53:04.560
(both laugh)

00:53:04.560 --> 00:53:06.040
PEDRO:
When something finally works,&nbsp;&nbsp;

00:53:06.040 --> 00:53:11.520
he'll call it reinforcement learning, that's&nbsp;
okay. But that doesn't answer the question of&nbsp;&nbsp;

00:53:11.520 --> 00:53:18.360
what to do now. To get more concretely to&nbsp;
what I think is their chances of success:&nbsp;&nbsp;

00:53:18.360 --> 00:53:23.720
So there's the sequential decision making problem.&nbsp;
That's what human intelligence solves and that's&nbsp;&nbsp;

00:53:23.720 --> 00:53:28.000
what AI has to solve, which is why I think a lot&nbsp;
of the people who do want to get to human level&nbsp;&nbsp;

00:53:28.000 --> 00:53:33.600
AI are so attracted to reinforcement learning.&nbsp;
But reinforcement learning is one approach to&nbsp;&nbsp;

00:53:33.600 --> 00:53:38.520
the problem of sequential decision making. At the&nbsp;
end of the day, I don't think– I could be wrong,&nbsp;&nbsp;

00:53:39.040 --> 00:53:44.120
I don't think that's the solution because the&nbsp;
core of reinforcement learning is this notion&nbsp;&nbsp;

00:53:44.120 --> 00:53:49.160
that you make some decisions now and then you only&nbsp;
get rewards much later. Like I make a move in the&nbsp;&nbsp;

00:53:49.160 --> 00:53:54.280
game of Go and then only 100 moves later do I win&nbsp;
or lose the game. The whole idea of reinforcement&nbsp;&nbsp;

00:53:54.280 --> 00:53:59.160
learning is to propagate that result back to the&nbsp;
present time. But what has happened over and over&nbsp;&nbsp;

00:53:59.160 --> 00:54:05.800
again since the 80s, since the field began, is&nbsp;
one of two things: either the rewards are fairly&nbsp;&nbsp;

00:54:05.800 --> 00:54:10.960
frequent and close to your actions, and in that&nbsp;
case you don't need reinforcement learning because&nbsp;&nbsp;

00:54:10.960 --> 00:54:16.000
you can just do supervised learning. Every time&nbsp;
we see a success of reinforcement learning, most&nbsp;&nbsp;

00:54:16.000 --> 00:54:21.920
recently with the things like training chatbots&nbsp;
to give answers that people like, so-called&nbsp;&nbsp;

00:54:21.920 --> 00:54:26.920
reinforcement learning from human feedback, it&nbsp;
immediately turns out that actually this was just&nbsp;&nbsp;

00:54:26.920 --> 00:54:33.080
supervised learning or you could do just as well&nbsp;
as with supervised learning. Or, if the rewards&nbsp;&nbsp;

00:54:33.080 --> 00:54:39.360
truly are delayed and sparse, it just doesn't&nbsp;
work. So I'm waiting for Rich to solve that&nbsp;&nbsp;

00:54:39.360 --> 00:54:44.440
problem. I hope that he does but so far I haven't&nbsp;
seen anything to convince me. Of course, there's&nbsp;&nbsp;

00:54:44.440 --> 00:54:50.000
also his famous paper about the bitter lesson that&nbsp;
day what really works is more data and scaling,&nbsp;&nbsp;

00:54:50.000 --> 00:54:54.920
which I think is not the complete truth but it's&nbsp;
a very interesting statement coming from him.

00:54:54.920 --> 00:54:55.440
CRAIG:
Well,&nbsp;&nbsp;

00:54:55.440 --> 00:54:58.500
it got us ChatGPT which blew everybody's mind.

00:54:58.500 --> 00:55:00.080
PEDRO:
Well, but here's the thing,&nbsp;&nbsp;

00:55:04.200 --> 00:55:09.520
this is a misunderstanding that people have&nbsp;
that is a very natural one. Scaling was one&nbsp;&nbsp;

00:55:09.520 --> 00:55:15.280
of the things that got us ChatGPT. Before OpenAI,&nbsp;
and actually Google before them and then Sundar&nbsp;&nbsp;

00:55:15.280 --> 00:55:20.360
screwed up, but that's another story, before the&nbsp;
whole run to scale up happened, which is what is&nbsp;&nbsp;

00:55:20.360 --> 00:55:26.560
happening now and I'm all for, there were a whole&nbsp;
series of things that had to happen such that&nbsp;&nbsp;

00:55:26.560 --> 00:55:32.400
then, scaling is the last element for this thing&nbsp;
to really take off. And really those things were&nbsp;&nbsp;

00:55:32.400 --> 00:55:36.280
much more important than– actually even scaling,&nbsp;
you had to have GPUs, you had to have embeddings,&nbsp;&nbsp;

00:55:36.280 --> 00:55:42.480
you had to have back propagation. The algorithm&nbsp;
that drives all of this, backpropagation,&nbsp;&nbsp;

00:55:42.480 --> 00:55:52.040
was invented in the 80s by some definition, maybe&nbsp;
even earlier, by a psychologist modeling child&nbsp;&nbsp;

00:55:52.040 --> 00:55:58.680
development and children's language learning.&nbsp;
So there's a whole series of things and it's&nbsp;&nbsp;

00:55:58.680 --> 00:56:06.240
easy to say, [Oh, the scaling did it.] No, I spent&nbsp;
the first part of my career working on scaling and&nbsp;&nbsp;

00:56:06.240 --> 00:56:11.200
then I realized mathematically that the algorithms&nbsp;
that we have, no matter how far we scale them,&nbsp;&nbsp;

00:56:11.200 --> 00:56:16.280
will still not be that smart. So you got to have&nbsp;
the people working on scaling but you also got to&nbsp;&nbsp;

00:56:16.280 --> 00:56:21.000
have the people developing the newer generation&nbsp;
of algorithms that you can then scale. A human&nbsp;&nbsp;

00:56:21.000 --> 00:56:26.600
brain is not a scaled ant brain. So I think just&nbsp;
scaling counter that story of the bitter lesson,&nbsp;&nbsp;

00:56:26.600 --> 00:56:30.960
isn't going to get us there. However, there is&nbsp;
an element of truth in it, which we have learned&nbsp;&nbsp;

00:56:30.960 --> 00:56:36.080
repeatedly in AI: that what tends to work well,&nbsp;
and this is really the notion behind the master&nbsp;&nbsp;

00:56:36.080 --> 00:56:42.640
algorithm, is actually, simple algorithms coupled&nbsp;
with a lot of data and a lot of compute. It's&nbsp;&nbsp;

00:56:42.640 --> 00:56:48.400
amazing how powerful that is, as opposed to very&nbsp;
clever solutions designed by very clever people.

