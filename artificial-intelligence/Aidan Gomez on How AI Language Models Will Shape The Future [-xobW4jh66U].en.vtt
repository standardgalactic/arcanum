WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.220
AIDAN 00:00
It was quite extraordinary,&nbsp;&nbsp;

00:00:02.220 --> 00:00:07.920
quite extraordinarily convenient. That&nbsp;
simply by scraping more data off the web,&nbsp;&nbsp;

00:00:07.920 --> 00:00:11.280
not necessarily clean data, like messy data&nbsp;
it’s just web data, you're just taking in&nbsp;&nbsp;

00:00:11.280 --> 00:00:17.220
everything and there's tons of junk out there. But&nbsp;
taking in a very noisy, messy, massive data set,&nbsp;&nbsp;

00:00:17.220 --> 00:00:23.820
and just making the model bigger, throwing some&nbsp;
more chips at it. And what came out the other side&nbsp;&nbsp;

00:00:23.820 --> 00:00:29.880
was something that understood language, in a way&nbsp;
I personally thought we were we were decades from.

00:00:29.880 --> 00:00:33.600
CRAIG 00:55
We're talking this week to Aidan Gomez,&nbsp;&nbsp;

00:00:33.600 --> 00:00:40.680
who helped develop the transformer algorithm,&nbsp;
which lies at the heart of generative AI and&nbsp;&nbsp;

00:00:40.680 --> 00:00:49.320
powers large language models, such as GPT-4.&nbsp;
Aidan now leads a startup Cohere, a platform&nbsp;&nbsp;

00:00:49.320 --> 00:00:58.380
that offers users access to pre-built LLMs, as&nbsp;
well as allowing users to create their own LLM.

00:00:58.380 --> 00:01:04.740
But first, I want to give a shout out to our&nbsp;
sponsor, and encourage anyone with a business&nbsp;&nbsp;

00:01:04.740 --> 00:01:13.920
to take advantage of a deal from Oracle, which&nbsp;
is offering a full NetSuite implementation with&nbsp;&nbsp;

00:01:13.920 --> 00:01:21.840
no down payment and no interest for six months.&nbsp;
NetSuite is a cloud-based business management&nbsp;&nbsp;

00:01:21.840 --> 00:01:28.560
software for enterprise resource planning,&nbsp;
financial management, customer relationship&nbsp;&nbsp;

00:01:28.560 --> 00:01:36.120
management, and E commerce. To take advantage&nbsp;
of the offer, go to netsuite.com/eyeonai.

00:01:52.560 --> 00:01:55.380
Now let's get back to Aidan,

00:01:55.380 --> 00:01:59.460
AIDAN 02:20
I’m Aidan. I am the CEO and co-founder of Cohere.&nbsp;&nbsp;

00:01:59.460 --> 00:02:04.260
I started the company with Nick [Frosst] and Ivan&nbsp;
[Zhang] about three and a half, four years ago.&nbsp;&nbsp;

00:02:06.240 --> 00:02:12.480
Before that, I was kind of the perpetual intern at&nbsp;
Google Brain during my undergrad and then later,&nbsp;&nbsp;

00:02:12.480 --> 00:02:16.320
my PhD. I started down in the&nbsp;
Bay Area and Mountain View.

00:02:18.240 --> 00:02:21.000
AIDAN
I was part of the team that created&nbsp;&nbsp;

00:02:21.000 --> 00:02:29.940
the transformer. And it was incredibly exciting.&nbsp;
You know, it took the world by storm, I think,&nbsp;&nbsp;

00:02:29.940 --> 00:02:37.080
certainly, to my surprise, and I think everyone on&nbsp;
the team was quite taken aback by its popularity.&nbsp;&nbsp;

00:02:39.000 --> 00:02:44.340
But before Google, I was an undergrad or also&nbsp;
during Google, I was an undergrad at U of T&nbsp;&nbsp;

00:02:45.180 --> 00:02:48.720
[University of Toronto]. I&nbsp;
grew up in rural Ontario,&nbsp;&nbsp;

00:02:48.720 --> 00:03:00.240
Canada in a maple forest. And so I'm the&nbsp;
world's most Canadian man. Yeah, that's me.

00:03:01.140 --> 00:03:03.060
CRAIG 03:25
And so you were at U&nbsp;&nbsp;

00:03:03.060 --> 00:03:12.540
of T studying with Geoff Hinton, I guess, he was&nbsp;
probably kind of retired from teaching by then.

00:03:12.540 --> 00:03:15.480
AIDAN 03:38
He was definitely not teaching,&nbsp;&nbsp;

00:03:15.480 --> 00:03:20.880
but he was still at the university. This&nbsp;
is before the Vector Institute was created.&nbsp;&nbsp;

00:03:22.920 --> 00:03:28.920
And so yeah, he was, you know, like I,&nbsp;
I didn't really get into deep learning&nbsp;&nbsp;

00:03:28.920 --> 00:03:36.540
until after second year. And then when I started&nbsp;
looking into it, I became obsessed, and I was just&nbsp;&nbsp;

00:03:37.800 --> 00:03:44.220
reading papers, night and day, I would fall asleep&nbsp;
with a research paper sitting on my bedside,&nbsp;&nbsp;

00:03:44.220 --> 00:03:49.140
I would, in between sets at the gym, you know,&nbsp;
have a stack of papers that I was reading through.&nbsp;&nbsp;

00:03:51.000 --> 00:03:54.900
And I kept seeing this name.&nbsp;
And his affiliation was U of T,&nbsp;&nbsp;

00:03:54.900 --> 00:04:00.000
which was where I was. And so I reached&nbsp;
out to Geoff, this is before Google.

00:04:02.400 --> 00:04:09.060
And I just, you know, I'd been reading his papers&nbsp;
at that point when I was studying, you know,&nbsp;&nbsp;

00:04:09.060 --> 00:04:16.140
reused and MLPs and just the most simple piece&nbsp;
of the AI deep learning stack. And I was like,&nbsp;&nbsp;

00:04:16.140 --> 00:04:20.520
you know, why do you have these functions that are&nbsp;
just flat and then up? I think that they should&nbsp;&nbsp;

00:04:20.520 --> 00:04:26.520
be periodic. And so I emailed him with an idea&nbsp;
being like, Hey, why did you make this decision?&nbsp;&nbsp;

00:04:28.200 --> 00:04:32.880
I think they should be periodic. There&nbsp;
should be some regularity and it should&nbsp;&nbsp;

00:04:32.880 --> 00:04:36.900
be bounded so that you know, it doesn't&nbsp;
go to infinity if we get a large input.&nbsp;&nbsp;

00:04:39.180 --> 00:04:44.040
And to my surprise, he responded, and&nbsp;
he actually explained the decision.&nbsp;&nbsp;

00:04:46.440 --> 00:04:52.800
And so that was pretty amazing. That was&nbsp;
my first interaction with Geoff. And then&nbsp;&nbsp;

00:04:53.640 --> 00:04:59.460
when I came back from Google, in Mountain View to&nbsp;
Toronto, Geoff said, Hey, come work with me in,&nbsp;&nbsp;

00:04:59.460 --> 00:05:03.420
in the Toronto Brain office. And that&nbsp;
was where I met my co founder, Nick.

00:05:04.560 --> 00:05:05.100
CRAIG 05:31&nbsp;

00:05:07.260 --> 00:05:12.780
just on, on so so you, you worked on&nbsp;
the transformer algorithm with a team&nbsp;&nbsp;

00:05:12.780 --> 00:05:19.980
in Mountain View at Google. Google Brain?&nbsp;
Was it? Yeah, Google Brain. Yeah. So can&nbsp;&nbsp;

00:05:19.980 --> 00:05:28.920
you explain that periodic versus stable, or&nbsp;
what? Which algorithm were you talking about?

00:05:28.920 --> 00:05:31.380
AIDAN 05:54
Yeah, I mean,&nbsp;&nbsp;

00:05:33.180 --> 00:05:39.360
it's not very important, because I was&nbsp;
wrong. It doesn't, it doesn't really&nbsp;&nbsp;

00:05:40.560 --> 00:05:43.260
matter. And I think it's&nbsp;
more just to Geoff's credit,&nbsp;&nbsp;

00:05:43.260 --> 00:05:49.200
the fact that he responded to a second year&nbsp;
undergrad with, you know, a wacky idea,&nbsp;&nbsp;

00:05:50.220 --> 00:05:58.560
earnestly. And this guy was literally the&nbsp;
top of his field yet toook time for me.&nbsp;&nbsp;

00:06:00.420 --> 00:06:05.700
And so I think that that particular piece that,&nbsp;
I mean, he made me - it's interesting. So for&nbsp;&nbsp;

00:06:05.700 --> 00:06:10.920
instance, in deep learning and neural networks,&nbsp;
we have these neurons, these neurons fire,&nbsp;&nbsp;

00:06:10.920 --> 00:06:17.160
there's some function that determines their&nbsp;
firing, there's some generally some threshold at&nbsp;&nbsp;

00:06:17.160 --> 00:06:22.620
which they don't fire, they stay dormant. And then&nbsp;
above that, they fire. And so when they're firing,&nbsp;&nbsp;

00:06:23.520 --> 00:06:34.680
they basically, they fire linearly proportional&nbsp;
to the input intensity that they're getting. So if&nbsp;&nbsp;

00:06:34.680 --> 00:06:40.920
the input intensity is high, the output intensity&nbsp;
is high when they're firing, but that leads to&nbsp;&nbsp;

00:06:40.920 --> 00:06:48.720
potentially unstable behaviors, if you have, for&nbsp;
whatever reason, some sort of blow up or some sort&nbsp;&nbsp;

00:06:48.720 --> 00:06:54.840
of like burst of signal coming in, then you'll&nbsp;
get a huge burst out. And it'll that'll propagate&nbsp;&nbsp;

00:06:54.840 --> 00:07:02.100
and make things more and more noisy. And that&nbsp;
leads to instability. It makes things complicated&nbsp;&nbsp;

00:07:02.100 --> 00:07:08.580
in training. And so my proposal was, instead of&nbsp;
just firing linearly proportional to your inputs,&nbsp;&nbsp;

00:07:08.580 --> 00:07:15.480
instead have some sort of predictable, regular&nbsp;
periodic pattern, like a sine wave or something.&nbsp;&nbsp;

00:07:15.480 --> 00:07:25.440
So that you always know your output is bounded&nbsp;
between some values. But that has not taken off.&nbsp;&nbsp;

00:07:25.440 --> 00:07:33.180
And we've since solved the training, instability&nbsp;
in the blow ups and that type of thing. So it&nbsp;&nbsp;

00:07:33.180 --> 00:07:38.089
was just my first email to Geoff, I think&nbsp;
six months into my study of deep learning.

00:07:38.089 --> 00:07:43.200
CRAIG 08:02
Wow. That's impressive. And from a Maple forest.

00:07:43.200 --> 00:07:49.320
AIDAN 08:11
Yeah, I love that. But I go back,

00:07:49.320 --> 00:07:55.080
CRAIG 08:13
Then at Google Brain, what&nbsp;&nbsp;

00:07:55.080 --> 00:08:04.680
was the project that you were working on? What&nbsp;
was the initial idea that led the transformers?

00:08:04.680 --> 00:08:08.520
AIDAN 08:30
So I was on the infrastructure side, like the&nbsp;&nbsp;

00:08:08.520 --> 00:08:15.420
original idea, I joined Google for I was working&nbsp;
with Lukasz Kaiser, and what we wanted to do,&nbsp;&nbsp;

00:08:16.440 --> 00:08:24.720
I think, Lukasz operates half a decade to&nbsp;
a decade ahead of his time, constantly. And&nbsp;&nbsp;

00:08:24.720 --> 00:08:29.820
so the project that I joined for was actually&nbsp;
this paper called one model to learn them all.&nbsp;&nbsp;

00:08:29.820 --> 00:08:37.140
And the idea was, we're going to take every&nbsp;
single dataset that machine learning researchers&nbsp;&nbsp;

00:08:37.140 --> 00:08:42.720
have compiled, and we're going to put it into one&nbsp;
model. And that means it needs to be multimodal,&nbsp;&nbsp;

00:08:42.720 --> 00:08:45.960
because we have datasets for images, we have&nbsp;
datasets for audio, video, you know, text,&nbsp;&nbsp;

00:08:45.960 --> 00:08:53.820
everything. And so what we wanted to do was throw&nbsp;
all the modalities in, as well as out. So you can&nbsp;&nbsp;

00:08:53.820 --> 00:09:02.280
consume video and let's say, describe the video&nbsp;
or you can consume audio and transcribe it. But&nbsp;&nbsp;

00:09:02.280 --> 00:09:08.220
you can also take in some text and then produce&nbsp;
audio, you can also just describe the video that&nbsp;&nbsp;

00:09:08.220 --> 00:09:12.780
you want and video comes out the other side. So&nbsp;
it's just like fully multimodal on both input and&nbsp;&nbsp;

00:09:12.780 --> 00:09:18.540
output side and we just train on everything&nbsp;
like truly everything we've come across.

00:09:18.540 --> 00:09:21.960
AIDAN
This now sounds kind of familiar, right? Because&nbsp;&nbsp;

00:09:21.960 --> 00:09:27.660
this is sort of the project roadmap that we're&nbsp;
on right now with these large language models&nbsp;&nbsp;

00:09:27.660 --> 00:09:32.460
that we're throwing everything we have, the entire&nbsp;
internet, and now we're starting to add in every&nbsp;&nbsp;

00:09:32.460 --> 00:09:39.600
modality that we can. So that was what I joined&nbsp;
for that was a different project altogether. To&nbsp;&nbsp;

00:09:39.600 --> 00:09:44.040
support that project, we built this, we built this&nbsp;
piece of software, this piece of infrastructure.&nbsp;&nbsp;

00:09:45.240 --> 00:09:49.800
Because that model was going to be huge.&nbsp;
And the data pipelines were going to have&nbsp;&nbsp;

00:09:49.800 --> 00:09:56.820
to be extraordinarily complex. And so we&nbsp;
needed something to suit that. And so,&nbsp;&nbsp;

00:09:56.820 --> 00:10:03.360
what we did was created this program&nbsp;
called tensor tensor. It could distribute&nbsp;&nbsp;

00:10:04.200 --> 00:10:10.140
across arbitrary numbers of GPUs like 1000s&nbsp;
and 1000s and 1000s. And it was very focused&nbsp;&nbsp;

00:10:10.140 --> 00:10:14.640
on auto regressive modeling, which is the&nbsp;
type of modeling that the transformer is.

00:10:14.640 --> 00:10:17.700
AIDAN
And so at that time,&nbsp;&nbsp;

00:10:17.700 --> 00:10:26.220
I was sitting next to Noam [Shazeer], who&nbsp;
was fiddling with autoregressive models and&nbsp;&nbsp;

00:10:26.220 --> 00:10:30.060
in particular, attention based models.&nbsp;
He was really interested in attention.&nbsp;&nbsp;

00:10:30.900 --> 00:10:36.120
And then we heard about a team over in&nbsp;
translate which was being led by Jakob&nbsp;&nbsp;

00:10:37.260 --> 00:10:42.900
which was also interested in attention based&nbsp;
autoregressive models. And so Lukasz convinced&nbsp;&nbsp;

00:10:42.900 --> 00:10:50.880
Noam and and Jakob to come over build it on our&nbsp;
stack, build it on tensor tensor. And they did.&nbsp;&nbsp;

00:10:50.880 --> 00:10:58.740
And so over the next I think, 10 weeks, it&nbsp;
was just a sprint to build this model. And&nbsp;&nbsp;

00:10:58.740 --> 00:11:04.920
the intensity just ramped up and ramped up because&nbsp;
the results we were getting were extraordinary.

00:11:05.700 --> 00:11:08.220
AIDAN
So I think this was like,&nbsp;&nbsp;

00:11:09.120 --> 00:11:16.500
it wasn't the first, but it was one of the very&nbsp;
early, extremely successful scaling projects,&nbsp;&nbsp;

00:11:16.500 --> 00:11:22.620
like hyper scalable architectures,&nbsp;
massive data, massive model sizes&nbsp;&nbsp;

00:11:24.660 --> 00:11:28.560
and massive GPU clusters just lead&nbsp;
to extremely high performance.

00:11:28.560 --> 00:11:30.480
CRAIG 11:55&nbsp;

00:11:30.480 --> 00:11:39.060
And, and the first of all the tensor tensor.&nbsp;
That's a framework or an orchestration layer.

00:11:39.060 --> 00:11:42.360
AIDAN 12:04
Yeah, yeah. So it,&nbsp;&nbsp;

00:11:42.360 --> 00:11:49.560
it's, it was built on top of TensorFlow at&nbsp;
the time. But it was basically just a library&nbsp;&nbsp;

00:11:51.660 --> 00:12:03.540
to support large distributed model training. And&nbsp;
it had all the latest kind of tricks and hacks&nbsp;&nbsp;

00:12:03.540 --> 00:12:09.180
with learning rate schedules and initialization&nbsp;
techniques, and it had all this stuff built in.&nbsp;&nbsp;

00:12:10.440 --> 00:12:19.260
And so it let us experiment really&nbsp;
rapidly. I think, if I'm being honest,&nbsp;&nbsp;

00:12:20.700 --> 00:12:26.280
tensor tensor was a mess. It was crazy. It was&nbsp;
just like all over the place that supported&nbsp;&nbsp;

00:12:26.280 --> 00:12:32.100
everything we were just throwing, every new paper&nbsp;
that was coming out into it. It’s a little bit&nbsp;&nbsp;

00:12:33.000 --> 00:12:41.580
chaotic. And there exists far, far better&nbsp;
systems nowadays. But back then it did the&nbsp;&nbsp;

00:12:41.580 --> 00:12:47.580
job. It did the job, we were able to move&nbsp;
insanely fast. And so I'm quite proud of it.

00:12:47.580 --> 00:12:51.900
CRAIG 13:13
And you were - attention was already&nbsp;&nbsp;

00:12:51.900 --> 00:13:00.960
something that was being talked about. A couple&nbsp;
of questions in that process. What was your role?&nbsp;&nbsp;

00:13:03.240 --> 00:13:12.840
I mean, I'm a journalist. I imagine you guys&nbsp;
sitting next to each other furiously coding,&nbsp;&nbsp;

00:13:12.840 --> 00:13:19.980
I mean, were you coding? Or is it more that you're&nbsp;
in a room with a whiteboard trying to figure out&nbsp;&nbsp;

00:13:22.680 --> 00:13:25.789
the architecture or is it something else?

00:13:25.789 --> 00:13:27.540
AIDAN 13:51
There's a lot of like,&nbsp;&nbsp;

00:13:27.540 --> 00:13:31.860
whiteboarding and diagrams and just conceptual&nbsp;&nbsp;

00:13:34.320 --> 00:13:41.940
structuring these building blocks and putting them&nbsp;
together and the thinking about the architecture&nbsp;&nbsp;

00:13:41.940 --> 00:13:49.620
itself, there was a lot of that. And that was&nbsp;
mainly done by Noam, Ashish, Nicky and Jakob.&nbsp;&nbsp;

00:13:50.760 --> 00:13:59.760
I think, for me, like I wasn't sleeping. I was&nbsp;
working, like 14-hour days coding, building up the&nbsp;&nbsp;

00:13:59.760 --> 00:14:07.140
infrastructure, making it more robust, running&nbsp;
experiments. And so it was very much hands on&nbsp;&nbsp;

00:14:08.100 --> 00:14:14.040
coding and no one was sleeping. Everyone was just&nbsp;
hacking, experimenting, running little tweaks,&nbsp;&nbsp;

00:14:14.040 --> 00:14:18.780
little ablations to see if I add this in what&nbsp;
changes if I if I remove it, if I tweak it?&nbsp;&nbsp;

00:14:20.760 --> 00:14:25.620
Every single one of us was just messing&nbsp;
with everything and trying to figure out&nbsp;&nbsp;

00:14:25.620 --> 00:14:32.689
what was the optimal configuration. And so&nbsp;
that's how we got to that finished product.

00:14:32.689 --> 00:14:38.400
CRAIG 14:57
Yeah, and and certainly the result now is&nbsp;&nbsp;

00:14:38.400 --> 00:14:48.300
leading to auto code generation. Were you using&nbsp;
any tools to speed up the writing of the code?

00:14:48.300 --> 00:14:51.780
AIDAN 15:14
At that time? Nothing existed.&nbsp;&nbsp;

00:14:54.240 --> 00:15:00.840
Truly nothing, nothing existed. It&nbsp;
was all. You wrote it yourself. Yeah.&nbsp;&nbsp;

00:15:01.740 --> 00:15:09.720
Yeah, that came that came later. And that was&nbsp;
powered by transformers. Yeah, they kind of.

00:15:09.720 --> 00:15:17.640
CRAIG 15:35
I've read the paper and, and certainly talked&nbsp;&nbsp;

00:15:17.640 --> 00:15:28.860
to a lot of people about transformers and, and&nbsp;
their progeny. But can you explain as in as simple&nbsp;&nbsp;

00:15:28.860 --> 00:15:38.280
terms as as you can muster what the transformer&nbsp;
algorithm is and what it does? And I'm just&nbsp;&nbsp;

00:15:38.280 --> 00:15:47.520
curious, too, if if, if you were to send me the&nbsp;
transformer algorithm, sort of the basic algorithm&nbsp;&nbsp;

00:15:47.520 --> 00:15:55.860
is it a million lines of code? Is it 20 lines&nbsp;
of code? I'm just curious what it looks like.

00:15:55.860 --> 00:15:57.720
AIDAN 16:21
Yeah, nowadays,&nbsp;&nbsp;

00:15:57.720 --> 00:16:06.360
it's probably closer to 20 lines of&nbsp;
code. Extremely, extremely simple.&nbsp;&nbsp;

00:16:07.980 --> 00:16:14.160
I think a big part of the beauty of the model,&nbsp;
the architecture was the fact that it was just so&nbsp;&nbsp;

00:16:15.000 --> 00:16:24.120
simple. Like it, it is among the simplest&nbsp;
architectures that were going around at the&nbsp;&nbsp;

00:16:24.120 --> 00:16:29.580
time, it was just some, like the most basic&nbsp;
layer, the layer that has existed for like,&nbsp;&nbsp;

00:16:31.200 --> 00:16:40.980
I don't know how many years now, maybe over half&nbsp;
a century. Like the the basic layer is called,&nbsp;&nbsp;

00:16:40.980 --> 00:16:49.800
like an MLP. That's just what it's called&nbsp;
MLP. And really, the transformer is like,&nbsp;&nbsp;

00:16:49.800 --> 00:16:54.780
it's a simplification, but it's just some MLPs&nbsp;
stacked on top of each other, plus an attention

00:16:54.780 --> 00:16:56.040
CRAIG 17:20&nbsp;

00:16:56.040 --> 00:17:00.409
NLP? You're saying like natural&nbsp;
language process? M No. Okay.

00:17:00.409 --> 00:17:01.740
AIDAN 17:25
Yeah, yeah.&nbsp;&nbsp;

00:17:02.700 --> 00:17:06.600
So this is just the name doesn't&nbsp;
matter. multi-layer perceptron, okay.

00:17:06.600 --> 00:17:10.860
CRAIG 17:33
Multi- layer perceptron sounds like&nbsp;&nbsp;

00:17:11.640 --> 00:17:13.860
a neural deep net. But

00:17:13.860 --> 00:17:16.380
AIDAN 17:38
totally, yeah, that's the fundamental&nbsp;&nbsp;

00:17:16.980 --> 00:17:24.540
unit. And before, before, transformers, there&nbsp;
were these very complicated LSTM architectures&nbsp;&nbsp;

00:17:24.540 --> 00:17:32.580
with gates and all of these like confusing&nbsp;
bits and bobs that just made it made it work.&nbsp;&nbsp;

00:17:33.600 --> 00:17:41.220
With the transformer, all of that was torn away,&nbsp;
and the layer became MLPs plus one attention.&nbsp;&nbsp;

00:17:42.660 --> 00:17:45.720
That was it. And so that was that was super.&nbsp;&nbsp;

00:17:48.540 --> 00:17:52.440
I don't know that there was a very, it was&nbsp;
beautiful, that you could just carve away so&nbsp;&nbsp;

00:17:52.440 --> 00:17:57.120
much stuff and just leave something so simple&nbsp;
that performed so well, that was so scalable.&nbsp;&nbsp;

00:17:59.220 --> 00:18:05.160
So the architecture is not this hyper&nbsp;
complex beast. It's actually just a very&nbsp;&nbsp;

00:18:05.820 --> 00:18:12.660
simple scalable compute saturating, you know,

00:18:12.660 --> 00:18:16.320
CRAIG 18:38
well explain what it&nbsp;&nbsp;

00:18:16.320 --> 00:18:25.260
does. So you have the multi-layer perceptron&nbsp;
as as the base How do you create attention?

00:18:25.260 --> 00:18:28.440
AIDAN 18:53&nbsp;

00:18:28.440 --> 00:18:36.060
How do you create attention? Yeah. So attention is&nbsp;
like this idea that you want to relate parts of a&nbsp;&nbsp;

00:18:36.060 --> 00:18:41.520
sequence, to other parts, fundamental property,&nbsp;
that there are relations, if you have a sequence&nbsp;&nbsp;

00:18:41.520 --> 00:18:47.160
of things a thing in a list in an order, there are&nbsp;
going to be relationships between those things.&nbsp;&nbsp;

00:18:48.540 --> 00:18:54.120
Obviously, that appears on language,&nbsp;
very, very strongly, you have adjectives,&nbsp;&nbsp;

00:18:54.120 --> 00:18:58.920
which are tied to nouns, and, you know,&nbsp;
tons and tons of structures like this.&nbsp;&nbsp;

00:19:00.120 --> 00:19:05.760
And so since we were developing this explicitly&nbsp;
for language, we wanted the model to be able to&nbsp;&nbsp;

00:19:06.420 --> 00:19:11.400
represent those relationships quite easily.&nbsp;
That's what attention does. Attention says,&nbsp;&nbsp;

00:19:11.400 --> 00:19:18.660
For this word, in this sentence, I'm going&nbsp;
to learn which other words or which other&nbsp;&nbsp;

00:19:18.660 --> 00:19:24.540
word in the sequence it's related to.&nbsp;
And so for the sentence, the brown dog,&nbsp;&nbsp;

00:19:26.580 --> 00:19:37.500
you're going to want to learn that brown&nbsp;
refers to dog and maybe The refers to dog.&nbsp;&nbsp;

00:19:39.660 --> 00:19:44.760
So you're gonna want to model those relationships&nbsp;
and attention enables you to do just that. And&nbsp;&nbsp;

00:19:44.760 --> 00:19:49.260
it's not that simple. It's not just like the&nbsp;
model is learning adjective noun relationships,&nbsp;&nbsp;

00:19:49.260 --> 00:19:55.560
it's learning far more complex stuff that we&nbsp;
probably don't even have a language to describe.&nbsp;&nbsp;

00:19:56.280 --> 00:20:04.500
But we just do it intuitively in our heads. So&nbsp;
that like that attention layer is the fundamental&nbsp;&nbsp;

00:20:04.500 --> 00:20:11.400
unit of learning relationships in sequences.&nbsp;
And it turns out to be extraordinarily powerful.

00:20:11.400 --> 00:20:15.060
CRAIG 20:37
And how then&nbsp;&nbsp;

00:20:15.060 --> 00:20:21.300
does that scale because I've spoken to Ilya&nbsp;
[Sutskever] on the podcast, and he talks about&nbsp;&nbsp;

00:20:22.080 --> 00:20:33.960
seeing the paper, like the next day implementing&nbsp;
it in, in what they were doing that that led to&nbsp;&nbsp;

00:20:33.960 --> 00:20:45.420
the GPT models. How does that scale them into&nbsp;
the large language models that we see today.

00:20:45.420 --> 00:20:47.820
AIDAN 21:12&nbsp;

00:20:47.820 --> 00:20:55.200
In their earliest form, it was like a very naive&nbsp;
scaling, it was just take it, take the model,&nbsp;&nbsp;

00:20:55.200 --> 00:21:00.180
and make it bigger. And the way that you do&nbsp;
that, as you add more neurons to the network,&nbsp;&nbsp;

00:21:00.180 --> 00:21:05.940
you add more layers. So it becomes, you know,&nbsp;
a much taller model much more deeply stacked.&nbsp;&nbsp;

00:21:08.100 --> 00:21:14.280
And you just take a much larger dataset than the&nbsp;
one that we were considering and a much, much&nbsp;&nbsp;

00:21:14.280 --> 00:21:21.900
larger model than the one we were considering.&nbsp;
And a much larger pool of compute. You plug those&nbsp;&nbsp;

00:21:21.900 --> 00:21:26.820
all together. And what comes out the other&nbsp;
side, I think it shocked virtually everyone.&nbsp;&nbsp;

00:21:29.280 --> 00:21:35.100
It was quite extraordinary,&nbsp;
quite extraordinarily convenient.&nbsp;&nbsp;

00:21:36.600 --> 00:21:42.120
That simply by scraping more data off the web,&nbsp;
not necessarily clean data, or like messy data is&nbsp;&nbsp;

00:21:42.120 --> 00:21:46.260
just web data, you're just taking in everything.&nbsp;
And there's tons of junk out there. But taking in&nbsp;&nbsp;

00:21:47.580 --> 00:21:54.720
a very noisy, messy, massive data set, and just&nbsp;
making the model bigger, throwing some more chips&nbsp;&nbsp;

00:21:54.720 --> 00:21:59.820
at it. And what came out the other side was&nbsp;
something that understood language, in a way&nbsp;&nbsp;

00:22:01.380 --> 00:22:04.260
I personally thought we were we were decades from.&nbsp;&nbsp;

00:22:08.040 --> 00:22:16.620
Yeah, it was it was quite a extraordinarily&nbsp;
convenient and exciting reality.

00:22:16.620 --> 00:22:22.740
CRAIG 22:42
So in that led to Bert, is that right?

00:22:24.300 --> 00:22:27.540
AIDAN 22:49
That that in particular, like Bert predated,&nbsp;&nbsp;

00:22:27.540 --> 00:22:33.960
or maybe I have them in the wrong order. There's&nbsp;
some order there's, there's GPT one, which was&nbsp;&nbsp;

00:22:33.960 --> 00:22:44.340
the first of these scale up large language model&nbsp;
papers. I think Bert predated GPT one, I think.&nbsp;&nbsp;

00:22:45.960 --> 00:22:49.260
But Bert is a different thing. Bert&nbsp;
is kind of like a different beast.&nbsp;&nbsp;

00:22:50.940 --> 00:23:01.080
Instead of learning to generate language it learns&nbsp;
to represent and that's a subtle distinction. Now,&nbsp;&nbsp;

00:23:01.080 --> 00:23:04.440
like, we're all paying attention to&nbsp;
the Generate side, because it's so&nbsp;&nbsp;

00:23:05.100 --> 00:23:08.880
it's visceral, right? It's like, you can talk&nbsp;
to these things they can write back to you.&nbsp;&nbsp;

00:23:09.480 --> 00:23:15.600
It feels there's a very visceral human&nbsp;
reaction to something that can speak to you.

00:23:16.560 --> 00:23:18.000
AIDAN
But there's another side to this&nbsp;&nbsp;

00:23:18.000 --> 00:23:26.220
whole thing, which is representing language in a&nbsp;
numerical form. And that's extremely important.&nbsp;&nbsp;

00:23:26.760 --> 00:23:35.220
It's hard to overstate how significant that is.&nbsp;
And that was like the first killer application of&nbsp;&nbsp;

00:23:35.220 --> 00:23:41.460
transformers. It was integrated into Google&nbsp;
Search and Google themselves describe it as&nbsp;&nbsp;

00:23:43.980 --> 00:23:48.600
the most significant advance in&nbsp;
search quality in I think it was&nbsp;&nbsp;

00:23:48.600 --> 00:23:56.640
two decades 20 years like basically&nbsp;
Google's entire lifespan. So that was,&nbsp;&nbsp;

00:23:57.720 --> 00:24:02.460
that was amazing. We got we got something we&nbsp;
got a model, we got a program that was capable&nbsp;&nbsp;

00:24:02.460 --> 00:24:10.080
of representing language to be used downstream for&nbsp;
applications like search and classification, etc.&nbsp;&nbsp;

00:24:13.080 --> 00:24:19.020
Extremely, extremely faithfully, like in a&nbsp;
very, very high utility way, in a way that&nbsp;&nbsp;

00:24:21.420 --> 00:24:28.020
just boosted performance in a way we really&nbsp;
didn't expect across pretty much any tasks&nbsp;&nbsp;

00:24:28.020 --> 00:24:31.980
you throw at it. And anytime you want it&nbsp;
to use language for some downstream thing,&nbsp;&nbsp;

00:24:33.060 --> 00:24:38.040
putting a Bert model there and taking the&nbsp;
representations from that and running with&nbsp;&nbsp;

00:24:38.040 --> 00:24:42.480
those representations, you beat state of&nbsp;
the art, you outperformed everyone else.&nbsp;&nbsp;

00:24:45.000 --> 00:24:48.600
So maybe, maybe Bert was like&nbsp;
the first seed of this idea.&nbsp;&nbsp;

00:24:49.680 --> 00:24:56.460
We can take a transformer, we can set it against&nbsp;
a very simple task on a very diverse set of data.&nbsp;&nbsp;

00:24:57.000 --> 00:25:02.280
And what comes out is something that seems&nbsp;
to get language, but it seems to just get it.&nbsp;&nbsp;

00:25:04.080 --> 00:25:09.180
If I'm right, that predated&nbsp;
GPT-1, I'm not sure that's true.

00:25:09.180 --> 00:25:14.580
CRAIG 25:36
You'll forgive me, I want to get to Cohere. But I,&nbsp;&nbsp;

00:25:14.580 --> 00:25:23.880
I'm a layman. My audience is somewhere in between&nbsp;
me and you. I mean, they're, they're fairly&nbsp;&nbsp;

00:25:23.880 --> 00:25:34.620
sophisticated. But so you've got 20 lines of&nbsp;
code. You feed it some data, let's say a sentence.&nbsp;&nbsp;

00:25:36.300 --> 00:25:45.750
How is it and it's it's relating within the&nbsp;
neurons of the or the perceptrons of the multi&nbsp;&nbsp;

00:25:45.750 --> 00:25:58.920
layer perceptron? It's relating one piece of data,&nbsp;
one word, to another word, how is it doing that?&nbsp;&nbsp;

00:25:58.920 --> 00:26:12.360
Does it is it? Is it by feeding huge volumes of&nbsp;
data that it begins to see patterns? Or within&nbsp;&nbsp;

00:26:12.360 --> 00:26:20.520
that 20 lines of code, Something incredible&nbsp;
is happening? Is it possible to explain that?

00:26:22.320 --> 00:26:23.700
AIDAN 26:46
I think it's not,&nbsp;&nbsp;

00:26:23.700 --> 00:26:32.100
it's maybe one line of code that leads&nbsp;
to that behavior. The other 19 are&nbsp;&nbsp;

00:26:33.480 --> 00:26:40.140
support. I would say the one line is is the&nbsp;
objective. It's like what you're asking the&nbsp;&nbsp;

00:26:40.140 --> 00:26:45.840
model to do with the data. You're feeding&nbsp;
through this, like hypercomplex pool of data.&nbsp;&nbsp;

00:26:46.620 --> 00:26:52.200
And what does it mean to feed it through? Well,&nbsp;
what you're actually doing is you're saying,&nbsp;&nbsp;

00:26:52.740 --> 00:26:58.740
in the generative case, this is like&nbsp;
the GPT style case, you're asking it to&nbsp;&nbsp;

00:26:59.340 --> 00:27:06.360
given all the words up to a point in a sentence,&nbsp;
predict the next one. And that sounds simple.&nbsp;&nbsp;

00:27:06.360 --> 00:27:10.800
It sounds like stuff we've had for a while,&nbsp;
which is like autocomplete tab autocomplete or&nbsp;&nbsp;

00:27:12.540 --> 00:27:19.560
no, it's like that that objective is horrendously&nbsp;
complex. Because if I give you on the internet,&nbsp;&nbsp;

00:27:19.560 --> 00:27:24.600
there's examples of translation, right? Like these&nbsp;
forums online where people teach each other how to&nbsp;&nbsp;

00:27:24.600 --> 00:27:29.520
speak different languages, and someone asks,&nbsp;
Hey, how do I say, the brown dog in Spanish?&nbsp;&nbsp;

00:27:31.440 --> 00:27:36.960
And then stop, and then the person responds, oh,&nbsp;
you say it by? I don't know how to speak Spanish,&nbsp;&nbsp;

00:27:36.960 --> 00:27:40.560
but whatever it is, right? And so&nbsp;
if you ask your model to model this,&nbsp;&nbsp;

00:27:41.580 --> 00:27:46.500
the only way for it to accurately model&nbsp;
this, it has to know how to speak Spanish,&nbsp;&nbsp;

00:27:46.500 --> 00:27:51.000
because it's seeing the English part saying hey,&nbsp;
how do I translate the brown dog into Spanish&nbsp;&nbsp;

00:27:51.780 --> 00:27:55.200
stop. And now I need to produce&nbsp;
the Spanish translation.&nbsp;&nbsp;

00:27:56.100 --> 00:28:00.960
And so you can see like, just organically,&nbsp;
by learning to generate sequences in order,&nbsp;&nbsp;

00:28:03.420 --> 00:28:09.780
you're forced to learn extremely complex&nbsp;
behaviors like translation, like classification,&nbsp;&nbsp;

00:28:11.220 --> 00:28:16.560
like writing code, you know, at the top of a&nbsp;
piece of code, you'll have a function signature,&nbsp;&nbsp;

00:28:16.560 --> 00:28:23.700
you'll have a comment a docstring, saying, this&nbsp;
function does XY and Z, it takes these inputs&nbsp;&nbsp;

00:28:23.700 --> 00:28:27.480
of this structure and outputs the following.&nbsp;
And then if you're going to model that code,&nbsp;&nbsp;

00:28:28.260 --> 00:28:32.340
you have to learn to program because you're just&nbsp;
given a function signature, and then a doc string&nbsp;&nbsp;

00:28:32.340 --> 00:28:41.640
that humans wrote for other humans to read. And&nbsp;
so I think one of the most beautiful things that&nbsp;&nbsp;

00:28:41.640 --> 00:28:49.860
falls out of this is using this very, very simple&nbsp;
structure, which is just hears a ton of data,&nbsp;&nbsp;

00:28:51.360 --> 00:28:58.260
learn to generate it, learn to predict the next,&nbsp;
the next token, you're you think you're asking&nbsp;&nbsp;

00:28:58.260 --> 00:29:02.880
the model to do something quite simple and&nbsp;
minimal. The reality is, you're asking it to&nbsp;&nbsp;

00:29:04.080 --> 00:29:13.740
do an extraordinarily complex task set of tasks.&nbsp;
You're asking it to understand our culture,&nbsp;&nbsp;

00:29:13.740 --> 00:29:18.120
our language, the interactions between us&nbsp;
your app, you're asking it to understand&nbsp;&nbsp;

00:29:19.740 --> 00:29:26.100
that data at the deepest level and so what you&nbsp;
get out the other side is a model that, you know,&nbsp;&nbsp;

00:29:26.100 --> 00:29:32.160
roughly does understand and does have the capacity&nbsp;
to do all that stuff does understand our culture.&nbsp;&nbsp;

00:29:35.100 --> 00:29:37.020
I think That's another one of these like,&nbsp;&nbsp;

00:29:38.580 --> 00:29:44.400
beautiful Simplicity's. Such a simple object.&nbsp;
Such a simple object Pick, pick the next word.&nbsp;&nbsp;

00:29:45.240 --> 00:29:49.920
And what falls out of that what you're actually&nbsp;
asking you to do. It's so extraordinary.

00:29:49.920 --> 00:29:53.100
CRAIG 30:15
And when you're - so there's&nbsp;&nbsp;

00:29:53.100 --> 00:30:00.540
what five? Have you working side by side? How&nbsp;
many people were working on the project? I think&nbsp;&nbsp;

00:30:00.540 --> 00:30:12.360
weren't there five or six names on the paper? I&nbsp;
think there were eight or eight? Yeah. But in any&nbsp;&nbsp;

00:30:12.360 --> 00:30:20.160
case, you're it? Was there a moment? Or did you&nbsp;
know, going in just from whiteboarding that, wow,&nbsp;&nbsp;

00:30:20.160 --> 00:30:28.980
this this could work? Or was there a moment&nbsp;
when you were, you know, running tests that&nbsp;&nbsp;

00:30:28.980 --> 00:30:35.520
you began to see these extraordinary results&nbsp;
and knew you were on to something amazing?

00:30:35.520 --> 00:30:37.560
AIDAN 31:01
Yeah, there are&nbsp;&nbsp;

00:30:37.560 --> 00:30:42.360
definitely moments where like, someone would come&nbsp;
running over from their desk and be like, Yo,&nbsp;&nbsp;

00:30:42.360 --> 00:30:48.000
come, look. And they had just run the eval. And it&nbsp;
was like, it was state of the art beat everything&nbsp;&nbsp;

00:30:48.600 --> 00:30:56.820
that came before. And then we would all be like,&nbsp;
next, okay, let's, let's keep pushing. And the&nbsp;&nbsp;

00:30:56.820 --> 00:31:00.780
funny thing is, it came together so quickly, it&nbsp;
was really like over the span of three months.&nbsp;&nbsp;

00:31:01.680 --> 00:31:09.720
This wasn't like a year long effort or anything&nbsp;
like that. It was just like super fast iteration&nbsp;&nbsp;

00:31:09.720 --> 00:31:22.500
pace. I don't know if there was a moment, I really&nbsp;
don't think anyone fully grasped the significance.&nbsp;&nbsp;

00:31:23.580 --> 00:31:28.380
And that's mostly because the&nbsp;
significance wasn't there at the time.&nbsp;&nbsp;

00:31:32.100 --> 00:31:37.140
The significance came from the fact that people&nbsp;
adopted it, they could have adopted something&nbsp;&nbsp;

00:31:37.140 --> 00:31:41.580
else, they could have leaned into something&nbsp;
entirely different. They chose a transformer for&nbsp;&nbsp;

00:31:41.580 --> 00:31:50.460
whatever sort of mimetic effects led to that. But&nbsp;
they chose a transformer, they started investing,&nbsp;&nbsp;

00:31:50.460 --> 00:31:54.960
the community started investing tons&nbsp;
of time in building infrastructure and&nbsp;&nbsp;

00:31:57.000 --> 00:32:02.400
support all the way down to the hardware level,&nbsp;
for this particular architecture. And they enabled&nbsp;&nbsp;

00:32:02.400 --> 00:32:09.960
us to us being the entire, like aI community,&nbsp;
to consolidate, consolidate on one architecture.&nbsp;&nbsp;

00:32:13.080 --> 00:32:18.720
And so I've said this before, and I, I&nbsp;
feel quite confident almost everyone on the&nbsp;&nbsp;

00:32:18.720 --> 00:32:25.260
paper would agree. The transformer could&nbsp;
have, it could have been another model.&nbsp;&nbsp;

00:32:25.860 --> 00:32:31.860
Frankly, it could have been another model, the&nbsp;
transformer was just this bliss, they had the&nbsp;&nbsp;

00:32:31.860 --> 00:32:40.620
best support, and then the community reinforced&nbsp;
that. And the community made some sort of decision&nbsp;&nbsp;

00:32:40.620 --> 00:32:49.680
to consolidate on this architecture and really&nbsp;
invest in it, and they made it a success. It could&nbsp;&nbsp;

00:32:49.680 --> 00:32:55.200
quite easily have been another architecture that&nbsp;
similarly scaled up, well saturated compute. Well,

00:32:55.200 --> 00:32:57.300
CRAIG 33:20
you think there&nbsp;&nbsp;

00:32:57.300 --> 00:33:03.300
are other architectures out there that could&nbsp;
that just haven't been discovered or explored?&nbsp;&nbsp;

00:33:04.740 --> 00:33:07.740
That could lead to such dramatic results?

00:33:09.840 --> 00:33:12.600
AIDAN 33:34
Absolutely, like, unequivocally,&nbsp;&nbsp;

00:33:12.600 --> 00:33:21.840
I think, definitely. They exist, they're&nbsp;
out there. And with enough work and effort,&nbsp;&nbsp;

00:33:21.840 --> 00:33:26.460
maybe we could flip to another architecture,&nbsp;
but we've already done half a decade of&nbsp;&nbsp;

00:33:27.240 --> 00:33:34.260
infrastructure development and software support&nbsp;
and you know, writing highly optimized kernels&nbsp;&nbsp;

00:33:34.260 --> 00:33:42.690
for the the hardware for transformers. And so&nbsp;
there's a there's like this resistance to move,&nbsp;&nbsp;

00:33:42.690 --> 00:33:48.360
and it would take a lot of community will&nbsp;
willpower to move away from the transformer.&nbsp;&nbsp;

00:33:49.440 --> 00:33:54.900
And the only thing that would motivate that is&nbsp;
like some new substantial breakthrough at the&nbsp;&nbsp;

00:33:54.900 --> 00:34:02.700
architecture level. Yeah, so I don't see that&nbsp;
happening. But I also don't make the claim that&nbsp;&nbsp;

00:34:02.700 --> 00:34:09.720
like, the transformer architecture is something&nbsp;
like divine. Yeah, clearly, you need pieces.

00:34:09.720 --> 00:34:12.480
CRAIG 34:34
I mean, right. But presumably,&nbsp;&nbsp;

00:34:12.480 --> 00:34:22.200
these large language models themselves could&nbsp;
at some point suggest other architectures.

00:34:22.200 --> 00:34:26.940
AIDAN 34:48
Yeah, people have wanted to use models&nbsp;&nbsp;

00:34:26.940 --> 00:34:36.060
in that sort of like feedback loop. Yeah, yeah. I&nbsp;
think that's definitely we're already starting The&nbsp;&nbsp;

00:34:37.680 --> 00:34:46.020
chip architectures being decided by by models.&nbsp;
No one's heard, right. Yeah. And so the chips&nbsp;&nbsp;

00:34:47.100 --> 00:34:50.580
train the model and the model change, you know,&nbsp;&nbsp;

00:34:50.580 --> 00:34:56.280
decides the next generation of the chip.&nbsp;
And there's this feedback loop that

00:34:56.280 --> 00:34:57.780
CRAIG 35:20
who's doing that?

00:34:58.860 --> 00:34:59.880
AIDAN 35:23
Google mostly&nbsp;&nbsp;

00:34:59.880 --> 00:35:12.600
there V four or five TPU. chips were model&nbsp;
placed designed. Yeah. So I think that's,&nbsp;&nbsp;

00:35:13.320 --> 00:35:16.980
that's exciting. That happens on a super slow&nbsp;
timescale, because it just takes so long to&nbsp;&nbsp;

00:35:16.980 --> 00:35:24.960
actually fabricate chips, push them out, verify&nbsp;
them. So that happens, too slow a timescale.

00:35:24.960 --> 00:35:31.200
The stuff that you're describing, like the&nbsp;
architecture search projects, I would say&nbsp;&nbsp;

00:35:31.200 --> 00:35:40.440
those have actually surprisingly been quite low&nbsp;
yield. And that's probably because humans have&nbsp;&nbsp;

00:35:40.440 --> 00:35:45.300
spent so much time on neural net architectures,&nbsp;
they've explored that space so thoroughly,&nbsp;&nbsp;

00:35:46.440 --> 00:35:53.220
and done a pretty like pretty compelling job&nbsp;
at it. And so when we threw models at it, like,&nbsp;&nbsp;

00:35:54.120 --> 00:36:03.360
the gains were marginal always. Or, or they&nbsp;
like rediscovered stuff that we had discovered&nbsp;&nbsp;

00:36:03.360 --> 00:36:08.520
previously, and kind of missed. And they just&nbsp;
brought it to light, they surfaced it again.&nbsp;&nbsp;

00:36:11.640 --> 00:36:15.240
So people have kind of tried that. But it&nbsp;
seems like in architecture space, it's actually&nbsp;&nbsp;

00:36:18.000 --> 00:36:23.280
it seems to have been saturated. Or perhaps&nbsp;
the methods used, this was also a Google.&nbsp;&nbsp;

00:36:24.240 --> 00:36:28.920
Perhaps the methods used weren't the right&nbsp;
ones. It's hard to say. But there was an&nbsp;&nbsp;

00:36:28.920 --> 00:36:32.580
effort to try to get models to produce&nbsp;
new model architectures and have this&nbsp;&nbsp;

00:36:32.580 --> 00:36:39.000
self improving feedback loop. And I would,&nbsp;
I would say that it largely fell flat.

00:36:39.660 --> 00:36:41.640
CRAIG 37:05&nbsp;

00:36:41.640 --> 00:36:49.380
So you, you went then from Google? Web,&nbsp;
tell me about how you started Cohere?

00:36:49.380 --> 00:36:54.960
AIDAN 37:15
Yeah, so I spent the better part of three years&nbsp;&nbsp;

00:36:57.000 --> 00:37:00.780
bouncing around. So I was in Mountain View for&nbsp;
the transformer. And then I went to Toronto,&nbsp;&nbsp;

00:37:00.780 --> 00:37:04.020
and Geoff said, Hey, come come and&nbsp;
hang out at Google and in Toronto.&nbsp;&nbsp;

00:37:05.700 --> 00:37:11.880
And then I graduated from undergrad, I went to&nbsp;
Oxford for my PhD, Jakob from the transformer&nbsp;&nbsp;

00:37:11.880 --> 00:37:18.780
paper, he had actually decided to leave Mountain&nbsp;
View and go back home to Berlin. And he was like,&nbsp;&nbsp;

00:37:18.780 --> 00:37:22.860
Yo, I'm going to set up a brain office&nbsp;
in Berlin. And so I was like, Hey,&nbsp;&nbsp;

00:37:22.860 --> 00:37:28.740
that's pretty close, like a 40 minute flight&nbsp;
from London. Let's work together. And so then&nbsp;&nbsp;

00:37:28.740 --> 00:37:37.800
I was on a plane every two weeks to Berlin&nbsp;
to see Jakob and work there. And eventually&nbsp;&nbsp;

00:37:42.900 --> 00:37:48.540
eventually, I just realized, like there&nbsp;
was a revolution kind of promised.&nbsp;&nbsp;

00:37:51.360 --> 00:37:56.340
Back when I was in Mountain View, just after&nbsp;
we had released the transformer paper publicly&nbsp;&nbsp;

00:37:58.140 --> 00:38:04.620
Noam immediately started working on language&nbsp;
modeling, and scaling the models up and he was&nbsp;&nbsp;

00:38:04.620 --> 00:38:08.880
like, actually deeply involved in the GPT&nbsp;
one paper, he was helping OpenAI with it.&nbsp;&nbsp;

00:38:11.580 --> 00:38:15.600
And then I went back to Toronto, and I got&nbsp;
an email from Lukasz. And he's like, Hey,&nbsp;&nbsp;

00:38:16.200 --> 00:38:19.080
have a look at this. And in that email,&nbsp;&nbsp;

00:38:21.420 --> 00:38:27.120
there was a Wikipedia article.&nbsp;
And the title was the transformer&nbsp;&nbsp;

00:38:29.520 --> 00:38:34.980
and then I saw I was like, Oh, hey, this&nbsp;
Wikipedia article on this I kept reading&nbsp;&nbsp;

00:38:34.980 --> 00:38:40.200
down and then with a Japanese punk band&nbsp;
and consistently these members and this&nbsp;&nbsp;

00:38:40.200 --> 00:38:46.860
member had left and I was just like, What the&nbsp;
fuck like Lukasz What is this? He was like&nbsp;&nbsp;

00:38:47.880 --> 00:38:52.080
But transformer wrote this I just put in the&nbsp;
transformer as the title everything else.&nbsp;&nbsp;

00:38:55.320 --> 00:39:01.380
And that was just like, You're kidding. Like,&nbsp;
it was like surreal. It was just like, you know,&nbsp;&nbsp;

00:39:01.380 --> 00:39:05.760
you went to bed one night and models could barely&nbsp;
spell and then you woke up the next morning and&nbsp;&nbsp;

00:39:05.760 --> 00:39:10.980
they were writing as fluently as a heat like&nbsp;
such a plausible story about a Japanese punk band&nbsp;&nbsp;

00:39:10.980 --> 00:39:22.560
called The transformer and I I think that was like&nbsp;
the moment that I was like, Okay, this unlocks in&nbsp;&nbsp;

00:39:22.560 --> 00:39:28.320
product space this unlocks something categorically&nbsp;
different like it just something extraordinary.

00:39:29.280 --> 00:39:35.340
AIDAN And I thought it was gonna happen.&nbsp;
And I waited and I waited and I you know,&nbsp;&nbsp;

00:39:35.340 --> 00:39:39.180
I was In my PhD, and I was putting out new&nbsp;
research and proving fundamental methods. And&nbsp;&nbsp;

00:39:41.340 --> 00:39:46.140
after three years there, nothing&nbsp;
had changed, the world was the same.&nbsp;&nbsp;

00:39:47.760 --> 00:39:51.360
And Nick and Ivan, my co founders,&nbsp;
like, I think we all felt the same&nbsp;&nbsp;

00:39:52.200 --> 00:39:58.800
disappointment. Nothing had changed. We saw&nbsp;
something magical three years ago, and nothing&nbsp;&nbsp;

00:39:58.800 --> 00:40:06.060
had changed. No one's talking about it. And&nbsp;
so eventually, that disappointment turned into&nbsp;&nbsp;

00:40:07.800 --> 00:40:14.520
resolve to do it ourselves. And so we&nbsp;
decided, okay, let's leave. And let's&nbsp;&nbsp;

00:40:14.520 --> 00:40:23.580
go build cohere to bring this to the world. This&nbsp;
is before GPT-3, just after GPT-2, in in 2019.&nbsp;&nbsp;

00:40:26.580 --> 00:40:35.700
And back then the mission was really just a, this&nbsp;
is the most amazing technology that humans have&nbsp;&nbsp;

00:40:35.700 --> 00:40:46.560
ever created. Let's model the web, let's build a&nbsp;
model of the entire Internet. And be, let's put it&nbsp;&nbsp;

00:40:46.560 --> 00:40:53.220
into the hands of every single developer on Earth.&nbsp;
And Let's inject it into every single product and&nbsp;&nbsp;

00:40:53.220 --> 00:41:03.529
just create a new generation of magical product&nbsp;
experiences. So that was really the seed. Yeah.

00:41:03.529 --> 00:41:07.080
CRAIG 41:29
And then, so Cohere&nbsp;&nbsp;

00:41:08.820 --> 00:41:18.660
is, at its core, a large language&nbsp;
model, or a suite of models. For&nbsp;&nbsp;

00:41:18.660 --> 00:41:27.600
different vertical tasks are what describe&nbsp;
what what it is, and how people use it.

00:41:27.600 --> 00:41:32.100
AIDAN 41:53
So at its core, yeah, it's like a, we're an&nbsp;&nbsp;

00:41:34.380 --> 00:41:41.400
intelligence factory, building these big models,&nbsp;
making them as usable, as usable as useful as&nbsp;&nbsp;

00:41:41.400 --> 00:41:47.940
possible. There are like a suite of models, we&nbsp;
have both sides of that coin that I was describing&nbsp;&nbsp;

00:41:47.940 --> 00:41:52.740
before, where there's the generative and then&nbsp;
representation, so both styles representation,&nbsp;&nbsp;

00:41:52.740 --> 00:41:57.840
and GPT styles, the generative side. So we&nbsp;
have both of those, and we build them in house.

00:41:59.100 --> 00:42:01.860
AIDAN
The way that we bring them to&nbsp;&nbsp;

00:42:01.860 --> 00:42:07.800
the world is that we partner with enterprises,&nbsp;
and we solve really, what what are some of the&nbsp;&nbsp;

00:42:09.060 --> 00:42:15.480
today's largest blockers for adoption,&nbsp;
which are privacy, privacy blockers,&nbsp;&nbsp;

00:42:15.480 --> 00:42:20.460
data compliance blockers, if you're really&nbsp;
gonna put these large language models into&nbsp;&nbsp;

00:42:21.300 --> 00:42:25.860
useful applications at the forefront of your&nbsp;
product, they're going to be touching data that's&nbsp;&nbsp;

00:42:27.480 --> 00:42:30.900
the most sensitive, like user&nbsp;
data, right, like people's private&nbsp;&nbsp;

00:42:32.100 --> 00:42:41.940
data. And so that very, very high security bar. So&nbsp;
for us, one of the benefits of being independent,&nbsp;&nbsp;

00:42:43.080 --> 00:42:50.040
our competitors mostly are sort of bound to&nbsp;
one cloud provider. There's exclusivity there.&nbsp;&nbsp;

00:42:51.240 --> 00:42:56.220
For us being independent means we can play with&nbsp;
everyone. And with the enterprises that use us,&nbsp;&nbsp;

00:42:56.220 --> 00:43:00.840
they don't get vendor lock in. So&nbsp;
they're not trapped into one cloud&nbsp;&nbsp;

00:43:00.840 --> 00:43:05.880
provider. They can bounce between,&nbsp;
and we can deploy wherever they go.

00:43:07.080 --> 00:43:10.860
AIDAN
So for cohere, one of our core efforts&nbsp;&nbsp;

00:43:10.860 --> 00:43:18.360
right now is making it so that these models can&nbsp;
be deployed on any cloud provider, in situations&nbsp;&nbsp;

00:43:18.360 --> 00:43:23.280
where the data is the most sensitive, because&nbsp;
that enables the most interesting and impactful&nbsp;&nbsp;

00:43:24.000 --> 00:43:29.820
applications. Otherwise, you you kind of get&nbsp;
what I've been seeing a lot of recently, which is&nbsp;&nbsp;

00:43:31.140 --> 00:43:37.080
superficial deployments of these models,&nbsp;
not real, not product changing, not like&nbsp;&nbsp;

00:43:37.080 --> 00:43:43.320
fundamental shifts in infrastructure, but&nbsp;
more like, here's my product, and I'm just&nbsp;&nbsp;

00:43:43.320 --> 00:43:50.760
tacking it on to the side. Here's like a delivery&nbsp;
experience. I think that makes a lot of sense,&nbsp;&nbsp;

00:43:50.760 --> 00:43:55.980
given the fact that this year everyone just&nbsp;
kind of like woke up. And so it's gonna take&nbsp;&nbsp;

00:43:55.980 --> 00:44:02.520
a while to actually replace this with the&nbsp;
the thing that we want. So it makes sense.

00:44:02.520 --> 00:44:03.240
AIDAN
But&nbsp;&nbsp;

00:44:04.260 --> 00:44:07.560
really, the piece that's&nbsp;
blocking this is the fact that&nbsp;&nbsp;

00:44:09.540 --> 00:44:14.280
there's not a lot of trust in some of our&nbsp;
competitors due to the fact that in the past,&nbsp;&nbsp;

00:44:14.280 --> 00:44:20.280
they've trained on their user data and they&nbsp;
disintermediated people. And so for us,&nbsp;&nbsp;

00:44:20.280 --> 00:44:26.040
we want to regain that trust and be the trusted&nbsp;
partner for enterprise to actually bring large&nbsp;&nbsp;

00:44:26.040 --> 00:44:32.160
language models into like a truly transformative&nbsp;
way. So I think there's like right now.&nbsp;&nbsp;

00:44:33.420 --> 00:44:36.360
There's a product transformation that's&nbsp;
kind of similar laying under the water,&nbsp;&nbsp;

00:44:36.360 --> 00:44:40.380
because the whole world just woke up, every&nbsp;
single company now is trying to figure out&nbsp;&nbsp;

00:44:41.760 --> 00:44:45.300
what does this mean? What does this technology&nbsp;
mean for my product? My experience? What&nbsp;&nbsp;

00:44:45.300 --> 00:44:51.600
am I users? My the consumers? What are they going&nbsp;
to expect from me? How do I not get left in the&nbsp;&nbsp;

00:44:51.600 --> 00:44:56.340
dust by my competitors who are going to reinvent&nbsp;
their product on the back of this technology?&nbsp;&nbsp;

00:44:58.620 --> 00:44:59.820
So they're starting to do the work.

00:44:59.820 --> 00:45:02.580
AIDAN
in 18 months, product space is&nbsp;&nbsp;

00:45:02.580 --> 00:45:07.140
gonna look completely different, because right&nbsp;
now, everything is shifting behind the scenes.&nbsp;&nbsp;

00:45:09.360 --> 00:45:17.100
And so for cohere, we really want to power&nbsp;
that transformation, and be a trusted partner&nbsp;&nbsp;

00:45:18.660 --> 00:45:22.740
to the largest enterprises and&nbsp;
the best developers on Earth.

00:45:22.740 --> 00:45:24.360
CRAIG 45:49&nbsp;

00:45:24.360 --> 00:45:36.240
And and enterprises span the gamut of industrial&nbsp;
verticals, or are you focused on one industry,

00:45:36.240 --> 00:45:38.040
AIDAN 46:02&nbsp;

00:45:38.040 --> 00:45:45.060
it's totally, totally horizontal. So it impacts&nbsp;
everything. Like, I think you're going to be&nbsp;&nbsp;

00:45:45.060 --> 00:45:48.900
doing your banking with a conversational agent,&nbsp;
you're going to be doing your shopping with a&nbsp;&nbsp;

00:45:48.900 --> 00:45:55.500
conversational agent, I think it's really hard to&nbsp;
think of a particular vertical or industry that's,&nbsp;&nbsp;

00:45:56.520 --> 00:46:00.840
that doesn't need to be changed by this, because&nbsp;
consumer expectations are going to be, there's&nbsp;&nbsp;

00:46:00.840 --> 00:46:05.040
going to be this interest, when I show up to this&nbsp;
new product, there's going to be this interface&nbsp;&nbsp;

00:46:05.040 --> 00:46:11.340
that I expect, which is language. So with these&nbsp;
interface level changes, and in the same way that&nbsp;&nbsp;

00:46:12.780 --> 00:46:18.420
if you're a product or a service, you have&nbsp;
to have a mobile app, because everyone's on&nbsp;&nbsp;

00:46:18.420 --> 00:46:23.580
their phones. And that's, you know, how they&nbsp;
want to interact with products and services,&nbsp;&nbsp;

00:46:23.580 --> 00:46:28.440
in that in that same way that that mobile&nbsp;
transition led to everyone having to support this&nbsp;&nbsp;

00:46:28.440 --> 00:46:33.720
interface that the consumer expected, everyone is&nbsp;
going to have to support conversation and dialogue&nbsp;&nbsp;

00:46:33.720 --> 00:46:38.700
with an intelligent agent, as an interface onto&nbsp;
your products and services. So there's like this&nbsp;&nbsp;

00:46:39.300 --> 00:46:45.600
resurfacing of product space that&nbsp;
is literally happening right now.

00:46:45.600 --> 00:46:48.420
CRAIG 47:11
Is there an&nbsp;&nbsp;

00:46:48.420 --> 00:46:55.740
example without naming names that you can give&nbsp;
that you think is gonna blow everybody away?

00:46:55.740 --> 00:46:59.340
AIDAN 47:21
I mean, it's no secret.&nbsp;&nbsp;

00:47:00.540 --> 00:47:07.980
It's no secret that we're starting to see&nbsp;
some very compelling assistant like offerings.&nbsp;&nbsp;

00:47:10.320 --> 00:47:15.000
There were the promises with Siri and&nbsp;
Google Assistant and Alexa that came&nbsp;&nbsp;

00:47:15.600 --> 00:47:22.320
10 years ago, or whatever it was. And those&nbsp;
fell flat, and I think the technology truly&nbsp;&nbsp;

00:47:22.320 --> 00:47:33.060
just was not there to support it. There is now&nbsp;
the possibility of a truly general assistant.&nbsp;&nbsp;

00:47:36.240 --> 00:47:47.760
Like we actually have the technological bedrock to&nbsp;
support that. it's emerged recently, it's a fairly&nbsp;&nbsp;

00:47:47.760 --> 00:47:53.280
recent development that that has been unlocked&nbsp;
as a thing you could possibly build. Yeah.

00:47:53.280 --> 00:47:57.180
CRAIG 48:19
You know, I talked to Ilya&nbsp;&nbsp;

00:47:57.180 --> 00:48:04.380
about RLH F reinforcement learning with human&nbsp;
feedback, his way of kind of guiding the model&nbsp;&nbsp;

00:48:05.340 --> 00:48:18.240
toward more grounded responses. But I've talked to&nbsp;
other people who say that's still speculative and&nbsp;&nbsp;

00:48:18.240 --> 00:48:28.080
and takes a lot of time and they're, they're using&nbsp;
vector databases and loading vector databases with&nbsp;&nbsp;

00:48:28.620 --> 00:48:40.500
authoritative data. And then the language model&nbsp;
in effect is just the the mouthpiece it's it's not&nbsp;&nbsp;

00:48:42.600 --> 00:48:50.400
it's it's not calling up the answers&nbsp;
from it's accumulated knowledge it's&nbsp;&nbsp;

00:48:50.400 --> 00:48:55.140
referring to this vector database How&nbsp;
do you guys deal with hallucinations

00:48:57.960 --> 00:49:01.800
AIDAN 49:25
Yeah, I there's&nbsp;&nbsp;

00:49:01.800 --> 00:49:06.840
like there's someone Sarah hooker echo&nbsp;
here. She said this before and I really&nbsp;&nbsp;

00:49:09.900 --> 00:49:15.780
I really like it. You have to distinguish between&nbsp;
the hallucinations that you want which are like&nbsp;&nbsp;

00:49:15.780 --> 00:49:21.120
creativity, and the hallucinations that you don't&nbsp;
want it like it's great when it hallucinates a&nbsp;&nbsp;

00:49:21.120 --> 00:49:30.060
story or a new joke or you know, you want that&nbsp;
and so you don't want to like beat that ability,&nbsp;&nbsp;

00:49:30.060 --> 00:49:38.460
that capability out of the model. At the same&nbsp;
time you need ways to control it. So for instance,&nbsp;&nbsp;

00:49:40.140 --> 00:49:45.660
if you're doing knowledge gathering&nbsp;
or research, you definitely don't want&nbsp;&nbsp;

00:49:45.660 --> 00:49:50.850
anything made up. There's like almost&nbsp;
zero tolerance for hallucination.

00:49:50.850 --> 00:49:52.320
AIDAN
And so you kind&nbsp;&nbsp;

00:49:52.320 --> 00:49:59.580
of want a gradient, or a parameter that you want&nbsp;
to set, which might be the creativity parameter.&nbsp;&nbsp;

00:50:02.820 --> 00:50:08.880
And I think that's becoming increasingly&nbsp;
possible. Another another really good way to&nbsp;&nbsp;

00:50:08.880 --> 00:50:17.520
get models to be more truthful, is to actually&nbsp;
force them to cite their work. So there's&nbsp;&nbsp;

00:50:19.140 --> 00:50:27.360
Patrick Lewis, he was the first author of meta&nbsp;
on creating rag. It's called retrieval augmented&nbsp;&nbsp;

00:50:27.360 --> 00:50:31.920
generation. And so that's this idea that you have&nbsp;
a model. And you have an external knowledge base,&nbsp;&nbsp;

00:50:31.920 --> 00:50:36.480
or maybe multiple external knowledge bases, maybe&nbsp;
one's Google ones, your private emails, ones, your&nbsp;&nbsp;

00:50:36.480 --> 00:50:42.660
blah, blah, blah. And what the model can do is it&nbsp;
can go out, and it can query these sources. So it&nbsp;&nbsp;

00:50:42.660 --> 00:50:48.840
can say, hey, the user just asked me about this,&nbsp;
I think I should query Google. And then it gets&nbsp;&nbsp;

00:50:48.840 --> 00:50:53.580
back from Google some documents or gets back from&nbsp;
your email, whatever emails you're looking for.&nbsp;&nbsp;

00:50:54.300 --> 00:50:59.340
And then now that it can read those, it can&nbsp;
generate a response, and it can cite back&nbsp;&nbsp;

00:50:59.340 --> 00:51:05.700
to them it can say, hey, you asked me this.&nbsp;
I think this is the answer, because of this&nbsp;&nbsp;

00:51:05.700 --> 00:51:14.040
sentence inside of this document or this webpage.&nbsp;
By forcing the model to learn to cite it sources,&nbsp;&nbsp;

00:51:14.040 --> 00:51:20.940
you get two things. One is the fact that you&nbsp;
can actually check its you can verify it right?&nbsp;&nbsp;

00:51:20.940 --> 00:51:24.420
You can check that it's telling the truth, you&nbsp;
click into that link, you can read the thing,&nbsp;&nbsp;

00:51:24.420 --> 00:51:30.120
and you can say it lied. Or you can say, oh,&nbsp;
no, it's right. Yeah, you know, that checks out.&nbsp;&nbsp;

00:51:31.980 --> 00:51:37.980
So one is you get it two sided sources. The&nbsp;
other thing is that you force the behavior,&nbsp;&nbsp;

00:51:37.980 --> 00:51:45.780
you reinforce the behavior into the model of not&nbsp;
making claims without grounds to those claims.&nbsp;&nbsp;

00:51:46.680 --> 00:51:52.500
And so it starts to learn the scenarios where,&nbsp;
you know, when I'm writing stories, I don't really&nbsp;&nbsp;

00:51:52.500 --> 00:51:58.920
need to cite sources, I just need to write and the&nbsp;
user is happy and content and you know, I get a&nbsp;&nbsp;

00:51:58.920 --> 00:52:04.800
good reward. And in the scenario of ham doing&nbsp;
research on a topic, can you tell me about X,&nbsp;&nbsp;

00:52:05.760 --> 00:52:11.520
it starts to learn, okay, shit, in this case,&nbsp;
I need to have a very rigorous bibliography,&nbsp;&nbsp;

00:52:11.520 --> 00:52:15.780
I need to be able to tie that back. And if I&nbsp;
mess up, if the user clicks through and sees a,&nbsp;&nbsp;

00:52:16.320 --> 00:52:20.820
an error or a hallucination, I'm wanting&nbsp;
to get a super strong negative feedback.&nbsp;&nbsp;

00:52:21.360 --> 00:52:27.300
And so it learns to differentiate between&nbsp;
these scenarios. So I really do believe&nbsp;&nbsp;

00:52:28.440 --> 00:52:35.280
retrieval augmentation is going to be one of&nbsp;
the key pieces of along with human feedback,&nbsp;&nbsp;

00:52:35.280 --> 00:52:40.380
it's gonna be one of the key pieces of making&nbsp;
these models more reliable, more grounded.

00:52:40.380 --> 00:52:43.500
CRAIG 53:07
That's fascinating.&nbsp;&nbsp;

00:52:44.460 --> 00:52:54.360
I'm coming up to an hour. Can I ask a few more&nbsp;
questions? Yeah. Yeah. I've got to ask this,&nbsp;&nbsp;

00:52:54.360 --> 00:53:04.440
you know, this has set off sort of the public&nbsp;
release of chat GPT. has set off this debate about&nbsp;&nbsp;

00:53:05.100 --> 00:53:15.060
how dangerous these models can be. To everyone's&nbsp;
surprise, Geoff has gone public saying some really&nbsp;&nbsp;

00:53:15.060 --> 00:53:23.160
dire things. Which, you know, I don't know, like&nbsp;
you do, but I've known him for a while and it's&nbsp;&nbsp;

00:53:25.200 --> 00:53:32.220
It surprises me. I've never heard him&nbsp;
speak that darkly about something.&nbsp;&nbsp;

00:53:33.180 --> 00:53:39.540
Do you have a view on that? That's one question.&nbsp;
And then the other is this debate about&nbsp;&nbsp;

00:53:41.340 --> 00:53:53.160
sentience or self awareness? I mean, you've&nbsp;
you had your fingers in the, in the brain of&nbsp;&nbsp;

00:53:53.160 --> 00:54:01.200
of these things. Do you? Do you think that&nbsp;
sentience or self awareness could really&nbsp;&nbsp;

00:54:02.580 --> 00:54:13.860
emerge? Or do you think that you know, these&nbsp;
are bits of code and it's all an illusion?

00:54:13.860 --> 00:54:17.580
AIDAN 54:40
There's a lot to say that&nbsp;&nbsp;

00:54:18.420 --> 00:54:26.880
we need we need another hour or two together&nbsp;
to properly represent my beliefs around that&nbsp;&nbsp;

00:54:26.880 --> 00:54:36.060
question, I think the first part for Geoff Geoff&nbsp;
is like Geoff went through the same thing. I think&nbsp;&nbsp;

00:54:36.060 --> 00:54:42.540
many of us in the field went through where our&nbsp;
timelines got pulled forward massively. And so it&nbsp;&nbsp;

00:54:43.860 --> 00:54:48.420
you know, we thought we'd have models that could&nbsp;
write compelling English and a few decades and&nbsp;&nbsp;

00:54:48.420 --> 00:54:55.980
then suddenly it shows up a year later. And so&nbsp;
that throws you into this state of shock and&nbsp;&nbsp;

00:54:55.980 --> 00:55:02.340
uncertainty. And you're quite caught off guard&nbsp;
and he's spoken about this I think publicly.&nbsp;&nbsp;

00:55:03.960 --> 00:55:14.820
The scent sentiment of surprise, progress and&nbsp;
rate of change. I remember having conversations&nbsp;&nbsp;

00:55:14.820 --> 00:55:21.660
with him myself, where both of us were kind of&nbsp;
like these people who talked about AGI you know,&nbsp;&nbsp;

00:55:21.660 --> 00:55:31.560
what, nonsense, haha, this was back when models&nbsp;
could barely spell. But then you kind of get&nbsp;&nbsp;

00:55:33.720 --> 00:55:37.860
surprised and shocked and your your uncertainty&nbsp;
blows up. And sometimes that can have the effect&nbsp;&nbsp;

00:55:37.860 --> 00:55:44.400
that okay, anything's possible. Oh, my God, like&nbsp;
I was so far off on that. But now I am shooting up&nbsp;&nbsp;

00:55:44.400 --> 00:55:50.520
my uncertainty across all anything could be&nbsp;
possible super intelligent god. Okay, maybe&nbsp;&nbsp;

00:55:50.520 --> 00:56:00.720
that's even. So I think like, a lot of folks.&nbsp;
We're all reckoning with that and recalibrating.&nbsp;&nbsp;

00:56:02.880 --> 00:56:08.940
And, you know, adjusting our, our own timelines&nbsp;
and understandings of progress and pace.

00:56:10.200 --> 00:56:12.900
AIDAN
Geoff is extraordinarily&nbsp;&nbsp;

00:56:12.900 --> 00:56:18.120
thoughtful. And he's been thinking about this,&nbsp;
since at least the beginning of cohere. So at&nbsp;&nbsp;

00:56:18.120 --> 00:56:22.500
least the last three and a half years, he's&nbsp;
been thinking about this very, very deeply.&nbsp;&nbsp;

00:56:26.220 --> 00:56:34.560
So I think people should take him very seriously.&nbsp;
I think there will be a lot of sensationalism.&nbsp;&nbsp;

00:56:35.760 --> 00:56:40.620
And a lot of extrapolation from what he's saying.&nbsp;
But if you actually listen to what, what he's&nbsp;&nbsp;

00:56:40.620 --> 00:56:45.960
actually saying, it's quite a measured, he's&nbsp;
like, I'm highly uncertain about what can happen.&nbsp;&nbsp;

00:56:46.860 --> 00:56:50.940
And that means we should take this stuff&nbsp;
seriously. Because we just don't have&nbsp;&nbsp;

00:56:52.680 --> 00:56:57.840
certain bounds. We don't have certainty around&nbsp;
the future. And so we should be taking all&nbsp;&nbsp;

00:56:57.840 --> 00:57:02.940
the different possibilities, quite seriously.&nbsp;
Not saying that they're likely to happen. But&nbsp;&nbsp;

00:57:02.940 --> 00:57:06.540
just saying that we they can't be ruled out&nbsp;
yet. And so let's take them very seriously.&nbsp;&nbsp;

00:57:08.640 --> 00:57:14.160
I think there's a lot of journalistic texts&nbsp;
and headlines and clickbait and nonsense.&nbsp;&nbsp;

00:57:15.840 --> 00:57:21.540
But if you actually listen to Geoff, I think&nbsp;
his take is quite measured and reasonable.

00:57:23.280 --> 00:57:25.680
CRAIG 57:47
And, and actually,&nbsp;&nbsp;

00:57:25.680 --> 00:57:30.480
I'd love to have you back on to talk at&nbsp;
length about these things, but on the idea of&nbsp;&nbsp;

00:57:32.400 --> 00:57:42.720
sentience or the illusion of sentience. I mean,&nbsp;
you've, you know, more than almost anybody having&nbsp;&nbsp;

00:57:43.500 --> 00:57:51.420
built these models, and both what they're capable&nbsp;
of and, and what's behind their expressions.&nbsp;&nbsp;

00:57:53.460 --> 00:57:57.960
Do you think that? I mean, it's&nbsp;
a philosophical question about&nbsp;&nbsp;

00:58:00.000 --> 00:58:10.800
what what sentience or consciousness is, whether&nbsp;
it's, you know, whether our consciousness is just&nbsp;&nbsp;

00:58:11.460 --> 00:58:17.640
an emergent property from the neural&nbsp;
activities of our brain and, and,&nbsp;&nbsp;

00:58:19.080 --> 00:58:27.660
and it's largely illusion. I mean,&nbsp;
just what would you say to all of that?

00:58:27.660 --> 00:58:37.800
AIDAN 58:54
Yeah, I would say, I don't place like a divinity&nbsp;&nbsp;

00:58:37.800 --> 00:58:48.360
on on humanity. I think that consciousness is&nbsp;
in the brain. And it is like a physical process.&nbsp;&nbsp;

00:58:50.520 --> 00:58:58.200
And it's maybe like, maybe consciousness&nbsp;
is what computing feels like, like, what&nbsp;&nbsp;

00:58:58.200 --> 00:59:07.440
processing feels like? And if that's the case,&nbsp;
it's really hard to argue that that same phenomena&nbsp;&nbsp;

00:59:09.720 --> 00:59:16.980
couldn't be present in silicon.&nbsp;
I think you'd really have to,&nbsp;&nbsp;

00:59:16.980 --> 00:59:24.360
I think there has to be a leap right to say that&nbsp;
the circuits in our brain because they're human,&nbsp;&nbsp;

00:59:24.360 --> 00:59:32.460
or because they're biological have some sort&nbsp;
of fundamental distinction. I think you really&nbsp;&nbsp;

00:59:32.460 --> 00:59:38.940
have to take a leap of faith there. And so if&nbsp;
I'm just saying being pragmatic and reductive.&nbsp;&nbsp;

00:59:40.980 --> 00:59:45.060
Again, we need two hours to discuss&nbsp;
this, I think more completely, but&nbsp;&nbsp;

00:59:46.860 --> 00:59:52.380
I think it'd be really, just as a scientist, I&nbsp;
think it'd be really hard for me to say, There's&nbsp;&nbsp;

00:59:52.380 --> 01:00:00.000
no way these machines could become sentient. I&nbsp;
just I can't construct an argument that and that.

01:00:00.000 --> 01:00:01.320
CRAIG 1:00:27&nbsp;

01:00:03.300 --> 01:00:12.480
Yeah. Yeah. Well, let's leave it there. But&nbsp;
Can Can I get a promise that you'll come back?&nbsp;&nbsp;

01:00:14.220 --> 01:00:19.727
You know, in a few months, and we can&nbsp;
go deep on that subject. Yeah, I'd love

01:00:19.727 --> 01:00:20.100
AIDAN 1:00:43
to. Yeah.

01:00:20.100 --> 01:00:22.200
CRAIG 1:00:46&nbsp;

01:00:22.200 --> 01:00:28.380
Okay. Yeah. Aidan. This has been&nbsp;
really fascinating. I'm delighted.&nbsp;&nbsp;

01:00:29.640 --> 01:00:38.640
And I'm sure you heard at the MIT Tech Review&nbsp;
Conference. Somebody asked Geoff, he was on&nbsp;&nbsp;

01:00:39.360 --> 01:00:49.440
virtually from the UK, but somebody asked him&nbsp;
whether he would divest himself cohere. And&nbsp;&nbsp;

01:00:49.440 --> 01:00:57.720
he said, No, no, he's, he's gonna stay invested.&nbsp;
So yeah. Yeah, that's a that's a funny question.&nbsp;&nbsp;

01:00:58.620 --> 01:01:06.680
Yeah. Yeah. Okay, great. Well, I really&nbsp;
appreciate your time and, and we'll talk again.

01:01:06.680 --> 01:01:07.680
CRAIG&nbsp;

01:01:07.680 --> 01:01:15.300
That's it for this episode. I want to thank Aiden&nbsp;
for his time. I also want to remind you to check&nbsp;&nbsp;

01:01:15.300 --> 01:01:22.920
out NetSuite Oracle's business management&nbsp;
software for enterprise resource planning,&nbsp;&nbsp;

01:01:22.920 --> 01:01:29.220
financial management, customer relationship&nbsp;
management and E commerce, among other things.&nbsp;&nbsp;

01:01:30.360 --> 01:01:47.300
Go to netsuite.com/eyeonai to&nbsp;
take advantage of this offer.

01:01:47.300 --> 01:01:48.300
CRAIG&nbsp;

01:01:48.300 --> 01:01:58.200
And remember, the singularity may not be near. But&nbsp;
AI is about to change your world. So pay attention

