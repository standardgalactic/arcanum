WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.980
NOAM
What's called AI and today has departed to&nbsp;&nbsp;

00:00:04.980 --> 00:00:13.140
basically pure engineering. It's designed&nbsp;
- the large language models are designed&nbsp;&nbsp;

00:00:13.140 --> 00:00:20.400
in such a way that in principle, they can't&nbsp;
tell you anything about language, learning,&nbsp;&nbsp;

00:00:21.060 --> 00:00:27.300
cognitive processes, generally. They can&nbsp;
produce useful devices like what I'm using,&nbsp;&nbsp;

00:00:27.300 --> 00:00:33.720
but the very design ensures that you'll&nbsp;
never understand, they'll never lead to&nbsp;&nbsp;

00:00:34.380 --> 00:00:41.780
any contribution to science. That's not a&nbsp;
criticism, any more than I'm criticizing captions.

00:00:41.780 --> 00:00:45.240
CRAIG
This week, I talk to Noam Chomsky,&nbsp;&nbsp;

00:00:45.240 --> 00:00:51.660
one of the preeminent intellectuals of our time.&nbsp;
Our conversation touched on the dichotomy between&nbsp;&nbsp;

00:00:51.660 --> 00:00:59.400
understanding and application in the field of&nbsp;
artificial intelligence. Chomsky argues that AI&nbsp;&nbsp;

00:00:59.400 --> 00:01:07.020
has shifted from a science aimed at understanding&nbsp;
cognition to a pure engineering field focused&nbsp;&nbsp;

00:01:07.020 --> 00:01:14.880
on creating useful, but not necessarily&nbsp;
explanatory, tools. He questions whether&nbsp;&nbsp;

00:01:14.880 --> 00:01:22.020
neural nets truly mirror how the brain functions&nbsp;
and whether they exhibit any true intelligence.&nbsp;&nbsp;

00:01:23.400 --> 00:01:29.100
He also suggests that advanced alien&nbsp;
life forms would likely have language&nbsp;&nbsp;

00:01:29.100 --> 00:01:37.260
structured similar to our own. Chomsky’s 94,&nbsp;&nbsp;

00:01:37.260 --> 00:01:44.460
and I reached him at his home where he appeared&nbsp;
with a clock hanging ominously over his head.

00:01:50.760 --> 00:01:51.600
You're in California.

00:01:51.600 --> 00:01:52.800
NOAM
Actually,&nbsp;&nbsp;

00:01:52.800 --> 00:01:55.680
I'm in Arizona, which is on California time.

00:01:55.680 --> 00:02:01.860
CRAIG
Yeah. So, you know, I wanted to talk to&nbsp;&nbsp;

00:02:01.860 --> 00:02:11.280
you because you have the, you know, one of the few&nbsp;
people with a deep understanding of linguistics&nbsp;&nbsp;

00:02:11.280 --> 00:02:21.000
and natural language processing, that has the&nbsp;
historical knowledge of where we are, how we got&nbsp;&nbsp;

00:02:21.000 --> 00:02:32.580
to where we are and what that might mean for the&nbsp;
future. I understand the, your criticisms of deep&nbsp;&nbsp;

00:02:32.580 --> 00:02:44.340
learning and what large language models are not&nbsp;
in terms of reasoning and, you know, understanding&nbsp;&nbsp;

00:02:44.340 --> 00:02:53.520
the, the underpinnings of language. But I thought&nbsp;
maybe I could ask you to talk about how this&nbsp;&nbsp;

00:02:53.520 --> 00:03:00.180
developed. I mean, going back to Minsky's thesis&nbsp;
at Princeton when he was, you know, before he&nbsp;&nbsp;

00:03:00.180 --> 00:03:10.380
turned against the perceptron, when he was talking&nbsp;
about nets as a possible model for biological&nbsp;&nbsp;

00:03:10.380 --> 00:03:17.700
processes in the brain. And then, you know,&nbsp;
how did how you see that things developed? And&nbsp;&nbsp;

00:03:18.480 --> 00:03:26.580
what were the failures that didn't get to where,&nbsp;
presumably, you would have wanted that research&nbsp;&nbsp;

00:03:26.580 --> 00:03:34.040
to go. And then, and then I have some other&nbsp;
questions. But is that enough to get started?

00:03:34.040 --> 00:03:38.280
NOAM
Let's, let's take an analogy.&nbsp;&nbsp;

00:03:39.360 --> 00:03:48.000
Suppose you're interested in&nbsp;
figuring out how insects navigate.&nbsp;&nbsp;

00:03:49.140 --> 00:03:58.080
Biological problem. So, one thing&nbsp;
you can do is say, let's try to&nbsp;&nbsp;

00:03:58.680 --> 00:04:04.980
study in detail what the desert ants are&nbsp;
doing in my backyard, how they're using&nbsp;&nbsp;

00:04:05.520 --> 00:04:14.160
solar azimus, so on and so forth. Something else&nbsp;
you could do is say, look it’s easy. I'll just&nbsp;&nbsp;

00:04:14.160 --> 00:04:22.440
build an automobile, which can navigate. Fine&nbsp;
does better than the desert ants, so who cares?&nbsp;&nbsp;

00:04:24.600 --> 00:04:31.260
Well, those are the two forms of artificial&nbsp;
intelligence. One is what Minsky was after,&nbsp;&nbsp;

00:04:32.340 --> 00:04:41.400
it's now kind of ridiculed as good&nbsp;
old-fashioned AI, GOFAI. We're past that stage.

00:04:41.400 --> 00:04:44.700
NOAM
Now we just build things that do it better.&nbsp;&nbsp;

00:04:45.420 --> 00:04:53.160
Okay. Like an airplane does better than an&nbsp;
eagle. So, who cares about how eagles fly?&nbsp;&nbsp;

00:04:54.480 --> 00:05:01.500
That's possible. But it's a difference&nbsp;
between totally different goals.&nbsp;&nbsp;

00:05:02.220 --> 00:05:09.960
Roughly speaking, science and engineering. It's&nbsp;
not a sharp difference. But first approximation.&nbsp;&nbsp;

00:05:10.860 --> 00:05:13.380
Either you're interested&nbsp;
in understanding something,&nbsp;&nbsp;

00:05:14.100 --> 00:05:19.020
or you're just interested in building&nbsp;
something that'll work for some purpose.&nbsp;&nbsp;

00:05:19.620 --> 00:05:29.280
Those are both fine occupations. Nothing wrong&nbsp;
with, I mean, when you say my criticism of large&nbsp;&nbsp;

00:05:29.280 --> 00:05:34.980
language models, that's not correct. I'm&nbsp;
using them right now. I'm reading captions.&nbsp;&nbsp;

00:05:36.300 --> 00:05:44.760
Captions are based on deep learning, clever&nbsp;
programming. Very useful. I'm hard of hearing. So,&nbsp;&nbsp;

00:05:44.760 --> 00:05:50.340
they're very helpful to me. No criticism.&nbsp;
But if somebody comes along and says, okay,&nbsp;&nbsp;

00:05:50.340 --> 00:05:59.160
this explains language. You tell them, it's kind&nbsp;
of like saying an airplane explains eagles flying.&nbsp;&nbsp;

00:05:59.760 --> 00:06:05.340
It’s the wrong question. It's not intended&nbsp;
to understand leave any understanding. It's&nbsp;&nbsp;

00:06:05.340 --> 00:06:13.680
intended to be for a useful purpose. That's fine.&nbsp;
No criticism. And yet, what's called AI and today&nbsp;&nbsp;

00:06:14.700 --> 00:06:25.020
has departed to basically pure engineering. It's&nbsp;
designed - the large language models are designed&nbsp;&nbsp;

00:06:25.020 --> 00:06:32.220
in such a way that in principle, they can't&nbsp;
tell you anything about language, learning,&nbsp;&nbsp;

00:06:32.820 --> 00:06:40.680
cognitive processes, generally. They can produce&nbsp;
useful devices like what I'm using, but the very&nbsp;&nbsp;

00:06:40.680 --> 00:06:48.420
design ensures that you'll never understand,&nbsp;
they'll never lead to any contribution to science.&nbsp;&nbsp;

00:06:49.260 --> 00:06:53.633
That's not a criticism, any more&nbsp;
than I'm criticizing captions. Yeah.

00:06:53.633 --> 00:06:55.380
CRAIG
Geoff Hinton&nbsp;&nbsp;

00:06:55.380 --> 00:07:04.440
says that, you know, his goal was to understand&nbsp;
the brain how the brain works. And he talks about&nbsp;&nbsp;

00:07:05.880 --> 00:07:15.300
AI as we know it today, supervised learning&nbsp;
and generative AI as useful byproducts,&nbsp;&nbsp;

00:07:15.300 --> 00:07:26.100
but that are not his goal or not the goal of&nbsp;
cognitive science or computational biology.&nbsp;&nbsp;

00:07:28.020 --> 00:07:36.960
Was there a point at which you think the&nbsp;
research lost a bead or is there research&nbsp;&nbsp;

00:07:36.960 --> 00:07:45.600
going on that people aren't paying attention to&nbsp;
that is not caught up in the usefulness of these&nbsp;&nbsp;

00:07:47.220 --> 00:07:49.500
other kinds of neural nets?

00:07:50.660 --> 00:07:51.660
NOAM&nbsp;

00:07:51.660 --> 00:07:57.600
Well, first of all, if you're interested in how&nbsp;
the brain works, the first question you ask is,&nbsp;&nbsp;

00:07:58.620 --> 00:08:06.780
does it work by neural nets? That's an open&nbsp;
question. There's plenty of critical analysis&nbsp;&nbsp;

00:08:06.780 --> 00:08:12.360
that argues that neural nets are not what's&nbsp;
involved even in simple things like memory.&nbsp;&nbsp;

00:08:13.380 --> 00:08:21.180
Sure, there's arguments that go back to&nbsp;
Helmholtz. Neural transmission is pretty&nbsp;&nbsp;

00:08:21.180 --> 00:08:30.240
slow as compared with ordinary, there's much&nbsp;
sharper criticism by people like Randy Gallistel,&nbsp;&nbsp;

00:08:30.240 --> 00:08:37.560
cognitive neuroscientist, has given pretty&nbsp;
sound arguments that neural nets, in principle,&nbsp;&nbsp;

00:08:38.100 --> 00:08:48.780
don't have the ability to capture&nbsp;
the core notion of a Turing machine.&nbsp;&nbsp;

00:08:50.460 --> 00:08:57.840
computational capacity, they just don't have the&nbsp;
capacity. And he's argued that the computational&nbsp;&nbsp;

00:08:57.840 --> 00:09:07.380
capacity is in much richer computational systems&nbsp;
in the brain, internals themselves, where there's&nbsp;&nbsp;

00:09:07.380 --> 00:09:15.420
very rich computational capacity goes way beyond&nbsp;
neural net. Some experimental evidence supports&nbsp;&nbsp;

00:09:15.420 --> 00:09:19.800
this. So, if you're interested in the brain,&nbsp;
that's the kind of thing you're looking at.&nbsp;&nbsp;

00:09:20.880 --> 00:09:28.320
Not just saying, can I make bigger neural net?&nbsp;
Okay, if you want to try it, and maybe it's their&nbsp;&nbsp;

00:09:28.320 --> 00:09:35.400
own place to look? So, the first question is,&nbsp;
is it even the right place to look? That's an&nbsp;&nbsp;

00:09:35.400 --> 00:09:43.140
open question in neuroscience. If you take a vote&nbsp;
among neuroscientists, almost all of them think&nbsp;&nbsp;

00:09:43.140 --> 00:09:49.260
that neural nets are the right place to look. But&nbsp;
you don't solve scientific questions by a vote.

00:09:50.220 --> 00:09:53.400
CRAIG
Yeah. The I mean,&nbsp;&nbsp;

00:09:53.400 --> 00:10:00.180
one of the things that's obvious is,&nbsp;
is neural nets, they may be a model,&nbsp;&nbsp;

00:10:00.180 --> 00:10:07.140
and they may mimic a portion of brain activity.&nbsp;
But there are so many other structures,

00:10:07.140 --> 00:10:10.020
NOAM
There’s all kinds of stuff going on in&nbsp;&nbsp;

00:10:10.020 --> 00:10:16.200
the brain, way down to the cellular level. There&nbsp;
are chemical interactions, plenty of other things.&nbsp;&nbsp;

00:10:16.860 --> 00:10:22.320
So maybe you'll learn something by&nbsp;
studying neural nets. If you do fine,&nbsp;&nbsp;

00:10:22.320 --> 00:10:29.100
everybody will be happy. But maybe that's not the&nbsp;
place to look, if you want to study, even simple&nbsp;&nbsp;

00:10:29.100 --> 00:10:38.700
things like just memory and associations. There&nbsp;
is now already evidence of associations internal&nbsp;&nbsp;

00:10:38.700 --> 00:10:48.180
to large cells in the hippocampus. Internal, which&nbsp;
means maybe something's going on at a deeper level&nbsp;&nbsp;

00:10:48.180 --> 00:10:56.640
where there's vastly more computational capacity.&nbsp;
Those are serious questions. So, there's nothing&nbsp;&nbsp;

00:10:56.640 --> 00:11:01.620
wrong with trying to construct models and seeing&nbsp;
if we can learn something from them. Again, fine.

00:11:01.620 --> 00:11:03.960
CRAIG
Building larger models,&nbsp;&nbsp;

00:11:03.960 --> 00:11:12.660
which is kind of the rage in the engineering&nbsp;
side of AI right now, does produce remarkable&nbsp;&nbsp;

00:11:12.660 --> 00:11:23.040
results. I mean, what was your reaction when you&nbsp;
saw chat GPT or GPT-4, or any of these models,&nbsp;&nbsp;

00:11:23.880 --> 00:11:32.250
that it's just sort of a clever, stochastic&nbsp;
parrot? Or that there was something deeper?

00:11:32.250 --> 00:11:34.560
NOAM
If you look at the design of the system,&nbsp;&nbsp;

00:11:35.340 --> 00:11:45.000
you can see it's like, an airplane explaining&nbsp;
flying, as nine do with it. In fact,&nbsp;&nbsp;

00:11:45.000 --> 00:11:52.920
it's immediately obvious, trivially obvious, not a&nbsp;
deep point, that it can't be teaching us anything.&nbsp;&nbsp;

00:11:53.580 --> 00:12:02.460
The reason is very simple. The large learning&nbsp;
models work just as well for impossible languages&nbsp;&nbsp;

00:12:02.460 --> 00:12:10.260
that children can’t acquire, as for the languages&nbsp;
they're trained on. So, it's as if a biologist&nbsp;&nbsp;

00:12:10.260 --> 00:12:18.180
came along and said, I got a great new theory&nbsp;
of organisms, lists a lot of organisms that&nbsp;&nbsp;

00:12:18.180 --> 00:12:24.720
possibly exist. A lot that can't possibly exist,&nbsp;
and I can tell you nothing about the difference.&nbsp;&nbsp;

00:12:26.400 --> 00:12:33.360
I mean, that's it's not a contribution to biology&nbsp;
- doesn't meet the first minimal condition.&nbsp;&nbsp;

00:12:34.320 --> 00:12:42.060
The first minimal condition is distinguish between&nbsp;
what's possible from what's not possible. If you&nbsp;&nbsp;

00:12:42.060 --> 00:12:50.040
can't do that, it's not a contribution to&nbsp;
science, that if it was biologists making&nbsp;&nbsp;

00:12:50.040 --> 00:12:57.300
that proposal you’d just laugh. Why shouldn't&nbsp;
we just laugh when an engineer from Silicon&nbsp;&nbsp;

00:12:57.300 --> 00:13:04.080
Valley says the same thing? So maybe they're&nbsp;
fun, maybe they're useful for something,&nbsp;&nbsp;

00:13:04.080 --> 00:13:12.000
maybe they're harmful? Those are the kinds of&nbsp;
questions you ask about pure technology. Take&nbsp;&nbsp;

00:13:12.000 --> 00:13:18.540
large language models. There is something they're&nbsp;
useful. Fact I'm using them right at this minute.&nbsp;&nbsp;

00:13:19.620 --> 00:13:28.140
Actions. Very helpful for people like me. Are&nbsp;
they harmful, and that can cause a lot of harm.&nbsp;&nbsp;

00:13:28.860 --> 00:13:38.280
Disinformation, defamation, preying on human&nbsp;
gullibility, plenty of examples. So they can&nbsp;&nbsp;

00:13:38.280 --> 00:13:46.560
cause harm, they can be abuse. Those are the kinds&nbsp;
of questions you ask about pure engineering, which&nbsp;&nbsp;

00:13:46.560 --> 00:13:54.180
can be very sophisticated and clever. The internal&nbsp;
combustion engine is a very sophisticated device.&nbsp;&nbsp;

00:13:55.260 --> 00:14:04.140
But we don't expect it to tell us anything about&nbsp;
how a gazelle runs. It’s just the wrong question.

00:14:06.120 --> 00:14:09.120
CRAIG
Although,&nbsp;&nbsp;

00:14:10.380 --> 00:14:18.900
you know, I talk a lot to Geoff Hinton, and he'll&nbsp;
be the first to concede the backpropagation is&nbsp;&nbsp;

00:14:18.900 --> 00:14:24.660
not there's no evidence of that. And, in fact,&nbsp;
there's a lot of evidence that it wouldn't work&nbsp;&nbsp;

00:14:25.320 --> 00:14:36.600
in the brain. Reinforcement Learning. You know,&nbsp;
I've spoken to rich Sutton that's been accepted&nbsp;&nbsp;

00:14:36.600 --> 00:14:48.660
as by a lot of people as an algorithmic model&nbsp;
for brain activity in, in part of the brain,&nbsp;&nbsp;

00:14:48.660 --> 00:15:00.300
the lower brain. So, in terms of exploring&nbsp;
the mechanisms of the brain, it seems that,&nbsp;&nbsp;

00:15:01.020 --> 00:15:05.460
that there is some usefulness, I mean,&nbsp;
as you said there's, on the one hand,&nbsp;&nbsp;

00:15:07.200 --> 00:15:16.200
people look at the principles. And then they built&nbsp;
through engineering just is the analogy of a bird&nbsp;&nbsp;

00:15:16.200 --> 00:15:22.620
to an airplane they, they've taken some of the&nbsp;
principles and applied it through engineering and&nbsp;&nbsp;

00:15:22.620 --> 00:15:29.580
created something useful. But there are scientists&nbsp;
that are looking at what's been created,&nbsp;&nbsp;

00:15:29.580 --> 00:15:39.420
like Hinton's criticism of backpropagation.&nbsp;
and are looking for other models that would&nbsp;&nbsp;

00:15:39.420 --> 00:15:47.940
fit with the principles they see in cognitive&nbsp;
science or in the brain. And I mentioned this&nbsp;&nbsp;

00:15:47.940 --> 00:15:55.320
forward-forward algorithm, which you said you&nbsp;
haven't looked at, but I found it compelling,&nbsp;&nbsp;

00:15:58.440 --> 00:16:07.680
in that it doesn't require, you know, signals to&nbsp;
be passing back through the neurons. I mean, they,&nbsp;&nbsp;

00:16:07.680 --> 00:16:19.320
they pass back, but then stimulate other&nbsp;
neurons as you move forward in time, but&nbsp;&nbsp;

00:16:22.140 --> 00:16:33.000
is there nothing that that's been learned in&nbsp;
the study of AI or the research of neural nets.

00:16:35.220 --> 00:16:38.400
NOAM
If you can find anything, it's great.&nbsp;&nbsp;

00:16:39.420 --> 00:16:46.860
Nothing against search. But it's just&nbsp;
that we have to remember you asked about&nbsp;&nbsp;

00:16:47.700 --> 00:16:57.180
chatbots. What do we learn from them? Zero, for&nbsp;
the simple reason that the systems work as well,&nbsp;&nbsp;

00:16:57.180 --> 00:17:07.800
for impossible languages as for possible ones,&nbsp;
so it's like the biologist with the new theory&nbsp;&nbsp;

00:17:07.800 --> 00:17:15.060
that has organisms and impossible ones and can't&nbsp;
tell the difference. Now, maybe by the look at the&nbsp;&nbsp;

00:17:15.060 --> 00:17:21.780
systems, you'll learn something about possible.&nbsp;
Okay, great, all-in favor of learning things.&nbsp;&nbsp;

00:17:22.620 --> 00:17:31.140
But there's no issues It's just that the&nbsp;
systems themselves - and there are great claims&nbsp;&nbsp;

00:17:31.680 --> 00:17:37.920
by some of the leading figures in the field,&nbsp;
we've solved the problem of language acquisition,&nbsp;&nbsp;

00:17:38.760 --> 00:17:47.220
namely zero contribution, because the&nbsp;
systems work as well for impossible&nbsp;&nbsp;

00:17:47.220 --> 00:17:53.640
languages. Therefore, they can't be telling&nbsp;
you anything about language acquisition,&nbsp;&nbsp;

00:17:54.600 --> 00:18:00.180
period. Maybe they're useful for&nbsp;
something else. Okay, let's take a look.

00:18:00.180 --> 00:18:04.140
CRAIG
Well, maybe for the audience&nbsp;&nbsp;

00:18:04.140 --> 00:18:12.780
that this is going out to, I understand what&nbsp;
you mean by impossible, but could you just give&nbsp;&nbsp;

00:18:12.780 --> 00:18:19.380
a brief synopsis of what you mean by impossible&nbsp;
languages for people that haven't read your work?

00:18:20.240 --> 00:18:21.240
NOAM&nbsp;

00:18:21.240 --> 00:18:28.680
Well, and there are certain general&nbsp;
properties that every infant knows,&nbsp;&nbsp;

00:18:30.900 --> 00:18:39.000
already tested down to two years old.&nbsp;
No evidence. Couldn't have evidence. So,&nbsp;&nbsp;

00:18:39.000 --> 00:18:48.960
one of the basic properties of language is that&nbsp;
the linguistic rules applied to structures, not&nbsp;&nbsp;

00:18:48.960 --> 00:18:57.660
linear strings. So, if you want to&nbsp;
take a sentence like ‘instinctively,&nbsp;&nbsp;

00:18:59.820 --> 00:19:10.740
birds that fly, swim’ - it means instinctively&nbsp;
they swim. Not instinctively they fly. Well,&nbsp;&nbsp;

00:19:11.400 --> 00:19:20.520
the adverb instinctively has to find a verb to&nbsp;
attach to. It skips the closest verb and finds&nbsp;&nbsp;

00:19:20.520 --> 00:19:27.360
the structurally closest ones. That principle&nbsp;
turns out to be universal for all structures,&nbsp;&nbsp;

00:19:27.360 --> 00:19:35.400
all constructions, in all languages. What it&nbsp;
means is that an infant from birth, as soon&nbsp;&nbsp;

00:19:35.400 --> 00:19:44.580
as you can test, automatically disregards linear&nbsp;
word order, and disregards 100% of what it hears,&nbsp;&nbsp;

00:19:44.580 --> 00:19:52.380
notice, as all we hear is words in linear order,&nbsp;
but you disregard them. And you deal only with&nbsp;&nbsp;

00:19:52.380 --> 00:19:59.940
abstract structures in your mind, which you&nbsp;
never hear. Take another simple example, take&nbsp;&nbsp;

00:20:00.780 --> 00:20:09.120
‘the friends of my brothers are in England.’&nbsp;
Who's in England, the friends or the brothers,&nbsp;&nbsp;

00:20:10.380 --> 00:20:16.440
the friends, not the brothers, the one that's&nbsp;
adjacent, you just disregard all the linear&nbsp;&nbsp;

00:20:16.440 --> 00:20:23.280
information means you disregard everything,&nbsp;
you hear everything. And you pay attention&nbsp;&nbsp;

00:20:23.280 --> 00:20:30.120
only to what your mind constructs. That's&nbsp;
the basic, most fundamental property of&nbsp;&nbsp;

00:20:30.120 --> 00:20:34.740
language. While you can make up impossible&nbsp;
languages that work with what you hear,&nbsp;&nbsp;

00:20:35.820 --> 00:20:41.280
simple rule, take the first relevant thing sociate&nbsp;&nbsp;

00:20:42.840 --> 00:20:48.300
friends of my brothers are here, brothers are&nbsp;
the closest things, and the brothers are here&nbsp;&nbsp;

00:20:48.960 --> 00:20:57.360
reveal rule much simpler than the rule we use. You&nbsp;
can construct languages that use only those simple&nbsp;&nbsp;

00:20:57.360 --> 00:21:05.640
rules that are based on the linear order what we&nbsp;
hear now, maybe children, people could acquire&nbsp;&nbsp;

00:21:05.640 --> 00:21:13.920
them as a puzzle, somehow, using nonlinguistic&nbsp;
capacities. But they're not what children, infants&nbsp;&nbsp;

00:21:13.920 --> 00:21:22.260
reflexively construct with no evidence, whereas&nbsp;
many things like this impossible and impossible&nbsp;&nbsp;

00:21:22.260 --> 00:21:28.740
languages. And nobody's tried it out. Because&nbsp;
it's too obvious how it's going to turn out. You&nbsp;&nbsp;

00:21:28.740 --> 00:21:36.480
take a large language model and apply it to one&nbsp;
of these models. Systems that use linear order,&nbsp;&nbsp;

00:21:36.480 --> 00:21:43.620
of course, it's going to work fine, trivial&nbsp;
rules. Well, that's a refutation of the system.

00:21:43.620 --> 00:21:45.180
CRAIG
You&nbsp;&nbsp;

00:21:45.180 --> 00:21:48.076
mean that if you trained a large language&nbsp;
model on impossible language, if you had a&nbsp;&nbsp;

00:21:48.076 --> 00:21:51.900
large enough corpus, then it would generate&nbsp;
impossible language. Is that what you mean?

00:21:52.520 --> 00:21:53.520
NOAM&nbsp;

00:21:53.520 --> 00:21:59.400
You don't even have to train it because the rules&nbsp;
are simple. Rules are much simpler than the rules&nbsp;&nbsp;

00:21:59.400 --> 00:22:06.060
of language. Like taking things that are, take&nbsp;
the example the friends of my brother are here,&nbsp;&nbsp;

00:22:06.780 --> 00:22:17.100
the way we actually do it, is we don't say, take&nbsp;
the noun phrase that's closest we don't do that&nbsp;&nbsp;

00:22:17.100 --> 00:22:22.500
would be trivial. We don't do it. What we say&nbsp;
is, first construct the structure in your mind,&nbsp;&nbsp;

00:22:23.160 --> 00:22:30.600
friends of my brothers, then figure out that the&nbsp;
central element in that structure is friends,&nbsp;&nbsp;

00:22:30.600 --> 00:22:36.420
not brothers. And then let's let us be&nbsp;
talking about the broad the head of it.&nbsp;&nbsp;

00:22:36.420 --> 00:22:42.900
It's pretty complicated computation. But that's&nbsp;
the one we do instantaneously and reflexively.&nbsp;&nbsp;

00:22:43.440 --> 00:22:49.980
And we ignore and we never see it, hear it.&nbsp;
Remember, we don't hear structures. All you hear&nbsp;&nbsp;

00:22:49.980 --> 00:22:55.440
is words, and then you are, what we hear is words,&nbsp;
and then you order we never use that information,&nbsp;&nbsp;

00:22:56.340 --> 00:23:03.600
we use only the much more look like complex. If&nbsp;
you think about it computationally. It's actually&nbsp;&nbsp;

00:23:03.600 --> 00:23:11.100
simpler. But that's a deeper question, which&nbsp;
is why we do it. Move to a different dimension,&nbsp;&nbsp;

00:23:11.100 --> 00:23:15.780
there's a reason for this. The reason&nbsp;
has to do with a theory of computation,&nbsp;&nbsp;

00:23:16.500 --> 00:23:26.820
if you're trying to construct an infinite array&nbsp;
of structured expressions, the simplest way to&nbsp;&nbsp;

00:23:26.820 --> 00:23:31.620
do that the simplest computational&nbsp;
procedure is binary set formation.&nbsp;&nbsp;

00:23:32.520 --> 00:23:36.840
But if you use binary set formation, you&nbsp;
just going to get structures do not order.&nbsp;&nbsp;

00:23:37.980 --> 00:23:44.640
So, what the brain is doing is the simplest&nbsp;
computational system, which happens to be very&nbsp;&nbsp;

00:23:44.640 --> 00:23:52.080
much harder to use. Nature doesn't care about&nbsp;
that. Nature constructs, the simplest system,&nbsp;&nbsp;

00:23:52.080 --> 00:24:01.860
doesn't care about if it's hard to use or not. I&nbsp;
mean, nature could have saved us a lot of trouble.&nbsp;&nbsp;

00:24:01.860 --> 00:24:09.720
If it had developed eight fingers instead of 10,&nbsp;
then we'd have a much better base for computation.&nbsp;&nbsp;

00:24:10.920 --> 00:24:16.740
That nature didn't care about that when&nbsp;
it developed and fingered. If you look&nbsp;&nbsp;

00:24:16.740 --> 00:24:22.800
at evolution pays no attention to function. It&nbsp;
just constructs the best system at each point.&nbsp;&nbsp;

00:24:23.520 --> 00:24:29.160
There's a lot of misleading talk about that.&nbsp;
You just think about the physics of evolution.&nbsp;&nbsp;

00:24:31.140 --> 00:24:39.600
Say a bacterium swallows another organism,&nbsp;
the basis for what became complex cells.&nbsp;&nbsp;

00:24:40.800 --> 00:24:47.460
Nature doesn't get the new system; it&nbsp;
reconstructs it in the simplest possible way.&nbsp;&nbsp;

00:24:48.240 --> 00:24:55.020
It doesn't pay any attention to how complex&nbsp;
organisms are going to behave. That’s not&nbsp;&nbsp;

00:24:55.020 --> 00:25:00.600
what nature can do. And that's the way&nbsp;
evolution works all the way down the line.&nbsp;&nbsp;

00:25:01.560 --> 00:25:09.660
So, not surprisingly, nature constructed&nbsp;
language so that it's computationally elegant.&nbsp;&nbsp;

00:25:10.260 --> 00:25:20.460
But dysfunctions; hard to use in many ways, not&nbsp;
nature's problem. Just like every other aspect of&nbsp;&nbsp;

00:25:20.460 --> 00:25:25.800
nature, you can think of a way in which you can&nbsp;
do it better but didn't happen stage by stage.

00:25:26.720 --> 00:25:30.480
CRAIG
Two questions from that one,&nbsp;&nbsp;

00:25:30.480 --> 00:25:39.120
one. So, your view is that artificial intelligence&nbsp;
as it's being called, particularly generative AI&nbsp;&nbsp;

00:25:40.800 --> 00:25:45.900
doesn't exhibit true intelligence?&nbsp;
Is that inside, right?

00:25:45.900 --> 00:25:47.820
NOAM
I wouldn't even say that&nbsp;&nbsp;

00:25:47.820 --> 00:25:53.160
it's irrelevant to the question of&nbsp;
intelligence. It's not its problem.&nbsp;&nbsp;

00:25:54.060 --> 00:26:04.440
A guy who designs a jet plane is not trying&nbsp;
to answer the question how to eagles fly. So,&nbsp;&nbsp;

00:26:04.440 --> 00:26:12.600
to say, well, it doesn't tell us how eagles fly&nbsp;
is the wrong question to ask. It’s not the goal.

00:26:12.600 --> 00:26:15.900
CRAIG
Except that what we're what&nbsp;&nbsp;

00:26:15.900 --> 00:26:22.740
people are struggling with right now. You know,&nbsp;
you've heard the existential threat argument,&nbsp;&nbsp;

00:26:22.740 --> 00:26:29.100
that that these models, if they get large enough,&nbsp;
they'll actually be more intelligent than humans.

00:26:29.100 --> 00:26:32.880
NOAM
That's science fiction. I mean,&nbsp;&nbsp;

00:26:32.880 --> 00:26:40.560
there is a theoretical possibility. You can&nbsp;
give a theoretical argument that in principle,&nbsp;&nbsp;

00:26:41.160 --> 00:26:52.080
a complex system with vast search capacity&nbsp;
could conceivably turn into something that&nbsp;&nbsp;

00:26:52.080 --> 00:27:00.960
would start to do things that you can't predict&nbsp;
maybe beyond. But that's even more remote than&nbsp;&nbsp;

00:27:01.740 --> 00:27:11.280
some distant asteroid maybe someday hitting the&nbsp;
Earth. It could happen. If you read and serious&nbsp;&nbsp;

00:27:11.280 --> 00:27:19.560
scientists on this, like Max Tegmark, his book&nbsp;
on the three levels of intelligence. He does&nbsp;&nbsp;

00:27:19.560 --> 00:27:30.960
give a sound theoretical argument as to how a&nbsp;
massive system could, say, run through all the&nbsp;&nbsp;

00:27:31.500 --> 00:27:41.820
scientific discoveries in history. Maybe find out&nbsp;
some better way of developing them and use that&nbsp;&nbsp;

00:27:41.820 --> 00:27:47.460
better way to design something new, which would&nbsp;
destroy us all. Yeah, it's in theory possible,&nbsp;&nbsp;

00:27:47.460 --> 00:27:54.120
but it's so remote from anything that's available&nbsp;
that it's a waste of time to think of them.

00:27:54.120 --> 00:27:57.480
CRAIG
Yeah, so your view&nbsp;&nbsp;

00:27:57.480 --> 00:28:07.560
is that whatever threat exists from generative AI&nbsp;
it's the more mundane threat of disinformation and

00:28:07.560 --> 00:28:10.620
NOAM
Disinformation defamation.&nbsp;&nbsp;

00:28:11.700 --> 00:28:18.360
gullibility. Gary Marcus has done&nbsp;
a lot of work on this. Real cases.&nbsp;&nbsp;

00:28:18.900 --> 00:28:25.500
Those are problems. I mean, you may have seen&nbsp;
that there was a sort of as a joke people,&nbsp;&nbsp;

00:28:26.220 --> 00:28:36.300
somebody developed the deformation of the Pope,&nbsp;
put an image of the Pope, somebody could do it for&nbsp;&nbsp;

00:28:36.300 --> 00:28:43.920
you duplicate your face. So, it looks more or less&nbsp;
like your face pretty much duplicate your voice&nbsp;&nbsp;

00:28:44.520 --> 00:28:54.600
develop a robot that looks kind of like you have&nbsp;
you say, some insane thing would be hard. Only&nbsp;&nbsp;

00:28:54.600 --> 00:29:02.700
an expert could tell whether it was you or&nbsp;
not. It was done already several times, but&nbsp;&nbsp;

00:29:02.700 --> 00:29:09.900
basically as a joke. When powerful institutions&nbsp;
get started on it, it's not going to be a joke.

00:29:10.800 --> 00:29:16.140
CRAIG
Another argument&nbsp;&nbsp;

00:29:16.140 --> 00:29:24.360
that's swirling around these large language&nbsp;
models is the question of sentience of whether&nbsp;&nbsp;

00:29:25.740 --> 00:29:31.560
if the model is large enough, and this goes a&nbsp;
little bit back to how there's a lot more going&nbsp;&nbsp;

00:29:31.560 --> 00:29:39.280
on in the brain than, than the neural network&nbsp;
of the cerebral cortex, but that that there&nbsp;&nbsp;

00:29:39.280 --> 00:29:46.920
is the potential for some kind of sentience,&nbsp;
not necessarily equivalent to human sentience.

00:29:47.580 --> 00:29:50.460
NOAM
These are&nbsp;&nbsp;

00:29:50.460 --> 00:29:59.280
vacuous questions like asking, does a submarine&nbsp;
really swim? You want to call that swimming, then&nbsp;&nbsp;

00:29:59.280 --> 00:30:04.680
it swims. Do you not want to call it swimming?&nbsp;
Then it's not. It's not a substantive question.

00:30:06.300 --> 00:30:09.420
CRAIG
In the, in the sense that it,&nbsp;&nbsp;

00:30:09.420 --> 00:30:13.140
it supports the view that that&nbsp;
there is no separation between&nbsp;&nbsp;

00:30:14.280 --> 00:30:18.900
consciousness in the mind, the&nbsp;
material activities of the brain

00:30:18.900 --> 00:30:21.120
NOAM
As a separation,&nbsp;&nbsp;

00:30:21.120 --> 00:30:25.140
that hasn't believed been&nbsp;
believed since the 17th century.&nbsp;&nbsp;

00:30:25.920 --> 00:30:32.520
John Locke, after Newton's demonstration&nbsp;
said well, this leaves us only with the&nbsp;&nbsp;

00:30:32.520 --> 00:30:40.080
possibility that thinking is some property&nbsp;
of organized matter. That's the 17th century.

00:30:40.080 --> 00:30:46.740
CRAIG
But the&nbsp;&nbsp;

00:30:46.740 --> 00:30:54.150
belief in a soul and consciousness as something&nbsp;
separate from the material biology, it persists.

00:30:54.150 --> 00:30:56.400
NOAM
People believe in all kinds of things,&nbsp;&nbsp;

00:30:56.400 --> 00:31:06.180
but within the rational part of the human&nbsp;
species, once Newton demonstrated that the&nbsp;&nbsp;

00:31:06.180 --> 00:31:14.100
mechanical model doesn't work, there's no material&nbsp;
universe in the only sense that was understood,&nbsp;&nbsp;

00:31:16.320 --> 00:31:22.500
the obvious conclusion was that since&nbsp;
matter, as Mr. Newton has demonstrated,&nbsp;&nbsp;

00:31:22.500 --> 00:31:29.700
has properties that we cannot conceive of,&nbsp;
they're not part of our intuitive picture.&nbsp;&nbsp;

00:31:29.700 --> 00:31:36.540
Since matter has those properties. Organized&nbsp;
matter can also have the property of thought.&nbsp;&nbsp;

00:31:37.260 --> 00:31:44.100
This was investigated all through the 18th&nbsp;
century ended up finally with Joseph Priestley,&nbsp;&nbsp;

00:31:45.240 --> 00:31:52.140
chemist, philosopher, late 18th century&nbsp;
gave pretty extensive discussions of how&nbsp;&nbsp;

00:31:52.980 --> 00:32:01.200
material organisms, material objects could have&nbsp;
properties of thought. You can even find it in&nbsp;&nbsp;

00:32:01.200 --> 00:32:09.780
Darwin's early books. It was kind of forgotten&nbsp;
after that, rediscovered in the late 20th century&nbsp;&nbsp;

00:32:09.780 --> 00:32:18.480
is some radical new discovery. Astonishing&nbsp;
hypothesis, matter can think. Of course it can,&nbsp;&nbsp;

00:32:19.260 --> 00:32:26.460
we're doing it right now. But the only problem&nbsp;
then is to find out what's involved and what&nbsp;&nbsp;

00:32:26.460 --> 00:32:33.720
we call thinking, what we call sentience. What are&nbsp;
the properties of whatever matter is we don't know&nbsp;&nbsp;

00:32:33.720 --> 00:32:41.520
what matter is, but whatever it turns out to be,&nbsp;
whatever constitutes the world - what physicists&nbsp;&nbsp;

00:32:41.520 --> 00:32:49.440
don't know, but whatever it is, the something&nbsp;
- organized elements of it can have various&nbsp;&nbsp;

00:32:49.440 --> 00:32:56.820
properties, like the properties that we are now&nbsp;
using, properties that we call sentience, then the&nbsp;&nbsp;

00:32:56.820 --> 00:33:03.600
question whether something else has sentience&nbsp;
is as interesting as whether airplanes fly?&nbsp;&nbsp;

00:33:04.680 --> 00:33:12.000
If you're talking English airplanes fly. If you're&nbsp;
talking Hebrew airplanes glide. They don't fly.&nbsp;&nbsp;

00:33:13.200 --> 00:33:20.180
It's not a it's not a substantive&nbsp;
question. Just what metaphors do we like.

00:33:20.180 --> 00:33:24.540
CRAIG
But what you're saying then is that&nbsp;&nbsp;

00:33:25.860 --> 00:33:35.820
neural nets may not be the engineering solution,&nbsp;
but that eventually, it may be possible to create&nbsp;&nbsp;

00:33:38.040 --> 00:33:45.150
a system outside of the human brain&nbsp;
that can think, whatever thinking is.

00:33:45.150 --> 00:33:46.020
NOAM
Can do what&nbsp;&nbsp;

00:33:46.020 --> 00:33:53.880
we call thinking, thinking how that whether it&nbsp;
thinks or not, is like asking do airplanes fly,&nbsp;&nbsp;

00:33:54.780 --> 00:34:03.120
not a substantive question. We shouldn't waste&nbsp;
time on questions that are completely meaningless.

00:34:03.920 --> 00:34:09.780
CRAIG
Going back to the history, then, you know,&nbsp;&nbsp;

00:34:09.780 --> 00:34:18.900
Minsky was very interested in the possibility of&nbsp;
nets, neural nets as a as a computational model,

00:34:19.560 --> 00:34:23.700
NOAM
In Minsky's time, it looked as if neural nets were&nbsp;&nbsp;

00:34:23.700 --> 00:34:30.960
the right place to look. Now, I think it's not so&nbsp;
obvious, especially because of Gallistel 's work,&nbsp;&nbsp;

00:34:31.680 --> 00:34:37.860
which is not accepted by most neuroscientists,&nbsp;
but seems to me pretty compelling.

00:34:37.860 --> 00:34:40.500
CRAIG
Can you talk a little bit about that&nbsp;&nbsp;

00:34:40.500 --> 00:34:45.420
because I haven't read that. And I, I'm guessing&nbsp;
our readers haven't our listeners haven't.

00:34:45.420 --> 00:34:50.100
NOAM
Gallistel is not the only one. Roger&nbsp;&nbsp;

00:34:50.100 --> 00:34:58.800
Penrose is another Nobel Prize winning physicist,&nbsp;
number of people have pointed out Gallistel mostly&nbsp;&nbsp;

00:34:58.800 --> 00:35:07.980
that have argued, I think, plausibly, that&nbsp;
the basic component of a computational system,&nbsp;&nbsp;

00:35:08.940 --> 00:35:15.600
the basic element of essentially a Turing&nbsp;
machine cannot be constructed from neural net.&nbsp;&nbsp;

00:35:16.620 --> 00:35:24.000
So, you have to look somewhere else with a&nbsp;
different form of computation. And he's also&nbsp;&nbsp;

00:35:24.000 --> 00:35:30.600
pointed out what in fact, is true that there's&nbsp;
much richer computational capacity in the brain&nbsp;&nbsp;

00:35:30.600 --> 00:35:41.760
than neural nets, even internal to a cell. There's&nbsp;
massive computational capacity, intracellular. So&nbsp;&nbsp;

00:35:41.760 --> 00:35:48.300
maybe that's involved in computation. And then&nbsp;
there's by now some experimental work, I think,&nbsp;&nbsp;

00:35:48.300 --> 00:35:57.780
given some evidence for this, but it's a problem&nbsp;
for neuroscientists to work on. You know, I'm not&nbsp;&nbsp;

00:35:57.780 --> 00:36:03.780
an expert in the field, I'm looking at it from the&nbsp;
outside. So don't take my opinion too seriously.&nbsp;&nbsp;

00:36:03.780 --> 00:36:10.200
But to me, it looks very compelling. But whatever&nbsp;
it is, neural nets or something else, there is&nbsp;&nbsp;

00:36:10.200 --> 00:36:17.340
some organization of them of whatever's there is&nbsp;
giving us the capacity to do what we're doing.

00:36:17.340 --> 00:36:19.980
NOAM
So, if you're a scientist, what you do is&nbsp;&nbsp;

00:36:21.120 --> 00:36:29.700
approach it in two different ways. One is, you try&nbsp;
to find the properties of the system. What is the&nbsp;&nbsp;

00:36:29.700 --> 00:36:36.480
nature of the system? That's first step, kind&nbsp;
of thing I was talking about before with stone,&nbsp;&nbsp;

00:36:37.080 --> 00:36:44.880
what are the properties of the system that&nbsp;
an infant automatically develops in the mind?&nbsp;&nbsp;

00:36:45.660 --> 00:36:50.220
And there's a lot of work on that. From&nbsp;
the other point of view, you can say,&nbsp;&nbsp;

00:36:50.220 --> 00:36:55.920
what can we learn about the brain that&nbsp;
relates to this? Actually, there is some work.&nbsp;&nbsp;

00:36:56.460 --> 00:37:07.380
So, there is neurophysiological neurophysiological&nbsp;
studies which have shown that there are artificial&nbsp;&nbsp;

00:37:07.380 --> 00:37:13.980
languages that violate the principle&nbsp;
that I mentioned, the structure dependent&nbsp;&nbsp;

00:37:13.980 --> 00:37:22.260
principle. If you train people on those, the&nbsp;
ordinary language centers don't function,&nbsp;&nbsp;

00:37:22.800 --> 00:37:27.960
you get diffuse functioning of the brain, which&nbsp;
means they're being treated as puzzles, basically.&nbsp;&nbsp;

00:37:28.980 --> 00:37:36.780
So, you can find some neurological correlates&nbsp;
of some of the things that are discovered by&nbsp;&nbsp;

00:37:36.780 --> 00:37:45.840
looking at the nature of the phenotype. It's&nbsp;
very hard for humans, for a number of reasons.

00:37:46.680 --> 00:37:48.000
NOAM
And we know a lot about human&nbsp;&nbsp;

00:37:48.000 --> 00:37:55.680
the physiology of human vision. But the reason is&nbsp;
because of invasive experiments with non-humans,&nbsp;&nbsp;

00:37:56.880 --> 00:38:04.500
cats, monkeys, and so on. Can't do that for&nbsp;
language. There aren’t any other organisms,&nbsp;&nbsp;

00:38:04.500 --> 00:38:12.000
it’s unique to humans. So, there's no comparative&nbsp;
studies, you can't do, you can think of a lot of&nbsp;&nbsp;

00:38:12.000 --> 00:38:18.960
invasive experiments, which teach you a lot. You&nbsp;
can't do them for ethical reasons. So, study of&nbsp;&nbsp;

00:38:19.620 --> 00:38:25.800
the neurophysiology of human&nbsp;
cognition is a uniquely hard problem.&nbsp;&nbsp;

00:38:26.580 --> 00:38:33.240
It’s in its basic elements like language,&nbsp;
it's just unique to the species.&nbsp;&nbsp;

00:38:34.080 --> 00:38:40.260
And in fact, a very recent development in&nbsp;
evolutionary history, probably the last couple&nbsp;&nbsp;

00:38:40.260 --> 00:38:48.660
100,000 years, which is nothing. So, you can't&nbsp;
do the invasive experiments for ethical reasons,&nbsp;&nbsp;

00:38:48.660 --> 00:38:53.760
you can think of them but can't do them,&nbsp;
fortunately. And there's no comparative&nbsp;&nbsp;

00:38:53.760 --> 00:39:02.520
evidence. So, it's much harder to do, you have to&nbsp;
do things like looking at blood flow in the brain,&nbsp;&nbsp;

00:39:03.180 --> 00:39:11.160
MRI kind of things, electrical stimulation,&nbsp;
looking from the outside stuff. It's not like&nbsp;&nbsp;

00:39:11.160 --> 00:39:16.560
doing the kinds of experiments you can&nbsp;
think of. So, it's very hard to find out&nbsp;&nbsp;

00:39:16.560 --> 00:39:24.360
the neurophysiological basis for things like&nbsp;
use of language, but it's one way to proceed.

00:39:24.360 --> 00:39:25.020
NOAM
And the&nbsp;&nbsp;

00:39:25.020 --> 00:39:31.200
other way to proceed is learn more about&nbsp;
the phenotype. It's like chemistry for&nbsp;&nbsp;

00:39:32.460 --> 00:39:40.800
hundreds of years. You just postulated the&nbsp;
existence of atoms. Nobody could see them,&nbsp;&nbsp;

00:39:40.800 --> 00:39:47.940
you know, why are they there? Because unless&nbsp;
there are atoms with Dalton's properties, you&nbsp;&nbsp;

00:39:47.940 --> 00:39:56.280
don't explain anything. Or early genetics. Early&nbsp;
genetics were before anybody had any idea what a&nbsp;&nbsp;

00:39:56.280 --> 00:40:04.320
gene is. You just looked at the properties of the&nbsp;
system, try to figure out what must be going on.&nbsp;&nbsp;

00:40:05.400 --> 00:40:12.900
That's the way astrophysics works. Most&nbsp;
of science works like that. This does too.

00:40:12.900 --> 00:40:14.160
CRAIG
When you&nbsp;&nbsp;

00:40:14.160 --> 00:40:22.320
talk about invasive exploration that there are&nbsp;
tools that are increasingly sophisticated. I'm&nbsp;&nbsp;

00:40:22.320 --> 00:40:29.040
thinking of Neuralink, Elon Musk's&nbsp;
startup that has these super fine&nbsp;&nbsp;

00:40:30.960 --> 00:40:38.760
electrodes that can be put into the brain&nbsp;
without damaging individual neurons.

00:40:38.760 --> 00:40:43.200
NOAM
There's actually I think,&nbsp;&nbsp;

00:40:43.200 --> 00:40:49.260
much more advanced than that is work that's&nbsp;
being done with patients under brain surgery.&nbsp;&nbsp;

00:40:50.880 --> 00:40:58.140
Under brain surgery because the brain is basically&nbsp;
exposed there are some noninvasive procedures&nbsp;&nbsp;

00:40:58.860 --> 00:41:06.780
that can be used to study what particular&nbsp;
parts of the brain, even what particular&nbsp;&nbsp;

00:41:06.780 --> 00:41:13.140
neurons are doing. It’s very delicate&nbsp;
work. But there is some work going on. One&nbsp;&nbsp;

00:41:14.100 --> 00:41:21.180
person is working on it is Andrea Moro, the same&nbsp;
person who designed the experiments that are&nbsp;&nbsp;

00:41:21.180 --> 00:41:27.560
described before about impossible languages.&nbsp;
That seems to me a promising direction.

00:41:27.560 --> 00:41:31.020
NOAM
There are other kinds of work that is If&nbsp;&nbsp;

00:41:32.160 --> 00:41:39.600
we could mention some that Alec Marantz at&nbsp;
NYU is doing interesting studies that have&nbsp;&nbsp;

00:41:40.140 --> 00:41:49.020
shed some light on the very elementary function&nbsp;
how do, how do words get stored in the brain&nbsp;&nbsp;

00:41:50.160 --> 00:41:59.880
what what's going on in the brain that tells us&nbsp;
that Blake is a possible word, but Bnake isn't for&nbsp;&nbsp;

00:41:59.880 --> 00:42:06.960
an English speaker. It is for Arabic speakers. And&nbsp;
what's going on in the brain that deals with that.&nbsp;&nbsp;

00:42:08.220 --> 00:42:19.620
Hard work. David Poeppel, another very good&nbsp;
neuroscientist has found evidence for things&nbsp;&nbsp;

00:42:19.620 --> 00:42:30.060
like free structure in the brain. But the&nbsp;
kinds of invasive experiments you can dream of&nbsp;&nbsp;

00:42:30.720 --> 00:42:36.960
,you can think of, he's just not allowed to do.&nbsp;
So, you have to try it in much indirect ways.

00:42:36.960 --> 00:42:39.900
CRAIG
Do you think that understanding&nbsp;&nbsp;

00:42:41.040 --> 00:42:44.400
cognition has advanced in your lifetime?&nbsp;&nbsp;

00:42:44.940 --> 00:42:52.380
And are you hopeful that we'll eventually&nbsp;
really understand how the brain thinks?

00:42:53.420 --> 00:42:54.420
NOAM&nbsp;

00:42:54.420 --> 00:43:00.720
Well, there's been vast improvement&nbsp;
in understanding the phenotype.&nbsp;&nbsp;

00:43:02.220 --> 00:43:07.200
That we know a great deal about that&nbsp;
was not known even a few years ago.&nbsp;&nbsp;

00:43:08.100 --> 00:43:16.980
There's been some progress in the neuroscience&nbsp;
that relates to it, but it's much harder.

00:43:18.180 --> 00:43:27.360
CRAIG
I'm just curious about where you are in not&nbsp;&nbsp;

00:43:27.360 --> 00:43:38.700
physically, you’re in Arizona, but where you are&nbsp;
in your thinking. Are you still pushing forward in&nbsp;&nbsp;

00:43:40.680 --> 00:43:50.100
trying to understand language in the brain? Or are&nbsp;
you sort of retired, so to speak at this point?

00:43:51.480 --> 00:43:52.680
NOAM
Very much&nbsp;&nbsp;

00:43:52.680 --> 00:43:56.220
involved in I mean, I don't&nbsp;
work on the neurophysiology.&nbsp;&nbsp;

00:43:58.620 --> 00:44:07.080
But I mentioned under Andrea Moro who is a good&nbsp;
friend. Alec Marantz. also a good friend, I follow&nbsp;&nbsp;

00:44:07.080 --> 00:44:15.060
the work they're doing, we interact, but my work&nbsp;
is just on the phenotype. What's the nature of the&nbsp;&nbsp;

00:44:15.060 --> 00:44:21.600
system? And there, I think we're learning a lot.&nbsp;
And we're in the middle of papers at the moment,&nbsp;&nbsp;

00:44:22.440 --> 00:44:31.080
looking at more subtle, complex properties. The&nbsp;
idea is essentially to find what I said about&nbsp;&nbsp;

00:44:31.080 --> 00:44:43.440
binary set formation, how can we show that from&nbsp;
the simplest computational procedures, we can&nbsp;&nbsp;

00:44:43.440 --> 00:44:54.180
account for the apparently complex and apparently&nbsp;
varied properties of the language systems. There's&nbsp;&nbsp;

00:44:54.180 --> 00:45:03.000
a fair amount of progress on that, unheard&nbsp;
of 20 or 30 years ago. So, this is all new.

00:45:03.000 --> 00:45:05.820
CRAIG
Understanding is one thing and then&nbsp;&nbsp;

00:45:07.140 --> 00:45:10.560
recreating it through computation&nbsp;&nbsp;

00:45:13.260 --> 00:45:21.120
in external hardware is another is that&nbsp;
a blind alley. Or do you think that that?

00:45:21.120 --> 00:45:21.960
NOAM&nbsp;

00:45:23.460 --> 00:45:28.740
Well, at the moment, I don't see any particular&nbsp;
point in it, if there is some point, okay.

00:45:30.300 --> 00:45:37.380
I mean, the kinds of things that we're&nbsp;
learning about the nature of language,&nbsp;&nbsp;

00:45:38.340 --> 00:45:46.500
I suppose you could construct some sort of&nbsp;
system that would duplicate them. But it doesn't&nbsp;&nbsp;

00:45:46.500 --> 00:45:56.100
seem any obvious point to it. It's like taking&nbsp;
chemistry and, 100 years ago and saying, can I&nbsp;&nbsp;

00:45:56.100 --> 00:46:07.080
construct models that will look sort of like a -
suppose you took a diagram for an organic molecule&nbsp;&nbsp;

00:46:07.860 --> 00:46:14.760
and studied its properties, you could&nbsp;
presumably construct a mechanical model&nbsp;&nbsp;

00:46:15.480 --> 00:46:21.780
that would do some of those things. Would it&nbsp;
be useful? Currently chemists didn't think so.&nbsp;&nbsp;

00:46:22.920 --> 00:46:27.480
If it would, okay, if it wouldn't, then don't.

00:46:27.480 --> 00:46:31.980
CRAIG
Nonetheless, I mean, we&nbsp;&nbsp;

00:46:31.980 --> 00:46:41.220
are using neural nets, even in this call. Do you&nbsp;
see? I mean, setting aside the question of whether&nbsp;&nbsp;

00:46:41.220 --> 00:46:50.040
or not they help understand, help us understand&nbsp;
anything about the brain? Are you excited at all&nbsp;&nbsp;

00:46:50.040 --> 00:46:58.860
in about the promise that these large models hold?&nbsp;
I mean because they do something very useful.

00:46:58.860 --> 00:47:03.300
NOAM
They are. Like I said, I'm using it right now.&nbsp;&nbsp;

00:47:04.140 --> 00:47:10.860
I think it's fine for me. Somebody who can't&nbsp;
hear to be able to read what you're saying.&nbsp;&nbsp;

00:47:11.580 --> 00:47:19.260
Yeah, pretty accurately. That's an achievement.&nbsp;
Great. That's indulgence. Technology.

00:47:19.260 --> 00:47:25.440
CRAIG
Who do you think is, is going to carry on&nbsp;&nbsp;

00:47:27.060 --> 00:47:30.480
your work from here? I mean,&nbsp;&nbsp;

00:47:30.480 --> 00:47:36.480
are there any students of yours who you&nbsp;
think we should be paying attention to?

00:47:37.640 --> 00:47:38.640
NOAM&nbsp;

00:47:38.640 --> 00:47:46.260
Quite a lot. There’s a lot of young people&nbsp;
doing fine work working closely with a&nbsp;&nbsp;

00:47:47.040 --> 00:47:55.080
small research group. By now spread all over&nbsp;
the world, we meet virtually from Japan,&nbsp;&nbsp;

00:47:55.080 --> 00:48:02.280
and other places recently working on the&nbsp;
kinds of problems I was talking about.&nbsp;&nbsp;

00:48:04.080 --> 00:48:09.240
Right now, I should say it's a pretty special&nbsp;
interest. Most linguists aren't interested in&nbsp;&nbsp;

00:48:09.240 --> 00:48:17.520
these foundational questions. But I think that's,&nbsp;
happens to be my interest, I want to see if we can&nbsp;&nbsp;

00:48:18.060 --> 00:48:25.560
show that, ultimately try to show that&nbsp;
language is essentially a natural object.

00:48:26.880 --> 00:48:30.360
NOAM
And there was an interesting paper,&nbsp;&nbsp;

00:48:31.320 --> 00:48:40.800
written about the time that I started working&nbsp;
on this by Albert Einstein in 1950. An article&nbsp;&nbsp;

00:48:40.800 --> 00:48:47.640
in Scientific American, which I read, but didn't&nbsp;
appreciate at the time, began to appreciate later,&nbsp;&nbsp;

00:48:48.360 --> 00:48:51.540
in which he talked about what&nbsp;
he called the miracle creed.&nbsp;&nbsp;

00:48:52.380 --> 00:49:03.000
It has an interesting history. Goes back to&nbsp;
Galileo. Galileo had a maxim saying, nature is&nbsp;&nbsp;

00:49:03.000 --> 00:49:08.820
simple. It doesn't do things in a complicated&nbsp;
way, if it could do them in a simple way.&nbsp;&nbsp;

00:49:09.420 --> 00:49:16.560
That’s Galileo’s maxim. Couldn't prove it.&nbsp;
But he said, I think that's the way it is.&nbsp;&nbsp;

00:49:16.560 --> 00:49:21.780
And it's the task of the scientists&nbsp;
to prove it. Well, over the centuries,&nbsp;&nbsp;

00:49:22.440 --> 00:49:30.180
it's been substantiated. Case after case. It&nbsp;
shows up in Leibniz’s principle of optimality.&nbsp;&nbsp;

00:49:30.180 --> 00:49:36.180
But by then there was a lot of evidence for&nbsp;
it. By now it's just a norm for science.&nbsp;&nbsp;

00:49:37.080 --> 00:49:44.820
It is what Einstein called the miracle creed.&nbsp;
Nature is simple. Our task is to show it.&nbsp;&nbsp;

00:49:44.820 --> 00:49:53.460
Can’t prove it. Skeptic and say I don't believe&nbsp;
it. Okay. But that's the way science works.

00:49:53.460 --> 00:49:56.880
NOAM
Well, the science works the same way&nbsp;&nbsp;

00:49:56.880 --> 00:50:04.560
for language. But I couldn't have proposed that&nbsp;
50 years ago, 20 years ago, I think now you can,&nbsp;&nbsp;

00:50:05.580 --> 00:50:13.140
that maybe language is just basically a&nbsp;
perfect computational system. At its base.&nbsp;&nbsp;

00:50:14.040 --> 00:50:21.360
You look at the phenomenon doesn't look like that.&nbsp;
But the same was true of biology. Go back to the&nbsp;&nbsp;

00:50:21.360 --> 00:50:32.400
1950s 1960s. Biologists assume that organisms&nbsp;
could vary so widely that each one has to be&nbsp;&nbsp;

00:50:32.400 --> 00:50:41.280
studied on its own without bias. By now that's all&nbsp;
forgotten. It’s recognized that since the Cambrian&nbsp;&nbsp;

00:50:41.280 --> 00:50:48.720
explosion, there's virtually no variation in the&nbsp;
kinds of organisms, fundamentally all the same&nbsp;&nbsp;

00:50:49.260 --> 00:50:56.460
deep homologies, and so on. So, even been&nbsp;
proposed that there's a universal genome,&nbsp;&nbsp;

00:50:57.300 --> 00:51:01.380
not totally accepted, but&nbsp;
not considered ridiculous.

00:51:01.380 --> 00:51:03.120
NOAM
Well, I think we're&nbsp;&nbsp;

00:51:03.120 --> 00:51:08.520
moving in the same direction with the study of&nbsp;
language. Now, let me say, again, there's not many&nbsp;&nbsp;

00:51:08.520 --> 00:51:15.540
linguists interested in this. Most linguists, like&nbsp;
most biologists are studying particular things,&nbsp;&nbsp;

00:51:15.540 --> 00:51:23.520
which is fine, you learn a lot that way. But&nbsp;
I think it is possible now to formulate a&nbsp;&nbsp;

00:51:24.360 --> 00:51:33.180
plausible thesis that language is a natural object&nbsp;
like others, which evolved in such ways to have&nbsp;&nbsp;

00:51:33.180 --> 00:51:39.840
perfect design, but to be highly dysfunctional,&nbsp;
because that's true of natural objects, generally,&nbsp;&nbsp;

00:51:39.840 --> 00:51:45.960
it's part of the nature of evolution, which&nbsp;
doesn't take into account possible functions.

00:51:48.120 --> 00:51:53.520
NOAM
In the last stage of evolution, the reproductive&nbsp;&nbsp;

00:51:53.520 --> 00:52:00.120
success that does take function into account,&nbsp;
natural selection. That's a fringe of evolution,&nbsp;&nbsp;

00:52:00.900 --> 00:52:09.000
just peripheral fringe, very important&nbsp;
not denigrated. But it's the basic part&nbsp;&nbsp;

00:52:09.000 --> 00:52:16.020
of evolution is constructing the optimal system&nbsp;
that meets the physical conditions established&nbsp;&nbsp;

00:52:16.020 --> 00:52:23.400
by some disruption in the system. That's&nbsp;
the core of evolution. What Turing studied,&nbsp;&nbsp;

00:52:25.440 --> 00:52:28.680
Darcy Thompson others by now I think it's&nbsp;&nbsp;

00:52:29.700 --> 00:52:34.860
understood. And I think maybe the&nbsp;
study of this particular biology,&nbsp;&nbsp;

00:52:35.520 --> 00:52:42.180
language is a biological object. So why should&nbsp;
it be different? Let's see if we can show it.

00:52:42.180 --> 00:52:44.580
CRAIG
There's been a lot of talk in the news&nbsp;&nbsp;

00:52:44.580 --> 00:52:54.060
recently about, you know, extraterrestrial craft&nbsp;
having been found by the government. And you know,&nbsp;&nbsp;

00:52:54.060 --> 00:53:01.860
I don't put much stock in it. But imagine that&nbsp;
there is an extraterrestrial life, advanced&nbsp;&nbsp;

00:53:02.700 --> 00:53:12.120
forms of life. Do you think that their language&nbsp;
would have developed the same way if it's based&nbsp;&nbsp;

00:53:13.320 --> 00:53:19.800
on these simple principles? Or is it? Could&nbsp;
there be other forms of language in other&nbsp;&nbsp;

00:53:19.800 --> 00:53:28.560
biological organisms that would be quote&nbsp;
unquote, impossible, in the human context,

00:53:28.560 --> 00:53:34.140
NOAM
Back around the 1960s, I guess, Minsky&nbsp;&nbsp;

00:53:36.000 --> 00:53:42.840
studied with one of his students, Daniel&nbsp;
Bobrow, studied the simplest Turing machines,&nbsp;&nbsp;

00:53:43.560 --> 00:53:52.200
fewest states, fewest symbols, and asked&nbsp;
what happens if you just let them run free?&nbsp;&nbsp;

00:53:53.580 --> 00:54:00.540
Well turned out that most of them&nbsp;
crash, either get into endless loops or&nbsp;&nbsp;

00:54:01.620 --> 00:54:09.180
just crash, don't proceed. But the ones that&nbsp;
didn't crash or produce the successor function.&nbsp;&nbsp;

00:54:10.560 --> 00:54:19.440
So, he suggested, what we're going to find if&nbsp;
any kind of intelligence develops is that will&nbsp;&nbsp;

00:54:19.440 --> 00:54:26.880
be based on the successor function. And if we want&nbsp;
to try to communicate with some extraterrestrial&nbsp;&nbsp;

00:54:26.880 --> 00:54:33.900
intelligence, we should first see if they have&nbsp;
the successor function and maybe build up from&nbsp;&nbsp;

00:54:33.900 --> 00:54:43.860
there. Well, turns out a successor happens to be&nbsp;
what you get from the simplest possible language.&nbsp;&nbsp;

00:54:44.940 --> 00:54:52.320
The language is one symbol. And the simplest form&nbsp;
of binary set formation basically gives you the&nbsp;&nbsp;

00:54:52.320 --> 00:54:57.300
successor function. Add a little bit more&nbsp;
to it, you get something like original to&nbsp;&nbsp;

00:54:57.960 --> 00:55:02.880
add a little bit more to it, you get something&nbsp;
like the core properties of language.&nbsp;&nbsp;

00:55:03.960 --> 00:55:10.380
So, it's conceivable that if there&nbsp;
is any extraterrestrial intelligence,&nbsp;&nbsp;

00:55:10.380 --> 00:55:16.320
it would have pursued the same course. Where&nbsp;
it goes from there, we don't know enough to say

00:55:16.320 --> 00:55:21.060
CRAIG
And back to the idea that there&nbsp;&nbsp;

00:55:21.060 --> 00:55:28.020
is no supernatural realm, that the consciousness&nbsp;
is, is an emergent property from the physical&nbsp;&nbsp;

00:55:30.120 --> 00:55:36.420
attributes of the brain,&nbsp;
do you believe in a higher&nbsp;&nbsp;

00:55:37.320 --> 00:55:42.960
intelligence behind the creation&nbsp;
or continuation of the universe?

00:55:44.360 --> 00:55:49.740
NOAM
I don't see any point in vacuous hypotheses.&nbsp;&nbsp;

00:55:51.540 --> 00:55:55.320
If you want to believe it.&nbsp;
Okay. It has no consequences.

00:55:55.320 --> 00:55:59.400
CRAIG
So yeah, yeah. But do you believe it?

00:55:59.400 --> 00:56:01.800
NOAM
No. I&nbsp;&nbsp;

00:56:01.800 --> 00:56:07.680
don't see any point in believing things for&nbsp;
which there's no evidence and do no work.

00:56:07.680 --> 00:56:10.620
CRAIG
Yeah. And another thing I've&nbsp;&nbsp;

00:56:10.620 --> 00:56:21.120
always wanted to ask someone like you, clearly,&nbsp;
your intelligence surpasses most people's.

00:56:21.120 --> 00:56:23.100
NOAM
I don't think so

00:56:23.100 --> 00:56:25.260
CRAIG
Well, that's interesting&nbsp;&nbsp;

00:56:25.260 --> 00:56:35.340
that you say that. You think is just a matter of&nbsp;
applying yourself to study throughout your career.

00:56:37.640 --> 00:56:38.640
NOAM&nbsp;

00:56:38.640 --> 00:56:48.780
I have certain talents I know. Like, not believing&nbsp;
things just because people believe them. And&nbsp;&nbsp;

00:56:48.780 --> 00:56:57.840
keeping an open mind and looking for arguments and&nbsp;
evidence, not anything we've been talking about.&nbsp;&nbsp;

00:56:57.840 --> 00:57:08.100
When meaningless questions are proposed, like, are&nbsp;
other organisms sentient or do submarine swim? I&nbsp;&nbsp;

00:57:08.100 --> 00:57:15.900
say let's discard them and look at meaningful&nbsp;
questions. If you just pursued common sense,&nbsp;&nbsp;

00:57:15.900 --> 00:57:21.840
like then I think you can make some progress. Same&nbsp;
on the questions we're talking about language.&nbsp;&nbsp;

00:57:22.560 --> 00:57:30.540
If you think it through, there's every reason why&nbsp;
the organic object language should be an object.&nbsp;&nbsp;

00:57:31.080 --> 00:57:39.240
If so, it should follow the general principles&nbsp;
of evolution, which satisfy what Einstein called&nbsp;&nbsp;

00:57:39.240 --> 00:57:46.740
the miracle creed. So why shouldn't language. So&nbsp;
let's pursue that? See how far we can go. I think&nbsp;&nbsp;

00:57:46.740 --> 00:57:52.820
that's just common sense. Many people think&nbsp;
it's superior intelligence. I don't think so.

00:57:52.820 --> 00:57:53.820
CRAIG&nbsp;

00:57:53.820 --> 00:57:57.780
That’s it for this episode. I&nbsp;
want to thank Noam for his time.&nbsp;&nbsp;

00:57:58.500 --> 00:58:11.700
If you’d like a transcript of this conversation,&nbsp;
you can find one on our website, eye-on.ai. In the&nbsp;&nbsp;

00:58:11.700 --> 00:58:21.660
meantime, the Singularity may not be near, but AI&nbsp;
is about to change your world. So, pay attention.

