WEBVTT
Kind: captions
Language: en

00:00:00.080 --> 00:00:03.240
Even if you train a system to have a world model&nbsp;
that can predict what's going to happen next,&nbsp;&nbsp;

00:00:03.240 --> 00:00:07.560
the world is really complicated and there's&nbsp;
probably all kinds of situations that the system&nbsp;&nbsp;

00:00:07.560 --> 00:00:13.480
hasn't been trained on and need to, you know,&nbsp;
fine-tune itself as it goes. The question of how&nbsp;&nbsp;

00:00:13.480 --> 00:00:18.960
we organize AI research going forward, which is&nbsp;
somewhat determined by how afraid people are of&nbsp;&nbsp;

00:00:18.960 --> 00:00:24.400
the consequences of AI. So if you have a rather&nbsp;
positive view of the impact of AI on society and&nbsp;&nbsp;

00:00:24.400 --> 00:00:29.240
you trust humanity and society and democracies&nbsp;
to use it in good ways, then the best way to make&nbsp;&nbsp;

00:00:29.240 --> 00:00:31.560
progress is to open research.&nbsp;
&nbsp;

00:00:31.560 --> 00:00:37.560
AI might be the most important new computer&nbsp;
technology ever. It's storming every industry&nbsp;&nbsp;

00:00:37.560 --> 00:00:43.400
and literally billions of dollars are being&nbsp;
invested, so buckle up. The problem is that&nbsp;&nbsp;

00:00:43.400 --> 00:00:49.680
AI needs a lot of speed and processing power.&nbsp;
So how do you compete without cost spiraling&nbsp;&nbsp;

00:00:49.680 --> 00:00:56.240
out of control? It's time to upgrade to&nbsp;
the next generation of the cloud Oracle&nbsp;&nbsp;

00:00:56.240 --> 00:01:03.960
Cloud Infrastructure, or OCI. OCI is a single&nbsp;
platform for your infrastructure, database,&nbsp;&nbsp;

00:01:03.960 --> 00:01:12.480
application development and AI needs. OCI has&nbsp;
four to eight times the bandwidth of other clouds,&nbsp;&nbsp;

00:01:12.480 --> 00:01:18.240
offers one consistent price instead of&nbsp;
variable regional pricing and, of course,&nbsp;&nbsp;

00:01:18.240 --> 00:01:25.360
nobody does data better than Oracle. So now you&nbsp;
can train your AI models at twice the speed and&nbsp;&nbsp;

00:01:25.360 --> 00:01:32.880
less than half the cost of other clouds. If you&nbsp;
want to do more and spend less, like Uber 8x8,&nbsp;&nbsp;

00:01:33.880 --> 00:01:46.040
and Databricks Mosaic, take a free test drive of&nbsp;
OCI at oracle.com/eyeonai. That's E-Y-E-O-N-A-I&nbsp;&nbsp;

00:01:46.040 --> 00:01:52.000
all run together oracle.com/eyeonai.&nbsp;
&nbsp;

00:01:52.000 --> 00:01:59.480
Hi, I'm Craig Smith. This is Eye on AI. In&nbsp;
this episode, I speak again with Yann LeCun,&nbsp;&nbsp;

00:01:59.480 --> 00:02:05.960
one of the founders of deep learning and someone&nbsp;
who followers of AI should need no introduction&nbsp;&nbsp;

00:02:05.960 --> 00:02:13.720
to. Yann talks about his work on developing world&nbsp;
models, on why he does not believe AI research&nbsp;&nbsp;

00:02:13.720 --> 00:02:21.480
poses a threat to humanity and why he thinks open&nbsp;
source AI models are the future. In the course of&nbsp;&nbsp;

00:02:21.480 --> 00:02:29.920
the conversation we talk about a new model Gaia&nbsp;
I, developed by a company called Wayve.AI. I'll&nbsp;&nbsp;

00:02:29.920 --> 00:02:36.200
have an episode with Wayve's founder to further&nbsp;
explore that world model, which has produced&nbsp;&nbsp;

00:02:36.200 --> 00:02:43.320
some startling results. I hope you find the&nbsp;
conversation with Yann as enlightening as I did.

00:02:43.320 --> 00:02:48.720
I mean, first, the notion of a world model&nbsp;
is the idea that the system would get some&nbsp;&nbsp;

00:02:48.720 --> 00:02:54.200
idea of the state of the world and be able to&nbsp;
predict the sort of following states of the&nbsp;&nbsp;

00:02:54.200 --> 00:02:58.800
world resulting from just the natural evolution&nbsp;
of the world or resulting from an action that&nbsp;&nbsp;

00:02:58.800 --> 00:03:04.920
the agent might take. If you have an idea&nbsp;
of the state of the world and you imagine&nbsp;&nbsp;

00:03:04.920 --> 00:03:11.360
an action that you're going to take and you&nbsp;
can predict the resulting state of the world,&nbsp;&nbsp;

00:03:11.360 --> 00:03:14.880
then that means you can predict what's going to&nbsp;
happen as a consequence of a sequence of actions.&nbsp;&nbsp;

00:03:14.880 --> 00:03:20.400
That means you can plan a sequence of actions to&nbsp;
arrive at a particular goal. That's really what&nbsp;&nbsp;

00:03:20.400 --> 00:03:28.640
a world model is. At least that's what the Wayve&nbsp;
people have understood the word in other contexts,&nbsp;&nbsp;

00:03:28.640 --> 00:03:34.320
like in the context of optimal control&nbsp;
and robotics and things like that. That's&nbsp;&nbsp;

00:03:34.320 --> 00:03:38.720
what a world model is. Now there's several&nbsp;
levels of complexity of those world models,&nbsp;&nbsp;

00:03:38.720 --> 00:03:45.440
whether they model yourself, the agent, or whether&nbsp;
they model the external world, which is much more&nbsp;&nbsp;

00:03:45.440 --> 00:03:56.320
complicated. Training a world model basically&nbsp;
consists in just observing the world go by and&nbsp;&nbsp;

00:03:56.320 --> 00:04:01.680
then learning to predict what's going to happen&nbsp;
next, or observing the world taking an action and&nbsp;&nbsp;

00:04:01.680 --> 00:04:08.320
then observing the resulting effect, an action that&nbsp;
you take as an agent or an action that you see&nbsp;&nbsp;

00:04:08.320 --> 00:04:16.960
other agents taking. That establishes causality.&nbsp;
Essentially, you could think of this as a causal&nbsp;&nbsp;

00:04:16.960 --> 00:04:24.200
model. Those models don't need to predict all the&nbsp;
details about the world, they don't need to be&nbsp;&nbsp;

00:04:24.200 --> 00:04:31.640
generative, they don't need to predict exactly&nbsp;
every pixels in a video, for example, because&nbsp;&nbsp;

00:04:31.640 --> 00:04:37.320
what you need to be able to predict is enough&nbsp;
details, some sort of abstract representation,&nbsp;&nbsp;

00:04:37.320 --> 00:04:49.960
to allow you to plan. You're assembling something&nbsp;
out of wood and you're going to put two planks&nbsp;&nbsp;

00:04:49.960 --> 00:04:56.800
together and attach them with screws. It doesn't&nbsp;
matter the details of which type of screwdriver&nbsp;&nbsp;

00:04:56.800 --> 00:05:01.920
you're using or the size of the screw within some&nbsp;
limits and things like that. There are details&nbsp;&nbsp;

00:05:01.920 --> 00:05:08.920
that in the end don't matter as to what the end&nbsp;
result will be or the precise grain of the wood&nbsp;&nbsp;

00:05:08.920 --> 00:05:14.400
and things of that type. You need to have some&nbsp;
abstract level of representation within which you&nbsp;&nbsp;

00:05:14.400 --> 00:05:21.520
can make the prediction without having to predict&nbsp;
every detail. That's why those JEPA architectures&nbsp;&nbsp;

00:05:21.520 --> 00:05:29.880
I've been advocating are useful. Models like the&nbsp;
Gaia 1 model from Wayve actually make predictions&nbsp;&nbsp;

00:05:29.880 --> 00:05:34.880
in an abstract representation space. There's been&nbsp;
a lot of work in that area for years, also at&nbsp;&nbsp;

00:05:34.880 --> 00:05:41.960
FAIR (Facebook AI Research), but generally the abstract representations&nbsp;were pre-trained. So, the encoders that would take&nbsp;&nbsp;

00:05:41.960 --> 00:05:47.600
images from videos and then encode them into some&nbsp;
representation were trained in some other way. The&nbsp;&nbsp;

00:05:47.600 --> 00:05:53.280
progress we've made over the last six months in&nbsp;
self-supervised learning for images and video&nbsp;&nbsp;

00:05:53.280 --> 00:06:00.480
is that now we can train the entire system to make&nbsp;
those predictions simultaneously. We have systems&nbsp;&nbsp;

00:06:00.480 --> 00:06:10.640
now that can learn good representations of images.&nbsp;
The basic idea is very simple. You take an image,&nbsp;&nbsp;

00:06:10.640 --> 00:06:17.000
you run it through an encoder, then you&nbsp;
corrupt that image. You mask parts of it,&nbsp;&nbsp;

00:06:17.000 --> 00:06:24.720
for example, or you transform it in various ways.&nbsp;
You blur it, you change the colors, you change the&nbsp;&nbsp;

00:06:24.720 --> 00:06:29.560
framing a little bit and you run that corrupted&nbsp;
image through the same encoder or something&nbsp;&nbsp;

00:06:29.560 --> 00:06:36.560
very similar, and then you train the encoder to&nbsp;
predict the features of the complete image from&nbsp;&nbsp;

00:06:36.560 --> 00:06:47.200
the features of the corrupted one. You're not&nbsp;
trying to reconstruct the perfect image, you're&nbsp;&nbsp;

00:06:47.200 --> 00:06:52.760
just trying to predict the representation of it,&nbsp;
and this is different. This is not generative in&nbsp;&nbsp;

00:06:52.760 --> 00:06:57.960
the sense that it does not produce pixels, and&nbsp;
that's the secret to getting self-supervised learning to&nbsp;&nbsp;

00:06:57.960 --> 00:07:04.120
work in the context of images and video. You don't&nbsp;
want to be predicting pixels. It doesn't work. You&nbsp;&nbsp;

00:07:04.120 --> 00:07:08.320
can produce pixels as an afterthought, which&nbsp;
is what the Gaia system is doing by sticking a&nbsp;&nbsp;

00:07:08.320 --> 00:07:13.240
decoder on it and with some diffusion models that&nbsp;
will produce a nice image. But that's kind of a&nbsp;&nbsp;

00:07:13.240 --> 00:07:19.640
second step. If you train the system by predicting&nbsp;
pixels, you just don't get good representations,&nbsp;&nbsp;

00:07:19.640 --> 00:07:24.640
you don't get good predictions, you get&nbsp;
blurry predictions most of the time. So&nbsp;&nbsp;

00:07:24.640 --> 00:07:31.280
that's what makes learning from images and video&nbsp;
fundamentally different from learning from text,&nbsp;&nbsp;

00:07:31.280 --> 00:07:37.640
because in text you don't have that problem. It's&nbsp;
easy to predict words, even if you cannot make a&nbsp;&nbsp;

00:07:37.640 --> 00:07:43.960
perfect prediction, because language is discrete.&nbsp;
So language is simple compared to the real world.

00:07:43.960 --> 00:07:52.640
And there's a lot written right&nbsp;
now about the energy required&nbsp;&nbsp;

00:07:52.640 --> 00:07:59.560
in the computational resources, the&nbsp;
GPUs required to train language&nbsp;&nbsp;

00:07:59.560 --> 00:08:07.360
models. Is it less in training a world&nbsp;
model, like using I-JEPA architecture?

00:08:07.360 --> 00:08:13.760
Well, it's hard to tell because there&nbsp;
is no equivalent training procedure,&nbsp;&nbsp;

00:08:13.760 --> 00:08:15.840
self-supervised training procedure for video,&nbsp;&nbsp;

00:08:15.840 --> 00:08:22.920
for example, that does not use JEPA. The&nbsp;
ones that are generative don't really work.

00:08:22.920 --> 00:08:30.840
Yeah, yeah Well, but this architecture could&nbsp;
also be applied to language, couldn't it?

00:08:30.840 --> 00:08:36.840
Oh yeah, absolutely yeah. So you could&nbsp;
very well use a JEPA architecture that&nbsp;&nbsp;

00:08:36.840 --> 00:08:41.400
makes predictions in representation&nbsp;
space and apply it to language.

00:08:41.400 --> 00:08:47.440
Yeah, definitely, and in that case&nbsp;
would it be less computationally&nbsp;&nbsp;

00:08:47.440 --> 00:08:54.080
intense than training a large&nbsp;
language model. It's possible.

00:08:54.080 --> 00:09:00.080
It's not entirely clear either. I mean, there&nbsp;
is some advantage, regardless of what technique&nbsp;&nbsp;

00:09:00.080 --> 00:09:05.760
you're using, to making those models really big.&nbsp;
They just seem to work better if you make them&nbsp;&nbsp;

00:09:05.760 --> 00:09:15.240
big. So if you make them bigger, right. So scaling&nbsp;
is useful. Contrary to some claims, I do not&nbsp;&nbsp;

00:09:15.240 --> 00:09:20.760
believe that scaling is sufficient. So, in other&nbsp;
words, we're not going to get anywhere close to&nbsp;&nbsp;

00:09:20.760 --> 00:09:32.040
human level AI, in fact, not even animal level&nbsp;
AI, by simply scaling up language models, even&nbsp;&nbsp;

00:09:32.040 --> 00:09:36.320
multi-model language models that we apply to&nbsp;
video. We're going to have to find new concepts,&nbsp;&nbsp;

00:09:36.320 --> 00:09:44.080
new architectures and I've written a vision&nbsp;
paper about this a while back of a different&nbsp;&nbsp;

00:09:44.080 --> 00:09:51.600
type of architecture that would be necessary for&nbsp;
this. So scaling is necessary, but not sufficient,&nbsp;&nbsp;

00:09:52.120 --> 00:10:01.600
and we're missing some basic ingredients to get to&nbsp;
human level AI. We're fooled by the fact that LLMs&nbsp;&nbsp;

00:10:01.600 --> 00:10:06.440
are fluent and so we think that they have human&nbsp;
level intelligence because they can manipulate&nbsp;&nbsp;

00:10:06.440 --> 00:10:17.920
language, but that's false and in fact, there's a&nbsp;
very good symptom for this, which is that we have&nbsp;&nbsp;

00:10:17.920 --> 00:10:25.880
systems that can pass the bar exam, but answering&nbsp;
questions from text by basically regurgitating&nbsp;&nbsp;

00:10:25.880 --> 00:10:33.920
what they've learned more or less by rote, but we&nbsp;
don't have completely autonomous level five self&nbsp;&nbsp;

00:10:33.920 --> 00:10:40.760
driving cars, or at least no system that can learn&nbsp;
to do this in about 20 hours of practice just like&nbsp;&nbsp;

00:10:40.760 --> 00:10:48.480
any 17-year-old, and we certainly don't have any&nbsp;
domestic robot that can clear up the dinner table&nbsp;&nbsp;

00:10:48.480 --> 00:10:54.040
and fill up the dishwasher - at task that any&nbsp;
10-year-old can learn in one shot. So clearly&nbsp;&nbsp;

00:10:54.040 --> 00:10:59.720
we're missing something big, and that something&nbsp;
is an ability to learn how the world works and&nbsp;&nbsp;

00:10:59.720 --> 00:11:06.000
the world is much more complicated than language&nbsp;
and also being able to plan and reason. Basically&nbsp;&nbsp;

00:11:06.000 --> 00:11:12.040
having a mental world model of what goes on&nbsp;
that allows us to plan and predict consequences&nbsp;&nbsp;

00:11:12.040 --> 00:11:18.280
of actions. That's what we're missing. And it's&nbsp;
going to take a while before we figure this out.

00:11:18.280 --> 00:11:30.080
You were on another paper that talked&nbsp;
about augmented language models and the&nbsp;&nbsp;

00:11:30.080 --> 00:11:36.520
embodied Turing test. Was that the same&nbsp;
paper? The embodied Turing test? Can you&nbsp;&nbsp;

00:11:36.520 --> 00:11:41.120
talk about that? First of all, what is&nbsp;
the embodied Turing test? I didn't.

00:11:41.120 --> 00:11:48.880
I didn't quite understand that. Well,&nbsp;
okay, it's, it's a different concept,&nbsp;&nbsp;

00:11:48.880 --> 00:11:56.760
but it's basically the idea that you it's&nbsp;
based on the, on the, the Moravec paradox,&nbsp;&nbsp;

00:11:56.760 --> 00:12:03.600
right? So Moravec many years ago noticed&nbsp;
that things that appeared difficult for&nbsp;&nbsp;

00:12:03.600 --> 00:12:09.560
humans turned out to sometimes be very easy&nbsp;
for computers to do, like playing chess,&nbsp;&nbsp;

00:12:09.560 --> 00:12:13.600
much better than humans, or, I don't&nbsp;
know, computing integrals or whatever,&nbsp;&nbsp;

00:12:13.600 --> 00:12:19.760
certainly doing arithmetics. But then there are&nbsp;
things that we take for granted as humans that we&nbsp;&nbsp;

00:12:19.760 --> 00:12:25.320
don't even consider them intelligent tasks that we&nbsp;
are incapable of reproducing with computers. And&nbsp;&nbsp;

00:12:25.320 --> 00:12:30.600
so that's where the embodied Turing test comes&nbsp;
in. Like you know, observe what a cat can do,&nbsp;&nbsp;

00:12:30.600 --> 00:12:39.400
or how fast a cat can learn new, new, new tricks.&nbsp;
Or you know how a cat can plan to jump on,&nbsp;&nbsp;

00:12:39.400 --> 00:12:45.360
you know, a bunch of different furniture to get&nbsp;
to the top of wherever it wants to go. That's an&nbsp;&nbsp;

00:12:45.360 --> 00:12:52.560
amazing feat that we can't reproduce with robots&nbsp;
today. So that's kind of the embodied Turing test,&nbsp;&nbsp;

00:12:52.560 --> 00:13:00.120
if you want, like you know, can you make a&nbsp;
robot that can behave, have behaviors that are&nbsp;&nbsp;

00:13:00.120 --> 00:13:06.320
indistinguishable from those of animals first of&nbsp;
all, and can acquire new ones, at the same, with the&nbsp;&nbsp;

00:13:06.320 --> 00:13:15.840
same efficiency as animals. Then the augmented LLM&nbsp;
paper is different. It's about how do you sort of&nbsp;&nbsp;

00:13:15.840 --> 00:13:22.320
minimally change large language models so that&nbsp;
they can use tools, so they can to some extent&nbsp;&nbsp;

00:13:22.320 --> 00:13:27.080
plan actions? Like you know, you need to compute&nbsp;
the product of two numbers, right, you just call&nbsp;&nbsp;

00:13:27.080 --> 00:13:31.880
a calculator and you know you're going to get&nbsp;
the product of those two numbers. And LLMs are&nbsp;&nbsp;

00:13:31.880 --> 00:13:37.280
notoriously bad for arithmetics, so they need to do&nbsp;
this kind of stuff or do a search, you know, using&nbsp;&nbsp;

00:13:37.280 --> 00:13:42.320
a search engine or database look up, or something&nbsp;
like that. So there's a lot of work on this right&nbsp;&nbsp;

00:13:42.320 --> 00:13:46.880
now and it's somewhat incremental, like you know.&nbsp;
How can you sort of minimally change LLM and&nbsp;&nbsp;

00:13:46.880 --> 00:13:54.560
take advantage of their current capabilities but&nbsp;
still augment them with the ability to use tools?

00:13:54.560 --> 00:14:02.880
Yeah, and I don't want to get too much into&nbsp;
the threat debate, but you know you're on&nbsp;&nbsp;

00:14:02.880 --> 00:14:09.280
one side. Your colleagues Geoff and Yoshua are&nbsp;
on the other. I recently saw a picture of the&nbsp;&nbsp;

00:14:09.280 --> 00:14:17.280
three of you. I think you put that up on social&nbsp;
media saying how you know you can disagree but&nbsp;&nbsp;

00:14:17.280 --> 00:14:27.040
still be friends. This idea of augmenting&nbsp;
language models with stronger reasoning&nbsp;&nbsp;

00:14:27.040 --> 00:14:34.960
capabilities and the ability and agency, the&nbsp;
ability to use tools, is precisely what Geoff&nbsp;&nbsp;

00:14:34.960 --> 00:14:43.160
and Yoshua are worried about. Can you just&nbsp;
get into why are you not worried about that?

00:14:43.160 --> 00:14:49.560
Okay, so first, first of all, this is&nbsp;
not necessarily what you're describing,&nbsp;&nbsp;

00:14:49.560 --> 00:14:57.160
is not necessarily what they are afraid&nbsp;
of. They, they, they are alerting people&nbsp;&nbsp;

00:14:57.160 --> 00:15:03.600
and various governments and others about various&nbsp;
dangers that they perceive. Okay, so one danger,&nbsp;

00:15:03.600 --> 00:15:09.400
one set of dangers are relatively short term.&nbsp;
There are things like, you know, bad people will&nbsp;&nbsp;

00:15:09.400 --> 00:15:16.440
use technology for bad things. What can bad people&nbsp;
use powerful AI systems for? And one concern&nbsp;&nbsp;

00:15:16.440 --> 00:15:24.200
that you know governments have been worried about&nbsp;
and Intelligence agencies encounter intelligence&nbsp;&nbsp;

00:15:24.200 --> 00:15:31.720
and stuff like that is, you know, could badly&nbsp;
intentioned organizations or countries use LLMs&nbsp;&nbsp;

00:15:31.720 --> 00:15:38.960
to help them, I don't know, design pathogens&nbsp;
or chemical weapons or other things, or cyb, or&nbsp;&nbsp;

00:15:38.960 --> 00:15:43.640
cyber attacks? You know things like that. Right&nbsp;
Now, those problems are not new. Those problems&nbsp;&nbsp;

00:15:43.640 --> 00:15:51.120
have been with us for a long time and the question&nbsp;
is what incremental help would AI systems bring to&nbsp;&nbsp;

00:15:51.120 --> 00:16:00.200
the table? So my opinion is that, as of today, AI&nbsp;
systems are not sophisticated enough to provide&nbsp;&nbsp;

00:16:00.200 --> 00:16:07.280
any significant help for such badly intentioned&nbsp;
people, because those systems are trained with&nbsp;&nbsp;

00:16:07.280 --> 00:16:11.520
public data that is publicly available on the&nbsp;
internet and they can't really invent anything.&nbsp;&nbsp;

00:16:11.520 --> 00:16:19.960
They're going to regurgitate with a little bit of&nbsp;
interpolation if you want, but they cannot produce&nbsp;&nbsp;

00:16:19.960 --> 00:16:27.480
anything that you can't get from a search engine&nbsp;
in a few minutes, so that actually that claim&nbsp;&nbsp;

00:16:27.480 --> 00:16:31.400
is being tested at the moment. There are people&nbsp;
who are actually kind of trying to figure it out,&nbsp;&nbsp;

00:16:31.400 --> 00:16:36.320
like is it the case that you can actually do&nbsp;
something - You're able to do something more&nbsp;&nbsp;

00:16:36.320 --> 00:16:42.160
dangerous with the sort of current AI technology&nbsp;
that you can't do with a search engine? Results&nbsp;&nbsp;

00:16:42.160 --> 00:16:49.640
are not out yet, but my hunch is that you know&nbsp;
it's not going to enable a lot of people to do&nbsp;&nbsp;

00:16:49.640 --> 00:16:55.960
significantly bad things. Then there is the issue&nbsp;
of things like code generation for cyber, cyber&nbsp;&nbsp;

00:16:55.960 --> 00:17:00.840
attacks and things like this, and those problems&nbsp;
have been with us for years. And the interesting&nbsp;&nbsp;

00:17:00.840 --> 00:17:06.600
thing that most people should know, like you know&nbsp;
also, for disinformation or attempts to corrupt the&nbsp;&nbsp;

00:17:06.600 --> 00:17:12.440
electoral process and things like this, and what's&nbsp;
very important for everyone to know is that the&nbsp;&nbsp;

00:17:12.440 --> 00:17:19.040
best countermeasures that we have against all of&nbsp;
those attacks currently use AI massively. Okay,&nbsp;&nbsp;

00:17:19.040 --> 00:17:26.000
so AI is used as a defense mechanism against&nbsp;
those attacks. It's not actually used to do the&nbsp;&nbsp;

00:17:26.000 --> 00:17:33.560
attacks yet, and so now it becomes the question&nbsp;
of you know who has the better system? Like other&nbsp;&nbsp;

00:17:33.560 --> 00:17:39.680
countermeasures, is the AI used by counter, by&nbsp;
the countermeasures, but significantly better&nbsp;&nbsp;

00:17:39.680 --> 00:17:43.960
than the AI used by the attackers so that, you know,&nbsp;&nbsp;

00:17:43.960 --> 00:17:50.400
the problem is satisfactorily mitigated, and&nbsp;
that's where we are. Now, the good news is that&nbsp;&nbsp;

00:17:50.400 --> 00:17:56.040
there are many more good guys than bad guys at&nbsp;
the moment. They're usually much more competent,&nbsp;&nbsp;

00:17:56.040 --> 00:18:02.240
they're usually much more sophisticated, they're&nbsp;
usually much better funded and they have a strong&nbsp;&nbsp;

00:18:02.240 --> 00:18:10.200
incentive to take down the attackers. So it's a&nbsp;
game of cat and mouse, just like every yeah, every&nbsp;&nbsp;

00:18:10.200 --> 00:18:16.440
security that's ever existed. There's nothing new&nbsp;
there. Okay, no, nothing quite so new. &nbsp;&nbsp;

00:18:16.440 --> 00:18:25.480
Yeah, but okay, but then there is the question&nbsp;
of existential risk, right, and this is something&nbsp;&nbsp;

00:18:25.480 --> 00:18:32.480
that both Geoff and Yoshua have been thinking of&nbsp;
fairly recently. So for Geoff, it's only sort of&nbsp;&nbsp;

00:18:32.480 --> 00:18:38.400
just before last summer that he became he started&nbsp;
thinking about this because before he thought he&nbsp;&nbsp;

00:18:38.400 --> 00:18:43.960
was convinced that the kind of algorithms that&nbsp;
we had were significantly inferior to the kind of&nbsp;&nbsp;

00:18:43.960 --> 00:18:51.000
learning algorithm that the brain used, and the&nbsp;
epiphany he had was that, in fact, no, because&nbsp;&nbsp;

00:18:51.000 --> 00:18:56.720
looking at the capabilities of large language&nbsp;
models that can do pretty amazing things with a&nbsp;&nbsp;

00:18:56.720 --> 00:19:00.520
relatively small number of neurons and synapses,&nbsp;
he said maybe they're more efficient than the&nbsp;&nbsp;

00:19:00.520 --> 00:19:04.600
brain and maybe the learning algorithm that we use,&nbsp;
back propagation, is actually better than whatever&nbsp;&nbsp;

00:19:04.600 --> 00:19:09.200
it is that the brain uses. So he started thinking&nbsp;
about, like you know what are the consequences,&nbsp;&nbsp;

00:19:09.200 --> 00:19:17.200
and but that's very recent and in my opinion he&nbsp;
hasn't thought about this enough. Yoshua went&nbsp;&nbsp;

00:19:17.200 --> 00:19:27.080
to a similar epiphany last winter where he started&nbsp;
thinking about the long-term consequences and came&nbsp;&nbsp;

00:19:27.080 --> 00:19:33.080
to the conclusion also that there was a potential&nbsp;
danger. They're both convinced that AI has&nbsp;&nbsp;

00:19:33.080 --> 00:19:38.160
enormous potential benefits. They're just worried&nbsp;
about the dangers. And they're both worried&nbsp;&nbsp;

00:19:38.160 --> 00:19:46.480
about the dangers because they have some doubts&nbsp;
about the ability of our institutions to do the&nbsp;&nbsp;

00:19:46.480 --> 00:19:57.440
best with technology Whether they are political,&nbsp;
economic, geopolitical, financial institutions or&nbsp;&nbsp;

00:19:57.440 --> 00:20:08.880
industrial to do the right thing, to be motivated&nbsp;
by the right thing. So if you trust the system,&nbsp;&nbsp;

00:20:08.880 --> 00:20:21.840
if you trust humanity and democracy, you might be&nbsp;
entitled to believe that society is going to make&nbsp;&nbsp;

00:20:21.840 --> 00:20:28.800
the best use of future technology. If you don't&nbsp;
believe in the solidity of those institutions,&nbsp;&nbsp;

00:20:28.800 --> 00:20:35.400
then you might be scared. I think I'm more&nbsp;
confident in humanity and democracy than they are,&nbsp;&nbsp;

00:20:35.400 --> 00:20:40.280
and whatever current systems than they are. I've&nbsp;
been thinking about this problem for much longer,&nbsp;&nbsp;

00:20:40.280 --> 00:20:49.240
actually, since at least 2014. So when I started&nbsp;
FAIR at Facebook at the time, it became pretty&nbsp;&nbsp;

00:20:49.240 --> 00:20:57.240
clear pretty early on that deploying AI systems&nbsp;
was going to have big consequences on people and&nbsp;&nbsp;

00:20:57.240 --> 00:21:02.400
society, and we got confronted to this very early,&nbsp;
and so I started thinking about those problems&nbsp;&nbsp;

00:21:02.400 --> 00:21:10.600
very early on. Things like countermeasures&nbsp;
against bias in AI systems, systematic bias,&nbsp;&nbsp;

00:21:10.600 --> 00:21:18.080
countermeasures against attacks, or detection of&nbsp;
hate speech in every language these are things&nbsp;&nbsp;

00:21:18.080 --> 00:21:23.040
that people at FAIR (Facebook AI Research) worked on and then&nbsp;were eventually deployed. To just to give you&nbsp;&nbsp;

00:21:23.040 --> 00:21:28.760
an example, the proportion of hate speech that&nbsp;
was taken down automatically by AI systems five&nbsp;&nbsp;

00:21:28.760 --> 00:21:37.560
years ago, in 2017, was about 20% to 25%. Last&nbsp;
year it was 95%, and the difference is entirely&nbsp;&nbsp;

00:21:37.560 --> 00:21:43.560
due to progress in natural language understanding.&nbsp;
Entirely due to transformers that are pretrained,&nbsp;&nbsp;

00:21:43.560 --> 00:21:48.400
self-supervised and can essentially detect&nbsp;
hate speech in any language. Not perfectly&nbsp;&nbsp;

00:21:48.400 --> 00:21:54.160
Nothing is perfect, it's never perfect. But AI is&nbsp;
just massively there and that's the solution. So&nbsp;&nbsp;

00:21:54.160 --> 00:22:01.200
I started thinking about those issues, including&nbsp;
existential risk, very early on, in fact, in 2015,&nbsp;&nbsp;

00:22:01.200 --> 00:22:07.920
early 2016, actually, I organized a conference&nbsp;
hosted at NYU on the future of AI where a lot&nbsp;&nbsp;

00:22:07.920 --> 00:22:16.920
of those questions were discussed. I invited&nbsp;
people like Nick Bostrom and Eric Schmidt and&nbsp;&nbsp;

00:22:16.920 --> 00:22:22.880
Mark Schroepfer, who was the CTO of Facebook&nbsp;
at the time, Demis Hassabis, a lot of people,&nbsp;&nbsp;

00:22:23.680 --> 00:22:29.200
both from the academic and AI research side and&nbsp;
from the industry side, and there were two days,&nbsp;&nbsp;

00:22:29.200 --> 00:22:33.840
a public day and kind of a more private day.&nbsp;
What came out of this is the creation of an&nbsp;&nbsp;

00:22:33.840 --> 00:22:39.520
institution called the Partnership on AI. This is&nbsp;
a discussion I had with Demis Hassabis, which was:&nbsp;&nbsp;

00:22:41.400 --> 00:22:46.560
would it be useful to have a forum where we can&nbsp;
discuss, before they happen, sort of bad things&nbsp;&nbsp;

00:22:46.560 --> 00:22:53.600
that could happen as a consequence of deploying&nbsp;
AI? Pretty soon, we brought on board Eric Horvitz&nbsp;&nbsp;

00:22:54.160 --> 00:22:58.400
and a bunch of other people. We co-founded&nbsp;
this thing called the Partnership on AI, which&nbsp;&nbsp;

00:22:58.400 --> 00:23:09.120
basically has been funding studies about AI ethics&nbsp;
and consequences of AI and publishing guidelines&nbsp;&nbsp;

00:23:09.120 --> 00:23:14.480
about how you do it right to minimize harm. So&nbsp;
this is not a new thing for me. I've been thinking&nbsp;&nbsp;

00:23:14.480 --> 00:23:20.320
about this for 10 years essentially, whereas&nbsp;
for Yoshua and Geoff it's much more recent.

00:23:20.320 --> 00:23:29.000
Yeah, but nonetheless, this augmented AI&nbsp;
or augmented language models that have&nbsp;&nbsp;

00:23:29.000 --> 00:23:37.000
stronger reasoning and agency raises the threat,&nbsp;&nbsp;

00:23:37.000 --> 00:23:42.000
regardless of whether or not it&nbsp;
can be countered to a higher level.

00:23:42.000 --> 00:23:50.320
Right, okay. So I guess the question there becomes&nbsp;
what is the blueprint of future AI systems that&nbsp;&nbsp;

00:23:50.320 --> 00:23:57.600
will be capable of reasoning and planning, will&nbsp;
understand how the world works, will be able to&nbsp;&nbsp;

00:23:58.200 --> 00:24:04.760
use tools and have agency and things like&nbsp;
that? Right? And I tell you they will not&nbsp;&nbsp;

00:24:04.760 --> 00:24:11.200
be autoregressive LLMs. So the problems that we&nbsp;
see at the moment of autoregressive LLMs are the&nbsp;&nbsp;

00:24:11.200 --> 00:24:17.240
fact that they hallucinate, they sometimes say&nbsp;
really stupid things, they don't really have&nbsp;&nbsp;

00:24:17.240 --> 00:24:22.200
a good understanding of the world. People claim&nbsp;
that they have some simple world model, but it's&nbsp;&nbsp;

00:24:22.200 --> 00:24:29.360
very implicit and it's really not good at all.&nbsp;
For example, you can tell an LLM that A is the&nbsp;&nbsp;

00:24:29.360 --> 00:24:36.160
same as B and then you ask if B is the same as A&nbsp;
and it will say I don't know or no, right? I mean,&nbsp;&nbsp;

00:24:36.720 --> 00:24:44.560
those things don't really understand logic or&nbsp;
anything like that, right? So the type of system&nbsp;&nbsp;

00:24:44.560 --> 00:24:52.520
that we're talking about that might approach animal level intelligence, let alone human level intelligence,&nbsp;&nbsp;

00:24:52.520 --> 00:25:00.400
have not been designed. They don't exist, and so&nbsp;
discussing their danger and their potential harm&nbsp;&nbsp;

00:25:00.400 --> 00:25:08.320
is a bit like discussing the sex of angels at&nbsp;
the moment, or, to be a little more accurate,&nbsp;&nbsp;

00:25:08.320 --> 00:25:14.120
perhaps it would be kind of like discussing&nbsp;
how we're going to make transatlantic flight at&nbsp;&nbsp;

00:25:14.120 --> 00:25:24.760
near the speed of sound safe when we haven't yet&nbsp;
invented the turbojet in 1925. We can speculate,&nbsp;&nbsp;

00:25:24.760 --> 00:25:33.560
but how did we make a turbojet safe? It required&nbsp;
decades of really careful engineering to make&nbsp;&nbsp;

00:25:33.560 --> 00:25:41.720
them incredibly reliable and now we can run&nbsp;
like halfway around the world with a two-engine&nbsp;&nbsp;

00:25:43.360 --> 00:25:51.000
turbojet aircraft. I mean, that's an incredible&nbsp;
feat. And it's not like people were discussing&nbsp;&nbsp;

00:25:51.000 --> 00:25:55.120
sort of philosophical questions about how you&nbsp;
make turbojet safe. It's just really careful&nbsp;&nbsp;

00:25:55.120 --> 00:26:07.480
and complicated engineering that no one none of us&nbsp;
would understand. So you know, how can we ask the&nbsp;&nbsp;

00:26:07.480 --> 00:26:12.880
AI community now to explain how AI systems are&nbsp;
going to be safe? We haven't invented them yet,&nbsp;&nbsp;

00:26:12.880 --> 00:26:19.640
yeah, okay. That said, I have some idea about&nbsp;
how we can design them so that they have these&nbsp;&nbsp;

00:26:19.640 --> 00:26:27.160
capabilities and, as a consequence, how they will&nbsp;
be safe. I call this objective-driven AI, so what&nbsp;&nbsp;

00:26:27.160 --> 00:26:36.520
that means is essentially systems that produce&nbsp;
their answer by planning their answer so as to&nbsp;&nbsp;

00:26:36.520 --> 00:26:42.720
satisfy an objective or a set of objectives. So&nbsp;
this is very different from current LLMs. Current&nbsp;&nbsp;

00:26:42.720 --> 00:26:47.120
LLMs just produce one word after the other, or&nbsp;
one token which is a transferable unit. It doesn't&nbsp;&nbsp;

00:26:47.120 --> 00:26:52.800
matter. They don't really think and plan ahead.&nbsp;
As we said before, they just produce one word&nbsp;&nbsp;

00:26:52.800 --> 00:26:59.200
after the other. That's not controllable. The only&nbsp;
thing we can do is see if what they've produced,&nbsp;&nbsp;

00:26:59.200 --> 00:27:05.880
check if what they've produced satisfies some&nbsp;
criterion or set of criteria, and then not&nbsp;&nbsp;

00:27:05.880 --> 00:27:13.280
produce an answer or produce a non-answer if the&nbsp;
answer that was produced isn't appropriate. But we&nbsp;&nbsp;

00:27:13.280 --> 00:27:21.920
can't really force them to produce an answer that&nbsp;
satisfies a set of objectives. So objective-driven&nbsp;&nbsp;

00:27:21.920 --> 00:27:29.640
AI is the opposite. The only thing that the&nbsp;
system can produce are answers that satisfy&nbsp;&nbsp;

00:27:29.640 --> 00:27:35.040
a certain number of objectives. So what objective&nbsp;
would be? Did you answer the question? Another&nbsp;&nbsp;

00:27:35.040 --> 00:27:40.200
objective could be, Is your answer understandable&nbsp;
by a 13-year-old, because you're talking to a&nbsp;&nbsp;

00:27:40.200 --> 00:27:49.560
13-year-old? Another would be is this, I don't know,&nbsp;
terrorist propaganda or something? You can have a&nbsp;&nbsp;

00:27:49.560 --> 00:27:54.080
number of criteria, like these, guardrails that&nbsp;
would guarantee that the answer that's produced&nbsp;&nbsp;

00:27:55.280 --> 00:28:00.960
satisfies certain criteria, whatever they are?&nbsp;
Same for a robot, you could guarantee that the&nbsp;&nbsp;

00:28:00.960 --> 00:28:05.560
sequence of actions that is produced will not&nbsp;
hurt anyone. Like you can have very low level&nbsp;&nbsp;

00:28:06.200 --> 00:28:12.640
guardrails of this type that say okay, you have&nbsp;
humans nearby and you're cooking, so you have a&nbsp;&nbsp;

00:28:12.640 --> 00:28:17.720
big knife in your hand, don't flair your arms,&nbsp;
okay, that would be a very simple guardrail to&nbsp;&nbsp;

00:28:17.720 --> 00:28:22.240
impose, and you can imagine having a whole bunch&nbsp;
of guardrails like this that will guarantee that&nbsp;&nbsp;

00:28:22.240 --> 00:28:30.360
the behavior of those systems would be safe and&nbsp;
that their primary goal would be to be basically&nbsp;&nbsp;

00:28:30.360 --> 00:28:39.440
subservient to us. So I do not believe that we'll&nbsp;
have AI systems that can work, that will not be&nbsp;&nbsp;

00:28:39.440 --> 00:28:44.200
subservient to us, will define their own goals - 
they will define their own sub-goals, but those&nbsp;&nbsp;

00:28:44.200 --> 00:28:50.720
sub-goals would be sub-goals of goals that we set&nbsp;
them, and will not have all kinds of guardrails&nbsp;&nbsp;

00:28:50.720 --> 00:28:54.840
that will guarantee their safety. And we're not&nbsp;
going to, It's not like we're going to invent&nbsp;&nbsp;

00:28:54.840 --> 00:28:59.920
a system and make a gigantic one that we know will&nbsp;
have human-level AI and just turn it on and then,&nbsp;&nbsp;

00:28:59.920 --> 00:29:04.200
from the next minute, is going to take over the&nbsp;
world. That's completely preposterous. What we're&nbsp;&nbsp;

00:29:04.200 --> 00:29:09.600
going to do is try with small ones, maybe as&nbsp;
smart as a mouse or something, maybe a dog,&nbsp;&nbsp;

00:29:09.600 --> 00:29:16.240
maybe a cat, maybe a dog maybe and work our way up&nbsp;
and then put some more guardrails, basically like&nbsp;&nbsp;

00:29:16.240 --> 00:29:22.840
we've engineered more and more powerful and more&nbsp;
reliable turbojets. It's an engineering problem.

00:29:22.840 --> 00:29:29.680
Yeah, yeah, you were also on a paper.&nbsp;
Maybe this is the one that talked about&nbsp;&nbsp;

00:29:29.680 --> 00:29:40.560
the embodied Turing test on neuro-AI.&nbsp;
Can you explain what neuro-AI is?

00:29:40.560 --> 00:29:48.600
Okay. Well, it's the idea that we should get&nbsp;
some inspiration from neuroscience to build&nbsp;&nbsp;

00:29:48.600 --> 00:29:57.520
AI systems and that there is something to be&nbsp;
learned from neuroscience and from cognitive&nbsp;&nbsp;

00:29:57.520 --> 00:30:05.400
science to drive the design of AI systems.&nbsp;
Some inspiration, something to be learned,&nbsp;&nbsp;

00:30:05.400 --> 00:30:09.880
as well as the other way around. What's&nbsp;
interesting right now is that the best models&nbsp;&nbsp;

00:30:09.880 --> 00:30:16.680
that we have of how, for example, the visual&nbsp;
cortex works is convolutional neural networks,&nbsp;&nbsp;

00:30:16.680 --> 00:30:23.600
which are also the models that we use to recognize&nbsp;
images, primarily in artificial systems. There is&nbsp;&nbsp;

00:30:24.640 --> 00:30:35.560
information being exchanged both ways. One way&nbsp;
to make progress in AI is to ignore nature and&nbsp;&nbsp;

00:30:35.560 --> 00:30:45.440
just try to solve problems in an engineering&nbsp;
fashion, if you want. I found interaction with&nbsp;&nbsp;

00:30:45.440 --> 00:30:52.880
neuroscience always thought-provoking. You&nbsp;
don't want to be copying nature too closely,&nbsp;&nbsp;

00:30:52.880 --> 00:30:57.240
because there are details in nature that are&nbsp;
irrelevant and there are principles on which&nbsp;&nbsp;

00:30:59.000 --> 00:31:04.760
natural intelligence is based that we haven't&nbsp;
discovered. But there is some inspiration to have,&nbsp;&nbsp;

00:31:04.760 --> 00:31:08.000
certainly convolutional nets were&nbsp;
inspired by the architecture of the visual&nbsp;&nbsp;

00:31:08.000 --> 00:31:14.960
cortex. The whole idea of neural nets and deep&nbsp;
learning came out of the idea that intelligence&nbsp;&nbsp;

00:31:14.960 --> 00:31:19.320
can emerge from a large collection of simple&nbsp;
elements that are connected with each other and&nbsp;&nbsp;

00:31:19.320 --> 00:31:27.160
change the nature of their interactions. That's&nbsp;
the whole idea. Inspiration from neuroscience&nbsp;&nbsp;

00:31:27.760 --> 00:31:34.240
has been extremely beneficial so far, and the&nbsp;
idea of neural AI is that you should keep going.&nbsp;&nbsp;

00:31:34.240 --> 00:31:41.840
You don't want to go too far. Going too far, for&nbsp;
example, is trying to reproduce some aspect of the&nbsp;&nbsp;

00:31:41.840 --> 00:31:49.240
functioning of neurons with electronics. I'm not&nbsp;
sure that's a good idea. I'm skeptical about this,

00:31:49.240 --> 00:31:57.560
for example. So your research right&nbsp;
now, are you, your main focus is&nbsp;&nbsp;

00:31:57.560 --> 00:32:05.400
on furthering the JEPA architecture into&nbsp;
other modalities, or where are you headed?

00:32:06.800 --> 00:32:14.080
Yeah, so, I mean, the long term goal is, you know,&nbsp;
to get machines to be as intelligent and learn&nbsp;&nbsp;

00:32:14.080 --> 00:32:19.560
as efficiently as animals and humans. Okay, and&nbsp;
the reason for this is that we need this because&nbsp;&nbsp;

00:32:19.560 --> 00:32:25.840
we need to amplify human intelligence, and so&nbsp;
intelligence is the most needed commodity that&nbsp;&nbsp;

00:32:25.840 --> 00:32:32.640
we want in the world, right? And so we could,&nbsp;
you know, possibly bring a new renaissance to&nbsp;&nbsp;

00:32:32.640 --> 00:32:37.960
humanity if we could amplify human intelligence&nbsp;
using machines, which we are already doing with&nbsp;&nbsp;

00:32:37.960 --> 00:32:42.840
computers, right, I mean, that's pretty much&nbsp;
what they've been designed to do. But even more,&nbsp;&nbsp;

00:32:42.840 --> 00:32:53.600
you know, imagine a future where every one of us&nbsp;
has an intelligent assistant with us at all times.&nbsp;&nbsp;

00:32:53.600 --> 00:32:59.920
They can be smarter than us. We shouldn't feel&nbsp;
threatened by that. We should feel like we are,&nbsp;&nbsp;

00:32:59.920 --> 00:33:06.600
like, you know, a director of a big lab or a CEO&nbsp;
of a company that has a staff working for them of&nbsp;&nbsp;

00:33:06.600 --> 00:33:10.520
people who are smarter than themselves. I mean,&nbsp;
we're used to this already. I'm used to this,&nbsp;&nbsp;

00:33:10.520 --> 00:33:15.480
certainly working with people who are smarter&nbsp;
than me. So we shouldn't feel threatened by this,&nbsp;&nbsp;

00:33:15.480 --> 00:33:23.480
but it's going to empower a lot of us, right,&nbsp;
and humanity as a whole. So I think that's a&nbsp;&nbsp;

00:33:23.480 --> 00:33:28.320
good thing. That's the overall practical goal. If&nbsp;
you want right. Then there's a scientific question&nbsp;&nbsp;

00:33:28.320 --> 00:33:33.160
that's behind this, which is really what is&nbsp;
intelligence and how do you build it? And&nbsp;&nbsp;

00:33:33.160 --> 00:33:38.640
then which is you know, how can a system learn&nbsp;
the way animals and humans seem to be learning&nbsp;&nbsp;

00:33:38.640 --> 00:33:45.560
so efficiently? And the next thing is, how do&nbsp;
we learn how the world works? By observation,&nbsp;&nbsp;

00:33:45.560 --> 00:33:52.000
by watching the world go by, through vision&nbsp;
and all the other senses. And animals can do&nbsp;&nbsp;

00:33:52.000 --> 00:33:57.560
this without language, right? So it has nothing&nbsp;
to do with language. It has to do with learning&nbsp;&nbsp;

00:33:57.560 --> 00:34:03.840
from sensory percepts and learning mostly&nbsp;
without acting, because any action you take&nbsp;&nbsp;

00:34:03.840 --> 00:34:08.600
can kill you. So it's better to be able to learn&nbsp;
as much as you can without actually acting at all,&nbsp;&nbsp;

00:34:08.600 --> 00:34:13.600
just observing, which is what babies do in the&nbsp;
first few months of life. They can't hardly do&nbsp;&nbsp;

00:34:13.600 --> 00:34:18.600
anything, right? So they mostly observe and&nbsp;
learn how the world works by observation.&nbsp;&nbsp;

00:34:18.600 --> 00:34:23.440
So what kind of learning takes place there?&nbsp;
So that's obviously kind of self-supervised,&nbsp;&nbsp;

00:34:23.440 --> 00:34:30.640
right, it's learning by prediction. That's an old&nbsp;
idea from cognitive science, and the thing is,&nbsp;&nbsp;

00:34:30.640 --> 00:34:34.520
you know, we can learn to predict videos.&nbsp;
But then we noticed that predicting videos,&nbsp;&nbsp;

00:34:34.520 --> 00:34:39.080
predicting pixels in video, is so infinitely&nbsp;
complicated that it doesn't work. And&nbsp;&nbsp;

00:34:39.080 --> 00:34:44.520
so then came this idea of JEPA right. Learn&nbsp;
representations so that you can make predictions&nbsp;&nbsp;

00:34:44.520 --> 00:34:50.200
in representation space, and that turned out to&nbsp;
work really well for learning image features,&nbsp;&nbsp;

00:34:50.200 --> 00:34:55.920
and now we're working on getting this to work for&nbsp;
video and eventually we'll be able to use this to&nbsp;&nbsp;

00:34:56.520 --> 00:35:03.480
learn world models where you show a piece of video&nbsp;
and then you say I'm going to take this action,&nbsp;&nbsp;

00:35:03.480 --> 00:35:10.400
predict what's going to happen next in the&nbsp;
world and you know, which is a bit where the&nbsp;&nbsp;

00:35:10.400 --> 00:35:16.720
Gaia system from Wayve is doing at a high level. But&nbsp;
we need this at various levels of abstraction so&nbsp;&nbsp;

00:35:16.720 --> 00:35:23.320
that we can build, you know, systems that&nbsp;
are more general than autonomous driving.

00:35:23.320 --> 00:35:33.560
Okay, that's the yeah, and it's my&nbsp;
fault so I won't go over the hour,&nbsp;&nbsp;

00:35:33.560 --> 00:35:44.360
but is it conceivable that someday&nbsp;
there will be a model that you, &nbsp;&nbsp;

00:35:44.360 --> 00:35:54.720
maybe embodied in a robot that is ingesting&nbsp;
video from its environment and learning,&nbsp;&nbsp;

00:35:54.720 --> 00:36:01.800
as it's just continuously learning and&nbsp;
getting smarter, and smarter, and smarter?

00:36:01.800 --> 00:36:09.360
Yeah, I mean, that's kind of a bit of a&nbsp;
necessity, the reason being that you know,&nbsp;&nbsp;

00:36:09.360 --> 00:36:13.520
even if you train a system to have a world model&nbsp;
that can predict what's going to happen next.&nbsp;&nbsp;

00:36:13.520 --> 00:36:17.880
The world is really complicated and there's&nbsp;
probably all kinds of situations that you,&nbsp;&nbsp;

00:36:17.880 --> 00:36:21.880
you know the system hasn't been&nbsp;
trained on and needs to, you know,&nbsp;&nbsp;

00:36:21.880 --> 00:36:32.440
fine tune itself as it goes. So you know, animals&nbsp;
and humans do this early in life by playing. So&nbsp;&nbsp;

00:36:32.440 --> 00:36:41.000
play is a way of learning your world model in&nbsp;
situations that basically you won't hurt you,&nbsp;&nbsp;

00:36:42.080 --> 00:36:46.840
and but then during life, of course, you&nbsp;
know, when we learn to drive, there's all&nbsp;&nbsp;

00:36:46.840 --> 00:36:52.400
kinds of these mistakes that we do initially,&nbsp;
that we don't do after having some experience,&nbsp;&nbsp;

00:36:52.400 --> 00:36:58.280
and that's because we're fine tuning our world&nbsp;
model to some extent. Yeah, learning a new task,&nbsp;&nbsp;

00:36:58.280 --> 00:37:04.360
we're basically just learning a new version of&nbsp;
our world model, right? So, yeah, I mean, this&nbsp;&nbsp;

00:37:04.360 --> 00:37:10.880
type of continuous, continual learning is going&nbsp;
to have to be present, but the overall power and&nbsp;&nbsp;

00:37:10.880 --> 00:37:15.120
intelligence of the system will be limited by, you&nbsp;
know, how much a computer neural nets it is&nbsp;&nbsp;

00:37:15.120 --> 00:37:20.720
using and various other constraints. You&nbsp;
know, computational constraints basically.

00:37:20.720 --> 00:37:27.080
You know you're still young and&nbsp;
this. I'm not sure about that. Well,&nbsp;&nbsp;

00:37:28.240 --> 00:37:30.520
you're younger than Geoff. Let me put it that way.

00:37:30.520 --> 00:37:35.200
I'm younger than Geoff, I'm&nbsp;
older than Yoshua, yeah.

00:37:35.200 --> 00:37:42.440
But this, the progress you've made on world&nbsp;
models is fairly rapid from my point of view&nbsp;&nbsp;

00:37:42.440 --> 00:37:55.000
watching it. Are you hopeful that within your&nbsp;
career you'll have embodied robots that are&nbsp;&nbsp;

00:37:55.000 --> 00:38:01.480
building world models through their interaction&nbsp;
in reality and then being able to? Well, I guess&nbsp;&nbsp;

00:38:01.480 --> 00:38:10.320
the other question on world models: do you then&nbsp;
combine it with a language model to do reasoning,&nbsp;&nbsp;

00:38:11.360 --> 00:38:17.480
or is the world model able to do reasoning on&nbsp;
its own? But are you hopeful that in your career&nbsp;&nbsp;

00:38:17.480 --> 00:38:22.960
you'll get to the point where you'll have&nbsp;
this continuous learning in a world model?

00:38:22.960 --> 00:38:28.680
Yeah, I sure hope so. I might have another, you&nbsp;
know, 10 useful years or something like this in&nbsp;&nbsp;

00:38:28.680 --> 00:38:36.120
research before my brain, you know, turns into&nbsp;
bechamel sauce, but, or something like that.&nbsp;&nbsp;

00:38:36.120 --> 00:38:44.880
You know 15 years, if I'm lucky, so, or perhaps&nbsp;
less, but yeah, I hope that there's going to be&nbsp;&nbsp;

00:38:44.880 --> 00:38:50.760
breakthroughs in that direction during that&nbsp;
time. Now, whether that will result in the&nbsp;&nbsp;

00:38:50.760 --> 00:38:57.000
kind of artifact that you're describing you know&nbsp;
robots that can Like you know domestic robots, for&nbsp;&nbsp;

00:38:57.000 --> 00:39:04.080
example, or self-driving cars that can learn fairly&nbsp;
quickly by themselves, I don't know, because there&nbsp;&nbsp;

00:39:04.080 --> 00:39:12.760
might be all kinds of obstacles that we have not&nbsp;
envisaged that may appear on the way. No, it's a&nbsp;&nbsp;

00:39:12.760 --> 00:39:19.920
constant in the history of AI that you have some&nbsp;
new idea and a breakthrough and you think that's&nbsp;&nbsp;

00:39:19.920 --> 00:39:25.600
going to solve all the world's problems, and then&nbsp;
you're going to hit a limitation and you have&nbsp;&nbsp;

00:39:25.600 --> 00:39:30.280
to go beyond that limitation. So it's like you&nbsp;
know you're climbing a mountain. You find a way&nbsp;&nbsp;

00:39:30.280 --> 00:39:36.600
to climb the mountain that you're seeing and you&nbsp;
know that once you get to the top you will have&nbsp;&nbsp;

00:39:36.600 --> 00:39:42.480
the problem solved, because now it's, you know,&nbsp;
a gentle slope down and once you get to the top,&nbsp;&nbsp;

00:39:42.480 --> 00:39:48.840
you realize that there is another mountain&nbsp;
behind it that you hadn't seen. So that's been&nbsp;&nbsp;

00:39:48.840 --> 00:39:55.440
the history of AI right, where people have come up&nbsp;
with sort of new concepts, new ideas, new ways to&nbsp;&nbsp;

00:39:55.440 --> 00:40:04.240
approach AI, reasoning, whatever perception, and&nbsp;
then realize that their idea basically was very&nbsp;&nbsp;

00:40:04.240 --> 00:40:17.080
limited. And so you know this. Inevitably we're&nbsp;
trying to figure out what's the next revolution in&nbsp;&nbsp;

00:40:17.080 --> 00:40:22.120
AI. That's what I'm trying to figure out, and so&nbsp;
you know, learning how the world works from video,&nbsp;&nbsp;

00:40:22.120 --> 00:40:29.320
having systems that have world models, allow&nbsp;
systems to reason and plan. And there's something&nbsp;&nbsp;

00:40:29.320 --> 00:40:37.600
I want to be very clear about, which is an answer&nbsp;
to your question, which is that you can have&nbsp;&nbsp;

00:40:37.600 --> 00:40:44.360
systems that reason and plan without manipulating&nbsp;
language. Animals are capable of amazing feats of&nbsp;&nbsp;

00:40:44.360 --> 00:40:51.800
planning and also, to some extent, reasoning. They&nbsp;
don't have language, at least most of them don't&nbsp;&nbsp;

00:40:52.320 --> 00:41:01.240
and so many of them don't have culture because&nbsp;
they are mostly solitary animals. So you know,&nbsp;&nbsp;

00:41:01.240 --> 00:41:10.840
it's only the animals that have some level of&nbsp;
culture. So the idea that a system can plan and&nbsp;&nbsp;

00:41:10.840 --> 00:41:16.960
reason is not connected with the idea that you&nbsp;
can manipulate language. Those are two different&nbsp;&nbsp;

00:41:16.960 --> 00:41:23.560
things. It needs to be able to manipulate abstract&nbsp;
notions, but those notions do not necessarily&nbsp;&nbsp;

00:41:23.560 --> 00:41:29.520
correspond to linguistic entities like words or&nbsp;
things like that. We can have mental images. If&nbsp;&nbsp;

00:41:29.520 --> 00:41:35.320
you want to think like you do, ask a physicist or&nbsp;
mathematician you know how their reason is very&nbsp;&nbsp;

00:41:35.320 --> 00:41:40.000
much in terms of sort of mental models, nothing&nbsp;
to do with language Then you can turn things into,&nbsp;&nbsp;

00:41:40.000 --> 00:41:49.920
into language. But that's a different story.&nbsp;
That's the second, second step. So you know,&nbsp;&nbsp;

00:41:49.920 --> 00:41:55.960
we're going to have to figure out how to do this.&nbsp;
Reasoning, hierarchical planning in machines&nbsp;&nbsp;

00:41:55.960 --> 00:42:01.560
reproduce this first and then, of course, you&nbsp;
know, sticking language on top of it will help,&nbsp;&nbsp;

00:42:01.560 --> 00:42:05.520
like it will make those systems smarter and be&nbsp;
able you know, it will allow us to communicate&nbsp;&nbsp;

00:42:05.520 --> 00:42:10.280
with them and teach them things and they're going&nbsp;
to be able to teach us things and stuff like that.&nbsp;&nbsp;

00:42:10.280 --> 00:42:16.000
But this is a different question really,&nbsp;
the question of how we organize AI research&nbsp;&nbsp;

00:42:16.000 --> 00:42:21.920
going forward, which is somewhat determined by how&nbsp;
afraid people are of the consequences of AI. So if&nbsp;&nbsp;

00:42:21.920 --> 00:42:28.440
you have a rather positive view of the impact of&nbsp;
AI on society and you trust humanity and society&nbsp;&nbsp;

00:42:28.440 --> 00:42:37.360
and democracy is to use it in good ways, then the&nbsp;
best way to make progress is to open research and&nbsp;&nbsp;

00:42:37.360 --> 00:42:44.080
for the people who are afraid of the consequences,&nbsp;
whether they are societal or geopolitical,&nbsp;&nbsp;

00:42:44.080 --> 00:42:49.920
they're putting pressure on governments around&nbsp;
the world to regulate AI in ways that basically&nbsp;&nbsp;

00:42:49.920 --> 00:42:57.600
limit access, particularly of open source code&nbsp;
and things like that, and it's a big debate at&nbsp;&nbsp;

00:42:57.600 --> 00:43:03.160
the moment. I'm very much on the side, so is&nbsp;
Meta, very much on the side of open research.

00:43:03.160 --> 00:43:09.600
Yeah, actually that was something I was going&nbsp;
to ask you and now that you brought it up,&nbsp;&nbsp;

00:43:09.600 --> 00:43:18.440
because I've been talking to people about this and&nbsp;
there is a view that, aside from the risks of open&nbsp;&nbsp;

00:43:18.440 --> 00:43:28.840
source again Geoff Hinton saying, would you open&nbsp;
source thermonuclear weapons? Aside from that,&nbsp;&nbsp;

00:43:28.840 --> 00:43:40.160
the question is whether open source can marshal&nbsp;
the resources to compete with proprietary models&nbsp;&nbsp;

00:43:40.160 --> 00:43:49.920
and because of the tremendous resources required&nbsp;
for when you're scaling these models. And there's&nbsp;&nbsp;

00:43:49.920 --> 00:43:55.880
a question as to whether or not Meta will&nbsp;
continue to open source future versions&nbsp;&nbsp;

00:43:55.880 --> 00:44:02.880
of Llama or not continue to open source,&nbsp;
but whether it'll continue to invest the&nbsp;&nbsp;

00:44:02.880 --> 00:44:11.480
resources needed to push the open source&nbsp;
models. So what do you think about that?

00:44:11.480 --> 00:44:16.160
Okay, there's a lot to say about this, Okay. So&nbsp;
first thing is there's no question that Meta will&nbsp;&nbsp;

00:44:16.160 --> 00:44:21.920
continue to invest the resources to build better&nbsp;
and better AI systems because it needs it for its&nbsp;&nbsp;

00:44:21.920 --> 00:44:30.720
own products. So the resources will be invested.&nbsp;
Now the next question is will we continue to open&nbsp;&nbsp;

00:44:30.720 --> 00:44:37.720
source the base models? And the answer is probably&nbsp;
yes, because that creates an ecosystem on top&nbsp;&nbsp;

00:44:37.720 --> 00:44:44.000
of which an entire industry can be built, and&nbsp;
there is no point having 50 different companies&nbsp;&nbsp;

00:44:44.560 --> 00:44:52.440
building proprietary, closed systems when you&nbsp;
can have one good open source base model that&nbsp;&nbsp;

00:44:52.440 --> 00:45:00.200
everybody can use. It's wasteful and it's not&nbsp;
a good idea. And another reason for having open&nbsp;&nbsp;

00:45:00.200 --> 00:45:09.200
source models is that nobody has, no entity as&nbsp;
powerful as it thinks it is, has a monopoly&nbsp;&nbsp;

00:45:09.200 --> 00:45:15.040
on good ideas. And so if you want people we can&nbsp;
have good, new, innovative ideas to contribute,&nbsp;&nbsp;

00:45:15.040 --> 00:45:19.240
you need an open source platform. If you want&nbsp;
the academic world to contribute, you need&nbsp;&nbsp;

00:45:19.240 --> 00:45:24.120
open source platforms. If you want the startup&nbsp;
world to be able to build customized products,&nbsp;&nbsp;

00:45:24.120 --> 00:45:28.840
you need open source base models, because&nbsp;
they don't have the resources to build, to&nbsp;&nbsp;

00:45:28.840 --> 00:45:39.800
train large models. And then there is the history&nbsp;
that shows that for foundational technology, for&nbsp;&nbsp;

00:45:39.800 --> 00:45:50.240
infrastructure type technology, open source always&nbsp;
wins. It's true of the software infrastructure of&nbsp;&nbsp;

00:45:50.240 --> 00:45:55.480
the internet. In the early 90s and mid 90s there&nbsp;
was a big battle between Sun Microsystems and&nbsp;&nbsp;

00:45:55.480 --> 00:46:05.400
Microsoft to deliver the software infrastructure&nbsp;
of the internet - operating systems, web servers,&nbsp;&nbsp;

00:46:05.400 --> 00:46:12.120
web browsers and various server side and client&nbsp;
side frameworks. They both lost. Nobody is talking&nbsp;&nbsp;

00:46:12.120 --> 00:46:24.200
about them anymore. The entire world of the web is&nbsp;
using Linux and Apache and MySQL and JavaScript,&nbsp;&nbsp;

00:46:24.200 --> 00:46:35.480
and even the basic core code for web browsers is&nbsp;
open source. So open source won by a huge margin.&nbsp;&nbsp;

00:46:35.480 --> 00:46:42.120
Why? Because it's safer, it gathers more people&nbsp;
to contribute. All the features are unnecessary,&nbsp;&nbsp;

00:46:42.120 --> 00:46:51.720
it's more reliable, Vulnerabilities are fixed&nbsp;
faster and it's customizable. So anybody&nbsp;&nbsp;

00:46:51.720 --> 00:46:57.236
can customize Linux to run on whatever&nbsp;
hardware they want. So open source wins.

00:46:57.236 --> 00:46:59.840
But it's the same thing.

00:47:00.480 --> 00:47:08.440
It's going to be the same thing. It's inevitable.&nbsp;
The people now who are climbing up like OpenAI,&nbsp;&nbsp;

00:47:08.440 --> 00:47:17.160
their system is based on publications&nbsp;
from all of us and from open platforms&nbsp;&nbsp;

00:47:17.160 --> 00:47:22.760
like PyTorch. ChatGPT is built using PyTorch.&nbsp;
PyTorch was produced originally by Meta. Now&nbsp;&nbsp;

00:47:22.760 --> 00:47:27.960
it's owned by the Linux Foundation. It's open&nbsp;
source. They've contributed to it, by the way,&nbsp;&nbsp;

00:47:29.280 --> 00:47:36.160
their LLM is based on transformer architectures&nbsp;
invented at Google. All the tricks to train,&nbsp;&nbsp;

00:47:36.160 --> 00:47:41.280
all those things came out of various papers&nbsp;
from all kinds of different institutions,&nbsp;&nbsp;

00:47:41.280 --> 00:47:48.000
including academia. All the fine-tuning techniques&nbsp;
are the same. So nobody works in a vacuum. The&nbsp;&nbsp;

00:47:48.000 --> 00:47:55.720
thing is, nobody can keep their advance and their&nbsp;
advantage for very long if they are secretive.

00:47:57.120 --> 00:48:02.920
Yeah, except that with these models, because&nbsp;
they're so compute intensive and they cost so&nbsp;&nbsp;

00:48:02.920 --> 00:48:09.520
much money to train, you need somebody&nbsp;
like Meta who's going to be willing to&nbsp;&nbsp;

00:48:09.520 --> 00:48:16.680
build them and open source them. That's why,&nbsp;
when I was asking whether they'll continue,&nbsp;&nbsp;

00:48:17.600 --> 00:48:24.080
obviously Meta will continue&nbsp;
building resource intensive models,&nbsp;&nbsp;

00:48:24.080 --> 00:48:28.560
but the question is whether they'll&nbsp;
continue to open source them.

00:48:30.080 --> 00:48:38.680
I'm telling you the only reason why Meta could&nbsp;
stop open sourcing models is legal. So if&nbsp;&nbsp;

00:48:38.680 --> 00:48:46.640
there is a law that outlaws open source AI&nbsp;
systems above a certain level of sophistication,&nbsp;&nbsp;

00:48:46.640 --> 00:48:54.680
then of course we can't do it. If there are&nbsp;
laws that, in the US or across the world,&nbsp;&nbsp;

00:48:55.640 --> 00:49:03.280
make it illegal to use public content to train AI&nbsp;
systems, then it's the end of AI for everybody,&nbsp;&nbsp;

00:49:03.280 --> 00:49:09.720
not just for open source, or at least the end of&nbsp;
the type of AI that we are talking about today.&nbsp;&nbsp;

00:49:09.720 --> 00:49:15.640
We might have new AI in the future, but that&nbsp;
doesn't require as much data. And then there&nbsp;&nbsp;

00:49:15.640 --> 00:49:28.200
is liability. If you believe that someone is&nbsp;
doing something bad with an AI system that was&nbsp;&nbsp;

00:49:28.200 --> 00:49:35.600
open sourced by Meta, then Meta is liable. Then&nbsp;
Meta will have a big incentive not to release it,&nbsp;&nbsp;

00:49:35.600 --> 00:49:41.960
obviously. So the entire question about this is&nbsp;
around legal reasons and political decisions.

00:49:41.960 --> 00:49:47.880
But on the idea of open source winning, don't&nbsp;
you need more people or more companies like&nbsp;&nbsp;

00:49:47.880 --> 00:49:53.560
Metal building the foundation models&nbsp;
and open sourcing them? Or could it be&nbsp;&nbsp;

00:49:53.560 --> 00:50:00.240
an open source ecosystem win based on&nbsp;
a single company building the models?

00:50:00.240 --> 00:50:03.400
No, I mean you need two or three, and&nbsp;
there are two or three, right. I mean,&nbsp;&nbsp;

00:50:03.400 --> 00:50:11.280
there is this Hugging Face. There is Mistral&nbsp;
in France, who is also embracing open source LLM&nbsp;&nbsp;

00:50:11.280 --> 00:50:19.280
they're very good LLM. It's a small one, but it's&nbsp;
very good. There are academic efforts like LAION.&nbsp;&nbsp;

00:50:20.560 --> 00:50:24.960
They don't have all the resources they need, but&nbsp;
they collect the data that is used by everyone,&nbsp;&nbsp;

00:50:24.960 --> 00:50:28.680
so everybody can contribute. One thing that&nbsp;
I think is really important to understand&nbsp;&nbsp;

00:50:28.680 --> 00:50:35.320
also is that there is a future in which I&nbsp;
described earlier, in which every one of us,&nbsp;&nbsp;

00:50:35.320 --> 00:50:41.440
every one of our interactions with the digital&nbsp;
world, would be mediated by an AI assistant,&nbsp;&nbsp;

00:50:41.440 --> 00:50:46.680
and this is going to be true for everyone around&nbsp;
the world, right? Everyone who has any kind of&nbsp;&nbsp;

00:50:46.680 --> 00:50:52.400
smart device. Eventually, it's going to be in our&nbsp;
augmented reality glasses, but for the time being,&nbsp;&nbsp;

00:50:52.400 --> 00:51:06.640
in our smartphones. And so imagine that future&nbsp;
where you are, I don't know, from Indonesia or&nbsp;&nbsp;

00:51:06.640 --> 00:51:19.400
Senegal or France and your entire digital diet&nbsp;
is done through the mediation of an AI system.&nbsp;&nbsp;

00:51:19.400 --> 00:51:24.920
Your government is not going to be happy about it.&nbsp;
Your government is going to want the local culture&nbsp;&nbsp;

00:51:24.920 --> 00:51:30.720
to be present in that system. It doesn't want&nbsp;
that system to be closed sourced and controlled&nbsp;&nbsp;

00:51:30.720 --> 00:51:40.400
by a company on the west coast of the US. So&nbsp;
just for reasons of preserving the diversity&nbsp;&nbsp;

00:51:40.400 --> 00:51:47.600
of culture across the world and not having our&nbsp;
entire information diet being biased by whatever&nbsp;&nbsp;

00:51:47.600 --> 00:51:53.400
it is that some company on the west coast of the&nbsp;
US thinks there's going to need to be open source&nbsp;&nbsp;

00:51:53.400 --> 00:52:03.240
platforms and they're going to be predominant in&nbsp;
at least outside the US for that reason, including&nbsp;&nbsp;

00:52:03.240 --> 00:52:08.200
China. There is all those talks about oh what if&nbsp;
China puts their hand on our open source code? I&nbsp;&nbsp;

00:52:08.200 --> 00:52:15.840
mean, China wants control over its own LLM because&nbsp;
they're a citizen to have access to certain types&nbsp;&nbsp;

00:52:15.840 --> 00:52:20.320
of information. So they're not going to use our&nbsp;
LLMs, they're going to train theirs. That they&nbsp;&nbsp;

00:52:20.320 --> 00:52:27.720
already have. And nobody is particularly ahead&nbsp;
of anybody else by more than about a year.

00:52:28.520 --> 00:52:36.320
And China is pushing open source. I mean, they're&nbsp;
very pro open source within their ecosystems.

00:52:36.320 --> 00:52:44.960
Some of them. There is no unified opinion there,&nbsp;
but I mean it's the same in the West, right,&nbsp;&nbsp;

00:52:44.960 --> 00:52:51.720
there are some governments that are too afraid&nbsp;
of the risks and then, or are thinking about it,&nbsp;&nbsp;

00:52:51.720 --> 00:52:56.800
and some others that are all for open source&nbsp;
because they see this as the only way for&nbsp;&nbsp;

00:52:56.800 --> 00:53:03.680
them to have any influence on the information,&nbsp;
the type of information and culture that would&nbsp;&nbsp;

00:53:03.680 --> 00:53:14.960
be mediated by those systems. So it's going to&nbsp;
have to be like Wikipedia, right? Wikipedia is built&nbsp;&nbsp;

00:53:14.960 --> 00:53:20.200
by millions of people who contribute to it from&nbsp;
all around the world, in all kinds of languages,&nbsp;&nbsp;

00:53:20.200 --> 00:53:26.080
and it has a system for vetting the information.&nbsp;
The way AI systems of the future will be taught&nbsp;&nbsp;

00:53:26.080 --> 00:53:31.160
and will be fine tuned will have to be the&nbsp;
same way. It will have to be crowd sourced,&nbsp;&nbsp;

00:53:31.160 --> 00:53:39.200
because something that matters to a farmer&nbsp;
in Southern India is probably not going to&nbsp;&nbsp;

00:53:39.200 --> 00:53:46.040
be taken into account by the fine tuning done&nbsp;
by some company on the West Coast of the US.

00:53:46.040 --> 00:53:52.120
AI might be the most important new computer&nbsp;
technology ever. It's storming every industry&nbsp;&nbsp;

00:53:52.120 --> 00:53:57.920
and literally billions of dollars are being&nbsp;
invested, so buckle up. The problem is that&nbsp;&nbsp;

00:53:57.920 --> 00:54:04.160
AI needs a lot of speed and processing power.&nbsp;
So how do you compete without cost spiraling&nbsp;&nbsp;

00:54:04.160 --> 00:54:10.760
out of control? It's time to upgrade to&nbsp;
the next generation of the cloud Oracle&nbsp;&nbsp;

00:54:10.760 --> 00:54:18.480
Cloud Infrastructure, or OCI. OCI is a single&nbsp;
platform for your infrastructure, database,&nbsp;&nbsp;

00:54:18.480 --> 00:54:27.000
application development and AI needs. OCI has&nbsp;
four to eight times the bandwidth of other clouds,&nbsp;&nbsp;

00:54:27.000 --> 00:54:32.720
offers one consistent price instead of&nbsp;
variable regional pricing. And, of course,&nbsp;&nbsp;

00:54:32.720 --> 00:54:39.880
nobody does data better than Oracle. So now you&nbsp;
can train your AI models at twice the speed and&nbsp;&nbsp;

00:54:39.880 --> 00:54:47.000
less than half the cost of other clouds. If&nbsp;
you want to do more and spend less, like Uber,&nbsp;&nbsp;

00:54:47.000 --> 00:55:00.480
8x8 and Databricks Mosaic, take a free test drive&nbsp;
of OCI at oracle.com/eyeonai. That's E-Y-E-O-N-A-I&nbsp;&nbsp;

00:55:00.480 --> 00:55:06.240
all run together: oracle.com/eyeonai.&nbsp;
&nbsp;&nbsp;

00:55:06.240 --> 00:55:12.320
That's it for this episode. I want to thank Yann&nbsp;
for his time. If you want to read a transcript&nbsp;&nbsp;

00:55:12.320 --> 00:55:22.360
of this conversation, you can find one on our&nbsp;
website eye-on.ai, that's eye-on.ai. And remember&nbsp;&nbsp;

00:55:22.360 --> 00:55:36.480
the singularity may not be near, but AI is&nbsp;
changing your world, so best pay attention.&nbsp;

00:55:36.480 --> 00:55:36.980
&nbsp;
&nbsp;

