WEBVTT
Kind: captions
Language: en

00:00:00.040 --> 00:00:29.400
PETER
Hi, I'm Peter Guagenti. I'm&nbsp;&nbsp;

00:00:29.400 --> 00:00:34.960
President and Chief Marketing Officer at Tabnine.&nbsp;
Iâ€™ve got a long history in enterprise software,&nbsp;&nbsp;

00:00:34.960 --> 00:00:40.040
including having built and scaled companies&nbsp;
like NGINX and Cockroach Labs. But actually,&nbsp;&nbsp;

00:00:40.040 --> 00:00:44.520
I spent most of my career as a web&nbsp;
developer and engineering lead myself,&nbsp;&nbsp;

00:00:44.520 --> 00:00:50.040
in management consulting, and in digital&nbsp;
agencies. I worked on over 100 different&nbsp;&nbsp;

00:00:50.040 --> 00:00:53.600
company projects over 15 years doing&nbsp;
what we now call digital transformation.

00:00:53.600 --> 00:00:55.200
CRAIG
And tell us about&nbsp;&nbsp;

00:00:55.200 --> 00:01:02.600
Tabnine. You know, I've heard a lot of people&nbsp;
refer to Tabnine and before this call I did a&nbsp;&nbsp;

00:01:02.600 --> 00:01:12.920
little bit of research but I'm not entirely clear&nbsp;
about the company's history and its direction. So,&nbsp;&nbsp;

00:01:12.920 --> 00:01:20.000
can you tell us about Tabnine's origins, what it&nbsp;
does, and what it's looking to do in the future?

00:01:20.000 --> 00:01:21.120
PETER
Absolutely. So&nbsp;&nbsp;

00:01:21.120 --> 00:01:27.720
Tabnine is actually the originator of the AI code&nbsp;
assistant category. So the company was founded by&nbsp;&nbsp;

00:01:27.720 --> 00:01:34.080
two very good friends, Eran Yahav and Dror Weiss.&nbsp;
Eran was actually a professor at the Technion,&nbsp;&nbsp;

00:01:34.080 --> 00:01:42.280
which is a computer science program in Israel.&nbsp;
They had actually been focused on how to&nbsp;&nbsp;

00:01:42.280 --> 00:01:47.840
leverage AI and machine learning to simplify&nbsp;
software development. They started this work&nbsp;&nbsp;

00:01:47.840 --> 00:01:56.200
almost a decade ago and had been making very good&nbsp;
advancements, and then LLMs emerged. Back in 2018,&nbsp;&nbsp;

00:01:56.200 --> 00:02:01.240
the team actually released the very first&nbsp;
LLM-based code assistant for Java, and that&nbsp;&nbsp;

00:02:01.240 --> 00:02:08.160
was Tabnine. So fast forward to today, the company&nbsp;
spent, in that time building out the capabilities,&nbsp;&nbsp;

00:02:08.160 --> 00:02:12.960
ironically, before CIOs or engineering leads&nbsp;
really appreciated it. So it was mostly just&nbsp;&nbsp;

00:02:12.960 --> 00:02:19.360
selling directly to developers. There's now&nbsp;
over a million users of Tabnine on a frequent&nbsp;&nbsp;

00:02:19.360 --> 00:02:27.680
basis. Millions have tried it and used it over the&nbsp;
years. About a year or so ago, the company really&nbsp;&nbsp;

00:02:27.680 --> 00:02:33.920
pivoted and started supporting large engineering&nbsp;
teams. As a significant player in the category,&nbsp;&nbsp;

00:02:33.920 --> 00:02:38.600
we had engineering managers coming to the company&nbsp;
and saying, [Okay, we want this capability]. And&nbsp;&nbsp;

00:02:38.600 --> 00:02:44.760
they were drawn, unlike some of the cloud provider&nbsp;
tools, they were drawn to a few things for us; one&nbsp;&nbsp;

00:02:44.760 --> 00:02:50.920
is, we've always done fully private deployments.&nbsp;
There's no code sharing, there's no user data&nbsp;&nbsp;

00:02:50.920 --> 00:02:57.240
sharing. It all either runs in your private&nbsp;
self hosted environment, either VPC or on Prem,&nbsp;&nbsp;

00:02:57.240 --> 00:03:03.120
or in a fully private SaaS environment which&nbsp;
we operate, where you have completely private&nbsp;&nbsp;

00:03:03.120 --> 00:03:09.680
individual environments. They also were drawn&nbsp;
to our model flexibility. So we created from the&nbsp;&nbsp;

00:03:09.680 --> 00:03:14.680
very beginning what we call the Tabnine Protected&nbsp;
Model. So we have our own LLM that sits under both&nbsp;&nbsp;

00:03:14.680 --> 00:03:20.040
our code completions and our chat agents, that's&nbsp;
actually trained only on permissively licensed&nbsp;&nbsp;

00:03:20.040 --> 00:03:25.200
code. We do support other LLMs; we actually have&nbsp;
a great model that we developed in partnership&nbsp;&nbsp;

00:03:25.200 --> 00:03:31.080
with Mistral. We work with all the large model&nbsp;
providers, including Anthropic and OpenAI. But&nbsp;&nbsp;

00:03:31.080 --> 00:03:37.440
the protected model gives highly regulated&nbsp;
or highly sensitive organizations an option&nbsp;&nbsp;

00:03:37.440 --> 00:03:42.760
to only use the model that they know they have&nbsp;
rights to use the code that are underneath it,&nbsp;&nbsp;

00:03:42.760 --> 00:03:48.160
which is pretty critical for many of these folks.&nbsp;
And then, you asked the question around where are&nbsp;&nbsp;

00:03:48.160 --> 00:03:55.160
we today and where we're going. We've really been&nbsp;
focusing on innovation around AI code assistance&nbsp;&nbsp;

00:03:55.160 --> 00:03:59.760
and other software development AI agents&nbsp;
that are highly personalized to individual&nbsp;&nbsp;

00:03:59.760 --> 00:04:06.080
engineering teams. We have a thesis which is that&nbsp;
generative AI is great, but if you look at, say,&nbsp;&nbsp;

00:04:06.080 --> 00:04:11.080
the code assistant category and use generative AI,&nbsp;
it's like hiring a seasoned software engineer off&nbsp;&nbsp;

00:04:11.080 --> 00:04:15.680
the street, right? You're going to end up with&nbsp;
someone who has very strong knowledge and really&nbsp;&nbsp;

00:04:15.680 --> 00:04:19.760
understands the principles, but if you ask them&nbsp;
how to do something, it'll do it in a way that's&nbsp;&nbsp;

00:04:19.760 --> 00:04:25.520
academically sound and correct but not necessarily&nbsp;
a fit for you. And we think that this is one of&nbsp;&nbsp;

00:04:25.520 --> 00:04:31.120
the things that's holding back wide-scale adoption&nbsp;
of generative AI, is fit to purpose. So we've&nbsp;&nbsp;

00:04:31.120 --> 00:04:38.000
really focused in, the last couple of years, and&nbsp;
continue to double down on leveraging context RAG,&nbsp;&nbsp;

00:04:38.000 --> 00:04:45.200
even customization to models and other connections&nbsp;
to other systems to really inform the AI platform.&nbsp;&nbsp;

00:04:45.200 --> 00:04:48.640
So instead of behaving like a software engineer&nbsp;
off the street, it behaves like somebody who's&nbsp;&nbsp;

00:04:48.640 --> 00:04:52.920
operated in the company for the last 5, 10&nbsp;
years; a fully onboarded, fully experienced&nbsp;&nbsp;

00:04:52.920 --> 00:04:58.880
software engineer. And we think that's the future.&nbsp;
We think the more context and personalization we&nbsp;&nbsp;

00:04:58.880 --> 00:05:04.640
can provide around these AI agents, where they&nbsp;
really do fit you, how you work, and your team's&nbsp;&nbsp;

00:05:04.640 --> 00:05:09.760
behaviors, standards, and guidelines, then&nbsp;
the more value that it's going to deliver.

00:05:09.760 --> 00:05:13.760
CRAIG
Yeah. Everybody has been following,&nbsp;&nbsp;

00:05:13.760 --> 00:05:21.280
and I've done a number of episodes on automated&nbsp;
code generation, and a lot of the things that have&nbsp;&nbsp;

00:05:21.280 --> 00:05:29.200
been coming out from AlphaCode to certainly&nbsp;
Copilot and CodeWhisperer. Most recently I&nbsp;&nbsp;

00:05:29.200 --> 00:05:39.280
talked to the founder of Julius AI, and everyone's&nbsp;
waiting for Devon for their general release. The&nbsp;&nbsp;

00:05:39.280 --> 00:05:48.320
question that's being asked about all of these is&nbsp;
at what point are they fully autonomous? At what&nbsp;&nbsp;

00:05:48.320 --> 00:05:57.240
point can they generate code that you can trust&nbsp;
without having them operating as a pair programmer&nbsp;&nbsp;

00:05:57.240 --> 00:06:10.080
and without having a coder make suggestions? I&nbsp;
know you guys came out with this Atlassian Jira,&nbsp;&nbsp;

00:06:10.080 --> 00:06:17.560
I don't know if it's a plugin, but integration&nbsp;
that I think is fairly autonomous. Can you talk&nbsp;&nbsp;

00:06:17.560 --> 00:06:26.320
about that? And then what's the process to get to&nbsp;
that full autonomy? Julius AI, who I had on fairly&nbsp;&nbsp;

00:06:26.320 --> 00:06:39.160
recently, is focused on tabular data, on creating&nbsp;
an autonomous agent that you can work with to&nbsp;&nbsp;

00:06:39.160 --> 00:06:48.760
manipulate tabular data. Is that where this going,&nbsp;
that there will be these narrow use cases that are&nbsp;&nbsp;

00:06:48.760 --> 00:06:59.520
refined enough that they're autonomous? At what&nbsp;
point does it break out into more general coding?

00:06:59.520 --> 00:07:02.560
PETER
Yeah. Well look, the way we think about this is&nbsp;&nbsp;

00:07:03.160 --> 00:07:07.400
there's a spectrum of capabilities inside of all&nbsp;
these tools. And it's not just code generation,&nbsp;&nbsp;

00:07:07.400 --> 00:07:14.800
it's break-fix, it's other generation like things&nbsp;
like documentation, it's fully autonomous testing,&nbsp;&nbsp;

00:07:14.800 --> 00:07:20.240
it's all of these things. We think about the&nbsp;
spectrum as running from an AI assistant at one&nbsp;&nbsp;

00:07:20.240 --> 00:07:26.920
end to a fully autonomous AI software engineer&nbsp;
on the other end. I would be careful about the&nbsp;&nbsp;

00:07:26.920 --> 00:07:32.000
hype that you see around some of the tools that&nbsp;
are out there that are showing really fancy demos&nbsp;&nbsp;

00:07:32.000 --> 00:07:37.760
and things like that. I can make a really fancy&nbsp;
demo as well. I can make something that looks like&nbsp;&nbsp;

00:07:37.760 --> 00:07:43.040
it's going to completely replace a software&nbsp;
engineer, but it's not realistic. It's not&nbsp;&nbsp;

00:07:43.040 --> 00:07:47.760
realistic. I think to answer your question, this&nbsp;
is going to sound slightly conflicted, but it's&nbsp;&nbsp;

00:07:47.760 --> 00:07:54.600
important to say is: yes, there are things we can&nbsp;
do fully autonomously today. We released around&nbsp;&nbsp;

00:07:54.600 --> 00:08:00.680
the Atlassian Team â€˜24 eventâ€“ we're an Atlassian&nbsp;
backed company through Atlassian Ventures. We're&nbsp;&nbsp;

00:08:00.680 --> 00:08:05.760
also a huge Atlassian partner. We want to get&nbsp;
people excited in showing them a straight Jira&nbsp;&nbsp;

00:08:05.760 --> 00:08:13.720
ticket-to-code tool, which actually does straight&nbsp;
app dev. It's not a, [here's some code and maybe&nbsp;&nbsp;

00:08:13.720 --> 00:08:19.840
you can go fine tune it]. It was taking a ticket&nbsp;
and actually creating a full node application.&nbsp;&nbsp;

00:08:20.720 --> 00:08:29.680
That is possible today; the question is, where&nbsp;
are the constraints? Where we're very focused is&nbsp;&nbsp;

00:08:29.680 --> 00:08:34.720
we're trying to take as many of these discrete&nbsp;
tasks as possible that a software engineer is&nbsp;&nbsp;

00:08:34.720 --> 00:08:40.760
working through today and take it to as full&nbsp;
completion as possible. So an example of this,&nbsp;&nbsp;

00:08:40.760 --> 00:08:45.840
obviously the first one was documentation. We do&nbsp;
complete documentation generation across multiple&nbsp;&nbsp;

00:08:45.840 --> 00:08:50.880
touchpoints. That's basically fully autonomous&nbsp;
now. You don't have to worry about that. Yes,&nbsp;&nbsp;

00:08:50.880 --> 00:08:54.640
we believe that we should always have a human in&nbsp;
the loop on these things. We think that there's&nbsp;&nbsp;

00:08:54.640 --> 00:09:00.600
significant value in having a human in the loop.&nbsp;
So even with that full autonomy, we want human&nbsp;&nbsp;

00:09:00.600 --> 00:09:06.400
review and validation, just like we have code&nbsp;
review and things like that from architects and&nbsp;&nbsp;

00:09:06.400 --> 00:09:11.720
managers today. That's what a software engineer&nbsp;
should be doing as AI works. A lot of that&nbsp;&nbsp;

00:09:11.720 --> 00:09:16.480
capability exists pretty deep in the product. It's&nbsp;
for break fix, we're seeing it, we have almost&nbsp;&nbsp;

00:09:16.480 --> 00:09:22.520
fully autonomous testing now available. That's&nbsp;
very strong. We've got a lot of this capability&nbsp;&nbsp;

00:09:22.520 --> 00:09:29.360
fully developed. To the point of where we get&nbsp;
to where there's effectively autonomous app dev,&nbsp;&nbsp;

00:09:30.000 --> 00:09:36.360
I think we are not far away. I think the way we're&nbsp;
going to get there, however, is probably through&nbsp;&nbsp;

00:09:36.360 --> 00:09:41.720
an iterative process and a slightly different&nbsp;
user experience than we have today with these&nbsp;&nbsp;

00:09:41.720 --> 00:09:50.560
AI platforms; where we actually have AI working&nbsp;
in conjunction with a software engineer to ask&nbsp;&nbsp;

00:09:50.560 --> 00:09:55.200
further questions, ask for clarification,&nbsp;
to propose options, and get feedback,&nbsp;&nbsp;

00:09:55.200 --> 00:10:00.760
which is actually how we work today. It's how&nbsp;
we work as humans today. So we're probably not&nbsp;&nbsp;

00:10:00.760 --> 00:10:05.960
very far off. We actually have, in the lab, pretty&nbsp;
advanced versions of a lot of this stuff already.&nbsp;&nbsp;

00:10:05.960 --> 00:10:09.720
But I think what's going to happen is we're going&nbsp;
to start building up that layer, take a piece at&nbsp;&nbsp;

00:10:09.720 --> 00:10:17.920
a time, by creating fully automated versions and&nbsp;
autonomous capabilities for pieces of it. Now,&nbsp;&nbsp;

00:10:17.920 --> 00:10:22.160
I want to say one really critical thing in this&nbsp;
because I think there's thisâ€“ the people who are&nbsp;&nbsp;

00:10:22.160 --> 00:10:26.400
here getting most excited about this are people&nbsp;
who are not software engineers. And frankly the&nbsp;&nbsp;

00:10:26.400 --> 00:10:29.440
people who get most excited are people who&nbsp;
don't usually really understand how software&nbsp;&nbsp;

00:10:29.440 --> 00:10:33.680
engineering works. So the idea that these AI&nbsp;
bots are going to come and we're not going to&nbsp;&nbsp;

00:10:33.680 --> 00:10:39.880
have software engineers anymore is foolishness.&nbsp;
Anybody who's ever worked in a microservices,&nbsp;&nbsp;

00:10:39.880 --> 00:10:45.560
data intensive, modern application will tell&nbsp;
you that the level of complexity in a modern&nbsp;&nbsp;

00:10:45.560 --> 00:10:51.480
application is such that it is highly unlikely&nbsp;
AI is going to completely replace humans anytime&nbsp;&nbsp;

00:10:51.480 --> 00:10:56.560
in the foreseeable future. What I suspect will&nbsp;
happen is as we automate more and more of these&nbsp;&nbsp;

00:10:56.560 --> 00:11:00.800
tasks and more and more of these processes, the&nbsp;
apps themselves are going to get more complex.&nbsp;&nbsp;

00:11:00.800 --> 00:11:05.560
It's going to unlock the ability for us to&nbsp;
do even more interesting things. And you're&nbsp;&nbsp;

00:11:05.560 --> 00:11:11.080
going to see an up leveling of the software&nbsp;
engineer themselves to get out of the doldrums,&nbsp;&nbsp;

00:11:11.080 --> 00:11:18.600
to get out of the sort of repeated tasks that tend&nbsp;
to eat up a lot of our time and more into design,&nbsp;&nbsp;

00:11:18.600 --> 00:11:24.280
evolution, and really advanced thinking. And&nbsp;
I look forward to that day. If you go back 20,&nbsp;&nbsp;

00:11:24.280 --> 00:11:27.440
30 years ago, we used to do things like memory&nbsp;
management, you used to have to figure out how&nbsp;&nbsp;

00:11:27.440 --> 00:11:32.160
to write to disk, and all these other crazy&nbsp;
things that we did when we did app dev. As&nbsp;&nbsp;

00:11:32.160 --> 00:11:36.400
we automated more and more of those things,&nbsp;
think about how advanced the applications got.&nbsp;&nbsp;

00:11:36.400 --> 00:11:40.460
I can't even imagine what the applications&nbsp;
are going to look like 20 years into that.

00:11:40.460 --> 00:11:42.320
CRAIG
Yeah. Although,&nbsp;&nbsp;

00:11:42.320 --> 00:11:49.760
if there is this iterative process where you're&nbsp;
talking to an LLM, or to a generative AI system&nbsp;&nbsp;

00:11:49.760 --> 00:11:59.240
and refining your intent to the point that&nbsp;
it can code something that's compilable,&nbsp;&nbsp;

00:12:02.200 --> 00:12:12.280
that does open up development to non-coders&nbsp;
like myself, who may not be creating extremely&nbsp;&nbsp;

00:12:12.280 --> 00:12:24.440
complicated apps but can create apps that the&nbsp;
code assistance aren't able to now. Do you&nbsp;&nbsp;

00:12:24.440 --> 00:12:35.440
think that's important, or do you think that's&nbsp;
kind of ancillary to the main direction of where&nbsp;&nbsp;

00:12:35.440 --> 00:12:44.000
all of this is heading, which is to enable&nbsp;
engineers to write increasingly complex apps?

00:12:44.000 --> 00:12:46.200
PETER
It's an interesting question&nbsp;&nbsp;

00:12:46.200 --> 00:12:54.240
because I think there's a question here on, what&nbsp;
is a software engineer and who makes applications?&nbsp;&nbsp;

00:12:54.240 --> 00:12:59.400
That's really what you're saying, because the only&nbsp;
thing that the AI assistant really replaces is the&nbsp;&nbsp;

00:12:59.400 --> 00:13:04.680
need for us to handwrite code. Because&nbsp;
at the end of the day, you think about&nbsp;&nbsp;

00:13:04.680 --> 00:13:09.880
a modern microservices-based application with&nbsp;
lots of nodes, inflows and outflows of data,&nbsp;&nbsp;

00:13:09.880 --> 00:13:15.800
and specific experiences that you're creating,&nbsp;
you do have to know how to architect and engineer&nbsp;&nbsp;

00:13:15.800 --> 00:13:20.960
that in order to do that well, even if someone&nbsp;
else is writing the code. I started my career&nbsp;&nbsp;

00:13:20.960 --> 00:13:26.360
as a developer and designer. I started out doing&nbsp;
both user experience, designing the system and&nbsp;&nbsp;

00:13:26.360 --> 00:13:31.560
then actually hand coding it, very quickly got&nbsp;
out of the code. Then eventually even got out of&nbsp;&nbsp;

00:13:31.560 --> 00:13:37.920
having to do all of the flows. However, I was&nbsp;
still architecting the actual product and the&nbsp;&nbsp;

00:13:37.920 --> 00:13:44.560
actual application. We kept moving up the stack&nbsp;
and all those things. And I think to imagine a&nbsp;&nbsp;

00:13:44.560 --> 00:13:48.960
world where someone who's a complete layman&nbsp;
and doesn't understand how the system works,&nbsp;&nbsp;

00:13:48.960 --> 00:13:53.280
to actually go and â€œwrite an application"&nbsp;
is probably unrealistic, right? Because you&nbsp;&nbsp;

00:13:53.280 --> 00:13:59.200
actually still have to understand the interplay&nbsp;
and interconnectedness of the data and the systems&nbsp;&nbsp;

00:13:59.200 --> 00:14:04.360
and the components around you. I'd imagine&nbsp;
though, that this starts to look 10, 20 years&nbsp;&nbsp;

00:14:04.360 --> 00:14:11.040
from now more like the way an architect builds&nbsp;
a skyscraper, where they don't have to go and&nbsp;&nbsp;

00:14:11.040 --> 00:14:14.960
plan for every piece of material, they don't have&nbsp;
to plan for every component that goes in but they&nbsp;&nbsp;

00:14:14.960 --> 00:14:18.240
have to have a vision. They have to understand how&nbsp;
these systems work and interact with each other,&nbsp;&nbsp;

00:14:18.240 --> 00:14:24.240
at least if you want them to perform well and to&nbsp;
be successful. I think you're seeing it already,&nbsp;&nbsp;

00:14:24.240 --> 00:14:29.200
probably sneak previews of it today, even&nbsp;
without AI: functions as a service, drag and drop&nbsp;&nbsp;

00:14:29.200 --> 00:14:34.280
componentry; I spent a significant portion of&nbsp;
my career working in open source frameworks like&nbsp;&nbsp;

00:14:34.280 --> 00:14:39.280
Drupal, WordPress, and Magento. And at the time&nbsp;
you could sort of LEGO snap together applications,&nbsp;&nbsp;

00:14:39.280 --> 00:14:43.720
but you still really did have to understand&nbsp;
how these things worked. And I'd imagine that,&nbsp;&nbsp;

00:14:43.720 --> 00:14:46.800
and this is pure speculation obviously,&nbsp;
but I'd imagine if you fast forward 10,&nbsp;&nbsp;

00:14:46.800 --> 00:14:50.920
20 years from today the people who are still going&nbsp;
to be most successful building applications are&nbsp;&nbsp;

00:14:50.920 --> 00:14:56.800
the ones who understand the total system and&nbsp;
how these systems work and pull together the&nbsp;&nbsp;

00:14:56.800 --> 00:15:01.640
right componentry. I just imagine a lot of it is&nbsp;
going to be generated automatically by AI for us.

00:15:01.640 --> 00:15:02.840
CRAIG
And Tabnine,&nbsp;&nbsp;

00:15:02.840 --> 00:15:08.720
you started as a Java tool;&nbsp;
are you in other languages now?

00:15:08.720 --> 00:15:10.760
PETER
Yeah. The very&nbsp;&nbsp;

00:15:10.760 --> 00:15:18.880
first LLM we built was basically just trained on&nbsp;
Java exclusively. This is back in 2018. So this is&nbsp;&nbsp;

00:15:18.880 --> 00:15:23.920
now six years ago, seven years ago, when we first&nbsp;
started doing this. The LLMs were very limited,&nbsp;&nbsp;

00:15:23.920 --> 00:15:30.960
the context windows were very small, we had to be&nbsp;
very focused. You fast forward to today however,&nbsp;&nbsp;

00:15:30.960 --> 00:15:37.840
and we support over 80 languages and frameworks.&nbsp;
We support anything that, really, any software&nbsp;&nbsp;

00:15:37.840 --> 00:15:43.440
engineer is typically going to work with. Then&nbsp;
on top of that, Tabnine, unlike most of our&nbsp;&nbsp;

00:15:43.440 --> 00:15:50.000
competitors, we actually do fine tuned models. So&nbsp;
we work with mostly very large engineering teams&nbsp;&nbsp;

00:15:50.000 --> 00:15:54.360
who are our enterprise customers. We have sort&nbsp;
of individual developers and small teams who use&nbsp;&nbsp;

00:15:54.360 --> 00:15:59.920
our SaaS product. But our enterprise customers&nbsp;
tend to be big banks, they tend to be defense&nbsp;&nbsp;

00:15:59.920 --> 00:16:04.480
organizations, they tend to be pharmaceutical&nbsp;
companies, software companies, or even hardware&nbsp;&nbsp;

00:16:04.480 --> 00:16:09.160
manufacturers. So with them, a lot of times we're&nbsp;
actually going and doing fine tuned versions of&nbsp;&nbsp;

00:16:09.160 --> 00:16:13.840
our models that are trained on their code. And&nbsp;
that starts to get really interesting when you do&nbsp;&nbsp;

00:16:13.840 --> 00:16:18.960
that because not only does it make the behavior of&nbsp;
the model stronger for your specific use cases but&nbsp;&nbsp;

00:16:18.960 --> 00:16:25.960
actually it allows us to close the gap on obscure&nbsp;
languages and frameworks that tend to be not a&nbsp;&nbsp;

00:16:25.960 --> 00:16:31.000
lot of great public training data for; you think&nbsp;
about like machine code for very specific hardware&nbsp;&nbsp;

00:16:31.000 --> 00:16:36.400
companies or sort of obscure software languages&nbsp;
that are used in financial services. You see a&nbsp;&nbsp;

00:16:36.400 --> 00:16:43.200
lot of that. So that 80 doesn't even cover it. I&nbsp;
feel like most of the time these days when someone&nbsp;&nbsp;

00:16:43.200 --> 00:16:50.800
asks us for language coverage, we just immediately&nbsp;
go in and start testing against their use case and&nbsp;&nbsp;

00:16:50.800 --> 00:16:55.440
start immediately seeing that it's actually super&nbsp;
performant. The powerful thing about these LLMs&nbsp;&nbsp;

00:16:55.440 --> 00:17:00.640
is that these LLMs basically know how to write&nbsp;
software. So as long as they have the visibility&nbsp;&nbsp;

00:17:00.640 --> 00:17:04.600
to the structure of a programming language,&nbsp;
they're incredibly successful with that language.

00:17:04.600 --> 00:17:06.560
CRAIG
Yeah. But&nbsp;&nbsp;

00:17:06.560 --> 00:17:14.000
then exactly that point, that they know&nbsp;
how to write software. That would argue&nbsp;&nbsp;

00:17:14.000 --> 00:17:22.560
that eventually they would be able&nbsp;
to architect a piece of software,&nbsp;&nbsp;

00:17:22.560 --> 00:17:30.920
at least guide a user in how to architect a piece&nbsp;
of software beyond the coding. Can you talk about&nbsp;&nbsp;

00:17:30.920 --> 00:17:39.400
developing the fully autonomous Jira tool? How&nbsp;
did you train it? What its capabilities are?&nbsp;&nbsp;

00:17:39.400 --> 00:17:45.400
Then talk about how do you then take that&nbsp;
process and broaden it to something else.

00:17:45.400 --> 00:17:47.320
PETER
So I think the most important thing&nbsp;&nbsp;

00:17:47.320 --> 00:17:51.200
to note, if you want to understand how this stuff&nbsp;
works, is that the model is only the foundation&nbsp;&nbsp;

00:17:51.200 --> 00:17:58.120
of the house. The actual interaction with those&nbsp;
models is where all of the actual success is. A&nbsp;&nbsp;

00:17:58.120 --> 00:18:04.840
perfect example of this, go and use something like&nbsp;
ChatGPT directly and ask it software development&nbsp;&nbsp;

00:18:04.840 --> 00:18:11.200
questions. Then go use Tabnine connected to GPT-4o&nbsp;
and ask the same exact question. You're going&nbsp;&nbsp;

00:18:11.200 --> 00:18:17.800
to get a radically more evolved answer. That's&nbsp;
actually because the LLMs themselves are really&nbsp;&nbsp;

00:18:17.800 --> 00:18:24.160
just a font of knowledge. That's what they really&nbsp;
are. And there's this whole other layer of work&nbsp;&nbsp;

00:18:24.160 --> 00:18:30.560
we do around context and what information actually&nbsp;
gets sent into the LLM when you ask it a question&nbsp;&nbsp;

00:18:30.560 --> 00:18:35.600
in order to give it appropriate context and&nbsp;
understanding, plus the prompt engineering itself,&nbsp;&nbsp;

00:18:35.600 --> 00:18:41.400
plus even just the UI/UX, things we do around&nbsp;
helping to structure the question or ask follow&nbsp;&nbsp;

00:18:41.400 --> 00:18:46.840
on questions and things like that. The way I think&nbsp;
about this is, and this is important context to&nbsp;&nbsp;

00:18:46.840 --> 00:18:53.280
explain the autonomous generation, if you and I&nbsp;
met on the street and you asked me, [I'm looking&nbsp;&nbsp;

00:18:53.280 --> 00:19:00.240
to buy a car, what car would I get?] and I knew&nbsp;
all about cars, I would give you 100 answers that&nbsp;&nbsp;

00:19:00.240 --> 00:19:06.520
are all potentially relevant to you. But if you're&nbsp;
my best friend and I know you've always wanted a&nbsp;&nbsp;

00:19:06.520 --> 00:19:14.040
convertible, you live in warm weather, you just&nbsp;
had a windfall of cash, you really want something&nbsp;&nbsp;

00:19:14.040 --> 00:19:18.040
fancy, and you want to be known, I'm going to&nbsp;
start directing you in a very specific case;&nbsp;&nbsp;

00:19:18.040 --> 00:19:23.520
versus, somebody who's, new family, they've&nbsp;
got three children, they do the school run,&nbsp;&nbsp;

00:19:23.520 --> 00:19:27.240
and they need something that's going to be safe&nbsp;
and practical for that. It's a totally different&nbsp;&nbsp;

00:19:27.240 --> 00:19:35.080
context. That understanding is really key to be&nbsp;
able to operate autonomously because it's never&nbsp;&nbsp;

00:19:35.080 --> 00:19:41.200
enough to be able just to answer the question, you&nbsp;
have to answer the question appropriately for the&nbsp;&nbsp;

00:19:41.200 --> 00:19:48.000
use case, for the context of the organization&nbsp;
itâ€™s serving, for the users it's serving,&nbsp;&nbsp;

00:19:48.000 --> 00:19:57.520
for 100 parameters. What we do today natively&nbsp;
in Tabnine is we use local code-base awareness,&nbsp;&nbsp;

00:19:57.520 --> 00:20:03.200
global code-based awareness, and then integrations&nbsp;
to non-code sources of information on top of all&nbsp;&nbsp;

00:20:03.200 --> 00:20:06.040
of our prompt engineering, all the work we do&nbsp;
in order to make sure that the recommendations&nbsp;&nbsp;

00:20:06.040 --> 00:20:10.720
come back really strong. So local code-based&nbsp;
awareness is literally looking at every open file,&nbsp;&nbsp;

00:20:10.720 --> 00:20:15.400
it's looking at all the things that are&nbsp;
accessible from there, things like the&nbsp;&nbsp;

00:20:15.400 --> 00:20:20.040
errors and other things visible from within the&nbsp;
IDE. Global code-based awareness is just that,&nbsp;&nbsp;

00:20:20.040 --> 00:20:25.440
it's a Git connection into whatever your repos&nbsp;
are, and then non-code is things like Jira and&nbsp;&nbsp;

00:20:25.440 --> 00:20:32.840
Confluence, whatever. So why is that relevant to&nbsp;
the autonomous? Because when you start giving it&nbsp;&nbsp;

00:20:32.840 --> 00:20:38.280
design parameters, it's not enough to have all the&nbsp;
design parameters; it has to understand what else&nbsp;&nbsp;

00:20:38.280 --> 00:20:45.480
it's interacting with and how else it works.&nbsp;
So the Jira autonomous agent that we built is&nbsp;&nbsp;

00:20:45.480 --> 00:20:52.000
actually very simplistic. It's really just looking&nbsp;
and saying, [this is the design specification, now&nbsp;&nbsp;

00:20:52.000 --> 00:20:57.040
let's actually go and generate the application.]&nbsp;
It makes a bunch of assumptions around language,&nbsp;&nbsp;

00:20:57.040 --> 00:21:02.400
it makes assumptions around data storage, it makes&nbsp;
a bunch of assumptions that are built into that,&nbsp;&nbsp;

00:21:02.400 --> 00:21:07.200
that if you have full context, then those&nbsp;
aren't assumptions anymore. Now they're&nbsp;&nbsp;

00:21:07.200 --> 00:21:14.960
actually parameters that are being fed in. So if&nbsp;
you think about where this has to go from hereâ€“&nbsp;&nbsp;

00:21:14.960 --> 00:21:22.560
and if you watch the video of the Jira agent we&nbsp;
built, it actually does ask clarifying questions.&nbsp;&nbsp;

00:21:22.560 --> 00:21:31.680
We think that that's the future of this, is taking&nbsp;
that spec and then identifying what it doesn't&nbsp;&nbsp;

00:21:31.680 --> 00:21:38.280
know. That's a really, really challenging problem&nbsp;
because AI is not self-aware. So how do we figure&nbsp;&nbsp;

00:21:38.280 --> 00:21:43.120
out what it doesn't know and what it needs to&nbsp;
know in order to solve that problem? That's really&nbsp;&nbsp;

00:21:43.120 --> 00:21:46.920
where this next wave of innovation is going to&nbsp;
come from, is being able to resolve that. Usually&nbsp;&nbsp;

00:21:46.920 --> 00:21:52.080
through a combination of what additional context&nbsp;
we feed it automatically, what additional context&nbsp;&nbsp;

00:21:52.080 --> 00:21:57.400
we ask for, and then what clarifying questions we&nbsp;
need resolved. I think that's going to get us to&nbsp;&nbsp;

00:21:57.400 --> 00:22:07.720
a place of where these agents are behaving more&nbsp;
like Jarvis in Ironman and less like your chatbot&nbsp;&nbsp;

00:22:07.720 --> 00:22:11.720
at the call center, where you're just trying&nbsp;
to get tracking information for your package.

00:22:11.720 --> 00:22:14.320
CRAIG
That context,&nbsp;&nbsp;

00:22:14.840 --> 00:22:24.360
the global context you were talking about, how&nbsp;
is that fed into the LLM? Do you load up a vector&nbsp;&nbsp;

00:22:24.360 --> 00:22:35.880
database with the code base around the problem&nbsp;
at hand, or have you just fine-tuned the model?

00:22:35.880 --> 00:22:37.200
PETER
In our case,&nbsp;&nbsp;

00:22:37.200 --> 00:22:42.680
it's actually a vector database and we're using&nbsp;
retrieval augmented generation. What we have&nbsp;&nbsp;

00:22:42.680 --> 00:22:47.960
done however, is we have a number of proprietary&nbsp;
things that we've done around what specifically&nbsp;&nbsp;

00:22:47.960 --> 00:22:53.600
we're using as context and in what way, because&nbsp;
that's really the art. The art of getting these&nbsp;&nbsp;

00:22:53.600 --> 00:23:00.520
things to respond the way you want them to is all&nbsp;
of the effort you do around that exactly. So what&nbsp;&nbsp;

00:23:00.520 --> 00:23:05.320
we have is basically a vector database that gets&nbsp;
built, it gets preloaded with the parameters that&nbsp;&nbsp;

00:23:05.320 --> 00:23:10.000
we believe are appropriate. And that's both based&nbsp;
on our local code-based awarenessâ€“ so if like&nbsp;&nbsp;

00:23:10.000 --> 00:23:14.720
you're one of our credit card-based customers it's&nbsp;
whatever we can access with your permission from&nbsp;&nbsp;

00:23:14.720 --> 00:23:20.520
within the IDE. If you are an enterprise customer,&nbsp;
then it's what an administrator gives us access to&nbsp;&nbsp;

00:23:20.520 --> 00:23:24.920
in their Git repo. That vector database for those&nbsp;
customers actually does live in their environment,&nbsp;&nbsp;

00:23:24.920 --> 00:23:29.680
not in ours; so does the model actually, if they&nbsp;
use one of our private models, it also lives in&nbsp;&nbsp;

00:23:29.680 --> 00:23:36.120
their environment. So that's all self-contained.&nbsp;
We basically have been testing and tuning this&nbsp;&nbsp;

00:23:36.120 --> 00:23:40.520
for years now around what is relevant and what&nbsp;
is not relevant, when we think about the prompt&nbsp;&nbsp;

00:23:40.520 --> 00:23:45.960
engineering around this and then what context&nbsp;
goes in. LLMs themselves are not smart things.&nbsp;&nbsp;

00:23:45.960 --> 00:23:51.880
I think this is where we find them remarkable&nbsp;
because of this idea that we can ask these&nbsp;&nbsp;

00:23:51.880 --> 00:23:58.720
sort of freeform questions, but if you actually&nbsp;
start using them for very specific generative&nbsp;&nbsp;

00:23:58.720 --> 00:24:03.920
use cases; you think about, there's a great&nbsp;
article recently in Fortune about the rise&nbsp;&nbsp;

00:24:03.920 --> 00:24:10.960
of generative AI in law. What you're discovering&nbsp;
is you go and ask ChatGPT legal questions and it&nbsp;&nbsp;

00:24:10.960 --> 00:24:16.320
hallucinates like you wouldn't imagine. It'll even&nbsp;
tell you things thatâ€“ it'll tell you citations&nbsp;&nbsp;

00:24:16.320 --> 00:24:23.000
that aren't real. And it's already caught a bunch&nbsp;
of lawyers out in cases live. But then you go and&nbsp;&nbsp;

00:24:23.000 --> 00:24:27.040
look at what LexisNexis is doing and these folks&nbsp;
who really understand it, they're doing some&nbsp;&nbsp;

00:24:27.040 --> 00:24:32.160
really advanced things once again to use their&nbsp;
knowledge to check the knowledge. Eventually,&nbsp;&nbsp;

00:24:32.160 --> 00:24:36.400
and we're already starting to do this with some&nbsp;
of our code review agents, which will roll out&nbsp;&nbsp;

00:24:36.400 --> 00:24:41.600
later this summer, we have models checking&nbsp;
models. So we have models sort of holding&nbsp;&nbsp;

00:24:41.600 --> 00:24:47.440
each other accountable in order to eliminate&nbsp;
any hallucination or identify any issues. I&nbsp;&nbsp;

00:24:47.440 --> 00:24:53.520
think that's the kind of effort that's going to be&nbsp;
required for generative AI to actually be useful.

00:24:53.520 --> 00:24:56.480
CRAIG
Yeah. On the vector database,&nbsp;&nbsp;

00:24:56.480 --> 00:25:07.280
presumably you have that workflow automated,&nbsp;
that you plug into. Yeah. Recently I was talking&nbsp;&nbsp;

00:25:07.280 --> 00:25:17.280
to people at Boston Consulting Group, who are way&nbsp;
ahead of the other groups on AI and generative AI,&nbsp;&nbsp;

00:25:17.280 --> 00:25:26.880
and they have been experimenting with, instead of&nbsp;
using a vector database, as these context windows&nbsp;&nbsp;

00:25:26.880 --> 00:25:34.840
get larger and larger, loading a couple&nbsp;
hundred pages of content of text, code,&nbsp;&nbsp;

00:25:34.840 --> 00:25:42.920
or whatever it is into the promptâ€“ it's not really&nbsp;
a system prompt, well maybe it is a system prompt.&nbsp;&nbsp;

00:25:44.600 --> 00:25:52.480
Then when you ask a question, the inference comes&nbsp;
out of that. It's like RAG but instead of having&nbsp;&nbsp;

00:25:52.480 --> 00:25:58.240
to first create a vector database, you just&nbsp;
put all the stuff front-loaded in the prompt.&nbsp;&nbsp;

00:25:59.000 --> 00:26:06.520
And understandably, that doesn't really&nbsp;
work for a lot of use cases because&nbsp;&nbsp;

00:26:06.520 --> 00:26:13.160
it creates a lot of latency issues.&nbsp;
Would that be an easier way to do it?

00:27:15.760 --> 00:27:36.080
PETER
No. I mean, it's easier from&nbsp;&nbsp;

00:27:36.080 --> 00:27:41.400
an engineering perspective but it's actually the&nbsp;
brute force method. I would argue that whoever is&nbsp;&nbsp;

00:27:41.400 --> 00:27:45.680
doing that doesn't actually understand how context&nbsp;
works. Because if you're just loading that in,&nbsp;&nbsp;

00:27:46.360 --> 00:27:51.440
an individual can do that with ChatGPT today,&nbsp;
and just copy and paste an entire catalog of&nbsp;&nbsp;

00:27:51.440 --> 00:27:56.600
information and ask it questions about that&nbsp;
information. I think the thing that they're&nbsp;&nbsp;

00:27:56.600 --> 00:28:01.560
missing in that is, some of that information is&nbsp;
relevant and some is not. And it's not just about&nbsp;&nbsp;

00:28:01.560 --> 00:28:06.280
latency. Yes, it'll dramatically increase your&nbsp;
latency if you're feeding in that much information&nbsp;&nbsp;

00:28:06.280 --> 00:28:10.160
for it to process in every single query that&nbsp;
you're putting out. But also, it might give&nbsp;&nbsp;

00:28:10.160 --> 00:28:17.400
you irrelevant information. Maybe that's good&nbsp;
if you're trying to, say, ask it to synthesize&nbsp;&nbsp;

00:28:17.400 --> 00:28:23.360
or summarize information or asking it to make&nbsp;
conclusions about a specific body of knowledge.&nbsp;&nbsp;

00:28:23.360 --> 00:28:27.240
That's a good use of feeding that in. But when&nbsp;
we're talking about what we're doing with software&nbsp;&nbsp;

00:28:27.240 --> 00:28:31.600
development, there are things that are relevant&nbsp;
and there are things that are irrelevant. There&nbsp;&nbsp;

00:28:31.600 --> 00:28:38.240
are good examples and there are bad examples and&nbsp;
you can feed it too much information and then get&nbsp;&nbsp;

00:28:38.240 --> 00:28:45.080
back mediocrity. I think when we talk about this,&nbsp;
we're talking about relevance and quality. That's&nbsp;&nbsp;

00:28:45.080 --> 00:28:50.680
the things that we're really focused on. One of&nbsp;
the biggest weaknesses with generative AI today,&nbsp;&nbsp;

00:28:50.680 --> 00:28:56.760
relevance and quality. When we're talking about&nbsp;
context in this place, we're not just using the&nbsp;&nbsp;

00:28:56.760 --> 00:29:03.720
technical concept of context, but what is&nbsp;
conceptually and intellectually appropriate&nbsp;&nbsp;

00:29:03.720 --> 00:29:08.760
context to answer the question well. That's&nbsp;
really where all of the fine tuning of us as&nbsp;&nbsp;

00:29:08.760 --> 00:29:13.240
AI code assistants, that's where we're spending&nbsp;
our time. And I think we're the pointy end of the&nbsp;&nbsp;

00:29:13.240 --> 00:29:18.440
spear for generative AI applications in general.&nbsp;
I think this approach that we're taking is going&nbsp;&nbsp;

00:29:18.440 --> 00:29:22.800
to be leveraged by everyone. We're seeing it&nbsp;
already with image generation, for example,&nbsp;&nbsp;

00:29:22.800 --> 00:29:28.400
where they're being very, very specific about&nbsp;
what parameters end up getting sent in order to&nbsp;&nbsp;

00:29:28.400 --> 00:29:33.840
make sure that the right imagery comes back.&nbsp;
That also includes constraints and controls.&nbsp;&nbsp;

00:29:34.960 --> 00:29:40.960
That's a whole other area that we've only seen&nbsp;
the tip of the iceberg. Which is, generation&nbsp;&nbsp;

00:29:40.960 --> 00:29:46.800
is one thing but what about constraining that&nbsp;
generation where it specifically fits certain&nbsp;&nbsp;

00:29:46.800 --> 00:29:52.000
use cases and certain parameters. That's another&nbsp;
layer of context that you're not going to achieve&nbsp;&nbsp;

00:29:52.000 --> 00:29:57.480
by just dumping in a bunch of material. You have&nbsp;
to be really selective about what gets sent over.

00:29:57.480 --> 00:30:00.520
CRAIG
And this RAG approach,&nbsp;&nbsp;

00:30:02.520 --> 00:30:11.080
that works for specific, very narrow&nbsp;
use cases. How then do you broaden&nbsp;&nbsp;

00:30:11.080 --> 00:30:19.755
that so that your genAI system can&nbsp;
do more than that particular problem?

00:30:19.755 --> 00:30:20.280
PETER
I'm going to get out&nbsp;&nbsp;

00:30:20.280 --> 00:30:28.000
of my technical depth very fast on this. I am&nbsp;
not a modeler, I am not a math PhD like our&nbsp;&nbsp;

00:30:28.000 --> 00:30:33.320
engineers are. Let me wrap it up in this way:&nbsp;
it's not just RAG; I think what we're talking&nbsp;&nbsp;

00:30:33.320 --> 00:30:40.760
about here is the complexity of how we manage the&nbsp;
whole experience. It's RAG, it's semantic memory,&nbsp;&nbsp;

00:30:40.760 --> 00:30:45.400
it's prompt engineering. It's even just how the&nbsp;
queries are posed and what information is sent&nbsp;&nbsp;

00:30:45.400 --> 00:30:50.280
over with the query in that prompt. There's&nbsp;
a lot of components that go in and what we've&nbsp;&nbsp;

00:30:50.280 --> 00:30:56.480
discovered is it's not an A, or B, or C: it's&nbsp;
really an A, and B, and C, and really tuning&nbsp;&nbsp;

00:30:56.480 --> 00:31:01.200
each of those for each of the use cases. So one&nbsp;
of the things that we innovated and we're proud,&nbsp;&nbsp;

00:31:01.200 --> 00:31:06.200
others are picking up now, is these AI agents,&nbsp;
in the chat. So things like, [explain this&nbsp;&nbsp;

00:31:06.200 --> 00:31:11.840
code to me, onboard me to this project, fix&nbsp;
this code, find security vulnerabilities],&nbsp;&nbsp;

00:31:11.840 --> 00:31:16.480
those sorts of things. And each of those,&nbsp;
the reason why they're structured as simple&nbsp;&nbsp;

00:31:16.480 --> 00:31:21.880
UX buttons or prompts, and they're structured&nbsp;
the way they are is because they have preloaded&nbsp;&nbsp;

00:31:21.880 --> 00:31:27.920
capability all behind them. And once we know&nbsp;
your intent, this is what you're trying to do,&nbsp;&nbsp;

00:31:28.800 --> 00:31:33.600
then we can actually go and give the best&nbsp;
possible answer based on an understanding of&nbsp;&nbsp;

00:31:33.600 --> 00:31:40.400
that intention. So there's a lot of complexity in&nbsp;
getting this right, there really is. I think it's&nbsp;&nbsp;

00:31:40.400 --> 00:31:46.960
very easy to sort of understate the importance&nbsp;
of these things and then at the same time be&nbsp;&nbsp;

00:31:46.960 --> 00:31:50.960
frustrated with the output when you're just asking&nbsp;
unformed questions and not getting a good answer.

00:31:50.960 --> 00:31:58.400
PETER
Yeah. But to go from fully autonomous, to&nbsp;&nbsp;

00:31:59.000 --> 00:32:10.320
handling of Jira tickets, to another obvious use&nbsp;
case. Where are you guys going with full autonomy?

00:32:10.320 --> 00:32:12.480
PETER
We're working through&nbsp;&nbsp;

00:32:12.480 --> 00:32:18.040
the entire SDLC. We actually think that code&nbsp;
generation is exciting and it sounds interesting,&nbsp;&nbsp;

00:32:18.040 --> 00:32:22.360
but where do we actually spend all of our time? We&nbsp;
spend all of our time in maintenance, we spend all&nbsp;&nbsp;

00:32:22.360 --> 00:32:27.880
of our time in break-fix, we spend all of our time&nbsp;
in things like refactoring, performance tuning,&nbsp;&nbsp;

00:32:27.880 --> 00:32:33.200
security issues. And actually most of the issues&nbsp;
in software development, the stuff that eats up&nbsp;&nbsp;

00:32:33.200 --> 00:32:39.800
a lot of the release time is actually code&nbsp;
review. So where we're going is working on&nbsp;&nbsp;

00:32:39.800 --> 00:32:45.280
building in as much autonomous resolution of&nbsp;
each of those tasks as possible. And frankly,&nbsp;&nbsp;

00:32:45.280 --> 00:32:51.200
we're moving out of the IDE. We're moving into&nbsp;
code review, we're moving into security review,&nbsp;&nbsp;

00:32:51.200 --> 00:32:57.520
we're moving into all these other areas. The other&nbsp;
thing that we're really focused on is making sure&nbsp;&nbsp;

00:32:58.360 --> 00:33:04.040
that the things that are being generated actually&nbsp;
suit the company standards. So later this summer&nbsp;&nbsp;

00:33:04.040 --> 00:33:07.960
we're going to be releasing a new set of&nbsp;
capabilities that allow you to constrain&nbsp;&nbsp;

00:33:07.960 --> 00:33:12.880
the AI. We've been very focused, obviously all&nbsp;
of us have been very focused on making them able&nbsp;&nbsp;

00:33:12.880 --> 00:33:19.680
to do more. Now we're at a point where we're&nbsp;
introducing, with all this AI generated code,&nbsp;&nbsp;

00:33:19.680 --> 00:33:26.720
a potential risk with poorly trained software&nbsp;
engineers, unmanaged, or lightly managed software&nbsp;&nbsp;

00:33:26.720 --> 00:33:30.240
engineers now introducing code that just&nbsp;
doesn't get accepted, it doesn't make it&nbsp;&nbsp;

00:33:30.240 --> 00:33:35.960
past the pull request. So simple things like,&nbsp;
[we follow the Google Java coding standards];&nbsp;&nbsp;

00:33:35.960 --> 00:33:40.960
okay, then let's make sure that all of the code&nbsp;
that is being generated or being refactored fits&nbsp;&nbsp;

00:33:40.960 --> 00:33:46.560
those standards so they make it past the pull&nbsp;
request. That's a very simple example but the&nbsp;&nbsp;

00:33:46.560 --> 00:33:52.800
bigger examples are: Does it use our functions?&nbsp;
Does it use our APIs? Is it structured the&nbsp;&nbsp;

00:33:52.800 --> 00:33:57.720
way we write those things? Does it follow our&nbsp;
security parameters? All those things. And that's&nbsp;&nbsp;

00:33:58.920 --> 00:34:05.360
a layer of AI capability that is generally&nbsp;
untapped today by most of the tools. But us as&nbsp;&nbsp;

00:34:05.360 --> 00:34:11.560
the first to market, we're leaning into those&nbsp;
now and really saying, how do we make this so&nbsp;&nbsp;

00:34:11.560 --> 00:34:16.400
it's even easier? And frankly, so we serve more&nbsp;
people right in the software development team.

00:34:16.400 --> 00:34:19.040
CRAIG
Yeah. That's interesting on the code review&nbsp;&nbsp;

00:34:19.040 --> 00:34:27.720
because I'm a journalist, not a practitioner,&nbsp;
but I've been talking to people about&nbsp;&nbsp;

00:34:30.480 --> 00:34:40.480
specifically on code, but on all generated&nbsp;
content. The more generated content there is&nbsp;&nbsp;

00:34:40.480 --> 00:34:51.760
on the internet, the more that generated content&nbsp;
is going to slip into training datasets. And if&nbsp;&nbsp;

00:34:51.760 --> 00:35:01.440
what's being generated is not the best that&nbsp;
can be written either in text or in code,&nbsp;&nbsp;

00:35:01.440 --> 00:35:11.840
you kind of pollute the training data. With&nbsp;
all of these people using code assistance,&nbsp;&nbsp;

00:35:11.840 --> 00:35:18.520
it's speeding up the writing of code and it's&nbsp;
increasing the volume of code being written,&nbsp;&nbsp;

00:35:18.520 --> 00:35:29.920
but it's not necessarily increasing the quality of&nbsp;
code being written. Does the code review take care&nbsp;&nbsp;

00:35:29.920 --> 00:35:34.860
of that problem, to go through and say, [wow,&nbsp;
this is spaghetti, it needs to be refactored]?

00:35:34.860 --> 00:35:39.160
PETER
Absolutely, it absolutely does. The things&nbsp;&nbsp;

00:35:39.160 --> 00:35:45.680
that we can do right now are things like making&nbsp;
sure that it fits the standards and the structure,&nbsp;&nbsp;

00:35:45.680 --> 00:35:50.640
making sure that it actually runs well, runs&nbsp;
correctly, making sure that it follows performance&nbsp;&nbsp;

00:35:50.640 --> 00:35:58.240
and security best practices. And getting a human&nbsp;
in the loop also allows us to make that code&nbsp;&nbsp;

00:35:58.240 --> 00:36:04.280
review agent even smarter. Because if they push&nbsp;
back, ask for clarification, or reject something&nbsp;&nbsp;

00:36:04.280 --> 00:36:09.640
that is a recommendation of the code review&nbsp;
agent, then we're going to make that quality&nbsp;&nbsp;

00:36:09.640 --> 00:36:15.480
even stronger. And I think you raise a really&nbsp;
important point which is, where's the training&nbsp;&nbsp;

00:36:15.480 --> 00:36:20.760
data? What is the training data? Was it good? And&nbsp;
I think a lot of the general purpose LLMs trained&nbsp;&nbsp;

00:36:20.760 --> 00:36:27.160
on anything they could find and it actually made&nbsp;
them weaker. It didn't make them stronger on these&nbsp;&nbsp;

00:36:27.160 --> 00:36:35.280
things. So we've really heavily focused on really&nbsp;
curating what training data went into our models.&nbsp;&nbsp;

00:36:35.280 --> 00:36:41.240
And then, even when we fine tune, if you don't&nbsp;
care about the copyright license compliance,&nbsp;&nbsp;

00:36:41.240 --> 00:36:44.760
which about two-thirds of our customers don't&nbsp;
care about license copyright compliance,&nbsp;&nbsp;

00:36:44.760 --> 00:36:50.320
a third really do. The two-thirds who don't, we&nbsp;
took the off-the-shelf Mistral open source and we&nbsp;&nbsp;

00:36:50.320 --> 00:36:56.440
fine-tuned it because we discovered that it needed&nbsp;
a little bit more guidance around how to perform&nbsp;&nbsp;

00:36:56.440 --> 00:37:01.200
certain tasks and what that looked like. So we&nbsp;
used our curated materials to do that. I think as&nbsp;&nbsp;

00:37:01.200 --> 00:37:07.280
more and more content, whatever it is, be it code,&nbsp;
imagery, or text gets generated by AI, we're gonna&nbsp;&nbsp;

00:37:07.280 --> 00:37:14.440
have to get more and more strict about what it is&nbsp;
we're actually training on. And, I think there's&nbsp;&nbsp;

00:37:14.440 --> 00:37:18.560
a debate around what volume of trading data is&nbsp;
required for high performance. Sam Altman very&nbsp;&nbsp;

00:37:18.560 --> 00:37:21.320
famously went in front of British parliament&nbsp;
and said, [oh, we couldn't build these things&nbsp;&nbsp;

00:37:21.320 --> 00:37:26.840
without accessing all of the wealth of human&nbsp;
knowledge in the world]. Then Mistral comes out&nbsp;&nbsp;

00:37:26.840 --> 00:37:33.360
and releases a model that's a direct competitor&nbsp;
in high performance that did it. So I think we're&nbsp;&nbsp;

00:37:33.360 --> 00:37:39.120
still only even learning now what is required to&nbsp;
make each of these models perform the way we want.

00:37:39.120 --> 00:37:41.400
CRAIG
Yeah. How do you&nbsp;&nbsp;

00:37:41.400 --> 00:37:50.860
curate the training data? Because of the volume of&nbsp;
data required, that's kind of a monumental task.

00:37:50.860 --> 00:37:52.000
PETER
It is a monumental&nbsp;&nbsp;

00:37:52.000 --> 00:37:56.480
task. We've been at it for years. And frankly, a&nbsp;
lot of these languages don't change. We're doing&nbsp;&nbsp;

00:37:56.480 --> 00:38:02.080
things like Java, Python, and others that have&nbsp;
been around for a very long time. So we've put&nbsp;&nbsp;

00:38:02.080 --> 00:38:08.320
in the work already on the foundation of&nbsp;
these things, we continue to add, curate,&nbsp;&nbsp;

00:38:08.320 --> 00:38:19.080
and pour through a lot of that stuff as well.&nbsp;
There's no silver bullet to that. You actually&nbsp;&nbsp;

00:38:19.080 --> 00:38:23.680
have to go and put in the work to be able to make&nbsp;
sure that you're using good, high quality sources.

00:38:23.680 --> 00:38:24.480
CRAIG
And does&nbsp;&nbsp;

00:38:24.480 --> 00:38:32.680
that mean having experienced&nbsp;
coders read code and blockâ€“

00:38:32.680 --> 00:38:35.180
PETER
Know what are trustworthy sources versus not.

00:38:35.180 --> 00:38:38.760
CRAIG
Yeah. Jensen Huang famously&nbsp;&nbsp;

00:38:38.760 --> 00:38:46.440
sort of pooh-poohed the need to learn to code&nbsp;
for future generations, but you're going to need&nbsp;&nbsp;

00:38:46.440 --> 00:38:56.400
people that do that curation, if nothing else, so&nbsp;
that you have clean code going in to these models.

00:38:56.400 --> 00:39:02.120
PETER
Look, Iâ€™d be careful about responding to things&nbsp;&nbsp;

00:39:02.120 --> 00:39:08.920
that sound good in marketing that aren't actually&nbsp;
based on reality. And I say this as somebody who&nbsp;&nbsp;

00:39:08.920 --> 00:39:17.280
cut my teeth building brands and building&nbsp;
companies around a reputation. The reality&nbsp;&nbsp;

00:39:17.280 --> 00:39:22.760
is that you are absolutely going to have to know&nbsp;
how to write code, you are. You're going to have&nbsp;&nbsp;

00:39:22.760 --> 00:39:27.600
to at least know how to read it and know that it's&nbsp;
performing the way you want it toâ€“ test it, all of&nbsp;&nbsp;

00:39:27.600 --> 00:39:36.200
those things. I think the reality is that curating&nbsp;
the training data alone means not just that you&nbsp;&nbsp;

00:39:36.200 --> 00:39:41.040
have to understand the code, you have to be able&nbsp;
to actually assess its quality and capability.&nbsp;&nbsp;

00:39:41.800 --> 00:39:46.160
I'll give you a real example of where we do this&nbsp;
today. You look at open source projects, that's&nbsp;&nbsp;

00:39:46.160 --> 00:39:52.280
a great example of this. Open source communities&nbsp;
are filled with hundreds, thousands, or millions&nbsp;&nbsp;

00:39:52.280 --> 00:39:56.320
of people issuing pull requests. Somebody&nbsp;
still has to go and make sure that that code&nbsp;&nbsp;

00:39:56.320 --> 00:40:03.920
reaches a certain standard. And depending upon the&nbsp;
criticality and the expectations of each of those&nbsp;&nbsp;

00:40:03.920 --> 00:40:11.120
projects, that bar may be pretty high. I operated&nbsp;
in two very different open source communities for&nbsp;&nbsp;

00:40:11.120 --> 00:40:18.960
a lot of my career: so the Drupal community, LAMP&nbsp;
stack, web content management, and then NGINX. In&nbsp;&nbsp;

00:40:18.960 --> 00:40:25.000
the Drupal community, it's not mission critical&nbsp;
for the most part; t's just web experiences.&nbsp;&nbsp;

00:40:25.000 --> 00:40:30.240
There is a standard for software development but&nbsp;
they also really encourage more contribution from&nbsp;&nbsp;

00:40:30.240 --> 00:40:37.440
people who are not computer scientists. So the bar&nbsp;
to get into that code base isâ€“ I wouldn't say low&nbsp;&nbsp;

00:40:37.440 --> 00:40:43.880
but it's definitely approachable and accessible.&nbsp;
NGINX, I can count on literally on my fingers and&nbsp;&nbsp;

00:40:43.880 --> 00:40:48.480
toes, the number of humans who've ever been&nbsp;
allowed to contribute code to that project&nbsp;&nbsp;

00:40:48.480 --> 00:40:54.640
because of how high performance it is, how mission&nbsp;
critical it is, how small it needs to be. It runs&nbsp;&nbsp;

00:40:54.640 --> 00:41:00.000
80% of the internet. So you actually have to&nbsp;
really be strict about what gets allowed in&nbsp;&nbsp;

00:41:00.000 --> 00:41:06.440
and what doesn't. So I think this training data,&nbsp;
the models themselves, and what they're based on,&nbsp;&nbsp;

00:41:06.440 --> 00:41:10.880
it's going to have a lot of those same super&nbsp;
high standards. It's going to have to in order&nbsp;&nbsp;

00:41:10.880 --> 00:41:17.580
for us to be secure and trust that it's actually&nbsp;
going to be able to work the way we want it to.

00:41:17.580 --> 00:41:22.880
CRAIG
Yeah. What kind of volume is required to train&nbsp;&nbsp;

00:41:24.320 --> 00:41:36.600
an autonomous code agent? Is it something that&nbsp;
a team of curators can come up with in a year?

00:41:36.600 --> 00:41:38.640
PETER
I couldn't even&nbsp;&nbsp;

00:41:38.640 --> 00:41:45.560
answer that question. I think the reality is that&nbsp;
we have some autonomy today. We're still figuring&nbsp;&nbsp;

00:41:45.560 --> 00:41:50.520
out where the edges are and we're still figuring&nbsp;
out what the gaps in knowledge are within that.&nbsp;&nbsp;

00:41:52.000 --> 00:42:01.000
The thing I would remember is, LLMs, why were&nbsp;
they successful with software early on? Why was&nbsp;&nbsp;

00:42:01.000 --> 00:42:06.960
that the first use case that actually worked?&nbsp;
Because what LLMs are best at are things that&nbsp;&nbsp;

00:42:06.960 --> 00:42:13.320
are incredibly well structured, are finite,&nbsp;
and well understood. And what is more finite,&nbsp;&nbsp;

00:42:13.320 --> 00:42:19.000
structured, and well understood in language&nbsp;
than programming languages? So when we see&nbsp;&nbsp;

00:42:19.000 --> 00:42:24.280
the performance of these models and software, I&nbsp;
think we're already seeing, okay, we've got more&nbsp;&nbsp;

00:42:24.280 --> 00:42:28.960
than enough training data. The LLMs are not&nbsp;
going to be where the advantage is anymore,&nbsp;&nbsp;

00:42:28.960 --> 00:42:33.720
and we're seeing it. I think we assumed that there&nbsp;
would be some level of commoditization on the&nbsp;&nbsp;

00:42:33.720 --> 00:42:39.640
LLMs. We are already seeing it. We have switchable&nbsp;
models within Tabnine. So you can run OpenAI,&nbsp;&nbsp;

00:42:39.640 --> 00:42:45.560
you can run Anthropic, you can run our models,&nbsp;
you can run Mistral, the different Mistral models;&nbsp;&nbsp;

00:42:45.560 --> 00:42:50.120
we're going to roll out some others. We're already&nbsp;
seeing that the relative performance between them,&nbsp;&nbsp;

00:42:50.120 --> 00:42:57.280
when given the appropriate prompts is very minimal&nbsp;
difference, very minimal difference. So give it&nbsp;&nbsp;

00:42:57.280 --> 00:43:02.800
a reasonable period of time, the LLM is no longer&nbsp;
going to be the thing that differentiates the&nbsp;&nbsp;

00:43:02.800 --> 00:43:07.280
application. It's going to be, what are you doing&nbsp;
about it; what are you doing to actually shape it?&nbsp;&nbsp;

00:43:07.280 --> 00:43:12.160
If the LLM is knowledge and insight, how are&nbsp;
you actually interacting with that knowledge&nbsp;&nbsp;

00:43:12.160 --> 00:43:17.200
and insight to actually generate something that is&nbsp;
useful and appropriate for your use case? And this&nbsp;&nbsp;

00:43:17.200 --> 00:43:22.640
has always been the case. You think about all of&nbsp;
software before this doesn't look that different&nbsp;&nbsp;

00:43:22.640 --> 00:43:28.040
from what we're dealing with with AI. It's a three&nbsp;
legged stool: What's the model, what's the data,&nbsp;&nbsp;

00:43:28.040 --> 00:43:32.480
and then what's the application and use case&nbsp;
in order to make it perform? The models are&nbsp;&nbsp;

00:43:32.480 --> 00:43:37.640
becoming commodity. The data you're using now to&nbsp;
shape that experience becomes even more critical.&nbsp;&nbsp;

00:43:37.640 --> 00:43:42.040
That's where we lean into things like context and&nbsp;
institutional knowledge inside of an enterprise&nbsp;&nbsp;

00:43:42.040 --> 00:43:48.040
and an engineering team. Then there's all of the&nbsp;
work; I think when we talk about full autonomy,&nbsp;&nbsp;

00:43:48.040 --> 00:43:53.720
around what then is that experience that somebody&nbsp;
who's trying to build the application, what is it&nbsp;&nbsp;

00:43:53.720 --> 00:43:58.160
that they're doing? What are they having to&nbsp;
ask? What additional information is required?&nbsp;&nbsp;

00:43:58.160 --> 00:44:01.869
What is the back and forth with the AI agent in&nbsp;
order to be able to generate that application?

00:44:01.869 --> 00:44:02.960
CRAIG 44:25
Yeah. Although&nbsp;&nbsp;

00:44:04.080 --> 00:44:08.960
when you say the models are becoming&nbsp;
commoditized, those are the pre trained&nbsp;&nbsp;

00:44:08.960 --> 00:44:17.480
models. But if you trained a model from&nbsp;
scratch, the architectures are commoditized,&nbsp;&nbsp;

00:44:17.480 --> 00:44:31.400
but if you took the algorithms and&nbsp;
trained them on highly curated code,&nbsp;&nbsp;

00:44:31.400 --> 00:44:39.840
you presumably would not need to then add fine&nbsp;
tuning or connect it to a RAG. It would have thatâ€“

00:44:39.840 --> 00:44:42.920
PETER
No, that's incorrect. That's incorrect. I mean,&nbsp;&nbsp;

00:44:42.920 --> 00:44:53.440
the thing that I think we as users really seem to&nbsp;
struggle with is not understanding the value and&nbsp;&nbsp;

00:44:53.440 --> 00:45:01.320
importance of the prompt. What questions do you&nbsp;
ask and how you ask those questions, what your&nbsp;&nbsp;

00:45:01.320 --> 00:45:08.280
expectations are, and what incremental insight&nbsp;
you provide as you do that, that is what makes a&nbsp;&nbsp;

00:45:08.280 --> 00:45:16.280
generative application successful or not. It is&nbsp;
that simple. And I challenge you once again; If&nbsp;&nbsp;

00:45:16.280 --> 00:45:22.280
you're unsure about that, go and use one of any,&nbsp;
not just us, any of the generative AI applications&nbsp;&nbsp;

00:45:22.280 --> 00:45:27.640
to accomplish a task, then do the same thing&nbsp;
through the chat UI straight into the LLM and&nbsp;&nbsp;

00:45:27.640 --> 00:45:32.960
go look at the difference. You can even do this,&nbsp;
for example in Tabnine, you can do it today. So we&nbsp;&nbsp;

00:45:32.960 --> 00:45:39.400
have the ability to turn on and off the workspace&nbsp;
awareness. So you can just go free trial, Tabnine,&nbsp;&nbsp;

00:45:39.400 --> 00:45:45.040
90 day free trial, go and use it, hook it into&nbsp;
your IDE, and then we have a toggle, [workspace on&nbsp;&nbsp;

00:45:45.040 --> 00:45:51.560
or off]. Open up a project, ask it a question with&nbsp;
workspace off, you'll get an academically accurate&nbsp;&nbsp;

00:45:51.560 --> 00:45:57.240
answer that may or may not be hyper-relevant to&nbsp;
you, usually not. It's going to be a generalized&nbsp;&nbsp;

00:45:57.240 --> 00:46:02.800
answer. You ask the right function, it'll give you&nbsp;
a good function. Then turn on workspace awareness&nbsp;&nbsp;

00:46:02.800 --> 00:46:07.800
and then you'll all of a sudden see, [oh, I&nbsp;
get it. You are writing a function in this way;&nbsp;&nbsp;

00:46:07.800 --> 00:46:13.000
you do it this same way ten times already in this&nbsp;
application against this API. So that's what I'm&nbsp;&nbsp;

00:46:13.000 --> 00:46:19.880
going to now recommend]. Take that to the next&nbsp;
logical step for writing an application. Unless&nbsp;&nbsp;

00:46:19.880 --> 00:46:26.520
you are writing the code itself, it's going to&nbsp;
have to make a number of decisions and have to&nbsp;&nbsp;

00:46:26.520 --> 00:46:31.520
follow certain assumptions around you, what&nbsp;
you're trying to achieve, who your user is,&nbsp;&nbsp;

00:46:31.520 --> 00:46:36.240
what it's for, how it's structured, and what&nbsp;
datasets to use, and what data sources to rely on,&nbsp;&nbsp;

00:46:36.240 --> 00:46:40.520
and where to read and write things. There's&nbsp;
so much that goes into that from an autonomy&nbsp;&nbsp;

00:46:40.520 --> 00:46:44.640
perspective. Where do you think that's going to&nbsp;
come from? It's not going to be native in the&nbsp;&nbsp;

00:46:44.640 --> 00:46:52.075
LLM. Personalization and having these agents be&nbsp;
aware of you, that is the future of generative AI.

00:46:52.075 --> 00:46:53.440
CRAIG
Yeah. And again,&nbsp;&nbsp;

00:46:54.440 --> 00:46:56.840
the way you guys are doing it is through RAG?

00:46:56.840 --> 00:46:57.600
PETER
RAG,&nbsp;&nbsp;

00:46:57.600 --> 00:47:00.760
plus semantic memory, plus&nbsp;
prompt engineering; I mean,&nbsp;&nbsp;

00:47:00.760 --> 00:47:04.320
there's a bunch of components that can use&nbsp;
it technically. And I will butcher it if&nbsp;&nbsp;

00:47:04.320 --> 00:47:11.040
you ask me to explain it. Itâ€™s a great follow up&nbsp;
conversation with our CTO or our model builders.

00:47:11.040 --> 00:47:12.200
CRAIG
Are there any case&nbsp;&nbsp;

00:47:12.200 --> 00:47:20.280
studies that you can talk about where Tabnine has&nbsp;
improved productivity by some particular metric?

00:47:20.280 --> 00:47:22.080
PETER
Yeah, we've seen it. I mean,&nbsp;&nbsp;

00:47:22.080 --> 00:47:27.560
even just looking at how long we've been in use,&nbsp;
how many developers we've had, and how much code&nbsp;&nbsp;

00:47:27.560 --> 00:47:34.960
automation we have, we've probably written 1 to&nbsp;
2% of all of the world's code at this point. So&nbsp;&nbsp;

00:47:34.960 --> 00:47:39.400
it's a staggering number when you actually look at&nbsp;
how many users we've had over the last six years&nbsp;&nbsp;

00:47:39.400 --> 00:47:44.760
and how much has been done. I'll give you a few&nbsp;
statistics that are interesting and noteworthy.&nbsp;&nbsp;

00:47:44.760 --> 00:47:47.840
I'll give you some that are Tabnine and some&nbsp;
that are just category, because it's actually&nbsp;&nbsp;

00:47:47.840 --> 00:47:54.600
very interesting when you look at the category.&nbsp;
We generate somewhere between 30% to 50% of all&nbsp;&nbsp;

00:47:54.600 --> 00:47:59.400
of the code on the projects that we touch for&nbsp;
our customers today. Once you get to a certain&nbsp;&nbsp;

00:47:59.400 --> 00:48:05.520
level of maturity and using it, that's the rough&nbsp;
range that we're at. The productivity impacts on&nbsp;&nbsp;

00:48:05.520 --> 00:48:11.680
that are very individual to individual teams.&nbsp;
So, rather than explain what I see at Tabnine,&nbsp;&nbsp;

00:48:12.480 --> 00:48:17.600
let me give you Carnegie Mellon's view of this.&nbsp;
Let me give you McKinsey's view of this, or IBM&nbsp;&nbsp;

00:48:17.600 --> 00:48:21.720
research. There's a bunch of really phenomenal&nbsp;
third party research that's been done around&nbsp;&nbsp;

00:48:21.720 --> 00:48:28.440
this where they've actually done true laboratory&nbsp;
testing around how much automation are we seeing&nbsp;&nbsp;

00:48:28.440 --> 00:48:37.160
for what tasks in software development? Where&nbsp;
does a developer usually spend their time? All&nbsp;&nbsp;

00:48:37.160 --> 00:48:43.080
the vendors talk about 50, 60% autonomous code&nbsp;
generation at our peak, but how much time are we&nbsp;&nbsp;

00:48:43.080 --> 00:48:47.200
really spending writing code, versus reviewing&nbsp;
code, versus maintaining code? So I love these&nbsp;&nbsp;

00:48:47.200 --> 00:48:53.080
third party studies. According to the studies,&nbsp;
and they all land at roughly the same number&nbsp;&nbsp;

00:48:53.080 --> 00:48:56.080
regardless of which tool they're using, which&nbsp;
is actually really interesting because all of us&nbsp;&nbsp;

00:48:56.080 --> 00:49:05.280
are very similar in performance as tools; 20% to&nbsp;
25% real world productivity savings for software&nbsp;&nbsp;

00:49:05.280 --> 00:49:12.360
engineering teams, 20% to 25%. I mean, you think&nbsp;
about how much effort we would put in to get 5%,&nbsp;&nbsp;

00:49:12.360 --> 00:49:20.240
10%. To be able to save one-fifth or a quarter&nbsp;
of someone's time and be able to redirect that&nbsp;&nbsp;

00:49:20.240 --> 00:49:24.400
towards your tech debt, redirect that&nbsp;
towards improving the application as&nbsp;&nbsp;

00:49:24.400 --> 00:49:30.280
opposed to maintaining the application. I think&nbsp;
this is incredible. When I first joined Tabnine,&nbsp;&nbsp;

00:49:30.280 --> 00:49:34.120
I've only been in the company now seven, eight&nbsp;
months. The very first thing my sales team asked&nbsp;&nbsp;

00:49:34.120 --> 00:49:38.960
me for was basically an ROI calculator. They&nbsp;
want to understand what it was. That's how I&nbsp;&nbsp;

00:49:38.960 --> 00:49:45.280
ended up going deep on all these third party&nbsp;
research studies. It was staggering when I did&nbsp;&nbsp;

00:49:45.280 --> 00:49:51.080
the ROI calculator because Tabnine is $39 per&nbsp;
user, per month for our enterprise product,&nbsp;&nbsp;

00:49:51.080 --> 00:49:58.760
$12 per user, per month for our pro product. The&nbsp;
productivity savings we're talking about are 50,&nbsp;&nbsp;

00:49:58.760 --> 00:50:05.320
60, $70,000 per engineer for a product that&nbsp;
costs you hundreds. So it becomes kind of a&nbsp;&nbsp;

00:50:05.320 --> 00:50:09.840
joke when you look at it, where you're like,&nbsp;
this is a fundamentally better way of working.&nbsp;&nbsp;

00:50:09.840 --> 00:50:16.440
The ROI is sort of clear. The question is,&nbsp;
not ROI, but where and how do you deploy&nbsp;&nbsp;

00:50:16.440 --> 00:50:21.800
it and then what do you do with that time&nbsp;
savings? Where do you reinvest that energy?

00:50:21.800 --> 00:50:25.240
CRAIG
Yeah. Because it could be, as you said,&nbsp;&nbsp;

00:50:25.240 --> 00:50:31.520
in code review or something like that, but it&nbsp;
could also be in writing just more code right?

00:50:31.520 --> 00:50:33.280
PETER
That's exactly what we're seeing. So&nbsp;&nbsp;

00:50:33.280 --> 00:50:38.480
we work with a hedge fund, as you mentioned, hedge&nbsp;
funds and these folks are incredibly competitive,&nbsp;&nbsp;

00:50:38.480 --> 00:50:41.840
like incredibly, incredibly competitive.&nbsp;
Anytime I think we're competitive in software,&nbsp;&nbsp;

00:50:41.840 --> 00:50:48.400
I then go look at financial services and I realize&nbsp;
we're nothing. The reality is they've been using&nbsp;&nbsp;

00:50:48.400 --> 00:50:52.240
us for quite some time. They've got it in use&nbsp;
across their entire engineering team. That's what&nbsp;&nbsp;

00:50:52.240 --> 00:50:59.160
they've been doing with it. What they've reported&nbsp;
back to us is they are increasing the burn down&nbsp;&nbsp;

00:50:59.160 --> 00:51:05.480
of their tech debt and increasing their time to&nbsp;
market for new features and for new capabilities.&nbsp;&nbsp;

00:51:06.320 --> 00:51:11.000
That's what we're all looking for. We're all&nbsp;
looking for that little bit more competitive edge,&nbsp;&nbsp;

00:51:11.640 --> 00:51:18.600
that little bit more capability and convenience in&nbsp;
what we do every day. Or, frankly with the layoffs&nbsp;&nbsp;

00:51:18.600 --> 00:51:23.600
we saw last year, tech debt increasing, and all&nbsp;
the pain that's come, if you're an engineering&nbsp;&nbsp;

00:51:23.600 --> 00:51:26.960
manager you're looking at your teams like,&nbsp;
my teams are already working 10 hours a day,&nbsp;&nbsp;

00:51:26.960 --> 00:51:30.600
they're getting burnt out, the quality&nbsp;
is dropping because of that, I'm losing&nbsp;&nbsp;

00:51:30.600 --> 00:51:37.480
my best people. I look at these agents as&nbsp;
coming at just the right time to be able to&nbsp;&nbsp;

00:51:37.480 --> 00:51:41.560
actually take some of the weight off of some&nbsp;
of these people. So it's not even necessarily&nbsp;&nbsp;

00:51:41.560 --> 00:51:45.840
that you're going to take that 20% savings and&nbsp;
reinvest it. You're going to have your team,&nbsp;&nbsp;

00:51:45.840 --> 00:51:48.980
at least in startup land, you're not going&nbsp;
to have your team working 10, 12 hours a day.

00:51:48.980 --> 00:51:50.040
CRAIG
Okay, we're&nbsp;&nbsp;

00:51:50.040 --> 00:51:57.640
up to about an hour. Is there anything I didn't&nbsp;
cover that you would like to have listeners hear?

00:51:58.320 --> 00:52:00.640
PETER
You've done a great job Craig,&nbsp;&nbsp;

00:52:00.640 --> 00:52:06.400
with covering the innovation in the space. I&nbsp;
think your interrogation of the data side is&nbsp;&nbsp;

00:52:06.400 --> 00:52:11.120
very interesting because I think there's a lack of&nbsp;
understanding around how this stuff really works,&nbsp;&nbsp;

00:52:11.120 --> 00:52:16.120
which I think is leaving us in a place where&nbsp;
there's notâ€“ there's almost unrealistic&nbsp;&nbsp;

00:52:16.120 --> 00:52:22.360
expectations with what the LLMs can do&nbsp;
out of the box. That risks of putting us&nbsp;&nbsp;

00:52:22.360 --> 00:52:26.520
into a trough of disillusionment with these&nbsp;
things because we have inflated expectations&nbsp;&nbsp;

00:52:28.000 --> 00:52:33.560
out of context. So I think that's really good.&nbsp;
I would throw one other thing out for you,&nbsp;&nbsp;

00:52:33.560 --> 00:52:37.920
which is actually, I think the biggest existential&nbsp;
crisis in generative AI today is actually, we have&nbsp;&nbsp;

00:52:37.920 --> 00:52:45.440
a trust crisis. I think this is a real issue and&nbsp;
we hear it in a very different way. I think most&nbsp;&nbsp;

00:52:45.440 --> 00:52:48.960
of the time when people look at our category,&nbsp;
they think of Copilot, they think of GitHub.&nbsp;&nbsp;

00:52:48.960 --> 00:52:53.920
GitHub came in after us but with a giant megaphone&nbsp;
and Microsoft's deep pockets, they've been able&nbsp;&nbsp;

00:52:53.920 --> 00:52:59.560
to go and insert themselves into engineering&nbsp;
teams all over the world. But at the same time,&nbsp;&nbsp;

00:52:59.560 --> 00:53:06.440
what we hear when we talk to Fortune 500 companies&nbsp;
is they don't trust what those companies are&nbsp;&nbsp;

00:53:06.440 --> 00:53:13.440
doing. They don't trust their behaviors towards&nbsp;
training data, and they don't trust what's&nbsp;&nbsp;

00:53:13.440 --> 00:53:19.360
happening with how the models were built. They&nbsp;
don't trust that the CTO of OpenAI can't explain&nbsp;&nbsp;

00:53:19.360 --> 00:53:26.640
in front of a journalist what it was trained on.&nbsp;
I can give you a list today of every single piece&nbsp;&nbsp;

00:53:26.640 --> 00:53:34.640
of code that Tabnine was trained on. So why are&nbsp;
they not answering that question? Then you look&nbsp;&nbsp;

00:53:34.640 --> 00:53:38.920
at some of the behaviors. There's a great New&nbsp;
York Times piece where they actually exposed the&nbsp;&nbsp;

00:53:38.920 --> 00:53:43.280
big tech brands changing their terms of service&nbsp;
so they can go and exploit the data. They showed&nbsp;&nbsp;

00:53:43.280 --> 00:53:48.360
them actually violating each other's terms of&nbsp;
service. There are active lawsuits against OpenAI,&nbsp;&nbsp;

00:53:48.360 --> 00:53:53.760
GitHub and others for what they've been doing with&nbsp;
regards to everything from journalist content,&nbsp;&nbsp;

00:53:53.760 --> 00:53:59.800
to imagery, to code. There's a ton of stuff&nbsp;
out there. And I think there's a problem with&nbsp;&nbsp;

00:53:59.800 --> 00:54:06.520
all of this, there's a problem with all of this,&nbsp;
which is we are risking stunting our own future&nbsp;&nbsp;

00:54:06.520 --> 00:54:13.480
as technology brands by not doing more to build&nbsp;
trust, by not doing more to respect our users,&nbsp;&nbsp;

00:54:13.480 --> 00:54:21.520
by not doing more to respect our creators. So I&nbsp;
think that's something as an individualâ€“ forget&nbsp;&nbsp;

00:54:21.520 --> 00:54:26.520
even my role at Tabnine for a minute; I've been&nbsp;
in technology now for 30 years, and I've been in&nbsp;&nbsp;

00:54:26.520 --> 00:54:32.000
multiple waves of innovation: the birth of the&nbsp;
web in 94, the birth of an explosion of open&nbsp;&nbsp;

00:54:32.000 --> 00:54:36.800
source in the 2000s and all that. And I've always&nbsp;
personally felt like I had two responsibilities.&nbsp;&nbsp;

00:54:36.800 --> 00:54:41.600
I had a responsibility as an entrepreneur to build&nbsp;
a great business, but I had a responsibility as a&nbsp;&nbsp;

00:54:41.600 --> 00:54:48.520
citizen and technical thought leader to see that&nbsp;
we're doing things the right way and that we're&nbsp;&nbsp;

00:54:48.520 --> 00:54:53.600
doing things in a way that actually betters&nbsp;
us all. I think we are really at risk right&nbsp;&nbsp;

00:54:53.600 --> 00:55:00.560
now with generative AI really benefiting a very&nbsp;
small few and hurting the others. And frankly,&nbsp;&nbsp;

00:55:01.080 --> 00:55:06.040
I feel like that is shortsighted thinking that&nbsp;
ultimately hurts all of us. It's a tragedy of&nbsp;&nbsp;

00:55:06.040 --> 00:55:13.200
the commons. So I'm hopeful that my peers and&nbsp;
others wake up and realize that building trust,&nbsp;&nbsp;

00:55:13.200 --> 00:55:16.600
not breaking trust is a critical part of&nbsp;
what we're doing; building relationships,&nbsp;&nbsp;

00:55:16.600 --> 00:55:20.560
not exploiting relationships is a critical part&nbsp;
of what we're doing, and that the real success&nbsp;&nbsp;

00:55:20.560 --> 00:55:26.560
of AI will come when we're building&nbsp;
opportunities for everyone to benefit&nbsp;&nbsp;

00:55:26.560 --> 00:55:29.880
from it. I know that's certainly the&nbsp;
attitude that myself and Tabnine take.

