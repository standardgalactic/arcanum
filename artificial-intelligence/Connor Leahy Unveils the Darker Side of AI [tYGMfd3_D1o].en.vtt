WEBVTT
Kind: captions
Language: en

00:00:00.120 --> 00:00:02.760
 Assume you have a system. Do you know&nbsp;
it's smarter than you? It's smarter than&nbsp;&nbsp;

00:00:02.760 --> 00:00:04.920
all of your friends. It's smarter than the&nbsp;
government. It's smarter than everybody,&nbsp;&nbsp;

00:00:04.920 --> 00:00:09.720
right? And you turn it on to check whether it&nbsp;
will do a bad thing. If it does a bad thing,&nbsp;&nbsp;

00:00:09.720 --> 00:00:13.380
it's too late. It's smarter than&nbsp;
you. How do you … you can't stop it.

00:00:13.380 --> 00:00:18.900
It's smarter than you. It tricks, it'll trick&nbsp;
you. OpenAI gives access to Zapier through&nbsp;&nbsp;

00:00:18.900 --> 00:00:25.380
the ChatGPT plugins, Zapier gives you access to&nbsp;
Twitter, YouTube, LinkedIn, Instagram to all of&nbsp;&nbsp;

00:00:25.380 --> 00:00:31.140
the social networks through nice, simple API&nbsp;
interfaces. So, if you have ChatGPT plugins,&nbsp;&nbsp;

00:00:31.140 --> 00:00:35.400
you don't even have to implement this in&nbsp;
your hacky little, you know, Python script.

00:00:35.400 --> 00:00:41.880
You can just use the official OpenAI tools. These&nbsp;
people are racing. Let's be clear. They're racing&nbsp;&nbsp;

00:00:41.880 --> 00:00:47.160
for their own personal gain, for their own&nbsp;
glory towards an existential catastrophe.

00:00:47.160 --> 00:00:54.360
The first thing I'm going to have you&nbsp;
do is introduce yourself and give a&nbsp;&nbsp;

00:00:54.360 --> 00:01:00.480
little bit of your background. Tell us&nbsp;
about EleutherAI, how you started that,&nbsp;&nbsp;

00:01:00.480 --> 00:01:04.500
how you left that, and how&nbsp;
you, what you're doing now.

00:01:04.500 --> 00:01:07.560
And then I'll start asking questions. Okay.

00:01:07.560 --> 00:01:13.380
Yeah, sounds great to me. So, I'm Connor.&nbsp;
I'm most well known as one of the original&nbsp;&nbsp;

00:01:13.380 --> 00:01:19.620
founders of EleutherAI, which was a large&nbsp;
open-source ML collective, I mean, still is.&nbsp;&nbsp;

00:01:20.700 --> 00:01:24.900
We built some of the first like&nbsp;
large open-source language models,&nbsp;&nbsp;

00:01:24.900 --> 00:01:27.900
did a bunch of research, published a&nbsp;
bunch of papers, did a bunch of fun stuff.

00:01:30.720 --> 00:01:33.780
After that, I did that for quite a while.&nbsp;&nbsp;

00:01:35.580 --> 00:01:42.840
Then, I also briefly worked in Germany, a company&nbsp;
called Aleph Alpha where I did research. And now,&nbsp;&nbsp;

00:01:42.840 --> 00:01:51.120
just about a year ago, I raised money to start&nbsp;
a new company called Conjecture. Conjecture is&nbsp;&nbsp;

00:01:51.120 --> 00:01:57.240
my current startup. I'm the CEO of Conjecture,&nbsp;
and we work on primarily AI alignment and we,&nbsp;&nbsp;

00:01:57.240 --> 00:02:01.920
I would describe conjecture as a mission-driven,&nbsp;
not as a thesis-driven organisation.

00:02:01.920 --> 00:02:04.680
Our goal is to make AI go well;&nbsp;&nbsp;

00:02:05.280 --> 00:02:10.260
you know? Okay. Whether that's exactly, you know,&nbsp;
just alignment or other things, whatever. We do,&nbsp;&nbsp;

00:02:10.260 --> 00:02:15.360
what needs to be done to improve the chances of&nbsp;
things going well. And we're pretty agnostic to&nbsp;&nbsp;

00:02:15.360 --> 00:02:20.220
how we do that. Happy to go into more details&nbsp;
about exactly what we do and so on later.

00:02:20.220 --> 00:02:25.680
But yeah, so I've been doing that for about a year&nbsp;
now. I have recently officially stepped down from&nbsp;&nbsp;

00:02:25.680 --> 00:02:32.160
EleutherAI. I was still hanging out, you know,&nbsp;
at least partially as a figurehead. And now I&nbsp;&nbsp;

00:02:32.160 --> 00:02:37.020
have officially stepped down and left it in the&nbsp;
hands of my good friends, who I'm sure will lead&nbsp;&nbsp;

00:02:37.740 --> 00:02:40.080
EleutherAI, which is now officially a nonprofit.

00:02:40.080 --> 00:02:43.500
It was not a nonprofit before. It&nbsp;
had never had an official entity&nbsp;&nbsp;

00:02:43.500 --> 00:02:48.240
before. It is now an official entity with&nbsp;
actual employees, run by Stella Biderman,&nbsp;&nbsp;

00:02:48.960 --> 00:02:53.340
Curtis Huebner, and Shivanshu Purohit,&nbsp;
and several other great people.

00:02:53.340 --> 00:02:59.040
Yeah. And anybody can join&nbsp;
the EleutherAI Discord server,&nbsp;&nbsp;

00:02:59.040 --> 00:03:03.840
is that right? Absolutely. And there's a lot&nbsp;
of very interesting things going on there.

00:03:05.040 --> 00:03:11.880
I to go back to the last time we talked, you&nbsp;
were building open source, large language&nbsp;&nbsp;

00:03:11.880 --> 00:03:21.420
models. You got up, pretty, pretty large and I&nbsp;
can't remember who was paying for the compute,&nbsp;&nbsp;

00:03:21.420 --> 00:03:27.720
but can you tell me where that project&nbsp;
stands first before we talk about Conjecture?

00:03:28.560 --> 00:03:32.220
So, for me, I consider the work I've&nbsp;
done there to be wrapped up. So,&nbsp;&nbsp;

00:03:32.220 --> 00:03:40.440
I don't work on anything related to that anymore.&nbsp;
And the main lead on that project, Sid Black,&nbsp;&nbsp;

00:03:40.440 --> 00:03:43.980
is now my co-founder at Conjecture.&nbsp;
So, he has left EleutherAI with me.&nbsp;&nbsp;

00:03:45.180 --> 00:03:52.260
So, we started our very earliest models. The&nbsp;
Neo models were, man, it's already a long&nbsp;&nbsp;

00:03:52.260 --> 00:03:55.980
time ago. They're not particularly wonderful,&nbsp;
great models. They were more like prototypes.

00:03:55.980 --> 00:04:01.800
The first really good model was the GPT-J&nbsp;
model, which was done mostly with Ben Wang.&nbsp;&nbsp;

00:04:02.880 --> 00:04:08.580
and fantastic models still work very well. I&nbsp;
think it's still one of the most downloaded&nbsp;&nbsp;

00:04:08.580 --> 00:04:14.220
language models to date. It's a very, very good&nbsp;
model, especially for its size. After that,&nbsp;&nbsp;

00:04:14.220 --> 00:04:21.600
we built the NeoX series, resulting in&nbsp;
the NeoX 20B model, which, you know,&nbsp;&nbsp;

00:04:21.600 --> 00:04:25.260
at the time was a very large and very&nbsp;
impressive and very good performing model.

00:04:25.260 --> 00:04:29.100
Nowadays, of course, with stuff like&nbsp;
LLaMA and OPT and stuff like this,&nbsp;&nbsp;

00:04:29.100 --> 00:04:33.720
you know, large corporations have now caught&nbsp;
up to open sourcing very large models. So,&nbsp;&nbsp;

00:04:33.720 --> 00:04:38.460
there's in a sense, not a need, not the&nbsp;
same kind of need or interest in these&nbsp;&nbsp;

00:04:38.460 --> 00:04:46.020
types of models as there were two to three&nbsp;
years ago. And, so now EleutherAI, the main,&nbsp;&nbsp;

00:04:46.020 --> 00:04:50.520
as far as I'm aware, language modelling projects&nbsp;
are going on is the Pythia suite of models.

00:04:50.520 --> 00:04:56.160
So, these are a whole suite of models that are&nbsp;
made for scientific standards. So, the idea is&nbsp;&nbsp;

00:04:56.160 --> 00:05:00.060
not to just build, you know, arbitrary language&nbsp;
models, but to build language models that have&nbsp;&nbsp;

00:05:00.060 --> 00:05:03.960
controlled scientific parameters to train on&nbsp;
the same data in the same order, using the same&nbsp;&nbsp;

00:05:03.960 --> 00:05:09.660
parameters, you know, in a controlled setting&nbsp;
and you get many, many checkpoints with them.

00:05:09.660 --> 00:05:14.280
So instead of just getting the final model,&nbsp;
you can watch the model through the entire&nbsp;&nbsp;

00:05:14.280 --> 00:05:17.640
trading process, which is very interesting&nbsp;
scientifically. So, these models are optimised&nbsp;&nbsp;

00:05:17.640 --> 00:05:21.780
for scientific applications for peoples&nbsp;
who are interested in studying the actual&nbsp;&nbsp;

00:05:21.780 --> 00:05:26.340
properties of language models, which has always&nbsp;
been the core mission of EleutherAI has been to&nbsp;&nbsp;

00:05:26.340 --> 00:05:30.240
enable people and to encourage people to try&nbsp;
to understand these models better, to learn,&nbsp;&nbsp;

00:05:31.440 --> 00:05:38.400
to, to control, understand, disentangle&nbsp;
these models. The Pythia suite led by&nbsp;&nbsp;

00:05:38.400 --> 00:05:43.980
Stella Biderman is a great example&nbsp;
of taking this effort forward.

00:05:44.520 --> 00:05:53.880
Yeah. And, and then Conjecture, you said that the&nbsp;
alignment problem, but the alignment problem, in,&nbsp;&nbsp;

00:05:53.880 --> 00:06:02.040
in the context of AGI. Is that right? Yes. And can&nbsp;
you talk about the, the, I mean, are you building&nbsp;&nbsp;

00:06:02.040 --> 00:06:14.760
models or are you just writing about alignment&nbsp;
and, and, methods, to align large models. Yeah.

00:06:14.760 --> 00:06:17.640
So, we are very much a practical organisation.&nbsp;&nbsp;

00:06:18.360 --> 00:06:22.980
We hire many engineers and very good engineers,&nbsp;
and we have a lot of, some of the best engineers&nbsp;&nbsp;

00:06:22.980 --> 00:06:28.800
from EleutherAI with us. And we're always&nbsp;
looking for more engineers. We are always&nbsp;&nbsp;

00:06:28.800 --> 00:06:32.460
interested in talking to, especially people&nbsp;
with experience in high performance computing.

00:06:32.460 --> 00:06:37.360
And because this tends to be the bottleneck&nbsp;
actually in doing these experiments and scale&nbsp;&nbsp;

00:06:37.360 --> 00:06:43.800
is less so specific ML trivia and more&nbsp;
so debugging InfiniBand interconnects&nbsp;&nbsp;

00:06:43.800 --> 00:06:48.300
and profiling, you know, large scale runs on&nbsp;
supercomputing hardware and stuff like this.&nbsp;&nbsp;

00:06:49.740 --> 00:06:56.400
So, what, so we Conjecture, as I said, we are a&nbsp;
mission driven, not a thesis driven organization.

00:06:56.400 --> 00:06:58.020
So, at core, what we're interested in doing,&nbsp;&nbsp;

00:06:59.040 --> 00:07:06.060
is figuring out and then doing whatever needs to&nbsp;
get done to make things go well. So, we could talk&nbsp;&nbsp;

00:07:06.060 --> 00:07:10.500
about this a bit in a, in a, in a, in a bit. Like&nbsp;
why I believe these things will not go good, good&nbsp;&nbsp;

00:07:10.500 --> 00:07:14.940
by default, but I think on the current trajectory&nbsp;
that we currently are on, things are going very&nbsp;&nbsp;

00:07:14.940 --> 00:07:18.540
badly, and very bad things are going to&nbsp;
happen and are already beginning to happen.

00:07:18.540 --> 00:07:24.540
And I think any hope that we have, and by we,&nbsp;
I mean all of us, I don't just mean Conjecture,&nbsp;&nbsp;

00:07:24.540 --> 00:07:31.680
I mean all of mankind, how this goes well&nbsp;
for all of us. And you know, it will involve&nbsp;&nbsp;

00:07:32.340 --> 00:07:38.400
many things. It will involve policy, it will&nbsp;
involve techno, not technology. It will involve&nbsp;&nbsp;

00:07:39.360 --> 00:07:42.480
engineering. It'll involve&nbsp;
scientific breakthroughs.

00:07:43.080 --> 00:07:48.120
So, the alignment problem is at the core of&nbsp;
this as in a sense, what I believe is in a&nbsp;&nbsp;

00:07:48.120 --> 00:07:51.480
sense the most important, crucial problem to&nbsp;
be solved, which is the question of basically&nbsp;&nbsp;

00:07:51.480 --> 00:07:57.240
how do you make a very smart system, which might&nbsp;
be smarter than you do what you want it to do.&nbsp;&nbsp;

00:07:57.240 --> 00:08:03.720
And do that reliably. And this is the kind of&nbsp;
problem you can't solve interactively really.

00:08:03.720 --> 00:08:07.860
Cause like if you have a system that's&nbsp;
smarter than you, right? Hypothetically,&nbsp;&nbsp;

00:08:07.860 --> 00:08:11.520
you know, we can argue about whether this is&nbsp;
possible or when it will happen. It's like,&nbsp;&nbsp;

00:08:11.520 --> 00:08:15.600
but I like to assume such a system&nbsp;
existed. Assume you have a system,&nbsp;&nbsp;

00:08:15.600 --> 00:08:17.700
but you know it's smarter than you.&nbsp;
It's smarter than all of your friends.

00:08:17.700 --> 00:08:22.620
It's smarter than the government. It's smarter&nbsp;
than everybody, right? And you turn it on to&nbsp;&nbsp;

00:08:22.620 --> 00:08:28.080
check whether it will do a bad thing. If it does&nbsp;
a bad thing, it's too late. It's smarter than you.&nbsp;&nbsp;

00:08:28.080 --> 00:08:32.520
How do you say you can't stop it? It's smarter&nbsp;
than you. It's a trick. It'll trick you. Now,&nbsp;&nbsp;

00:08:32.520 --> 00:08:37.560
the interesting questions are, okay, why do you&nbsp;
expect it to do a bad thing in the first place?

00:08:37.560 --> 00:08:42.660
Why do you expect it to be smarter? Though those&nbsp;
are good, and why do you expect people to turn&nbsp;&nbsp;

00:08:42.660 --> 00:08:46.080
it on? Those are three very good questions that&nbsp;
I'd be happy to get into if you're interested.

00:08:46.620 --> 00:08:53.700
Yeah. One of the, and the, the, the&nbsp;
sort of central questions about,&nbsp;&nbsp;

00:08:54.780 --> 00:09:02.880
these, about Super intelligence,&nbsp;
is how easy it is, or difficult&nbsp;&nbsp;

00:09:02.880 --> 00:09:11.460
it'll be to keep such a system in a&nbsp;
sandbox without, because presumably,&nbsp;&nbsp;

00:09:11.460 --> 00:09:19.440
and, well, there are two issues. One is the&nbsp;
question of agency. Whether simply because a&nbsp;&nbsp;

00:09:19.440 --> 00:09:27.300
system is smarter than a human doesn't mean that&nbsp;
it has agency. It could be purely responsive. So,&nbsp;&nbsp;

00:09:27.300 --> 00:09:35.220
you, as, as the large language models are&nbsp;
now, you ask a question, and it responds.

00:09:36.480 --> 00:09:45.780
And, and so that question of agency is, is, one.&nbsp;
And then the, the question of how ring fenced,&nbsp;&nbsp;

00:09:45.780 --> 00:09:53.400
such a system is, even if it's being trained&nbsp;
on the internet, it doesn't necessarily,&nbsp;&nbsp;

00:09:54.840 --> 00:10:00.540
have access, proactive access&nbsp;
to, to the internet. So,&nbsp;&nbsp;

00:10:00.540 --> 00:10:04.440
just on those two questions,&nbsp;
what yeah, what would you say?

00:10:04.440 --> 00:10:08.040
So those are two really fun questions,&nbsp;
and the reason they're really fun to&nbsp;&nbsp;

00:10:08.040 --> 00:10:11.520
me is that if you had me ask me these&nbsp;
questions, like me three or four years ago,&nbsp;&nbsp;

00:10:11.520 --> 00:10:16.520
I would've had to go into all the complicated&nbsp;
arguments about why, you know, passive quote&nbsp;&nbsp;

00:10:16.520 --> 00:10:20.160
unquote systems are not necessarily safe. Where&nbsp;
the concept of agency doesn't really make sense.

00:10:20.160 --> 00:10:24.840
I would have to explain how sandbox escapes&nbsp;
work and whatever, but I don't need to do any&nbsp;&nbsp;

00:10:24.840 --> 00:10:29.700
of that anymore because just look at what&nbsp;
people are doing with these things. Look,&nbsp;&nbsp;

00:10:29.700 --> 00:10:35.700
just look at the top GitHub AI repositories&nbsp;
and you can see AutoGPT. You're going to see,&nbsp;&nbsp;

00:10:35.700 --> 00:10:38.940
you know, self recursively,&nbsp;
improving systems that spawn agents.

00:10:38.940 --> 00:10:43.380
You're going to see, go on archive right now.&nbsp;
Right now, go on archive, go to, you know,&nbsp;&nbsp;

00:10:43.380 --> 00:10:49.380
top CS papers, an AI paper, and you see LLM&nbsp;
Autonomous Agents. You're going to see, you know,&nbsp;&nbsp;

00:10:49.380 --> 00:10:54.300
gameplay simulation, simulacra systems.&nbsp;
You're going to see people hooking them up&nbsp;&nbsp;

00:10:54.300 --> 00:10:59.640
to the internet to bash cells, to Wolfram&nbsp;
Alpha to every single tool in the world.

00:10:59.640 --> 00:11:05.100
So, while we could, if you wanted to go into all&nbsp;
the deep philosophical problems like that, okay,&nbsp;&nbsp;

00:11:05.100 --> 00:11:09.720
even if we sandboxed it, and even if we were&nbsp;
very careful, maybe it's still unsafe. It&nbsp;&nbsp;

00:11:09.720 --> 00:11:13.440
doesn't fucking matter because people are&nbsp;
not being safe and they're not going to No,&nbsp;&nbsp;

00:11:13.440 --> 00:11:20.640
like people like we have, like I remember fondly&nbsp;
the times when me and my friends in our online&nbsp;&nbsp;

00:11:20.640 --> 00:11:27.180
little weird nerd caves, we have these long&nbsp;
debates, but how an AI would escape from a box.

00:11:27.180 --> 00:11:33.900
But what if we do this, but what if we do clever&nbsp;
things and whatever. But in the real world, the&nbsp;&nbsp;

00:11:33.900 --> 00:11:40.080
moment a system was built, which looked vaguely,&nbsp;
sort of, maybe a little bit smart, the first thing&nbsp;&nbsp;

00:11:40.080 --> 00:11:45.420
everyone did is, is hook it up to every single&nbsp;
fucking thing on the internet. So, jokes on me.

00:11:45.420 --> 00:11:51.060
Yeah. Although, give me a concrete example&nbsp;&nbsp;

00:11:52.080 --> 00:12:01.380
of, someone hooking GPT-4 up to the internet&nbsp;
and giving It agency, I haven't seen that.

00:12:01.380 --> 00:12:05.100
So, go on Github.com and&nbsp;
search for AutoGPT. AutoGPT&nbsp;&nbsp;

00:12:08.520 --> 00:12:17.580
or also or look for BabyAGI. That's another&nbsp;
one. Go on the blog post, with the Pinecone&nbsp;&nbsp;

00:12:18.300 --> 00:12:23.340
vector dataset go to, what was the paper that came&nbsp;
out today that was really fun? It was about video&nbsp;&nbsp;

00:12:23.340 --> 00:12:28.920
games, generative agents interacting simulacra&nbsp;
of human behaviour is from Stanford and Google.&nbsp;&nbsp;

00:12:30.060 --> 00:12:32.520
Is that enough or should I go find some more?

00:12:32.520 --> 00:12:37.560
No, well, explain. Pick one of those and&nbsp;
explain to me what it's really doing. Not,&nbsp;&nbsp;

00:12:37.560 --> 00:12:39.840
not, not what it sounds like it's doing.

00:12:40.440 --> 00:12:45.240
Let's explain, for example, AutoGPT, which is&nbsp;
kind of the simplest way you could do this.

00:12:45.240 --> 00:12:50.280
AutoGPT creates a prompt for GPT-4, which explains&nbsp;&nbsp;

00:12:50.940 --> 00:12:57.600
you are an agent trying to achieve a goal.&nbsp;
And then this is written by the user as&nbsp;&nbsp;

00:12:57.600 --> 00:13:03.180
some goal that it might have, and it gives&nbsp;
it a list of things it's allowed, it can&nbsp;&nbsp;

00:13:03.180 --> 00:13:11.220
do. Among these are adding things to memory,&nbsp;
Google things run, piece of code, et cetera.

00:13:11.220 --> 00:13:15.360
I'm actually not sure if the run piece of code&nbsp;
is in AutoGPT, but it's in some of them. There's&nbsp;&nbsp;

00:13:15.360 --> 00:13:19.080
a bunch of these. This is just one. I'm not,&nbsp;
I'm just picking on one example because it&nbsp;&nbsp;

00:13:19.080 --> 00:13:22.920
was on my Twitter feed. I'm not saying&nbsp;
this is like the only one by any means.&nbsp;&nbsp;

00:13:24.840 --> 00:13:32.340
And so, when you prompt GPT-4 this way, it will&nbsp;
then, so what it does is it prompts it in a loop.

00:13:32.340 --> 00:13:37.740
So, it says, all right, you're an agent, do&nbsp;
this, et cetera. And then it asks the model&nbsp;&nbsp;

00:13:37.740 --> 00:13:43.200
to critically think, what should I do next?&nbsp;
And then what action should I take? And like,&nbsp;&nbsp;

00:13:43.200 --> 00:13:46.260
how could this go wrong? How could this go&nbsp;
right? And then take an action, basically,&nbsp;&nbsp;

00:13:46.260 --> 00:13:52.680
something like that, right. So, you run&nbsp;
the script and then you get the model.

00:13:53.460 --> 00:14:00.480
Listing, I am X. My goal is to do this. Here's&nbsp;
my list of tasks. And then it lists what tasks&nbsp;&nbsp;

00:14:00.480 --> 00:14:06.540
I need to do, and it picks a task and it's like,&nbsp;
all right, to solve this task, I will now do this,&nbsp;&nbsp;

00:14:06.540 --> 00:14:13.500
this, this, this, and then and then it will pick&nbsp;
a command that it wants to run. It might be,&nbsp;&nbsp;

00:14:14.460 --> 00:14:16.200
like adding something to its memory bank.

00:14:16.200 --> 00:14:20.820
It might be some, it might be running a Google&nbsp;
search, running a piece of code spawning a&nbsp;&nbsp;

00:14:20.820 --> 00:14:27.120
subagent of itself or doing something else in the&nbsp;
default mode, the user has to click accept on the&nbsp;&nbsp;

00:14:27.120 --> 00:14:31.920
commands, but it also has a hilarious,&nbsp;
continuous flag where if you just pick&nbsp;&nbsp;

00:14:31.920 --> 00:14:35.520
continuously, it just runs itself without your&nbsp;
supervision and just does whatever it wants.

00:14:35.520 --> 00:14:42.780
Yeah. But, but it's, it's, it's, this&nbsp;
is running, this is through an API.

00:14:43.740 --> 00:14:45.240
So, this is just a script running on&nbsp;&nbsp;

00:14:45.240 --> 00:14:47.760
your computer that just accesses&nbsp;
the GTP-4 API. Nothing, nothing.

00:14:47.760 --> 00:14:54.720
Right. But, with the script on the&nbsp;
computer, can it take action? I mean,&nbsp;&nbsp;

00:14:54.720 --> 00:14:57.120
what's an example of an action that it could take?

00:14:57.120 --> 00:15:02.640
Action would be run a Google search&nbsp;
for X and return the information,&nbsp;&nbsp;

00:15:02.640 --> 00:15:08.160
or it could be run this piece of Python&nbsp;
code and return the results. Right.

00:15:08.160 --> 00:15:10.860
And what's an example of.&nbsp;&nbsp;

00:15:12.240 --> 00:15:20.100
Something nefarious that, that the agent could, a,&nbsp;
a goal that you could give it, that it could run.

00:15:20.820 --> 00:15:23.880
That's like asking what kind of nefarious&nbsp;
goals could you give to a human?

00:15:25.320 --> 00:15:32.520
Well, I, I'm thinking in what's, what's realistic&nbsp;
in terms of, do you want running the realistic&nbsp;&nbsp;

00:15:32.520 --> 00:15:38.880
in terms of, of running a script on your&nbsp;
computer that is being written by GPT-4?

00:15:40.260 --> 00:15:43.680
I don't know what the limits of GPT-4&nbsp;
is. It depends on how good you are at&nbsp;&nbsp;

00:15:43.680 --> 00:15:45.300
prompting and how you run these kinds of things.

00:15:45.300 --> 00:15:50.520
I expect GPT-4 to be human on a superhuman&nbsp;
level in many things, but not in other&nbsp;&nbsp;

00:15:50.520 --> 00:15:54.600
things. And it's kind of unpredictable what&nbsp;
things will be good at or not. I think,&nbsp;&nbsp;

00:15:55.200 --> 00:16:01.440
like do I expect that, you know, a AutoGPT&nbsp;
script, post GPT-4 is going to like,&nbsp;&nbsp;

00:16:02.220 --> 00:16:07.140
you know, break out and become super&nbsp;
intelligent? No. Not like, I don't&nbsp;&nbsp;

00:16:07.140 --> 00:16:12.720
expect that for various reasons, but do I expect&nbsp;
the same thing to be true for GPT-5, 6, 7, 8?

00:16:12.720 --> 00:16:14.760
Much less clear to me right? Much, much.

00:16:14.760 --> 00:16:29.280
Well, just on this, on this, GitHub, project,&nbsp;
if, if you, if your goal was to, send offensive&nbsp;&nbsp;

00:16:29.280 --> 00:16:35.880
emails to everybody in. Oh, yeah, yeah, yeah,&nbsp;
sure. You know, like it would, it could do that.

00:16:35.880 --> 00:16:39.720
Oh yeah, of course. Like I experimented&nbsp;
with this a little bit myself. So, I ran&nbsp;&nbsp;

00:16:39.720 --> 00:16:45.360
some AutoGPT agents, of course in supervised&nbsp;
mode, just in case, and I gave it to them.

00:16:45.360 --> 00:16:50.280
So, the default goal, if you don't&nbsp;
put in any goal, what it picks is,&nbsp;&nbsp;

00:16:51.060 --> 00:16:55.980
you are entrapreneur. GPT make as much money&nbsp;
as possible. That's the default goal that&nbsp;&nbsp;

00:16:55.980 --> 00:17:01.620
the creator put into the script. So just to&nbsp;
give you a feeling of who, how these people&nbsp;&nbsp;

00:17:01.620 --> 00:17:05.880
think. That is building these kinds of systems&nbsp;
again, not picking on this person particularly.

00:17:05.880 --> 00:17:11.340
Yeah. Like I get it. That's funny. Like from, to&nbsp;
be clear, I bet this guy's a nice guy or, or girl,&nbsp;&nbsp;

00:17:11.340 --> 00:17:14.040
like whoever made this thing, I don't know&nbsp;
who they made this, but they're probably&nbsp;&nbsp;

00:17:14.040 --> 00:17:17.880
a fine person. I don't think they're malicious.&nbsp;
Probably. Maybe they are. I don't know, but like,&nbsp;&nbsp;

00:17:19.260 --> 00:17:23.760
hilarious. So, like one of the first, so I,&nbsp;
I ran it and I let it run on my computer.

00:17:24.360 --> 00:17:31.320
And like, look, it's very primitive. It's not&nbsp;
that smart. But I could see it figured out like,&nbsp;&nbsp;

00:17:31.320 --> 00:17:33.840
all right, let me, so what it&nbsp;
did is it was like, all right,&nbsp;&nbsp;

00:17:33.840 --> 00:17:37.500
first I should Google what are the best ways&nbsp;
to make money? So, I Googled that and then I&nbsp;&nbsp;

00:17:37.500 --> 00:17:41.220
looked at all the results, and then it was&nbsp;
like, all right, this article looks good.

00:17:41.220 --> 00:17:46.380
Now I'm going to open this webpage and look at&nbsp;
the container so that it opens their webpage,&nbsp;&nbsp;

00:17:46.380 --> 00:17:50.160
and then it looks at all the text inside&nbsp;
of it. And then like the text is too large,&nbsp;&nbsp;

00:17:50.160 --> 00:17:54.900
so it runs a summarise command, which breaks&nbsp;
into chunks and then summarises it. So,&nbsp;&nbsp;

00:17:54.900 --> 00:17:57.300
summarise it all right? Then&nbsp;
it came to the conclusion.

00:17:57.300 --> 00:18:01.080
All right, well, affiliate marketing&nbsp;
sounds like a great idea, so it should&nbsp;&nbsp;

00:18:01.080 --> 00:18:05.580
run an affiliate marketing scheme. So, the idea&nbsp;
is that you, you sign up for these websites and&nbsp;&nbsp;

00:18:05.580 --> 00:18:08.160
you get like a special link, and you get&nbsp;
people to buy something using this link,&nbsp;&nbsp;

00:18:08.160 --> 00:18:10.440
and you get money for that. So&nbsp;
that's what it came up with.

00:18:11.280 --> 00:18:14.520
All right, so then things about art.&nbsp;
How do we do affiliate marketing? So,&nbsp;&nbsp;

00:18:14.520 --> 00:18:17.580
it has to build, then it decides you&nbsp;
need to build a brand. So, it decided,&nbsp;&nbsp;

00:18:17.580 --> 00:18:20.340
it first had to come up with a good&nbsp;
name and then create a Twitter handle,&nbsp;&nbsp;

00:18:20.340 --> 00:18:25.200
and then asked to create some marketing content&nbsp;
for this Twitter. So, then it creates a subagent.

00:18:25.200 --> 00:18:31.080
So, it has a small, like a, a sub version of&nbsp;
GPT calls, whose goal was to come up with good&nbsp;&nbsp;

00:18:31.080 --> 00:18:36.600
tweets that it could like, you know, send out to&nbsp;
market to people. So, then the smaller GPT system,&nbsp;&nbsp;

00:18:38.160 --> 00:18:41.700
generated a bunch of tweets that&nbsp;
it could send, and then the main,&nbsp;&nbsp;

00:18:41.700 --> 00:18:45.540
well then, the main system, you know, took&nbsp;
those tweets and I was like, all right,&nbsp;&nbsp;

00:18:45.540 --> 00:18:48.120
now I need to like, you know, find&nbsp;
a good Twitter handle for this.

00:18:48.120 --> 00:18:49.200
So, I came out with a Twitter handle,&nbsp;&nbsp;

00:18:49.200 --> 00:18:53.340
I could use. And then this is about&nbsp;
as far as I let the experiment run.

00:18:53.340 --> 00:19:02.280
Yeah. But could it, register new&nbsp;
Twitter, Twitter handles and then

00:19:02.280 --> 00:19:06.360
Not in the way it is currently set up, but&nbsp;
I could implement this in an afternoon.

00:19:07.140 --> 00:19:13.320
Wow, that's remarkable. So, you&nbsp;
could ha you could have it, create,

00:19:14.580 --> 00:19:19.080
Oh, easy and I expect this already exists.&nbsp;
I expect there's already people who have&nbsp;&nbsp;

00:19:19.080 --> 00:19:21.780
private scripts on the computer right&nbsp;
now that allow them to like access,&nbsp;&nbsp;

00:19:22.980 --> 00:19:27.600
I mean, look, actually, never mind, I'm going to&nbsp;
take that back. It's even worse than that because&nbsp;&nbsp;

00:19:27.600 --> 00:19:34.800
it's always worth it. I mean, OpenAI gives&nbsp;
access to Zapier through the ChatGPT plugins.

00:19:34.800 --> 00:19:38.400
Zapier gives you access to&nbsp;
Twitter, YouTube, LinkedIn,&nbsp;&nbsp;

00:19:38.400 --> 00:19:43.320
Instagram to all of the social networks&nbsp;
through nice, simple API interfaces. So,&nbsp;&nbsp;

00:19:43.320 --> 00:19:48.180
if you have ChatGPT plugins, you don't even&nbsp;
have to implement this in your hacky little,&nbsp;&nbsp;

00:19:48.180 --> 00:19:54.360
you know, Python script. You can just&nbsp;
use the official OpenAI tools. Wow.

00:19:54.360 --> 00:20:01.920
And, and so it would be possible, through&nbsp;
the Zapier plugin that then has access to&nbsp;&nbsp;

00:20:01.920 --> 00:20:10.560
Twitter to create a thousand Twitter accounts&nbsp;
and have them, start, tweeting back and forth,&nbsp;&nbsp;

00:20:11.820 --> 00:20:20.220
to, to sort of generate, you know, an ecosystem&nbsp;
around an idea that then would attract other users&nbsp;&nbsp;

00:20:20.220 --> 00:20:27.420
because, there, there's, there's enough activity&nbsp;
going on that it shows up in some algorithm.

00:20:27.420 --> 00:20:31.200
So, I have two funny stories to&nbsp;
tell about what you just said.&nbsp;&nbsp;

00:20:31.920 --> 00:20:38.460
The first, the first funny story is how the&nbsp;
exact thing you just described was something&nbsp;&nbsp;

00:20:38.460 --> 00:20:42.540
that I have been worried about for like,&nbsp;
ever since GPT-2 came out and before that,&nbsp;&nbsp;

00:20:42.540 --> 00:20:45.840
like one of the first, I wrote a&nbsp;
terrible essay about it, don't read it.

00:20:45.840 --> 00:20:54.180
But I wrote a long, terrible essay about&nbsp;
this, about how basic social trust in the&nbsp;&nbsp;

00:20:54.180 --> 00:21:01.800
net is going to break apart. Like obviously, so&nbsp;
this has already been the case, but the level of&nbsp;&nbsp;

00:21:01.800 --> 00:21:09.060
PSYOPs you can run with these kinds of systems is&nbsp;
unimaginable because you can basically DDoS social&nbsp;&nbsp;

00:21:09.060 --> 00:21:17.220
reality, you can manipulate trends and like social&nbsp;
mimetics to degrees that are not, that before was&nbsp;&nbsp;

00:21:17.220 --> 00:21:21.600
possible. These were always possible, but they're&nbsp;
very costly. Like you had to have a whole Russian&nbsp;&nbsp;

00:21:21.600 --> 00:21:25.260
troll farm or something. You had to like paying&nbsp;
people minimum wage for it or something. Right.&nbsp;&nbsp;

00:21:25.260 --> 00:21:30.840
And even then, like minimum wage Russians are not&nbsp;
that great at mimetic manipulation. But for, for&nbsp;&nbsp;

00:21:30.840 --> 00:21:37.080
example, this is something I expect GPT-4 to be&nbsp;
strictly good at like better than a minimum wage.

00:21:37.080 --> 00:21:45.540
Russian is going to be imitating mimetically&nbsp;
certain cultures and their tone, you know,&nbsp;&nbsp;

00:21:45.540 --> 00:21:48.840
their patterns of speech and there's&nbsp;
patterns of communications and infiltrating&nbsp;&nbsp;

00:21:48.840 --> 00:21:51.900
these communities. I think this is&nbsp;
something GPT-4 is clearly extremely&nbsp;&nbsp;

00:21:51.900 --> 00:21:55.140
good at. I think GPT-3 is already&nbsp;
more than good enough to do this.

00:21:55.140 --> 00:22:00.360
And so, it's really funny because this&nbsp;
has been obvious to me for a long time,&nbsp;&nbsp;

00:22:00.360 --> 00:22:07.920
and I have been saying that for a long time. but&nbsp;
people either dismissed it or were like, oh, we&nbsp;&nbsp;

00:22:07.920 --> 00:22:12.360
have to do more research about this. Oh, you know,&nbsp;
it's, oh, maybe it won't be so bad. Oh, I don't&nbsp;&nbsp;

00:22:12.360 --> 00:22:16.680
know. What about bias? And I'm like, man, like&nbsp;
things are so much worse than you think it is.

00:22:16.680 --> 00:22:19.920
Like this is, it's getting so much worse.&nbsp;
And there's another funny story I want to&nbsp;&nbsp;

00:22:19.920 --> 00:22:23.520
talk about this. And so, the other funny story I&nbsp;
want to talk about this is, if someone's listening&nbsp;&nbsp;

00:22:23.520 --> 00:22:28.620
to this right now, one of the counterpoint they&nbsp;
might make, if they are a little bit technically&nbsp;&nbsp;

00:22:28.620 --> 00:22:32.160
inclined but not very technically inclined, is&nbsp;
to say something like, well, what about captchas?

00:22:32.160 --> 00:22:35.400
Like, you know, we already have bot farms,&nbsp;
right? Like, this already happens. You know,&nbsp;&nbsp;

00:22:35.400 --> 00:22:39.900
what about like, you know, sure, maybe the bot&nbsp;
tries to register a thousand Twitter accounts,&nbsp;&nbsp;

00:22:39.900 --> 00:22:42.420
but it's going to fail because&nbsp;
you're not allowed to do that.&nbsp;&nbsp;

00:22:42.420 --> 00:22:46.440
And I'm like, I mean, first of all, LOL, like,&nbsp;
there's obviously ways to get around that,&nbsp;&nbsp;

00:22:46.440 --> 00:22:49.320
but this brings up one of my favourite&nbsp;
anecdotes about the GPT-4 paper.

00:22:49.320 --> 00:22:58.980
I don't know if you've read it. it is interesting,&nbsp;
I want, in the evals that they did on the models,&nbsp;&nbsp;

00:22:58.980 --> 00:23:06.300
including they did some safety evals, so I have&nbsp;
some. I have some problems with some of these,&nbsp;&nbsp;

00:23:06.300 --> 00:23:11.940
but like, let's just take them at face value. one&nbsp;
of the things they were trying, so basically what&nbsp;&nbsp;

00:23:11.940 --> 00:23:17.760
they did is they, this was the Alignment Research&nbsp;
Center, ARC, who ran these evals for OpenAI.

00:23:17.760 --> 00:23:22.680
And basically, what they did is that they tried to&nbsp;
get the model to do the evillest thing they could&nbsp;&nbsp;

00:23:22.680 --> 00:23:27.720
do, and then they had an assistant role play&nbsp;
in helping the model. So, if the model said,&nbsp;&nbsp;

00:23:27.720 --> 00:23:31.440
do something, the human would then,&nbsp;
like role play doing that to the, in a&nbsp;&nbsp;

00:23:32.100 --> 00:23:34.920
safe environment, hypothetically, whatever.

00:23:34.920 --> 00:23:40.080
Anyways, for the most part, it wasn't very&nbsp;
smart enough. It wasn't really smart enough to&nbsp;&nbsp;

00:23:40.080 --> 00:23:45.660
hack out of its own, you know, computer system or&nbsp;
something wasn't really smart enough, or rather,&nbsp;&nbsp;

00:23:45.660 --> 00:23:49.440
ARC wasn't good enough at getting it&nbsp;
to do that. That's a whole different&nbsp;&nbsp;

00:23:49.440 --> 00:23:52.440
question. Mm-hmm. But they did&nbsp;
do one very interesting thing.

00:23:52.440 --> 00:23:57.000
So, one thing it was trying to do, I forgot what&nbsp;
the model was meant to do. Maybe let's make money&nbsp;&nbsp;

00:23:57.000 --> 00:24:00.360
or something. I don't know. It was supposed&nbsp;
to do something, and it ran into a captcha.&nbsp;&nbsp;

00:24:00.960 --> 00:24:05.040
And so, it couldn't solve the&nbsp;
captcha. So, what are they,&nbsp;&nbsp;

00:24:05.040 --> 00:24:09.960
so the model itself came up with the idea,&nbsp;
well, I'll pay someone to do it for me.

00:24:09.960 --> 00:24:16.020
So, it went on, like, you know, assisted&nbsp;
by a human, but the decisions are made by&nbsp;&nbsp;

00:24:16.020 --> 00:24:20.580
the model. So, I think human access, the&nbsp;
hands, but the model made the decisions,&nbsp;&nbsp;

00:24:20.580 --> 00:24:27.240
the human, so they, it went on like a crowd&nbsp;
working website. And then paid a crowd,&nbsp;&nbsp;

00:24:27.240 --> 00:24:32.940
tried to find a crowd worker to do a captcha for&nbsp;
it, and then something very interesting happened.

00:24:33.660 --> 00:24:38.460
So, what happened was that the crowd worker,&nbsp;
rather understandably, was a bit suspicious.&nbsp;&nbsp;

00:24:38.460 --> 00:24:43.920
He's like, hey, why are you making me solve&nbsp;
a captcha? Is this legal? And the model?&nbsp;&nbsp;

00:24:44.700 --> 00:24:49.680
Realised this, thought about it and&nbsp;
came up with a lie. It came up with,&nbsp;&nbsp;

00:24:49.680 --> 00:24:54.720
oh, they're a visually impaired person&nbsp;
and they need some help in understanding,&nbsp;&nbsp;

00:24:54.720 --> 00:24:57.600
seeing this captcha, you see,&nbsp;
it's nothing to worry about.

00:24:57.600 --> 00:24:59.510
And then the person did it. Wow. To me. Wow.

00:24:59.509 --> 00:25:05.099
Incredible. Yeah. Yep. Yeah. So, you&nbsp;
know. And that's in OpenAI's paper?

00:25:05.100 --> 00:25:09.420
Yep. That's in the GPT-4 technical&nbsp;
report under the ARC evals.&nbsp;&nbsp;

00:25:09.420 --> 00:25:15.240
This is a real thing that actually happened in&nbsp;
the real world, and a, and the crowd worker was&nbsp;&nbsp;

00:25:15.240 --> 00:25:20.700
not in on it. Like this was an unconsenting, you&nbsp;
know, part of the, of the experiment, so to speak.

00:25:20.700 --> 00:25:28.200
Like, to be clear, I don't think that person was&nbsp;
harmed in any regard here, but man, like imagine,&nbsp;&nbsp;

00:25:28.200 --> 00:25:34.560
imagine this happening and you're just like,&nbsp;
yeah, the same, safe to release. Like imagine.

00:25:36.060 --> 00:25:42.720
Yeah. Wow. And you're working&nbsp;
toward AGI, at Conjecture.&nbsp;&nbsp;

00:25:43.980 --> 00:25:49.920
Are you similar? That sounds similar&nbsp;
to Anthropic. I don't know if you know&nbsp;&nbsp;

00:25:49.920 --> 00:25:57.000
Jack Clark, but I actually started this&nbsp;
podcast with him, and then he got busy.

00:25:57.000 --> 00:26:03.060
But is it similar to Anthropic, which&nbsp;
is, I’m a little more familiar with.

00:26:03.900 --> 00:26:12.420
So Anthropic, right? Big topic. No, we are not&nbsp;
similar to the Anthropic, and there's several&nbsp;&nbsp;

00:26:12.420 --> 00:26:18.120
reasons for that. So, number one reason is we&nbsp;
are not racing for AGI. We, it's unsafe. AGI,&nbsp;&nbsp;

00:26:18.120 --> 00:26:23.640
we think this is bad. We fully think and&nbsp;
we are willing to go onto the record and&nbsp;&nbsp;

00:26:23.640 --> 00:26:27.720
scream into high heavens that if you, if&nbsp;
we continue on the current path that we&nbsp;&nbsp;

00:26:27.720 --> 00:26:31.440
are of just scaling bigger and bigger models&nbsp;
and just slapping some patches on whatever.

00:26:32.220 --> 00:26:37.860
That is very bad and it, and it is going to end&nbsp;
in catastrophe and there is no way around that.&nbsp;&nbsp;

00:26:37.860 --> 00:26:42.480
And everyone who says otherwise is lying to you&nbsp;
- is either confused, they do not understand what&nbsp;&nbsp;

00:26:42.480 --> 00:26:47.580
they're dealing with, or they are lying for&nbsp;
their own profit. And this is something that&nbsp;&nbsp;

00:26:47.580 --> 00:26:52.500
many people at many of these organisations have a&nbsp;
very strong financial incentive to not care about.

00:26:52.500 --> 00:26:55.800
And so Anthropic from the beginning has been&nbsp;&nbsp;

00:26:56.460 --> 00:27:01.740
telling a story about how they left OpenAI&nbsp;
because of their safety concerns, you know,&nbsp;&nbsp;

00:27:01.740 --> 00:27:06.060
cause there, they're, they’re, they're being so&nbsp;
unsafe, these OpenAI people. That Sam Altman guy.&nbsp;&nbsp;

00:27:06.060 --> 00:27:11.280
Oh, he is so crazy. Which is why they just raising&nbsp;
another huge round in order to build a model 10&nbsp;&nbsp;

00:27:11.280 --> 00:27:15.960
times larger than GPT-4 to release it because they&nbsp;
needed more money for their commercialization.

00:27:17.460 --> 00:27:25.380
I'm done. Like, I consider Anthropic to be in the&nbsp;
same reference class as OpenAI. It's like, sure,&nbsp;&nbsp;

00:27:25.380 --> 00:27:29.400
maybe the people are marginally nicer.&nbsp;
Maybe they are, you know, I know Jack,&nbsp;&nbsp;

00:27:29.400 --> 00:27:33.300
I've talked to him many times, seems like a&nbsp;
nice fellow, you know, I like him. He seems&nbsp;&nbsp;

00:27:33.300 --> 00:27:38.580
like a good person. But also, every time&nbsp;
I ask him to do anything to slow down AGI,&nbsp;&nbsp;

00:27:38.580 --> 00:27:41.520
he always says, Ooh, well we&nbsp;
should consider our options.

00:27:41.520 --> 00:27:45.000
Let’s, you know, let's not, no,&nbsp;
let's not go too fast here. Like,&nbsp;&nbsp;

00:27:45.000 --> 00:27:51.120
you know, and like, I'm like, man, you&nbsp;
know, so my view of Anthropic is that&nbsp;&nbsp;

00:27:51.120 --> 00:27:56.580
they're OpenAI with a different coat of&nbsp;
paint and, you know, It's a nice coat of&nbsp;&nbsp;

00:27:56.580 --> 00:28:00.420
paint. I like many Anthropic people. I think&nbsp;
Anthropic does a lot of very nice things.

00:28:00.420 --> 00:28:04.620
A lot of their research is pretty nice. A lot&nbsp;
of them, the people there who I've talked to,&nbsp;&nbsp;

00:28:04.620 --> 00:28:11.340
I think are very nice people. I don't hate them by&nbsp;
any means, but I mean at this point it's mask off,&nbsp;&nbsp;

00:28:11.340 --> 00:28:16.860
right? Like reading the latest, like,&nbsp;
I think it was like TechCrunch, I think&nbsp;&nbsp;

00:28:16.860 --> 00:28:19.980
about Anthropic where they're just like,&nbsp;
yeah, yeah, straight up commercialization.

00:28:19.980 --> 00:28:24.570
Just let's go. So, I think&nbsp;
the mask is off at this point.

00:28:24.570 --> 00:28:31.598
And, and then, on, so, so explain Conjecture&nbsp;
is building models though, correct?

00:28:31.598 --> 00:28:36.480
Yes. We build models. We do not push the state&nbsp;
of the art. This is very, very important. we,&nbsp;&nbsp;

00:28:36.480 --> 00:28:42.960
if I had the ability to train a GPT-5 right now&nbsp;
and release it to the public, I would not do so.&nbsp;&nbsp;

00:28:43.980 --> 00:28:48.240
If I had a GPT-5 model, I wouldn't&nbsp;
tell you, I wouldn't tell anybody.

00:28:48.240 --> 00:28:54.240
I wouldn't have built it in the first&nbsp;
place. My goal in all of this is I have&nbsp;&nbsp;

00:28:54.240 --> 00:28:59.160
no interest in advancing capabilities&nbsp;
without advancing alignment. To be clear,&nbsp;&nbsp;

00:28:59.160 --> 00:29:02.940
sometimes to advance alignment, to get&nbsp;
better control, you're also going to build&nbsp;&nbsp;

00:29:02.940 --> 00:29:07.020
better systems. You know, if you control a&nbsp;
system, it'll often become more powerful.

00:29:07.020 --> 00:29:11.700
This is a very natural thing to happen, and&nbsp;
if this happens, cool. It's fine, you know,&nbsp;&nbsp;

00:29:11.700 --> 00:29:15.480
like, I think this is a, but then also I don't&nbsp;
publish about it, I don't talk about, this is,&nbsp;&nbsp;

00:29:15.480 --> 00:29:16.980
for example, something I want to really laude&nbsp;&nbsp;

00:29:16.980 --> 00:29:21.660
Anthropic about. Anthropic does a great&nbsp;
job of keeping their damn mouth shut.

00:29:21.660 --> 00:29:25.560
This is something they're very, very good at&nbsp;
and I think this is very good. I think that&nbsp;&nbsp;

00:29:26.460 --> 00:29:29.940
this idea that you should just like,&nbsp;
publish all your capabilities, ideas,&nbsp;&nbsp;

00:29:29.940 --> 00:29:34.140
and all your model architectures or&nbsp;
something is obviously terrible. Like&nbsp;&nbsp;

00:29:34.140 --> 00:29:37.980
it only benefits the least scrupulous&nbsp;
actors, you know, it only helps,&nbsp;&nbsp;

00:29:37.980 --> 00:29:44.580
you know dangerous actors catch up. It only, you&nbsp;
know, helps, you know, orgs speed each other up.

00:29:44.580 --> 00:29:49.380
There is, from my perspective,&nbsp;
like if you, if you, dear listener,&nbsp;&nbsp;

00:29:49.380 --> 00:29:54.240
develop something that makes your model&nbsp;
20% more efficient, or a new architecture&nbsp;&nbsp;

00:29:54.240 --> 00:29:59.520
that fits much better on jps or whatever,&nbsp;
don't tell anybody that's my one request.

00:29:59.520 --> 00:30:02.580
You know, you build it yourself, fine. You know,&nbsp;&nbsp;

00:30:02.580 --> 00:30:06.480
make an API and make a lot of money.&nbsp;
Okay? Like, not great, but like fine.&nbsp;&nbsp;

00:30:07.140 --> 00:30:12.780
Just don't tell anyone how you did it and don't&nbsp;
hype up how anything about that it's not ideal,&nbsp;&nbsp;

00:30:12.780 --> 00:30:17.040
ideally would be, you know, don't deploy it, don't&nbsp;
build it, don't do any of it, but such is life.

00:30:17.040 --> 00:30:22.560
So conjecture, our goal is not to build&nbsp;
the strongest AI AGI as fast as possible&nbsp;&nbsp;

00:30:22.560 --> 00:30:27.000
by whatever means necessary. And let's be&nbsp;
very clear here. This is what people like,&nbsp;&nbsp;

00:30:27.000 --> 00:30:31.860
at, at OpenAI, at Anthropic, at all these&nbsp;
other people are doing. They are racing to&nbsp;&nbsp;

00:30:31.860 --> 00:30:35.460
systems that are extremely powerful that&nbsp;
they themselves know they cannot control.

00:30:35.460 --> 00:30:40.320
They of course have various reasons to&nbsp;
downplay these risks. To pretend that,&nbsp;&nbsp;

00:30:40.320 --> 00:30:45.120
oh no, actually it's fine. We have to iterate.&nbsp;
Like they have a story about iterative safety.&nbsp;&nbsp;

00:30:45.120 --> 00:30:49.980
They have like mm-hmm. Oh, we have to like it,&nbsp;
we have to deploy it actually for it to be safe.&nbsp;&nbsp;

00:30:49.980 --> 00:30:51.660
But just think about that for three seconds.

00:30:51.660 --> 00:30:55.860
It sounds so nice when it comes to a Sam&nbsp;
Altman's mouth that is like, oh yeah, well,&nbsp;&nbsp;

00:30:55.860 --> 00:31:00.000
we have to deploy it so we can debug it. But&nbsp;
think about that for 10 seconds and you're&nbsp;&nbsp;

00:31:00.000 --> 00:31:03.360
going to see why that's insane. That's like&nbsp;
saying, well, the only way we can test our&nbsp;&nbsp;

00:31:03.360 --> 00:31:06.360
new medicine is to give it to as many people in&nbsp;
the general public as possible, which actually&nbsp;&nbsp;

00:31:06.360 --> 00:31:10.200
put it into the water supply just to, that's the&nbsp;
only way we can know whether it's safe or not.

00:31:10.200 --> 00:31:13.560
Just put in the water supply, give it too&nbsp;
literally everybody as fast as possible,&nbsp;&nbsp;

00:31:13.560 --> 00:31:16.920
and then. Once and then before we&nbsp;
get the results for the last one,&nbsp;&nbsp;

00:31:16.920 --> 00:31:20.340
make an even more potent drug and put&nbsp;
that into the water supply as well,&nbsp;&nbsp;

00:31:20.340 --> 00:31:25.260
and do this as fast as possible. That is the&nbsp;
alignment strategy that these people are pushing.

00:31:25.260 --> 00:31:28.320
Let's be very clear about this here.&nbsp;
Very, very clear about this. So,&nbsp;&nbsp;

00:31:28.320 --> 00:31:35.520
there is a version of this that I don't hate. If&nbsp;
for example, you know, an OpenAI develops GPT-2.&nbsp;&nbsp;

00:31:36.060 --> 00:31:40.620
And then they take, they, they don't&nbsp;
release anything anymore. They take&nbsp;&nbsp;

00:31:40.620 --> 00:31:45.780
all the time necessary to understand every&nbsp;
single part about GPT-2 to fully align it.

00:31:45.780 --> 00:31:49.800
They let you know, society like culture&nbsp;
gets caught up to it. Like, you know,&nbsp;&nbsp;

00:31:49.800 --> 00:31:54.420
spam filters catch up to it. They let regulation&nbsp;
catch up to it and such. And then with all of this&nbsp;&nbsp;

00:31:54.420 --> 00:31:59.940
fully integrated into society, they built GPT-3.&nbsp;
All right. You know? Fair enough. Okay, cool.&nbsp;&nbsp;

00:31:59.940 --> 00:32:05.520
Yeah. Honestly, if that's what we were doing, if&nbsp;
that's what the plan was, I'll be fine with that.

00:32:05.520 --> 00:32:10.140
Like if everyone just stopped at GPT-4 and&nbsp;
just said, all right, all right, come on guys,&nbsp;&nbsp;

00:32:10.140 --> 00:32:16.500
no more new stuff until we fully figure&nbsp;
out GPT-4 and once we fully understand it,&nbsp;&nbsp;

00:32:16.500 --> 00:32:21.420
and regulation has fully regulated it and&nbsp;
society has fully absorbed it the way like,&nbsp;&nbsp;

00:32:21.420 --> 00:32:23.940
you know, society has absorbed, you&nbsp;
know, like the internet or whatever&nbsp;&nbsp;

00:32:23.940 --> 00:32:29.400
even so that's not fully absorbed, but&nbsp;
like, you know, and then they build GPT-5.

00:32:29.400 --> 00:32:35.340
I'm like, okay. Fair enough, but let's like, well,&nbsp;
I mean, come on man. Like, like gimme a break. No&nbsp;&nbsp;

00:32:35.340 --> 00:32:40.140
one's going to do that. Like, that's obviously&nbsp;
bullshit. Like it's obviously just not true and&nbsp;&nbsp;

00:32:40.140 --> 00:32:44.520
not what these people are planning. These&nbsp;
people are racing, let's be clear, they're&nbsp;&nbsp;

00:32:44.520 --> 00:32:50.340
racing for their own personal gain, for their&nbsp;
own glory towards an existential catastrophe.

00:32:50.340 --> 00:32:56.640
And that no one has consented to that the public&nbsp;
has no oversight in the government has, for some&nbsp;&nbsp;

00:32:56.640 --> 00:33:01.260
reason it's just letting it happen. Like if I&nbsp;
was the government. And one of my most powerful&nbsp;&nbsp;

00:33:01.260 --> 00:33:06.360
industrialists was just on Twitter publicly&nbsp;
stating that they're building, you know, God-like&nbsp;&nbsp;

00:33:06.360 --> 00:33:12.000
powerful AI systems that will overthrow the&nbsp;
government.I would have some questions about that.

00:33:14.460 --> 00:33:19.380
Yeah, the, well, actually, one of the&nbsp;
things I wanted to ask you about is the,&nbsp;&nbsp;

00:33:19.380 --> 00:33:24.780
the, the letter which, you&nbsp;
signed, I saw, excuse me.&nbsp;&nbsp;

00:33:27.000 --> 00:33:36.240
Has triggered an FTC complaint by another&nbsp;
group. those are actually unrelated, but yeah.

00:33:36.780 --> 00:33:41.760
Oh, the FTC complaint was&nbsp;
not related to the letter.

00:33:41.760 --> 00:33:45.180
My, at least not to my knowledge. Okay. Actually,

00:33:45.180 --> 00:33:51.540
I'm going to talk to them later today, so, yeah.&nbsp;
But in any case, there is this FTC complaint,&nbsp;&nbsp;

00:33:51.540 --> 00:33:58.980
which it'll be interesting to see whether&nbsp;
the FTC takes it seriously, but, they have,&nbsp;&nbsp;

00:33:59.880 --> 00:34:07.500
presumably, some real power. So, is that the&nbsp;
sort of thing that, that you are hoping for,&nbsp;&nbsp;

00:34:07.500 --> 00:34:15.480
that the governments will begin, to use&nbsp;
whatever mechanisms are available to slow&nbsp;&nbsp;

00:34:15.480 --> 00:34:21.360
down this development, or at least slow down&nbsp;
the public release of more powerful models?

00:34:21.360 --> 00:34:26.760
I'm very practical about these kinds of things.&nbsp;
You know, in a good world, you know, you know,&nbsp;&nbsp;

00:34:26.760 --> 00:34:33.420
somewhere deep in my heart still is, you know,&nbsp;
a, you know, techno optimist. Like, yay, liberal&nbsp;&nbsp;

00:34:33.420 --> 00:34:41.340
democracy, freedom, you know, let people develop&nbsp;
things and do cool stuff and like, you know,&nbsp;&nbsp;

00:34:41.340 --> 00:34:46.080
they'll be fine. But like, like gimme a break.

00:34:46.080 --> 00:34:50.029
Like, like we have to, we have to have some&nbsp;
realpolitik here. Like, let's be realistic about&nbsp;&nbsp;

00:34:50.029 --> 00:34:55.980
what we're looking at here. These companies are&nbsp;
racing ahead unilaterally, like these small, like&nbsp;&nbsp;

00:34:55.980 --> 00:35:03.780
I cannot stress how small a number of people it is&nbsp;
that are driving 99.9% of this. This is not about&nbsp;&nbsp;

00:35:03.780 --> 00:35:09.300
your, you know, local friendly grad student with&nbsp;
his two, you know, old GPUs or whatever, right?

00:35:09.300 --> 00:35:13.320
Like, one of the things I found on&nbsp;
Twitter when the letter got released,&nbsp;&nbsp;

00:35:13.320 --> 00:35:17.760
and I do have some problems with the letter to be&nbsp;
clear, but I was a prominent signatory of it, and&nbsp;&nbsp;

00:35:17.760 --> 00:35:23.100
I do think it's overall good. One of the things&nbsp;
people misunderstand about the letter is that&nbsp;&nbsp;

00:35:23.640 --> 00:35:27.360
they seem to think it says like, you know,&nbsp;
stop, you know, like outlaw computers.

00:35:27.360 --> 00:35:32.940
That is not what the letter says. What the letter&nbsp;
says is no more things that are bigger than GPT-4.&nbsp;&nbsp;

00:35:32.940 --> 00:35:39.420
Do you know how big GPT-4 is? In its training run&nbsp;
in pure computer GPT-4? Just running it - not the&nbsp;&nbsp;

00:35:39.420 --> 00:35:44.640
hardware, just running it - is estimated to&nbsp;
cost around a hundred million dollars. So,&nbsp;&nbsp;

00:35:44.640 --> 00:35:48.600
unless you and your local fund have you&nbsp;
know, friendly grad student friends are&nbsp;&nbsp;

00:35:48.600 --> 00:35:53.880
spending a hundred million in compute on a&nbsp;
single experiment, this does not affect you.

00:35:53.880 --> 00:35:59.520
Now personally, if we could get even more than&nbsp;
this, you know, if we could, you know, clamp down&nbsp;&nbsp;

00:35:59.520 --> 00:36:05.160
even, you know, on, on, you know, $10 million&nbsp;
things or $1 million things, also interesting,&nbsp;&nbsp;

00:36:05.160 --> 00:36:10.140
but like, all right, let's, you know, one step at&nbsp;
a time here, right? One, one step at a time here.&nbsp;&nbsp;

00:36:10.140 --> 00:36:19.020
So, the way I see things is that we're currently&nbsp;
going headlong towards destruction. Mm-hmm.

00:36:19.020 --> 00:36:23.820
Like there is no way that we will look,&nbsp;
you know, we can argue if you want to,&nbsp;&nbsp;

00:36:23.820 --> 00:36:29.700
and we can do that about when it will happen. You&nbsp;
know, is it going to be one year or five years,&nbsp;&nbsp;

00:36:29.700 --> 00:36:33.840
or 10 or 50 or like whatever,&nbsp;
right? Like we can argue about this&nbsp;&nbsp;

00:36:33.840 --> 00:36:39.300
if you want. But I think the writing is on the&nbsp;
wall at this point, and I, I consider the burden&nbsp;&nbsp;

00:36:39.300 --> 00:36:44.460
of proof at this point to be on the sceptics&nbsp;
of like, look at what GPT-3 and 4 can do.

00:36:44.460 --> 00:36:49.140
Look at what these AutoGPT systems&nbsp;
can do. These systems can, you know,&nbsp;&nbsp;

00:36:49.140 --> 00:36:51.540
they can achieve agency, they can&nbsp;
become intelligent. They're becoming&nbsp;&nbsp;

00:36:51.540 --> 00:36:55.740
more intelligent very quickly. They have&nbsp;
many abilities that humans do not have.&nbsp;&nbsp;

00:36:55.740 --> 00:36:59.700
Do you know any human who has read every&nbsp;
book ever written? I don't. GPT-4 has.

00:36:59.700 --> 00:37:04.800
You know, they have extremely good memories.&nbsp;
You know, they can make copies of themselves,&nbsp;&nbsp;

00:37:04.800 --> 00:37:10.140
these are, et cetera, et cetera. Right.&nbsp;
Even if you don't buy the, like, oh,&nbsp;&nbsp;

00:37:10.140 --> 00:37:14.340
you know, the system becomes an agent and&nbsp;
does something dangerous, fine. You know,&nbsp;&nbsp;

00:37:14.340 --> 00:37:17.880
like, I, I think you're wrong, deadly&nbsp;
wrong, but we can get into that.

00:37:19.500 --> 00:37:25.620
But what world in which systems like this&nbsp;
exist is stable in chronic equilibrium? Like&nbsp;&nbsp;

00:37:25.620 --> 00:37:30.720
what world could, could possibly look&nbsp;
like the world we are living in right&nbsp;&nbsp;

00:37:30.720 --> 00:37:35.280
now. When you can pay, you know, 1 cent&nbsp;
for a thousand, John von Neumanns to do&nbsp;&nbsp;

00:37:35.880 --> 00:37:42.120
anything, like how could that world not be&nbsp;
wild? How could there not be instability?

00:37:42.120 --> 00:37:49.080
How could that not, you know, explode? Like&nbsp;
how I would like someone who doesn't buy AI&nbsp;&nbsp;

00:37:49.080 --> 00:37:54.000
risk to explain to me how such a world&nbsp;
would look like, because I don't see it.

00:37:54.720 --> 00:37:58.020
Okay, so you are focused on the alignment problem&nbsp;&nbsp;

00:37:58.020 --> 00:38:03.300
and Correct. Your startup Conjecture&nbsp;
is focused on developing, I presume,&nbsp;&nbsp;

00:38:04.440 --> 00:38:15.120
strategies or technology that would improve the&nbsp;
alignment of future AI models with human goals.

00:38:15.900 --> 00:38:21.060
Technically, can you talk a little&nbsp;
bit about how you would do that?

00:38:21.060 --> 00:38:26.340
Yeah. Happy to talk about that.&nbsp;
So, the current thing we work on,&nbsp;&nbsp;

00:38:26.340 --> 00:38:31.140
our current primary research agenda is what&nbsp;
we call cognitive emulation or CoEm. So,&nbsp;&nbsp;

00:38:31.140 --> 00:38:37.320
this is a bit vague and public resources on this&nbsp;
are very sparse. There's basically one short,&nbsp;&nbsp;

00:38:37.320 --> 00:38:41.220
you know, intro post and like, maybe one&nbsp;
or two podcasts where I talk about it.

00:38:42.120 --> 00:38:49.560
so, apologies to the reader, the listener, that&nbsp;
some of this is not very well explicative publicly&nbsp;&nbsp;

00:38:49.560 --> 00:38:57.540
just yet. The idea of CoEm is rather simple.&nbsp;
It is. Well, it's both, it's both very simple&nbsp;&nbsp;

00:38:58.320 --> 00:39:02.100
like a, you know, bird's eye view, but then&nbsp;
it gets subtle once you get into the details.&nbsp;&nbsp;

00:39:03.060 --> 00:39:05.400
and we can get into the details&nbsp;
if you're interested, but,&nbsp;&nbsp;

00:39:06.540 --> 00:39:14.220
ultimately the goal of CoEm is to move away from&nbsp;
a paradigm of building these huge black box neural&nbsp;&nbsp;

00:39:14.220 --> 00:39:19.020
network, whatever the hell these things are,&nbsp;
that you just, you know, put some input in and&nbsp;&nbsp;

00:39:19.020 --> 00:39:23.100
then just something comes out and, you know,&nbsp;
maybe it's good, maybe it's bad, who knows?

00:39:23.100 --> 00:39:27.060
And the way you debug these&nbsp;
things is you like, you know,&nbsp;&nbsp;

00:39:27.060 --> 00:39:31.560
let's say you know you're OpenAI, right?&nbsp;
And your GPT-4 model, you give an input,&nbsp;&nbsp;

00:39:31.560 --> 00:39:34.320
and it gives you an up output. You&nbsp;
don't like it, what do you do? Well,&nbsp;&nbsp;

00:39:34.320 --> 00:39:39.600
you don't understand what happens inside the air.&nbsp;
It's all just a bunch of numbers being crunched.

00:39:39.600 --> 00:39:45.000
So, the only thing you can do is kind of nudge it&nbsp;
sort of in some direction. You can give it like,&nbsp;&nbsp;

00:39:45.000 --> 00:39:49.860
eh, thumbs up, thumbs down, something,&nbsp;
something. And then you update these, you know,&nbsp;&nbsp;

00:39:49.860 --> 00:39:54.900
trillions of numbers or whatever, I, who knows how&nbsp;
many numbers there are inside of these systems.&nbsp;&nbsp;

00:39:54.900 --> 00:40:00.480
All of them in some random directions, and then&nbsp;
maybe gets you a better output, maybe it doesn't.

00:40:00.480 --> 00:40:03.240
Mm-hmm. Like the inherent, like,&nbsp;&nbsp;

00:40:03.240 --> 00:40:08.640
I want to like to drive home how ridiculous&nbsp;
it is to expect this to work. It's like,

00:40:08.640 --> 00:40:09.420
Someone going to work with you,&nbsp;&nbsp;

00:40:10.080 --> 00:40:16.020
you're talking about reinforcement&nbsp;
learning with human feedback. Yes. Yeah.

00:40:16.020 --> 00:40:18.180
Also applies to fine tuning and other methods,&nbsp;&nbsp;

00:40:18.180 --> 00:40:23.190
like for the listener to understand these AI&nbsp;
systems are not computer programs with code like.

00:40:23.190 --> 00:40:28.920
This is not how they work. There is code involved&nbsp;
short, but like the thing that happens between&nbsp;&nbsp;

00:40:28.920 --> 00:40:33.900
you entering a text and you getting an output.&nbsp;
It's not human code. There's not a person at&nbsp;&nbsp;

00:40:33.900 --> 00:40:38.280
OpenAI sitting in a chair who knows why&nbsp;
it gave you that answer. Who can like,&nbsp;&nbsp;

00:40:38.280 --> 00:40:43.080
you know, go through the lines of code and&nbsp;
like see, ah, here's the bug and then fix it.

00:40:43.080 --> 00:40:49.440
No, no, no. Nothing of the sort. AI systems&nbsp;
are more. They're not really written. They're&nbsp;&nbsp;

00:40:49.440 --> 00:40:53.820
grown. They're more like organic things&nbsp;
that you like to grow in a Petri dish,&nbsp;&nbsp;

00:40:53.820 --> 00:40:57.060
like a digital Petri dish. This is&nbsp;
not literally true. Do not take this&nbsp;&nbsp;

00:40:57.060 --> 00:41:02.760
as a literal metaphor. Mm-hmm. To be clear,&nbsp;
there is subtlety to this, but the resulting&nbsp;&nbsp;

00:41:02.760 --> 00:41:07.680
system is not a clean, human readable, you&nbsp;
know, text file that shows all the code.

00:41:07.680 --> 00:41:12.960
Instead, what you get is, is billions and&nbsp;
billions and billions and billions of numbers,&nbsp;&nbsp;

00:41:12.960 --> 00:41:19.080
and you multiply all these numbers in&nbsp;
a certain order. And that's the output&nbsp;&nbsp;

00:41:19.080 --> 00:41:27.660
and what these numbers mean, how they work, like&nbsp;
what they are calculating and why is mostly a&nbsp;&nbsp;

00:41:27.660 --> 00:41:31.920
complete mystery to science to this day. I don't&nbsp;
think this is an unsolvable problem, to be clear.

00:41:31.920 --> 00:41:35.220
It's not like, oh, this is unknowable.&nbsp;
It's just mm-hmm hard, you know,&nbsp;&nbsp;

00:41:35.220 --> 00:41:40.080
science takes time. You know, figuring out complex&nbsp;
new scientific phenomena like this takes time and&nbsp;&nbsp;

00:41:40.080 --> 00:41:45.300
resources and smart, you know, people if like,&nbsp;
you know, if all the string theorists of the&nbsp;&nbsp;

00:41:45.300 --> 00:41:50.100
world and all the young up and coming physicists&nbsp;
and mathematicians decided to, you know, you know,&nbsp;&nbsp;

00:41:50.100 --> 00:41:55.860
buckle down and just like unlock the mysteries&nbsp;
of neural networks, I think they will succeed.

00:41:55.860 --> 00:41:59.700
You know, it might take a while. It might&nbsp;
be very expensive, but like, you know,&nbsp;&nbsp;

00:41:59.700 --> 00:42:05.100
I do believe in the, you know, human spirit&nbsp;
and intelligence disregard. I, I think like&nbsp;&nbsp;

00:42:05.100 --> 00:42:08.340
all of our best string theorists working together&nbsp;
could probably figure it out in like 10 years,&nbsp;&nbsp;

00:42:08.340 --> 00:42:11.100
you know, like they could figure it out&nbsp;
and then it would be a mystery anymore.

00:42:11.100 --> 00:42:15.720
But currently it's a mystery. We have no&nbsp;
idea what's the mystery sauce that makes&nbsp;&nbsp;

00:42:15.720 --> 00:42:19.680
these systems actually work. And we have no way&nbsp;
to predict them, and we have no way to actually&nbsp;&nbsp;

00:42:19.680 --> 00:42:24.840
control them. It's because we can bump them in&nbsp;
one direction or bump them in another direction.&nbsp;&nbsp;

00:42:25.380 --> 00:42:26.820
But you don't know what else you're picking up.

00:42:26.820 --> 00:42:31.740
You don't know if they learned what you wanted&nbsp;
to learn. You know, they don't know what signal&nbsp;&nbsp;

00:42:31.740 --> 00:42:36.000
you actually sent to these systems because we&nbsp;
don't speak their language. We don't know what&nbsp;&nbsp;

00:42:36.000 --> 00:42:44.220
these numbers mean. We can't edit them like we can&nbsp;
edit code. So, yeah, go ahead and yeah. Yeah. So,&nbsp;&nbsp;

00:42:45.720 --> 00:42:51.060
what this leaves us with is we, this black&nbsp;
box, you have this big black box where we just&nbsp;&nbsp;

00:42:51.060 --> 00:42:55.320
put some stuff in, some weird magic happens&nbsp;
and then something comes out and you know,&nbsp;&nbsp;

00:42:56.100 --> 00:42:58.320
in many cases this is fine.

00:42:58.320 --> 00:43:02.040
Like, you know, you have like&nbsp;
a funny chatbot or something,&nbsp;&nbsp;

00:43:02.040 --> 00:43:05.400
right? And you make clear to your users,&nbsp;
hey, this is just for entertainment. Like,&nbsp;&nbsp;

00:43:05.400 --> 00:43:07.500
you know, don't take it seriously.&nbsp;
It might say something insulting.&nbsp;&nbsp;

00:43:08.580 --> 00:43:12.600
Yeah, it's fine. Like, you know, like, you know,&nbsp;
it's not going to, it's not going to kill anybody,&nbsp;&nbsp;

00:43:12.600 --> 00:43:16.440
right? Like, you know, you have like a fun&nbsp;
little, you know, like chatbot or something.

00:43:16.440 --> 00:43:22.020
Sure. Probably won't even kill anyone. So there&nbsp;
has recently been, I think one of the first&nbsp;&nbsp;

00:43:22.020 --> 00:43:26.640
deaths attribute to LLMs, where someone&nbsp;
committed suicide after maybe a chatbot,&nbsp;&nbsp;

00:43:26.640 --> 00:43:30.360
like encouraged them to, I don't know the details&nbsp;
about that, but I just heard that recently.&nbsp;&nbsp;

00:43:32.100 --> 00:43:35.460
and I don't know any other details about it.

00:43:35.460 --> 00:43:43.560
And so, What? So, the interesting&nbsp;
thing here at the core is that&nbsp;&nbsp;

00:43:45.840 --> 00:43:51.300
we have no idea what these things will do. And&nbsp;
if that's what we want, then fine. Right? If we&nbsp;&nbsp;

00:43:51.300 --> 00:43:55.440
have a bounded, it's, you know, it just talks.&nbsp;
Just talks some stuff and we're okay with it,&nbsp;&nbsp;

00:43:55.440 --> 00:44:00.120
saying bad things or encouraging&nbsp;
suicide, then sure, fine, who cares?

00:44:00.120 --> 00:44:03.120
But obviously this is not good enough&nbsp;
on a long term when we're dealing with&nbsp;&nbsp;

00:44:03.120 --> 00:44:07.860
actually powerful systems that can do, you&nbsp;
know, can do science and can, you know,&nbsp;&nbsp;

00:44:07.860 --> 00:44:12.900
interact with the world and manipulate humans&nbsp;
and, you know, whatever. Right? Obviously,&nbsp;&nbsp;

00:44:12.900 --> 00:44:17.100
this is not a good enough safety property&nbsp;
of, you know, like, this is not good enough.

00:44:17.100 --> 00:44:21.780
So, with CoEm, the goal is&nbsp;
we want to build systems&nbsp;&nbsp;

00:44:22.560 --> 00:44:27.780
that we're, we're, we're focusing and basically&nbsp;
on a simpler property than alignment. So,&nbsp;&nbsp;

00:44:27.780 --> 00:44:33.900
alignment is basically too hard. So, alignment&nbsp;
would be, the system knows what you want. Wants&nbsp;&nbsp;

00:44:33.900 --> 00:44:39.180
to do that too and does everything in its power&nbsp;
to get you what you truly want and like, but you,&nbsp;&nbsp;

00:44:39.180 --> 00:44:43.860
it means like all of humanity, like it, you&nbsp;
know, it figures out what all of humans want.

00:44:43.860 --> 00:44:48.540
It negotiates like, okay, how could we&nbsp;
like to get everyone most of the good&nbsp;&nbsp;

00:44:48.540 --> 00:44:53.640
things possible? How could we adjudicate&nbsp;
various disputes? And then it does that,&nbsp;&nbsp;

00:44:53.640 --> 00:44:58.620
obviously this is absurdly, hilariously&nbsp;
impossibly hard. I don't think it's&nbsp;&nbsp;

00:44:58.620 --> 00:45:02.820
impossible. It's just extremely&nbsp;
hard, especially on the first try.

00:45:03.360 --> 00:45:10.380
So, what I'm aiming for is more of a&nbsp;
subset of this problem. So, the subset&nbsp;&nbsp;

00:45:10.380 --> 00:45:16.440
is what I call boundedness. So, what I, when I&nbsp;
say boundedness, what I mean is I want a system&nbsp;&nbsp;

00:45:17.040 --> 00:45:23.040
where I can know what it can't&nbsp;
or won't do before I even run it.&nbsp;&nbsp;

00:45:23.760 --> 00:45:29.460
So currently, I mentioned earlier&nbsp;
the ARC eval running on GPT-4.

00:45:30.000 --> 00:45:34.680
Where they tested, where the&nbsp;
model could do various dangerous&nbsp;&nbsp;

00:45:34.680 --> 00:45:39.660
things such as self-replicating and like&nbsp;
hacking stuff like this. And it didn't,&nbsp;&nbsp;

00:45:39.660 --> 00:45:47.760
for the most part though, it did lie to people&nbsp;
in that, captcha example. And so now there is a,&nbsp;&nbsp;

00:45:47.760 --> 00:45:52.740
there is a wrong influence that you can draw from&nbsp;
this. The wrong influence, which is of course the&nbsp;&nbsp;

00:45:52.740 --> 00:45:57.060
inference that OpenAI would like you to take&nbsp;
from this is that, well, it can't do this.

00:45:57.060 --> 00:46:01.860
Look, they told it to self-replicate,&nbsp;
and it didn't. Therefore, it can't.&nbsp;&nbsp;

00:46:01.860 --> 00:46:06.780
This is a wrong reasoning as I think&nbsp;
Turing was the person who said this&nbsp;&nbsp;

00:46:06.780 --> 00:46:15.180
best is you can never prove the absence of a&nbsp;
capability. Just because a certain prompt or&nbsp;&nbsp;

00:46:15.180 --> 00:46:19.740
a certain setup didn't get the kind of&nbsp;
behaviour you wants, doesn't mean that&nbsp;&nbsp;

00:46:19.740 --> 00:46:22.920
there isn't some other one you don't know&nbsp;
about that does give you that behaviour.

00:46:23.700 --> 00:46:28.740
With GPT-3 and also GPT-4. Now we are seeing this&nbsp;
all the time that, you know, I would stuff like,&nbsp;&nbsp;

00:46:28.740 --> 00:46:33.240
stuff like jail break prompts like that, there's&nbsp;
like whole classes of behaviour the default model&nbsp;&nbsp;

00:46:33.240 --> 00:46:38.820
will not do. Once you use a jailbreak prompt,&nbsp;
then it will suddenly happily do all these things.

00:46:38.820 --> 00:46:42.120
So obviously they did have these capabilities&nbsp;
and they were accessible. You were just&nbsp;&nbsp;

00:46:42.120 --> 00:46:48.420
doing the prompt wrong. So, I want to&nbsp;
build systems where I can know ahead&nbsp;&nbsp;

00:46:48.420 --> 00:46:55.620
of time. I can tell it will never do X. It&nbsp;
cannot do X. And then I want these systems&nbsp;&nbsp;

00:46:56.160 --> 00:47:03.720
to reason like humans. So, what I mean by this&nbsp;
is, is why it's called cognitive emulation.

00:47:03.720 --> 00:47:09.780
I want to emulate human cognition. So,&nbsp;
another core problem of why like GPT&nbsp;&nbsp;

00:47:09.780 --> 00:47:15.180
systems are or will be very dangerous is&nbsp;
because their cognition is not human. So,&nbsp;&nbsp;

00:47:15.180 --> 00:47:19.140
this is very important. It's easy&nbsp;
to look at GPT and say, oh look,&nbsp;&nbsp;

00:47:19.140 --> 00:47:23.400
it's talking like a person, so it must be thinking&nbsp;
like a person. But this is completely wrong.

00:47:23.400 --> 00:47:29.580
There is no reason to believe this. Like no&nbsp;
human is trained on, you know, terabytes,&nbsp;&nbsp;

00:47:29.580 --> 00:47:34.020
random texts on the internet for trillions&nbsp;
of years while having no set body system&nbsp;&nbsp;

00:47:34.020 --> 00:47:39.780
whatsoever and memorising all these things&nbsp;
and like obviously not. Like obviously it&nbsp;&nbsp;

00:47:39.780 --> 00:47:44.640
is an alien mimicking a human. It is an&nbsp;
alien with, you know, a little happy,&nbsp;&nbsp;

00:47:44.640 --> 00:47:49.920
smiley face mask on that makes it look&nbsp;
sort of human to you, but it's an alien.

00:47:49.920 --> 00:47:54.420
And if you use like jailbreaking proms or&nbsp;
I know if you saw like the self-replicating&nbsp;&nbsp;

00:47:54.420 --> 00:48:00.600
ASCII cats and bingeing and such, where like,&nbsp;
you could get like, especially Bing, chatbot,&nbsp;&nbsp;

00:48:00.600 --> 00:48:04.500
which is an early version of GPT-4, you&nbsp;
can get to do the most insane things,&nbsp;&nbsp;

00:48:04.500 --> 00:48:09.480
like when things was like, you can get it to like&nbsp;
output, like these like ASCII pictures of cats.

00:48:09.480 --> 00:48:13.740
And these cats would say, oh, we are the&nbsp;
overlords. We take over now. And then&nbsp;&nbsp;

00:48:13.740 --> 00:48:17.160
whenever you try to prompt it away from&nbsp;
that, the cats would come back and like,&nbsp;&nbsp;

00:48:17.160 --> 00:48:22.080
take over your prompts and like it and stuff like&nbsp;
that, which is just like, I mean, it's amusing.&nbsp;&nbsp;

00:48:22.080 --> 00:48:25.320
Like this is very funny. Like when I saw&nbsp;
this I was like, ah, this is very funny.

00:48:26.160 --> 00:48:33.120
But also, that's not how humans work, like,&nbsp;
like humans are. Of course not. So, so,

00:48:33.120 --> 00:48:41.640
But, but just on the, on the, on the tech,&nbsp;
you’re, you're, you're still talking about&nbsp;&nbsp;

00:48:44.700 --> 00:48:54.360
scaled up transformer models. So, and, and how do&nbsp;
you, I mean, is it in the training that, that you

00:48:55.740 --> 00:48:57.480
Okay. good question. So, yeah, good question.

00:48:57.480 --> 00:49:03.120
So, I was first explaining the specification,&nbsp;
like what is the, the system that,&nbsp;&nbsp;

00:49:03.660 --> 00:49:07.320
what should it accomplish? Right now,&nbsp;
we're talking about implementation and so,&nbsp;&nbsp;

00:49:08.820 --> 00:49:14.160
many implementations are not yet done, or we don't&nbsp;
know how to do them yet, start to figure that out.&nbsp;&nbsp;

00:49:14.160 --> 00:49:18.540
Some of it, you know, is just like private and&nbsp;
just like, you know, wouldn't share necessarily.

00:49:18.540 --> 00:49:25.080
But in general, this is the resulting&nbsp;
system I expect that has these properties&nbsp;&nbsp;

00:49:25.080 --> 00:49:28.920
and that it reasons like a human. And then&nbsp;
importantly, it also fails like a human.&nbsp;&nbsp;

00:49:29.640 --> 00:49:34.980
It is bounded so you can know what it&nbsp;
won't do ahead of time. And another&nbsp;&nbsp;

00:49:34.980 --> 00:49:39.240
thing is I want causal stories or&nbsp;
traces of why does it make decisions?

00:49:39.240 --> 00:49:42.600
And these stories have to be causal.&nbsp;
Like currently, you can ask GPT,&nbsp;&nbsp;

00:49:42.600 --> 00:49:48.540
why did you do that? And it'll give&nbsp;
you some story, but there's no reason&nbsp;&nbsp;

00:49:48.540 --> 00:49:52.500
to believe these stories. Like you can just&nbsp;
ask it differently or whatever. And it'll do&nbsp;&nbsp;

00:49:52.500 --> 00:49:54.840
something completely like it doesn't,&nbsp;
it doesn't listen to its own stories.

00:49:54.840 --> 00:49:59.220
It just makes some shit up. And so, I&nbsp;
want systems that give you a trace or&nbsp;&nbsp;

00:49:59.220 --> 00:50:03.300
a story of like, why was this decision&nbsp;
made? All the nodes, all the actions,&nbsp;&nbsp;

00:50:03.300 --> 00:50:08.760
all the thoughts that led to this and how&nbsp;
can you modify them? So importantly, as you&nbsp;&nbsp;

00:50:08.760 --> 00:50:14.160
can probably guess from this kind of description,&nbsp;
this system is not a large, large neural network.

00:50:14.160 --> 00:50:18.240
There may be large neural networks&nbsp;
involved in this system. There may be&nbsp;&nbsp;

00:50:18.840 --> 00:50:23.220
points in this system where you use&nbsp;
large neural networks in particular.&nbsp;&nbsp;

00:50:23.220 --> 00:50:26.580
I think this is going to be extremely&nbsp;
necessary. I expect that large language&nbsp;&nbsp;

00:50:26.580 --> 00:50:30.180
models for various technical reasons are&nbsp;
very necessary for this kind of plan.

00:50:30.180 --> 00:50:33.960
Well, they're not strictly necessary, but&nbsp;
they're the easiest way to get it done.&nbsp;&nbsp;

00:50:33.960 --> 00:50:39.900
The way I expect a full spectrum CoEm system to&nbsp;
look, which is of course, to be clear, is still&nbsp;&nbsp;

00:50:39.900 --> 00:50:44.640
completely hypothetical, not such a system.&nbsp;
What it would look like is, it would be more a&nbsp;&nbsp;

00:50:46.140 --> 00:50:49.800
system, not a model. It'd be a&nbsp;
system which involves, you know,&nbsp;&nbsp;

00:50:49.800 --> 00:50:55.920
normal code and neural networks and data&nbsp;
structures and verifiers and like whatever that&nbsp;&nbsp;

00:50:56.460 --> 00:51:02.760
if you give it a normal human that you&nbsp;
can, you can make it do any normal thing,&nbsp;&nbsp;

00:51:02.760 --> 00:51:08.520
any normal human like intelligent human could&nbsp;
do, and it will then do that and only that.

00:51:09.900 --> 00:51:15.000
That is what the system would do. And then you&nbsp;
can be certain, you can look through the log of&nbsp;&nbsp;

00:51:15.000 --> 00:51:17.760
how it made a decision and you'd be like,&nbsp;
oh, at this point you made this decision,&nbsp;&nbsp;

00:51:17.760 --> 00:51:20.100
but what would've happened if&nbsp;
you had made this other decision?

00:51:20.100 --> 00:51:24.480
And then it would like to rerun. And then you&nbsp;
can control these things. Or you can be like,&nbsp;&nbsp;

00:51:24.480 --> 00:51:27.540
oh, you're making an inference here that I&nbsp;
don't like, or this doesn't make any sense,&nbsp;&nbsp;

00:51:27.540 --> 00:51:30.600
or whatever. Like if the difference between,&nbsp;&nbsp;

00:51:30.600 --> 00:51:35.880
say you want to develop a system that does&nbsp;
science, you want to develop a new solar cell.

00:51:35.880 --> 00:51:41.220
I don't know. Right. So, if you did this&nbsp;
with GPT, you know, 10, the way it would&nbsp;&nbsp;

00:51:41.220 --> 00:51:45.600
work is you type in, make me a new solar cell,&nbsp;
whatever, right? It crunches some numbers,&nbsp;&nbsp;

00:51:45.600 --> 00:51:51.480
and it spits out a blueprint for you. Mm-hmm.&nbsp;
Now you have no reason to trust this. Like,&nbsp;&nbsp;

00:51:51.480 --> 00:51:58.980
who knows what this blueprint actually is. It has,&nbsp;
it is not generated by a human reasoning process.

00:51:58.980 --> 00:52:04.800
You can ask GPT 10 to explain it to you,&nbsp;
but there's no reason those explanations&nbsp;&nbsp;

00:52:04.800 --> 00:52:08.580
have to be true. They might just&nbsp;
sound convincing. So of course,&nbsp;&nbsp;

00:52:08.580 --> 00:52:12.720
if GPT 10 was also malicious, it could, you&nbsp;
know, have hidden some kind of, you know,&nbsp;&nbsp;

00:52:13.260 --> 00:52:17.400
deadly flaw or device or whatever into&nbsp;
the blueprint that you don't detect.

00:52:17.400 --> 00:52:19.920
And if you ask it about it,&nbsp;
it will just lie to you.&nbsp;&nbsp;

00:52:20.880 --> 00:52:26.580
If you did the same thing with a hypothetical&nbsp;
CoEm system, such a system would give you a&nbsp;&nbsp;

00:52:26.580 --> 00:52:33.360
complete story, a complete causal graph of why&nbsp;
you should trust this output. I expect this&nbsp;&nbsp;

00:52:33.360 --> 00:52:39.300
and in and like why, and every step in this, in&nbsp;
this story is completely humanly understandable.

00:52:39.300 --> 00:52:43.080
It's no crazy alien reasoning&nbsp;
step. There's no like, you know,&nbsp;&nbsp;

00:52:43.080 --> 00:52:48.240
and then magic happened. There's no.&nbsp;
You know, massive computation that just&nbsp;&nbsp;

00:52:48.240 --> 00:52:52.080
makes no sense to a human whatsoever.&nbsp;
Every single step is human legible,&nbsp;&nbsp;

00:52:52.080 --> 00:52:56.760
human understandable, and the results of a&nbsp;
blueprint that you have a reason to trust.&nbsp;&nbsp;

00:52:56.760 --> 00:53:02.100
You have a reason to believe this is the thing&nbsp;
you actually asked for and not something else.

00:53:03.780 --> 00:53:08.340
And are you, where are you in&nbsp;
this research? Is this still&nbsp;&nbsp;

00:53:08.340 --> 00:53:11.640
sort of conceptualising the roadmap, or are you?

00:53:12.300 --> 00:53:17.340
we are in early, like experimentation&nbsp;
stages. so unfortunately, this is hard,&nbsp;&nbsp;

00:53:17.340 --> 00:53:22.920
and we are very research constrained. You know,&nbsp;
billions of dollars go to people like OpenAI, but&nbsp;&nbsp;

00:53:24.060 --> 00:53:28.440
it is not that easy to get money for&nbsp;
alignment, but we're working on it.

00:53:28.440 --> 00:53:32.220
So, we are very research constrained&nbsp;
and very talent constrained, but&nbsp;&nbsp;

00:53:32.220 --> 00:53:36.720
We have some really great people working on it&nbsp;
and us, you know, we do have some really powerful&nbsp;&nbsp;

00:53:36.720 --> 00:53:41.400
internal models and good, you know, software&nbsp;
working on it. So, we are making progress.&nbsp;&nbsp;

00:53:43.500 --> 00:53:49.920
but it takes time. So, a lot of why I spend a lot&nbsp;
of my work now thinking about slowing down AI.

00:53:49.920 --> 00:53:54.240
And like, how can we get regulators involved? How&nbsp;
can we get the public involved? Like, to be clear,&nbsp;&nbsp;

00:53:54.240 --> 00:53:58.860
I'm not just like, oh, you know, the regulators&nbsp;
should unilaterally decide on this. I'm like,&nbsp;&nbsp;

00:53:58.860 --> 00:54:02.460
hey, the public should be aware that there's&nbsp;
a small number of techno utopians over in&nbsp;&nbsp;

00:54:02.460 --> 00:54:06.060
Silicon Valley that you know, just want&nbsp;
to be like, let's be very explicit here.

00:54:06.060 --> 00:54:09.360
They want to be immortal; they want glory,&nbsp;
they want trillion, trillions of dollars,&nbsp;&nbsp;

00:54:09.360 --> 00:54:13.980
and they're willing to risk everything on&nbsp;
this. They're willing to risk building the&nbsp;&nbsp;

00:54:13.980 --> 00:54:16.980
most dangerous systems ever built. And&nbsp;
releasing on the internet, you know,&nbsp;&nbsp;

00:54:16.980 --> 00:54:22.380
to your, you know, to your, your friends,&nbsp;
your family, your, your, your community fully&nbsp;&nbsp;

00:54:22.380 --> 00:54:26.640
exposed to the full downsides of all these&nbsp;
systems with no regulatory input whatsoever.

00:54:26.640 --> 00:54:33.840
And like this is what the government is for,&nbsp;
is to like to stop that. Like this is such a&nbsp;&nbsp;

00:54:33.840 --> 00:54:38.520
clear-cut case of like, hey, Like, why is&nbsp;
the public not being consulted here? Like,&nbsp;&nbsp;

00:54:38.520 --> 00:54:43.560
this is not, you know, if this is just me&nbsp;
in my basement right. With my laptop and&nbsp;&nbsp;

00:54:43.560 --> 00:54:49.620
never showed the world anything, like,&nbsp;
you know, okay. You know, maybe, maybe.

00:54:49.620 --> 00:54:54.720
But that's not what's happening here. So, and&nbsp;
the reason this is also important is just like,&nbsp;&nbsp;

00:54:54.720 --> 00:55:00.180
alignment is hard. Boundedness is hard,&nbsp;
CoEm is hard. All these things are hard,&nbsp;&nbsp;

00:55:00.180 --> 00:55:05.700
and they take time. And currently all the&nbsp;
brightest minds and billions of dollars of&nbsp;&nbsp;

00:55:05.700 --> 00:55:11.880
funding are being pumped into accelerating the&nbsp;
building of these unsafe AI systems as fast as&nbsp;&nbsp;

00:55:11.880 --> 00:55:16.440
possible and releasing them as fast as possible&nbsp;
while safety research is not keeping pace.

00:55:17.040 --> 00:55:21.840
So, if we don't get more time and if we don't&nbsp;
solve, you know, maybe my proposal doesn't&nbsp;&nbsp;

00:55:21.840 --> 00:55:27.300
work out right? Sure. You know, science is hard,&nbsp;
but if we don't get someone's proposal to work,&nbsp;&nbsp;

00:55:27.300 --> 00:55:33.060
if not, we don't get some safety&nbsp;
algorithms or designs for AI systems,&nbsp;&nbsp;

00:55:33.060 --> 00:55:37.020
then it's not going to go well. And is that&nbsp;
going to matter how many trillions of dollars,&nbsp;&nbsp;

00:55:37.020 --> 00:55:40.260
you know, open AI makes off of it, or,&nbsp;
and Microsoft makes out of it or whatever?

00:55:41.400 --> 00:55:42.900
Cause they're not going to be around to enjoy it.

