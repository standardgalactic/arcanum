WEBVTT
Kind: captions
Language: en

00:00:05.760 --> 00:01:02.440
Waymo 
 
Drago&nbsp;

00:01:05.720 --> 00:04:21.720
So I was tech lead of 3D vision and post&nbsp;
estimation for Street View and got a lot&nbsp;&nbsp;

00:04:21.720 --> 00:04:26.960
of exposure to all kinds of platforms that the&nbsp;
team was collecting data with, including boats,&nbsp;&nbsp;

00:04:26.960 --> 00:04:35.080
snowmobiles, backpacks, and of course cars. And&nbsp;
afterwards, I got back a bit closer to my machine&nbsp;&nbsp;

00:04:35.080 --> 00:04:42.800
learning AI roots. I worked on the team and&nbsp;
eventually ended up leading a team that developed,&nbsp;&nbsp;

00:04:43.400 --> 00:04:51.880
even in those early years 2013 to 2015, large&nbsp;
neural networks for understanding images. And&nbsp;&nbsp;

00:04:51.880 --> 00:05:01.960
so in those times, we had a very exciting set of&nbsp;
developments and architectures between Vantage&nbsp;&nbsp;

00:05:01.960 --> 00:05:09.160
and we won the ImageNet challenges in 2014. That&nbsp;
used to be a very popular competition on object&nbsp;&nbsp;

00:05:09.160 --> 00:05:15.160
understanding and detection and classification.&nbsp;
So we won those two challenges in 2014 with&nbsp;&nbsp;

00:05:15.160 --> 00:05:24.000
the team that I lead and some collaborators from&nbsp;
Google Brain. And we also work a lot on launching&nbsp;&nbsp;

00:05:24.000 --> 00:05:31.800
the backends that would annotate Google Photos,&nbsp;
Google Photos was first of its kind to offer deep&nbsp;&nbsp;

00:05:31.800 --> 00:05:38.160
semantic annotation, photos enabling search, and&nbsp;
all the photos applications that they would do,&nbsp;&nbsp;

00:05:38.160 --> 00:05:45.120
where they suggest you various events and albums&nbsp;
and so on and organize them. That was based on&nbsp;&nbsp;

00:05:45.120 --> 00:05:51.760
backends and models that we developed at the&nbsp;
time, and that were launched in the Google data&nbsp;&nbsp;

00:05:51.760 --> 00:06:03.040
centers. And around 2015, when I felt that deep&nbsp;
learning has entered the level of maturity that is&nbsp;&nbsp;

00:06:03.040 --> 00:06:08.840
sufficient to potentially make autonomous driving&nbsp;
a reality, I got back into the autonomous vehicle&nbsp;&nbsp;

00:06:08.840 --> 00:06:16.120
space, into robotics that I had been exposed to&nbsp;
in my early lab days. And so I joined initially&nbsp;&nbsp;

00:06:16.120 --> 00:06:23.320
Zoox for two and a half years. It's an autonomous&nbsp;
driving startup. I led the perception team there,&nbsp;&nbsp;

00:06:23.320 --> 00:06:31.480
helping them build out their, essentially&nbsp;
3D understanding and the machine learning&nbsp;&nbsp;

00:06:31.480 --> 00:06:38.040
around it. And then I came to Waymo, right. So&nbsp;
that's my story more or less. I would say I'm a&nbsp;&nbsp;

00:06:38.040 --> 00:06:47.120
machine learning person. And my initial expertise&nbsp;
had been on understanding images and 3D data,&nbsp;&nbsp;

00:06:47.120 --> 00:06:54.880
geometry. And that's generalized to other&nbsp;
applications, since autonomous driving is a,&nbsp;&nbsp;

00:06:54.880 --> 00:07:02.680
it's a complete robotics stack, which includes&nbsp;
all the key robotic systems, perception, planning,&nbsp;&nbsp;

00:07:02.680 --> 00:07:09.960
behavior, prediction, simulation, mapping,&nbsp;
localization, and so on. So, by now I have&nbsp;&nbsp;

00:07:09.960 --> 00:07:15.980
this maybe more of a cross functional exposure to&nbsp;
all of this through the lens of machine learning.

00:07:15.980 --> 00:07:17.920
CRAIG
Your undergraduate&nbsp;&nbsp;

00:07:17.920 --> 00:07:25.480
was where? And then at Stanford, you were studying&nbsp;
under Daphne Koller. And what was that work about?

00:07:25.480 --> 00:07:26.720
Drago
Actually I did my undergrad&nbsp;&nbsp;

00:07:26.720 --> 00:07:33.360
at Stanford too but afterwards, I continued into&nbsp;
a PhD that took six years. So I spent a total of&nbsp;&nbsp;

00:07:33.360 --> 00:07:40.560
10 years at Stanford, it’s hard to get me to leave&nbsp;
for a while. I worked closely with Sebastian Thrun&nbsp;&nbsp;

00:07:40.560 --> 00:07:49.360
because I was Daphne's first PhD student that&nbsp;
worked on computer vision perception topics and we&nbsp;&nbsp;

00:07:49.360 --> 00:07:57.600
wanted access to robots and data from robots. And&nbsp;
at the time, Sebastian wasn't even at Stanford.&nbsp;&nbsp;

00:07:57.600 --> 00:08:04.160
But Daphne somehow made a connection with him. And&nbsp;
I ended up visiting his lab in Carnegie Mellon and&nbsp;&nbsp;

00:08:04.160 --> 00:08:09.320
collecting data with robots and shorting one&nbsp;
robot. I'm a software person, unfortunately&nbsp;&nbsp;

00:08:09.320 --> 00:08:16.760
there's things I learned the hard way there.&nbsp;
But Sebastian was always a commenter of sorts,&nbsp;&nbsp;

00:08:16.760 --> 00:08:24.640
even though my primary advisor was Daphne. I think&nbsp;
I've always worked with Sebastian as well, and we&nbsp;&nbsp;

00:08:24.640 --> 00:08:29.160
have joint works and he was a key influence too.
 
CRAIG&nbsp;

00:08:29.160 --> 00:08:34.640
Yeah. And so Waymo, when you&nbsp;
got involved with the company,&nbsp;&nbsp;

00:08:34.640 --> 00:08:43.040
where was it in its development and where are&nbsp;
you right now? I mean, I know you have robotaxis&nbsp;&nbsp;

00:08:43.040 --> 00:08:50.500
operating in San Francisco and Phoenix and I&nbsp;
hear you're gonna have them operating in LA.
 &nbsp;

00:08:50.500 --> 00:08:53.480
Drago
Yeah, we have San Francisco and Phoenix&nbsp;&nbsp;

00:08:53.480 --> 00:08:59.440
today. You can experience autonomous driving&nbsp;
it exists at scale in these cities. We give&nbsp;&nbsp;

00:09:00.640 --> 00:09:07.400
10s of 1000s of rides to users in both cities&nbsp;
every week. And Waymo overall, has given our a&nbsp;&nbsp;

00:09:07.400 --> 00:09:15.320
million of paid rides, driving over 10 million&nbsp;
miles autonomously as well. So that is the two&nbsp;&nbsp;

00:09:15.320 --> 00:09:21.480
main markets that we have. The Phoenix area that&nbsp;
we cover today is around five times the size of&nbsp;&nbsp;

00:09:21.480 --> 00:09:30.640
San Francisco. And next, we're looking to expand&nbsp;
and start actually paid rides. In the coming weeks&nbsp;&nbsp;

00:09:30.640 --> 00:09:38.720
in Los Angeles. We got a permit a couple of weeks&nbsp;
ago, to expand to Los Angeles and charge and so we&nbsp;&nbsp;

00:09:38.720 --> 00:09:48.200
will start doing this. And we also started doing&nbsp;
driverless rides for employees in Austin, Texas.&nbsp;&nbsp;

00:09:48.200 --> 00:09:54.280
So we're going to be expanding in this fourth&nbsp;
market that we have announced as well. Now, my&nbsp;&nbsp;

00:09:54.280 --> 00:10:01.520
time at Waymo has been a wonderful time, seeing&nbsp;
a transformation of the stack and getting to&nbsp;&nbsp;

00:10:01.520 --> 00:10:08.560
maturity that I think has been a great experience,&nbsp;
and working with the talented team. I think when I&nbsp;&nbsp;

00:10:08.560 --> 00:10:17.200
joined Waymo in 2018, In the summer, Waymo had hit&nbsp;
some great milestones. One of them was, in 2015&nbsp;&nbsp;

00:10:17.200 --> 00:10:24.600
it gave the first fully autonomous ride in Austin&nbsp;
actually, a city we're getting back to. The person&nbsp;&nbsp;

00:10:24.600 --> 00:10:33.880
called Steve Mahan, friend of the company, as a&nbsp;
blind person, the first ride involved him. And&nbsp;&nbsp;

00:10:33.880 --> 00:10:39.800
so we had done that and we were working towards&nbsp;
our first commercial deployment. And so we were&nbsp;&nbsp;

00:10:39.800 --> 00:10:48.000
evolving the stack to make sure that it will be a&nbsp;
safe and scalable deployment. And this eventually&nbsp;&nbsp;

00:10:48.000 --> 00:10:55.840
happened in 2020, when we launched our Waymo One&nbsp;
service in Phoenix, East Valley, around Chandler.&nbsp;&nbsp;

00:10:55.840 --> 00:11:02.160
And so that's been the journey. So, evolved&nbsp;
the first fully autonomous service in Phoenix,&nbsp;&nbsp;

00:11:02.160 --> 00:11:10.360
East Valley, then get to San Francisco, and then&nbsp;
significantly upgrade the stack to be able to&nbsp;&nbsp;

00:11:10.360 --> 00:11:16.520
handle the complex, dense urban environment of&nbsp;
San Francisco. And now we continue to all these,&nbsp;&nbsp;

00:11:16.520 --> 00:11:23.280
more cities expanding in Phoenix, we will be&nbsp;
working also on tackling more highways, which is&nbsp;&nbsp;

00:11:23.280 --> 00:11:30.160
important in large areas like Phoenix. Our area,&nbsp;
if we don't go on highways takes almost an hour&nbsp;&nbsp;

00:11:30.160 --> 00:11:38.920
to cross on certain streets today. So, that's I&nbsp;
think, maybe the trajectory of the last five and&nbsp;&nbsp;

00:11:38.920 --> 00:11:45.000
a half years. It's been tremendous also in terms&nbsp;
of development of machine learning and machine&nbsp;&nbsp;

00:11:45.000 --> 00:11:52.760
learning technology. So in 2015, when I joined,&nbsp;
oh 2018 Excuse me, when I joined Waymo there was&nbsp;&nbsp;

00:11:52.760 --> 00:11:58.880
already a lot of impressive machine learning work.&nbsp;
Especially, there was a lot of really interesting,&nbsp;&nbsp;

00:11:58.880 --> 00:12:05.880
unique work on 3D perception using LIDAR. And&nbsp;
this was in collaboration with Alex Krizhevsky,&nbsp;&nbsp;

00:12:05.880 --> 00:12:12.440
and the Dali Angelo, and other people from Google&nbsp;
Brain, Alex Krizhevsky being of the 2012 AlexNet&nbsp;&nbsp;

00:12:12.440 --> 00:12:20.480
fame. So he took a lot of interest in autonomous&nbsp;
driving as an application. And he worked a lot&nbsp;&nbsp;

00:12:20.480 --> 00:12:27.600
with Waymo and developed some really interesting&nbsp;
work, including inspiration to a first paper Waymo&nbsp;&nbsp;

00:12:27.600 --> 00:12:35.200
ever did, even though we published it in 20, early&nbsp;
2019. It's called ChauffeurNet, it's actually&nbsp;&nbsp;

00:12:35.200 --> 00:12:44.080
a model, a train model to predict and drive by&nbsp;
imitating others. That was really groundbreaking,&nbsp;&nbsp;

00:12:44.080 --> 00:12:50.720
unique work at the time. And then that was still&nbsp;
relatively early in its development and maturity,&nbsp;&nbsp;

00:12:50.720 --> 00:12:55.720
but it was very exciting as a concept. So we&nbsp;
publish that and we took all these technologies,&nbsp;&nbsp;

00:12:55.720 --> 00:13:01.360
and we worked a ton on expanding them and&nbsp;
transforming Waymo stack. I think at this&nbsp;&nbsp;

00:13:01.360 --> 00:13:10.200
point Waymo is an AI first company. So we strive&nbsp;
to maximize the impact of machine learning and its&nbsp;&nbsp;

00:13:10.200 --> 00:13:12.640
power across most of our applications. 
 
CRAIG&nbsp;

00:13:12.640 --> 00:13:21.240
Yep, I haven't spoken to anybody about&nbsp;
autonomous driving for a while. I did&nbsp;&nbsp;

00:13:21.240 --> 00:13:29.200
an episode with Alex Kendall, at Wayve AI, but we&nbsp;
were really talking about world models. In the in&nbsp;&nbsp;

00:13:29.200 --> 00:13:38.920
the tech stack is Waymo, from what I've read,&nbsp;
you have detailed custom maps for navigation,&nbsp;&nbsp;

00:13:38.920 --> 00:13:47.240
you have the suite of sensors, and then you have&nbsp;
neural nets that are kind of wired together to&nbsp;&nbsp;

00:13:47.240 --> 00:13:59.280
process that data and guide the car. How much&nbsp;
hard coded stuff is in that stack, and how much&nbsp;&nbsp;

00:13:59.280 --> 00:14:01.600
is pure machine learning?
 
Drago&nbsp;

00:14:01.600 --> 00:14:06.120
So big parts of the whole stack are machine&nbsp;
learning. And this spans all the core systems&nbsp;&nbsp;

00:14:06.120 --> 00:14:16.880
of Waymo. This spans things like the perception&nbsp;
system. And it also covers major models,&nbsp;&nbsp;

00:14:16.880 --> 00:14:23.200
applying machine learning to planning,&nbsp;
behavior prediction or others and simulation,&nbsp;&nbsp;

00:14:23.200 --> 00:14:29.120
and also lots of machine learning for employed&nbsp;
also in evaluation extracting signal from,&nbsp;&nbsp;

00:14:29.120 --> 00:14:34.640
for example, simulation or other tests we&nbsp;
perform on the car. So at this point, I&nbsp;&nbsp;

00:14:34.640 --> 00:14:42.480
would say all major parts of Waymo have powerful,&nbsp;
large machine learning models underpinning them.
 &nbsp;

00:14:42.480 --> 00:14:46.520
CRAIG
Are all those models on the car,&nbsp;&nbsp;

00:14:46.520 --> 00:14:50.400
or what's on the car, what's in the cloud?
 
Drago&nbsp;

00:14:50.400 --> 00:14:57.000
So we have some flexibility. I think, especially&nbsp;
the whole evaluation system simulator, that is&nbsp;&nbsp;

00:14:57.000 --> 00:15:03.520
in the cloud, right, because we develop a variant&nbsp;
of the stack, and then we need to test it. And we&nbsp;&nbsp;

00:15:03.520 --> 00:15:11.640
tested in the data centers to a large extent, even&nbsp;
though we also tested with human safety operators.&nbsp;&nbsp;

00:15:11.640 --> 00:15:17.880
And we want to have specialized areas where we&nbsp;
can stage certain rare situations. But testing&nbsp;&nbsp;

00:15:17.880 --> 00:15:26.320
in the cloud and simulation is key. I think&nbsp;
generally we avoid requiring a human to drive&nbsp;&nbsp;

00:15:26.320 --> 00:15:33.360
our cars at anytime. It's possible to ask a human&nbsp;
occasionally, specific question in some very,&nbsp;&nbsp;

00:15:33.360 --> 00:15:39.280
very rare situations when preferably the vehicle&nbsp;
is usually, almost always, the vehicle is stopped&nbsp;&nbsp;

00:15:39.280 --> 00:15:47.320
at that time, but that is also extremely rare.&nbsp;
And we're trying to minimize this. So the vast&nbsp;&nbsp;

00:15:47.320 --> 00:15:54.200
majority of the system that drive the car, they're&nbsp;
on the car on the local compute, and they operate&nbsp;&nbsp;

00:15:54.760 --> 00:15:57.836
fully autonomously as advertised.
 
CRAIG&nbsp;

00:15:57.836 --> 00:16:03.980
And so how many models or neural networks&nbsp;
are involved in the driving system?
 &nbsp;

00:16:03.980 --> 00:16:06.800
Drago
So there are several, right. I think&nbsp;&nbsp;

00:16:06.800 --> 00:16:15.240
the history of our space is one of consolidating&nbsp;
models, right. And traditionally, many players in&nbsp;&nbsp;

00:16:15.240 --> 00:16:21.200
the space, and also when machine learning was&nbsp;
in its relatively early stages and there was&nbsp;&nbsp;

00:16:21.200 --> 00:16:28.320
limited computing data, it has been beneficial&nbsp;
to, or at least at the time, to partition the&nbsp;&nbsp;

00:16:28.320 --> 00:16:35.280
tasks and have maybe one neural net to detect&nbsp;
bounding boxes and another to segment, say,&nbsp;&nbsp;

00:16:35.280 --> 00:16:41.360
the scene into semantic segmentation of various&nbsp;
kinds and another maybe—. So traditionally,&nbsp;&nbsp;

00:16:41.360 --> 00:16:50.240
that was the history. And now, what's changed&nbsp;
is we understand the power of what, the models&nbsp;&nbsp;

00:16:50.240 --> 00:16:57.560
have become a lot more powerful, they're scaling&nbsp;
properties have become much improved, especially&nbsp;&nbsp;

00:16:57.560 --> 00:17:05.600
with the advent of transformers, we understand&nbsp;
a lot better how to make models do well on many,&nbsp;&nbsp;

00:17:05.600 --> 00:17:10.720
many tasks at the same time. That's actually&nbsp;
been an area of research by us and the whole&nbsp;&nbsp;

00:17:10.720 --> 00:17:15.520
community. And some of these AI trends can&nbsp;
be leveraged to train very large models,&nbsp;&nbsp;

00:17:15.520 --> 00:17:21.640
to transfer knowledge from the internet, and fine&nbsp;
tune on your tasks. So we have ways to make models&nbsp;&nbsp;

00:17:21.640 --> 00:17:28.840
do well and a lot more tasks today. And there&nbsp;
is benefit then, in efficiency, in simplicity,&nbsp;&nbsp;

00:17:29.440 --> 00:17:34.920
and quality usually, to consolidate. So there&nbsp;
has been a push to consolidate. That said,&nbsp;&nbsp;

00:17:34.920 --> 00:17:39.040
it's an ongoing process. There's some very&nbsp;
interesting question whether you should have one&nbsp;&nbsp;

00:17:39.040 --> 00:17:48.320
model or several, there is challenges in having&nbsp;
just one model in our case, especially if you need&nbsp;&nbsp;

00:17:48.320 --> 00:17:55.760
to iterate on it and ensure it satisfies I mean,&nbsp;
hundreds to 1000s of requirements, right. You&nbsp;&nbsp;

00:17:55.760 --> 00:18:04.080
need some way to be able to do work on parallel&nbsp;
problems in this domain, to ensure safety. And so&nbsp;&nbsp;

00:18:04.080 --> 00:18:11.560
there is this constraint. And there is some other&nbsp;
constraints that make it difficult for our case,&nbsp;&nbsp;

00:18:11.560 --> 00:18:19.640
for example, compared to say, traditional&nbsp;
robotics, maybe you talk to Peter Novig or Vincent&nbsp;&nbsp;

00:18:19.640 --> 00:18:25.600
Vanhoucke they have it on a robot, and then they&nbsp;
have a world model and they can train it end to&nbsp;&nbsp;

00:18:25.600 --> 00:18:31.800
end. For us, it's more challenging infrastructure&nbsp;
wise, because we have, say, over a dozen camera to&nbsp;&nbsp;

00:18:31.800 --> 00:18:39.440
two dozens, we have a few LiDAR, we have a few&nbsp;
radar fitting all this data for all in one on&nbsp;&nbsp;

00:18:39.440 --> 00:18:46.320
the computer and scaling it is quite daunting. And&nbsp;
that even motivates potentially training things in&nbsp;&nbsp;

00:18:46.320 --> 00:18:56.280
stages and so on, right. So I would say that the&nbsp;
answer is we are evolving, and we are evolving,&nbsp;&nbsp;

00:18:56.280 --> 00:19:02.320
satisfying, both when the learnings in the&nbsp;
industry and the requirements in production,&nbsp;&nbsp;

00:19:02.320 --> 00:19:04.920
and generally the trend is to consolidate.
 
CRAIG&nbsp;

00:19:04.920 --> 00:19:12.760
Yeah. And I know it's a very complicated system,&nbsp;
but can you can you sort of walk us through how&nbsp;&nbsp;

00:19:12.760 --> 00:19:21.040
the car drives? I mean, they're the sensors and&nbsp;
they're feeding the data into a perception model,&nbsp;&nbsp;

00:19:21.040 --> 00:19:25.420
and then the perception model, I mean,&nbsp;
just kind of walk me through the pathway.

00:19:25.420 --> 00:19:27.280
Drago
So, the simplest way to think&nbsp;&nbsp;

00:19:27.280 --> 00:19:34.480
is the onboard system, right. And there's a lot&nbsp;
of very interesting problems and questions when&nbsp;&nbsp;

00:19:34.480 --> 00:19:40.200
you build the simulator as well. But let's talk&nbsp;
about, which is also machine learning task to me,&nbsp;&nbsp;

00:19:40.200 --> 00:19:45.200
by the way, I'm a machine learning person and&nbsp;
ultimately, we want to solve problem with the data&nbsp;&nbsp;

00:19:45.200 --> 00:19:52.960
we have as much as possible. But the traditional&nbsp;
autonomous vehicle stack takes data from sensors,&nbsp;&nbsp;

00:19:52.960 --> 00:19:58.000
can be camera, LiDAR, or radar. And then there&nbsp;
is a perception stack that processes this data,&nbsp;&nbsp;

00:19:58.000 --> 00:20:03.120
and creates a representation of the world.&nbsp;
And this representation of the world,&nbsp;&nbsp;

00:20:03.120 --> 00:20:12.760
one of the key aspects of it is it wants to&nbsp;
be expressive, but also concise, right. And in&nbsp;&nbsp;

00:20:12.760 --> 00:20:18.680
that representation, then you train your behavior&nbsp;
and potentially simulation models because that's&nbsp;&nbsp;

00:20:18.680 --> 00:20:23.080
a representation of the world, and then you&nbsp;
need to in that representation of the world,&nbsp;&nbsp;

00:20:23.080 --> 00:20:28.720
you need to plan what you're doing. And you need&nbsp;
to also potentially imagine what the world is&nbsp;&nbsp;

00:20:28.720 --> 00:20:33.560
going to do in response to your plans to validate&nbsp;
that your class can be safe. So essentially,&nbsp;&nbsp;

00:20:33.560 --> 00:20:38.200
there's this main two parts to the stack&nbsp;
perception produces a model of the world,&nbsp;&nbsp;

00:20:38.200 --> 00:20:43.160
and then in that model of the world in plan, you&nbsp;
validate your plans, that's the behavior system.&nbsp;&nbsp;

00:20:43.160 --> 00:20:48.560
And the model of the world is very interesting.&nbsp;
So there is a lot of interesting question of what&nbsp;&nbsp;

00:20:48.560 --> 00:20:55.640
it can or should be. Traditionally, we have&nbsp;
opted for at least partially interpretable,&nbsp;&nbsp;

00:20:55.640 --> 00:21:02.400
intermediate representation. So one of the things&nbsp;
that has been very beneficial in our space, and I&nbsp;&nbsp;

00:21:02.400 --> 00:21:08.640
think it's generally superior to treating all the&nbsp;
sensors as a, an ordered collection of sensors is&nbsp;&nbsp;

00:21:08.640 --> 00:21:15.080
to take their data, fuse their representations in&nbsp;
the knowledge provided to them, and create what&nbsp;&nbsp;

00:21:15.080 --> 00:21:22.320
I call a bird's eye view model of the world. So&nbsp;
you, essentially, you center a grid, or implicit&nbsp;&nbsp;

00:21:22.320 --> 00:21:29.000
grid on the space around the vehicle, and you&nbsp;
essentially populate this grid with everything,&nbsp;&nbsp;

00:21:29.000 --> 00:21:35.560
all the information that you require from the&nbsp;
sensor. So that's been very popular and there&nbsp;&nbsp;

00:21:35.560 --> 00:21:40.760
is certain benefits to having interpretable&nbsp;
representations, people understand them,&nbsp;&nbsp;

00:21:40.760 --> 00:21:45.920
they know what the problems are, they&nbsp;
can annotate or correct them. And so,&nbsp;&nbsp;

00:21:45.920 --> 00:21:51.120
the drawback of those is you need to label data&nbsp;
to create them typically, they're representations&nbsp;&nbsp;

00:21:51.120 --> 00:21:58.760
that we have chosen, because they satisfy&nbsp;
requirements interpretability, testability,&nbsp;&nbsp;

00:21:58.760 --> 00:22:06.320
right. Potentially people can look at the outputs&nbsp;
of a system and impose further constraints or&nbsp;&nbsp;

00:22:06.320 --> 00:22:10.040
corrections that's why you asked me,&nbsp;
is all your system just a neural net,&nbsp;&nbsp;

00:22:10.040 --> 00:22:15.520
our system is hybrid even though big parts of it&nbsp;
are neural net and we strive to solve the majority&nbsp;&nbsp;

00:22:15.520 --> 00:22:21.160
of the problem with machine In learning, we still&nbsp;
build such a presentation and design where you can&nbsp;&nbsp;

00:22:21.160 --> 00:22:28.600
control and steer this system by experts, right,&nbsp;
that can introspect the results and introduce&nbsp;&nbsp;

00:22:28.600 --> 00:22:34.480
additional constraints requirements on top of what&nbsp;
the machine learning models do. And that has been&nbsp;&nbsp;

00:22:34.480 --> 00:22:37.160
very beneficial for robustness and safety.
 
CRAIG&nbsp;

00:22:37.160 --> 00:22:45.040
I guess that's where Wayve is different. They,&nbsp;
they're creating a world model and then everything&nbsp;&nbsp;

00:22:45.040 --> 00:22:50.600
happens, all the planning happens in that,&nbsp;
the simulation and planning happens within&nbsp;&nbsp;

00:22:50.600 --> 00:22:51.760
the world model.
 
Drago&nbsp;

00:22:51.760 --> 00:22:56.480
I actually don't think they’re that different,&nbsp;
honestly. I think that it was oversold. I have&nbsp;&nbsp;

00:22:56.480 --> 00:23:01.680
a perception system, it produces a compressed&nbsp;
version of the world, in our case, some of it&nbsp;&nbsp;

00:23:01.680 --> 00:23:09.120
is interpretable but you can also have various&nbsp;
embeddings or quantized embeddings of tokens,&nbsp;&nbsp;

00:23:09.120 --> 00:23:14.520
trained end to end that can also be&nbsp;
produced, right. It's a question of&nbsp;&nbsp;

00:23:14.520 --> 00:23:21.080
choices in that representation. And then given&nbsp;
that representation, you have prediction models,&nbsp;&nbsp;

00:23:21.080 --> 00:23:26.120
and these are you can call them, they’re world&nbsp;
models. And there is a question of which exactly&nbsp;&nbsp;

00:23:26.120 --> 00:23:31.960
world models do build? Do you build the model that&nbsp;
just dreams all the video, all your cameras? Well,&nbsp;&nbsp;

00:23:31.960 --> 00:23:40.400
you can but on board, maybe that is overkill.&nbsp;
So then which things do you choose to dream and&nbsp;&nbsp;

00:23:40.400 --> 00:23:47.000
autoregressive or predict concisely so that you&nbsp;
still satisfy your requirements, that's a very&nbsp;&nbsp;

00:23:47.000 --> 00:23:54.000
heavy way. But we still have major machine&nbsp;
learning models that are still word models,&nbsp;&nbsp;

00:23:54.000 --> 00:24:01.200
just the design of them may be different. And&nbsp;
we also can train end to end. However, right,&nbsp;&nbsp;

00:24:01.200 --> 00:24:07.640
we have also put historically a lot of focus&nbsp;
on having intermediate representations because&nbsp;&nbsp;

00:24:07.640 --> 00:24:14.080
we have found they're very important so far,&nbsp;
in actually building a fully working system.&nbsp;&nbsp;

00:24:14.080 --> 00:24:20.040
Machine learning is great, but it has limitations,&nbsp;
there's always cases where eventually you want to&nbsp;&nbsp;

00:24:20.040 --> 00:24:26.560
impose guardrails of constraints and intermediate&nbsp;
representations are very powerful for doing it.&nbsp;&nbsp;

00:24:26.560 --> 00:24:35.480
Now, lately, there's been yet one more trend with&nbsp;
large language models and the like. Now there is a&nbsp;&nbsp;

00:24:35.480 --> 00:24:42.280
lot, kind of language based representations.&nbsp;
So language compatible embeddings are very,&nbsp;&nbsp;

00:24:42.280 --> 00:24:48.960
very popular for robotics because they bring a ton&nbsp;
of knowledge from the internet and the that said,&nbsp;&nbsp;

00:24:48.960 --> 00:24:53.120
the model, the language models that they&nbsp;
trained on, which makes it, I mean, the system&nbsp;&nbsp;

00:24:53.120 --> 00:24:59.040
understands a lot more common sense and be able&nbsp;
to interact with humans. So now that is also very&nbsp;&nbsp;

00:24:59.040 --> 00:25:07.200
interesting to us, right. So language or language&nbsp;
compatible representations are great. That said,&nbsp;&nbsp;

00:25:07.200 --> 00:25:13.760
there is also another learning in our domain, you&nbsp;
really need these representations to be spatially&nbsp;&nbsp;

00:25:13.760 --> 00:25:18.920
correct. So our models, perception system needs&nbsp;
to be very powerful in spatial reasoning because&nbsp;&nbsp;

00:25:18.920 --> 00:25:25.480
spatial reasoning, the ability to represent the&nbsp;
world and ultimately evaluate your plans in the&nbsp;&nbsp;

00:25:25.480 --> 00:25:30.640
world is key to ensuring safety. And so these&nbsp;
concepts are now overlapping and I think we have&nbsp;&nbsp;

00:25:30.640 --> 00:25:37.840
a lot of exciting, both experience, expertise, and&nbsp;
of course, we're also experimenting with the best&nbsp;&nbsp;

00:25:37.840 --> 00:25:40.640
mix of these concepts.
 
CRAIG&nbsp;

00:25:40.640 --> 00:25:46.680
Yeah. I mean, I just had Vincent Vanhoucke&nbsp;
on the podcast talking about RT2. And in that&nbsp;&nbsp;

00:25:46.680 --> 00:25:47.474
system, which is amazing, and I mean,&nbsp;
stuff is advancing so quickly, how do you—
 &nbsp;

00:25:47.474 --> 00:25:50.040
Drago
We’re very much inspired by their work and follow&nbsp;&nbsp;

00:25:50.040 --> 00:25:57.440
it. And I think there's a lot of parallels to our&nbsp;
domain. There is certain differences where we are&nbsp;&nbsp;

00:25:57.440 --> 00:26:03.800
a lot more concerned about correctly predicting&nbsp;
and modeling the behavior of others just because&nbsp;&nbsp;

00:26:03.800 --> 00:26:11.348
in our domain, it's so important to interact&nbsp;
correctly with all the traffic participants.
 &nbsp;

00:26:11.348 --> 00:26:12.560
CRAIG
Yeah. You know, a lot of people don't&nbsp;&nbsp;

00:26:12.560 --> 00:26:15.160
think of cars as robots, but they are.
 
Drago&nbsp;

00:26:15.160 --> 00:26:19.068
They're absolutely robots with well&nbsp;
established hardware stack, yes.
 &nbsp;

00:26:19.068 --> 00:26:24.120
CRAIG
Yeah. Is your system, could it be adopted,&nbsp;&nbsp;

00:26:24.120 --> 00:26:27.840
or adapted for other kinds of robots?
 
Drago&nbsp;

00:26:27.840 --> 00:26:35.560
I think, definitely, it can be adapted. I think&nbsp;
a lot of the concepts across the various robotic&nbsp;&nbsp;

00:26:35.560 --> 00:26:41.520
systems over time, I believe are converging. There&nbsp;
are still certain peculiarities in our domain,&nbsp;&nbsp;

00:26:41.520 --> 00:26:47.520
you can think of our robot, maybe as the first&nbsp;
manifestation, maybe the most economically-&nbsp;&nbsp;

00:26:47.520 --> 00:26:53.720
hopefully impactful, and application impactful of&nbsp;
most robotic applications. At least I believe it,&nbsp;&nbsp;

00:26:53.720 --> 00:26:58.880
and that's why I'm in this domain. But a lot&nbsp;
of these concepts, of you, know how to build&nbsp;&nbsp;

00:26:58.880 --> 00:27:03.800
their stock, how to test your stock, how to ensure&nbsp;
robustness, what kind of machine learning models&nbsp;&nbsp;

00:27:03.800 --> 00:27:09.560
you do. I think we're we're becoming more and&nbsp;
more similar across robotic, but yet, in our&nbsp;&nbsp;

00:27:09.560 --> 00:27:15.200
case safety, especially of high speed, we have&nbsp;
a lot more requirements of those kinds. You need&nbsp;&nbsp;

00:27:15.200 --> 00:27:21.760
to need to have very fast reactions, you need to&nbsp;
have very high bar to safety, the interacting, it&nbsp;&nbsp;

00:27:21.760 --> 00:27:28.920
sometimes in parts of second with other agents who&nbsp;
could behave also irrationally or adversarially,&nbsp;&nbsp;

00:27:28.920 --> 00:27:35.080
we need to handle all of this. And the other part&nbsp;
is we have very many sensors on the vehicle to see&nbsp;&nbsp;

00:27:35.080 --> 00:27:40.920
in every possible direction. And in our case,&nbsp;
we've opted for a lot of additional safety by&nbsp;&nbsp;

00:27:40.920 --> 00:27:46.040
adding LIDAR radar, which active sensors, they&nbsp;
ensure that even if machine learning in cameras&nbsp;&nbsp;

00:27:46.040 --> 00:27:50.640
misses something, we have other ways to capture&nbsp;
it, right? So when we put all this together,&nbsp;&nbsp;

00:27:50.640 --> 00:27:55.680
there are certain differences. But if you&nbsp;
build the stack for autonomous vehicles,&nbsp;&nbsp;

00:27:55.680 --> 00:29:43.040
we can build on top of the strengths of this&nbsp;
stack, we can adapt it to many other elements.

00:29:44.200 --> 00:29:44.760
CRAIG&nbsp;

00:30:05.800 --> 00:30:10.680
Yeah, and as the system evolves, when&nbsp;
you were saying that it's evolved a lot,&nbsp;&nbsp;

00:30:10.680 --> 00:30:16.320
and then there's all this new research coming&nbsp;
out with generative AI and large language models,&nbsp;&nbsp;

00:30:16.320 --> 00:30:17.560
multimodal models, how do you—
 
Drago&nbsp;

00:30:17.560 --> 00:30:23.480
It's a great time in our space. I think robotics&nbsp;
is, I think progress in robotics, I mean, is&nbsp;&nbsp;

00:30:23.480 --> 00:30:30.240
accelerating even though we're an example of&nbsp;
already a successful robotics product out there&nbsp;&nbsp;

00:30:30.240 --> 00:30:31.840
in the real world.
 
CRAIG&nbsp;

00:30:33.200 --> 00:30:37.036
How do integrate these new technologies&nbsp;
as they come along? Obviously you do have&nbsp;&nbsp;

00:30:37.036 --> 00:30:40.440
a whole research arm? That's the assessing, but&nbsp;
when do you decide to bring that into the car?
 &nbsp;

00:30:40.440 --> 00:30:43.000
Drago
So the great thing at Waymo is we have the&nbsp;&nbsp;

00:30:43.000 --> 00:30:48.840
data and the frameworks to test the performance&nbsp;
of our stack. And so the history of Waymo,&nbsp;&nbsp;

00:30:48.840 --> 00:30:56.440
as you know, we've been around 15 years, and the&nbsp;
technology in this 15 years has been dramatically&nbsp;&nbsp;

00:30:56.440 --> 00:31:02.840
evolving. And every two or three years, there's&nbsp;
major advances in our space, right. And so we've&nbsp;&nbsp;

00:31:02.840 --> 00:31:10.920
had to continually rethink and redesign our stack&nbsp;
to take advantage of the latest technologies. And&nbsp;&nbsp;

00:31:10.920 --> 00:31:16.840
so the core is, is not machine learning, is we&nbsp;
have the data and the evaluation, right. We have&nbsp;&nbsp;

00:31:16.840 --> 00:31:21.760
collected, as you know, we have many hundreds of&nbsp;
vehicles out there in the world, various sensors,&nbsp;&nbsp;

00:31:21.760 --> 00:31:26.800
we have a lot of systems that can process&nbsp;
this data, collect the interesting cases,&nbsp;&nbsp;

00:31:26.800 --> 00:31:32.200
potentially automatically annotate things we&nbsp;
want, if we put super rare things we can get,&nbsp;&nbsp;

00:31:32.200 --> 00:31:37.200
we have systems to annotate human guidance and so&nbsp;
on, right. And so we have all this data and now&nbsp;&nbsp;

00:31:37.200 --> 00:31:43.960
it's a question of, okay, well, it's not that hard&nbsp;
to try new paradigm or models and compare to the&nbsp;&nbsp;

00:31:43.960 --> 00:31:51.560
performance of your previous models. And anytime&nbsp;
we can show a significant improvement, then we&nbsp;&nbsp;

00:31:51.560 --> 00:31:58.400
evolved the stack and this was on many times&nbsp;
already so we continue doing this. But the data&nbsp;&nbsp;

00:31:58.400 --> 00:32:03.000
is the key, right? So because you have all this&nbsp;
data, and you have all these testing frameworks,&nbsp;&nbsp;

00:32:03.000 --> 00:32:08.680
and safety frameworks, and very rich evaluations,&nbsp;
the key is to be able to tell when you're better,&nbsp;&nbsp;

00:32:08.680 --> 00:32:11.640
right. And we worked hard to be able to do that.
 
CRAIG&nbsp;

00:32:11.640 --> 00:32:19.920
Yeah, there's so many questions I want to ask,&nbsp;
one is on corner cases that arise in this long&nbsp;&nbsp;

00:32:19.920 --> 00:32:30.440
tail. I read something this is for five years ago&nbsp;
I think, of a case where an autonomous vehicle, it&nbsp;&nbsp;

00:32:30.440 --> 00:32:38.920
may have been Waymo, drove up behind one of these&nbsp;
big tanker trucks with a highly polished back that&nbsp;&nbsp;

00:32:38.920 --> 00:32:46.720
was like a mirror and that confused the system.&nbsp;
How do you deal with that? Do you add that to&nbsp;&nbsp;

00:32:46.720 --> 00:32:55.980
your training data or do you tweak the algorithm?&nbsp;
I mean, how do you deal with those corner cases?
 &nbsp;

00:32:55.980 --> 00:32:58.400
Drago
So, you can annotate your data, right. And you&nbsp;&nbsp;

00:32:58.400 --> 00:33:02.800
can use it either for training or for testing. And&nbsp;
clearly, the very first thing you want to ensure&nbsp;&nbsp;

00:33:02.800 --> 00:33:07.920
when you, say very rare cases is you want a test&nbsp;
set to ensure that you got it right. And then you&nbsp;&nbsp;

00:33:07.920 --> 00:33:13.280
look for more cases like this, and you have those&nbsp;
two training sets to help you get it right. But&nbsp;&nbsp;

00:33:13.280 --> 00:33:20.280
I think in our case, because we have also radar,&nbsp;
and we have LiDAR, and we have camera, we will see&nbsp;&nbsp;

00:33:20.280 --> 00:33:26.360
the truck, we want see just car reflected in the&nbsp;
truck. So we by design have mitigated this to a&nbsp;&nbsp;

00:33:26.360 --> 00:33:31.360
large extent compared to camera on the system. For&nbsp;
example, some people had an idea, well, let's put&nbsp;&nbsp;

00:33:31.360 --> 00:33:37.640
a TV on the back of my truck and it’ll show you&nbsp;
the world in front of the truck, in the TV, right?&nbsp;&nbsp;

00:33:37.640 --> 00:33:42.080
Well imagine the autonomous driving car with&nbsp;
a camera looking at that TV thinking what's in&nbsp;&nbsp;

00:33:42.080 --> 00:33:46.600
front of me, I don't know, is there a car there?&nbsp;
Maybe it's not so relevant because it may look&nbsp;&nbsp;

00:33:46.600 --> 00:33:53.240
even far away in some cases but there can be a lot&nbsp;
of confusing things. And that's why having more&nbsp;&nbsp;

00:33:53.240 --> 00:33:59.400
sensors, so different modalities, quite conducive&nbsp;
to safety. That's not the only one but we collect&nbsp;&nbsp;

00:33:59.400 --> 00:34:04.880
these rare examples, we mine for them, we add&nbsp;
them to the training and testing data, and we&nbsp;&nbsp;

00:34:04.880 --> 00:34:09.960
validate the stack can handle them. I think it's&nbsp;
a standard engineering machine learning approach.
 &nbsp;

00:34:09.960 --> 00:34:11.760
CRAIG
Yep. Can you talk about the open&nbsp;&nbsp;

00:34:11.760 --> 00:34:19.920
data set, the Waymo open data set? And as part of&nbsp;
that, Vincent was talking about the RTX project&nbsp;&nbsp;

00:34:19.920 --> 00:34:27.960
where they collected data from all these various&nbsp;
labs where the data has been siloed for years.&nbsp;&nbsp;

00:34:27.960 --> 00:34:36.200
And then combined it and then sent it back to&nbsp;
the labs to train on the bigger dataset and there&nbsp;&nbsp;

00:34:36.200 --> 00:34:44.280
were improvements. Were you part of RTX? And how&nbsp;
does the Waymo open data set fit in your stack,&nbsp;&nbsp;

00:34:44.280 --> 00:34:46.280
and then in your training?
 
Drago&nbsp;

00:34:46.280 --> 00:34:51.240
So let me explain. I mean, generally we’re&nbsp;
separate markets, I think that dataset is&nbsp;&nbsp;

00:34:51.240 --> 00:34:56.680
primarily focused at objects or robots, that&nbsp;
manipulation is on a table, right. And ours is&nbsp;&nbsp;

00:34:56.680 --> 00:35:02.400
focused on understanding the outdoors, environment&nbsp;
and traffic scenes. So it has somewhat different&nbsp;&nbsp;

00:35:02.400 --> 00:35:07.680
parameters. The reason why we made the Waymo open&nbsp;
data set is based on the realizations we had in&nbsp;&nbsp;

00:35:07.680 --> 00:35:13.240
the early years, especially around 2018, when we&nbsp;
really started the project, but even before. We&nbsp;&nbsp;

00:35:13.240 --> 00:35:18.800
did research in the space, and the data sets that&nbsp;
were available, even though Seminole did great,&nbsp;&nbsp;

00:35:18.800 --> 00:35:26.760
there was a dataset called KITTI at the time that&nbsp;
was made in 2011, 12 by Karlsruhe and I think&nbsp;&nbsp;

00:35:26.760 --> 00:35:33.720
European collaborators. So, we started seeing that&nbsp;
dataset does not satisfy our needs when we build&nbsp;&nbsp;

00:35:33.720 --> 00:35:39.000
the actual stacks. And the problem there was it&nbsp;
was typically very, very small. And when the data&nbsp;&nbsp;

00:35:39.000 --> 00:35:45.120
set is small for a machine learning person, it&nbsp;
distorts the kind of models that win. So then that&nbsp;&nbsp;

00:35:45.120 --> 00:35:52.040
dataset, to do well, it encourages you to build&nbsp;
models that either have a lot of built in bias,&nbsp;&nbsp;

00:35:52.040 --> 00:35:59.280
or you built in a lot of overfitting techniques,&nbsp;
which is not at all what you're gonna do when you&nbsp;&nbsp;

00:35:59.280 --> 00:36:05.960
build the real stack. And so we understood the&nbsp;
set of problems they covered were a bit limited,&nbsp;&nbsp;

00:36:05.960 --> 00:36:10.840
but what was even more so is the data. And&nbsp;
so the academic community, such talented,&nbsp;&nbsp;

00:36:10.840 --> 00:36:20.040
great community— And it's not easy to get your&nbsp;
hands on large amounts of reasonably clean,&nbsp;&nbsp;

00:36:20.040 --> 00:36:24.520
autonomous vehicle data. I mean, you need to have&nbsp;
a vehicle you need to place all the sensors you&nbsp;&nbsp;

00:36:24.520 --> 00:36:30.680
need to instrument them, you need to align the&nbsp;
times and calibrations in the poses is just very&nbsp;&nbsp;

00:36:30.680 --> 00:36:35.600
prohibitive, right. And so we wanted to enable&nbsp;
the community and the best way to do this and to&nbsp;&nbsp;

00:36:35.600 --> 00:36:41.360
unlock the talent is to give them the right kind&nbsp;
of data. And through this Waymo open dataset,&nbsp;&nbsp;

00:36:41.360 --> 00:36:47.280
this was our attempt to push what is possible&nbsp;
in the standards, the amount of data and the&nbsp;&nbsp;

00:36:47.280 --> 00:36:53.800
quality of data that this community would get. And&nbsp;
I would like to think that we did a good part into&nbsp;&nbsp;

00:36:53.800 --> 00:37:00.440
popularizing and enabling a lot of very strong&nbsp;
AI work, and in addition to releasing data and&nbsp;&nbsp;

00:37:00.440 --> 00:37:05.400
enriching it with more and more labels over five&nbsp;
years. So we constantly improve the dataset based&nbsp;&nbsp;

00:37:05.400 --> 00:37:11.080
on feedback and our understanding about the space,&nbsp;
and we expand the things you can do with it. They&nbsp;&nbsp;

00:37:11.080 --> 00:37:17.760
expand the set of tasks you could potentially&nbsp;
try to show state of the art in. And we also,&nbsp;&nbsp;

00:37:18.360 --> 00:37:25.360
as we introduce new tasks and labels, we create&nbsp;
challenges. And the challenge allows our different&nbsp;&nbsp;

00:37:25.360 --> 00:37:32.560
researchers across the world to compare their&nbsp;
solutions on criteria that are informed by&nbsp;&nbsp;

00:37:32.560 --> 00:37:37.920
experience in the space. For example, what's good&nbsp;
3D semantic segmentation? What is a good metric&nbsp;&nbsp;

00:37:37.920 --> 00:37:43.800
for simulated agents, right? We take the learnings&nbsp;
we have from our space, and help them essentially&nbsp;&nbsp;

00:37:43.800 --> 00:37:50.440
create leaderboards where people can compare&nbsp;
their contributions on definitions of problems&nbsp;&nbsp;

00:37:50.440 --> 00:37:57.640
informed by us. So this has been an ongoing five&nbsp;
year effort and we're launching actually, next&nbsp;&nbsp;

00:37:57.640 --> 00:38:04.560
week, today's March 12, when we're taping this.&nbsp;
We expect next week to launch or the week after,&nbsp;&nbsp;

00:38:04.560 --> 00:38:11.520
our 2024 versions of challenges. There&nbsp;
are four challenges on exciting problems,&nbsp;&nbsp;

00:38:11.520 --> 00:38:22.120
3D semantic segmentation, 3D occupancy prediction,&nbsp;
and 3D flow prediction challenge. Then we have&nbsp;&nbsp;

00:38:22.120 --> 00:38:27.000
motion prediction in terms of trajectory&nbsp;
challenge and embodying simulated agents.&nbsp;&nbsp;

00:38:27.000 --> 00:38:33.280
So these are the challenges this year, and&nbsp;
the winners will get prizes and the best&nbsp;&nbsp;

00:38:33.280 --> 00:38:40.760
work will get invitations to present it CVPR&nbsp;
Premier Computer Vision Conference in June,&nbsp;&nbsp;

00:38:40.760 --> 00:38:47.320
at our workshop that we're organizing with other&nbsp;
academic collaborators. So I encourage people to&nbsp;&nbsp;

00:38:47.320 --> 00:38:51.600
check it out. Especially if you're interested&nbsp;
in our domain or doing research in our domain.&nbsp;&nbsp;

00:38:51.600 --> 00:38:56.520
I think there's some very exciting challenges&nbsp;
this year. And I believe this is our fourth or&nbsp;&nbsp;

00:38:56.520 --> 00:39:04.960
fifth year of the challenges. So we do our best to&nbsp;
sustain improvements to the dataset and the amount&nbsp;&nbsp;

00:39:04.960 --> 00:39:08.280
of tasks we release over time.
 
CRAIG&nbsp;

00:39:08.280 --> 00:39:14.600
Yeah, on the dataset, how is it collected?
 
Drago&nbsp;

00:39:15.920 --> 00:39:23.280
It was collected by Waymo vehicles, the cities&nbsp;
of San Francisco and Phoenix. The majority of&nbsp;&nbsp;

00:39:23.280 --> 00:39:30.560
the data was collected around 2018, or 2017,&nbsp;
which is when we chose a lot of the run segments&nbsp;&nbsp;

00:39:30.560 --> 00:39:40.760
or the snippets of real data that we decided to&nbsp;
release. And we released 2000 or so run segments.&nbsp;&nbsp;

00:39:40.760 --> 00:39:49.080
They're sequences of LiDAR and camera data for 20&nbsp;
seconds. And we've released since then 100,000 run&nbsp;&nbsp;

00:39:49.080 --> 00:39:54.600
segments, both with intermediate representation&nbsp;
in terms of road graph and bounding boxes&nbsp;&nbsp;

00:39:54.600 --> 00:39:59.680
moving around for the various objects. But we also&nbsp;
released LiDAR for those segments, in this series,&nbsp;&nbsp;

00:39:59.680 --> 00:40:06.040
even image embeddings. So 100,000 segments is very&nbsp;
low, very large number. It's quite prohibitive to&nbsp;&nbsp;

00:40:06.040 --> 00:40:13.400
release all the sensor data, camera data for five&nbsp;
plus cameras on this but we released compressed&nbsp;&nbsp;

00:40:13.400 --> 00:40:20.440
versions of the camera representations to enable&nbsp;
the Atmore research, and to combine in camera&nbsp;&nbsp;

00:40:20.440 --> 00:40:26.960
Lidar and intermediate representation, to see&nbsp;
how good prediction can be made. The key for&nbsp;&nbsp;

00:40:26.960 --> 00:40:34.440
us is that this imitative prediction of agents is&nbsp;
one of the key crutches in our domain for machine&nbsp;&nbsp;

00:40:34.440 --> 00:40:40.280
learning, because we observe how everyone drives,&nbsp;
we observe how everyone walks. But in general,&nbsp;&nbsp;

00:40:40.280 --> 00:40:46.040
you can learn a lot about driving by watching&nbsp;
drivers and that's a key. That's one of those&nbsp;&nbsp;

00:40:46.040 --> 00:40:49.640
field domains where, you know, maybe for&nbsp;
other robots, you never see how a certain&nbsp;&nbsp;

00:40:49.640 --> 00:40:54.920
dog robot should walk much in the real world.&nbsp;
But for driving, you do. And so it's really,&nbsp;&nbsp;

00:40:54.920 --> 00:40:58.880
really powerful data and we have been pushing&nbsp;
a lot of researchers. You can see some of our&nbsp;&nbsp;

00:40:58.880 --> 00:41:06.400
challenges this year of predicting various things&nbsp;
to take advantage of this key part of the data.
 &nbsp;

00:41:06.400 --> 00:41:08.840
CRAIG
Are there other contributors to&nbsp;&nbsp;

00:41:08.840 --> 00:41:17.000
the dataset, other self driving car companies, or&nbsp;
other labs that that have data that's relevant?
 &nbsp;

00:41:17.000 --> 00:41:19.680
Drago
I think people contribute other&nbsp;&nbsp;

00:41:19.680 --> 00:41:27.240
labels to our dataset and do potentially their&nbsp;
own leaderboards or competitions on top of that.&nbsp;&nbsp;

00:41:27.840 --> 00:41:32.440
There are other companies that have released&nbsp;
data sets as well. We're not the only one since&nbsp;&nbsp;

00:41:32.440 --> 00:41:38.400
we started, right. So this also shows that&nbsp;
the companies themselves understand that this&nbsp;&nbsp;

00:41:38.400 --> 00:41:45.320
is an important thing to do. And as the field&nbsp;
evolves, your understanding of the problems we&nbsp;&nbsp;

00:41:45.320 --> 00:41:52.840
want to make as challenges and the data needed to&nbsp;
support them evolves. And so you I think we will&nbsp;&nbsp;

00:41:52.840 --> 00:41:59.400
be as a community, and I hope it Waymo we can&nbsp;
sustain what we're doing. But as a community,&nbsp;&nbsp;

00:41:59.400 --> 00:42:04.600
I would expect that there'll be a lot more&nbsp;
interesting releases and challenges possible. 
 &nbsp;

00:42:04.600 --> 00:42:07.040
CRAIG
Yeah, the data as it’s coming&nbsp;&nbsp;

00:42:07.040 --> 00:42:14.840
in from your cars, are the cars sharing data with&nbsp;
each other in real time? Or is the data going back&nbsp;&nbsp;

00:42:14.840 --> 00:42:16.800
to a central repository?
 
Drago&nbsp;

00:42:16.800 --> 00:42:21.600
So it’s collecting in a central repository. When&nbsp;
our cars drive, they typically don't send each&nbsp;&nbsp;

00:42:21.600 --> 00:42:26.760
other's data to each other that presumes that&nbsp;
you will have very high bandwidth connections,&nbsp;&nbsp;

00:42:26.760 --> 00:42:32.480
it's actually a lot of data, it's hundreds of&nbsp;
millions of pixels in LiDAR scans a second. So&nbsp;&nbsp;

00:42:32.480 --> 00:42:36.240
that's why the storage even for the things&nbsp;
we're releasing is so big, it's almost a&nbsp;&nbsp;

00:42:36.240 --> 00:42:42.800
terabyte, right? If not more. And so even when we&nbsp;
compress it, right? So these are the challenges.&nbsp;&nbsp;

00:42:42.800 --> 00:42:48.600
But when they're in the data center, sometimes we&nbsp;
see the scene from several vehicles at the same&nbsp;&nbsp;

00:42:48.600 --> 00:42:55.000
time, right, which can of course, help us at that&nbsp;
time reconstruct much higher fidelity models of&nbsp;&nbsp;

00:42:55.000 --> 00:43:00.320
well, what is seen by now, a collection of Waymo&nbsp;
vehicles, not a single vehicle, this is possible,&nbsp;&nbsp;

00:43:00.320 --> 00:43:03.840
but it's not common practice in the field.
 
CRAIG&nbsp;

00:43:05.680 --> 00:43:10.800
Part of the trajectory is to make&nbsp;
the models generalize more and more.
 &nbsp;

00:43:10.800 --> 00:43:12.560
Drago
That's one of the main tasks&nbsp;&nbsp;

00:43:12.560 --> 00:43:18.000
of the machine learning problem, you make the most&nbsp;
of the data, you have both to build the agent and&nbsp;&nbsp;

00:43:18.000 --> 00:43:22.720
to build this environment to test the agent.&nbsp;
These are the two main problems as I see them,&nbsp;&nbsp;

00:43:22.720 --> 00:43:28.120
right. Now, of course, you can break them down&nbsp;
into subproblems but that is at the core of it&nbsp;&nbsp;

00:43:28.120 --> 00:43:33.640
for the data you have that you're collecting,&nbsp;
and maybe keeping how much of this can I learn&nbsp;&nbsp;

00:43:33.640 --> 00:43:39.000
with my models. The more I can learn, right, the&nbsp;
more we tilt the system towards machine learning&nbsp;&nbsp;

00:43:39.000 --> 00:43:43.520
solutions because those just scale. Then I go&nbsp;
to any new environment, I collect more data,&nbsp;&nbsp;

00:43:43.520 --> 00:43:45.480
it will just learn from it, right.
 
CRAIG&nbsp;

00:43:45.480 --> 00:43:51.120
In different parts of the world, I remember in&nbsp;
the early days there was a lot of talk about,&nbsp;&nbsp;

00:43:51.120 --> 00:43:59.440
you train a system in Scandinavia and then try&nbsp;
and operate it in, I don't know, the Philippines.
 &nbsp;

00:43:59.440 --> 00:44:03.560
Drago
Yeah, I go to Singapore and everyone drives&nbsp;&nbsp;

00:44:03.560 --> 00:44:06.760
on the left side, right, for example?
 
CRAIG&nbsp;

00:44:06.760 --> 00:44:14.040
Yea, do you need different datasets to train&nbsp;
models for different environments, different&nbsp;&nbsp;

00:44:14.040 --> 00:44:19.860
regions? Or do the models generalize enough&nbsp;
that they can quickly adapt to a new region?
 &nbsp;

00:44:19.860 --> 00:44:22.960
Drago
So we've seen that with large models, right,&nbsp;&nbsp;

00:44:22.960 --> 00:44:28.360
they actually benefit combining the data and this&nbsp;
parallel with development in the generative AI&nbsp;&nbsp;

00:44:28.360 --> 00:44:33.720
field, right. They try to train them on some&nbsp;
large internet scale datasets. Or when you&nbsp;&nbsp;

00:44:33.720 --> 00:44:38.440
train a translation model, often you're training&nbsp;
it on multiple languages at the same time. So when&nbsp;&nbsp;

00:44:38.440 --> 00:44:43.480
size is not the constraint, there is positive&nbsp;
feedback across different tasks and different&nbsp;&nbsp;

00:44:43.480 --> 00:44:50.040
regions. And we've seen some of the seven in the&nbsp;
case of, say, car data helps trucks, right. So you&nbsp;&nbsp;

00:44:50.040 --> 00:44:55.440
need now a lot less drag data to do well, because&nbsp;
you can just mix in large proportions of the car&nbsp;&nbsp;

00:44:55.440 --> 00:45:01.760
data that you have. And lo and behold the results&nbsp;
improve, right? And the same is true across&nbsp;&nbsp;

00:45:01.760 --> 00:45:07.800
cities, and environments, and conditions for&nbsp;
larger models, we essentially would look to pull&nbsp;&nbsp;

00:45:07.800 --> 00:45:14.800
all the data and train much larger model and then&nbsp;
essentially, potentially distill this model, or&nbsp;&nbsp;

00:45:14.800 --> 00:45:20.560
adjust it to fine tune it to the specific cases as&nbsp;
needed. But that seems to be the recipe today. So&nbsp;&nbsp;

00:45:20.560 --> 00:45:27.920
we aim to have one Waymo driver and as few of them&nbsp;
as possible. Traditionally, so far, all the cities&nbsp;&nbsp;

00:45:27.920 --> 00:45:35.560
we drive is a single set of models. It's not yet&nbsp;
fragmented, said that's really hard to maintain,&nbsp;&nbsp;

00:45:35.560 --> 00:45:38.274
and it's not scalable. That's not what you want. 
 
CRAIG&nbsp;

00:45:38.274 --> 00:45:43.720
Yeah, I don't know if you can talk about&nbsp;
the Cruise debacle. We don't have to talk&nbsp;&nbsp;

00:45:43.720 --> 00:45:51.580
about the company, but what went wrong there?&nbsp;
And how is Waymo avoiding those problems? 
 &nbsp;

00:45:51.580 --> 00:45:54.280
Drago
I mean, I would say that, to me,&nbsp;&nbsp;

00:45:54.280 --> 00:46:03.160
the lesson is safety comes first. It's paramount,&nbsp;
right, and I think trust is key. And so it's very&nbsp;&nbsp;

00:46:03.160 --> 00:46:09.200
easy to lose people's trust. And ultimately,&nbsp;
I would like to think that so far Waymo has&nbsp;&nbsp;

00:46:09.200 --> 00:46:16.360
a track record of is expanding thoughtfully&nbsp;
and successfully in the domains where we have&nbsp;&nbsp;

00:46:16.360 --> 00:46:24.160
operated. And so we want to expand relative to&nbsp;
what we believe is beneficial and safe. And I&nbsp;&nbsp;

00:46:24.160 --> 00:46:31.360
think we have a lot of data to show that our&nbsp;
deployments, I think, compared to, especially&nbsp;&nbsp;

00:46:31.360 --> 00:46:42.360
to humans, in various studies is shown that crash&nbsp;
rate is a lot less and our property damage claims,&nbsp;&nbsp;

00:46:42.360 --> 00:46:50.360
there is actually a collaboration with a Swiss&nbsp;
insurance company showing injury claims went to&nbsp;&nbsp;

00:46:50.360 --> 00:46:58.520
zero with our vehicles. That’s based on 3.8 miles&nbsp;
that they analyzed and property damage claims went&nbsp;&nbsp;

00:46:58.520 --> 00:47:06.640
down 76%, right. And this is track record that we&nbsp;
want to have and continue expanding on. So we look&nbsp;&nbsp;

00:47:06.640 --> 00:47:12.720
to expand thoughtfully. And I think the way to do&nbsp;
it thoughtfully is to have extremely robust safety&nbsp;&nbsp;

00:47:12.720 --> 00:47:22.600
methodologies and analyze what you put out there&nbsp;
with incredible rigor. And honestly, our safety&nbsp;&nbsp;

00:47:22.600 --> 00:47:28.400
methodology is very multifaceted. So there is&nbsp;
no one golden hammer to ensure the safety of the&nbsp;&nbsp;

00:47:28.400 --> 00:47:35.120
system and there is no one golden machine learning&nbsp;
model simulator to prove that your system is safe.&nbsp;&nbsp;

00:47:35.120 --> 00:47:42.960
Our learning is, you need to approach what we have&nbsp;
with a variety of methodologies and make sure that&nbsp;&nbsp;

00:47:42.960 --> 00:47:49.120
we looked at it from many angles. And so this is&nbsp;
actually one of the most main recipes that make&nbsp;&nbsp;

00:47:49.120 --> 00:47:54.960
Waymo Waymo, is that we have been working on this&nbsp;
for 15 years. And there is a level of maturity&nbsp;&nbsp;

00:47:54.960 --> 00:47:59.960
that honestly when I joined Waymo at the time,&nbsp;
that was one of the things that most impressed me,&nbsp;&nbsp;

00:47:59.960 --> 00:48:01.640
is that type of work.
 
CRAIG&nbsp;

00:48:01.640 --> 00:48:12.080
Yeah, so it wasn't that the data set was&nbsp;
inferior, or the models were inferior,&nbsp;&nbsp;

00:48:12.080 --> 00:48:21.340
or there weren't enough guardrails, or hard&nbsp;
coded rules; it's sort of a combination?
 &nbsp;

00:48:21.340 --> 00:48:23.440
Drago
I mean, you want to build a&nbsp;&nbsp;

00:48:23.440 --> 00:48:29.520
system that you convinced yourself is safe to put&nbsp;
out there, right. That's a high bar. That's not&nbsp;&nbsp;

00:48:29.520 --> 00:48:35.600
a simple question to solve, right. And I think&nbsp;
at Waymo we tested extremely extensively in a&nbsp;&nbsp;

00:48:35.600 --> 00:48:45.200
variety of different ways. And that determines how&nbsp;
quickly we expand, is our success in our tests,&nbsp;&nbsp;

00:48:45.200 --> 00:48:52.280
right. We do this also thoughtfully. We need to&nbsp;
engage and work with the community. So, Phoenix,&nbsp;&nbsp;

00:48:52.280 --> 00:49:01.440
we have been delivering rides close to six years&nbsp;
now, right. And over time, now we're at a scale&nbsp;&nbsp;

00:49:01.440 --> 00:49:09.960
which is quite noticeable and thousands of people&nbsp;
or maybe tens of thousands take us every week;&nbsp;&nbsp;

00:49:09.960 --> 00:49:17.400
but it's not like we, were not reckless,&nbsp;
right. It's hard for me to talk about Cruise,&nbsp;&nbsp;

00:49:17.400 --> 00:49:25.960
per se. This is more of a lesson for us. I think&nbsp;
that ultimately the details of the incident that&nbsp;&nbsp;

00:49:25.960 --> 00:49:31.520
had them grounded, I think a lot of the details&nbsp;
are out there and people can judge for themselves.&nbsp;&nbsp;

00:49:31.520 --> 00:49:39.400
But we need to ultimately build trust and a lot of&nbsp;
this trust is based on the safety record you have.

00:49:39.400 --> 00:49:42.400
CRAIG
Yeah, from what I&nbsp;&nbsp;

00:49:42.400 --> 00:49:51.320
understand China has more robotaxis deployed than&nbsp;
the US. Do you think that they're, presumably you&nbsp;&nbsp;

00:49:51.320 --> 00:49:57.920
pay a lot of attention to what they've got on the&nbsp;
road, do you think that they're ahead of you guys?&nbsp;&nbsp;

00:49:57.920 --> 00:50:06.320
Or is everybody kind of on the leading— self&nbsp;
driving car systems on par? And then the other&nbsp;&nbsp;

00:50:06.320 --> 00:50:14.240
question I want to ask is— as I see we're gonna&nbsp;
run out of time, there's a lot of confusion about&nbsp;&nbsp;

00:50:14.240 --> 00:50:26.200
Tesla's system. Does it approach what Waymo is&nbsp;
doing? Or is it something different altogether?
 &nbsp;

00:50:26.200 --> 00:50:27.840
Drago
So maybe let me first&nbsp;&nbsp;

00:50:27.840 --> 00:50:34.320
address the Chinese companies question. I mean,&nbsp;
clearly, a few companies there have started fully&nbsp;&nbsp;

00:50:34.320 --> 00:50:41.120
autonomous deployments. Unfortunately, I don't&nbsp;
have full visibility into their system, I have not&nbsp;&nbsp;

00:50:41.120 --> 00:50:48.200
had the chance to ride in them or study them. I&nbsp;
mean, a lot of that design is proprietary. I think&nbsp;&nbsp;

00:50:48.200 --> 00:50:55.640
ultimately, one needs to experience the service&nbsp;
and the people who are best suited to applying&nbsp;&nbsp;

00:50:55.640 --> 00:51:03.920
how we all compare technology wise, is people who&nbsp;
experience all these different services, right.&nbsp;&nbsp;

00:51:05.280 --> 00:51:15.560
Clearly in China that’s an area of investment and&nbsp;
maybe even government assisted domain, right. That&nbsp;&nbsp;

00:51:15.560 --> 00:51:21.880
said, they're not here currently in meaningful&nbsp;
deployments, so we're not head-to-head with&nbsp;&nbsp;

00:51:21.880 --> 00:51:27.000
them. And I don't follow them that closely even&nbsp;
though I understand that, you know, that is an&nbsp;&nbsp;

00:51:27.000 --> 00:51:34.640
area of rapid development. In terms of companies&nbsp;
like Tesla that deployed more Driver Assist type&nbsp;&nbsp;

00:51:34.640 --> 00:51:44.760
technologies, I think, you know, I appreciate the&nbsp;
machine learning work and innovations they do;&nbsp;&nbsp;

00:51:44.760 --> 00:51:52.480
I think to me in their nature, the system is&nbsp;
not full self-driving. So they do good machine&nbsp;&nbsp;

00:51:52.480 --> 00:51:57.160
learning but as with most machine learning,&nbsp;
I’m a machine learning person and as much as&nbsp;&nbsp;

00:51:57.160 --> 00:52:06.040
I appreciate it, right, there comes a time when&nbsp;
you get to situations that are extremely rare,&nbsp;&nbsp;

00:52:06.040 --> 00:52:13.280
and machine learning cannot handle. And to have&nbsp;
real deployment out there, you need to carefully&nbsp;&nbsp;

00:52:13.280 --> 00:52:20.040
think through your stack to make sure that even if&nbsp;
machine learning does not solve everything 100%,&nbsp;&nbsp;

00:52:20.040 --> 00:52:26.000
your full self-drive product does. And that's a&nbsp;
very big gap. I think that's not easy to do at&nbsp;&nbsp;

00:52:26.000 --> 00:52:33.200
all. And it leads potentially to rethinking core&nbsp;
parts of your design, if you are to go there. And&nbsp;&nbsp;

00:52:33.200 --> 00:52:39.040
so I mean, all of our machine learning models&nbsp;
are improving. To release a driverless service,&nbsp;&nbsp;

00:52:39.040 --> 00:52:45.120
you need to also answer the question of how to&nbsp;
make sure the whole thing is fully robust. And&nbsp;&nbsp;

00:52:45.120 --> 00:52:51.040
that adds a level of design and complexity that&nbsp;
a lot of those companies have not tackled yet.
 &nbsp;

00:52:51.040 --> 00:52:53.280
CRAIG
Yeah, would your system ever be&nbsp;&nbsp;

00:52:53.280 --> 00:52:57.440
available for consumers to buy?
 
Drago&nbsp;

00:52:57.440 --> 00:53:06.160
I mean, in the beginning Waymo’s charter,&nbsp;
Waymo’s driver, we eventually want to enable many&nbsp;&nbsp;

00:53:06.160 --> 00:53:13.400
different kinds of use cases and form factors. I&nbsp;
think our first, kind of, flagship application is&nbsp;&nbsp;

00:53:13.400 --> 00:53:20.040
ride-hailing. But we're building a general stack.&nbsp;
I mean, we discussed that maybe we can transfer a&nbsp;&nbsp;

00:53:20.040 --> 00:53:25.640
lot of the models or types of models that were&nbsp;
built for autonomous vehicles to other mobility&nbsp;&nbsp;

00:53:25.640 --> 00:53:36.080
robots. But even before that, we aspire to build&nbsp;
a stack that learns from that, and generalizes&nbsp;&nbsp;

00:53:36.080 --> 00:53:43.880
across these platforms, and enables various&nbsp;
autonomous driving applications, preferably&nbsp;&nbsp;

00:53:43.880 --> 00:53:52.680
with as similar of a system as possible, or&nbsp;
maybe the same system. That is possible. So I&nbsp;&nbsp;

00:53:52.680 --> 00:53:59.160
think down the line, we have the opportunity&nbsp;
to build solutions for personal vehicles,&nbsp;&nbsp;

00:53:59.160 --> 00:54:05.960
for trucks, potentially other form factors. But&nbsp;
I think ultimately, for a company to succeed,&nbsp;&nbsp;

00:54:05.960 --> 00:54:12.160
you need to nail your first product. I think&nbsp;
where we are is, I mean, I personally really&nbsp;&nbsp;

00:54:12.160 --> 00:54:17.720
enjoy riding in our vehicles. I think it's a&nbsp;
delightful experience. Of course, I'm biased,&nbsp;&nbsp;

00:54:17.720 --> 00:54:24.280
but they’re available. Everyone can try them.&nbsp;
You can come to San Francisco, and Phoenix,&nbsp;&nbsp;

00:54:24.280 --> 00:54:32.360
and soon LA and Austin, and just ride them,&nbsp;
judge for yourself. But it exists, it's real,&nbsp;&nbsp;

00:54:32.360 --> 00:54:38.040
it's out there, and we've proven that the product&nbsp;
exists. Now it's on us to make sure we can have a&nbsp;&nbsp;

00:54:38.040 --> 00:54:41.000
great business doing it and expand from there.
 
CRAIG 52&nbsp;

00:54:41.000 --> 00:54:49.520
Yeah, and how long do you think before every city&nbsp;
has a robotaxi service? And then how long beyond&nbsp;&nbsp;

00:54:49.520 --> 00:54:57.960
that before they're autonomous vehicles on the&nbsp;
highways and all over the United States? I mean,&nbsp;&nbsp;

00:54:57.960 --> 00:55:06.120
I understand it's largely a regulatory issue,&nbsp;
but in terms of the reliability and the safety.
 &nbsp;

00:55:06.120 --> 00:55:13.120
Drago
We are expanding every year, rapidly. You can&nbsp;&nbsp;

00:55:13.120 --> 00:55:20.800
think about it for now in a multiplicative phase.&nbsp;
So every year we go many times the driverless&nbsp;&nbsp;

00:55:20.800 --> 00:55:26.320
deployments we've had before, right. And even&nbsp;
though that's thoughtful expansion, I believe.&nbsp;&nbsp;

00:55:26.960 --> 00:55:32.280
So it will take a few years to continue this&nbsp;
trend until people really feel it all over the&nbsp;&nbsp;

00:55:32.280 --> 00:55:38.440
place. And to me, mostly, I don't count how many&nbsp;
cities we will be in. I want to build the stack&nbsp;&nbsp;

00:55:38.440 --> 00:55:47.760
that makes it economically great and scalable to&nbsp;
do a city, n+1. And so there is a ton of headroom&nbsp;&nbsp;

00:55:47.760 --> 00:55:53.480
still possible with the technologies that are&nbsp;
being developed. And so I hope we can facilitate&nbsp;&nbsp;

00:55:53.480 --> 00:56:03.240
it. I believe it's not far out. My personally&nbsp;
great hope is that this multiplicative expansion&nbsp;&nbsp;

00:56:03.240 --> 00:56:10.520
of operational design domain, in its scope&nbsp;
and volume, if we sustain it for a few years,&nbsp;&nbsp;

00:56:10.520 --> 00:56:12.937
people will see these cars in many more places.
 
CRAIG&nbsp;

00:56:12.937 --> 00:56:15.760
But you think, I mean, you're quite a bit&nbsp;
younger than me. But do you think in 10 years,&nbsp;&nbsp;

00:56:15.760 --> 00:56:19.120
when I'm still— got my arms, I hope?
 
Drago&nbsp;

00:56:19.120 --> 00:56:23.880
I mean, sure, I'm an optimist generally. And as&nbsp;
an optimist, I would say it should be less than&nbsp;&nbsp;

00:56:23.880 --> 00:56:26.120
10 years and probably by some margin.
 
CRAIG&nbsp;

00:56:26.120 --> 00:56:34.400
Yeah, that's exciting. And where in the&nbsp;
development of the, either the data or the tech&nbsp;&nbsp;

00:56:34.400 --> 00:56:37.320
stack, what are you most excited about?
 
Drago&nbsp;

00:56:37.320 --> 00:56:44.680
I'm excited about, I mean, all this great&nbsp;
technology we have that allows us to generalize&nbsp;&nbsp;

00:56:44.680 --> 00:56:51.040
a lot better from the data we have. And I think&nbsp;
we understand better and better how to land these,&nbsp;&nbsp;

00:56:51.040 --> 00:56:58.560
kind of, scaling multipliers. So the more I can&nbsp;
learn from the data, and that they have, right,&nbsp;&nbsp;

00:56:58.560 --> 00:57:04.520
I'm excited about keeping up with improving our&nbsp;
scaling multipliers on what we can learn from&nbsp;&nbsp;

00:57:04.520 --> 00:57:11.800
the data, the more we do this, the faster&nbsp;
we will expect, right. Because ultimately,&nbsp;&nbsp;

00:57:11.800 --> 00:57:18.040
machine learning, a lot of the other things, So&nbsp;
when you get to the hybrid design of the system,&nbsp;&nbsp;

00:57:18.040 --> 00:57:25.160
then when you need humans to essentially add&nbsp;
expert expertise or guardrails. That doesn't&nbsp;&nbsp;

00:57:25.160 --> 00:57:32.400
scale very well. But machine learning is our&nbsp;
great scaling tool. And we can use it, the&nbsp;&nbsp;

00:57:32.400 --> 00:57:37.960
faster we will, we will grow across the areas that&nbsp;
we serve. The other part that really encourages me&nbsp;&nbsp;

00:57:37.960 --> 00:57:45.640
actually is, maybe the thing that makes it easier&nbsp;
to do this, which is; so what we've discovered is,&nbsp;&nbsp;

00:57:45.640 --> 00:57:52.480
after you cover some of these areas, for example,&nbsp;
Phoenix is more, parts of it are 45 miles an hour&nbsp;&nbsp;

00:57:52.480 --> 00:57:57.240
road with certain challenges. San Francisco is&nbsp;
dense-urban with certain challenges. Maybe we&nbsp;&nbsp;

00:57:57.240 --> 00:58:06.680
cover the highway; now there’s a lot of work on&nbsp;
harnessing the highway and starting to enable&nbsp;&nbsp;

00:58:06.680 --> 00:58:11.440
that for customers. I think what we saw is&nbsp;
that after you have, say, Phoenix and San&nbsp;&nbsp;

00:58:11.440 --> 00:58:18.120
Francisco and you go to LA or Austin, it takes&nbsp;
a lot less work and effort. And a lot of it is&nbsp;&nbsp;

00:58:18.120 --> 00:58:24.640
actually in validating our system, that it does&nbsp;
well as opposed to actually making it do well. So,&nbsp;&nbsp;

00:58:24.640 --> 00:58:31.600
the learnings generalize, and that's a great&nbsp;
kind of wind at our backs. And so as long as&nbsp;&nbsp;

00:58:31.600 --> 00:58:37.120
we design the system well, and we don't, I&nbsp;
mean, again, partition every city for itself,&nbsp;&nbsp;

00:58:37.120 --> 00:58:42.320
now we actually benefit from San Francisco to&nbsp;
do LA, and we benefit from Phoenix to do LA,&nbsp;&nbsp;

00:58:42.320 --> 00:58:48.360
and then they have more cities that the city&nbsp;
plus one will benefit from that. And so, I see&nbsp;&nbsp;

00:58:48.360 --> 00:58:56.920
it also as, that's a great property, right. Scale&nbsp;
begets scale in sense. And then, you know, having&nbsp;&nbsp;

00:58:56.920 --> 00:59:01.480
these deployments, having all these experiences,&nbsp;
and the data from them will help us with next.
&nbsp;

00:59:01.480 --> 00:59:03.600
CRAIG
So the ambition is&nbsp;&nbsp;

00:59:03.600 --> 00:59:13.720
that there'll be fleets of robotaxis in every city&nbsp;
and town. And will it be, from what you can see,&nbsp;&nbsp;

00:59:13.720 --> 00:59:20.720
I know that you're not on the business end of&nbsp;
this, but will it be inexpensive enough that&nbsp;&nbsp;

00:59:20.720 --> 00:59:27.600
people will not need to own a car? You just, on&nbsp;
your phone, you summon a robotaxi, it arrives&nbsp;&nbsp;

00:59:27.600 --> 00:59:33.260
in five minutes, you go where you're going? I&nbsp;
mean, in much of the way that Uber operates?
 &nbsp;

00:59:33.260 --> 00:59:35.840
Drago
So I will tell you now, right, we're roughly&nbsp;&nbsp;

00:59:35.840 --> 00:59:42.280
the same price, but we’re much better experience,&nbsp;
personally. Again, I'm a Waymo representative so&nbsp;&nbsp;

00:59:42.280 --> 00:59:48.440
that's my personal bias, but you're in the vehicle&nbsp;
by yourself, you can play the music you like,&nbsp;&nbsp;

00:59:48.440 --> 00:59:54.160
there is a certain level of, I mean, it's a hand&nbsp;
vehicle. It's a premium experience today for the&nbsp;&nbsp;

00:59:54.160 --> 01:00:01.040
price of a normal experience, right. I think over&nbsp;
time there is a tremendous opportunity to optimize&nbsp;&nbsp;

01:00:01.040 --> 01:00:09.880
both our models and vehicle cost, and operations&nbsp;
to make it yet more affordable than it is. Now,&nbsp;&nbsp;

01:00:09.880 --> 01:00:16.160
there is a lot more work that needs to be done and&nbsp;
I think it will shift from, for now we don't even&nbsp;&nbsp;

01:00:16.160 --> 01:00:24.200
have as many vehicles as is the demand, over time-&nbsp;
hopefully. As we make it more affordable and yet&nbsp;&nbsp;

01:00:24.200 --> 01:00:33.680
better experience, then it's a beneficial look. If&nbsp;
we can succeed with this, we'll expand the market.

