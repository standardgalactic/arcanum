.
.
am1B am2B · · · amnB


.
150 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES
It can be shown that ⊗ is associative and that
(A ⊗ B)(C ⊗ D) = AC ⊗ BD
(A ⊗ B)
> = A
> ⊗ B
> ,
whenever AC and BD are well defined. Then it is immediately verified that Wn is given by
the following neat recursive equations:
Wn =
 Wn−1 ⊗

1
1

I2n−1 ⊗

−
1
1

,
with W0 = (1). If we let
B1 = 2  1 0
0 1 =

2 0
0 2
and for n ≥ 1,
Bn+1 = 2  Bn 0
0 I2n

,
then it is not hard to use the Kronecker product formulation of Wn to obtain a rigorous
proof of the equation
Wn
> Wn = Bn, for all n ≥ 1.
The above equation offers a clean justification of the fact that the columns of Wn are pairwise
orthogonal.
Observe that the right block (of size 2n × 2
n−1
) shows clearly how the detail coefficients
in the second half of the vector c are added and subtracted to the entries in the first half of
the partially reconstructed vector after n − 1 steps.
5.4 Multiresolution Signal Analysis with Haar Bases
An important and attractive feature of the Haar basis is that it provides a multiresolution
analysis of a signal. Indeed, given a signal u, if c = (c1, . . . , c2n ) is the vector of its Haar coef-
ficients, the coefficients with low index give coarse information about u, and the coefficients
with high index represent fine information. For example, if u is an audio signal corresponding
to a Mozart concerto played by an orchestra, c1 corresponds to the “background noise,” c2
to the bass, c3 to the first cello, c4 to the second cello, c5, c6, c7, c7 to the violas, then the
violins, etc. This multiresolution feature of wavelets can be exploited to compress a signal,
that is, to use fewer coefficients to represent it. Here is an example.
Consider the signal
u = (2.4, 2.2, 2.15, 2.05, 6.8, 2.8, −1.1, −1.3),
whose Haar transform is
c = (2, 0.2, 0.1, 3, 0.1, 0.05, 2, 0.1).
5.4. MULTIRESOLUTION SIGNAL ANALYSIS WITH HAAR BASES 151
The piecewise-linear curves corresponding to u and c are shown in Figure 5.6. Since some of
the coefficients in c are small (smaller than or equal to 0.2) we can compress c by replacing
them by 0. We get
c2 = (2, 0, 0, 3, 0, 0, 2, 0),
and the reconstructed signal is
u2 = (2, 2, 2, 2, 7, 3, −1, −1).
The piecewise-linear curves corresponding to u2 and c2 are shown in Figure 5.7.
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
2
1
0
1
2
3
4
5
6
7
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
0.5
1
1.5
2
2.5
3
Figure 5.6: A signal and its Haar transform.
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
1
0
1
2
3
4
5
6
7
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
0.5
1
1.5
2
2.5
3
Figure 5.7: A compressed signal and its compressed Haar transform.
An interesting (and amusing) application of the Haar wavelets is to the compression of
audio signals. It turns out that if your type load handel in Matlab an audio file will be
loaded in a vector denoted by y, and if you type sound(y), the computer will play this piece
of music. You can convert y to its vector of Haar coefficients c. The length of y is 73113,
152 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES
0 1 2 3 4 5 6 7
x 104
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
0 1 2 3 4 5 6 7
x 104
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
Figure 5.8: The signal “handel” and its Haar transform.
so first tuncate the tail of y to get a vector of length 65536 = 216. A plot of the signals
corresponding to y and c is shown in Figure 5.8. Then run a program that sets all coefficients
of c whose absolute value is less that 0.05 to zero. This sets 37272 coefficients to 0. The
resulting vector c2 is converted to a signal y2. A plot of the signals corresponding to y2 and
c2 is shown in Figure 5.9. When you type sound(y2), you find that the music doesn’t differ
0 1 2 3 4 5 6 7
x 104
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
0 1 2 3 4 5 6 7
x 104
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
Figure 5.9: The compressed signal “handel” and its Haar transform.
much from the original, although it sounds less crisp. You should play with other numbers
greater than or less than 0.05. You should hear what happens when you type sound(c). It
plays the music corresponding to the Haar transform c of y, and it is quite funny.
5.5. HAAR TRANSFORM FOR DIGITAL IMAGES 153
5.5 Haar Transform for Digital Images
Another neat property of the Haar transform is that it can be instantly generalized to
matrices (even rectangular) without any extra effort! This allows for the compression of
digital images. But first we address the issue of normalization of the Haar coefficients. As
we observed earlier, the 2n × 2
n matrix Wn of Haar basis vectors has orthogonal columns,
but its columns do not have unit length. As a consequence, Wn
>
is not the inverse of Wn,
but rather the matrix
Wn
−1 = DnWn
>
with Dn = diag 2
−n
,
|{z} 2
−n
2
0
, 2
|
−(n−1)
{z
, 2
−(n−1)
}
2
1
, 2
|
−(n−2)
, . . . ,
{z 2
−(n−2)
}
2
2
, . . . , 2
−1
, . . . , 2
−1
|
{z
}
2n−1

.
Definition 5.5. The orthogonal matrix
Hn = WnD
1
n
2
whose columns are the normalized Haar basis vectors, with
D
1
n
2 = diag 2
− n
2 , 2
− n
2
|{z}
2
0
, 2
−
n−1
2 , 2
−
n−1
2
|
{z
}
2
1
, 2
−
n−2
2 , . . . , 2
−
n−2
2
|
{z
}
2
2
, . . . , 2
− 1
2 , . . . , 2
− 1
2
|
{z
}
2n−1

is called the normalized Haar transform matrix. Given a vector (signal) u, we call c = Hn
> u
the normalized Haar coefficients of u.
Because Hn is orthogonal, Hn
−1 = Hn
>
.
Then a moment of reflection shows that we have to slightly modify the algorithms to
compute Hn
> u and Hnc as follows: When computing the sequence of u
j
s, use
u
j+1(2i − 1) = (u
j
(i) + u
j
(2j + i))/
√
2
u
j+1(2i) = (u
j
(i) − u
j
(2j + i))/
√
2,
and when computing the sequence of c
j
s, use
c
j
(i) = (c
j+1(2i − 1) + c
j+1(2i))/
√
2
c
j
(2j + i) = (c
j+1(2i − 1) − c
j+1(2i))/
√
2.
Note that things are now more symmetric, at the expense of a division by √
2. However, for
long vectors, it turns out that these algorithms are numerically more stable.
Remark: Some authors (for example, Stollnitz, Derose and Salesin [168]) rescale c by 1/
√
2
n
and u by √
2
n. This is because the norm of the basis functions ψk
j
is not equal to 1 (under
154 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES
the inner product h f, gi =
R
1
0
f(t)g(t)dt). The normalized basis functions are the functions
√
2
jψk
j
.
Let us now explain the 2D version of the Haar transform. We describe the version using
the matrix Wn, the method using Hn being identical (except that Hn
−1 = Hn
>
, but this does
not hold for Wn
−1
). Given a 2m × 2
n matrix A, we can first convert the rows of A to their
Haar coefficients using the Haar transform Wn
−1
, obtaining a matrix B, and then convert the
columns of B to their Haar coefficients, using the matrix Wm
−1
. Because columns and rows
are exchanged in the first step,
B = A(Wn
−1
)
> ,
and in the second step C = Wm
−1B, thus, we have
C = Wm
−1A(Wn
−1
)
> = DmWm
>AWn Dn.
In the other direction, given a 2m × 2
n matrix C of Haar coefficients, we reconstruct the
matrix A (the image) by first applying Wm to the columns of C, obtaining B, and then Wn
>
to the rows of B. Therefore
A = WmCWn
>
.
Of course, we don’t actually have to invert Wm and Wn and perform matrix multiplications.
We just have to use our algorithms using averaging and differencing. Here is an example.
If the data matrix (the image) is the 8 × 8 matrix
A =


64 2 3 61 60 6 7 57
9 55 54 12 13 51 50 16
17 47 46 20 21 43 42 24
40 26 27 37 36 30 31 33
32 34 35 29 28 38 39 25
41 23 22 44 45 19 18 48
49 15 14 52 53 11 10 56
8 58 59 5 4 62 63 1


,
then applying our algorithms, we find that
C =


32
0 0 0 0 0 0 0 0
.5 0 0 0 0 0 0 0
0 0 0 0 4 −4 4 −4
0 0 0 0 4
0 0 0.5 0.5 27 −
−
25 23
4 4
−
−
21
4
0 0 −0.5 −0.5 −11 9 −7 5
0 0 0.5 0.5
0 0 −0.5 −0.5 21
−5 7
−23 25
−9 11
−27


.
5.5. HAAR TRANSFORM FOR DIGITAL IMAGES 155
As we can see, C has more zero entries than A; it is a compressed version of A. We can
further compress C by setting to 0 all entries of absolute value at most 0.5. Then we get
C2 =


32
0 0 0 0 0 0 0 0
.5 0 0 0 0 0 0 0
0 0 0 0 4 −4 4 −4
0 0 0 0 4
0 0 0 0 27 −
−
25 23
4 4
−
−
21
4
0 0 0 0 −11 9 −7 5
0 0 0 0
0 0 0 0 21
−5 7
−23 25
−9 11
−27


.
We find that the reconstructed image is
A2 =


63
9.
.
5 55
5 1.
.
5 3
5 53
.
.
5 61
5 11
.
.
5 59
5 13
.
.
5 5
5 51
.
.
5 7
5 49
.
.
5 57
5 15
.
.
5
5
17.5 47.5 45.5 19.5 21.5 43.5 41.5 23.5
39
31
.
.
5 25
5 33
.
.
5 27
5 35
.
.
5 37
5 29
.
.
5 35
5 27
.
.
5 29
5 37
.
.
5 31
5 39
.
.
5 33
5 25
.
.
5
5
41
49
.
.
5 23
5 15
.
.
5 21
5 13
.
.
5 43
5 51
.
.
5 45
5 53
.
.
5 19
5 11
.
.
5 17
5 9.
.
5 55
5 47.
.
5
5
7.5 57.5 59.5 5.5 3.5 61.5 63.5 1.5


,
which is pretty close to the original image matrix A.
It turns out that Matlab has a wonderful command, image(X) (also imagesc(X), which
often does a better job), which displays the matrix X has an image in which each entry
is shown as a little square whose gray level is proportional to the numerical value of that
entry (lighter if the value is higher, darker if the value is closer to zero; negative values are
treated as zero). The images corresponding to A and C are shown in Figure 5.10. The
compressed images corresponding to A2 and C2 are shown in Figure 5.11. The compressed
versions appear to be indistinguishable from the originals!
If we use the normalized matrices Hm and Hn, then the equations relating the image
matrix A and its normalized Haar transform C are
C = Hm
>AHn
A = HmCHn
>
.
The Haar transform can also be used to send large images progressively over the internet.
Indeed, we can start sending the Haar coefficients of the matrix C starting from the coarsest
coefficients (the first column from top down, then the second column, etc.), and at the
receiving end we can start reconstructing the image as soon as we have received enough
data.
156 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES
Figure 5.10: An image and its Haar transform.
Figure 5.11: Compressed image and its Haar transform.
Observe that instead of performing all rounds of averaging and differencing on each row
and each column, we can perform partial encoding (and decoding). For example, we can
perform a single round of averaging and differencing for each row and each column. The
result is an image consisting of four subimages, where the top left quarter is a coarser version
of the original, and the rest (consisting of three pieces) contain the finest detail coefficients.
We can also perform two rounds of averaging and differencing, or three rounds, etc. The
second round of averaging and differencing is applied to the top left quarter of the image.
Generally, the kth round is applied to the 2m+1−k × 2
n+1−k
submatrix consisting of the first
2
m+1−k
rows and the first 2n+1−k
columns (1 ≤ k ≤ n) of the matrix obtained at the end of
the previous round. This process is illustrated on the image shown in Figure 5.12. The result
of performing one round, two rounds, three rounds, and nine rounds of averaging is shown in
Figure 5.13. Since our images have size 512 × 512, nine rounds of averaging yields the Haar
transform, displayed as the image on the bottom right. The original image has completely
5.5. HAAR TRANSFORM FOR DIGITAL IMAGES 157
Figure 5.12: Original drawing by Durer.
disappeared! We leave it as a fun exercise to modify the algorithms involving averaging and
differencing to perform k rounds of averaging/differencing. The reconstruction algorithm is
a little tricky.
A nice and easily accessible account of wavelets and their uses in image processing and
computer graphics can be found in Stollnitz, Derose and Salesin [168]. A very detailed
account is given in Strang and and Nguyen [172], but this book assumes a fair amount of
background in signal processing.
We can find easily a basis of 2n × 2
n = 22n vectors wij (2n × 2
n matrices) for the linear
map that reconstructs an image from its Haar coefficients, in the sense that for any 2n × 2
n
matrix C of Haar coefficients, the image matrix A is given by
A =
2
n X
i=1
2
n X
j=1
cijwij .
Indeed, the matrix wij is given by the so-called outer product
wij = wi(wj )
> .
158 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES
Figure 5.13: Haar tranforms after one, two, three, and nine rounds of averaging.
5.6. HADAMARD MATRICES 159
Similarly, there is a basis of 2n × 2
n = 22n vectors hij (2n × 2
n matrices) for the 2D Haar
transform, in the sense that for any 2n × 2
n matrix A, its matrix C of Haar coefficients is
given by
C =
2
n X
i=1
2
n X
j=1
aijhij .
If the columns of W−1 are w1
0
, . . . , w2
0n , then
hij = wi
0
(wj
0
)
> .
We leave it as exercise to compute the bases (wij ) and (hij ) for n = 2, and to display the
corresponding images using the command imagesc.
5.6 Hadamard Matrices
There is another famous family of matrices somewhat similar to Haar matrices, but these
matrices have entries +1 and −1 (no zero entries).
Definition 5.6. A real n × n matrix H is a Hadamard matrix if hij = ±1 for all i, j such
that 1 ≤ i, j ≤ n and if
H
> H = nIn.
Thus the columns of a Hadamard matrix are pairwise orthogonal. Because H is a square
matrix, the equation H> H = nIn shows that H is invertible, so we also have HH> = nIn.
The following matrices are example of Hadamard matrices:
H2 =

1 1
1 −1

, H4 =


1 1 1 1
1 −1 1 −1
1 1 −1 −1
1 −1 −1 1

 ,
and
H8 =


1 1 1 1 1 1 1 1
1 −1 1 −1 1 −1 1 −1
1 1 −1 −1 1 1 −1 −1
1
1 1 1 1
−1 −1 1 1
−1
−
−
1
1
−
−
1 1
1 −1
1 −1 1 −1 −1 1 −1 1
1 1
1 −1
−
−
1
1 1
−1 −
−
1
1 1 1
−1 1 1
−1


.
A natural question is to determine the positive integers n for which a Hadamard matrix
of dimension n exists, but surprisingly this is an open problem. The Hadamard conjecture is
that for every positive integer of the form n = 4k, there is a Hadamard matrix of dimension
n.
What is known is a necessary condition and various sufficient conditions.
160 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES
Theorem 5.1. If H is an n×n Hadamard matrix, then either n = 1, 2, or n = 4k for some
positive integer k.
Sylvester introduced a family of Hadamard matrices and proved that there are Hadamard
matrices of dimension n = 2m for all m ≥ 1 using the following construction.
Proposition 5.2. (Sylvester, 1867) If H is a Hadamard matrix of dimension n, then the
block matrix of dimension 2n, 
H H
H −H

,
is a Hadamard matrix.
If we start with
H2 =

1 1
1 −1

,
we obtain an infinite family of symmetric Hadamard matrices usually called Sylvester–
Hadamard matrices and denoted by H2m. The Sylvester–Hadamard matrices H2, H4 and
H8 are shown on the previous page.
In 1893, Hadamard gave examples of Hadamard matrices for n = 12 and n = 20. At the
present, Hadamard matrices are known for all n = 4k ≤ 1000, except for n = 668, 716, and
892.
Hadamard matrices have various applications to error correcting codes, signal processing,
and numerical linear algebra; see Seberry, Wysocki and Wysocki [154] and Tropp [177]. For
example, there is a code based on H32 that can correct 7 errors in any 32-bit encoded block,
and can detect an eighth. This code was used on a Mariner spacecraft in 1969 to transmit
pictures back to the earth.
For every m ≥ 0, the piecewise affine functions plf((H2m)i) associated with the 2m rows
of the Sylvester–Hadamard matrix H2m are functions on [0, 1] known as the Walsh functions.
It is customary to index these 2m functions by the integers 0, 1, . . . , 2
m −1 in such a way that
the Walsh function Wal(k, t) is equal to the function plf((H2m)i) associated with the Row i
of H2m that contains k changes of signs between consecutive groups of +1 and consecutive
groups of −1. For example, the fifth row of H8, namely
￾
1 −1 −1 1 1 −1 −1 1
 ,
has five consecutive blocks of +1s and −1s, four sign changes between these blocks, and thus
is associated with Wal(4, t). In particular, Walsh functions corresponding to the rows of H8
(from top down) are:
Wal(0, t), Wal(7, t), Wal(3, t), Wal(4, t), Wal(1, t), Wal(6, t), Wal(2, t), Wal(5, t).
Because of the connection between Sylvester–Hadamard matrices and Walsh functions,
Sylvester–Hadamard matrices are called Walsh–Hadamard matrices by some authors. For
5.7. SUMMARY 161
every m, the 2m Walsh functions are pairwise orthogonal. The countable set of Walsh
functions Wal(k, t) for all m ≥ 0 and all k such that 0 ≤ k ≤ 2
m − 1 can be ordered in
such a way that it is an orthogonal Hilbert basis of the Hilbert space L2
([0, 1)]; see Seberry,
Wysocki and Wysocki [154].
The Sylvester–Hadamard matrix H2m plays a role in various algorithms for dimension
reduction and low-rank matrix approximation. There is a type of structured dimension￾reduction map known as the subsampled randomized Hadamard transform, for short SRHT;
see Tropp [177] and Halko, Martinsson and Tropp [86]. For `  n = 2m, an SRHT matrix
is an ` × n matrix of the form
Φ = r n
`
RHD,
where
1. D is a random n × n diagonal matrix whose entries are independent random signs.
2. H = n
−1/2Hn, a normalized Sylvester–Hadamard matrix of dimension n.
3. R is a random ` × n matrix that restricts an n-dimensional vector to ` coordinates,
chosen uniformly at random.
It is explained in Tropp [177] that for any input x such that k xk 2 = 1, the probability
that |(HDx)i
| ≥ p n−1
log(n) for any i is quite small. Thus HD has the effect of “flattening”
the input x. The main result about the SRHT is that it preserves the geometry of an entire
subspace of vectors; see Tropp [177] (Theorem 1.3).
5.7 Summary
The main concepts and results of this chapter are listed below:
• Haar basis vectors and a glimpse at Haar wavelets.
• Kronecker product (or tensor product) of matrices.
• Hadamard and Sylvester–Hadamard matrices.
• Walsh functions.
162 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES
5.8 Problems
Problem 5.1. (Haar extravaganza) Consider the matrix
W3,3 =


1 0 0 0 1 0 0 0
1 0 0 0 −1 0 0 0
0 1 0 0 0 1 0 0
0 1 0 0 0
0 0 1 0 0 0 1 0
−1 0 0
0 0 1 0 0 0 −1 0
0 0 0 1 0 0 0 1
0 0 0 1 0 0 0 −1


.
(1) Show that given any vector c = (c1, c2, c3, c4, c5, c6, c7, c8), the result W3,3c of applying
W3,3 to c is
W3,3c = (c1 + c5, c1 − c5, c2 + c6, c2 − c6, c3 + c7, c3 − c7, c4 + c8, c4 − c8),
the last step in reconstructing a vector from its Haar coefficients.
(2) Prove that the inverse of W3,3 is (1/2)W3
>,3
. Prove that the columns and the rows of
W3,3 are orthogonal.
(3) Let W3,2 and W3,1 be the following matrices:
W3,2 =


1 0 1 0 0 0 0 0
1 0 −1 0 0 0 0 0
0 1 0 1 0 0 0 0
0 1 0
0 0 0 0 1 0 0 0
−1 0 0 0 0
0 0 0 0 0 1 0 0
0 0 0 0 0 0 1 0
0 0 0 0 0 0 0 1


, W3,1 =


1 1 0 0 0 0 0 0
1 −1 0 0 0 0 0 0
0 0 1 0 0 0 0 0
0 0 0 1 0 0 0 0
0 0 0 0 1 0 0 0
0 0 0 0 0 1 0 0
0 0 0 0 0 0 1 0
0 0 0 0 0 0 0 1


.
Show that given any vector c = (c1, c2, c3, c4, c5, c6, c7, c8), the result W3,2c of applying W3,2
to c is
W3,2c = (c1 + c3, c1 − c3, c2 + c4, c2 − c4, c5, c6, c7, c8),
the second step in reconstructing a vector from its Haar coefficients, and the result W3,1c of
applying W3,1 to c is
W3,1c = (c1 + c2, c1 − c2, c3, c4, c5, c6, c7, c8),
the first step in reconstructing a vector from its Haar coefficients.
Conclude that
W3,3W3,2W3,1 = W3,
5.8. PROBLEMS 163
the Haar matrix
W3 =


1 1 1 0 1 0 0 0
1 1 1 0 −1 0 0 0
1 1 −1 0 0 1 0 0
1 1
1 −1 0 1 0 0 1 0
−1 0 0 −1 0 0
1 −1 0 1 0 0 −1 0
1
1
−
−
1 0
1 0 −
−
1 0 0 0 1
1 0 0 0 −1


.
Hint. First check that
W3,2W3,1 =

W2 04,4
04,4 I4

,
where
W2 =


1 1 1 0
1 1
1 −1 0 1
−1 0
1 −1 0 −1

 .
(4) Prove that the columns and the rows of W3,2 and W3,1 are orthogonal. Deduce from
this that the columns of W3 are orthogonal, and the rows of W3
−1
are orthogonal. Are the
rows of W3 orthogonal? Are the columns of W3
−1
orthogonal? Find the inverse of W3,2 and
the inverse of W3,1.
Problem 5.2. This is a continuation of Problem 5.1.
(1) For any n ≥ 2, the 2n × 2
n matrix Wn,n is obtained form the two rows
1, 0, . . . , 0
|
2
{zn−1
}
, 1, 0, . . . , 0
|
2
{zn−1
}
1, 0, . . . , 0
|
2
{zn−1
}
, −1, 0, . . . , 0
|
2
{zn−1
}
by shifting them 2n−1 − 1 times over to the right by inserting a zero on the left each time.
Given any vector c = (c1, c2, . . . , c2n ), show that Wn,nc is the result of the last step in the
process of reconstructing a vector from its Haar coefficients c. Prove that Wn,n
−1 = (1/2)Wn,n
>,
and that the columns and the rows of Wn,n are orthogonal.
(2) Given a m × n matrix A = (aij ) and a p × q matrix B = (bij ), the Kronecker product
(or tensor product) A ⊗ B of A and B is the mp × nq matrix
A ⊗ B =


a11B a12B · · · a1nB
a21
.
B a22B · · · a2nB
.
.
.
.
.
.
.
.
.
.
.
am1B am2B · · · amnB


.
164 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES
It can be shown (and you may use these facts without proof) that ⊗ is associative and that
(A ⊗ B)(C ⊗ D) = AC ⊗ BD
(A ⊗ B)
> = A
> ⊗ B
> ,
whenever AC and BD are well defined.
Check that
Wn,n =
 I2n−1 ⊗

1
1

I2n−1 ⊗

−
1
1

,
and that
Wn =
 Wn−1 ⊗

1
1

I2n−1 ⊗

−
1
1

.
Use the above to reprove that
Wn,nWn,n
> = 2I2n .
Let
B1 = 2  1 0
0 1 =
