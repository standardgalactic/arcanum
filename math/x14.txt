 and Ae =


12 6 0
12 0
0 −6 4
−4

 ,
7.5. SYSTEMS OF LINEAR EQUATIONS AND DETERMINANTS 227
and we have


1 1 1
2
3 3
−2 −
−
2
3




12 6 0
12 0
0 −6 4
−4

 = 24


1 0 0
0 1 0
0 0 1


with det(A) = 24.
When K is a field, an element a ∈ K is invertible iff a 6 = 0. In this case, the second part
of the proposition can be stated as A is invertible iff det(A) 6 = 0. Note in passing that this
method of computing the inverse of a matrix is usually not practical.
7.5 Systems of Linear Equations and Determinants
We now consider some applications of determinants to linear independence and to solving
systems of linear equations. Although these results hold for matrices over certain rings, their
proofs require more sophisticated methods. Therefore, we assume again that K is a field
(usually, K = R or K = C).
Let A be an n × n-matrix, x a column vectors of variables, and b another column vector,
and let A1
, . . . , An denote the columns of A. Observe that the system of equations Ax = b,


a1 1 a1 2 . . . a1 n
a2 1 a2 2 . . . a2 n
.
.
.
.
.
.
.
.
.
.
.
.
an 1 an 2 . . . an n




x
x
1
2
.
.
x
.
n


=


b
b
1
2
.
.
b
.
n


is equivalent to
x1A
1 + · · · + xjA
j + · · · + xnA
n = b,
since the equation corresponding to the i-th row is in both cases
ai 1x1 + · · · + ai jxj + · · · + ai nxn = bi
.
First we characterize linear independence of the column vectors of a matrix A in terms
of its determinant.
Proposition 7.11. Given an n × n-matrix A over a field K, the columns A1
, . . . , An of
A are linearly dependent iff det(A) = det(A1
, . . . , An
) = 0. Equivalently, A has rank n iff
det(A) 6 = 0.
Proof. First assume that the columns A1
, . . . , An of A are linearly dependent. Then there
are x1, . . . , xn ∈ K, such that
x1A
1 + · · · + xjA
j + · · · + xnA
n = 0,
228 CHAPTER 7. DETERMINANTS
where xj 6 = 0 for some j. If we compute
det(A
1
, . . . , x1A
1 + · · · + xjA
j + · · · + xnA
n
, . . . , An
) = det(A
1
, . . . , 0, . . . , An
) = 0,
where 0 occurs in the j-th position. By multilinearity, all terms containing two identical
columns Ak
for k 6 = j vanish, and we get
det(A
1
, . . . , x1A
1 + · · · + xjA
j + · · · + xnA
n
, . . . , An
) = xj det(A
1
, . . . , An
) = 0.
Since xj 6 = 0 and K is a field, we must have det(A1
, . . . , An
) = 0.
Conversely, we show that if the columns A1
, . . . , An of A are linearly independent, then
det(A1
, . . . , An
) 6 = 0. If the columns A1
, . . . , An of A are linearly independent, then they
form a basis of Kn
, and we can express the standard basis (e1, . . . , en) of Kn
in terms of
A1
, . . . , An
. Thus, we have


e
e
1
2
.
.
e
.
n


=


b1 1 b1 2 . . . b1 n
b2 1 b2 2 . . . b2 n
.
.
.
.
.
.
.
.
.
.
.
.
bn 1 bn 2 . . . bn n




A
A
1
2
.
.
A
.
n


,
for some matrix B = (bi j ), and by Proposition 7.8, we get
det(e1, . . . , en) = det(B) det(A
1
, . . . , An
),
and since det(e1, . . . , en) = 1, this implies that det(A1
, . . . , An
) 6 = 0 (and det(B) 6 = 0). For
the second assertion, recall that the rank of a matrix is equal to the maximum number of
linearly independent columns, and the conclusion is clear.
We now characterize when a system of linear equations of the form Ax = b has a unique
solution.
Proposition 7.12. Given an n × n-matrix A over a field K, the following properties hold:
(1) For every column vector b, there is a unique column vector x such that Ax = b iff the
only solution to Ax = 0 is the trivial vector x = 0, iff det(A) 6 = 0.
(2) If det(A) 6 = 0, the unique solution of Ax = b is given by the expressions
xj =
det(A1
, . . . , Aj−1
, b, Aj+1, . . . , An
)
det(A1
, . . . , Aj−1
, Aj
, Aj+1, . . . , An)
,
known as Cramer’s rules .
(3) The system of linear equations Ax = 0 has a nonzero solution iff det(A) = 0.
7.6. DETERMINANT OF A LINEAR MAP 229
Proof. (1) Assume that Ax = b has a single solution x0, and assume that Ay = 0 with y 6 = 0.
Then,
A(x0 + y) = Ax0 + Ay = Ax0 + 0 = b,
and x0 + y 6 = x0 is another solution of Ax = b, contradicting the hypothesis that Ax = b has
a single solution x0. Thus, Ax = 0 only has the trivial solution. Now assume that Ax = 0
only has the trivial solution. This means that the columns A1
, . . . , An of A are linearly
independent, and by Proposition 7.11, we have det(A) 6 = 0. Finally, if det(A) 6 = 0, by
Proposition 7.10, this means that A is invertible, and then for every b, Ax = b is equivalent
to x = A−1
b, which shows that Ax = b has a single solution.
(2) Assume that Ax = b. If we compute
det(A
1
, . . . , x1A
1 + · · · + xjA
j + · · · + xnA
n
, . . . , An
) = det(A
1
, . . . , b, . . . , An
),
where b occurs in the j-th position, by multilinearity, all terms containing two identical
columns Ak
for k 6 = j vanish, and we get
xj det(A
1
, . . . , An
) = det(A
1
, . . . , Aj−1
, b, Aj+1, . . . , An
),
for every j, 1 ≤ j ≤ n. Since we assumed that det(A) = det(A1
, . . . , An
) 6 = 0, we get the
desired expression.
(3) Note that Ax = 0 has a nonzero solution iff A1
, . . . , An are linearly dependent (as
observed in the proof of Proposition 7.11), which, by Proposition 7.11, is equivalent to
det(A) = 0.
As pleasing as Cramer’s rules are, it is usually impractical to solve systems of linear
equations using the above expressions. However, these formula imply an interesting fact,
which is that the solution of the system Ax = b are continuous in A and b. If we assume that
the entries in A are continuous functions aij (t) and the entries in b are are also continuous
functions bj (t) of a real parameter t, since determinants are polynomial functions of their
entries, the expressions
xj (t) = det(A1
, . . . , Aj−1
, b, Aj+1, . . . , An
)
det(A1
, . . . , Aj−1
, Aj
, Aj+1, . . . , An)
are ratios of polynomials, and thus are also continuous as long as det(A(t)) is nonzero.
Similarly, if the functions aij (t) and bj (t) are differentiable, so are the xj (t).
7.6 Determinant of a Linear Map
Given a vector space E of finite dimension n, given a basis (u1, . . . , un) of E, for every linear
map f : E → E, if M(f) is the matrix of f w.r.t. the basis (u1, . . . , un), we can define
det(f) = det(M(f)). If (v1, . . . , vn) is any other basis of E, and if P is the change of basis
230 CHAPTER 7. DETERMINANTS
matrix, by Corollary 4.6, the matrix of f with respect to the basis (v1, . . . , vn) is P
−1M(f)P.
By Proposition 7.9, we have
det(P
−1M(f)P) = det(P
−1
) det(M(f)) det(P) = det(P
−1
) det(P) det(M(f)) = det(M(f)).
Thus, det(f) is indeed independent of the basis of E.
Definition 7.8. Given a vector space E of finite dimension, for any linear map f : E → E,
we define the determinant det(f) of f as the determinant det(M(f)) of the matrix of f in
any basis (since, from the discussion just before this definition, this determinant does not
depend on the basis).
Then we have the following proposition.
Proposition 7.13. Given any vector space E of finite dimension n, a linear map f : E → E
is invertible iff det(f) 6 = 0.
Proof. The linear map f : E → E is invertible iff its matrix M(f) in any basis is invertible
(by Proposition 4.2), iff det(M(f)) 6 = 0, by Proposition 7.10.
Given a vector space of finite dimension n, it is easily seen that the set of bijective linear
maps f : E → E such that det(f) = 1 is a group under composition. This group is a
subgroup of the general linear group GL(E). It is called the special linear group (of E), and
it is denoted by SL(E), or when E = Kn
, by SL(n, K), or even by SL(n).
7.7 The Cayley–Hamilton Theorem
We next discuss an interesting and important application of Proposition 7.10, the Cayley–
Hamilton theorem. The results of this section apply to matrices over any commutative ring
K. First we need the concept of the characteristic polynomial of a matrix.
Definition 7.9. If K is any commutative ring, for every n × n matrix A ∈ Mn(K), the
characteristic polynomial PA(X) of A is the determinant
PA(X) = det(XI − A).
The characteristic polynomial PA(X) is a polynomial in K[X], the ring of polynomials
in the indeterminate X with coefficients in the ring K. For example, when n = 2, if
A =

a b
c d ,
then
PA(X) =

 
 X
−
−
c X
a −
−
b
d



= X
2 − (a + d)X + ad − bc.
7.7. THE CAYLEY–HAMILTON THEOREM 231
We can substitute the matrix A for the variable X in the polynomial PA(X), obtaining a
matrix PA. If we write
PA(X) = X
n + c1X
n−1 + · · · + cn,
then
PA = A
n + c1A
n−1 + · · · + cnI.
We have the following remarkable theorem.
Theorem 7.14. (Cayley–Hamilton) If K is any commutative ring, for every n × n matrix
A ∈ Mn(K), if we let
PA(X) = X
n + c1X
n−1 + · · · + cn
be the characteristic polynomial of A, then
PA = A
n + c1A
n−1 + · · · + cnI = 0.
Proof. We can view the matrix B = XI − A as a matrix with coefficients in the polynomial
ring K[X], and then we can form the matrix e B which is the transpose of the matrix of
cofactors of elements of B. Each entry in Be is an (n − 1) × (n − 1) determinant, and thus a
polynomial of degree a most n − 1, so we can write Be as
Be = X
n−1B0 + X
n−2B1 + · · · + Bn−1,
for some n × n matrices B0, . . . , Bn−1 with coefficients in K. For example, when n = 2, we
have
B =

X
−
−
c X
a −
−
b
d

, Be =

X
c X
− d b
− a

= X

1 0
0 1 +

−
c
d b
−a

.
By Proposition 7.10, we have
BBe = det(B)I = PA(X)I.
On the other hand, we have
BBe = (XI − A)(X
n−1B0 + X
n−2B1 + · · · + X
n−j−1Bj + · · · + Bn−1),
and by multiplying out the right-hand side, we get
BBe = X
nD0 + X
n−1D1 + · · · + X
n−jDj + · · · + Dn,
with
D0 = B0
D1 = B1 − AB0
.
.
.
Dj = Bj − ABj−1
.
.
.
Dn−1 = Bn−1 − ABn−2
Dn = −ABn−1.
232 CHAPTER 7. DETERMINANTS
Since
PA(X)I = (X
n + c1X
n−1 + · · · + cn)I,
the equality
X
nD0 + X
n−1D1 + · · · + Dn = (X
n + c1X
n−1 + · · · + cn)I
is an equality between two matrices, so it requires that all corresponding entries are equal,
and since these are polynomials, the coefficients of these polynomials must be identical,
which is equivalent to the set of equations
I = B0
c1I = B1 − AB0
.
.
.
cjI = Bj − ABj−1
.
.
.
cn−1I = Bn−1 − ABn−2
cnI = −ABn−1,
for all j, with 1 ≤ j ≤ n − 1. If, as in the table below,
A
n = A
nB0
c1A
n−1 = A
n−1
(B1 − AB0)
.
.
.
cjA
n−j = A
n−j
(Bj − ABj−1)
.
.
.
cn−1A = A(Bn−1 − ABn−2)
cnI = −ABn−1,
we multiply the first equation by An
, the last by I, and generally the (j + 1)th by An−j
,
when we add up all these new equations, we see that the right-hand side adds up to 0, and
we get our desired equation
A
n + c1A
n−1 + · · · + cnI = 0,
as claimed.
As a concrete example, when n = 2, the matrix
A =

a b
c d
satisfies the equation
A
2 − (a + d)A + (ad − bc)I = 0.
7.7. THE CAYLEY–HAMILTON THEOREM 233
Most readers will probably find the proof of Theorem 7.14 rather clever but very myste￾rious and unmotivated. The conceptual difficulty is that we really need to understand how
polynomials in one variable “act” on vectors in terms of the matrix A. This can be done and
yields a more “natural” proof. Actually, the reasoning is simpler and more general if we free
ourselves from matrices and instead consider a finite-dimensional vector space E and some
given linear map f : E → E. Given any polynomial p(X) = a0Xn + a1Xn−1 + · · · + an with
coefficients in the field K, we define the linear map p(f): E → E by
p(f) = a0f
n + a1f
n−1 + · · · + anid,
where f
k = f ◦ · · · ◦ f, the k-fold composition of f with itself. Note that
p(f)(u) = a0f
n
(u) + a1f
n−1
(u) + · · · + anu,
for every vector u ∈ E. Then we define a new kind of scalar multiplication ·: K[X]×E → E
by polynomials as follows: for every polynomial p(X) ∈ K[X], for every u ∈ E,
p(X) · u = p(f)(u).
It is easy to verify that this is a “good action,” which means that
p · (u + v) = p · u + p · v
(p + q) · u = p · u + q · u
(pq) · u = p · (q · u)
1 · u = u,
for all p, q ∈ K[X] and all u, v ∈ E. With this new scalar multiplication, E is a K[X]-module.
If p = λ is just a scalar in K (a polynomial of degree 0), then
λ · u = (λid)(u) = λu,
which means that K acts on E by scalar multiplication as before. If p(X) = X (the monomial
X), then
X · u = f(u).
Now if we pick a basis (e1, . . . , en) of E, if a polynomial p(X) ∈ K[X] has the property
that
p(X) · ei = 0, i = 1, . . . , n,
then this means that p(f)(ei) = 0 for i = 1, . . . , n, which means that the linear map p(f)
vanishes on E. We can also check, as we did in Section 7.2, that if A and B are two n × n
matrices and if (u1, . . . , un) are any n vectors, then
A ·


B ·


u1
.
.
u
.
n



 = (AB) ·


u1
.
.
.
un

 .
234 CHAPTER 7. DETERMINANTS
This suggests the plan of attack for our second proof of the Cayley–Hamilton theorem.
For simplicity, we prove the theorem for vector spaces over a field. The proof goes through
for a free module over a commutative ring.
Theorem 7.15. (Cayley–Hamilton) For every finite-dimensional vector space over a field
K, for every linear map f : E → E, for every basis (e1, . . . , en), if A is the matrix over f
over the basis (e1, . . . , en) and if
PA(X) = X
n + c1X
n−1 + · · · + cn
is the characteristic polynomial of A, then
PA(f) = f
n + c1f
n−1 + · · · + cnid = 0.
Proof. Since the columns of A consist of the vector f(ej ) expressed over the basis (e1, . . . , en),
we have
f(ej ) =
nX
i=1
ai jei
, 1 ≤ j ≤ n.
Using our action of K[X] on E, the above equations can be expressed as
X · ej =
nX
i=1
ai j · ei
, 1 ≤ j ≤ n,
which yields
j−1
X
i=1
−ai j · ei + (X − aj j ) · ej +
nX
i=j+1
−ai j · ei = 0, 1 ≤ j ≤ n.
Observe that the transpose of the characteristic polynomial shows up, so the above system
can be written as


X − a1 1 −a2 1 · · · −an 1
−a1 2 X − a2 2 · · · −an 2
.
.
.
.
.
.
.
.
.
.
.
.
−a1 n −a2 n · · · X − an n


·


e
e
1
2
.
.
e
.
n


=


0
0
.
.
0
.


.
If we let B = XI − A> , then as in the previous proof, if Be is the transpose of the matrix of
cofactors of B, we have
BBe = det(B)I = det(XI − A
> )I = det(XI − A)I = PAI.
But since
B ·


e1
e2
.
e
.
.
n


=


0
0
.
.
0
.


,
7.8. PERMANENTS 235
and since Be is matrix whose entries are polynomials in K[X], it makes sense to multiply on
the left by Be and we get
e
B · B ·


e1
e2
.
e
.
.
n


= (BBe) ·


e1
e2
.
.
e
.
n


= PAI ·


e1
e2
.
.
e
.
n


= Be ·


0
0
.
.
0
.


=


0
0
.
.
0
.


;
that is,
PA · ej = 0, j = 1, . . . , n,
which proves that PA(f) = 0, as claimed.
If K is a field, then the characteristic polynomial of a linear map f : E → E is independent
of the basis (e1, . . . , en) chosen in E. To prove this, observe that the matrix of f over another
basis will be of the form P
−1AP, for some inverible matrix P, and then
det(XI − P
−1AP) = det(XP −1
IP − P
−1AP)
= det(P
−1
(XI − A)P)
= det(P
−1
) det(XI − A) det(P)
= det(XI − A).
Therefore, the characteristic polynomial of a linear map is intrinsic to f, and it is denoted
by Pf .
The zeros (roots) of the characteristic polynomial of a linear map f are called the eigen￾values of f. They play an important role in theory and applications. We will come back to
this topic later on.
7.8 Permanents
Recall that the explicit formula for the determinant of an n × n matrix is
det(A) = X
π∈Sn

(π)aπ(1) 1 · · · aπ(n) n.
If we drop the sign  (π) of every permutation from the above formula, we obtain a quantity
known as the permanent:
per(A) = X
π∈Sn
aπ(1) 1 · · · aπ(n) n.
Permanents and determinants were investigated as early as 1812 by Cauchy. It is clear from
the above definition that the permanent is a multilinear symmetric form. We also have
per(A) = per(A
> ),
236 CHAPTER 7. DETERMINANTS
and the following unsigned version of the Laplace expansion formula:
per(A) = ai 1per(Ai 1) + · · · + ai jper(Ai j ) + · · · + ai nper(Ai n),
for i = 1, . . . , n. However, unlike determinants which have a clear geometric interpretation as
signed volumes, permanents do not have any natural geometric interpretation. Furthermore,
determinants can be evaluated efficiently, for example using the conversion to row reduced
echelon form, but computing the permanent is hard.
Permanents turn out to have various combinatorial interpretations. One of these is in
terms of perfect matchings of bipartite graphs which we now discuss.
See Definition 20.5 for the definition of an undirected graph. A bipartite (undirected)
graph G = (V, E) is a graph whose set of nodes V can be partitioned into two nonempty
disjoint subsets V1 and V2, such that every edge e ∈ E has one endpoint in V1 and one
endpoint in V2.
An example of a bipartite graph with 14 nodes is shown in Figure 7.3; its nodes are
partitioned into the two sets {x1, x2, x3, x4, x5, x6, x7} and {y1, y2, y3, y4, y5, y6, y7}.
x1 x2 x3 x4 x5 x6 x7
y1 y2 y3 y4 y5 y6 y7
Figure 7.3: A bipartite graph G.
A matching in a graph G = (V, E) (bipartite or not) is a set M of pairwise non-adjacent
edges, which means that no two edges in M share a common vertex. A perfect matching is
a matching such that every node in V is incident to some edge in the matching M (every
node in V is an endpoint of some edge in M). Figure 7.4 shows a perfect matching (in red)
in the bipartite graph G.
Obviously, a perfect matching in a bipartite graph can exist only if its set of nodes has
a partition in two blocks of equal size, say {x1, . . . , xm} and {y1, . . . , ym}. Then there is
a bijection between perfect matchings and bijections π : {x1, . . . , xm} → {y1, . . . , ym} such
that π(xi) = yj
iff there is an edge between xi and yj
.
Now every bipartite graph G with a partition of its nodes into two sets of equal size as
above is represented by an m × m matrix A = (aij ) such that aij = 1 iff there is an edge
between xi and yj
, and aij = 0 otherwise. Using the interpretation of perfect matchings as
7.9. SUMMARY 237
x1 x2 x3 x4 x5 x6 x7
y1 y2 y3 y4 y5 y6 y7
Figure 7.4: A perfect matching in the bipartite graph G.
bijections π : {x1, . . . , xm} → {y1, . . . , ym}, we see that the permanent per(A) of the (0, 1)-
matrix A representing the bipartite graph G counts the number of perfect matchings in G.
In a famous paper published in 1979, Leslie Valiant proves that computing the permanent
is a #P-complete problem. Such problems are suspected to be intractable. It is known that
if a polynomial-time algorithm existed to solve a #P-complete problem, then we would have
P = NP, which is believed to be very unlikely.
Another combinatorial interpretation of the permanent can be given in terms of systems
of distinct representatives. Given a finite set S, let (A1, . . . , An) be any sequence of nonempty
subsets of S (not necessarily distinct). A system of distinct representatives (for short SDR)
of the sets A1, . . . , An is a sequence of n distinct elements (a1, . . . , an), with ai ∈ Ai
for i =
1, . . . , n. The number of SDR’s of a sequence of sets plays an important role in combinatorics.
Now, if S = {1, 2, . . . , n} and if we associate to any sequence (A1, . . . , An) of nonempty
subsets of S the matrix A = (aij ) defined such that aij = 1 if j ∈ Ai and aij = 0 otherwise,
then the permanent per(A) counts the number of SDR’s of the sets A1, . . . , An.
This interpretation of permanents in terms of SDR’s can be used to prove bounds for the
permanents of various classes of matrices. Interested readers are referred to van Lint and
Wilson [180] (Chapters 11 and 12). In particular, a proof of a theorem known as Van der
Waerden conjecture is given in Chapter 12. This theorem states that for any n × n matrix
A with nonnegative entries in which all row-sums and column-sums are 1 (doubly stochastic
matrices), we have
per(A) ≥
n!
nn
,
with equality for the matrix in which all entries are equal to 1/n.
7.9 Summary
The main concepts and results of this chapter are listed below:
238 CHAPTER 7. DETERMINANTS
• Permutations, transpositions, basics transpositions.
• Every permutation can be written as a composition of permutations.
• The parity of the number of transpositions involved in any decomposition of a permu￾tation σ is an invariant; it is the signature  (σ) of the permutation σ.
• Multilinear maps (also called n-linear maps); bilinear maps.
• Symmetric and alternating multilinear maps.
• A basic property of alternating multilinear maps (Lemma 7.4) and the introduction of
the formula expressing a determinant.
• Definition of a determinant as a multlinear alternating map D : Mn(K) → K such that
D(I) = 1.
• We define the set of algorithms Dn, to compute the determinant of an n × n matrix.
• Laplace expansion according to the ith row; cofactors.
• We prove that the algorithms in Dn compute determinants (Lemma 7.5).
• We prove that all algorithms in Dn compute the same determinant (Theorem 7.6).
• We give an interpretation of determinants as signed volumes.
• We prove that det(A) = det(A> ).
• We prove that det(AB) = det(A) det(B).
• The adjugate Ae of a matrix A.
• Formula for the inverse in terms of the adjugate.
• A matrix A is invertible iff det(A) 6 = 0.
• Solving linear equations using Cramer’s rules.
• Determinant of a linear map.
• The characteristic polynomial of a matrix.
• The Cayley–Hamilton theorem.
• The action of the polynomial ring induced by a linear map on a vector space.
• Permanents.
• Permanents count the number of perfect matchings in bipartite graphs.
7.10. FURTHER READINGS 239
• Computing the permanent is a #P-perfect problem (L. Valiant).
• Permanents count the number of SDRs of sequences of subsets of a given set.
7.10 Further Readings
Thorough expositions of the material covered in Chapter 3–6 and 7 can be found in Strang
[170, 169], Lax [113], Lang [109], Artin [7], Mac Lane and Birkhoff [118], Hoffman and Kunze
[94], Dummit and Foote [54], Bourbaki [25, 26], Van Der Waerden [179], Serre [156], Horn
and Johnson [95], and Bertin [15]. These notions of linear algebra are nicely put to use in
classical geometry, see Berger [11, 12], Tisseron [175] and Dieudonn´e [49].
7.11 Problems
Problem 7.1. Prove that every transposition can be written as a product of basic transpo￾sitions.
Problem 7.2. (1) Given two vectors in R
2 of coordinates (c1−a1, c2−a2) and (b1−a1, b2−a2),
prove that they are linearly dependent iff






a
a
1
2
b1 c1
b2 c2
1 1 1




 
= 0.
(2) Given three vectors in R
3 of coordinates (d1−a1, d2−a2, d3−a3), (c1−a1, c2−a2, c3−a3),
and (b1 − a1, b2 − a2, b3 − a3), prove that they are linearly dependent iff








a
a
1
2
b1 c1 d1
b2 c2 d2
a3 b3 c3 d3
1 1 1 1






 
= 0.
Problem 7.3. Let A be the (m + n) × (m + n) block matrix (over any field K) given by
A =

A1 A2
0 A4

,
where A1 is an m × m matrix, A2 is an m ×n matrix, and A4 is an n×n matrix. Prove that
det(A) = det(A1) det(A4).
Use the above result to prove that if A is an upper triangular n×n matrix, then det(A) =
a11a22 · · · ann.
240 CHAPTER 7. DETERMINANTS
Problem 7.4. Prove that if n ≥ 3, then
det


1 + x1y1 1 + x1y2 · · · 1 + x1yn
1 + x2y1 1 + x2y2 · · · 1 + x2yn
.
.
.
.
.
.
.
.
.
.
.
.
1 + xny1 1 + xny2 · · · 1 + xnyn


= 0.
Problem 7.5. Prove that








1 4 9 16
4 9 16 25
16 25 36 49
9 16 25 36






 = 0.
Problem 7.6. Consider the n × n symmetric matrix
A =


2 5 2 0
1 2 0 0 . . .
. . .
0 0
0 0
0 2 5 2 . . . 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
