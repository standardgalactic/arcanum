X
α∈I
aα.
If we add the righthand side to aβ, using associativity and the definition of an indexed sum,
we get
aβ +

X
α∈Ik
01
aα
 +

X
j∈J

X α∈Ij
aα
 =
 aβ +

X
α∈Ik
01
aα
 +

X
j∈J

X α∈Ij
aα

=

X
α∈Ik1
aα
 +

X
j∈J

X α∈Ij
aα

=
X
k∈K

X α∈Ik
aα
 ,
as claimed.
68 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
If I = (1, . . . , n), we also write P n
i=1 ai
instead of P i∈I
ai
. Since + is associative, Propo￾sition 3.2 shows that the sum P n
i=1 ai
is independent of the grouping of its elements, which
justifies the use the notation a1 + · · · + an (without any parentheses).
If we also assume that our associative binary operation on A is commutative, then we
can show that the sum P i∈I
ai does not depend on the ordering of the index set I.
Proposition 3.3. Given any nonempty set A equipped with an associative and commutative
binary operation +: A × A → A, for any two nonempty finite sequences I and J of distinct
natural numbers such that J is a permutation of I (in other words, the underlying sets of I
and J are identical), for every sequence (ai)i∈I of elements in A, we have
X
α∈I
aα =
X
α∈J
aα.
Proof. We proceed by induction on the number p of elements in I. If p = 1, we have I = J
and the proposition holds trivially.
If p > 1, to simplify notation, assume that I = (1, . . . , p) and that J is a permutation
(i1, . . . , ip) of I. First, assume that 2 ≤ i1 ≤ p−1, let J
0 be the sequence obtained from J by
deleting i1, I
0 be the sequence obtained from I by deleting i1, and let P = (1, 2, . . . , i1−1) and
Q = (i1 + 1, . . . , p−1, p). Observe that the sequence I
0 is the concatenation of the sequences
P and Q. By the induction hypothesis applied to J
0 and I
0 , and then by Proposition 3.2
applied to I
0 and its partition (P, Q), we have
α
X∈J0
aα =
X
α∈I
0
aα =

i1−1
X
i=1
ai
 +

p
X
i=i1+1
ai

.
If we add the lefthand side to ai1
, by definition we get
X
α∈J
aα.
If we add the righthand side to ai1
, we get
ai1 +

i1−1
X
i=1
ai
 +

p
X
i=i1+1
ai

.
Using associativity, we get
ai1 +

i1−1
X
i=1
ai
 +

p
X
i=i1+1
ai
 =
 ai1 +

i1−1
X
i=1
ai
 +

p
X
i=i1+1
ai

,
3.3. INDEXED FAMILIES; THE SUM NOTATION P i∈I
ai 69
then using associativity and commutativity several times (more rigorously, using induction
on i1 − 1), we get

ai1 +

i1−1
X
i=1
ai
 +

p
X
i=i1+1
ai
 =

i1−1
X
i=1
ai
 + ai1 +

p
X
i=i1+1
ai

=
p
X
i=1
ai
,
as claimed.
The cases where i1 = 1 or i1 = p are treated similarly, but in a simpler manner since
either P = () or Q = () (where () denotes the empty sequence).
Having done all this, we can now make sense of sums of the form P i∈I
ai
, for any finite
indexed set I and any family a = (ai)i∈I of elements in A, where A is a set equipped with a
binary operation + which is associative and commutative.
Indeed, since I is finite, it is in bijection with the set {1, . . . , n} for some n ∈ N, and any
total ordering  on I corresponds to a permutation I of {1, . . . , n} (where we identify a
permutation with its image). For any total ordering  on I, we define P i∈I, ai as
X
i∈I,
ai =
X
j∈I
aj
.
Then for any other total ordering  0 on I, we have
i∈
XI, 0
ai =
X
j∈I 0
aj
,
and since I and I 0 are different permutations of {1, . . . , n}, by Proposition 3.3, we have
j
X∈I
aj =
X
j∈I 0
aj
.
Therefore, the sum P i∈I, ai does not depend on the total ordering on I. We define the sum
P
i∈I
ai as the common value P i∈I, ai
for all total orderings  of I.
Here are some examples with A = R:
1. If I = {1, 2, 3}, a = {(1, 2),(2, −3),(3,
√
2)}, then P i∈I
ai = 2 − 3 + √
2 = −1 + √
2.
2. If I = {2, 5, 7}, a = {(2, 2),(5, −3),(7,
√
2)}, then P i∈I
ai = 2 − 3 + √
2 = −1 + √
2.
3. If I = {r, g, b}, a = {(r, 2),(g, −3),(b, 1)}, then P i∈I
ai = 2 − 3 + 1 = 0.
70 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
3.4 Linear Independence, Subspaces
One of the most useful properties of vector spaces is that they possess bases. What this
means is that in every vector space E, there is some set of vectors, {e1, . . . , en}, such that
every vector v ∈ E can be written as a linear combination,
v = λ1e1 + · · · + λnen,
of the ei
, for some scalars, λ1, . . . , λn ∈ R. Furthermore, the n-tuple, (λ1, . . . , λn), as above
is unique.
This description is fine when E has a finite basis, {e1, . . . , en}, but this is not always the
case! For example, the vector space of real polynomials, R[X], does not have a finite basis
but instead it has an infinite basis, namely
1, X, X2
, . . . , Xn
, . . .
One might wonder if it is possible for a vector space to have bases of different sizes, or even
to have a finite basis as well as an infinite basis. We will see later on that this is not possible;
all bases of a vector space have the same number of elements (cardinality), which is called
the dimension of the space. However, we have the following problem: If a vector space has
an infinite basis, {e1, e2, . . . , }, how do we define linear combinations? Do we allow linear
combinations
λ1e1 + λ2e2 + · · ·
with infinitely many nonzero coefficients?
If we allow linear combinations with infinitely many nonzero coefficients, then we have
to make sense of these sums and this can only be done reasonably if we define such a sum
as the limit of the sequence of vectors, s1, s2, . . . , sn, . . ., with s1 = λ1e1 and
sn+1 = sn + λn+1en+1.
But then, how do we define such limits? Well, we have to define some topology on our space,
by means of a norm, a metric or some other mechanism. This can indeed be done and this
is what Banach spaces and Hilbert spaces are all about but this seems to require a lot of
machinery.
A way to avoid limits is to restrict our attention to linear combinations involving only
finitely many vectors. We may have an infinite supply of vectors but we only form linear
combinations involving finitely many nonzero coefficients. Technically, this can be done by
introducing families of finite support. This gives us the ability to manipulate families of
scalars indexed by some fixed infinite set and yet to be treat these families as if they were
finite.
With these motivations in mind, given a set A, recall that an I-indexed family (ai)i∈I
of elements of A (for short, a family) is a function a: I → A, or equivalently a set of pairs
{(i, ai) | i ∈ I}. We agree that when I = ∅, (ai)i∈I = ∅. A family (ai)i∈I is finite if I is finite.
3.4. LINEAR INDEPENDENCE, SUBSPACES 71
Remark: When considering a family (ai)i∈I , there is no reason to assume that I is ordered.
The crucial point is that every element of the family is uniquely indexed by an element of
I. Thus, unless specified otherwise, we do not assume that the elements of an index set are
ordered.
If A is an abelian group with identity 0, we say that a family (ai)i∈I has finite support if
ai = 0 for all i ∈ I − J, where J is a finite subset of I (the support of the family).
Given two disjoint sets I and J, the union of two families (ui)i∈I and (vj )j∈J , denoted as
(ui)i∈I ∪ (vj )j∈J , is the family (wk)k∈(I∪J) defined such that wk = uk if k ∈ I, and wk = vk
if k ∈ J. Given a family (ui)i∈I and any element v, we denote by (ui)i∈I ∪k (v) the family
(wi)i∈I∪{k} defined such that, wi = ui
if i ∈ I, and wk = v, where k is any index such that
k /∈ I. Given a family (ui)i∈I , a subfamily of (ui)i∈I is a family (uj )j∈J where J is any subset
of I.
In this chapter, unless specified otherwise, is assumed that all families of scalars have
finite support.
Definition 3.3. Let E be a vector space. A vector v ∈ E is a linear combination of a family
(ui)i∈I of elements of E iff there is a family (λi)i∈I of scalars in K such that
v =
X
i∈I
λiui
.
When I = ∅, we stipulate that v = 0. (By Proposition 3.3, sums of the form P i∈I
λiui are
well defined.) We say that a family (ui)i∈I is linearly independent iff for every family (λi)i∈I
of scalars in K, X
i∈I
λiui = 0 implies that λi = 0 for all i ∈ I.
Equivalently, a family (ui)i∈I is linearly dependent iff there is some family (λi)i∈I of scalars
in K such that
X
i∈I
λiui = 0 and λj 6 = 0 for some j ∈ I.
We agree that when I = ∅, the family ∅ is linearly independent.
Observe that defining linear combinations for families of vectors rather than for sets of
vectors has the advantage that the vectors being combined need not be distinct. For example,
for I = {1, 2, 3} and the families (u, v, u) and (λ1, λ2, λ1), the linear combination
X
i∈I
λiui = λ1u + λ2v + λ1u
makes sense. Using sets of vectors in the definition of a linear combination does not allow
such linear combinations; this is too restrictive.
72 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
Unravelling Definition 3.3, a family (ui)i∈I is linearly dependent iff either I consists of a
single element, say i, and ui = 0, or |I| ≥ 2 and some uj
in the family can be expressed as
a linear combination of the other vectors in the family. Indeed, in the second case, there is
some family (λi)i∈I of scalars in K such that
X
i∈I
λiui = 0 and λj 6 = 0 for some j ∈ I,
and since |I| ≥ 2, the set I − {j} is nonempty and we get
uj =
X
i∈(I−{j})
−λ
−
j
1λiui
.
Observe that one of the reasons for defining linear dependence for families of vectors
rather than for sets of vectors is that our definition allows multiple occurrences of a vector.
This is important because a matrix may contain identical columns, and we would like to say
that these columns are linearly dependent. The definition of linear dependence for sets does
not allow us to do that.
The above also shows that a family (ui)i∈I is linearly independent iff either I = ∅, or I
consists of a single element i and ui 6 = 0, or |I| ≥ 2 and no vector uj
in the family can be
expressed as a linear combination of the other vectors in the family.
When I is nonempty, if the family (ui)i∈I is linearly independent, note that ui 6 = 0 for
all i ∈ I. Otherwise, if ui = 0 for some i ∈ I, then we get a nontrivial linear dependence
P
i∈I
λiui = 0 by picking any nonzero λi and letting λk = 0 for all k ∈ I with k 6 = i, since
λi0 = 0. If |I| ≥ 2, we must also have ui 6 = uj
for all i, j ∈ I with i 6 = j, since otherwise we
get a nontrivial linear dependence by picking λi = λ and λj = −λ for any nonzero λ, and
letting λk = 0 for all k ∈ I with k 6 = i, j.
Thus, the definition of linear independence implies that a nontrivial linearly independent
family is actually a set. This explains why certain authors choose to define linear indepen￾dence for sets of vectors. The problem with this approach is that linear dependence, which
is the logical negation of linear independence, is then only defined for sets of vectors. How￾ever, as we pointed out earlier, it is really desirable to define linear dependence for families
allowing multiple occurrences of the same vector.
Example 3.2.
1. Any two distinct scalars λ, µ 6 = 0 in K are linearly dependent.
2. In R
3
, the vectors (1, 0, 0), (0, 1, 0), and (0, 0, 1) are linearly independent. See Figure
3.7.
3. In R
4
, the vectors (1, 1, 1, 1), (0, 1, 1, 1), (0, 0, 1, 1), and (0, 0, 0, 1) are linearly indepen￾dent.
3.4. LINEAR INDEPENDENCE, SUBSPACES 73
Figure 3.7: A visual (arrow) depiction of the red vector (1, 0, 0), the green vector (0, 1, 0),
and the blue vector (0, 0, 1) in R
3
.
4. In R
2
, the vectors u = (1, 1), v = (0, 1) and w = (2, 3) are linearly dependent, since
w = 2u + v.
See Figure 3.8.
(2,3)
2u
v
w
Figure 3.8: A visual (arrow) depiction of the pink vector u = (1, 1), the dark purple vector
v = (0, 1), and the vector sum w = 2u + v.
When I is finite, we often assume that it is the set I = {1, 2, . . . , n}. In this case, we
denote the family (ui)i∈I as (u1, . . . , un).
The notion of a subspace of a vector space is defined as follows.
Definition 3.4. Given a vector space E, a subset F of E is a linear subspace (or subspace)
of E iff F is nonempty and λu + µv ∈ F for all u, v ∈ F, and all λ, µ ∈ K.
74 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
It is easy to see that a subspace F of E is indeed a vector space, since the restriction
of +: E × E → E to F × F is indeed a function +: F × F → F, and the restriction of
·: K × E → E to K × F is indeed a function ·: K × F → F.
Since a subspace F is nonempty, if we pick any vector u ∈ F and if we let λ = µ = 0,
then λu + µu = 0u + 0u = 0, so every subspace contains the vector 0.
The following facts also hold. The proof is left as an exercise.
Proposition 3.4.
(1) The intersection of any family (even infinite) of subspaces of a vector space E is a
subspace.
(2) Let F be any subspace of a vector space E. For any nonempty finite index set I,
if (ui)i∈I is any family of vectors ui ∈ F and (λi)i∈I is any family of scalars, then
P
i∈I
λiui ∈ F.
The subspace {0} will be denoted by (0), or even 0 (with a mild abuse of notation).
Example 3.3.
1. In R
2
, the set of vectors u = (x, y) such that
x + y = 0
is the subspace illustrated by Figure 3.9.
Figure 3.9: The subspace x + y = 0 is the line through the origin with slope −1. It consists
of all vectors of the form λ(−1, 1).
2. In R
3
, the set of vectors u = (x, y, z) such that
x + y + z = 0
is the subspace illustrated by Figure 3.10.
3.4. LINEAR INDEPENDENCE, SUBSPACES 75
Figure 3.10: The subspace x+y +z = 0 is the plane through the origin with normal (1, 1, 1).
3. For any n ≥ 0, the set of polynomials f(X) ∈ R[X] of degree at most n is a subspace
of R[X].
4. The set of upper triangular n×n matrices is a subspace of the space of n×n matrices.
Proposition 3.5. Given any vector space E, if S is any nonempty subset of E, then the
smallest subspace h Si (or Span(S)) of E containing S is the set of all (finite) linear combi￾nations of elements from S.
Proof. We prove that the set Span(S) of all linear combinations of elements of S is a subspace
of E, leaving as an exercise the verification that every subspace containing S also contains
Span(S).
First, Span(S) is nonempty since it contains S (which is nonempty). If u =
P i∈I
λiui
and v =
P j∈J µjvj are any two linear combinations in Span(S), for any two scalars λ, µ ∈ K,
λu + µv = λ
X
i∈I
λiui + µ
X
j∈J
µjvj
=
X
i∈I
λλiui +
X
j∈J
µµjvj
=
X
i∈I−J
λλiui +
X
i∈I∩J
(λλi + µµi)ui +
X
j∈J−I
µµjvj
,
which is a linear combination with index set I ∪ J, and thus λu + µv ∈ Span(S), which
proves that Span(S) is a subspace.
One might wonder what happens if we add extra conditions to the coefficients involved
in forming linear combinations. Here are three natural restrictions which turn out to be
important (as usual, we assume that our index sets are finite):
76 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
(1) Consider combinations P i∈I
λiui
for which
X
i∈I
λi = 1.
These are called affine combinations. One should realize that every linear combination
P
i∈I
λiui can be viewed as an affine combination. For example, if k is an index not
in I, if we let J = I ∪ {k}, uk = 0, and λk = 1 −
P i∈I
λi
, then P j∈J
λjuj
is an affine
combination and
X
i∈I
λiui =
X
j∈J
λjuj
.
However, we get new spaces. For example, in R
3
, the set of all affine combinations of
the three vectors e1 = (1, 0, 0), e2 = (0, 1, 0), and e3 = (0, 0, 1), is the plane passing
through these three points. Since it does not contain 0 = (0, 0, 0), it is not a linear
subspace.
(2) Consider combinations P i∈I
λiui
for which
λi ≥ 0, for all i ∈ I.
These are called positive (or conic) combinations. It turns out that positive combina￾tions of families of vectors are cones. They show up naturally in convex optimization.
(3) Consider combinations P i∈I
λiui
for which we require (1) and (2), that is
X
i∈I
λi = 1, and λi ≥ 0 for all i ∈ I.
These are called convex combinations. Given any finite family of vectors, the set of all
convex combinations of these vectors is a convex polyhedron. Convex polyhedra play a
very important role in convex optimization.
Remark: The notion of linear combination can also be defined for infinite index sets I.
To ensure that a sum P i∈I
λiui makes sense, we restrict our attention to families of finite
support.
Definition 3.5. Given any field K, a family of scalars (λi)i∈I has finite support if λi = 0
for all i ∈ I − J, for some finite subset J of I.
If (λi)i∈I is a family of scalars of finite support, for any vector space E over K, for any
(possibly infinite) family (ui)i∈I of vectors ui ∈ E, we define the linear combination P i∈I
λiui
as the finite linear combination P j∈J
λjuj
, where J is any finite subset of I such that λi = 0
for all i ∈ I − J. In general, results stated for finite families also hold for families of finite
support.
3.5. BASES OF A VECTOR SPACE 77
3.5 Bases of a Vector Space
Given a vector space E, given a family (vi)i∈I , the subset V of E consisting of the null vector
0 and of all linear combinations of (vi)i∈I is easily seen to be a subspace of E. The family
(vi)i∈I is an economical way of representing the entire subspace V , but such a family would
be even nicer if it was not redundant. Subspaces having such an “efficient” generating family
(called a basis) play an important role and motivate the following definition.
Definition 3.6. Given a vector space E and a subspace V of E, a family (vi)i∈I of vectors
vi ∈ V spans V or generates V iff for every v ∈ V , there is some family (λi)i∈I of scalars in
K such that
v =
X
i∈I
λivi
.
We also say that the elements of (vi)i∈I are generators of V and that V is spanned by (vi)i∈I ,
or generated by (vi)i∈I . If a subspace V of E is generated by a finite family (vi)i∈I , we say
that V is finitely generated. A family (ui)i∈I that spans V and is linearly independent is
called a basis of V .
Example 3.4.
1. In R
3
, the vectors (1, 0, 0), (0, 1, 0), and (0, 0, 1), illustrated in Figure 3.9, form a basis.
2. The vectors (1, 1, 1, 1),(1, 1, −1, −1),(1, −1, 0, 0),(0, 0, 1, −1) form a basis of R
4 known
as the Haar basis. This basis and its generalization to dimension 2n are crucial in
wavelet theory.
3. In the subspace of polynomials in R[X] of degree at most n, the polynomials 1, X, X2
,
. . . , Xn
form a basis.
4. The Bernstein polynomials  n
k

(1 − X)
n−kXk
for k = 0, . . . , n, also form a basis of
that space. These polynomials play a major role in the theory of spline curves.
The first key result of linear algebra is that every vector space E has a basis. We begin
with a crucial lemma which formalizes the mechanism for building a basis incrementally.
Lemma 3.6. Given a linearly independent family (ui)i∈I of elements of a vector space E, if
v ∈ E is not a linear combination of (ui)i∈I , then the family (ui)i∈I ∪k (v) obtained by adding
v to the family (ui)i∈I is linearly independent (where k /∈ I).
Proof. Assume that µv +
P i∈I
λiui = 0, for any family (λi)i∈I of scalars in K. If µ 6 = 0, then
µ has an inverse (because K is a field), and thus we have v = −
P i∈I
(µ
−1λi)ui
, showing
that v is a linear combination of (ui)i∈I and contradicting the hypothesis. Thus, µ = 0. But
then, we have P i∈I
λiui = 0, and since the family (ui)i∈I is linearly independent, we have
λi = 0 for all i ∈ I.
78 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
The next theorem holds in general, but the proof is more sophisticated for vector spaces
that do not have a finite set of generators. Thus, in this chapter, we only prove the theorem
for finitely generated vector spaces.
Theorem 3.7. Given any finite family S = (ui)i∈I generating a vector space E and any
linearly independent subfamily L = (uj )j∈J of S (where J ⊆ I), there is a basis B of E such
that L ⊆ B ⊆ S.
Proof. Consider the set of linearly independent families B such that L ⊆ B ⊆ S. Since this
set is nonempty and finite, it has some maximal element (that is, a subfamily B = (uh)h∈H
of S with H ⊆ I of maximum cardinality), say B = (uh)h∈H. We claim that B generates
E. Indeed, if B does not generate E, then there is some up ∈ S that is not a linear
combination of vectors in B (since S generates E), with p /∈ H. Then by Lemma 3.6, the
family B0 = (uh)h∈H∪{p} is linearly independent, and since L ⊆ B ⊂ B0 ⊆ S, this contradicts
the maximality of B. Thus, B is a basis of E such that L ⊆ B ⊆ S.
Remark: Theorem 3.7 also holds for vector spaces that are not finitely generated. In this
case, the problem is to guarantee the existence of a maximal linearly independent family B
such that L ⊆ B ⊆ S. The existence of such a maximal family can be shown using Zorn’s
lemma, see Appendix C and the references given there.
A situation where the full generality of Theorem 3.7 is needed is the case of the vector
space R over the field of coefficients Q. The numbers 1 and √
2 are linearly independent
over Q, so according to Theorem 3.7, the linearly independent family L = (1,
√
2) can be
extended to a basis B of R. Since R is uncountable and Q is countable, such a basis must
be uncountable!
The notion of a basis can also be defined in terms of the notion of maximal linearly
independent family and minimal generating family.
Definition 3.7. Let (vi)i∈I be a family of vectors in a vector space E. We say that (vi)i∈I a
maximal linearly independent family of E if it is linearly independent, and if for any vector
w ∈ E, the family (vi)i∈I ∪k {w} obtained by adding w to the family (vi)i∈I is linearly
dependent. We say that (vi)i∈I a minimal generating family of E if it spans E, and if for
any index p ∈ I, the family (vi)i∈I−{p} obtained by removing vp from the family (vi)i∈I does
not span E.
The following proposition giving useful properties characterizing a basis is an immediate
consequence of Lemma 3.6.
Proposition 3.8. Given a vector space E, for any family B = (vi)i∈I of vectors of E, the
following properties are equivalent:
(1) B is a basis of E.
3.5. BASES OF A VECTOR SPACE 79
(2) B is a maximal linearly independent family of E.
(3) B is a minimal generating family of E.
Proof. We will first prove the equivalence of (1) and (2). Assume (1). Since B is a basis, it is
a linearly independent family. We claim that B is a maximal linearly independent family. If
B is not a maximal linearly independent family, then there is some vector w ∈ E such that
the family B0 obtained by adding w to B is linearly independent. However, since B is a basis
of E, the vector w can be expressed as a linear combination of vectors in B, contradicting
the fact that B0 is linearly independent.
Conversely, assume (2). We claim that B spans E. If B does not span E, then there is
some vector w ∈ E which is not a linear combination of vectors in B. By Lemma 3.6, the
family B0 obtained by adding w to B is linearly independent. Since B is a proper subfamily
of B0 , this contradicts the assumption that B is a maximal linearly independent family.
Therefore, B must span E, and since B is also linearly independent, it is a basis of E.
Now we will prove the equivalence of (1) and (3). Again, assume (1). Since B is a basis,
it is a generating family of E. We claim that B is a minimal generating family. If B is not
a minimal generating family, then there is a proper subfamily B0 of B that spans E. Then,
every w ∈ B −B0 can be expressed as a linear combination of vectors from B0 , contradicting
the fact that B is linearly independent.
Conversely, assume (3). We claim that B is linearly independent. If B is not linearly
independent, then some vector w ∈ B can be expressed as a linear combination of vectors
in B0 = B − {w}. Since B generates E, the family B0 also generates E, but B0 is a
proper subfamily of B, contradicting the minimality of B. Since B spans E and is linearly
independent, it is a basis of E.
The second key result of linear algebra is that for any two bases (ui)i∈I and (vj )j∈J of a
vector space E, the index sets I and J have the same cardinality. In particular, if E has a
finite basis of n elements, every basis of E has n elements, and the integer n is called the
dimension of the vector space E.
To prove the second key result, we can use the following replacement lemma due to
Steinitz. This result shows the relationship between finite linearly independent families and
finite families of generators of a vector space. We begin with a version of the lemma which is
a bit informal, but easier to understand than the precise and more formal formulation given
in Proposition 3.10. The technical difficulty has to do with the fact that some of the indices
need to be renamed.
Proposition 3.9. (Replacement lemma, version 1) Given a vector space E, let (u1, . . . , um)
be any finite linearly independent family in E, and let (v1, . . . , vn) be any finite family such
that every ui is a linear combination of (v1, . . . , vn). Then we must have m ≤ n, and there
is a replacement of m of the vectors vj by (u1, . . . , um), such that after renaming some of the
indices of the vj s, the families (u1, . . . , um, vm+1, . . . , vn) and (v1, . . . , vn) generate the same
subspace of E.
80 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
Proof. We proceed by induction on m. When m = 0, the family (u1, . . . , um) is empty, and
the proposition holds trivially. For the induction step, we have a linearly independent family
(u1, . . . , um, um+1). Consider the linearly independent family (u1, . . . , um). By the induction
hypothesis, m ≤ n, and there is a replacement of m of the vectors vj by (u1, . . . , um), such
that after renaming some of the indices of the vs, the families (u1, . . . , um, vm+1, . . . , vn) and
(v1, . . . , vn) generate the same subspace of E. The vector um+1 can also be expressed as a lin￾ear combination of (v1, . . . , vn), and since (u1, . . . , um, vm+1, . . . , vn) and (v1, . . . , vn) generate
the same subspace, um+1 can be expressed as a linear combination of (u1, . . . , um, vm+1, . . .,
vn), say
um+1 =
mX
i=1
λiui +
nX
j=m+1
λjvj
.
We claim that λj 6 = 0 for some j with m + 1 ≤ j ≤ n, which implies that m + 1 ≤ n.
Otherwise, we would have
um+1 =
mX
i=1
λiui
,
a nontrivial linear dependence of the ui
, which is impossible since (u1, . . . , um+1) are linearly
independent.
Therefore, m + 1 ≤ n, and after renaming indices if necessary, we may assume that
λm+1 6 = 0, so we get
vm+1 = −
mX
i=1
(λ
−
m
1
+1λi)ui − λ
−
m
1
+1um+1 −
nX
j=m+2
(λ
−
m
1
+1λj )vj
.
Observe that the families (u1, . . . , um, vm+1, . . . , vn) and (u1, . . . , um+1, vm+2, . . . , vn) generate
the same subspace, since um+1 is a linear combination of (u1, . . . , um, vm+1, . . . , vn) and vm+1
is a linear combination of (u1, . . . , um+1, vm+2, . . . , vn). Since (u1, . . . , um, vm+1, . . . , vn) and
(v1, . . . , vn) generate the same subspace, we conclude that (u1, . . . , um+1, vm+2, . . . , vn) and
and (v1, . . . , vn) generate the same subspace, which concludes the induction hypothesis.
Here is an example illustrating the replacement lemma. Consider sequences (u1, u2, u3)
and (v1, v2, v3, v4, v5), where (u1, u2, u3) is a linearly independent family and with the uis
expressed in terms of the vj s as follows:
u1 = v4 + v5
u2 = v3 + v4 − v5
u3 = v1 + v2 + v3.
From the first equation we get
v4 = u1 − v5,
3.5. BASES OF A VECTOR SPACE 81
and by substituting in the second equation we have
u2 = v3 + v4 − v5 = v3 + u1 − v5 − v5 = u1 + v3 − 2v5.
From the above equation we get
v3 = −u1 + u2 + 2v5,
and so
u3 = v1 + v2 + v3 = v1 + v2 − u1 + u2 + 2v5.
Finally, we get
v1 = u1 − u2 + u3 − v2 − 2v5
Therefore we have
v1 = u1 − u2 + u3 − v2 − 2v5
v3 = −u1 + u2 + 2v5
v4 = u1 − v5,
which shows that (u1, u2, u3, v2, v5) spans the same subspace as (v1, v2, v3, v4, v5). The vectors
(v1, v3, v4) have been replaced by (u1, u2, u3), and the vectors left over are (v2, v5). We can
rename them (v4, v5).
For the sake of completeness, here is a more formal statement of the replacement lemma
(and its proof).
Proposition 3.10. (Replacement lemma, version 2) Given a vector space E, let (ui)i∈I be
any finite linearly independent family in E, where |I| = m, and let (vj )j∈J be any finite family
such that every ui is a linear combination of (vj )j∈J , where |J| = n. Then there exists a set
L and an injection ρ: L → J (a relabeling function) such that L ∩ I = ∅, |L| = n − m, and
the families (ui)i∈I ∪ (vρ(l))l∈L and (vj )j∈J generate the same subspace of E. In particular,
m ≤ n.
Proof. We proceed by induction on |I| = m. When m = 0, the family (ui)i∈I is empty, and
the proposition holds trivially with L = J (ρ is the identity). Assume |I| = m + 1. Consider
the linearly independent family (ui)i∈(I−{p})
, where p is any member of I. By the induction
hypothesis, there exists a set L and an injection ρ: L → J such that L ∩ (I − {p}) = ∅,
|L| = n− m, and the families (ui)i∈(I−{p}) ∪(vρ(l))l∈L and (vj )j∈J generate the same subspace
of E. If p ∈ L, we can replace L by (L − {p}) ∪ {p
0 } where p
0 does not belong to I ∪ L, and
replace ρ by the injection ρ
0 which agrees with ρ on L − {p} and such that ρ
0 (p
0 ) = ρ(p).
Thus, we can always assume that L ∩ I = ∅. Since up is a linear combination of (vj )j∈J
and the families (ui)i∈(I−{p}) ∪ (vρ(l))l∈L and (vj )j∈J generate the same subspace of E, up is
a linear combination of (ui)i∈(I−{p}) ∪ (vρ(l))l∈L. Let
up =
X
i∈(I−{p})
λiui +
X
l∈L
λlvρ(l)
. (1)
82 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
If λl = 0 for all l ∈ L, we have
X
i∈(I−{p})
λiui − up = 0,
contradicting the fact that (ui)i∈I is linearly independent. Thus, λl 6 = 0 for some l ∈ L, say
l = q. Since λq 6 = 0, we have
vρ(q) =
X
i∈(I−{p})
(−λ
−
q
1λi)ui + λ
−
q
1up +
X
l∈(L−{q})
(−λ
−
q
1λl)vρ(l)
. (2)
We claim that the families (ui)i∈(I−{p}) ∪ (vρ(l))l∈L and (ui)i∈I ∪ (vρ(l))l∈(L−{q}) generate the
same subset of E. Indeed, the second family is obtained from the first by replacing vρ(q) by up,
and vice-versa, and up is a linear combination of (ui)i∈(I−{p}) ∪(vρ(l))l∈L, by (1), and vρ(q)
is a
linear combination of (ui)i∈I∪(vρ(l))l∈(L−{q})
, by (2). Thus, the families (ui)i∈I∪(vρ(l))l∈(L−{q})
and (vj )j∈J generate the same subspace of E, and the proposition holds for L − {q} and the
restriction of the injection ρ: L → J to L − {q}, since L∩I = ∅ and |L| = n − m imply that
(L − {q}) ∩ I = ∅ and |L − {q}| = n − (m + 1).
The idea is that m of the vectors vj can be replaced by the linearly independent uis in
such a way that the same subspace is still generated. The purpose of the function ρ: L → J
is to pick n − m elements j1, . . . , jn−m of J and to relabel them l1, . . . , ln−m in such a way
that these new indices do not clash with the indices in I; this way, the vectors vj1
, . . . , vjn−m
who “survive” (i.e. are not replaced) are relabeled vl1
, . . . , vln−m, and the other m vectors vj
with j ∈ J − {j1, . . . , jn−m} are replaced by the ui
. The index set of this new family is I ∪L.
Actually, one can prove that Proposition 3.10 implies Theorem 3.7 when the vector space
is finitely generated. Putting Theorem 3.7 and Proposition 3.10 together, we obtain the
following fundamental theorem.
Theorem 3.11. Let E be a finitely generated vector space. Any family (ui)i∈I generating E
contains a subfamily (uj )j∈J which is a basis of E. Any linearly independent family (ui)i∈I
can be extended to a family (uj )j∈J which is a basis of E (with I ⊆ J). Furthermore, for
every two bases (ui)i∈I and (vj )j∈J of E, we have |I| = |J| = n for some fixed integer n ≥ 0.
Proof. The first part follows immediately by applying Theorem 3.7 with L = ∅ and S =
(ui)i∈I . For the second part, consider the family S
0 = (ui)i∈I ∪(vh)h∈H, where (vh)h∈H is any
finitely generated family generating E, and with I ∩ H = ∅. Then apply Theorem 3.7 to
L = (ui)i∈I and to S
0 . For the last statement, assume that (ui)i∈I and (vj )j∈J are bases of
E. Since (ui)i∈I is linearly independent and (vj )j∈J spans E, Proposition 3.10 implies that
|I| ≤ |J|. A symmetric argument yields |J| ≤ |I|.
Remark: Theorem 3.11 also holds for vector spaces that are not finitely generated. This
can be shown as follows. Let (ui)i∈I be a basis of E, let (vj )j∈J be a generating family of E,
3.5. BASES OF A VECTOR SPACE 83
and assume that I is infinite. For every j ∈ J, let Lj ⊆ I be the finite set
Lj = {i ∈ I | vj =
X
i∈I
λiui
, λi 6 = 0}.
Let L =
S j∈J Lj
. By definition L ⊆ I, and since (ui)i∈I is a basis of E, we must have I = L,
since otherwise (ui)i∈L would be another basis of E, and this would contradict the fact that
(ui)i∈I is linearly independent. Furthermore, J must be infinite, since otherwise, because
the Lj are finite, I would be finite. But then, since I =
S j∈J Lj with J infinite and the Lj
finite, by a standard result of set theory, |I| ≤ |J|. If (vj )j∈J is also a basis, by a symmetric
argument, we obtain |J| ≤ |I|, and thus, |I| = |J| for any two bases (ui)i∈I and (vj )j∈J of E.
Definition 3.8. When a vector space E is not finitely generated, we say that E is of infinite
dimension. The dimension of a finitely generated vector space E is the common dimension
n of all of its bases and is denoted by dim(E).
Clearly, if the field K itself is viewed as a vector space, then every family (a) where a ∈ K
and a 6 = 0 is a basis. Thus dim(K) = 1. Note that dim({0}) = 0.
Definition 3.9. If E is a vector space of dimension n ≥ 1, for any subspace U of E, if
dim(U) = 1, then U is called a line; if dim(U) = 2, then U is called a plane; if dim(U) = n−1,
then U is called a hyperplane. If dim(U) = k, then U is sometimes called a k-plane.
Let (ui)i∈I be a basis of a vector space E. For any vector v ∈ E, since the family (ui)i∈I
generates E, there is a family (λi)i∈I of scalars in K, such that
v =
X
i∈I
λiui
.
A very important fact is that the family (λi)i∈I is unique.
Proposition 3.12. Given a vector space E, let (ui)i∈I be a family of vectors in E. Let v ∈ E,
and assume that v =
P i∈I
λiui. Then the family (λi)i∈I of scalars such that v =
P i∈I
λiui
is unique iff (ui)i∈I is linearly independent.
Proof. First, assume that (ui)i∈I is linearly independent. If (µi)i∈I is another family of scalars
in K such that v =
P i∈I µiui
, then we have
X
i∈I
(λi − µi)ui = 0,
and since (ui)i∈I is linearly independent, we must have λi−µi = 0 for all i ∈ I, that is, λi = µi
for all i ∈ I. The converse is shown by contradiction. If (ui)i∈I was linearly dependent, there
would be a family (µi)i∈I of scalars not all null such that
X
i∈I
µiui = 0
84 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
and µj 6 = 0 for some j ∈ I. But then,
v =
X
i∈I
λiui + 0 = X
i∈I
λiui +
X
i∈I
µiui =
X
i∈I
(λi + µi)ui
,
with λj 6 = λj +µj since µj 6 = 0, contradicting the assumption that (λi)i∈I is the unique family
such that v =
P i∈I
λiui
.
Definition 3.10. If (ui)i∈I is a basis of a vector space E, for any vector v ∈ E, if (xi)i∈I is
the unique family of scalars in K such that
v =
X
i∈I
xiui
,
each xi
is called the component (or coordinate) of index i of v with respect to the basis (ui)i∈I .
Given a field K and any (nonempty) set I, we can form a vector space K(I) which, in
some sense, is the standard vector space of dimension |I|.
Definition 3.11. Given a field K and any (nonempty) set I, let K(I) be the subset of the
cartesian product KI
consisting of all families (λi)i∈I with finite support of scalars in K.
3
We define addition and multiplication by a scalar as follows:
(λi)i∈I + (µi)i∈I = (λi + µi)i∈I ,
and
λ · (µi)i∈I = (λµi)i∈I .
It is immediately verified that addition and multiplication by a scalar are well defined.
Thus, K(I)
is a vector space. Furthermore, because families with finite support are consid￾ered, the family (ei)i∈I of vectors ei
, defined such that (ei)j = 0 if j 6 = i and (ei)i = 1, is
clearly a basis of the vector space K(I)
. When I = {1, . . . , n}, we denote K(I) by Kn
. The
function ι: I → K(I)
, such that ι(i) = ei
for every i ∈ I, is clearly an injection.

When I is a finite set, K(I) = KI
, but this is false when I is infinite. In fact, dim(K(I)
) =
|I|, but dim(KI
) is strictly greater when I is infinite.
3.6 Matrices
In Section 2.1 we introduced informally the notion of a matrix. In this section we define
matrices precisely, and also introduce some operations on matrices. It turns out that matri￾ces form a vector space equipped with a multiplication operation which is associative, but
noncommutative. We will explain in Section 4.1 how matrices can be used to represent linear
maps, defined in the next section.
3Where KI denotes the set of all functions from I to K.
3.6. MATRICES 85
Definition 3.12. If K = R or K = C, an m ×n-matrix over K is a family (ai j )1≤i≤m, 1≤j≤n
of scalars in K, represented by an array


a1 1 a1 2 . . . a1 n
a2 1 a2 2 . . . a2 n
.
.
.
.
.
.
.
.
.
.
.
.
am 1 am 2 . . . am n


In the special case where m = 1, we have a row vector , represented by
(a1 1 · · · a1 n)
and in the special case where n = 1, we have a column vector , represented by


a1 1
.
am
.
.
1

 .
In these last two cases, we usually omit the constant index 1 (first index in case of a row,
second index in case of a column). The set of all m × n-matrices is denoted by Mm,n(K)
or Mm,n. An n × n-matrix is called a square matrix of dimension n. The set of all square
matrices of dimension n is denoted by Mn(K), or Mn.
Remark: As defined, a matrix A = (ai j )1≤i≤m, 1≤j≤n is a family, that is, a function from
{
the indices. Thus, the matrix
1, 2, . . . , m} × {1, 2, . . . , n} to
A
K
can be represented in many different ways as an array, by
. As such, there is no reason to assume an ordering on
adopting different orders for the rows or the columns. However, it is customary (and usually
convenient) to assume the natural ordering on the sets {1, 2, . . . , m} and {1, 2, . . . , n}, and
