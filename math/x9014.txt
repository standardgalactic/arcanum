(Ak
)k∈K+ are identical. However, K is a basis of dimension m so this subspace has dimension
m, and since K+ also has m elements, it must be a basis. Therefore, u
+ = u(θ
j
+
) is a basic
feasible solution.
The above case is the most common one, but other situations may arise. In what follows,
we discuss all eventualities.
Case (A).
We have cj −
P k∈K γk
j
ck ≤ 0 for all j /∈ K. Then it turns out that u is an optimal
solution. Otherwise, we are in Case (B).
Case (B).
We have cj −
P k∈K γk
j
ck > 0 for some j /∈ K (not necessarily unique). There are three
subcases.
Case (B1).
If for some j /∈ K as above we also have γk
j ≤ 0 for all k ∈ K, since uk ≥ 0 for all
k ∈ K, this places no restriction on θ, and the objective function is unbounded above. This
is demonstrated by Example 46.3 with K = {3, 4} and j = 2 since γ{
2
3,4} = (−1, 0).
Case (B2).
There is some index j
+ ∈/ K such that simultaneously
(1) cj+ −
P k∈K γk
j
+
ck > 0, which means that the objective function can potentially be
increased;
(2) There is some k ∈ K such that γ
j
+
k > 0, and for every k ∈ K, if γ
j
+
k > 0 then uk > 0,
which implies that θ
j
+
> 0.
If we pick θ = θ
j
+
where
θ
j
+
= min
uk
γ
j+
k




γk
j
+
> 0, k ∈ K
 > 0,
46.2. THE SIMPLEX ALGORITHM IN GENERAL 1581
then the feasible solution u
+ given by
u
+
i =



ui − θ
j
+
γi
j
+
if i ∈ K
θ
j
+
if i = j
+
0 if i /∈ K ∪ {j
+}
is a vertex of P(A, b). If we pick any index k
− ∈ K such that θ
j
+
= uk− /γk
j+
− , then
K+ = (K − {k
−})∪ {j
+} is a basis for u
+. The vector Aj
+
enters the new basis K+, and the
vector Ak−
leaves the old basis K. This is a pivoting step. The objective function increases
strictly. This is demonstrated by Example 46.2 with K = {3, 4, 5}, j = 1, and k = 4, Then
γ{
1
3,4,5} = (−1, 1, 0), with γ4
1 = 1. Since u = (0, 0, 1, 3, 2), θ
1 =
u
γ
1
4
4
= 3, and the new optimal
solutions becomes u
+ = (3, 0, 1 − 3(−1), 3 − 3(1), 2 − 3(0)) = (3, 0, 4, 0, 2).
Case (B3).
There is some index j /∈ K such that cj −
P k∈K γk
j
ck > 0, and for each of the indices
j /∈ K satisfying the above property we have simultaneously
(1) cj −
P k∈K γk
j
ck > 0, which means that the objective function can potentially be in￾creased;
(2) There is some k ∈ K such that γk
j > 0, and uk = 0, which implies that θ
j = 0.
Consequently, the objective function does not change. In this case, u is a degenerate basic
feasible solution.
We can associate to u
+ = u a new basis K+ as follows: Pick any index j
+ ∈/ K such that
cj+ −
X
k∈K
γk
j
+
ck > 0,
and any index k
− ∈ K such that
γ
j
+
k− > 0,
and let K+ = (K − {k
−}) ∪ {j
+}. As in Case (B2), The vector Aj
+
enters the new basis
K+, and the vector Ak−
leaves the old basis K. This is a pivoting step. However, the
objective function does not change since θ
j+ = 0. This is demonstrated by Example 46.1
with K = {3, 4}, j = 2, and k = 3.
It is easy to prove that in Case (A) the basic feasible solution u is an optimal solution,
and that in Case (B1) the linear program is unbounded. We already proved that in Case
(B2) the vector u
+ and its basis K+ constitutes a basic feasible solution, and the proof in
Case (B3) is similar. For details, see Ciarlet [41] (Chapter 10).
1582 CHAPTER 46. THE SIMPLEX ALGORITHM
It is convenient to reinterpret the various cases considered by introducing the following
sets:
B1 =
n j /∈ K | cj −
X
k∈K
γk
j
ck > 0, max
k∈K
γk
j ≤ 0
o
B2 =
 j /∈ K | cj −
k
X∈K
γk
j
ck > 0, max
k∈K
γk
j > 0, minn
u
γk
k
j



k ∈ K, γk
j > 0
o > 0

B3 =
 j /∈ K | cj −
k
X∈K
γk
j
ck > 0, max
k∈K
γk
j > 0, minn
u
γk
k
j



k ∈ K, γk
j > 0
o = 0 ,
and
B = B1 ∪ B2 ∪ B3 =
n j /∈ K | cj −
X
k∈K
γk
j
ck > 0
o .
Then it is easy to see that the following equivalences hold:
Case (A) ⇐⇒ B = ∅, Case (B) ⇐⇒ B 6 = ∅
Case (B1) ⇐⇒ B1 6 = ∅
Case (B2) ⇐⇒ B2 6 = ∅
Case (B3) ⇐⇒ B3 6 = ∅.
Furthermore, Cases (A) and (B), Cases (B1) and (B3), and Cases (B2) and (B3) are mutually
exclusive, while Cases (B1) and (B2) are not.
If Case (B1) and Case (B2) arise simultaneously, we opt for Case (B1) which says that
the Linear Program (P) is unbounded and terminate the algorithm.
Here are a few remarks about the method.
In Case (B2), which is the path followed by the algorithm most frequently, various choices
have to be made for the index j
+ ∈/ K for which θ
j
+
> 0 (the new index in K+). Similarly,
various choices have to be made for the index k
− ∈ K leaving K, but such choices are
typically less important.
Similarly in Case (B3), various choices have to be made for the new index j
+ ∈/ K going
into K+. In Cases (B2) and (B3), criteria for making such choices are called pivot rules.
Case (B3) only arises when u is a degenerate vertex. But even if u is degenerate, Case
(B2) may arise if uk > 0 whenever γk
j > 0. It may also happen that u is nondegenerate but
as a result of Case (B2), the new vertex u
+ is degenerate because at least two components
uk1 − θ
j
+
γ
j
+
k1
and uk2 − θ
j
+
γ
j
+
k2
vanish for some distinct k1, k2 ∈ K.
Cases (A) and (B1) correspond to situations where the algorithm terminates, and Case
(B2) can only arise a finite number of times during execution of the simplex algorithm, since
the objective function is strictly increased from vertex to vertex and there are only finitely
many vertices. Therefore, if the simplex algorithm is started on any initial basic feasible
solution u0, then one of three mutually exclusive situations may arise:
46.2. THE SIMPLEX ALGORITHM IN GENERAL 1583
(1) There is a finite sequence of occurrences of Case (B2) and/or Case (B3) ending with an
occurrence of Case (A). Then the last vertex produced by the algorithm is an optimal
solution. This is what occurred in Examples 46.1 and 46.2.
(2) There is a finite sequence of occurrences of Case (B2) and/or Case (B3) ending with
an occurrence of Case (B1). We conclude that the problem is unbounded, and thus
has no solution. This is what occurred in Example 46.3.
(3) There is a finite sequence of occurrences of Case (B2) and/or Case (B3), followed by
an infinite sequence of Case (B3). If this occurs, the algorithm visits the some basis
twice. This a phenomenon known as cycling. In this eventually the algorithm fails to
come to a conclusion.
There are examples for which cycling occur, although this is rare in practice. Such an
example is given in Chvatal [40]; see Chapter 3, pages 31-32, for an example with seven
variables and three equations that cycles after six iterations under a certain pivot rule.
The third possibility can be avoided by the choice of a suitable pivot rule. Two of these
rules are Bland’s rule and the lexicographic rule; see Chvatal [40] (Chapter 3, pages 34-38).
Bland’s rule says: choose the smallest of the eligible incoming indices j
+ ∈/ K, and
similarly choose the smallest of the eligible outgoing indices k
− ∈ K.
It can be proven that cycling cannot occur if Bland’s rule is chosen as the pivot rule.
The proof is very technical; see Chvatal [40] (Chapter 3, pages 37-38), Matousek and Gard￾ner [123] (Chapter 5, Theorem 5.8.1), and Papadimitriou and Steiglitz [134] (Section 2.7).
Therefore, assuming that some initial basic feasible solution is provided, and using a suitable
pivot rule (such as Bland’s rule), the simplex algorithm always terminates and either yields
an optimal solution or reports that the linear program is unbounded. Unfortunately, Bland’s
rules is one of the slowest pivot rules.
The choice of a pivot rule affects greatly the number of pivoting steps that the simplex
algorithms goes through. It is not our intention here to explain the various pivot rules.
We simply mention the following rules, referring the reader to Matousek and Gardner [123]
(Chapter 5, Section 5.7) or to the texts cited in Section 44.1.
1. Largest coefficient, or Dantzig’s rule.
2. Largest increase.
3. Steepest edge.
4. Bland’s Rule.
5. Random edge.
1584 CHAPTER 46. THE SIMPLEX ALGORITHM
The steepest edge rule is one of the most popular. The idea is to maximize the ratio
c(u
+ − u)
k
u
+ − uk
.
The random edge rule picks the index j
+ ∈/ K of the entering basis vector uniformly at
random among all eligible indices.
Let us now return to the issue of the initialization of the simplex algorithm. We use the
Linear Program (Pb) introduced during the proof of Theorem 45.7.
Consider a Linear Program (P2)
maximize cx
subject to Ax = b and x ≥ 0,
in standard form where A is an m × n matrix of rank m.
First, observe that since the constraints are equations, we can ensure that b ≥ 0, because
every equation aix = bi where bi < 0 can be replaced by −aix = −bi
. The next step is to
introduce the Linear Program (Pb) in standard form
maximize − (xn+1 + · · · + xn+m)
subject to Ab xb = b and xb ≥ 0,
where Ab and xb are given by
b
A =
￾ A Im
 , xb =


x1
.
.
.
xn+m

 .
Since we assumed that b ≥ 0, the vector xb = (0n, b) is a feasible solution of (Pb), in fact a basic
feasible solutions since the matrix associated with the indices n+ 1, . . . , n+m is the identity
matrix Im. Furthermore, since xi ≥ 0 for all i, the objective function −(xn+1 + · · · + xn+m)
is bounded above by 0.
If we execute the simplex algorithm with a pivot rule that prevents cycling, starting with
the basic feasible solution (0n, d), since the objective function is bounded by 0, the simplex
algorithm terminates with an optimal solution given by some basic feasible solution, say
(u
∗
, w∗
), with u
∗ ∈ R
n and w
∗ ∈ R
m.
As in the proof of Theorem 45.7, for every feasible solution u ∈ P(A, b), the vector (u, 0m)
is an optimal solution of (Pb). Therefore, if w
∗ 6 = 0, then P(A, b) = ∅, since otherwise for
every feasible solution u ∈ P(A, b) the vector (u, 0m) would yield a value of the objective
function −(xn+1 + · · · + xn+m) equal to 0, but (u
∗
, w∗
) yields a strictly negative value since
w
∗ 6 = 0.
46.3. HOW TO PERFORM A PIVOTING STEP EFFICIENTLY 1585
Otherwise, w
∗ = 0, and u
∗
is a feasible solution of (P2). Since (u
∗
, 0m) is a basic
feasible solution of (Pb) the columns corresponding to nonzero components of u
∗ are linearly
independent. Some of the coordinates of u
∗
could be equal to 0, but since A has rank m
we can add columns of A to obtain a basis K∗ associated with u
∗
, and u
∗
is indeed a basic
feasible solution of (P2).
Running the simplex algorithm on the Linear Program Pb to obtain an initial feasible
solution (u0, K0) of the linear program (P2) is called Phase I of the simplex algorithm.
Running the simplex algorithm on the Linear Program (P2) with some initial feasible solution
(u0, K0) is called Phase II of the simplex algorithm. If a feasible solution of the Linear
Program (P2) is readily available then Phase I is skipped. Sometimes, at the end of Phase
I, an optimal solution of (P2) is already obtained.
In summary, we proved the following fact worth recording.
Proposition 46.1. For any Linear Program (P2)
maximize cx
subject to Ax = b and x ≥ 0,
in standard form, where A is an m × n matrix of rank m and b ≥ 0, consider the Linear
Program ( bP) in standard form
maximize − (xn+1 + · · · + xn+m)
subject to Ab xb = b and xb ≥ 0.
The simplex algorithm with a pivot rule that prevents cycling started on the basic feasible
solution xb = (0n, b) of ( bP) terminates with an optimal solution (u
∗
, w∗
).
(1) If w
∗ 6 = 0, then P(A, b) = ∅, that is, the Linear Program (P2) has no feasible solution.
(2) If w
∗ = 0, then P(A, b) 6 = ∅, and u
∗
is a basic feasible solution of (P2) associated with
some basis K.
Proposition 46.1 shows that determining whether the polyhedron P(A, b) defined by a
system of equations Ax = b and inequalities x ≥ 0 is nonempty is decidable. This decision
procedure uses a fail-safe version of the simplex algorithm (that prevents cycling), and the
proof that it always terminates and returns an answer is nontrivial.
46.3 How to Perform a Pivoting Step Efficiently
We now discuss briefly how to perform the computation of (u
+, K+) from a basic feasible
solution (u, K).
In order to avoid applying permutation matrices it is preferable to allow a basis K to be
a sequence of indices, possibly out of order. Thus, for any m × n matrix A (with m ≤ n)
1586 CHAPTER 46. THE SIMPLEX ALGORITHM
and any sequence K = (k1, k2, · · · , km) of m elements with ki ∈ {1, . . . , n}, the matrix AK
denotes the m × m matrix whose ith column is the kith column of A, and similarly for any
vector u ∈ R
n
(resp. any linear form c ∈ (R
n
)
∗
), the vector uK ∈ R
m (the linear form
cK ∈ (R
m)
∗
) is the vector whose ith entry is the kith entry in u (resp. the linear whose ith
entry is the kith entry in c).
For each nonbasic j /∈ K, we have
A
j = γk
j
1
A
k1 + · · · + γk
j
mA
km = AKγK
j
,
so the vector γK
j
is given by γK
j = A
−
K
1Aj
, that is, by solving the system
AKγK
j = A
j
. (∗γ)
To be very precise, since the vector γK
j
depends on K its components should be denoted by
(γK
j
)ki
, but as we said before, to simplify notation we write γk
j
i
instead of (γK
j
)ki
.
In order to decide which case applies ((A), (B1), (B2), (B3)), we need to compute the
numbers cj −
P k∈K γk
j
ck for all j /∈ K. For this, observe that
cj −
X
k∈K
γk
j
ck = cj − cKγK
j = cj − cKA
−
K
1A
j
.
If we write βK = cKA
−
K
1
, then
cj −
X
k∈K
γk
j
ck = cj − βKA
j
,
and we see that βK
> ∈ R
m is the solution of the system βK
> = (A
−
K
1
)
> c
>k
, which means that
βK
> is the solution of the system
A
>KβK
> = c
>K. (∗β)
Remark: Observe that since u is a basis feasible solution of (P), we have uj = 0 for all
j /∈ K, so u is the solution of the equation AKuK = b. As a consequence, the value of the
objective function for u is cu = cKuK = cKA
−
K
1
b. This fact will play a crucial role in Section
47.2 to show that when the simplex algorithm terminates with an optimal solution of the
Linear Program (P), then it also produces an optimal solution of the Dual Linear Program
(D).
Assume that we have a basic feasible solution u, a basis K for u, and that we also have
the matrix AK as well its inverse A
−
K
1
(perhaps implicitly) and also the inverse (A>K)
−1 of
A>K (perhaps implicitly). Here is a description of an iteration step of the simplex algorithm,
following almost exactly Chvatal (Chvatal [40], Chapter 7, Box 7.1).
An Iteration Step of the (Revised) Simplex Method
46.3. HOW TO PERFORM A PIVOTING STEP EFFICIENTLY 1587
Step 1. Compute the numbers cj −
P k∈K γk
j
ck = cj − βKAj
for all j /∈ K, and for this,
compute βK
> as the solution of the system
A
>KβK
> = c
>K.
If cj − βKAj ≤ 0 for all j /∈ K, stop and return the optimal solution u (Case (A)).
Step 2. If Case (B) arises, use a pivot rule to determine which index j
+ ∈/ K should enter
the new basis K+ (the condition cj+ − βKAj
+
> 0 should hold).
Step 3. Compute maxk∈K γ
j
+
k
. For this, solve the linear system
AKγ
j
+
K = A
j
+
.
Step 4. If maxk∈K γ
j
+
k ≤ 0, then stop and report that Linear Program (P) is unbounded
(Case (B1)).
Step 5. If maxk∈K γ
j
+
k > 0, use the ratios uk/γj
+
k
for all k ∈ K such that γ
j
+
k > 0 to
compute θ
j
+
, and use a pivot rule to determine which index k
− ∈ K such that θ
j
+
= uk− /γk
j+
−
should leave K (Case (B2)).
If maxk∈K γ
j
+
k = 0, then use a pivot rule to determine which index k
− for which γ
j
+
k− > 0
should leave the basis K (Case (B3)).
Step 6. Update u, K, and AK, to u
+ and K+, and AK+ . During this step, given the
basis K specified by the sequence K = (k1, . . . , k` , . . . , km), with k
− = k` , then K+ is the
sequence obtained by replacing k` by the incoming index j
+, so K+ = (k1, . . . , j+, . . . , km)
with j
+ in the ` th slot.
The vector u is easily updated. To compute AK+ from AK we take advantage of the fact
that AK and AK+ only differ by a single column, namely the ` th column Aj
+
, which is given
by the linear combination
A
j
+
= AKγ
j
+
K .
To simplify notation, denote γ
j
+
K by γ, and recall that k
− = k` . If K = (k1, . . . , km), then
AK = [Ak1
· · · Ak−
· · · Aim], and since AK+ is the result of replacing the ` th column Ak−
of
AK by the column Aj
+
, we have
AK+ = [A
k1
· · · A
j
+
· · · A
im] = [A
k1
· · · AKγ · · · A
im] = AKE(γ),
where E(γ) is the following invertible matrix obtained from the identity matrix Im by re￾placing its ` th column by γ:
E(γ) =


1 γ1
.
.
.
.
.
.
1 γ` −1
γ`
γ` +1 1
.
.
.
.
.
.
γm 1


.
1588 CHAPTER 46. THE SIMPLEX ALGORITHM
Since γ` = γ
j
+
k− > 0, the matrix E(γ) is invertible, and it is easy to check that its inverse is
given by
E(γ)
−1 =


1 −γ
−1
`
γ1
.
.
.
.
.
.
1 −γ
−1
`
γ` −1
γ
−1
`
−γ
−1
`
γ` +1 1
.
.
.
.
.
.
−γ`
−1
γm 1


,
which is very cheap to compute. We also have
A
−
K
1
+ = E(γ)
−1A
−
K
1
.
Consequently, if AK and A
−
K
1
are available, then AK+ and A
−
K
1
+ can be computed cheaply
in terms of AK and A
−
K
1
and matrices of the form E(γ). Then the systems (∗γ) to find the
vectors γK
j
can be solved cheaply.
Since
A
>K+ = E(γ)
> A
>K
and
(A
>K+ )
−1 = (A
>K)
−1
(E(γ)
> )
−1
,
the matrices A>K+ and (A>K+ )
−1
can also be computed cheaply from A>K, (A>K)
−1
, and matrices
of the form E(γ)
> . Thus the systems (∗β) to find the linear forms βK can also be solved
cheaply.
A matrix of the form E(γ) is called an eta matrix ; see Chvatal [40] (Chapter 7). We
showed that the matrix AKs obtained after s steps of the simplex algorithm can be written
as
AKs = AKs−1Es
for some eta matrix Es, so Ak
s can be written as the product
AKs = E1E2 · · · Es
of s eta matrices. Such a factorization is called an eta factorization. The eta factorization
can be used to either invert AKs or to solve a system of the form AKs γ = Aj
+
iteratively.
Which method is more efficient depends on the sparsity of the Ei
.
In summary, there are cheap methods for finding the next basic feasible solution (u
+, K+)
from (u, K). We simply wanted to give the reader a flavor of these techniques. We refer the
reader to texts on linear programming for detailed presentations of methods for implementing
efficiently the simplex method. In particular, the revised simplex method is presented in
Chvatal [40], Papadimitriou and Steiglitz [134], Bertsimas and Tsitsiklis [21], and Vanderbei
[181].
46.4. THE SIMPLEX ALGORITHM USING TABLEAUX 1589
46.4 The Simplex Algorithm Using Tableaux
We now describe a formalism for presenting the simplex algorithm, namely (full) tableaux .
This is the traditional formalism used in all books, modulo minor variations. A particularly
nice feature of the tableau formalism is that the update of a tableau can be performed using
elementary row operations identical to the operations used during the reduction of a matrix
to row reduced echelon form (rref). What differs is the criterion for the choice of the pivot.
Since the quantities cj −cKγK
j
play a crucial role in determining which column Aj
should
come into the basis, the notation cj
is used to denote cj − cKγK
j
, which is called the reduced
cost of the variable xj
. The reduced costs actually depend on K so to be very precise we
should denote them by (cK)j
, but to simplify notation we write cj
instead of (cK)j
. We will
see shortly how (cK+ )i
is computed in terms of (cK)i
.
Observe that the data needed to execute the next step of the simplex algorithm are
(1) The current basic solution uK and its basis K = (k1, . . . , km).
(2) The reduced costs cj = cj − cKA
−
K
1Aj = cj − cKγK
j
, for all j /∈ K.
(3) The vectors γK
j = (γk
j
i
)
m
i=1 for all j /∈ K, that allow us to express each Aj as AKγK
j
.
All this information can be packed into a (m + 1) × (n + 1) matrix called a (full) tableau
organized as follows:
cKuK c1 · · · cj
· · · cn
uk1 γ1
1
· · · γ1
j
· · · γ1
n
.
.
.
.
.
.
.
.
.
.
.
.
ukm γm
1
· · · γm
j
· · · γm
n
It is convenient to think as the first row as Row 0, and of the first column as Column 0.
Row 0 contains the current value of the objective function and the reduced costs. Column
0, except for its top entry, contains the components of the current basic solution uK, and
the remaining columns, except for their top entry, contain the vectors γK
j
. Observe that
the γK
j
corresponding to indices j in K constitute a permutation of the identity matrix
Im. The entry γ
j
+
k− is called the pivot element. A tableau together with the new basis
K+ = (K − {k
−}) ∪ {j
+} contains all the data needed to compute the new uK+ , the new
γK
j
+ , and the new reduced costs (cK+ )j
.
If we define the m × n matrix Γ as the matrix Γ = [γK
1
· · · γK
n
] whose jth column is γK
j
,
and c as the row vector c = (c1 · · · cn), then the above tableau is denoted concisely by
cKuK c
uK Γ
1590 CHAPTER 46. THE SIMPLEX ALGORITHM
We now show that the update of a tableau can be performed using elementary row
operations identical to the operations used during the reduction of a matrix to row reduced
echelon form (rref).
If K = (k1, . . . , km), j
+ is the index of the incoming basis vector, k
− = k` is the index
of the column leaving the basis, and if K+ = (k1, . . . , k` −1, j+, k` +1, . . . , km), since AK+ =
AKE(γ
j
+
K ), the new columns γK
j
+ are computed in terms of the old columns γK
j
using (∗γ)
and the equations
γK
j
+ = A
−
K
1
+ A
j = E(γ
j
+
K )
−1A
−
K
1A
j = E(γ
j
+
K )
−1
γK
j
.
Consequently, the matrix Γ+ is given in terms of Γ by
Γ
+ = E(γ
j
+
K )
−1Γ.
But the matrix E(γ
j
+
K )
−1
is of the form
E(γ
j
+
K )
−1 =


1 −(γ
j
+
k− )
−1γ
j
+
k1
.
.
.
.
.
.
1 −(γ
j
+
k− )
−1γk
j
`
+
−1
(γ
j
+
k− )
−1
−(γ
j
+
k− )
−1γ
j
+
k` +1
1
.
.
.
.
.
.
−(γ
j
+
