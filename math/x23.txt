
1/2
.
See Figure 9.6.
3. For E = C
2
,
k
(u1, u2)k = |u1 + iu2| + |u1 − iu2|.
The reader should check that they satisfy all the axioms of a norm.
Some work is required to show the triangle inequality for the ` p
-norm.
328 CHAPTER 9. VECTOR NORMS AND MATRIX NORMS
K1 K0.5 0 0.5 1
K1
K0.5
0.5
1
Figure 9.2: The top figure is {x ∈ R
2
| kxk 2 ≤ 1}, while the bottom figure is {x ∈ R
3
|
k
xk 2 ≤ 1}.
Proposition 9.1. If E = C
n or E = R
n
, for every real number p ≥ 1, the ` p
-norm is indeed
a norm.
Proof. The cases p = 1 and p = ∞ are easy and left to the reader. If p > 1, then let q > 1
such that
1
p
+
1
q
= 1.
We will make use of the following fact: for all α, β ∈ R, if α, β ≥ 0, then
αβ ≤
α
p
p
+
β
q
q
. (∗)
To prove the above inequality, we use the fact that the exponential function t 7→ e
t
satisfies
the following convexity inequality:
e
θx+(1−θ)y ≤ θex + (1 − θ)e
y
,
9.1. NORMED VECTOR SPACES 329
K1 K0.5 0 0.5 1
K1
K0.5
0.5
1
Figure 9.3: The top figure is {x ∈ R
2
| kxk ∞ ≤ 1}, while the bottom figure is {x ∈ R
3
|
k
xk ∞ ≤ 1}.
for all x, y ∈ R and all θ with 0 ≤ θ ≤ 1.
Since the case αβ = 0 is trivial, let us assume that α > 0 and β > 0. If we replace θ by
1/p, x by p log α and y by q log β, then we get
e
1
p
p log α+ 1
q
q log β ≤
1
p
e
p log α +
1
q
e
q log β
,
which simplifies to
αβ ≤
α
p
p
+
β
q
q
,
as claimed.
We will now prove that for any two vectors u, v ∈ E, (where E is of dimension n), we
have
nX
i=1
|uivi
| ≤ kuk p
k
vk q
. (∗∗)
330 CHAPTER 9. VECTOR NORMS AND MATRIX NORMS
K1 K0.5 0 0.5 1
K1
K0.5
0.5
1
Figure 9.4: The relationships between the closed unit balls from the ` 1
-norm, the Euclidean
norm, and the sup-norm.
Since the above is trivial if u = 0 or v = 0, let us assume that u 6 = 0 and v 6 = 0. Then
Inequality (∗) with α = |ui
|/ k uk p
and β = |vi
|/ k vk q
yields
|uivi
|
k
uk p
k
vk q
≤
|ui
|
p
p k uk
p
p
+
|vi
|
q
q k vk
q
q
,
for i = 1, . . . , n, and by summing up these inequalities, we get
nX
i=1
|uivi
| ≤ kuk p
k
vk q
,
as claimed. To finish the proof, we simply have to prove that property (N3) holds, since
(N1) and (N2) are clear. For i = 1, . . . , n, we can write
(|ui
| + |vi
|)
p = |ui
|(|ui
| + |vi
|)
p−1 + |vi
|(|ui
| + |vi
|)
p−1
,
9.1. NORMED VECTOR SPACES 331
Figure 9.5: The unit closed unit ball {(u1, u2) ∈ R
2
| k(u1, u2)k ≤ 1}, where k (u1, u2)k =
|u1| + 2|u2|.
so that by summing up these equations we get
nX
i=1
(|ui
| + |vi
|)
p =
nX
i=1
|ui
|(|ui
| + |vi
|)
p−1 +
nX
i=1
|vi
|(|ui
| + |vi
|)
p−1
,
and using Inequality (∗∗), with V ∈ E where Vi = (|ui
| + |vi
|)
p−1
, we get
nX
i=1
(|ui
| + |vi
|)
p ≤ kuk p
k
V k q + k vk p
k
V k q = (k uk p + k vk p
)

nX
i=1
(|ui
| + |vi
|)
(p−1)q

1/q
.
However, 1/p + 1/q = 1 implies pq = p + q, that is, (p − 1)q = p, so we have
nX
i=1
(|ui
| + |vi
|)
p ≤ (k uk p + k vk p
)

nX
i=1
(|ui
| + |vi
|)
p

1/q
,
which yields

nX
i=1
(|ui
| + |vi
|)
p

1−1/q
=

nX
i=1
(|ui
| + |vi
|)
p

1/p
≤ kuk p + k vk p
.
Since |ui + vi
| ≤ |ui
| + |vi
|, the above implies the triangle inequality k u + vk p ≤ kuk p + k vk p
,
as claimed.
For p > 1 and 1/p + 1/q = 1, the inequality
nX
i=1
|uivi
| ≤ 
nX
i=1
|ui
|
p

1/p nX
i=1
|vi
|
q

1/q
332 CHAPTER 9. VECTOR NORMS AND MATRIX NORMS
Figure 9.6: The unit closed unit ball {(u1, u2) ∈ R
2
| k(u1, u2)k ≤ 1}, where k (u1, u2)k =
￾
(u1 + u2)
2 + u
2
1

1/2
.
is known as H¨older’s inequality. For p = 2, it is the Cauchy–Schwarz inequality.
Actually, if we define the Hermitian inner product h−, −i on C
n by
h
u, vi =
nX
i=1
uivi
,
where u = (u1, . . . , un) and v = (v1, . . . , vn), then
|hu, vi| ≤
nX
i=1
|uivi
| =
nX
i=1
|uivi
|,
so H¨older’s inequality implies the following inequalities.
Corollary 9.2. (H¨older’s inequalities) For any real numbers p, q, such that p, q ≥ 1 and
1
p
+
1
q
= 1,
(with q = +∞ if p = 1 and p = +∞ if q = 1), we have the inequalities
nX
i=1
|uivi
| ≤ 
nX
i=1
|ui
|
p

1/p nX
i=1
|vi
|
q

1/q
and
|hu, vi| ≤ kuk p
k
vk q
, u, v ∈ C
n
.
9.1. NORMED VECTOR SPACES 333
For p = 2, this is the standard Cauchy–Schwarz inequality. The triangle inequality for
the ` p
-norm,

nX
i=1
(|ui + vi
|)
p

1/p
≤

nX
i=1
|ui
|
p

1/p
+

nX
i=1
|vi
|
p

1/p
,
is known as Minkowski’s inequality.
When we restrict the Hermitian inner product to real vectors, u, v ∈ R
n
, we get the
Euclidean inner product
h
u, vi =
nX
i=1
uivi
.
It is very useful to observe that if we represent (as usual) u = (u1, . . . , un) and v = (v1, . . . , vn)
(in R
n
) by column vectors, then their Euclidean inner product is given by
h
u, vi = u
> v = v
> u,
and when u, v ∈ C
n
, their Hermitian inner product is given by
h
u, vi = v
∗u = u
∗v.
In particular, when u = v, in the complex case we get
k
uk
2
2 = u
∗u,
and in the real case this becomes
k
uk
2
2 = u
> u.
As convenient as these notations are, we still recommend that you do not abuse them; the
notation h u, vi is more intrinsic and still “works” when our vector space is infinite dimen￾sional.
Remark: If 0 < p < 1, then x 7→ kxk p
is not a norm because the triangle inequality
fails. For example, consider x = (2, 0) and y = (0, 2). Then x + y = (2, 2), and we have
k
xk p = (2p + 0p
)
1/p = 2, k yk p = (0p + 2p
)
1/p = 2, and k x + yk p = (2p + 2p
)
1/p = 2(p+1)/p
.
Thus
k
x + yk p = 2(p+1)/p
, k xk p + k yk p = 4 = 22
.
Since 0 < p < 1, we have 2p < p + 1, that is, (p + 1)/p > 2, so 2(p+1)/p > 2
2 = 4, and the
triangle inequality k x + yk p ≤ kxk p + k yk p
fails.
Observe that
k
(1/2)xk p = (1/2) k xk p = k (1/2)yk p = (1/2) k yk p = 1, k (1/2)(x + y)k
p = 21/p
,
and since p < 1, we have 21/p > 2, so
k
(1/2)(x + y)k
p = 21/p > 2 = (1/2) k xk p + (1/2) k yk p
,
334 CHAPTER 9. VECTOR NORMS AND MATRIX NORMS
and the map x 7→ kxk p
is not convex.
For p = 0, for any x ∈ R
n
, we have
k
xk 0 = |{i ∈ {1, . . . , n} | xi 6 = 0}|,
the number of nonzero components of x. The map x 7→ kxk 0
is not a norm this time because
Axiom (N2) fails. For example,
k
(1, 0)k 0 = k (10, 0)k 0 = 1 6 = 10 = 10 k (1, 0)k 0
.
The map x 7→ kxk 0
is also not convex. For example,
k
(1/2)(2, 2)k 0 = k (1, 1)k 0 = 2,
and
k
(2, 0)k 0 = k (0, 2)k 0 = 1,
but
k
(1/2)(2, 2)k 0 = 2 > 1 = (1/2) k (2, 0)k 0 + (1/2) k (0, 2)k 0
.
Nevertheless, the “zero-norm” x 7→ kxk 0
is used in machine learning as a regularizing
term which encourages sparsity, namely increases the number of zero components of the
vector x.
The following proposition is easy to show.
Proposition 9.3. The following inequalities hold for all x ∈ R
n
(or x ∈ C
n
):
k
xk ∞ ≤ kxk 1 ≤ nk xk ∞,
k
xk ∞ ≤ kxk 2 ≤
√
nk xk ∞,
k
xk 2 ≤ kxk 1 ≤
√
nk xk 2.
Proposition 9.3 is actually a special case of a very important result: in a finite-dimensional
vector space, any two norms are equivalent.
Definition 9.2. Given any (real or complex) vector space E, two norms k k a
and k k b
are
equivalent iff there exists some positive reals C1, C2 > 0, such that
k
uk a ≤ C1 k uk b
and k uk b ≤ C2 k uk a
, for all u ∈ E.
There is an illuminating interpretation of Definition 9.2 in terms of open balls. For any
radius ρ > 0 and any x ∈ E, consider the open a-ball of center x and radius ρ (with respect
the norm k k a
),
Ba(x, ρ) = {z ∈ E | kz − xk a < ρ}.
9.1. NORMED VECTOR SPACES 335
We claim that there is some open b-ball Bb(x, r) of radius r > 0 and center x,
Bb(x, r) = {z ∈ E | kz − xk n < r},
such that
Bb(x, r) ⊆ Ba(x, ρ).
Indeed, if we pick r = ρ/C1, for any z ∈ E, if k z − xk b < ρ/C1, then
k
z − xk a ≤ C1 k z − xk b < C1(ρ/C1) = ρ,
which means that
Bb(x, ρ/C1) ⊆ Ba(x, ρ).
Similarly, for any radius ρ > 0 and any x ∈ E, we have
Ba(x, ρ/C2) ⊆ Bb(x, ρ).
Now given a normed vector space (E, k k ), a subset U of E is said to be open (with
respect to the norm k k ) if either U = ∅ or if for every x ∈ U, there is some open ball B(x, ρ)
(for some ρ > 0) such that B(x, ρ) ⊆ U.
The collection U of open sets defined by the norm k k is called the topology on E induced
by the norm k k . What we showed above regarding the containments of open a-balls and
open b-balls immediately implies that two equivalent norms induce the same topology on E.
This is the reason why the notion of equivalent norms is important.
Given any norm k k on a vector space of dimension n, for any basis (e1, . . . , en) of E,
observe that for any vector x = x1e1 + · · · + xnen, we have
k
xk = k x1e1 + · · · + xnenk ≤ |x1| ke1k + · · · + |xn| kenk ≤ C(|x1| + · · · + |xn|) = C k xk 1
,
with C = max1≤i≤n k eik and with the norm k xk 1
defined as
k
xk 1 = k x1e1 + · · · + xnenk = |x1| + · · · + |xn|.
The above implies that
| kuk − kvk | ≤ ku − vk ≤ C k u − vk 1
,
and this implies the following corollary.
Corollary 9.4. For any norm u 7→ kuk on a finite-dimensional (complex or real) vector
space E, the map u 7→ kuk is continuous with respect to the norm k k 1
.
336 CHAPTER 9. VECTOR NORMS AND MATRIX NORMS
Let S1
n−1
be the unit sphere with respect to the norm k k 1
, namely
S1
n−1 = {x ∈ E | kxk 1 = 1}.
Now S1
n−1
is a closed and bounded subset of a finite-dimensional vector space, so by Heine–
Borel (or equivalently, by Bolzano–Weiertrass), S1
n−1
is compact. On the other hand, it
is a well known result of analysis that any continuous real-valued function on a nonempty
compact set has a minimum and a maximum, and that they are achieved. Using these facts,
we can prove the following important theorem:
Theorem 9.5. If E is any real or complex vector space of finite dimension, then any two
norms on E are equivalent.
Proof. It is enough to prove that any norm k k is equivalent to the 1-norm. We already proved
that the function x 7→ kxk is continuous with respect to the norm k k 1
, and we observed that
the unit sphere S1
n−1
is compact. Now we just recalled that because the function f : x 7→ kxk
is continuous and because S1
n−1
is compact, the function f has a minimum m and a maximum
M, and because k xk is never zero on S1
n−1
, we must have m > 0. Consequently, we just
proved that if k xk 1 = 1, then
0 < m ≤ kxk ≤ M,
so for any x ∈ E with x 6 = 0, we get
m ≤ kx/ k xk 1
k ≤ M,
which implies
m k xk 1 ≤ kxk ≤ M k xk 1
.
Since the above inequality holds trivially if x = 0, we just proved that k k and k k 1
are
equivalent, as claimed.
Remark: Let P be a n × n symmetric positive definite matrix. It is immediately verified
that the map x 7→ kxk P
given by
k
xk P = (x
> P x)
1/2
is a norm on R
n
called a quadratic norm. Using some convex analysis (the L¨owner–John
ellipsoid), it can be shown that any norm k k on R
n
can be approximated by a quadratic
norm in the sense that there is a quadratic norm k k P
such that
k
xk P ≤ kxk ≤ √
n k xk P
for all x ∈ R
n
;
see Boyd and Vandenberghe [29], Section 8.4.1.
Next we will consider norms on matrices.
9.2. MATRIX NORMS 337
9.2 Matrix Norms
For simplicity of exposition, we will consider the vector spaces Mn(R) and Mn(C) of square
n × n matrices. Most results also hold for the spaces Mm,n(R) and Mm,n(C) of rectangular
m × n matrices. Since n × n matrices can be multiplied, the idea behind matrix norms is
that they should behave “well” with respect to matrix multiplication.
Definition 9.3. A matrix norm k k on the space of square n × n matrices in Mn(K), with
K = R or K = C, is a norm on the vector space Mn(K), with the additional property called
submultiplicativity that
k
ABk ≤ kAk k Bk ,
for all A, B ∈ Mn(K). A norm on matrices satisfying the above property is often called a
submultiplicative matrix norm.
Since I
2 = I, from k Ik = k I
2k ≤ kIk
2
, we get k Ik ≥ 1, for every matrix norm.
Before giving examples of matrix norms, we need to review some basic definitions about
matrices. Given any matrix A = (aij ) ∈ Mm,n(C), the conjugate A of A is the matrix such
that
Aij = aij , 1 ≤ i ≤ m, 1 ≤ j ≤ n.
The transpose of A is the n × m matrix A> such that
A
>ij = aji, 1 ≤ i ≤ m, 1 ≤ j ≤ n.
The adjoint of A is the n × m matrix A∗
such that
A
∗ = (A> ) = (A)
> .
When A is a real matrix, A∗ = A> . A matrix A ∈ Mn(C) is Hermitian if
A
∗ = A.
If A is a real matrix (A ∈ Mn(R)), we say that A is symmetric if
A
> = A.
A matrix A ∈ Mn(C) is normal if
AA∗ = A
∗A,
and if A is a real matrix, it is normal if
AA> = A
> A.
A matrix U ∈ Mn(C) is unitary if
UU∗ = U
∗U = I.
338 CHAPTER 9. VECTOR NORMS AND MATRIX NORMS
A real matrix Q ∈ Mn(R) is orthogonal if
QQ> = Q
> Q = I.
Given any matrix A = (aij ) ∈ Mn(C), the trace tr(A) of A is the sum of its diagonal
elements
tr(A) = a11 + · · · + ann.
It is easy to show that the trace is a linear map, so that
tr(λA) = λtr(A)
and
tr(A + B) = tr(A) + tr(B).
Moreover, if A is an m × n matrix and B is an n × m matrix, it is not hard to show that
tr(AB) = tr(BA).
We also review eigenvalues and eigenvectors. We content ourselves with definition in￾volving matrices. A more general treatment will be given later on (see Chapter 15).
Definition 9.4. Given any square matrix A ∈ Mn(C), a complex number λ ∈ C is an
eigenvalue of A if there is some nonzero vector u ∈ C
n
, such that
Au = λu.
If λ is an eigenvalue of A, then the nonzero vectors u ∈ C
n
such that Au = λu are called
eigenvectors of A associated with λ; together with the zero vector, these eigenvectors form a
subspace of C
n denoted by Eλ(A), and called the eigenspace associated with λ.
Remark: Note that Definition 9.4 requires an eigenvector to be nonzero. A somewhat
unfortunate consequence of this requirement is that the set of eigenvectors is not a subspace,
since the zero vector is missing! On the positive side, whenever eigenvectors are involved,
there is no need to say that they are nonzero. In contrast, even if we allow 0 to be an
eigenvector, in order for a scalar λ to be an eigenvalue, there must be a nonzero vector u
such that Au = λu. Without this restriction, since A0 = λ0 = 0 for all λ, every scalar would
be an eigenvector, which would make the definition of an eigenvalue trivial and useless. The
fact that eigenvectors are nonzero is implicitly used in all the arguments involving them,
so it seems preferable (but perhaps not as elegant) to stipulate that eigenvectors should be
nonzero.
If A is a square real matrix A ∈ Mn(R), then we restrict Definition 9.4 to real eigenvalues
λ ∈ R and real eigenvectors. However, it should be noted that although every complex
9.2. MATRIX NORMS 339
matrix always has at least some complex eigenvalue, a real matrix may not have any real
eigenvalues. For example, the matrix
A =

0
1 0
−1

has the complex eigenvalues i and −i, but no real eigenvalues. Thus, typically even for real
matrices, we consider complex eigenvalues.
Observe that λ ∈ C is an eigenvalue of A
• iff Au = λu for some nonzero vector u ∈ C
n
• iff (λI − A)u = 0
• iff the matrix λI − A defines a linear map which has a nonzero kernel, that is,
• iff λI − A not invertible.
However, from Proposition 7.10, λI − A is not invertible iff
det(λI − A) = 0.
Now det(λI − A) is a polynomial of degree n in the indeterminate λ, in fact, of the form
λ
n − tr(A)λ
n−1 + · · · + (−1)n
det(A).
Thus we see that the eigenvalues of A are the zeros (also called roots) of the above polyno￾mial. Since every complex polynomial of degree n has exactly n roots, counted with their
multiplicity, we have the following definition:
Definition 9.5. Given any square n × n matrix A ∈ Mn(C), the polynomial
det(λI − A) = λ
n − tr(A)λ
n−1 + · · · + (−1)n
det(A)
is called the characteristic polynomial of A. The n (not necessarily distinct) roots λ1, . . . , λn
of the characteristic polynomial are all the eigenvalues of A and constitute the spectrum of
A. We let
ρ(A) = max
1≤i≤n
|λi
|
be the largest modulus of the eigenvalues of A, called the spectral radius of A.
Since the eigenvalues λ1, . . . , λn of A are the zeros of the polynomial
det(λI − A) = λ
n − tr(A)λ
n−1 + · · · + (−1)n
det(A),
we deduce (see Section 15.1 for details) that
tr(A) = λ1 + · · · + λn
det(A) = λ1 · · · λn.
340 CHAPTER 9. VECTOR NORMS AND MATRIX NORMS
Proposition 9.6. For any matrix norm k k on Mn(C) and for any square n × n matrix
A ∈ Mn(C), we have
ρ(A) ≤ kAk .
Proof. Let λ be some eigenvalue of A for which |λ| is maximum, that is, such that |λ| = ρ(A).
If u (6= 0) is any eigenvector associated with λ and if U is the n × n matrix whose columns
are all u, then Au = λu implies
AU = λU,
and since
|λ| kUk = k λUk = k AUk ≤ kAk k Uk
and U 6 = 0, we have k Uk 6 = 0, and get
ρ(A) = |λ| ≤ kAk ,
as claimed.
Proposition 9.6 also holds for any real matrix norm k k on Mn(R) but the proof is more
subtle and requires the notion of induced norm. We prove it after giving Definition 9.7.
It turns out that if A is a real n × n symmetric matrix, then the eigenvalues of A are all
real and there is some orthogonal matrix Q such that
A = Qdiag(λ1, . . . , λn)Q
> ,
where diag(λ1, . . . , λn) denotes the matrix whose only nonzero entries (if any) are its diagonal
entries, which are the (real) eigenvalues of A. Similarly, if A is a complex n × n Hermitian
matrix, then the eigenvalues of A are all real and there is some unitary matrix U such that
A = Udiag(λ1, . . . , λn)U
∗
,
where diag(λ1, . . . , λn) denotes the matrix whose only nonzero entries (if any) are its diagonal
entries, which are the (real) eigenvalues of A. See Chapter 17 for the proof of these results.
We now return to matrix norms. We begin with the so-called Frobenius norm, which is
just the norm k k 2
on C
n
2
, where the n × n matrix A is viewed as the vector obtained by
concatenating together the rows (or the columns) of A. The reader should check that for
any n × n complex matrix A = (aij ),

X
n
i,j=1
|aij |
2

1/2
=
p tr(A∗A) = p tr(AA∗
).
Definition 9.6. The Frobenius norm k k F
is defined so that for every square n × n matrix
A ∈ Mn(C),
k
Ak F =

X
n
i,j=1
|aij |
2

1/2
=
p tr(AA∗
) = p tr(A∗A).
9.2. MATRIX NORMS 341
The following proposition show that the Frobenius norm is a matrix norm satisfying other
nice properties.
Proposition 9.7. The Frobenius norm k k F
on Mn(C) satisfies the following properties:
(1) It is a matrix norm; that is, k ABk F ≤ kAk F
k Bk F
, for all A, B ∈ Mn(C).
(2) It is unitarily invariant, which means that for all unitary matrices U, V , we have
k
Ak F = k UAk F = k AV k F = k UAV k F
.
(3) p ρ(A∗A) ≤ kAk F ≤
√
n
p ρ(A∗A), for all A ∈ Mn(C).
Proof. (1) The only property that requires a proof is the fact k ABk F ≤ kAk F
k Bk F
. This
follows from the Cauchy–Schwarz inequality:
k
ABk 2
F =
nX
i,j=1




nX
k=1
aikbkj

 

2
≤
nX
i,j=1

nX
h=1
|aih|
2

nX
k=1
|bkj |
2

=

X
n
i,h=1
|aih|
2
 X
n
k,j=1
|bkj |
2
 = k Ak
2
F
k Bk
2
F
.
(2) We have
k
Ak
2
F = tr(AA∗
) = tr(AV V ∗A
∗
) = tr(AV (AV )
∗
) = k AV k 2
F
,
and
k
Ak
2
F = tr(A
∗A) = tr(A
∗U
∗UA) = k UAk 2
F
.
The identity
k
Ak F = k UAV k F
follows from the previous two.
(3) It is shown in Section 15.1 that the trace of a matrix is equal to the sum of its
eigenvalues. Furthermore, A∗A is symmetric positive semidefinite (which means that its
eigenvalues are nonnegative), so ρ(A∗A) is the largest eigenvalue of A∗A and
ρ(A
∗A) ≤ tr(A
∗A) ≤ nρ(A
∗A),
which yields (3) by taking square roots.
Remark: The Frobenius norm is also known as the Hilbert-Schmidt norm or the Schur
norm. So many famous names associated with such a simple thing!
342 CHAPTER 9. VECTOR NORMS AND MATRIX NORMS
9.3 Subordinate Norms
We now give another method for obtaining matrix norms using subordinate norms. First we
need a proposition that shows that in a finite-dimensional space, the linear map induced by
a matrix is bounded, and thus continuous.
Proposition 9.8. For every norm k k on C
n
(or R
n
), for every matrix A ∈ Mn(C) (or
A ∈ Mn(R)), there is a real constant CA ≥ 0, such that
k
Auk ≤ CA k uk ,
for every vector u ∈ C
n
(or u ∈ R
n
if A is real).
Proof. For every basis (e1, . . . , en) of C
n
(or R
n
), for every vector u = u1e1 + · · · + unen, we
have
k
Auk = k u1A(e1) + · · · + unA(en)k
≤ |u1| kA(e1)k + · · · + |un| kA(en)k
≤ C1(|u1| + · · · + |un|) = C1 k uk 1
,
where C1 = max1≤i≤n k A(ei)k . By Theorem 9.5, the norms k k and k k 1
are equivalent, so
there is some constant C2 > 0 so that k uk 1 ≤ C2 k uk for all u, which implies that
k
Auk ≤ CA k uk ,
where CA = C1C2.
Proposition 9.8 says that every linear map on a finite-dimensional space is bounded. This
implies that every linear map on a finite-dimensional space is continuous. Actually, it is not
hard to show that a linear map on a normed vector space E is bounded iff it is continuous,
regardless of the dimension of E.
Proposition 9.8 implies that for every matrix A ∈ Mn(C) (or A ∈ Mn(R)),
x
sup
∈Cn
x6=0
k
k
Ax
xk
k
≤ CA.
Since k λuk = |λ| kuk , for every nonzero vector x, we have
k
Axk
k
xk
=
k
xk k A(x/ k xk )k
k
xk
= k A(x/ k xk )k ,
which implies that
sup
x∈Cn
x6=0
k
Axk
k
xk
= sup
x∈Cn
k
xk =1
k
Axk .
9.3. SUBORDINATE NORMS 343
Similarly
sup
x∈Rn
x6=0
k
Axk
k
xk
= sup
x∈Rn
k
xk =1
k
Axk .
The above considerations justify the following definition.
Definition 9.7. If k k is any norm on C
n
, we define the function k k op on Mn(C) by
k
