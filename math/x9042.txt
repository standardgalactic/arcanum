2psf
p + q
,
2qsf
p + q
,
psf + qsf
p + q
.
These values can be avoided by requiring the strict inequality
max 
2psf
p + q
,
2qsf
p + q

< Î½.
Then the following corollary holds.
Theorem 54.3. For every optimal solution (w, b, Î·, , Î¾) of Problem (SVMs2
0 ) with w 6 = 0
and Î· > 0, if
max{2pf /(p + q), 2qf /(p + q)} < Î½ < min{2p/(p + q), 2q/(p + q)},
then some ui0 and some vj0
is a support vector.
Proof. We proceed by contradiction. Suppose that for every optimal solution with w 6 = 0
and Î· > 0 no ui
is a blue support vector or no vj
is a red support vector. Since Î½ <
min{2p/(p + q), 2q/(p + q)}, Proposition 54.2 holds, so there is another optimal solution.
But since the critical values of Î½ are avoided, the proof of Proposition 54.2 shows that the
value of the objective function for this new optimal solution is strictly smaller than the
original optimal value, a contradiction.
54.7. EXISTENCE OF SUPPORT VECTORS FOR (SVMs2
0 ) 1977
We also have the following proposition that gives a sufficient condition implying that Î·
and b can be found in terms of an optimal solution (Î», Âµ) of the dual.
Proposition 54.4. If (w, b, Î·, , Î¾) is an optimal solution of Problem (SVMs2
0 ) with w 6 = 0
and Î· > 0, if
max{2pf /(p + q), 2qf /(p + q)} < Î½ < min{2p/(p + q), 2q/(p + q)},
then Î· and b can always be determined from an optimal solution (Î», Âµ) of the dual in terms
of a single support vector.
Proof. By Theorem 54.3 some ui0 and some vj0
is a support vector. As we already explained,
Problem (SVMs2
0 ) satisfies the conditions for having a zero duality gap. Therefore, for
optimal solutions we have
L(w, , Î¾, b, Î·, Î», Âµ, Î±, Î²) = G(Î», Âµ, Î±, Î²),
which means that
1
2
w
> w âˆ’ Î½Î· +
1
p + q

p
X
i=1

i +
q
X
j=1
Î¾j
 = âˆ’
1
2
ï¿¾
Î»
> Âµ
>
 X
> X

Âµ
Î»

,
and since
w = âˆ’X

Âµ
Î»

,
we get
1
p + q

p
X
i=1

i +
q
X
j=1
Î¾j
 = Î½Î· âˆ’
ï¿¾ Î»
> Âµ
>
 X
> X

Âµ
Î»

. (âˆ—)
Let KÎ» = {i âˆˆ {1, . . . , p} | Î»i = Ks} and KÂµ = {j âˆˆ {1, . . . , q} | Âµj = Ks}. By definition,
pf = |KÎ»| and qf = |KÂµ| (here we assuming that Ks = 1/(p + q)). By complementary
slackness the following equations are active:
w
> ui âˆ’ b = Î· âˆ’  i i âˆˆ KÎ»
âˆ’w
> vj + b = Î· âˆ’ Î¾j j âˆˆ KÂµ.
But (âˆ—) can be written as
1
p + q

X
iâˆˆKÎ»

i +
X
jâˆˆKÂµ
Î¾j
 = Î½Î· âˆ’
ï¿¾ Î»
> Âµ
>
 X
> X

Âµ
Î»

, (âˆ—âˆ—)
and since

i = Î· âˆ’ w
> ui + b i âˆˆ KÎ»
Î¾j = Î· + w
> vj âˆ’ b j âˆˆ KÂµ,
1978 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
by substituting in the Equation (âˆ—âˆ—) we get

Î½ âˆ’
pf + qf
p + q

Î· =
pf âˆ’ qf
p + q
b +
1
p + q
w
>

X
iâˆˆKÂµ
vj âˆ’
i
XâˆˆKÎ»
ui
 +
ï¿¾ Î»
> Âµ
>
 X
> X

Âµ
Î»

.
We also know that w
> ui0 âˆ’ b = Î· and âˆ’w
> vj0 + b = Î· for some i0 and some j0. In the first
case b = âˆ’Î· + w
> ui0
, and by substituting b in the above equation we get the equation

Î½ âˆ’
pf + qf
p + q

Î· = âˆ’
pf âˆ’ qf
p + q
Î· +
pf âˆ’ qf
p + q
w
> ui0 +
1
p + q
w
>

X
iâˆˆKÂµ
vj âˆ’
i
XâˆˆKÎ»
ui

+
ï¿¾ Î»
> Âµ
>
 X
> X

Âµ
Î»

,
that is,

Î½ âˆ’
p
2
+
qf
q

Î· =
pf âˆ’ qf
p + q
w
> ui0 +
1
p + q
w
>

X
iâˆˆKÂµ
vj âˆ’
i
XâˆˆKÎ»
ui

+
ï¿¾ Î»
> Âµ
>
 X
> X

Âµ
Î»

.
In the second case b = Î· + w
> vj0
, and we get the equation

Î½ âˆ’
pf + qf
p + q

Î· =
pf âˆ’ qf
p + q
Î· +
pf âˆ’ qf
p + q
w
> vj0 +
1
p + q
w
>

X
iâˆˆKÂµ
vj âˆ’
i
XâˆˆKÎ»
ui

+
ï¿¾ Î»
> Âµ
>
 X
> X

Âµ
Î»

,
that is,

Î½ âˆ’
p
2
+
pf
q

Î· =
pf âˆ’ qf
p + q
w
> vj0 +
1
p + q
w
>

X
iâˆˆKÂµ
vj âˆ’
i
XâˆˆKÎ»
ui

+
ï¿¾ Î»
> Âµ
>
 X
> X

Âµ
Î»

.
We need to choose Î½ such that 2pf /(p + q) âˆ’ Î½ 6 = 0 and 2qf /(p + q) âˆ’ Î½ 6 = 0. Since by
Proposition 54.1, we have max{2pf /(p + q), 2qf /(p + q)} â‰¤ Î½, it suffices to pick Î½ such that
max{2pf /(p + q), 2qf /(p + q)} < Î½. If this condition is satisfied we can solve for Î·, and then
we find b from either b = âˆ’Î· + w
> ui0 or b = Î· + w
> vj0
.
Remark: Of course the hypotheses of the proposition imply that w
> ui0âˆ’b = Î· and âˆ’w
> vj0+
b = Î· for some i0 and some j0. Thus we can also compute b and Î· using the formulae
b =
w
> (ui0 + vj0
)
2
Î· =
w
> (ui0 âˆ’ vj0
)
2
.
54.7. EXISTENCE OF SUPPORT VECTORS FOR (SVMs2
0 ) 1979
The interest of Proposition 54.4 lies in the fact that it allows us to compute b and Î· knowing
only a single support vector.
In practice we can only find support vectors of type 1 so Proposition 54.4 is useful if we
can only find some blue support vector of type 1 or some red support vector of type 1.
As earlier, if we define IÎ» and IÂµ as
IÎ» = {i âˆˆ {1, . . . , p} | 0 < Î»i < Ks}
IÂµ = {j âˆˆ {1, . . . , q} | 0 < Âµj < Ks},
then we have the following cases to compute Î· and b.
(1) If IÎ» 6 = âˆ… and IÂµ 6 = âˆ…, then
b = w
>
ï£«
ï£­

X
iâˆˆIÎ»
ui
 /|IÎ»| +

X
jâˆˆIÂµ
vj
 /|IÂµ|
ï£¶
ï£¸ /2
Î· = w
>
ï£«
ï£­

X
iâˆˆIÎ»
ui
 /|IÎ»| âˆ’  X
jâˆˆIÂµ
vj
 /|IÂµ|
ï£¶
ï£¸ /2.
(2) If IÎ» 6 = âˆ… and IÂµ = âˆ…, then
b = âˆ’Î· + w
>

X
iâˆˆIÎ»
ui
 /|IÎ»|
((p + q)Î½ âˆ’ 2qf )Î· = (pf âˆ’ qf )w
>

X
iâˆˆIÎ»
ui
 /|IÎ»| + w
>

X
iâˆˆKÂµ
vj âˆ’
i
XâˆˆKÎ»
ui

+ (p + q)
ï¿¾ Î»
> Âµ
>
 X
> X

Âµ
Î»

.
(3) If IÎ» = âˆ… and IÂµ 6 = âˆ…, then
b = Î· + w
>

X
jâˆˆIÂµ
vj
 /|IÂµ|
((p + q)Î½ âˆ’ 2pf )Î· = (pf âˆ’ qf )w
>

X
jâˆˆIÂµ
vj
 /|IÂµ| + w
>

X
iâˆˆKÂµ
vj âˆ’
i
XâˆˆKÎ»
ui

+ (p + q)
ï¿¾ Î»
> Âµ
>
 X
> X

Âµ
Î»

.
The above formulae correspond to Ks = 1/(p + q). In general we have to replace the
rightmost (p + q) by 1/Ks.
1980 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
We have examples where there is a single support vector of type 1 and Î½ = 2qf /(p + q),
so the above method fails. Curiously, perturbing Î½ slightly yields a solution with some blue
support vector of type 1 and some red support vector of type 1, and so we have not yet
found an example where the above method succeeds with a single support vector of type 1.
This suggests to conduct some perturbation analysis but it appears to be nontrivial.
Among its advantages, the support vector machinery is conducive to finding interesting
statistical bounds in terms of the VC dimension, a notion invented by Vapnik and Cherï¿¾novenkis. We will not go into this here and instead refer the reader to Vapnik [182] (especially,
Chapter 4 and Chapters 9-13).
54.8 Solving SVM (SVMs2
0
) Using ADMM
In order to solve (SVMs2
0 ) using ADMM we need to write the matrix corresponding to the
constraints in equational form,
p
X
i=1
Î»i âˆ’
q
X
j=1
Âµj = 0
p
X
i=1
Î»i +
q
X
j=1
Âµj âˆ’ Î³ = Km
Î»i + Î±i = Ks, i = 1, . . . , p
Âµj + Î²j = Ks, j = 1, . . . , q,
with Km = (p + q)KsÎ½. This is the (p + q + 2) Ã— (2(p + q) + 1) matrix A given by
A =
ï£«
ï£¬ï£¬ï£¬ï£¬ï£­
1
>p âˆ’1
>q
0
>p
0
>q
0
1
>p 1
>q
0
>p
0
>q âˆ’1
Ip 0p,q Ip 0p,q 0p
0q,p Iq 0q,p Iq 0q
ï£¶
ï£·ï£·ï£·ï£·ï£¸
.
Observe the remarkable analogy with the matrix arising in Î½-regression in Section 56.3,
except that p = q = m and that âˆ’1 is replaced by +1. We leave it as an exercise to prove
that A has rank p + q + 2. The right-hand side is
c =
ï£«
ï£­
0
Km
Ks1p+q
ï£¶
ï£¸ .
The symmetric positive semidefinite (p+q)Ã—(p+q) matrix P defining the quadratic functional
is
P = X
> X, with X =
ï¿¾ âˆ’u1 Â· Â· Â· âˆ’up v1 Â· Â· Â· vq
 ,
54.8. SOLVING SVM (SVMs2
0 ) USING ADMM 1981
and
q = 0p+q.
Since there are 2(p + q) + 1 Lagrange multipliers (Î», Âµ, Î±, Î², Î³), the (p + q) Ã— (p + q) matrix
X> X must be augmented with zeroâ€™s to make it a (2(p + q) + 1) Ã— (2(p + q) + 1) matrix Pa
given by
Pa =

X> X 0p+q,p+q+1
0p+q+1,p+q 0p+q+1,p+q+1
,
and similarly q is augmented with zeros as the vector qa = 02(p+q)+1.
As we mentioned in Section 54.5, since Î· â‰¥ 0 for an optimal solution, we can drop the
constraint Î· â‰¥ 0 from the primal problem. In this case there are 2(p+q) Lagrange multipliers
(Î», Âµ, Î±, Î²). It is easy to see that the objective function of the dual is unchanged and the set
of constraints is
p
X
i=1
Î»i âˆ’
q
X
j=1
Âµj = 0
p
X
i=1
Î»i +
q
X
j=1
Âµj = Km
Î»i + Î±i = Ks, i = 1, . . . , p
Âµj + Î²j = Ks, j = 1, . . . , q,
with Km = (p + q)KsÎ½. The constraint matrix corresponding to this system of equations is
the (p + q + 2) Ã— 2(p + q) matrix A2 given by
A2 =
ï£«
ï£¬ï£¬ï£¬ï£¬ï£­
1
>p âˆ’1
>q
0
>p
0
>q
1
>p 1
>q
0
>p
0
>q
Ip 0p,q Ip 0p,q
0q,p Iq 0q,p Iq
ï£¶
ï£·ï£·ï£·ï£·ï£¸
.
We leave it as an exercise to prove that A2 has rank p + q + 2. The right-hand side is
c2 =
ï£«
ï£­
0
Km
Ks1p+q
ï£¶
ï£¸ .
The symmetric positive semidefinite (p+q)Ã—(p+q) matrix P defining the quadratic functional
is
P = X
> X, with X =
ï¿¾ âˆ’u1 Â· Â· Â· âˆ’up v1 Â· Â· Â· vq
 ,
and
q = 0p+q.
1982 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
Since there are 2(p + q) Lagrange multipliers the (p + q) Ã— (p + q) matrix X> X must be
augmented with zeroâ€™s to make it a 2(p + q) Ã— 2(p + q) matrix P2a given by
P2a =

X> X 0p+q,p+q
0p+q,p+q 0p+q,p+q

,
and similarly q is augmented with zeros as the vector q2a = 02(p+q)
.
The Matlab programs implementing the above method are given in Appendix B, Section
B.2. We ran our program on two sets of 30 points each generated at random using the
following code which calls the function runSVMs2pbv3:
rho = 10;
u16 = 10.1*randn(2,30)+7 ;
v16 = -10.1*randn(2,30)-7;
[~,~,~,~,~,~,w3] = runSVMs2pbv3(0.37,rho,u16,v16,1/60)
We picked K = 1/60 and various values of Î½ starting with Î½ = 0.37, which appears to be
the smallest value for which the method converges; see Figure 54.11.
In this example, pf = 10, qf = 11, pm = 12, qm = 12. The quadratic solver converged
after 8121 steps to reach primal and dual residuals smaller than 10âˆ’10
.
Reducing Î½ below Î½ = 0.37 has the effect that pf , qf , pm, qm decrease but the following
situation arises. Shrinking Î· a little bit has the effect that pf = 9, qf = 10, pm = 10, qm = 11.
Then max{pf , qf } = min{pm, qm} = 10, so the only possible value for Î½ is Î½ = 20/60 =
1/3 = 0.3333333 Â· Â· Â· . When we run our program with Î½ = 1/3, it returns a value of Î· less
than 10âˆ’13 and a value of w whose components are also less than 10âˆ’13. This is probably
due to numerical precision. Values of Î½ less than 1/3 cause the same problem. It appears
that the geometry of the problem constrains the values of pf , qf , pm, qm in such a way that
it has no solution other than w = 0 and Î· = 0.
Figure 54.12 shows the result of running the program with Î½ = 0.51. We have pf =
15, qf = 16, pm = 16, qm = 16. Interestingly, for Î½ = 0.5, we run into the singular situation
where there is only one support vector and Î½ = 2pf /(p + q).
54.8. SOLVING SVM (SVMs2
0 ) USING ADMM 1983
-30 -20 -10 0 10 20 30 40
-30
-20
-10
0
10
20
30
Figure 54.11: Running (SVMs2
0 ) on two sets of 30 points; Î½ = 0.37.
-30 -20 -10 0 10 20 30 40
-30
-20
-10
0
10
20
30
Figure 54.13: Running (SVMs2
0 ) on two sets of 30 points; Î½ = 0.71.
Next Figure 54.13 shows the result of running the program with Î½ = 0.71. We have
1984 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
-30 -20 -10 0 10 20 30 40
-30
-20
-10
0
10
20
30
Figure 54.12: Running (SVMs2
0 ) on two sets of 30 points; Î½ = 0.51.
pf = 21, qf = 21, pm = 22, qm = 23. Interestingly, for Î½ = 0.7, we run into the singular
situation where there are no support vectors.
For our next to the last run, Figure 54.14 shows the result of running the program with
Î½ = 0.95. We have pf = 28, qf = 28, pm = 29, qm = 29.
Figure 54.15 shows the result of running the program with Î½ = 0.97. We have pf =
29, qf = 29, pm = 30, qm = 30, which shows that the largest margin has been achieved.
However, after 80000 iterations the dual residual is less than 10âˆ’12 but the primal residual is
approximately 10âˆ’4
(our tolerance for convergence is 10âˆ’10, which is quite high). Nevertheless
the result is visually very good.
54.9 Soft Margin Support Vector Machines; (SVMs3)
In this section we consider a variation of Problem (SVMs2
0 ) by adding the term (1/2)b
2
to
the objective function. The result is that in minimizing the Lagrangian to find the dual
function G, not just w but also b is determined and Î· is determined under a mild condition
on Î½. We also suppress the constraint Î· â‰¥ 0 which turns out to be redundant.
54.9. SOFT MARGIN SUPPORT VECTOR MACHINES; (SVMs3) 1985
-30 -20 -10 0 10 20 30 40
-30
-20
-10
0
10
20
30
Figure 54.14: Running (SVMs2
0 ) on two sets of 30 points; Î½ = 0.95.
Soft margin SVM (SVMs3):
minimize
1
2
w
> w +
1
2
b
2 + (p + q)Ks
 âˆ’Î½Î· +
p +
1
q
ï¿¾

>
Î¾
>
 1p+q

subject to
w
> ui âˆ’ b â‰¥ Î· âˆ’  i
, i â‰¥ 0 i = 1, . . . , p
âˆ’ w
> vj + b â‰¥ Î· âˆ’ Î¾j
, Î¾j â‰¥ 0 j = 1, . . . , q.
To simplify the presentation we assume that Ks = 1/(p + q). When writing a computer
program it is more convenient to assume that Ks is arbitrary. In this case, Î½ needs to be
replaced by (p + q)KsÎ½ in all the formulae.
The Lagrangian L(w, , Î¾, b, Î·, Î», Âµ, Î±, Î²) with Î», Î± âˆˆ R
p
+, Âµ, Î² âˆˆ R
q
+ is given by
L(w, , Î¾, b, Î·, Î», Âµ, Î±, Î²) = 1
2
w
> w + w
> X

Âµ
Î»

+
b
2
2
âˆ’ Î½Î· + Ks(
> 1p + Î¾
> 1q) âˆ’ 
> (Î» + Î±)
âˆ’ Î¾
> (Âµ + Î²) + b(1
>p Î» âˆ’ 1
>q Âµ) + Î·(1
>p Î» + 1
>q Âµ)
=
1
2
w
> w + w
> X

Âµ
Î»

+
b
2
2
+ b(1
>p Î» âˆ’ 1
>q Âµ) + Î·(1
>p Î» + 1
>q Âµ âˆ’ Î½)
+ 
> (Ks1p âˆ’ (Î» + Î±)) + Î¾
> (Ks1q âˆ’ (Âµ + Î²)).
1986 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
-30 -20 -10 0 10 20 30 40
-30
-20
-10
0
10
20
30
Figure 54.15: Running (SVMs2
0 ) on two sets of 30 points; Î½ = 0.97.
To find the dual function G(Î», Âµ, Î±, Î²), we minimize L(w, , Î¾, b, Î·, Î», Âµ, Î±, Î²) with respect
to w, , Î¾, b, and Î·. Since the Lagrangian is convex and (w, , Î¾, b, Î·) âˆˆ R
n Ã—R
p Ã—R
q Ã—RÃ—R,
a convex open set, by Theorem 40.13, the Lagrangian has a minimum in (w, , Î¾, b, Î·) iff
âˆ‡Lw,,Î¾,b,Î· = 0, so we compute its gradient with respect to w, , Î¾, b, Î·, and we get
âˆ‡Lw,,Î¾,b,Î· =
ï£«
ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­
X

Âµ
Î»

+ w
Ks1p âˆ’ (Î» + Î±)
Ks1q âˆ’ (Âµ + Î²)
1
b
>
p
+
Î»
1
+
>
p Î»
1
>q
âˆ’
Âµ
1
âˆ’
>
q Âµ
Î½
ï£¶
ï£·ï£·ï£·ï£·ï£·ï£·ï£¸
.
By setting âˆ‡Lw,,Î¾,b,Î· = 0 we get the equations
w = âˆ’X

Âµ
Î»

(âˆ—w)
Î» + Î± = Ks1p
Âµ + Î² = Ks1q
1
>p Î» + 1
>q Âµ = Î½,
and
b = âˆ’(1
>p Î» âˆ’ 1
>q Âµ). (âˆ—b)
54.9. SOFT MARGIN SUPPORT VECTOR MACHINES; (SVMs3) 1987
The second and third equations are equivalent to the box constraints
0 â‰¤ Î»i
, Âµj â‰¤ Ks, i = 1, . . . , p, j = 1, . . . , q.
Since we assumed that the primal problem has an optimal solution with w 6 = 0, we have
X

Âµ
Î»

6
= 0.
Plugging back w from (âˆ—w) and b from (âˆ—b) into the Lagrangian, we get
G(Î», Âµ, Î±, Î²) = 1
2
ï¿¾
Î»
> Âµ
>
 X
> X

Âµ
Î»

âˆ’
ï¿¾ Î»
> Âµ
>
 X
> X

Âµ
Î»

+
2
1
b
2 âˆ’ b
2
= âˆ’
1
2
ï¿¾
Î»
> Âµ
>
 X
> X

Âµ
Î»

âˆ’
2
1
b
2
= âˆ’
1
2
ï¿¾
Î»
> Âµ
>

 X
> X +

1p1
>p âˆ’1p1
>q
âˆ’1q1
>p 1q1
>q
  Âµ
Î»

,
so the dual function is independent of Î±, Î² and is given by
G(Î», Âµ) = âˆ’
1
2
ï¿¾
Î»
> Âµ
>

 X
> X +

1p1
>p âˆ’1p1
>q
âˆ’1q1
>p 1q1
>q
  Âµ
Î»

.
The dual program is given by
maximize âˆ’
1
2
ï¿¾
Î»
> Âµ
>

 X
> X +

1p1
>p âˆ’1p1
>q
âˆ’1q1
>p 1q1
>q
  Âµ
Î»

subject to
p
X
i=1
Î»i +
q
X
j=1
Âµj = Î½
0 â‰¤ Î»i â‰¤ Ks, i = 1, . . . , p
0 â‰¤ Âµj â‰¤ Ks, j = 1, . . . , q.
Finally, the dual program is equivalent to the following minimization program:
Dual of the Soft margin SVM (SVMs3):
