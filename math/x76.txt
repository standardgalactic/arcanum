32.3. NOETHERIAN RINGS AND HILBERTâ€™S BASIS THEOREM 1137
Each bi
is an ideal in Z/30Z. Furthermore
Z/30Z = (Z/30Z)/(2Z/30Z) Ã— (Z/30Z)/(3Z/30Z) Ã— (Z/30Z)/(5Z/30Z),
where
e1 = (1, 0, 0) â†’ 15, e2 = (0, 1, 0) â†’ 10, e3 = (0, 0, 1) â†’ 6,
since
152
= 15, 102
= 10, 6
2
= 6
15 10 = 15 6 = 10 6 = 0, 15 + 10 + 6 = 1.
Note that 15 corresponds to 1 âˆˆ (Z/30Z)/(2Z/30Z), 10 corresponds to
1 âˆˆ (Z/30Z)/(3Z/30Z), while 6 corresponds to 1 âˆˆ (Z/30Z)/(5Z/30Z).
32.3 Noetherian Rings and Hilbertâ€™s Basis Theorem
Given a (commutative) ring A (with unit element 1), an ideal A âŠ† A is said to be finitely
generated if there exists a finite set {a1, . . . , an} of elements from A so that
A = (a1, . . . , an) = {Î»1a1 + Â· Â· Â· + Î»nan | Î»i âˆˆ A, 1 â‰¤ i â‰¤ n}.
If K is a field, it turns out that every polynomial ideal A in K[X1, . . . , Xm] is finitely
generated. This fact due to Hilbert and known as Hilbertâ€™s basis theorem, has very important
consequences. For example, in algebraic geometry, one is interested in the zero locus of a set
of polyomial equations, i.e., the set, V (P), of n-tuples (Î»1, . . . , Î»n) âˆˆ Kn
so that
Pi(Î»1, . . . , Î»n) = 0
for all polynomials Pi(X1, . . . , Xn) in some given family, P = (Pi)iâˆˆI . However, it is clear
that
V (P) = V (A),
where A is the ideal generated by P. Then, Hilbertâ€™s basis theorem says that V (A) is actually
defined by a finite number of polynomials (any set of generators of A), even if P is infinite.
The property that every ideal in a ring is finitely generated is equivalent to other natural
properties, one of which is the so-called ascending chain condition, abbreviated a.c.c. Before
proving Hilbertâ€™s basis theorem, we explore the equivalence of these conditions.
Definition 32.4. Let A be a commutative ring with unit 1. We say that A satisfies the
ascending chain condition, for short, the a.c.c, if for every ascending chain of ideals
A1 âŠ† A2 âŠ† Â· Â· Â· âŠ† Ai âŠ† Â· Â· Â· ,
1138 CHAPTER 32. UFDâ€™S, NOETHERIAN RINGS, HILBERTâ€™S BASIS THEOREM
there is some integer n â‰¥ 1 so that
Ai = An for all i â‰¥ n + 1.
We say that A satisfies the maximum condition if every nonempty collection C of ideals in
A has a maximal element, i.e., there is some ideal A âˆˆ C which is not contained in any other
ideal in C.
Proposition 32.17. A ring A satisfies the a.c.c if and only if it satisfies the maximum
condition.
Proof. Suppose that A does not satisfy the a.c.c. Then, there is an infinite strictly ascending
sequence of ideals
A1 âŠ‚ A2 âŠ‚ Â· Â· Â· âŠ‚ Ai âŠ‚ Â· Â· Â· ,
and the collection C = {Ai} has no maximal element.
Conversely, assume that A satisfies the a.c.c. Let C be a nonempty collection of ideals
Since C is nonempty, we may pick some ideal A1 in C. If A1 is not maximal, then there is
some ideal A2 in C so that
A1 âŠ‚ A2.
Using this process, if C has no maximal element, we can define by induction an infinite
strictly increasing sequence
A1 âŠ‚ A2 âŠ‚ Â· Â· Â· âŠ‚ Ai âŠ‚ Â· Â· Â· .
However, the a.c.c. implies that such a sequence cannot exist. Therefore, C has a maximal
element.
Having shown that the a.c.c. condition is equivalent to the maximal condition, we now
prove that the a.c.c. condition is equivalent to the fact that every ideal is finitely generated.
Proposition 32.18. A ring A satisfies the a.c.c if and only if every ideal is finitely generated.
Proof. Assume that every ideal is finitely generated. Consider an ascending sequence of
ideals
A1 âŠ† A2 âŠ† Â· Â· Â· âŠ† Ai âŠ† Â· Â· Â· .
Observe that A =
S i Ai
is also an ideal. By hypothesis, A has a finite generating set
{a1, . . . , an}. By definition of A, each ai belongs to some Aji
, and since the Ai
form an
ascending chain, there is some m so that ai âˆˆ Am for i = 1, . . . , n. But then,
Ai = Am
for all i â‰¥ m + 1, and the a.c.c. holds.
Conversely, assume that the a.c.c. holds. Let A be any ideal in A and consider the family
C of subideals of A that are finitely generated. The family C is nonempty, since (0) is a
subideal of A. By Proposition 32.17, the family C has some maximal element, say B. For
32.3. NOETHERIAN RINGS AND HILBERTâ€™S BASIS THEOREM 1139
any a âˆˆ A, the ideal B + (a) (where B + (a) = {b + Î»a | b âˆˆ B, Î» âˆˆ A}) is also finitely
generated (since B is finitely generated), and by maximality, we have
B = B + (a).
So, we get a âˆˆ B for all a âˆˆ A, and thus, A = B, and A is finitely generated.
Definition 32.5. A commutative ring A (with unit 1) is called noetherian if it satisfies the
a.c.c. condition. A noetherian domain is a noetherian ring that is also a domain.
By Proposition 32.17 and Proposition 32.18, a noetherian ring can also be defined as a
ring that either satisfies the maximal property or such that every ideal is finitely generated.
The proof of Hilbertâ€™s basis theorem will make use the following lemma:
Lemma 32.19. Let A be a (commutative) ring. For every ideal A in A[X], for every i â‰¥ 0,
let Li(A) denote the set of elements of A consisting of 0 and of the coefficients of Xi
in all
the polynomials f(X) âˆˆ A which are of degree i. Then, the Li(A)â€™s form an ascending chain
of ideals in A. Furthermore, if B is any ideal of A[X] so that A âŠ† B and if Li(A) = Li(B)
for all i â‰¥ 0, then A = B.
Proof. That Li(A) is an ideal and that Li(A) âŠ† Li+1(A) follows from the fact that if f(X) âˆˆ
A and g(X) âˆˆ A, then f(X) + g(X), Î»f(X), and Xf(X) all belong to A. Now, let g(X) be
any polynomial in B, and assume that g(X) has degree n. Since Ln(A) = Ln(B), there is
some polynomial fn(X) in A, of degree n, so that g(X) âˆ’ fn(X) is of degree at most n âˆ’ 1.
Now, since A âŠ† B, the polynomial g(X) âˆ’ fn(X) belongs to B. Using this process, we can
define by induction a sequence of polynomials fn+i(X) âˆˆ A, so that each fn+i(X) is either
zero or has degree n âˆ’ i, and
g(X) âˆ’ (fn(X) + fn+1(X) + Â· Â· Â· + fn+i(X))
is of degree at most n âˆ’ i âˆ’ 1. Note that this last polynomial must be zero when i = n, and
thus, g(X) âˆˆ A.
We now prove Hilbertâ€™s basis theorem. The proof is substantially Hilbertâ€™s original proof.
A slightly shorter proof can be given but it is not as transparent as Hilbertâ€™s proof (see the
remark just after the proof of Theorem 32.20, and Zariski and Samuel [194], Chapter IV,
Section 1, Theorem 1).
Theorem 32.20. (Hilbertâ€™s basis theorem) If A is a noetherian ring, then A[X] is also a
noetherian ring.
Proof. Let A be any ideal in A[X], and denote by L the set of elements of A consisting of 0
and of all the coefficients of the highest degree terms of all the polynomials in A. Observe
that
L =
[
i
Li(A).
1140 CHAPTER 32. UFDâ€™S, NOETHERIAN RINGS, HILBERTâ€™S BASIS THEOREM
Thus, L is an ideal in A (this can also be proved directly). Since A is noetherian, L is
finitely generated, and let {a1, . . . , an} be a set of generators of L. Let f1(X), . . . , fn(X) be
polynomials in A having respectively a1, . . . , an as highest degree term coefficients. These
polynomials generate an ideal B. Let q be the maximum of the degrees of the fi(X)â€™s. Now,
pick any polynomial g(X) âˆˆ A of degree d â‰¥ q, and let aXd be its term of highest degree.
Since a âˆˆ L, we have
a = Î»1a1 + Â· Â· Â· + Î»nan,
for some Î»i âˆˆ A. Consider the polynomial
g1(X) =
nX
i=1
Î»ifi(X)X
dâˆ’di
,
where di
is the degree of fi(X). Now, g(X) âˆ’ g1(X) is a polynomial in A of degree at most
d âˆ’ 1. By repeating this procedure, we get a sequence of polynomials gi(X) in B, having
strictly decreasing degrees, and such that the polynomial
g(X) âˆ’ (g1(X) + Â· Â· Â· + gi(X))
is of degree at most d âˆ’ i. This polynomial must be of degree at most q âˆ’ 1 as soon as
i = d âˆ’ q + 1. Thus, we proved that every polynomial in A of degree d â‰¥ q belongs to B.
It remains to take care of the polynomials in A of degree at most q âˆ’ 1. Since A is
noetherian, each ideal Li(A) is finitely generated, and let {ai1, . . . , aini
} be a set of generators
for Li(A) (for i = 0, . . . , q âˆ’ 1). Let fij (X) be a polynomial in A having aijXi as its highest
degree term. Given any polynomial g(X) âˆˆ A of degree d â‰¤ q âˆ’ 1, if we denote its term of
highest degree by aXd
, then, as in the previous argument, we can write
a = Î»1ad1 + Â· Â· Â· + Î»nd
adnd
,
and we define
g1(X) =
nd X
i=1
Î»ifdi(X)X
dâˆ’di
,
where di
is the degree of fdi(X). Then, g(X) âˆ’ g1(X) is a polynomial in A of degree at most
d âˆ’ 1, and by repeating this procedure at most q times, we get an element of A of degree 0,
and the latter is a linear combination of the f0i
â€™s. This proves that every polynomial in A
of degree at most q âˆ’ 1 is a combination of the polynomials fij (X), for 0 â‰¤ i â‰¤ q âˆ’ 1 and
1 â‰¤ j â‰¤ ni
. Therefore, A is generated by the fk(X)â€™s and the fij (X)â€™s, a finite number of
polynomials.
Remark: Only a small part of Lemma 32.19 was used in the above proof, namely, the fact
that Li(A) is an ideal. A shorter proof of Theorem 32.21 making full use of Lemma 32.19
can be given as follows:
32.4. FUTHER READINGS 1141
Proof. (Second proof) Let (Ai)iâ‰¥1 be an ascending sequence of ideals in A[X]. Consider
the doubly indexed family (Li(Aj )) of ideals in A. Since A is noetherian, by the maximal
property, this family has a maximal element Lp(Aq). Since the Li(Aj )â€™s form an ascending
sequence when either i or j is fixed, we have Li(Aj ) = Lp(Aq) for all i and j with i â‰¥ p and
j â‰¥ q, and thus, Li(Aq) = Li(Aj ) for all i and j with i â‰¥ p and j â‰¥ q. On the other hand,
for any fixed i, the a.c.c. shows that there exists some integer n(i) so that Li(Aj ) = Li(An(i))
for all j â‰¥ n(i). Since Li(Aq) = Li(Aj ) when i â‰¥ p and j â‰¥ q, we may take n(i) = q if
i â‰¥ p. This shows that there is some n0 so that n(i) â‰¤ n0 for all i â‰¥ 0, and thus, we have
Li(Aj ) = Li(An(0)) for every i and for every j â‰¥ n(0). By Lemma 32.19, we get Aj = An(0)
for every j â‰¥ n(0), establishing the fact that A[X] satisfies the a.c.c.
Using induction, we immediately obtain the following important result.
Corollary 32.21. If A is a noetherian ring, then A[X1, . . . , Xn] is also a noetherian ring.
Since a field K is obviously noetherian (since it has only two ideals, (0) and K), we also
have:
Corollary 32.22. If K is a field, then K[X1, . . . , Xn] is a noetherian ring.
32.4 Futher Readings
The material of this Chapter is thoroughly covered in Lang [109], Artin [7], Mac Lane and
Birkhoff [118], Bourbaki [25, 26], Malliavin [119], Zariski and Samuel [194], and Van Der
Waerden [179].
1142 CHAPTER 32. UFDâ€™S, NOETHERIAN RINGS, HILBERTâ€™S BASIS THEOREM
Chapter 33
Tensor Algebras and Symmetric
Algebras
Tensors are creatures that we would prefer did not exist but keep showing up whenever
multilinearity manifests itself.
One of the goals of differential geometry is to be able to generalize â€œcalculus on R
nâ€ to
spaces more general than R
n
, namely manifolds. We would like to differentiate functions
f : M â†’ R defined on a manifold, optimize functions (find their minima or maxima), but
also to integrate such functions, as well as compute areas and volumes of subspaces of our
manifold.
The suitable notion of differentiation is the notion of tangent map, a linear notion. One
of the main discoveries made at the beginning of the twentieth century by PoincarÂ´e and Elie Â´
Cartan, is that the â€œrightâ€ approach to integration is to integrate differential forms, and not
functions. To integrate a function f, we integrate the form fÏ‰, where Ï‰ is a volume form on
the manifold M. The formalism of differential forms takes care of the process of the change
of variables quite automatically, and allows for a very clean statement of Stokesâ€™ formula.
Differential forms can be combined using a notion of product called the wedge product,
but what really gives power to the formalism of differential forms is the magical operation d
of exterior differentiation. Given a form Ï‰, we obtain another form dÏ‰, and remarkably, the
following equation holds
ddÏ‰ = 0.
As silly as it looks, the above equation lies at the core of the notion of cohomology,
a powerful algebraic tool to understand the topology of manifolds, and more generally of
topological spaces.
Elie Cartan had many of the intuitions that lead to the cohomology of differential forms, Â´
but it was George de Rham who defined it rigorously and proved some important theorems
about it. It turns out that the notion of Laplacian can also be defined on differential forms
using a device due to Hodge, and some important theorems can be obtained: the Hodge
1143
1144 CHAPTER 33. TENSOR ALGEBRAS
decomposition theorem, and Hodgeâ€™s theorem about the isomorphism between the de Rham
cohomology groups and the spaces of harmonic forms.
To understand all this, one needs to learn about differential forms, which turn out to be
certain kinds of skew-symmetric (also called alternating) tensors.
If oneâ€™s only goal is to define differential forms, then it is possible to take some short
cuts and to avoid introducing the general notion of a tensor. However, tensors that are not
necessarily skew-symmetric arise naturally, such as the curvature tensor, and in the theory
of vector bundles, general tensor products are needed.
Consequently, we made the (perhaps painful) decision to provide a fairly detailed exï¿¾position of tensors, starting with arbitrary tensors, and then specializing to symmetric and
alternating tensors. In particular, we explain rather carefully the process of taking the dual
of a tensor (of all three flavors).
We refrained from following the approach in which a tensor is defined as a multilinear
map defined on a product of dual spaces, because it seems very artificial and confusing
(certainly to us). This approach relies on duality results that only hold in finite dimension,
and consequently unecessarily restricts the theory of tensors to finite dimensional spaces. We
also feel that it is important to begin with a coordinate-free approach. Bases can be chosen
for computations, but tensor algebra should not be reduced to raising or lowering indices.
Readers who feel that they are familiar with tensors should probably skip this chapter
and the next. They can come back to them â€œby need.â€
We begin by defining tensor products of vector spaces over a field and then we investigate
some basic properties of these tensors, in particular the existence of bases and duality. After
this we investigate special kinds of tensors, namely symmetric tensors and skew-symmetric
tensors. Tensor products of modules over a commutative ring with identity will be discussed
very briefly. They show up naturally when we consider the space of sections of a tensor
product of vector bundles.
Given a linear map f : E â†’ F (where E and F are two vector spaces over a field K),
we know that if we have a basis (ui)iâˆˆI for E, then f is completely determined by its values
f(ui) on the basis vectors. For a multilinear map f : E
n â†’ F, we donâ€™t know if there is such
a nice property but it would certainly be very useful.
In many respects tensor products allow us to define multilinear maps in terms of their
action on a suitable basis. The crucial idea is to linearize, that is, to create a new vector space
E
âŠ—n
such that the multilinear map f : E
n â†’ F is turned into a linear map fâŠ— : E
âŠ—n â†’ F
which is equivalent to f in a strong sense. If in addition, f is symmetric, then we can define
a symmetric tensor power Symn
(E), and every symmetric multilinear map f : E
n â†’ F is
turned into a linear map f : Symn
(E) â†’ F which is equivalent to f in a strong sense.
Similarly, if f is alternating, then we can define a skew-symmetric tensor power V n
(E), and
every alternating multilinear map is turned into a linear map fâˆ§ :
V
n
(E) â†’ F which is
equivalent to f in a strong sense.
33.1. LINEAR ALGEBRA PRELIMINARIES: DUAL SPACES AND PAIRINGS 1145
Tensor products can be defined in various ways, some more abstract than others. We try
to stay down to earth, without excess.
Before proceeding any further, we review some facts about dual spaces and pairings.
Pairings will be used to deal with dual spaces of tensors.
33.1 Linear Algebra Preliminaries: Dual Spaces and
Pairings
We assume that we are dealing with vector spaces over a field K. As usual the dual space E
âˆ—
of a vector space E is defined by E
âˆ— = Hom(E, K). The dual space E
âˆ—
is the vector space
consisting of all linear maps Ï‰: E â†’ K with values in the field K.
A problem that comes up often is to decide when a space E is isomorphic to the dual
F
âˆ— of some other space F (possibly equal to E). The notion of pairing due to Pontrjagin
provides a very clean criterion.
Definition 33.1. Given two vector spaces E and F over a field K, a map hâˆ’, âˆ’i: EÃ—F â†’ K
is a nondegenerate pairing iff it is bilinear and iff h u, vi = 0 for all v âˆˆ F implies u = 0, and
h
u, vi = 0 for all u âˆˆ E implies v = 0. A nondegenerate pairing induces two linear maps
Ï•: E â†’ F
âˆ— and Ïˆ: F â†’ E
âˆ— defined such that for all for all u âˆˆ E and all v âˆˆ F, Ï•(u) is the
linear form in F
âˆ— and Ïˆ(v) is the linear form in E
âˆ— given by
Ï•(u)(y) = h u, yi for all y âˆˆ F
Ïˆ(v)(x) = h x, vi for all x âˆˆ E.
Schematically, Ï•(u) = h u, âˆ’i and Ïˆ(v) = hâˆ’, vi .
Proposition 33.1. For every nondegenerate pairing hâˆ’, âˆ’i: E Ã—F â†’ K, the induced maps
Ï•: E â†’ F
âˆ— and Ïˆ: F â†’ E
âˆ— are linear and injective. Furthermore, if E and F are finite
dimensional, then Ï•: E â†’ F
âˆ— and Ïˆ: F â†’ E
âˆ— are bijective.
Proof. The maps Ï•: E â†’ F
âˆ— and Ïˆ: F â†’ E
âˆ— are linear because u, v 7â†’ hu, vi is bilinear.
Assume that Ï•(u) = 0. This means that Ï•(u)(y) = h u, yi = 0 for all y âˆˆ F, and as our
pairing is nondegenerate, we must have u = 0. Similarly, Ïˆ is injective. If E and F are finite
dimensional, then dim(E) = dim(E
âˆ—
) and dim(F) = dim(F
âˆ—
). However, the injectivity of Ï•
and Ïˆ implies that that dim(E) â‰¤ dim(F
âˆ—
) and dim(F) â‰¤ dim(E
âˆ—
). Consequently dim(E) â‰¤
dim(F) and dim(F) â‰¤ dim(E), so dim(E) = dim(F). Therefore, dim(E) = dim(F
âˆ—
) and Ï•
is bijective (and similarly dim(F) = dim(E
âˆ—
) and Ïˆ is bijective).
Proposition 33.1 shows that when E and F are finite dimensional, a nondegenerate pairing
induces canonical isomorphims Ï•: E â†’ F
âˆ— and Ïˆ: F â†’ E
âˆ—
; that is, isomorphisms that do
not depend on the choice of bases. An important special case is the case where E = F and
we have an inner product (a symmetric, positive definite bilinear form) on E.
1146 CHAPTER 33. TENSOR ALGEBRAS
Remark: When we use the term â€œcanonical isomorphism,â€ we mean that such an isomorï¿¾phism is defined independently of any choice of bases. For example, if E is a finite dimenï¿¾sional vector space and (e1, . . . , en) is any basis of E, we have the dual basis (e
âˆ—
1
, . . . , eâˆ—
n
) of
E
âˆ—
(where, e
âˆ—
i
(ej ) = Î´i j ), and thus the map ei
7â†’ e
âˆ—
i
is an isomorphism between E and E
âˆ—
.
This isomorphism is not canonical.
On the other hand, if hâˆ’, âˆ’i is an inner product on E, then Proposition 33.1 shows that
the nondegenerate pairing hâˆ’, âˆ’i on E Ã—E induces a canonical isomorphism between E and
E
âˆ—
. This isomorphism is often denoted [ : E â†’ E
âˆ—
, and we usually write u
[ for [ (u), with
u âˆˆ E. Schematically, u
[ = h u, âˆ’i. The inverse of [ is denoted ] : E
âˆ— â†’ E, and given any
linear form Ï‰ âˆˆ E
âˆ—
, we usually write Ï‰
] for ] (Ï‰). Schematically, Ï‰ = h Ï‰
] , âˆ’i.
Given any basis, (e1, . . . , en) of E (not necessarily orthonormal), let (gij ) be the n Ã— nï¿¾matrix given by gij = h ei
, ej i (the Gram matrix of the inner product). Recall that the dual
basis (e
âˆ—
1
, . . . , eâˆ—
n
) of E
âˆ—
consists of the coordinate forms e
âˆ—
i âˆˆ E
âˆ—
, which are characterized by
the following properties:
e
âˆ—
i
(ej ) = Î´ij , 1 â‰¤ i, j â‰¤ n.
The inverse of the Gram matrix (gij ) is often denoted by (g
ij ) (by raising the indices).
The tradition of raising and lowering indices is pervasive in the literature on tensors.
It is indeed useful to have some notational convention to distinguish between vectors and
linear forms (also called one-forms or covectors). The usual convention is that coordinates
of vectors are written using superscripts, as in u =
P
n
i=1 u
i
ei
, and coordinates of one-forms
are written using subscripts, as in Ï‰ =
P
n
i=1 Ï‰ie
âˆ—
i
. Actually, since vectors are indexed with
subscripts, one-forms are indexed with superscripts, so e
âˆ—
i
should be written as e
i
.
The motivation is that summation signs can then be omitted, according to the Einstein
summation convention. According to this convention, whenever a summation variable (such
as i) appears both as a subscript and a superscript in an expression, it is assumed that it is
involved in a summation. For example the sum P n
i=1 u
i
ei
is abbreviated as
u
i
ei
,
and the sum P n
i=1 Ï‰ie
i
is abbreviated as
Ï‰ie
i
.
In this text we will not use the Einstein summation convention, which we find somewhat
confusing, and we will also write e
âˆ—
i
instead of e
i
.
The maps [ and ] can be described explicitly in terms of the Gram matrix of the inner
product and its inverse.
Proposition 33.2. For any vector space E, given a basis (e1, . . . , en) for E and its dual
basis (e
âˆ—
1
, . . . , eâˆ—
n
) for E
âˆ—
, for any inner product hâˆ’, âˆ’i on E, if (gij ) is its Gram matrix, with
33.1. LINEAR ALGEBRA PRELIMINARIES: DUAL SPACES AND PAIRINGS 1147
gij = h ei
, ej i
, and (g
ij ) is its inverse, then for every vector u =
P
n
j=1 u
j
ej âˆˆ E and every
one-form Ï‰ =
P
n
i=1 Ï‰ie
âˆ—
i âˆˆ E
âˆ—
, we have
u
[ =
nX
i=1
Ï‰ie
âˆ—
i
, with Ï‰i =
nX
j=1
giju
j
,
and
Ï‰
] =
nX
j=1
(Ï‰
] )
j
ej
, with (Ï‰
] )
i =
nX
j=1
g
ijÏ‰j
.
Proof. For every u =
P
n
j=1 u
j
ej
, since u
[ (v) = h u, vi for all v âˆˆ E, we have
u
[ (ei) = h u, eii =

nX
j=1
u
j
ej
, ei
 =
nX
j=1
u
j
h
ej
, eii =
nX
j=1
giju
j
,
so we get
u
[ =
nX
i=1
Ï‰ie
âˆ—
i
, with Ï‰i =
nX
j=1
giju
j
.
If we write Ï‰ âˆˆ E
âˆ— as Ï‰ =
P
n
i=1 Ï‰ie
âˆ—
i
and Ï‰
] âˆˆ E as Ï‰
] =
P
n
j=1(Ï‰
] )
j
ej
, since
Ï‰i = Ï‰(ei) = h Ï‰
] , eii =
nX
j=1
(Ï‰
] )
j
gij , 1 â‰¤ i â‰¤ n,
we get
(Ï‰
] )
i =
nX
j=1
g
ijÏ‰j
,
where (g
ij ) is the inverse of the matrix (gij ).
The map [ has the effect of lowering (flattening!) indices, and the map ] has the effect
of raising (sharpening!) indices.
Here is an explicit example of Proposition 33.2. Let (e1, e2) be a basis of E such that
h
e1, e1i = 1, h e1, e2i = 2, h e2, e2i = 5.
Then
g =

1 2
2 5 , gâˆ’1 =

âˆ’
5
2 1
âˆ’2

.
Set u = u
1
e1 + u
2
e2 and observe that
u
[ (e1) = h u
1
e1 + u
2
e2, e1i = h e1, e1i u
1 + h e2, e1i u
2 = g11u
1 + g12u
2 = u
1 + 2u
2
u
[ (e2) = h u
1
e1 + u
2
e2, e2i = h e1, e2i u
1 + h e2, e2i u
2 = g21u
1 + g22u
2 = 2u
1 + 5u
2
,
1148 CHAPTER 33. TENSOR ALGEBRAS
which in turn implies that
u
[ = Ï‰1e
âˆ—
1 + Ï‰2e
âˆ—
2 = u
[ (e1)e
âˆ—
1 + u
[ (e2)e
âˆ—
2 = (u
1 + 2u
2
)e
âˆ—
1 + (2u
1 + 5u
2
)e
âˆ—
2
.
Given Ï‰ = Ï‰1e
âˆ—
1 + Ï‰2e
âˆ—
2
, we calculate Ï‰
] = (Ï‰
] )
1
e1 + (Ï‰
] )
2
e2 from the following two linear
equalities:
Ï‰1 = Ï‰(e1) = h Ï‰
] , e1i = h (Ï‰
] )
1
e1 + (Ï‰
] )
2
e2, e1i
= h e1, e1i (Ï‰
] )
1 + h e2, e1i (Ï‰
] )
2 = (Ï‰
] )
1 + 2(Ï‰
] )
2 = g11(Ï‰
] )
1 + g12(Ï‰
] )
2
Ï‰2 = Ï‰(e2) = h Ï‰
] , e2i = h (Ï‰
] )
1
e1 + (Ï‰
] )
2
e2, e2i
= h e1, e2i (Ï‰
] )
1 + h e2, e2i (Ï‰
] )
2 = 2(Ï‰
] )
1 + 5(Ï‰
] )
2 = g21(Ï‰
] )
1 + g22(Ï‰
] )
2
.
These equalities are concisely written as

Ï‰
Ï‰
1
2

=

1 2
2 5 
(Ï‰
] )
1
(Ï‰
] )
2
 = g

(Ï‰
] )
1
(Ï‰
] )
2

.
Then

(Ï‰
] )
1
(Ï‰
] )
2
 = g
âˆ’1
 Ï‰
Ï‰
1
2

=

âˆ’
5
2 1
âˆ’2
  Ï‰
Ï‰
1
2

,
which in turn implies
(Ï‰
] )
1 = 5Ï‰1 âˆ’ 2Ï‰2, (Ï‰
] )
2 = âˆ’2Ï‰1 + Ï‰2,
i.e.
Ï‰
] = (5Ï‰1 âˆ’ 2Ï‰2)e1 + (âˆ’2Ï‰1 + Ï‰2)e2.
The inner product hâˆ’, âˆ’i on E induces an inner product on E
âˆ— denoted hâˆ’, âˆ’iEâˆ— , and
given by
h
Ï‰1, Ï‰2i Eâˆ— = h Ï‰1
]
, Ï‰2
]
i
, for all Ï‰1, Ï‰2 âˆˆ E
âˆ—
.
Then we have
h
u
[ , v[ i Eâˆ— = h (u
[ )
] ,(v
[ )
] i = h u, vi for all u, v âˆˆ E.
If (e1, . . . , en) is a basis of E and gij = h ei
, ej i
, as
(e
âˆ—
i
)
] =
nX
k=1
g
ikek,
an easy computation shows that
h
e
âˆ—
i
, eâˆ—
j
i Eâˆ— = h (e
âˆ—
i
)
] ,(e
âˆ—
j
)
] i = g
ij ;
33.1. LINEAR ALGEBRA PRELIMINARIES: DUAL SPACES AND PAIRINGS 1149
that is, in the basis (e
âˆ—
1
, . . . , eâˆ—
n
), the inner product on E
âˆ—
is represented by the matrix (g
ij ),
the inverse of the matrix (gij ).
The inner product on a finite vector space also yields a canonical isomorphism between
the space Hom(E, E; K) of bilinear forms on E, and the space Hom(E, E) of linear maps
from E to itself. Using this isomorphism, we can define the trace of a bilinear form in an
intrinsic manner. This technique is used in differential geometry, for example, to define the
divergence of a differential one-form.
Proposition 33.3. If hâˆ’, âˆ’i is an inner product on a finite vector space E (over a field,
K), then for every bilinear form f : E Ã— E â†’ K, there is a unique linear map f
\ : E â†’ E
such that
f(u, v) = h f
\ (u), vi , for all u, v âˆˆ E.
The map f 7â†’ f
\ is a linear isomorphism between Hom(E, E; K) and Hom(E, E).
Proof. For every g âˆˆ Hom(E, E), the map given by
f(u, v) = h g(u), vi , u, v âˆˆ E,
is clearly bilinear. It is also clear that the above defines a linear map from Hom(E, E) to
Hom(E, E; K). This map is injective, because if f(u, v) = 0 for all u, v âˆˆ E, as hâˆ’, âˆ’i is
an inner product, we get g(u) = 0 for all u âˆˆ E. Furthermore, both spaces Hom(E, E) and
Hom(E, E; K) have the same dimension, so our linear map is an isomorphism.
If (e1, . . . , en) is an orthonormal basis of E, then we check immediately that the trace of
a linear map g (which is independent of the choice of a basis) is given by
tr(g) =
nX
i=1
h
g(ei), eii
,
where n = dim(E).
Definition 33.2. We define the trace of the bilinear form f by
tr(f) = tr(f
\ ).
From Proposition 33.3, tr(f) is given by
tr(f) =
nX
i=1
f(ei
, ei),
for any orthonormal basis (e1, . . . , en) of E. We can also check directly that the above
expression is independent of the choice of an orthonormal basis.
1150 CHAPTER 33. TENSOR ALGEBRAS
We demonstrate how to calculate tr(f) where f : R
2Ã—R
2 â†’ R with f((x1, y1),(x2, y2)) =
x1x2+2x2y1+3x1y2âˆ’y1y2. Under the standard basis for R
2
, the bilinear form f is represented
as
ï¿¾
x1 y1


1 3
2 âˆ’1
 
x
y2
2

.
This matrix representation shows that
f
\ =

1 3
2 âˆ’1

>
=

1 2
3 âˆ’1

,
and hence
tr(f) = tr(f
\ ) = tr  1 2
3 âˆ’1

= 0.
We will also need the following proposition to show that various families are linearly
independent.
Proposition 33.4. Let E and F be two nontrivial vector spaces and let (ui)iâˆˆI be any family
of vectors ui âˆˆ E. The family (ui)iâˆˆI is linearly independent iff for every family (vi)iâˆˆI of
vectors vi âˆˆ F, there is some linear map f : E â†’ F so that f(ui) = vi
for all i âˆˆ I.
Proof. Left as an exercise.
33.2 Tensors Products
First we define tensor products, and then we prove their existence and uniqueness up to
isomorphism.
Definition 33.3. Let K be a given field, and let E1, . . . , En be n â‰¥ 2 given vector spaces.
For any vector space F, a map f : E1 Ã— Â· Â· Â· Ã— En â†’ F is multilinear iff it is linear in each of
its argument; that is,
f(u1, . . . ui1
, v + w, ui+1, . . . , un) = f(u1, . . . ui1
, v, ui+1, . . . , un)
+ f(u1, . . . ui1
, w, ui+1, . . . , un)
f(u1, . . . ui1
, Î»v, ui+1, . . . , un) = Î»f(u1, . . . ui1
, v, ui+1, . . . , un),
for all uj âˆˆ Ej (j 6 = i), all v, w âˆˆ Ei and all Î» âˆˆ K, for i = 1 . . . , n.
The set of multilinear maps as above forms a vector space denoted L(E1, . . . , En; F) or
Hom(E1, . . . , En; F). When n = 1, we have the vector space of linear maps L(E, F) (also
denoted Hom(E, F)). (To be very precise, we write HomK(E1, . . . , En; F) and HomK(E, F).)
33.2. TENSORS PRODUCTS 1151
Definition 33.4. A tensor product of n â‰¥ 2 vector spaces E1, . . . , En is a vector space T
together with a multilinear map Ï•: E1 Ã— Â· Â· Â· Ã— En â†’ T, such that for every vector space F
and for every multilinear map f : E1Ã—Â· Â· Â·Ã—En â†’ F, there is a unique linear map fâŠ— : T â†’ F
with
f(u1, . . . , un) = fâŠ—(Ï•(u1, . . . , un)),
for all u1 âˆˆ E1, . . . , un âˆˆ En, or for short
f = fâŠ— â—¦ Ï•.
Equivalently, there is a unique linear map fâŠ— such that the following diagram commutes.
E1 Ã— Â· Â· Â· Ã— En â—†â—†â—†â—†â—†
f
â—†â—†â—†â—†â—†â—†&
Ï•
/
T


fâŠ—
F
The above property is called the universal mapping property of the tensor product (T, Ï•).
We show that any two tensor products (T1, Ï•1) and (T2, Ï•2) for E1, . . . , En, are isomorphic.
Proposition 33.5. Given any two tensor products (T1, Ï•1) and (T2, Ï•2) for E1, . . . , En, there
is an isomorphism h: T1 â†’ T2 such that
Ï•2 = h â—¦ Ï•1.
Proof. Focusing on (T1, Ï•1), we have a multilinear map Ï•2 : E1 Ã— Â· Â· Â· Ã— En â†’ T2, and thus
there is a unique linear map (Ï•2)âŠ— : T1 â†’ T2 with
Ï•2 = (Ï•2)âŠ— â—¦ Ï•1
as illustrated by the following commutative diagram.
E1 Ã— Â· Â· Â· Ã— En â–¼â–¼â–¼â–¼
Ï•
â–¼â–¼
2
â–¼â–¼â–¼â–¼â–¼â–¼&
Ï•1 /
T1


(Ï•2)âŠ—
T2
Similarly, focusing now on on (T2, Ï•2), we have a multilinear map Ï•1 : E1 Ã— Â· Â· Â· Ã— En â†’ T1,
and thus there is a unique linear map (Ï•1)âŠ— : T2 â†’ T1 with
Ï•1 = (Ï•1)âŠ— â—¦ Ï•2
1152 CHAPTER 33. TENSOR ALGEBRAS
as illustrated by the following commutative diagram.
E1 Ã— Â· Â· Â· Ã— En â–¼â–¼â–¼â–¼
Ï•
â–¼â–¼
1
â–¼â–¼â–¼â–¼â–¼â–¼&
Ï•2 /
T2


(Ï•1)âŠ—
T1
Putting these diagrams together, we obtain the commutative diagrams
T1


(Ï•2)âŠ—
E1 Ã— Â· Â· Â· Ã— En â–¼â–¼â–¼â–¼
Ï•
â–¼â–¼
1
