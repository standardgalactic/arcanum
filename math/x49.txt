so using the fact that
KUj = ωj
2AUj
, j = 1, . . . , n,
the equation
A
d
2u
dt2
+ Ku = 0
yields
nX
j=1
[(uej )
00 + ωj
2uej
]AUj = 0.
Since A is invertible and since (U1
, . . . , Un
) are linearly independent, the vectors (AU1
,
. . . , AUn
) are linearly independent, and consequently we get the system of n ODEs’
(uej )
00 + ωj
2uej = 0, 1 ≤ j ≤ n.
Each of these equation has a well-known solution of the form
uej = Aj cos ωj t + Bj sin ωj t.
19.3. TIME-DEPENDENT BOUNDARY PROBLEMS 697
Therefore, the solution of our approximation problem is given by
u =
nX
j=1
(Aj cos ωj t + Bj sin ωj t)U
j
,
and the constants Aj
, Bj are obtained from the intial conditions
u(x, 0) = ua,0(x), 0 ≤ x ≤ L,
∂u
∂t (x, 0) = ua,1(x), 0 ≤ x ≤ L,
by expressing ua,0 and ua,1 on the modal basis (U
1
, . . . , Un
). Furthermore, the modal func￾tions (U
1
, . . . , Un
) form an orthonormal basis of Va for the inner product a.
If we use the vector space VN
0 of piecewise affine functions, we find that the matrices A
and K are familar! Indeed,
A =
1
h


2 −1 0 0 0
−1 2 −1 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 −
0 0 0
1 2
−1 2
−1


and
K =
h
6


4 1 0 0 0
1 4 1 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 1 4 1
0 0 0 1 4


.
To conclude this section, let us discuss briefly the wave equation for an elastic membrane,
as described in Section 19.2. This time, we look for a function u: R+ × Ω → R satisfying
the following conditions:
1
c
2
∂
2u
∂t2
(x, t) − ∆u(x, t) = f(x, t), x ∈ Ω, t > 0,
u(x, t) = 0, x ∈ Γ, t ≥ 0 (boundary condition),
u(x, 0) = ui,0(x), x ∈ Ω (intitial condition),
∂u
∂t (x, 0) = ui,1(x), x ∈ Ω (intitial condition).
Assuming that f = 0, we look for solutions in the subspace V of the Sobolev space H0
1
(Ω)
consisting of functions v such that v = 0 on Γ. Multiplying by a test function v ∈ V and
using Green’s first identity, we get the weak formulation of our problem:
698 CHAPTER 19. INTRODUCTION TO THE FINITE ELEMENTS METHOD
Find a function u ∈ V such that
d
2
dt2
h
u, vi + a(u, v) = 0, for all v ∈ V and all t ≥ 0
u(x, 0) = ui,0(x), x ∈ Ω (intitial condition),
∂u
∂t (x, 0) = ui,1(x), x ∈ Ω (intitial condition),
where a: V × V → R is the bilinear form given by
a(u, v) = Z
Ω
 ∂x
∂u
1
∂v
∂x1
+
∂u
∂x2
∂v
∂x2

dx,
and
h
u, vi =
Z
Ω
uvdx.
As usual, we find approximations of our problem by using finite dimensional subspaces
Va of V . Picking some basis (w1, . . . , wn) of Va, and triangulating Ω, as before, we obtain
the equation
A
d
2u
dt2
+ Ku = 0,
u(x, 0) = ua,0(x), x ∈ Γ,
∂u
∂t (x, 0) = ua,1(x), x ∈ Γ,
where A = (h wi
, wj i ) and K = (a(wi
, wj )) are two symmetric positive definite matrices.
In principle, the problem is solved, but, it may be difficult to find good spaces Va, good
triangulations of Ω, and good bases of Va, to be able to compute the matrices A and K, and
to ensure that they are sparse.
Chapter 20
Graphs and Graph Laplacians; Basic
Facts
In this chapter and the next we present some applications of linear algebra to graph theory.
Graphs (undirected and directed) can be defined in terms of various matrices (incidence and
adjacency matrices), and various connectivity properties of graphs are captured by properties
of these matrices. Another very important matrix is associated with a (undirected) graph:
the graph Laplacian. The graph Laplacian is symmetric positive definite, and its eigenvalues
capture some of the properties of the underlying graph. This is a key fact that is exploited
in graph clustering methods, the most powerful being the method of normalized cuts due to
Shi and Malik [160]. This chapter and the next constitute an introduction to algebraic and
spectral graph theory. We do not discuss normalized cuts, but we discuss graph drawings.
Thorough presentations of algebraic graph theory can be found in Godsil and Royle [77] and
Chung [39].
We begin with a review of basic notions of graph theory. Even though the graph Laplacian
is fundamentally associated with an undirected graph, we review the definition of both
directed and undirected graphs. For both directed and undirected graphs, we define the
degree matrix D, the incidence matrix B, and the adjacency matrix A. Then we define a
weighted graph. This is a pair (V, W), where V is a finite set of nodes and W is a m × m
symmetric matrix with nonnegative entries and zero diagonal entries (where m = |V |).
For every node vi ∈ V , the degree d(vi) (or di) of vi
is the sum of the weights of the edges
adjacent to vi
:
di = d(vi) =
mX
j=1
wi j .
The degree matrix is the diagonal matrix
D = diag(d1, . . . , dm).
The notion of degree is illustrated in Figure 20.1. Then we introduce the (unnormalized)
699
700 CHAPTER 20. GRAPHS AND GRAPH LAPLACIANS; BASIC FACTS
18
Degree of a node:
di = ¦j Wi,j
Degree matrix:
Dii = ¦j Wi,j
Figure 20.1: Degree of a node.
graph Laplacian L of a directed graph G in an “old-fashion” way, by showing that for any
orientation of a graph G,
BB> = D − A = L
is an invariant. We also define the (unnormalized) graph Laplacian L of a weighted graph
G = (V, W) as L = D − W. We show that the notion of incidence matrix can be generalized
to weighted graphs in a simple way. For any graph Gσ obtained by orienting the underlying
graph of a weighted graph G = (V, W), there is an incidence matrix Bσ
such that
B
σ
(B
σ
)
> = D − W = L.
We also prove that
x
> Lx =
1
2
mX
i,j=1
wi j (xi − xj )
2
for all x ∈ R
m.
Consequently, x
> Lx does not depend on the diagonal entries in W, and if wi j ≥ 0 for all
i, j ∈ {1, . . . , m}, then L is positive semidefinite. Then if W consists of nonnegative entries,
the eigenvalues 0 = λ1 ≤ λ2 ≤ . . . ≤ λm of L are real and nonnegative, and there is an
orthonormal basis of eigenvectors of L. We show that the number of connected components
of the graph G = (V, W) is equal to the dimension of the kernel of L, which is also equal to
the dimension of the kernel of the transpose (Bσ
)
> of any incidence matrix Bσ obtained by
orienting the underlying graph of G.
We also define the normalized graph Laplacians Lsym and Lrw, given by
Lsym = D
−1/2LD−1/2 = I − D
−1/2W D−1/2
Lrw = D
−1L = I − D
−1W,
and prove some simple properties relating the eigenvalues and the eigenvectors of L, Lsym
and Lrw. These normalized graph Laplacians show up when dealing with normalized cuts.
701
Next, we turn to graph drawings (Chapter 21). Graph drawing is a very attractive appli￾cation of so-called spectral techniques, which is a fancy way of saying that that eigenvalues
and eigenvectors of the graph Laplacian are used. Furthermore, it turns out that graph
clustering using normalized cuts can be cast as a certain type of graph drawing.
Given an undirected graph G = (V, E), with |V | = m, we would like to draw G in R
n
for
n (much) smaller than m. The idea is to assign a point ρ(vi) in R
n
to the vertex vi ∈ V , for
every vi ∈ V , and to draw a line segment between the points ρ(vi) and ρ(vj ). Thus, a graph
drawing is a function ρ: V → R
n
.
We define the matrix of a graph drawing ρ (in R
n
) as a m × n matrix R whose ith row
consists of the row vector ρ(vi) corresponding to the point representing vi
in R
n
. Typically,
we want n < m; in fact n should be much smaller than m.
Since there are infinitely many graph drawings, it is desirable to have some criterion to
decide which graph is better than another. Inspired by a physical model in which the edges
are springs, it is natural to consider a representation to be better if it requires the springs
to be less extended. We can formalize this by defining the energy of a drawing R by
E(R) = X
{vi,vj}∈E
k
ρ(vi) − ρ(vj )k
2
,
where ρ(vi) is the ith row of R and k ρ(vi) − ρ(vj )k
2
is the square of the Euclidean length of
the line segment joining ρ(vi) and ρ(vj ).
Then “good drawings” are drawings that minimize the energy function E. Of course, the
trivial representation corresponding to the zero matrix is optimum, so we need to impose
extra constraints to rule out the trivial solution.
We can consider the more general situation where the springs are not necessarily identical.
This can be modeled by a symmetric weight (or stiffness) matrix W = (wij ), with wij ≥ 0.
In this case, our energy function becomes
E(R) = X
{vi,vj}∈E
wij k ρ(vi) − ρ(vj )k
2
.
Following Godsil and Royle [77], we prove that
E(R) = tr(R
> LR),
where
L = D − W,
is the familiar unnormalized Laplacian matrix associated with W, and where D is the degree
matrix associated with W.
It can be shown that there is no loss in generality in assuming that the columns of R
are pairwise orthogonal and that they have unit length. Such a matrix satisfies the equation
702 CHAPTER 20. GRAPHS AND GRAPH LAPLACIANS; BASIC FACTS
R> R = I and the corresponding drawing is called an orthogonal drawing. This condition
also rules out trivial drawings.
Then we prove the main theorem about graph drawings (Theorem 21.2), which essentially
says that the matrix R of the desired graph drawing is constituted by the n eigenvectors of
L associated with the smallest nonzero n eigenvalues of L. We give a number examples of
graph drawings, many of which are borrowed or adapted from Spielman [163].
20.1 Directed Graphs, Undirected Graphs, Incidence
Matrices, Adjacency Matrices, Weighted Graphs
Definition 20.1. A directed graph is a pair G = (V, E), where V = {v1, . . . , vm} is a set of
nodes or vertices, and E ⊆ V × V is a set of ordered pairs of distinct nodes (that is, pairs
(u, v) ∈ V × V with u 6 = v), called edges. Given any edge e = (u, v), we let s(e) = u be the
source of e and t(e) = v be the target of e.
Remark: Since an edge is a pair (u, v) with u 6 = v, self-loops are not allowed. Also, there
is at most one edge from a node u to a node v. Such graphs are sometimes called simple
graphs.
An example of a directed graph is shown in Figure 20.2.
1
v4
v5
v1 v2
v3
e1
e7
e2 e3 e4
e5
e6
Figure 20.2: Graph G1.
Definition 20.2. For every node v ∈ V , the degree d(v) of v is the number of edges leaving
or entering v:
d(v) = |{u ∈ V | (v, u) ∈ E or (u, v) ∈ E}|.
We abbreviate d(vi) as di
. The degree matrix , D(G), is the diagonal matrix
D(G) = diag(d1, . . . , dm).
20.1. DIRECTED GRAPHS, UNDIRECTED GRAPHS, WEIGHTED GRAPHS 703
For example, for graph G1, we have
D(G1) =


2 0 0 0 0
0 4 0 0 0
0 0 3 0 0
0 0 0 3 0
0 0 0 0 2


.
Unless confusion arises, we write D instead of D(G).
Definition 20.3. Given a directed graph G = (V, E), for any two nodes u, v ∈ V , a path
from u to v is a sequence of nodes (v0, v1, . . . , vk) such that v0 = u, vk = v, and (vi
, vi+1) is
an edge in E for all i with 0 ≤ i ≤ k − 1. The integer k is the length of the path. A path
is closed if u = v. The graph G is strongly connected if for any two distinct nodes u, v ∈ V ,
there is a path from u to v and there is a path from v to u.
Remark: The terminology walk is often used instead of path, the word path being reserved
to the case where the nodes vi are all distinct, except that v0 = vk when the path is closed.
The binary relation on V × V defined so that u and v are related iff there is a path from
u to v and there is a path from v to u is an equivalence relation whose equivalence classes
are called the strongly connected components of G.
Definition 20.4. Given a directed graph G = (V, E), with V = {v1, . . . , vm}, if E =
{
given by
e1, . . . , en}, then the incidence matrix B(G) of G is the m × n matrix whose entries bi j are
bi j =



+1 if
−1 if
s
t(
(
e
e
j
j
) =
) =
v
v
i
i
0 otherwise.
Here is the incidence matrix of the graph G1:
B =


−
1 1 0 0 0 0 0
0
1 0 −1 −1 1 0 0
−1 1 0 0 0 1
0 0 0 1 0
0 0 0 0 −1 1 0
−1 −1


.
Observe that every column of an incidence matrix contains exactly two nonzero entries,
+1 and −1. Again, unless confusion arises, we write B instead of B(G).
When a directed graph has m nodes v1, . . . , vm and n edges e1, . . . , en, a vector x ∈ R
m
can be viewed as a function x: V → R assigning the value xi to the node vi
. Under this
interpretation, R
m is viewed as R
V
. Similarly, a vector y ∈ R
n
can be viewed as a function
704 CHAPTER 20. GRAPHS AND GRAPH LAPLACIANS; BASIC FACTS 1
v4
v5
v1 v2
v3
a
g
b c d
e
f
Figure 20.3: The undirected graph G2.
in R
E. This point of view is often useful. For example, the incidence matrix B can be
interpreted as a linear map from R
E to R
V
, the boundary map, and B> can be interpreted
as a linear map from R
V
to R
E, the coboundary map.
Remark: Some authors adopt the opposite convention of sign in defining the incidence
matrix, which means that their incidence matrix is −B.
Undirected graphs are obtained from directed graphs by forgetting the orientation of the
edges.
Definition 20.5. A graph (or undirected graph) is a pair G = (V, E), where V = {v1, . . . , vm}
is a set of nodes or vertices, and E is a set of two-element subsets of V (that is, subsets
{u, v}, with u, v ∈ V and u 6 = v), called edges.
Remark: Since an edge is a set {u, v}, we have u 6 = v, so self-loops are not allowed. Also,
for every set of nodes {u, v}, there is at most one edge between u and v. As in the case of
directed graphs, such graphs are sometimes called simple graphs.
An example of a graph is shown in Figure 20.3.
Definition 20.6. For every node v ∈ V , the degree d(v) of v is the number of edges incident
to v:
d(v) = |{u ∈ V | {u, v} ∈ E}|.
The degree matrix D(G) (or simply, D) is defined as in Definition 20.2.
Definition 20.7. Given a (undirected) graph G = (V, E), for any two nodes u, v ∈ V , a path
from u to v is a sequence of nodes (v0, v1, . . . , vk) such that v0 = u, vk = v, and {vi
, vi+1} is
an edge in E for all i with 0 ≤ i ≤ k − 1. The integer k is the length of the path. A path is
closed if u = v. The graph G is connected if for any two distinct nodes u, v ∈ V , there is a
path from u to v.
20.1. DIRECTED GRAPHS, UNDIRECTED GRAPHS, WEIGHTED GRAPHS 705
Remark: The terminology walk or chain is often used instead of path, the word path being
reserved to the case where the nodes vi are all distinct, except that v0 = vk when the path
is closed.
The binary relation on V ×V defined so that u and v are related iff there is a path from u
to v is an equivalence relation whose equivalence classes are called the connected components
of G.
The notion of incidence matrix for an undirected graph is not as useful as in the case of
directed graphs
Definition 20.8. Given a graph G = (V, E), with V = {v1, . . . , vm}, if E = {e1, . . . , en},
then the incidence matrix B(G) of G is the m × n matrix whose entries bi j are given by
bi j =
(
0 otherwise
+1 if ej = {vi
.
, vk} for some k
Unlike the case of directed graphs, the entries in the incidence matrix of a graph (undi￾rected) are nonnegative. We usually write B instead of B(G).
Definition 20.9. If G = (V, E) is a directed or an undirected graph, given a node u ∈ V , any
node v ∈ V such that there is an edge (u, v) in the directed case or {u, v} in the undirected
case is called adjacent to u, and we often use the notation
u ∼ v.
Observe that the binary relation ∼ is symmetric when G is an undirected graph, but in
general it is not symmetric when G is a directed graph.
The notion of adjacency matrix is basically the same for directed or undirected graphs.
Definition 20.10. Given a directed or undirected graph G = (V, E), with V = {v1, . . . , vm},
the adjacency matrix A(G) of G is the symmetric m × m matrix (ai j ) such that
(1) If G is directed, then
ai j =
(
1 if there is some edge (
0 otherwise.
vi
, vj ) ∈ E or some edge (vj
, vi) ∈ E
(2) Else if G is undirected, then
ai j =
(
0 otherwise
1 if there is some edge
.
{vi
, vj} ∈ E
706 CHAPTER 20. GRAPHS AND GRAPH LAPLACIANS; BASIC FACTS
As usual, unless confusion arises, we write A instead of A(G). Here is the adjacency
matrix of both graphs G1 and G2:
A =


0 1 1 0 0
1 0 1 1 1
1 1 0 1 0
0 1 1 0 1
0 1 0 1 0


.
If G = (V, E) is an undirected graph, the adjacency matrix A of G can be viewed as a
linear map from R
V
to R
V
, such that for all x ∈ R
m, we have
(Ax)i =
X
j∼i
xj
;
that is, the value of Ax at vi
is the sum of the values of x at the nodes vj adjacent to vi
. The
adjacency matrix can be viewed as a diffusion operator . This observation yields a geometric
interpretation of what it means for a vector x ∈ R
m to be an eigenvector of A associated
with some eigenvalue λ; we must have
λxi =
X
j∼i
xj
, i = 1, . . . , m,
which means that the the sum of the values of x assigned to the nodes vj adjacent to vi
is
equal to λ times the value of x at vi
.
Definition 20.11. Given any undirected graph G = (V, E), an orientation of G is a function
σ : E → V × V assigning a source and a target to every edge in E, which means that for
every edge {u, v} ∈ E, either σ({u, v}) = (u, v) or σ({u, v}) = (v, u). The oriented graph
Gσ obtained from G by applying the orientation σ is the directed graph Gσ = (V, Eσ
), with
E
σ = σ(E).
The following result shows how the number of connected components of an undirected
graph is related to the rank of the incidence matrix of any oriented graph obtained from G.
Proposition 20.1. Let G = (V, E) be any undirected graph with m vertices, n edges, and
c connected components. For any orientation σ of G, if B is the incidence matrix of the
oriented graph Gσ
, then c = dim(Ker (B> )), and B has rank m − c. Furthermore, the
nullspace of B> has a basis consisting of indicator vectors of the connected components of
G; that is, vectors (z1, . . . , zm) such that zj = 1 iff vj is in the ith component Ki of G, and
zj = 0 otherwise.
Proof. (After Godsil and Royle [77], Section 8.3). The fact that rank(B) = m − c will be
proved last.
Let us prove that the kernel of B> has dimension c. A vector z ∈ R
m belongs to the
kernel of B> iff B> z = 0 iff z
> B = 0. In view of the definition of B, for every edge {vi
, vj}
20.1. DIRECTED GRAPHS, UNDIRECTED GRAPHS, WEIGHTED GRAPHS 707
of G, the column of B corresponding to the oriented edge σ({vi
, vj}) has zero entries except
for a +1 and a −1 in position i and position j or vice-versa, so we have
zi = zj
.
An easy induction on the length of the path shows that if there is a path from vi to vj
in G
(unoriented), then zi = zj
. Therefore, z has a constant value on any connected component of
G. It follows that every vector z ∈ Ker (B> ) can be written uniquely as a linear combination
z = λ1z
1 + · · · + λcz
c
,
where the vector z
i
corresponds to the ith connected component Ki of G and is defined such
that
z
i
j =
(
0 otherwise
1 iff vj ∈ K
.
i
This shows that dim(Ker (B> )) = c, and that Ker (B> ) has a basis consisting of indicator
vectors.
Since B> is a n × m matrix, we have
m = dim(Ker (B
> )) + rank(B
> ),
and since we just proved that dim(Ker (B> )) = c, we obtain rank(B> ) = m − c. Since B
and B> have the same rank, rank(B) = m − c, as claimed.
Definition 20.12. Following common practice, we denote by 1 the (column) vector (of
dimension m) whose components are all equal to 1.
Since every column of B contains a single +1 and a single −1, the rows of B> sum to
zero, which can be expressed as
B
> 1 = 0.
According to Proposition 20.1, the graph G is connected iff B has rank m−1 iff the nullspace
of B> is the one-dimensional space spanned by 1.
In many applications, the notion of graph needs to be generalized to capture the intuitive
idea that two nodes u and v are linked with a degree of certainty (or strength). Thus, we
assign a nonnegative weight wi j to an edge {vi
, vj}; the smaller wi j is, the weaker is the
link (or similarity) between vi and vj
, and the greater wi j is, the stronger is the link (or
similarity) between vi and vj
.
Definition 20.13. A weighted graph is a pair G = (V, W), where V = {v1, . . . , vm} is a
set of nodes or vertices, and W is a symmetric matrix called the weight matrix , such that
wi j ≥ 0 for all i, j ∈ {1, . . . , m}, and wi i = 0 for i = 1, . . . , m. We say that a set {vi
, vj} is an
edge iff wi j > 0. The corresponding (undirected) graph (V, E) with E = {{vi
, vj} | wi j > 0},
is called the underlying graph of G.
708 CHAPTER 20. GRAPHS AND GRAPH LAPLACIANS; BASIC FACTS
Remark: Since wi i = 0, these graphs have no self-loops. We can think of the matrix W as
a generalized adjacency matrix. The case where wi j ∈ {0, 1} is equivalent to the notion of a
graph as in Definition 20.5.
We can think of the weight wi j of an edge {vi
, vj} as a degree of similarity (or affinity) in
an image, or a cost in a network. An example of a weighted graph is shown in Figure 20.4.
The thickness of an edge corresponds to the magnitude of its weight.
15
Encode Pairwise Relationships as a Weighted Graph
Figure 20.4: A weighted graph.
Definition 20.14. Given a weighted graph G = (V, W), for every node vi ∈ V , the degree
d(vi) of vi
is the sum of the weights of the edges adjacent to vi
:
d(vi) =
mX
j=1
wi j .
Note that in the above sum, only nodes vj such that there is an edge {vi
, vj} have a nonzero
contribution. Such nodes are said to be adjacent to vi
, and we write vi ∼ vj
. The degree
matrix D(G) (or simply, D) is defined as before, namely by D(G) = diag(d(v1), . . . , d(vm)).
The weight matrix W can be viewed as a linear map from R
V
to itself. For all x ∈ R
m,
we have
(W x)i =
X
j∼i
wijxj
;
that is, the value of W x at vi
is the weighted sum of the values of x at the nodes vj adjacent
to vi
.
Observe that W1 is the (column) vector (d(v1), . . . , d(vm)) consisting of the degrees of
the nodes of the graph.
We now define the most important concept of this chapter: the Laplacian matrix of a
graph. Actually, as we will see, it comes in several flavors.
20.2. LAPLACIAN MATRICES OF GRAPHS 709
20.2 Laplacian Matrices of Graphs
Let us begin with directed graphs, although as we will see, graph Laplacians are funda￾mentally associated with undirected graph. The key proposition below shows how given an
undirected graph G, for any orientation σ of G, Bσ
(Bσ
)
> relates to the adjacency matrix
A (where Bσ
is the incidence matrix of the directed graph Gσ
). We reproduce the proof in
Gallier [71] (see also Godsil and Royle [77]).
Proposition 20.2. Given any undirected graph G, for any orientation σ of G, if Bσ
is the
incidence matrix of the directed graph Gσ
, A is the adjacency matrix of Gσ
, and D is the
degree matrix such that Di i = d(vi), then
B
σ
(B
σ
)
> = D − A.
Consequently, L = Bσ
(Bσ
)
> is independent of the orientation σ of G, and D−A is symmetric
and positive semidefinite; that is, the eigenvalues of D − A are real and nonnegative.
Proof. The entry Bσ
(Bσ
)
>i j is the inner product of the ith row b
σ
i
, and the jth row b
σ
j
of Bσ
.
If i = j, then as
b
σ
i k =



+1 if
−1 if
s
t(
(
e
e
k
k
) =
) =
v
v
i
i
0 otherwise
we see that b
σ
i
· b
σ
i = d(vi). If i 6 = j, then b
σ
i
· b
σ
j 6 = 0 iff there is some edge ek with s(ek) = vi
and t(ek) = vj or vice-versa (which are mutually exclusive cases, since Gσ arises by orienting
an undirected graph), in which case, b
σ
i
· b
σ
j = −1. Therefore,
B
σ
(B
σ
)
> = D − A,
as claimed.
For every x ∈ R
m, we have
x
> Lx = x
> B
σ
(B
σ
)
> x = ((B
σ
)
> x)
> (B
σ
)
> x =
  (B
σ
)
> x

2
2
≥ 0,
since the Euclidean norm k k 2
is positive (definite). Therefore, L = Bσ
(Bσ
)
> is positive
semidefinite. It is well-known that a real symmetric matrix is positive semidefinite iff its
eigenvalues are nonnegative.
Definition 20.15. The matrix L = Bσ
(Bσ
)
> = D − A is called the (unnormalized) graph
Laplacian of the graph Gσ
. The (unnormalized) graph Laplacian of an undirected graph
G = (V, E) is defined by
L = D − A.
710 CHAPTER 20. GRAPHS AND GRAPH LAPLACIANS; BASIC FACTS
For example, the graph Laplacian of graph G1 is
L =


−
−
2
1 4
1
−
−
1
1 3
−
−
1 0 0
1 −1 −1
−1 0
0
0
−
−
1
1 0
−1 3
−1 2
−1


.
Observe that each row of L sums to zero (because (Bσ
)
> 1 = 0). Consequently, the vector
1 is in the nullspace of L.
Remarks:
1. With the unoriented version of the incidence matrix (see Definition 20.8), it can be
shown that
BB> = D + A.
2. As pointed out by Evangelos Chatzipantazis, Proposition 20.2 in which the incidence
matrix Bσ
is replaced by the incidence matrix B of any arbitrary directed graph G
does not hold. The problem is that such graphs may have both edges (vi
, vj ) and
(vj
, vi) between two distinct nodes vi and vj
, and as a consequence, the inner product
bi
· bj = −2 instead of −1. A simple counterexample is given by the directed graph
with three vertices and four edges whose incidence matrix is given by
B =

−
1
0 0 1 1
1 1
−1 0
−1 0
−1

 .
We have
BB> =

−
−
3
2 3
1
−
−
2
1 2
−
−
1
1

 6 =


3 0 0
0 3 0
0 0 2

 −


0 1 1
1 0 1
1 1 0

 = D − A.
The natural generalization of the notion of graph Laplacian to weighted graphs is this:
Definition 20.16. Given any weighted graph G = (V, W) with V = {v1, . . . , vm}, the
(unnormalized) graph Laplacian L(G) of G is defined by
L(G) = D(G) − W,
where D(G) = diag(d1, . . . , dm) is the degree matrix of G (a diagonal matrix), with
di =
mX
j=1
wi j .
As usual, unless confusion arises, we write D instead of D(G) and L instead of L(G).
20.2. LAPLACIAN MATRICES OF GRAPHS 711
The graph Laplacian can be interpreted as a linear map from R
V
to itself. For all x ∈ R
V
,
we have
(Lx)i =
X
j∼i
wij (xi − xj ).
It is clear from the equation L = D − W that each row of L sums to 0, so the vector 1
is the nullspace of L, but it is less obvious that L is positive semidefinite. One way to prove
it is to generalize slightly the notion of incidence matrix.
Definition 20.17. Given a weighted graph G = (V, W), with V = {v1, . . . , vm}, if {e1, . . .,
en} are the edges of the underlying graph of G (recall that {vi
, vj} is an edge of this graph
iff wij > 0), for any oriented graph Gσ obtained by giving an orientation to the underlying
graph of G, the incidence matrix Bσ of Gσ
is the m × n matrix whose entries bi j are given
by
bi j =



+
√wij if s(ej ) = vi
−
√wij if t(ej ) = vi
0 otherwise.
For example, given the weight matrix
W =


0 3 6 3
3 0 0 3
6 0 0 3
3 3 3 0

 ,
the incidence matrix B corresponding to the orientation of the underlying graph of W where
an edge (i, j) is oriented positively iff i < j is
B =


1.7321 2.4495 1.7321 0 0
−1.
0
7321 0 0 1
−2.4495 0 0 1
.7321 0
.7321
0 0 −1.7321 −1.7321 −1.7321

 .
The reader should verify that BB> = D − W. This is true in general, see Proposition 20.3.
It is easy to see that Proposition 20.1 applies to the underlying graph of G. For any
oriented graph Gσ obtained from the underlying graph of G, the rank of the incidence matrix
Bσ
is equal to m−c, where c is the number of connected components of the underlying graph
of G, and we have (Bσ
)
> 1 = 0. We also have the following version of Proposition 20.2 whose
proof is immediately adapted.
Proposition 20.3. Given any weighted graph G = (V, W) with V = {v1, . . . , vm}, if Bσ
is
the incidence matrix of any oriented graph Gσ obtained from the underlying graph of G and
D is the degree matrix of G, then
B
σ
(B
σ
)
> = D − W = L.
712 CHAPTER 20. GRAPHS AND GRAPH LAPLACIANS; BASIC FACTS
Consequently, Bσ
(Bσ
)
> is independent of the orientation of the underlying graph of G and
L = D − W is symmetric and positive semidefinite; that is, the eigenvalues of L = D − W
are real and nonnegative.
Another way to prove that L is positive semidefinite is to evaluate the quadratic form
x
> Lx.
Proposition 20.4. For any m × m symmetric matrix W = (wij ), if we let L = D − W
where D is the degree matrix associated with W (that is, di =
P
m
j=1 wij ), then we have
x
> Lx =
1
2
mX
i,j=1
