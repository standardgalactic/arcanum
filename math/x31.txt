Let p: E → E/Ker (f) be the canonical surjection, f : E/Ker (f) → Im f be the isomor￾phism induced by f, and j : Im f → F be the inclusion map. Then, we have
f = j ◦ f ◦ p,
which implies that
f
> = p
> ◦ f
> ◦ j
> .
Since p is surjective, p
> is injective, since j is injective, j
> is surjective, and since f is
bijective, f
> is also bijective. It follows that (E/Ker (f))∗ = Im(f
> ◦ j
> ), and we have
Im f
> = Im p
> .
Since p: E → E/Ker (f) is the canonical surjection, by Proposition 11.9 applied to U =
Ker (f), we get
Im f
> = Im p
> = (Ker (f))0
,
as claimed.
In summary, the equation
Im f
> = (Ker (f))0
applies in any dimension, and it implies that
Ker (f) = (Im f
> )
0
.
The following proposition shows the relationship between the matrix representing a linear
map f : E → F and the matrix representing its transpose f
> : F
∗ → E
∗
.
Proposition 11.14. Let E and F be two vector spaces, and let (u1, . . . , un) be a basis for E
and (v1, . . . , vm) be a basis for F. Given any linear map f : E → F, if M(f) is the m × n￾matrix representing f w.r.t. the bases (u1, . . . , un) and (v1, . . . , vm), then the n × m-matrix
M(f
> ) representing f
> : F
∗ → E
∗ w.r.t. the dual bases (v1
∗
, . . . , vm
∗
) and (u
∗
1
, . . . , u∗
n
) is the
transpose M(f)
> of M(f).
428 CHAPTER 11. THE DUAL SPACE AND DUALITY
Proof. Recall that the entry ai j in row i and column j of M(f) is the i-th coordinate of
f(uj ) over the basis (v1, . . . , vm). By definition of vi
∗
, we have h vi
∗
, f(uj )i = ai j . The entry
a
>j i in row j and column i of M(f
> ) is the j-th coordinate of
f
> (vi
∗
) = a
>1 iu
∗
1 + · · · + a
>j iu
∗
j + · · · + a
>n iu
∗
n
over the basis (u
∗
1
, . . . , u∗
n
), which is just a
>j i = f
> (vi
∗
)(uj ) = h f
> (vi
∗
), uj i
. Since
h
vi
∗
, f(uj )i = h f
> (vi
∗
), uj i
,
we have ai j = a
>j i, proving that M(f
> ) = M(f)
> .
We now can give a very short proof of the fact that the rank of a matrix is equal to the
rank of its transpose.
Proposition 11.15. Given an m × n matrix A over a field K, we have rk(A) = rk(A> ).
Proof. The matrix A corresponds to a linear map f : Kn → Km, and by Theorem 11.12,
rk(f) = rk(f
> ). By Proposition 11.14, the linear map f
> corresponds to A> . Since rk(A) =
rk(f), and rk(A> ) = rk(f
> ), we conclude that rk(A) = rk(A> ).
Thus, given an m×n-matrix A, the maximum number of linearly independent columns is
equal to the maximum number of linearly independent rows. There are other ways of proving
this fact that do not involve the dual space, but instead some elementary transformations
on rows and columns.
Proposition 11.15 immediately yields the following criterion for determining the rank of
a matrix:
Proposition 11.16. Given any m×n matrix A over a field K (typically K = R or K = C),
the rank of A is the maximum natural number r such that there is an invertible r×r submatrix
of A obtained by selecting r rows and r columns of A.
For example, the 3 × 2 matrix
A =


a11 a12
a21 a22
a31 a32


has rank 2 iff one of the three 2 × 2 matrices

a11 a12
a21 a22 
a11 a12
a31 a32 
a21 a22
a31 a32
is invertible.
If we combine Proposition 7.11 with Proposition 11.16, we obtain the following criterion
for finding the rank of a matrix.
11.7. PROPERTIES OF THE DOUBLE TRANSPOSE 429
Proposition 11.17. Given any m×n matrix A over a field K (typically K = R or K = C),
the rank of A is the maximum natural number r such that there is an r × r submatrix B of
A obtained by selecting r rows and r columns of A, such that det(B) 6 = 0.
This is not a very efficient way of finding the rank of a matrix. We will see that there
are better ways using various decompositions such as LU, QR, or SVD.
11.7 Properties of the Double Transpose
First we have the following property showing the naturality of the eval map.
Proposition 11.18. For any linear map f : E → F, we have
f
>> ◦ evalE = evalF ◦ f,
or equivalently the following diagram commutes:
E
∗∗ f>>
/
F
∗∗
E
evalE
O
O
f
/
F.
evalF
O
O
Proof. For every u ∈ E and every ϕ ∈ F
∗
, we have
(f
>> ◦ evalE)(u)(ϕ) = h f
>> (evalE(u)), ϕi
= h evalE(u), f > (ϕ)i
= h f
> (ϕ), ui
= h ϕ, f(u)i
= h evalF (f(u)), ϕi
= h (evalF ◦ f)(u), ϕi
= (evalF ◦ f)(u)(ϕ),
which proves that f
>> ◦ evalE = evalF ◦ f, as claimed.
If E and F are finite-dimensional, then evalE and evalF are isomorphisms, so Proposition
11.18 shows that
f
>> = evalF ◦ f ◦ eval−
E
1
. (∗)
The above equation is often interpreted as follows: if we identify E with its bidual E
∗∗ and
F with its bidual F
∗∗, then f
>> = f. This is an abuse of notation; the rigorous statement
is (∗).
As a corollary of Proposition 11.18, we obtain the following result.
430 CHAPTER 11. THE DUAL SPACE AND DUALITY
Proposition 11.19. If dim(E) is finite, then we have
Ker (f
>> ) = evalE(Ker (f)).
Proof. Indeed, if E is finite-dimensional, the map evalE : E → E
∗∗ is an isomorphism, so
every ϕ ∈ E
∗∗ is of the form ϕ = evalE(u) for some u ∈ E, the map evalF : F → F
∗∗ is
injective, and we have
f
>> (ϕ) = 0 iff f
>> (evalE(u)) = 0
iff evalF (f(u)) = 0
iff f(u) = 0
iff u ∈ Ker (f)
iff ϕ ∈ evalE(Ker (f)),
which proves that Ker (f
>> ) = evalE(Ker (f)).
Remarks: If dim(E) is finite, following an argument of Dan Guralnik, the fact that rk(f) =
rk(f
> ) can be proven using Proposition 11.19.
Proof. We know from Proposition 11.11 applied to f
> : F
∗ → E
∗
that
Ker (f
>> ) = (Im f
> )
0
,
and we showed in Proposition 11.19 that
Ker (f
>> ) = evalE(Ker (f)).
It follows (since evalE is an isomorphism) that
dim((Im f
> )
0
) = dim(Ker (f
>> )) = dim(Ker (f)) = dim(E) − dim(Im f),
and since
dim(Im f
> ) + dim((Im f
> )
0
) = dim(E),
we get
dim(Im f
> ) = dim(Im f).
As indicated by Dan Guralnik, if dim(E) is finite, the above result can be used to prove
the following result.
Proposition 11.20. If dim(E) is finite, then for any linear map f : E → F, we have
Im f
> = (Ker (f))0
.
11.8. THE FOUR FUNDAMENTAL SUBSPACES 431
Proof. From
h
f
> (ϕ), ui = h ϕ, f(u)i
for all ϕ ∈ F
∗ and all u ∈ E, we see that if u ∈ Ker (f), then h f
> (ϕ), ui = h ϕ, 0i = 0,
which means that f
> (ϕ) ∈ (Ker (f))0
, and thus, Im f
> ⊆ (Ker (f))0
. For the converse, since
dim(E) is finite, we have
dim((Ker (f))0
) = dim(E) − dim(Ker (f)) = dim(Im f),
but we just proved that dim(Im f
> ) = dim(Im f), so we get
dim((Ker (f))0
) = dim(Im f
> ),
and since Im f
> ⊆ (Ker (f))0
, we obtain
Im f
> = (Ker (f))0
,
as claimed.
Remarks:
1. By the duality theorem, since (Ker (f))00 = Ker (f), the above equation yields another
proof of the fact that
Ker (f) = (Im f
> )
0
,
when E is finite-dimensional.
2. The equation
Im f
> = (Ker (f))0
is actually valid even if when E if infinite-dimensional, but we will not prove this here.
11.8 The Four Fundamental Subspaces
Given a linear map f : E → F (where E and F are finite-dimensional), Proposition 11.11
revealed that the four spaces
Im f, Im f
> , Ker f, Ker f
>
play a special role. They are often called the fundamental subspaces associated with f. These
spaces are related in an intimate manner, since Proposition 11.11 shows that
Ker f = (Im f
> )
0
Ker f
> = (Im f)
0
,
432 CHAPTER 11. THE DUAL SPACE AND DUALITY
and Theorem 11.12 shows that
rk(f) = rk(f
> ).
It is instructive to translate these relations in terms of matrices (actually, certain linear
algebra books make a big deal about this!). If dim(E) = n and dim(F) = m, given any basis
(u1, . . . , un) of E and a basis (v1, . . . , vm) of F, we know that f is represented by an m × n
matrix A = (ai j ), where the jth column of A is equal to f(uj ) over the basis (v1, . . . , vm).
Furthermore, the transpose map f
> is represented by the n × m matrix A> (with respect to
the dual bases). Consequently, the four fundamental spaces
Im f, Im f
> , Ker f, Ker f
>
correspond to
(1) The column space of A, denoted by Im A or R(A); this is the subspace of R
m spanned
by the columns of A, which corresponds to the image Im f of f.
(2) The kernel or nullspace of A, denoted by Ker A or N (A); this is the subspace of R
n
consisting of all vectors x ∈ R
n
such that Ax = 0.
(3) The row space of A, denoted by Im A> or R(A> ); this is the subspace of R
n
spanned
by the rows of A, or equivalently, spanned by the columns of A> , which corresponds
to the image Im f
> of f
> .
(4) The left kernel or left nullspace of A denoted by Ker A> or N (A> ); this is the kernel
(nullspace) of A> , the subspace of R
m consisting of all vectors y ∈ R
m such that
A> y = 0, or equivalently, y
> A = 0.
Recall that the dimension r of Im f, which is also equal to the dimension of the column
space Im A = R(A), is the rank of A (and f). Then, some our previous results can be
reformulated as follows:
1. The column space R(A) of A has dimension r.
2. The nullspace N (A) of A has dimension n − r.
3. The row space R(A> ) has dimension r.
4. The left nullspace N (A> ) of A has dimension m − r.
The above statements constitute what Strang calls the Fundamental Theorem of Linear
Algebra, Part I (see Strang [170]).
The two statements
Ker f = (Im f
> )
0
Ker f
> = (Im f)
0
translate to
11.8. THE FOUR FUNDAMENTAL SUBSPACES 433
(1) The nullspace of A is the orthogonal of the row space of A.
(2) The left nullspace of A is the orthogonal of the column space of A.
The above statements constitute what Strang calls the Fundamental Theorem of Linear
Algebra, Part II (see Strang [170]).
Since vectors are represented by column vectors and linear forms by row vectors (over a
basis in E or F), a vector x ∈ R
n
is orthogonal to a linear form y iff
yx = 0.
Then, a vector x ∈ R
n
is orthogonal to the row space of A iff x is orthogonal to every row
of A, namely Ax = 0, which is equivalent to the fact that x belong to the nullspace of A.
Similarly, the column vector y ∈ R
m (representing a linear form over the dual basis of F
∗
)
belongs to the nullspace of A> iff A> y = 0, iff y
> A = 0, which means that the linear form
given by y
> (over the basis in F) is orthogonal to the column space of A.
Since (2) is equivalent to the fact that the column space of A is equal to the orthogonal
of the left nullspace of A, we get the following criterion for the solvability of an equation of
the form Ax = b:
The equation Ax = b has a solution iff for all y ∈ R
m, if A> y = 0, then y
> b = 0.
Indeed, the condition on the right-hand side says that b is orthogonal to the left nullspace
of A; that is, b belongs to the column space of A.
This criterion can be cheaper to check that checking directly that b is spanned by the
columns of A. For example, if we consider the system
x1 − x2 = b1
x2 − x3 = b2
x3 − x1 = b3
which, in matrix form, is written Ax = b as below:

 0 1
1 −1 0
−1
−1 0 1




x
x
x
1
2
3

 =


b
b
b
1
2
3

 ,
we see that the rows of the matrix A add up to 0. In fact, it is easy to convince ourselves that
the left nullspace of A is spanned by y = (1, 1, 1), and so the system is solvable iff y
> b = 0,
namely
b1 + b2 + b3 = 0.
Note that the above criterion can also be stated negatively as follows:
The equation Ax = b has no solution iff there is some y ∈ R
m such that A> y = 0 and
y
> b 6 = 0.
434 CHAPTER 11. THE DUAL SPACE AND DUALITY
Since A> y = 0 iff y
> A = 0, we can view y
> as a row vector representing a linear form,
and y
> A = 0 asserts that the linear form y
> vanishes on the columns A1
, . . . , An of A but
does not vanish on b. Since the linear form y
> defines the hyperplane H of equation y
> z = 0
(with z ∈ R
m), geometrically the equation Ax = b has no solution iff there is a hyperplane
H containing A1
, . . . , An and not containing b.
11.9 Summary
The main concepts and results of this chapter are listed below:
• The dual space E
∗ and linear forms (covector ). The bidual E
∗∗
.
• The bilinear pairing h−, −i: E
∗ × E → K (the canonical pairing).
• Evaluation at v: evalv : E
∗ → K.
• The map evalE : E → E
∗∗
.
• Othogonality between a subspace V of E and a subspace U of E
∗
; the orthogonal V
0
and the orthogonal U
0
.
• Coordinate forms.
• The Duality theorem (Theorem 11.4).
• The dual basis of a basis.
• The isomorphism evalE : E → E
∗∗ when dim(E) is finite.
• Pairing between two vector spaces; nondegenerate pairing; Proposition 11.6.
• Hyperplanes and linear forms.
• The transpose f
> : F
∗ → E
∗ of a linear map f : E → F.
• The fundamental identities:
Ker f
> = (Im f)
0
and Ker f = (Im f
> )
0
(Proposition 11.11).
• If F is finite-dimensional, then
rk(f) = rk(f
> ).
(Theorem 11.12).
11.10. PROBLEMS 435
• The matrix of the transpose map f
> is equal to the transpose of the matrix of the map
f (Proposition 11.14).
• For any m × n matrix A,
rk(A) = rk(A
> ).
• Characterization of the rank of a matrix in terms of a maximal invertible submatrix
(Proposition 11.16).
• The four fundamental subspaces:
Im f, Im f
> , Ker f, Ker f
> .
• The column space, the nullspace, the row space, and the left nullspace (of a matrix).
• Criterion for the solvability of an equation of the form Ax = b in terms of the left
nullspace.
11.10 Problems
Problem 11.1. Prove the following properties of transposition:
(f + g)
> = f
> + g
>
(g ◦ f)
> = f
> ◦ g
>
id>E = idE∗ .
Problem 11.2. Let (u1, . . . , un−1) be n − 1 linearly independent vectors ui ∈ C
n
. Prove
that the hyperlane H spanned by (u1, . . . , un−1) is the nullspace of the linear form
x 7→ det(u1, . . . , un−1, x), x ∈ C
n
.
Prove that if A is the n × n matrix whose columns are (u1, . . . , un−1, x), and if ci =
(−1)i+n det(Ain) is the cofactor of ain = xi
for i = 1, . . . , n, then H is defined by the
equation
c1x1 + · · · + cnxn = 0.
Problem 11.3. (1) Let ϕ: R
n × R
n → R be the map defined by
ϕ((x1, . . . , xn),(y1, . . . , yn)) = x1y1 + · · · + xnyn.
Prove that ϕ is a bilinear nondegenerate pairing. Deduce that (R
n
)
∗
is isomorphic to R
n
.
Prove that ϕ(x, x) = 0 iff x = 0.
436 CHAPTER 11. THE DUAL SPACE AND DUALITY
(2) Let ϕL : R
4 × R
4 → R be the map defined by
ϕL((x1, x2, x3, x4),(y1, y2, y3, , y4)) = x1y1 − x2y2 − x3y3 − x4y4.
Prove that ϕ is a bilinear nondegenerate pairing.
Show that there exist nonzero vectors x ∈ R
4
such that ϕL(x, x) = 0.
Remark: The vector space R
4
equipped with the above bilinear form called the Lorentz
form is called Minkowski space.
Problem 11.4. Given any two subspaces V1, V2 of a finite-dimensional vector space E, prove
that
(V1 + V2)
0 = V1
0 ∩ V2
0
(V1 ∩ V2)
0 = V1
0 + V2
0
.
Beware that in the second equation, V1 and V2 are subspaces of E, not E
∗
.
Hint. To prove the second equation, prove the inclusions V1
0+V2
0 ⊆ (V1∩V2)
0 and (V1∩V2)
0 ⊆
V1
0 + V2
0
. Proving the second inclusion is a little tricky. First, prove that we can pick a
subspace W1 of V1 and a subspace W2 of V2 such that
1. V1 is the direct sum V1 = (V1 ∩ V2) ⊕ W1.
2. V2 is the direct sum V2 = (V1 ∩ V2) ⊕ W2.
3. V1 + V2 is the direct sum V1 + V2 = (V1 ∩ V2) ⊕ W1 ⊕ W2.
Problem 11.5. (1) Let A be any n × n matrix such that the sum of the entries of every
row of A is the same (say c1), and the sum of entries of every column of A is the same (say
c2). Prove that c1 = c2.
(2) Prove that for any n ≥ 2, the 2n − 2 equations asserting that the sum of the entries
of every row of A is the same, and the sum of entries of every column of A is the same are
lineary independent. For example, when n = 4, we have the following 6 equations
a11 + a12 + a13 + a14 − a21 − a22 − a23 − a24 = 0
a21 + a22 + a23 + a24 − a31 − a32 − a33 − a34 = 0
a31 + a32 + a33 + a34 − a41 − a42 − a43 − a44 = 0
a11 + a21 + a31 + a41 − a12 − a22 − a32 − a42 = 0
a12 + a22 + a32 + a42 − a13 − a23 − a33 − a43 = 0
a13 + a23 + a33 + a43 − a14 − a24 − a34 − a44 = 0.
Hint. Group the equations as above; that is, first list the n − 1 equations relating the rows,
and then list the n − 1 equations relating the columns. Prove that the first n − 1 equations
11.10. PROBLEMS 437
are linearly independent, and that the last n − 1 equations are also linearly independent.
Then, find a relationship between the two groups of equations that will allow you to prove
that they span subspace V
r and V
c
such that V
r ∩ V
c = (0).
(3) Now consider magic squares. Such matrices satisfy the two conditions about the sum
of the entries in each row and in each column to be the same number, and also the additional
two constraints that the main descending and the main ascending diagonals add up to this
common number. Traditionally, it is also required that the entries in a magic square are
positive integers, but we will consider generalized magic square with arbitrary real entries.
For example, in the case n = 4, we have the following system of 8 equations:
a11 + a12 + a13 + a14 − a21 − a22 − a23 − a24 = 0
a21 + a22 + a23 + a24 − a31 − a32 − a33 − a34 = 0
a31 + a32 + a33 + a34 − a41 − a42 − a43 − a44 = 0
a11 + a21 + a31 + a41 − a12 − a22 − a32 − a42 = 0
a12 + a22 + a32 + a42 − a13 − a23 − a33 − a43 = 0
a13 + a23 + a33 + a43 − a14 − a24 − a34 − a44 = 0
a22 + a33 + a44 − a12 − a13 − a14 = 0
a41 + a32 + a23 − a11 − a12 − a13 = 0.
In general, the equation involving the descending diagonal is
a22 + a33 + · · · + ann − a12 − a13 − · · · − a1n = 0 (r)
and the equation involving the ascending diagonal is
an1 + an−12 + · · · + a2n−1 − a11 − a12 − · · · − a1n−1 = 0. (c)
Prove that if n ≥ 3, then the 2n equations asserting that a matrix is a generalized magic
square are linearly independent.
Hint. Equations are really linear forms, so find some matrix annihilated by all equations
except equation r, and some matrix annihilated by all equations except equation c.
Problem 11.6. Let U1, . . . , Up be some subspaces of a vector space E, and assume that
they form a direct sum U = U1 ⊕ · · · ⊕ Up. Let ji
: Ui → U1 ⊕ · · · ⊕ Up be the canonical
injections, and let πi
: U1
∗ × · · · × Up
∗ → Ui
∗ be the canonical projections. Prove that there is
an isomorphism f from (U1 ⊕ · · · ⊕ Up)
∗
to U1
∗ × · · · × Up
∗
such that
πi ◦ f = ji
>
, 1 ≤ i ≤ p.
Problem 11.7. Let U and V be two subspaces of a vector space E such that E = U ⊕ V .
Prove that
E
∗ = U
0 ⊕ V
0
.
438 CHAPTER 11. THE DUAL SPACE AND DUALITY
Chapter 12
Euclidean Spaces
Rien n’est beau que le vrai.
—Hermann Minkowski
12.1 Inner Products, Euclidean Spaces
So far the framework of vector spaces allows us to deal with ratios of vectors and linear
combinations, but there is no way to express the notion of angle or to talk about orthogonality
of vectors. A Euclidean structure allows us to deal with metric notions such as angles,
orthogonality, and length (or distance).
This chapter covers the bare bones of Euclidean geometry. Deeper aspects of Euclidean
geometry are investigated in Chapter 13. One of our main goals is to give the basic properties
of the transformations that preserve the Euclidean structure, rotations and reflections, since
they play an important role in practice. Euclidean geometry is the study of properties
invariant under certain affine maps called rigid motions. Rigid motions are the maps that
preserve the distance between points.
We begin by defining inner products and Euclidean spaces. The Cauchy–Schwarz in￾equality and the Minkowski inequality are shown. We define orthogonality of vectors and of
subspaces, orthogonal bases, and orthonormal bases. We prove that every finite-dimensional
Euclidean space has orthonormal bases. The first proof uses duality and the second one the
Gram–Schmidt orthogonalization procedure. The QR-decomposition for invertible matrices
is shown as an application of the Gram–Schmidt procedure. Linear isometries (also called or￾thogonal transformations) are defined and studied briefly. We conclude with a short section
in which some applications of Euclidean geometry are sketched. One of the most important
applications, the method of least squares, is discussed in Chapter 23.
For a more detailed treatment of Euclidean geometry see Berger [11, 12], Snapper and
Troyer [162], or any other book on geometry, such as Pedoe [136], Coxeter [44], Fresnel [65],
Tisseron [175], or Cagnac, Ramis, and Commeau [32]. Serious readers should consult Emil
439
440 CHAPTER 12. EUCLIDEAN SPACES
Artin’s famous book [6], which contains an in-depth study of the orthogonal group, as well
as other groups arising in geometry. It is still worth consulting some of the older classics,
such as Hadamard [84, 85] and Rouch´e and de Comberousse [139]. The first edition of [84]
was published in 1898 and finally reached its thirteenth edition in 1947! In this chapter it is
assumed that all vector spaces are defined over the field R of real numbers unless specified
otherwise (in a few cases, over the complex numbers C).
First we define a Euclidean structure on a vector space. Technically, a Euclidean structure
over a vector space E is provided by a symmetric bilinear form on the vector space satisfying
some extra properties. Recall that a bilinear form ϕ: E × E → R is definite if for every
u ∈ E, u 6 = 0 implies that ϕ(u, u) 6 = 0, and positive if for every u ∈ E, ϕ(u, u) ≥ 0.
Definition 12.1. A Euclidean space is a real vector space E equipped with a symmetric
bilinear form ϕ: E ×E → R that is positive definite. More explicitly, ϕ: E ×E → R satisfies
the following axioms:
ϕ(u1 + u2, v) = ϕ(u1, v) + ϕ(u2, v),
ϕ(u, v1 + v2) = ϕ(u, v1) + ϕ(u, v2),
ϕ(λu, v) = λϕ(u, v),
ϕ(u, λv) = λϕ(u, v),
ϕ(u, v) = ϕ(v, u),
u 6 = 0 implies that ϕ(u, u) > 0.
The real number ϕ(u, v) is also called the inner product (or scalar product) of u and v. We
also define the quadratic form associated with ϕ as the function Φ: E → R+ such that
Φ(u) = ϕ(u, u),
for all u ∈ E.
Since ϕ is bilinear, we have ϕ(0, 0) = 0, and since it is positive definite, we have the
stronger fact that
ϕ(u, u) = 0 iff u = 0,
that is, Φ(u) = 0 iff u = 0.
Given an inner product ϕ: E × E → R on a vector space E, we also denote ϕ(u, v) by
u · v or h u, vi or (u|v),
and p Φ(u) by k uk .
Example 12.1. The standard example of a Euclidean space is R
n
, under the inner product
· defined such that
(x1, . . . , xn) · (y1, . . . , yn) = x1y1 + x2y2 + · · · + xnyn.
This Euclidean space is denoted by E
n
.
12.1. INNER PRODUCTS, EUCLIDEAN SPACES 441
There are other examples.
Example 12.2. For instance, let E be a vector space of dimension 2, and let (e1, e2) be a
basis of E. If a > 0 and b
2 − ac < 0, the bilinear form defined such that
ϕ(x1e1 + y1e2, x2e1 + y2e2) = ax1x2 + b(x1y2 + x2y1) + cy1y2
yields a Euclidean structure on E. In this case,
Φ(xe1 + ye2) = ax2 + 2bxy + cy2
.
Example 12.3. Let C[a, b] denote the set of continuous functions f : [a, b] → R. It is
easily checked that C[a, b] is a vector space of infinite dimension. Given any two functions
f, g ∈ C[a, b], let
h
f, gi =
Z
b
a
f(t)g(t)dt.
We leave it as an easy exercise that h−, −i is indeed an inner product on C[a, b]. In the case
where a = −π and b = π (or a = 0 and b = 2π, this makes basically no difference), one
should compute
h
sin px,sin qxi , h sin px, cos qxi , and h cos px, cos qxi ,
for all natural numbers p, q ≥ 1. The outcome of these calculations is what makes Fourier
analysis possible!
Example 12.4. Let E = Mn(R) be the vector space of real n × n matrices. If we view
a matrix A ∈ Mn(R) as a “long” column vector obtained by concatenating together its
columns, we can define the inner product of two matrices A, B ∈ Mn(R) as
h
A, Bi =
nX
i,j=1
aij bij ,
which can be conveniently written as
h
A, Bi = tr(A
> B) = tr(B
> A).
Since this can be viewed as the Euclidean product on R
n
2
, it is an inner product on Mn(R).
The corresponding norm
k
Ak F =
p tr(A> A)
is the Frobenius norm (see Section 9.2).
Let us observe that ϕ can be recovered from Φ.
442 CHAPTER 12. EUCLIDEAN SPACES
Proposition 12.1. We have
ϕ(u, v) = 1
2
[Φ(u + v) − Φ(u) − Φ(v)]
for all u, v ∈ E. We say that ϕ is the polar form of Φ.
Proof. By bilinearity and symmetry, we have
Φ(u + v) = ϕ(u + v, u + v)
= ϕ(u, u + v) + ϕ(v, u + v)
= ϕ(u, u) + 2ϕ(u, v) + ϕ(v, v)
= Φ(u) + 2ϕ(u, v) + Φ(v).
If E is finite-dimensional and if ϕ: E × E → R is a bilinear form on E, given any basis
(e1, . . . , en) of E, we can write x =
P
n
i=1 xiei and y =
P
n
j=1 yjej
, and we have
ϕ(x, y) = ϕ

nX
i=1
xiei
,
nX
j=1
yjej
 =
X
n
i,j=1
xiyjϕ(ei
, ej ).
If we let G be the matrix G = (ϕ(ei
, ej )), and if x and y are the column vectors associated
with (x1, . . . , xn) and (y1, . . . , yn), then we can write
ϕ(x, y) = x
> Gy = y
> G
> x.
Note that we are committing an abuse of notation since x =
P
n
i=1 xiei
is a vector in E, but
the column vector associated with (x1, . . . , xn) belongs to R
n
. To avoid this minor abuse, we
could denote the column vector associated with (x1, . . . , xn) by x (and similarly y for the
column vector associated with (y1, . . . , yn)), in wich case the “correct” expression for ϕ(x, y)
is
ϕ(x, y) = x
> Gy.
However, in view of the isomorphism between E and R
n
, to keep notation as simple as
possible, we will use x and y instead of x and y.
Also observe that ϕ is symmetric iff G = G> , and ϕ is positive definite iff the matrix G
is positive definite, that is,
x
> Gx > 0 for all x ∈ R
n
, x 6 = 0.
The matrix G associated with an inner product is called the Gram matrix of the inner
product with respect to the basis (e1, . . . , en).
Conversely, if A is a symmetric positive definite n×n matrix, it is easy to check that the
bilinear form
h
x, yi = x
> Ay
12.1. INNER PRODUCTS, EUCLIDEAN SPACES 443
is an inner product. If we make a change of basis from the basis (e1, . . . , en) to the basis
(f1, . . . , fn), and if the change of basis matrix is P (where the jth column of P consists of
the coordinates of fj over the basis (e1, . . . , en)), then with respect to coordinates x
0 and y
0
over the basis (f1, . . . , fn), we have
x
> Gy = x
0> P
> GP y0 ,
so the matrix of our inner product over the basis (f1, . . . , fn) is P
> GP. We summarize these
facts in the following proposition.
Proposition 12.2. Let E be a finite-dimensional vector space, and let (e1, . . . , en) be a basis
of E.
1. For any inner product h−, −i on E, if G = (h ei
, ej i ) is the Gram matrix of the inner
product h−, −i w.r.t. the basis (e1, . . . , en), then G is symmetric positive definite.
2. For any change of basis matrix P, the Gram matrix of h−, −i with respect to the new
basis is P
> GP.
3. If A is any n × n symmetric positive definite matrix, then
h
x, yi = x
> Ay
is an inner product on E.
We will see later that a symmetric matrix is positive definite iff its eigenvalues are all
positive.
One of the very important properties of an inner product ϕ is that the map u 7→
p Φ(u)
is a norm.
Proposition 12.3. Let E be a Euclidean space with inner product ϕ, and let Φ be the
corresponding quadratic form. For all u, v ∈ E, we have the Cauchy–Schwarz inequality
ϕ(u, v)
2 ≤ Φ(u)Φ(v),
the equality holding iff u and v are linearly dependent.
We also have the Minkowski inequality
p
Φ(u + v) ≤
p Φ(u) + p Φ(v),
the equality holding iff u and v are linearly dependent, where in addition if u 6 = 0 and v 6 = 0,
then u = λv for some λ > 0.
444 CHAPTER 12. EUCLIDEAN SPACES
Proof. For any vectors u, v ∈ E, we define the function T : R → R such that
T(λ) = Φ(u + λv),
for all λ ∈ R. Using bilinearity and symmetry, we have
Φ(u + λv) = ϕ(u + λv, u + λv)
= ϕ(u, u + λv) + λϕ(v, u + λv)
= ϕ(u, u) + 2λϕ(u, v) + λ
2ϕ(v, v)
= Φ(u) + 2λϕ(u, v) + λ
2Φ(v).
Since ϕ is positive definite, Φ is nonnegative, and thus T(λ) ≥ 0 for all λ ∈ R. If Φ(v) = 0,
then v = 0, and we also have ϕ(u, v) = 0. In this case, the Cauchy–Schwarz inequality is
trivial, and v = 0 and u are linearly dependent.
Now assume Φ(v) > 0. Since T(λ) ≥ 0, the quadratic equation
λ
2Φ(v) + 2λϕ(u, v) + Φ(u) = 0
cannot have distinct real roots, which means that its discriminant
∆ = 4(ϕ(u, v)
2 − Φ(u)Φ(v))
is null or negative, which is precisely the Cauchy–Schwarz inequality
ϕ(u, v)
2 ≤ Φ(u)Φ(v).
Let us now consider the case where we have the equality
ϕ(u, v)
2 = Φ(u)Φ(v).
There are two cases. If Φ(v) = 0, then v = 0 and u and v are linearly dependent. If Φ(v) 6 = 0,
then the above quadratic equation has a double root λ0, and we have Φ(u + λ0v) = 0. Since
ϕ is positive definite, Φ(u + λ0v) = 0 implies that u + λ0v = 0, which shows that u and v
are linearly dependent. Conversely, it is easy to check that we have equality when u and v
are linearly dependent.
The Minkowski inequality
p
Φ(u + v) ≤
p Φ(u) + p Φ(v)
is equivalent to
Φ(u + v) ≤ Φ(u) + Φ(v) + 2p Φ(u)Φ(v).
However, we have shown that
2ϕ(u, v) = Φ(u + v) − Φ(u) − Φ(v),
12.1. INNER PRODUCTS, EUCLIDEAN SPACES 445
and so the above inequality is equivalent to
ϕ(u, v) ≤
p Φ(u)Φ(v),
which is trivial when ϕ(u, v) ≤ 0, and follows from the Cauchy–Schwarz inequality when
ϕ(u, v) ≥ 0. Thus, the Minkowski inequality holds. Finally assume that u 6 = 0 and v 6 = 0,
and that
p
Φ(u + v) = p Φ(u) + p Φ(v).
When this is the case, we have
ϕ(u, v) = p Φ(u)Φ(v),
and we know from the discussion of the Cauchy–Schwarz inequality that the equality holds
iff u and v are linearly dependent. The Minkowski inequality is an equality when u or v is
null. Otherwise, if u 6 = 0 and v 6 = 0, then u = λv for some λ 6 = 0, and since
ϕ(u, v) = λϕ(v, v) = p Φ(u)Φ(v),
by positivity, we must have λ > 0.
Note that the Cauchy–Schwarz inequality can also be written as
|ϕ(u, v)| ≤ p Φ(u)
p Φ(v).
Remark: It is easy to prove that the Cauchy–Schwarz and the Minkowski inequalities still
hold for a symmetric bilinear form that is positive, but not necessarily definite (i.e., ϕ(u, v) ≥
0 for all u, v ∈ E). However, u and v need not be linearly dependent when the equality holds.
The Minkowski inequality
p
Φ(u + v) ≤
p Φ(u) + p Φ(v)
shows that the map u 7→
p Φ(u) satisfies the convexity inequality (also known as triangle
inequality), condition (N3) of Definition 9.1, and since ϕ is bilinear and positive definite, it
also satisfies conditions (N1) and (N2) of Definition 9.1, and thus it is a norm on E. The
norm induced by ϕ is called the Euclidean norm induced by ϕ.
The Cauchy–Schwarz inequality can be written as
|u · v| ≤ kukk vk ,
and the Minkowski inequality as
k
u + vk ≤ kuk + k vk .
446 CHAPTER 12. EUCLIDEAN SPACES
If u and v are nonzero vectors then the Cauchy–Schwarz inequality implies that
−1 ≤
u · v
k
uk k vk
≤ +1.
Then there is a unique θ ∈ [0, π] such that
cos θ =
u · v
k
uk k vk
.
We have u = v iff θ = 0 and u = −v iff θ = π. For 0 < θ < π, the vectors u and v are
linearly independent and there is an orientation of the plane spanned by u and v such that
θ is the angle between u and v. See Problem 12.8 for the precise notion of orientation. If u
is a unit vector (which means that k uk = 1), then the vector
(k vk cos θ)u = (u · v)u = (v · u)u
is called the orthogonal projection of v onto the space spanned by u.
Remark: One might wonder if every norm on a vector space is induced by some Euclidean
inner product. In general this is false, but remarkably, there is a simple necessary and
sufficient condition, which is that the norm must satisfy the parallelogram law:
k
u + vk
2 + k u − vk
2 = 2(k uk
2 + k vk
2
).
See Figure 12.1.
u
v
Figure 12.1: The parallelogram law states that the sum of the lengths of the diagonals of
the parallelogram determined by vectors u and v equals the sum of all the sides.
If h−, −i is an inner product, then we have
k
u + vk
2 = k uk
2 + k vk
2 + 2h u, vi
k
u − vk
