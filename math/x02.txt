y1 + z1
y2 + z2
y3 + z3

 .
49
50 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
Also, given a vector
x =


x1
x2
x3

 ,
we define the additive inverse −x of x (pronounced minus x) as
−x =


−x1
−x2
−x3

 .
Observe that −x = (−1)x, the scalar multiplication of x by −1.
The set of all vectors with three components is denoted by R
3×1
. The reason for using
the notation R
3×1
rather than the more conventional notation R
3
is that the elements of
R
3×1 are column vectors; they consist of three rows and a single column, which explains the
superscript 3 × 1. On the other hand, R
3 = R × R × R consists of all triples of the form
(x1, x2, x3), with x1, x2, x3 ∈ R, and these are row vectors. However, there is an obvious
bijection between R
3×1 and R
3 and they are usually identified. For the sake of clarity, in
this introduction, we will denote the set of column vectors with n components by R
n×1
.
An expression such as
x1u + x2v + x3w
where u, v, w are vectors and the xis are scalars (in R) is called a linear combination. Using
this notion, the problem of solving our linear system
x1u + x2v + x3w = b.
is equivalent to determining whether b can be expressed as a linear combination of u, v, w.
Now if the vectors u, v, w are linearly independent, which means that there is no triple
(x1, x2, x3) 6 = (0, 0, 0) such that
x1u + x2v + x3w = 03,
it can be shown that every vector in R
3×1
can be written as a linear combination of u, v, w.
Here, 03 is the zero vector
03 =


0
0
0

 .
It is customary to abuse notation and to write 0 instead of 03. This rarely causes a problem
because in most cases, whether 0 denotes the scalar zero or the zero vector can be inferred
from the context.
In fact, every vector z ∈ R
3×1
can be written in a unique way as a linear combination
z = x1u + x2v + x3w.
3.1. MOTIVATIONS: LINEAR COMBINATIONS, LINEAR INDEPENDENCE, RANK51
This is because if
z = x1u + x2v + x3w = y1u + y2v + y3w,
then by using our (linear!) operations on vectors, we get
(y1 − x1)u + (y2 − x2)v + (y3 − x3)w = 0,
which implies that
y1 − x1 = y2 − x2 = y3 − x3 = 0,
by linear independence. Thus,
y1 = x1, y2 = x2, y3 = x3,
which shows that z has a unique expression as a linear combination, as claimed. Then our
equation
x1u + x2v + x3w = b
has a unique solution, and indeed, we can check that
x1 = 1.4
x2 = −0.4
x3 = −0.4
is the solution.
But then, how do we determine that some vectors are linearly independent?
One answer is to compute a numerical quantity det(u, v, w), called the determinant of
(u, v, w), and to check that it is nonzero. In our case, it turns out that
det(u, v, w) =


 


1 2 −1
2 1 1
1 −2 −2






= 15,
which confirms that u, v, w are linearly independent.
Other methods, which are much better for systems with a large number of variables,
consist of computing an LU-decomposition or a QR-decomposition, or an SVD of the matrix
consisting of the three columns u, v, w,
A =
￾ u v w =


1 2 −1
2 1 1
1 −2 −2

 .
If we form the vector of unknowns
x =


x1
x2
x3

 ,
52 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
then our linear combination x1u + x2v + x3w can be written in matrix form as
x1u + x2v + x3w =


1 2 −1
2 1 1
1 −2 −2




x
x
x
1
2
3

 ,
so our linear system is expressed by


1 2
2 1 1
−1
1 −2 −2




x
x
x
1
2
3

 =


1
2
3

 ,
or more concisely as
Ax = b.
Now what if the vectors u, v, w are linearly dependent? For example, if we consider the
vectors
u =


1
2
1

 v =


2
1
−1

 w =


−1
1
2

 ,
we see that
u − v = w,
a nontrivial linear dependence. It can be verified that u and v are still linearly independent.
Now for our problem
x1u + x2v + x3w = b
it must be the case that b can be expressed as linear combination of u and v. However,
it turns out that u, v, b are linearly independent (one way to see this is to compute the
determinant det(u, v, b) = −6), so b cannot be expressed as a linear combination of u and v
and thus, our system has no solution.
If we change the vector b to
b =


3
3
0

 ,
then
b = u + v,
and so the system
x1u + x2v + x3w = b
has the solution
x1 = 1, x2 = 1, x3 = 0.
3.1. MOTIVATIONS: LINEAR COMBINATIONS, LINEAR INDEPENDENCE, RANK53
Actually, since w = u − v, the above system is equivalent to
(x1 + x3)u + (x2 − x3)v = b,
and because u and v are linearly independent, the unique solution in x1 + x3 and x2 − x3 is
x1 + x3 = 1
x2 − x3 = 1,
which yields an infinite number of solutions parameterized by x3, namely
x1 = 1 − x3
x2 = 1 + x3.
In summary, a 3 × 3 linear system may have a unique solution, no solution, or an infinite
number of solutions, depending on the linear independence (and dependence) or the vectors
u, v, w, b. This situation can be generalized to any n × n system, and even to any n × m
system (n equations in m variables), as we will see later.
The point of view where our linear system is expressed in matrix form as Ax = b stresses
the fact that the map x 7→ Ax is a linear transformation. This means that
A(λx) = λ(Ax)
for all x ∈ R
3×1 and all λ ∈ R and that
A(u + v) = Au + Av,
for all u, v ∈ R
3×1
. We can view the matrix A as a way of expressing a linear map from R
3×1
to R
3×1 and solving the system Ax = b amounts to determining whether b belongs to the
image of this linear map.
Given a 3 × 3 matrix
A =


a11 a12 a13
a21 a22 a23
a31 a32 a33

 ,
whose columns are three vectors denoted A1
, A2
, A3
, and given any vector x = (x1, x2, x3),
we defined the product Ax as the linear combination
Ax = x1A
1 + x2A
2 + x3A
3 =


a11x1 + a12x2 + a13x3
a21x1 + a22x2 + a23x3
a31x1 + a32x2 + a33x3

 .
The common pattern is that the ith coordinate of Ax is given by a certain kind of product
called an inner product, of a row vector , the ith row of A, times the column vector x:
￾
ai1 ai2 ai3
 ·


x1
x2
x3

 = ai1x1 + ai2x2 + ai3x3.
54 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
More generally, given any two vectors x = (x1, . . . , xn) and y = (y1, . . . , yn) ∈ R
n
, their
inner product denoted x · y, or h x, yi , is the number
x · y =
￾ x1 x2 · · · xn
 ·


y1
y2
.
.
y
.
n


=
nX
i=1
xiyi
.
Inner products play a very important role. First, we quantity
k
xk 2 =
√
x · x = (x
2
1 + · · · + x
2
n
)
1/2
is a generalization of the length of a vector, called the Euclidean norm, or ` 2
-norm. Second,
it can be shown that we have the inequality
|x · y| ≤ kxk k yk ,
so if x, y 6 = 0, the ratio (x · y)/(k xk k yk ) can be viewed as the cosine of an angle, the angle
between x and y. In particular, if x · y = 0 then the vectors x and y make the angle π/2,
that is, they are orthogonal. The (square) matrices Q that preserve the inner product, in
the sense that h Qx, Qyi = h x, yi for all x, y ∈ R
n
, also play a very important role. They can
be thought of as generalized rotations.
Returning to matrices, if A is an m × n matrix consisting of n columns A1
, . . . , An
(in
R
m), and B is a n × p matrix consisting of p columns B1
, . . . , Bp
(in R
n
) we can form the p
vectors (in R
m)
AB1
, . . . , ABp
.
These p vectors constitute the m × p matrix denoted AB, whose jth column is ABj
. But
we know that the ith coordinate of ABj
is the inner product of the ith row of A by the jth
column of B,
￾
ai1 ai2 · · · ain ·


b1j
b2j
.
.
bnj
.


=
nX
k=1
aikbkj .
Thus we have defined a multiplication operation on matrices, namely if A = (aik) is a m × n
matrix and if B = (bjk) if n × p matrix, then their product AB is the m × n matrix whose
entry on the ith row and the jth column is given by the inner product of the ith row of A
by the jth column of B,
(AB)ij =
nX
k=1
aikbkj .
Beware that unlike the multiplication of real (or complex) numbers, if A and B are two n×n
matrices, in general, AB 6 = BA.
3.1. MOTIVATIONS: LINEAR COMBINATIONS, LINEAR INDEPENDENCE, RANK55
Suppose that A is an n × n matrix and that we are trying to solve the linear system
Ax = b,
with b ∈ R
n
. Suppose we can find an n × n matrix B such that
BAi = ei
, i = 1, . . . , n,
with ei = (0, . . . , 0, 1, 0 . . . , 0), where the only nonzero entry is 1 in the ith slot. If we form
the n × n matrix
In =


0 1 0
1 0 0
· · ·
· · · 0 0
0 0
0 0 1
.
· · · 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0
0 0 0
· · ·
· · · 1 0
0 1


,
called the identity matrix , whose ith column is ei
, then the above is equivalent to
BA = In.
If Ax = b, then multiplying both sides on the left by B, we get
B(Ax) = Bb.
But is is easy to see that B(Ax) = (BA)x = Inx = x, so we must have
x = Bb.
We can verify that x = Bb is indeed a solution, because it can be shown that
A(Bb) = (AB)b = Inb = b.
What is not obvious is that BA = In implies AB = In, but this is indeed provable. The
matrix B is usually denoted A−1 and called the inverse of A. It can be shown that it is the
unique matrix such that
AA−1 = A
−1A = In.
If a square matrix A has an inverse, then we say that it is invertible or nonsingular , otherwise
we say that it is singular . We will show later that a square matrix is invertible iff its columns
are linearly independent iff its determinant is nonzero.
In summary, if A is a square invertible matrix, then the linear system Ax = b has the
unique solution x = A−1
b. In practice, this is not a good way to solve a linear system because
computing A−1
is too expensive. A practical method for solving a linear system is Gaussian
elimination, discussed in Chapter 8. Other practical methods for solving a linear system
56 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
Ax = b make use of a factorization of A (QR decomposition, SVD decomposition), using
orthogonal matrices defined next.
Given an m × n matrix A = (akl), the n × m matrix A> = (a
>ij ) whose ith row is the
ith column of A, which means that a
>ij = aji for i = 1, . . . , n and j = 1, . . . , m, is called the
transpose of A. An n × n matrix Q such that
QQ> = Q
> Q = In
is called an orthogonal matrix . Equivalently, the inverse Q−1 of an orthogonal matrix Q is
equal to its transpose Q> . Orthogonal matrices play an important role. Geometrically, they
correspond to linear transformation that preserve length. A major result of linear algebra
states that every m × n matrix A can be written as
A = V ΣU
> ,
where V is an m×m orthogonal matrix, U is an n×n orthogonal matrix, and Σ is an m×n
matrix whose only nonzero entries are nonnegative diagonal entries σ1 ≥ σ2 ≥ · · · ≥ σp,
where p = min(m, n), called the singular values of A. The factorization A = V ΣU
> is called
a singular decomposition of A, or SVD.
The SVD can be used to “solve” a linear system Ax = b where A is an m × n matrix,
even when this system has no solution. This may happen when there are more equations
that variables (m > n) , in which case the system is overdetermined.
Of course, there is no miracle, an unsolvable system has no solution. But we can look
for a good approximate solution, namely a vector x that minimizes some measure of the
error Ax − b. Legendre and Gauss used k Ax − bk
2
2
, which is the squared Euclidean norm
of the error. This quantity is differentiable, and it turns out that there is a unique vector
x
+ of minimum Euclidean norm that minimizes k Ax − bk
2
2
. Furthermore, x
+ is given by the
expression x
+ = A+b, where A+ is the pseudo-inverse of A, and A+ can be computed from
an SVD A = V ΣU
> of A. Indeed, A+ = UΣ
+V
> , where Σ+ is the matrix obtained from Σ
by replacing every positive singular value σi by its inverse σi
−1
, leaving all zero entries intact,
and transposing.
Instead of searching for the vector of least Euclidean norm minimizing k Ax − bk
2
2
, we
can add the penalty term K k xk
2
2
(for some positive K > 0) to k Ax − bk
2
2
and minimize the
quantity k Ax − bk
2
2 + K k xk
2
2
. This approach is called ridge regression. It turns out that
there is a unique minimizer x
+ given by x
+ = (A> A + KIn)
−1A> b, as shown in the second
volume.
Another approach is to replace the penalty term K k xk
2
2
by K k xk 1
, where k xk 1 = |x1| +
· · · + |xn| (the ` 1
-norm of x). The remarkable fact is that the minimizers x of k Ax − bk
2
2 +
K k xk 1
tend to be sparse, which means that many components of x are equal to zero. This
approach known as lasso is popular in machine learning and will be discussed in the second
volume.
3.1. MOTIVATIONS: LINEAR COMBINATIONS, LINEAR INDEPENDENCE, RANK57
Another important application of the SVD is principal component analysis (or PCA), an
important tool in data analysis.
Yet another fruitful way of interpreting the resolution of the system Ax = b is to view
this problem as an intersection problem. Indeed, each of the equations
x1 + 2x2 − x3 = 1
2x1 + x2 + x3 = 2
x1 − 2x2 − 2x3 = 3
defines a subset of R
3 which is actually a plane. The first equation
x1 + 2x2 − x3 = 1
defines the plane H1 passing through the three points (1, 0, 0),(0, 1/2, 0),(0, 0, −1), on the
coordinate axes, the second equation
2x1 + x2 + x3 = 2
defines the plane H2 passing through the three points (1, 0, 0),(0, 2, 0),(0, 0, 2), on the coor￾dinate axes, and the third equation
x1 − 2x2 − 2x3 = 3
defines the plane H3 passing through the three points (3, 0, 0),(0, −3/2, 0),(0, 0, −3/2), on
the coordinate axes. See Figure 3.1.
2x + 2x - x = 1 1 2 3
2x + x + x = 2 1 2 3
x -2x -2x = 3 1 2 3
Figure 3.1: The planes defined by the preceding linear equations.
58 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
x -2x -2x = 3 1 2 3
2x + x + x = 2 1 2 3
2x + 2x - x = 1 1 2 3
(1.4, -0.4, -0.4)
Figure 3.2: The solution of the system is the point in common with each of the three planes.
The intersection Hi∩Hj of any two distinct planes Hi and Hj
is a line, and the intersection
H1 ∩ H2 ∩ H3 of the three planes consists of the single point (1.4, −0.4, −0.4), as illustrated
in Figure 3.2.
The planes corresponding to the system
x1 + 2x2 − x3 = 1
2x1 + x2 + x3 = 2
x1 − x2 + 2x3 = 3,
are illustrated in Figure 3.3.
2x + 2x - x = 1 1 2 3
2x + x + x = 2 1 2 3
1 2 3
x - x +2x = 3 1 2 3
Figure 3.3: The planes defined by the equations x1 + 2x2 − x3 = 1, 2x1 + x2 + x3 = 2, and
x1 − x2 + 2x3 = 3.
3.1. MOTIVATIONS: LINEAR COMBINATIONS, LINEAR INDEPENDENCE, RANK59
This system has no solution since there is no point simultaneously contained in all three
planes; see Figure 3.4.
2x + 2x - x = 1 1 2 3
x - x +2x = 3 1 2 3
2x + x + x = 2 2x + x + x = 2 11 22 33
Figure 3.4: The linear system x1 + 2x2 − x3 = 1, 2x1 + x2 + x3 = 2, x1 − x2 + 2x3 = 3 has
no solution.
Finally, the planes corresponding to the system
x1 + 2x2 − x3 = 3
2x1 + x2 + x3 = 3
x1 − x2 + 2x3 = 0,
are illustrated in Figure 3.5.
2x + 2x - x = 3
1
1
1
2
2 3
3
2x + x + x = 3 2 3
x - x + 2x = 0 1 2 3
1
Figure 3.5: The planes defined by the equations x1 + 2x2 − x3 = 3, 2x1 + x2 + x3 = 3, and
x1 − x2 + 2x3 = 0.
60 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
This system has infinitely many solutions, given parametrically by (1 − x3, 1 + x3, x3).
Geometrically, this is a line common to all three planes; see Figure 3.6.
2x + 2x - x = 3
1 2 3
x - x + 2x = 0 1 2 3
2x + x + x = 3 1 2 3
Figure 3.6: The linear system x1 + 2x2 − x3 = 3, 2x1 + x2 + x3 = 3, x1 − x2 + 2x3 = 0 has
the red line common to all three planes.
Under the above interpretation, observe that we are focusing on the rows of the matrix
A, rather than on its columns, as in the previous interpretations.
Another great example of a real-world problem where linear algebra proves to be very
effective is the problem of data compression, that is, of representing a very large data set
using a much smaller amount of storage.
Typically the data set is represented as an m × n matrix A where each row corresponds
to an n-dimensional data point and typically, m ≥ n. In most applications, the data are not
independent so the rank of A is a lot smaller than min{m, n}, and the the goal of low-rank
decomposition is to factor A as the product of two matrices B and C, where B is a m × k
matrix and C is a k ×n matrix, with k  min{m, n} (here,  means “much smaller than”):


A
m × n


=


B
m × k



 C
k × n


Now it is generally too costly to find an exact factorization as above, so we look for a
low-rank matrix A0 which is a “good” approximation of A. In order to make this statement
precise, we need to define a mechanism to determine how close two matrices are. This can
be done using matrix norms, a notion discussed in Chapter 9. The norm of a matrix A is a
3.2. VECTOR SPACES 61
nonnegative real number k Ak which behaves a lot like the absolute value |x| of a real number
x. Then our goal is to find some low-rank matrix A0 that minimizes the norm
k
A − A
0 k
2
,
over all matrices A0 of rank at most k, for some given k  min{m, n}.
Some advantages of a low-rank approximation are:
1. Fewer elements are required to represent A; namely, k(m + n) instead of mn. Thus
less storage and fewer operations are needed to reconstruct A.
2. Often, the process for obtaining the decomposition exposes the underlying structure of
the data. Thus, it may turn out that “most” of the significant data are concentrated
along some directions called principal directions.
Low-rank decompositions of a set of data have a multitude of applications in engineering,
including computer science (especially computer vision), statistics, and machine learning.
As we will see later in Chapter 23, the singular value decomposition (SVD) provides a very
satisfactory solution to the low-rank approximation problem. Still, in many cases, the data
sets are so large that another ingredient is needed: randomization. However, as a first step,
linear algebra often yields a good initial solution.
We will now be more precise as to what kinds of operations are allowed on vectors. In
the early 1900, the notion of a vector space emerged as a convenient and unifying framework
for working with “linear” objects and we will discuss this notion in the next few sections.
3.2 Vector Spaces
For every n ≥ 1, let R
n be the set of n-tuples x = (x1, . . . , xn). Addition can be extended to
R
n as follows:
(x1, . . . , xn) + (y1, . . . , yn) = (x1 + y1, . . . , xn + yn).
We can also define an operation ·: R × R
n → R
n as follows:
λ · (x1, . . . , xn) = (λx1, . . . , λxn).
The resulting algebraic structure has some interesting properties, those of a vector space.
However, keep in mind that vector spaces are not just algebraic
objects; they are also geometric objects.
Vector spaces are defined as follows.
62 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
Definition 3.1. Given a field K (with addition + and multiplication ∗), a vector space over
K (or K-vector space) is a set E (of vectors) together with two operations +: E × E → E
(called vector addition),1 and ·: K × E → E (called scalar multiplication) satisfying the
following conditions for all α, β ∈ K and all u, v ∈ E;
(V0) E is an abelian group w.r.t. +, with identity element 0;2
(V1) α · (u + v) = (α · u) + (α · v);
(V2) (α + β) · u = (α · u) + (β · u);
(V3) (α ∗ β) · u = α · (β · u);
(V4) 1 · u = u.
In (V3), ∗ denotes multiplication in the field K.
Given α ∈ K and v ∈ E, the element α · v is also denoted by αv. The field K is often
called the field of scalars.
Unless specified otherwise or unless we are dealing with several different fields, in the rest
of this chapter, we assume that all K-vector spaces are defined with respect to a fixed field
K. Thus, we will refer to a K-vector space simply as a vector space. In most cases, the field
K will be the field R of reals.
From (V0), a vector space always contains the null vector 0, and thus is nonempty.
From (V1), we get α · 0 = 0, and α · (−v) = −(α · v). From (V2), we get 0 · v = 0, and
(−α) · v = −(α · v).
Another important consequence of the axioms is the following fact:
Proposition 3.1. For any u ∈ E and any λ ∈ K, if λ 6 = 0 and λ · u = 0, then u = 0.
Proof. Indeed, since λ 6 = 0, it has a multiplicative inverse λ
−1
, so from λ · u = 0, we get
λ
−1
· (λ · u) = λ
−1
· 0.
However, we just observed that λ
−1
· 0 = 0, and from (V3) and (V4), we have
λ
−1
· (λ · u) = (λ
−1λ) · u = 1 · u = u,
and we deduce that u = 0.
1The symbol + is overloaded, since it denotes both addition in the field K and addition of vectors in E.
It is usually clear from the context which + is intended.
2The symbol 0 is also overloaded, since it represents both the zero in K (a scalar) and the identity element
of E (the zero vector). Confusion rarely arises, but one may prefer using 0 for the zero vector.
3.2. VECTOR SPACES 63
Remark: One may wonder whether axiom (V4) is really needed. Could it be derived from
the other axioms? The answer is no. For example, one can take E = R
n and define
·: R × R
n → R
n by
λ · (x1, . . . , xn) = (0, . . . , 0)
for all (x1, . . . , xn) ∈ R
n and all λ ∈ R. Axioms (V0)–(V3) are all satisfied, but (V4) fails.
Less trivial examples can be given using the notion of a basis, which has not been defined
yet.
The field K itself can be viewed as a vector space over itself, addition of vectors being
addition in the field, and multiplication by a scalar being multiplication in the field.
Example 3.1.
1. The fields R and C are vector spaces over R.
2. The groups R
n and C
n are vector spaces over R, with scalar multiplication given by
λ(x1, . . . , xn) = (λx1, . . . , λxn),
for any λ ∈ R and with (x1, . . . , xn) ∈ R
n or (x1, . . . , xn) ∈ C
n
, and C
n
is a vector
space over C with scalar multiplication as above, but with λ ∈ C.
3. The ring R[X]n of polynomials of degree at most n with real coefficients is a vector
space over R, and the ring C[X]n of polynomials of degree at most n with complex
coefficients is a vector space over C, with scalar multiplication λ·P(X) of a polynomial
P(X) = amX
m + am−1X
m−1 + · · · + a1X + a0
(with ai ∈ R or ai ∈ C) by the scalar λ (in R or C), with m ≤ n, given by
λ · P(X) = λamX
m + λam−1X
m−1 + · · · + λa1X + λa0.
4. The ring R[X] of all polynomials with real coefficients is a vector space over R, and the
ring C[X] of all polynomials with complex coefficients is a vector space over C, with
the same scalar multiplication as above.
5. The ring of n × n matrices Mn(R) is a vector space over R.
6. The ring of m × n matrices Mm,n(R) is a vector space over R.
7. The ring C(a, b) of continuous functions f : (a, b) → R is a vector space over R, with
the scalar multiplication λf of a function f : (a, b) → R by a scalar λ ∈ R given by
(λf)(x) = λf(x), for all x ∈ (a, b).
64 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
8. A very important example of vector space is the set of linear maps between two vector
spaces to be defined in Section 11.1. Here is an example that will prepare us for the
vector space of linear maps. Let X be any nonempty set and let E be a vector space.
The set of all functions f : X → E can be made into a vector space as follows: Given
any two functions f : X → E and g : X → E, let (f + g): X → E be defined such that
(f + g)(x) = f(x) + g(x)
for all x ∈ X, and for every λ ∈ R, let λf : X → E be defined such that
(λf)(x) = λf(x)
for all x ∈ X. The axioms of a vector space are easily verified.
Let E be a vector space. We would like to define the important notions of linear combi￾nation and linear independence.
Before defining these notions, we need to discuss a strategic choice which, depending
how it is settled, may reduce or increase headaches in dealing with notions such as linear
combinations and linear dependence (or independence). The issue has to do with using sets
of vectors versus sequences of vectors.
3.3 Indexed Families; the Sum Notation P i∈I
ai
Our experience tells us that it is preferable to use sequences of vectors; even better, indexed
families of vectors. (We are not alone in having opted for sequences over sets, and we are in
good company; for example, Artin [7], Axler [10], and Lang [109] use sequences. Nevertheless,
some prominent authors such as Lax [113] use sets. We leave it to the reader to conduct a
survey on this issue.)
Given a set A, recall that a sequence is an ordered n-tuple (a1, . . . , an) ∈ An of elements
from A, for some natural number n. The elements of a sequence need not be distinct and
the order is important. For example, (a1, a2, a1) and (a2, a1, a1) are two distinct sequences
in A3
. Their underlying set is {a1, a2}.
What we just defined are finite sequences, which can also be viewed as functions from
the function. This viewpoint is fruitful, because it allows us to define (countably) infinite
{1, 2, . . . , n} to the set A; the ith element of the sequence (a1, . . . , an) is the image of i under
sequences as functions s: N → A. But then, why limit ourselves to ordered sets such as
{1, . . . , n} or N as index sets?
The main role of the index set is to tag each element uniquely, and the order of the tags
is not crucial, although convenient. Thus, it is natural to define the notion of indexed family.
3.3. INDEXED FAMILIES; THE SUM NOTATION P i∈I
ai 65
Definition 3.2. Given a set A, an I-indexed family of elements of A, for short a family,
is a function a: I → A where I is any set viewed as an index set. Since the function a is
determined by its graph
{(i, a(i)) | i ∈ I},
the family a can be viewed as the set of pairs a = {(i, a(i)) | i ∈ I}. For notational simplicity,
we write ai
instead of a(i), and denote the family a = {(i, a(i)) | i ∈ I} by (ai)i∈I .
For example, if I = {r, g, b, y} and A = N, the set of pairs
a = {(r, 2),(g, 3),(b, 2),(y, 11)}
is an indexed family. The element 2 appears twice in the family with the two distinct tags
r and b.
When the indexed set I is totally ordered, a family (ai)i∈I is often called an I-sequence.
Interestingly, sets can be viewed as special cases of families. Indeed, a set A can be viewed
as the A-indexed family {(a, a) | a ∈ I} corresponding to the identity function.
Remark: An indexed family should not be confused with a multiset. Given any set A, a
multiset is a similar to a set, except that elements of A may occur more than once. For
example, if A = {a, b, c, d}, then {a, a, a, b, c, c, d, d} is a multiset. Each element appears
with a certain multiplicity, but the order of the elements does not matter. For example, a
has multiplicity 3. Formally, a multiset is a function s: A → N, or equivalently a set of pairs
{
N
(
-indexed family, since distinct elements may have the same multiplicity (such as
a, i) | a ∈ A}. Thus, a multiset is an A-indexed family of elements from N, but not a
c an d in
the example above). An indexed family is a generalization of a sequence, but a multiset is a
generalization of a set.
We also need to take care of an annoying technicality, which is to define sums of the
form P i∈I
ai
, where I is any finite index set and (ai)i∈I is a family of elements in some set
A equiped with a binary operation +: A × A → A which is associative (Axiom (G1)) and
commutative. This will come up when we define linear combinations.
The issue is that the binary operation + only tells us how to compute a1 + a2 for two
elements of A, but it does not tell us what is the sum of three of more elements. For example,
how should a1 + a2 + a3 be defined?
What we have to do is to define a1+a2+a3 by using a sequence of steps each involving two
elements, and there are two possible ways to do this: a1 + (a2 +a3) and (a1 +a2) +a3. If our
operation + is not associative, these are different values. If it associative, then a1+(a2+a3) =
(a1 + a2) + a3, but then there are still six possible permutations of the indices 1, 2, 3, and if
+ is not commutative, these values are generally different. If our operation is commutative,
then all six permutations have the same value. Thus, if + is associative and commutative,
it seems intuitively clear that a sum of the form P i∈I
ai does not depend on the order of the
operations used to compute it.
66 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
This is indeed the case, but a rigorous proof requires induction, and such a proof is
surprisingly involved. Readers may accept without proof the fact that sums of the form
P
i∈I
ai are indeed well defined, and jump directly to Definition 3.3. For those who want to
see the gory details, here we go.
First, we define sums P i∈I
ai
, where I is a finite sequence of distinct natural numbers,
say I = (i1, . . . , im). If I = (i1, . . . , im) with m ≥ 2, we denote the sequence (i2, . . . , im) by
I − {i1}. We proceed by induction on the size m of I. Let
X
i∈I
ai = ai1
, if m = 1,
X
i∈I
ai = ai1 +

X
i∈I−{i1}
ai

, if m > 1.
For example, if I = (1, 2, 3, 4), we have
X
i∈I
ai = a1 + (a2 + (a3 + a4)).
If the operation + is not associative, the grouping of the terms matters. For instance, in
general
a1 + (a2 + (a3 + a4)) 6 = (a1 + a2) + (a3 + a4).
However, if the operation + is associative, the sum P i∈I
ai should not depend on the grouping
of the elements in I, as long as their order is preserved. For example, if I = (1, 2, 3, 4, 5),
J1 = (1, 2), and J2 = (3, 4, 5), we expect that
X
i∈I
ai =

X
j∈J1
aj
 +

X
j∈J2
aj

.
This indeed the case, as we have the following proposition.
Proposition 3.2. Given any nonempty set A equipped with an associative binary operation
+: A × A → A, for any nonempty finite sequence I of distinct natural numbers and for
any partition of I into p nonempty sequences Ik1
, . . . , Ikp
, for some nonempty sequence K =
(k1, . . . , kp) of distinct natural numbers such that ki < kj
implies that α < β for all α ∈ Iki
and all β ∈ Ikj
, for every sequence (ai)i∈I of elements in A, we have
X
α∈I
aα =
X
k∈K

X α∈Ik
aα
 .
Proof. We proceed by induction on the size n of I.
If n = 1, then we must have p = 1 and Ik1 = I, so the proposition holds trivially.
3.3. INDEXED FAMILIES; THE SUM NOTATION P i∈I
ai 67
Next, assume n > 1. If p = 1, then Ik1 = I and the formula is trivial, so assume that
p ≥ 2 and write J = (k2, . . . , kp). There are two cases.
Case 1. The sequence Ik1 has a single element, say β, which is the first element of I.
In this case, write C for the sequence obtained from I by deleting its first element β. By
definition,
X
α∈I
aα = aβ +

X
α∈C
aα
 ,
and
X
k∈K

X α∈Ik
aα
 = aβ +

X
j∈J

X α∈Ij
aα
 .
Since |C| = n − 1, by the induction hypothesis, we have

X
α∈C
aα
 =
X
j∈J

X α∈Ij
aα
 ,
which yields our identity.
Case 2. The sequence Ik1 has at least two elements. In this case, let β be the first element
of I (and thus of Ik1
), let I
0 be the sequence obtained from I by deleting its first element β,
let Ik
01
be the sequence obtained from Ik1 by deleting its first element β, and let Ik
0i = Iki
for
i = 2, . . . , p. Recall that J = (k2, . . . , kp) and K = (k1, . . . , kp). The sequence I
0 has n − 1
elements, so by the induction hypothesis applied to I
0 and the Ik
0i
, we get
α
X∈I
0
aα =
X
k∈K

X α∈Ik
0
aα
 =

X
α∈Ik
01
aα
 +

X
j∈J

X α∈Ij
aα
 .
If we add the lefthand side to aβ, by definition we get
