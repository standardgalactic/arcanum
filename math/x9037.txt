bjak(κ(xj
, xk) − κ(xk, xj )).
Thus u
∗KSu is real iff KS is symmetric.
Consequently we make the following definition.
Definition 53.3. Let X be a nonempty set. A function κ: X × X → R is a (real) positive
definite kernel if κ(x, y) = κ(y, x) for all x, y ∈ X, and for every finite subset S = {x1, . . . , xp}
of X, if KS is the p × p real symmetric matrix
KS = (κ(xi
, xj ))1≤i,j≤p,
1916 CHAPTER 53. POSITIVE DEFINITE KERNELS
then we have
u
> KS u =
p
X
i,j=1
κ(xi
, xj )uiuj ≥ 0, for all u ∈ R
p
.
Among other things, the next proposition shows that a positive definite kernel satisfies
the Cauchy–Schwarz inequality.
Proposition 53.4. A Hermitian 2 × 2 matrix
A =

a
b d
b

is positive semidefinite if and only if a ≥ 0, d ≥ 0, and ad − |b|
2 ≥ 0.
Let κ: X × X → C be a positive definite kernel. For all x, y ∈ X, we have
|κ(x, y)|
2 ≤ κ(x, x)κ(y, y).
Proof. For all x, y ∈ C, we have
￾
x y


a
b d
b
  x
y

=
￾ x y


ax
bx +
+
dy
by
= a|x|
2 + bxy + bxy + d|y|
2
.
If A is positive semidefinite, then we already know that a ≥ 0 and d ≥ 0. If a = 0, then
we must have b = 0, since otherwise we can make bxy + bxy, which is twice the real part of
bxy, as negative as we want. In this case, ad − |b|
2 = 0.
If a > 0, then
a|x|
2 + bxy + bxy + d|y|
2 = a

 
 x +
a
b
y




2
+
|y|
2
a
(ad − |b|
2
).
If ad−|b|
2 < 0, we can pick y 6 = 0 and x = −(by)/a, so that the above expression is negative.
Therefore, ad − |b|
2 ≥ 0. The converse is trivial.
If x = y, the inequality |κ(x, y)|
2 ≤ κ(x, x)κ(y, y) is trivial. If x 6 = y, the inequality
follows by applying the criterion for being positive semidefinite to the matrix

κ
κ
(
(
x, x
x, y)
) κ
κ
(
(
x, y
y, y)
)

,
as claimed.
The following property due to I. Schur (1911) shows that the pointwise product of two
positive definite kernels is also a positive definite kernel.
53.2. BASIC PROPERTIES OF POSITIVE DEFINITE KERNELS 1917
Proposition 53.5. (I. Schur) If κ1 : X × X → C and κ2 : X × X → C are two positive
definite kernels, then the function κ: X × X → C given by κ(x, y) = κ1(x, y)κ2(x, y) for all
x, y ∈ X is also a positive definite kernel.
Proof. It suffices to prove that if A = (ajk) and B = (bjk) are two Hermitian positive
semidefinite p × p matrices, then so is their pointwise product C = A ◦ B = (ajkbjk) (also
known as Hadamard or Schur product). Recall that a Hermitian positive semidefinite matrix
A can be diagonalized as A = UΛU
∗
, where Λ is a diagonal matrix with nonnegative entries
and U is a unitary matrix. Let Λ1/2 be the diagonal matrix consisting of the positive square
roots of the diagonal entries in Λ. Then we have
A = UΛU
∗ = UΛ
1/2Λ
1/2U
∗ = UΛ
1/2
(UΛ
1/2
)
∗
.
Thus if we set R = UΛ
1/2
, we have
A = RR∗
,
which means that
ajk =
p
X
h=1
rjhrkh.
Then for any u ∈ C
p
, we have
u
∗
(A ◦ B)u =
p
X
j,k=1
ajkbjkujuk
=
p
X
j,k=1
p
X
h=1
rjhrkhbjkujuk
=
p
X
h=1
p
X
j,k=1
bjkujrjhukrkh.
Since B is positive semidefinite, for each fixed h, we have
p
X
j,k=1
bjkujrjhukrkh =
p
X
j,k=1
bjkzjzk ≥ 0,
as we see by letting z = (u1r1h, . . . , uprph),
In contrast, the ordinary product AB of two symmetric positive semidefinite matrices A
and B may not be symmetric positive semidefinite; see Section 8.9 for an example.
Here are other ways of obtaining new positive definite kernels from old ones.
Proposition 53.6. Let κ1 : X ×X → C and κ2 : X ×X → C be two positive definite kernels,
f : X → C be a function, ψ: X → R
N be a function, κ3 : R
N ×R
N → C be a positive definite
kernel, and a ∈ R be any positive real number. Then the following functions are positive
definite kernels:
1918 CHAPTER 53. POSITIVE DEFINITE KERNELS
(1) κ(x, y) = κ1(x, y) + κ2(x, y).
(2) κ(x, y) = aκ1(x, y).
(3) κ(x, y) = f(x)f(y).
(4) κ(x, y) = κ3(ψ(x), ψ(y)).
(5) If B is a symmetric positive semidefinite n × n matrix, then the map
κ: R
n × R
n → R given by
κ(x, y) = x
> By
is a positive definite kernel.
Proof. (1) For every finite subset S = {x1, . . . , xp} of X, if K1 is the p × p matrix
K1 = (κ1(xk, xj ))1≤j,k≤p
and if if K2 is the p × p matrix
K2 = (κ2(xk, xj ))1≤j,k≤p,
then for any u ∈ C
p
, we have
u
∗
(K1 + K2)u = u
∗K1u + u
∗K2u ≥ 0,
since u
∗K1u ≥ 0 and u
∗K2u ≥ 0 because κ2 and κ2 are positive definite kernels, which means
that K1 and K2 are positive semidefinite.
(2) We have
u
∗
(aK1)u = au∗K1u ≥ 0,
since a > 0 and u
∗K1u ≥ 0.
(3) For every finite subset S = {x1, . . . , xp} of X, if K is the p × p matrix
K = (κ(xk, xj ))1≤j,k≤p = (f(xk)f(xj ))1≤j,k≤p
then we have
u
∗Ku = u
> K> u =
p
X
j,k=1
κ(xj
, xk)ujuk =
p
X
j,k=1
ujf(xj )ukf(xk) =

  

p
X
j=1
ujf(xj )




2
≥ 0.
(4) For every finite subset S = {x1, . . . , xp} of X, the p × p matrix K given by
K = (κ(xk, xj ))1≤j,k≤p = (κ3(ψ(xk), ψ(xj )))1≤j,k≤p
is symmetric positive semidefinite since κ3 is a positive definite kernel.
53.2. BASIC PROPERTIES OF POSITIVE DEFINITE KERNELS 1919
(5) As in the proof of Proposition 53.5 (adapted to the real case) there is a matrix R
such that
B = RR> ,
so
κ(x, y) = x
> By = x
> RR> y = (R
> x)
> R
> y = h R
> x, R> yi ,
so κ is the kernel function given by the feature map ϕ(x) = R> x from R
n
to itself, and by
Proposition 53.1, it is a symmetric positive definite kernel.
Proposition 53.7. Let κ1 : X × X → C be a positive definite kernel, and let p(z) be a
polynomial with nonnegative coefficients. Then the following functions κ defined below are
also positive definite kernels.
(1) κ(x, y) = p(κ1(x, y)).
(2) κ(x, y) = e
κ1(x,y)
.
(3) If X is real Hilbert space with inner product h−, −iX and corresponding norm k k X
,
κ(x, y) = e
−
k
x−yk
2
2σ2
X
for any σ > 0.
Proof. (1) If p(z) = amz
m + · · · + a1z + a0, then
p(κ1(x, y)) = amκ1(x, y)
m + · · · + a1κ1(x, y) + a0.
Since ak ≥ 0 for k = 0, . . . , m, by Proposition 53.5 and Proposition 53.6(2), each func￾tion akκi(x, y)
k with 1 ≤ k ≤ m is a positive definite kernel, by Proposition 53.6(3) with
f(x) = √
a0, the constant function a0 is a positive definite kernel, and by Proposition 53.6(1),
p(κ1(x, y)) is a positive definite kernel.
(2) We have
e
κ1(x,y) =
∞X
k=0
κ1(x, y
k!
)
k
.
By (1), the partial sums
mX
k=0
κ1(x, y
k!
)
k
are positive definite kernels, and since e
κ1(x,y)
is the (uniform) pointwise limit of positive
definite kernels, it is also a positive definite kernel.
(3) By Proposition 53.6(2), since the map (x, y) 7→ hx, yi X is obviously a positive definite
kernel (the feature map is the identity) and since σ 6 = 0, the function (x, y) 7→ hx, yi X/σ2
is
a positive definite kernel (by Proposition 53.6(2)), so by (2),
κ1(x, y) = e
h
x,yi X
σ2
1920 CHAPTER 53. POSITIVE DEFINITE KERNELS
is a positive definite kernel. Let f : X → R be the function given by
f(x) = e
−
k
xk
2
2σ2
.
Then by Proposition 53.6(3),
κ2(x, y) = f(x)f(y) = e
−
k
xk
2
2σ2 e
−
k
yk
2
2σ2 = e
−
k
xk
2
X+k yk
2
X
2σ2
is a positive definite kernel. By Proposition 53.5, the function κ1κ2 is a positive definite
kernel, that is
κ1(x, y)κ2(x, y) = e
h
x,yi X
σ2 e
−
k
xk
2
X+k yk
2
X
2σ2 = e
h
x,yi X
σ2 −
k
xk
2
X+k yk
2
X
2σ2 = e
−
k
x−yk
2
X
2σ2
is a positive definite kernel.
Definition 53.4. The positive definite kernel
κ(x, y) = e
−
k
x−yk
2
2σ2
X
is called a Gaussian kernel.
This kernel requires a feature map in an infinite-dimensional space because it is an infinite
sum of distinct kernels.
Remark: If κ1 is a positive definite kernel, the proof of Proposition 53.7(3) is immediately
adapted to show that
κ(x, y) = e
−
κ1(x,x)+κ1(y,y)−2κ1(x,y)
2σ2
is a positive definite kernel.
Next we prove that every positive definite kernel arises from a feature map in a Hilbert
space which is a function space.
53.3 Hilbert Space Representation of a Positive
Definite Kernel
The following result shows how to construct a so-called reproducing kernel Hilbert space, for
short RKHS, from a positive definite kernel.
Theorem 53.8. Let κ: X × X → C be a positive definite kernel on a nonempty set X. For
every x ∈ X, let κx : X → C be the function given by
κx(y) = κ(x, y), y ∈ X.
53.3. HILBERT SPACE REPRESENTATION OF A POSITIVE KERNEL 1921
Let H0 be the subspace of the vector space C
X of functions from X to C spanned by the
family of functions (κx)∈X, and let ϕ: X → H0 be the map given by ϕ(x) = κx. There is a
Hermitian inner product h−, −i on H0 such that
κ(x, y) = h ϕ(x), ϕ(y)i , for all x, y ∈ X.
The completion H of H0 is a Hilbert space, and the map η : H → C
X given by
η(f)(x) = h f, κxi , x ∈ X,
is linear and injective, so H can be identified with a subspace of C
X. We also have
κ(x, y) = h ϕ(x), ϕ(y)i , for all x, y ∈ X.
For all f ∈ H0 and all x ∈ X,
h
f, κxi = f(x), (∗)
a property known as the reproducing property.
Proof.
Step 1. Define a candidate inner product.
For any two linear combinations f =
P
p
j=1 αjκxj
and g =
P
q
k=1 βkκyk
in H0, with
xj
, yk ∈ X and αj
, βk ∈ C, define h f, gi by
h
f, gi =
p
X
j=1
q
X
k=1
αjβkκ(xj
, yk). (†)
At first glance, the above expression appears to depend on the expression of f and g as linear
combinations, but since κ(xj
, yk) = κ(yk, xj ), observe that
q
X
k=1
βkf(yk) =
p
X
j=1
q
X
k=1
αjβkκ(xj
, yk) =
p
X
j=1
αjg(xj ), (∗)
and since the first and the third term are equal for all linear combinations representing f
and g, we conclude that (†) depends only on f and g and not on their representation as a
linear combination.
Step 2. Prove that the inner product defined by (†) is Hermitian semidefinite.
Obviously (†) defines a Hermitian sequilinear form. For every f ∈ H0, we have
h
f, fi =
p
X
j,k=1
αjαkκ(xj
, xk) ≥ 0,
since κ is a positive definite kernel.
1922 CHAPTER 53. POSITIVE DEFINITE KERNELS
Step 3. Prove that the inner product defined by (†) is positive definite.
For any finite subset {f1, . . . , fn} of H0 and any z ∈ C
n
, we have
nX
j,k=1
h
fj
, fki zjzk =
*
nX
j=1
zjfj
,
nX
j=1
zjfj
+ ≥ 0,
which shows that the map (f, g) 7→ hf, gi from H0 × H0 to C is a positive definite kernel.
Observe that for all f ∈ H0 and all x ∈ X, (†) implies that
h
f, κxi =
k
X
j=1
αjκ(xj
, x) = f(x),
a property known as the reproducing property. The above implies that
h
κx, κyi = κ(x, y). (∗∗)
By Proposition 53.4 applied to the positive definite kernel (f, g) 7→ hf, gi , we have
|hf, κxi|2 ≤ hf, fih κx, κxi ,
that is,
|f(x)|
2 ≤ hf, fi κ(x, x),
so h f, fi = 0 implies that f(x) = 0 for all x ∈ X, which means that h−, −i as defined by (†)
is positive definite. Therefore, h−, −i is a Hermitian inner product on H0, and by (∗∗) and
since ϕ(x) = κx, we have
κ(x, y) = h ϕ(x), ϕ(y)i , for all x, y ∈ X.
Step 4. Define the embedding η.
Let H be the Hilbert space which is the completion of H0, so that H0 is dense in H. The
map η : H → C
X given by
η(f)(x) = h f, κxi
is obviously linear, and it is injective because the family (κx)x∈X spans H0 which is dense in
H, thus it is also dense in H, so if h f, κxi = 0 for all x ∈ X, then f = 0.
Corollary 53.9. If we identify a function f ∈ H with the function η(f), then we have the
reproducing property
h
f, κxi = f(x), for all f ∈ H and all x ∈ X.
If X is finite, then C
X is finite-dimensional. If X is a separable topological space and if κ is
continuous, then it can be shown that H is a separable Hilbert space.
53.4. KERNEL PCA 1923
Also, if κ: X × X → R is a real symmetric positive definite kernel, then we see im￾mediately that Theorem 53.8 holds with H0 a real Euclidean space and H a real Hilbert
space.
~
Remark: If X = G, where G is a locally compact group, then a function p: G → C (not
necessarily continuous) is positive semidefinite if for all s1, . . . , sn ∈ G and all ξ1, . . . , ξn ∈ C,
we have
nX
j,k=1
p(s
−
j
1
sk)ξkξj ≥ 0.
So if we define κ: G × G → C by
κ(s, t) = p(t
−1
s),
then κ is a positive definite kernel on G. If p is continuous, then it is known that p arises
from a unitary representation U : G → U(H) of the group G in a Hilbert space H with
inner product h−, −i (a homomorphism with a certain continuity property), in the sense
that there is some vector x0 ∈ H such that
p(s) = h U(s)(x0), x0i , for all s ∈ G.
Since the U(s) are unitary operators on H,
p(t
−1
s) = h U(t
−1
s)(x0), x0i = h U(t
−1
)(U(s)(x0)), x0i
= h U(t)
∗
(U(s)(x0)), x0i = h U(s)(x0), U(t)(x0)i ,
which shows that
κ(s, t) = h U(s)(x0), U(t)(x0)i ,
so the map ϕ: G → H given by
ϕ(s) = U(s)(x0)
is a feature map into the feature space H. This theorem is due to Gelfand and Raikov (1943).
The proof of Theorem 53.8 is essentially identical to part of Godement’s proof of the
above result about the correspondence between functions of positive type and unitary rep￾resentations; see Helgason [90], Chapter IV, Theorem 1.5. Theorem 53.8 is a little more
general since it does not assume that X is a group, but when G is a group, the feature map
arises from a unitary representation.
53.4 Kernel PCA
As an application of kernel functions, we discuss a generalization of the method of principal
component analysis (PCA). Suppose we have a set of data S = {x1, . . . , xn} in some input
space X , and pretend that we have an embedding ϕ: X → F of X in a (real) feature space
(F,h−, −i), but that we only have access to the kernel function κ(x, y) = h ϕ(x), ϕ(y)i . We
would like to do PCA analysis on the set ϕ(S) = {ϕ(x1), . . . , ϕ(xn)}.
There are two obstacles:
1924 CHAPTER 53. POSITIVE DEFINITE KERNELS
(1) We need to center the data and compute the inner products of pairs of centered data.
More precisely, if the centroid of ϕ(S) is
µ =
1
n
(ϕ(x1) + · · · + ϕ(xn)),
then we need to compute the inner products h ϕ(x) − µ, ϕ(y) − µi .
(2) Let us assume that F = R
d with the standard Euclidean inner product and that
the data points ϕ(xi) are expressed as row vectors Xi of an n × d matrix X (as it
is customary). Then the inner products κ(xi
, xj ) = h ϕ(xi), ϕ(xj )i are given by the
kernel matrix K = XX> . Be aware that with this representation, in the expression
j
h
ϕ
th component (
(xi), ϕ(xj )i , ϕ(
Y
x
k
i
)
) is a
j of the principal component
d-dimensional column vector, while
Yk (viewed as a
ϕ(xi) =
n-dimensional column
Xi
>
. However, the
vector) is given by the projection of Xbj = Xj − µ onto the direction uk (viewing µ as a
d-dimensional row vector), which is a unit eigenvector of the matrix (X − µ)
> (X − µ)
(where Xb = X − µ is the matrix whose jth row is Xbj = Xj − µ), is given by the inner
product
h
Xj − µ, uki = (Yk)j
;
see Definition 23.2 and Theorem 23.11. The problem is that we know what the matrix
(X − µ)(X − µ)
> is from (1), because it can be expressed in terms of K, but we don’t
know what (X − µ)
> (X − µ) is because we don’t have access to Xb = X − µ.
Both difficulties are easily overcome. For (1) we have
h
ϕ(x) − µ, ϕ(y) − µi =
* ϕ(x) −
n
1
nX
k=1
ϕ(xk), ϕ(y) −
1
n
nX
k=1
ϕ(xk)
+
= κ(x, y) −
1
n
nX
i=1
κ(x, xi) −
n
1 X
n
j=1
κ(xj
, y) +
n
1
2
X
n
i,j=1
κ(xi
, xj ).
For (2), if K is the kernel matrix K = (κ(xi
, xj )), then the kernel matrix Kb corresponding
to the kernel function κb given by
κb(x, y) = h ϕ(x) − µ, ϕ(y) − µi
can be expressed in terms of K. Let 1 be the column vector (of dimension n) whose entries
are all 1. Then 11> is the n×n matrix whose entries are all 1. If A is an n×n matrix, then
1
> A is the row vector consisting of the sums of the columns of A, A1 is the column vector
consisting of the sums of the rows of A, and 1
> A1 is the sum of all the entries in A. Then
it is easy to see that the kernel matrix corresponding to the kernel function κb is given by
Kb = K −
1
n
11> K −
1
n
K11> +
1
n2
(1
> K1)11> .
53.4. KERNEL PCA 1925
Suppose Xb = X − µ has rank r. To overcome the second problem, note that if
Xb = V DU >
is an SVD for Xb, then
Xb
> = UD> V
>
is an SVD for Xb> , and the r×r submatrix of D> consisting of the first r rows and r columns
of D> (and D), is the diagonal Σr matrix consisting of the singular values σ1 ≥ · · · ≥ σr of
Xb, so we can express the matrix Ur consisting of the first r columns uk of U in terms of the
matrix Vr consisting of the first r columns vk of V (1 ≤ k ≤ r) as
Ur = Xb
> VrΣ
−
r
1
.
Furthermore, σ1
2 ≥ · · · ≥ σr
2 are the nonzero eigenvalues of Kb = XbXb> , and the columns of
Vr are corresponding unit eigenvectors of Kb. From
Ur = Xb
> VrΣ
−
r
1
the kth column uk of Ur (which is a unit eigenvector of Xb> Xb associated with the eigenvalue
σk
2
) is given by
uk =
nX
i=1
σk
−1
(vk)iXbi
> =
nX
i=1
σk
−1
(vk)iϕ[(xi), 1 ≤ k ≤ r,
so the projection of ϕ[(x) onto uk is given by
h
[
ϕ(x), uki =
* ϕ[(x),
nX
i=1
σk
−1
(vk)iϕ[(xi)
+
=
nX
i=1
σk
−1
(vk)i
D ϕ[(x), ϕ[(xi)
E =
nX
i=1
σk
−1
(vk)iκb(x, xi).
Therefore, the jth component of the principal component Yk in the principal direction uk is
given by
(Yk)j = h Xj − µ, uki =
nX
i=1
σk
−1
(vk)iκb(xj
, xi) =
nX
i=1
σk
−1
(vk)iKbij .
The generalization of kernel PCA to a general embedding ϕ: X → F of X in a (real)
feature space (F,h−, −i) (where F is not restricted to be equal to R
d
) with the kernel matrix
K given by
Kij = h ϕ(xi), ϕ(xj )i ,
goes as follows.
1926 CHAPTER 53. POSITIVE DEFINITE KERNELS
• Let r be the rank of Kb, where
Kb = K −
1
n
11> K −
1
n
K11> +
1
n2
(1
> K1)11> ,
let σ1
2 ≥ · · · ≥ σr
2 be the nonzero eigenvalues of Kb, and let v1, . . . , vr be corresponding
unit eigenvectors. The notation
αk = σk
−1
vk
is often used, where the αk are called the dual variables.
• The column vector Yk (1 ≤ k ≤ r) defined by
Yk =
 
nX
i=1
(αk)iKbij!
n
j=1
is called the kth kernel principal component (for short kth kernel PCA) of the data set
S = {x1, . . . , xn} in the direction uk =
P
n
i=1 σk
−1
(vk)iXbi
>
(even though the matrix Xb
is not known).
53.5 Summary
The main concepts and results of this chapter are listed below:
• Feature map, feature embedding, feature space.
• Kernel function.
• Positive definite kernel, real positive definite kernel.
• Gram matrix.
• Hadamard product, Schur product.
• Gaussian kernel.
• Reproducing kernel Hilbert space (RKHS).
• Reproducing property.
• Intersection kernel, union complement kernel, agreement kernel.
• Kernel PCA.
• k-th kernel PCA.
53.6. PROBLEMS 1927
53.6 Problems
Problem 53.1. Referring back to Example 53.3, prove that if ϕ1 : X → R
n1 and ϕ2 : X →
R
n2 are two feature maps and if κ1(x, y) = h ϕ1(x), ϕ1(y)i and κ2(x, y) = h ϕ2(x), ϕ2(y)i are
the corresponding kernel functions, then the map defined by
κ(x, y) = κ1(x, y)κ2(x, y)
is a kernel function, for the feature space R
n1 × R
n2 and the feature map
ϕ(x)(i,j) = (ϕ1(x))i(ϕ2(x))j
, 1 ≤ i ≤ n1, 1 ≤ j ≤ n2.
Problem 53.2. Referring back to Example 53.3, prove that the feature embedding ϕ: X →
R(
n+
m
m−1
) given by
ϕ(i1,...,in)(x) = 
i1 · · ·
m
in

1/2
(ϕ1(x))i
1
1
(ϕ1(x))i
1
2
· · ·(ϕ1(x))i
1
n
, i1 + i2 + · · · + in = m, ij ∈ N,
where the n-tuples (i1, . . . , in) are ordered lexicographically, defines the kernel function κ
given by κ(x, y) = (κ1(x, y))m.
Problem 53.3. In Example 53.6, prove that for any two subsets A1 and A2 of D,
h
ϕ(A1), ϕ(A2)i = 2|A1∩A2|
,
the number of common subsets of A1 and A2.
Problem 53.4. Prove that the pointwise limit of positive definite kernels is also a positive
definite kernel.
Problem 53.5. Prove that if κ1 is a positive definite kernel, then
κ(x, y) = e
−
κ1(x,x)+κ1(y,y)−2κ1(x,y)
2σ2
is a positive definite kernel.
1928 CHAPTER 53. POSITIVE DEFINITE KERNELS
Chapter 54
Soft Margin Support Vector Machines
In Sections 50.5 and 50.6 we considered the problem of separating two nonempty disjoint
finite sets of p blue points {ui}
p
i=1 and q red points {vj}
q
j=1 in R
n
. The goal is to find a
hyperplane H of equation w
> x − b = 0 (where w ∈ R
n
is a nonzero vector and b ∈ R),
such that all the blue points ui are in one of the two open half-spaces determined by H, and
all the red points vj are in the other open half-space determined by H; see Figure 54.1.
u
u
u
u
1
2
3
p
v
v
v
v
v 1
2
3
4
up
u3
u1
u2
v1
q
vq
v
2
v3
Figure 54.1: Two examples of the SVM separation problem. The left figure is SVM in R
2
,
while the right figure is SVM in R
3
.
SVM picks a hyperplane which maximizes the minimum distance from these points to the
hyperplane.
In this chapter we return to the problem of separating two disjoint sets of points, {ui}
p
i=1
and {vj}
q
j=1, but this time we do not assume that these two sets are separable. To cope with
nonseparability, we allow points to invade the safety zone around the separating hyperplane,
and even points on the wrong side of the hyperplane. Such a method is called soft margin
support vector machine. We discuss variations of this method, including ν-SV classification.
In each case we present a careful derivation of the dual.
1929
wT x - b = 0
wT
x - b
= 0
1930 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
If the sets of points {u1, . . . , up} and {v1, . . . , vq} are not linearly separable (with ui
, vj ∈
R
n
), we can use a trick from linear programming which is to introduce nonnegative “slack
variables”  = ( 1, . . . , p) ∈ R
p and ξ = (ξ1, . . . , ξq) ∈ R
q
to relax the “hard” constraints
w
> ui − b ≥ δ i = 1, . . . , p
−w
> vj + b ≥ δ j = 1, . . . , q
of Problem (SVMh1) from Section 50.5 to the “soft” constraints
w
> ui − b ≥ δ −  i
, i ≥ 0 i = 1, . . . , p
−w
> vj + b ≥ δ − ξj
, ξj ≥ 0 j = 1, . . . , q.
Recall that w ∈ R
n and b, δ ∈ R.
If  i > 0, the point ui may be misclassified, in the sense that it can belong to the margin
(the slab), or even to the wrong half-space classifying the negative (red) points. See Figures
54.5 (2) and (3). Similarly, if ξj > 0, the point vj may be misclassified, in the sense that it
can belong to the margin (the slab), or even to the wrong half-space classifying the positive
(blue) points. We can think of  i as a measure of how much the constraint w
> ui − b ≥ δ
is violated, and similarly of ξj as a measure of how much the constraint −w
> vj + b ≥ δ is
violated. If  = 0 and ξ = 0, then we recover the original constraints. By making  and ξ
large enough, these constraints can always be satisfied. We add the constraint w
> w ≤ 1 and
we minimize −δ.
If instead of the constraints of Problem (SVMh1) we use the hard constraints
w
> ui − b ≥ 1 i = 1, . . . , p
−w
> vj + b ≥ 1 j = 1, . . . , q
of Problem (SVMh2) (see Example 50.6), then we relax to the soft constraints
w
> ui − b ≥ 1 −  i
, i ≥ 0 i = 1, . . . , p
−w
> vj + b ≥ 1 − ξj
, ξj ≥ 0 j = 1, . . . , q.
In this case there is no constraint on w, but we minimize (1/2)w
> w.
Ideally we would like to find a separating hyperplane that minimizes the number of
misclassified points, which means that the variables  i and ξj should be as small as possible,
but there is a trade-off in maximizing the margin (the thickness of the slab), and minimizing
