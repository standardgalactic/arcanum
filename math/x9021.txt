J(uk − ρk∇Juk
) = inf
ρ∈R
J(uk − ρ∇Juk
).
This optimization problem only succeeds if the above minimization problem has a
unique solution.
(4) Gradient descent method with backtracking line search. In this method, the step pa￾rameter is obtained by performing a backtracking line search.
We have the following useful result about the convergence of the gradient method with
optimal parameter.
Proposition 49.13. Let J : R
n → R be an elliptic functional. Then the gradient method
with optimal stepsize parameter converges.
Proof. Since J is elliptic, by Theorem 49.8(3), the functional J has a unique minimum u
characterized by ∇Ju = 0. Our goal is to prove that the sequence (uk)k≥0 constructed using
the gradient method with optimal parameter converges to u, starting from any initial vector
u0. Without loss of generality we may assume that uk+1 6 = uk and ∇Juk
6 = 0 for all k, since
otherwise the method converges in a finite number of steps.
Step 1 . Show that any two consecutive descent directions are orthogonal and
J(uk) − J(uk+1) ≥
α
2
k
uk − uk+1k
2
.
Let ϕk : R → R be the function given by
ϕk(ρ) = J(uk − ρ∇Juk
).
Since the function ϕk is strictly convex and coercive, by Theorem 49.8(2), it has a unique
minimum ρk which is the unique solution of the equation ϕ
0k
(ρ) = 0. By the chain rule
ϕ
0k
(ρ) = dJuk−ρ∇Juk
(−∇Juk
)
= −h∇Juk−ρ∇Juk
, ∇Juk
i
,
1690 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
and since uk+1 = uk − ρk∇Juk we get
h∇Juk+1 , ∇Juk
i = 0,
which shows that two consecutive descent directions are orthogonal.
Since uk+1 = uk − ρk∇Juk
and we assumed that that uk+1 6 = uk, we have ρk 6 = 0, and we
also get
h∇Juk+1 , uk+1 − uki = 0.
By the inequality of Theorem 49.8(1) we have
J(uk) − J(uk+1) ≥
α
2
k
uk − uk+1k
2
.
Step 2 . Show that limk7→∞ k uk − uk+1k = 0.
It follows from the inequality proven in Step 1 that the sequence (J(uk))k≥0 is decreasing
and bounded below (by J(u), where u is the minimum of J), so it converges and we conclude
that
lim
k7→∞
(J(uk) − J(uk+1)) = 0,
which combined with the preceding inequality shows that
lim
k7→∞
k
uk − uk+1k = 0.
Step 3 . Show that k∇Juk
k ≤
  ∇Juk − ∇Juk+1
  .
Using the orthogonality of consecutive descent directions, by Cauchy–Schwarz we have
k∇Juk
k
2 = h∇Juk
, ∇Juk − ∇Juk+1 i
≤ k∇Juk
k

 ∇Juk − ∇Juk+1
  ,
so that
k∇Juk
k ≤
  ∇Juk − ∇Juk+1
  .
Step 4 . Show that limk7→∞ k∇Juk
k = 0.
Since the sequence (J(uk))k≥0 is decreasing and the functional J is coercive, the sequence
(uk)k≥0 must be bounded. By hypothesis, the derivative dJ is of J is continuous, so it is
uniformly continuous over compact subsets of R
n
; here we are using the fact that R
n
is
finite dimensional. Hence, we deduce that for every  > 0, there is some δ > 0 such that if
k
uk − uk+1k < δ then


dJuk − dJuk+1
  2
< .
49.6. GRADIENT DESCENT METHODS FOR UNCONSTRAINED PROBLEMS 1691
But by definition of the operator norm and using the Cauchy–Schwarz inequality


dJuk − dJuk+1
  2
= sup
k
wk =1
|dJuk
(w) − dJuk+1 (w)|
= sup
k
wk =1
|h∇Juk − ∇Juk+1 , wi|
≤
  ∇Juk − ∇Juk+1
  .
But we also have


∇Juk − ∇Juk+1
 
2
= h∇Juk − ∇Juk+1 , ∇Juk − ∇Juk+1 i
= dJuk
(∇Juk − ∇Juk+1 ) − dJuk+1 (∇Juk − ∇Juk+1 )
≤
  dJuk − dJuk+1
 
2
2
,
and so


dJuk − dJuk+1
  2
=
  ∇Juk − ∇Juk+1
  .
It follows that since
lim
k7→∞
k
uk − uk+1k = 0
then
lim
k7→∞


∇Juk − ∇Juk+1
  = lim
k7→∞


dJuk − dJuk+1
  2
= 0,
and using the fact that
k∇Juk
k ≤
  ∇Juk − ∇Juk+1
  ,
we obtain
lim
k7→∞
k∇Juk
k = 0.
Step 5 . Finally we can prove the convergence of the sequence (uk)k≥0.
Since J is elliptic and since ∇Ju = 0 (since u is the minimum of J over R
n
), we have
α k uk − uk
2 ≤ h∇Juk − ∇Ju, uk − ui
= h∇Juk
, uk − ui
≤ k∇Juk
k k uk − uk .
Hence, we obtain
k
uk − uk ≤ 1
α
k∇Juk
k
, (b)
and since we showed that
lim
k7→∞
k∇Juk
k = 0,
we see that the sequence (uk)k≥0 converges to the mininum u.
1692 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
Remarks: As with the previous proposition, the assumption of finite dimensionality is
crucial. The proof provides an a priori bound on the error k uk − uk .
If J is a an elliptic quadratic functional
J(v) = 1
2
h
Av, vi − hb, vi ,
we can use the orthogonality of the descent directions ∇Juk
and ∇Juk+1 to compute ρk.
Indeed, we have ∇Jv = Av − b, so
0 = h∇Juk+1 , ∇Juk
i = h A(uk − ρk(Auk − b)) − b, Auk − bi ,
which yields
ρk =
k
wkk
2
h
Awk, wki
, with wk = Auk − b = ∇Juk
.
Consequently, a step of the iteration method takes the following form:
(1) Compute the vector
wk = Auk − b.
(2) Compute the scalar
ρk =
k
wkk
2
h
Awk, wki
.
(3) Compute the next vector uk+1 by
uk+1 = uk − ρkwk.
This method is of particular interest when the computation of Aw for a given vector w is
cheap, which is the case if A is sparse.
Example 49.1. For a particular illustration of this method, we turn to the example provided
by Shewchuk, with A =

3 2
2 6 and b =

−
2
8

, namely
J(x, y) = 1
2
￾
x y 
3 2
2 6 
x
y

−
￾ 2 −8


x
y

=
3
2
x
2 + 2xy + 3y
2 − 2x + 8y.
This quadratic ellipsoid, which is illustrated in Figure 49.2, has a unique minimum at
(2, −2). In order to find this minimum via the gradient descent with optimal step size
49.6. GRADIENT DESCENT METHODS FOR UNCONSTRAINED PROBLEMS 1693
Figure 49.2: The ellipsoid J(x, y) = 3
2
x
2 + 2xy + 3y
2 − 2x + 8y.
x
K4 K2 0 2 4
y
K4
K2
2
4
Figure 49.3: The level curves of J(x, y) = 3
2
x
2 + 2xy + 3y
2 − 2x + 8y and the associated
gradient vector field ∇J(x, y) = (3x + 2y − 2, 2x + 6y + 8).
parameter, we pick a starting point, say uk = (−2, −2), and calculate the search direction
wk = ∇J(−2, −2) = (−12, −8). Note that
∇J(x, y) = (3x + 2y − 2, 2x + 6y + 8) =  3 2
2 6 
x
y

−

−
2
8

is perpendicular to the appropriate elliptical level curve; see Figure 49.3. We next perform
the line search along the line given by the equation −8x + 12y = −8 and determine ρk. See
Figures 49.4 and 49.5.
1694 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
x
K4 K2 0 2 4
y
K4
K2
2
4
Figure 49.4: The level curves of J(x, y) = 3
2
x
2 + 2xy + 3y
2 − 2x + 8y and the red search line
with direction ∇J(−2, −2) = (−12, −8)
x
K4 K2 0 2 4
y
K4
K2
2
4
(-2,-2)
(2/25, -46/75)
Figure 49.5: Let uk = (−2, −2). When traversing along the red search line, we look for
the green perpendicular gradient vector. This gradient vector, which occurs at uk+1 =
(2/25, −46/75), provides a minimal ρk, since it has no nonzero projection on the search line.
In particular, we find that
ρk =
k
wkk
2
h
Awk, wki
=
13
75
.
This in turn gives us the new point
uk+1 = uk −
13
75
wk = (−2, −2) −
13
75
(−12, −8) = 
25
2
, −
46
75
,
and we continue the procedure by searching along the gradient direction ∇J(2/25, −46/75) =
(−224/75, 112/25). Observe that uk+1 = ( 25
2
, −
46
75 ) has a gradient vector which is perpen￾dicular to the search line with direction vector wk = ∇J(−2, −2) = (−12 − 8); see Figure
49.7. CONVERGENCE OF GRADIENT DESCENT WITH VARIABLE STEPSIZE 1695
49.5. Geometrically this procedure corresponds to intersecting the plane −8x + 12y =
−8 with the ellipsoid J(x, y) = 2
3x
2 + 2xy + 3y
2 − 2x + 8y to form the parabolic curve
f(x) = 25/6x
2 −2/3x−4, and then locating the x-coordinate of its apex which occurs when
f
0 (x) = 0, i.e when x = 2/25; see Figure 49.6. After 31 iterations, this procedure stabi￾Figure 49.6: Two views of the intersection between the plane −8x + 12y = −8 and the
ellipsoid J(x, y) = 2
3x
2 + 2xy + 3y
2 − 2x + 8y. The point uk+1 is the minimum of the
parabolic intersection.
lizes to point (2, −2), which as we know, is the unique minimum of the quadratic ellipsoid
J(x, y) = 2
3x
2 + 2xy + 3y
2 − 2x + 8y.
A proof of the convergence of the gradient method with backtracking line search, under
the hypothesis that J is strictly convex, is given in Boyd and Vandenberghe[29] (Section
9.3.1). More details on this method and the steepest descent method for the Euclidean norm
can also be found in Boyd and Vandenberghe [29] (Section 9.3).
49.7 Convergence of Gradient Descent with Variable
Stepsize
We now give a sufficient condition for the gradient method with variable stepsize parameter
to converge. In addition to requiring J to be an elliptic functional, we add a Lipschitz
1696 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
condition on the gradient of J. This time the space V can be infinite dimensional.
Proposition 49.14. Let J : V → R be a continuously differentiable functional defined on a
Hilbert space V . Suppose there exists two constants α > 0 and M > 0 such that
h∇Jv − ∇Ju, v − ui ≥ α k v − uk
2
for all u, v ∈ V ,
and the Lipschitz condition
k∇Jv − ∇Juk ≤ M k v − uk for all u, v ∈ V .
If there exists two real numbers a, b ∈ R such that
0 < a ≤ ρk ≤ b ≤
2α
M2
for all k ≥ 0,
then the gradient method with variable stepsize parameter converges. Furthermore, there is
some constant β > 0 (depending on α, M, a, b) such that
β < 1 and k uk − uk ≤ β
k
k u0 − uk ,
where u ∈ V is the unique minimum of J.
Proof. By hypothesis the functional J is elliptic, so by Theorem 49.8(2) it has a unique
minimum u characterized by the fact that ∇Ju = 0. Then since uk+1 = uk −ρk∇Juk
, we can
write
uk+1 − u = (uk − u) − ρk(∇Juk − ∇Ju). (∗)
Using the inequalities
h∇Juk − ∇Ju, uk − ui ≥ α k uk − uk
2
and
k∇Juk − ∇Juk ≤ M k uk − uk ,
and assuming that ρk > 0, it follows that
k
uk+1 − uk
2 = k uk − uk
2 − 2ρkh∇Juk − ∇Ju, uk − ui + ρ
2
k k∇Juk − ∇Juk
2
≤
 1 − 2αρk + M2
ρ
2
k
 k uk − uk
2
.
Consider the function
T(ρ) = M2
ρ
2 − 2αρ + 1.
Its graph is a parabola intersecting the y-axis at y = 1 for ρ = 0, it has a minimum for
ρ = α/M2
, and it also has the value y = 1 for ρ = 2α/M2
; see Figure 49.7. Therefore if we
pick a, b and ρk such that
0 < a ≤ ρk ≤ b < 2α
M2
,
49.7. CONVERGENCE OF GRADIENT DESCENT WITH VARIABLE STEPSIZE 1697
we ensure that for ρ ∈ [a, b] we have
T(ρ)
1/2 = (M2
ρ
2 − 2αρ + 1)1/2 ≤ (max{T(a), T(b)})
1/2 = β < 1.
Then by induction we get
k
uk+1 − uk ≤ β
k+1 k u0 − uk ,
which proves convergence.
(0,1)
a b
α
M2
α
M2 α
M2 ( , 1 - )
α
M2
2
y = 1
Figure 49.7: The parabola T(ρ) used in the proof of Proposition 49.14.
Remarks: In the proof of Proposition 49.14, it is the fact that V is complete which plays
a crucial role. If J is twice differentiable, the hypothesis
k∇Jv − ∇Juk ≤ M k v − uk for all u, v ∈ V
can be expressed as
sup
v∈V


∇2
Jv

 ≤ M.
In the case of a quadratic elliptic functional defined over R
n
,
J(v) = h Av, vi − hb, vi ,
the upper bound 2α/M2
can be improved. In this case we have
∇Jv = Av − b,
and we know that α = λ1 and M = λn do the job, where λ1 is the smallest eigenvalue of A
and λn is the largest eigenvalue of A. Hence we can pick a, b such that
0 < a ≤ ρk ≤ b < 2λ1
λ
2
n
.
1698 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
Since uk+1 = uk − ρk∇Juk
and ∇Juk = Auk − b, we have
uk+1 − u = (uk − u) − ρk(Auk − Au) = (I − ρkA)(uk − u),
so we get
k
uk+1 − uk ≤ kI − ρkAk 2
k
uk − uk .
However, since I − ρkA is a symmetric matrix, k I − ρkAk 2
is the largest absolute value of
its eigenvalues, so
k
I − ρkAk 2 ≤ max{|1 − ρkλ1|, |1 − ρkλn|}.
The function
µ(ρ) = max{|1 − ρλ1|, |1 − ρλn|}
is a piecewise affine function, and it is easy to see that if we pick a, b such that
0 < a ≤ ρk ≤ b < 2
λn
,
then
max
ρ∈[a,b]
µ(ρ) ≤ max{µ(a), µ(b)} < 1.
Therefore, the upper bound 2λ1/λ2
n
can be replaced by 2/λn, which is typically much larger.
A “good” pick for ρk is 2/(λ1 + λn) (as opposed to λ1/λ2
n
for the first version). In this case
|1 − ρkλ1| = |1 − ρkλn| =
λn − λ1
λn + λ1
,
so we get
β =
λn − λ1
λn + λ1
=
λn
λ1
− 1
λn
λ1
+ 1
=
cond2(A) − 1
cond2(A) + 1,
where cond2(A) = λn/λ1 is the condition number of the matrix A with respect to the spectral
norm. Thus we see that the larger the condition number of A is, the slower the convergence
of the method will be. This is not surprising since we already know that linear systems
involving ill-conditioned matrices (matrices with a large condition number) are problematic
and prone to numerical instability. One way to deal with this problem is to use a method
known as preconditioning.
We only described the most basic gradient descent methods. There are numerous variants,
and we only mention a few of these methods.
The method of scaling consists in using −ρkDk∇Juk
as descent direction, where Dk is
some suitably chosen symmetric positive definite matrix.
In the gradient method with extrapolation, uk+1 is determined by
uk+1 = uk − ρk∇Juk + βk(uk − uk−1).
49.8. STEEPEST DESCENT FOR AN ARBITRARY NORM 1699
Another rule for choosing the stepsize is Armijo’s rule.
These methods, and others, are discussed in detail in Berstekas [17].
Boyd and Vandenberghe discuss steepest descent methods for various types of norms
besides the Euclidean norm; see Boyd and Vandenberghe [29] (Section 9.4). Here is brief
summary.
49.8 Steepest Descent for an Arbitrary Norm
The idea is to make h∇Juk
, dki as negative as possible. To make the question sensible, we
have to limit the size of dk or normalize by the length of dk.
Let k k be any norm on R
n
. Recall from Section 14.7 that the dual norm is defined by
k
yk
D
= sup
x∈Rn
k
xk =1
|hx, yi|.
Definition 49.8. A normalized steepest descent direction (with respect to the norm k k ) is
any unit vector dnsd,k which achieves the minimum of the set of reals
{h∇Juk
, di | kdk = 1}.
By definition, k dnsd,kk = 1.
A unnormalized steepest descent direction dsd,k is defined as
dsd,k = k∇Juk
k
D
dnsd,k.
It can be shown that
h∇Juk
, dsd,ki = −(k∇Juk
k
D
)
2
;
see Boyd and Vandenberghe [29] (Section 9.4).
The steepest descent method (with respect to the norm k k ) consists of the following steps:
Given a starting point u0 ∈ dom(J) do:
repeat
(1) Compute the steepest descent direction dsd,k.
(2) Line search. Perform an exact or backtracking line search to find ρk.
(3) Update. uk+1 = uk + ρkdsd,k.
1700 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
until stopping criterion is satisfied.
If k k is the ` 2
-norm, then we see immediately that dsd,k = −∇Juk
, so in this case the
method coincides with the steepest descent method for the Euclidean norm as defined at the
beginning of Section 49.6 in (3) and (4).
If P is a symmetric positive definite matrix, it is easy to see that k zk P = (z
> P z)
1/2 = 

P
1/2
z

2
is a norm. Then it can be shown that the normalized steepest descent direction is
dnsd,k = −(∇Ju
>k
P
−1∇Juk
)
−1/2P
−1∇Juk
,
the dual norm is k zk
D =
  P
−1/2
z

2
, and the steepest descent direction with respect to k k P
is given by
dsd,k = −P
−1∇Juk
.
A judicious choice for P can speed up the rate of convergence of the gradient descent
method; see see Boyd and Vandenberghe [29] (Section 9.4.1 and Section 9.4.4).
If k k is the ` 1
-norm, then it can be shown that dnsd,k is determined as follows: let i be
any index for which k∇Juk
k ∞ = |(∇Juk
)i
|. Then
dnsd,k = −sign  ∂x
∂J
i
(uk)
 ei
,
where ei
is the ith canonical basis vector, and
dsd,k = −
∂J
∂xi
(uk)ei
.
For more details, see Boyd and Vandenberghe [29] (Section 9.4.2 and Section 9.4.4). It is
also shown in Boyd and Vandenberghe [29] (Section 9.4.3) that the steepest descent method
converges for any norm k k and any strictly convex function J.
One of the main goals in designing a gradient descent method is to ensure that the
convergence factor is as small as possible, which means that the method converges as quickly
as possible. Machine learning has been a catalyst for finding such methods. A method
discussed in Strang [171] (Chapter VI, Section 4) consists in adding a momentum term to
the gradient. In this method, uk+1 and dk+1 are determined by the following system of
equations:
uk+1 = uk − ρdk
dk+1 − ∇Juk+1 = βdk.
Of course the trick is to choose ρ and β in such a way that the convergence factor
is as small as possible. If J is given by a quadratic functional, say (1/2)u
> Au − b
> u, then
∇Juk+1 = Auk+1−b so we obtain a linear system. It turns out that the rate of convergence of
49.9. NEWTON’S METHOD FOR FINDING A MINIMUM 1701
the method is determined by the largest and the smallest eigenvalues of A. Strang discusses
this issue in the case of a 2 × 2 matrix. Convergence is significantly accelerated.
Another method is known as Nesterov acceleration. In this method,
uk+1 = uk + β(uk − uk−1) − ρ∇Juk+γ(uk−uk−1)
,
where β, ρ, γ are parameters. For details, see Strang [171] (Chapter VI, Section 4).
Lax also discusses other methods in which the step ρk is chosen using roots of Chebyshev
polynomials; see Lax [113], Chapter 17, Sections 2–4.
A variant of Newton’s method described in Section 41.2 can be used to find the minimum
of a function belonging to a certain class of strictly convex functions. This method is the
special case of the case where the norm is induced by a symmetric positive definite matrix
P, namely P = ∇2J(x), the Hessian of J at x.
49.9 Newton’s Method For Finding a Minimum
If J : Ω → R is a convex function defined on some open subset Ω of R
n which is twice
differentiable and if its Hessian ∇2J(x) is symmetric positive definite for all x ∈ Ω, then by
Proposition 40.12(2), the function J is strictly convex. In this case, for any x ∈ Ω, we have
the quadratic norm induced by P = ∇2J(x) as defined in the previous section, given by
k
uk ∇2J(x) = (u
> ∇2
J(x) u)
1/2
.
The steepest descent direction for this quadratic norm is given by
dnt = −(∇2
J(x))−1∇Jx.
The norm of dnt for the the quadratic norm defined by ∇2J(x) is given by
(d
>nt∇2
J(x) dnt)
1/2 =
￾ −(∇Jx)
> (∇2
J(x))−1∇2
J(x)(−(∇2
J(x))−1∇Jx)

1/2
=
￾ (∇Jx)
> (∇2
J(x))−1∇Jx

1/2
.
Definition 49.9. Given a function J : Ω → R as above, for any x ∈ Ω, the Newton step dnt
is defined by
dnt = −(∇2
J(x))−1∇Jx,
and the Newton decrement λ(x) is defined by
λ(x) = ￾ (∇Jx)
> (∇2
J(x))−1∇Jx

1/2
.
Observe that
h∇Jx, dnti = (∇Jx)
> (−(∇2
J(x))−1∇Jx) = −λ(x)
2
.
1702 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
If ∇Jx 6 = 0, we have λ(x) 6 = 0, so h∇Jx, dnti < 0, and dnt is indeed a descent direction. The
number h∇Jx, dnti is the constant that shows up during a backtracking line search.
A nice feature of the Newton step and of the Newton decrement is that they are affine
invariant. This means that if T is an invertible matrix and if we define g by g(y) = J(T y),
if the Newton step associated with J is denoted by dJ,nt and similarly the Newton step
associated with g is denoted by dg,nt, then it is shown in Boyd and Vandenberghe [29]
(Section 9.5.1) that
dg,nt = T
−1
dJ,nt,
and so
x + dJ,nt = T(y + dg,nt).
A similar properties applies to the Newton decrement.
Newton’s method consists of the following steps: Given a starting point u0 ∈ dom(J) and
a tolerance  > 0 do:
repeat
(1) Compute the Newton step and decrement
dnt,k = −(∇2J(uk))−1∇Juk
and λ(uk)
2 = (∇Juk
)
> (∇2J(uk))−1∇Juk
.
(2) Stopping criterion. quit if λ(uk)
2/2 ≤  .
(3) Line Search. Perform an exact or backtracking line search to find ρk.
(4) Update. uk+1 = uk + ρkdnt,k.
Observe that this is essentially the descent procedure of Section 49.8 using the Newton
step as search direction, except that the stopping criterion is checked just after computing
the search direction, rather than after the update (a very minor difference).
The convergence of Newton’s method is thoroughly analyzed in Boyd and Vandenberghe
[29] (Section 9.5.3). This analysis is made under the following assumptions:
(1) The function J : Ω → R is a convex function defined on some open subset Ω of R
n
which is twice differentiable and its Hessian ∇2J(x) is symmetric positive definite for
all x ∈ Ω. This implies that there are two constants m > 0 and M > 0 such that
mI  ∇2J(x)  MI for all x ∈ Ω, which means that the eigenvalues of ∇2J(x) belong
to [m, M].
(2) The Hessian is Lipschitzian, which means that there is some L ≥ 0 such that


∇2
J(x) − ∇2
J(y)

2
≤ L k x, yk 2
for all x, y ∈ Ω.
It turns out that the iterations of Newton’s method fall into two phases, depending
whether k∇Juk
k 2 ≥ η or k∇Juk
k 2 < η, where η is a number which depends on m, L, and the
constant α used in the backtracking line search, and η ≤ m2/L.
49.9. NEWTON’S METHOD FOR FINDING A MINIMUM 1703
(1) The first phase, called the damped Newton phase, occurs while k∇Juk
k 2 ≥ η. During
this phase, the procedure can choose a step size ρk = t < 1, and there is some constant
γ > 0 such that
J(uk+1) − J(uk) ≤ −γ.
(2) The second phase, called the quadratically convergent phase or pure Newton phase,
occurs while k∇Juk
k 2 < η. During this phase, the step size ρk = t = 1 is always
chosen, and we have
L
2m2

 ∇Juk+1
  2
≤

2m
L
2
k∇Juk
k 2

2
. (∗1)
If we denote the minimal value of f by p
∗
, then the number of damped Newton steps is
at most
J(u0) − p
∗
γ
.
Equation (∗1) and the fact that η ≤ m2/L shows that if k∇Juk
k 2 < η, then
  ∇Juk+1
  2
<
η. It follows by induction that for all ` ≥ k, we have
L
2m2

 ∇Ju` +1
  2
≤

2m
L
2
k∇Ju` k 2

2
, (∗2)
and thus (since η ≤ m2/L and k∇Juk
k 2 < η, we have (L/m2
) k∇Juk
k 2 < (L/m2
)η ≤ 1), so
L
2m2
k∇Ju` k 2 ≤

2m
L
2
k∇Juk
k 2

2
` −k
≤

1
2

2
` −k
, ` ≥ k. (∗3)
It is shown in Boyd and Vandenberghe [29] (Section 9.1.2) that the hypothesis mI  ∇2J(x)
implies that
J(x) − p
∗ ≤
1
2m
k∇Jxk
2
2
x ∈ Ω.
As a consequence, by (∗3), we have
J(u` ) − p
∗ ≤
1
2m
k∇Ju` k
2
2 ≤
2m3
L2
 2
1

2
` −k+1
. (∗4)
Equation (∗4) shows that the convergence during the quadratically convergence phase is
very fast. If we let

0 =
2m3
L2
,
then Equation (∗4) implies that we must have J(u` ) − p
∗ ≤  after no more than
log2
log2
( 0/)
1704 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
iterations. The term log2
log2
( 0/) grows extremely slowly as  goes to zero, and for practical
purposes it can be considered constant, say five or six (six iterations gives an accuracy of
about  ≈ 5 · 10−20
0).
In summary, the number of Newton iterations required to find a minimum of J is ap￾proximately bounded by
J(u0) − p
∗
γ
+ 6.
Examples of the application of Newton’s method and further discussion of its efficiency
are given in Boyd and Vandenberghe [29] (Section 9.5.4). Basically, Newton’s method has
a faster convergence rate than gradient or steepest descent. Its main disadvantage is the
cost for forming and storing the Hessian, and of computing the Newton step, which requires
solving a linear system.
There are two major shortcomings of the convergence analysis of Newton’s method as
sketched above. The first is a pracical one. The complexity estimates involve the constants
m, M, and L, which are almost never known in practice. As a result, the bound on the
number of steps required is almost never known specifically.
The second shortcoming is that although Newton’s method itself is affine invariant, the
analysis of convergence is very much dependent on the choice of coordinate system. If the
coordinate system is changed, the constants m, M, L also change. This can be viewed as an
aesthetic problem, but it would be nice if an analysis of convergence independent of an affine
change of coordinates could be given.
Nesterov and Nemirovski discovered a condition on functions that allows an affine￾invariant convergence analysis. This property, called self-concordance, is unfortunately not
very intuitive.
Definition 49.10. A (partial) convex function f defined on R is self-concordant if
|f
000 (x)| ≤ 2(f
00 (x))3/2
for all x ∈ R.
A (partial) convex function f defined on R
n
is self-concordant if for every nonzero v ∈ R
n
and all x ∈ R
n
, the function t 7→ J(x + tv) is self-concordant.
Affine and convex quadratic functions are obviously self-concordant, since f
000 = 0. There
are many more interesting self-concordant functions, for example, the function
X 7→ − log det(X), where X is a symmetric positive definite matrix.
Self-concordance is discussed extensively in Boyd and Vandenberghe [29] (Section 9.6).
The main point of self-concordance is that a coordinate system-invariant proof of convergence
can be given for a certain class of strictly convex self-concordant functions. This proof is
given in Boyd and Vandenberghe [29] (Section 9.6.4). Given a starting value u0, we assume
that the sublevel set {x ∈ R
n
| J(x) ≤ J(u0)} is closed and that J is bounded below. Then
there are two parameters η and γ as before, but depending only on the parameters α, β
involved in the line search, such that:
49.10. CONJUGATE GRADIENT METHODS; UNCONSTRAINED PROBLEMS 1705
(1) If λ(uk) > η, then
J(uk+1) − J(uk) ≤ −γ.
(2) If λ(uk) ≤ η, then the backtracking line search selects t = 1 and we have
2λ(uk+1) ≤ (2λ(uk))2
.
As a consequence, for all ` ≥ k, we have
J(u` ) − p
∗ ≤ λ(u` )
2 ≤

1
2

2
` −k+1
.
In the end, accuracy  > 0 is achieved in at most
20 − 8α
αβ(1 − 2α)
2
(J(u0) − p
∗
) + log2
log2
(1/)
iterations, where α and β are the constants involved in the line search. This bound is
obviously independent of the chosen coordinate system.
Contrary to intuition, the descent direction dk = −∇Juk
given by the opposite of the
gradient is not always optimal. In the next section we will see how a better direction can be
picked; this is the method of conjugate gradients.
