Ak op = sup
x∈Cn
x6=0
k
Axk
k
xk
= sup
x∈Cn
k
xk =1
k
Axk .
The function A 7→ kAk op is called the subordinate matrix norm or operator norm induced
by the norm k k .
Another notation for the operator norm of a matrix A (in particular, used by Horn and
Johnson [95]), is |||A|||.
It is easy to check that the function A 7→ kAk op is indeed a norm, and by definition, it
satisfies the property
k
Axk ≤ kAk op k xk , for all x ∈ C
n
.
A norm k k op on Mn(C) satisfying the above property is said to be subordinate to the vector
norm k k on C
n
. As a consequence of the above inequality, we have
k
ABxk ≤ kAk op k Bxk ≤ kAk op k Bk op k xk ,
for all x ∈ C
n
, which implies that
k
ABk op ≤ kAk op k Bk op for all A, B ∈ Mn(C),
showing that A 7→ kAk op is a matrix norm (it is submultiplicative).
Observe that the operator norm is also defined by
k
Ak op = inf{λ ∈ R | kAxk ≤ λ k xk , for all x ∈ C
n
}.
Since the function x 7→ kAxk is continuous (because | kAyk − kAxk | ≤ kAy − Axk ≤
CA k x − yk ) and the unit sphere S
n−1 = {x ∈ C
n
| kxk = 1} is compact, there is some
x ∈ C
n
such that k xk = 1 and
k
Axk = k Ak op .
Equivalently, there is some x ∈ C
n
such that x 6 = 0 and
k
Axk = k Ak op k xk .
Consequently we can replace sup by max in the definition of k Ak op (and inf by min), namely
k
Ak op = max
x∈Cn
k
xk =1
k
Axk .
344 CHAPTER 9. VECTOR NORMS AND MATRIX NORMS
The definition of an operator norm also implies that
k
Ik op = 1.
The above shows that the Frobenius norm is not a subordinate matrix norm for n ≥ 2
(why?).
If k k is a vector norm on C
n
, the operator norm k k op that it induces applies to matrices
in Mn(C). If we are careful to denote vectors and matrices so that no confusion arises, for
example, by using lower case letters for vectors and upper case letters for matrices, it should
be clear that k Ak op is the operator norm of the matrix A and that k xk is the vector norm of
x. Consequently, following common practice to alleviate notation, we will drop the subscript
“op” and simply write k Ak instead of k Ak op.
The notion of subordinate norm can be slightly generalized.
Definition 9.8. If K = R or K = C, for any norm k k on Mm,n(K), and for any two norms
k k
a
on Kn and k k b
on Km, we say that the norm k k is subordinate to the norms k k a
and
k k
b
if
k
Axk b ≤ kAk k xk a
for all A ∈ Mm,n(K) and all x ∈ Kn
.
Remark: For any norm k k on C
n
, we can define the function k k R
on Mn(R) by
k
Ak R = sup
x∈Rn
x6=0
k
Axk
k
xk
= sup
x∈Rn
k
xk =1
k
Axk .
The function A 7→ kAk R
is a matrix norm on Mn(R), and
k
Ak R ≤ kAk ,
for all real matrices A ∈ Mn(R). However, it is possible to construct vector norms k k on C
n
and real matrices A such that
k
Ak R < k Ak .
In order to avoid this kind of difficulties, we define subordinate matrix norms over Mn(C).
Luckily, it turns out that k Ak R = k Ak for the vector norms, k k 1
, k k
2
, and k k ∞.
We now prove Proposition 9.6 for real matrix norms.
Proposition 9.9. For any matrix norm k k on Mn(R) and for any square n × n matrix
A ∈ Mn(R), we have
ρ(A) ≤ kAk .
Proof. We follow the proof in Denis Serre’s book [156]. If A is a real matrix, the problem is
that the eigenvectors associated with the eigenvalue of maximum modulus may be complex.
We use a trick based on the fact that for every matrix A (real or complex),
ρ(A
k
) = (ρ(A))k
,
9.3. SUBORDINATE NORMS 345
which is left as an exercise (use Proposition 15.7 which shows that if (λ1, . . . , λn) are the
(not necessarily distinct) eigenvalues of A, then (λ
k
1
, . . . , λk
n
) are the eigenvalues of Ak
, for
k ≥ 1).
Pick any complex matrix norm k k c
on C
n
(for example, the Frobenius norm, or any
subordinate matrix norm induced by a norm on C
n
). The restriction of k k c
to real matrices
is a real norm that we also denote by k k c
. Now by Theorem 9.5, since Mn(R) has finite
dimension n
2
, there is some constant C > 0 so that
k
Bk c ≤ C k Bk , for all B ∈ Mn(R).
Furthermore, for every
 k ≥ 1 and for every real n×n matrix A, by Proposition 9.6, ρ(Ak
) ≤

Ak


c
, and because k k is a matrix norm,
  Ak

 ≤ kAk
k
, so we have
(ρ(A))k = ρ(A
k
) ≤
  A
k


c
≤ C
  A
k

 ≤ C k Ak
k
,
for all k ≥ 1. It follows that
ρ(A) ≤ C
1/k k Ak , for all k ≥ 1.
However because C > 0, we have limk7→∞ C
1/k = 1 (we have limk7→∞ k
1
log(C) = 0). There￾fore, we conclude that
ρ(A) ≤ kAk ,
as desired.
We now determine explicitly what are the subordinate matrix norms associated with the
vector norms k k 1
, k k
2
, and k k ∞.
Proposition 9.10. For every square matrix A = (aij ) ∈ Mn(C), we have
k
Ak 1 = sup
x∈Cn
k
xk 1=1
k
Axk 1 = max
j
nX
i=1
|aij |
k
Ak ∞ = sup
x∈Cn
k
xk ∞=1
k
Axk ∞ = max
i
nX
j=1
|aij |
k
Ak 2 = sup
x∈Cn
k
xk 2=1
k
Axk 2 =
p ρ(A∗A) = p ρ(AA∗
).
Note that k Ak 1
is the maximum of the ` 1
-norms of the columns of A and k Ak ∞ is the
maximum of the ` 1
-norms of the rows of A. Furthermore, k A∗k
2 = k Ak 2
, the norm k k 2
is
unitarily invariant, which means that
k
Ak 2 = k UAV k 2
for all unitary matrices U, V , and if A is a normal matrix, then k Ak 2 = ρ(A).
346 CHAPTER 9. VECTOR NORMS AND MATRIX NORMS
Proof. For every vector u, we have
k
Auk 1 =
X
i




X j
aijuj



 ≤
X
j
|uj
|
X
i
|aij | ≤  max
j
X
i
|aij |
 k uk 1
,
which implies that
k
Ak 1 ≤ max
j
nX
i=1
|aij |.
It remains to show that equality can be achieved. For this let j0 be some index such that
max
j
X
i
|aij | =
X
i
|aij0
|,
and let ui = 0 for all i 6 = j0 and uj0 = 1.
In a similar way, we have
k
Auk ∞ = max
i




X
j
aijuj



 ≤
 max
i
X
j
|aij |
 k uk ∞ ,
which implies that
k
Ak ∞ ≤ max
i
nX
j=1
|aij |.
To achieve equality, let i0 be some index such that
max
i
X
j
|aij | =
X
j
|ai0j
|.
The reader should check that the vector given by
uj =
(
ai0j
|ai0j |
if ai0j 6 = 0
1 if ai0j = 0
works.
We have
k
Ak
2
2 = sup
x∈Cn
x
∗x=1
k
Axk 2
2 = sup
x∈Cn
x
∗x=1
x
∗A
∗Ax.
Since the matrix A∗A is symmetric, it has real eigenvalues and it can be diagonalized with
respect to a unitary matrix. These facts can be used to prove that the function x 7→ x
∗A∗Ax
has a maximum on the sphere x
∗x = 1 equal to the largest eigenvalue of A∗A, namely,
ρ(A∗A). We postpone the proof until we discuss optimizing quadratic functions. Therefore,
k
Ak 2 =
p ρ(A∗A).
9.3. SUBORDINATE NORMS 347
Let use now prove that ρ(A∗A) = ρ(AA∗
). First assume that ρ(A∗A) > 0. In this case, there
is some eigenvector u (6= 0) such that
A
∗Au = ρ(A
∗A)u,
and since ρ(A∗A) > 0, we must have Au 6 = 0. Since Au 6 = 0,
AA∗
(Au) = A(A
∗Au) = ρ(A
∗A)Au
which means that ρ(A∗A) is an eigenvalue of AA∗
, and thus
ρ(A
∗A) ≤ ρ(AA∗
).
Because (A∗
)
∗ = A, by replacing A by A∗
, we get
ρ(AA∗
) ≤ ρ(A
∗A),
and so ρ(A∗A) = ρ(AA∗
).
If ρ(A∗A) = 0, then we must have ρ(AA∗
) = 0, since otherwise by the previous reasoning
we would have ρ(A∗A) = ρ(AA∗
) > 0. Hence, in all case
k
Ak
2
2 = ρ(A
∗A) = ρ(AA∗
) = k A
∗
k 2
2
.
For any unitary matrices U and V , it is an easy exercise to prove that V
∗A∗AV and A∗A
have the same eigenvalues, so
k
Ak
2
2 = ρ(A
∗A) = ρ(V
∗A
∗AV ) = k AV k 2
2
,
and also
k
Ak
2
2 = ρ(A
∗A) = ρ(A
∗U
∗UA) = k UAk 2
2
.
Finally, if A is a normal matrix (AA∗ = A∗A), it can be shown that there is some unitary
matrix U so that
A = UDU∗
,
where D = diag(λ1, . . . , λn) is a diagonal matrix consisting of the eigenvalues of A, and thus
A
∗A = (UDU∗
)
∗UDU∗ = UD∗U
∗UDU∗ = UD∗DU∗
.
However, D∗D = diag(|λ1|
2
, . . . , |λn|
2
), which proves that
ρ(A
∗A) = ρ(D
∗D) = max
i
|λi
|
2 = (ρ(A))2
,
so that k Ak 2 = ρ(A).
Definition 9.9. For A = (aij ) ∈ Mn(C), the norm k Ak 2
is often called the spectral norm.
348 CHAPTER 9. VECTOR NORMS AND MATRIX NORMS
Observe that Property (3) of Proposition 9.7 says that
k
Ak 2 ≤ kAk F ≤
√
n k Ak 2
,
which shows that the Frobenius norm is an upper bound on the spectral norm. The Frobenius
norm is much easier to compute than the spectral norm.
The reader will check that the above proof still holds if the matrix A is real (change
unitary to orthogonal), confirming the fact that k Ak R = k Ak for the vector norms k k 1
, k k
2
,
and k k ∞. It is also easy to verify that the proof goes through for rectangular m×n matrices,
with the same formulae. Similarly, the Frobenius norm given by
k
Ak F =

mX
i=1
nX
j=1
|aij |
2

1/2
=
p tr(A∗A) = p tr(AA∗
)
is also a norm on rectangular matrices. For these norms, whenever AB makes sense, we have
k
ABk ≤ kAk k Bk .
Remark: It can be shown that for any two real numbers p, q ≥ 1 such that 1
p
+
1
q
= 1, we
have
k
A
∗
k
q = k Ak p = sup{<(y
∗Ax) | kxk p = 1, k yk q = 1} = sup{|hAx, yi| | kxk p = 1, k yk q = 1},
where k A∗k
q
and k Ak p
are the operator norms.
Remark: Let (E, k k ) and (F, k k ) be two normed vector spaces (for simplicity of notation,
we use the same symbol k k for the norms on E and F; this should not cause any confusion).
Recall that a function f : E → F is continuous if for every a ∈ E, for every  > 0, there is
some η > 0 such that for all x ∈ E,
if k x − ak ≤ η then k f(x) − f(a)k ≤ .
It is not hard to show that a linear map f : E → F is continuous iff there is some constant
C ≥ 0 such that
k
f(x)k ≤ C k xk for all x ∈ E.
If so, we say that f is bounded (or a linear bounded operator ). We let L(E; F) denote the
set of all continuous (equivalently, bounded) linear maps from E to F. Then we can define
the operator norm (or subordinate norm) k k on L(E; F) as follows: for every f ∈ L(E; F),
k
fk = sup
x∈E
x6=0
k
f(x)k
k
xk
= sup
x∈E
k
xk =1
k
f(x)k ,
9.4. INEQUALITIES INVOLVING SUBORDINATE NORMS 349
or equivalently by
k
fk = inf{λ ∈ R | kf(x)k ≤ λ k xk , for all x ∈ E}.
Here because E may be infinite-dimensional, sup can’t be replaced by max and inf can’t
be replaced by min. It is not hard to show that the map f 7→ kfk is a norm on L(E; F)
satisfying the property
k
f(x)k ≤ kfk k xk
for all x ∈ E, and that if f ∈ L(E; F) and g ∈ L(F; G), then
k
g ◦ fk ≤ kgk k fk .
Operator norms play an important role in functional analysis, especially when the spaces E
and F are complete.
9.4 Inequalities Involving Subordinate Norms
In this section we discuss two technical inequalities which will be needed for certain proofs
in the last three sections of this chapter. First we prove a proposition which will be needed
when we deal with the condition number of a matrix.
Proposition 9.11. Let k k be any matrix norm, and let B ∈ Mn(C) such that k Bk < 1.
(1) If k k is a subordinate matrix norm, then the matrix I + B is invertible and


(I + B)
−1

 ≤
1 − k
1
Bk
.
(2) If a matrix of the form I + B is singular, then k Bk ≥ 1 for every matrix norm (not
necessarily subordinate).
Proof. (1) Observe that (I + B)u = 0 implies Bu = −u, so
k
uk = k Buk .
Recall that
k
Buk ≤ kBk k uk
for every subordinate norm. Since k Bk < 1, if u 6 = 0, then
k
Buk < k uk ,
which contradicts k uk = k Buk . Therefore, we must have u = 0, which proves that I + B is
injective, and thus bijective, i.e., invertible. Then we have
(I + B)
−1 + B(I + B)
−1 = (I + B)(I + B)
−1 = I,
350 CHAPTER 9. VECTOR NORMS AND MATRIX NORMS
so we get
(I + B)
−1 = I − B(I + B)
−1
,
which yields


(I + B)
−1

 ≤ 1 + k Bk
  (I + B)
−1


,
and finally,
(2) If I + B is singular, then −

1 is an eigenvalue of
(I + B)
−1

 ≤
1 − k
1
B
B
k
.
, and by Proposition 9.6, we get
ρ(B) ≤ kBk , which implies 1 ≤ ρ(B) ≤ kBk .
The second inequality is a result is that is needed to deal with the convergence of se￾quences of powers of matrices.
Proposition 9.12. For every matrix A ∈ Mn(C) and for every  > 0, there is some subor￾dinate matrix norm k k such that
k
Ak ≤ ρ(A) + .
Proof. By Theorem 15.5, there exists some invertible matrix U and some upper triangular
matrix T such that
A = UT U −1
,
and say that
T =


λ1 t12 t13 · · · t1n
0 λ2 t23 · · · t2n
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 · · · λn−1 tn−1 n
0 0 · · · 0 λn


,
where λ1, . . . , λn are the eigenvalues of A. For every δ 6 = 0, define the diagonal matrix
Dδ = diag(1, δ, δ2
, . . . , δn−1
),
and consider the matrix
(UDδ)
−1A(UDδ) = Dδ
−1T Dδ =


λ1 δt12 δ
2
t13 · · · δ
n−1
t1n
0 λ2 δt23 · · · δ
n−2
t2n
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 · · · λn−1 δtn−1 n
0 0 · · · 0 λn


.
Now define the function k k : Mn(C) → R by
k
Bk =
  (UDδ)
−1B(UDδ)
 ∞
,
9.5. CONDITION NUMBERS OF MATRICES 351
for every B ∈ Mn(C). Then it is easy to verify that the above function is the matrix norm
subordinate to the vector norm
v 7→
  (UDδ)
−1
v
 ∞
.
Furthermore, for every  > 0, we can pick δ so that
nX
j=i+1
|δ
j−i
tij | ≤ , 1 ≤ i ≤ n − 1,
and by definition of the norm k k ∞, we get
k
Ak ≤ ρ(A) + ,
which shows that the norm that we have constructed satisfies the required properties.
Note that equality is generally not possible; consider the matrix
A =

0 1
0 0 ,
for which ρ(A) = 0 < k Ak , since A 6 = 0.
9.5 Condition Numbers of Matrices
Unfortunately, there exist linear systems Ax = b whose solutions are not stable under small
perturbations of either b or A. For example, consider the system


10 7 8 7
7 5 6 5
8 6 10 9
7 5 9 10




x
x
1
2
x
x
3
4

 =


32
23
33
31

 .
The reader should check that it has the solution x = (1, 1, 1, 1). If we perturb slightly the
right-hand side as b + ∆b, where
∆b =


0.1
−
0
0
.1
.1
−0.1

 ,
we obtain the new system


10 7 8 7
7 5 6 5
8 6 10 9
7 5 9 10




x1 + ∆x1
x2 + ∆x2
x3 + ∆x3
x4 + ∆x4

 =


32.1
22.9
33.1
30.9

 .
352 CHAPTER 9. VECTOR NORMS AND MATRIX NORMS
The new solution turns out to be x + ∆x = (9.2, −12.6, 4.5, −1.1), where
∆x = (9.2, −12.6, 4.5, −1.1) − (1, 1, 1, 1) = (8.2, −13.6, 3.5, −2.1).
Then a relative error of the data in terms of the one-norm,
k
∆bk 1
k
bk 1
=
0.4
119
=
4
1190
≈
1
300
,
produces a relative error in the input
k
∆xk 1
k
xk 1
=
27.4
4
≈ 7.
So a relative error of the order 1/300 in the data produces a relative error of the order 7/1
in the solution, which represents an amplification of the relative error of the order 2100.
Now let us perturb the matrix slightly, obtaining the new system


7
10 7 8
.08 5.04 6 5
.1 7.2
8 5.98 9.98 9
6.99 4.99 9 9.98




x1 + ∆x1
x2 + ∆x2
x3 + ∆x3
x4 + ∆x4

 =


32
23
33
31

 .
This time the solution is x + ∆x = (−81, 137, −34, 22). Again a small change in the data
alters the result rather drastically. Yet the original system is symmetric, has determinant 1,
and has integer entries. The problem is that the matrix of the system is badly conditioned,
a concept that we will now explain.
Given an invertible matrix A, first assume that we perturb b to b+∆b, and let us analyze
the change between the two exact solutions x and x + ∆x of the two systems
Ax = b
A(x + ∆x) = b + ∆b.
We also assume that we have some norm k k and we use the subordinate matrix norm on
matrices. From
Ax = b
Ax + A∆x = b + ∆b,
we get
∆x = A
−1∆b,
and we conclude that
k
∆xk ≤
  A
−1


k ∆bk
k
bk ≤ kAk k xk .
9.5. CONDITION NUMBERS OF MATRICES 353
Consequently, the relative error in the result k ∆xk / k xk is bounded in terms of the relative
error k ∆bk / k bk in the data as follows:
k
∆xk
k
xk
≤
￾ k Ak
  A
−1



k ∆bk
k
bk
.
Now let us assume that A is perturbed to A+∆A, and let us analyze the change between
the exact solutions of the two systems
Ax = b
(A + ∆A)(x + ∆x) = b.
The second equation yields Ax + A∆x + ∆A(x + ∆x) = b, and by subtracting the first
equation we get
∆x = −A
−1∆A(x + ∆x).
It follows that
k
∆xk ≤
  A
−1


k ∆Ak k x + ∆xk ,
which can be rewritten as
k
x
k + ∆
∆xk
xk
≤
￾ k Ak
  A
−1



k
k ∆
A
A
kk
.
Observe that the above reasoning is valid even if the matrix A + ∆A is singular, as long
as x + ∆x is a solution of the second system. Furthermore, if k ∆Ak is small enough, it is
not unreasonable to expect that the ratio k ∆xk / k x + ∆xk is close to k ∆xk / k xk . This will
be made more precise later.
In summary, for each of the two perturbations, we see that the relative error in the result
is bounded by the relative error in the data, multiplied the number k Ak k A−1k
. In fact, this
factor turns out to be optimal and this suggests the following definition:
Definition 9.10. For any subordinate matrix norm k k , for any invertible matrix A, the
number
cond(A) = k Ak
  A
−1


is called the condition number of A relative to k k .
The condition number cond(A) measures the sensitivity of the linear system Ax = b to
variations in the data b and A; a feature referred to as the condition of the system. Thus,
when we says that a linear system is ill-conditioned, we mean that the condition number of
its matrix is large. We can sharpen the preceding analysis as follows:
Proposition 9.13. Let A be an invertible matrix and let x and x + ∆x be the solutions of
the linear systems
Ax = b
A(x + ∆x) = b + ∆b.
354 CHAPTER 9. VECTOR NORMS AND MATRIX NORMS
If b 6 = 0, then the inequality
k
∆xk
k
xk
≤ cond(A)
k
∆bk
k
bk
holds and is the best possible. This means that for a given matrix A, there exist some vectors
b 6 = 0 and ∆b 6 = 0 for which equality holds.
Proof. We already proved the inequality. Now, because k k is a subordinate matrix norm,
there exist some vectors x 6 = 0 and ∆b 6 = 0 for which


A
−1∆b
 =
  A
−1


k ∆bk and k Axk = k Ak k xk .
Proposition 9.14. Let A be an invertible matrix and let x and x + ∆x be the solutions of
the two systems
Ax = b
(A + ∆A)(x + ∆x) = b.
If b 6 = 0, then the inequality
k
x
k + ∆
∆xk
xk
≤ cond(A)
k
k
∆
A
A
kk
holds and is the best possible. This means that given a matrix A, there exist a vector b 6 = 0
and a matrix ∆A 6 = 0 for which equality holds. Furthermore, if k ∆Ak is small enough (for
instance, if k ∆Ak < 1/ k A−1k
), we have
k
∆xk
k
xk
≤ cond(A)
k
∆Ak
k
Ak
(1 + O(k ∆Ak ));
in fact, we have
