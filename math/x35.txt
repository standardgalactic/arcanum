For example,
R
i,j =


1
.
.
.
1
0 0 · · · 0 −1
0 1 · · · 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 · · · 1 0
1 0 · · · 0 0
1
.
.
.
1


.
(1) Prove that the Ri,j are rotation matrices. Use the matrices Rij to form a basis of the
n × n skew-symmetric matrices.
486 CHAPTER 12. EUCLIDEAN SPACES
(2) Consider the n × n symmetric matrices S
i,j defined for all i, j with 1 ≤ i < j ≤ n
and n ≥ 3, such that the only nonzero entries are
S
i,j (i, j) = 1
S
i,j (i, i) = 0
S
i,j (j, i) = 1
S
i,j (j, j) = 0
S
i,j (k, k) = 1, 1 ≤ k ≤ n, k 6 = i, j,
and if i + 2 ≤ j then S
i,j (i + 1, i + 1) = −1, else if i > 1 and j = i + 1 then S
i,j (1, 1) = −1,
and if i = 1 and j = 2, then S
i,j (3, 3) = −1.
For example,
S
i,j =


1
.
.
.
1
0 0 · · · 0 1
0 −1 · · · 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 · · · 1 0
1 0 · · · 0 0
1
.
.
.
1


.
Note that S
i,j has a single diagonal entry equal to −1. Prove that the S
i,j are rotations
matrices.
Use Problem 3.15 together with the S
i,j to form a basis of the n×n symmetric matrices.
(3) Prove that if n ≥ 3, the set of all linear combinations of matrices in SO(n) is the
space Mn(R) of all n × n matrices.
Prove that if n ≥ 3 and if a matrix A ∈ Mn(R) commutes with all rotations matrices,
then A commutes with all matrices in Mn(R).
What happens for n = 2?
Problem 12.13. (1) Let H be the affine hyperplane in R
n given by the equation
a1x1 + · · · + anxn = c,
with ai 6 = 0 for some i, 1 ≤ i ≤ n. The linear hyperplane H0 parallel to H is given by the
equation
a1x1 + · · · + anxn = 0,
12.11. PROBLEMS 487
and we say that a vector y ∈ R
n
is orthogonal (or perpendicular ) to H iff y is orthogonal to
H0. Let h be the intersection of H with the line through the origin and perpendicular to H.
Prove that the coordinates of h are given by
c
a
2
1 + · · · + a
2
n
(a1, . . . , an).
(2) For any point p ∈ H, prove that k hk ≤ kpk . Thus, it is natural to define the distance
d(O, H) from the origin O to the hyperplane H as d(O, H) = k hk . Prove that
d(O, H) = |c|
(a
2
1 + · · · + a
2
n
)
1
2
.
(3) Let S be a finite set of n ≥ 3 points in the plane (R
2
). Prove that if for every pair of
distinct points pi
, pj ∈ S, there is a third point pk ∈ S (distinct from pi and pj ) such that
pi
, pj
, pk belong to the same (affine) line, then all points in S belong to a common (affine)
line.
Hint. Proceed by contradiction and use a minimality argument. This is either ∞-hard or
relatively easy, depending how you proceed!
Problem 12.14. (The space of closed polygons in R
2
, after Hausmann and Knutson)
An open polygon P in the plane is a sequence P = (v1, . . . , vn+1) of points vi ∈ R
2
called vertices (with n ≥ 1). A closed polygon, for short a polygon, is an open polygon
P = (v1, . . . , vn+1) such that vn+1 = v1. The sequence of edge vectors (e1, . . . , en) associated
with the open (or closed) polygon P = (v1, . . . , vn+1) is defined by
ei = vi+1 − vi
, i = 1, . . . , n.
Thus, a closed or open polygon is also defined by a pair (v1,(e1, . . . , en)), with the vertices
given by
vi+1 = vi + ei
, i = 1, . . . , n.
Observe that a polygon (v1,(e1, . . . , en)) is closed iff
e1 + · · · + en = 0.
Since every polygon (v1,(e1, . . . , en)) can be translated by −v1, so that v1 = (0, 0), we
may assume that our polygons are specified by a sequence of edge vectors.
Recall that the plane R
2
is isomorphic to C, via the isomorphism
(x, y) 7→ x + iy.
We will represent each edge vector ek by the square of a complex number wk = ak+ibk. Thus,
every sequence of complex numbers (w1, . . . , wn) defines a polygon (namely, (w1
2
, . . . , wn
2
)).
488 CHAPTER 12. EUCLIDEAN SPACES
This representation is many-to-one: the sequences (±w1, . . . , ±wn) describe the same poly￾gon. To every sequence of complex numbers (w1, . . . , wn), we associate the pair of vectors
(a, b), with a, b ∈ R
n
, such that if wk = ak + ibk, then
a = (a1, . . . , an), b = (b1, . . . , bn).
The mapping
(w1, . . . , wn) 7→ (a, b)
is clearly a bijection, so we can also represent polygons by pairs of vectors (a, b) ∈ R
n × R
n
.
(1) Prove that a polygon P represented by a pair of vectors (a, b) ∈ R
n × R
n
is closed iff
a · b = 0 and k ak 2 = k bk 2
.
(2) Given a polygon P represented by a pair of vectors (a, b) ∈ R
n × R
n
, the length l(P)
of the polygon P is defined by l(P) = |w1|
2 + · · · + |wn|
2
, with wk = ak + ibk. Prove that
l(P) = k ak
2
2 + k bk
2
2
.
Deduce from (a) and (b) that every closed polygon of length 2 with n edges is represented
by a n × 2 matrix A such that A> A = I.
Remark: The space of all a n × 2 real matrices A such that A> A = I is a space known as
the Stiefel manifold S(2, n).
(3) Recall that in R
2
, the rotation of angle θ specified by the matrix
Rθ =

cos
sin θ
θ −
cos
sin
θ
θ

is expressed in terms of complex numbers by the map
z 7→ zeiθ
.
Let P be a polygon represented by a pair of vectors (a, b) ∈ R
n × R
n
. Prove that the
polygon Rθ(P) obtained by applying the rotation Rθ to every vertex wk
2 = (ak + ibk)
2 of P
is specified by the pair of vectors
(cos(θ/2)a − sin(θ/2)b, sin(θ/2)a + cos(θ/2)b) =


a1 b1
a2 b2
.
.
.
.
.
.
an bn


 −
cos(
sin(
θ/
θ/
2) sin(
2) cos(
θ/
θ/
2)
2) .
(4) The reflection ρx about the x-axis corresponds to the map
z 7→ z,
12.11. PROBLEMS 489
whose matrix is,

1 0
0 −1

.
Prove that the polygon ρx(P) obtained by applying the reflection ρx to every vertex wk
2 =
(ak + ibk)
2 of P is specified by the pair of vectors
(a, −b) =


a1 b1
a2 b2
.
.
.
.
.
.
an bn



1 0
0 −1

.
(5) Let Q ∈ O(2) be any isometry such that det(Q) = −1 (a reflection). Prove that there
is a rotation R−θ ∈ SO(2) such that
Q = ρx ◦ R−θ.
Prove that the isometry Q, which is given by the matrix
Q =

cos
sin θ
θ
−
sin
cos
θ
θ

,
is the reflection about the line corresponding to the angle θ/2 (the line of equation y =
tan(θ/2)x).
Prove that the polygon Q(P) obtained by applying the reflection Q = ρx ◦ R−θ to every
vertex wk
2 = (ak + ibk)
2 of P, is specified by the pair of vectors
(cos(θ/2)a + sin(θ/2)b, sin(θ/2)a − cos(θ/2)b) =


a1 b1
a2 b2
.
.
.
.
.
.
an bn



cos(
sin(θ/
θ/
2)
2) sin(
− cos(
θ/
θ/
2)
2) .
(6) Define an equivalence relation ∼ on S(2, n) such that if A1, A2 ∈ S(2, n) are any n×2
matrices such that A>1 A1 = A>2 A2 = I, then
A1 ∼ A2 iff A2 = A1Q for some Q ∈ O(2).
Prove that the quotient G(2, n) = S(2, n)/ ∼ is in bijection with the set of all 2-dimensional
subspaces (the planes) of R
n
. The space G(2, n) is called a Grassmannian manifold.
Prove that up to translations and isometries in O(2) (rotations and reflections), the
n-sided closed polygons of length 2 are represented by planes in G(2, n).
490 CHAPTER 12. EUCLIDEAN SPACES
Problem 12.15. (1) Find two symmetric matrices, A and B, such that AB is not symmetric.
(2) Find two matrices A and B such that
e
A
e
B
6 = e
A+B
.
Hint. Try
A = π


0 0 0
0 0
0 1 0
−1

 and B = π


−
0 0 1
0 0 0
1 0 0

 ,
and use the Rodrigues formula.
(3) Find some square matrices A, B such that AB 6 = BA, yet
e
A
e
B = e
A+B
.
Hint. Look for 2 × 2 matrices with zero trace and use Problem 9.15.
Problem 12.16. Given a field K and any nonempty set I, let K(I) be the subset of the
cartesian product KI
consisting of all functions λ: I → K with finite support, which means
that λ(i) = 0 for all but finitely many i ∈ I. We usually denote the function defined by λ as
(λi)i∈I , and call is a family indexed by I. We define addition and multiplication by a scalar
as follows:
(λi)i∈I + (µi)i∈I = (λi + µi)i∈I ,
and
α · (µi)i∈I = (αµi)i∈I .
(1) Check that K(I)
is a vector space.
(2) If I is any nonempty subset, for any i ∈ I, we denote by ei the family (ej )j∈I defined
so that
ej =
(
1 if
0 if
j
j
=
6
=
i
i.
Prove that the family (ei)i∈I is linearly independent and spans K(I)
, so that it is a basis of
K(I)
called the canonical basis of K(I)
. When I is finite, say of cardinality n, then prove
that K(I)
is isomorphic to Kn
.
(3) The function ι: I → K(I)
, such that ι(i) = ei
for every i ∈ I, is clearly an injection.
For any other vector space F, for any function f : I → F, prove that there is a unique
linear map f : K(I) → F, such that
f = f ◦ ι,
as in the following commutative diagram:
I
ι /
f !
❈❈❈❈❈❈❈❈❈
K(I)


f
F
.
12.11. PROBLEMS 491
We call the vector space K(I)
the vector space freely generated by the set I.
Problem 12.17. (Some pitfalls of infinite dimension) Let E be the vector space freely
generated by the set of natural numbers, N = {0, 1, 2, . . .}, and let (e0, e1, e2, . . . , en, . . .) be
its canonical basis. We define the function ϕ such that
ϕ(ei
, ej ) =



δij if i, j ≥ 1,
1 if
1/2
j
if
i
i
=
= 0
j
, j
= 0
≥
,
1,
1/2
i
if i ≥ 1, j = 0,
and we extend ϕ by bilinearity to a function ϕ: E×E → K. This means that if u =
P i∈N
λiei
and v =
P j∈N µjej
, then
ϕ

X
i∈N
λiei
,
X
j∈N
µjej
 =
X
i,j∈N
λiµjϕ(ei
, ej ),
but remember that λi 6 = 0 and µj 6 = 0 only for finitely many indices i, j.
(1) Prove that ϕ is positive definite, so that it is an inner product on E.
What would happen if we changed 1/2
j
to 1 (or any constant)?
(2) Let H be the subspace of E spanned by the family (ei)i≥1, a hyperplane in E. Find
H⊥ and H⊥⊥, and prove that
H 6 = H
⊥⊥.
(3) Let U be the subspace of E spanned by the family (e2i)i≥1, and let V be the subspace
of E spanned by the family (e2i−1)i≥1. Prove that
U
⊥ = V
V
⊥ = U
U
⊥⊥ = U
V
⊥⊥ = V,
yet
(U ∩ V )
⊥ 6 = U
⊥ + V
⊥
and
(U + V )
⊥⊥ 6 = U + V.
If W is the subspace spanned by e0 and e1, prove that
(W ∩ H)
⊥ 6 = W⊥ + H
⊥.
492 CHAPTER 12. EUCLIDEAN SPACES
(4) Consider the dual space E
∗ of E, and let (e
∗
i
)i∈N be the family of dual forms of the
basis (ei)i∈N . Check that the family (e
∗
i
)i∈N is linearly independent.
(5) Let f ∈ E
∗ be the linear form defined by
f(ei) = 1 for all i ∈ N.
Prove that f is not in the subspace spanned by the e
∗
i
. If F is the subspace of E
∗
spanned
by the e
∗
i
and f, find F
0 and F
00, and prove that
F 6 = F
00
.
Chapter 13
QR-Decomposition for Arbitrary
Matrices
13.1 Orthogonal Reflections
Hyperplane reflections are represented by matrices called Householder matrices. These ma￾trices play an important role in numerical methods, for instance for solving systems of linear
equations, solving least squares problems, for computing eigenvalues, and for transforming a
symmetric matrix into a tridiagonal matrix. We prove a simple geometric lemma that imme￾diately yields the QR-decomposition of arbitrary matrices in terms of Householder matrices.
Orthogonal symmetries are a very important example of isometries. First let us review
the definition of projections, introduced in Section 6.1, just after Proposition 6.7. Given a
vector space E, let F and G be subspaces of E that form a direct sum E = F ⊕ G. Since
every u ∈ E can be written uniquely as u = v + w, where v ∈ F and w ∈ G, we can define
the two projections pF : E → F and pG : E → G such that pF (u) = v and pG(u) = w. In
Section 6.1 we used the notation π1 and π2, but in this section it is more convenient to use
pF and pG.
It is immediately verified that pG and pF are linear maps, and that
p
2
F = pF , p2
G = pG, pF ◦ pG = pG ◦ pF = 0, and pF + pG = id.
.
Definition 13.1. Given a vector space E, for any two subspaces F and G that form a direct
sum E = F ⊕ G, the symmetry (or reflection) with respect to F and parallel to G is the
linear map s: E → E defined such that
s(u) = 2pF (u) − u,
for every u ∈ E.
493
494 CHAPTER 13. QR-DECOMPOSITION FOR ARBITRARY MATRICES
Because pF + pG = id, note that we also have
s(u) = pF (u) − pG(u)
and
s(u) = u − 2pG(u),
s
2 = id, s is the identity on F, and s = −id on G.
We now assume that E is a Euclidean space of finite dimension.
Definition 13.2. Let E be a Euclidean space of finite dimension n. For any two subspaces
F and G, if F and G form a direct sum E = F ⊕ G and F and G are orthogonal, i.e.,
F = G⊥, the orthogonal symmetry (or reflection) with respect to F and parallel to G is the
linear map s: E → E defined such that
s(u) = 2pF (u) − u = pF (u) − pG(u),
for every u ∈ E. When F is a hyperplane, we call s a hyperplane symmetry with respect to
F (or reflection about F), and when G is a plane (and thus dim(F) = n − 2), we call s a
flip about F.
A reflection about a hyperplane F is shown in Figure 13.1.
u
s(u)
pG (u)
− pG (u)
pF (u)
F
G
Figure 13.1: A reflection about the peach hyperplane F. Note that u is purple, pF (u) is blue
and pG(u) is red.
For any two vectors u, v ∈ E, it is easily verified using the bilinearity of the inner product
that
k
u + vk
2 − ku − vk
2 = 4(u · v). (∗)
In particular, if u · v = 0, then k u + vk = k u − vk . Then since
u = pF (u) + pG(u)
13.1. ORTHOGONAL REFLECTIONS 495
and
s(u) = pF (u) − pG(u),
and since F and G are orthogonal, it follows that
pF (u) · pG(v) = 0,
and thus by (∗)
k
s(u)k = k pF (u) − pG(u)k = k pF (u) + pG(u)k = k uk ,
so that s is an isometry.
Using Proposition 12.10, it is possible to find an orthonormal basis (e1, . . . , en) of E
consisting of an orthonormal basis of F and an orthonormal basis of G. Assume that F
has dimension p, so that G has dimension n − p. With respect to the orthonormal basis
(e1, . . . , en), the symmetry s has a matrix of the form

Ip 0
0 −In−p

.
Thus, det(s) = (−1)n−p
, and s is a rotation iff n − p is even. In particular, when F is
a hyperplane H, we have p = n − 1 and n − p = 1, so that s is an improper orthogonal
transformation. When F = {0}, we have s = −id, which is called the symmetry with respect
to the origin. The symmetry with respect to the origin is a rotation iff n is even, and
an improper orthogonal transformation iff n is odd. When n is odd, since s ◦ s = id and
det(s) = (−1)n = −1, we observe that every improper orthogonal transformation f is the
composition f = (f ◦ s) ◦ s of the rotation f ◦ s with s, the symmetry with respect to the
origin. When G is a plane, p = n − 2, and det(s) = (−1)2 = 1, so that a flip about F is
a rotation. In particular, when n = 3, F is a line, and a flip about the line F is indeed a
rotation of measure π as illustrated by Figure 13.2.
Remark: Given any two orthogonal subspaces F, G forming a direct sum E = F ⊕ G, let
f be the symmetry with respect to F and parallel to G, and let g be the symmetry with
respect to G and parallel to F. We leave as an exercise to show that
f ◦ g = g ◦ f = −id.
When F = H is a hyperplane, we can give an explicit formula for s(u) in terms of any
nonnull vector w orthogonal to H. Indeed, from
u = pH(u) + pG(u),
since pG(u) ∈ G and G is spanned by w, which is orthogonal to H, we have
pG(u) = λw
496 CHAPTER 13. QR-DECOMPOSITION FOR ARBITRARY MATRICES
F
G
p (u) F u s(u)
Figure 13.2: A flip in R
3
is a rotation of π about the F axis.
for some λ ∈ R, and we get
u · w = λk wk
2
,
and thus
pG(u) = (u · w)
k
wk 2
w.
Since
s(u) = u − 2pG(u),
we get
s(u) = u − 2
(u · w)
k
wk 2
w.
Since the above formula is important, we record it in the following proposition.
Proposition 13.1. Let E be a finite-dimensional Euclidean space and let H be a hyperplane
in E. For any nonzero vector w orthogonal to H, the hyperplane reflection s about H is
given by
s(u) = u − 2
(u · w)
k
wk 2
w, u ∈ E.
Such reflections are represented by matrices called Householder matrices, which play an
important role in numerical matrix analysis (see Kincaid and Cheney [102] or Ciarlet [41]).
Definition 13.3. A Householder matrix is a matrix of the form
H = In − 2
WW>
k
Wk 2
= In − 2
WW>
W> W
,
where W ∈ R
n
is a nonzero vector.
13.1. ORTHOGONAL REFLECTIONS 497
Householder matrices are symmetric and orthogonal. It is easily checked that over an
orthonormal basis (e1, . . . , en), a hyperplane reflection about a hyperplane H orthogonal to
a nonzero vector w is represented by the matrix
H = In − 2
WW>
k
Wk 2
,
where W is the column vector of the coordinates of w over the basis (e1, . . . , en). Since
pG(u) = (u · w)
k
wk 2
w,
the matrix representing pG is
WW>
W> W
,
and since pH + pG = id, the matrix representing pH is
In −
WW>
W> W
.
These formulae can be used to derive a formula for a rotation of R
3
, given the direction w
of its axis of rotation and given the angle θ of rotation.
The following fact is the key to the proof that every isometry can be decomposed as a
product of reflections.
Proposition 13.2. Let E be any nontrivial Euclidean space. For any two vectors u, v ∈ E,
if k uk = k vk , then there is a hyperplane H such that the reflection s about H maps u to v,
and if u 6 = v, then this reflection is unique. See Figure 13.3.
Proof. If u = v, then any hyperplane containing u does the job. Otherwise, we must have
H = {v − u}
⊥, and by the above formula,
s(u) = u − 2
(u · (v − u))
k
(v − u)k
2
(v − u) = u +
2k uk
2 − 2u · v
k
(v − u)k
2
(v − u),
and since
k
(v − u)k
2 = k uk
2 + k vk
2 − 2u · v
and k uk = k vk , we have
k
(v − u)k
2 = 2k uk
2 − 2u · v,
and thus, s(u) = v.

If E is a complex vector space and the inner product is Hermitian, Proposition 13.2
is false. The problem is that the vector v −u does not work unless the inner product
u· v is real! The proposition can be salvaged enough to yield the QR-decomposition in terms
of Householder transformations; see Section 14.5.
We now show that hyperplane reflections can be used to obtain another proof of the
QR-decomposition.
498 CHAPTER 13. QR-DECOMPOSITION FOR ARBITRARY MATRICES
H
v-u
u s(u) = v
Figure 13.3: In R
3
, the (hyper)plane perpendicular to v − u reflects u onto v.
13.2 QR-Decomposition Using Householder Matrices
First we state the result geometrically. When translated in terms of Householder matrices,
we obtain the fact advertised earlier that every matrix (not necessarily invertible) has a
QR-decomposition.
Proposition 13.3. Let E be a nontrivial Euclidean space of dimension n. For any orthonor￾mal basis (e1, . . ., en) and for any n-tuple of vectors (v1, . . ., vn), there is a sequence of n
isometries h1, . . . , hn such that hi is a hyperplane reflection or the identity, and if (r1, . . . , rn)
are the vectors given by
rj = hn ◦ · · · ◦ h2 ◦ h1(vj ),
then every rj is a linear combination of the vectors (e1, . . . , ej ), 1 ≤ j ≤ n. Equivalently, the
matrix R whose columns are the components of the rj over the basis (e1, . . . , en) is an upper
triangular matrix. Furthermore, the hi can be chosen so that the diagonal entries of R are
nonnegative.
Proof. We proceed by induction on n. For n = 1, we have v1 = λe1 for some λ ∈ R. If
λ ≥ 0, we let h1 = id, else if λ < 0, we let h1 = −id, the reflection about the origin.
For n ≥ 2, we first have to find h1. Let
r1,1 = k v1k .
13.2. QR-DECOMPOSITION USING HOUSEHOLDER MATRICES 499
If v1 = r1,1e1, we let h1 = id. Otherwise, there is a unique hyperplane reflection h1 such that
h1(v1) = r1,1 e1,
defined such that
h1(u) = u − 2
(u · w1)
k
w1k
2
w1
for all u ∈ E, where
w1 = r1,1 e1 − v1.
The map h1 is the reflection about the hyperplane H1 orthogonal to the vector w1 = r1,1 e1 −
v1. See Figure 13.4. Letting
e2
v1 H1
r
1,1
e1
Figure 13.4: The construction of h1 in Proposition 13.3.
r1 = h1(v1) = r1,1 e1,
it is obvious that r1 belongs to the subspace spanned by e1, and r1,1 = k v1k is nonnegative.
Next assume that we have found k linear maps h1, . . . , hk, hyperplane reflections or the
identity, where 1 ≤ k ≤ n − 1, such that if (r1, . . . , rk) are the vectors given by
rj = hk ◦ · · · ◦ h2 ◦ h1(vj ),
then every rj
is a linear combination of the vectors (e1, . . . , ej ), 1 ≤ j ≤ k. See Figure
13.5. The vectors (e1, . . . , ek) form a basis for the subspace denoted by Uk
0
, the vectors
(ek+1, . . . , en) form a basis for the subspace denoted by Uk
00
, the subspaces Uk
0
and Uk
00
are
orthogonal, and E = Uk
0 ⊕ Uk
00
. Let
uk+1 = hk ◦ · · · ◦ h2 ◦ h1(vk+1).
We can write
uk+1 = u
0k+1 + u
00k+1,
500 CHAPTER 13. QR-DECOMPOSITION FOR ARBITRARY MATRICES
e direction
e direction
e direction
1
2
3
v
v
1
2
e direction
e direction
e direction
1
2
3
v
1
h
1
r
1
Figure 13.5: The construction of r1 = h1(v1) in Proposition 13.3.
where u
0k+1 ∈ Uk
0
and u
00k+1 ∈ Uk
00
. See Figure 13.6. Let
rk+1,k+1 = k u
00k+1k .
If u
00k+1 = rk+1,k+1 ek+1, we let hk+1 = id. Otherwise, there is a unique hyperplane reflection
hk+1 such that
hk+1(u
00k+1) = rk+1,k+1 ek+1,
defined such that
hk+1(u) = u − 2
(u · wk+1)
k
wk+1k 2
wk+1
for all u ∈ E, where
wk+1 = rk+1,k+1 ek+1 − u
00k+1.
The map hk+1 is the reflection about the hyperplane Hk+1 orthogonal to the vector wk+1 =
rk+1,k+1 ek+1−u
00k+1. However, since u
00k+1, ek+1 ∈ Uk
00
and Uk
0
is orthogonal to Uk
00
, the subspace
Uk
0
is contained in Hk+1, and thus, the vectors (r1, . . . , rk) and u
0k+1, which belong to Uk
0
, are
invariant under hk+1. This proves that
hk+1(uk+1) = hk+1(u
0k+1) + hk+1(u
00k+1) = u
0k+1 + rk+1,k+1 ek+1
is a linear combination of (e1, . . . , ek+1). Letting
rk+1 = hk+1(uk+1) = u
0k+1 + rk+1,k+1 ek+1,
since uk+1 = hk ◦ · · · ◦ h2 ◦ h1(vk+1), the vector
rk+1 = hk+1 ◦ · · · ◦ h2 ◦ h1(vk+1)
is a linear combination of (e1, . . . , ek+1). See Figure 13.7. The coefficient of rk+1 over ek+1
is rk+1,k+1 = k u
00k+1k , which is nonnegative. This concludes the induction step, and thus the
proof.
13.2. QR-DECOMPOSITION USING HOUSEHOLDER MATRICES 501
e direction
e direction
e direction
2
3
v2
h1
(v2)
e direction
e direction
e direction
1
2
3
h1
(v2)
u
2
u2 ‘ ‘’
2
Figure 13.6: The construction of u2 = h1(v2) and its decomposition as u2 = u
02 + u
002
.
Remarks:
(1) Since every hi
is a hyperplane reflection or the identity,
ρ = hn ◦ · · · ◦ h2 ◦ h1
is an isometry.
(2) If we allow negative diagonal entries in R, the last isometry hn may be omitted.
(3) Instead of picking rk,k = k u
00k
k
, which means that
wk = rk,k ek − u
00k
,
where 1 ≤ k ≤ n, it might be preferable to pick rk,k = −ku
00k
k
if this makes k wkk
2
larger, in which case
wk = rk,k ek + u
00k
.
Indeed, since the definition of hk involves division by k wkk
2
, it is desirable to avoid
division by very small numbers.
(4) The method also applies to any m-tuple of vectors (v1, . . . , vm), with m ≤ n. Then
R is an upper triangular m × m matrix and Q is an n × m matrix with orthogonal
columns (Q> Q = Im). We leave the minor adjustments to the method as an exercise
to the reader
Proposition 13.3 directly yields the QR-decomposition in terms of Householder transfor￾mations (see Strang [169, 170], Golub and Van Loan [80], Trefethen and Bau [176], Kincaid
and Cheney [102], or Ciarlet [41]).
502 CHAPTER 13. QR-DECOMPOSITION FOR ARBITRARY MATRICES
e direction
e direction
e direction
e direction
1
2
3
u2
‘’
e direction
e direction
e direction
e direction
1
2
3
h1
(v2)
u2
‘
h2(u2
‘’)
h2 h1
(v2)
2
2
Figure 13.7: The construction of h2 and r2 = h2 ◦ h1(v2) in Proposition 13.3.
Theorem 13.4. For every real n × n matrix A, there is a sequence H1, . . ., Hn of matrices,
where each Hi is either a Householder matrix or the identity, and an upper triangular matrix
R such that
R = Hn · · · H2H1A.
As a corollary, there is a pair of matrices Q, R, where Q is orthogonal and R is upper
triangular, such that A = QR (a QR-decomposition of A). Furthermore, R can be chosen
so that its diagonal entries are nonnegative.
Proof. The jth column of A can be viewed as a vector vj over the canonical basis (e1, . . . , en)
of E
n
(where (ej )i = 1 if i = j, and 0 otherwise, 1 ≤ i, j ≤ n). Applying Proposition 13.3
to (v1, . . . , vn), there is a sequence of n isometries h1, . . . , hn such that hi
is a hyperplane
reflection or the identity, and if (r1, . . . , rn) are the vectors given by
rj = hn ◦ · · · ◦ h2 ◦ h1(vj ),
