For example,
R
i,j =
ï£«
ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­
1
.
.
.
1
0 0 Â· Â· Â· 0 âˆ’1
0 1 Â· Â· Â· 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 Â· Â· Â· 1 0
1 0 Â· Â· Â· 0 0
1
.
.
.
1
ï£¶
ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£¸
.
(1) Prove that the Ri,j are rotation matrices. Use the matrices Rij to form a basis of the
n Ã— n skew-symmetric matrices.
486 CHAPTER 12. EUCLIDEAN SPACES
(2) Consider the n Ã— n symmetric matrices S
i,j defined for all i, j with 1 â‰¤ i < j â‰¤ n
and n â‰¥ 3, such that the only nonzero entries are
S
i,j (i, j) = 1
S
i,j (i, i) = 0
S
i,j (j, i) = 1
S
i,j (j, j) = 0
S
i,j (k, k) = 1, 1 â‰¤ k â‰¤ n, k 6 = i, j,
and if i + 2 â‰¤ j then S
i,j (i + 1, i + 1) = âˆ’1, else if i > 1 and j = i + 1 then S
i,j (1, 1) = âˆ’1,
and if i = 1 and j = 2, then S
i,j (3, 3) = âˆ’1.
For example,
S
i,j =
ï£«
ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­
1
.
.
.
1
0 0 Â· Â· Â· 0 1
0 âˆ’1 Â· Â· Â· 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 Â· Â· Â· 1 0
1 0 Â· Â· Â· 0 0
1
.
.
.
1
ï£¶
ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£¸
.
Note that S
i,j has a single diagonal entry equal to âˆ’1. Prove that the S
i,j are rotations
matrices.
Use Problem 3.15 together with the S
i,j to form a basis of the nÃ—n symmetric matrices.
(3) Prove that if n â‰¥ 3, the set of all linear combinations of matrices in SO(n) is the
space Mn(R) of all n Ã— n matrices.
Prove that if n â‰¥ 3 and if a matrix A âˆˆ Mn(R) commutes with all rotations matrices,
then A commutes with all matrices in Mn(R).
What happens for n = 2?
Problem 12.13. (1) Let H be the affine hyperplane in R
n given by the equation
a1x1 + Â· Â· Â· + anxn = c,
with ai 6 = 0 for some i, 1 â‰¤ i â‰¤ n. The linear hyperplane H0 parallel to H is given by the
equation
a1x1 + Â· Â· Â· + anxn = 0,
12.11. PROBLEMS 487
and we say that a vector y âˆˆ R
n
is orthogonal (or perpendicular ) to H iff y is orthogonal to
H0. Let h be the intersection of H with the line through the origin and perpendicular to H.
Prove that the coordinates of h are given by
c
a
2
1 + Â· Â· Â· + a
2
n
(a1, . . . , an).
(2) For any point p âˆˆ H, prove that k hk â‰¤ kpk . Thus, it is natural to define the distance
d(O, H) from the origin O to the hyperplane H as d(O, H) = k hk . Prove that
d(O, H) = |c|
(a
2
1 + Â· Â· Â· + a
2
n
)
1
2
.
(3) Let S be a finite set of n â‰¥ 3 points in the plane (R
2
). Prove that if for every pair of
distinct points pi
, pj âˆˆ S, there is a third point pk âˆˆ S (distinct from pi and pj ) such that
pi
, pj
, pk belong to the same (affine) line, then all points in S belong to a common (affine)
line.
Hint. Proceed by contradiction and use a minimality argument. This is either âˆž-hard or
relatively easy, depending how you proceed!
Problem 12.14. (The space of closed polygons in R
2
, after Hausmann and Knutson)
An open polygon P in the plane is a sequence P = (v1, . . . , vn+1) of points vi âˆˆ R
2
called vertices (with n â‰¥ 1). A closed polygon, for short a polygon, is an open polygon
P = (v1, . . . , vn+1) such that vn+1 = v1. The sequence of edge vectors (e1, . . . , en) associated
with the open (or closed) polygon P = (v1, . . . , vn+1) is defined by
ei = vi+1 âˆ’ vi
, i = 1, . . . , n.
Thus, a closed or open polygon is also defined by a pair (v1,(e1, . . . , en)), with the vertices
given by
vi+1 = vi + ei
, i = 1, . . . , n.
Observe that a polygon (v1,(e1, . . . , en)) is closed iff
e1 + Â· Â· Â· + en = 0.
Since every polygon (v1,(e1, . . . , en)) can be translated by âˆ’v1, so that v1 = (0, 0), we
may assume that our polygons are specified by a sequence of edge vectors.
Recall that the plane R
2
is isomorphic to C, via the isomorphism
(x, y) 7â†’ x + iy.
We will represent each edge vector ek by the square of a complex number wk = ak+ibk. Thus,
every sequence of complex numbers (w1, . . . , wn) defines a polygon (namely, (w1
2
, . . . , wn
2
)).
488 CHAPTER 12. EUCLIDEAN SPACES
This representation is many-to-one: the sequences (Â±w1, . . . , Â±wn) describe the same polyï¿¾gon. To every sequence of complex numbers (w1, . . . , wn), we associate the pair of vectors
(a, b), with a, b âˆˆ R
n
, such that if wk = ak + ibk, then
a = (a1, . . . , an), b = (b1, . . . , bn).
The mapping
(w1, . . . , wn) 7â†’ (a, b)
is clearly a bijection, so we can also represent polygons by pairs of vectors (a, b) âˆˆ R
n Ã— R
n
.
(1) Prove that a polygon P represented by a pair of vectors (a, b) âˆˆ R
n Ã— R
n
is closed iff
a Â· b = 0 and k ak 2 = k bk 2
.
(2) Given a polygon P represented by a pair of vectors (a, b) âˆˆ R
n Ã— R
n
, the length l(P)
of the polygon P is defined by l(P) = |w1|
2 + Â· Â· Â· + |wn|
2
, with wk = ak + ibk. Prove that
l(P) = k ak
2
2 + k bk
2
2
.
Deduce from (a) and (b) that every closed polygon of length 2 with n edges is represented
by a n Ã— 2 matrix A such that A> A = I.
Remark: The space of all a n Ã— 2 real matrices A such that A> A = I is a space known as
the Stiefel manifold S(2, n).
(3) Recall that in R
2
, the rotation of angle Î¸ specified by the matrix
RÎ¸ =

cos
sin Î¸
Î¸ âˆ’
cos
sin
Î¸
Î¸

is expressed in terms of complex numbers by the map
z 7â†’ zeiÎ¸
.
Let P be a polygon represented by a pair of vectors (a, b) âˆˆ R
n Ã— R
n
. Prove that the
polygon RÎ¸(P) obtained by applying the rotation RÎ¸ to every vertex wk
2 = (ak + ibk)
2 of P
is specified by the pair of vectors
(cos(Î¸/2)a âˆ’ sin(Î¸/2)b, sin(Î¸/2)a + cos(Î¸/2)b) =
ï£«
ï£¬ï£¬ï£¬ï£­
a1 b1
a2 b2
.
.
.
.
.
.
an bn
ï£¶
ï£·ï£·ï£·ï£¸
 âˆ’
cos(
sin(
Î¸/
Î¸/
2) sin(
2) cos(
Î¸/
Î¸/
2)
2) .
(4) The reflection Ïx about the x-axis corresponds to the map
z 7â†’ z,
12.11. PROBLEMS 489
whose matrix is,

1 0
0 âˆ’1

.
Prove that the polygon Ïx(P) obtained by applying the reflection Ïx to every vertex wk
2 =
(ak + ibk)
2 of P is specified by the pair of vectors
(a, âˆ’b) =
ï£«
ï£¬ï£¬ï£¬ï£­
a1 b1
a2 b2
.
.
.
.
.
.
an bn
ï£¶
ï£·ï£·ï£·ï£¸

1 0
0 âˆ’1

.
(5) Let Q âˆˆ O(2) be any isometry such that det(Q) = âˆ’1 (a reflection). Prove that there
is a rotation Râˆ’Î¸ âˆˆ SO(2) such that
Q = Ïx â—¦ Râˆ’Î¸.
Prove that the isometry Q, which is given by the matrix
Q =

cos
sin Î¸
Î¸
âˆ’
sin
cos
Î¸
Î¸

,
is the reflection about the line corresponding to the angle Î¸/2 (the line of equation y =
tan(Î¸/2)x).
Prove that the polygon Q(P) obtained by applying the reflection Q = Ïx â—¦ Râˆ’Î¸ to every
vertex wk
2 = (ak + ibk)
2 of P, is specified by the pair of vectors
(cos(Î¸/2)a + sin(Î¸/2)b, sin(Î¸/2)a âˆ’ cos(Î¸/2)b) =
ï£«
ï£¬ï£¬ï£¬ï£­
a1 b1
a2 b2
.
.
.
.
.
.
an bn
ï£¶
ï£·ï£·ï£·ï£¸

cos(
sin(Î¸/
Î¸/
2)
2) sin(
âˆ’ cos(
Î¸/
Î¸/
2)
2) .
(6) Define an equivalence relation âˆ¼ on S(2, n) such that if A1, A2 âˆˆ S(2, n) are any nÃ—2
matrices such that A>1 A1 = A>2 A2 = I, then
A1 âˆ¼ A2 iff A2 = A1Q for some Q âˆˆ O(2).
Prove that the quotient G(2, n) = S(2, n)/ âˆ¼ is in bijection with the set of all 2-dimensional
subspaces (the planes) of R
n
. The space G(2, n) is called a Grassmannian manifold.
Prove that up to translations and isometries in O(2) (rotations and reflections), the
n-sided closed polygons of length 2 are represented by planes in G(2, n).
490 CHAPTER 12. EUCLIDEAN SPACES
Problem 12.15. (1) Find two symmetric matrices, A and B, such that AB is not symmetric.
(2) Find two matrices A and B such that
e
A
e
B
6 = e
A+B
.
Hint. Try
A = Ï€
ï£«
ï£­
0 0 0
0 0
0 1 0
âˆ’1
ï£¶
ï£¸ and B = Ï€
ï£«
ï£­
âˆ’
0 0 1
0 0 0
1 0 0
ï£¶
ï£¸ ,
and use the Rodrigues formula.
(3) Find some square matrices A, B such that AB 6 = BA, yet
e
A
e
B = e
A+B
.
Hint. Look for 2 Ã— 2 matrices with zero trace and use Problem 9.15.
Problem 12.16. Given a field K and any nonempty set I, let K(I) be the subset of the
cartesian product KI
consisting of all functions Î»: I â†’ K with finite support, which means
that Î»(i) = 0 for all but finitely many i âˆˆ I. We usually denote the function defined by Î» as
(Î»i)iâˆˆI , and call is a family indexed by I. We define addition and multiplication by a scalar
as follows:
(Î»i)iâˆˆI + (Âµi)iâˆˆI = (Î»i + Âµi)iâˆˆI ,
and
Î± Â· (Âµi)iâˆˆI = (Î±Âµi)iâˆˆI .
(1) Check that K(I)
is a vector space.
(2) If I is any nonempty subset, for any i âˆˆ I, we denote by ei the family (ej )jâˆˆI defined
so that
ej =
(
1 if
0 if
j
j
=
6
=
i
i.
Prove that the family (ei)iâˆˆI is linearly independent and spans K(I)
, so that it is a basis of
K(I)
called the canonical basis of K(I)
. When I is finite, say of cardinality n, then prove
that K(I)
is isomorphic to Kn
.
(3) The function Î¹: I â†’ K(I)
, such that Î¹(i) = ei
for every i âˆˆ I, is clearly an injection.
For any other vector space F, for any function f : I â†’ F, prove that there is a unique
linear map f : K(I) â†’ F, such that
f = f â—¦ Î¹,
as in the following commutative diagram:
I
Î¹ /
f !
âˆâˆâˆâˆâˆâˆâˆâˆâˆ
K(I)


f
F
.
12.11. PROBLEMS 491
We call the vector space K(I)
the vector space freely generated by the set I.
Problem 12.17. (Some pitfalls of infinite dimension) Let E be the vector space freely
generated by the set of natural numbers, N = {0, 1, 2, . . .}, and let (e0, e1, e2, . . . , en, . . .) be
its canonical basis. We define the function Ï• such that
Ï•(ei
, ej ) =
ï£±
ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£³
Î´ij if i, j â‰¥ 1,
1 if
1/2
j
if
i
i
=
= 0
j
, j
= 0
â‰¥
,
1,
1/2
i
if i â‰¥ 1, j = 0,
and we extend Ï• by bilinearity to a function Ï•: EÃ—E â†’ K. This means that if u =
P iâˆˆN
Î»iei
and v =
P jâˆˆN Âµjej
, then
Ï•

X
iâˆˆN
Î»iei
,
X
jâˆˆN
Âµjej
 =
X
i,jâˆˆN
Î»iÂµjÏ•(ei
, ej ),
but remember that Î»i 6 = 0 and Âµj 6 = 0 only for finitely many indices i, j.
(1) Prove that Ï• is positive definite, so that it is an inner product on E.
What would happen if we changed 1/2
j
to 1 (or any constant)?
(2) Let H be the subspace of E spanned by the family (ei)iâ‰¥1, a hyperplane in E. Find
HâŠ¥ and HâŠ¥âŠ¥, and prove that
H 6 = H
âŠ¥âŠ¥.
(3) Let U be the subspace of E spanned by the family (e2i)iâ‰¥1, and let V be the subspace
of E spanned by the family (e2iâˆ’1)iâ‰¥1. Prove that
U
âŠ¥ = V
V
âŠ¥ = U
U
âŠ¥âŠ¥ = U
V
âŠ¥âŠ¥ = V,
yet
(U âˆ© V )
âŠ¥ 6 = U
âŠ¥ + V
âŠ¥
and
(U + V )
âŠ¥âŠ¥ 6 = U + V.
If W is the subspace spanned by e0 and e1, prove that
(W âˆ© H)
âŠ¥ 6 = WâŠ¥ + H
âŠ¥.
492 CHAPTER 12. EUCLIDEAN SPACES
(4) Consider the dual space E
âˆ— of E, and let (e
âˆ—
i
)iâˆˆN be the family of dual forms of the
basis (ei)iâˆˆN . Check that the family (e
âˆ—
i
)iâˆˆN is linearly independent.
(5) Let f âˆˆ E
âˆ— be the linear form defined by
f(ei) = 1 for all i âˆˆ N.
Prove that f is not in the subspace spanned by the e
âˆ—
i
. If F is the subspace of E
âˆ—
spanned
by the e
âˆ—
i
and f, find F
0 and F
00, and prove that
F 6 = F
00
.
Chapter 13
QR-Decomposition for Arbitrary
Matrices
13.1 Orthogonal Reflections
Hyperplane reflections are represented by matrices called Householder matrices. These maï¿¾trices play an important role in numerical methods, for instance for solving systems of linear
equations, solving least squares problems, for computing eigenvalues, and for transforming a
symmetric matrix into a tridiagonal matrix. We prove a simple geometric lemma that immeï¿¾diately yields the QR-decomposition of arbitrary matrices in terms of Householder matrices.
Orthogonal symmetries are a very important example of isometries. First let us review
the definition of projections, introduced in Section 6.1, just after Proposition 6.7. Given a
vector space E, let F and G be subspaces of E that form a direct sum E = F âŠ• G. Since
every u âˆˆ E can be written uniquely as u = v + w, where v âˆˆ F and w âˆˆ G, we can define
the two projections pF : E â†’ F and pG : E â†’ G such that pF (u) = v and pG(u) = w. In
Section 6.1 we used the notation Ï€1 and Ï€2, but in this section it is more convenient to use
pF and pG.
It is immediately verified that pG and pF are linear maps, and that
p
2
F = pF , p2
G = pG, pF â—¦ pG = pG â—¦ pF = 0, and pF + pG = id.
.
Definition 13.1. Given a vector space E, for any two subspaces F and G that form a direct
sum E = F âŠ• G, the symmetry (or reflection) with respect to F and parallel to G is the
linear map s: E â†’ E defined such that
s(u) = 2pF (u) âˆ’ u,
for every u âˆˆ E.
493
494 CHAPTER 13. QR-DECOMPOSITION FOR ARBITRARY MATRICES
Because pF + pG = id, note that we also have
s(u) = pF (u) âˆ’ pG(u)
and
s(u) = u âˆ’ 2pG(u),
s
2 = id, s is the identity on F, and s = âˆ’id on G.
We now assume that E is a Euclidean space of finite dimension.
Definition 13.2. Let E be a Euclidean space of finite dimension n. For any two subspaces
F and G, if F and G form a direct sum E = F âŠ• G and F and G are orthogonal, i.e.,
F = GâŠ¥, the orthogonal symmetry (or reflection) with respect to F and parallel to G is the
linear map s: E â†’ E defined such that
s(u) = 2pF (u) âˆ’ u = pF (u) âˆ’ pG(u),
for every u âˆˆ E. When F is a hyperplane, we call s a hyperplane symmetry with respect to
F (or reflection about F), and when G is a plane (and thus dim(F) = n âˆ’ 2), we call s a
flip about F.
A reflection about a hyperplane F is shown in Figure 13.1.
u
s(u)
pG (u)
âˆ’ pG (u)
pF (u)
F
G
Figure 13.1: A reflection about the peach hyperplane F. Note that u is purple, pF (u) is blue
and pG(u) is red.
For any two vectors u, v âˆˆ E, it is easily verified using the bilinearity of the inner product
that
k
u + vk
2 âˆ’ ku âˆ’ vk
2 = 4(u Â· v). (âˆ—)
In particular, if u Â· v = 0, then k u + vk = k u âˆ’ vk . Then since
u = pF (u) + pG(u)
13.1. ORTHOGONAL REFLECTIONS 495
and
s(u) = pF (u) âˆ’ pG(u),
and since F and G are orthogonal, it follows that
pF (u) Â· pG(v) = 0,
and thus by (âˆ—)
k
s(u)k = k pF (u) âˆ’ pG(u)k = k pF (u) + pG(u)k = k uk ,
so that s is an isometry.
Using Proposition 12.10, it is possible to find an orthonormal basis (e1, . . . , en) of E
consisting of an orthonormal basis of F and an orthonormal basis of G. Assume that F
has dimension p, so that G has dimension n âˆ’ p. With respect to the orthonormal basis
(e1, . . . , en), the symmetry s has a matrix of the form

Ip 0
0 âˆ’Inâˆ’p

.
Thus, det(s) = (âˆ’1)nâˆ’p
, and s is a rotation iff n âˆ’ p is even. In particular, when F is
a hyperplane H, we have p = n âˆ’ 1 and n âˆ’ p = 1, so that s is an improper orthogonal
transformation. When F = {0}, we have s = âˆ’id, which is called the symmetry with respect
to the origin. The symmetry with respect to the origin is a rotation iff n is even, and
an improper orthogonal transformation iff n is odd. When n is odd, since s â—¦ s = id and
det(s) = (âˆ’1)n = âˆ’1, we observe that every improper orthogonal transformation f is the
composition f = (f â—¦ s) â—¦ s of the rotation f â—¦ s with s, the symmetry with respect to the
origin. When G is a plane, p = n âˆ’ 2, and det(s) = (âˆ’1)2 = 1, so that a flip about F is
a rotation. In particular, when n = 3, F is a line, and a flip about the line F is indeed a
rotation of measure Ï€ as illustrated by Figure 13.2.
Remark: Given any two orthogonal subspaces F, G forming a direct sum E = F âŠ• G, let
f be the symmetry with respect to F and parallel to G, and let g be the symmetry with
respect to G and parallel to F. We leave as an exercise to show that
f â—¦ g = g â—¦ f = âˆ’id.
When F = H is a hyperplane, we can give an explicit formula for s(u) in terms of any
nonnull vector w orthogonal to H. Indeed, from
u = pH(u) + pG(u),
since pG(u) âˆˆ G and G is spanned by w, which is orthogonal to H, we have
pG(u) = Î»w
496 CHAPTER 13. QR-DECOMPOSITION FOR ARBITRARY MATRICES
F
G
p (u) F u s(u)
Figure 13.2: A flip in R
3
is a rotation of Ï€ about the F axis.
for some Î» âˆˆ R, and we get
u Â· w = Î»k wk
2
,
and thus
pG(u) = (u Â· w)
k
wk 2
w.
Since
s(u) = u âˆ’ 2pG(u),
we get
s(u) = u âˆ’ 2
(u Â· w)
k
wk 2
w.
Since the above formula is important, we record it in the following proposition.
Proposition 13.1. Let E be a finite-dimensional Euclidean space and let H be a hyperplane
in E. For any nonzero vector w orthogonal to H, the hyperplane reflection s about H is
given by
s(u) = u âˆ’ 2
(u Â· w)
k
wk 2
w, u âˆˆ E.
Such reflections are represented by matrices called Householder matrices, which play an
important role in numerical matrix analysis (see Kincaid and Cheney [102] or Ciarlet [41]).
Definition 13.3. A Householder matrix is a matrix of the form
H = In âˆ’ 2
WW>
k
Wk 2
= In âˆ’ 2
WW>
W> W
,
where W âˆˆ R
n
is a nonzero vector.
13.1. ORTHOGONAL REFLECTIONS 497
Householder matrices are symmetric and orthogonal. It is easily checked that over an
orthonormal basis (e1, . . . , en), a hyperplane reflection about a hyperplane H orthogonal to
a nonzero vector w is represented by the matrix
H = In âˆ’ 2
WW>
k
Wk 2
,
where W is the column vector of the coordinates of w over the basis (e1, . . . , en). Since
pG(u) = (u Â· w)
k
wk 2
w,
the matrix representing pG is
WW>
W> W
,
and since pH + pG = id, the matrix representing pH is
In âˆ’
WW>
W> W
.
These formulae can be used to derive a formula for a rotation of R
3
, given the direction w
of its axis of rotation and given the angle Î¸ of rotation.
The following fact is the key to the proof that every isometry can be decomposed as a
product of reflections.
Proposition 13.2. Let E be any nontrivial Euclidean space. For any two vectors u, v âˆˆ E,
if k uk = k vk , then there is a hyperplane H such that the reflection s about H maps u to v,
and if u 6 = v, then this reflection is unique. See Figure 13.3.
Proof. If u = v, then any hyperplane containing u does the job. Otherwise, we must have
H = {v âˆ’ u}
âŠ¥, and by the above formula,
s(u) = u âˆ’ 2
(u Â· (v âˆ’ u))
k
(v âˆ’ u)k
2
(v âˆ’ u) = u +
2k uk
2 âˆ’ 2u Â· v
k
(v âˆ’ u)k
2
(v âˆ’ u),
and since
k
(v âˆ’ u)k
2 = k uk
2 + k vk
2 âˆ’ 2u Â· v
and k uk = k vk , we have
k
(v âˆ’ u)k
2 = 2k uk
2 âˆ’ 2u Â· v,
and thus, s(u) = v.

If E is a complex vector space and the inner product is Hermitian, Proposition 13.2
is false. The problem is that the vector v âˆ’u does not work unless the inner product
uÂ· v is real! The proposition can be salvaged enough to yield the QR-decomposition in terms
of Householder transformations; see Section 14.5.
We now show that hyperplane reflections can be used to obtain another proof of the
QR-decomposition.
498 CHAPTER 13. QR-DECOMPOSITION FOR ARBITRARY MATRICES
H
v-u
u s(u) = v
Figure 13.3: In R
3
, the (hyper)plane perpendicular to v âˆ’ u reflects u onto v.
13.2 QR-Decomposition Using Householder Matrices
First we state the result geometrically. When translated in terms of Householder matrices,
we obtain the fact advertised earlier that every matrix (not necessarily invertible) has a
QR-decomposition.
Proposition 13.3. Let E be a nontrivial Euclidean space of dimension n. For any orthonorï¿¾mal basis (e1, . . ., en) and for any n-tuple of vectors (v1, . . ., vn), there is a sequence of n
isometries h1, . . . , hn such that hi is a hyperplane reflection or the identity, and if (r1, . . . , rn)
are the vectors given by
rj = hn â—¦ Â· Â· Â· â—¦ h2 â—¦ h1(vj ),
then every rj is a linear combination of the vectors (e1, . . . , ej ), 1 â‰¤ j â‰¤ n. Equivalently, the
matrix R whose columns are the components of the rj over the basis (e1, . . . , en) is an upper
triangular matrix. Furthermore, the hi can be chosen so that the diagonal entries of R are
nonnegative.
Proof. We proceed by induction on n. For n = 1, we have v1 = Î»e1 for some Î» âˆˆ R. If
Î» â‰¥ 0, we let h1 = id, else if Î» < 0, we let h1 = âˆ’id, the reflection about the origin.
For n â‰¥ 2, we first have to find h1. Let
r1,1 = k v1k .
13.2. QR-DECOMPOSITION USING HOUSEHOLDER MATRICES 499
If v1 = r1,1e1, we let h1 = id. Otherwise, there is a unique hyperplane reflection h1 such that
h1(v1) = r1,1 e1,
defined such that
h1(u) = u âˆ’ 2
(u Â· w1)
k
w1k
2
w1
for all u âˆˆ E, where
w1 = r1,1 e1 âˆ’ v1.
The map h1 is the reflection about the hyperplane H1 orthogonal to the vector w1 = r1,1 e1 âˆ’
v1. See Figure 13.4. Letting
e2
v1 H1
r
1,1
e1
Figure 13.4: The construction of h1 in Proposition 13.3.
r1 = h1(v1) = r1,1 e1,
it is obvious that r1 belongs to the subspace spanned by e1, and r1,1 = k v1k is nonnegative.
Next assume that we have found k linear maps h1, . . . , hk, hyperplane reflections or the
identity, where 1 â‰¤ k â‰¤ n âˆ’ 1, such that if (r1, . . . , rk) are the vectors given by
rj = hk â—¦ Â· Â· Â· â—¦ h2 â—¦ h1(vj ),
then every rj
is a linear combination of the vectors (e1, . . . , ej ), 1 â‰¤ j â‰¤ k. See Figure
13.5. The vectors (e1, . . . , ek) form a basis for the subspace denoted by Uk
0
, the vectors
(ek+1, . . . , en) form a basis for the subspace denoted by Uk
00
, the subspaces Uk
0
and Uk
00
are
orthogonal, and E = Uk
0 âŠ• Uk
00
. Let
uk+1 = hk â—¦ Â· Â· Â· â—¦ h2 â—¦ h1(vk+1).
We can write
uk+1 = u
0k+1 + u
00k+1,
500 CHAPTER 13. QR-DECOMPOSITION FOR ARBITRARY MATRICES
e direction
e direction
e direction
1
2
3
v
v
1
2
e direction
e direction
e direction
1
2
3
v
1
h
1
r
1
Figure 13.5: The construction of r1 = h1(v1) in Proposition 13.3.
where u
0k+1 âˆˆ Uk
0
and u
00k+1 âˆˆ Uk
00
. See Figure 13.6. Let
rk+1,k+1 = k u
00k+1k .
If u
00k+1 = rk+1,k+1 ek+1, we let hk+1 = id. Otherwise, there is a unique hyperplane reflection
hk+1 such that
hk+1(u
00k+1) = rk+1,k+1 ek+1,
defined such that
hk+1(u) = u âˆ’ 2
(u Â· wk+1)
k
wk+1k 2
wk+1
for all u âˆˆ E, where
wk+1 = rk+1,k+1 ek+1 âˆ’ u
00k+1.
The map hk+1 is the reflection about the hyperplane Hk+1 orthogonal to the vector wk+1 =
rk+1,k+1 ek+1âˆ’u
00k+1. However, since u
00k+1, ek+1 âˆˆ Uk
00
and Uk
0
is orthogonal to Uk
00
, the subspace
Uk
0
is contained in Hk+1, and thus, the vectors (r1, . . . , rk) and u
0k+1, which belong to Uk
0
, are
invariant under hk+1. This proves that
hk+1(uk+1) = hk+1(u
0k+1) + hk+1(u
00k+1) = u
0k+1 + rk+1,k+1 ek+1
is a linear combination of (e1, . . . , ek+1). Letting
rk+1 = hk+1(uk+1) = u
0k+1 + rk+1,k+1 ek+1,
since uk+1 = hk â—¦ Â· Â· Â· â—¦ h2 â—¦ h1(vk+1), the vector
rk+1 = hk+1 â—¦ Â· Â· Â· â—¦ h2 â—¦ h1(vk+1)
is a linear combination of (e1, . . . , ek+1). See Figure 13.7. The coefficient of rk+1 over ek+1
is rk+1,k+1 = k u
00k+1k , which is nonnegative. This concludes the induction step, and thus the
proof.
13.2. QR-DECOMPOSITION USING HOUSEHOLDER MATRICES 501
e direction
e direction
e direction
2
3
v2
h1
(v2)
e direction
e direction
e direction
1
2
3
h1
(v2)
u
2
u2 â€˜ â€˜â€™
2
Figure 13.6: The construction of u2 = h1(v2) and its decomposition as u2 = u
02 + u
002
.
Remarks:
(1) Since every hi
is a hyperplane reflection or the identity,
Ï = hn â—¦ Â· Â· Â· â—¦ h2 â—¦ h1
is an isometry.
(2) If we allow negative diagonal entries in R, the last isometry hn may be omitted.
(3) Instead of picking rk,k = k u
00k
k
, which means that
wk = rk,k ek âˆ’ u
00k
,
where 1 â‰¤ k â‰¤ n, it might be preferable to pick rk,k = âˆ’ku
00k
k
if this makes k wkk
2
larger, in which case
wk = rk,k ek + u
00k
.
Indeed, since the definition of hk involves division by k wkk
2
, it is desirable to avoid
division by very small numbers.
(4) The method also applies to any m-tuple of vectors (v1, . . . , vm), with m â‰¤ n. Then
R is an upper triangular m Ã— m matrix and Q is an n Ã— m matrix with orthogonal
columns (Q> Q = Im). We leave the minor adjustments to the method as an exercise
to the reader
Proposition 13.3 directly yields the QR-decomposition in terms of Householder transforï¿¾mations (see Strang [169, 170], Golub and Van Loan [80], Trefethen and Bau [176], Kincaid
and Cheney [102], or Ciarlet [41]).
502 CHAPTER 13. QR-DECOMPOSITION FOR ARBITRARY MATRICES
e direction
e direction
e direction
e direction
1
2
3
u2
â€˜â€™
e direction
e direction
e direction
e direction
1
2
3
h1
(v2)
u2
â€˜
h2(u2
â€˜â€™)
h2 h1
(v2)
2
2
Figure 13.7: The construction of h2 and r2 = h2 â—¦ h1(v2) in Proposition 13.3.
Theorem 13.4. For every real n Ã— n matrix A, there is a sequence H1, . . ., Hn of matrices,
where each Hi is either a Householder matrix or the identity, and an upper triangular matrix
R such that
R = Hn Â· Â· Â· H2H1A.
As a corollary, there is a pair of matrices Q, R, where Q is orthogonal and R is upper
triangular, such that A = QR (a QR-decomposition of A). Furthermore, R can be chosen
so that its diagonal entries are nonnegative.
Proof. The jth column of A can be viewed as a vector vj over the canonical basis (e1, . . . , en)
of E
n
(where (ej )i = 1 if i = j, and 0 otherwise, 1 â‰¤ i, j â‰¤ n). Applying Proposition 13.3
to (v1, . . . , vn), there is a sequence of n isometries h1, . . . , hn such that hi
is a hyperplane
reflection or the identity, and if (r1, . . . , rn) are the vectors given by
rj = hn â—¦ Â· Â· Â· â—¦ h2 â—¦ h1(vj ),
