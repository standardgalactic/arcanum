of the argument, but the proof is basically unchanged.
28.1. THE CARTAN–DIEUDONNE THEOREM, HERMITIAN CASE ´ 989
Proposition 28.7. Let E be a nontrivial Hermitian space of dimension n. Given any
orthonormal basis (e1, . . . , en), for any n-tuple of vectors (v1, . . . , vn), there is a sequence of
n − 1 isometries h1, . . . , hn−1, such that hi is a hyperplane reflection or the identity, and if
(r1, . . . , rn) are the vectors given by
rj = hn−1 ◦ · · · ◦ h2 ◦ h1(vj ) 1 ≤ j ≤ n,
then every rj
is a linear combination of the vectors (e1, . . . , ej ), (1 ≤ j ≤ n). Equivalently,
the matrix R whose columns are the components of the rj over the basis (e1, . . . , en) is an
upper triangular matrix. Furthermore, if we allow one more isometry hn of the form
hn = ρen, ϕn ◦ · · · ◦ ρe1,ϕ1
after h1, . . . , hn−1, we can ensure that the diagonal entries of R are nonnegative.
Proof. The proof is very similar to the proof of Proposition 13.3, but it needs to be modified
a little bit since Proposition 28.1 is weaker than Proposition 13.2. We explain how to modify
the induction step, leaving the base case and the rest of the proof as an exercise.
As in the proof of Proposition 13.3, the vectors (e1, . . . , ek) form a basis for the subspace
denoted as Uk
0
, the vectors (ek+1, . . . , en) form a basis for the subspace denoted as Uk
00
, the
subspaces Uk
0
and Uk
00
are orthogonal, and E = Uk
0 ⊕ Uk
00
. Let
uk+1 = hk ◦ · · · ◦ h2 ◦ h1(vk+1).
We can write
uk+1 = u
0k+1 + u
00k+1,
where u
0k+1 ∈ Uk
0
and u
00k+1 ∈ Uk
00
. Let
rk+1,k+1 =
  u
00k+1
 , and e
iθk+1 |u
00k+1 · ek+1| = u
00k+1 · ek+1.
If u
00k+1 = e
iθk+1 rk+1,k+1 ek+1, we let hk+1 = id. Otherwise, by Proposition 28.1, there is a
unique hyperplane reflection hk+1 such that
hk+1(u
00k+1) = e
iθk+1 rk+1,k+1 ek+1,
where hk+1 is the reflection about the hyperplane Hk+1 orthogonal to the vector
wk+1 = rk+1,k+1 ek+1 − e
−iθk+1 u
00k+1.
At the end of the induction, we have a triangular matrix R, but the diagonal entries
e
iθj rj, j of R may be complex. Letting
hn+1 = ρen, −θn ◦ · · · ◦ ρe1,−θ1
,
we observe that the diagonal entries of the matrix of vectors
r
0j = hn+1 ◦ hn ◦ · · · ◦ h2 ◦ h1(vj )
is triangular with nonnegative entries.
990 CHAPTER 28. ISOMETRIES OF HERMITIAN SPACES
Remark: For numerical stability, it is preferable to use wk+1 = rk+1,k+1 ek+1 + e
−iθk+1 u
00k+1
instead of wk+1 = rk+1,k+1 ek+1 − e
−iθk+1 u
00k+1. The effect of that choice is that the diagonal
entries in R will be of the form −e
iθj rj, j = e
i(θj+π)
rj, j . Of course, we can make these entries
nonegative by applying
hn+1 = ρen, π−θn ◦ · · · ◦ ρe1,π−θ1
after hn.
As in the Euclidean case, Proposition 28.7 immediately implies the QR-decomposition
for arbitrary complex n×n-matrices, where Q is now unitary (see Kincaid and Cheney [102],
Golub and Van Loan [80], Trefethen and Bau [176], or Ciarlet [41]).
Proposition 28.8. For every complex n × n-matrix A, there is a sequence H1, . . . , Hn−1
of matrices, where each Hi is either a Householder matrix or the identity, and an upper
triangular matrix R, such that
R = Hn−1 · · · H2H1A.
As a corollary, there is a pair of matrices Q, R, where Q is unitary and R is upper triangular,
such that A = QR (a QR-decomposition of A). Furthermore, R can be chosen so that its
diagonal entries are nonnegative. This can be achieved by a diagonal matrix D with entries
such that |dii| = 1 for i = 1, . . . , n, and we have A = QeRe with
Qe = H1 · · · Hn−1D, Re = D
∗R,
where Re is upper triangular and has nonnegative diagonal entries
Proof. It is essentially identical to the proof of Proposition 13.4, and we leave the details as
an exercise. For the last statement, observe that hn ◦ · · · ◦ h1 is also an isometry.
As in the Euclidean case, the QR-decomposition has applications to least squares prob￾lems. It is also possible to convert any complex matrix to bidiagonal form.
28.2 Affine Isometries (Rigid Motions)
In this section, we study very briefly the affine isometries of a Hermitian space. Most results
holding for Euclidean affine spaces generalize without any problems to Hermitian spaces.
The characterization of the set of fixed points of an affine map is unchanged. Similarly,
every affine isometry f (of a Hermitian space) can be written uniquely as
f = t ◦ g, with t ◦ g = g ◦ t,
where g is an isometry having a fixed point, and t is a translation by a vector τ such that
−→f (τ ) = τ , and with some additional nice properties (see Proposition 28.13). A generalization
28.2. AFFINE ISOMETRIES (RIGID MOTIONS) 991
of the Cartan–Dieudonn´e theorem can easily be shown: every affine isometry in Is(n, C) can
be written as the composition of at most 2n − 1 isometries if it has a fixed point, or else as
the composition of at most 2n+1 isometries, where all these isometries are affine hyperplane
reflections except for possibly one affine Hermitian reflection. We also prove that every rigid
motion in SE(n, C) is the composition of at most 2n − 2 flips (for n ≥ 3).
Definition 28.2. Given any two nontrivial Hermitian affine spaces E and F of the same
finite dimension n, a function f : E → F is an affine isometry (or rigid map) iff it is an
affine map and



−−−−−→
f(a)f(b)

 =

 
−→ab

 ,
for all a, b ∈ E. When E = F, an affine isometry f : E → E is also called a rigid motion.
Thus, an affine isometry is an affine map that preserves the distance. This is a rather
strong requirement, but unlike the Euclidean case, not strong enough to force f to be an
affine map.
The following simple Proposition is left as an exercise.
Proposition 28.9. Given any two nontrivial Hermitian affine spaces E and F of the same
finite dimension n, an affine map f : E → F is an affine isometry iff its associated linear
map
−→f :
−→E →
−→F is an isometry. An affine isometry is a bijection.
As in the Euclidean case, given an affine isometry f : E → E, if −→f is a rotation, we call
f a proper (or direct) affine isometry, and if −→f is a an improper linear isometry, we call f
a an improper (or skew) affine isometry. It is easily shown that the set of affine isometries
f : E → E forms a group, and those for which −→f is a rotation is a subgroup. The group
of affine isometries, or rigid motions, is a subgroup of the affine group GA(E, C) denoted
as Is(E, C) (or Is(n, C) when E = C
n
). The subgroup of Is(E, C) consisting of the direct
rigid motions is also a subgroup of SA(E, C), and it is denoted as SE(E, C) (or SE(n, C),
when E = C
n
). The translations are the affine isometries f for which −→f = id, the identity
map on
−→E . The following Proposition is the counterpart of Proposition 14.14 for isometries
between Hermitian vector spaces.
Proposition 28.10. Given any two nontrivial Hermitian affine spaces E and F of the same
finite dimension n, for every function f : E → F, the following properties are equivalent:
(1) f is an affine map and
  
−−−−−→
f(a)f(b)

 =

 
−→ab

 , for all a, b ∈ E.
(2)
  
−−−−−→
f(a)f(b)

 =

 
−→ab

 , and there is some Ω ∈ E such that
f(Ω + i
−→ab) = f(Ω) + i(
−−−−−−−−−−→
f(Ω)f(Ω + −→ab)),
for all a, b ∈ E.
992 CHAPTER 28. ISOMETRIES OF HERMITIAN SPACES
Proof. Obviously, (1) implies (2). The proof that that (2) implies (1) is similar to the proof
of Proposition 27.7, but uses Proposition 14.14 instead of Proposition 12.12. The details are
left as an exercise.
Inspection of the proof shows immediately that Proposition 27.8 holds for Hermitian
spaces. For the sake of completeness, we restate the Proposition in the complex case.
Proposition 28.11. Let E be any complex affine space of finite dimension For every affine
map f : E → E, let F ix(f) = {a ∈ E | f(a) = a} be the set of fixed points of f. The
following properties hold:
(1) If f has some fixed point a, so that F ix(f) 6 = ∅, then F ix(f) is an affine subspace of
E such that
F ix(f) = a + E(1,
−→f ) = a + Ker (−→f − id),
where E(1,
−→f ) is the eigenspace of the linear map −→f for the eigenvalue 1.
(2) The affine map f has a unique fixed point iff E(1,
−→f ) = Ker (−→f − id) = {0}.
Affine orthogonal symmetries are defined just as in the Euclidean case, and Proposition
27.9 also applies to complex affine spaces.
Proposition 28.12. Given any affine complex space E, if f : E → E and g : E → E
are affine orthogonal symmetries about parallel affine subspaces F1 and F2, then g ◦ f is a
translation defined by the vector 2
−→ab, where
−→ab is any vector perpendicular to the common
direction −→F of F1 and F2 such that
 

−→ab

 is the distance between F1 and F2, with a ∈ F1
and b ∈ F2. Conversely, every translation by a vector τ is obtained as the composition of
two affine orthogonal symmetries about parallel affine subspaces F1 and F2 whose common
direction is orthogonal to τ =
−→ab, for some a ∈ F1 and some b ∈ F2 such that the distance
betwen F1 and F2 is
 

−→ab

 /2.
It is easy to check that the proof of Proposition 27.10 also holds in the Hermitian case.
Proposition 28.13. Let E be a Hermitian affine space of finite dimension n. For every
affine isometry f : E → E, there is a unique affine isometry g : E → E and a unique
translation t = tτ , with
−→f (τ ) = τ (i.e., τ ∈ Ker (−→f − id)), such that the set F ix(g) = {a ∈
E, | g(a) = a} of fixed points of g is a nonempty affine subspace of E of direction
−→G = Ker (−→f − id) = E(1,
−→f ),
and such that
f = t ◦ g and t ◦ g = g ◦ t.
Furthermore, we have the following additional properties:
28.2. AFFINE ISOMETRIES (RIGID MOTIONS) 993
(a) f = g and τ = 0 iff f has some fixed point, i.e., iff F ix(f) 6 = ∅.
(b) If f has no fixed points, i.e., F ix(f) = ∅, then dim(Ker (−→f − id)) ≥ 1.
The remarks made in the Euclidean case also apply to the Hermitian case. In particular,
the fact that E has finite dimension is only used to prove (b).
A version of the Cartan–Dieudonn´e also holds for affine isometries, but it may not be
possible to get rid of Hermitian reflections entirely.
Theorem 28.14. Let E be an affine Hermitian space of dimension n ≥ 1. Every affine
isometry in Is(n, C) can be written as the composition of at most 2n − 1 affine isometries
if it has a fixed point, or else as the composition of at most 2n + 1 affine isometries, where
all these isometries are affine hyperplane reflections except for possibly one affine Hermitian
reflection. When n ≥ 2, the identity is the composition of any reflection with itself.
Proof. The proof is very similar to the proof of Theorem 27.11, except that it uses Theorem
28.5 instead of Theorem 27.1. The details are left as an exercise.
When n ≥ 3, as in the Euclidean case, we can characterize the affine isometries in
SE(n, C) in terms of flips, and we can even bound the number of flips by 2n − 2.
Theorem 28.15. Let E be a Hermitian affine space of dimension n ≥ 3. Every rigid motion
f ∈ SE(E, C) is the composition of an even number of affine flips f = f2k ◦ · · · ◦ f1, where
k ≤ n − 1.
Proof. It is very similar to the proof of theorem 27.12, but it uses Proposition 28.6 instead
of Proposition 27.5. The details are left as an exercise.
A more detailed study of the rigid motions of Hermitian spaces of dimension 2 and 3
would seem worthwhile, but we are not aware of any reference on this subject.
994 CHAPTER 28. ISOMETRIES OF HERMITIAN SPACES
Chapter 29
The Geometry of Bilinear Forms;
Witt’s Theorem; The
Cartan–Dieudonn´e Theorem
29.1 Bilinear Forms
In this chapter, we study the structure of a K-vector space E endowed with a nondegenerate
bilinear form ϕ: E × E → K (for any field K), which can be viewed as a kind of generalized
inner product. Unlike the case of an inner product, there may be nonzero vectors u ∈ E such
that ϕ(u, u) = 0 so the map u 7→ ϕ(u, u) can no longer be interpreted as a notion of square
length (also, ϕ(u, u) may not be real and positive!). However, the notion of orthogonality
survives: we say that u, v ∈ E are orthogonal iff ϕ(u, v) = 0. Under some additional
conditions on ϕ, it is then possible to split E into orthogonal subspaces having some special
properties. It turns out that the special cases where ϕ is symmetric (or Hermitian) or skew￾symmetric (or skew-Hermitian) can be handled uniformly using a deep theorem due to Witt
(the Witt decomposition theorem (1936)).
We begin with the very general situation of a bilinear form ϕ: E×F → K, where K is an
arbitrary field, possibly of characteristric 2. Actually, even though at first glance this may
appear to be an unnecessary abstraction, it turns out that this situation arises in attempting
to prove properties of a bilinear map ϕ: E ×E → K, because it may be necessary to restrict
ϕ to different subspaces U and V of E. This general approach was pioneered by Chevalley
[37], E. Artin [6], and Bourbaki [24]. The third source was a major source of inspiration, and
many proofs are taken from it. Other useful references include Snapper and Troyer [162],
Berger [12], Jacobson [98], Grove [83], Taylor [174], and Berndt [14].
Definition 29.1. Given two vector spaces E and F over a field K, a map ϕ: E × F → K
is a bilinear form iff the following conditions hold: For all u, u1, u2 ∈ E, all v, v1, v2 ∈ F, for
995
996 CHAPTER 29. THE GEOMETRY OF BILINEAR FORMS; WITT’S THEOREM
all λ, µ ∈ K, we have
ϕ(u1 + u2, v) = ϕ(u1, v) + ϕ(u2, v)
ϕ(u, v1 + v2) = ϕ(u, v1) + ϕ(u, v2)
ϕ(λu, v) = λϕ(u, v)
ϕ(u, µv) = µϕ(u, v).
A bilinear form as in Definition 29.1 is sometimes called a pairing. The first two conditions
imply that ϕ(0, v) = ϕ(u, 0) = 0 for all u ∈ E and all v ∈ F.
If E = F, observe that
ϕ(λu + µv, λu + µv) = λϕ(u, λu + µv) + µϕ(v, λu + µv)
= λ
2ϕ(u, u) + λµϕ(u, v) + λµϕ(v, u) + µ
2ϕ(v, v).
If we let λ = µ = 1, we get
ϕ(u + v, u + v) = ϕ(u, u) + ϕ(u, v) + ϕ(v, u) + ϕ(v, v).
If ϕ is symmetric, which means that
ϕ(u, v) = ϕ(v, u) for all u, v ∈ E,
then
2ϕ(u, v) = ϕ(u + v, u + v) − ϕ(u, u) − ϕ(v, v). (∗)
The function Φ defined such that
Φ(u) = ϕ(u, u) u ∈ E,
is called the quadratic form associated with ϕ. If the field K is not of characteristic 2, then
ϕ is completely determined by its quadratic form Φ. The symmetric bilinear form ϕ is called
the polar form of Φ. This suggests the following definition.
Definition 29.2. A function Φ: E → K is a quadratic form on E if the following conditions
hold:
(1) We have Φ(λu) = λ
2Φ(u), for all u ∈ E and all λ ∈ E.
(2) The map ϕ
0 given by ϕ
0 (u, v) = Φ(u+v)−Φ(u)−Φ(v) is bilinear. Obviously, the map
ϕ
0 is symmetric.
Since Φ(x + x) = Φ(2x) = 4Φ(x), we have
ϕ
0 (u, u) = 2Φ(u) u ∈ E.
29.1. BILINEAR FORMS 997
If the field K is not of characteristic 2, then ϕ = 2
1ϕ
0 is the unique symmetric bilinear form
such that that ϕ(u, u) = Φ(u) for all u ∈ E. The bilinear form ϕ =
1
2
ϕ
0 is called the polar
form of Φ. In this case, there is a bijection between the set of bilinear forms on E and the
set of quadratic forms on E.
If K is a field of characteristic 2, then ϕ
0 is alternating, which means that
ϕ
0 (u, u) = 0 for all u ∈ E.
Thus if K is a field of characteristic 2, then Φ cannot be recovered from the symmetric
bilinear form ϕ
0 .
If (e1, . . . , en) is a basis of E, it is easy to show that
Φ

nX
i=1
λiei
 =
nX
i=1
λ
2
i Φ(ei) +X
i6=j
λiλjϕ
0 (ei
, ej ).
This shows that the quadratic form Φ is completely determined by the scalars Φ(ei) and
ϕ
0 (ei
, ej ) (i 6 = j). Furthermore, given any bilinear form ψ: E × E → K (not necessarily
symmetric) we can define a quadratic form Φ by setting Φ(x) = ψ(x, x), and we immediately
check that the symmetric bilinear form ϕ
0 associated with Φ is given by ϕ
0 (u, v) = ψ(u, v) +
ψ(v, u). Using the above facts, it is not hard to prove that given any quadratic form Φ,
there is some (nonsymmetric) bilinear form ψ such that Φ(u) = ψ(u, u) for all u ∈ E (see
Bourbaki [24], Section §3.4, Proposition 2). Thus, quadratic forms are more general than
symmetric bilinear forms (except in characteristic 6 = 2).
Definition 29.3. Given any bilinear form ϕ: E × E → K where K is a field of any charac￾teristic, we say that ϕ is alternating if
ϕ(u, u) = 0 for all u ∈ E,
and skew-symmetric if
ϕ(v, u) = −ϕ(u, v) for all u, v ∈ E.
If K is a field of any characteristic, the identity
ϕ(u + v, u + v) = ϕ(u, u) + ϕ(u, v) + ϕ(v, u) + ϕ(v, v)
shows that if ϕ is alternating, then
ϕ(v, u) = −ϕ(u, v) for all u, v ∈ E,
that is, ϕ is skew-symmetric. Conversely, if the field K is not of characteristic 2, then a
skew-symmetric bilinear map is alternating, since ϕ(u, u) = −ϕ(u, u) implies ϕ(u, u) = 0.
An important consequence of bilinearity is that a pairing yields a linear map from E into
F
∗ and a linear map from F into E
∗
(where E
∗ = HomK(E, K), the dual of E, is the set of
linear maps from E to K, called linear forms).
998 CHAPTER 29. THE GEOMETRY OF BILINEAR FORMS; WITT’S THEOREM
Definition 29.4. Given a bilinear map ϕ: E × F → K, for every u ∈ E, let lϕ(u) be the
linear form in F
∗ given by
lϕ(u)(y) = ϕ(u, y) for all y ∈ F ,
and for every v ∈ F, let rϕ(v) be the linear form in E
∗ given by
rϕ(v)(x) = ϕ(x, v) for all x ∈ E.
Because ϕ is bilinear, the maps lϕ : E → F
∗ and rϕ : F → E
∗ are linear.
Definition 29.5. A bilinear map ϕ: E×F → K is said to be nondegenerate iff the following
conditions hold:
(1) For every u ∈ E, if ϕ(u, v) = 0 for all v ∈ F, then u = 0, and
(2) For every v ∈ F, if ϕ(u, v) = 0 for all u ∈ E, then v = 0.
The following proposition shows the importance of lϕ and rϕ.
Proposition 29.1. Given a bilinear map ϕ: E × F → K, the following properties hold:
(a) The map lϕ is injective iff Property (1) of Definition 29.5 holds.
(b) The map rϕ is injective iff Property (2) of Definition 29.5 holds.
(c) The bilinear form ϕ is nondegenerate and iff lϕ and rϕ are injective.
(d) If the bilinear form ϕ is nondegenerate and if E and F have finite dimensions, then
dim(E) = dim(F), and lϕ : E → F
∗ and rϕ : F → E
∗ are linear isomorphisms.
Proof. (a) Assume that (1) of Definition 29.5 holds. If lϕ(u) = 0, then lϕ(u) is the linear
form whose value is 0 for all y; that is,
lϕ(u)(y) = ϕ(u, y) = 0 for all y ∈ F ,
and by (1) of Definition 29.5, we must have u = 0. Therefore, lϕ is injective. Conversely, if
lϕ is injective, and if
lϕ(u)(y) = ϕ(u, y) = 0 for all y ∈ F ,
then lϕ(u) is the zero form, and by injectivity of lϕ, we get u = 0; that is, (1) of Definition
29.5 holds.
(b) The proof is obtained by swapping the arguments of ϕ.
(c) This follows from (a) and (b).
(d) If E and F are finite dimensional, then dim(E) = dim(E
∗
) and dim(F) = dim(F
∗
).
Since ϕ is nondegenerate, lϕ : E → F
∗ and rϕ : F → E
∗ are injective, so dim(E) ≤ dim(F
∗
) =
dim(F) and dim(F) ≤ dim(E
∗
) = dim(E), which implies that
dim(E) = dim(F),
and thus, lϕ : E → F
∗ and rϕ : F → E
∗ are bijective.
29.1. BILINEAR FORMS 999
As a corollary of Proposition 29.1, we have the following characterization of a nondegen￾erate bilinear map. The proof is left as an exercise.
Proposition 29.2. Given a bilinear map ϕ: E × F → K, if E and F have the same finite
dimension, then the following properties are equivalent:
(1) The map lϕ is injective.
(2) The map lϕ is surjective.
(3) The map rϕ is injective.
(4) The map rϕ is surjective.
(5) The bilinear form ϕ is nondegenerate.
Observe that in terms of the canonical pairing between E
∗ and E given by
h
f, ui = f(u), f ∈ E
∗
, u ∈ E,
(and the canonical pairing between F
∗ and F), we have
ϕ(u, v) = h lϕ(u), vi = h rϕ(v), ui u ∈ E, v ∈ F.
Proposition 29.3. Given a bilinear map ϕ: E × F → K, if ϕ is nondegenerate and E and
F are finite-dimensional, then dim(E) = dim(F) = n, and for every basis (e1, . . . , en) of E,
there is a basis (f1, . . . , fn) of F such that ϕ(ei
, fj ) = δij , for all i, j = 1, . . . , n.
Proof. Since ϕ is nondegenerate, by Proposition 29.1 we have dim(E) = dim(F) = n, and
by Proposition 29.2, the linear map rϕ is bijective. Then, if (e
∗
1
, . . . , e∗
n
) is the dual basis (in
E
∗
) of the basis (e1, . . . , en), the vectors (f1, . . . , fn) given by fi = rϕ
−1
(e
∗
i
) form a basis of F,
and we have
ϕ(ei
, fj ) = h rϕ(fj ), eii = h e
∗
i
, ej i = δij ,
as claimed.
If E = F and ϕ is symmetric, then we have the following interesting result.
Theorem 29.4. Given any bilinear form ϕ: E×E → K with dim(E) = n, if ϕ is symmetric
(possibly degenerate) and K does not have characteristic 2, then there is a basis (e1, . . . , en)
of E such that ϕ(ei
, ej ) = 0, for all i 6 = j.
Proof. We proceed by induction on n ≥ 0, following a proof due to Chevalley. The base
case n = 0 is trivial. For the induction step, assume that n ≥ 1 and that the induction
hypothesis holds for all vector spaces of dimension n−1. If ϕ(u, v) = 0 for all u, v ∈ E, then
the statement holds trivially. Otherwise, since K does not have characteristic 2, equation
2ϕ(u, v) = ϕ(u + v, u + v) − ϕ(u, u) − ϕ(v, v) (∗)
1000 CHAPTER 29. THE GEOMETRY OF BILINEAR FORMS; WITT’S THEOREM
show that there is some nonzero vector e1 ∈ E such that ϕ(e1, e1) 6 = 0 since otherwise ϕ
would vanish for all u, v ∈ E. We claim that the set
H = {v ∈ E | ϕ(e1, v) = 0}
has dimension n − 1, and that e1 ∈/ H.
This is because
H = Ker (lϕ(e1)),
where lϕ(e1) is the linear form in E
∗ determined by e1. Since ϕ(e1, e1) 6 = 0, we have e1 ∈/ H,
the linear form lϕ(e1) is not the zero form, and thus its kernel is a hyperplane H (a subspace
of dimension n − 1). Since dim(H) = n − 1 and e1 ∈/ H, we have the direct sum
E = H ⊕ Ke1.
By the induction hypothesis applied to H, we get a basis (e2, . . . , en) of vectors in H such
that ϕ(ei
, ej ) = 0, for all i 6 = j with 2 ≤ i, j ≤ n. Since ϕ(e1, v) = 0 for all v ∈ H and since
ϕ is symmetric, we also have ϕ(v, e1) = 0 for all v ∈ H, so we obtain a basis (e1, . . . , en) of
E such that ϕ(ei
, ej ) = 0, for all i 6 = j.
If E and F are finite-dimensional vector spaces and if (e1, . . . , em) is a basis of E and
(f1, . . . , fn) is a basis of F then the bilinearity of ϕ yields
ϕ

mX
i=1
xiei
,
nX
j=1
yjfj
 =
mX
i=1
nX
j=1
xiϕ(ei
, fj )yj
.
This shows that ϕ is completely determined by the n × m matrix M = (mij ) with mij =
ϕ(ej
, fi), and in matrix form, we have
ϕ(x, y) = x
> M> y = y
> Mx,
where x and y are the column vectors associated with (x1, . . . , xm) ∈ Km and (y1, . . . , yn) ∈
Kn
. As in Section 12.1, we are committing the slight abuse of notation of letting x denote
both the vector x =
P
n
i=1 xiei and the column vector associated with (x1, . . . , xn) (and
similarly for y).
Definition 29.6. If (e1, . . . , em) is a basis of E and (f1, . . . , fn) is a basis of F, for any
bilinear form ϕ: E × F → K, the n × m matrix M = (mij ) given by mij = ϕ(ej
, fi) for
i = 1, . . . , n and j = 1, . . . , m is called the matrix of ϕ with respect to the bases (e1, . . . , em)
and (f1, . . . , fn).
The following fact is easily proved.
Proposition 29.5. If m = dim(E) = dim(F) = n, then ϕ is nondegenerate iff the matrix
M is invertible iff det(M) 6 = 0.
29.1. BILINEAR FORMS 1001
As we will see later, most bilinear forms that we will encounter are equivalent to one
whose matrix is of the following form:
1. In, −In.
2. If p + q = n, with p, q ≥ 1,
Ip,q =

Ip 0
0 −Iq

3. If n = 2m,
Jm.m =

0 Im
−Im 0

4. If n = 2m,
Am,m = Im.mJm.m =

0 Im
Im 0

.
If we make changes of bases given by matrices P and Q, so that x = P x0 and y = Qy0 ,
then the new matrix expressing ϕ is P
> MQ. In particular, if E = F and the same basis
is used, then the new matrix is P
> MP. This shows that if ϕ is nondegenerate, then the
determinant of ϕ is determined up to a square element.
Observe that if ϕ is a symmetric bilinear form (E = F) and if K does not have charac￾teristic 2, then by Theorem 29.4, there is a basis of E with respect to which the matrix M
representing ϕ is a diagonal matrix. If K = R or K = C, this allows us to classify completely
the symmetric bilinear forms. Recall that Φ(u) = ϕ(u, u) for all u ∈ E.
Proposition 29.6. Given any bilinear form ϕ: E × E → K with dim(E) = n, if ϕ is
symmetric and K does not have characteristic 2, then there is a basis (e1, . . . , en) of E such
that
Φ

nX
i=1
xiei
 =
rX
i=1
λix
2
i
,
for some λi ∈ K − {0} and with r ≤ n. Furthermore, if K = C, then there is a basis
(e1, . . . , en) of E such that
Φ

nX
i=1
xiei
 =
rX
i=1
x
2
i
,
and if K = R, then there is a basis (e1, . . . , en) of E such that
Φ

nX
i=1
xiei
 =
p
X
i=1
x
2
i −
i=
X
p+
p+1
q
x
2
i
,
with 0 ≤ p, q and p + q ≤ n.
1002 CHAPTER 29. THE GEOMETRY OF BILINEAR FORMS; WITT’S THEOREM
Proof. The first statement is a direct consequence of Theorem 29.4. If K = C, then every
λi has a square root µi
, and if replace ei by ei/µi
, we obtained the desired form.
If K = R, then there are two cases:
1. If λi > 0, let µi be a positive square root of λi and replace ei by ei/µi
.
2. If λi < 0, et µi be a positive square root of −λi and replace ei by ei/µi
.
In the nondegenerate case, the matrices corresponding to the complex and the real case
are, In, −In, and Ip,q. Observe that the second statement of Proposition 29.6 holds in any
field in which every element has a square root. In the case K = R, we can show that(p, q)
only depends on ϕ.
Definition 29.7. Let ϕ: E ×E → R be any symmetric real bilinear form. For any subspace
U of E, we say that ϕ is positive definite on U iff ϕ(u, u) > 0 for all nonzero u ∈ U, and we
say that ϕ is negative definite on U iff ϕ(u, u) < 0 for all nonzero u ∈ U. Then, let
r = max{dim(U) | U ⊆ E, ϕ is positive definite on U}
and let
s = max{dim(U) | U ⊆ E, ϕ is negative definite on U}
Proposition 29.7. (Sylvester’s inertia law ) Given any symmetric bilinear form ϕ: E×E →
R with dim(E) = n, for any basis (e1, . . . , en) of E such that
Φ

nX
i=1
xiei
 =
p
X
i=1
x
2
i −
i=
X
p+
p+1
q
x
2
i
,
with 0 ≤ p, q and p + q ≤ n, the integers p, q depend only on ϕ; in fact, p = r and q = s,
with r and s as defined above.
Proof. If we let U be the subspace spanned by (e1, . . . , ep), then ϕ is positive definite on
U, so r ≥ p. Similarly, if we let V be the subspace spanned by (ep+1, . . . , ep+q), then ϕ is
negative definite on V , so s ≥ q.
Next, if W1 is any subspace of maximum dimension such that ϕ is positive definite on
W1, and if we let V
0 be the subspace spanned by (ep+1, . . . , en), then ϕ(u, u) ≤ 0 on V
0 , so
W1 ∩ V
0 = (0), which implies that dim(W1) + dim(V
0 ) ≤ n, and thus, r + n − p ≤ n; that
is, r ≤ p. Similarly, if W2 is any subspace of maximum dimension such that ϕ is negative
definite on W2, and if we let U
0 be the subspace spanned by (e1, . . . , ep, ep+q+1, . . . , en), then
ϕ(u, u) ≥ 0 on U
0 , so W2 ∩ U
0 = (0), which implies that s + n − q ≤ n; that is, s ≤ q.
Therefore, p = r and q = s, as claimed
These last two results can be generalized to ordered fields. For example, see Snapper and
Troyer [162], Artin [6], and Bourbaki [24].
29.2. SESQUILINEAR FORMS 1003
29.2 Sesquilinear Forms
In order to accomodate Hermitian forms, we assume that some involutive automorphism,
λ 7→ λ, of the field K is given. This automorphism of K satisfies the following properties:
(λ + µ) = λ + µ
(λµ) = λ µ
λ = λ.
Since any field automorphism maps the multiplicative unit 1 to itself, we have 1 = 1.
If the automorphism λ 7→ λ is the identity, then we are in the standard situation of a
bilinear form. When K = C (the complex numbers), then we usually pick the automorphism
of C to be conjugation; namely, the map
a + ib 7→ a − ib.
Definition 29.8. Given two vector spaces E and F over a field K with an involutive au￾tomorphism λ 7→ λ, a map ϕ: E × F → K is a (right) sesquilinear form iff the following
conditions hold: For all u, u1, u2 ∈ E, all v, v1, v2 ∈ F, for all λ, µ ∈ K, we have
ϕ(u1 + u2, v) = ϕ(u1, v) + ϕ(u2, v)
ϕ(u, v1 + v2) = ϕ(u, v1) + ϕ(u, v2)
ϕ(λu, v) = λϕ(u, v)
ϕ(u, µv) = µϕ(u, v).
Again, ϕ(0, v) = ϕ(u, 0) = 0. If E = F, then we have
ϕ(λu + µv, λu + µv) = λϕ(u, λu + µv) + µϕ(v, λu + µv)
= λλϕ(u, u) + λµϕ(u, v) + λµϕ(v, u) + µµϕ(v, v).
If we let λ = µ = 1 and then λ = 1, µ = −1, we get
ϕ(u + v, u + v) = ϕ(u, u) + ϕ(u, v) + ϕ(v, u) + ϕ(v, v)
ϕ(u − v, u − v) = ϕ(u, u) − ϕ(u, v) − ϕ(v, u) + ϕ(v, v),
so by subtraction, we get
2(ϕ(u, v) + ϕ(v, u)) = ϕ(u + v, u + v) − ϕ(u − v, u − v) for u, v ∈ E.
If we replace v by λv (with λ 6 = 0), we get
2(λϕ(u, v) + λϕ(v, u)) = ϕ(u + λv, u + λv) − ϕ(u − λv, u − λv),
1004 CHAPTER 29. THE GEOMETRY OF BILINEAR FORMS; WITT’S THEOREM
and by combining the above two equations, we get
2(λ − λ)ϕ(u, v) = λϕ(u + v, u + v) − λϕ(u − v, u − v)
− ϕ(u + λv, u + λv) + ϕ(u − λv, u − λv). (∗)
If the automorphism λ 7→ λ is not the identity, then there is some λ ∈ K such that λ−λ 6 = 0,
and if K is not of characteristic 2, then we see that the sesquilinear form ϕ is completely
determined by its restriction to the diagonal (that is, the set of values {ϕ(u, u) | u ∈ E}).
In the special case where K = C, we can pick λ = i, and we get
4ϕ(u, v) = ϕ(u + v, u + v) − ϕ(u − v, u − v) + iϕ(u + λv, u + λv) − iϕ(u − λv, u − λv).
Remark: If the automorphism λ 7→ λ is the identity, then in general ϕ is not determined
by its value on the diagonal, unless ϕ is symmetric.
In the sesquilinear setting, it turns out that the following two cases are of interest:
1. We have
ϕ(v, u) = ϕ(u, v), for all u, v ∈ E,
in which case we say that ϕ is Hermitian. In the special case where K = C and the
involutive automorphism is conjugation, we see that ϕ(u, u) ∈ R, for u ∈ E.
2. We have
ϕ(v, u) = −ϕ(u, v), for all u, v ∈ E,
in which case we say that ϕ is skew-Hermitian.
We observed that in characteristic different from 2, a sesquilinear form is determined
by its restriction to the diagonal. For Hermitian and skew-Hermitian forms, we have the
following kind of converse.
Proposition 29.8. If ϕ is a nonzero Hermitian or skew-Hermitian form and if ϕ(u, u) = 0
for all u ∈ E, then K is of characteristic 2 and the automorphism λ 7→ λ is the identity.
Proof. We give the proof in the Hermitian case, the skew-Hermitian case being left as an
exercise. Assume that ϕ is alternating. From the identity
ϕ(u + v, u + v) = ϕ(u, u) + ϕ(u, v) + ϕ(u, v) + ϕ(v, v),
we get
ϕ(u, v) = −ϕ(u, v) for all u, v ∈ E.
Since ϕ is not the zero form, there exist some nonzero vectors u, v ∈ E such that ϕ(u, v) = 1.
For any λ ∈ K, we have
λϕ(u, v) = ϕ(λu, v) = −ϕ(λu, v) = −λ ϕ(u, v),
29.2. SESQUILINEAR FORMS 1005
and since ϕ(u, v) = 1, we get
λ = −λ for all λ ∈ K.
For λ = 1, we get 1 = −1, which means that K has characterictic 2. But then
λ = −λ = λ for all λ ∈ K,
so the automorphism λ 7→ λ is the identity.
The definition of the linear maps lϕ and rϕ requires a small twist due to the automorphism
λ 7→ λ.
Definition 29.9. Given a vector space E over a field K with an involutive automorphism
λ 7→ λ, we define the K-vector space E as E with its abelian group structure, but with
scalar multiplication given by
(λ, u) 7→ λu.
Given two K-vector spaces E and F, a semilinear map f : E → F is a function, such that
for all u, v ∈ E, for all λ ∈ K, we have
f(u + v) = f(u) + f(v)
f(λu) = λf(u).
Because λ = λ, observe that a function f : E → F is semilinear iff it is a linear map
f : E → F. The K-vector spaces E and E are isomorphic, since any basis (ei)i∈I of E is also
a basis of E.
The maps lϕ and rϕ are defined as follows:
For every u ∈ E, let lϕ(u) be the linear form in F
∗ defined so that
lϕ(u)(y) = ϕ(u, y) for all y ∈ F ,
and for every v ∈ F, let rϕ(v) be the linear form in E
∗ defined so that
rϕ(v)(x) = ϕ(x, v) for all x ∈ E.
The reader should check that because we used ϕ(u, y) in the definition of lϕ(u)(y), the
function lϕ(u) is indeed a linear form in F
∗
. It is also easy to check that lϕ is a linear
map lϕ : E → F
∗
, and that rϕ is a linear map rϕ : F → E
∗
(equivalently, lϕ : E → F
∗ and
rϕ : F → E
∗ are semilinear).
The notion of a nondegenerate sesquilinear form is identical to the notion for bilinear
forms. For the convenience of the reader, we repeat the definition.
Definition 29.10. A sesquilinear map ϕ: E × F → K is said to be nondegenerate iff the
following conditions hold:
1006 CHAPTER 29. THE GEOMETRY OF BILINEAR FORMS; WITT’S THEOREM
(1) For every u ∈ E, if ϕ(u, v) = 0 for all v ∈ F, then u = 0, and
(2) For every v ∈ F, if ϕ(u, v) = 0 for all u ∈ E, then v = 0.
Proposition 29.1 translates into the following proposition. The proof is left as an exercise.
Proposition 29.9. Given a sesquilinear map ϕ: E × F → K, the following properties hold:
(a) The map lϕ is injective iff Property (1) of Definition 29.10 holds.
(b) The map rϕ is injective iff Property (2) of Definition 29.10 holds.
(c) The sesquilinear form ϕ is nondegenerate and iff lϕ and rϕ are injective.
(d) If the sesquillinear form ϕ is nondegenerate and if E and F have finite dimensions,
then dim(E) = dim(F), and lϕ : E → F
∗ and rϕ : F → E
∗ are linear isomorphisms.
Propositions 29.2 and 29.3 also generalize to sesquilinear forms. We also have the follow￾ing version of Theorem 29.4, whose proof is left as an exercise.
Theorem 29.10. Given any sesquilinear form ϕ: E × E → K with dim(E) = n, if ϕ is
Hermitian and K does not have characteristic 2, then there is a basis (e1, . . . , en) of E such
that ϕ(ei
, ej ) = 0, for all i 6 = j.
As in Section 29.1, if E and F are finite-dimensional vector spaces and if (e1, . . . , em) is
a basis of E and (f1, . . . , fn) is a basis of F then the sesquilinearity of ϕ yields
ϕ

mX
i=1
xiei
,
nX
j=1
yjfj
 =
mX
i=1
nX
j=1
xiϕ(ei
, fj )yj
.
This shows that ϕ is completely determined by the n × m matrix M = (mij ) with mij =
ϕ(ej
, fi), and in matrix form, we have
ϕ(x, y) = x
> M> y = y
∗Mx,
where x and y are the column vectors associated with (x1, . . . , xm) ∈ Km and (y1
, . . . , yn
) ∈
Kn
, and y
∗ = y
> . As earlier, we are committing the slight abuse of notation of letting x
denote both the vector x =
P
n
i=1 xiei and the column vector associated with (x1, . . . , xn)
(and similarly for y).
Definition 29.11. If (e1, . . . , em) is a basis of E and (f1, . . . , fn) is a basis of F, for any
sesquilinear form ϕ: E × F → K, the n × m matrix M = (mij ) given by mij = ϕ(ej
, fi) for
i = 1, . . . , n and j = 1, . . . , m is called the matrix of ϕ with respect to the bases (e1, . . . , em)
and (f1, . . . , fn).
29.3. ORTHOGONALITY 1007
Proposition 29.5 also holds for sesquilinear forms and their matrix representations.
Observe that if ϕ is a Hermitian form (E = F) and if K does not have characteristic 2,
then by Theorem 29.10, there is a basis of E with respect to which the matrix M representing
ϕ is a diagonal matrix. If K = C, then these entries are real, and this allows us to classify
completely the Hermitian forms.
Proposition 29.11. Given any Hermitian form ϕ: E × E → C with dim(E) = n, there is
a basis (e1, . . . , en) of E such that
Φ

nX
i=1
xiei
 =
p
X
i=1
x
2
i −
i=
X
p+
p+1
q
x
2
i
,
with 0 ≤ p, q and p + q ≤ n.
The proof of Proposition 29.11 is the same as the real case of Proposition 29.6. Sylvester’s
inertia law (Proposition 29.7) also holds for Hermitian forms: p and q only depend on ϕ.
29.3 Orthogonality
In this section we assume that we are dealing with a sesquilinear form ϕ: E × F → K. We
allow the automorphism λ 7→ λ to be the identity, in which case ϕ is a bilinear form. This
way, we can deal with properties shared by bilinear forms and sesquilinear forms in a uniform
fashion. Orthogonality is such a property.
Definition 29.12. Given a sesquilinear form ϕ: E ×F → K, we say that two vectors u ∈ E
and v ∈ F are orthogonal (or conjugate) if ϕ(u, v) = 0. Two subsets E
0 ⊆ E and F
0 ⊆ F
are orthogonal if ϕ(u, v) = 0 for all u ∈ E
0 and all v ∈ F
0 . Given a subspace U of E, the
right orthogonal space of U, denoted U
⊥, is the subspace of F given by
U
⊥ = {v ∈ F | ϕ(u, v) = 0 for all u ∈ U},
and given a subspace V of F, the left orthogonal space of V , denoted V
⊥, is the subspace of
E given by
V
⊥ = {u ∈ E | ϕ(u, v) = 0 for all v ∈ V }.
When E and F are distinct, there is little chance of confusing the right orthogonal
subspace U
⊥ of a subspace U of E and the left orthogonal subspace V
⊥ of a subspace V of
F. However, if E = F, then ϕ(u, v) = 0 does not necessarily imply that ϕ(v, u) = 0, that is,
orthogonality is not necessarily symmetric. Thus, if both U and V are subsets of E, there
is a notational ambiguity if U = V . In this case, we may write U
⊥r
for the right orthogonal
and U
⊥l
for the left orthogonal.
The above discussion brings up the following point: When is orthogonality symmetric?
1008 CHAPTER 29. THE GEOMETRY OF BILINEAR FORMS; WITT’S THEOREM
If ϕ is bilinear, it is shown in E. Artin [6] (and in Jacobson [98]) that orthogonality is
symmetric iff either ϕ is symmetric or ϕ is alternating (ϕ(u, u) = 0 for all u ∈ E).
If ϕ is sesquilinear, the answer is more complicated. In addition to the previous two
cases, there is a third possibility:
ϕ(u, v) =  ϕ(v, u) for all u, v ∈ E,
where  is some nonzero element in K. We say that ϕ is  -Hermitian. Observe that
ϕ(u, u) =  ϕ(u, u),
so if ϕ is not alternating, then ϕ(u, u) 6 = 0 for some u, and we must have   = 1. The most
common cases are
1.  = 1, in which case ϕ is Hermitian, and
2.  = −1, in which case ϕ is skew-Hermitian.
If ϕ is alternating and K is not of characteristic 2, then equation (∗) from Section 29.2
implies that the automorphism λ 7→ λ must be the identity if ϕ is nonzero. If so, ϕ is
skew-symmetric, so  = −1.
In summary, if ϕ is either symmetric, alternating, or  -Hermitian, then orthogonality is
symmetric, and it makes sense to talk about the orthogonal subspace U
⊥ of U.
Observe that if ϕ is  -Hermitian, then
rϕ = lϕ.
This is because
lϕ(u)(y) = ϕ(u, y)
rϕ(u)(y) = ϕ(y, u)
=  ϕ(u, y),
so rϕ = lϕ.
If E and F are finite-dimensional with bases (e1, . . . , em) and (f1, . . . , fn), and if ϕ is
represented by the n × m matrix M, then ϕ is  -Hermitian iff
M = M∗
,
where M∗ = (M)
> (as usual). This captures the following kinds of familiar matrices:
1. Symmetric matrices ( = 1)
2. Skew-symmetric matrices ( = −1)
29.3. ORTHOGONALITY 1009
3. Hermitian matrices ( = 1)
4. Skew-Hermitian matrices ( = −1).
Going back to a sesquilinear form ϕ: E × F → K, for any subspace U of E, it is easy to
check that
U ⊆ (U
⊥)
⊥,
and that for any subspace V of F, we have
V ⊆ (V
⊥)
⊥.
For simplicity of notation, we write U
⊥⊥ instead of (U
⊥)
⊥ (and V
⊥⊥ instead of (V
⊥)
⊥).
Given any two subspaces U1 and U2 of E, if U1 ⊆ U2, then U2
⊥ ⊆ U1
⊥. Indeed, if v ∈ U2
⊥
then ϕ(u2, v) = 0 for all u2 ∈ U2, and since U1 ⊆ U2 this implies that ϕ(u1, v) = 0 for all
u1 ∈ U1, which shows that v ∈ U1
⊥. Similarly for any two subspaces V1, V2 of F, if V1 ⊆ V2,
then V2
⊥ ⊆ V1
⊥. As a consequence,
U
⊥ = U
⊥⊥⊥, V ⊥ = V
⊥⊥⊥.
First, we have U
⊥ ⊆ U
⊥⊥⊥. Second, from U ⊆ U
⊥⊥, we get U
⊥⊥⊥ ⊆ U
⊥, so U
⊥ = U
⊥⊥⊥.
The other equation is proved is a similar way.
Observe that ϕ is nondegenerate iff E
⊥ = {0} and F
⊥ = {0}. Furthermore, since
