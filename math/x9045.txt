+ η(1
>p λ + 1
>q µ − ν)
+ Ks(
>  + ξ
> ξ) − 
> λ − ξ
> µ + b(1
>p λ − 1
>q µ) + 1
2
b
2
.
To find the dual function G(λ, µ) we minimize L(w, , ξ, b, η, λ, µ) with respect to w, , ξ, b,
and η. Since the Lagrangian is convex and (w, , ξ, b, η) ∈ R
n×R
p×R
q×R×R, a convex open
set, by Theorem 40.13, the Lagrangian has a minimum in (w, , ξ, b, η) iff ∇Lw,,ξ,b,η = 0, so
54.15. SOFT MARGIN SVM; (SVMs5) 2011
we compute ∇Lw,,ξ,b,η. The gradient ∇Lw,,ξ,b,η is given by
∇Lw,,ξ,b,η =


w + X

µ
λ

2Ks − λ
2Ksξ − µ
1
b
>
p
+
λ
1
+
>
p λ
1
>q
−
µ
1
−
>
q µ
ν


.
By setting ∇Lw,,ξ,b,η = 0 we get the equations
w = −X

µ
λ

, (∗w)
2Ks = λ
2Ksξ = µ
b = −(1
>p λ − 1
>q µ)
1
>p λ + 1
>q µ = ν.
As we said earlier, both w an b are determined by λ and µ. We can use the equations to
obtain the following expression for the dual function G(λ, µ, γ),
G(λ, µ, γ) = −
1
4Ks
(λ
> λ + µ
> µ) −
1
2
￾
λ
> µ
>
 X
> X

µ
λ

−
b
2
2
= −
1
2
￾
λ
> µ
>

 X
> X +

1p1
>p −1p1
>q
−1q1
>p 1q1
>q

+
2K
1
s
Ip+q
 
µ
λ

.
Consequently the dual program is equivalent to the minimization program
Dual of the Soft margin SVM (SVMs5):
minimize
1
2
￾
λ
> µ
>

 X
> X +

1p1
>p −1p1
>q
−1q1
>p 1q1
>q

+
2K
1
s
Ip+q
 
µ
λ

subject to
p
X
i=1
λi +
q
X
j=1
µj = ν
λi ≥ 0, i = 1, . . . , p
µj ≥ 0, j = 1, . . . , q.
It is shown in Section 54.16 how the dual program is solved using ADMM from Section
52.6. If the primal problem is solvable, this yields solutions for λ and µ.
The constraint
p
X
i=1
λi +
q
X
j=1
µj = ν
2012 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
implies that either there is some i0 such that λi0 > 0 or there is some j0 such that µj0 > 0,
so we have  i0 > 0 or ξj0 > 0, which means that at least one point is misclassified. Thus
Problem (SVMs5) should only be used when the sets {ui} and {vj} are not linearly separable.
We can use the fact that the duality gap is 0 to find η. We have
1
2
w
> w +
b
2
2
− νη + Ks(
>  + ξ
> ξ)
= −
1
2
￾
λ
> µ
>

 X
> X +

1p1
>p −1p1
>q
−1q1
>p 1q1
>q

+
2K
1
s
Ip+q
 
µ
λ

,
so we get
νη = Ks(
>  + ξ
> ξ) + ￾ λ
> µ
>

 X
> X +

1p1
>p −1p1
>q
−1q1
>p 1q1
>q

+
4K
1
s
Ip+q
 
µ
λ

=
￾ λ
> µ
>

 X
> X +

1p1
>p −1p1
>q
−1q1
>p 1q1
>q

+
2K
1
s
Ip+q
 
µ
λ

.
The above confirms that at optimality we have η ≥ 0.
Remark: If we do not assume that Ks = 1/(p+q), then the above formula must be replaced
by
(p + q)Ksνη =
￾ λ
> µ
>

 X
> X +

1p1
>p −1p1
>q
−1q1
>p 1q1
>q

+
2K
1
s
Ip+q
 
µ
λ

.
There is a version of Theorem 54.8 stating that for a fixed Ks, the solution to Problem
(SVMs5) is unique and independent of the value of ν.
Theorem 54.9. For Ks and ν fixed, if Problem (SVMs5) succeeds then it has a unique solu￾tion. If Problem (SVMs5) succeeds and returns (λ, µ, η, w, b) for the value ν and (λ
κ
, µκ
, ηκ
,
w
κ
, b
κ
) for the value κν with κ > 0, then
λ
κ = κλ, µκ = κµ, ηκ = κη, wκ = κw, bκ = κb.
As a consequence, δ = η/ k wk = η
κ/ k w
κk = δ
κ
, and the hyperplanes Hw,b, Hw,b+η and Hw,b−η
are independent of ν.
Proof. The proof is an easy adaptation of the proof of Theorem 54.8 so we only give a sketch.
The two crucial points are that the matrix
P = X
> X +

1p1
>p −1p1
>q
−1q1
>p 1q1
>q

+
2K
1
s
Ip+q
is symmetric positive definite and that we have the single equational constraint
1
>p λ + 1
>q µ = (p + q)Ksν
54.16. SOLVING SVM (SVMs5) USING ADMM 2013
defining the convex set
U =

µ
λ

∈ R
p
+
+q
| 1
>p λ + 1
>q µ = (p + q)Ksν
 .
The proof is essentially the proof of 54.8 using the above SPD matrix and convex set.
The “kernelized” version of Problem (SVMs5) is the following:
Soft margin kernel SVM (SVMs5):
minimize
1
2
h
w, wi +
1
2
b
2 − νη + Ks(
>  + ξ
> ξ)
subject to
h
w, ϕ(ui)i − b ≥ η −  i
, i = 1, . . . , p
− hw, ϕ(vj )i + b ≥ η − ξj
, j = 1, . . . , q,
with Ks = 1/(p + q).
Tracing through the derivation of the dual program, we obtain
Dual of the Soft margin kernel SVM (SVMs5):
minimize
1
2
￾
λ
> µ
>

 K +

1p1
>p −1p1
>q
−1q1
>p 1q1
>q

+
2K
1
s
Ip+q
 
µ
λ

subject to
p
X
i=1
λi +
q
X
j=1
µj = ν
λi ≥ 0, i = 1, . . . , p
µj ≥ 0, j = 1, . . . , q,
where K is the kernel matrix of Section 54.1. Then w, b, and f(x) are obtained exactly as
in Section 54.13.
54.16 Solving SVM (SVMs5) Using ADMM
In order to solve (SVM5) using ADMM we need to write the matrix corresponding to the
constraints in equational form,
p
X
i=1
λi +
q
X
j=1
µj = Km,
with Km = (p + q)Ksν. This is the 1 × (p + q) matrix A given by
A =
￾ 1
>p 1
>q

.
2014 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
Obviously, A has rank 1. The right-hand side is
c = Km.
The symmetric positive definite (p + q) × (p + q) matrix P defining the quadratic functional
is
P = X
> X +

1p1
>p −1p1
>q
−1q1
>p 1q1
>q

+
2K
1
s
Ip+q, with X =
￾ −u1 · · · −up v1 · · · vq
 ,
and
q = 0p+q.
Since there are p + q Lagrange multipliers (λ, µ), the (p + q) × (p + q) matrix P does not
have to be augmented with zero’s.
We ran our Matlab implementation of the above version of (SVMs5) on the data set of
Section 54.14. Since the value of ν is irrelevant, we picked ν = 1. First we ran our program
with K = 190; see Figure 54.24. We have pm = 23 and qm = 18. The program does not
converge for K ≥ 200.
-30 -20 -10 0 10 20 30 40
-30
-20
-10
0
10
20
30
Figure 54.24: Running (SVMs5) on two sets of 30 points; K = 190.
54.17. SUMMARY AND COMPARISON OF THE SVM METHODS 2015
-30 -20 -10 0 10 20 30 40
-30
-20
-10
0
10
20
30
Figure 54.25: Running (SVMs5) on two sets of 30 points; K = 1/13000.
Our second run was made with K = 1/13000; see Figure 54.25. We have pm = 30 and
qm = 30 and we see that the width of the slab is a bit excessive. This example demonstrates
that the margin lines need not contain data points.
Method (SVMs5) always returns a value for b and η smaller than the value returned by
(SVMs4) (because of the term (1/2)b
2 added to the objective function) but in this example
the difference is too small to be noticed.
54.17 Summary and Comparison of the SVM Methods
In this chapter we considered six variants for solving the soft margin binary classification
problem for two sets of points {ui}
p
i=1 and {vj}
q
j=1 using support vector classification meth￾ods. The objective is to find a separating hyperplane Hw,b of equation w
> x−b = 0. We also
try to find two “margin hyperplanes” Hw,b+δ of equation w
> x − b − δ = 0 (the blue margin
hyperplane) and Hw,b−δ of equation w
> x − b + δ = 0 (the red margin hyperplane) such that
δ is as big as possible and yet the number of misclassified points is minimized, which is
achieved by allowing an error  i ≥ 0 for every point ui
, in the sense that the constraint
w
> ui − b ≥ δ −  i
should hold, and an error ξj ≥ 0 for every point vj
, in the sense that the constraint
−w
> vj + b ≥ δ − ξj
2016 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
should hold.
The goal is to design an objective function that minimizes  and ξ and maximizes δ.
The optimization problem should also solve for w and b, and for this some constraint has to
be placed on w. Another goal is to try to use the dual program to solve the optimization
problem, because the solutions involve inner products, and thus the problem is amenable to
a generalization using kernel functions.
The first attempt, which is to use the objective function
J(w, , ξ, b, δ) = −δ + K
￾  > ξ
>
 1p+q
and the constraint w
> w ≤ 1, does not work very well because this constraint needs to be
guarded by a Lagrange multiplier γ ≥ 0, and as a result, minimizing the Lagrangian L to
find the dual function G gives an equation for solving w of the form
2γw = −X
>

µ
λ

,
but if the sets {ui}
p
i=1 and {vj}
q
j=1 are not linearly separable, then an optimal solution may
occurs for γ = 0, in which case it is impossible to determine w. This is Problem (SVMs1)
considered in Section 54.1.
Soft margin SVM (SVMs1):
minimize − δ + K

p
X
i=1

i +
q
X
j=1
ξj

subject to
w
> ui − b ≥ δ −  i
, i ≥ 0 i = 1, . . . , p
− w
> vj + b ≥ δ − ξj
, ξj ≥ 0 j = 1, . . . , q
w
> w ≤ 1.
It is customary to write ` = p + q.
It is shown in Section 54.1 that the dual program is equivalent to the following minimiza￾tion program:
54.17. SUMMARY AND COMPARISON OF THE SVM METHODS 2017
Dual of the Soft margin SVM (SVMs1):
minimize ￾ λ
> µ
>
 X
> X

µ
λ

subject to
p
X
i=1
λi −
q
X
j=1
µj = 0
p
X
i=1
λi +
q
X
j=1
µj = 1
0 ≤ λi ≤ K, i = 1, . . . , p
0 ≤ µj ≤ K, j = 1, . . . , q.
The points ui and vj are naturally classified in terms of the values of λi and µj
. The
numbers of points in each category have a direct influence on the choice of the parameter
K. Let us summarize some of the keys items from Definition 54.1.
The vectors ui on the blue margin Hw,b+δ and the vectors vj on the red margin Hw,b−δ are
called support vectors. Support vectors correspond to vectors ui
for which w
> ui − b − δ = 0
(which implies  i = 0), and vectors vj
for which w
> vj − b + δ = 0 (which implies ξj = 0).
Support vectors ui such that 0 < λi < K and support vectors vj such that 0 < µj < K are
support vectors of type 1 . Support vectors of type 1 play a special role so we denote the sets
of indices associated with them by
Iλ = {i ∈ {1, . . . , p} | 0 < λi < K}
Iµ = {j ∈ {1, . . . , q} | 0 < µj < K}.
We denote their cardinalities by numsvl1 = |Iλ| and numsvm1 = |Iµ|.
The vectors ui
for which λi = K and the vectors vj
for which µj = K are said to fail the
margin. The sets of indices associated with the vectors failing the margin are denoted by
Kλ = {i ∈ {1, . . . , p} | λi = K}
Kµ = {j ∈ {1, . . . , q} | µj = K}.
We denote their cardinalities by pf = |Kλ| and qf = |Kµ|.
Vectors ui such that λi > 0 and vectors vj such that µj > 0 are said to have margin at
most δ. The sets of indices associated with these vectors are denoted by
Iλ>0 = {i ∈ {1, . . . , p} | λi > 0}
Iµ>0 = {j ∈ {1, . . . , q} | µj > 0}.
2018 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
We denote their cardinalities by pm = |Iλ>0| and qm = |Iµ>0|.
Obviously, pf ≤ pm and qf ≤ qm. There are p − pm points ui classified correctly on the
blue side and outside the δ-slab and there are q − qm points vj classified correctly on the red
side and outside the δ-slab. Intuitively a blue point that fails the margin is on the wrong
side of the blue margin and a red point that fails the margin is on the wrong side of the red
margin.
It can be shown that that K must be chosen so that
max 
2p
1
m
,
1
2qm

≤ K ≤ min 
2
1
pf
,
1
2qf

.
If the optimal value is 0, then γ = 0 and X

µ
λ

= 0, so in this case it is not possible
to determine w. However, if the optimal value is > 0, then once a solution for λ and µ is
obtained, we have
γ =
1
2

￾
λ
> µ
>
 X
> X

µ
λ

1/2
w =
1
2γ

p
X
i=1
λiui −
q
X
j=1
µjvj

,
so we get
w =
p
X
i=1
λiui −
q
X
j=1
µjvj

￾
λ
> µ
>
 X> X

µ
λ

1/2
,
If the following mild hypothesis holds, then b and δ can be found.
Standard Margin Hypothesis for (SVMs1). There is some index i0 such that 0 <
λi0 < K and there is some index j0 such that 0 < µj0 < K. This means that some ui0
is a
support vector of type 1 on the blue margin, and some vj0
is a support vector of type 1 on
the red margin.
If the Standard Margin Hypothesis for (SVMs1) holds, then  i0 = 0 and µj0 = 0, and
we have the active equations
w
> ui0 − b = δ and − w
> vj0 + b = δ,
and we obtain the value of b and δ as
b =
1
2
w
> (ui0 + vj0
)
δ =
1
2
w
> (ui0 − vj0
).
54.17. SUMMARY AND COMPARISON OF THE SVM METHODS 2019
The second more successful approach is to add the term (1/2)w
> w to the objective
function and to drop the constraint w
> w ≤ 1. There are several variants of this method,
depending on the choice of the regularizing term involving  and ξ (linear or quadratic), how
the margin is dealt with (implicitly with the term 1 or explicitly with a term η), and whether
the term (1/2)b
2
is added to the objective function or not.
These methods all share the property that if the primal problem has an optimal solution
with w 6 = 0, then the dual problem always determines w, and then under mild conditions
which we call standard margin hypotheses, b and η can be determined. Then  and ξ can
be determined using the constraints that are active. When (1/2)b
2
is added to the objective
function, b is determined by the equation
b = −(1
>p λ − 1
>q µ).
All these problems are convex and the constraints are qualified, so the duality gap is zero,
and if the primal has an optimal solution with w 6 = 0, then it follows that η ≥ 0.
We now consider five variants in more details.
(1) Basic soft margin SVM: (SVMs2).
This is the optimization problem in which the regularization term K
￾  > ξ
>
 1p+q is
linear and the margin δ is given by δ = 1/ k wk :
minimize
1
2
w
> w + K
￾  > ξ
>
 1p+q
subject to
w
> ui − b ≥ 1 −  i
, i ≥ 0 i = 1, . . . , p
− w
> vj + b ≥ 1 − ξj
, ξj ≥ 0 j = 1, . . . , q.
This problem is the classical one discussed in all books on machine learning or pattern
analysis, for instance Vapnik [182], Bishop [23], and Shawe–Taylor and Christianini
[159]. It is shown in Section 54.3 that the dual program is
Dual of the Basic soft margin SVM: (SVMs2):
minimize
1
2
￾
λ
> µ
>
 X
> X

µ
λ

−
￾ λ
> µ
>
 1p+q
subject to
p
X
i=1
λi −
q
X
j=1
µj = 0
0 ≤ λi ≤ K, i = 1, . . . , p
0 ≤ µj ≤ K, j = 1, . . . , q.
2020 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
We can use the dual program to solve the primal. Once λ ≥ 0, µ ≥ 0 have been found,
w is given by
w = −X

µ
λ

=
p
X
i=1
λiui −
q
X
j=1
µjvj
,
but b is not determined by the dual.
The complementary slackness conditions imply that if  i > 0, then λi = K, and if
ξj > 0, then µj = K. Consequently, if λi < K, then  i = 0 and ui
is correctly
classified, and similarly if µj < K, then ξj = 0 and vj
is correctly classified.
A priori nothing prevents the situation where λi = K for all nonzero λi or µj = K for
all nonzero µj
. If this happens, we can rerun the optimization method with a larger
value of K. If the following mild hypothesis holds then b can be found.
Standard Margin Hypothesis for (SVMs2). There is some support vector ui0 of
type 1 on the blue margin, and some support vector vj0 of type 1 on the red margin.
If the Standard Margin Hypothesis for (SVMs2) holds then  i0 = 0 and µj0 = 0,
and then we have the active equations
w
> ui0 − b = 1 and − w
> vj0 + b = 1,
and we obtain
b =
1
2
w
> (ui0 + vj0
).
(2) Basic Soft margin ν-SVM Problem (SVMs2
0 ).
This a generalization of Problem (SVMs2) for a version of the soft margin SVM coming
from Problem (SVMh2), obtained by adding an extra degree of freedom, namely instead
of the margin δ = 1/ k wk , we use the margin δ = η/ k wk where η is some positive
constant that we wish to maximize. To do so, we add a term −Kmη to the objective
function. We have the following optimization problem:
minimize
1
2
w
> w − Kmη + Ks
￾ 
> ξ
>
 1p+q
subject to
w
> ui − b ≥ η −  i
, i ≥ 0 i = 1, . . . , p
− w
> vj + b ≥ η − ξj
, ξj ≥ 0 j = 1, . . . , q
η ≥ 0,
where Km > 0 and Ks > 0 are fixed constants that can be adjusted to determine the
influence of η and the regularizing term.
54.17. SUMMARY AND COMPARISON OF THE SVM METHODS 2021
This version of the SVM problem was first discussed in Sch¨olkopf, Smola, Williamson,
and Bartlett [147] under the name of ν-SVC , and also used in Sch¨olkopf, Platt, Shawe–
Taylor, and Smola [146].
In order for the problem to have a solution we must pick Km and Ks so that
Km ≤ min{2pKs, 2qKs}.
It is shown in Section 54.5 that the dual program is
Dual of the Basic Soft margin ν-SVM Problem (SVMs2
0 ):
minimize
2
1 ￾
λ
> µ
>
 X
> X

µ
λ

subject to
p
X
i=1
λi −
q
X
j=1
µj = 0
p
X
i=1
λi +
q
X
j=1
µj ≥ Km
0 ≤ λi ≤ Ks, i = 1, . . . , p
0 ≤ µj ≤ Ks, j = 1, . . . , q.
If the primal problem has an optimal solution with w 6 = 0, then using the fact that the
duality gap is zero we can show that η ≥ 0. Thus constraint η ≥ 0 could be omitted.
As in the previous case w is given by
w = −X

µ
λ

=
p
X
i=1
λiui −
q
X
j=1
µjvj
,
but b and η are not determined by the dual.
If we drop the constraint η ≥ 0, then the inequality
p
X
i=1
λi +
q
X
j=1
µj ≥ Km
is replaced by the equation
p
X
i=1
λi +
q
X
j=1
µj = Km.
It convenient to define ν > 0 such that
ν =
Km
(p + q)Ks
,
2022 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
so that the objective function J(w, , ξ, b, η) is given by
J(w, , ξ, b, η) = 1
2
w
> w + (p + q)Ks
 −νη +
p +
1
q
￾

>
ξ
>
 1p+q
 .
Since we obtain an equivalent problem by rescaling by a common positive factor, the￾oretically it is convenient to normalize Ks as
Ks =
1
p + q
,
in which case Km = ν. This method is called the ν-support vector machine.
Under the Standard Margin Hypothesis for (SVMs2
0 ), there is some support vector
ui0 of type 1 and some support vector vj0 of type 1, and by the complementary slackness
conditions  i0 = 0 and ξj0 = 0, so we have the two active constraints
w
> ui0 − b = η, −w
> vj0 + b = η,
and we can solve for b and η and we get
b =
w
> (ui0 + vj0
)
2
η =
w
> (ui0 − vj0
)
2
.
Due to numerical instability, when writing a computer program it is preferable to
compute the lists of indices Iλ and Iµ given by
Iλ = {i ∈ {1, . . . , p} | 0 < λi < Ks}, Iµ = {j ∈ {1, . . . , q} | 0 < µj < Ks}.
