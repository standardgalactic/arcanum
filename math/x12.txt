ϕ1(u0)
ϕ1
is a linear form that vanishes on H since both ϕ1 and ϕ2 vanish on H, but also vanishes on
Ku0 since

ϕ2 −
ϕ2(u0)
ϕ1(u0)
ϕ1
 (λu0) = ϕ2(λu0) −
ϕ2(u0)
ϕ1(u0)
ϕ1(λu0)
= λϕ2(u0) − λ
ϕ2(u0)
ϕ1(u0)
ϕ1(u0) = λϕ2(u0) − λϕ2(u0) = 0
for all λ ∈ K. Since E = H ⊕ Ku0, we deduce that ϕ2 − αϕ2 vanishes on E, with
α =
ϕ2(u0)
ϕ1(u0)
6
= 0,
and so ϕ2 = αϕ1, as claimed.
198 CHAPTER 6. DIRECT SUMS
It is immediately verified that if H is a hyperplane in E defined by a nonzero linear form
ϕ so that H = Ker ϕ, then for any nonzero α ∈ K, the linear form αϕ is a nonzero linear
form that also defines H, that is, H = Ker αϕ. This fact with the second part of Proposition
6.22 shows that a hyperplane H in E is defined by the one-dimensional subspace of the dual
E
∗ of E consisting of all the linear forms that vanish on H (including the zero linear form).
This is an instance of duality.
6.4 Summary
The main concepts and results of this chapter are listed below:
• Direct products, sums, direct sums.
• Projections.
• The fundamental equation
dim(E) = dim(Ker f) + dim(Im f) = dim(Ker f) + rk(f)
(Proposition 6.16).
• Grassmann’s relation
dim(U) + dim(V ) = dim(U + V ) + dim(U ∩ V ).
• Characterizations of a bijective linear map f : E → F.
• Rank of a matrix.
6.5 Problems
Problem 6.1. Let V and W be two subspaces of a vector space E. Prove that if V ∪ W is
a subspace of E, then either V ⊆ W or W ⊆ V .
Problem 6.2. Prove that for every vector space E, if f : E → E is an idempotent linear
map, i.e., f ◦ f = f, then we have a direct sum
E = Ker f ⊕ Im f,
so that f is the projection onto its image Im f.
Problem 6.3. Let U1, . . . , Up be any p ≥ 2 subspaces of some vector space E and recall
that the linear map
a: U1 × · · · × Up → E
6.5. PROBLEMS 199
is given by
a(u1, . . . , up) = u1 + · · · + up,
with ui ∈ Ui
for i = 1, . . . , p.
(1) If we let Zi ⊆ U1 × · · · × Up be given by
Zi =

 u1, . . . , ui−1, −
j=1
X
p
,j6=i
uj
, ui+1, . . . , up



 

X
p
j=1,j6=i
uj ∈ Ui ∩

X
p
j=1,j6=i
Uj

,
for i = 1, . . . , p, then prove that
Ker a = Z1 = · · · = Zp.
In general, for any given i, the condition Ui ∩

P
p
j=1,j6=i Uj
 = (0) does not necessarily
imply that Zi = (0). Thus, let
Z =

 u1, . . . , ui−1, ui
, ui+1, . . . , up




 ui = −
j=1
X
p
,j6=i
uj
, ui ∈ Ui ∩

X
p
j=1,j6=i
Uj

, 1 ≤ i ≤ p
 .
Since Ker a = Z1 = · · · = Zp, we have Z = Ker a. Prove that if
Ui ∩

X
p
j=1,j6=i
Uj
 = (0) 1 ≤ i ≤ p,
then Z = Ker a = (0).
(2) Prove that U1 + · · · + Up is a direct sum iff
Ui ∩

X
p
j=1,j6=i
Uj
 = (0) 1 ≤ i ≤ p.
Problem 6.4. Assume that E is finite-dimensional, and let fi
: E → E be any p ≥ 2 linear
maps such that
f1 + · · · + fp = idE.
Prove that the following properties are equivalent:
(1) fi
2 = fi
, 1 ≤ i ≤ p.
(2) fj ◦ fi = 0, for all i 6 = j, 1 ≤ i, j ≤ p.
Hint. Use Problem 6.2.
200 CHAPTER 6. DIRECT SUMS
Let U1, . . . , Up be any p ≥ 2 subspaces of some vector space E. Prove that U1 + · · · + Up
is a direct sum iff
Ui ∩

i−1
X
j=1
Uj
 = (0), i = 2, . . . , p.
Problem 6.5. Given any vector space E, a linear map f : E → E is an involution if
f ◦ f = id.
(1) Prove that an involution f is invertible. What is its inverse?
(2) Let E1 and E−1 be the subspaces of E defined as follows:
E1 = {u ∈ E | f(u) = u}
E−1 = {u ∈ E | f(u) = −u}.
Prove that we have a direct sum
E = E1 ⊕ E−1.
Hint. For every u ∈ E, write
u =
u + f(u)
2
+
u − f(u)
2
.
(3) If E is finite-dimensional and f is an involution, prove that there is some basis of E
with respect to which the matrix of f is of the form
Ik,n−k =

Ik 0
0 −In−k

,
where Ik is the k × k identity matrix (similarly for In−k) and k = dim(E1). Can you give a
geometric interpretation of the action of f (especially when k = n − 1)?
Problem 6.6. An n × n matrix H is upper Hessenberg if hjk = 0 for all (j, k) such that
j − k ≥ 0. An upper Hessenberg matrix is unreduced if hi+1i 6 = 0 for i = 1, . . . , n − 1.
Prove that if H is a singular unreduced upper Hessenberg matrix, then dim(Ker (H)) = 1.
Problem 6.7. Let A be any n × k matrix.
(1) Prove that the k × k matrix A> A and the matrix A have the same nullspace. Use
this to prove that rank(A> A) = rank(A). Similarly, prove that the n × n matrix AA> and
the matrix A> have the same nullspace, and conclude that rank(AA> ) = rank(A> ).
We will prove later that rank(A> ) = rank(A).
(2) Let a1, . . . , ak be k linearly independent vectors in R
n
(1 ≤ k ≤ n), and let A be the
n × k matrix whose ith column is ai
. Prove that A> A has rank k, and that it is invertible.
Let P = A(A> A)
−1A> (an n × n matrix). Prove that
P
2 = P
P
> = P.
6.5. PROBLEMS 201
What is the matrix P when k = 1?
(3) Prove that the image of P is the subspace V spanned by a1, . . . , ak, or equivalently
the set of all vectors in R
n of the form Ax, with x ∈ R
k
. Prove that the nullspace U of P is
the set of vectors u ∈ R
n
such that A> u = 0. Can you give a geometric interpretation of U?
Conclude that P is a projection of R
n onto the subspace V spanned by a1, . . . , ak, and
that
R
n = U ⊕ V.
Problem 6.8. A rotation Rθ in the plane R
2
is given by the matrix
Rθ =

cos
sin θ
θ −
cos
sin
θ
θ

.
(1) Use Matlab to show the action of a rotation Rθ on a simple figure such as a triangle
or a rectangle, for various values of θ, including θ = π/6, π/4, π/3, π/2.
(2) Prove that Rθ is invertible and that its inverse is R−θ.
(3) For any two rotations Rα and Rβ, prove that
Rβ ◦ Rα = Rα ◦ Rβ = Rα+β.
Use (2)-(3) to prove that the rotations in the plane form a commutative group denoted
SO(2).
Problem 6.9. Consider the affine map Rθ,(a1,a2)
in R
2 given by

y
y
1
2

=

cos
sin θ
θ −
cos
sin
θ
θ
  x
x
1
2

+

a
a
1
2

.
(1) Prove that if θ 6 = k2π, with k ∈ Z, then Rθ,(a1,a2) has a unique fixed point (c1, c2),
that is, there is a unique point (c1, c2) such that

c
c
1
2

= Rθ,(a1,a2)

c
c
1
2

,
and this fixed point is given by

c
c
1
2

=
2 sin(
1
θ/2) 
cos(
sin(π/
π/
2
2
−
−
θ/
θ/
2) cos(
2) − sin(
π/
π/
2
2
−
−
θ/
θ/
2)
2)  a
a
1
2

.
(2) In this question we still assume that θ 6 = k2π, with k ∈ Z. By translating the
coordinate system with origin (0, 0) to the new coordinate system with origin (c1, c2), which
202 CHAPTER 6. DIRECT SUMS
means that if (x1, x2) are the coordinates with respect to the standard origin (0, 0) and if
(x
01
, x02
) are the coordinates with respect to the new origin (c1, c2), we have
x1 = x
01 + c1
x2 = x
02 + c2
and similarly for (y1, y2) and (y1
0
, y2
0
), then show that

y
y
1
2

= Rθ,(a1,a2)

x
x
1
2

becomes

y
y
1
2
0
0

= Rθ

x
01
x
02

.
Conclude that with respect to the new origin (c1, c2), the affine map Rθ,(a1,a2) becomes
the rotation Rθ. We say that Rθ,(a1,a2)
is a rotation of center (c1, c2).
(3) Use Matlab to show the action of the affine map Rθ,(a1,a2) on a simple figure such as a
triangle or a rectangle, for θ = π/3 and various values of (a1, a2). Display the center (c1, c2)
of the rotation.
What kind of transformations correspond to θ = k2π, with k ∈ Z?
(4) Prove that the inverse of Rθ,(a1,a2)
is of the form R−θ,(b1,b2)
, and find (b1, b2) in terms
of θ and (a1, a2).
(5) Given two affine maps Rα,(a1,a2) and Rβ,(b1,b2)
, prove that
Rβ,(b1,b2) ◦ Rα,(a1,a2) = Rα+β,(t1,t2)
for some (t1, t2), and find (t1, t2) in terms of β, (a1, a2) and (b1, b2).
Even in the case where (a1, a2) = (0, 0), prove that in general
Rβ,(b1,b2) ◦ Rα 6 = Rα ◦ Rβ,(b1,b2)
.
Use (4)-(5) to show that the affine maps of the plane defined in this problem form a
nonabelian group denoted SE(2).
Prove that Rβ,(b1,b2) ◦Rα,(a1,a2)
is not a translation (possibly the identity) iff α +β 6 = k2π,
for all k ∈ Z. Find its center of rotation when (a1, a2) = (0, 0).
If α+β = k2π, then Rβ,(b1,b2) ◦Rα,(a1,a2)
is a pure translation. Find the translation vector
of Rβ,(b1,b2) ◦ Rα,(a1,a2)
.
Problem 6.10. (Affine subspaces) A subset A of R
n
is called an affine subspace if either
A = ∅, or there is some vector a ∈ R
n and some subspace U of R
n
such that
A = a + U = {a + u | u ∈ U}.
6.5. PROBLEMS 203
We define the dimension dim(A) of A as the dimension dim(U) of U.
(1) If A = a + U, why is a ∈ A?
What are affine subspaces of dimension 0? What are affine subspaces of dimension 1
(begin with R
2
)? What are affine subspaces of dimension 2 (begin with R
3
)?
Prove that any nonempty affine subspace is closed under affine combinations.
(2) Prove that if A = a + U is any nonempty affine subspace, then A = b + U for any
b ∈ A.
(3) Let A be any nonempty subset of R
n
closed under affine combinations. For any
a ∈ A, prove that
Ua = {x − a ∈ R
n
| x ∈ A}
is a (linear) subspace of R
n
such that
A = a + Ua.
Prove that Ua does not depend on the choice of a ∈ A; that is, Ua = Ub for all a, b ∈ A. In
fact, prove that
Ua = U = {y − x ∈ R
n
| x, y ∈ A}, for all a ∈ A,
and so
A = a + U, for any a ∈ A.
Remark: The subspace U is called the direction of A.
(4) Two nonempty affine subspaces A and B are said to be parallel iff they have the same
direction. Prove that that if A 6= B and A and B are parallel, then A ∩ B = ∅.
Remark: The above shows that affine subspaces behave quite differently from linear sub￾spaces.
Problem 6.11. (Affine frames and affine maps) For any vector v = (v1, . . . , vn) ∈ R
n
, let
b
v ∈ R
n+1 be the vector vb = (v1, . . . , vn, 1). Equivalently, vb = (vb1, . . . , vbn+1) ∈ R
n+1 is the
vector defined by
b
vi =
(
vi
if 1 ≤ i ≤ n,
1 if i = n + 1.
(1) For any m + 1 vectors (u0, u1, . . . , um) with ui ∈ R
n and m ≤ n, prove that if the m
vectors (u1 − u0, . . . , um − u0) are linearly independent, then the m + 1 vectors (ub0, . . . , ubm)
are linearly independent.
(2) Prove that if the m + 1 vectors (ub0, . . . , ubm) are linearly independent, then for any
choice of i, with 0 ≤ i ≤ m, the m vectors uj − ui
for j ∈ {0, . . . , m} with j − i 6 = 0 are
linearly independent.
204 CHAPTER 6. DIRECT SUMS
Any m + 1 vectors (u0, u1, . . . , um) such that the m + 1 vectors (ub0, . . . , ubm) are linearly
independent are said to be affinely independent.
From (1) and (2), the vector (u0, u1, . . . , um) are affinely independent iff for any any choice
of i, with 0 ≤ i ≤ m, the m vectors uj − ui
for j ∈ {0, . . . , m} with j − i 6 = 0 are linearly
independent. If m = n, we say that n + 1 affinely independent vectors (u0, u1, . . . , un) form
an affine frame of R
n
.
(3) if (u0, u1, . . . , un) is an affine frame of R
n
, then prove that for every vector v ∈ R
n
,
there is a unique (n+ 1)-tuple (λ0, λ1, . . . , λn) ∈ R
n+1, with λ0 +λ1 +· · ·+λn = 1, such that
v = λ0u0 + λ1u1 + · · · + λnun.
The scalars (λ0, λ1, . . . , λn) are called the barycentric (or affine) coordinates of v w.r.t. the
affine frame (u0, u1, . . . , un).
If we write ei = ui − u0, for i = 1, . . . , n, then prove that we have
v = u0 + λ1e1 + · · · + λnen,
and since (e1, . . . , en) is a basis of R
n
(by (1) & (2)), the n-tuple (λ1, . . . , λn) consists of the
standard coordinates of v − u0 over the basis (e1, . . . , en).
Conversely, for any vector u0 ∈ R
n and for any basis (e1, . . . , en) of R
n
, let ui = u0 + ei
for i = 1, . . . , n. Prove that (u0, u1, . . . , un) is an affine frame of R
n
, and for any v ∈ R
n
, if
v = u0 + x1e1 + · · · + xnen,
with (x1, . . . , xn) ∈ R
n
(unique), then
v = (1 − (x1 + · · · + xx))u0 + x1u1 + · · · + xnun,
so that (1−(x1 +· · ·+xx)), x1, · · · , xn), are the barycentric coordinates of v w.r.t. the affine
frame (u0, u1, . . . , un).
The above shows that there is a one-to-one correspondence between affine frames (u0, . . .,
un) and pairs (u0,(e1, . . . , en)), with (e1, . . . , en) a basis. Given an affine frame (u0, . . . , un),
we obtain the basis (e1, . . . , en) with ei = ui −u0, for i = 1, . . . , n; given the pair (u0,(e1, . . .,
en)) where (e1, . . . , en) is a basis, we obtain the affine frame (u0, . . . , un), with ui = u0 + ei
,
for i = 1, . . . , n. There is also a one-to-one correspondence between barycentric coordinates
w.r.t. the affine frame (u0, . . . , un) and standard coordinates w.r.t. the basis (e1, . . . , en).
The barycentric cordinates (λ0, λ1, . . . , λn) of v (with λ0 + λ1 + · · · + λn = 1) yield the
standard coordinates (λ1, . . . , λn) of v − u0; the standard coordinates (x1, . . . , xn) of v − u0
yield the barycentric coordinates (1 − (x1 + · · · + xn), x1, . . . , xn) of v.
(4) Recall that an affine map is a map f : E → F between vector spaces that preserves
affine combinations; that is,
f
 
mX
i=1
λiui
! =
mX
i=1
λif(ui),
6.5. PROBLEMS 205
for all u1 . . . , um ∈ E and all λi ∈ K such that P m
i=1 λi = 1.
Let (u0, . . . , un) be any affine frame in R
n and let (v0, . . . , vn) be any vectors in R
m. Prove
that there is a unique affine map f : R
n → R
m such that
f(ui) = vi
, i = 0, . . . , n.
(5) Let (a0, . . . , an) be any affine frame in R
n and let (b0, . . . , bn) be any n + 1 points in
R
n
. Prove that there is a unique (n + 1) × (n + 1) matrix
A =

B w
0 1
corresponding to the unique affine map f such that
f(ai) = bi
, i = 0, . . . , n,
in the sense that
Abai = b bi
, i = 0, . . . , n,
and that A is given by
A =
 b b0
b b1 · · · b bn

￾ b a0 b a1 · · · b an

−1
.
Make sure to prove that the bottom row of A is (0, . . . , 0, 1).
In the special case where (a0, . . . , an) is the canonical affine frame with ai = ei+1 for
i = 0, . . . , n − 1 and an = (0, . . . , 0) (where ei
is the ith canonical basis vector), show that
￾
b
a0 b a1 · · · b an
 =


1 0
0 1
· · ·
· · ·
0 0
0 0
.
.
.
.
.
.
.
.
. 0 0
0 0
1 1
· · ·
· · ·
1 0
1 1


and
￾
b
a0 b a1 · · · b an

−1
=


1 0
0 1
· · ·
· · ·
0 0
0 0
.
.
.
.
.
.
.
.
. 0 0
0 0 · · ·
−1 −1 · · · −
1 0
1 1


.
For example, when n = 2, if we write bi = (xi
, yi), then we have
A =


x1 x2 x3
y1 y2 y3
1 1 1




−
1 0 0
0 1 0
1 −1 1

 =


x1 − x3 x2 − x3 x3
y1 − y3 y2 − y3 y3
0 0 1

 .
206 CHAPTER 6. DIRECT SUMS
(6) Recall that a nonempty affine subspace A of R
n
is any nonempty subset of R
n
closed
under affine combinations. For any affine map f : R
n → R
m, for any affine subspace A of
R
n
, and any affine subspace B of R
m, prove that f(A) is an affine subspace of R
m, and that
f
−1
(B) is an affine subspace of R
n
.
Chapter 7
Determinants
In this chapter all vector spaces are defined over an arbitrary field K. For the sake of
concreteness, the reader may safely assume that K = R.
7.1 Permutations, Signature of a Permutation
This chapter contains a review of determinants and their use in linear algebra. We begin
with permutations and the signature of a permutation. Next, we define multilinear maps
and alternating multilinear maps. Determinants are introduced as alternating multilinear
maps taking the value 1 on the unit matrix (following Emil Artin). It is then shown how
to compute a determinant using the Laplace expansion formula, and the connection with
the usual definition is made. It is shown how determinants can be used to invert matrices
and to solve (at least in theory!) systems of linear equations (the Cramer formulae). The
determinant of a linear map is defined. We conclude by defining the characteristic polynomial
of a matrix (and of a linear map) and by proving the celebrated Cayley-Hamilton theorem
which states that every matrix is a “zero” of its characteristic polynomial (we give two proofs;
one computational, the other one more conceptual).
Determinants can be defined in several ways. For example, determinants can be defined
in a fancy way in terms of the exterior algebra (or alternating algebra) of a vector space.
We will follow a more algorithmic approach due to Emil Artin. No matter which approach
is followed, we need a few preliminaries about permutations on a finite set. We need to
show that every permutation on n elements is a product of transpositions, and that the
parity of the number of transpositions involved is an invariant of the permutation. Let
[n] = {1, 2 . . . , n}, where n ∈ N, and n > 0.
Definition 7.1. A permutation on n elements is a bijection π : [n] → [n]. When n = 1, the
only function from [1] to [1] is the constant map: 1 7→ 1. Thus, we will assume that n ≥ 2.
A transposition is a permutation τ : [n] → [n] such that, for some i < j (with 1 ≤ i < j ≤ n),
τ (i) = j, τ (j) = i, and τ (k) = k, for all k ∈ [n] − {i, j}. In other words, a transposition
exchanges two distinct elements i, j ∈ [n]. A cyclic permutation of order k (or k-cycle) is a
207
208 CHAPTER 7. DETERMINANTS
permutation σ : [n] → [n] such that, for some sequence (i1, i2, . . . , ik) of distinct elements of
[n] with 2 ≤ k ≤ n,
σ(i1) = i2, σ(i2) = i3, . . . , σ(ik−1) = ik, σ(ik) = i1,
and σ(j) = j, for j ∈ [n]− {i1, . . . , ik}. The set {i1, . . . , ik} is called the domain of the cyclic
permutation, and the cyclic permutation is usually denoted by (i1 i2 . . . ik).
If τ is a transposition, clearly, τ ◦ τ = id. Also, a cyclic permutation of order 2 is a
transposition, and for a cyclic permutation σ of order k, we have σ
k = id. Clearly, the
composition of two permutations is a permutation and every permutation has an inverse
which is also a permutation. Therefore, the set of permutations on [n] is a group often
denoted Sn. It is easy to show by induction that the group Sn has n! elements. We will
also use the terminology product of permutations (or transpositions), as a synonym for
composition of permutations.
A permutation σ on n elements, say σ(i) = ki
for i = 1, . . . , n, can be represented in
functional notation by the 2 × n array

1 · · · i · · · n
k1 · · · ki
· · · kn

known as Cauchy two-line notation. For example, we have the permutation σ denoted by

1 2 3 4 5 6
2 4 3 6 5 1 .
A more concise notation often used in computer science and in combinatorics is to rep￾resent a permutation by its image, namely by the sequence
σ(1) σ(2) · · · σ(n)
written as a row vector without commas separating the entries. The above is known as
the one-line notation. For example, in the one-line notation, our previous permutation σ is
represented by
2 4 3 6 5 1.
The reason for not enclosing the above sequence within parentheses is avoid confusion with
the notation for cycles, for which is it customary to include parentheses.
The following proposition shows the importance of cyclic permutations and transposi￾tions.
Proposition 7.1. For every n ≥ 2, for every permutation π : [n] → [n], there is a partition
of [n] into r subsets called the orbits of π, with 1 ≤ r ≤ n, where each set J in this partition
is either a singleton {i}, or it is of the form
J = {i, π(i), π2
(i), . . . , πri−1
(i)},
7.1. PERMUTATIONS, SIGNATURE OF A PERMUTATION 209
where ri
is the smallest integer, such that, π
ri (i) = i and 2 ≤ ri ≤ n. If π is not the identity,
then it can be written in a unique way (up to the order) as a composition π = σ1 ◦ . . . ◦ σs
of cyclic permutations with disjoint domains, where s is the number of orbits with at least
two elements. Every permutation π : [n] → [n] can be written as a nonempty composition of
transpositions.
Proof. Consider the relation Rπ defined on [n] as follows: iRπj iff there is some k ≥ 1 such
that j = π
k
(i). We claim that Rπ is an equivalence relation. Transitivity is obvious. We
claim that for every i ∈ [n], there is some least r (1 ≤ r ≤ n) such that π
r
(i) = i.
Indeed, consider the following sequence of n + 1 elements:
h
i, π(i), π2
(i), . . . , πn
(i)i .
Since [n] only has n distinct elements, there are some h, k with 0 ≤ h < k ≤ n such that
π
h
(i) = π
k
(i),
and since π is a bijection, this implies π
k−h
(i) = i, where 0 ≤ k − h ≤ n. Thus, we proved
that there is some integer m ≥ 1 such that π
m(i) = i, so there is such a smallest integer r.
Consequently, Rπ is reflexive. It is symmetric, since if j = π
k
(i), letting r be the least
r ≥ 1 such that π
r
(i) = i, then
i = π
kr(i) = π
k(r−1)(π
k
(i)) = π
k(r−1)(j).
Now, for every i ∈ [n], the equivalence class (orbit) of i is a subset of [n], either the singleton
{i} or a set of the form
J = {i, π(i), π2
(i), . . . , πri−1
(i)},
where ri
is the smallest integer such that π
ri (i) = i and 2 ≤ ri ≤ n, and in the second case,
the restriction of π to J induces a cyclic permutation σi
, and π = σ1 ◦ . . . ◦ σs, where s is the
number of equivalence classes having at least two elements.
For the second part of the proposition, we proceed by induction on n. If n = 2, there are
exactly two permutations on [2], the transposition τ exchanging 1 and 2, and the identity.
However, id2 = τ
2
. Now, let n ≥ 3. If π(n) = n, since by the induction hypothesis, the
restriction of π to [n − 1] can be written as a product of transpositions, π itself can be
written as a product of transpositions. If π(n) = k 6 = n, letting τ be the transposition such
that τ (n) = k and τ (k) = n, it is clear that τ ◦ π leaves n invariant, and by the induction
hypothesis, we have τ ◦ π = τm ◦ . . . ◦ τ1 for some transpositions, and thus
π = τ ◦ τm ◦ . . . ◦ τ1,
a product of transpositions (since τ ◦ τ = idn).
210 CHAPTER 7. DETERMINANTS
Remark: When π = idn is the identity permutation, we can agree that the composition of
0 transpositions is the identity. The second part of Proposition 7.1 shows that the transpo￾sitions generate the group of permutations Sn.
In writing a permutation π as a composition π = σ1 ◦ . . . ◦ σs of cyclic permutations, it
is clear that the order of the σi does not matter, since their domains are disjoint. Given
a permutation written as a product of transpositions, we now show that the parity of the
number of transpositions is an invariant.
Definition 7.2. For every n ≥ 2, since every permutation π : [n] → [n] defines a partition
of r subsets over which π acts either as the identity or as a cyclic permutation, let  (π),
called the signature of π, be defined by  (π) = (−1)n−r
, where r is the number of sets in the
partition.
If τ is a transposition exchanging i and j, it is clear that the partition associated with
τ consists of n − 1 equivalence classes, the set {i, j}, and the n − 2 singleton sets {k}, for
k ∈ [n] − {i, j}, and thus,  (τ ) = (−1)n−(n−1) = (−1)1 = −1.
Proposition 7.2. For every n ≥ 2, for every permutation π : [n] → [n], for every transpo￾sition τ , we have

(τ ◦ π) = − (π).
Consequently, for every product of transpositions such that π = τm ◦ . . . ◦ τ1, we have

(π) = (−1)m,
which shows that the parity of the number of transpositions is an invariant.
Proof. Assume that τ (i) = j and τ (j) = i, where i < j. There are two cases, depending
whether i and j are in the same equivalence class Jl of Rπ, or if they are in distinct equivalence
classes. If i and j are in the same class Jl
, then if
Jl = {i1, . . . , ip, . . . iq, . . . ik},
where ip = i and iq = j, since
τ (π(π
−1
(ip))) = τ (ip) = τ (i) = j = iq
and
τ (π(iq−1)) = τ (iq) = τ (j) = i = ip,
it is clear that Jl splits into two subsets, one of which is {ip, . . . , iq−1}, and thus, the number
of classes associated with τ ◦ π is r + 1, and  (τ ◦ π) = (−1)n−r−1 = −(−1)n−r = − (π). If i
and j are in distinct equivalence classes Jl and Jm, say
{i1, . . . , ip, . . . ih}
7.2. ALTERNATING MULTILINEAR MAPS 211
and
{j1, . . . , jq, . . . jk},
where ip = i and jq = j, since
τ (π(π
−1
(ip))) = τ (ip) = τ (i) = j = jq
and
τ (π(π
−1
(jq))) = τ (jq) = τ (j) = i = ip,
we see that the classes Jl and Jm merge into a single class, and thus, the number of classes
associated with τ ◦ π is r − 1, and  (τ ◦ π) = (−1)n−r+1 = −(−1)n−r = − (π).
Now, let π = τm ◦ . . . ◦ τ1 be any product of transpositions. By the first part of the
proposition, we have

(π) = (−1)m−1

(τ1) = (−1)m−1
(−1) = (−1)m,
since  (τ1) = −1 for a transposition.
Remark: When π = idn is the identity permutation, since we agreed that the composition
of 0 transpositions is the identity, it it still correct that (−1)0 =  (id) = +1. From the
proposition, it is immediate that  (π
0 ◦ π) =  (π
0 ) (π). In particular, since π
−1 ◦ π = idn, we
get  (π
−1
) =  (π).
We can now proceed with the definition of determinants.
7.2 Alternating Multilinear Maps
First we define multilinear maps, symmetric multilinear maps, and alternating multilinear
maps.
Remark: Most of the definitions and results presented in this section also hold when K is
a commutative ring and when we consider modules over K (free modules, when bases are
needed).
Let E1, . . . , En, and F, be vector spaces over a field K, where n ≥ 1.
Definition 7.3. A function f : E1 × . . . × En → F is a multilinear map (or an n-linear
map) if it is linear in each argument, holding the others fixed. More explicitly, for every i,
1 ≤ i ≤ n, for all x1 ∈ E1, . . ., xi−1 ∈ Ei−1, xi+1 ∈ Ei+1, . . ., xn ∈ En, for all x, y ∈ Ei
, for all
λ ∈ K,
f(x1, . . . , xi−1, x + y, xi+1, . . . , xn) = f(x1, . . . , xi−1, x, xi+1, . . . , xn)
+ f(x1, . . . , xi−1, y, xi+1, . . . , xn),
f(x1, . . . , xi−1, λx, xi+1, . . . , xn) = λf(x1, . . . , xi−1, x, xi+1, . . . , xn).
212 CHAPTER 7. DETERMINANTS
When F = K, we call f an n-linear form (or multilinear form). If n ≥ 2 and E1 =
E2 = . . . = En, an n-linear map f : E × . . . × E → F is called symmetric, if f(x1, . . . , xn) =
f(xπ(1), . . . , xπ(n)) for every permutation π on {1, . . . , n}. An n-linear map f : E×. . .×E → F
is called alternating, if f(x1, . . . , xn) = 0 whenever xi = xi+1 for some i, 1 ≤ i ≤ n − 1 (in
other words, when two adjacent arguments are equal). It does no harm to agree that when
n = 1, a linear map is considered to be both symmetric and alternating, and we will do so.
When n = 2, a 2-linear map f : E1 × E2 → F is called a bilinear map. We have already
seen several examples of bilinear maps. Multiplication ·: K × K → K is a bilinear map,
treating K as a vector space over itself.
The operation h−, −i: E
∗ × E → K applying a linear form to a vector is a bilinear map.
Symmetric bilinear maps (and multilinear maps) play an important role in geometry
(inner products, quadratic forms) and in differential calculus (partial derivatives).
A bilinear map is symmetric if f(u, v) = f(v, u), for all u, v ∈ E.
Alternating multilinear maps satisfy the following simple but crucial properties.
Proposition 7.3. Let f : E × . . . × E → F be an n-linear alternating map, with n ≥ 2. The
following properties hold:
(1)
f(. . . , xi
, xi+1, . . .) = −f(. . . , xi+1, xi
, . . .)
(2)
f(. . . , xi
, . . . , xj
, . . .) = 0,
where xi = xj
, and 1 ≤ i < j ≤ n.
(3)
f(. . . , xi
, . . . , xj
, . . .) = −f(. . . , xj
, . . . , xi
, . . .),
where 1 ≤ i < j ≤ n.
(4)
f(. . . , xi
, . . .) = f(. . . , xi + λxj
, . . .),
for any λ ∈ K, and where i 6 = j.
Proof. (1) By multilinearity applied twice, we have
f(. . . , xi + xi+1, xi + xi+1, . . .) = f(. . . , xi
, xi
, . . .) + f(. . . , xi
, xi+1, . . .)
+ f(. . . , xi+1, xi
, . . .) + f(. . . , xi+1, xi+1, . . .),
and since f is alternating, this yields
0 = f(. . . , xi
, xi+1, . . .) + f(. . . , xi+1, xi
, . . .),
7.2. ALTERNATING MULTILINEAR MAPS 213
that is, f(. . . , xi
, xi+1, . . .) = −f(. . . , xi+1, xi
, . . .).
(2) If xi = xj and i and j are not adjacent, we can interchange xi and xi+1, and then xi
and xi+2, etc, until xi and xj become adjacent. By (1),
f(. . . , xi
, . . . , xj
, . . .) = f(. . . , xi
, xj
, . . .),
where  = +1 or −1, but f(. . . , xi
, xj
, . . .) = 0, since xi = xj
, and (2) holds.
(3) follows from (2) as in (1). (4) is an immediate consequence of (2).
Proposition 7.3 will now be used to show a fundamental property of alternating multilin￾ear maps. First we need to extend the matrix notation a little bit. Let E be a vector space
over K. Given an n × n matrix A = (ai j ) over K, we can define a map L(A): E
n → E
n as
follows:
L(A)1(u) = a1 1u1 + · · · + a1 nun,
. . .
L(A)n(u) = an 1u1 + · · · + an nun,
for all u1, . . . , un ∈ E and with u = (u1, . . . , un). It is immediately verified that L(A) is
linear. Then given two n×n matrices A = (ai j ) and B = (bi j ), by repeating the calculations
establishing the product of matrices (just before Definition 3.12), we can show that
L(AB) = L(A) ◦ L(B).
It is then convenient to use the matrix notation to describe the effect of the linear map L(A),
as


L(A)1(u)
L(A)2(u)
.
.
L(A)
.
n(u)


=


a1 1 a1 2 . . . a1 n
a2 1 a2 2 . . . a2 n
.
.
.
.
.
.
.
.
.
.
.
.
an 1 an 2 . . . an n




u
u
1
2
.
.
