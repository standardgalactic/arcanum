rk+1,k+1 =
  u
00k+1
 , and e
iθk+1 |u
00k+1 · ek+1| = u
00k+1 · ek+1.
If u
00k+1 = e
iθk+1 rk+1,k+1 ek+1, we let hk+1 = id. Otherwise, by Proposition 14.19(1) (with
u = u
00k+1 and v = rk+1,k+1 ek+1), there is a unique hyperplane reflection hk+1 such that
hk+1(u
00k+1) = e
iθk+1 rk+1,k+1 ek+1,
where hk+1 is the reflection about the hyperplane Hk+1 orthogonal to the vector
wk+1 = rk+1,k+1 ek+1 − e
−iθk+1 u
00k+1.
At the end of the induction, we have a triangular matrix R, but the diagonal entries
e
iθj rj, j of R may be complex. Letting
hn = ρen, −θn ◦ · · · ◦ ρe1,−θ1
,
we observe that the diagonal entries of the matrix of vectors
r
0j = hn ◦ hn−1 ◦ · · · ◦ h2 ◦ h1(vj )
is triangular with nonnegative entries.
Remark: For numerical stability, it is preferable to use wk+1 = rk+1,k+1 ek+1 + e
−iθk+1 u
00k+1
instead of wk+1 = rk+1,k+1 ek+1 − e
−iθk+1 u
00k+1. The effect of that choice is that the diagonal
entries in R will be of the form −e
iθj rj, j = e
i(θj+π)
rj, j . Of course, we can make these entries
nonegative by applying
hn = ρen, π−θn ◦ · · · ◦ ρe1,π−θ1
after hn−1.
As in the Euclidean case, Proposition 14.20 immediately implies the QR-decomposition
for arbitrary complex n×n-matrices, where Q is now unitary (see Kincaid and Cheney [102]
and Ciarlet [41]).
Proposition 14.21. For every complex n × n-matrix A, there is a sequence H1, . . . , Hn−1
of matrices, where each Hi is either a Householder matrix or the identity, and an upper
triangular matrix R, such that
R = Hn−1 · · · H2H1A.
As a corollary, there is a pair of matrices Q, R, where Q is unitary and R is upper triangular,
such that A = QR (a QR-decomposition of A). Furthermore, R can be chosen so that its
diagonal entries are nonnegative. This can be achieved by a diagonal matrix D with entries
such that |dii| = 1 for i = 1, . . . , n, and we have A = QeRe with
Qe = H1 · · · Hn−1D, Re = D
∗R,
where Re is upper triangular and has nonnegative diagonal entries.
Proof. It is essentially identical to the proof of Proposition 13.4, and we leave the details as
an exercise. For the last statement, observe that hn ◦ · · · ◦ h1 is also an isometry.
14.6. ORTHOGONAL PROJECTIONS AND INVOLUTIONS 539
14.6 Orthogonal Projections and Involutions
In this section we begin by assuming that the field K is not a field of characteristic 2. Recall
that a linear map f : E → E is an involution iff f
2 = id, and is idempotent iff f
2 = f. We
know from Proposition 6.9 that if f is idempotent, then
E = Im(f) ⊕ Ker (f),
and that the restriction of f to its image is the identity. For this reason, a linear idempotent
map is called a projection. The connection between involutions and projections is given by
the following simple proposition.
Proposition 14.22. For any linear map f : E → E, we have f
2 = id iff 1
2
(id − f) is a
projection iff 2
1
(id + f) is a projection; in this case, f is equal to the difference of the two
projections 1
2
(id + f) and 1
2
(id − f).
Proof. We have

1
2
(id − f)

2
=
1
4
(id − 2f + f
2
)
so

1
2
(id − f)

2
=
1
2
(id − f) iff f
2 = id.
We also have

1
2
(id + f)

2
=
1
4
(id + 2f + f
2
),
so

1
2
(id + f)

2
=
1
2
(id + f) iff f
2 = id.
Obviously, f =
1
2
(id + f) −
1
2
(id − f).
Proposition 14.23. For any linear map f : E → E, let U
+ = Ker ( 1
2
(id − f)) and let
U
− = Im( 1
2
(id − f)). If f
2 = id, then
U
+ = Ker  1
2
(id − f)
 = Im 1
2
(id + f)
 ,
and so, f(u) = u on U
+ and f(u) = −u on U
−.
Proof. If f
2 = id, then
(id − f) ◦ (id + f) = id − f
2 = id − id = 0,
which implies that
Im 1
2
(id + f)
 ⊆ Ker  1
2
(id − f)
 .
540 CHAPTER 14. HERMITIAN SPACES
Conversely, if u ∈ Ker ￾ 1
2
(id − f)
 , then f(u) = u, so
1
2
(id + f)(u) = 1
2
(u + u) = u,
and thus
Ker  1
2
(id − f)
 ⊆ Im 1
2
(id + f)
 .
Therefore,
U
+ = Ker  1
2
(id − f)
 = Im 1
2
(id + f)
 ,
and so, f(u) = u on U
+ and f(u) = −u on U
−.
We now assume that K = C. The involutions of E that are unitary transformations are
characterized as follows.
Proposition 14.24. Let f ∈ GL(E) be an involution. The following properties are equiva￾lent:
(a) The map f is unitary; that is, f ∈ U(E).
(b) The subspaces U
− = Im( 2
1
(id − f)) and U
+ = Im( 1
2
(id + f)) are orthogonal.
Furthermore, if E is finite-dimensional, then (a) and (b) are equivalent to (c) below:
(c) The map is self-adjoint; that is, f = f
∗
.
Proof. If f is unitary, then from h f(u), f(v)i = h u, vi for all u, v ∈ E, we see that if u ∈ U
+
and v ∈ U
−, we get
h
u, vi = h f(u), f(v)i = h u, −vi = −hu, vi ,
so 2h u, vi = 0, which implies h u, vi = 0, that is, U
+ and U
− are orthogonal. Thus, (a)
implies (b).
Conversely, if (b) holds, since f(u) = u on U
+ and f(u) = −u on U
−, we see that
h
f(u), f(v)i = h u, vi if u, v ∈ U
+ or if u, v ∈ U
−. Since E = U
+ ⊕ U
− and since U
+ and U
−
are orthogonal, we also have h f(u), f(v)i = h u, vi for all u, v ∈ E, and (b) implies (a).
If E is finite-dimensional, the adjoint f
∗ of f exists, and we know that f
−1 = f
∗
. Since
f is an involution, f
2 = id, which implies that f
∗ = f
−1 = f.
A unitary involution is the identity on U
+ = Im( 2
1
(id + f)), and f(v) = −v for all
v ∈ U
− = Im( 1
2
(id−f)). Furthermore, E is an orthogonal direct sum E = U
+ ⊕U
−. We say
that f is an orthogonal reflection about U
+. In the special case where U
+ is a hyperplane,
we say that f is a hyperplane reflection. We already studied hyperplane reflections in the
Euclidean case; see Chapter 13.
14.6. ORTHOGONAL PROJECTIONS AND INVOLUTIONS 541
If f : E → E is a projection (f
2 = f), then
(id − 2f)
2 = id − 4f + 4f
2 = id − 4f + 4f = id,
so id − 2f is an involution. As a consequence, we get the following result.
Proposition 14.25. If f : E → E is a projection (f
2 = f), then Ker (f) and Im(f) are
orthogonal iff f
∗ = f.
Proof. Apply Proposition 14.24 to g = id − 2f. Since id − g = 2f we have
U
+ = Ker 
2
1
(id − g)
 = Ker (f)
and
U
− = Im 1
2
(id − g)
 = Im(f),
which proves the proposition.
A projection such that f = f
∗
is called an orthogonal projection.
If (a1 . . . , ak) are k linearly independent vectors in R
n
, let us determine the matrix P of
the orthogonal projection onto the subspace of R
n
spanned by (a1, . . . , ak). Let A be the
n×k matrix whose jth column consists of the coordinates of the vector aj over the canonical
basis (e1, . . . , en).
Any vector in the subspace (a1, . . . , ak) is a linear combination of the form Ax, for some
x ∈ R
k
. Given any y ∈ R
n
, the orthogonal projection P y = Ax of y onto the subspace
spanned by (a1, . . . , ak) is the vector Ax such that y − Ax is orthogonal to the subspace
spanned by (a1, . . . , ak) (prove it). This means that y − Ax is orthogonal to every aj
, which
is expressed by
A
> (y − Ax) = 0;
that is,
A
> Ax = A
> y.
The matrix A> A is invertible because A has full rank k, thus we get
x = (A
> A)
−1A
> y,
and so
P y = Ax = A(A
> A)
−1A
> y.
Therefore, the matrix P of the projection onto the subspace spanned by (a1 . . . , ak) is given
by
P = A(A
> A)
−1A
> .
The reader should check that P
2 = P and P
> = P.
542 CHAPTER 14. HERMITIAN SPACES
14.7 Dual Norms
In the remark following the proof of Proposition 9.10, we explained that if (E, k k ) and
(F, k k ) are two normed vector spaces and if we let L(E; F) denote the set of all continuous
(equivalently, bounded) linear maps from E to F, then, we can define the operator norm (or
subordinate norm) k k on L(E; F) as follows: for every f ∈ L(E; F),
k
fk = sup
x∈E
x6=0
k
f(x)k
k
xk
= sup
x∈E
k
xk =1
k
f(x)k .
In particular, if F = C, then L(E; F) = E
0 is the dual space of E, and we get the operator
norm denoted by k k ∗
given by
k
fk ∗ = sup
x∈E
k
xk =1
|f(x)|.
The norm k k ∗
is called the dual norm of k k on E
0 .
Let us now assume that E is a finite-dimensional Hermitian space, in which case E
0 = E
∗
.
Theorem 14.6 implies that for every linear form f ∈ E
∗
, there is a unique vector y ∈ E so
that
f(x) = h x, yi ,
for all x ∈ E, and so we can write
k
fk ∗ = sup
x∈E
k
xk =1
|hx, yi|.
The above suggests defining a norm k k D
on E.
Definition 14.13. If E is a finite-dimensional Hermitian space and k k is any norm on E,
for any y ∈ E we let
k
yk
D
= sup
x∈E
k
xk =1
|hx, yi|,
be the dual norm of k k (on E). If E is a real Euclidean space, then the dual norm is defined
by
k
yk
D
= sup
x∈E
k
xk =1
h
x, yi
for all y ∈ E.
Beware that k k is generally not the Hermitian norm associated with the Hermitian inner
product. The dual norm shows up in convex programming; see Boyd and Vandenberghe [29],
Chapters 2, 3, 6, 9.
The fact that k k D
is a norm follows from the fact that k k ∗
is a norm and can also be
checked directly. It is worth noting that the triangle inequality for k k D
comes “for free,” in
the sense that it holds for any function p: E → R.
14.7. DUAL NORMS 543
Proposition 14.26. For any function p: E → R, if we define p
D by
p
D(x) = sup
p(z)=1
|hz, xi|,
then we have
p
D(x + y) ≤ p
D(x) + p
D(y).
Proof. We have
p
D(x + y) = sup
p(z)=1
|hz, x + yi|
= sup
p(z)=1
(|hz, xi + h z, yi|)
≤ sup
p(z)=1
(|hz, xi| + |hz, yi|)
≤ sup
p(z)=1
|hz, xi| + sup
p(z)=1
|hz, yi|
= p
D(x) + p
D(y).
Definition 14.14. If p: E → R is a function such that
(1) p(x) ≥ 0 for all x ∈ E, and p(x) = 0 iff x = 0;
(2) p(λx) = |λ|p(x), for all x ∈ E and all λ ∈ C;
(3) p is continuous, in the sense that for some basis (e1, . . . , en) of E, the function
(x1, . . . , xn) 7→ p(x1e1 + · · · + xnen)
from C
n
to R is continuous,
then we say that p is a pre-norm.
Obviously, every norm is a pre-norm, but a pre-norm may not satisfy the triangle in￾equality.
Corollary 14.27. The dual norm of any pre-norm is actually a norm.
Proposition 14.28. For all y ∈ E, we have
k
yk
D
= sup
x∈E
k
xk =1
|hx, yi| = sup
x∈E
k
xk =1
<h
x, yi .
Proof. Since E is finite dimensional, the unit sphere S
n−1 = {x ∈ E | kxk = 1} is compact,
so there is some x0 ∈ S
n−1
such that
k
yk
D = |hx0, yi|.
544 CHAPTER 14. HERMITIAN SPACES
If h x0, yi = ρeiθ, with ρ ≥ 0, then
|he
−iθx0, yi| = |e
−iθh x0, yi| = |e
−iθρeiθ| = ρ,
so
k
yk
D = ρ = h e
−iθx0, yi , (∗)
with
  e
−iθx0

 = k x0k = 1. On the other hand,
<h
x, yi ≤ |hx, yi|,
so by (∗) we get
k
yk
D
= sup
x∈E
k
xk =1
|hx, yi| = sup
x∈E
k
xk =1
<h
x, yi ,
as claimed.
Proposition 14.29. For all x, y ∈ E, we have
|hx, yi| ≤ kxk k yk
D
|hx, yi| ≤ kxk
D
k
yk .
Proof. If x = 0, then h x, yi = 0 and these inequalities are trivial. If x 6 = 0, since k x/ k xkk = 1,
by definition of k yk
D
, we have
|hx/ k xk , yi| ≤ sup
k
zk =1
|hz, yi| = k yk
D
,
which yields
|hx, yi| ≤ kxk k yk
D
.
The second inequality holds because |hx, yi| = |hy, xi|.
It is not hard to show that for all y ∈ C
n
,
k
yk
D
1 = k yk ∞
k
yk
D
∞ = k yk 1
k
yk
D
2 = k yk 2
.
Thus, the Euclidean norm is autodual. More generally, the following proposition holds.
Proposition 14.30. If p, q ≥ 1 and 1/p + 1/q = 1, or p = 1 and q = ∞, or p = ∞ and
q = 1, then for all y ∈ C
n
, we have
k
yk
D
p = k yk q
.
14.7. DUAL NORMS 545
Proof. By H¨older’s inequality (Corollary 9.2), for all x, y ∈ C
n
, we have
|hx, yi| ≤ kxk p
k
yk q
,
so
k
yk
D
p = sup
x∈Cn
k
xk p=1
|hx, yi| ≤ kyk q
.
For the converse, we consider the cases p = 1, 1 < p < +∞, and p = +∞. First assume
p = 1. The result is obvious for y = 0, so assume y 6 = 0. Given y, if we pick xj = 1
for some index j such that k yk ∞ = max1≤i≤n |yi
| = |yj
|, and xk = 0 for k 6 = j, then
|hx, yi| = |yj
| = k yk ∞, so k yk
D
1 = k yk ∞.
Now we turn to the case 1 < p < +∞. Then we also have 1 < q < +∞, and the equation
1/p + 1/q = 1 is equivalent to pq = p + q, that is, p(q − 1) = q. Pick zj = yj
|yj
|
q−2
for
j = 1, . . . , n, so that
k
zk p =
 
nX
j=1
|zj
|
p
!
1/p
=
 
nX
j=1
|yj
|
(q−1)p
!
1/p
=
 
nX
j=1
|yj
|
q
!
1/p
.
Then if x = z/ k zk p
, we have
|hx, yi| =



P
n
j=1 zjyj

 
k
zk p
=



P
n
j=1 yjyj
|yj
|
q−2



k
zk p
=
P
n
j=1 |yj
|
q

P
n
j=1 |yj
|
q

1/p =
 
nX
j=1
|yj
|
q
!
1/q
= k yk q
.
Thus k yk
D
p = k yk q
.
Finally, if p = ∞, then pick xj = yj/|yj
| if yj 6 = 0, and xj = 0 if yj = 0. Then
|hx, yi| =


  

nX
yj6=0
yjyj/|yj
|





 =
X
yj6=0
|yj
| = k yk 1
.
Thus k yk
D
∞ = k yk 1
.
We can show that the dual of the spectral norm is the trace norm (or nuclear norm)
also discussed in Section 22.5. Recall from Proposition 9.10 that the spectral norm k Ak 2
of
a matrix A is the square root of the largest eigenvalue of A∗A, that is, the largest singular
value of A.
Proposition 14.31. The dual of the spectral norm is given by
k
Ak
D
2 = σ1 + · · · + σr,
where σ1 > · · · > σr > 0 are the singular values of A ∈ Mn(C) (which has rank r).
546 CHAPTER 14. HERMITIAN SPACES
Proof. In this case the inner product on Mn(C) is the Frobenius inner product h A, Bi =
tr(B∗A), and the dual norm of the spectral norm is given by
k
Ak
D
2 = sup{|tr(A
∗B)| | kBk 2 = 1}.
If we factor A using an SVD as A = V ΣU
∗
, where U and V are unitary and Σ is a diagonal
matrix whose r nonzero entries are the singular values σ1 > · · · > σr > 0, where r is the
rank of A, then
|tr(A
∗B)| = |tr(UΣV
∗B)| = |tr(ΣV
∗BU)|,
so if we pick B = V U∗
, a unitary matrix such that k Bk 2 = 1, we get
|tr(A
∗B)| = tr(Σ) = σ1 + · · · + σr,
and thus
k
Ak
D
2 ≥ σ1 + · · · + σr.
Since k Bk 2 = 1 and U and V are unitary, by Proposition 9.10 we have k V
∗BUk 2 =
k
Bk 2 = 1. If Z = V
∗BU, by definition of the operator norm
1 = k Zk 2 = sup{kZxk 2
| kxk 2 = 1},
so by picking x to be the canonical vector ej
, we see that k Z
jk
2 ≤ 1 where Z
j
is the jth
column of Z, so |zjj | ≤ 1, and since
|tr(ΣV
∗BU)| = |tr(ΣZ)| =


 

rX
j=1
σjzjj


 
 ≤
rX
j=1
σj
|zjj | ≤
rX
j=1
σj
,
and we conclude that
|tr(ΣV
∗BU)| ≤
rX
j=1
σj
.
The above implies that
k
Ak
D
2 ≤ σ1 + · · · + σr,
and since we also have k Ak
D
2 ≥ σ1 + · · · + σr, we conclude that
k
Ak
D
2 = σ1 + · · · + σr,
proving our proposition.
Definition 14.15. Given any complex matrix n × n matrix A of rank r, its nuclear norm
(or trace norm) is given by
k
Ak N = σ1 + · · · + σr.
14.7. DUAL NORMS 547
The nuclear norm can be generalized to m × n matrices (see Section 22.5). The nuclear
norm σ1 + · · · + σr of an m × n matrix A (where r is the rank of A) is denoted by k Ak N
.
The nuclear norm plays an important role in matrix completion. The problem is this. Given
a matrix A0 with missing entries (missing data), one would like to fill in the missing entries
in A0 to obtain a matrix A of minimal rank. For example, consider the matrices
A0 =

∗ ∗
1 2 , B0 =

∗
1 ∗
4

, C0 =

1 2
3 ∗

.
All can be completed with rank 1. For A0, use any multiple of (1, 2) for the second row. For
B0, use any numbers b and c such that bc = 4. For C0, the only possibility is d = 6.
A famous example of this problem is the Netflix competition. The ratings of m films by
n viewers goes into A0. But the customers didn’t see all the movies. Many ratings were
missing. Those had to be predicted by a recommender system. The nuclear norm gave a
good solution that needed to be adjusted for human psychology.
Since the rank of a matrix is not a norm, in order to solve the matrix completion problem
we can use the following “convex relaxation.” Let A0 be an incomplete m × n matrix:
Minimize k Ak N
subject to A = A0 in the known entries.
The above problem has been extensively studied, in particular by Cand`es and Recht.
Roughly, they showed that if A is an n × n matrix of rank r and K entries are known in
A, then if K is large enough (K > Cn5/4
r log n), with high probability, the recovery of A is
perfect. See Strang [171] for details (Section III.5).
We close this section by stating the following duality theorem.
Theorem 14.32. If E is a finite-dimensional Hermitian space, then for any norm k k on
E, we have
k
yk
DD = k yk
for all y ∈ E.
Proof. By Proposition 14.29, we have
|hx, yi| ≤ kxk
D
k
yk ,
so we get
k
yk
DD
= sup
k
xk
D=1
|hx, yi| ≤ kyk , for all y ∈ E.
It remains to prove that
k
yk ≤ kyk
DD
, for all y ∈ E.
Proofs of this fact can be found in Horn and Johnson [95] (Section 5.5), and in Serre [156]
(Chapter 7). The proof makes use of the fact that a nonempty, closed, convex set has a
supporting hyperplane through each of its boundary points, a result known as Minkowski’s
548 CHAPTER 14. HERMITIAN SPACES
lemma. For a geometric interpretation of supporting hyperplane see Figure 14.1. This result
is a consequence of the Hahn–Banach theorem; see Gallier [72]. We give the proof in the
case where E is a real Euclidean space. Some minor modifications have to be made when
dealing with complex vector spaces and are left as an exercise.
x
Figure 14.1: The orange tangent plane is a supporting hyperplane to the unit ball in R
3
since this ball is entirely contained in “one side” of the tangent plane.
Since the unit ball B = {z ∈ E | kzk ≤ 1} is closed and convex, the Minkowski lemma
says for every x such that k xk = 1, there is an affine map g of the form
g(z) = h z, wi − hx, wi
with k wk = 1, such that g(x) = 0 and g(z) ≤ 0 for all z such that k zk ≤ 1. Then it is clear
that
sup
k
zk =1
h
z, wi = h x, wi ,
and so
k
wk
D = h x, wi .
It follows that
k
xk
DD ≥ hw/ k wk
D
, xi =
h
x, wi
k
wk
D = 1 = k xk
for all x such that k xk = 1. By homogeneity, this is true for all y ∈ E, which completes the
proof in the real case. When E is a complex vector space, we have to view the unit ball B
as a closed convex set in R
2n and we use the fact that there is real affine map of the form
g(z) = <h z, wi − <hx, wi
such that g(x) = 0 and g(z) ≤ 0 for all z with k zk = 1, so that k wk
D = <h x, wi .
More details on dual norms and unitarily invariant norms can be found in Horn and
Johnson [95] (Chapters 5 and 7).
14.8. SUMMARY 549
14.8 Summary
The main concepts and results of this chapter are listed below:
• Semilinear maps.
• Sesquilinear forms; Hermitian forms.
• Quadratic form associated with a sesquilinear form.
• Polarization identities.
• Positive and positive definite Hermitian forms; pre-Hilbert spaces, Hermitian spaces.
• Gram matrix associated with a Hermitian product.
• The Cauchy–Schwarz inequality and the Minkowski inequality.
• Hermitian inner product, Hermitian norm.
• The parallelogram law.
• The musical isomorphisms [ : E → E
∗ and ] : E
∗ → E; Theorem 14.6 (E is finite￾dimensional).
• The adjoint of a linear map (with respect to a Hermitian inner product).
• Existence of orthonormal bases in a Hermitian space (Proposition 14.11).
• Gram–Schmidt orthonormalization procedure.
• Linear isometries (unitary transformations).
• The unitary group, unitary matrices.
• The unitary group U(n).
• The special unitary group SU(n).
• QR-Decomposition for arbitrary complex matrices.
• The Hadamard inequality for complex matrices.
• The Hadamard inequality for Hermitian positive semidefinite matrices.
• Orthogonal projections and involutions; orthogonal reflections.
• Dual norms.
• Nuclear norm (also called trace norm).
• Matrix completion.
550 CHAPTER 14. HERMITIAN SPACES
14.9 Problems
Problem 14.1. Let (E,h−, −i) be a Hermitian space of finite dimension. Prove that if
f : E → E is a self-adjoint linear map (that is, f
∗ = f), then h f(x), xi ∈ R for all x ∈ E.
Problem 14.2. Prove the polarization identities of Proposition 14.1.
Problem 14.3. Let E be a real Euclidean space. Give an example of a nonzero linear map
f : E → E such that h f(u), ui = 0 for all u ∈ E.
Problem 14.4. Prove Proposition 14.9.
Problem 14.5. (1) Prove that every matrix in SU(2) is of the form
A =

−
a
c
+
+
ib c
id a
+
−
id
ib , a2 + b
2 + c
2 + d
2 = 1, a, b, c, d ∈ R,
(2) Prove that the matrices

1 0
0 1 ,

0
i
−
0
i

,

−
0 1
1 0 ,

0
i 0
i

all belong to SU(2) and are linearly independent over C.
(3) Prove that the linear span of SU(2) over C is the complex vector space M2(C) of all
complex 2 × 2 matrices.
Problem 14.6. The purpose of this problem is to prove that the linear span of SU(n) over
C is Mn(C) for all n ≥ 3. One way to prove this result is to adapt the method of Problem
12.12, so please review this problem.
Every complex matrix A ∈ Mn(C) can be written as
A =
A + A∗
2
+
A − A∗
2
where the first matrix is Hermitian and the second matrix is skew-Hermitian. Observe that
if A = (zij ) is a Hermitian matrix, that is A∗ = A, then zji = zij , so if zij = aij + ibij with
aij , bij ∈ R, then aij = aji and bij = −bji. On the other hand, if A = (zij ) is a skew-Hermitian
