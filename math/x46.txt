The following convergence result is proven in Ciarlet [41] (Chapter 6, Theorem 6.3.10
and Serre [156] (Chapter 13, Theorem 13.2). It is rarely applicable in practice, except for
symmetric (or Hermitian) positive definite matrices, as we will see shortly.
Theorem 18.1. Suppose the (complex) nÃ—n matrix A is invertible, diagonalizable, and that
its eigenvalues Î»1, . . . , Î»n have different moduli, so that
|Î»1| > |Î»2| > Â· Â· Â· > |Î»n| > 0.
If A = PÎ›P
âˆ’1
, where Î› = diag(Î»1, . . . , Î»n), and if P
âˆ’1 has an LU-factorization, then the
strictly lower-triangular part of Ak converges to zero, and the diagonal of Ak converges to Î›.
Proof. We reproduce the proof in Ciarlet [41] (Chapter 6, Theorem 6.3.10). The strategy is
to study the asymptotic behavior of the matrices Pk = Q1Q2 Â· Â· Â· Qk. For this, it turns out
that we need to consider the powers Ak
.
Step 1 . Let Rk = Rk Â· Â· Â· R2R1. We claim that
A
k = (Q1Q2 Â· Â· Â· Qk)(Rk Â· Â· Â· R2R1) = PkRk. (âˆ—3)
18.1. THE BASIC QR ALGORITHM 651
We proceed by induction. The base case k = 1 is trivial. For the induction step, from
(âˆ—2), we have
PkAk+1 = APk.
Since Ak+1 = RkQk = Qk+1Rk+1, we have
Pk+1Rk+1 = PkQk+1Rk+1Rk = PkAk+1Rk = APkRk = AAk = A
k+1
establishing the induction step.
Step 2 . We will express the matrix Pk as Pk = QQekDk, in terms of a diagonal matrix
Dk with unit entries, with Q and Qek unitary.
Let P = QR, a QR-factorization of P (with R an upper triangular matrix with positive
diagonal entries), and P
âˆ’1 = LU, an LU-factorization of P
âˆ’1
. Since A = PÎ›P
âˆ’1
, we have
A
k = PÎ›
kP
âˆ’1 = QRÎ›
kLU = QR(Î›kLÎ›
âˆ’k
)Î›kU. (âˆ—4)
Here, Î›âˆ’k
is the diagonal matrix with entries Î»
âˆ’
i
k
. The reason for introducing the matrix
Î›
kLÎ›
âˆ’k
is that its asymptotic behavior is easy to determine. Indeed, we have
(Î›kLÎ›
âˆ’k
)ij =
ï£±
ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£³
0 if i < j
1 if i = j

Î»i
Î»j

k
Lij if i > j.
The hypothesis that |Î»1| > |Î»2| > Â· Â· Â· > |Î»n| > 0 implies that
lim
k7â†’âˆž
Î›
kLÎ›
âˆ’k = I. (â€ )
Note that it is to obtain this limit that we made the hypothesis on the moduli of the
eigenvalues. We can write
Î›
kLÎ›
âˆ’k = I + Fk, with lim
k7â†’âˆž
Fk = 0,
and consequently, since R(Î›kLÎ›
âˆ’k
) = R(I + Fk) = R + RFkRâˆ’1R = (I + RFkRâˆ’1
)R, we
have
R(Î›kLÎ›
âˆ’k
) = (I + RFkR
âˆ’1
)R. (âˆ—5)
By Proposition 9.11(1), since limk7â†’âˆž Fk = 0, and thus limk7â†’âˆž RFkRâˆ’1 = 0, the matrices
I + RFkRâˆ’1 are invertible for k large enough. Consequently for k large enough, we have a
QR-factorization
I + RFkR
âˆ’1 = QekRek, (âˆ—6)
with ( eRk)ii > 0 for i = 1, . . . , n. Since the matrices e Qk are unitary, we have
   Qek


2
= 1,
so the sequence (Qek) is bounded. It follows that it has a convergent subsequence (Qe` ) that
converges to some matrix Qe, which is also unitary. Since
Re` = (Qe` )
âˆ—
(I + RF` R
âˆ’1
),
652 CHAPTER 18. COMPUTING EIGENVALUES AND EIGENVECTORS
we deduce that the subsequence (Re` ) also converges to some matrix Re, which is also upper
triangular with positive diagonal entries. By passing to the limit (using the subsequences),
we get Re = (Qe)
âˆ—
, that is,
I = QeR. e
By the uniqueness of a QR-decomposition (when the diagonal entries of R are positive), we
get
Qe = Re = I.
Since the above reasoning applies to any subsequences of (Qek) and (Rek), by the uniqueness
of limits, we conclude that the â€œfullâ€ sequences (Qek) and (Rek) converge:
lim
k7â†’âˆž
Qek = I, lim
k7â†’âˆž
Rek = I.
Since by (âˆ—4),
A
k = QR(Î›kLÎ›
âˆ’k
)Î›kU,
by (âˆ—5),
R(Î›kLÎ›
âˆ’k
) = (I + RFkR
âˆ’1
)R,
and by (âˆ—6)
I + RFkR
âˆ’1 = QekRek,
we proved that
A
k = (QQek)(RekRÎ›
kU). (âˆ—7)
Observe that QQek is a unitary matrix, and RekRÎ›
kU is an upper triangular matrix, as a
product of upper triangular matrices. However, some entries in Î› may be negative, so
we canâ€™t claim that RekRÎ›
kU has positive diagonal entries. Nevertheless, we have another
QR-decomposition of Ak
,
A
k = (Q eQk)( eRkRÎ›
kU) = PkRk.
It is easy to prove that there is diagonal matrix Dk with |(Dk)ii| = 1 for i = 1, . . . , n, such
that
Pk = QQekDk. (âˆ—8)
The existence of Dk is consequence of the following fact: If an invertible matrix B has two
QR factorizations B = Q1R1 = Q2R2, then there is a diagonal matrix D with unit entries
such that Q2 = DQ1.
The expression for Pk in (âˆ—8) is that which we were seeking.
Step 3 . Asymptotic behavior of the matrices Ak+1 = Pk
âˆ—APk.
Since A = PÎ›P
âˆ’1 = QRÎ›Râˆ’1Qâˆ’1 and by (âˆ—8), Pk = QQekDk, we get
Ak+1 = Dk
âˆ—
(Qek)
âˆ—Q
âˆ—QRÎ›R
âˆ’1Q
âˆ’1QQekDk = Dk
âˆ—
(Qek)
âˆ—RÎ›R
âˆ’1QekDk. (âˆ—9)
18.1. THE BASIC QR ALGORITHM 653
Since limk7â†’âˆž Qek = I, we deduce that
lim
k7â†’âˆž
(Qek)
âˆ—RÎ›R
âˆ’1Qek = RÎ›R
âˆ’1 =
ï£«
ï£¬ï£¬ï£¬ï£­
Î»
0
.
.
1
Î»
âˆ— Â· Â· Â· âˆ—
2 Â· Â· Â· âˆ—
.
.
.
.
.
.
.
0 0 Â· Â· Â· Î»n
ï£¶
ï£·ï£·ï£·ï£¸
,
an upper triangular matrix with the eigenvalues of A on the diagonal. Since R is upper
triangular, the order of the eigenvalues is preserved. If we let
Dk = (Qek)
âˆ—RÎ›R
âˆ’1Qek, (âˆ—10)
then by (âˆ—9) we have Ak+1 = Dk
âˆ—DkDk, and since the matrices Dk are diagonal matrices, we
have
(Ak+1)jj = (Dk
âˆ—DkDk)ij = (Dk)ii(Dk)jj (Dk)ij ,
which implies that
(Ak+1)ii = (Dk)ii, i = 1, . . . , n, (âˆ—11)
since |(Dk)ii| = 1 for i = 1, . . . , n. Since limk7â†’âˆž Dk = RÎ›Râˆ’1
, we conclude that the strictly
lower-triangular part of Ak+1 converges to zero, and the diagonal of Ak+1 converges to Î›.
Observe that if the matrix A is real, then the hypothesis that the eigenvalues have distinct
moduli implies that the eigenvalues are all real and simple.
The following Matlab program implements the basic QR-method using the function qrv4
from Section 12.8.
function T = qreigen(A,m)
T = A;
for k = 1:m
[Q R] = qrv4(T);
T = R*Q;
end
end
Example 18.1. If we run the function qreigen with 100 iterations on the 8 Ã— 8 symmetric
matrix
A =
ï£«
ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­
4 1 0 0 0 0 0 0
1 4 1 0 0 0 0 0
0 1 4 1 0 0 0 0
0 0 1 4 1 0 0 0
0 0 0 1 4 1 0 0
0 0 0 0 1 4 1 0
0 0 0 0 0 1 4 1
0 0 0 0 0 0 1 4
ï£¶
ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£¸
,
654 CHAPTER 18. COMPUTING EIGENVALUES AND EIGENVECTORS
we find the matrix
T =
ï£«
ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­
5
0
.
.
8794 0
0015 5
.
.
0015 0
5321 0
.
.
0000
0001 0
âˆ’0
.0000
.0000 0
âˆ’0
.0000
.0000 0
âˆ’0
.0000 0
.0000 0.
.
0000
0000 0
âˆ’0
.0000
.0000
0 0.0001 5.0000 0.0000 âˆ’0.0000 0.0000 0.0000 0.0000
0 0 0
0 0 0 0
.0000 4.
.
3473 0
0000 3
.
.
0000 0
6527 0
.
.
0000 0
0000 0
.
.
0000 0
0000 âˆ’0
.0000
.0000
0 0 0 0 0.0000 3.0000 0.0000 âˆ’0.0000
0 0 0 0 0 0
0 0 0 0 0 0 0
.0000 2.
.
4679 0
0000 2
.
.
0000
1206
ï£¶
ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£¸
.
The diagonal entries match the eigenvalues found by running the Matlab function eig(A).
If several eigenvalues have the same modulus, then the proof breaks down, we can no
longer claim (â€ ), namely that
lim
k7â†’âˆž
Î›
kLÎ›
âˆ’k = I.
If we assume that P
âˆ’1 has a suitable â€œblock LU-factorization,â€ it can be shown that the
matrices Ak+1 converge to a block upper-triangular matrix, where each block corresponds to
eigenvalues having the same modulus. For example, if A is a 9 Ã— 9 matrix with eigenvalues
Î»i such that |Î»1| = |Î»2| = |Î»3| > |Î»4| > |Î»5| = |Î»6| = |Î»7| = |Î»8| = |Î»9|, then Ak converges to
a block diagonal matrix (with three blocks, a 3 Ã— 3 block, a 1 Ã— 1 block, and a 5 Ã— 5 block)
of the form
ï£«
ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­
? ? ?
? ? ?
âˆ— âˆ— âˆ— âˆ— âˆ— âˆ—
âˆ— âˆ— âˆ— âˆ— âˆ— âˆ—
? ? ?
âˆ— âˆ— âˆ— âˆ— âˆ— âˆ—
0 0 0 ? âˆ— âˆ— âˆ— âˆ— âˆ—
0 0 0 0
0 0 0 0
? ? ? ? ? ? ? ? ? ?
0 0 0 0
0 0 0 0
0 0 0 0
? ? ? ? ?
? ? ? ? ? ? ? ? ? ?
ï£¶
ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£¸
.
See Ciarlet [41] (Chapter 6 Section 6.3) for more details.
Under the conditions of Theorem 18.1, in particular, if A is a symmetric (or Hermitian)
positive definite matrix, the eigenvectors of A can be approximated. However, when A is
not a symmetric matrix, since the upper triangular part of Ak does not necessarily converge,
one has to be cautious that a rigorous justification is lacking.
Suppose we apply the QR algorithm to a matrix A satisfying the hypotheses of Theorem
Theorem 18.1. For k large enough, Ak+1 = Pk
âˆ—APk is nearly upper triangular and the
diagonal entries of Ak+1 are all distinct, so we can consider that they are the eigenvalues of
Ak+1, and thus of A. To avoid too many subscripts, write T for the upper triangular matrix
18.2. HESSENBERG MATRICES 655
obtained by settting the entries of the part of Ak+1 below the diagonal to 0. Then we can
find the corresponding eigenvectors by solving the linear system
T v = tiiv,
and since T is upper triangular, this can be done by bottom-up elimination. We leave it as
an exercise to show that the following vectors v
i = (v1
i
, . . . , vn
i
) are eigenvectors:
v
1 = e1,
and if i = 2, . . . , n, then
v
i
j =
ï£±
ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£³
0 if i + 1 â‰¤ j â‰¤ n
1 if j = i
âˆ’
tjj+1vj
i
+1 + Â· Â· Â· + tjivi
i
tjj âˆ’ tii
if i âˆ’ 1 â‰¥ j â‰¥ 1.
Then the vectors (Pkv
1
, . . . , Pkv
n
) are a basis of (approximate) eigenvectors for A. In the
special case where T is a diagonal matrix, then v
i = ei
for i = 1, . . . , n and the columns of
Pk are an orthonormal basis of (approximate) eigenvectors for A.
If A is a real matrix whose eigenvalues are not all real, then there is some complex pair of
eigenvalues Î» + iÂµ (with Âµ 6 = 0), and the QR-algorithm cannot converge to a matrix whose
strictly lower-triangular part is zero. There is a way to deal with this situation using upper
Hessenberg matrices which will be discussed in the next section.
Since the convergence of the QR method depends crucially only on the fact that the part
of Ak below the diagonal goes to zero, it would be highly desirable if we could replace A
by a similar matrix U
âˆ—AU easily computable from A having lots of zero strictly below the
diagonal. We canâ€™t expect U
âˆ—AU to be a diagonal matrix (since this would mean that A was
easily diagonalized), but it turns out that there is a way to construct a matrix H = U
âˆ—AU
which is almost triangular, except that it may have an extra nonzero diagonal below the
main diagonal. Such matrices called Hessenberg matrices are discussed in the next section.
18.2 Hessenberg Matrices
Definition 18.1. An n Ã— n matrix (real or complex) H is an (upper) Hessenberg matrix if
it is almost triangular, except that it may have an extra nonzero diagonal below the main
diagonal. Technically, hjk = 0 for all (j, k) such that j âˆ’ k â‰¥ 2.
The 5 Ã— 5 matrix below is an example of a Hessenberg matrix.
H =
ï£«
ï£¬ï£¬ï£¬ï£¬ï£­
h
âˆ— âˆ— âˆ— âˆ— âˆ—
21 âˆ— âˆ— âˆ— âˆ—
0 h32 âˆ— âˆ— âˆ—
0 0
0 0 0
h43
h
âˆ— âˆ—
54 âˆ—
ï£¶
ï£·ï£·ï£·ï£·ï£¸
.
656 CHAPTER 18. COMPUTING EIGENVALUES AND EIGENVECTORS
The following result can be shown.
Theorem 18.2. Every n Ã— n complex or real matrix A is similar to an upper Hessenberg
matrix H, that is, A = UHUâˆ—
for some unitary matrix U. Furthermore, U can be constructed
as a product of Householder matrices (the definition is the same as in Section 13.1, except
that W is a complex vector, and that the inner product is the Hermitian inner product on
C
n
). If A is a real matrix, then U is an orthogonal matrix (and H is a real matrix).
Theorem 18.2 and algorithms for converting a matrix to Hessenberg form are discussed
in Trefethen and Bau [176] (Lecture 26), Demmel [48] (Section 4.4.6, in the real case), Serre
[156] (Theorem 13.1), and Meyer [125] (Example 5.7.4, in the real case). The proof of
correctness is not difficult and will be the object of a homework problem.
The following functions written in Matlab implement a function to compute a Hessenberg
form of a matrix.
The function house constructs the normalized vector u defining the Householder reflection
that zeros all but the first entries in a vector x.
function [uu, u] = house(x)
tol = 2*10^(-15); % tolerance
uu = x;
p = size(x,1);
% computes l^1-norm of x(2:p,1)
n1 = sum(abs(x(2:p,1)));
if n1 <= tol
u = zeros(p,1); uu = u;
else
l = sqrt(xâ€™*x); % l^2 norm of x
uu(1) = x(1) + signe(x(1))*l;
u = uu/sqrt(uuâ€™*uu);
end
end
The function signe(z) returms âˆ’1 if z < 0, else +1.
The function buildhouse builds a Householder reflection from a vector uu.
function P = buildhouse(v,i)
% This function builds a Householder reflection
% [I 0 ]
% [0 PP]
% from a Householder reflection
% PP = I - 2uu*uuâ€™
% where uu = v(i:n)
18.2. HESSENBERG MATRICES 657
% If uu = 0 then P - I
%
n = size(v,1);
if v(i:n) == zeros(n - i + 1,1)
P = eye(n);
else
PP = eye(n - i + 1) - 2*v(i:n)*v(i:n)â€™;
P = [eye(i-1) zeros(i-1, n - i + 1); zeros(n - i + 1, i - 1) PP];
end
end
The function Hessenberg1 computes an upper Hessenberg matrix H and an orthogonal
matrix Q such that A = Q> HQ.
function [H, Q] = Hessenberg1(A)
%
% This function constructs an upper Hessenberg
% matrix H and an orthogonal matrix Q such that
% A = Qâ€™ H Q
n = size(A,1);
H = A;
Q = eye(n);
for i = 1:n-2
% H(i+1:n,i)
[~,u] = house(H(i+1:n,i));
% u
P = buildhouse(u,1);
Q(i+1:n,i:n) = P*Q(i+1:n,i:n);
H(i+1:n,i:n) = H(i+1:n,i:n) - 2*u*(uâ€™)*H(i+1:n,i:n);
H(1:n,i+1:n) = H(1:n,i+1:n) - 2*H(1:n,i+1:n)*u*(uâ€™);
end
end
Example 18.2. If
A =
ï£«
ï£¬ï£¬ï£­
1 2 3 4
2 3 4 5
3 4 5 6
4 5 6 7
ï£¶
ï£·ï£·ï£¸ ,
658 CHAPTER 18. COMPUTING EIGENVALUES AND EIGENVECTORS
running Hessenberg1 we find
H =
ï£«
ï£¬ï£¬ï£­
1.0000 âˆ’5.3852 0 0
âˆ’
âˆ’
5
0
.
.
3852 15
0000 âˆ’1
.
.
2069
6893
âˆ’
âˆ’
1
0
.
.
6893
2069
âˆ’
âˆ’
0
0
.
.
0000
0000
0 âˆ’0.0000 0.0000 0.0000
ï£¶
ï£·ï£·ï£¸
Q =
ï£«
ï£¬ï£¬ï£­
1.0000 0 0 0
0
0 0
âˆ’0
.8339 0
.3714 âˆ’0
.1516
.5571 âˆ’
âˆ’
0
0
.
.
7428
5307
0 0.4082 âˆ’0.8165 0.4082
ï£¶
ï£·ï£·ï£¸ .
An important property of (upper) Hessenberg matrices is that if some subdiagonal entry
Hp+1p = 0, then H is of the form
H =

H11 H12
0 H22
,
where both H11 and H22 are upper Hessenberg matrices (with H11 a p Ã— p matrix and H22 a
(n âˆ’ p) Ã— (n âˆ’ p) matrix), and the eigenvalues of H are the eigenvalues of H11 and H22. For
example, in the matrix
H =
ï£«
ï£¬ï£¬ï£¬ï£¬ï£­
h
âˆ— âˆ— âˆ— âˆ— âˆ—
21 âˆ— âˆ— âˆ— âˆ—
0 h32 âˆ— âˆ— âˆ—
0 0
0 0 0
h43
h
âˆ— âˆ—
54 âˆ—
ï£¶
ï£·ï£·ï£·ï£·ï£¸
,
if h43 = 0, then we have the block matrix
H =
ï£«
ï£¬ï£¬ï£¬ï£¬ï£­
h
âˆ— âˆ— âˆ— âˆ— âˆ—
21 âˆ— âˆ— âˆ— âˆ—
0 h32 âˆ— âˆ— âˆ—
0 0 0
0 0 0 h
âˆ— âˆ—
54 âˆ—
ï£¶
ï£·ï£·ï£·ï£·ï£¸
.
Then the list of eigenvalues of H is the concatenation of the list of eigenvalues of H11 and
the list of the eigenvalues of H22. This is easily seen by induction on the dimension of the
block H11.
More generally, every upper Hessenberg matrix can be written in such a way that it has
diagonal blocks that are Hessenberg blocks whose subdiagonal is not zero.
Definition 18.2. An upper Hessenberg n Ã— n matrix H is unreduced if hi+1i 6 = 0 for i =
1, . . . , n âˆ’ 1. A Hessenberg matrix which is not unreduced is said to be reduced.
18.2. HESSENBERG MATRICES 659
The following is an example of an 8 Ã— 8 matrix consisting of three diagonal unreduced
Hessenberg blocks:
H =
ï£«
ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­
? ? ?
âˆ— âˆ— âˆ— âˆ— âˆ—
h21 ? ? âˆ— âˆ— âˆ— âˆ— âˆ—
0 h32 ? âˆ— âˆ— âˆ— âˆ— âˆ—
0 0 0 ? ? ? âˆ— âˆ—
0 0 0 h54 ? ? âˆ— âˆ—
0 0 0 0 h65 ? âˆ— âˆ—
0 0 0 0 0 0
0 0 0 0 0 0 h
? ?87 ?
ï£¶
ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£¸
.
An interesting and important property of unreduced Hessenberg matrices is the following.
Proposition 18.3. Let H be an n Ã— n complex or real unreduced Hessenberg matrix. Then
every eigenvalue of H is geometrically simple, that is, dim(EÎ») = 1 for every eigenvalue Î»,
where EÎ» is the eigenspace associated with Î». Furthermore, if H is diagonalizable, then every
eigenvalue is simple, that is, H has n distinct eigenvalues.
Proof. We follow Serreâ€™s proof [156] (Proposition 3.26). Let Î» be any eigenvalue of H, let
M = Î»In âˆ’ H, and let N be the (n âˆ’ 1) Ã— (n âˆ’ 1) matrix obtained from M by deleting its
first row and its last column. Since H is upper Hessenberg, N is a diagonal matrix with
entries âˆ’hi+1i 6 = 0, i = 1, . . . , n âˆ’ 1. Thus N is invertible and has rank n âˆ’ 1. But a matrix
has rank greater than or equal to the rank of any of its submatrices, so rank(M) = n âˆ’ 1,
since M is singular. By the rank-nullity theorem, rank(Ker N) = 1, that is, dim(EÎ») = 1, as
claimed.
If H is diagonalizable, then the sum of the dimensions of the eigenspaces is equal to n,
which implies that the eigenvalues of H are distinct.
As we said earlier, a case where Theorem 18.1 applies is the case where A is a symmetric
(or Hermitian) positive definite matrix. This follows from two facts.
The first fact is that if A is Hermitian (or symmetric in the real case), then it is easy
to show that the Hessenberg matrix similar to A is a Hermitian (or symmetric in real case)
tridiagonal matrix . The conversion method is also more efficient. Here is an example of a
symmetric tridiagonal matrix consisting of three unreduced blocks:
H =
ï£«
ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­
Î±1 Î²1 0 0 0 0 0 0
Î²1 Î±2 Î²2 0 0 0 0 0
0 Î²2 Î±3 0 0 0 0 0
0 0 0 Î±4 Î²4 0 0 0
0 0 0 Î²4 Î±5 Î²5 0 0
0 0 0 0 Î²5 Î±6 0 0
0 0 0 0 0 0 Î±7 Î²7
0 0 0 0 0 0 Î²7 Î±8
ï£¶
ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£¸
.
660 CHAPTER 18. COMPUTING EIGENVALUES AND EIGENVECTORS
Thus the problem of finding the eigenvalues of a symmetric (or Hermitian) matrix reduces
to the problem of finding the eigenvalues of a symmetric (resp. Hermitian) tridiagonal matrix,
and this can be done much more efficiently.
The second fact is that if H is an upper Hessenberg matrix and if it is diagonalizable, then
there is an invertible matrix P such that H = PÎ›P
âˆ’1 with Î› a diagonal matrix consisting
of the eigenvalues of H, such that P
âˆ’1 has an LU-decomposition; see Serre [156] (Theorem
13.3).
As a consequence, since any symmetric (or Hermitian) tridiagonal matrix is a block diagï¿¾onal matrix of unreduced symmetric (resp. Hermitian) tridiagonal matrices, by Proposition
18.3, we see that the QR algorithm applied to a tridiagonal matrix which is symmetric (or
Hermitian) positive definite converges to a diagonal matrix consisting of its eigenvalues. Let
us record this important fact.
Theorem 18.4. Let H be a symmetric (or Hermitian) positive definite tridiagonal matrix.
If H is unreduced, then the QR algorithm converges to a diagonal matrix consisting of the
eigenvalues of H.
Since every symmetric (or Hermitian) positive definite matrix is similar to a tridiagonal
symmetric (resp. Hermitian) positive definite matrix, we deduce that we have a method
for finding the eigenvalues of a symmetric (resp. Hermitian) positive definite matrix (more
accurately, to find approximations as good as we want for these eigenvalues).
If A is a symmetric (or Hermitian) matrix, since its eigenvalues are real, for some Âµ > 0
large enough (pick Âµ > Ï(A)), A + ÂµI is symmetric (resp. Hermitan) positive definite, so
we can apply the QR algorithm to an upper Hessenberg matrix similar to A + ÂµI to find its
eigenvalues, and then the eigenvalues of A are obtained by subtracting Âµ.
The problem of finding the eigenvalues of a symmetric matrix is discussed extensively in
Parlett [135], one of the best references on this topic.
The upper Hessenberg form also yields a way to handle singular matrices. First, checking
the proof of Proposition 14.21 that an n Ã— n complex matrix A (possibly singular) can be
factored as A = QR where Q is a unitary matrix which is a product of Householder reflections
and R is upper triangular, it is easy to see that if A is upper Hessenberg, then Q is also upper
Hessenberg. If H is an unreduced upper Hessenberg matrix, since Q is upper Hessenberg and
R is upper triangular, we have hi+1i = qi+1irii for i = 1 . . . , n âˆ’ 1, and since H is unreduced,
rii 6 = 0 for i = 1, . . . , n âˆ’ 1. Consequently H is singular iff rnn = 0. Then the matrix RQ is a
matrix whose last row consists of zeroâ€™s thus we can deflate the problem by considering the
(n âˆ’ 1) Ã— (n âˆ’ 1) unreduced Hessenberg matrix obtained by deleting the last row and the
last column. After finitely many steps (not larger that the multiplicity of the eigenvalue 0),
there remains an invertible unreduced Hessenberg matrix. As an alternative, see Serre [156]
(Chapter 13, Section 13.3.2).
As is, the QR algorithm, although very simple, is quite inefficient for several reasons. In
the next section, we indicate how to make the method more efficient. This involves a lot of
work and we only discuss the main ideas at a high level.
18.3. MAKING THE QR METHOD MORE EFFICIENT USING SHIFTS 661
18.3 Making the QR Method More Efficient
Using Shifts
To improve efficiency and cope with pairs of complex conjugate eigenvalues in the case of
real matrices, the following steps are taken:
(1) Initially reduce the matrix A to upper Hessenberg form, as A = UHUâˆ—
. Then apply
the QR-algorithm to H (actually, to its unreduced Hessenberg blocks). It is easy to
see that the matrices Hk produced by the QR algorithm remain upper Hessenberg.
(2) To accelerate convergence, use shifts, and to deal with pairs of complex conjugate
eigenvalues, use double shifts.
(3) Instead of computing a QR-factorization explicitly while doing a shift, perform an
implicit shift which computes Ak+1 = Qâˆ—
kAkQk without having to compute a QRï¿¾factorization (of Ak âˆ’ ÏƒkI), and similarly in the case of a double shift. This is the
most intricate modification of the basic QR algorithm and we will not discuss it here.
This method is usually referred as bulge chasing. Details about this technique for
real matrices can be found in Demmel [48] (Section 4.4.8) and Golub and Van Loan
[80] (Section 7.5). Watkins discusses the QR algorithm with shifts as a bulge chasing
method in the more general case of complex matrices [187, 188].
Let us repeat an important remark made in the previous section. If we start with a
matrix H in upper Hessenberg form, if at any stage of the QR algorithm we find that some
subdiagonal entry (Hk)p+1p = 0 or is very small, then Hk is of the form
Hk =

H11 H12
0 H22
,
where both H11 and H22 are upper Hessenberg matrices (with H11 a p Ã— p matrix and H22
a (n âˆ’ p) Ã— (n âˆ’ p) matrix), and the eigenvalues of Hk are the eigenvalues of H11 and H22.
For example, in the matrix
H =
ï£«
ï£¬ï£¬ï£¬ï£¬ï£­
h
âˆ— âˆ— âˆ— âˆ— âˆ—
21 âˆ— âˆ— âˆ— âˆ—
0 h32 âˆ— âˆ— âˆ—
0 0
0 0 0
h43
h
âˆ— âˆ—
54 âˆ—
ï£¶
ï£·ï£·ï£·ï£·ï£¸
,
if h43 = 0, then we have the block matrix
H =
ï£«
ï£¬ï£¬ï£¬ï£¬ï£­
h
âˆ— âˆ— âˆ— âˆ— âˆ—
21 âˆ— âˆ— âˆ— âˆ—
0 h32 âˆ— âˆ— âˆ—
0 0 0
0 0 0 h
âˆ— âˆ—
54 âˆ—
ï£¶
ï£·ï£·ï£·ï£·ï£¸
.
662 CHAPTER 18. COMPUTING EIGENVALUES AND EIGENVECTORS
Then we can recursively apply the QR algorithm to H11 and H22.
In particular, if (Hk)nnâˆ’1 = 0 or is very small, then (Hk)nn is a good approximation of
an eigenvalue, so we can delete the last row and the last column of Hk and apply the QR
algorithm to this submatrix. This process is called deflation. If (Hk)nâˆ’1nâˆ’2 = 0 or is very
small, then the 2 Ã— 2 â€œcorner blockâ€

(Hk)nâˆ’1nâˆ’1 (Hk)nâˆ’1n
(Hk)nnâˆ’1 (Hk)nn 
appears, and its eigenvalues can be computed immediately by solving a quadratic equation.
Then we deflate Hk by deleting its last two rows and its last two columns and apply the QR
algorithm to this submatrix.
Thus it would seem desirable to modify the basic QR algorithm so that the above situï¿¾ations arises, and this is what shifts are designed for. More precisely, under the hypotheses
of Theorem 18.1, it can be shown (see Ciarlet [41], Section 6.3) that the entry (Ak)ij with
i > j converges to 0 as |Î»i/Î»j
|
k
converges to 0. Also, if we let ri be defined by
r1 =

  
Î»2
Î»1




, ri = max 

 

Î»i
Î»iâˆ’1




,




Î»i+1
Î»i





, 2 â‰¤ i â‰¤ n âˆ’ 1, rn =

  
Î»n
Î»nâˆ’1




,
then there is a constant C (independent of k) such that
|(Ak)ii âˆ’ Î»i
| â‰¤ Cri
k
, 1 â‰¤ i â‰¤ n.
In particular, if H is upper Hessenberg, then the entry (Hk)i+1i converges to 0 as |Î»i+1/Î»i
|
k
converges to 0. Thus if we pick Ïƒk close to Î»i
, we expect that (Hk âˆ’ ÏƒkI)i+1i converges to 0
as |(Î»i+1 âˆ’Ïƒk)/(Î»i âˆ’Ïƒk)|
k
converges to 0, and this ratio is much smaller than 1 as Ïƒk is closer
to Î»i
. Typically, we apply a shift to accelerate convergence to Î»n (so i = n âˆ’ 1). In this
case, both (Hk âˆ’ ÏƒkI)nnâˆ’1 and |(Hk âˆ’ ÏƒkI)nn âˆ’ Î»n| converge to 0 as |(Î»n âˆ’ Ïƒk)/(Î»nâˆ’1 âˆ’ Ïƒk)|
k
converges to 0.
A shift is the following modified QR-steps (switching back to an arbitrary matrix A, since
the shift technique applies in general). Pick some Ïƒk, hopefully close to some eigenvalue of
A (in general, Î»n), and QR-factor Ak âˆ’ ÏƒkI as
Ak âˆ’ ÏƒkI = QkRk,
and then form
Ak+1 = RkQk + ÏƒkI.
Since
Ak+1 = RkQk + ÏƒkI
= Q
âˆ—
kQkRkQk + Q
âˆ—
kQkÏƒk
= Q
âˆ—
k
(QkRk + ÏƒkI)Qk
= Q
âˆ—
kAkQk,
18.3. MAKING THE QR METHOD MORE EFFICIENT USING SHIFTS 663
Ak+1 is similar to Ak, as before. If Ak is upper Hessenberg, then it is easy to see that Ak+1
is also upper Hessenberg.
If A is upper Hessenberg and if Ïƒi
is exactly equal to an eigenvalue, then Ak âˆ’ ÏƒkI is
singular, and forming the QR-factorization will detect that Rk has some diagonal entry equal
to 0. Assuming that the QR-algorithm returns (Rk)nn = 0 (if not, the argument is easily
adapted), then the last row of RkQk is 0, so the last row of Ak+1 = RkQk + ÏƒkI ends with
Ïƒk (all other entries being zero), so we are in the case where we can deflate Ak (and Ïƒk is
indeed an eigenvalue).
The question remains, what is a good choice for the shift Ïƒk?
Assuming again that H is in upper Hessenberg form, it turns out that when (Hk)nnâˆ’1
is small enough, then a good choice for Ïƒk is (Hk)nn. In fact, the rate of convergence is
quadratic, which means roughly that the number of correct digits doubles at every iteration.
The reason is that shifts are related to another method known as inverse iteration, and such
a method converges very fast. For further explanations about this connection, see Demmel
[48] (Section 4.4.4) and Trefethen and Bau [176] (Lecture 29).
One should still be cautious that the QR method with shifts does not necessarily converge,
and that our convergence proof no longer applies, because instead of having the identity
Ak = PkRk, we have
(A âˆ’ ÏƒkI)Â· Â· Â·(A âˆ’ Ïƒ2I)(A âˆ’ Ïƒ1I) = PkRk.
Of course, the QR algorithm loops immediately when applied to an orthogonal matrix
A. This is also the case when A is symmetric but not positive definite. For example, both
the QR algorithm and the QR algorithm with shifts loop on the matrix
A =

0 1
1 0 .
In the case of symmetric matrices, Wilkinson invented a shift which helps the QR algoï¿¾rithm with shifts to make progress. Again, looking at the lower corner of Ak, say
B =

anâˆ’1 bnâˆ’1
bnâˆ’1 an

,
the Wilkinson shift picks the eigenvalue of B closer to an. If we let
Î´ =
anâˆ’1 âˆ’ an
2
,
it is easy to see that the eigenvalues of B are given by
Î» =
an +
2
anâˆ’1 Â±
q Î´
2 + b
2
nâˆ’1
.
664 CHAPTER 18. COMPUTING EIGENVALUES AND EIGENVECTORS
It follows that
Î» âˆ’ an = Î´ Â±
q Î´
2 + b
2
nâˆ’1
,
and from this it is easy to see that the eigenvalue closer to an is given by
Âµ = an âˆ’
sign(Î´)b
2
nâˆ’1
(|Î´| +
p Î´
2 + b
2
nâˆ’1
)
.
If Î´ = 0, then we pick arbitrarily one of the two eigenvalues. Observe that the Wilkinson
shift applied to the matrix
A =

0 1
1 0
is either +1 or âˆ’1, and in one step, deflation occurs and the algorithm terminates successfully.
We now discuss double shifts, which are intended to deal with pairs of complex conjugate
eigenvalues.
Let us assume that A is a real matrix. For any complex number Ïƒk with nonzero imaginary
part, a double shift consists of the following steps:
Ak âˆ’ ÏƒkI = QkRk
Ak+1 = RkQk + ÏƒkI
Ak+1 âˆ’ ÏƒkI = Qk+1Rk+1
Ak+2 = Rk+1Qk+1 + ÏƒkI.
From the computation made for a single shift, we have Ak+1 = Qâˆ—
kAkQk and Ak+2 =
Qâˆ—
k+1Ak+1Qk+1, so we obtain
Ak+2 = Q
âˆ—
k+1Q
âˆ—
kAkQkQk+1.
The matrices Qk are complex, so we would expect that the Ak are also complex, but
remarkably we can keep the products QkQk+1 real, and so the Ak also real. This is highly
desirable to avoid complex arithmetic, which is more expensive.
Observe that since
Qk+1Rk+1 = Ak+1 âˆ’ ÏƒkI = RkQk + (Ïƒk âˆ’ Ïƒk)I,
we have
QkQk+1Rk+1Rk = Qk(RkQk + (Ïƒk âˆ’ Ïƒk)I)Rk
= QkRkQkRk + (Ïƒk âˆ’ Ïƒk)QkRk
= (Ak âˆ’ ÏƒkI)
2 + (Ïƒk âˆ’ Ïƒk)(Ak âˆ’ ÏƒkI)
= A
2
k âˆ’ 2(< Ïƒk)Ak + |Ïƒk|
2
I.
18.3. MAKING THE QR METHOD MORE EFFICIENT USING SHIFTS 665
If we assume by induction that matrix Ak is real (with k = 2` +1, ` â‰¥ 0), then the matrix
S = A2
k âˆ’ 2(< Ïƒk)Ak + |Ïƒk|
2
I is also real, and since QkQk+1 is unitary and Rk+1Rk is upper
triangular, we see that
S = QkQk+1Rk+1Rk
is a QR-factorization of the real matrix S, thus QkQk+1 and Rk+1Rk can be chosen to be
real matrices, in which case (QkQk+1)
âˆ—
is also real, and thus
Ak+2 = Q
âˆ—
k+1Q
âˆ—
kAkQkQk+1 = (QkQk+1)
âˆ—AkQkQk+1
