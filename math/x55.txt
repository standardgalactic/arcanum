.
a
.
k
ck


=


0
.
.
0
.

 ,
782 CHAPTER 23. APPLICATIONS OF SVD AND PSEUDO-INVERSES
in the least squares sense, subject to the conditions a
>i aj = δi j , for all i, j with 1 ≤ i, j ≤ k,
where the matrix of the system is a block diagonal matrix consisting of k diagonal blocks
(X, 1), where 1 denotes the column vector (1, . . . , 1) ∈ R
n
.
Again it is easy to see that each hyperplane Hi must pass through the centroid µ of
X1, . . . , Xn, and by switching to the centered data Xi − µ we get the system


X − µ 0 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
0 0 · · · X − µ




a1
.
.
a
.
k

 =


0
.
.
0
.

 ,
with a
>i aj = δi j for all i, j with 1 ≤ i, j ≤ k.
If V DU > = X −µ is an SVD decomposition, it is easy to see that a least squares solution
of this system is given by the last k columns of U, assuming that the main diagonal of D
consists of the singular values σ1 ≥ σ2 ≥ · · · ≥ σd of X −µ arranged in descending order. But
now the (d − k)-dimensional subspace Ud−k cut out by the hyperplanes defined by a1, . . . , ak
is simply the orthogonal complement of (a1, . . . , ak), which is the subspace spanned by the
first d − k columns of U.
So the best (d−k)-dimensional affine subpsace Ak approximating X1, . . . , Xn in the least
squares sense is
Ak = µ + Ud−k,
where Ud−k is the linear subspace spanned by the first d−k principal directions of X −µ, that
is, the first d−k columns of U. Consequently, we get the following interesting interpretation
of PCA (actually, principal directions):
Theorem 23.12. Let X be an n × d matrix of data points X1, . . . , Xn, and let µ be the
centroid of the Xi’s. If X − µ = V DU > is an SVD decomposition of X − µ and if the
main diagonal of D consists of the singular values σ1 ≥ σ2 ≥ · · · ≥ σd, then a best (d − k)-
dimensional affine approximation Ak of X1, . . . , Xn in the least squares sense is given by
Ak = µ + Ud−k,
where Ud−k is the linear subspace spanned by the first d − k columns of U, the first d − k
principal directions of X − µ (1 ≤ k ≤ d − 1).
Example 23.11. Going back to Example 23.10, a best 1-dimensional affine approximation
A1 is the affine line passing through (µ1, µ2) = (1824.4, 5.6) of direction u1 = (0.9995, 0.0325).
Example 23.12. Suppose in the data set of Example 23.5 that we add the month of birth
of every mathematician as a feature. We obtain the following data set.
23.5. BEST AFFINE APPROXIMATION 783
Name month year length
Carl Friedrich Gauss 4 1777 0
Camille Jordan 1 1838 12
Adrien-Marie Legendre 9 1752 0
Bernhard Riemann 9 1826 15
David Hilbert 1 1862 2
Henri Poincar´e 4 1854 5
Emmy Noether 3 1882 0
Karl Weierstrass 10 1815 0
Eugenio Beltrami 10 1835 2
Hermann Schwarz 1 1843 20
The mean of the first column is 5.2, and the centered data set is given below.
Name month year length
Carl Friedrich Gauss −1.2 −51.4 −5.6
Camille Jordan −4.2 9.6 6.4
Adrien-Marie Legendre 3.8 −76.4 −5.6
Bernhard Riemann 3.8 −2.4 9.4
David Hilbert −4.2 33.6 −3.6
Henri Poincar´e −1.2 25.6 −0.6
Emmy Noether −2.2 53.6 −5.6
Karl Weierstrass 4.8 13.4 −5.6
Eugenio Beltrami 4.8 6.6 −3.6
Hermann Schwarz −4.2 14.6 14.4
Running SVD on this data set we get
U =

−
−
0
0
0
.0394 0
.
.
9987 0
0327 −0
.
.
1717 0
0390 0
.9844 0
.
.
.
9844
0332
1730

 , D =


117.
0 22
0706 0 0
.0390 0
0 0 10.1571
0 0 0
0 0 0
0 0 0
0 0 0
0 0 0
0 0 0
0 0 0


,
784 CHAPTER 23. APPLICATIONS OF SVD AND PSEUDO-INVERSES
and
V D =


51.4683 3.3013 −3.8569
−9.9623 −6.6467 −2.7082
76.6327 3.1845 0.2348
2.2393 −8.6943 5.2872
−
−
33
25
.
.
6038 4
5941 1
.
.
1334
3833
−
−
3
0
.
.
6415
4350
−53.4333 7.2258 −1.3547
−13.0100 6.8594 4.2010
−
−
15
6.
.
2843 4
2173 −14
.6254 4
.3266 −1
.3212
.1581


, X − µ =


−1.2000 −51.4000 −5.6000
−4.2000 9.6000 6.4000
3.8000 −76.4000 −5.6000
3.8000 −2.4000 9.4000
−
−
4
1
.
.
2000 33
2000 25
.
.
6000
6000
−
−
3
0
.
.
6000
6000
−2.2000 53.6000 −5.6000
4.8000 13.4000 −5.6000
−
4
4
.8000 6
.2000 14
.
.
6000
6000 14
−3
.
.
4000
6000


.
The first principal direction u1 = (0.0394, −0.9987, −0.0327) is basically the opposite
of the y-axis, and the most significant feature is the year of birth. The second principal
direction u2 = (0.1717, 0.0390, −0.9844) is close to the opposite of the z-axis, and the second
most significant feature is the lenght of beards. A best affine plane is spanned by the vectors
u1 and u2.
There are many applications of PCA to data compression, dimension reduction, and
pattern analysis. The basic idea is that in many cases, given a data set X1, . . . , Xn, with
Xi ∈ R
d
, only a “small” subset of m < d of the features is needed to describe the data set
accurately.
If u1, . . . , ud are the principal directions of X −µ, then the first m projections of the data
(the first m principal components, i.e., the first m columns of V D) onto the first m principal
directions represent the data without much loss of information. Thus, instead of using the
original data points X1, . . . , Xn, with Xi ∈ R
d
, we can use their projections onto the first m
principal directions Y1, . . . , Ym, where Yi ∈ R
m and m < d, obtaining a compressed version
of the original data set.
For example, PCA is used in computer vision for face recognition. Sirovitch and Kirby
(1987) seem to be the first to have had the idea of using PCA to compress facial images.
They introduced the term eigenpicture to refer to the principal directions, ui
. However, an
explicit face recognition algorithm was given only later by Turk and Pentland (1991). They
renamed eigenpictures as eigenfaces.
For details on the topic of eigenfaces, see Forsyth and Ponce [64] (Chapter 22, Section
22.3.2), where you will also find exact references to Turk and Pentland’s papers.
Another interesting application of PCA is to the recognition of handwritten digits. Such
an application is described in Hastie, Tibshirani, and Friedman, [88] (Chapter 14, Section
14.5.1).
23.6 Summary
The main concepts and results of this chapter are listed below:
23.7. PROBLEMS 785
• Least squares problems.
• Existence of a least squares solution of smallest norm (Theorem 23.1).
• The pseudo-inverse A+ of a matrix A.
• The least squares solution of smallest norm is given by the pseudo-inverse (Theorem
23.2)
• Projection properties of the pseudo-inverse.
• The pseudo-inverse of a normal matrix.
• The Penrose characterization of the pseudo-inverse.
• Data compression and SVD.
• Best approximation of rank < r of a matrix.
• Principal component analysis.
• Review of basic statistical concepts: mean, variance, covariance, covariance matrix .
• Centered data, centroid.
• The principal components (PCA).
• The Rayleigh–Ritz theorem (Theorem 23.10).
• The main theorem: SVD yields PCA (Theorem 23.11).
• Best affine approximation.
• SVD yields a best affine approximation (Theorem 23.12).
• Face recognition, eigenfaces.
23.7 Problems
Problem 23.1. Consider the overdetermined system in the single variable x:
a1x = b1, . . . , amx = bm,
with a
2
1 + · · · + a
2
m 6 = 0. Prove that the least squares solution of smallest norm is given by
x
+ =
a1b1 + · · · + ambm
a
2
1 + · · · + a
2
m
.
786 CHAPTER 23. APPLICATIONS OF SVD AND PSEUDO-INVERSES
Problem 23.2. Let X be an m × n real matrix. For any strictly positive constant K > 0,
the matrix X> X + KIn is invertible. Prove that the limit of the matrix (X> X + KIn)
−1X>
when K goes to zero is equal to the pseudo-inverse X+ of X.
Problem 23.3. Use Matlab to find the pseudo-inverse of the 8 × 6 matrix
A =


64 2 3 61 60 6
9 55 54 12 13 51
17 47 46 20 21 43
40 26 27 37 36 30
32 34 35 29 28 38
41 23 22 44 45 19
49 15 14 52 53 11
8 58 59 5 4 62


.
Observe that the sums of the columns are all equal to 260. Let b be the vector of
dimension 8 whose coordinates are all equal to 256. Find the solution x
+ of the system
Ax = b.
Problem 23.4. The purpose of this problem is to show that Proposition 23.9 (the Eckart–
Young theorem) also holds for the Frobenius norm. This problem is adapted from Strang
[171], Section I.9.
Suppose the m ×n matrix B of rank at most k minimizes k A − Bk F
. Start with an SVD
of B,
B = V

D
0 0
0

U
> ,
where D is a diagonal k × k matrix. We can write
A = V

L + E
G H
+ R F  U
> ,
where L is strictly lower triangular in the first k rows, E is diagonal, and R is strictly upper
triangular, and let
C = V

L + D
0 0
+ R F U
> ,
which clearly has rank ≤ k.
(1) Prove that
k
A − Bk
2
F = k A − Ck
2
F + k Lk
2
F + k Rk
2
F + k Fk
2
F
.
Since k A − Bk F
is minimal, show that L = R = F = 0.
Similarly, show that G = 0.
23.7. PROBLEMS 787
(2) We have
V
> AU =

E
0 H
0

, V > BU =

D
0 0
0

,
where E is diagonal, so deduce that
1. D = diag(σ1, . . . , σk).
2. The singular values of H must be the smallest n − k singular values of A.
3. The minimum of k A − Bk F must be k Hk F = (σk
2
+1 + · · · + σr
2
)
1/2
.
Problem 23.5. Prove that the closest rank 1 approximation (in k k 2
) of the matrix
A =

3 0
4 5
is
A1 =
3
2

1 1
3 3 .
Show that the Eckart–Young theorem fails for the operator norm k k ∞ by finding a rank
1 matrix B such that k A − Bk ∞ < k A − A1k ∞.
Problem 23.6. Find a closest rank 1 approximation (in k k 2
) for the matrices
A =


3 0 0
0 2 0
0 0 1

 , A =

0 3
2 0 , A =

2 1
1 2 .
Problem 23.7. Find a closest rank 1 approximation (in k k 2
) for the matrix
A =

cos
sin θ
θ −
cos
sin
θ
θ

.
Problem 23.8. Let S be a real symmetric positive definite matrix and let S = UΣU
> be a
diagonalization of S. Prove that the closest rank 1 matrix (in the L
2
-norm) to S is u1σ1u
>1
,
where u1 is the first column of U.
788 CHAPTER 23. APPLICATIONS OF SVD AND PSEUDO-INVERSES
Part II
Affine and Projective Geometry
789
Chapter 24
Basics of Affine Geometry
L’alg`ebre n’est qu’une g´eom´etrie ´ecrite; la g´eom´etrie n’est qu’une alg`ebre figur´ee.
—Sophie Germain
24.1 Affine Spaces
Geometrically, curves and surfaces are usually considered to be sets of points with some
special properties, living in a space consisting of “points.” Typically, one is also interested
in geometric properties invariant under certain transformations, for example, translations,
rotations, projections, etc. One could model the space of points as a vector space, but this is
not very satisfactory for a number of reasons. One reason is that the point corresponding to
the zero vector (0), called the origin, plays a special role, when there is really no reason to have
a privileged origin. Another reason is that certain notions, such as parallelism, are handled
in an awkward manner. But the deeper reason is that vector spaces and affine spaces really
have different geometries. The geometric properties of a vector space are invariant under
the group of bijective linear maps, whereas the geometric properties of an affine space are
invariant under the group of bijective affine maps, and these two groups are not isomorphic.
Roughly speaking, there are more affine maps than linear maps.
Affine spaces provide a better framework for doing geometry. In particular, it is possible
to deal with points, curves, surfaces, etc., in an intrinsic manner, that is, independently
of any specific choice of a coordinate system. As in physics, this is highly desirable to
really understand what is going on. Of course, coordinate systems have to be chosen to
finally carry out computations, but one should learn to resist the temptation to resort to
coordinate systems until it is really necessary.
Affine spaces are the right framework for dealing with motions, trajectories, and physical
forces, among other things. Thus, affine geometry is crucial to a clean presentation of
kinematics, dynamics, and other parts of physics (for example, elasticity). After all, a rigid
motion is an affine map, but not a linear map in general. Also, given an m × n matrix A
791
792 CHAPTER 24. BASICS OF AFFINE GEOMETRY
and a vector b ∈ R
m, the set U = {x ∈ R
n
| Ax = b} of solutions of the system Ax = b is an
affine space, but not a vector space (linear space) in general.
Use coordinate systems only when needed!
This chapter proceeds as follows. We take advantage of the fact that almost every affine
concept is the counterpart of some concept in linear algebra. We begin by defining affine
spaces, stressing the physical interpretation of the definition in terms of points (particles)
and vectors (forces). Corresponding to linear combinations of vectors, we define affine com￾binations of points (barycenters), realizing that we are forced to restrict our attention to
families of scalars adding up to 1. Corresponding to linear subspaces, we introduce affine
subspaces as subsets closed under affine combinations. Then, we characterize affine sub￾spaces in terms of certain vector spaces called their directions. This allows us to define a
clean notion of parallelism. Next, corresponding to linear independence and bases, we define
affine independence and affine frames. We also define convexity. Corresponding to linear
maps, we define affine maps as maps preserving affine combinations. We show that every
affine map is completely defined by the image of one point and a linear map. Then, we
investigate briefly some simple affine maps, the translations and the central dilatations. At
this point, we give a glimpse of affine geometry. We prove the theorems of Thales, Pappus,
and Desargues. After this, the definition of affine hyperplanes in terms of affine forms is
reviewed. The section ends with a closer look at the intersection of affine subspaces.
Our presentation of affine geometry is far from being comprehensive, and it is biased
toward the algorithmic geometry of curves and surfaces. For more details, the reader is
referred to Pedoe [136], Snapper and Troyer [162], Berger [11, 12], Coxeter [44], Samuel
[142], Tisseron [175], Fresnel [65], Vienne [185], and Hilbert and Cohn-Vossen [92].
Suppose we have a particle moving in 3D space and that we want to describe the trajectory
of this particle. If one looks up a good textbook on dynamics, such as Greenwood [82], one
finds out that the particle is modeled as a point, and that the position of this point x is
determined with respect to a “frame” in R
3 by a vector. Curiously, the notion of a frame is
rarely defined precisely, but it is easy to infer that a frame is a pair (O,(e1, e2, e3)) consisting
of an origin O (which is a point) together with a basis of three vectors (e1, e2, e3). For
example, the standard frame in R
3 has origin O = (0, 0, 0) and the basis of three vectors
e1 = (1, 0, 0), e2 = (0, 1, 0), and e3 = (0, 0, 1). The position of a point x is then defined by
the “unique vector” from O to x.
But wait a minute, this definition seems to be defining frames and the position of a point
without defining what a point is! Well, let us identify points with elements of R
3
. If so, given
any two points a = (a1, a2, a3) and b = (b1, b2, b3), there is a unique free vector , denoted by
−→ab, from a to b, the vector
−→ab = (b1 − a1, b2 − a2, b3 − a3). Note that
b = a +
−→ab,
24.1. AFFINE SPACES 793
O
a
b
−→ab
Figure 24.1: Points and free vectors.
addition being understood as addition in R
3
. Then, in the standard frame, given a point
x = (x1, x2, x3), the position of x is the vector −→Ox = (x1, x2, x3), which coincides with the
point itself. In the standard frame, points and vectors are identified. Points and free vectors
are illustrated in Figure 24.1.
What if we pick a frame with a different origin, say Ω = (ω1, ω2, ω3), but the same basis
vectors (e1, e2, e3)? This time, the point x = (x1, x2, x3) is defined by two position vectors:
−→Ox = (x1, x2, x3)
in the frame (O,(e1, e2, e3)) and
−→Ωx = (x1 − ω1, x2 − ω2, x3 − ω3)
in the frame (Ω,(e1, e2, e3)). See Figure 24.2.
This is because
−→Ox =
−→OΩ + −→Ωx and −→OΩ = (ω1, ω2, ω3).
We note that in the second frame (Ω,(e1, e2, e3)), points and position vectors are no longer
identified. This gives us evidence that points are not vectors. It may be computationally
convenient to deal with points using position vectors, but such a treatment is not frame
invariant, which has undesirable effects.
Inspired by physics, we deem it important to define points and properties of points that
are frame invariant. An undesirable side effect of the present approach shows up if we attempt
to define linear combinations of points. First, let us review the notion of linear combination
of vectors. Given two vectors u and v of coordinates (u1, u2, u3) and (v1, v2, v3) with respect
794 CHAPTER 24. BASICS OF AFFINE GEOMETRY
e3
e3
e2
e2
e
1
e1
Ω
x
O
Figure 24.2: The two position vectors for the point x.
to the basis (e1, e2, e3), for any two scalars λ, µ, we can define the linear combination λu+µv
as the vector of coordinates
(λu1 + µv1, λu2 + µv2, λu3 + µv3).
If we choose a different basis (e
01
, e02
, e03
) and if the matrix P expressing the vectors (e
01
, e02
, e03
)
over the basis (e1, e2, e3) is
P =


a1 b1 c1
a2 b2 c2
a3 b3 c3

 ,
which means that the columns of P are the coordinates of the e
0j
over the basis (e1, e2, e3),
since
u1e1 + u2e2 + u3e3 = u
01
e
01 + u
02
e
02 + u
03
e
03
and
v1e1 + v2e2 + v3e3 = v1
0
e
01 + v2
0
e
02 + v3
0
e
03
,
it is easy to see that the coordinates (u1, u2, u3) and (v1, v2, v3) of u and v with respect to
the basis (e1, e2, e3) are given in terms of the coordinates (u
01
, u02
, u03
) and (v1
0
, v2
0
, v3
0
) of u and
v with respect to the basis (e
01
, e02
, e03
) by the matrix equations


u
u
1
2
u3

 = P


u
01
u
02
u
03

 and


v
v
v
1
2
3

 = P


v
v
v
1
2
3
0
0
0

 .
From the above, we get
Ωx
Ox
24.1. AFFINE SPACES 795


u
01
u
02
u
03

 = P
−1


u
u
u
1
2
3

 and


v
v
v
1
2
3
0
0
0

 = P
−1


v
v
v
1
2
3

 ,
and by linearity, the coordinates
(λu01 + µv1
0
, λu02 + µv2
0
, λu03 + µv3
0
)
of λu + µv with respect to the basis (e
01
, e02
, e03
) are given by


λu01 + µv01
λu02 + µv02
λu03 + µv3
0

 = λP −1


u
u
u
1
2
3

 + µP −1


v
v
v
1
2
3

 = P
−1


λu1 + µv1
λu2 + µv2
λu3 + µv3

 .
Everything worked out because the change of basis does not involve a change of origin. On the
other hand, if we consider the change of frame from the frame (O,(e1, e2, e3)) to the frame
(Ω,(e1, e2, e3)), where −→OΩ = (ω1, ω2, ω3), given two points a, b of coordinates (a1, a2, a3)
and (b1, b2, b3) with respect to the frame (O,(e1, e2, e3)) and of coordinates (a
01
, a02
, a03
) and
(b
01
, b02
, b03
) with respect to the frame (Ω,(e1, e2, e3)), since
(a
01
, a02
, a03
) = (a1 − ω1, a2 − ω2, a3 − ω3)
and
(b
01
, b02
, b03
) = (b1 − ω1, b2 − ω2, b3 − ω3),
the coordinates of λa + µb with respect to the frame (O,(e1, e2, e3)) are
(λa1 + µb1, λa2 + µb2, λa3 + µb3),
but the coordinates
(λa01 + µb01
, λa02 + µb02
, λa03 + µb03
)
of λa + µb with respect to the frame (Ω,(e1, e2, e3)) are
(λa1 + µb1 − (λ + µ)ω1, λa2 + µb2 − (λ + µ)ω2, λa3 + µb3 − (λ + µ)ω3),
which are different from
(λa1 + µb1 − ω1, λa2 + µb2 − ω2, λa3 + µb3 − ω3),
unless λ + µ = 1. See Figure 24.3.
Thus, we have discovered a major difference between vectors and points: The notion of
linear combination of vectors is basis independent, but the notion of linear combination of
points is frame dependent. In order to salvage the notion of linear combination of points,
some restriction is needed: The scalar coefficients must add up to 1.
796 CHAPTER 24. BASICS OF AFFINE GEOMETRY
e3
e3
e2
e2
e
1
e
1
Ω
O
= (3,4,5)
a = (1,1,1)
b = (2,3,1)
a + b = (3,4,2) = (0, 0, -3)
e3
e3
e2
e2
e
1
e1
Ω
O
= (3,4,5)
a = (-2,-3,-4)
b = (-1,-1,-4)
a + b = (-3, -4, -8) = (0, 0, -3)
Figure 24.3: The top figure shows the location of the “point” sum a + b with respect to the
frame (O,(e1, e2, e3)), while the bottom figure shows the location of the “point” sum a + b
with respect to the frame (Ω,(e1, e2, e3)).
A clean way to handle the problem of frame invariance and to deal with points in a more
intrinsic manner is to make a clearer distinction between points and vectors. We duplicate
R
3
into two copies, the first copy corresponding to points, where we forget the vector space
structure, and the second copy corresponding to free vectors, where the vector space structure
is important. Furthermore, we make explicit the important fact that the vector space R
3 acts
on the set of points R
3
: Given any point a = (a1, a2, a3) and any vector v = (v1, v2, v3),
we obtain the point
a + v = (a1 + v1, a2 + v2, a3 + v3),
which can be thought of as the result of translating a to b using the vector v. We can imagine
that v is placed such that its origin coincides with a and that its tip coincides with b. This
action +: R
3 × R
3 → R
3
satisfies some crucial properties. For example,
a + 0 = a,
(a + u) + v = a + (u + v),
24.1. AFFINE SPACES 797
and for any two points a, b, there is a unique free vector −→ab such that
b = a +
−→ab.
It turns out that the above properties, although trivial in the case of R
3
, are all that is
needed to define the abstract notion of affine space (or affine structure). The basic idea is
to consider two (distinct) sets E and −→E , where E is a set of points (with no structure) and
−→E is a vector space (of free vectors) acting on the set E.
Did you say “A fine space”?
Intuitively, we can think of the elements of −→E as forces moving the points in E, considered
as physical particles. The effect of applying a force (free vector) u ∈
−→E to a point a ∈ E is
a translation. By this, we mean that for every force u ∈
−→E , the action of the force u is to
“move” every point a ∈ E to the point a + u ∈ E obtained by the translation corresponding
to u viewed as a vector. Since translations can be composed, it is natural that −→E is a vector
space.
For simplicity, it is assumed that all vector spaces under consideration are defined over
the field R of real numbers. Most of the definitions and results also hold for an arbitrary
field K, although some care is needed when dealing with fields of characteristic different
from zero. It is also assumed that all families (λi)i∈I of scalars have finite support. Recall
that a family (λi)i∈I of scalars has finite support if λi = 0 for all i ∈ I −J, where J is a finite
subset of I. Obviously, finite families of scalars have finite support, and for simplicity, the
reader may assume that all families of scalars are finite. The formal definition of an affine
space is as follows.
Definition 24.1. An affine space is either the degenerate space reduced to the empty set,
or a triple 
 E,
−→E , +
 consisting of a nonempty set E (of points), a vector space −→E (of trans￾lations, or free vectors), and an action +: E ×
−→E → E, satisfying the following conditions.
(A1) a + 0 = a, for every a ∈ E.
(A2) (a + u) + v = a + (u + v), for every a ∈ E, and every u, v ∈
−→E .
(A3) For any two points a, b ∈ E, there is a unique u ∈
−→E such that a + u = b.
The unique vector u ∈
−→E such that a + u = b is denoted by
−→ab, or sometimes by ab, or
even by b − a. Thus, we also write
b = a +
−→ab
(or b = a + ab, or even b = a + (b − a)).
The dimension of the affine space 
 E,
−→E , +
 is the dimension dim(−→E ) of the vector space
−→E . For simplicity, it is denoted by dim(E).
798 CHAPTER 24. BASICS OF AFFINE GEOMETRY
E −→E
a
b = a + u
c = a + w
u
v
w
Figure 24.4: Intuitive picture of an affine space.
Conditions (A1) and (A2) say that the (abelian) group −→E acts on E, and Condition (A3)
says that −→E acts transitively and faithfully on E. Note that
−−−−−→
a(a + v) = v
for all a ∈ E and all v ∈
−→E , since
−−−−−→
a(a + v) is the unique vector such that a+v = a+
−−−−−→
a(a + v).
Thus, b = a + v is equivalent to
−→ab = v. Figure 24.4 gives an intuitive picture of an affine
space. It is natural to think of all vectors as having the same origin, the null vector.
The axioms defining an affine space 
 E,
−→E , +
 can be interpreted intuitively as saying
that E and −→E are two different ways of looking at the same object, but wearing different
sets of glasses, the second set of glasses depending on the choice of an “origin” in E. Indeed,
we can choose to look at the points in E, forgetting that every pair (a, b) of points defines a
unique vector
−→ab in
−→E , or we can choose to look at the vectors u in
−→E , forgetting the points
in E. Furthermore, if we also pick any point a in E, a point that can be viewed as an origin
in E, then we can recover all the points in E as the translated points a + u for all u ∈
−→E .
This can be formalized by defining two maps between E and −→E .
For every a ∈ E, consider the mapping from −→E to E given by
u 7→ a + u,
where u ∈
−→E , and consider the mapping from E to
−→E given by
b 7→
−→ab,
where b ∈ E. The composition of the first mapping with the second is
u 7→ a + u 7→
−−−−−→
a(a + u),
24.1. AFFINE SPACES 799
which, in view of (A3), yields u. The composition of the second with the first mapping is
b 7→
−→ab 7→ a +
−→ab,
which, in view of (A3), yields b. Thus, these compositions are the identity from −→E to
−→E
and the identity from E to E, and the mappings are both bijections.
When we identify E with −→E via the mapping b 7→
−→ab, we say that we consider E as the
vector space obtained by taking a as the origin in E, and we denote it by Ea. Because Ea is
a vector space, to be consistent with our notational conventions we should use the notation
−→Ea (using an arrow), instead of Ea. However, for simplicity, we stick to the notation Ea.
Thus, an affine space 
 E,
−→E , +
 is a way of defining a vector space structure on a set of
points E, without making a commitment to a fixed origin in E. Nevertheless, as soon as
we commit to an origin a in E, we can view E as the vector space Ea. However, we urge
the reader to think of E as a physical set of points and of −→E as a set of forces acting on E,
rather than reducing E to some isomorphic copy of R
n
. After all, points are points, and not
