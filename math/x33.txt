
 − 2/3


1
1
1

 + 1/6

−
1
1
2

 = 1/2


−
1
0
1

 .
To complete the orthonormal basis, normalize u
03
to obtain
u3 = 1/
√
2


−
1
0
1

 .
An illustration of this example is provided by Figure 12.4.
Remarks:
(1) The QR-decomposition can now be obtained very easily, but we postpone this until
Section 12.8.
(2) The proof of Proposition 12.10 also works for a countably infinite basis for E, producing
a countably infinite orthonormal basis.
12.4. EXISTENCE AND CONSTRUCTION OF ORTHONORMAL BASES 461
e2
u
u 1
2
‘
u 1direction
u2
direction
e3 u3
‘
Figure 12.4: The top figure shows the construction of the blue u
02
as perpendicular to the
orthogonal projection of e2 onto u1, while the bottom figure shows the construction of the
green u
03
as normal to the plane determined by u1 and u2.
It should also be said that the Gram–Schmidt orthonormalization procedure that we have
presented is not very stable numerically, and instead, one should use the modified Gram–
Schmidt method. To compute u
0k+1, instead of projecting ek+1 onto u1, . . . , uk in a single
step, it is better to perform k projections. We compute u
k
1
+1, uk
2
+1, . . . , uk
k
+1 as follows:
u
k
1
+1 = ek+1 − (ek+1 · u1) u1,
u
k
i+1
+1 = u
k
i
+1 − (u
k
i
+1
· ui+1) ui+1,
where 1 ≤ i ≤ k − 1. It is easily shown that u
0k+1 = u
k
k
+1
.
Example 12.10. Let us apply the modified Gram–Schmidt method to the (e1, e2, e3) basis
of Example 12.9. The only change is the computation of u
03
. For the modified Gram–Schmidt
procedure, we first calculate
u
3
1 = e3 − (e3 · u1)u1 =


1
1
0

 − 2/3


1
1
1

 = 1/3


−
1
1
2

 .
Then
u
3
2 = u
3
1 − (u
3
1
· u2)u2 = 1/3


−
1
1
2

 + 1/6

−
1
1
2

 = 1/2


−
1
0
1

 ,
462 CHAPTER 12. EUCLIDEAN SPACES
u 1direction
u2
direction
e3
u3
1
u 1direction
u2
direction
u1
3
u3
2
Figure 12.5: The top figure shows the construction of the blue u
3
1
as perpendicular to the
orthogonal projection of e3 onto u1, while the bottom figure shows the construction of the
sky blue u
3
2
as perpendicular to the orthogonal projection of u
3
1
onto u2.
and observe that u
3
2 = u
03
. See Figure 12.5.
The following Matlab program implements the modified Gram–Schmidt procedure.
function q = gramschmidt4(e)
n = size(e,1);
for i = 1:n
q(:,i) = e(:,i);
for j = 1:i-1
r = q(:,j)’*q(:,i);
q(:,i) = q(:,i) - r*q(:,j);
end
r = sqrt(q(:,i)’*q(:,i));
q(:,i) = q(:,i)/r;
end
end
If we apply the above function to the matrix


1 1 1
1 0 1
1 1 0

 ,
12.4. EXISTENCE AND CONSTRUCTION OF ORTHONORMAL BASES 463
the ouput is the matrix


0
0
0
.
.
.
5774 0
5774
5774 0
−0
.
.
4082 0
4082
.8165 −
−
0
0
.7071
.
.
0000
7071

 ,
which matches the result of Example 12.9.
Example 12.11. If we consider polynomials and the inner product
h
f, gi =
Z
1
−1
f(t)g(t)dt,
applying the Gram–Schmidt orthonormalization procedure to the polynomials
1, x, x2
, . . . , xn
, . . . ,
which form a basis of the polynomials in one variable with real coefficients, we get a family
of orthonormal polynomials Qn(x) related to the Legendre polynomials.
The Legendre polynomials Pn(x) have many nice properties. They are orthogonal, but
their norm is not always 1. The Legendre polynomials Pn(x) can be defined as follows.
Letting fn be the function
fn(x) = (x
2 − 1)n
,
we define Pn(x) as follows:
P0(x) = 1, and Pn(x) = 1
2
nn!
f
(n)
n
(x),
where fn
(n)
is the nth derivative of fn.
They can also be defined inductively as follows:
P0(x) = 1,
P1(x) = x,
Pn+1(x) = 2n + 1
n + 1
xPn(x) −
n
n + 1
Pn−1(x).
Here is an explicit summation for Pn(x):
Pn(x) = 1
2
n
b
X
n/2c
k=0
(−1)k
 n
k

2n −
n
2k

x
n−2k
.
The polynomials Qn are related to the Legendre polynomials Pn as follows:
Qn(x) = r 2n
2
+ 1Pn(x).
464 CHAPTER 12. EUCLIDEAN SPACES
Example 12.12. Consider polynomials over [−1, 1], with the symmetric bilinear form
h
f, gi =
Z
1
−1
1
√
1 − t
2
f(t)g(t)dt.
We leave it as an exercise to prove that the above defines an inner product. It can be shown
that the polynomials Tn(x) given by
Tn(x) = cos(n arccos x), n ≥ 0,
(equivalently, with x = cos θ, we have Tn(cos θ) = cos(nθ)) are orthogonal with respect to
the above inner product. These polynomials are the Chebyshev polynomials. Their norm is
not equal to 1. Instead, we have
h
Tn, Tni =
(
π
2
if n > 0,
π if n = 0.
Using the identity (cos θ + isin θ)
n = cos nθ + isin nθ and the binomial formula, we obtain
the following expression for Tn(x):
Tn(x) =
b
n/2c
X
k=0

2
n
k

(x
2 − 1)kx
n−2k
.
The Chebyshev polynomials are defined inductively as follows:
T0(x) = 1
T1(x) = x
Tn+1(x) = 2xTn(x) − Tn−1(x), n ≥ 1.
Using these recurrence equations, we can show that
Tn(x) = (x −
√
x
2 − 1)n + (x +
√
x
2 − 1)n
2
.
The polynomial Tn has n distinct roots in the interval [−1, 1]. The Chebyshev polynomials
play an important role in approximation theory. They are used as an approximation to a
best polynomial approximation of a continuous function under the sup-norm (∞-norm).
The inner products of the last two examples are special cases of an inner product of the
form
h
f, gi =
Z
1
−1
W(t)f(t)g(t)dt,
where W(t) is a weight function. If W is a continuous function such that W(x) > 0 on
(−1, 1), then the above bilinear form is indeed positive definite. Families of orthogonal
12.5. LINEAR ISOMETRIES (ORTHOGONAL TRANSFORMATIONS) 465
polynomials used in approximation theory and in physics arise by a suitable choice of the
weight function W. Besides the previous two examples, the Hermite polynomials correspond
to W(x) = e
−x
2
, the Laguerre polynomials to W(x) = e
−x
, and the Jacobi polynomials
to W(x) = (1 − x)
α
(1 + x)
β
, with α, β > −1. Comprehensive treatments of orthogonal
polynomials can be found in Lebedev [114], Sansone [144], and Andrews, Askey and Roy [3].
We can also prove the following proposition regarding orthogonal spaces.
Proposition 12.11. Given any nontrivial Euclidean space E of finite dimension n ≥ 1, for
any subspace F of dimension k, the orthogonal complement F
⊥ of F has dimension n − k,
and E = F ⊕ F
⊥. Furthermore, we have F
⊥⊥ = F.
Proof. From Proposition 12.9, the subspace F has some orthonormal basis (u1, . . . , uk). This
linearly independent family (u1, . . . , uk) can be extended to a basis (u1, . . . , uk, vk+1, . . . , vn),
and by Proposition 12.10, it can be converted to an orthonormal basis (u1, . . . , un), which
contains (u1, . . . , uk) as an orthonormal basis of F. Now any vector w = w1u1+· · ·+wnun ∈ E
is orthogonal to F iff w · ui = 0, for every i, where 1 ≤ i ≤ k, iff wi = 0 for every i, where
1 ≤ i ≤ k. Clearly, this shows that (uk+1, . . . , un) is a basis of F
⊥, and thus E = F ⊕F
⊥, and
F
⊥ has dimension n − k. Similarly, any vector w = w1u1 + · · · + wnun ∈ E is orthogonal to
F
⊥ iff w · ui = 0, for every i, where k + 1 ≤ i ≤ n, iff wi = 0 for every i, where k + 1 ≤ i ≤ n.
Thus, (u1, . . . , uk) is a basis of F
⊥⊥, and F
⊥⊥ = F.
12.5 Linear Isometries (Orthogonal Transformations)
In this section we consider linear maps between Euclidean spaces that preserve the Euclidean
norm. These transformations, sometimes called rigid motions, play an important role in
geometry.
Definition 12.5. Given any two nontrivial Euclidean spaces E and F of the same finite
dimension n, a function f : E → F is an orthogonal transformation, or a linear isometry, if
it is linear and
k
f(u)k = k uk , for all u ∈ E.
Remarks:
(1) A linear isometry is often defined as a linear map such that
k
f(v) − f(u)k = k v − uk ,
for all u, v ∈ E. Since the map f is linear, the two definitions are equivalent. The
second definition just focuses on preserving the distance between vectors.
(2) Sometimes, a linear map satisfying the condition of Definition 12.5 is called a metric
map, and a linear isometry is defined as a bijective metric map.
466 CHAPTER 12. EUCLIDEAN SPACES
An isometry (without the word linear) is sometimes defined as a function f : E → F (not
necessarily linear) such that
k
f(v) − f(u)k = k v − uk ,
for all u, v ∈ E, i.e., as a function that preserves the distance. This requirement turns out to
be very strong. Indeed, the next proposition shows that all these definitions are equivalent
when E and F are of finite dimension, and for functions such that f(0) = 0.
Proposition 12.12. Given any two nontrivial Euclidean spaces E and F of the same finite
dimension n, for every function f : E → F, the following properties are equivalent:
(1) f is a linear map and k f(u)k = k uk , for all u ∈ E;
(2) k f(v) − f(u)k = k v − uk , for all u, v ∈ E, and f(0) = 0;
(3) f(u) · f(v) = u · v, for all u, v ∈ E.
Furthermore, such a map is bijective.
Proof. Clearly, (1) implies (2), since in (1) it is assumed that f is linear.
Assume that (2) holds. In fact, we shall prove a slightly stronger result. We prove that
if
k
f(v) − f(u)k = k v − uk
for all u, v ∈ E, then for any vector τ ∈ E, the function g : E → F defined such that
g(u) = f(τ + u) − f(τ )
for all u ∈ E is a map satisfying Condition (2), and that (2) implies (3). Clearly, g(0) =
f(τ ) − f(τ ) = 0.
Note that from the hypothesis
k
f(v) − f(u)k = k v − uk
for all u, v ∈ E, we conclude that
k
g(v) − g(u)k = k f(τ + v) − f(τ ) − (f(τ + u) − f(τ ))k ,
= k f(τ + v) − f(τ + u)k ,
= k τ + v − (τ + u)k ,
= k v − uk ,
for all u, v ∈ E. Since g(0) = 0, by setting u = 0 in
k
g(v) − g(u)k = k v − uk ,
12.5. LINEAR ISOMETRIES (ORTHOGONAL TRANSFORMATIONS) 467
we get
k
g(v)k = k vk
for all v ∈ E. In other words, g preserves both the distance and the norm.
To prove that g preserves the inner product, we use the simple fact that
2u · v = k uk
2 + k vk
2 − ku − vk
2
for all u, v ∈ E. Then since g preserves distance and norm, we have
2g(u) · g(v) = k g(u)k
2 + k g(v)k
2 − kg(u) − g(v)k
2
= k uk
2 + k vk
2 − ku − vk
2
= 2u · v,
and thus g(u)· g(v) = u· v, for all u, v ∈ E, which is (3). In particular, if f(0) = 0, by letting
τ = 0, we have g = f, and f preserves the scalar product, i.e., (3) holds.
Now assume that (3) holds. Since E is of finite dimension, we can pick an orthonormal
basis (e1, . . . , en) for E. Since f preserves inner products, (f(e1), . . ., f(en)) is also orthonor￾mal, and since F also has dimension n, it is a basis of F. Then note that since (e1, . . . , en)
and (f(e1), . . . , f(en)) are orthonormal bases, for any u ∈ E we have
u =
nX
i=1
(u · ei)ei =
nX
i=1
uiei
and
f(u) =
nX
i=1
(f(u) · f(ei))f(ei),
and since f preserves inner products, this shows that
f(u) =
nX
i=1
(f(u) · f(ei))f(ei) =
nX
i=1
(u · ei)f(ei) =
nX
i=1
uif(ei),
which proves that f is linear. Obviously, f preserves the Euclidean norm, and (3) implies
(1).
Finally, if f(u) = f(v), then by linearity f(v − u) = 0, so that k f(v − u)k = 0, and since
f preserves norms, we must have k v − uk = 0, and thus u = v. Thus, f is injective, and
since E and F have the same finite dimension, f is bijective.
Remarks:
(i) The dimension assumption is needed only to prove that (3) implies (1) when f is not
known to be linear, and to prove that f is surjective, but the proof shows that (1)
implies that f is injective.
468 CHAPTER 12. EUCLIDEAN SPACES
(ii) The implication that (3) implies (1) holds if we also assume that f is surjective, even
if E has infinite dimension.
In (2), when f does not satisfy the condition f(0) = 0, the proof shows that f is an affine
map. Indeed, taking any vector τ as an origin, the map g is linear, and
f(τ + u) = f(τ ) + g(u) for all u ∈ E.
By Proposition 24.7, this shows that f is affine with associated linear map g.
This fact is worth recording as the following proposition.
Proposition 12.13. Given any two nontrivial Euclidean spaces E and F of the same finite
dimension n, for every function f : E → F, if
k
f(v) − f(u)k = k v − uk for all u, v ∈ E,
then f is an affine map, and its associated linear map g is an isometry.
In view of Proposition 12.12, we usually abbreviate “linear isometry” as “isometry,”
unless we wish to emphasize that we are dealing with a map between vector spaces.
We are now going to take a closer look at the isometries f : E → E of a Euclidean space
of finite dimension.
12.6 The Orthogonal Group, Orthogonal Matrices
In this section we explore some of the basic properties of the orthogonal group and of
orthogonal matrices.
Proposition 12.14. Let E be any Euclidean space of finite dimension n, and let f : E → E
be any linear map. The following properties hold:
(1) The linear map f : E → E is an isometry iff
f ◦ f
∗ = f
∗
◦ f = id.
(2) For every orthonormal basis (e1, . . . , en) of E, if the matrix of f is A, then the matrix
of f
∗
is the transpose A> of A, and f is an isometry iff A satisfies the identities
A A> = A
> A = In,
where In denotes the identity matrix of order n, iff the columns of A form an orthonor￾mal basis of R
n
, iff the rows of A form an orthonormal basis of R
n
.
12.6. THE ORTHOGONAL GROUP, ORTHOGONAL MATRICES 469
Proof. (1) The linear map f : E → E is an isometry iff
f(u) · f(v) = u · v,
for all u, v ∈ E, iff
f
∗
(f(u)) · v = f(u) · f(v) = u · v
for all u, v ∈ E, which implies
(f
∗
(f(u)) − u) · v = 0
for all u, v ∈ E. Since the inner product is positive definite, we must have
f
∗
(f(u)) − u = 0
for all u ∈ E, that is,
f
∗
◦ f = id.
But an endomorphism f of a finite-dimensional vector space that has a left inverse is an
isomorphism, so f ◦ f
∗ = id. The converse is established by doing the above steps backward.
(2) If (e1, . . . , en) is an orthonormal basis for E, let A = (ai j ) be the matrix of f, and let
B = (bi j ) be the matrix of f
∗
. Since f
∗
is characterized by
f
∗
(u) · v = u · f(v)
for all u, v ∈ E, using the fact that if w = w1e1 + · · · + wnen we have wk = w · ek for all k,
1 ≤ k ≤ n, letting u = ei and v = ej
, we get
bj i = f
∗
(ei) · ej = ei
· f(ej ) = ai j ,
for all i, j, 1 ≤ i, j ≤ n. Thus, B = A> . Now if X and Y are arbitrary matrices over the
basis (e1, . . . , en), denoting as usual the jth column of X by Xj
, and similarly for Y , a simple
calculation shows that
X
> Y = (X
i
· Y
j
)1≤i,j≤n.
Then it is immediately verified that if X = Y = A, then
A
> A = A A> = In
iff the column vectors (A1
, . . . , An
) form an orthonormal basis. Thus, from (1), we see that
(2) is clear (also because the rows of A are the columns of A> ).
Proposition 12.14 shows that the inverse of an isometry f is its adjoint f
∗
. Recall that
the set of all real n × n matrices is denoted by Mn(R). Proposition 12.14 also motivates the
following definition.
Definition 12.6. A real n × n matrix is an orthogonal matrix if
A A> = A
> A = In.
470 CHAPTER 12. EUCLIDEAN SPACES
Remark: It is easy to show that the conditions A A> = In, A> A = In, and A−1 = A> , are
equivalent.
Given any two orthonormal bases (u1, . . . , un) and (v1, . . . , vn), if P is the change of
basis matrix from (u1, . . . , un) to (v1, . . . , vn), since the columns of P are the coordinates
of the vectors vj with respect to the basis (u1, . . . , un), if vj1 =
P
n
i1=1 pi1j1 ui1 and vj2 = P
n
i2=1 pi2j2 ui2
, since (u1, . . . , un) is orthonormal,
vj1
· vj2 =
nX
i1=1
nX
i2=1
pi1j1 pi2j2
(ui1
· ui2
) =
nX
i=1
pij1 pij2
,
and since (v1, . . . , vn) is orthonormal, vj1
· vj2 = δj1 j2
, so the columns of P are orthonormal,
and by Proposition 12.14 (2), the matrix P is orthogonal.
The proof of Proposition 12.12 (3) also shows that if f is an isometry, then the image of an
orthonormal basis (u1, . . . , un) is an orthonormal basis. Students often ask why orthogonal
matrices are not called orthonormal matrices, since their columns (and rows) are orthonormal
bases! I have no good answer, but isometries do preserve orthogonality, and orthogonal
matrices correspond to isometries.
Recall that the determinant det(f) of a linear map f : E → E is independent of the
choice of a basis in E. Also, for every matrix A ∈ Mn(R), we have det(A) = det(A> ), and
for any two n × n matrices A and B, we have det(AB) = det(A) det(B). Then if f is an
isometry, and A is its matrix with respect to any orthonormal basis, A A> = A> A = In
implies that det(A)
2 = 1, that is, either det(A) = 1, or det(A) = −1. It is also clear that
the isometries of a Euclidean space of dimension n form a group, and that the isometries of
determinant +1 form a subgroup. This leads to the following definition.
Definition 12.7. Given a Euclidean space E of dimension n, the set of isometries f : E → E
forms a subgroup of GL(E) denoted by O(E), or O(n) when E = R
n
, called the orthogonal
group (of E). For every isometry f, we have det(f) = ±1, where det(f) denotes the deter￾minant of f. The isometries such that det(f) = 1 are called rotations, or proper isometries,
or proper orthogonal transformations, and they form a subgroup of the special linear group
SL(E) (and of O(E)), denoted by SO(E), or SO(n) when E = R
n
, called the special or￾thogonal group (of E). The isometries such that det(f) = −1 are called improper isometries,
or improper orthogonal transformations, or flip transformations.
12.7 The Rodrigues Formula
When n = 3 and A is a skew symmetric matrix, it is possible to work out an explicit formula
for e
A. For any 3 × 3 real skew symmetric matrix
A =


−
0
c
b a
−
0
c b
−
0
a

 ,
12.7. THE RODRIGUES FORMULA 471
if we let θ =
√
a
2 + b
2 + c
2 and
B =

ab b
a
2 ab ac
2
bc
ac bc c2

 ,
then we have the following result known as Rodrigues’ formula (1840). The (real) vector
space of n × n skew symmetric matrices is denoted by so(n).
Proposition 12.15. The exponential map exp: so(3) → SO(3) is given by
e
A = cos θ I3 +
sin θ
θ
A +
(1 − cos θ)
θ
2
B,
or, equivalently, by
e
A = I3 +
sin θ
θ
A +
(1 − cos θ)
θ
2
A
2
if θ 6 = 0, with e
03 = I3.
Proof sketch. First observe that
A
2 = −θ
2
I3 + B,
since
A
2 =


−
0
c
b a
−
0
c b
−
0
a




−
0
c
b a
−
0
c b
−
0
a

 =


−c
2
ac cb
ab
− b
2
−c
2
ba ca
− a
2
−b
2
cb
− a
2


=


−a
2 −
0 0
0
b
2 − c
2
−a
2 −
0 0
b
2 − c
2
−a
2 −
0
b
2 − c
2

 +

ab b
a
2
ba ca
2
cb
ac cb c2


= −θ
2
I3 + B,
and that
AB = BA = 0.
From the above, deduce that
A
3 = −θ
2A,
and for any k ≥ 0,
A
4k+1 = θ
4kA,
A
4k+2 = θ
4kA
2
,
A
4k+3 = −θ
4k+2A,
A
4k+4 = −θ
4k+2A
2
.
472 CHAPTER 12. EUCLIDEAN SPACES
Then prove the desired result by writing the power series for e
A and regrouping terms so
that the power series for cos θ and sin θ show up. In particular
e
A = I3 +
X
p≥1
Ap
p!
= I3 +
X
p≥0
A2p+1
(2p + 1)! +
X
p≥1
A2p
(2p)!
= I3 +
X
p≥0
(−1)p
θ
2p
(2p + 1)!A +
X
p≥1
(−1)p−1
θ
2(p−1)
(2p)! A
2
= I3 +
A
θ
X
p≥0
(−1)p
θ
2p+1
(2p + 1)! −
A2
θ
2
X
p≥1
(−1)p
θ
2p
(2p)!
= I3 +
sin θ
θ
A −
A2
θ
2
X
p≥0
(−1)p
θ
2p
(2p)! +
A2
θ
2
= I3 +
sin θ
θ
A +
(1 − cos θ)
θ
2
A
2
,
as claimed.
The above formulae are the well-known formulae expressing a rotation of axis specified
by the vector (a, b, c) and angle θ.
The Rodrigues formula can used to show that the exponential map exp: so(3) → SO(3)
is surjective.
Given any rotation matrix R ∈ SO(3), we have the following cases:
(1) The case R = I is trivial.
(2) If R 6 = I and tr(R) 6 = −1, then
exp−1
(R) = 
2 sin
θ
θ
(R − R
T
)



 1 + 2 cos θ = tr(R)
 .
(Recall that tr(R) = r1 1 + r2 2 + r3 3, the trace of the matrix R).
Then there is a unique skew-symmetric B with corresponding θ satisfying 0 < θ < π
such that e
B = R.
(3) If R 6 = I and tr(R) = −1, then R is a rotation by the angle π and things are more
complicated, but a matrix B can be found. We leave this part as a good exercise: see
Problem 17.8.
The computation of a logarithm of a rotation in SO(3) as sketched above has applications
in kinematics, robotics, and motion interpolation.
As an immediate corollary of the Gram–Schmidt orthonormalization procedure, we obtain
the QR-decomposition for invertible matrices.
12.8. QR-DECOMPOSITION FOR INVERTIBLE MATRICES 473
12.8 QR-Decomposition for Invertible Matrices
Now that we have the definition of an orthogonal matrix, we can explain how the Gram–
Schmidt orthonormalization procedure immediately yields the QR-decomposition for matri￾ces.
Definition 12.8. Given any real n × n matrix A, a QR-decomposition of A is any pair of
n×n matrices (Q, R), where Q is an orthogonal matrix and R is an upper triangular matrix
such that A = QR.
Note that if A is not invertible, then some diagonal entry in R must be zero.
Proposition 12.16. Given any real n × n matrix A, if A is invertible, then there is an
orthogonal matrix Q and an upper triangular matrix R with positive diagonal entries such
that A = QR.
Proof. We can view the columns of A as vectors A1
, . . . , An
in E
n
. If A is invertible, then
they are linearly independent, and we can apply Proposition 12.10 to produce an orthonor￾mal basis using the Gram–Schmidt orthonormalization procedure. Recall that we construct
vectors Qk and Q
0 k as follows:
Q
0
1 = A
1
, Q1 =
Q
0 1
k
Q
0 1k
,
and for the inductive step
Q
0
k+1 = A
k+1 −
k
X
i=1
(A
k+1
· Q
i
) Q
i
