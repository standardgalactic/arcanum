−n
,
with αn ∈ {0, 1}.
(2) Does the result in (1) hold if U is not closed?
40.5. PROBLEMS 1485
Problem 40.5. Prove that the function f with domain dom(f) = R − {0} given by f(x) =
1/x2 has the property that f
00 (x) > 0 for all x ∈ dom(f), but it is not convex. Why isn’t
Proposition 40.12 applicable?
Problem 40.6. (1) Prove that the function x 7→ e
ax (on R) is convex for any a ∈ R.
(2) Prove that the function x 7→ x
a
is convex on {x ∈ R | x > 0}, for all a ∈ R such that
a ≤ 0 or a ≥ 1.
Problem 40.7. (1) Prove that the function x 7→ |x|
p
is convex on R for all p ≥ 1.
(2) Prove that the function x 7→ log x is concave on {x ∈ R | x > 0}.
(3) Prove that the function x 7→ x log x is convex on {x ∈ R | x > 0}.
Problem 40.8. (1) Prove that the function f given by f(x1, . . . , xn) = max{x1, . . . , xn} is
convex on R
n
.
(2) Prove that the function g given by g(x1, . . . , xn) = log(e
x1 + · · · + e
xn ) is convex on
R
n
.
Prove that
max{x1, . . . , xn} ≤ g(x1, . . . , xn) ≤ max{x1, . . . , xn} + log n.
Problem 40.9. In Problem 39.6, it was shown that
dfA(X) = tr(A
−1X)
D
2
f(A)(X1, X2) = −tr(A
−1X1A
−1X2),
for all n × n real matrices X, X1, X2, where f is the function defined on GL+
(n, R) (the
n × n real invertible matrices of positive determinants), given by
f(A) = log det(A).
Assume that A is symmetric positive definite and that X is symmetric.
(1) Prove that the eigenvalues of A−1X are real (even though A−1X may not be sym￾metric).
Hint. Since A is symmetric positive definite, then so is A−1
, so we can write A−1 = S
2
for
some symmetric positive definite matrix S, and then
A
−1X = S
2X = S(SXS)S
−1
.
(2) Prove that the eigenvalues of (A−1X)
2 are nonnegative. Deduce that
D
2
f(A)(X, X) = −tr((A
−1X)
2
) < 0
for all nonzero symmetric matrices X and SPD matrices A. Conclude that the function
X 7→ log det X is strictly concave on the set of symmetric positive definite matrices.
1486 CHAPTER 40. EXTREMA OF REAL-VALUED FUNCTIONS
Chapter 41
Newton’s Method and Its
Generalizations
In Chapter 40 we investigated the problem of determining when a function J : Ω → R defined
on some open subset Ω of a normed vector space E has a local extremum. Proposition 40.1
gives a necessary condition when J is differentiable: if J has a local extremum at u ∈ Ω,
then we must have
J
0 (u) = 0.
Thus we are led to the problem of finding the zeros of the derivative
J
0 : Ω → E
0 ,
where E
0 = L(E; R) is the set of linear continuous functions from E to R; that is, the dual
of E, as defined in the remark after Proposition 40.8.
This leads us to consider the problem in a more general form, namely, given a function
f : Ω → Y from an open subset Ω of a normed vector space X to a normed vector space Y ,
find
(i) Sufficient conditions which guarantee the existence of a zero of the function f; that is,
an element a ∈ Ω such that f(a) = 0.
(ii) An algorithm for approximating such an a, that is, a sequence (xk) of points of Ω whose
limit is a.
In this chapter we discuss Newton’s method and some of it generalizations to give (partial)
answers to Problems (i) and (i).
41.1 Newton’s Method for Real Functions of a Real
Argument
When X = Y = R, we can use Newton’s method to find a zero of a function f : Ω → R. We
pick some initial element x0 ∈ R “close enough” to a zero a of f, and we define the sequence
1487
1488 CHAPTER 41. NEWTON’S METHOD AND ITS GENERALIZATIONS
(xk) by
xk+1 = xk −
f(xk)
f
0 (xk)
,
for all k ≥ 0, provided that f
0 (xk) 6 = 0. The idea is to define xk+1 as the intersection of the
x-axis with the tangent line to the graph of the function x 7→ f(x) at the point (xk, f(xk)).
Indeed, the equation of this tangent line is
y − f(xk) = f
0 (xk)(x − xk),
and its intersection with the x-axis is obtained for y = 0, which yields
x = xk −
f(xk)
f
0 (xk)
,
as claimed. See Figure 41.1. x k
( xk , f( ) xk )
xk+1
( xk+1 , f( ) xk+1 )
xk+2
Figure 41.1: The construction of two stages in Newton’s method.
Example 41.1. If α > 0 and f(x) = x
2 − α, Newton’s method yields the sequence
xk+1 =
1
2

xk +
x
α
k

to compute the square root √
α of α. It can be shown that the method converges to √
α for
any x0 > 0; see Problem 41.1. Actually, the method also converges when x0 < 0! Find out
what is the limit.
41.2. GENERALIZATIONS OF NEWTON’S METHOD 1489
The case of a real function suggests the following method for finding the zeros of a
function f : Ω → Y , with Ω ⊆ X: given a starting point x0 ∈ Ω, the sequence (xk) is defined
by
xk+1 = xk − (f
0 (xk))−1
(f(xk)) (∗)
for all k ≥ 0.
For the above to make sense, it must be ensured that
(1) All the points xk remain within Ω.
(2) The function f is differentiable within Ω.
(3) The derivative f
0 (x) is a bijection from X to Y for all x ∈ Ω.
These are rather demanding conditions but there are sufficient conditions that guarantee
that they are met. Another practical issue is that it may be very costly to compute (f
0 (xk))−1
at every iteration step. In the next section we investigate generalizations of Newton’s method
which address the issues that we just discussed.
41.2 Generalizations of Newton’s Method
Suppose that f : Ω → R
n
is given by n functions fi
: Ω → R, where Ω ⊆ R
n
. In this case,
finding a zero a of f is equivalent to solving the system
f1(a1 . . . , an) = 0
f2(a1 . . . , an) = 0
.
.
.
fn(a1 . . . , an) = 0.
In the standard Newton method, the iteration step is given by (∗), namely
xk+1 = xk − (f
0 (xk))−1
(f(xk)),
and if we define ∆xk as ∆xk = xk+1 − xk, we see that ∆xk = −(f
0 (xk))−1
(f(xk)), so ∆xk is
obtained by solving the equation
f
0 (xk)∆xk = −f(xk),
and then we set xk+1 = xk + ∆xk.
The generalization is as follows.
Variant 1. A single iteration of Newton’s method consists in solving the linear system
(J(f)(xk))∆xk = −f(xk),
1490 CHAPTER 41. NEWTON’S METHOD AND ITS GENERALIZATIONS
and then setting
xk+1 = xk + ∆xk,
where J(f)(xk) =  ∂fi
∂xj
(xk)
 is the Jacobian matrix of f at xk.
In general it is very costly to compute J(f)(xk) at each iteration and then to solve the
corresponding linear system. If the method converges, the consecutive vectors xk should
differ only a little, as also the corresponding matrices J(f)(xk). Thus, we are led to several
variants of Newton’s method.
Variant 2. This variant consists in keeping the same matrix for p consecutive steps (where
p is some fixed integer ≥ 2):
xk+1 = xk − (f
0 (x0))−1
(f(xk)), 0 ≤ k ≤ p − 1
xk+1 = xk − (f
0 (xp))−1
(f(xk)), p ≤ k ≤ 2p − 1
.
.
.
xk+1 = xk − (f
0 (xrp))−1
(f(xk)), rp ≤ k ≤ (r + 1)p − 1
.
.
.
Variant 3. Set p = ∞, that is, use the same matrix f
0 (x0) for all iterations, which leads
to iterations of the form
xk+1 = xk − (f
0 (x0))−1
(f(xk)), k ≥ 0,
Variant 4. Replace f
0 (x0) by a particular matrix A0 which is easy to invert:
xk+1 = xk − A
−
0
1
f(xk), k ≥ 0.
In the last two cases, if possible, we use an LU factorization of f
0 (x0) or A0 to speed up the
method. In some cases, it may even possible to set A0 = I.
The above considerations lead us to the definition of a generalized Newton method, as in
Ciarlet [41] (Chapter 7). Recall that a linear map f ∈ L(E; F) is called an isomorphism iff
f is continuous, bijective, and f
−1
is also continuous.
Definition 41.1. If X and Y are two normed vector spaces and if f : Ω → Y is a function
from some open subset Ω of X, a generalized Newton method for finding zeros of f consists
of
(1) A sequence of families (Ak(x)) of linear isomorphisms from X to Y , for all x ∈ Ω and
all integers k ≥ 0;
(2) Some starting point x0 ∈ Ω;
41.2. GENERALIZATIONS OF NEWTON’S METHOD 1491
(3) A sequence (xk) of points of Ω defined by
xk+1 = xk − (Ak(x` ))−1
(f(xk)), k ≥ 0, (∗∗)
where for every integer k ≥ 0, the integer ` satisfies the condition
0 ≤ ` ≤ k.
With ∆xk = xk+1 − xk, Equation (∗∗) is equivalent to solving the equation
Ak(x` )(∆xk) = −f(xk)
and setting xk+1 = xk + ∆xk. The function Ak(x) usually depends on f
0 .
Definition 41.1 gives us enough flexibility to capture all the situations that we have
previously discussed:
Function Index
Variant 1: Ak(x) = f
0 (x), ` = k
Variant 2: Ak(x) = f
0 (x), ` = min{rp, k}, if rp ≤ k ≤ (r + 1)p − 1, r ≥ 0
Variant 3: Ak(x) = f
0 (x), ` = 0
Variant 4: Ak(x) = A0,
where A0 is a linear isomorphism from X to Y . The first case corresponds to Newton’s
original method and the others to the variants that we just discussed. We could also have
Ak(x) = Ak, a fixed linear isomorphism independent of x ∈ Ω.
Example 41.2. Consider the matrix function f given by
f(X) = A − X
−1
,
with A and X invertible n × n matrices. If we apply Variant 1 of Newton’s method starting
with any n × n matrix X0, since the derivative of the function g given by g(X) = X−1
is
dgX(Y ) = −X−1Y X−1
, we have
fX
0(Y ) = X
−1Y X−1
,
so
(fX
0)
−1
(Y ) = XY X
and the Newton step is
Xk+1 = Xk − (fX
0k
)
−1
(f(Xk)) = Xk − Xk(A − Xk
−1
)Xk,
which yields the sequence (Xk) with
Xk+1 = Xk(2I − AXk), k ≥ 0.
In Problem 41.5, it is shown that Newton’s method converges to A−1
iff the spectral radius
of I − X0A is strictly smaller than 1, that is, ρ(I − X0A) < 1.
1492 CHAPTER 41. NEWTON’S METHOD AND ITS GENERALIZATIONS
The following theorem inspired by the Newton–Kantorovich theorem gives sufficient con￾ditions that guarantee that the sequence (xk) constructed by a generalized Newton method
converges to a zero of f close to x0. Although quite technical, these conditions are not very
surprising.
Theorem 41.1. Let X be a Banach space, let f : Ω → Y be differentiable on the open subset
Ω ⊆ X, and assume that there are constants r, M, β > 0 such that if we let
B = {x ∈ X | kx − x0k ≤ r} ⊆ Ω,
then
(1)
sup
k≥0
sup
x∈B


A
−
k
1
(x)

L(Y ;X)
≤ M,
(2) β < 1 and
sup
k≥0
sup
x,x0 ∈B
k
f
0 (x) − Ak(x
0 )k L(X;Y ) ≤
β
M
(3)
k
f(x0)k ≤ r
M
(1 − β).
Then the sequence (xk) defined by
xk+1 = xk − A
−
k
1
(x` )(f(xk)), 0 ≤ ` ≤ k
is entirely contained within B and converges to a zero a of f, which is the only zero of f in
B. Furthermore, the convergence is geometric, which means that
k
xk − ak ≤ k x1 − x0k
1 − β
β
k
.
Proof. We follow Ciarlet [41] (Theorem 7.5.1, Section 7.5). The proof has three steps.
Step 1. We establish the following inequalities for all k ≥ 1.
k
xk − xk−1k ≤ M k f(xk−1)k (a)
k
xk − x0k ≤ r (xk ∈ B) (b)
k
f(xk)k ≤ β
M
k
xk − xk−1k . (c)
We proceed by induction on k, starting with the base case k = 1. Since
x1 = x0 − A
−
0
1
(x0)(f(x0)),
41.2. GENERALIZATIONS OF NEWTON’S METHOD 1493
we have x1 − x0 = −A
−
0
1
(x0)(f(x0)), so by (1) and (3) and since 0 < β < 1, we have
k
x1 − x0k ≤ M k f(x0)k ≤ r(1 − β) ≤ r,
establishing (a) and (b) for k = 1. We also have f(x0) = −A0(x0)(x1 − x0), so
−f(x0) − A0(x0)(x1 − x0) = 0 and thus
f(x1) = f(x1) − f(x0) − A0(x0)(x1 − x0).
By the mean value theorem (Proposition 39.12) applied to the function x 7→ f(x)−A0(x0)(x),
by (2), we get
k
f(x1)k ≤ sup
x∈B
k
f
0 (x) − A0(x0)k k x1 − x0k ≤ β
M
k
x1 − x0k ,
which is (c) for k = 1. We now establish the induction step.
Since by definition
xk − xk−1 = −A
−
k
1
−1
(x` )(f(xk−1)), 0 ≤ ` ≤ k − 1,
by (1) and the fact that by the induction hypothesis for (b), x` ∈ B, we get
k
xk − xk−1k ≤ M k f(xk−1)k ,
which proves (a) for k. As a consequence, since by the induction hypothesis for (c),
k
f(xk−1)k ≤ β
M
k
xk−1 − xk−2k ,
we get
k
xk − xk−1k ≤ M k f(xk−1)k ≤ β k xk−1 − xk−2k , (∗1)
and by repeating this step,
k
xk − xk−1k ≤ β
k−1
k x1 − x0k . (∗2)
Using (∗2) and (3), we obtain
k
xk − x0k ≤
k
X
j=1
k
xj − xj−1k ≤ 
k
X
j=1
β
j−1
!
k x1 − x0k
≤
k
x1 − x0k
1 − β
≤
M
1 − β
k
f(x0)k ≤ r,
which proves that xk ∈ B, which is (b) for k.
Since
xk − xk−1 = −A
−
k
1
−1
(x` )(f(xk−1))
1494 CHAPTER 41. NEWTON’S METHOD AND ITS GENERALIZATIONS
we also have f(xk−1) = −Ak−1(x` )(xk − xk−1), so we have
f(xk) = f(xk) − f(xk−1) − Ak−1(x` )(xk − xk−1),
and as in the base case, applying the mean value theorem (Proposition 39.12) to the function
x 7→ f(x) − Ak−1(x` )(x), by (2), we obtain
k
f(xk)k ≤ sup
x∈B
k
f
0 (x) − Ak−1(x` )k k xk − xk−1k ≤ β
M
k
xk − xk−1k ,
proving (c) for k.
Step 2. Prove that f has a zero in B.
To do this we prove that (xk) is a Cauchy sequence. This is because, using (∗2), we have
k
xk+j − xkk ≤
j−1
X
i=0
k
xk+i+1 − xk+ik ≤ β
k
 X
j
i=0
−1
β
i
!
k x1 − x0k
≤
β
k
1 − β
k
x1 − x0k ,
for all k ≥ 0 and all j ≥ 0, proving that (xk) is a Cauchy sequence. Since B is a closed ball
in a complete normed vector space, B is complete and the Cauchy sequence (xk) converges
to a limit a ∈ B. Since f is continuous on Ω (because it is differentiable), by (c) we obtain
k
f(a)k = lim
k7→∞
k
f(xk)k ≤ β
M
lim
k7→∞
k
xk − xk−1k = 0,
which yields f(a) = 0.
Since
k
xk+j − xkk ≤ β
k
1 − β
k
x1 − x0k ,
if we let j tend to infinity, we obtain the inequality
k
xk − ak = k a − xkk ≤ β
k
1 − β
k
x1 − x0k ,
which is the last statement of the theorem.
Step 3. Prove that f has a unique zero in B.
Suppose f(a) = f(b) = 0 with a, b ∈ B. Since A
−
0
1
(x0)(A0(x0)(b − a)) = b − a, we have
b − a = −A
−
0
1
(x0)(f(b) − f(a) − A0(x0)(b − a)),
which by (1) and (2) and the mean value theorem implies that
k
b − ak ≤
  A
−
0
1
(x0)
 sup
x∈B
k
f
0 (x) − A0(x0)k k b − ak ≤ β k b − ak .
Since 0 < β < 1, the inequality k b − ak ≤ β k b − ak is only possible if a = b.
41.2. GENERALIZATIONS OF NEWTON’S METHOD 1495
It should be observed that the conditions of Theorem 41.1 are typically quite stringent.
It can be shown that Theorem 41.1 applies to the function f of Example 41.1 given by
f(x) = x
2 − α with α > 0, for any x0 > 0 such that
6
7
α ≤ x
2
0 ≤
6
5
α,
with β = 2/5, r = (1/6)x0, M = 3/(5x0). Such values of x0 are quite close to √
α.
If we assume that we already know that some element a ∈ Ω is a zero of f, the next
theorem gives sufficient conditions for a special version of a generalized Newton method to
converge. For this special method the linear isomorphisms Ak(x) are independent of x ∈ Ω.
Theorem 41.2. Let X be a Banach space and let f : Ω → Y be differentiable on the open
subset Ω ⊆ X. If a ∈ Ω is a point such that f(a) = 0, if f
0 (a) is a linear isomorphism, and
if there is some λ with 0 < λ < 1/2 such that
sup
k≥0
k
Ak − f
0 (a)k L(X;Y ) ≤
λ
k
(f
0 (a))−1k
L(Y ;X)
,
then there is a closed ball B of center a such that for every x0 ∈ B, the sequence (xk) defined
by
xk+1 = xk − A
−
k
1
(f(xk)), k ≥ 0,
is entirely contained within B and converges to a, which is the only zero of f in B. Further￾more, the convergence is geometric, which means that
k
xk − ak ≤ β
k
k x0 − ak ,
for some β < 1.
A proof of Theorem 41.2 can be found in Ciarlet [41] (Section 7.5).
For the sake of completeness, we state a version of the Newton–Kantorovich theorem
which corresponds to the case where Ak(x) = f
0 (x). In this instance, a stronger result can
be obtained especially regarding upper bounds, and we state a version due to Gragg and
Tapia which appears in Problem 7.5-4 of Ciarlet [41].
Theorem 41.3. (Newton–Kantorovich) Let X be a Banach space, and let f : Ω → Y be
differentiable on the open subset Ω ⊆ X. Assume that there exist three positive constants
λ, µ, ν and a point x0 ∈ Ω such that
0 < λµν ≤
1
2
,
1496 CHAPTER 41. NEWTON’S METHOD AND ITS GENERALIZATIONS
and if we let
ρ
− =
1 −
√
1 − 2λµν
µν
ρ
+ =
1 + √
1 − 2λµν
µν
B = {x ∈ X | kx − x0k < ρ−}
Ω
+ = {x ∈ Ω | kx − x0k < ρ+},
then B ⊆ Ω, f
0 (x0) is an isomorphism of L(X; Y ), and
sup


(f
0 (x

0
(
))
f
0
−
(
1
x
f
0
(
))
x
−
0
1
)



≤
≤
µ,
λ,
x,y∈Ω+
k
f
0 (x) − f
0 (y)k ≤ ν k x − yk .
Then f
0 (x) is isomorphism of L(X; Y ) for all x ∈ B, and the sequence defined by
xk+1 = xk − (f
0 (xk))−1
(f(xk)), k ≥ 0
is entirely contained within the ball B and converges to a zero a of f which is the only zero
of f in Ω
+. Finally, if we write θ = ρ
−/ρ+, then we have the following bounds:
k
xk − ak ≤ 2
√
1 − 2λµν
λµν
θ
2k
1 − θ
2k
k
x1 − x0k if λµν < 1
2
k
xk − ak ≤ k x1
2
k
−
−1
x0k
if λµν =
1
2
,
and
1 + p (1 + 4
2 k xk+1
θ
2k
−
(1 +
xkk
θ
2k
)
−2
)
≤ kxk − ak ≤ θ
2k−1
k xk − xk−1k .
We can now specialize Theorems 41.1 and 41.2 to the search of zeros of the derivative
J
0 : Ω → E
0 , of a function J : Ω → R, with Ω ⊆ E. The second derivative J
00 of J is
a continuous bilinear form J
00 : E × E → R, but is is convenient to view it as a linear
map in L(E, E0 ); the continuous linear form J
00 (u) is given by J
00 (u)(v) = J
00 (u, v). In our
next theorem, which follows immediately from Theorem 41.1, we assume that the Ak(x) are
isomorphisms in L(E, E0 ).
Theorem 41.4. Let E be a Banach space, let J : Ω → R be twice differentiable on the open
subset Ω ⊆ E, and assume that there are constants r, M, β > 0 such that if we let
B = {x ∈ E | kx − x0k ≤ r} ⊆ Ω,
then
41.2. GENERALIZATIONS OF NEWTON’S METHOD 1497
(1)
sup
k≥0
sup
x∈B


A
−
k
1
(x)

L(E0 ;E)
≤ M,
(2) β < 1 and
sup
k≥0
sup
x,x0 ∈B
k
J
00 (x) − Ak(x
0 )k L(E;E0 ) ≤
β
M
(3)
k
J
0 (x0)k ≤ r
M
(1 − β).
Then the sequence (xk) defined by
xk+1 = xk − A
−
k
1
(x` )(J
0 (xk)), 0 ≤ ` ≤ k
is entirely contained within B and converges to a zero a of J
0 , which is the only zero of J
0
in B. Furthermore, the convergence is geometric, which means that
k
xk − ak ≤ k x1 − x0k
1 − β
β
k
.
In the next theorem, which follows immediately from Theorem 41.2, we assume that the
Ak(x) are isomorphisms in L(E, E0 ) that are independent of x ∈ Ω.
Theorem 41.5. Let E be a Banach space and let J : Ω → R be twice differentiable on the
open subset Ω ⊆ E. If a ∈ Ω is a point such that J
0 (a) = 0, if J
00 (a) is a linear isomorphism,
and if there is some λ with 0 < λ < 1/2 such that
sup
k≥0
k
Ak − J
00 (a)k L(E;E0 ) ≤
λ
k
(J
00 (a))−1k
L(E0 ;E)
,
then there is a closed ball B of center a such that for every x0 ∈ B, the sequence (xk) defined
by
xk+1 = xk − A
−
k
1
(J
0 (xk)), k ≥ 0,
is entirely contained within B and converges to a, which is the only zero of J
0 in B. Fur￾thermore, the convergence is geometric, which means that
k
xk − ak ≤ β
k
k x0 − ak ,
for some β < 1.
When E = R
n
, the Newton method given by Theorem 41.4 yields an iteration step of
the form
xk+1 = xk − A
−
k
1
(x` )∇J(xk), 0 ≤ ` ≤ k,
1498 CHAPTER 41. NEWTON’S METHOD AND ITS GENERALIZATIONS
where ∇J(xk) is the gradient of J at xk (here, we identify E
0 with R
n
). In particular,
Newton’s original method picks Ak = J
00 , and the iteration step is of the form
xk+1 = xk − (∇2
J(xk))−1∇J(xk), k ≥ 0,
where ∇2J(xk) is the Hessian of J at xk.
Example 41.3. Let us apply Newton’s original method to the function J given by J(x) =
1
3
x
3 − 4x. We have J
0 (x) = x
2 − 4 and J
00 (x) = 2x, so the Newton step is given by
xk+1 = xk −
x
2
k − 4
2xk
=
1
2

xk +
x
4
k

.
This is the sequence of Example 41.1 to compute the square root of 4. Starting with any
x0 > 0 it converges very quickly to 2.
As remarked in Ciarlet [41] (Section 7.5), generalized Newton methods have a very wide
range of applicability. For example, various versions of gradient descent methods can be
viewed as instances of Newton method. See Section 49.9 for an example.
Newton’s method also plays an important role in convex optimization, in particular,
interior-point methods. A variant of Newton’s method dealing with equality constraints has
been developed. We refer the reader to Boyd and Vandenberghe [29], Chapters 10 and 11,
for a comprehensive exposition of these topics.
41.3 Summary
The main concepts and results of this chapter are listed below:
• Newton’s method for functions f : R → R.
• Generalized Newton methods.
• The Newton-Kantorovich theorem.
41.4 Problems
Problem 41.1. If α > 0 and f(x) = x
2 − α, Newton’s method yields the sequence
xk+1 =
1
2

xk +
x
α
k

to compute the square root √
α of α.
41.4. PROBLEMS 1499
(1) Prove that if x0 > 0, then xk > 0 and
xk+1 −
√
α =
1
2xk
(xk −
√
α)
2
xk+2 − xk+1 =
1
2xk+1
(α − x
2
k+1)
for all k ≥ 0. Deduce that Newton’s method converges to √
α for any x0 > 0.
(2) Prove that if x0 < 0, then Newton’s method converges to −
√
α.
Problem 41.2. (1) If α > 0 and f(x) = x
2 − α, show that the conditions of Theorem 41.1
are satisfied by any β ∈ (0, 1) and any x0 such that
|x
2
0 − α| ≤ 4β(1 − β)
(β + 2)2
x
2
0
,
with
r =
β
β + 2
x0, M =
β + 2
4x0
.
(2) Prove that the maximum of the function defined on [0, 1] by
β 7→
4β(1 − β)
(β + 2)2
has a maximum for β = 2/5. For this value of β, check that r = (1/6)x0, M = 3/(5x0), and
6
7
α ≤ x
2
0 ≤
6
5
α.
Problem 41.3. Consider generalizing Problem 41.1 to the matrix function f given by
f(X) = X2 − C, where X and C are two real n × n matrices with C symmetric posi￾tive definite. The first step is to determine for which A does the inverse dfA
−1
exist. Let g be
the function given by g(X) = X2
. From Problem 39.1 we know that the derivative at A of
the function g is dgA(X) = AX + XA, and obviously dfA = dgA. Thus we are led to figure
out when the linear matrix map X 7→ AX + XA is invertible. This can be done using the
Kronecker product.
Given an m × n matrix A = (aij ) and a p × q matrix B = (bij ), the Kronecker product
(or tensor product) A ⊗ B of A and B is the mp × nq matrix
A ⊗ B =


a11B a12B · · · a1nB
a21
.
B a22B · · · a2nB
.
.
.
.
.
.
.
.
.
.
.
am1B am2B · · · amnB


.
1500 CHAPTER 41. NEWTON’S METHOD AND ITS GENERALIZATIONS
It can be shown (and you may use these facts without proof) that ⊗ is associative and that
(A ⊗ B)(C ⊗ D) = AC ⊗ BD
(A ⊗ B)
> = A
> ⊗ B
> ,
whenever AC and BD are well defined.
Given any n × n matrix X, let vec(X) be the vector in R
n
2
obtained by concatenating
the rows of X.
(1) Prove that AX = Y iff
(A ⊗ In)vec(X) = vec(Y )
and XA = Y iff
(In ⊗ A
> )vec(X) = vec(Y ).
Deduce that AX + XA = Y iff
((A ⊗ In) + (In ⊗ A
> ))vec(X) = vec(Y ).
In the case where n = 2 and if we write
A =

a b
c d ,
check that
A ⊗ I2 + I2 ⊗ A
> =


2a c b 0
c
b a +
0
d
a +
0
d c
b
0 c b 2d

 .
The problem is determine when the matrix (A ⊗ In) + (In ⊗ A> ) is invertible.
Remark: The equation AX + XA = Y is a special case of the equation AX + XB = C
(sometimes written AX − XB = C), called the Sylvester equation, where A is an m × m
matrix, B is an n × n matrix, and X, C are m × n matrices; see Higham [91] (Appendix B).
