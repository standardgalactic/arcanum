51.3. BASIC PROPERTIES OF SUBGRADIENTS AND SUBDIFFERENTIALS 1845
If f is a proper function on R, then its effective domain being convex is an interval whose
relative interior is an open interval (a, b). In Proposition 51.16, we can pick y = 1 so h y, ui =
u, and for any x ∈ (a, b), since the limits f−
0(x) = −f
0 (x; −1) and f+
0(x) = f
0 (x; 1) exist, with
f−
0(x) ≤ f+
0(x), we deduce that ∂f(x) = [f−
0(x), f+
0(x)]. The numbers α ∈ [f−
0(x), f+
0(x)] are
the slopes of nonvertical lines in R
2 passing through (x, f(x)) that are supporting lines to
the epigraph epi(f) of f.
Example 51.10. If f is the celebrated ReLU function (ramp function) from deep learning
defined so that
ReLU(x) = max{x, 0} =
(
0 if
x if
x <
x ≥
0
0,
then ∂ ReLU(0) = [0, 1]. See Figure 51.20. The function ReLU is differentiable for x 6 = 0,
with ReLU0 (x) = 0 if x < 0 and ReLU0 (x) = 1 if x > 0.
Figure 51.20: The graph of the ReLU function.
Proposition 51.16 has several interesting consequences.
Proposition 51.17. Let f : R
n → R ∪ {−∞, +∞} be a convex function. For any x ∈ R
n
, if
f(x) is finite and if f is subdifferentiable at x, then f is proper. If f is not subdifferentiable
at x, then there is some y 6 = 0 such that
f
0 (x; y) = −f
0 (x; −y) = −∞.
Proposition 51.17 is proven in Rockafellar [138] (Theorem 23.3). It confirms that im￾proper convex functions are rather pathological objects, because if a convex function is
subdifferentiable for some x such that f(x) is finite, then f must be proper. This is because
if f(x) is finite, then the subgradient inequality implies that f majorizes an affine function,
which is proper.
The next theorem is one of the most important results about the connection between one￾sided directional derivatives and subdifferentials. It sharpens the result of Theorem 51.14.
1846 CHAPTER 51. SUBGRADIENTS AND SUBDIFFERENTIALS ~
Theorem 51.18. Let f : R
n → R∪{+∞} be a proper convex function. For any x /∈ dom(f),
we have ∂f(x) = ∅. For any x ∈ relint(dom(f)), we have ∂f(x) 6 = ∅, the map y 7→ f
0 (x; y)
is convex, closed and proper, and
f
0 (x; y) = sup
u∈∂f(x)
h
y, ui = δ
∗
(y|∂f(x)) for all y ∈ R
n
.
The subdifferential ∂f(x) is nonempty and bounded (also closed and convex) if and only if
x ∈ int(dom(f)), in which case f
0 (x; y) is finite for all y ∈ R
n
.
Theorem 51.18 is proven in Rockafellar [138] (Theorem 23.4). If we write
dom(∂f) = {x ∈ R
n
| ∂f(x) 6 = ∅},
then Theorem 51.18 implies that
relint(dom(f)) ⊆ dom(∂f) ⊆ dom(f).
However, dom(∂f) is not necessarily convex as shown by the following counterexample.
Example 51.11. Consider the proper convex function defined on R
2 given by
f(x, y) = max{g(x), |y|},
where
g(x) = ( 1
+
−
∞
√
x
if
if
x <
x ≥
0
0
.
See Figure 51.21. It is easy to see that dom(f) = {(x, y) ∈ R
2
| x ≥ 0}, but
dom(∂f) = {(x, y) ∈ R
2
| x ≥ 0} − {(0, y) | −1 < y < 1}, which is not convex.
The following theorem is important because it tells us when a convex function is differ￾entiable in terms of its subdifferential, as shown in Rockafellar [138] (Theorem 25.1).
Theorem 51.19. Let f be a convex function on R
n
, and let x ∈ R
n
such that f(x) is finite.
If f is differentiable at x then ∂f(x) = {∇fx} (where ∇fx is the gradient of f at x) and we
have
f(z) ≥ f(x) + h z − x, ∇fxi for all z ∈ R
n
.
Conversely, if ∂f(x) consists of a single vector, then ∂f(x) = {∇fx} and f is differentiable
at x.
The first direction is easy to prove. Indeed, if f is differentiable at x, then
f
0 (x; y) = h y, ∇fxi for all y ∈ R
n
,
so by Proposition 51.16, a vector u is a subgradient at x iff
h
y, ∇fxi ≥ hy, ui for all y ∈ R
n
,
so h y, ∇fx − ui ≥ 0 for all y, which implies that u = ∇fx.
We obtain the following corollary.
51.3. BASIC PROPERTIES OF SUBGRADIENTS AND SUBDIFFERENTIALS 1847
Figure 51.21: The graph of the function from Example 51.11 with a view along the positive
x axis.
Corollary 51.20. Let f be a convex function on R
n
, and let x ∈ R
n
such that f(x) is finite.
If f is differentiable at x, then f is proper and x ∈ int(dom(f)).
The following theorem shows that proper convex functions are differentiable almost ev￾erywhere.
Theorem 51.21. Let f be a proper convex function on R
n
, and let D be the set of vectors
where f is differentiable. Then D is a dense subset of int(dom(f)), and its complement in
int(dom(f)) has measure zero. Furthermore, the gradient map x 7→ ∇fx is continuous on
D.
Theorem 51.21 is proven in Rockafellar [138] (Theorem 25.5).
Remark: If f : (a, b) → R is a finite convex function on an open interval of R, then the set
D where f is differentiable is dense in (a, b), and (a, b) − D is at most countable. The map
f
0 is continuous and nondecreasing on D. See Rockafellar [138] (Theorem 25.3).
We also have the following result showing that in “most cases” the subdifferential ∂f(x)
can be constructed from the gradient map; see Rockafellar [138] (Theorem 25.6).
Theorem 51.22. Let f be a closed proper convex function on R
n
. If int(dom(f)) 6 = ∅, then
for every x ∈ dom(f), we have
∂f(x) = conv(S(x)) + Ndom(f)(x)
where Ndom(f)(x) is the normal cone to dom(f) at x, and S(x) is the set of all limits of
sequences of the form ∇fx1
, ∇fx2
, . . . , ∇fxp
, . . ., where x1, x2, . . . , xp, . . . is a sequence in
dom(f) converging to x such that each ∇fxp
is defined.
1848 CHAPTER 51. SUBGRADIENTS AND SUBDIFFERENTIALS ~
The next two results generalize familiar results about derivatives to subdifferentials.
Proposition 51.23. Let f1, . . . , fn be proper convex functions on R
n
, and let f = f1+· · ·+fn.
For x ∈ R
n
, we have
∂f(x) ⊇ ∂f1(x) + · · · + ∂fn(x).
If T n
i=1 relint(dom(fi)) 6 = ∅, then
∂f(x) = ∂f1(x) + · · · + ∂fn(x).
Proposition 51.23 is proven in Rockafellar [138] (Theorem 23.8).
The next result can be viewed as a generalization of the chain rule.
Proposition 51.24. Let f be the function given by f(x) = h(Ax) for all x ∈ R
n
, where h
is a proper convex function on R
m and A is an m × n matrix. Then
∂f(x) ⊇ A
> (∂h(Ax)) for all x ∈ R
n
.
If the range of A contains a point of relint(dom(h)), then
∂f(x) = A
> (∂h(Ax)).
Proposition 51.24 is proven in Rockafellar [138] (Theorem 23.9).
51.4 Additional Properties of Subdifferentials
In general, if f : R
n → R is a function (not necessarily convex) and f is differentiable at x,
we expect that the gradient ∇fx of f at x is normal to the level set {z ∈ R
n
| f(z) = f(x)} at
f(x). An analogous result, as illustrated in Figure 51.22, holds for proper convex functions
in terms of subdifferentials.
Proposition 51.25. Let f be a proper convex function on R
n
, and let x ∈ R
n
be a vector
such that f is subdifferentiable at x but f does not achieve its minimum at x. Then the
normal cone NC(x) at x to the sublevel set C = {z ∈ R
n
| f(z) ≤ f(x)} is the closure of the
convex cone spanned by ∂f(x).
Proposition 51.25 is proven in Rockafellar [138] (Theorem 23.7).
The following result sharpens Proposition 51.8.
Proposition 51.26. Let f be a closed proper convex function on R
n
, and let S be a nonempty
closed and bounded subset of int(dom(f)). Then
∂f(S) = [
x∈S
∂f(x)
51.4. ADDITIONAL PROPERTIES OF SUBDIFFERENTIALS 1849
x
(x, f(x)) graph of f: R 2
 -> R
x
sublevel set C
x
sublevel set C
cone spanned by vf(x)
N (x) C
Figure 51.22: Let f be the proper convex function whose graph in R
3
is the peach polyhedral
surface. The sublevel set C = {z ∈ R
2
| f(z) ≤ f(x)} is the orange square which is closed
on three sides. Then the normal cone NC(x) is the closure of the convex cone spanned by
∂f(x).
is nonempty, closed and bounded. If
α = sup
y∈∂f(S)
k
yk 2 < +∞,
then f is Lipschitizan on S, and we have
f
0 (x; z) ≤ α k zk 2
for all x ∈ S and all z ∈ R
n
|f(y) − f(x)| ≤ α k y − zk 2
for all x, y ∈ S.
Proposition 51.24 is proven in Rockafellar [138] (Theorem 24.7).
The subdifferentials of a proper convex function f and its conjugate f
∗ are closely related.
First, we have the following proposition from Rockafellar [138] (Theorem 12.2).
Proposition 51.27. Let f be convex function on R
n
. The conjugate function f
∗ of f
is a closed and convex function, proper iff f is proper. Furthermore, (cl(f))∗ = f
∗
, and
f
∗∗ = cl(f).
As a corollary of Proposition 51.27, it can be shown that
f
∗
(y) = sup
x∈relint(dom(f))
(h x, yi − f(x)).
The following result is proven in Rockafellar [138] (Theorem 23.5).
1850 CHAPTER 51. SUBGRADIENTS AND SUBDIFFERENTIALS ~
Proposition 51.28. For any proper convex function f on R
n and for any vector x ∈ R
n
,
the following conditions on a vector y ∈ R
n are equivalent.
(a) y ∈ ∂f(x).
(b) The function h z, yi − f(z) achieves its supremum in z at z = x.
(c) f(x) + f
∗
(y) ≤ hx, yi .
(d) f(x) + f
∗
(y) = h x, yi .
If (cl(f))(x) = f(x), then there are three more conditions all equivalent to the above condi￾tions.
(a∗
) x ∈ ∂f ∗
(y).
(b∗
) The function h x, zi − f
∗
(z) achieves its supremum in z at z = y.
(a∗∗) y ∈ ∂(cl(f))(x).
The following results are corollaries of Proposition 51.28; see Rockafellar [138] (Corollaries
23.5.1, 23.5.2, 23.5.3).
Corollary 51.29. For any proper convex function f on R
n
, if f is closed, then y ∈ ∂f(x)
iff x ∈ ∂f ∗
(y), for all x, y ∈ R
n
.
Corollary 51.29 states a sort of adjunction property.
Corollary 51.30. For any proper convex function f on R
n
, if f is subdifferentiable at
x ∈ R
n
, then (cl(f))(x) = f(x) and ∂(cl(f))(x) = ∂f(x).
Corollary 51.30 shows that the closure of a proper convex function f agrees with f
whereever f is subdifferentiable.
Corollary 51.31. For any proper convex function f on R
n
, for any nonempty closed convex
subset C of R
n
, for any y ∈ R
n
, the set ∂δ∗
(y|C) = ∂IC
∗
(y) consists of the vectors x ∈ R
n
(if
any) where the linear form z 7→ hz, yi achieves its maximum over C.
There is a notion of approximate subgradient which turns out to be useful in optimization
theory; see Bertsekas [19, 17].
Definition 51.17. Let f : R
n → R ∪ {+∞} be any proper convex function. For any  > 0,
for any x ∈ R
n
, if f(x) is finite, then an  -subgradient of f at x is any vector u ∈ R
n
such
that
f(z) ≥ f(x) −  + h z − x, ui , for all z ∈ R
n
.
See Figure 51.23. The set of all  -subgradients of f at x is denoted ∂ f(x) and is called the

-subdifferential of f at x.
51.4. ADDITIONAL PROPERTIES OF SUBDIFFERENTIALS 1851
(1,1)
(0,3/2)
(1/2,-1)
(1,1)
(0,3/2)
(1/2,-1) ε
subgradient ε-subgradient
Figure 51.23: Let f : R → R ∪ {−∞, +∞} be the piecewise function defined by f(x) = x+ 1
for x ≥ 1 and f(x) = −
1
2
x + 2
3
for x < 1. Its epigraph is the shaded blue region in R
2
. The
line 1
2
(x − 1) + 1 (with normal ( 1
2
, −1) is a supporting hyperplane to the graph of f(x) at
(1, 1) while the line 1
2
(x − 1) + 1 −  is the hyperplane associated with the  -subgradient at
x = 1 and shows that u =
1
2
∈ ∂ f(x).
The set ∂ f(x) can be defined in terms of the conjugate of the function hx given by
hx(y) = f(x + y) − f(x), for all y ∈ R
n
.
Proposition 51.32. Let f : R
n → R∪ {+∞} be any proper convex function. For any  > 0,
if hx is given by
hx(y) = f(x + y) − f(x), for all y ∈ R
n
,
then
h
∗
x
(y) = f
∗
(y) + f(x) − hx, yi for all y ∈ R
n
and
∂ f(x) = {u ∈ R
n
| h
∗
x
(u) ≤  }.
Proof. We have
h
∗
x
(y) = sup
z∈Rn
(h y, zi − hx(z))
= sup
z∈Rn
(h y, zi − f(x + z) + f(x))
= sup
x+z∈Rn
(h y, x + zi − f(x + z) − hy, xi + f(x))
= f
∗
(y) + f(x) − hx, yi .
Observe that u ∈ ∂ f(x) iff for every y ∈ R
n
,
f(x + y) ≥ f(x) −  + h y, ui
<z-x,u> +f(x) u = 1/2
f(x) -ε + <z-x,u>
1852 CHAPTER 51. SUBGRADIENTS AND SUBDIFFERENTIALS ~
iff

≥ hy, ui − f(x + y) + f(x) = h y, ui − hx(y).
Since by definition
h
∗
x
(u) = sup
y∈Rn
(h y, ui − hx(y)),
we conclude that
∂ f(x) = {u ∈ R
n
| h
∗
x
(u) ≤  },
as claimed.
Remark: By Fenchel’s inequality h
∗
x
(y) ≥ 0, and by Proposition 51.28(d), the set of vectors
where h
∗
x vanishes is ∂f(x).
The equation ∂ f(x) = {u ∈ R
n
| h
∗
x
(u) ≤  } shows that ∂ f(x) is a closed convex set.
As  gets smaller, the set ∂ f(x) decreases, and we have
∂f(x) = \
>0
∂ f(x).
However δ
∗
(y|∂ f(x)) = I∂
∗

f(x)
(y) does not necessarily decrease to δ
∗
(y|∂f(x)) = I∂f
∗
(x)
(y) as

decreases to zero. The discrepancy corresponds to the discrepancy between f
0 (x; y) and
δ
∗
(y|∂f(x)) = I∂f
∗
(x)
(y) and is due to the fact that f is not necessarily closed (see Proposition
51.16) as shown by the following result proven in Rockafellar [138] (Theorem 23.6).
Proposition 51.33. Let f be a closed and proper convex function, and let x ∈ R
n
such that
f(x) is finite. Then
f
0 (x; y) = lim

↓0
δ
∗
(y|∂ f(x)) = lim

↓0
I∂
∗

f(x)
(y) for all y ∈ R
n
.
The theory of convex functions is rich and we have only given a sample of some of the
most significant results that are relevant to optimization theory. There are a few more
results regarding the minimum of convex functions that are particularly important due to
their applications to optimization theory.
51.5 The Minimum of a Proper Convex Function
Let h be a proper convex function on R
n
. The general problem is to study the minimum of
h over a nonempty convex set C in R
n
, possibly defined by a set of inequality and equality
constraints. We already observed that minimizing h over C is equivalent to minimizing the
proper convex function f given by
f(x) = h(x) + IC(x) = ( h
+
(
∞
x) if
if
x
x /
∈
∈
C
C.
Therefore it makes sense to begin by considering the problem of minimizing a proper convex
function f over R
n
. Of course, minimizing over R
n
is equivalent to minimizing over dom(f).
51.5. THE MINIMUM OF A PROPER CONVEX FUNCTION 1853
Definition 51.18. Let f be a proper convex function on R
n
. We denote by inf f the quantity
inf f = inf
x∈dom(f)
f(x).
This is the minimum of the function f over R
n
(it may be equal to −∞).
For every α ∈ R, we have the sublevel set
sublevα(f) = {x ∈ R
n
| f(x) ≤ α}.
By Proposition 51.2, we know that the sublevel sets sublevα(f) are convex and that
dom(f) = [
α∈R
sublevα(f).
Observe that sublevα(f) = ∅ if α < inf f. If inf f > −∞, then for α = inf f, the sublevel
set sublevα(f) consists of the set of vectors where f achieves it minimum.
Definition 51.19. Let f be a proper convex function on R
n
. If inf f > −∞, then the
sublevel set sublevinf f (f) is called the minimum set of f (this set may be empty). See
Figure 51.24.
(x,f(x))
graph of f: R 2
 -> R
y
f (x;y) ‘ ≥0
minimum set of f
x
Figure 51.24: Let f be the proper convex function whose graph is the surface of the upward
facing pink trough. The minimum set of f is the light pink square of R
2 which maps to
the bottom surface of the trough in R
3
. For any x in the minimum set, f
0 (x; y) ≥ 0, a fact
substantiated by Proposition 51.34.
It is important to determine whether the minimum set is empty or nonempty, or whether
it contains a single point. As we noted in Theorem 40.13(2), if f is strictly convex then the
minimum set contains at most one point.
1854 CHAPTER 51. SUBGRADIENTS AND SUBDIFFERENTIALS ~
In any case, we know from Proposition 51.2 and Proposition 51.3 that the minimum set
of f is convex, and closed iff f is closed.
Subdifferentials provide the first criterion for deciding whether a vector x ∈ R
n belongs
to the minimum set of f. Indeed, the very definition of a subgradient says that x ∈ R
n
belongs to the minimum set of f iff 0 ∈ ∂f(x). Using Proposition 51.16, we obtain the
following result.
Proposition 51.34. Let f be a proper convex function over R
n
. A vector x ∈ R
n
belongs
to the minimum set of f iff
0 ∈ ∂f(x)
iff f(x) is finite and
f
0 (x; y) ≥ 0 for all y ∈ R
n
.
Of course, if f is differentiable at x, then ∂f(x) = {∇fx}, and we obtain the well-known
condition ∇fx = 0.
There are many ways of expressing the conditions of Proposition 51.34, and the minimum
set of f can even be characterized in terms of the conjugate function f
∗
. The notion of
direction of recession plays a key role.
Definition 51.20. Let f : R
n → R ∪ {+∞} be any function. A direction of recession of f
is any non-zero vector u ∈ R
n
such that for every x ∈ dom(f), the function λ 7→ f(x + λu)
is nonincreasing (this means that for all λ1, λ2 ∈ R, if λ1 < λ2, then x + λ1u ∈ dom(f),
x + λ2u ∈ dom(f), and f(x + λ2u) ≤ f(x + λ1u)).
Example 51.12. Consider the function f : R
2 → R given by f(x, y) = 2x + y
2
. Since
f(x + λu, y + λv) = 2(x + λu) + (y + λv)
2 = 2x + y
2 + 2(u + yv)λ + v
2λ
2
,
if v 6 = 0, we see that the above quadratic function of λ increases for λ ≥ −(u + yv)/v2
. If
v = 0, then the function λ 7→ 2x + y
2 + 2uλ decreases to −∞ when λ goes to +∞ if u < 0,
so all vectors (−u, 0) with u > 0 are directions of recession. See Figure 51.25.
The function f(x, y) = 2x + x
2 + y
2 does not have any direction of recession, because
f(x + λu, y + λv) = 2x + x
2 + y
2 + 2(u + ux + yv)λ + (u
2 + v
2
)λ
2
,
and since (u, v) 6 = (0, 0), we have u
2 + v
2 > 0, so as a function of λ, the above quadratic
function increases for λ ≥ −(u + ux + yv)/(u
2 + v
2
). See Figure 51.25.
In fact, the above example is typical. For any symmetric positive definite n×n matrix A
and any vector b ∈ R
n
, the quadratic strictly convex function q given by q(x) = x
> Ax + b
> x
has no directions of recession. For any u ∈ R
n
, with u 6 = 0, we have
q(x + λu) = (x + λu)
> A(x + λu) + b
> (x + λu)
= x
> Ax + b
> x + (2x
> Au + b
> u)λ + (u
> Au)λ
2
.
51.5. THE MINIMUM OF A PROPER CONVEX FUNCTION 1855
f(x,y) = 2x + y2
f(x,y) = 2x + x + y 2 2
Figure 51.25: The graphs of the two functions discussed in Example 51.12. The graph of
f(x, y) = 2x+y
2
slopes ”downward” along the negative x-axis, reflecting the fact that (−u, 0)
is a direction of recession.
Since u 66 = 0 and A is SPD, we have u
> Au > 0, and the above quadratic function increases
for λ ≥ −(2x
> Au + b
> u)/(2u
> Au).
The above fact yields an important trick of convex optimization. If f is any proper closed
and convex function, then for any quadratic strictly convex function q, the function h = f +q
is a proper and closed strictly convex function that has a minimum which is attained for a
unique vector. This trick is at the core of the method of augmented Lagrangians, and in
particular ADMM. Surprisingly, a rigorous proof requires the deep theorem below.
One should be careful not to conclude hastily that if a convex function is proper and
closed, then dom(f) and Im(f) are also closed. Also, a closed and proper convex function
may not attain its minimum. For example, the function f : R → R ∪ {+∞} given by
f(x) =



1
x
if x > 0
+∞ if x ≤ 0
is a proper, closed and convex function, but dom(f) = (0, +∞) and Im(f) = (0, +∞). Note
that inf f = 0 is not attained. The problem is that f has 1 has a direction of recession as
evidenced by the graph provided in Figure 51.26.
The following theorem is proven in Rockafellar [138] (Theorem 27.1).
1856 CHAPTER 51. SUBGRADIENTS AND SUBDIFFERENTIALS ~
Figure 51.26: The graph of the partial function f(x) = x
1
for x > 0. The graph of this
function decreases along the x-axis since 1 is a direction of recession.
Theorem 51.35. Let f be a proper and closed convex function over R
n
. The following
statements hold:
(1) We have inf f = −f
∗
(0). Thus f is bounded below iff 0 ∈ dom(f
∗
).
(2) The minimum set of f is equal to ∂f ∗
(0). Thus the infimum of f is attained (which
means that there is some x ∈ R
n
such that f(x) = inf f) iff f
∗
is subdifferentiable
at 0. This condition holds in particular when 0 ∈ relint(dom(f
∗
)). Moreover, 0 ∈
relint(dom(f
∗
)) iff every direction of recession of f is a direction in which f is con￾stant.
(3) For the infimum of f to be finite but unattained, it is necessary and sufficient that
f
∗
(0) be finite and (f
∗
)
0 (0; y) = −∞ for some y ∈ R
n
.
(4) The minimum set of f is a nonempty bounded set iff 0 ∈ int(dom(f
∗
)). This condition
holds iff f has no directions of recession.
(5) The minimum set of f consists of a unique vector x iff f
∗
is differentiable at x and
x = ∇f0
∗
.
(6) For each α ∈ R, the support function of sublevα(f) is the closure of the positively
homogeneous convex function generated by f
∗ + α. If f is bounded below, then the
support function of the minimum set of f is the closure of the directional derivative
map y 7→ (f
∗
)
0 (0; y).
In view of the importance of Theorem 51.35(4), we state this property as the following
corollary.
51.5. THE MINIMUM OF A PROPER CONVEX FUNCTION 1857
Corollary 51.36. Let f be a closed proper convex function on R
n
. Then the minimal set of
f is a non-empty bounded set iff f has no directions of recession. In particular, if f has no
directions of recession, then the minimum inf f of f is finite and attained for some x ∈ R
n
.
Theorem 51.14 implies the following result which is very important for the design of
optimization procedures.
Proposition 51.37. Let f be a proper and closed convex function over R
n
. The function
h given by h(x) = f(x) + q(x) obtained by adding any strictly convex quadratic function q
of the form q(x) = x
> Ax + b
> x (where A is symmetric positive definite) is a proper closed
strictly convex function such that inf h is finite, and there is a unique x
∗ ∈ R
n
such that h
attains its minimum in x
∗
(that is, h(x
∗
) = inf h).
Proof. By Theorem 51.14 there is some affine form ϕ given by ϕ(x) = c
> x + α (with α ∈ R)
such that f(x) ≥ ϕ(x) for all x ∈ R
n
. Then we have
h(x) = f(x) + q(x) ≥ x
> Ax + (b
> + c
> )x + α for all x ∈ R
n
.
Since A is symmetric positive definite, by Example 51.12, the quadratic function Q given
by Q(x) = x
> Ax + (b
> + c
> )x + α has no directions of recession. Since h(x) ≥ Q(x) for
all x ∈ R
n
, we claim that h has no directions of recession. Otherwise, there would be some
nonzero vector u, such that the function λ 7→ h(x + λu) is nonincreasing for all x ∈ dom(h),
so h(x + λu) ≤ β for some β for all λ. But we showed that for λ large enough, the function
λ 7→ Q(x + λu) increases like λ
2
, so for λ large enough, we will have Q(x + λu) > β,
contradicting the fact that h majorizes Q. By Corollary 51.36, h has a finite minimum x
∗
which is attained.
If f and g are proper convex functions and if g is strictly convex, then f + g is a proper
function. For all x, y ∈ R
n
, for any λ such that 0 < λ < 1, since f is convex and g is strictly
convex, we have
f((1 − λ)x + λy) ≤ (1 − λ)f(x) + λf(y)
g((1 − λ)x + λy) < (1 − λ)g(x) + λg(y),
so we deduce that
f((1 − λ)x + λy) + g((1 − λ)x + λy) < ((1 − λ)(f(x) + g(x)) + λ(f(x) + g(x))),
which shows that f + g is strictly convex. Then, as f + q is strictly convex, it has a unique
minimum at x
∗
.
We now come back to the problem of minimizing a proper convex function h over a
nonempty convex subset C . Here is a nice characterization.
Proposition 51.38. Let h be a proper convex function on R
n
, and let C be a nonempty
convex subset of R
n
.
1858 CHAPTER 51. SUBGRADIENTS AND SUBDIFFERENTIALS ~
(1) For any x ∈ R
n
, if there is some y ∈ ∂h(x) such that −y ∈ NC(x), that is, −y is
normal to C at x, then h attains its minimum on C at x.
(2) If relint(dom(h)) ∩ relint(C) 6 = ∅, then the converse of (1) holds. This means that if
h attains its minimum on C at x, then there is some y ∈ ∂h(x) such that −y ∈ NC(x).
Proposition 51.38 is proven in Rockafellar [138] (Theorem 27.4). The proof is actually
quite simple.
Proof. (1) By Proposition 51.34, h attains its minimum on C at x iff
0 ∈ ∂(h + IC)(x).
By Proposition 51.23, since
∂h(x) + ∂IC(x) ⊆ ∂(h + IC)(x),
if 0 ∈ ∂h(x) + ∂IC(x), then h attains its minimum on C at x. But we saw in Section 51.2
that ∂IC(x) = NC(x), the normal cone to C at x. Then the condition 0 ∈ ∂h(x) + ∂IC(x)
says that there is some y ∈ ∂h(x) such that y + z = 0 for some z ∈ NC(x), and this is
equivalent to −y ∈ NC(x).
(2) By definition of IC, the condition relint(dom(h)) ∩ relint(C) 6 = ∅ is the hypothesis
of Proposition 51.23 to have
∂(h + IC)(x) = ∂h(x) + ∂IC(x).
If h attains its minimum on C at x, then by Proposition 51.34 we have 0 ∈ ∂(h + IC)(x),
so 0 ∈ ∂h(x) + ∂IC(x) = ∂h(x) + NC(x), and by the reasoning of Part (1), this means that
there is some y ∈ ∂h(x) such that −y ∈ NC(x).
Remark: A polyhedral function is a convex function whose epigraph is a polyhedron. It is
easy to see that Proposition 51.38(2) also holds in the following cases
(1) C is a H-polyhedron and relint(dom(h)) ∩ C 6 = ∅
(2) h is polyhedral and dom(h) ∩ relint(C) 6 = ∅.
(3) Both h and C are polyhedral, and dom(h) ∩ C 6 = ∅.
51.6. GENERALIZATION OF THE LAGRANGIAN FRAMEWORK 1859
51.6 Generalization of the Lagrangian Framework
Essentially all the results presented in Section 50.3, Section 50.7, Section 50.8, and Section
50.9 about Lagrangians and Lagrangian duality generalize to programs involving a proper
and convex objective function J, proper and convex inequality constraints, and affine equality
constraints. The extra generality is that it is no longer assumed that these functions are
differentiable. This theory is thoroughly discussed in Part VI, Section 28, of Rockafellar
[138], for programs called ordinary convex programs. We do not have the space to even
sketch this theory but we will spell out some of the key results.
We will be dealing with programs consisting of an objective function J : R
n → R∪ {+∞}
which is convex and proper, subject to m ≥ 0 inequality contraints ϕi(v) ≤ 0, and p ≥ 0
affine equality constraints ψj (v) = 0. The constraint functions ϕi are also convex and proper,
and we assume that
relint(dom(J)) ⊆ relint(dom(ϕi)), dom(J) ⊆ dom(ϕi), i = 1, . . . , m.
Such programs are called ordinary convex programs. Let
U = dom(J) ∩ {v ∈ R
n
| ϕi(v) ≤ 0, ψj (v) = 0, 1 ≤ i ≤ m, 1 ≤ j ≤ p},
be the set of feasible solutions. We are seeking elements in u ∈ U that minimize J over U.
A generalized version of Theorem 50.18 holds under the above hypotheses on J and the
constraints ϕi and ψj
, except that in the KKT conditions, the equation involving gradients
must be replaced by the following condition involving subdifferentials:
0 ∈ ∂
 J +
mX
i=1
λiϕi +
p
X
j=1
µjψj
! (u),
with λi ≥ 0 for i = 1, . . . , m and µj ∈ R for j = 1, . . . , p (where u ∈ U and J attains its
minimum at u).
The Lagrangian L(v, λ, ν) of our problem is defined as follows: Let
Em = {x ∈ R
m+p
| xi ≥ 0, 1 ≤ i ≤ m}.
Then
L(v, λ, µ) =



J(v) + P m
i=1 λiϕi(v) + P p
j=1 µjψj (v) if (λ, µ) ∈ Em, v ∈ dom(J)
−∞ if (λ, µ) ∈/ Em, v ∈ dom(J)
+∞ if v /∈ dom(J).
For fixed values (λ, µ) ∈ R
m
+ × R
p
, we also define the function h: R
n → R ∪ {+∞} given
by
h(x) = J(x) +
mX
i=1
λiϕi(x) +
p
X
j=1
µjψj (x),
1860 CHAPTER 51. SUBGRADIENTS AND SUBDIFFERENTIALS ~
whose effective domain is dom(J) (since we are assuming that dom(J) ⊆ dom(ϕi), i =
1, . . . , m). Thus h(x) = L(x, λ, µ), but h is a function only of x, so we denote it differently
to avoid confusion (also, technically, L(x, λ, µ) may take the value −∞, but h does not).
Since J and the ϕi are proper convex functions and the ψj are affine, the function h is a
proper convex function.
A proof of a generalized version of Theorem 50.18 can be obtained by putting together
Theorem 28.1, Theorem 28.2, and Theorem 28.3, in Rockafellar [138]. For the sake of
completeness, we state these theorems. Here is Theorem 28.1.
Theorem 51.39. (Theorem 28.1, Rockafellar) Let (P) be an ordinary convex program. Let
(λ, µ) ∈ R
m
+ × R
p
be Lagrange multipliers such that the infimum of the function h = J +
P
m
i=1 λiϕi +
P
p
j=1 µjψj
is finite and equal to the optimal value of J over U. Let D be the
minimal set of h over R
n
, and let I = {i ∈ {1, . . . , m} | λi = 0}. If D0 is the subset of D
consisting of vectors x such that
ϕi(x) ≤ 0 for all i ∈ I
ϕi(x) = 0 for all i /∈ I
ψj (x) = 0 for all j = 1, . . . , p,
then D0 is the set of minimizers of (P) over U.
And now here is Theorem 28.2.
Theorem 51.40. (Theorem 28.2, Rockafellar) Let (P) be an ordinary convex program, and
let I ⊆ {1, . . . , m} be the subset of indices of inequality constraints that are not affine.
Assume that the optimal value of (P) is finite, and that (P) has at least one feasible solution
x ∈ relint(dom(J)) such that
ϕi(x) < 0 for all i ∈ I.
Then there exist some Lagrange multipliers (λ, µ) ∈ R
m
+ × R
p
(not necessarily unique) such
that
(a) The infimum of the function h = J +
P
m
i=1 λiϕi +
P
p
j=1 µjψj is finite and equal to the
optimal value of J over U.
The hypotheses of Theorem 51.40 are qualification conditions on the constraints, essen￾tially Slater’s conditions from Definition 50.6.
Definition 51.21. Let (P) be an ordinary convex program, and let I ⊆ {1, . . . , m} be the
subset of indices of inequality constraints that are not affine. The constraints are qualified
is there is a feasible solution x ∈ relint(dom(J)) such that
ϕi(x) < 0 for all i ∈ I.
Finally, here is Theorem 28.3 from Rockafellar [138].
51.6. GENERALIZATION OF THE LAGRANGIAN FRAMEWORK 1861
Theorem 51.41. (Theorem 28.3, Rockafellar) Let (P) be an ordinary convex program. If
x ∈ R
n and (λ, µ) ∈ R
m
+ × R
p
, then (λ, µ) and x have the property that
(a) The infimum of the function h = J +
P
m
i=1 λiϕi +
P
p
j=1 µjψj is finite and equal to the
optimal value of J over U, and
(b) The vector x is an optimal solution of (P) (so x ∈ U),
iff (x, λ, µ) is a saddle point of the Lagrangian L(x, λ, µ) of (P).
Moreover, this condition holds iff the following KKT conditions hold:
(1) λ ∈ R
m
+ , ϕi(x) ≤ 0, and λiϕi(x) = 0 for i = 1, . . . , m.
(2) ψj (x) = 0 for j = 1, . . . , p.
(3) 0 ∈ ∂J(x) + P m
i=1 λi∂ϕi(x) + P p
j=1 µj∂ψj (x).
Observe that by Theorem 51.40, if the optimal value of (P) is finite and if the constraints
are qualified, then Condition (a) of Theorem 51.41 holds for (λ, µ). As a consequence we
obtain the following corollary of Theorem 51.41 attributed to Kuhn and Tucker, which is
one of the main results of the theory. It is a generalized version of Theorem 50.18.
Theorem 51.42. (Theorem 28.3.1, Rockafellar) Let (P) be an ordinary convex program
satisfying the hypothesis of Theorem 51.40, which means that the optimal value of (P) is
finite, and that the constraints are qualified. In order that a vector x ∈ R
n
be an optimal
solution to (P), it is necessary and sufficient that there exist Lagrange multipliers (λ, µ) ∈
R
m
+ × R
