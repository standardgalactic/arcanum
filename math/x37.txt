We will see later that a Hermitian matrix is positive definite iff its eigenvalues are all
positive.
The following result reminiscent of the first polarization identity of Proposition 14.1 can
be used to prove that two linear maps are identical.
Proposition 14.3. Given any Hermitian space E with Hermitian product h−, −i, for any
linear map f : E → E, if h f(x), xi = 0 for all x ∈ E, then f = 0.
14.1. HERMITIAN SPACES, PRE-HILBERT SPACES 521
Proof. Compute h f(x + y), x + yi and h f(x − y), x − yi :
h
f(x + y), x + yi = h f(x), xi + h f(x), yi + h f(y), xi + h y, yi
h
f(x − y), x − yi = h f(x), xi − hf(x), yi − hf(y), xi + h y, yi ;
then subtract the second equation from the first to obtain
h
f(x + y), x + yi − hf(x − y), x − yi = 2(h f(x), yi + h f(y), xi ).
If h f(u), ui = 0 for all u ∈ E, we get
h
f(x), yi + h f(y), xi = 0 for all x, y ∈ E.
Then the above equation also holds if we replace x by ix, and we obtain
ih f(x), yi − ih f(y), xi = 0, for all x, y ∈ E,
so we have
h
f(x), yi + h f(y), xi = 0
h
f(x), yi − hf(y), xi = 0,
which implies that h f(x), yi = 0 for all x, y ∈ E. Since h−, −i is positive definite, we have
f(x) = 0 for all x ∈ E; that is, f = 0.
One should be careful not to apply Proposition 14.3 to a linear map on a real Euclidean
space because it is false! The reader should find a counterexample.
The Cauchy–Schwarz inequality and the Minkowski inequalities extend to pre-Hilbert
spaces and to Hermitian spaces.
Proposition 14.4. Let h E, ϕi be a pre-Hilbert space with associated quadratic form Φ. For
all u, v ∈ E, we have the Cauchy–Schwarz inequality
|ϕ(u, v)| ≤ p Φ(u)
p Φ(v).
Furthermore, if h E, ϕi is a Hermitian space, the equality holds iff u and v are linearly de￾pendent.
We also have the Minkowski inequality
p
Φ(u + v) ≤
p Φ(u) + p Φ(v).
Furthermore, if h E, ϕi is a Hermitian space, the equality holds iff u and v are linearly de￾pendent, where in addition, if u 6 = 0 and v 6 = 0, then u = λv for some real λ such that
λ > 0.
522 CHAPTER 14. HERMITIAN SPACES
Proof. For all u, v ∈ E and all µ ∈ C, we have observed that
ϕ(u + µv, u + µv) = ϕ(u, u) + 2< (µϕ(u, v)) + |µ|
2ϕ(v, v).
Let ϕ(u, v) = ρeiθ, where |ϕ(u, v)| = ρ (ρ ≥ 0). Let F : R → R be the function defined such
that
F(t) = Φ(u + teiθv),
for all t ∈ R. The above shows that
F(t) = ϕ(u, u) + 2t|ϕ(u, v)| + t
2ϕ(v, v) = Φ(u) + 2t|ϕ(u, v)| + t
2Φ(v).
Since ϕ is assumed to be positive, we have F(t) ≥ 0 for all t ∈ R. If Φ(v) = 0, we must have
ϕ(u, v) = 0, since otherwise, F(t) could be made negative by choosing t negative and small
enough. If Φ(v) > 0, in order for F(t) to be nonnegative, the equation
Φ(u) + 2t|ϕ(u, v)| + t
2Φ(v) = 0
must not have distinct real roots, which is equivalent to
|ϕ(u, v)|
2 ≤ Φ(u)Φ(v).
Taking the square root on both sides yields the Cauchy–Schwarz inequality.
For the second part of the claim, if ϕ is positive definite, we argue as follows. If u and v
are linearly dependent, it is immediately verified that we get an equality. Conversely, if
|ϕ(u, v)|
2 = Φ(u)Φ(v),
then there are two cases. If Φ(v) = 0, since ϕ is positive definite, we must have v = 0, so u
and v are linearly dependent. Otherwise, the equation
Φ(u) + 2t|ϕ(u, v)| + t
2Φ(v) = 0
has a double root t0, and thus
Φ(u + t0e
iθv) = 0.
Since ϕ is positive definite, we must have
u + t0e
iθv = 0,
which shows that u and v are linearly dependent.
If we square the Minkowski inequality, we get
Φ(u + v) ≤ Φ(u) + Φ(v) + 2p Φ(u)
p Φ(v).
However, we observed earlier that
Φ(u + v) = Φ(u) + Φ(v) + 2< (ϕ(u, v)).
14.1. HERMITIAN SPACES, PRE-HILBERT SPACES 523
Thus, it is enough to prove that
<
(ϕ(u, v)) ≤
p Φ(u)
p Φ(v),
but this follows from the Cauchy–Schwarz inequality
|ϕ(u, v)| ≤ p Φ(u)
p Φ(v)
and the fact that < z ≤ |z|.
If ϕ is positive definite and u and v are linearly dependent, it is immediately verified that
we get an equality. Conversely, if equality holds in the Minkowski inequality, we must have
<
(ϕ(u, v)) = p Φ(u)
p Φ(v),
which implies that
|ϕ(u, v)| =
p Φ(u)
p Φ(v),
since otherwise, by the Cauchy–Schwarz inequality, we would have
<
(ϕ(u, v)) ≤ |ϕ(u, v)| <
p Φ(u)
p Φ(v).
Thus, equality holds in the Cauchy–Schwarz inequality, and
<
(ϕ(u, v)) = |ϕ(u, v)|.
But then we proved in the Cauchy–Schwarz case that u and v are linearly dependent. Since
we also just proved that ϕ(u, v) is real and nonnegative, the coefficient of proportionality
between u and v is indeed nonnegative.
As in the Euclidean case, if h E, ϕi is a Hermitian space, the Minkowski inequality
p
Φ(u + v) ≤
p Φ(u) + p Φ(v)
shows that the map u 7→
p Φ(u) is a norm on E. The norm induced by ϕ is called the
Hermitian norm induced by ϕ. We usually denote p Φ(u) by k uk , and the Cauchy–Schwarz
inequality is written as
|u · v| ≤ kukk vk .
Since a Hermitian space is a normed vector space, it is a topological space under the
topology induced by the norm (a basis for this topology is given by the open balls B0(u, ρ)
of center u and radius ρ > 0, where
B0(u, ρ) = {v ∈ E | kv − uk < ρ}.
If E has finite dimension, every linear map is continuous; see Chapter 9 (or Lang [111, 112],
Dixmier [51], or Schwartz [150, 151]). The Cauchy–Schwarz inequality
|u · v| ≤ kukk vk
524 CHAPTER 14. HERMITIAN SPACES
shows that ϕ: E × E → C is continuous, and thus, that k k is continuous.
If h E, ϕi is only pre-Hilbertian, k uk is called a seminorm. In this case, the condition
k
uk = 0 implies u = 0
is not necessarily true. However, the Cauchy–Schwarz inequality shows that if k uk = 0, then
u · v = 0 for all v ∈ E.
Remark: As in the case of real vector spaces, a norm on a complex vector space is induced
by some positive definite Hermitian product h−, −i iff it satisfies the parallelogram law:
k
u + vk
2 + k u − vk
2 = 2(k uk
2 + k vk
2
).
This time the Hermitian product is recovered using the polarization identity from Proposition
14.1:
4h u, vi = k u + vk
2 − ku − vk
2 + i k u + ivk 2 − i k u − ivk 2
.
It is easy to check that h u, ui = k uk
2
, and
h
v, ui = h u, vi
h
iu, vi = ih u, vi ,
so it is enough to check linearity in the variable u, and only for real scalars. This is easily
done by applying the proof from Section 12.1 to the real and imaginary part of h u, vi ; the
details are left as an exercise.
We will now basically mirror the presentation of Euclidean geometry given in Chapter
12 rather quickly, leaving out most proofs, except when they need to be seriously amended.
14.2 Orthogonality, Duality, Adjoint of a Linear Map
In this section we assume that we are dealing with Hermitian spaces. We denote the Her￾mitian inner product by u · v or h u, vi . The concepts of orthogonality, orthogonal family of
vectors, orthonormal family of vectors, and orthogonal complement of a set of vectors are
unchanged from the Euclidean case (Definition 12.2).
For example, the set C[−π, π] of continuous functions f : [−π, π] → C is a Hermitian
space under the product
h
f, gi =
Z
π
−π
f(x)g(x)dx,
and the family (e
ikx)k∈Z is orthogonal.
Propositions 12.4 and 12.5 hold without any changes. It is easy to show that





nX
i=1
ui




2
=
nX
i=1
k
uik
2 +
X
1≤i<j≤n
2< (ui
· uj ).
14.2. ORTHOGONALITY, DUALITY, ADJOINT OF A LINEAR MAP 525
Analogously to the case of Euclidean spaces of finite dimension, the Hermitian product
induces a canonical bijection (i.e., independent of the choice of bases) between the vector
space E and the space E
∗
. This is one of the places where conjugation shows up, but in this
case, troubles are minor.
Given a Hermitian space E, for any vector u ∈ E, let ϕ
l
u
: E → C be the map defined
such that
ϕ
l
u
(v) = u · v, for all v ∈ E.
Similarly, for any vector v ∈ E, let ϕ
r
v
: E → C be the map defined such that
ϕ
r
v
(u) = u · v, for all u ∈ E.
Since the Hermitian product is linear in its first argument u, the map ϕ
r
v
is a linear form
in E
∗
, and since it is semilinear in its second argument v, the map ϕ
l
u
is also a linear form
in E
∗
. Thus, we have two maps [ l
: E → E
∗ and [ r
: E → E
∗
, defined such that
[
l
(u) = ϕ
l
u
, and [ r
(v) = ϕ
r
v
.
Proposition 14.5. The equations ϕ
l
u = ϕ
r
u and [ l = [ r hold.
Proof. Indeed, for all u, v ∈ E, we have
[
l
(u)(v) = ϕ
l
u
(v)
= u · v
= v · u
= ϕ
r
u
(v)
= [
r
(u)(v).
Therefore, we use the notation ϕu for both ϕ
l
u and ϕ
r
u
, and [ for both [ l and [ r
.
Theorem 14.6. Let E be a Hermitian space E. The map [ : E → E
∗ defined such that
[
(u) = ϕ
l
u = ϕ
r
u
for all u ∈ E
is semilinear and injective. When E is also of finite dimension, the map [ : E → E
∗
is a
canonical isomorphism.
Proof. That [ : E → E
∗
is a semilinear map follows immediately from the fact that [ = [ r
,
and that the Hermitian product is semilinear in its second argument. If ϕu = ϕv, then
ϕu(w) = ϕv(w) for all w ∈ E, which by definition of ϕu and ϕv means that
w · u = w · v
for all w ∈ E, which by semilinearity on the right is equivalent to
w · (v − u) = 0 for all w ∈ E,
which implies that u = v, since the Hermitian product is positive definite. Thus, [ : E → E
∗
is injective. Finally, when E is of finite dimension n, E
∗
is also of dimension n, and then
[
: E → E
∗
is bijective. Since [ is semilinar, the map [ : E → E
∗
is an isomorphism.
526 CHAPTER 14. HERMITIAN SPACES
The inverse of the isomorphism [ : E → E
∗
is denoted by ] : E
∗ → E.
As a corollary of the isomorphism [ : E → E
∗ we have the following result.
Proposition 14.7. If E is a Hermitian space of finite dimension, then every linear form
f ∈ E
∗
corresponds to a unique v ∈ E, such that
f(u) = u · v, for every u ∈ E.
In particular, if f is not the zero form, the kernel of f, which is a hyperplane H, is precisely
the set of vectors that are orthogonal to v.
Remarks:
1. The “musical map” [ : E → E
∗
is not surjective when E has infinite dimension. This
result can be salvaged by restricting our attention to continuous linear maps and by
assuming that the vector space E is a Hilbert space.
2. Dirac’s “bra-ket” notation. Dirac invented a notation widely used in quantum me￾chanics for denoting the linear form ϕu = [ (u) associated to the vector u ∈ E via the
duality induced by a Hermitian inner product. Dirac’s proposal is to denote the vectors
u in E by |ui , and call them kets; the notation |ui is pronounced “ket u.” Given two
kets (vectors) |ui and |vi , their inner product is denoted by
h
u|vi
(instead of |ui·|vi ). The notation h u|vi for the inner product of |ui and |vi anticipates
duality. Indeed, we define the dual (usually called adjoint) bra u of ket u, denoted by
h
u|, as the linear form whose value on any ket v is given by the inner product, so
h
u|(|vi ) = h u|vi .
Thus, bra u = h u| is Dirac’s notation for our [ (u). Since the map [ is semi-linear, we
have
h
λu| = λh u|.
Using the bra-ket notation, given an orthonormal basis (|u1i , . . . , |uni ), ket v (a vector)
is written as
|vi =
nX
i=1
h
v|uii|uii
,
and the corresponding linear form bra v is written as
h
v| =
nX
i=1
h
v|uiih ui
| =
nX
i=1
h
ui
|vih ui
|
over the dual basis (h u1|, . . . ,h un|). As cute as it looks, we do not recommend using
the Dirac notation.
14.2. ORTHOGONALITY, DUALITY, ADJOINT OF A LINEAR MAP 527
The existence of the isomorphism [ : E → E
∗
is crucial to the existence of adjoint maps.
Indeed, Theorem 14.6 allows us to define the adjoint of a linear map on a Hermitian space.
Let E be a Hermitian space of finite dimension n, and let f : E → E be a linear map. For
every u ∈ E, the map
v 7→ u · f(v)
is clearly a linear form in E
∗
, and by Theorem 14.6, there is a unique vector in E denoted
by f
∗
(u), such that
f
∗
(u) · v = u · f(v),
that is,
f
∗
(u) · v = u · f(v), for every v ∈ E.
The following proposition shows that the map f
∗
is linear.
Proposition 14.8. Given a Hermitian space E of finite dimension, for every linear map
f : E → E there is a unique linear map f
∗
: E → E such that
f
∗
(u) · v = u · f(v), for all u, v ∈ E.
Proof. Careful inspection of the proof of Proposition 12.8 reveals that it applies unchanged.
The only potential problem is in proving that f
∗
(λu) = λf ∗
(u), but everything takes place
in the first argument of the Hermitian product, and there, we have linearity.
Definition 14.6. Given a Hermitian space E of finite dimension, for every linear map
f : E → E, the unique linear map f
∗
: E → E such that
f
∗
(u) · v = u · f(v), for all u, v ∈ E
given by Proposition 14.8 is called the adjoint of f (w.r.t. to the Hermitian product).
The fact that
v · u = u · v
implies that the adjoint f
∗ of f is also characterized by
f(u) · v = u · f
∗
(v),
for all u, v ∈ E.
Given two Hermitian spaces E and F, where the Hermitian product on E is denoted
by h−, −i1
and the Hermitian product on F is denoted by h−, −i2
, given any linear map
f : E → F, it is immediately verified that the proof of Proposition 14.8 can be adapted to
show that there is a unique linear map f
∗
: F → E such that
h
f(u), vi 2 = h u, f ∗
(v)i
1
for all u ∈ E and all v ∈ F. The linear map f
∗
is also called the adjoint of f.
As in the Euclidean case, the following properties immediately follow from the definition
of the adjoint map.
528 CHAPTER 14. HERMITIAN SPACES
Proposition 14.9. (1) For any linear map f : E → F, we have
f
∗∗ = f.
(2) For any two linear maps f, g : E → F and any scalar λ ∈ R:
(f + g)
∗ = f
∗ + g
∗
(λf)
∗ = λf ∗
.
(3) If E, F, G are Hermitian spaces with respective inner products h−, −i1,h−, −i2, and
h−, −i3, and if f : E → F and g : F → G are two linear maps, then
(g ◦ f)
∗ = f
∗
◦ g
∗
.
As in the Euclidean case, a linear map f : E → E (where E is a finite-dimensional
Hermitian space) is self-adjoint if f = f
∗
. The map f is positive semidefinite iff
h
f(x), xi ≥ 0 all x ∈ E;
positive definite iff
h
f(x), xi > 0 all x ∈ E, x 6 = 0.
An interesting corollary of Proposition 14.3 is that a positive semidefinite linear map must
be self-adjoint. In fact, we can prove a slightly more general result.
Proposition 14.10. Given any finite-dimensional Hermitian space E with Hermitian prod￾uct h−, −i, for any linear map f : E → E, if h f(x), xi ∈ R for all x ∈ E, then f is
self-adjoint. In particular, any positive semidefinite linear map f : E → E is self-adjoint.
Proof. Since h f(x), xi ∈ R for all x ∈ E, we have
h
f(x), xi = h f(x), xi
= h x, f(x)i
= h f
∗
(x), xi ,
so we have
h
(f − f
∗
)(x), xi = 0 all x ∈ E,
and Proposition 14.3 implies that f − f
∗ = 0.
Beware that Proposition 14.10 is false if E is a real Euclidean space.
As in the Euclidean case, Theorem 14.6 can be used to show that any Hermitian space
of finite dimension has an orthonormal basis. The proof is unchanged.
Proposition 14.11. Given any nontrivial Hermitian space E of finite dimension n ≥ 1,
there is an orthonormal basis (u1, . . . , un) for E.
14.3. LINEAR ISOMETRIES (ALSO CALLED UNITARY TRANSFORMATIONS) 529
The Gram–Schmidt orthonormalization procedure also applies to Hermitian spaces of
finite dimension, without any changes from the Euclidean case!
Proposition 14.12. Given a nontrivial Hermitian space E of finite dimension n ≥ 1, from
any basis (e1, . . . , en) for E we can construct an orthonormal basis (u1, . . . , un) for E with
the property that for every k, 1 ≤ k ≤ n, the families (e1, . . . , ek) and (u1, . . . , uk) generate
the same subspace.
Remark: The remarks made after Proposition 12.10 also apply here, except that in the
QR-decomposition, Q is a unitary matrix.
As a consequence of Proposition 12.9 (or Proposition 14.12), given any Hermitian space
of finite dimension n, if (e1, . . . , en) is an orthonormal basis for E, then for any two vectors
u = u1e1 + · · · + unen and v = v1e1 + · · · + vnen, the Hermitian product u · v is expressed as
u · v = (u1e1 + · · · + unen) · (v1e1 + · · · + vnen) =
nX
i=1
uivi
,
and the norm k uk as
k
uk = k u1e1 + · · · + unenk =

nX
i=1
|ui
|
2

1/2
.
The fact that a Hermitian space always has an orthonormal basis implies that any Gram
matrix G can be written as
G = Q
∗Q,
for some invertible matrix Q. Indeed, we know that in a change of basis matrix, a Gram
matrix G becomes G0 = P
∗GP. If the basis corresponding to G0 is orthonormal, then G0 = I,
so G = (P
−1
)
∗P
−1
.
Proposition 12.11 also holds unchanged.
Proposition 14.13. Given any nontrivial Hermitian space E of finite dimension n ≥ 1, for
any subspace F of dimension k, the orthogonal complement F
⊥ of F has dimension n − k,
and E = F ⊕ F
⊥. Furthermore, we have F
⊥⊥ = F.
14.3 Linear Isometries (Also Called Unitary Transfor￾mations)
In this section we consider linear maps between Hermitian spaces that preserve the Hermitian
norm. All definitions given for Euclidean spaces in Section 12.5 extend to Hermitian spaces,
530 CHAPTER 14. HERMITIAN SPACES
except that orthogonal transformations are called unitary transformation, but Proposition
12.12 extends only with a modified Condition (2). Indeed, the old proof that (2) implies
(3) does not work, and the implication is in fact false! It can be repaired by strengthening
Condition (2). For the sake of completeness, we state the Hermitian version of Definition
12.5.
Definition 14.7. Given any two nontrivial Hermitian spaces E and F of the same finite
dimension n, a function f : E → F is a unitary transformation, or a linear isometry, if it is
linear and
k
f(u)k = k uk , for all u ∈ E.
Proposition 12.12 can be salvaged by strengthening Condition (2).
Proposition 14.14. Given any two nontrivial Hermitian spaces E and F of the same finite
dimension n, for every function f : E → F, the following properties are equivalent:
(1) f is a linear map and k f(u)k = k uk , for all u ∈ E;
(2) k f(v) − f(u)k = k v − uk and f(iu) = if(u), for all u, v ∈ E.
(3) f(u) · f(v) = u · v, for all u, v ∈ E.
Furthermore, such a map is bijective.
Proof. The proof that (2) implies (3) given in Proposition 12.12 needs to be revised as
follows. We use the polarization identity
2ϕ(u, v) = (1 + i)(k uk
2 + k vk
2
) − ku − vk
2 − ik u − ivk 2
.
Since f(iv) = if(v), we get f(0) = 0 by setting v = 0, so the function f preserves distance
and norm, and we get
2ϕ(f(u), f(v)) = (1 + i)(k f(u)k
2 + k f(v)k
2
) − kf(u) − f(v)k
2
− ik f(u) − if(v)k
2
= (1 + i)(k f(u)k
2 + k f(v)k
2
) − kf(u) − f(v)k
2
− ik f(u) − f(iv)k
2
= (1 + i)(k uk
2 + k vk
2
) − ku − vk
2 − ik u − ivk 2
= 2ϕ(u, v),
which shows that f preserves the Hermitian inner product as desired. The rest of the proof
is unchanged.
Remarks:
14.4. THE UNITARY GROUP, UNITARY MATRICES 531
(i) In the Euclidean case, we proved that the assumption
k
f(v) − f(u)k = k v − uk for all u, v ∈ E and f(0) = 0 (20 )
implies (3). For this we used the polarization identity
2u · v = k uk
2 + k vk
2 − ku − vk
2
.
In the Hermitian case the polarization identity involves the complex number i. In fact,
the implication (20 ) implies (3) is false in the Hermitian case! Conjugation z 7→ z
satisfies (20 ) since
|z2 − z1| = |z2 − z1| = |z2 − z1|,
and yet, it is not linear!
(ii) If we modify (2) by changing the second condition by now requiring that there be some
τ ∈ E such that
f(τ + iu) = f(τ ) + i(f(τ + u) − f(τ ))
for all u ∈ E, then the function g : E → E defined such that
g(u) = f(τ + u) − f(τ )
satisfies the old conditions of (2), and the implications (2) → (3) and (3) → (1) prove
that g is linear, and thus that f is affine. In view of the first remark, some condition
involving i is needed on f, in addition to the fact that f is distance-preserving.
14.4 The Unitary Group, Unitary Matrices
In this section, as a mirror image of our treatment of the isometries of a Euclidean space,
we explore some of the fundamental properties of the unitary group and of unitary matrices.
As an immediate corollary of the Gram–Schmidt orthonormalization procedure, we obtain
the QR-decomposition for invertible matrices. In the Hermitian framework, the matrix of
the adjoint of a linear map is not given by the transpose of the original matrix, but by
the conjugate of the original matrix. For the reader’s convenience we recall the following
definitions from Section 9.2.
Definition 14.8. Given a complex m × n matrix A, the transpose A> of A is the n × m
matrix A> =
￾ a
>i j defined such that
a
>i j = aj i,
and the conjugate A of A is the m × n matrix A = (bi j ) defined such that
bi j = ai j
532 CHAPTER 14. HERMITIAN SPACES
for all i, j, 1 ≤ i ≤ m, 1 ≤ j ≤ n. The adjoint A∗ of A is the matrix defined such that
A
∗ = (A> ) = ￾ A

> .
Proposition 14.15. Let E be any Hermitian space of finite dimension n, and let f : E → E
be any linear map. The following properties hold:
(1) The linear map f : E → E is an isometry iff
f ◦ f
∗ = f
∗
◦ f = id.
(2) For every orthonormal basis (e1, . . . , en) of E, if the matrix of f is A, then the matrix
of f
∗
is the adjoint A∗ of A, and f is an isometry iff A satisfies the identities
A A∗ = A
∗A = In,
where In denotes the identity matrix of order n, iff the columns of A form an orthonor￾mal basis of C
n
, iff the rows of A form an orthonormal basis of C
n
.
Proof. (1) The proof is identical to that of Proposition 12.14 (1).
(2) If (e1, . . . , en) is an orthonormal basis for E, let A = (ai j ) be the matrix of f, and let
B = (bi j ) be the matrix of f
∗
. Since f
∗
is characterized by
f
∗
(u) · v = u · f(v)
for all u, v ∈ E, using the fact that if w = w1e1 + · · · + wnen, we have wk = w · ek, for all k,
1 ≤ k ≤ n; letting u = ei and v = ej
, we get
bj i = f
∗
(ei) · ej = ei
· f(ej ) = f(ej ) · ei = ai j ,
for all i, j, 1 ≤ i, j ≤ n. Thus, B = A∗
. Now if X and Y are arbitrary matrices over the basis
(e1, . . . , en), denoting as usual the jth column of X by Xj
, and similarly for Y , a simple
calculation shows that
Y
∗X = (X
j
· Y
i
)1≤i,j≤n.
Then it is immediately verified that if X = Y = A, then A∗A = A A∗ = In iff the column
vectors (A1
, . . . , An
) form an orthonormal basis. Thus, from (1), we see that (2) is clear.
Proposition 12.14 shows that the inverse of an isometry f is its adjoint f
∗
. Proposition
12.14 also motivates the following definition.
Definition 14.9. A complex n × n matrix is a unitary matrix if
A A∗ = A
∗A = In.
14.4. THE UNITARY GROUP, UNITARY MATRICES 533
Remarks:
(1) The conditions A A∗ = In, A∗A = In, and A−1 = A∗ are equivalent. Given any two
orthonormal bases (u1, . . . , un) and (v1, . . . , vn), if P is the change of basis matrix from
(u1, . . . , un) to (v1, . . . , vn), it is easy to show that the matrix P is unitary. The proof
of Proposition 14.14 (3) also shows that if f is an isometry, then the image of an
orthonormal basis (u1, . . . , un) is an orthonormal basis.
(2) Using the explicit formula for the determinant, we see immediately that
det(A) = det(A).
If f is a unitary transformation and A is its matrix with respect to any orthonormal
basis, from AA∗ = I, we get
det(AA∗
) = det(A) det(A
∗
) = det(A)det(A> ) = det(A)det(A) = | det(A)|
2
,
and so | det(A)| = 1. It is clear that the isometries of a Hermitian space of dimension
n form a group, and that the isometries of determinant +1 form a subgroup.
This leads to the following definition.
Definition 14.10. Given a Hermitian space E of dimension n, the set of isometries f : E →
E forms a subgroup of GL(E, C) denoted by U(E), or U(n) when E = C
n
, called the
unitary group (of E). For every isometry f we have | det(f)| = 1, where det(f) denotes
the determinant of f. The isometries such that det(f) = 1 are called rotations, or proper
isometries, or proper unitary transformations, and they form a subgroup of the special
linear group SL(E, C) (and of U(E)), denoted by SU(E), or SU(n) when E = C
n
, called
the special unitary group (of E). The isometries such that det(f) 6 = 1 are called improper
isometries, or improper unitary transformations, or flip transformations.
A very important example of unitary matrices is provided by Fourier matrices (up to a
factor of √
n), matrices that arise in the various versions of the discrete Fourier transform.
For more on this topic, see the problems, and Strang [169, 172].
The group SU(2) turns out to be the group of unit quaternions, invented by Hamilton.
This group plays an important role in the representation of rotations in SO(3) used in
computer graphics and robotics; see Chapter 16.
Now that we have the definition of a unitary matrix, we can explain how the Gram–
Schmidt orthonormalization procedure immediately yields the QR-decomposition for matri￾ces.
Definition 14.11. Given any complex n×n matrix A, a QR-decomposition of A is any pair
of n × n matrices (U, R), where U is a unitary matrix and R is an upper triangular matrix
such that A = UR.
534 CHAPTER 14. HERMITIAN SPACES
Proposition 14.16. Given any n × n complex matrix A, if A is invertible, then there is a
unitary matrix U and an upper triangular matrix R with positive diagonal entries such that
A = UR.
The proof is absolutely the same as in the real case!
Remark: If A is invertible and if A = U1R1 = U2R2 are two QR-decompositions for A,
then
R1R2
−1 = U1
∗U2.
Then it is easy to show that there is a diagonal matrix D with diagonal entries such that
|dii| = 1 for i = 1, . . . , n, and U2 = U1D, R2 = D∗R1.
We have the following version of the Hadamard inequality for complex matrices. The
proof is essentially the same as in the Euclidean case but it uses Proposition 14.16 instead
of Proposition 12.16.
Proposition 14.17. (Hadamard) For any complex n × n matrix A = (aij ), we have
| det(A)| ≤
nY
i=1

nX
j=1
|aij |
2

1/2
and | det(A)| ≤
nY
j=1

nX
i=1
|aij |
2

1/2
.
Moreover, equality holds iff either A has orthogonal rows in the left inequality or orthogonal
columns in the right inequality.
We also have the following version of Proposition 12.18 for Hermitian matrices. The
proof of Proposition 12.18 goes through because the Cholesky decomposition for a Hermitian
positive definite A matrix holds in the form A = B∗B, where B is upper triangular with
positive diagonal entries. The details are left to the reader.
Proposition 14.18. (Hadamard) For any complex n×n matrix A = (aij ), if A is Hermitian
positive semidefinite, then we have
det(A) ≤
nY
i=1
aii.
Moreover, if A is positive definite, then equality holds iff A is a diagonal matrix.
14.5 Hermitian Reflections and QR-Decomposition
If A is an n × n complex singular matrix, there is some (not necessarily unique) QR￾decomposition A = QR with Q a unitary matrix which is a product of Householder re-
flections and R an upper triangular matrix, but the proof is more involved. One way to
proceed is to generalize the notion of hyperplane reflection. This is not really surprising
since in the Hermitian case there are improper isometries whose determinant can be any
unit complex number. Hyperplane reflections are generalized as follows.
14.5. HERMITIAN REFLECTIONS AND QR-DECOMPOSITION 535
Definition 14.12. Let E be a Hermitian space of finite dimension. For any hyperplane H,
for any nonnull vector w orthogonal to H, so that E = H ⊕ G, where G = Cw, a Hermitian
reflection about H of angle θ is a linear map of the form ρH, θ : E → E, defined such that
ρH, θ(u) = pH(u) + e
iθpG(u),
for any unit complex number e
iθ 6 = 1 (i.e. θ 6 = k2π). For any nonzero vector w ∈ E, we
denote by ρw,θ the Hermitian reflection given by ρH,θ, where H is the hyperplane orthogonal
to w.
Since u = pH(u) + pG(u), the Hermitian reflection ρw, θ is also expressed as
ρw, θ(u) = u + (e
iθ − 1)pG(u),
or as
ρw, θ(u) = u + (e
iθ − 1) (u · w)
k
wk
2 w.
Note that the case of a standard hyperplane reflection is obtained when e
iθ = −1, i.e., θ = π.
In this case,
ρw, π(u) = u − 2
(u · w)
k
wk
2 w,
and the matrix of such a reflection is a Householder matrix, as in Section 13.1, except that
w may be a complex vector.
We leave as an easy exercise to check that ρw, θ is indeed an isometry, and that the inverse
of ρw, θ is ρw, −θ. If we pick an orthonormal basis (e1, . . . , en) such that (e1, . . . , en−1) is an
orthonormal basis of H, the matrix of ρw, θ is

In−1 0
0 e
iθ
We now come to the main surprise. Given any two distinct vectors u and v such that
k
using two Hermitian reflections!
uk = k vk , there isn’t always a hyperplane reflection mapping u to v, but this can be done
Proposition 14.19. Let E be any nontrivial Hermitian space.
(1) For any two vectors u, v ∈ E such that u 6 = v and k uk = k vk , if u · v = e
iθ|u · v|, then
the (usual) reflection s about the hyperplane orthogonal to the vector v − e
−iθu is such
that s(u) = e
iθv.
(2) For any nonnull vector v ∈ E, for any unit complex number e
iθ 6 = 1, there is a Hermi￾tian reflection ρv,θ such that
ρv,θ(v) = e
iθv.
As a consequence, for u and v as in (1), we have ρv,−θ ◦ s(u) = v.
536 CHAPTER 14. HERMITIAN SPACES
Proof. (1) Consider the (usual) reflection about the hyperplane orthogonal to w = v −e
−iθu.
We have
s(u) = u − 2
(u · (v − e
−iθu))
k
v − e
−iθuk
2
(v − e
−iθu).
We need to compute
−2u · (v − e
−iθu) and (v − e
−iθu) · (v − e
−iθu).
Since u · v = e
iθ|u · v|, we have
e
−iθu · v = |u · v| and e
iθv · u = |u · v|.
Using the above and the fact that k uk = k vk , we get
−2u · (v − e
−iθu) = 2e
iθ k uk
2 − 2u · v,
= 2e
iθ(k uk
2 − |u · v|),
and
(v − e
−iθu) · (v − e
−iθu) = k vk
2 + k uk
2 − e
−iθu · v − e
iθv · u,
= 2(k uk
2 − |u · v|),
and thus,
−2
(u · (v − e
−iθu))
k
(v − e
−iθu)k
2
(v − e
−iθu) = e
iθ(v − e
−iθu).
But then,
s(u) = u + e
iθ(v − e
−iθu) = u + e
iθv − u = e
iθv,
and s(u) = e
iθv, as claimed.
(2) This part is easier. Consider the Hermitian reflection
ρv,θ(u) = u + (e
iθ − 1) (u · v)
k
vk
2
v.
We have
ρv,θ(v) = v + (e
iθ − 1) (v · v)
k
vk
2
v,
= v + (e
iθ − 1)v,
= e
iθv.
Thus, ρv,θ(v) = e
iθv. Since ρv,θ is linear, changing the argument v to e
iθv, we get
ρv,−θ(e
iθv) = v,
and thus, ρv,−θ ◦ s(u) = v.
14.5. HERMITIAN REFLECTIONS AND QR-DECOMPOSITION 537
Remarks:
(1) If we use the vector v + e
−iθu instead of v − e
−iθu, we get s(u) = −e
iθv.
(2) Certain authors, such as Kincaid and Cheney [102] and Ciarlet [41], use the vector
u + e
iθv instead of the vector v + e
−iθu. The effect of this choice is that they also get
s(u) = −e
iθv.
(3) If v = k uk e1, where e1 is a basis vector, u · e1 = a1, where a1 is just the coefficient
of u over the basis vector e1. Then, since u · e1 = e
iθ|a1|, the choice of the plus sign
in the vector k uk e1 + e
−iθu has the effect that the coefficient of this vector over e1 is
k
(we need to divide by the square norm of this vector).
uk + |a1|, and no cancellations takes place, which is preferable for numerical stability
We now show that the QR-decomposition in terms of (complex) Householder matrices
holds for complex matrices. We need the version of Proposition 14.19 and a trick at the end
of the argument, but the proof is basically unchanged.
Proposition 14.20. Let E be a nontrivial Hermitian space of dimension n. Given any
orthonormal basis (e1, . . . , en), for any n-tuple of vectors (v1, . . . , vn), there is a sequence
of n − 1 isometries h1, . . . , hn−1, such that hi is a (standard) hyperplane reflection or the
identity, and if (r1, . . . , rn) are the vectors given by
rj = hn−1 ◦ · · · ◦ h2 ◦ h1(vj ), 1 ≤ j ≤ n,
then every rj is a linear combination of the vectors (e1, . . . , ej ), (1 ≤ j ≤ n). Equivalently,
the matrix R whose columns are the components of the rj over the basis (e1, . . . , en) is an
upper triangular matrix. Furthermore, if we allow one more isometry hn of the form
hn = ρen, ϕn ◦ · · · ◦ ρe1,ϕ1
after h1, . . . , hn−1, we can ensure that the diagonal entries of R are nonnegative.
Proof. The proof is very similar to the proof of Proposition 13.3, but it needs to be modified
a little bit since Proposition 14.19 is weaker than Proposition 13.2. We explain how to
modify the induction step, leaving the base case and the rest of the proof as an exercise.
As in the proof of Proposition 13.3, the vectors (e1, . . . , ek) form a basis for the subspace
denoted as Uk
0
, the vectors (ek+1, . . . , en) form a basis for the subspace denoted as Uk
00
, the
subspaces Uk
0
and Uk
00
are orthogonal, and E = Uk
0 ⊕ Uk
00
. Let
uk+1 = hk ◦ · · · ◦ h2 ◦ h1(vk+1).
We can write
uk+1 = u
0k+1 + u
00k+1,
538 CHAPTER 14. HERMITIAN SPACES
where u
0k+1 ∈ Uk
0
and u
00k+1 ∈ Uk
00
. Let
