to represent A as an array according to this ordering of the rows and columns.
We define some operations on matrices as follows.
Definition 3.13. Given two m × n matrices A = (ai j ) and B = (bi j ), we define their sum
A + B as the matrix C = (ci j ) such that ci j = ai j + bi j ; that is,


a1 1 a1 2 . . . a1 n
a2 1 a2 2 . . . a2 n
.
.
.
.
.
.
.
.
.
.
.
.
am 1 am 2 . . . am n


+


b1 1 b1 2 . . . b1 n
b2 1 b2 2 . . . b2 n
.
.
.
.
.
.
.
.
.
.
.
.
bm 1 bm 2 . . . bm n


=


a1 1 + b1 1 a1 2 + b1 2 . . . a1 n + b1 n
a2 1 + b2 1 a2 2 + b2 2 . . . a2 n + b2 n
.
.
.
.
.
.
.
.
.
.
.
.
am 1 + bm 1 am 2 + bm 2 . . . am n + bm n


.
86 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
For any matrix A = (ai j ), we let −A be the matrix (−ai j ). Given a scalar λ ∈ K, we define
the matrix λA as the matrix C = (ci j ) such that ci j = λai j ; that is
λ


a1 1 a1 2 . . . a1 n
a2 1 a2 2 . . . a2 n
.
.
.
.
.
.
.
.
.
.
.
.
am 1 am 2 . . . am n


=


λa1 1 λa1 2 . . . λa1 n
λa2 1 λa2 2 . . . λa2 n
.
.
.
.
.
.
.
.
.
.
.
.
λam 1 λam 2 . . . λam n


.
Given an m × n matrices A = (ai k) and an n× p matrices B = (bk j ), we define their product
AB as the m × p matrix C = (ci j ) such that
ci j =
nX
k=1
ai kbk j ,
for 1 ≤ i ≤ m, and 1 ≤ j ≤ p. In the product AB = C shown below


a1 1 a1 2 . . . a1 n
a2 1 a2 2 . . . a2 n
.
.
.
.
.
.
.
.
.
.
.
.
am 1 am 2 . . . am n




b1 1 b1 2 . . . b1 p
b2 1 b2 2 . . . b2 p
.
.
.
.
.
.
.
.
.
.
.
.
bn 1 bn 2 . . . bn p


=


c1 1 c1 2 . . . c1 p
c2 1 c2 2 . . . c2 p
.
.
.
.
.
.
.
.
.
.
.
.
cm 1 cm 2 . . . cm p


,
note that the entry of index i and j of the matrix AB obtained by multiplying the matrices
A and B can be identified with the product of the row matrix corresponding to the i-th row
of A with the column matrix corresponding to the j-column of B:
(ai 1 · · · ai n)


b1 j
.
.
.
bn j

 =
nX
k=1
ai kbk j .
Definition 3.14. The square matrix In of dimension n containing 1 on the diagonal and 0
everywhere else is called the identity matrix . It is denoted by
In =


1 0 . . . 0
0 1
.
. . . 0
.
.
.
.
.
.
.
.
.
0 0 . . . 1
.
.


Definition 3.15. Given an m × n matrix A = (ai j ), its transpose A> = (a
>j i), is the
n × m-matrix such that a
>j i = ai j , for all i, 1 ≤ i ≤ m, and all j, 1 ≤ j ≤ n.
The transpose of a matrix A is sometimes denoted by At
, or even by tA. Note that the
transpose A> of a matrix A has the property that the j-th row of A> is the j-th column of
3.6. MATRICES 87
A. In other words, transposition exchanges the rows and the columns of a matrix. Here is
an example. If A is the 5 × 6 matrix
A =


1 2 3 4 5 6
7 1 2 3 4 5
8 7 1 2 3 4
10 9 8 7 1 2
9 8 7 1 2 3


,
then A> is the 6 × 5 matrix
A
> =


1 7 8 9 10
2 1 7 8 9
3 2 1 7 8
4 3 2 1 7
5 4 3 2 1
6 5 4 3 2


.
The following observation will be useful later on when we discuss the SVD. Given any
m × n matrix A and any n × p matrix B, if we denote the columns of A by A1
, . . . , An and
the rows of B by B1, . . . , Bn, then we have
AB = A
1B1 + · · · + A
nBn.
For every square matrix A of dimension n, it is immediately verified that AIn = InA = A.
Definition 3.16. For any square matrix A of dimension n, if a matrix B such that AB =
BA = In exists, then it is unique, and it is called the inverse of A. The matrix B is also
denoted by A−1
. An invertible matrix is also called a nonsingular matrix, and a matrix that
is not invertible is called a singular matrix.
The following result is a matrix analog of Proposition 3.21.
Proposition 3.13. If a square matrix A ∈ Mn(K) has a left inverse, that is a matrix B
such that BA = In, or a right inverse, that is a matrix C such that AC = In, then A is
actually invertible. Furthermore, B = A−1 and C = A−1
.
Proof. Proposition 3.13 follows from Proposition 3.21 and the fact that matrices represent
linear maps. We can also give a direct proof as follows. Suppose that there is a matrix B
such that BA = In. This implies that the columns A1
, . . . , An of A are linearly independent,
because if
Aλ = λ1A
1 + · · · + λnA
n = 0,
where λ ∈ Kn
is the column vector
λ =


λ1
.
.
λ
.
n

 ,
88 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
for some λ1, . . . , λn ∈ K, by multiplying both sides of the equation Aλ = 0 by B we get
BAλ = Inλ = λ = B0 = 0,
so λ = 0. Then since (A1
, . . . , An
) are n linearly independent vectors in Kn
, they form
a basis of Kn
. Consequently, for every vector b ∈ Kn
, there is a unique column vector
(x1, . . . , xn) ∈ Kn
such that
Ax = x1A
1 + · · · + xnA
n = b,
where x is the column vector
x =


x
.
.
1
x
.
n

 .
Thus we can solve the n equations
Axj = ej
, 1 ≤ j ≤ n,
where ej = (0, . . . , 0, 1, 0, . . . , 0) is the jth canonical basis vector in Kn
. These equations
yield the matrix equation
AX = In,
where X = (x
1
· · · x
n
) is the n × n matrix whose jth column is x
j
. Consequently, X is a
right inverse of A. Now A has a left inverse B and a right inverse X, so by Proposition 2.2,
we have X = B, so A is invertible and its inverse is equal to B.
Let us now assume that there is a matrix C such that AC = In. We can repeat the
previous proof with C playing the role of A and A playing the role of B to conclude that
C is invertible and that C
−1 = A. But then C
−1
is invertible with inverse C, and since
C = (C
−1
)
−1 = A−1
, we conclude that A is invertible and that its inverse is equal to C.
Using Proposition 2.3 (or mimicking the computations in its proof), we note that if A
and B are two n × n invertible matrices, then AB is also invertible and (AB)
−1 = B−1A−1
.
An important criterion for a square matrix to be invertible is stated next. Another proof
is provided in Proposition 4.4 .
Proposition 3.14. A square matrix A ∈ Mn(K) is invertible iff its columns (A1
, . . . , An
)
are linearly independent.
Proof. If A is invertible, then in particular it has a left inverse A−1
, so the first part of
the proof of Proposition 3.13 with B = A−1 proves that the columns (A1
, . . . , An
) of A
are linearly independent. This fact is also proven as part of the proof of Proposition 4.4.
Conversely, assume that the columns (A1
, . . . , An
) of A are linearly independent. The second
part of the proof of Proposition 3.13 shows that A is invertible.
3.6. MATRICES 89
Another useful criterion for a square matrix to be invertible is stated next.
Proposition 3.15. A square matrix A ∈ Mn(K) is invertible iff for any x ∈ Kn
, the
equation Ax = 0 implies that x = 0.
Proof. If A is invertible and if Ax = 0, then by multiplying both sides of the equation x = 0
by A−1
, we get
A
−1Ax = Inx = x = A
−1
0 = 0.
Conversely, for any x = (x1, . . . , xn) ∈ Kn
, since
Ax = x1A
1 + · · · + xnA
n
,
the condition Ax = 0 implies x = 0 is equivalent to the linear independence of the columns
(A1
, . . . , An
) of A. By Proposition 3.14, the matrix A is invertible.
It is immediately verified that the set Mm,n(K) of m ×n matrices is a vector space under
addition of matrices and multiplication of a matrix by a scalar.
Definition 3.17. The m × n-matrices Eij = (eh k), are defined such that ei j = 1, and
eh k = 0, if h 6 = i or k 6 = j; in other words, the (i, j)-entry is equal to 1 and all other entries
are 0.
Here are the Eij matrices for m = 2 and n = 3:
E11 =

1 0 0
0 0 0 , E12 =

0 1 0
0 0 0 , E13 =

0 0 1
0 0 0
E21 =

0 0 0
1 0 0 , E22 =

0 0 0
0 1 0 , E23 =

0 0 0
0 0 1 .
It is clear that every matrix A = (ai j ) ∈ Mm,n(K) can be written in a unique way as
A =
mX
i=1
nX
j=1
ai jEij .
Thus, the family (Eij )1≤i≤m,1≤j≤n is a basis of the vector space Mm,n(K), which has dimension
mn.
Remark: Definition 3.12 and Definition 3.13 also make perfect sense when K is a (com￾mutative) ring rather than a field. In this more general setting, the framework of vector
spaces is too narrow, but we can consider structures over a commutative ring A satisfying
all the axioms of Definition 3.1. Such structures are called modules. The theory of modules
is (much) more complicated than that of vector spaces. For example, modules do not always
have a basis, and other properties holding for vector spaces usually fail for modules. When
a module has a basis, it is called a free module. For example, when A is a commutative
90 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
ring, the structure An
is a module such that the vectors ei
, with (ei)i = 1 and (ei)j = 0 for
j 6 = i, form a basis of An
. Many properties of vector spaces still hold for An
. Thus, An
is a
free module. As another example, when A is a commutative ring, Mm,n(A) is a free module
with basis (Ei,j )1≤i≤m,1≤j≤n. Polynomials over a commutative ring also form a free module
of infinite dimension.
The properties listed in Proposition 3.16 are easily verified, although some of the com￾putations are a bit tedious. A more conceptual proof is given in Proposition 4.1.
Proposition 3.16. (1) Given any matrices A ∈ Mm,n(K), B ∈ Mn,p(K), and C ∈ Mp,q(K),
we have
(AB)C = A(BC);
that is, matrix multiplication is associative.
(2) Given any matrices A, B ∈ Mm,n(K), and C, D ∈ Mn,p(K), for all λ ∈ K, we have
(A + B)C = AC + BC
A(C + D) = AC + AD
(λA)C = λ(AC)
A(λC) = λ(AC),
so that matrix multiplication ·: Mm,n(K) × Mn,p(K) → Mm,p(K) is bilinear.
The properties of Proposition 3.16 together with the fact that AIn = InA = A for all
square n×n matrices show that Mn(K) is a ring with unit In (in fact, an associative algebra).
This is a noncommutative ring with zero divisors, as shown by the following example.
Example 3.5. For example, letting A, B be the 2 × 2-matrices
A =

1 0
0 0 , B =

0 0
1 0 ,
then
AB =

1 0
0 0 
0 0
1 0 =

0 0
0 0 ,
and
BA =

0 0
1 0 
1 0
0 0 =

0 0
1 0 .
Thus AB 6 = BA, and AB = 0, even though both A, B 6 = 0.
3.7. LINEAR MAPS 91
3.7 Linear Maps
Now that we understand vector spaces and how to generate them, we would like to be able
to transform one vector space E into another vector space F. A function between two vector
spaces that preserves the vector space structure is called a homomorphism of vector spaces,
or linear map. Linear maps formalize the concept of linearity of a function.
Keep in mind that linear maps, which are transformations of
space, are usually far more important than the spaces
themselves.
In the rest of this section, we assume that all vector spaces are over a given field K (say
R).
Definition 3.18. Given two vector spaces E and F, a linear map between E and F is a
function f : E → F satisfying the following two conditions:
f(x + y) = f(x) + f(y) for all x, y ∈ E;
f(λx) = λf(x) for all λ ∈ K, x ∈ E.
Setting x = y = 0 in the first identity, we get f(0) = 0. The basic property of linear maps
is that they transform linear combinations into linear combinations. Given any finite family
(ui)i∈I of vectors in E, given any family (λi)i∈I of scalars in K, we have
f

X
i∈I
λiui
 =
X
i∈I
λif(ui).
The above identity is shown by induction on |I| using the properties of Definition 3.18.
Example 3.6.
1. The map f : R
2 → R
2 defined such that
x
0 = x − y
y
0 = x + y
is a linear map. The reader should check that it is the composition of a rotation by
π/4 with a magnification of ratio √
2.
2. For any vector space E, the identity map id: E → E given by
id(u) = u for all u ∈ E
is a linear map. When we want to be more precise, we write idE instead of id.
92 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
3. The map D : R[X] → R[X] defined such that
D(f(X)) = f
0 (X),
where f
0 (X) is the derivative of the polynomial f(X), is a linear map.
4. The map Φ: C([a, b]) → R given by
Φ(f) = Z
b
a
f(t)dt,
where C([a, b]) is the set of continuous functions defined on the interval [a, b], is a linear
map.
5. The function h−, −i: C([a, b]) × C([a, b]) → R given by
h
f, gi =
Z
b
a
f(t)g(t)dt,
is linear in each of the variable f, g. It also satisfies the properties h f, gi = h g, fi and
h
f, fi = 0 iff f = 0. It is an example of an inner product.
Definition 3.19. Given a linear map f : E → F, we define its image (or range) Im f = f(E),
as the set
Im f = {y ∈ F | (∃x ∈ E)(y = f(x))},
and its Kernel (or nullspace) Ker f = f
−1
(0), as the set
Ker f = {x ∈ E | f(x) = 0}.
The derivative map D : R[X] → R[X] from Example 3.6(3) has kernel the constant
polynomials, so Ker D = R. If we consider the second derivative D ◦ D : R[X] → R[X], then
the kernel of D ◦D consists of all polynomials of degree ≤ 1. The image of D : R[X] → R[X]
is actually R[X] itself, because every polynomial P(X) = a0Xn + · · · + an−1X + an of degree
n is the derivative of the polynomial Q(X) of degree n + 1 given by
Q(X) = a0
Xn+1
n + 1
+ · · · + an−1
X2
2
+ anX.
On the other hand, if we consider the restriction of D to the vector space R[X]n of polyno￾mials of degree ≤ n, then the kernel of D is still R, but the image of D is the R[X]n−1, the
vector space of polynomials of degree ≤ n − 1.
Proposition 3.17. Given a linear map f : E → F, the set Im f is a subspace of F and the
set Ker f is a subspace of E. The linear map f : E → F is injective iff Ker f = (0) (where
(0) is the trivial subspace {0}).
3.7. LINEAR MAPS 93
Proof. Given any x, y ∈ Im f, there are some u, v ∈ E such that x = f(u) and y = f(v),
and for all λ, µ ∈ K, we have
f(λu + µv) = λf(u) + µf(v) = λx + µy,
and thus, λx + µy ∈ Im f, showing that Im f is a subspace of F.
Given any x, y ∈ Ker f, we have f(x) = 0 and f(y) = 0, and thus,
f(λx + µy) = λf(x) + µf(y) = 0,
that is, λx + µy ∈ Ker f, showing that Ker f is a subspace of E.
First, assume that Ker f = (0). We need to prove that f(x) = f(y) implies that x = y.
However, if f(x) = f(y), then f(x) − f(y) = 0, and by linearity of f we get f(x − y) = 0.
Because Ker f = (0), we must have x − y = 0, that is x = y, so f is injective. Conversely,
assume that f is injective. If x ∈ Ker f, that is f(x) = 0, since f(0) = 0 we have f(x) = f(0),
and by injectivity, x = 0, which proves that Ker f = (0). Therefore, f is injective iff
Ker f = (0).
Since by Proposition 3.17, the image Im f of a linear map f is a subspace of F, we can
define the rank rk(f) of f as the dimension of Im f.
Definition 3.20. Given a linear map f : E → F, the rank rk(f) of f is the dimension of
the image Im f of f.
A fundamental property of bases in a vector space is that they allow the definition of
linear maps as unique homomorphic extensions, as shown in the following proposition.
Proposition 3.18. Given any two vector spaces E and F, given any basis (ui)i∈I of E,
given any other family of vectors (vi)i∈I in F, there is a unique linear map f : E → F such
that f(ui) = vi for all i ∈ I. Furthermore, f is injective iff (vi)i∈I is linearly independent,
and f is surjective iff (vi)i∈I generates F.
Proof. If such a linear map f : E → F exists, since (ui)i∈I is a basis of E, every vector x ∈ E
can written uniquely as a linear combination
x =
X
i∈I
xiui
,
and by linearity, we must have
f(x) = X
i∈I
xif(ui) = X
i∈I
xivi
.
Define the function f : E → F, by letting
f(x) = X
i∈I
xivi
, x =
X
i∈I
xiui
, (†)
94 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
for some unique coordinates (xi)i∈I of x.
To prove that f as defined by (†) is linear it suffices to prove that
f(λx + µy) = λf(x) + µf(y)
for all x, y ∈ E and all λ, µ ∈ R. Since (ui)i∈I is a basis of E, we have
x =
X
i∈I
xiui
, y =
X
i∈I
yiui
,
for some unique coordinates (xi)i∈I of x and (yi)i∈I of y, and by (†) we have
f(x) = X
i∈I
xivi
, f(y) = X
i∈I
yivi
,
and since
λx + µy = λ

X
i∈I
xiui
 + µ

X
i∈I
yiui
 =
X
i∈I
(λxi + µyi)ui
,
by (†),
f(λx + µy) = f

X
i∈I
(λxi + µyi)ui
 =
X
i∈I
(λxi + µyi)vi
= λ

X
i∈I
xivi
 + µ

X
i∈I
yivi
 = λf(x) + µf(y),
proving that f is linear. The map f is unique by (†), and obviously, f(ui) = vi
.
Now assume that f is injective. Let (λi)i∈I be any family of scalars, and assume that
X
i∈I
λivi = 0.
Since vi = f(ui) for every i ∈ I, we have
f

X
i∈I
λiui
 =
X
i∈I
λif(ui) = X
i∈I
λivi = 0.
Since f is injective iff Ker f = (0), we have
X
i∈I
λiui = 0,
and since (ui)i∈I is a basis, we have λi = 0 for all i ∈ I, which shows that (vi)i∈I is linearly
independent. Conversely, assume that (vi)i∈I is linearly independent. Since (ui)i∈I is a basis
of E, every vector x ∈ E is a linear combination x =
P i∈I
λiui of (ui)i∈I . If
f(x) = f

X
i∈I
λiui
 = 0,
3.7. LINEAR MAPS 95
then
X
i∈I
λivi =
X
i∈I
λif(ui) = f

X
i∈I
λiui
 = 0,
and λi = 0 for all i ∈ I because (vi)i∈I is linearly independent, which means that x = 0.
Therefore, Ker f = (0), which implies that f is injective. The part where f is surjective is
left as a simple exercise.
Figure 3.11 provides an illustration of Proposition 3.18 when E = R
3 and V = R
2
u = (1,0,0) 1
u = (0,1,0) 2
u = (0,0,1) 3 v = (1,1) 1 v = (-1,1) 2
v = (1,0) 3
f(u )1 - f(u ) 2
2f(u ) 3
E = 
f
F = R R2
3
f is not injective
defining f
Figure 3.11: Given u1 = (1, 0, 0), u2 = (0, 1, 0), u3 = (0, 0, 1) and v1 = (1, 1), v2 = (−1, 1),
v3 = (1, 0), define the unique linear map f : R
3 → R
2 by f(u1) = v1, f(u2) = v2, and
f(u3) = v3. This map is surjective but not injective since f(u1 − u2) = f(u1) − f(u2) =
(1, 1) − (−1, 1) = (2, 0) = 2f(u3) = f(2u3).
By the second part of Proposition 3.18, an injective linear map f : E → F sends a basis
(ui)i∈I to a linearly independent family (f(ui))i∈I of F, which is also a basis when f is
bijective. Also, when E and F have the same finite dimension n, (ui)i∈I is a basis of E, and
f : E → F is injective, then (f(ui))i∈I is a basis of F (by Proposition 3.8).
We can now show that the vector space K(I) of Definition 3.11 has a universal property
that amounts to saying that K(I)
is the vector space freely generated by I. Recall that
ι: I → K(I)
, such that ι(i) = ei
for every i ∈ I, is an injection from I to K(I)
.
Proposition 3.19. Given any set I, for any vector space F, and for any function f : I → F,
there is a unique linear map f : K(I) → F, such that
f = f ◦ ι,
96 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
as in the following diagram:
I
ι /
f !
❈❈❈❈❈❈❈❈❈
K(I)


f
F
Proof. If such a linear map f : K(I) → F exists, since f = f ◦ ι, we must have
f(i) = f(ι(i)) = f(ei),
for every i ∈ I. However, the family (ei)i∈I is a basis of K(I)
, and (f(i))i∈I is a family of
vectors in F, and by Proposition 3.18, there is a unique linear map f : K(I) → F such that
f(ei) = f(i) for every i ∈ I, which proves the existence and uniqueness of a linear map f
such that f = f ◦ ι.
The following simple proposition is also useful.
Proposition 3.20. Given any two vector spaces E and F, with F nontrivial, given any
family (ui)i∈I of vectors in E, the following properties hold:
(1) The family (ui)i∈I generates E iff for every family of vectors (vi)i∈I in F, there is at
most one linear map f : E → F such that f(ui) = vi for all i ∈ I.
(2) The family (ui)i∈I is linearly independent iff for every family of vectors (vi)i∈I in F,
there is some linear map f : E → F such that f(ui) = vi for all i ∈ I.
Proof. (1) If there is any linear map f : E → F such that f(ui) = vi
for all i ∈ I, since
(ui)i∈I generates E, every vector x ∈ E can be written as some linear combination
x =
X
i∈I
xiui
,
and by linearity, we must have
f(x) = X
i∈I
xif(ui) = X
i∈I
xivi
.
This shows that f is unique if it exists. Conversely, assume that (ui)i∈I does not generate E.
Since F is nontrivial, there is some some vector y ∈ F such that y 6 = 0. Since (ui)i∈I does
not generate E, there is some vector w ∈ E that is not in the subspace generated by (ui)i∈I .
By Theorem 3.11, there is a linearly independent subfamily (ui)i∈I0 of (ui)i∈I generating the
same subspace. Since by hypothesis, w ∈ E is not in the subspace generated by (ui)i∈I0
, by
Lemma 3.6 and by Theorem 3.11 again, there is a basis (ej )j∈I0∪J of E, such that ei = ui
for all i ∈ I0, and w = ej0
for some j0 ∈ J. Letting (vi)i∈I be the family in F such that
vi = 0 for all i ∈ I, defining f : E → F to be the constant linear map with value 0, we have
a linear map such that f(ui) = 0 for all i ∈ I. By Proposition 3.18, there is a unique linear
3.7. LINEAR MAPS 97
f
u = (1,0,0) 1
u = (0,1,0) 2
E = F = R R2
3
u = (1,0,0) 1
u = (0,1,0) 2
E = F = R R2
3
w = (0,0,1)
w = (0,0,1)
defining f as the zero
defining g
y = (1,0)
g(w) = y
Figure 3.12: Let E = R
3 and F = R
2
. The vectors u1 = (1, 0, 0), u2 = (0, 1, 0) do not
generate R
3
since both the zero map and the map g, where g(0, 0, 1) = (1, 0), send the peach
xy-plane to the origin.
map g : E → F such that g(w) = y, and g(ej ) = 0 for all j ∈ (I0 ∪ J) − {j0}. By definition
of the basis (ej )j∈I0∪J of E, we have g(ui) = 0 for all i ∈ I, and since f 6 = g, this contradicts
the fact that there is at most one such map. See Figure 3.12.
(2) If the family (ui)i∈I is linearly independent, then by Theorem 3.11, (ui)i∈I can be
extended to a basis of E, and the conclusion follows by Proposition 3.18. Conversely, assume
that (ui)i∈I is linearly dependent. Then there is some family (λi)i∈I of scalars (not all zero)
such that
X
i∈I
λiui = 0.
By the assumption, for any nonzero vector y ∈ F, for every i ∈ I, there is some linear map
fi
: E → F, such that fi(ui) = y, and fi(uj ) = 0, for j ∈ I − {i}. Then we would get
0 = fi(
X
i∈I
λiui) = X
i∈I
λifi(ui) = λiy,
and since y 6 = 0, this implies λi = 0 for every i ∈ I. Thus, (ui)i∈I is linearly independent.
Given vector spaces E, F, and G, and linear maps f : E → F and g : F → G, it is easily
verified that the composition g ◦ f : E → G of f and g is a linear map.
98 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
Definition 3.21. A linear map f : E → F is an isomorphism iff there is a linear map
g : F → E, such that
g ◦ f = idE and f ◦ g = idF . (∗)
The map g in Definition 3.21 is unique. This is because if g and h both satisfy g◦f = idE,
f ◦ g = idF , h ◦ f = idE, and f ◦ h = idF , then
g = g ◦ idF = g ◦ (f ◦ h) = (g ◦ f) ◦ h = idE ◦ h = h.
The map g satisfying (∗) above is called the inverse of f and it is also denoted by f
−1
.
Observe that Proposition 3.18 shows that if F = R
n
, then we get an isomorphism between
any vector space E of dimension |J| = n and R
n
. Proposition 3.18 also implies that if E
and F are two vector spaces, (ui)i∈I is a basis of E, and f : E → F is a linear map which is
an isomorphism, then the family (f(ui))i∈I is a basis of F.
One can verify that if f : E → F is a bijective linear map, then its inverse f
−1
: F → E,
as a function, is also a linear map, and thus f is an isomorphism.
Another useful corollary of Proposition 3.18 is this:
Proposition 3.21. Let E be a vector space of finite dimension n ≥ 1 and let f : E → E be
any linear map. The following properties hold:
(1) If f has a left inverse g, that is, if g is a linear map such that g ◦ f = id, then f is an
isomorphism and f
−1 = g.
(2) If f has a right inverse h, that is, if h is a linear map such that f ◦ h = id, then f is
an isomorphism and f
−1 = h.
Proof. (1) The equation g ◦ f = id implies that f is injective; this is a standard result
about functions (if f(x) = f(y), then g(f(x)) = g(f(y)), which implies that x = y since
g ◦ f = id). Let (u1, . . . , un) be any basis of E. By Proposition 3.18, since f is injective,
(f(u1), . . . , f(un)) is linearly independent, and since E has dimension n, it is a basis of
E (if (f(u1), . . . , f(un)) doesn’t span E, then it can be extended to a basis of dimension
strictly greater than n, contradicting Theorem 3.11). Then f is bijective, and by a previous
observation its inverse is a linear map. We also have
g = g ◦ id = g ◦ (f ◦ f
−1
) = (g ◦ f) ◦ f
−1 = id ◦ f
−1 = f
−1
.
(2) The equation f ◦ h = id implies that f is surjective; this is a standard result about
functions (for any y ∈ E, we have f(h(y)) = y). Let (u1, . . . , un) be any basis of E. By
Proposition 3.18, since f is surjective, (f(u1), . . . , f(un)) spans E, and since E has dimension
n, it is a basis of E (if (f(u1), . . . , f(un)) is not linearly independent, then because it spans
E, it contains a basis of dimension strictly smaller than n, contradicting Theorem 3.11).
Then f is bijective, and by a previous observation its inverse is a linear map. We also have
h = id ◦ h = (f
−1
◦ f) ◦ h = f
−1
◦ (f ◦ h) = f
−1
◦ id = f
−1
.
This completes the proof.
3.7. LINEAR MAPS 99
Definition 3.22. The set of all linear maps between two vector spaces E and F is denoted by
Hom(E, F) or by L(E; F) (the notation L(E; F) is usually reserved to the set of continuous
linear maps, where E and F are normed vector spaces). When we wish to be more precise and
specify the field K over which the vector spaces E and F are defined we write HomK(E, F).
The set Hom(E, F) is a vector space under the operations defined in Example 3.1, namely
(f + g)(x) = f(x) + g(x)
for all x ∈ E, and
(λf)(x) = λf(x)
for all x ∈ E. The point worth checking carefully is that λf is indeed a linear map, which
uses the commutativity of ∗ in the field K (typically, K = R or K = C). Indeed, we have
(λf)(µx) = λf(µx) = λµf(x) = µλf(x) = µ(λf)(x).
When E and F have finite dimensions, the vector space Hom(E, F) also has finite di￾mension, as we shall see shortly.
Definition 3.23. When E = F, a linear map f : E → E is also called an endomorphism.
The space Hom(E, E) is also denoted by End(E).
It is also important to note that composition confers to Hom(E, E) a ring structure.
Indeed, composition is an operation ◦: Hom(E, E) × Hom(E, E) → Hom(E, E), which is
associative and has an identity idE, and the distributivity properties hold:
(g1 + g2) ◦ f = g1 ◦ f + g2 ◦ f;
g ◦ (f1 + f2) = g ◦ f1 + g ◦ f2.
The ring Hom(E, E) is an example of a noncommutative ring.
It is easily seen that the set of bijective linear maps f : E → E is a group under compo￾sition.
Definition 3.24. Bijective linear maps f : E → E are also called automorphisms. The
group of automorphisms of E is called the general linear group (of E), and it is denoted by
GL(E), or by Aut(E), or when E = R
n
, by GL(n, R), or even by GL(n).
Although in this book, we will not have many occasions to use quotient spaces, they are
fundamental in algebra. The next section may be omitted until needed.
100 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS
3.8 Quotient Spaces
Let E be a vector space, and let M be any subspace of E. The subspace M induces a relation
≡M on E, defined as follows: For all u, v ∈ E,
u ≡M v iff u − v ∈ M.
We have the following simple proposition.
Proposition 3.22. Given any vector space E and any subspace M of E, the relation ≡M
is an equivalence relation with the following two congruential properties:
1. If u1 ≡M v1 and u2 ≡M v2, then u1 + u2 ≡M v1 + v2, and
2. if u ≡M v, then λu ≡M λv.
Proof. It is obvious that ≡M is an equivalence relation. Note that u1 ≡M v1 and u2 ≡M v2
are equivalent to u1 − v1 = w1 and u2 − v2 = w2, with w1, w2 ∈ M, and thus,
(u1 + u2) − (v1 + v2) = w1 + w2,
and w1 + w2 ∈ M, since M is a subspace of E. Thus, we have u1 + u2 ≡M v1 + v2. If
u − v = w, with w ∈ M, then
λu − λv = λw,
and λw ∈ M, since M is a subspace of E, and thus λu ≡M λv.
Proposition 3.22 shows that we can define addition and multiplication by a scalar on the
set E/M of equivalence classes of the equivalence relation ≡M.
Definition 3.25. Given any vector space E and any subspace M of E, we define the following
operations of addition and multiplication by a scalar on the set E/M of equivalence classes
of the equivalence relation ≡M as follows: for any two equivalence classes [u], [v] ∈ E/M, we
have
[u] + [v] = [u + v],
λ[u] = [λu].
By Proposition 3.22, the above operations do not depend on the specific choice of represen￾tatives in the equivalence classes [u], [v] ∈ E/M. It is also immediate to verify that E/M is
a vector space. The function π : E → E/F, defined such that π(u) = [u] for every u ∈ E, is
a surjective linear map called the natural projection of E onto E/F. The vector space E/M
is called the quotient space of E by the subspace M.
Given any linear map f : E → F, we know that Ker f is a subspace of E, and it is
immediately verified that Im f is isomorphic to the quotient space E/Ker f.
3.9. LINEAR FORMS AND THE DUAL SPACE 101
3.9 Linear Forms and the Dual Space
We already observed that the field K itself (K = R or K = C) is a vector space (over itself).
The vector space Hom(E, K) of linear maps from E to the field K, the linear forms, plays
a particular role. In this section, we only define linear forms and show that every finite￾dimensional vector space has a dual basis. A more advanced presentation of dual spaces and
duality is given in Chapter 11.
Definition 3.26. Given a vector space E, the vector space Hom(E, K) of linear maps from
E to the field K is called the dual space (or dual) of E. The space Hom(E, K) is also denoted
by E
∗
, and the linear maps in E
∗ are called the linear forms, or covectors. The dual space
E
∗∗ of the space E
∗
is called the bidual of E.
As a matter of notation, linear forms f : E → K will also be denoted by starred symbol,
such as u
∗
, x
∗
, etc.
If E is a vector space of finite dimension n and (u1, . . . , un) is a basis of E, for any linear
form f
