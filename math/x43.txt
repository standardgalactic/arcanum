e
θA = [cos θ,sin θ u].
Proposition 16.8. The exponential map exp: su(2) → SU(2) is surjective
Proof. We give an algorithm to find the logarithm A ∈ su(2) of a unit quaternion
q =

−
α β
β α

with α = a + bi and β = c + id.
If q = I (i.e. a = 1), then A = 0. If q = −I (i.e. a = −1), then
A = ±π

0
i
−
0
i

.
16.5. THE EXPONENTIAL MAP exp: su(2) → SU(2) 601
Otherwise, a 6 = ±1 and (b, c, d) 6 = (0, 0, 0), and we are seeking some A = θB ∈ su(2) with
det(B) = 1 and 0 < θ < π, such that, by Proposition 16.7,
q = e
θB = cos θI + sin θB.
Let
B =

iu1 u2 + iu3
−u2 + iu3 −iu1

,
with u = (u1, u2, u3) a unit vector. We must have
a = cos θ, eθB − (e
θB)
∗ = q − q
∗
.
Since 0 < θ < π, we have sin θ 6 = 0, and
2 sin θ

iu1 u2 + iu3
−u2 + iu3 −iu1

=

α
−
−
2β
α
α
2
−
β
α

.
Thus, we get
u1 =
1
sin θ
b, u2 + iu3 =
1
sin θ
(c + id);
that is,
cos θ = a (0 < θ < π)
(u1, u2, u3) = 1
sin θ
(b, c, d).
Since a
2+b
2+c
2+d
2 = 1 and a = cos θ, the vector (b, c, d)/ sin θ is a unit vector. Furthermore
if the quaternion q is of the form q = [cos θ,sin θu] where u = (u1, u2, u3) is a unit vector
(with 0 < θ < π), then
A = θ

iu1 u2 + iu3
−u2 + iu3 −iu1

(∗log)
is a logarithm of q.
Observe that not only is the exponential map exp: su(2) → SU(2) surjective, but the
above proof shows that it is injective on the open ball
{θB ∈ su(2) | det(B) = 1, 0 ≤ θ < π}.
Also, unlike the situation where in computing the logarithm of a rotation matrix R ∈
SO(3) we needed to treat the case where tr(R) = −1 (the angle of the rotation is π) in a
special way, computing the logarithm of a quaternion (other than ±I) does not require any
case analysis; no special case is needed when the angle of rotation is π.
602 CHAPTER 16. UNIT QUATERNIONS AND ROTATIONS IN SO(3)
16.6 Quaternion Interpolation ~
We are now going to derive a formula for interpolating between two quaternions. This
formula is due to Ken Shoemake, once a Penn student and my TA! Since rotations in SO(3)
can be defined by quaternions, this has applications to computer graphics, robotics, and
computer vision.
First we observe that multiplication of quaternions can be expressed in terms of the
inner product and the cross-product in R
3
. Indeed, if q1 = [a, u1] and q2 = [a2, u2], it can be
verified that
q1q2 = [a1, u1][a2, u2] = [a1a2 − u1 · u2, a1u2 + a2u1 + u1 × u2]. (∗mult)
We will also need the identity
u × (u × v) = (u · v)u − (u · u)v.
Given a quaternion q expressed as q = [cos θ,sin θ u], where u is a unit vector, we can
interpolate between I and q by finding the logs of I and q, interpolating in su(2), and then
exponentiating. We have
A = log(I) =  0 0
0 0 , B = log(q) = θ

iu1 u2 + iu3
−u2 + iu3 −iu1

,
and so q = e
B. Since SU(2) is a compact Lie group and since the inner product on su(2)
given by
h
X, Y i = tr(X
> Y )
is Ad(SU(2))-invariant, it induces a biinvariant Riemannian metric on SU(2), and the curve
λ 7→ e
λB, λ ∈ [0, 1]
is a geodesic from I to q in SU(2). We write q
λ = e
λB. Given two quaternions q1 and q2,
because the metric is left invariant, the curve
λ 7→ Z(λ) = q1(q1
−1
q2)
λ
, λ ∈ [0, 1]
is a geodesic from q1 to q2. Remarkably, there is a closed-form formula for the interpolant
Z(λ).
Say q1 = [cos θ,sin θ u] and q2 = [cos ϕ,sin ϕ v], and assume that q1 6 = q2 and q1 6 = −q2.
First, we compute q
−1
q2. Since q
−1 = [cos θ, − sin θ u], we have
q
−1
q2 = [cos θ cos ϕ + sin θ sin ϕ(u · v), − sin θ cos ϕ u + cos θ sin ϕ v − sin θ sin ϕ(u × v)].
Define Ω by
cos Ω = cos θ cos ϕ + sin θ sin ϕ(u · v). (∗Ω)
16.6. QUATERNION INTERPOLATION ~ 603
Since q1 6 = q2 and q1 6 = −q2, we have 0 < Ω < π, so we get
q
−1
1
q2 =
 cos Ω, sin Ω (− sin θ cos ϕ u + cos θ
sin Ω
sin ϕ v − sin θ sin ϕ(u × v)

,
where the term multiplying sin Ω is a unit vector because q1 and q2 are unit quaternions, so
q
−1
1
q2 is also a unit quaternion. By (∗log), we have
(q1
−1
q2)
λ =
 cos λΩ, sin λΩ
(− sin θ cos ϕ u + cos θ
sin Ω
sin ϕ v − sin θ sin ϕ(u × v)

.
Next we need to compute q1(q1
−1
q2)
λ
. The scalar part of this product is
s = cos θ cos λΩ + sin λΩ
sin Ω sin2
θ cos ϕ(u · u) −
sin λΩ
sin Ω sin θ sin ϕ cos θ(u · v)
+
sin λΩ
sin Ω sin2
θ sin ϕ(u · (u × v)).
Since u · (u × v) = 0, the last term is zero, and since u · u = 1 and
sin θ sin ϕ(u · v) = cos Ω − cos θ cos ϕ,
we get
s = cos θ cos λΩ + sin λΩ
sin Ω sin2
θ cos ϕ −
sin λΩ
sin Ω cos θ(cos Ω − cos θ cos ϕ)
= cos θ cos λΩ + sin λΩ
sin Ω (sin2
θ + cos2
θ) cos ϕ −
sin λΩ
sin Ω cos θ cos Ω
=
(cos λΩ sin Ω − sin λΩ cos Ω) cos θ
sin Ω +
sin λΩ
sin Ω cos ϕ
=
sin(1 − λ)Ω
sin Ω cos θ +
sin λΩ
sin Ω cos ϕ.
The vector part of the product q1(q1
−1
q2)
λ
is given by
ν = −
sin λΩ
sin Ω cos θ sin θ cos ϕ u +
sin λΩ
sin Ω cos2
θ sin ϕ v −
sin λΩ
sin Ω cos θ sin θ sin ϕ(u × v)
+ cos λΩ sin θ u −
sin λΩ
sin Ω sin2
θ cos ϕ(u × u) + sin λΩ
sin Ω cos θ sin θ sin ϕ(u × v)
−
sin λΩ
sin Ω sin2
θ sin ϕ(u × (u × v)).
We have u × u = 0, the two terms involving u × v cancel out,
u × (u × v) = (u · v)u − (u · u)v,
604 CHAPTER 16. UNIT QUATERNIONS AND ROTATIONS IN SO(3)
and u · u = 1, so we get
ν = −
sin λΩ
sin Ω cos θ sin θ cos ϕ u + cos λΩ sin θ u +
sin λΩ
sin Ω cos2
θ sin ϕ v
+
sin λΩ
sin Ω sin2
θ sin ϕ v −
sin λΩ
sin Ω sin2
θ sin ϕ(u · v)u.
Using
sin θ sin ϕ(u · v) = cos Ω − cos θ cos ϕ,
we get
ν = −
sin λΩ
sin Ω cos θ sin θ cos ϕ u + cos λΩ sin θ u +
sin λΩ
sin Ω sin ϕ v
−
sin λΩ
sin Ω sin θ(cos Ω − cos θ cos ϕ)u
= cos λΩ sin θ u +
sin λΩ
sin Ω sin ϕ v −
sin λΩ
sin Ω sin θ cos Ω u
=
(cos λΩ sin Ω − sin λΩ cos Ω)
sin Ω sin θ u +
sin λΩ
sin Ω sin ϕ v
=
sin(1 − λ)Ω
sin Ω sin θ u +
sin λΩ
sin Ω sin ϕ v.
Putting the scalar part and the vector part together, we obtain
q1(q1
−1
q2)
λ =

sin(1
sin Ω
− λ)Ω
cos θ +
sin
sin Ω
λΩ
cos ϕ,
sin(1
sin Ω
− λ)Ω
sin θ u +
sin
sin Ω
λΩ
sin ϕ v ,
=
sin(1 − λ)Ω
sin Ω [cos θ, sin θ u] + sin λΩ
sin Ω [cos ϕ, sin ϕ v].
This yields the celebrated slerp interpolation formula
Z(λ) = q1(q1
−1
q2)
λ =
sin(1 − λ)Ω
sin Ω q1 +
sin λΩ
sin Ω q2,
with
cos Ω = cos θ cos ϕ + sin θ sin ϕ(u · v).
16.7 Nonexistence of a “Nice” Section from SO(3) to
SU(2)
We conclude by discussing the problem of a consistent choice of sign for the quaternion q
representing a rotation R = ρq ∈ SO(3). We are looking for a “nice” section s: SO(3) →
SU(2), that is, a function s satisfying the condition
ρ ◦ s = id,
where ρ is the surjective homomorphism ρ: SU(2) → SO(3).
16.7. NONEXISTENCE OF A “NICE” SECTION FROM SO(3) TO SU(2) 605
Proposition 16.9. Any section s: SO(3) → SU(2) of ρ is neither a homomorphism nor
continuous.
Intuitively, this means that there is no “nice and simple ” way to pick the sign of the
quaternion representing a rotation.
The following proof is due to Marcel Berger.
Proof. Let Γ be the subgroup of SU(2) consisting of all quaternions of the form q =
[a,(b, 0, 0)]. Then, using the formula for the rotation matrix Rq corresponding to q (and
the fact that a
2 + b
2 = 1), we get
Rq =


1 0 0
0 2
0 2
a
2
ab
− 1
2
−
a
2
2
−
ab
1

 .
Since a
2 + b
2 = 1, we may write a = cos θ, b = sin θ, and we see that
Rq =


1 0 0
0 cos 2
0 sin 2θ
θ −
cos 2
sin 2
θ
θ

 ,
a rotation of angle 2θ around the x-axis. Thus, both Γ and its image are isomorphic to
SO(2), which is also isomorphic to U(1) = {w ∈ C | |w| = 1}. By identifying i and i, and
identifying Γ and its image to U(1), if we write w = cos θ + isin θ ∈ Γ, the restriction of the
map ρ to Γ is given by ρ(w) = w
2
.
We claim that any section s of ρ is not a homomorphism. Consider the restriction of s
to U(1). Then since ρ ◦ s = id and ρ(w) = w
2
, for −1 ∈ ρ(Γ) ≈ U(1), we have
−1 = ρ(s(−1)) = (s(−1))2
.
On the other hand, if s is a homomorphism, then
(s(−1))2 = s((−1)2
) = s(1) = 1,
contradicting (s(−1))2 = −1.
We also claim that s is not continuous. Assume that s(1) = 1, the case where s(1) = −1
being analogous. Then s is a bijection inverting ρ on Γ whose restriction to U(1) must be
given by
s(cos θ + isin θ) = cos(θ/2) + isin(θ/2), −π ≤ θ < π.
If θ tends to π, that is z = cos θ +isin θ tends to −1 in the upper-half plane, then s(z) tends
to i, but if θ tends to −π, that is z tends to −1 in the lower-half plane, then s(z) tends to
−i, which shows that s is not continuous.
606 CHAPTER 16. UNIT QUATERNIONS AND ROTATIONS IN SO(3)
Another way (due to Jean Dieudonn´e) to prove that a section s of ρ is not a homomor￾phism is to prove that any unit quaternion is the product of two unit pure quaternions.
Indeed, if q = [a, u] is a unit quaternion, if we let q1 = [0, u1], where u1 is any unit vector
orthogonal to u, then
q1q = [−u1 · u, au1 + u1 × u] = [0, au1 + u1 × u] = q2
is a nonzero unit pure quaternion. This is because if a 6 = 0 then au1+u1×u 6 = 0 (since u1×u
is orthogonal to au1 6 = 0), and if a = 0 then u 6 = 0, so u1 × u 6 = 0 (since u1 is orthogonal to
u). But then, q1
−1 = [0, −u1] is a unit pure quaternion and we have
q = q
−1
1
q2,
a product of two pure unit quaternions.
We also observe that for any two pure quaternions q1, q2, there is some unit quaternion
q such that
q2 = qq1q
−1
.
This is just a restatement of the fact that the group SO(3) is transitive. Since the kernel
of ρ: SU(2) → SO(3) is {I, −I}, the subgroup s(SO(3)) would be a normal subgroup of
index 2 in SU(2). Then we would have a surjective homomorphism η from SU(2) onto the
quotient group SU(2)/s(SO(3)), which is isomorphic to {1, −1}. Now, since any two pure
quaternions are conjugate of each other, η would have a constant value on the unit pure
quaternions. Since k = ij, we would have
η(k) = η(ij) = (η(i))2 = 1.
Consequently, η would map all pure unit quaternions to 1. But since every unit quaternion is
the product of two pure quaternions, η would map every unit quaternion to 1, contradicting
the fact that it is surjective onto {−1, 1}.
16.8 Summary
The main concepts and results of this chapter are listed below:
• The group SU(2) of unit quaternions.
• The skew field H of quaternions.
• Hamilton’s identities.
• The (real) vector space su(2) of 2 × 2 skew Hermitian matrices with zero trace.
• The adjoint representation of SU(2).
16.9. PROBLEMS 607
• The (real) vector space su(2) of 2 × 2 Hermitian matrices with zero trace.
• The group homomorphism r : SU(2) → SO(3); Ker (r) = {+I, −I}.
• The matrix representation Rq of the rotation rq induced by a unit quaternion q.
• Surjectivity of the homomorphism r : SU(2) → SO(3).
• The exponential map exp: su(2) → SU(2).
• Surjectivity of the exponential map exp: su(2) → SU(2).
• Finding a logarithm of a quaternion.
• Quaternion interpolation.
• Shoemake’s slerp interpolation formula.
• Sections s: SO(3) → SU(2) of r : SU(2) → SO(3).
16.9 Problems
Problem 16.1. Verify the quaternion identities
i
2 = j
2 = k
2 = ijk = −1,
ij = −ji = k,
jk = −kj = i,
ki = −ik = j.
Problem 16.2. Check that for every quaternion X = a1 + bi + cj + dk, we have
XX∗ = X
∗X = (a
2 + b
2 + c
2 + d
2
)1.
Conclude that if X 6 = 0, then X is invertible and its inverse is given by
X
−1 = (a
2 + b
2 + c
2 + d
2
)
−1X
∗
.
Problem 16.3. Given any two quaternions X = a1+bi+cj+dk and Y = a
0 1+b
0 i+c
0 j+d
0 k,
prove that
XY = (aa0 − bb0 − cc0 − dd0 )1 + (ab0 + ba0 + cd0 − dc0 )i
+ (ac0 + ca0 + db0 − bd0 )j + (ad0 + da0 + bc0 − cb0 )k.
Also prove that if X = [a, U] and Y = [a
0 , U0 ], the quaternion product XY can be
expressed as
XY = [aa0 − U · U
0 , aU0 + a
0 U + U × U
0 ].
608 CHAPTER 16. UNIT QUATERNIONS AND ROTATIONS IN SO(3)
Problem 16.4. Let Ad: SU(2) → GL(su(2)) be the map defined such that for every
q ∈ SU(2),
Adq(A) = qAq∗
, A ∈ su(2),
where q
∗
is the inverse of q (since SU(2) is a unitary group) Prove that the map Adq is an
invertible linear map from su(2) to itself and that Ad is a group homomorphism.
Problem 16.5. Prove that every Hermitian matrix with zero trace is of the form xσ3 +
yσ2 + zσ1, with
σ1 =

0 1
1 0 , σ2 =

0
i
−
0
i

, σ3 =

1 0
0 −1

.
Check that i = iσ3, j = iσ2, and that k = iσ1.
Problem 16.6. If
B =


0 −u3 u2
u3 0 −u1
−u2 u1 0

 ,
and if we write θ =
p u
2
1 + u
2
2 + u
2
3
(with 0 ≤ θ ≤ π), then the Rodrigues formula says that
e
B = I +
sin θ
θ
B +
(1 − cos θ)
θ
2
B
2
, θ 6 = 0,
with e
0 = I. Check that tr(e
B) = 1 + 2 cos θ. Prove that the quaternion q corresponding to
the rotation R = e
B (with B 6 = 0) is given by
q =
 cos
2
θ

, sin
2
θ

 u1
θ
,
u2
θ
,
u3
θ


.
Problem 16.7. For every matrix A ∈ su(2), with
A =

iu1 u2 + iu3
−u2 + iu3 −iu1

,
prove that if we write θ =
p u
2
1 + u
2
2 + u
2
3
, then
e
A = cos θI +
sin θ
θ
A, θ 6 = 0,
and e
0 = I. Conclude that e
A is a unit quaternion representing the rotation of angle 2θ and
axis (u1, u2, u3) (or I when θ = kπ, k ∈ Z).
Problem 16.8. Write a Matlab program implementing the method of Section 16.4 for
finding a unit quaternion corresponding to a rotation matrix.
Problem 16.9. Show that there is a very simple method for producing an orthonormal
frame in R
4 whose first vector is any given nonnull vector (a, b, c, d).
16.9. PROBLEMS 609
Problem 16.10. Let i, j, and k, be the unit vectors of coordinates (1, 0, 0), (0, 1, 0), and
(0, 0, 1) in R
3
.
(1) Describe geometrically the rotations defined by the following quaternions:
p = (0, i), q = (0, j).
Prove that the interpolant Z(λ) = p(p
−1
q)
λ
is given by
Z(λ) = (0, cos(λπ/2)i + sin(λπ/2)j).
Describe geometrically what this rotation is.
(2) Repeat Question (1) with the rotations defined by the quaternions
p =
 
2
1
,
√
2
3
i
! , q = (0, j).
Prove that the interpolant Z(λ) is given by
Z(λ) = 1
2
cos(λπ/2),
√
2
3
cos(λπ/2)i + sin(λπ/2)j
! .
Describe geometrically what this rotation is.
(3) Repeat Question (1) with the rotations defined by the quaternions
p =
 √
1
2
, √
1
2
i
 , q =
 0, √
1
2
(i + j)
 .
Prove that the interpolant Z(λ) is given by
Z(λ) =  √
1
2
cos(λπ/3) − √
1
6
sin(λπ/3),
(1/
√
2 cos(λπ/3) + 1/
√
6 sin(λπ/3))i + √
2
6
sin(λπ/3)j
 .
Problem 16.11. Prove that
w × (u × v) = (w · v)u − (u · w)v.
Conclude that
u × (u × v) = (u · v)u − (u · u)v.
610 CHAPTER 16. UNIT QUATERNIONS AND ROTATIONS IN SO(3)
Chapter 17
Spectral Theorems in Euclidean and
Hermitian Spaces
17.1 Introduction
The goal of this chapter is to show that there are nice normal forms for symmetric matrices,
skew-symmetric matrices, orthogonal matrices, and normal matrices. The spectral theorem
for symmetric matrices states that symmetric matrices have real eigenvalues and that they
can be diagonalized over an orthonormal basis. The spectral theorem for Hermitian matrices
states that Hermitian matrices also have real eigenvalues and that they can be diagonalized
over a complex orthonormal basis. Normal real matrices can be block diagonalized over an
orthonormal basis with blocks having size at most two and there are refinements of this
normal form for skew-symmetric and orthogonal matrices.
The spectral result for real symmetric matrices can be used to prove two characterizations
of the eigenvalues of a symmetric matrix in terms of the Rayleigh ratio. The first charac￾terization is the Rayleigh–Ritz theorem and the second one is the Courant–Fischer theorem.
Both results are used in optimization theory and to obtain results about perturbing the
eigenvalues of a symmetric matrix.
In this chapter all vector spaces are finite-dimensional real or complex vector spaces.
17.2 Normal Linear Maps: Eigenvalues and Eigenvec￾tors
We begin by studying normal maps, to understand the structure of their eigenvalues and
eigenvectors. This section and the next three were inspired by Lang [109], Artin [7], Mac
Lane and Birkhoff [118], Berger [11], and Bertin [15].
611
612 CHAPTER 17. SPECTRAL THEOREMS
Definition 17.1. Given a Euclidean or Hermitian space E, a linear map f : E → E is
normal if
f ◦ f
∗ = f
∗
◦ f.
A linear map f : E → E is self-adjoint if f = f
∗
, skew-self-adjoint if f = −f
∗
, and orthogonal
if f ◦ f
∗ = f
∗ ◦ f = id.
Obviously, a self-adjoint, skew-self-adjoint, or orthogonal linear map is a normal linear
map. Our first goal is to show that for every normal linear map f : E → E, there is an
orthonormal basis (w.r.t. h−, −i) such that the matrix of f over this basis has an especially
nice form: it is a block diagonal matrix in which the blocks are either one-dimensional
matrices (i.e., single entries) or two-dimensional matrices of the form

−
λ µ
µ λ .
This normal form can be further refined if f is self-adjoint, skew-self-adjoint, or orthog￾onal. As a first step we show that f and f
∗ have the same kernel when f is normal.
Proposition 17.1. Given a Euclidean space E, if f : E → E is a normal linear map, then
Ker f = Ker f
∗
.
Proof. First let us prove that
h
f(u), f(v)i = h f
∗
(u), f ∗
(v)i
for all u, v ∈ E. Since f
∗
is the adjoint of f and f ◦ f
∗ = f
∗ ◦ f, we have
h
f(u), f(u)i = h u,(f
∗
◦ f)(u)i ,
= h u,(f ◦ f
∗
)(u)i ,
= h f
∗
(u), f ∗
(u)i .
Since h−, −i is positive definite,
h
f(u), f(u)i = 0 iff f(u) = 0,
h
f
∗
(u), f ∗
(u)i = 0 iff f
∗
(u) = 0,
and since
h
f(u), f(u)i = h f
∗
(u), f ∗
(u)i ,
we have
f(u) = 0 iff f
∗
(u) = 0.
Consequently, Ker f = Ker f
∗
.
17.2. NORMAL LINEAR MAPS: EIGENVALUES AND EIGENVECTORS 613
Assuming again that E is a Hermitian space, observe that Proposition 17.1 also holds.
We deduce the following corollary.
Proposition 17.2. Given a Hermitian space E, for any normal linear map f : E → E, we
have Ker (f) ∩ Im(f) = (0).
Proof. Assume v ∈ Ker (f) ∩ Im(f), which means that v = f(u) for some u ∈ E, and
f(v) = 0. By Proposition 17.1, Ker (f) = Ker (f
∗
), so f(v) = 0 implies that f
∗
(v) = 0.
Consequently,
0 = h f
∗
(v), ui
= h v, f(u)i
= h v, vi ,
and thus, v = 0.
We also have the following crucial proposition relating the eigenvalues of f and f
∗
.
Proposition 17.3. Given a Hermitian space E, for any normal linear map f : E → E, a
vector u is an eigenvector of f for the eigenvalue λ (in C) iff u is an eigenvector of f
∗
for
the eigenvalue λ.
Proof. First it is immediately verified that the adjoint of f − λ id is f
∗ − λ id. Furthermore,
f − λ id is normal. Indeed,
(f − λ id) ◦ (f − λ id)∗ = (f − λ id) ◦ (f
∗ − λ id),
= f ◦ f
∗ − λf − λf ∗ + λλ id,
= f
∗
◦ f − λf ∗ − λf + λλ id,
= (f
∗ − λ id) ◦ (f − λ id),
= (f − λ id)∗
◦ (f − λ id).
Applying Proposition 17.1 to f − λ id, for every nonnull vector u, we see that
(f − λ id)(u) = 0 iff (f
∗ − λ id)(u) = 0,
which is exactly the statement of the proposition.
The next proposition shows a very important property of normal linear maps: eigenvec￾tors corresponding to distinct eigenvalues are orthogonal.
Proposition 17.4. Given a Hermitian space E, for any normal linear map f : E → E, if
u and v are eigenvectors of f associated with the eigenvalues λ and µ (in C) where λ 6 = µ,
then h u, vi = 0.
614 CHAPTER 17. SPECTRAL THEOREMS
Proof. Let us compute h f(u), vi in two different ways. Since v is an eigenvector of f for µ,
by Proposition 17.3, v is also an eigenvector of f
∗
for µ, and we have
h
f(u), vi = h λu, vi = λh u, vi ,
and
h
f(u), vi = h u, f ∗
(v)i = h u, µvi = µh u, vi ,
where the last identity holds because of the semilinearity in the second argument. Thus
λh u, vi = µh u, vi ,
that is,
(λ − µ)h u, vi = 0,
which implies that h u, vi = 0, since λ 6 = µ.
We can show easily that the eigenvalues of a self-adjoint linear map are real.
Proposition 17.5. Given a Hermitian space E, all the eigenvalues of any self-adjoint linear
map f : E → E are real.
Proof. Let z (in C) be an eigenvalue of f and let u be an eigenvector for z. We compute
h
f(u), ui in two different ways. We have
h
f(u), ui = h zu, ui = zh u, ui ,
and since f = f
∗
, we also have
h
f(u), ui = h u, f ∗
(u)i = h u, f(u)i = h u, zui = zh u, ui .
Thus,
zh u, ui = zh u, ui ,
which implies that z = z, since u 6 = 0, and z is indeed real.
There is also a version of Proposition 17.5 for a (real) Euclidean space E and a self-adjoint
map f : E → E since every real vector space E can be embedded into a complex vector space
EC, and every linear map f : E → E can be extended to a linear map fC : EC → EC.
Definition 17.2. Given a real vector space E, let EC be the structure E × E under the
addition operation
(u1, u2) + (v1, v2) = (u1 + v1, u2 + v2),
and let multiplication by a complex scalar z = x + iy be defined such that
(x + iy) · (u, v) = (xu − yv, yu + xv).
The space EC is called the complexification of E.
17.2. NORMAL LINEAR MAPS: EIGENVALUES AND EIGENVECTORS 615
It is easily shown that the structure EC is a complex vector space. It is also immediate
that
(0, v) = i(v, 0),
and thus, identifying E with the subspace of EC consisting of all vectors of the form (u, 0),
we can write
(u, v) = u + iv.
Observe that if (e1, . . . , en) is a basis of E (a real vector space), then (e1, . . . , en) is also
a basis of EC (recall that ei
is an abbreviation for (ei
, 0)).
A linear map f : E → E is extended to the linear map fC : EC → EC defined such that
fC(u + iv) = f(u) + if(v).
For any basis (e1, . . . , en) of E, the matrix M(f) representing f over (e1, . . . , en) is iden￾tical to the matrix M(fC) representing fC over (e1, . . . , en), where we view (e1, . . . , en) as a
basis of EC. As a consequence, det(zI − M(f)) = det(zI − M(fC)), which means that f
and fC have the same characteristic polynomial (which has real coefficients). We know that
every polynomial of degree n with real (or complex) coefficients always has n complex roots
(counted with their multiplicity), and the roots of det(zI − M(fC)) that are real (if any) are
the eigenvalues of f.
Next we need to extend the inner product on E to an inner product on EC.
The inner product h−, −i on a Euclidean space E is extended to the Hermitian positive
definite form h−, −iC on EC as follows:
h
u1 + iv1, u2 + iv2i C = h u1, u2i + h v1, v2i + i(h v1, u2i − hu1, v2i ).
It is easily verified that h−, −iC is indeed a Hermitian form that is positive definite,
and it is clear that h−, −iC agrees with h−, −i on real vectors. Then given any linear map
f : E → E, it is easily verified that the map fC
∗ defined such that
fC
∗
(u + iv) = f
∗
(u) + if ∗
(v)
for all u, v ∈ E is the adjoint of fC w.r.t. h−, −iC.
Proposition 17.6. Given a Euclidean space E, if f : E → E is any self-adjoint linear map,
then every eigenvalue λ of fC is real and is actually an eigenvalue of f (which means that
there is some real eigenvector u ∈ E such that f(u) = λu). Therefore, all the eigenvalues of
f are real.
616 CHAPTER 17. SPECTRAL THEOREMS
Proof. Let EC be the complexification of E, h−, −iC the complexification of the inner product
h−, −i on E, and fC : EC → EC the complexification of f : E → E. By definition of fC and
h−, −iC, if f is self-adjoint, we have
h
fC(u1 + iv1), u2 + iv2i C = h f(u1) + if(v1), u2 + iv2i C
= h f(u1), u2i + h f(v1), v2i + i(h u2, f(v1)i − hf(u1), v2i )
= h u1, f(u2)i + h v1, f(v2)i + i(h f(u2), v1i − hu1, f(v2)i )
= h u1 + iv1, f(u2) + if(v2)i C
= h u1 + iv1, fC(u2 + iv2)i C,
which shows that fC is also self-adjoint with respect to h−, −iC.
As we pointed out earlier, f and fC have the same characteristic polynomial det(zI−fC) =
det(zI − f), which is a polynomial with real coefficients. Proposition 17.5 shows that the
zeros of det(zI − fC) = det(zI − f) are all real, and for each real zero λ of det(zI − f), the
linear map λid − f is singular, which means that there is some nonzero u ∈ E such that
f(u) = λu. Therefore, all the eigenvalues of f are real.
Proposition 17.7. Given a Hermitian space E, for any linear map f : E → E, if f is skew￾self-adjoint, then f has eigenvalues that are pure imaginary or zero, and if f is unitary, then
f has eigenvalues of absolute value 1.
Proof. If f is skew-self-adjoint, f
∗ = −f, and then by the definition of the adjoint map, for
any eigenvalue λ and any eigenvector u associated with λ, we have
λh u, ui = h λu, ui = h f(u), ui = h u, f ∗
(u)i = h u, −f(u)i = −hu, λui = −λh u, ui ,
and since u 6 = 0 and h−, −i is positive definite, h u, ui 6 = 0, so
λ = −λ,
which shows that λ = ir for some r ∈ R.
If f is unitary, then f is an isometry, so for any eigenvalue λ and any eigenvector u
associated with λ, we have
|λ|
2
h u, ui = λλh u, ui = h λu, λui = h f(u), f(u)i = h u, ui ,
and since u 6 = 0, we obtain |λ|
2 = 1, which implies
|λ| = 1.
17.3. SPECTRAL THEOREM FOR NORMAL LINEAR MAPS 617
17.3 Spectral Theorem for Normal Linear Maps
Given a Euclidean space E, our next step is to show that for every linear map f : E → E
there is some subspace W of dimension 1 or 2 such that f(W) ⊆ W. When dim(W) = 1, the
subspace W is actually an eigenspace for some real eigenvalue of f. Furthermore, when f is
normal, there is a subspace W of dimension 1 or 2 such that f(W) ⊆ W and f
∗
(W) ⊆ W.
The difficulty is that the eigenvalues of f are not necessarily real. One way to get around
this problem is to complexify both the vector space E and the inner product h−, −i as we
did in Section 17.2.
Given any subspace W of a Euclidean space E, recall that the orthogonal complement
W⊥ of W is the subspace defined such that
W⊥ = {u ∈ E | hu, wi = 0, for all w ∈ W}.
Recall from Proposition 12.11 that E = W ⊕ W⊥ (this can be easily shown, for example,
by constructing an orthonormal basis of E using the Gram–Schmidt orthonormalization
procedure). The same result also holds for Hermitian spaces; see Proposition 14.13.
As a warm up for the proof of Theorem 17.12, let us prove that every self-adjoint map on
a Euclidean space can be diagonalized with respect to an orthonormal basis of eigenvectors.
Theorem 17.8. (Spectral theorem for self-adjoint linear maps on a Euclidean space) Given
a Euclidean space E of dimension n, for every self-adjoint linear map f : E → E, there is
an orthonormal basis (e1, . . . , en) of eigenvectors of f such that the matrix of f w.r.t. this
basis is a diagonal matrix


λ1
λ2
. . .
. . .
.
.
.
.
.
.
.
.
.
.
.
. . . λ
.
n


,
with λi ∈ R.
Proof. We proceed by induction on the dimension n of E as follows. If n = 1, the result is
trivial. Assume now that n ≥ 2. From Proposition 17.6, all the eigenvalues of f are real, so
pick some eigenvalue λ ∈ R, and let w be some eigenvector for λ. By dividing w by its norm,
we may assume that w is a unit vector. Let W be the subspace of dimension 1 spanned by w.
Clearly, f(W) ⊆ W. We claim that f(W⊥) ⊆ W⊥, where W⊥ is the orthogonal complement
of W.
Indeed, for any v ∈ W⊥, that is, if h v, wi = 0, because f is self-adjoint and f(w) = λw,
we have
h
f(v), wi = h v, f(w)i
= h v, λwi
= λh v, wi = 0
618 CHAPTER 17. SPECTRAL THEOREMS
since h v, wi = 0. Therefore,
f(W⊥) ⊆ W⊥.
Clearly, the restriction of f to W⊥ is self-adjoint, and we conclude by applying the induction
hypothesis to W⊥ (whose dimension is n − 1).
We now come back to normal linear maps. One of the key points in the proof of Theorem
17.8 is that we found a subspace W with the property that f(W) ⊆ W implies that f(W⊥) ⊆
W⊥. In general, this does not happen, but normal maps satisfy a stronger property which
ensures that such a subspace exists.
The following proposition provides a condition that will allow us to show that a nor￾mal linear map can be diagonalized. It actually holds for any linear map. We found the
inspiration for this proposition in Berger [11].
Proposition 17.9. Given a Hermitian space E, for any linear map f : E → E and any
subspace W of E, if f(W) ⊆ W, then f
∗
￾ W⊥
 ⊆ W⊥. Consequently, if f(W) ⊆ W and
