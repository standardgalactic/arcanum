2
v
> Av âˆ’ v
> b + Î»
> (Cv âˆ’ d),
where Î» is viewed as a column vector. Now because A is a symmetric matrix, it is easy to
show that
âˆ‡L(v, Î») =  Av âˆ’
Cv
b
âˆ’
+
d
C
> Î»

.
Therefore, the necessary condition for constrained local extrema is
Av + C
> Î» = b
Cv = d,
which can be expressed in matrix form as

A C>
C 0
  Î»
v

=

d
b

,
where the matrix of the system is a symmetric matrix. We should not be surprised to find
the system discussed later in Chapter 42, except for some renaming of the matrices and
vectors involved. As we will show in Section 42.2, the function J has a minimum iff A is
positive definite, so in general, if A is only a symmetric matrix, the critical points of the
Lagrangian do not correspond to extrema of J.
40.1. LOCAL EXTREMA AND LAGRANGE MULTIPLIERS 1467
Remark: If the Jacobian matrix Jac(Ï•)(v) = ï¿¾ (âˆ‚Ï•i/âˆ‚xj )(v)
 has rank m for all v âˆˆ U
(which is equivalent to the linear independence of the linear forms dÏ•i(v)), then we say that
0 âˆˆ R
m is a regular value of Ï•. In this case, it is known that
U = {v âˆˆ â„¦ | Ï•(v) = 0}
is a smooth submanifold of dimension n âˆ’ m of R
n
. Furthermore, the set
TvU = {w âˆˆ R
n
| dÏ•i(v)(w) = 0, 1 â‰¤ i â‰¤ m} =
m\
i=1
Ker dÏ•i(v)
is the tangent space to U at v (a vector space of dimension n âˆ’ m). Then, the condition
dJ(v) + Âµ1dÏ•1(v) + Â· Â· Â· + ÂµmdÏ•m(v) = 0
implies that dJ(v) vanishes on the tangent space TvU. Conversely, if dJ(v)(w) = 0 for all
w âˆˆ TvU, this means that dJ(v) is orthogonal (in the sense of Definition 11.3) to TvU.
Since (by Theorem 11.4 (b)) the orthogonal of TvU is the space of linear forms spanned
by dÏ•1(v), . . . , dÏ•m(v), it follows that dJ(v) must be a linear combination of the dÏ•i(v).
Therefore, when 0 is a regular value of Ï•, Theorem 40.2 asserts that if u âˆˆ U is a local
extremum of J, then dJ(u) must vanish on the tangent space TuU. We can say even more.
The subset Z(J) of â„¦ given by
Z(J) = {v âˆˆ â„¦ | J(v) = J(u)}
(the level set of level J(u)) is a hypersurface in â„¦, and if dJ(u) 6 = 0, the zero locus of dJ(u)
is the tangent space TuZ(J) to Z(J) at u (a vector space of dimension n âˆ’ 1), where
TuZ(J) = {w âˆˆ R
n
| dJ(u)(w) = 0}.
Consequently, Theorem 40.2 asserts that
TuU âŠ† TuZ(J);
this is a geometric condition.
We now return to the general situation where E1 and E2 may be infinite-dimensional
normed vector spaces (with E1 a Banach space) and we state and prove the following general
result about the method of Lagrange multipliers.
Theorem 40.4. (Necessary condition for a constrained extremum) Let â„¦ âŠ† E1 Ã— E2 be an
open subset of a product of normed vector spaces, with E1 a Banach space (E1 is complete),
let Ï•: â„¦ â†’ E2 be a C
1
-function (which means that dÏ•(Ï‰) exists and is continuous for all
Ï‰ âˆˆ â„¦), and let
U = {(u1, u2) âˆˆ â„¦ | Ï•(u1, u2) = 0}.
1468 CHAPTER 40. EXTREMA OF REAL-VALUED FUNCTIONS
Moreover, let u = (u1, u2) âˆˆ U be a point such that
âˆ‚Ï•
âˆ‚x2
(u1, u2) âˆˆ L(E2; E2) and  âˆ‚x
âˆ‚Ï•
2
(u1, u2)

âˆ’1
âˆˆ L(E2; E2),
and let J : â„¦ â†’ R be a function which is differentiable at u. If J has a constrained local
extremum at u, then there is a continuous linear form Î›(u) âˆˆ L(E2; R) such that
dJ(u) + Î›(u) â—¦ dÏ•(u) = 0.
Proof. The plan of attack is to use the implicit function theorem; Theorem 39.14. Observe
that the assumptions of Theorem 39.14 are indeed met. Therefore, there exist some open
subsets U1 âŠ† E1, U2 âŠ† E2, and a continuous function g : U1 â†’ U2 with (u1, u2) âˆˆ U1Ã—U2 âŠ† â„¦
and such that
Ï•(v1, g(v1)) = 0
for all v1 âˆˆ U1. Moreover, g is differentiable at u1 âˆˆ U1 and
dg(u1) = âˆ’

âˆ‚x
âˆ‚Ï•
2
(u)

âˆ’1
â—¦
âˆ‚Ï•
âˆ‚x1
(u).
It follows that the restriction of J to (U1 Ã— U2) âˆ© U yields a function G of a single variable,
with
G(v1) = J(v1, g(v1))
for all v1 âˆˆ U1. Now the function G is differentiable at u1 and it has a local extremum at u1
on U1, so Proposition 40.1 implies that
dG(u1) = 0.
By the chain rule,
dG(u1) = âˆ‚J
âˆ‚x1
(u) + âˆ‚J
âˆ‚x2
(u) â—¦ dg(u1)
=
âˆ‚J
âˆ‚x1
(u) âˆ’
âˆ‚J
âˆ‚x2
(u) â—¦

âˆ‚x
âˆ‚Ï•
2
(u)

âˆ’1
â—¦
âˆ‚Ï•
âˆ‚x1
(u).
From dG(u1) = 0, we deduce
âˆ‚J
âˆ‚x1
(u) = âˆ‚J
âˆ‚x2
(u) â—¦

âˆ‚x
âˆ‚Ï•
2
(u)

âˆ’1
â—¦
âˆ‚Ï•
âˆ‚x1
(u),
and since we also have
âˆ‚J
âˆ‚x2
(u) = âˆ‚J
âˆ‚x2
(u) â—¦

âˆ‚x
âˆ‚Ï•
2
(u)

âˆ’1
â—¦
âˆ‚Ï•
âˆ‚x2
(u),
40.1. LOCAL EXTREMA AND LAGRANGE MULTIPLIERS 1469
if we let
Î›(u) = âˆ’
âˆ‚J
âˆ‚x2
(u) â—¦

âˆ‚x
âˆ‚Ï•
2
(u)

âˆ’1
,
then we get
dJ(u) = âˆ‚J
âˆ‚x1
(u) + âˆ‚J
âˆ‚x2
(u)
=
âˆ‚J
âˆ‚x2
(u) â—¦

âˆ‚x
âˆ‚Ï•
2
(u)

âˆ’1
â—¦

âˆ‚x
âˆ‚Ï•
1
(u) + âˆ‚Ï•
âˆ‚x2
(u)

= âˆ’Î›(u) â—¦ dÏ•(u),
which yields dJ(u) + Î›(u) â—¦ dÏ•(u) = 0, as claimed.
Finally, we prove Theorem 40.2.
Proof of Theorem 40.2. The linear independence of the m linear forms dÏ•i(u) is equivalent
to the fact that the m Ã— n matrix A =
ï¿¾ (âˆ‚Ï•i/âˆ‚xj )(u)
 has rank m. By reordering the
columns, we may assume that the first m columns are linearly independent. To conform to
the set-up of Theorem 40.4 we define E1 and E2 as
E1 =

nX
i=m+1
viei
| (vm+1, . . . , vn) âˆˆ R
nâˆ’m
 , E2 =

mX
i=1
viei
| (v1, . . . , vm) âˆˆ R
m
 .
If we let Ïˆ: â„¦ â†’ R
m be the function defined by
Ïˆ(vm+1, . . . , vn, v1, . . . , vm) = (Ï•1(v), . . . , Ï•m(v))
for all (vm+1, . . . , vn, v1, . . . , vm) âˆˆ â„¦, with v = (v1, . . . , vn), then we see that âˆ‚Ïˆ/âˆ‚x2(u) is
invertible and both âˆ‚Ïˆ/âˆ‚x2(u) and its inverse are continuous, so that Theorem 40.4 applies,
and there is some (continuous) linear form Î›(u) âˆˆ L(R
m; R) such that
dJ(u) + Î›(u) â—¦ dÏˆ(um+1, . . . , un, u1, . . . , um) = 0,
namely
dJ(u) + Î›(u) â—¦ dÏ•(u) = 0.
However, Î›(u) is defined by some m-tuple (Î»1(u), . . . , Î»m(u)) âˆˆ R
m, and in view of the
definition of Ï•, the above equation is equivalent to
dJ(u) + Î»1(u)dÏ•1(u) + Â· Â· Â· + Î»m(u)dÏ•m(u) = 0.
The uniqueness of the Î»i(u) is a consequence of the linear independence of the dÏ•i(u).
We now investigate conditions for the existence of extrema involving the second derivative
of J.
1470 CHAPTER 40. EXTREMA OF REAL-VALUED FUNCTIONS
40.2 Using Second Derivatives to Find Extrema
For the sake of brevity, we consider only the case of local minima; analogous results are
obtained for local maxima (replace J by âˆ’J, since maxu J(u) = âˆ’ minu âˆ’J(u)). We begin
with a necessary condition for an unconstrained local minimum.
Proposition 40.5. Let E be a normed vector space and let J : â„¦ â†’ R be a function, with â„¦
some open subset of E. If the function J is differentiable in â„¦, if J has a second derivative
D2J(u) at some point u âˆˆ â„¦, and if J has a local minimum at u, then
D
2
J(u)(w, w) â‰¥ 0 for all w âˆˆ E.
Proof. Pick any nonzero vector w âˆˆ E. Since â„¦ is open, for t small enough, u + tw âˆˆ â„¦ and
J(u + tw) â‰¥ J(u), so there is some open interval I âŠ† R such that
u + tw âˆˆ â„¦ and J(u + tw) â‰¥ J(u)
for all t âˆˆ I. Using the Taylorâ€“Young formula and the fact that we must have dJ(u) = 0
since J has a local minimum at u, we get
0 â‰¤ J(u + tw) âˆ’ J(u) = t
2
2
D
2
J(u)(w, w) + t
2
k wk
2

(tw),
with limt7â†’0  (tw) = 0, which implies that
D
2
J(u)(w, w) â‰¥ 0.
Since the argument holds for all w âˆˆ E (trivially if w = 0), the proposition is proven.
One should be cautioned that there is no converse to the previous proposition. For examï¿¾ple, the function f : x 7â†’ x
3 has no local minimum at 0, yet df(0) = 0 and D2
f(0)(u, v) = 0.
Similarly, the reader should check that the function f : R
2 â†’ R given by
f(x, y) = x
2 âˆ’ 3y
3
has no local minimum at (0, 0); yet df(0, 0) = 0 since df(x, y) = (2x, âˆ’9y
2
), and for u =
(u1, u2), D2
f(0, 0)(u, u) = 2u
2
1 â‰¥ 0 since
D
2
f(x, y)(u, u) = ï¿¾ u1 u2


2 0
0 âˆ’18y
 
u
u
1
2

.
See Figure 40.3.
When E = R
n
, Proposition 40.5 says that a necessary condition for having a local
minimum is that the Hessian âˆ‡2J(u) be positive semidefinite (it is always symmetric).
We now give sufficient conditions for the existence of a local minimum.
40.2. USING SECOND DERIVATIVES TO FIND EXTREMA 1471
Figure 40.3: The graph of f(x, y) = x
2 âˆ’ 3y
3
. Note that (0, 0) not a local extremum despite
the fact that df(0, 0) = 0.
Theorem 40.6. Let E be a normed vector space, let J : â„¦ â†’ R be a function with â„¦ some
open subset of E, and assume that J is differentiable in â„¦ and that dJ(u) = 0 at some point
u âˆˆ â„¦. The following properties hold:
(1) If D2J(u) exists and if there is some number Î± âˆˆ R such that Î± > 0 and
D
2
J(u)(w, w) â‰¥ Î± k wk
2
for all w âˆˆ E,
then J has a strict local minimum at u.
(2) If D2J(v) exists for all v âˆˆ â„¦ and if there is a ball B âŠ† â„¦ centered at u such that
D
2
J(v)(w, w) â‰¥ 0 for all v âˆˆ B and all w âˆˆ E,
then J has a local minimum at u.
Proof. (1) Using the formula of Taylorâ€“Young, for every vector w small enough, we can write
J(u + w) âˆ’ J(u) = 1
2
D
2
J(u)(w, w) + k wk
2

(w)
â‰¥

1
2
Î± +  (w)
 k wk
2
with limw7â†’0  (w) = 0. Consequently if we pick r > 0 small enough that | (w)| < Î±/2 for all
w with k wk < r, then J(u + w) > J(u) for all u + w âˆˆ B, where B is the open ball of center
u and radius r. This proves that J has a local strict minimum at u.
1472 CHAPTER 40. EXTREMA OF REAL-VALUED FUNCTIONS
(2) The formula of Taylorâ€“Maclaurin shows that for all u + w âˆˆ B, we have
J(u + w) = J(u) + 1
2
D
2
J(v)(w, w) â‰¥ J(u),
for some v âˆˆ (u, u+w) (recall that (u, u+w) = {(1âˆ’Î»)(u+w)+Î»(u+w) | 0 < Î» < 1}).
There are no converses of the two assertions of Theorem 40.6. However, there is a
condition on D2J(u) that implies the condition of Part (1). Since this condition is easier to
state when E = R
n
, we begin with this case.
Recall that a nÃ—n symmetric matrix A is positive definite if x
> Ax > 0 for all x âˆˆ R
nâˆ’{0}.
In particular, A must be invertible.
Proposition 40.7. For any symmetric matrix A, if A is positive definite, then there is some
Î± > 0 such that
x
> Ax â‰¥ Î± k xk
2
for all x âˆˆ R
n
.
Proof. Pick any norm in R
n
(recall that all norms on R
n are equivalent). Since the unit
sphere S
nâˆ’1 = {x âˆˆ R
n
| kxk = 1} is compact and since the function f(x) = x
> Ax is never
zero on S
nâˆ’1
, the function f has a minimum Î± > 0 on S
nâˆ’1
. Using the usual trick that
x = k xk (x/ k xk ) for every nonzero vector x âˆˆ R
n and the fact that the inequality of the
proposition is trivial for x = 0, from
x
> Ax â‰¥ Î± for all x with k xk = 1,
we get
x
> Ax â‰¥ Î± k xk
2
for all x âˆˆ R
n
,
as claimed.
We can combine Theorem 40.6 and Proposition 40.7 to obtain a useful sufficient condition
for the existence of a strict local minimum. First let us introduce some terminology.
Definition 40.6. Given a function J : â„¦ â†’ R as before, say that a point u âˆˆ â„¦ is a
nondegenerate critical point if dJ(u) = 0 and if the Hessian matrix âˆ‡2J(u) is invertible.
Proposition 40.8. Let J : â„¦ â†’ R be a function defined on some open subset â„¦ âŠ† R
n
. If
J is differentiable in â„¦ and if some point u âˆˆ â„¦ is a nondegenerate critical point such that
âˆ‡2J(u) is positive definite, then J has a strict local minimum at u.
Remark: It is possible to generalize Proposition 40.8 to infinite-dimensional spaces by findï¿¾ing a suitable generalization of the notion of a nondegenerate critical point. Firstly, we
assume that E is a Banach space (a complete normed vector space). Then we define the
dual E
0 of E as the set of continuous linear forms on E, so that E
0 = L(E; R). Following
Lang, we use the notation E
0 for the space of continuous linear forms to avoid confusion
40.3. USING CONVEXITY TO FIND EXTREMA 1473
with the space E
âˆ— = Hom(E, R) of all linear maps from E to R. A continuous bilinear map
Ï•: E Ã— E â†’ R in L2(E, E; R) yields a map Î¦ from E to E
0 given by
Î¦(u) = Ï•u,
where Ï•u âˆˆ E
0 is the linear form defined by
Ï•u(v) = Ï•(u, v).
It is easy to check that Ï•u is continuous and that the map Î¦ is continuous. Then we say
that Ï• is nondegenerate iff Î¦: E â†’ E
0 is an isomorphism of Banach spaces, which means
that Î¦ is invertible and that both Î¦ and Î¦âˆ’1 are continuous linear maps. Given a function
J : â„¦ â†’ R differentiable on â„¦ as before (where â„¦ is an open subset of E), if D2J(u) exists
for some u âˆˆ â„¦, we say that u is a nondegenerate critical point if dJ(u) = 0 and if D2J(u) is
nondegenerate. Of course, D2J(u) is positive definite if D2J(u)(w, w) > 0 for all w âˆˆ Eâˆ’{0}.
Using the above definition, Proposition 40.7 can be generalized to a nondegenerate posiï¿¾tive definite bilinear form (on a Banach space) and Theorem 40.8 can also be generalized to
the situation where J : â„¦ â†’ R is defined on an open subset of a Banach space. For details
and proofs, see Cartan [34] (Part I Chapter 8) and Avez [9] (Chapter 8 and Chapter 10).
In the next section we make use of convexity; both on the domain â„¦ and on the function
J itself.
40.3 Using Convexity to Find Extrema
We begin by reviewing the definition of a convex set and of a convex function.
Definition 40.7. Given any real vector space E, we say that a subset C of E is convex if
either C = âˆ… or if for every pair of points u, v âˆˆ C, the line segment connecting u and v is
contained in C, i.e.,
(1 âˆ’ Î»)u + Î»v âˆˆ C for all Î» âˆˆ R such that 0 â‰¤ Î» â‰¤ 1.
Given any two points u, v âˆˆ E, the line segment [u, v] is the set
[u, v] = {(1 âˆ’ Î»)u + Î»v âˆˆ E | Î» âˆˆ R, 0 â‰¤ Î» â‰¤ 1}.
Clearly, a nonempty set C is convex iff [u, v] âŠ† C whenever u, v âˆˆ C. See Figure 40.4 for an
example of a convex set.
Definition 40.8. If C is a nonempty convex subset of E, a function f : C â†’ R is convex
(on C) if for every pair of points u, v âˆˆ C,
f((1 âˆ’ Î»)u + Î»v) â‰¤ (1 âˆ’ Î»)f(u) + Î»f(v) for all Î» âˆˆ R such that 0 â‰¤ Î» â‰¤ 1;
1474 CHAPTER 40. EXTREMA OF REAL-VALUED FUNCTIONS
(a)
(b)
u
v
u
v
Figure 40.4: Figure (a) shows that a sphere is not convex in R
3
since the dashed green line
does not lie on its surface. Figure (b) shows that a solid ball is convex in R
3
.
the function f is strictly convex (on C) if for every pair of distinct points u, v âˆˆ C (u 6 = v),
f((1 âˆ’ Î»)u + Î»v) < (1 âˆ’ Î»)f(u) + Î»f(v) for all Î» âˆˆ R such that 0 < Î» < 1;
see Figure 40.5. The epigraph1 epi(f) of a function f : A â†’ R defined on some subset A of
R
n
is the subset of R
n+1 defined as
epi(f) = {(x, y) âˆˆ R
n+1 | f(x) â‰¤ y, x âˆˆ A}.
A function f : C â†’ R defined on a convex subset C is concave (resp. strictly concave) if
(âˆ’f) is convex (resp. strictly convex).
It is obvious that a function f is convex iff its epigraph epi(f) is a convex subset of R
n+1
.
Example 40.4. Here are some common examples of convex sets.
â€¢ Subspaces V âŠ† E of a vector space E are convex.
â€¢ Affine subspaces, that is, sets of the form u+V , where V is a subspace of E and u âˆˆ E,
are convex.
â€¢ Balls (open or closed) are convex. Given any linear form Ï•: E â†’ R, for any scalar
c âˆˆ R, the closed halfâ€“spaces
HÏ•,c
+ = {u âˆˆ E | Ï•(u) â‰¥ c}, HÏ•,c
âˆ’ = {u âˆˆ E | Ï•(u) â‰¤ c},
are convex.
1â€œEpiâ€ means above.
40.3. USING CONVEXITY TO FIND EXTREMA 1475
u v
l = (1-Î»)f(u) + Î»f(v)
f
(a)
u v
l = (1-Î»)f(u) + Î»f(v)
f
(b)
Figure 40.5: Figures (a) and (b) are the graphs of real valued functions. Figure (a) is the
graph of convex function since the blue line lies above the graph of f. Figure (b) shows the
graph of a function which is not convex.
â€¢ Any intersection of halfâ€“spaces is convex.
â€¢ More generally, any intersection of convex sets is convex.
Example 40.5. Here are some common examples of convex and concave functions.
â€¢ Linear forms are convex functions (but not strictly convex).
â€¢ Any norm k k : E â†’ R+ is a convex function.
â€¢ The max function,
max(x1, . . . , xn) = max{x1, . . . , xn}
is convex on R
n
.
â€¢ The exponential x 7â†’ e
cx is strictly convex for any c 6 = 0 (c âˆˆ R).
â€¢ The logarithm function is concave on R+ âˆ’ {0}.
â€¢ The log-determinant function log det is concave on the set of symmetric positive definite
matrices. This function plays an important role in convex optimization.
An excellent exposition of convexity and its applications to optimization can be found in
Boyd [29].
1476 CHAPTER 40. EXTREMA OF REAL-VALUED FUNCTIONS
Here is a necessary condition for a function to have a local minimum with respect to a
convex subset U.
Theorem 40.9. (Necessary condition for a local minimum on a convex subset) Let J : â„¦ â†’ R
be a function defined on some open subset â„¦ of a normed vector space E and let U âŠ† â„¦ be
a nonempty convex subset. Given any u âˆˆ U, if dJ(u) exists and if J has a local minimum
in u with respect to U, then
dJ(u)(v âˆ’ u) â‰¥ 0 for all v âˆˆ U.
Proof. Let v = u + w be an arbitrary point in U. Since U is convex, we have u + tw âˆˆ U for
all t such that 0 â‰¤ t â‰¤ 1. Since dJ(u) exists, we can write
J(u + tw) âˆ’ J(u) = dJ(u)(tw) + k twk  (tw)
with limt7â†’0  (tw) = 0. However, because 0 â‰¤ t,
J(u + tw) âˆ’ J(u) = t(dJ(u)(w) + k wk  (tw))
and since u is a local minimum with respect to U, we have J(u + tw) âˆ’ J(u) â‰¥ 0, so we get
t(dJ(u)(w) + k wk  (tw)) â‰¥ 0.
The above implies that dJ(u)(w) â‰¥ 0, because otherwise we could pick t > 0 small enough
so that
dJ(u)(w) + k wk  (tw) < 0,
a contradiction. Since the argument holds for all v = u + w âˆˆ U, the theorem is proven.
Observe that the convexity of U is a substitute for the use of Lagrange multipliers, but
we now have to deal with an inequality instead of an equality.
In the special case where U is a subspace of E we have the following result.
Corollary 40.10. With the same assumptions as in Theorem 40.9, if U is a subspace of E,
if dJ(u) exists and if J has a local minimum in u with respect to U, then
dJ(u)(w) = 0 for all w âˆˆ U.
Proof. In this case since u âˆˆ U we have 2u âˆˆ U, and for any u + w âˆˆ U, we must have
2uâˆ’(u+w) = uâˆ’w âˆˆ U. The previous theorem implies that dJ(u)(w) â‰¥ 0 and dJ(u)(âˆ’w) â‰¥
0, that is, dJ(u)(w) â‰¤ 0, so dJ(u) = 0. Since the argument holds for w âˆˆ U (because U is a
subspace, if u, w âˆˆ U, then u + w âˆˆ U), we conclude that
dJ(u)(w) = 0 for all w âˆˆ U.
We will now characterize convex functions when they have a first derivative or a second
derivative.
40.3. USING CONVEXITY TO FIND EXTREMA 1477
Proposition 40.11. (Convexity and first derivative) Let f : â„¦ â†’ R be a function differï¿¾entiable on some open subset â„¦ of a normed vector space E and let U âŠ† â„¦ be a nonempty
convex subset.
(1) The function f is convex on U iff
f(v) â‰¥ f(u) + df(u)(v âˆ’ u) for all u, v âˆˆ U.
(2) The function f is strictly convex on U iff
f(v) > f(u) + df(u)(v âˆ’ u) for all u, v âˆˆ U with u 6 = v.
See Figure 40.6.
u v
f
(u, f(u))
(v, f(v))
(y,v)
v - u
y - v
y = f(u) + df(u)(v-u)
Figure 40.6: An illustration of a convex valued function f. Since f is convex it always lies
above its tangent line.
Proof. Let u, v âˆˆ U be any two distinct points and pick Î» âˆˆ R with 0 < Î» < 1. If the
function f is convex, then
f((1 âˆ’ Î»)u + Î»v) â‰¤ (1 âˆ’ Î»)f(u) + Î»f(v),
which yields
f((1 âˆ’ Î»)u + Î»v) âˆ’ f(u)
Î»
â‰¤ f(v) âˆ’ f(u).
It follows that
df(u)(v âˆ’ u) = lim
Î»7â†’0
f((1 âˆ’ Î»)u + Î»v) âˆ’ f(u)
Î»
â‰¤ f(v) âˆ’ f(u).
1478 CHAPTER 40. EXTREMA OF REAL-VALUED FUNCTIONS
If f is strictly convex, the above reasoning does not work, because a strict inequality is not
necessarily preserved by â€œpassing to the limit.â€ We have recourse to the following trick: for
any Ï‰ such that 0 < Ï‰ < 1, observe that
(1 âˆ’ Î»)u + Î»v = u + Î»(v âˆ’ u) = Ï‰ âˆ’ Î»
Ï‰
u +
Î»
Ï‰
(u + Ï‰(v âˆ’ u)).
If we assume that 0 < Î» â‰¤ Ï‰, the convexity of f yields
f(u + Î»(v âˆ’ u)) = f
 1 âˆ’
Ï‰
Î»

u +
Ï‰
Î»
(u + Ï‰(v âˆ’ u)) â‰¤
Ï‰ âˆ’
Ï‰
Î»
f(u) +
Ï‰
Î»
f(u + Ï‰(v âˆ’ u)).
If we subtract f(u) to both sides, we get
f(u + Î»(v âˆ’ u)) âˆ’ f(u)
Î»
â‰¤
f(u + Ï‰(v âˆ’ u)) âˆ’ f(u)
Ï‰
.
Now since 0 < Ï‰ < 1 and f is strictly convex,
f(u + Ï‰(v âˆ’ u)) = f((1 âˆ’ Ï‰)u + Ï‰v) < (1 âˆ’ Ï‰)f(u) + Ï‰f(v),
which implies that
f(u + Ï‰(v âˆ’ u)) âˆ’ f(u)
Ï‰
< f(v) âˆ’ f(u),
and thus we get
f(u + Î»(v âˆ’ u)) âˆ’ f(u)
Î»
â‰¤
f(u + Ï‰(v âˆ’ u)) âˆ’ f(u)
Ï‰
< f(v) âˆ’ f(u).
If we let Î» go to 0, by passing to the limit we get
df(u)(v âˆ’ u) â‰¤
f(u + Ï‰(v âˆ’ u)) âˆ’ f(u)
Ï‰
< f(v) âˆ’ f(u),
which yields the desired strict inequality.
Let us now consider the converse of (1); that is, assume that
f(v) â‰¥ f(u) + df(u)(v âˆ’ u) for all u, v âˆˆ U.
For any two distinct points u, v âˆˆ U and for any Î» with 0 < Î» < 1, we get
f(v) â‰¥ f(v + Î»(u âˆ’ v)) âˆ’ Î»df(v + Î»(u âˆ’ v))(u âˆ’ v)
f(u) â‰¥ f(v + Î»(u âˆ’ v)) + (1 âˆ’ Î»)df(v + Î»(u âˆ’ v))(u âˆ’ v),
and if we multiply the first inequality by 1 âˆ’Î» and the second inequality by Î» and them add
up the resulting inequalities, we get
(1 âˆ’ Î»)f(v) + Î»f(u) â‰¥ f(v + Î»(u âˆ’ v)) = f((1 âˆ’ Î»)v + Î»u),
which proves that f is convex.
The proof of the converse of (2) is similar, except that the inequalities are replaced by
strict inequalities.
40.3. USING CONVEXITY TO FIND EXTREMA 1479
We now establish a convexity criterion using the second derivative of f. This criterion is
often easier to check than the previous one.
Proposition 40.12. (Convexity and second derivative) Let f : â„¦ â†’ R be a function twice
differentiable on some open subset â„¦ of a normed vector space E and let U âŠ† â„¦ be a nonempty
convex subset.
(1) The function f is convex on U iff
D
2
f(u)(v âˆ’ u, v âˆ’ u) â‰¥ 0 for all u, v âˆˆ U.
(2) If
D
2
f(u)(v âˆ’ u, v âˆ’ u) > 0 for all u, v âˆˆ U with u 6 = v,
then f is strictly convex.
Proof. First assume that the inequality in Condition (1) is satisfied. For any two distinct
points u, v âˆˆ U, the formula of Taylorâ€“Maclaurin yields
f(v) âˆ’ f(u) âˆ’ df(u)(v âˆ’ u) = 1
2
D
2
f(w)(v âˆ’ u, v âˆ’ u)
=
Ï
2
2
D
2
f(w)(v âˆ’ w, v âˆ’ w),
for some w = (1 âˆ’ Î»)u + Î»v = u + Î»(v âˆ’ u) with 0 < Î» < 1, and with Ï = 1/(1 âˆ’ Î») > 0,
so that v âˆ’ u = Ï(v âˆ’ w). Since D2
f(w)(v âˆ’ w, v âˆ’ w) â‰¥ 0 for all u, w âˆˆ U, we conclude by
applying Proposition 40.11(1).
Similarly, if (2) holds, the above reasoning and Proposition 40.11(2) imply that f is
strictly convex.
To prove the necessary condition in (1), define g : â„¦ â†’ R by
g(v) = f(v) âˆ’ df(u)(v),
where u âˆˆ U is any point considered fixed. If f is convex, since
g(v) âˆ’ g(u) = f(v) âˆ’ f(u) âˆ’ df(u)(v âˆ’ u),
Proposition 40.11 implies that f(v) âˆ’ f(u) âˆ’ df(u)(v âˆ’ u) â‰¥ 0, which implies that g has a
local minimum at u with respect to all v âˆˆ U. Therefore, we have dg(u) = 0. Observe that
g is twice differentiable in â„¦ and D2
g(u) = D2
f(u), so the formula of Taylorâ€“Young yields
for every v = u + w âˆˆ U and all t with 0 â‰¤ t â‰¤ 1,
0 â‰¤ g(u + tw) âˆ’ g(u) = t
2
2
D
2
f(u)(tw, tw) + k twk 2

(tw)
=
t
2
2
(D2
f(u)(w, w) + 2 k wk
2

(wt)),
with limt7â†’0  (wt) = 0, and for t small enough, we must have D2
f(u)(w, w) â‰¥ 0, as claimed.
1480 CHAPTER 40. EXTREMA OF REAL-VALUED FUNCTIONS
The converse of Proposition 40.12 (2) is false as we see by considering the strictly convex
function f given by f(x) = x
4 and its second derivative at x = 0.
Example 40.6. On the other hand, if f is a quadratic function of the form
f(u) = 1
2
u
> Au âˆ’ u
> b
where A is a symmetric matrix, we know that
df(u)(v) = v
> (Au âˆ’ b),
so
f(v) âˆ’ f(u) âˆ’ df(u)(v âˆ’ u) = 1
2
v
> Av âˆ’ v
> b âˆ’
1
2
u
> Au + u
> b âˆ’ (v âˆ’ u)
> (Au âˆ’ b)
=
1
2
v
> Av âˆ’
1
2
u
> Au âˆ’ (v âˆ’ u)
> Au
=
1
2
v
> Av +
1
2
u
> Au âˆ’ v
> Au
=
1
2
(v âˆ’ u)
> A(v âˆ’ u).
Therefore, Proposition 40.11 implies that if A is positive semidefinite, then f is convex and
if A is positive definite, then f is strictly convex. The converse follows by Proposition 40.12.
We conclude this section by applying our previous theorems to convex functions defined
on convex subsets. In this case local minima (resp. local maxima) are global minima (resp.
global maxima). The next definition is the special case of Definition 40.1 in which W = E
but it does not hurt to state it explicitly.
Definition 40.9. Let f : E â†’ R be any function defined on some normed vector space (or
more generally, any set). For any u âˆˆ E, we say that f has a minimum in u (resp. maximum
in u) if
f(u) â‰¤ f(v) (resp. f(u) â‰¥ f(v)) for all v âˆˆ E.
We say that f has a strict minimum in u (resp. strict maximum in u) if
f(u) < f(v) (resp. f(u) > f(v)) for all v âˆˆ E âˆ’ {u}.
If U âŠ† E is a subset of E and u âˆˆ U, we say that f has a minimum in u (resp. strict
minimum in u) with respect to U if
f(u) â‰¤ f(v) for all v âˆˆ U (resp. f(u) < f(v) for all v âˆˆ U âˆ’ {u}),
and similarly for a maximum in u (resp. strict maximum in u) with respect to U with â‰¤
changed to â‰¥ and < to >.
40.3. USING CONVEXITY TO FIND EXTREMA 1481
Sometimes, we say global maximum (or minimum) to stress that a maximum (or a minï¿¾imum) is not simply a local maximum (or minimum).
Theorem 40.13. Given any normed vector space E, let U be any nonempty convex subset
of E.
(1) For any convex function J : U â†’ R, for any u âˆˆ U, if J has a local minimum at u in
U, then J has a (global) minimum at u in U.
(2) Any strict convex function J : U â†’ R has at most one minimum (in U), and if it does,
then it is a strict minimum (in U).
(3) Let J : â„¦ â†’ R be any function defined on some open subset â„¦ of E with U âŠ† â„¦ and
assume that J is convex on U. For any point u âˆˆ U, if dJ(u) exists, then J has a
minimum in u with respect to U iff
dJ(u)(v âˆ’ u) â‰¥ 0 for all v âˆˆ U.
(4) If the convex subset U in (3) is open, then the above condition is equivalent to
dJ(u) = 0.
Proof. (1) Let v = u + w be any arbitrary point in U. Since J is convex, for all t with
0 â‰¤ t â‰¤ 1, we have
J(u + tw) = J(u + t(v âˆ’ u)) = J((1 âˆ’ t)u + tv) â‰¤ (1 âˆ’ t)J(u) + tJ(v),
which yields
J(u + tw) âˆ’ J(u) â‰¤ t(J(v) âˆ’ J(u)).
Because J has a local minimum at u, there is some t0 with 0 < t0 < 1 such that
0 â‰¤ J(u + t0w) âˆ’ J(u) â‰¤ t0(J(v) âˆ’ J(u)),
which implies that J(v) âˆ’ J(u) â‰¥ 0.
(2) If J is strictly convex, the above reasoning with w 6 = 0 shows that there is some t0
with 0 < t0 < 1 such that
0 â‰¤ J(u + t0w) âˆ’ J(u) < t0(J(v) âˆ’ J(u)),
which shows that u is a strict global minimum (in U), and thus that it is unique.
(3) We already know from Theorem 40.9 that the condition dJ(u)(vâˆ’u) â‰¥ 0 for all v âˆˆ U
is necessary (even if J is not convex). Conversely, because J is convex, careful inspection
of the proof of Part (1) of Proposition 40.11 shows that only the fact that dJ(u) exists is
needed to prove that
J(v) âˆ’ J(u) â‰¥ dJ(u)(v âˆ’ u) for all v âˆˆ U,
1482 CHAPTER 40. EXTREMA OF REAL-VALUED FUNCTIONS
and if
dJ(u)(v âˆ’ u) â‰¥ 0 for all v âˆˆ U,
then
J(v) âˆ’ J(u) â‰¥ 0 for all v âˆˆ U,
as claimed.
(4) If U is open, then for every u âˆˆ U we can find an open ball B centered at u of radius

small enough so that B âŠ† U. Then for any w 6 = 0 such that k wk < , we have both
v = u + w âˆˆ B and v
0 = u âˆ’ w âˆˆ B, so Condition (3) implies that
dJ(u)(w) â‰¥ 0 and dJ(u)(âˆ’w) â‰¥ 0,
which yields
dJ(u)(w) = 0.
Since the above holds for all w 6 = 0 such such that k wk <  and since dJ(u) is linear, we
leave it to the reader to fill in the details of the proof that dJ(u) = 0.
Example 40.7. Theorem 40.13 can be used to rederive the fact that the least squares
solutions of a linear system Ax = b (where A is an m Ã— n matrix) are given by the normal
equation
A
> Ax = A
> b.
For this, we consider the quadratic function
J(v) = 1
2
k
Av âˆ’ bk
2
2 âˆ’
1
2
k
bk
2
2
,
and our least squares problem is equivalent to finding the minima of J on R
n
. A computation
reveals that
J(v) = 1
2
k
Av âˆ’ bk
2
2 âˆ’
1
2
k
bk
2
2
=
1
2
(Av âˆ’ b)
> (Av âˆ’ b) âˆ’
1
2
b
> b
=
1
2
(v
> A
> âˆ’ b
> )(Av âˆ’ b) âˆ’
1
2
b
> b
=
1
2
v
> A
> Av âˆ’ v
> A
> b,
and so
dJ(u) = A
> Au âˆ’ A
> b.
Since A> A is positive semidefinite, the function J is convex, and Theorem 40.13(4) implies
that the minima of J are the solutions of the equation
A
> Au âˆ’ A
> b = 0.
40.4. SUMMARY 1483
The considerations in this chapter reveal the need to find methods for finding the zeros
of the derivative map
dJ : â„¦ â†’ E
0 ,
where â„¦ is some open subset of a normed vector space E and E
0 is the space of all continuous
linear forms on E (a subspace of E
âˆ—
). Generalizations of Newtonâ€™s method yield such methods
and they are the object of the next chapter.
40.4 Summary
The main concepts and results of this chapter are listed below:
â€¢ Local minimum, local maximum, local extremum, strict local minimum, strict local
maximum.
â€¢ Necessary condition for a local extremum involving the derivative; critical point.
â€¢ Local minimum with respect to a subset U, local maximum with respect to a subset
U, local extremum with respect to a subset U.
â€¢ Constrained local extremum.
â€¢ Necessary condition for a constrained extremum.
â€¢ Necessary condition for a constrained extremum in terms of Lagrange multipliers.
â€¢ Lagrangian.
â€¢ Critical points of a Lagrangian.
â€¢ Necessary condition of an unconstrained local minimum involving the second-order
derivative.
â€¢ Sufficient condition for a local minimum involving the second-order derivative.
â€¢ A sufficient condition involving nondegenerate critical points.
â€¢ Convex sets, convex functions, concave functions, strictly convex functions, strictly
concave functions.
â€¢ Necessary condition for a local minimum on a convex set involving the derivative.
â€¢ Convexity of a function involving a condition on its first derivative.
â€¢ Convexity of a function involving a condition on its second derivative.
â€¢ Minima of convex functions on convex sets.
1484 CHAPTER 40. EXTREMA OF REAL-VALUED FUNCTIONS
40.5 Problems
Problem 40.1. Find the extrema of the function J(v1, v2) = v2
2 on the subset U given by
U = {(v1, v2) âˆˆ R
2
| v1
2 + v2
2 âˆ’ 1 = 0}.
Problem 40.2. Find the extrema of the function J(v1, v2) = v1 + (v2 âˆ’ 1)2 on the subset U
given by
U = {(v1, v2) âˆˆ R
2
| v1
2 = 0}.
Problem 40.3. Let A be an n Ã— n real symmetric matrix, B an n Ã— n symmetric positive
definite matrix, and let b âˆˆ R
n
.
(1) Prove that a necessary condition for the function J given by
J(v) = 1
2
v
> Av âˆ’ b
> v
to have an extremum at u âˆˆ U, with U defined by
U = {v âˆˆ R
n
| v
> Bv = 1},
is that there is some Î» âˆˆ R such that
Au âˆ’ b = Î»Bu.
(2) Prove that there is a symmetric positive definite matrix S such that B = S
2
. Prove
that if b = 0, then Î» is an eigenvalue of the symmetric matrix S
âˆ’1ASâˆ’1
.
(3) Prove that for all (u, Î») âˆˆ U Ã— R, if Au âˆ’ b = Î»Bu, then
J(v) âˆ’ J(u) = 1
2
(v âˆ’ u)
> (A âˆ’ Î»B)(v âˆ’ u)
for all v âˆˆ U. Deduce that without additional assumptions, it is not possible to conclude
that u is an extremum of J on U.
Problem 40.4. Let E be a normed vector space, and let U be a subset of E such that for
all u, v âˆˆ U, we have (1/2)(u + v) âˆˆ U.
(1) Prove that if U is closed, then U is convex.
Hint. Every real Î¸ âˆˆ (0, 1) can be written as
Î¸ =
X
nâ‰¥1
Î±n2
