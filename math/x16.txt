.
The first three columns of the above matrix are linearly dependent.
So one of the entries a
(
i k
k) with k ≤ i ≤ n can be chosen as pivot, and we permute the kth
row with the ith row, obtaining the matrix α
(k) = (αj l
(k)
). The new pivot is πk = αk k
(k)
, and
we zero the entries i = k + 1, . . . , n in column k by adding −αi k
(k)
/πk times row k to row i.
At the end of this step, we have Ak+1. Observe that the first k − 1 rows of Ak are identical
to the first k − 1 rows of Ak+1.
The process of Gaussian elimination is illustrated in schematic form below:


× × × ×
× × × ×
× × × ×
× × × ×

 =⇒


× × × ×
0 × × ×
0 × × ×
0 × × ×

 =⇒


× × × ×
0
0
0
× × ×
0
0
× ×
× ×

 =⇒


× × × ×
0
0 0
0 0
× × ×
× ×
0 ×

 .
8.3 Elementary Matrices and Row Operations
It is easy to figure out what kind of matrices perform the elementary row operations used
during Gaussian elimination. The key point is that if A = P B, where A, B are m × n
matrices and P is a square matrix of dimension m, if (as usual) we denote the rows of A and
B by A1, . . . , Am and B1, . . . , Bm, then the formula
aij =
mX
k=1
pikbkj
giving the (i, j)th entry in A shows that the ith row of A is a linear combination of the rows
of B:
Ai = pi1B1 + · · · + pimBm.
Therefore, multiplication of a matrix on the left by a square matrix performs row opera￾tions. Similarly, multiplication of a matrix on the right by a square matrix performs column
operations
The permutation of the kth row with the ith row is achieved by multiplying A on the left
by the transposition matrix P(i, k), which is the matrix obtained from the identity matrix
8.3. ELEMENTARY MATRICES AND ROW OPERATIONS 255
by permuting rows i and k, i.e.,
P(i, k) =


1
1
0 1
1
.
.
.
1
1 0
1
1


.
For example, if m = 3,
P(1, 3) =


0 0 1
0 1 0
1 0 0

 ,
then
P(1, 3)B =


0 0 1
0 1 0
1 0 0




b11 b12 · · · · · · · · · b1n
b21 b22 · · · · · · · · · b2n
b31 b32 · · · · · · · · · b3n

 =


b31 b32 · · · · · · · · · b3n
b21 b22 · · · · · · · · · b2n
b11 b12 · · · · · · · · · b1n

 .
Observe that det(P(i, k)) = −1. Furthermore, P(i, k) is symmetric (P(i, k)
> = P(i, k)), and
P(i, k)
−1 = P(i, k).
During the permutation Step (2), if row k and row i need to be permuted, the matrix A
is multiplied on the left by the matrix Pk such that Pk = P(i, k), else we set Pk = I.
Adding β times row j to row i (with i 6 = j) is achieved by multiplying A on the left by
the elementary matrix ,
Ei,j;β = I + βei j ,
where
(ei j )k l =

1 if
0 if
k
k
=
6
=
i
i
and
or l 6 =
l =
j,
j
i.e.,
Ei,j;β =


1
1
1
1
.
.
.
1
β 1
1
1


or Ei,j;β =


1
1
1 β
1
.
.
.
1
1
1
1


,
256 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM
on the left, i > j, and on the right, i < j. The index i is the index of the row that is changed
by the multiplication. For example, if m = 3 and we want to add twice row 1 to row 3, since
β = 2, j = 1 and i = 3, we form
E3,1;2 = I + 2e31 =


1 0 0
0 1 0
0 0 1

 +


0 0 0
0 0 0
2 0 0

 =


1 0 0
0 1 0
2 0 1

 ,
and calculate
E3,1;2B =


1 0 0
0 1 0
2 0 1




b11 b12 · · · · · · · · · b1n
b21 b22 · · · · · · · · · b2n
b31 b32 · · · · · · · · · b3n


=


b11 b12 · · · · · · · · · b1n
b21 b22 · · · · · · · · · b2n
2b11 + b31 2b12 + b32 · · · · · · · · · 2b1n + b3n

 .
Observe that the inverse of Ei,j;β = I + βei j is Ei,j;−β = I − βei j and that det(Ei,j;β) = 1.
Therefore, during Step 3 (the elimination step), the matrix A is multiplied on the left by a
product Ek of matrices of the form Ei,k;βi,k , with i > k.
Consequently, we see that
Ak+1 = EkPkAk,
and then
Ak = Ek−1Pk−1 · · · E1P1A.
This justifies the claim made earlier that Ak = MkA for some invertible matrix Mk; we can
pick
Mk = Ek−1Pk−1 · · · E1P1,
a product of invertible matrices.
The fact that det(P(i, k)) = −1 and that det(Ei,j;β) = 1 implies immediately the fact
claimed above: We always have
det(Ak) = ± det(A).
Furthermore, since
Ak = Ek−1Pk−1 · · · E1P1A
and since Gaussian elimination stops for k = n, the matrix
An = En−1Pn−1 · · · E2P2E1P1A
is upper-triangular. Also note that if we let M = En−1Pn−1 · · · E2P2E1P1, then det(M) = ±1,
and
det(A) = ± det(An).
The matrices P(i, k) and Ei,j;β are called elementary matrices. We can summarize the
above discussion in the following theorem:
8.4. LU-FACTORIZATION 257
Theorem 8.1. (Gaussian elimination) Let A be an n × n matrix (invertible or not). Then
there is some invertible matrix M so that U = MA is upper-triangular. The pivots are all
nonzero iff A is invertible.
Proof. We already proved the theorem when A is invertible, as well as the last assertion.
Now A is singular iff some pivot is zero, say at Stage k of the elimination. If so, we must
have a
(
i k
k) = 0 for i = k, . . . , n; but in this case, Ak+1 = Ak and we may pick Pk = Ek = I.
Remark: Obviously, the matrix M can be computed as
M = En−1Pn−1 · · · E2P2E1P1,
but this expression is of no use. Indeed, what we need is M−1
; when no permutations are
needed, it turns out that M−1
can be obtained immediately from the matrices Ek’s, in fact,
from their inverses, and no multiplications are necessary.
Remark: Instead of looking for an invertible matrix M so that MA is upper-triangular, we
can look for an invertible matrix M so that MA is a diagonal matrix. Only a simple change
to Gaussian elimination is needed. At every Stage k, after the pivot has been found and
pivoting been performed, if necessary, in addition to adding suitable multiples of the kth
row to the rows below row k in order to zero the entries in column k for i = k + 1, . . . , n,
also add suitable multiples of the kth row to the rows above row k in order to zero the
entries in column k for i = 1, . . . , k − 1. Such steps are also achieved by multiplying on
the left by elementary matrices Ei,k;βi,k , except that i < k, so that these matrices are not
lower-triangular matrices. Nevertheless, at the end of the process, we find that An = MA,
is a diagonal matrix.
This method is called the Gauss-Jordan factorization. Because it is more expensive than
Gaussian elimination, this method is not used much in practice. However, Gauss-Jordan
factorization can be used to compute the inverse of a matrix A. Indeed, we find the jth
column of A−1 by solving the system Ax(j) = ej (where ej
is the jth canonical basis vector
of R
n
). By applying Gauss-Jordan, we are led to a system of the form Djx
(j) = Mjej
, where
Dj
is a diagonal matrix, and we can immediately compute x
(j)
.
It remains to discuss the choice of the pivot, and also conditions that guarantee that no
permutations are needed during the Gaussian elimination process. We begin by stating a
necessary and sufficient condition for an invertible matrix to have an LU-factorization (i.e.,
Gaussian elimination does not require pivoting).
8.4 LU-Factorization
Definition 8.1. We say that an invertible matrix A has an LU-factorization if it can be
written as A = LU, where U is upper-triangular invertible and L is lower-triangular, with
Li i = 1 for i = 1, . . . , n.
258 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM
A lower-triangular matrix with diagonal entries equal to 1 is called a unit lower-triangular
matrix. Given an n×n matrix A = (ai j ), for any k with 1 ≤ k ≤ n, let A(1 : k, 1 : k) denote
the submatrix of A whose entries are ai j , where 1 ≤ i, j ≤ k.
1 For example, if A is the 5 × 5
matrix
A =


a11 a12 a13 a14 a15
a21 a22 a23 a24 a25
a31 a32 a33 a34 a35
a41 a42 a43 a44 a45
a51 a52 a53 a54 a55


,
then
A(1 : 3, 1 : 3) =


a11 a12 a13
a21 a22 a23
a31 a32 a33

 .
Proposition 8.2. Let A be an invertible n × n-matrix. Then A has an LU-factorization
A = LU iff every matrix A(1 : k, 1 : k) is invertible for k = 1, . . . , n. Furthermore, when A
has an LU-factorization, we have
det(A(1 : k, 1 : k)) = π1π2 · · · πk, k = 1, . . . , n,
where πk is the pivot obtained after k −1 elimination steps. Therefore, the kth pivot is given
by
πk =



a11 = det(A(1 : 1, 1 : 1)) if k = 1
det(A(1 : k, 1 : k))
det(A(1 : k − 1, 1 : k − 1)) if k = 2, . . . , n.
Proof. First assume that A = LU is an LU-factorization of A. We can write
A =

A(1 : k, 1 : k) A2
A3 A4

=

L1 0
L3 L4
 
U1 U2
0 U4

=

L1U1 L1U2
L3U1 L3U2 + L4U4

,
where L1, L4 are unit lower-triangular and U1, U4 are upper-triangular. (Note, A(1 : k, 1 : k),
L1, and U1 are k ×k matrices; A2 and U2 are k ×(n−k) matrices; A3 and L3 are (n−k)×k
matrices; A4, L4, and U4 are (n − k) × (n − k) matrices.) Thus,
A(1 : k, 1 : k) = L1U1,
and since U is invertible, U1 is also invertible (the determinant of U is the product of the
diagonal entries in U, which is the product of the diagonal entries in U1 and U4). As L1 is
invertible (since its diagonal entries are equal to 1), we see that A(1 : k, 1 : k) is invertible
for k = 1, . . . , n.
Conversely, assume that A(1 : k, 1 : k) is invertible for k = 1, . . . , n. We just need to
show that Gaussian elimination does not need pivoting. We prove by induction on k that
the kth step does not need pivoting.
1We are using Matlab’s notation.
8.4. LU-FACTORIZATION 259
This holds for k = 1, since A(1 : 1, 1 : 1) = (a1 1), so a1 1 6 = 0. Assume that no pivoting
was necessary for the first k − 1 steps (2 ≤ k ≤ n − 1). In this case, we have
Ek−1 · · · E2E1A = Ak,
where L = Ek−1 · · · E2E1 is a unit lower-triangular matrix and Ak(1 : k, 1 : k) is upper￾triangular, so that LA = Ak can be written as

L1 0
L3 L4
 
A(1 : k, 1 : k) A2
A3 A4

=

U1 B2
0 B4

,
where L1 is unit lower-triangular and U1 is upper-triangular. (Once again A(1 : k, 1 : k), L1,
and U1 are k × k matrices; A2 and B2 are k × (n − k) matrices; A3 and L3 are (n − k) × k
matrices; A4, L4, and B4 are (n − k) × (n − k) matrices.) But then,
L1A(1 : k, 1 : k)) = U1,
where L1 is invertible (in fact, det(L1) = 1), and since by hypothesis A(1 : k, 1 : k) is
invertible, U1 is also invertible, which implies that (U1)kk 6 = 0, since U1 is upper-triangular.
Therefore, no pivoting is needed in Step k, establishing the induction step. Since det(L1) = 1,
we also have
det(U1) = det(L1A(1 : k, 1 : k)) = det(L1) det(A(1 : k, 1 : k)) = det(A(1 : k, 1 : k)),
and since U1 is upper-triangular and has the pivots π1, . . . , πk on its diagonal, we get
det(A(1 : k, 1 : k)) = π1π2 · · · πk, k = 1, . . . , n,
as claimed.
Remark: The use of determinants in the first part of the proof of Proposition 8.2 can be
avoided if we use the fact that a triangular matrix is invertible iff all its diagonal entries are
nonzero.
Corollary 8.3. (LU-Factorization) Let A be an invertible n × n-matrix. If every matrix
A(1 : k, 1 : k) is invertible for k = 1, . . . , n, then Gaussian elimination requires no pivoting
and yields an LU-factorization A = LU.
Proof. We proved in Proposition 8.2 that in this case Gaussian elimination requires no
pivoting. Then since every elementary matrix Ei,k;β is lower-triangular (since we always
arrange that the pivot πk occurs above the rows that it operates on), since Ei,k
−1
;β = Ei,k;−β
and the Eks are products of Ei,k;βi,k s, from
En−1 · · · E2E1A = U,
where U is an upper-triangular matrix, we get
A = LU,
where L = E1
−1E2
−1
· · · En
−1
−1
is a lower-triangular matrix. Furthermore, as the diagonal
entries of each Ei,k;β are 1, the diagonal entries of each Ek are also 1.
260 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM
Example 8.1. The reader should verify that


2 1 1 0
4 3 3 1
8 7 9 5
6 7 9 8

 =


1 0 0 0
2 1 0 0
4 3 1 0
3 4 1 1




2 1 1 0
0 1 1 1
0 0 2 2
0 0 0 2


is an LU-factorization.
One of the main reasons why the existence of an LU-factorization for a matrix A is
interesting is that if we need to solve several linear systems Ax = b corresponding to the
same matrix A, we can do this cheaply by solving the two triangular systems
Lw = b, and Ux = w.
There is a certain asymmetry in the LU-decomposition A = LU of an invertible matrix A.
Indeed, the diagonal entries of L are all 1, but this is generally false for U. This asymmetry
can be eliminated as follows: if
D = diag(u11, u22, . . . , unn)
is the diagonal matrix consisting of the diagonal entries in U (the pivots), then if we let
U
0 = D−1U, we can write
A = LDU0 ,
where L is lower- triangular, U
0 is upper-triangular, all diagonal entries of both L and U
0
are 1, and D is a diagonal matrix of pivots. Such a decomposition leads to the following
definition.
Definition 8.2. We say that an invertible n×n matrix A has an LDU-factorization if it can
be written as A = LDU0 , where L is lower- triangular, U
0 is upper-triangular, all diagonal
entries of both L and U
0 are 1, and D is a diagonal matrix.
We will see shortly than if A is real symmetric, then U
0 = L
> .
As we will see a bit later, real symmetric positive definite matrices satisfy the condition of
Proposition 8.2. Therefore, linear systems involving real symmetric positive definite matrices
can be solved by Gaussian elimination without pivoting. Actually, it is possible to do better:
this is the Cholesky factorization.
If a square invertible matrix A has an LU-factorization, then it is possible to find L and U
while performing Gaussian elimination. Recall that at Step k, we pick a pivot πk = aik
(k)
6 = 0
in the portion consisting of the entries of index j ≥ k of the k-th column of the matrix Ak
obtained so far, we swap rows i and k if necessary (the pivoting step), and then we zero the
entries of index j = k + 1, . . . , n in column k. Schematically, we have the following steps:


× × × × ×
0 × × × ×
0
0
× × × ×
a
(k)
ik × × ×
0 × × × ×


pivot
=⇒


× × × × ×
0 a
(k)
ik × × ×
0 × × × ×
0
0
×
× × × ×
× × ×


elim=⇒


× × × × ×
0
0
× × × ×
0 × × ×
0
0
0
0
× × ×
× × ×


.
8.4. LU-FACTORIZATION 261
More precisely, after permuting row k and row i (the pivoting step), if the entries in column
k below row k are αk+1k, . . . , αnk, then we add −αjk/πk times row k to row j; this process
is illustrated below:


a
(k)
kk
a
(k)
k+1k
.
.
.
a
(k)
ik
.
.
a
(
.
k)
nk


pivot
=⇒


a
(k)
ik
a
(k)
k+1k
.
.
.
a
(k)
kk
.
.
a
(
.
k)
nk


=


πk
αk+1k
.
α
.
.
ik
.
.
.
αnk


elim=⇒


πk
0
.
.
.
0
0
.
.
.


.
Then if we write ` jk = αjk/πk for j = k + 1, . . . , n, the kth column of L is


0
.
.
.
0
1
`
k+1k
.
.
.
`
nk


.
Observe that the signs of the multipliers −αjk/πk have been flipped. Thus, we obtain the
unit lower triangular matrix
L =


1 0 0 · · · 0
`
21 1 0 · · · 0
`
31 ` 32 1 · · · 0
.
.
.
.
.
.
.
.
.
.
.
. 0
`
n1 ` n2 ` n3 · · · 1


.
It is easy to see (and this is proven in Theorem 8.5) that if the result of Gaussian elimination
(without pivoting) is U = En−1 · · · E1A, so that L = E1
−1E2
−1
· · · En
−
−
1
1
, then
Ek =


1 · · · 0 0 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 · · · 1 0 · · · 0
0 · · · −` k+1k 1 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 · · · −` nk 0 · · · 1


and Ek
−1 =


1
.
· · · 0 0 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 · · · 1 0 · · · 0
0 · · · ` k+1k 1 · · · 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 · · · ` nk 0 · · · 1


,
so the kth column of Ek
−1
is the kth column of L.
262 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM
Unfortunately, even though L
−1 = En−1 · · · E2E1, the matrices Ek occur in the wrong
order and the kth column of L
−1
is not the kth column of Ek.
Here is an example illustrating the method.
Example 8.2. Given
A = A1 =


1 1 1 0
1
1 1
−1 0 1
−1 0
1 −1 0 −1

 ,
we have the following sequence of steps: The first pivot is π1 = 1 in row 1, and we subtract
row 1 from rows 2, 3, and 4. We get
A2 =


1 1 1 0
0
0 0
−2 −
−
1 1
2 0
0 −2 −1 −1


L1 =


1 0 0 0
1 1 0 0
1 0 1 0
1 0 0 1

 .
The next pivot is π2 = −2 in row 2, and we subtract row 2 from row 4 (and add 0 times row
2 to row 3). We get
A3 =


1 1 1 0
0
0 0
−2 −
−
1 1
2 0
0 0 0 −2


L2 =


1 0 0 0
1 1 0 0
1 0 1 0
1 1 0 1

 .
The next pivot is π3 = −2 in row 3, and since the fourth entry in column 3 is already a zero,
we add 0 times row 3 to row 4. We get
A4 =


1 1 1 0
0
0 0
−2 −
−
1 1
2 0
0 0 0 −2


L3 =


1 0 0 0
1 1 0 0
1 0 1 0
1 1 0 1

 .
The procedure is finished, and we have
L = L3 =


1 0 0 0
1 1 0 0
1 0 1 0
1 1 0 1


U = A4 =


1 1 1 0
0
0 0
0 0 0
−2 −
−
1 1
2 0
−2

 .
It is easy to check that indeed
LU =


1 0 0 0
1 1 0 0
1 0 1 0
1 1 0 1




1 1 1 0
0
0 0
0 0 0
−2 −
−
1 1
2 0
−2

 =


1 1 1 0
1
1 1
1
−
−
1 0 1
1 0
−1 0
−1

 = A.
We now show how to extend the above method to deal with pivoting efficiently. This is
the P A = LU factorization.
8.5. P A = LU FACTORIZATION 263
8.5 P A = LU Factorization
The following easy proposition shows that, in principle, A can be premultiplied by some
permutation matrix P, so that P A can be converted to upper-triangular form without using
any pivoting. Permutations are discussed in some detail in Section 30.3, but for now we
just need this definition. For the precise connection between the notion of permutation (as
discussed in Section 30.3) and permutation matrices, see Problem 8.16.
Definition 8.3. A permutation matrix is a square matrix that has a single 1 in every row
and every column and zeros everywhere else.
It is shown in Section 30.3 that every permutation matrix is a product of transposition
matrices (the P(i, k)s), and that P is invertible with inverse P
> .
Proposition 8.4. Let A be an invertible n × n-matrix. There is some permutation matrix
P so that (P A)(1 : k, 1 : k) is invertible for k = 1, . . . , n.
Proof. The case n = 1 is trivial, and so is the case n = 2 (we swap the rows if necessary). If
n ≥ 3, we proceed by induction. Since A is invertible, its columns are linearly independent;
in particular, its first n − 1 columns are also linearly independent. Delete the last column of
A. Since the remaining n − 1 columns are linearly independent, there are also n − 1 linearly
independent rows in the corresponding n × (n − 1) matrix. Thus, there is a permutation
of these n rows so that the (n − 1) × (n − 1) matrix consisting of the first n − 1 rows is
invertible. But then there is a corresponding permutation matrix P1, so that the first n − 1
rows and columns of P1A form an invertible matrix A0 . Applying the induction hypothesis
to the (n − 1) × (n − 1) matrix A0 , we see that there some permutation matrix P2 (leaving
the nth row fixed), so that (P2P1A)(1 : k, 1 : k) is invertible, for k = 1, . . . , n − 1. Since A
is invertible in the first place and P1 and P2 are invertible, P1P2A is also invertible, and we
are done.
Remark: One can also prove Proposition 8.4 using a clever reordering of the Gaussian
elimination steps suggested by Trefethen and Bau [176] (Lecture 21). Indeed, we know that
if A is invertible, then there are permutation matrices Pi and products of elementary matrices
Ei
, so that
An = En−1Pn−1 · · · E2P2E1P1A,
where U = An is upper-triangular. For example, when n = 4, we have E3P3E2P2E1P1A = U.
We can define new matrices E1
0
, E2
0
, E3
0 which are still products of elementary matrices so
that we have
E3
0E2
0E1
0P3P2P1A = U.
Indeed, if we let E3
0 = E3, E2
0 = P3E2P3
−1
, and E1
0 = P3P2E1P2
−1P3
−1
, we easily verify that
each Ek
0
is a product of elementary matrices and that
E3
0E2
0E1
0P3P2P1 = E3(P3E2P3
−1
)(P3P2E1P2
−1P3
−1
)P3P2P1 = E3P3E2P2E1P1.
264 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM
It can also be proven that E1
0
, E2
0
