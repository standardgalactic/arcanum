w = −X

µ
λ

, (∗w)
2Ks = λ
2Ksξ = µ
1
>p λ = 1
>q µ
1
>p λ + 1
>q µ = ν + γ.
The last two equations are identical to the last two equations obtained in Problem
(SVMs2
0 ). We can use the other equations to obtain the following expression for the dual
function G(λ, µ, γ),
G(λ, µ, γ) = −
1
4Ks
(λ
> λ + µ
> µ) −
1
2
￾
λ
> µ
>
 X
> X

µ
λ

= −
1
2
￾
λ
> µ
>

 X
> X +
2K
1
s
Ip+q
 
µ
λ

.
Consequently the dual program is equivalent to the minimization program
54.13. SOFT MARGIN SVM; (SVMs4) 2001
Dual of the Soft margin SVM (SVMs4):
minimize
1
2
￾
λ
> µ
>

 X
> X +
2K
1
s
Ip+q
 
µ
λ

subject to
p
X
i=1
λi −
q
X
j=1
µj = 0
p
X
i=1
λi +
q
X
j=1
µj ≥ ν
λi ≥ 0, i = 1, . . . , p
µj ≥ 0, j = 1, . . . , q.
The above program is similar to the program that was obtained for Problem (SVMs2
0 )
but the matrix X> X is replaced by the matrix X> X +(1/2Ks)Ip+q, which is positive definite
since Ks > 0, and also the inequalities λi ≤ Ks and µj ≤ Ks no longer hold.
It is shown in Section 54.14 how the dual program is solved using ADMM from Section
52.6. If the primal problem is solvable, this yields solutions for λ and µ. We obtain w from
λ and µ, as in Problem (SVMs2
0 ); namely,
w = −X

µ
λ

=
p
X
i=1
λiui −
q
X
j=1
µjvj
.
Since the variables  i and ξj are not restricted to be nonnegative we no longer have
complementary slackness conditions involving them, but we know that

=
λ
2Ks
, ξ =
µ
2Ks
.
Also since the constraints
p
X
i=1
λi ≥
ν
2
and
q
X
j=1
µj ≥
ν
2
imply that there is some i0 such that λi0 > 0 and some j0 such that µj0 > 0, we have  i0 > 0
and ξj0 > 0, which means that at least two points are misclassified, so Problem (SVMs4)
should only be used when the sets {ui} and {vj} are not linearly separable.
Because  i = λi/(2Ks), ξj = µj/(2Ks), and there is no upper bound Ks on λi and µj
,
the classification of the points is simpler than in the previous cases.
(1) If λi = 0, then  i = 0 and the inequality w
> ui − b − η ≥ 0 holds. If equality holds then
ui
is a support vector on the blue margin (the hyperplane Hw,b+η). Otherwise ui
is
2002 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
in the blue open half-space bounded by the margin hyperplane Hw,b+η (not containing
the separating hyperplane Hw,b). See Figure 54.20.
Similarly, if µj = 0, then ξj = 0 and the inequality −w
> vj + b − η ≥ holds. If
equality holds then vj
is a support vector on the red margin (the hyperplane Hw,b−η).
Otherwise vj
is in the red open half-space bounded by the margin hyperplane Hw,b−η
(not containing the separating hyperplane Hw,b). See Figure 54.20.
v
w x - b = 0 T
w x - b + T η = 0
w x - b - T η = 0
u i
єi = 0
i
j ξ
j
 j
= 0
λ = 0
μ = 0
Correctly classified on blue margin
1
ui
2
1
Correctly classified on red margin vj
2
Figure 54.20: When λi = 0, ui
is correctly classified on or outside the blue margin. When
µj = 0, vj
is correctly classified on or outside outside the red margin.
(2) If λi > 0, then  i = λi/(2Ks) > 0. The corresponding constraint is active, so we have
w
> ui − b = η −  i
.
If  i ≤ η, then the points ui
is inside the slab bounded by the blue margin hyperplane
Hw,b+η and the separating hyperplane Hw,b. If  i > η, then the point ui belongs to the
open half-space bounded by the separating hyperplane and containing the red margin
hyperplane (the red side); it is misclassified. See Figure 54.21.
Similarly, if µj > 0, then ξj = µj/(2Ks) > 0. The corresponding constraint is active,
so we have
−w
> vj + b = η − ξj
.
If ξj ≤ η, then the points vj
is inside the slab bounded by the red margin hyperplane
Hw,b−η and the separating hyperplane Hw,b. If ξj > η, then the point vj belongs to the
open half-space bounded by the separating hyperplane and containing the blue margin
hyperplane (the blue side); it is misclassified. See Figure 54.21.
We can use the fact that the duality gap is 0 to find η. We have
1
2
w
> w − νη + Ks(
>  + ξ
> ξ) = −
1
2
￾
λ
> µ
>

 X
> X +
2K
1
s
Ip+q
 
µ
λ

,
54.13. SOFT MARGIN SVM; (SVMs4) 2003
v
w x - b = 0 T
w x - b + T η = 0
w x - b - T η = 0
ui v
w x - b = 0 T
w x - b + T η = 0
w x - b - T η = 0
u i
v
w x - b = 0 T
w x - b + T η = 0
w x - b - T η= 0
ui
(2)
λ i
λ i Є < η
j
μ 
j
i
j
ξ < η
vj Є = i η
λ i > 0
μj
 
ξ = j η
(1) Correctly classified in slab
Misclassified vj ξ > η
Є > i η
j
μj
 
> 0
> 0
> 0
> 0
> 0
Figure 54.21: The classification of points for SVMs4 when the Lagrange multipliers are
positive. The left illustration of Figure (1) is when ui
is inside the margin yet still on the
correct side of the separating hyperplane w
> x − b = 0. Similarly, vj
is inside the margin on
the correct side of the separating hyperplane. The right illustration depicts ui and vj on the
separating hyperplane. Figure (2) illustrations a misclassification of ui and vj
.
and since
w = −X

µ
λ

we get
νη = Ks(
>  + ξ
> ξ) + ￾ λ
> µ
>

 X
> X +
4K
1
s
Ip+q
 
µ
λ

=
￾ λ
> µ
>

 X
> X +
2K
1
s
Ip+q
 
µ
λ

.
The above confirms that at optimality we have η ≥ 0.
Remark: If we do not assume that Ks = 1/(p+q), then the above formula must be replaced
by
(p + q)Ksνη =
￾ λ
> µ
>

 X
> X +
2K
1
s
Ip+q
 
µ
λ

.
Since η is determined independently of the existence of support vectors, the margin
hyperplane Hw,b+η may not contain any point ui and the margin hyperplane Hw,b−η may not
contain any point vj
.
We can solve for b using some active constraint corresponding to any i0 such that λi0 > 0
and any j0 such that µj0 > 0 (by a previous remark, the constraints imply that such i0 and
2004 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
j0 must exist). To improve numerical stability we average over the following sets of indices.
Let Iλ and Iµ be the set of indices given by
Iλ = {i ∈ {1, . . . , p} | λi > 0}
Iµ = {j ∈ {1, . . . , q} | µj > 0},
and let pm = |Iλ| and qm = |Iµ|. We obtain the formula
b =

w
>

X
i∈Iλ
ui
 /pm +

X
j∈Iµ
vj
 /qm
 +

X
i∈Iλ

i
 /pm −

X
j∈Iµ
ξj
 /qm

 /2.
We now prove that for a fixed Ks, the solution to Problem (SVMs4) is unique and
independent of the value of ν.
Theorem 54.8. For Ks and ν fixed, if Problem (SVMs4) succeeds, then it has a unique solu￾tion. If Problem (SVMs4) succeeds and returns (λ, µ, η, w, b) for the value ν and (λ
κ
, µκ
, ηκ
,
w
κ
, b
κ
) for the value κν with κ > 0, then
λ
κ = κλ, µκ = κµ, ηκ = κη, wκ = κw, bκ = κb.
As a consequence, δ = η/ k wk = η
κ/ k w
κk = δ
κ
, and the hyperplanes Hw,b, Hw,b+η and Hw,b−η
are independent of ν.
Proof. We already observed that for an optimal solution with η > 0, we have γ = 0. This
means that (λ, µ) is a solution of the problem
minimize
1
2
￾
λ
> µ
>

 X
> X +
2K
1
s
Ip+q
 
µ
λ

subject to
p
X
i=1
λi −
q
X
j=1
µj = 0
p
X
i=1
λi +
q
X
j=1
µj = ν
λi ≥ 0, i = 1, . . . , p
µj ≥ 0, j = 1, . . . , q.
Since Ks > 0 and X> X is symmetric positive semidefinite, the matrix
P = X> X + 2K
1
s
Ip+q is symmetric positive definite. Let Ω = R
p+q and let U be the convex
set given by
U =
(

µ
λ

∈ R
p
+
+q

 


 
1
>p −1
>q
1
>p 1
>q
!

µ
λ

=

(p + q
0
)Ksν

)
.
54.13. SOFT MARGIN SVM; (SVMs4) 2005
Since the matrix P is symmetric positive definite, the functional
F(λ, µ) = −G(λ, µ) = 1
2
￾
λ
> µ
>
 P

µ
λ

is strictly convex and U is convex, so by Theorem 40.13(2,4), if it has a minimum, then it is
unique. Consider the convex set
U
κ =
(

µ
λ

∈ R
p
+
+q

 


 
1
>p −1
>q
1
>p 1
>q
!

µ
λ

=

(p + q
0
)Ksκν )
.
Observe that
κU =
(

κµ
κλ
∈ R
p
+
+q

 


 
1
>p −1
>q
1
>p 1
>q
!

κµ
κλ
=

(p + q
0
)Ksκν )
= U
κ
.
By Theorem 40.13(3), (λ, µ) ∈ U is a minimum of F over U iff
dFλ,µ 
λ
0 − λ
µ
0 − µ

≥ 0 for all 
µ
λ
0
0

∈ U.
Since
dFλ,µ 
λ
0 − λ
µ
0 − µ

=
￾ λ
> µ
>
 P

λ
0 − λ
µ
0 − µ

the above conditions are equivalent to
￾
λ
> µ
>
 P

λ
0 − λ
µ
0 − µ

≥ 0
 
1
>p −1
>q
1
>p 1
>q
!

µ
λ

=

(p + q
0
)Ksν

λ, λ0 ∈ R
p
+, µ, µ0 ∈ R
q
+.
Since κ > 0, by multiplying the above inequality by κ
2 and the equations by κ, the following
conditions hold:
￾
κλ> κµ>  P

κλ0 − κλ
κµ0 − κµ
≥ 0
 
1
>p −1
>q
1
>p 1
>q
!

κµ
κλ
=

(p + q
0
)Ksκν
κλ, κλ0 ∈ R
p
+, κµ, κµ0 ∈ R
q
+.
By Theorem 40.13(3), (κλ, κµ) ∈ U
κ
is a minimum of F over U
κ
, and because F is strictly
convex and U
κ
is convex, if F has a minimum over U
κ
, then (κλ, κµ) ∈ U
κ
is the unique
minimum. Therefore, λ
κ = κλ, µκ = κµ.
2006 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
Since w is given by the equation
w = −X

µ
λ

and since we just showed that λ
κ = κλ, µκ = κµ, we deduce that w
κ = κw.
We showed earlier that η is given by the equation
(p + q)Ksνη =
￾ λ
> µ
>

 X
> X +
2K
1
s
Ip+q
 
µ
λ

.
If we replace ν by κν, since λ is replaced by κλ and µ by κν, we see that η
κ = κη. Finally,
b is given by the equation
b =
w
> (ui0 + vj0
) +  i0 − ξj0
2
for and i0 such that λi0 > 0 and any j0 such that µj0 > 0. If λ is replaced by κλ and µ by
κµ, since  = λ/(2Ks) and ξ = µ/(2Ks), we see that  is replaced by κ and ξ by κξ, so
b
κ = κb.
Since w
κ = κw and η
κ = κη we obtain δ = η/ k wk = η
κ/ k w
κk = δ
κ
. Since w
κ = κw,
η
κ = κη and b
κ = κb, the normalized equations of the hyperplanes Hw,b, Hw,b+η and Hw,b−η
(obtained by dividing by k wk ) are all identical, so the hyperplanes Hw,b, Hw,b+η and Hw,b−η
are independent of ν.
The width of the slab is controlled by K. The larger K is the smaller is the width of
the slab. Theoretically, since this method does not rely on support vectors to compute b,
it cannot fail if a solution exists, but in practice the quadratic solver does not converge for
values of K that are too large. However, the method handles very small values of K, which
can yield slabs of excessive width.
The “kernelized” version of Problem (SVMs4) is the following:
Soft margin kernel SVM (SVMs4):
minimize
1
2
h
w, wi − νη + Ks(
>  + ξ
> ξ)
subject to
h
w, ϕ(ui)i − b ≥ η −  i
, i = 1, . . . , p
− hw, ϕ(vj )i + b ≥ η − ξj
, j = 1, . . . , q
η ≥ 0,
with Ks = 1/(p + q).
By going over the derivation of the dual program, we obtain
54.14. SOLVING SVM (SVMs4) USING ADMM 2007
Dual of the Soft margin kernel SVM (SVMs4):
minimize
1
2
￾
λ
> µ
>

 K +
2K
1
s
Ip+q
 
µ
λ

subject to
p
X
i=1
λi −
q
X
j=1
µj = 0
p
X
i=1
λi +
q
X
j=1
µj ≥ ν
λi ≥ 0, i = 1, . . . , p
µj ≥ 0, j = 1, . . . , q,
where K is the kernel matrix of Section 54.1. Then w, b, and f(x) are obtained exactly as
in Section 54.5.
54.14 Solving SVM (SVMs4) Using ADMM
In order to solve (SVMs4) using ADMM we need to write the matrix corresponding to the
constraints in equational form,
p
X
i=1
λi −
q
X
j=1
µj = 0
p
X
i=1
λi +
q
X
j=1
µj − γ = Km,
with Km = (p + q)Ksν. This is the 2 × (p + q + 1) matrix A given by
A =
 
1
>p −1
>q
0
1
>p 1
>q −1
!
.
We leave it as an exercise to prove that A has rank 2. The right-hand side is
c =
 K
0
m

.
The symmetric positive semidefinite (p+q)×(p+q) matrix P defining the quadratic functional
is
P = X
> X +
2K
1
s
Ip+q, with X =
￾ −u1 · · · −up v1 · · · vq
 ,
and
q = 0p+q.
2008 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
Since there are p + q + 1 Lagrange multipliers (λ, µ, γ), the (p + q) × (p + q) matrix P must
be augmented with zero’s to make it a (p + q + 1) × (p + q + 1) matrix Pa given by
Pa =

X> X 0p+q
0
>p+q
0

,
and similarly q is augmented with zeros as the vector qa = 0p+q+1.
As in Section 54.8, since η ≥ 0 for an optimal solution, we can drop the constraint η ≥ 0
from the primal problem. In this case, there are p + q Lagrange multipliers (λ, µ). It is easy
to see that the objective function of the dual is unchanged and the set of constraints is
p
X
i=1
λi −
q
X
j=1
µj = 0
p
X
i=1
λi +
q
X
j=1
µj = Km,
with Km = (p + q)Ksν. The matrix corresponding to the above equations is the 2 × (p + q)
matrix A2 given by
A2 =
 
1
>p −1
>q
1
>p 1
>q
!
.
We leave it as an exercise to prove that A2 has rank 2. The right-hand side is
c2 =
 K
0
m

.
The symmetric positive semidefinite (p+q)×(p+q) matrix P defining the quadratic functional
is
P = X
> X +
2K
1
s
Ip+q, with X =
￾ −u1 · · · −up v1 · · · vq
 ,
and
q = 0p+q.
Since there are p + q Lagrange multipliers (λ, µ), the (p + q) × (p + q) matrix P need not be
augmented with zero’s, so P2a = P and similarly q2a = 0p+q.
We ran our Matlab implementation of the above version of (SVMs4) on the data set of
Section 54.12. Since the value of ν is irrelevant, we picked ν = 1. First we ran our program
with K = 190; see Figure 54.22. We have pm = 23 and qm = 18. The program does not
converge for K ≥ 200.
54.15. SOFT MARGIN SVM; (SVMs5) 2009
-30 -20 -10 0 10 20 30 40
-30
-20
-10
0
10
20
30
Figure 54.22: Running (SVMs4) on two sets of 30 points; K = 190.
Our second run was made with K = 1/12000; see Figure 54.23. We have pm = 30 and
qm = 30 and we see that the width of the slab is a bit excessive. This example demonstrates
that the margin lines need not contain data points.
54.15 Soft Margin SVM; (SVMs5)
In this section we consider the version of Problem (SVMs4) in which we add the term (1/2)b
2
to the objective function. We also drop the constraint η ≥ 0 which is redundant.
Soft margin SVM (SVMs5):
minimize
1
2
w
> w +
1
2
b
2 + (p + q)Ks
 −νη +
p +
1
q
(
>  + ξ
> ξ)

subject to
w
> ui − b ≥ η −  i
, i = 1, . . . , p
− w
> vj + b ≥ η − ξj
, j = 1, . . . , q,
where ν and Ks are two given positive constants. As we saw earlier, it is convenient to pick
Ks = 1/(p + q). When writing a computer program, it is preferable to assume that Ks is
arbitrary. In this case ν must be replaced by (p + q)Ksν in all the formulae.
One of the advantages of this methods is that  is determined by λ, ξ is determined by
µ (as in (SVMs4)), and both η and b determined by λ and µ. As the previous method, this
2010 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
-30 -20 -10 0 10 20 30 40
-30
-20
-10
0
10
20
30
Figure 54.23: Running (SVMs4) on two sets of 30 points; K = 1/12000.
method does not require support vectors to compute b. We can omit the constraint η ≥ 0,
because for an optimal solution it can be shown using duality that η ≥ 0.
A drawback of Program (SVMs5) is that for fixed Ks, the quantity δ = η/ k wk and the
hyperplanes Hw,b, Hw,b+η and Hw,b−η are independent of ν. This will be shown in Theorem
54.9. Thus this method is less flexible than (SVMs2
0 ) and (SVMs3).
The Lagrangian is given by
L(w, , ξ, b, η, λ, µ) = 1
2
w
> w +
2
1
b
2 − νη + Ks(
>  + ξ
> ξ) + w
> X

µ
λ

− 
> λ − ξ
> µ + b(1
>p λ − 1
>q µ) + η(1
>p λ + 1
>q µ)
=
1
2
w
> w + w
> X

µ
λ

