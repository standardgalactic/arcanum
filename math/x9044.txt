w = âˆ’X

Âµ
Î»

, (âˆ—w)
2Ks = Î»
2KsÎ¾ = Âµ
1
>p Î» = 1
>q Âµ
1
>p Î» + 1
>q Âµ = Î½ + Î³.
The last two equations are identical to the last two equations obtained in Problem
(SVMs2
0 ). We can use the other equations to obtain the following expression for the dual
function G(Î», Âµ, Î³),
G(Î», Âµ, Î³) = âˆ’
1
4Ks
(Î»
> Î» + Âµ
> Âµ) âˆ’
1
2
ï¿¾
Î»
> Âµ
>
 X
> X

Âµ
Î»

= âˆ’
1
2
ï¿¾
Î»
> Âµ
>

 X
> X +
2K
1
s
Ip+q
 
Âµ
Î»

.
Consequently the dual program is equivalent to the minimization program
54.13. SOFT MARGIN SVM; (SVMs4) 2001
Dual of the Soft margin SVM (SVMs4):
minimize
1
2
ï¿¾
Î»
> Âµ
>

 X
> X +
2K
1
s
Ip+q
 
Âµ
Î»

subject to
p
X
i=1
Î»i âˆ’
q
X
j=1
Âµj = 0
p
X
i=1
Î»i +
q
X
j=1
Âµj â‰¥ Î½
Î»i â‰¥ 0, i = 1, . . . , p
Âµj â‰¥ 0, j = 1, . . . , q.
The above program is similar to the program that was obtained for Problem (SVMs2
0 )
but the matrix X> X is replaced by the matrix X> X +(1/2Ks)Ip+q, which is positive definite
since Ks > 0, and also the inequalities Î»i â‰¤ Ks and Âµj â‰¤ Ks no longer hold.
It is shown in Section 54.14 how the dual program is solved using ADMM from Section
52.6. If the primal problem is solvable, this yields solutions for Î» and Âµ. We obtain w from
Î» and Âµ, as in Problem (SVMs2
0 ); namely,
w = âˆ’X

Âµ
Î»

=
p
X
i=1
Î»iui âˆ’
q
X
j=1
Âµjvj
.
Since the variables  i and Î¾j are not restricted to be nonnegative we no longer have
complementary slackness conditions involving them, but we know that

=
Î»
2Ks
, Î¾ =
Âµ
2Ks
.
Also since the constraints
p
X
i=1
Î»i â‰¥
Î½
2
and
q
X
j=1
Âµj â‰¥
Î½
2
imply that there is some i0 such that Î»i0 > 0 and some j0 such that Âµj0 > 0, we have  i0 > 0
and Î¾j0 > 0, which means that at least two points are misclassified, so Problem (SVMs4)
should only be used when the sets {ui} and {vj} are not linearly separable.
Because  i = Î»i/(2Ks), Î¾j = Âµj/(2Ks), and there is no upper bound Ks on Î»i and Âµj
,
the classification of the points is simpler than in the previous cases.
(1) If Î»i = 0, then  i = 0 and the inequality w
> ui âˆ’ b âˆ’ Î· â‰¥ 0 holds. If equality holds then
ui
is a support vector on the blue margin (the hyperplane Hw,b+Î·). Otherwise ui
is
2002 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
in the blue open half-space bounded by the margin hyperplane Hw,b+Î· (not containing
the separating hyperplane Hw,b). See Figure 54.20.
Similarly, if Âµj = 0, then Î¾j = 0 and the inequality âˆ’w
> vj + b âˆ’ Î· â‰¥ holds. If
equality holds then vj
is a support vector on the red margin (the hyperplane Hw,bâˆ’Î·).
Otherwise vj
is in the red open half-space bounded by the margin hyperplane Hw,bâˆ’Î·
(not containing the separating hyperplane Hw,b). See Figure 54.20.
v
w x - b = 0 T
w x - b + T Î· = 0
w x - b - T Î· = 0
u i
Ñ”i = 0
i
j Î¾
j
 j
= 0
Î» = 0
Î¼ = 0
Correctly classified on blue margin
1
ui
2
1
Correctly classified on red margin vj
2
Figure 54.20: When Î»i = 0, ui
is correctly classified on or outside the blue margin. When
Âµj = 0, vj
is correctly classified on or outside outside the red margin.
(2) If Î»i > 0, then  i = Î»i/(2Ks) > 0. The corresponding constraint is active, so we have
w
> ui âˆ’ b = Î· âˆ’  i
.
If  i â‰¤ Î·, then the points ui
is inside the slab bounded by the blue margin hyperplane
Hw,b+Î· and the separating hyperplane Hw,b. If  i > Î·, then the point ui belongs to the
open half-space bounded by the separating hyperplane and containing the red margin
hyperplane (the red side); it is misclassified. See Figure 54.21.
Similarly, if Âµj > 0, then Î¾j = Âµj/(2Ks) > 0. The corresponding constraint is active,
so we have
âˆ’w
> vj + b = Î· âˆ’ Î¾j
.
If Î¾j â‰¤ Î·, then the points vj
is inside the slab bounded by the red margin hyperplane
Hw,bâˆ’Î· and the separating hyperplane Hw,b. If Î¾j > Î·, then the point vj belongs to the
open half-space bounded by the separating hyperplane and containing the blue margin
hyperplane (the blue side); it is misclassified. See Figure 54.21.
We can use the fact that the duality gap is 0 to find Î·. We have
1
2
w
> w âˆ’ Î½Î· + Ks(
>  + Î¾
> Î¾) = âˆ’
1
2
ï¿¾
Î»
> Âµ
>

 X
> X +
2K
1
s
Ip+q
 
Âµ
Î»

,
54.13. SOFT MARGIN SVM; (SVMs4) 2003
v
w x - b = 0 T
w x - b + T Î· = 0
w x - b - T Î· = 0
ui v
w x - b = 0 T
w x - b + T Î· = 0
w x - b - T Î· = 0
u i
v
w x - b = 0 T
w x - b + T Î· = 0
w x - b - T Î·= 0
ui
(2)
Î» i
Î» i Ð„ < Î·
j
Î¼ 
j
i
j
Î¾ < Î·
vj Ð„ = i Î·
Î» i > 0
Î¼j
 
Î¾ = j Î·
(1) Correctly classified in slab
Misclassified vj Î¾ > Î·
Ð„ > i Î·
j
Î¼j
 
> 0
> 0
> 0
> 0
> 0
Figure 54.21: The classification of points for SVMs4 when the Lagrange multipliers are
positive. The left illustration of Figure (1) is when ui
is inside the margin yet still on the
correct side of the separating hyperplane w
> x âˆ’ b = 0. Similarly, vj
is inside the margin on
the correct side of the separating hyperplane. The right illustration depicts ui and vj on the
separating hyperplane. Figure (2) illustrations a misclassification of ui and vj
.
and since
w = âˆ’X

Âµ
Î»

we get
Î½Î· = Ks(
>  + Î¾
> Î¾) + ï¿¾ Î»
> Âµ
>

 X
> X +
4K
1
s
Ip+q
 
Âµ
Î»

=
ï¿¾ Î»
> Âµ
>

 X
> X +
2K
1
s
Ip+q
 
Âµ
Î»

.
The above confirms that at optimality we have Î· â‰¥ 0.
Remark: If we do not assume that Ks = 1/(p+q), then the above formula must be replaced
by
(p + q)KsÎ½Î· =
ï¿¾ Î»
> Âµ
>

 X
> X +
2K
1
s
Ip+q
 
Âµ
Î»

.
Since Î· is determined independently of the existence of support vectors, the margin
hyperplane Hw,b+Î· may not contain any point ui and the margin hyperplane Hw,bâˆ’Î· may not
contain any point vj
.
We can solve for b using some active constraint corresponding to any i0 such that Î»i0 > 0
and any j0 such that Âµj0 > 0 (by a previous remark, the constraints imply that such i0 and
2004 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
j0 must exist). To improve numerical stability we average over the following sets of indices.
Let IÎ» and IÂµ be the set of indices given by
IÎ» = {i âˆˆ {1, . . . , p} | Î»i > 0}
IÂµ = {j âˆˆ {1, . . . , q} | Âµj > 0},
and let pm = |IÎ»| and qm = |IÂµ|. We obtain the formula
b =
ï£«
ï£­w
>

X
iâˆˆIÎ»
ui
 /pm +

X
jâˆˆIÂµ
vj
 /qm
 +

X
iâˆˆIÎ»

i
 /pm âˆ’

X
jâˆˆIÂµ
Î¾j
 /qm
ï£¶
ï£¸ /2.
We now prove that for a fixed Ks, the solution to Problem (SVMs4) is unique and
independent of the value of Î½.
Theorem 54.8. For Ks and Î½ fixed, if Problem (SVMs4) succeeds, then it has a unique soluï¿¾tion. If Problem (SVMs4) succeeds and returns (Î», Âµ, Î·, w, b) for the value Î½ and (Î»
Îº
, ÂµÎº
, Î·Îº
,
w
Îº
, b
Îº
) for the value ÎºÎ½ with Îº > 0, then
Î»
Îº = ÎºÎ», ÂµÎº = ÎºÂµ, Î·Îº = ÎºÎ·, wÎº = Îºw, bÎº = Îºb.
As a consequence, Î´ = Î·/ k wk = Î·
Îº/ k w
Îºk = Î´
Îº
, and the hyperplanes Hw,b, Hw,b+Î· and Hw,bâˆ’Î·
are independent of Î½.
Proof. We already observed that for an optimal solution with Î· > 0, we have Î³ = 0. This
means that (Î», Âµ) is a solution of the problem
minimize
1
2
ï¿¾
Î»
> Âµ
>

 X
> X +
2K
1
s
Ip+q
 
Âµ
Î»

subject to
p
X
i=1
Î»i âˆ’
q
X
j=1
Âµj = 0
p
X
i=1
Î»i +
q
X
j=1
Âµj = Î½
Î»i â‰¥ 0, i = 1, . . . , p
Âµj â‰¥ 0, j = 1, . . . , q.
Since Ks > 0 and X> X is symmetric positive semidefinite, the matrix
P = X> X + 2K
1
s
Ip+q is symmetric positive definite. Let â„¦ = R
p+q and let U be the convex
set given by
U =
(

Âµ
Î»

âˆˆ R
p
+
+q

 


 
1
>p âˆ’1
>q
1
>p 1
>q
!

Âµ
Î»

=

(p + q
0
)KsÎ½

)
.
54.13. SOFT MARGIN SVM; (SVMs4) 2005
Since the matrix P is symmetric positive definite, the functional
F(Î», Âµ) = âˆ’G(Î», Âµ) = 1
2
ï¿¾
Î»
> Âµ
>
 P

Âµ
Î»

is strictly convex and U is convex, so by Theorem 40.13(2,4), if it has a minimum, then it is
unique. Consider the convex set
U
Îº =
(

Âµ
Î»

âˆˆ R
p
+
+q

 


 
1
>p âˆ’1
>q
1
>p 1
>q
!

Âµ
Î»

=

(p + q
0
)KsÎºÎ½ )
.
Observe that
ÎºU =
(

ÎºÂµ
ÎºÎ»
âˆˆ R
p
+
+q

 


 
1
>p âˆ’1
>q
1
>p 1
>q
!

ÎºÂµ
ÎºÎ»
=

(p + q
0
)KsÎºÎ½ )
= U
Îº
.
By Theorem 40.13(3), (Î», Âµ) âˆˆ U is a minimum of F over U iff
dFÎ»,Âµ 
Î»
0 âˆ’ Î»
Âµ
0 âˆ’ Âµ

â‰¥ 0 for all 
Âµ
Î»
0
0

âˆˆ U.
Since
dFÎ»,Âµ 
Î»
0 âˆ’ Î»
Âµ
0 âˆ’ Âµ

=
ï¿¾ Î»
> Âµ
>
 P

Î»
0 âˆ’ Î»
Âµ
0 âˆ’ Âµ

the above conditions are equivalent to
ï¿¾
Î»
> Âµ
>
 P

Î»
0 âˆ’ Î»
Âµ
0 âˆ’ Âµ

â‰¥ 0
 
1
>p âˆ’1
>q
1
>p 1
>q
!

Âµ
Î»

=

(p + q
0
)KsÎ½

Î», Î»0 âˆˆ R
p
+, Âµ, Âµ0 âˆˆ R
q
+.
Since Îº > 0, by multiplying the above inequality by Îº
2 and the equations by Îº, the following
conditions hold:
ï¿¾
ÎºÎ»> ÎºÂµ>  P

ÎºÎ»0 âˆ’ ÎºÎ»
ÎºÂµ0 âˆ’ ÎºÂµ
â‰¥ 0
 
1
>p âˆ’1
>q
1
>p 1
>q
!

ÎºÂµ
ÎºÎ»
=

(p + q
0
)KsÎºÎ½
ÎºÎ», ÎºÎ»0 âˆˆ R
p
+, ÎºÂµ, ÎºÂµ0 âˆˆ R
q
+.
By Theorem 40.13(3), (ÎºÎ», ÎºÂµ) âˆˆ U
Îº
is a minimum of F over U
Îº
, and because F is strictly
convex and U
Îº
is convex, if F has a minimum over U
Îº
, then (ÎºÎ», ÎºÂµ) âˆˆ U
Îº
is the unique
minimum. Therefore, Î»
Îº = ÎºÎ», ÂµÎº = ÎºÂµ.
2006 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
Since w is given by the equation
w = âˆ’X

Âµ
Î»

and since we just showed that Î»
Îº = ÎºÎ», ÂµÎº = ÎºÂµ, we deduce that w
Îº = Îºw.
We showed earlier that Î· is given by the equation
(p + q)KsÎ½Î· =
ï¿¾ Î»
> Âµ
>

 X
> X +
2K
1
s
Ip+q
 
Âµ
Î»

.
If we replace Î½ by ÎºÎ½, since Î» is replaced by ÎºÎ» and Âµ by ÎºÎ½, we see that Î·
Îº = ÎºÎ·. Finally,
b is given by the equation
b =
w
> (ui0 + vj0
) +  i0 âˆ’ Î¾j0
2
for and i0 such that Î»i0 > 0 and any j0 such that Âµj0 > 0. If Î» is replaced by ÎºÎ» and Âµ by
ÎºÂµ, since  = Î»/(2Ks) and Î¾ = Âµ/(2Ks), we see that  is replaced by Îº and Î¾ by ÎºÎ¾, so
b
Îº = Îºb.
Since w
Îº = Îºw and Î·
Îº = ÎºÎ· we obtain Î´ = Î·/ k wk = Î·
Îº/ k w
Îºk = Î´
Îº
. Since w
Îº = Îºw,
Î·
Îº = ÎºÎ· and b
Îº = Îºb, the normalized equations of the hyperplanes Hw,b, Hw,b+Î· and Hw,bâˆ’Î·
(obtained by dividing by k wk ) are all identical, so the hyperplanes Hw,b, Hw,b+Î· and Hw,bâˆ’Î·
are independent of Î½.
The width of the slab is controlled by K. The larger K is the smaller is the width of
the slab. Theoretically, since this method does not rely on support vectors to compute b,
it cannot fail if a solution exists, but in practice the quadratic solver does not converge for
values of K that are too large. However, the method handles very small values of K, which
can yield slabs of excessive width.
The â€œkernelizedâ€ version of Problem (SVMs4) is the following:
Soft margin kernel SVM (SVMs4):
minimize
1
2
h
w, wi âˆ’ Î½Î· + Ks(
>  + Î¾
> Î¾)
subject to
h
w, Ï•(ui)i âˆ’ b â‰¥ Î· âˆ’  i
, i = 1, . . . , p
âˆ’ hw, Ï•(vj )i + b â‰¥ Î· âˆ’ Î¾j
, j = 1, . . . , q
Î· â‰¥ 0,
with Ks = 1/(p + q).
By going over the derivation of the dual program, we obtain
54.14. SOLVING SVM (SVMs4) USING ADMM 2007
Dual of the Soft margin kernel SVM (SVMs4):
minimize
1
2
ï¿¾
Î»
> Âµ
>

 K +
2K
1
s
Ip+q
 
Âµ
Î»

subject to
p
X
i=1
Î»i âˆ’
q
X
j=1
Âµj = 0
p
X
i=1
Î»i +
q
X
j=1
Âµj â‰¥ Î½
Î»i â‰¥ 0, i = 1, . . . , p
Âµj â‰¥ 0, j = 1, . . . , q,
where K is the kernel matrix of Section 54.1. Then w, b, and f(x) are obtained exactly as
in Section 54.5.
54.14 Solving SVM (SVMs4) Using ADMM
In order to solve (SVMs4) using ADMM we need to write the matrix corresponding to the
constraints in equational form,
p
X
i=1
Î»i âˆ’
q
X
j=1
Âµj = 0
p
X
i=1
Î»i +
q
X
j=1
Âµj âˆ’ Î³ = Km,
with Km = (p + q)KsÎ½. This is the 2 Ã— (p + q + 1) matrix A given by
A =
 
1
>p âˆ’1
>q
0
1
>p 1
>q âˆ’1
!
.
We leave it as an exercise to prove that A has rank 2. The right-hand side is
c =
 K
0
m

.
The symmetric positive semidefinite (p+q)Ã—(p+q) matrix P defining the quadratic functional
is
P = X
> X +
2K
1
s
Ip+q, with X =
ï¿¾ âˆ’u1 Â· Â· Â· âˆ’up v1 Â· Â· Â· vq
 ,
and
q = 0p+q.
2008 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
Since there are p + q + 1 Lagrange multipliers (Î», Âµ, Î³), the (p + q) Ã— (p + q) matrix P must
be augmented with zeroâ€™s to make it a (p + q + 1) Ã— (p + q + 1) matrix Pa given by
Pa =

X> X 0p+q
0
>p+q
0

,
and similarly q is augmented with zeros as the vector qa = 0p+q+1.
As in Section 54.8, since Î· â‰¥ 0 for an optimal solution, we can drop the constraint Î· â‰¥ 0
from the primal problem. In this case, there are p + q Lagrange multipliers (Î», Âµ). It is easy
to see that the objective function of the dual is unchanged and the set of constraints is
p
X
i=1
Î»i âˆ’
q
X
j=1
Âµj = 0
p
X
i=1
Î»i +
q
X
j=1
Âµj = Km,
with Km = (p + q)KsÎ½. The matrix corresponding to the above equations is the 2 Ã— (p + q)
matrix A2 given by
A2 =
 
1
>p âˆ’1
>q
1
>p 1
>q
!
.
We leave it as an exercise to prove that A2 has rank 2. The right-hand side is
c2 =
 K
0
m

.
The symmetric positive semidefinite (p+q)Ã—(p+q) matrix P defining the quadratic functional
is
P = X
> X +
2K
1
s
Ip+q, with X =
ï¿¾ âˆ’u1 Â· Â· Â· âˆ’up v1 Â· Â· Â· vq
 ,
and
q = 0p+q.
Since there are p + q Lagrange multipliers (Î», Âµ), the (p + q) Ã— (p + q) matrix P need not be
augmented with zeroâ€™s, so P2a = P and similarly q2a = 0p+q.
We ran our Matlab implementation of the above version of (SVMs4) on the data set of
Section 54.12. Since the value of Î½ is irrelevant, we picked Î½ = 1. First we ran our program
with K = 190; see Figure 54.22. We have pm = 23 and qm = 18. The program does not
converge for K â‰¥ 200.
54.15. SOFT MARGIN SVM; (SVMs5) 2009
-30 -20 -10 0 10 20 30 40
-30
-20
-10
0
10
20
30
Figure 54.22: Running (SVMs4) on two sets of 30 points; K = 190.
Our second run was made with K = 1/12000; see Figure 54.23. We have pm = 30 and
qm = 30 and we see that the width of the slab is a bit excessive. This example demonstrates
that the margin lines need not contain data points.
54.15 Soft Margin SVM; (SVMs5)
In this section we consider the version of Problem (SVMs4) in which we add the term (1/2)b
2
to the objective function. We also drop the constraint Î· â‰¥ 0 which is redundant.
Soft margin SVM (SVMs5):
minimize
1
2
w
> w +
1
2
b
2 + (p + q)Ks
 âˆ’Î½Î· +
p +
1
q
(
>  + Î¾
> Î¾)

subject to
w
> ui âˆ’ b â‰¥ Î· âˆ’  i
, i = 1, . . . , p
âˆ’ w
> vj + b â‰¥ Î· âˆ’ Î¾j
, j = 1, . . . , q,
where Î½ and Ks are two given positive constants. As we saw earlier, it is convenient to pick
Ks = 1/(p + q). When writing a computer program, it is preferable to assume that Ks is
arbitrary. In this case Î½ must be replaced by (p + q)KsÎ½ in all the formulae.
One of the advantages of this methods is that  is determined by Î», Î¾ is determined by
Âµ (as in (SVMs4)), and both Î· and b determined by Î» and Âµ. As the previous method, this
2010 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
-30 -20 -10 0 10 20 30 40
-30
-20
-10
0
10
20
30
Figure 54.23: Running (SVMs4) on two sets of 30 points; K = 1/12000.
method does not require support vectors to compute b. We can omit the constraint Î· â‰¥ 0,
because for an optimal solution it can be shown using duality that Î· â‰¥ 0.
A drawback of Program (SVMs5) is that for fixed Ks, the quantity Î´ = Î·/ k wk and the
hyperplanes Hw,b, Hw,b+Î· and Hw,bâˆ’Î· are independent of Î½. This will be shown in Theorem
54.9. Thus this method is less flexible than (SVMs2
0 ) and (SVMs3).
The Lagrangian is given by
L(w, , Î¾, b, Î·, Î», Âµ) = 1
2
w
> w +
2
1
b
2 âˆ’ Î½Î· + Ks(
>  + Î¾
> Î¾) + w
> X

Âµ
Î»

âˆ’ 
> Î» âˆ’ Î¾
> Âµ + b(1
>p Î» âˆ’ 1
>q Âµ) + Î·(1
>p Î» + 1
>q Âµ)
=
1
2
w
> w + w
> X

Âµ
Î»

