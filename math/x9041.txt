is in
the closed half space on the blue side bounded by the blue margin hyperplane Hw,b+η.
If λi > 0, then by (1) and (2) above, the point ui
is on the blue margin.
Similarly, if ξj = 0, then the point vj
is correctly classified. If µj = 0, then vj
is in the
closed half space on the red side bounded by the red margin hyperplane Hw,b−η, and if
µj > 0, then the point vj
is on the red margin.
Also observe that if λi > 0, then ui
is in the closed half space bounded by the blue hyper￾plane Hw,b+η and containing the separating hyperplane Hw,b (including the blue hyperplane
Hw,b+η).
Similarly, if µj > 0, then vj
is in the closed half space bounded by the red hyperplane
Hw,b+η and containing the separating hyperplane Hw,b (including the red hyperplane Hw,b+η).
Definition 54.3. Vectors ui such that λi > 0 and vectors vj such that µj > 0 are said to
have margin at most δ. The sets of indices associated with these vectors are denoted by
Iλ>0 = {i ∈ {1, . . . , p} | λi > 0}
Iµ>0 = {j ∈ {1, . . . , q} | µj > 0}.
We denote their cardinalities by pm = |Iλ>0| and qm = |Iµ>0|.
Vectors ui such that  i > 0 and vectors vj such that ξj > 0 are said to strictly fail the
margin. The corresponding sets of indices are denoted by
Eλ = {i ∈ {1, . . . , p} |  i > 0}
Eµ = {j ∈ {1, . . . , q} | ξj > 0}.
We write psf = |Eλ| and qsf = |Eµ|.
1964 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
We have the inclusions Eλ ⊆ Kλ and Eµ ⊆ Kµ. The difference between the first sets and
the second sets is that the second sets may contain support vectors of type 2 such that λi = Ks
and  i = 0, or µj = Ks and ξj = 0. We also have the equations Iλ ∪ (Kλ − Eλ) ∪ Eλ = Iλ>0
and Iµ ∪ (Kµ − Eµ) ∪ Eµ = Iµ>0, and the inequalities psf ≤ pf ≤ pm and qsf ≤ qf ≤ qm.
The blue points ui of index i ∈ Iλ>0 are classified as follows:
(1) If i ∈ Iλ, then ui
is a support vector of type 1 (λi < Ks).
(2) If i ∈ Kλ − Eλ, then ui
is a support vector of type 2 (λi = Ks).
(3) If i ∈ Eλ, then ui strictly fails the margin, that is  i > 0.
Similarly the red points vj of index j ∈ Iµ>0 are classified as follows:
(1) If j ∈ Iµ, then vj
is a support vector of type 1 (µj < Ks).
(2) If j ∈ Kµ − Eµ, then vj
is a support vector of type 2 (µj = Ks).
(3) If j ∈ Eµ, then vj strictly fails the margin, that is ξj > 0.
Note that pm − pf is the number of blue support vectors of type 1 and qm − qf is the
number of red support vectors of type 1. The remaining blue points ui
for which λi = 0 are
either exceptional support vectors or they are (strictly ) in the open half-space corresponding
to the blue side. Similarly, the remaining red points vj
for which µj = 0 are either exceptional
support vectors or they are (strictly) in the open half-space corresponding to the red side.
It is shown in Section 54.8 how the dual program is solved using ADMM from Section
52.6. If the primal problem is solvable, this yields solutions for λ and µ. Once a solution for
λ and µ is obtained, we have
w = −X

µ
λ

=
p
X
i=1
λiui −
q
X
j=1
µjvj
.
As we said earlier, the hypotheses of Theorem 50.17(2) hold, so if the primal problem
(SVMs2
0 ) has an optimal solution with w 6 = 0, then the dual problem has a solution too, and
the duality gap is zero. Therefore, for optimal solutions we have
L(w, , ξ, b, η, λ, µ, α, β, γ) = G(λ, µ, α, β, γ),
which means that
1
2
w
> w − Kmη + Ks

p
X
i=1

i +
q
X
j=1
ξj
 = −
1
2
￾
λ
> µ
>
 X
> X

µ
λ

,
and since
w = −X

µ
λ

,
54.5. SOFT MARGIN SUPPORT VECTOR MACHINES; (SVMs2
0 ) 1965
we get
1
2
￾
λ
> µ
>
 X
> X

µ
λ

− Kmη + Ks

p
X
i=1

i +
q
X
j=1
ξj
 = −
1
2
￾
λ
> µ
>
 X
> X

µ
λ

,
which yields
η =
Ks
Km

p
X
i=1

i +
q
X
j=1
ξj
 +
K
1
m
￾
λ
> µ
>
 X
> X

µ
λ

. (∗)
Therefore, we confirm that η ≥ 0.
Remarks: Since we proved that if the Primal Problem (SVMs2
0 ) has an optimal solution
with w 6 = 0, then η ≥ 0, one might wonder why the constraint η ≥ 0 was included. If we
delete this constraint, it is easy to see that the only difference is that instead of the equation
1
>p λ + 1
>q µ = Km + γ (∗1)
we obtain the equation
1
>p λ + 1
>q µ = Km. (∗2)
If η > 0, then by complementary slackness γ = 0, in which case (∗1) and (∗2) are equivalent.
But if η = 0, then γ could be strictly positive.
The option to omit the constraint η ≥ 0 in the primal is slightly advantageous because
then the dual involves 2(p+q) instead of 2(p+q) + 1 Lagrange multipliers, so the constraint
matrix is a (p + q + 2) × 2(p + q) matrix instead of a (p + q + 2) × (2(p + q) + 1) matrix
and the matrix defining the quadratic functional is a 2(p + q) × 2(p + q) matrix instead of a
(2(p + q) + 1) × (2(p + q) + 1) matrix; see Section 54.8.
Under the Standard Margin Hypothesis for (SVMs2
0 ), there is some i0 such that
0 < λi0 < Ks and some j0 such that 0 < µj0 < Ks, and by the complementary slackness
conditions  i0 = 0 and ξj0 = 0, so we have the two active constraints
w
> ui0 − b = η, −w
> vj0 + b = η,
and we can solve for b and η and we get
b =
w
> ui0 + w
> vj0
2
η =
w
> ui0 − w
> vj0
2
δ =
η
k
wk
.
Due to numerical instability, when writing a computer program it is preferable to compute
the lists of indices Iλ and Iµ given by
Iλ = {i ∈ {1, . . . , p} | 0 < λi < Ks}
Iµ = {j ∈ {1, . . . , q} | 0 < µj < Ks}.
1966 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
Then it is easy to see that we can compute b and η using the following averaging formulae:
b = w
>



X
i∈Iλ
ui
 /|Iλ| +

X
j∈Iµ
vj
 /|Iµ|

 /2
η = w
>



X
i∈Iλ
ui
 /|Iλ| −  X
j∈Iµ
vj
 /|Iµ|

 /2.
The “kernelized” version of Problem (SVMs2
0 ) is the following:
Soft margin kernel SVM (SVMs2
0 ):
minimize
2
1
h
w, wi − Kmη + Ks
￾ 
> ξ
>
 1p+q
subject to
h
w, ϕ(ui)i − b ≥ η −  i
, i ≥ 0 i = 1, . . . , p
− hw, ϕ(vj )i + b ≥ η − ξj
, ξj ≥ 0 j = 1, . . . , q
η ≥ 0.
Tracing through the derivation of the dual program we obtain
Dual of the Soft margin kernel SVM (SVMs2
0 ):
minimize
1
2
￾
λ
> µ
>
 K

µ
λ

subject to
p
X
i=1
λi −
q
X
j=1
µj = 0
p
X
i=1
λi +
q
X
j=1
µj ≥ Km
0 ≤ λi ≤ Ks, i = 1, . . . , p
0 ≤ µj ≤ Ks, j = 1, . . . , q,
where K is the kernel matrix of Section 54.1.
As in Section 54.3, we obtain
w =
p
X
i=1
λiϕ(ui) −
q
X
j=1
µjϕ(vj ),
54.6. CLASSIFICATION OF THE DATA POINTS IN TERMS OF ν (SVMs2
0 ) 1967
so
b =
1
2

p
X
i=1
λi(κ(ui
, ui0
) + κ(ui
, vj0
)) −
q
X
j=1
µj (κ(vj
, ui0
) + κ(vj
, vj0
)) ,
and the classification function
f(x) = sgn(h w, ϕ(x)i − b)
is given by
f(x) = sgn
p
X
i=1
λi(2κ(ui
, x) − κ(ui
, ui0
) − κ(ui
, vj0
))
−
q
X
j=1
µj (2κ(vj
, x) − κ(vj
, ui0
) − κ(vj
, vj0
)) .
54.6 Classification of the Data Points in Terms
of ν (SVMs2
0
)
For a finer classification of the points it turns out to be convenient to consider the ratio
ν =
Km
(p + q)Ks
.
First note that in order for the constraints to be satisfied, some relationship between Ks and
Km must hold. In addition to the constraints
0 ≤ λi ≤ Ks, 0 ≤ µj ≤ Ks,
we also have the constraints
p
X
i=1
λi =
q
X
j=1
µj
p
X
i=1
λi +
q
X
j=1
µj ≥ Km
which imply that
p
X
i=1
λi ≥
Km
2
and X
q
j=1
µj ≥
Km
2
. (†)
Since λ, µ are all nonnegative, if λi = Ks for all i and if µj = Ks for all j, then
Km
2
≤
p
X
i=1
λi ≤ pKs and Km
2
≤
X
q
j=1
µj ≤ qKs,
1968 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
so these constraints are not satisfied unless Km ≤ min{2pKs, 2qKs}, so we assume that
Km ≤ min{2pKs, 2qKs}. The equations in (†) also imply that there is some i0 such that
λi0 > 0 and some j0 such that µj0 > 0, and so pm ≥ 1 and qm ≥ 1.
For a finer classification of the points we find it convenient to define ν > 0 such that
ν =
Km
(p + q)Ks
,
so that the objective function J(w, , ξ, b, η) is given by
J(w, , ξ, b, η) = 1
2
w
> w + (p + q)Ks
 −νη +
p +
1
q
￾

>
ξ
>
 1p+q
 .
Observe that the condition Km ≤ min{2pKs, 2qKs} is equivalent to
ν ≤ min
p
2
+
p
q
,
p
2
+
q
q

≤ 1.
Since we obtain an equivalent problem by rescaling by a common positive factor, theo￾retically it is convenient to normalize Ks as
Ks =
1
p + q
,
in which case Km = ν. This method is called the ν-support vector machine. Actually, to
program the method, it may be more convenient assume that Ks is arbitrary. This helps in
avoiding λi and µj to become to small when p + q is relatively large.
The equations (†) and the box inequalities
0 ≤ λi ≤ Ks, 0 ≤ µj ≤ Ks
also imply the following facts:
Proposition 54.1. If Problem (SVMs2
0 ) has an optimal solution with w 6 = 0 and η > 0,
then the following facts hold:
(1) Let pf be the number of points ui such that λi = Ks, and let qf the number of points
vj such that µj = Ks. Then pf , qf ≤ ν(p + q)/2.
(2) Let pm be the number of points ui such that λi > 0, and let qm the number of points vj
such that µj > 0. Then pm, qm ≥ ν(p + q)/2. We have pm ≥ 1 and qm ≥ 1.
(3) If pf ≥ 1 or qf ≥ 1, then ν ≥ 2/(p + q).
54.6. CLASSIFICATION OF THE DATA POINTS IN TERMS OF ν (SVMs2
0 ) 1969
Proof. (1) Recall that for an optimal solution with w 6 = 0 and η > 0, we have γ = 0, so by
(∗γ) we have the equations
p
X
i=1
λi =
Km
2
and X
q
j=1
µj =
Km
2
.
The point ui
fails to achieve the margin iff λi = Ks = Km/(ν(p + q)), so if there are pf such
points then
Km
2
=
p
X
i=1
λi ≥
Kmpf
ν(p + q)
,
so
pf ≤
ν(p + q)
2
.
A similar reasoning applies if vj
fails to achieve the margin δ with P p
i=1 λi replaced by
P
q
j=1 µj
.
(2) A point ui has margin at most δ iff λi > 0. If
Iλ>0 = {i ∈ {1, . . . , p} | λi > 0} and pm = |Iλ>0|,
then
Km
2
=
p
X
i=1
λi =
X
i∈Iλ>0
λi
,
and since λi ≤ Ks = Km/(ν(p + q)), we have
Km
2
=
X
i∈Iλ>0
λi ≤
Kmpm
ν(p + q)
,
which yields
pm ≥
ν(p + q)
2
.
A similar reasoning applies if a point vj has margin at most δ. We already observed that (†)
implies that pm ≥ 1 and qm ≥ 1.
(3) This follows immediately from (1).
Observe that pf = qf = 0 means that there are no points in the open slab containing
the separating hyperplane, namely, the points ui and the points vj are separable. So if the
points ui and the points vj are not separable, then we must pick ν such that 2/(p+q) ≤ ν ≤
min{2p/(p + q), 2q/(p + q)} for the method to succeed. Otherwise, the method is trying to
produce a solution where w = 0 and η = 0, and it does not converge (γ is nonzero). Actually,
Proposition 54.1 yields more accurate bounds on ν for the method to converge, namely
max 
p
2
+
pf
q
,
p
2
+
qf
q

≤ ν ≤ min 
2pm
p + q
,
2qm
p + q

.
1970 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
By a previous remark, pf ≤ pm and qf ≤ qm, the first inequality being strict if there is some
i such that 0 < λi < K, and the second inequality being strict if there is some j such that
0 < µj < K. This will be the case under the Standard Margin Hypothesis.
Observe that a small value of ν keeps pf and qf small, which is achieved if the δ-slab is
narrow (to avoid having points on the wrong sides of the margin hyperplanes). A large value
of ν allows pm and qm to be fairly large, which is achieved if the δ-slab is wide. Thus the
smaller ν is, the narrower the δ-slab is, and the larger ν is, the wider the δ-slab is. This is
the opposite of the behavior that we witnessed in ν-regression (see Section 56.1).
54.7 Existence of Support Vectors for (SVMs2
0
)
We now consider the issue of the existence of support vectors. We will show that in the
“generic case” there is always some blue support vector and some red support vector. The
term generic has to do with the choice of ν and will be explained below.
Given any real numbers u, v, x, y, if max{u, v} < min{x, y}, then u < x and v < y. This
is because u, v ≤ max{u, v} < min{x, y} ≤ x, y. Consequently, since by Proposition 54.1,
max{2pf /(p + q), 2qf /(p + q)} ≤ ν, if ν < min{2p/(p + q), 2q/(p + q)}, then pf < p and
qf < q, and since psf ≤ pf and qsf ≤ qf , we also have psf < p and qsf < q. This implies
that there are constraints corresponding to some i /∈ Eλ (in which case  i = 0) and to some
j /∈ Eµ (in which case ξj = 0), of the form
w
> ui − b ≥ η i /∈ Eλ
−w
> vj + b ≥ η j /∈ Eµ.
If w
> ui − b = η for some i /∈ Eλ and −w
> vj + b = η for some j /∈ Eµ, then we have a blue
support vector and a red support vector. Otherwise, we show how to modify b and η to
obtain an optimal solution with a blue support vector and a red support vector.
Proposition 54.2. For every optimal solution (w, b, η, , ξ) of Problem (SVMs2
0 ) with w 6 = 0
and η > 0, if
ν < min{2p/(p + q), 2q/(p + q)}
and if either no ui is a support vector or no vj is a support vector, then there is another
optimal solution (for the same w) with some i0 such that  i0 = 0 and w
> ui0 − b = η, and
there is some j0 such that ξj0 = 0 and −w
> vj0 + b = η; in other words, some ui0 and some
vj0
is a support vector; in particular, psf < p and qsf < q.
Proof. We just explained that psf < p and qsf < q, so the following constraints hold:
w
> ui − b = η −  i  i > 0 i ∈ Eλ
−w
> vj + b = η − ξj ξj > 0 j ∈ Eµ
w
> ui − b ≥ η i /∈ Eλ
−w
> vj + b ≥ η j /∈ Eµ,
54.7. EXISTENCE OF SUPPORT VECTORS FOR (SVMs2
0 ) 1971
where there is some i /∈ Eλ and some j /∈ Eµ.
If our optimal solution does not have a blue support vector and a red support vector,
then either w
> ui − b > η for all i /∈ Eλ or −w
> vj + b > η for all j /∈ Eµ.
Case 1 . We have
w
> ui − b > η i /∈ Eλ
−w
> vj + b ≥ η j /∈ Eµ.
There are two subcases.
Case 1a. Assume that there is some j /∈ Eµ such that −w
> vj + b = η. Our strategy
is to increase η and b by a small amount θ in such a way that some inequality becomes an
equation for some i /∈ Eλ. Geometrically, this amounts to raising the separating hyperplane
Hw,b and increasing the width of the slab, keeping the red margin hyperplane unchanged.
See Figure 54.7. η
η
red support vector
no blue support vectors
η
red support vector
blue support vector 
θ
η
θ
Figure 54.7: In this illustration points with errors are denoted by open circles. In the original,
upper left configuration, there is no blue support vector. By raising the pink separating
hyperplane and increasing the margin, we end up with a blue support vector.
Let us pick θ such that
θ = (1/2) min{w
> ui − b − η | i /∈ Eλ}.
w x - (b + θ) - (η + θ) = 0
T
w x - (b + θ) = 0
w x - (b + θ) + (η + θ) = 0
w x - b - η= 0
T
T
T
w x - b + η = 0
w x - b = 0
T
T
1972 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
Our hypotheses imply that θ > 0. We can write
w
> ui − (b + θ) = η + θ − ( i + 2θ)  i > 0 i ∈ Eλ
−w
> vj + b + θ = η + θ − ξj ξj > 0 j ∈ Eµ
w
> ui − (b + θ) ≥ η + θ i /∈ Eλ
−w
> vj + b + θ ≥ η + θ j /∈ Eµ.
By hypothesis
−w
> vj + b + θ = η + θ for some j /∈ Eµ,
and by the choice of θ,
w
> ui − (b + θ) = η + θ for some i /∈ Eλ.
The new value of the objective function is
ω(θ) = 1
2
w
> w − ν(η + θ) + 1
p + q

X
i∈Eλ
( i + 2θ) + X
j∈Eµ
ξj

=
1
2
w
> w − νη +
1
p + q

X
i∈Eλ

i +
X
j∈Eµ
ξj
 −
 ν −
2psf
p + q

θ.
By Proposition 54.1 we have
max 
p
2
+
pf
q
,
p
2
+
qf
q

≤ ν
and psf ≤ pf and qsf ≤ qf , which implies that
ν −
2psf
p + q
≥ 0, (∗1)
and so ω(θ) ≤ ω(0). If inequality (∗1) is strict, then this contradicts the optimality of the
original solution. Therefore, ν = 2psf /(p + q), ω(θ) = ω(0), and (w, b + θ, η + θ,  + 2θ, ξ) is
an optimal solution such that
w
> ui − (b + θ) = η + θ
−w
> vj + b + θ = η + θ
for some i /∈ Eλ and some j /∈ Eµ.
Case 1b. We have −w
> vj + b > η for all j /∈ Eµ. Our strategy is to increase η and
the errors by a small θ in such a way that some inequality becomes an equation for some
i /∈ Eλ or for some j /∈ Eµ. Geometrically, this corresponds to increasing the width of the
slab, keeping the separating hyperplane unchanged. See Figures 54.8 and 54.9. Then we are
reduced to Case 1a or Case 2a.
54.7. EXISTENCE OF SUPPORT VECTORS FOR (SVMs2
0 ) 1973
η
η
no red support vectors
no blue support vectors
η
η
red support vector
θ
θ
no blue support vectors
Figure 54.8: In this illustration points with errors are denoted by open circles. In the
original, upper left configuration, there is no blue support vector and no red support vector.
By increasing the margin, we end up with a red support vector and reduce to Case 1a.
We have
w
> ui − b = η −  i  i > 0 i ∈ Eλ
−w
> vj + b = η − ξj ξj > 0 j ∈ Eµ
w
> ui − b > η i /∈ Eλ
−w
> vj + b > η j /∈ Eµ.
Let us pick θ such that
θ = min{w
> ui − b − η, −w
> vj + b − η | i /∈ Eλ, j /∈ Eµ}.
Our hypotheses imply that θ > 0. We can write
w
> ui − b = η + θ − ( i + θ)  i > 0 i ∈ Eλ
−w
> vj + b = η + θ − (ξj + θ) ξj > 0 j ∈ Eµ
w
> ui − b ≥ η + θ i /∈ Eλ
−w
> vj + b ≥ η + θ j /∈ Eµ,
and by the choice of θ, either
w
> ui − b = η + θ for some i /∈ Eλ
w x - b - (η + θ) = 0
T
T
T
T
T
T
w x - b - η= 0
w x - b + η = 0
w x - b = 0
w x - b = 0
w x - b + (η + θ) = 0
1974 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
η
η
no red support vectors
no blue support vectors
η
η
no red support vectors
blue support vector θ
θ
Case 2a
Figure 54.9: In this illustration points with errors are denoted by open circles. In the
original, upper left configuration, there is no blue support vector and no red support vector.
By increasing the margin, we end up with a blue support vector and reduce to Case 2a.
or
−w
> vj + b = η + θ for some j /∈ Eµ.
The new value of the objective function is
ω(θ) = 1
2
w
> w − ν(η + θ) + 1
p + q

X
i∈Eλ
( i + θ) + X
j∈Eµ
(ξj + θ)

=
1
2
w
> w − νη +
1
p + q

X
i∈Eλ

i +
X
j∈Eµ
ξj
 −
 ν −
psf
p +
+
q
qsf  θ.
Since max{2pf /(p + q), 2qf /(p + q)} ≤ ν implies that (pf + qf )/(p + q) ≤ ν and psf ≤ pf ,
qsf ≤ qf , we have
ν −
psf + qsf
p + q
≥ 0, (∗2)
and so ω(θ) ≤ ω(0). If inequality (∗2) is strict, then this contradicts the optimality of the
original solution. Therefore, ν = (psf +qsf )/(p+q), ω(θ) = ω(0) and (w, b, η +θ, +θ, ξ +θ)
is an optimal solution such that either
w
> ui − b = η + θ for some i /∈ Eλ
or
−w
> vj + b = η + θ for some j /∈ Eµ.
w x - b - (η + θ) = 0
w x - b - η= 0
T
T
T
w x - b + η = 0
w x - b = 0
T
T
T
w x - b = 0
w x - b + (η + θ) = 0
54.7. EXISTENCE OF SUPPORT VECTORS FOR (SVMs2
0 ) 1975
We are now reduced to Case 1a or Case 2a.
Case 2 . We have
w
> ui − b ≥ η i /∈ Eλ
−w
> vj + b > η j /∈ Eµ.
There are two subcases.
Case 2a. Assume that there is some i /∈ Eλ such that w
> ui − b = η. Our strategy is to
increase η and decrease b by a small amount θ in such a way that some inequality becomes an
equation for some j /∈ Eµ. Geometrically, this amounts to lowering the separating hyperplane
Hw,b and increasing the width of the slab, keeping the blue margin hyperplane unchanged.
See Figure 54.10.
η
η
no red support vectors
blue support vector
η
η
red support vector
blue support vector
θ
θ
Figure 54.10: In this illustration points with errors are denoted by open circles. In the
original, upper left configuration, there is no red support vector. By lowering the pink
separating hyperplane and increasing the margin, we end up with a red support vector.
Let us pick θ such that
θ = (1/2) min{−w
> vj + b − η | j /∈ Eµ}.
Our hypotheses imply that θ > 0. We can write
w
> ui − (b − θ) = η + θ −  i  i > 0 i ∈ Eλ
−w
> vj + b − θ = η + θ − (ξj + 2θ) ξj > 0 j ∈ Eµ
w
> ui − (b − θ) ≥ η + θ i /∈ Eλ
−w
> vj + b − θ ≥ η + θ j /∈ Eµ.
w x - b - η= 0
T
T
T
w x - b + η = 0
w x - b = 0
w x - (b-θ) - (η+θ)= 0
T
T
T
w x - (b-θ) + (η+θ) = 0
w x - (b - θ) = 0
1976 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
By hypothesis
w
> ui − (b − θ) = η + θ for some i /∈ Eλ,
and by the choice of θ,
−w
> vj + b − θ = η + θ for some j /∈ Eµ.
The new value of the objective function is
ω(θ) = 1
2
w
> w − ν(η + θ) + 1
p + q

X
i∈Eλ

i +
X
j∈Eµ
(ξj + 2θ)

=
1
2
w
> w − νη +
1
p + q

X
i∈Eλ

i +
X
j∈Eµ
ξj
 −
 ν −
2qsf
p + q

θ.
The rest of the proof is similar to Case 1a with psf replaced by qsf .
Case 2b. We have w
> ui − b > η for all i /∈ Eλ. Since by hypothesis −w
> vj + b > η for
all j /∈ Eµ, Case 2b is identical to Case 1b, and we are done.
A subtle point here is that Proposition 54.2 shows that if there is an optimal solution,
then there is one with a blue and a red support vector, but it does not guarantee that these
are support vectors of type 1. Since the dual program does not determine b and η unless
these support vectors are of type 1, from a practical point of view this proposition is not
helpful.
The proof of Proposition 54.2 reveals that there are three critical values for ν:
