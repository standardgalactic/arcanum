is in
the closed half space on the blue side bounded by the blue margin hyperplane Hw,b+Î·.
If Î»i > 0, then by (1) and (2) above, the point ui
is on the blue margin.
Similarly, if Î¾j = 0, then the point vj
is correctly classified. If Âµj = 0, then vj
is in the
closed half space on the red side bounded by the red margin hyperplane Hw,bâˆ’Î·, and if
Âµj > 0, then the point vj
is on the red margin.
Also observe that if Î»i > 0, then ui
is in the closed half space bounded by the blue hyperï¿¾plane Hw,b+Î· and containing the separating hyperplane Hw,b (including the blue hyperplane
Hw,b+Î·).
Similarly, if Âµj > 0, then vj
is in the closed half space bounded by the red hyperplane
Hw,b+Î· and containing the separating hyperplane Hw,b (including the red hyperplane Hw,b+Î·).
Definition 54.3. Vectors ui such that Î»i > 0 and vectors vj such that Âµj > 0 are said to
have margin at most Î´. The sets of indices associated with these vectors are denoted by
IÎ»>0 = {i âˆˆ {1, . . . , p} | Î»i > 0}
IÂµ>0 = {j âˆˆ {1, . . . , q} | Âµj > 0}.
We denote their cardinalities by pm = |IÎ»>0| and qm = |IÂµ>0|.
Vectors ui such that  i > 0 and vectors vj such that Î¾j > 0 are said to strictly fail the
margin. The corresponding sets of indices are denoted by
EÎ» = {i âˆˆ {1, . . . , p} |  i > 0}
EÂµ = {j âˆˆ {1, . . . , q} | Î¾j > 0}.
We write psf = |EÎ»| and qsf = |EÂµ|.
1964 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
We have the inclusions EÎ» âŠ† KÎ» and EÂµ âŠ† KÂµ. The difference between the first sets and
the second sets is that the second sets may contain support vectors of type 2 such that Î»i = Ks
and  i = 0, or Âµj = Ks and Î¾j = 0. We also have the equations IÎ» âˆª (KÎ» âˆ’ EÎ») âˆª EÎ» = IÎ»>0
and IÂµ âˆª (KÂµ âˆ’ EÂµ) âˆª EÂµ = IÂµ>0, and the inequalities psf â‰¤ pf â‰¤ pm and qsf â‰¤ qf â‰¤ qm.
The blue points ui of index i âˆˆ IÎ»>0 are classified as follows:
(1) If i âˆˆ IÎ», then ui
is a support vector of type 1 (Î»i < Ks).
(2) If i âˆˆ KÎ» âˆ’ EÎ», then ui
is a support vector of type 2 (Î»i = Ks).
(3) If i âˆˆ EÎ», then ui strictly fails the margin, that is  i > 0.
Similarly the red points vj of index j âˆˆ IÂµ>0 are classified as follows:
(1) If j âˆˆ IÂµ, then vj
is a support vector of type 1 (Âµj < Ks).
(2) If j âˆˆ KÂµ âˆ’ EÂµ, then vj
is a support vector of type 2 (Âµj = Ks).
(3) If j âˆˆ EÂµ, then vj strictly fails the margin, that is Î¾j > 0.
Note that pm âˆ’ pf is the number of blue support vectors of type 1 and qm âˆ’ qf is the
number of red support vectors of type 1. The remaining blue points ui
for which Î»i = 0 are
either exceptional support vectors or they are (strictly ) in the open half-space corresponding
to the blue side. Similarly, the remaining red points vj
for which Âµj = 0 are either exceptional
support vectors or they are (strictly) in the open half-space corresponding to the red side.
It is shown in Section 54.8 how the dual program is solved using ADMM from Section
52.6. If the primal problem is solvable, this yields solutions for Î» and Âµ. Once a solution for
Î» and Âµ is obtained, we have
w = âˆ’X

Âµ
Î»

=
p
X
i=1
Î»iui âˆ’
q
X
j=1
Âµjvj
.
As we said earlier, the hypotheses of Theorem 50.17(2) hold, so if the primal problem
(SVMs2
0 ) has an optimal solution with w 6 = 0, then the dual problem has a solution too, and
the duality gap is zero. Therefore, for optimal solutions we have
L(w, , Î¾, b, Î·, Î», Âµ, Î±, Î², Î³) = G(Î», Âµ, Î±, Î², Î³),
which means that
1
2
w
> w âˆ’ KmÎ· + Ks

p
X
i=1

i +
q
X
j=1
Î¾j
 = âˆ’
1
2
ï¿¾
Î»
> Âµ
>
 X
> X

Âµ
Î»

,
and since
w = âˆ’X

Âµ
Î»

,
54.5. SOFT MARGIN SUPPORT VECTOR MACHINES; (SVMs2
0 ) 1965
we get
1
2
ï¿¾
Î»
> Âµ
>
 X
> X

Âµ
Î»

âˆ’ KmÎ· + Ks

p
X
i=1

i +
q
X
j=1
Î¾j
 = âˆ’
1
2
ï¿¾
Î»
> Âµ
>
 X
> X

Âµ
Î»

,
which yields
Î· =
Ks
Km

p
X
i=1

i +
q
X
j=1
Î¾j
 +
K
1
m
ï¿¾
Î»
> Âµ
>
 X
> X

Âµ
Î»

. (âˆ—)
Therefore, we confirm that Î· â‰¥ 0.
Remarks: Since we proved that if the Primal Problem (SVMs2
0 ) has an optimal solution
with w 6 = 0, then Î· â‰¥ 0, one might wonder why the constraint Î· â‰¥ 0 was included. If we
delete this constraint, it is easy to see that the only difference is that instead of the equation
1
>p Î» + 1
>q Âµ = Km + Î³ (âˆ—1)
we obtain the equation
1
>p Î» + 1
>q Âµ = Km. (âˆ—2)
If Î· > 0, then by complementary slackness Î³ = 0, in which case (âˆ—1) and (âˆ—2) are equivalent.
But if Î· = 0, then Î³ could be strictly positive.
The option to omit the constraint Î· â‰¥ 0 in the primal is slightly advantageous because
then the dual involves 2(p+q) instead of 2(p+q) + 1 Lagrange multipliers, so the constraint
matrix is a (p + q + 2) Ã— 2(p + q) matrix instead of a (p + q + 2) Ã— (2(p + q) + 1) matrix
and the matrix defining the quadratic functional is a 2(p + q) Ã— 2(p + q) matrix instead of a
(2(p + q) + 1) Ã— (2(p + q) + 1) matrix; see Section 54.8.
Under the Standard Margin Hypothesis for (SVMs2
0 ), there is some i0 such that
0 < Î»i0 < Ks and some j0 such that 0 < Âµj0 < Ks, and by the complementary slackness
conditions  i0 = 0 and Î¾j0 = 0, so we have the two active constraints
w
> ui0 âˆ’ b = Î·, âˆ’w
> vj0 + b = Î·,
and we can solve for b and Î· and we get
b =
w
> ui0 + w
> vj0
2
Î· =
w
> ui0 âˆ’ w
> vj0
2
Î´ =
Î·
k
wk
.
Due to numerical instability, when writing a computer program it is preferable to compute
the lists of indices IÎ» and IÂµ given by
IÎ» = {i âˆˆ {1, . . . , p} | 0 < Î»i < Ks}
IÂµ = {j âˆˆ {1, . . . , q} | 0 < Âµj < Ks}.
1966 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
Then it is easy to see that we can compute b and Î· using the following averaging formulae:
b = w
>
ï£«
ï£­

X
iâˆˆIÎ»
ui
 /|IÎ»| +

X
jâˆˆIÂµ
vj
 /|IÂµ|
ï£¶
ï£¸ /2
Î· = w
>
ï£«
ï£­

X
iâˆˆIÎ»
ui
 /|IÎ»| âˆ’  X
jâˆˆIÂµ
vj
 /|IÂµ|
ï£¶
ï£¸ /2.
The â€œkernelizedâ€ version of Problem (SVMs2
0 ) is the following:
Soft margin kernel SVM (SVMs2
0 ):
minimize
2
1
h
w, wi âˆ’ KmÎ· + Ks
ï¿¾ 
> Î¾
>
 1p+q
subject to
h
w, Ï•(ui)i âˆ’ b â‰¥ Î· âˆ’  i
, i â‰¥ 0 i = 1, . . . , p
âˆ’ hw, Ï•(vj )i + b â‰¥ Î· âˆ’ Î¾j
, Î¾j â‰¥ 0 j = 1, . . . , q
Î· â‰¥ 0.
Tracing through the derivation of the dual program we obtain
Dual of the Soft margin kernel SVM (SVMs2
0 ):
minimize
1
2
ï¿¾
Î»
> Âµ
>
 K

Âµ
Î»

subject to
p
X
i=1
Î»i âˆ’
q
X
j=1
Âµj = 0
p
X
i=1
Î»i +
q
X
j=1
Âµj â‰¥ Km
0 â‰¤ Î»i â‰¤ Ks, i = 1, . . . , p
0 â‰¤ Âµj â‰¤ Ks, j = 1, . . . , q,
where K is the kernel matrix of Section 54.1.
As in Section 54.3, we obtain
w =
p
X
i=1
Î»iÏ•(ui) âˆ’
q
X
j=1
ÂµjÏ•(vj ),
54.6. CLASSIFICATION OF THE DATA POINTS IN TERMS OF Î½ (SVMs2
0 ) 1967
so
b =
1
2

p
X
i=1
Î»i(Îº(ui
, ui0
) + Îº(ui
, vj0
)) âˆ’
q
X
j=1
Âµj (Îº(vj
, ui0
) + Îº(vj
, vj0
)) ,
and the classification function
f(x) = sgn(h w, Ï•(x)i âˆ’ b)
is given by
f(x) = sgn
p
X
i=1
Î»i(2Îº(ui
, x) âˆ’ Îº(ui
, ui0
) âˆ’ Îº(ui
, vj0
))
âˆ’
q
X
j=1
Âµj (2Îº(vj
, x) âˆ’ Îº(vj
, ui0
) âˆ’ Îº(vj
, vj0
)) .
54.6 Classification of the Data Points in Terms
of Î½ (SVMs2
0
)
For a finer classification of the points it turns out to be convenient to consider the ratio
Î½ =
Km
(p + q)Ks
.
First note that in order for the constraints to be satisfied, some relationship between Ks and
Km must hold. In addition to the constraints
0 â‰¤ Î»i â‰¤ Ks, 0 â‰¤ Âµj â‰¤ Ks,
we also have the constraints
p
X
i=1
Î»i =
q
X
j=1
Âµj
p
X
i=1
Î»i +
q
X
j=1
Âµj â‰¥ Km
which imply that
p
X
i=1
Î»i â‰¥
Km
2
and X
q
j=1
Âµj â‰¥
Km
2
. (â€ )
Since Î», Âµ are all nonnegative, if Î»i = Ks for all i and if Âµj = Ks for all j, then
Km
2
â‰¤
p
X
i=1
Î»i â‰¤ pKs and Km
2
â‰¤
X
q
j=1
Âµj â‰¤ qKs,
1968 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
so these constraints are not satisfied unless Km â‰¤ min{2pKs, 2qKs}, so we assume that
Km â‰¤ min{2pKs, 2qKs}. The equations in (â€ ) also imply that there is some i0 such that
Î»i0 > 0 and some j0 such that Âµj0 > 0, and so pm â‰¥ 1 and qm â‰¥ 1.
For a finer classification of the points we find it convenient to define Î½ > 0 such that
Î½ =
Km
(p + q)Ks
,
so that the objective function J(w, , Î¾, b, Î·) is given by
J(w, , Î¾, b, Î·) = 1
2
w
> w + (p + q)Ks
 âˆ’Î½Î· +
p +
1
q
ï¿¾

>
Î¾
>
 1p+q
 .
Observe that the condition Km â‰¤ min{2pKs, 2qKs} is equivalent to
Î½ â‰¤ min
p
2
+
p
q
,
p
2
+
q
q

â‰¤ 1.
Since we obtain an equivalent problem by rescaling by a common positive factor, theoï¿¾retically it is convenient to normalize Ks as
Ks =
1
p + q
,
in which case Km = Î½. This method is called the Î½-support vector machine. Actually, to
program the method, it may be more convenient assume that Ks is arbitrary. This helps in
avoiding Î»i and Âµj to become to small when p + q is relatively large.
The equations (â€ ) and the box inequalities
0 â‰¤ Î»i â‰¤ Ks, 0 â‰¤ Âµj â‰¤ Ks
also imply the following facts:
Proposition 54.1. If Problem (SVMs2
0 ) has an optimal solution with w 6 = 0 and Î· > 0,
then the following facts hold:
(1) Let pf be the number of points ui such that Î»i = Ks, and let qf the number of points
vj such that Âµj = Ks. Then pf , qf â‰¤ Î½(p + q)/2.
(2) Let pm be the number of points ui such that Î»i > 0, and let qm the number of points vj
such that Âµj > 0. Then pm, qm â‰¥ Î½(p + q)/2. We have pm â‰¥ 1 and qm â‰¥ 1.
(3) If pf â‰¥ 1 or qf â‰¥ 1, then Î½ â‰¥ 2/(p + q).
54.6. CLASSIFICATION OF THE DATA POINTS IN TERMS OF Î½ (SVMs2
0 ) 1969
Proof. (1) Recall that for an optimal solution with w 6 = 0 and Î· > 0, we have Î³ = 0, so by
(âˆ—Î³) we have the equations
p
X
i=1
Î»i =
Km
2
and X
q
j=1
Âµj =
Km
2
.
The point ui
fails to achieve the margin iff Î»i = Ks = Km/(Î½(p + q)), so if there are pf such
points then
Km
2
=
p
X
i=1
Î»i â‰¥
Kmpf
Î½(p + q)
,
so
pf â‰¤
Î½(p + q)
2
.
A similar reasoning applies if vj
fails to achieve the margin Î´ with P p
i=1 Î»i replaced by
P
q
j=1 Âµj
.
(2) A point ui has margin at most Î´ iff Î»i > 0. If
IÎ»>0 = {i âˆˆ {1, . . . , p} | Î»i > 0} and pm = |IÎ»>0|,
then
Km
2
=
p
X
i=1
Î»i =
X
iâˆˆIÎ»>0
Î»i
,
and since Î»i â‰¤ Ks = Km/(Î½(p + q)), we have
Km
2
=
X
iâˆˆIÎ»>0
Î»i â‰¤
Kmpm
Î½(p + q)
,
which yields
pm â‰¥
Î½(p + q)
2
.
A similar reasoning applies if a point vj has margin at most Î´. We already observed that (â€ )
implies that pm â‰¥ 1 and qm â‰¥ 1.
(3) This follows immediately from (1).
Observe that pf = qf = 0 means that there are no points in the open slab containing
the separating hyperplane, namely, the points ui and the points vj are separable. So if the
points ui and the points vj are not separable, then we must pick Î½ such that 2/(p+q) â‰¤ Î½ â‰¤
min{2p/(p + q), 2q/(p + q)} for the method to succeed. Otherwise, the method is trying to
produce a solution where w = 0 and Î· = 0, and it does not converge (Î³ is nonzero). Actually,
Proposition 54.1 yields more accurate bounds on Î½ for the method to converge, namely
max 
p
2
+
pf
q
,
p
2
+
qf
q

â‰¤ Î½ â‰¤ min 
2pm
p + q
,
2qm
p + q

.
1970 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
By a previous remark, pf â‰¤ pm and qf â‰¤ qm, the first inequality being strict if there is some
i such that 0 < Î»i < K, and the second inequality being strict if there is some j such that
0 < Âµj < K. This will be the case under the Standard Margin Hypothesis.
Observe that a small value of Î½ keeps pf and qf small, which is achieved if the Î´-slab is
narrow (to avoid having points on the wrong sides of the margin hyperplanes). A large value
of Î½ allows pm and qm to be fairly large, which is achieved if the Î´-slab is wide. Thus the
smaller Î½ is, the narrower the Î´-slab is, and the larger Î½ is, the wider the Î´-slab is. This is
the opposite of the behavior that we witnessed in Î½-regression (see Section 56.1).
54.7 Existence of Support Vectors for (SVMs2
0
)
We now consider the issue of the existence of support vectors. We will show that in the
â€œgeneric caseâ€ there is always some blue support vector and some red support vector. The
term generic has to do with the choice of Î½ and will be explained below.
Given any real numbers u, v, x, y, if max{u, v} < min{x, y}, then u < x and v < y. This
is because u, v â‰¤ max{u, v} < min{x, y} â‰¤ x, y. Consequently, since by Proposition 54.1,
max{2pf /(p + q), 2qf /(p + q)} â‰¤ Î½, if Î½ < min{2p/(p + q), 2q/(p + q)}, then pf < p and
qf < q, and since psf â‰¤ pf and qsf â‰¤ qf , we also have psf < p and qsf < q. This implies
that there are constraints corresponding to some i /âˆˆ EÎ» (in which case  i = 0) and to some
j /âˆˆ EÂµ (in which case Î¾j = 0), of the form
w
> ui âˆ’ b â‰¥ Î· i /âˆˆ EÎ»
âˆ’w
> vj + b â‰¥ Î· j /âˆˆ EÂµ.
If w
> ui âˆ’ b = Î· for some i /âˆˆ EÎ» and âˆ’w
> vj + b = Î· for some j /âˆˆ EÂµ, then we have a blue
support vector and a red support vector. Otherwise, we show how to modify b and Î· to
obtain an optimal solution with a blue support vector and a red support vector.
Proposition 54.2. For every optimal solution (w, b, Î·, , Î¾) of Problem (SVMs2
0 ) with w 6 = 0
and Î· > 0, if
Î½ < min{2p/(p + q), 2q/(p + q)}
and if either no ui is a support vector or no vj is a support vector, then there is another
optimal solution (for the same w) with some i0 such that  i0 = 0 and w
> ui0 âˆ’ b = Î·, and
there is some j0 such that Î¾j0 = 0 and âˆ’w
> vj0 + b = Î·; in other words, some ui0 and some
vj0
is a support vector; in particular, psf < p and qsf < q.
Proof. We just explained that psf < p and qsf < q, so the following constraints hold:
w
> ui âˆ’ b = Î· âˆ’  i  i > 0 i âˆˆ EÎ»
âˆ’w
> vj + b = Î· âˆ’ Î¾j Î¾j > 0 j âˆˆ EÂµ
w
> ui âˆ’ b â‰¥ Î· i /âˆˆ EÎ»
âˆ’w
> vj + b â‰¥ Î· j /âˆˆ EÂµ,
54.7. EXISTENCE OF SUPPORT VECTORS FOR (SVMs2
0 ) 1971
where there is some i /âˆˆ EÎ» and some j /âˆˆ EÂµ.
If our optimal solution does not have a blue support vector and a red support vector,
then either w
> ui âˆ’ b > Î· for all i /âˆˆ EÎ» or âˆ’w
> vj + b > Î· for all j /âˆˆ EÂµ.
Case 1 . We have
w
> ui âˆ’ b > Î· i /âˆˆ EÎ»
âˆ’w
> vj + b â‰¥ Î· j /âˆˆ EÂµ.
There are two subcases.
Case 1a. Assume that there is some j /âˆˆ EÂµ such that âˆ’w
> vj + b = Î·. Our strategy
is to increase Î· and b by a small amount Î¸ in such a way that some inequality becomes an
equation for some i /âˆˆ EÎ». Geometrically, this amounts to raising the separating hyperplane
Hw,b and increasing the width of the slab, keeping the red margin hyperplane unchanged.
See Figure 54.7. Î·
Î·
red support vector
no blue support vectors
Î·
red support vector
blue support vector 
Î¸
Î·
Î¸
Figure 54.7: In this illustration points with errors are denoted by open circles. In the original,
upper left configuration, there is no blue support vector. By raising the pink separating
hyperplane and increasing the margin, we end up with a blue support vector.
Let us pick Î¸ such that
Î¸ = (1/2) min{w
> ui âˆ’ b âˆ’ Î· | i /âˆˆ EÎ»}.
w x - (b + Î¸) - (Î· + Î¸) = 0
T
w x - (b + Î¸) = 0
w x - (b + Î¸) + (Î· + Î¸) = 0
w x - b - Î·= 0
T
T
T
w x - b + Î· = 0
w x - b = 0
T
T
1972 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
Our hypotheses imply that Î¸ > 0. We can write
w
> ui âˆ’ (b + Î¸) = Î· + Î¸ âˆ’ ( i + 2Î¸)  i > 0 i âˆˆ EÎ»
âˆ’w
> vj + b + Î¸ = Î· + Î¸ âˆ’ Î¾j Î¾j > 0 j âˆˆ EÂµ
w
> ui âˆ’ (b + Î¸) â‰¥ Î· + Î¸ i /âˆˆ EÎ»
âˆ’w
> vj + b + Î¸ â‰¥ Î· + Î¸ j /âˆˆ EÂµ.
By hypothesis
âˆ’w
> vj + b + Î¸ = Î· + Î¸ for some j /âˆˆ EÂµ,
and by the choice of Î¸,
w
> ui âˆ’ (b + Î¸) = Î· + Î¸ for some i /âˆˆ EÎ».
The new value of the objective function is
Ï‰(Î¸) = 1
2
w
> w âˆ’ Î½(Î· + Î¸) + 1
p + q

X
iâˆˆEÎ»
( i + 2Î¸) + X
jâˆˆEÂµ
Î¾j

=
1
2
w
> w âˆ’ Î½Î· +
1
p + q

X
iâˆˆEÎ»

i +
X
jâˆˆEÂµ
Î¾j
 âˆ’
 Î½ âˆ’
2psf
p + q

Î¸.
By Proposition 54.1 we have
max 
p
2
+
pf
q
,
p
2
+
qf
q

â‰¤ Î½
and psf â‰¤ pf and qsf â‰¤ qf , which implies that
Î½ âˆ’
2psf
p + q
â‰¥ 0, (âˆ—1)
and so Ï‰(Î¸) â‰¤ Ï‰(0). If inequality (âˆ—1) is strict, then this contradicts the optimality of the
original solution. Therefore, Î½ = 2psf /(p + q), Ï‰(Î¸) = Ï‰(0), and (w, b + Î¸, Î· + Î¸,  + 2Î¸, Î¾) is
an optimal solution such that
w
> ui âˆ’ (b + Î¸) = Î· + Î¸
âˆ’w
> vj + b + Î¸ = Î· + Î¸
for some i /âˆˆ EÎ» and some j /âˆˆ EÂµ.
Case 1b. We have âˆ’w
> vj + b > Î· for all j /âˆˆ EÂµ. Our strategy is to increase Î· and
the errors by a small Î¸ in such a way that some inequality becomes an equation for some
i /âˆˆ EÎ» or for some j /âˆˆ EÂµ. Geometrically, this corresponds to increasing the width of the
slab, keeping the separating hyperplane unchanged. See Figures 54.8 and 54.9. Then we are
reduced to Case 1a or Case 2a.
54.7. EXISTENCE OF SUPPORT VECTORS FOR (SVMs2
0 ) 1973
Î·
Î·
no red support vectors
no blue support vectors
Î·
Î·
red support vector
Î¸
Î¸
no blue support vectors
Figure 54.8: In this illustration points with errors are denoted by open circles. In the
original, upper left configuration, there is no blue support vector and no red support vector.
By increasing the margin, we end up with a red support vector and reduce to Case 1a.
We have
w
> ui âˆ’ b = Î· âˆ’  i  i > 0 i âˆˆ EÎ»
âˆ’w
> vj + b = Î· âˆ’ Î¾j Î¾j > 0 j âˆˆ EÂµ
w
> ui âˆ’ b > Î· i /âˆˆ EÎ»
âˆ’w
> vj + b > Î· j /âˆˆ EÂµ.
Let us pick Î¸ such that
Î¸ = min{w
> ui âˆ’ b âˆ’ Î·, âˆ’w
> vj + b âˆ’ Î· | i /âˆˆ EÎ», j /âˆˆ EÂµ}.
Our hypotheses imply that Î¸ > 0. We can write
w
> ui âˆ’ b = Î· + Î¸ âˆ’ ( i + Î¸)  i > 0 i âˆˆ EÎ»
âˆ’w
> vj + b = Î· + Î¸ âˆ’ (Î¾j + Î¸) Î¾j > 0 j âˆˆ EÂµ
w
> ui âˆ’ b â‰¥ Î· + Î¸ i /âˆˆ EÎ»
âˆ’w
> vj + b â‰¥ Î· + Î¸ j /âˆˆ EÂµ,
and by the choice of Î¸, either
w
> ui âˆ’ b = Î· + Î¸ for some i /âˆˆ EÎ»
w x - b - (Î· + Î¸) = 0
T
T
T
T
T
T
w x - b - Î·= 0
w x - b + Î· = 0
w x - b = 0
w x - b = 0
w x - b + (Î· + Î¸) = 0
1974 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
Î·
Î·
no red support vectors
no blue support vectors
Î·
Î·
no red support vectors
blue support vector Î¸
Î¸
Case 2a
Figure 54.9: In this illustration points with errors are denoted by open circles. In the
original, upper left configuration, there is no blue support vector and no red support vector.
By increasing the margin, we end up with a blue support vector and reduce to Case 2a.
or
âˆ’w
> vj + b = Î· + Î¸ for some j /âˆˆ EÂµ.
The new value of the objective function is
Ï‰(Î¸) = 1
2
w
> w âˆ’ Î½(Î· + Î¸) + 1
p + q

X
iâˆˆEÎ»
( i + Î¸) + X
jâˆˆEÂµ
(Î¾j + Î¸)

=
1
2
w
> w âˆ’ Î½Î· +
1
p + q

X
iâˆˆEÎ»

i +
X
jâˆˆEÂµ
Î¾j
 âˆ’
 Î½ âˆ’
psf
p +
+
q
qsf  Î¸.
Since max{2pf /(p + q), 2qf /(p + q)} â‰¤ Î½ implies that (pf + qf )/(p + q) â‰¤ Î½ and psf â‰¤ pf ,
qsf â‰¤ qf , we have
Î½ âˆ’
psf + qsf
p + q
â‰¥ 0, (âˆ—2)
and so Ï‰(Î¸) â‰¤ Ï‰(0). If inequality (âˆ—2) is strict, then this contradicts the optimality of the
original solution. Therefore, Î½ = (psf +qsf )/(p+q), Ï‰(Î¸) = Ï‰(0) and (w, b, Î· +Î¸, +Î¸, Î¾ +Î¸)
is an optimal solution such that either
w
> ui âˆ’ b = Î· + Î¸ for some i /âˆˆ EÎ»
or
âˆ’w
> vj + b = Î· + Î¸ for some j /âˆˆ EÂµ.
w x - b - (Î· + Î¸) = 0
w x - b - Î·= 0
T
T
T
w x - b + Î· = 0
w x - b = 0
T
T
T
w x - b = 0
w x - b + (Î· + Î¸) = 0
54.7. EXISTENCE OF SUPPORT VECTORS FOR (SVMs2
0 ) 1975
We are now reduced to Case 1a or Case 2a.
Case 2 . We have
w
> ui âˆ’ b â‰¥ Î· i /âˆˆ EÎ»
âˆ’w
> vj + b > Î· j /âˆˆ EÂµ.
There are two subcases.
Case 2a. Assume that there is some i /âˆˆ EÎ» such that w
> ui âˆ’ b = Î·. Our strategy is to
increase Î· and decrease b by a small amount Î¸ in such a way that some inequality becomes an
equation for some j /âˆˆ EÂµ. Geometrically, this amounts to lowering the separating hyperplane
Hw,b and increasing the width of the slab, keeping the blue margin hyperplane unchanged.
See Figure 54.10.
Î·
Î·
no red support vectors
blue support vector
Î·
Î·
red support vector
blue support vector
Î¸
Î¸
Figure 54.10: In this illustration points with errors are denoted by open circles. In the
original, upper left configuration, there is no red support vector. By lowering the pink
separating hyperplane and increasing the margin, we end up with a red support vector.
Let us pick Î¸ such that
Î¸ = (1/2) min{âˆ’w
> vj + b âˆ’ Î· | j /âˆˆ EÂµ}.
Our hypotheses imply that Î¸ > 0. We can write
w
> ui âˆ’ (b âˆ’ Î¸) = Î· + Î¸ âˆ’  i  i > 0 i âˆˆ EÎ»
âˆ’w
> vj + b âˆ’ Î¸ = Î· + Î¸ âˆ’ (Î¾j + 2Î¸) Î¾j > 0 j âˆˆ EÂµ
w
> ui âˆ’ (b âˆ’ Î¸) â‰¥ Î· + Î¸ i /âˆˆ EÎ»
âˆ’w
> vj + b âˆ’ Î¸ â‰¥ Î· + Î¸ j /âˆˆ EÂµ.
w x - b - Î·= 0
T
T
T
w x - b + Î· = 0
w x - b = 0
w x - (b-Î¸) - (Î·+Î¸)= 0
T
T
T
w x - (b-Î¸) + (Î·+Î¸) = 0
w x - (b - Î¸) = 0
1976 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
By hypothesis
w
> ui âˆ’ (b âˆ’ Î¸) = Î· + Î¸ for some i /âˆˆ EÎ»,
and by the choice of Î¸,
âˆ’w
> vj + b âˆ’ Î¸ = Î· + Î¸ for some j /âˆˆ EÂµ.
The new value of the objective function is
Ï‰(Î¸) = 1
2
w
> w âˆ’ Î½(Î· + Î¸) + 1
p + q

X
iâˆˆEÎ»

i +
X
jâˆˆEÂµ
(Î¾j + 2Î¸)

=
1
2
w
> w âˆ’ Î½Î· +
1
p + q

X
iâˆˆEÎ»

i +
X
jâˆˆEÂµ
Î¾j
 âˆ’
 Î½ âˆ’
2qsf
p + q

Î¸.
The rest of the proof is similar to Case 1a with psf replaced by qsf .
Case 2b. We have w
> ui âˆ’ b > Î· for all i /âˆˆ EÎ». Since by hypothesis âˆ’w
> vj + b > Î· for
all j /âˆˆ EÂµ, Case 2b is identical to Case 1b, and we are done.
A subtle point here is that Proposition 54.2 shows that if there is an optimal solution,
then there is one with a blue and a red support vector, but it does not guarantee that these
are support vectors of type 1. Since the dual program does not determine b and Î· unless
these support vectors are of type 1, from a practical point of view this proposition is not
helpful.
The proof of Proposition 54.2 reveals that there are three critical values for Î½:
