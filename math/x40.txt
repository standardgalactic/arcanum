diagonal entries may be 2 Ã— 2 matrices or real entries).
Theorem 15.6 is not a very practical result but it is a useful theoretical result to cope
with matrices that cannot be diagonalized. For example, it can be used to prove that
every complex matrix is the limit of a sequence of diagonalizable matrices that have distinct
eigenvalues!
15.3 Location of Eigenvalues
If A is an n Ã— n complex (or real) matrix A, it would be useful to know, even roughly, where
the eigenvalues of A are located in the complex plane C. The Gershgorin discs provide some
precise information about this.
Definition 15.5. For any complex n Ã— n matrix A, for i = 1, . . . , n, let
Ri
0
(A) =
nX
j=1
j6=i
|ai j |
and let
G(A) =
n[
i=1
{z âˆˆ C | |z âˆ’ ai i| â‰¤ Ri
0
(A)}.
Each disc {z âˆˆ C | |z âˆ’ ai i| â‰¤ Ri
0
(A)} is called a Gershgorin disc and their union G(A) is
called the Gershgorin domain. An example of Gershgorin domain for A =
ï£«
ï£­
1 2 3
4
7 8 1 +
i 6
i
ï£¶
ï£¸
is illustrated in Figure 15.1.
Although easy to prove, the following theorem is very useful:
Theorem 15.9. (Gershgorinâ€™s disc theorem) For any complex nÃ—n matrix A, all the eigenï¿¾values of A belong to the Gershgorin domain G(A). Furthermore the following properties
hold:
568 CHAPTER 15. EIGENVECTORS AND EIGENVALUES
>>
>>
>>
>>
d1, d2, d3 ;
10 5 0 5 10 15
10
5
5
10
15
?
,
Figure 15.1: Let A be the 3 Ã— 3 matrix specified at the end of Definition 15.5. For this
particular A, we find that R1
0
(A) = 5, R2
0
(A) = 10, and R3
0
(A) = 15. The blue/purple disk
is |z âˆ’ 1| â‰¤ 5, the pink disk is |z âˆ’ i| â‰¤ 10, the peach disk is |z âˆ’ 1 âˆ’ i| â‰¤ 15, and G(A) is
the union of these three disks.
(1) If A is strictly row diagonally dominant, that is
|ai i| >
nX
j=1, j6=i
|ai j |, for i = 1, . . . , n,
then A is invertible.
(2) If A is strictly row diagonally dominant, and if ai i > 0 for i = 1, . . . , n, then every
eigenvalue of A has a strictly positive real part.
Proof. Let Î» be any eigenvalue of A and let u be a corresponding eigenvector (recall that we
must have u 6 = 0). Let k be an index such that
|uk| = max
1â‰¤iâ‰¤n
|ui
|.
Since Au = Î»u, we have
(Î» âˆ’ ak k)uk =
nX
j=1
j6=k
ak juj
,
which implies that
|Î» âˆ’ ak k||uk| â‰¤
nX
j=1
j6=k
|ak j ||uj
| â‰¤ |uk|
nX
j=1
j6=k
|ak j |.
Since u 6 = 0 and |uk| = max1â‰¤iâ‰¤n |ui
|, we must have |uk| 6= 0, and it follows that
|Î» âˆ’ ak k| â‰¤
nX
j=1
j6=k
|ak j | = Rk
0
(A),
15.3. LOCATION OF EIGENVALUES 569
and thus
Î» âˆˆ {z âˆˆ C | |z âˆ’ ak k| â‰¤ Rk
0
(A)} âŠ† G(A),
as claimed.
(1) Strict row diagonal dominance implies that 0 does not belong to any of the Gershgorin
discs, so all eigenvalues of A are nonzero, and A is invertible.
(2) If A is strictly row diagonally dominant and ai i > 0 for i = 1, . . . , n, then each of the
Gershgorin discs lies strictly in the right half-plane, so every eigenvalue of A has a strictly
positive real part.
In particular, Theorem 15.9 implies that if a symmetric matrix is strictly row diagonally
dominant and has strictly positive diagonal entries, then it is positive definite. Theorem 15.9
is sometimes called the Gershgorinâ€“Hadamard theorem.
Since A and A> have the same eigenvalues (even for complex matrices) we also have a
version of Theorem 15.9 for the discs of radius
Cj
0
(A) =
nX
i=1
i6=j
|ai j |,
whose domain G(A> ) is given by
G(A
> ) =
n[
i=1
{z âˆˆ C | |z âˆ’ ai i| â‰¤ Ci
0
(A)}.
Figure 15.2 shows G(A> ) for A =
ï£«
ï£­
1 2 3
4
7 8 1 +
i 6
i
ï£¶
ï£¸.
10 5 0 5 10
10
5
5
10
Figure 15.2: Let A be the 3 Ã— 3 matrix specified at the end of Definition 15.5. For this
particular A, we find that C1
0
(A) = 11, C2
0
(A) = 10, and C3
0
(A) = 9. The pale blue disk is
|
union of these three disks.
z âˆ’ 1| â‰¤ 11, the pink disk is |z âˆ’ i| â‰¤ 10, the ocher disk is |z âˆ’ 1 âˆ’ i| â‰¤ 9, and G(A> ) is the
570 CHAPTER 15. EIGENVECTORS AND EIGENVALUES
Thus we get the following:
Theorem 15.10. For any complex n Ã— n matrix A, all the eigenvalues of A belong to the
intersection of the Gershgorin domains G(A) âˆ© G(A> ). See Figure 15.3. Furthermore the
following properties hold:
(1) If A is strictly column diagonally dominant, that is
|ai i| >
nX
i=1, i6=j
|ai j |, for j = 1, . . . , n,
then A is invertible.
(2) If A is strictly column diagonally dominant, and if ai i > 0 for i = 1, . . . , n, then every
eigenvalue of A has a strictly positive real part.
10 5 0 5 10 15
10
5
5
10
15
G(A ) T g G(A)
10 5 0 5 10 15
10
5
5
10
15
G(A ) T h G(A)
Figure 15.3: Let A be the 3 Ã— 3 matrix specified at the end of Definition 15.5. The colored
region in the second figure is G(A) âˆ© G(A> ).
There are refinements of Gershgorinâ€™s theorem and eigenvalue location results involving
other domains besides discs; for more on this subject, see Horn and Johnson [95], Sections
6.1 and 6.2.
Remark: Neither strict row diagonal dominance nor strict column diagonal dominance are
necessary for invertibility. Also, if we relax all strict inequalities to inequalities, then row
diagonal dominance (or column diagonal dominance) is not a sufficient condition for invertï¿¾ibility.
15.4. CONDITIONING OF EIGENVALUE PROBLEMS 571
15.4 Conditioning of Eigenvalue Problems
The following n Ã— n matrix
A =
ï£«
ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­
0
1 0
1 0
.
.
.
.
.
.
1 0
1 0
ï£¶
ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£¸
has the eigenvalue 0 with multiplicity n. However, if we perturb the top rightmost entry of
A by  , it is easy to see that the characteristic polynomial of the matrix
A( ) =
ï£«
ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­
0 
1 0
1 0
.
.
.
.
.
.
1 0
1 0
ï£¶
ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£¸
is Xn âˆ’  . It follows that if n = 40 and  = 10âˆ’40
, A(10âˆ’40) has the eigenvalues 10âˆ’1
e
k2Ï€i/40
with k = 1, . . . , 40. Thus, we see that a very small change ( = 10âˆ’40) to the matrix A causes
a significant change to the eigenvalues of A (from 0 to 10âˆ’1
e
k2Ï€i/40 ). Indeed, the relative
error is 10âˆ’39. Worse, due to machine precision, since very small numbers are treated as 0,
the error on the computation of eigenvalues (for example, of the matrix A(10âˆ’40)) can be
very large.
This phenomenon is similar to the phenomenon discussed in Section 9.5 where we studied
the effect of a small perturbation of the coefficients of a linear system Ax = b on its solution.
In Section 9.5, we saw that the behavior of a linear system under small perturbations is
governed by the condition number cond(A) of the matrix A. In the case of the eigenvalue
problem (finding the eigenvalues of a matrix), we will see that the conditioning of the problem
depends on the condition number of the change of basis matrix P used in reducing the matrix
A to its diagonal form D = P
âˆ’1AP, rather than on the condition number of A itself. The
following proposition in which we assume that A is diagonalizable and that the matrix norm
k k
satisfies a special condition (satisfied by the operator norms k k p
for p = 1, 2,âˆž), is due
to Bauer and Fike (1960).
Proposition 15.11. Let A âˆˆ Mn(C) be a diagonalizable matrix, P be an invertible matrix,
and D be a diagonal matrix D = diag(Î»1, . . . , Î»n) such that
A = P DP âˆ’1
,
572 CHAPTER 15. EIGENVECTORS AND EIGENVALUES
and let k k be a matrix norm such that
k
diag(Î±1, . . . , Î±n)k = max
1â‰¤iâ‰¤n
|Î±i
|,
for every diagonal matrix. Then for every perturbation matrix âˆ†A, if we write
Bi = {z âˆˆ C | |z âˆ’ Î»i
| â‰¤ cond(P) k âˆ†Ak},
for every eigenvalue Î» of A + âˆ†A, we have
Î» âˆˆ
n[
k=1
Bk.
Proof. Let Î» be any eigenvalue of the matrix A + âˆ†A. If Î» = Î»j
for some j, then the result
is trivial. Thus assume that Î» 6 = Î»j
for j = 1, . . . , n. In this case the matrix D âˆ’ Î»I is
invertible (since its eigenvalues are Î» âˆ’ Î»j
for j = 1, . . . , n), and we have
P
âˆ’1
(A + âˆ†A âˆ’ Î»I)P = D âˆ’ Î»I + P
âˆ’1
(âˆ†A)P
= (D âˆ’ Î»I)(I + (D âˆ’ Î»I)
âˆ’1P
âˆ’1
(âˆ†A)P).
Since Î» is an eigenvalue of A + âˆ†A, the matrix A + âˆ†A âˆ’ Î»I is singular, so the matrix
I + (D âˆ’ Î»I)
âˆ’1P
âˆ’1
(âˆ†A)P
must also be singular. By Proposition 9.11(2), we have
1 â‰¤
  (D âˆ’ Î»I)
âˆ’1P
âˆ’1
(âˆ†A)P
 ,
and since k k is a matrix norm,


(D âˆ’ Î»I)
âˆ’1P
âˆ’1
(âˆ†A)P
 â‰¤
  (D âˆ’ Î»I)
âˆ’1



 P
âˆ’1


k âˆ†Ak k Pk ,
so we have
1 â‰¤
  (D âˆ’ Î»I)
âˆ’1



 P
âˆ’1


k âˆ†Ak k Pk .
Now (D âˆ’ Î»I)
âˆ’1
is a diagonal matrix with entries 1/(Î»i âˆ’ Î»), so by our assumption on the
norm,


(D âˆ’ Î»I)
âˆ’1

 =
mini(|Î»
1
i âˆ’ Î»|)
.
As a consequence, since there is some index k for which mini(|Î»i âˆ’ Î»|) = |Î»k âˆ’ Î»|, we have


(D âˆ’ Î»I)
âˆ’1

 =
1
|Î»k âˆ’ Î»|
,
and we obtain
|Î» âˆ’ Î»k| â‰¤
  P
âˆ’1


k âˆ†Ak k Pk = cond(P) k âˆ†Ak ,
which proves our result.
15.5. EIGENVALUES OF THE MATRIX EXPONENTIAL 573
Proposition 15.11 implies that for any diagonalizable matrix A, if we define Î“(A) by
Î“(A) = inf{cond(P) | P
âˆ’1AP = D},
then for every eigenvalue Î» of A + âˆ†A, we have
Î» âˆˆ
n[
k=1
{z âˆˆ C
n
| |z âˆ’ Î»k| â‰¤ Î“(A) k âˆ†Ak}.
Definition 15.6. The number Î“(A) = inf{cond(P) | P
âˆ’1AP = D} is called the conditioning
of A relative to the eigenvalue problem.
If A is a normal matrix, since by Theorem 17.22, A can be diagonalized with respect
to a unitary matrix U, and since for the spectral norm k Uk 2 = 1, we see that Î“(A) = 1.
Therefore, normal matrices are very well conditionned w.r.t. the eigenvalue problem. In
fact, for every eigenvalue Î» of A + âˆ†A (with A normal), we have
Î» âˆˆ
n[
k=1
{z âˆˆ C
n
| |z âˆ’ Î»k| â‰¤ kâˆ†Ak 2
}.
If A and A+âˆ†A are both symmetric (or Hermitian), there are sharper results; see Proposition
17.28.
Note that the matrix A( ) from the beginning of the section is not normal.
15.5 Eigenvalues of the Matrix Exponential
The Schur decomposition yields a characterization of the eigenvalues of the matrix exponenï¿¾tial e
A in terms of the eigenvalues of the matrix A. First we have the following proposition.
Proposition 15.12. Let A and U be (real or complex) matrices and assume that U is
invertible. Then
e
UAUâˆ’1
= UeAU
âˆ’1
.
Proof. A trivial induction shows that
UApU
âˆ’1 = (UAU âˆ’1
)
p
,
and thus
e
UAUâˆ’1
=
X
pâ‰¥0
(UAU âˆ’1
)
p
p!
=
X
pâ‰¥0
UApU
âˆ’1
p!
= U
 
X
pâ‰¥0
Ap
p!
!
U
âˆ’1 = UeAU
âˆ’1
,
as claimed.
574 CHAPTER 15. EIGENVECTORS AND EIGENVALUES
Proposition 15.13. Given any complex n Ã— n matrix A, if Î»1, . . . , Î»n are the eigenvalues
of A, then e
Î»1
, . . . , eÎ»n are the eigenvalues of e
A. Furthermore, if u is an eigenvector of A
for Î»i, then u is an eigenvector of e
A for e
Î»i
.
Proof. By Theorem 15.5, there is an invertible matrix P and an upper triangular matrix T
such that
A = P T P âˆ’1
.
By Proposition 15.12,
e
P T P âˆ’1
= P eTP
âˆ’1
.
Note that e
T =
P pâ‰¥0
T
p
p!
is upper triangular since T
p
is upper triangular for all p â‰¥ 0. If
Î»1, Î»2, . . . , Î»n are the diagonal entries of T, the properties of matrix multiplication, when
combined with an induction on p, imply that the diagonal entries of T
p are Î»
p
1
, Î»p
2
, . . . , Î»p
n
.
This in turn implies that the diagonal entries of e
T are P pâ‰¥0
Î»
p
i
p! = e
Î»i
for 1 â‰¤ i â‰¤ n. Since
A and T are similar matrices, we know that they have the same eigenvalues, namely the
diagonal entries Î»1, . . . , Î»n of T. Since e
A = e
P T P âˆ’1
= P eTP
âˆ’1
, and e
T
is upper triangular,
we use the same argument to conclude that both e
A and e
T have the same eigenvalues, which
are the diagonal entries of e
T
, where the diagonal entries of e
T are of the form e
Î»1
, . . . , eÎ»n .
Now, if u is an eigenvector of A for the eigenvalue Î», a simple induction shows that u is an
eigenvector of An
for the eigenvalue Î»
n
, from which is follows that
e
Au =
 I +
1!
A
+
A2
2! +
A3
3! + . . .  u = u + Au +
A2
2! u +
A3
3! u + . . .
= u + Î»u +
Î»
2
2! u +
Î»
3
3! u + Â· Â· Â· =
 1 + Î» +
Î»
2
2! +
Î»
3
3! + . . .  u = e
Î»u,
which shows that u is an eigenvector of e
A for e
Î»
.
As a consequence, we obtain the following result.
Proposition 15.14. For every complex (or real) square matrix A, we have
det(e
A
) = e
tr(A)
,
where tr(A) is the trace of A, i.e., the sum a1 1 + Â· Â· Â· + an n of its diagonal entries.
Proof. The trace of a matrix A is equal to the sum of the eigenvalues of A. The determinant
of a matrix is equal to the product of its eigenvalues, and if Î»1, . . . , Î»n are the eigenvalues of
A, then by Proposition 15.13, e
Î»1
, . . . , eÎ»n are the eigenvalues of e
A, and thus
det ï¿¾ e
A
 = e
Î»1
Â· Â· Â· e
Î»n = e
Î»1+Â·Â·Â·+Î»n = e
tr(A)
,
as desired.
15.6. SUMMARY 575
If B is a skew symmetric matrix, since tr(B) = 0, we deduce that det(e
B) = e
0 = 1.
This allows us to obtain the following result. Recall that the (real) vector space of skew
symmetric matrices is denoted by so(n).
Proposition 15.15. For every skew symmetric matrix B âˆˆ so(n), we have e
B âˆˆ SO(n),
that is, e
B is a rotation.
Proof. By Proposition 9.23, e
B is an orthogonal matrix. Since tr(B) = 0, we deduce that
det(e
B) = e
0 = 1. Therefore, e
B âˆˆ SO(n).
Proposition 15.15 shows that the map B 7â†’ e
B is a map exp: so(n) â†’ SO(n). It is not
injective, but it can be shown (using one of the spectral theorems) that it is surjective.
If B is a (real) symmetric matrix, then
(e
B
)
> = e
B> = e
B
,
so e
B is also symmetric. Since the eigenvalues Î»1, . . . , Î»n of B are real, by Proposition
15.13, since the eigenvalues of e
B are e
Î»1
, . . . , eÎ»n and the Î»i are real, we have e
Î»i > 0 for
i = 1, . . . , n, which implies that e
B is symmetric positive definite. In fact, it can be shown
that for every symmetric positive definite matrix A, there is a unique symmetric matrix B
such that A = e
B; see Gallier [72].
15.6 Summary
The main concepts and results of this chapter are listed below:
â€¢ Diagonal matrix .
â€¢ Eigenvalues, eigenvectors; the eigenspace associated with an eigenvalue.
â€¢ Characteristic polynomial.
â€¢ Trace.
â€¢ Algebraic and geometric multiplicity.
â€¢ Eigenspaces associated with distinct eigenvalues form a direct sum (Proposition 15.3).
â€¢ Reduction of a matrix to an upper-triangular matrix.
â€¢ Schur decomposition.
â€¢ The Gershgorinâ€™s discs can be used to locate the eigenvalues of a complex matrix; see
Theorems 15.9 and 15.10.
â€¢ The conditioning of eigenvalue problems.
â€¢ Eigenvalues of the matrix exponential. The formula det(e
A) = e
tr(A)
.
576 CHAPTER 15. EIGENVECTORS AND EIGENVALUES
15.7 Problems
Problem 15.1. Let A be the following 2 Ã— 2 matrix
A =

1
1
âˆ’
âˆ’
1
1

.
(1) Prove that A has the eigenvalue 0 with multiplicity 2 and that A2 = 0.
(2) Let A be any real 2 Ã— 2 matrix
A =

a b
c d .
Prove that if bc > 0, then A has two distinct real eigenvalues. Prove that if a, b, c, d > 0,
then there is a positive eigenvector u associated with the largest of the two eigenvalues of A,
which means that if u = (u1, u2), then u1 > 0 and u2 > 0.
(3) Suppose now that A is any complex 2 Ã— 2 matrix as in (2). Prove that if A has the
eigenvalue 0 with multiplicity 2, then A2 = 0. Prove that if A is real symmetric, then A = 0.
Problem 15.2. Let A be any complex n Ã— n matrix. Prove that if A has the eigenvalue
0 with multiplicity n, then An = 0. Give an example of a matrix A such that An = 0 but
A 6 = 0.
Problem 15.3. Let A be a complex 2 Ã— 2 matrix, and let Î»1 and Î»2 be the eigenvalues of
A. Prove that if Î»1 6 = Î»2, then
e
A =
Î»1e
Î»2 âˆ’ Î»2e
Î»1
Î»1 âˆ’ Î»2
I +
e
Î»1 âˆ’ e
Î»2
Î»1 âˆ’ Î»2
A.
Problem 15.4. Let A be the real symmetric 2 Ã— 2 matrix
A =

a b
b c .
(1) Prove that the eigenvalues of A are real and given by
Î»1 =
a + c +
p 4b
2 + (a âˆ’ c)
2
2
, Î»2 =
a + c âˆ’
p 4b
2 + (a âˆ’ c)
2
2
.
(2) Prove that A has a double eigenvalue (Î»1 = Î»2 = a) if and only if b = 0 and a = c;
that is, A is a diagonal matrix.
(3) Prove that the eigenvalues of A are nonnegative iff b
2 â‰¤ ac and a + c â‰¥ 0.
(4) Prove that the eigenvalues of A are positive iff b
2 < ac, a > 0 and c > 0.
15.7. PROBLEMS 577
Problem 15.5. Find the eigenvalues of the matrices
A =

3 0
1 1 , B =

1 1
0 3 , C = A + B =

4 1
1 4 .
Check that the eigenvalues of A + B are not equal to the sums of eigenvalues of A plus
eigenvalues of B.
Problem 15.6. Let A be a real symmetric nÃ—n matrix and B be a real symmetric positive
definite n Ã— n matrix. We would like to solve the generalized eigenvalue problem: find Î» âˆˆ R
and u 6 = 0 such that
Au = Î»Bu. (âˆ—)
(1) Use the Cholesky decomposition B = CC> to show that Î» and u are solutions of
the generalized eigenvalue problem (âˆ—) iff Î» and v are solutions of the (ordinary) eigenvalue
problem
C
âˆ’1A(C
> )
âˆ’1
v = Î»v, with v = C
> u.
Check that C
âˆ’1A(C
> )
âˆ’1
is symmetric.
(2) Prove that if Au1 = Î»1Bu1, Au2 = Î»2Bu2, with u1 6 = 0, u2 6 = 0 and Î»1 6 = Î»2, then
u
>1 Bu2 = 0.
(3) Prove that Bâˆ’1A and C
âˆ’1A(C
> )
âˆ’1 have the same eigenvalues.
Problem 15.7. The sequence of Fibonacci numbers, 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, . . . , is
given by the recurrence
Fn+2 = Fn+1 + Fn,
with F0 = 0 and F1 = 1. In matrix form, we can write

Fn+1
Fn

=

1 1
1 0 
Fn
Fnâˆ’1

, n â‰¥ 1,

F
F
1
0

=

1
0

.
(1) Show that

Fn+1
Fn

=

1 1
1 0
n 
1
0

.
(2) Prove that the eigenvalues of the matrix
A =

1 1
1 0
are
Î» =
1 Â±
âˆš
5
2
.
The number
Ï• =
1 + âˆš
5
2
578 CHAPTER 15. EIGENVECTORS AND EIGENVALUES
is called the golden ratio. Show that the eigenvalues of A are Ï• and âˆ’Ï•
âˆ’1
.
(3) Prove that A is diagonalized as
A =

1 1
1 0 = âˆš
1
5

Ï•
1 1
âˆ’Ï•
âˆ’1
  Ï•
0 âˆ’Ï•
0
âˆ’1
 
1 Ï•
âˆ’1
âˆ’1 Ï•

.
Prove that

Fn+1
Fn

= âˆš
1
5

Ï•
1 1
âˆ’Ï•
âˆ’1
  Ï•
n
âˆ’(âˆ’Ï•
âˆ’1
)
n

,
and thus
Fn =
1
âˆš
5
(Ï•
n âˆ’ (âˆ’Ï•
âˆ’1
)
n
) = 1
âˆš
5
" 
1 +
2
âˆš
5
!
n
âˆ’
 
1 âˆ’
2
âˆš
5
!
n#
, n â‰¥ 0.
Problem 15.8. Let A be an n Ã— n matrix. For any subset I of {1, . . . , n}, let AI,I be the
matrix obtained from A by first selecting the columns whose indices belong to I, and then
the rows whose indices also belong to I. Prove that
Ï„k(A) = X
IâŠ†{1,...,n}
|I|=k
det(AI,I ).
Problem 15.9. (1) Consider the matrix
A =
ï£«
ï£­
0 0 âˆ’a3
1 0 âˆ’a2
0 1 âˆ’a1
ï£¶
ï£¸ .
Prove that the characteristic polynomial Ï‡A(z) = det(zI âˆ’ A) of A is given by
Ï‡A(z) = z
3 + a1z
2 + a2z + a3.
(2) Consider the matrix
A =
ï£«
ï£¬ï£¬ï£­
0 0 0 âˆ’a4
1 0 0 âˆ’a3
0 1 0 âˆ’a2
0 0 1 âˆ’a1
ï£¶
ï£·ï£·ï£¸ .
Prove that the characteristic polynomial Ï‡A(z) = det(zI âˆ’ A) of A is given by
Ï‡A(z) = z
4 + a1z
3 + a2z
2 + a3z + a4.
15.7. PROBLEMS 579
(3) Consider the n Ã— n matrix (called a companion matrix )
A =
ï£«
ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­
0 0 0 Â· Â· Â· 0 âˆ’an
1 0 0 Â· Â· Â· 0 âˆ’anâˆ’1
0 1 0 Â· Â· Â· 0 âˆ’anâˆ’2
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 .
.
. 0 âˆ’a2
0 0 0 Â· Â· Â· 1 âˆ’a1
ï£¶
ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£¸
.
Prove that the characteristic polynomial Ï‡A(z) = det(zI âˆ’ A) of A is given by
Ï‡A(z) = z
n + a1z
nâˆ’1 + a2z
nâˆ’2 + Â· Â· Â· + anâˆ’1z + an.
Hint. Use induction.
Explain why finding the roots of a polynomial (with real or complex coefficients) and
finding the eigenvalues of a (real or complex) matrix are equivalent problems, in the sense
that if we have a method for solving one of these problems, then we have a method to solve
the other.
Problem 15.10. Let A be a complex n Ã— n matrix. Prove that if A is invertible and if the
eigenvalues of A are (Î»1, . . . , Î»n), then the eigenvalues of Aâˆ’1 are (Î»
âˆ’
1
1
, . . . , Î»âˆ’
n
1
). Prove that
if u is an eigenvector of A for Î»i
, then u is an eigenvector of Aâˆ’1
for Î»
âˆ’
i
1
.
Problem 15.11. Prove that every complex matrix is the limit of a sequence of diagonalizable
matrices that have distinct eigenvalues.
Problem 15.12. Consider the following tridiagonal n Ã— n matrices
A =
ï£«
ï£¬ï£¬ï£¬ï£¬ï£¬ï£­
âˆ’
2
1 2
âˆ’1 0
âˆ’1
.
.
.
.
.
.
.
.
.
âˆ’1 2 âˆ’1
0 âˆ’1 2
ï£¶
ï£·ï£·ï£·ï£·ï£·ï£¸
, S =
ï£«
ï£¬ï£¬ï£¬ï£¬ï£¬ï£­
0 1 0
1 0 1
.
.
.
.
.
.
.
.
.
1 0 1
0 1 0
ï£¶
ï£·ï£·ï£·ï£·ï£·ï£¸
.
Observe that A = 2I âˆ’ S and show that the eigenvalues of A are Î»k = 2 âˆ’ Âµk, where the Âµk
