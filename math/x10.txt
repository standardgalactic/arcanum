
2 0
0 2
and for n â‰¥ 1,
Bn+1 = 2  Bn 0
0 I2n

.
Prove that
Wn
> Wn = Bn, for all n â‰¥ 1.
(3) The matrix Wn,i is obtained from the matrix Wi,i (1 â‰¤ i â‰¤ n âˆ’ 1) as follows:
Wn,i =

Wi,i 02
i
,2nâˆ’2
i
02nâˆ’2
i
,2
i I2nâˆ’2
i

.
It consists of four blocks, where 02
i
,2nâˆ’2
i and 02nâˆ’2
i
,2
i are matrices of zeros and I2nâˆ’2
i is the
identity matrix of dimension 2n âˆ’ 2
i
.
Explain what Wn,i does to c and prove that
Wn,nWn,nâˆ’1 Â· Â· Â· Wn,1 = Wn,
where Wn is the Haar matrix of dimension 2n
.
Hint. Use induction on k, with the induction hypothesis
Wn,kWn,kâˆ’1 Â· Â· Â· Wn,1 =

Wk 02
k,2nâˆ’2
k
02nâˆ’2
k,2
k I2nâˆ’2
k

.
5.8. PROBLEMS 165
Prove that the columns and rows of Wn,k are orthogonal, and use this to prove that the
columns of Wn and the rows of Wn
âˆ’1 are orthogonal. Are the rows of Wn orthogonal? Are
the columns of Wn
âˆ’1 orthogonal? Prove that
Wn,k
âˆ’1 =

1
2Wk,k
> 02
k,2nâˆ’2
k
02nâˆ’2
k,2
k I2nâˆ’2
k

.
Problem 5.3. Prove that if H is a Hadamard matrix of dimension n, then the block matrix
of dimension 2n, 
H H
H âˆ’H

,
is a Hadamard matrix.
Problem 5.4. Plot the graphs of the eight Walsh functions Wal(k, t) for k = 0, 1, . . . , 7.
Problem 5.5. Describe a recursive algorithm to compute the product H2m x of the Sylvesterâ€“
Hadamard matrix H2m by a vector x âˆˆ R
2m
that uses m recursive calls.
166 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES
Chapter 6
Direct Sums
In this chapter all vector spaces are defined over an arbitrary field K. For the sake of
concreteness, the reader may safely assume that K = R.
6.1 Sums, Direct Sums, Direct Products
There are some useful ways of forming new vector spaces from older ones, in particular,
direct products and direct sums. Regarding direct sums, there is a subtle point, which is
that if we attempt to define the direct sum E
` F of two vector spaces using the cartesian
product E Ã— F, we donâ€™t quite get the right notion because elements of E Ã— F are ordered
pairs, but we want E
` F = F
` E. Thus, we want to think of the elements of E
` F as
unordrered pairs of elements. It is possible to do so by considering the direct sum of a family
(Ei)iâˆˆ{1,2}, and more generally of a family (Ei)iâˆˆI . For simplicity, we begin by considering
the case where I = {1, 2}.
Definition 6.1. Given a family (Ei)iâˆˆ{1,2} of two vector spaces, we define the (external)
direct sum E1
` E2 (or coproduct) of the family (Ei)iâˆˆ{1,2} as the set
E1
a E2 = {{h1, ui ,h 2, vi} | u âˆˆ E1, v âˆˆ E2},
with addition
{h1, u1i ,h 2, v1i} + {h1, u2i ,h 2, v2i} = {h1, u1 + u2i ,h 2, v1 + v2i},
and scalar multiplication
Î»{h1, ui ,h 2, vi} = {h1, Î»ui ,h 2, Î»vi}.
We define the injections in1 : E1 â†’ E1
` E2 and in2 : E2 â†’ E1
` E2 as the linear maps
defined such that,
in1(u) = {h1, ui ,h 2, 0i},
and
in2(v) = {h1, 0i ,h 2, vi}.
167
168 CHAPTER 6. DIRECT SUMS
Note that
E2
a E1 = {{h2, vi ,h 1, ui} | v âˆˆ E2, u âˆˆ E1} = E1
a E2.
Thus, every member {h1, ui ,h 2, vi} of E1
` E2 can be viewed as an unordered pair consisting
of the two vectors u and v, tagged with the index 1 and 2, respectively.
Remark: In fact, E1
` E2 is just the product Q iâˆˆ{1,2} Ei of the family (Ei)iâˆˆ{1,2}.

This is not to be confused with the cartesian product E1 Ã—E2. The vector space E1 Ã—E2
is the set of all ordered pairs h u, vi , where u âˆˆ E1, and v âˆˆ E2, with addition and
multiplication by a scalar defined such that
h
u1, v1i + h u2, v2i = h u1 + u2, v1 + v2i ,
Î»h u, vi = h Î»u, Î»vi .
There is a bijection between Q iâˆˆ{1,2} Ei and E1 Ã— E2, but as we just saw, elements of
Q
iâˆˆ{1,2} Ei are certain sets. The product E1 Ã— Â· Â· Â· Ã— En of any number of vector spaces
can also be defined. We will do this shortly.
The following property holds.
Proposition 6.1. Given any two vector spaces, E1 and E2, the set E1
` E2 is a vector
space. For every pair of linear maps, f : E1 â†’ G and g : E2 â†’ G, there is a unique linear
map, f + g : E1
` E2 â†’ G, such that (f + g) â—¦ in1 = f and (f + g) â—¦ in2 = g, as in the
following diagram:
E1
in1


f
'
P
P
P
P
P P PP
P
P
P
P
P
P
P
P
E1
` E2
f+g
/
G
E2
in2
O
O
â™¥â™¥â™¥â™¥â™¥â™¥â™¥â™¥â™¥
g
â™¥â™¥â™¥â™¥â™¥â™¥â™¥7
Proof. Define
(f + g)({h1, ui ,h 2, vi}) = f(u) + g(v),
for every u âˆˆ E1 and v âˆˆ E2. It is immediately verified that f + g is the unique linear map
with the required properties.
We already noted that E1
` E2 is in bijection with E1 Ã— E2. If we define the projections
Ï€1 : E1
` E2 â†’ E1 and Ï€2 : E1
` E2 â†’ E2, such that
Ï€1({h1, ui ,h 2, vi}) = u,
and
Ï€2({h1, ui ,h 2, vi}) = v,
we have the following proposition.
6.1. SUMS, DIRECT SUMS, DIRECT PRODUCTS 169
Proposition 6.2. Given any two vector spaces, E1 and E2, for every pair of linear maps,
f : D â†’ E1 and g : D â†’ E2, there is a unique linear map, f Ã— g : D â†’ E1
` E2, such that
Ï€1 â—¦ (f Ã— g) = f and Ï€2 â—¦ (f Ã— g) = g, as in the following diagram:
E1
D
fÃ—g
/
f
â™¥â™¥â™¥â™¥â™¥â™¥â™¥â™¥â™¥â™¥â™¥â™¥â™¥â™¥â™¥â™¥7
P
P
P
P
P
P
P
g
P
P
P
P P P P
P
P
(
E1
` E2
Ï€1
O
O


Ï€2
E2
Proof. Define
(f Ã— g)(w) = {h1, f(w)i ,h 2, g(w)i},
for every w âˆˆ D. It is immediately verified that f Ã— g is the unique linear map with the
required properties.
Remark: It is a peculiarity of linear algebra that direct sums and products of finite families
are isomorphic. However, this is no longer true for products and sums of infinite families.
When U, V are subspaces of a vector space E, letting i1 : U â†’ E and i2 : V â†’ E be the
inclusion maps, if U
` V is isomomorphic to E under the map i1 + i2 given by Proposition
6.1, we say that E is a direct sum of U and V , and we write E = U
` V (with a slight abuse
of notation, since E and U
` V are only isomorphic). It is also convenient to define the sum
U1 + Â· Â· Â· + Up and the internal direct sum U1 âŠ• Â· Â· Â· âŠ• Up of any number of subspaces of E.
Definition 6.2. Given p â‰¥ 2 vector spaces E1, . . . , Ep, the product F = E1 Ã— Â· Â· Â· Ã— Ep can
be made into a vector space by defining addition and scalar multiplication as follows:
(u1, . . . , up) + (v1, . . . , vp) = (u1 + v1, . . . , up + vp)
Î»(u1, . . . , up) = (Î»u1, . . . , Î»up),
for all ui
, vi âˆˆ Ei and all Î» âˆˆ R. The zero vector of E1 Ã— Â· Â· Â· Ã— Ep is the p-tuple
( 0
|, . . . ,
{z 0
}
p
),
where the ith zero is the zero vector of Ei
.
With the above addition and multiplication, the vector space F = E1 Ã— Â· Â· Â· Ã— Ep is called
the direct product of the vector spaces E1, . . . , Ep.
As a special case, when E1 = Â· Â· Â· = Ep = R, we find again the vector space F = R
p
. The
projection maps pri
: E1 Ã— Â· Â· Â· Ã— Ep â†’ Ei given by
pri(u1, . . . , up) = ui
170 CHAPTER 6. DIRECT SUMS
are clearly linear. Similarly, the maps ini
: Ei â†’ E1 Ã— Â· Â· Â· Ã— Ep given by
ini(ui) = (0, . . . , 0, ui
, 0, . . . , 0)
are injective and linear. If dim(Ei) = ni and if (e
i
1
, . . . , ei
ni
) is a basis of Ei
for i = 1, . . . , p,
then it is easy to see that the n1 + Â· Â· Â· + np vectors
(e
1
1
, 0, . . . , 0), . . . , (e
1
n1
, 0, . . . , 0),
.
.
.
.
.
.
.
.
.
(0, . . . , 0, ei
1
, 0, . . . , 0), . . . , (0, . . . , 0, ei
ni
, 0, . . . , 0),
.
.
.
.
.
.
.
.
.
(0, . . . , 0, e
p
1
), . . . , (0, . . . , 0, ep
np
)
form a basis of E1 Ã— Â· Â· Â· Ã— Ep, and so
dim(E1 Ã— Â· Â· Â· Ã— Ep) = dim(E1) + Â· Â· Â· + dim(Ep).
Let us now consider a vector space E and p subspaces U1, . . . , Up of E. We have a map
a: U1 Ã— Â· Â· Â· Ã— Up â†’ E
given by
a(u1, . . . , up) = u1 + Â· Â· Â· + up,
with ui âˆˆ Ui
for i = 1, . . . , p. It is clear that this map is linear, and so its image is a subspace
of E denoted by
U1 + Â· Â· Â· + Up
and called the sum of the subspaces U1, . . . , Up. By definition,
U1 + Â· Â· Â· + Up = {u1 + Â· Â· Â· + up | ui âˆˆ Ui
, 1 â‰¤ i â‰¤ p},
and it is immediately verified that U1 + Â· Â· Â· + Up is the smallest subspace of E containing
U1, . . . , Up. This also implies that U1 + Â· Â· Â· + Up does not depend on the order of the factors
Ui
; in particular,
U1 + U2 = U2 + U1.
Definition 6.3. For any vector space E and any p â‰¥ 2 subspaces U1, . . . , Up of E, if the
map a defined above is injective, then the sum U1 + Â· Â· Â· + Up is called a direct sum and it is
denoted by
U1 âŠ• Â· Â· Â· âŠ• Up.
The space E is the direct sum of the subspaces Ui
if
E = U1 âŠ• Â· Â· Â· âŠ• Up.
6.1. SUMS, DIRECT SUMS, DIRECT PRODUCTS 171
As in the case of a sum, U1 âŠ• U2 = U2 âŠ• U1.
If the map a is injective, then by Proposition 3.17 we have Ker a = {( 0
|, . . . ,
{z 0
}
p
)} where
each 0 is the zero vector of E, which means that if ui âˆˆ Ui
for i = 1, . . . , p and if
u1 + Â· Â· Â· + up = 0,
then (u1, . . . , up) = (0, . . . , 0), that is, u1 = 0, . . . , up = 0.
Proposition 6.3. If the map a: U1 Ã— Â· Â· Â· Ã—Up â†’ E is injective, then every u âˆˆ U1 +Â· Â· Â·+Up
has a unique expression as a sum
u = u1 + Â· Â· Â· + up,
with ui âˆˆ Ui, for i = 1, . . . , p.
Proof. If
u = v1 + Â· Â· Â· + vp = w1 + Â· Â· Â· + wp,
with vi
, wi âˆˆ Ui
, for i = 1, . . . , p, then we have
w1 âˆ’ v1 + Â· Â· Â· + wp âˆ’ vp = 0,
and since vi
, wi âˆˆ Ui and each Ui
is a subspace, wi âˆ’vi âˆˆ Ui
. The injectivity of a implies that
wiâˆ’vi = 0, that is, wi = vi
for i = 1, . . . , p, which shows the uniqueness of the decomposition
of u.
Proposition 6.4. If the map a: U1 Ã— Â· Â· Â· Ã— Up â†’ E is injective, then any p nonzero vectors
u1, . . . , up with ui âˆˆ Ui are linearly independent.
Proof. To see this, assume that
Î»1u1 + Â· Â· Â· + Î»pup = 0
for some Î»i âˆˆ R. Since ui âˆˆ Ui and Ui
is a subspace, Î»iui âˆˆ Ui
, and the injectivity of a
implies that Î»iui = 0, for i = 1, . . . , p. Since ui 6 = 0, we must have Î»i = 0 for i = 1, . . . , p;
that is, u1, . . . , up with ui âˆˆ Ui and ui 6 = 0 are linearly independent.
Observe that if a is injective, then we must have Ui âˆ© Uj = (0) whenever i 6 = j. However,
this condition is generally not sufficient if p â‰¥ 3. For example, if E = R
2 and U1 the line
spanned by e1 = (1, 0), U2 is the line spanned by d = (1, 1), and U3 is the line spanned by
e2 = (0, 1), then U1âˆ©U2 = U1âˆ©U3 = U2âˆ©U3 = {(0, 0)}, but U1+U2 = U1+U3 = U2+U3 = R
2
,
so U1 + U2 + U3 is not a direct sum. For example, d is expressed in two different ways as
d = (1, 1) = (1, 0) + (0, 1) = e1 + e2.
172 CHAPTER 6. DIRECT SUMS
See Figure 6.1.
e
1 U1
e
U3
2 (1,1)
U2
Figure 6.1: The linear subspaces U1, U2, and U3 illustrated as lines in R
2
.
As in the case of a sum, U1 âŠ• U2 = U2 âŠ• U1. Observe that when the map a is injective,
then it is a linear isomorphism between U1 Ã— Â· Â· Â· Ã— Up and U1 âŠ• Â· Â· Â· âŠ• Up. The difference is
that U1 Ã— Â· Â· Â· Ã— Up is defined even if the spaces Ui are not assumed to be subspaces of some
common space.
If E is a direct sum E = U1âŠ•Â· Â· Â·âŠ•Up, since any p nonzero vectors u1, . . . , up with ui âˆˆ Ui
are linearly independent, if we pick a basis (uk)kâˆˆIj
in Uj
for j = 1, . . . , p, then (ui)iâˆˆI with
I = I1 âˆª Â· Â· Â· âˆª Ip is a basis of E. Intuitively, E is split into p independent subspaces.
Conversely, given a basis (ui)iâˆˆI of E, if we partition the index set I as I = I1 âˆª Â· Â· Â· âˆª Ip,
then each subfamily (uk)kâˆˆIj
spans some subspace Uj of E, and it is immediately verified
that we have a direct sum
E = U1 âŠ• Â· Â· Â· âŠ• Up.
Definition 6.4. Let f : E â†’ E be a linear map. For any subspace U of E, if f(U) âŠ† U we
say that U is invariant under f.
Assume that E is finite-dimensional, a direct sum E = U1 âŠ• Â· Â· Â· âŠ• Up, and that each Uj
is invariant under f. If we pick a basis (ui)iâˆˆI as above with I = I1 âˆª Â· Â· Â· âˆª Ip and with
each (uk)kâˆˆIj
a basis of Uj
, since each Uj
is invariant under f, the image f(uk) of every basis
vector uk with k âˆˆ Ij belongs to Uj
, so the matrix A representing f over the basis (ui)iâˆˆI is
a block diagonal matrix of the form
A =
ï£«
ï£¬ï£¬ï£¬ï£­
A1
A2
.
.
.
Ap
ï£¶
ï£·ï£·ï£·ï£¸
,
6.1. SUMS, DIRECT SUMS, DIRECT PRODUCTS 173
with each block Aj a dj Ã— dj -matrix with dj = dim(Uj ) and all other entries equal to 0. If
dj = 1 for j = 1, . . . , p, the matrix A is a diagonal matrix.
There are natural injections from each Ui to E denoted by ini
: Ui â†’ E.
Now, if p = 2, it is easy to determine the kernel of the map a: U1 Ã— U2 â†’ E. We have
a(u1, u2) = u1 + u2 = 0 iff u1 = âˆ’u2, u1 âˆˆ U1, u2 âˆˆ U2,
which implies that
Ker a = {(u, âˆ’u) | u âˆˆ U1 âˆ© U2}.
Now, U1 âˆ© U2 is a subspace of E and the linear map u 7â†’ (u, âˆ’u) is clearly an isomorphism
between U1 âˆ© U2 and Ker a, so Ker a is isomorphic to U1 âˆ© U2. As a consequence, we get the
following result:
Proposition 6.5. Given any vector space E and any two subspaces U1 and U2, the sum
U1 + U2 is a direct sum iff U1 âˆ© U2 = (0).
An interesting illustration of the notion of direct sum is the decomposition of a square
matrix into its symmetric part and its skew-symmetric part. Recall that an n Ã— n matrix
A âˆˆ Mn is symmetric if A> = A, skew -symmetric if A> = âˆ’A. It is clear that s
S(n) = {A âˆˆ Mn | A
> = A} and Skew(n) = {A âˆˆ Mn | A
> = âˆ’A}
are subspaces of Mn, and that S(n) âˆ© Skew(n) = (0). Observe that for any matrix A âˆˆ Mn,
the matrix H(A) = (A + A> )/2 is symmetric and the matrix S(A) = (A âˆ’ A> )/2 is skewï¿¾symmetric. Since
A = H(A) + S(A) = A + A>
2
+
A âˆ’ A>
2
,
we see that Mn = S(n) +Skew(n), and since S(n)âˆ©Skew(n) = (0), we have the direct sum
Mn = S(n) âŠ• Skew(n).
Remark: The vector space Skew(n) of skew-symmetric matrices is also denoted by so(n).
It is the Lie algebra of the group SO(n).
Proposition 6.5 can be generalized to any p â‰¥ 2 subspaces at the expense of notation.
The proof of the following proposition is left as an exercise.
Proposition 6.6. Given any vector space E and any p â‰¥ 2 subspaces U1, . . . , Up, the folï¿¾lowing properties are equivalent:
(1) The sum U1 + Â· Â· Â· + Up is a direct sum.
174 CHAPTER 6. DIRECT SUMS
(2) We have
Ui âˆ©

X
p
j=1,j6=i
Uj
 = (0), i = 1, . . . , p.
(3) We have
Ui âˆ©

iâˆ’1
X
j=1
Uj
 = (0), i = 2, . . . , p.
Because of the isomorphism
U1 Ã— Â· Â· Â· Ã— Up â‰ˆ U1 âŠ• Â· Â· Â· âŠ• Up,
we have
Proposition 6.7. If E is any vector space, for any (finite-dimensional) subspaces U1, . . .,
Up of E, we have
dim(U1 âŠ• Â· Â· Â· âŠ• Up) = dim(U1) + Â· Â· Â· + dim(Up).
If E is a direct sum
E = U1 âŠ• Â· Â· Â· âŠ• Up,
since every u âˆˆ E can be written in a unique way as
u = u1 + Â· Â· Â· + up
with ui âˆˆ Ui
for i = 1 . . . , p, we can define the maps Ï€i
: E â†’ Ui
, called projections, by
Ï€i(u) = Ï€i(u1 + Â· Â· Â· + up) = ui
.
It is easy to check that these maps are linear and satisfy the following properties:
Ï€j â—¦ Ï€i =
(
Ï€i
if i = j
0 if i 6 = j,
Ï€1 + Â· Â· Â· + Ï€p = idE.
For example, in the case of the direct sum
Mn = S(n) âŠ• Skew(n),
the projection onto S(n) is given by
Ï€1(A) = H(A) = A + A>
2
,
6.1. SUMS, DIRECT SUMS, DIRECT PRODUCTS 175
and the projection onto Skew(n) is given by
Ï€2(A) = S(A) = A âˆ’ A>
2
.
Clearly, H(A)+S(A) = A, H(H(A)) = H(A), S(S(A)) = S(A), and H(S(A)) = S(H(A)) =
0.
A function f such that f â—¦ f = f is said to be idempotent. Thus, the projections Ï€i are
idempotent. Conversely, the following proposition can be shown:
Proposition 6.8. Let E be a vector space. For any p â‰¥ 2 linear maps fi
: E â†’ E, if
fj â—¦ fi =
(
0
fi
if
if
i
i
6
=
=
j,
j
f1 + Â· Â· Â· + fp = idE,
then if we let Ui = fi(E), we have a direct sum
E = U1 âŠ• Â· Â· Â· âŠ• Up.
We also have the following proposition characterizing idempotent linear maps whose proof
is also left as an exercise.
Proposition 6.9. For every vector space E, if f : E â†’ E is an idempotent linear map, i.e.,
f â—¦ f = f, then we have a direct sum
E = Ker f âŠ• Im f,
so that f is the projection onto its image Im f.
We now give the definition of a direct sum for any arbitrary nonempty index set I. First,
let us recall the notion of the product of a family (Ei)iâˆˆI . Given a family of sets (Ei)iâˆˆI , its
product Q iâˆˆI Ei
, is the set of all functions f : I â†’
S iâˆˆI Ei
, such that, f(i) âˆˆ Ei
, for every
i âˆˆ I. It is one of the many versions of the axiom of choice, that, if Ei 6 = âˆ… for every i âˆˆ I,
then Q iâˆˆI Ei 6 = âˆ…. A member f âˆˆ
Q iâˆˆI Ei
, is often denoted as (fi)iâˆˆI . For every i âˆˆ I, we
have the projection Ï€i
:
Q
iâˆˆI Ei â†’ Ei
, defined such that, Ï€i((fi)iâˆˆI ) = fi
. We now define
direct sums.
Definition 6.5. Let I be any nonempty set, and let (Ei)iâˆˆI be a family of vector spaces.
The (external) direct sum ` iâˆˆI Ei (or coproduct) of the family (Ei)iâˆˆI is defined as follows:
`
iâˆˆI Ei consists of all f âˆˆ
Q iâˆˆI Ei
, which have finite support, and addition and multiï¿¾plication by a scalar are defined as follows:
(fi)iâˆˆI + (gi)iâˆˆI = (fi + gi)iâˆˆI ,
Î»(fi)iâˆˆI = (Î»fi)iâˆˆI .
We also have injection maps ini
: Ei â†’
` iâˆˆI Ei
, defined such that, ini(x) = (fi)iâˆˆI , where
fi = x, and fj = 0, for all j âˆˆ (I âˆ’ {i}).
176 CHAPTER 6. DIRECT SUMS
The following proposition is an obvious generalization of Proposition 6.1.
Proposition 6.10. Let I be any nonempty set, let (Ei)iâˆˆI be a family of vector spaces, and
let G be any vector space. The direct sum ` iâˆˆI Ei
is a vector space, and for every family
(hi)iâˆˆI of linear maps hi
: Ei â†’ G, there is a unique linear map

X
iâˆˆI
hi

:
a
iâˆˆI
Ei â†’ G,
such that, (
P
iâˆˆI
hi) â—¦ ini = hi, for every i âˆˆ I.
Remarks:
(1) One might wonder why the direct sum ` iâˆˆI Ei consists of familes of finite support
instead of arbitrary families; in other words, why didnâ€™t we define the direct sum of
the family (Ei)iâˆˆI as Q iâˆˆI Ei? The product space Q iâˆˆI Ei with addition and scalar
multiplication defined as above is also a vector space but the problem is that any
linear map b h:
Q
iâˆˆI Ei â†’ G such that b h â—¦ ini = hi
for all âˆˆ I must be given by
b
h((ui)âˆˆI ) = X
iâˆˆI
hi(ui),
and if I is infinite, the sum on the right-hand side is infinite, and thus undefined! If I
is finite then Q iâˆˆI Ei and ` iâˆˆI Ei are isomorphic.
(2) When Ei = E, for all i âˆˆ I, we denote ` iâˆˆI Ei by E
(I)
. In particular, when Ei = K,
for all i âˆˆ I, we find the vector space K(I) of Definition 3.11.
We also have the following basic proposition about injective or surjective linear maps.
Proposition 6.11. Let E and F be vector spaces, and let f : E â†’ F be a linear map. If
f : E â†’ F is injective, then there is a surjective linear map r : F â†’ E called a retraction,
such that r â—¦ f = idE. See Figure 6.2. If f : E â†’ F is surjective, then there is an injective
linear map s: F â†’ E called a section, such that f â—¦ s = idF . See Figure 6.3.
Proof. Let (ui)iâˆˆI be a basis of E. Since f : E â†’ F is an injective linear map, by Proposition
3.18, (f(ui))iâˆˆI is linearly independent in F. By Theorem 3.7, there is a basis (vj )jâˆˆJ of F,
where I âŠ† J, and where vi = f(ui), for all i âˆˆ I. By Proposition 3.18, a linear map r : F â†’ E
can be defined such that r(vi) = ui
, for all i âˆˆ I, and r(vj ) = w for all j âˆˆ (J âˆ’ I), where w
is any given vector in E, say w = 0. Since r(f(ui)) = ui
for all i âˆˆ I, by Proposition 3.18,
we have r â—¦ f = idE.
Now, assume that f : E â†’ F is surjective. Let (vj )jâˆˆJ be a basis of F. Since f : E â†’ F
is surjective, for every vj âˆˆ F, there is some uj âˆˆ E such that f(uj ) = vj
. Since (vj )jâˆˆJ is a
basis of F, by Proposition 3.18, there is a unique linear map s: F â†’ E such that s(vj ) = uj
.
Also, since f(s(vj )) = vj
, by Proposition 3.18 (again), we must have f â—¦ s = idF .
The converse of Proposition 6.11 is obvious.
6.2. MATRICES OF LINEAR MAPS AND MULTIPLICATION BY BLOCKS 177
u = (1,0) 1
u = (1,1)
2
f(x,y) = (x,y,0)
v = f(u ) = (1,0,0) 1 1 v = f(u ) = (1,1,0) 2 2
v = (0,0,1) 3
r(x,y,z) = (x,y)
E = R2
F = R3
Figure 6.2: Let f : E â†’ F be the injective linear map from R
2
to R
3 given by f(x, y) =
(x, y, 0). Then a surjective retraction is given by r : R
3 â†’ R
2
is given by r(x, y, z) = (x, y).
Observe that r(v1) = u1, r(v2) = u2, and r(v3) = 0 .
6.2 Matrices of Linear Maps and Multiplication by
Blocks
Direct sums yield a fairly easy justification of matrix block multiplication. The key idea
is that the representation of a linear map f : E â†’ F over a basis (u1, . . . , un) of E and
a basis (v1, . . . , vm) of F by a matrix A = (aij ) of scalars (in K) can be generalized to
the representation of f over a direct sum decomposition E = E1 âŠ• Â· Â· Â· âŠ• En of E and a
direct sum decomposition F = F1 âŠ• Â· Â· Â· âŠ• Fm of F in terms of a matrix (fij ) of linear maps
fij : Ej â†’ Fi
. Futhermore, matrix multiplication of scalar matrices extends naturally to
matrix multiplication of matrices of linear maps. We simply have to replace multiplication
of scalars in K by the composition of linear maps.
Let E and F be two vector spaces and assume that they are expressed as direct sums
E =
nM
j=1
Ej
, F =
mM
i=1
Fi
.
Definition 6.6. Given any linear map f : E â†’ F, we define the linear maps fij : Ej â†’ Fi
as follows. Let pri
F
: F â†’ Fi be the projection of F = F1 âŠ• Â· Â· Â· âŠ• Fm onto Fi
. If fj
: Ej â†’ F
is the restriction of f to Ej
, which means that for every vector xj âˆˆ Ej
,
fj (xj ) = f(xj ),
then we define the map fij : Ej â†’ Fi by
fij = pri
F
â—¦ fj
,
178 CHAPTER 6. DIRECT SUMS
so that if xj âˆˆ Ej
, then
f(xj ) = fj (xj ) =
mX
i=1
fij (xj ), with fij (xj ) âˆˆ Fi
. (â€ 1)
Observe that we are summing over the index i, which eventually corresponds to summing
the entries in the jth column of the matrix representing f; see Definition 6.7.
We see that for every vector x = x1 + Â· Â· Â· + xn âˆˆ E, with xj âˆˆ Ej
, we have
f(x) =
nX
j=1
fj (xj ) =
nX
j=1
mX
i=1
fij (xj ) =
mX
i=1
nX
j=1
fij (xj ).
Observe that conversely, given a family (fij )1â‰¤iâ‰¤m,1â‰¤jâ‰¤n of linear maps fij : Ej â†’ Fi
, we
obtain the linear map f : E â†’ F defined such that for every x = x1 + Â· Â· Â· + xn âˆˆ E, with
xj âˆˆ Ej
, we have
f(x) =
mX
i=1
nX
j=1
fij (xj ).
As a consequence, it is easy to check that there is an isomorphism between the vector
spaces
Hom(E, F) and Y
1â‰¤iâ‰¤m
1â‰¤jâ‰¤n
Hom(Ej
, Fi).
Example 6.1. Suppose that E = E1 âŠ•E2 and F = F1 âŠ•F2 âŠ•F3, and that we have six maps
fij : Ej â†’ Fi
for i = 1, 2, 3 and j = 1, 2. For any x = x1 + x2, with x1 âˆˆ E1 and x2 âˆˆ E2, we
have
y1 = f11(x1) + f12(x2) âˆˆ F1
y2 = f21(x1) + f22(x2) âˆˆ F2
y3 = f31(x1) + f32(x2) âˆˆ F3,
f1(x1) = f11(x1) + f21(x1) + f31(x1)
f2(x2) = f12(x2) + f22(x2) + f32(x2),
and
f(x) = f1(x1) + f2(x2) = y1 + y2 + y3
= f11(x1) + f12(x2) + f21(x1) + f22(x2) + f31(x1) + f32(x2).
These equations suggest the matrix notation
ï£«
ï£­
y
y
1
2
y3
ï£¶
ï£¸ =
ï£«
ï£­
f11 f12
f21 f22
f31 f32
ï£¶
ï£¸

x
x
1
2

.
6.2. MATRICES OF LINEAR MAPS AND MULTIPLICATION BY BLOCKS 179
In general we proceed as follows. For any x = x1 + Â· Â· Â· + xn with xj âˆˆ Ej
, if y = f(x),
since F = F1 âŠ• Â· Â· Â· âŠ• Fm, the vector y âˆˆ F has a unique decomposition y = y1 + Â· Â· Â· + ym
with yi âˆˆ Fi
, and since fij : Ej â†’ Fi
, we have P n
j=1 fij (xj ) âˆˆ Fi
, so P
n
j=1 fij (xj ) âˆˆ Fi
is the
ith component of f(x) over the direct sum F = F1 âŠ• Â· Â· Â· âŠ• Fm; equivalently
prF
i
(f(x)) =
nX
j=1
fij (xj ), 1 â‰¤ i â‰¤ m.
Consequently, we have
yi =
nX
j=1
fij (xj ), 1 â‰¤ i â‰¤ m. (â€ 2)
This time we are summing over the index j, which eventually corresponds to multiplying the
ith row of the matrix representing f by the n-tuple (x1, . . . , xn); see Definition 6.7.
All this suggests a generalization of the matrix notation Ax, where A is a matrix of
scalars and x is a column vector of scalars, namely to write
ï£«
ï£¬ï£­
y1
.
y
.
.
m
ï£¶
ï£·ï£¸ =
ï£«
ï£¬ï£­
f1 1 . . . f1 n
.
.
.
.
.
.
.
.
.
fm 1 . . . fm n
ï£¶
ï£·ï£¸
ï£«
ï£¬ï£­
x1
.
.
.
xn
ï£¶
ï£·ï£¸ , (â€ 3)
which is an abbreviation for the m equations
yi =
nX
j=1
fij (xj ), i = 1, . . . , m.
The interpretation of the multiplication of an m Ã— n matrix of linear maps fij by a column
vector of n vectors xj âˆˆ Ej
is to apply each fij to xj to obtain fij (xj ) and to sum over the
index j to obtain the ith output vector. This is the generalization of multiplying the scalar
aij by the scalar xj
. Also note that the jth column of the matrix (fij ) consists of the maps
(f1j
, . . . , fmj ) such that (f1j (xj ), . . . , fmj (xj )) are the components of f(xj ) = fj (xj ) over the
direct sum F = F1 âŠ• Â· Â· Â· âŠ• Fm.
In the special case in which each Ej and each Fi
is one-dimensional, this is equivalent
to choosing a basis (u1, . . . , un) in E so that Ej
is the one-dimensional subspace Ej = Kuj
,
and a basis (v1, . . . , vm) in Fj so that Fi
is the one-dimensional subspace Fi = Kvi
. In this
case every vector x âˆˆ E is expressed as x = x1u1 + Â· Â· Â· + xnun, where the xi âˆˆ K are scalars
and similarly every vector y âˆˆ F is expressed as y = y1v1 + Â· Â· Â· + ymvm, where the yi âˆˆ K
are scalars. Each linear map fij : Ej â†’ Fi
is a map between the one-dimensional spaces Kuj
and Kvi
, so it is of the form fij (xjuj ) = aijxjvi
, with xj âˆˆ K, and so the matrix (fij ) of
linear maps fij is in one-to-one correspondence with the matrix (aij ) of scalars in K, and
Equation (â€ 3) where the xj and yi are vectors is equivalent to the same familar equation
where the xj and yi are the scalar coordinates of x and y over the respective bases.
180 CHAPTER 6. DIRECT SUMS
Definition 6.7. Let E and F be two vector spaces and assume that they are expressed as
direct sums
E =
nM
j=1
Ej
, F =
mM
i=1
Fi
.
Given any linear map f : E â†’ F, if (fij )1â‰¤iâ‰¤m,1â‰¤jâ‰¤n is the familiy of linear maps fij : Ej â†’ Fi
defined in Definition 6.6, the m Ã— n matrix of linear maps
M(f) =
ï£«
ï£¬ï£­
f1 1 . . . f1 n
.
.
.
.
.
.
.
.
.
fm 1 . . . fm n
ï£¶
ï£·ï£¸
is called the matrix of f with respect to the decompositions L n
j=1 Ej
, and L m
i=1 Fi of E and
F as direct sums.
For any x = x1 + Â· Â· Â· + xn âˆˆ E with xj âˆˆ Ej and any y = y1 + Â· Â· Â· + ym âˆˆ F with yi âˆˆ Fi
,
we have y = f(x) iff
ï£«
ï£¬ï£­
y1
.
y
.
.
m
ï£¶
ï£·ï£¸ =
ï£«
ï£¬ï£­
f1 1 . . . f1 n
.
.
.
.
.
.
.
.
.
fm 1 . . . fm n
ï£¶
ï£·ï£¸
ï£«
ï£¬ï£­
x1
.
.
.
xn
ï£¶
ï£·ï£¸ ,
where the matrix equation above means that the system of m equations
yi =
nX
j=1
fij (xj ), i = 1 . . . , m, (â€ )
holds.
But now we can also promote matrix multiplication. Suppose we have a third space G
written as a direct sum. It is more convenient to write
E =
p
M
k=1
Ek, F =
nM
j=1
Fj
, G =
mM
i=1
Gi
.
Assume we also have two linear maps f : E â†’ F and g : F â†’ G. Now we have the n Ã— p
matrix of linear maps B = (fjk) and the m Ã— n matrix of linear maps A = (gij ). We would
like to find the m Ã— p matrix associated with g â—¦ f.
By definition of fk : Ek â†’ F and fjk : Ek â†’ Fj
, if xk âˆˆ Ek, then
fk(xk) = f(xk) =
nX
j=1
fjk(xk), with fjk(xk) âˆˆ Fj
, (âˆ—1)
and similarly, by definition of gj
: Fj â†’ G and gij : Fj â†’ Gi
, if yj âˆˆ Fj
, then
gj (yj ) = g(yj ) =
mX
i=1
gij (yj ), with gij (yj ) âˆˆ Gi
. (âˆ—2)
6.2. MATRICES OF LINEAR MAPS AND MULTIPLICATION BY BLOCKS 181
If we write h = g â—¦ f, we need to find the maps hk : Ek â†’ G, with
hk(xk) = h(xk) = (g â—¦ f)(xk),
and hik : Ek â†’ Gi given by
hik = pri
G â—¦ hk.
We have
hk(xk) = (g â—¦ f)(xk) = g(f(xk)) = g(fk(xk))
= g

nX
j=1
fjk(xk)
 by (âˆ—1)
=
nX
j=1
g(fjk(xk)) =
nX
j=1
gj (fjk(xk)) since g is linear
=
