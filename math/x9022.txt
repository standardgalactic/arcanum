49.10 Conjugate Gradient Methods for Unconstrained
Problems
The conjugate gradient method due to Hestenes and Stiefel (1952) is a gradient descent
method that applies to an elliptic quadratic functional J : R
n → R given by
J(v) = 1
2
h
Av, vi − hb, vi ,
where A is an n × n symmetric positive definite matrix. Although it is presented as an
iterative method, it terminates in at most n steps.
As usual, the conjugate gradient method starts with some arbitrary initial vector u0 and
proceeds through a sequence of iteration steps generating (better and better) approximations
uk of the optimal vector u minimizing J. During an iteration step, two vectors need to be
determined:
(1) The descent direction dk.
(2) The next approximation uk+1. To find uk+1, we need to find the stepsize ρk > 0 and
then
uk+1 = uk − ρkdk.
Typically, ρk is found by performing a line search along the direction dk, namely we
find ρk as the real number such that the function ρ 7→ J(uk − ρdk) is minimized.
1706 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
We saw in Proposition 49.13 that during execution of the gradient method with optimal
stepsize parameter that any two consecutive descent directions are orthogonal. The new
twist with the conjugate gradient method is that given u0, u1, . . . , uk, the next approximation
uk+1 is obtained as the solution of the problem which consists in minimizing J over the affine
subspace uk + Gk, where Gk is the subspace of R
n
spanned by the gradients
∇Ju0
, ∇Ju1
, . . . , ∇Juk
.
We may assume that ∇Ju` 6 = 0 for ` = 0, . . . , k, since the method terminates as soon as
∇Juk = 0. A priori the subspace Gk has dimension ≤ k + 1, but we will see that in fact it
has dimension k + 1. Then we have
uk + Gk =
 uk +
k
X
i=0
αi∇Jui




 αi ∈ R, 0 ≤ i ≤ k
 ,
and our minimization problem is to find uk+1 such that
uk+1 ∈ uk + Gk and J(uk+1) = inf
v∈uk+Gk
J(v).
In the gradient method with optimal stepsize parameter the descent direction dk is pro￾portional to the gradient ∇Juk
, but in the conjugate gradient method, dk is equal to ∇Juk
corrected by some multiple of dk−1.
The conjugate gradient method is superior to the gradient method with optimal stepsize
parameter for the following reasons proved correct later:
(a) The gradients ∇Jui
and ∇Juj
are orthogonal for all i, j with 0 ≤ i 6 = j ≤ k. This implies
that if ∇Jui
6 = 0 for i = 0, . . . , k, then the vectors ∇Jui
are linearly independent, so
the method stops in at most n steps.
(b) If we write ∆` = u` +1 − u` = −ρ` d` , the second remarkable fact about the conjugate
gradient method is that the vectors ∆` satisfy the following conditions:
h
A∆` , ∆ii = 0 0 ≤ i < ` ≤ k.
The vectors ∆` and ∆i are said to be conjugate with respect to the matrix A (or
A-conjugate). As a consequence, if ∆` 6 = 0 for ` = 0, . . . , k, then the vectors ∆` are
linearly independent.
(c) There is a simple formula to compute dk+1 from dk, and to compute ρk.
We now prove the above facts. We begin with (a).
Proposition 49.15. Assume that ∇Jui
6 = 0 for i = 0, . . . , k. Then the minimization prob￾lem, find uk+1 such that
uk+1 ∈ uk + Gk and J(uk+1) = inf
v∈uk+Gk
J(v),
has a unique solution, and the gradients ∇Jui
and ∇Juj
are orthogonal for all i, j with
0 ≤ i 6 = j ≤ k + 1.
49.10. CONJUGATE GRADIENT METHODS; UNCONSTRAINED PROBLEMS 1707
Proof. The affine space u` + G` is closed and convex, and since J is a quadratic elliptic
functional it is coercive and strictly convex, so by Theorem 49.8(2) it has a unique minimum
in u` + G` . This minimum u` +1 is also the minimum of the problem, find u` +1 such that
u` +1 ∈ u` + G` and J(u` +1) = inf
v∈G`
J(u` + v),
and since G` is a subspace, by Corollary 40.10 we must have
dJu` +1 (w) = 0 for all w ∈ G` ,
that is
h∇Ju` +1 , wi = 0 for all w ∈ G` .
Since G` is spanned by (∇Ju0
, ∇Ju1
, . . . , ∇Ju` ), we obtain
h∇Ju` +1 , ∇Juj
i = 0, 0 ≤ j ≤ `,
and since this holds for ` = 0, . . . , k, we get
h∇Jui
, ∇Juj
i = 0, 0 ≤ i 6 = j ≤ k + 1,
which shows the second part of the proposition.
As a corollary of Proposition 49.15, if ∇Jui
6 = 0 for i = 0, . . . , k, then the vectors ∇Jui
are
linearly independent and Gk has dimension k + 1. Therefore, the conjugate gradient method
terminates in at most n steps. Here is an example of a problem for which the gradient
descent with optimal stepsize parameter does not converge in a finite number of steps.
Example 49.2. Let J : R
2 → R be the function given by
J(v1, v2) = 1
2
(α1v1
2 + α2v2
2
),
where 0 < α1 < α2. The minimum of J is attained at (0, 0). Unless the initial vector
u0 = (u
0
1
, u0
2
) has the property that either u
0
1 = 0 or u
0
2 = 0, we claim that the gradient
descent with optimal stepsize parameter does not converge in a finite number of steps.
Observe that
∇J(v1,v2) =

α1v1
α2v2

.
As a consequence, given uk, the line search for finding ρk and uk+1 yields uk+1 = (0, 0) iff
there is some ρ ∈ R such that
u
k
1 = ρα1u
k
1
and u
k
2 = ρα2u
k
2
.
Since α1 6 = α2, this is only possible if either u
k
1 = 0 or u
k
2 = 0. The formulae given just before
Proposition 49.14 yield
u
k
1
+1 =
α2
2
(α2 − α1)u
k
1
(u
k
2
)
2
α1
3
(u
k
1
)
2 + α2
3
(u
k
2
)
2
, uk
2
+1 =
α1
2
(α1 − α2)u
k
2
(u
k
1
)
2
α1
3
(u
k
1
)
2 + α2
3
(u
k
2
)
2
,
which implies that if u
k
1 6 = 0 and u
k
2 6 = 0, then u
k
1
+1 6 = 0 and u
k
2
+1 6 = 0, so the method runs
forever from any initial vector u0 = (u
0
1
, u0
2
) such that u
0
1 6 = 0 and, u
0
2 6 = 0.
1708 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
We now prove (b).
Proposition 49.16. Assume that ∇Jui
6 = 0 for i = 0, . . . , k, and let ∆` = u` +1 − u` , for
`
= 0, . . . , k. Then ∆` 6 = 0 for ` = 0, . . . , k, and
h
A∆` , ∆ii = 0, 0 ≤ i < ` ≤ k.
The vectors ∆0, . . . , ∆k are linearly independent.
Proof. Since J is a quadratic functional we have
∇Jv+w = A(v + w) − b = Av − b + Aw = ∇Jv + Aw.
It follows that
∇Ju` +1 = ∇Ju` +∆` = ∇Ju` + A∆` , 0 ≤ ` ≤ k. (∗1)
By Proposition 49.15, since
h∇Jui
, ∇Juj
i = 0, 0 ≤ i 6 = j ≤ k,
we get
0 = h∇Ju` +1, ∇Ju` i = k∇Ju` k
2 + h A∆` , ∇Ju` i , ` = 0, . . . , k,
and since by hypothesis ∇Jui
6 = 0 for i = 0, . . . , k, we deduce that
∆` 6 = 0, ` = 0, . . . , k.
If k ≥ 1, for i = 0, . . . , ` − 1 and ` ≤ k we also have
0 = h∇Ju` +1 , ∇Jui
i = h∇Ju` , ∇Jui
i + h A∆` , ∇Jui
i
= h A∆` , ∇Jui
i
.
Since ∆j = uj+1 − uj ∈ Gj and Gj
is spanned by (∇Ju0
, ∇Ju1
, . . . , ∇Juj
), we obtain
h
A∆` , ∆j i = 0, 0 ≤ j < ` ≤ k.
For the last statement of the proposition, let w0, w1, . . . , wk be any k + 1 nonzero vectors
such that
h
Awi
, wj i = 0, 0 ≤ i < j ≤ k.
We claim that w0, w1, . . . , wk are linearly independent.
If we have a linear dependence P k
i=0 λiwi = 0, then we have
0 =  A

k
X
i=0
λiwi

, wj
 =
k
X
i=0
λih Awi
, wj i = λj h Awj
, wj i
,
where we form these inner products for j = 0, . . . , k, in that order. Since A is symmet￾ric positive definite (because J is a quadratic elliptic functional) and wj 6 = 0, we have
h
linearly independent.
Awj
, wj i > 0, and so λj = 0 for j = 0, . . . , k. Therefore the vectors w0, w1, . . . , wk are
49.10. CONJUGATE GRADIENT METHODS; UNCONSTRAINED PROBLEMS 1709
Remarks:
(1) Since A is symmetric positive definite, the bilinear map (u, v) 7→ hAu, vi is an inner
product h−, −iA on R
n
. Consequently, two vectors u, v are conjugate with respect to
the matrix A (or A-conjugate), which means that h Au, vi = 0, iff u and v are orthogonal
with respect to the inner product h−, −iA.
(2) By picking the descent direction to be −∇Juk
, the gradient descent method with
optimal stepsize parameter treats the level sets {u | J(u) = J(uk)} as if they were
spheres. The conjugate gradient method is more subtle, and takes the “geometry”
of the level set {u | J(u) = J(uk)} into account, through the notion of conjugate
directions.
(3) The notion of conjugate direction has its origins in the theory of projective conics and
quadrics where A is a 2 × 2 or a 3 × 3 matrix and where u and v are conjugate iff
u
> Av = 0.
(4) The terminology conjugate gradient is somewhat misleading. It is not the gradients
who are conjugate directions, but the descent directions.
By definition of the vectors ∆` = u` +1 − u` , we can write
∆` =
`
X
i=0
δi
` ∇Jui
, 0 ≤ ` ≤ k. (∗2)
In matrix form, we can write
￾
∆0 ∆1 · · · ∆k
 =
￾ ∇Ju0 ∇Ju1
· · · ∇Juk



δ
0
0
δ
1
0
· · · δ
k−1
0
δ
k
0
0 δ
1
1
· · · δ
k−1
1
δ
k
1
0 0 · · · δ2
k−1
δ2
k
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 · · · 0 δ
.
k
k


,
which implies that δ`
` 6 = 0 for ` = 0, . . . , k.
In view of the above fact, since ∆` and d` are collinear, it is convenient to write the
descent direction d` as
d` =
`
−1
X
i=0
λ
`i ∇Jui + ∇Ju` , 0 ≤ ` ≤ k. (∗3)
Our next goal is to compute uk+1, assuming that the coefficients λ
k
i
are known for i =
0, . . . , k, and then to find simple formulae for the λ
k
i
.
The problem reduces to finding ρk such that
J(uk − ρkdk) = inf
ρ∈R
J(uk − ρdk),
1710 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
and then uk+1 = uk − ρkdk. In fact, by (∗2), since
∆k =
k
X
i=0
δi
k ∇Jui = δk
k
 X
k
i=0
−1
δ
k
i
δ
k
k
∇Jui + ∇Juk

,
we must have
∆k = δk
k
dk and ρk = −δk
k
. (∗4)
Remarkably, the coefficients λ
k
i
and the descent directions dk can be computed easily
using the following formulae.
Proposition 49.17. Assume that ∇Jui
6 = 0 for i = 0, . . . , k. If we write
d` =
`
−1
X
i=0
λ
`i ∇Jui + ∇Ju` , 0 ≤ ` ≤ k,
then we have
(†)



λ
k
i =
k∇Juk
k
2
k∇Jui
k
2
, 0 ≤ i ≤ k − 1,
d0 = ∇Ju0
d` = ∇Ju` +
k∇Ju` k
2


∇Ju` −1


2
d` −1, 1 ≤ ` ≤ k.
Proof. Since by (∗4) we have ∆k = δk
kdk, δk
k 6 = 0, (by Proposition 49.16) we have
h
Ad` , ∆ii = 0, 0 ≤ i < ` ≤ k.
By (∗1) we have ∇Ju` +1 = ∇Ju` + A∆` , and since A is a symmetric matrix, we have
0 = h Adk, ∆` i = h dk, A∆` i = h dk, ∇Ju` +1 − ∇Ju` i ,
for ` = 0, . . . , k − 1. Since
dk =
k−1
X
i=0
λ
k
i ∇Jui + ∇Juk
,
we have

X
k
i=0
−1
λ
k
i ∇Jui + ∇Juk
, ∇Ju` +1 − ∇Ju`
 = 0, 0 ≤ ` ≤ k − 1.
Since by Proposition 49.15 the gradients ∇Jui
are pairwise orthogonal, the above equations
yield
−λ
k
k−1

 ∇Juk−1


2
+ k∇Juk
k
2 = 0
−λ
k
`
k∇Ju` k
2 + λ
k
`
+1
  ∇Ju` +1
 
2
= 0, 0 ≤ ` ≤ k − 2, k ≥ 2,
49.10. CONJUGATE GRADIENT METHODS; UNCONSTRAINED PROBLEMS 1711
and an easy induction yields
λ
k
i =
k∇Juk
k
2
k∇Jui
k
2
, 0 ≤ i ≤ k − 1.
Consequently, using (∗3) we have
dk =
k−1
X
i=0
k∇Juk
k
2
k∇Jui
k
2 ∇Jui + ∇Juk
= ∇Juk +
k∇Juk
k
2


∇Juk−1


2
 
X
k
i=0
−2

 ∇Juk−1


2
k∇Jui
k
2 ∇Jui + ∇Juk−1
!
= ∇Juk + 
k∇Juk
k
2

∇Juk−1


2
dk−1,
which concludes the proof.
It remains to compute ρk, which is the solution of the line search
J(uk − ρkdk) = inf
ρ∈R
J(uk − ρdk).
Since J is a quadratic functional, a basic computation left to the reader shows that the
function to be minimized is
ρ 7→
ρ
2
2
h
Adk, dki − ρh∇Juk
, dki + J(uk),
whose mininum is obtained when its derivative is zero, that is,
ρk =
h∇Juk
, dki
h
Adk, dki
. (∗5)
In summary, the conjugate gradient method finds the minimum u of the elliptic quadratic
functional
J(v) = 1
2
h
Av, vi − hb, vi
by computing the sequence of vectors u1, d1, . . . , uk−1, dk−1, uk, starting from any vector u0,
with
d0 = ∇Ju0
.
If ∇Ju0 = 0, then the algorithm terminates with u = u0. Otherwise, for k ≥ 0, assuming
that ∇Jui
6 = 0 for i = 1, . . . , k, compute
(∗6)



ρk =
h∇Juk
, dki
h
Adk, dki
uk+1 = uk − ρkdk
dk+1 = ∇Juk+1 +


∇Juk+1
 
2
k∇Juk
k
2
dk.
1712 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
If ∇Juk+1 = 0, then the algorihm terminates with u = uk+1.
As we showed before, the algorithm terminates in at most n iterations.
Example 49.3. Let us take the example of Section 49.6 and apply the conjugate gradient
procedure. Recall that
J(x, y) = 1
2
￾
x y  
3 2
2 6  
x
y

−
￾ 2 −8


x
y

=
3
2
x
2 + 2xy + 3y
2 − 2x + 8y.
Note that ∇Jv = (3x + 2y − 2, 2x + 6y + 8),
Initialize the procedure by setting
u0 = (−2, −2), d0 = ∇Ju0 = (−12, −8)
Step 1 involves calculating
ρ0 =
h∇Ju0
, d0i
h
Ad0, d0i
=
13
75
u1 = u0 − ρ0d0 = (−2, −2) −
13
75
(−12, −8) = 
25
2
, −
46
75
d1 = ∇Ju1 +
||∇Ju1
||2
||∇Ju0
||2
d0 =
 −
2912
625
,
18928
5625 
.
Observe that ρ0 and u1 are precisely the same as in the case the case of gradient descent with
optimal step size parameter. The difference lies in the calculation of d1. As we will see, this
change will make a huge difference in the convergence to the unique minimum u = (2, −2).
We continue with the conjugate gradient procedure and calculate Step 2 as
ρ1 =
h∇Ju1
, d1i
h
Ad1, d1i
=
75
82
u2 = u1 − ρ1d1 =

25
2
, −
46
75
−
75
82 
−
2912
625
,
18928
5625 
= (2, −2)
d2 = ∇Ju2 +
||∇Ju2
||2
||∇Ju1
||2
d1 = (0, 0).
Since ∇Ju2 = 0, the procedure terminates in two steps, as opposed to the 31 steps needed
for gradient descent with optimal step size parameter.
Hestenes and Stiefel realized that Equations (∗6) can be modified to make the computa￾tions more efficient, by having only one evaluation of the matrix A on a vector, namely dk.
The idea is to compute ∇uk
inductively.
49.10. CONJUGATE GRADIENT METHODS; UNCONSTRAINED PROBLEMS 1713
Since by (∗1) and (∗4) we have ∇Ju` +1 = ∇Ju` + A∆` = ∇Ju` − ρ` Ad` , the gradient
∇Ju` +1 can be computed iteratively:
∇J0 = Au0 − b
∇Ju` +1 = ∇Ju` − ρ` Ad` .
Since by Proposition 49.17 we have
dk = ∇Juk + 
k∇Juk
k
2

∇Juk−1


2
dk−1
and since dk−1 is a linear combination of the gradients ∇Jui
for i = 0, . . . , k − 1, which are
all orthogonal to ∇Juk
, we have
ρk =
h∇Juk
, dki
h
Adk, dki
=
k∇Juk
k
2
h
Adk, dki
.
It is customary to introduce the term rk defined as
rk = ∇Juk = Auk − b (∗7)
and to call it the residual. Then the conjugate gradient method consists of the following
steps. We intitialize the method starting from any vector u0 and set
d0 = r0 = Au0 − b.
The main iteration step is (k ≥ 0):
(∗8)



ρk =
k
rkk
2
uk+1 = u
h
Ad
k −
k, d
ρk
k
d
ik
rk+1 = rk − ρkAdk
βk+1 =
k
rk+1k
2
k
rkk
2
dk+1 = rk+1 + βk+1 dk.

Beware that some authors define the residual rk as rk = b−Auk and the descent direction
dk as −dk. In this case, the second equation becomes
uk+1 = uk + ρkdk.
Since d0 = r0, the equations
rk+1 = rk − ρkAdk
dk+1 = rk+1 + βk+1dk
1714 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
imply by induction that the subspace Gk is spanned by (r0, r1, . . . , rk) and (d0, d1, . . . , dk) is
the subspace spanned by
(r0, Ar0, A2
r0, . . . , Ak
r0).
Such a subspace is called a Krylov subspace.
If we define the error ek as ek = uk − u, then e0 = u0 − u and Ae0 = Au0 − Au =
Au0 − b = d0 = r0, and then because
uk+1 = uk − ρkdk
we see that
ek+1 = ek − ρkdk.
Since dk belongs to the subspace spanned by (r0, Ar0, A2
r0, . . . , Ak
r0) and r0 = Ae0, we see
that dk belongs to the subspace spanned by (Ae0, A2
e0, A3
e0, . . . , Ak+1e0), and then by induc￾tion we see that ek+1 belongs to the subspace spanned by (e0, Ae0, A2
e0, A3
e0, . . . , Ak+1e0).
This means that there is a polynomial Pk of degree ≤ k such that Pk(0) = 1 and
ek = Pk(A)e0.
This is an important fact because it allows for an analysis of the convergence of the
conjugate gradient method; see Trefethen and Bau [176] (Lecture 38). For this, since A is
symmetric positive definite, we know that h u, vi A = h Av, ui is an inner product on R
n whose
associated norm is denoted by k vk A
. Then observe that if e(v) = v − u, then
k
e(v)k
2
A = h Av − Au, v − ui
= h Av, vi − 2h Au, vi + h Au, ui
= h Av, vi − 2h b, vi + h b, ui
= 2J(v) + h b, ui .
It follows that v = uk minimizes k e(v)k A
on uk−1+Gk−1 since uk minimizes J on uk−1+Gk−1.
Since ek = Pk(A)e0 for some polynomial Pk of degree ≤ k such that Pk(0) = 1, if we let Pk
be the set of polynomials P(t) of degree ≤ k such that P(0) = 1, then we have
k
ekk A = inf
P ∈Pk
k
P(A)e0k A
.
Since A is a symmetric positive definite matrix it has real positive eigenvalues λ1, . . . , λn and
there is an orthonormal basis of eigenvectors h1, . . . , hn so that if we write e0 =
P
n
j=1 ajhj
.
then we have
k
e0k
2
A = h Ae0, e0i =

nX
i=1
aiλihi
,
nX
j=1
ajhj
 =
nX
j=1
a
2
jλj
and
k
P(A)e0k
2
A = h AP(A)e0, P(A)e0i =

nX
i=1
aiλiP(λi)hi
,
nX
j=1
ajP(λj )hj
 =
nX
j=1
a
2
jλj (P(λj ))2
.
49.10. CONJUGATE GRADIENT METHODS; UNCONSTRAINED PROBLEMS 1715
These equations imply that
k
ekk A ≤
 inf
P ∈Pk
max
1≤i≤n
|P(λi)|
 k e0k A
.
It can be shown that the conjugate gradient method requires of the order of
n
3 additions,
n
3 multiplications,
2n divisions.
In theory, this is worse than the number of elementary operations required by the
Cholesky method. Even though the conjugate gradient method does not seem to be the
best method for full matrices, it usually outperforms other methods for sparse matrices.
The reason is that the matrix A only appears in the computation of the vector Adk. If the
matrix A is banded (for example, tridiagonal), computing Adk is very cheap and there is no
need to store the entire matrix A, in which case the conjugate gradient method is fast. Also,
although in theory, up to n iterations may be required, in practice, convergence may occur
after a much smaller number of iterations.
Using the inequality
k
ekk A ≤
 inf
P ∈Pk
max
1≤i≤n
|P(λi)|
 k e0k A
,
by choosing P to be a shifted Chebyshev polynomial, it can be shown that
k
ekk A ≤ 2

√
√
κ
κ
−
+ 1
1

k
k
e0k A
,
where κ = cond2(A); see Trefethen and Bau [176] (Lecture 38, Theorem 38.5). Thus the
rate of convergence of the conjugate gradient method is governed by the ratio
p
cond2(A) − 1
p
cond2(A) + 1
,
where cond2(A) = λn/λ1 is the condition number of the matrix A. Since A is positive
definite, λ1 is its smallest eigenvalue and λn is its largest eigenvalue.
The above fact leads to the process of preconditioning, a method which consists in replac￾ing the matrix of a linear system Ax = b by an “equivalent” one for example M−1A (since
M is invertible, the system Ax = b is equivalent to the system M−1Ax = M−1
b), where M is
chosen so that M−1A is still symmetric positive definite and has a smaller condition number
than A; see Trefethen and Bau [176] (Lecture 40) and Demmel [48] (Section 6.6.5).
1716 CHAPTER 49. GENERAL RESULTS OF OPTIMIZATION THEORY
The method of conjugate gradients can be generalized to functionals that are not neces￾sarily quadratic. The stepsize parameter ρk is still determined by a line search which consists
in finding ρk such that
J(uk − ρkdk) = inf
ρ∈R
J(uk − ρdk).
This is more difficult than in the quadratic case and in general there is no guarantee that ρk
is unique, so some criterion to pick ρk is needed. Then
uk+1 = uk − ρkdk,
and the next descent direction can be chosen in two ways:
(1) (Polak–Ribi`ere)
dk = ∇Juk +
h∇Juk
,
 ∇Juk − ∇Juk−1
i

∇Juk−1


2
dk−1,
(2) (Fletcher–Reeves)
dk = ∇Juk + 
k∇Juk
k
2

∇Juk−1


2
dk−1.
Consecutive gradients are no longer orthogonal so these methods may run forever. There
are various sufficient criteria for convergence. In practice, the Polak–Ribi`ere method con￾verges faster. There is no longer any guarantee that these methods converge to a global
minimum.
49.11 Gradient Projection Methods for Constrained
Optimization
