)2t
(1 + t
2
)
2
=
1 âˆ’ 2t
2 âˆ’ 3t
4 âˆ’ 2t
2 + 2t
4
(1 + t
2
)
2
=
1 âˆ’ 4t
2 âˆ’ t
4
(1 + t
2
)
2
.
The nodal cubic passes through the origin for t = Â±1, and for t = âˆ’1 the tangent vector is
(1, âˆ’1), and for t = 1 the tangent vector is (âˆ’1, âˆ’1). The cone of feasible directions C(0)
at the origin is given by
C(0) = {(u1, u2) âˆˆ R
2
| u1 + u2 â‰¥ 0, |u1| â‰¥ |u2|}.
This is not a convex cone since it contains the sector delineated by the lines u2 = u1 and
u2 = âˆ’u1, but also the ray supported by the vector (âˆ’1, 1).
The two crucial properties of the cone of feasible directions are shown in the following
proposition.
50.1. THE CONE OF FEASIBLE DIRECTIONS 1733
(i.)
(ii.)
Figure 50.4: Figure (i.) illustrates U as the shaded gray region which lies between the line
y = âˆ’x and nodal cubic. Figure (ii.) shows the cone of feasible directions, C(0), as the
union of the turquoise triangular cone and the turquoise directional ray (âˆ’1, 1).
Proposition 50.1. Let U be any nonempty subset of a normed vector space V .
(1) For any u âˆˆ U, the cone C(u) of feasible directions at u is closed.
(2) Let J : â„¦ â†’ R be a function defined on an open subset â„¦ containing U. If J has a local
minimum with respect to the set U at a point u âˆˆ U, and if Ju
0
exists at u, then
Ju
0
(v âˆ’ u) â‰¥ 0 for all v âˆˆ u + C(u).
Proof. (1) Let (wn)nâ‰¥0 be a sequence of vectors wn âˆˆ C(u) converging to a limit w âˆˆ V . We
may assume that w 6 = 0, since 0 âˆˆ C(u) by definition, and thus we may also assume that
wn 6 = 0 for all n â‰¥ 0. By definition, for every n â‰¥ 0, there is a sequence (u
n
k
)kâ‰¥0 of vectors
in V and some wn 6 = 0 such that
(1) u
n
k âˆˆ U and u
n
k
6 = u for all k â‰¥ 0, and limk7â†’âˆž u
n
k = u.
(2) There is a sequence (Î´k
n
)kâ‰¥0 of vectors Î´k
n âˆˆ V such that
u
n
k = u + k u
n
k âˆ’ uk
wn
k
wnk
+ k u
n
k âˆ’ uk Î´k
n
, lim
k7â†’âˆž
Î´k
n = 0, wn 6 = 0.
Let ( n)nâ‰¥0 be a sequence of real numbers  n > 0 such that limn7â†’âˆž  n = 0 (for example,

n = 1/(n + 1)). Due to the convergence of the sequences (u
n
k
) and (Î´k
n
) for every fixed n,
1734 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
there exist an integer k(n) such that
Consider the sequence (u
n
k(n)
)n

â‰¥
u
0
n
k
. We have
(n) âˆ’ u
 â‰¤  n,
  Î´k
n
(n)

 â‰¤  n.
u
n
k(n) âˆˆ U, un
k(n) 6 = 0, for all n â‰¥ 0, limn7â†’âˆž
u
n
k(n) = u,
and we can write
u
n
k(n) = u +
  u
n
k(n) âˆ’ u

w
k
wk
+
  u
n
k(n) âˆ’ u

 Î´k
n
(n) +

wn
k
wnk
âˆ’
w
k
wk

.
Since limk7â†’âˆž(wn/ k wnk ) = w/ k wk , we conclude that w âˆˆ C(u). See Figure 50.5.
w1
w2
wn
w
u1
u1
1
2
u1
k
w1
w1
w
u2
uk
2
2
u2
1
uk
n
un
un
2
1
w
u
U
Figure 50.5: Let U be the mint green region in R
2 with u = (0, 0). Let (wn)nâ‰¥0 be a sequence
of vectors (points) along the upper dashed curve which converge to w. By following the
dashed orange longitudinal curves, and selecting an appropriate vector(point), we construct
the dark green curve in U, which passes through u, and at u has tangent vector proportional
to w.
(2) Let w = v âˆ’u be any nonzero vector in the cone C(u), and let (uk)k â‰¥0 be a sequence
of vectors in U âˆ’ {u} such that
(1) limk7â†’âˆž uk = u.
(2) There is a sequence (Î´k)kâ‰¥0 of vectors Î´k âˆˆ V such that
uk âˆ’ u = k uk âˆ’ uk
w
k
wk
+ k uk âˆ’ uk Î´k, lim
k7â†’âˆž
Î´k = 0, w 6 = 0,
(3) J(u) â‰¤ J(uk) for all k â‰¥ 0.
50.2. ACTIVE CONSTRAINTS AND QUALIFIED CONSTRAINTS 1735
Since J is differentiable at u, we have
0 â‰¤ J(uk) âˆ’ J(u) = Ju
0
(uk âˆ’ u) + k uk âˆ’ uk  k, (âˆ—)
for some sequence ( k)kâ‰¥0 such that limk7â†’âˆž  k = 0. Since Ju
0
is linear and continuous, and
since
uk âˆ’ u = k uk âˆ’ uk
w
k
wk
+ k uk âˆ’ uk Î´k, lim
k7â†’âˆž
Î´k = 0, w 6 = 0,
(âˆ—) implies that
0 â‰¤
k
uk âˆ’ uk
k
wk
(Ju
0
(w) + Î·k),
with
Î·k = k wk (Ju
0
(Î´k) +  k).
Since Ju
0
is continuous, we have limk7â†’âˆž Î·k = 0. But then Ju
0
(w) â‰¥ 0, since if Ju
0
(w) < 0,
then for k large enough the expression Ju
0
(w) + Î·k would be negative, and since uk 6 = u, the
expression (k uk âˆ’ uk / k wk )(Ju
0
(w) + Î·k) would also be negative, a contradiction.
From now on we assume that U is defined by a set of inequalities, that is
U = {x âˆˆ â„¦ | Ï•i(x) â‰¤ 0, 1 â‰¤ i â‰¤ m},
where the functions Ï•i
: â„¦ â†’ R are continuous (and usually differentiable). As we explained
earlier, an equality constraint Ï•i(x) = 0 is treated as the conjunction of the two inequalities
Ï•i(x) â‰¤ 0 and âˆ’Ï•i(x) â‰¤ 0. Later on we will see that when the functions Ï•i are convex, since
âˆ’
the time being we wonâ€™t.
Ï•i
is not necessarily convex, it is desirable to treat equality constraints separately, but for
50.2 Active Constraints and Qualified Constraints
Our next goal is find sufficient conditions for the cone C(u) to be convex, for any u âˆˆ U. For
this we assume that the functions Ï•i are differentiable at u. It turns out that the constraints
Ï•i that matter are those for which Ï•i(u) = 0, namely the constraints that are tight, or as
we say, active.
Definition 50.3. Given m functions Ï•i
: â„¦ â†’ R defined on some open subset â„¦ of some
vector space V , let U be the set defined by
U = {x âˆˆ â„¦ | Ï•i(x) â‰¤ 0, 1 â‰¤ i â‰¤ m}.
For any u âˆˆ U, a constraint Ï•i
is said to be active at u if Ï•i(u) = 0, else inactive at u if
Ï•i(u) < 0.
If a constraint Ï•i
is active at u, this corresponds to u being on a piece of the boundary
of U determined by some of the equations Ï•i(u) = 0; see Figure 50.6.
1736 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
y = x 2
y = x 2
(1,1)
(1/4, 1/2) w
w
(i.)
(ii.)
Figure 50.6: Let U be the light purple planar region which lies between the curves y = x
2 and
y
2 = x. Figure (i.) illustrates the boundary point (1, 1) given by the equalities yâˆ’x
2 = 0 and
y
2âˆ’x = 0. The affine translate of cone of feasible directions, C(1, 1), is illustrated by the pink
triangle whose sides are the tangent lines to the boundary curves. Figure (ii.) illustrates
the boundary point (1/4, 1/2) given by the equality y
2 âˆ’ x = 0. The affine translate of
C(1/4, 1/2) is the lilac half space bounded by the tangent line to y
2 = x through (1/4, 1/2).
Definition 50.4. For any u âˆˆ U, with
U = {x âˆˆ â„¦ | Ï•i(x) â‰¤ 0, 1 â‰¤ i â‰¤ m},
we define I(u) as the set of indices
I(u) = {i âˆˆ {1, . . . , m} | Ï•i(u) = 0}
where the constraints are active. We define the set C
âˆ—
(u) as
C
âˆ—
(u) = {v âˆˆ V | (Ï•
0i
)u(v) â‰¤ 0, i âˆˆ I(u)}.
Since each (Ï•
0i
)u is a linear form, the subset
C
âˆ—
(u) = {v âˆˆ V | (Ï•
0i
)u(v) â‰¤ 0, i âˆˆ I(u)}
y - 1 â‰¤ 1/2(x-1)
y - 1/2 â‰¤ x - 1/4
y - 1
â‰¥2(x-1)
50.2. ACTIVE CONSTRAINTS AND QUALIFIED CONSTRAINTS 1737
is the intersection of half spaces passing through the origin, so it is a convex set, and obviously
it is a cone. If I(u) = âˆ…, then C
âˆ—
(u) = V .
The special kinds of H-polyhedra of the form C
âˆ—
(u) cut out by hyperplanes through the
origin are called H-cones. It can be shown that every H-cone is a polyhedral cone (also
called a V-cone), and conversely. The proof is nontrivial; see Gallier [73] and Ziegler [195].
We will prove shortly that we always have the inclusion
C(u) âŠ† C
âˆ—
(u).
However, the inclusion can be strict, as in Example 50.1. Indeed for u = (0, 0) we have
I(0, 0) = {1, 2} and since
(Ï•
01
)(u1,u2) = (âˆ’1 âˆ’ 1), (Ï•
02
)(u1,u2) = (3u
2
1 + u
2
2 âˆ’ 2u1 2u1u2 + 2u2),
we have (Ï•
02
)(0,0) = (0 0), and thus C
âˆ—
(0) = {(u1, u2) âˆˆ R
2
| u1 + u2 â‰¥ 0} as illustrated in
Figure 50.7.
x
K2 K1 0 1 2
y
K2
K1
1
2
C (u) *
C(u)
Figure 50.7: For u = (0, 0), C
âˆ—
(u) is the sea green half space given by u1 + u2 â‰¥ 0. This
half space strictly contains C(u), namely the union of the turquoise triangular cone and the
directional ray (âˆ’1, 1).
The conditions stated in the following definition are sufficient conditions that imply that
C(u) = C
âˆ—
(u), as we will prove next.
Definition 50.5. For any u âˆˆ U, with
U = {x âˆˆ â„¦ | Ï•i(x) â‰¤ 0, 1 â‰¤ i â‰¤ m},
if the functions Ï•i are differentiable at u (in fact, we only this for i âˆˆ I(u)), we say that the
constraints are qualified at u if the following conditions hold:
1738 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
(a) Either the constraints Ï•i are affine for all i âˆˆ I(u), or
(b) There is some nonzero vector w âˆˆ V such that the following conditions hold for all
i âˆˆ I(u):
(i) (Ï•
0i
)u(w) â‰¤ 0.
(ii) If Ï•i
is not affine, then (Ï•
0i
)u(w) < 0.
Condition (b)(ii) implies that u is not a critical point of Ï•i
for every i âˆˆ I(u), so there
is no singularity at u in the zero locus of Ï•i
. Intuitively, if the constraints are qualified at u
then the boundary of U near u behaves â€œnicely.â€
The boundary points illustrated in Figure 50.6 are qualified. Observe that
U = {x âˆˆ R
2
| Ï•1(x, y) = y
2 âˆ’ x â‰¤ 0, Ï•2(x, y) = x
2 âˆ’ y â‰¤ 0}. For u = (1, 1), I(u) = {1, 2},
(Ï•
01
)(1,1) = (âˆ’1 2), (Ï•
02
)(1,1) = (2 âˆ’ 1), and w = (âˆ’1, âˆ’1) ensures that (Ï•
01
)(1,1) and (Ï•
01
)(1,1)
satisfy Condition (b) of Definition 50.5. For u = (1/4, 1/2), I(u) = {1}, (Ï•
01
)(1,1) = (âˆ’1 1),
and w = (1, 0) will satisfy Condition (b).
In Example 50.1, the constraint Ï•2(u1, u2) = 0 is not qualified at the origin because
(Ï•
02
)(0,0) = (0, 0); in fact, the origin is a self-intersection. In the example below, the origin is
also a singular point, but for a different reason.
Example 50.2. Consider the region U âŠ† R
2 determined by the two curves given by
Ï•1(u1, u2) = u2 âˆ’ max(0, u3
1
)
Ï•2(u1, u2) = u
4
1 âˆ’ u2.
We have I(0, 0) = {1, 2}, and since (Ï•1)
0
(0,0)(w1, w2) = (0 1)ï¿¾ w1
w2

= w2 and (Ï•
02
)(0,0)(w1, w2) =
(0 âˆ’ 1)ï¿¾ w1
w2

= âˆ’w2, we have C
âˆ—
(0) = {(u1, u2) âˆˆ R
2
| u2 = 0}, but the constraints are
not qualified at (0, 0) since it is impossible to have simultaneously (Ï•
01
)(0,0)(w1, w2) < 0 and
(Ï•
02
)(0,0)(w1, w2) < 0, so in fact C(0) = {(u1, u2) âˆˆ R
2
| u1 â‰¥ 0, u2 = 0} is strictly contained
in C
âˆ—
(0); see Figure 50.8.
Proposition 50.2. Let u be any point of the set
U = {x âˆˆ â„¦ | Ï•i(x) â‰¤ 0, 1 â‰¤ i â‰¤ m},
where â„¦ is an open subset of the normed vector space V , and assume that the functions Ï•i
are differentiable at u (in fact, we only this for i âˆˆ I(u)). Then the following facts hold:
(1) The cone C(u) of feasible directions at u is contained in the convex cone C
âˆ—
(u); that
is,
C(u) âŠ† C
âˆ—
(u) = {v âˆˆ V | (Ï•
0i
)u(v) â‰¤ 0, i âˆˆ I(u)}.
50.2. ACTIVE CONSTRAINTS AND QUALIFIED CONSTRAINTS 1739
Ï†
1
Ï† (u , u ) 1 2 (u , u ) 1 2
2
Ï†
2 (u , u ) 1 2
Ï†
1
(u , u ) 1 2
(i.)
(ii.)
Figure 50.8: Figures (i.) and (ii.) illustrate the purple moon shaped region associated with
Example 50.2. Figure (i.) also illustrates C(0), the cone of feasible directions, while Figure
(ii.) illustrates the strict containment of C(0) in C
âˆ—
(0).
(2) If the constraints are qualified at u (and the functions Ï•i are continuous at u for all
i /âˆˆ I(u) if we only assume Ï•i differentiable at u for all i âˆˆ I(u)), then
C(u) = C
âˆ—
(u).
Proof. (1) For every i âˆˆ I(u), since Ï•i(v) â‰¤ 0 for all v âˆˆ U and Ï•i(u) = 0, the function âˆ’Ï•i
has a local minimum at u with respect to U, so by Proposition 50.1(2), we have
(âˆ’Ï•
0i
)u(v) â‰¥ 0 for all v âˆˆ C(u),
which is equivalent to (Ï•
0i
)u(v) â‰¤ 0 for all v âˆˆ C(u) and for all i âˆˆ I(u), that is, u âˆˆ C
âˆ—
(u).
(2)(a) First, let us assume that Ï•i
is affine for every i âˆˆ I(u). Recall that Ï•i must be
given by Ï•i(v) = hi(v) + ci
for all v âˆˆ V , where hi
is a linear form and ci âˆˆ R. Since the
derivative of a linear map at any point is itself,
(Ï•
0i
)u(v) = hi(v) for all v âˆˆ V .
1740 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
Pick any nonzero w âˆˆ C
âˆ—
(u), which means that (Ï•
0i
)u(w) â‰¤ 0 for all i âˆˆ I(u). For any
sequence ( k)kâ‰¥0 of reals  k > 0 such that limk7â†’âˆž  k = 0, let (uk)kâ‰¥0 be the sequence of
vectors in V given by
uk = u +  kw.
We have uk âˆ’u =  kw 6 = 0 for all k â‰¥ 0 and limk7â†’âˆž uk = u. Furthermore, since the functions
Ï•i are continuous for all i /âˆˆ I, we have
0 > Ï•i(u) = lim
k7â†’âˆž
Ï•i(uk),
and since Ï•i
is affine and Ï•i(u) = 0 for all i âˆˆ I, we have Ï•i(u) = hi(u) + ci = 0, so
Ï•i(uk) = hi(uk) + ci = hi(uk) âˆ’ hi(u) = hi(uk âˆ’ u) = (Ï•
0i
)u(uk âˆ’ u) =  k(Ï•
0i
)u(w) â‰¤ 0, (âˆ—0)
which implies that uk âˆˆ U for all k large enough. Since
uk âˆ’ u
k
uk âˆ’ uk
=
w
k
wk
for all k â‰¥ 0,
we conclude that w âˆˆ C(u). See Figure 50.9.
w = (-1/3,-1/3)
u
u1
u2
u
u3
k
w
w
Figure 50.9: Let U be the peach triangle bounded by the lines y = 0, x = 0, and y = âˆ’x+ 1.
Let u satisfy the affine constraint Ï•(x, y) = y + x âˆ’ 1. Since Ï•
0(x,y) = (1 1), set w = (âˆ’1, âˆ’1)
and approach u along the line u + tw.
(2)(b) Let us now consider the case where some function Ï•i
is not affine for some i âˆˆ I(u).
Let w 6 = 0 be some vector in V such that Condition (b) of Definition 50.5 holds, namely: for
all i âˆˆ I(u), we have
(i) (Ï•
0i
)u(w) â‰¤ 0.
(ii) If Ï•i
is not affine, then (Ï•
0i
)u(w) < 0.
x + y - 1
= 0
50.2. ACTIVE CONSTRAINTS AND QUALIFIED CONSTRAINTS 1741
Pick any nonzero vector v âˆˆ C
âˆ—
(u), which means that (Ï•
0i
)u(v) â‰¤ 0 for all i âˆˆ I(u), and let
Î´ > 0 be any positive real number such that v + Î´w 6 = 0. For any sequence ( k)kâ‰¥0 of reals

k > 0 such that limk7â†’âˆž  k = 0, let (uk)kâ‰¥0 be the sequence of vectors in V given by
uk = u +  k(v + Î´w).
We have uk âˆ’ u =  k(v + Î´w) 6 = 0 for all k â‰¥ 0 and limk7â†’âˆž uk = u. Furthermore, since the
functions Ï•i are continuous for all i /âˆˆ I(u), we have
0 > Ï•i(u) = lim
k7â†’âˆž
Ï•i(uk) for all i /âˆˆ I(u). (âˆ—1)
Equation (âˆ—0) of the previous case shows that for all i âˆˆ I(u) such that Ï•i
is affine, since
(Ï•
0i
)u(v) â‰¤ 0, (Ï•
0i
)u(w) â‰¤ 0, and  k, Î´ > 0, we have
Ï•i(uk) =  k((Ï•
0i
)u(v) + Î´(Ï•
0i
)u(w)) â‰¤ 0 for all i âˆˆ I(u) and Ï•i affine. (âˆ—2)
Furthermore, since Ï•i
is differentiable and Ï•i(u) = 0 for all i âˆˆ I(u), if Ï•i
is not affine we
have
Ï•i(uk) =  k((Ï•
0i
)u(v) + Î´(Ï•
0i
)u(w)) +  k k uk âˆ’ uk Î·k(uk âˆ’ u)
with limk ukâˆ’uk7â†’0 Î·k(uk âˆ’ u) = 0, so if we write Î±k = k uk âˆ’ uk Î·k(uk âˆ’ u), we have
Ï•i(uk) =  k((Ï•
0i
)u(v) + Î´(Ï•
0i
)u(w) + Î±k)
with limk7â†’âˆž Î±k = 0, and since (Ï•
0i
)u(v) â‰¤ 0, we obtain
Ï•i(uk) â‰¤  k(Î´(Ï•
0i
)u(w) + Î±k) for all i âˆˆ I(u) and Ï•i not affine. (âˆ—3)
Equations (âˆ—1),(âˆ—2),(âˆ—3) show that uk âˆˆ U for k sufficiently large, where in (âˆ—3), since
(Ï•
0i
)u(w) < 0 and Î´ > 0, even if Î±k > 0, when limk7â†’âˆž Î±k = 0, we will have Î´(Ï•
0i
)u(w)+Î±k < 0
for k large enough, and thus  k(Î´(Ï•
0i
)u(w) + Î±k) < 0 for k large enough.
Since
uk âˆ’ u
k
uk âˆ’ uk
=
v + Î´w
k
v + Î´wk
for all k â‰¥ 0, we conclude that v +Î´w âˆˆ C(u) for Î´ > 0 small enough. But now the sequence
(vn)nâ‰¥0 given by
vn = v +  nw
converges to v, and for n large enough, vn âˆˆ C(u). Since by Proposition 50.1(1), the cone
C(u) is closed, we conclude that v âˆˆ C(u). See Figure 50.10.
In all cases, we proved that C
âˆ—
(u) âŠ† C(u), as claimed.
In the case of m affine constraints aix â‰¤ bi
, for some linear forms ai and some bi âˆˆ R,
for any point u âˆˆ R
n
such that aiu = bi
for all i âˆˆ I(u), the cone C(u) consists of all v âˆˆ R
n
such that aiv â‰¤ 0, so u + C(u) consists of all points u + v such that
ai(u + v) â‰¤ bi
for all i âˆˆ I(u),
which is the cone cut out by the hyperplanes determining some face of the polyhedron defined
by the m constraints aix â‰¤ bi
.
We are now ready to prove one of the most important results of nonlinear optimization.
1742 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
w
v
u1
u2
u3
uk
w
v
Î´
w
v
u
Î´w
v
u
Ï†
Ï†
â€˜
â€˜
1
1
(
(
)
)
u
u
â‰¤
â‰¤ 0
0
(i.)
(ii.)
Figure 50.10: Let U be the pink lounge in R
2
. Let u satisfy the non-affine constraint Ï•1(u).
Choose vectors v and w in the half space (Ï•
01
)u â‰¤ 0. Figure (i.) approaches u along the line
u + t(Î´w + v) and shows that v + Î´w âˆˆ C(u) for fixed Î´. Figure (ii.) varies Î´ in order that
the purple vectors approach v as Î´ â†’ âˆž.
50.3 The Karushâ€“Kuhnâ€“Tucker Conditions
If the domain U is defined by inequality constraints satisfying mild differentiability conditions
and if the constraints at u are qualified, then there is a necessary condition for the function
J to have a local minimum at u âˆˆ U involving generalized Lagrange multipliers. The proof
uses a version of Farkas lemma. In fact, the necessary condition stated next holds for infiniteï¿¾dimensional vector spaces because there a version of Farkas lemma holding for real Hilbert
spaces, but we will content ourselves with the version holding for finite dimensional normed
vector spaces. For the more general version, see Theorem 48.12 (or Ciarlet [41], Chapter 9).
We will be using the following version of Farkas lemma.
Proposition 50.3. (Farkas Lemma, Version I) Let A be a real mÃ—n matrix and let b âˆˆ R
m
be any vector. The linear system Ax = b has no solution x â‰¥ 0 iff there is some nonzero
50.3. THE KARUSHâ€“KUHNâ€“TUCKER CONDITIONS 1743
linear form y âˆˆ (R
m)
âˆ—
such that yA â‰¥ 0
>n and yb < 0.
We will use the version of Farkas lemma obtained by taking a contrapositive, namely: if
yA â‰¥ 0
>n
implies yb â‰¥ 0 for all linear forms y âˆˆ (R
m)
âˆ—
, then the linear system Ax = b has
some solution x â‰¥ 0.
Actually, it is more convenient to use a version of Farkas lemma applying to a Euclidean
vector space (with an inner product denoted hâˆ’, âˆ’i). This version also applies to an infinite
dimensional real Hilbert space; see Theorem 48.12. Recall that in a Euclidean space V the
inner product induces an isomorphism between V and V
0 , the space of continuous linear
forms on V . In our case, we need the isomorphism ] from V
0 to V defined such that for
every linear form Ï‰ âˆˆ V
0 , the vector Ï‰
] âˆˆ V is uniquely defined by the equation
Ï‰(v) = h v, Ï‰] i for all v âˆˆ V .
In R
n
, the isomorphism between R
n and (R
n
)
âˆ— amounts to transposition: if y âˆˆ (R
n
)
âˆ—
is
a linear form and v âˆˆ R
n
is a vector, then
yv = v
> y
> .
The version of the Farkasâ€“Minskowski lemma in term of an inner product is as follows.
Proposition 50.4. (Farkasâ€“Minkowski) Let V be a Euclidean space of finite dimension with
inner product hâˆ’, âˆ’i (more generally, a Hilbert space). For any finite family (a1, . . . , am) of
m vectors ai âˆˆ V and any vector b âˆˆ V , for any v âˆˆ V ,
if h ai
, vi â‰¥ 0 for i = 1, . . . , m implies that h b, vi â‰¥ 0,
then there exist Î»1, . . . , Î»m âˆˆ R such that
Î»i â‰¥ 0 for i = 1, . . . , m, and b =
mX
i=1
Î»iai
,
that is, b belong to the polyhedral cone cone(a1, . . . , am).
Proposition 50.4 is the special case of Theorem 48.12 which holds for real Hilbert spaces.
We can now prove the following theorem.
Theorem 50.5. Let Ï•i
: â„¦ â†’ R be m constraints defined on some open subset â„¦ of a finiteï¿¾dimensional Euclidean vector space V (more generally, a real Hilbert space V ), let J : â„¦ â†’ R
be some function, and let U be given by
U = {x âˆˆ â„¦ | Ï•i(x) â‰¤ 0, 1 â‰¤ i â‰¤ m}.
For any u âˆˆ U, let
I(u) = {i âˆˆ {1, . . . , m} | Ï•i(u) = 0},
1744 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
and assume that the functions Ï•i are differentiable at u for all i âˆˆ I(u) and continuous at u
for all i /âˆˆ I(u). If J is differentiable at u, has a local minimum at u with respect to U, and
if the constraints are qualified at u, then there exist some scalars Î»i(u) âˆˆ R for all i âˆˆ I(u),
such that
Ju
0 +
X
iâˆˆI(u)
Î»i(u)(Ï•
0i
)u = 0, and Î»i(u) â‰¥ 0 for all i âˆˆ I(u).
The above conditions are called the Karushâ€“Kuhnâ€“Tucker optimality conditions. Equivaï¿¾lently, in terms of gradients, the above conditions are expressed as
âˆ‡Ju +
X
iâˆˆI(u)
Î»i(u)âˆ‡(Ï•i)u = 0, and Î»i(u) â‰¥ 0 for all i âˆˆ I(u).
Proof. By Proposition 50.1(2), we have
Ju
0
(w) â‰¥ 0 for all w âˆˆ C(u), (âˆ—1)
and by Proposition 50.2(2), we have C(u) = C
âˆ—
(u), where
C
âˆ—
(u) = {v âˆˆ V | (Ï•
0i
)u(v) â‰¤ 0, i âˆˆ I(u)}, (âˆ—2)
so (âˆ—1) can be expressed as: for all w âˆˆ V ,
if w âˆˆ C
âˆ—
(u) then Ju
0
(w) â‰¥ 0,
or
if âˆ’ (Ï•
0i
)u(w) â‰¥ 0 for all i âˆˆ I(u), then Ju
0
(w) â‰¥ 0. (âˆ—3)
Under the isomorphism ] , the vector (Ju
0
)
] is the gradient âˆ‡Ju, so that
Ju
0
(w) = h w, âˆ‡Jui , (âˆ—4)
and the vector ((Ï•
0i
)u)
] is the gradient âˆ‡(Ï•i)u, so that
(Ï•
0i
)u(w) = h w, âˆ‡(Ï•i)ui . (âˆ—5)
Using Equations (âˆ—4) and (âˆ—5), Equation (âˆ—3) can be written as: for all w âˆˆ V ,
if h w, âˆ’âˆ‡(Ï•i)ui â‰¥ 0 for all i âˆˆ I(u), then h w, âˆ‡Jui â‰¥ 0. (âˆ—6)
By the Farkasâ€“Minkowski proposition (Proposition 50.4), there exist some sacalars Î»i(u) for
all i âˆˆ I(u), such that Î»i(u) â‰¥ 0 and
âˆ‡Ju =
X
iâˆˆI(u)
Î»i(u)(âˆ’âˆ‡(Ï•i)u),
that is
âˆ‡Ju +
X
iâˆˆI(u)
Î»i(u)âˆ‡(Ï•i)u = 0,
50.3. THE KARUSHâ€“KUHNâ€“TUCKER CONDITIONS 1745
and using the inverse of the isomorphism ] (which is linear), we get
Ju
0 +
X
iâˆˆI(u)
Î»i(u)(Ï•
0i
)u = 0,
as claimed.
Since the constraints are inequalities of the form Ï•i(x) â‰¤ 0, there is a way of expressing
the Karushâ€“Kuhnâ€“Tucker optimality conditions, often abbreviated as KKT conditions, in a
way that does not refer explicitly to the index set I(u):
Ju
0 +
mX
i=1
Î»i(u)(Ï•
0i
)u = 0, (KKT1)
and
mX
i=1
Î»i(u)Ï•i(u) = 0, Î»i(u) â‰¥ 0, i = 1, . . . , m. (KKT2)
Indeed, if we have the strict inequality Ï•i(u) < 0 (the constraint Ï•i
is inactive at u),
since all the terms Î»i(u)Ï•i(u) are nonpositive, we must have Î»i(u) = 0; that is, we only need
to consider the Î»i(u) for all i âˆˆ I(u). Yet another way to express the conditions in (KKT2)
is
Î»i(u)Ï•i(u) = 0, Î»i(u) â‰¥ 0, i = 1, . . . , m. (KKT02
)
In other words, for any i âˆˆ {1, . . . , m}, if Ï•i(u) < 0, then Î»i(u) = 0; that is,
â€¢ if the constraint Ï•i
is inactive at u, then Î»i(u) = 0.
By contrapositive, if Î»i(u) 6 = 0, then Ï•i(u) = 0; that is,
â€¢ if Î»i(u) 6 = 0, then the constraint Ï•i is active at u.
The conditions in (KKT02
) are referred to as complementary slackness conditions.
The scalars Î»i(u) are often called generalized Lagrange multipliers. If V = R
n
, the
necessary conditions of Theorem 50.5 are expressed as the following system of equations and
inequalities in the unknowns (u1, . . . , un) âˆˆ R
n and (Î»1, . . . , Î»m) âˆˆ R
m
+ :
âˆ‚J
âˆ‚x1
(u) + Î»1
âˆ‚Ï•1
âˆ‚x1
(u) + Â· Â· Â· + Î»m
âˆ‚Ï•m
âˆ‚x1
(u) = 0
.
.
.
.
.
.
âˆ‚J
âˆ‚xn
(u) + Î»1
âˆ‚Ï•n
âˆ‚x1
(u) + Â· Â· Â· + Î»m
âˆ‚Ï•m
âˆ‚xn
(u) = 0
Î»1Ï•1(u) + Â· Â· Â· + Î»mÏ•m(u) = 0
Ï•1(u) â‰¤ 0
.
.
.
.
.
.
Ï•m(u) â‰¤ 0
Î»1, . . . , Î»m â‰¥ 0.
1746 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
Example 50.3. Let J, Ï•1 and Ï•2 be the functions defined on R by
J(x) = x
Ï•1(x) = âˆ’x
Ï•2(x) = x âˆ’ 1.
In this case
U = {x âˆˆ R | âˆ’x â‰¤ 0, x âˆ’ 1 â‰¤ 0} = [0, 1].
Since the constraints are affine, they are automatically qualified for any u âˆˆ [0, 1]. The
system of equations and inequalities shown above becomes
1 âˆ’ Î»1 + Î»2 = 0
âˆ’Î»1x + Î»2(x âˆ’ 1) = 0
âˆ’x â‰¤ 0
x âˆ’ 1 â‰¤ 0
Î»1, Î»2 â‰¥ 0.
The first equality implies that Î»1 = 1 + Î»2. The second equality then becomes
âˆ’(1 + Î»2)x + Î»2(x âˆ’ 1) = 0,
which implies that Î»2 = âˆ’x. Since 0 â‰¤ x â‰¤ 1, or equivalently âˆ’1 â‰¤ âˆ’x â‰¤ 0, and Î»2 â‰¥ 0,
we conclude that Î»2 = 0 and Î»1 = 1 is the solution associated with x = 0, the minimum of
J(x) = x over [0, 1]. Observe that the case x = 1 corresponds to the maximum and not a
minimum of J(x) = x over [0, 1].
Remark: Unless the linear forms (Ï•
0i
)u for i âˆˆ I(u) are linearly independent, the Î»i(u) are
generally not unique. Also, if I(u) = âˆ…, then the KKT conditions reduce to Ju
0 = 0. This is
not surprising because in this case u belongs to the relative interior of U.
If the constraints are all affine equality constraints, then the KKT conditions are a bit
simpler. We will consider this case shortly.
The conditions for the qualification of nonaffine constraints are hard (if not impossible)
to use in practice, because they depend on u âˆˆ U and on the derivatives (Ï•
0i
)u. Thus it is
desirable to find simpler conditions. Fortunately, this is possible if the nonaffine functions
Ï•i are convex.
Definition 50.6. Let U âŠ† â„¦ âŠ† V be given by
U = {x âˆˆ â„¦ | Ï•i(x) â‰¤ 0, 1 â‰¤ i â‰¤ m},
where â„¦ is an open subset of the Euclidean vector space V . If the functions Ï•i
: â„¦ â†’ R are
convex, we say that the constraints are qualified if the following conditions hold:
50.3. THE KARUSHâ€“KUHNâ€“TUCKER CONDITIONS 1747
(a) Either the constraints Ï•i are affine for all i = 1, . . . , m and U 6 = âˆ…, or
(b) There is some vector v âˆˆ â„¦ such that the following conditions hold for i = 1, . . . , m:
(i) Ï•i(v) â‰¤ 0.
(ii) If Ï•i
is not affine, then Ï•i(v) < 0.
The above qualification conditions are known as Slaterâ€™s conditions.
Condition (b)(i) also implies that U has nonempty relative interior. If â„¦ is convex, then
U is also convex. This is because for all u, v âˆˆ â„¦, if u âˆˆ U and v âˆˆ U, that is Ï•i(u) â‰¤ 0 and
Ï•i(v) â‰¤ 0 for i = 1, . . . , m, since the functions Ï•i are convex, for all Î¸ âˆˆ [0, 1] we have
Ï•i((1 âˆ’ Î¸)u + Î¸v) â‰¤ (1 âˆ’ Î¸)Ï•i(u) + Î¸Ï•i(v) since Ï•i
is convex
â‰¤ 0 since 1 âˆ’ Î¸ â‰¥ 0, Î¸ â‰¥ 0, Ï•i(u) â‰¤ 0, Ï•i(v) â‰¤ 0,
and any intersection of convex sets is convex.

It is important to observe that a nonaffine equality constraint Ï•i(u) = 0 is never qualified.
Indeed, Ï•i(u) = 0 is equivalent to Ï•i(u) â‰¤ 0 and âˆ’Ï•i(u) â‰¤ 0, so if these constraints
are qualified and if Ï•i
is not affine then there is some nonzero vector v âˆˆ â„¦ such that both
Ï•i(v) < 0 and âˆ’Ï•i(v) < 0, which is impossible. For this reason, equality constraints are
often assumed to be affine.
The following theorem yields a more flexible version of Theorem 50.5 for constraints given
by convex functions. If in addition, the function J is also convex, then the KKT conditions
are also a sufficient condition for a local minimum.
Theorem 50.6. Let Ï•i
: â„¦ â†’ R be m convex constraints defined on some open convex subset
â„¦ of a finite-dimensional Euclidean vector space V (more generally, a real Hilbert space V ),
let J : â„¦ â†’ R be some function, let U be given by
U = {x âˆˆ â„¦ | Ï•i(x) â‰¤ 0, 1 â‰¤ i â‰¤ m},
and let u âˆˆ U be any point such that the functions Ï•i and J are differentiable at u.
(1) If J has a local minimum at u with respect to U, and if the constraints are qualified,
then there exist some scalars Î»i(u) âˆˆ R, such that the KKT condition hold:
Ju
0 +
mX
i=1
Î»i(u)(Ï•
0i
)u = 0
and
mX
i=1
Î»i(u)Ï•i(u) = 0, Î»i(u) â‰¥ 0, i = 1, . . . , m.
1748 CHAPTER 50. INTRODUCTION TO NONLINEAR OPTIMIZATION
Equivalently, in terms of gradients, the above conditions are expressed as
âˆ‡Ju +
mX
i=1
Î»i(u)âˆ‡(Ï•i)u = 0,
and
mX
i=1
Î»i(u)Ï•i(u) = 0, Î»i(u) â‰¥ 0, i = 1, . . . , m.
(2) Conversely, if the restriction of J to U is convex and if there exist scalars (Î»1, . . . , Î»m) âˆˆ
R
m
+ such that the KKT conditions hold, then the function J has a (global) minimum
at u with respect to U.
Proof. (1) It suffices to prove that if the convex constraints are qualified according to Defï¿¾inition 50.6, then they are qualified according to Definition 50.5, since in this case we can
apply Theorem 50.5.
If v âˆˆ â„¦ is a vector such that Condition (b) of Definition 50.6 holds and if v 6 = u, for any
i âˆˆ I(u), since Ï•i(u) = 0 and since Ï•i
is convex, by Proposition 40.11(1),
Ï•i(v) â‰¥ Ï•i(u) + (Ï•
