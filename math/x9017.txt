minimize yb
subject to yA â‰¥ c,
1622 CHAPTER 47. LINEAR PROGRAMMING AND DUALITY
where y âˆˆ (R
âˆ—
)
m. Since c â‰¤ 0, observe that y = 0>m is a feasible solution of the dual.
If a basic solution u of (P2) is found such that u â‰¥ 0, then cu = yb for y = cKA
âˆ’
K
1
,
and we have found an optimal solution u for (P2) and y for (D). The dual simplex method
makes progress by attempting to make negative components of u zero and by decreasing the
objective function of the dual program.
The dual simplex method starts with a basic solution (u, K) of Ax = b which is not
feasible but for which y = cKA
âˆ’
K
1
is dual feasible. In many cases the original linear program
is specified by a set of inequalities Ax â‰¤ b with some bi < 0, so by adding slack variables it is
easy to find such basic solution u, and if in addition c â‰¤ 0, then because the cost associated
with slack variables is 0, we see that y = 0 is a feasible solution of the dual.
Given a basic solution (u, K) of Ax = b (feasible or not), y = cKA
âˆ’
K
1
is dual feasible
iff cKA
âˆ’
K
1A â‰¥ c, and since cKA
âˆ’
K
1AK = cK, the inequality cKA
âˆ’
K
1A â‰¥ c is equivalent to
cKA
âˆ’
K
1AN â‰¥ cN , that is,
cN âˆ’ cKA
âˆ’
K
1AN â‰¤ 0, (âˆ—1)
where N = {1, . . . , n} âˆ’ K. Equation (âˆ—1) is equivalent to
cj âˆ’ cKÎ³K
j â‰¤ 0 for all j âˆˆ N, (âˆ—2)
where Î³K
j = A
âˆ’
K
1Aj
. Recall that the notation cj
is used to denote cj âˆ’ cKÎ³K
j
, which is called
the reduced cost of the variable xj
.
As in the simplex algorithm we need to decide which column Ak
leaves the basis K and
which column Aj
enters the new basis K+, in such a way that y
+ = cK+ A
âˆ’
K
1
+ is a feasible
solution of (D), that is, cN+ âˆ’ cK+ A
âˆ’
K
1
+ AN+ â‰¤ 0, where N + = {1, . . . , n} âˆ’ K+. We use
Proposition 46.2 to decide wich column k
âˆ’ should leave the basis.
Suppose (u, K) is a solution of Ax = b for which y = cKA
âˆ’
K
1
is dual feasible.
Case (A). If u â‰¥ 0, then u is an optimal solution of (P2).
Case (B). There is some k âˆˆ K such that uk < 0. In this case pick some k
âˆ’ âˆˆ K such
that ukâˆ’ < 0 (according to some pivot rule).
Case (B1). Suppose that Î³k
j
âˆ’ â‰¥ 0 for all j /âˆˆ K (in fact, for all j, since Î³k
j
âˆ’ âˆˆ {0, 1} for
all j âˆˆ K). If so, we we claim that (P2) is not feasible.
Indeed, let v be some basic feasible solution. We have v â‰¥ 0 and Av = b, that is,
nX
j=1
vjA
j = b,
so by multiplying both sides by A
âˆ’
K
1
and using the fact that by definition Î³K
j = A
âˆ’
K
1Aj
, we
obtain
nX
j=1
vjÎ³K
j = A
âˆ’
K
1
b = uK.
47.5. THE DUAL SIMPLEX ALGORITHM 1623
But recall that by hypothesis ukâˆ’ < 0, yet vj â‰¥ 0 and Î³k
j
âˆ’ â‰¥ 0 for all j, so the component of
index k
âˆ’ is zero or positive on the left, and negative on the right, a contradiction. Therefore,
(P2) is indeed not feasible.
Case (B2). We have Î³k
j
âˆ’ < 0 for some j.
We pick the column Aj
entering the basis among those for which Î³k
j
âˆ’ < 0. Since we
assumed that cj âˆ’ cKÎ³K
j â‰¤ 0 for all j âˆˆ N by (âˆ—2), consider
Âµ
+ = max âˆ’
cj âˆ’ cKÎ³K
j
Î³k
j
âˆ’




Î³k
j
âˆ’ < 0, j âˆˆ N
 = max âˆ’
cj
Î³
j
kâˆ’




Î³k
j
âˆ’ < 0, j âˆˆ N
 â‰¤ 0,
and the set
N(Âµ
+) =  j âˆˆ N

 
 âˆ’
cj
Î³
j
kâˆ’
= Âµ
+
 .
We pick some index j
+ âˆˆ N(Âµ
+) as the index of the column entering the basis (using
some pivot rule).
Recall that by hypothesis ci âˆ’ cKÎ³K
i â‰¤ 0 for all j /âˆˆ K and ci âˆ’ cKÎ³K
i = 0 for all i âˆˆ K.
Since Î³
j
+
kâˆ’ < 0, for any index i such that Î³k
i
âˆ’ â‰¥ 0, we have âˆ’Î³k
i
âˆ’ /Î³j
+
kâˆ’ â‰¥ 0, and since by
Proposition 46.2
ci âˆ’ cK+ Î³
i
K+ = ci âˆ’ cKÎ³
i
K âˆ’
Î³k
i
âˆ’
Î³k
j+
âˆ’
(cj+ âˆ’ cKÎ³
j
+
K ),
we have ci âˆ’ cK+ Î³K
i
+ â‰¤ 0. For any index i such that Î³k
i
âˆ’ < 0, by the choice of j
+ âˆˆ Kâˆ—
,
âˆ’
ci âˆ’ cKÎ³K
i
Î³k
i
âˆ’
â‰¤ âˆ’
cj+ âˆ’ cKÎ³
j
+
K
Î³
j+
kâˆ’
,
so
ci âˆ’ cKÎ³
i
K âˆ’
Î³k
i
âˆ’
Î³k
j+
âˆ’
(cj+ âˆ’ cKÎ³
j
+
K ) â‰¤ 0,
and again, ciâˆ’cK+ Î³K
i
+ â‰¤ 0. Therefore, if we let K+ = (Kâˆ’{k
âˆ’})âˆª{j
+}, then y
+ = cK+ A
âˆ’
K
1
+
is dual feasible. As in the simplex algorithm, Î¸
+ is given by
Î¸
+ = ukâˆ’ /Î³j
+
kâˆ’ â‰¥ 0,
and u
+ is also computed as in the simplex algorithm by
u
+
i =
ï£±
ï£´ï£²
ï£´ï£³
ui âˆ’ Î¸
j
+
Î³i
j
+
if i âˆˆ K
Î¸
j
+
if i = j
+
0 if i /âˆˆ K âˆª {j
+}
.
1624 CHAPTER 47. LINEAR PROGRAMMING AND DUALITY
The change in the objective function of the primal and dual program (which is the same,
since uK = A
âˆ’
K
1
b and y = cKA
âˆ’
K
1
is chosen such that cu = cKuK = yb) is the same as in the
simplex algorithm, namely
Î¸
+
 c
j
+
âˆ’ cKÎ³K
j
+

.
We have Î¸
+ > 0 and c
j
+
âˆ’ cKÎ³
j
+
K â‰¤ 0, so if c
j
+
âˆ’ cKÎ³
j
+
K < 0, then the objective function of
the dual program decreases strictly.
Case (B3). Âµ
+ = 0.
The possibity that Âµ
+ = 0, that is, c
j
+
âˆ’cKÎ³
j
+
K = 0, may arise. In this case, the objective
function doesnâ€™t change. This is a case of degeneracy similar to the degeneracy that arises
in the simplex algorithm. We still pick j
+ âˆˆ N(Âµ
+), but we need a pivot rule that prevents
cycling. Such rules exist; see Bertsimas and Tsitsiklis [21] (Section 4.5) and Papadimitriou
and Steiglitz [134] (Section 3.6).
The reader surely noticed that the dual simplex algorithm is very similar to the simplex
algorithm, except that the simplex algorithm preserves the property that (u, K) is (primal)
feasible, whereas the dual simplex algorithm preserves the property that y = cKA
âˆ’
K
1
is dual
feasible. One might then wonder whether the dual simplex algorithm is equivalent to the
simplex algorithm applied to the dual problem. This is indeed the case, there is a one-to-one
correspondence between the dual simplex algorithm and the simplex algorithm applied to
the dual problem in maximization form. This correspondence is described in Papadimitriou
and Steiglitz [134] (Section 3.7).
The comparison between the simplex algorithm and the dual simplex algorithm is best
illustrated if we use a description of these methods in terms of (full) tableaux .
Recall that a (full) tableau is an (m + 1) Ã— (n + 1) matrix organized as follows:
âˆ’cKuK c1 Â· Â· Â· cj
Â· Â· Â· cn
uk1 Î³1
1
Â· Â· Â· Î³1
j
Â· Â· Â· Î³1
n
.
.
.
.
.
.
.
.
.
.
.
.
ukm Î³m
1
Â· Â· Â· Î³m
j
Â· Â· Â· Î³m
n
The top row contains the current value of the objective function and the reduced costs,
the first column except for its top entry contain the components of the current basic solution
uK, and the remaining columns except for their top entry contain the vectors Î³K
j
. Observe
that the Î³K
j
corresponding to indices j in K constitute a permutation of the identity matrix
Im. A tableau together with the new basis K+ = (K âˆ’ {k
âˆ’}) âˆª {j
+} contains all the data
needed to compute the new uK+ , the new Î³K
j
+ , and the new reduced costs ci âˆ’(Î³k
i
âˆ’ /Î³j
+
kâˆ’ )cj+ .
When executing the simplex algorithm, we have uk â‰¥ 0 for all k âˆˆ K (and uj = 0 for
all j /âˆˆ K), and the incoming column j
+ is determined by picking one of the column indices
such that cj > 0. Then the index k
âˆ’ of the leaving column is determined by looking at the
minimum of the ratios uk/Î³j
+
k
for which Î³
j
+
k > 0 (along column j
+).
47.5. THE DUAL SIMPLEX ALGORITHM 1625
On the other hand, when executing the dual simplex algorithm, we have cj â‰¤ 0 for all
j /âˆˆ K (and ck = 0 for all k âˆˆ K), and the outgoing column k
âˆ’ is determined by picking one
of the row indices such that uk < 0. The index j
+ of the incoming column is determined by
looking at the maximum of the ratios âˆ’cj/Î³k
j
âˆ’ for which Î³k
j
âˆ’ < 0 (along row k
âˆ’).
More details about the comparison between the simplex algorithm and the dual simplex
algorithm can be found in Bertsimas and Tsitsiklis [21] and Papadimitriou and Steiglitz
[134].
Here is an example of the the dual simplex method.
Example 47.2. Consider the following linear program in standard form:
Maximize âˆ’ 4x1 âˆ’ 2x2 âˆ’ x3
subject to
ï£«
ï£­
âˆ’
âˆ’
1 1
1
4
âˆ’
âˆ’
1 2 1 0 0
2 1 0 1 0
âˆ’4 0 0 1
ï£¶
ï£¸
ï£«
ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­
x1
x2
x3
x4
x5
x6
ï£¶
ï£·ï£·ï£·ï£·ï£·ï£·ï£¸
=
ï£«
ï£­
âˆ’
âˆ’
2
3
4
ï£¶
ï£¸ and x1, x2, x3, x4, x5, x6 â‰¥ 0.
We initialize the dual simplex procedure with (u, K) where u =
ï£«
ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­
0
0
âˆ’
0
3
âˆ’
2
4
ï£¶
ï£·ï£·ï£·ï£·ï£·ï£·ï£¸
and K = (4, 5, 6).
The initial tableau, before explicitly calculating the reduced cost, is
0 c1 c2 c3 c4 c5 c6
u4 = âˆ’3
u5 = âˆ’4
âˆ’
âˆ’
1
4
âˆ’
âˆ’
1 2 1 0 0
2 1 0 1 0
u6 = 2 1 1 âˆ’4 0 0 1
.
Since u has negative coordinates, Case (B) applies, and we will set k
âˆ’ = 4. We must now
determine whether Case (B1) or Case (B2) applies. This determination is accomplished by
scanning the first three columns in the tableau and observing each column has a negative
entry. Thus Case (B2) is applicable, and we need to determine the reduced costs. Observe
that c = (âˆ’4, âˆ’2, âˆ’1, 0, 0, 0), which in turn implies c(4,5,6) = (0, 0, 0). Equation (âˆ—2) implies
1626 CHAPTER 47. LINEAR PROGRAMMING AND DUALITY
that the nonzero reduced costs are
c1 = c1 âˆ’ c(4,5,6)
ï£«
ï£­
âˆ’
âˆ’
1
1
4
ï£¶
ï£¸ = âˆ’4
c2 = c2 âˆ’ c(4,5,6)
ï£«
ï£­
âˆ’
âˆ’
1
1
2
ï£¶
ï£¸ = âˆ’2
c3 = c3 âˆ’ c(4,5,6)
ï£«
ï£­
âˆ’
1
4
2
ï£¶
ï£¸ = âˆ’1,
and our tableau becomes
0 âˆ’4 âˆ’2 âˆ’1 0 0 0
u4 = âˆ’3 âˆ’1 âˆ’1 2 1 0 0
u5 = âˆ’4 âˆ’4 âˆ’2 1 0 1 0
u6 = 2 1 1 âˆ’4 0 0 1
.
Since k
âˆ’ = 4, our pivot row is the first row of the tableau. To determine candidates for j
+,
we scan this row, locate negative entries and compute
Âµ
+ = max âˆ’
Î³
cj
4
j


 
Î³4
j < 0, j âˆˆ {1, 2, 3}
 = max
âˆ’
1
2
,
âˆ’
1
4

= âˆ’2.
Since Âµ
+ occurs when j = 2, we set j
+ = 2. Our new basis is K+ = (2, 5, 6). We must
normalize the first row of the tableau, namely multiply by âˆ’1, then add twice this normalized
row to the second row, and subtract the normalized row from the third row to obtain the
updated tableau.
0 âˆ’4 âˆ’2 âˆ’1 0 0 0
u2 = 3 1 1 âˆ’2 âˆ’1 0 0
u5 = 2 âˆ’2 0 âˆ’3 âˆ’2 1 0
u6 = âˆ’1 0 0 âˆ’2 1 0 1
It remains to update the reduced costs and the value of the objective function by adding
twice the normalized row to the top row.
6 âˆ’2 0 âˆ’5 âˆ’2 0 0
u2 = 3 1 1 âˆ’2 âˆ’1 0 0
u5 = 2 âˆ’2 0 âˆ’3 âˆ’2 1 0
u6 = âˆ’1 0 0 âˆ’2 1 0 1
We now repeat the procedure of Case (B2) and set k
âˆ’ = 6 (since this is the only negative
entry of u
+). Our pivot row is now the third row of the updated tableau, and the new Âµ
+
47.5. THE DUAL SIMPLEX ALGORITHM 1627
becomes
Âµ
+ = max âˆ’
Î³
cj
6
j


 
Î³6
j < 0, j âˆˆ {1, 3, 4}
 = max
âˆ’
2
5

= âˆ’
5
2
,
which implies that j
+ = 3. Hence the new basis is K+ = (2, 5, 3), and we update the tableau
by taking âˆ’
1
2
of Row 3, adding twice the normalized Row 3 to Row 1, and adding three
times the normalized Row 3 to Row 2.
6 âˆ’2 0 âˆ’5 âˆ’2 0 0
u2 = 4 1 1 0 âˆ’2 0 âˆ’1
u5 = 7/2 âˆ’2 0 0 âˆ’7/2 1 âˆ’3/2
u3 = 1/2 0 0 1 âˆ’1/2 0 âˆ’1/2
It remains to update the objective function and the reduced costs by adding five times the
normalized row to the top row.
17/2 âˆ’2 0 0 âˆ’9/2 0 âˆ’5/2
u2 = 4 1 1 0 âˆ’2 0 âˆ’1
u5 = 7/2 âˆ’2 0 0 âˆ’
7
2
1 âˆ’3/2
u3 = 1/2 0 0 1 âˆ’1/2 0 âˆ’1/2
Since u
+ has no negative entries, the dual simplex method terminates and objective function
âˆ’4x1 âˆ’ 2x2 âˆ’ x3 is maximized with âˆ’
17
2
at (0, 4,
1
2
). See Figure 47.5.
(0, 4, 1/2)
Figure 47.5: The objective function âˆ’4x1âˆ’2x2âˆ’x3 is maximized at the intersection between
the blue plane âˆ’x1 âˆ’ x2 + 2x3 = âˆ’3 and the pink plane x1 + x2 âˆ’ 4x3 = 2.
z
=
1/2
1628 CHAPTER 47. LINEAR PROGRAMMING AND DUALITY
47.6 The Primal-Dual Algorithm
Let (P2) be a linear program in standard form
maximize cx
subject to Ax = b and x â‰¥ 0,
where A is an m Ã— n matrix of rank m, and (D) be its dual given by
minimize yb
subject to yA â‰¥ c,
where y âˆˆ (R
m)
âˆ—
.
First we may assume that b â‰¥ 0 by changing every equation P n
j=1 aijxj = bi with bi < 0
to P n
j=1 âˆ’aijxj = âˆ’bi
. If we happen to have some feasible solution y of the dual program
(D), we know from Theorem 47.13 that a feasible solution x of (P2) is an optimal solution iff
the equations in (âˆ—P ) hold. If we denote by J the subset of {1, . . . , n} for which the equalities
yAj = cj
hold, then by Theorem 47.13 a feasible solution x of (P2) is an optimal solution iff
xj = 0 for all j /âˆˆ J.
Let |J| = p and N = {1, . . . , n} âˆ’ J. The above suggests looking for x âˆˆ R
n
such that
X
jâˆˆJ
xjA
j = b
xj â‰¥ 0 for all j âˆˆ J
xj = 0 for all j /âˆˆ J,
or equivalently
AJ xJ = b, xJ â‰¥ 0, (âˆ—1)
and
xN = 0nâˆ’p.
To search for such an x, we just need to look for a feasible xJ , and for this we can use
the Restricted Primal linear program (RP) defined as follows:
maximize âˆ’ (Î¾1 + Â· Â· Â· + Î¾m)
subject to ï¿¾ AJ Im


xJ
Î¾

= b and x, Î¾ â‰¥ 0.
47.6. THE PRIMAL-DUAL ALGORITHM 1629
Since by hypothesis b â‰¥ 0 and the objective function is bounded above by 0, this linear
program has an optimal solution (x
âˆ—
J
, Î¾âˆ—
).
If Î¾
âˆ— = 0, then the vector u
âˆ— âˆˆ R
n given by u
âˆ—
J = x
âˆ—
J
and u
âˆ—
N = 0nâˆ’p is an optimal solution
of (P).
Otherwise, Î¾
âˆ— > 0 and we have failed to solve (âˆ—1). However we may try to use Î¾
âˆ—
to
improve y. For this consider the Dual (DRP) of (RP):
minimize zb
subject to zAJ â‰¥ 0
z â‰¥ âˆ’1
>m.
Observe that the Program (DRP) has the same objective function as the original Dual
Program (D). We know by Theorem 47.12 that the optimal solution (x
âˆ—
J
, Î¾âˆ—
) of (RP) yields
an optimal solution z
âˆ— of (DRP) such that
z
âˆ—
b = âˆ’(Î¾1
âˆ— + Â· Â· Â· + Î¾m
âˆ—
) < 0.
In fact, if Kâˆ—
is the basis associated with (x
âˆ—
J
, Î¾âˆ—
) and if we write
Ab =
ï¿¾ AJ Im

and b c = [0>p âˆ’ 1
> ], then by Theorem 47.12 we have
z
âˆ— = b cKâˆ— bA
âˆ’
K
1
âˆ— = âˆ’1
>m âˆ’ (cKâˆ— )(p+1,...,p+m)
,
where (cKâˆ— )(p+1,...,p+m) denotes the row vector of reduced costs in the final tableau correï¿¾sponding to the last m columns.
If we write
y(Î¸) = y + Î¸zâˆ—
,
then the new value of the objective function of (D) is
y(Î¸)b = yb + Î¸zâˆ—
b, (âˆ—2)
and since z
âˆ—
b < 0, we have a chance of improving the objective function of (D), that is,
decreasing its value for Î¸ > 0 small enough if y(Î¸) is feasible for (D). This will be the case
iff y(Î¸)A â‰¥ c iff
yA + Î¸zâˆ—A â‰¥ c. (âˆ—3)
Now since y is a feasible solution of (D) we have yA â‰¥ c, so if z
âˆ—A â‰¥ 0, then (âˆ—3) is satisfied
and y(Î¸) is a solution of (D) for all Î¸ > 0, which means that (D) is unbounded. But this
implies that (P) is not feasible.
1630 CHAPTER 47. LINEAR PROGRAMMING AND DUALITY
Let us take a closer look at the inequalities z
âˆ—A â‰¥ 0. For j âˆˆ J, since z
âˆ—
is an optimal
solution of (DRP), we know that z
âˆ—AJ â‰¥ 0, so if z
âˆ—Aj â‰¥ 0 for all j âˆˆ N, then (P2) is not
feasible.
Otherwise, there is some j âˆˆ N = {1, . . . , n} âˆ’ J such that
z
âˆ—A
j < 0,
and then since by the definition of N we have yAj > cj
for all j âˆˆ N, if we pick Î¸ such that
0 < Î¸ â‰¤
yAj âˆ’ cj
âˆ’z
âˆ—Aj
j âˆˆ N, zâˆ—A
j < 0,
then we decrease the objective function y(Î¸)b = yb + Î¸zâˆ—
b of (D) (since z
âˆ—
b < 0). Therefore
we pick the best Î¸, namely
Î¸
+ = min
yAj âˆ’ cj
âˆ’z
âˆ—Aj


 
j /âˆˆ J, zâˆ—A
j < 0
 > 0. (âˆ—4)
Next we update y to y
+ = y(Î¸
+) = y + Î¸
+z
âˆ—
, we create the new restricted primal with
the new subset
J
+ = {j âˆˆ {1, . . . , n} | y
+A
j = cj},
and repeat the process.
Here are the steps of the primal-dual algorithm.
Step 1. Find some feasible solution y of the Dual Program (D). We will show later
that this is always possible.
Step 2. Compute
J
+ = {j âˆˆ {1, . . . , n} | yAj = cj}.
Step 3. Set J = J
+ and solve the Problem (RP) using the simplex algorithm, starting
from the optimal solution determined during the previous round, obtaining the
optimal solution (x
âˆ—
J
, Î¾âˆ—
) with the basis Kâˆ—
.
Step 4.
If Î¾
âˆ— = 0, then stop with an optimal solution u
âˆ—
for (P) such that u
âˆ—
J = x
âˆ—
J
and the
other components of u
âˆ— are zero.
Else let
z
âˆ— = âˆ’1
>m âˆ’ (cKâˆ— )(p+1,...,p+m)
,
be the optimal solution of (DRP) corresponding to (x
âˆ—
J
, Î¾âˆ—
) and the basis Kâˆ—
.
If z
âˆ—Aj â‰¥ 0 for all j /âˆˆ J, then stop; the Program (P) has no feasible solution.
47.6. THE PRIMAL-DUAL ALGORITHM 1631
Else compute
Î¸
+ = min âˆ’
yAj âˆ’ cj
z
âˆ—Aj


 
j /âˆˆ J, zâˆ—A
j < 0
 , y+ = y + Î¸
+z
âˆ—
,
and
J
+ = {j âˆˆ {1, . . . , n} | y
+A
j = cj}.
Go back to Step 3.
The following proposition shows that at each iteration we can start the Program (RP)
with the optimal solution obtained at the previous iteration.
Proposition 47.14. Every j âˆˆ J such that Aj
is in the basis of the optimal solution Î¾
âˆ—
belongs to the next index set J
+.
Proof. Such an index j âˆˆ J correspond to a variable Î¾j such that Î¾j > 0, so by complementary
slackness, the constraint z
âˆ—Aj â‰¥ 0 of the Dual Program (DRP) must be an equality, that
is, z
âˆ—Aj = 0. But then we have
y
+A
j = yAj + Î¸
+z
âˆ—A
j = cj
,
which shows that j âˆˆ J
+.
If (u
âˆ—
, Î¾âˆ—
) with the basis Kâˆ—
is the optimal solution of the Program (RP), Proposition
47.14 together with the last property of Theorem 47.12 allows us to restart the (RP) in Step
3 with (u
âˆ—
, Î¾âˆ—
)Kâˆ— as initial solution (with basis Kâˆ—
). For every j âˆˆ J âˆ’ J
+, column j is
deleted, and for every j âˆˆ J
+ âˆ’ J, the new column Aj
is computed by multiplying b A
âˆ’
K
1
âˆ— and
Aj
, but b A
âˆ’
K
1
âˆ— is the matrix Î“âˆ—
[1:m; p + 1:p + m] consisting of the last m columns of Î“âˆ—
in the
final tableau, and the new reduced cj
is given by cj âˆ’ z
âˆ—Aj
. Reusing the optimal solution of
the previous (RP) may improve efficiency significantly.
Another crucial observation is that for any index j0 âˆˆ N such that
Î¸
+ = (yAj0 âˆ’ cj0
)/(âˆ’z
âˆ—Aj0 ), we have
y
+Aj0 = yAj0 + Î¸
+z
âˆ—A
j0 = cj0
,
and so j0 âˆˆ J
+. This fact that be used to ensure that the primal-dual algorithm terminates
in a finite number of steps (using a pivot rule that prevents cycling); see Papadimitriou and
Steiglitz [134] (Theorem 5.4).
It remains to discuss how to pick some initial feasible solution y of the Dual Program
(D). If cj â‰¤ 0 for j = 1, . . . , n, then we can pick y = 0. If we are dealing with a minimization
problem, the weight cj are often nonnegative, so from the point of view of maximization we
will have âˆ’cj â‰¤ 0 for all j, and we will be able to use y = 0 as a starting point.
Going back to our primal problem in maximization form and its dual in minimization
form, we still need to deal with the situation where cj > 0 for some j, in which case there
1632 CHAPTER 47. LINEAR PROGRAMMING AND DUALITY
may not be any obvious y feasible for (D). Preferably we would like to find such a y very
cheaply.
There is a trick to deal with this situation. We pick some very large positive number M
and add to the set of equations Ax = b the new equation
x1 + Â· Â· Â· + xn + xn+1 = M,
with the new variable xn+1 constrained to be nonnegative. If the Program (P) has a feaï¿¾sible solution, such an M exists. In fact it can shown that for any basic feasible solution
u = (u1, . . . , un), each |ui
| is bounded by some expression depending only on A and b; see
Papadimitriou and Steiglitz [134] (Lemma 2.1). The proof is not difficult and relies on the
fact that the inverse of a matrix can be expressed in terms of certain determinants (the adjuï¿¾gates). Unfortunately, this bound contains m! as a factor, which makes it quite impractical.
Having added the new equation above, we obtain the new set of equations

A 0n
1
>n
1
  xn
x
+1
=
 M
b

,
with x â‰¥ 0, xn+1 â‰¥ 0, and the new objective function given by
ï¿¾
c 0


xn
x
+1
= cx.
The dual of the above linear program is
minimize yb + ym+1M
subject to yAj + ym+1 â‰¥ cj j = 1, . . . , n
ym+1 â‰¥ 0.
If cj > 0 for some j, observe that the linear form ye given by
e
yi =
(
0 if 1 â‰¤ i â‰¤ m
max1â‰¤jâ‰¤n{cj} > 0
is a feasible solution of the new dual program. In practice, we can choose M to be a number
close to the largest integer representable on the computer being used.
Here is an example of the primal-dual algorithm given in the Math 588 class notes of T.
Molla [128].
Example 47.3. Consider the following linear program in standard form:
Maximize âˆ’ x1 âˆ’ 3x2 âˆ’ 3x3 âˆ’ x4
subject to
ï£«
ï£­
3 4
3
6 4 0 1
âˆ’2 6
âˆ’3 1
âˆ’1
ï£¶
