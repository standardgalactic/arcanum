


x
x
x
x
1
2
3
4

 =


2
1
4

 and x1, x2, x3, x4 ≥ 0.
47.6. THE PRIMAL-DUAL ALGORITHM 1633
The associated Dual Program (D) is
Minimize 2y1 + y2 + 4y3
subject to ￾ y1 y2 y3



3 4
3
6 4 0 1
−2 6
−3 1
−1

 ≥
￾ −1 −3 −3 −1
 .
We initialize the primal-dual algorithm with the dual feasible point y = (−1/3 0 0).
Observe that only the first inequality of (D) is actually an equality, and hence J = {1}. We
form the Restricted Primal Program (RP1)
Maximize − (ξ1 + ξ2 + ξ3)
subject to


3 1 0 0
3 0 1 0
6 0 0 1




x
ξ
ξ
ξ
1
2
3
1

 =


2
1
4

 and x1, ξ1, ξ2, ξ3 ≥ 0.
We now solve (RP1) via the simplex algorithm. The initial tableau with K = (2, 3, 4) and
J = {1} is
x1 ξ1 ξ2 ξ3
7 12 0 0 0
ξ1 = 2 3 1 0 0
ξ2 = 1 3 0 1 0
ξ3 = 4 6 0 0 1
.
For (RP1), ˆc = (0, −1, −1, −1), (x1, ξ1, ξ2, ξ3) = (0, 2, 1, 4), and the nonzero reduced cost is
given by
0 − (−1 − 1 − 1)


3
3
6

 = 12.
Since there is only one nonzero reduced cost, we must set j
+ = 1. Since
min{ξ1/3, ξ2/3, ξ3/6} = 1/3, we see that k
− = 3 and K = (2, 1, 4). Hence we pivot through
the red circled 3 (namely we divide row 2 by 3, and then subtract 3× (row 2) from row 1,
6× (row 2) from row 3, and 12× (row 2) from row 0), to obtain the tableau
x1 ξ1 ξ2 ξ3
3 0 0 −4 0
ξ1 = 1 0 1 −1 0
x1 = 1/3 1 0 1/3 0
ξ3 = 2 0 0 −2 1
.
At this stage the simplex algorithm for (RP1) terminates since there are no positive reduced
costs. Since the upper left corner of the final tableau is not zero, we proceed with Step 4 of
the primal dual algorithm and compute
z
∗ = (−1 − 1 − 1) − (0 − 4 0) = (−1 3 − 1),
1634 CHAPTER 47. LINEAR PROGRAMMING AND DUALITY
yA2 − c2 = (−1/3 0 0)

−
4
4
2

 + 3 =
5
3
, z∗A
2 = −(−1 3 − 1)

−
4
4
2

 = 14,
yA4 − c4 = (−1/3 0 0)

−
1
1
1

 + 1 =
3
2
, z∗A
4 = −(−1 3 − 1)

−
1
1
1

 = 5,
so
θ
+ = min 
42
5
,
15
2

=
42
5
,
and we conclude that the new feasible solution for (D) is
y
+ = (−1/3 0 0) + 5
42
(−1 3 − 1) = (−19/42 5/14 − 5/42).
When we substitute y
+ into (D), we discover that the first two constraints are equalities,
and that the new J is J = {1, 2}. The new Reduced Primal (RP2) is
Maximize − (ξ1 + ξ2 + ξ3)
subject to


6 4 0 0 1
3 4 1 0 0
3 −2 0 1 0




x1
x2
ξ1
ξ2
ξ3


=


2
1
4

 and x1, x2, ξ1, ξ2, ξ3 ≥ 0.
Once again, we solve (RP2) via the simplex algorithm, where ˆc = (0, 0, −1, −1, −1), (x1, x2,
ξ1, ξ2, ξ3) = (1/3, 0, 1, 0, 2) and K = (3, 1, 5). The initial tableau is obtained from the final
tableau of the previous (RP1) by adding a column corresponding the the variable x2, namely
Ab
−
K
1A
2 =


1
0 1
0
−
−
/
1 0
2 1
3 0



−
4
4
2

 =

−2
6
8
/3

 ,
with
c2 = c2 − z
∗A
2 = 0 −
￾ −1 3 −1


−
4
4
2

 = 14,
and we get
x1 x2 ξ1 ξ2 ξ3
3 0 14 0 −4 0
ξ1 = 1 0 6 1 −1 0
x1 = 1/3 1 −2/3 0 1/3 0
ξ3 = 2 0 8 0 −2 1
.
47.6. THE PRIMAL-DUAL ALGORITHM 1635
Note that j
+ = 2 since the only positive reduced cost occurs in column 2. Also observe
that since min{ξ1/6, ξ3/8} = ξ1/6 = 1/6, we set k
− = 3, K = (2, 1, 5) and pivot along the
red 6 to obtain the tableau
x1 x2 ξ1 ξ2 ξ3
2/3 0 0 −7/3 −5/3 0
x2 = 1/6 0 1 1/6 −1/6 0
x1 = 4/9 1 0 1/9 2/9 0
ξ3 = 2/3 0 0 −4/3 −2/3 1
.
Since the reduced costs are either zero or negative the simplex algorithm terminates, and
we compute
z
∗ = (−1 − 1 − 1) − (−7/3 − 5/3 0) = (4/3 2/3 − 1),
y
+A
4 − c4 = (−19/42 5/14 − 5/42)

−
1
1
1

 + 1 = 1/14,
z
∗A
4 = −(4/3 2/3 − 1)

−
1
1
1

 = 1/3,
so
θ
+ =
3
14
,
y
+ = (−19/42 5/14 − 5/42) + 5
14
(4/3 2/3 − 1) = (−1/6 1/2 − 1/3).
When we plug y
+ into (D), we discover that the first, second, and fourth constraints are
equalities, which implies J = {1, 2, 4}. Hence the new Restricted Primal (RP3) is
Maximize − (ξ1 + ξ2 + ξ3)
subject to


6 4 1 0 0 1
3 4 1 1 0 0
3 −2 −1 0 1 0




x1
x2
x4
ξ1
ξ2
ξ3


=


2
1
4

 and x1, x2, x4, ξ1, ξ2, ξ3 ≥ 0.
The initial tableau for (RP3), with ˆc = (0, 0, 0, −1, −1, −1), (x1, x2, x4, ξ1, ξ2, ξ3) = (4/9, 1/6,
0, 0, 0, 2/3) and K = (2, 1, 6), is obtained from the final tableau of the previous (RP2) by
adding a column corresponding the the variable x4, namely
Ab
−
K
1A
4 =


−
1
1
4
/
/
/
6
9 2
3
−
−
1
2
/
/
/
9 0
6 0
3 1



−
1
1
1

 =

−
1
1
1
/
/
/
3
3
9

 ,
1636 CHAPTER 47. LINEAR PROGRAMMING AND DUALITY
with
c4 = c4 − z
∗A
4 = 0 −
￾ 4/3 2/3 −1


−
1
1
1

 = 1/3,
and we get
x1 x2 x4 ξ1 ξ2 ξ3
2/3 0 0 1/3 −7/3 −5/3 0
x2 = 1/6 0 1 1/3 1/6 −1/6 0
x1 = 4/9 1 0 −1/9 1/9 2/9 0
ξ3 = 2/3 0 0 1/3 −4/3 −2/3 1
.
Since the only positive reduced cost occurs in column 3, we set j
+ = 3. Furthermore
since min{x2/(1/3), ξ3/(1/3)} = x2/(1/3) = 1/2, we let k
− = 2, K = (3, 1, 6), and pivot
around the red circled 1/3 to obtain
x1 x2 x4 ξ1 ξ2 ξ3
1/2 0 −1 0 −5/2 −3/2 0
x4 = 1/2 0 3 1 1/2 −1/2 0
x1 = 1/2 1 1/3 0 1/6 1/6 0
ξ3 = 1/2 0 −1 0 −3/2 −1/2 1
.
At this stage there are no positive reduced costs, and we must compute
z
∗ = (−1 − 1 − 1) − (−5/2 − 3/2 0) = (3/2 1/2 − 1),
y
+A
3 − c3 = (−1/6 1/2 − 1/3)


−
6
0
3

 + 3 = 13/2,
z
∗A
3 = −(3/2 1/2 − 1)


−
6
0
3

 = 3/2,
so
θ
+ =
13
3
,
y
+ = (−1/6 1/2 − 1/3) + 13
3
(3/2 1/2 − 1) = (19/3 8/3 − 14/3).
47.6. THE PRIMAL-DUAL ALGORITHM 1637
We plug y
+ into (D) and discover that the first, third, and fourth constraints are equalities.
Thus, J = {1, 3, 4} and the Restricted Primal (RP4) is
Maximize − (ξ1 + ξ2 + ξ3)
subject to


3
3 6
6 0 1 0 0 1
−3 1 1 0 0
−1 0 1 0




x1
x3
x4
ξ1
ξ2
ξ3


=


2
1
4

 and x1, x3, x4, ξ1, ξ2, ξ3 ≥ 0.
The initial tableau for (RP4), with ˆc = (0, 0, 0, −1, −1, −1), (x1, x3, x4, ξ1, ξ2, ξ3) = (1/2,
0, 1/2, 0, 0, 1/2) and K = (3, 1, 6) is obtained from the final tableau of the previous (RP3)
by replacing the column corresponding to the variable x2 by a column corresponding to the
variable x3, namely
Ab
−
K
1A
3 =


−
1
1
3
/
/
/
6 1
2
2
−
−
1
1
/
/
/
6 0
2 0
2 1




−
6
0
3

 =


−
1
3
9
/
/
/
2
2
2

 ,
with
c3 = c3 − z
∗A
3 = 0 −
￾ 3/2 1/2 −1



−
6
0
3

 = 3/2,
and we get
x1 x3 x4 ξ1 ξ2 ξ3
1/2 0 3/2 0 −5/2 −3/2 0
x4 = 1/2 0 −9/2 1 1/2 −1/2 0
x1 = 1/2 1 1/2 0 1/6 1/6 0
ξ3 = 1/2 0 3/2 0 −3/2 −1/2 1
.
By analyzing the top row of reduced cost, we see that j
+ = 2. Furthermore, since
min{x1/(1/2), ξ3/(3/2)} = ξ3/(3/2) = 1/3, we let k
− = 6, K = (3, 1, 2), and pivot along the
red circled 3/2 to obtain
x1 x3 x4 ξ1 ξ2 ξ3
0 0 0 0 −1 −1 −1
x4 = 2 0 0 1 −4 −2 3
x1 = 1/3 1 0 0 2/3 1/3 −1/3
x3 = 1/3 0 1 0 −1 −1/3 2/3
.
Since the upper left corner of the final tableau is zero and the reduced costs are all ≤ 0,
we are finally finished. Then y = (19/3 8/3 − 14/3) is an optimal solution of (D), but more
1638 CHAPTER 47. LINEAR PROGRAMMING AND DUALITY
importantly (x1, x2, x3, x4) = (1/3, 0, 1/3, 2) is an optimal solution for our original linear
program and provides an optimal value of −10/3.
The primal-dual algorithm for linear programming doesn’t seem to be the favorite method
to solve linear programs nowadays. But it is important because its basic principle, to use a re￾stricted (simpler) primal problem involving an objective function with fixed weights, namely
1, and the dual problem to provide feedback to the primal by improving the objective func￾tion of the dual, has led to a whole class of combinatorial algorithms (often approximation
algorithms) based on the primal-dual paradigm. The reader will get a taste of this kind of
algorithm by consulting Papadimitriou and Steiglitz [134], where it is explained how clas￾sical algorithms such as Dijkstra’s algorithm for the shortest path problem, and Ford and
Fulkerson’s algorithm for max flow can be derived from the primal-dual paradigm.
47.7 Summary
The main concepts and results of this chapter are listed below:
• Strictly separating hyperplane.
• Farkas–Minkowski proposition.
• Farkas lemma, version I, Farkas lemma, version II.
• Distance of a point to a subset.
• Dual linear program, primal linear program.
• Dual variables, primal variables.
• Complementary slackness conditions.
• Dual simplex algorithm.
• Primal-dual algorithm.
• Restricted primal linear program.
47.8 Problems
Problem 47.1. Let (v1, . . . , vn) be a sequence of n vectors in R
d and let V be the d × n
matrix whose j-th column is vj
. Prove the equivalence of the following two statements:
(a) There is no nontrivial positive linear dependence among the vj
, which means that there
is no nonzero vector, y = (y1, . . . , yn) ∈ R
n
, with yj ≥ 0 for j = 1, . . . , n, so that
y1v1 + · · · + ynvn = 0
or equivalently, V y = 0.
47.8. PROBLEMS 1639
(b) There is some vector, c ∈ R
d
, so that c
> V > 0, which means that c
> vj > 0, for
j = 1, . . . , n.
Problem 47.2. Check that the dual in maximization form (D00 ) of the Dual Program (D0 )
(which is the dual of (P) in maximization form),
maximize − b
> y
>
subject to − A
> y
> ≤ −c
> and y
> ≥ 0,
where y ∈ (R
m)
∗
, gives back the Primal Program (P).
Problem 47.3. In a General Linear Program (P) with n primal variables x1, . . . , xn and
objective function P n
j=1 cjxj (to be maximized), the m constraints are of the form
nX
j=1
aijxj ≤ bi
,
nX
j=1
aijxj ≥ bi
,
nX
j=1
aijxj = bi
,
for i = 1, . . . , m, and the variables xj satisfy an inequality of the form
xj ≥ 0,
xj ≤ 0,
xj ∈ R,
for j = 1, . . . , n. If y1, . . . , ym are the dual variables, show that the dual program of the
linear program in standard form equivalent to (P) is equivalent to the linear program whose
objective function is P m
i=1 yibi (to be minimized) and whose constraints are determined as
follows:
if



xj ≥ 0
xj ≤ 0
xj ∈ R



, then



X
m
i=1
aijyi ≥ cj
mX
i=1
aijyi ≤ cj
mX
i=1
aijyi = cj



,
1640 CHAPTER 47. LINEAR PROGRAMMING AND DUALITY
and
if



X
n
j=1
aijxj ≤ bi
nX
j=1
aijxj ≥ bi
nX
j=1
aijxj = bi



, then



yi ≥ 0
yi ≤ 0
yi ∈ R



.
Problem 47.4. Apply the procedure of Problem 47.3 to show that the dual of the (general)
linear program
maximize 3x1 + 2x2 + 5x3
subject to
5x1 + 3x2 + x3 = −8
4x1 + 2x2 + 8x3 ≤ 23
6x1 + 7x2 + 3x3 ≥ 1
x1 ≤ 4, x3 ≥ 0
is the (general) linear program:
minimize − 8y1 + 23y2 − y3 + 4y4
subject to
5y1 + 4y2 − 6y3 + y4 = 3
3y1 + 2y2 − 7y3 = 2
y1 + 8y2 − 3y3 ≥ 5
y2, y3, y4 ≥ 0.
Problem 47.5. (1) Prove that the dual of the (general) linear program
maximize cx
subject to Ax = b and x ∈ R
n
is
minimize yb
subject to yA = c and y ∈ R
m.
(2) Prove that the dual of the (general) linear program
maximize cx
subject to Ax ≥ b and x ≥ 0
47.8. PROBLEMS 1641
is
minimize yb
subject to yA ≥ c and y ≤ 0.
Problem 47.6. Use the complementary slackness conditions to confirm that
x1 = 2, x2 = 4, x3 = 0, x4 = 0, x5 = 7, x6 = 0
is an optimal solution of the following linear program (from Chavatal [40], Chapter 5):
maximize 18x1 − 7x2 + 12x3 + 5x4 + 8x6
subject to
2x1 − 6x2 + 2x3 + 7x4 + 3x5 + 8x6 ≤ 1
−3x1 − x2 + 4x3 − 3x4 + x5 + 2x6 ≤ −2
8x1 − 3x2 + 5x3 − 2x4 + 2x6 ≤ 4
4x1 + 8x3 + 7x4 − x5 + 3x6 ≤ 1
5x1 + 2x2 − 3x3 + 6x4 − 2x5 − x6 ≤ 5
x1, x2, x3, x4, x5, x6 ≥ 0.
Problem 47.7. Check carefully that the dual simplex method is equivalent to the simplex
method applied to the dual program in maximization form.
1642 CHAPTER 47. LINEAR PROGRAMMING AND DUALITY
Part VIII
NonLinear Optimization
1643
Chapter 48
Basics of Hilbert Spaces
Most of the “deep” results about the existence of minima of real-valued functions proven in
Chapter 49 rely on two fundamental results of Hilbert space theory:
(1) The projection lemma, which is a result about nonempty, closed, convex subsets of a
Hilbert space V .
(2) The Riesz representation theorem, which allows us to express a continuous linear form
on a Hilbert space V in terms of a vector in V and the inner product on V .
The correctness of the Karush–Kuhn–Tucker conditions appearing in Lagrangian duality
follows from a version of the Farkas–Minkowski proposition, which also follows from the
projection lemma.
Thus, we feel that it is indispensable to review some basic results of Hilbert space theory,
although in most applications considered here the Hilbert space in question will be finite￾dimensional. However, in optimization theory, there are many problems where we seek to
find a function minimizing some type of energy functional (often given by a bilinear form),
in which case we are dealing with an infinite dimensional Hilbert space, so it necessary to
develop tools to deal with the more general situation of infinite-dimensional Hilbert spaces.
48.1 The Projection Lemma
Given a Hermitian space h E, ϕi , we showed in Section 14.1 that the function k k : E → R
defined such that k uk =
p ϕ(u, u), is a norm on E. Thus, E is a normed vector space. If E
is also complete, then it is a very interesting space.
Recall that completeness has to do with the convergence of Cauchy sequences. A normed
vector space h E, k ki is automatically a metric space under the metric d defined such that
d(u, v) = k v −uk (see Chapter 37 for the definition of a normed vector space and of a metric
space, or Lang [111, 112], or Dixmier [51]). Given a metric space E with metric d, a sequence
1645
1646 CHAPTER 48. BASICS OF HILBERT SPACES
(an)n≥1 of elements an ∈ E is a Cauchy sequence iff for every  > 0, there is some N ≥ 1
such that
d(am, an) <  for all m, n ≥ N.
We say that E is complete iff every Cauchy sequence converges to a limit (which is unique,
since a metric space is Hausdorff).
Every finite dimensional vector space over R or C is complete. For example, one can
show by induction that given any basis (e1, . . . , en) of E, the linear map h: C
n → E defined
such that
h((z1, . . . , zn)) = z1e1 + · · · + znen
is a homeomorphism (using the sup-norm on C
n
). One can also use the fact that any two
norms on a finite dimensional vector space over R or C are equivalent (see Chapter 9, or
Lang [112], Dixmier [51], Schwartz [150]).
However, if E has infinite dimension, it may not be complete. When a Hermitian space is
complete, a number of the properties that hold for finite dimensional Hermitian spaces also
hold for infinite dimensional spaces. For example, any closed subspace has an orthogonal
complement, and in particular, a finite dimensional subspace has an orthogonal complement.
Hermitian spaces that are also complete play an important role in analysis. Since they were
first studied by Hilbert, they are called Hilbert spaces.
Definition 48.1. A (complex) Hermitian space h E, ϕi which is a complete normed vector
space under the norm k k induced by ϕ is called a Hilbert space. A real Euclidean space
h
E, ϕi which is complete under the norm k k induced by ϕ is called a real Hilbert space.
All the results in this section hold for complex Hilbert spaces as well as for real Hilbert
spaces. We state all results for the complex case only, since they also apply to the real case,
and since the proofs in the complex case need a little more care.
Example 48.1. The space ` 2 of all countably infinite sequences x = (xi)i∈N of complex
numbers such that P ∞
i=0 |xi
|
2 < ∞ is a Hilbert space. It will be shown later that the map
ϕ: `
2 × ` 2 → C defined such that
ϕ ((xi)i∈N,(yi)i∈N) =
∞X
i=0
xiyi
is well defined, and that ` 2
is a Hilbert space under ϕ. In fact, we will prove a more general
result (Proposition A.3).
Example 48.2. The set C
∞[a, b] of smooth functions f : [a, b] → C is a Hermitian space
under the Hermitian form
h
f, gi =
Z
b
a
f(x)g(x)dx,
but it is not a Hilbert space because it is not complete. It is possible to construct its
completion L2
([a, b]), which turns out to be the space of Lebesgue integrable functions on
[a, b].
48.1. THE PROJECTION LEMMA 1647
Theorem 37.63 yields a quick proof of the fact that any Hermitian space E (with Hermi￾tian product h−, −i) can be embedded in a Hilbert space Eh.
Theorem 48.1. Given a Hermitian space (E,h−, −i) (resp. Euclidean space), there is a
Hilbert space (Eh,h−, −ih) and a linear map ϕ: E → Eh, such that
h
u, vi = h ϕ(u), ϕ(v)i h
for all u, v ∈ E, and ϕ(E) is dense in Eh. Furthermore, Eh is unique up to isomorphism.
Proof. Let ( bE, k k Eb
) be the Banach space, and let ϕ: E → Eb be the linear isometry, given
by Theorem 37.63. Let k uk =
p h u, ui (with u ∈ E) and Eh = Eb. If E is a real vector space,
we know from Section refsec5bis that the inner product h−, −i can be expressed in terms of
the norm k uk by the polarity equation
h
u, vi =
1
2
(k u + vk
2 − kuk
2 − kvk
2
),
and if E is a complex vector space, we know from Section 14.1 that we have the polarity
equation
h
u, vi =
1
4
(k u + vk
2 − ku − vk
2 + ik u + ivk 2 − ik u − ivk 2
).
By the Cauchy-Schwarz inequality, |hu, vi| ≤ kukk vk , the map h−, −i: E × E → C (resp.
h−
get around this problem by using the polarity equations to extend it to a continuous map.
, −i: E × E → R) is continuous. However, it is not uniformly continuous, but we can
By continuity, the polarity equations also hold in Eh, which shows that h−, −i extends to
a positive definite Hermitian inner product (resp. Euclidean inner product) h−, −ih on Eh
induced by k k Eb
extending h−, −i.
Remark: We followed the approach in Schwartz [149] (Chapter XXIII, Section 42. Theorem
2). For other approaches, see Munkres [131] (Chapter 7, Section 43), and Bourbaki [27].
One of the most important facts about finite-dimensional Hermitian (and Euclidean)
spaces is that they have orthonormal bases. This implies that, up to isomorphism, every
finite-dimensional Hermitian space is isomorphic to C
n
(for some n ∈ N) and that the inner
product is given by
h
(x1, . . . , xn),(y1, . . . , yn)i =
nX
i=1
xiyi
.
Furthermore, every subspace W has an orthogonal complement W⊥, and the inner product
induces a natural duality between E and E
∗
(actually, between E and E
∗
) where E
∗
is the
space of linear forms on E.
When E is a Hilbert space, E may be infinite dimensional, often of uncountable dimen￾sion. Thus, we can’t expect that E always have an orthonormal basis. However, if we modify
1648 CHAPTER 48. BASICS OF HILBERT SPACES
the notion of basis so that a “Hilbert basis” is an orthogonal family that is also dense in E,
i.e., every v ∈ E is the limit of a sequence of finite combinations of vectors from the Hilbert
basis, then we can recover most of the “nice” properties of finite-dimensional Hermitian
spaces. For instance, if (uk)k∈K is a Hilbert basis, for every v ∈ E, we can define the Fourier
coefficients ck = h v, uki /k ukk , and then, v is the “sum” of its Fourier series P k∈K ckuk. How￾ever, the cardinality of the index set K can be very large, and it is necessary to define what
it means for a family of vectors indexed by K to be summable. We will do this in Section
A.1. It turns out that every Hilbert space is isomorphic to a space of the form ` 2
(K), where
`
2
(K) is a generalization of the space of Example 48.1 (see Theorem A.8, usually called the
Riesz-Fischer theorem).
Our first goal is to prove that a closed subspace of a Hilbert space has an orthogonal
complement. We also show that duality holds if we redefine the dual E
0 of E to be the space
of continuous linear maps on E. Our presentation closely follows Bourbaki [27]. We also
were inspired by Rudin [140], Lang [111, 112], Schwartz [150, 149], and Dixmier [51]. In fact,
we highly recommend Dixmier [51] as a clear and simple text on the basics of topology and
analysis. To achieve this goal, we must first prove the so-called projection lemma.
Recall that in a metric space E, a subset X of E is closed iff for every convergent sequence
(xn) of points xn ∈ X, the limit x = limn→ ∞ xn also belongs to X. The closure X of X is
the set of all limits of convergent sequences (xn) of points xn ∈ X. Obviously, X ⊆ X. We
say that the subset X of E is dense in E iff E = X, the closure of X, which means that
every a ∈ E is the limit of some sequence (xn) of points xn ∈ X. Convex sets will again play
a crucial role. In a complex vector space E, a subset C ⊆ E is convex if (1 − λ)x + λy ∈ C
for all x, y ∈ C and all real λ ∈ [0, 1]. Observe that a subspace is convex.
First we state the following easy “parallelogram law,” whose proof is left as an exercise.
Proposition 48.2. If E is a Hermitian space, for any two vectors u, v ∈ E, we have
k
u + vk
2 + k u − vk
2 = 2(k uk
2 + k vk
2
).
From the above, we get the following proposition:
Proposition 48.3. If E is a Hermitian space, given any d, δ ∈ R such that 0 ≤ δ < d, let
B = {u ∈ E | kuk < d} and C = {u ∈ E | kuk ≤ d + δ}.
For any convex set such A that A ⊆ C − B, we have
k
v − uk ≤ √
12dδ,
for all u, v ∈ A (see Figure 48.1).
48.1. THE PROJECTION LEMMA 1649
C B
A u
v
Figure 48.1: Inequality of Proposition 48.3.
Proof. Since A is convex, 2
1
(u + v) ∈ A if u, v ∈ A, and thus, k 1
2
(u + v)k ≥ d. From the
parallelogram equality written in the form



1
2
(u + v)


2
+

 
1
2
(u − v)


2
=
1
2
￾
k
uk
2 + k vk
2

,
since δ < d, we get



1
2
(u − v)


2
=
1
2
￾
k
uk
2 + k vk
2
 −

 
1
2
(u + v)


2
