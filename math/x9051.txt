For example, for m = 3 we have the 8 Ã— 13 matrix
ï£«
ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­
1 1 1
1 1 1 1 1 1 0 0 0 0 0 0 1
âˆ’1 âˆ’1 âˆ’1 0 0 0 0 0 0 0
1 0 0 0 0 0 1 0 0 0 0 0 0
0 1 0 0 0 0 0 1 0 0 0 0 0
0 0 1 0 0 0 0 0 1 0 0 0 0
0 0 0 1 0 0 0 0 0 1 0 0 0
0 0 0 0 1 0 0 0 0 0 1 0 0
0 0 0 0 0 1 0 0 0 0 0 1 0
ï£¶
ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£¸
.
We leave it as an exercise to show that A has rank 2m + 2. Recall that
q =

âˆ’
y
y

and we also define the vector c (of dimension 2m + 2) as
c =
ï£«
ï£­
0
CÎ½
C
m
12m
ï£¶
ï£¸ .
The constraints are given by the system of affine equations Ax = c, where
x =
ï¿¾ Î»
> Âµ
> Î±
> Î²
> Î³

> .
2092 CHAPTER 56. Î½-SV REGRESSION
Since there are 4m + 1 Lagrange multipliers (Î», Âµ, Î±, Î², Î³), we need to pad the 2m Ã— 2m
matrix P with zeros to make it into a (4m + 1) Ã— (4m + 1) matrix
Pa =

P 02m,2m+1
02m+1,2m 02m+1,2m+1
.
Similarly, we pad q with zeros to make it a vector qa of dimension 4m + 1,
qa =

02m
q
+1
.
In order to solve our dual program, we apply ADMM to the quadractic functional
1
2
x
> Pax + qa
> x,
subject to the constraints
Ax = c, x â‰¥ 0,
with Pa, qa, A, b and x, as above.
Since for an optimal solution with  > 0 we must have Î³ = 0 (from the KKT condiï¿¾tions), we can solve the dual problem with the following set of constraints only involving the
Lagrange multipliers (Î», Âµ, Î±, Î²),
mX
i=1
Î»i âˆ’
mX
i=1
Âµi = 0
mX
i=1
Î»i +
mX
i=1
Âµi = CÎ½
Î» + Î± =
C
m
, Âµ + Î² =
C
m
,
which corresponds to the (2m + 2) Ã— 4m A2 given by
A2 =
ï£«
ï£¬ï£¬ï£¬ï£¬ï£­
1
>m âˆ’1
>m 0
>m 0
>m
1
>m 1
>m 0
>m 0
>m
Im 0m,m Im 0m,m
0m,m Im 0m,m Im
ï£¶
ï£·ï£·ï£·ï£·ï£¸
.
We leave it as an exercise to show that A2 has rank 2m + 2. We define the vector c2 (of
dimension 2m + 2) as
c2 = c =
ï£«
ï£­
0
CÎ½
C
m
12m
ï£¶
ï£¸ .
56.3. SOLVING Î½-REGRESSION USING ADMM 2093
Since there are 4m Lagrange multipliers (Î», Âµ, Î±, Î²), we need to pad the 2m Ã— 2m matrix
P with zeros to make it into a 4m Ã— 4m matrix
P2a =

P 02m,2m
02m,2m 02m,2m

.
Similarly, we pad q with zeros to make it a vector q2a of dimension 4m,
q2a =

02
q
m

.
We implemented the above methods in Matlab; see Appendix B, Section B.4. Choosing
C = m is typically a good choice because then the values of Î»i and Âµj are not too small
(C/m = 1). If C is chosen too small, we found that numerical instability increases drastically
and very poor results are obtained. Increasing C tends to encourage sparsity.
We ran our Matlab implementation of the above method on the set of 50 points generated
at random by the program shown below with C = 50 and various values of Î½ starting with
Î½ = 0.03:
X13 = 15*randn(50,1);
ww13 = 1;
y13 = X13*ww13 + 10*randn(50,1) + 20;
[~,~,~,~,~,~,~,~,w1] = runuregb(rho,0.03,X13,y13,50)
Figure 56.11 shows the result of running the program with Î½ = 0.03. We have pf =
0, qf = 0, pm = 2 and qm = 1. There are 47 points strictly inside the slab. The slab is large
enough to contain all the data points, so none of them is considered an error.
The next value of Î½ is Î½ = 0.21, see Figure 56.12. We have pf = 4, qf = 5, pm = 6 and
qm = 6. There are 38 points strictly inside the slab.
2094 CHAPTER 56. Î½-SV REGRESSION
-40 -30 -20 -10 0 10 20 30 40 50 60
-40
-20
0
20
40
60
80
Figure 56.11: Running Î½-SV regression on a set of 50 points; Î½ = 0.03.
-40 -30 -20 -10 0 10 20 30 40 50 60
-40
-20
0
20
40
60
80
Figure 56.12: Running Î½-SV regression on a set of 50 points; Î½ = 0.21.
The next value of Î½ is Î½ = 0.5, see Figure 56.13. We have pf = 12, qf = 12, pm = 13 and
56.3. SOLVING Î½-REGRESSION USING ADMM 2095
qm = 14. There are 23 points strictly inside the slab.
-40 -30 -20 -10 0 10 20 30 40 50 60
-40
-20
0
20
40
60
80
Figure 56.13: Running Î½-SV regression on a set of 50 points; Î½ = 0.5.
-40 -30 -20 -10 0 10 20 30 40 50 60
-40
-20
0
20
40
60
80
Figure 56.14: Running Î½-SV regression on a set of 50 points; Î½ = 0.7.
The next value of Î½ is Î½ = 0.7, see Figure 56.14. We have pf = 17, qf = 17, pm = 18 and
2096 CHAPTER 56. Î½-SV REGRESSION
qm = 19. There are 13 points strictly inside the slab.
The last value of Î½ is Î½ = 0.97, see Figure 56.15. We have pf = 23, qf = 24, pm = 25
and qm = 25. There are 0 points strictly inside the slab. The slab is so narrow that it does
not contain any of the points xi
in it. Running the program with any value Î½ > 0.97 yields

= 0.
-40 -30 -20 -10 0 10 20 30 40 50 60
-40
-20
0
20
40
60
80
Figure 56.15: Running Î½-SV regression on a set of 50 points; Î½ = 0.97.
56.4 Kernel Î½-SV Regression
Since the formulae for w, b, and f(x),
w =
mX
i=1
(Âµi âˆ’ Î»i)xi
b =
1
2
ï¿¾
yi0 + yj0 âˆ’ w
> (xi0 + xj0
)

f(x) =
mX
i=1
(Âµi âˆ’ Î»i)x
>i x + b,
only involve inner products among the data points xi and x, and since the objective function
we can kernelize the
âˆ’G(Î±, Âµ) of the dual program also only involves inner products among the data points
Î½-SV regression method.
xi
,
56.4. KERNEL Î½-SV REGRESSION 2097
As in the previous section, we assume that our data points {x1, . . . , xm} belong to a set X
and we pretend that we have feature space (F,hâˆ’, âˆ’i) and a feature embedding map Ï•: X â†’
F, but we only have access to the kernel function Îº(xi
, xj ) = h Ï•(xi), Ï•(xj )i . We wish to
perform Î½-SV regression in the feature space F on the data set {(Ï•(x1), y1), . . . ,(Ï•(xm), ym)}.
Going over the previous computation, we see that the primal program is given by
Program kernel Î½-SV Regression:
minimize
1
2
h
w, wi + C
 Î½ +
m
1
mX
i=1
(Î¾i + Î¾i
0
)

subject to
h
w, Ï•(xi)i + b âˆ’ yi â‰¤  + Î¾i
, Î¾i â‰¥ 0 i = 1, . . . , m
âˆ’ hw, Ï•(xi)i âˆ’ b + yi â‰¤  + Î¾i
0
, Î¾i
0 â‰¥ 0 i = 1, . . . , m

â‰¥ 0,
minimizing over the variables w, , b, Î¾, and Î¾
0 .
The Lagrangian is given by
L(w, b, Î», Âµ, Î³, Î¾, Î¾0 , , Î±, Î²) = 1
2
h
w, wi +
* w,
mX
i=1
(Î»i âˆ’ Âµi)Ï•(xi)
+
+ 
 CÎ½ âˆ’ Î³ âˆ’
mX
i=1
(Î»i + Âµi)
! +
mX
i=1
Î¾i

m
C
âˆ’ Î»i âˆ’ Î±i

+
mX
i=1
Î¾i
0
 m
C
âˆ’ Âµi âˆ’ Î²i
 + b
 
mX
i=1
(Î»i âˆ’ Âµi)
! âˆ’
mX
i=1
(Î»i âˆ’ Âµi)yi
.
Setting the gradient âˆ‡Lw,,b,Î¾,Î¾0 of the Lagrangian to zero, we also obtain the equations
w =
mX
i=1
(Âµi âˆ’ Î»i)Ï•(xi), (âˆ—w)
mX
i=1
Î»i âˆ’
mX
i=1
Âµi = 0
mX
i=1
Î»i +
mX
i=1
Âµi + Î³ = CÎ½
Î» + Î± =
C
m
, Âµ + Î² =
C
m
.
Using the above equations, we find that the dual function G is independent of the variables
Î², Î±, Î², and we obtain the following dual program:
2098 CHAPTER 56. Î½-SV REGRESSION
Dual Program kernel Î½-SV Regression:
minimize
1
2
mX
i,j=1
(Î»i âˆ’ Âµi)(Î»j âˆ’ Âµj )Îº(xi
, xj ) +
mX
i=1
(Î»i âˆ’ Âµi)yi
subject to
mX
i=1
Î»i âˆ’
mX
i=1
Âµi = 0
mX
i=1
Î»i +
mX
i=1
Âµi â‰¤ CÎ½
0 â‰¤ Î»i â‰¤
C
m
, 0 â‰¤ Âµi â‰¤
C
m
, i = 1, . . . , m,
minimizing over Î± and Âµ.
Everything we said before also applies to the kernel Î½-SV regression method, except that
xi
is replaced by Ï•(xi) and that the inner product hâˆ’, âˆ’i must be used, and we have the
formulae
w =
mX
i=1
(Âµi âˆ’ Î»i)Ï•(xi)
b =
1
2
 
yi0 + yj0 âˆ’
mX
i=1
(Âµi âˆ’ Î»i)(Îº(xi
, xi0
) + Îº(xi
, xj0
))!
f(x) =
mX
i=1
(Âµi âˆ’ Î»i)Îº(xi
, x) + b,
expressions that only involve Îº.
Remark: There is a variant of Î½-SV regression obtained by setting Î½ = 0 and holding  > 0
fixed. This method is called  -SV regression or (linear)  -insensitive SV regression. The
corresponding optimization program is
Program  -SV Regression:
minimize
1
2
w
> w +
m
C
mX
i=1
(Î¾i + Î¾i
0
)
subject to
w
> xi + b âˆ’ yi â‰¤  + Î¾i
, Î¾i â‰¥ 0 i = 1, . . . , m
âˆ’ w
> xi âˆ’ b + yi â‰¤  + Î¾i
0
, Î¾i
0 â‰¥ 0 i = 1, . . . , m,
minimizing over the variables w, b, Î¾, and Î¾
0 , holding  fixed.
It is easy to see that the dual program is
56.5. Î½-REGRESSION VERSION 2; PENALIZING b 2099
Dual Program  -SV Regression:
minimize
2
1
mX
i,j=1
(Î»i âˆ’ Âµi)(Î»j âˆ’ Âµj )x
>i xj +
mX
i=1
(Î»i âˆ’ Âµi)yi + 
mX
i=1
(Î»i + Âµi)
subject to
mX
i=1
Î»i âˆ’
mX
i=1
Âµi = 0
0 â‰¤ Î»i â‰¤
C
m
, 0 â‰¤ Âµi â‰¤
C
m
, i = 1, . . . , m,
minimizing over Î± and Âµ.
The constraint
mX
i=1
Î»i +
mX
i=1
Âµi â‰¤ CÎ½
is gone but the extra term  P m
i=1(Î»i + Âµi) has been added to the dual function, to prevent
Î»i and Âµi
from blowing up.
There is an obvious kernelized version of  -SV regression. It is easy to show that Î½-SV
regression subsumes  -SV regression, in the sense that if Î½-SV regression succeeds and yields
w, b,  > 0, then  -SV regression with the same C and the same value of  also succeeds
and returns the same pair (w, b). For more details on these methods, see SchÂ¨olkopf, Smola,
Williamson, and Bartlett [147].
Remark: The linear penalty function P m
i=1(Î¾i+Î¾i
0
) can be replaced by the quadratic penalty
function P m
i=1(Î¾i
2 + Î¾i
0
2
); see Shaweâ€“Taylor and Christianini [159] (Chapter 7). In this case,
it is easy to see that for an optimal solution we must have Î¾i â‰¥ 0 and Î¾i
0 â‰¥ 0, so we may
omit the constraints Î¾i â‰¥ 0 and Î¾i
0 â‰¥ 0. We must also have Î³ = 0 so we omit the variable Î³
as well. It can be shown that Î¾ = (m/2C)Î» and Î¾
0 = (m/2C)Âµ. This problem is very similar
to the Soft Margin SVM (SVMs4) discussed in Section 54.13.
56.5 Î½-Regression Version 2; Penalizing b
Yet another variant of Î½-SV regression is to add the term 2
1
b
2
to the objective function.
We will see that solving the dual not only determines w but also b and  (provided a mild
condition on Î½). We wish to solve the following program:
2100 CHAPTER 56. Î½-SV REGRESSION
Program Î½-SV Regression Version 2
minimize
1
2
w
> w +
1
2
b
2 + C
 Î½ +
m
1
mX
i=1
(Î¾i + Î¾i
0
)

subject to
w
> xi + b âˆ’ yi â‰¤  + Î¾i
, Î¾i â‰¥ 0 i = 1, . . . , m
âˆ’ w
> xi âˆ’ b + yi â‰¤  + Î¾i
0
, Î¾i
0 â‰¥ 0 i = 1, . . . , m,
minimizing over the variables w, b, , Î¾, and Î¾
0 . The constraint  â‰¥ 0 is omitted since the
problem has no solution if  < 0.
We leave it as an exercise to show that the new Lagrangian is
L(w, b, Î», Âµ, Î¾, Î¾0 , , Î±, Î²) = 1
2
w
> w + w
>
 
mX
i=1
(Î»i âˆ’ Âµi)xi
!
+ 
 CÎ½ âˆ’
mX
i=1
(Î»i + Âµi)
! +
mX
i=1
Î¾i

m
C
âˆ’ Î»i âˆ’ Î±i

+
mX
i=1
Î¾i
0
 m
C
âˆ’ Âµi âˆ’ Î²i
 +
1
2
b
2 + b
 
mX
i=1
(Î»i âˆ’ Âµi)
! âˆ’
mX
i=1
(Î»i âˆ’ Âµi)yi
.
If we set the Laplacian âˆ‡Lw,,b,Î¾,Î¾0 to zero we obtain the equations
w =
mX
i=1
(Âµi âˆ’ Î»i)xi = X
> (Âµ âˆ’ Î») (âˆ—w)
CÎ½ âˆ’
mX
i=1
(Î»i + Âµi) = 0
b +
mX
i=1
(Î»i âˆ’ Âµi) = 0
C
m
âˆ’ Î» âˆ’ Î± = 0,
C
m
âˆ’ Âµ âˆ’ Î² = 0.
We obtain the new equation
b = âˆ’
mX
i=1
(Î»i âˆ’ Âµi) = âˆ’(1
>mÎ» âˆ’ 1
>mÂµ) (âˆ—b)
determining b, which replaces the equation
mX
i=1
Î»i âˆ’
mX
i=1
Âµi = 0.
56.5. Î½-REGRESSION VERSION 2; PENALIZING b 2101
Plugging back w from (âˆ—w) and b from (âˆ—b) into the Lagrangian we get
G(Î», Âµ, Î±, Î²) = âˆ’
1
2
ï¿¾
Î»
> Âµ
>
 P

Âµ
Î»

âˆ’ q
>

Âµ
Î»

+
1
2
b
2 âˆ’ b
2
= âˆ’
1
2
ï¿¾
Î»
> Âµ
>
 P

Âµ
Î»

âˆ’ q
>

Âµ
Î»

âˆ’
1
2
b
2
= âˆ’
1
2
ï¿¾
Î»
> Âµ
>

 P +

1m1
>m âˆ’1m1
>m
âˆ’1m1
>m 1m1
>m
  Âµ
Î»

âˆ’ q
>

Âµ
Î»

,
with
P =

XX> âˆ’XX>
âˆ’XX> XX>  =

âˆ’
K
K K
âˆ’K

and
q =

âˆ’
y
y

.
The new dual program is
Dual Program Î½-SV Regression Version 2
minimize
1
2
ï¿¾
Î»
> Âµ
>

 P +

1m1
>m âˆ’1m1
>m
âˆ’1m1
>m 1m1
>m
  Âµ
Î»

+ q
>

Âµ
Î»

subject to
mX
i=1
Î»i +
mX
i=1
Âµi = CÎ½
0 â‰¤ Î»i â‰¤
C
m
, 0 â‰¤ Âµi â‰¤
C
m
, i = 1, . . . , m.
Definition 56.1 and Definition 56.2 are unchanged. We have the following version of
Proposition 56.2 showing that pf , qf , pm an qm have direct influence on the choice of Î½.
Proposition 56.7. (1) Let pf be the number of points xi such that Î»i = C/m, and let qf
be the number of points xi such that Âµi = C/m. We have pf + qf â‰¤ mÎ½.
(2) Let pm be the number of points xi such that Î»i > 0, and let qm be the number of points
xi such that Âµi > 0. We have pm + qm â‰¥ mÎ½.
(3) If pf â‰¥ 1 or qf â‰¥ 1, then Î½ â‰¥ 1/m.
Proof. (1) Let KÎ» and KÂµ be the sets of indices corresponding to points failing the margin,
KÎ» = {i âˆˆ {1, . . . , m} | Î»i = C/m}
KÂµ = {i âˆˆ {1, . . . , m} | Âµi = C/m}.
2102 CHAPTER 56. Î½-SV REGRESSION
By definition pf = |KÎ»|, qf = |KÂµ|. Since the equation
mX
i=1
Î»i +
mX
j=1
Âµj = CÎ½
holds, by definition of KÎ» and KÂµ we have
(pf + qf )
C
m
=
X
iâˆˆKÎ»
Î»i +
X
jâˆˆKÂµ
Âµj â‰¤
mX
i=1
Î»i +
mX
j=1
Âµj = CÎ½,
which implies that
pf + qf â‰¤ mÎ½.
(2) Let IÎ»>0 and IÂµ>0 be the sets of indices
IÎ»>0 = {i âˆˆ {1, . . . , m} | Î»i > 0}
IÂµ>0 = {i âˆˆ {1, . . . , m} | Âµi > 0}.
By definition pm = |IÎ»>0|, qm = |IÂµ>0|. We have
mX
i=1
Î»i +
mX
j=1
Âµj =
X
iâˆˆIÎ»>0
Î»i +
X
jâˆˆIÂµ>0
Âµj = CÎ½.
Since Î»i â‰¤ C/m and Âµj â‰¤ C/m, we obtain
CÎ½ â‰¤ (pm + qm)
C
m
,
that is, pm + qm â‰¥ mÎ½.
(3) follows immediately from (1).
Proposition 56.7 yields the following bounds on Î½:
pf + qf
m
â‰¤ Î½ â‰¤
pm + qm
m
.
Again, the smaller Î½ is, the wider the  -slab is, and the larger Î½ is, the narrower the  -slab
is.
Remark: It can be shown that for any optimal solution with w 6 = 0 and  > 0, if the
inequalities (pf + qf )/m < Î½ < 1 hold, then some point xi
is a support vector. The proof is
essentially Case 1b in the proof of Proposition 56.4. We leave the details as an exercise.
56.5. Î½-REGRESSION VERSION 2; PENALIZING b 2103
The new dual program is solved using ADMM. The (2m+1)Ã—4m matrix A3 corresponding
to the equational constraints
mX
i=1
Î»i +
mX
i=1
Âµi = CÎ½
Î» + Î± =
C
m
, Âµ + Î² =
C
m
,
is given by
A3 =
ï£«
ï£¬ï£¬ï£­
1
Im
>
m 1
>m 0
>m 0
>m
0m,m Im 0m,m
0m,m Im 0m,m Im
ï£¶
ï£·ï£·ï£¸ .
We leave it as an exercise to show that A3 has rank 2m + 1. We define the vector c3 (of
dimension 2m + 1) as
c3 =

CÎ½
C
m
12m

.
Since there are 4m Lagrange multipliers (Î», Âµ, Î±, Î²), we need to pad the 2m Ã— 2m matrix
P3 = P +

1m1
>m âˆ’1m1
>m
âˆ’1m1
>m 1m1
>m

with zeros to make it into a 4m Ã— 4m matrix
P3a =

P3 02m,2m
02m,2m 02m,2m

.
Similarly, we pad q with zeros to make it a vector q3a of dimension 4m,
q3a =

02
q
m

.
It remains to compute  . Ther are two methods to do this.
The first method assumes the Standard Margin Hypothesis, which is that there is
some i0 such that 0 < Î»i0 < C/m or there is some j0 such that 0 < Âµj0 < C/m; in other
words, there is some support vector of type 1. By the complementary slackness conditions,
Î¾i0 = 0 or Î¾j
00 = 0, so we have either w
> xi0 + b âˆ’ yi0 =  or âˆ’w
> xj0 âˆ’ b + yj0 =  , which
determines  .
Due to numerical instability, when writing a computer program it is preferable to compute
the lists of indices IÎ» and IÂµ given by
IÎ» = {i âˆˆ {1, . . . , m} | 0 < Î»i < C/m}
IÂµ = {j âˆˆ {1, . . . , m} | 0 < Âµj < C/m}.
2104 CHAPTER 56. Î½-SV REGRESSION
Then it is easy to see that we can compute  using the following averaging formulae: if
IÎ» 6 = âˆ…, then

= w
>

X
iâˆˆIÎ»
xi
 /|IÎ»| + b âˆ’

X
iâˆˆIÎ»
yi
 /|IÎ»|,
and if IÂµ 6 = âˆ…, then

= âˆ’w
>

X
jâˆˆIÂµ
xj
 /|IÂµ| âˆ’ b +

X
iâˆˆIÂµ
yi
 /|IÂµ|.
The second method uses duality. Under a mild condition, expressing that the duality
gap is zero, we can determine  in terms of Î», Âµ and b. This is because points xi that fail the
margin, which means that Î»i = C/m or Âµi = C/m, are the only points for which Î¾i > 0 or
Î¾i
0 > 0. But in this case we have an active constraint
w
> xi + b âˆ’ yi =  + Î¾i (âˆ—Î¾)
or
âˆ’w
> xi âˆ’ b + yi =  + Î¾i
0
, (âˆ—Î¾
0 )
so Î¾i and Î¾i
0
can be expressed in terms of w and b. Since the duality gap is zero for an optimal
solution, the optimal value of the primal is equal to the optimal value of the dual. Using the
fact that
w = X
> (Âµ âˆ’ Î»)
b = âˆ’(1
>mÎ» âˆ’ 1
>mÂµ) = ï¿¾ Î»
> Âµ
>


âˆ’1m
1m

we obtain an expression for the optimal value of the primal. First we have
1
2
w
> w +
1
2
b
2 =
1
2
(Î»
> âˆ’ Âµ
> )XX> (Î» âˆ’ Âµ) + 1
2
ï¿¾
Î»
> Âµ
>


1m1
>m âˆ’1m1
>m
âˆ’1m1
>m 1m1
>m
  Âµ
Î»

=
1
2
ï¿¾
Î»
> Âµ
>

 P +

1m1
>m âˆ’1m1
>m
âˆ’1m1
>m 1m1
>m
