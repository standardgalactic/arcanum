0 = J − {i}, and using Formulae (2) and (4) of Proposition
34.18, we have
h
e
∗
J
,(e
∗
H y eH∪{i}) ∧ eJ−{i}i = h e
∗
J
, ρ{i},Hei ∧ eJ−{i}i = h e
∗
J
, ρ{i},Hρ{i},J−{i}eJ i = ρ{i},Hρ{i},J−{i}.
If we let

i,J,H = ρ{i},Hρ{i},J−{i},
we have  i,J,H = +1 if the parity of the number of j ∈ J such that j < i is the same as the
parity of the number of h ∈ H such that h < i, and  i,J,H = −1 otherwise.
Finally we obtain the following criterion in terms of quadratic equations (Pl¨ucker’s equa￾tions) for the decomposability of an alternating tensor.
Proposition 34.29. (Grassmann-Pl¨ucker’s Equations) For z =
P I
λI eI ∈
V
p E, the con￾ditions for z 6 = 0 to be decomposable are
X
i∈J−H

i,J,HλH∪{i}λJ−{i} = 0,
with  i,J,H = ρ{i},Hρ{i},J−{i}, for all H, J ⊆ {1, . . . , n} such that |H| = p−1, |J| = p + 1, and
all i ∈ J − H.
34.9. THE GRASSMANN-PLUCKER’S EQUATIONS AND GRASSMANNIANS ¨ ~ 1231
Using the above criterion, it is a good exercise to reprove that if dim(E) = n, then every
tensor in V n−1
(E) is decomposable. We already proved this fact as a corollary of Proposition
34.23.
Given any z =
P I
λI eI ∈
V
p E where dim(E) = n, the family of scalars (λI ) (with
I = {i1 < · · · < ip} ⊆ {1, . . . , n} listed in increasing order) is called the Pl¨ucker coordinates
of z.The Grassmann-Pl¨ucker’s equations give necessary and sufficient conditions for any
nonzero z to be decomposable.
For example, when dim(E) = n = 4 and p = 2, these equations reduce to the single
equation
λ12λ34 − λ13λ24 + λ14λ23 = 0.
However, it should be noted that the equations given by Proposition 34.29 are not indepen￾dent in general.
We are now in the position to prove that the Grassmannian G(p, n) can be embedded in
the projective space RP(
n
p)−1
,
For any n ≥ 1 and any k with 1 ≤ p ≤ n, recall that the Grassmannian G(p, n) is the
set of all linear p-dimensional subspaces of R
n
(also called p-planes). Any p-dimensional
subspace U of R
n
is spanned by p linearly independent vectors u1, . . . , up in R
n
; write U =
span(u1, . . . , uk). By Proposition 34.8, (u1, . . . , up) are linearly independent iff u1∧· · ·∧up 6 =
0. If (v1, . . . , vp) are any other linearly independent vectors spanning U, then we have
vj =
p
X
i=1
aijui
, 1 ≤ j ≤ p,
for some aij ∈ R, and by Proposition 34.2
v1 ∧ · · · ∧ vp = det(A) u1 ∧ · · · ∧ up,
where A = (aij ). As a consequence, we can define a map iG : G(p, n) → RP(
n
p)−1
such that
for any k-plane U, for any basis (u1, . . . , up) of U,
iG(U) = [u1 ∧ · · · ∧ up],
the point of RP(
n
p)−1
given by the one-dimensional subspace of R
(
n
p)
spanned by u1 ∧· · ·∧up.
Proposition 34.30. The map iG : G(p, n) → RP(
n
p)−1
is injective.
Proof. Let U and V be any two p-planes and assume that iG(U) = iG(V ). This means that
there is a basis (u1, . . . , up) of U and a basis (v1, . . . , vp) of V such that
v1 ∧ · · · ∧ vp = c u1 ∧ · · · ∧ up
1232 CHAPTER 34. EXTERIOR TENSOR POWERS AND EXTERIOR ALGEBRAS
for some nonzero c ∈ R. The above implies that the smallest subspaces W and W0 of R
n
such that u1 ∧ · · · ∧ up ∈
V
p W and v1 ∧ · · · ∧ vp ∈
V
p W0 are identical, so W = W0 . By
Corollary 34.26, this smallest subspace W has both (u1, . . . , up) and (v1, . . . , vp) as bases, so
the vj are linear combinations of the ui (and vice-versa), and U = V .
Since any nonzero z ∈
V
p R
n
can be uniquely written as
z =
X
I
λI eI
in terms of its Pl¨ucker coordinates (λI ), every point of RP(
n
p)−1
is defined by the Pl¨ucker
coordinates (λI ) viewed as homogeneous coordinates. The points of RP(
n
p)−1
correspond￾ing to one-dimensional spaces associated with decomposable alternating p-tensors are the
points whose coordinates satisfy the Grassmann-Pl¨ucker’s equations of Proposition 34.29.
Therefore, the map iG embeds the Grassmannian G(p, n) as an algebraic variety in RP(
n
p)−1
defined by equations of degree 2.
We can replace the field R by C in the above reasoning and we obtain an embedding of
the complex Grassmannian GC(p, n) as an algebraic variety in CP(
n
p)−1
defined by equations
of degree 2.
In particular, if n = 4 and p = 2, the equation
λ12λ34 − λ13λ24 + λ14λ23 = 0
is the homogeneous equation of a quadric in CP5
known as the Klein quadric. The points
on this quadric are in one-to-one correspondence with the lines in CP3
.
There is also a simple algebraic criterion to decide whether the smallest subspaces U and
V associated with two nonzero decomposable vectors u1 ∧ · · · ∧ up and v1 ∧ · · · ∧ vq have a
nontrivial intersection.
Proposition 34.31. Let E be any n-dimensional vector space over a field K, and let U
and V be the smallest subspaces of E associated with two nonzero decomposable vectors
u = u1 ∧ · · · ∧ up ∈
V
p U and v = v1 ∧ · · · ∧ vq ∈
V
q
V . The following properties hold:
(1) We have U ∩ V = (0) iff u ∧ v 6 = 0.
(2) If U ∩ V = (0), then U + V is the least subspace associated with u ∧ v.
Proof. Assume U ∩ V = (0). We know by Corollary 34.26 that (u1, . . . , up) is a basis of U
and (v1, . . . , vq) is a basis of V . Since U ∩V = (0), (u1, . . . , up, v1, . . . , vq) is a basis of U +V ,
and by Proposition 34.8, we have
u ∧ v = u1 ∧ · · · ∧ up ∧ v1 ∧ · · · ∧ vq 6 = 0.
This also proves (2).
34.10. VECTOR-VALUED ALTERNATING FORMS 1233
Conversely, assume that dim(U ∩V ) ≥ 1. Pick a basis (w1, . . . , wr) of W = U ∩V , and ex￾tend this basis to a basis (w1, . . . , wr, wr+1, . . . , wp) of U and to a basis (w1, . . . , wr, wp+1, . . .,
wp+q−r) of V . By Corollary 34.26, (u1, . . . , up) is also basis of U, so
u1 ∧ · · · ∧ up = a w1 ∧ · · · ∧ wr ∧ wr+1 ∧ · · · ∧ wp
for some a ∈ K, and (v1, . . . , vq) is also basis of V , so
v1 ∧ · · · ∧ vq = b w1 · · · ∧ wr ∧ wp+1 ∧ · · · ∧ wp+q−r
for some b ∈ K, and thus
u ∧ v = u1 ∧ · · · ∧ up ∧ v1 ∧ · · · ∧ vq = 0
since it contains some repeated wi
, with 1 ≤ i ≤ r.
As an application of Proposition 34.31, consider two projective lines D1 and D2 in RP3
,
which means that D1 and D2 correspond to two 2-planes in R
4
, and thus by Proposition
34.30, to two points in RP(
4
2)−1 = RP5
. These two points correspond to the 2-vectors
z = a1,2e1 ∧ e2 + a1,3e1 ∧ e3 + a1,4e1 ∧ e4 + a2,3e2 ∧ e3 + a2,4e2 ∧ e4 + a3,4e3 ∧ e4
and
z
0 = a
01,2
e1 ∧ e2 + a
01,3
e1 ∧ e3 + a
01,4
e1 ∧ e4 + a
02,3
e2 ∧ e3 + a
02,4
e2 ∧ e4 + a
03,4
e3 ∧ e4
whose Pl¨ucker coordinates, (where ai,j = λij ), satisfy the equation
λ12λ34 − λ13λ24 + λ14λ23 = 0
of the Klein quadric, and D1 and D2 intersect iff z ∧ z
0 = 0 iff
a1,2a
03,4 − a1,3a
03,4 + a1,4a
02,3 + a2,3a
01,4 − a2,4a
01,3 + a3,4a
01,2 = 0.
Observe that for D1 fixed, this is a linear condition. This fact is very helpful for solving
problems involving intersections of lines. A famous problem is to find how many lines in RP3
meet four given lines in general position. The answer is at most 2.
34.10 Vector-Valued Alternating Forms
The purpose of this section is to present the technical background needed to understand
vector-valued differential forms, in particular in the case of Lie groups where differential
forms taking their values in a Lie algebra arise naturally.
In this section the vector space E is assumed to have finite dimension. We know that
there is a canonical isomorphism V n
(E
∗
) ∼= Altn
(E; K) between alternating n-forms and
1234 CHAPTER 34. EXTERIOR TENSOR POWERS AND EXTERIOR ALGEBRAS
alternating multilinear maps. As in the case of general tensors, the isomorphisms provided
by Propositions 34.5, 33.17, and 34.10, namely
Altn
(E; F) ∼= Hom ^
n
(E), F
Hom ^
n
(E), F ∼=

^
n
(E)

∗
⊗ F

^
n
(E)

∗
∼=
^
n
(E
∗
)
yield a canonical isomorphism
Altn
(E; F) ∼=

^
n
(E
∗
)
 ⊗ F
which we record as a corollary.
Corollary 34.32. For any finite-dimensional vecgtor space E and any vector space F, we
have a canonical isomorphism
Altn
(E; F) ∼=

^
n
(E
∗
)
 ⊗ F.
Note that F may have infinite dimension. This isomorphism allows us to view the tensors
in V n
(E
∗
)⊗F as vector-valued alternating forms, a point of view that is useful in differential
geometry. If (f1, . . . , fr) is a basis of F, every tensor ω ∈
V
n
(E
∗
) ⊗ F can be written as
some linear combination
ω =
rX
i=1
αi ⊗ fi
,
with αi ∈
V
n
(E
∗
).We also let
^
(E; F) = M
n=0 
^
n
(E
∗
)
! ⊗ F =

^ (E)
 ⊗ F.
Given three vector spaces, F, G, H, if we have some bilinear map Φ: F × G → H, then
we can define a multiplication operation
∧Φ :
^ (E; F) ×
^ (E; G) →
^ (E; H)
as follows: For every pair (m, n), we define the multiplication
∧Φ :
 

^
m
(E
∗
)
 ⊗ F
! ×
 

^
n
(E
∗
)
 ⊗ G
! −→ 
m^+n
(E
∗
)
 ⊗ H
34.10. VECTOR-VALUED ALTERNATING FORMS 1235
by
ω ∧Φ η = (α ⊗ f) ∧Φ (β ⊗ g) = (α ∧ β) ⊗ Φ(f, g).
As in Section 34.5 (following H. Cartan [35]), we can also define a multiplication
∧Φ : Altm(E; F) × Altn
(E; G) −→ Altm+n
(E; H)
directly on alternating multilinear maps as follows: For f ∈ Altm(E; F) and g ∈ Altn
(E; G),
(f ∧Φ g)(u1, . . . , um+n) = X
σ∈shuffle(m,n)
sgn(σ) Φ f(uσ(1), . . . , uσ(m)), g(uσ(m+1), . . . , uσ(m+n))
 ,
where shuffle(m, n) consists of all (m, n)-“shuffles;” that is, permutations σ of {1, . . . m + n}
such that σ(1) < · · · < σ(m) and σ(m + 1) < · · · < σ(m + n).
A special case of interest is the case where F = G = H is a Lie algebra and Φ(a, b) = [a, b]
is the Lie bracket of F. In this case, using a basis (f1, . . . , fr) of F, if we write ω =
P i αi⊗fi
and η =
P j
βj ⊗ fj
, we have
ω ∧Φ η = [ω, η] = X
i,j
αi ∧ βj ⊗ [fi
, fj
].
It is customary to denote ω∧Φ η by [ω, η] (unfortunately, the bracket notation is overloaded).
Consequently,
[η, ω] = (−1)mn+1[ω, η].
In general not much can be said about ∧Φ, unless Φ has some additional properties. In
particular, ∧Φ is generally not associative.
We now use vector-valued alternating forms to generalize both the µ map of Proposition
34.14 and generalize Proposition 33.17 by defining the map
µF :
 
^
n
(E
∗
)
! ⊗ F −→ Altn
(E; F)
on generators by
µF ((v1
∗ ∧ · · · ∧ vn
∗
) ⊗ f)(u1, . . . , un) = (det(vj
∗
(ui))f,
with v1
∗
, . . . , vn
∗ ∈ E
∗
, u1, . . . , un ∈ E, and f ∈ F.
Proposition 34.33. The map
µF :
 
^
n
(E
∗
)
! ⊗ F −→ Altn
(E; F)
defined as above is a canonical isomorphism for every n ≥ 0. Furthermore, given any three
vector spaces, F, G, H, and any bilinear map Φ: F × G → H, for all ω ∈ (
V
n
(E
∗
)) ⊗ F and
all η ∈ (
V
n
(E
∗
)) ⊗ G,
µH(ω ∧Φ η) = µF (ω) ∧Φ µG(η).
1236 CHAPTER 34. EXTERIOR TENSOR POWERS AND EXTERIOR ALGEBRAS
Proof. Since we already know that (V n
(E
∗
))⊗F and Altn
(E; F) are isomorphic, it is enough
to show that µF maps some basis of (V n
(E
∗
)) ⊗ F to linearly independent elements. Pick
some bases (e1, . . . , ep) in E and (fj )j∈J in F. Then we know that the vectors e
∗
I ⊗ fj
, where
I ⊆ {1, . . . , p} and |I| = n, form a basis of (V n
(E
∗
)) ⊗ F. If we have a linear dependence
X
I,j
λI,jµF (e
∗
I ⊗ fj ) = 0,
applying the above combination to each (ei1
, . . . , ein
) (I = {i1, . . . , in}, i1 < · · · < in), we
get the linear combination
X
j
λI,jfj = 0,
and by linear independence of the fj
’s, we get λI,j = 0 for all I and all j. Therefore, the
µF (e
∗
I ⊗ fj ) are linearly independent, and we are done. The second part of the proposition
is checked using a simple computation.
The following proposition will be useful in dealing with vector-valued differential forms.
Proposition 34.34. If (e1, . . . , ep) is any basis of E, then every element ω ∈ (
V
n
(E
∗
)) ⊗ F
can be written in a unique way as
ω =
X
I
e
∗
I ⊗ fI , fI ∈ F,
where the e
∗
I
are defined as in Section 34.2.
Proof. Since, by Proposition 34.7, the e
∗
I
form a basis of V n
(E
∗
), elements of the form e
∗
I ⊗f
span (V n
(E
∗
)) ⊗ F. Now if we apply µF (ω) to (ei1
, . . . , ein
), where I = {i1, . . . , in} ⊆
{1, . . . , p}, we get
µF (ω)(ei1
, . . . , ein
) = µF (e
∗
I ⊗ fI )(ei1
, . . . , ein
) = fI .
Therefore, the fI are uniquely determined by f.
Proposition 34.34 can also be formulated in terms of alternating multilinear maps, a fact
that will be useful to deal with differential forms.
Corollary 34.35. Define the product ·: Altn
(E; R) × F → Altn
(E; F) as follows: For all
ω ∈ Altn
(E; R) and all f ∈ F,
(ω · f)(u1, . . . , un) = ω(u1, . . . , un)f,
for all u1, . . . , un ∈ E. Then for every ω ∈ (
V
n
(E
∗
)) ⊗ F of the form
ω = u
∗
1 ∧ · · · ∧ u
∗
n ⊗ f,
we have
µF (u
∗
1 ∧ · · · ∧ u
∗
n ⊗ f) = µF (u
∗
1 ∧ · · · ∧ u
∗
n
) · f.
34.11. PROBLEMS 1237
Then Proposition 34.34 yields the following result.
Proposition 34.36. If (e1, . . . , ep) is any basis of E, then every element ω ∈ Altn
(E; F)
can be written in a unique way as
ω =
X
I
e
∗
I
· fI , fI ∈ F,
where the e
∗
I
are defined as in Section 34.2.
34.11 Problems
Problem 34.1. Complete the induction argument used in the proof of Proposition 34.1 (2).
Problem 34.2. Prove Proposition 34.2.
Problem 34.3. Prove Proposition 34.9.
Problem 34.4. Show that the pairing given by (∗) in Section 34.4 is nondegenerate.
Problem 34.5. Let Ia be the two-sided ideal generated by all tensors of the form u ⊗ u ∈
V
⊗2
. Prove that
m^
(V ) ∼= V
⊗m/(Ia ∩ V
⊗m).
Problem 34.6. Complete the induction proof of Proposition 34.12.
Problem 34.7. Prove the following lemma: If V is a vector space with dim(V ) ≤ 3, then
α ∧ α = 0 whenever α ∈
V (V ).
Problem 34.8. Prove Proposition 34.13.
Problem 34.9. Given two graded algebras E and F, define E ⊗b F to be the vector space
E ⊗ F, but with a skew-commutative multiplication given by
(a ⊗ b) ∧ (c ⊗ d) = (−1)deg(b)deg(c)
(ac) ⊗ (bd),
where a ∈ E
m, b ∈ F
p
, c ∈ E
n
, d ∈ F
q
. Show that
^
(E ⊕ F) ∼=
^ (E) ⊗b
^ (F).
Problem 34.10. If h−, −i denotes the inner product on V , recall that we defined an inner
product on V k
V , also denoted h−, −i, by setting
h
u1 ∧ · · · ∧ uk, v1 ∧ · · · ∧ vki = det(h ui
, vj i ),
for all ui
, vi ∈ V , and extending h−, −i by bilinearity.
Show that if (e1, . . . , en) is an orthonormal basis of V , then the basis of V k
V consisting
of the eI (where I = {i1, . . . , ik}, with 1 ≤ i1 < · · · < ik ≤ n) is also an orthonormal basis of
V
k
V .
1238 CHAPTER 34. EXTERIOR TENSOR POWERS AND EXTERIOR ALGEBRAS
Problem 34.11. Show that
(u
∗ ∧ v
∗
) y z = u
∗
y
(v
∗
y
z),
whenever u
∗ ∈
V
k E
∗
, v
∗ ∈
V
p−k E
∗
, and z ∈
V
p+q E.
Problem 34.12. Prove Statement (3) of Proposition 34.18.
Problem 34.13. Prove Proposition 34.19.
Also prove the identity
u
∗
y
(x ∧ y) = (−1)s
(u
∗
y x) ∧ y + x ∧ (u
∗
y
y),
where u
∗ ∈ E
∗
, x ∈
V
q+1−s E, and y ∈
V
s E.
Problem 34.14. Use the Grassmann-Pl¨ucker’s equations prove that if dim(E) = n, then
every tensor in V n−1
(E) is decomposable.
Problem 34.15. Recall that the map
µF :
 
^
n
(E
∗
)
! ⊗ F −→ Altn
(E; F)
is defined on generators by
µF ((v1
∗ ∧ · · · ∧ vn
∗
) ⊗ f)(u1, . . . , un) = (det(vj
∗
(ui))f,
with v1
∗
, . . . , vn
∗ ∈ E
∗
, u1, . . . , un ∈ E, and f ∈ F.
Given any three vector spaces, F, G, H, and any bilinear map Φ: F × G → H, for all
ω ∈ (
V
n
(E
∗
)) ⊗ F and all η ∈ (
V
n
(E
∗
)) ⊗ G prove that
µH(ω ∧Φ η) = µF (ω) ∧Φ µG(η).
Chapter 35
Introduction to Modules; Modules
over a PID
35.1 Modules over a Commutative Ring
In this chapter we introduce modules over a commutative ring (with unity). After a quick
overview of fundamental concepts such as free modules, torsion modules, and some basic
results about them, we focus on finitely generated modules over a PID and we prove the
structure theorems for this class of modules (invariant factors and elementary divisors). Our
main goal is not to give a comprehensive exposition of modules, but instead to apply the
structure theorem to the K[X]-module Ef defined by a linear map f acting on a finite￾dimensional vector space E, and to obtain several normal forms for f, including the rational
canonical form.
A module is the generalization of a vector space E over a field K obtained replacing the
field K by a commutative ring A (with unity 1). Although formally the definition is the same,
the fact that some nonzero elements of A are not invertible has some serious consequences.
For example, it is possible that λ · u = 0 for some nonzero λ ∈ A and some nonzero u ∈ E,
and a module may no longer have a basis.
For the sake of completeness, we give the definition of a module, although it is the same
as Definition 3.1 with the field K replaced by a ring A. In this chapter, all rings under
consideration are assumed to be commutative and to have an identity element 1.
Definition 35.1. Given a ring A, a (left) module over A (or A-module) is a set M (of vectors)
together with two operations +: M ×M → M (called vector addition),1 and ·: A×M → M
(called scalar multiplication) satisfying the following conditions for all α, β ∈ A and all
u, v ∈ M;
(M0) M is an abelian group w.r.t. +, with identity element 0;
1The symbol + is overloaded, since it denotes both addition in the ring A and addition of vectors in M.
It is usually clear from the context which + is intended.
1239
1240 CHAPTER 35. INTRODUCTION TO MODULES; MODULES OVER A PID
(M1) α · (u + v) = (α · u) + (α · v);
(M2) (α + β) · u = (α · u) + (β · u);
(M3) (α ∗ β) · u = α · (β · u);
(M4) 1 · u = u.
Given α ∈ A and v ∈ M, the element α · v is also denoted by αv. The ring A is often
called the ring of scalars.
Unless specified otherwise or unless we are dealing with several different rings, in the rest
of this chapter, we assume that all A-modules are defined with respect to a fixed ring A.
Thus, we will refer to a A-module simply as a module.
From (M0), a module always contains the null vector 0, and thus is nonempty. From
(M1), we get α · 0 = 0, and α · (−v) = −(α · v). From (M2), we get 0 · v = 0, and
(−α) · v = −(α · v). The ring A itself can be viewed as a module over itself, addition of
vectors being addition in the ring, and multiplication by a scalar being multiplication in the
ring.
When the ring A is a field, an A-module is a vector space. When A = Z, a Z-module is
just an abelian group, with the action given by
0 · u = 0,
n · u = u
| + · · ·
{z + u
}
n
, n > 0
n · u = −(−n) · u, n < 0.
All definitions from Section 3.4, linear combinations, linear independence and linear
dependence, subspaces renamed as submodules, apply unchanged to modules. Proposition
3.5 also holds for the module spanned by a set of vectors. The definition of a basis (Definition
3.6) also applies to modules, but the only result from Section 3.5 that holds for modules
is Proposition 3.12. Unfortunately, it is longer true that every module has a basis. For
example, for any nonzero integer n ∈ Z, the Z-module Z/nZ has no basis since n · x = 0 for
all x ∈ Z/nZ. Similarly, Q, as a Z-module, has no basis. Any two distinct nonzero elements
p1/q1 and p2/q2 are linearly dependent, since
(p2q1)

p
q1
1
 − (p1q2)

p
q2
2
 = 0.
Furthermore, the Z-module Q is not finitely generated. For if {p1/q1, · · · pn/qn} ⊂ Q gener￾ated Q, then for any x = r/s ∈ Q, we have
c1
p1
q1
+ · · · + cn
pn
qn
=
r
s
,
35.1. MODULES OVER A COMMUTATIVE RING 1241
where ci ∈ Z for i = 1, . . . , n. The left hand side of the preceding line is equivalent to
c1p1q2 · · · qn + · · · + cnpnq1 · · · qn−1
q1q2 · · · qn
,
where the numerator is an element of the ideal in Z spanned by (c1, c2, · · · , cn). Since Z is
a PID, there exists a ∈ Z such that (a) is the ideal spanned by (c1, c2, · · · , cn). Thus
c1
p1
q1
+ · · · + cn
pn
qn
=
ma
q1q2 · · · qn
=
r
s
,
where m ∈ Z. Set
a
q1q2 · · · qn
=
a1
b
, (a1, b) = 1.
Then if Q was a finitely generated Z-module, we deduce that for all x ∈ Q
x =
r
s
= m
a1
b
,
whenever a1/b is a fixed rational number, clearly a contradiction. (In particular let x = 1/p
where p is a fixed prime p > b. If ma1/b = 1/p, then ma1 ∈ Z with ma1 = b1/p, an
impossiblity since (b1, p) = 1 and p > b1.)
Definition 3.11 can be generalized to rings and yields free modules.
Definition 35.2. Given a commutative ring A and any (nonempty) set I, let A(I) be the
subset of the cartesian product AI
consisting of all families (λi)i∈I with finite support of
scalars in A.
2 We define addition and multiplication by a scalar as follows:
(λi)i∈I + (µi)i∈I = (λi + µi)i∈I ,
and
λ · (µi)i∈I = (λµi)i∈I .
It is immediately verified that addition and multiplication by a scalar are well defined.
Thus, A(I)
is a module. Furthermore, because families with finite support are considered, the
family (ei)i∈I of vectors ei
, defined such that (ei)j = 0 if j 6 = i and (ei)i = 1, is clearly a basis
of the module A(I)
. When I = {1, . . . , n}, we denote A(I) by An
. The function ι: I → A(I)
,
such that ι(i) = ei
for every i ∈ I, is clearly an injection.
Definition 35.3. An A-module M is free iff it has a basis.
The module A(I)
is a free module.
All definitions from Section 3.7 apply to modules, linear maps, kernel, image, except the
definition of rank, which has to be defined differently. Propositions 3.17, 3.18, 3.19, and
2Where AI denotes the set of all functions from I to A.
1242 CHAPTER 35. INTRODUCTION TO MODULES; MODULES OVER A PID
3.20 hold for modules. However, the other propositions do not generalize to modules. The
definition of an isomorphism generalizes to modules. As a consequence, a module is free iff
it is isomorphic to a module of the form A(I)
.
Section 3.8 generalizes to modules. Given a submodule N of a module M, we can define
the quotient module M/N.
If a is an ideal in A and if M is an A-module, we define aM as the set of finite sums of
the form
a1m1 + · · · + akmk, ai ∈ a, mi ∈ M.
It is immediately verified that aM is a submodule of M.
Interestingly, the part of Theorem 3.11 that asserts that any two bases of a vector space
have the same cardinality holds for modules. One way to prove this fact is to “pass” to a
vector space by a quotient process.
Theorem 35.1. For any free module M, any two bases of M have the same cardinality.
Proof sketch. We give the argument for finite bases, but it also holds for infinite bases. The
trick is to pick any maximal ideal m in A (whose existence is guaranteed by Theorem C.3).
Then, A/m is a field, and M/mM can be made into a vector space over A/m; we leave the
details as an exercise. If (u1, . . . , un) is a basis of M, then it is easy to see that the image of
this basis is a basis of the vector space M/mM. By Theorem 3.11, the number n of elements
in any basis of M/mM is an invariant, so any two bases of M must have the same number
of elements.
Definition 35.4. The common number of elements in any basis of a free module is called
the dimension (or rank) of the free module.
One should realize that the notion of linear independence in a module is a little tricky.
According to the definition, the one-element sequence (u) consisting of a single nonzero
vector is linearly independent if for all λ ∈ A, if λu = 0 then λ = 0. However, there are free
modules that contain nonzero vectors that are not linearly independent! For example, the
ring A = Z/6Z viewed as a module over itself has the basis (1), but the zero-divisors, such
as 2 or 4, are not linearly independent. Using language introduced in Definition 35.5, a free
module may have torsion elements. There are also nonfree modules such that every nonzero
vector is linearly independent, such as Q over Z.
All definitions from Section 4.1 about matrices apply to free modules, and so do all
the propositions. Similarly, all definitions from Section 6.1 about direct sums and direct
products apply to modules. All propositions that do not involve extending bases still hold.
The important Proposition 6.15 survives in the following form.
35.1. MODULES OVER A COMMUTATIVE RING 1243
Proposition 35.2. Let f : E → F be a surjective linear map between two A-modules with F
a free module. Given any basis (v1, . . . , vr) of F, for any r vectors u1, . . . , ur ∈ E such that
f(ui) = vi for i = 1, . . . , r, the vectors (u1, . . . , ur) are linearly independent and the module
E is the direct sum
E = Ker (f) ⊕ U,
where U is the free submodule of E spanned by the basis (u1, . . . , ur).
Proof. Pick any w ∈ E, write f(w) over the basis (v1, . . . , vr) as f(w) = a1v1 + · · · + arvr,
and let u = a1u1 + · · · + arur. Observe that
f(w − u) = f(w) − f(u)
= a1v1 + · · · + arvr − (a1f(u1) + · · · + arf(ur))
= a1v1 + · · · + arvr − (a1v1 + · · · + arvr)
= 0.
Therefore, h = w − u ∈ Ker (f), and since w = h + u with h ∈ Ker (f) and u ∈ U, we have
E = Ker (f) + U.
If u = a1u1 + · · · + arur ∈ U also belongs to Ker (f), then
0 = f(u) = f(a1u1 + · · · + arur) = a1v1 + · · · + arvr,
and since (v1, . . . , vr) is a basis, ai = 0 for i = 1, . . . , r, which shows that Ker (f) ∩ U = (0).
Therefore, we have a direct sum
E = Ker (f) ⊕ U.
Finally, if
a1u1 + · · · + arur = 0,
the above reasoning shows that ai = 0 for i = 1, . . . , r, so (u1, . . . , ur) are linearly indepen￾dent. Therefore, the module U is a free module.
One should be aware that if we have a direct sum of modules
U = U1 ⊕ · · · ⊕ Um,
every vector u ∈ U can be written is a unique way as
u = u1 + · · · + um,
with ui ∈ Ui but, unlike the case of vector spaces, this does not imply that any m nonzero
vectors (u1, . . . , um) are linearly independent. For example, we have the direct sum
Z/2Z ⊕ Z/2Z
1244 CHAPTER 35. INTRODUCTION TO MODULES; MODULES OVER A PID
where Z/2Z is viewed as a Z-modules, but (1, 0) and (0, 1) are not linearly independent,
since
2(1, 0) + 2(0, 1) = (0, 0).
A useful fact is that every module is a quotient of some free module. Indeed, if M is
an A-module, pick any spanning set I for M (such a set exists, for example, I = M), and
consider the unique homomorphism ϕ: A(I) → M extending the identity function from I to
itself. Then we have an isomorphism A(I)/Ker (ϕ) ≈ M.
In particular, if M is finitely generated, we can pick I to be a finite set of generators, in
which case we get an isomorphism An/Ker (ϕ) ≈ M, for some natural number n. A finitely
generated module is sometimes called a module of finite type.
The case n = 1 is of particular interest. A module M is said to be cyclic if it is generated
by a single element. In this case M = Ax, for some x ∈ M. We have the linear map
mx : A → M given by a 7→ ax for every a ∈ A, and it is obviously surjective since M = Ax.
Since the kernel a = Ker (mx) of mx is an ideal in A, we get an isomorphism A/a ≈ Ax.
Conversely, for any ideal a of A, if M = A/a, we see that M is generated by the image x of
1 in M, so M is a cyclic module.
The ideal a = Ker (mx) is the set of all a ∈ A such that ax = 0. This is called the
annihilator of x, and it is the special case of the following more general situation.
Definition 35.5. If M is any A-module, for any subset S of M, the set of all a ∈ A such
that ax = 0 for all x ∈ S is called the annihilator of S, and it is denoted by Ann(S). If
S = {x}, we write Ann(x) instead of Ann({x}). A nonzero element x ∈ M is called a torsion
element iff Ann(x) 6 = (0). The set consisting of all torsion elements in M and 0 is denoted
by Mtor.
It is immediately verified that Ann(S) is an ideal of A, and by definition,
Mtor = {x ∈ M | (∃a ∈ A, a 6 = 0)(ax = 0)}.
If a ring has zero divisors, then the set of all torsion elements in an A-module M may not
be a submodule of M. For example, if M = A = Z/6Z, then Mtor = {2, 3, 4}, but 3 + 4 = 1
is not a torsion element. Also, a free module may not be torsion-free because there may be
torsion elements, as the example of Z/6Z as a free module over itself shows.
However, if A is an integral domain, then a free module is torsion-free and Mtor is a
submodule of M. (Recall that an integral domain is commutative).
Proposition 35.3. If A is an integral domain, then for any A-module M, the set Mtor of
torsion elements in M is a submodule of M.
Proof. If x, y ∈ M are torsion elements (x, y 6 = 0), then there exist some nonzero elements
a, b ∈ A such that ax = 0 and by = 0. Since A is an integral domain, ab 6 = 0, and then for
all λ, µ ∈ A, we have
ab(λx + µy) = bλax + aµby = 0.
35.1. MODULES OVER A COMMUTATIVE RING 1245
Therefore, Mtor is a submodule of M.
The module Mtor is called the torsion submodule of M. If Mtor = (0), then we say that
M is torsion-free, and if M = Mtor, then we say that M is a torsion module.
If M is not finitely generated, then it is possible that Mtor 6 = 0, yet the annihilator of
Mtor is reduced to 0. For example, let take the Z-module
Z/2Z × Z/3Z × Z/5Z × · · · × Z/pZ × · · · ,
where p ranges over the set of primes. Call this module M and the set of primes P. Observe
that M is generated by {αp}p∈P , where αp is the tuple whose only nonzero entry is 1p, the
generator of Z/pZ, i.e.,
αp = (0, 0, 0, · · · , 1p, 0, · · ·), Z/pZ = {n · 1p}
p
n
−
=0
1
.
In other words, M is not finitely generated. Furthermore, since p·1p = 0, we have {αp}p∈P ⊂
Mtor. However, because p ranges over all primes, the only possible nonzero annihilator of
{αp}p∈P would be the product of all the primes. Hence Ann({αp}p∈P ) = (0). Because of the
subset containment, we conclude that Ann(Mtor) = (0).
However, if M is finitely generated, it is not possible that Mtor 6 = 0, yet the annihilator
of Mtor is reduced to 0, since if x1, . . . , xn generate M and if a1, . . . , an annihilate x1, . . . , xn,
then a1 · · · an annihilates every element of M.
Proposition 35.4. If A is an integral domain, then for any A-module M, the quotient
module M/Mtor is torsion free.
Proof. Let x be an element of M/Mtor and assume that ax = 0 for some a 6 = 0 in A. This
means that ax ∈ Mtor, so there is some b 6 = 0 in A such that bax = 0. Since a, b 6 = 0 and A
is an integral domain, ba 6 = 0, so x ∈ Mtor, which means that x = 0.
If A is an integral domain and if F is a free A-module with basis (u1, . . . , un), then F
can be embedded in a K-vector space FK isomorphic to Kn
, where K = Frac(A) is the
fraction field of A. Similarly, any submodule M of F is embedded into a subspace MK of
FK. Note that any linearly independent vectors (u1, . . . , um) in the A-module M remain
linearly independent in the vector space MK, because any linear dependence over K is of
the form
a1
b1
u1 + · · · +
am
bm
um = 0
for some ai
, bi ∈ A, with b1 · · · bm 6 = 0, so if we multiply by b1 · · · bm 6 = 0, we get a lin￾ear dependence in the A-module M. Then we see that the maximum number of linearly
independent vectors in the A-module M is at most n. The maximum number of linearly
independent vectors in a finitely generated submodule of a free module (over an integral
domain) is called the rank of the module M. If (u1, . . . , um) are linearly independent where
1246 CHAPTER 35. INTRODUCTION TO MODULES; MODULES OVER A PID
m is the rank of m, then for every nonzero v ∈ M, there are some a, a1, . . . , am ∈ A, not all
zero, such that
av = a1u1 + · · · + amum.
We must have a 6 = 0, since otherwise, linear independence of the ui would imply that
a1 = · · · = am = 0, contradicting the fact that a, a1, . . . , am ∈ A are not all zero.
Unfortunately, in general, a torsion-free module is not free. For example, Q as a Z-module
is torsion-free but not free. If we restrict ourselves to finitely generated modules over PID’s,
then such modules split as the direct sum of their torsion module with a free module, and a
torsion module has a nice decomposition in terms of cyclic modules.
The following proposition shows that over a PID, submodules of a free module are free.
There are various ways of proving this result. We give a proof due to Lang [109] (see Chapter
III, Section 7).
Proposition 35.5. If A is a PID and if F is a free A-module of dimension n, then every
submodule M of F is a free module of dimension at most n.
Proof. Let (u1, . . . , un) be a basis of F, and let Mr = M ∩(Au1 ⊕· · ·⊕Aur), the intersection
of M with the free module generated by (u1, . . . , ur), for r = 1, . . . , n. We prove by induction
on r that each Mr is free and of dimension at most r. Since M = Mr for some r, this will
prove our result.
Consider M1 = M ∩ Au1. If M1 = (0), we are done. Otherwise let
a = {a ∈ A | au1 ∈ M}.
It is immediately verified that a is an ideal, and since A is a PID, a = a1A, for some a1 ∈ A.
Since we are assuming that M1 6 = (0), we have a1 6 = 0, and a1u1 ∈ M. If x ∈ M1, then
x = au1 for some a ∈ A, so a ∈ a1A, and thus a = ba1 for some b ∈ A. It follows that
M1 = Aa1u1, which is free.
Assume inductively that Mr is free of dimension at most r < n, and let
a = {a ∈ A | (∃b1 ∈ A)· · ·(∃br ∈ A)(b1u1 + · · · + brur + aur+1 ∈ M)}.
It is immediately verified that a is an ideal, and since A is a PID, a = ar+1A, for some
ar+1 ∈ A. If ar+1 = 0, then Mr+1 = Mr, and we are done.
If ar+1 6 = 0, then there is some v1 ∈ Au1 ⊕ · · · ⊕ Aur such that
w = v1 + ar+1ur+1 ∈ M.
For any x ∈ Mr+1, there is some v ∈ Au1⊕· · ·⊕Aur and some a ∈ A such that x = v+aur+1.
Then, a ∈ ar+1A, so there is some b ∈ A such that a = bar+1. As a consequence
x − bw = v − bv1 ∈ Mr,
