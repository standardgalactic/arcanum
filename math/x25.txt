k
âˆ†xk
k
xk
â‰¤ cond(A)
k
âˆ†Ak
k
Ak
 1 âˆ’ kAâˆ’
1
1k k âˆ†Ak

.
Proof. The first inequality has already been proven. To show that equality can be achieved,
let w be any vector such that w 6 = 0 and
and let Î² 6 = 0 be any real number. Now the vectors


A
âˆ’1w
 =
  A
âˆ’1


k wk ,
âˆ†x = âˆ’Î²Aâˆ’1w
x + âˆ†x = w
b = (A + Î²I)w
9.5. CONDITION NUMBERS OF MATRICES 355
and the matrix
âˆ†A = Î²I
sastisfy the equations
Ax = b
(A + âˆ†A)(x + âˆ†x) = b
k
âˆ†xk = |Î²|
  A
âˆ’1w
 = k âˆ†Ak
  A
âˆ’1


k x + âˆ†xk .
Finally we can pick Î² so that âˆ’Î² is not equal to any of the eigenvalues of A, so that
A + âˆ†A = A + Î²I is invertible and b is is nonzero.
If k âˆ†Ak < 1/ k Aâˆ’1k
, then
so by Proposition 9.11, the matrix


A
âˆ’1
I
âˆ†
+
A

Aâˆ’
â‰¤
1âˆ†


A
A
âˆ’
is invertible and
1


k âˆ†Ak < 1,


(I + A
âˆ’1âˆ†A)
âˆ’1

 â‰¤
1 âˆ’ kA
1
âˆ’1âˆ†Ak
â‰¤
1
1 âˆ’ kAâˆ’1k k âˆ†Ak
.
Recall that we proved earlier that
âˆ†x = âˆ’A
âˆ’1âˆ†A(x + âˆ†x),
and by adding x to both sides and moving the right-hand side to the left-hand side yields
(I + A
âˆ’1âˆ†A)(x + âˆ†x) = x,
and thus
x + âˆ†x = (I + A
âˆ’1âˆ†A)
âˆ’1x,
which yields
âˆ†x = ((I + A
âˆ’1âˆ†A)
âˆ’1 âˆ’ I)x = (I + A
âˆ’1âˆ†A)
âˆ’1
(I âˆ’ (I + A
âˆ’1âˆ†A))x
= âˆ’(I + A
âˆ’1âˆ†A)
âˆ’1A
âˆ’1
(âˆ†A)x.
From this and


(I + A
âˆ’1âˆ†A)
âˆ’1

 â‰¤
1 âˆ’ kAâˆ’
1
1k k âˆ†Ak
,
we get
k
âˆ†xk â‰¤ k Aâˆ’1k k âˆ†Ak
1 âˆ’ kAâˆ’1k k âˆ†Ak
k
xk ,
which can be written as
k
âˆ†xk
k
xk
â‰¤ cond(A)
k
âˆ†Ak
k
Ak
 1 âˆ’ kAâˆ’
1
1k k âˆ†Ak

,
which is the kind of inequality that we were seeking.
356 CHAPTER 9. VECTOR NORMS AND MATRIX NORMS
Remark: If A and b are perturbed simultaneously, so that we get the â€œperturbedâ€ system
(A + âˆ†A)(x + âˆ†x) = b + âˆ†b,
it can be shown that if k âˆ†Ak < 1/ k Aâˆ’1k
(and b 6 = 0), then
k
âˆ†xk
k
xk
â‰¤
cond(A)
1 âˆ’ kAâˆ’1k k âˆ†Ak

k k
âˆ†
A
A
kk
+
k
k
âˆ†
b
b
kk

;
see Demmel [48], Section 2.2 and Horn and Johnson [95], Section 5.8.
We now list some properties of condition numbers and figure out what cond(A) is in the
case of the spectral norm (the matrix norm induced by k k 2
). First, we need to introduce a
very important factorization of matrices, the singular value decomposition, for short, SVD.
It can be shown (see Section 22.2) that given any n Ã— n matrix A âˆˆ Mn(C), there
exist two unitary matrices U and V , and a real diagonal matrix Î£ = diag(Ïƒ1, . . . , Ïƒn), with
Ïƒ1 â‰¥ Ïƒ2 â‰¥ Â· Â· Â· â‰¥ Ïƒn â‰¥ 0, such that
A = V Î£U
âˆ—
.
Definition 9.11. Given a complex n Ã— n matrix A, a triple (U, V, Î£) such that A = V Î£U
âˆ—
,
where U and V are n Ã— n unitary matrices and Î£ = diag(Ïƒ1, . . . , Ïƒn) is a diagonal matrix of
real numbers Ïƒ1 â‰¥ Ïƒ2 â‰¥ Â· Â· Â· â‰¥ Ïƒn â‰¥ 0, is called a singular decomposition (for short SVD) of
A. If A is a real matrix, then U and V are orthogonal matrices The nonnegative numbers
Ïƒ1, . . . , Ïƒn are called the singular values of A.
The factorization A = V Î£U
âˆ—
implies that
A
âˆ—A = UÎ£
2U
âˆ—
and AAâˆ— = V Î£
2V
âˆ—
,
which shows that Ïƒ1
2
, . . . , Ïƒn
2 are the eigenvalues of both Aâˆ—A and AAâˆ—
, that the columns
of U are corresponding eigenvectors for Aâˆ—A, and that the columns of V are corresponding
eigenvectors for AAâˆ—
.
Since Ïƒ1
2
is the largest eigenvalue of Aâˆ—A (and AAâˆ—
), note that p Ï(Aâˆ—A) = p Ï(AAâˆ—
) =
Ïƒ1.
Corollary 9.15. The spectral norm k Ak 2
of a matrix A is equal to the largest singular value
of A. Equivalently, the spectral norm k Ak 2
of a matrix A is equal to the ` âˆ-norm of its
vector of singular values,
k
Ak 2 = max
1â‰¤iâ‰¤n
Ïƒi = k (Ïƒ1, . . . , Ïƒn)k âˆ .
Since the Frobenius norm of a matrix A is defined by k Ak F =
p tr(Aâˆ—A) and since
tr(A
âˆ—A) = Ïƒ1
2 + Â· Â· Â· + Ïƒn
2
where Ïƒ1
2
, . . . , Ïƒn
2 are the eigenvalues of Aâˆ—A, we see that
k
Ak F = (Ïƒ1
2 + Â· Â· Â· + Ïƒn
2
)
1/2 = k (Ïƒ1, . . . , Ïƒn)k
2
.
9.5. CONDITION NUMBERS OF MATRICES 357
Corollary 9.16. The Frobenius norm of a matrix is given by the ` 2
-norm of its vector of
singular values; k Ak F = k (Ïƒ1, . . . , Ïƒn)k
2
.
In the case of a normal matrix if Î»1, . . . , Î»n are the (complex) eigenvalues of A, then
Ïƒi = |Î»i
|, 1 â‰¤ i â‰¤ n.
Proposition 9.17. For every invertible matrix A âˆˆ Mn(C), the following properties hold:
(1)
cond(A) â‰¥ 1,
cond(A) = cond(A
âˆ’1
)
cond(Î±A) = cond(A) for all Î± âˆˆ C âˆ’ {0}.
(2) If cond2(A) denotes the condition number of A with respect to the spectral norm, then
cond2(A) = Ïƒ1
Ïƒn
,
where Ïƒ1 â‰¥ Â· Â· Â· â‰¥ Ïƒn are the singular values of A.
(3) If the matrix A is normal, then
cond2(A) = |Î»1|
|Î»n|
,
where Î»1, . . . , Î»n are the eigenvalues of A sorted so that |Î»1| â‰¥ Â· Â· Â· â‰¥ |Î»n|.
(4) If A is a unitary or an orthogonal matrix, then
cond2(A) = 1.
(5) The condition number cond2(A) is invariant under unitary transformations, which
means that
cond2(A) = cond2(UA) = cond2(AV ),
for all unitary matrices U and V .
Proof. The properties in (1) are immediate consequences of the properties of subordinate
matrix norms. In particular, AAâˆ’1 = I implies
1 = k Ik â‰¤ kAk
  A
âˆ’1

 = cond(A).
(2) We showed earlier that k Ak
2
2 = Ï(Aâˆ—A), which is the square of the modulus of the largest
eigenvalue of Aâˆ—A. Since we just saw that the eigenvalues of Aâˆ—A are Ïƒ1
2 â‰¥ Â· Â· Â· â‰¥ Ïƒn
2
, where
Ïƒ1, . . . , Ïƒn are the singular values of A, we have
k
Ak 2 = Ïƒ1.
358 CHAPTER 9. VECTOR NORMS AND MATRIX NORMS
Now if A is invertible, then Ïƒ1 â‰¥ Â· Â· Â· â‰¥ Ïƒn > 0, and it is easy to show that the eigenvalues of
(Aâˆ—A)
âˆ’1 are Ïƒn
âˆ’2 â‰¥ Â· Â· Â· â‰¥ Ïƒ1
âˆ’2
, which shows that


A
âˆ’1


2
= Ïƒn
âˆ’1
,
and thus
cond2(A) = Ïƒ1
Ïƒn
.
(3) This follows from the fact that k Ak 2 = Ï(A) for a normal matrix.
(4) If A is a unitary matrix, then Aâˆ—A = AAâˆ— = I, so Ï(Aâˆ—A) = 1, and k Ak 2 = p
Ï(Aâˆ—A) = 1. We also have k Aâˆ’1k
2 = k Aâˆ—k
2 =
p Ï(AAâˆ—
) = 1, and thus cond(A) = 1.
(5) This follows immediately from the unitary invariance of the spectral norm.
Proposition 9.17 (4) shows that unitary and orthogonal transformations are very wellï¿¾conditioned, and Part (5) shows that unitary transformations preserve the condition number.
In order to compute cond2(A), we need to compute the top and bottom singular values
of A, which may be hard. The inequality
k
Ak 2 â‰¤ kAk F â‰¤
âˆš
n k Ak 2
,
may be useful in getting an approximation of cond2(A) = k Ak 2
k Aâˆ’1k
2
, if Aâˆ’1
can be
determined.
Remark: There is an interesting geometric characterization of cond2(A). If Î¸(A) denotes
the least angle between the vectors Au and Av as u and v range over all pairs of orthonormal
vectors, then it can be shown that
cond2(A) = cot(Î¸(A)/2)).
Thus if A is nearly singular, then there will be some orthonormal pair u, v such that Au and
Av are nearly parallel; the angle Î¸(A) will the be small and cot(Î¸(A)/2)) will be large. For
more details, see Horn and Johnson [95] (Section 5.8 and Section 7.4).
It should be noted that in general (if A is not a normal matrix) a matrix could have
a very large condition number even if all its eigenvalues are identical! For example, if we
consider the n Ã— n matrix
A =
ï£«
ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­
1 2 0 0
0 1 2 0
. . .
. . .
0 0
0 0
0 0 1 2 . . . 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 . . . 0 1 2 0
0 0
0 0
. . .
. . .
0 0 1 2
0 0 0 1
ï£¶
ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£¸
,
9.5. CONDITION NUMBERS OF MATRICES 359
it turns out that cond2(A) â‰¥ 2
nâˆ’1
.
A classical example of matrix with a very large condition number is the Hilbert matrix
H(n)
, the n Ã— n matrix with
Hij
(n) =

i + j
1
âˆ’ 1

.
For example, when n = 5,
H
(5) =
ï£«
ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­
1
1
2
1
3
1
4
1
5
1
2
1
3
1
4
1
5
1
6
1
3
1
4
1
5
1
6
1
7
1
4
1
5
1
6
1
7
1
8
1
5
1
6
1
7
1
8
1
9
ï£¶
ï£·ï£·ï£·ï£·ï£·ï£·ï£¸
.
It can be shown that
cond2(H
(5)) â‰ˆ 4.77 Ã— 105
.
Hilbert introduced these matrices in 1894 while studying a problem in approximation
theory. The Hilbert matrix H(n)
is symmetric positive definite. A closed-form formula can
be given for its determinant (it is a special form of the so-called Cauchy determinant); see
Problem 9.15. The inverse of H(n)
can also be computed explicitly; see Problem 9.15. It can
be shown that
cond2(H
(n)
) = O((1 + âˆš
2)4n
/
âˆš
n).
Going back to our matrix
A =
ï£«
ï£¬ï£¬ï£­
10 7 8 7
7 5 6 5
8 6 10 9
7 5 9 10
ï£¶
ï£·ï£·ï£¸ ,
which is a symmetric positive definite matrix, it can be shown that its eigenvalues, which in
this case are also its singular values because A is SPD, are
Î»1 â‰ˆ 30.2887 > Î»2 â‰ˆ 3.858 > Î»3 â‰ˆ 0.8431 > Î»4 â‰ˆ 0.01015,
so that
cond2(A) = Î»1
Î»4
â‰ˆ 2984.
The reader should check that for the perturbation of the right-hand side b used earlier, the
relative errors k âˆ†xk /k xk and k âˆ†xk /k xk satisfy the inequality
k
âˆ†xk
k
xk
â‰¤ cond(A)
k
âˆ†bk
k
bk
and comes close to equality.
360 CHAPTER 9. VECTOR NORMS AND MATRIX NORMS
9.6 An Application of Norms: Solving Inconsistent
Linear Systems
The problem of solving an inconsistent linear system Ax = b often arises in practice. This
is a system where b does not belong to the column space of A, usually with more equations
than variables. Thus, such a system has no solution. Yet we would still like to â€œsolveâ€ such
a system, at least approximately.
Such systems often arise when trying to fit some data. For example, we may have a set
of 3D data points
{p1, . . . , pn},
and we have reason to believe that these points are nearly coplanar. We would like to find
a plane that best fits our data points. Recall that the equation of a plane is
Î±x + Î²y + Î³z + Î´ = 0,
with (Î±, Î², Î³) 6 = (0, 0, 0). Thus, every plane is either not parallel to the x-axis (Î± 6 = 0) or not
parallel to the y-axis (Î² 6 = 0) or not parallel to the z-axis (Î³ 6 = 0).
Say we have reasons to believe that the plane we are looking for is not parallel to the
z-axis. If we are wrong, in the least squares solution, one of the coefficients, Î±, Î², will be
very large. If Î³ 6 = 0, then we may assume that our plane is given by an equation of the form
z = ax + by + d,
and we would like this equation to be satisfied for all the pi
â€™s, which leads to a system of n
equations in 3 unknowns a, b, d, with pi = (xi
, yi
, zi);
ax1 + by1 + d = z1
.
.
.
.
.
.
axn + byn + d = zn.
However, if n is larger than 3, such a system generally has no solution. Since the above
system canâ€™t be solved exactly, we can try to find a solution (a, b, d) that minimizes the
least-squares error
nX
i=1
(axi + byi + d âˆ’ zi)
2
.
This is what Legendre and Gauss figured out in the early 1800â€™s!
In general, given a linear system
Ax = b,
we solve the least squares problem: minimize k Ax âˆ’ bk
2
2
.
9.7. LIMITS OF SEQUENCES AND SERIES 361
Fortunately, every n Ã— m-matrix A can be written as
A = V DU >
where U and V are orthogonal and D is a rectangular diagonal matrix with non-negative
entries (singular value decomposition, or SVD); see Chapter 22.
The SVD can be used to solve an inconsistent system. It is shown in Chapter 23 that
there is a vector x of smallest norm minimizing k Ax âˆ’ bk 2
. It is given by the (Penrose)
pseudo-inverse of A (itself given by the SVD).
It has been observed that solving in the least-squares sense may give too much weight to
â€œoutliers,â€ that is, points clearly outside the best-fit plane. In this case, it is preferable to
minimize (the ` 1
-norm)
nX
i=1
|axi + byi + d âˆ’ zi
|.
This does not appear to be a linear problem, but we can use a trick to convert this
minimization problem into a linear program (which means a problem involving linear conï¿¾straints).
Note that |x| = max{x, âˆ’x}. So by introducing new variables e1, . . . , en, our minimizaï¿¾tion problem is equivalent to the linear program (LP):
minimize e1 + Â· Â· Â· + en
subject to axi + byi + d âˆ’ zi â‰¤ ei
âˆ’(axi + byi + d âˆ’ zi) â‰¤ ei
1 â‰¤ i â‰¤ n.
Observe that the constraints are equivalent to
ei â‰¥ |axi + byi + d âˆ’ zi
|, 1 â‰¤ i â‰¤ n.
For an optimal solution, we must have equality, since otherwise we could decrease some ei
and get an even better solution. Of course, we are no longer dealing with â€œpureâ€ linear
algebra, since our constraints are inequalities.
We prefer not getting into linear programming right now, but the above example provides
a good reason to learn more about linear programming!
9.7 Limits of Sequences and Series
If x âˆˆ R or x âˆˆ C and if |x| < 1, it is well known that the sums P n
k=0 x
k = 1+x+x
2+Â· Â· Â·+x
n
converge to the limit 1/(1 âˆ’ x) when n goes to infinity, and we write
âˆX
k=0
x
k =
1 âˆ’
1
x
.
362 CHAPTER 9. VECTOR NORMS AND MATRIX NORMS
For example,
âˆX
k=0
2
1
k
= 2.
Similarly, the sums
Sn =
nX
k=0
x
k
k!
converge to e
x when n goes to infinity, for every x (in R or C). What if we replace x by a
real or complex n Ã— n matrix A?
The partial sums P n
k=0 Ak and P n
k=0
Ak
k!
still make sense, but we have to define what is
the limit of a sequence of matrices. This can be done in any normed vector space.
Definition 9.12. Let (E, kk ) be a normed vector space. A sequence (un)nâˆˆN in E is any
function u: N â†’ E. For any v âˆˆ E, the sequence (un) converges to v (and v is the limit of
the sequence (un)) if for every  > 0, there is some integer N > 0 such that
k
un âˆ’ vk <  for all n â‰¥ N.
Often we assume that a sequence is indexed by Nâˆ’ {0}, that is, its first term is u1 rather
than u0.
If the sequence (un) converges to v, then since by the triangle inequality
k
um âˆ’ unk â‰¤ kum âˆ’ vk + k v âˆ’ unk ,
we see that for every  > 0, we can find N > 0 such that k um âˆ’ vk < /2 and k un âˆ’ vk < /2
for all m, n â‰¥ N, and so
k
um âˆ’ unk <  for all m, n â‰¥ N.
The above property is necessary for a convergent sequence, but not necessarily sufficient.
For example, if E = Q, there are sequences of rationals satisfying the above condition, but
whose limit is not a rational number. For example, the sequence P n
k=1
1
k!
converges to e, and
the sequence P n
k=0(âˆ’1)k
2k
1
+1 converges to Ï€/4, but e and Ï€/4 are not rational (in fact, they
are transcendental). However, R is constructed from Q to guarantee that sequences with the
above property converge, and so is C.
Definition 9.13. Given a normed vector space (E, k k ), a sequence (un) is a Cauchy sequence
if for every  > 0, there is some N > 0 such that
k
um âˆ’ unk <  for all m, n â‰¥ N.
If every Cauchy sequence converges, then we say that E is complete. A complete normed
vector spaces is also called a Banach space.
9.7. LIMITS OF SEQUENCES AND SERIES 363
A fundamental property of R is that it is complete. It follows immediately that C is also
complete. If E is a finite-dimensional real or complex vector space, since any two norms are
equivalent, we can pick the ` âˆ norm, and then by picking a basis in E, a sequence (un) of
vectors in E converges iff the n sequences of coordinates (u
i
n
) (1 â‰¤ i â‰¤ n) converge, so any
finite-dimensional real or complex vector space is a Banach space.
Let us now consider the convergence of series.
Definition 9.14. Given a normed vector space (E, k k ), a series is an infinite sum P âˆ
k=0 uk
of elements uk âˆˆ E. We denote by Sn the partial sum of the first n + 1 elements,
Sn =
nX
k=0
uk.
Definition 9.15. We say that the series P âˆ
k=0 uk converges to the limit v âˆˆ E if the sequence
(Sn) converges to v, i.e., given any  > 0, there exists a positive integer N such that for all
n â‰¥ N,
k
Sn âˆ’ vk < .
In this case, we say that the series is convergent. We say that the series P âˆ
k=0 uk converges
absolutely if the series of norms P âˆ
k=0 k ukk is convergent.
If the series P âˆ
k=0 uk converges to v, since for all m, n with m > n we have
mX
k=0
uk âˆ’ Sn =
mX
k=0
uk âˆ’
nX
k=0
uk =
mX
k=n+1
uk,
if we let m go to infinity (with n fixed), we see that the series P âˆ
k=n+1 uk converges and that
v âˆ’ Sn =
âˆX
k=n+1
uk.
There are series that are convergent but not absolutely convergent; for example, the series
âˆX
k=1
(âˆ’1)kâˆ’1
k
converges to ln 2, but P âˆ
k=1 k
1 does not converge (this sum is infinite).
If E is complete, the converse is an enormously useful result.
Proposition 9.18. Assume (E, k k ) is a complete normed vector space. If a series P âˆ
k=0 uk
is absolutely convergent, then it is convergent.
364 CHAPTER 9. VECTOR NORMS AND MATRIX NORMS
Proof. If P âˆ
k=0 uk is absolutely convergent, then we prove that the sequence (Sm) is a Cauchy
sequence; that is, for every  > 0, there is some p > 0 such that for all n â‰¥ m â‰¥ p,
k
Sn âˆ’ Smk â‰¤ .
Observe that
k
Sn âˆ’ Smk = k um+1 + Â· Â· Â· + unk â‰¤ kum+1k + Â· Â· Â· + k unk ,
and since the sequence P âˆ
k=0 k ukk converges, it satisfies Cauchyâ€™s criterion. Thus, the seï¿¾quence (Sm) also satisfies Cauchyâ€™s criterion, and since E is a complete vector space, the
sequence (Sm) converges.
Remark: It can be shown that if (E, k k ) is a normed vector space such that every absolutely
convergent series is also convergent, then E must be complete (see Schwartz [150]).
An important corollary of absolute convergence is that if the terms in series P âˆ
k=0 uk
are rearranged, then the resulting series is still absolutely convergent and has the same
sum. More precisely, let Ïƒ be any permutation (bijection) of the natural numbers. The
series P âˆ
k=0 uÏƒ(k)
is called a rearrangement of the original series. The following result can be
shown (see Schwartz [150]).
Proposition 9.19. Assume (E, k k ) is a normed vector space. If a series P âˆ
k=0 uk is converï¿¾gent as well as absolutely convergent, then for every permutation Ïƒ of N, the series P âˆ
k=0 uÏƒ(k)
is convergent and absolutely convergent, and its sum is equal to the sum of the original series:
âˆX
k=0
uÏƒ(k) =
âˆX
k=0
uk.
In particular, if (E, k k ) is a complete normed vector space, then Proposition 9.19 holds.
We now apply Proposition 9.18 to the matrix exponential.
9.8 The Matrix Exponential
Proposition 9.20. For any n Ã— n real or complex matrix A, the series
âˆX
k=0
Ak
k!
converges absolutely for any operator norm on Mn(C) (or Mn(R)).
9.8. THE MATRIX EXPONENTIAL 365
Proof. Pick any norm on C
n
(or R
n
) and let kk be the corresponding operator norm on
Mn(C). Since Mn(C) has dimension n
2
, it is complete. By Proposition 9.18, it suffices to
show that the series of nonnegative reals P n
k=0

 
Ak
k!



converges. Since k k is an operator
norm, this a matrix norm, so we have
nX
k=0




Ak
k!




â‰¤
nX
k=0
k
A
k!
k
k
â‰¤ e
k
Ak
.
Thus, the nondecreasing sequence of positive real numbers P n
k=0

 
Ak
k!



is bounded by e
k
Ak ,
and by a fundamental property of R, it has a least upper bound which is its limit.
Definition 9.16. Let E be a finite-dimensional real or complex normed vector space. For
any n Ã— n matrix A, the limit of the series
âˆX
k=0
Ak
k!
is the exponential of A and is denoted e
A.
A basic property of the exponential x 7â†’ e
x with x âˆˆ C is
e
x+y = e
x
e
y
, for all x, y âˆˆ C.
As a consequence, e
x
is always invertible and (e
x
)
âˆ’1 = e
âˆ’x
. For matrices, because matrix
multiplication is not commutative, in general,
e
A+B = e
A
e
B
fails! This result is salvaged as follows.
Proposition 9.21. For any two n Ã— n complex matrices A and B, if A and B commute,
that is, AB = BA, then
e
A+B = e
A
e
B
.
A proof of Proposition 9.21 can be found in Gallier [72].
Since A and âˆ’A commute, as a corollary of Proposition 9.21, we see that e
A is always
invertible and that
(e
A
)
âˆ’1 = e
âˆ’A
.
It is also easy to see that
(e
A
)
> = e
A>
.
366 CHAPTER 9. VECTOR NORMS AND MATRIX NORMS
In general, there is no closed-form formula for the exponential e
A of a matrix A, but for
skew symmetric matrices of dimension 2 and 3, there are explicit formulae. Everyone should
enjoy computing the exponential e
A where
A =

0
Î¸
âˆ’
0
Î¸

.
If we write
J =

0
1 0
âˆ’1

,
then
A = Î¸J
The key property is that
J
2 = âˆ’I.
Proposition 9.22. If A = Î¸J, then
e
A = cos Î¸I + sin Î¸J =

cos
sin Î¸
Î¸ âˆ’
cos
sin
Î¸
Î¸

.
Proof. We have
A
4n = Î¸
4n
I2,
A
4n+1 = Î¸
4n+1J,
A
4n+2 = âˆ’Î¸
4n+2I2,
A
4n+3 = âˆ’Î¸
4n+3J,
and so
e
A = I2 +
Î¸
1!J âˆ’
Î¸
2
2! I2 âˆ’
Î¸
3
3! J +
Î¸
4
4! I2 +
Î¸
5
5! J âˆ’
Î¸
6
6! I2 âˆ’
Î¸
7
7! J + Â· Â· Â· .
Rearranging the order of the terms, we have
e
A =
 1 âˆ’
Î¸
2
2! +
Î¸
4
4! âˆ’
Î¸
6
6! + Â· Â· Â·  I2 +

1!
Î¸
âˆ’
Î¸
3
3! +
Î¸
5
5! âˆ’
Î¸
7
7! + Â· Â· Â·  J.
We recognize the power series for cos Î¸ and sin Î¸, and thus
e
A = cos Î¸I2 + sin Î¸J,
that is
e
A =

cos
sin Î¸
Î¸ âˆ’
cos
sin
Î¸
Î¸

,
as claimed.
9.9. SUMMARY 367
Thus, we see that the exponential of a 2 Ã— 2 skew-symmetric matrix is a rotation matrix.
This property generalizes to any dimension. An explicit formula when n = 3 (the Rodriguesâ€™
formula) is given in Section 12.7.
Proposition 9.23. If B is an n Ã— n (real) skew symmetric matrix, that is, B> = âˆ’B, then
Q = e
B is an orthogonal matrix, that is
Q
> Q = QQ> = I.
Proof. Since B> = âˆ’B, we have
