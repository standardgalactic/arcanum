2 = k uk
2 + k vk
2 − 2h u, vi ,
u + v
v- u
12.1. INNER PRODUCTS, EUCLIDEAN SPACES 447
and by adding and subtracting these identities, we get the parallelogram law and the equation
h
u, vi =
1
4
(k u + vk
2 − ku − vk
2
),
which allows us to recover h−, −i from the norm.
Conversely, if k k is a norm satisfying the parallelogram law, and if it comes from an
inner product, then this inner product must be given by
h
u, vi =
1
4
(k u + vk
2 − ku − vk
2
).
We need to prove that the above form is indeed symmetric and bilinear.
Symmetry holds because k u − vk = k−(u − v)k = k v − uk . Let us prove additivity in
the variable u. By the parallelogram law, we have
2(k x + zk
2 + k yk
2
) = k x + y + zk
2 + k x − y + zk
2
which yields
k
x + y + zk
2 = 2(k x + zk
2 + k yk
2
) − kx − y + zk
2
k
x + y + zk
2 = 2(k y + zk
2 + k xk
2
) − ky − x + zk
2
,
where the second formula is obtained by swapping x and y. Then by adding up these
equations, we get
k
x + y + zk
2 = k xk
2 + k yk
2 + k x + zk
2 + k y + zk
2 −
1
2
k
x − y + zk
2 −
1
2
k
y − x + zk
2
.
Replacing z by −z in the above equation, we get
k
x + y − zk
2 = k xk
2 + k yk
2 + k x − zk
2 + k y − zk
2 −
1
2
k
x − y − zk
2 −
1
2
k
y − x − zk
2
,
Since k x − y + zk = k−(x − y + z)k = k y − x − zk and k y − x + zk = k−(y − x + z)k =
k
x − y − zk , by subtracting the last two equations, we get
h
x + y, zi =
1
4
(k x + y + zk
2 − kx + y − zk
2
)
=
1
4
(k x + zk
2 − kx − zk
2
) + 1
4
(k y + zk
2 − ky − zk
2
)
= h x, zi + h y, zi ,
as desired.
Proving that
h
λx, yi = λh x, yi for all λ ∈ R
448 CHAPTER 12. EUCLIDEAN SPACES
is a little tricky. The strategy is to prove the identity for λ ∈ Z, then to promote it to Q,
and then to R by continuity.
Since
h−u, vi =
1
4
(k−u + vk
2 − k−u − vk
2
)
=
1
4
(k u − vk
2 − ku + vk
2
)
= −hu, vi ,
the property holds for λ = −1. By linearity and by induction, for any n ∈ N with n ≥ 1,
writing n = n − 1 + 1, we get
h
λx, yi = λh x, yi for all λ ∈ N,
and since the above also holds for λ = −1, it holds for all λ ∈ Z. For λ = p/q with p, q ∈ Z
and q 6 = 0, we have
qh (p/q)u, vi = h pu, vi = ph u, vi ,
which shows that
h
(p/q)u, vi = (p/q)h u, vi ,
and thus
h
λx, yi = λh x, yi for all λ ∈ Q.
To finish the proof, we use the fact that a norm is a continuous map x 7→ kxk . Then, the
continuous function t 7→ 1
t
h
tu, vi defined on R − {0} agrees with h u, vi on Q − {0}, so it is
equal to h u, vi on R − {0}. The case λ = 0 is trivial, so we are done.
We now define orthogonality.
12.2 Orthogonality and Duality in Euclidean Spaces
An inner product on a vector space gives the ability to define the notion of orthogonality.
Families of nonnull pairwise orthogonal vectors must be linearly independent. They are
called orthogonal families. In a vector space of finite dimension it is always possible to find
orthogonal bases. This is very useful theoretically and practically. Indeed, in an orthogonal
basis, finding the coordinates of a vector is very cheap: It takes an inner product. Fourier
series make crucial use of this fact. When E has finite dimension, we prove that the inner
product on E induces a natural isomorphism between E and its dual space E
∗
. This allows
us to define the adjoint of a linear map in an intrinsic fashion (i.e., independently of bases).
It is also possible to orthonormalize any basis (certainly when the dimension is finite). We
give two proofs, one using duality, the other more constructive using the Gram–Schmidt
orthonormalization procedure.
12.2. ORTHOGONALITY AND DUALITY IN EUCLIDEAN SPACES 449
Definition 12.2. Given a Euclidean space E, any two vectors u, v ∈ E are orthogonal, or
perpendicular , if u · v = 0. Given a family (ui)i∈I of vectors in E, we say that (ui)i∈I is
orthogonal if ui
· uj = 0 for all i, j ∈ I, where i 6 = j. We say that the family (ui)i∈I is
orthonormal if ui
· uj = 0 for all i, j ∈ I, where i 6 = j, and k uik = ui
· ui = 1, for all i ∈ I.
For any subset F of E, the set
F
⊥ = {v ∈ E | u · v = 0, for all u ∈ F},
of all vectors orthogonal to all vectors in F, is called the orthogonal complement of F.
Since inner products are positive definite, observe that for any vector u ∈ E, we have
u · v = 0 for all v ∈ E iff u = 0.
It is immediately verified that the orthogonal complement F
⊥ of F is a subspace of E.
Example 12.5. Going back to Example 12.3 and to the inner product
h
f, gi =
Z
π
−π
f(t)g(t)dt
on the vector space C[−π, π], it is easily checked that
h
sin px,sin qxi =

0 if
π if
p
p
6
=
=
q
q
,
,
p, q
p, q
≥
≥
1,
1,
h
cos px, cos qxi =

π
0 if
if p
p
=
6
=
q
q
,
,
p, q
p, q
≥
≥
1,
0,
and
h
sin px, cos qxi = 0,
for all p ≥ 1 and q ≥ 0, and of course, h 1, 1i =
R
π
−π
dx = 2π.
As a consequence, the family (sin px)p≥1∪(cos qx)q≥0 is orthogonal. It is not orthonormal,
but becomes so if we divide every trigonometric function by √
π, and 1 by √
2π.
Proposition 12.4. Given a Euclidean space E, for any family (ui)i∈I of nonnull vectors in
E, if (ui)i∈I is orthogonal, then it is linearly independent.
Proof. Assume there is a linear dependence
X
j∈J
λjuj = 0
450 CHAPTER 12. EUCLIDEAN SPACES
for some λj ∈ R and some finite subset J of I. By taking the inner product with ui
for
any i ∈ J, and using the the bilinearity of the inner product and the fact that ui
· uj = 0
whenever i 6 = j, we get
0 = ui
· 0 = ui
·
 
X
j∈J
λjuj
!
=
X
j∈J
λj (ui
· uj ) = λi(ui
· ui),
so
λi(ui
· ui) = 0, for all i ∈ J,
and since ui 6 = 0 and an inner product is positive definite, ui
· ui 6 = 0, so we obtain
λi = 0, for all i ∈ J,
which shows that the family (ui)i∈I is linearly independent.
We leave the following simple result as an exercise.
Proposition 12.5. Given a Euclidean space E, any two vectors u, v ∈ E are orthogonal iff
k
u + vk
2 = k uk
2 + k vk
2
.
See Figure 12.2 for a geometrical interpretation.
v
Figure 12.2: The sum of the lengths of the two sides of a right triangle is equal to the length
of the hypotenuse; i.e. the Pythagorean theorem.
One of the most useful features of orthonormal bases is that they afford a very simple
method for computing the coordinates of a vector over any basis vector. Indeed, assume
that (e1, . . . , em) is an orthonormal basis. For any vector
x = x1e1 + · · · + xmem,
u
+
v
u
12.2. ORTHOGONALITY AND DUALITY IN EUCLIDEAN SPACES 451
if we compute the inner product x · ei
, we get
x · ei = x1e1 · ei + · · · + xiei
· ei + · · · + xmem · ei = xi
,
since
ei
· ej =

0 if
1 if
i
i
6
=
=
j
j,
is the property characterizing an orthonormal family. Thus,
xi = x · ei
,
which means that xiei = (x · ei)ei
is the orthogonal projection of x onto the subspace
generated by the basis vector ei
. See Figure 12.3. If the basis is orthogonal but not necessarily
e i x ei i
Θ
Figure 12.3: The orthogonal projection of the red vector x onto the black basis vector ei
is
the maroon vector xiei
. Observe that x · ei = k xk cos θ.
orthonormal, then
xi =
x · ei
ei
· ei
=
x · ei
k
eik
2
.
All this is true even for an infinite orthonormal (or orthogonal) basis (ei)i∈I .

However, remember that every vector x is expressed as a linear combination
x =
X
i∈I
xiei
where the family of scalars (xi)i∈I has finite support, which means that xi = 0 for all
i ∈ I − J, where J is a finite set. Thus, even though the family (sin px)p≥1 ∪ (cos qx)q≥0 is
orthogonal (it is not orthonormal, but becomes so if we divide every trigonometric function by
x
452 CHAPTER 12. EUCLIDEAN SPACES
√
π, and 1 by √
2π; we won’t because it looks messy!), the fact that a function f ∈ C0
[−π, π]
can be written as a Fourier series as
f(x) = a0 +
∞X
k=1
(ak cos kx + bk sin kx)
does not mean that (sin px)p≥1 ∪ (cos qx)q≥0 is a basis of this vector space of functions,
because in general, the families (ak) and (bk) do not have finite support! In order for this
infinite linear combination to make sense, it is necessary to prove that the partial sums
a0 +
nX
k=1
(ak cos kx + bk sin kx)
of the series converge to a limit when n goes to infinity. This requires a topology on the
space.
A very important property of Euclidean spaces of finite dimension is that the inner
product induces a canonical bijection (i.e., independent of the choice of bases) between the
vector space E and its dual E
∗
. The reason is that an inner product ·: E × E → R defines
a nondegenerate pairing, as defined in Definition 11.4. Indeed, if u · v = 0 for all v ∈ E then
u = 0, and similarly if u · v = 0 for all u ∈ E then v = 0 (since an inner product is positive
definite and symmetric). By Proposition 11.6, there is a canonical isomorphism between E
and E
∗
. We feel that the reader will appreciate if we exhibit this mapping explicitly and
reprove that it is an isomorphism.
The mapping from E to E
∗
is defined as follows.
Definition 12.3. For any vector u ∈ E, let ϕu : E → R be the map defined such that
ϕu(v) = u · v, for all v ∈ E.
Since the inner product is bilinear, the map ϕu is a linear form in E
∗
. Thus, we have a map
[
: E → E
∗
, defined such that
[
(u) = ϕu.
Theorem 12.6. Given a Euclidean space E, the map [ : E → E
∗ defined such that
[
(u) = ϕu
is linear and injective. When E is also of finite dimension, the map [ : E → E
∗
is a canonical
isomorphism.
12.2. ORTHOGONALITY AND DUALITY IN EUCLIDEAN SPACES 453
Proof. That [ : E → E
∗
is a linear map follows immediately from the fact that the inner
product is bilinear. If ϕu = ϕv, then ϕu(w) = ϕv(w) for all w ∈ E, which by definition of ϕu
means that u · w = v · w for all w ∈ E, which by bilinearity is equivalent to
(v − u) · w = 0
for all w ∈ E, which implies that u = v, since the inner product is positive definite. Thus,
[
: E → E
∗
is injective. Finally, when E is of finite dimension n, we know that E
∗
is also of
dimension n, and then [ : E → E
∗
is bijective.
The inverse of the isomorphism [ : E → E
∗
is denoted by ] : E
∗ → E.
As a consequence of Theorem 12.6 we have the following corollary.
Corollary 12.7. If E is a Euclidean space of finite dimension, every linear form f ∈ E
∗
corresponds to a unique u ∈ E such that
f(v) = u · v, for every v ∈ E.
In particular, if f is not the zero form, the kernel of f, which is a hyperplane H, is precisely
the set of vectors that are orthogonal to u.
Remarks:
(1) The “musical map” [ : E → E
∗
is not surjective when E has infinite dimension. The
result can be salvaged by restricting our attention to continuous linear maps, and by
assuming that the vector space E is a Hilbert space (i.e., E is a complete normed vector
space w.r.t. the Euclidean norm). This is the famous “little” Riesz theorem (or Riesz
representation theorem).
(2) Theorem 12.6 still holds if the inner product on E is replaced by a nondegenerate
symmetric bilinear form ϕ. We say that a symmetric bilinear form ϕ: E × E → R is
nondegenerate if for every u ∈ E,
if ϕ(u, v) = 0 for all v ∈ E, then u = 0.
For example, the symmetric bilinear form on R
4
(the Lorentz form) defined such that
ϕ((x1, x2, x3, x4), (y1, y2, y3, y4)) = x1y1 + x2y2 + x3y3 − x4y4
is nondegenerate. However, there are nonnull vectors u ∈ R
4
such that ϕ(u, u) = 0,
which is impossible in a Euclidean space. Such vectors are called isotropic.
454 CHAPTER 12. EUCLIDEAN SPACES
Example 12.6. Consider R
n with its usual Euclidean inner product. Given any differen￾tiable function f : U → R, where U is some open subset of R
n
, by definition, for any x ∈ U,
the total derivative dfx of f at x is the linear form defined so that for all u = (u1, . . . , un) ∈ R
n
,
dfx(u) =  ∂x
∂f
1
(x) · · ·
∂f
∂xn
(x)



u1
u
.
.
.
n

 =
nX
i=1
∂f
∂xi
(x) ui
.
The unique vector v ∈ R
n
such that
v · u = dfx(u) for all u ∈ R
n
is the transpose of the Jacobian matrix of f at x, the 1 × n matrix

∂x
∂f
1
(x) · · ·
∂f
∂xn
(x)
 .
This is the gradient grad(f)x of f at x, given by
grad(f)x =


∂x
∂f
1
(x)
.
.
.
∂f
∂xn
(x)


.
Example 12.7. Given any two vectors u, v ∈ R
3
, let c(u, v) be the linear form given by
c(u, v)(w) = det(u, v, w) for all w ∈ R
3
.
Since
det(u, v, w) =


 


u1 v1 w1
u2 v2 w2
u3 v3 w3






= w1




u2 v2
u3 v3




− w2




u1 v1
u3 v3




+ w3




u1 v1
u2 v2




= w1(u2v3 − u3v2) + w2(u3v1 − u1v3) + w3(u1v2 − u2v1),
we see that the unique vector z ∈ R
3
such that
z · w = c(u, v)(w) = det(u, v, w) for all w ∈ R
3
is the vector
z =


u2v3 − u3v2
u3v1 − u1v3
u1v2 − u2v1

 .
This is just the cross-product u × v of u and v. Since det(u, v, u) = det(u, v, v) = 0, we see
that u×v is orthogonal to both u and v. The above allows us to generalize the cross-product
to R
n
. Given any n − 1 vectors u1, . . . , un−1 ∈ R
n
, the cross-product u1 × · · · × un−1 is the
unique vector in R
n
such that
(u1 × · · · × un−1) · w = det(u1, . . . , un−1, w) for all w ∈ R
n
.
12.3. ADJOINT OF A LINEAR MAP 455
Example 12.8. Consider the vector space Mn(R) of real n × n matrices with the inner
product
h
A, Bi = tr(A
> B).
Let s: Mn(R) → R be the function given by
s(A) =
nX
i,j=1
aij ,
where A = (aij ). It is immediately verified that s is a linear form. It is easy to check that
the unique matrix Z such that
h
Z, Ai = s(A) for all A ∈ Mn(R)
is the matrix Z = ones(n, n) whose entries are all equal to 1.
12.3 Adjoint of a Linear Map
The existence of the isomorphism [ : E → E
∗
is crucial to the existence of adjoint maps.
The importance of adjoint maps stems from the fact that the linear maps arising in physical
problems are often self-adjoint, which means that f = f
∗
. Moreover, self-adjoint maps can
be diagonalized over orthonormal bases of eigenvectors. This is the key to the solution of
many problems in mechanics and engineering in general (see Strang [169]).
Let E be a Euclidean space of finite dimension n, and let f : E → E be a linear map.
For every u ∈ E, the map
v 7→ u · f(v)
is clearly a linear form in E
∗
, and by Theorem 12.6, there is a unique vector in E denoted
by f
∗
(u) such that
f
∗
(u) · v = u · f(v),
for every v ∈ E. The following simple proposition shows that the map f
∗
is linear.
Proposition 12.8. Given a Euclidean space E of finite dimension, for every linear map
f : E → E, there is a unique linear map f
∗
: E → E such that
f
∗
(u) · v = u · f(v), for all u, v ∈ E.
Proof. Given u1, u2 ∈ E, since the inner product is bilinear, we have
(u1 + u2) · f(v) = u1 · f(v) + u2 · f(v),
for all v ∈ E, and
(f
∗
(u1) + f
∗
(u2)) · v = f
∗
(u1) · v + f
∗
(u2) · v,
456 CHAPTER 12. EUCLIDEAN SPACES
for all v ∈ E, and since by assumption,
f
∗
(u1) · v = u1 · f(v) and f
∗
(u2) · v = u2 · f(v),
for all v ∈ E. Thus we get
(f
∗
(u1) + f
∗
(u2)) · v = (u1 + u2) · f(v) = f
∗
(u1 + u2) · v,
for all v ∈ E. Since our inner product is positive definite, this implies that
f
∗
(u1 + u2) = f
∗
(u1) + f
∗
(u2).
Similarly,
(λu) · f(v) = λ(u · f(v)),
for all v ∈ E, and
(λf ∗
(u)) · v = λ(f
∗
(u) · v),
for all v ∈ E, and since by assumption,
f
∗
(u) · v = u · f(v),
for all v ∈ E, we get
(λf ∗
(u)) · v = λ(u · f(v)) = (λu) · f(v) = f
∗
(λu) · v
for all v ∈ E. Since [ is bijective, this implies that
f
∗
(λu) = λf ∗
(u).
Thus, f
∗
is indeed a linear map, and it is unique since [ is a bijection.
Definition 12.4. Given a Euclidean space E of finite dimension, for every linear map
f : E → E, the unique linear map f
∗
: E → E such that
f
∗
(u) · v = u · f(v), for all u, v ∈ E
given by Proposition 12.8 is called the adjoint of f (w.r.t. to the inner product). Linear
maps f : E → E such that f = f
∗ are called self-adjoint maps.
Self-adjoint linear maps play a very important role because they have real eigenvalues,
and because orthonormal bases arise from their eigenvectors. Furthermore, many physical
problems lead to self-adjoint linear maps (in the form of symmetric matrices).
Remark: Proposition 12.8 still holds if the inner product on E is replaced by a nondegen￾erate symmetric bilinear form ϕ.
12.3. ADJOINT OF A LINEAR MAP 457
Linear maps such that f
−1 = f
∗
, or equivalently
f
∗
◦ f = f ◦ f
∗ = id,
also play an important role. They are linear isometries, or isometries. Rotations are special
kinds of isometries. Another important class of linear maps are the linear maps satisfying
the property
f
∗
◦ f = f ◦ f
∗
,
called normal linear maps. We will see later on that normal maps can always be diagonalized
over orthonormal bases of eigenvectors, but this will require using a Hermitian inner product
(over C).
Given two Euclidean spaces E and F, where the inner product on E is denoted by h−, −i1
and the inner product on F is denoted by h−, −i2, given any linear map f : E → F, it is
immediately verified that the proof of Proposition 12.8 can be adapted to show that there
is a unique linear map f
∗
: F → E such that
h
f(u), vi 2 = h u, f ∗
(v)i 1
for all u ∈ E and all v ∈ F. The linear map f
∗
is also called the adjoint of f.
The following properties immediately follow from the definition of the adjoint map:
(1) For any linear map f : E → F, we have
f
∗∗ = f.
(2) For any two linear maps f, g : E → F and any scalar λ ∈ R:
(f + g)
∗ = f
∗ + g
∗
(λf)
∗ = λf ∗
.
(3) If E, F, G are Euclidean spaces with respective inner products h−, −i1,h−, −i2, and
h−, −i3, and if f : E → F and g : F → G are two linear maps, then
(g ◦ f)
∗ = f
∗
◦ g
∗
.
Remark: Given any basis for E and any basis for F, it is possible to characterize the matrix
of the adjoint f
∗ of f in terms of the matrix of f and the Gram matrices defining the inner
products; see Problem 12.5. We will do so with respect to orthonormal bases in Proposition
12.14(2). Also, since inner products are symmetric, the adjoint f
∗ of f is also characterized
by
f(u) · v = u · f
∗
(v),
for all u, v ∈ E.
458 CHAPTER 12. EUCLIDEAN SPACES
12.4 Existence and Construction of Orthonormal
Bases
We can also use Theorem 12.6 to show that any Euclidean space of finite dimension has an
orthonormal basis.
Proposition 12.9. Given any nontrivial Euclidean space E of finite dimension n ≥ 1, there
is an orthonormal basis (u1, . . . , un) for E.
Proof. We proceed by induction on n. When n = 1, take any nonnull vector v ∈ E, which
exists since we assumed E nontrivial, and let
u =
v
k
vk
.
If n ≥ 2, again take any nonnull vector v ∈ E, and let
u1 =
v
k
vk
.
Consider the linear form ϕu1 associated with u1. Since u1 6 = 0, by Theorem 12.6, the linear
form ϕu1
is nonnull, and its kernel is a hyperplane H. Since ϕu1
(w) = 0 iff u1 · w = 0,
the hyperplane H is the orthogonal complement of {u1}. Furthermore, since u1 6 = 0 and
the inner product is positive definite, u1 · u1 6 = 0, and thus, u1 ∈/ H, which implies that
E = H ⊕ Ru1. However, since E is of finite dimension n, the hyperplane H has dimension
n−1, and by the induction hypothesis, we can find an orthonormal basis (u2, . . . , un) for H.
Now because H and the one dimensional space Ru1 are orthogonal and E = H ⊕ Ru1, it is
clear that (u1, . . . , un) is an orthonormal basis for E.
As a consequence of Proposition 12.9, given any Euclidean space of finite dimension n,
if (e1, . . . , en) is an orthonormal basis for E, then for any two vectors u = u1e1 + · · · + unen
and v = v1e1 + · · · + vnen, the inner product u · v is expressed as
u · v = (u1e1 + · · · + unen) · (v1e1 + · · · + vnen) =
nX
i=1
uivi
,
and the norm k uk as
k
uk = k u1e1 + · · · + unenk =

nX
i=1
u
2
i

1/2
.
The fact that a Euclidean space always has an orthonormal basis implies that any Gram
matrix G can be written as
G = Q
> Q,
12.4. EXISTENCE AND CONSTRUCTION OF ORTHONORMAL BASES 459
for some invertible matrix Q. Indeed, we know that in a change of basis matrix, a Gram
matrix G becomes G0 = P
> GP. If the basis corresponding to G0 is orthonormal, then G0 = I,
so G = (P
−1
)
> P
−1
.
There is a more constructive way of proving Proposition 12.9, using a procedure known as
the Gram–Schmidt orthonormalization procedure. Among other things, the Gram–Schmidt
orthonormalization procedure yields the QR-decomposition for matrices, an important tool
in numerical methods.
Proposition 12.10. Given any nontrivial Euclidean space E of finite dimension n ≥ 1,
from any basis (e1, . . . , en) for E we can construct an orthonormal basis (u1, . . . , un) for E,
with the property that for every k, 1 ≤ k ≤ n, the families (e1, . . . , ek) and (u1, . . . , uk)
generate the same subspace.
Proof. We proceed by induction on n. For n = 1, let
u1 =
e1
k
e1k
.
For n ≥ 2, we also let
u1 =
e1
k
e1k
,
and assuming that (u1, . . . , uk) is an orthonormal system that generates the same subspace
as (e1, . . . , ek), for every k with 1 ≤ k < n, we note that the vector
u
0k+1 = ek+1 −
k
X
i=1
(ek+1 · ui) ui
is nonnull, since otherwise, because (u1, . . . , uk) and (e1, . . . , ek) generate the same subspace,
(e1, . . . , ek+1) would be linearly dependent, which is absurd, since (e1, . . ., en) is a basis.
Thus, the norm of the vector u
0k+1 being nonzero, we use the following construction of the
vectors uk and u
0k
:
u
01 = e1, u1 =
u
01
k
u
01k
,
and for the inductive step
u
0k+1 = ek+1 −
k
X
i=1
(ek+1 · ui) ui
, uk+1 =
u
0k+1
k
u
0k+1k
,
where 1 ≤ k ≤ n − 1. It is clear that k uk+1k = 1, and since (u1, . . . , uk) is an orthonormal
system, we have
u
0k+1 · ui = ek+1 · ui − (ek+1 · ui)ui
· ui = ek+1 · ui − ek+1 · ui = 0,
for all i with 1 ≤ i ≤ k. This shows that the family (u1, . . . , uk+1) is orthonormal, and since
(u1, . . . , uk) and (e1, . . . , ek) generates the same subspace, it is clear from the definition of
uk+1 that (u1, . . . , uk+1) and (e1, . . . , ek+1) generate the same subspace. This completes the
induction step and the proof of the proposition.
460 CHAPTER 12. EUCLIDEAN SPACES
Note that u
0k+1 is obtained by subtracting from ek+1 the projection of ek+1 itself onto the
orthonormal vectors u1, . . . , uk that have already been computed. Then u
0k+1 is normalized.
Example 12.9. For a specific example of this procedure, let E = R
3 with the standard
Euclidean norm. Take the basis
e1 =


1
1
1

 e2 =


1
0
1

 e3 =


1
1
0

 .
Then
u1 = 1/
√
3


1
1
1

 ,
and
u
02 = e2 − (e2 · u1)u1 =


1
0
1

 − 2/3


1
1
1

 = 1/3

−
1
1
2

 .
This implies that
u2 = 1/
√
6

−
1
1
2

 ,
and that
u
03 = e3 − (e3 · u1)u1 − (e3 · u2)u2 =


1
1
0
