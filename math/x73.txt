, 1 ≤ i ≤ nm+1,
30.7. POLYNOMIAL INTERPOLATION (LAGRANGE, NEWTON, HERMITE) 1085
and let
P(X) =
j=m,i
X=nj+1
j=0,i=0
λ
i
jPj
i
(X).
We can think of P(X) as a generalized Newton interpolant. We can compute the deriva￾tives DkPj
i
, for 1 ≤ k ≤ nj+1, and if we look for the Hermite basis polynomials Hj
i
(X) such
that DiHj
i
(αj ) = 1 and DkHj
i
(αl) = 0, for k 6 = i or l 6 = j, 1 ≤ j, l ≤ m + 1, 0 ≤ i, k ≤ nj
,
we find that we have to solve triangular systems of linear equations. Thus, as in the simple
case n1 = . . . = nm+1 = 0, we can solve successively for the λ
i
j
. Obviously, the computations
are quite formidable and we leave such considerations for further study.
1086 CHAPTER 30. POLYNOMIALS, IDEALS AND PID’S
Chapter 31
Annihilating Polynomials and the
Primary Decomposition
In this chapter all vector spaces are defined over an arbitrary field K.
In Section 7.7 we explained that if f : E → E is a linear map on a K-vector space E,
then for any polynomial p(X) = a0Xd + a1Xd−1 + · · · + ad with coefficients in the field K,
we can define the linear map p(f): E → E by
p(f) = a0f
d + a1f
d−1 + · · · + adid,
where f
k = f ◦ · · · ◦ f, the k-fold composition of f with itself. Note that
p(f)(u) = a0f
d
(u) + a1f
d−1
(u) + · · · + adu,
for every vector u ∈ E. Then we showed that if E is finite-dimensional and if χf (X) =
det(XI −f) is the characteristic polynomial of f, by the Cayley–Hamilton theorem, we have
χf (f) = 0.
This fact suggests looking at the set of all polynomials p(X) such that
p(f) = 0.
Such polynomials are called annihilating polynomials of f, the set of all these polynomials,
denoted Ann(f), is called the annihilator of f, and the Cayley-Hamilton theorem shows that
it is nontrivial since it contains a polynomial of positive degree. It turns out that Ann(f)
contains a polynomial mf of smallest degree that generates Ann(f), and this polynomial
divides the characteristic polynomial. Furthermore, the polynomial mf encapsulates a lot of
information about f, in particular whether f can be diagonalized. One of the main reasons
for this is that a scalar λ ∈ K is a zero of the minimal polynomial mf if and only if λ is an
eigenvalue of f.
1087
1088 CHAPTER 31. ANNIHILATING POLYNOMIALS; PRIMARY DECOMPOSITION
The first main result is Theorem 31.6 which states that if f : E → E is a linear map on
a finite-dimensional space E, then f is diagonalizable iff its minimal polynomial m is of the
form
m = (X − λ1)· · ·(X − λk),
where λ1, . . . , λk are distinct elements of K.
One of the technical tools used to prove this result is the notion of f-conductor ; see
Definition 31.2. As a corollary of Theorem 31.6 we obtain results about finite commuting
families of diagonalizable or triangulable linear maps.
If f : E → E is a linear map and λ ∈ K is an eigenvalue of f, recall that the eigenspace
Eλ associated with λ is the kernel of the linear map λid − f. If all the eigenvalues λ1 . . . , λk
of f are in K and if f is diagonalizable, then
E = Eλ1 ⊕ · · · ⊕ Eλk
,
but in general there are not enough eigenvectors to span E. A remedy is to generalize the
notion of eigenvector and look for (nonzero) vectors u (called generalized eigenvectors) such
that
(λid − f)
r
(u) = 0, for some r ≥ 1.
Then, it turns out that if the minimal polynomial of f is of the form
m = (X − λ1)
r1
· · ·(X − λk)
rk
,
then r = ri does the job for λi
; that is, if we let
Wi = Ker (λi
id − f)
ri
,
then
E = W1 ⊕ · · · ⊕ Wk.
The above facts are parts of the primary decomposition theorem (Theorem 31.11). It is a
special case of a more general result involving the factorization of the minimal polynomial
m into its irreducible monic factors; see Theorem 31.10.
Theorem 31.11 implies that every linear map f that has all its eigenvalues in K can be
written as f = D + N, where D is diagonalizable and N is nilpotent (which means that
Nr = 0 for some positive integer r). Furthermore D and N commute and are unique. This
is the Jordan decomposition, Theorem 31.12.
The Jordan decomposition suggests taking a closer look at nilpotent maps. We prove that
for any nilpotent linear map f : E → E on a finite-dimensional vector space E of dimension
n over a field K, there is a basis of E such that the matrix N of f is of the form
N =


0 ν1 0 · · · 0 0
0 0 ν2 · · · 0 0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 · · ·
0 0 0 · · ·
0
0 0
νn


,
31.1. ANNIHILATING POLYNOMIALS AND THE MINIMAL POLYNOMIAL 1089
where νi = 1 or νi = 0; see Theorem 31.16. As a corollary we obtain the Jordan form; which
involves matrices of the form
Jr(λ) =


λ
0 λ
1 0
1
· · ·
· · ·
0
0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 0 0 .
.
. 1
0 0 0 · · · λ


,
called Jordan blocks; see Theorem 31.17.
31.1 Annihilating Polynomials and the Minimal Poly￾nomial
Given a linear map f : E → E, it is easy to check that the set Ann(f) of polynomials that
annihilate f is an ideal. Furthermore, when E is finite-dimensional, the Cayley–Hamilton
Theorem implies that Ann(f) is not the zero ideal. Therefore, by Proposition 30.10, there is
a unique monic polynomial mf that generates Ann(f). Results from Chapter 30, especially
about gcd’s of polynomials, will come handy.
Definition 31.1. If f : E → E is a linear map on a finite-dimensional vector space E,
the unique monic polynomial mf (X) that generates the ideal Ann(f) of polynomials which
annihilate f (the annihilator of f) is called the minimal polynomial of f.
The minimal polynomial mf of f is the monic polynomial of smallest degree that an￾nihilates f. Thus, the minimal polynomial divides the characteristic polynomial χf , and
deg(mf ) ≥ 1. For simplicity of notation, we often write m instead of mf .
If A is any n × n matrix, the set Ann(A) of polynomials that annihilate A is the set of
polynomials
p(X) = a0X
d + a1X
d−1 + · · · + ad−1X + ad
such that
a0A
d + a1A
d−1 + · · · + ad−1A + adI = 0.
It is clear that Ann(A) is a nonzero ideal and its unique monic generator is called the minimal
polynomial of A. We check immediately that if Q is an invertible matrix, then A and Q−1AQ
have the same minimal polynomial. Also, if A is the matrix of f with respect to some basis,
then f and A have the same minimal polynomial.
The zeros (in K) of the minimal polynomial of f and the eigenvalues of f (in K) are
intimately related.
Proposition 31.1. Let f : E → E be a linear map on some finite-dimensional vector space
E. Then λ ∈ K is a zero of the minimal polynomial mf (X) of f iff λ is an eigenvalue of f
1090 CHAPTER 31. ANNIHILATING POLYNOMIALS; PRIMARY DECOMPOSITION
iff λ is a zero of χf (X). Therefore, the minimal and the characteristic polynomials have the
same zeros (in K), except for multiplicities.
Proof. First assume that m(λ) = 0 (with λ ∈ K, and writing m instead of mf ). If so, using
polynomial division, m can be factored as
m = (X − λ)q,
with deg(q) < deg(m). Since m is the minimal polynomial, q(f) 6 = 0, so there is some
nonzero vector v ∈ E such that u = q(f)(v) 6 = 0. But then, because m is the minimal
polynomial,
0 = m(f)(v)
= (f − λid)(q(f)(v))
= (f − λid)(u),
which shows that λ is an eigenvalue of f.
Conversely, assume that λ ∈ K is an eigenvalue of f. This means that for some u 6 = 0,
we have f(u) = λu. Now it is easy to show that
m(f)(u) = m(λ)u,
and since m is the minimal polynomial of f, we have m(f)(u) = 0, so m(λ)u = 0, and since
u 6 = 0, we must have m(λ) = 0.
Proposition 31.2. Let f : E → E be a linear map on some finite-dimensional vector space
E. If f diagonalizable, then its minimal polynomial is a product of distinct factors of degree
1.
Proof. If we assume that f is diagonalizable, then its eigenvalues are all in K, and if λ1, . . . , λk
are the distinct eigenvalues of f, and then by Proposition 31.1, the minimal polynomial m
of f must be a product of powers of the polynomials (X − λi). Actually, we claim that
m = (X − λ1)· · ·(X − λk).
For this we just have to show that m annihilates f. However, for any eigenvector u of f, one
of the linear maps f − λi
id sends u to 0, so
m(f)(u) = (f − λ1id) ◦ · · · ◦ (f − λkid)(u) = 0.
Since E is spanned by the eigenvectors of f, we conclude that
m(f) = 0.
It turns out that the converse of Proposition 31.2 is true, but this will take a little work
to establish it.
31.2. MINIMAL POLYNOMIALS OF DIAGONALIZABLE LINEAR MAPS 1091
31.2 Minimal Polynomials of Diagonalizable
Linear Maps
In this section we prove that if the minimal polynomial mf of a linear map f is of the form
mf = (X − λ1)· · ·(X − λk)
for distinct scalars λ1, . . . , λk ∈ K, then f is diagonalizable. This is a powerful result that
has a number of implications. But first we need of few properties of invariant subspaces.
Given a linear map f : E → E, recall that a subspace W of E is invariant under f if
f(u) ∈ W for all u ∈ W. For example, if f : R
2 → R
2
is f(x, y) = (−x, y), the y-axis is
invariant under f.
Proposition 31.3. Let W be a subspace of E invariant under the linear map f : E → E
(where E is finite-dimensional). Then the minimal polynomial of the restriction f | W of
f to W divides the minimal polynomial of f, and the characteristic polynomial of f | W
divides the characteristic polynomial of f.
Sketch of proof. The key ingredient is that we can pick a basis (e1, . . . , en) of E in which
(e1, . . . , ek) is a basis of W. The matrix of f over this basis is a block matrix of the form
A =

B C
0 D

,
where B is a k × k matrix, D is an (n − k) × (n − k) matrix, and C is a k × (n − k) matrix.
Then
det(XI − A) = det(XI − B) det(XI − D),
which implies the statement about the characteristic polynomials. Furthermore,
A
i =

Bi Ci
0 Di

,
for some k × (n − k) matrix Ci
. It follows that any polynomial which annihilates A also
annihilates B and D. So the minimal polynomial of B divides the minimal polynomial of
A.
For the next step, there are at least two ways to proceed. We can use an old-fashion
argument using Lagrange interpolants, or we can use a slight generalization of the notion of
annihilator. We pick the second method because it illustrates nicely the power of principal
ideals.
What we need is the notion of conductor (also called transporter).
Definition 31.2. Let f : E → E be a linear map on a finite-dimensional vector space E, let
W be an invariant subspace of f, and let u be any vector in E. The set Sf (u, W) consisting
of all polynomials q ∈ K[X] such that q(f)(u) ∈ W is called the f-conductor of u into W.
1092 CHAPTER 31. ANNIHILATING POLYNOMIALS; PRIMARY DECOMPOSITION
Observe that the minimal polynomial mf of f always belongs to Sf (u, W), so this is a
nontrivial set. Also, if W = (0), then Sf (u,(0)) is just the annihilator of f. The crucial
property of Sf (u, W) is that it is an ideal.
Proposition 31.4. If W is an invariant subspace for f, then for each u ∈ E, the f-conductor
Sf (u, W) is an ideal in K[X].
We leave the proof as a simple exercise, using the fact that if W invariant under f, then
W is invariant under every polynomial q(f) in Sf (u, W).
Since Sf (u, W) is an ideal, it is generated by a unique monic polynomial q of smallest
degree, and because the minimal polynomial mf of f is in Sf (u, W), the polynomial q divides
m.
Definition 31.3. The unique monic polynomial which generates Sf (u, W) is called the
conductor of u into W.
Example 31.1. For example, suppose f : R
2 → R
2 where f(x, y) = (x, 0). Observe that
W = {(x, 0) ∈ R
2} is invariant under f. By representing f as 
1 0
0 0 , we see that mf (X) =
χf (X) = X2 − X. Let u = (0, y). Then Sf (u, W) = (X) and we say X is the conductor of
u into W.
Proposition 31.5. Let f : E → E be a linear map on a finite-dimensional space E and
assume that the minimal polynomial m of f is of the form
m = (X − λ1)
r1
· · ·(X − λk)
rk
,
where the eigenvalues λ1, . . . , λk of f belong to K. If W is a proper subspace of E which is
invariant under f, then there is a vector u ∈ E with the following properties:
(a) u /∈ W;
(b) (f − λid)(u) ∈ W, for some eigenvalue λ of f.
Proof. Observe that (a) and (b) together assert that the conductor of u into W is a polyno￾mial of the form X − λi
. Pick any vector v ∈ E not in W, and let g be the conductor of v
into W, i.e. g(f)(v) ∈ W. Since g divides m and v /∈ W, the polynomial g is not a constant,
and thus it is of the form
g = (X − λ1)
s1
· · ·(X − λk)
sk
,
with at least some si > 0. Choose some index j such that sj > 0. Then X − λj
is a factor
of g, so we can write
g = (X − λj )q. (*)
31.2. MINIMAL POLYNOMIALS OF DIAGONALIZABLE LINEAR MAPS 1093
By definition of g, the vector u = q(f)(v) cannot be in W, since otherwise g would not be
of minimal degree. However, (∗) implies that
(f − λj
id)(u) = (f − λj
id)(q(f)(v))
= g(f)(v)
is in W, which concludes the proof.
We can now prove the main result of this section.
Theorem 31.6. Let f : E → E be a linear map on a finite-dimensional space E. Then f is
diagonalizable iff its minimal polynomial m is of the form
m = (X − λ1)· · ·(X − λk),
where λ1, . . . , λk are distinct elements of K.
Proof. We already showed in Proposition 31.2 that if f is diagonalizable, then its minimal
polynomial is of the above form (where λ1, . . . , λk are the distinct eigenvalues of f).
For the converse, let W be the subspace spanned by all the eigenvectors of f. If W 6 = E,
since W is invariant under f, by Proposition 31.5, there is some vector u /∈ W such that for
some λj
, we have
(f − λj
id)(u) ∈ W.
Let v = (f − λj
id)(u) ∈ W. Since v ∈ W, we can write
v = w1 + · · · + wk
where f(wi) = λiwi (either wi = 0 or wi
is an eigenvector for λi), and so for every polynomial
h, we have
h(f)(v) = h(λ1)w1 + · · · + h(λk)wk,
which shows that h(f)(v) ∈ W for every polynomial h. We can write
m = (X − λj )q
for some polynomial q, and also
q − q(λj ) = p(X − λj )
for some polynomial p. We know that p(f)(v) ∈ W, and since m is the minimal polynomial
of f, we have
0 = m(f)(u) = (f − λj
id)(q(f)(u)),
which implies that q(f)(u) ∈ W (either q(f)(u) = 0, or it is an eigenvector associated with
λj ). However,
q(f)(u) − q(λj )u = p(f)((f − λj
id)(u)) = p(f)(v),
and since p(f)(v) ∈ W and q(f)(u) ∈ W, we conclude that q(λj )u ∈ W. But, u /∈ W, which
implies that q(λj ) = 0, so λj
is a double root of m, a contradiction. Therefore, we must have
W = E.
1094 CHAPTER 31. ANNIHILATING POLYNOMIALS; PRIMARY DECOMPOSITION
Remark: Proposition 31.5 can be used to give a quick proof of Theorem 15.5.
31.3 Commuting Families of Diagonalizable and Trian￾gulable Maps
Using Theorem 31.6, we can give a short proof about commuting diagonalizable linear maps.
Definition 31.4. If F is a family of linear maps on a vector space E, we say that F is a
commuting family iff f ◦ g = g ◦ f for all f, g ∈ F.
Proposition 31.7. Let F be a finite commuting family of diagonalizable linear maps on a
vector space E. There exists a basis of E such that every linear map in F is represented in
that basis by a diagonal matrix.
Proof. We proceed by induction on n = dim(E). If n = 1, there is nothing to prove. If
n > 1, there are two cases. If all linear maps in F are of the form λid for some λ ∈
K, then the proposition holds trivially. In the second case, let f ∈ F be some linear
map in F which is not a scalar multiple of the identity. In this case, f has at least two
distinct eigenvalues λ1, . . . , λk, and because f is diagonalizable, E is the direct sum of the
corresponding eigenspaces Eλ1
, . . . , Eλk
. For every index i, the eigenspace Eλi
is invariant
under f and under every other linear map g in F, since for any g ∈ F and any u ∈ Eλi
,
because f and g commute, we have
f(g(u)) = g(f(u)) = g(λiu) = λig(u)
so g(u) ∈ Eλi
. Let Fi be the family obtained by restricting each f ∈ F to Eλi
. By
Proposition 31.3, the minimal polynomial of every linear map f | Eλi
in Fi divides the
minimal polynomial mf of f, and since f is diagonalizable, mf is a product of distinct
linear factors, so the minimal polynomial of f | Eλi
is also a product of distinct linear
factors. By Theorem 31.6, the linear map f | Eλi
is diagonalizable. Since k > 1, we have
dim(Eλi
) < dim(E) for i = 1, . . . , k, and by the induction hypothesis, for each i there is
a basis of Eλi
over which f | Eλi
is represented by a diagonal matrix. Since the above
argument holds for all i, by combining the bases of the Eλi
, we obtain a basis of E such that
the matrix of every linear map f ∈ F is represented by a diagonal matrix.
Remark: Proposition 31.7 also holds for infinite commuting families F of diagonalizable
linear maps, because E being finite dimensional, there is a finite subfamily of linearly inde￾pendent linear maps in F spanning F.
There is also an analogous result for commuting families of linear maps represented by
upper triangular matrices. To prove this we need the following proposition.
Proposition 31.8. Let F be a nonempty finite commuting family of triangulable linear maps
on a finite-dimensional vector space E. Let W be a proper subspace of E which is invariant
under F. Then there exists a vector u ∈ E such that:
31.3. COMMUTING FAMILIES OF LINEAR MAPS 1095
1. u /∈ W.
2. For every f ∈ F, the vector f(u) belongs to the subspace W ⊕ Ku spanned by W and
u.
Proof. By renaming the elements of F if necessary, we may assume that (f1, . . . , fr) is a
basis of the subspace of End(E) spanned by F. We prove by induction on r that there exists
some vector u ∈ E such that
1. u /∈ W.
2. (fi − αi
id)(u) ∈ W for i = 1, . . . , r, for some scalars αi ∈ K.
Consider the base case r = 1. Since f1 is triangulable, its eigenvalues all belong to K
since they are the diagonal entries of the triangular matrix associated with f1 (this is the
easy direction of Theorem 15.5), so the minimal polynomial of f1 is of the form
m = (X − λ1)
r1
· · ·(X − λk)
rk
,
where the eigenvalues λ1, . . . , λk of f1 belong to K. We conclude by applying Proposition
31.5.
Next assume that r ≥ 2 and that the induction hypothesis holds for f1, . . . , fr−1. Thus,
there is a vector ur−1 ∈ E such that
1. ur−1 ∈/ W.
2. (fi − αi
id)(ur−1) ∈ W for i = 1, . . . , r − 1, for some scalars αi ∈ K.
Let
Vr−1 = {w ∈ E | (fi − αi
id)(w) ∈ W, i = 1, . . . , r − 1}.
Clearly, W ⊆ Vr−1 and ur−1 ∈ Vr−1. We claim that Vr−1 is invariant under F. This is
because, for any v ∈ Vr−1 and any f ∈ F, since f and fi commute, we have
(fi − αi
id)(f(v)) = f((fi − αi
id)(v)), 1 ≤ i ≤ r − 1.
Now (fi −αi
id)(v) ∈ W because v ∈ Vr−1, and W is invariant under F, so f(fi −αi
id)(v)) ∈
W, that is, (fi − αi
id)(f(v)) ∈ W.
Consider the restriction gr of fr to Vr−1. The minimal polynomial of gr divides the
minimal polynomial of fr, and since fr is triangulable, just as we saw for f1, the minimal
polynomial of fr is of the form
m = (X − λ1)
r1
· · ·(X − λk)
rk
,
where the eigenvalues λ1, . . . , λk of fr belong to K, so the minimal polynomial of gr is of the
same form. By Proposition 31.5, there is some vector ur ∈ Vr−1 such that
1096 CHAPTER 31. ANNIHILATING POLYNOMIALS; PRIMARY DECOMPOSITION
1. ur ∈/ W.
2. (gr − αrid)(ur) ∈ W for some scalars αr ∈ K.
Now since ur ∈ Vr−1, we have (fi −αi
id)(ur) ∈ W for i = 1, . . . , r −1, so (fi −αi
id)(ur) ∈ W
for i = 1, . . . , r (since gr is the restriction of fr), which concludes the proof of the induction
step. Finally, since every f ∈ F is the linear combination of (f1, . . . , fr), Condition (2) of
the inductive claim implies Condition (2) of the proposition.
We can now prove the following result.
Proposition 31.9. Let F be a nonempty finite commuting family of triangulable linear maps
on a finite-dimensional vector space E. There exists a basis of E such that every linear map
in F is represented in that basis by an upper triangular matrix.
Proof. Let n = dim(E). We construct inductively a basis (u1, . . . , un) of E such that if Wi
is the subspace spanned by (u1 . . . , ui), then for every f ∈ F,
f(ui) = a
f
1iu1 + · · · + a
f
iiui
,
for some a
f
ij ∈ K; that is, f(ui) belongs to the subspace Wi
.
We begin by applying Proposition 31.8 to the subspace W0 = (0) to get u1 so that for all
f ∈ F,
f(u1) = α1
f
u1.
For the induction step, since Wi
invariant under F, we apply Proposition 31.8 to the subspace
Wi
, to get ui+1 ∈ E such that
1. ui+1 ∈/ Wi
.
2. For every f ∈ F, the vector f(ui+1) belong to the subspace spanned by Wi and ui+1.
Condition (1) implies that (u1, . . . , ui
, ui+1) is linearly independent, and Condition (2) means
that for every f ∈ F,
f(ui+1) = a
f
1i+1u1 + · · · + a
f
i+1i+1ui+1,
for some a
f
i+1j ∈ K, establishing the induction step. After n steps, each f ∈ F is represented
by an upper triangular matrix.
Observe that if F consists of a single linear map f and if the minimal polynomial of f is
of the form
m = (X − λ1)
r1
· · ·(X − λk)
rk
,
with all λi ∈ K, using Proposition 31.5 instead of Proposition 31.8, the proof of Proposition
31.9 yields another proof of Theorem 15.5.
31.4. THE PRIMARY DECOMPOSITION THEOREM 1097
31.4 The Primary Decomposition Theorem
If f : E → E is a linear map and λ ∈ K is an eigenvalue of f, recall that the eigenspace Eλ
associated with λ is the kernel of the linear map λid − f. If all the eigenvalues λ1 . . . , λk of
f are in K, it may happen that
E = Eλ1 ⊕ · · · ⊕ Eλk
,
but in general there are not enough eigenvectors to span E. What if we generalize the notion
of eigenvector and look for (nonzero) vectors u such that
(λid − f)
r
(u) = 0, for some r ≥ 1?
It turns out that if the minimal polynomial of f is of the form
m = (X − λ1)
r1
· · ·(X − λk)
rk
,
then r = ri does the job for λi
; that is, if we let
Wi = Ker (λi
id − f)
ri
,
then
E = W1 ⊕ · · · ⊕ Wk.
This result is very nice but seems to require that the eigenvalues of f all belong to K.
Actually, it is a special case of a more general result involving the factorization of the
minimal polynomial m into its irreducible monic factors (see Theorem 30.17),
m = p
r
1
1
· · · p
r
k
k
,
where the pi are distinct irreducible monic polynomials over K.
Theorem 31.10. (Primary Decomposition Theorem) Let f : E → E be a linear map on
the finite-dimensional vector space E over the field K. Write the minimal polynomial m of
f as
m = p
r
1
1
· · · p
r
k
k
,
where the pi are distinct irreducible monic polynomials over K, and the ri are positive inte￾gers. Let
Wi = Ker (p
r
i
i
(f)), i = 1, . . . , k.
Then
(a) E = W1 ⊕ · · · ⊕ Wk.
(b) Each Wi is invariant under f.
(c) The minimal polynomial of the restriction f | Wi of f to Wi is p
r
i
i
.
1098 CHAPTER 31. ANNIHILATING POLYNOMIALS; PRIMARY DECOMPOSITION
Proof. The trick is to construct projections πi using the polynomials p
r
j
j
so that the range
of πi
is equal to Wi
. Let
gi = m/pr
i
i =
Y
j6=i
p
r
j
j
.
Note that
p
r
i
igi = m.
Since p1, . . . , pk are irreducible and distinct, they are relatively prime. Then using Proposi￾tion 30.14, it is easy to show that g1, . . . , gk are relatively prime. Otherwise, some irreducible
polynomial p would divide all of g1, . . . , gk, so by Proposition 30.14 it would be equal to one
of the irreducible factors pi
. But that pi
is missing from gi
, a contradiction. Therefore, by
Proposition 30.15, there exist some polynomials h1, . . . , hk such that
g1h1 + · · · + gkhk = 1.
Let qi = gihi and let πi = qi(f) = gi(f)hi(f). We have
q1 + · · · + qk = 1,
and since m divides qiqj
for i 6 = j, we get
π1 + · · · + πk = id
πiπj = 0, i 6 = j.
(We implicitly used the fact that if p, q are two polynomials, the linear maps p(f) ◦ q(f)
and q(f) ◦ p(f) are the same since p(f) and q(f) are polynomials in the powers of f, which
commute.) Composing the first equation with πi and using the second equation, we get
π
2
i = πi
.
Therefore, the πi are projections, and E is the direct sum of the images of the πi
. Indeed,
every u ∈ E can be expressed as
u = π1(u) + · · · + πk(u).
Also, if
π1(u) + · · · + πk(u) = 0,
then by applying πi we get
0 = πi
2
(u) = πi(u), i = 1, . . . k.
To finish proving (a), we need to show that
Wi = Ker (p
r
i
i
(f)) = πi(E).
31.4. THE PRIMARY DECOMPOSITION THEOREM 1099
If v ∈ πi(E), then v = πi(u) for some u ∈ E, so
p
r
i
i
(f)(v) = p
r
i
i
(f)(πi(u))
= p
r
i
i
(f)gi(f)hi(f)(u)
= hi(f)p
r
i
i
(f)gi(f)(u)
= hi(f)m(f)(u) = 0,
because m is the minimal polynomial of f. Therefore, v ∈ Wi
.
Conversely, assume that v ∈ Wi = Ker (p
r
i
i
(f)). If j 6 = i, then gjhj
is divisible by p
r
i
i
, so
gj (f)hj (f)(v) = πj (v) = 0, j 6 = i.
Then since π1 + · · · + πk = id, we have v = πiv, which shows that v is in the range of πi
.
Therefore, Wi = Im(πi), and this finishes the proof of (a).
If p
r
i
i
(f)(u) = 0, then p
r
i
i
(f)(f(u)) = f(p
r
i
i
(f)(u)) = 0, so (b) holds.
If we write fi = f | Wi
, then p
r
i
i
(fi) = 0, because p
r
i
i
(f) = 0 on Wi (its kernel). Therefore,
the minimal polynomial of fi divides p
r
i
i
. Conversely, let q be any polynomial such that
q(fi) = 0 (on Wi). Since m = p
r
i
igi
, the fact that m(f)(u) = 0 for all u ∈ E shows that
p
r
i
i
(f)(gi(f)(u)) = 0, u ∈ E,
and thus Im(gi(f)) ⊆ Ker (p
r
i
i
(f)) = Wi
. Consequently, since q(f) is zero on Wi
,
q(f)gi(f) = 0 for all u ∈ E.
But then qgi
is divisible by the minimal polynomial m = p
r
i
igi of f, and since p
r
i
i and gi are
relatively prime, by Euclid’s proposition, p
r
i
i must divide q. This finishes the proof that the
minimal polynomial of fi
is p
r
i
i
, which is (c).
To best understand the projection constructions of Theorem 31.10, we provide the fol￾lowing two explicit examples of the primary decomposition theorem.
Example 31.2. First let f : R
3 → R
3 be defined as f(x, y, z) = (y, −x, z). In terms of the
standard basis f is represented by the 3 × 3 matrix Xf :=


0
1 0 0
0 0 1
−1 0
. Then a simple
calculation shows that mf (x) = χf (x) = (x
2 + 1)(x−1). Using the notation of the preceding
proof set
m = p1p2, p1 = x
2 + 1, p2 = x − 1.
Then
g1 =
m
p1
= x − 1, g2 =
m
p2
= x
2 + 1.
1100 CHAPTER 31. ANNIHILATING POLYNOMIALS; PRIMARY DECOMPOSITION
We must find h1, h2 ∈ R[x] such that g1h1 + g2h2 = 1. In general this is the hard part
of the projection construction. But since we are only working with two relatively prime
polynomials g1, g2, we may apply the Euclidean algorithm to discover that
−
x + 1
2
(x − 1) + 1
2
(x
2 + 1) = 1,
where h1 = −
x+1
2 while h2 =
1
2
. By definition
π1 = g1(f)h1(f) = −
1
2
(Xf − id)(Xf + id) = −
1
2
(Xf
2 − id) =


1 0 0
0 1 0
0 0 0

 ,
and
π2 = g2(f)h2(f) = 1
2
(Xf
2 + id) =


0 0 0
0 0 0
0 0 1

 .
Then R
3 = W1 ⊕ W2, where
W1 = π1(R
3
) = Ker (p1(Xf )) = Ker (Xf
2 + id) = Ker


0 0 0
0 0 0
0 0 1

 = {(x, y, 0) ∈ R
3
},
W2 = π2(R
3
) = Ker (p2(Xf )) = Ker (Xf − id) = Ker


−
1
0 0 0
1 −
−
1 0
1 0

 = {(0, 0, z) ∈ R
3
}.
Example 31.3. For our second example of the primary decomposition theorem let f : R
3 →
R

3 be defined as f(x, y, z) = (y, −x + z, −y), with standard matrix representation Xf =

0
1 0
−1 0
−1
0 1 0

. A simple calculation shows that mf (x) = χf (x) = x(x
2 + 2). Set
p1 = x
2 + 2, p2 = x, g1 =
mf
p1
= x, g2 =
mf
p2
= x
2 + 2.
Since gcd(g1, g2) = 1, we use the Euclidean algorithm to find
h1 = −
1
2
x, h2 =
1
2
,
such that g1h1 + g2h2 = 1. Then
π1 = g1(f)h1(f) = −
1
2
Xf
2 =


1
2
0 −
1
2
−
0 1 0
1
2
0
1
2

 ,
31.4. THE PRIMARY DECOMPOSITION THEOREM 1101
while
π2 = g2(f)h2(f) = 1
2
(Xf
2 + 2id) =


1
2
0
1
2
0 0 0
1
2
0
1
2

 .
Although it is not entirely obvious, π1 and π2 are indeed projections since
π
2
1 =


1
2
0 −
1
2
−
0 1 0
1
2
0
1
2




1
2
0 −
1
2
−
0 1 0
1
2
0
1
2

 =


1
2
0 −
1
2
−
0 1 0
1
2
0
1
2

 = π1,
and
π
2
2 =


1
2
0
1
2
0 0 0
1
2
0
1
2



