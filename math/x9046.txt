Then b and Î· are given by the following averaging formulae:
b = w
>
ï£«
ï£­

X
iâˆˆIÎ»
ui
 /|IÎ»| +

X
jâˆˆIÂµ
vj
 /|IÂµ|
ï£¶
ï£¸ /2
Î· = w
>
ï£«
ï£­

X
iâˆˆIÎ»
ui
 /|IÎ»| âˆ’  X
jâˆˆIÂµ
vj
 /|IÂµ|
ï£¶
ï£¸ /2.
Proposition 54.1 yields bounds on Î½ for the method to converge, namely
max 
p
2
+
pf
q
,
p
2
+
qf
q

â‰¤ Î½ â‰¤ min 
2pm
p + q
,
2qm
p + q

.
54.17. SUMMARY AND COMPARISON OF THE SVM METHODS 2023
In Section 54.7 we investigate conditions on Î½ that ensure that some point ui0 and
some point vj0
is a support vector. Theorem 54.3 shows that for every optimal solution
(w, b, Î·, , Î¾) of Problem (SVMs2
0 ) with w 6 = 0 and Î· > 0, if
max{2pf /(p + q), 2qf /(p + q)} < Î½ < min{2p/(p + q), 2q/(p + q)},
then some ui0 and some vj0
is a support vector. Under the same conditions on Î½
Proposition 54.4 shows that Î· and b can always be determined in terms of (Î», Âµ) using
a single support vector.
(3) Soft margin Î½-SVM Problem (SVMs3). This is the variation of Problem (SVMs2
0 )
obtained by adding the term (1/2)b
2
to the objective function. The result is that
in minimizing the Lagrangian to find the dual function G, not just w but also b is
determined. We also suppress the constraint Î· â‰¥ 0 which turns out to be redundant.
If Î½ > (pf +qf )/(p+q), then Î· is also determined. The fact that b and Î· are determined
by the dual seems to be an advantage of Problem (SVMs3).
The optimization problem is
minimize
1
2
w
> w +
1
2
b
2 + (p + q)Ks
 âˆ’Î½Î· +
p +
1
q
ï¿¾

>
Î¾
>
 1p+q

subject to
w
> ui âˆ’ b â‰¥ Î· âˆ’  i
, i â‰¥ 0 i = 1, . . . , p
âˆ’ w
> vj + b â‰¥ Î· âˆ’ Î¾j
, Î¾j â‰¥ 0 j = 1, . . . , q.
Theoretically it is convenient to assume that Ks = 1/(p + q). Otherwise, Î½ needs to
be replaced by (p + q)KsÎ½ in all the formulae below.
It is shown in Section 54.13 that the dual is given by
Dual of the Soft margin Î½-SVM Problem (SVMs3):
minimize
1
2
ï¿¾
Î»
> Âµ
>

 X
> X +

1p1
>p âˆ’1p1
>q
âˆ’1q1
>p 1q1
>q
  Âµ
Î»

subject to
p
X
i=1
Î»i +
q
X
j=1
Âµj = Î½
0 â‰¤ Î»i â‰¤ Ks, i = 1, . . . , p
0 â‰¤ Âµj â‰¤ Ks, j = 1, . . . , q.
2024 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
Once a solution for Î» and Âµ is obtained, we have
w = âˆ’X

Âµ
Î»

=
p
X
i=1
Î»iui âˆ’
q
X
j=1
Âµjvj
b = âˆ’
p
X
i=1
Î»i +
q
X
j=1
Âµj
.
Note that the constraint
p
X
i=1
Î»i âˆ’
q
X
j=1
Âµj = 0
occurring in the dual of Program (SVMs2
0 ) has been traded for the equation
b = âˆ’
p
X
i=1
Î»i +
q
X
j=1
Âµj
determining b.
If Î½ > (pf + qf )/(p + q), then Î· is determined by expressing that the duality gap is
zero. We obtain
((p + q)Î½ âˆ’ pf âˆ’ qf )Î· = (pf âˆ’ qf )b + w
>

X
jâˆˆKÂµ
vj âˆ’
i
XâˆˆKÎ»
ui

+
1
Ks
ï¿¾
Î»
> Âµ
>

 X
> X +

1p1
>p âˆ’1p1
>q
âˆ’1q1
>p 1q1
>q
  Âµ
Î»

.
In practice another way to compute Î· is to assume the Standard Margin Hypothesis
for (SVMs3). Under the Standard Margin Hypothesis for (SVMs3), either some
ui0
is a support vector of type 1 or some vj0
is a support vector of type 1. By the
complementary slackness conditions  i0 = 0 or Î¾j0 = 0, so we have
w
> ui0 âˆ’ b = Î·, or âˆ’ w
> vj0 + b = Î·,
and we can solve for Î·. As in (SVMs2
0 ) we get more numerically stable formulae by
averaging over the sets IÎ» and IÂµ.
Proposition 54.5 gives bounds Î½, namely
pf + qf
p + q
â‰¤ Î½ â‰¤
pm + qm
p + q
.
In Section 54.11 we investigate conditions on Î½ that ensure that either there is some
blue support vector ui0 or there is some red support vector vj0
. Theorem 54.7 shows
54.17. SUMMARY AND COMPARISON OF THE SVM METHODS 2025
that for every optimal solution (w, b, Î·, , Î¾) of Problem (SVMs3) with w 6 = 0 and Î· > 0,
if
(psf + qsf )/(p + q) < Î½ < 1,
then some ui0 or some vj0
is a support vector.
(4) Basic Quadratic Soft margin Î½-SVM Problem (SVMs4). This is the version of
Problem (SVMs2
0 ) in which instead of using the linear function Ks
ï¿¾ 
> Î¾
>
 1p+q as a
regularizing function we use the quadratic function K(k  k
2
2 + k Î¾k
2
2
). The optimization
problem is
minimize
1
2
w
> w + (p + q)Ks
 âˆ’Î½Î· +
p +
1
q
(
>  + Î¾
> Î¾)

subject to
w
> ui âˆ’ b â‰¥ Î· âˆ’  i
, i = 1, . . . , p
âˆ’ w
> vj + b â‰¥ Î· âˆ’ Î¾j
, j = 1, . . . , q
Î· â‰¥ 0,
where Î½ and Ks are two given positive constants. As we saw earlier, theoretically, it is
convenient to pick Ks = 1/(p + q). When writing a computer program, it is preferable
to assume that Ks is arbitrary. In this case Î½ needs to be replaced by (p + q)KsÎ½ in
all the formulae obtained with Ks = 1/(p + q).
In this method, it is no longer necessary to require  â‰¥ 0 and Î¾ â‰¥ 0, because an optimal
solution satisfies these conditions.
One of the advantages of this methods is that  is determined by Î», Î¾ is determined by
Âµ, and Î· and b are determined by Î» and Âµ. We can omit the constraint Î· â‰¥ 0, because
for an optimal solution it can be shown using duality that Î· â‰¥ 0; see Section 54.14.
For Ks and Î½ fixed, if Program (SVMs4) has an optimal solution, then it is unique; see
Theorem 54.8.
A drawback of Program (SVMs4) is that for fixed Ks, the quantity Î´ = Î·/ k wk and the
hyperplanes Hw,b, Hw,b+Î· and Hw,bâˆ’Î· are independent of Î½. This is shown in Theorem
54.8. Thus this method is less flexible than (SVMs2
0 ) and (SVMs3).
It is shown in Section 54.9 that the dual is given by
2026 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
Dual of the Basic Quadratic Soft margin Î½-SVM Problem (SVMs4):
minimize
1
2
ï¿¾
Î»
> Âµ
>

 X
> X +
2
1
K
Ip+q
 
Âµ
Î»

subject to
p
X
i=1
Î»i âˆ’
q
X
j=1
Âµj = 0
p
X
i=1
Î»i +
q
X
j=1
Âµj â‰¥ Î½
Î»i â‰¥ 0, i = 1, . . . , p
Âµj â‰¥ 0, j = 1, . . . , q.
The above program is similar to the program that was obtained for Problem (SVMs2
0 )
but the matrix X> X is replaced by the matrix X> X + (1/2K)Ip+q, which is positive
definite since K > 0, and also the inequalities Î»i â‰¤ K and Âµj â‰¤ K no longer hold. If
the constraint Î· â‰¥ 0 is dropped, then the inequality
p
X
i=1
Î»i +
q
X
j=1
Âµj â‰¥ Î½
is replaced by the equation
p
X
i=1
Î»i +
q
X
j=1
Âµj = Î½.
We obtain w from Î» and Âµ, and Î³, as in Problem (SVMs2
0 ); namely,
w = âˆ’X

Âµ
Î»

=
p
X
i=1
Î»iui âˆ’
q
X
j=1
Âµjvj
and Î· is given by
(p + q)KsÎ½Î· =
ï¿¾ Î»
> Âµ
>

 X
> X +
2K
1
s
Ip+q
 
Âµ
Î»

.
The constraints imply that there is some io such that Î»i0 > 0 and some j0 such that
Âµj0 > 0, which means that at least two points are misclassified, so Problem (SVMs4)
should only be used when the sets {ui} and {vj} are not linearly separable. We can
solve for b using the active constraints corresponding to any i0 such that Î»i0 > 0 and
any j0 such that Âµj0 > 0. To improve numerical stability we average over the sets of
indices IÎ» and IÂµ.
54.17. SUMMARY AND COMPARISON OF THE SVM METHODS 2027
(5) Quadratic Soft margin Î½-SVM Problem (SVMs5). This is the variant of Problem
(SVMs4) in which we add the term (1/2)b
2
to the objective function. We also drop the
constraint Î· â‰¥ 0 which is redundant. We have the following optimization problem:
minimize
1
2
w
> w +
1
2
b
2 + (p + q)Ks
 âˆ’Î½Î· +
p +
1
q
(
>  + Î¾
> Î¾)

subject to
w
> ui âˆ’ b â‰¥ Î· âˆ’  i
, i = 1, . . . , p
âˆ’ w
> vj + b â‰¥ Î· âˆ’ Î¾j
, j = 1, . . . , q,
where Î½ and Ks are two given positive constants. As we saw earlier, it is convenient
to pick Ks = 1/(p + q). When writing a computer program, it is preferable to assume
that Ks is arbitrary. In this case Î½ must be replaced by (p + q)KsÎ½ in all the formulae.
One of the advantages of this methods is that  is determined by Î», Î¾ is determined
by Âµ (as in (SVMs4)), and both Î· and b determined by Î» and Âµ. We can omit the
constraint Î· â‰¥ 0, because for an optimal solution it can be shown using duality that
Î· â‰¥ 0. For Ks and Î½ fixed, if Program (SVMs5) has an optimal solution, then it is
unique; see Theorem 54.9.
A drawback of Program (SVMs5) is that for fixed Ks, the quantity Î´ = Î·/ k wk and the
hyperplanes Hw,b, Hw,b+Î· and Hw,bâˆ’Î· are independent of Î½. This is shown in Theorem
54.9. Thus this method is less flexible than (SVMs2
0 ) and (SVMs3).
It is shown in Section 54.15 that the dual of Program (SVMs5) is given by
Dual of the Quadratic Soft margin Î½-SVM Problem (SVMs5):
minimize
1
2
ï¿¾
Î»
> Âµ
>

 X
> X +

1p1
>p âˆ’1p1
>q
âˆ’1q1
>p 1q1
>q

+
2
1
K
Ip+q
 
Âµ
Î»

subject to
p
X
i=1
Î»i +
q
X
j=1
Âµj = Î½
Î»i â‰¥ 0, i = 1, . . . , p
Âµj â‰¥ 0, j = 1, . . . , q.
2028 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
This time we obtain w, b, Î·,  and Î¾ from Î» and Âµ:
w =
p
X
i=1
Î»iui âˆ’
q
X
j=1
Âµjvj
b = âˆ’
p
X
i=1
Î»i +
q
X
j=1
Âµj

=
Î»
2K
Î¾ =
Âµ
2K
,
and
(p + q)KsÎ½Î· =
ï¿¾ Î»
> Âµ
>

 X
> X +

1p1
>p âˆ’1p1
>q
âˆ’1q1
>p 1q1
>q

+
2K
1
s
Ip+q
 
Âµ
Î»

.
The constraint
p
X
i=1
Î»i +
q
X
j=1
Âµj = Î½
implies that either there is some i0 such that Î»i0 > 0 or there is some j0 such that
Âµj0 > 0, we have  i0 > 0 or Î¾j0 > 0, which means that at least one point is misclassified,
so Problem (SVMs5) should only be used when the sets {ui} and {vj} are not linearly
separable.
These methods all have a kernelized version.
We implemented all these methods in Matlab, solving the dual using ADMM.
From a theoretical point of view, Problems (SVMs4) and (SVMs5) seem to have more
advantages than the others since they determine w, b, Î· and b without requiring any condition
about support vectors of type 1. However, from a practical point of view, Problems (SVMs4)
and (SVMs5) are less flexible that (SVMs2
0 ) and (SVMs3), and we have observed that (SVMs4)
and (SVMs5) are unable to produce as small a margin Î´ as (SVMs2
0 ) and (SVMs3).
54.18 Problems
Problem 54.1. Prove the following inequality
max 
2p
1
m
,
1
2qm

â‰¤ K â‰¤ min 
2
1
pf
,
1
2qf

stated just after Definition 54.1.
54.18. PROBLEMS 2029
Problem 54.2. Prove the averaging formulae
b = w
>
ï£«
ï£­

X
iâˆˆIÎ»
ui
 /|IÎ»| +

X
jâˆˆIÂµ
vj
 /|IÂµ|
ï£¶
ï£¸ /2
Î´ = w
>
ï£«
ï£­

X
iâˆˆIÎ»
ui
 /|IÎ»| âˆ’  X
jâˆˆIÂµ
vj
 /|IÂµ|
ï£¶
ï£¸ /2
stated at the end of Section 54.1.
Problem 54.3. Prove that the matrix
A =
ï£«
ï£¬ï£¬ï£¬ï£¬ï£­
1
>p âˆ’1
>q
0
>p
0
>q
1
>p 1
>q
0
>p
0
>q
Ip 0p,q Ip 0p,q
0q,p Iq 0q,p Iq
ï£¶
ï£·ï£·ï£·ï£·ï£¸
has rank p + q + 2.
Problem 54.4. Prove that the dual program of the kernel version of (SVMs1) is given by:
Dual of Soft margin kernel SVM (SVMs1):
minimize ï¿¾ Î»
> Âµ
>
 K

Âµ
Î»

subject to
p
X
i=1
Î»i =
q
X
j=1
Âµj =
1
2
0 â‰¤ Î»i â‰¤ K, i = 1, . . . , p
0 â‰¤ Âµj â‰¤ K, j = 1, . . . , q,
where K is the ` Ã— ` kernel symmetric matrix (with ` = p + q) given by
Kij =
ï£±
ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£³
Îº(ui
, uj ) 1 â‰¤ i â‰¤ p, 1 â‰¤ j â‰¤ q
âˆ’Îº(ui
, vjâˆ’p) 1 â‰¤ i â‰¤ p, p + 1 â‰¤ j â‰¤ p + q
âˆ’Îº(viâˆ’p, uj ) p + 1 â‰¤ i â‰¤ p + q, 1 â‰¤ j â‰¤ p
Îº(viâˆ’p, vjâˆ’q) p + 1 â‰¤ i â‰¤ p + q, p + 1 â‰¤ j â‰¤ p + q.
Problem 54.5. Prove the averaging formula
b = w
>
ï£«
ï£­

X
iâˆˆIÎ»
ui
 /|IÎ»| +

X
jâˆˆIÂµ
vj
 /|IÂµ|
ï£¶
ï£¸ /2
stated in Section 54.3.
2030 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
Problem 54.6. Prove that the kernel version of Program (SVMs2) is given by:
Dual of Soft margin kernel SVM (SVMs2):
minimize
1
2
ï¿¾
Î»
> Âµ
>
 K

Âµ
Î»

âˆ’
ï¿¾ Î»
> Âµ
>
 1p+q
subject to
p
X
i=1
Î»i âˆ’
q
X
j=1
Âµj = 0
0 â‰¤ Î»i â‰¤ K, i = 1, . . . , p
0 â‰¤ Âµj â‰¤ K, j = 1, . . . , q,
where K is the ` Ã— ` kernel symmetric matrix (with ` = p + q) given at the end of Section
54.1.
Problem 54.7. Prove that the matrix
A =
ï£«
ï£¬ï£¬ï£­
1
I
>
p
p
âˆ’1
>q
0
>p
0
>q
0p,q Ip 0p,q
0q,p Iq 0q,p Iq
ï£¶
ï£·ï£·ï£¸
has rank p + q + 1.
Problem 54.8. Prove that the matrices
A =
ï£«
ï£¬ï£¬ï£¬ï£¬ï£­
1
>p âˆ’1
>q
0
>p
0
>q
0
1
>p 1
>q
0
>p
0
>q âˆ’1
Ip 0p,q Ip 0p,q 0p
0q,p Iq 0q,p Iq 0q
ï£¶
ï£·ï£·ï£·ï£·ï£¸
and A2 =
ï£«
ï£¬ï£¬ï£¬ï£¬ï£­
1
>p âˆ’1
>q
0
>p
0
>q
1
>p 1
>q
0
>p
0
>q
Ip 0p,q Ip 0p,q
0q,p Iq 0q,p Iq
ï£¶
ï£·ï£·ï£·ï£·ï£¸
have rank p + q + 2.
Problem 54.9. Prove that the kernel version of Program (SVMs2
0 ) is given by:
54.18. PROBLEMS 2031
Dual of the Soft margin kernel SVM (SVMs2
0 ):
minimize
2
1 ï¿¾
Î»
> Âµ
>
 K

Âµ
Î»

subject to
p
X
i=1
Î»i âˆ’
q
X
j=1
Âµj = 0
p
X
i=1
Î»i +
q
X
j=1
Âµj â‰¥ Km
0 â‰¤ Î»i â‰¤ Ks, i = 1, . . . , p
0 â‰¤ Âµj â‰¤ Ks, j = 1, . . . , q,
where K is the kernel matrix of Section 54.1.
Problem 54.10. Prove the formulae determining b in terms of Î· stated just before Theorem
54.8.
Problem 54.11. Prove that the matrix
A =
ï£«
ï£¬ï£¬ï£­
1
I
>
p
p
1
>q
0
>p
0
>q
0p,q Ip 0p,q
0q,p Iq 0q,p Iq
ï£¶
ï£·ï£·ï£¸
has rank p + q + 1.
Problem 54.12. Prove that the kernel version of Program (SVMs3) is given by:
Dual of the Soft margin kernel SVM (SVMs3):
minimize
1
2
ï¿¾
Î»
> Âµ
>

 K +

1p1
>p âˆ’1p1
>q
âˆ’1q1
>p 1q1
>q
  Âµ
Î»

subject to
p
X
i=1
Î»i +
q
X
j=1
Âµj = Î½
0 â‰¤ Î»i â‰¤ Ks, i = 1, . . . , p
0 â‰¤ Âµj â‰¤ Ks, j = 1, . . . , q,
where K is the kernel matrix of Section 54.1.
Problem 54.13. Prove that the matrices
A =
 
1
>p âˆ’1
>q
0
1
>p 1
>q âˆ’1
!
and A2 =
 
1
>p âˆ’1
>q
1
>p 1
>q
!
have rank 2.
2032 CHAPTER 54. SOFT MARGIN SUPPORT VECTOR MACHINES
Problem 54.14. Implement Program (SVMs4) in Matlab. You may adapt the programs
given in Section B.2 and Section B.3.
Problem 54.15. Prove that the kernel version of Program (SVMs4) is given by:
Dual of the Soft margin kernel SVM (SVMs4):
minimize
1
2
ï¿¾
Î»
> Âµ
>

 K +
p +
2
q
Ip+q
 
Âµ
Î»

subject to
p
X
i=1
Î»i âˆ’
q
X
j=1
Âµj = 0
p
X
i=1
Î»i +
q
X
j=1
Âµj â‰¥ Î½
Î»i â‰¥ 0, i = 1, . . . , p
Âµj â‰¥ 0, j = 1, . . . , q,
where K is the kernel matrix of Section 54.1.
Problem 54.16. Implement Program (SVMs5) in Matlab. You may adapt the programs
given in Section B.2 and Section B.3.
Problem 54.17. Prove that the kernel version of Program (SVMs5) is given by:
Dual of the Soft margin kernel SVM (SVMs5):
minimize
1
2
ï¿¾
Î»
> Âµ
>

 K +

1p1
>p âˆ’1p1
>q
âˆ’1q1
>p 1q1
>q

+
p +
2
q
Ip+q
 
Âµ
Î»

subject to
p
X
i=1
Î»i +
q
X
j=1
Âµj = Î½
Î»i â‰¥ 0, i = 1, . . . , p
Âµj â‰¥ 0, j = 1, . . . , q,
where K is the kernel matrix of Section 54.1.
Chapter 55
Ridge Regression, Lasso, Elastic Net
In this chapter we discuss linear regression. This problem can be cast as a learning problem.
We observe a sequence of (distinct) pairs ((x1, y1), . . . ,(xm, ym)) called a set of training data
(or predictors), where xi âˆˆ R
n and yi âˆˆ R, viewed as input-output pairs of some unknown
function f that we are trying to infer. The simplest kind of function is a linear function
f(x) = x
> w, where w âˆˆ R
n
is a vector of coefficients usually called a weight vector . Since
the problem is overdetermined and since our observations may be subject to errors, we canâ€™t
solve for w exactly as the solution of the system Xw = y, where X is the m Ã— n matrix
X =
ï£«
ï£¬ï£­
x
>1
.
.
x
.
>
m
ï£¶
ï£·ï£¸ ,
