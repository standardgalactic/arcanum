
R S
0 0 Π,
where Q is an orthogonal n×n matrix, R is an r ×r invertible upper triangular matrix, and
S is an r × (p − r) matrix (assuming C has rank r). If we let
x = Q
>

y
z

,
where y ∈ R
r and z ∈ R
n−r
, then C
> x = 0 becomes
Π
>

R> 0
S
> 0

Qx = Π>  R> 0
S
> 0
 
y
z

= 0,
which implies y = 0, and every solution of C
> x = 0 is of the form
x = Q
>

z
0

.
Our original problem becomes
minimize (y
> z
> )QAQ>  y
z

subject to z
> z = 1, z ∈ R
n−r
,
y = 0, y ∈ R
r
.
42.3. MAXIMIZING A QUADRATIC FUNCTION ON THE UNIT SPHERE 1525
Thus the constraint C
> x = 0 has been simplified to y = 0, and if we write
QAQ> =

G11 G12
G>12 G22
,
our problem becomes
minimize z
> G22z
subject to z
> z = 1, z ∈ R
n−r
,
a standard eigenvalue problem.
Remark: There is a way of finding the eigenvalues of G22 which does not require the QR￾factorization of C. Observe that if we let
J =

0 0
0 In−r

,
then
JQAQ> J =

0 0
0 G22
,
and if we set
P = Q
> JQ,
then
P AP = Q
> JQAQ> JQ.
Now, Q> JQAQ> JQ and JQAQ> J have the same eigenvalues, so P AP and JQAQ> J also
have the same eigenvalues. It follows that the solutions of our optimization problem are
among the eigenvalues of K = P AP, and at least r of those are 0. Using the fact that CC+
is the projection onto the range of C, where C
+ is the pseudo-inverse of C, it can also be
shown that
P = I − CC+,
the projection onto the kernel of C
> . So P can be computed directly in terms of C. In
particular, when n ≥ p and C has full rank (the columns of C are linearly independent),
then we know that C
+ = (C
> C)
−1C
> and
P = I − C(C
> C)
−1C
> .
This fact is used by Cour and Shi [42] and implicitly by Yu and Shi [192].
The problem of adding affine constraints of the form N > x = t, where t 6 = 0, also comes
up in practice. At first glance, this problem may not seem harder than the linear problem in
which t = 0, but it is. This problem was extensively studied in a paper by Gander, Golub,
and von Matt [75] (1989).
1526 CHAPTER 42. QUADRATIC OPTIMIZATION PROBLEMS
Gander, Golub, and von Matt consider the following problem: Given an (n+m)×(n+m)
real symmetric matrix A (with n > 0), an (n+m)×m matrix N with full rank, and a nonzero
vector t ∈ R
m with
  (N > )
+t
 < 1 (where (N > )
+ denotes the pseudo-inverse of N > ),
minimize x
> Ax
subject to x
> x = 1, N > x = t, x ∈ R
n+m.
The condition
  (N > )
+t
 < 1 ensures that the problem has a solution and is not trivial.
The authors begin by proving that the affine constraint N > x = t can be eliminated. One
way to do so is to use a QR decomposition of N. If
N = P

R
0

,
where P is an orthogonal (n + m) × (n + m) matrix and R is an m × m invertible upper
triangular matrix, then if we observe that
x
> Ax = x
> P P > AP P > x,
N
> x = (R
> 0)P
> x = t,
x
> x = x
> P P > x = 1,
and if we write
P
> AP =

B Γ
>
Γ C

,
where B is an m × m symmetric matrix, C is an n × n symmetric matrix, Γ is an m × n
matrix, and
P
> x =

y
z

,
with y ∈ R
m and z ∈ R
n
, we then get
x
> Ax = y
> By + 2z
> Γy + z
> Cz,
R
> y = t,
y
> y + z
> z = 1.
Thus
y = (R
> )
−1
t,
and if we write
s
2 = 1 − y
> y > 0
and
b = Γy,
42.4. SUMMARY 1527
we get the simplified problem
minimize z
> Cz + 2z
> b
subject to z
> z = s
2
, z ∈ R
m.
Unfortunately, if b 6 = 0, Proposition 23.10 is no longer applicable. It is still possible to find
the minimum of the function z
> Cz + 2z
> b using Lagrange multipliers, but such a solution
is too involved to be presented here. Interested readers will find a thorough discussion in
Gander, Golub, and von Matt [75].
42.4 Summary
The main concepts and results of this chapter are listed below:
• Quadratic optimization problems; quadratic functions.
• Symmetric positive definite and positive semidefinite matrices.
• The positive semidefinite cone ordering.
• Existence of a global minimum when A is symmetric positive definite.
• Constrained quadratic optimization problems.
• Lagrange multipliers; Lagrangian.
• Primal and dual problems.
• Quadratic optimization problems: the case of a symmetric invertible matrix A.
• Quadratic optimization problems: the general case of a symmetric matrix A.
• Adding linear constraints of the form C
> x = 0.
• Adding affine constraints of the form C
> x = t, with t 6 = 0.
• Maximizing a quadratic function over the unit sphere.
• Maximizing a quadratic function over an ellipsoid.
• Maximizing a Hermitian quadratic form.
• Adding linear constraints of the form C
> x = 0.
• Adding affine constraints of the form N > x = t, with t 6 = 0.
1528 CHAPTER 42. QUADRATIC OPTIMIZATION PROBLEMS
42.5 Problems
Problem 42.1. Prove that the relation
A  B
between any two n×n matrices (symmetric or not) iff A−B is symmetric positive semidefinite
is indeed a partial order.
Problem 42.2. (1) Prove that if A is symmetric positive definite, then so is A−1
.
(2) Prove that if C is a symmetric positive definite m × m matrix and A is an m × n
matrix of rank n (and so m ≥ n and the map x 7→ Ax is injective), then A> CA is symmetric
positive definite.
Problem 42.3. Find the minimum of the function
Q(x1, x2) = 1
2
(2x
2
1 + x
2
2
)
subject to the constraint
x1 − x2 = 3.
Problem 42.4. Consider the problem of minimizing the function
f(x) = 1
2
x
> Ax − x
> b
in the case where we add an affine constraint of the form C
> x = t, with t ∈ R
m and t 6 = 0,
and where C is an n×m matrix of rank m ≤ n. As in Section 42.2, use a QR-decomposition
C = P

R
0

,
where P is an orthogonal n×n matrix and R is an m×m invertible upper triangular matrix,
and write
x = P

y
z

,
to deduce that
R
> y = t.
Give the details of the reduction of this constrained minimization problem to an uncon￾strained minimization problem involving the matrix P
> AP.
Problem 42.5. Find the maximum and the minimum of the function
Q(x, y) = ￾ x y 
1 2
2 1 
x
y

on the unit circle x
2 + y
2 = 1.
Chapter 43
Schur Complements and Applications
Schur complements arise naturally in the process of inverting block matrices of the form
M =

C D
A B
and in characterizing when symmetric versions of these matrices are positive definite or
positive semidefinite. These characterizations come up in various quadratic optimization
problems; see Boyd and Vandenberghe [29], especially Appendix B. In the most general
case, pseudo-inverses are also needed.
In this chapter we introduce Schur complements and describe several interesting ways in
which they are used. Along the way we provide some details and proofs of some results from
Appendix A.5 (especially Section A.5.5) of Boyd and Vandenberghe [29].
43.1 Schur Complements
Let M be an n × n matrix written as a 2 × 2 block matrix
M =

C D
A B
,
where A is a p × p matrix and D is a q × q matrix, with n = p + q (so B is a p × q matrix
and C is a q × p matrix). We can try to solve the linear system

C D
A B  x
y

=

d
c

,
that is,
Ax + By = c,
Cx + Dy = d,
1529
1530 CHAPTER 43. SCHUR COMPLEMENTS AND APPLICATIONS
by mimicking Gaussian elimination. If we assume that D is invertible, then we first solve
for y, getting
y = D
−1
(d − Cx),
and after substituting this expression for y in the first equation, we get
Ax + B(D
−1
(d − Cx)) = c,
that is,
(A − BD−1C)x = c − BD−1
d.
If the matrix A − BD−1C is invertible, then we obtain the solution to our system
x = (A − BD−1C)
−1
(c − BD−1
d),
y = D
−1
(d − C(A − BD−1C)
−1
(c − BD−1
d)).
If A is invertible, then by eliminating x first using the first equation, we obtain analogous
formulas involving the matrix D − CA−1B. The above formulas suggest that the matrices
A − BD−1C and D − CA−1B play a special role and suggest the following definition:
Definition 43.1. Given any n × n block matrix of the form
M =

C D
A B
,
where A is a p × p matrix and D is a q × q matrix, with n = p + q (so B is a p × q matrix
and C is a q × p matrix), if D is invertible, then the matrix A − BD−1C is called the Schur
complement of D in M. If A is invertible, then the matrix D − CA−1B is called the Schur
complement of A in M.
The above equations written as
x = (A − BD−1C)
−1
c − (A − BD−1C)
−1BD−1
d,
y = −D
−1C(A − BD−1C)
−1
c
+ (D
−1 + D
−1C(A − BD−1C)
−1BD−1
)d,
yield a formula for the inverse of M in terms of the Schur complement of D in M, namely

C D
A B −1
=

(A − BD−1C)
−1 −(A − BD−1C)
−1BD−1
−D−1C(A − BD−1C)
−1 D−1 + D−1C(A − BD−1C)
−1BD−1

.
A moment of reflection reveals that

C D
A B −1
=

(A − BD−1C)
−1 0
−D−1C(A − BD−1C)
−1 D−1
 
I −BD−1
0 I

,
and then

C D
A B −1
=

I 0
−D−1C I 
(A − BD−1C)
−1 0
0 D−1
 
I −BD−1
0 I

.
By taking inverses, we obtain the following result.
43.1. SCHUR COMPLEMENTS 1531
Proposition 43.1. If the matrix D is invertible, then

C D
A B
=

I BD−1
0 I
 
A − BD−1C 0
0 D
 
I 0
D−1C I .
The above expression can be checked directly and has the advantage of requiring only
the invertibility of D.
Remark: If A is invertible, then we can use the Schur complement D − CA−1B of A to
obtain the following factorization of M:

C D
A B
=

I 0
CA−1
I
 
A
0 D − CA
0
−1B
 
I A−1B
0 I

.
If D − CA−1B is invertible, we can invert all three matrices above, and we get another
formula for the inverse of M in terms of (D − CA−1B), namely,

C D
A B −1
=

A−1 + A−1B(D − CA−1B)
−1CA−1 −A−1B(D − CA−1B)
−1
−(D − CA−1B)
−1CA−1
(D − CA−1B)
−1

.
If A, D and both Schur complements A − BD−1C and D − CA−1B are all invertible, by
comparing the two expressions for M−1
, we get the (non-obvious) formula
(A − BD−1C)
−1 = A
−1 + A
−1B(D − CA−1B)
−1CA−1
.
Using this formula, we obtain another expression for the inverse of M involving the Schur
complements of A and D (see Horn and Johnson [95]):
Proposition 43.2. If A, D and both Schur complements A − BD−1C and D − CA−1B are
all invertible, then

C D
A B −1
=

(A − BD−1C)
−1 −A−1B(D − CA−1B)
−1
−(D − CA−1B)
−1CA−1
(D − CA−1B)
−1

.
If we set D = I and change B to −B, we get
(A + BC)
−1 = A
−1 − A
−1B(I − CA−1B)
−1CA−1
,
a formula known as the matrix inversion lemma (see Boyd and Vandenberghe [29], Appendix
C.4, especially C.4.3).
1532 CHAPTER 43. SCHUR COMPLEMENTS AND APPLICATIONS
43.2 Symmetric Positive Definite Matrices and
Schur Complements
If we assume that our block matrix M is symmetric, so that A, D are symmetric and C = B> ,
then we see by Proposition 43.1 that M is expressed as
M =

B
A B
>
D

=

I BD−1
0 I
 
A − BD−1B> 0
0 D
 
I BD−1
0 I

>
,
which shows that M is similar to a block diagonal matrix (obviously, the Schur complement,
A − BD−1B> , is symmetric). As a consequence, we have the following version of “Schur’s
trick” to check whether M  0 for a symmetric matrix.
Proposition 43.3. For any symmetric matrix M of the form
M =

B
A B
>
C

,
if C is invertible, then the following properties hold:
(1) M  0 iff C  0 and A − BC−1B>  0.
(2) If C  0, then M  0 iff A − BC−1B>  0.
Proof. (1) Since C is invertible, we have
M =

B
A B
>
C

=

I BC−1
0 I
 
A − BC−1B> 0
0 C
 
I BC−1
0 I

>
. (∗)
Observe that

I BC−1
0 I

−1
=

I −BC−1
0 I

,
so (∗) yields

I −BC−1
0 I
  B
A B
>
C
 
I −BC−1
0 I

>
=

A − BC−1B> 0
0 C

,
and we know that for any symmetric matrix T, here T = M, and any invertible matrix N,
here
N =

I −BC−1
0 I

,
the matrix T is positive definite (T  0) iff NT N > (which is obviously symmetric) is positive
definite (NT N >  0). But a block diagonal matrix is positive definite iff each diagonal block
is positive definite, which concludes the proof.
(2) This is because for any symmetric matrix T and any invertible matrix N, we have
T  0 iff NT N >  0.
43.3. SP SEMIDEFINITE MATRICES AND SCHUR COMPLEMENTS 1533
Another version of Proposition 43.3 using the Schur complement of A instead of the
Schur complement of C also holds. The proof uses the factorization of M using the Schur
complement of A (see Section 43.1).
Proposition 43.4. For any symmetric matrix M of the form
M =

B
A B
>
C

,
if A is invertible then the following properties hold:
(1) M  0 iff A  0 and C − B> A−1B  0.
(2) If A  0, then M  0 iff C − B> A−1B  0.
Here is an illustration of Proposition 43.4(2). Consider the nonlinear quadratic constraint
(Ax + b)
> (Ax + b) ≤ c
> x + d,
were A ∈ Mn(R), x, b, c ∈ R
n and d ∈ R. Since obviously I = In is invertible and I  0, we
have

(Ax
I Ax
+ b)
> c
> x
+
+
b
d


0
iff c
> x + d − (Ax + b)
> (Ax + b)  0 iff (Ax + b)
> (Ax + b) ≤ c
> x + d, since the matrix (a
scalar) c
> x + d − (Ax + b)
> (Ax + b) is the Schur complement of I in the above matrix.
The trick of using Schur complements to convert nonlinear inequality constraints into
linear constraints on symmetric matrices involving the semidefinite ordering  is used exten￾sively to convert nonlinear problems into semidefinite programs; see Boyd and Vandenberghe
[29].
When C is singular (or A is singular), it is still possible to characterize when a symmetric
matrix M as above is positive semidefinite, but this requires using a version of the Schur
complement involving the pseudo-inverse of C, namely A − BC+B> (or the Schur comple￾ment, C − B> A+B, of A). We use the criterion of Proposition 42.5, which tells us when a
quadratic function of the form 1
2
x
> P x − x
> b has a minimum and what this optimum value
is (where P is a symmetric matrix).
43.3 Symmetric Positive Semidefinite Matrices and
Schur Complements
We now return to our original problem, characterizing when a symmetric matrix
M =

B
A B
>
C

1534 CHAPTER 43. SCHUR COMPLEMENTS AND APPLICATIONS
is positive semidefinite. Thus, we want to know when the function
f(x, y) = (x
> y
> )

B
A B
>
C
 
x
y

= x
> Ax + 2x
> By + y
> Cy
has a minimum with respect to both x and y. If we hold y constant, Proposition 42.5 implies
that f(x, y) has a minimum iff A  0 and (I − AA+)By = 0, and then the minimum value
is
f(x
∗
, y) = −y
> B
> A
+By + y
> Cy = y
> (C − B
> A
+B)y.
Since we want f(x, y) to be uniformly bounded from below for all x, y, we must have (I −
AA+)B = 0. Now f(x
∗
, y) has a minimum iff C − B> A+B  0. Therefore, we have
established that f(x, y) has a minimum over all x, y iff
A  0, (I − AA+)B = 0, C − B
> A
+B  0.
Similar reasoning applies if we first minimize with respect to y and then with respect to x,
but this time, the Schur complement A − BC+B> of C is involved. Putting all these facts
together, we get our main result:
Theorem 43.5. Given any symmetric matrix
M =

B
A B
>
C

the following conditions are equivalent:
(1) M  0 (M is positive semidefinite).
(2) A  0, (I − AA+)B = 0, C − B> A+B  0.
(3) C  0, (I − CC+)B> = 0, A − BC+B>  0.
If M  0 as in Theorem 43.5, then it is easy to check that we have the following
factorizations (using the fact that A+AA+ = A+ and C
+CC+ = C
+):

B
A B
>
C

=

I BC+
0 I
 
A − BC+B> 0
0 C
  C
+
I
B> I
0

and

B
A B
>
C

=

B>
I
A+ I
0
  A
0 C − B
0
>
A+B
 
I A+B
0 I

.
43.4. SUMMARY 1535
43.4 Summary
The main concepts and results of this chapter are listed below:
• Schur complements.
• The matrix inversion lemma.
• Symmetric positive definite matrices and Schur complements.
• Symmetric positive semidefinite matrices and Schur complements.
43.5 Problems
Problem 43.1. Prove that maximizing the function g(λ) given by
g(λ) = c0 + λc1 − (b0 + λb1)
> (A0 + λA1)
+(b0 + λb1),
subject to
A0 + λA1  0, b0 + λb1 ∈ range(A0 + λA1),
with A0, A1 some n×n symmetric positive semidefinite matrices, b0, b1 ∈ R
n
, and c0, c1 ∈ R,
is equivalent to maximizing γ subject to the constraints
λ ≥ 0

A0 + λA1 b0 + λb1
(b0 + λb1)
> c0 + λc1 − γ


0.
Problem 43.2. Let a1, . . . , am be m vectors in R
n and assume that they span R
n
.
(1) Prove that the matrix
mX
k=1
aka
>k
is symmetric positive definite.
(2) Define the matrix X by
X =
 
mX
k=1
aka
>k
!
−1
.
Prove that
 
P
m
k=1 aka
>k
ai
a
>i
1
!

0, i = 1, . . . , m.
Deduce that
a
>i Xai ≤ 1, 1 ≤ i ≤ m.
1536 CHAPTER 43. SCHUR COMPLEMENTS AND APPLICATIONS
Problem 43.3. Consider the function g of Example 39.11 defined by
g(a, b, c) = log(ac − b
2
),
where ac − b
2 > 0. We found that the Hessian matrix of g is given by
Hg(a, b, c) = 1
(ac − b
2
)
2


−c
2 2bc −b
2
2bc −2(b
2 + ac) 2ab
−b
2 2ab −a
2

 .
Use the Schur complement (of a
2
) to prove that the matrix −Hg(a, b, c) is symmetric
positive definite if ac − b
2 > 0 and a, c > 0.
Part VII
Linear Optimization
1537
Chapter 44
Convex Sets, Cones, H-Polyhedra
44.1 What is Linear Programming?
What is linear programming? At first glance, one might think that this is some style of com￾puter programming. After all, there is imperative programming, functional programming,
object-oriented programming, etc. The term linear programming is somewhat misleading,
because it really refers to a method for planning with linear constraints, or more accurately,
an optimization method where both the objective function and the constraints are linear.1
Linear programming was created in the late 1940’s, one of the key players being George
Dantzing, who invented the simplex algorithm. Kantorovitch also did some pioneering work
on linear programming as early as 1939. The term linear programming has a military con￾notation because in the early 1950’s it was used as a synonym for plans or schedules for
training troops, logistical supply, resource allocation, etc. Unfortunately the term linear
programming is well established and we are stuck with it.
Interestingly, even though originally most applications of linear programming were in
the field of economics and industrial engineering, linear programming has become an im￾portant tool in theoretical computer science and in the theory of algorithms. Indeed, linear
programming is often an effective tool for designing approximation algorithms to solve hard
problems (typically NP-hard problems). Linear programming is also the “baby version” of
convex programming, a very effective methodology which has received much attention in
recent years.
Our goal is to present the mathematical underpinnings of linear programming, in par￾ticular the existence of an optimal solution if a linear program is feasible and bounded, and
the duality theorem in linear programming, one of the deepest results in this field. The
duality theorem in linear programming also has significant algorithmic implications but we
do not discuss this here. We present the simplex algorithm, the dual simplex algorithm, and
the primal dual algorithm. We also describe the tableau formalism for running the simplex
1Again, we witness another unfortunate abuse of terminology; the constraints are in fact affine.
1539
1540 CHAPTER 44. CONVEX SETS, CONES, H-POLYHEDRA
algorithm and its variants. A particularly nice feature of the tableau formalism is that the
update of a tableau can be performed using elementary row operations identical to the op￾erations used during the reduction of a matrix to row reduced echelon form (rref). What
differs is the criterion for the choice of the pivot.
However, we do not discuss other methods such as the ellipsoid method or interior points
methods. For these more algorithmic issues, we refer the reader to standard texts on linear
programming. In our opinion, one of the clearest (and among the most concise!) is Matousek
and Gardner [123]; Chvatal [40] and Schrijver [148] are classics. Papadimitriou and Steiglitz
[134] offers a very crisp presentation in the broader context of combinatorial optimization,
and Bertsimas and Tsitsiklis [21] and Vanderbei [181] are very complete.
Linear programming has to do with maximizing a linear cost function c1x1 + · · · + cnxn
with respect to m “linear” inequalities of the form
ai1x1 + · · · + ainxn ≤ bi
.
These constraints can be put together into an m × n matrix A = (aij ), and written more
concisely as
Ax ≤ b.
For technical reasons that will appear clearer later on, it is often preferable to add the
nonnegativity constaints xi ≥ 0 for i = 1, . . . , n. We write x ≥ 0. It is easy to show that
every linear program is equivalent to another one satisfying the constraints x ≥ 0, at the
expense of adding new variables that are also constrained to be nonnegative. Let P(A, b) be
the set of feasible solutions of our linear program given by
P(A, b) = {x ∈ R
n
| Ax ≤ b, x ≥ 0}.
Then there are two basic questions:
(1) Is P(A, b) nonempty, that is, does our linear program have a chance to have a solution?
(2) Does the objective function c1x1 + · · · + cnxn have a maximum value on P(A, b)?
The answer to both questions can be no. But if P(A, b) is nonempty and if the objective
function is bounded above (on P(A, b)), then it can be shown that the maximum of c1x1 +
· · ·
Perhaps surprisingly, this result is not so easy to prove (unless one has the simplex method
+ cnxn is achieved by some x ∈ P(A, b). Such a solution is called an optimal solution.
at his disposal). We will prove this result in full detail (see Proposition 45.1).
The reason why linear constraints are so important is that the domain of potential optimal
solutions P(A, b) is convex . In fact, P(A, b) is a convex polyhedron which is the intersection
of half-spaces cut out by affine hyperplanes. The objective function being linear is convex,
and this is also a crucial fact. Thus, we are led to study convex sets, in particular those that
arise from solutions of inequalities defined by affine forms, but also convex cones.
44.2. AFFINE SUBSETS, CONVEX SETS, HYPERPLANES, HALF-SPACES 1541
We give a brief introduction to these topics. As a reward, we provide several criteria for
testing whether a system of inequalities
Ax ≤ b, x ≥ 0
has a solution or not in terms of versions of the Farkas lemma (see Proposition 50.3 and
Proposition 47.4). Then we give a complete proof of the strong duality theorem for linear
programming (see Theorem 47.7). We also discuss the complementary slackness conditions
and show that they can be exploited to design an algorithm for solving a linear program
that uses both the primal problem and its dual. This algorithm known as the primal dual
algorithm, although not used much nowadays, has been the source of inspiration for a whole
class of approximation algorithms also known as primal dual algorithms.
We hope that this chapter and the next three will be a motivation for learning more
about linear programming, convex optimization, but also convex geometry. The “bible” in
convex optimization is Boyd and Vandenberghe [29], and one of the best sources for convex
geometry is Ziegler [195]. This is a rather advanced text, so the reader may want to begin
with Gallier [73].
44.2 Affine Subsets, Convex Sets, Affine Hyperplanes,
Half-Spaces
We view R
n as consisting of column vectors (n×1 matrices). As usual, row vectors represent
linear forms, that is linear maps ϕ: R
n → R, in the sense that the row vector y (a 1 × n
matrix) represents the linear form ϕ if ϕ(x) = yx for all x ∈ R
n
. We denote the space of
linear forms (row vectors) by (R
n
)
∗
.
Recall that a linear combination of vectors in R
n
is an expression
λ1x1 + · · · + λmxm
where x1, . . . , xm ∈ R
n and where λ1, . . . , λm are arbitrary scalars in R. Given a sequence of
vectors S = (x1, . . . , xm) with xi ∈ R
n
, the set of all linear combinations of the vectors in S is
the smallest (linear) subspace containing S called the linear span of S, and denoted span(S).
A linear subspace of R
n
is any nonempty subset of R
n
closed under linear combinations.
Definition 44.1. An affine combination of vectors in R
n
is an expression
λ1x1 + · · · + λmxm
where x1, . . . , xm ∈ R
n and where λ1, . . . , λm are scalars in R satisfying the condition
λ1 + · · · + λm = 1.
Given a sequence of vectors S = (x1, . . . , xm) with xi ∈ R
n
