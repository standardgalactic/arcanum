Adv
an
tages
of
Decision
Lists
and
Implicit
Negativ
es
in
Inductiv
e
Logic
Programming

Mary
Elaine
Cali
and
Ra
ymond
J.
Mo
oney
Departmen
t
of
Computer
Sciences
Univ
ersit
y
of
T
exas
at
Austin
Austin,
TX

fmecali,m
o
oneyg@cs.utexas.edu
Jan
uary
0,
		
Abstract
This
pap
er
demonstrates
the
capabilities
of
F
oidl,
an
inductiv
e
logic
programming
(ILP)
system
whose
distinguishin
g
c
haracteristics
are
the
abilit
y
to
pro
duce
rst-order
decision
lists,
the
use
of
an
output
completeness
assumption
to
pro
vide
implicit
negativ
e
examples,
and
the
use
of
in
tensional
bac
kground
kno
wledge.
The
dev
elopmen
t
of
F
oidl
w
as
originally
motiv
ated
b
y
the
problem
of
learning
to
generate
the
past
tense
of
English
v
erbs;
ho
w
ev
er,
this
pap
er
demonstrates
its
sup
erior
p
erformance
on
t
w
o
dieren
t
sets
of
b
enc
hmark
ILP
problems.
T
ests
on
the
nite
elemen
t
mesh
design
problem
sho
w
that
F
oidl's
decision
lists
enable
it
to
pro
duce
b
etter
results
than
all
other
ILP
systems
whose
results
on
this
problem
ha
v
e
b
een
rep
orted.
T
ests
with
a
selection
of
list-pro
cessing
problems
from
Bratk
o's
in
tro
ductory
Prolog
text
demonstrate
that
the
com
bination
of
implicit
negativ
es
and
in
tensionalit
y
allo
w
F
oidl
to
learn
correct
programs
from
far
few
er
examples
than
F
oil.
Keyw
ords:
inductive
lo
gic
pr
o
gr
amming

In
tro
duction
New
mac
hine
learning
algorithms
are
often
motiv
ated
b
y
the
diculties
of
solving
new
prob-
lems
with
existings
algorithms.
While
this
can
b
e
an
imp
ortan
t
source
of
progress,
it
could
lead
to
a
proliferation
of
o
v
erly
sp
ecialized
systems
designed
to
handle
a
narro
w
range
of
learning
problems.
Therefore,
it
is
imp
ortan
t
to
test
new
systems
with
problems
other
than
those
whic
h
originally
motiv
ated
the
systems'
dev
elopmen
t.

This
researc
h
w
as
supp
orted
b
y
a
fello
wship
from
A
T&T
a
w
arded
to
the
rst
author
and
b
y
the
National
Science
F
oundation
under
gran
t
IRI-	0	.


Mo
oney
and
Cali
(		)
in
tro
duced
F
oidl,
a
new
inductiv
e
logic
programming
(ILP)
system
motiv
ated
b
y
problems
with
applying
previously
existing
ILP
tec
hniques
to
the
prob-
lem
of
generating
the
past
tense
of
English
v
erbs
from
the
base
form
of
the
v
erb.
F
oidl
is
based
on
F
oil
(Quinlan,
		0)
but
has
three
distinguishing
c
haracteristics:
)
it
uses
in
tensional
bac
kground
kno
wledge;
)
it
a
v
oids
the
need
for
explicit
negativ
e
examples
b
y
using
an
output
completeness
assumption
to
create
implici
t
negativ
es;
and
)
it
is
able
to
create
rst-order
decision
lists
(ordered
lists
of
clauses,
eac
h
ending
in
a
cut).
These
c
har-
acteristics
allo
w
F
oidl
to
p
erform
w
ell
on
the
past
tense
problem,
b
ettering
all
previous
results
(Mo
oney
&
Cali,
		).
In
this
pap
er,
w
e
presen
t
results
that
sho
w
that
the
F
oidl
algorithm
is
useful
for
more
than
the
problem
for
whic
h
it
w
as
originally
designed.
W
e
ha
v
e
tested
the
system
on
t
w
o
standard
ILP
problems:
the
nite
elemen
t
mesh
design
in
tro
duced
b
y
Dolsak
and
Mug-
gleton
(		)
and
a
selection
of
the
list-pro
cessing
programs
from
Bratk
o
(		0)
used
b
y
Quinlan
and
Cameron-Jones
(		).
W
e
compare
F
oidl's
p
erformance
to
F
oil
and
to
FF
oil
(Quinlan,
submitted),
a
v
ersion
of
F
oil
whic
h
learns
single-output
functions.
The
rst
order
decision
lists
enable
F
oidl
to
ac
hiev
er
b
etter
accuracy
on
the
nite
elemen
t
mesh
design
problem
than
has
previously
b
een
rep
orted
for
an
ILP
system,
and
F
oidl's
in
tension-
alit
y
and
use
of
implicit
negativ
es
allo
w
it
to
learn
correct
programs
for
the
list-pro
cessing
examples
from
small
sets
of
randomly
selected
examples.
The
remainder
of
the
pap
er
is
organized
as
follo
ws.
Section

pro
vides
bac
kground
on
F
oil
and
FF
oil.
Section

summarizes
the
F
oidl
algorithm.
Section

presen
ts
our
results
on
the
nite
elemen
t
mesh
design
and
list-pro
cessing
problems.
Section

discusses
related
w
ork,
and
Section

summarizes
and
presen
ts
our
conclusions.

Bac
kground
.
F
OIL
Since
F
oidl
is
based
on
F
oil,
w
e
presen
t
a
brief
review
of
this
imp
ortan
t
ILP
system;
see
articles
on
F
oil
for
a
more
complete
description
(Quinlan,
		0;
Quinlan
&
Cameron-Jones,
		;
Cameron-Jones
&
Quinlan,
		).

F
oil
learns
a
function-free,
rst-order,
Horn-
clause
denition
of
a
tar
get
predicate
in
terms
of
itself
and
other
b
ackgr
ound
predicates.
The
input
consists
of
extensional
denitions
of
these
predicates
as
tuples
of
constan
ts
of
sp
ecied
t
yp
es.
F
oil
also
requires
negativ
e
examples
of
the
target
concept,
whic
h
can
b
e
supplied
directly
or
computed
using
a
closed-w
orld
assumption.
Giv
en
this
input,
F
oil
learns
a
program
one
clause
at
a
time
using
a
greedy-co
v
ering
algorithm
that
can
b
e
summarized
as
follo
ws:
Let
p
ositives-to-c
over
=
p
ositive
examples.
While
p
ositives-to-c
over
is
not
empt
y
Find
a
clause,
C
,
that
covers
a
p
referably
la
rge
subset
of
p
ositive
s-to-c
over
but
covers
no
negative
examples.

F
oil
is
also
a
v
ailable
b
y
anon
ymous
FTP
from
ftp.cs.su.oz.au
in
the
le
pub/foil.sh.


Add
C
to
the
developing
denition.
Remove
examples
covered
b
y
C
from
p
ositives-to-c
over.
The
\nd
a
clause"
step
is
implem
en
ted
b
y
a
general-to-sp
ecic
hill-clim
bing
searc
h
that
adds
an
teceden
ts
to
the
dev
eloping
clause
one
at
a
time.
A
t
eac
h
step,
it
ev
aluates
p
ossible
literals
that
migh
t
b
e
added
and
selects
one
that
maximiz
es
an
information-gain
heuristic.
The
algorithm
main
tains
a
set
of
tuples
that
satisfy
the
curren
t
clause
and
includes
bindings
for
an
y
new
v
ariables
in
tro
duced
in
the
b
o
dy
.
The
gain
metric
ev
aluates
literals
based
on
the
n
um
b
er
of
p
ositiv
e
and
negativ
e
tuples
co
v
ered,
preferring
literals
that
co
v
er
man
y
p
ositiv
es
and
few
negativ
es.
The
pap
ers
referenced
ab
o
v
e
pro
vide
details
and
information
on
additional
features.
.
FF
OIL
FF
oil
is
a
descendan
t
of
F
oil
with
mo
dications(Quinlan,
submitted),
somewhat
similar
to
F
oidl's,
that
sp
ecialize
it
for
learning
functional
relations.

First,
FF
oil
assumes
that
the
nal
argumen
t
of
the
relation
is
an
output
ar
gument
and
that
the
other
argumen
ts
of
the
relation
uniquely
determine
the
output
argumen
t.
This
assumption
is
used
to
pro
vide
implicit
negativ
e
examples:
eac
h
p
ositiv
e
example
under
consideration
whose
output
v
ariable
is
not
b
ound
b
y
the
clause
under
construction
is
considered
to
represen
t
one
p
ositiv
e
and
r
-

negativ
es,
where
r
is
the
n
um
b
er
of
constan
ts
in
the
range
of
the
function.
Second,
FF
oil
assumes
that
eac
h
clause
will
end
in
a
cut,
so
that
previously
co
v
ered
examples
can
b
e
safely
ignored
in
the
construction
of
subsequen
t
clauses.
Th
us,
FF
oil,
lik
e
F
oidl,
constructs
rst-order
decision
lists,
though
it
constructs
the
clauses
in
the
same
order
as
they
app
ear
in
the
program,
while
F
oidl
constructs
its
clauses
in
the
rev
erse
order.

The
F
OIDL
Algorithm
F
oidl
adds
three
ma
jor
features
to
F
oil:
)
In
tensional
sp
ecication
of
bac
kground
kno
wl-
edge,
)
Output
completeness
as
a
substitute
for
explicit
negativ
e
examples,
and
)
Supp
ort
for
learning
rst-order
decision
lists.
W
e
no
w
describ
e
the
mo
dications
made
to
incorp
orate
these
features.
As
describ
ed
ab
o
v
e,
F
oil
assumes
bac
kground
predicates
are
pro
vided
with
extensional
denitions;
ho
w
ev
er,
this
is
burdensome
and
frequen
tly
in
tractable.
Pro
viding
an
in
ten-
sional
denition
in
the
form
of
general
Prolog
clauses
is
generally
preferable.
In
tensional
bac
kground
denitions
are
not
restricted
to
function-free
pure
Prolog
and
can
exploit
all
features
of
the
language.
Mo
difying
F
oil
to
use
in
tensional
bac
kground
is
straigh
tforw
ard.
Instead
of
matc
hing
a
literal
against
a
set
of
tuples
to
determine
whether
or
not
it
co
v
ers
an
example,
the
Prolog
in
terpreter
is
used
in
an
attempt
to
pro
v
e
that
the
literal
can
b
e
satised
using
the
in
tensional
denitions.
Unlik
e
F
oil,
F
oidl
do
es
not
main
tain
expanded
tuples;
p
ositiv
e
and

The
dev
elopmen
t
of
FF
oil
w
as
partially
motiv
ated
b
y
our
w
ork
on
F
oidl.


negativ
e
examples
of
the
target
concept
are
repro
v
ed
for
eac
h
alternativ
e
sp
ecialization
of
the
dev
eloping
clause.
Learning
without
explicit
negativ
es
requires
an
alternate
metho
d
of
ev
aluating
the
utilit
y
of
a
clause.
In
F
oidl
a
mo
de
declaration
and
an
assumption
of
output
c
ompleteness
(that
for
ev
ery
unique
input
pattern
app
earing
in
the
training
set,
the
training
set
includes
all
p
ositiv
e
examples
with
that
input
pattern)
together
determine
a
set
of
implici
t
negativ
e
examples.
Consider
the
predicate,
last(Elemen
t,
Lis
t)
whic
h
holds
when
Element
is
the
last
el-
emen
t
of
List.
Pro
viding
the
mo
de
declaration
last(-,+)
indicates
that
the
predicate
should
pro
vide
the
correct
nal
elemen
t
when
a
list,
the
standard
denition
of
last/.
Since
the
nal
elemen
t
of
a
giv
en
list
is
unique,
an
y
set
of
p
ositiv
e
examples
of
this
predicate
will
b
e
output
complete.
Ho
w
ev
er,
output
completeness
can
also
b
e
applied
to
non-functional
cases
suc
h
as
append(-,
-,
+),
meaning
that
all
p
ossible
pairs
of
lists
that
can
b
e
app
ended
together
to
pro
duce
a
list
are
included
in
the
training
set
(e.g.
append([]
,[
a,b
],[
a,b
])
,
append([a]
,[b
],
[a,
b])
,
append([a,
b],
[],
[a,b])).
Giv
en
an
output
completeness
assumption,
determining
whether
a
clause
is
o
v
erly-general
is
straigh
tforw
ard.
F
or
eac
h
p
ositiv
e
example,
an
output
query
is
made
to
determine
all
out-
puts
for
the
giv
en
input
(e.g.
last([a,c,
b],
X)).
If
an
y
outputs
are
generated
that
are
not
p
ositiv
e
examples,
the
clause
still
co
v
ers
negativ
e
examples
and
requires
further
sp
ecializa-
tion.
In
addition,
in
order
to
compute
the
gain
of
alternativ
e
literals
during
sp
ecialization,
the
negativ
e
co
v
erage
of
a
clause
needs
to
b
e
quan
tied.
Eac
h
ground,
incorrect
answ
er
to
an
output
query
clearly
coun
ts
as
a
single
negativ
e
example
(e.g.
last([a,c,
b],
[a])).
Ho
w
ev
er,
output
queries
will
frequen
tly
pro
duce
answ
ers
with
univ
ersally
quan
tied
v
ari-
ables.
F
or
example,
giv
en
the
o
v
erly-general
clause
last(A,B)
:-
append(C,D,
A).
,
the
query
last([a,c
,b]
,
X)
generates
the
answ
er
last([a,c,
t],
Y).
This
implicitl
y
repre-
sen
ts
co
v
erage
of
an
innite
n
um
b
er
of
negativ
e
examples.
In
order
to
quan
tify
negativ
e
co
v
erage,
F
oidl
uses
a
parameter
u
to
represen
t
the
total
n
um
b
er
of
p
ossible
terms
in
the
univ
erse.
The
negativ
e
co
v
erage
represen
ted
b
y
a
non-ground
answ
er
to
an
output
query
is
then
estimated
as
u
v
 p,
where
v
is
the
n
um
b
er
of
v
ariable
argumen
ts
in
the
answ
er
and
p
is
the
n
um
b
er
of
p
ositiv
e
examples
with
whic
h
the
answ
er
unies.
The
u
v
term
stands
for
the
n
um
b
er
of
unique
ground
outputs
represen
ted
b
y
the
answ
er
(e.g.
the
answ
er
append(X,Y,
[a,
b])
stands
for
u

dieren
t
ground
outputs)
and
the
p
term
stands
for
the
n
um
b
er
of
these
that
represen
t
p
ositiv
e
examples.
This
allo
ws
F
oidl
to
quan
tify
co
v
erage
of
large
n
um
b
ers
of
implicit
negativ
e
examples
without
ev
er
explicitly
constructing
them.
Unfortunately
,
this
estimate
is
not
sensitiv
e
enough.
F
or
example,
in
the
past
tense
domain
b
oth
clauses
past(A,B)
:-
split(A,C,D
).
past(A,B)
:-
split(B,A,C
).
where
split(A,B,C)
is
equiv
alen
t
to
app
end(B,C,A)
when
B
and
C
are
non-empt
y
lists
co
v
er
u
implicit
negativ
e
examples
for
the
output
query
past([a,c,
t],
X)
since
the
rst
pro
duces
the
answ
er
past([a,c
,t]
,
Y)
and
the
second
pro
duces
the
answ
er
past([a,c,t
],
[a,c,t
|
Y]).
Ho
w
ev
er,
the
second
clause
is
clearly
b
etter
since
it
at
least
requires
the
output
to


b
e
the
input
with
some
sux
added.
Since
there
are
presumably
more
w
ords
than
there
are
w
ords
that
start
with
\a-c-t"
(assuming
the
total
n
um
b
er
of
w
ords
is
nite),
the
rst
clause
should
b
e
considered
to
co
v
er
more
negativ
e
examples.
Therefore,
argumen
ts
that
are
partially
instan
tiated,
suc
h
as
[a,c,t
|
Y],
are
coun
ted
as
only
a
fraction
of
a
v
ariable
when
calculating
v
.
Sp
ecically
,
a
partially
instan
tiated
output
argumen
t
is
scored
as
the
fraction
of
its
subterms
that
are
v
ariables,
e.g.
[a,c,t
|
Y]
coun
ts
as
only
=
of
a
v
ariable
argumen
t.
Therefore,
the
rst
clause
ab
o
v
e
is
scored
as
co
v
ering
u
implicit
negativ
es
and
the
second
as
co
v
ering
only
u
=
.
Giv
en
reasonable
v
alues
for
u
and
the
n
um
b
er
of
p
ositiv
es
co
v
ered
b
y
eac
h
clause,
the
literal
split(B,A
,C)
will
b
e
preferred.
The
algorithm
incorp
orating
in
tensional
bac
kground
kno
wledge
and
implicit
negativ
es
is:
Initialize
C
to
R(V

;
V

;
:::;
V
k
)
:-.
where
R
is
the
ta
rget
p
redicate
with
a
rit
y
k
.
Initialize
T
to
contain
the
examples
in
p
ositives-to-c
over
and
output
queries
fo
r
all
p
ositive
examples.
While
T
contains
output
queries
Find
the
b
est
literal
L
to
add
to
the
clause.
Let
T
0
b
e
the
subset
of
p
ositive
examples
in
T
that
can
still
b
e
p
rove
d
as
instances
of
the
ta
rget
concept
using
the
sp
ecialized
clause,
plus
the
output
querie
s
in
T
that
still
p
ro
duce
inco
rrect
answ
ers.
Replace
T
b
y
T
0
.
Since
expanded
tuples
are
not
pro
duced,
the
information-gain
heuristic
for
pic
king
the
b
est
literal
is
simply:
g
ain(L)
=
jT
0
j

(I
(T
)
 I
(T
0
)):
()
jT
j
is
computed
as
the
n
um
b
er
of
p
ositiv
e
examples
in
T
plus
the
sum
of
the
n
um
b
er
of
implicit
negativ
es
co
v
ered
b
y
eac
h
output
query
in
T
.
This
is
the
algorithm
for
IF
oil
(In
tensional
F
oil),
whic
h
is
simply
F
oidl
with
the
decision
list
feature
turned
o,
making
the
system
useful
for
non-functional
relations.
F
oidl's
nal
feature
is
that
it
can
pro
duce
rst-order
decision
lists.
As
describ
ed
ab
o
v
e,
these
are
ordered
sets
of
clauses
eac
h
ending
in
a
cut.
When
answ
ering
an
output
query
,
the
cuts
simply
eliminate
all
but
the
rst
answ
er
pro
duced
when
trying
the
clauses
in
order.
Therefore,
this
represen
tation
is
similar
to
prop
ositional
decision
lists
(Riv
est,
	),
whic
h
are
ordered
lists
of
pairs
(rules)
of
the
form
(t
i
;
c
i
)
where
the
test
t
i
is
a
conjunction
of
features
and
c
i
is
a
category
lab
el
and
an
example
is
assigned
to
the
category
of
the
rst
pair
whose
test
it
satises.
In
the
original
algorithm
of
Riv
est
(	)
and
in
CN
(Clark
&
Niblett,
		),
rules
are
learned
in
the
order
they
app
ear
in
the
nal
decision
list
(i.e.
new
rules
are
app
ended
to
the
end
of
the
list
as
they
are
learned).
Ho
w
ev
er,
W
ebb
and
Brki

c
(		)
argue
for
learning
decision
lists
in
the
rev
erse
order
since
most
preference
functions
tend
to
learn
more
general
rules
rst,
and
these
are
b
est
p
ositioned
as
default
cases
to
w
ards
the
end.
They
in
tro
duce
an
algorithm,
pr
ep
end,
that
learns
decision
lists
in
rev
erse
order
and
presen
t
results
indicating
that
in
most
cases
it
learns
simpler
decision
lists
with
sup
erior
predictiv
e
accuracy
.
F
oidl
can
b
e
seen
as
generalizing
pr
ep
end
to
the
rst-order
case
for
target
predicates
represen
ting
functions.


The
resulting
clause-sp
ecialization
algorithm
can
no
w
b
e
summarized
as
follo
ws:
Initialize
C
to
R(V

;
V

;
:::;
V
k
)
:-.
where
R
is
the
ta
rget
p
redicate
with
a
rit
y
k
.
Initialize
T
to
contain
the
examples
in
p
ositives-to-c
over
and
output
queries
fo
r
all
p
ositive
examples.
While
T
contains
output
queries
Find
the
b
est
literal
L
to
add
to
the
clause.
Let
T
0
b
e
the
subset
of
p
ositive
examples
in
T
whose
output
query
still
p
ro
duces
a
rst
answ
er
that
unies
with
the
co
rrect
answ
er,
plus
the
output
queries
in
T
that
either
)
Pro
duce
a
non-ground
rst
answ
er
that
unies
with
the
co
rrect
answ
er,
o
r
)
Pro
duce
an
inco
rrect
answ
er
but
p
ro
duce
a
co
rrect
answ
er
using
a
p
reviously
lea
rned
clause.
Replace
T
b
y
T
0
.
T
o
handle
the
list-pro
cessing
tasks,
F
oidl
requires
t
w
o
features
that
w
ere
unnecessary
for
the
past-tense
task:
recursion
and
determinate
literals.
Both
of
these
are
features
of
F
oil,
but
the
c
hange
from
an
extensional
system
to
an
in
tensional
one
requires
that
the
features
b
e
implem
e
n
ted
somewhat
dieren
tly
.
T
o
handle
recursion,
F
oidl
b
egins
b
y
adding
all
of
the
p
ositiv
e
examples
in
the
training
set
to
the
curren
t
theory
.
When
a
new
clause
is
b
eing
created,
b
efore
eac
h
example
is
pro
v
ed
with
the
new
clause,
the
example
is
temp
orarily
remo
v
ed
from
the
theory
so
that
it
can't
b
e
used
to
pro
v
e
itself.
Once
the
clause
is
complete,
F
oidl
greedily
remo
v
es
examples
from
the
theory
p
ermanen
tly
.
F
oidl
a
v
oids
innitely
recursiv
e
clauses
b
y
using
a
depth
b
ound
when
pro
ving
examples.
Determinate
literals
are
literals
whic
h
ma
y
ha
v
e
little
or
no
gain,
but
add
at
least
one
new
v
ariable
whic
h
can
only
tak
e
on
exactly
one
v
alue
for
eac
h
p
ositiv
e
tuple
and
and
no
more
than
one
v
alue
for
eac
h
negativ
e
tuple
when
extensional
tuples
are
main
tained
as
in
F
oil(Cameron-Jones
&
Quinlan,
		).
Since
F
oidl
do
es
not
main
tain
the
extensional
tuples,
it
m
ust
recognize
that
there
is
exactly
one
p
ossible
pro
of
of
the
literal
giv
en
the
bindings
pro
duced
b
y
the
rest
of
the
clause.

Exp
erimen
tal
Results
.
Finite
Elemen
t
Mesh
Design
The
nite
elemen
t
mesh
design
application,
in
tro
duced
b
y
Muggleton
and
F
eng
(		),
concerns
the
division
of
an
ob
ject
in
to
regions
for
nite
elemen
t
sim
ulation.
The
regions
are
created
b
y
cutting
eac
h
edge
of
the
ob
ject
in
to
a
n
um
b
er
of
in
terv
als.
The
ILP
task
is
to
learn
to
generate
a
suitable
n
um
b
er
of
in
terv
als
for
eac
h
edge:
division
in
to
to
o
man
y
in
terv
als
leads
to
excessiv
e
computation
in
the
sim
ulation;
division
in
to
to
o
few
leads
to
a
p
o
or
appro
ximation
of
the
b
eha
vior
of
the
ob
ject.
The
tests
describ
ed
here
use
data
concerning
v
e
ob
jects
with
a
total
of

edges.
The
function
to
b
e
learned
is
mesh(A,B)
where
A
is
an
edge
and
B
is
the
n
um
b
er
of
in
terv
als
that


Ob
ject
Edges
Correct
Error


F
oil
FF
oil
F
oidl
F
oil
FF
oil
F
oidl
A







B

	



0

C




0


D







E
	


	



T
otal


00
0
0
0

(%)
(%)
(	%)
(%)
(%)
(%)
T
able
:
Results
for
nite
elemen
t
mesh
design
data
the
edge
should
b
e
divided
in
to.
The
bac
kground
information
consists
of
thirt
y
relations
describing
v
arious
prop
erties
of
the
edges
and
their
relationship
to
other
edges
in
the
ob
ject.
W
e
ran
v
e
trials;
in
eac
h,
the
learning
system
w
as
pro
vided
with
the
information
ab
out
four
of
the
ob
jects,
and
the
resulting
program
w
as
tested
on
the
edges
from
the
remaining
ob
ject.
T
able

sho
ws
our
results
for
F
oidl
along
with
those
rep
orted
in
Quinlan
(submitted)
for
F
oil
and
FF
oil.
W
e
rep
ort
b
oth
the
n
um
b
er
of
edges
whic
h
exactly
matc
h
the
exp
ert's
and
the
n
um
b
er
for
whic
h
the
system
is
no
more
that
one
a
w
a
y
,
since
b
eing
close
ma
y
b
e
useful
in
this
t
yp
e
of
problem.
F
oidl
and
FF
oil
b
oth
exploit
the
functional
nature
of
the
problem
using
decision
lists
and
pro
duce
results
that
are
dramatically
b
etter
than
those
of
F
oil.
F
oidl's
results
are
sligh
tly
b
etter
than
FF
oil's
o
v
erall,
but
not
signican
tly
so.
Both
systems
also
p
erform
m
uc
h
b
etter
than
results
rep
orted
for
mF
oil
and
Golem
(%
and
	%
correct
resp
ectiv
ely)
(La
vra

c
&
D

zeroski,
		).
.
List-pro
cessing
Programs
F
or
another
test
of
F
oidl's
capabilities,
w
e
used
a
selection
of
the
list-pro
cessing
problems
to
whic
h
Quinlan
and
Cameron-Jones
(		)
applied
F
oil.
These
are
a
sequence
of
list-
pro
cessing
examples
and
exercises
from
Chapter

of
(Bratk
o,
		0).
F
or
eac
h
problem,
the
bac
kground
pro
vided
consists
of
the
relations
encoun
tered
previously
.
In
his
exp
erimen
ts,
Quinlan
uses
t
w
o
univ
erses:
all
lists
on
three
elemen
ts
of
length
up
to
three
and
all
lists
on
four
elemen
t
s
of
length
up
to
four.
F
or
eac
h
problem,
F
oil
w
as
pro
vided
with
all
p
ositiv
e
examples
of
the
relation
in
the
sp
ecied
univ
erse
and
generates
negativ
es
using
the
closed
w
orld
assumption.
Because
F
oil
m
ust
either
b
e
pro
vided
with
explicit
negativ
es
or
b
e
able
to
generate
negativ
es
using
a
closed
w
orld
assumption,
it
cannot
learn
the
v
arious
relations
from
a
smaller
set
of
examples
if
only
p
ositiv
es
are
pro
vided.
W
e
sp
eculated
that
F
oidl's
implici
t
negativ
es
w
ould
enable
it
to
learn
sev
eral
of
the
list-pro
cessing
relations
from
smaller
sets
of
p
ositiv
e
examples.
Previous
exp
erimen
ts
with
learning
list-pro
cessing
Prolog
programs
ha
v
e
emplo
y
ed
sp
ecially
constructed
sets
of
exam-
ples
that
are
guaran
teed
to
b
e
complete
in
some
sense.
Ho
w
ev
er,
ideally
an
ILP
system
should
b
e
able
to
learn
suc
h
programs
from
random
examples
rather
than
carefully-selected
sets
(Zelle
&
Mo
oney
,
		;
Cohen,
		).
Consequen
tly
,
w
e
compared
systems
on
randomly


0
20
40
60
80
100
0
5
10
15
20
Percentage of Programs Correct
Training Examples
FOIDL
IFOIL
FFOIL
0
20
40
60
80
100
0
5
10
15
20
Percentage of Programs Correct
Training Examples
FOIDL
IFOIL
FFOIL
last
shift
0
5
10
15
20
25
30
35
40
45
50
0
5
10
15
20
Percentage of Programs Correct
Training Examples
FOIDL
IFOIL
FFOIL
translate
Figure
:
Results
for
functional
list
pro
cessing
programs
selected
examples.
F
or
eac
h
subset
size,
w
e
ran
0
exp
erimen
ts,
and
w
e
rep
ort
the
p
ercen
tage
of
trials
out
of
the
0
that
the
system
is
able
to
learn
a
correct
denition
of
the
relation
for
lists
of
arbitrary
length.
W
e
use
this
denition
of
accuracy
b
ecause
in
this
t
yp
e
of
domain
it
seems
more
approprate
than
a
measure
of
the
abilit
y
of
incorrect
programs
to
classify
examples.
The
relations
w
e
considered
naturally
divide
in
to
t
w
o
distinct
sets
of
exp
erimen
ts.
Sev
eral
of
them
are
functional,
making
them
appropriate
candidates
for
tests
with
systems
that
learn
decision
lists.
Quinlan
(submitted)
sho
w
ed
that
FF
oil
can
learn
these
functional
relations
more
quic
kly
than
F
oil,
but
he
did
not
explore
the
p
ossibilit
y
of
learning
from
few
er
examples.
W
e
ran
parallel
trials
on
the
functional
relations
from
the
set
using
F
oidl,
FF
oil,
and
IF
oil,
whic
h
is
F
oidl
without
decision
lists.
On
t
w
o
of
the
functions:
rev
erse
and
conc,
our
results
w
ere
not
encouraging.
Rev
erse
requires
the
en
tire
univ
erse
of
0
examples
when
lists
are
limited
to
length
three
for
all
three
of
the
systems.
IF
oil
and
F
oidl
b
egin
to
b
e
able
to
learn
correct
denitions
for
conc
with
subsets
of
0
examples
from
the

total.
Figure

sho
ws
the
results
for
last,
shift
and
translate.
F
oidl,
b
oth
with
and
without


decision
lists,
is
able
to
learn
correct
denitions
from
relativ
ely
small
subsets
of
the
p
ositiv
e
examples.
FF
oil's
p
erformance
on
these
tasks
p
oin
ts
to
one
of
the
adv
an
tages
of
the
F
oidl
algorithm.
The
correct
programs
for
b
oth
last
and
translate
are
recursiv
e,
but
shift
do
es
not
require
recursion,
and
shift
is
the
only
one
of
these
functions
whic
h
FF
oil
p
erforms
w
ell
on.
This
is
b
ecause
the
random
subsets
of
examples
rarely
pro
vide
a
sequence
of
examples
of
the
t
yp
e
that
the
extensional
algorithm
requires
in
order
to
learn
recursiv
e
rules.
Since
F
oidl
in
terpret
b
oth
the
bac
kground
and
the
rules
b
eing
learned
in
tensionally
,
it
requires
any
base
example
or
clause,
not
the
imme
diately
preceding
example.
Th
us,
to
learn
the
rule
last(A,[B|
C])
:-
last(A,C)
.
FF
oil
w
ould
require
sequences
of
examples
suc
h
as:
last(,[,
,
])
.
last(,[,
])
.
last(,[]
).
while
F
oidl
w
ould
need
only
the
last
item
and
one
of
the
other
t
w
o
from
the
sequence.
F
oidl
and
IF
oil
p
erform
iden
tically
on
shift,
but
quite
dieren
tly
on
the
other
t
w
o
tasks.
IF
oil
p
erforms
b
etter
with
few
er
examples
on
last
and
throughout
on
translate,
but
F
oidl
catc
hes
up
and
surpasses
IF
oil's
p
erformance
on
last.
This
seems
to
b
e
b
ecause
ha
ving
v
ery
few
examples
tends
to
lead
the
decision
list
to
create
bad
clauses
as
the
default
clause,
leading
to
incorrect
denitions.
Once
there
are
enough
examples,
ho
w
ev
er,
the
decision
list
bias
b
ecomes
helpful
on
last.
Besides
demonstrating
the
p
erformance
of
F
oidl
on
functions,
w
e
wished
to
examine
the
usefulness
of
the
implicit
negativ
es
in
non-functional
applications.
So
w
e
also
ran
an
exp
erimen
t
with
four
of
the
non-functional
list-pro
cessing
problems.
Again
w
e
randomly
selected
subsets
of
p
ositiv
e
examples
from
the
univ
erse
of
lists
of
length
up
to
three
on
three
elemen
ts
(0
subsets
of
eac
h
size),
and
w
e
ran
F
oidl
(without
decision
lists,
of
course)
on
eac
h
subset
to
determine
whether
it
pro
duced
a
correct
program
for
lists
of
arbitrary
length.
Because
of
the
requiremen
t
that
the
input
b
e
output
complete,
the
subsets
w
ere
c
hosen
not
b
y
randomly
selecting
the
examples,
but
b
y
randomly
selecting
a
set
of
inputs
and
then
pro
viding
the
system
with
all
of
the
p
ositiv
e
examples
with
those
inputs.
F
or
the
relations
tested,
the
a
v
erage
n
um
b
er
of
examples
p
er
input
v
aries
from
barely
o
v
er

for
deleting
an
item
from
a
list
to

for
sublist.
The
results
for
the
non-functional
relations
tested
app
ear
in
Figure
.
Although
it
learns
some
relations
b
etter
than
other,
F
oidl
clearly
can
exploit
its
abilit
y
to
use
implici
t
negativ
e
examples
to
learn
these
relations
from
far
few
er
p
ositiv
e
examples
than
F
oil
requires
(
for
mem
b
er,

for
del,

for
insert,
and
0
for
sublist).

Related
W
ork
The
rst
learning
system
to
fo
cus
on
learning
functions
only
w
as
Filp
(Bergadano
&
Gunetti,
		).
It
assumes
that
the
target
and
all
bac
kground
relations
are
functional
and
uses
this
	

0
20
40
60
80
100
0
1
2
3
4
5
6
7
8
Percentage of Programs Correct
Number of Distinct Inputs
member
del
insert
sublist
Figure
:
IF
oil's
p
erformance
on
non-functional
list
pro
cessing
programs
kno
wledge
to
limit
its
searc
h
for
literals.
Ho
w
ev
er,
these
assumptions
prev
en
t
its
use
on
the
tasks
considered
here
b
ecause
they
t
ypically
in
v
olv
e
non-functional
bac
kground
relations.
Also,
although
Filp
assumes
that
its
target
relation
is
function,
the
denitions
learned
consist
of
unordered
sets
of
clauses.
Other
systems,
most
notably
Lopster(Lap
oin
te
&
Mat
win,
		)
and
Cr
ust
a
cean
(Aha,
Ling,
Mat
win,
&
Lap
oin
te,
		),
ha
v
e
addressed
learning
relations
from
v
ery
small
sets
of
examples.
These
systems
can
learn
certain
recursiv
e
relations
from
ev
en
few
er
ex-
amples
than
F
oidl,
but
they
can
only
learn
relations
with
a
particular
recursiv
e
structure,
and
they
use
the
assumption
that
the
target
relation
has
this
particular
structure.
F
oidl
requires
a
few
more
examples,
but
this
is
b
ecause
it
is
a
more
general
system,
w
orking
on
a
n
um
b
er
of
other
t
yp
es
of
problems,
and
b
ecause
F
oidl
do
es
not
mak
e
assumptions
ab
out
the
structure
of
the
solution,
only
ab
out
the
nature
of
the
data.
Tw
o
other
systems
use
in
tensional
negativ
e
examples,
but
neither
is
as
general
and
ex-
ible
as
F
oidl.
Bergadano,
Gunetti,
and
T
rinc
hero
(		)
allo
ws
the
user
to
supply
an
in
tensional
denition
of
negativ
e
examples
that
co
v
ers
a
large
set
of
ground
instances
(e.g
(past([a,c,
t],
X),
not(equal(X
,
[a,c,t,e,
d])
)));
ho
w
ev
er,
to
b
e
equiv
alen
t
to
out-
put
completeness,
the
user
w
ould
ha
v
e
to
explicitly
pro
vide
a
separate
in
tensional
negativ
e
denition
for
eac
h
p
ositiv
e
example.
The
non-monotonic
seman
tics
used
to
eliminate
the
need
for
negativ
e
examples
in
Cla
udien
(De
Raedt
&
Bruyno
oghe,
		)
has
the
same
eect
as
an
output
completeness
assumption
in
the
case
where
all
argumen
ts
of
the
tar-
get
relation
are
outputs.
Ho
w
ev
er,
output
completeness
p
ermits
more
exibilit
y
b
y
allo
wing
some
argumen
ts
to
b
e
sp
ecied
as
inputs
and
only
coun
ting
as
negativ
e
examples
those
extra
outputs
generated
for
sp
ecic
inputs
in
the
training
set.
The
system
most
similar
to
F
oidl
is
FF
oil.
Both
systems
use
decision
lists
to
learn
functions,
and
b
oth
pro
vide
implici
t
negativ
es.
Ho
w
ev
er,
there
are
sev
eral
imp
ortan
t
dif-
0

ference
b
et
w
een
the
systems.
The
t
w
o
most
ob
vious
are
the
in
tensional
v
ersus
extensional
bac
kground
and
the
dieren
t
order
in
whic
h
they
learn
clauses.
Also
signican
t,
though
more
subtle,
are
the
dierences
in
their
handling
of
implici
t
negativ
es.
FF
oil
determines
the
n
um
b
er
of
negativ
es
co
v
ered
based
on
the
range
of
the
function
as
sp
ecied
in
the
pro-
vided
constan
ts.
It
do
es
not
allo
w
for
the
p
ossibilit
y
that
some,
but
not
all
of
those
constan
ts
migh
t
b
e
co
v
ered,
and
it
do
es
not
allo
w
for
the
diculties
of
generating
all
of
the
constan
ts
for
a
function
an
in
tractably
large
range,
suc
h
as
the
standard
past
tense
task.
F
or
ex-
p
erimen
ts
with
FF
oil
on
the
past
tense
task,
Quinlan
still
uses
a
sp
ecial
form
ulation
of
the
problem
in
v
en
ted
to
allo
w
F
oil
some
success
at
the
task,
whic
h
exploits
the
kno
wledge
that
the
English
past
tense
is
formed
using
suxes
in
order
to
greatly
reduce
the
n
um
b
er
of
constan
ts
required
in
order
to
pro
duce
negativ
es.
The
nal
distinction
b
et
w
een
the
t
w
o
systems
is
F
oidl's
greater
exibilit
y
.
The
implici
t
negativ
es
can
b
e
used
without
decision
lists
to
handle
non-functional
relations.

Conclusions
W
e
ha
v
e
sho
wn
that
F
oidl,
an
ILP
system
originally
designed
to
address
a
particular
prob-
lem
in
natural
language
pro
cessing,
has
imp
ortan
t
adv
an
tages
outside
of
natural
language
pro
cessing.
F
oidl's
results
on
the
nite
elemen
t
mesh
design
problem
are
b
etter
than
an
y
others
rep
orted
for
an
ILP
system,
and
it
also
p
erforms
w
ell
on
sev
eral
list-pro
cessing
tasks.
W
e
b
eliev
e
that
these
results
indicate
that
F
oidl's
inno
v
ations{decision
lists
and
implicit
negativ
es
generated
using
the
output
completeness
assumption{will
pro
v
e
useful
for
a
v
ariet
y
of
mac
hine
learning
tasks.
References
Aha,
D.
W.,
Ling,
C.
X.,
Mat
win,
S.,
&
Lap
oin
te,
S.
(		).
Learning
singly
recursiv
e
relations
from
small
datasets.
In
Workshop
on
Inductive
L
o
gic
-
IJCAI	,
pp.
{.
Bergadano,
F.,
&
Gunetti,
D.
(		).
An
in
teractiv
e
system
to
learn
functional
logic
pro-
grams.
In
Pr
o
c
e
e
dings
of
the
Thirte
enth
International
Joint
Confer
enc
e
on
A
rticial
Intel
ligenc
e,
pp.
0{0
	
Cham
b
ery
,
F
rance.
Bergadano,
F.,
Gunetti,
D.,
&
T
rinc
hero,
U.
(		).
The
diculties
of
learning
logic
pro-
grams
with
cut.
Journal
of
A
rticial
Intel
ligenc
e
R
ese
ar
ch,
,
	{0.
Bratk
o,
I.
(		0).
Pr
olo
g
Pr
o
gr
amming
for
A
rticial
Intel
ligenc
e.
Addison
W
esley
,
Read-
ing:MA.
Cameron-Jones,
R.
M.,
&
Quinlan,
J.
R.
(		).
Ecien
t
top-do
wn
induction
of
logic
programs.
SIGAR
T
Bul
letin,

(),
{.
Clark,
P
.,
&
Niblett,
T.
(		).
The
CN
induction
algorithm.
Machine
L
e
arning,
,
{.


Cohen,
W.
W.
(		).
P
ac-learning
a
resticted
class
of
recursiv
e
logic
programs.
In
Pr
o
c
e
e
d-
ings
of
the
Eleventh
National
Confer
enc
e
on
A
rticial
Intel
ligenc
e,
pp.
{	
W
ash-
ington,
D.C.
De
Raedt,
L.,
&
Bruyno
oghe,
M.
(		).
A
theory
of
clausal
disco
v
ery
.
In
Pr
o
c
e
e
dings
of
the
Thirte
enth
International
Joint
Confer
enc
e
on
A
rticial
Intel
ligenc
e,
pp.
0{0
Cham
b
ery
,
F
rance.
Dolsak,
B.,
&
Muggleton,
S.
(		).
The
application
of
inductiv
e
logic
programming
to
nite-elemen
t
mesh
design.
In
Muggleton,
S.
(Ed.),
Inductive
L
o
gic
Pr
o
gr
amming,
pp.
{.
Academic
Press,
New
Y
ork.
Lap
oin
te,
S.,
&
Mat
win,
S.
(		).
Sub-unication:
A
to
ol
for
ecien
t
induction
of
recursiv
e
programs.
In
Pr
o
c
e
e
dings
of
the
Ninth
International
Confer
enc
e
on
Machine
L
e
arning,
pp.
{
Ab
erdeen,
Scotland.
La
vra

c,
N.,
&
D

zeroski,
S.
(Eds.).
(		).
Inductive
L
o
gic
Pr
o
gr
amming:
T
e
chniques
and
Applic
ations.
Ellis
Horw
o
o
d.
Mo
oney
,
R.
J.,
&
Cali,
M.
E.
(		).
Induction
of
rst-order
decision
lists:
Results
on
learning
the
past
tense
of
English
v
erbs.
Journal
of
A
rticial
Intel
ligenc
e
R
ese
ar
ch,
,
{.
Muggleton,
S.,
&
F
eng,
C.
(		).
Ecien
t
induction
of
logic
programs.
In
Muggleton,
S.
(Ed.),
Inductive
L
o
gic
Pr
o
gr
amming,
pp.
{	.
Academic
Press,
New
Y
ork.
Quinlan,
J.
R.
(submitted).
Learning
rst-order
denitions
of
functions.
A
rticial
Intel
li-
genc
e.
Quinlan,
J.
R.,
&
Cameron-Jones,
R.
M.
(		).
F
OIL:
A
midterm
rep
ort.
In
Pr
o
c
e
e
dings
of
the
Eur
op
e
an
Confer
enc
e
on
Machine
L
e
arning,
pp.
{0
Vienna.
Quinlan,
J.
(		0).
Learning
logical
denitions
from
relations.
Machine
L
e
arning,

(),
	{.
Riv
est,
R.
L.
.
(	).
Learning
decision
lists.
Machine
L
e
arning,

(),
	{.
W
ebb,
G.
I.,
&
Brki

c,
N.
(		).
Learning
decision
lists
b
y
prep
ending
inferred
rules.
In
Pr
o
c
e
e
dings
of
the
A
ustr
alian
Workshop
on
Machine
L
e
arning
and
Hybrid
Systems,
pp.
{0
Melb
ourne,
Australia.
Zelle,
J.
M.,
&
Mo
oney
,
R.
J.
(		).
Com
bining
top-do
wn
and
b
ottom-up
metho
ds
in
inductiv
e
logic
programming.
In
Pr
o
c
e
e
dings
of
the
Eleventh
International
Confer
enc
e
on
Machine
L
e
arning,
pp.
{
New
Brunswic
k,
NJ.


