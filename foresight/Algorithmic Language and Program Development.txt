TEXTS AND MONOGRAPHS IN COMPUTER SCIENCE 
Edited by David Gries 
ALGORITHMIC 
LANGUAGE 
AND PROGRAM 
DEVELOPMENT 
Friedrich L. Bauer 
Hans Wossner 
, 
Springer-Verlag 
Berlin Heidelberg New York 


Texts and Monographs in Computer Science 
Editor 
David Gries 
Advisory Board 
F. L. Bauer 
K.S. Fu 
J. J. Horning 
R. Reddy 
D. C. Tsichritzis 
W. M. Waite 


Algorithmic Language 
and Program Development 
F. L. Bauer 
H. Wossner 
In collaboration with 
H. Partsch and P. Pepper 
With 109 Figures 
Springer-Verlag 
Berlin Heidelberg New York 1982 

Friedrich L. Bauer Hans W ossner 
lnstitut fiir Informatik, Technische Universitat Mi.inchen 
Postfach 202420, D-8000 Mi.inchen 2, Federal Republic of Germany 
David Gries 
Department of Computer Science, Cornell University 
Ithaca, NY 14853, USA 
Translation of the German edition "Algorithmische Sprache und 
Programmentwicklung" 
Springer-Verlag Berlin Heidelberg New York 1981 
AMS Subject Classification: 68 A 05, 68 A 10, 68 A 20, 68 A 25, 68 A 30 
CR Subject Classification: 4.20, 4.22, 5.22, 5.24, 5.25, 5.27 
ISBN-13: 978-3-642-61809-3 
DOl: 10.1007/978-3-642-61807-9 
e-ISBN-13: 978-3-642-61807-9 
Library of Congress Cataloging in Publication Data 
Bauer, Friedrich Ludwig, 1924 -
Algorithmic language and program development. 
(Texts and monographs in computer science) 
Translation of: Algorithmische Sprache und Programmentwicklung. 
Bibliography: p. Includes index. 
1. Electronic digital computers - Programming. 
2. Programming languages (Electronic computers) 
I. WOssner, H. (Hans) II. Title. III. Series. 
QA76.6.B39513 1982 001.64'2 82-10364 
This work is subject to copyright. All rights are reserved, whether the whole or part of the 
material is concerned, specifically those of translation, reprinting, re-use of illustrations, 
broadcasting, reproduction by photocopying machine or similar means, and storage in data 
banks. Under § 54 of the German Copyright Law where copies are made for other than private 
use, a fee is payable to Verwertungsgesellschaft Wort, Munich. 
©by Springer-Verlag Berlin Heidelberg 1982 
Sotkover reprint of the hardcover 1st edition 1982 
Typesetting: K + V Fotosatz GmbH, Beerfelden. Printing: Beltz Offsetdruck, Hems bach 
Binding: Konrad Triltsch, Wllrzburg. 
2145/3140..543210 

In memoriam 
KLAUS SAMELSON 
1918-1980 


Preface 
The title of this book contains the words ALGORITHMIC LANGUAGE, in the singular. 
This is meant to convey the idea that it deals not so much with the diversity of program-
ming languages, but rather with their commonalities. The task of formal program develop-
ment proved to be the ideal frame for demonstrating this unity. It allows classifying 
concepts and distinguishing fundamental notions from notational features; and it leads 
immediately to a systematic disposition. This approach is supported by didactic, practical, 
and theoretical considerations. The clarity of the structure of a programming language de-
signed according to the principles of program transformation is remarkable. 
Of course there are various notations for such a language. The notation used in this 
book is mainly oriented towards ALGOL 68, but is also strongly influenced by PASCAL 
- it could equally well have been the other way round. In the appendices there are occa-
sional references to the styles used in ALGOL, PASCAL, LISP, and elsewhere. 
The book is divided clearly into three parts: the first four chapters concentrate on the 
level of "applicative" formulation, which is characterized by function application as the 
dominant language element, and includes problem specifications. The transition to the 
level of "procedural" formulation - which is characterized by the appearance of program 
variables - is motivated in Chap. 4 and carried out in Chaps. 5 and 6. In Chap. 7, further 
development leads to concepts which are particularly important in systems programming 
for present-day machines: organized stores, pointers and nexuses; characteristically, on this 
level program variables and pointers are in some sense considered as independent objects. 
The transitions between these levels are described by definitional transformations. 
More generally, we view the entire process of programming as being a chain of program 
transformations; the individual classes of transformation rules prove to be an excellent 
didactic means for structuring the material. 
The trichotomy mentioned above is fundamental. It has become customary to distin-
guish between "applicative" and "procedural"; the relative success of LISP and APL is ac-
counted for by the advantages of applicative programming. Backus argues (1978b): "I now 
regard all conventional languages (e.g., the FORTRANs, the ALGOLs, their successors 
and derivatives) as increasingly complex elaborations of the style of programming dictated 
by the von Neumann computer. These 'von Neumann languages' create enormous, 
unnecessary intellectual roadblocks in thinking about programs and in creating the higher 
level combining forms required in a really powerful programming methodology." 
Here, however, it cannot be the purpose to take sides with one camp or the other. The 
experienced programmer must master both styles as well as the transition between them. It 
is also important to distinguish the third level, the level of independent variables and 
pointers, from the others. The relatively small size of Chap. 7 indicates on the one hand 
that the description is restricted to the basics, including many points known from the liter-

VIII 
Preface 
ature on systems programming such as D. E. Knuth's "The Art of Computer Program-
ming" or R. M. Graham's "Principles of Systems Programming". On the other hand, it 
also indicates that the theoretical foundations of this field still need further development. 
Every section of this book has a mathematical background of varying degree of devel-
opment. Some important basic notions stem from lattice theory. This has been impressive-
ly shown by the foundational work of D. Scott. Behind the computation structures of the 
third chapter stands the modern theory of universal algebra; in particular, works of G. 
Birkhoff and A. Tarski are important here. Since we were not presenting a mathematical 
textbook, we must frequently content ourselves with hints and references. 
The book has developed from lectures and accompanying exercises that were presented 
in recent years at the Technical University of Munich. It arose in close interaction with the 
project CIP ("Computer-aided Intuition-guided Programming") and the subproject 
"Wide spectrum language and program transformations" in the Sonderforschungsbereich 
49, Programming Technology, at the Technical University of Munich. Although it at-
tempts to lay the foundations for a well-rounded, consistent "science of programming", 
much had to remain fragmentary. In some places it was necessary to take seemingly uncon-
ventional positions in order to overcome inflexibility and to correct biased doctrines. In 
this respect the book addresses not only students but also their academic teachers. 
Thus, this is not purely a beginner's text; although, as it is frequently said, "no previ-
ous knowledge will be assumed" - a certain kind of previous knowledge can even be an 
obstacle - nevertheless an understanding of the manifold interconnections requires a cer-
tain training of thought. Neither is the book a monograph, since the ordering of the mate-
rial has been influenced decisively by didactic considerations. Rather, the book is directed 
towards first-year graduate students, as were the lectures from which it has arisen; how-
ever, it may also serve to give a guideline for introductory teaching. 
This book reflects thirty years of dealing with the computer. Above all, Zuse, Rutis-
hauser, Samelson, McCarthy and Floyd, Landin and Strachey, Dijkstra and Hoare have 
influenced with their ideas the intellectual lines of development that led to this book; we 
acknowledge this gratefully. Further names would need to be mentioned here; references 
to these will be found in the text, where critical notes also contain an appreciation. 
We thank the late K. Samelson along with G. Seegmuller, C. A. R. Hoare, A. P. 
Ershov, D. Gries, M. Griffiths, H. Langmaack, and M. Woodger for many impulses and 
discussions. However, our thanks are especially due to the Munich co-workers, above all 
M. Broy, W. Dosch, F. Geiselbrechtinger, R. Gnatz, U. Hill-Samelson, B. Krieg-
Bruckner, A. Laut, B. Moller, G. Schmidt and M. Wirsing; in particular, H. Partsch and 
P. Pepper have contributed considerably to the structure, contents, and presentation 
of the book. We are specifically grateful to Mrs Peggy Geiselbrechtinger who translated an 
earlier version of this book from German. We also thank all the others who have devoted 
much toil and patience to the completion of the book, notably the team at Springer-
Verlag. 
Munich, Spring 1982 
F. L. Bauer, H. Wossner 

Table of Contents 
Introduction ............................................................ . 
0.1 
On the Etymology of the Word Algorithm............................... 
1 
0.2 How Algorithms are Characterized . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
3 
0.3 Programming as an Evolutionary Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
8 
0.4 How to Solve it...................................................... 
10 
Chapter 1. Routines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
13 
1.1 The Parameter Concept . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
13 
1.2 Declaration of a Routine . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
16 
1.3 Hierarchical Construction of Routines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
17 
1.3.1 Primitive Routines and Computational Structures................... 
17 
1.3.2 The Principle of Substitution. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
18 
1. 3. 3 Alternatives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
22 
1.3.4 Input/Output. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
23 
1.4 Recursive Routines and Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
24 
1.4.1 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
24 
1.4.2 Proof of Termination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
27 
1.4.3 Taxonomy of Recursion. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
28 
1.4.4 The Level of Applicative Formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
31 
1.5 Mathematical Semantics: Fixpoint Theory..................... . . . . . . . . . . 
32 
1.5.1 Recursive Routines and Functional Equations . . . . . . . . . . . . . . . . . . . . . . 
32 
1.5.2 Fixpoint Theory................................................ 
35 
1.6 Proofs by Induction of Properties of Routines . . . . . . . . . . . . . . . . . . . . . . . . . . . 
40 
1.6.1 Computationallnduction........................................ 
40 
1.6.2 Structural Induction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
43 
1. 7 Operational Semantics: Machines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
44 
1.7.1 Unfolding and Folding.......................................... 
45 
1. 7.2 Partial Computation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
47 
1.7.3 Text Substitution Machines...................................... 
49 
1. 7.4 The Stack Machine .. .. . .. .. .. . .. .. .. .. .. . .. .. .. . . .. .. .. . .. .. .. . 
52 
1.8 Restriction of the Parameter Domain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
66 
1.9 Dijkstra's Guards. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
67 
1.10 Pre-Algorithmic Formulations by Means of Choice and Determination . . . . . . 
72 
1.10.1 The Choice Operator . .. .. .. . .. .. .. . .. .. . . .. .. . .. .. . . . . .. . . .. . . 
73 
1.10.2 The Determination Operator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
75 
1.11 Semantics of Non-Deterministic Constructions. . . . . . . . . . . . . . . . . . . . . . . . . . . 
76 

X 
Table of Contents 
1.11.1 Pre-Algorithms and Algorithms 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
76 
1.11.2 Deriving Algorithms from Pre-Algorithms 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
78 
1o11o 3 Mathematical Semantics of Non-Determinate Routines 0 0 0 0 0 0 0 0 0 0 0 0 0 
82 
1.11.4 Operational Semantics of Non-Deterministic Algorithms 0 0 0 0 0 0 0 0 0 0 0 0 
85 
1012 Routines with a Multiple Result 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
87 
1013 Structuring of Routines 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
89 
101301 Structuring by Means of Abstraction and Embedding 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
89 
1.1302 Segments and Suppressed Parameters 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
94 
1.1303 Object Declarations 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
97 
1.1304 Result Parameters and the Actualization Taboo 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
101 
1.14 Routines as Parameters and Results 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
104 
101401 Routines as Results 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
106 
1.1402 Functional Programming 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
107 
1.1403 The Delay Rule 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
109 
Addendum: Notations 0 0 0 0 o 0 0 0 0 0 0 0 0 0 0 o o 0 o 0 0 0 0 0 0 0 o 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
112 
Chapter 2. Objects and Object Structures o o 0 0 o o o o o 0 0 0 0 0 0 0 0 0 0 0 0 0 o 0 0 0 0 0 0 0 0 0 0 0 0 0 
117 
201 
Denotations 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
118 
202 Scope of a Freely Chosen Designation 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
120 
203 
Kinds ofObjectso 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
120 
204 Sets of Objects, Modes 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
121 
205 Composite Modes and Objects 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
126 
206 Selectors, Structures with Direct (Selector) Access 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
127 
2.601 Compounds 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 o 0 0 0 0 0 0 0 0 0 0 o 0 
128 
20602 Arrays 0 0 0 0 0 0 o o o 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 o 0 0 o o o 0 0 0 o 0 o o 0 0 o o o o o 0 o 0 o o 0 0 0 0 o o o 
129 
20603 The Selection Structure of Compound and Array 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
131 
207 Mode Variants 0 0 0 0 0 0 0 o 0 0 0 0 0 0 0 0 0 0 0 o 0 0 0 0 o o o 0 0 0 0 o 0 0 0 0 0 o o o o o 0 o 0 0 0 o 0 0 0 o o o 
133 
208 Introduction of New Modes: Summary 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
137 
209 Recursive Object Structures 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
140 
20901 Definition of Recursive Object Structures 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
140 
20902 Object Diagrams 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 o 0 0 o 0 0 0 0 0 o 0 0 0 0 0 0 
145 
20903 Operational Detailing of Objects 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 o 0 0 0 0 0 
151 
2010 Algorithms with Linear Object Structures 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 o 0 o 
154 
2011 The Recursive Object Structure "File" 0 0 0 0 0 o o 0 0 0 0 0 0 0 0 0 0 0 o 0 0 o 0 0 o 0 o 0 0 0 0 0 0 0 
160 
2.11.1 "Knitting" ofSequenceso 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 o 0 
160 
2011.2 Files 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 o o 0 0 0 o o 0 0 0 0 0 o 0 0 o o o o o o o 0 0 0 0 o o o o o o o o 
161 
2012 Algorithms with Cascade-Type Object Structures 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
163 
2013 Traversal and Scanning of Recursive Object Structures 0 0 0 0 o 0 o o 0 0 0 0 0 0 0 0 0 0 0 0 
166 
2o14 Infinite Objects 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 o 0 o 0 0 0 0 0 0 0 0 o 0 0 0 
169 
2o14o1 Nexuses of Objects 0 o 0 0 0 0 0 0 0 0 0 0 0 0 o o o 0 0 0 0 0 0 0 0 o o o o o o o o o o o o o o o o 0 o 0 
170 
201402 Lazy Evaluation 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
173 
2 015 Some Peculiarities of Arrays 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
177 
201501 Arrays with Computed Index Bounds o 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
177 
2015 02 Induced Operations for Arrays 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 o o o o o 0 0 0 
178 
2016 Routines with Multiple Results Revisited 0 0 o 0 o 0 0 o 0 0 0 0 0 0 o o o o o o o o 0 0 0 0 0 o o o o o 
179 
Addendum: Notations 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 o 0 0 0 0 0 0 o 0 0 0 0 0 0 o 0 0 0 o o 
180 

Table of Contents 
XI 
Chapter 3. Computational Structures. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
185 
3.1 
Concrete Computational Structures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
186 
3.1.1 Encapsulation Effect . .. .. .. . . . . . . . .. . . . . .. . . . . . . . . . . . . . . . . . . . . . 
186 
3 .1. 2 Properties of Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
188 
3 .1. 3 Definition of Concrete Computational Structures . . . . . . . . . . . . . . . . . . . 
189 
3.1 .4 Atomic Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
191 
3.2 Abstract Computational Structures and Abstract Types . . . . . . . . . . . . . . . . . . . 
195 
3 .2.1 Fundamental Concepts. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
195 
3.2.2 Semantics of Abstract Computational Structures and Abstract Types . . 
199 
3.2.3 Completeness of Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
203 
3.2.4 Concretization of an Abstract Type . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
204 
3.2.5 Notation and First Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
205 
3.2.6 Constructors and Selectors....................................... 
211 
3.3 Abstract Arrays . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
213 
3.3.1 One-Side-Flexible Arrays . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
214 
3.3.2 Two-Side-Flexible Arrays...................... . . . . . . . . . . . . . . . . . . 
216 
3.3.3 Aggregates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
219 
3.4 Sequence-Type Computational Structures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
221 
3.4.1 Stack, Deck and Queue . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
221 
3.4.2 Excursus: Divisibility Theory in Semi-Groups... . . . . . . . . . . . . . . . . . . . . 
224 
3.4.3 Sequence and Word . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
225 
3.4.4 Forgetful Functors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
229 
3.4.5 Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
231 
3.5 Number-Type Computational Structures................................ 
235 
3.5.1 Peano Numbers................. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
235 
3.5.2 Cycle Numbers and Natural Numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
241 
3.5.3 Excursus: Extension by Means of Formal Quotients . . . . . . . . . . . . . . . . . 
243 
3.5.4 Integers.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
244 
3.5.5 Rational Numbers........... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
247 
3.5.6 Positional Systems and B-al-Fractions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
249 
3.6 Changing Abstract Types and Object Structures............. . . . . . . . . . . . . . 
252 
3.6.1 Type Change and Related Types.................................. 
252 
3.6.2 Concretization... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
254 
3 .6. 3 Implementation of Concrete Computational Structures . . . . . . . . . . . . . . 
258 
3.6.4 Example: Binarization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
259 
3.6.5 Example: Packing of Objects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
264 
Addendum: Notations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
266 
Chapter 4. Transformation into Repetitive Form . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
269 
4.1 
Schemes and Transformations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
269 
4.2 Treatment of Linear Recursion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
272 
4.2.1 The Technique of Re-Bracketing. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
272 
4.2.2 The Technique of Operand Commutation. . . . . . . . . . . . . . . . . . . . . . . . . . 
275 
4.2.3 Function Inversion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
278 
4.2.4 Function Inversion According to Paterson and Hewitt . . . . . . . . . . . . . . . 
282 

XII 
Table of Contents 
4.2.5 Function Inversion by Introducing Stacks . . . . . . . . . . . . . . . . . . . . . . . . . . 
283 
4.3 Treatment of Non-Linear Recursions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
287 
4.3.1 Method of Functional Embedding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
288 
4.3.2 Arithmetization of the Flow of Control............................ 
294 
4.3.3 Special Cases of Nested Recursion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
296 
4.3.4 The Technique of Range-of-Values Tabulation . . . . . . . . . . . . . . . . . . . . . 
299 
4.4 Disentanglement of the Control. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
302 
4.4.1 Disentangled Routines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
302 
4.4.2 Disentangling Recursive Routines by Means of Function Inversion . . . . . 
304 
4.4.3 Reshaping the Type of Control Flow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
308 
Chapter 5. Program Variables. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
311 
5.1 
The Origin of Program Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
311 
5.1.1 Specialization of the Stack Machine............................... 
313 
5.1.2 Specialization of the Range-of-Values Machine . . . . . . . . . . . . . . . . . . . . . 
317 
5.2 Formal Introduction of Program Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
320 
5 .2.1 Sequentialization of Object Declarations. . . . . . . . . . . . . . . . . . . . . . . . . . . 
320 
5.2.2 Program Variables as a Means for Saving Identifiers................. 
323 
5.2.3 Expressions with Side-Effects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
326 
5 .2.4 Complete Sequentialization of Collective Assignments . . . . . . . . . . . . . . . 
329 
5.3 Procedures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
331 
5.3.1 Program Variables as Parameters................................. 
331 
5.3.2 Actualization Taboo, Alias Ban and Suppressed Variable Parameters . . 
336 
5.3.3 SharingofVariables............................................ 
339 
5.3.4 Initialization................................................... 
340 
5.3.5 Properties of Program Variables.................................. 
342 
5 .4 Axiomatic Description of Programming Languages . . . . . . . . . . . . . . . . . . . . . . . 
342 
5.4.1 Predicate Transformers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
343 
5.4.2 Program Verification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
347 
5.5 Variables for Structured Objects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
350 
5.5.1 Selective Alteration............................................. 
351 
5.5.2 Remarks on Input/Output....................................... 
352 
Addendum: Notations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
353 
Chapter 6. Control Elements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
355 
6.1 
Deparameterization and Formal Treatment of Repetition . . . . . . . . . . . . . . . . . . 
355 
6.1.1 Deparameterization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
355 
6.1.2 Semantics of Repetition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
360 
6.1.3 Analytical Treatment of the Protocol Stack . . . . . . . . . . . . . . . . . . . . . . . . 
362 
6.2 Jumps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
364 
6.2.1 Simple Call as a Basic Control Element . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
364 
6.2.2 Introduction of Jumps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
367 
6.3 The General do-od Construction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
371 
6.4 Loops.............................................................. 
373 
6.4.1 Rejecting and Non-Rejecting Repetition . . . . . . . . . . . . . . . . . . . . . . . . . . . 
373 
6.4.2 Counted Repetition. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
376 

Table of Contents 
XIII 
6.5 Loops and Repetitive Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
377 
6.6 Sequential Circuits. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
378 
6. 7 Flow Diagrams . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
381 
6. 7.1 Classical Flow Diagrams. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
381 
6. 7.2 Splitting and Collection .. .. .. . .. .. .. .. . .. .. . . . .. . .. . . . .. . . . . . .. . 
384 
6. 7.3 Coordinated Flow Diagrams .. . .. .. .. . .. .. . . . . .. .. . .. .. .. . . .. . .. . 
388 
6.8 Petri Nets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
393 
6.8.1 Theory of Petri Nets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
393 
6.8.2 Construction of Petri Nets, Connection to Coordinated Flow 
Diagrams . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
397 
6.9 boo! Petri Nets, Signals .. .. . . . . .. . .. .. . . . .. .. . .. .. . . .. .. .. . .. .. . . .. .. 
400 
6.10 nat Petri Nets, Semaphores . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
405 
Addendum: Notations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
408 
Chapter 7. Organized Storages and Linked Lists . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
409 
7.1 
Organized Storages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
409 
7 .1.1 Selective Updating. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
410 
7.1.2 Collecting and Composing Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
412 
7.1.3 Computed Variables............................................ 
413 
7 .1.4 Constructing Organized Storages and Generating Variables . . . . . . . . . . . 
415 
7 .1.5 Advantages and Disadvantages of Organized Storages . . . . . . . . . . . . . . . 
418 
7.2 Identity of Variables and Alias Ban Revisited . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
419 
7.2.1 Revision ofthe Assignment Axiom................................ 
419 
7 .2.2 Checking the Actualization Taboo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
420 
7.3 Implementing Object Structures by Organized Storages. . . . . . . . . . . . . . . . . . . . 
422 
7.4 Linked-List Implementation of Organized Storages . . . . . . . . . . . . . . . . . . . . . . . 
425 
7.4.1 References to Variables: Pointers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
425 
7.4.2 Wirth's Connection............................................. 
431 
7.4.3 Link Variables .................. .".............................. 
432 
7.4.4 Implementing Computational Structures Using Linked Lists . . . . . . . . . . 
435 
7.4.5 Properties of Pointers........................................... 
437 
7.5 Improvement of Algorithms Working on Linked Lists by Selective Updating . 
438 
7.5.1 Algorithms for One-Way Linked Lists............................. 
438 
7.5.2 Algorithms for Two-Way Linked Lists . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
441 
7.6 Addressing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
443 
7.6.1 Addresses for Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
444 
7 .6.2 Jump Addresses. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
445 
7 .6.3 Genuine Addresses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
446 
7 .6.4 Outlook to Systems Programming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
448 
Addendum: Notations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
449 
Conclusion. Programming as an Evolutionary Process . . . . . . . . . . . . . . . . . . . . . . . . . 
451 
Program Specification and Development in a Uniform Language . . . . . . . . . . . . . . . . 
451 
Conceptual Organization of the Algorithmic Language . . . . . . . . . . . . . . . . . . . . . . . . 
455 
Tools to Be Used . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
456 
Methodology of Programming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
457 

XIV 
Table of Contents 
Bibliography 
459 
Index................................................................... 
471 
Glossary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
494 

Notice to the Reader 
Side remarks that may be skipped at a first reading are printed in small type. 
Certain basic mathematical concepts together with brief definitions are collected in a 
glossary at the end of the book. The exercises interspersed in the text are numbered consec-
utively within the individual sections, so that for example Exercise 1 in section 1.7.2 may 
be referred to as 1.7.2-1. References such as McCarthy 1961 or McCarthy (1961) refer to 
the bibliography. 
Acknowledgements 
The frontispiece of Chap. 0 and Figure 0.1 are reprinted from 
K. Menninger, "Zahlwort und Ziffer", Vol. II, 2nd ed., Gottingen: Vandenhoeck & 
Ruprecht 1958. 
Figure 0.2 is reprinted from 
A. Risen, "Rechenbuch", Frankfurt 1574, facsimile reprint, Brensbach/Odw.: Satyr-
Verlag, 1978. 
The frontispiece of Chap. 1 and Figure 1.9 are adapted from 
F. L. Bauer, "Andrei and the Monster - Six Lessons in Informatics", Technische Uni-
versitat Mi.inchen, Abteilung Mathematik, Report No. 7401, 1974. 
The frontispiece of Chap. 2 shows the reverse of a (never coined) medal, the design of 
which is contained in a letter by Leibniz from January 2, 1697 to the Duke Rudolf August 
of Braunschweig and LO.neburg; it is reprinted from 
C. G. Ludovici, "Ausfilhrlicher Entwurf einer vollstiindigen Historie der Leibnitzi-
schen Philosophie", Part I, Leipzig: Lowe 1737. 
Figure 2.1 is reprinted from 
F. L. Bauer, G. Goos, "lnformatik", Part I, 2nd ed., Berlin-Heidelberg-New York: 
Springer 1973. 
Figure 2.2 is reproduced with permission of the publisher from 
L. J. Hoffman, "Modern Methods for Computer Security and Privacy", Englewood 
Cliffs, N. J.: Prentice-Hall1977. 
The frontispieces of Chaps 3 and 4 are reprinted by permission of the publisher from 
SO.ddeutsche Zeitung, MO.nchen. 
The frontispiece of Chap. 5 is a reproduction of Figure 11 from Table 6 in 
J.P. C. Kent, B. Overbeck, A. U. Stylow, "Die romische MO.nze", MO.nchen: Hirmer 
1973, reprinted by permission. 

XVI 
The frontispiece of Chap. 6 is reprinted from 
Goldstine, von Neumann 1947. 
The frontispiece of Chap. 7 is reprinted from 
McCarthy 1960 (Commun. ACM 3). 
Acknowledgements 
Figure 6.19 and the table in footnote 18, p. 385, are excerpted and translated by permission 
of DIN Deutsches Institut fiir Normung e.V., Berlin from German Industrial Standards 
DIN 66001 and DIN 44300, resp. The translation has not been reviewed by DIN. 

Pythagoras (right) and Boethius (left) 
Introduction 
0.1 On the Etymology of the Word Algorithm 
Mukhammad ibn Musa abu Djafar al-Khorezmi was born about 780 in the area south of 
Lake Aral known today as Uzbekistan and died about 850. He lived in Bagdad in the 
"House of Wisdom" of the caliph al-Mamun, at the time when the principal works of 
Greek mathematicians were being translated into Arabic. His work "Kitab hisab al-'adad 
at-hindi", in Latin called "algorithmi de numero indorum", shows Indian influence in the 
terms used and in the tendency to formulate in an algebraic way. Later it was shortly 
named fiber algorithmi. The dispute which arose in the 15th century between the algorists 
who calculated by means of figures (whose technical skill originated from the algorismus-
writings of the scholastics, from translations and revisions from the Arabic) and the 
abacists who taught calculating "on the lines" originating from the Roman Abacus (whose 
influence persisted into the 17th century, and in Russia up to the present day) is shown on 
contemporary wood carvings (Fig. 0.1). 

2 
Introduction 
Fig. 0.1. An abacist and an a/gorist. From a book on the seven liberal arts by Robert Recorde, 
physician in ordinary to the king 
At the time of Adam Riese, algorithms were tasks as "difficult" as doubling (cf. Fig. 
0.2), halving, multiplication, or division with numbers written in decimal figures. Later 
more serious algebraic problems arose such as to determine the square root of a number in 
decimal form. Stifel (in the arithmetica integra, Niirnberg 1544) and Cardano (in the ars 
magna sive de regulis a/gebraicis, Niirnberg 1 545) gave algorithms for the solution of some 
higher algebraic equations. Even Leibniz speaks of the "Algorithm of Multiplication". 
~upliren 
:t'ff>ret lt!iC lm dn 34~1 SWC\)fa{ti~tn f~(t. 't~tJ 
j~m dlfo: 6cbrcib btc aaN "or bicblmad) cill 
~inicn bdntnbcrt~c& an m forbcrtft ~uvflr bic 
crtfe ~igur . .1t'o•nvt dn S4~ll>ic l>u mit cincr ~~, 
~ur fcf)rdbcnma~tllfofc~bfct'nl>cn. s:mo mit 
'"'(\)en/ fcf)rdb bic cr~c/ ~ic 4nbcr &c~a(t im 
finn. ~am4cb bupfir bic anbcr/"nb gib barml 
l>as btt &c~a(tm ~a~tlmnb fcf)rdb abmnaf!l Ne 
cr~c ~gurtwo !tuO i'or~anocntt>nb bu~lir f1m 
&i~ ~urtclijfcn!.Cc fd)rd&c gan~ au61als folg~n. 
l>c 'fJ"cmpd au,wdfcn. 
41Z3Z 
98765 
8:&-\61- 197HO 
~ 
6870!_ 
137408 
iij 
~ro&a. 
Fig. 0.2. Doubling according to Adam Riese (1574) 

0.2 How Algorithms are Characterized 
3 
With the further development of mathematics the term "algorithm" 1 attained the peculiar 
flavour of being mechanically performable, a type of work not particularly enticing for the 
mathematician. 
Such algorithms were already known in antiquity, for instance ancient Egyptian multi-
plication (cf. 1.13.1.3), Babylonian methods of solving certain systems of quadratic equa-
tions by integral numbers (according to 0. Neugebauer), or Euclid's algorithm to deter-
mine the greatest common divisor of two natural numbers, which can be found in the 
seventh Book of Elements (around 300 B.C.) and which probably dates back to Eudoxus 
(around 375 B.C.). 
With the rise of modern program controlled computers the term algorithm once again 
acquired a good name. It was recognized that the discovery of an algorithm - rather than 
its practical execution - could be a mathematical achievement (for example Rutishauser's 
qd-algorithm, 1954, and Wynn's e-algorithm, 1956) 2• The expression "algorithmic 
language" was used in 1958 by Bottenbruch. 
Today the term algorithm means "a general method of solving a certain type of prob-
lem", "playing with figures and symbols according to fixed rules", "the absolute essence 
of a routine", "a specific set of rules (i.e., a recipe) which, if followed exactly, will 
guarantee a successful result". The algorithm for "doing the sum" is a good example of 
such a procedure. The particular addition problem confronting one may never have been 
solved before, but one has no trouble solving it. We refrain from distinguishing particular 
algorithms among the abundance that we experience in our daily life, from starting a 
motor car to preparing crepes Suzette. 
Sometime beforehand "algorithms" were investigated in mathematical logic (Skolem 1923, GOdel 
1931, Church 1936, Turing 1936) in connection with the proof for the unsolvability of the decision 
problem of predicate logic proposed by Hilbert around 1920 or with the word problem of group 
theory on which A. Thue worked in 1914. 
In 1951 A. A. Markov gave the first straightforward precise definition of the term algorithm for 
character strings, avoiding the indirect way which uses a one-to-one mapping into the natural num-
bers ("Godelization"). 
What follows is a clarification of the concept algorithm, based on and motivated by 
elementary programming experience. 
0.2 How Algorithms are Characterized 
0.2.1 The word algorithm is sometimes used to mean a general instruction and sometimes 
to mean some particular execution of that instruction. Furthermore the instruction as such 
must be distinguished from its written form, which is often called program (at least in 
jargon). 
The Oxford English Dictionary states: "Algorithm: erroneous refashioning of algorism". 
2 Lonseth speaks of an "Algorithm of Hotelling" in 1947. This is to our knowledge the earliest 
mention of the word algorithm in connection with numerical methods. Neither in Householder's 
"Principles of Numerical Analysis", 1953, nor in Faddeev's "Computational methods of linear 
algebra", 1950, does the word play a prominent role. 

4 
Introduction 
Programming languages serve especially for the textual formulation of algorithms 
intended to be executed on computers 3• They vary not only notationally but also in their 
repertoire. In some programming languages the expressive power is limited deliberately, 
whereas in others a wider range of concepts is deliberately sought, depending on whether it 
is intended to simplify the (mechanical) translation or to ease the use of the programming 
language. Many of these languages are not much better than blurred mixtures. Some pro-
gramming languages are not universal, they do not provide for the description of all 
algorithms that can be described in some way. 
On the other hand - according to present-day estimation - the descriptive means of the above 
mentioned Markov algorithms are believed to be universal. A number of other independent attempts 
to reach universal descriptive means, namely description by partially recursive functions and descrip-
tion by Turing machines, have proved to be equivalent. This supports the thesis (Church) that those 
formal descriptions cover all the possibilities of the intuitive notion "computability". It is irrelevant 
here that a description by partially recursive functions seems more problem-oriented or that a 
description by a Turing machine seems more machine-oriented. 
In order to solve simpler problems by adequate means, it may be more suitable to use a 
language which is not universal, or correspondingly4 to use a machine with simpler 
mechanisms 5• For the present, however, let us consider universal machines and the totality 
of algorithms which can be executed on them. 
It should also be pointed out that undecidable problems exist, i.e. problems for which 
no algorithm can be formulated, even with the aid of universal descriptive means. 
Among these are tasks which the computer scientist cannot or should not avoid, e.g. the embedd-
ing of an arbitrary Chomsky-2-language in a Chomsky-1-language such that no derivation leads into 
a blind alley. In order to achieve practical results despite the general undecidability, usually a limited 
problem is considered, here the restriction to Chomsky-2-languages which have (m,n)-bounded 
context 6. 
For a simple example of an incomputable function over the integers (and for the 
"diagonalisation method" used for the construction of such examples) see e.g. Davis 1958, 
p. xvii. 
0.2.2 All algorithms have two characteristic properties in common, irrespective of 
changing notations and mechanisms, namely finiteness of the description and 
effectiveness. 
Finiteness of the description means that the algorithm should have a finite textual 
representation. Certain elementary components of this text designate what are called 
"steps". The "course of execution" of an algorithm is a directed graph the nodes of which 
are marked by elements from the finite set of "steps". 
3 From the immense number of programming languages just a few are mentioned which are 
interesting in their historical development: Zuse's "Plankalkiil" (1945), FORTRAN (1956), 
ALGOL (1958, 1960), LISP (1961), APL (1962), EULER (1966), SIMULA (1967), ALGOL 68 
(1968), PASCAL (1970). 
4 Obviously, there is a correspondence between abstract machines and the classes of algorithms 
which they can execute, and therefore also between abstract machines and classes of programming 
languages. 
5 e.g. to use a push down automaton or a finite automaton instead of a Turing machine. 
6 See for example F. L. Bauer, J. Bickel (eds.): "Advanced Course on Compiler Construction". 
Lecture Notes in Computer Science Vol. 21, 2nd ed., Springer 1976. 

0.2 How Algorithms are Characterized 
5 
Effectiveness then means that each of these "steps" in each course of execution must be 
mechanically performable. 
(Counterexample: «If a given infinite sequence of 0 and 1 (defined by a finite protocol) 
is the binary fraction representation of a transcendental real number, take 1, otherwise 
take 0».) 
0.2.3 A property which is of interest for theoretical as well as practical reasons is 
termination: the algorithm comes to an end in a finite number of steps (terminating 
algorithm). 
Non-terminating algorithms defining 'computable real numbers' (Borel1912) have been studied 
by Myhill1953. A non-terminating algorithm for the computation of the (transcendental) number e 
goes back to Lambert 1766: Starting with A 0 = 1, A 1 = 2 and B0 = 0, B1 = 1, compute 
Ai+l = (4X i + 2) X A;+ A;_ 1 and Bi+l = (4 Xi+ 2) X B; + B;_ 1 
. 
. 
3 
19 
193 
2721 
49171 
and form the ratiOnal numbers (A; + B;)I(A; -
B;). I.e. 1' 7' 71' 1001 • 18089 • · · · 
This sequence (which converges faster than the usual Taylor series) gains more correct decimal 
digits at each step than at the step before. It is not difficult to formulate an algorithm which produces 
successively the decimal digits of the decimal fraction. 
Euclid's proof of the existence of an infinite number of primes can be interpreted, too, as a 
nonterminating algorithm generating as many primes as one wants. 
Another property is that of determinism: the course of execution (not necessarily 
linearly ordered steps) is uniquely specified. 
Nondeterministic algorithms were first introduced in automata theory (Rabin, Scott 1959). 
As an example of a nondeterministic algorithm we consider the insertion of a given element x into 
a sorted sequence yielding another sorted sequence. If the sequence is empty we simply return the 
element. Otherwise we decompose the sequence into a left part u, an element t, and a right part v; this 
decomposition can be done in an arbitrary way, and it is at this point that the nondeterminism arises. 
Now xis compared with t and, depending on the result, inserted either into u or into v by the same 
procedure. 
We note that by choosing specific decompositions of s we obtain different sorting strategies, such 
as linear (straight) or binary sorting. 
Efficiency must be distinguished from effectiveness. It can be vaguely expressed like 
this: One algorithm is more efficient than another if it accomplishes the same with less 
effort. Effort might be measured in terms of the number of comparable steps, but it can 
have other aspects such as storage space needed for execution by certain machines. 
(Example: Cramer's rule is less efficient for a 10 x 10 system of equations than the 
Gaussian algorithm.) 
Efficiency is simply a question of practicability -
however a very important one. 
Sometimes there is an effective solution to a problem which is not at all an efficient solution. 
There are (surprisingly) even problems which have an effective solution, which is nevertheless in 
practice not workable, e.g. the chess problem "Can White play to win, even if Black plays in the best 
possible way?" 
In order to solve this problem (Knuth 1973) the set of all games is represented as a tree. Each node 
contains the position and, as a unique characterization, the succession of moves which leads to it (and 
therefore the graph is free of cycles). Each node in addition only has a finite number of successors. 
Only a limited number of moves exist because of the tie rule "three times the same position with the 
same player's turn to move". 

6 
Introduction 
This tree is marked as follows: 
1. Mark all terminal nodes that represent a winning position for White (when it is Black's turn). 
2. Repeat as long as this changes the marking: Mark a node among the unmarked nodes 
2a. if at least one of its successors is marked (in case of White's turn) or 
2b. if all of its successors are marked (in case of Black's turn). 
If after termination of the marking algorithm the root of the tree is marked, the answer is "yes", 
otherwise "no". 
This algorithm is probably not the most efficient one 7 but we must face the expectation that any 
more efficient or the most efficient algorithm - if the like exists -
is not executable in practice by 
any concrete machine. In such a case we say an algorithm can be executed "in principle" - it can be 
executed by a Gedanken machine. The algorithm for a typical chess problem position as shown in 
Fig. 0.3 can even be performed in a practical sense. Fig. 0.4 shows the tree with marked terminal 
nodes, Fig. 0.5 after conclusion of the marking algorithm (according to Zagler). 
The marking makes it possible to obtain the strategy tree for White (Fig. 0.6). 
It can happen that the inverse of a (one-to-one) mapping is far more awkward to cal-
culate than the mapping itself. An example of such a "trapdoor" is the multiplication of 
two prime numbers. It takes only fractions of a second to multiply two prime numbers 
with 30 decimal digits (equivalent to 100 binary digits) on present-day machines, whereas 
the factorization of the product into the two prime factors would take billions of years, 
using classical methods, and it is open whether much more efficient general methods 
exist. 8 
Problems of this type are dealt with in complexity theory, which is outside the scope of this book. 
The last example shows that through using a one-to-one mapping no information is lost but that 
an unpractical representation can evolve. The one-to-one mapping of finite sequences of natural 
numbers into the natural numbers 
which is often used for theoretical purposes ("GOdelization"), only allows the retrieval of the original 
information in a very inefficient way. 
0.2.4 For practical purposes, the notational appearance of algorithms cannot be neglected. 
For the construction and application of programs or, as we would prefer to put it, for 
8 
7 
6 
l 
5 
4 
~ 
3 
2 
a 
b 
c 
d 
e 
g 
Fig. 0.3 
7 Its execution on the fastest computers available today would take many orders of magnitude 
longer than the lifetime of the solar system. 
8 For a study of some sophisticated number-theoretic methods and for further literature, see 
Schnorr 1980. 

0.2 How Algorithms are Characterized 
7 
g4~e5 
h5~ 
~a6 
gh5~, 
A, { ) 
( J'A\ ff\ l\ l ;rt·~ 1 
h6 e5 
g6 
e5 
g6 e5 
g5 gh5: g5 
g6 e5 g5 gh5: 
g5 gh5: g5 g5 gh5: 
I h3~ h)\6 l )4 h3~/ )4l 11 \ hA/ 1 
h( ~ 
(A f~A ~(~ ~~ ~~ 
f'~~ 
g7 e5 g7 e5 
g6 
g6 g7 e5 
g6 g6 
g6 
g6 g7 e5 
g6 g6 
g6 
g6 
g6 g6 
l AJ 'A ~l) ~A i 
~ ') i ~ 
A~ i ~ 
h2 a6 
h3h2 a6 h3 
h3 h3h2 a6 h3 
h3 
h3 h3 h3 
h2 a6 h3 
h3 h3 
~ ~ 
~~ ~ ~ 
~ H ~ ~ 
~ 
~ ~ ~ 1 ~ ~ 
~ ~ 
g7 
g7 
g7gi 
g7 g7 
g7 g7g7 
g7 g7 
g7 
g7 g7 g7 
g7 
g7 g7 
g7 g7 
~ ~ u ~ ~ 
~ u ~~ 
~ 
~ ~ ~ ~ ~ ~ ~ ~ 
Fig. 0.4 
systematic program development, it is of paramount importance that formulations of 
algorithms be humanly legible. 
However, a definition of the term algorithm to be used for theoretical investigation "must simply 
be such that it can be handled formally in an easy way, so that statements regarding the term 
algorithm itself can be made in a simple way. On the other hand the algorithms which are to be 
written with this precision do not need to be perfectly 'legible'" (Eickel1974). This applies to Turing 
machines and Markov algorithms. 
For this reason Turing machines and Markov algorithms cannot be used as a base if the practical 
aspects of algorithmic languages are not to be neglected. "Turing machines are not conceptually dif-
ferent from the automatic computers in general use, but they are very poor in their control structure . 
. . . Of course, most of the theory of computability deals with questions which are not concerned with 
the particular ways computations are represented. It is sufficient that computable functions be 
represented somehow by symbolic expressions, e.g. numbers, and that functions computable in terms 
of given functions be somehow represented by expressions computable in terms of expressions 
representing the original functions. However, a practical theory of computation must be applicable to 
particular algorithms. The same objection applies to basing a theory of computation on Markov's 
normal algorithms as applies to basing it on properties of the integers; namely flow of control is 
described awkwardly" (McCarthy 1961 ). 
Attempts to base a strict definition of the term algorithm on production systems of formal 
languages would also fail to conform to the methodology of this book. 
We will therefore base the definition of the term algorithm on the theory of recursive 
functions, however not in the form of the original Church-Kleene formalism. " ... both the 
original Church-Kleene formalism and the formalism using the minimalization operation 

8 
Introduction 
Fig. 0.5 
use integer calculations to control the flow of the calculations. That this can be done is 
noteworthy, but controlling the flow in this way is less natural than using conditional ex-
pressions which control the flow directly" (McCarthy 1963). As a base we will assume the 
if-then-else construct introduced by McCarthy in 1959 which has already influenced 
ALGOL 60. 
For the equivalence with "partially recursive functions" see McCarthy 1961. 
Incidentally it would be wrong to concentrate only on the requirement that the 
algorithms can be formulated easily and clearly. It should also be possible to handle 
algorithmic formulations formally in a simple manner, in order to perform the program 
transformations on which evolutionary programming is based. 
0.3 Programming as an Evolutionary Process 
The requirements of finiteness of description and of effectiveness mean that an algorithm 
can "in principle" always be executed mechanically, and that includes by a person. The 
kind of machine that is used will depend on the type of the algorithm. At any rate when 

0.3 Programming as an Evolutionary Process 
9 
~·~ 
~h5 
a6' 
95 
e5 
h4Aa6 
)5 
A ) 
~ 
96 
e5 
/6 e5 
95 
h3/{ 
h~6 h4 1 
J4 
tAf)A~ 
~ 
9 7 e5 
9 7 e5 
96 
96 9 7 e5 
96 
96 
ff A! 'A ~H ) 
~ 
~ 
h2 a6 
h3 h2 a6 h3 
h3 h3 
h3 
1 ~ 
~~ ~ ~ 
~ ~ 
~ 
97 
97 
9797 
97 97 
97 97 
97 
~~ u~~ ~~ 
~ 
Fig. 0.6 
beginning to solve a problem a different type of machine is often assumed, a different style 
of formulation is used ("thinking in another machine") from that which is used in reality 
in the final stage. From a problem-oriented formulation we pass to a machine-oriented one 
or in other words from an abstract machine to a concrete machine. 
Practical programming should comprise the (stepwise) development of an algorithm 
from the problem-oriented to the machine-oriented version. Only very seldom will the 
problem-oriented version already be the machine-oriented one. However, it is to be expect-
ed that even in the initial solving of the problem an inconscious transition takes place from 
a truly problem-oriented formulation to one for which a formal language exists (more 
precisely: to a formulation within the framework of an acquired programming language). 
This is in particular to be expected if inadequate programming languages such as 
FORTRAN or BASIC have narrowed conceptualization. 
Program development is an evolutionary process starting from a (possibly non-opera-
tional) formulation of a problem, with the following three goals 
(1) to obtain an (operational) algorithm 
(2) to refine the algorithm with respect to the capabilities of a concrete machine 
(3) to improve the algorithm with respect to efficiency. 
In order to reach these goals, a number of steps are usually required which are not neces-
sarily independent of each other. 
Several examples of program development will be pursued in this book. This will de-
monstrate the wide spectrum of language that is used in practice in formulating 

10 
Introduction 
algorithms. In general, simultaneous refinement both of object and of operation structures 
(computational structures, see Chap. 3) is often made, for example if transition to another 
object structure allows operational amelioration. 
Normally, the end point of program development today (still) is marked by the com-
pletely binary organization of the von Neumann machine - that is, circuitry. 9 Circuitry, 
however, does not have to be organized in the way the sequential, stored-program machine 
does it. Pretty soon, other machine architectures, relying more on data flow than the 
classical von Neumann machine, may come into competition. Program development thus 
is open-ended, its methods should be flexible enough to adapt to different, technology-
dependent machine styles. 
0.4 How to Solve it 
True problems are problems without obvious solutions. Of course, how a solution can be 
reached remains a question. Often a brainwave, an idea, more generally, intuition leads to 
an algorithm. 
Example: can a mutilated chessboard (Fig. 0. 7) 
[[] 
Fig. 0.7 
be covered with 31 dominoes? 
The answer is found unexpectedly if the board and also the dominoes are given a black 
and white colouring (Fig. 0.8). 
Fig. 0.8 
The new problem can be answered in an elementary way: since there are 32 black and 
30 white fields, there is no way to establish with 31 dominoes the checkerboard pattern. 
Now assume the original problem would have a solution. Then the original dominoes 
9 The fact that the evolutionary program development leads as far as to circuitry is expressed by the 
catchword "Uniformity of software and hardware" (Wiehle 1973). 

0.4 How to Solve it 
11 
could be coloured to show the checkerboard pattern, and each domino would have a black 
and a white field. Thus we would have a solution to the new problem. Therefore, the 
original problem has no solution. 
In this case, too, a change of object structure is undertaken. The refinement of the 
objects leads to the idea for a solution. 
More often than one expects the solution is already concealed in the problem specifica-
tion - not only in «calculate 3 x 4» but also in «subtract b from a provided a ~ b, i.e. 
find that particular x for that add(x, b) = a» (comp. 1.10). A solution is within reach if 
one starts asking oneself what addition means. Giving a recursive definition for add leads 
to a recursive solution for the original problem as well (comp. 1.11). 
Often the solution can be obtained constructively if all that is given is its existence and 
uniqueness - one has, however, to look for the solution as in a picture puzzle. As every-
body who has tried knows: To solve a problem needs both experience and skill, intuition 
and ingenuity. 


Chapter 1. Routines 
A recursive situation (Hommage a Saul Steinberg) 
"Mathematical formulation allows us to remain much 
further from the computer than would otherwise be 
the case, and in this context any programming 
language is already too near." 
Griffiths 197 5 
In order to obtain a general concept of "algorithm", routines are introduced in this 
chapter and their construction is investigated. Recursive routines and systems of routines 
require special attention. For the moment it is of little importance of which mode and type 
the objects of such routines are. They are assumed to be given as primitive sets of objects 
together with certain characteristic primitive operations ("primitive computational struc-
tures"). The construction of sets of objects and the potential inner structure of objects will 
be dealt with in detail in Chaps. 2 and 3. 
Important concepts in this chapter are parameters, the principle of substitution, recur-
sion and suppression of parameters. The constructs in consideration here (level of "ap-
plicative", "functional" formulation) are devoid of explicit control elements (a "natural 
sequentialization" is determined solely by the principle of substitution) and accordingly 
devoid of variables in the sense of programming languages, they show Quine's "referential 
transparency" (Quine 1960). The introduction of program variables and control elements 
is postponed until Chaps. 5 and 6. 
1.1 The Parameter Concept 
If we consider well-known formulas from a compendium such as, for instance, the calcula-
tion of the volume of a truncated cone 
we will find that designations of different nature appear: n or better still -j- (taken as one 
symbol, compare h in quantum physics) denotes a certain (irrational) real number, and is 

14 
1. Routines 
therefore a constant, whereas r, Rand h can vary. These "variables" (in the mathematical 
sense) are called the parameters of the formula. The formula describes a computation -
an algorithm. It is what we call a routine. 
The parameterization of a problem makes it possible to-state a general solution in the 
form of a routine, instead of listing a catalogue of the solutions for all desired instances. 
Parameterization creates generality and is therefore a worthwhile investment. Its im-
portance as a method of solution is dealt with later (1.4). 
In certain applications of the above formula h may be kept constant, then only rand R 
are parameters. Such different interpretations of a formula make it necessary that para-
meters be marked as such. This problem was recognized in its depth by Church 1941 
(Lambda-calculus) who introduced as an abstraction operation the notation of marking a 
designation as a parameter. A listing of the parameters is placed in front of the formula, 
the body of the routine, for example 
(real r, real R, real h) real: f x h x (r2 + r x R + R 2). 
Here real indicates the object set IR of numerically real numbers. 
r, Rand hare user-coined, freely chosen designations. By marking them as parameters 
they are bound in the respective routines, i.e. their meaning is restricted to the routine as 
b 
their scope or range of binding (compare "integration variable" xin JJ(x)dx, or the use of 
xin {x E IN: x2 :::; 73}and 3X E IN: x = 2x2 -
3). 
In 
(realr, reaiR) real: f x h x (r2 + r x R + R 2), 
the designations f and h are not bound. h is non-local for the routine, it is also freely 
chosen but bound "further outside", f is a standard denotation for a fixed object. 
The free choice of parameter designations means that they may be replaced within their 
range of binding (consistently) by any other designations. For example, it follows through 
consistent substitution of the parameter designations that (int indicates the object set 7L of 
integers) 
(int a, int b) int: a - b 
and 
(int b, int a) int: b -
a 
represent the very same routine, whereas in 
(int b, int a) int: a - b 
the parameters (and not only their designations) are interchanged. However, as only the 
order of the parameters is changed and thus can be compensated by a suitably changed ap-
plication, the last routine is still considered to be essentially the same as the others. On the 
other hand the two routines for integers 

1.1 The Parameter Concept 
(int a, int b) int: (a + b) x (a -
b) 
(int a, int b) int: a2 -
b2 
15 
are not equal but only (functionally) equivalent. They represent different algorithms which 
define the very same function. 
The concept of a parameter is of course not restricted to the case that the routine is 
operatively formulated, for example 
(real a) int: «the greatest integer that does not surpass a». 
Designations not marked as parameters, such as h in 
(real r, real R) real: f x h x (r2 + r x R + R 2 ) 
would have to be called, according to mathematical usage ("free variable"), "free 
designations". This would result in the almost paradoxical diction that bound designations 
can be freely exchanged, whereas free designations cannot. For didactical reasons the word 
"free designation" is avoided and the word "constant" is used instead. 
In the preceding discussion the mapping character of a routine f emerges: Routines 
define junctions. In mathematical jargon our example would be written thus, 1 
f: (r, R, h) r+ f X h X (r2 + r X R + R 2) 
and as a supplement the domain and range would be stated as 
Correspondingly, we have supplemented the parameter list (r, R, h) by a list of object sets, 
from which the parameters and the result are taken, and combined these in the heading 
(real r, real R, real h) real: 
which prefixes the body of the routine. 
The mapping type of the routine is characterized by its functionality, 
funct (real, real, real) real 
funct (int, int) int 
funct (real) int 
corresponds to 
rR x 
rR x 
rR __. rR 
corresponds to 
7L x 7L --> 7L 
corresponds to 
rR --> 7L 
real, nat, int etc. denote object sets, also called modes (or "sorts", "types"). 
The routines above are said to be, according to their number of parameters, ternary, 
binary, unary. Constants like 0, 1, 2, n are nullary routines, f is of functionality 
funct real 
which corresponds to __. rR. 
In the "type-free" notation of the Lambda-calculus of Church our example would read 
A.f.R h. f X h X (r2 + r X R + R 2). 

16 
1. Routines 
The call, i.e. the application of a routine to a list of objects called arguments can be ex-
pressed by appending this list of arguments, for example 
((real r, real R, real h) real: f x h x (r2 + r x R + R 2)) (0.19, 0.26, 9.6) 
((int a, int b) int: (a+ b) x (a- b)) (17, -8) 
((real a) int: «the greatest integer that does not surpass a») (29.893) 
This means that the ("formal") parameters are to be substituted respectively in their order 
by objects, the ("actual") arguments, wherever they appear (in the body) 2• 
For the present, it should remain undecided whether substitution takes place before 
execution of the body or during execution "as required". Different methods will be dis-
cussed later in more detail as "computation rules". However in the sequel we will pay 
attention as to whether we can choose one of these possibilities freely (if the result is inde-
pendent of the kind of execution) or whether there are restrictions. 
1.2 Declaration of a Routine 
In order to be able to use a routine with ease it is also given a freely chosen designation (like 
fin the above mapping). This is done by a declaration such as 
functf"' (real r, real R, real h) real: f x h x (r2 + r x R + R 2) 
or 
funct squdiff = (int a, int b) int: (a + b) x (a -
b) 
or 
funct round =(real a) int: «the greatest integer that does not surpass a» 
For the above cases a call would then simply read 
f(0.19, 0.26, 9.6), squdiff (17, -8), round(29.893) 
and the substitution of the (formal) parameters by (actual) objects yields the nullary routines 
real: f x 9.6 x (0.192 + 0.19 x 0.26 + 0.262) 
int: (17 + ( -8)) x (17 - ( -8)) 
int: «the greatest integer that does not surpass 29.893» 
2 The term call comprises the application of a routine both in functional and in operational nota-
tion. Operation is used particularly when a routine is applied in a bracket-free or an infix notation. 
We will consider such notations (see below) only as notational variants of the (completely 
parenthesized) functional notation. 

1.3 Hierarchical Construction of Routines 
17 
In the first example it is presupposed 3 that . + . and . x. are associative binary routines 
with the functionality (real, real) real, that . 2 is a unary routine with the functionality 
(real) real, and that 0.19, 0.26 and 9.6 denote objects of the mode real. As these standard 
designations for arithmetic operations are also used with other functionalities, in which 
e.g. real is substituted by int, the above notation alone (which is brought about simply by 
substitution) is not complete. The following would be complete 
real: (real: f) x (real: 9.6) x (real: (real: (real: 0.19)2) + 
(real: (real: 0.19) x (real: 0.26)) + 
(real: (real: 0.26)2)) 
Of course we will try to eliminate superfluous notation wherever the context shows clearly 
what should be supplied 4• 
1.3 Hierarchical Construction of Routines 
In constructing a routine one generally bases the construction on other routines. In the 
previous example, 
funct squdiff = (int a, int b) int: (a + b) x (a -
b) 
is based on the routines . x. , . + . and . - . . 
funct heron = (rat a, rat b, rat c) rat: 
s(a, b, c) x (s(a, b, c) -
a) x (s(a, b, c) -
b) x (s(a, b, c) -
c) 
is based 5 not only on . x. and . - . but also on another routine s, yet to be declared such 
that s(a, b, c) gives the half circumference of a triangle having the sides a, b, c. 
1.3.1 Primitive Routines and Computational Structures 
A routine A is directly based on a routine B if in the text (of the body) of A a call of B 
appears. A is based (indirectly) on B if A is based directly on a routine C which again is 
directly or indirectly based on B. Routines not (yet) specified by a declaration 6 and 
therefore not based further on other routines are called primitive routines 7• 
By using dots we indicate that (and how) the call deviates from the (completely parenthesized) 
functional notation. 
4 This can be exaggerated as shown in ALGOL 68. 
5 rat x stands for x e <Q, see 3.4.5. 
6 It is presupposed that the main aspects of their meaning (for the level of program development in 
question) are pragmatically clear (pragmatical point of view) or that the meaning ("semantic 
interpretation") remains open (formal point of view). 
7 In this connection Dijkstra speaks of' "ready-made" arithmetic operations' (Dijkstra 1969). 

18 
1. Routines 
Let us call a unit made up of sets of objects and accompanying routines a computa-
tional structure. The most prominent examples are the computational structure IN of 
natural numbers and the computational structure 7L of integral numbers. 
For IN and 7L as computational structures . x., . + . and . - . designate binary 
primitive routines (operations) for numbers, -. designates in addition a unary routine for 
numbers, 0 and 1 denote each an object (the number »zero«, the number »one«) which is 
also obtained by a nullary primitive operation, a parameterless primitive routine. The 
symbol t denotes a primitive computable object from IR (which by the way cannot be com-
pletely written down in any radix system, but can be "obtained" by a non-terminating 
algorithm). 
The declaration of a routine presupposes that a primitive computational structure is 
given; for its objects an additional operation, based on the primitive operations, is defined 
by the routine. Such an enlarged unit of operations together with the given primitive 
objects can form again a computational structure. This will become clearer in Chap. 3. 
If the example heron is supplemented by the declaration of the routine s, the following 
system of (two) routines is obtained. 
funct heron = (rat a, rat b, rat c) rat: 
s(a,b,c) x (s(a,b,c)- a) x (s(a,b,c)- b) x (s(a,b,c)- c) 
funct s = (rat u, rat v, rat w) rat: (u + v + w)/2 
where now the routine s is hierarchically subordinated to heron. 
Primitive operations are the binary associative operations . x. and . + . , the binary opera-
tions . -. and ./., and the nullary operation 2, the object »two« (unless it is preferred to 
understand ./2 as a unary operation «half», in which case it is better to write it as . /2). 
Table 1.3 .1 gives a survey of computational structures which for a while are presuppos-
ed to be primitive, according to their general usage. (Some of them can be further 
explained in terms of other computational structures. For details about this and about the 
abstract definition "computational structure" see the next sections and Chap. 3.) 
1.3.2 The Principle of Substitution 
The fundamental tool for the construction of a routine A from (primitive or declared) 
routines is the 
Principle of Substitution: In the body of a routine A every parameter position of a call of a 
routine can be occupied by a parameter of A or again by a call of a routine. 
Such an expression can be represented by a Kantorovic tree (following L. V. Kanto-
rovic, 1957) which is obtained in a linear representation, if (completely bracketed) func-
tional notation is used. For example the above routines would result in the Kantorovic 
trees of Fig. 1.1. Note: In view of associativity, for example . x. x. is regarded as a single 
(ternary) operation. 

1.3 Hierarchical Construction of Routines 
Table 1.3.1. Computational structures 
1. Universal objects and universal operations 
1.1 Computational structure with bool (Boolean lattice 182) 
Object set 
boo I 
Distinguished elements 
boo I 
T 
true 
F 
false 
Operations 
(bool) bool 
l• 
not 
(bool, bool) bool 
.A. 
and 
.v. 
or 
Predicate 
(bool, bool) bool 
.<. 
It 
(truth values »true«, »false«) 
(»true«) 
(»false«) 
(negation) 
(conjunction) 
(disjunction) 
(less predicate, false < true) 
1.2 Universally available predicates (in all computational structures) for arbitrary mode 11 
(JI. !1) bool 
eq 
(equality predicate) 
. *. 
ne 
(inequality predicate) 
1.3 Universal pseudo object of any mode 11 
!1 
Q 
(pseudo object »undefined«) 
2. Computational structures which are frequently presupposed as primitives 
2.1a Computational structure with char (linear ordering Y, ordinal numbers 1., 2., 3., ... ) 
Object set 
19 
char 
((arbitrary) finite non-empty character 
set, linearly ordered: finite alphabet) 
Distinguished elements 
char 
Operations and Predicates (see 2.1 b) 
a 
w 
2.1b Computational structure with bit (ordinal number 2.) 
Object set 
bit 
Distinguished elements 
bit 
0 
L 
Operations and Predicates for 11 = char, bit 
(!1) !1 
succ. 
(!1, 11> bool 
pred. 
·~· 
·~· 
.<. 
.>. 
succ 
pred 
le 
ge 
It 
gt 
(first character) 
(last character) 
(two-element alphabet) 
(first character) 
(last character) 
(successor, »undefined« for w, L) 
(predecessor, »Undefined« for a, 0) 
(less-or-equal predicate) 
(greater-or-equal predicate) 
(less predicate) 
(greater predicate) 
2.2a Computational structure with nat (ordered commutative semiring IN) 
Object set 
nat 
Distinguished elements 
nat 
0 
1 
2 
(natural numbers including zero, 
linearly ordered) 
(»zero«) 
(>>one«) 
(>>tWO«) 

20 
1. Routines 
Table 1.3.1 (continued) 
Operations and Predicates 
as for char, but succ totally defined, 0 corresponds to a, w is missing; in addition 
(nat, nat) nat 
. +. 
add 
(sum) 
(nat) nat 
(nat) boo! 
(nat, nat) bool 
sub 
. x. 
mult 
.div. div 
.mod. mod 
.1. 
. x2 
dup/ 
. /2 
med 
sq 
V. 
sqrt 
odd. 
odd 
even. even 
·I· 
meas 
(difference, m - n with m < n is 
»undefined«) 
(product) 
(quotient, . div 0 is >>Undefined«) 
(remainder, . mod 0 is >>Undefined«) 
(like div, but >>Undefined«, if 
remainder is unequal to zero) 
(doubling, "duplication") 
(halving, ,mediation", 
n /2, >>Undefined« for odd n) 
(square) 
(square root, partially defined) 
(odd predicate) 
(even predicate) 
(divides predicate, measures) 
2.2b Computational structure with int (ordered commutative ring :&') 
Object set 
int 
Distinguished elements, Operations and Predicates 
as for nat, but pred and sub totally defined, in addition 
(int) int 
minus 
sign. sign 
(int) nat 
1·1 
abs 
(integers, linearly ordered) 
(negativum) 
(signum) 
(absolute value) 
2.3 Computational structure with sequ 11 (free semigroup 11* over 11: commutative semigroup with 
neutral element) 
Object set 
sequ 11 
(sequences, strings, words of objects 
of mode ,._, lexicographically linearly 
ordered, if 11linearly ordered) 
Distinguished element 
sequ 11 
¢ 
empty 
(empty sequence) 
Operations and Predicates 
(sequ ,.., sequ 11) sequ 11 
.&. 
cone 
(concatenation) 
(sequ 11) 11 
top. 
top 
("top", "leftmost", "first" element, 
»undefined« for ¢) 
( sequ 11) sequ 11 
rest. 
rest 
(right remainder, >>Undefined« for ¢) 
( sequ ,.., 11) sequ 11 
append 
(appending an element "top", "left", 
"front") 
(sequ 11) 11 
bottom. 
bottom 
("bottom", "rightmost", "last" 
element, >>Undefined« for ¢) 
( sequ 11) sequ 11 
upper. 
upper 
(left remainder, >>Undefined« for ¢) 
( sequ ,.., 11) sequ 11 
stock 
(appending an element "bottom", 
"right", "behind") 
(sequ 11) nat 
1·1 
length 
(length) 
(sequ ,._, sequ 11) bool 
.lp. 
/part 
(left-part predicate) 
.rp. 
rpart 
(right-part predicate) 
·~· 
le 
(less-or-equal predicate) 
·~· 
ge 
(greater-or-equal predicate) 
.<. 
It 
(less predicate) 
.>. 
gt 
(greater predicate) 

1.3 Hierarchical Construction of Routines 
f 
11\ 
r R h 
squdiff 
1\ 
a 
b 
heron 
/I\ 
a b c 
s 
-
/I\ 
u v w 
/i\ 
f 
h )j\ 
2 
2 
I I\\ 
r 
r 
R R 
~~ 
s 
.-. 
.-. 
.-. 
/1\ /\ /\ 1\ 
abc 
sa 
sb 
sc 
/1\ /I\ /1\ 
abcabcabc 
. /. 
oder 
./2 
/\ 
I 
.+.+. 2 
.+.+. 
/I\ 
/I\ 
u v w 
u v w 
21 
Fig. 1.1 
Kantorovic trees can be looked at as abbreviated calculation forms (for an example see 
Fig. 1.2). Thus, they represent the data flow - what is commonly called data flow dia-
grams, are special Kantorovic trees. 
R 
R 
Fig. 1.2 

22 
1. Routines 
The principle of substitution can be illustrated by the possibility of textual replacement 
("copying", "direct insertion") of a routine. For example, from heron and s one obtains 
funct heron ""(rat a, rat b, rat c) rat: 
(a + b + c)/2 x ((a + b + c)/2 - a) x ((a + b + c)/2 - b) x 
((a+ b + c)/2 - c)) 
One says, s has been eliminated; it is the designation s which disappears. (The converse 
case, the introduction of routines with freely chosen designations as a structuring tool, will 
be discussed in 1.13). 
1.3.3 Alternatives 
A further important element -
apart from the principle of substitution -
for the 
construction of routines is the binary branching or alternative, in which one of two objects 
(of the same mode) is chosen, depending on a truth value (true or false) from the 
primitive object set boo!. The alternative can be introduced as a universal, i.e. generally 
valid and therefore primitive ternary operation with the specification (for arbitrary mode Jl) 
funct (boo!, "' Jl) 11 
and the mapping property8 
(b 
) 
[ x, if b = true 
,x,y r-> 
y, if b = false 
The usual notation for the alternative reads 
if >truth value< then >yes-object< else >no-object< fi 
where the operands >truth value< as well as >yes-object< and >no-object< can be given 
(according to the principle of substitution) as parameters or as (results of) calls of one 
routine with the result mode boo! and of two routines with the coinciding result mode Jl· 
An expression which contains an alternative is called a conditional expression. 
Example: calculation of the absolute value 
funct abs "" (int a) int: 
if a ~ 0 then a 
else -a fi 
Routines which yield truth values (and are used for branching) are called Boolean 
routines, recognition routines, their bodies are Boolean expressions or predicates. We 
8 Note that this selection operation is independent of y, if b = true and independent of x, if b = 
false -
this property will be of importance in 1.5. 

1.3 Hierarchical Construction of Routines 
23 
assume that the test whether two objects are equal or not exists as a universal Boolean 
routine (see 2.4). Likewise we assume the computational structure of the truth values to be 
universal. Table 1.3.1 gives a survey of universal objects and routines. 
By definition we have the fundamental transformation rule for alternatives: 
if 
>truth value< then >yes-object< else >no-object< fi 
is equivalent to 
if ..., >truth value< then mo-object< else >yes-object< fi 
For the frequently occurring nesting of alternatives 
if >truth value 1 < then >yes-object< 
else if >truth value 2< then mo-yes-object< 
else mo-no-object< 
fi fi 
there is the abbreviated notation of sequential branching 
if >truth value 1 < then >yes-object< 
elsf >truth value 2< then mo-yes-object< 
else mo-no-object< 
fi 
The simplification, which means the introduction of a right-associative symbol elsf 
instead of else in order to save if-fi-brackets, can also be iterated. 
For Boolean objects a, b 
a "' b (sequential disjunction, conditional disjunction) 
is shorthand for if a then true else b fi and 
a ;;. b (sequential conjunction, conditional conjunction) 
stands for if a then b else false fi 
(McCarthy 1960). 
It is also advisable to write 
a ,;. b (sequential subjunction) 
short for ..., a "' b, i.e. if a then b else true fi . 
1.3.4 Input/Output 
From a functional point of view input and output operations are not special operations. 
For the moment it is sufficient to assume that if a person or a machine performs a certain 
routine, the parameter heading of that routine is to be considered as the "incentive to input 
arguments of the required modes", moreover that on termination of the routine the 

24 
1. Routines 
respective result "is to be made visible". If tables or columns are involved, one has to work 
with sequences, see Table 1.3.1. If such a sequence acts as a parameter, its consecutive ele-
ments can be worked with, using top and rest. Resulting sequences are built up using 
append. Examples (e), (f) and (g) in the next section illustrate this. 
1.4 Recursive Routines and Systems 
A routine can not only be based on other routines but also on itself, following the general 
method of reduction of a problem to a "simpler case" of the same problem. We call a rou-
tine recursive if it is based directly or indirectly on itself. A system of routines is called 
recursive if it includes at least one recursive routine. It is not possible to eliminate a directly 
recursive routine by textual substitution. 
Intuitively speaking, every recursive call of a routine can be interpreted as establishing 
a new instance of a calculation form for that routine (a new incarnation - the term was 
introduced by Dijkstra 1960). 
The property of termination which is naively desirable for algorithms is no longer 
obvious in recursive routines. The vicious circle of (infinite) recursion can be avoided by 
means of branching. Thereby a call of a routine terminates if it causes only a finite number 
of further calls of the same routine or other (recursive) routines. A routine terminates 
when every call terminates. 
1.4.1 Examples 
The following are examples of recursive routines and systems. 
(a) the "classical" definition of the factorial 
funct fac = (nat n) nat: 
if n = 0 then 1 
else n x fac(n -
1) fi 
with equality, subtraction (actually only test on zero and predecessor function), multiplica-
tion, 1 and 0 as primitives. The termination can easily be shown here: A callfac(m) for a 
natural number m causes exactly m further calls ofjac (induction over m). Fig. 1.3 shows a 
calculation sheet for fac. Note that the computation proceeds in establishing m + 1 
individual calculation sheets for fac(m), fac(m -
1), ... , fac(1), fac(O). 
(b) a routine gcd for the computation of the greatest common divisor within the system 
(gcd, mod) with test on zero . =0, less predicate . <. and subtraction . -. as primitives 
funct gcd = (nat a, nat b) nat: 
if b = 0 then a 
else gcd (b, mod(a, b)) fi, 
funct mod = (nat a, nat b) nat: 
if a < b then a 
else mod (a - b, b) fi 

1.4 Recursive Routines and Systems 
25 
n 
n 
I 
I 
Fig. 1.3 
It is obvious that mod terminates if and only if b =1= 0; gcd is always terminating. 
Exercise 1: Prove for natural numbers m, n, with m ~ n andfk ;;;; n < fk+l• wheref;, i ~ 1, are the 
Fibonacci-numbers 
that the call gcd(m, n) causes at most k other calls of gcd (G. Lame 1844). 
(c) The related system 
funct gcd "' (nat a, nat b) nat: 
if b = 0 then a 
else gcd(b, mod(a, b)) fi, 
funct mod "'(nat a, nat b) nat: 
if lt(a, b) then a 
else mod (sub(a, b), b) fi, 
funct sub "' (nat a, nat b) nat: 
if b = 0 then a 
else sub(pred(a), pred(b)) fi, 
funct It "' (nat a, nat b) bool: 
if b = 0 then false 
else if a = 0 then true 
else lt(pred(a), pred(b)) fi fi 

26 
1. Routines 
with test on zero and predecessor function pred as the only primitives. This system is 
"deeper founded" in comparison to example (b). 
The termination proof is the same as the one for (b) with the addition that each call 
lt(a, b) causes min(a, b) further calls of it. Likewise, sub(a, b) terminates provided lt(a, b) 
yields false. 
(d) a system of recognition routines, which determines whether natural numbers are odd or 
even 
funct odd "' (nat n) boo I: 
n 
=1= 0 A even(pred(n)), 
funct even "' (nat n) bool: 
n =1= 0 ,;. odd(pred(n)) 
The test on nonzero and the predecessor function are primitive here. Every call of one of 
these routines for a number i causes exactly i further calls of odd or even. 
Exercise 2: Rewrite odd and even using alternatives. 
(e) a system of routines which reduces a sequence of signs to one sign (where + and - are 
represented by true and false, respectively) 
functpos "'(sequ boola) bool: 
if a = 0 then true 
else if top(a) then pos(rest(a)) 
else neg(rest(a)) fi fi, 
funct neg "' (sequ boola) bool: 
if a = 0 then false 
else if top(a) then neg(rest(a)) 
else pos(rest(a)) II fi 
with the primitives . = 0 as well as top and rest (comp. Table 1.3.1). 
Here a call pos (s) for a sequence s of the length Is I results exactly in Is I further calls of 
pos, neg. 
(f) a routine for the concatenation of two sequences of objects of an arbitrary mode J1 
(comp. Table 1.3.1.) 
funct cone "' ( sequ J1 a, sequ J1 b) sequ Jl: 
if a = 0 then b 
else append(conc(rest(a), b), top(a)) fi 
The operations . = 0, top and rest as well as append (which simply adds an element at the 
left side of a sequence) are primitive. Termination is due to the finite length of (the actual 
value of) a, as in the last example. 

1.4 Recursive Routines and Systems 
27 
(g) a routine sO for recognizing all bit sequences (comp. Table 1.3.1) which end with an 
even number of L, in the system (sO, sl), 
funct sO ... (sequ bit u) bool: 
if u = 0 then true 
else if top(u) = 0 then sO(rest(u)) 
funct sJ = (sequ bit u) bool: 
if u = 0 then false 
else sO(rest(u)) fl 
1.4.2 Proof of Termination 
else sl(rest(u)) fi fi, 
The informal ideas illustrated by these examples can be transformed into the following 
method for termination proofs, which is very useful in practice. 9 
Let 
funct F = (A.x) Jl: ... F(k; [x J) ... 
be a routine with n recursive calls F(k; [xJ ), i = 1, ... , n, in its body. A mapping 
is assigned to every one of these calls, where v is a countable set with a Noetherian strict 
ordering <::, such as the set of natural numbers with their natural ordering. These 
mappings must fulfill the following condition of monotonicity: 
B;(k; [x J) <:: B;(x) and 
B;(kj [x J) ~ cS;(x), 
j * i 
This means, that each function B; is strictly decreasing under the parameter mapping k; of 
the call to which it belongs, and may not increase under any of the other kj. 
As only finite decreasing chains are possible in a Noetherian ordering, the existence of 
such mappings implies the termination of the routine. 
Frequently it will happen that all the cS; can be chosen identically. This method can be 
applied analogously to systems of routines. (Complex cases of recursion will be dealt with 
again in 1.6.) 
A simple example is the routine which merges two sorted sequences into a new sequence 
where (just as in the original sequences) equal elements appear only once: 
9 This method, based on earlier suggestions by Floyd 1966, was developed principally by Dijkstra 
(see Dijkstra 1976). 

28 
funct merge "" (sequ "a, sequ "b) sequ 11: 
if a = 0 then b 
elsf b = 0 then a 
elsf top(a) < top(b) then append(merge(rest(a), b), top(a)) 
elsf top(a) = top(b) then append(merge(rest(a), rest(b)), top(a)) 
1. Routines 
else append(merge(a, rest(b)), top(b)) 
fi 
1)1: (a, b) f-+ Ia lis assigned to the first recursive call merge(rest(a), b). It follows that 
1>t (rest(a), b) 
= lrest(a) I= Ia 1- 1 < Ia I= o1 (a, b) 
o1 (rest(a), rest(b)) = lrest(a) I= Ia 1- 1 < Ia I= o1 (a, b) 
o1 (a, rest(b)) 
= Ia I 
= o1 (a, b) 
The condition of monotonicity is therefore fulfilled for 1)1 , in all recursive calls. 
1)1 can likewise be used for the second recursive call merge(rest(a), rest(b)). For the 
third call merge(a, rest(b)), o3: (a, b) f-+ lb I must be defined anew. The monotonicity con-
dition can be proved in a similar way. 
Exercise 1: Give for the example above a termination proof using one (strictly monotonic) mapping 
onto IN. 
Exercise 2: Vary the above termination proof by using a suitable Noetherian ordering on sequences 
instead of a mapping onto IN. 
1.4.3 Taxonomy of Recursion 
Both the ("macroscopic") structure of a system and the ("microscopic") inner structure of 
every single routine (as expressed by its Kantorovic tree) are characteristic for the situation 
- "situation" literally understood - of the recursion. For instance the inner structure of a 
routine/shows nested recursion if in the body off a call of the formj( ... , f, ... ) occurs. 
It shows cascade-type recursion 10 if in the body off several calls off occur beside each 
other, for example in the form f + f as in the calculation of the Fibonacci numbers ( cf. 
1.4.1-1) by 
functjib "" (nat n) nat: 
if n ~ 1 then n 
else jib(n -
2) + fib(n -
1) fi 
This is most easily seen from the (recursive) Kantorovic tree (Fig. 1.4a). 
n 
"" 
n 
.+. 
/\ /\ 
n 
1 
fib 
fib 
I I 
. . . . 
I\ I\ 
n 
2 n 
1 
Fig. 1.4a 
10 Called "multiple recursion" by R. Peter (see Peter 1976). 

1.4 Recursive Routines and Systems 
29 
Nested and cascade-type recursion is excluded in linear recursion which is characterized 
by the fact that two recursive calls off may occur only in different branches of the same 
alternative and thus will never happen together. Whether linear recursion is present can 
also be seen easily from the Kantorovic tree, in our example merge from 1.4.2 (Fig. 1.4b). 
merge = 
1\ 
a b 
append 
append 
append 
1\ 1\ 1\ 
merge 
top 
merge 
top 
merge 
top 
rL \ ! rL \st ! I \st l 
I 
I I 
I 
Fig. 1.4b 
a 
a 
b 
b 
The routinesjac and cone and every single one from the system c) of 1.4.1 are linear re-
cursive, in the latter case we have a system of linear recursive routines, illustrated by the 
Kantorovic trees in Fig. 1.5a. 
gcd 
sub 
-
1\ 
/\ 
a 
b 
I\ 
a 
gcd 
a 
b 
l\ 
a 
sub 
1\ 
/\ 
b 0 
b mod 
b 0 
pred pred 
1\ 
I I 
a 
b 
a 
b 
mod 
It 
-
1\ 
1\ 
a 
b 
It 
a 
mod 
a 
b 
1\ 
1\ 
a 
b 
sub b 
1\ 
true It 
1\ 
1\ 
a 
b 
a 0 
pred pred 
Fig. 1.5a 
I I 
a 
b 
The system (gcd, mod) (example (b), 1.4.1), having.-. and . <. as primitives, shows 
nested recursion with respect to gcd and mod; while each routine in itself is linear 
recursive, the system is not. The systems (odd, even) and (pos, neg), however, are linear 

30 
1. Routines 
pos 
I 
neg 
I 
a 
a 
neg pos 
I I 
rest rest 
I I 
a 
a 
top 
I 
a 
pos neg 
I I 
rest rest 
I I 
a 
a 
top 
I 
a 
Fig. 1.5b 
recursive systems of routines (Fig. 1.5b). This difference will turn up in 1. 7.4 and in Chaps. 
5, 6 when dealing with recursion. 
The overall ("macroscopic") structure of a system (which in the extreme case can be 
made up of one single routine) can be represented by a (directed) graph of calls. Each rou-
tine of the system is represented by a node, edges lead from one routine to all the routines 
on which it is based. 
For the examples above we have the following graphs of calls 11 (which are very much 
shortened by comparison to the complete system of the Kantorovic trees), ignoring primi-
tive routines (Fig. 1.6). Both (a) and (f) are direct-recursive routines, (b) and (c) are 
hierarchically recursive systems. (d) and (g) are examples of indirectly recursive systems in 
which one routine can be eliminated (1.3.2, compare exercise 1.7.1-2). Finally (e) is a 
mutually recursive system. 12 
a) 
c) 
d) 
f) 
Fig. 1.6a- g 
In addition non-recursive routines such as 
(h) abs (1.3.3) 
11 The similarity between the graph of calls and the transition graph of a corresponding recognition 
automaton is obvious. 
12 This classification will also be justified in Chaps. 4 and 6. 

1.4 Recursive Routines and Systems 
31 
and hierarchical systems of non-recursive routines such as 
(i) (heron, s) 
(1.3.1) 
have a trivial graph of calls (Fig. 1.6 h, i). 
i) 
Fig. 1.6h, i 
A call of a non-primitive routine is said to be a simple call, if it is the last action in the 
body of the routine; in the corresponding Kantorovic tree there is - apart from branch-
ings -
no other operation above it. More about the importance of this special case in 
1.7.4. 
A routine (or a system of routines) having simple calls only is necessarily linear recur-
sive, it is especially called repetitive (or "iterative", McCarthy 1962). The term "tail recur-
sion" is used, too. The particular routines gcd, mod, sub and It (but notfac and cone, and 
also not merge) are repetitive routines, the systems (odd, even) and (pos, neg) (but not the 
system (gcd, mod)) are repetitive systems. For repetitive routines and systems, special 
treatment will be found in 1.7.4 and in Chaps. 5 and 6. 
1.4.4 The Level of Applicative Formulation 
The level of applicative 13 or functionaJ1 4 formulation is determined by the descriptive 
means introduced so far for the construction of routines and systems. Based on 
abstraction and application, this level is characterized by the principle of substitution and 
by (binary) branchings. 
The set of all the routines and systems which can be functionally formulated based on a 
set S of primitive routines is called C(S). It is known that C(succ, . = ., 0) comprises the 
arithmetical operations and ordering relations and thus the algebra of natural numbers. 
The same holds for C(succ, pred, . = 0, 0) where . = 0 is the (unary) operation "test on 
zero". (Note also that only pred and. =0 are used for the system 1.4.1 c). 
Exercise 1: Give a definition of pred in C(succ, . =., 0). 
Exercise 2: Construct a system add, mult for addition and multiplication in C(succ, pred, . =0, 0). 
Exercise 3: Give a definition of the predicate (nat x) boo!: «X is prime» in C(succ, • =., 0). 
Exercise 4: Give a definition of the Euler rp-function (nat x) nat: «the number of numbers smaller 
than x and relatively prime to X» in C(succ, . =., 0). 
The Kantorovic tree notation shows clearly the natural sequentialization suggested by 
the principle of substitution. No operation - apart from branching - can be carried out 
unless all its operands are available. (We will meet other, artificial sequentializations later.) 
There is no preference whatever given to the order in which the objects adjacent to each 
13 Also denotative, declarative. 
14 For 'functional programming' in a narrower sense, see 1.14.2. 

32 
1. Routines 
other in parameter positions are acquired. This can take place collectively or sequentially 
in any order (e.g. also in their textual order from left to right) or partly collectively and 
partly sequentially. This is a consequence of the previous remark that the order of the 
parameters is arbitrary, but fixed once chosen. The pattern of acquiring parameters just 
described is called collateral. 
1.5 Mathematical Semantics: Fixpoint Theory 
Notational questions for the definition of recursive routines have been discussed in 1.4. 
The meaning of such routines was intuitively clear for the collection of simple examples 
listed there. A formal explanation can be given by assigning to recursive routines, too, a 
mapping of their domain qp into their range !1 (see 1.1). This method has become known 
under the key-word "denotational semantics" (Scott 1970; see Tennent 1976, Stoy 1977). 15 
1.5.1 Recursive Routines and Functional Equations 
1.5.1.1 In the case of nonrecursive routines, such as heron, the mapping is obviously 
uniquely defined as a composition of the primitive functions in question. In contrast to 
this, recursive routines represent a system of equations for mappings, which may have 
several solutions as shown by the following example (Morris 1968): 
funct morris = (int x, int y) int: 
(*) 
if x = y then succ y 
else morris(x, morris(pred x, succ y)) fi 
In this case morris is the "unknown" (of the functional equation). The functional equation 
is satisfied by the functions 
morris0(x,y) = def succ x 
as well as 
morris1 (x, y) = cter if x ~ y then succ x else pred y fi . 
For morris0 the proof is obtained by simple substitution: 
if x = y then succ y 
else morris0(x, morris0(pred x, succ y)) fi 
= if x = y then succ y 
else succ x fi 
= SUCC X 
= morris0(x,y) 
15 Later we will meet further methods of formalization - operative semantics in 1. 7 and axiomatic 
semantics in 5 .4. 

1.5 Mathematical Semantics: Fixpoint Theory 
For morris1 the proof requires more effort, in detail: 
if x = y then succ y 
else morris1 (x, morris1 (pred x, succ y)) fl 
= if x = y then succ y 
else if x ~ morris1 (pred x, succ y) 
then succ x 
else pred morris1 (pred x, succ y) fi fi 
= if x = y then succ y 
else if x ~ (if pred x ~ succ y 
then succ pred x 
else pred succ y fi) 
then succ x 
else pred morris1 (pred x, succ y) fi fi 
= if x = y then succ y 
else if pred x ~ succ y 
then (if x ~ x then succ x 
else pred morris1 (pred x, succ y) fi) 
else (if x ~ y then succ x 
else pred morris1 (pred x, succ y) fi) fi fi 
(Simplification of the branch with x ~ x) 
= if x = y then succ y 
else if pred x ~ succ y 
then succx 
else if x ~ y then succ x 
else pred (if pred x ~ succ y 
then succ pred x 
else pred succ y fi) fi fi fl 
33 
(Simplification of the innermost branch with pred x ~ succ y which is false under the 
dominating branch with x < y, and substitution of y by x in the first line) 
= if x = y then succ x 
else if pred x ~ succ y 
then succ x 
else if x ~ y then succ x 
else pred pred succ y fi fi fi 
(The last branch is subject to the condition x =1= y " pred x < succ y " x < y which 
reduces to x < y.) 

34 
= if x e; y then succ x 
else pred y fi 
= morris1(x,y) 
1. Routines 
1.5.1.2 The definition (*) of morris has the general form funct F "" r [F], where 
r [F] = def (lnt x, int y) int: if x = y then succ y 
else F(x, F(pred x, succ y)) fi 
with the corresponding functional equation 
F(x) = r [F] (x) 
i.e. 
x = (Xp x2 , ... x,) is an element from the domain ~ = ~1 x ~2 x ... x 
~,. Fis a 
variable (in the mathematical sense) for functions ~ __. ~and r is a functional, i.e. a map-
ping of the set of functions into itself. 
Mappings such as morris0 and morris1 which solve an equation like F(x) = r [ F J (x) (or 
a system of equations .Fj(x) = ri [F1, F 2, ... , Fn] (x), j = 1, 2, ... , n) are called fixpoints 
of the functional r or the system of functionals ri. Just as an algebraic equation can have 
several different solutions, a functional can also have different fixpoints, e.g. 
morris0 ;j= morris1 since 
morris0(0, 1) = 1, 
morris1 (0, 1) = 0 
This is even more obvious for the functional equation (Manna, Ness, Vuillemin 1973) 
of the routine 
funct mnv "" (nat x) nat: 
(**) 
if x = 0 then 0 
else mnv(x + 1) fi 
which has an infinite number of different fixpoints: 
mnv;(x) = def if x = 0 then 0 else i fi 
Finally, the functional equation of the routine 
(.*.) funct K "" (nat x) nat: if K(x) = 0 then 1 else 0 fi 
has no fixpoint at all. 
The question suggests itself: what mappings are defined by the routines morris and 
mnv, i.e. what value should be given to a call such as morris(O, 1) or mnv(2)? 
No result at all is preferred to an arbitrary one in such cases where several fixpoints exist. In fact 
the two calls above do not terminate, they therefore do not yield a result. mnv(x), for example, 
terminates only (and delivers the result 0) if x = 0. 

1.5 Mathematical Semantics: Fixpoint Theory 
35 
Using D for »undefined«, the following mapping can also be assigned to the routine 
mnv: 
mnva(x) = def if x = 0 then 0 else D fi 
mnva is likewise a fixpoint of the equation (**), when we operate naively with D: 
if x = 0 then 0 
else mnva(x + 1) fi 
= if x = 0 then 0 
else If x + 1 = o then o 
else D fi fi 
if x = 0 then 0 
else Dfi 
Moreover, the equation( ... ) now has the fixpoint Ka(X) = def D. 
TheuseofsuchapseudoobjectDmeansthatboththedomain P = P1 x P2 x ... x P, 
and the range f!ll of all routines in question must be extended by the element D to 
!1\+ x ~+ x . . . x P/ and f!ll +, respectively, where for an arbitrary set Jl, Jt+ = def 
Jl u {D}. The introduction of D means further, that we can restrict our attention to total 
functions.In jargon, Dis often called "bottom". Other denotations used are 1., w. 
1.5.2 Fixpoint Theory 
1.5.2.1 This extension by the pseudo object D provides the formal basis for a theory 16 in 
which exactly one mapping is assigned to every (recursive) routine. It will be sufficient to 
give a short sketch. 
A function Pt x ~+ x . . . x P,+ --+ f!ll + is called strict when it yields D whenever at 
least one of its arguments is D. Nullary functions, in particular, are therefore strict. The 
"natural extension" of functions P --+ f!ll to strict functions P + --+ f!ll + is evident. 
Strictness is required for all primitive operations and predicates, the equality operation 
is also extended to a strict function. The only exception is branching, for which (with a, be 
P +)there holds: 
If true then a else D fi = cter a 
If false then D else b fi = def b 
if D then a else b fi 
= def D 
The abbreviating constructs with "'and ~ (cf. 1.3.3) are thus not strict and differ in this 
respect from the usual Boolean operations; therefore we have 
16 This idea originates from D. Scott 1970; the particular fixpoint theory used here (theory of the 
"smash product") has been given by de Roever 1972. 

36 
true v D = D v true = D, 
but 
true "" D = true, D "" true = D. 
The following system shows how important it is that alternatives are not strict: 
funct divtwo = (nat a) nat: 
if even a then med(a) 
else med(a -
1) fi, 
funct med = (nat a) nat: 
if a = 0 then 0 
else med(a -
2) + 1 fi 
1. Routines 
The routine med is defined only for an even a. Therefore within the body of divtwo one of 
the two calls med(a), med(a- 1) always yields D. However the branching makes sure that 
exactly that branch with the defined object is chosen. 
For theoretical investigations, it is preferable to reduce functions with several para-
meters to functions with a corresponding set of parameters as single argument. If one 
restricts oneself to strict functions, then indeed all parameter sets which contain D at least 
once, can be identified and subsumed ("smashed") under the common denotation D. In 
the sequel, we will adhere to this convention and consider only unary functions. 
A second ("classical") theory (cf. Manna 1974) does not make use of this simplification and con-
siders also non-strict functions. 
1.5.2.2 With the help of the pseudo object D, a (partial) ordering can be defined in any set 
Jt +. It is called flat ordering and defined by (see Stoy 1977) 
X l;;; ,r+ Y # def X = D V X = Y 
This induces a partial ordering for functions ~+ ..... ~+: 
A function fi is said to be weaker than a function j 2 , fi l;;; f 2 , if and only if v x e ~ +: 
fi (x) l;;; rR+ ! 2 (x) i.e. 
vxe ~+:jj(x) = D v fi(x) =f2(x) 
Another diction for this is "fi approximatesj2". Moreover, fi is said to be stronger thanj2 
if ! 2 is weaker than Ji . 
Obviously, the function Q, which results always in D: Q(x) = D, is weaker than any 
function. 
A function is said to be monotonic, if 
v x,y e ~+: x l;;; p+ y => j(x) C rR+ f(y) 
Obviously, every strict function is also monotonic. 
Exercise 1: Show that the following holds: 
f2(x) = D =- fJfx) = D 
provided ! 1 is weaker than / 2 • 

1.5 Mathematical Semantics: Fixpoint Theory 
37 
It can now be shown that 
Lemma: An ascending chain 
of strict (and thus monotonic) junctions!; possesses a least upper bound lub {!;} which is 
likewise strict. 
Thus both 
II i E IN : f; [;;; lub {J;} 
and 
(IIi E IN: f; [;;; g) =>lub {f;} [;;; g hold, 
the!; are increasingly better approximations to the limit lub {!; }. 
The partial order !;;; for functions leads immediately to the notions monotonicity and 
continuity of functionals. 
A functional Tis called monotonic if for all strict functions j, g 
f!: g => rif] !: r[g] 
A monotonic functional r is called continuous if for every increasing chain of strict func-
tions 
r[lub{J;}j = lub{r[/;]} 
In analogy to a result in the "classical" theory, one can show (de Roever 1972) 
Theorem: Every junctional which is constructed by means of branching and substitution 
from strict junctions and the junction variable is monotonic and even continu-
ous. 
Thus, all functionals considered so far are continuous. The following is an important 
theorem for continuous functions. 
Theorem: (Fixpoint theorem, S.C. Kleene 1952): Every continuous junctional rpossesses 
a uniquely defined weakest fixpoint !min which is strict and coincides with 
lub {t;} 
where!; is defined inductively by afunctional iteration starting with Q: 
f 0(X) = D 
f;+l(x) = r[l;] (x), 
i = 0, 1, 2, ... 
One shows first, that all the f; are strict. Next one proves that lub {!;}is a fixpoint, and 
that lub{{;} !;;; g holds for any fixpoint of r, which also yields uniqueness. 

38 
Example: 
The routine declaration 
funct zer = (int n) int: 
if n > 0 then n - 1 else zer(zer(n + 2)) fi 
has the following corresponding functional 
r [f] = cter (int n) int: if n > 0 then n - 1 else f(j(n + 2)) fi 
The chain generated by r is 
f 0(n) = D 
ft (n) = if n > 0 then n - 1 else D fi 
f 2(n) = if n > 0 then n - 1 
else if n + 1 > 0 then n else D fi fi 
if n > 0 then n - 1 
elsf n = 0 then 0 
else D 
fi 
and in general (i ~ 2) 
Ji(n) = 
if n > 0 
then n - 1 
elsf n + 1 + 2 x (i - 2) > 0 then 0 
else D 
fi 
An immediate consequence for the limit is 
lub{Ji(n)} =if n > 0 then n- 1 else 0 fi 
1. Routines 
(Note that if the domain of zer is restricted to nat we already have for all i ~ 2: 
Ji(n) = if n > 0 then n - 1 else 0 fi 
.) 
1.5.2.3 The above fixpoint theorem, which is also valid for systems (and is not restricted to 
natural numbers as objects), suggests the following mathematical semantics of routines: 
The mapping that belongs to a routine funct F = r [F] where r is a continuous 
functional, is defined to be the weakest fixpoint of r. 
In this way, the routine zer defines the following mapping 
zermin(n) =cterifn >Othenn -1else0fi 
For the functional of(**), the weakest fixpoint mnvmin is indeed mnv0 ; for (.".), it is K0 • 
Now for the example (*) the following mapping is obtained by functional iteration and 
therefore is the weakest fixpoint of the routine morris: 
morrismin(x,y) = cter if x ~ y " even (x - y) then succ x else D fi 

1.5 Mathematical Semantics: Fixpoint Theory 
39 
On the other hand morris0(x, y) = def succ xis no longer a fix point 17 after the natural ex-
tension, since 
r[morris0](x,y) 
if x = y then succ y else succ x fi 
equals Q for y = D irrespective of x. However 
morris2(x,y) = def if x = y then succ y else succ x fi 
which is equal to morris0 for x =1= D, y =1= D, is a fixpoint. 
Exercise 2: Show that the functionals in morris, mnv and K are monotonic. 
According to the fixpoint theorem we obtain strict functions when recursive routines 
are built over strict functions by means of branchings and substitution. 
At least one short remark concerning the second theory mentioned above is appropriate here. In 
this theory, a partial ordering r;;: 1 is introduced for parameter tuples, and monotonic (instead of 
strict) functions are defined straightforwardly on !!?1 + x !!?2 + x ... x !!?/. The theory is then con-
tinued similarly; on the other hand proofs from e.g. Manna 1974 can be rewritten for our case. For all 
the routines discussed so far both theories yield limits which are the same. However this does not 
always have to be the case as the following example shows. 
Consider the routine (Manna, Ness, Vuillemin 1973) 
funct ble "' (int x, int y) int: 
If x = 0 then 1 
else ble(x- 1, ble(x- y, y)) fi 
In the first ("smash product") theory - where the parameter tuple in its entirety is Q as soon as one 
of the components is Q -
the fixpoint for b/e is the limit of the chain j 0 , ! 1, j 2 .•• , where 
f 0(x,y) = Q 
! 1 (x, y) = 
if x = o then 1 else f 0(x - 1 ,J0(x - y, y)) fi 
if x = 0 then 1 else Q fi 
f 2(x,y) = 
if x = 0 then 1 elsej1(x- 1,f1(x- y, y)) fi 
if x = o then 1 
Similarly 
else ! 1 (x -
1, if x - y = o then 1 else Q fi) fi 
if x = 0 
then 1 
elsf x- y = 0 thenj1(x- 1, 1) 
else f 1 (x- 1, Q) fi 
if x = 0 v (x,y) = (1, 1) then 1 
else Qfi 
j 3(x,y) = if x = 0 v (x,y) = (1, 1) v (x,y) = (2, 2) v 
and so on. 
Finally the limit 
(x,y) = (2, I) then 1 
else Q fi 
fmin(x,y) = if x = 0 v (x > 0 " y > 0 " y lx) then 1 
else Q fi 
17 morris0 is not strict, either. 

40 
1. Routines 
results as the weakest fixpoint according to the first theory. 
On the other hand the second ("classical") theory yields the chain 90 , 91 , 92, ••• , where 
9o(x,y) = Q 
91 (x, y) = . . . = 
if x = 0 then 1 else Q fi 
92(x,y)= ... = 
ifx=O 
then1 
Similarly 
elsf x - y = 0 then 91 (x - 1, 1) 
else 9! (x - 1' m fi 
if x = Othen 1 
elsf x = y then if x - 1 = o then 1 else Q fi 
else if x - 1 = 0 then 1 else Q fl fl 
if x = 0 then 1 
elsf x = 1 then 1 
else Qfi 
if x = o v x = 1 then 1 else Q fi 
93(x,y) = if x = 0 v x = 1 v x = 2 then 1 else Q fi 
and so on. 
This finally yields the limit 
9min(x,y) = if x ~ 0 then 1 else Q fi 
Note thatfmin ~ 9min andfmin f 9min· !min cannot even be a fixpoint of ble in the second theory. 
This follows from the fact that 9min is the weakest fixpoint there. 
Conversely 9min is not a fixpoint in the first theory. This can be verified immediately by simple 
substitution. 9min is also not strict. 
On the other hand it can be concluded from a theorem by Vuillemin 1975 that if the fixpoint de-
fined according to the second theory is strict, it is equal to the fixpoint of the first theory. As can be 
seen from the very special construction of ble this condition is violated only in very exotic cases. This 
means that the distinction between the two theories is principally of theoretical interest and has no in-
fluence at all on the vast majority of all routines found in practice. 
The fixpoint theory is also useful for non-terminating recursions. The most prominent 
examples for this are recursions which define a context-free grammar (an infinite set), see 
3.4.4. Another application will be met in 2.14.2. 
1.6 Proofs by Induction of Properties of Routines 
In proving properties of recursive routines induction is usually required (McCarthy 1961). 
Two methods have proved especially useful in practice, computational induction which is 
based on the fixpoint theory, and structural induction which is modelled on the usual 
mathematical induction methods. Strictly speaking, an induction proof amounts to the de-
monstration of a proof strategy (Lorenzen 1962). 
1.6.1 Computational Induction 
In order to prove a certain property P (jminJ for the weakest fixpointfmin of a routine 

1.6 Proofs by Induction of Properties of Routines 
41 
functj = r [!] 
one shows that P holds for every function of the chain fo, fi, / 2, ••• of iterated functions 
starting with/0 = Q, i.e. 
/ 0(X) = D 
f;+dx) = r[/;j(x), i = 0, 1, 2, ... 
Then one infers that P holds for the limit fmin -
lub{/;}. Although not valid for all 
predicates P, this does apply to those of the form 18 
P l!J : a [fJ (x) = P [!] (x) 
where a and pare continuous functionals. 
The start of the induction is trivial here because P VoJ is evidently true, if a and pare 
composed only of strict functions and if /indeed occurs in a [f] as well as in p [fJ, since 
then 
For induction, it remains to be shown that P remains valid when moving from one 
member of the chain to the next. This is certainly the case when P [ T [!] J can be derived 
from the induction hypothesis P[/J for arbitrary functions/. 
As a summary of the above we have the 
Principle of Computational Induction (de Bakker, Scott 1969, cf. Manna 1974): 
The weakest fixpointfmin of a routine 
functj = r [!] 
has the property P iJminJ if P holds for Q and remains invariant under the application 19 of 
r: 
v J, /strict: P l!J => P [r [f]J 
This method carries over directly to systems of routines. A case of special importance is 
the proof that two routines/and g are equivalent, formulated as a property of the system 
(J, g): 
p lt min • 9minJ 
where 
P ([, gJ: f(x) = g(x) 
18 Only cases necessary for this book are considered. For a more detailed treatment see e.g. Manna 
1974 where a larger class of admissible predicates is given. 
19 The application of ' is to be done according to the theory of the smash product, see the com-
putation of !min for ble in 1.5. 

42 
Example: For the routine 
functjac = (nat n) nat: 
if n = 0 then 1 
else n x jac(n -
1) fi 
in short 
funct fac = r [fac J 
and 
functjact =(nat n, nat z) nat: 
If n = 0 then z 
else jact(n - 1, z x n) fi 
in short 
funct fact = u [fact J 
it is to be shown that 
P (facmin• fact min] 
where 
P (f, gJ: f(n) = g(n, 1) 
To begin with, the more general property Q (facmin• factminJ, where 
Q(f,gJ:f(n) = g(n, 1) 
A 
ax g(n,z) = g(n, ax z) 
1. Routines 
is shown, from which P (facmin• jactmiJ follows immediately. To this end, the invariance 
of Q under application of rand u (i.e. of ron f and u on g) is shown: 
and 
r [f] (n) 
= if n = 0 then 1 
else n x f(n -
1) fi 
= hyp if n = 0 then 1 
else n x g(n - 1, 1) fi 
= hyp if n = 0 then 1 
else g(n - 1, n x 1) fi 
= if n = 0 then 1 
else g(n - 1, 1 x n) fi 
= u [g] (n, 1) 
ax u[g] (n,z) 
= a x if n = 0 then z 
else g(n - 1, z x n) fi 

1.6 Proofs by Induction of Properties of Routines 
= if n = 0 then a x z 
else a x g(n - 1, z x n) fi 
= hyp if n = 0 then a x z 
else g(n - 1, a x (z x n)) fi 
= if n = 0 then a x z 
else g(n - 1, (a x z) x n) fi 
= a[g](n, ax z) 
43 
For this proof we mainly used the fact that multiplication is an associative operation, with 
neutral element 1. (This will be dealt with again in 4.2.1.) 
1.6.2 Structural Induction 
In contrast to computational induction, structural induction (Burstall1969) is not founded 
on the fixpoint theory. Therefore there are no restrictions whatsoever for the properties 
P (FJ. The only requirement is that there is a Noetherian (strict) ordering 
<: on the 
domain qJ of the routine. As in the induction methods in mathematics the following is to 
be shown here: the property P holds for an arbitrary element a e qJif it holds for all b <: a. 
Formally this leads to the 
Principle of Structural Induction: 
For a routine F whose domain qJ possesses a Noetherian ordering <( the property 
V X E q): P (F(x)) 
holds if20 
V a E q): ((V b E q), b <: a: P (F(b))) => P (F(a))) 
Example: For the routine (1.4.1 (b)) 
funct mod = (nat a, nat b) nat: 
if a< b then a 
else mod(a -
b, b) fi 
there holds 
P(mod): b * 0 => mod(a, b) < b 
Proof: 
may serve here as a Noetherian ordering. 
20 The start of the induction is included in this condition: If there exists for some a0 no smaller b, the 
premise is true and thus P (F(a0)) is to be shown. 

44 
1. Routines 
Induction hypothesis: 
mod(c,b) < b holds for all c <a. 
Then, because of b * 0, in particular 
mod(a - b, b) < b 
According to the definition of mod it follows immediately that: 
if a< b: mod(a,b) =a< b, 
if 
a~ b: mod(a,b) = mod(a- b, b)< b 
(Note that this property P of mod is necessary in proving the termination of the routine 
gcd in example (c) of 1.4.1.) 
Structural induction can also be used to prove termination in, for example, more com-
plicated cases of nested recursions: 
Show the validity of P (ack): "the routine ack terminates for all arguments from its 
parameter domain", where 
funct ack = (nat x, nat y) nat: 
if x = 0 then y + 1 
elsf y = 0 then ack(x - 1, 1) 
else ack(x -
1, ack(x, y -
1)) fi 
Proof (Manna, Ness, Vuillemin 1973): The lexicographic ordering of the pairs of numbers 
is taken as the Noetherian ordering <( 
In this ordering the parameters of all three recursive calls within ack are smaller than (x, y). 
Therefore according to the induction hypothesis all these calls terminate. Hence ack(x,y) 
itself terminates, too. 
Compared to the related method of 1.4.2, this proof of termination has the advantage that suit-
able functions li are not to be given explicitly. 
1. 7 Operational Semantics: Machines 
The meaning of a routine does not necessarily have to be explained by describing its corre-
sponding mapping. Another method is to specify an abstract machine and to show how 
this machine executes the routine. This second method is called operational semantics. 
Two basic types of machines are considered here, text substitution machines and stack 
machines. These machines are also helpful for understanding non-recursive routines, 
although here the operative semantics is intuitively obvious. 

1. 7 Operational Semantics: Machines 
45 
1.7.1 Unfolding and Folding 
To gain a clearer understanding of the text substitution machines which will be dealt with 
in the sequel we will now discuss two elementary program transformations (comp. 
Burstall, Darlington 1977) which (under certain provisions) preserve the weakest fixpoint: 
a) Unfolding i.e. the textual replacement of a call of a routine by its body under sub-
stitution of the formal parameters by the actual objects (arguments), and conversely 
b) folding i.e. the textual replacement of an expression by the call of a routine whose 
body is equal to this expression after the substitution of the parameters. 
We have already met both transformations, e.g. in 1.3, where the auxiliary routines 
was introduced by folding within Heron's formula .. This embedding was based on the 
principle of substitution; the "verification" is done by means of "copying" or of "direct 
insertion", i.e. by unfolding s. 
Unfolding and folding are, however, of special importance in recursive situations. Un-
folding can be used to "unroll" a recursion 21 , which means the (repeated) extraction of a 
step from the recursion. Thus a single unfolding of fac (1.4.1, example (a)) results in 
functjac =(nat n) nat: 
if n = 0 then 1 
else n x (if n - 1 = 0 then 1 
else (n - 1) x fac((n - 1) - 1) fi) fi 
Distribution of the condition and simple algebra yield 
functjac =(nat n) nat: 
if n = 0 then 1 
else if n = 1 then n x 1 
else n x (n -
1) x fac(n -
2) fi fi 
Now the recursion takes only half as many steps as before and ends with the termina-
tion cases n = 0 or n = 1, according to whether n is even or odd. A further simplification 
lies in substituting the result n x 1 in the branch n = 1 of the inner alternative by 1 x 1, 
i.e. by 1 ('specialization of the unique case'). The final result is 
functjac = (nat n) nat: 
if 
n = 0 then 1 
elsf n = 1 then 1 
else n x (n -
1) x fac(n -
2) fi 
This can be further simplified to 
functjac =(nat n) nat: 
if n ~ 1 then 1 
else n x (n -
1) x fac(n -
2) fi 
21 Ger. "Strecken", this originates from Rutishauser 1952. 

46 
1. Routines 
Exercise I: Unfold the call of mod in the routine gcd of 1.4.l(b) and simplify. 
A further field of application is given by the following example in which repeated un-
folding indicates a method of transforming a given routine into an equivalent variant. 22 
From the routine 
funct gcd = (nat a, nat b) nat: 
(*) 
if 
b = 0 then a 
elsf a < b then gcd(b, a) 
else gcd(a -
b, b) fi 
unfolding of the last line and a few algebraic manipulations lead stepwise to 
if 
b = 0 
then a 
elsf a < b 
then gcd(b, a) 
elsf a < 2 x b then gcd(b, a - b) 
else gcd(a- 2 x b, b) fi, 
if 
b = 0 
then a 
elsf a < b 
then gcd(b, a) 
elsf a < 2 x b then gcd(b, a - b) 
elsf a < 3 x b then gcd(b,a- 2 x b) 
else gcd(a - 3 x b, b) fl 
finally for arbitrary q ~ 1 
if 
b = 0 
then a 
elsf a < b 
then gcd(b, a) 
elsf a < 2 x b then gcd(b, a - b) 
elsf a< q x bthen gcd(b, a- (q- 1) x b) 
else gcd(a - q x b, b) 
fi 
Let q be such that 
a ~ q x b and a < (q + 1) x b 
then because of a ~ q x b we have 
if b = 0 then a 
else gcd(a - q x b, b) fi 
From a < (q + 1) x bit follows that a - q x b < b, so that further unfolding of gcd 
yields 
22 The equivalence must however be established by a formal induction proof (1.6). 

1. 7 Operational Semantics: Machines 
47 
if b = 0 then a 
else gcd(b, a -
q x b) fi 
However, as for the given q, a - q x b is the remainder of the division of a by b (compare 
1.4.1(b)), we obtain 
funct gcd = (nat a, nat b) nat: 
(**) 
if b = 0 then a 
else gcd(b, mod(a, b)) fi 
where, according to the induction step, mod(a, b) has obviously to be defined as 
funct mod = (nat a, nat b) nat: 
if a < b then a 
else mod(a -
b, b) fi 
In 1.11.2 we shall evolve a method for avoiding this difficult formal transition from(*) to 
(**). 
Exercise 2: Show that one routine can be eliminated in both the systems (d) and (g) of 1.4.1 by un-
folding. 
Exercise 3: Eliminate medfrom the system divtwo, med in 1.5.2. 
In unfolding and folding there is the risk that the fixpoint might change. 
In unfolding, this problem, as a matter of fact, only arises in the case of artificial examples such as 
ble and is harmless: The weakest fix point of the new routine is then stronger than the weakest fix point 
of the old routine, hence this transformation is not critical in practical applications. (If the second 
("classical") theory is used as mathematical semantics the weakest fixpoint is invariant under unfold-
ing in all cases). 
In folding, on the other hand, the property of termination may get lost. The fixpoint of the new 
routine is then weaker than the fixpoint of the old one. The most trivial example is that the body of a 
routine F of course fulfills all the conditions in order to be folded to a call of F. The result is the rou-
tine 
funct F ,. (l.x) p: F(x) 
which is obviously totally undefined and thus equivalent to n. 
1. 7.2 Partial Computation 
If in a call of a routine an argument is a constant, i.e. not a formal parameter itself, the 
operation of unfolding causes this constant object to occur in the body of the routine 
(partial instantiation) and thus renders simplification possible, especially the reduction of 
branchings. As an example let us start with the routine originating from Ackermann 1928: 
funct hyp =(nat i, natx, naty) nat: 
if i = 0 then succ y 
else if y = 0 then if 
i = 1 then x 
elsf i = 2 then 0 
else 1 fi 
else hyp(pred i, x, hyp(i, x, predy)) fi fi 

48 
Unfolding the call hyp(O, x, y) simply yields succ y. 
Unfolding the call hyp(1, x, y) in the embedding 
funct addh = (nat x, nat y) nat: hyp(1, x, y) 
yields 
if y = 0 then x 
else hyp(O, x, hyp(1, x, pred y)) fi 
1. Routines 
Applying the above equivalence hyp(O, x, y) = succ y and folding then leads to 
funct addh = (nat x, nat y) nat: 
if y = 0 then x 
else succ addh(x, pred y) fi 
that is the definition of addition based on succ and pred (compare exercise 1.4.4-2). 
For the next embedding 
funct multh = (nat x, nat y) nat: hyp(2, x, y) 
unfolding results likewise in 
If y = 0 then 0 
else hyp(1, x, hyp(2, x, pred y)) fi 
Two folding operations then yield 
funct multh = (nat x, nat y) nat: 
if y = 0 then 0 
else addh(x, multh(x, pred y)) fi 
that is the definition of multiplication based on pred and on the addh just defined 
(compare also exercise 1.4.4-2). 
In the same way for the embedding 
funct powh = (nat x, nat y) nat: hyp(3, x, y) 
we obtain the recursive definition of the power function 
funct powh = (nat x, nat y) nat: 
if y = 0 then 1 
else multh(x, powh(x, pred y)) fi 
For i > 3 functions called hyperpower functions result from this partial computation. 
Another possible embedding by means of hyp is 
funct ackh = (nat x, nat y) nat: hyp(x, 2, y) 

1. 7 Operational Semantics: Machines 
Unfolding and (double) folding result in 
funct ackh = (nat x, nat y) nat: 
if x = 0 then y + 1 
elsf y = 0 then if 
x = 1 then 2 
elsf x = 2 then 0 
else 1 fi 
else ackh(x -
1, ackh(x, y -
1)) fi 
- a definition which we have already met in 1.6.2 but in a slightly altered form. 
49 
Finally let us consider further calls of hyp in which two of the arguments are constants, 
e.g. 
hyp(2, 3, x) or hyp (2, x, 3) 
The call hyp(2, 3, x) is equivalent to multh(3, x). No further simplification is possible. The 
same does not hold for hyp(2, x, 3) i.e. for multh(x, 3). 
Unfolding results first of all in 
addh(x, multh(x, 2)) 
then 
addh(x, addh(x, multh(x, 1))) 
and finally 
addh(x, addh(x, addh(x, 0))) 
The recursion of multh can therefore be unrolled into the form of an expression if the 
second argument is a constant. Ershov 1977 calls such a process a "mixed computation". 
1.7.3 Text Substitution Machines 
As an extreme case of partial computation, a call of a routine in which all the arguments 
are constants can be unfolded and simplified alternately, until only one constant object re-
mains - the result of the call. A formalization of this process leads to the definition 23 of 
how text substitution machines operate (such machines are trivial in the non-recursive 
case) and defines formally how working with a multitude of calculation forms is 
organized. 
Let funct F = r [F] be a (recursive) routine 24 ranging over a domain !fft. For an input 
value d e !fft the sequence of terms t0 , t1, t2 , ••• , is constructed as follows: 
1. Begin with t0 =def F(d) 
2. For i = 0, 1, 2, ... construct ti+ 1 from ti in two steps: 
23 Manna 1974, p. 375. 
24 Note that there can be several occurrences ofF in r. The extension to systems is obvious. 

50 
1. Routines 
2.1 Unfolding ("substitution"): certain occurrences ofF in ti are simultaneously unfolded; 
2.2. Simplification: primitive operations are evaluated as far as possible. (Alternatives, 
especially, are reduced as far as possible to single branches by evaluating their conditions.) 
This process, called execution, ends when no further step applies, i.e. when the last term tk 
is an object. In this case, we say the machine "terminates". 
The various text substitution machines differ from each other in the way in which the 
expression certain is specified in step 2.1. For example in the Herbrand-Kleene machine 
certain is replaced by all ("full-substitution computation rule") 25 • 
Example: Computation of ble(1, 0) (compare 1.5) 
funct ble = (int x, int y) int: 
if x = 0 then 1 else ble(x - 1, ble(x - y, y)) fi 
t0 is ble(1, 0) 
r----------------------------------------1 
I 
I 
t1 from :if 1 = 0 then 1 else ble(1 -1, ble(1- 0, 0)) fi : 
I 
I 
L----------------------------------------~ 
simplified: ble(O, ble(1, 0)) 
t2 from if 0 = 0 then 1 else ble(0-1, ble(O- :===], [==])) fi 
simplified: 1 
Exercise 1: Compute ble(2, 1). 
Exercise 2: Compute gcd(18,30) according to the routines a) of 1.4.1 and(*), resp., using the Her-
brand-Kleene machine. 
Other known computation rules which require less effort are: 
Substitution of the leftmost F (leftmost-outermost rule). 
Substitution of the leftmost F whose arguments do not contain any more F's (leftmost-
innermost rule). 
In short, the leftmost-innermost rule requires all arguments to be passed on by their value 
("call by value"), in accordance with the "natural sequentialization" of 1.4.4, while the 
leftmost-outermost rule passes them on as expressions ("call by name"). There are also 
other rules, in particular, the 'normal computation rule', Manna-Ness-Vuillemin 1973, 
Vuillemin 1975, which can be viewed as a strategic combination of the leftmost-innermost 
and leftmost-outermost rules (see 1.14.3). 
These computation rules are not equivalent. For the call ble(1, 0), a machine working 
on the principle of the leftmost-outermost rule terminates and yields the value 1, whereas a 
machine working according to the leftmost-innermost rule does not terminate. Generally, 
the leftmost-outermost machine yields, as can be shown by induction, 
blew(x,y) = der if x ~ 0 then 1 else D fi 
while the other produces the weaker result 
25 Strictly speaking, Kleene 1936 gave a somewhat more subtle rule, based on Herbrand 1931. 

1.7 Operational Semantics: Machines 
51 
bleu(x,y) = cter if x = 0 v (x > 0 A y > 0 A y jx) then 1 else .Q fi 
which is exactly the weakest fixpointfmin(x,y), see 1.5. 
The fixpoint theorem suggests the demand for a machine to compute exactly the 
weakest fixpoint, i.e. to produce (well-defined) results for that part of the domain where 
all the fixpoints have equal values, and not to terminate in the rest of the domain. 
Machines which always compute the weakest fixpoint are called safe. 
As was shown in 1.5 a routine can have several different weakest fixpoints, depending on the 
mathematical theory in use. Hence the term "safe machine" is relative only to the corresponding 
theory. 
It can be shown that (for the first ("smash product") theory) the leftmost-innermost 
rule always gives the weakest fixpoint, while the leftmost-outermost rule, and the full-sub-
stitution rule, sometimes give a stronger fixpoint. 26 
Thus, a machine working according to the leftmost-innermost rule is a safe machine 
(for the first theory). This is of major practical importance, since the leftmost-innermost 
rule corresponds to natural sequentialization, to the "call by value" which has become 
widely common through ALGOL. According to this rule, a function is only evaluated 
when all its arguments are known 27 • This means, of course, that expressions in argument 
position are evaluated only once, while for the leftmost-outermost rule, they are to be 
evaluated anew any time the corresponding parameter occurs in the body. This can be seen 
already in the simple example 
funct dupl = (int a) int: a + a 
for the call, say 
dupl(powh(2, m)) 
Thus if we assume in the sequel that (cf. 1.5) all recursive definitions are based on strict 
functions, and therefore a weakest fixpoint in the first theory is always determined, the 
text substitution machine which works according to the leftmost-innermost rule computes 
this fixpoint (which in turn is a strict function). We call such a machine, using concrete 
calculation sheets, also an ALGOL machine. 
Exercise 3: How many steps are necessary for the computation of ble(4, 2) with the full computation 
rule and with the leftmost-innermost rule? 
Exercise 4: Compute ack(2, 0) and ack(4, 4) for the Ackermann-Hermes function ack of 1.6.2. 
It could easily be overlooked that simpler recursive routines exist besides monsters such 
as ack or ble which lend their flavour to the fixpoint theory. For linear recursive systems 
and thus especially for repetitive systems all computation rules coincide, the fixpoint 
theory is trivial, and the effort is reduced merely to a termination proof, cf. 1.4.2. 
26 Leftmost-outermost and full-substitution rules yield the weakest fixpoint of the second ("clas-
sical") theory. 
27 Alternatives, however, are reduced even if the non-chosen branch ist not yet known. Being non-
strict, they play a distinguished role in the first theory anyway. 

52 
1. Routines 
1.7.4 The Stack Machine 
1.7.4.1 In 1.7.3 we discussed text substitution machines for the execution of (recursive) 
routines, which differ from each other merely in their substitution mechanism. It became 
evident that instead of the "full computation rule" the leftmost-innermost rule, which 
requires less effort, can be used subject to certain security precautions. In the sequel such 
an ALGOL machine - the "stack machine" - will be specified in detail. It appears to be 
more mechanized than the theoretical text substitution machines. It is characterized by 
working mainly sequentially, it has a clear organization for distributing work and uses 
special mechanisms to cope with recursion by "stacking up" the incarnations. 28 
The stack machine (Fig. 1. 7) has two processing units: an arithmetic-logical unit which 
performs (primitive) operations, and a control unit which controls the course of execution 
according to a program sheet. Finally it has two facilities for temporary storage: a value 
stack, also called parameter stack, and a protocol stack. 
arithmetic-logical 
unit 
control unit 
value stack 
v 
Fig. 1.7. Constituents of the stack machine 
-~---
protocol stack 
p 
program sheet 
We will consider the stack machine and its facilities only informally, with a view to 
gaining knowledge about the nature of recursion to guide the further development of the 
algorithmic language. Concepts such as "arithmetic-logical unit" or "stack" which are 
used pragmatically here will be defined formally and given a precise meaning in the course 
of the further development of the language. 
For the sake of simplicity let us assume that the description of a routine is available 
both as a complete Kantorovic tree and in the usual linear notation (comp. 1.4.3). Then it 
is easy to provide the corresponding bracket-free postfix notation. Corresponding to the 
special role of the if. then. else. fi-construction in the text substitution machine we will 
retain iftruethen, else and fi as separating postfix symbols. 
The routine jib 
functjib = (nat n) nat: 
if n ~ 1 then 1 
else jib(n -
2) + jib(n -
1) fi 
reads in a bracket-free postfix notation 
n 1 ~ iftruethen 1 else n 2 - fib n 1 - jib + fi 
28 More recently, other recursive machines on the basis of text substitution have been discussed: 
Berkling 1974, Mag6 1979 ("reduction machines"), Dennis 1979 ("data flow machines"). 

1. 7 Operational Semantics: Machines 
53 
We call this a program for a stack machine which is written on the program sheet and 
according to which the machine works in a left-to-right sequence. 
Intermediate results are stored in the value stack (of no specific mode) and are held 
there in typical stack manner (last-in-first-out) awaiting the operations as they come up 
(US Patent Bauer, Samelson 1958). Recursion corresponds to calls of various incarnations 
by "return jumps" of the same routine, the succession of which is recorded in the protocol 
stack. A non-recursive routine does not need the protocol stack as we will see later on. 
For easier recognition each constant and each parameter is prefixed by the (instruction) 
symbols load, loadp resp.; in addition the (instruction) symbol exec is inserted before 
every operation sign and every routine designation and supplemented by the number of 
parameters and results 29• The instruction symbol return is inserted after the complete 
description of a routine, supplemented by the number of results (at the end of primitive 
routines we can assume a return). 
The first instruction of a routine is labelled with the name of the routine. (We will mark 
also the other instructions so that we can more easily describe the course of computation.) 
1.7.4.2 Such a program has the following syntactic form (in extended BNF syntax) 
<program> 
:: = <instruction sequence> <label>: return [m] 
<instruction sequence> :: = {<label>: dnstructiom}* 
<instruction> 
.. -
load <object> lloadp <parameter> I 
Here 
exec <function> [<p>, m] I 
iftruethen <instruction sequence> 
else <instruction sequence> fi I 
goto <function> [<p>] 
<object> 
means a constant indicated by an object designation 
<parameter> means 
a freely chosen designation for a formal parameter 
<function> 
means 
the name of a routine 
<p> 
<n 
<label> 
means the number of parameters in the function concerned 
means 
the number of results of the function concerned 
means a uniquely distinguishing serial number or the designation of a 
routine. 
The above example results in 
fib: loadp n 
2: load 1 
3: exec ~ [2, 1] 
4: lftruethen 5: load 1 
else 6: loadp n 
29 Systematically speaking, load 1 could be understood to mean exec 1 [0, 1]. 

54 
15: return [1] 
7: load 2 
8: exec -
[2, 1] 
9: execjib[1, 1] 
10: loadp n 
11: load 1 
12: exec -
[2, 1] 
13: execjib[1, 1] 
14: exec + [2, 1] 
fi 
1. Routines 
1.7.4.3 The mode of operation of the stack machine can be roughly characterized as fol-
lows: 
The control instructions exec ... and return ... cause (with the help of the protocol 
stack P) the calling of (sub)-programs including access to their parameter values in the 
value stack V, or the return from subprograms while replacing their parameter values 
which are no longer needed in the value stack V by the result or results of the call just 
finished. A possibility of simplification to be discussed in 1. 7 .4.4 leads to the additional 
control instruction goto. 
The (untyped) value stack V contains operand values (arguments) exclusively. 
The protocol stack P which represents the organization of the calls contains pairs com-
posed of a label and a natural number - the distance. The last entry in P contains respecti-
vely the label of the point to which to return and the distance between the first of the 
corresponding actual parameter values and the last entry in V. 
In detail, the instructions function as follows: 
exec <functiom [<p>, <n] 
- reduces the distance in the top entry of P by <p> -
m (if P is not empty) 
- then stacks the pair (<label> of the succeeding instruction, <p>) in P 
- initiates the execution of <function> ("return jump") 
return [m] 
stores away the latest results in V (quantity m) and then deletes them from V 
- deletes in V the (no longer needed) parameter values (their quantity is the distance of 
the last entry in P reduced by m) 
- then stacks the stored results in V 
- continues the execution at the instruction whose label is the last entry of P and deletes 
the last entry of P 
load <object> 
- stacks <object> in V 
- increases (if P is not empty) the distance in the last entry of P by 1 
loadp <parameter> 
- stacks the i-th parameter concerned in V (it is read from an entry farther below in V, 
whose position is obtained from i and the distance in the last entry of P) 
- increases the distance in the last entry of P by 1 

1. 7 Operational Semantics: Machines 
55 
Table 1.7.4.a. Computation of fib(3) 
Instruction 
v 
p 
100: 
load 3 
3 
101: 
exec jib [1, 1] 
~ 
(102,1) 
fib: 
loadp n 
~ 3 
(102,2) 
2: 
load 1 
~ 3 1 
(102,3) 
3: 
exec ~ [2, 1] 
~ F 
(102,2) 
4: 
iftruethen 
~ 
(102,1) 
6: 
loadp n 
~ 3 
(102,2) 
7: 
load 2 
~ 3 2 
(102,3) 
8: 
exec -
[2, 1] 
~ 1 
(102,2) 
9: 
exec fib [1, 1] 
H 
(102,2) 
(10,1) 
fib: 
loadp n 
~ 1 1 
l 
(10,2) 
2: 
load 1 
2 1 1 1 
(10,3) 
3: 
exec~ [2, 1] 
HT 
(10,2) 
4: 
iftruethen 
2 1 
(10,1) 
5: 
load 1 
H 
1 
(102,2) 
(10,2) 
15: 
return [1] 
2 1 
(102,2) 
L:_____. 10: 
10: 
loadp n 
~ 1 3 
(102,3) 
11: 
load 1 
~ 1 3 1 
(102,4) 
12: 
exec- [2, 1] 
2 1 2 
(102,3) 
13: 
exec fib [1, 1] 
~ 1 2 
(102,3) 
(14,1) 
jib: 
loadp n 
~ 1 2 2 
(102,3) 
(14,2) 
2: 
load 1 
2 1 2 2 1 
(14,3) 
3: 
exec ~ [2, 1] 
2 1 2 F 
(14,2) 
4: 
lftruethen 
2 1 2 
(14,1) 
6: 
loadp n 
~ 1 2 2 
(14,2) 
7: 
load 2 
~ 1 2 2 2 
(14,3) 
8: 
exec- [2, 11 
2 1 2 0 
(14,2) 
9: 
exec fib [1, 1] 
~ 1 2 Q 
(14,2) 
(10,1) 
jib: 
loadp n 
~ 12 Q 0 
! 
(10,2) 
2: 
load 1 
212Q01 
(10,3) 
3: 
exec~ [2, 11 
21 2 Q T 
(10,2) 
4: 
iftruethen 
2 1 2 Q 
(10,1) 
5: 
load 1 
~ 12 Q 1 
(14,2) 
(10,2) 
15: 
return [1] 
2 1 2 1 
(14,2) 
L. 10: 
10: 
loadp n 
~ 1 2 1 2 
(14,3) 
11: 
load 1 
2 1 2 1 2 1 
(14,4) 
12: 
exec- [2, 1] 
~ 1 2 1 1 
(14,3) 
13: 
exec fib [1, 1] 
2 1 2 1 1 
(14,3) 
(14,1) 
jib: 
loadp n 
~12111 
I 
(14,2) 
2: 
load 1 
~ 1 2 1 1 1 1 
(14,3) 
3: 
exec~ [2, 1] 
21211T 
(14,2) 
4: 
iftruethen 
~ 1211 
(14,1) 
5: 
load 1 
~ 1 2 1 1 1 
(14,3) 
(14,2) 
15: 
return [1] 
21 2 1 1 
(14,3) 
L. 14: 
14: 
exec+ [2, 1] 
~ 1 2 2 
(102,3) 
(14,2) 
15: 
return [1] 
~ 1 2 
(102,3) 
14: 
14: 
exec+ [2, 1] 
~ 3 
(102,2) 
15: 
return [1] 
3 
102: 
102: 
/VVVVVV 

56 
iftruethen <instruction sequence> else <instruction sequence> fi 
- determines if the last entry in Vis true or false 
- deletes the last entry in V 
- decreases the distance in the last entry of P by 1 
- executes the first or the second <instruction sequence>, resp. 
goto <function> [<p>] 
1. Routines 
- stores away and deletes the top parameter values of <function> entered last (quantity 
<P>) in V 
- deletes in Vthe (no longer needed) parameter values (their quantity is the distance in the 
top entry of P decreased by <P>) 
- then stacks the stored parameter values in V 
- makes <P> the distance in the top entry of P 
- initiates the execution of <function> ("genuine jump"). 
The mode of operation of a stack machine for routines with several results as well as 
for systems of routines is evident (compare also example gcd, mod and exercise 1.7.4-2). 
Example 1 
For the routine jib, Table 1.7.4.a shows the course of computation of jib(3). It can be as-
sumed that the instruction sequence 
100: load 3 
101: execjib[1, 1] 
102: vvvvvvv 
is initiated from outside - if it is not a component of some superior program. After termi-
nation (last return) the machine waits for further instructions. The result is in the value 
stack. 
To enhance clarity the parameter values (parameter tuples) in the value stack are under-
lined from that execution point onwards in which the exec instruction acquires the para-
meters up to their release (and deletion) by the corresponding return instruction. 
Exercise 1: Write a stack machine program for the routine b/e (see 1.5.2.3) and trace the course of 
computation of ble(2, 1). 
Let us consider a further example of a program for the stack machine. 
Example 2 
For the routine fac of 1.4.1 (a) the Kantorovic tree 
(Fig. 1.8) reads in linear notation 
n 0 = iftruethen 1 else n n 1 - fac x fi 
Thus we have the following program: 
jac: loadp n 
2: load o 
3: exec = [2, 1] 
fac I 
n 
1\ 
n 
0 
Fig. 1.8 
.x. 
1\ 
n 
fac 
I 
1\ 
n 

1. 7 Operational Semantics: Machines 
57 
4: iftruethen 5: load 1 
else 6: loadp n 
7: loadp n 
8: load 1 
9: exec -
[2, 1] 
10: execjac[1,1] 
11: exec x [2, 1] fi 
12: return [1] 
Table 1.7.4.b. Computation of fac(3) 
Instruction 
v 
p 
100: 
load 3 
101: 
exec fac [1, 1] 
~ 
(102,1) 
jac: 
loadp n 
~ 3 
(102,2) 
2: 
load o 
~ 3 0 
(102,3) 
3: 
exec= [2, 11 
~ F 
(102,2) 
4: 
iftruethen 
~ 
(102,1) 
6: 
loadp n 
~ 3 
(102,2) 
7: 
loadp n 
~ 3 3 
(102,3) 
8: 
load 1 
~ 3 3 1 
(102,4) 
9: 
exec- [2, 1] 
3. 3 2 
(102,3) 
10: 
execjac(1, 1] 
J.32 
(102,3) 
(11,1) 
fac: 
loadp n 
~ 3 2 2 
(102,3) 
(11,2) 
2: 
load o 
~ 3 2 2 0 
(11 ,3) 
3: 
exec= [2, 1] 
~ 3 2 F 
(11 ,2) 
4: 
iftruethen 
~ 3 2 
(11,1) 
6: 
loadp n 
~ 3 22 
(11,2) 
7: 
loadp n 
~ 3 2 2 2 
(11,3) 
8: 
load 1 
~32221 
(11,4) 
9: 
exec- [2, 1] 
~ 3 2 2 1 
(11,3) 
10: 
execjac[1, 1] 
n 221 
(11,3) 
(11,1) 
fac: 
loadp n 
~ 3 2 2 1 1 
(11,3) 
(11 ,2) 
2: 
load o 
~ 3 2 2 1 1 0 
(11 ,3) 
3: 
exec= [2, 11 
n 2 21 F 
(11 ,2) 
4: 
iftruethen 
~ 3f 2 1 
(11 ,1) 
6: 
loadp n 
~ 3 2 2 1 1 
(11 ,2) 
7: 
Ioadp n 
~ 3 2 2 1 1 1 
(11 ,3) 
8: 
load 1 
~ 3 2 2 1 1 1 1 
(II ,4) 
9: 
exec- [2, 1] 
~ 3 2 2 1 1 0 
(II ,3) 
10: 
execjac [1, 1] 
J.32211Q 
(II ,3) 
(11,1) 
fac: 
loadp n 
~ 3 2 2 1 1 Q 0 
(II ,3) 
(II ,2) 
2: 
load 0 
~ 3 2 2 1 1 Q 0 0 
~ 
(II ,3) 
3: 
exec= [2, 1] 
~ 3 2 2 1 1 Q T 
(II ,2) 
4: 
iftruethen 
J.32211Q 
(II ,1) 
5: 
load 1 
~ 3 2 2 1 1 Q 1 
(II ,3) 
(II ,2) 
I2: 
return [1] 
~ 3 2 2 1 1 1 
(I I ,3) 
L.:. 11: 
11: 
exec x [2, 1] 
~ 3 2 2 1 1 
(JI,3) 
(11 ,2) 
12: 
return [1] 
~ 3 2 2 1 
(JI,3) 
11: 
11: 
exec x [2, 1] 
~ 3 2 2 
(102,3) 
(JI,2) 
I2: 
return [1] 
~ 3 2 
(102,3) 
11: 
11: 
exec x [2, 1] 
~ 6 
(102,2) 
12: 
return [1] 
6 
I02: 
I02: 

58 
Table 1. 7 .4. b shows the course of computation of fac(3) initiated by 
100: load 3 
101: execjac[1, 1] 
102:vvvvvvv 
1. Routines 
1. 7 .4.4 In the case of repetitive routines and systems the stack machine can be considerably 
simplified. 
Example 3 
For the system of routines (gcd, mod) of 1.4.1 (b) the Kantorovic trees (compare 1.4.3) in 
linear notation read 
b 0 = iftruethen a else b a b mod gcd fi 
a b < iftruethen a else a b -
b mod fi 
Thus we obtain the program system for the stack machine: 
gcd: loadp b 
2: load 0 
3: exec = [2, 1] 
4: iftruethen 5: loadp a 
else 6: loadp b 
7: loadp a 
8: loadp b 
11: return [1] 
mod: loadp a 
13: loadp b 
9: exec mod [2, 1] 
10: exec gcd [2, 1] fi 
14: exec < [2, 1] 
15: iftruethen 16: loadp a 
else 17: loadp a 
18: loadp b 
22: return [1] 
19: exec -
[2, 1] 
20: loadp b 
21: exec mod[2, 1] fl 
Table 1.7.4.c shows the course of computation of gcd (30, 18), initiated by 
101: load 30 
102: load 18 
103: exec gcd[2, 1] 
104: vvvvvvv 

Table 1.7.4.c. Computation of gcd(30, 18) 
Instruction 
101: 
load 30 
102: 
load 18 
103: 
exec gcd [2, 1] 
gcd: 
loadp b 
2: 
load 0 
3: 
exec = [2, 1] 
4: 
iftruethen 
6: 
loadp b 
7: 
loadp a 
8: 
loadp b 
9: 
exec mod [2, 1] 
mod: 
loadp a 
13: 
loadp b 
14: 
exec < [2, 1] 
15: 
lftruethen 
17: 
loadp a 
18: 
loadp b 
19: 
exec -
[2, 1] 
20: 
loadp b 
21: 
exec mod [2, 1] 
mod: 
loadp a 
13: 
loadp b 
14: 
exec < [2, 1] 
15: 
iftruethen 
16: 
loadp a 
22: 
return [1] 
22: 
return [1] 
10: 
exec gcd [2, 1] 
gcd: 
loadp b 
2: 
load o 
3: 
exec = [2, 1] 
4: 
iftruethen 
v 
30 
30 18 
30 !1 
30 !1 18 
30 !]_18 0 
30 !1 F 
JQ!]_ 
JQ !]_18 
30 !1 18 30 
30 !]_18 30 18 
30 !118 JQ !1 
30 !1 18 JQ !1 30 
30 !]_18 JQ !1 30 18 
30 !]_18 JQ !1 F 
JQ !]_18 JQ !1 
30 !1 18 JQ !1 30 
JQ !1 18 30 !1 30 18 
JQ !1 18 30 !1 12 
30 !1 18 30 !]_12 18 
30 !]_18 30 !1 .!1 !1 
30 !118 JQ !1 .!1 !1 12 
30 !]_18 JQ 18 .!1 !1 12 18 
30 !]_18 JQ !1 .!1 !1 T 
JQ !118 JQ !1 .!1 !1 
30 !]_18 JQ !1 .!1 !1 12 
30 !]_18 JQ !]_12 
30 !]_18 12 
30 !1 !1 .!1 
30 !1 !1 12 12 
30 !1 !1 .!1 12 
0 
30 !1 !1 .!1 F 
30 !1 !1 .!1 
p 
(104,2) 
(104,3) 
(104,4) 
(104,3) 
(104,2) 
(104,3) 
(104,4) 
(104,5) 
(104,4) 
(104,4) 
(104,4) 
(104,4) 
(104,3) 
'T' 
(10,2) 
(10,3) 
(10,4) 
(10,3) 
(10,2) 
(10,3) 
(10,4) 
(10,3) 
(10,4) 
(10,3) 
'l" 
(22,2) 
(22,3) 
(22,4) 
(22,3) 
(22,2) 
(10,3) 
(2~ 
(10,3) 
22: 
L--------> 10: 
(11,2) 
(11,3) 
(11,4) 
(11,3) 
(11,2) 
:_, 
~ 
~-
0 
~ r 
o. 
~ 
:::: 
"' 2: 
~ 
v. 
'0 

Table 1.7.4.c (continued) 
~ 
Instruction 
v 
p 
6: 
loadp b 
.N 18 ~ 1112 
(11 ,3) 
7: 
loadp a 
30 ~ ~ 1112 18 
(11 ,4) 
8: 
loadp b 
30 ~ ~ 1112 18 12 
(11 ,5) 
9: 
exec mod [2, 1] 
.N ~ ~ 11 12 ~ 11 
(11 ,4) 
(10,2) 
mod: 
loadp a 
30 ~ ~ 11 12 ~ 12 18 
(11 ,4) 
(10,3) 
13: 
loadp b 
.N 18 ~ 11 12 ~ 11 18 12 
(10,4) 
14: 
exec < [2, 1] 
30 ~ ~ 11 12 ~ 12 F 
(10,3) 
15: 
iftruethen 
30 ~ ~ 11 12 ~ 12 
(10,2) 
17: 
loadp a 
30 ~ ~ 1112 ~ 1118 
(10,3) 
18: 
loadp b 
.N ~ ~ 11 12 ~ 12 18 12 
(10,4) 
19: 
exec -
[2, 1] 
.N ~ ~ 11 12 ~ 11 6 
(10,3) 
20: 
loadp b 
.N ~ ~ 1112 ~ 11 6 12 
(10,4) 
21: 
exec mod [2, 1] 
.N ~ ~ 11 12 ~ 12 
fi 11 
(10,3) 
(22,2) 
mod: 
loadp a 
30 ~ 18 11 12 ~ 11 
fi 11 6 
T' 
(22,3) 
13: 
loadp b 
.N ~ ~ 11 12 ~ 12 
fi 11 6 12 
(22,4) 
14: 
exec < [2, 1] 
dQ ~ ~ 1112 ~ 11 
fi 11 T 
(22,3) 
15: 
iftruethen 
.N ~ ~ 12 12 ~ 11 
fi 11 
(22,2) 
16: 
loadp a 
30 ~ ~ 11 12 ~ 12 
fi 11 6 
(10,3) 
(22,3) 
22: 
return [1] 
.N 18 ~ 11 12 ~ 11 6 
(11 ,4) 
(10,3) 
L..:.......... 22: 
22: 
return [1] 
.N 18 18 11 12 6 
(11 ,4) 
10: 
10: 
exec gcd [2, 1] 
.N~~1111fi 
(11 ,3) 
(11 ,2) 
gcd: 
loadp b 
30 ~ ~ 1111 6 6 
(11 ,3) 
(11,3) 
2: 
load o 
.N 18 18 1111 fi 
6 0 
(ll,4) 
3: 
exec = [2, 1] 
.N~~1111 fi 
F 
(ll,3) 
4: 
iftruethen 
.N~~1111 fi 
(11 ,2) 
6: 
loadp b 
.N 18 18 1111 fi 
6 
(ll ,3) 
7: 
loadp a 
30 18 ~ 12 12 fi 
6 12 
(11 ,4) 
8: 
loadp b 
.N ~ ~ 1111 fi 
6 12 6 
(11 ,5) 
9: 
exec mod [2, 1] 
.N ~ ~ 1111 fi 
6 12 fi 
(11 ,4) 
(10,2) 
:-" 
mod: 
loadp a 
30 ~ 18 1111 fi 
6 11 
fi 12 
(11,4) 
(10,3) 
~ 
0 
13: 
loadp b 
.N ~ ~ 1112 6 6 11 fi 12 6 
f 
(10,4) 
"' 5· 
14: 
exec < [2, 1] 
.N 18 18 1111 fi 
6 11 
fi F 
(10,3) 
&l 

Table 1. 7 .4.c (continued) 
-
:._, 
Instruction 
v 
p 
0 "' 
~ 
15: 
iftruethen 
30 il il g g 
{! 
6 g 
{! 
(10,2) 
~-
0 
17: 
loadp a 
1Q il il g g 
{! 
6 g 
{! 12 
(10,3) 
::I a 
18: 
loadp b 
1Q il il g g 
{! 
6 g 
{! 12 6 
(10,4) 
C/l 
(I) 
19: 
exec -
[2, 11 
30 il il g g 
{! 
6 g 
{! 
6 
(10,3) 
s 
20: 
loadp b 
lQilil!l!l {! 6!l {! 
6 6 
(10,4) 
!· 
21: 
exec mod [2, 1] 
30 il il g g 
{! 
6 g 
{! 
{! 
{! 
(10,3) 
(22,2) 
mod: 
loadp a 
lQilil!l!l {! 6!l {! 
{! {!6 
(10,3) 
(22,3) 
a:: 
13: 
loadp b 
lQilil!l!l {! 6!l {! 
{! 
{! 6 6 
(22,4) 
"' 
(") 
14: 
exec < [2, 1] 
30 il il 12 !l {! 
6 !l {! 
{! 
6 F 
(22,3) 
e: 
::I 
I5: 
iftruethen 
30 il il 12 g 
6 6 g 
{! 
{! 
{! 
(22,2) 
0: 
17: 
loadp a 
lQilil!l!l {! 6!l {! 
{! 
{!6 
(22,3) 
I8: 
loadp b 
lQilil!l!l {! 6!l {i 
{! {!66 
(22,4) 
I9: 
exec -
[2, 1] 
lQilil!l!l 6 6!l {! 
6 {!0 
(22,3) 
20: 
loadp b 
1Q il il 12 g 
{! 
6 g 
6 6 
{! 0 6 
(22,4) 
2I: 
exec mod [2, 1] 
lQilil!l!l {i 6!l 6 
{! fi!l{i 
(22,3) 
(22,2) 
mod: 
loadp a 
lQilil!l!l 6 6!l {! 
{! {!!)60 
'T" 
(22,3) 
13: 
loadp b 
1Q il il 12 g 
{! 
6 g 
{! 
{! 
{! !l {! 0 6 
(22,4) 
I4: 
exec < [2, 1] 
30 il il 12 !l 6 6 !l {! 
6 fi !l {! T 
(22,3) 
I5: 
iftruethen 
30 il il 12 g 
6 6 g 
6 6 
{! !l 6 
(22,2) 
I6: 
loadp a 
30 il il 12 g 
6 6 g 
6 
{! 
{! !l {! 0 
(22,3) 
(22,3) 
22: 
return [1] 
30 il il 12 g 
{! 
6 g 
6 6 
{! 0 
(10,3) 
(22,3) 
L...:. 22: 
22: 
return [1] 
30 il il g g 
{! 
6 g 
6 0 
(II ,4) 
(10,3) 
22: 
22: 
return [1] 
lQilil!l!l {! 
6 0 
(II ,4) 
10: 
10: 
exec gcd [2, 1] 
1Q il il 12 g 
6 6 !l 
(II ,3) 
(11 ,2) 
gcd: 
loadp b 
1Q il il 12 g 
{! 
{! !l 0 
(JI,3) 
(11,3) 
2: 
load 0 
1Q il il 12 g 
{! 
{! !l 0 0 
I 
(11,4) 
3: 
exec= [2, 1] 
30 illi 12 !l {! 
{! !l T 
(JI,3) 
4: 
iftruethen 
lQlJ!lJ!!l!l {! 
{! !l 
(II ,2) 
5: 
loadp a 
lQlJ!lJ!!l!l {! 
{! !l 6 
(II ,3) 
(II ,3) 
11: 
return [1] 
lQlJ!lJ!!l!l {! 
6 
(II ,3) 
(II ,3) 
11: 
11: 
return [1] 
1Q 1J! 18 g 
6 
(104,3) 
(II ,3) 
11: 
11: 
return [1] 
1Q 1J! 6 
(104,3) 
11: 
2:! 
11: 
return [1] 
6 
104: 
104: 

62 
1. Routines 
Considering the last two examples we notice that the parameter values or tuples of 
previous calls stacked further below (which are always marked by underlining) are no 
longer needed. The value stack, therefore, contains useless information which in a 
realization takes up unnecessary space. A glance at example 1 shows, however, that this 
waste is not inherent; in the most general cases of nonlinear recursion (compare 1.4.2) the 
parameter values stacked further below are really needed and used again. 
In example 2 the routine is linear recursive and the program of the stack machine is 
such 30 that the mode of operation of a stack machine, with regard to the value stack, could 
be simplified. It is possible to overwrite the old parameter values by the new ones. The 
protocol stack, however, retains its function. 
As example 3 shows the treatment of the protocol stack can also be simplified if, after 
return from a called program, no more operations are pending but a further return follows 
immediately. This special case (simple call, compare 1.4.3) holds for all calls which directly 
precede a return within the course of computation. In such a case a call such as 
10: exec gcd[2, 1] 
can be replaced by the simpler jump call 
10: goto gcd[2] 
where the new instruction goto hardly affects the protocol stack -
only the distance is 
revised -
and overwrites the parameter values or parameter value lists in the value stack 
(see 1.7.4.3, mode of operation of the instructions). 
The corresponding simplified program of the stack machine for example 3 is 
gcd: loadp b 
2: load 0 
3: exec = [2, 1] 
4: iftruethen 5: loadp a 
else 6: loadp b 
7: loadp a 
B: loadp b 
9: exec mod(2, 1] 
10: goto gcd[2] 
fi 
11: return [1] 
30 As the operands of x have to be acquired collaterally and multiplication is commutative the fol-
lowing could have been written: 
else 
6: loadp n 
7: load 1 
12: return [1] 
8: exec -
[2, 1] 
9: exec jac [1, 1] 
10: loadp n 
11: exec x [2, 1] 
li 
We now have to have access to all stacked parameter values. 

1. 7 Operational Semantics: Machines 
mod: load a 
13: load b 
14: exec < [2, 1] 
15: iftruethen 16: load a 
else 17: load a 
18: load b 
22: return [1] 
19: exec -
[2, 1] 
20: load b 
21: goto mod [2] fi 
63 
Table 1. 7 .4.d shows the corresponding course of computation of gcd(30, 18) in example 3. 
Obviously a considerable amount of storage is saved. In particular, storage demand is 
independent of the depth of recursion and thus is bounded. 
1.7.4.5 The Kantorovic tree shows whether we have a simple call and whether, as a result, a 
jump call instead of an exec call can be used. There is a corresponding syntactic condition 
and consequently the simplification can be accomplished by a compiler. 31 For a repetitive 
routine or system, jump calls can be used for all nonprimitive routines. 
In such a restricted stack machine which only permits repetitive systems the parameter 
stack reduces to a parameter register (a register which holds the actual parameter values) 
and to a register for the build-up of a new parameter tuple. The latter is still a stack but its 
depth is bounded once a program is given (stack of intermediate results, "Zahlkeller" in 
Bauer, Samelson 1957): The protocol stack disappears completely. 32 
Let us call such a machine with bounded storage demand a Babbage-Zuse-machine. 
Although it can only deal with the simplest types of recursion - nested repetitive types, to 
be exact - its power is quite considerable: In "990Jo" of all practical cases it is possible to 
massage a routine so that it is executable on a Babbage-Zuse-machine. We will discuss 
these possibilities in Chap. 4. The "simple call" will be taken up again in Chap. 6 in the 
form of a "jump". 
Exercise 2: Write a program for the mutually recursive systems (odd, even) of 1.4.1 (d) and (pos, neg) 
of 1.4.1 (e) for the stack machine. 
31 The BLISS Compiler (Wulf eta!. 1971) took advantage of this. 
32 Even in the nested situation of routines, each of which in itself is repetitive, as happens in the 
system gcd, mod, the value stack collapses to a set of parameter registers, one for each routine and 
to a stack of intermediate results; and the protocol stack can be functionally simplified by using 
simple return jumps in gcd for the calls of the subordinate routine mod. This is the typical 
situation of "nested loops". Simple return jumps can in particular be used for all calls of primitive 
routines. Storage demand is still bounded. 

64 
1. Routines 
Table 1. 7 .4.d. Simplified computation of gcd (30, 18) with jump calls 
Instruction 
v 
p 
101: 
load 30 
30 
102: 
load 18 
30 18 
103: 
exec gcd [2, 1] 
30 11 
(104,2) 
gcd: 
loadp b 
30 11 18 
(104,3) 
2: 
load o 
30 11 18 0 
(104,4) 
3: 
exec= [2, 1] 
30 11 F 
(104,3) 
4: 
iftruethen 
1Q11 
(104,2) 
6: 
loadp b 
1Q 11 18 
(104,3) 
7: 
loadp a 
1Q 11 18 30 
(104,4) 
8: 
loadp b 
1Q 11 18 30 18 
(104,5) 
9: 
exec mod [2, 1] 
30 11 18 1Q 11 
(104,4) 
(10,2) 
mod: 
loadp a 
1Q 11 18 30 11 30 
(104,4) 
(10,3) 
13: 
loadp b 
1Q 11 18 1Q 11 30 18 
(10,4) 
14: 
exec < [2, 1] 
1Q 11 18 1Q 11 F 
(10,3) 
15: 
iftruethen 
1Q 11 18 1Q 11 
(10,2) 
17: 
loadp a 
1Q 11 18 1Q 11 30 
(10,3) 
18: 
loadp b 
1Q 11 18 1Q 11 30 18 
(10,4) 
19: 
exec- [2, 1] 
1Q 11 18 1Q 11 12 
(10,3) 
20: 
loadp b 
1Q 11 18 1Q 11 12 18 
(10,4) 
21: 
goto mod [2] 
1Q 11 18 ll 11 
(10,2) 
mod: 
loadp a 
1Q 11 18 ll 11 12 
(10,3) 
13: 
loadp b 
1Q 11 18 ll 11 12 18 
(10,4) 
14: 
exec < [2, 1] 
1Q 11 18 ll 11 T 
(10,3) 
15: 
iftruethen 
1Q 11 18 ll 11 
(10,2) 
16: 
loadp a 
1Q 11 18 ll 18 12 
(104,4) uE 
22: 
return [1] 
1Q 11 18 12 
(104,4) 
10: 
10: 
goto gcd [2] 
11ll 
(104,2) 
gcd: 
loadp b 
11ll 12 
(104,3) 
2: 
load o 
11ll 12 0 
(104,4) 
3: 
exec = [2, 1] 
11llf 
(104,3) 
4: 
lftruethen 
11ll 
(104,2) 
6: 
loadp b 
11ll 12 
(104,3) 
7: 
loadp a 
11ll 12 18 
(104,4) 
8: 
loadp b 
11ll 12 18 12 
(104,5) 
9: 
exec mod [2, 1] 
11ll 12 11ll 
(104,4) 
(10,2) 
mod: 
loadp a 
11ll 12 11ll 18 
(104,4) 
(10,3) 
13: 
loadp b 
11ll 12 11ll 18 12 
(10,4) 
14: 
exec < [2, 1] 
11 12 12 11ll F 
(10,3) 
15: 
iftruethen 
11ll 12 11 12 
(10,2) 
17: 
loadp a 
11 ll 12 11 ll 18 
(10,3) 
18: 
loadp b 
11 ll 12 11 ll 18 12 
(10,4) 
19: 
exec- [2, 1] 
11ll 12 11ll 6 
(10,3) 
20: 
loadp b 
11ll 12 11ll 6 12 
(10,4) 
21: 
goto mod [2] 
11ll 12 fill 
(10,2) 
mod: 
loadp a 
11ll 12 fill 6 
(10,3) 
13: 
loadp b 
11ll 12 fill 6 12 
(10,4) 
14: 
exec < [2, 1] 
11 12 12 fill T 
(10,3) 
15: 
iftruethen 
11ll 12 !lll 
(10,2) 
16: 
loadp a 
11 12 12 fi 12 6 
(104,4) 
(10,3) 
22: 
return [1] 
11ll 12 6 
(104,4) 
~ 
10: 

1. 7 Operational Semantics: Machines 
65 
Tabelle 1.7.4.d (continued) 
Instruction 
v 
p 
10: 
goto gcd [2] 
11 
(! 
(104,2) 
gcd: 
loadp b 
11 
(! 6 
(104,3) 
2: 
load o 
12 
(! 6 0 
(104,4) 
3: 
exec= [2, 1] 
11 (! 
F 
(104,3) 
4: 
iftruethen 
11 
(! 
(104,2) 
6: 
loadp b 
11 (! 
6 
(104,3) 
7: 
loadp a 
11 
(! 6 12 
(104,4) 
8: 
loadp b 
11 (! 
6 12 6 
(104,5) 
9: 
exec mod [2, 1] 
11 
(! 611 (! 
(104,4) 
(10,2) 
mod: 
loadp a 
11 
(! 611 (! 12 
(104,4) 
(10,3) 
13: 
loadp b 
11 fi 612 (! 12 6 
(10,4) 
14: 
exec < [2, 11 
12 (! 612 § F 
(10,3) 
15: 
iftruethen 
12 (! 612 (! 
(10,2) 
17: 
loadp a 
11 
(! 612 § 12 
(10,3) 
18: 
loadp b 
12 (! 612 (! 12 6 
(10,4) 
19: 
exec- [2, 1] 
11 
(! 611 
(! 6 
(10,3) 
20: 
loadp b 
12 § 611 (! 6 6 
(10,4) 
21: 
goto mod [2] 
11 § 6 
(! 
§ 
(10,2) 
mod: 
loadp a 
11 § 6 
(! 
(! 6 
(10,3) 
13: 
loadp b 
11 § 6 § 
§ 6 6 
(10,4) 
14: 
exec < [2, 1] 
11 § 6 
(! 
§ 
F 
(10,3) 
15: 
iftruethen 
11 § 6 § 
§ 
(10,2) 
17: 
loadp a 
12 (! 6 § 
(! 6 
(10,3) 
18: 
loadp b 
11 § 6 (! 
§ 6 6 
(10,4) 
19: 
exec -
[2, 1] 
11 § 6 
(! 
§ 0 
(10,3) 
20: 
loadp b 
11 § 6 
(! 
§ 0 6 
(10,4) 
21: 
goto mod [2] 
11 § 6 Q § 
(10,2) 
mod: 
loadp a 
11 
(! 6 Q § 0 
(10,3) 
13: 
loadp b 
12 § 6 Q § 0 6 
(10,4) 
14: 
exec < [2, 1] 
11 fi 6 Q § T 
(10,3) 
15: 
iftruethen 
11 § 6 Q § 
(10,2) 
16: 
loadp a 
1§. 
(! 6 Q § 0 
(104,4) uE 
22: 
return [1] 
11 
(! 6 0 
(104,4) 
10: 
10: 
goto gcd [21 
§ Q 
(104,2) 
gcd: 
loadp b 
§ Q 0 
(104,3) 
2: 
load o 
(! Q 0 0 
(104,4) 
3: 
exec= [2, 11 
(! Q T 
(104,3) 
4: 
iftruethen 
§ Q 
(104,2) 
5: 
loadp a 
§ Q 6 
(104,3) 
11: 
return [1] 
6 
104: 
104: 
'VVVVV 

66 
1. Routines 
1.8 Restriction of the Parameter Domain 
Occasionally a routine is not defined for the complete parameter domain, perhaps because 
it is deliberately restricted - an example would be the restriction to positive arguments for 
the routine gcd -
or maybe because a restriction follows in a natural way from the prob-
lem. This already holds for some routines which are usually regarded as being primitive, 
such as sub over nat. If sub is defined as the inverse of addition, then the solution x of 
x + b = a is unique, but it only exists for a ~ b. A similar restriction also applies to pred, 
further details will be found in 1.11.2. There are other cases of natural restrictions of the 
parameter domain as for example gcd (the greatest common divisor of 0 and 0 is, literally 
speaking, undefined - even though the system (b) of 1.4.1 yields the value 0) and mod (the 
divisor must not be 0). 
In order to be defined the call of a routine must of course take place under such 
circumstances as guarantee that the restriction of the domain is met. It is therefore 
advisable to add such restrictions to the routine in a suitable form. 
A notation such as 
funct sub = (nat a, nat b) nat: 
if a < b then D 
else if b = 0 then a 
else sub(pred(a), pred(b)) fi fi 
using explicitly the universal pseudo object »undefined« with the denotation Q is not satis-
factory. Instead of artificially extending the mapping to a total function on the domain, it 
is preferable to restrict the parameter domain to the proper domain in order to achieve a 
total mapping without using Q in the text. 
Frequently routines with restricted domains occur in systems under circumstances 
which exactly suit them. Such systems often emerge in a natural way during program devel-
opment; an example is given by the system (c) of 1.4.1. Placing a condition at the entry of 
the body of the routine would mean unnecessarily doubling the amount of work. 
Furthermore one should be able to see a restriction of the domain immediately from the 
heading of a routine. 
or 
Thus, we write for example 
funct sub= (nat a, nat b: a ~b) nat: 
if b = 0 then a 
else sub(pred(a), pred(b)) fi 
funct mod = (nat a, nat b: b * 0) nat: 
if a < b then a 
else mod(a -
b, b) fi 
Note that this notation is somewhat similar to the usual characterization of a set by 
means of predicates: 
{(nat a, nat b): a ~ b} 

1.9 Dijkstra's Guards 
67 
or 
{(nat a, nat b): b * 0} 
are the domains of these routines. The restricting predicates, such as 
a~ b, 
b * 0 
are also called assertions (Wirth 1973). The conditions of branchings should be expected to 
act as guards guaranteeing the assertions on partially defined routines (and primitive 
routines) occurring in a branch. 
1.9 Dijkstra's Guards 
1.9.1 Branchings are not necessarily binary, i.e. they are not always alternatives. For 
instance in the following algorithm for a lexicographic comparison of sequences a, b of ob-
jects of mode J.l we compare top(a) with top( b) and differentiate between three cases. It is 
presupposed ("asserted", see 1.8) that neither of the sequences a, b is a front part of the 
other (Fano condition). 
top(a) < top(b) -
then a is before b 
top(a) > top(b) -
then b is before a 
top(a) = top(b) -
then the comparison has to be continued with rest(a) and rest(b) 
The cases do not need to be disjoint: for integers, e.g., we have 
abs(x) = 
x 
for x ~ 0 
abs(x) = -x for x ~ 0 
Furthermore it can happen that the result is not defined for all cases which arise. Or the 
result may not always be unique, e.g. the sign of an integer or rational number x, defined 
as that number which, when multiplied by abs(x), results in x: this is so for every number 
whenx = 0. 
Finally it is recommendable in the case of the alternative, too, to state the condition ex-
plicitly for the second branch, which is the negation of the condition for the first branch 
and was indicated hitherto by else. Ease of reading and of formal transformation com-
pensate for the (trivial) extra writing. As a result of these and other considerations Dijkstra 
introduced (Dijkstra 1975) what he called guards - truth values which are written before 
an object, where 
true means: the object is permissible, i.e. may be chosen, 
false means: the object is not permissible, i.e. may not be chosen. 

68 
1. Routines 
According to the principle of substitution Boolean expressions, i.e. predicates, can 
stand in place of truth values. In accordance with the notation used so far 33 we write the 
following guarded branching 
if >truth value 1< then >Object 1< 
} 
U >truth value 2< then >Object 2< 
U >truth value n< then >Object n< fi 
n ~ 1 
as a 2n-ary (n E IN) universal operation of the specification 
funct (bool, "' bool, "' ... , boo I, Jl) 11 
with the meaning 
"evaluate first all guards, then choose 
some permissible object (possibly .Q), if one exists and none of the guards is .Q; 
.Q, if no permissible object exists or if one of the guards is .Q". 
As with the alternative, this operation, too, is not strict. 
A reasonable border case of this notation is 
if >truth value< then >object< fi 
(with the meaning "take the object, if the guard permits it, otherwise take .Q"), whereas 
if fi obviously would have the meaning .Q and can be dispensed with. 
How the alternative is written in this notation is obvious: 
if 
>truth value< then >yes-object< else >no-object< fi 
is equivalent to 
if 
>truth value< then >yes-object< 
U -, >truth value< then >no-object< fi 
Dijkstra advocates writing alternatives with guards on principle, because one is too 
easily prone to subsume the "complement" under the else case and could thus overlook 
further special cases. 
The above examples are therefore written as 
funct camp = (sequ 11 a, sequ 11 b: «Fano condition holds for a, b») bool: 
if top(a) < top(b) then true 
U top(a) > top(b) then false 
U top(a) = top(b) then comp(rest(a), rest(b)) fi 
33 Dijkstra uses an arrow -+ instead of then. Because of the manifold other uses of the arrow (for 
transitions, derivations, and as a sign of subjunction) we will retain the symbol then. Dijkstras 
sign U corresponds to the symbol 1 of the direct sum which appears in regular expressions and in 
BNF Grammars. 

1.9 Dijkstra's Guards 
funct abs = (int a) int: 
if a ~ 0 then 
a 
D a ~ 0 then -a fi 
funct sign = (int a: a =1= 0) int: 
if a > 0 then 
1 
D a < 0 then - 1 fi 
Moreover, version(*) of gcd in 1.7.1 can now be written 
funct gcd = (nat a, nat b) nat: 
if b = 0 then a 
D b > 0 " a< b then gcd(b,a) 
Db> 0 " a~ bthen gcd(a- b,b) fi 
69 
More importantly, the notation using guards frequently allows a problem-oriented for-
mulation, where sequential branching (cf. 1.3.3) would seem arbitrary. For example, the 
routine merge from 1.4.2 can be written more appropriately 
funct merge"' (sequ Jla, sequ 11b) sequ 11: 
if a=¢ 
thenb 
U b = ¢ 
then a 
U a =1= ¢ A b =1= ¢then if top(a) < top(b) then append(merge(rest(a),b), top(a)) 
U top(a) = top(b) then append(merge(rest(a), rest(b)), top(a)) 
U top(a) > top(b) then append(merge(a, rest(b )), top(b )) 
fi fi 
1.9.2 In the case of guarded branchings there is no rule as to which object out of several 
permissible objects has to be chosen. This means (in mathematical semantics) that a rou-
tine no longer necessarily defines a function but only a correspondence. Therefore when 
dealing with a guarded branching we speak of a "non-deterministic construction". 
We call a routine determinate when it describes a function and nondeterminate when it 
describes a proper (i.e. nonfunctional) correspondence, an "ambiguous function". We 
apply these terms to single calls of a routine as well. A call is determinate when there is 
exactly one possible result, otherwise it is called nondetenninate. 
The above example abs shows a routine which is determinate although it contains over-
lapping guarded branchings. This may also be the case for recursive routines, as for 
instance in the following example: 
Let 11 be an arbitrary mode with a binary associative operation p and a neutral element 
e, and 
x" =defxpxp ... xpe, neiN 
'----v-----1 
n 
is to be computed ("power operation"). This is achieved by the following (determinate!) 
routine 

70 
1. Routines 
funct pow = (Jl a, nat n) 11: 
if n = 0 
then e 
(*) 
0 n > 0 
then ap pow(a, pred n) 
0 n > 0 11 even n then pow(ap a, n/2) 
fi 
For termination it does not matter if for n > 0 the second or the third branch is used; n 
is decreased at any rate and hence the algorithm terminates. 34 (For its derivation see 1.11.4 
below). 
The slightly altered version 
funct pow' = (Jl a, nat n) 11: 
if n = 0 then e 
(**) 
0 n > 0 then ap pow'(a, pred n) 
0 even n then pow'(ap a, n/2) 
fi 
does not necessarily terminate, therefore it can also yield D: For n = 0 the third branch 
can always be chosen - this is not necessary but possible. This version is nondeterminate, 
it may result in Dorin pow( a, n), where pow is defined as above and is determinate. 
Exercise 1: Give an algorithm which merges two sorted sequences into a new sequence (see 1.4.2) 
without suppressing equal elements. 
Nondeterminate routines can be obtained in particular by inverting mappings which 
are not one-to-one 35• An example is the inversion of absinthe two versions 
funct inversabs = (int x) int: 
if x ~ 0 then x 
0 x ~ o then - x fi 
funct inversabs' = (nat x) int: 
if true then x 
0 true then -x fi 
where both x and - x can be used for x ~ 0; for x < 0 the result of inversabs is D. 
A guarded branching 
if p1 then a1 
0 p 2 then a2 
0 Pn then a. fi 
is certainly determinate for arbitrary objects a;. if P; 
11 Pk = false for all i, k (i =F k) 
("disjoint guards"). It is totally defined (i.e. =F D) if p 1 v p 2 v ... v Pn = true and no 
permissible object is equal to D. 
1.9.3 Special cases, where a guard is the constant false or the constant true, allow the 
notation to be shortened. In the first case the branch concerned can be simply eliminated 
(and if only if fi remains, it can be substituted by D). In the second case a frequent 
occurrence of only formally guarded branches (with true as guards) will be particularly 
annoying. 
34 The method for termination proofs given in 1.4.2 is also usable for nondeterminate routines. 
35 In classical cryptology, with the use of homophones it is precisely those correspondences which 
are the inverse of mappings that are considered. 

1.9 Dijkstra's Guards 
Thus we write 
(2 0 3 0 5 0 7) 
for «some prime number smaller than 10» 
and 
(3 0 - 3) 
for «some solution of x 2 = 9» or for inversabs' (3). 
In general, if all guards are true, we write 
a; for true then a;, 
and replace finally 
if vvvvvvv f i by ( 'VVVVVV ) 
• 
To this end we can introduce the construction 
>truth value< then >Object< 
as 
>guarded object< 
71 
and 0 as a symbol for the (commutative and associative) formation of a choice of objects 
and guarded objects, and enclose this choice in the brackets if fi (or if no object is guarded, 
possibly in (}). 
Note that 
(2 0 3) = (3 0 4), 
and also (2 0 3) = (2 0 3) 
are equivalent to (true 0 false), as the sets of possible values are equaP6• 
Likewise, 
a = (b 0 c) 
does not mean a = b v a = c (which is determinate), but is equivalent to 
(a= b D a = c) 
It is now possible to write the above routine pow in the following variant: 
if n = 0 
0 n > 0 
0 n > 0 
then e 
then (ap pow(a, pred n) 0 pow( a, pred n) pa) 
" even n then (pow(ap a, n/2) 0 sq(pow(a, n/2))) 
fi 
where the primitive sq can be defined as 
funct sq = (Jlx) 11: xpx 
36 (true 0 false) is by no means a new object; funct arbitbool = bool: (true 0 false) is a (para-
meterless) nondeterminate routine. 

72 
1. Routines 
1.9.4 The fact that for the leftmost-innermost computation rule (cf. 1. 7.3) expressions in 
argument position are evaluated only once, turns out to be particularly advantageous in 
non-determinate situations. One intuitively expects that the following two routines 
funct dupl = (int t) int: t + t 
and 
funct dupl' = (int t) int: 2 x t 
are equivalent. For the non-determinate calls 
dupl(a D b) and dupl'(a D b) 
this is so if the leftmost-innermost rule is used; for some other computation rules it is not 
so. 
Closely connected with this is the fact that unfolding (in contrast to the determinate 
case, see 1.7.1) is no longer harmless: the call 
dupl(a Db) 
is not equivalent to the expression 
(aD b) + (aD b) 
obtained by unfolding. Folding, on the other hand, does not give rise to new problems, but 
generally leads to less nondeterminacy; in the extreme it may establish determinacy, see 
1.11.3, "descebdant". 
So far we tacitly assumed that the guards are always determinate. In accordance with 
the leftmost-innermost computation rule, we repeat that for an alternative with a non-
determinate condition or for a guarded branching with nondeterminate guards, evaluation 
of the condition is the first step to be taken. This has the effect that for example 
if (true D false) then a else b fi 
is indeed equivalent to 
(aD b) 
1.10 Pre-Algorithmic Formulations by Means of Choice and 
Determination 
Problems are frequently posed in the "descriptive" or "implicit" form of a predicate which 
characterizes the result or results. The predicate is not always a simple one as in 

1.10 Pre-Algorithmic Formulations by Means of Choice and Determination 
73 
"that x: x = 3". 
For the descriptive specification 
"a natural numbers whose successor is a" 
the characteristic predicate is 
succ(s) = a 
Problems which can be characterized by a predicate include all inverses of functions 
and mappings as well as the solution of equations "some y: y = f(x)", hence e.g. 
"an integer whose square is 1 ", 
"a zero of the polynomial 9'(x)", 
"a natural number t which, when multiplied by 0, yields 0". 
Other nondeterminate implicit specifications are 
"an odd natural number", 
"a sequence v which is a right part of a given sequence a (a trailer)". 
The specification may even make use of quantifiers, as in the last example where the 
characteristic predicate is 
3 sequ " u: a = u & v 
For 
"a maximum element t from the set of all natural numbers which both divide a and 
divide b" 
the characteristic predicate reads 
t I a A t I b A v nat y: (y I a A y I b =- y ~ t) 
In general such predicates do not uniquely determine the characterized element. Frequent-
ly it remains to choose from a finite or from a countable number of possibilities; occa-
sionally, as e.g. in the cases 
"an integer whose square is -1 ", 
"a solution toft x 0 = 1", 
no element with the desired property exists at all. 
1.10.1 The Choice Operator 
In order to stress the character of the choice of an element from a subset (characterized by 
a predicate p) of objects of the mode 11 we use the expression "some ... " and write 
tf{Jlx: p(x) }, 
in short 
11 JlX: p(x) for "some x such that p(x)" 

74 
1. Routines 
using the (non-deterministic) choice operator 'I· In the case that the characterized subset is 
empty we define the result to be D, more precisely 
(q JlX: p(x)) = D <* def {Jtx: p(x)} = 1/J 
Hence some of the above examples read: 
'I nat t: 
'I nat s: 
'I int x: 
'I nat t: 
t I a A t I b A v nat y: (y I a A y I b =- y ~ t) 
succ(s) = a 
X j 2 = 1 
t X 0 = 0 
, sequ J1 v: 3 sequ J1 u : a = u & v 
, nat t: 
t x o = 1 
Considered as routines the first two examples are determinate but only partially defined: 
a = b = 0 admits every natural number as a common divisor, there is no maximum ele-
ment in this set; 
a = 0 is not the successor of any natural number. 
By introducing suitable assertions we obtain the totally defined routines 
funct gcd = (nat a, nat b: a * 0 v b * 0) nat: 
'I nat t: t I a A t I b A v nat y: (y I a A y I b =- y ~ t) 
funct pred = (nat a: a * 0) nat: 
'I nat s: succ(s) = a 
The last example above is also determinate, however the result is D. The remaining 
examples are non-determinate: 
funct unit 
= lnt: 11 int x: x i 2 = 1 
funct arbitrary = nat: 11 nat t: t x 0 = 0 
funct trailer 
= (sequ J1 a) sequ J1: 11 sequ J1 v: 3 sequ J1 u : a = u & v 
The choice operator 11 is non-deterministic in the same way as described for guarded 
branchings: the choice is arbitrary. 37 If the characteristic subset has just one element the 
choice operator is determinate. 
Choice operator and guarded branching are related: 
Provided both a and b are determinate and different from D, 
37 The p-operator (Hilbert, Bernays 1934) which frequently occurs in logic is a deterministic imple-
mentation of the choice operator. It is defined as follows: IJ i..x: p(x) means min {i..x: p(x)}, where 
a linear Noetherian ordering (well-ordering) is assumed in i... 
The condition that a linear Noetherian ordering exists in i.. (and that it can be constructed) also 
guarantees that the p-operator is well defined for non-empty sets: the minimal element can always 
be taken, comp. also 2.4. 
The ,_operator was introduced by Hilbert and Bernays 1939. 

1.10 Pre-Algorithmic Formulations by Means of Choice and Determination 
(aD b) is equivalent to 11J1X: x = a v x = b 
and 
if p then aD q then b fi is equivalent to 11 J1X: (p ~ (x = a)) v (q ~ (x = b)) 
These relationships even could be used as a definition for the guarded branching. 
Exercise 1: Show by reduction to the ,-operator, that 
if p then a 0 q then b fi 
is the same as 
if q then b 0 p then a fi 
. 
Exercise 2: Show by reduction to the ,-operator that 
if p then a 0 q then b fi 
is determinate if p 
A q yields false, 
is defined if 
p v q yields true. 
Exercise 3: Under what conditions is 
if p then a 0 q then a fi 
equivalent to a? ("Splitting a tautology") 
1.10.2 The Determination Operator 
75 
The choice operator 11 occasionally produces a unique result, i.e. it may be determinate. 
The determination operator38 1 (from Greek 'laoc;) is defined as follows: 
It corresponds to the choice operator if the choice operator is determinate otherwise it 
yields the result D. It is therefore, by definition, always determinate. 39 We write 
z{Jlx: p(x) }, 
in short 
1 J1X: p(x) for "that x such that p(x)". 
Hence we can write 
funct gcd = (nat a, nat b: a * 0 v b * 0) nat: 
1 nat t: t I a A t I b A v nat y: (y I a A y I b => y ~ t) 
and this corresponds to the definition in 1.10.1, if we can prove determinacy - which is 
not difficult in this case: If !1 and t2 satisfied the predicate, we would have t1 ~ t2 and t2 ~ 
t1 • From the antisymmetry of an ordering -
here ~ - it follows that t1 = t2 , i.e. the 
result is unique. 
Exercise 1: Show that gcd(a, b) * Q holds within the given domain. 
38 Also called description operator (McCarthy 1961 ). 
39 Hilbert and Bernays 1934, based on Whitehead, Russell1910, also introduced the determination 
operator. In 1945 Zuse used it in his "Plankalkiil" for programming purposes. It has been 
rediscovered in 1966 by Landin, who writes "this discussion . . . reveals the possibility that 
primitives might be sensationally non-algorithmic". 

76 
1. Routines 
Exercise 2: Show that the definition 
funct gcd = (nat a, nat b: a * b v b * 0) nat: 
1natx:xla "xlb "vnaty:(yla "Ylb,. Ylx) 
is equivalent to the above definition which is based on the linear ordering ~. 
Exercise 3: Explain the determination operator in terms of the choice operator. 
Likewise we can write 
funct pred = (nat a: a * 0) nat: 
1 nat s: succ(s) = a 
if we conclude uniqueness from the 4th Peano axiom for natural numbers which states that 
succ(s1) = succ(s0 implies s1 = s2• 
This will be continued when introducing natural numbers as a computational structure 
in 3.5.1, see also 3.5.2. 
Exercise 4: Let M be a set, 0 an element of that set and succ a mapping M .... M with the properties 
(PI) 0 eM 
(P2) 
X EM => succ(x) eM 
(P3) x eM =o succ(x) * 0 
(P4) x eM 1\ y eM " succ(x) = succ(y) .. x = y 
Show that: 
I) M is infinite: there exists a one-to-one mapping of M onto a proper subset of M. 
2a) The propostion 
"For a eM· a * 0 there exists s eM: succ(s) = a" does not follow from (PI) to (P4). 
2b) The proposition 
"There exists nos eM: succ(s) = s" does not follow from (PI) to (P4). 
3) Let pred be defined for a eM: a * 0 as in the routine above. Which of the properties 
are needed to prove the following? 
x eM =o pred(succ(x)) = x 
4) What is required to show that 
x eM, x * 0 .. succ(pred(x)) = x ? 
1.11 Semantics of Non-Deterministic Constructions 
1.11.1 Pre-Algorithms and Algorithms 
Certain specifications with the operators Tf and tare directly of algorithmic character, e.g. 
Tf JlX: X = a V X = b V X = C 
or 
Tf J1X: X = f(d) 

1.11 Semantics of Non-Deterministic Constructions 
77 
if the individual ,steps", viz. the comparisons and the computation of j, are guaranteed to 
be effective. 
Generally it can be said that there is always an algorithm to determine 
11 J!X: p(x) or 1 JlX: p(x) 
if p is a Boolean routine totally defined on 11 and thus is terminating and determinate, and 
if 11 is a finite set. Then a trivial machine exists, the search machine, which by exhausting 11 
establishes whether such a characteristic object exists. If so the machine delivers such an 
object as a result, otherwise it yields D40• However the operators 11 and 1 are not tied down 
to a particular (search) algorithm. 
In the general case of an infinite object set 11 and arbitrary predicates, the effectiveness 
of such constructions is, however, not obvious. In particular, in the case of routines for 
whose formulation the quantifiers v and 3 are used, besides the operators 11 and 1, we may 
be wise to speak of pre-algorithms. At any rate they need transforming into algorithms or 
at least a proof that they are algorithms. This could perhaps be done by eliminating the 11 
or 1 operators (or by restricting their application to finite sets), which may give rise to 
recursive definitions. The technique of unfolding and folding (1. 7.1) plays an important 
part here, as the following examples will show. 
A pre-algorithmic, predicative (i.e. non-operational) formulation has some advan-
tages. First of all it may be problem-oriented, many problems occur in this form. 41 
Furthermore it often permits -
in the simplest way -
the establishment of properties 
which are not conspicuous in the algorithms later developed from them. Commutativity 
and associativity for gcd are almost immediately obvious, solely by virtue of the corre-
sponding laws for Boolean operations. 
In working with predicative formulations the following fundamental transformation 
rule, which can be called the Exportation of Independent Conditions, is frequently used: 
For (defined and determinate) predicates P(x), Q(x) on 11 and predicates p, q in which 
x does not occur free, under the condition 
p A q => ((3 !JX: P(x)) <* (3 !JX: Q(x))) 
the following holds: 
'1 J!X: (p A P(x)) V (q A Q(x)) 
is equivalent to 
if p then , J!X: P(x) 
D q then ,,.x: Q(x) fi 
(and analogously for 1). 
40 An index system consisting of edge-punched cards plus accessories is a good approximation to 
such a machine. The exhaustive solution by means of a search machine is sometimes contemptu-
ously referred to as the .. British Museum Method" because of its inefficiency. 
41 Not however all. For example in developing arithmetic, pred is constructed from succ in a predica-
tive way, but addition and multiplication are introduced naturally in a recursive form. 

78 
1. Routines 
1.11.2 Deriving Algorithms from Pre-Algorithms 
Unfolding followed by algebraic transformations and subsequent folding often permits the 
elimination of 1f and 1 operators, thus transforming a pre-algorithm into a proper 
algorithm. 
A first example begins directly with algebraic transformations: proceeding from the 
characterization given at the end of 1.7.1 we obtain the following 
funct mod = (nat a, nat b: b =1= 0) nat: 
1 nat r: (3 nat q: r + q x b = a A 0 ~ r < b) 
1 nat r: (a ~ b A 
3 nat q: r + q x b = a A 0 ~ r < b) 
v (a < b A 
3 nat q: r + q x b = a A 0 ~ r < b) 
if a ~ b then 1 nat r: (3 nat q: r + q x b = a A 0 ~ r < b) 
D a < b then 1 nat r: (3 nat q: r + q x b = a A 0 ~ r < b) fi 
if a ~ b then 1 nat r: (3 nat q: r + (q -
1) x b = a -
b A 0 ~ r < b) 
D a < b then 1 nat r: r = a 
fi 
if a ~ b then 1 nat r: (3 nat q': r + q' x b 
D a < b then a 
From this, folding and rewriting as an alternative yields 
funct mod = (nat a, nat b: b =1= 0) nat: 
if a ~ b then mod(a -
b, b) 
else a 
fi 
a- b A 0 ~ r <b) 
fi 
One starts with the ,splitting of a tautology" which in our case leads to the distinction 
between the cases a ~ b and a < b 42 and uses exportation. Subsequently, one uses 
algebraic modifications, in our case 
q x b -
b = (q -
1) x b (distributive law) 
and simplifications. 
It must in principle be proved that the routine acquired in this way terminates. In this 
case we obtain the proof of termination (comp. 1.4.2) by considering the first argument: it 
is monotonously decreasing in every call. After at most a recursion steps the algorithm 
terminates. A better estimation will be found in 1.12. 
gcd is a further example. It is convenient here first to introduce the maximizing 
operator ,greatest element" with respect to an ordering p on a Subset JV of 11 43 
42 Intuition is generally required to find such a suitable splitting. 
43 Due to the antisymmetry of p, such an x - if it exists -
is uniquely defined, see 1.10.2. 
We have also the weak maximum operator (,maximal element") 
wmaxP.! =cter'IJlX:xe I A vze.A:xpz=>x=z 
Note that z p x -
due to anti symmetry - implies -, (x p z) v x = z. that is x p z => x = z. The 
reverse holds if the ordering is linear. Then maxP and wmaxP are the same. 

1.11 Semantics of Non-Deterministic Constructions 
With 
Jll(a, b) = {nat t: t Ia " t I b} 
the following holds 
funct gcd = (nat a, nat b: a * 0 v b * 0) nat: 
max:ii .AI(a, b) 
Obviously 
max:ii A/(a,a) =a, 
and 
JV(a,b) = Jll(a -b,b) 
holds for a ~ b. 
Hence 
gcd(a, a) = a 
and fora~ b: 
gcd(a -
b, b) = gcd(a, b) 
Furthermore .k(a, b) = .AI(b, a), and thus 
gcd(a, b) = gcd(b, a); 
for a ~ b we have 
gcd(a, b -
a) = gcd(a, b) 
A possible recursive version for gcd is therefore 
funct gcd' = (nat a, nat b: a * 0 v b * 0) nat: 
if a ~ b then gcd'(a, b -
a) 
U a= b then a 
U a ~ b then gcd'(a -
b, b) fi 
79 
which, however, does not always terminate. The first line of the branching can always be 
chosen for a = 0, the third line always for b = 0. Mere restriction to a * 0 1\ b * 0 does 
not work. If, in addition, the first and third lines are suitably restricted we obtain disjoint 
guards and the determinate routine 

80 
funct gcd = (nat a, nat b: a =1= 0 " b =1= 0) nat: 
If a < b then gcd(a, b -
a) 
D a= b then a 
D a > b then gcd(a -
b, b) fi 
1. Routines 
whose domain indeed has been restricted to a =1= 0 " b =1= 0, such that gcd terminates. This 
is shown according to the method given in 1.4.2 by choosing o(a, b) = a + b. This also 
shows that the algorithm for computing gcd(a, b) requires at most a + b recursion steps. 
The following algorithm is unsymmetrical and hence less elegant, but it even terminates 
for a = 0 and forb = 0 (comp. 1.7.1 (*)): 
funct gcd = (nat a, nat b) nat: 
if b = 0 then a 
D b > 0 " a< b then gcd(b,a) 
D b > 0 " a ~ b then gcd(a -
b, b) fl 
It results from the relations derived earlier (under further application of commutativity) 
and from the fact that 
JV(a, 0) = {nat x: x Ia " x IO} = {nat x: x Ia}, 
thus 
max~ JV(a, 0) = a 
The algorithm terminates always. The function o(a, b) = a + 2b can be used to show 
this. Altogether the computation of gcd(a, b) according to this algorithm terminates at the 
latest after a + 2b steps. 
If, however, we note that an execution of the second line is necessarily followed by an 
execution of the third line, we combine them to obtain 
funct gcd = (nat a, nat b) nat: 
if b = 0 then a 
D b > 0 " a < b then gcd(b -
a, a) 
D b > 0 " a ~ b then gcd(a -
b, b) fi 
The function o(a, b) = a + 2b can also be used here to prove termination. 
The version (b) of 1.4.1, too, can be obtained directly from the pre-algorithmic version 
by showing that gcd(a, b) = gcd(b, mod(a, b)) - in a similar way as above and by using 
commutativity. (For the proof of termination see 1.4.1 and 1.6.2.) 
Another example is subtraction considered as the inversion of addition, where we refer 
to an already recursively defined routine: 
funct sub = (nat a, nat b: a ~ b) nat: 
1 nat x: add(x, b) = a, 
funct add = (nat a, nat b) nat: 
if b = 0 then a 
else succ add(a, pred b) fi 

1.11 Semantics of Non-Deterministic Constructions 
Unfolding yields 
funct sub "' (nat a, nat b: a ~ b) nat: 
1 nat x: if b = 0 then x 
else succ add(x, pred b) fi = a 
if b = 0 then 1 nat x: x = a 
else 1 nat x: succ add(x, pred b) = a fi 
if b = 0 then a 
else 1 nat x: add(x, pred b) = pred a fi 
81 
Here we have used the basic rule about exportation of independent conditions and the 
algebraic modification 
pred succ y = y 
By folding we finally obtain (comp. 1.8) 
funct sub "' (nat a, nat b: a ~ b) nat: 
if b = 0 then a 
else sub (pred a, pred b) fi 
Considering the previous examples we can see that the transformations could just as 
easily have been carried out if the routines mod, gcd and sub had been defined using the ,... 
operator. The same recursive versions would have resulted. There is no difference in the 
technical treatment of the two operators (comp. the basic rule for 11 and for 1 at the end of 
1.11.1); the z..operator only needs an additional uniqueness proof. 
Exercise I: Derive the non-deterministic routine (*) of 1.9.2 from 
funct pow = (Jla, nat n) 11: 
if n = 0 then e 
else appow(a, pred(n)) fi 
Exercise 2: Derive an algorithm from 
funct less = (nat n) nat: 
'1 nat x: x < n 
Exercise 3: What does 
1 nat x: x ~ a A sub(x, a) = b 
mean, based on the recursive routine sub as defined above? 
A repeated warning is appropriate: The use of the existential operators 71 and 1 is 
dangerous in that constructs are possible for which no operational solution is known. 
However, in the overwhelming majority of practical cases, this danger can be mastered. 
Nevertheless, in order to obtain operational solutions, tools from second order predicate 
logic, e.g. induction (comp. 1.6), may be necessary. 
Incidentally, the use of the quantifiers v and 3 in recursive definitions also gives rise to 
complications in fixpoint theory (see Manna 1974, p. 368, Example 5-7.2), the functional r 
being not necessarily any longer continuous. 

82 
1. Routines 
1.11.3 Mathematical Semantics of Non-Determinate Routines 
1.11.3.1 The mathematical semantics of non-determinate routines considers them (after 
having introduced, as in 1.5.1, the pseudo object .Q) as left-total correspondences. In 
building up a fixpoint theory, one introduces the relation ,weaker" for correspondences. 
Just as in the case of functions, a correspondence ! 1 approximates a correspondence h, if and 
only if / 1 is more often undefined than/2 but is otherwise equal to h. This intuitive description can be 
formalized (Egli 1975) by differentiating between the two cases: the value .Qis possible for a givenxin 
! 1, or it is not, 
f 'f 
+ 
n>+. [if(X, Q)E/1 1\Y * .Q:(x,y)E/1 =>(X,y)Eh 
jL 2'*def'IXE!J:l ,JE<n. • 
-
tf (X, .Q) ~ft: 
(X,y) E/1 # (x,y) Eh 
For the routine pow in 1.9, version(**) is therefore weaker than version(*). 
The relation . ~ . is again antisymmetric and thus defines a (partial) ordering for correspondences, 
which agrees for functions with the one given in 1.5.2. For more details, see de Bakker 1976, Plotkin 
1976, Broy et al. 1979. 
1.11.3.2 Apart from the ,weaker"-relation, there is another important relation between 
certain non-determinate routines, a relation which becomes trivial for determinate 
routines. 
If Fd and Fare two ,ambiguous functions" (1.9.2) over a domain ;;}, Fd is said to be a 
descendant ofF (McCarthy 1961) if for each x e ;;} every possibly resulting value of Fd(x) 
is also a possibly resulting value of F(x), and Fd(x) is not undefined unless F(x) is 
undefined. 
Introducing .Q as in 1.5.1, Fd and Fwill be made total and the conditions above reduce 
to inclusion: 
v x e;;} +, y e 9f +: (x,y) eFd => (x,y) eF, in shortFd ~ F 
We take this as the general definition of a descendant Fd of F. 
The concept of a descendant is fundamental for working with non-determinate 
routines: 
By its very nature, a problem which is solved by the routine F is also solved by a 
descendant Fd. 
Note that the converse is not true. However, the descendant of a descendant is again a 
descendant - the relation is transitive. 
If F1 is a descendant of F2 and vice versa, then F1 and F2 are (functionally) equivalent. 
In connection with the leftmost-innermost rule, folding can result in computing 
common argument expressions once and thus can lead, if it changes anything at all, to a 
descendant. See the expression (aD b) + (aD b) in 1.9 and its descendant dupl(a D b). 
Descendants, moreover, turn up naturally in connection with choice operators. 
Let Fbe a choice expression '1 11x: p(x). We get a descendant Fd if the set to which the 
choice operator is applied is restricted without becoming empty. Thus the expression 
1111x: q(x) is a descendant Fd ofF, if v JlX: q(x) => p(x), but {11x: q(x)} * 0 unless 
{11x: p(x)} = 0. 
We have the fundamental transformation rule 

1.11 Semantics of Non-Deterministic Constructions 
83 
'111X: p(x) 
--+-- {(v 11x: q(x) => p(x)) A ((3 11x: p(x)) => (3 11x: q(x))) 
'111X: q(x) 
The one-sidedness expresses the fact that in general we do not have equivalence. The 
applicability condition is stated to the right. 
Example: 
Let F be the algorithm (*) of 1.9 
funct pow = (lla, nat n) 11: 
(*) 
if n = 0 
then e 
D n > 0 
then a p pow(a, pred n) 
D n > 0 A even n then pow(a p a, n/2) 
fi 
Let Fd emerge from F through restriction of the second branch to 
n > 0 A odd n then a p pow(a, pred n) 
Fd is a descendant of F. As the guards are now disjoint, Fd can be rewritten as a deter-
ministic implementation (.*.) of the nondeterministic but determinate algorithm (*): 
funct pow = (11 a, nat n) 11: 
if n = 0 
then e 
D n > 0 A 
odd n then a p pow(a, pred n) 
D n > 0 A even n then pow(a p a, n/2) 
fl 
or 
if n = 0 
then e 
(***) 
elsf odd n then a p pow(a, pred n) 
else pow(a p a, n/2) 
fi 
1.11.3.3 Nondeterminate algorithms can be as useful as determinate algorithms. An 
example is the following algorithm which yields an arbitrary natural number smaller than n 
(McCarthy 1961): 
funct less = (nat n: n > 0) nat: 
if n > 1 then /ess(pred n) 
D n > 0 then pred n 
fi 
less is a nondeterminate algorithm: /ess(n) is not determinate for n > 1. 
Considering the "ambiguous function" less as primitive and defining 
funct ult = (nat a, nat n) nat: 
if n = 0 then tff [a J 
else ult(a, less(n)) fi 
then 

84 
1. Routines 
is the only fixpoint of ult; ult(a, n) always terminates and yields the result t (a). 
Let 
funct ultg = (nat a, nat n) nat: 
if n = 0 then t (a) 
else ultg(a, g(n)) fl 
where g is a descendant of less. Then ultg(a, n), too, always terminates and yields the result 
t (a J . Note that both 
(nat n: n > 0) nat: pred n 
and 
(nat n: n > 0) nat: 0 
are descendants of less. 
Non-deterministic algorithms have very slowly made their way into computer science. 
Floyd wrote in 1967 "Programs to solve combinatorial search problems may often be 
simply written by using multiple-valued functions ... " and gave a simple non-deterministic 
algorithm for finding some solution of the eight-queens problem. 
The importance of nondeterministic, but determinate algorithms is that the nondeter-
ministic construction provides a certain freedom which can often be skilfully used during 
the further development of the program ("delayed design decision"). 
To find an implementation strategy from the point of view of efficiency is often a dif-
ficult but worthwhile task. A totally defined, terminating, nondeterministic algorithm lies 
between Scylla and Charybdis: if cases are added or if the guards are extended there is the 
danger that it no longer terminates or even gives wrong results. If cases are omitted or if 
the guards are restricted there is the risk that it aborts in an undefined situation. 
An example for a determinate implementation of, say, (a U b), apart from a, is the 
branching 
if odd n then a 
U even n then b fi 
which depends on an arbitrary parameter n. Anyone who cares to can implement (aU b) 
stochastically if he doesn't mind the work involved. If a well-ordering exists for the objects 
to be chosen, the smallest element with regard to this ordering can be selected (see 1.1 0.1, 
,u-operator), since min(a, b) is also a descendant: (aU b) is nothing but if true then aU 
true then b fi; this can be restricted in an admissible way to 
if a ~ b then a U a ~ b then b fi 
This example suggests a final remark about the nature of non-deterministic construc-
tion. Floyd writes: "Because the word 'nondeterministic' has a double meaning, it is 

1.11 Semantics of Non-Deterministic Constructions 
85 
perhaps desirable to make clear that nondeterministic algorithms are not probabilistic, 
random or Monte Carlo algorithms". In actual fact it is simply required that an arbitrary 
object be chosen. This may take place according to some rule (such as "the respective left-
most", "the least often used") but does not have to do so. Floyd used the word "free will". 
Dijkstra uses a similar expression: " ... not equipped with an unbiased coin, but with a 
totally erratic demon, such a demon makes all ... probabilistic questions a priori void". 44 
We say the choice takes place arbitrarily and may speak of ambiguous functions. 
"Ambigous functions are not really functions. For each prescription of values to the argu-
ments the ambigous function has a collection of possible values" (McCarthy 1961). 
Apart from this totally erratic nondeterminism, where the choice does not discriminate 
undefined objects, variants have been discussed where a choice is altogether undefined 
when one permissible object is undefined (demonic nondeterminism), or that an undefined 
object is only chosen when no defined object is permissible (angelic nondeterminism) 
(Hoare, see Manna 1980). 
1.11.4 Operational Semantics of Non-Deterministic Algorithms 
Within the framework of mathematical semantics one can only differentiate between func-
tions and (proper) correspondences. The terms determinate and nonderterminate refer 
only to the cardinality of the set of values which may result from an algorithm. Opera-
tional semantics, on the other hand, allows a more subtle analysis in asking whether a 
certain result can be obtained in one or in several ways. We may think here of the obvious 
generalization to a non-deterministic text substitution machine 45 or also of a suitably 
extended stack machine, which first evaluates the guards and then chooses a permissible 
branch. 
The following considerations are based particularly on the concept of "execution" 
(comp. 1.7.3). 
An execution is called regular if it ends with a result different from D. 
An execution is called abortive if it ends with the result D as a consequence of leading 
to an operation with an undefined result (in particular to a guarded branching with an 
empty choice). 
Otherwise, that is if the execution does not come to an end (and thereby yields D), it is 
nonterminating. Thus, a terminating execution is regular or aborts. 
Routines and calls of routines can now be classified according to the number and kind 
of possible executions. For a call F(x) of a routine we consider the set six (F) of all possible 
executions and the set {!.f x(F) of all regular executions. 
A call (of a nondeterministic construction) is determinate if and only if all the execu-
tions yield the same result. 
A call F(x) is said to be regular if all the possible executions are regular (i.e . .s:lx(F) = 
{!.fx<F)), and it is called terminating if all the possible executions are terminating. A routine 
is called regular/terminating if all permissible calls are regular/terminating. 
44 If, however, the totality of possible objects is exhausted, one also speaks of "backtracking non-
determinism" (Kennaway, Hoare 1980). 
45 Again, several computation rules are possible (Broy 1980). 

86 
1. Routines 
A special way to obtain a descendant, already used informally above, is the restriction, 
i.e. the modification of a guarded branching by changing its guards P; into p{ subject to the 
condition that 
pf => P; and p 1 v ... v Pn = p{ v . . . v p~ 
hold. The descendant Fd obtained by restriction has the following property which 
characterizes an operational descendant: 
The set of executions possible for Fd is certainly contained in the set of executions for F 
itself, i.e. 
All regular executions remaining for Fd are also regular executions for F, i.e. 
There is no new abortive execution caused by the restriction of the choice set. 
Lemma: If Fd is an operational descendant ofF, then 
(i) F is terminating => Fd is terminating 
(ii) F is regular => Fd is regular 
(iii) F is determinate => Fd is determinate. 
The possible relations between the original routine and a descendant can be illustrated 
by an example (comp. (**) in 1.9) 
funct pow = (Jla, nat n) 11: 
if n = 0 
then e 
Dn=O 
~~~w~p~~ 
D n > 0 
then a p pow(a, pred n) 
D n > 0 " even n then pow(a p a, n/2) 
fi 
This routine allows different executions, is not determinate and, moreover, does not 
necessarily terminate. The reason is that for every parameter value different executions are 
possible, those which yield the correct result -
the power, and those where the second 
branch is ultimately always chosen - these executions do not terminate. 
A descendant results from cancelling the second branch. Although it is not yet deter-
ministic it is already determinate and regular (therefore also terminating). In fact, it is the 
version(*) considered in 1.9. 
If the fourth branch is now cancelled as well, a descendant results which is again 
regular. Indeed, it could be rewritten in the deterministic form of an alternative. This, 
however, is not an efficient implementation compared with the deterministic implementa-
tion(.*.) obtained in 1.11.3 by restricting there the second branch. 
If, however, the third branch is cancelled instead of the fourth, no descendant results; 
the algorithm is terminating but is not regular, because the choice in the case of odd n is 
empty and hence the execution aborts. 
From the mere fact that a descendant is regular/terminating it cannot be concluded 
that the original algorithm has the corresponding property. 

1.12 Routines with a Multiple Result 
87 
Two algorithms F1, F2 are called operationally equivalent if 
If F1 is an operational descendant of F2 and vice versa, then F1 and F2 are operationally 
equivalent. 
Operationally equivalent algorithms are, of course, also equivalent in the sense of 1.1. 
(int a, int b) int: (aD b) and (int a, int b) int: (b D a) 
are, for example, operationally equivalent and thus (functionally) equivalent. The 
converse is not true: 
(int a, int b) int: (a + b) x (a -
b) 
and 
(int a, int b) int: a2 -
b2 
are not operationally equivalent. 
With regard to the "weaker" -relation of mathematical semantics no general statements can be 
made about operational descendants: On the one hand nonterminating or abortive executions may 
disappear (e.g. by cancelling a branch), hence .Q could disappear from the total of possible results. On 
the other hand also some proper values may disappear; this generally renders two operational 
descendants incomparable. 
1.12 Routines with a Multiple Result 
A routine can have a multiple result. In this case the expressions for computing the 
individual components are placed side by side and enclosed in brackets ("collateral", just 
as the parameters). The individual modes are given respectively in the heading. For ex-
ample 
funct ord = (int a, int b) (int, int): 
if a ~ b then (a, b) 
D a~ b then (b,a) fi 
In order to give an example of a predicative formulation as well, we extend the defini-
tion of mod in 1.11.4 to obtain quotient and remainder. The elimination of the determina-
tion operator is done in analogous fashion to mod. The following results 46 
46 Comparison of the specification of natdiv with the related one of mod shows that z nat q -
provided q is not needed -
can be weakened to 3 nat q. This justifies the word 'existential 
operators' for 'I and z. 

88 
1. Routines 
funct natdiv = (nat a, nat b: b =1= 0) (nat, nat): 
1 (nat q, nat r): r + q x b = a " 0 ~ r < b 
If a ~ b then 1 (nat q, nat r): r + q x b = a " 0 ~ r < b 
D a < b then 1 (nat q, nat r): r + q x b = a " 0 ~ r < b fi 
if a ~ b then 1 (nat q, nat r): r + (q -
1) x b = a - b " 0 ~ r < b 
D a < b then 1 (nat q, nat r): q = 0 " r = a 
fi 
if a ~ b then (1, 0) + 1 (nat q', nat r): r + q' x b = a - b " 0 ~ r < b 
D a < b then (0, a) 
fi, 
and finally by folding 
funct natdiv = (nat a, nat b: b =1= 0) (nat, nat): 
if a ~ b then (1, 0) + natdiv(a - b, b) 
D a < b then (0, a) 
fi 
The addition indicated by + is to be extended componentwise to pairs of numbers. 
The termination of the resulting algorithm is to be proved just as in the case of mod. 
The number of recursion steps is obviously equal to the quotient q. This also yields a better 
estimate (by comparison with 1.11.4) of the number of recursion steps in mod. 
Obviously the above algorithm can be partitioned into the following components -
without affecting termination -
funct div = (nat a, nat b: b =1= 0) nat: 
if a ~ b then 1 + div(a - b, b) 
D a < b then 0 
fi 
and 
funct mod = (nat a, nat b: b =1= 0) nat: 
if a ~ b then mod(a -
b, b) 
D a < b then a 
fi 
Exercise 1: Transform the "combined" routine based on div and mod 
funct dm = (nat a, nat b: b * 0) (nat, nat): 
(div(a, b), mod(a, b)) 
into an independent recursive routine by means of unfolding, re-shaping and folding 
("merging"). 
The (non-deterministic) task mentioned in 1.10.1 of forming a trailer permits a natural 
extension to the parsing task ( cf. trailer in 1.1 0.1) 
funct part = (sequ ~a) (sequ 14 sequ ~): 
17(sequ ~u, sequ ~v): a = u & v 
with a variant which for non-empty sequences yields a partition into a (possibly empty) left 
part, a "selected" element and a (possibly empty) right part 

1.13 Structuring of Routines 
funct parse .. (sequ "a: a =F 0) (sequ "' "' sequ Jl): 
11 (sequ JlU, Jlf, sequ JlV): a = u & append(v, t) 
1.13 Structuring of Routines 
89 
We have already met examples of structured routines in the systems heron, s (1.3.1) or gcd, 
mod (1.4.1. (b)). Structuring is an important tool for program development ("structured 
programming"). By means of subordination and subdivision routines and systems can fre-
quently be written more clearly and be amenable to more efficient execution. Subordina-
tion, if it is made explicit can help to save listing of parameters ("suppressed parameters"). 
Subdivision for the extraction of common subexpressions suggests an abbreviated notation 
- object declaration. Object declarations introduce identifiers for intermediate results -
this leads to the concept result parameter. 
1.13.1 Structuring by Means of Abstraction and Embedding 
1.13.1.1 A routine can be structured by deriving a new (sub-)routine from a subexpression 
occurring in its body. This method is called abstraction (comp. 1.1). Thus from a subex-
pression of the routine for calculating the volume of a truncated cone (comp. 1.2) 
functj .. (real r, real R, real h) real: t x (r2 + r x R + R 2) x h 
a new routine can be derived 
funct m .. (real r, real R) real: r 2 + r x R + R 2 
Using this routine we obtain 
functj .. (real r, real R, real h) real: t x m(r, R) x h 
After a (sub-)routine has been defined by abstraction, the replacement of the correspond-
ing subexpression by a call of this routine formally represents a "folding" (1.7.1). 
An abstraction is especially beneficial if it renders possible the replacement of several 
subexpressions by simultaneous unfolding, subexpressions which are either textually 
identical or coincide in form up to some additional parametrization. Thus, the system 
(heron, s) in 1.3.1 originates from Herons formula after modification into 
(a+ b + c)/2 x ((a+ b + c)/2- a) x ((a+ b + c)/2- b) x ((a+ b + c)/2- c) 
by extracting the common subexpressions (a + b + c)/2. However, since each factor has 
the form 
(a + b + c)/2 - x 

90 
1. Routines 
these similar subexpressions, too, can be extracted with the help of a single routine, which 
leads to the system (heron, t): 
funct heron = (rat a, rat b, rat c) rat: 
t(a, b, C, 0) X t(a, b, C, a) X t(a, b, C, b) X t(a, b, C, C), 
funct t = (rat u, rat v, rat w, rat x) rat: 
(u + v + w)/2 -
x 
The following method describes in general the extraction of (common) subexpressions 0'1, 
0'2, • 00, 0'n from a routine g by abstracting them as routines s1, s2, ••• , sn: 
A routine g with (different) subexpressions 0'; [a J of respective modes J.li (i = 1, ... , n) 
which are all contained in an enclosing expression ':1 (of mode p) can be transformed thus: 
funct g = (l..a) p: ':1 [a, 0'1 [aJ, oo., 0'n(a]] 47 
funct g = (I.. a) p: ':§ (a, s1 (a), 00., sn(a)J, 
functs1 =(l..a)llJ.: 0'1[aJ, 
functsn = (l..a) 11n: 0'n [a] 
Note that the transformation is valid in both directions. The equivalence is based on the 
principle of substitution. 
Exercise I: Give the transformation rule which corresponds to the extraction of simi I a r subexpres-
sions. 
1.13.1.2 Apart from this obvious possibility there is another way of structuring, namely: 
embedding. Here a routine is derived not from the subexpression Iff; itself but from the ex-
pression ':§ which comprises occurrences of these identical subexpressions. 
The extraction of (a + b + c)/2 from Heron's formula can be achieved not only by the 
systems (heron, s) or (heron, t) but also by the system 
funct heron "' (rat a, rat b, rat c) rat: 
he(a, b, c, (a + b + c)/2}, 
funct he 
= (rat u, rat v, rat w, rats) rat: 
s x (s -
u) x (s -
v) x (s -
w) 
In this special case where the enclosing expression is the entire body of the original routine 
we speak of complete embedding, otherwise of incomplete embedding. 
The extraction of common subexpressions from a routine g by means of embedding into 
a routine f can more generally be described in the following way: a routine g with different 
(common) subexpressions rff; (a] of the modes J.li (i = 1, ... , n) which are all contained in 
the enclosing expression ff of mode 11 can be transformed under certain conditions as fol-
lows: 
47 By using special parentheses e.g. in rff; (x J, xis distinguished as a subexpression of rff;; rff; (b J then 
arises by replacing all instances of x by b. 

1.13 Structuring of Routines 
functg "'(J..a) p: '(§(a, sr(a, 0'1 (aJ, ... , rffn (aJJJ 
funct g "' (A. a) p: ':§ (a,J(a, 0'1 (a J, ... , rffn (a J )] , 
functj "' (I.. a, 111 s1, ... , JlnSn) 11: ff (a, S1, •.. , snJ 
91 
The new routine f, therefore, has an additional parameter for each of the subexpressions 
to be treated. The body of fis obtained by replacing every occurrence of an rff; (a J in the 
enclosing expression ff by the corresponding parameter S;. In this general form the rule 
describes incomplete embedding; without the enclosing expression ':§ we have the case of 
complete embedding. 
Substitution of the subexpression ff in the body of g by a call of the routine f formally 
represents a "folding". Since termination may be lost -
as has been noted at the end of 
1. 7.1 - this transformation is only valid if termination of g implies termination of the sys-
tem (g,J). It is sufficient (but not necessary) for this that g is not recursive, i.e. is not called 
within any of the subexpressions rff; (aJ. Moreover, it is necessary to ensure that all calls of 
fare defined. The transformation is valid in the opposite direction if each of the rff; (a J is 
determinate or occurs only once. 
The term "embedding" - at least in the case of complete embedding - is justified as an applica-
tion of the usual mathematical terminology: The two routines above are mappings g: '!i -+ 91 andj: 
'!i x C -+ 91 and a set C0 ~ C exists such that for the restriction of jon '!i x C0 we havej I p xso =g. 
As far as the notation is concerned the previous method of forming an auxiliary routine 
from every common subexpression could be considered clearer and more direct. However, 
according to the principle of substitution which serves as a basis, every auxiliary routine is 
called as often as the corresponding common subexpression occurs. In general, this leads 
to inefficiency in comparison to embedding, because (by means of the single call of the 
auxiliary routine) the common subexpression is only computed once, according to the left-
most-innermost computation rule. 
In the example 
funct p "' (rat z) rat: 
(z + 1/z) i 3 + (z + 1/z) i 2 -
2 x (z + 1/z) -
1 
an embedding 
funct p "' (rat z) rat: q(z, z + 1/z), 
funct q "' (rat z, rat t) rat: t i 3 + t i 2 -
2 x t - 1 
is obviously preferable to a structuring by means of abstraction 
funct p "' (rat z) rat: t(z) i 3 + t(z) i 2 -
2 x t(z) -
1 
funct t "' (rat w) rat: w + 1/w 
Here the result of embedding can be further simplified: the parameter z no longer occurs 
outside of the common subexpression and consequently can be omitted in the routine q. 
Note that in this example the same effect could have been achieved with the following 
embedding: 
funct p "' (rat z) rat: q(z + 1/z) -
1, 
funct q "' (rat t) rat: t i 3 + t i 2 - 2 x t 

92 
1. Routines 
An embedding, therefore, is not uniquely determined. 
Extraction of common subexpressions by means of embedding is especially indicated 
when they contain calls of complicated routines. 
Thus the computation of the expression 
p(x) x lnp(x) + (1 - p(x)) x /n(1 - p(x)) 
in which p (as a primitive routine) is a complicated computation is abbreviated, by embed-
ding, to the following (in which a parameter can again be saved in g) 
funct h = (real x) real: g(p(x)), 
funct g = (real s) real: s x /n(s) + (1 -
s) x /n(1 -
s) 
or further by embedding to 
funct h = (real x) real: g(p(x)), 
funct g "" (real s) real: f(s, 1 - s), 
funct f "" (real s1, real s2) real: s1 x /n(s1) + s2 x /n(s2) 
On the other hand by abstraction instead of embedding we obtain 
funct h "" (real x) real: g(p(x)), 
funct g "" (real s) real: j(s) + f(1 - s), 
funct f "" (real t) real: t x ln(t) 
Embedding and abstraction can obviously be applied together. 
1.13.1.3 In recursive situations the extraction of common subexpressions can be of special 
importance. As an example we consider the following routine for ancient Egyptian multi-
plication (nowadays in some places called 'peasant multiplication'), a "fast" variant of 
regular multiplication: 
funct bmult "" (int a, nat n) int: 
if n = 0 then 0 
else if even n then bmult(a, n div 2) + bmult(a, n div 2) 
else bmult(a, n div 2) + bmult(a, n div 2) + a fi fi 
Because of the cascade-type expansion of the calls this version is hardly more efficient 
than the usual multiplication. We can change this if we structure by incomplete embed-
ding, and with the aid of the routine 
funct dup/ "" (int t) int: t + t 
extract the two subexpressions which contain the calls 
funct bmu/t "" (int a, nat n) int: 
if n = 0 then 0 
else if even n then dupl(bmult(a, n div 2)) 
else dupl(bmult(a, n div 2)) + a fi fi 

1.13 Structuring of Routines 
93 
A complete embedding would not lead to the desired goal here, we would obtain 
instead 
funct bmult = (int a, nat n) int: f(a, n, dupl(bmult(a, n div 2))) 
funct f = (int a, nat n, int s) int: 
if n = 0 then 0 
else if even n then s 
else s + a fi fi 
However, this system no longer terminates because the recursive call has been extracted 
from the branch which causes termination. 
The most comprehensive embedding which still guarantees termination is obviously 
funct bmult = (int a, nat n) int: 
if n = 0 then 0 
else f'(a, n, dupl(bmult(a, n div 2))) fi, 
funct f' = (int a, nat n, int s) int: 
if even n then s 
else s + a fi 
By (carelessly) extracting subexpressions from a branching the definedness can also be 
lost (even in non-recursive situations). This is shown by the following variant of the above 
example: 
funct bmult = (int a, nat n) int: 
if n = OthenO 
else if even n then bmult(a, n/2) + bmult(a, n/2) 
else bmult(a, (n - 1)/2) + bmult(a, (n - 1)/2) 
+ a 
fi fi . 
The incomplete embedding, with the help of dupl, works as before. However, in the 
(still terminating) more comprehensice embedding 
funct bmult = (int a, nat n) int: 
if n = 0 then 0 
else f'(a, n, dupl(bmult(a, n/2)), dupl(bmult(a, (n - 1)/2))) fi, 
functj' = (int a, nat n, int s1, int s2) int: 
if even n then s1 
else s2 + a fi 
the call off' is always undefined, because either n/2 or (n -
1)/2 is undefined (comp. 
1.3.1). 
Exercise 2: Obtain a fast variant by means of halving for gcd (1.7.1, (*))and for mod (1.4.1, (b)). 

94 
1. Routines 
1.13.2 Segments and Suppressed Parameters 
"In ordinary mathematical communication, these uses 
of where require no explanation." 
Landin 1966 
1.13.2.1 In applying the methods of structuring of the last paragraph (sub-) routines such 
as t, he or dup/ are developed which are hierarchically subordinate (1.4.3) to the respective 
original routines. Hierarchical subordination is also demonstrated by other previous 
examples. This subordination is given implicitly by the principle of substitution. 
The subordination of a routine or a system of routines can be made explicit by marking 
the area of the subordination - called a segment - by suitable brackets. Such segment 
brackets are e.g. begin. end or the comer brackets r. J ' they can be dispensed with if the 
segment is already marked by bracket symbols such as if.then.else. fi or if. then. D. then. 
. . . fi. The subordinate routine is listed inside the segment. Such a segment acts as a 
(generalized) expression. 
For the system (heron, t) we obtain e.g. 
funct heron = (rat a, rat b, rat c) rat: 
f t(a,b,c,O) X t(a,b,c,a) X t(a,b,c,b) X t(a,b,c,c) 
where funct t = (rat a, rat b, rat c, rat x) rat: 
(a + b + c)/2 - x 
J 
or alternatively 
funct heron = (rat a, rat b, rat c) rat: 
r funct t = (rat a, rat b, rat c, rat x) rat: 
(a + b + c)/2 -
x 
within t(a, b, c, 0) x t(a, b, c, a) x t(a, b, c, b) x t(a, b, c, c) J 
As the course of the specified computation within the segment brackets is already 
determined by the principle of substitution (in the sense of functional application) an ex-
plicit notational indication by the use of where or within is not necessary. Therefore we 
will in the sequel occasionally use the comma as a separator instead of the symbols where 
and within. A notational indication for the subordination, however, is essential for the 
suppression of parameters which we shall now discuss. 
1.13.2.2 In the routine heron the parameters a, b, c in the individual calls of tare not 
changed. Such examples suggest suppressing the specification of those parameters which 
are not changed in any of the calls. This idea arose in connection with ALGOL between 
1958 and 1960. Such a suppression of parameters is of course only possible if the routine 
with the parameters to be suppressed is explicitly subordinated to another routine, such 
that the suppressed parameters appear 48 again as parameters of the enclosing routine. Sup-
pressed parameters are often called implicit parameters; in their context they become rela-
tive constants. 
In our examples we can write the following 
48 Above all they must have the same designation. This can always be achieved by simple re-naming. 

1.13 Structuring of Routines 
or 
funct heron = (rat a, rat b, rat c) rat: 
f t(O) X t(a) X t(b) X t(c)) where 
funct t = (rat x) rat: (a + b + c)/2 - x J 
funct p = (rat z) rat: 
f t j 3 + t j 2 - 2 X t -
1 where 
funct t = rat: z + 1/z 
J 
Likewise we have (comp. 1.3.1) 
funct heron = (rat a, rat b, rat c) rat: 
f S X (s -
a) X (s -
b) X (s -
c) where 
funct s = rat: (a + b + c)/2 
J 
but also 
funct heron = (rat a, rat b, rat c) rat: 
r he((a + b + c)/2) where 
funct he = (rats) rat: s x (s -
a) x (s -
b) x (s -
c) J 
95 
A necessary and sufficient condition for the possibility of suppression is that the para-
meter in question be "fixed": A parameter X; in the i-th position of a routine f (of a system 
S) is called fixed if a calif( ... , X;, ••• ) leads to no other calls of /(within the systemS) 
than calls again of the form/( ... , X;, ••• ). Thus, all parameters of non-recursive routines 
are fixed. 
If now the calif( ... , a, ... ) occurs, a copy off( of S) is subordinated which is obtained 
as follows: In f (in S), X; is replaced throughout by a and then a is suppressed as a para-
meter. 
This process of parameter suppression is normally done subconsciously by a 
programmer who is familiar with mathematical notation. 
By suppressing the fixed parameter b in 
funct mod = (nat a, nat b: b * 0) nat: 
if a < b then a 
else mod(a -
b, b) fi 
the following results from the system (b') of Table 1 (p. 452): 
funct gcd = (nat a, nat b) nat: 
r if b = 0 then a 
else gcd(b, mod(a)) fi where 
funct mod = (nat a) nat: 
If a < b then a 
else mod(a -
b) fi 
J 
Note that the parameter x of the routine hyp in 1. 7.2 is a fixed parameter, its instantia-
tion there which leads to ackh does not lead to a simplification of the recursion. 

96 
1. Routines 
Suppressed parameters of a routine are also called non-local, listed parameters are 
called local. The range of binding of a parameter designation is the body of that routine in 
which the parameter is listed. Parameters which are not listed at all are global - we must 
presume that they are declared "further outside". 
1.13.2.3 We must now distinguish between the range of binding of a parameter designation 
and its scope. If we continue to choose the parameter designation freely, then the same 
designation can appear for both a parameter designation in one routine and a parameter 
designation in one of its subordinate routines. Within the latter the "own" designation has 
preference and overrules the designation introduced further outside whose scope is smaller 
than its binding - it is reduced by the binding of the subordinate routine. One also speaks 
of a "hole". 
The operational semantics of subordinate routines can be defined by cancelling the suppression 
(while explicitly reintroducing the suppressed parameters) and going back to the text substitution 
machine, or to the stack machine. 
For the ALGOL machine, it is only consistent to use a more efficient operational semantics: look-
ing at suppressed parameters locally as constants, a routine declaration can be interpreted as an 
instantiation of its non-local parameters (partial computation, cf.1.7.2). This is to be done, however, 
in "dynamic" execution; in the example above it means the definition of a new routine mod for every 
incarnation of gcd. 
If non-local parameters of subordinate routines are in principle considered as relative constants -
as happens during the process of program development -
we obtain a rule for their scope which is 
known in compiler construction as "static scoping". 
In contrast, the "dynamic scoping" described by Dijkstra in 1960 brings in certain cases imple-
mentation advantages but also confusing complications for understanding execution. In the original 
form of LISP ("pure LISP"), dynamic scoping was assumed. Since, however, dynamic scoping gives 
difficulties to a mathematical semantics (Gordon 1975, Steele, Sussmann 1978), even LISP users 
more recently show a tendency to accept static scoping (Steele 1977). Furthermore, static scoping is 
theoretically more powerful than dynamic scoping (Langmaack, Olderog 1980). 
1.13.2.4 Suppression of parameters is of particular importance in connection with 
structuring by abstraction (1.13.1). It allows transition from an expression to a segment 
with a system of subordinate routines all parameters of which are suppressed (depara-
meterized routines): 
funct g = (J.a) p: r <'# (a,s1, ... , snJ where 
funct s1 = 1'1: e1 [a J , 
funct sn = 11n= en [a J J 
is shorthand for the system 
funct g = (A. a) p: 
<'# (a, s1 (a), ... , sn(a)J, 
funct s1 = (A. a) J1!: e1 [a J, 
funct Sn = (/.a) 11n: en (a J 
where, of course, none of the s; occurs outside of this system. 
The segment above is a structuring of the expression 
<'#Ca. e1 CaJ, ... , e"CaJJ 

1.13 Structuring of Routines 
97 
where, of course, nos; occurs in any of the Iff;. Under this condition, equivalence holds. 
In concluding, we again consider abstraction and embedding for the (simple) example 
of an expression 
J'i" [ ~ [ ff [e J J J 
Successive abstraction means to unroll the formula, 
I h where 
funct h = J.l3: J'i" [gJ, 
funct g = 
~-~z: ~ [jJ, 
functj = J.l!: .r[eJ J 
while successive embedding means to build it up 
I functj = (floa) J.l!: ff[aJ, 
functg =(J.l!b)~-~z: ~[bJ, 
funct h =(l-Iz c) 113: J'i" [cJ 
within h(g(f(e))) 
J 
1.13.3 Object Declarations 
1.13.3.1 Although the routines that result from structuring by means of abstraction can be 
written more clearly by suppressing parameters, the inefficiency in comparison to embed-
ding (mentioned in 1.13.1) still remains because the (sub-)routine is still being called 
repeatedly. 
In order to avoid writing the seemingly indirect embedding version as in our example 
funct p = (rat z) rat: I q(z + 1/z) where 
funct q = (rat t) rat: t i 3 + t i 2 - 2 x t - 1 J 
a notation with the same meaning is introduced which is formally modelled on the abstrac-
tion 
funct p = (rat z) rat: 
I t i 3 + t i 2 - 2 x t - 1 where 
funct t = rat: z + 1/z 
J 
namely 
funct p = (rat z) rat: 
I rat t = z + 1/z within 
ti3+ti2-2xt-1J 

98 
1. Routines 
Similarly, instead of (comp. 1.13.2.2) 
funct heron = (rat a, rat b, rat c) rat: 
I he((a + b + c)/2) where 
funct he = (rats) rat: s x (s -
a) x (s -
b) x (s -
c) J 
we write "more simply", i.e. without introducing visibly an auxiliary routine, 
funct heron = (rat a, rat b, rat c) rat: 
I rats = (a + b + c)/2 within s x (s - a) x (s - b) x (s - c) J 
We call 
rat t = z + 1/z or rats = (a + b + c)/2 
an object declaration, the introduced designation an (intermediate) result designation or 
also a local auxiliary designation and the construction in square brackets a segment, which 
acts as a (generalized) expression. 
The same applies to a collective object declaration 
introducing a set s1 , s2 , ••• , sn of designations, see below. 
Here we recognize an essential construct of present-day programming as a notational 
variant of a strictly applicative formulation (we will find a similar situation for variables 
and jumps). 
The situation was reversed in ALGOL 68; the object declaration there is fundamental and is used 
for defining the meaning of a procedure call. 
By introducing object declarations the following results as an "abbreviated" version of 
the "ancient Egyptian multiplication" of 1.13.1.1: 
funct bmult = (int a, nat n) int: 
if n = 0 then 0 
else if even n then int m = bmult(a, n/2) within 
m+m 
else int m = bmult(a, (n - 1)/2) within 
m + m + a 
fi fi 
The definition of a (collective) object declaration in this book is done generally through 
the introduction of a shorthand notation: 
funct g = (A.a)p: 
I(JitS1, ... , J.lnSn) = (@"1 [aJ, ... , tB"n[aJ) within 
'1/ [a, s1, ... , snJ 
J 
funct g = (A. a) p: I j(t'o"1 [aJ, ... , @"n [a]) where 
functj = (JitSt, ... , f.lnSn) p: '1/ [a, St, ... , snJ j 

1.13 Structuring of Routines 
99 
whereby j, of course, occurs neither in ~nor in one of the cf!i, and also not outside the 
system. 
Another way of expressing this is as follows (Landin 1964): The segment 
stands for the call 
{(JltS10 ... , ~~nsn) p: ~ [a,s1, ... , s.]) (cf!1 [a], ... , cf!n [a]) 
Accordingly, range of binding and scope of the designation(s) introduced by an object 
declaration are those of the corresponding parameter(s}, i.e. the segment immediately 
embracing the object declaration - notwithstanding possible holes in the scope. 
Looking back to 1.13.1, we note that the segment above - the body of g - with the 
collective object declaration is a structuring of the expression 
by embedding. 
1.13.3.2 The fact that embedding results in a one-time computation of common subexpres-
sions (1.13.1) has the consequence that for an object declaration, evaluation also occurs 
just once. This justifies indeed the wording "object declaration". Thus, replacement of 
declarations for deparametrized functions (comp. 1.13.2.4) by object declarations (under 
the conditions stated in 1.13.1.2) saves calls: 
I ~ [s1, ... , s.J where 
funct s1 = f.1t: cf!1 [a], 
funct s. = 11n: cf!. [a J J 
I (Jlt S1, ... , ll.s.) = ( 
cf!1 [a J, ... , cf!. [a J) within ~ [s1, ... , snJ J 
Note, however, that the converse does not hold without further conditions which have 
been stated above. 
By successive embedding we obtain hierarchically ordered object declarations (which 
are to be distinguished from collective object declarations), for example (cf. 1.13.2.4) 
I Jlo a = e within 
lf.ltb = .r[aJ within 
I f.12C = ~ [b] within 
.1f [cJ 
J J J 
In this hierarchical ordering, segment brackets are piling up at the end; this suggests a 
simplified notation with the right-associative 49 sequencing symbol ";" replacing the 
symbol "within": 
49 Note the parallel to the introduction of the sequential branching in 1.3.3. 

100 
1. Routines 
i~~oa=e; f.ltb=.r[aJ; llzC='#[bJ; £[cJJ 
We shall use this notation first in Chap. 4, the semicolon as a sequencing symbol will 
play a particular role in Chap. 5 in connection with the introduction of variables. 
1.13.3.3 As a notational device, object declarations are especially useful in the case of rou-
tines with multiple results, if the results are used individually for further computation. In 
the example natdiv of 1.12 it is no longer necessary to extend the addition to pairs of 
numbers. We obtain 
funct natdiv = (nat a, nat b: b * 0) (nat, nat): 
if a ~ b then (nat q, nat r) = natdiv(a -
b, b) within 
(q + 1, r) 
0 a < b then (0, a) 
fi 
This, however, is only a transcription of 
funct natdiv = (nat a, nat b: b * 0) (nat, nat): 
If a ~ b thenj(natdiv(a -
b, b)) where 
funct f = (nat q, nat r) (nat, nat): (q + 1, r) 
0 a < b then (0, a) 
fi 
The problem of sorting an element into a linearly ordered sequence 
funct insort = (sequ 11 a, 11 x: issorted(a)) sequ 11: 
11 sequ l.l b: issorted(b) A 3 sequ l.l u, sequ 11 v: 
a = u & v A b = u & append(v, x) 
using the non-deterministic operation parse of 1.12 is obviously solved by 
funct insort = (sequ 11a, 11x: issorted(a)) sequ 11: 
if a = 0 then append(a, x) 
0 a * 0 then (sequ J1U, J1f, sequ J1V) = parse(a) within 
if x ~ t then insort(u, x) & append(v, t) 
0 x ~ t then u & append(insort(v,x), t) fi fi 
Termination is guaranteed by the fact that the length of u and the length of v is always 
smaller than the length of a. 
Different methods of sorting develop through different kinds of deterministic descend-
ants of parse. 
The additional condition 
0 ~ length(u) -
length(v) ~ 1 
in the specification of parse results in a "balanced partitioning" and yields the algorithm of 
"binary sorting"; termination occurs after n steps if 2"- 1 ~ /ength(a) < 2". 

1.13 Structuring of Routines 
101 
The additional condition 
u = 0 
means that parse is implemented by the triple ( 0, top, rest); the algorithm simplifies to 
"linear sorting": 
funct insort = (sequ JW, JlX: issorted(a)) sequ 11: 
if a= 0 
then append (a, x) 
else if x ~ top(a) then append(a, x) 
D x ~ top(a) then append(insort(rest(a), x), top(a)) fi fi 
Exercise 1: Give a (recursive) definition of the predicate issorted such that the solution above can be 
formally derived from the specification. 
Finally let us treat an example which originates from Gries and Griffiths: 
Letjbe a monotonic increasing integer function on the interval [1 .. n]. Find a routine 
which establishes which function value occurs most frequently (i.e. whose frequency is not 
exceeded by any other function value) and how often this value appears. 
For n = 1 we obtain ({(1), 1). For an arbitrary n we comparef(n) withf(n - numb), 
where numb is the established maximum frequency for the first n - 1 components: if both 
function values are the same, then all the values in between are also the same because of 
monotonicity. Thereforef(n) appears numb + 1 times and is the most frequent function 
value. Otherwise the most frequent function value so far remains dominant. 
Therefore we have (fis suppressed as a parameter) 
funct grigri = (nat n: n =F 0) (int, nat): 
if n = 1 then ({(1), 1) 
D n =F 1 then (int dom, nat numb) = grigri(n -
1) within 
if f(n) = f(n - numb) then (f(n), numb + 1) 
D f(n) > f(n - numb) then (dom, numb) 
fi fi 
Exercise 2: Does grigri also terminate if the condition of monotonicity is not satisfied? What effect 
has the routine then? 
1.13.4 Result Parameters and the Actualization Taboo 
1.13.4.1 A routine call simply delivers a result. An object declaration introduces a designa-
tion for an intermediate result. It may be desirable to export an (intermediate or final) 
result under an exchangeable designation, in particular for routines with a multiple result. 
A result parameter must be introduced for this purpose. 

102 
1. Routines 
In the declaration of a routine with result parameter(s) we choose a notation which is 
usual in mathematics and in which a mapping arrow 5° is written between the object para-
meter and the result parameter, such as (comp. 1.13.1) 
functjou* "' (rat z) ~-+ (rat w): w = z + 1/z 
or in a routine for determining the maximum and minimum of two numbers 
funct ord* "' (nat a, nat b) ~-+ (nat x, nat y): 
(x,y) = if a ;;;; b then (a, b) 
D a ~ b then (b,a) fl 
The concept "result parameter" seems to be paradoxical because such a parameter is al-
ready actualized in the call, although it refers to results. Certainly, a result is not passed 
into the routine, but a reference as to where the result should be delivered is given 
(Andrei's paradox, see Fig. 1.9). 
Andrei hands 
Vladimir 
an address 
to which he 
is to deliver 
a case 
which has 
been left 
at the station. 
Fig. 1.9. A seeming paradox 
50 A similar notation was intended for ALGOL 58. Rutishauser, Samelson and others advocated 
such a distinction between ordinary parameters ("input parameters") and result parameters in 
ALGOL 60, but without success. Wirth distinguished notationally between input and result para-
meters at least in ALGOL W ("call by value result"), a similar course was taken by BCPL and an 
illegal FORTRAN variant. 

1.13 Structuring of Routines 
103 
A call now shows the actual filling of the parameters: For an ordinary parameter 
(which is now for the sake of distinction called argument parameter) it is the argument, for 
a result parameter it is the actual result designation. (The calls of the routines in question 
do not deliver any other result and thus cannot occur in expressions.) 
We could write 
jou*(3) 1-+ (s) 
or 
ord*(3, 5) 1-+ (u, v) 
if the actual result designations s or u, v are again result parameters themselves. If, how-
ever, the result parameter is not simply passed through in this way, then it must have a 
declaration. It is characteristic of result parameters that this declaration takes place with 
the call -
since all the results are used afterwards. This declaration can be expressed by 
writing for the actual result the same as is written at the left side of the object declaration 
- thus in our example 
jou*(3) 1-+ (rats) 
or 
ord*(3, 5) 1-+ (nat u, nat v) 
Such a call then amounts to an object declaration such as 
rats = ((rat z) rat: z + 1/z) (3) 
that is 
or 
rats = 10/3 
(nat u, nat v) .. ((nat a, nat b)(nat, nat): 
if a~ b then (a, b) 
a ~ b then (b, a) fl) (3, 5) 
that is 
(nat u, nat v) = (5, 3) 
This object declaration results from formally substituting the actual result designations 
and the actual parameter values. 
If the generalized expression 
II jou*(3) 1-+ (rats) within 
s i 3 + s i 2 - 2 x s -
1J where 
funct jou* = (rat z) 1-+ (rat w): w = z + 1/z J 

104 
is to be interpreted, parameter replacement yields 
I rats = 10/3 within 
si3+st2-2xs-1J 
1. Routines 
1.13.4.2 A system of routines is meaningless unless all the routines have different designa-
tions. Accordingly all object declarations within a certain segment must introduce objects 
with different designations. This requirement is so self-evident that its omission up to now 
could hardly have been noticed. 51 
For the call of a routine with result parameters a drastic restriction emerges - whose 
importance has been under-estimated for a long time -, that is the actualization taboo: no 
two actual designations for result parameters should be the same in one call. 
It is obvious that a call such as 
ord*(3, 5) 1-+ (a, a) 
is meaningless. 
It should be considered more an advantage than a disadvantage that this strict taboo 
excludes cases such as 
ord*(3, 3) 1-+ (a, a) 
with which one might possibly tinker. 
The use of result parameters is defined entirely on the applicative level, as illustrated in the 
examples, i.e. it does not require the concept of program variables, which is to be introduced in 
Chap. 5. On the other hand variables used as parameters have no pure result character. they are, by 
nature, transient parameter (camp. 5.3.1). 
Result parameters can often be saved: an object declaration will suffice, and instead of 
ord*(3, 5) >-> (nat u, nat v) within u -
v 
one can immediately write what this means, 
(nat u, nat v) =- ord (3, 5) within u -
v 
with the normal routine ord from 1.12. (Compare also the examples grigri and insort.) Nevertheless 
there is the possibility of passing through result parameters, which provides a notational advantage. 
1.14 Routines as Parameters and Results 
If one wishes in the example grigri to change the function f from call to call, it has to be 
made a parameter. Thus, the routine grigri obtains the new heading 
funct grigri = (funct ({nat x: x * 0}) int J, nat n: n * 0) (int, nat) 
51 In cases where we have used the same designation for several routines it has always referred to 
equivalent versions of one and the same routine. 

1.14 Routines as Parameters and Results 
105 
Here, 
funct ({nat x: x =1= 0}) int 
is the functionality of the parameter f 
For a routine that acts as a parameter, its functionality serves to indicate its mode; the 
totality of routines of a given mapping type is itself a mode. Thus grigri defines a 
functional of mode 
funct (funct ({nat x: x =1= 0}) int, {nat n: n =1= 0}) (int, nat) 
As a further example we give the recursive routine (comp. 1.9) 
funct pow ... (nat a, nat n, funct (nat, nat) nat F, nat e) nat: 
if n =1= 0 then F(a, pow(a, pred n, F, e)) 
else e 
fi 
with the functionality 
funct (nat, nat, funct (nat, nat) nat, nat) nat 
Let us consider the instantiation (for add comp. exercise 1.4.4-2) 
pow(a, n, add, 0) 
in the body of the routine 
funct mult ... (nat a, nat n) nat: pow(a, n, add, 0) 
Unfolding leads to 
funct mult ... (nat a, nat n) nat: 
if n =1= 0 then add(a, pow(a, pred n, add, 0)) 
else o 
fi 
Folding yields the recursive definition 
funct mult ... (nat a, nat n) nat: 
if n =1= 0 then add(a, mult(a, pred n)) 
else o 
fi 
that is the definition of multiplication, comp. exercise 1.4.4-2. 
Likewise pow(a, n, mull, 1), that is 
pow(a, n, (nat a', nat n') nat: pow(a~ n', add, 0), 1) 
yields the definition of the power operation. We have already met such a stepwise con-
struction in 1. 7.2 in connection with the Ackermann function. In that function it took 

106 
1. Routines 
place with respect to the formal parameter i of the step enumeration. It is obvious that the 
use of routines as parameters offers more freedom. 
If we consider that objects can be obtained by nullary routines (comp. 1.3.1) the exten-
sion to arbitrary routines as parameters seems natural. 
The mechanism for passing parameters has, of course, to be defined. In the case of text substitu-
tion machines the method - text substitution - is intuitively clear. It carries over to systems of rou-
tines. 
If we proceed in the same way with the stack machine, we have to permit the retention of routine 
designations in the value stack and their use in calls in the same way as for other parameters. We can-
not go into details here as to how this rather complicated task can be immediately mechanized (Hewitt 
1977). How to deal with routines as parameters can be found in early descriptions of ALGOL trans-
lators and LISP interpreters; static vs. dynamic scoping leads, however, to different mechanisms. 
Later modifications made in transition from "pure LISP" to LISP 1.5, e.g. the FUNARO trick, 
suggest to extend the ALGOL mechanism to LISP, too (Simon 1978). 
1.14.1 Routines as Results 
The examples above suggest that routines be also permitted as results of routines, in par-
ticular if by instantiation of k parameters of an n-ary function a (n - k)-ary function is to 
be defined. This motivates the introduction of multi-stepped parameterization (Schon-
finkel1924, Curry, Feys 1958, Turner 1979), allowing construction of a new routine by 
instantiating some of the parameters. Groups of parameters which are to be instantiated 
together are - arranged from left to right - separated from each other by a dot. 
Thus, for example, we define the routine 
funct pow' = (funct (nat, nat) nat F, nat e). (nat a, nat n) nat: 
if n * 0 then F(a, pow'(F, e) (a, pred n)) 
else e 
fi 
with the functionality 
funct (funct (nat, nat) nat, nat) funct (nat, nat) nat 
and obtain by the call pow'(add, 0) the routine for multiplication and with the call 
pow'(pow'(add, 0), succ(O)) the routine for powering. 
A complete call could, for example, be 
pow'(add, 0) (3, 5) 
Note that a routine which delivers a routine can always be considered as a hi-stepped 
function and can be formulated accordingly: a mapping of A into B -+ C - where B -+ C 
is the set of mappings of B into C -
can be viewed as a mapping of A x B into C: 
A -+ (B -+ C) ~ A x B -+ C 
(see Scott 1981). 
By repeated application of this rewriting rule, the most general case of nested mappings 
can be reduced notationally to routines which deliver only objects as results. No more 
general forms of routines are needed, provided multi-stepped parametrization is allowed. 

1.14 Routines as Parameters and Results 
107 
Routines which deliver routines have been dealt with in the Lambda-calculus of Church 1941, 
which is universal in the sense of Church's thesis. Very few programming languages allow the full 
generality of the Lambda-calculus. ALGOL 60, for example, allows only routines which deliver 
objects, and only such routines may turn up as parameters ("ALGOL 60-type" routines). In the light 
of the consideration above, this restriction does not limit universality (Langmaack 1974). ALGOL 68 
allows routines as results in principle, but imposes too narrow restrictions on their use. LISP/N, an 
adaptation of LISP (Simon 1978, Lippe, Simon 1980) comprises the full Lambda-calculus including 
the Lambda-calculus schemata introduced by Fischer 1972. 
Wirth's standard compiler for PASCAL is even narrower than ALGOL 60, by restricting the ar-
guments of formal parameters which are routines to objects ("depth two restriction"). Fischer 1972 
has shown, that under this restriction a stack machine working with a stack of stacks can perform the 
task of the text substitution machine. 
Exercise 1: Reduce a mapping of functions into functions to a (hi-stepped) routine. 
Exercise 2: Reduce mappings of the forms (A -+ B) -+ ((C -+ D) .... E) and (A -+ B) -+ 
(C -+ (D -+ E)) to multistepped routines. 
Untyped notation -
as is customary for the Lambda-calculus -
also permits self-
application (McCarthy 1961, Landin 1966, Ledgard 1971). 
Let 
funct self '"' ( A n, A F) A: if n = 0 then 1 
else n x F(n - 1, F) fi 
Unfolding of the call self(n, self) in the routine 
funct fa "" (A n) A: self(n, self) 
leads to 
functfa '"'(An) A: if n = 0 then 1 
else n x self(n -
1, self) fl 
folding yields 
functfa ""(An) A: if n = 0 then 1 
else n x fa(n -
1) fi 
Self-application of the non-recursive routine self, therefore, yields a recursive routine, namely fac 
(comp. 1.4.1 (a)). 
Due to practical considerations which are also shared by theorists from a philosophical point of 
view, 52 only typed routines are permitted in ALGOL 68. 
If one wants to make do with finite type specification, one must forego true self-application. It is 
questionable if self-application is of practical value, although it is more powerful under theoretical 
aspects (Damm, Fehr 1978). 
1.14.2 Functional Programming 
Problems, in which routines are objects, lead in a natural way to routines which have rou-
tines as parameters and as results. Common examples are convolution of functions and 
Fourier transformation. In its generality this field of "formula and function manipula-
52 "Our way of thinking (about routines) is typed" (K. Indermark). Self-application may also lead to 
paradoxical routines (Tennent 1976). 

108 
1. Routines 
tion" is still difficult to implement if loss of efficiency at runtime is to be avoided; as a rule 
individual program development is required. 
An important fundamental task is the composition of compatible functions, 
funct Omikron = (funct (Jl) v a, funct (v) p b). (J1X) p: b(a(x)) 
with the property 
Omikron(u, v)(x) = v(u(x)) 
Another task is the extension of operations on the result type to operations on the func-
tion. These are called "induced operations". An example would be 
funct addi = (funct (Jl) nat u, funct (Jl) nat v). (J1X) nat: u(x) + v(x) 
This task is especially important when predicates i.e. Boolean routines are to be composed. 
As a rule "geometrical" objects can be understood as sets. When manipulating such 
objects M it is appropriate instead to manipulate their characteristic predicates 
("geometric locus") 
funct m = (J1X) bool: x eM 
This leads to applications e.g. in graphical output systems 53 and, more generally, in opera-
tions with sets (see 3.4.5.2). 
The characteristic predicate for the intersection of two sets represented by their charac-
teristic predicates can be obtained e.g. by 
funct meet = (funct (Jl) bool u, funct (Jl) bool v). (J1X) bool: u(x) " v(x) 
In addi and meet we have the special cases Beta(+) and Beta(A) of a functional form Beta 
for induced binary operators, 
funct Beta = (funct (v, v) Krho). 
with the property 
(funct (Jl) v u, funct (Jl) v v). 
(JlX) K: rho(u(x), v(x)) 
(Beta(rho) (u, v)) (x) = rho(u(x), v(x)) 
Working with functional forms such as Omikron or Beta is functional programming 
(Backus) par excellence. It can also be applied in ordinary programming if the function to 
be described is constructed by means of some standard functors from primitive functions. 
The first approaches in this direction can be found in APL, more complete ones in Backus 
1973, 1978a and Turner 1979. 
53 For mechanical devices it is frequently more useful to represent them by coordinates as parameters 
than by characteristic predicates. 

1.14 Routines as Parameters and Results 
109 
An advantage of this style of programming is that no identifiers for arguments need be 
introduced or used: To give a trivial example, 
functj"' (real x) real: sq(sin(ha/f(x))) 
can be obtained by functional programming as 
functj"' Omikron(Omikron(half, sin), sq) 
or more briefly in infix notation 54 
functj"' sq o (sin o half) 
For functional programming it is also important to have available operations with 
arbitrary arity (e.g. n-fold sum, n-fold product). A possibility to achieve this is the use of 
sequences as parameters. 
1.14.3 The Delay Rule 
Finally we should mention that there is a distinction between an object and a nullary rou-
tine in the call: an expression which is in the position of an object parameter is evaluated 
according to the first theory and the leftmost-innermost-rule; an expression which stands 
in the position of a parameter for a nullary function is not evaluated. For determinate and 
defined expressions this means a difference solely in the evaluation strategy, but for non-
determinate expressions it presents a possible difference in the result: 
dup/(3 D-3) where funct dupl "' (int t) int: t + 
means (comp. 1.9) (6 D-6); 
dup/*(int: (3 D-3)) where funct dupl* "' (funct int f) int: f + f 
means (6 D 0 D-6); dup/(3 D-3) is a descendant of dup/*(int: 3 D-3). This situation, in the 
non-deterministic case, indicates a differentiation which does not appear in the usual 
Lambda-calculus. 
Instead of passing a routine as an object in the position of a functional parameter one 
frequently changes the computation rule, roughly speaking, to a local leftmost-outer-
most evaluation, i.e. the imported expression is evaluated when and only when it is en-
countered in the course of execution. This interpretation of an argument expression as a 
nullary function is known as "call by name" in the jargon of current programming 
languages, in contrast to the usual "call by value". 
But how should we parameterize a function 
if x = 0 then 1 else y + y fl 
54 In mathematics, also the converse order of operands is used. 

110 
1. Routines 
with respect toy in the most efficient way? In the case of 
(nat x, nat y) nat 
the actual expression for y is evaluated once. In the case of 
(nat x, funct nat y) nat 
it is not evaluated at all, if x = 0, and is evaluated twice if x * 0. 
It is most economical of all not to evaluate for x = 0, and to evaluate once for x * 0, 
i.e. to evaluate when and only when it is necessary and never again ("call by need", Wads-
worth 1971). In contrast with the leftmost-innermost rule, the value of the parameter is not 
requested at once, the whole expression is passed on, as in the leftmost-outermost rule; as 
soon as an occurrence of some F is to be substituted, however, all call of the same form are 
done in the same step. Such a strategic choice between leftmost-innermost and leftmost-
outermost rule leads to the delay rule (Vuillemin 1973): Substitute the leftmost F and all 
calls of the same form. 
Vuillemin describes a successive marking algorithm for determining all calls of the same 
form; in execution on ordinary machines one can use a pointer representation of the 
textual expressions 55• It is obvious that the delay rule is superior to both the leftmost-inner-
most and the leftmost-outermost rule with respect to the number of substitution steps. 
(For the mathematical semantics, we refer to the literature 56.) For example, ack(2,1) 
requires 14 steps (29 with the leftmost-outermost, 14 with the leftmost-innermost rule), 
ble(8, 2) needs 9 steps (9 with the leftmost-outermost, 341 with the leftmost-innermost 
rule). 
Exercise 1: Execute ack (2, 1), using the delay rule, the leftmost-innermost rule and the leftmost-
outermost rule. 
All these computation rules still have a disadvantage. To see this, we consider the fol-
lowing example: 
A routine natinterval delivers an interval of natural numbers in the form of an ascend-
ing sequence, natinterval(2, N) is subjected to a routine which sifts all non-primes. This 
gives the system 
funct natinterval = (nat i, nat k) sequ nat: 
if i > k then 0 else append(natinterval(i + 1, k), i) fi, 
funct multsieve = (sequ nat a, nat x) sequ nat: 
if a = 0 then a 
else if xI top(a) then multsieve(rest(a), x) 
else append(multsieve(rest(a), x), top(a)) fi fi, 
55 The delay rule would mean overall increased effort on the machines that prevail today, only the 
Burroughs B 5000 exhibits somewhat more friendly behaviour with the 'operand call syllable'. 
56 A relevant quotation: 'It is obvious that the semantics can be adjusted to fit any mechanical 
evaluation method one chooses' (Henderson, Morris 1976). 

1.14 Routines as Parameters and Results 
funct primesieve = (sequ nat a) sequ nat: 
if a = 0 then a 
else append(primesieve(multsieve(rest(a), top(a))), top(a)) fi 
and the call 
primesieve(natinterva/(2, N)) 
delivers the sequence of all primes up to N. 
111 
In this example, a text substitution machine working with any of the discussed com-
putation rules first forms the sequence of all numbers between 2 and N, and follows this 
with the sieve process. It would be more sensible, however, to proceed as follows (inter-
mediate steps are not recorded) in a demand-driven way: 
primesieve(natinterva/(2, N)) 
append(primesieve(multsieve(natinterva/(3, N), 2)), 2) 
append(append(primesieve(multsieve(multsieve(natinterva/(4, N), 2), 3)), 3), 2) 
append(append(primesieve(multsieve(multsieve(natinterva/(5, N), 2), 3)), 3), 2) 
append (append (append (primesieve ( 
multsieve(multsieve(multsieve(natinterva/(6, N), 2), 3), 5)), 5), 3), 2) 
No step is ever carried out until it is strictly necessary, the calls of rest in multsieve and 
primesieve drive the computation (by replacing rest(append(x, S)) by x and in the same 
step top(append(x, S)) by S). As soon as natinterval(k, N) becomes empty, all suspended 
calls of multsieve collapse, finally the suspended calls of append build up the resulting 
sequence ("lazy evaluation"). 
This shows that the call 
primesieve (nats (2)) 
where 
funct nats = (nat i) sequ nat: append(nats(i + 1), i) 
leads to the same series of execution steps as above, with the subtle difference that it does 
not terminate. Note that nats is a routine which does not terminate for any computation 
rule whatsoever; primesieve(nats(2)) would be caught according to any of these computa-
tion rules immediately in the non-ending buildup of all natural numbers. Following lazy 
evaluation, something is done which in the circumstances is more sensible: the production 
of the prime number sequence is indeed carried out step by step; the partial results can be 
taken from the intermediate texts. That this is advantageous is seen immediately if 
somebody wants, say, the fifth prime, writing 
top (rest (rest (rest (rest (primesieve ( nats (2))))) )) 

112 
1. Routines 
Computation with lazy evaluation - again for append - terminates, although the expres-
sion contains a call of a routine which is in itself non-terminating. 
This shows that there is no reason to refrain completely from using non-terminating 
routines (we have seen the same to hold with non-deterministic constructions in deter-
minate routines). Lazy evaluation will turn up again in 2.14.3 in connection with infinite 
objects. 
"The difference is that the bound is not explicit in the 
algorithm itself." 
Friedman, Wise 1978 
Addendum to Chapter 1. Notations 
The notation used corresponds basically to that of ALGOL 68. For extensions by guarded 
constructs the notation proposed by Dijkstra has been, in principle, maintained. The use 
of quantifiers v and 3 as well as the choice operator and the description operator was 
adapted to ALGOL notation. In particular 1 JLX: p(x) stands for lxP(x). 
In formal logic, different notations for defining recursive functions are in use. Authors 
oriented towards theoretical computer science, e.g. Manna, write 
F(n) "' if n = 0 then 1 else n x F(n -
1) 
(where mode indications are considered superfluous because they are constantly nat or 
int). 
Function declarations in ALGOL 60 look slightly different, e.g. 
funct F = (nat n) nat: 
if n = 0 then 1 else n x F(n -
1) fi 
there reads 
integer procedure F(n); integer n; value n; 
if n = 0 then F: = 1 else F: = n x F(n -
1) 
(because mode nat is not available). The occurrence of the symbol . -
signifies the 
dominance of the concept "program variable" in ALGOL 60. 
In PASCAL (1971) we have 
function F(const n: integer): integer; 
begin if n = 0 then F: = 
1 else F: = n * F(n -
1) end 
more closely related to ALGOL 60 than to ALGOL 68 (notational peculiarities are provid-
ed for indirectly recursive systems). Finally the LISP form, which demands strict prefix 
notations, looks like 

Addendum to Chapter 1. Notations 
DEFINE (( 
(FACT (LAMBDA (N) (COND ((ZEROP N) 1) 
(T (TIMES N (FACT(SUB1 N))) ))) ))) 
where the bracket-prone notation is not very inviting. 
113 
A number of "programming languages" (which do not deserve to be called algorithmic 
languages because of conceptual poverty) induce monstrous constructions, allegedly to 
guarantee efficiency and machine orientation. The PL/I version of the above serves as an 
example: 
FAC: PROCEDURE (N) RECURSIVE RETURNS (FIXED DECIMAL); 
DECLARE N FIXED DECIMAL; 
IF N = 0 THEN RETURN (1); 
ELSE RETURN (N • FAC(N - 1)); 
ENDFAC; 
or the SNOBOL version 
DEFINE ('FAC(N)') 
: (FACEND) 
FAC FAC = EQ(N) 1 
: S (RETURN) 
FAC = N • FAC(N -
1) : (RETURN) 
FACEND 
PASCAL provides a restricted form of object declarations ("constant declarations") 
where mode indications are suppressed 
const a= rff 
The MESA form is 
a: T = rff 
which corresponds to our t a "' rff. 
Landin recognized the relation between object declarations and the Lambda-calculus in 
1964. He uses 
sr (xJ where X = :!l 
as well as 
let X = :!l; 3' (XJ 
instead of the Lambda-expression (.l.X. 3' (X])( q'). Another notation is 
with X = :!l; sr (xJ 

114 
1. Routines 
Result parameters existed in their pure form in ALGOL 58. They were merged com-
pletely with variable parameters in ALGOL 60, whereas ALGOL W provided at least a no-
tational separation (Wirth, Hoare 1966). Result parameters are also clearly indicated in 
ALPHARD. Though following in general the PASCAL notation, Wulf et al. (1976) write 
function f(a: x) returns (b: p) 
which corresponds to 
funct f = (xa) ~-+ (Jlb) 
in our case. The CLU notation (Liskov et al. 1977) is similar. ALPHARD incorporates 
assertions into the procedure heading and adds them to the body as conditions to be satis-
fied by the result: for example one writes (let rff be an expression) 
function f(a: X) returns (b: p) 
pre _qJ(aJ postb = c(aJ 
corresponding to our 
functj = (xa: _qJ (a]) ~-+ (Jlb): b = rff (aJ 
This notation has the advantage of allowing additional conditions to be required of the re-
sult e.g. 
function sign (a: integer) returns (b: integer) 
pre a * 0 post b x a = abs(a) 
where we would write 
funct sign = (int a: a * 0) int: '1 int b: b x a = abs(a) 
Furthermore, it can be used on the procedural level (comp. Chap. 5) dealing with 
variables, where the pre- and postconditions indicated by pre and post are of particular 
importance (comp. 5.4). 
The binary branching in its ALGOL 60 form 
if !?I then Yj else .9'2 
is very common. The termination symbol II (ALGOL 68) simplifies the syntax. 
McCarthy (1961) writes 
(!?11 -+ rff1, !?12 -+ rff2 , T-+ rff3) 
and shorter 
where we have 

Addendum to Chapter 1. Notations 
115 
Other means frequently employed to illustrate and check complex branching cascades 
("decision tables") are only notational variants. 
Where infix or prefix notation replaces function notation in the case of primitive 
operations, the number of brackets is usually reduced by precedence rules. ALGOL 68 
leads with ten precedence levels, followed closely by ALGOL 60 with nine. PASCAL, 
where the number of operations is increased with respect to ALGOL 60, tries to make do 
with four levels: Negation (unary operator) has highest precedence, followed by the 
"multiplication operators", such as x, I, div, mod, "· Then there are the "addition 
operators", such as+,-, v, and finally the "comparison operators"=, *· <, ~. >, 
;a, on the lowest level. There is no power operator in PAS CAL. 
As long as there is no agreement concerning precedence rules, it is better to separate the 
part of the syntax describing them from the rest of the syntax and only deal with functional 
composition ("abstract syntax", McCarthy 1962). 


-
' 
\ 
Medal designed by Leibniz on the occasion of his dis-
covery of binary computing 
Chapter 2. Objects and Object Structures 
"An object is composed in general of components 
which are objects themselves .... There are elementary 
and composite objects. An elementary object has no 
components, it corresponds to the terminal of 
generative syntax." 
Zemanek 1968 
This chapter proceeds on the basis of the terms object and object set. Structured objects, 
in particular, are introduced where the structuring can also be recursive. Important 
examples such as sequences and cascades are treated in more detail. 
Note that the concepts discussed in the preceding (first) chapter were completely inde-
pendent of the choice of the object set and its meaning ("semantic interpretation"). Con-
versely in this chapter we will not deal with the algorithmic aspects discussed in the first 
chapter. 
Not only algorithms, but also objects often undergo an evolution during the program-
ming process - a refinement of the object structure ("data structure"). In the next (third) 
chapter the simultaneous development of program and object structure ("joint refine-
ment", Dijkstra 1969) will be discussed. There the interplay of the introduction of new 
operations and new object sets leads to the concept of a general computational structure. 
(A first step in this direction was the class concept of SIMULA.) 

118 
2.1 Denotations 
Objects can be e.g. 
numbers 
truth values 
sequences, 
treated as examples in chapter 1, but also 
telephone numbers 
playing-cards 
motor vehicle registrations 
fingerprints 
geometrical figures 
functions IR -+ IR 
formulas. 
2. Objects and Object Structures 
In order to be able to use individual objects we have denotations for them. There are 
standard denotations for certain objects, e.g. 
literals such as 
'a' 'jfk' 'adam.!.riese' 
numerals such as 
5 17 257 
word symbols such as true 
false 
chiffres such as 
(213) 457- 3547 
graphemes such as 
for character strings 
for numbers 
for truth values 
for telephone connections ("phone 
numbers") 
for playing-card suits. 
Standard denotations exist by general agreement ("pragmatically") and are available as 
language elements. First of all there are standard denotations for all objects which are ele-
mentary, i.e. whose definition is not based on other objects. In order to provide additional 
denotations as a base in the development of programs, the language in use must permit the 
association of standard denotations to other objects than elementary ones. 
As a matter of fact there should be not more than one standard denotation for each ob-
ject, but there is a snag in this requirement (for 100 we may also find + 100, or for 0.38 we 
have .38). Some objects have no standard denotation at all or, to be more precise, do not 
need any, e.g. variables (s. Chap. 7). However, even for objects which have a standard de-
notation it is sometimes suitable to use a non-standard denotation, such as (comp. exercise 
1.4.4-3) 
z nat x: prim(x) " (v nat y: prim(y) ""y ~ x) 
(predicative denotation) or (with the help of a routine ack for the Ackermann-Hermes 
function, 1.6.2) 
ack(4, 4) 
(operational denotation). 

2.1 Denotations 
119 
Of course for the natural number ack(4, 4) there is also a standard denotation which 
nobody has written down yet: if one could write 4 digits per second one would need 
101019727 seconds - for comparison: the world is approximately 1017 seconds old. 
An operational denotation for 21 is 17 + 4. Operational denotations such as 
succ(O), succ(succ(O)), succ(succ(succ(O))) 
(by applying a primitive operation succ) serve first and foremost for the definition of the 
natural numbers 1, 2, 3 (see also 2.9.3, operational detailing of objects). In Chap. 3 we will 
in general introduce the objects of abstract computational structures in an operational 
way. Some standard denotations, e.g. for negative integers, for rational numbers and 
complex numbers are actually operational. Even a notation such as 357 is already 
operational and means (3 x 10 + 5) x 10 + 7. Not only the object structure of a com-
posite object (2.5) but also an operational meaning is hidden behind the decimal-digit 
notation for numbers. 
Furthermore there are user-coined, freely chosen designations ("identifiers") which can 
be introduced by means of a declaration for a nullary operation such as 
funct millionf = int: 10 x 10 x 10 x 10 x 10 x 10 
funct indf 
= int: (0 D 1) 
They yield upon call a particular object. Note that as a rule this is the same object (a 
"constant") in every call, but that it does not have to be (nondeterminate routine indf, rou-
tine random in ALGOL 68 !). However, objects can always be denoted by constants, i.e. 
by nullary determinate routines. 
A freely chosen designation can also be introduced by means of an object declaration 
(for a computed object, i.e. for an object with an operational denotation or for a 
predicatively described object). This permits abbreviation of both the operational denota-
tion and the computation of the object, as shown in 1.13.3 (a descriptive and operational 
improvement). 
Examples: 
int million = int: 10 x 10 x 10 x 10 x 10 x 10 
lnt ind 
= int: (1 D 0) 
real pi 
= real: «Ludolph's number, the ratio of the circumference to the diameter 
of a circle» 1 
Note that from then on ind - wherever it is used - always denotes the same object; it is 
not determined whether this is always 1 or always 0. 
In the sense of an arbitrarily good approximation such an introduction of 1r. is tolerable (and 
practically indispensable). There is, however, no terminating algorithm which delivers 1r. as a 
decimal fraction or dual fraction. 

120 
2. Objects and Object Structures 
2.2 Scope of a Freely Chosen Designation 
The scope of a freely chosen designation is restricted to that section in which the corre-
sponding declaration occurs (1.13.2). Section delimiters like I· J (frequently replaced by 
begin . end) are necessary unless this function is already served by bracketing symbols 
such as if. then. else. fi or while. do. od (Chap. 5). The scope of formal parameters of 
a routine is likewise restricted to the routine because of the binding character of the 
designation. 
If the outer frame disappears while expanding a routine, then the scope must remain 
restricted to the original frame of the routine, if the semantics is to remain unchanged -
otherwise we must explicitly re-name. Thus the nested structure of the scope of designa-
tions evolves in a natural way. We will use section delimiters to delimit syntactic units, 
which have the original character of routines and thus restrict the scope of the designations 
introduced within them. 
Note that explicit re-namings can only be avoided by providing that if identifiers clash, 
the identifier "further inside" overrules the one "further outside", cf. 1.13.2. 
Example: 
funct h = (real x) real: 
I real s = p(x) within 
s x ln(s) + I real s = 1 
s x ln(s) 
2.3 Kinds of Objects 
- p(x) within 
J J 
We distinguish between simple objects, which (in a given context) cannot or should not be 
broken down further into components, and composite objects (see 2.5). It is often a matter 
of opinion whether an object is considered to be simple or composite, cf. int, rat, compl, 
string, real. Further examples of composite objects are sequences and sets which will be 
given an abstract definition in the next chapter. 
Furthermore, there are elementary objects2, i.e. those objects on which our computa-
tions are ultimately based. Non-elementary objects, on the other hand, are justified only 
by the existence of elementary objects, for example 
routines, 
(program) variables and 
pointers. 
The introduction of routines (to be more general: computational structures) as objects 
dates back to Rutishauser 1958, as far as programming is concerned. Church has already 
taken this step in logic with his Lambda-calculus. In ALGOL 68 procedures can occur both 
as parameters and as results of routines, likewise in SIMULA (cf. 1.14). 
2 Composite objects are in this context eo ipso non-elementary. 

2.4 Sets of Objects, Modes 
121 
(Program) variables allow varying objects to be kept under a constant designation. 
Pointers serve, among other things, the construction of a nexus of variables. In general, 
there are no standard denotations for them nor for variables (apart from cases such as AC 
for an "accumulator"), they are "generated" (see Chap. 7) objects. 
Just as there are primitive operations in algorithms there are also primitive objects 
which (in the given context) are not explained further. The concept "primitive" must be 
considered a relative one in both cases. 
2.4 Sets of Objects, Modes 
2.4.1 By definition, sets of objects ("domains") are composed of well-distinguishable and 
well-distinguished objects. It is always defined whether any two elements out of the set are 
equal or not (universal equality relation =, '*Y. In general, sets of objects are characteriz-
ed by typical primitive operations defined on them (for details see Chap. 3). Often sets of 
objects are ordered or even linearly ordered. We also speak of modes. 
So as to be able to make inductive assertions about the sets of objects to be used, it is 
convenient to require that a Noetherian ordering be given for them or can be explicitly con-
structed 4• To be able to handle the sets of objects used it is, as a rule, even required that 
they be recursively enumerable (or: effectively countable), namely that there is a comput-
able function to enumerate them 5• Note that IR is not countable and therefore not recur-
sively enumerable. It is therefore preferable to restrict oneself to computable real numbers 
(Borel 1912, Myhill 1953). Sets of objects, too, have designations, such as nat, bool, 
string, which are called indicants. Indicants of object sets serve in certain cases for the 
construction of test operators checking whether the object to which they are applied be-
longs to the set of objects, and if so, yield true, otherwise false; e.g. 
nat :: x stands for the test function x e fN 
By means of such indicants, it is asserted in the heading of a routine that parameters 
belong to the given sets of objects (and possibly satisfy further conditions, cf. 1.8). 
Indicants, followed by a colon, serve as specifiers (just as at the end of the heading of a 
routine for establishing the result mode). We will refer to this again when we discuss com-
posite objects. Furthermore, indicants are used in connection with quantifiers and with the 
operators of choice and of determination ( cf. 1.1 0). 
Examples of sets of objects (and corresponding indicants) which are widely considered 
universal are (comp. Table 1.3.1) 
3 Independent of the context x = x should hold, and if pis a predicate, (x = y A p(x)) => p(y). For 
composite objects equality is based on the equality of the components (s. 2.5). 
4 In IR, ~ is a linear ordering but not a Noetherian ordering, that is, not a well-ordering. According 
to the axiom of choice, IR can be well-ordered. So far nobody has given an explicit well-ordering 
for this non-countable set. 
5 For the relevant concepts see Davis 1958. 

122 
2. Objects and Object Structures 
Usual mathematical 
Mode indicant 
Mode 
notation 
rN 
nat 
Set of natural numbers 
(including zero) 
z 
int 
Set of integers 
IBz 
boo! 
Set of truth values 
and 
'"/' 
char 
Character set 
'"/'* 
sequ char 
Set of character sequences 
"''is an arbitrary finite 6 (or at least enumerable), linearly ordered set of characters, i.e. an 
alphabet (if the set is not finite, it is required that the linear ordering be Noetherian, i.e. be 
a well-ordering, with the cardinality of the natural numbers). 
Standard denotations for number-like objects and for truth values (true and false) are 
common. Standard denotations for objects of the modes char and sequ char are 
characters of character sequences placed between apostrophes. 
2.4.2 Finite sets of objects can be introduced by (finite) enumeration of the denotations for 
the respective objects, called atoms; the denotations thereby become standard denota-
tions. Thus 
atomic { +. <>. Q, • } 
atomic {masculine, feminine} 
atomic {1., 2., 3., 4., 5., 6., 7., 8., 9.} 
atomic {0, L} 
are explicit indicants of sets of objects, they are constructed by means of the set brackets{,} 
from the standard denotations of their elements, provided that these are different from 
ordinary letters. Symbols such as • 
or 3. thus become standard denotations, just as 
masculine or true 7• 
For "enumerated sets" of this kind, a linear ordering, namely the ordering of the 
enumeration sequence, is simultaneously definable. Based on this the relations <, ~, >, 
~ are universal. For the sake of clarity, the intention of using this ordering should be 
made visible in the enumeration, perhaps by using " <" instead of "," as separator 
(s. below). In addition, the functions succ and pred -
which deliver the following or 
preceding value in the enumeration (if it exists) - are universal. Note that no repetition is 
allowed in the enumeration (set!). Even if no further operations - especially no composi-
tions -
are defined a priori on these sets of objects, their use is widespread, at least 
because of the universal ordering relation. 
Of course for abbreviation freely chosen mode indicants can be introduced by mode 
declarations, e.g. 
6 According to Bolzano, a set is said to be finite if it cannot be mapped one-to-one onto a proper 
subset of itself. 
7 In PASCAL such freely chosen "key words" are typographically not distinguished. 

2.4 Sets of Objects, Modes 
123 
mode suit 
= atomic { + < <) < Q < + } 
mode sex 
= atomic {masculine, feminine} 
mode status = atomic {single, married, widowed, divorced} 
mode ordinal =atomic {1. < 2. < 3. < 4. < 5. < 6. < 7. < 8. < 9.} 
mode bit 
=atomic {0 < L} 
2.4.3 Sets of objects can also be introduced as enumerated subsets of a set of objects al-
ready introduced: 
int {1, 3, 2} 
ordinal {2., 5., 8.} 
or can be introduced as intervals from already existing ordered sets, by giving the two 
(included) end elements: 
int [1900 .. 1999] 
suit [+ .. QJ 
ordinal [3 ... 7.] 
real [0 .. 2 x pi] 
e.g. in the mode declarations 
or 
mode trio = int {1, 2, 3} 
mode century 
mode consolation prize 
mode arc 
= int [1900 .. 1999] 
= ordinal [3 ... 7.] 
= real [0 .. 2 x pi] 
An enumerated subset or an interval carries the ordering which is already given for its ele-
ments (induced ordering). 
Sets can also be defined, using predicates, by comprehension if they are subsets of a 
given basic set: e.g. the set of all odd integers is explicitly indicated by {int x: odd x }. Thus 
a new mode can also be introduced by means of a mode declaration like 
mode oddnumber = {int x: odd x} 
or 
mode pnat ,.. {nat x: x * 0} 
The general scheme for this reads 8 
mode Jl' = {Jlx: p(x)} 
where p is a function with the functionality (Jl) boo I. The subset enumeration discussed 
above and the construction of intervals are special cases of this, e.g. 
8 Greek letters in bold type (generic symbols, cf. Table 1.3.1) are scheme parameters: they can be 
interpreted as concrete modes. For more details see Chap. 4. 

124 
2. Objects and Object Structures 
mode trio 
= {int x: x = 1 v x = 2 v x = 3} 
mode century = {int x: 1900 ~ x A x ~ 1999} 
More details will be given in 2.8. 
The ordering of the complete set is again carried over to the subset (induced ordering). 
In the sequel we will normally assume that the sets of objects under consideration are 
linearly ordered. Note that the requirement that the set allows a Noetherian ordering 
(especially, that it is recursively enumerable) does not mean that the linearly ordered set 
should also in fact be a well-ordering: integers with the natural ordering ~ are not well-
ordered. They can, however, be well-ordered: 
0, 1, -1, 2, -2, 3, -3, ... 
Referring to the importance of freely introduced sets of objects determined by 
enumeration Wirth says (Wirth 1976, p. 6, 7): 
"In many programs integers are used when numerical properties are not involved and 
when the integer represents a choice from a small number of alternatives." 
Such misuse of integers should be avoided. If, for instance, for the identification of chess 
board squares, normal denotations such as c2 or h7 are actually to be used and not pairs of 
numbers such as (3, 2) or (8, 7), then we must be able to introduce operations for these 
freely chosen sets of objects which describe e.g. capturing by a pawn (Pc2 x Rd3). We will 
refer to this again in 3 .1.4. 
2.4.4 Finally, a word about the examples introduced at the beginning: ~ could be 
constructed from the enumerated object set 
mode boo! = atomic {false < true} 
By means of alternatives operations could then be introduced, e.g. 
funct not = (boola) bool: 
if a then false else true fi 9 
and 
funct and =(boola, bool b) bool: 
if a then If b then true else false fi 
else if b then false else false fi fP 0 
Because an ordering is introduced in the enumeration, relations such as <, ~, >, ~, 
are defined for bool, too. Note that a ~ b means the subjunction a => b. 
9 or simpler 
funct not "" (bool a) bool: a = false 
(if one uses the universal equality relation). 
10 The simpler version 
if a then b else false fl 
("sequential conjunction", 1.3.3) is not strict in the sense of 1.5. Both versions are equivalent, 
however, if b is defined. 

2.4 Sets of Objects, Modes 
125 
For "f/ the following holds: if "f/ or the mode char is used, this often actually means that 
precise details regarding the characters or the character set cannot or should not yet be 
given (parameterization of the mode, one should rather use x instead of char, where xis a 
generic symbol). 
"//* and 7L are examples of the construction of composite objects. In the first case they 
are recursively defined (2.9) and in the second case they are introduced as pairs (2.5). For 
abbreviation, we use 
mode string "' sequ char 
Thus IN remains: a finite enumeration is not possible. The complete set of objects is re-
cursively enumerable and is defined using a primitive operation succ and a distinguished 
object denoted by 0 (Peano's axioms). Such descriptional means are outside the scope of 
this chapter, we will deal with this again in Chap. 3. This is sufficient reason for assuming 
IN or nat usually to be a primitive set of objects. By the way, it cannot be concealed that 
declarations such as 
mode nat "' IN [0 .. 248 -
1] 
are secretly implied everywhere, e.g. in the PASCAL report of 1971. 
2.4.5 Multi-stepped parameterization (comp. 1.14) is absolutely necessary in the following 
situation: object sets such as 
int [1900 .. 1999] 
or {nat x: mod(x, 3) = 1} 
are computed modes. It is legitimate to parameterize the computation, that is to speak of 
int [n .. m] or {nat x: mod(x, p) = 1} 
If, for example, int[1900 .. 1999] is the mode indicant for a parameter of a routine 
such as 
(int [1900 .. 1999] year) bool: «year is leap year» 
then all is well. But, if lnt [n .. m] is to serve as mode indicant for the parameter year, then 
the routine must be parameterized in two steps, e.g. 
funct q "' (int n, int m) . (int [n .. m] year) bool: <<Year is leap year» 
The call 
q(1900, 1999) 
(partial instantiation) delivers the routine above, the complete call 
q(1900, 1999) (1924) 
yields the result true. 

126 
2. Objects and Object Structures 
2.5 Composite Modes and Objects 
A composite 11 object is an n-tuple (e1, ••• , en), n ~ 2, of objects e; of the component 
mode IIi· The set of all such composite objects is called the composite mode (Ill, ... , Jln). If 
all component modes are linearly ordered another linear order is induced on the set of the 
tuples by the lexicographic order; this determines the relations ~, ~, <, and >. 
The equality relations =, =1= for composite objects of the same mode are universally 
defined, based on the equality of the components. 
The constructor which composes an object of mode (Ill, ... , Jln) from the objects e; is 
written in the form 
(Ill, ... , Jln): (el, ... ,en> 
It is in general mandatory to specify the corresponding composed mode. This can, how-
ever, be omitted if the mode can be determined uniquely from the context. 
Examples: 
(int, nat): (3, 5) 
is an element of (int, nat), 
(nat, nat, lnt, boo!): (4, 12, -2, true) is an element of (nat, nat, int, boo!) . 
The collection of (single) objects used in routines having a multiple result is conceptual-
ly (and syntactically) distinguished from the construction of a composite object. We will 
return to this subject in 2.16. 
For purposes of abbreviation one again introduces freely-chosen mode indications for 
composite modes by means of mode declarations, e.g. 
or 
mode rat = (int, nat) 
mode pile = (nat, nat, lnt, boo!) 
mode date = (int [1 .. 31], int [1 .. 12], int[1900 .. 1999]) 
mode pers = (string, date) 
For the above examples they permit the use of the abbreviated notation 
rat: (3, 5) 
or 
pile: (4, 12, -2, true) 
Object declarations are of course also possible, e.g. 
rat a = rat: (3, 5) or pile b = pile: (4, 12, -2, true) 
in short: 
rat a = (3, 5) 
or pile b = (4, 12, -2, true) 
11 Also structured object (ALGOL 68, Wirth 1967). 

2.6 Selectors, Structures with Direct (Selector) Access 
127 
Composite objects in which all components are of the same mode are said to be homogene-
ous (example: array, see 2.6.2). 
There are of course trivial n-tuples for n = 1 and n = 0: For the borderline case of a 
"composite object" with one component there is only a slight difference between nat: a 
and (nat): (a) (see 2.8, widening). 
The 0-tuple of objects with the standard denotation 12 0 is a universal, untyped special 
object, the empty object or null object, called empty sequence or empty word in special 
cases. Other denotations used are empty, A, e. 
In a mathematical sense the preceding paragraph concerns the construction of the 
cardinal (or direct) product 13 of sets. The cardinal product A x B of two sets is the set 
{ (x, y): x e A, y e B} of all ordered pairs of elements from A and elements from B. The 
cardinal product is obviously not commutative. It is also not associative: ( (x, y ), z) is 
different to (x, (y, z )). See 3.4.4 for the construction of associative equivalence classes 
(cartesian product A ®B). For the present we will assume for n-tuples successive pairing 
with an element annexed on the right. This corresponds to the usual order of writing (left 
associativity). Thus 
(nat, nat, int, bool) stands for (((nat, nat), int), bool) 
For the cardinality of the cardinal product of finite sets 
card(A x B) = card(A) · card(B) 
holds. 
If 0 denotes the empty set, then 
card(A x0) = 0 = card(0xA) 
Two "canonical mappings" belong to the cardinal product of two sets 
nA xn= A X B __.A, PA xn= A X B __. B, whereby 
nAxB(a,b) =a and PAx 8 (a,b) = b 
("projection functions" for selection). 
2.6 Selectors, Structures with Direct (Selector) Access 
Frequently, in fact almost as a general rule, we want to work directly with the individual 
components of a composite object. In order to have easy access to any one component, 
that is to have all projection functions immediately available, we use selectors. This access 
is called selection. 
12 Introduced by Manna and Waldinger. 
13 Strictly speaking this is an ordinal product, comp. G. Birkhoff, "Lattice Theory", 3rd ed., p. 55, 
198, if one considers the fact that the resulting set is lexicographically ordered. 

128 
2. Objects and Object Structures 
Selectors (projection functions) can be derived from objects (e.g. "the third from the 
left") and are then called indexes, or they are introduced as freely chosen identifiers. 
If a component of a composite object is a composite object itself this again can be 
subjected to selection. We then speak of a multi-stepped selection which is composed of 
(ultimately elementary) selections according to the composition of functions. 
The following diagram showing the composition of the mode pers (see 2.5) serves as an 
example 
pers 
~ ~ 
string 
date------
·-------- \ 
. 
mt [1 .. 31] 
mt [1 .. 12] 
mt [1900 .. 1999] 
Selection, considered as a function, is only partially defined. Correspondingly, the 
composition of selections is not always defined. A chaining condition must be fulfilled to 
allow the composition of selections (as in the case of functions). If a selection by means of 
a selector s1 can be applied to the result of a selection by means of s2 , in short: 
s1 is applicable after s2 , 
and if the corresponding composite selection (the "product") 
s1 o s2 is applicable after s3 , 
then 
s2 is applicable after s3 and s1 after s2 o s3; 
thus the law of associativity holds: 
Looked at mathematically: the entirety of selections forms a category 14• 
Because of associativity it would appear natural to assign a composite or a multi-
stepped selector to the complete selection which has been effected by the sequence s1 , s2 , 
... , sk and to write this selector as s1 os2 o · · · osk 15 • For purposes of distinction we call the 
selectors given directly single-stepped selectors. 
2.6.1 Compounds 
If the specification of a composite mode contains -
in addition to the modes of the 
components - the corresponding selectors as freely chosen identifiers the corresponding 
composite objects are called compounds: 
14 We also define the result of a not applicable selection to be Q. 
15 McCarthy already considered the composition of selectors in 1960, e.g. he writes caddar for car o 
cdr o cdr o car. 

2.6 Selectors, Structures with Direct (Selector) Access 
129 
mode rat 
= (nat numerator, int denominator) 
mode complex = (real re, real im) 
mode date 
= (int [1 .. 31] day, int [1 .. 12] month, int [1900 .. 1999] year) 
mode person 
= (string name, string first name, date date of birth, 
sex sex, status status) 
Note that (real re, real im) and (real im, real re) do not define exactly the same 
compound modes, but essentially (i.e. up to position) the same compound modes, as e.g. 
(int a, nat b) int: alb and (nat b, int a) int: alb 
define essentially the same routine (see 1.1). 
For access to the component of a compound v determined by the selectors we write 
s of v or 
v.s 
Examples: 
For complex z = (4.0, 3.0) 
reofz or z.re 
for date h = (30, 12, 1976) 
year of h or h. year 
for person x = (,bauer', ,martin', date: (14, 6, 1976), masculine, single) 
day of (date of birth of x) or x. date of birth. day 
(double-stepped access). 
As the example 
funct birthday = (person x) (int [1 .. 31], int [1 .. 12]): 
(day of date of birth of x, month of date of birth of x) 
shows, the use of collective selection ("simultaneous", not multi-stepped) is suggested e.g. 
(name, first name) of x, 
(day, month) of date of birth of x 
2.6.2 Arrays 
The fact that selectors can be composed suggests considering selectors as objects. Besides 
composition, a partially defined successor operation «next selecton> is available as an 
operation for elementary selectors. The corresponding linear ordering then induces a 

130 
2. Objects and Object Structures 
lexicographic ordering for composite selectors and thus a general successor operation for 
the selectors of finitely composed objects. Hence (certain) selectors can also be opera-
tionally determined. It is no longer necessary to specify them by means of freely chosen 
identifiers. Such computable selectors or indexes are especially advantageous in the case of 
homogeneous composite modes: 
Let the (usually nonempty and finite) index set v be well-ordered. A homogeneous 
structure array (i.e. a sequence of objects of the same basic mode Jl, written as v array Jl) 
is introduced as set of all n-tuples of elements of mode J1 where n = card(v). There is an 
order-preserving one-to-one mapping between index and component position. Note that 
no index is left out. Because the index set vis well-ordered, access is uniquely determined: 
the smallest element of v gives access to the first component of the n-tuple. 
Examples of mode declarations: 
mode output 
mode account 
modeq 
-
int [1 .. 12] array int (in ALGOL68 : [1 : 12] int) 
= atomic {black, red} array nat 
-
bool array suit 
i 
i 
index mode 
basic mode 
Note that int array J1 is not permissible because int with respect to ;:§ is not a well-
ordering. The infinite array nat array J1 is unrealistic, too, since we cannot write down the 
constructor, however see 3.3.1 ("indexed sequence"). 
For accessing the component of the n-tuple t determined by the index i we write 
t[ijl 6 
or t. i 17 
and also call this selection indexing. 
Examples are: 
for output x = (20, 20, 23, 19, 20, 20, 21, 23, 22, 21, 21, 21) 
x[5] 
or x. 5 
for account balance = (1450, 7280) 
balance [red] 
or balance. red 
for q game = ( +, Q ) 
game [true] or game. true 
16 For historical reasons t[i] stands fort;. actually it leads to an unnecessary distinction between the 
indexes and the (explicit) selectors in the case of records. 
17 Known as Dewey notation for i e IN especially in multi-stepped application, also called "decimal 
classification". It originates from Francis Galton 1889 (comp. Knuth 1973). 

2.6 Selectors, Structures with Direct (Selector) Access 
131 
As the indexes are objects they can also be computed (in contrast to identifiers). By the 
principle of substitution we have the possibility to write 
x[3 x 3], x[n + 1], game (q 1\ r] 
There are notational abbreviations for multi-stepped arrays, e.g. for 
int [1 .. 24] array (int [1 .. 60] array real) x 
in short 
int [1 .. 24, 1 .. 60] array real x (in ALGOL 68: [1: 24, 1: 60] real x) 
Correspondingly, 
x[23, 45] 
stands for x[23] [45] 
2.6.3 The Selection Structure of Compound and Array 
Mathematically, both compounds and arrays are elements of the cartesian product @A i of a 
iel 
family of sets A; (i E I). In the case of an array the elements of the index set I (in addition to being 
selectors) are objects. Because of homogeneity of arrays we have, furthermore, the special case that 
for all i the A; are equal to a set A. Thus, these arrays form exactly the cardinal power A I, also 
defined as the set of all mappings of I into A, where I is the index set and A is the basic set. Each 
mapping from A I assigns an element of A to every index i E I; for a fixed i the selector function 
results, running over all mappings from A I, as a mapping of A I into A. 
Note that card(A I) = (card(A))card(I>. Note also that for finite I 
AI=A@A@ ... @A 
'-v----J 
card(!) 
A 0 is a singleton, viz. A 0 = { ¢ ), where ¢ denotes the 0-tuple. In other words A 0 is order-isomorphic 
to the ordinal number 1. of the cardinality 1, i.e. A 0 ~ 1 .. Note also that A Q9 1. ~A ~ 1. Q9 A, 
and that A 1· ~ A. Moreover, A Q9 0 ~ 0 ~ 0 Q9 A ( ~ means "order-isomorphic"). 
The indicant empty denotes the universal mode which contains as sole element the 
universal special object 0 (0-tuple), i.e. 
mode empty =atomic {0} 
Furthermore we state that 
(Ji. empty) and (empty, f.l), 
and also (Jl) 
with respect to tuple construction are order-isomorphic to f.1 and that, as a result of this iso-
morphism, 
( 0, x) and (x, 0 ), and also 
(x) 
correspond to the object x. 

132 
2. Objects and Object Structures 
A set of composite objects all of whose components allow immediate access by means 
of selection is called structure with direct access. Compound and array belong to this 
category. In the case of an array we speak of a structure with selector-sequential direct 
access because the components can be examined in succession by using the successor func-
tion succ which is defined on the linearly ordered index set. This use of the successor func-
tion is independent of the homogeneity of the structure. 
The notation of the mode specification and the access for an array - with finite index 
set - is obviously an abbreviation of the corresponding notation for a homogeneous com-
pound; e.g. 
nat [1 .. 4] array 11 
or 
nat {1, 2, 3, 4} array 11 
stand for 
(11 one, 11 two, 11 three, 11four) 
and if r is an object of such a mode 
r[2] 
stands for 
two of r 
Furthermore 
atomic {black, white} array 11 
stands for 
(11 black, 11 white) 
If an index set is used for several arrays a freely chosen indicant can be introduced with 
the advantage that we can replace the enumeration by a particular mode specification and 
compute the indexes. 
For non-finite index sets, however, the notation as an array can no longer be replaced 
by a notation as a compound. (On the other hand in 2.9 an extension of compounds is 
discussed which permits "potentially infinite", i.e. unlimited object structures 18 .) 
For implementation purposes it is naturally of importance whether the index set is 
finite or not. In ALGOL and PASCAL for example finiteness of the index set is required. 
However, nat array 11 can cum grano sa/is also be implemented (see 3.3.1 and 7.4.4). 
There is a natural relation between arrays and functions: e.g. an array of mode 
nat [1 .. 12] array int 
corresponds to a routine of mode 
18 Under the termination requirement for algorithms, the complete data structure which belongs to 
an infinite index set cannot be processed, but of course segments of arbitrary size can be 
processed. (See also 3.3.3, "aggregates".) 

2.7 Mode Variants 
133 
funct (nat [1 .. 12]) int 
with index range nat [1 .. 12] as parameter domain. 
To be more general an array 
v [a .. b] array 11 or v array 11 
corresponds to a routine of mode 
funct (v [a .. b]) 11 
or 
funct (v) 11 
Arrays are "frozen functions". 
Thus e.g. the routine grigri of 1.13.3 and 1.14 develops immediately into a routine 
which has an array a of mode nat [1 .. N] array int instead off as suppressed parameter: 
funct grigri = (nat n: n =1= 0 A n ~ N) (int, nat): 
if n = 1 then (a[1], 1) 
D n =1= 1 then (int dom, nat numb) = grigri(n -
1) within 
if a[n] = a[n -
numb] then (a[n], numb + 1) 
D a[n] > a[n -
numb] then (dom, numb) 
fi fi 
Composite objects can also result from combining compound and array construction, 
e.g.19 
mode fstack x = (nat level, nat [1 .. 1024] array x data) 
or 
mode table 11 x = nat [1 .. 1 00] array (11 arg, x value) 
We will only discuss compounds in the following paragraphs and deal again with some 
special characteristics of arrays in 2.15. 
2.7 Mode Variants 
2.7.1 It is often convenient and indeed natural to consider two or more sets of objects as 
variants of a mode. Thus in certain computations natural numbers, rational numbers and 
integers can occur as alternatives. The position of a point can be given by cartesian 
coordinates (real, real) or by polar coordinates (real, arc). We introduce such variants of 
a varying mode by means of mode declarations such as 
mode number = nat 1 int 1 rat 
mode point 
= (real x, real y) 1 (real r, arc phi) 
19 An arbitrary mode is again expressed by a generic sign such as X• ll· 

134 
2. Objects and Object Structures 
mode person = (string name, 
string first name, 
date date of birth, 
sex sex, 
I 
real weight, 
real length of beard, 
status status) 
(string name, 
string first name, 
date date of birth, 
sex sex, 
real chest measurement, 
real waist measurement, 
real hip measurement, 
status status) 
If all the variants are linearly ordered, then the varying mode is likewise linearly ordered: 
all objects of the first variant come before all objects of the second variant etc. 
As shown by the last example (according to Wirth 1976) variants can also occur as sub-
tuples (in fact this case will frequently occur in practice). In this example the objects for a 
personal record are of the structure 
(string, string, date, sex, (real, real! real, real, real), status) 
A corresponding abbreviated mode declaration of such a mode might read: 
mode person = (string name, 
string first name, 
date date of birth, 
sex sex, 
(real weight, real length of beard I real chest measurement, real 
waist measurement, real hip measurement), 
status status) 
Declarations of objects composed in this way can vary within the variants, e.g. 
number a = number: (nat: 3}, number b = number: (int: -2}, 
number c = number: (rat: (4, 7)), 
in short 
or 
number a= nat: 3, number b = int: -2, number c s rat: (4, 7), 
point x = point: (real: 4.0, real: 3.0), 
pointy = point: (real: 5.0, arc: 0.6435 >, 

2.7 Mode Variants 
135 
in short 
point x .. (4.0, real: 3.0), pointy .. (5.0, arc: 0.6435) 
Variants were introduced in 1961 by McCarthy. To consider two object sets as variants 
is, mathematically speaking, the same as forming the direct union 20 of sets. The direct 
union A ~ B of two sets of objects A, B is a new set of objects, i.e. the union of two 
disjoint subsets A' and B' of a certain basic set, one of which can be mapped one-to-one 
onto A, the other can be mapped one-to-one onto B, i.e. ( ~ means "isomorphic") 
A~ B =cterA' u B', 
where A' ~ A, B' ~ Band A' n B' = 0. The elements of A ~ B which correspond to the 
elements x e A or y e Bare written x' or y'. 
Note: A ~A therefore is not A: the direct union A ~ B is not simply the set union of 
A and B. On the contrary the elements of one A and of the other A should be considered to 
be different from each other: for example by marking one with the index 1 and the other 
with the index 2. According to this interpretation 
A ~A~ 2. x A 
holds where 2. is the ordinal number of cardinality 2 (comp. also bit, Table 1.3.1). 
Furthermore (for finite sets A, B) 
card (A ~ B) = card (A) + card (B) 
and 
A ~ 0 ~ A 
(comp. A x 0 = 0) 
holds. 
Canonical mappings are 
"injection functions" (comp. 2.8, widening) 
iA~B= A --+A ~ B, 
jA~B= B--+ A ~ B, 
where 
iA~8 (a) = a' 
jA~B(b) = b' 
"test functions" (comp. 2.4.1, test operators A :: x, B :: x) 
PA~B= A~ B--+ IB, 
where 
PA~8 (x) = (x eA') 
qA~B: A ~ B--+ IB, 
qA~8 (x) = (x eB') 
and "projection functions" 
20 Also called cardinal sum. Strictly speaking it is an ordinal sum, if we take into consideration that 
the resulting set is ordered. 

136 
2. Objects and Object Structures 
nAI±lB:A®B--->A, where 
nAI±lB(x')=x if x'eA', 
nA l±lB(x') undefined otherwise 
PA(!)B:A®B--->B, where 
PAI±l8(x')=x if x'eB', 
PA(!)B(x') undefined otherwise. 
The direct union is not commutative with respect to the ordering. In case of more than two 
variants we will again assume a continued pairing with an element annexed to the right (left 
associativity). 
If Jl and A are already disjoint subsets of a basic set, then the direct union can be 
interpreted as a normal union (and the isomorphism .' as identity). The union mode of 
ALGOL 68 (union) corresponds to this disjoint union. 
Test functions for objects of the varying mode number, for example, are obtained 
from the test operators nat :: . , int :: . and rat :: . 
If for an object of a varying mode an operation defined for only one variant is to be 
carried out, then the conformity of the range must be tested, e.g. for an object z of the 
mode number: 
if nat :: z then nat : z D lnt :: z then - int : z D rat :: z then - rat : z fi 
which usually is abbreviated to 
if nat:: zthenzD int :: zthen -zD rat:: zthen -zfi 
shorter 
if -,nat:: z then -z else z fi 
The following can be said for the connection between a varying mode 11 I A and the 
choice operator: 
If x is an obje~.:i of the mode Jl and y an object of the mode A, then the (non-
deterministic) routine 111 A: (x D y) yields a choice from the objects x' andy' of the varying 
mode IliA. 
2.7.2 Occasionally a varying mode occurring in a tuple is linked with other components. 
The three body measurements (in the above example) might only be ascertained from 
females and in any case the length of beard only from males. With the help of such 
components a comprehension can be used to guard the variants: we may have 
mode person = (string name, 
string first name, 
date date of birth, 
(sex {masculine} sex, real weight, real length of beard 
1 sex sex, real chest measurement, 
real waist measurement, real hip measurement), 
status status) 
or by complete separation ("discrimination") of the sexes (with disjoint sets for the sex-
component) 

2.8 Introduction of New Modes: Summary 
mode person = (string name, 
string first name, 
date date of birth, 
(sex {masculine} sex, real weight, rea! length of beard 
I sex {feminine} sex, real chest measurement, 
real waist measurement, real hip measurement), 
status status) 
137 
For discrimination purposes an additional component, a discriminator, can of course 
be used 21 • For example point can be introduced as a discriminated structure: 
mode point = (boo! {true} tag, real x, real y) 
1 (boo! {false} tag, real r, arc phi) 
Declarations for objects of this mode are 
point a = (true, 4.0, 3.0) 
and 
point b = (false, 5.0, 1.128) 
Another example: 
mode point1 = ({real r: r > 0} radius, arc phi) 
I (real {0} radius) 
The guards of the individual variants may be - but do not have to be - disjoint. In 
the latter case an "indiscrimination" remains. For each object of a varying mode at least 
one guard is fulfilled (i.e. one variant is released). In the special case of two variants with 
disjoint guards whose adjunction yields true we speak of alternative variants. 
Note: Indiscriminate object structures are similar to non-determinate routines. Just as 
determinate implementations of routines can be introduced, object structures can be 
implemented by discriminated alternative variants. 
2.8 Introduction of New Modes: Summary 
Forming the direct product of two modes, e.g. 
(int, nat) 
21 Compare the discriminating component in the variant record of PASCAL. 

138 
2. Objects and Object Structures 
obviously produces new objects. But a varying mode such as 
nat I int I rat 
also gives rise to new objects; the varying mode decomposes into (disjoint) subsets which 
are only isomorphic to nat, to int and to rat. 
This situation suggests the assumption that forming a new mode by comprehension 
with the help of a predicate as in 
{nat x: x * 0} 
where forming an interval 
int [1900 .. 1999] 
and enumeration 
int {1, 2, 3} 
are special cases, means also forming a set of absolutely new objects, which is only 
isomorphic to the specified subset. 
Thus we define: A mode expression, which is built from direct products, direct unions 
and set comprehension (in any combination) introduces new objects and thus defines a 
new mode, different from the constituent modes. 
Transition to the elements of the new mode is a mapping, which is normally expressed 
by prefixing the construct with the mode indicant, e.g. 
rat: (3, 5) 
number: (3, 5) 
pnat: 17 
century: 1984 
trio: 2 
See also point: (real: 5.0, arc: 0.6435) in 2.7. 
In this strict sense, the mode arc is absolutely different from the mode real; arc and 
real have no element in common. Likewise 
cart: (0.87, 0.53) and polar: (0.87, 0.53) 
where 
mode cart 
= (real x, real y) and 
mode polar = (real r, arc phi) 
are different objects; data in polar coordinates are quite distinct from cartesian data. 

2.8 Introduction of New Modes: Summary 
139 
Moreover, if one introduces two modes 
mode dm 
= nat, 
mode dollar "' nat 
one obtains different modes, the mode nat on the right hand side of the declaration is to 
be considered as a one-term expression -
e.g. a one-term variant. Thus, dm: 30 and 
dollar: 30 are different objects, which helps mnemonically against confusion. 
For new modes, obtained by comprehension with the help of a predicate, the definition 
above differs from a naive and self-suggesting subset relation. The background of this 
motivation will be made clear in Chap. 3; there the definition of modes will be made as a 
matter of principle in an abstract, axiomatic way, and thus will be established only up to 
isomorphism. This strict philosophy has the advantage of avoiding the problem of 
equivalence of modes. 
Naturally, we will try to suppress the indicants expressing mode transition, whenever 
the transition is clear from the context. To this end, one introduces a transitive relation 
" . is submode of . ", based merely on textual evidence, which is generated by 
(1) If mode J1 "' J1t 1112 I· •. l11n 
or mode J1 "' (J.) 
then 11; (i = 1, ... , n) or J., resp., is submode of Jl, 
(2) If model = {Jlx: p(x)} 
then J. is submode of Jl. 
This allows us now to state: If a mode J. is submode of a mode Jl, then transition from 
an element of J. to an element of Jl does not have to be indicated, provided it is clear from 
the context ("implicit widening"). 
A transition is certainly clear from the context if 
(1) the resulting mode of an object which is used on the right-hand side of an object 
declaration is seen from the mode indicant of this object declaration (or of a variable 
declaration, see 5.2.2), or 
(2) the resulting mode of an object which is an argument in a routine is seen from the 
specification of this parameter, or 
(3) the resulting mode of an object delivered as a result of a routine is seen from the result 
specification. 
(The converse transition to elements of a submode in general is only partially defined 
and has to be guarded, e.g. by the corresponding test.) 
Some programming languages define additional mode affinities. In this respect, 
ALGOL 68 again is extreme. 
Finally, the "classical" mode affinities should be discussed. They are contained in the 
definitions 
mode int = (nat debit, nat credit) 1 nat, 
mode rat "' (int numerator, nat denominator) I int 
or 

140 
2. Objects and Object Structures 
mode nat = {int x: x ~ 0}, 
mode rat = (int numerator, nat denominator) I int 
where the pairing represents in each case the "proper" extension of the number range. 
Note that these mode affinities now accompany quite naturally a constructive introduc-
tion of nat and rat. Details are given in Chap. 3. 
2.9 Recursive Object Structures 
As with routines, so in the case of a mode declaration we say that it is based on a mode. If 
this is the declared mode, directly or indirectly, we obtain recursive mode declarations 
(McCarthy 1961, Hoare 1970, 1973). 
2.9.1 Definition of Recursive Object Structures 
2.9.1.1 A mode declaration 
mode stri = (stria, char b) 
is comparable to a routine such as 
funct f = (int n) int: n x f(n -
1) 
in that it does not terminate. Termination can only be achieved by using variants. Thus 
mode rs x = x l<rs x trunk, x item) 
defines an object set with x as a primitive. It comprises 
x I <x. x> I «x. X), x> I <«x. X), X), x> I··· I< · · · «x. X), X).··, x> 
n 
for every n e IN. 
The termination requirement can also be interpreted to the effect that every object is 
made up of a finite number of subobjects ("finiteness of the objects", comp. 2.14). 
mode lisp x = x I (lisp X car, lisp x cdr) 
is also a recursively defined object set which includes 
xI <x. X) I «x. X), X) I <x. <x. X)) I «x. X), <x. X)) I··· I <x. <x. · · · <x. x> · · .)) 
n 
for every n e IN. 

2.9 Recursive Object Structures 
141 
:x,, again, must be considered as primitive. In LISP 22 the corresponding objects are also 
called atoms. The structural difference between rs :x. and lisp :x. is obvious. 
According to the definition :x. is a submode of rs :x,, every element of :x. corresponds to 
an element of rs:x,. It is thus permissible to write rs :x,: a if a is an object of :x,. The same 
holds for lisp :x,. 
Neither rs :x. nor lisp :x. contain the zero-fold direct product, the singleton empty which 
comprises the 0-tuple, the universal special object o. This can easily be remedied in rs :x,: 
mode rsequ :x. = empty l(rsequ :x. trunk, :x. item) 
Now, however, neither :x. nor rs :x. is a submode of rsequ :x,. Therefore is is not permissible 
to write directly 
rsequ :x.: a or rsequ :x,: rs :x.: a 
when a is an object of :x,; it should correctly be written 
rsequ :x.: ( 0, a> 
Similarly rsequ :x,: 0 denotes the typed 0-tuple of the object structure rsequ :x,. 
Of course here again we strive to write a instead of ( 0, a) and to identify (empty, :x,) 
with :x,; however, the unabbreviated, correct notation is clearer and also more suitable for 
implementation. 
or 
Similarly by replacing right by left, lsequ :x. and Is :x. are introduced: 
mode lsequ :x. = empty l<:x. item, lsequ :x. trunk) 
mode Is :x. = :x.l<:x. item, Is :x. trunk) 
A third kind of recursive object structure is shown by 
mode cs :x. = :x.l ( cs :x. left, :x. node, cs :x. right) 
mode case :x. = empty !(case :x,left, :x. node, case :x. right) 
McCarthy had already considered the first two of the three most important kinds of re-
cursive object structures in 1961. Structures of the rs and rsequ kind are often called 
"linear lists" 23 or sequences, whereas the lisp kind comprises the "list structures proper" 
which McCarthy used 24 in 1960 in the programming language LISP -
parallel to ap-
22 The relationship to the list structure used as a basis in LISP becomes clearer in the analogous form 
mode lisp = atom 1 (lisp car, lisp cdr) 
, 
which is based on a basic mode atom, in our example ()(primitive) 
mode atom = X· 
23 Knuth: "linear list". They originate from "push-down lists" (Newell, Shaw 1957) and from the 
"cellar" (Bauer, Samelson 1957, cf. Samelson, Bauer 1959). 
24 See also Bobrow and Raphael1964, not to be confused with "list structure" or "List" (with capi-
tal L) in Knuth 1973 p. 312, 406, see below. 

142 
2. Objects and Object Structures 
proaches by Newell and Simon in IPL, 1956. Structures of the cs or case kind which we 
call cascades are now usually called "binary trees". Unfortunately a "binary tree" is not a 
special case of a tree in the sense of graph theory but a finite bifurcating ("binary") arbo-
rescence which is, in addition, ordered 25• The selectors left and right are not interchange-
able. 
The difference between lisp x on the one hand and cs x or case 1. on the other hand is 
mainly that the arborescence cs 1. or case xis labelled: every (non-empty) node contains 
an element of the mode 'l· The arborescence lisp ]., on the contrary, is only leaved: only 
the terminal nodes ("leaves") comprise an element ("atom") of the mode 'l· 
Exercise I: Give a recursive definition for "ternary trees" and also for "trees" with arbitrary forking. 
Exercise 2: Represent an arbitrary (i.e. an arbitrarily forked) "tree" (finite, labelled, ordered 
arborescence) by means of lisp x. cs x. case X· 
Exercise 3: Compare the code trees for the Morse code (Fig. 2.1) and for the Huffman code (Fig. 
2.2). Which one suggests rather a structure of the lisp kind and which one rather a 
structure of the cs kind? Specify the corresponding standard representations. 
H V F U L A P J 8 X C Y Z 
Fig. 2.1. Morse code 
Fig. 2.2. Huffman code 
25 Knuth 1973, p. 309, 362. 
z 
Q 

2.9 Recursive Object Structures 
143 
2.9.1.2 The similarity of the recursive definition of sequences and of linear recursive 
routines such as fac is obvious, as is also the similarity of the recursive definition of 
cascades and of routines such as fib (1.4.2). The parallel extends further: There are also 
indirectly recursive and mutually recursive systems of mode declarations which define 
object structures. 
The following example is a system: 
mode expression = (term opdl, operator op, term opd2), 
mode term 
= id l(lbrack lb, expression subexpr, rbrack rb) 
with the primitives operator, lbrack, rbrack and id. It corresponds to the BNF grammar 
<expressiom :: = <term> <Operator> <term> 
<term> :: = <id> I <lbrack> <expressiom <rbrack> 
Further examples can be derived by suitably rewriting context-free grammars. It is 
obvious how object structures pertaining to context-free grammars (and vice versa) are 
useful in connection with recognizers and parsers for such grammars. Let us note that 
there are especially simple systems of recursive mode declarations which correspond to the 
special case of right linear regular grammars, we call them right linear systems. Trivially Is 
and lsequ fall under this heading. 
2.9.1.3 For an informal description of the form of recursively defined object structures we 
have different kinds of graph diagrams, for example the fork diagrams (Bauer 1971) 
a) for rs 1.. and rsequ 1.. (Fig. 2.3) 
Fig. 2.3 
b) for lisp 1.. (Fig. 2.4) 
: :: : : : : : : :: : 

144 
2. Objects and Object Structures 
c) for case x (Fig. 2.5) 
• • • •  0 
••• 
. . . . . . . . 
y 
Fig. 2.5 
d) for mode lsrsequ x = empty I (rsequ x r, lsrsequ x k) (Fig. 2.6). 
Fig. 2.6 
The latter is a two-step hierarchical recursion which is also determined by 
mode lsrsequ x = lsequ rsequ x 
it is the appropriate structure to be used for the representation of sequences of numbers 
written in decimal notation. There are also more complicated recursions, for example the 
following generalization of lisp x: 
mode plex x = x llsequ plex x 
(comp. Vuillemin 1975, p. 9 where a pointer implementation for the above and also a 
recognizing algorithm is suggested) or the generalization of exercise 2.9.1-1 for arbitrarily 
forked cascades. 
Related is the "List structure" (with capital L, see Knuth 1973) 
mode list x = lsequ <x I list x) 
or equivalently 
mode list x = empty I (x, list X) I (list x, list x) 
Such types of structures are frequently met in machine-oriented programming. 

2.9 Recursive Object Structures 
145 
Exercise 4: What is the difference between 
mode llst1 = atom llsequ list1 
and 
mode llst2 = lsequ (atom llist2) 
? 
2.9.2 Object Diagrams 
"Objects can also be represented in the form of 
particular trees (arborescences) or of a nesting of 
abstract 'black boxes'." 
Zemanek 1968 
2.9.2.1 For the description of objects composed according to a particular structure we 
likewise use diagrams in addition to the operational standard denotation by constructor 
brackets. These are object diagrams, either (left) graph diagrams 26 or (right) box diagrams 
(where there are more baroque and more gothic forms): 
a) for the object (( ((L, 0 ), 0 ), L ), L) of mode rs bit (Fig. 2.7) 
L 
ralo L 
L 
Fig. 2.7 
b) for essentially the same object (( ( (( O, L ), 0 ), 0 ), L ), L) of mode rsequ bit (Fig. 
2.8) 
L 
1---
~0 0 
L 
L 
Fig. 2.8 
26 "Each instance of a data structure is represented by a graph ("V-graph") constructed from atoms, 
nodes and links. Atoms represent data with no substructure, links are given labels called selectors 
and are directed" (Earley 1971). Earley even used graphs for the definition of object structures. 

146 
2. Objects and Object Structures 
c) for the object ( ((0, (L, 0 )), 0 ), (0, ( L, 0))) of mode lisp bit (Fig. 2.9) 
r---
0~ 
0 
0 
L 0 
Fig. 2.9 
d) for (( ( L, ( 0 )), 0, ( L )), L, (( ( 0 ), L ), L )), considered as an object of mode 
case bit (Fig. 2.10) 
0 
0 
Fig. 2.10 
e) for ((, ( ((, (a, +, b), ) ), x, c), ) ), considered as an object of mode term, where 
+ and x are of mode operator, 
a, band c of mode id, 
( of mode lbrack and 
) of mode rbrack (Fig. 2.11) 
a 
b 
+ 
Fig. 2.11 
2.9.2.2 Instead of graph diagrams, abbreviated diagrams (Kantorovic 1957) are also used 
(especially in structures connected with formal grammars), as in Fig. 2.12. 

2.9 Recursive Object Structures 
a 
b 
\I 
+ 
I 
( ) 
c 
\I 
X 
I 
<I) 
or rather 
upside down 27 
I 
( ) 
I 
X 1\ 
( ) 
c 
I 
+ 
1\ 
a 
b 
Fig. 2.12 
The box diagrams are related to the object diagrams to be discussed in 2.9.3. 
147 
Note that graph and box diagrams are only "extended" bracket notations, and that in 
the case of box diagrams the order of elements still has to be given proper consideration. In 
any case the right and left position in lisp and case makes a difference, an onion type "set 
interpretation" would be confusing here. In addition empty fork endings (or broken off 
fork prongs) occur in rsequ and case. Note that in the case of case it is not immaterial 
whether the right or the left prong is broken off. In any case 
L 
L 
f 
~ 
and 
T 
as "binary trees" are different. 
2.9.2.3 It is striking that McCarthy (1961) discusses the modes lsequ x and lisp x but that 
only the latter is implemented in LISP. The reason, of course, is that after excluding 0 
lsequ x as well as rsequ x can be implemented in lisp X· This is done, e.g., by means of 
the following (reversible) transfer operations: 28 
funct transfer = (lsequ x a: a * 0) lisp x: 
if trunk of a = 0 then item of a 
else (item of a, transfer (trunk of a)) fi 
funct transfer = (rsequ x a: a * 0) lisp x: 
if trunk of a = 0 then item of a 
else (transfer (trunk of a), item of a) fi 
The object rsequ nat: ( (( (( 0, 3 ), 2 ), 5 ), 7 ), 0) is mapped into the object 
lisp nat: (( ((3, 2 ), 5 ), 7 ), 0) with the graph diagram (Fig. 2.13), 
27 "In computer science, trees are not growing into heaven" (K. Samelson). See also Knuth 1973, 
p. 307. 
28 Since X is a submode of lisp x, item of a is short for lisp x: item of a (implicit widening); more-
over the constructor lisp x: (., . ) is shortened to (., . ), cf. 2.5. 

148 
2. Objects and Object Structures 
3 
2 
7 
0 
\; 
Fig. 2.13 
fF~214 
the object lsequ nat: (3, (2, (5, (7, (0, 0))))) is mapped into the object 
lisp nat: (3, (2, (5, (7, 0)))) with the graph diagram (Fig. 2.14). 
If, therefore, lisp x is implemented by pointers (7 .4), then an implementation for 
rsequ x and for lsequ x also results 29• The re-shaping of a left sequence to a right 
sequence by means of "re-bracketing", as above (see also 2.11.1) requires a fundamental 
structural change of the objects implemented as lisp X· 
rsequ x and lsequ x can also be implemented using case X· A transfer function would 
be 
funct transfer = (rsequ x a) case x: 
if a= 0 then 0 
else (transfer(trunk of a), item of a, 0) fi 
The above object of the mode rsequ nat is now mapped into the object of the mode case 
nat with the diagram (Fig. 2.15). Obviously lisp x can be realized in terms of case x by 
extending (lisp x car, lisp x cdr) to a 3-pronged fork with an insignificant node element 
and extending x to a 3-pronged fork with empty left and right components. 
Applied to the object lisp nat: (( ((3, 2}, 5 }, 7 }, 0}, this yields the following diagram 
(Fig. 2.16) (with ro as an insignificant element). For a joint implementation of rsequ x and 
lsequ x by means of case x see 2.14. 
3 
3 
2 
2 
5 
Fig. 2.15 
Fig. 2.16 
2.9.2.4 The examples have so far shown (recursive) object structures with undiscriminated 
variants. Discrimination can be introduced here by including at least one additional 
component as a discriminator. For rs x, for example, there is the discriminated implemen-
tation 
mode rsd x = (bool {false} tag, x item) l(bool {true} tag, rsd x trunk, x item) 
29 Still more general than lisp X is list x. see 7.6.3. 

2.9 Recursive Object Structures 
149 
For the object 30(T, (T, (T, (T, (F, L ), 0 ), 0 ), L ), L) of mode rsd bitthe diagrams of 
Fig. 2.17 result. 
In the box diagrams special places are allocated to the discriminators in order to 
indicate the relationship of the remaining components, e.g. in the form of Fig. 2.18. 
F 
L 
TT 
T T~~l 
L 0 0 L L 
L 
Fig. 2.17 
T 
T 
T 
T 
T 
or 
T 
r----
T 
T 
lJo 0 
L L 
~ 
0 
L L 
Fig. 2.18 
lisp x is similarly treated. The following is a discriminated implementation: 
mode llspd x = (bool {false} tag, x atom) 1 
(bool {true} tag, lispd x car, lispd x cdr) 
For example for the object 
(T, (T, (T, (F, 0), (T, (F, L), (F, L))), (F, 0)), (T, (F, 0), (T, (F, L), (F, 0)))) 
of mode lispd bit we have the box diagram of Fig. 2.19. 
T 
T 
T 
T 
F F 
T 
-,--
F 
T 
F F 
w. 
0 
L I L 0 
0 
L 0 
Fig. 2.19 
The system expression, term, too, can be treated thus: term is changed to termd, 
mode termd = (bool {false} tag, ld iden) 1 
(boo! {true} tag, lbrack lb, expression subexpr, rbrack rb) 
30 T, F stand for true or false (see Table 1.3.1). 

150 
2. Objects and Object Structures 
The Figs. 2.20 and 2.21 show examples of some box diagrams (comp. example e) above); 
for illustration the Kantorovic diagrams are given, too. 
a 
b 
\I 
+ I 
( ) 
c 
\I 
T 
T 
F 
X 
I 
) 
X 
C 
(I) 
Fig. 2.20 
a 
b 
\I 
+ 
I 
a 
( ) 
\I 
I 
T 
T 
F 
-
I 
( ) 
b 
\I 
X 
F 
T 
I--
(~+rrL 
( 
( 
a 
I 
) 
X 
b 
) 
I 
( ) 
Fig. 2.21 
I 
In analogy to rsd 1. we have 
mode rsequd 1. = (bool {false} tag) 1 
(boo I {true} tag, rsequd 1. trunk, 1. item) 
as a discriminated implementation of rsequ X· This special case corresponds to the 
construction aimed at by Wirth 1976 in (4.10). 
For the representation of the objects composed according to this structure, e.g. for the 
object 
(~ (~ (T, (T, (~ (F), L), 0), 0), L), L) 
with the abbreviation (F) for (F, 0) the diagrams of Fig. 2.22 result. 
F 
Fig. 2.22 
T T 
T 
T 
T l:l L 0 
0 
L 
L 

2.9 Recursive Object Structures 
151 
Similarly 
mode cased x = (bool {false} tag) I 
(bool {true} tag, cased x left, x node, cased x right) 
is a discriminated implementation of case X· 
The object corresponding to the object of example (d) above is 
(T, (T, (T, (F), l, (T, (F), 0, (F))), 0, (T, (F), L, (F))), 
L, (T, (T, (T, (F), 0, (F)), L, (F)), L, (F))) 
2.9.2.5 Of course the discriminators do not have to be of mode bool. The length is a useful 
information about sequences, and with its help discrimination can likewise be performed, 
e.g. in the following implementation of rsequ x: 
mode rsequc x =(nat {0} length) l<{nat i: i > 0} length, rsequc x trunk, x item) 
where the object 
(5, (4, (3, (2, (1, (0), L), 0), 0), L), L) 
corresponds to the above example of an object of mode rsequd bit; Fig. 2.23 shows the 
corresponding box diagram. 
5 4 3 2 
l fl L 0 
0 
L 
L 
Fig. 2.23 
In this example objects x of the two variants are still discriminated by the two cases 
length of x = 0 and length of x > 0. However, if there are more than two variants, then 
one boolean discriminator is, of course, no longer sufficient and we must introduce either 
more boolean discriminators or a discriminator of some other mode (with a suitable num-
ber of objects). 
2.9.3 Operational Detailing of Objects 
"Program structure and data structure merge in our 
view." 
Friedman, Wise 1978 
The diagrams used in the preceding paragraphs suggest writing down detailed construction 
of composite objects. As a matter of fact an expression such as 

152 
2. Objects and Object Structures 
(24, (6, (2, (1' 0)))) 
which yields a left sequence can be structured both outwardly (by embedding) and inward-
ly (by abstraction). When auxiliary identifiers are introduced for intermediate objects, 
complete object embedding yieldsJ1 
llsequ nat a5 = O; 
lsequ nat a4 = (1, a5 ); 
lsequ nat aJ = (2, a4 ); 
lsequ nat a2 = (6, aJ ); 
lsequ nat a1 = (24, a2 ); 
a, 
J 
complete abstraction 
(with the introduction of auxiliary functions 
with suppressed parameters) yields 
I a1 where 
funct a1 = lsequ nat: (24, a2), 
funct a2 = lsequ nat: (6, aJ), 
funct aJ = lsequ nat: (2, a4), 
funct a4 = lsequ nat: (1, a5 ), 
funct a5 = lsequ nat: 0 
J 
Fig. 2.24 
Both forms can be illustrated by one and the same object diagram. With a view to later use 
in 7 .4, we introduce another form of the diagram (Fig. 2.24) where the double-lined arrows 
indicate substitution. 
For the object 
(((0, (l, 0)), 0), (0, (l, 0))) 
of mode lisp bit considered in 2.9.1 c) the completely detailed form with object diagram 
(Fig. 2.25) reads 
I a0 where 
funct a0 = lisp bit: (a1, a2 ), 
funct a1 "" lisp bit: (aJ, 0), 
funct a2 = lisp bit: (0, a4 ), 
funct aJ =lisp bit: (0, a5 ), 
funct a4 = lisp bit: (l, 0), 
funct a5 =lisp bit: (l, 0) J 
31 As an abbreviation (1.13 .3 .2) for 
functj1 = (lsequ nat a5) lsequ nat: (1, a5), 
funct ! 2 = (lsequ nat a4) lsequ nat: (2, a4 ), 
functjJ = (lsequ nat aJ) lsequ nat: (6, aJ), 
functj4 = (lsequ nat a~ lsequ nat: (24, a2> 
withinf4ifJ(h(f1 ( ¢)))) 
~, 
(0,~.) 
a4 
(L,O) 
Fig. 2.25 

2.9 Recursive Object Structures 
153 
Note that the individual routines of the system a0 •• as have no priority among one an-
other: the notational order is independent of the calling structure. In the case of detailing 
by embedding we must specify a certain order (with the exception of possible collective 
object declarations), e.g. 
jlisp bit as = (l, 0); 
(lisp bit a4, lisp bit a3) = ( (l, 0 ), (0, as)); 
(lisp bit a2, lisp bit a1) = ( (0, a4 ), (a3, 0)); 
lisp bit a0 = (a1, a2 ); 
or also 
I (lisp bit as, lisp bit a4) = ( (l, 0 ), (l, 0 )); 
(lisp bit a3, lisp bit a2) = ((0, as), (0, a4)); 
lisp bit a1 = (a3, 0); 
lisp bit a0 = (a1, a2 ); 
J 
J 
In the sequel we will only use detailing by abstraction. 
According to definition, components of object diagrams can be actually substituted 
(along the direction of the arrows), and so we obtain incompletely detailed object diagrams 
(Fig. 2.26). 
a~ 
(24, (6,.)) 
a~ 
(2, ( 1 ,<>) ) 
or 
a~ 
·2 ·~:2 
(0, (L, 0)) 
(L, 0) 
) ) 
Fig. 2.26 
Substitution amounts to the elimination of auxiliary functions from the system. By 
complete substitution we obtain the standard representation again. 
Common subexpressions need only one abbreviating routine and are thus of multiple 
use in the object diagram (common subobjects), e.g. in the above example (as = a4, a3 = 
az) 
I a0 where 
funct a0 = lisp bit: (a1, a2), 
funct a1 = lisp bit: (a2, 0), 
funct a2 = lisp bit: (0, a4 ), 
funct a4 = lisp bit: (l, 0) J 
a~ 
·~''-
(~d 
a4(". -n\ 
Fig. 2.27 
( L, 0) 
This is for the moment the principal advantage of the detailing of objects. 

154 
2. Objects and Object Structures 
2.10 Algorithms with Linear Object Structures 
2.10.1 The recursive structures lsequ x or rsequ x are prototypes of linear object struc-
tures. Here are some examples to show the use of such objects. A basic form of construc-
tion of such object structures is the construction of tables (we will deal with construction 
by way of sorting later). The scheme 
funct tab = (nat n, funct (nat) x F) rsequ x: 
if n = 0 then 0 
else (tab(n - 1, F), F(n)) fi 
for tabulating the values F(i), i = 1, ... , n, of a function F: nat ..... xis typical for table 
construction. Frequently, however, the value of the preceding element is used for comput-
ing a new element ("two-term recurrence"). This slight generalization yields (with the 
initial value F 1) 
funct tabrec = (nat n, funct (nat, X) x G, x FJ) rsequ x: 
if n = 0 then 0 
D n = 1 then ( 0. Fl ) 
D n ~ 2 then f rsequ X h = tabrec(n - 1, G, Fl) within 
(h, G(n, item of h)) 
J fi 
IfF occurring in tab is recursive with the special form 
funct F = (nat n) x: if n = 1 then FJ 
else G(n, F(n -
1)) fi 
then tab can be transformed to tabrec. 
Example: Tabulate i! for i = 1, ... , n. 
We have 
functjac = (nat i) nat: 
if i = 1 then 1 
else i x jac(i -
1) fi 
Thus 
G(n, u) = n x u 
Fl 
= 1 
and therefore 
funct tabjac = (nat n) rsequ nat: 
if n = 0 then 0 
D n = 1 then ( 0, 1) 
D n ~ 2 then I rsequ nat h = tabfac(n -
1) within 
(h, n x item of h) 
J fi 

2.10 Algorithms with Linear Object Structures 
155 
Exercise 1: Extend the scheme Iabree to the case of a "three-term recurrence". 
tab and tabree can be transformed into an even simpler, "repetitive" form by what is 
called "function inversion" (see Chap. 4). The same holds for some of the algorithms yet 
to be discussed. 
2.10.2 Let us now rewrite the examples (e) and (f) from 1.4.1 which contain explicitly 
specified object structures. The left sequence lsequ bit can be used for sequ bool. Then 
pos of the following system yields the "parity" bit: 
funct pos "" (lsequ bit a) bit: 
if a = 0 then 0 
else if item of a = 0 then pos(trunk of a) 
U item of a = L then neg(trunk of a) fi fi, 
funct neg a (lsequ bit a) bit: 
If a = 0 then L 
else if item of a = 0 then neg(trunk of a) 
U item of a = L then pos(trunk of a) fi fi 
The main difference with respect to 1.4.1 (e) is that the primitives top and rest are now 
implemented by the selector functions item of, trunk of of the concrete object structure 
lsequ bit. The comparison with the universal 0-tuple 0, and the object set bit remain 
primitive. 
We observe that partially defined operations such as top and rest occur naturally only 
in branchings which guarantee their applicability. 
Equality of two (left) sequences can be reduced to componentwise coincidence, 
funct equ "" (lsequ x a, lsequ x b) boo I: 
if a =1= 0 A b =1= 0 then item of a = item of b 11. equ(trunk of a, trunk of b) 
else a = 0 A b = 0 
fi . 
Note that the use of the conditional And instead of the common And makes the routine 
repetitive and considerably more efficient. 
Concatenation of two left sequences is harder. It can only be done by successive left-
annexing of the components of the left-hand side sequence (comp. 1.4.1(f)): 
funct leone a (lsequ x a, lsequ x b) lsequ x: 
if a = 0 then b 
else (item of a, /eone(trunk of a, b)) fi 
Analogously for right sequences 
funct reone a (rsequ x a, rsequ x b) rsequ x: 
If b = 0 then a 
else (reone(a, trunk of b), item of b ) fi 

156 
2. Objects and Object Structures 
Note that for x of mode "/.. 
lconc(lsequ x: (x, 0), b) = lsequ x: (x, b) and 
rconc(a, rsequ x: ( 0. x)) = rsequ x: (a, x) 
To annex an element at the left side of a left sequence is of course trivial (see append 
below). The task of annexing an element at the right side of a left sequence is quite dif-
ferent. This is done by means of an embedding in leone, 
funct stock = (lsequ "/..a, "/.. x) lsequ x: lconc(a, lsequ x: (x, 0 )) 
Exercise 2: Trace the execution of stock(OLLO, L). 
A further example is the task of "reversing" a left sequence, illustrated by the defini-
tion 
funct reverse = (lsequ "/..a) lsequ x: 
if a = 0 then a 
else stock(reverse(trunk of a), item of a) fi 
It is possible to embed it in the task of "appending" the reversed left sequence on the 
left side of another left sequence; we have a more efficient repetitive solution 
funct reverse = (lsequ "/..a) lsequ x: 
I F(a, 0) where 
funct F = (lsequ x a, lsequ x b) lsequ x: 
if a = 0 then b 
else F(trunk of a, (item of a, b)) fl J 
Exercise 3: Trace the execution of reverse(LOOO). 
Exercise 4: Show formally that lconc(reverse(a), a) is a palindrome, i.e. reverse(lconc(reverse(a), a)) 
= lconc(reverse(a), a). 
Exercise 5: In order to determine if lconc(reverse(b), a) is a palindrome of even length, the following 
routines are proposed 
funct pall = (lsequ 'X a, lsequ 'X b) boo I: 
(equ(a, b) 0 If a * 0 then pall(trunk of a, (item of a, b)) 
else b = 0 
fi) 
funct pal2 = (lsequ 'X a, lsequ 'X b) boo I: 
(equ(a, b) " if a * 0 then pal2 (trunk of a, (item of a, b)) 
else b = 0 
fi) 
What is their course of execution? Do they solve the problem? 
The operation 
funct length = (lsequ x a) nat: 
if a = 0 then 0 else length(trunk of a) + 1 fl 
is useful in many applications. 

2.10 Algorithms with Linear Object Structures 
157 
2.10.3 The most "remote" element of a left sequence a can be found by means of item of 
reverse(a), more efficiently by 
funct bottom = (lsequ x a: a =I= 0) x: 
if trunk of a = 0 then item of a 
else bottom (trunk of a) fi 
This element is obviously deleted by the routine 
funct upper = (lsequ x a: a =I= 0) lsequ x: 
reverse(trunk of reverse(a)) 
The following is more efficient: 
funct upper = (lsequ x a: a =1= 0) lsequ x: 
if trunk of a = 0 then 0 
else (item of a, upper (trunk of a)) fi 
Exercise 6: Derive bottom from bottom( a) = item of reverse( a) by unfolding and folding. 
Exercise 7: Derive the latter version of upper from upper(a} = 1lsequ xx: stock(x, bottom(a)) =a. 
Left sequences by themselves (right sequences, too) have thus unsymmetrical "access" 
properties: appending, inspection and deletion is only immediately possible on one side; on 
the other side it requires more complicated manipulations. Objects with this type of access 
(irrespective of whether they are right or left sequences) are called stacks. Other terms 
frequently used are push-down list or LIFO ("last-in-first-out") list. As distinguished from 
the uniform access behaviour discussed in 2.6 we call this a structure with strictly 
sequential access. Technical equipment very often has only this kind of limited access. We 
will meet an abstract definition in Chap. 3. 
It is in the nature of the recursive definition that we no longer have an (elementary) 
selector for every component -
corresponding to the fact that we can have an unlimited 
number of components but have a fixed number of elementary selectors. The composite 
selectors (see 2.6) which point to the indirectly accessible components are also called 
iterated selectors. 
Note that 
(A") a = 0 '!I stock(upper(a), bottom(a)) = a 
(R") upper(stock(a, x)) = a 
(T") bottom(stock(a, x)) = x 
holds analogously to 
(A') a = 0 '!I append(rest(a), top(a)) = a 
(R') rest(append(a, x)) = a 
(T') top(append(a, x)) = x 
where top, rest and append are defined by 

158 
2. Objects and Object Structures 
funct top 
= (lsequ x a: a =1= 0) x: item of a 
funct rest 
= (lsequ x a: a =1= 0) lsequ x: trunk of a 
funct append = (lsequ x a, x x) lsequ x: (x, a) 
These functions are mere abbreviations. In Chap. 3, full symmetry of access will be 
established. The consequences for implementations by arrays and for pointer implementa-
tions will be discussed in Chap. 7. 
2.10.4 For the task of deciding if a left sequence a contains an element x, 
funct contains = (lsequ x a, x x) bool: 
3 (lsequ xu, lsequ X v): a = /conc(append(u, x), v) 
we have the repetitive solution 
funct contains = (lsequ x a, x x) bool: 
if a = 0 then false 
else if top(a) = x then true 
else contains(rest(a), x) fi fi 
Another important task is scanning the elements of a stack for a property p to produce 
a stack of /-images of all elements found ("linear search"): 
funct scan = (lsequ x a, funct (x) boo I p, funct (X) Jl/) lsequ 11: 
if a= 0 then a 
else if p(top(a)) then append(scan(rest(a), p, f),f(top(a))) 
else scan (rest(a), p, f) 
fi fi 
A special case of this is the sieve algorithm (compl. multsieve in 1.14) 
funct sieve = (lsequ x a, funct(x) bool p) lsequ x: 
if a = 0 then a 
else if p(top(a)) then append(sieve(rest(a), p), top(a)) 
else sieve(rest(a), p) 
fi fi 
It is more complicated to replace (every occurrence of) an element x of a stack a by a 
single element or even a stack b: 
functrep/ace = (lsequ xa, XX, lsequ X b) lsequ x: 
if a= 0 then a 
else if top(a) = xthen lconc(b, replace(rest(a),x, b)) 
else append(replace(rest(a),x, b), top(a)) fi fi. 
A related operation is the deletion of (all occurrences of) a certain element from a left 
sequence, it is obtained from 
funct deletea/1 = (lsequ x a, x x) lsequ x: replace(a, x, 0) 

2.10 Algorithms with Linear Object Structures 
by partial computation (bin replace is a fixed parameter!) 
funct deleteall = (lsequ xa, xx) lsequ x: 
if a= Othena 
else if top(a) = x then deleteall(rest(a), x) 
else append(deleteall(rest(a),x), top(a)) fi fi 
159 
In contrast to this, for the deletion of the first occurrence (from the top) of an element, 
there is the routine 
funct delete = (lsequ x a, x x) lsequ x: 
if a = 0 then a 
else If top(a) = x then rest(a) 
else append(delete(rest(a), x), top(a)) fi fi 
If only the !-image of the first element found is required, scan can be simplified to 
functsearch "'(lsequ xa, funct (X} bOOip, funct (X) Xf: 3 XX: {p(x) 1\COntains(a, X))) 11: 
if p(top(a)) thenj(top(a)) 
else search(rest(a), p, f) fl 
Exercise 8: Give a modification of replace, which on the assumption contains( a, x) replaces some 
occurrence of x in a by b. 
Finally we must discuss the insertion of an element at its proper place. We assume that 
a left sequence (lsequ X) is already sorted (in increasing order) with respect to a criterion 
m: x-+ int, i.e. for all subsequences a with rest(a) =1= 0 
m(top(a)) ~ m(top(rest(a))); 
then the element xis to be inserted properly ("linear sorting", comp. 1.13.3): 
funct sort = (lsequ x a, x x, funct (X) int m) lsequ x: 
if a = 0 " m(top(a)) ~ m(x) 
then append(a, x) 
else append(sort(rest(a), x, m), top(a)) fi 
The task of decomposing a left sequence whilst simultaneously constructing one which 
is sorted is based on the above algorithm: 
funct tabsort = (lsequ x b, funct (x) int m) lsequ x: 
tabsortl ( b, m, 0) , 
funct tabsortl = (lsequ x b, funct (X) int m, lsequ x c) lsequ x: 
if b = 0 then c 
else tabsortl (rest(b), m, sort(c, top(b), m)) fi 
tabsortl ("sorting by direct insertion") solves the more general problem of successively 
sorting the elements of b into the sorted sequence c. The task of tabsort and tabsortl 
occurs during the transmission of sports events such as straight downhill racing or ski 

160 
2. Objects and Object Structures 
jumping, where b is the sequence of incoming results which have to be repeatedly inserted 
in the proper order. 
Let us finally discuss a complete example from Wirth 1976: setting up a frequency 
index. The essential part of the program 4.1 there reads in our abstract from 
mode w = <x key, nat numb), 
functjrequ = (lsequ x b) lsequ w: insp(b, 0), 
funct insp = (lsequ x b, lsequ w r) lsequ w: 
if b = 0 then r 
else insp(rest(b), count(top(b), r)) fi, 
funct count = (X x, lsequ w r) lsequ w: 
if r = 0 then append( 0, (x, 1)) 
0 r =I= 0 A key of top(r) = x 
then append(rest(r), (x, numb of top(r) + 1 )) 
0 r =I= 0 A key of top(r) =1= x 
then append(count(x, rest(r)), top(r)) 
fi 
For a stack b of objects of mode x, frequ yields a stack of objects of mode w, which repre-
sents a frequency table: every object of mode x contained in b occurs exactly once as a left 
element of the pair. The table, therefore, describes a function. 
This example shows a two-level object structure which is frequently met in applica-
tions: Stacks of pairs provide correspondence tables (and describe relations) in full 
generality. Stacks of arrays are important, too, for the description of general relations. 
2.11 The Recursive Object Structure "File" 
The presumably most important class of linear object structures apart from stacks is char-
acterized by permitting access "somewhere in the middle". 
2.11.1 "Knitting" of Sequences 
Let us first discuss the interplay of right and left sequences. The concatenation of a right 
sequence with a left sequence (resulting in a left sequence) is simple: 
funct concatl = (rsequ x a, lsequ x b) lsequ x: 
if a = 0 then b 
else concatl(trunk of a, (item of a, b)) fi 
This operation is well known from knitting. 
An important task is to change a right sequence into a left sequence (and vice versa) by 
means of "re-bracketing". We obtain a routine to perform this task by embedding: 

2.11 The Recursive Object Structure "File" 
funct make/sequl = (rsequ l a) lsequ x: concatl(a, 0) 
Switching sides we have correspondingly 
funct concatr = (rsequ l a, lsequ l b) rsequ x: 
if b = 0 then a 
else concatr( (a, item of b), trunk of b) fi 
funct makersequl = (lsequ l a) rsequ x: concatr( 0, a) 
Thus new possibilities arise of describing access at the "wrong" end: 
for lsequ l a and l x (comp. 2.10.1) 
bottom(a) 
upper( a) 
stock(a, x) 
corresponds to 
corresponds to 
corresponds to 
item of makersequl(a) 
make/sequl (trunk of makersequl (a)) 
makelsequl (makersequl (a), x) 
(analoguous for rsequ l a). Access does not however become more efficient. 
161 
The conversion routine which retains the "inner structure" (order of access) instead of 
the "outer structure" (notational order) is, of course, simpler: 
funct readlsequ = (rsequ l a) lsequ x: 
if a = 0 then a 
else (item of a, read/sequ(trunk of a)) fi 
The notational order, however, is "reverted". This can be compensated for and we obtain 
in place of makelsequl 
funct makelsequ2a = (rsequ l a) lsequ x: readlsequ(rreverse(a)) 
or 
funct makelsequ2b = (rsequ l a) lsequ x: reverse(read/sequ(a)) 
where rreverse is defined analogously to reverse in 2.1 0.1. These versions are considered to 
be operatively less efficient than makelsequl because of the twofold recursion over the 
whole sequence. A version based on stock (see 2.10.1), in particular, is operatively less effi-
cient than makelsequl, 
funct makelsequ3 = (rsequ l a) lsequ x: 
if a= 0 then 0 
else stock(makelsequ3(trunk of a), item of a) fl 
2.11.2 Files 
The simplicity of the algorithms concatr and concatl for "knitting" a right and a left 
sequence together suggests considering a pair of right and left sequences over the same ob-
ject set l as a new object: 

162 
2. Objects and Object Structures 
mode file 'X = (rsequ 'X I, lsequ 'X r) 
This can be illustrated as a file or a folder or a book (Fig. 2.28). Such a structure is 
called a file. The leaves are "turned over" in a forward direction 
file 
folder 
book 
Fig. 2.28 
funct advance = (file 'X m: r of m =1= 0) file x: 
(rsequ x: (/of m, item of r of m), trunk of r of m) 
and backwards 
funct recede = (file 'X m: I of m =1= 0) file x: 
(trunk of I of m, lsequ x: 1/Vem of I of m, r of m)) 
These typical operations have the property that (e.g.) 
concatr(l of m, r of m) 
remains invariant. 
Thus if 
funct close = (file 'X m) file x: 
(concatr(l of m, r of m), 0) 
and 
funct open = (file 'X m) file x: 
( 0, concatl(l of m, r of m)) 
are introduced, a file is put into one of the two particular forms: "the book is shut". 
"Appending" at the end can be done by 
funct append = (file 'X m, 'X x) file x: (/of c/ose(m), lsequ x: (x, 0)) 
file: ( O, 0) denotes the "empty file", of course. 
The operations search, delete, and insert can be described very simply using files. For 
"tearing out a page" we have e.g. 
funct extract = (file 'X m)(x., file x): 
(item of r of m, (/of m, trunk of r of m)) 

2.12 Algorithms with Cascade-Type Object Structures 
163 
Exercise 1: Transfer the routines contains, replace, delete analogously to files. 
It is sometimes more convenient to introduce a mode roll (Fig. 2.29), 
mode roll x = (rsequ x l, xjoint, lsequ x r) 
where the joint component is at the "distinguished access position". 
roll, scroll 
Fig. 2.29 
Exercise 2: Transfer the above considerations analogously to operations on rolls. 
Remark: We could just as well have introduced a file as a pair of stacks with suitably re-
written basic operations. The way we proceed seems "intuitively" to be more to the point. 
Comp. 3.4.4. 
Exercise 3: Which basic operations would be introduced for an array of stacks ('block structure')? 
Files and rolls are prototypes of so-called sequential files. The access method for objects of the file 
kind is somewhat more flexible than that for a stack. However, here too we speak of structures with 
strictly sequential access, and include right and left sequences as borderline cases of files. The access 
method for the objects represented on some technical apparatus corresponds to that of files. This is 
true especially for input-output media which represent merely "variables" for objects of the file type 
(see 5.5). There are frequently restrictions, e.g. that only advance and open are permitted and thus 
append (m, x) is permitted only if r of m = ¢, i.e. if close (m) = m. 
Exercise 4: Give a formal proof for the idempotency of open. 
2.12 Algorithms with Cascade-Type Object Structures 
"I find it difficult to believe that whenever I see a tree 
I am really seeing a string of symbols." 
J. McCarthy 
The simplest operations on an object of mode lisp x (see 2.9.1) are the selector functions. 
However, they are only partially defined and (just as for sequences such as rs x and Is X) 
they need the test operator x:: in order to guarantee definedness. In many cases, however, 
safe use of selector functions is guaranteed in a natural way by means of guards. Otherwise 
we must resort to explicit discriminators. 
Further basic operations for objects of mode lisp x are "concatenating" two such lisp 
structures A, B to form a single structure and "appending" a lisp structure A to another 
structure B (which of course can happen at different points). In the first case we simply use 
the constructor: 

164 
2. Objects and Object Structures 
funct cons = (lisp :x. A, lisp :x. B) lisp :x.: (A, B) 
If A is the implementation of a left sequence and B the implementation of a right 
sequence, then cons(B, A) results in the implementation of a file (Fig. 2.30). 
cons 
A 
/ 
' ;:\ 
4 
8 
7 
4 
In the latter case we have the routine 
8 
funct rconc = (lisp :x. A, lisp :x. B) lisp :x.: 
if 
:X.:: B then (A, B) 
7 
Fig. 2.30 
D ..., :x. :: B then (rconc(A, car of B), cdr of B) fi 
The use of the identifier rconc is fully justified because the result is again the implementa-
tion of a right sequence if A and Bare implementations of right sequences (comp. 2.10.1). 
Two examples should illustrate the effect of rconc (Fig. 2.31). Intuitively rconc replaces 
the farthest "outside left" leaf x of B by cons(A, x). 
Fig. 2.31 
£." 
' \ 
3 
2 
[\ 
!;"' 
\ 
5 
3 
2 
2 
35 
8 
2 
35 
8 

2.12 Algorithms with Cascade-Type Object Structures 
165 
Switching sides, corresponding to left sequences we have 
funct leone = (lisp x A, lisp x B) lisp x: 
if 
X :: A then (A, B) 
0 ..., x ::A then (car of A, leone( cdr of A, B)) fi 
The essential feature of both algorithms is the branching based on the test operator x::. 
Note that a distinction between x and lisp xis not sufficient because an object of the mode 
x can be widened to an object of mode lisp x (comp. 2.8). If A is a leaf and if the second 
branch were not blocked, then car of A and cdr of A would be undefined here. 
The task above was greatly simplified due to appending at a very special point. How-
ever, in the general case it has to be decided whether a given element x happens to be a leaf. 
This results in searching through the whole structure. Thus, we have for example the 
recognition algorithm with cascade-type recursion 
funct contains = (lisp x A, x x) bool: 
if X :: A then A = x 
else contains(car of A, x) v contains(cdr of A, x) fi 
The task to replace some element x wherever it occurs in A by some lisp structure B is per-
formed by the algorithm 
funct replace = (lisp x A, x x, lisp x B) lisp x: 
if 
X :: A then if A = x then B 
0 A =I= X then A fi 
0 ..., x ::A then (replace(car of A, x, B), replace(cdr of A, x, B)) fi 
Other basic problems corresponding to those of 2.10 also lead to cascade-type recur-
sion. This situation is typical not only for lisp x structures, it also holds for case x and 
more complicated structures like list x; in general non-linear recursion is to be expected 
for all recursive object structures which are not linear recursive. Such algorithms with a 
recursive structure reflecting the object structure are more transparent than the usual back-
tracking algorithms, but less efficient. For "binary trees" the backtracking method 
degenerates into the traversal method to be discussed in 2.13. 
If one replaces in contains the Or by a choice, 
... else (contains(car of A, x) 0 contains(cdr of A, x)) fi 
one obtains an algorithm with linear recursion, which, however, is not only nondeter-
ministic but also nondeterminate. The algorithm is able to deliver true, if xis a leaf in A -
this happens if and only if the demon is in a good temper. For the connection of such 
algorithms (see also the nondeterministic recognition algorithm pall for palindromes in 
2.10-5) with nondeterministic automata see 6.7.1. 

166 
2. Objects and Object Structures 
2.13 Traversal and Scanning of Recursive Object Structures 
2.13.1 Traversal of the whole structure in the case of linear recursive structures - e.g. to 
test the elements for a property ("scanning") or to count them - is trivial. Every element 
can be ultimately reached by iterating trunk of and then using item of. This simple 
"reading" does not work for "branched" structures. 
The problem of systematically visiting all atoms, one by one, in an object of mode 
lisp 1. or all nodes in an object of mode case 1. ("tree" traversal) proves to be fundamen-
tal. Ultimately this means "linearizing" such objects, i.e. turning them into a stack. As, 
however, every atom and every node is determined by a composite selector, the problem 
amounts to the linear ordering of sets of composite selectors. The iterated application of 
the successor function defined by this to the first selector in this ordering exhausts all the 
possibilities. 
A linear ordering of the composite selectors is given immediately by a lexicographical 
ordering, provided a linear ordering is given for the individual elementary selectors. There 
are two possibilities for lisp x: the orders 
car, cdr and cdr, car 
Accordingly two traversal paths result. An example is given by Fig. 2.32 (where the num-
bers indicate the order of traversal). 
5 
5 
7 
8 
17 
18 
18 
17 
8 
7 
Fig. 2.32 
There are six possibilities for case 1. or cs 1. which can be divided into three classes: 
a) the order node, left, right or node, right, left ("prefix ordering"), 
b) the order left, node, right or right, node, left ("infix ordering"), 
c) the order left, right, node or right, left, node ("postfix ordering"). 
The above names orginate from the well-known methods of linearly representing 
Kantorovic trees for binary operations. For example the cascade (in abbreviated notation) 
X 1\ 
+ /\ /\ 
a 
I 
d 
X 
/\ 
b 
c 1\ 
e 
f 
yields the nodes in the order 

2.13 Traversal and Scanning of Recursive Object Structures 
a) X 
b) a 
+ 
+ 
a 
b 
I 
I 
b (bracket-free 
c prefix notation 32) 
c (infix 
~ notation 33) 
d 
X 
e 
e 
X 
f 
f 
a) according to the prefix ordering node, left, right, 
b) according to the infix ordering left, node, right, or 
c) according to the postfix ordering left, right, node. 
c) a 
b 
c 
I 
+ 
d (bracket-free 
postfix notation 34) 
e 
f 
X 
X 
167 
2.13.2 In the sequel we will specify some algorithms (traversing algorithms) for traversal of 
non-linear object structures. For that purpose we will specify the corresponding transfor-
mation into a stack and thus reduce the problem to the sequential "reading" of this stack. 
For lisp structures we have according to the order car, cdr (for leone see 2.10): 
funct traverse/is! =(lisp zA) lsequ z: 
If l ::A then (A, 0 > 
else /conc(traverse/ist(car of A), traverselist(cdr of A)) fi 
and for cascades according to the prefix ordering node, left, right: 
funct traversetree = (case l A) lsequ x: 
if A = 0 then 0 
else append (leone (traverse tree (left of A), traverse tree (right of A)), 
node of A) fi . 
The postfix ordering left, right, node results if stock is used instead of append, it is also 
produced when the prefix ordering node, right, left is first established and then the result is 
reversed. In the case of (frequent) search processes in cascades, it is most efficient first to 
transform the cascade into a stack and then to carry out the search process in the stack 
(discussed in 2.10), e.g., 
contains(traversetree(A), x) 
For computation, use of lazy evaluation (1.14.3) is advisable: It will save effort by travers-
ing only the needed part of the tree. (Note that this observation on the benefit of lazy 
evaluation applies to all cases of function composition.) 
32 Also called "Polish Notation" in honour of Jan Lukasiewicz who introduced it in 1929 
("Elementy logiki matematycznej", seel:.ukasiewicz 1963). 
33 Note that this notation does not uniquely permit the reconstruction of brackets: ((a + b)/c) x 
((d -e) x f) yields a different Kantorovic tree but the same traversal order. 
34 Misleadingly called "reverse Polish notation". 

168 
2. Objects and Object Structures 
Finally we discuss the construction of a cascade from a stack, that is the reverse of the 
task above. The (unique) reverse of the infix ordering does not exist (see footnote 33). 
Reversing the prefix and the postfix ordering is possible if the terminal nodes ("leaves") 
can be distinguished from the other nodes. In general a stack can be transformed into a 
cascade in different ways, so that the stack can be regained e.g. by prefix ordering (Fig. 
2.33). 
a 
I 
a 
b 
a 
/\ 
I 
1\ 
b 
e 
c 
b 
c 
1\ 
1\ 
/\ 
c 
d 
d 
e 
d 
e 
For the reversal there is the problem specification 
funct buildtree = (lsequ x a) case x: 
11 case x x: traversetree(x) = a 
The following is a solution as can be verified 
funct buildtree = (lsequ x a) case x: 
If a = 0 then 0 
Fig. 2.33 
else (lsequ xu, lsequ x v) = part(rest(a)); 
(buildtree(u), top(a), buildtree(v)) 
fi 
where part (comp. 1.12) is defined as 
functpart = (lsequ X a)(lsequ x, lsequ x): 
11 (lsequ xu, lsequ x v): lconc(u, v) = a 
For a special implementation we may require ( comp. also 1.13 .4) the resulting cascades 
to be balanced ("balanced binary tree", Adelson-Velskii, Landis 1962, Bayer 1971), i.e. 
for every node the number of nodes in the right subcascades should be the same as, or at 
most one less than, the number in the left one. That means that 
0 ~ length(u) - length(v) ~ 1 
is introduced in part as an additional condition. 
Stacks represented by balanced cascades have the advantage of having the shortest 
access to all elements, at the cost of a complicated object structure. 
Exercise 1: Specify a routine which is an implementation of part. 
A general treatment of algorithms whose structure follows the inner form of a certain 
object structure has been discussed by von Henke 1975. 

2.14 Infinite Objects 
169 
2.14 Infinite Objects 
"The user is free to perceive the infinite structure as 
an infinite graph if he wishes." 
Friedman, Wise 1978 
To be more precise, the finiteness requirement ("finiteness of the objects") mentioned in 
2.9 corresponds to the property that for each composite object every sequence of (permis-
sible) selectors ultimately terminates, i.e. leads to a simple object. Conversely for every 
object and for every one of its components there is a finite sequence s0, s1, ••• , sk of selec-
tors which leads to it. We say: such objects of a terminating recursive object structure are 
finite objects. 
In accordance with Turski 1971 we call object structures semiregular if every simple 
component is chosen by one and only one 35 (possibly composite) selector. Turski calls 
semiregular structures regular if all simple components are accessed in the same number of 
selector steps. 
All structures constructed from arrays and compounds in a non-recursive way are semi-
regular; those homogeneously constructed are regular. Turski also discusses structures 
which are not semiregular (he calls them "irregular"). In this case of non-semiregular 
structures it can happen that a specific simple component can be chosen in different ways 
and that for certain selections no further subsequent selection leads to a simple component 
(Fig. 2.34). 
Fig. 2.34 
If recursively defined object structures are restricted to finite objects then these struc-
tures are obviously also semiregular. The restriction is important: the recursive definitions 
of modes can be considered as algorithms for generating objects sets. These functions, in 
general, have fixpoints which correspond to non-finite objects (Scott 1970). Thus we have 
e.g. an object z of mode lsequ X with the property of being cyclic: trunk of trunk of trunk 
of z = z; there is a non-terminating selector sequence for this object. The restriction is not 
unrealistic: objects constructed through terminating algorithms by means of constructors 
are necessarily finite objects. 
However, non-finite objects can have the property of not being semiregular. The ele-
ment z above can be chosen both by trunk o trunk o trunk and by 
trunk o trunk o trunk o trunk o trunk o trunk. 
In order to capture non-finite objects in mathematical semantics we must consider not 
only - as in 1.5 -
fixpoints of junctionals but also fixpoints of junctions. By analogy 
with the equation 
35 Several occurrences of one and the same object are to be differentiated. 

170 
2. Objects and Object Structures 
fac(x) "' r [fac J (x) 
we now might for example consider 
nat x "'x + 1 
or 
natx "'x x x 
(The first of these equations has only D as a fixpoint, whereas the second has three fix-
points, D, 0 and 1, of which Dis of course the weakest.) We will not examine the corre-
sponding extensions of mathematical semantics here and will restrict ourselves to 
operational semantics in 2.14.2 based on routines having all their parameters suppressed. 
The principal problem in operational semantics has so far been to establish that the 
computation terminates if and only if the result is not D. It is obvious that this presents 
special difficulties with non-finite objects. A possible solution is given by "switching" 
(locally) to lazy evaluation. 
2.14.1 Nexuses of Objects 
2.14.1.1 Even the objects in 2.9.3, constructed in detail by hierarchical systems of para-
meterfree routines are finite. Routines for constructing non-finite objects, on the other 
hand, can be obtained as fixpoints of non-terminating recursive systems of depara-
meterized routines. 
Extending a recursively defined mode not by D, as was done in 1.5, but by the set of all 
infinite objects of this mode, has the effect that the system 
funct a "' lsequ rat: (2, b), 
functb "'lsequ rat: (-1, c), 
funct c "' lsequ rat: (1/2, a> 
has one fixpoint 36, respectively, for a, b, and c, and e.g. the routine z in the system 
funct z "' lsequ rat: (0, a}, 
funct a "' lsequ rat: (2, b), 
funct b "' lsequ rat: ( -1, c), 
funct c "' lsequ rat: (1/2, a} 
defines an object z whose detailed object diagram is given by Fig. 2.35 and which can also 
be informally described by 
(0, (2, ( -1' (1 /2, (2, (- 1' (1/2, ( ... } }) }) }) } 
objects connected in this way are said to form a nexus of objects. 
36 For a more thorough theoretical discussion see Scott 1976. 

2.14 Infinite Objects 
171 
Fig. 2.35 
The subobjects of z 
trunk of z, trunk o trunk of z and trunk o trunk o trunk of z 
are even cyclic of order 3. The objects a, b and c form a "ring list". In contrast with the 
case of finite objects there is no longer a hierarchical subordination: Some objects contain 
themselves as a subobject. 
Note that the infinite objects in question are finitely described ("finitary"): Each of 
them contains only a finite number of different subobjects. 
Complete substitution is a non-terminating process for non-finite objects. Partial sub-
stitution is possible; in the example above, however, one substitution arrow must remain, 
e.g. 
(0~)))) 
The finite and non-finite nexuses of objects of mode lsequ x (and of other linear object 
structures) definable by finite systems of deparameterized routines are easy to survey: the 
(directed) graph underlying the detailed object diagram has the in-degree 1 and is thus, ac-
cording to an elementary theorem of graph theory 37, an arborescence or a cycle with 
protruding arborescences. In the first case we have a finite nexus and in the last an infinite 
nexus. An object is thus either a linear list or a ring list with a (possibly empty) protruding 
linear list. Figure 2.36 shows an example of a nexus from lsequ nat 38• 
Exercise I: Give a description oj4/3, 2/7, 11271 in the decimal system by a nexus of objects of mode 
lsequ char. 
2.14.1.2 Cascade-type object structures, too, allow finite and infinite objects. The graphs 
which underlie the detailed diagrams are no longer essentially restricted in their generality. 
For this reason we only pick out a few nexus classes: 
37 Which oddly enough is missing in some books about graph theory for computer scientists. 
38 The diagram has the following underlying structure of its objects q: 
item of trunk of q = 91(1 + 3 x item of q), where 
funct 9' .. (natx: x * 0) nat: 1natz: 3 naty: x = 2Y x z A -, 2lz 

172 
2. Objects and Object Structures 
Fig. 2.36 
a) The class of hierarchical two-way cascades for which examples were given in 2.9 - the 
class of all finite objects of mode case X· 
b) The class of linear two-way lists, infinite objects of mode case X· 
c) The class of two-way ring-lists, infinite objects of mode case X· 
Formal definition for (b) and (c) should be unnecessary and the examples in Fig. 2.37 
and Fig. 2.38 for case char will suffice. 
a2 
a 3 
a4 
IO,'D',~,·c.~ ,'8', ~.'A',OI 
a, 
Fig. 2.37 
Fig. 2.38 

2.14 Infinite Objects 
173 
The usefulness of such infinite objects is evident: right or left sequences can be derived 
from them by dispensing with the arrows pointing to the right-components or to the /eft-
components, e.g. Fig. 2.39 or Fig. 2.40. 
( 0, 'D', 0) v, 
'C', 0) v, 
'8', 0) v, 
'A', 0) 
Fig. 2.39 
(o.·o·.v(o,·c·.v(o.·B·.v(o.·A·,o) 
Fig. 2.40 
Joint implementation of lsequ x and rsequ x by linear two-way lists makes the opera-
tions bottom and top, upper and rest, stock and append equally efficient. Thus complete 
symmetry is introduced. It is particularly suitable for implementing files and in particular 
rolls 39• 
Exercise 2: Specify the operations advance, recede etc. for an implementation of files by linear two-
way lists. 
Two-way lists are generally characterized in that left o right and right o left, when ap-
plicable, yield the identical mapping. However, this shows clearly that the two-way lists of 
(b) and (c) can be defined by means of finite systems of deparameterized routines; they are 
finitary. 
In the latter case left o left o .. o left 
'----v-----1 
right o right o .. o right for a certain 
n 
n 
n > 0 holds in every nexus in addition to left o right = right o left. 
2.14.2 Lazy Evaluation 
2.14.2.1 Extensive object diagrams cannot be described by explicitly specifying a suitable 
number of deparameterized routines - for practical as well as for essential reasons if they 
are not bounded. In order to convert e.g. a natural number into a left sequence lsequ bit 
we can proceed recursively and declare the appropriate deparameterized auxiliary routine 
in the body of the recursive routine; this is then declared separately for every incarnation 
and establishes the substitution link: from 
funct convert "' (nat a) lsequ bit: 
if a = 0 then 0 
else if even(a) then (0, convert(a/2)) 
0 odd(a) then (l, convert((a -
1)/2)) fi fi 
we obtain by means of detailing 
39 The structure case xis more suitable for rolls than for files. Two-way lists of mode lisp x are un-
interesting. 

174 
funct convert = (nat a) lsequ bit: 
if a = 0 then 0 
else If where 
functj = lsequ bit: 
2. Objects and Object Structures 
if even(a) then (0, convert(a/2)) 
D odd(a) then (l, convert((a -
1)12)) fi J fi 
For example convert(13) leads to the object (l, (0, (l, (l, 0)) )), where during 
computation the routines j(ll, jl2l, jl3l, jl4l are declared and cause the substitution accord-
ing to the following diagram (Fig. 2.41). 
f(l) 
f121 
f131 
f141 
'( 
L, ~( 
0, ~( 
L, ~( 
L, <>) 
Fig. 2.41 
However, we may suppose that we do not want to perform the substitution process at 
all, but that we want to retain the detailing structure instead. The result of the conversion 
is then not a left sequence but a routine for obtaining this left sequence. This would mean 
an alteration of the routine convert: the heading would then have to read (comp. 1.14) 
funct convert = (nat a) funct lsequ bit: 
What we are seeking lies in between, neither an immediate evaluation, nor no 
evaluation, but just a lazy evaluation (Henderson and Morris 1976): The evaluation should 
be delayed as long as possible (Vuillemin: "Never do today what you can put off until to-
morrow"); it is suspended (Friedman and Wise 1978) until it becomes urgent. This means, 
roughly speaking, a "local switching" in the evaluation of certain primitive 
(constructor-)operations. We express this by placing lazy before the function to be 
declared 40: 
funct convert = (nat a) lsequ bit: 
if a= 0 then 0 
else 
I jwhere 
lazy functj = lsequ bit: 
if even(a) then (0, convert(a/2)) 
D odd( a) then (l, convert((a -
1)12)) fi J fi 
The effect of convert(13) is now that each of the four incarnations off waits for its 
evaluation, i.e. the substitution process; this means that the actual "formation" of the 
composite object is only intended and not yet undertaken. 
For the moment this still sounds like a word game because on the applicative level - on 
which we are -
the composition of objects is (still) an abstract process. Nothing other 
than textual composition known as the creation of terms takes place in the text substitution 
40 A lazy evaluation could also be used throughout. But altogether this would only mean extra work 
if done on the von Neumann machine. 

2.14 Infinite Objects 
175 
machine. We will meet this aspect in Chap. 3 again. An implementation of the 
composition of objects on the von Neumann machine, however, entails work: storage, 
management, addressing; it is therefore of advantage if in 
lsequ bit z = convert(13) within trunk of z 
the actual formation of ( L, (0, ( L, ( L, 0)))) is deferred because only a part of it is used 
subsequently, namely (0, (l, (l, 0))). (Friedman and Wise 1976: "cons should not 
evaluate its arguments".) Thus lazy in general is to mean the transition to complete term 
creation 41 • Henderson and Morris 1976 have given an operational semantics for it. 
2.14.2.2 If one follows the term creation in the example of producing a sequence of primes 
in 1.14.3, one sees that the lazy evaluation used there is concerned with the constructor 
function append. 
Exercise 1: Rewrite the example of producing a sequence of primes in 1.14.3, using lazy evaluation. 
The introduction of lazy evaluation was motivated in 1.14.3 independently of the oc-
currence of infinite objects. But for infinite objects it is indispensable. If we evaluated a 
call of the routine z of 2.14.1 with the usual computation rules, it would not terminate. 
Non-terminating systems of routines which serve to define infinite objects will, on the 
other hand, be permitted if every call of a routine of such a system is safe due to the func-
tion being declared for lazy evaluation only (prefix lazy). 
Thus, the example at the beginning of 2.14.1 is to be written 
lazy funct z = lsequ rat: (0, a), 
lazy funct a = lsequ rat: (2, b), 
lazy funct b = lsequ rat: ( -1, c), 
lazy funct c = lsequ rat: (1/2, a) 
2.14.2.3 We shall now describe the two-way list belonging to a left sequence. Let f = 
transit(a, z) be the leftmost case compound which belongs to the left sequence a, having z 
as its left component. Then the node component off is to be item of a, and the right com-
ponent of/is the leftmost case compound gbelonging to trunk of a, with/as its left com-
ponent: g = trans (trunk of a, f). Thus f = case x: (z, item of a, g) (Fig. 2.42). 
f= trans (a,z) 
g =trans (trunk of a ,f) 
~ 
(z, item of a, ~ 
(f, ... 
Fig. 2.42 
We can thus specify the construction of a two-way list as an implementation of right 
sequences in the following way: 
41 Friedman and Wise call objects produced by term creation "computational structures". We shall 
use this word in greater generality in Chap. 3. 

176 
funct transit = (lsequ 1.. a) case x.: trans(a, 0), 
funct trans = (lsequ 1.. a, case 1.. z) case x.: 
if a= 0 then 0 
else r f where 
2. Objects and Object Structures 
lazy funct f = case x.: <z. item of a, g), 
lazy funct g = case x.: trans(trunk of a, f) J fi 
For the call (1.. = char) 
transit((,D', (,C', (,B', (,A', 0))))) 
the pairs (f(tl, g<1l), (j<2l, g<2l) and (j<3l, g(3l) are introduced in the incarnations. We obtain 
a detailed object]= j<1l which contains the deferred evaluations. It is illustrated by the 
object diagram (Fig. 2.43). Note that g<n = j<2l, g<2J = j<3l, 
f 
gl11 
gO-l 
gl31 
k 
"I .,'D', ~,'C', ~,'8', ~,'A',. 1Jf 
f111 
f121 
f'31 
Fig. 2.43 
We can also eliminate g from the system j, g; we then have 
funct transit = (lsequ 1.. a) case x.: trans(a, 0), 
funct trans = (lsequ 1.. a, case 1.. z) case x.: 
if a= 0 
then 0 
else r jwhere 
lazy funct f = case x.: (Z, item of a, trans (trunk of a, f)) J fi 
where for the above call a different detailed object results, its object diagram now contains 
only the one kind of substitution arrows (Fig. 2.44) ("left sequence with back 
references") 42• 
" 
" 
f 
k 
\1 •• 'D', I .• 'C', I .• '8', I \(A', • I~ 
f(3)~" 
fiZl 
Fig. 2.44 
42 Deparameterized routines with lazy evaluation correspond to the "streams" of Landin 1965. They 
are on the applicative level the true image of the so-called "references". In the special case of 
"references to a compound of variables" we shall meet them again in 7.4.1 under the concept of 
"pointer". 

2.15 Some Peculiarities of Arrays 
177 
Note that the nexus contains four objects, J is one of them. The algorithm can be 
specified such that is also yields the object K and is thus completely symmetrical. This 
applies in particular to access from both sides by means of selectors. We can even produce 
the pair (j, K) as an object case x: (], 1. f) where 1. is a special element. 
Closing two-way lists to two-way ring-lists by means of a list head case x: (], 1., K) has 
however the disadvantage that the test for the end (comparison with 0) must be replaced 
by a test for the special element 1. (comparison with 1.). 
Exercise I: Give an algorithm for the concatenation of two linear two-way lists. 
Exercise 2: The routine 
funct rotate = (lsequ x a: a * 0) lsequ x: stock(trunk of a, item of a) 
suggests the conversion of finite right sequences into (one-way) ring structures. Specify 
the algorithm for this. How is rotate expressed in the ring structures? 
2.15 Some Peculiarities of Arrays 
Arrays have objects as selectors and are homogeneous. They are therefore related to 
mappings. Some of the resulting peculiarities will be now discussed. 
2.15.1 Arrays with Computed Index Bounds 
Index sets are frequently computed modes (comp. 2.4.5}, such that their index bounds 
evolve during computation. The best-known example are arrays indexed by whole 
numbers with computed lower and upper bounds, 
int [n .. m] array 11 
If objects of such parameterized modes occur as parameters of a routine then the latter 
must be parameterized (comp. 2.4.5) in two steps, e.g. 
funct tdist =(nat n). (nat [1 .. n] array real a, nat [1 .. n] array real b) real: 
I d(n) where 
funct d =(nat i) real: 
if i = Othen 0 
else abs(a[i] - b[i]) + d(i- 1) fi J 
The partial call tdist(20) then yields a routine which permits the computation of the 
Chebyshev distance of two 20-component arrays: If u and v indicate two 20-component 
arrays, then the complete call tdist(20) (u, v) yields their Chebyshev distance. 
A similar problem occurs when the result of a routine is of a computed mode. The 
result mode must first be established before the evaluation of the call can begin. 
But first of all we have another difficulty: objects of an array with computed index 
bounds cannot be formed with the enumerative constructor used hitherto. In Chap. 3 a 

178 
2. Objects and Object Structures 
more general class of arrays will be introduced for which more powerful constructors can 
be used. For the moment it is sufficient to make the observation that the direct product 
used, when forming arrays, is an associative cartesian product. Let a be an object of mode 
v[lwb .. upb] array J1 and x,y objects of mode"' then two constructors are introduced: 
v [lwb .. succ(upb)] array ": (a, x) and 
v[pred(lwb) .. upb] array": (y, a), where 
(y, (a,x)) = ((y,a),x) holds. 
Thus we can compute e.g. the square of the components of an array: 
funct sq = (nat n). (nat [1 .. n] array real a) nat [1 .. n] array real: 
I s(n) where 
funct s = (nat i) nat [1 .. i] array real: 
if i = 0 then 0 
else (s(i - 1), a[i] i 2) fi J 
For the auxiliary routine s the result mode must first be established in every call. To this 
end we therefore define: In single- (or multi-) stepped parameterized routines the comput-
ed modes of the auxiliary routines are established successively. Finally a parameterless 
routine results which is evaluated after its mode is established. 
2.15.2 Induced Operations for Arrays 
Induced operations for functions were introduced in 1.14. The similarity between arrays 
and functions - both are mappings - suggests inducing on the whole array, operations 
that exist for the (uniform) component mode of an array. 
For example an array Rho(a, b) is to be determined for two arrays a, b such that 
Rho(a, b)[i] = rho(a [i], b[i]) 
holds. By analogy with 1.14 a functional form Beta can be introduced 
Rho = Beta(rho), 
where 43 
funct Beta = (funct (IJ, Jl) K rho). ( v array "a, v array " b) v array K: 
1 v array K c: v vi: c[i] = rho(a[i], b[i]) 
In the sense of functional programming (comp. 1.14) Beta and similar forms for other 
functionalities will be universally presupposed and will even be notationally suppressed as 
far as possible 44• In any case it is useful to leave open the implementation of such 
43 The complete analogy to 1.14 would be 
funct Beta= (funct(JI. p)K rho).(v array pa, v array pb).(v i) K: rho(a[i], b[i]) 
However this yields only a routine and not an array which is a "frozen function". 
44 This can be learned from APL. 

2.16 Routines with Multiple Results Revisited 
179 
operations induced on arrays (the execution can be done very efficiently on array 
processors). 
Exercise 1: Give an implementation of Beta with the help of the two constructors of 2.15.1 and the 
expressions max(v), min(v)for the upper and the lower bound of a mode v. 
2.16 Routines with Multiple Results Revisited 
Just as we can consider routines with several parameters as being routines with a single 
composite parameter we can also consider routines with several results as being routines 
with a single composite result. 
Thus it is possible to have access to the individual results directly (by means of corre-
sponding selectors) which can also be notationally beneficial in recursive calls: instead of 
using a collective auxiliary declaration, as in the routine natdiv of 1.13.3, 
funct natdiv = (nat a, nat b: b * O)(nat, nat): 
if a < b then (0, a) 
else (nat q, nat r) = natdiv(a -
b, b) within 
(q + 1, r) 
fl 
we introduce a composite mode 
mode p = (nat quot, nat rem) 
outside of natdiv and can now write 
funct natdiv = (nat a, nat b: b * 0) p: 
if a < b then (0, a) 
else p x = natdiv(a - b, b) within 
(quotofx+1,remofx) 
fi 
Of course it is of no benefit to introduce the composite result mode explicitly in the 
heading, e.g., 
funct natdiv = (nat a, nat b: b * O)(nat quot, nat rem): 
if a < b then (0, a) 
0 a ~ b then (nat quot, nat rem) x = natdiv(a - b, b) within 
(quot of x +1, rem of x) 
fi 
quot and rem are still selectors here but are only valid within natdiv. 
Comparison with 1.13.4 shows the relationship between selectors and result para-
meters. Obviously if object declarations for composite objects are used we can manage 
without any result parameters. 

180 
2. Objects and Object Structures 
For the example grigri in the version of 2.6 we can write similarly - with a composite 
result mode 
mode result = (int dom, nat numb) 
funct grigri = (nat n: n =1= 0 " n ~ N) result: 
if n = 1 then result: (a[1], 1) 
D n =1= 1 then if a[n] = a[n - numb of grigri(n -
1)] 
then (a[n], numb of grigri(n -
1) + 1) 
else grigri (n -
1) 
fi fi 
if for the moment we disregard the double call of grigri. Again it is better to write 
funct grigri = (nat n: n =1= 0 " n ~ N) result: 
if n = 1 then (a[1], 1) 
D n =1= 1 then result h = grigri(n -
1) within 
ifa[n] = a[n- numbofh] 
then (a[n], numb of h + 1) 
else h 
fi fi 
Addendum to Chapter 2. Notations 
The notation used is basically oriented towards ALGOL 68. As the latter is somewhat 
scanty with regard to object structures PASCAL constructs have been incorporated. 
In PASCAL key-words and mode indicant, however, are not printed in bold face and 
thus e.g. enumerations are denoted as follows: 
type suit 
type sex 
type Boolean 
= (club, diamond, heart, spade) 
= (masculine, feminine) 
= (false, true) 
Restriction of object sets by predicates in PASCAL is not possible, except in intervals: 
type century 
= 1900 .. 1999 
type consolation prize = 3 .. 7 
Compounds are called "records" in PASCAL and are written 
type complex = record re: real; 
im: real end 
type date 
record day: 1 .. 31; 
month: 1 .. 12; 
year: 1900 .. 1999 end 
type person 
record name: a/fa; 
firstname: a/fa; 

Addendum to Chapter 2. Notations 
181 
date of birth: date; 
sex: (masculine, feminine); 
status: (sing, mar, wid, div) end 
In Wirth 1976 (1. 7) "record" objects are denoted by enclosing them in round brackets and 
prefixing them with the respective indicant 
complex (1.0, -1.0) 
date 
(30, 12, 1976) 
person ('bauer', 'martin', date (14, 6, 1976), masculine, sing) 
Components are selected by using Dewey notation, e.g. 
r. re 
x. date of birth. day 
"Arrays" in PASCAL read 
type output = array [1 .. 12] of real 
type account = array [(black, red)] of integer 
type q 
= array [Boolean] of suit 
"Array" objects are denoted similarly to "records" (comp. Wirth 1976, 1.6) 
output (20, 20, 23, 19, 20, 20, 21, 23, 22, 21, 21, 21) 
account (1450, 7280) 
q (club, heart) 
Selection takes place in an ALGOL-like manner, 
x[5], balance [red], game [true] 
Multi-stepped arrays, e.g. 
array [1 .. 12] of array [1 .. 30] of real 
can be shortened to 
array [1 .. 12, 1 .. 30] of real 
PASCAL provides variants in the structured type "variant record", though only in the 
form of disjoint variants 
type point = record case label: 
(cartesian, polar) of 
cartesian: (x, y: real); 
polar: 
(r: real; phi: angle) end 
A variant mode formed from disjoint modes is expressed in ALGOL 68 by the union 
mode number = union (int, real) 

182 
2. Objects and Object Structures 
Note, however, that disjointness of modes is required even after applying possible coercion 
operations. This is indicated by demanding the variants in a union not to be "related". 
Neither ALGOL 68 nor the orthodox version of PASCAL for which compilers exist 
provide for general recursive object structures. The only special sequence-type structure 
present is file of ... . It corresponds at best to the object structure ,file" of2.11, although 
it is tied to a buffering mechanism which results from the implementation (comp. 7.2). 
Otherwise both ALGOL 68 and PASCAL require (as a substitute) pointer implementa-
tions (comp. 7.1 or 7.2). 
Similar to Hoare 1973, von Henke 1975 describes object structures using a notation 
modelled on BNF and otherwise related to PASCAL 
rsequ :: = 0 lappend(trunk: rsequ, item: atom) 
Here the constructor append and the selectors item and trunk are simultaneously intro-
duced. According to this in 2.9.1.1 we could have written 
mode rsequ x "' atomic { 0} I append (rsequ l trunk, l item) 
(comp. Bauer eta!. 1981). 
A predecessor of compounds can be found in COBOL, where compositions of compo-
sitions are possible, but only words of decimal numbers of fixed length or alphanumeric 
signs are allowed as primitive modes (general mode declarations are not possible): 
01 ADDRESS 
02 STREET 
03 NR PICTURE 9 
(4) 
03 STR PICTURE A (20) 
02HOMETOWN 
03 TOWN PICTURE 
A (20) 
03 STATE PICTURE 
9 
(2) 
03 AREA CODE PICTURE 9 
(5) 
STATE OF HOMETOWN OF ADDRESS 
This corresponds, apart from access restrictions, to a list of general mode declarations 
mode address 
= (street street, hometown hometown) 
mode street 
"' (nat [1 .. 4] array dec nr, nat [1 .. 20] array char str) 
mode hometown "' (nat [1 .. 20] array char town, 
nat [1 .. 2] array char state, 
nat [1 .. 5] array dec area code) 
and presuming a to be of mode address the selection reads 
state of hometown of a 
Multi-stepped arrays (maximum is three steps) are written in a similar way. 

Addendum to Chapter 2. Notations 
183 
PL/I notation for compounds is based to a certain degree on COBOL. 
Primitive modes in SNOBOL are int, string and real. Compounds have no modes, 
only the composition structure is specified. 
DATA ('COMPL (RE, IM)') 
specifies the untyped structure COMPL together with the selectors RE(.) and IM(.) as 
well as COMPL(., .). In our case this would read (using arbitrary modes 11· v) 
mode complex = (J1 re, vim) 
where 
re of . , im of . 
are selectors, 
and 
complex: (. , . ) 
is a constructor . 
Arrays are likewise untyped in SNOBOL; the only structure is a string. EULER as well as 
GEDANKEN are untyped, and APL provides only for "numbers". BCPL, BLISS and 
other systems implementation languages are restricted to binary words for obvious 
reasons. 


A waiting queue (Hiirlimann) 
Chapter 3. Computational Structures 
"If at a given level of refinement one is interested 
only in the behavioural characteristics of certain data 
objects, then any attempt to abstract data must be 
based upon those characteristics, and only those 
characteristics. The introduction of attributes, e.g. a 
representation, can only serve to cloud the relevant 
issues!" 
Guttag 1975 
In chapter 2 it became apparent that newly introduced object sets never occur "isolated" 
but always together with operations characteristic of them. Thus the canonical operations 
termed constructors and selectors belong to composite objects (comp. 2.5 and 2.6); the 
comparison operator is invariably available for all object sets (comp. 2.4). In chapter 1 we 
have pragmatically presupposed certain object sets such as bool, nat, int, sequ X etc. and 
their characteristic operations. 
This situation suggests considering object sets and their typical operations conceptually 
together as a unit ("computational structure") and characterizing them as such notational-
ly. This chapter deals with the introduction and use of such computational structures. 
As the operations of a computational structure are considered to be available as 
"primitives" to the outside, algorithms based on them can be formulated on a "higher 
level". For instance the routine fac of 1.4.1 has been formulated based on the operations 
subtraction, multiplication and comparison from the computational structure of natural 
numbers (Table 1.3.1) without immediately having to consider an implementation of these 
three operations. Computational structures are thus also an important tool for solving a 
problem by "stepwise refinement" (Wirth 1971). 
Well-known examples of such "modularization" are the input-output operations which 
are offered by programming languages, or, more generally, the operating system functions 
at the disposal of a user in his program. Instead of introducing such operations piecemeal 
into a language, as has often been done, they can just as well be incorporated uniformly in 
the form of suitable computational structures. A related application of computational 
structures, which is of increasing importance today, is given by the access functions for 
data bases. 

186 
3. Computational Structures 
The latter example clearly reveals a further aspect of computational structures: the user 
does not need to know how the information is internally "stored" within the data base; all 
that is of importance to him is the answers he may expect in reply to his inquiries. Such 
computational structures, defined independently of any representation, whose object sets 
and operations are characterized only by their properties, are called "abstract computa-
tional structures". 
A final important characteristic of computational structures is the safety which results 
from a certain "encapsulation effect". As the user does not know all the details of there-
presentation but only selected operations necessary for solving his problems, some com-
mon sources of mistakes are eliminated. 
Hierarchical encapsulation frequently leads in practice to increased transparency. 
As we have already done with object structures -
e.g. arrays with computed index 
bounds - we also provide for parametrization of computational structures. This provides 
wider applicability. 
3.1 Concrete Computational Structures 
First we deal with a notation which combines object sets and operations into one unit. The 
aspect of abstraction which frees us from certain representations is here ignored complete-
ly. 
3.1.1 Encapsulation Effect 
First of all we want to show how "encapsulation", i.e. protecting the implementation de-
tails against access from the user, enhances safety. Let us look at the object structure 
(comp. 2.9.2) based on rN 
mode rsequc bool "" (nat {0} length)! 
({nat i: i > 0} length, rsequc bool trunk, bool item) 
As the selector length occurs in both variants the selection length of. is always defined. 
Moreover, length can serve for the discrimination of the two variants as the two modes are 
based on disjoint subsets of nat. 
However, this discriminator only differentiates between zero and non-zero. The object 
structure therefore permits not only objects such as 
(0), 
(1, (0), T) and 
(2, (1, (0), T), F) 
but also e.g. 
(3, (0), T) and 
(1, (1, (0), T), F) 
The identifier length induces the user to reading the length directly from the object by 
means of this selector although it can only be determined using the recursive routine 
lengthc, 

3.1 Concrete Computational Structures 
funct lengthe = (rsequc bool a) nat: 
if length of a = 0 then 0 
else lengthe(trunk of a) + 1 fi 
For such a direct reading to be correct we must require as a condition the assertion 
(L) length of a = lengthe(a) 
This is (after elimination of the routine lengthe) equivalent to 
If length of a = 0 then true 
else length of a = length of trunk of a + 1 fi 
or, using the sequential disjunction (comp. 1.3.3), 
length of a = 0 "' length of a = length of trunk of a + 1 
187 
As soon as one imposes such conditions on object structures one has to ensure that they 
are observed. It is obvious that the free use of the constructor belonging to the object 
structure can lead to a violation of the conditions; hence its free use must be prevented and 
instead we must provide suitably defined routines for forming new objects, which preserve 
the validity of the conditions. In our example this is done by means of the routine 
funct appende = (rsequc boola, bool x) rsequc bool: 
(length of a + 1, a, x) 
If a satisfies the condition (L), then appende(a, x) also satisfies this condition. 
It is now a natural step to hide the selectors as well as the constructor and to allow com-
position of and access to objects solely by means of suitable operations. 
In our example we obtain a unit consisting of one object structure and five routines; we 
mark the encapsulation notationally by brackets and add a heading: 
(rsequc bool, emptye, isemptye, tope, reste, appende, lengthe): 
1 mode rsequc bool =<nat {o} length) 1 
( {nat i: i > 0} length, rsequc boo I trunk, boo I item), 
= rsequc bool: (0), 
= (rsequc boola) bool: a= (0), 
= (rsequc boola: -, isemptye(a)) bool: item of a, 
funct emptye 
funct isemptye 
funct tope 
funct reste 
funct appende 
= (rsequc boola: -, isemptye(a)) rsequc bool: trunk of a, 
= (rsequc boola, bool x) rsequc bool: 
(length of a + 1, a, x), 
funct lengthe 
= (rsequc boola) nat: length of a 
J . 
The heading specifies the indicants of the modes and the identifiers of the operations 
which are to be available outside. This specification is necessary, because the unit may 
comprise auxiliary operations and modes, the denotations of which should be hidden. 

188 
3. Computational Structures 
For the encapsulated modes only the indicants are known outside, not the internal 
realizations of the object structures; for the encapsulated routines apart from their iden-
tifiers only their functionalities are known and not the body. For notational simplification, 
however, we include only the identifiers in the heading; they are substitutes for the full 
functionalities to be found in the body. 
In addition, we stipulate that outside only those objects are available which can be 
generated using the available operations. Thus, introducing a computational structure is 
generally accompanied by a restriction of the object set; in our example to those objects 
which correspond to internal objects satisfying the condition (L). 
3.1.2 Properties of Operations 
The encapsulation effect, in its strict form, not only hides the internal structure of the 
defined objects, but also prevents improper manipulation of these objects by means of the 
operations specified in the heading, because - apart from the names and the correspond-
ing functionalities -
nothing is known of them outside. Thus the encapsulation effect 
forces a description of these routines by means of (outwardly accessible) characteristic 
properties ("axioms", "laws") which result from certain relationships between the 
operations. In the preceding example, among others, the following properties hold (comp. 
2.10): 
law R: restc(appendc(a, x)) = a, 
law T: topc(appendc(a, x)) = x, 
law A: --, isemptyc(a) ,;. appendc(restc(a), topc(a)) = a 
Here we use a notation similar to that for predicates; for simplicity universal quanti-
fications are omitted wherever they can be supplied from the context (i.e. from the func-
tionalities of the corresponding operations). Universal quantification does here not extend 
to the pseudo object D. Hence we could write more explicitly: 
law R: v (rsequc bool a, bool x): restc(appendc(a, x)) = a 
and 
law A: v rsequc bool a: --, isemptyc(a) ,;. appendc(restc(a), topc(a)) 
a 
or 
law A: v rsequc bool a: isemptyc(a) " appendc(restc(a), topc(a)) = a 
For better legibility different properties (marked by the keyword law and an identifier) 
will be separated by "," instead of "A". 
As these characterizing properties are to be accessible from outside they are added to 
the heading of a computational structure - as assertions related to the components listed 
there. 
Thus for the example of 3.1.1 we have the extended heading 
(rsequc bool, emptyc, isemptyc, tope, restc, appendc, lengthc: 
law R: restc(appendc(a, x)) = a, 

3.1 Concrete Computational Structures 
189 
law T: tope(appende(a, x)) = x, 
law A: 
..., isemptye(a) ,;. appende(reste(a), tope(a)) = a, 
law L1: lengthe(emptye) = 0, 
law L2: lengthe(appende(a, x)) = lengthe(a) + 1, 
law E1: isemptye(emptye), 
law E2: ..., isemptye(appende(a, x)) 
). 
The proof of properties L1, L2, E1, E2 is straightforward. 
3.1.3 Definition of Concrete Computational Structures 
3.1.3.1lt can be seen immediately from the last example that all considerations and con-
structions are unaffected, if the mode bool (in rsequc bool) is replaced by any other 
mode such as nat or lisp. This suggests not confining ourselves to any particular mode but 
prefixing the heading by the mode as a parameter. In this case one should more exactly 
speak of a computational structure scheme. 
A unit consisting of a heading (possibly prefixed by some parameters) and the body 
comprising routines and object structures, is called a concrete computational structure. It 
has to be proved that the routines given in the body satisfy the properties occurring in the 
heading. 
With the identifier RSC we have the following computational structure for our preced-
ing example: 
structure RSC = (mode x)(rsequc J(, emptye, isemptye, tope, reste, 
appende, lengthe: 
law R: reste(appende(a, x)) = a, 
law T: tope(appende(a, x)) = x, 
law A: ..., isemptye(a) ,;. appende(reste(a), tope(a)) = a, 
law L1: lengthe(emptye) = 0, 
law L2: lengthc(appendc(a, x)) = lengthc(a) + 1, 
law E1: isemptye(emptye), 
law E2: ..., isemptye(appende(a, x)) 
): 
I in terms of IN: 
mode rsequc x = (nat {0} length) 1 
funct emptye 
funct isemptye 
funct tope 
funct reste 
funct appende 
funct lengthe 
({nat i: i > 0} length, rsequc x trunk, x item), 
= rsequc x: (0), 
= (rsequc x a) bool: a = (0), 
"' (rsequc x a: ..., isemptye(a)) x: item of a, 
= (rsequc x a: ..., isemptye(a)) rsequc x: trunk of a, 
= (rsequc x a, x x) rsequc x: (length of a + 1, a, x), 
= (rsequc x a) nat: length of a 
J 
In this example nat occurs; the computationa_l structure RSC is based (apart from X) 
on the computational structure 
IN 
consisting of nat and the corresponding 

190 
3. Computational Structures 
operations (comp. Table 1.3.1, 2.2a) which -
contrary to x -
should not be 
interchangeable. The universal computational structure 182 comprising bool (comp. Table 
1.3.1, 1.1) - which is always presumed to be basic - is also fixed. 
If the computational structure to be defined is based hierarchically on other computa-
tional structures A, ... , Z which are considered as primitive, we indicate this by writing 
"in terms of A, ... , Z:" at the beginning of the body; however, the universal computa-
tional structure 182 is usually not listed. 
rsequc x together with the selections length of., trunk of., item of., and the con-
structor rsequc x: (., . ) is called the hidden object structure1 which is the basis for this 
computational structure. It is obvious that the objects generated from elements of the 
given ("imported") mode x by means of emptyc and appendc are exactly those which 
satisfy the condition (L) mentioned above. 
Within the body of the computational structure RSC the object structure rsequc x still 
comprises those objects which violate the condition (L). On the other hand, they play no 
role outside of the body, since they cannot be generated by operations available outside; in 
accordance with our stipulation of encapsulation, they do not "exist" outside. 
3.1.3.2 It is, by the way, simpler to carry along the length of a sequence in the outermost 
position only. Using the hidden object structure 
(nat length, rsequ x s) 
results in the alternative body for RSC 
I in terms of IN: 
mode rsequc x "' (nat length, rsequ x s), 
funct emptyc 
"' rsequc x: (0, 0 ), 
funct isemptyc "' (rsequc x a) bool: a = (0, 0 ), 
funct tope 
"' (rsequc x a: --, isemptyc(a)) x: item of s of a, 
funct restc 
"' (rsequc x a: --, isemptyc(a)) rsequc x: 
(length of a - 1, trunk of sofa), 
funct appendc 
"' (rsequc x a, x x) rsequc x: 
(length of a + 1, rsequ x: (sofa, x)), 
funct lengthc 
"' (rsequc x a) nat: length of a 
J 
Again, encapsulation guarantees that the length component always is the correct length, 
i.e. that (L) holds. 
From the outside we cannot tell which body supports the computational structure. 
Furthermore it is irrelevant whether right sequences or left sequences are used. 
Exercise 1: Using contains from 2.10, give a computational structure for (right) sequences without 
repeated elements. 
Exercise 2: Give a computational structure for (right) sequences which are sorted (comp. sort, 
2.10.4). 
1 First attempts to introduce such hidden object structures can be found in SIMULA 67. 

3.1 Concrete Computational Structures 
191 
3.1.3.3 Not only modes but also objects (and routines) can be parameters of computa-
tional structures. An example is the following computational structure bounded stack with 
the parameter N for the maximal length (comp. also Wulf eta!. 1976): 
structure BS = (mode x. nat N: N > O)(bs x. empty, top, rest, 
append, isempty, isfu/1: 
lawR: -, isfull(a),;. rest(append(a,x)) =a, 
lawT: -, isfull(a),;. top(append(a,x)) = x, 
law A: -, isempty(a) ,;. append(rest(a), top(a)) = a, 
law E1: isempty(empty), 
law E2: -, isfull(a) ,;. -, isempty(append(a, x)), 
law F1: -, isfull(empty), 
law F2: -, isempty(a) ,;. -, isfull(rest(a)), 
law F3: isfull(a) <* (length(a) = N}, 
law L1: length(empty) = 0, 
law L2: -, isfull(a) ,;. length(append(a,x)) = length(a) + 1 
): 
lin terms of IN: 
mode bs x 
"' empty i(bs x trunk, x item), 
funct length 
"' (bs x b) nat: 
if b = 0 then 0 
else length (trunk of b) + 1 fi, 
funct empty 
"' bs x: 0, 
funct top 
"' (bs x b: -, isempty(b)) x: item of b, 
funct rest 
"' (bs x b: -, isempty(b)) bs x: trunk of b, 
funct append "' (bs x b, x x: -, isfull(b)) bs x: (b, x), 
funct isempty = (bs x b) bool: b = 0, 
funct isfull 
"' (bs x b) bool: length(b) = N 
J 
Note that outside the body the mode bs xis restricted to objects a of length length(a) 
~ N; in the body the mode coincides with rsequ x after suitable renaming. The operation 
length, incidentally, is an auxiliary operation which is necessary for a characterization of 
the computational structure by properties; length is not available "outside": the user 
should not have to bother with it, since isempty and isfu/1 suffice for formulating the asser-
tions of the operations top, rest and append. 
3.1.4 Atomic Examples 
The preceding examples were always based (in addition to nat) on an arbitrary primitive 
mode x and were thus (parameterized) classes of examples. In the sequel some examples of 
unparameterized, "atomic" computational structures will be given. 
(a) An alphabet of four elements (comp. also 2.4.2) 
structure SUIT "' (suit, dom, succ, a, oo: 
law ST1: 
-, dom (oo, a), 

192 
3. Computational Structures 
law ST2: 
a =1= oo ,;. dom (a, oo), 
law ST3: 
a =1= oo 
1\ 
b =1= oo ,;. dom (a, b) = dom(succ(a), succ(b)), 
law S1: 
a =1= oo ,;. succ (a) =1= a, 
law S2: 
succ(succ(succ(a))) = oo, 
law TRANS: dom(a, b) 1\ dom(b, c) => dom(a, c), 
law IRR: 
--, dom(a, a), 
law LIN: 
a =1= b => dom(a, b) vdom(b, a) 
): 
I mode suit =atomic{+, <(>, Q, .}, 
funct dom = (suit a, suit b) bool: 
if a = oo then false 
elsf b = oo then true 
else dom(succ(a), succ(b)) fi, 
funct succ = (suit a: a =1= oo) suit: 
if a = + then <(> 
D a=<(> thenQ 
D a = Q then • 
fi, 
funct a 
= suit: +, 
funct oo 
= suit: • 
J 
Following the same principle, computational structures describing games such as 
bridge or chess 2 are constructed. It is another matter to find strategies for such games -
exhaustive methods are generally intolerably inefficient (comp. 0.2). 
(b) The computational structure of binary switching operations 
structure BIT= (bit,./\., .v., .', T: 
law COMM: a A b = b A a, 
law ASSOC: (a A b) A c =a A (b A c), 
law IDEMP: a A a = a, 
lawHUNT: (b'Aa)'A(a'Ab)' =a, 
law MORG: a v b = (a'A b1' 
): 
I mode bit = atomic {0, L}, 
funct ./\. = (bit a, bit b) bit: if a = L then b else 0 fi, 
funct . v. = (bit a, bit b) bit: if a = L then L else b fi, 
funct .' 
= (bit a) bit: if a = L then 0 else L fi, 
funct T 
= bit: L 
J 
The specified properties form an axiom system for Boolean Algebra (Huntington 
1933). 
2 In his "PlankalkUI" of 1945 Zuse specified binary object structures and some pertinent routines for 
positions and moves in chess. 

3.1 Concrete Computational Structures 
Exercise 1: Using COMMand ASSOC show that 
<l>(a, b~ A <l>(a', b~ = <I>(b, a~ A <l>(b', a~ and 
a' A b = a" A b' => <I>(a, b) = <I>(b, a~ 
holds for 
<I>( a, b) = def (b' A a~' A (a' A b)'. 
By means of HUNT derive from the above that 
aAa' = bAb' and a= a" 
(c) A graphic example 
193 
One of the best unorthodox examples of computational structures is a system for drawing 
paths. The topic has been taken up repeatedly, as e.g. by Wirth 1976 (p. 130). The 
following computational structures are suitable: 
1. A computational structure which comprises rotations through a right angle and 
reflections of a translation vector l: 
structure PLOTTER "' (go, l r., s.: 
law 11: r r r r x = x, 
law 12: s s x = x, 
law 13: r s r s x = x, 
law R1: r x * x, 
law R2: r r x * x, 
law R3: r r r x * x, 
law SO: s x * x, 
law S1 : s l = r l, 
law S2: s r l = l, 
law S3: s r r l = r r r l, 
law S4: s r r r l = r r l ): 
I mode go "' atomic {l, _., ~. +- }, 
funct l 
"' go: i, 
funct r. 
"' (go x) go: if x = i then _. 
0 x = ->then~ 
0 x = ! then+-
0 x = +- then i fi, 
funct s. "' (go x) go: if x = i then _. 
0 x = _.then i 
0 x = ! then+-
0 x = +- then ! fi J 
Exercise 2: Show for the computational structure PLOTTER that the properties S2, S3, S4 can be 
derived from S1 and 11, 12, 13. 
Exercise 3: Show for the computational structure PLOTTER that the properties 12, 13, R3 and SO 
can be derived from the other properties. 
2. The following computational structure based on PLOTTER comprises all Hilbert 
curves (mode hilb) and a function next which transforms one Hilbert curve into the next: 

194 
structure HILBERTCURVE "" (hilb, null, next): 
I in terms of PLOTTER: 
mode hilb "" sequ go, 
funct null "" hilb: empty, 
funct rot. "" (hilb k) hilb: 
if k = null then null 
3. Computational Structures 
else append(rot rest(k), r top(k)) fi, 
funct refl. "" (hilb k) hilb: 
if k = null then null 
else append(refl rest(k), s top(k)) fi, 
funct next "" (hilb k) hilb: 
(rot rot refl k) & (r r r l) & k & r & k & r r & refl k J 
null denotes the Hilbert curve of order 0; its successor k1 = cter next( null), the Hilbert 
curve of order 1 , is 
k! = empty & r r r r & empty & r & empty & r r & empty 
=rrrf&f&rf. 
The corresponding composition of 
r r r r = <- ' r = i' and 
r r = -> 
(following the arrows) yields the Hilbert curve of first order: 
c 
Further transition to k2 = cter next (k1) leads via 
refl kl = u. rot refl kt = c. rot rot refl kl = n 
and the components 
n~CtC~lJ 
to the Hilbert curve of second order 
and so on. 

3.2 Abstract Computational Structures and Abstract Types 
195 
3.2 Abstract Computational Structures and Abstract Types 
We have seen that object structures - after introducing suitable routines - can be hidden 
using the encapsulation of a computational structure and that the knowledge of some 
"characteristic" properties of these routines is all that is necessary for using this computa-
tional structure. This, in particular, reveals more clearly similarities between different 
object structures. 
The existence of an alternative concrete basis for the computational structure RSC of 
3.1.3 has already shown this. Moreover, this computational structure could be based as 
well on objects of the mode rsequ lor on objects of the mode lsequ l with corresponding 
operations empty, isempty, top, rest, append, and length (comp. 2.10), having each the 
same properties. It is even possible - as shown in 2.9.2.3 - to use objects of the mode 
case l or lisp x as a basis. 
The structural equivalence of these examples of concrete computational structures 
shows that their prime importance lies in the entirety of the available operations with their 
characteristic properties (last-in-first-out organization) and not in their implementation by 
means of concrete algorithms on one or another concrete object structure. 
Such related computational structures are said to be of the same abstract type. 
"Abstraction" means here that the object sets and operations are no longer constructed ex-
plicitly but are characterized solely by specifying their properties, i.e. that they are describ-
ed independently of a certain representation. 
This method is known in algebra as the "axiomatic method". In laying the foundations for 
algorithmic languages this approach has been pursued by C. A. R. Hoare (1972), B. Liskov and S. N. 
Zilles (1974), and J. V. Guttag (1975). 
In the following paragraphs the semantics of abstract types and of abstract computa-
tional structures will be clarified. 
It should be noted that the semantics of routines as introduced in Chap. 1 itself can be defined by 
means of a suitable abstract type (Goguen eta!. 1977, Pepper 1979, Bauer 1981, Broy eta!. 1982). 
3.2.1 Fundamental Concepts 
3.2.1.1 In general, concrete computational structures are composed of a family of object 
sets called carriers3, a number of (determinate, partial) operations on these carriers and a 
series of properties of these operations. Hence a computational structure is an algebraic 
structure, briefly called an algebra in the sequel. The object structures of Chap. 2 are 
therefore - together with their constructors and selectors - just abbreviated forms of cer-
tain computational structures (see 3.2.6). 
For given x. the concrete computational structure RSC(X) (3.1.3) is an algebra with the 
carriers rsequc l• x. nat, bool (as the equality operation "=" is taken to be universally 
defined for all modes, bool is always a carrier 4). Only the "new" operations emptyc, 
We speak of a homogeneous or heterogeneous structure according to whether one or more carriers 
are subject to operations: RSC is a heterogeneous structure. 
4 All computational structures therefore - except IB2 itself - are, strictly speaking, heterogeneous. 

196 
3. Computational Structures 
isemptyc, tope, restc, appendc, and lengthc which concern the "new" carrier rsequc x are 
specified in the heading; of course, all "primitive" operations for x, nat, and bool are also 
available. 
This situation is characteristic of the construction of computational structures: in 
general a new carrier, with new operations defined on it, is introduced and is based 
hierarchically on the carriers and operations of primitive computational structures already 
known. 
For example, RSC(x) is based on the primitive computational structure IN with the 
carrier nat; in addition RSC(X) and IN are based on the universal computational structure 
182 with the carrier bool. We have the following hierarchy diagram: 
RSC(x) 
~ 
tN 
/ 
IB2 
Thus in a computational structure we differentiate between the defined carriers 5 and 
the given ("imported") primitive carriers 1.!31, ••• , 1.13n which include instantiated mode 
parameters. 
Accordingly the set of operations is partitioned into the two subsets ~D and ~P• where 
~D comprises all operations in which a defined carrier occurs on an argument or result 
position, while the operations of ~P affect the primitive carriers exclusively. 
In defining a computational structure, only the core, i.e. the defined carriers and ~D to-
gether with their properties, is specified in the heading, but note that the 1.13; as well as ~P 
are constituents of the corresponding structure. The heading of the concrete computa-
tional structure contains only the core of the relevant information about the corresponding 
abstract type. Functionalities and properties of the operations from ~P are to be found in 
the appropriate primitive computational structures. 
3.2.1.2 We now define: 
The identifiers for the carriers of an algebra and the symbols for the operations to-
gether with the corresponding functionalities form a set which is called the signature I: of 
that algebra. 
The signature can be illustrated by a signature diagram, i.e. a bipartite graph with the 
carriers as one sort and the operations as the other sort of nodes. 
For I:Rsc we have, for example, the diagram Fig. 3.1 for the core of the signature. 
In addition we specify a set G: of properties of the operations of a signature, which are 
essentially predicates over identities in the operation symbols and free variables. The iden-
tities use the symbol = for universal comparison which is deliberately not specially listed 
as an operation of the signature. Free variables are assumed to be universally quantified, 
see the preceding examples. 
5 Also "types of interest" (Guttag 1975). We shall use the word "type" with a different meaning, as 
inS. MacLane, "Categories for the working mathematician", Springer, New York 1971. 

3.2 Abstract Computational Structures and Abstract Types 
eemptyc 
isemptyc 
• ~ 
197 
Fig. 3.1 
A signature ~ with fixed primitive carriers and operations from some algebras (if any) 
together with a set of properties Q: gives an abstract type (~. Q;) over these "imported" 
algebras. 
An algebra is said to be of the abstract type(~. Q;) if its signature is ~and its operations 
satisfy the properties in Q:. An algebra which is of a given abstract type is also called a 
model of this type. Algebras of the same signature are called homologous. 
Example 1: Abstract type "Stack". 
Given the signature with the core 
{mode stack x. 
funct stack x empty, 
funct (stack x) bool isempty, 
funct ({stack xx: --, isempty(x)}) x top, 
funct ({stack x x: --, isempty(x)}) stack x rest, 
funct (stack x. x) stack x append, 
funct (stack X) nat length 
(x, nat and bool being primitive) and the set of properties 
{law R: rest(append(a, x)) = a, 
law T: top(append(a,x)) = x, 
law A: --, isempty(a) ,;. append(rest(a), top(a)) = a, 
law L1: length(empty) = 0, 
law L2: length(append(a, x)) = length(a) + 1, 
law E1: isempty(empty), 
law E2: --, isempty(append(a,x)) 
It is obvious that (after suitable renaming) the concrete computational structure RSCis 
of this type. More precisely: each one of the two bodies given in 3.1.3.1 and 3.1.3.2 gives a 
model of this type. Models can also be built with objects of the mode lsequ x or rsequ x 
from Chap. 2, even with objects of the mode lisp x or case x, comp. 2.9.2.3. We shall 
therefore take the abstract type above as the type of a stack with length operation. 
Example 2: Abstract type "Group". 
Let an abstract type G have the signature ~G 
{mode g, funct (g, g) g . o ., funct (g) g inv, funct g e} 

198 
3. Computational Structures 
(boo! being the only primitive carrier) and let the property set ~G be 
{law ASSOC: (a o b) o c = a o (b o c), 
law LI: 
inv(a) o a = e, 
law RI: 
a o inv(a) = e, 
law INV: 
inv(inv(a)) = a, 
lawLN: 
lawRN: 
e o a= a, 
aoe=a 
There are obviously many non-isomorphic models of this abstract type G = (~, ~) 
(called "groups"), e.g. 
(a) the algebra with g = {n} and the operations 
inv: 
n 1-+ n, 
o.: (n, n) 1-+ n and 
e: 
1-+ n 
("one-element group") 
(b) the algebra with g = {odd, even} and the operations 
e: 
~-+ even 
inv: 
even 1-+ even 
odd 1-+ odd and 
o.: (even, even) ~-+ even 
(even, odd) 1--> odd 
(odd, even) 1-+ odd 
(odd, odd) 1-+ even 
("cyclic group of order 2"). 
3.2.1.3 Whereas in example 1 the models specified have been rather similar, this is not the 
case in example 2: the models given there have different cardinality of g; and there are 
many more models, even with a non-countable carrier set g, say the group of rotations of 
a Euclidean plane. 
Thus, the question arises which models exist for a given abstract type (L, ~). 
Certain algebras are of special importance when examining this question theoretically 
and practically: for every signature L there exists an algebra of the absolutely free abstract 
type (L, 0), namely the term algebra Wr. The carriers of Wr consist of all terms 6 over L, 
i.e. all wellformed strings which can be constructed from the elements of the primitive 
carriers and the operation symbols in ~D· Wellformed means, that the functionalities of 
the operation symbols are respected. It can be seen from the range of the outermost 
operation symbol to which carrier such a term belongs. Equality of terms in Wr means 
literal identity. 
The terms of the term algebra are just the usual expressions for the composition of 
operations. Thus we have the following relation between the term algebra Wr and a given 
6 Often called words (over 1:). These terms essentially form the "language" in which the properties~ 
are expressed. A common synonym for term algebra is word algebra. The notion "string of sym-
bols" was already used by Thue in 1914. 

3.2 Abstract Computational Structures and Abstract Types 
199 
homologous algebra A of signature l:: A term tin l: is interpreted in A by considering the 
operation symbols occurring in t as the corresponding operations of A and evaluating the 
expression thus obtained 7• 
In Example 1 
top(append(rest(append(empty, a)), b)) 
is a term of the term algebra - with a, bas elements of X· The interpretation of this term 
in the computational structure RSC(X), i.e. the evaluation of 
topc(appendc(restc(appendc(emptyc, a)), b)) 
yields the result b. (Note that e.g. tope is an operation, but top only an operation symbol.) 
The partial function <pA: Wl: --->A which maps each term in Wl: to the corresponding 
element of A is called interpretation of Wl: in A. 
3.2.2 Semantics of Abstract Computational Structures and Abstract Types 
3.2.2.1 In general there are many algebras of a given abstract type (l:, Q:) which do not 
necessarily have to be isomorphic. This is already shown by the simple example of the 
abstract type "group" which comprises all groups - finite and infinite. Whereas in mathe-
matics such an abundance is welcome, in computer science the requirement of finite de-
scription of algorithms makes us strive to describe all structures constructively. A decisive 
step in this direction is the 
Principle of Generation: We restrict our attention to those models of a type in which 
defined carriers contain only elements which can be generated (finitely) from the ele-
ments of the primitive carriers Ill; using the operations ~D· 
An element can be (finitely) generated if and only if -
starting from objects of the 
given primitive sets Ill; and nullary operations - it can be obtained by finitely often apply-
ing operations from L. 
Note that according to our stipulation in 3.1.1 every concrete computational structure 
satisfies this requirement, provided all the (recursive) routines defined in its body 
terminate. 
The advantage of the Principle of Generation is that it provides for induction methods 
("algebraic induction", "structural induction", "data type induction") which often permit 
proofs of properties which are not deducible from the given properties of an abstract type 
alone because they are not valid in models which are not finitely generated. 
3.2.2.2 We now turn to a more formal characterization of the finitely generated algebras of 
a given signature l:. Such an algebra was said to be finitely generated if each of its elements 
can be obtained by finitely often applying operations of l:. This however, is equivalent to 
requiring that each element is the result of interpreting a term of the term algebra Wl: in A. 
Thus, a :E-algebra A is finitely generated if and only if the interpretation <pA: Wl: ---> A is 
surjective. 
7 Considering a term algorithmically as a 'detailed object' (comp. 2.14.2) explains the origin of the 
expression "computational structure". 

200 
3. Computational Structures 
Now let A be a finitely generated algebra, a computational structure of type I: and cpA: 
Wl:-+ A the corresponding interpretation of the term algebra. Let/be an n-ary operation 
from L Interpreting a termj(t1 , ••• , tn) in A means applying the operation/A of A corre-
sponding to the operation symboljto the results of interpreting the argument terms t; in A. 
In other words 
We call a partial mapping with such a behaviour a homomorphism of an algebra of 
signature I: into a homologous algebra, more precisely a l:-homomorphism. A surjective 
homomorphism is also called an epimorphism. For this generalization of common notions 
to partial mappings comp. also Broy, Wirsing 1980. 
Thus we can state: 
Every computational structure of signature I: is an epimorphic image of the term 
algebra wl:. 
3.2.2.3 A given element b of a computational structure A may be the image of many terms 
under the interpretation cpA. For example, in any algebra of the abstract type "Stack" 
(comp. 3.2.1.2, Example 1) the image of all the terms in the set 
{b, top(append(empty, b)), top(append(rest(append(empty, x)), b)), ... } 
is the same. The epimorphism cpA: Wl: -+A defines classes on Wl:. 
Thus there is an equivalence relation =(cpA) on Wl:. Two terms t and t' are equivalent, 
if they are interpreted identically in A: 
=(cpA) is even a :E-congruence relation, i.e. it is compatible with the operations: if 
t; =(cpA)tf (i = 1, ... , n) then also 
Because of this latter property, the set Wl: I =(cpA) of equivalence classes of =(cpA) can be 
viewed as a model, too, and it is easy to see that it is isomorphic to A. Wl: I=( cpA) is called 
the quotient Structure Of Wl: by =(cpA)• 
Thus we have the following result: 
Every computational structure of signature I: is isomorphic to a quotient structure of 
the term algebra Wl:, and vice versa. 
In other words, the quotient structures W1: I = 
(cpA) form a system of representants for 
all computational structures of signature I: and allow to single out all computational struc-
tures of a given abstract type (I:, ~). 
3.2.2.4 How are the various computational structures of a given signature related to each 
other? Since all computational structures of signature I: are epimorphic images of W1:, 
there can exist at most one (surjective) homomorphism ljf: A -+ B among any two A, B of 

3.2 Abstract Computational Structures and Abstract Types 
201 
them (and thus also a corresponding congruence relation =(w) such that B == AI =(w)). 
In this case we say that A is finer than B orB is coarser than A; this is also denoted by 
writing A __. B. 
The relation "finer" is obviously reflexive and transitive. If A is finer than Band B 
finer than A, then =(q>A) and =(q>8) are included in each other and therefore identical; 
this means that A and Bare isomorphic. Thus, "finer" defines a (partial) ordering of the 
classes of isomorphic computational structures of signature I:. 
We can now define: 
An abstract computational structure (abstract algebra) of a given signature I: is a class of 
isomorphic computational structures (algebras) of signature L The abstract computa-
tional structures of one and the same signature I: are (partially) ordered. 
For the rest of this section we shall speak of abstract computational structures only and 
consider the quotient structures of the term algebra as representants. We denote by [A] the 
abstract computational structure to which a (concrete) computational structure A belongs. 
3.2.2.5 We now consider those computational structures of signature I: which also satisfy 
the properties ~of an abstract type (I:, ~).According to what was said above, we restrict 
our discussion to abstract computational structures and denote the set of all these by 
CSr.,($· If ~ is empty, we simply get the set CSr., 0 of all computational structures of 
signature I:. In general, CSr.,f$ is a subset of CSr., 0• How can it be characterized? 
There is also a partial ordering induced among the abstract computational structures in 
CSr.,($· If among them there is a finest one, it is called the initial abstract computational 
structure8 and denoted by I r., lj; if there is a coarsest one, it is called the terminal abstract 
computational structure of CSr., 15 and denoted by Tr.,f5· The members of these classes, too, 
are called initial or terminal, respectively. 
Wr. itself is an algebra of the absolutely free type (I:, 0); [ Wr.l is, if we restrict ourselves 
to total operations, the initial abstract computational structure of CSr., 0• There is also a 
trivial abstract algebra 11 of this type, every carrier set of which is a singleton set. If we 
restrict ourselves to total operations, 11 is even terminal. 
[ Wr.J is the initial abstract computational structure of CSr.,'" if~ consists of inequalities 
only; likewise 1 is still the terminal abstract computational structure if ~ consists of 
equalities only, for an example see 3.2.1.2, 2(a). In general, however, the intial and the 
terminal abstract computational structures (if they exist at all) are different from [ W r.l and 
1, respectively9• 
If contradictory properties~ are required, CSr.,~< is empty. Classically one shows by giv-
ing a model that the set of properties is consistent, i.e. that no contradiction can be derived 
from the properties. Note, however, that the class CSr., lj of finitely generated models may 
be empty even if the set~ of properties is consistent10• 
CSr.,~< contains only a single abstract computational structure if and only if initial and 
terminal algebras exist and coincide; we then speak of a monomorphic abstract type. For 
8 The concepts are taken from category theory. 
9 Wirsing et al. 1980 and Wirsing, Broy 1980 have given rather general conditions for the existence 
of initial and terminal algebras. 
10 The restriction to CSr.,~< leads to the situation that there may be propositions which are valid in 
CSr.,~< but not provable by Q: (incompleteness theorem of Godel-Rosser). For such a proposition 
there is always an algebra which is not finitely generated and in which the proposition is wrong 
(GMel's completeness theorem for first-order predicate calculus). Comp. e.g. Shoenfield 1967. 

202 
3. Computational Structures 
examples see 3.2.5 and 3.2.6. In general it is difficult to arrange the properties such that 
exactly this border between polymorphic and empty abstract types is met. 
A normal form system is a system of terms representing the quotient classes of the 
initial algebra. The classical proof method for monomorphicity aims first at a normal form 
system and shows that different terms from the normal form system ("normal forms") 
remain different under any interpretation. 
The situation of CSr.~ and CSr,0 can be illustrated by a diagram (Fig. 3.2) showing the 
ordering of abstract computational structures by the relation "finer". The order interval 
CSu is hatched; it is assumed that initial and terminal algebras exist. 
Fig. 3.2 
3.2.2.6 Loosely speaking, in L-algebras strictly finer than an initial algebra Ir.~· elements 
are taken to be different which ought to be equal because of Q;. In L-algebras that are 
strictly coarser than a terminal algebra Tr.~· elements are equal which should be different 
because of Q;. 
In an initial algebra all elements are different which are different in some algebra of 
CSr.~; in a terminal algebra all elements are considered equal which are equal in some 
algebra of CSr.~· Hence an initial algebra contains the "maximum number" and a 
terminal algebra the "minimum number" of different elements. A terminal algebra is, 
from a practical viewpoint, the least redundant one, an initial algebra is the "richest" one. 
The initial algebra (if it exists) has the (theoreticp.lly relevant) advantage that all com-
putational structures can be derived from it by homomorphisms, i.e. by forming quotient 
structures. Moreover, two algorithms defined over a polymorphic type are certainly 
equivalent if for the initial algebra they show the same effect to the outside. 
At first sight one might thus be tempted to prefer the initial algebra to the terminal algebra. The 
following fact, however, would rather suggest giving priority to the terminal algebras: let A be an 
arbitrary structure from CSr.~· Then in every case a terminal algebra T from CSr.~ can be 
"represented" by means of A, i.e. there is a homomorphism of A into T; an initial algebra, however, 
can only be represented by A when A itself is initial. 
Which of the algebras is to be the model of an abstract type? We prefer to leave it un-
determined which algebra from CSr, ~is meant in the case of a polymorphic abstract type. 
This allows us to delay the decision which monomorphic abstract type is to be used even-
tually: this abstract type may be fixed later by adding further properties. 
3.2.2.7 If in the Principle of Generation we drop the restriction to finitely generated terms 
we obtain an extended theory which also comprises infinite objects. Already the example 
of infinite sequences of O's and L's leads to non-countable sets - O,L-sequences can be 
read as dual fractions and thus are of the same cardinality as the set of real numbers 
between 0 and 1. In order to stay within the class of countable and even enumerable sets 
one has to restrict oneself to those infinite terms which can in some way be described 

3.2 Abstract Computational Structures and Abstract Types 
203 
finitely. We have seen examples for this in 2.14; also the computable real numbers 
mentioned in 0.2.2 as well as computable infinite sequences fall into this class. 
3.2.3 Completeness of Properties 
3.2.3.1 The fact that there may be non-isomorphic algebras in the set CS'E.fli is not only in-
teresting for the theorist but has also important consequences in practice. For instance, let 
A and B be two algebras of the abstract type (:E, Q;) and t be an arbitrary term of the term 
algebra W'E ending in a primitive carrier 1)3;. Then the interpretation (i.e. the "evaluation") 
of t in A does not necessarily yield the same result as the interpretation of t in B. 
This means - as the terms of the term algebra represent exactly the expressions (i.e. 
the "programs") which can be formulated with the operations of the computational struc-
ture - that the same "program" can yield different results when evaluated, according to 
which algebra it is based on. This can be illustrated by a simple example: if we drop the 
property 
lawT: top(append(a,x)) = x 
\ 
in 3.2.1.2, Example 1, and thus change over to a smaller property set Q:', the operational 
essence of top is no longer completely fixed. We can then imagine e.g. two concrete com-
putational structures, in one of which top - as usual - yields the element appended last, 
whereas in the other top always yields a fixed element n. Both structures are of the (poly-
morphic) type (:E, Q:'). The interpretation of the term 
top(append(empty, x)) 
however either yields n or x. 
This suggests demanding at least that the interpretation of a term t e W'E yielding a 
primitive object, gives the same result in all algebras of CSI:.fli (over primitive carrier sets 
1)3;) 11. 
A sufficient criterion for this property is "sufficient completeness": The property set Q: 
of an abstract type (:E, Q:) is sufficiently complete 12 (Guttag 1975) if every term t e W'E end-
ing in a primitive carrier 1)3; can be "reduced" to an object of 1)3; using the properties in Q:. 
Counterexample: If we omit the property T as above, the term 
,top(append(rest(append(empty, x)), y))" 
can be reduced to 
,top(append(empty, y))", 
using the remaining properties, but cannot be reduced further, e.g. toy. 
11 This corresponds exactly to the real situation e.g. in a data base: the internal storage of the infor-
mation is of no interest, on the other hand the answers to all possible queries must be well defined. 
12 Since this definition does not correspond exactly to the usual notion of completeness in formal 
logic, Guttag has added the qualifier "sufficiently". 

204 
3. Computational Structures 
As that criterion is difficult to verify, Guttag (1975) has given a method of determining 
from the external form of ~ whether the properties are sufficiently complete or not -
provided one conforms to certain notational restrictions. Essentially all operations not 
ending in a defined carrier have to be specified in their effect on all operations ending in a 
defined carrier. We cannot go further into this method but only point out that all abstract 
types in this chapter fulfil the criterion of being sufficiently complete, unless the contrary is 
explicitly stated. 
3.2.3.2 Abstract types whose properties are not sufficiently complete are for example un-
avoidable if in a polymorphic abstract type an equivalence relation eq is to be introduced 
by the law 
law EQ: eq(a, b) <* a = b 
In this case, there are terms s, t such that s = t holds in some model A and --, (s = t) holds 
in some other model B; thus eq(s, t) yields true or false, resp. 
From this we obtain the (theoretically interesting) criterion for monomorphicity: If an 
abstract type (:E, ~)is such that it remains sufficiently complete after being extended by eq 
and EQ, it is either monomorphic or empty. 
In case the set of properties of an abstract type is not sufficiently complete, interpreta-
tion-invariant algorithms can still be formulated over the computational structures of this 
type provided they make use only of those operations which satisfy the condition that any 
term which is exclusively built with them and ends in a primitive carrier ~; can be reduced 
to an object of~;· For these algorithms it is irrelevant whether they are interpreted in a 
terminal, an initial or in any other computational structure of this polymorphic type. 
Of course, it may happen that in a polymorphic abstract type a special comparison with 
a fixed element n, i.e. the term 
a=n 
is interpretation-invariant. A corresponding (unary) operation can then be added to the 
signature without endangering the sufficient completeness of the properties, see isempty in 
3.2.5.1. 
In the case of a sufficiently complete set of properties which are written exclusively as 
identities over the operation symbols and free variables, the existence of (initial) models 
can always be proved. This total restriction to identities favoured by some authors is not 
absolutely necessary, as implications and inequalities in a restricted form do not endanger 
the existence of models and thus can also be permitted. 
There is incidentally no requirement that the property set be "minimal"; i.e. properties 
could easily be contained in~ which can be derived from the others. Such "superfluous" 
properties can even often lead to a better understanding of the abstract type in practice, 
comp. 3.1.4 (a). 
3.2.4 Concretization of an Abstract Type 
The theoretical considerations in the last sections allow us to draw conclusions for 
practical work with abstract types and computational structures. 

3.2 Abstract Computational Structures and Abstract Types 
205 
In order to show that a computational structure R of signature I: is a model of an 
abstract type (I:, ~)we must first prove that the operations of R satisfy all the properties of 
~. (Methods such as computational induction or structural induction will generally be 
necessary to do this.) If we succeed, R is indeed a model of the type (I:, ~)and it is called a 
concretization of the type (I:, ~). 
It is in general difficult to prove that a concretization R is terminal or initial. This lies 
outside the scope of this book. It is just as difficult to find a congruence relation = for a 
non-terminal R, such that Rl= is terminal. Specifying such a congruence relation, 
incidentally, is equivalent to giving an explicit routine for the coarsest equality allowed 
among all models of the type. 
In 3.6.2 we shall study numerous examples of concretizations. 
3.2.5 Notation and First Examples 
3.2.5.1 To describe an abstract type (comp. 3.2.1) we use a notation similar to the notation 
for (concrete) computational structures (comp. 3.1.3), with the difference that in the case 
of abstract types the signature must be completely specified in the heading 13 • In addition 
the parentheses are replaced by suitable keywords. 
The abstract type of the common stacks in 2.10.3 then reads as follows: 
type STACK =(mode x) stack x. empty, isempty, top, rest, append: 
mode stack x. 
funct stack x empty, 
funct (stack X) boo! isempty, 
funct ({stack x s: --, isempty(s) }) x top, 
funct ({stack X s: --, isempty(s) }) stack xrest, 
funct (stack x. x) stack x append, 
law R: rest(append(s, x)) = s, 
law T: top(append(s, x)) = x, 
law A: --, isempty(s) ,;. append(rest(s), top(s)) = s, 
law E1: isempty(empty), 
law E2: --, isempty(append(s, x)) 
endoftype 
where x is a parameter 14 and the properties are sufficiently complete (comp. 3.2.3). 
STACK can be shown to be monomorphic (see 3.2.2.5). 
Obviously s = empty implies isempty(s) because of E1. The reverse implication is 
proved by E2 and the Principle of Generation. 
In addition to the mode and operation indications specified in the heading (before the 
colon), which are to be available externally, "hidden" mode and routine specifications 
may occur (after the colon). For example this would apply to the case of an abstract type 
BSTACK which encompasses the concrete computational structure BS (comp. 3.1.3.3). 
13 The specification of functionalities in the case of concrete computational structures can be sup-
pressed for notational simplicity. For mnemonic reasons the indicants of the defined carriers 
usually bear the indicants of the imported parametric modes. 
14 According to the remark in 3.1.3 we should here more precisely speak of a type scheme. 

206 
3. Computational Structures 
Furthermore it is possible to express hierarchical dependencies between different ab-
stract types by replacing certain specifications and properties by a type indication 15 • 
Thus we now can describe the abstract type specified in 3 .2.1, Example 1, formally as 
type LSTACK "'(mode x) !stack x. empty!, isemptyl, top!, rest!, append!, length: 
(!stack x. empty!, isemptyl, top!, rest!, append!) isoftype STACK (X), 
funct (!stack x) nat length, 
law L1: length (empty!) = 0, 
law L2: length (appendl(s, x)) = length(s) + 1 
endoftype 
From the type indication 
(!stack x. empty!, isemptyl, top!, rest!, append!) isoftype STACK (X) 
both the functionalities, e.g. 
funct (!stack x. x) I stack l append! 
and the accompanying properties, e.g. 
law A: ' isemptyl(s) ,.;. appendl(restl(s), topl(s)) = s 
can easily be gained. 
Moreover, in this example we have used the possibility of re-naming (e.g. top! instead 
of top) which we will meet again and again. 
The type indication also permits a notational abbreviation of the heading of concrete 
computational structures. Thus for the heading of the (concrete) computational structure 
RSC of 3.1.3 we can now write 
structure RSC "' (mode X) LSTACK (X) 
The operation names defined by a structure specified in this way are already fixed by the 
corresponding type definition -
in other words, renaming is done by suitable new type 
definitions 16 • 
3.2.5.2 An abstract computational structure is written like a concrete one, one difference 
being that the body bracketed by I and J is missing; hence the transition from an abstract 
computational structure to a concrete one is the supplying of its missing body. 
15 The indication of belonging to a type (in short type indication) shows similarity to the operations 
for the "construction of theories" in Burs tall, Goguen 1977. A warning with regard to such type 
indications is appropriate at this point: if they are used one must be very careful not to generate in-
consistent property systems. 
16 This notation emphasizes the fact that the relevant statements about algorithms which are based 
on a computational structure are derived from the properties (and not from the formulation of the 
body); we have thus the possibility of exchanging the body of a computational structure without 
having to simultaneously change all the algorithms based on this computational structure. 

3.2 Abstract Computational Structures and Abstract Types 
207 
Furthermore, keywords like initial or terminal may express that the initial or the 
terminal abstract computational structure of the specified type is meant; some or that 
may indicate that some abstract computational structure or (for a monomorphic type) the 
abstract computational structure of the specified type is to be considered, for example 
structure STACK = (mode X) that STACK (x) 
Examples of abstract computational structures can be obtained by deleting the bodies 
in the examples SUIT, BIT and PLOTTER of concrete computational structures in 3.1. 
Other examples will be dealt with in the sequel. 
3.2.5.3 McCarthy's lists (object structure lisp of 2.9.1) can be described by an abstract 
type whose essential properties 17 show a striking resemblance to those of STACK. 
type LISP = (mode atom) lisp, car, cdr, cons, mklisp, mkatom, isatom: 
mode lisp, 
funct ({lisp I: --, isatom (I)}) lisp car, 
funct ({lisp 1: --, isatom (I)}) lisp cdr, 
funct (lisp, lisp) lisp cons, 
funct (atom) lisp mklisp, 
funct ({lisp 1: isatom(l)}) atom mkatom, 
funct (lisp) bool isatom, 
law CAR: 
car(cons(k, I)) = k, 
law CDR: 
cdr(cons(k, I)) = I, 
law CONS: 
--, isatom(l) ,;. cons(car(l), cdr(!)) = I, 
law ISATOM1: isatom(mklisp(a)), 
law ISATOM2: --, isatom(cons(k, 1)), 
law MKATOM: mkatom(mklisp(a)) = a, 
law MKLISP: 
isatom(l) ,;. mklisp(mkatom(l)) = I endoftype 
The objects generated by mklisp from the imported mode atom correspond to empty 
of STACK; the operation isatom corresponds to isempty. 
Due to the similarity of CAR, CDR, CONS with R, T, A (3.1.2) many of McCarthy's 
definitions (e.g. concatenation of lists) and theorems can be carried over to STACK. On 
the other hand the remaining properties reveal important differences. Note that LISP is 
homogeneous with respect to cons, car and cdr. 
3.2.5.4 Guided by the algebraic concept of a group we obtain an abstract type G (comp. 
3.2.1.2): 
type G = group, . o., inv, e: 
mode group, 
funct (group, group) group . o., 
funct (group) group inv, 
funct group e, 
law ASSOC: (a o b) o c = a o (b o c), 
17 Most of these properties are already specified in McCarthy 1960. 

208 
3. Computational Structures 
lawLI: 
inv(a) o a = e, 
lawRI: 
a o inv(a) = e, 
law INV: 
inv(inv(a)) = a, 
lawLN: 
e o a= a, 
lawRN: 
a o e =a 
endoftype 
Of course, every group satisfies the properties from ASSOC up to RN. However, due 
to the required Principle of Generation in 3.2.3, the trivial one-element group is the only 
abstract computational structure of type G; it is simultaneously initial and terminal, i.e. G 
is monomorphic provided we restrict ourselves to total operations 18 • 
If we add a further element x (with the help of the nullary operation funct group x) 
and the properties 
lawU: x * e, 
law Z2: x o x = e, 
we obtain the monomorphic abstract type Z2 of the (commutative) cyclic group of order 2: 
type Z2 = group, . o., inv, e, x: 
(group, . o., inv, e) isoftype G, 
funct group x, 
lawU: x*e, 
law Z2: x o x = e 
endoftype 
If we simply add the element x without any further property we obtain a polymorphic 
abstract type. Again, the trivial·one-element group is the terminal algebra; the "free group 
jj1", i.e. the (commutative) cyclic group of non-finite order isomorphic to the integers 
under addition (with x as "one" and e as "zero") is the initial algebra 19• In between, all 
cyclic groups of finite order are models, where A is finer than B (i.e. a homomorphism 
A -> B exists) if and only if the order of B divides the order of A. 
Taking U only, the one-element group is no longer a model; then there is no terminal 
computational structure. 
By adding an operation widen to the abstract type G we obtain the polymorphic 
abstract type of "groups generated by a set r.. of generators" 
type GENGROUP = (mode r..) group r.. •. o., inv, e, widen: 
(group r.. •. o. , inv, e) isoftype G, 
funct (r..) group r.. widen, 
law SEP: widen(x) = widen(y) 
=> 
x = y 
endoftype 
whose initial algebra is the freely generated group over a generating set '1..· The property 
SEP, i.e. the injectivity of widen, is superfluous for the initial algebra. 
18 The model 2(b) given in 3.2.1.2 is not a computational structure of this type since it does not 
satisfy the Principle of Generation. However, it is a computational structure of the abstract type 
Z2 to be defined in the sequel. 
19 Many details for finitely generated groups ("discrete groups") can be found in H. S.M. Coxeter, 
W. 0. J. Moser, "Generators and relations for discrete groups", 3rd ed., Springer 1972. 

3.2 Abstract Computational Structures and Abstract Types 
209 
Exercise 1: (a) Specify an abstract type MONOID which describes the algebraic concept of a 
"monoid", i.e. a semi-group with neutral element. 
(b) Give an alternative formulation of the abstract type GENGROUP in which the 
structural relation between monoids and groups is used to full advantage. 
3.2.5.5 The next example shows a typical polymorphic type. 
For the routine over STACK 
funct contains = (stack l a, l x) bool: 
if isempty(a) then false 
else if top(a) = x then true 
else contains(rest(a), x) fl fl 
(comp. 2.10.4) together with append and empty from STACK we can prove the properties 
C1 and C2 of the following type: 
type COST = (mode"/.) cost "f., empty, append, contains: 
mode cost "/.o 
funct cost l empty, 
funct (cost "/.o "/.) cost l append, 
funct (cost "/.o "/.) bool contains, 
law C1: --, contains(empty, x), 
law C2:contains(append(a, x), y) 
<* 
x = y v contains(a, y) endoftype 
For this type [WI;] is the initial algebra. A strictly coarser model is obtained by 
associating with each term from cost l the frequency index for the x-elements occurring in 
it: 
The two different terms 
append(append(append(empty, x), y), x) 
and 
append(append(append(empty, y), x), x) 
correspond to the same frequency index 
~; 
2T1 
the two different terms 
append(append(empty, x), y) 
and 
append(append(empty, y), x) 

210 
3. Computational Structures 
correspond to the same frequency index 
X I y 
0 
1T1 
A terminal model, finally, associates just the set of occurring elements with each term: 
The two different frequency indices 
* *
y 
and 
1 
1 
there correspond to the same set {x, y }. 
Thus COST is polymorphic. 
Exercise 1: Use COST for describing the construction of graphs. 
Exercise 2: Give a model for COST based on (right) sequences without repeated elements (comp. 
exercise 3.1.3-1). 
3.2.5.6 "Complex" objects are generally pairs of elements of an imported computational 
structure G which is assumed to be a group with the operation . o. and neutral element e, 
with operations defined componentwise. Hence we have the following type definition: 
type CPL = (mode X) cpl, plus, invers, neutr, re, im, cpl 
based on GENGROUP (x): 
mode cpl, 
funct (cpl, cpl) cpl plus, 
funct (cpl) cpl invers, 
funct cpl neutr, 
funct (cpl) group x re, 
funct (cpl) group x im, 
funct (group x. group x) cpl cpl, 
law RP: re(plus(a, b)) = re(a) o re(b), 
law IP: im(plus(a, b)) = im(a) o im(b), 
law RI: re(invers(a)) = inv(re(a)), 
law II: im(invers(a)) = inv(im(a)), 
law RN: re(neutr) = e, 
law IN: im(neutr) = e, 
law CR: cpl(re(a), im(a)) = a, 
law RC: re(cpl(x, y)) = x, 
law IC: im(cpl(x, y)) = y, 
law E: 
a = b <* (re(a) = re(b) A im(a) = im(b)) endoftype 
The hierarchical basing of a newly defined abstract type on already existing ("import-
ed") abstract types A, ... , Z is expressed by writing "based on A, ... , Z" at the end of the 
heading. 
The property E is deducible from the other properties; it expresses that the universal 
equality in each model of CPL depends on the universal equality in the respective 
underlying model Gin GENGROUP. 

3.2 Abstract Computational Structures and Abstract Types 
211 
The following group properties can be proved for every structure of this abstract type: 
law ASSOC: plus(a, plus(b, c)) = plus(plus(a, b), c), 
law LI: 
plus(invers(a), a) = neutr, 
law RI: 
plus(a, invers(a)) = neutr, 
law INV: 
invers(invers(a)) = a, 
lawLN: 
lawRN: 
plus(neutr, a) = a, 
plus(a, neutr) = a 
If, in addition to the above example, we demand that the imported computational 
structure G be a commutative ring of "numbers" with a multiplication operation . x. , then 
we achieve a type definition for "complex numbers" by additionally introducing 
funct (cpl, cpl} cpl mu/t, 
law RM: re(mult(a, b)) = re(a) x re(b) o invers(im(a) x im(b)), 
law IM: im(mult(a, b)) = re(a) x im(b) o im(a) x re(b) 
This type, in turn, describes a ring. If in particular G is the ring of the rational or real num-
bers, we obtain the corresponding complex numbers 20• 
3.2.6 Constructors and Selectors 
In the preceding example of the abstract type of "complex numbers" cpl is an operation 
which constructs one cpl object from two objects, i.e. a "constructor" operation. On the 
other hand re or im are "selector" operations decomposing a cpl object into its "compo-
nents". These three functions together with the test for equality and the corresponding 
laws (RC, IC, CR and E) are typical of computational structures whose elements are 
composite objects (comp. 2.5, 2.6). 
In general a (non-recursive) composite mode e.g. 
mode comp = (Xt sel1, 1..z sel2 , ••• , ln seln) 
can be represented as a computational structure of the following monomorphic abstract 
type: 
type COMP = (mode :x;1, mode Xz, ... , mode Xn) comp, cons, sel1, sel2 , ... , seln: 
mode comp, 
funct (:x;1, Xz, ... , Xn) comp cons, 
funct (comp} :x;1 sel1, 
funct (comp} 1..z sel2 , 
funct (comp} ln seln, 
law CS: 
cons(sel1 (a), sel2 (a), ... , seln(a)) = a, 
law SC1: sel1(cons(x1, x2, ... , xn)) = x1 , 
20 Comp. also 10.2.7 of the ALGOL 68 report. 

212 
3. Computational Structures 
law SCn: se/n(cons(x1, x2, ••• , xn)) = Xn, 
law E: 
a = b <> (se/1(a) = se/1(b) A se/2(a) = se/2(b) A 
... A seln(a) = se/n(b)) 
endoftype 
Exercise 1: Prove CS and E, using the Principle of Generation and the other laws. 
Exercise 2: Replace the introduction of the composite modes complex and date of 2.6 by the 
definition of suitable abstract types. 
Analogously, the abstract type of computational structures describing objects of a 
varying mode, say 
mode v =It 112 
can be specified as follows: 
type V = (mode It• mode I 2) v, inj1, injl> is1, is2, projJ> proj2 : 
mode v, 
funct (I1) v inj1, 
funct (l2} v inj2, 
funct (v) boo! is1, 
funct (v) bool is2, 
funct ({v x: is1(x)}} It proj1, 
funct ({v x: is2(x)}) 12proj2, 
law 111: is1(inh(x)), 
lawl12: 1 is1(inj2 (x)), 
law 121: 1 is2 (inj 1 (x)), 
law 122: is2 (inj2(x)), 
law PU: proj1 (inj1 (x)) = x, 
law Pl2: proj2(inj2 (x)) = x, 
law IP1: is1 (x) ,;. inj1 (proj1 (x)) = x, 
law IP2: is2(x) ,;. inj2(proj2(x)) = x, 
lawE: x = y <> 
if is1(x) A is1(y) thenproh(x) = proj1(y) 
0 is2(x) A is1(y) then false 
0 is1(x) A is2(y) then false 
0 is2(x) A is2 (y) thenproj2(x) = proj2(y) fi endoftype 
This abstract type is also monomorphic. Moreover, the laws E, IP1 and IP2 can be 
proved from the other laws. 
Finally, the abstract type of computational structures whose elements are certain recur-
sively composed objects can be specified. Here it is expedient to combine construction and 
injection as well as selection and projection into one single operation. 
A mode such as 
mode rcomp = (I sell> rcomp se/2) 1 empty 

3.3 Abstract Arrays 
213 
can be described by a monomorphic abstract type as follows (cons and empty are 
constructor operations): 
type RCOMP = (mode X) rcomp, cons, se/1, se/2, empty, isempty: 
mode rcomp, 
funct (x, rcomp) rcomp cons, 
funct ({rcomp a: .., isempty(a)}) xse/1, 
funct ({rcomp a: -, isempty(a)}) rcomp se/2, 
funct rcomp empty, 
funct (rcomp) bool isempty, 
law CS: -, isempty(a) ,;. cons(se/1(a), se/2 (a)) = a, 
law SC1: se/1(cons(x, a)) = x, 
law SC2: sel2 (cons(x, a)) = a, 
law E1: isempty(empty), 
law E2: 
-, isempty(cons(a, x)), 
law E: 
a= b <* if 
isempty(a) " 
isempty(b) then true 
0 -, isempty(a) " 
isempty(b) then false 
0 
isempty(a) " -, isempty(b) then false 
0 -, isempty(a) " -, isempty(b) then 
sel1(a) = se/1(b) "sel2(a) = se/2 (b) 
fi 
endoftype. 
As we can see, this abstract type corresponds exactly to the abstract type STACK defin-
ed in 3.2.5, apart from renamings. E can be proved from the other laws. 
For another example, see the abstract type LISP in 3.2.5.2. 
Conversely, all type definitions which - apart from the test for equality - contain 
only constructor and selector functions satisfying "reduction" properties such as CS and 
SC1, SC2 can be abbreviated by suitable mode declarations. 
Incidentally, the fact that in type definitions such "reduction" properties do not always 
occur (see e.g. type COST, 3.2.5.5) shows that the method of defining object structures by 
means of abstract types is far more general than the method of mode declarations by 
means of direct product and direct sum. 
It should be clear how in principle to introduce a monomorphic abstract type ARRAY 
(index, x) for arrays of the mode index array x (see 2.6.2). An essential restriction here is 
that Index is finite. 
3.3 Abstract Arrays 
As we stated at the beginning, the essential purpose of type definitions is to give an 
abstract description of structures which are usually described implicitly (by a particular im-
plementation) or only verbally. A typical example is afforded by different kinds of arrays 21 
such as the flexible arrays of ALGOL 68. 
21 The necessity of describing the subtle concept "array" more precisely has been pointed out by 
Hoare 1972 and Dijkstra 1976. 

214 
3. Computational Structures 
3.3.1 One-Side-Flexible Arrays 
3.3.1.1 The most important requirements on arrays extending beyond the trivial case of 
finite fixed index sets can be already demonstrated by the following example of "one-side-
flexible arrays". Here index is an object set with a linear Noetherian ordering (comp. 
2.6.2) which therefore contains a minimal element min; in addition the operations «succes-
sor of.» and (partially) «predecessor of.» are defined for index, they are named succ and 
pred. The most prominent representative of such an index set is nat with the minimal 
element 0. Such an index set is a computational structure whose abstract type can be 
described as follows: 
type INDEX =index, min, succ, pred, . ~-= 
mode index, 
funct index min, 
funct (index) index succ, 
funct ({Index i: i * min}) index pred, 
funct (index, index) boo! . ~. , 
law S1: succ(a) * min, 
law S2: succ(a) = succ(b) => a = b, 
law S3: a * min ,;. succ(pred(a)) = a, 
law S4: pred(succ(a)) = a, 
law LE: (succ(a) ~ succ(b)) <> (a ~ b), 
law M1: min ~a 
law M2: -, (succ(a) ~ min) 
law E: 
a = b <> (a ~ b A b 
~a) 
endoftype 
A model of this (monomorphic) abstract type can be found in 3.5.1.1. 
3.3.1.2 Using INDEX the abstract type of one-side-flexible arrays or indexed sequences can 
be defined: 
type FLEX = (structure INDEX, mode :x;: INDEX isoftype INDEX) 
index flex '1.- in it, is in it, ext, rem, hib, se/, aft: 
mode index flex '1.-
funct index flex :x: init, 
funct (index flex :x;) boo! isinit, 
funct (index flex '1.-
~:) index flex :x: ext, 
funct ({index flex :x: a: -, isinit(a)}) index flex x rem, 
funct ({index flex x a: -, isinit(a)}) index hib, 
funct ({index flex x a, index i: -, isinit(a) A i ~ hib(a)}) x se/, 
funct ({index flex x a, index i: -, isinit(a) A i ~ hib(a)}, x) 
index flex x aft, 
law HIB: hib(ext(j, m)) = if isinit(j) then min 
law ALT:i ~ hib(ext(j, m)) ,;. 
alt(ext(j, m), i, x) = 
else succ(hib(j)) fi, 
if i = hib(ext(j, m)) then ext(j, x) 
else ext(alt(j, i, x), m) fi, 

3.3 Abstract Arrays 
law SEL: i ~ hib(ext(f, m)) ~ 
se/(ext(j, m), i) = 
if i = hib(ext(j, m)) then m else se/(j, i) fi, 
law A: 
1 isinit(j) ,;. ext (rem(j), se/(j, hib(j))) = j, 
law R: 
rem(ext(j, m)) = j, 
lawl1: 
isinit(init), 
law 12: 
1 isinit(ext(j, m)), 
215 
law 13: 
1 isinit(j) ,;. (isinit(rem(j)) <* (hib(j) = min)) endoftype . 
Note that for the chosen indexing hib yields the "largest occurring index"; thus hib(init) is 
undefined and hib(ext(init, m)) = min. 
Exercise 1: Can A be derived from the other laws of FLEX? 
Exercise 2: For the abstract type FLEX, prove the following laws: 
law HIB2: 1 isinit(f) ,;, hib (rem (f)) = pred (hib (f)), 
law HIB3: (I isinit(f) 
~>. i ;a; hib(f)) ,;, hib(alt(f, i, m)) = hib(f), 
law SEL2: h isinit(f) ~ i ;a; pred(hib(f))) ,;, sel(rem(f), i) = sel(f, i), 
law SEL3: (I isinit(f) ~ i ;a; hib(f) A j ;a; hib(f)) 
,;, sel(alt(f, i, m), j) = if i = j then m 
else set (f, j) fi 
3.3.1.3 Although nat flex 1.. is not identical with nat array 1.. (comp. 2.6.2) we can define 
in analogy to ordinary arrays: 
funct . [.] "" (index flex 1.. a, index i: 1 isinit(a) A i ~ hib (a)) x: sel(a, i) 
On the other hand, FLEX contains no operation which corresponds to the explicit con-
structor of arrays (with fixed index set); if such a constructor (say for 
mode v "" nat [1 .. 4]) was 
here the term 
has to be written down explicitly. 
3.3.1.4 If one defines over FLEX 
funct selhib "" (index flex xf: 1 isinit(f)) x: sel(j, hib(j)) 
the affinity between one-side-flexible arrays and stacks becomes apparent: 
(Index flex ~ init, isinit, selhib, rem, ext) from the abstract type FLEX is a model of the 
abstract type STACK with the signature (stack ~ empty, isempty, top, rest, append), 
where the indices remain hidden. The laws A, R, E1 and E2 are transliterations, while T re-
sults from SEL after substituting hib(ext(j, m)) fori. We may say that STACK can be im-
plemented by FLEX (with an arbitrary index of type INDEX). 

216 
3. Computational Structures 
The operation aft can be based on the operations in it, hib, ext and se/hib: from AL T 
and R the following recursive routine results: 
fUnCt aft "'(indeX fleX 1,.f, indeX i, 1.. m: I isinit(j) A i ~ hib(j)) indeX fleX 1..: 
if i = hib(j) then ext(rem(j), m) 
else ext(alt(rem(j), i, m), selhib(j)) fi 
Exercise 3: Give a recursive definition of sel, based on selhib and rem. 
3.3.1.5 To FLEX one can add an operation 
funct ({index flex 1.. a: --, isinit(a) }) index flex 1.. truncshift 
(which corresponds to upper, comp. 2.10). It effects a left shift of the indexing and 
removal of a[min] according to the properties 
law HIBS: --, isinit(a) ,;. 
if hib(a) = min then truncshift(a) = init 
else hib(truncshift(a)) = pred(hib(a)) fi 
law SELS: succU) ~ hib(a) ,;. sel(truncshift(a), j) = se/(a, succU)) 
This suggests the introduction of another operation 
funct se/min = (index flex xf: --, isinit(j)) x: sel(j, min) 
(which corresponds to bottom). 
3.3.2 Two-Side-Flexible Arrays 
It is fairly obvious how the abstract type FLEX has to be extended in order to define two-
side-flexible arrays which can grow and shrink in two directions: a 'symmetric' index set 
indexs is to be introduced and the operations hiext and /oext for extending upwards and 
downwards, the operations hirem and lorem for contraction. Besides hib we have lob re-
presenting the smallest occurring index. 
For the origin of indexing, we require that 
hib(hiext(init, x)) = /ob(hiext(init, x)) = origin 
and 
hib(loext(init, x)) = lob(loext(init, x)) = pred(origin), 
where origin is a distinguished element of the index set, e.g. 0 for int. 
Apart from some further convenient operations which can be based on those mention-
ed, we want in this way to specify Dijkstra's "arrays" (Dijkstra 1976, Chap. 11) in the 
form of an abstract definition. Now, indexs is still a linearly ordered object set, but 
without a minimal element; this object structure could be described by the following type 
definition: 

3.3 Abstract Arrays 
217 
type INDEXS = indexs, origin, succ, pred, . ~-, . ~-: 
mode indexs, 
funct lndexs origin, 
funct (indexs) indexs succ, 
funct (indexs) indexs pred, 
funct (indexs, indexs) bool . ~-, 
funct (indexs, indexs) boo I . ~., 
law S2': 
pred(x) = pred(y) =o x = y, 
law S2: 
succ(x) = succ(y) =o x = y, 
law S4': 
succ(pred(x)) = x, 
law S4: 
pred(succ(x)) = x, 
law LEi: succ(x) ~ succ(y) # x ~ y, 
law LE2: x ~ x, 
law LE3: 
1 (x ~ pred(x)), 
law LE3': 1 (succ(x) ~ x), 
law LE4: x ~ y => x ~ succ(y), 
law LE4': x ~ pred(y) =ox ~ y, 
law E: 
x = y # (x ~ y 
A y ~ x), 
law GE: x ~ y # y ~ x 
endoftype 
Note that the nullary operation origin does not figure in the laws. For a model of this 
(under restriction to total operations monomorphic) abstract type see 3.5.4. 
For any computational structure of this abstract type the validity of the following pro-
positions can be shown for every element j of indexs: Both 
( {indexs i: i ~ j}, j, pred, succ, . ~.) and 
({indexs i: i ~ j}, j, succ, pred, . ~.) 
match the signature and satisfy the laws of the abstract type INDEX, if the operations are 
suitably restricted. 
Unfortunately, this is not enough for a suitable definition of two-side-flexible arrays. 
The reason is that - in contrast to FLEX - not all objects can be expressed by init, hiext 
and loext alone. In fact, hib(a) ~ pred(origin) may hold for an object a; it can then only 
be represented using hirem too. In other words: hirem (and correspondingly lorem) are in-
dispensable operations in constructing terms for two-side-flexible arrays. 
A definition which followed this argument in a straightforward manner would, how-
ever, lead to quite a voluminous and not very transparent system of laws. This can be 
avoided if the empty array is not rigidly coupled with the index origin, but instead every 
element from the index set is allowed as origin of indexing; this implies a parametrization 
of init. Thus we have the following definition for two-side-flexible arrays: 
type BIFLEX =(structure INDEXS, mode x: INDEXS isoftype INDEXS) 
indexs biflex X. init, isinit, hiext, /oext, hirem, lorem, hib, lob, sel, aft: 
mode lndexs biflex 'f., 
funct (indexs) indexs biflex x init, 
funct (indexs biflex l) bool isinit, 
funct (indexs biflex x. x) indexs biflex x hiext, 

218 
3. Computational Structures 
funct (indexs biflex x, :x,) indexs biflex :x. loext, 
funct ({indexs biflex :x. a: --, isinit(a)}) indexs biflex :x. hirem, 
funct ({indexs biflex :x. a: --, isinit(a)}) indexs biflex :x,lorem, 
funct (indexs biflex :x. a) indexs hib, 
funct (lndexs biflex :x. a) lndexs lob, 
funct ({indexs biflex :x. a, indexs i: lob(a) ;;;; i ;;;; hib(a) }) :x. set, 
funct ({indexs biflex :x,a, indexs i, :x,x: lob(a);;;; i;;;; hib(a)}) 
indexs biflex :x. aft, 
law 1: 
isinit(a) ~ --, (lob(a) ;;;; hib(a)), 
law Hi: hib(init(i)) = pred(i), 
law H2: hib(hiext(a, x)) = succ(hib(a)), 
law H3: hib(loext(a, x)) = hib(a), 
law L1: 
lob(init(i)) = i, 
law L2: 
lob(hiext(a, x)) = lob(a), 
law L3: 
lob(loext(a, x)) = pred(lob(a)), 
law S1: 
lob(a) ;;;; i ;;;; hib(hiext(a, x)) ,;. 
sel(hiext(a, x), i) = 
if i = hib(hiext(a, x)) then x 
else sel(a, i) fi, 
law S2: 
lob(loext(a, x)) ;;;; i ;;;; hib(a) ,;. 
sel(loext(a, x), i) = 
if i = lob(loext(a, x)) then x 
else sel(a, i) fi, 
law A1: 
lob(a) ;;;; i ;;;; hib(hiext(a, x)) ,;. 
alt(hiext(a, x), i, y) = 
If i = hib (hiext(a, x)) then hiext(a, y) 
else hiext(alt(a, i, y), x) fi, 
law A2: 
lob(loext(a, x)) ;;;; i ;;;; hib(a) ,;. 
alt(loext(a, x), i, y) = 
if i = lob(loext(a, x)) then loext(a, y) 
else loext(alt(a, i, y), x) fi, 
law L01: lorem(loext(a, x)) = a, 
law L02: lorem(hiext(a, x)) = 
if isinit(a) then init(succ(lob(a))) 
else hiext(lorem(a), x) fi, 
law HI1: hirem(hiext(a, x)) = a, 
law HI2: hirem(loext(a, x)) = 
if isinit(a) then init(hib(a)) 
else loext(hirem(a), x) fi 
endoftype 
init and se/ correspond respectively to the 0-tuple 0 and the selection . [ .] in ordinary 
arrays; hiext and loext correspond to the special constructors for extending fixed arrays 
discussed in 2.15.1. 
Exercise 1: For the abstract type BIFLEX show that isinit(init(i)) holds and that 
hirem(loext(init(i), x)) = init(pred(i)) 
lorem(hiext(init(i), x)) = init(succ(i)) 

3.3 Abstract Arrays 
219 
It may be of advantage to include an operation 
funct (indexs biflex x) indexs biflex x shift 
which permits shifting of the indices to the "left". Then we need the additional properties 
law !NITS: shijt(init(i)) = init(pred(i)), 
law HIBS: hib(shijt(a)) = pred(hib(a)), 
law LOBS: lob(shijt(a)) = pred(lob(a)), 
law SELS: lob(a) ;;i! succU) ;;i! hib(a) .;, sel(shijt(a), j) = sel(a, succU)) 
Exercise 2: Can shift be based on the operations of BIFLEX? 
Exercise 3: Give a definition of files and rolls based on BIFLEX. 
3.3.3 Aggregates 
The following abstract type of aggregates called GREX 22 is less elaborate than that of 
flexible arrays. This type admits every (not necessarily ordered or finite) recursively 
enumerable object set as an index set (comp. also Guttag 1975 and Wulf eta!. 1976, who 
confusingly use the term VECTOR). Instead of indexing one also speaks here of naming or 
addressing. A technical realization is associative storage. This computational structure can 
be found in Hoare, Wirth 1973 where it is described in a fashion oriented towards formal 
logic. In practice aggregates serve to establish functional relations. 
In order to dispense with the ordering of the index set we first replace aft and ext as well 
as hiext and loext by a more general operation put whose domain is not restricted within 
the index set. Furthermore, the notions of "least" and "greatest" index are no longer 
meaningful. Thus, the operations hib and lob are dropped and for set a more general 
parameter restriction is introduced using the predicate isaccessible: In any object a of the 
mode to be defined the component named by i can be accessed, i.e. isaccessible (a, i) holds, 
if during the construction of a this component has been treated (by put) at least once. 
By renaming set as get (in order to indicate the wider domain) we obtain (with an 
arbitrary v instead of Index): 
type GREX "" (mode v, mode X) v grex 'JJ vac, put, get, isaccessible: 
mode v grex x. 
funct v grex x vac, 
funct ( v grex 'JJ v, x) v grex x put, 
funct ({v grex x g, vi: isaccessible(g, i)}) x get, 
funct (v grex 'JJ v) bool isaccessible, 
law GP: 
isaccessible(put(g, i, x), j) .;, 
get(put(g, i, x), j) = if i = j then x 
else get(g, j) fi, 
law NACC: ---, isaccessible(vac, i), 
law ACC: 
isaccessible(put(g, i, x), j) # 
i = j v isaccessible(g, j) endoftype 
22 From the Latin grex: herd, flock - the root of "aggregate". 

220 
3. Computational Structures 
For get(g, i) one also may write g[i] where 
funct .[.] = (v grex 1.. g, vi: isaccessible(g, i)) x: get(g, i) 
Exercise 1: Can the term x be recovered from put(put(put(vac, i, x), j, y), i, z) using the operations 
of the abstract type GREX? 
Exercise 2: Add to GREX an operation 
funct ({v grex x g, vi: isaccessible(g, i)}) v grex x clear 
which for a given index i removes from the aggregate the component named by i. 
This type definition again shows clearly the difference between initial and terminal 
algebra. The following property 
law PP: put(put(g, i, x), j, y) 
if i = j then put(g, i, y) 
else put(put(g, j, y), i, x) fi 
holds for the terminal algebra but not for the initial algebra. 
WI: is an initial model; in this model "history is fully remembered". A non-isomorphic 
model is obtained using tables: the columns are headed by the indices and into the columns 
are entered the elements in the order in which they appear as arguments of put in the terms 
(this can be viewed as a family of stacks). In this model the two different terms 
put(put(put(vac, 3, 'a'), 5, 'b'), 3, 'c') and 
put(put(put(vac, 5, 'b'), 3, 'a'), 3, 'c') 
have the same interpretation 
... 2 
6 ... 
This model can be modified into a terminal one if one uses an eraser when entering ele-
ments into the columns and thus "completely forgets the history". The terms shown above 
as well as the terms 
put(put(vac, 5, 'b'), 3, 'c') and put(put(vac, 3, 'c'), 5, 'b') 
then all have the interpretation 
... 2 
6 ... 

3.4 Sequence-Type Computational Structures 
221 
With respect to possibilities of realization the polymorphism of this type is thus quite 
useful. 
Remark: In data bases, in addition to the "sequential files" belonging to file and roll 
of 2.11.2, FLEX and BIFLEX together with fixed arrays (2.6.2) are "files with selector-
sequential access". STACK, DECK, SEQU (comp. 3.4) are known as "files with strictly 
sequential access", and GREX and fixed compounds (2.6.1) as "files with direct access". 
3.4 Sequence-Type Computational Structures 
When specifying an abstract type, one of the most important decisions is to determine 
which operations should be primitive, i.e. parts of the signature, and which should only be 
added "outside", based on these primitive operations. Depending on this decision we 
obtain different but closely related abstract types. The methods and consequences of such 
variation become particularly clear in the class of sequence-type structures. 
3.4.1 Stack, Deck and Queue 
3.4.1.1 We will start out from the abstract type STACK of 3.2.5. We can eliminate the 
typical asymmetry of this structure by including the operations at the "wrong end", 
bottom, upper and stock. 
If these operations are added "outside" they must be based on the primitives top, rest 
and append in a rather inefficient way, e.g. (comp. 2.10) 
funct stock "' (stack 1. a, 1. x) stack x: 
if isempty(a) then append(empty, x) 
else append(stock(rest(a), x), top(a)) fi 
Then an implementation of STACK which is already symmetric as desired (e.g. by 
linear two-way lists as in 2.14.1) requires stock to be implemented by this very inefficient 
recursion mechanism. On the other hand, if stock is included in the operations of the 
signature and characterized axiomatically, efficient solutions are possible (e.g. by using the 
properties of a two-way list). 
The price we have to pay for greater flexibility, however, is a larger set of properties (to 
be proved upon implementation). Specifying the required new properties is generally not 
very difficult as they are usually very closely related to the (recursive) routines. 
Thus for the routine contains defined in 2.10.2, we can derive - besides the properties 
law C1: -, contains(empty, x), 
law C2: contains(append(a, x), y) 
<* x = y v contains(a, y) 
already used in 3.2.5.5 - also 
law C3: -, isempty(a) =* contains(a, top(a)) 

222 
3. Computational Structures 
Similarly, from the routine delete which also has been defined in 2.10, 
funct delete = (stack x s, x x) stack x: 
if isempty(s) then empty 
else if top(s) = x then rest(s) 
else append(delete(rest(s), x), top(s)) fi fi 
we can derive the properties 
law D1: 
delete(empty, x) = empty, 
law D2: 
delete(append(s, x), y) = If x = y then s 
else append(delete(s, y), x) fi, 
law REST: -, isempty(s) ,;. rest(s) = delete(s, top(s)) 
By adding these properties and the specifications 
funct (stack '1.J x) bool contains, 
funct (stack '1.J x) stack x delete 
to the definition of STACK we reach the definition of a new abstract type SC, which 
defines the same object set as STACK. 
Finally, in SC the operation rest and the laws R and REST could be omitted and rest 
could be defined "outside": 
funct rest = (stack x s: -, isempty(s)) stack x: delete(s, top(s)) 
In consequence, A is to be replaced by 
law ID: -, isempty(s) ,;. append(delete(s, top(s)), top(s)) = s 
Thus, we obtain a new type CODEL with the heading 
type CODEL = (mode X) codel '1.J empty, isempty, top, append, contains, delete: 
The functionalities are obtained from above, the laws are T, ID, E1, E2, C1, C2, C3, D1, 
D2. 
3.4.1.2 Knuth has introduced the name deque23 for stack objects having symmetrical 
"access" properties. deck, referring to a deck of cards is also quite suitable: a card can be 
removed both from the top and the bottom. Thus we define: 
type DECK = (mode X) deck '1.J empty, isempty, top, rest, append, bottom, upper, 
stock: 
(deck '1.J empty, isempty, top, rest, append) isoftype STACK (x}, 
(deck '1.J empty, isempty, bottom, upper, stock) isoftype STACK (x), 
23 "Double-ended queue". 

3 .4 Sequence-Type Computational Structures 
law RS: rest(stock(d, m)) = if isempty(d) then empty 
else stock(rest(d), m) fi, 
law TS: top(stock(d, m)) = if isempty(d) then m 
else top(d) fi, 
law UA: upper(append(d, m)) = if isempty(d) then empty 
else append(upper(d), m) fi, 
law BA: bottom(append(d, m)) = If isempty(d) then m 
223 
else bottom(d) fi endoftype. 
The symmetry of this structure is obvious from the specified properties. Compared to 
STACK, the set of properties has expanded to more than twice its size as the relationship 
between "non-compatible" operations must be explained (properties RS to BA). 
The symmetry also becomes manifest in an affinity between decks and two-side-flexible 
arrays: For an arbitrary element i of index, 
(index biflex x. init(i), isinit, selhib, hirem, hiext, sellob, lorem, loext) 
match the signature and satisfy the laws of DECK, provided all terms initU) are identified; 
the indices again remain hidden (comp. 3.3.1.4). 
In addition to "enriching" a computational structure with new operations it can also be 
"impoverished" by removing operations. In this way possibly more efficient implementa-
tions can be achieved. We will return to such type changes in 3.6.1. Furthermore it is 
advisable, for practical use, not to include more primitives in an abstract type than are 
really needed "outside". 
3.4.1.3 If in DECK we restrict ourselves to top, rest and stock - in addition to empty and 
isempty -
the "access" behaviour is that of a queue 24• We speak usually of the 
"beginning" and the "end" of a queue (and often use the name front instead of top, and 
rear instead of rest): 
type QUEUE = (mode X) queue x. empty, isempty, top, rest, stock: 
mode queue x. 
funct queue x empty, 
funct (queue X) boo! isempty, 
funct ({queue x q: -., isempty(q)}) x top, 
funct ({queue x q: -., isempty(q)}) queue x rest, 
funct (queue x. X) queue x stock, 
law RS: rest(stock(q, x)) = if isempty(q) then empty 
else stock(rest(q), x) fi, 
law TS: top(stock(q, x)) = if isempty(q) then x 
else top(q) fi, 
law E1: isempty(empty), 
law E2: -., isempty(stock(q, x)) 
endoftype. 
Dual to this definition the structure of queues can of course also be based on bottom, 
upper and append; the two versions are identical (apart from renaming). 
24 Also called "circular store" or FIFO list (first-in-first-out). 

224 
3. Computational Structures 
The type DECK can now be written in a simpler way by means of the abstract type 
QUEUE: 
type DECK = (mode X) 
deck x. empty, isempty, top, rest, append, bottom, upper, stock: 
(deck x. empty, isempty, top, rest, append) isoftype STACK (X), 
(deck x. empty, isempty, bottom, upper, stock) isoftype STACK (x), 
(deck x. empty, isempty, top, rest, stock) isoftype QUEUE (x), 
(deck x. empty, isempty, bottom, upper, append) isoftype QUEUE (X) 
endoftype 
3.4.2 Excursus: Divisibility Theory in Semi-Groups 
The structures of sequences and words to be discussed in the next section as well as the 
number-like structures in 3.5 have the properties of semi-groups. For that reason an intro-
ductory comment on some properties of semi-groups seems appropriate. 
In an arbitrary monoid M = (M, o, e) we define the predicates 
b lp a <* def 3 c: a = b o c 
b rp a <* def 3 c: a = c o b 
and clearly we have the 
(" b is left part of a"), 
(" b is right part of a") 
Theorem: lp and rp are reflexive and transitive, i.e. quasi-orderings. 
In addition 
Theorem: e is the least element for lp and rp, i.e. 
v a EM: e lp a, e rp a 
obviously holds. 
Furthermore we have 
Theorem: .if lp (rp) is an ordering, then e is indivisible, i.e. 
boa=e 
=> a=e=b 
(Proof: Let b o a = e. Then b lp e (a rp e). Because of e lp b (e rp a), e = b (e = a) 
holds. Frome = b follows a = e (from e = a follows b = e).) 
In addition to this necessary condition we also have a sufficient condition by the 
Theorem: If M is left-cancellative (right-cancellative) and e is indivisible, i.e. if 
x o a = x o b 
=> 
a = b and b o a = e 
=> 
a = e = b, 
then lp (rp) is an ordering. 
(Proof: a lp b 1\ b lp a => 3 x, y: a o x = b 1\ b o y = a 
=> 
b o y o x = b 
=> 
e = y o x 
=> 
x = e = y =>a = b.) 

3.4 Sequence-Type Computational Structures 
Furthermore we obviously have the 
Theorem: lp (rp) is left-compatible (right-compatible) with o, i.e. 
alp b => (d o a) lp (d o b) 
225 
A free monoid is 25 right- and left-cancellative and has an indivisible neutral element. 
Both lp and rp are then orderings, where lp is left-compatible and rp right-compatible with 
o. In general, however, lp is not right-compatible (and rp not left-compatible) with o. 
lp and rp collapse into one relation p if M is commutative. If M is commutative and 
cancellative with an indivisible neutral element e, then (M, o, e, p) is a commutative 
ordered monoid. 
Algorithms for lp or rp are dealt with in connection with the computational structures 
of the following section. 
Exercise 1: How can the operation lp be defined in STACK, based only on empty, isempty and ap-
pend? 
3.4.3 Sequence and Word 
3.4.3.1 For a computational structure of the abstract type DECK a routine for the con-
catenation of two objects deck x a, deck x b can be easily be defined, e.g. (comp. 
1.4.1 (f)): 
funct cone = (deck x a, deck x b) deck x: 
if isempty(a) then b 
else append(conc(rest(a), b), top(a)) fi 
By analogy with the transition from STACK to DECK, cone can also be included in the 
type definition. To explain the meaning of this new operation, we must describe its inter-
action with the operations rest, top and upper, bottom. We obtain the abstract type SEQU 
of sequences of elements of mode x: 
type SEQU = (mode x) 
sequ "h empty, isempty, top, rest, append, bottom, upper, stock, cone: 
(sequ "h empty, isempty, top, rest, append, bottom, upper, stock) 
isoftype DECK (X), 
funct (sequ "h sequ x) sequ x cone, 
law RC: 
--, (isempty(s) " isempty(t)) ~ 
law TC': 
law Lt: 
rest(conc(s, t)) = if isempty(s) then rest(t) 
else conc(rest(s), t) fi, 
--, (isempty(s) 
1\ isempty(t)) ,;. 
top(conc(s, t)) = if isempty(s) then top(t) 
else top(s) fi, 
--, (isempty(s) 
1\ isempty(t)) ,;. 
upper(conc(s, t)) = if isempty(t) then upper(s) 
else conc(s, upper (f)) fi, 
25 For details see 3.4.3.3. 

226 
3. Computational Structures 
lawBC: 
..., (isempty(s) " isempty(t)) ,;. 
bottom(conc(s, t)) = if isempty(t) then bottom(s) 
else bottom (t) fi, 
law E3: 
isempty(conc(s, t)) <> isempty(s) " isempty(t), 
law ASSOC: conc(s, conc(t, u)) = conc(conc(s, t), u), 
law LN: 
conc(empty, s) = s, 
law RN: 
conc(s, empty) = s, 
law W: 
append(empty, x) = stock(empty, x) 
endoftype 
The property E3 means that empty is indivisible (comp. 3.4.2). 
The last four properties can be derived from the others. This is shown by the example 
of LN and W: 
If isempty(s) holds, LN follows immediately from E3; otherwise we have 
conc(empty, s) =(A,EJ) append(rest(conc(empty, s)), top(conc(empty, s))) 
= (Rc, rq append(rest(s), top(s)) 
W is shown as follows: 
append(empty, x) =(RS,TS) append(rest(stock(empty, x)), top(stock(empty, x))) 
=(A) stock(empty, x) 
Exercise 1: Derive ASSOCfrom the other properties (Skolem 1923). 
Exercise 2: Show that W already holds in DECK. 
Incidentally, from LN and the properties of isempty we obtain 
isempty(a) => conc(a, b) = b 
and from RC and TC 
..., isempty(a) ,;. rest(conc(a, b)) = conc(rest(a), b) 
" top(conc(a, b)) = top(a) 
thus with A of DECK or STACK 
..., isempty(a) ,;. conc(a, b) = append(conc(rest(a), b), top(a)) 
follows. Combining these we again obtain the recursive routine cone above. 
In infix notation, concatenation is frequently denoted by the symbol &: 
funct .&. = (sequ 1. a, sequ 1. b) sequ x: conc(a, b) 
Comp. also Table 1.3.1. 

3.4 Sequence-Type Computational Structures 
227 
Exercise 3: Show analogously that the following repetitive routine for cone can be derived from the 
properties of SEQU: 
funct cone .. (sequ x a, sequ x b) sequ x: 
if isempty(a) then b 
else conc(upper(a), append(b, bottom (a)) fi 
Every computational structure of the abstract type SEQU "comprises" computational 
structures of the abstract types DECK, STACK and QUEUE (as well as of the abstract 
type WORD to be discussed in the sequel) and those of the abstract types STACK and 
QUEUE even twofold, comp. the definition of DECK in 3.4.1.3. 
Exercise 4: Specify all type indications which describe the relationship between SEQU on the one 
hand and DECK, STACK and QUEUE on the other hand. 
3.4.3.2 Operations /part and rpart for lp or rp of 3.4.2 can be described efficiently in 
STACK, DECK and SEQU, e.g. 26 
funct /part = (sequ l a, sequ l b) bool: 
if isempty(a) then true 
elsf isempty(b) then false 
else top(a) = top(b) tA lpart(rest(a), rest(b)) fl 
This algorithm is clearly related to the algorithm for lexicographic comparison in 
linearly ordered "f_, 
funct le = (sequ l a, sequ l b) bool: 
if isempty(a) then true 
elsf isempty(b) then false 
else top(a) < top(b) 'V' 
(top(a) = top(b) tA le(rest(a), rest(b))) fl 
Exercise 5: Show that lpart(a, b) => le (a, b). Show that lpart and le coincide iff xis a singleton or the 
empty set. 
Exercise 6: Specify an algorithm cutoff for "cutting off a sequence b from a sequence a" under the 
assertion /part( a, b) and show that cutoff and cone neutralize each other. 
3.4.3.3 The operation cone of SEQU, together with the element empty, has the property of 
a semi-group with neutral element, i.e. of a monoid 27• If we discard all the operations from 
SEQU except cone and empty, the new abstract type of words over the alphabet l results: 
type WORD = (mode "/_) word "/_, empty, isempty, widen, cone: 
mode word 'l· 
funct word l empty, 
funct (word l) bool isempty, 
funct ("/_) word l widen, 
26 The statement 
v a, be M C sequ x: -, lpart(a, b) 
is called the "Fano-condition forM" in 
coding theory. 
27 Comp. exercise 3.2.5-1 (a). 

228 
3. Computational Structures 
funct (word x. word x) word x cone, 
law ASSOC: conc(u, conc(v, w)) = conc(conc(u, v), w), 
law LN: 
conc(empty, w) = w, 
lawRN: 
lawE1: 
lawE2: 
lawE3: 
lawSEP: 
conc(w, empty) = w, 
isempty(empty), 
' isempty(widen(m)), 
isempty(conc(u, v)) => (isempty(u) A isempty(v)), 
widen(m) = widen(n) => m = n 
endoftype 
Compare this abstract type with GENGROUP in 3.2.5.4. 
Without the operation widen (which effects the transition form an object x of x to the word con-
sisting of x only), according to the Principle of Generation the defined carrier of every computational 
structure of this type would contain the single element empty only. This transition, written word x: x 
in Chap. 2, is realized in SEQU by append(empty, x) or stock(empty, x) and must be rendered 
possible in WORD (where neither append nor stock is available) by the additional operation widen. 
The injectivity of widen expressed by SEP is satisfied automatically in an initial algebra; however, 
SEP guarantees that also in all other models the word-objects obtained by widening different x-
objects are not identified. Nevertheless, WORD is not monomorphic: The free commutative monoid 
over x with commutative operation cone is an abstract computational structure of type WORD(x) 
different from the initial algebra. 
Exercise 7: Give a concrete computational structure for the free commutative monoid over finite x 
(hint: frequency index, comp. 3.2.5.5). 
In the theory of formal languages a computational structure of type WORD is defined by the star 
operation: 
Let A be an arbitrary set. Then 
A • = def U A", where A 0 = def { ¢ }, A i+ 1 = def A ®A i 
neiN 
and ® denotes the cartesian product. An element (x1, (x2, ( .•. <x., ¢) ... ) ) ) of A • is usually 
written as x1 x2 •.• x •. Now let 
empty = def ¢, 
widen (x) = def (X, ¢) and 
conc(x1 ••• xk,y1 ••• y1) =def x1 ••• xky1 ••• y1, 
conc(w, ¢) =def cone(¢, w) =ctefw 
With these definitions, the word algebra x• is an initial algebra of the abstract type WORD (X)· 
The monoid x • has the algebraic property of being free over x. This means 28 that 
(1) every element w of the monoid x• is obtained by continued concatenation of a finite number of 
elements of x: 
(2) for every semigroup H every mapping from x into H can be uniquely extended to a semigroup-
homomorphism from x• into H. 
The second property holds, because, according to the definition of term equality, 
28 For the relevant concepts see also Franco P. Preparata, Raymond T. Yeh, "Introduction to 
Discrete Structures", Addison-Wesley 1973, p. 173-176. The properties (1), (2) originate from 
Dyck 1882. 

3.4 Sequence-Type Computational Structures 
229 
implies 
n=m and X;=Y;U=1, ... ,n) 
Because of the Principle of Generation, property (1) is also satisfied. 
The fact that the initial algebra is unique (up to isomorphism) corresponds to the well-known 
Theorem: A free monoid over a basic set is specified (up to isomorphism) by that basic set (hence we 
also call x• the free monoid of the words over the set x). 
3.4.3.4 Although WORD can be based on STACK, neither STACK nor SEQU can be bas-
ed algorithmically on WORD: in computational structures of the type WORD there are no 
direct operations available to break the elements apart. E.g. top can then only be imple-
mented exhaustively (with the help of the universal equality relation) provided xis of finite 
cardinality. Exhaustive algorithms for lp and rp of 3.4.2 are also quite inefficient over the 
abstract type WORD(x). A solution would be to include lp and rp as primitives within the 
type WORD. 
In the abstract type WORD (X) the commutative law holds if and only if xis a singleton 
or the empty set; then lp and rp collapse into one ordering p compatible with cone. We will 
meet this case again when dealing with stroke number (3.5.1). 
3.4.4 Forgetful Functors 
Let m x be a mode defined over a basic mode x using only direct sum and direct product. 
word X has the property that for every such object set m x, there is a forgetful function 
v: m x _. word x 
which "eliminates brackets" by mapping an equivalence class of m x with respect to as-
sociativity into a corresponding element of word 1.29• This function is characterized by the 
fact that it is compatible with replacing every constructor by multiple concatenation. For 
case 1. we have e.g. the forgetful function 
funct cascforget "' (case 1. x) word 1.: 
if X= 0 
then empty 
else cascjorget(lejt of x) & widen(node of x) & cascjorget(right of x) fi 
It is easily seen how to describe algorithmically the forgetful function for arbitrary 
object structures. The mapping which produces for every such mode m x the 
corresponding forgetful function v into word 1. or into sequ 1. is called the forgetful 
functor of the associative law. 
Forgetful functions are of special importance in the theory of formal languages: for 
context-free grammars they effect the transition from syntax parsing trees to the corre-
sponding words of the language. 
29 We can also take sequ "X instead of word "X· This even becomes mandatory if decomposition 
operations are required. 

230 
3. Computational Structures 
A system of mode declarations describes a context-free grammar as follows: each 
defined mode m; corresponds to a syntactic variable <m;>, each mode declaration to a set of 
productions and each primitive mode tj to a set ~ of terminal symbols. 
For example (comp. 2.9.1.2), the grammar 
<expression> :: = <term> <Operator> <term> 
<term> :: = <id> I <lbrack> <expression> <rbrack> 
is represented by the following system of mode declarations: 
mode expression = (term, operator, term), 
mode term = id I (lbrack, expression, rbrack) 
For each of the m; we have then a forgetful function 
v;: m; --+ word t 
where 
is the set of all terminal symbols. 
The word problem "is x a word form;?" for an x in word tis then the problem of 
deciding whether xis in the range of v;. The grammar is unambiguous if and only if all the 
V; are injective. Parsing algorithms are thus operative formulations of the inverse mappings 
v;- 1; now sequ t has to be used instead of word t. 
The word problem is trivial for the grammar belonging to the mode declaration 
mode rsequ x =empty l(rsequ x trunk, x item) 
Every x in sequ xis a word for rsequ X· The grammar is unambiguous and the parsing 
algorithm reads 
funct rsparse = (sequ x a) rsequ x: 
if isempty(a) then 0 
else (rsparse(rest(a)), top(a)) fi 
Even for right-linear systems of mode declarations the word problem, ambiguity prob-
lem and parsing problem are easily solved: A recognition algorithm in the particularly 
simple form of a repetitive system of routines can be obtained from the deterministic auto-
maton belonging to the corresponding right-linear regular grammar (comp. 1.4.1, (d) and 
(g)). 
Exercise 1: Specify a method which yields a recognition algorithm for an arbitrary right-linear system 
of mode declarations. 
This is different for more general systems of mode declarations. In general the context-
free grammars corresponding to them are ambiguous. Already for case x or list x the 
forgetful functions are not injective. 

3.4 Sequence-Type Computational Structures 
231 
The weakened parsing problem requires one to give an algorithm for finding an 
arbitrary inverse image for every x in the range of V;. There are nondeterministic 
algorithms which achieve this; indeed there is a general method which -
for arbitrary 
object structures - yields a (nondeterministic, recursive) algorithm for the weakened pars-
ing problem. This is in accordance with the theorem that for every context-free grammar 
there is a nondeterministic recognizing machine with a single pushdown store 30• 
Here is an example: Towards the end of 2.13 traversing algorithms were given for list 1. 
and case X· The algorithms with the traversing order car, cdr or left, node, right are exact-
ly those corresponding to the forgetful function (if we take word 1. instead of stack 1. as 
the result mode). 
It is well-known that an object is not uniquely reconstructable from the bracket-free 
infix notation, comp. 2.13. list 1. and case 1. are ambiguous in the sense of formal 
languages. However, an algorithm for weak parsing is easily specified: corresponding to 
rsparse above, it forms those cascades which are implementations of right sequences: 
funct cascparse = (sequ 1. a) case x: 
if isempty(a) then 0 
else (cascparse(rest(a)), top(a), 0) fi 
This is a deterministic version of the nondeterministic algorithm 
funct ndcascparse = (sequ 1. a) case x: 
if isempty(a) then 0 
else (sequ 1. u, 1. t, sequ 1. v) = parse(a) within 
(ndcascparse(u), t, ndcascparse(v)) 
fi 
based on parse (comp. 1.12). 
Such nondeterministic algorithms are, however, not very useful: they do not search in a 
sufficiently systematic way. If only deterministic parsing algorithms are to be used the class 
of context-free grammars has to be restricted. This leads to a theory originating from 
Eickel and Paul1964, which has become practically important in the form it has been given 
since 1965 (LR(k)-grammars, LL(k)-grammars), see e.g. Aho, Ullman 1972. 
3.4.5 Sets 
3.4.5.1 We want to pass from the abstract type CODEL (comp. 3.4.1.1) to abstract types . 
which characterize set-like object structures. 
For sets the order of insertion of elements is irrelevant. Thus, in addition to the 
properties of CODEL, one requires the property 
law RC: append(append(s, y), x) = append(append(s, x), y) 
of right-commutativity. 
30 Comp. Hopcroft, Ullman 1969. 

232 
3. Computational Structures 
Then, in order to retain consistency, one can no longer require Tin its original form, 
because otherwise one would have for all x, y from x: 
y =(T) top(append(append(empty, x), y)) 
= (RC) top(append(append(empty, y), x)) = (TJ x 
Hence if we require the validity of RC, we must require weaker properties for top. The 
property 
lawS: top(append(empty, x)) = x 
is the weakest form ofT which is still compatible with RC. But using S instead ofT means 
that the "behaviour" of top is softened, too: top can now yield an arbitrary component in-
stead of the respective "topmost" element - in every model, of course, a well determined 
one (comp. 3.2.3.1). This suggests the use of a new operation name, say elem. 
Renaming empty, isempty, append, and top as emptybag, isemptybag, collect, and 
elem leads to a new abstract type BAG 31 which describes the object set of the multisets: 
type BAG = (mode J() bag "b emptybag, isemptybag, collect, elem, delete, contains: 
mode bag J(, 
funct bag x emptybag, 
funct (bag J() bool isemptybag, 
funct (bag "b J() bag x collect, 
funct ({bag x b: --, isemptybag(b)}) x elem, 
funct (bag "b J() bag x delete, 
funct (bag "b x) boo I contains, 
law E1: isemptybag(emptybag), 
law E2: --, isemptybag(collect(b, x)), 
law C1: --, contains(emptybag, x), 
law C2: contains(collect(b, y), x) <* x = y v contains(b, x), 
law C3: --, isemptybag(b) ,;. contains(b, elem(b)), 
law D1: delete(emptybag, x) = emptybag, 
law D2: delete(collect(b, x), y) = if x = y then b 
else collect(delete(b, y), x) fi, 
law RC: collect(collect(b, x), y) = collect(collect(b, y), x), 
lawS: 
elem(collect(emptybag, x)) = x, 
law ID: --, isemptybag(b) ,;. 
col/ect(delete(b, elem(b)), elem(b)) = b 
endoftype 
Note that the property set of BAG is not sufficiently complete: elem is not completely 
characterized. This, however, gives the freedom to define elem differently in different im-
plementations. Note, that this is why an initial model does not exist. 
Taking (stack "b empty, isempty, append, delete, contains) from some model of the 
monomorphic abstract type STACK()() (or CODEL(J()), forming equivalence classes with 
31 The term originates from J. Guttag. 

3.4 Sequence-Type Computational Structures 
233 
respect to the property RC -
i.e. defining equality in a new way -
and defining elem 
suitably (whose role cannot be played by top because of RC) results in a model of the 
abstract type BAG(X). For a finite x a frequency index (comp. 3.2.5.5) gives a model of 
BAG(x). A related model can be obtained using sorted stacks. 
CODEL (and also STACK) is mapped into BAG by the forgetful functor of the 
commutative law, which "forgets" the succession of appending elements. 
3.4.5.2 Over BAG (as well as over STACK) an operation insert can be defined by the 
properties 
law IN: 
contains(b, x) => insert(b, x) = b 
law NIN: -, contains(b, x) => insert(b, x) = co//ect(b, x) 
which immediately translate into a recursive routine. Then we have also 
law D2': delete(insert(b, x), y) = if x = y then delete(b, y) 
else insert(delete(b, y), x) fi 
E2, C2, S, RC, and ID hold analogously for insert. 
Now introducing insert into BAG together with the corresponding laws and then re-
moving collect together with its laws leads to a new abstract type FINSET, which defines 
the object set of finite subsets of a given basic set x32• 33 • 
Renaming emptybag, isemptybag, collect and contains as emptyset, isemptyset, insert, 
and iselem gives 
type FINSET = (mode x) finset x, emptyset, isemptyset, insert, elem, delete, iselem: 
mode finset x, 
funct flnset x emptyset, 
funct (finset X) boo! isemptyset, 
funct (flnset x, X) finset x insert, 
funct ( {finset x s: -, isemptyset(s)}) x elem, 
funct (finset x, X) finset x delete, 
funct (finset x, x) boo! iselem, 
law E1: isemptyset(emptyset), 
law E2: -, isemptyset(insert(s, x)), 
law C1: -, iselem(emptyset, x), 
law C2: iselem(insert(s, y), x) ~ x = y v iselem(s, x), 
law C3: -, isemptyset(s) ,;. iselem(s, elem(s)), 
law D1: delete(emptyset, x) = emptyset, 
law D2': delete(insert(s, x), y) = if x = y then delete(s, y) 
else insert(delete(s, y),· x) fi, 
law IN: iselem(s, x) => insert(s, x) = s, 
32 If xis of finite cardinality then the object set finset xis the powerset IJ.l(X) of x with 
card (finset X) = 2card(X). 
If, however, X is not finite, then, due to the Principle of Generation, finset X comprises only the 
set lj.lf (X) of all finite subsets of X· 
33 For the definition of sets by abstract types see also Goguen eta!. 1978. The idea dates back to von 
Henke 1975. 

234 
law RC: insert(insert(s, x), y) = insert(insert(s, y), x), 
lawS: 
e/em(insert(emptyset, x)) = x, 
3. Computational Structures 
law ID: -, isemptyset(s) ,;. insert(delete(s, elem(s)), elem(s)) = s 
endoftype 
Exercise 1: Show, that C2 and IN imply 
law IDEMP: insert(insert(s, x), x) = insert(s, x). 
BAG is mapped into FINSET by the forgetful functor of the idempotent law which 
"forgets" the frequency of inserted elements. 
Exercise 2: Describe the abstract types that originate from WORD by the forgetful functors of the 
commutative and the idempotent law. 
Thus models for FINSET (X) can be obtained from models of BAG(x) by forming 
equivalence classes with respect to IN. In particular, forming equivalence classes with 
respect to RC and IN in any model of STACK(x) (or CODEL(x)) results in a model in 
which determining equality amounts to a cumbersome routine, but insert and elem remain 
simple. Representatives for the equivalence classes are obtained by a restriction to sorted 
stacks (with respect to some linear ordering in X) without repetition. This allows the deter-
mination of equality in a much simpler way, but it renders insert a complicated routine 
requiring sorting (comp. sort, 2.10). elem can e.g. produce the least element of a stack. 
A frequency index reduces to the frequencies 1 and 0, it corresponds to a representation 
of finset x by characteristic functions: 
In FINSET(x) 
funct e = (finset xa).(xx) bool: iselem(a, x) 
defines a mapping e: finset X-+ (X) bool, of sets into predicates on X· This mapping is in-
jective: If a * b, then either a contains an element x that does not belong to b, or vice 
versa; this means that the corresponding predicates are different. However, the mapping is 
for non-finite x not surjective: the characteristic functions of infinite subsets of x do not 
occur as images. But for the characteristic functions of finite subsets its inverse is totally 
defined: for each such predicate p on x there is a set from finset x. viz. 
I finset X a: v X x: e(a)(x) .. p(x)' i.e. 
1 finset x a: v x x: ise/em(a, x) <> p(x) 
This can be abbreviated (in familiar notation) to 
{x x: p(x)} 
Building sets of objects with the help of predicates is thus a prealgorithmic operation over 
FINSET(x). 
Using the operations defined in FINSET the usual set operations intersection, union 
and difference 34 (comp. Table 1.3.1) can now be formulated, e.g. for the union one ob-
tains from cone by replacing append with insert and rest with delete: 
34 The infix operation symbols n, u, \are commoner. For the operation iselem we may use in infix 
notation the symbol E which originates from the first letter of the Greek 8anv, the copula «is» of 
Aristotelian logic. 

3.5 Number-Type Computational Structures 
funct. u. = (finset x a, finset x b) finset x: 
if isemptyset(a) then b 
else insert((delete(a, elem(a)) u b), elem(a)) fi 
If x is finite, the complement operation can be specified algorithmically as well. 
Exercise 3: Specify the remaining set operations and also the subset relation. 
235 
For algorithms over the abstract type FINSET (and also over BAG) it is sometimes use-
ful to have a function split that is defined as follows: 
funct split = (finset X s: --, isemptyset(s)) (finset x, x): 
(delete(s, elem(s)), elem(s)) 
For STACK we have correspondingly 
funct split = (stack x a: --, isempty(a))(stack x, x): 
(rest(a), top(a)) 
3.5 Number-Type Computational Structures 
Polymorphic abstract types are suitable for exposing properties common to several non-
isomorphic computational structures. By adding further properties it is then possible to 
pass to monomorphic abstract types which characterize the corresponding abstract com-
putational structures. 
We shall demonstrate this using a polymorphic abstract type which comprises the com-
putational structures both of the natural numbers and the cycle numbers. Proceeding from 
the natural numbers the integers and the rational numbers are then introduced as abstract 
computational structures, with a prospect on H-al-fractions. 
3.5.1 Peano Numbers 
3.5.1.1 A computational structure for natural numbers which is close to their cultural roots 
exists in the form of stacks (or sequences, words) over a one-element alphabet, comp. 
3.4.3.4. 
If in the structure 
structure IN = some STACK (stroke) where 
mode stroke = atomic { 1} 
we use the abbreviation 
mode nat = stack stroke 

236 
for stroke numbers, the following operations can be defined: 
funct 0 
= nat: empty, 
funct isO 
=(nat n) bool: isempty(n), 
funct succ = (nat n) nat: append(n, I), 
funct pred = (nat n: 1 isO(n)) nat: rest(n) 
where because of E1 and E2 
law SO: isO(O) 
and 
law S1: 1 isO(succ(a)) 
3. Computational Structures 
hold. From the further properties of STACK (comp. 3.2.5) we can then derive the follow-
ing properties (comp. INDEX in 3.3.1) of the stroke numbers: 
(a) Because of property A, succ is inverse to pred: 
law S3: 1 isO(a) ,;. succ(pred(a)) = a 
(b) Because of property R, pred is inverse to succ: 
law S4: pred(succ(a)) = a 
(c) lnjectivity of succ: 
law S2: succ(a) = succ(b) => a = b 
Proof: From succ(a) = succ(b) it follows that 
a = pred(succ(a)) = pred(succ(b)) = b, owing to S4. 
Because of (a), pred(a) = 1 nat x: succ(x) = a. 
(d) lnjectivity of pred: analogous. 
Addition of stroke numbers can be defined directly as concatenation. 
From /part (comp. 3.4.3.2) we obtain a relation "is part of" on fN: 
funct. ~· =(nat a, nat b) bool: 
if 1 isO(a) " 
1 isO(b) then pred(a) ~ pred(b) 
else isO(a) 
fi 
The following properties of . ~. are immediate: 
law M1: 0 ~a (because of SO), 
law M2: 1 (succ(a) ~ 0) 
(because of S1) 
and 
law LE: succ(a) ~ succ(b) <* a ~ b 

3.5 Number-Type Computational Structures 
237 
Thus (nat, 0, succ, pred,. ;;;;.) is a model of the abstract type INDEX with the signature 
(index, min, succ, pred, . ;;;; . ). 
Moreover (comp. the remark at the end of 3.4.3.4) 
law REFL: 
a ;;;; a, 
law TRANS: a ;;;; b A b ;;;; c => a ;;;; c, 
law ANTIS: a ;;;; b A b ;;;; a => a = b, 
law TOTAL: a ;;;; b v b ;;;; a 
hold, and thus the stroke numbers form a linearly ordered commutative monoid under 
concatenation. 
By analogy with equ in 2.10, an equality on IN can be defined operatively: 
funct . ~. = (nat a, nat b) boo I: 
if 1 isO(a) 
A 
1 isO(b) then pred(a) ~ pred(b) 
for.~. we have 
a~ b <>a= b and 
a ~ 0 <> isO(a) 
else isO(a) 
A isO(b) 
fi 
3.5.1.2 It is common practice to write IN also for the carrier set nat of IN. The pair (succ, 0) 
thus has the following properties: 
(P1) 
(P2) 
(P3) 
(P4) 
(Rob) 
0 E IN, 
succ E (IN --+ IN), 
v a E IN: succ(a) * 0, 
V a, b E IN: succ(a) = succ(b) => a = b, 
v a E IN, a * 0: 3 x E IN: succ(x) = a 
This system is a variant of the "system of axioms" usually named after Peano 35 ; the in-
duction axiom is replaced by the weaker axiom (Rob). The weakening was investigated by 
Robinson 1950. In an axiomatic treatment of the natural numbers these properties may 
serve as a starting point. 
However, we shall at first not postulate (P3) but only the injectivity of succ and in-
vestigate how far arithmetic can be developed under these weaker assumptions. Hence we 
define 
type PEA = pea, succ, 0: 
mode pea, 
funct pea 0, 
funct (pea) pea succ, 
law INJ: succ(a) = succ(b) => a = b 
endoftype 
35 It was already stated in similar form by Dedekind in his paper "Was sind und was sollen die Zah-
len?" (Braunschweig 1877). 

238 
3. Computational Structures 
Note that (Pi) and (P2) are expressed by the signature, and that (Rob) is a 
consequence of the Principle of Generation. 
By construction, rN is obviously a computational structure of the (polymorphic) type 
PEA. 
The non-finite models of the type PEA are i) the natural numbers rN which fulfil (P3), 
2) the integers, and 3) the "transfinite" or "non-standard" models -
the latter are 
excluded as computational structures by the Principle of Generation. 
3.5.1.3 Motivated by the definitions over rN we shall now define arithmetic operations over 
PEA. First of all we introduce pred as the inverse of succ; (Rob) guarantees existence in 
the case of a =1= 0, and INJ guarantees uniqueness: 
functpred =(pea a: a =1= 0) pea: z pea x: succ(x) =a 
For comparison to the above see once again exercise 1.i0.2-4. 
From this definition, 83 (with min interpreted as 0) follows at once. 84 follows from 
INJ. 
Now defining comparison by 
funct. ~· = (pea a, pea b) bool: 
if a =1= 0 " b =1= 0 then pred(a) ~ pred(b) 
else a = 0 
fi 
allows the derivation of Mi. On the other hand, neither LE nor M2 can be derived without 
(P3) or 8i. 
A routine for addition is obtained by re-writing the concatenation cone (3.4.3) from 
nat "" stack stroke for pea (note that top(a) = I): 
funct add = (pea a, pea b) pea: 
if a = 0 then b 
else succ(add(pred(a), b)) fi 
By analogy with exercise 3.4.3-i the following property can be shown: 
law A880C: add(a, add(b, c)) = add(add(a, b), c) 
The routine add corresponds to 8kolem's definition of addition (i923). He shows com-
mutativity for it in the following way: Let succ(O) be denoted by i. Then he shows first 
law COMMi: add(a, i) = add(i, a) 
Proof: From the definition of add, we obtain add(1, a) = succ(a). 
We now use induction on a. Obviously 
add(O, i) 
i = succ(O) = add(i, 0). 

3.5 Number-Type Computational Structures 
Let 
add(a, 1) = add(1, a) (induction hypothesis). 
Then 
add(succ(a), 1) = add(add(1, a), 1) 
(ASSOC) 
= add(1, add(a, 1)) 
(ind.hyp.) 
= add(1, add(1, a)) 
= add(1, succ(a)). Thus, COMM1 holds for all a. 
Next he shows 
law COMM: add(a, b) = add(b, a) 
Proof (by induction on b): 
Because of COMM1, COMM holds for b = 1 . 
Let 
add(a, b) = add(b, a) (induction hypothesis). 
Then 
add(a, succ(b)) = add(a, add(1, b)) 
(ASSOC) 
= add(add(a, 1), b) 
(COMM1) 
= add(add(1, a), b) 
(ASSOC) 
= add(1, add(a, b)) 
(ind.hyp.) 
= add(1, add(b, a)) 
(ASSOC) 
= add(add(1, b), a) 
= add(succ(b), a). Thus, COMM holds for all a, b. 
Furthermore subtraction is derived by rewriting cutoff from exercise 3.4.3-5: 
funct sub =(pea a, pea b: b ~a) pea: 
if b = 0 then a 
else sub(pred(a), pred(b)) fi 
where 
law 11: sub(add(a, b), b) = a, 
and 
law 12: b ~ a =- add(sub(a, b), b) = a 
hold, analogous to the properties proved there. 
Exercise I: Develop an algorithm from the specification 
funct sub = (pea a, pea b: b ~ a) pea: 1 pea x: add(x, b) = a 
239 

240 
3. Computational Structures 
Exercise 2: Show that sub is right-commutative, i.e. that sub(sub(a, b), c) = sub(sub(a, c), b). 
Show more generally that the inverse of an associative and commutative operation is 
right-commutative. 
Exercise 3: Show that sub(a, sub(b, c)) = sub(c, sub(b, a)) and thus sub(a, sub(a, b)) = b. 
3.5.1.4 In STACK(stroke) (comp. 2.10.2) 
law RE: reverse(a) = a 
holds. This would suggest performing addition according to the pattern concatr in 2.11. 
McCarthy used the resulting version in 1961 (comp. also exercise 3.4.3-3): 
funct sum = (pea a, pea b) pea: 
If b = 0 then a 
else sum(succ(a), pred(b)) fi 
Trivially, 
law RN: sum(a, 0) = a 
holds. A proof by induction (using RN) is necessary for 
law C: sum(succ(a), b) = succ(sum(a, b)) 
and 
law RC: sum(sum(a, b), c) = sum(sum(a, c), b) (right-commutativity) 
Proof for C: 
C holds for b = 0: sum (succ(a), 0) = succ(a) = succ(sum (a, 0)). 
Let sum(succ(a), pred(b)) = succ(sum(a, pred(b))) (induction hypothesis). 
Then 
sum(succ(a), b) 
(unfold sum) 
(ind.hyp.) 
(fold) 
if b = 0 then succ(a) 
else sum(succ(a), pred(b)) fl 
if b = 0 then succ(a) 
else succ(sum(a, pred(b))) fi 
= succ (if b = 0 then a 
else sum(a, pred(b)) fi) 
= succ(sum(a, b)). 
Thus C is shown. 
Proof for RC: 
RC holds for c = 0: sum(sum(a, b), 0) = sum(a, b) = sum(sum(a, 0), b). 
Let sum(sum(a, b), pred(c)) = sum(sulil1(a, pred(c)), b) (induction hypothesis). 

3.5 Number-Type Computational Structures 
Then 
sum(sum(a, b), c) 
(unfold outer sum) 
= if c = 0 then sum(a, b) 
else sum(succ(sum(a, b)), pred(c)) fi 
(C) 
= if c = 0 then sum(a, b) 
else sum(sum(succ(a), b), pred(c)) fi 
(ind.hyp.) 
= if c = 0 then sum(a, b) 
else sum(sum(succ(a), pred(c)), b) fi 
= sum (if c = 0 then a 
else sum(succ(a), pred(c)) fi, b) 
(fold) 
= sum(sum(a, c), b). 
Thus RC is shown. 
241 
Note that the properties S3 and S4 as well as INJ were not used to prove C and RC. 
If, however, we do use S3, we can show 
law LN: sum(O, a) = a 
Proof: 
LN holds for a = 0: sum(O,O) = 0. 
Let sum(O, pred(O)) = pred(a) (induction hypothesis). 
Then 
sum(O, a) 
(unfold) 
if a = 0 then 0 
else sum(succ(O), pred(a)) fi 
(C) 
= if a = 0 then 0 
else succ(sum(O, pred(a))) fi 
(ind.hyp.) 
= if a = 0 then 0 
else succ(pred(a)) fi 
(S3) 
= if a = 0 then 0 
else a fi 
=a. 
Thus LN is shown. 
By algebraic manipulation we now obtain from RC and LN the commutative law and 
then the associative law (McCarthy 1961). 
Functional equivalence of add and sum also needs an inductive proof (comp. the two 
versions of cone in 3.4.3). 
3.5.2 Cycle Numbers and Natural Numbers 
3.5.2.1 A unary operation like succ has many models; every transition graph is one of 
them. Injectivity allows as finite models only cycles. These models are called cycle num-
bers, they obey the law 

242 
3. Computational Structures 
law CYCL: 3 pea a: succ(a) = 0 
which is just the negation of (P3), i.e. of St. 
Supplementing PEA by the property CYCL, i.e ., (P3), leads to a new abstract type; 
pred can now be defined totally. The integers are a model, but they cannot be generated in 
this abstract type 36• {0} with succ(O) = 0 is the terminal model '11; an initial algebra does 
not exist - provided we restrict ourselves to total functions. 
Cycle numbers are used in digital circuits, e.g. cycle numbers with a period 2N are the 
states of a cyclic N-bit counter. The addition of cycle numbers, as derived from add or 
sum, is common addition modulo the period. In some cryptographic methods, such an ad-
dition is used. 
Basing a predicate less on the solvability of additive equations, 
funct less ""(pea a, pea b) bool: 3 pea x: add(a, x) = b 
a relation is obtained which may be wider than the order . :;:;; . 37 defined above; in fact this 
is the case for cycle numbers 38: less then always yields true. Correspondingly, the inverse 
of add is then totally defined, the routine sub thus terminates for all arguments ( comp. al-
so 3.5.1-1). The same is true for the routine sum above and its inverse dijf, 
funct diff"" (pea a, pea b: less(b, a)) pea: 
if b = 0 then a 
else pred(diff(a, pred(b))) fi 
Certain nexuses of cyclic non-finite objects from stack x (comp. {a, b, c}in 2.14.1) and 
of two-way ring-lists from case x are models for cycle numbers, too. (In Chap. 7 this will 
provide a way of implementing cycle numbers with the help of pointers.) 
3.5.2.2 Supplementing PEA with the property S1, i.e. (P3), results in a monomorphic ab-
stract type: every computational structure is isomorphic to the term algebra. Now, both 
LE and M2 can be derived and the type INDEX is nothing but an "enrichment" (see 3.6.1) 
of PEA supplemented by S1; because all its models are isomorphic to the natural numbers 
introduced as stroke numbers, each one is a linearly ordered, commutative monoid with 
cancellation property and indivisible neutral element. The further development of arithme-
tic involves the introduction of multiplication by means of iterated addition ( comp. pow in 
1.9) and an application of divisibility theory to the multiplicative semigroup. This leads 
into number theory. Thus we obtain the operations and properties of the (abstract) com-
putational structure IN as given in 1.3.1; they show IN as an ordered commutative semiring. 
For cycle numbers, too, the introduction of multiplication by iterated addition is in-
dicated. Thus multiplication algorithms for natural numbers and for cycle numbers coin-
cide. This does not imply, however, that division algorithms for natural numbers neces-
sarily carry over to cycle numbers, since for them there is no counterpart to the . :;:;; . order-
ing. 
36 In INDEXS, pred belongs to the constructor functions. 
37 For cycle numbers . ::::; . is an ordering on the cycle cut at 0. 
38 If CYCL holds, then 0 is decomposable in the sense of divisibility theory (3 .4.2), thus divisibility 
does not have to yield an ordering. 

3.5 Number-Type Computational Structures 
243 
We can thus conclude: The abstract type PEA describes the commutative semigroups 
generated by one element (succ(O)) under an associative composition (add or sum): both 
the semigroup of the natural numbers and the finite cyclic groups. 
3.5.3 Excursus: Extension by Means of Formal Quotients 
A semigroup with neutral element, i.e. a monoid, can be extended to a group by introduc-
ing equivalence classes of pairs. The following more general theorem 39 extends 3 .4.2 
(Malcev 1939, comp. Clifford, Preston 1961): 
Theorem: A commutative cance/lative monoid (S, e, o) with an indivisible neutral element 
e (in which the ordering p is compatible with o) can be extended to an ordered group by 
introducing equivalence classes of pairs. 
For the proof we introduce pairs (x, y) of elements from S, and define a relation ~ 
(a, b) ~ (c, d) <>del a o d p c o b 
and an operation o 
(a, b) o (c, d) =der<a o c, do b) 
This operation o is commutative and associative and has (e, e) as its neutral element. 
Furthermore it is compatible with the relation ~: 
(a, b) ~ {c, d) => a o d p c o b =>a o d o x o y p c o b o x o y 
=> a 0 X 0 d 0 y p C 0 X 0 b 0 y => (a 0 X, b 0 y) ~ (c 0 X, d 0 y) 
=>(a, b) o (x,y) ~ (c,d) o (x,y) 
The relation ~ is reflexive: 
(a, b) ~ (a, b) 
and transitive: 
(a, b) ~ {c, d) A (c, d) ~ (e,f) =>(a, b) ~ (e,f) 
~ induces an equivalence relation -: 
(a, b)- (c,d) <>der(a,b) ~ (c,d) A (c,d) ~(a, b) 
we have 
(a, b) - (c, d) <> a o d = b o c. 
39 The theorem can be generalized to certain non-commutative cases which, however, do not include 
the case word x with x having more than one element, which would be of interest to us. 

244 
3. Computational Structures 
Let [x,y] denote the equivalence class of the pair (x, y). Then 
[a, b] = [c, d] '* a o d = b o c 
For these equivalence classes a relation ~ is defined by 
[a, b] ~ [c, d] '*cter<a, b) ~ (c, d) 
This relation is even antisymmetric and hence an ordering. 
The operation o can be extended to equivalence classes as well: 
[a,b] o [c,d] =cterla o c, bod] 
[e, e] is the equivalence class of the pairs (a, a); it is the neutral element. Thus 
[a, b] o [b, a] = [e, e] 
and we may define 
[a, b]- 1 =ctef [b, a] 
Exchanging the components of a representative thus amounts to taking the inverse. Every 
equivalence class thus has an inverse. 
In addition (see above) 
[a, b] ~ [c, d] => [a, b] o [x, y] ~ [c, d] o [x, y] 
holds. 
Therefore the equivalence classes form an ordered group (G, [e, e], o, ~), which was 
to be proved. 
The original monoid Sis embedded in this group by virtue of 
a 1--> [a, e] 
Every element of G is a formal quotient of elements in S, i.e. it is a quotient of the em-
beddings of elements from S: 
[a, b] = [a, e] o [b, e]- 1 
Elements [a, e] with a =1= e are said to be positive, elements [e, a] with a =1= e are said to be 
negative, [e, e] is also called zero. 
3.5.4 Integers 
3.5.4.1 An abstract characterization of "integers" is already given by the abstract type 
INDEXS defined in 3.3.2. A model7l. for integers is obtained by applying extension by for-

3.5 Number-Type Computational Structures 
245 
mal quotients to the computational structure IN of the type PEA (3.5.1). This is possible, 
since this computational structure is a commutative cancellative monoid with 0 as indivis-
ible neutral element (and ;;i! as the divisibility relation). In this way we obtain the following 
concrete computational structure (debit denotes the "positive", credit the "negative" por-
tion of an integer, these notions go back to Fra Luca Pacioli, 1494): 
structure 7L = INDEXS: 
fin terms of IN: 
mode int 
= (nat debit, nat credit), 
funct origin = int: (0,0), 
functpred =(inti) int: (debit of i, succ credit of i), 
funct succ 
=(inti) int: (succ debit of i, credit of i), 
funct. ;;i!. = (int a, int b) bool: 
add(debit of a, credit of b) ;;i! add(credit of a, debit of b), 
funct . ~. = (int a, int b) bool: b ;;i! a 
J 
Note that the equality relation = here extends beyond the identity of components, so 
that 
. a = b <* (debit of a = debit of b) " (credit of a = credit of b) 
does not hold, but 
a = b <* add (debit of a, credit of b) = add (credit of a, debit of b) 
For addition and subtraction we have 
funct . +. = (int a, int b) int: 
(add(debit of a, debit of b), add(credit of a, credit of b)), 
funct.-. = (int a, int b) int: 
(add(debit of a, credit of b), add(credit of a, debit of b)) 
Multiplication of integers is defined (based on addition and multiplication of natural 
numbers) similar to a "complex multiplication" (comp. 3.2.5): 
funct . x. = (lnt a, lnt b) int: 
(add(mult(debit of a, debit of b), mult(credit of a, credit of b)), 
add(mult(debit of a, credit of b), mult(credit of a, debit of b))) 
The natural numbers are embedded into the integers by 
funct widen = (nat n) int: (n, 0) 
3.5.4.2 If we want to include nat as a submode of int, 
mode int = nat I (nat debit, nat credit), 
we need a test function 

246 
3. Computational Structures 
funct (lnt) bool isnat 
By means of branching we can then base addition on the addition of natural numbers: 
functp/us ""(int a, int b) int: 
if isnat(a) 
then if isnat(b) 
then add(a, b) 
else (add(a, debit of b), credit of b) fi 
else if isnat(b) 
then (add(debit of a, b), credit of a) 
else (add(debit of a, debit of b), add(credit of a, credit of b)) fi 
fi 
Operatively, this is more efficient than always explicitly applying the embedding function 
widen to operands of the mode nat (or using an implicit mode-widening mechanism). 
3.5.4.3 Of course, the given representation corresponds to the "debit and credit calcula-
tion" in book-keeping. In book-keeping one occasionally strikes a balance, i.e. one con-
siders a special representative of the equivalence class, an element "in normal form": two 
elements belong to the same equivalence class if their normal forms are identica/40• The set 
of elements of the form (a, 0) or (0, a) is suitable as a normal form, because ~is a total 
relation on the additive semigroup of natural numbers. 
This suggests a (shorter) representation of integers by pairs composed of sign and 
absolute value: 
mode sint "" (bool sign, nat abs) 
The connection between both representations is given by a routine 
funct norm "" (int i) sint: 
if debit of i ~ credit of i then (false, credit of i - debit of i) 
D credit of i ~ debit of i then (true, debit of i - credit of i) fi 
Hence we have the correspondence 
< b> 
[(true, a - b), 
norm: a, 
r+ 
(false, b - a), 
if a~ b 
if a~ b 
This correspondence is not determinate, because norm( (0,0)) is equivalent to ((false, 
0) 0 (true, 0)). 
The "double zero" becomes a problem when implementing the ordering: it is not per-
mitted to take (false, 0) less than - and hence different from -
(true, 0) ("splitting of 
zero") 41 • 
40 Every book-keeper knows that it does not pay to strike a balance after every transaction. In 
double-entry book-keeping - for reasons of checking - the debit totals and credit totals are dealt 
with separately. 
41 In any case this is confusing and error-prone, although a certain leading manufacturer has been 
able to get away with ignoring such considerations for years. 

3.5 Number-Type Computational Structures 
247 
The sign-abs representation is advantageous if multiplication has to be implemented. In 
spite of this, the debit-credit representation is used almost exclusively for representing 
integers with a bounded number of (binary) digits (see 3.5.6). In order to represent the 
numbers between - BN and BN -
1, the debit-credit representation with the following 
normal forms is used: 
for non-negative numbers, 
for negative numbers. 
This yields the representation - again as a pair (bool, nat) 
k 
[<true, k), 
f-+ 
(false, k + BN), 
if k ~ 0 
if k < 0 
(this is called modulo-representation or EN-complement representation 42). 
Further relevant details are usually treated in courses on computer architecture and 
hardware. 
3.5.5 Rational Numbers 
Multiplication of natural numbers is associative; the element 1, the successor of zero, is the 
neutral element. The natural numbers therefore also form a (commutative) semigroup with 
respect to multiplication, with 0 as an annihilating element and 1 as a neutral element. 
Because of the zero this semigroup is not cancellative: 
0 · a = 0 · b does not imply a = b 
The semigroup of positive natural numbers with respect to multiplication is, however, a 
commutative, cancellative semigroup with indivisible neutral element. Divisibility theory, 
therefore, yields an ordering: the divisibility relation, in the classical sense43 , of the posi-
tive natural numbers 
a I b # def 3 X =F 0: a · X = b 
Again it is possible to extend this to a group - the group of the positive rational numbers. 
The integers, excluding the zero element, also permit extension to a group -
with 
respect to multiplication: the group of rational numbers excluding zero. 
Exercise 1: Give an abstract type for the rational numbers. 
It is obvious how a concrete computational structure using pairs of integers can be ob-
tained. It amounts to the introduction of an equivalence relation ~ for such pairs. 
42 It seems that von Neumann was the first to consider this representation for computers; for the log-
arithms of trigonometric functions it is common practice. 
43 In order to prove that the divisibility relation is an ordering, it is usually shown in an elementary 
way that a ~ b follows from a I b, i.e. that the divisibility relation can be embedded 
("topologically sorted") into the less-or-equal relation. 

248 
3. Computational Structures 
There are two peculiarities here: firstly the divisibility relation I (corresponding to the 
relation ~) in the multiplicative semigroup of natural numbers is no longer total. There 
exist pairs of non-zero natural numbers or integers (a, b) such that neither a I b nor b I a 
hold. Hence simple normal forms as in the previous case can no longer be obtained. 
We can, however, show that for every two natural numbers a, b - different from zero 
-
there is always a greatest common divisor gcd(a, b) and a least common multiple 
lcm(a, b), such that Dedekind's relation 
gcd(a, b) · lcm(a, b) = a· b 
holds. 
In positive rational numbers the greatest common divisor is used in order to pass from 
an arbitrary pair (a, b) to the simplified pair (a!gcd(a, b), blgcd(a, b)) of relatively prime 
numbers as a normal form. In order to prevent the numerator and denominator from in-
creasing rapidly it is worth-while working always with this reduced representation when 
implementing rational arithmetic. 
Exercise 2: Specify an algorithm which determines a/gcd(a, b) and b/gcd(a, b) directly - i.e. with-
out calculating gcd(a, b) explicitly. 
Secondly we also want to define addition for rational numbers. For pairs with a com-
mon denominator it is obvious that we require 
[a, c] + [b, c] = [a + b, c] 
In the general case we establish the least common multiple of both denominators and thus 
obtain 
[a, c] + [b, d] 
[a· (lcm(c, d)/c), lcm(c, d)] + [b · (lcm(c, d)/d], lcm(c, d)] 
[a· (lcm(c, d)/c) + b · (lcm(c, d)! d), lcm(c, d)] 
which is advantageous especially when using reduced representation (note, however, that 
the result can still sometimes be simplified: 
[1,15] + [1,10] = [2,30] + [3,30] = [5,30] 
[1,6] 
). 
It is easier to take c · d as a common multiple: 
[a, c] + [b, d] = [a · d + b · c, c · d]; 
but the price is an increase of the numbers. 
Exercise 3: Specify an algorithm which establishes lcm(c, d)/c, lcm(c, d)ld and lcm(c, d) directly. 
We proceed in the same way for subtraction and for comparison. In the reduced repre-
sentation the denominator may always be taken positive. 
The addition defined in this way proves to be associative, with subtraction as the 
inverse operation. However, a neutral element with respect to addition is missing. The 

3.5 Number-Type Computational Structures 
249 
equivalence class of the elements of the form (0, b) with b > 0, with the representative 
0 = def [0,1] can be used as a neutral element. In this way we obtain the field of the rational 
numbers. 
The pair (0, 0) is of no use. Elements of the form (a, 0) with a * 0 can be added. If we introduce as 
a normal form [1, 0] (denoted by + oo) and [ -1, 0] (denoted by - oo) and specify addition ap-
propriately, we obtain an ordered algebraic structure (not a field!) which is of lattice theoretical im-
portance in advanced calculus. 
If unique inverse elements are required, [- 1, 0] and [1, 0] must collapse into one element 
(denoted by oo). Now 1/0 = oo and 1/oo = 0; oo -
oo and oo/oo are undefined just as is 0/0. We 
obtain an algebraic structure which is important both in projective geometry of the line and as the 
circle of numbers in the calculus (real segment of the number sphere of complex analysis). 
The Zurich computer ERMETH which was built in the fifties under the influence of Rutishauser 
included a special number oo and the relevant arithmetic which could be used to advantage especially 
in continued fraction algorithms. 
3.5.6 Positional Systems and H-al-Fractions 
Positional systems serve to represent numbers. Using a fixed given sequence v; (i = 1, 2, ... ) 
of (strictly increasing) positional values, a number a is represented by a sequence of coeffi-
cients I an an-l ... a0 / if 
n 
a= [ a;v; 
;~o 
holds. 
Historically, many systems of coins, weights, and measures are positional systems 
not to speak of the worldwide system of seconds, minutes, hours, days and so on. Even up 
to 1971 a non-decimal positional system persisted in the currency of Great Britain; a price 
tag such as 5-8-3 in a shop window in Carnaby Street meant£ 5, 8s, 3 d, where a shilling (s) 
was 12 pence (d, "denar") and a pound(£) 20 shillings, i.e. 240 pence. If the price is to be 
expressed in pence, the positional values are, therefore, 1 in the last position, 12 in the last 
but one, and 240 in the last but two. 
Today, generally radix systems are used for number systems. These are positional sys-
tems in which the sequence of powers of a natural numer B (B ~ 2), the base, serves as the 
sequence of positional values. In everyday life we almost always have B = 10 (decimal sys-
tem). Leibniz in his "arithmetica dyadica" has already discussed the use of the base 2 (dual 
system). The decimalization of weights and measures is a child of the French Revolution. 
Great Britain and the USA resisted it up to the present days. 
Integers can be represented in a positional system by sequences of integers, i.e. as 
objects of the mode sequ int 44• The leftmost coefficient different from zero is called the 
"leading digit". Addition is then defined componentwise irrespective of the positional 
values: For 
44 Strictly speaking this means the introduction of a computational structure 
structure !NT = some SEQU (int). 

250 
3. Computational Structures 
we have 
a+ b = c ~ /ckck_ 1 .•• c0/ 
with 
c; =a;+ b; 
(0 ~ i ~ k = max(m,n)), 
where - in the case e.g. m < n -
b; is set to 0 form + 1 ~ i ~ n. 
Subtraction is obtained in the same way. The empty sequence is a representation of 
zero. Note that this is so for arbitrary positional systems, hence for radix systems with an 
arbitrary base B. 
For radix systems, however, multiplication becomes simple. It emerges as a 
convolution 
a· b = c with 
C; = 
L av · bi-v (0 ~ i ~ m + n) 
O~v~i 
Proof: 
L avBV. 
L 
b~B~ = 
L 
L 
av. b;_vBVBi-v 
O:;;;v~n 
O~IJ.~m 
O~v<n v~i~m+v 
Division as the inverse of multiplication can be performed recurrently45 • 
Exercise 1: Specify suitable formulations for the above representation of integers and the basic 
arithmetic operations. 
Up to now we have not assumed that the representations are normalized: a price tag 
4-27-15 would be unusual but have a definite meaning. This suggests the introduction of 
equivalence classes of numerically equivalent representations. According to Faltin et a!. 
1975 this is achieved formally in a simple way by means of the definition 
a - b # 
3 c: a = b + c · k 
where k has the special representation 
(the numerical value of k being zero). cis called the carry and k the carry constant. 
The cleared representation with the properties 
0 ~ a; < B and an =I= 0, 
where the a; are called figures or digits, 
now serves as the normal form of positive elements. This is a consequence of the following 
theorem: 
Theorem: If a and b are cleared positive representations and if a - b, then a = b. 
45 Such a division algorithm with subsequent clearing of the result (see below) can be of advantage 
by comparison to conventional division. 

3.5 Number-Type Computational Structures 
251 
Proof: Since a - b, a - b = c · k for some c. Assume without loss of generality that 
Then - B c0 = d0 • Because of the premises 
0 ~ a; < B and 0 ~ b; < B, therefore 
- B < d; < B 
Thus c0 = 0 and hence a0 = b0 • Now, either n = 0 which means we are finished, or again 
-Bc1 = d1, etc. 
Exercise 2: Let the natural numbers be represented by a cleared radix representation of an arbitrary 
base B as objects of the computational structure 
structure NUMBER = some SEQU (figure) 
Give an algorithm for each of the following: 
a) addition 
b) multiplication 
c) comparison. Under what conditions can comparison also be performed as lexico-
graphic comparison? 
The mode figure with corresponding operations is assumed to be primitive. 
For the element zero an obvious representation would be the empty sequence. Instead 
we choose n = 0 and a0 = 0, i.e. zero has a leading zero. 
In the modulo-representation we choose an+ 1 = - B for negative numbers, but other-
wise 0 ~ a; < B (i ~ n). For non-negative numbers we have an+t = 0, i.e. exactly one 
leading zero. 
We can similarly introduce (non-terminating) B-al-fractions, namely as sequences 
that is by means of representations in int biflex int satisfying the condition 
3 N: vi > N: a; = 0 
The definitions of the arithmetic operations, carry constant and equivalence remain unchanged. 
However, one has to restrict oneself to the subring of "bounded" elements characterized by the con-
dition 
3Z>0:vm;;;O: r 
laviBn+v~z·Bn 
v~ -m 
and demand further conditions for the carry c. The equivalence classes then form a Dedekind-
complete ordered field, the field IR of the real numbers. The cleared bounded elements can serve as 
representatives (Faltin et al. 1975). 
However, the algorithms corresponding to the constructions for the arithmetic opera-
tions given above no longer terminate, and most real numbers are not even computable. By 
restriction to the subset of terminating B-al-fractions with a constant number of positions 
we obtain the family of object sets usually denoted by real. Details of calculation with 
such objects ("floating point numbers") are discussed in numerical analysis (Householder 
1953). 

252 
3. Computational Structures 
3.6 Changing Abstract Types and Object Structures 
In the preceding sections we have studied numerous examples of abstract types and 
algorithms over them. We shall now clarify the role of abstract types in the process of 
program development. 
In program construction the development of abstract types and computational struc-
tures usually appears in the form of a hierarchy of different formulations, with a type 
definition at the top and a concrete computational structure expressing a set of machine 
operations at the bottom. The intermediate levels in this hierarchy are again type defini-
tions and concrete computational structures. 
In accordance with this hierarchy of different levels of formulation we distinguish 
various sorts of transitions which have an important function in the process of program 
development. We shall thus consider in the sequel type change, concretization and imple-
mentation (of concrete computational structures in terms of others). 
As already mentioned in 3.2.6, mode declarations are nothing but abbreviations for 
certain monomorphic type definitions; thus the implementation of modes by other modes 
is a special case of these developments. 
3.6.1 Type Change and Related Types 
Often an abstract type already well-understood proves too poor or too rich for a given 
problem, so that one wants to pass to another "related" abstract type. It may be that a fre-
quently used operation which is formulated over the type is to be incorporated into the 
type (together with its properties), or that in the applications of the type certain operations 
(and laws concerning them) are not used and that one wants to omit them in order to admit 
"more" models. It is further possible that operations are not completely characterized and 
that the type thus has "too many" models so that laws have to be added, or conversely, 
that certain laws are not needed and that one wants to omit them, again to admit "more" 
models. 
So let A = (I:, ~) and B = (I:', ~') be abstract types. We say that A and B are 
(directly) related if they stand in one of the following relations. 
(1) A ~ B ~ der A and Bare homologous and the laws of A imply those of B. 
If A ~ B holds we say that A is stronger than B. 
~ is reflexive and transitive. In partic-
ular, A ~ B holds if A is a monomorphic characterization of an abstract computational 
structure of the polymorphic type B; thus in 3.5.1 we have INDEX ~ PEA. 
(2) A + B ~ cter A ~ B A B ~ A 
(i.e. A and B have the same operations and equivalent laws.) 
If A + B holds we say that A and B are strongly equivalent. + is an equivalence relation. 
From a given abstract type a strongly equivalent type can be obtained by adding or 
removing a law which can be derived from the other laws. Less trivial is the strong 

3.6 Changing Abstract Types and Object Structures 
253 
equivalence of two abstract types which characterize the computational structure BIT 
from 3.1.4, one of them by the laws listed there, the other one by explicitly describing the 
operations by laws of the form (composition tables) 
(T')' = T, 
T' "T' = T', T' " T = T', T "T' = T', 
T' v T' = T', T' v T = T, 
T v T' = T, 
(3) A~ B <*cter 
a) I: comprises I:' 
b)(£ implies (£' 
c) every term over I: of a sort of I:' can be reduced to a term over I:' using (£. 
If A ~ B holds, we say that A is richer than B or an enrichment of B (Burstall, Goguen 
1977). ~ again is reflexive and transitive. We have met this relation in 3.2 in connection 
with the type indication; there we had 
SEQU ~DECK and 
DECK~ STACK. 
Obviously, A~ B A B ~ A is equivalent to A + B. 
The examples mentioned above show the particularly simple and practically important 
case of the operative enrichment: the added operations can be expressed algorithmically 
based on the other operations (such that the postulated properties are satisfied). 
It is clear that adding an operation and its properties in general leads to an abstract type 
which defines different carrier sets. This is excluded in the case of an operative enrichment. 
A further example is provided by the abstract type STACK. In 3.4.1 we have enriched 
it operatively by adding the operations contains and delete. From the recursive definitions 
of these functions over STACK we have deduced the properties 
C1,C2,C3,D1,D2,REST 
Adding also these laws leads to a new abstract type SC. By these additional laws the newly 
added operations are reduced to those already present; the recursive definitions may even 
be deduced as laws in SC. Hence SC is an operative enrichment of STACK. Moreover, in 
3.4.1 we have passed to a new abstract type CODEL by omitting the operation rest and the 
laws Rand REST (and replacing A with I D). Since, however, rest can be formulated over 
CODEL as shown in 3.4.1, CODEL can be enriched operatively to SC, too. 
On the contrary, the operations of SEQU cannot be formulated operatively over 
WORD; (after adding widen) SEQU is an enrichment of WORD, but not an operative 
one. Finally, BAG and FIN SET - though homologous - do not stand in any of the three 
relations. 
For further details on relations between abstract types see Broy et al. 1980. 

254 
3. Computational Structures 
3.6.2 Concretization 
The carrier sets and operations defined by an abstract type can be used in the surrounding 
program just like those of a concrete computational structure, i.e. new routines can be bas-
ed on the operations considering these as primitives, and new object structures can be bas-
ed on the carriers, also considering these as primitives. The computational structures which 
are assumed to be primitive in a program define an abstract machine for which the pro-
gram is formulated. However, in the course of program development, that is "on the way 
towards the machine", there must be a step in which a model of the type is realized by 
concrete object structures and routines. This transition from an abstract type to a concrete 
computational structure was called concretization in 3.2.4. 
In order to prove that a given concrete computational structure is a concretization of 
an abstract type we need to show that the concrete operations satisfy all the properties list-
ed in the type. In general, induction methods will be necessary (comp. 3.2.4). 
Incidentally, the effort which these proofs require is one of the principal reasons why the set of 
laws should be kept to a minimum, although no such requirement was made in the definition of 
abstract types. It is often preferable to prove from the laws additional properties algebraically on the 
abstract level. 
3.6.2.1 For an abstract type which corresponds to a mode declaration, concretization by 
means of this mode declaration itself is obvious. Thus we have e.g. the following concreti-
zation of the abstract type STACK: 
structure STACK = (mode i) STACK (i): 
r mode empty = atomic {0}, 
mode stack x = empty I (X item, stack x trunk) 
funct empty 
= stack x: 0, 
funct isempty = (stack x s) boo!: empty:: s, 
funct top 
=(stack xs: 1 isempty(s)) x: item of s, 
funct rest 
= (stack x s: 1 isempty(s)) stack x: trunk of s, 
funct append = (stack x s, x x) stack x: (x, s > 
J 
For such concretizations the proof of the laws is almost trivial (comp. 2.10.3). 
Exercise 1: Prove that the following computational structure is a concretization of the type Z2 of the 
cyclic, commutative group of order 2 specified in 3.2.5: 
structure Z2 .. Z2: 
I mode group .. atomic{:=::::. Xl. 
funct . o. 
.. (group a, group b) group: 
if a = e then b 
funct inv 
funct e 
funct x 
0 a = x then if b = e then x 
0 b = x then e fi fi , 
=(group a) group: a, 
.. group: :=::::, 
.. group: X 
3.6.2.2 If an abstract type allows to find a core set of operations on which the other 
operations can be based, using (descriptive) formulations derived from the laws, then 
when concretizing the type it is sufficient in the first place to consider only this core set. A 

3.6 Changing Abstract Types and Object Structures 
255 
completely operative concretization can later be obtained by eliminating the descriptive 
language constructs from the basic operations. 
This method of partial concretization allows one to prove algebraically on the abstract 
level that the based operations satisfy the laws. For a particular concretization it is then 
sufficient to show that the core set is correctly realized. 
As an example consider STACK. 
In order to derive a specification for the operation rest we use 
law A: -, isempty(s) =* append(rest(s), top(s)) = s 
and 
law AA: append(a, x) = append(b, y) # (a = b) " (x = y) 
which is derivable from Rand T. 
By applying A a (possibly) ambiguous function 
funct rest = (stack x s: -, isempty(s)) stack x: 
11 stack x t: 3 x y: append(t, y) = s 
is obtained. 
For this routine the property R can now be proved: 
rest(append(s, x)) 
=(unfold) (q stack X t: 3 X y: append(t, y) = append(s, x)) 
=cAAJ ('1 stack x t: 3 xy: (t = s) " (y = x)) 
= s 
Note that the validity of the property R guarantees the determinacy of the operation rest. 
Hence rest should be specified more precisely by 
funct rest = (stack x s: -, isempty(s)) stack x: 
I stack X t: 3 xy: append(t, y) = s 
Similarly the following specification for top can be derived: 
funct top = (stack x s: -, isempty(s)) stack x: 
1 xy: 3 stack X t: append(t, y) = s 
3.6.2.3 In our next example we consider a concretization of the abstract type GREX (v, X) 
(3.3.3) by means of (fixed) arrays, using a special element !: 
structure AGREX = (mode v, mode x) GREX (v, X): 
I mode xt 
= xlatomic {!}, 
mode v grex x 
= v array xt , 
funct vac = v grex x: 1 v array xt g: v vi: g[i] = !, 
funct get 
= (v grex x g, vi: isaccessible(g, i)) x: g[i], 

256 
3. Computational Structures 
funct put 
= (v grex x g, vi, x m) v grex x: 
z v array x1 k: k[i] = m A v v j: j * i => kUJ = gUJ, 
funct isaccessible = (v grex x g, vi) bool: g[i] * l 
J 
Note that v has to be finite and linearly ordered if fixed arrays are to be used; the 
ordering, however, is not used explicitly in the operations. 
For a well-ordered, possibly infinite set index, GREX(index, X) can also be concretiz-
ed by means of a structure 
structure FLEX= some FLEX (index, x1) where 
mode x1 = x I atomic {!} 
A realization of put is 
funct put = (index grex x g, index i, x x) index grex x: 
if i ~ hib(g) then alt(g, i, x) 
0 i = succ(hib(g)) then ext(g, x) 
0 i > succ(hib(g)) then put(ext(g, l), i, x) fi 
The concretizations for isaccessib/e, vac and get are trivial. 
3.6.2.4 Concretizing STACK by means of FLEX is trivial, comp. 3.3.1.4. A concretization 
of QUEUE by means of FLEX and nat can be based on the operation truncshift (comp. 
3.4.1.3). If in the case of queues we work with FLEX without truncshift, the range of hib is 
infinite even for queues of limited length. In concretizing decks and sequences by means of 
FLEX and nat it is preferable to pass to the symmetrical extension BIFLEX of FLEX. 
3.6.2.5 In 3.6.2.3 we discussed how GREX can be concretized in terms of FLEX. 
Conversely, since index is well-ordered (comp. 3.3.1), it is possible to concretize FLEX by 
means of GREX using a level counter: 
structure GFLEX =(mode X) FLEX (nat, X): 
I in terms of some GREX (nat, x): 
mode nat flex x =(nat i, {nat grex xx: x * vac} g) I 
nat grex x {vac}, 
funct init 
= nat flex x: vac, 
funct isinit 
=(nat flex xf) bool:f = init, 
funct ext 
=(nat flex xf. xx) nat flex x: 
if isinit(f) then (min, put( vac, min, x)) 
else (succi off, put(g off, succi off, x) > fi, 
funct rem 
=(nat flex xf: --, isinit(f)) nat flex x: 
if hib(f) = min then init 
else (pred ioff, goff> fi, 
funct hib 
= (nat flex xf: --, isinit(f)) nat: i off, 
funct sel 
= (nat flex xf. nat i: --, isinit (f) 11. 
i ~ hib (f)) x: get (goff, i), 
funct aft 
=(nat flex xf. nat i, xx: --, isinit(f) 11. 
i ~ hib(f)) nat flex x: (ioff,put(goff, i, x)> 
J 

3.6 Changing Abstract Types and Object Structures 
257 
Note that in this concretization different objects (i, g) and (i, g'} represent the same 
element of nat flex x provided that g and g' coincide "below" the counter i. 
From this counter representation - by combining it with the concretization of STACK 
by means of FLEX - we also get a counter representation of STACK by means of GREX 
which corresponds to the realization of stacks by means of an associative memory: 
structure GSTACK = (mode y) STACK (y): 
I in terms of some GREX (nat, y): 
mode stack x = (nat topmost, nat grex x data), 
funct empty = stack x: (0, vac}, 
funct isempty = (stack x s) bool: topmost of s = 0, 
funct Dp 
= (stack x s: 1 isempty(s)) x: get(data of s, topmost of s), 
funct rest 
= (stack x s: 1 isempty(s)) stack x: 
(pred topmost of s, data of s}, 
funct append = (stack x s, x x) stack x: 
(succ topmost of s, put(data of s, succ topmost of s, x) > 
J . 
Exercise 2: Prove that GSTACK is a concretization of STACK. 
Exercise 3: Concretize QUEUE()() by means of 
structure NGREX = (mode )() some GREX (nat, )() 
Exercise 4: Give a concretization of queues of bounded length N by means of aggregates over cyclic 
numbers (comp. 3.5.2) of suitable period ("circular store"). 
3.6.2.6 The relationship between CODEL (3.4.1) and BAG (3.4.5) suggests a concretiza-
tion of BAG by means of sorted stacks, which has already been mentioned in 3.4.5.2: 
structure BAG = (mode y) BAG (y): 
I in terms of some STACK (y): 
mode bag x 
= stack x. 
funct emptybag = bag x: empty, 
funct isemptybag = (bag x b) bool: isempty(b), 
funct collect 
= (bag x b, x x) bag x: insort(b, x), 
funct elem 
=(bag x b: 1 isemptybag(b)) x: top(b), 
funct delete 
= (bag x b, x x) bag x: 
if isempty(b) 
then b 
else if top(b) = x then rest(b) 
else append(delete(rest(b), x), top(b)) fi fi, 
funct contains 
= (bag x b, x x) boo I: 1 isempty (b) A 
(top(b) = x v contains(rest(b), x)) J . 
For insort see 1.13.3. In this concretization an operative formulation of the universal 
equality relation for bags is provided by the routine equ for stacks (comp. 2.10). In many 
algorithms, however, the universal equality relation is not used; we can then do without 
sorting. 
Exercise 5: Give an operative formulation of the universal equality relation for bags in a concretiza-
tion of BAG by means of unsorted stacks. 

258 
3. Computational Structures 
In the same way FINSET can be concretized. However, if an efficient operative for-
mulation of the universal equality relation is needed, it is expedient to modify insort so that 
it avoids multiple insertion of elements, e.g.: 
funct insortl "' (stack x a, x x: issorted(a)) stack x: 
if isempty(a) then append(a, x) 
else if top(a) = x then a 
D top(a) > x then append(a, x) 
D top(a) < x then append(insortl (rest(a), x), top(a)) fi fi. 
See 3.6.4.2 for another concretization of FINSET (by means of array or GREX). 
3.6.3 Implementation of Concrete Computational Structures 
In the course of program development we re-shape not only routines, e.g., for the purpose 
of "removing recursion" (see Chap. 4), but also object structures, where, in general, we 
choose new structures that permit more efficient operations or are "closer to the 
computer". A change in the object structures almost always causes a change in the opera-
tions. The natural frame for such a common development of object structures and (basic) 
operations is a computational structure which ensures - due to encapsulation - that all 
alterations are independent of and have no influence on the remaining program text. 
With the notion "implementation" to be defined we want to capture those transitions 
between concrete computational structures which do not influence programs formulated 
over the set of operations provided by the computational structures. It is characteristic for 
such transitions that all (objects and) operations of the original structure are represented 
homomorphical/y by (objects and) operations of the new structure. In general, only part of 
the (objects and) operations will be needed for this purpose. 
By a reduct of an algebra A we mean a restriction of A to a subsignature of A. A con-
traction of a concrete computational structure A is the structure generated by the opera-
tions of a reduct of A. Now let A be a concrete computational structure whose signature :E 
is contained in the signature of another concrete computational structure B, and let B' be 
the contraction of B to :E. We call Ban implementation of A if there is a homomorphism 
from B' to A. Furthermore, if Cis a concretization of the monomorphic abstract type C, 
A is called implementable over C if there is an operative enrichment of C whose com-
putational structures are implementations of A. 
Since e.g. STACK evolved as an abstraction from the object structures rsequ x. 
lsequ "J., rsequc "J., these object structures - if they were to be represented as concrete 
computational structures - would all be concretizations of STACK and thus pairwise im-
plementations of each other. 
As a further example we consider the computational structure BS from 3.1.3 which 
concretizes the monomorphic abstract type 
type BS "' (mode x. nat N: N > 0) bs "J., empty, top, rest, append, isempty, isfull: 
funct bs x empty, 
funct ({bs x b: 1 isempty(b)}) x top, 
fUnCt ({bS "J. b: I isempty(b)}) bS xrest, 

3.6 Changing Abstract Types and Object Structures 
funct ({bs 1.. b, xx: 1 isful/(b)}) bs 1.. append, 
funct (bs x) bool isempty, 
funct (bs 1.. )bool isfull, 
funct (bs X) nat length, 
law R: 
1 isjull(b) .;. rest(append (J, x)) = b, 
law T: 
1 isjull(b) ,;, top(append(b, x)) = x, 
law A: 
1 isempty(b) .;. append(rest(b), top(b)) = b, 
law E1: isempty(empty), 
law E2: 1 isjull(b) ,;, 1 isempty(oppend(b, x)), 
law F1: 1 isfull(empty), 
law F2: 1 isempty(b) ,;, 1 isfull(rest(b)), 
law F3: isfull(b) <> (length(b) = N), 
law L1: length(empty) = 0, 
law L2: 1 isfull(b) .;. length(append(b, x)) = length(b) + 1 endoftype 
259 
Another concretization of this type can be implemented over GREX using a counter: 
structure GBS = (mode x. nat N: N > 0) BS (J., N): 
I in terms of some GREX (v, X) where 
mode v 
=nat [1 .. N]: 
mode bs 1.. 
=(nat [0 .. N] i, v grex 1.. a), 
funct empty = bs x: (0, vac), 
funct isempty = (bs 1.. b) boo I: i of b = 0, 
funct top 
= (bs 1.. b: 1 isempty(b)) x: get(a of b, i of b), 
funct rest 
= (bs 1.. b: 1 isempty(b)) bs x: (i of b - 1, a of b), 
funct append = (bs 1.. b, xx: 1 isfull(b)) bs x: 
(i of b + 1, put(a of b, i of b + 1, x) ), 
funct isfull 
= (bs 1.. b) bool: i of b = N, 
funct length 
= (bs 1.. b) nat: i of b 
J 
Now according to our definition, BS and GBS are implementations of each other. A 
further implementation of v grex 1.. by v array x1 (comp. 3.6.2.3) may follow. 
The next two sections are devoted to two particular implementation techniques: binari-
zation and packing of objects. 
3.6.4 Example: Binarization 
An alphabet is said to be binary when it is composed of exactly two symbols, e.g. 
mode bool = atomic {false, true} 
mode bit 
= atomic {0, L} 
or 
atomic {0, 1 }. atomic { +, - }, atomic {masculine, feminine} 

260 
3. Computational Structures 
Binarization is the reduction of object representations to binary words, i.e. to 
sequences sequ bit of binary symbols, frequently those of bounded length, bs bit, or to 
arrays nat [1 .. N] array bit of a fixed length N. 
3.6.4.1 A representation of objects by binary words, i.e. an injective mapping from the 
object set into the set of binary words, is called a (binary) encoding. 
The usual binary encoding of the natural numbers is the radix representation to the 
base 2 (3.5.6) with the positional values 1 for Land 0 for 0; we speak of dual numbers. 
The range of the mapping is the set of all binary words without leading 0. The ordering of 
the natural numbers induces an ordering (comp. exercise 3.5.6-2c)) on the set of all binary 
words - the natural ordering. 
A binary encoding of a well-ordered object set is said to be direct if it is an order-iso-
morphism onto an initial segment of the range of binary words under the natural ordering. 
The mapping is then bijective and the encoding is thus reversible. The transition to dual 
numbers is therefore a direct binary encoding. 
The transition from the stroke numbers to the dual numbers is done in two steps: first-
ly, operatively improved versions of the routines are introduced, secondly a more suitable 
object structure is chosen. 
We set out from a concrete computational structure NAT whose type EPEA is an 
operative enrichment of PEA by a set of operations: 
structure NAT = EPEA: 
I in terms of some SEQU (stroke): 
mode stroke = atomic {I}, 
mode nat 
= sequ stroke, 
funct zero 
= nat: empty, 
funct one 
= nat: succ(zero), 
funct add 
= (nat a, nat b) nat: 
if a = zero 
then b 
elsf b = zero then a 
else if odd(a) " odd(b) then 2X succ(add(pred(a) 12, pred(b) /2)) 
D odd(a) " even(b) then succ(2x add(pred(a) 12, b /2)) 
D even(a) " odd(b) then succ(2x add(a 12, pred(b) /2)) 
D even(a) " even(b) then 2x add(a 12, b 12) 
fi fi, 
funct mult 
= (nat a, nat b) nat: 
if b = zero then zero 
else if odd(b) then add(a, mult(2x a, pred(b) /2)) 
D even(b) then mult(2x a, b /2) 
fi fi, 
funct succ 
= (nat a) nat: stock(a, 1>. 
functpred 
=(nat a: a =1= zero) nat: rest(a), 
funct odd 
=(nat a) bool: if a= zero then false else even(pred(a)) fi, 
funct even 
= (nat a) bool: if a = zero then true else odd(pred(a)) fi, 
funct 2x. 
= (nat a) nat: conc(a, a), 
funct ./2 
= (nat a: even(a)) nat: 
if a = zero then zero 
else succ(pred(pred(a)) /2) fi 
J . 

3.6 Changing Abstract Types and Object Structures 
261 
Now the routines add and mutt need much fewer recursive calls than the corresponding 
versions of 3.5.1. In order that they be actually more efficient, doubling and halving have 
to be realized efficiently. For this purpose a binary representation seems appropriate. For 
the moment it is based on the original stroke number representation by means of suitable 
conversion functions conv and repr which are reciprocal to each other: 
structure DUALS = EPEA: 
I in terms of NAT, some SEQU (bit): 
mode dual = sequ bit, 
funct conv = (dual a) nat: 
if isempty(a) then zero 
else add(val(bottom(a)), 2x conv(upper(a))) fi, 
funct val = (bit b) nat: 
if b = 0 then zero else one fi, 
funct repr = (nat n) dual: 
if n = zero then empty 
funct zero 
funct one 
funct add 
funct mutt 
funct succ 
funct pred 
funct odd 
funct even 
funct 2x. 
funct ./2 
else if even(n) then stock(repr(n 12), 0) 
= dual: repr(zero), 
=dual: repr(one), 
else stock(repr(pred(n) /2), L) fi fl, 
=(dual a, dual b) dual: repr(add(conv(a), conv(b))), 
=(dual a, dual b) dual: repr(mutt(conv(a), conv(b))), 
= (dual a) dual: repr(succ(conv(a))), 
= (dual a: a * zero) dual: repr(pred(conv(a))), 
=(dual a) bool: odd(conv(a)), 
= (dual a) bool: even(conv(a)), 
= (dual a) dual: repr(2x conv(a)), 
= (dual a) dual: repr(conv(a) /2) 
J 
Note that conv identifies all sequences which are identical up to leading O's. The 
"normalized" sequences without leading O's generated by repr may serve as represen-
tatives of the corresponding equivalence classes. 
Since this computational structure is based hierarchically on the computational 
structure NAT its operations are not recursive. For example, the operation add of DUALS 
is based on the operation add of NAT. 
Utilizing the particular formulations of the operations we can now eliminate the 
dependency on NAT. We shall indicate this in the example of doubling: Unfolding repr in 
the body of 2x. gives 
if 2x conv(a) = zero then empty 
Using the properties 
else if even(2x conv(a)) 
then stock(repr((2x conv(a)) 12), 0) 
else stock(repr((2x conv(a)) 12), L) fi fi 
2x conv(a) = zero '* isempty(a) 

262 
3. Computational Structures 
and 
even(2x conv(a)) '* true 
we can simplify this expression to 
if isempty(a) then empty 
else stock(repr((2x conv(a)) 12), 0) fi 
by applying 
(2x n) 12 = n 
and 
repr(conv(a)) = a 
we obtain 
if isempty(a) then empty 
else stock(a, 0) fi 
By analogy, all the other operations of DUALS can be formulated directly over the 
mode sequ bit without referring to the computational structure NAT. This gives, among 
others, the following correspondences: 
zero 
... empty 
one 
... sequ bit: stock(empty, L) 
odd(a) 
<-> if isempty(a) then false else bottom(a) = L fi 
even(a) 
... if isempty(a) then true else bottom(a) = 0 fi 
2x a 
... if isempty(a) then empty else stock(a, 0) fi 
succ(2x a) ... stock(a, L) 
a 12 
... If isempty(a) then empty else upper(a) fi 
The gain in efficiency is obvious. 
Having removed hierarchical basing, we can dispense with the original computational 
structure NAT and use the structure DUALS instead; DUALS is changed to NAT and 
dual re-named to nat. Finally we obtain 
structure NAT= EPEA: 
I in terms of some SEQU (bit): 
mode nat = sequ bit, 
funct zero = nat: empty, 
funct one = nat: stock(empty, L), 
funct add = (nat a, nat b) nat: 
if isempty(a) then b 
elsf isempty(b) then a 
else if odd(a) A odd(b) then stock(succ(add(upper(a), 
upper(b))), 0) 

3.6 Changing Abstract Types and Object Structures 
U odd(a) A even(b) then stock(add(upper(a), upper( b)), L) 
U even(a) A odd(b)then stock(add(upper(a), upper(b)), L) 
263 
U even(a) A even(b) then stock(add(upper(a), upper(b)), 0) fi fi, 
funct mult = (nat a, nat b) nat: 
if isempty(b) 
then empty 
else if odd(b) then add(a, mult(stock(a, 0), upper(b))) 
U even(b) then mult(stock(a, 0), upper(b)) 
fi fi, 
funct succ = (nat a) nat: 
if isempty(a) then one 
elsf even(a) then stock(upper(a), L) 
else stock(succ(upper(a)), 0) fl, 
funct pred = (nat a: a * zero) nat: 
if a = one then empty 
elsf odd(a) then stock(upper(a), 0) 
else stock(pred(upper(a)), L) fi, 
funct odd = (nat a) bool: 
if isempty(a) then false else bottom(a) = L fi, 
funct even = (nat a) bool: 
if isempty(a) then true else bottom(a) = 0 fi, 
funct 2x. = (nat a) nat: 
if isempty(a) then empty else stock(a, 0) fi, 
funct ./2 = (nat a) nat: 
If isempty(a) then empty else upper(a) fi 
J . 
This computational structure is an implementation of the original computational 
structure NAT and, apart from the renaming of the mode dual, it is also an implementa-
tion of the computational structure DUALS. 
As the representation of nat by sequ bit is hidden, all dual numbers are guaranteed to 
have no leading zeros (comp. 3.5.6, cleared representation). 
Furthermore, the new structure NAT is interesting in the following sense: After suit-
able embedding and removal of recursion we immediately obtain the control flow of addi-
tion and multiplication control circuitry. What is obtained in this way is unlimited integer 
arithmetic with an arbitrary word length (which is not wired in any computer on the 
market). Obviously every arithmetic unit can be abstractly considered as a computational 
structure and described as such. 
The classical fixed point arithmetic is obtained by introducing a fixed (highest) word length Nand 
by mapping, accordingly, nat [1 .. 2N- 1] onto a bounded sequence bs bit (in terms of some BS 
(bit, N)). If we also describe the treatment of signs in the case of integers, we have derived the basic 
circuits of ordinary computer arithmetic. 
3.6.4.2 We are likewise led to binarization when representing subsets of a finite object set 
X· The mapping (comp. 3.4.5) ~(a): x -+ bool, defined by 
(X x) bool: iselem (a, x) 
is called the characteristic function ~(a) of the subset a of mode finset X· The considera-
tions of 3.4.5 have shown: Every subset a is uniquely defined by its characteristic function: 

264 
3. Computational Structures 
finset xa = {xx: ~(a)(x)} 
Thus, the set of all (classes of equivalent) boolean routines on x forms a model of 
FINSET(X). 
Arrays, however, are nothing but "frozen functions". If X has the cardinality Nand if 
its objects are x1 , x2, ••• , xN then a can be represented by an array nat [1 .. N] array bool. 
We obtain the reversible encoding 
. \set x _. nat [1 .. N] array bool 
y. l 
a r+ A, where A [i] = cter ise/em (a, X;) 
This encoding is the basis for a (terminal) concretization of FINSET(x) (comp. 3.4.5) with 
the help of nat [1 .. N] array bool for a finite object set X· 
Exercise I: Complete this concretization. 
Exercise 2: Give a terminal model for BAG (J() for finite X· 
If we replace false, true by 0, L, we obtain a binarization for finset x by means of 
nat [1 .. N] array bit. The set operations intersection, union, and complement corre-
spond to A, v and 1 applied to the components of the operands. In larger computers these 
operations are frequently available in parallel for all bits in a word. The concrete nature of 
the objects of x is immaterial. They may even be composite objects without affecting the 
representation. Of course a large number of bits is needed when N is large. If the word 
length is fixed, this leads to a correspondingly large number of computer words. 
For a non-finite object set xa concretization of FINSET (X) by means of GREX results 
in a similar way. Essentially, the predicate ise/em(s, x) is represented by get(s, x): 
structure GFINSET = (mode X) FINSET (X): 
I in terms of some GREX (x, bool): 
mode finset x = x grex bool, 
funct emptyset = tinsel x: vac, 
funct isemptyset = (finset x s) bool: 
funct insert 
funct e/em 
funct delete 
funct iselem 
v xx: 1 iselem(s, x), 
= (finset x s, x x) finset x: put(s, x, true), 
= (finset x s: 1 isemptyset(s)) x: 
«some fixed x x such that ise/em (s, x)», 
= (finset x s, x x) finset x: put(s, x, false), 
= (finset x s, x x) bool: isaccessible(s, x) A get(s, x) J 
3.6.5 Example: Packing of Objects 
Binarization essentially reduces all objects of a mode 11 to a uniform mode sequ bool or 
sequ bit. If the object set is non-finite, such as int, nat, sequ x. stack x. file x etc., no 
maximum length can be given for the binary words. However, k bits are sufficient for re-

3.6 Changing Abstract Types and Object Structures 
265 
presenting an object of a finite object set Jl if 2k-l < card (Jl) ~ 2k; 46 in this case we can 
encode into words of fixed length N ~ k. Under certain circumstances it may then be pos-
sible to pack 47 several components of an array or a compound into one such word. 
As an example we consider composite objects of the mode date (comp. 2.6), 
mode date = (int [1 .. 31] day, int [1 .. 12] month, int [1900 .. 1999] year) 
The set int [1 .. 31] with 31 objects requires 5-bit-words, the set int [1 .. 12] requires 4-bit-
words and the set int [1900 .. 1999] with 100 objects requires 7-bit-words. An object of the 
mode date, therefore, requires 16 bits altogether and can for example be packed into a 
half word in the case of a 32-bit-word. In the case of an array of objects of the mode date 
every two objects can be packed into a 32-bit-word. 
In the case of a 7-bit-encoding of e.g. the intervallnt [900 .. 999], by using the direct 
encoding for int we can omit those bits which are the same in all equivalent binary words. 
As 
900 = 512 + 256 + 128 + 4 
999 = 512 + 256 + 128 + 103 
these are just the first three bits of the equivalent binary words of length 10. The last seven 
remain for the encoding. 
For the interval int [1900 .. 1999] in our example it is not quite so simple: we have 
1900 = 1024 + 512 + 256 
+ 64 + 44 
1999 = 1024 + 512 + 256 + 128 + 64 + 15 
If we now use eight bits instead of seven no special reconstruction measures are necessary, 
if the year is used numerically. The interval int [1000 .. 1025] shows, however, that in this 
way all possible savings of bits can be lost. 
We can remedy this by subtracting a suitable number, e.g. the lower bound of the inter-
val before encoding directly (relative encoding). The numerical value can then only be ob-
tained by reconstruction. However, when objects of an interval are subject only to the 
binary operation . -. or to the comparison operation, we no longer need to reconstruct the 
numerical equivalent (translation invariance of subtraction and comparison operations). 
Addition of a suitable integer to a relative-encoded number produces the correct 
(relative-encoded) result (translation covariance of addition). This will prove to be impor-
tant later for relative addressing (comp. 7.6.1). 
As such considerations serve only to increase efficiency (in this case saving of storage 
space), they should by no means occur in the problem solution itself: they are suitably 
concealed within a computational structure. In program development the computational 
structure of the unpacked objects is usually dealt with first. It is later implemented by a 
computational structure for packed objects. 
46 Thus we need card(:~;) bits for representing all subsets of a set X according to 3.6.4.2. 
47 PASCAL provides the possibility to force the compiler to «pack» and «unpack» composite 
objects. 

266 
3. Computational Structures 
Addendum to Chapter 3. Notations 
The introduction of computational structures and abstract types in programming 
languages is very new and suitable notations are still in the process of development. Zilles 
1974 uses the specification 
Functionality 
Axioms 
CREATE: -+STACK 
TOP(PUSH(S, /))=I 
PUSH: 
STACKxiNTEGER-+ STACK TOP(CREATE) = INTEGERERROR 
POP: 
STACK-+ STACK 
POP (PUSH (S, /)) = S 
TOP: 
STACK-+ INTEGER 
POP (CREATE) = STACKERROR, 
for giving the functionalities and the properties of the operations of a computational struc-
ture "stack". 
Liskov, Zilles 1975 give a specification in the following form: 
1 STACK(CREATE) 
2 (STACK (S) & INTEGER(/) 
) STACK (PUSH (S, /)) & 
[POP (S) * STACKERROR ) STACK (POP (S))] & 
[TOP (S) * INTEGERERROR ) INTEGER (TOP (S))] 
3 (VA) [A (CREATE) & 
(V S) (V /) [STACK (S) & INTEGER(/) & A (S) 
) A (PUSH (S, /)) & 
[S * CREATE ) A (POP (S))]] 
) (V S) (STACK (S) ) A (S)] 
] 
4 STACK (S) & INTEGER (/) ) PUSH (S, I) * CREATE 
5 STACK (S) & STACK (S') & INTEGER(/) 
) (PUSH (S, /) = PUSH (S', /) ) S = S'] 
6 STACK (S) & INTEGER(/) ) TOP (PUSH (S, /)) = I 
7 TOP (CREATE) = INTEGERERROR 
8 STACK (S) & INTEGER(/) ) POP (PUSH (S, /)) = S 
9 POP (CREATE) = STACKERROR 
1 and 2 together with 7 and 9 determine the signature, i.e. the functionalities and there-
strictions on the domains (CREATE, PUSH, POP, TOP correspond to empty, append, 
rest, top in STACK (3.2.1)). 4 corresponds to the property E2 there, 6 and 8 are the 
counterparts of T and R; 5 expresses the injectivity of PUSH with respect to its first 
argument (comp. AA in 3.6.2.2). Note that 4 and 5 can be derived from the other axioms. 
The data type induction axiom 3, finally, is equivalent to the Principle of Generation. 
Guttag 1975 and also Goguen, Tardo 1977 use a notation which is strongly oriented 
towards algebra. 
Concrete computational structures are specified in CLU (Liskov et a!. 1977) and 
ALPHARD (Wulf eta!. 1976). The computational structure BS (3.1.2), implemented by 
records comprising a counter and an array of fixed length (comp. 3.6.3), has in CLU the 
heading 

Addendum to Chapter 3. Notations 
stack: cluster (element-type: type) 
is push, pop, top, erasetop, empty; 
rep (type_param: type) = (tp: integer; 
e_type: type; 
stk: array (1 .. ) 
of type_param, vvvvvvv 
267 
to which the implementation of push, pop, top, erasetop, empty is annexed. In 
ALPHARD the same information is divided into specification of functionality ("specifica-
tion") and construction of the object structures ("representation"). 
PASCAL provides as set-like objects only subsets of a finite set m; the corresponding 
mode indication is set of m. Sets are denoted using square brackets, 
[1' 4, 9, 16, 25] 
as braces are reserved for comments. 
The programming language SETL which is oriented at set theory provides besides the 
explicit formulation of sets by enumeration 
{1, 4, 9, 16, 25}, 
also a characterization by predicates, e.g. 
{i x i, i E nat 11 ~ i ~ 5}, 
or short 
{i X i, 1 ~ i ~ 5} 


Mutuality (Selzer, "Nonthing II") 
Chapter 4. Transformation into Repetitive Form 
" ... the transformation from recursion to iteration is 
one of the most fundamental concepts of computer 
science." 
D. E. Knuth 1974 
The stack machine requires a protocol stack and a value stack in order to store "pending" 
operations and their operands. This is not necessary for repetitive routines and systems. 
The stack machine can then degenerate into a Babbage-Zuse machine. In this chapter we 
discuss methods and approaches for the transformation of certain recursive routines and 
hierarchically structured systems of routines into repetitive form. 
4.1 Schemes and Transformations 
In 1.4.3 classes of routines (and systems) were introduced - linear recursive routines and 
repetitive routines. Certain subclasses of these can be simply described syntactically: rou-
tines of the form 
funct L = (Am) p: 
if 1!4 (mJ then cp(L (x (mJ), t (mJ) 
else £(mJ 
fi 
(where .%: A ---> A and cp: (p X v) ---> p, t: A ---> v, £': A ---> p) are linear recursive routines, 
and those of the form 

270 
4. Transformation into Repetitive Form 
funct R "' (J. m) p: 
if~ [mJ then R(..?t' [mJ) 
else £ [m J 
fi 
are in particular repetitive. 
Such a "form" is called a scheme of routines: the scheme parameters standing for 
modes (J., p, ... ), expressions (,;{, ,g; Jf, ... ), and predicates (~. 't, ... ) can be 
instantiated (interpreted) by concrete modes and routines (we should pay attention here to 
the correct composition of modes). Such an interpretation I associates with each scheme, L 
orR, a routine denoted by L1 or R1 • 
Two schemes are called (strongly) equivalent 1 if for all interpretations the associated 
routines are equivalent with respect to the range-of-values (comp. 1.1). Analogously the 
concepts "operationally equivalent" and "descendant" are directly transferred from rou-
tines to schemes (comp. 1.11.4 and 1.11.3). Note that the equivalence includes that both 
schemes either terminate or do not terminate. 
Frequently two schemes are not equivalent for all interpretations but only for those 
which conform to certain restricting conditions such as associativity of operations, 
existence of neutral elements, etc. In these cases the schemes are called equivalent with 
respect to a class of interpretations 2 (defined by the conditions). 
Every pair of schemes P, Q defines a transformation 
p 
{ 't 
Q 
where the condition 't indicates the class of interpretations for which Q is a descendant of 
P (it may be empty). If Q is a descendant of P for all interpretations, the condition 't can 
be omitted. An obvious example is the determinate implementation of a non- deterministic 
construction.In many cases the reverse transformation 
p 
Q 
is valid only for another condition 't' (comp. e.g. (b) below). If one does not mind losing 
information, both transformations can be combined in the symmetric transformation 
p 
{ 't 1\ 't' 
Q 
Paterson and Hewitt 1970. 
2 This term is used by Courcelle and Nivat 1976. 

4.1 Schemes and Transformations 
Examples of transformations: 
(a) "Negation of the condition" 
if 
:!1 (m J then o/1 (m J else "f/ (m J fi 
if 1 
:!1 (mJ then r(mJ else o/1 (m] fi 
This is an elementary strong equivalence for alternatives (1.3.3). 
(b) "Cascade-type test" 
if 
:!1 (mJ 
0 
:!1 (mJ 
1\ %'1 (m J 
then °k'1 (m J 
A 
1 
%'1 (m) then o//2 (m J 
A %'2 (m J 
then 11 (m J 
271 
true} 
0 1 
:!1 (mJ 
0 1 
:!1 (mJ 
1\ I 
%'2 ( m J then yZ ( m] fi 
[vm: %'1 (mJ * D 1\ %'2 [m) * D 
:!1, 
%'~> %'2 determinate 
if .OJ ( m) then if %'1 ( m J then o//1 ( m J 
else o//2 (m) fi 
else if %'2 [m J then 11 [m) 
else rz (m) fi fi 
This follows - like (a) - from the definition of guarded expressions. 
If 1:?1 = %'2 we obtain from (b) 
(c) "Exchange of two tests" 
u [!I Cm J then if \:? Cm J then °v1 Cm J 
else o//2 ( m J fi 
else if %' Cm J then ·tj Cm) 
else 12 ( m) fi fi 
vm: ~:?(m) *Q} 
{vm::!l(m) *Q 
if %' ( m J then if .OJ ( m J then o//1 ( m J 
else 11 [m) fi 
else if :!1 ( m J then 1k'2 [m) 
else rz CmJ fi fi 
Similarly, with :!1 ( m J 1\ \:? C m) = false we obtain 
(d) "Exchange of disjoint branches" 
if :!1 C m J then °k' C m) 
else if 1:? [m) then t{m) 
else 1fl (m) fi fi 
if 1:? (mJ then f"'(mJ 
else if .OJ ( m J then o/1 ( m) 
else 1fl [ m J fi fi 
\vm: :!1 (m] * Q A %'(m) * Q 
l 
A (go (m) 
A 
%' (m) = false) 

272 
4. Transformation into Repetitive Form 
These examples of transformations are usually applied "intuitively" and concern 
cascades of branchings only. (Theoretical investigations in this field were formerly called 
"switching theory" and were recently revived - without acknowledging the connection -
as "decision table techniques".) 
In the following paragraphs nontrivial transformations of recursively defined routines 
will be dealt with, in particular those which lead to repetitive routines. Transformations 
such as 
(e) "Distribution of an alternative" (comp. 1.7.1) 
cp(F(if ~ [mJ then .x; [mJ else .Xz[mJ fi), rff [m]) 
and corresponding transformations for non-deterministic constructs will frequently be 
used. 
4.2 Treatment of Linear Recursion 
The transformations considered in this section are restricted to linear recursive routines 3• 
Typically, we proceed from a scheme L, as was discussed in 4.1. Pioneer work on these 
methods was done in 1966 by Cooper, who introduced the techniques of operand com-
mutation and function inversion. The technique of re-bracketing, now to be discussed 
first, was mentioned by Darlington and Burstall in 1973 and was likewise attributed to 
Cooper. 
4.2.1 The Technique of Re-Bracketing 
The idea behind the technique of re-bracketing is as follows: If the scheme L terminates 
after the n-th incarnation we have 
where a; stands for rff [.?f; [ m J J and b for £' [ .Jf" [ m J J. 
Now we assume, that to cp: p x v __. p there corresponds another function ("associative 
dual") IJI: v x v -+ v such that within the domain of the arguments 
cp( cp(r, s), t) = cp(r, IJI(S, t)) 
holds. Then the following expressions for L(m) can be derived one after another by 
successive re-bracketing, where the above equality is always applied to the leftmost cp: 
3 For simplicity we will frequently use the term "routines" instead of "routine-schemes". 

4.2 Treatment of Linear Recursion 
(jl{<jl(<jJ( ... <jl(<jJ(b, on_ 1), on_ 2), ... , o3), o2), lji(01, o0)) 
<jJ(<jJ( ... <jl(<jJ(b, On-1), On-2), ... , 03), \j/(02, \j/(01, Oo))) 
<jl(<jJ(b, On-1), lji(On-2• \ji(On-3• ... \j/(03, \j/(02, \j/(01' Oo))) ... ))) 
<jl(b, lji(On-1, lji(On-2• ... \j/(03, \j/(02, \j/(01' Oo))) ... ))) 
273 
Computation can now take place without "pending" operations. The transformation is 
thus 
funct L = (A. m) p: 
if !!d Cm J then <jJ(L(.Jt' [m ]), Iff [m ]) 
else £[mJ 
fi I \j/: v X v---> v: 
funct L = (A. m) p: 
L 
<jl( <jl(r, s), t) = <jl(r, \j/(S, t)) 
I if !!d [m]then G(x[mJ, Iff [m]) 
else £ C m J 
fi where 
funct G = (A. m, v z) p: 
if !!d [mJ then G(:t' [mJ, \j/(1! [mJ, z)) 
else<jJ(£[m],z) 
fij 
This is formally proved by induction. 
For arbitrary (jl the determination of a suitable function \jl is only in exceptional cases 
sufficiently simple to be of practical use. Note that for a given routine there may be dif-
ferent parsings into (jl and Iff. 
Example: Let p be a linear space 1', and v a semigroup of linear mappings of this space 
into itself. Choose as \jl the (associative) composition of these mappings. 
However, if v = p and (jl is associative, and therefore - with r as instead of <jl(r, s) -
'II v r, s, t: (r as) a t = r a (s a t)4, 
then (jl can be chosen as ljl. The following transformation emerges: 
funct L = (A. m) p: 
if !!d [mJ thenL(x[m]) a Iff [m] 
else £[mJ 
fi 
{ (r as) a t = r a (s a t) 
funct L = (A. m) p: 
I if !!d [m]then G(x[mJ, Iff [m]) 
else £ Cm J 
fi where 
funct G = (A. m, p z) p: 
if !!d [mJ then G(x[mJ, Iff [mJ az) 
else £ C m J a z 
fl J 
4 In the following transformations we will mostly omit the quantification in the conditions (which 
can be easily supplied). 

274 
4. Transformation into Repetitive Form 
Example: The routine jac (1.4.1 a)) results from the scheme L by interpreting 
land p as 
nat 
tW (mJ 
as 
m * 0 
.;r (m J as m- 1 
0' (m J as 
m 
ras as s x r (note the order!) 
.YP(m] 
as 1 
a is associative. The following embedding results: 
functjac = (nat m) nat: 
I if m * 0 then G(m - 1, m) 
else 1 
fi where 
funct G = (nat m, nat z) nat: 
if m * 0 then G(m - 1, z x m) 
else z x 1 
fi J 
The last scheme can be simplified if p has a neutral element e with respect to the opera-
tion a. We then obtain the transformation 
funct L =(I.. m) p: 
if tW Cm J then L(.;t' [m ]>a 0' [m J 
else Jf [m J 
fi 
(<r a s) a t = r a (s a t) 
l v p r: r a e = r 
funct L = (l m) p: 
I G(m, e) where 
funct G = (I.. m, p z) p: 
if tW [mJ then G(.;t'[mJ, 0' [mJ a z) 
else Jf C m J a z 
fi J 
For the examplejac we obtain directly (e being interpreted as 1) 
functjac = (nat m) nat: 
I G(m, 1) where 
funct G = (nat m, nat z) nat: 
if m * 0 then G(m- 1, z x m) 
else z x 1 
fi J 
Note that the notation here has been shortened, but that an additional multiplication 
(by 1) has been introduced. G is obviously equivalent with fact of 1.6.1. 
Finally, we consider another special case which concerns some examples in Chap. 2, 
namely the scheme 

4.2 Treatment of Linear Recursion 
funct R = (sequ x a, J1Y) sequ x: 
if:!# (a, yJ then £(a, yJ 
else append(R(rest(a), y), top(a)) fi. 
275 
Because append(a, x) = conc(append(empty, x), a) (3.4.3.1) and because of the as-
sociativity of cone (comp. 3.4.3) this scheme is equivalent to 
funct R ., (sequ x a, J1Y) sequ x: 
I G(a, empty) where 
funct G = (sequ x a, sequ x z) sequ x: 
if :!# (a, y J then cone(£' (a, y J, z) 
else G(rest(a), stock(z, top(a))) fi J 
Here we have finally replaced conc(z, append(empty, top(a))) by stock(z, top(a)) 
(compare the definition of cone in 3.4.3.1 and of stock in 3.4.1.1). 
Corresponding transformation schemes can be applied to some of the routines of 2.10; 
e.g. for sort we obtain 
funct sort = (sequ x a, x x, funct (x) int m) sequ x: 
I g(a, 0) where 
funct g = (sequ x a, sequ x z) sequ x: 
if a = 0 '"' m(top(a)) ~ m(x) 
then conc(append(a, x), z) 
else g(rest(a), stock(z, top (a))) fi J 
On the other hand an application of the scheme to cone results in a tautology which is 
to be expected. 
The occurrence of stock indicates the importance of symmetrical implementations of 
STACK (see also 2.14.1.2 and 7.5.2). 
Exercise 1: Modify delete of 2.10 such that the scheme above is applicable. 
Exercise 2: For the scheme 
funct M =(sequ x a, JLY) p: 
if JJJ [a, yJ then .Yf' [a, yJ 
else y (M(rest(a), y), 2! (top(a), y J> fi 
(where y is associative) give a transformation to repetitive form and apply it to the rou-
tines length and contains of 2.10. 
4.2.2 The Technique of Operand Commutation 
"Pending" operations can also be eliminated if the order of their executions can be 
permuted, that is if ljf: p x v --+ p exists such that 
(1) 
cp(llf(r, s), t) = llf(cp(r, t), s) 
holds. The expression (with b = £' (mo J, mo = x" [m J and a; = rff (xi (m JJ) 

276 
4. Transformation into Repetitive Form 
then changes into 
provided that for all v x 
(2} 
cp(Jf[moJ, x) = IJI(Jf[moJ, x) 
holds. If cp is right-commutative, 
cp( cp(r, s }, t) = cp( cp(r, t), s), 
then IJ1 can be chosen to be cp; (2) trivially holds. If cp is associative, 
cp(cp(r, s}, t) = cp(r, cp(s, t}), 
then IJI(r, s) = cp(s, r) will do; (2) turns into the condition 
cp(Jf[moJ,x) = cp(x, Jf[moJ). 
A prerequisite for this transformation to work is that for mo, the argument on termination 
of the recursion, the result .Jit" [ mo J is known from the very start. 
In particular, the argument on termination is not needed if in the scheme L the expres-
sion Jf [ m J does not depend on m. Thus we have the transformation 
funct L "' (A. m) p: 
if .OJ [mJ then cp(L(X' [mJ), ,g [mj) 
else co 
fi {IJI: P x v ..... p: 
q>(IJI(r, s), t) = IJI(q>(r, t), s) 
funct L "' (A. m) p: 
cp(co, x) = IJI(co, x) 
I G(m, co) where 
funct G "' (A. m, p z) p: 
if~ [mJ then G(X'[mJ, IJI(Z, ,g [mJ)) 
else z 
fi J 
where co is a constant. Here, too, the formal proof requires induction. 
Example: For the routine fac we know that cp is right-commntative, co = 1. We obtain 
(camp. 4.2.1) 
functjac "' (nat m) nat: 
I G(m, 1) where 
funct G "' (nat m, nat z) nat: 
if m * 0 then G(m - 1, m x z) 
else z 
fi J 

4.2 Treatment of Linear Recursion 
277 
The argument on termination of the recursion can also be determined if 
{Am: -, fJ [ m J} is a singleton, then the value on termination is A mo = 1 Am: -, fJ [ m J . 
In this case the condition !!I [ m J in the scheme L may be replaced directly by m * mo. 
If the above-mentioned special case does not apply then the argument on termination 
of the recursion can still be determined by "precomputation". However, this results in an 
involved and in general inefficient repetitive system: 
funct L = (Am) p: 
if !18 (mJ then cp(L(.:X' [mJ), c [m]) 
else £' [ m J 
fi {111: P x v -+ p: 
cp(IJI(r, s), t) = IJI(q>(r, t), s) 
1Jf(£[moJ, r) = cp(£'[moJ, r) 
funct L = (Am) p: 
1 determinate 
I G(m, £(moJ) where 
Amo = F(m), 
funct F = (An) A: 
if !!I [nJ thenF(1[nJ) 
else n 
fi, 
funct G =(Am, pz) p: 
if !!I [mJ then G(1[mJ, w(z, C [mJ)) 
else z 
fi J 
Note the following possible simplification: 
I G(m, F'(m)) where 
funct F' = (An) l: 
if !!I [n) thenF'(1[n]) 
. else £[nJ 
fi, 
funct G = .. . 
J 
We have an important special case, if cp does not depend on its second argument, that is 
cp(r, s) = y(r). Right-commutativity then holds and we obtain e.g. the transformation 5 
funct L = (l m) p: 
if !!I [mJ then y(L(.:X' [m])) 
else co 
fi 
funct L = (l m) p: 
I G(m, co) where 
funct G = (lm, pz) p: 
if !!I [m J then G(.:X' [m J, y(z)) 
else z 
fi J 
5 This transformation, with co as a fixed parameter, can be found in Morris 1971. 

278 
4. Transformation into Repetitive Form 
If, on the other hand, !!A ( m J is again of the form m * mo, one obtains the version 
funct L "' (l m) p: 
ifm * motheny(L(.f(mJ)) 
else x(mJ 
fi 
funct L "' (l m) p: 
I G(m, x(moJ) where 
funct G "' (l m, p z) p: 
if m * mo then G(.f (m J, y(z)) 
else z 
fi J 
4.2.3 Function Inversion 
4.2.3.1 The techniques of re-bracketing or operand commutation require rather stringent 
conditions on the function q>, such as associativity or right-commutativity. 
The technique which we now consider is based on function inversion. Starting from the 
argument on termination it reconstructs the parameter values of all the incarnations and 
performs the "pending" operations upon them until the initial parameter value is recover-
ed. The only prerequisite is that the function .f is invertible on its range. 
Based again on a "precomputation", we have the following transformation 
funct L "' (l m) p: 
if !!A (mJ then q>(L(.f (mJ), c (mJ) 
else Jf (m J 
fi 
funct L "' (l m) p: 
I R(mo, J'i"(mo]) where 
l mo "'P(m), 
funct P "' (l n) l: 
if !!A (nJ then P(.f (n ]) 
else n 
fi, 
funct R "' (ly, p z) p: 
{ v lx: .i(x-(xJJ = x 
if y * m then R(.i (YJ, q>(Z, C (.i[yJJ)) 
else z 
fi J 
Note that mo is used twice. The transformation is intuitively clear, the formal proof 
requires induction. 
By the way, it is sufficient if the condition 
3 .i: V X E {.fi(mJ: i E fN, mEl}: .i(.f(xJJ =X 
holds where lis the domain of L. 

4.2 Treatment of Linear Recursion 
279 
Example: A. = int, 1: is doubling. Then Jr is halving (of even numbers). 
If 1: is injective and of the order oo, then the above condition can certainly be fulfilled. 
The routine for f must nevertheless be given explicitly. 
A further example is given by the computation of cosine in the interval [- 1t .. 7t]. 
(X) 
2i 
Only in the interval [- 1t/8 .. 1t/8] should the series expansion rc(x) = [ (- 1) i _x_ be 
used for approximation. Therefore the recurrence relation 
;~o 
(2i)! 
cos(x) = 2cos2(xl2) -
1 
is used for arguments outside the above interval. Thus we obtain the elegant (but not very 
efficient) routine6 
funct cos = (real x) real: 
if lx I> t then 2 x cos2(x/2) -
1 
else rc(x) 
fi 
With the inverse function Jr [x) = 2 x x, application of the scheme yields 
funct cos = (real x) real: 
f R(xo, rc(xo)) where 
real xo = P(x), 
funct P =(real x) real: 
if lx I > t then P(x/2) 
else x 
fi, 
funct R =(real y, real z) real: 
if y * X then R (2 X y, 2 X z2 -
1) 
else z 
fi J 
Note here, by the way, that due to possible rounding errors, the comparison y * x should 
be replaced by IY - xI> 10-•, where e depends on the precision of the machine. 
4.2.3.2 In the present case, in particular, a variant of functional inversion with an "ac-
companying count" suggests itself. A further parameter of mode nat is added toP and R. 
It is incremented in P and decremented in R. It is initialized to 0 when calling P. The final 
value of this parameter is added as a further result to P and the counting parameter of R is 
initialized to it when calling R. Hence the testy * m can be replaced by the test of the 
counting parameter for 0. The only advantage could be the avoidance of a test for equality 
of complex objects involving much work or an unsafe test for equality involving calcula-
tion with rounding. 
For the above example the following results: 
funct cos = (real x) real: 
f R(io, xo, rc(xo)) where 
(nat io, real xo) = P(O, x), 
6 In Numerical Analysis more efficient and stable methods are applied in which approximating poly-
nomials are used. 

280 
funct P = (nat i, real x)(nat, real): 
if lx I> t then P(i + 1, x/2) 
else (i, x) 
fi, 
funct R =(nat i, real y, real z) real: 
4. Transformation into Repetitive Form 
if i =F 0 then R(i- 1, 2 x y, 2 x z2 -
1) 
else z 
fi J 
4.2.3.3 Just as in the case of operand commutation there is a possibility of simplification if 
mo can be determined from the condition for termination: 
funct L = (1. m) p: 
if m =F mo then cp(L(.Jf [m]), ~ [m]) 
else £' [ m J 
fi 
funct L = (I. m) p: 
I R(mo, £[mo]) where 
funct R = (l.y, p z) p: 
{ v h: 1[f[xJJ = x 
if y =F m then R ( 1 (y J , cp(z, ~ [ 1 (y J J)) 
else z 
fi J 
For the routine jac (see above) we have e.g. 
funct fac = (nat m) nat: 
I R(O, 1), 
funct R = (nat y, nat z) nat: 
if y =F m then R(y + 1, (y + 1) 
else z 
x z) 
fi J 
4.2.3.4 Another kind of "function inversion" technique is provided by the processing of 
sequences "from left to right" or "from right to left". The first of these two directions is 
expressed by the pair of operations (top, rest), the second by the pair (bottom, upper). The 
inversion, therefore, does not apply to a single function but to several cooperating func-
tions. This is expressed in the following scheme: 
funct L = (sequ ll m) p: 
if 1 isempty(m) then cp (L(rest(m)), ~ [top(m)]) 
else co 
fi 
funct L = (sequ ll m) p: 
G(m, co) where 
funct G = (sequ JlY, p z) p: 
if 1 isempty(y) then G(upper(y), cp(z, ~ [bottom(y)])) 
else z 
fi 
and its dual form as used in the following example: 

4.2 Treatment of Linear Recursion 
281 
The routine for computing the value of a number in radix notation for the base B (see 
3.5.6) 
funct conv = (sequ digits) nat: 
if 1 
isempty(s) then B x conv(upper(s)) 
else o 
+ val(bottom(s)) 
by applying the transformation rule, is converted into 
funct conv = (sequ digits) nat: 
I w(s, 0) where 
funct w = (sequ digit y, nat z) nat: 
if 1 
isempty(y) then w(rest(y), B 
else z 
x z + val(top(y))) 
fi 
fi J 
Exercise 1: Show that the linear recursive routine cone in 1.4.1 and the repetitive version cone in 3.4.3 
are connected by the transformation above. 
Exercise 2: What is obtained by function inversion from the routine (comp. 1.11.3.2) 
funct pow .. (p a, sequ bit n, p z) p: 
if isempty(n) then z 
elsf bottom(n) = 0 then bis(pow(a, upper(n), z)) 
0 bottom(n) = L then ap bis(pow(a, upper(n), z)) fi 
where funct bis .. (p a) p: apa. 
4.2.3.5 Remark: The "induction scheme" 7 
f(k, 0) = g(k) 
f(k, i + 1) = h(f(k, i), k, i) 
which is characteristic of "primitive recursive" functions is written in our notation as fol-
lows: 
functj = (11 k, nat i) p: 
if i > o then h(f(k, i -
1), k, i -
1) 
else g(k) 
fi 
:i exists, mo proves to be 0. Function inversion yields 
funct f = (11 k, nat i) p: 
I G(O, g(k)) where 
funct G = (nat y, p z) p: 
if y * i then G(y + 1, h(z, k, y + 1)) 
else z 
fi J 
7 Comp. Hermes 1978, § 10. Note that the following result is of theoretical importance only. The 
practical problem of bringing a given primitive recursive function in this form is not considered. 

282 
4. Transformation into Repetitive Form 
(Note that actually A. = ("' nat) and thus 
I G(k, 0, g(k)) where 
funct G = (I' x, nat y, p z) p: 
if (x, y) =1= (k, i) then G(x, y + 1, h(z, x, y + 1)) 
else z 
fi J 
results schematically. As xis a fixed parameter of G, the above simplification is derived.) 
The class of primitive recursive functions is, therefore, not wider than the class of func-
tions defined by repetitive routines and systems (comp. Rice 1965). 
4.2.4 Function Inversion According to Paterson and Hewitt 
Even if there are none of the above restrictions, that is if an arbitrary interpretation is per-
mitted for the linear recursive scheme L, L can be changed into a repetitive scheme. The 
corresponding transformation given by Paterson and Hewitt in 1970, however, is only of 
theoretical interest, as it leads to very inefficient computations. 
We will proceed from the consideration at the beginning of 4.2.1: If the scheme L 
terminates after the n-th incarnation, then L(m) = v0, where 
v; = <p(v;+ 1, a;) (i = 0, ... , n- 1) 
and 
v. = b 
If the value of n is established first, b = .Yt [ £" ( m J J and then successively 
can be repetitively computed. 
In this method the use of the inverse function :i - as shown in the last paragraph -
for the transition from £i+ 1 [ m J to £; [ m J is replaced by computing £; [ m J anew in each 
case. The following routine serves this purpose 
funct inv = (A. m, A. v) A.: 
if £ [ m J = v then m 
else inv(£ [m J, v) fi 
It terminates on comparison with the (known) value of £i+ 1 [ m J. In the scheme for func-
tion inversion (4.2.3) all occurrences of :i (Y J are replaced by the call inv(m, y). 
Obviously £ is used about m times. Therefore it would be better to store the already 
computed values of£; (this leads to the introduction of stacks, 4.2.5). 
A variant of this method which is suitable for composite objects with an involved com-
parison operation is obtained if one does not compare with £i+ 1 [ m J but with the counter 
i. Then £ i [ m J is computed by 

4.2 Treatment of Linear Recursion 
283 
funct it = (A. m, nat i) A.: 
if i * 0 then it ( .Jt' ( m J , i -
1) 
else m 
fi 
A condition is that the greatest value of i be determined at the beginning. Altogether we 
obtain the transformation scheme 
funct L = (A. m) p: 
if !!I ( m J then cp(L ( .Jt' ( m J J, <! ( m J) 
else Jf'(mJ 
fi 
funct L = (A. m) p: 
I G(no, Jf' ( mo J) where 
(A. mo, nat no) = n(m, 0}, 
funct n = (J..y, nat i)(J.., nat): 
if !!I (y J then n ( .Jt' (y J , i + 1) 
else {y, i) 
II, 
funct it = (J..y, nat i) A.: 
if i * 0 then it ( 1 (y J , i -
1) 
else y 
II, 
funct G = (nat i, p z) p: 
{ .Jt' determinate 
if i * 0 then G(i - 1, cp(z, c (it(m, i - 1}])) 
else z 
fi J 
4.2.5 Function Inversion by Introducing Stacks 
4.2.5.1 In most of the methods discussed so far the transition from linear recursive rou-
tines to repetitive ones is only possible under certain conditions (4.2.1, 4.2.2, 4.2.3). In this 
paragraph methods are developed which, by extending the object structure, generally 
render possible such a transition without leading to the inefficient computations which re-
sult from the Paterson-Hewitt transformation (4.2.4). 
In paragraph 4.2.3 it was shown that a routine of the form 
funct L = (A. x) p: 
if !!I (xJ then cp(L(.Jt' (x]), <! (x]) 
else Jf' (x J 
fi 
can be transformed into a repetitive routine, whenever for .Jt' an inverse function .i' exists 
such that .i' ( .Jt' (x J J = x. (Note that the property .Jt' ( .i' (x J J = xis not demanded.) 
Considering a computation structure of the abstract type STACK()() of 3.2.5 with this 
in mind, one finds the property R 
rest(append(s, x)) = s 
guaranteed for every object stack 1.. s and arbitrary 1.. x. Therefore the operation rest -

284 
4. Transformation into Repetitive Form 
with respect to the stack - is the inverse function for the operation append. The following 
development is based on this fact. 
To avoid excessive nesting of brackets in the sequel we write 
s & x 
for 
append(s, x) 
tops 
for 
top(s) 
rests for 
rest(s) 
Our first step is to embed L into an equivalent routine L * having an additional parameter 
of mode stack)..: 
funct L = ().. x) p: 
I L *(x, empty) where 
funct L * = ().. x, stack ).. sx) p: 
if !1# [x) then <p(L*(f[x), sx&x), c [x)) 
else £ [x} 
fi J 
The additional parameter sx seems to be superfluous as it is not used anywhere in the 
body of the routine. Its effect is simply that for the function 
1*[x,sx) = (1[xJ,sx&x) 
an inverse function 
.i• [x, sx J = (top sx, rest sx) 
now exists (which, by the way, depends only on the second argument sx). Applying the 
scheme in 4.2.3 to L * now yields in quite a formal way 
funct L = ().. x) p: 
I R(xo, so, .Yf [xo J) where 
()..xo, stack )..so) = P(x, empty), 
funct P = ().. x, stack ).. sx)().., stack )..): 
if !!J [xJ then P(f[xJ, sx&x) 
else (x, sx) 
fi, 
funct R = ().. y, stack ).. sy, p z) p: 
if (y, sy) * (x, empty) 
then R (top sy, rest sy, <p(z, c [top sy J)) 
else z 
fi J 
The routine R can be simplified considerably by substituting for the comparison (y, sy) 
* (x, empty) the equivalent expression sy * empty. In this way the parameter yin R be-
comes completely superfluous. This also permits a simplification of P by directly returning 
the value of .Yf [xo J instead of xo. A summary of the complete development so far yields 
the general transformation rule 

4.2 Treatment of Linear Recursion 
funct L = (J..x) p: 
if !!A CxJ then cp(L(.Jf [xJ), C [xJ) 
else .1f [x J 
fi 
funct L = (I.. x) p: 
I R(P(x, empty)) where 
funct P = (A. x, stack J.. sx)(stack A., p): 
if !!A [xJ thenP(.Jf(xJ,sx&x) 
else (sx, .Jf [x J) 
fi, 
funct R = (stack J.. sy, p z) p: 
if sy =1= empty then R(rest sy, cp(z, C [top syJ)) 
else z 
fi j 
285 
As this transformation does not depend on any condition it represents a universal 
method of transferring linear recursive routines into repetitive routines. Note, however, 
that the non-repetitive character of the recursion is only passed from the routine to the ob-
ject structure. 
4.2.5.2 The introduction of stacks can also help to achieve increased efficiency in routines 
which contain abbreviating object declarations. Consider for instance the scheme 
funct L 1 = (I.. x) p: 
illY = "# [xJ within 
if !!A [x, y J then cp(L1 ( .Jf [x, y J ), C [x, y J) 
else .Jf [x, y J 
fi J 
Three steps are necessary for the treatment of the routine L 1 : 
-
elimination of the object declaration by consistently replacing y by "# [x J , 
-
application of the above scheme, 
-
re-introduction of the object declaration llY = "# [ ... J. 
Hence we obtain for determinate "# 
funct L 1 = (I.. x) p: 
I R(P(x, empty)) where 
funct P = (I.. x, stack 1.. sx)(stack A., p): 
illY = "# [xJ within 
if !!A [x,yJ thenP(.Jf[x,yJ,sx&x) 
else (sx, .1f [x, y J) 
fi J , 
funct R = (stack J.. sy, p z) p: 
if sy =1= empty then ll y = "# [top sy J within 
R (rest sy, cp(z, c [top sy, y J)) 
else z 
fi j 
It is obvious that for all parameter values the expression "# has to be computed both in 
the "to" direction in the routine P and in the "fro" direction in the routine R. This double 
computation should be avoided, especially if "# is time-consuming. The introduction of a 
further (at first superfluous) parameter is again of assistance here: 

286 
4. Transformation into Repetitive Form 
funct L 1 "" (I.. x) p: 
I L 1* (x, empty, empty) where 
functLt* ""(l..x, stack J..sx, stack llSY) p: 
illY = ':1 [xJ within 
If !!A [x, y J then cp(Lt*(.1' [x, y J, sx & x, sy & y), c [x, y J) 
else £ [x, y J 
fi J J 
The decisive property is now that for the two stacks 
top sy = ':1 [top sx J 
always holds. After transformation by function inversion the declaration 
llY = ':1 [top sx] in routine R can be replaced by llY = topsy. 
This development can also be summarized in a general transformation scheme 8 : 
funct L 1 = (I.. x) p: 
illY "" ':1 [x J within 
if !!A [x, y J then cp(L1 ( .f [x, y J ), C [x, y J) 
else £ [x, y J 
fi J 
funct L 1 = (l..x) p: 
I R(P(x, empty, empty)) where 
funct P = (l..x, stack J..sx, stack llsy)(stack 1.., stack 14 p): 
illY = ':1 [xJ within 
if !!A [x, yJ then P(.f [x, yJ, sx & x, sy & y) 
else (sx, sy, £ [x, y J) 
fi J , 
funct R = (stack 1.. sx, stack ll sy, p z) p: 
If sx * empty then llY = topsy within 
R (rest sx, rest sy, cp(z, c [top sx, y J)) 
else z 
fi J 
This transformation clearly exposes two important situations in which the introduction 
of the stack sy is unnecessary: 
1. y does not occur in the expression C : in routine R the declaration ll y = top sy and 
thus also the parameter sy become superfluous. This shows quite formally, that a double 
computation does not take place. The parameter sy disappears in P, too. 
2. y does not occur in the expression .f: as in the first case, the object declaration in P 
now becomes superfluous. With the disappearance of the stack sy the object declaration in 
Ragain reads llY = ':1 [top sx J. The one and only computation of ':1 [x J now occurs in R. 
8 For the derivation of this result, we have assumed that '# is determinate. The result, however, also 
holds under weaker conditions. 

4.3 Treatment of Non-Linear Recursions 
287 
4.3 Treatment of Non-Linear Recursions 
By no means all non-linear recursive schemes can be transformed into repetitive form with-
out introducing auxiliary stacks 9• This can already be seen from the simple scheme with 
cascade-type recursion structure 
funct S = (Ax) p: 
if to (xJ then q>(S(..H' (x]), S(Af (x])) 
else .Yf (x J 
fi 
(jib of 1.4.2 is of that type and some algorithms for cascade-type structures in 2.13 are 
also). Paterson, Hewitt and also Strong showed in 1970 that this scheme cannot (without 
further restrictions) be transformed into a repetitive form. In order to prove this they used 
a certain interpretation, namely the concretization of A and pas sequ {Q, L, R}, of ..H' (x J 
as append(x, L), of At (x] as append(x, R), and of q>(s, t) as append(conc(s, t), Q) (the 
so-called "free interpretation"). If .Yf moreover is considered as an identity and to (x J as 
/ength(x) * n for an arbitrary fixed n > 0, then S(empty) terminates and yields 
forn = 1: QLR 
for n = 2: aa~~~Q~fi~ 
for n = 3: QQQLi:l.RIXOLRLRRLQQLLRRLRQLRRRRR 
......__, 
.......__, 
\....._/ '-./ 
\...___) ·v 
\.J v 
etc. 
The attached arcs show the structure of these sequences. 
Paterson and Hewitt now show that at least n + 1 parameters are needed for the 
repetitive computation of S(empty) and that therefore no equivalent repetitive scheme 
exists, because n is not bounded. 
One might suppose that the introduction of a stack of parameter values would help to 
overcome this difficulty. However, Paterson and Hewitt show by means of a further inter-
pretation that with repetitive routines there are difficulties which originate from the con-
trol structure (that is, that the protocol stack in the stack machine is likewise indis-
pensable) 10• 
Accordingly, we will investigate special cases for non-linear recursion, namely those in 
which the explosion of parameter values is controllable (method of functional embedding, 
4.3.1) and those in which the control is manageable (arithmetization of the control, 4.3.2). 
For general nested recursions one is almost completely dependent on individual consider-
ations (4.3.3). 
In addition Paterson and Hewitt point out that neither the set of (uninterpreted) 
schemes which can be transformed into repetitive schemes nor its complement are enumer-
9 The fact that the Babbage-Zuse machine is universal does not contradict this (see Chap. 6 for the 
relation to flow diagrams which are, as is well known, universal for representing computable func-
tions). What it means is simply, that for every recursive routine an equivalent (but not necessarily 
operatively equivalent) repetitive routine can be given. 
10 Interpretation of A. and pas boo!, with Jt and .!Vas predicates and q>(s, t) as if s then t else false 
fi, .JI' (x J as true. It is shown, that every repetitive scheme for a certain interpretation no longer 
terminates in all the situations in which S terminates. 

288 
4. Transformation into Repetitive Form 
able. This theoretical result shows the limits of mechanization. Strong states a narrower 
class of schemes ("anarchic schemes") within which the repetitive schemes can effectively 
be determined. 
4.3.1 Method of Functional Embedding 
In some cases of non-linear recursion for which no direct transformations are possible a 
simple method is successful: Based on the given routine, a "generalized expression" is 
formed from which this routine can be regained by specialization. With the help of the 
transformations unfolding and folding (comp. 1.7.1) the new routine is subsequently 
transformed into a form which is based only on itself and is hopefully repetitive. 
This method is illustrated by the following example (Gnatz, Pepper 1977): 
The routine 11 (originating in de Rham 1947) 
functfuse = (pnat n) pnat: 
if n = 1 
then 1 
D n > 1 " even n thenfuse(f) 
D n > 1 " odd n thenfuse("2 1) + fuse("; 1) fi 
suggests an embedding into a linear combination of fuse(m) and fuse(m + 1), for 
example 
funct F = (pnat m, nat a, nat b: (a, b) =F (0, 0)) pnat: 
a x fuse(m) + b x fuse(m + 1), 
wherefuse(n) = F(n, 1, 0). 
Unfolding ofF with the definition of fuse yields (after algebraic manipulation): 
funct F = (pnat m, nat a, nat b: (a, b) =F (0, 0)) pnat: 
if m = 1 
then a + b x fuse(f) 
D m > 1 " even m then a x fuse( f) + b x [fuse( f) + fuse(f + 1)] 
D m > 1 "odd m then a X [fuse(m2 1) +fuse(m; 1)] + b xfuse(m; 1)fi 
It should be noted that even m implies odd (m + 1), etc. 
Further algebraic manipulations (and renaming of mas n) result in 
funct F = (pnat n, nat a, nat b: (a, b) =F (0, 0)) pnat: 
if n = 1 
then a + b 
D n > 1 " even n then (a + b) x fuse( f) + b x fuse(f + 1) 
D n > 1 " odd n then a x fuse("2 1) + (a + b) x fuse("1, 1 + 1) fi 
This, however, can be folded to the repetitive routine 
11 pnat means {nat n: n * 0} (comp. 2.4). 

4.3 Treatment of Non-Linear Recursions 
funct F"" (pnat n, nat a, nat b: (a, b) =1= (0, 0)) pnat: 
if n = 1 
then a + b 
0 n > 1 " even n then F( f, a + b, b) 
On>1 AOddn thenF("; 1,a,a+b)fi 
Because fuse(n) = F(n, 1, 0), we obtain altogether the embedding 
funct fuse "" (pnat n) pnat: F(n, 1, 0), 
funct F 
"" (pnat n, nat a, nat b: (a, b) * (0, 0)) pnat: 
if n = 1 then a + b 
else if even n then F(f, a + b, b) 
0 odd n then F(";_ 1 , a, a + b) fi fi 
289 
Remark: How can this problem be "solved intuitively"? The calling structure can be 
"computed", see e.g. Fig. 4.1 (with f as an abbreviation of fuse): 
l\ 
f(5) 
+ 
f(6) 
1\ 
\ 
f(2) + f(3) 
+ 
f(3) 
I /\ 1\ 
f(l) + f(l) + f(2) + f(l) + f(2) 
I 
I 
f(l) 
f(l) 
Fig. 4.1 
If we count how often f(i) and how often f(i + 1) occur here, we obtain "by induction" 
the repetitive solution from above: 
Induction hypothesis: f(N) yields 
a calls f(i) 
b calls f(i + 1). 
Case A: i odd, i = 2 x j + 1 . 
Then 
f(i) splits into f(j) and f(j + 1), 
f(i + 1) splits into f(j + 1). 
Result: 
f(N) yields 
a calls f(j) 
a+ bcallsf(j + 1); wherej = (i- 1)12; 
Case B: i even, i = 2 x j. 
Then 
f(i) splits into f(j), 
f(i + 1) splits intof(j) andf(j + 1); where)= i/2. 
Result: 
f(N) yields 
a + b calls f(j) 
b calls f(j + 1). 

290 
4. Transformation into Repetitive Form 
Related to this is the fact thatfuse(n), by establishing the number of the calls produc-
ed, counts the number of paths which are found in the following (cylindrically closed) 
diagram leading from n to 1. 
Fig. 4.2 
Thus the combinatorial character of fuse is somewhat clearert 2• 
A function inversion could also be tried: First of all the sequence it = N, i 2, i 3, ••• , 
is = 1 is established, whereik+t = ik div 2 and then the following is constructed in reverse 
order: 
For N = 11 we have e.g. 
it = 11 
i2 = 5 
i3 = 
2 
i4 = 
1; 
and constructing the reverse order 
f(j4) = f(1) = 1, 
f(j3) = f(2) = 1 , 
asi2 * 2 x i 3, at firstf(j3 + 1) = f(3) = 2, then 
fUJ = f(5) = 3, 
asi1 * 2 x i 2, at firstf(j2 + 1) = f(6) = 2, then 
f(jt) = f(11) = 5 
This also leads to the same repetitive solution. 
Annotation: Table of values for fuse 
2 
3 
4 
5 
6 
7 
8 
9 10 11 
12 13 14 15 16 17 
18 19 
2 
3 
2 
3 
4 
3 
5 
2 
5 
3 
4 
5 
4 
7 
20 21 
22 23 
24 25 
26 27 
28 
29 30 
31 
32 
33 
3 
8 
5 
7 
2 
7 
5 
8 
3 
7 
4 
5 
6 
12 In addition there is a number-theoretic interpretation for fuse, see de Rham 1947. 

4.3 Treatment of Non-Linear Recursions 
291 
The routine for computing the Fibonacci numbers (comp. exercise 1.4.1-1) 
functjib = (pnat n) pnat: 
if n = 1 v n = 2 then 1 
D n > 2 
thenjib(n - 2) + jib(n - 1) fi 
is treated similarly. Again, an embedding in a general linear combination of fib(m) and 
jib(m + 1) is suggested 13 : 
functj = (pnat m, nat a, nat b) nat: 
a x jib(m) + b x jib(m + 1) 
Unfolding of jib(m + 1) leads to 
if m + 1 = 1 then a x jib(m) + b 
D m + 1 = 2 then a x fib(m) + b 
D m + 1 > 2 then a x fib(m) + 
b x jib(m -
1) + b x jib(m) fi 
The branch if m + 1 = 1 vvvvv is void for pnat m and the following remains: 
if m = 1 then a x jib(1) + b 
D m > 1 then b x jib(m -
1) + (a + b) x jib(m) fi 
By folding we finally obtain: 
if m = 1 then a + b 
D m > 1 thenj(m- 1, b, a+ b) fi 
Sincejib(n) = f(n, 1, 0), we obtain 
functjib = (pnat n) pnat: f(n, 1, 0), 
functj = (pnat m, nat a, nat b) nat: 
if m = 1 then a + b 
D m > 1 then f(m -
1, b, a + b) fi 
Besides the development of actual routines like those in 4.2 the method of functional 
embedding has a further field of application: Transformation schemes can frequently be 
proved by this method. Following McCarthy's principle of recursion induction (McCarthy 
1961) for proving two recursively defined functions equivalent, it is to be shown that they 
both satisfy a common functional equation (see also 1.6.1). 
Thus, the equivalence of the following two schemes H, G can be shown (Wossner 
1974), provided that !!I [mJ implies the definedness of 8(m): 
13 Here the trivial combination with (a, b) = (0, 0) is not necessarily excluded. 

292 
4. Transformation into Repetitive Form 
funct H "' (Am, 11 r, 11 s) 11: 
if 
!5J [mJ 
A 
!5J [l>(m)J then CJ>(H[I>(m), r, sJ, H[1>2(m), r, s}) 
D 
!5J [mJ 
A 
..., !5J [l>(m)J then r 
D ..., !5J [ m J 
then s 
fi 
funct G "' (Am, 11 r, 11 s) 11: 
if 
!5J [mJ then G(l>(m), cp(r, s), r) 
D ..., :?J [ m J then s 
fi 
At first it is shown that G is equivalent to 
funct G' "'(Am, 11r, 11s) 11: 
if 
!5J [mJ 
A 
!5J [l>(m)J then G'(l>(m), cp(r, s), r) 
D 
!5J [mJ 
A 
..., !5J [l>(m)J then r 
D ..., .o/1 [ m J 
then s 
fi 
G can be written as 14 
funct G "' (A m, 11 r, 11 s) 11: 
if 
!5J [mJ 
A 
!5J [l>(m)J then G(l>(m), cp(r, s), r) 
D 
!5J [mJ 
A 
..., !!J [l>(m)) then G(l>(m), CJ>(r, s), r) 
D ..., !5J [ m J 
then s 
fi 
under the condition ..., !5J [l>(m)}, however 
G(l>(m), cp(r, s), r) = r 
holds according to the definition of G. 
The equivalence of the schemes G' and His proved by an auxiliary scheme F which 
arises as follows: in functional notation (see 1.6) let H(x) = 't [Hj(x), G'(x) = a [ G'] (x). 
According to the pattern of H, F is now defined as F(x) = 't [ G 'J (x ), 
funct F "' (Am, 11 r, 11 s) 11: 
if 
!5J [mJ 
A 
!5J (l>(m)} then cp(G'(I>(m), r, s), G'(l)2(m), r, s)) 
D 
!5J [m) 
A 
..., !5J [l>(m)) then r 
D ..., !5J [ m} 
then s 
fi 
Unfolding of the first call of G' yields 
if 
.o/1 [m} 
A 
.o/1 [l>(m)) then 
cp(if 
!5J [l>(m)J 
A 
!5J [1>2(m)) then G'(l)2(m), cp(r, s), r) 
D 
!5J [l>(m)) 
A 
..., !5J [1>2(m)} then r 
D ..., !5J [l>(m)} 
then s 
fi, 
G'(l)2(m), r, s)) 
D 
!5J [m) 
A 
..., !5J [l>(m)) then r 
D ..., !5J [ m J 
then s 
fi 
14 Note: In G, q> is computed one time more than in G' (the result of this superfluous computation is 
not used but it always has to be defined). 

4.3 Treatment of Non-Linear Recursions 
293 
or (distribution of <p) 
if 
!I [m J A 
!I [&(m)] A 
!I [&2(m)] then q>(G'(&2(m), q>(r, s), r), G' (&2(m), r, s)) 
U 
!I [mJ A 
!I [&(m)] A 1 !I [&2(m)J thenq>(r,G'(&2(m},r,s)) 
U 
!I [mJ A 
.~ [&(m)] A 
1 
.~ [&(m)J then vvvvvvvvvv 
U 
!I [m J A 1 !I [&(m)] 
then r 
U 1 
!I [ m J 
then s 
fi 
. 
The call G'(o2(m), r, s) in the first and second line can now be replaced by the 
equivalent call G(o2(m), r, s); subsequent unfolding yields 
if 
~ [o2(m)J then G(o3(m}, <p(r, s), r) 
D 1 
~ [o2(m)J then s 
fi 
Under the guard ~ [o2(m)J therefore, 
G'(OZ(m), r, s) = G(o3(m), <p(r, s), r) = G'(o3(m}, <p(r, s), r) 
holds and under the guard 1 
~ [o2(m)J 
G'(OZ(m), r, s) = G(o2(m}, r, s) = s 
The third line, which is guarded by the contradictory condition ~ [o(m)J 
" 
1 
~ [o(m)J, can be altered arbitrarily, for example tor. Hence suitable splitting of the 
guards yields 
if 
.~ [mJ 
A 
.~ [&(m)] then 
if 
!I [Mm>J 
A 
!I [&2(m)J then q>(G'(&2(m}, q>(r, s), r), G'(&3(m}, q>(r, s), r)) 
U 
!I [&(ml] 
A 
1 !I [&2(m)J then q>(r, s) 
U 1 !I [&(m)] 
then r 
fi 
U 
!I [m J A 1 !I (&(m)] then r 
U 1 !I [ m J 
then s 
fi . 
Finally folding with the definition of F leads to 
if 
~ [mJ " 
~ [o(m)J thenF(o(m}, <p(r, s), r) 
D 
~ [mJ " 
1 
~ [o(m)J then r 
D 1 
~ [ m J 
then s 
fi 
By comparison, we obtain: F satisfies the defining equation of G', i.e. formally (in func-
tional notation) 
F(x) = a [F] (x) 

294 
4. Transformation into Repetitive Form 
Since G'(x) = a [G'](x), we have F = G' (about termination see below). 
Because of this equivalence of Fand G', from the definition F(x) = -r [ G'] (x) we now 
getF(x) = -r[F](x). By comparing with H(x) = -r[H](x) we finally obtainF = H, thus 
altogether we have G' = F = H. 
Note: According to fixpoint theory, we have to show that the two fixpoints G' and H 
are indeed total functions; this means that equivalence only is shown if for both the 
schemes G' and H termination is proved in addition (to be more exact, if the respective 
interpretations of the schemes terminate). 
For termination of the schemes under consideration, the "classical" condition 15 
is sufficient, as can be shown by induction. 
The equivalence just proved can be applied to the routine for computing the Fibonacci 
numbers (comp. above): with the interpretations 
of 
A., " 
q> 
~ [mJ 
o(m) 
as 
as 
as 
as 
pnat, nat 
+ 
m > 1 
m- 1 
and the initial values r = 1 and s = 1, we have 
jib(N) = G(N, 1, 1) 
for N > 0 
Compare the resulting interpretation of G with the auxiliary routine fused above for fib. 
4.3.2 Arithmetization of the Flow of Control 
For special nested recursions it is possible to analyze the flow of control (that is the order 
of application of the operations involved) and to find a repetitive form which yields the 
same flow. Certain "arithmetizing" functions which map all the relevant information 
about the flow and the parameter values one-to-one onto a closed interval of natural num-
bers ("arithmetization") play a crucial rl'>le. Conversely the necessary information can be 
recovered from the values of this interval (Partsch, Pepper 1976). 
We consider the following scheme 
funct F "' (nat i, p x) p: 
if i > 0 then F(i - 1, q>(i, F(i - 1, x))) 
else x 
fi 
The controlling task of the parameter i becomes clearer if the nested calls are arranged in a 
detailed form by introducing auxiliary identifiers: 
15 This condition is fulfilled in the subsequent interpretations of li and !!B. 

4.3 Treatment of Non-Linear Recursions 
funct F = (nat i, p x) p: 
if i > 0 then pxi = F(i- i, x) within 
I p x2 "" cp(i, xi) within 
I px3 = F(i- i, x2) within 
x3 
J J 
else x 
295 
fi 
An example of this scheme is the generation of the Gray code for words of a given 
length n, i.e. the construction of a stack of successive codewords. For this we use the 
primitive modes and operations 
mode codeword 
= «codeword of a given length n » 
mode code 
= stack codeword 
funct next 
= (nat i, codex) code: 
I codeword a 
= top(x) within 
I codeword b = «codeword a with an altered i-th bit» within 
append (x, b) 
J J 
With the interpretation of cp by next and of p by code we obtain with 
funct gray =(nat i, codex) code: 
if i > 0 then code xi = gray(i - i, x) within 
I code x2 = next(i, xi) within 
I code x3 = gray(i- i, x2) within 
x3 
J J 
else x 
the desired result from 
gray(n, xo) where codexo = append(empty, «(l, L, ... , L)») 
fi 
Remark: The routine gray is frequently written without a result. In place of the opera-
tion next there is a printing instruction. However, this form requires a "non-local 
variable" for the corresponding last state of the codeword (comp. Chap. 5). A further 
well-known routine which essentially fits into this scheme is the routine which solves the 
"Towers of Hanoi'' problem. 
For the scheme under consideration it is typical that there exists a parameter which, 
working as a counter, controls the recursion, whereas the actual work is done by the other 
parameters. The arithmetizing functions mentioned above serve primarily to determine the 
value of the "control"-parameter and thus the operations to be executed with the other 
parameters (routine yin the following scheme). 
Transformation into repetitive form is done according to the following transformation 
scheme: 

296 
4. Transformation into Repetitive Form 
funct F = (nat i, l x) l: 
if i > 0 then F(i- 1, cp(i, F(i -
1, x))) 
else x 
fi 
funct F = (nat n, l x) l: 
I G(1, x) where 
funct G = (nat c, l y) l: 
if c ~ 2"- 1 then G(c + 1, cp(y(c), y)) 
else y 
fi 
funct y = (nat c) nat: 
t nat i: 3 natp: c = p · 2i-l A 
, 
(2ip) J 
The table of values for y 
c 1 2 3 4 5 6 7 8 9 10 11 1213 1415 16 17 
18 
19 .. . 
y(c) 
2 
3 
2 
4 
2 
3 
2 
5 
1 
2 
1 .. . 
shows intuitively that i = y(c) yields the corresponding value of the parameter i of the 
original recursion. A very simple realization of y could be successive division by 2 until a 
remainder occurs. 
We omit a formal proof of this transformation, since another, more systematic deriva-
tion will emerge in 6.1.3. Here the method can be explained as follows: the information 
about the flow of control and the respective values of the parameters is thought to be 
"stacked" by means of a suitable encoding, where y is the inversion of this encoding. 
Generalizations of the scheme Fare possible, in particular to the form 
... then a(F(i -1, cp(i, F(i - 1, J3(x))))) ... 
or to a form with an arbitrary number of nested calls of F. 
4.3.3 Special Cases of Nested Recursion 
In this paragraph we consider routines with nested recursion of the form 
funct F = (l x) l: 
if & [xJ then ':1 [xJ 
else F(F( ff [x J)) fi 
The routine zer from 1.5.2 and a series of other variants such as McCarthy's "91-function" 
(Manna, McCarthy 1969) belong to this class. 
In contrast to the schemeS of 4.3, scheme F can be transformed into a repetitive form 
without auxiliary conditions. By embedding we obtain 

4.3 Treatment of Non-Linear Recursions 
funct F "' (A. x) A.: 
I Q(x, 1) where 
funct Q "' (A. x, nat i) A.: 
if i = 0 then x 
n i * 0 then if !Y' [xJ then Q(r# [xJ, i- 1) 
else Q(ff[xJ, i + 1) fi fi J 
The proof is carried out inductively where, by unfolding and folding, 
Q(x, i) = Fi(x) 
297 
is shown. Again, the mechanism of the protocol stack is "arithmetized" in this transforma-
tion. 
However, for suitable classes of interpretations transformations can be found which 
lead to routines considerably more efficient than Q. 
Manna, Ness, Vuillemin (1973) show for example, that Fis an idempotent function, if 
r4 [x J is interpreted as an identity. F is then even equivalent to 
funct F1 "' (A. x) A.: 
if !Y' [x J then x 
else F1(ff[x]) fi 
This is a special case of the more general class for which 
!Y' [x J => !Y' [ r4 [x J J. The following is obvious here: 
Theorem l:Ijvx: !Y' [xJ => !Y' [r# [xJJ, thenF(x) = r#n+ 1 [ff"[xJJ, wheren = n(x) is 
determined by 
!Y' [ff"[xJJ 11 vi, 0 ~ i < n: -, !Y' [ffi[xJJ, 
provided such an n (independent of x) exists. 
This means, however: If v x: !Y' [xJ 
=> !Y' [ r4 [x J J, then F is equivalent to the linear 
recursive routine 
funct H 1 "' (A. x) A.: 
if !Y' [xJ then r4 [x J 
else r4 [H1 (ff [x J >J fi 
The following is easily seen: Under the further condition that 
(1) r4 and ffcommute, i.e. r4 [sr[xJJ = ff[r# [xJJ, 
F is equivalent to the repetitive routine 
funct H "' (I.. x) A.: 
I K(x, x) where 
funct K "' (A. x, 1.. z) A.: 
if !Y' [x J then r1 [zJ 
else K(ff[xJ, r1 [sr[xJJ> fi J 

298 
4. Transformation into Repetitive Form 
Another interesting question is, for which class (i.e. under which conditions) Pis equi-
valent to the repetitive routine 
funct G = (1 x) 1: 
if fJ' [xJ then C§ [xJ 
else G( C§ [sr[xJJ> fi 
If in addition to (1) F(x) and G(x) terminate, then 16: 
F(x) yields 
'# [( '# ff)k [xJJ, 
G(x) yields 
'#[('#ff)j[xJJ 
where j and k are not necessarily equal. 
Using this condition and the further requirement 17 
(2) vx: fJ' [xJ = fJ' ['# ff[xJJ 
one can show 
Theorem 2: Under the conditions (1) and (2) F(x) = G(x) holds for those x which have 
the property fJ' [ x J v fJ' [ .r C x J J. 
Hence F and G are equivalent in particular when 
vx:..., rJ>[xJ = rJ>[ff[xJJ 
Proof of theorem 2 (computational induction): 
't [F] (x) = if 
fJ' [xJ then '# [xJ 
D ..., fJ' [xJ then F(F(ff[x])) fi 
= if 
fJ' [xJ then '# [xJ 
D..., rJ>[xJ thenF(if 
rJ>[ff[xJJ then C§ff[xJ 
D ..., fJ' [ ff [xJ J then F(F( ff ff [x J)) fi) fl 
(using the property fJ' [xJ v fJ' [sr[xJJ this can be simplified to:) 
= if 
fJ' [xJ then '# [xJ 
D ..., fJ' [xJ then F( '# ff [x]) fi 
(in the case of..., fJ' [xJ as assumed fJ' [ff [xJJ holds. Then because of (2) ..., fJ' [xJ = 
fJ' [ '# ff [ ff[xJ J J also holds. This is equivalent to fJ' [ ff [ C§ ff [xJ J J because of (1). Hence 
the value y = '# ff [xJ satisfies the condition fJ' [y J v 
fJ' [ ff [y J J, and the induction 
hypothesis can now be used:) 
16 t§ ff denotes the composition of ff and t§ • 
17 The examples at the end of this paragraph show that (2) is closely related to the termination of F. 

4.3 Treatment of Non-Linear Recursions 
If 
iJ' (x] then ':§ (x J 
D..., iJ'(xJ thenG(':§ff(x])fi 
cr[G](x) 
299 
This theorem establishes the equivalence ofF and G only for a subset of all possible 
arguments x. However, using this as a base the following general theorem can be shown: 
Theorem 3: Under the above conditions (1) and (2) 
vx:F(x) = G"+ 1(ff"(x]) 
ho/dsjoracertain n = n(x) 
Complete equivalence ofF and G is established by the additional condition 
(3) v x: iJ' (x] v ..., iJ' ( ':§ (x]J, i.e. iJ' ( ':§ (x]J => iJ' (x] 
Theorem 4: Under the conditions (1}, (2) and (3) 
v x: F(x) = G(x) 
holds. 
(The proof of this theorem requires a complicated analysis of the recursion.) 
For the important special case that iJ' (x] is of the form x ~ x0 , ':§ ff (x] > xis a suf-
ficient condition for the termination of F. (2) follows immediately from this. Moreover we 
assume (1). The cases ':§ (x J ~ x and ':§ (xJ ~ x lead then to F = H 1 = Hand F = G, resp. 
Incidentally, it can immediately be seen in the second case that in particular for 
':§ ff (nJ = n + 1 (with ).. interpreted as int) the routine F is equivalent to 
funct G1 = (int n) int: 
if n ~no then ':§ (nJ else ':§(no] fi 
(The examples of functions mentioned above belong to this class.) 
Exercise 1: Compute G1 asfixpoint ofF, i.e. as lublf;} according to 1.5. 
4.3.4 The Technique of Range-of-Values Tabulation 
A special form of recursion which frequently appears is the range-of-values recursion 18• It 
is characterized in that for a routine F of this kind, there is a well-ordering of the 
parameter mode ).. such that at the point of a call F(x) of the routine every (recursive) call 
uses only parameter values which precede x in this ordering. 
If the arguments on termination -
that is those parameter values for which the rou-
tines terminates - are known from the beginning, then the range-of-values can be succes-
sively tabulated, proceeding from the smallest argument on termination a ("memo-
functions", Michie 1968, "dynamic programming", Aho eta!. 1974). A sequence with the 
parameter mode ).. as a hidden index set can be used as a table (comp. tabrec in 2.10.1}, 
alternatively a flexible array of objects of the result mode p, e.g. objects of mode ind flex p 
(introduced in 3.3.1) with mode ind = {nat x: x ~ a}, if).. is the mode nat. 
18 Functions defined by the range-of-values recursion are primitive recursive (comp. Hermes 1978, 
p. 82). 

300 
4. Transformation into Repetitive Form 
In the sequel we will restrict ourselves to the case A. = nat and use flexible arrays for 
the tabulation: A general tabulating scheme for arbitrary functions h with the parameter 
mode nat and the result mode p is given by 
funct tab = (nat n, funct (nat) ph: n ~ a) ind flex p: 
if n = a then ext(init, h(a)) 
else ext(tab(n- 1, h), h(n)) fi 
all function values from h(a) to h(n) inclusively are listed. 
Obviously 
sel(tab(n, f), k) yields exactly f(k), if k ~ n 
This finally allows embedding. 
The technique of function inversion can be applied to tab and the following results: 
funct tab = (nat n, funct (nat) ph: n ~ a) ind flex p: 
I t(a, ext(init, h(a))) where 
funct t = (nat y, ind flex p z: y ~ a) ind flex p: 
if y =1= n then t(y + 1, ext(z, h(y + 1))) 
else z 
fi J 
By shifting the ext-operation this can be transformed into 
funct tab = (nat n, funct (nat) p h: n ~ a) ind flex p: 
I t(a, init) where 
funct t = (nat y, ind flex p z: y ~ a) ind flex p: 
if y =1= n + 1 then t(y + 1, ext(z, h(y))) 
else z 
fi J 
In this form the parameter z always represents exactly tab(y- 1, h). This means, how-
ever, that by unfolding h (more precisely, of that function which is inserted for h as an 
actual parameter) 
e.xt(z, h(y)) changes into ext(z, « ... h(ki[y]) ... ») 
with one or more calls of the form h (ki [y J ), where a ~ ki [y J ~ y - 1 holds. According to 
the above h (ki [y ]) can in turn be replaced by sel(tab(y - 1, h), ki [y J) and thus by sel(z, 
ki [y J ). The routine t has thus become completely independent of the routine h, as all 
recursive calls h (ki [y J) have been replaced by set (z, ki [y J) and the terminating branches of 
h cannot, by definition, contain any h. 
By embedding we obtain the (informal) transformation scheme 

4.3 Treatment of Non-Linear Recursions 
301 
funct F =(nat n: n ~a) p: « ... F(.fti [n]) ... » 
-----+-----)vi: .fti[n] < n 
(«a is the smallest argument on termination» 
funct F = (nat n: n ~ a) p: 
I sel(t(a, init), n) where 
funct t = (nat y, lnd flex p z: y ~ a) lnd flex p: 
if y =1= n + 1 then t(y + 1, ext(z, « ... sel(z, .fti (y]) ... »)) 
else z 
fi J 
The notation « ... X ... » indicates that the body of F is to be inserted here under corre-
sponding substitution of the recursive calls. 
Example: 
functfuse = (pnat n) pnat: 
if n = 1 
then 1 
D n > 1 A even n then fuse( f) 
D n > 1 A odd n thenfuse("2 1) + fuse("~ 1 ) fi 
The range-of-values condition is satisfied, the argument on termination a is 1, thus ind is 
pnat. 
Application of the scheme yields 
funct fuse = (pnat n) pnat: 
I sel(t(1, init), n) where 
funct t = (pnat y, pnat flex pnat z) pnat flex pnat: 
if y =1= n + 1 then t(y + 1, ext(z, *)) 
else z 
fi J 
where for * the following has to be inserted: 
if y = 1 
then 1 
D y > 1 A even y then sel(z, -fJ 
D y > 1 A 
odd y then se/(z, Y;1) + sel(z, Y;1) fi 
If in addition the terminating case is extracted we obtain 
functfusc = (pnat n) pnat: 
!if n = 1 then 1 
else pnat flex pnat tab = t(2, ext(init, 1)) within 
set (tab, n) 
fi where 
funct t = (pnaty, pnat flex pnat z: y ~ 2) pnat flex pnat: 
If y =1= n + 1 
then t(y + 1, ext(z, if even y then sel(z, -f) 
D oddy then sel(z, Y;~) + se/(z, Y; 1) fi)) 
else z 
fi J 

302 
4. Transformation into Repetitive Form 
The technique applies as well for systems if a succession of calls is found for which the 
range-of-values condition is satisfied. 
Exercise I: Apply the technique described to the routine jib of 1.4.3. 
Why can a sequence be used here instead of a flexible array? 
Exercise 2: The volume /(n) of then-dimensional unit sphere is determined by the recursion 
/(n) = I(n -
1) x S(n) 
S(n) = n~l X S(n -
2) 
(n ~ 1), 
(n ~ 2), 
/(0) = 1 
S(O) = 11, 
8(1) = 2. 
Use the technique of range-of-values tabulation for the computation of l(n). Investigate 
the saving of calls in comparison to the usual recursive computation 19• 
4.4 Disentanglement of the Control 
In 4.2.3 and 4.2.5 function inversion was used to describe repetitively the execution of 
linear recursive routines. Function inversion is applicable not only in this case but also for 
more general types of recursion, in order to obtain a "disentangled form" that leads to a 
degeneration of the value stack of the stack machine. In certain cases an initial form suit-
able for this can be achieved by transformation of the type of control, using other meth-
ods. 
The following considerations will again illustrate the method used in 4.2.3 and cast 
light on the detailization used in 4.3.2. They are, however, of more general importance and 
will be taken up again in Chap. 6. 
4.4.1 Disentangled Routines 
The stack machine (1. 7 .4) generally requires a protocol stack apart from the value stack. In 
the case of repetitive routines both are superfluous. In the case of linear recursive routines 
at least the protocol stack becomes very simple, as the same returning point is always re-
corded. It is for this more profound reason that in 4.2.5 - after the introduction of a stack 
for parameter values -
we could already change over to repetitive routines: The "way 
back" with the help of the second routine R mirrors precisely the identical returning points 
in the protocol stack. For general recursive routines the role of the protocol stack is no 
longer trivial, such a simple transition to repetitive routines is no longer possible ( comp. 
4.3). However, it will be shown that in more general cases function inversion can serve to 
establish a form of the routine such that the task of the parameter stack becomes trivial. 
First of all, as was done in 4.3 .2, a routine is detailed by the (possibly hierarchical) 
introduction of auxiliary object declarations until no parameter positions remain which 
contain an expression, and thus in particular recursive calls are "isolated". The routine 
morris of 1.5.1 then reads 
19 A slight mathematical transformation would however be of advantage, e.g. 
S(2i -
1) x S(2i) = -+ 11 and hence /(2i) = 1f 11; holds. 

4.4 Disentanglement of the Control 
funct morris "' (int x, int y) int: 
if x = y then succ y 
else f (int X1, int y1) "' (pred X, SUCC y) Within 
f int y2 "' morris(x1, y1) 
within 
morris(x, Yz) 
J J fl 
or, using the parentheses-saving notation already introduced in 1.13.3, 
funct morris "' (int x, int y) int: 
if x = y then succ y 
else (int x1, lnt y1) = (pred x, succ y); 
int y 2 = morris(x1, y 1); 
morris(x, y 2) 
fi 
303 
The resulting sequence is such that an object identifier is always declared before it is 
applied. 
Such a detailed form of a recursive routine is said to be disentangled if none of the 
parameters (and none of the auxiliary identifiers) is used both before and after the very 
same recursive call 20• This means, however, that during the execution on a stack machine 
these parameters (and auxiliary identifiers) are never called for from the depth of the stack; 
the parameter stack degenerates into a parameter register (as in 1. 7 .4. 5). 
The routine morris above is not disentangled; the parameter x violates the condition. 
Linear recursive routines also are in general not disentangled, as is shown by the detailed 
form of the scheme L (4.1): 
funct L "' (Ax) p: 
If ~ [xJ then h
1 "' .Jt' [x}; 
p z "'L(x1); 
q>(z, rff [x}) 
else .1t [x J 
fi 
On the other hand, the special linear recursive routine 
functpbw "'(int a, nat e) int: 
if e = 0 then a 
else sq(pbw(a, e -
1)) fi 
is in detailed form already disentangled. Repetitive routines are always disentangled. 
The scheme Fin 4.3.3 ("91-function") is disentangled. In detailed form it reads 
funct F "' (Ax) A: 
if iY [xJ then @ [xJ 
else AX1 "' sr[x}; 
AZ1 "'F(x1); 
A Z2 "'F(z1); 
fi 
20 "Before" and "after" are understood in the sense of the "natural control flow" (1.4.3), that is in 
the ordering given by the Kantorovic tree. 

304 
4. Transformation into Repetitive Form 
An analytical treatment of the control flow was possible in this case ("arithmetization of 
the protocol stack"). 
There is a more general result: The control flow of disentangled routines can be sub-
jected to (individual) analysis. This will be discussed in 6.1.3. 
4.4.2 Disentangling Recursive Routines by Means of Function Inversion 
The aim of the following consideration is to produce the disentangled form of a recursive 
routine. Function inversion, as it turns out, serves exactly this purpose. 
4.4.2.1 In the above scheme L the identifiers x1 and z satisfy the condition of disentangle-
ment. The parameter x, however, does not. The decisive step towards disentanglement is 
the delivery of the actual parameter value as an additional result. The scheme L then 
changes into 
funct L = (l x) p: 
I b where 
(la, pb) =L*(x), 
funct L * = (l x)(l, p): 
if !!i [x J then l x1 = .XC [x J ; 
(l y, p Z) "' L*(x1); 
(x, cp(z, rff [x J)) 
else (x, .Jf' [x J) 
fi J 
The additional result has at first no functional use whatever. Nevertheless, the relation 
Y = .x--[xj 
holds, and conversely - if .XC possesses the inverse :i 
x=:i[vJ 
If .XC does not possess such an inverse, we may use a stack again. In all the following 
schemes we then have to replace 
lx 
.x--[xj 
:i [xJ 
by 
by 
by 
(l x, stack l sx) 
.XC*[x, sxJ = (f[xj, sx&x) 
:i* [x, sx J = (top sx, rest sx) 
If, according to the relation x = :i [v J, xis replaced, a new form of L *evolves: 
functL* = (lx)(l, p): 
if !!d [x J then l x1 = .XC [x J ; 
(ly, pz) = L*(x1); 
(:i[vJ, cp(z, rff [:i[vJ])) 
else (x, .Jf' [x J) 
fi 

4.4 Disentanglement of the Control 
305 
This form is now disentangled. Moreover, a comparison with the repetitive form in sec-
tion 4.2.3 shows immediately that the part in front of the recursive call of L *has moved to 
the "precomputing" routine P and the part after the call of L *has moved to the routine R. 
Thus, the method there is a special case of disentangling by means of function inversion. 
4.4.2.2 The disentangling method, however, is not restricted to linear recursive routines. 
We now consider an example of cascade-type recursion 21 : 
funct F = (A. x) p: 
if ~ [xJ then q>(F(.xj [x]), F(,XZ [x]), c [x]) 
else .1t [x J 
fi 
The detailed form reads 
funct F = (A. x) p: 
if~ [xJ then A.x1 = .xt [xJ; pz1 = F(x1); 
A.x2 = .x; [xJ; p z2 = F(x2); 
q>(Z1, z2, c [x]) 
else .1t [x J 
fi 
The parameter x and the auxiliary identifier z1 violate the condition of disentanglement. 
It is apparent here that care should be taken when introducing a detailed form. The form 
funct F = V· x) p: 
if :!1 Lx J then A x1 = f 1 (x J ; lx2 = f 2 (x J ; 
p z1 ""F(x1); p z2 = F(x2); 
Ql(~~z2 , C (x]) 
else .1t LXJ 
fi 
which is also permissible is not as advantageous as the above form ofF because here x2 violates dis-
entanglement as well as x and z1 • This means - if no inverse functions exist - that a stack of mode 
stack A double the size is needed. For this reason the detailed form should be chosen such that as 
many auxiliary identifiers as possible already satisfy the condition of disentanglement. 
As in the method applied to scheme L an additional result of mode A. is now introduced 
in F and one obtains the following embedding: 
funct F = (A. x) p: 
r b where 
(A. a, p b) = F*(x), 
funct F* = (A. x)(J., p): 
if~ (xJ then J.x1 = .xt(xJ; (J.y1, pz1) = F*(x1); 
J.x2 = f 2 (xJ; (A.y2 , pzJ = F*(xJ; 
(x, q>(Zt, Z2, C (x J)) 
else (x, .1t (x J) 
fi J 
21 We can assume that f 1 and .x-2 are not equal, as otherwise we would have the linear recursive 
routine L above. 

306 
4. Transformation into Repetitive Form 
With the inverse functions .ij and 1 2 we have the equivalences 
y1 = 1 1 [xJ 
and x = .ij (Y1] 
Y2 = 12 [x] 
and x = 12 lY2J 
Thus F acquires - apart from z1 -
the desired form 
funct F* = (J..x)(J.., p): 
if £6 [xJ then J..x1 = .t; [xJ; 
(J..y~> pz1) = F*(x1); 
h2 = 12 [ i'i (Y1JJ; (J..y2, P Zz) = F*(x2); 
(12 lY2J, q>(ZI, Z2, C [ 12 lY2JJ)) 
else (x, £' [x J) 
fi 
As there is no possibility of working with an inverse function with respect to z1 the only 
viable way is to introduce a stack on parameter and on result positions (in the same way as 
in L 1 in 4.2.5): 
funct F = (A. x) p: 
I b where 
(I.. a, stack psb, pb) = F*(x, empty), 
functF* = (J..s, stack psz)(J.., stack p, p): 
if £6 [x] then (J..x1 , stack psr1) = (11 [xJ, sz); 
(J..y1, stack psz~> pz1) = F*(x1, sr1); 
(J..x2, stack psr2) = (1z[xJ, sz&z1); 
(I..JI, stack p sz2, p z2) = F*(x2, sr2); 
(x, sz, q>(z~> z2 , C [x J)) 
else (x, sz, £' [x J) 
fi J 
In addition to the relations between x and y1 or x and Yz given above we now have also 
sz1 = sr1 = sz 
sz2 = sr2 = sz & z1 and sz = rest sz2 , 
z1 = top sz2 
We take full advantage of these equivalences to produce a disentangled form; e.g. in 
the result of the then-branch, sz can be replaced by the equivalent expression rest sz2 , 
which satisfies the condition of disentanglement. Altogether we obtain: 
funct F* = (A. x, stack p sz)(J.., stack p, p): 
if £6 [xJ then (l..xp stack psr1) 
= (.t; [xJ, sz); 
(l..y~> stack p sz~> p z1) = F*(x1, sr1); 
(J..x2, stack psr2) 
= (.Jf2 [11 (Yt]J, sz1 &z1); 
(A.y2 , stack psz2 , p z2) = F*(x2, sr2); 
(12 (Y2], rest sz2 , q>(top sz2, z2 , rf [ 1z lY2J J )) 
else (x, sz, £' (x J) 
fi 
The stack sz is frequently termed an "intermediate result stack". It can be seen, however, that it 
does not differ from a stack for parameters and local auxiliary identifiers. Its only peculiarity is that it 
cannot be avoided - even at the cost of multiple computations. 

4.4 Disentanglement of the Control 
307 
If the routinefhas more than two adjacent calls, we can either introduce for each call 
(excluding the last one) a stack of its own of mode stack p or we can enter all intermediate 
results Z; successively in a stack sz. In the function cp all Z; are replaced by the expressions 
top sz, top rest sz, top rest rest sz, etc. 
We have stated that in general a stack can be used when the inverse functions i'j and 
.i;_ do not exist. An interesting variant arises if only one of the two exists, e.g. i'j . We can 
then define 
.i'j* [x, sxJ = (1'1 (xJ, sx), 
1'2*[x,sxJ = (1'2 (xJ,sx&x) 
and obtain as inverse functions 
.ij•[x,sxJ = (i'j[xJ,sx), 
f 2* [x, sxJ = (top sx, rest sx) 
This means that as soon as no inverse exists for at least one of the functions 1;, a stack 
must be introduced as an additional parameter. This stack, however, remains constant for 
all calls for which an inverse function exists 22• 
4.4.2.3 Let us now consider nested recursions. An example is the scheme 
funct G = (I.. x) p: 
if !J (xJ then cp(G(w(G(.i"1 [xJ), 1'2 [xJ)), C [xJ) 
else .If [x J 
fi 
In a detailed form it reads 
funct G = (I.. x) p: 
if !J [xJ then J..x1 = .i1'[xJ; 
pz1 = G(x1); 
J..x2 '"' IJI(Z~> 1'2 [x]); p z2 = G(x2); 
cp(z2, C [x J) 
else .If (x J 
fi 
It can be seen immediately that this type of recursion is easier to handle than a cascade-
type recursion, as the intermediate results z1 and z2 already satisfy the condition of dis-
entanglement, but there is in general no possibility of directly constructing the value of the 
parameter x from the value x2 by means of an inverse function. Therefore, normally a 
stack must be introduced. The routine G thus becomes 
funct G = (I.. x) p: 
1 b where 
(1.. a, stack 1.. sa, p b) = G*(x, empty), 
funct G* = (J..x, stack J..sx)(J.., stack A., p): 
if !J [xJ then (J..x1, stack J..sx1) 
= (.i'j [xJ, sx); 
(J..y~> stack J..syp pz1) = G*(x1 , sx1); 
22 This drastically reduces the required storage space. For this reason it is advantageous to give the 
programmer access to the stacking mechanism instead of hiding it within a complex compiler. 

308 
4. Transformation into Repetitive Form 
(lx2, stack l..sxJ 
e (IJI(Z1, JG[ii. lYtJ]), 
SYt & i;_ [ytJ ); 
(I..Y!, stack l..sh, pz2) e G*(x2, sx2); 
(top sh, rest SY!, cp(z2, c [top sh])) 
else (x, sx, .1f Cx J) 
fl J 
4.4.3 Reshaping the Type of Control Flow 
Sometimes it is possible to change the control flow into another, more efficiently manage-
able type. The scheme 
funct F e (I.. x) p: 
if ~ [x J then F( .t'j [x J) a F( 1 2 [x J) a C [x J 
else .tt [x J 
fi 
(with an associative operation cr) may serve as an example. 
In order to succeed with the method of functional embedding - as in the examples fuse 
and jib in 4.3.1 - we require not only the associativity of a but also a close relation be-
tween .t'j and 1 2 (e.g . .t'j [x J = 1 2 [ 1 2 [x J J ). In the case of the range-of-values tabulation 
in 4.3.4, .t'j and 1 2 also have to satisfy drastic conditions. 
For an important class of examples no such conditions are fulfilled for .t'j and 1 2 • The 
above scheme is typical for "processing of trees", where e.g. case x. lisp x etc. stand for 
the mode 1.. and the functions .t'j, 1 2 mean "left subtree" or "right subtree". In such a case 
we normally have to introduce stacks. As shown in 4.4.2.2 two stacks are required for such 
cascade-type recursions, one for the parameters and one for the intermediate results. 
However, if the operation a is associative (as already suggested by the missing brackets 
in the above scheme) the stack of intermediate results can be avoided. In order to make the 
notation somewhat simpler we also assume a neutral element e for cr. 
With these assumptions the technique of re-bracketing can be applied (comp. 4.2.1). 
We first obtain 
funct Fe (l..x) p: 
I G(x, e) where 
funct G e (l..x, p z) p: 
if~ [xJ then G(Xj [xJ, (F(12 [xJ) a Iff [xJ) a z) 
else .tf [xJ a z 
fi J 
According to the equivalence F(x) = G(x, e) the inner call ofF can be replaced (as the 
termination of G is decided by the first parameter x only, no complications arise); because 
of associativity of a the following results: 
funct Fe (l..x) p: 
I G(x, e) where 
funct G e (l..x, p z) p: 
if ~ [xJ then G(11 [xJ, G(JG [xJ, e) a (Iff [xJ a z)) 
else .tf [xJ a z 
fi J 

4.4 Disentanglement of the Control 
309 
An important property of G (which, by the way, is necessary for the proof of the trans-
formation by re-bracketing 23 ) is, that for arbitrary l a, p b, p c 
G(a, b) a c = G(a, b a c) 
holds. Together with e a b = b this yields 
funct F = (l x) p: 
I G(x, e) where 
funct G = (l x, p z) p: 
if £6 [xJ then G(.%1 [xJ, G(.%2 [xJ, C [xJ a z)) 
else £' [xJ a z 
fi J 
As already shown in 4.4.2.3, such a nested recursion is more suitable with respect to the 
stack of intermediate results; this can be seen here, too, through the detailed form 
funct G = (l x, p z) p: 
if £6 [xJ then (l x1 , p z1) = (.%2 [xJ, C [x] a z); 
p r1 
= G(xp z1); 
(lx2, P Zz) = (.tl [xJ, rt); 
p r2 
= G(x2, z2); 
Tz 
else £' [xJ a z 
fi 
Only the parameter x violates the condition of disentanglement. As nothing should be as-
sumed for .tJ and .%2 the inverse function must be managed with the help of a stack: 
funct F = (l x) p: 
I b where 
(stack lsa, pb) = G(x, empty, e), 
funct G = (l x, stack l sx, p z) (stack l, p): 
if £6 [x] then (l x1, stack l sx1, p z1) = (.%2 [x], sx & x, C [x] a z); 
(stack l sy1, p r1) 
= G(x1, SX1, Z1); 
(l x2, stack l sx2, p z2) = (.tJ [top sy1], rest sy1, r1); 
(stack l SYz, p r2) 
= G(x2 , sx2 , z2); 
(SYz, Tz) 
else £' [x] a z 
fi J . 
Example: From the routine traversetree of 2.13 which shows cascade-type recursion, in 
view of associativity of concatenation the following "simpler" routine with nested recur-
sion is obtained 
funct traversetree =(case XA) lsequ x: 
I G(A, 0) where 
funct G =(case xA, lsequ xz) lsequ x: 
if A = 0 then z 
else G(lejtof A, G(rightof A, append(z, node of A))) fi J 
23 Compare also the property Q lfac] proved in 1.6. 


r ulij:. tt-
u.1(,) n 
' m t te 
"t n~.. 
Chapter 5. Program Variables 
"Variables serve as carriers of values." 
Rutishauser 1967 
"The basic elements ... are objects and variables. 
Objects are the data entities that are created and 
manipulated by ... programs. Variables are just the 
names used in a program to refer to objects." 
Liskov et a!. 1977 
5.1 The Origin of Program Variables 
The first four chapters of this book were able to dispense with program variables. There 
are at least three different ways - conceptually independent of each other - of introduc-
ing program variables. In 5.1.1 we characterize (composite) program variables as rudi-
mentary value stacks in case the stack machine processes only repetitive programs. The 
range-of-values machine is introduced in 5.1.2. This is a machine which is restricted to 
computing primitive recursive functions and hence is not universal. In a certain special case 
(of then-term recursion) the stack reduces to a shift variable, in the case n = 2 to the usual 
program variable. Program variables can be also understood as conceptual extensions of 
result parameters (1.14.2) with sequentialized, "varying" attributions. We will commence 
our consideration in 5.2 with a program variable concept which is based on the idea of 
economizing the number of object identifiers. 

312 
5. Program Variables 
The concept "program variable" entails both terminological and semantic problems. Even the 
variables of analysis had to undergo historical changes. If one spoke of an 'independent variable' or 
or a 'dependent variable' in the 19th century, the principal idea was that in changing the first variable 
the other 'dependent' variable also changed - a mechanistic interpretation. This concept of a func-
tion which was tailored to Newton's "fluxions" became quite obsolete as Bolzano and later Weier-
strass gave continuous functions which were nowhere differentiable. A new understanding of the 
nature of a function based on the concept of mapping came into use. The word 'variable' remained 
all the same; if for a mapping f: M .... Nthe elementwise relation x >-> y = f(x) is considered, then 
we quite freely call x andy "variables". As a (determinate) routine causes a mapping, we could, in 
this sense, have termed a variable what was called a parameter in Chap. 1. We have refrained from 
using this term in order to avoid terminological confusion. 
In algebra, too, e.g. in connection with an algebraic equation, the expression "variable" was at 
first used carelessly. There was also a revolution at the turn of the century when one began to dif-
ferentiate and no longer spoke of variables but of "indeterminates" (Perron), "generators" or 
"transcendental elements" (Steinitz) and called e.g. GF4 the "extension of GF2 under the adjunction 
of an indeterminate". In this way formal derivation -
closer to Leibniz than to Newton -
is a 
calculus, which remains defined e.g. in finite fields in which every topology required for a conception 
of "variability" is trivial. 
Up to the present day the somewhat misleading term "variable" has remained in general use for a 
literal, which stands for an element not (yet) specified in detail - a "variability" in a new sense. An 
"indeterminate" which in algebra is nothing but a "generating element" can always be interpreted as 
a variable by virtue of the principle of substitution (comp. e.g. van der Waerden 1937, p. 50- 51). In 
algebra the older -
in the sense of analysis -
more primitive conception of a function as a 
"computing expression" is more profound and the algebraist prefers to speak of "indeterminates" in 
order "to exclude a relapse into the point of view of a variability" (Hasse 1951). The computer 
scientist might consider this to be an exegetic quarrel among mathematicians and remain unaffected 
by it. In actual fact informatics could use the word and the term "variable" exactly in the sense given 
above (of "literal for an element not (yet) specified in detail") as it tends more towards algebra than 
to analysis. Theoretical informatics which is close to mathematical logic does so (comp. e.g. Manna 
1974, p. 79: "To distinguish between the two types of symbols, the quantified symbols are called 
variables, while the nonquantified symbols are called constants"). 
In accordance with this we introduced the concept of "constants" in 1.1. We were cautious with 
terminology, however, and did not speak of variables but of parameters. 
That which in programming has commonly been called "variable" since von Neumann 
1947 we will call "program variable" (also "store variable", "von Neumann variable"). In 
a fuzzy way we can find the term in the ALGOL 58 report in sentences like "Constituents 
of ... expressions ... are numbers, variables, elementary arithmetic operators ... ". A 
similar situation prevails in the ALGOL 60 report whereby, thanks to the absence of 
constant-declarations in these programming languages, it cannot be decided when a 
program variable or when a variable in the mathematical sense is meant. In particular, 
because input parameters and result parameters are not distinguished -
one of the 
detrimental influences of FORTRAN -, in compensation the peculiar parameter passing 
mechanisms ('call by value', 'call by name', 'call by reference') became necessary. Seeg-
mtiller's doctoral thesis 1966 was the first to bring about a clarification, and from then on 
the distinction was made in most of the recently developed programming languages be-
tween 'variables', i.e. program variables, and 'constants', i.e. mere identifiers for objects. 
In this respect PASCAL and ALGOL 68 are on the same footing, even if ALGOL 68 
provides variables only in the form of an implementation by references. 

5.1 The Origin of Program Variables 
313 
5.1.1 Specialization of the Stack Machine 
In 1. 7.4 it was shown by means of the stack machine that in certain cases recursion which 
otherwise requires a value stack and a protocol stack can be carried out in a technically 
simpler way. For repetitive routines the value stack shrinks to a parameter register in which 
not all arguments or argument lists have to be stored but only the last one. This is a register 
which is initialized during the first call (initiated by exec) and is then overwritten in the 
other calls (initiated by goto). 
That which corresponds to the register on the programming side is called a list of pro-
gram variables to which the objects of the respective argument list are attributed. Program 
variables are usually typed, i.e., they are distinguished with respect to the mode of the 
objects they contain. Above all, this enhances clarity and facilitates checking. Moreover in 
programming which is very much machine-oriented this corresponds to the fact that 
objects of different modes almost always have quite different "storage space requirement" 
(transition to a binary object structure may be concealed behind this phrase -
comp. 
3.6.4). As a mode indication for program variables we use var followed by the mode 
indication of the (possible) values 1 ("current values") of the variables, that is e.g. var nat, 
var boo!, also var "and var stack Jl· Freely chosen identifiers are used to name program 
variables. 
In the case of (direct-recursive) repetitive routines program variables may serve to 
clarify the simplified mode of operation of the stack machine, as well as the notation. To 
show this we take the simplest case, the repetitive schemeR (4.1): 
funct R "' (A. m) p: 
if~ (mJ thenR(.f(m]) 
else £(m] 
fl 
The introduction of a program variable and its initialization take place simultaneously 
at the beginning, written e.g. as 
var A. v := m 
("initialized declaration of a program variable"). 
A new value is attributed ("assigned") to the variable only when the recursion is to be 
continued, this is written e.g. as 
v := .f(vJ 
and must be repeated as long as the condition ~ (vJ is satisfied. This can be written 
while ~ (v] do v: = .f (vJ od 
Finally £ ( v J yields the result. Altogether we obtain the iterative version without 
explicitly visible recursion 
Content is not used as a synonym for value. We reserve "content" for variables which (comp. 
7.4.1.1) have become containers at the level of explicit addresses. 

314 
5. Program Variables 
funct R "' (l. m) p: 
I var l. v : = m; 
while!!# [v] do v: = ~ [vJ od; 
£' [vJ 
J 
which by definition is equivalent to the above version. It is composed of three parts, 
initialized declaration, repetition and result computation which are separated by 
semicolons, their order being emphasized. This version describes explicitly the mode of 
operation of the Babbage-Zuse machine 2 which was introduced in 1. 7.4 as a specialized 
stack machine. The separation into three parts is just as typical as the circumstance that no 
program variable is introduced which is not initialized. 
The assignment v: = ~ [vJ (and also the initialization) must be understood in general 
as a collective assignment. In the schemeR, l. m stands in general for a collection of para-
meters. A program variable is then needed for each single parameter. In a suitable notation 
the program variables are collected into a list and the objects assigned to them are combin-
ed to form tuples, e.g. 
(var nat n, var nat m) : = (N, 1) 
and 
(n, m): = (n -
1, m x n) 
in the example which we will now discuss. 
Generalizing slightly, we can consider routines which are embedded in repetitive rou-
tines. Such situations occur e.g. by applying the technique of re-bracketing and in the 
special cases of the other two Cooper transformations in Chap. 4. 
The repetitive embedded routine S 
funct S "' (l. m) p: 
I R( w [m ]) where 
funct R "' (11 a) p: 
if!!# [aJ thenR(~[a]) 
else £'[a J 
fi J 
yields by definition the version without explicit recursion 
funct S "' (l. m) p: 
I var 11 v : = w [mJ ; 
while !!# [vJ do v: = ~ [vJ od; 
ff [v J 
J 
which differs from the above form only in the occurrence of the initializing expression W. 
Exercise 1: Formulate the transformations in 4.2 so that they lead to versions without explicit recur-
sion. 
2 Babbage (1837) quite appropriately called his machine a "mill". 

5.1 The Origin of Program Variables 
315 
Thus for the routine fac of 4.2.1 we have 
functjac = (nat N) nat: 
I (var nat n, var nat m) : = (N, 1); 
while n * 0 do (n, m) : = (n - 1, m x n) od; 
m 
J 
The recursion of a hierarchically structured system of repetitive recursive routines can 
be "removed" in a structured way. 
Example 1.4.1 (b), slightly rewritten as 
funct gcd = (nat a, nat b) nat: 
if b =1= 0 then gcd(b, mod(a, b)) 
else a 
fi, 
funct mod = (nat a, nat b) nat: 
if a ~ b then mod(a -
b, b) 
else a 
fi 
becomes 
funct gcd = (nat A, nat B) nat: 
I (var nat a, var nat b) : = (A, B); 
while b =1= 0 do (a, b) : = (b, mod(a, b)) od; 
a 
J, 
funct mod = (nat A, nat B) nat: 
I (var nat a, var nat b) : = (A, B); 
while a~ b do (a, b):= (a- b, b) od; 
a 
J 
In this example we recognize a frequently occurring opportunity for simplification. 
Fixed parameters - which remain unchanged in the recursive call - lead to dispensable 
program variables because they are never changed. If b is dispensed with in the above 
example we obtain 
funct mod = (nat A, nat B) nat: 
I var nat a : = A; 
while a ~ B do a : = a - B od; 
a 
J 
(The same effect is achieved by first suppressing fixed parameters.) 
Remark: In order to avoid copying mistakes as far as possible while "removing" recur-
sion, we consider it practical to use the original parameter identifiers (lower case) as 
variable identifiers and to use upper case for the new (bound) parameter identifiers. 
mod now can be inserted in gcd. From hierarchically structured systems we get nested 
repetitions, in our example: 

316 
5. Program Variables 
funct gcd "' (nat A, nat B) nat: 
I (var nat a, var nat b ) : = (A, B); 
while b * 0 do (a, b) : = (b, I var nat u : = a; 
whileu ~ bdou:= u- bod; 
u 
J) od; 
a 
J. 
(We have avoided a clash of identifiers here by introducing the letter u instead of a in the 
body of mod) 3• 
As a final example let us presume that a table for a function F: 11 ---> vis to be comput-
ed. 
We specify: A table for Fwith arguments between a and b is a pair (a, F(a)) followed 
by a table for Fwith arguments between next(a) and b, where next is a strictly monotonic 
but otherwise arbitrary function. (Usually next is taken to be linear, next(a) = a + 1), 
where I) is called an argument increment.) The description just given is linear recursive and 
reads when formalized (comp. 2.10.1) 
funct tabulate "' (funct (11) v F, 11 a, 11 b, funct (11) 11 next) sequ (J.L, v): 
I tab(a) where 
funct tab "' (11x) sequ (J.L, v): 
if x > b then 0 
else append(tab(next(x)), (x, F(x) )) fi J 
the repetitive form is (4.2.1) 
funct tabulate "' (funct (11) v F, 11 a, 11 b, funct (11) 11 next) sequ (Jl, v): 
I tab(a, 0) where 
funct tab "' (11x, sequ (J.L, v) s) sequ (J.L, v): 
if x > b then s 
else tab(next(x), stock(s, (x, F(x)))) fi J 
According to the above this can be rewritten as 
funct tabulate "' (funct (11) v F, 11A, 11 B, funct (11) 11 next) sequ (J.L, v): 
I (var 11x, var sequ (J.L, v) s): = (A, 0); 
while x ~ B do (x, s) : = (next(x), stock(s, (x, F(x) ))) od; s J 
In this example the program variable actually coincides with the "independent 
variable" x of the "function y = F(x)", and a program variable could also be introduced 
which corresponds to the "dependent variable" y. (By the way, the table-building step 
s : = stock(s, (x, F(x) )) is often concealed behind a so called printing instruction 
print(x, F(x)).) 
This correspondence quite possibly suggested the general use of the word "variable" in pro-
gramming. One has only to cast a glance at any text book on practical analysis written before 1910 
Note that u is declared and initialized in the innermost segment, i.e. block, that is in the functional-
ly proper position. There is no reason for a (non-initialized) declaration of u further outside. 

5.1 The Origin of Program Variables 
317 
(for instance Bruns, "Grundlinien des wissenschaftlichen Rechnens", Leipzig 1903) to recognize how 
important tabulation, subtabulation and interpolation were at that time. Babbage's "difference 
engine" was built to compute tables, and the ENIAC of world war II, von Neumann's mental chal-
lenge, was attached to the Aberdeen Proving Ground and computed firing tables - as did Aiken's 
machines. The fact that von Neumann's designing and programming philosophy (which to a great 
extent set the tone of the development in the US and England) included storage cells which could be 
given new contents, i.e. which could be "variably" set, enhanced the introduction of the term 
"variable" as an abstraction of storage cells, together with the first approaches to "automatic pro-
gramming". 
Incidentally, von Neumann himself with his explanation of bound variable (in Gold-
stine, von Neumann 1947, p. 90- 91) also contributed to (possible) confusion. He correct-
ly describes "free variables" and connects them to parameters. He describes a program 
variable in a typical way, but assumes wrongly that the term "bound variable" taken from 
logic exactly characterizes it: 
'A mathematical-logical procedure of any but the lowest degree of complexity cannot fail to 
require variables for its description. It is important to visualize that these variables are of two kinds, 
namely: First, a kind of variable for which the variable that occurs in an induction (or more precisely: 
with respect to which the induction takes place) is typical. Such a variable exists only within the 
problem. It assumes a sequence of different values in the course of the procedure that solves this 
problem, and these values are successively determined by that procedure as it develops. It is 
impossible to substitute a value for it and senseless to attribute a value to it "from the outside". Such 
a variable is called (with a term borrowed from formal logics) a bound variable. Second, there is 
another kind of variable for which the parameters of the problem are typical - indeed it is essentially 
the same thing as a parameter. Such a variable has a fixed value throughout the procedure that solves 
the problem, i.e. a fixed value for the entire problem. If it is treated as a variable in the process of 
planning the coded sequence, then a value has to be substituted for it and attributed to it ("from the 
outside"), in order to produce a coded sequence that can actually be fed into the machine. Such a 
variable is called (again, borrowing a term from formal logics) a free variable.' 
The "induction" von Neumann mentions is a special case of recursion. We will deal 
with it in the next section. 
5.1.2 Specialization of the Range-of-Values Machine 
The technique described in 4.3.4 suggests a machine which -
deviating from the stack 
machine and restricted to range-of-values recursions -
recurrently computes the recur-
sively defined value with the help of a table to be constructed according to the range-of-
values. Typically there is no recursive call for the range-of-values machine but only the ex-
traction of already computed values from the range-of-values table as well as the entry of 
the next newly computed value into the table. The table, therefore, replaces the value and 
protocol stacks of the stack machine as a storing device. The control unit of the range-of-
values machine accomplishes the complete execution beginning with the initial value of the 
parameter which is the smallest element in the (well-ordered) parameter range and ending 
with the parameter value which was required. This computation strategy corresponds 
exactly to Rutishauser's for-statement (1952) which reads in its general form 
where .r<~> stands for f(m~), m0 is the minimum element and m~ the 1-1-th element with 
respect to the well-ordering of the (countable) parameter range. The range-of-values con-
dition requires hv(i) < i, v = 1 ... r - 1. 

318 
5. Program Variables 
It is clear how a program for a range-of-values machine is obtained from the definition 
of a routine written according to the notation used hitherto. In the example fuse from 
4.3.4 we have 
«fori= 1 (1)n:fusc<il = if i = 1 
then 1 
D i > 1 A even i thenjusc(i) 
D i > 1 A odd i thenjusce-;1) + fusc(i~ 1 ) fi» 
In the case of a range-of-values recursion we speak of r-term recurrence if in the body 
of routine f only 
/(i-1), j<i-2), •.. , j<i-r+ 1> occur, and therefore hv(i) 
i- v 4• 
A single-term recurrence is the non-recursive case, a 2-term recurrence leads to an im-
portant special case (which was already mentioned in 2.10.1): 
«for i = 0 (1) n: j<i> = if i = 0 then «initial value» 
else P1 (i, J<i-1>] fi» 
The range-of-values machine generally constructs the table unnecessarily (unless it is need-
ed anyhow): only the last entry is needed. Thus in a suitably simplified range-of-values ma-
chine the table can shrink to a single "register" which contains step by step the last entry. 
This is Rutishauser's idea of index suppression 5• 
In programming, a program variable is the counterpart to this register. In ALGOL 68-
like notation we have (at first with a non-initialized variable f) 
r var J..j; 
for nat i from 0 by 1 ton do f: = if i = 0 then «initial value» 
else PJ [i, J] 
fi od; 
f 
J 
which can also be transformed into an initialized version 
4 If h~(i) = i - k x ll· the range-of-values recursion splits into k chains, the treatment of which can 
proceed as described in the sequel. 
Rutishauser does not yet use the term variable in his epoch-making 1952 paper on "Rechenplanfer-
tigung". He writes (p. 26) for the inner loop of the matrix multiplication 
hj_ 1 + (au x bjk) => hj 
and later adds (p. 29): "the following storage cells are reserved: 99 for hj (not depending on j, be-
cause the old h-values are no longer needed) ... ". Rutishauser introduced program variables, by 
means of index suppression, in the transition from the iteration 
xj = cp(xj_ 1) 
to the notation of the "yield instruction" (with Zuse's arrow) 
cp(x) => x 

5.1 The Origin of Program Variables 
319 
I var J,.,f: = «initial value»; 
for nat i from 1 by 1 ton dof: = !2 [i, f] od; 
f 
J 
To be more general, for the special linear recursion (comp. 4.1) 
funct F "' (/.., m) p: 
if m = m0 then .?it[mJ 
else qJ (F(pred m), m) fi 
(if, in the well-ordering of A. m0 is the minimum element of /..,and pred is the predecessor 
function 
pred m~ = m~_ 1 
hence the condition of range-of-values recursion is satisfied, i.e. we have a (terminating) 
2-term recurrence), we obtain, by definition, the equivalent version 
funct F "' (/.., m) p: 
I var p v : = .?It [ m0J ; 
for J,.,y from succ m0 tom do v: = qJ (v, y) od; 
v 
J 
Note that the "controlled variable" y is bound to the assignment and is not a program 
variable. Note also that by definition termination is guaranteed. 
Example: For fac we obtain - with /..,replaced by nat, m0 by 0 andy by i: 
functjac "' (nat m) nat: 
I var nat v : = 1; 
for nat if rom 1 by 1 to m do v : = i x v od; 
v 
J 
Program variables are thus immediately motivated by the special case of 2-term recur-
rence. However, for r-term recurrence (r ~ 3) we obtain likewise a simplification of the 
range-of-values machine: as only the last r -
1 table entries are needed the table can be 
replaced by a buffer store (a shift register). Accordingly, corresponding to 
funct F "' (/.., m) p: 
if m = m0 
then .?lt'0[mJ 
am= m1 
then .Jfj [mJ 
am= mr-2 then Jf,_2[mJ 
am~ mr_ 1 then qJ(F(pred m), F(pred2 m), ... , F(predr- 1 m), m) fi 
(for r ~ 2) there is, under the above conditions, by definition the equivalent version 

320 
5. Program Variables 
funct F = (/., m) p: 
r (var p v1, var p Vz, ... var p vr-1) := (Jfo[mo], £1 [m1J, ... £'r-zCmr_z]). 
for J.,y from succr- 1 mo tom do 
, 
(v1, Vz, .•. Vr-2• Vr-1) := (Vz, V3, ... Vr-1• <p(V1, Vz, •.. Vr-1•y)) Od; 
J 
5.2 Formal Introduction of Program Variables 
One could consider the "non-recursive" form of repetitive routines as being defined by the 
corresponding recursive form. However, it is customary to use program variables more 
generously than in these special forms. This requires independent introduction. The prop-
erties ascribed to them are indeed apparent from the manner of their use for repetitive rou-
tines as described in 5.1.1. The simplification of the range-of-values machine for 2-term 
and r-term (r > 2) recurrence also sheds light upon the character of program variables. 
Furthermore variables have a certain result character which at first suggests their use as 
result parameters. The transient character of variables becomes evident in their free use as 
parameters. 
After the preceding introductory discussion we will formally introduce program 
variables in this paragraph as "a means of saving identifiers" and therefore interpret the 
program variable - which is the basic element of the procedural level - as a notational 
abbreviation for an applicative formulation. 
5.2.1 Sequentialization of Object Declarations 
Recall1.13 .3 where object declarations were introduced as notational abbreviations. If the 
expression 
C§ [tJ, 
yielding an object of mode p and containing the subexpression @' of mode )., -
is 
implemented by 
g [ t] where 
funct g = (J.,x) p: W [x] 
then the abbreviation for this reads 
J., x = t within 
W [x] 
The object declaration emphasizes that the (common) subexpression @' has only to be 
computed once. 
Likewise the collective object declaration 

5.2 Formal Introduction of Program Variables 
(JI. x, v y) "' ( c, §) within 
~ [x, yJ 
is an abbreviation for the implementation 
g(C, §)where 
funct g "'(JI.X, v y) p: ~ [x, yJ 
of ~ [ c, .rJ. 
We should distinguish this from the implementation of ~ [ @', ff [ @' J J as 
f(C) where 
(•) 
functj"' (Ji.X) p: g(x, .r[xJ), 
funct g"' (JI.X, v y) p: ~ [x, yJ 
By suppressing parameters we can also write 
f(C) where 
functj"' (Ji.X) p: 
I g(ff[x]) where 
funct g "'(v y) p: ~ [x, yJ J 
321 
For this two-stage system we have as an abbreviation the two-stage declaration (the seg-
ment) 
JI.X"' c within 
Ivy "' .r [xJ within 
~ [x, yJ 
J 
Generalization to more than two stages is evident. The piling up of right angular 
brackets was the motive in 1.13.3.2 for using a special separator - the semicolon. The 
detailed form 
(u) 
JI.X"' C; vy"' .r[xJ; ~ [x, yJ 
simply represents a notational abbreviation of (•). 
Suppression of parameters leads to hierarchical subordination of routines, thus the 
natural flow of control ( comp. 1.4.4) in a system is clarified and rendered more explicit. 
The use of the semicolon accentuates this. The semicolon is an explicit sequentia/ization 
symbol. The "sequential" formulation ( **) shows clearly the natural flow of control of the 
applicative formulation (•). Not every system permits sequential formulation. The dis-
entangling discussed in 4.4.1 serves precisely - by reshaping - to produce a version which 
permits sequential formulation. Disentangling means, therefore, preparation for 
sequentialization. 

322 
5. Program Variables 
Examples: 
(1) Reshaping of the expression (comp. 1.13.1.2) 
ti3+ti2-2xt-1 
by using the law of distribution yields the Horner form 
((t + 1) X t -
2) X t -
1 
Structuring of this expression yields the segment 
I real h1 = t + 1 within 
I real h2 = h1 x t -
2 within 
I real h3 = h2 x t- 1 within h3 J J J 
or, explicitly sequentialized, the segment 
real h1 = t + 1; real h2 = h1 x t- 2; real h3 = h2 x t- 1; h3 
(2) Continued formation of the arithmetical-geometrical mean 
(an+!• bn+1) =((an+ bn)/2, Van X bn) 
yields the n-stage nesting of segments 
I (real a1, real b1) = ((a0 + b0)12, sqrt(a0 x b0)) within 
I (real a2, real b2) = ((a1 + b1)12, sqrt(a1 x b1)) within 
or, explicitly sequentialized, the segment 
(real a1, real b1) = ((a0 + b0)12, sqrt(a0 x b0)); 
(real a2, real b2) = ((a1 + b1)12, sqrt(a1 x b1)); 
(real an, real bn) = ((an-1 + bn-1)/2, sqrt(an-1 X bn-1)); 
an 
In accordance with 1.13 we define a segment to be 
(a) >expression< 
or (b) >expression< where >declaration< 
or (c) >declaration< within >expression< 
or (d) >declaration<; >segment<. 
Here >declaration< stands for a (system of) routine(s) and for (collective) object declara-
tions. 
A segment contained in segment brackets is itself an expression which is often called a 
generalized expression. 

5.2 Formal Introduction of Program Variables 
323 
5.2.2 Program Variables as a Means for Saving Identifiers 
If an object is no longer required its identifier can be used for another object. Outside the 
range of an identifier this is clearly valid. However, we now want to do this within the 
range (where the identifier in question is only to be used for a new object of same mode). 
To be more exact: it should be permissible to re-use an auxiliary identifier, which has been 
introduced by means of an object declaration, for another object which will be computed 
later in the explicit sequentialization. As an example it should be permitted in the segment 
J.. V = t X t; J.. W = v X v; W X t 
to make do with a single result identifier, by changing perhaps the first object declaration 
J.. v = t x t to the declaration of a program variable v 
var J.. v: = t x t 
and the second to the assignment 6 
v:=vxv 
thus producing the segment 
var J.. v : = t x t; v : = v x v; v x t 
and doing without w. An assignment replaces an object declaration if its identifier has 
been dispensed with (comp. Burstall1968, Pepper 1979). 
Evidently this is possible only because the original v in the final result is no longer need-
ed. A segment 
J.. a = t x t; J.. b = a x a; a x b 
does not allow the re-use of a instead of b. 
When variables occur we no longer speak of segments but of blocks. By analogy with 
the concept "segment" we define: 
A block is composed of an (initialized) variable declaration, followed by a (possibly 
empty) sequence of declarations and assignments, separated by semicolons, and is ter-
minated by an expression which determines a result to be delivered. 
Just as in the case of segments, a block contained in segment brackets is an expression. 
6 "Assignment" only expresses that a declaration has already been made and the identifier in 
question is re-used; a "container concept" is a possible, but not necessary interpretation. 

324 
5. Program Variables 
We define: 
The block 
varJ..x:= rff0;x:= rff1 [xJ; ... ;x:= r.r.[xJ; C§[xJ 
(At) 
is the same as the segment 
The identifiers X; must be "new"; they should not already exist in the block. Further-
more there may not be an assignment to x in rff; nor in C§. 
This definition can, of course, be generalized to the case that other object declarations 
are scattered among the assignments x : = rff; [x J, e.g . 
... ;x:= rff;[xJ; JlY = .r[xJ;x:= rff;+ 1 [x,yJ; ... 
The assignment symb~l : = distinguishes clearly an assignment to a program variable 
from an object declaration 7• val x denotes the current value of a variable x; but val is 
superfluous in expressions 8• 
Example: 
Instead of (comp. 5.2.1) 
real h 1 = t + 1; real h 2 = h 1 x t - 2; real h 3 = h 2 x t - 1; h 3 
the following can be written - saving identifiers - by introducing a program variable h: 
var real h: = t + 1; h: = val h x t - 2; h: = val h x t - 1; val h , 
in short 
var real h: = t + 1; h: = h x t - 2; h: = h x t - 1; h 
From a collective object declaration (1.13.3) a collective variable declaration or a col-
lective assignment 9 is obtained by the above-mentioned transition: 
(J..x, JlY) = (rff, ff); (J..u, J1 v) = ( C§ [x, yJ, .Yl'[x, yJ); f(u, vJ 
can be rewritten 
(var J..x, var J1 y) : = ( rff, ff); (x, y) : = ( C§ [x, y J , J'l' [x, y J ); f (x, y J 
7 Sometimes the symbol <- is used. It would be more advantageous syntactically and with respect to 
transformations (comp. 5.2) to write the assignment in the opposite direction. This was introduced 
by Zuse in the "Piankalkiil" 1945 with the "yield symbol" =>. 
8 In the sequel val is used only for clarification purposes. An interpretation of val as an operator 
will become necessary only in Chap. 7, when transition to the container concept is made. 
9 Also "simultaneous assignment" (CPL, 1963), "multiple assignment" (Hoare 1973) or "concur-
rent assignment" (Dijkstra 1976). 

5.2 Formal Introduction of Program Variables 
325 
saving u and v. A collective declaraction is one declaration, a collective assignment is one 
assignment. Of course, no two variables on the left-hand side of a collective variable 
declaration or assignment may be equal. 
The construction of applicative routines is based, apart from the principle of substi-
tution, on branchings. We also have to specify the interplay of alternatives and variables. 
According to the principle of substitution 
l..x = 0'; if !!4 [x J then 1.. y = §j [x J ; ~1 [y J 
else J..y = SS[xJ; ~2 [y] fi 
is, on the applicative level, equivalent to (for a determinate 0') 
if !!4 [0'] then l..x = 0'; l..y = §j [xJ; ~~ (yJ 
else l..x = 0'; l..y = SS[xJ; ~2 (y] fi 
From the latter we can derive 
if !!4 [ 0' J then var l..x : = 0'; x : = §j [x J ; ~1 [x J 
else var 1..x : = 0'; x: = .~ [xJ; ~2 [x] fi 
In both branches of the alternative there is a block with a declaration of a program 
variable x; these variables could also have different identifiers as they are not related. But 
we do not want to introduce two different identifiers, on the contrary, we want only a 
single declaration. Thus we define 
var l..x: = 0'; if !!4 [xJ then x: = §j [xJ; ~1 [xJ 
else x:= SS[xJ; ~2 [x] fl 
(A2) 
is equivalent to 
l..x ,. 0'; if !!4 [x J then 1.. y = §j [x J ; ~1 [y J 
else 1.. y "' SS Cx J ; ~2 (y J fi 
with the obvious generalization to an arbitrary number of assignments before the alter-
native and within its branches; just as in the case of (At) other object declarations may be 
interspersed. The definition carries over to other forms of branching. 
The two definitions (A1) and (A2) form the basis for introducing variables. Certain notational 
variants, however, do not yet appear here, e.g. 
varh:= c0;var11y:= ff0 [xJ; ... ;(x,y):=(C;[x,yJ, 9_i[x,y]); ... 
or 
(varl.x, varJly):= (C0, .£>0); .•• ;x:= r!;[x,yJ; ... ;y:= 9_i[x,yJ; ... 
However, it can easily be seen that no new difficulties arise. 

326 
5. Program Variables 
Declared program variables also have a range of binding which - according to (A1) 
and (A2) - is defined similarly to the range of binding of corresponding object declara-
tions, that is as the block which immediately contains the declaration in question. The 
definition of the scope in 1.13 .2 also carries over. Viewed operationally, the range of bind-
ing is often called the "lifetime" of the program variable. 
The nested structure of scopes and ranges (introduced with ALGOL 60 by Samelson) 
which thus evolves is called block structure. It is a counterpart to the hierarchical structure 
of systems of routines. 
Declarations of program variables to which, apart from initialization, no further 
assignment occurs can be viewed as object declarations. Conversely, therefore, object 
declarations can be implemented by single-assignment variables. This applies to many pro-
gramming languages, completely (ALGOL 60) or partially (PASCAL). All object declara-
tions at least can thus be dispensed with. However, the use of such "constant" variables is 
of no advantage whatsoever. For clarity it is, at least initially, preferable to keep the object 
declarations. 
However, a complete transition to variables is the rule on the level of systems program-
ming in order to suit a stored-program machine (Chap. 7). 
5.2.3 Expressions with Side-Effects 
5.2.3.1 In a segment 
it can happen that rff1 [x1_ 1J itself is a segment of the form 
Of course, using one variable x instead of x0 , x1 , ••• , x1, we can write 
var Ax : = rff0 ; .•• ; x: = 
lAY = ff [x J; W [y J J ; ... 
Moreover, since we have assumed that W does not depend on x1_ 1 , the identifier y can also 
be saved and the total result is: 
var Ax : = rff0; ••• ; 
x : = I x : = ff Cx J; W Cx J J ; ... 
The generalized expression I x : = ff [x J; W [x J J thus contains an assignment to a 
variable which is not itself declared in the segment (or block). In such a case we speak of an 
expression with side-effect. These expressions with side-effect are explained semantically 
by means of suitable extensions of the definitions (A1) and (A2). 
Of course, in the example above, we can do as well without side-effect, since 
is equivalent to 

5.2 Formal Introduction of Program Variables 
327 
according to the definition of the object declaration in 1.13.3. 
Some programming languages, such as ADA, disallow or restrict the use of side effects 
for the following reason: Certain restrictions must be observed when using expressions 
with side-effect in collateral situations. While proper expressions can generally be placed 
side by side as arguments (e.g. in parameter positions, in collective declarations and in col-
lective assignments), this is not always the case for expressions with side-effect. Thus, for 
example, the two generalized expressions 
x:=x+1;x+a and x:=x-1;x-a 
can not be used collaterally, because the same variable is used and altered in both. But the 
expressions 
x:=x+1;x+a and y:=y-1;x-a 
too, can not be placed side by side. This restriction in the introduction of expressions with 
side-effect can be expressed generally as follows: 
A collection of (generalized) expressions 6'1 , ••• , en can be used collaterally, if and only 
if no program variable to which a value is assigned within some expression If; occurs within 
any 1!1 U =1= i) 10• 
5.2.3.2 The next step is to permit a segment or a block no longer to yield a result but only 
to have "side-effects"; we call this a statement or a pure block, resp. The following defini-
tions introduce these constructs formally: 
The pure block 
I Y; X:= @' J 
(where Y stands for a sequence of assignments or declarations) is 
(A3) 
equivalent to the assignment of a block 
X:= I Y; @' J 
- provided x is not declared in Y. 
10 Of course we could define ad hoc some order for expressions which have a variable in common, 
but this would mean that only specific computation rules (in the sense of 1. 7 .3) would be per-
mitted. Whenever in the sequel generalized expressions occur in a scheme in collateral situation, 
such as 
(xt,Xz):= (i.'l't; o1't J, I Y'z; CzJ) 
it is tacitly assumed that they can be used in this way. 

328 
5. Program Variables 
and 
The alternative statement 
if fA then x : = rff1 else x : = rff2 fi 
(A4) 
is equivalent to the assignment of an alternative 
x : = if fA then rff1 else rff2 fi 
Guarded statements can be introduced in a similar way (see also 5.4.1). 
The repetition constructs introduced informally in 5.1 are statements, too. 
As an example, consider the routine (•) of 5.1.1. 
The right-hand side of the assignment 
(a, b) : = (b, I var nat u : = a; while u ~ b do u : = u -
b od; val u J ) 
contains neither a declaration of nor an assignment to b; going back to the definition (A1), 
it can be transformed into a block, 
(a, b):= I var nat u :=a; while u ~ b do u := u- bod; (b,u) J 
Now using (A3), we obtain 
I var nat u : = a; 
while u ~ b do u : = u -
b od; 
(a, b) : = (b, u) 
J 
Since statements and pure blocks do not yield results, they cannot occur in collateral 
situations; hence the objections raised above against side-effects do not apply to them. 
5.2.3.3 The "empty" statement is also denoted by skip, i.e. the equivalence class of all as-
signments of the form 
is represented by skip. From (A1) it follows that skip can be omitted within a non-empty 
segment. 

5.2 Formal Introduction of Program Variables 
5.2.4 Complete Sequentialization of Collective Assignments 
As a collective object declaration may be sequentialized in any order, 
(l.x, JLY) "'(<%',~within 
~ [x, yJ 
is equivalent to both 
var l.x := <%'; var llY := S'; ~ [x, yJ 
and 
varlly:= sr;varl.x:= t%'; ~[x,y] 
(Note that neither x nor y can occur freely in t%' and ff.) 
329 
On the other hand, a collective assignment can only be sequentialized under special 
conditions: thus, 
(x,y):= (~[x,yJ, Jf[x,y]) 
can not be reshaped into 
x: = ~ [x, y J; y: = £' [x, y J 
which can be seen immediately from the definition (A1). 
Example: The continued computation of the arithmetical-geometrical mean can be written 
as follows, saving identifiers, by introducing two program variables a, b: 
(var real a, var real b) : = (ao, b0); 
(a, b) : = ((val a + val b)/2, sqrt(val a x val b)); 
(a, b):= ((val a+ val b)/2, sqrt(val ax val b)); 
(a, b):= ((val a + val b)/2, sqrt(val a x val b)); 
val a 
The following sequence is not identical to the one above. It is a result of the common 
mistake of "overwriting a variable" 11 : 
var real a : = a0 ; var real b : = b0 ; 
a:= (val a + val b)/2; b: = (sqrt(val a x val b); 
a:= (val a + val b)/2; b: = (sqrt(val a x val b); 
a:= (val a + val b)/2; b: = (sqrt(val a x val b); 
val a 
11 Also called "Gauss-Seidel-Effect" in jargon. In the given example the second sequence, by the 
way, also yields a mathematically interesting limit, see Carlson 1971. 

330 
5. Program Variables 
We could instead proceed in a safe way. A k-membered collective assignment 
is split into a definition of k (auxiliary) constants of the respective mode 
and a collective assignment of the constants 
Now this collective assignment can be arbitrarily sequentialized, that is e.g. from left to 
right 
In an alternative technique the value of every variable is copied: 
(lt Kt, lv;.K2, · · 'J...kKk) = (at, a2, • • ak), 
(at' a2, •• ak) : = ( 0'/K), 0'2(K), •. 0'k(K)) 
where @'~(K) is obtained from Iff~ by substituting all av by Kv. 
Usually, however, the programmer strives to eliminate auxiliary constants. All the 
auxiliary constants can be eliminated if a permutation 1t of the indexes is found such that 
the variables an(t)• am2), an(3) •.• an(i-t) do not occur in 0'n(i) (graded expression system). 
Then this order can be chosen for sequentialization and H~ can be replaced by C~ every-
where. 
Example: 
(a, b) : = (a + 1, a + b) 
After re-ordering, the expression system is graded 
(b, a) : = (a + b, a + 1) 
since b does not occur in a + 1. Sequentialization yields 
b: = a + b; a:= a + 1 
In general it is a complicated problem to find a graded expression system - if one exists at 
all - or a sequence which requires a minimum number of constants (Belady 1966). 
In sequentialization structural information gets lost - that is information as to which 
program components are computable independently of each other. Recovery of this infor-
mation is generally very troublesome. Therefore complete sequentialization should only be 
done at the end of a development process. 

5.3 Procedures 
331 
A Babbage-Zuse machine which only processes completely sequentialized routines is 
called a sequential machine 12• As usual, expressions are also broken down into single 
operations when the results of the operations are assigned to auxiliary variables. 
5.3 Procedures 
In the previous section program variables occurred only inside an expression, they were 
invisible from outside. Accordingly the functional character of routines was completely 
retained. In this chapter program variables are permitted as parameters too, and thus new 
problems arise: actualization taboo (comp. 1.13.4) and procedure side-effect. 
5.3.1 Program Variables as Parameters 
5.3.1.1 For routines of the functionality funct (1.) A, such as 
functj = (A a) #.: JT [a J 
which have the same argument mode and result mode, calls of the form 
x: = f(val x) 
often arise, where xis a variable of mode var #.. If there are a sufficient number of assign-
ments of this kind it is convenient to put the assignment into the body of the routinejas a 
notational abbreviation. However, as not always the same variable is used, program 
variables must be permitted as parameters. In the above example we have 
procj* = (var #. v): v := JT[val vJ 
with the call 
f*(x) 
We call routines which contain program variables as parameters procedures and 
identify them by the keyword proc instead of funct. These parameters are marked as pro-
gram variables in the heading by placing var in front of the mode indication. If a routine, 
such as f* above, does not deliver a result, no result mode is, of course, indicated in the 
heading. 
12 This name should be a reminder of the word "sequence" e.g. in Aiken's Mark I Automatic 
Sequence Controlled Calculator and Mark II Selective Sequence Controlled Calculator, or in W. 
J. Eckert's Selective Sequence Electronic Calculator (comp. Randell1973). In 1947 one already 
differentiated between sequencing = setting up a command sequence, and coding = punching the 
program (Harvard Symposium 1947). 

332 
5. Program Variables 
Thus the following definition results (note that A. a can stand for a whole list of para-
meters): 
The callf*(x) of the procedure 
procj* = (var A. v): v := .r(val vJ 
(A5) 
is equivalent to the assignment 
x: = j(val x) (where xis of mode var A.) with a call of the junction 
functf=(A.a)J..: .r(aJ 
We can view this in the following way: One argument parameter and one result para-
meter (comp. 1.13.4) are fused to form a transient parameter - also called an access para-
meter - having the character of a program variable. 
After the assignment has been put into the procedure, that is when the body has the 
form v : = ff (val v J , the definitions (A1)- (A4) can be used for further simplification. 
Now the variable parameter v can serve to save "local" object identifiers in the body. 
Example: The routine ord (1.12) for determining the maximum and minimum of two num-
bers can be reshaped into a procedure for the ordering of two numbers 
proc ord* = (var nat r, var nat s): 
(r, s) : = if r ~ s then (r, s) 
D r ~ s then (s, r) fi 
Using (A4) and replacing (r, s) : = (r, s) by skip, we obtain the descendant 
proc ord* = (var nat r, var nat s): 
if r ~ s then skip 
D r < s then (r, s) : = (s, r) fi 
The variable x in the callj*(x) can either be declared in an outer block or be a formal 
parameter of a superior procedure itself ("passing on" of parameters). 
As a special notational variant the initialized declaration can be performed at the actual 
parameter position in the call - parallel to the case of the result parameter in 1.13.4: 
f*(var A. x : = #') 
is therefore equivalent to 
var J..x := .t';j*(x) 
Example: A routine for incrementing a variable 

5.3 Procedures 
333 
proc count = (var Ax): 
X:= SUCC X 
It could be applied as follows, distinguishing the three cases above: 
(a) by a pre-declaration of the actual parameter (the "usual case"): 
funct count to three = A: 
I var An:= 0; 
count(n); count(n); count(n); 
val n 
J 
(b) by passing on a parameter: 
proc count three times = (var Ax): 
I count(x); count(x); count(x) J 
(c) by declaration in the (first) call: 
funct count to three = A: 
I count (var An : = 0); count(n); count(n); 
val n 
J 
Some generalizations of the above rule (AS) are necessary in practice: 
f can contain further argument parameters which are not to be fused with result para-
meters, e.g. 
functj =(A a, ll b) A: .r (a, b] 
with the intended use 
X:= /(Val X, C) 
In this case the corresponding procedure and call are 
procj"' = (var A v, ll b): 
v: = .r(val v, b] 
and f"(x, 0') 
5.3.1.2 Likewise it can happen that further "pure" results exist, which are not to be fused 
with an argument parameter. Suppose we have 
functj = (Aa)(A, p): (.r(aJ, "#(a]) 
with an intended call 
(x, y) : = j(val x) 

334 
5. Program Variables 
The corresponding procedure 
procf* ""(var lv) p: f"lh ""val v; v := ff[h]; w [h] J 
still has a result, its call is an expression with side-effect and may occur in an assignment 
Y: = f*(x) 
(If the expression I§ (a J does not depend on a here, the sequentialization in the body off* 
is possible without introducing an auxiliary object identifier h for the original value of v.) 
These two forms of generalization can also occur as a mixed form. 
5.3.1.3 In all cases discussed so far transient variable parameters have been considered, 
i.e. the "fusion" of an argument parameter and a result (parameter) to form a variable 
parameter. The "degenerate cases" can now be permitted in which only an argument 
parameter 13 or only a result parameter is changed into a variable parameter ("input 
variable", "result variable"). 
In the first case ("input variable") we have 
functj"" (la) p: ff[aJ 
with the intended call 
y := f(val x) 
from which 
proc f* = (var l v) p: ff [val v J 
with the call 
Y: = f*(x) 
is derived. 
Note that here in the body off* the variable parameter v may not in general be used to 
save further object declarations. This would result in the value of x being changed after the 
callf*(x). It is of no advantage whatsoever to use variable parameters if there is no assign-
ment to them in the body. 
The second case ("result variable") is different: from 
functj = (la) p: ff[aJ 
with the intended call 
y: = j(rff) 
' 
13 Expressed in ALGOL 60 by the value specification, see Rutishauser 1967, p. 181. Dijkstra (1976) 
demands that such variables should be marked. They can then be interpreted as object 
parameters. 

5.3 Procedures 
335 
the following emerges: 
procj* =(I.. a, var p v): v: = ff [a] 
with the call 
j*(rff,y) 
The result variable v can here be readily used for simplification in the body off*. Now, 
however, there could be meaningful non-initialized variable declarations (see 5.3.4). The 
call 
var py: = j(rff) 
now becomes 
var p y; f*( rff, y) 
or directly 
f*( rff, var p y) 
(comp. also 5.3.4). 
Examples: For certain routines of 5.1.1 we form as counterparts procedures with variable 
parameters. Thus, starting with fac, we can introduce 
proc jac* = (var nat n, var nat m): 
while n =1= 0 do (n, m): = (n -
1, m x n) od 
we then have 
funct jac = (nat N) nat: 
I (var nat n, var nat m) : = (N, 1); 
jac*(n, m); 
val m 
J 
nand mare transient parameters injac*. Only m acts as the real result; n, finally, has 
always the value 0, a result which is not particularly interesting. 
On the basis of the first version of mod in 5.1.1 we obtain 
proc m* = (var nat a, var nat b): 
while a!;; b do (a, b):= (a- b, b) od 
If we simplify the assignment to a : = a - b, that is 
proc m* = (var nat a, var nat b): 
while a !;; b do a : = a - b od 

336 
5. Program Variables 
then var nat b is an "input variable". But instead we can use an argument parameter nat B 
in accordance with the second version of mod in 5.1.1. 
If we distinguish between parameters according to 
a) arguments (ordinary argument parameters) 
b) results (pure result parameters) 
c) transients (transient parameters), 
then their r6le is played principally by 
a) normal objects 
b) intermediate result designations 
c) program variables. 
Program variables can take on all three r61es. This contributes as much to their 
glamour as it does to the misery of those for whom the difference is blurred. 
If a procedure still yields a result its body is an expression with side-effect. If it does not 
deliver a result - in this case we also speak of a pure procedure - then its body is a state-
ment, possibly a (pure) block. This also holds correspondingly for calls. 
We thus extend the hitherto existing possibilities for constructing a segment or a block 
by defining: 
The call of a procedure is an expression (with side-effect). 
The call of a pure procedure is a statement. 
If expressions with side-effect are disallowed, only pure procedures are left. 
5.3.2 Actualization Taboo, Alias Ban and Suppressed Variable Parameters 
5.3.2.1 Naturally the actualization taboo (comp. 1.13.4) also holds for variable para-
meters, due to their result character: no two actual variable parameters in a call may be 
equal. (The same applies to the left-hand side of a collective assignment, comp. 5.2.2.) 
It is therefore forbidden to write a call such asfac*(v, v); this would lead to the mean-
ingless assignment (v, v) : = (v - 1, v X v). 
Likewise a call 
ord*(a, a) 
is of course meaningless - even if in this special case there is no assignment conflict. 
Only variable parameters which are not assigned to in the body ("input variables") 
could be excluded from this actualization taboo. This, however, is unnecessary, as 
ordinary argument parameters can (and should) be used in their stead. 
On the level of completely sequentialized routines the actualization taboo must still be 
obeyed, but its purpose seems now less obvious. For the procedure 14 
proc s = (var int x, var int y): 
(X, y) : = (2 X X, y + 1) 
14 For example as an auxiliary procedure for dealing with the problem 
(nat n: n * 0) nat: t nat a: 2° ~ n < 2•+ 1 

5.3 Procedures 
337 
which multiplies one variable by 2 and increments another by 1, indeed the call s(a, a) will 
be considered meaningless by everybody. But for the sequentialized implementation 
proc sl = (var int x, var lnt y): 
X:= 2 X x; y : = y + 1 
ALGOL 60 e.g. permits the ca11 15 
sl (a, a) 
with the effect 
a:=2Xa+ 
However if we sequentialized as follows 
proc s2 = (var int x, var int y): 
y:=y+ 1;x:=2 XX 
then s2(a, a) would effect 
a:=2xa+2 
Taking the actualization taboo into consideration, sl and s2 are equivalent procedures. 
The surprising effect of the abusing call is also due to the fact that sl (a, a) and s2(a, a) are 
no longer the same. From the point of view of program transformation and program 
verification the actualization taboo should therefore be required on the level of complete 
sequentialization too. 
Another example (Dijkstra) refers to the rotation of a vector in the plane by an angle q>. 
In completely sequentialized form some would solve the problem perhaps in the form of 
proc rotl = (var real u, var real v): 
jreal h = u x cosq>- v x sinq>; v := u x sinq> + v x cosq>; u := h J 
others by the (completely equivalent) version 
proc rot2 = (var real u, var real v): 
I real h = u; u : = u x cos q> - v x sin q>; v : = h x sin q> + v x cos q> J 
A call rotl (a, a) or rot2(a, a) has of course nothing to do with the original problem, it 
is meaningless. If it is permitted, as in ALGOL 60, then rot] (a, a) means a : = a x (cos q> 
- sinq>), whereas rot2(a, a) has the effect a:= a x (cosq> + sinq>). Two routines which 
are equivalent while obeying the actualization taboo are different if the actualization taboo 
is violated. In any case it is better to proceed from a collective assignment, 
15 In 1960 it took a lot of effort to restrain over-zealous ALGOL-followers from seeing a special 
ALGOL 60 advantage here. 

338 
5. Program Variables 
proc rot "' (var real u, var real v): 
(u, v) := (u X COS<p- V X sin<p, U X sin<p + V X COS<p) 
Then one is hardly tempted to consider a call rot(a, a). 
Somebody might find it difficult to accept the actualization taboo for the procedure 
proc exchO "'(varAS, var At): (s, t) := (t, s) 
where exchO(a, a) "does absolutely no harm", in particular if the possibly more efficient 
equivalent version 
proc exch 1 "' (var As, var l t): 
if s = t then skip 
else (s, t) : = (t, s) fi 
is used. Indeed, in this body there will be dynamically no clash of identifiers. The actuali-
zation taboo is here a matter of discipline versus convenience. 
5.3.2.2 A further requirement has a certain connection with the actualization taboo: 
Let us consider the statement 
If .t'1 does not depend on y and .t'2 does not depend on x, it is equivalent to 
and thus to 
These transformations are correct only when x and y do not stand for the same pro-
gram variable. From the point of view of program transformation and verification, there-
fore, one is highly motivated to require: 
Program variables with different identifiers are different program variables. 
This is an important requirement for program variables. In the version "The very same 
variable is not permitted to have two different identifiers" this is known as the alias ban. 
The alias ban simplifies the verification of the actualization taboo: Only the literal identity 
of the identifiers must be checked. 
Horning and Gries, especially, have pointed out the danger of the identification of 
variables with regard to assignment. Roughly speaking it is absurd to imagine, with an as-
signment such as x: = 3, that there is at the same time an assignment of 3 to the variable y 
which is identified with x. A weakness of ALGOL 68 is its promotion of the identification 
of variables. 
5.3.2.3 For simplifying the notation, the possibility to suppress variable parameters is in-
dicated - just as in the case of normal parameters. However, with suppression, the warn-

5.3 Procedures 
339 
ing of a possible alteration of the contents - due to the result character of the variable -
is lost. Such a hidden alteration of variables is a procedure side-effect which is a frequent 
cause of errors. The suppression of variable parameters should therefore be undertaken 
with prudence, even in the case of pure procedures. (The same holds for program variables 
which are non-local for a segment.) 
In the case of suppressed variable parameters the additional precaution must be taken 
to ensure that a substitution connected with a call does not contradict the actualization 
taboo in the result. A routine 
proc soso = (var int x): I a:= a + 1; x: = 2 x x J 
with a suppressed variable parameter a must not be called with the actual variable a: 
soso(a) violates the actualization taboo. 
For these reasons Dijkstra demanded that, for every segment, all the variables used 
there should be specified, even if they are declared further outside (Dijkstra 1976). 
5.3.3 Sharing of Variables 
The danger of identification of variables is also illustrated by the problem of shared varia-
bles: 
The routine (•) in 5.2.3.2 uses a 'private' variable u for the innermost block. We want 
to show that this variable can be saved. To this end, the routines gcd and mod should be 
based on suitable routines with variable parameters. For mod we have from 5.3.1 
funct mod = (nat A, nat B) nat: 
I proc m* = (var nat u, nat B): 
while u ;;:;; B do u: = u - Bod; 
var nat u : = A; m*(u, B); val u 
J 
and for gcd analogously 
funct gcd = (nat A, nat B) nat: 
I proc g* = (var nat a, var nat b): 
while b * 0 do (a, b) : = (b, mod(a, b)) od; 
(var nat a, var nat b) : = (A, B); g*(a, b); val a J 
In g* the call mod(a, b) - more precisely mod(val a, val b) - can now be replaced by 
the segment m*(a, b); a- more precisely m*(a, val b); a. The formal program variable u 
of m* is replaced by the actual program variable a of g*. The program variable u disap-
pears together with its declaration. All the operations on u are now carried out on the pro-
gram variable a. 
Of course, such a proceeding is only possible because the statement 
(a, b) : = (b, I var nat u: = a; m*(u, b); val u J) 

340 
5. Program Variables 
(which results from unfolding of mod) on the one hand contains an assignment to a - the 
old value of a is thus destroyed - , on the other hand the variable a does not occur in the 
block after its value being assigned to u. 
By inserting m * we obtain 
proc g* = (var nat a, var nat b): 
while b * 0 do (a, b) : = (b, I while a ~ b do a:= a - bod; 
val a 
J) od 
Note that the expression with side-effect 
while a ~ b do a:= a - bod; val a 
does not share any variable with the expression b. Thus the collective assignment can be 
transformed to (proof-worthy!) 
while a ~ b do a : = a - bod; 
(a, b) : = (b, a) 
and after finally unfolding g* in gcd we have 
funct gcd = (nat A, nat B) nat: 
I (var nat a, var nat b) : = (A, B); 
while b * 0 do while a ~ b do a:= a - bod; 
(a, b) : = (b, a) 
od; 
val a 
J 
Now compare with(*). There-translation into a system of two recursive routines is also 
instructive. 
5.3.4 Initialization 
In the definitions (A1), (A2) only initialized variable declarations are explained. There-
quirement of complete initialization (initialization rule) excludes notations such as 
var int x; if :?J then x : = 1 else skip fi or 
varintx,y; if !1d thenx:= 1 elsey:= 1 fi 
A completely defined initialization could be e.g. 
var int x : = if .'14 then 1 else 2 fi 
But 
var int x : = if :?J then 1 else D fi 

5.3 Procedures 
341 
is incompletely defined, a "halfway impossible task". The initialization 
var l..x: = Q 
is completely undefined and leads to abortion. On the other hand 
var 1.. x : = (l) , 
where 
funct (l) = 1..: 11 1.. x: true , 
is a well-defined pseudo initialization and important for some implementations. It is often 
abbreviated to var 1.. x. 
Variable parameters (formal parameters specified in the heading or suppressed para-
meters) which are pure result parameters are not initialized, due to their nature. Actually 
they should be treated separately from variables (comp. 5.3.1 and 1.13.4). 
In order to formally maintain the initialization rule for variables which have a pure re-
sult character one performs a pseudo initialization using an insignificant object (l). 
The initialization rule is frequently violated for reasons of run-time efficiency. Con-
sider the routine (comp. (•) in 5.1.1 and the modification in 5.2.3.2) 
funct gcd = (nat A, nat B) nat: 
f (var nat a, var nat b) : = (A, B); 
while b * 0 do var nat u :=a; 
while u ~ b do u : = 
(a, b):= (b,u) 
val a 
u - bod; 
od; 
J 
In this routine, u will be declared anew in every step of the outer repetition. Therefore, the 
declaration of u is rather made in the outermost block, which means, however, that an 
initialization is no longer possible (only a pseudo initialization by an insignificant object (l) 
is feasible): 
funct gcd = (nat A, nat B) nat: 
f (var nat a, var nat b, var nat u) : = (A, B, (l) ); 
while b * 0 do u : = a; 
while u ~ b do u : = u -
b od; 
(a, b) : = (b, u) 
od; 
val a 
J 
If, however, storage space is to be saved, such export of a declaration is not indicated. (In 
the present case, u can be shared with a anyhow). 
Exercise 1: Transform gcd into a completely sequentialized form with non-initialized variable decla-
rations. 

342 
5. Program Variables 
5.3.5 Properties of Program Variables 
Summarized the most important properties 16 of program variables are the following: 
(1) The identifiers of program variables are bound identifiers. They are bound by a decla-
ration which is always initialized. The range of binding is the smallest block, enclosed 
by segment brackets or the like, which contains the declaration. The scope, however, 
which is restricted to this block, can have holes caused by overruling (comp. 1.13.2). 
(2) Objects are assigned to program variables according to the particular mode. The corre-
spondence is variable: program variables are initialized and updated. They always have 
a ("variable") value, possibly (l). The range of binding, considered operationally, is 
their "lifetime". 
(3) Every incarnation of the call of a recursive routine 17 has its own private "co-existing" 
variables - if any variables are declared at all. 
(4) Variables have the character of results. The actualization taboo holds. 
(5) Program variables with different identifiers are different program variables. Program 
variables have a "unique identity" (alias ban). 
(6) Variables are dependent objects, if we want to consider them as objects at all. There 
are other objects -
their value objects - without which they are meaningless. Vari-
ables have no real existence if they are not declared. 
(7) Program variables differ from the usual objects in that only a few universal operations 
are defined for them (assignment, comparison for literal identity, see 7.2.2) and that 
no particular operations on them can be specified. 
With the introduction of the semicolon as an explicit sequentialization symbol in 5.2.1 
we have entered the procedural level. This step was completed by permitting variables as 
parameters. 
In accordance with the "variable" character of program variables the term execution 
position is introduced on the proceduralleveJ1 8• We can thus formulate: "A variable can 
have different values in different execution positions" or "Two different variables can 
have the same value in the same execution position". The execution position, together with 
the totality of the variables, form an alternative means for describing the semantics of the 
procedural level. 
Beginning with McCarthy 1962 various semantic systems have been given: the "Vienna 
definition", "denotational semantics", Floyd-Hoare-semantics. We will deal (in 5.4) only with the 
latter (in its strict form of predicate transformation as introduced by Dijkstra) because it corresponds 
to an important programming method. We will also establish its connection with the applicative level. 
5.4 Axiomatic Description of Programming Languages 
Based on approaches by Floyd (1966), Hoare (1969) developed a system for describing pro-
gramming languages which has become known as "axiomatic semantics". Strictly speaking 
16 A characterization of program variables by their properties was first given by Dijkstra 1970 (in a 
letter to Hoare of Aug. 31, 1970, EWD-292). 
17 The overruling effect introduced in 1.13.2 can be seen particularly in recursive situations. 
18 Not to be confused with state in the sense of McCarthy 1962 (see also 5.4.1). 

5 .4 Axiomatic Description of Programming Languages 
343 
it is not semantics 19 but a calculus with derivation rules and axioms in which proofs about 
programs can be carried out. 
The requirement that the semantics of a language must satisfy the derivation rules and 
axioms of a given calculus - i.e. must be a "model" for it - naturally turns this calculus 
into an exact description of the language20. In this chapter e.g. variables have been derived 
from the applicative level -
with the help of the rules (A1)- (A5) - and therefore have 
been based on mathematical or operational semantics. When, in the sequel, we give a 
calculus for "reasoning" about program variables, the semantics given by (A1)- (A5) 
must correspond to the rules of this calculus. 
Dijkstra (1974, 1975) has turned Hoare's method into a "calculus for predicate trans-
formation". For this "axiomatic semantics", models in the form of denotational semantics 
can be given (Plotkin 1980, Stoy 1981). The subject can be expressed very elegantly and 
with a natural extension to the non-deterministic case as a relational calculus (Schmidt 
1982). 
5.4.1 Predicate Transformers 
The possibility of making propositions about values of variables in every execution posi-
tion21 is fundamental for the calculus of predicate transformation. Thus for an arbitrary 
statement s1 in a program we can give a precondition f!J, i.e. a proposition as to which 
values certain variables have before the execution of sf, as well as a postcondition £!!',i.e. a 
proposition as to which values these variables have after the execution of sf. 
Then we use the notation 
with the following interpretation: "If f!J is true before the execution of sf, then £!!' is true 
after the execution of s1". A construction :% of the programming language can now be 
characterized by giving a suitable precondition f!J to every postcondition £!!', such that 
f!J {:%} £!!' holds. 
Dijkstra (1974, 1975) has given this method a more precise form by associating with the 
postcondition £!!' not an arbitrary precondition but the weakest precondition 22, written as 
wp(f I£!!'). Dijkstra calls the rules that specify this association predicate transformers. 
Thus wp(f I£!!'){.:%}£!!' holds. 
With respect to a description by predicate transformers, two statements Y 1 and Y2 are 
equivalent if 
w p ( 5'1 I £!!' ) = w p ( Y2 I £!!' ) holds for all postconditions £!!'. 
19 At least not in the sense in which the concept is used in Chap. 1. 
20 " ... is equivalent to accepting the axioms and rules of inference as the ultimately definitive specifi-
cation of the meaning of the language." (Hoare 1969). 
21 The correspondence of all variables to their values is usually known as the "state vector" 
(McCarthy 1962). Every assignment changes this vector. All propositions as to the values of varia· 
bles are therefore propositions about the state vector. The expression "state vector" (not partic-
ularly suitable) was taken from quantum mechanics. 
22 Hoare is satisfied with a sufficient condition, whereas Dijkstra works with a sufficient and neces-
sary condition. 

344 
5. Program Variables 
We now consider in detail the predicate transformers for the different forms of state-
ments. 
The following is intuitively clear: The assignment x : = C and the postcondition rJl [ x J 
have rJl [cJ 23 as the weakest precondition. We have (for determinate and defined C) the 
"Assignment axiom" 
wp(x: = C I rJl [x]) 
r!ll [cJ. 
This assignment axiom is to hold for initializing assignments too. 
The fact that the assignment axiom is satisfied by virtue of the rule (A1) in 5.2.2 is seen 
as follows 24: Consider the segment x: = .r [xJ; ':# [xJ and let its delivered result be r. 
Thus ':# [x J = r is the postcondition for the assignment x : = ff [x J . The weakest 
precondition then is ':# [ ff [x J J = r on the basis of the axiom above. 
The rule (A1) yields a segment which is equivalent to the one above: J. x1 "' .r [xJ; 
':# [x1J . According to the principle of substitution it delivers (for a determinate ff) a value 
r equal to ':# [ ff [x J J. 
In general, several assignments follow the declaration of program variables before an 
expression turns up which determines a result to be delivered. The semicolon is not only a 
separator, it becomes a meaningful symbol for the composition of assignments: A state-
ment .9"1 followed by a statement .9"2 and connected by a semicolon is also a statement. 
The meaning of this construction follows stepwise from the 
"Axiom of composition" 
Again, equivalence with the meaning of hierarchical object declarations can be proved. 
The call of a pure procedure is also covered by the term "statement": parameter 
actualization yields a statement in the above sense. 
For the alternative statement (5.2.3) we have intuitively the 
"Axiom of branching" 
wp(if Pi then .9"1 else .9"2 fi I r!ll) = Pi 11 wp(Y1 I r!ll) v ' 
Pi 11 wp(Y2 1 r!ll) 
which agrees with the meaning defined in (A4). 
In addition to alternative statements (5.2.3) guarded statements are introduced. If .9"1 , 
.9"2, ••• , Yn are statements, then 
if !!11 then S'1 D !!12 then .9"2 D ... D Pin then .9'" fi 
23 Recall: The notation !Jt [x J stands for an expression (here a predicate) in which x can occur 
arbitrarily - in particular, can even not occur at all. ~ [,r J then stands for an expression which 
evolves when every occurrence of x in !Jt is replaced by the expression 6'. 
24 Strictly formal proof (Pepper 1979, Broy et al. 1980) falls outside the scope of this book. 

5.4 Axiomatic Description of Programming Languages 
345 
is also a statement. The meaning is defined by the 
"Axiom of the guarded statement" 
wp(if .JIJ1 then .'1'1 0 9J2 then .'1'2 0 ... 0 .JIJ" then.'/'" fi I£!?) = 
(9J1 v 9J2 v ... v .J~J.) A vi= 1, ... , n: .JIJ;--+ wp(Sj I£!?) 
Note that .JIJ; --+ wp (Sf I £!?) holds, if !!d; = false. If all the guards block, then the weakest 
precondition is false, i.e. the effect of the "impossible task" abort with the definition 
wp(abort I£!?) = false for every postcondition £!?. 
By virtue of the axiom of the guarded statement the (generalized) rule (A4) from 5.2.3 
X:= if .JIJ1 then S'j 0 9J2 then .'1'2 0 ... 0 .JIJ n then .'l'n fi 
if 9J1 then x: = S'j 0 9J2 then x: = .'1'2 0 ... 0 .JIJ" then x: = .'!'" fi 
certainly holds if the disjunction of the guards always yields true: Otherwise we have 
x : = Q above and abort below. It is therefore reasonable to define x: = Q as "impossible 
task" abort. 
We used skip to denote the "empty statement", now defined by 
wp(skip I£!?) = £!? for every postcondition £!?. 
From the axioms we can infer that the single-membered guarded statement 
if .JIJ then .'1' fi 
is not equivalent to the one-sided alternative 
if .JIJ then .'1' else skip fi, 
for we see at once that 
wp(if .JIJ then .'1' fi I£!?)= .JIJ A wp(YI £!?), 
wp(if .JIJ then .'1' else skip fi 1 £!?) = !!d A wp(YI £!?) v (I .JIJ A £!?) 
hold. 

346 
5. Program Variables 
Exercise 1: Show the equivalence of x : = x and skip with the help of the axioms above. 
Exercise 2: Show the equivalence of the predicates 
(,g A .9'1) v (--, ,g A .9'2) 
and (,g -+ .9'1) 
A (1 ,g -+ .9'2) 
There is also a characteristic predicate transformer for the repetition: 
"Axiom of repetition" 
wp(while ~do S"od I fil') = (3K, K i1:; 0: <Px(fll')), 
where 
Q>o( f)!') = --, ~ 1\ f)!' 
<P;( f!l') = wp(if ~ then S" else skip fi I <P;- 1 ( f!l' )) 
The connection between this recursive definition and the introduction of repetition in 5.1.1 
is obvious. The following theorem derived from the axiom of repetition is important for 
applications: 
Theorem: IffY"~--+ wp(Yi 9') holds 
then fY "wp(while ~do S"od I true)--+ wp(while ~do S"od I fY"--, ~) 
holds. 
A predicate fY which satisfies the premise is called an invariant of the repetition. 
wp (while ~ do S" od I true) is the (weakest) precondition for termination 25 • 
Note that a suitable invariant fY must always be sought first and that the above 
theorem does not permit it to be simply "computed" for an arbitrary postcondition. This is 
because a recursion is concealed behind the harmless-looking repetition. An invariant of 
the repetition can, however, be obtained immediately from an equivalent recursive defini-
tion. 
The calculus of predicate transformation is completed by the usual derivation rules of 
predicate logic and the following general properties that are defined for predicate trans-
formers: 
(1) Isotonicity 
if 
P1 --+ f!l' holds then 
wp ( S" I P1) --+ wp ( S" I f!l') holds 
(2) Conjunction compatibility 
wp(S" I P11 "Plz) = wp(S" I P11) "wp(S" I P12) 
(3) Weak disjunction compatibility 
wp(S" I P11 v P12) +- wp(.'l' I P11) v wp(S" I P12) holds 
(4) "Law of the excluded miracle" (Dijkstra) 
wp(S" I false)= false 
25 Hoare (1969) calls .9' an invariant, if .9' { Y'}.9' holds. In the wp-calculus, termination is included. 

5.4 Axiomatic Description of Programming Languages 
347 
If .'/' is deterministic we even have 
(3') Disjunction compatibility 
wp(Y I £11 v £/J = wp(Y I £11) v wp(Y I £12) 
It can be seen immediately that these laws hold for the assignment axiom. In the non-deter-
ministic case a counterexample for (3') can easily be found. 
Remark: The alias ban (5.3.2) is important for the utility of this calculus. More precisely: The assign-
ment axiom requires a subtle refinement in the case of the identification of variables. This will be 
taken up again in 7 .2. Here an example may suffice to show that thoughtless use of the assignment 
axiom for identified variables leads to errors (according to Gries). 
Let x andy be identified, thus val x = val y always holds. Consider the assignment x : = 5 with 
the postcondition x2 = y; this yields wp(x: = 5 lx2 = y) = (25 = y). 
On the other hand - because of the identification of x andy - the postcondition x 2 = y is equiv-
alent to x 2 = x and thus to x = 1 v x = 0. To achieve this by the assignment x: = 5 is a "mission im-
possible" because wp(x: = 51 x = 1 v x = 0) = false. The weakest precondition is therefore not 
(25 = y) but false. 
Exercise 3: Show the validity of the two transformations 26: 
_y_:_=_11_;f-x_:_=_1_2 [x does not occur int1 
y does not occur in t 2 
x:= <l'z;y:= 1&'1 
Is the alias ban important here? 
5.4.2 Program Verification 
The weakest precondition can now be "computed" for a given piece of program and some 
postcondition 27• The "backward computation" of predicates in this way is not unnatural, 
on the contrary it is even tailored to the problem. If a condition which characterizes a 
problem is chosen as a postcondition and if for a certain program the weakest precondition 
true results, then this program is a solution to the problem; we say the program is verified. 
Example: For the problem 
(int X) int: Tf lnt Y: Y > 0 " I Yl > lXI 
a possible solution is 
(int X) int: 
I var int x: = X; if x ~ 0 then x: = x + 1 else x: = - (x -
1) fi; x J 
26 These two rules are part of an axiom system on which McCarthy (1962) based his semantics (see 
also de Bakker 1969). 
27 Incidentally, the composition and the branching axiom would look alike if for a precondition the 
strongest postcondition were sought. In contrast, the assignment axiom is more complicated in the 
reverse direction. 

348 
5. Program Variables 
which can be verified as follows: 
The postcondition 
{!,f (xJ =cterX > 0 1\ lxl > lXI 
results directly from the problem, thus 
wp(var intx: =X; ifx ~ 0 thenx: = x + 1 elsex: = - (x- 1) fi 1 {!,f [xJ) 
= wp (var intx : = X I wp (if x ~ 0 then x : = x + 1 else x: = - (x - 1) fi I {!,f (x J)) 
= wp(varintx:=XIx~O A wp(x:=x+ 11{!-f (xJ) v 
x<O A wp(x:= -(x-1)1{!-f (xJ)) 
=wp(varintx:=XIx~O/\ {!,f(x+1J vx<OA {!,f(-(x-1)]) 
= wp(varintx:=XIx~O AX+ 1 >IXIvx<O 1\ -x+ 1 >lXI) 
=X~ 0 1\ X+ 1 > lXI v X< 0 1\ -X+ 1 > lXI 
= X ~ 0 v X < 0 = true 
As already mentioned, verification is not trivial if the axiom of repetition is needed: for 
a problem defined by the postcondition !1 a suitable predicate (an "invariant") iY must be 
found, so that both 
and 
iY 1\ ~ --> wp(S" I IY) 
hold. The latter condition is fulfilled in particular if 
~ --> (IY = wp(S" I IY)) 
holds; then iY is a property which is even invariant under .'!'. The problem of finding for a 
given condition ~ and a statement .5" such a predicate iY with respect to an arbitrary post-
condition !1 is usually a puzzle. iY does not have to be uniquely determined and may even 
not exist at all. We take the example 
proc m = (nat A, nat D: D * 0) nat: 
I var nat x : = A; 
while x ~ D do x: = x - Dod; x J 
(monotonicity guarantees termination). 
With respect to conditional repetition we have 
~ =x~D and Y=x:=x-D 
thus 
wp(S" I iY (xJ) = iY (x- DJ 

5.4 Axiomatic Description of Programming Languages 
349 
Therefore fJJ must be such that 
fJJ [xJ 
A x < D = P2 and 
fJJ [xJ 
A x ~ D __. fJJ [x - D] hold 
This cannot be satisfied for P2 = true. Of course, we are not interested in a solution 
for P2 = false ( fJJ [x J = false is such a solution). What is a "reasonable" postcondition? 
This is not a fair question. The second equation rather hints at a "reasonable" invariant 
property: The condition x ~ D guarantees that the subtraction x - D can be performed, 
the second equation therefore amounts to the assertion: fJJ [x J must be periodic with 
period D. 
If we choose D lx for fJJ [x J, from the above requirement fJJ A 1 
~ = P2, we obtain D lx 
A 
x < D = Pl, d. h. P2 = (x = 0). Therefore D lx is an invariant only for this single -
not particularly interesting -
problem. In addition D I x is not guaranteed by the 
initializing assignment x: = A. 
Enough of this torture! If we choose 
then the second equation is likewise satisfied, we obtain 
P2 = x < D A D I(A - x) 
i.e. we face the problem (comp. mod, 1.11.2) 
1 nat x: x < D A D I<A - x) 
fJJ [x J = D I (A - x) is the precondition of the repetition, fJJ [A J = D I (A - A) = true is 
the precondition of the initialization, therefore we have a solution for the problem posed. 
This example should have demonstrated that finding an invariant condition is no easy 
matter. But verification is not meant to be a riddle. Gries writes in 1979 " ... it is difficult 
to prove an existing program correct. Instead, the correctness proof and the program 
should be developed hand-in-hand - with the former usually leading to the latter". 
Proceeding from the problem 
1 nat x: x < D A D I(A - x) 
we should therefore choose the postcondition 
P2 = x < D A D I(A - x) 
and split this into the form 
1 ~ A 
fJJ [xJ 
with 
~ = x ~ D and 
fJJ [xJ = D I(A - x) 

350 
5. Program Variables 
.'1' = x: = x - Dis defined under the condition !!11 and leaves iY [xJ invariant. Thus we 
obtain iY [xJ as precondition of 
while X ~ D do X : = X -
D od 
Prefixing x: = A yields the new precondition iY [AJ = D i(A -A) = true, that is 
true-+ wp(x: =A; while x ~ D do x: = x- Dod lx < D A D i(A - x)) 
Thus the body is developed; termination has still to be proved. 
But comparison with 1.11.4 (the derivation of a recursive version) shows that this meth-
od is technically identical and differs only in the notation used. 
Program verification can also be of importance as a supplement to the derivation of an 
iterative program by a sequence of program transformations. In this case there is little dif-
ficulty in finding suitable invariant properties, but it may be possible to abbreviate the 
proof in comparison to the derivation. It is then however more of a check than a proof, yet 
the check provides some security with respect to blunders when carrying out complicated 
transformations. 
5.5 Variables for Structured Objects 
Of course there are also variables for structured, composite objects. Note, that the selec-
tion of a component of the current value of a variable a, as in 
rest(a) or a[U] - more clearly written 
rest(val a) 
or (val a)[U) 
does not produce a variable and therefore can not occur on the left hand side of an assign-
ment. 
Variables for particular classes of composite objects are given special names. Thus a 
variable for a stack is called a pushdown, a variable for a queue a buffer, a variable for an 
array a display, a variable for a file a tape. 
In the transition from recursive functions based on such computation structures to 
procedural programs with variables for composite objects, e.g. for sequences, typically as-
signments like 
a:= append(val a, U) 
evolve. append is an example for those functions of the underlying computational struc-
ture which have composite objects as their results. If these functions are replaced by 
procedures with variable parameters for the respective compositions, e.g. variables for 
sequences, the following operations result: 

5.5 Variables for Structured Objects 
proc push = (var sequ J1 a, J1 U): a:= append(val a, U) 
proc pop = (var sequ J1 a: val a * 0): a : = rest(val a) 
funct last = (sequ JlA: A * 0) !I= top(A) 
proc trunc = (var sequ J1 a: val a * 0): a:= upper(vala) 
fu net first = ( sequ J1 A : A * 0) Jl: bottom (A) 
351 
Typically only push, pop and last are available for a pushdown, and push, trunc and 
first for buffers 28• In this way conglomerations consisting of variable parameters, 
procedures, and functions evolve which are usually called modules. The systematic 
introduction of modules related to abstract types can be found in Laut 198029• 
5.5.1 Selective Alteration 
In modules in particular those procedures are important which cause the replacement of a 
certain component of a composition, i.e. the selective alteration of the contents of the vari-
able. This is a rather complex matter, even for arrays and aggregates; it requires the forma-
tion of e.g. a new array by taking all the components from the contents of the variable with 
the exception of one, which is the new element, that is e.g. 
a : = (a [1], - 8, a[3]) 
If Index array J1 is defined in terms of the type FLEX of 3.3.2 then the operation all is 
available, in transition to a module selective alteration is defined by the procedure 
proc alter = (var Index flex J1 a, Index/, J1 X): a : = alt(a, I, X) 
The complication is thus completely transferred to the operation alt. 
A recursive (or after suitable transformation iterative) construction of the new array is 
even recommendable in the case of fixed index bounds if the index set is exceptionally 
large. The problem will often suggest using the computational structures FLEX or 
BIFLEX instead of arrays with computed index bounds. 
If alter is introduced as a primitive the complete construction of an array can also be 
realized by successively computing the result by alter on a "variable for the array". Note 
that also in this case the variable must be initialized, according to its nature, when intro-
duced - namely with init. 
Frequently the misleading notation 
,a [I] : = X" is used for alter(a, L X), i.e. for a:= alt(a, L X) 
such a notation should be reserved for arrays of variables (see 7.1). Dijkstra 1976 stressed 
the danger of self-deception involved in this notation 30 and pleaded for a notation 
28 Knuth writes a <= U for push (a, U), probably unaware of using Zuse's assignment symbol (revers-
ed), but accentuating the assignment character of this operation. 
29 There especially a simple criterion can be found guaranteeing that the alias ban is obeyed in the 
transition to modules. 
30 " ... in order to stress that such an operation affects the array ... as a whole". 

352 
5. Program Variables 
a: alter(/, X) or "less puritanical" 
a:[/] =X 
In full generality, selective alteration of composite objects is not always intricate: selec-
tive alteration of the top element of a pushdown or of the joint-component of a file is rela-
tively uncomplicated. 
5_5.2 Remarks on Input/Output 
It is here appropriate to say a word about input/output and peripheral store. Dealing with 
input/output does in principle not require additional language constructs, but simply a 
distinction of special variables for sequences, where in the case of pure output only the 
operation push is available and in the case of pure input only the operations trunc and first 
are available. In a mixed case input/output is a buffer 31 • Let 10 be a generic, global, 
standard identifier for such a buffer of objects of type 11; then print and read are schemes 
defined by 
proc print = (11X): push(IO, X) 
and 
proc read = (var 11 a): I a:= first(IO); trunc(IO) J 
Likewise, peripheral storage units are functionally nothing but special variables for 
files and arrays, characterized by relative ease or complication for particular access 
operations. 
Magnetic tape units are realizations for tapes for which procedures corresponding to 
a : = advance( a) and a:= open(a) (comp. 2.11.2) are usually available. The joint-compo-
nent has distinguished access: it is the only component that can be read, and it is also the 
only one that may be altered selectively, in the course of which for most magnetic tape 
units the r-component is cleared: 
a:= roll: (/of a, X, 0) 
Rotating devices, such as magnetic drum or disk units, are variables for flexible arrays 
of bounded length, i.e. displays; procedures corresponding to a : = ext( a, X) and a : = 
rem( a) together with a/teras described above are available, access (through alter and set) is 
unrestricted and direct ("random access"). 
This functional approach also allows a clear separation to be made between access 
tasks and formatting tasks, the latter being treated by typical symbol manipulation rou-
tines. 
A final remark: We must distinguish conceptually between variables for structured ob-
jects and composite objects whose elements are variables. The use of the latter is char-
acteristic of the level of machine-oriented systems programming. We will deal with this in 
7.1. 
31 At the time when core storage was still a rarity, buffering was often done by means of punch 
cards. Rutishauser based his revolving algorithm for matrix inversion on this medium. 

Addendum to Chapter 5. Notations 
353 
Addendum to Chapter 5. Notations 
Those programming languages, which are in principle restricted to program variables only, 
must at least differentiate in a procedure call between genuine variable parameters ("call 
by reference") and constant variables which replace objects ("call by value"). In PASCAL 
("parametric variables", "parametric constants", Wirth 1973) this is done by the notation 
procedure multiply(x, y: integer; var z: integer); 
which permits an applicative interpretation of parametric constants. 
A third type of "parameter passing", "call by name", which means the textual passing 
("call by expression", Strachey, Wilkes 1961), comp. 1.7.3, was provided in ALGOL 60 
and had to serve as a substitute for procedure-parameters (in PASCAL "parametric proce-
dures"). Unfortunately the "call by reference" which was present in FORTRAN is missing 
in ALGOL 60. There was a "call by value result" in ALGOL W. 
Frequently abbreviating notations are permitted for declarations of several variables of 
the same mode, such as in ALGOL 60 int a, b, c. In PASCAL such a declaration takes 
place in the form var a: Tor var a, b, c: Twhere T specifies the mode of the values. 
In many programming languages it is not possible to combine the declaration with an 
initial assignment (ALGOL 60, PASCAL). In MESA, which otherwise follows along the 
PASCAL line, there is a distinction made between the initialized variable declaration 
a: T +- 4 and the constant declaration a: T = C. The latter is what we have described as 
an object declaration. 
There is an abundance of notations for (counted and uncounted) repetitions. We will 
deal with them in the addendum to Chap. 6. 
Program variables necessarily violate Quine's (1960) request for "referential trans-
parency": that a "variable" ( = a designation) has the same value at every position in the 
text. Rutishauser took into account this natural requirement (in the case of range-of-values 
recursion); for example in 
h:=h+1 
he imagined "invisible" (bracketed) state indexes 
h(i+l): = h(i) + 1 
The same method is used in LUCID (the only difference being the notation) to connect the 
procedural notation with the transparence of applicative semantics: 
first a = 1 
next a= a+ 
The symbol => was used by Zuse in the "Plankalkiil" (1945) in the (proper) direction as 
an assignment symbol 
a+3=>h 

354 
5. Program Variables 
ALGOL introduced : =, 
h:=a+3 
which is also used in PASCAL, CLU and many other languages. An arrow is also 
frequently used (ALPHARD, MESA): 
h<-a+3 
Zuse's order is "more natural"; this is obvious in machine-oriented ameliorations as well 
as in linked assignments (see 7.3.1). 
In ALGOL 68 ("strict language") there is a variable declaration of the form ref int a = 
loc int. It can be abbreviated to int a, which is often confusing. 
Occasionally program variable declarations are omitted and are assumed "at their first 
occurrence of an assignment to the freely chosen denotation". This can be accepted in the 
case of untyped variables, but it is barbaric to link letters with certain modes. 
For input/ output, in most programming languages mode indications are suppressed. In 
FORTRAN, the generic nature of 10 is reflected by the use of a corresponding format 
statement. 

II 
2 
III 
au/ + bu 1 + c 
v1 = 
du, + e 
to A.2i 
(m + 2i)0 
to C 
A control flow diagram (von Neumann) 
Chapter 6. Control Elements 
"The absence of goto's does not always make a 
program better." 
Geschke et al. 1977 
The introduction of variables in the last chapter enables us now to expose the execution of 
certain systems. In the present chapter tools will be introduced for the explicit description 
of executions. In this we do not need to restrict ourselves to sequential executions. Petri 
nets are used to illustrate coordinated executions. 
6.1 Deparameterization and Formal Treatment of Repetition 
Repetitions were dealt with informally in 5.1 in order to motivate some important char-
acteristics of the program variables to be introduced in 5.2. A formal treatment of repeti-
tion on the basis of 5.2 must now be undertaken. We first discuss deparameterization of 
disentangled routines and its consequences for control of executions. 
6.1.1 Deparameterization 
6.1.1.1 On present day machines as well as on the stack machine (comp. 1.7.4), a recursive 
routine can be executed much more efficiently if the parameter stack collapses to a para-
meter register. On the level of program variables, this has the consequence that we intro-
duce as a counterpart to a recursive routine a nullary procedure with suppressed variable 
parameters. For the formal derivation of such a procedure, it will tum out as a necessary 
and sufficient condition that the recursive routine is in disentangled form. To illustrate the 
steps of this derivation we use the simple example of 4.4.1 

356 
funct pbw = (int A, nat E) int: 
if E = 0 then A 
else sq(pbw(A, E -
1)) fi 
6. Control Elements 
Here A is a fixed parameter which needs no treatment. A sufficiently detailed form of pbw 
is 
funct pbw = (int A, nat E) int: 
If E = 0 then A 
else nat H = E -
1; sq(pbw(A, H)) fi 
(a) Firstly we define a new procedure pbw*, based on pbw 
proc pbw* = (int A, var nat e) int: pbw(A, e) 
(b) Unfolding the call of pbw results in 
proc pbw* = (int A, var nat e) int: 
if e = 0 then A 
else nat H = e -
1; sq(pbw(A, H)) fi 
(c) In the body of the procedure, the auxiliary identifier H can be replaced by the program 
variable e itself, according to rule (A1) of 5.2.2. The conditions listed there show im-
mediately that this is only possible because e does not occur after the declaration of H, 
in other words because the routine pbw is disentangled. We thus obtain 
proc pbw* = (int A, var nat e) int: 
if e = 0 then A 
else e: = e -
1; sq(pbw(A, e)) fl 
(d) Folding with the procedure pbw* as defined in step (a) now results in the recursive 
version 
proc pbw* = (int A, var nat e) int: 
if e = 0 then A 
else e: = e -
1; sq(pbw*(A, e)) fi 
(e) As a final step, after subordinating pbw* to pbw, its parameters can be suppressed 
which leads to 
funct pbw = (int A, nat E) int: 
J var nat e : = E; pbw* 
where proc pbw* = int: 
if e = 0 then A 
else e := e- 1; sq(pbw*) fi J 

6.1 Deparameterization and Formal Treatment of Repetition 
357 
For the success of the method step (c) is vital. That this step is not always possible is 
demonstrated by the simple counterexample 
functjac = (nat N) nat: 
if N * 0 then N x jac(N- 1) 
else 1 
fi 
Here the critical intermediate version reads 
proc fac* = (var nat n) nat: 
if n * 0 then nat H = n - 1; n x fac(H) 
else 1 
fi 
The auxiliary identifier H cannot be replaced by n. 
6.1.1.2 Now the essential purpose of the method of disentangling in 4.4.2 becomes 
obvious: 
There we have shown that every recursive routine (and also every system) can be dis-
entangled, although this often requires the explicit introduction of stacks. Thus in general 
the following important result holds: 
Every (recursive) routine can be deparameterized; the same holds for systems of rou-
tines. 
The method described above frequently allows one to replace not only the parameters, 
but also the results of a routine by program variables. In such a case one obtains pure 
procedures. We consider the following routine (comp. 1.5) as a somewhat more ambitious 
example: 
funct morris = (int X, int Y) int: 
if X= Ythen succ Y 
else morris(X, morris(pred X, succ Y)) fi 
A disentangled form can here be obtained by function inversion with respect to the first 
parameter, it reads (in accordance with 4.4.2.3) 
funct morris' = (lnt X, int Y) (int, int): 
if X = Y then (X, succ Y) 
else (int X1, int Y1) = (pred X, succ Y); 
(int X 2 , int Y2) "" morris' (X1, Y1); 
(int X 3 , int Y3 ) = ( succ X2 , Y2); 
fl 
Note that with the use of the inverse function succ we have X 3 = succ X2 = succ 
X1 = succ pred X = X. (X1 equals X2 , since the first result of morris' is the value of its 
first parameter; this is essential in the construction method of 4.4.2). 
Now by slightly varying step (a) of the method above, we define the new procedure 
proc morris* = (var int x, var int y): (x, y) : = morris'(x, y) 

358 
6. Control Elements 
All the other steps are unchanged. First of all, unfolding together with the rule (A4) 
from 5.2.3 yields 
proc morris* = (var int x, var int y): 
if x = y then (x, y) : = (x, succ y) 
else (x, y) : = r (int XI' int Yl) = (pred X, succ y); 
(int X 2 , int Y2) = morris'(X11 Y1); 
int X 3 = succ X 2; 
morris'(X3, Y0 
J fi 
The object identifiers Xi, Yi can now be successively replaced by the program variables x, y: 
proc morris* = (var int x, var int y): 
if x = y then (x, y) : = (x, succ y) 
else (x, y) : = (pred x, succ y); 
(x, y) : = morris'(x, y); 
x: = succ x; 
(x, y): = morris'(x, y) 
fi 
Folding with the original version of morris* and suppression of the parameters x, y - to-
gether with minor simplifications - produces finally 
funct morris = (int A, int B) int: 
r (var int X, var int y) : = (A, B); morris*; val y 
where proc morris* = : 
if x = y then y : = succ y 
else (x, y) : = (pred x, succ y); 
morris*; 
x: = succ x; 
morris* 
fi J 
6.1.1.3 The example pbw suggests to consider for disentangled linear recursion a 
transformation scheme like 
functP = (I..M) 1..: 
if Pi [MJ then cp(P(.i' [MJ)) 
else .ff [MJ 
fi 
functP = (J..M) A.: 
fvar J..m := M; P**; m where 
proc P** =: if Pi [ m J then m : = 1 [ m J ; 
P**; 
m:= cp(m) 
else m: = Jf [m J fi J 

6.1 Deparameterization and Formal Treatment of Repetition 
359 
Likewise, for the special non-linear recursion scheme to which morris' belongs the follow-
ing transformation holds: 
functN =(AM) A: 
if~ (MJ then cp(N(IJI(N(f (M])))) 
else Jf' (MJ 
fi 
funct N = (AM) A: 
I var Am : = M; N**; m where 
procN** =:if~ (mJ thenm := f(mJ; 
N**; 
m:= IJI(m); 
N**; 
m: = cp(m) 
else m: = £' (m J fi J 
This scheme covers also the scheme F of 4.3.3 ("91-function"), IJI and cp being the identity. 
6.1.1.4 The disentangled form was developed for some non-linear recursion schemes 
in 4.4. To complete this, the corresponding deparameterized version will now be given. 
In the cascade-type recursive routine Fin 4.4.2 
funct F = (Ax) p: 
if ~ (x] then cp(F(.:t; (x]), F(f2 (x]), ~ (x]) 
else £' (x J 
fl 
the variables v, sv and z can be introduced after disentanglement. These carry the corre-
sponding intermediate results and are suppressed as parameters in F*. This yields 1 
funct F = (Ax) p: 
I (var A v, var stack p sv, p z) : = (x, empty,(')); 
F*; z where 
procF* =: 
if~ (vJ then v := f 1 (v]; 
F*; 
v:= ~(vJ; 
(v, sv): = (f2 (vJ, sv & z); 
F*; 
v := ..i2 (vJ; 
(sv, z): = (rest sv, cp(top sv, z, ~ (v])) 
else z: = £' (vJ 
fi J 
N is an insignificant object, comp. 5.3.4. The use of an uninitialized variable cannot be avoided 
here as z has pure result character for F*. 

360 
Analogously for the nested recursive routine G in 4.4.2 
funct G = (I.. x) p: 
if flJ [xJ then cp(G(Iji(G(.tj [xJ), .::f2 [xJ)), rff [xJ) 
else .1f Cx J 
fi 
we have a deparameterized form with the variables v, sv and z: 
funct G = (I.. x) p: 
~ (Var /.. V, Var StaCk /.. SV, Var p Z) : = (X, empty, N); 
G*; z where 
proc G* =: 
if flJ [vJ then v: = .::f1 [vJ; 
G*; 
v : = ;ft [v J; 
(v, sv) := (lji(Z, .Jf2 [vJ), sv& v); 
G*; 
6. Control Elements 
( v, sv, z) : = (top sv, rest sv, cp(z, rff [top sv J)) 
else z : = .1f [v J 
fi J 
Finally a special case ofF in 4.4.3 was transformed to nested recursion (by re-bracket-
ing according to the associativity of a) and then disentangled. From the final version there 
we obtain a formulation which clearly shows what has been gained in efficiency: 
funct F = (I.. x) p: 
r (var 1.. v, var stack 1.. sv, p z) : = (x, empty, e); 
F*; zwhere 
procF* =: 
if flJ [vJ then (v, sv, z) : = (.::f2 [vJ, sv & v, Iff [vJ a z); 
F*; 
(v, sv): = (.::f1 [top svJ, rest sv); 
F*· ' 
else z : = .1f [v J a z 
fi J 
The result of this deparameterization in fact reveals the control structure of the algorithm. 
6.1.2 Semantics of Repetition 
Deparameterization is especially appropriate in the case of repetitive routines and systems. 
For instance, from the embedding (comp. 1.6.1) 
functjac = (natN) nat: G(N,1), 
funct G = (nat N, nat M) nat: 
if N * 0 then G(N - 1, M X N) 
else M 
fi 
we obtain after appropriate treatment of G the embedding 

6.1 Deparameterization and Formal Treatment of Repetition 
funct fac "' (nat N) nat: 
j (var nat n, var nat m) : = (N, 1); G* where 
proc G* "' nat: 
if n * 0 then (n, m) : = (n - 1, m 
else m 
x n); G* 
361 
fi J 
For repetitive systems the method of deparameterization leads to a general transforma-
tion rule which we will deal with in 6.2. For a directly recursive repetitive routine we have 
the transformation rule 
funct R "' (l M) p: 
if~ [MJ then R(f [MJ) 
else £[MJ 
fi 
funct R "' (l M) p: 
j var l m : = M; R* where 
procR*"' p: if~ [mJ thenm:= f[mJ;R* 
else £[mJ 
fi J 
Exercise 1: In 4.2.1, 4.2.2 and 4.2.3 transformations were given which lead from linear recursion to 
repetitive recursion. Combine these schemes, respectively, to the above (which depara-
meterizes repetitive routines) so that schemes result which directly transform linear recur-
sive routines into deparameterized procedures. 
The procedure R * in the above transformation can be changed into a pure procedure 
by "extracting" the computation of£ [ m J which occurs in the termination branch, that is 
by inserting this computation in the dominant routine after the call of the procedure. We 
have the following variant of the transformation 
funct R "' (l M) p: 
if ~ [MJ then R(f [MJ) 
else £[MJ 
fi 
funct R "'(lM) p: 
j var l m: = M; R**; £ [m J where 
proc R** "': 
if ~ [ m J then m : = f [ m J ; R ** 
else skip 
fi J 
This form means exactly that the (collective) assignment m : = f [ m J is repeated as 
long as ~ [ m J holds. As soon as ~ [ m J is violated the recursion stops. 
Now we can define the repetition, introduced informally in 5.1, as a notational variant 
of the above recursive procedure: 

362 
6. Control Elements 
The pure procedure (with suppressed variable parameter) 
proc R =: while ~ do !I' od 
where !/' is an arbitrary statement is recursively defined by 
proc R =: if ~ then !/'; R else skip fl 
Put another way, 
while ~ do !I' od 
is the same as 
if ~ then !/'; while ~ do !I' od 
else skip 
fi 
If we compare the transformation with the repetitive versions in 5.1 we find complete con-
formity. 
The counted repetition which corresponds directly to the range-of-values recursion can 
be reduced to a "normal" repetition or to a recursive version. For more details see 6.4.4. 
Exercise 2: Transform the routines fac, gcd and mod from 1.4.1 into the form of repetitions by 
applying suitable transformations. 
6.1.3 Analytical Treatment of the Protocol Stack 
We obtained the following form in 6.1.1 as a prototype of the result of disentangling and 
deparameterizing of different types of recursion 
proeM=: 
if ~ then 91; M; .92; M; .93 
else !1'4 
fi 
This scheme can be treated by "arithmetization of the control flow" exactly as was 
done in 4.3.2. We will consider here another method which looks slightly different and is 
more oriented toward the concept of a protocol stack - the analytical treatment of the 
control flow by a stack of binary marks. The basic connection with the method previously 
described is obvious, if we note that a stack of binary marks corresponds to a binary num-
ber, and it is certainly easy to compare the stack operations used here with the arithmetical 
operations used in 4.3.2, especially with the auxiliary function y. 
Therefore a stack of binary marks is introduced, where 
mode mark = atomic{.t 2.} 

6.1 Deparameterization and Formal Treatment of Repetition 
363 
and M is embedded into the procedure M* with a mark stack as an additional parameter: 
proc M 
=: M* (empty), 
proc M* = (stack markp): 
if 1!1 then .91; M*(p & 1.); .'1'2; M*(p & 2. ); .93 
else .94 
fi 
When a recursive call is terminated we can tell by the stack p where work will be con-
tinued: If top p = 1., then work is continued with .'1'2, if top p = 2., work is continued 
with .9'3· 
With the help of the information contained in the stack p the recursive calls can be 
transferred to the branches of a (here binary) branching and thus a decisive step has been 
taken in the direction of a repetitive form. 
However the above formulation "work is continued with ... " already shows that addi-
tional recursive calls are necessary at those points where the original routine Mterminated. 
We must therefore differentiate between two cases of recursive calls: the "genuine" 
recursions which correspond to calls which were already there, and "fake" recursions 
which prompt the work to be continued at the points where the recursion previously 
terminated. We can easily distinguish between these two cases by using different routines 
rec, cont. Altogether we obtain the repetitive system 
proc M =: rec(empty), 
proc rec ... (stack markp): 
if 1!1 then Yj; rec(p & 1.) 
else .'14; cont(p) 
fi, 
proc cant= (stack markp): 
if p =1= empty then if top p = 1. then .'1'2; rec(rest p & 2.) 
D top p = 2. then .9'3; cont(rest p) 
fi 
else skip 
fi 
The recursion ends precisely when we reach a case of termination at which the stack p is 
empty. This case in cant is found in the branch with skip. 
It is instructive to consider the special case of linear recursion once more. The depara-
meterized form corresponds to the scheme 
proc L =: 
if 1!1 then .91; L; .'1'2 
else .93 
fi 
A transformation of the above type thus yields 
proc L 
"': rec(empty), 
proc rec =(stack markp): 
if 1!1 then .9j; rec(p & 1.) 
else .9'3; cont(p) 
fi, 
proc cant = (stack markp): 
if p =1= empty then if top p = 1. then .'1'2; cont(rest p) fi 
else skip 
fi 

364 
6. Control Elements 
Obviously the stack p contains only 1. 's (that is, stroke numbers): it would be sufficient 
to hold their number in a counter. The sole purpose of the counter is then to cause 
termination as soon as it is 0. (The parameter stack can also carry out this task.) In addi-
tion it can be seen that cant no longer calls rec. Thus there results 
proc L 
=: rec(O), 
proc rec = (natp): 
if !11 then .9'1; rec(p + 1) 
else .9'3; cont(p) 
fi, 
proc cant= (natp): 
if p =1= 0 then Y2; cont(p -
1) 
else skip 
fi 
This form corresponds in general to those variants of functional inversion which use an 
"accompanying count" (comp. the example cos in 4.2.3). 
The method of introducing parameter and protocol stacks shown in the last two sec-
tions is a systematic and formal application of a specific method which can also be easily 
extended to the case of three or more calls in the body of the routine (with corresponding 
ternary etc. marks). This method can be supplementary to the usual techniques used in 
compiler construction for implementing recursive routines, which on the whole amount to 
the implementation of a stack machine. It is related to a method which Scholl 1976 
proposed as "Traversing the tree of calls". 
6.2 Jumps 
6.2.1 Simple Call as a Basic Control Element 
We have so far "removed" recursion in certain direct-recursive and nested recursive rou-
tines, i.e. we have reduced them to repetitions. We now consider mutually recursive systems. 
As an example we take a routine positive based on a system pas, neg (comp. 1.4.1( e)), which 
reduces a given stack of signs to a single sign, with mode sign = atomic { + , - }: 
funct positive = (stack sign A) sign: 
I pos(A) where 
funct pos = (stack sign A) sign: 
if A 
=1= empty then if top(A) 
D top(A) = 
else + 
funct neg = (stack sign A) sign: 
if A 
=1= empty then if top (A) 
D top(A) = 
else -
+ then pos(rest(A)) 
- then neg(rest(A)) fi 
fi' 
+ then neg(rest(A)) 
- then pos(rest(A)) fi 
fi J 

6.2 Jumps 
365 
By inspecting this example we see that the calls of pos and neg are the last actions dur-
ing the execution of the corresponding calling routine. Thus we have here the special case 
which has already occurred in repetitive routines and in which a return to the calling rou-
tine is no longer required, so that the actions of the return organization can be saved 2• 
This special case of a call of a routine q which is the last action during the execution of 
a routine p has been called a simple call (comp. 1.4.3). 
Such simple calls are preliminary forms of jumps in the usual sense (comp. 1.7.4). 
Their importance lies in the fact that for the continuation of the execution the following 
holds: 
"Without cancelling any of the return duties outstanding, continue at the beginning of 
(in short: goto) the called routine" 3. 
A call of the routine itself can also be treated in the same way 4 and leads to an iteration 
in the manner shown in Chap. 5. This is already a special case in which a simple call 
renders possible a complete removal of recursion by means of control elements - iteration 
or general jump. 
In our example above which is a reperirit·e 5_\'51em, i.e. a mutuall) n:cur'>i\t: ')'tem in 
which only simple calls occur, recursion removal takes place in several elementary steps. 
As in 6.1, we first introduce variables to carry out along with the call the operations to 
be performed on the parameters. By introducing suitable new procedures pos*, neg*, we 
obtain first 
funct positive = (stack sign A) sign: 
I var stack sign v: = A; pos*(v) where 
proc pos* = (var stack sign v) sign: pos(v), 
proc neg* = (var stack sign v) sign: neg(v), 
functpos =(stack sign A) sign: 
if A * empty then if top(A) = 
0 top(A) 
else+ 
funct neg = (stack sign A) sign: 
if A *empty then if top(A) 
0 top(A) 
else-
+ then pos(rest(A)) 
then neg(rest(A )) fi 
+ then neg(rest(A )) 
then pos(rest(A)) fi 
fi' 
fi J 
The method of 6.1 can now be carried through for both pos* and neg*. For pos*, say, 
the result is (using the abbreviating stack operations pop and last) 
proc pos* = (var stack sign v) sign: 
if v * empty then if last v = + then pop v; pos*(v) 
0 last v = 
then pop v; neg*(v) fi 
else + 
fi 
2 Knuth's (1974) remark is relevant: "Rule number one for simplifying procedure calls is: If the last 
action of procedure p before it returns is to call procedure q, simply goto the beginning of proce-
dure q instead." 
3 This rule was introduced by Gill in 1965, Haskell pointed out its importance 1975 independent of 
Knuth. The BLISS compiler (Wulf et al. 1973) is capable of detecting this simplification. 
4 Knuth (1974, p. 281): "When q = p, the argument is perhaps a bit subtle, but it's all right." 

366 
6. Control Elements 
Next, the parameter v can be suppressed both in pos* and neg*. Altogether, we obtain 
funct positive = (stack sign A) sign: 
I var stack sign v : = A; pos* where 
proc pos* = sign: 
if v =1= empty then if last v = + then pop v; pos* 
0 last v = - then pop v; neg* fi 
else + 
fi, 
proc neg* = sign: 
if v * empty then if last v = 
0 last v = 
else-
+ then pop v; neg* 
-
then pop v; pos* fi 
fi J 
For clarity we will also mark the simple calls informally by inserting (go to) , and the 
"usual" points of return (i.e. the points of termination) by inserting (return) : 5 
functpositive =(stack signA) sign: 
I var stack sign v: = A; ( goto) pos* where 
procpos* =sign: 
if v * empty then if last v = + then pop v; 
0 last v = - then pop v; 
else + (return) 
proc neg* =sign: 
if v * empty then if last v = + then pop v; 
0 last v = 
-
then pop v; 
else -
(return) 
( goto) pos* 
(go to) neg* fi 
(goto) neg* 
(go to) pos* fi 
fi' 
fi J 
In general (return) indicates the termination of the call which is at present actual 
("the last pending"), that is, in our example, the termination of the call pos* in the second 
line - and thus in fact the termination of the execution of positive. 
Altogether, the transformation process presented here can be abstracted from and can 
be summarized in a - very informal - transformation scheme (the proof of which follows 
exactly the steps above): 
funct F = (l. M) p: 
IF;(':# (MJ) where 
funct F1 = (l. M) p: /VVVVVVvVVV', 
funct F; = (l. M) p: 
if 
0 &; then Yfj [ MJ 
5 ( goto ) and ( return) serve only as commentaries, i.e. they have no semantic meaning 
whatsoever. 

6.2 Jumps 
fi' 
funct F" = (l M) p: 
V'ANVVVVVvV J 
funct F = (l M) p: 
I var l m : = rg [MJ; (goto) F; where 
proc F1 = p: vvvvvvvvvvv- , 
procF; = p: 
if 
D .9'; then £; [ m J (return) 
fi' 
proc Fn = p: VVVVVV'JV'JVVV 
367 
J 
The informal notation of this scheme is to indicate that the repetitive system is made up 
of n routines, all of which have essentially the same structure as the given representative F;. 
In particular, termination cases and (directly or indirectly recursive) calls may occur arbi-
trarily mixed in the guarded branching. 
" ... there is a considerable similarity between labels 
and the identifiers of parameterless ... procedures" 
Landin 1965 
6.2.2 Introduction of Jumps 
Genuine jumps (with labels as destinations) result from (conditional and unconditional) 
simple calls if we reduce the headings of the routines to labels. The ( goto) , inserted in-
formally, must now be taken seriously and must be interpreted as jump goto to such a 
label resulting from "degeneration" 6• Simultaneously the (return) inserted informally 
at all termination points must also be taken seriously and must be understood as an exit 
return from the body of the routine. Thus it has likewise the meaning of a jump, 
specifically of a jump (return jump) 7 behind the calling point of that routine which directly 
encloses the system; in addition, however, a result can be returned. 
6 This applies particularly to the "initial call" of the system. 
7 The idea of a return jump from a procedure can already be found in ALGOL 58. 

368 
6. Control Elements 
This introduction of jumps, however, is more than a mere notational formality, since it 
corresponds to a change in the semantical interpretation. Originally a call of a (parameter-
free) routine is defined to be a textual replacement, which is illustrated by the insertion ar-
row (comp. 2.9.3): 
procpos*=~ 
... ;pos*~ 
Now, the text form is fixed; one "jumps" with the finger which accompanies the course of 
execution ("jump arrow"): 
gotopos* 
I 
The reversal in the direction of the arrow is characteristic of the conceptual change. 
In the example of 6.2.1 (return) referred originally to the termination of pos*. How-
ever, as pos* disappears as a routine, return refers to the next enclosing routine, that is to 
positive. For our example with this final transition which brings about complete sequen-
tialization we obtain 
funct positive "' (stack sign A) sign: 
fvar stack sign v: =A; gotopos*; 
pos*: if v * empty then if last v = + then pop v; goto pos* 
D last v = -
then pop v; goto neg* fi 
else + return 
fi; 
neg*: if v *empty then if last v = + then pop v; goto neg* 
D las tv = - then pop v; goto pos* fi 
else -
return 
fi J 
Thus we have here a form in which even the simple call within a routine, the repetition, 
is realized iteratively by a jump. We will look at the implementation of the repetition by 
jumps more closely in 6.4. It should be noted here that it can be developed conceptually 
from simple calls. 
Let us summarize: a system of mutually recursive routines of such a special form, in 
which only simple calls occur, can (by means of a system of correspondingly special rou-
tines) be transformed directly into an iterative form with jumps ("interlinked form"): 
funct F "' (I.. M) p: 
l Fj( '1 [MJ) where 
funct F1 .. (I.. M) p: rvvvvvvvvvv , 
funct F; .. (I.. M) p: 
if 

6.2 Jumps 
U !1'; then £! [MJ 
U Ili; then F1.(1;[M]) 
I 
fi, 
funct F" = (A. M) p: vvvvvvvvvvv J 
funct F = (A. M) p: 
I var A. m : = 
'"# [MJ; goto Fj; 
Fl : NVVVVVVVVV ; 
U !J'i then £! [ m J return 
U !!!;then m := .t;[mJ; gotoF1 I 
fi; 
J 
Exercise 1: Let the following routine be given (with mode brack = atomic{[,]}) 
funct k = (sequ brack a) boo!: 
I kk(a, 0) where 
funct kk .. (sequ brack a, nat i) boo!: 
if a = empty then i = 0 
0 a* empty 
then if top(a) = [ 
then kk(rest(a), i + 1) 
0 top(a) = I 
then if i > 0 then kk (rest (a), i -
1) 
0 i = 0 then false 
fi fi fi J 
369 
which represents a recognition algorithm for correct bracket sequences. Proceeding from 
this formulation, carry out the following transformations in steps: 
(a) Deparameterization of the recursive calls and marking of the simple calls and the 
termination cases. 
(b) Transition to an iterative form with jumps. 
By means of the transformation process carried out during the consideration of simple 
calls, the semantics of jumps can be based on the semantics of routines in a natural way. 
They evolve as simple calls, i.e. as special calls 8 which (contrary to normal calls) do not 
require a return organization. The question as to whether jumps are conceptually simpler 
than routines is left open9• However the following warning is appropriate: 
8 Knuth 1974: " ... This shows that procedure calls include goto-statements as a special case." 
9 Knuth 1974: "It cannot be argued that procedures are conceptually simpler than goto's, although 
some people have made such a claim." Among the first was van Wijngaarden 1964. 

370 
6. Control Elements 
Jumps in a technical sense (where only the new position of the "instruction counter" is 
of importance) can only be obtained if exclusively simple calls occur in the whole 
program. Otherwise one has to conform to an already existing return duty. 
In any case, a semantics results automatically for the jumps introduced in this conceptual 
way. The jumps introduced by transformations are natural, they mirror the situations of a 
repetitive system. 
However there are restrictions on the use of jumps: there are the same scope rules for 
labelled segments as there are for the routines from which they evolve; the scope of a label 
remains the same as for the original identifier of a routine. 
We hope this exterminates the worst jump-monsters, which caused the -
not un-
justified -
"goto considered harmful" crusade. 
In any case the transformation process makes the operative semantics of jumps more 
transparent than the semantics of Kandzia, Langmaack (1973) or Strachey, Wadsworth 
(1974), which treat jumps in great generality. Clint, Hoare (1971), too, have given a 
deductive semantics of jumps - as an extension to Hoare's axiomatic basis for correctness 
proofs for programs - which leads to an invariant technique, similar to the usual loop 
invariant technique. 
A more general kind of routine 10 permits apart from simple calls (characterized by 
G§!jjJ ) also calls which lead from anywhere within a routine F; to another routine Fj of 
a system. Control is passed by such a swap - written as swap to - according to the fol-
lowing rules: The current position in F; is "recorded" just as in an ordinary return jump, 
and if a subsequent swap to F; occurs (from an arbitrary F1), the execution ofF; is con-
tinued at the "recorded point". If a swap to Fj from F; was the last action when executing 
F; (special case of a simple call) control returns to the beginning ofF; after the next swap 
to F;. A system of routines which permits such swaps is called a system of coroutines (Con-
way 1963). 
Existing coroutine mechanisms show remarkable differences in details. Coroutines 
entangle jumps and variables in a complicated way, introducing a multi-dimensional 
control state. In many cases, they obscure the program structure. In particular, they seem-
ingly do not evolve in a natural way from applicative constructs. 
Coroutines are especially not required in the formulation of problems with "quasi-
parallel processing" (Knuth 1973, p. 293), such as merging problems. For example, the 
routine merge of 1.9.1 can be brought into repetitive form by using re-bracketing: 
funct merge = (sequ 1. a, sequ 1. b) sequ x: 
I merger(a, b, 0) where 
funct merger = (sequ 1. a, sequ 1. b, sequ 1. r) sequ x: 
if a = 0 then b & r 
U b = 0 then a & r 
elsf bottom(a) ~ bottom(b) then 
merger(upper(a), b, append(r, bottom(a))) 
U bottom(a) ~ bottom(b) then 
merger(a, upper(b), append(r, bottom(b))) fi J 
10 A further generalization lies in the class concept of SIMULA 67, see Dahl, Hoare 1972. 

6.3 The General do-ad-Construction 
371 
Exercise 2: Show that merge(a, b) = merge(b, a). 
Finally it should be noted that elimination of jumps by textual insertion corresponds to 
elimination of routine calls by textual replacement. 
6.3 The General do-ad-Construction 
Apart from the repetition using while there is a more general notation corresponding to 
parameter-free repetitive routines with several directly recursive calls that was introduced 
by Dijkstra in connection with guarded commands. 
For a repetitive routine with parameters we transform (as in 6.2.1 by introducing an 
auxiliary procedure) to the deparameterized form with simple calls. Then the body of the 
(nullary) auxiliary procedure is parenthesized with do-od and the simple calls of the 
procedure itself are deleted. In addition, we replace the indication of the return by leave, 
where leave (just as return) means an unlabelled jump which leads to the point after the 
do-od construction. Finally the body of the auxiliary procedure -
thus modified -
is 
inserted at the appropriate calling point. 
For the examplejac (comp. 4.2.1 and 5.1.1) we first have as in 6.1.2 
funct G = (nat N, nat M) nat: 
I (var nat n, var nat m) : = (N, M); ( goto) G* where 
proc G* = nat: 
if n * 0 then (n, m) := (n- 1, m x n); (goto) 
U n = 0 then m 
(return) 
G* 
fi J 
(For clarity the simple calls and return points are again informally indicated.) Thus we ob-
tain 
functjac = (nat N) nat: 
I (var nat n, var nat m) : = (N, 1); 
do if n * 0 then (n, m) : = (n - 1, m 
U n = 0 then m leave 
x n) 
fi od J 
It is obvious that the do-od construction here is simply a notational abbreviation for 
the definition and the (single) call of the parameter-free auxiliary routine G*. 
Exercise 1: Transform the routine given in exercise 6.2.2-1 into an iterative form with the do-od con-
struction. 
In general the do-od loop is defined by the following transformation: 

372 
funct F = p: 
if 
D 1!1 then Y'; ( goto) F 
D 'C then r! (return) 
fi 
funct F = p: 
do if 
D 1!1 then Y' 
D <e then r! leave 
fi 
od 
6. Control Elements 
Here Y' stands for a statement and t for an expression with side-effect. All the properties 
of the recursive form of the routine Fare transferred directly to the do-od construction by 
virtue of this definition: Both forms are undefined if at some time during the repetition all 
guards block. Likewise termination must be equally guaranteed in both forms. 
For example, the routine merge from 6.2.2 develops into 
funct merge = ( sequ "A, sequ "B) sequ ": 
I (var sequ "a, var sequ "b, var sequ "r) : = (A, B, 0); 
do if a = 0 then b & r leave 
D b = 0 then a & r leave 
elsf bottom(a) ~ bottom(b) then 
(a, r) : = (upper(a), append(r, bottom(a))) 
D bottom(a) ~ bottom(b) then 
(b, r) : = (upper(b), append(r, bottom(b))) fi od J 
Note the similarity of the do-od construction to the jump implementation 
funct F = p: 
IF: if 
D 1!1 then Y'; goto F 
D <e then t return 
fi 
J 
The classical jump implementation of loops evolves in this way: 

6.4 Loops 
373 
Of course, variants of the recursive formulation of the routine F correspond to variants 
of the do-od construction; thus we obtain for pure procedures - somewhat generalized -
the following, where Y; .r and o/i are statements: 
proc F =: 
1 Y; if 
D fJ then .9; ( goto) F 
D ?! then o/i 
(return) 
fi 
procF =: 
do Y; if 
D fJ then .r 
D re then o/i leave 
J 
fi 
od 
Note again the similarity with the jump implementation 
proc F =: 
IF: Y; if 
D fJ then .r; goto F 
D re then o/i return 
fi 
J 
6.4 Loops 
We use the name loop for the special case - mentioned in 6.2.2 - of a routine with just 
one simple call of the same routine. In the sequel, different forms of loops are considered 
and their correspondence to constructs dealt with so far is discussed. 
In addition to the general do-od construction introduced in the last section there are 
also special notations for loops. The meaning of these notations can be explained by trans-
formations into suitable do-od constructions. 
6.4.1 Rejecting and Non-Rejecting Repetition 
The repetition statement present in most higher programming languages has the form of 
the rejecting repetition: 

374 
6. Control Elements 
while !!A do Y od 
L 
do if !!d then Y else leave fi od 
Exercise 1: Show that this definition is consistent with the introduction of the while construction in 
6.1.2. 
The non-rejecting repetition is defined by 
do :T until ce od 
do .r; if ce then leave else skip fi od 
It is appropriate to generalize these constructions a little more and to consider "until 
!!A" and "while -, !!J" as being equivalent generally. Thus two more kinds of loops are 
added, namely 
until !!A do Y od 
L 
do if !!A then leave else Y fi od 
and 
do :T while ce od 
do :T; if ce then skip else leave fi od 
The "(n + t) loop" (Dijkstra) as a mixed form of repetition occurs in numerous prob-
lems. It has the general form 
do Y; 
if !!J then leave else skip fi; 
:T 
od 
or equivalently 
do Y; if !!J then leave else :T fi od 
If one is willing to duplicate program text, then the (n + t) loop can be changed into 
the form of a rejecting repetition. The following transformations are possible, as can easily 
be shown by reduction to the recursive routine. 

6.4 Loops 
375 
do Y; if flJ then leave else skip fi; .'T od 
--------------r-------------
II 
Y; do if flJ then leave else sktp fi; .'T; Y; od 
Y; do if !Jd then leave else .'T; Y fi od 
Y; until !Jd do .'T; Y od 
The form of a non-rejecting loop is also possible according to the derivation 
do Y; if flJ then leave else skip fi; :rod 
12 
------------------~----------------
Y; if 1 
flJ then .'T; do Y; if flJ then leave else skip fi; .'T od else skip fi 
11 
Y; if 1 
flJ then do .'T; Y; if flJ then leave else skip fi od else skip fi 
Y; if 1 
flJ then do .'T; Y until flJ od else skip fi 
If one develops the corresponding form with jumps from this form (according to the trans-
formation from 6.3) one obtains with simple modifications 
Y; if 1 
flJ then M: .'T; 
Y; if 1 
flJ then goto M else skip fi 
else skip fi 
A simplification at this level is immediately possible. It eliminates the duplication of text: 
goto Z; 
(1) 
M: .'T; 
Z: Y; if 1 
flJ then goto M else skip fi 
(By the way this form cannot be directly reproduced by a do-od construction, as jumps 
into a loop are forbidden in almost all programming languages, and rightly so.) 
(2) 
If we compare version (1) with a jump implementation of the original (n + t) loop 
M: Y'; 
if !?t then goto Z' else skip fi; 
.r; gotoM; 
z I 
: 'VVVVV'VVV 
11 This transformation is often called "revolving a loop". 
12 This transformation has been called "unrolling a loop" (Rutishauser 1952). 

376 
6. Control Elements 
we find that it is somewhat more efficient because the conditional jump is jointly used for leaving and 
for closing the loop. (Such considerations are typical for optimizations on machine code level.) 
6.4.2 Counted Repetition 
A special form of recursion occurs so frequently in practice that most higher programming 
languages provide a particular notation for it, viz. counted repetition. Generally it is a 
range-of-values recursion (comp. 5.1.2) over a linearly ordered domain of mode (A., ~), 
where a parameter, the counter, is "incremented" at every step using the natural successor 
operation of A.. 
With this background, counted repetition is defined by the general transformation 
proc R =- (A. start, J.. limit): 
I G (start) where 
proc G =- (A. counter): 
if counter ~ limit then .9'; G (succ counter) else skip fi J 
proc R =- (A. start, J.. limit): 
for J.. counter from start to limit do .9' od 
By virtue of this definition, the range of binding of the identifier counter is restricted to 
the statement .9' = .9' [counter] (and the comparison with limit). 
An obvious notational variant of the counted repetition reads 
for J.. [start .. limit] counter do .9' od 
with the following generalization to a successive repetition over an arbitrary well-ordered 
(finite) object set 11. 
for 11. counter do .9' od 
Just as in the case of other loop forms a jump implementation can be derived from the 
recursive definition for counted repetition: 
for J.. counter from start to limit do .9' od 
I var J.. counter : = start; 
M: if counter ~ limit then .9'; counter:= succ counter; 
gotoM 
fi J 
(The converse holds only under the condition that .9' contains no assignment to counter.) 

6.5 Loops and Repetitive Systems 
377 
6.5 Loops and Repetitive Systems 
An implementation with jumps has been given in 6.2 for repetitive systems of routines -
that is for systems with simple calls exclusively. 
In 6.4 a particular notation was introduced for the special case of a loop. This notation 
should be more readable by comparison to the otherwise equivalent form with jumps. It is 
to be shown here that this notation can also be used - at least partly - in repetitive sys-
tems. 
For purposes of illustration we will consider once more our standard example in a de-
parameterized form 
funct positive = (stack sign A) sign: 
I var stack sign v : = A; pos* where 
proc pos* = sign: 
if v * empty then if last v = 
D last v = 
else+ 
proc neg* = sign: 
if v * empty then if last v = 
D last v = 
else -
+ then pop v; pos* 
then pop v; neg* fi 
+ then pop v; neg* 
then pop v; pos* fi 
fi' 
fi J 
Further treatment of this system will be formally simpler if the nested alternatives are 
concentrated to a simple branching. In order to make the notation readable we use the 
notational abbreviation ("sequential and", 1.3.3) 
a A b for 13 
if a then b else false fi 
and obtain 
funct positive "" (stack sign A) sign: 
I var stack sign v : = A; pos* where 
proc pos* = sign: 
if v * empty A last v = 
D v * empty A last v = 
D v =empty 
proc neg* = sign: 
if v * empty A last v = 
D v * empty A last v = 
D v =empty 
+ then pop v; pos* 
- then pop v; neg* 
then + 
fi, 
+ then pop v; neg* 
- then pop v; pos* 
then -
fi J 
If we consider the procedures pos* or neg* individually14, then they satisfy all the 
requirements for the introduction of (while) loops: 
13 a A b can not be understood as a routine (in infix-notation) as the parameters of routines are 
evaluated collaterally; this is exactly what we want to avoid here. 
14 The other procedure of the pair is thus considered as primitive and not as a component of a jointly 
formed system. 

378 
6. Control Elements 
funct positive = (stack sign A) sign: 
1 var stack sign v: = A; pos* where 
proc pos* = sign: 
I while v *empty 11. last v = + do pop v od; 
if v * empty then pop v; neg* 
D v = empty then + 
fi J , 
proc neg* = sign: 
I while v *empty 11. last v = + do pop v od; 
if v *empty then pop v; pos* 
D v = empty then -
fi J J 
For simplification in pos* we took advantage of the fact that after termination of the 
loop the top element of v is definitely a " - ", if v is not empty; similarly for neg*. 
If we reconsider the system as a whole we can see that it is still repetitive. (We have the 
special case that all the simple calls refer only to the corresponding other routine.) The 
transformation of repetitive systems into jump form therefore yields 
funct positive =(stack sign A) sign: 
I var stack sign v : = A; go to pos*; 
pos*: while v * empty 11. last v = + do pop v od; 
if v * empty then pop v; goto neg* 
D v = empty then + return 
fi; 
neg*: while v *empty 11. last v = + do pop v od; 
if v * empty then pop v; goto pos* 
D v = empty then -
return 
fi J 
(Of course we could dispense with the first goto pos* and the goto neg*.) 
6.6 Sequential Circuits 
Binary arithmetic units, fundamental components of a computer, were described on a 
functional basis in 3.6.4.1 in terms of computational structures. We want to show now 
that (binary) control units can also be described functionally, thus fitting completely into 
descriptions by means of control elements. 
To illustrate this we will use the system positive of 6.2 again. 
Proceeding from the last form of positive in the previous paragraph the following discussion is 
possible on the "level of variables", too. However, great care must be taken in the individual trans-
formation steps; this leads to a rather involved derivation. 
Thus it is advisable - as in many other cases - to proceed from the pure, applicative level here, 
too. 
The symmetrical form of pos and neg calls directly for simplifications. The following 
method is a general approach for the treatment of such symmetrical systems: 
For every routine an additional Boolean parameter is introduced as a "switch". The 
whole system is "fused" into one single routine, where the bodies of the previous single 

6.6 Sequential Circuits 
379 
routines now become branches guarded by the corresponding switches. Of course there is 
the prerequisite that all the routines of the system have the same result mode. 
Applied to the system positive, we have 
functpositive "'(stack signA) sign: 
I pn(A, true, false) where 
funct pn "' (stack sign A, boo I po, boo I ne: ne = 1 po) sign: 
if po then if A =1= empty 
then if top(A) = + thenpn(rest(A), true, false) 
D top(A) = - thenpn(rest(A), false, true) fl 
else + 
fi 
D ne then if A =1= empty 
then if top(A) = + thenpn(rest(A), false, true) 
D top(A) = - thenpn(rest(A), true, false) fi 
else -
fi fi J . 
In this example, of course, one Boolean parameter would suffice, because we always have ne = 
1 po. Generally a control parameter of the mode nat [1 .. n] is sufficient for a system with n rou-
tines. Such a possible increase in efficiency by encoding is, however, not essential for the general 
method. 
It can be seen immediately that some elementary transformations are possible for the 
branchings (comp. 4.1, transformation (c): "exchange of two tests"). Moreover, the con-
stants true and false can be replaced by poor ne in the recursive calls of pn as a prepara-
tion for the following step, as the guards show the corresponding values of po and ne: 
functpositive "'(stack signA) sign: 
I pn(A, true, false) where 
funct pn "' (stack sign A, boo I po, boo I ne: ne = 1 po) sign: 
if A =1= empty 
then if top(A) = + then if po thenpn(rest(A), po, ne) 
D ne thenpn(rest(A),po, ne) fi 
D top (A) = 
- then if po then pn (rest (A), ne, po) 
D ne thenpn(rest(A), ne, po) fi fl 
else if po then + 
D nethen- fi 
fi J 
The next improvement is immediately apparent: an alternative with identical branches 
reduces to one of these branches. We therefore apply the simple transformation 
if f!B then Y else Y fi 
changes into Y 
and obtain 

380 
6. Control Elements 
funct positive = (stack sign A) sign: 
I pn(A, true, false) where 
funct pn = (stack sign A, bool po, bool ne: ne = 1 po) sign: 
If A *empty 
then if top(A) = + thenpn(rest(A), po, ne) 
0 top(A) = - then pn(rest(A), ne, po) fl 
else if po then + 
0 ne then -
fi 
fi J 
Applying the transformations of the previous paragraphs, this can be transformed into a 
form either using loops or jumps, e.g. into 
funct positive = (stack sign A) sign: 
I (var stack sign va, var bool vpo, var bool vne) : = (A, true, false); 
while va * empty do 
If last va = -
then (vpo, vne): = (vne, vpo) else skip fi; 
pop va 
od; 
if vpo then + 0 vne then -
fi 
J 
We must take special precaution with permissible sequentialization, i.e. pop va must 
not be placed in front of the branching. The superfluous assignment ( vpo, vne) : = ( vpo, 
vne) in the case last va = + has been dispensed with. 
The pair of Boolean variables (vpo, vne) corresponds to a flip-flop. Altogether, from 
the last form of the routine positive a realization as a binary control unit is evident, i.e. a 
realization as a sequential circuit which initiates the operations ( vpo, vne) : = ( vne, vpo) 
and pop va, for as long as va is not empty. As mentioned above, the assignment can be 
transcribed to ( vpo, vne) : = ( 1 vpo, 1 vne) and one of the variables can be dispensed 
with. 
The following example shows clearly that this method is also applicable to systems with 
more than two routines: Let a system be given for determining whether a number, in 
binary representation, can be divided by 3: 
funct divisible = (sequ bit y) bool: 
I restO(y) where 
funct restO = (sequ bit x) bool: 
if x = 0 then true 
0 x * 0 then if bottom(x) = 0 then restO(upper(x)) 
funct restl = (sequ bit x) bool: 
if x = 0 then false 
else rest I (upper(x)) fi fi, 
0 x * 0 then if bottom(x) = 0 then rest2(upper(x)) 
funct rest2 = (sequ bit x) bool: 
if x = 0 then false 
else restO(upper(x)) fi fi, 
0 x * 0 then if bottom(x) = 0 then restl (upper(x)) 
else rest2(upper(x)) fi fi J 

6. 7 Flow Diagrams 
381 
For this system, e.g. written in the form with jumps (after obvious simplification) we ob-
tain the control unit simulating an automaton with three states: 
funct divisible = (sequ bit y) boo!: 
I var sequ bit v : = y; 
(var boo! hrO, var boo! hrl, var boo! hr2) : = (true, false, false); 
M: if v =1= 0 then if bottom(v) = 0 
then (hrl, hr2) : = (hr2, hrl) 
else (hrO, hrl) : = (hrl, hrO) fi; 
v: = upper(v); goto M 
else hrO return 
fl J 
Exercise 1: Carry out the transformation process sketched above in all its individual steps. 
Exercise 2: Give a variant of divisible using a switching variable of mode var nat [1 .. 31. 
Here, too, the binary realization by means of a sequential circuit with three flip-flops is 
obvious. (This sequencing, by the way, will prove to be a special case of more general pro-
cesses in Petri nets.) 
Further improvement is obtained if we do not assign a separate Boolean variable to 
every routine, but instead code up to 2N routines by means of N Boolean variables. A 
coded sequential circuit corresponds to this. 
As we can consider addressed storage functionally as an array of variables (see Chap. 
7), a complete computer not only can actually be described functionally in "higher" 
programming language concepts, but almost calls for the introduction of these concepts 
(Schecher 1970). 
6. 7 Flow Diagrams 
All processes 15 treated up to now - apart from demanding operands collaterally (1.7.3) -
were of a sequential nature. In the sequel we will also consider parallel and concurrent pro-
cesses and discuss the problems which arise when coordinating concurrent processes. 
6. 7.1 Classical Flow Diagrams 
If we consider the processes resulting from the equivalent (see 4.2) recursive routines 
(comp. 1.4.1) 
functjac = (nat n) nat: 
if n = 0 then 1 else n x jac(n - 1) fi 
and (comp. 6.1.2) 
15 A process is meant to be the execution of a piece of program {comp. 1.7 .3, 6.2.2). The definition 
'execution in time' is too narrow: A repetition can take place in time {pulse) and also in space 
{ornament). A description of addition {comp. 3.6.6) can be both the description of a serial adder 
{execution in time) and of a parallel adder {execution in space). 

382 
6. Control Elements 
functjac =(nat n) nat: G(n, 1), 
funct G =(nat n, nat m) nat: 
if n = 0 then m else G(n - 1, m x n) fi 
we can sketch pictures for both processes, e.g. like those in Fig. 6.1. 
fac(5) 
G(5,1) 
5 X fac(4) 
G(4,5) 
4 X fac(3) 
G(3,20) 
3 X fac(2) 
G(2,60) 
2 >< fac(l) 
G(1,120) 
ll " ~a~( 0) 1_1 
I 
G(0,120) 
I 120 
Fig. 6.1 
,J 
The individual incarnations of fac or G exist independently of each other (comp. 1. 7.4 
and the "pending" operations in the function inversion, 4.2.3). For the second process, 
however, we can dispense with the return organization (and thus do without different in-
carnations, see 6.2, simple call). This is expressed by the fact that (after the introduction of 
a jump) the "current position" in the flow diagram 16 with branching and junction (Fig. 
6.2), supplemented by the state of the variables u, v, already encompasses the recursive 
situation: Recursion has "crept into the variable". Accordingly, the methods for proving 
the "correctness" of programs are directed towards the states of variables and properties 
with respect to these states at the nodes of the flow diagram; semantics using predicate 
transformers is tailored to this. 
16 Usual flow diagram standards do not provide a symbol for the delivery of a result; we use the 
symbol for display. 

6.7 Flow Diagrams 
383 
One might assume that every process could be described by a suitable flow diagram. 
However we know from 4.3 that there are classes of recursive routines for which no flow 
diagram exists (Paterson, Hewitt 1970). In short: recursion is more powerful than itera-
tion. 
The concept of the flow diagram suggests generalization so that it covers even the most 
general recursive situations. This is possible by replacing the graph of the flow diagram by 
a hierarchical graph in which certain nodes are graphs themselves (Pratt 1969). 
x :=rest (x) 
y :=append 
(y,top(x)) 
Fig. 6.3 
X= OVy= 0 
top(x) * top(y) 
top(x) = top(y) 
Flow diagrams are not necessarily determinate. The flow diagram of a "non-deter-
ministic finite machine with one pushdown store" in Fig. 6.3 (comp. routine pall in 2.10-5) 
has the purpose of verifying that a given word xis a palindrome of even length. It is non-
determinate: The branching which is marked by hatching allows one to go either one of 
two ways. However, there exists an "accepting" execution (which takes the path to the 
right after having examined half of the word) if and only if x is a palindrome of even 
length. At the exits of the branchings, those guards are listed which permit entry to the cor-
responding branch 17 • The guard true has been suppressed. 
17 Here we have generalized the common symbol for branching to include "nondeterministic branch-
ing with guarded branches". 

384 
6. Control Elements 
Exercise 1: Give a flow diagram for recognizing whether a given word is a palindrome of even length, 
based on the routine pal2 of2.10-5. 
Exercise 2: What are the conditions for 
if p v q then A else B fi 
and 
if p then A 
elsf q then A 
else B fi 
to be equivalent? 
6. 7.2 Splitting and Collection 
6. 7 .2.1 After the translation of a program into a flow diagram we might find it useful now 
to indicate - by a lighting bulb - the current position, i.e. the operation to be performed 
(Fig. 6.4). Such an indication is especially suggestive in the case of a sequential circuit 
(6.6). Exactly one of theN position bulbs, which show the states of theN flip-flops, is lit at 
a time. 
then 
(u,v) := {u-1,v xu) 
goto f 
else 
v 
Fig. 6.4 
Yet particularly in the case of a sequential circuit the following questions arise im-
mediately: 
Why does always light up only one bulb? 
Could subprocesses sometimes be performed independently of each other (especially in 
parallel)? 
Could the control unit become "schizophrenic"? 
These questions are irrelevant in connection with classical flow diagrams because of 
their strictly sequential nature. They become more important when we consider collateral 
situations which usually occur in early forms of problem solutions. 
In the above example the collective assignment 
(u, v) : = (u -
1, v x u) 

6.7 Flow Diagrams 
385 
implicitly contains such a parallel situation which becomes visible after rewriting it into 
(comp. 5.2.4) 
(natx, naty) ""(u- 1, v xu); 
(u, v): = (x, y) 
Using special brackets IT, lJ to indicate parallel execution (they correspond to the 
bracket symbols parbegin, parend introduced by Dijkstra 1965), the above can be re-
written as 
IT nat x "" u -
1 II nat y "" v x u lJ 
ITu:=xllv:=ylJ 
In flow diagrams, parallel constructions are represented by splitting and collection 18 
with special graphic symbols. 
18 According to German Industrial Standards DIN 44300: 
No. 
Designation 
Definition 
83 
Splitting 
A point in a program flow plan* from which several 
(Ger. 
branches* in the execution* of the program can be followed in 
Aufs pal tung) 
parallel. The designation is also used for the splitting process 
in the description of the program execution. 
Remark: For symbol see German Industrial Standards 66001. 
84 
Collection 
A point in a program flow plan* in which -
in program 
(Ger. 
execution* -
all the parallel actions in the branches joining 
Sammlung, 
together must be terminated before the continuing branch can 
Fr. 
be followed. 
ralliement) 
The designation is also used for the collection process in the 
description of the program execution. 
Remark: For symbol see German Industrial Standards 66 001. 
According to German Industrial Standards DIN 66001: 
No. 
Symbol 
Designation and Comment 
4.4. 
Synchronization in parallel 
(parallel mode) 
4.4.1. 
J 
Splitting 
One incoming branch, 
I I I 
several outgoing branches 
4.4.2. 
I I I 
Collection 
Several incoming branches, 
I 
one outgoing branch 
4.4.3. 
I I I 
Synchronization section 
Some number of incoming 
I I I 
and outgoing branches 

386 
6. Control Elements 
Thus we have for the example under consideration Fig. 6.5 where two position bulbs now 
light up simultaneously. 
u :=x ~I 
v:= Y 
Fig. 6.5 
6.7.2.2 In general, parallel constructions are introduced by a defining transformation; for 
a collective assignment we have 
{vi, j: i =F j: X; does not occur freely in gj 
Similar transformations hold for collective object and variable declarations. The condition 
for the transformation is a special case of the Bernstein condition (Bernstein 1966), see also 
5.2.3: 
A collection of statements Y, can be executed in parallel if and only if no program 
variable to which a value is assigned within some .'lj occurs within any YjU * i) 19• 
Whenever in the sequel the parallel notation 
IT st II Yz II··· II Y" lJ 
is used, it is tacitly assumed that the Bernstein condition is fulfilled for the Y, - a similar 
assumption was made about collateral generalized expressions. 
If for a collection of statements .'lj = def .'!' [iJ (i = 1, 2, ... , n) the Bernstein condition 
is fulfilled, the counted repetition (6.4.2) 
for nat [ 1 .. n J i do .'/ [iJ od 
19 A slightly weaker condition guarantees that the statements Y, can be executed in any order: 
Auxiliary variables for completely internal results can be "shared" between two statements. Note 
that the statements I h: = a; a:= b; b: = h J and I h: = c; c: = d; d: = h J (where his not 
used later on) can be executed in any order, but not in parallel. 

6. 7 Flow Diagrams 
387 
can be executed in parallel, i.e. like 
rr Y[1J IIY[2] II···IIY[nJ lJ 
6.7.2.3 Of course there are other implementations of the problem above, e.g. a strictly 
sequential one such as (comp. 5.2.4) 
nat x = u - 1; nat y = v x u; u : = x; v : = y 
but also 
nat x = u - 1; nat y = v x u; v : = y; u : = x 
which can be shortened to 
nat x = u - 1; v: = v x u; u : = x 
and for (partially) parallel execution can be written as 
rr nat X = u - 1 II v : = v X u lJ ; u : = X 
This can again be sequentialized into 
V : = V X U; nat X "' U -
1; U : = X 
and shortened to 
v: = v x u; u: = u - 1 
The corresponding formulations using flow diagrams are obvious. 
The modifications in the example above are proved by showing that the Bernstein con-
dition is fulfilled for the statements in question. A calculus of such transformations can be 
established, one of the transformation rules being for example (Broy 1980) 
If 5"2 is interpreted as skip, this gives 
rr st11Y3JJ 
i f 
{Bernstein ( S1 , 5"3) 
st;SJ 

388 
using the further rules 
rr skip II .<1 lJ ... rr .<1 lJ ... .<1 
For alternative statements there is the distributive transformation 
rr if ~then !II else .9'2 fi 11.9'3 lJ 
{Bernstein ( ~, 93) 
if ~ then rr !II 1193 lJ else rr .9'2 1193 lJ fi 
A similar one holds for guarded statements. 
6. Control Elements 
When using flow diagrams with splitting and collection special care must always be taken for 
"correct" collection after splitting into concurrent subprocesses. This is shown by the solution of the 
problem of assigning in two steps the greatest (corresponding to the relation ~)of 4 objects to a vari-
able x (Dijkstra 1965). 
The at first obvious solution with splitting and collection (Fig. 6.6) on closer examination turns 
out to be wrong. Out of altogether six concurrent subprocesses, three subprocesses remain in a 
waiting state after determining the maximum of a tuple (a1, a2, a3, a4) and would lead to wrong 
results in case of a subsequent determination of the maximum for a tuple (ai, ai, aj, a,!). 
6.7.3 Coordinated Flow Diagrams 
The important issue in concurrent situations is the independence of the operations oc-
curring. Additional problems appear if, furthermore, a coordination of the individual 
operations (to be performed in parallel) is required, as for example in the producer-con-
sumer problem: 
A factory produces parts and consumes them (in "units"). It can store only a limited 
amount max (max > 0), and of course cannot store any negative amounts. The pro-
duction and consumption processes are thus independent of each other, but they must 
be coordinated (because of the limited storage space). 

6.7 Flow Diagrams 
389 
The problem could be solved sequentially with the help of three procedures. Here the 
order of production and consumption of units (within the limits of the given storage 
capacity) remains open: 
(nat max, nat stock supply): 
I var nat amount : = stock supply; distributor where 
proc producer = : 
I «produce unit»; amount : = amount + 1; 
distributor 
J , 
( •) 
proc consumer = : 
I «consume unit»; amount : = amount - 1; 
distributor 
J , 
proc distributor = : 
if amount > 0 
then consumer 
D amount < max then producer fi 
J 
Note that the system (•) does not terminate, in contrast to most of the algorithms con-
sidered so far. This "case of infinity" is entirely realistic in operating systems where pro-
grams terminate but the operating system continues infinitely. 
The system ( •) can again be described by a flow diagram using non-deterministic 
branching with guarded branches (Fig. 6.7). The light bulbs show the initial state of the 
system. 
amount< max 
0 
amount> o 
~ 
~ 
•Produce. 
ccconsume» 
amount:=® 
amount:=~ 
amount +1 
amount -1 
Fig. 6.7 
An organization such as in ( •) could apply to a clog-maker who owns a shop. He either 
makes clogs or he serves customers. The question now is, how do we organize this 
"division of labour"? Why should not the producer and consumer work independently of 
each other? Only the incrementing and decrementing of the stock must be coordinated. 
The distributor is no longer necessary, but instead we need coordination so that the con-
current subprocesses do not count simultaneously (mutual exclusion, in short mutex). A 
suitable coordinated flow diagram is shown in Fig. 6.8. 
Note that the guarded statements are protected by guards the values of which can be 
changed by concurrent subprocesses. We accordingly specify that a process waits if no 

390 
6. Control Elements 
I 
I 
. 
0 
1f amount < max 
if amount> 0 0 
then «produce>• 
then «consume» 
fi 
fi 
I 
0 
I 
l 
I 
amount:=@ 
amount:=® 
amount +1 
amount -1 
I 
I 
I 
I 
Fig. 6.8 
guard releases it20 • Up to now, i.e. by restricting ourselves to pure sequential processes, we 
could say the result is >>Undefined« because there was no way of cancelling the waiting 
situation. (However, "infinite" wait situations can also occur in concurrent processes, 
comp. 6.9, deadly embrace). 
Altogether there are three concurrent subprocesses: the "producer loop", the "con-
sumer loop" and the store bookkeeping (consisting of two subprocesses). Both producer 
loop and consumer loop must cooperate with the store bookkeeping in a particular way: 
They are joined for a certain part of their execution. These "parts" which cannot be 
executed simultaneously by both loops are called critical sections21 . They are shown with 
heavy lines in the diagram. The positions of the processes again are marked by the position 
bulbs. 
Critical sections must begin with a collection and terminate with a splitting. If, due to a 
faulty coordination of the individual components, a mutual interference occurs, we speak 
of a collision. 
20 This can be implemented as a "dynamic waiting". E. g. we can replace 
amount< max 
by the waiting loop 
amount=max 
amount< max 
The general method is obvious. 
21 A related method is the "method of the signal panel" for single-track street cars: Only a driver 
possessing the signal panel is allowed to travel on a critical section. However, in this way a critical 
section can only be travelled on alternately, whereas in the producer-consumer example both the 
producer and the consumer can pass through the critical section several times in succession. 

6. 7 Flow Diagrams 
391 
Branching such as in the last diagram (Fig. 6.8) is completely non-deterministic, it can 
lead towards the left or towards the right - as long as the collection condition (comp. 
6.7.2.1, footnote) is satisfied. In the example under consideration, however, this is trivial. 
The diagram cannot be described using IT and lJ only. We shall see in 6.9 how a linear 
notation can be achieved. 
The diagram in Fig. 6.9 is equivalent to the one of Fig. 6.8 but simply cyclically rotated. 
I 
I 
I 
I 
~ 
'f 
0 
if amount> 0 ® 
1 amount < max 
then «produce .. 
then .. consume• 
fi 
fi 
I 
I 
amount:=® 
amount:-0 
amount +1 
amount -1 
I 
I 
Fig. 6.9 
In addition different positions of the processes are marked here 22. Note that a situation as in Fig. 
6.10 is not provided for and cannot be obtained from the initial situation given in Fig. 6.8, as the tran-
sitions from one position to another are fixed by the conditions for splitting and collection (comp. 
6.7.2.1, footnote 18) in coordinated flow diagrams. 
I 
I 
® 
0 
I 
0 
I 
I 
I 
0 
® 
I 
I 
I 
I 
Fig. 6.10 
22 The two sections which are "live" in this situation are mutually not critical: decreasing amount 
does not invalidate the condition amount < max. 

392 
6. Control Elements 
It is intuitively clear in this example how to get from one situation to a successive one. A formal 
explanation will be given in 6.8 by means of the semantics of Petri nets. It will also become apparent 
when a coordinated flow diagram is "meaningful". 
Another version where the waiting conditions are expressed explicitly can be found in 
Fig. 6.11. 
Fig. 6.11 
0 
.. produce» 
amount:=® 
amount +1 
<<COnsume)) 
amount:=® 
amount -1 
A further important variant is obtained if two Boolean variables are introduced as 
guards (Fig. 6.12). 
A simplification is immediately apparent: the assignment in the left branch 
nonempty: = amount > 0 (or nonju/1: = amount < max in the right branch) 
can be replaced by the assignment 
nonempty : = true 
(or nonfu/1: = true) 
because incrementing (or decrementing) 
amount : = amount + 1 
(or amount:= amount -
1) 
precedes both assignments respectively, and thus 
{amount ~ 0} amount : = amount + 1 {amount > 0} 
(or {amount ~ max} amount:= amount - 1 {amount < max}) 
holds. 

6.8 Petri Nets 
Fig. 6.12 
6.8 Petri Nets 
0 
amount:= 
amount +1; 
nonfu/1:= 
amount < max; 
nonempty := 
amount> 0 
((consume,, 
amount:= 
amount-1; 
nonempty := 
amount> 0; 
nonfu/1 :-
amount< max 
393 
The concept of Petri nets treated below is the abstract background of coordinated flow 
diagrams with (deterministic or non-deterministic) branching and junction, splitting and 
collection. 
6.8.1 Theory of Petri Nets 
A Petri net23 is a bipartite directed graph, i.e. a directed graph with two disjoint sets of 
nodes: places and transitions whose elements are indicated by 0 or I. The edges of the 
graph each lead from an element of one set of nodes to an element of the other set. Every 
place from which an edge leads to a transition Tis called an input place of T, and every 
place to which an edge of T leads is called an output place of T. 
In addition to the static properties which are represented by the graph, a Petri net has 
also dynamic properties which result from an (alterable) marking of the places. We dis-
tinguish between nat or bool Petri nets according to the marking of the places by natural 
23 The theory of Petri nets originates from C. A. Petri, who dealt with the description of informa-
tion flow in systems in his doctoral thesis (Petri 1962). For further information see Peterson1981. 

394 
6. Control Elements 
numbers or by truth values 24• In a representation of a Petri net in the form of a diagram 
the marking is usually represented by tokens. An alteration of the marking is permissible if 
at least one transition exists, all of whose input places are marked with true (in the case of 
a boo! Petri net) or with a natural number ~ 1 (in the case of a nat Petri net). If the 
marking is altered through such a transition we say that the transition "fires". A per-
missible alteration of the marking of a boo! Petri net is then carried out by setting - for 
the firing transition -
all input places false and all output places true. In the case of a 
permissible alteration of the marking of a nat Petri net, (for one transition again) the 
markings of all input places are decreased by 1 and the markings of all output places are in-
creased by 1. 
Exercise 1: Define a Petri net in which - by means of a single permissible alteration of the marking 
- a place 5 is marked if and only if 
a) a place a and a place J:l are marked; 
b) a place a or a place J:l is marked; 
c) out of three places a, J:l, y at least two are marked. 
The marking of a Petri net is said to be live if every alteration of the marking leads to a 
situation which again allows an alteration. Otherwise the marking is said to be terminat-
ingzs. 
An example of a live marking is a marking which permits a cycle. With the Petri net 
(Dennis 1973) in Fig. 6.13 the place markings 26 
(1 ,2) 
(3,2) 
(5,6) 
(1,6) 
form a cycle. (1 ,3} would be an example of a terminating marking. 
c 
5 
6 
Fig. 6.13 
24 In principle markings with objects of an arbitrary mode A, for which a well-ordering is defined, 
are conceivable. 
25 In particular also when the given marking does not permit any alteration. 
26 If the places are denoted by numbers, it is sufficient - for characterizing a marking with truth 
values - to indicate those places which are marked with true. 

6. 8 Petri Nets 
Exercise 2: {a) Give initial markings for the bool Petri net of Fig. 6.14 
which permit 
(1) no alteration 
{2) exactly one alteration 
{3) a cyclic succession of alterations with the period 3. 
395 
(b) Do initial markings exist for this Petri net which do not belong to any one of the three 
given classes? 
3 
4 
Fig. 6.14 
Another example of a live marking is shown by the nat Petri net of Fig. 6.15. 
Fig. 6.15 
A Petri net in which at each instance exactly one place is marked by 1 or true and in 
which all other places are marked by 0 or false is called a one-token Petri net. Such a one-
token net describes a sequential process by virtue of the (trivial) 
Theorem: If every transition has exactly one input place and one output place and if we 
have a one-token marking, then we have a one-token Petri net. The reverse also 
holds. 
If the transitions are omitted in a one-token Petri net we have once again a classical 
sequential flow diagram with places as process positions. In a one-token-net, however, 
several edges can emerge from a place. Such edges are called alternatives and the Petri net 
has a branching at this point. Likewise junctions, i.e. places where several edges end, can 
occur. In a certain marking situation there are usually several permissible marking altera-
tions. In this case it can be decided arbitrarily which alteration is to be made. With regard 
to the simulation (see below) we speak of a free choice. It can also happen that, for two 
permissible alterations, carrying out one can make the other inadmissible (the two 
transitions have an input place in common). The transitions are said to be in conflict -
this can be resolved by free choice. 
For one-token nets there is a free choice in the branching (which is controlled by the 
evaluation of the prevailing conditions in the case of an interpretation) and the transitions 
are always in conflict. In multi-token nets the possibility of alteration no longer depends 
on the marking of a single branching. 

396 
6. Control Elements 
In the Petri net of Fig. 6.13 with the marking (2,3,4) the transitions d and e are in 
conflict. 
Exercise 3: Which transitions could be in conflict in the net of exercise 2? Give corresponding mark-
ings. 
A sequence of permissible alterations of the marking (in the sequential case it is a 
sequence of place or position changes) is called a simulation of a Petri net or a game. Such 
a sequence is in general non-determinate because of free choice. A game terminates if there 
is no further permissible alteration. 
A marking of a Petri net is said to be safe, if no game leads to a "front-end collision", 
that is if for every permissible alteration the output places of the corresponding transitions 
are free, i.e. are marked with false in a bool Petri net and with 0 in a nat Petri net 27• 
For the Petri net of Fig. 6.13 the markings given in the cycle are safe. In the bool Petri 
net of Fig. 6.16 (1 ,3,5) is a safe marking, whereas e.g. (1 ,4,5) is not safe, because the out-
put place 5 is already marked for the permissible alteration at transition d. 
Fig. 6.16 
An important proposition about safe markings is given by the following 
Theorem: Safe markings form a subset of markings which is closed under the (permissible) 
alterations. 
There are Petri nets which do not allow any safe marking, such as 
and those for which every one-token marking is safe: one-token Petri nets. 
The relationship between safe nat Petri nets and bool Petri nets is shown by 
Theorem: A nat Petri net with a safe marking can always be replaced by a suitably marked 
(safe) bool Petri net and vice versa (1 ~ true, 0 ~ false). 
27 For nat Petri nets the proposition that in no permissible alteration a place is marked with more 
than one token is equivalent (Dennis). 

6.8 Petri Nets 
397 
If we restrict ourselves in certain applications to safe markings, we can immediately 
change over to safe bool Petri nets. On the other hand genuine nat Petri nets are 
sometimes useful. 
Exercise 4: Give the set of safe markings for the Petri net of exercise 2. 
6.8.2 Construction of Petri Nets, Connection to Coordinated Flow Diagrams 
Petri nets can always be decomposed so that they turn out to be constructed from the ele-
ments shown in Fig. 6.17. 
-1 
"annihilator'' 
-o 
"terminal place" 
1-
"generator'' 
o-
"initial place" 
·I 
trivial transition 
--()--
trivial place 
---+=::: "splitting" 
---()<:: "branching" 
::::.t-
"collection" 
::::.0-
"junction" 
Fig. 6.17 
A transition such as a "synchronization" 
can be decomposed by introducing an auxiliary place: 
To begin with, Petri nets are uninterpreted models. When provided with an 
interpretation ( = the marking of the places) they serve for designing, describing and 
analysing systems. Their principal advantage is that they provide adequate descriptional 
means for parallel, concurrent or non-deterministic situations. 
For example the meaning of the (non-deterministic) guarded branching (camp. 1.9) 
if ~ 1 then J/1 U ~ 2 then J/2 fi 
can be described by the following interpreted Petri net (Fig. 6.18): 

398 
6. Control Elements 
T 
T 
Fig. 6.18 
There is a one-to-one correspondence (Fig. 6.19) between safe bool Petri nets or safe 
nat Petri nets and the coordinated flow diagrams treated in 6.7.3. 
DIN 66001 
Petri net 
Branching A 
A 
Junction y 
Splitting 
Collection 

6.8 Petri Nets 
399 
Beginning ? 
T 
End 
6 
6 
Path 
I 
+ 
Operation 
Fig. 6.19 
The rules for altering the marking in a Petri net are mirrored by the conditions for 
splitting and collecting. The possibility of free choice is represented by the non-
deterministic branching. 
As coordinated flow diagrams do not allow arbitrary markings - in contrast to general 
Petri nets -
a "front-end collision" can (and must) be avoided in coordinated flow 
diagrams by suitably controlling the processes 28 • Thus coordinated flow diagrams corre-
spond to safe Petri nets. 
28 The following diagram (Fig. 6.20) modelled on the Petri net of Fig. 6.15 does not show a suitable 
control of the process; since it does not allow non-trivial safe markings it is not a coordinated flow 
diagram. 
0 
Fig. 6.20 

400 
6. Control Elements 
6.9 bool Petri Nets, Signals 
The following bool Petri nets (Fig. 6.21, Fig. 6.22) together with the initial markings 
correspond to the flow diagrams Fig. 6.7 and Fig. 6.8: 
Fig. 6.21 
Fig. 6.22 
The first is a one-token Petri net, the process is non-determinate, but strictly sequen-
tial. The second is a safe non-sequential bool Petri net. The "critical sections" are again 
indicated by heavy lines. Mutual exclusion applies only in the case of altering the variable 
amount. It is assumed that the tests amount < max or amount > 0 do not interfere with 
each other 29• Neither the producer nor the consumer is interested in the values of amount. 
Their sole interest is if the store is full or not and empty or not, respectively. 
In order to characterize the marking state in a bool Petri net we can now introduce 
special variables called signals30 with the mode indication flag. In a flow diagram signals 
cause trains to stop or go ahead. They are of the character var boo I and can be tested. The 
most important difference between them and normal Boolean variables is that they belong 
to a module containing two special operations (introduced by Dijkstra 1965 as elements of 
programming languages), corresponding to the basic components of mutual exclusion: the 
blocking operation p signal (Dutch passeren) and the releasing operation v signal (Dutch 
vrijgeven) where signal is of mode flag. In coordinated flow diagrams the blocking or 
releasing operators are also denoted by p and v (Fig. 6.23). 
29 Normally two people can read one newspaper simultaneously without disturbing each other, 
though one of them might become nervous. 
30 Dijkstra 1965 calls signals binary semaphores (comp. 6.10). 

6.9 bool Petri Nets, Signals 
401 
p signal 
signal 
0 
0 
signal 
v signal 
Fig. 6.23 
p signal can be implemented as a waiting loop 
f: if signal then signal:= false 
else goto f 
fi 
It is assumed that the test and the setting of the signal are indivisible, i.e., that no other 
operation which might change the signal can interfere 31 • 
I 
psignal a 
I 
«segment A« 
1 
v signal a 
p signal b 
I 
«segment B» 
I 
v signal b 
p signal c 
I 
Fig. 6.24 
31 Corresponding circuits could easily be invented. 

402 
6. Control Elements 
Accordingly 
v signal 
can be implemented as 
signal : = true 
where the assignment is also assumed to be indivisible. 
The operations p and v can now be used in a trivial way in order to reproduce the 
automatic signalling section in railroad traffic (Fig. 6.24). 
Their application - together with splitting and collection - also permits the treatment 
in linear notation of more complicated coordination problems. 
If in the producer-consumer problem (Fig. 6.12) in 6.7.3 we introduce - in addition to 
the signal mutex which provides the mutual exclusion -
two further signals nonfull and 
nonempty (all of the mode flag) we obtain the coordinated flow diagram in Fig. 6.25. 
I 
I 
vmutexO 
vmutexo 
-
---r-
--,--- -
r----.., 
I 
I 
I 
I 
r----.., 
I 
I 
I 
.... ___ 
~~~--
.--+--~~ 
---
~ 
.--+--~-, 
I 
-='----o 
--
~--~ 
-------=-o 
--
~ 
p non full 
0 ~utex 
p nonempty 
(--~~ 
----~ 
I 
I 
0 
t 
' 
0 
I 
I 
I 
~ 
.produce» 
\nonfu/1 
'© 
// 
( /(/ 
I I r-----+-------------lr-.... ..., 
: : 
______ .:: ___ -_-_-:::.:. 0 
/ 1 
pmutex; 
I 1 
amount :=amount+1; 
I I H 
if amount < max 
then v nonfu/1 else skip fi; 
vnonempty 
I I 
I I 
I I 
I I 
--------------
1 I 
---..,------T---
: L_ 
_ . ....1 
L 
_ 
--
------------ ----------
Fig. 6.25 
I 
ccconsume» 
I 
1 
I 
I 
I 
I 
J 
1 
nonempty_/ 
! 
~/ 
: 
'>, 
' ' 
r-----~~------------+---~, ~ : 
_._ ... _____ .:: __ .:: __ .:. 0 : : 
pmutex; 
1 1 
I I 
amount:= amount -1; 
1 1 
if amount> 0 
I ~ 
then v nonempty else skip fi; + 1 
v nonfu/1 
I I 
I I 
~::~:~::::;:~~ 
: : 
------~~J-
L'.:.=-..=-..=-.=-.=-~_J 

6.9 bool Petri Nets, Signals 
403 
Here all the merely coordinating elements are in broken lines. Note that in the left 
branch 
nonfu/1: = amount < max (or nonempty: = amount > 0 in the right branch) 
has been replaced by 
if amount < max then nonfu/1 : = true else skip fi 
(or if amount > 0 then nonempty : = true else skip fi) 
because the preceding p nonfu/1 (or p nonempty) has assigned the value false to the signal 
nonfu/1 (or nonempty). According to the argument in 6.7.3 
nonempty : = amount > 0 (or nonfu/1 : = amount < max) 
has also been abbreviated to 
v nonempty (or v nonfu/1) 
If we change from the flow diagram above to the clearer representation in Fig. 6.26 
p~ 
c:( 
p nonfu/1; 
pnonempty 
•produce .. 
•consume• 
I 
J 
pmutex; 
p mutex; 
amount :=amount +1; 
amount:= amount -1; 
if amount < max 
if amount> 0 
then v nonfu/1 else skip fi; 
then v nonempty else skip fi; 
vnonempty 
v nonfu/1 
I 
I 
v mutex 
vmutex 
) 
~ 
Fig. 6.26 
we can also change to a linear notation -
after separating the concurrent sections and 
synchronizing them by signals: 
[I P: p nonfu/1; 
«produce»; 
p mutex; 
amount : = amount + 1; 
if amount < max 
then v nonfu/1 else skip fi; 

404 
v nonfull; 
v mutex; 
gotoP 
C: p nonempty; 
«consume»; 
p mutex; 
amount : = amount -1; 
if amount> 0 
II 
then v nonempty else skip fi; 
v nonfull; 
v mutex; 
goto c 
6. Control Elements 
lJ 
In general, coordinated flow diagrams can be reshaped with the help of signals so that 
the various sections are notationally separable and thus the whole diagram can be written 
in linear form, using IT and lJ . 
In the example just given the signals mutex and nonfull (or nonempty) serve different 
purposes. mutex provides a mutual exclusion (protection of the variable amount against 
simultaneous access) for the critical sections, whereas nonfull and nonempty only serve to 
control the internal executions of and the communication between both sections. Signals 
of the latter kind for which p occurs only in a single section are said to be private for this 
section. 
The p and v operations - as stated above - can be directly implemented by waiting 
loops. Further simplifications arise: Since nonfull and nonempty are private signals, the 
assignments nonfull : = false and nonempty : = false can be extracted from the 
operations p nonfull and p nonempty, respectively, and postponed to the next occurrence 
of the respective variables. 32 
If we combine, e.g., 
nonfull : = false; if amount < max then nonfull: = true else skip fi into 
nonfull : = amount < max 
and in addition introduce full, empty as negations of nonfull, nonempty we obtain the 
usual programs 
f1: if full then goto f1 else skip fi; 
«produce»; 
f~: if mutex 
then mutex : = false 
else goto f~ 
fi; 
amount : = amount + 1 ; 
full 
: = amount = max; 
empty : = false; 
mutex : = true; 
gotofl 
JI': if empty then gotof{' else skip fi; 
«consume»; 
f~': if mutex 
then mutex: = false 
else gotof~' 
fi; 
amount : = amount - 1; 
empty : = amount= 0; 
full 
: = false; 
mutex : = true; 
gotof{' 
32 Only for mutex an indivisible p is still necessary. 

6.10 nat Petri Nets, Semaphores 
405 
The use of signals, that is the use of Boolean variables for "waiting by remote control", is obvious 
here. Boolean variables can also be used for branching ("remote control switches"). Moreover, the 
method of describing a process by means of a number of Boolean control variables corresponds to the 
method in 6.6, which leads to sequential circuits. The introduction of signals is a preliminary step in 
this direction. 
If we forget to provide v nonempty in the left section (and v nonfu/1 in the right section) 
blocking occurs: if the left process waits at p nonfu/1 it cannot be set in motion by any ac-
tion (the same holds for p nonempty in the right process). 
Two processes can even block each other simultaneously. Assume that v nonfu/1 in the 
right process is dependent on a condition which can only be produced by the left process. 
Such a blocking is called a deadly embrace. 
Exercise 2: Controlling traffic at road junctions can be considered as a coordination problem of 
parallel processes (road users). 
(a) Apply the terms deadly embrace, critical section, collision and front-end collision 
(comp. 6.7.3) analogously to road traffic and explain what meaning they have. 
(b) State which of the following traffic regulations are free from a deadly embrace and 
describe, if applicable, the deadly embrace situation: 
(1) Road junction with a priority road, 
(2) road junction with a traffic rule "right before left", 
(3) circular traffic in which road users in the circle have priority, 
(4) circular traffic in which road users who want to join the circle have priority. 
(c) Why are deadly embraces tolerated in road traffic and how are they dissolved? 
6.10 nat Petri Nets, Semaphores 
There is also a genuine nat Petri net for the producer-consumer problem in which the 
function of the variable amount is taken over by a place (a "counting" Petri net), Fig. 
6.27. It is symmetrical and results from overlaying two asymmetrical Petri nets of a form 
(Fig. 6.28) which is a basic type of buffering. It occurs (in operating systems) e.g. when 
Fig. 6.27 
Fig. 6.28 

406 
6. Control Elements 
output (of objects of mode 1..) is buffered, if the processor and the output unit are con-
trolled independently and if buffering is not restricted (Fig. 6.29, the buffer is a variable 
for objects of the mode queue A., comp. 5.5). There are everyday instances, such as a 
doctor's waiting room acting as a buffer between reception and treatment. 
Fig. 6.29 
.. give element 
to buffer .. 
0 
«take element 
from buffer" 
In a symmetrical form in the case of restricted buffering we have (Fig. 6.30): 
Fig. 6.30 
.. give element 
to buffer» 
I 
I 
I 
«take element 
from buffer .. 
free-counter 
occupied-counter 
0 
In order to represent the markings of a "counting place" we can again introduce special 
variables, called semaphores33, with the mode indication sema. In contrast to signals, 
they are of the character var nat. Once again, the two permissible operations of this 
module have the designations p and v. 
For an object semaphore of mode sema, 
psemaphore 
can be implemented as 
33 Semaphores which have been used since 1791 as optical telegraphs have generally more than two 
possible wing positions. 

6.10 nat Petri Nets, Semaphores 
407 
f: if semaphore > 0 then semaphore : = semaphore - 1 
else goto f 
fi 
(dynamic waiting) 
and 
v semaphore 
can be implemented as 
semaphore : = semaphore + 
By analogy with signals, the testing and re-setting of a semaphore must be indivisible 
operations here, too. 
For the producer-consumer problem with restricted possibilities of buffering we have 
two counting places free amount and occupied amount, and in the left section we write 
p free amount; «give element to buffer»; v occupied amount 
and in the right section 
p occupied amount; «take element from buffer»; v free amount 
The concepts private, blocking and deadly embrace introduced with the signals can be 
applied analogously to semaphores. 
nat Petri nets and semaphores 34 are often more convenient than boo! Petri nets and 
signals for treating coordination problems. They can, however, be dispensed with 35 
because natural numbers and integers are already available for counting. For cooperation 
and coordination we can make do with the concepts based on boo! Petri nets. There exist 
Petri nets, incidentally, which can only be represented in a somewhat artificial way by ar-
rays of signals or semaphores (Parnas 1972). 
Apart from the system of signals or semaphores discussed above, which uses "test-and-
set" variables (Dekker, comp. Dijkstra 1965), systems have been proposed which make 
more liberal use of coordinating variables ("conditional critical regions", Hoare 1971). 
Program transformations connected with the wait-statement of Hoare are studied in Broy 
1980. 
Kosaraju 1973 has shown that even general Petri nets are not able to handle certain co-
ordination problems. Thus, more recent proposals provide variable-free communication 
mechanisms between parallel processes ("Communicating Sequential Processes", Hoare 
1978, "Distributed Processes", Brinch Hansen 1978). 
34 Dijkstra calls these general semaphores in order to distinguish them from binary semaphores. 
35 Dijkstra (1965): "In this section we shall show the superfluity of the general semaphore and we 
shall do so ... using binary semaphores only." 

408 
Addendum to Chapter 6. Notations 
Rutishauser had already (1952) a counted repetition in the form 
,For j = 1(1) 10: 
h1_ 1 + (a;1 x b1k) ~ h1 
end indexj" 
6. Control Elements 
which, however, had to be recoded before input. The mixture of counted and condition-
dependent repetition in ALGOL 60 leads to the possibility of influencing dynamically the 
course of indexes, which was in general a disadvantage. The pure conditional repetition in 
ALGOL 60 was rejecting and was written 
while !!# do Y; 
a form which found widespread use. Termination by od (revised ALGOL 68) simplifies the 
syntax. The non-rejecting repetition was the most notorious trap in FORTRAN. In 
PASCAL it can be found in the form 
repeat Y until !!# 
There are various suggestions for "(n ++)-loops"; as an adequate notation 0.-J. Dahl 
suggested 
loop: Y; while-, !!#: .r repeat 
which comprises the rejecting or non-rejecting repetition for empty Y or :T. The introduc-
tion of leave or return in the body of routines originates from BCPL and BLISS. 
Co-procedures ("co-routines") were introduced by M. E. Conway in 1963 and became 
known through SIMULA I (1966). 
The do-od-loop is only a special case of an "in-place" definition of a procedure, comp. 
Hehner 1979. 
A linear notation for concurrent computations now begins to develop. With the nota-
tion ( 5"1, .'1'2) ALGOL 68 only permits statements side by side which can be carried out 
sequentially in any order (not necessarily parallel), comp. footnote 19. This somewhat 
wider class has practically no advantage over the parallel construction. 

McCarthy's list structures 
Chapter 7. Organized Storages and Linked Lists 
"Machine language coding is ... the original dark 
craft from which we try to abstract ourselves, but into 
which we all lapse from time to time." 
Turski 1978 
"Von Neumann languages constantly keep our noses 
pressed in the dirt of address computation and the 
separate computation of single words." 
Backus 1978a 
In this chapter an object-like character is conceded to program variables: for example they 
can occur as components of a composition ("organized storages"). Topics discussed are 
consequences of this such as the "generation" of variables, peculiarities with regard to 
identity as well as the possibilities of implementing recursively defined object structures by 
means of organized storages. Forming nexuses (2.14) of variables leads - after transition 
to another semantic model - to the introduction of pointers and the formation of linked 
lists. Finally we will discuss the transition to addresses. These terms lead to a borderline 
across which the domain of machine-oriented (system) programming expands. For further 
information see e.g. Graham 1975. Here we are trying to show primarily that it is not ne-
cessary to start the discussion with a particular machine organization. 
Storage implementation, i.e. the transition from a variable for composite objects of a 
certain mode to a corresponding composition of variables, is an important change of the 
object structure in the direction of conventional computers. But it is a dangerous step as it 
gives access to the storage organization. 
7.1 Organized Storages 
Program variables, as derived from the stack machine and the range-of-values machine, 
have storing properties. We therefore call a set of program variables a storage. The totality 
of all declared variables for an iterative routine or an iterative system is the storage of this 
routine or system. 

410 
7. Organized Storages and Linked Lists 
A set of variables as a unit already occurs as a part of the collective assignment and of 
collective declarations for variables. The parameter list in general contains both variables 
and ordinary, elementary (comp. 2.3) objects. Thus it is only logical to permit composite 
objects to be composed of variables as well as of ordinary objects. 
For the present the notation and meaning introduced in Chap. 2 can be carried over to 
this extension without any problems. In the same way the computational structures discus-
sed in Chap. 3 that are parameterized with x can be built on variables, too; we only need to 
interpret the mode x as var Jlfor some J11• Such structures formed from variables are called 
structured or organized storages. A container philosophy is not necessarily implied, but it 
can be used as an illustration. Due to the alias ban, an object composed of variables may 
be used as a component of a composite object only once. This supports the common 
requirement that the objects constructed be semiregular. 
Some special organized storages have particular names: a stack of variables is called a 
pushdown store (LIFO store), a buffer store 2 (FIFO store) is understood to be a queue of 
variables. Pushdown stores and buffer stores are usually restricted to a certain maximal 
length. The terms are analogous to pushdown and buffer as terms for stack and queue 
variables. 
A file or a roll of variables is called tape store. A (two-side-flexible) array of variables is 
called a linear store, an array of fixed length also a register; the shift-register is simul-
taneously a register and a buffer store of fixed length. An output device in connection with 
a matching input device (e.g. card punch/card reader) can serve as a buffer store. Single 
input or output devices are functionally degenerated buffer stores with read-only or write-
only character. Secondary memories usually function as tape stores or linear stores of 
restricted length according to their construction - as magnetic tape units or as drums and 
disks. 
A composition of variables is an object which (like any other composite object) can be 
given an identifier by a declaration and can also be assigned to a suitable program variable. 
The alias ban means mutatis mutandis that the very same composition of variables may be 
given at most one identifier and cannot be assigned twice. 
7.1.1 Selective Updating 
The components of a composition of variables can of course have objects assigned to them 
just as in the case of other variables. If, for example, a is an object of the mode nat [1 .. 3] 
array var int, we can write 
a[2]: = -8 
this is an ordinary assignment to the "subscripted variable" a [2]. 
An assignment to a component of an object composed of variables is called a selective 
updating of the object. 
In this context it is important that for x no operations other than the universal test for equality are 
assumed. 
2 "buffer" is here used in a narrow sense. 

7.1 Organized Storages 
411 
If we compare the selective updating 
a[/]:= X 
with the complicated routine in 5.5.1 for 
a: [I] =X 
that is for the selective alteration of a component of the content of a variable for a compo-
site object, we recognize that the simplicity of selective updating justifies more than ever 
the introduction of organized, structured stores. From now on we have the opportunity -
by a change of the computational structure - of switching from variables for composite 
objects to compositions of variables and of implementing the selective alteration of the 
content of a variable by subsequent assignments to this variable by means of selective up-
dating. 
There is of course selective updating for all kinds of constructions according to which 
objects can be composed. Invariably only those components can be updated which are va-
riables themselves (and are accessible by a selector). 
Let e.g. c be a one-side-flexible array of variables, that is an object of the mode index 
lfex var int, or d a sequence of variables, an object of the mode sequ var int. Then the 
I-th element of c can be updated, 
sel(c, I):= 17 
provided 
I~ hib(c) 
or both ends of the sequence can be updated, 
(top(d), bottom(d)): = (5, -3) 
provided 
-, isempty(d) h. -, isempty(rest(d)) 
holds. Other components can also be updated here by multi-stepped selection. 
Let e be a roll of variables, a tape store that is an object of the mode roll var int. Then 
the "read/write position" - the "distinguished access position" (2.11.2) - can be updat-
ed, 
joint of e : = 7 
Note, however, that rest(d) or I of e are of course not variables (for sequences) but 
sequences of variables and can therefore not be updated "all at once". 

412 
7. Organized Storages and Linked Lists 
7.1.2 Collec:ting and Composing Variables 
We are thus well-advised to distinguish between a variable for a composite object and a 
corresponding composition of variables. For example 
var nat [1 .. 3] array int 
is a completely different mode to 
nat [1 .. 3] array var int 
An object of the mode nat [1 .. 3] array int can as usual be assigned to a program variable 
x of the mode var nat [1 .. 3] array int: 
X:= (3, -2, -7) 
If, on the other hand, a is an array of variables, e.g. of mode nat [1 .. 3] array var int, 
then the collective assignment 
(a [1], a [2], a [3]) : = (3, -2, -7) 
to the three "subscripted variables" which are components of a is possible, analogous to 
(r, s, t) : = (3, - 2, -7) 
where r, s, tare three variables of the mode var int. However, we cannot write 
a : = (3, -2, -7) 
as a is not an identifier of a variable for objects of mode nat [1 .. 3] array int. For 
conceptual clarity this should not be introduced as a notational abbreviation; we should 
use the notation 
x: = (3, -2, -7 ), 
or more precisely: x: = nat [1 .. 3] array int: (3, -2, -7) 
only in the case of an assignment to a variable x of the mode var nat[1 .. 3] array int. 
In other words: the brackets on the left side of a collective assignment must not be con-
sidered as constructor brackets; we are not concerned here with a composition of variables 
but only with a collection of variables. 
The actualization taboo, however, has to be extended from the collection to the com-
position of variables: Neither one and the same variable nor the same composition of vari-
ables may be incorporated more than once into a composition of variables. 
In many programming languages, however, this essential difference between variables 
for composite objects and compositions of variables is notationally suppressed; confusing 
mixed notations as in the following (inadmissible) construction can be found: 

7.1 Organized Storages 
413 
proc inv = (var nat [1 .. 3] array real a): 
I real rsq = (a[1] x a[1] + a[2] x a[2] + a[3] x a[3]); 
a[1] : = a[1]1rsq; a[2] : = a[2]1rsq; a[3] : = a(3]1rsq 
J 
A routine with such a body is legal if it has an array of three variables as parameter: 
proc invl = (nat [1 .. 3] array var real a): 
I real rsq = (a[1] x a[1] + a[2] x a[2] + a[3] x a[3]); 
a[1] := a(1]1rsq; a[2] := a(2]1rsq; a[3] := a[3]1rsq 
J 
and a routine with such a heading is legal if the assignment to the variable occurs collective-
ly: 
proc inv2 = (var nat [1 .. 3] array real a): 
I real rsq = a[1] x a[1] + a[2] x a[2] + a[3] 
a:= (a(1]1rsq, a[2]1rsq, a(3]1rsq) 
x a[3]; 
J 
The mixed form above understandably makes the programmer feel insecure; it cannot 
occur if the val-specification is not suppressed, as a[1], according to the mode specifica-
tion, stands for val(a[1]) at the right-hand sides in invl, whereas (val a)[1] is meant in 
inv2. 
In programming languages completely geared to program variables, such as ALGOL 
60, the distinction between variables for composite objects and organized stores is not only 
notationally but also conceptually blurred. This simplicity, however, complicates the 
transition between applicative and procedural programming. 
7.1.3 Computed Variables 
In permitting variables as components of composite objects, the variables themselves are 
conceded a certain object-like character. They can now occur as results of operations, 
namely selections, and thus they can be computed from a variable form. 
Let a again be of the mode nat [1 .. 3] array var int, then e.g. a [53x,:i-_1iJ fori = 1 is of 
course the "subscripted variable" a[2]. 
The next step would be to permit variables as a result of other operations, e.g. as are-
sult of a branching in the variable form 
if PA then x else y fi : = 3, 
which appears as a harmless notational abbreviation for 
if PA then x : = 3 else y : = 3 fi 3 
3 It would not help to forbid such a construction, as this restriction could always be bypassed by 
using compositions of the mode bool array var 11· 

414 
7. Organized Storages and Linked Lists 
In view of the actualization taboo great care should of course be taken with computed 
variables: two variable forms, outwardly different, could denote the same variable. The 
collective assignment (comp. 7.1.1) 
(top(d), bottom(d)): = (5, -3) 
violates the actualization taboo if length (d) = 1. 
Extending the alias ban to different variable forms, which could yield the same 
variable, would lead to intolerable restrictions. On the level of organized stores there is no 
longer a mechanical way of verifying the actualization taboo, one has to live with the risk 
of violating it unintentionally and is therefore forced to individual justification. If this is 
neglected, working with organized stores will almost certainly lead to programming 
mistakes. 
The introduction of computed variables also leads to other difficulties. In 5.3.2 we con-
sidered a "harmless" procedure for exchanging the contents of two variables, 
proc exchO = (var 1.. s, var 1.. t): (s, t) : = (t, s) 
whose body can be completely sequentialized according to the considerations in 5.2.4: 
proc exch2 = (var /.. s, var /.. t): I /.. H1 = t; t : = s; s: = H1 J 
Note that both exchO and exch2 are symmetrical in both parameters. The calls 
exch2(a[i + 1], i) and exch2(i, a[i + 1]) 
with textual substitution now yield the sequences 
/..H1 = i; i:= a[i + 1]; a[i + 1] := H1 or 
I..H1 = a[i + 1]; a[i + 1] := i; i:= H 1 
where the former however does not yield the expected result. The reason for this is that a 
variable is required as a parameter, yet a[i + 1] is only a variable form, i.e. an expression 
for computing a variable. If instead of exch2(a[i + 1], i) we use the construction 
/..R = i + 1; exch2(a[R], i) 
not only is there a clear statement of what is meant, but the result is also the same as from 
/..R = i + 1; exch2(i, a[R]) 
and from 
/..R = i + 1; exchO(a[R], i) 
If variable forms are permitted at all in positions of variable parameters the semantics 
of calls must be extended so that the actual variable is the first to be computed4• 
In the above case of an array the index expression must be evaluated first. 
4 In machine-oriented programming this is done by "call by reference". 

7.1 Organized Storages 
415 
7.1.4 Constructing Organized Storages and Generating Variables 
We might suppose that an organized storage, such as a sequence or a flexible array of vari-
ables, could be constructed by employing variables already declared. 
If again r, s and t are already declared variables of mode var int, then a simple 
construction such as 
nat [1 .. 3] array var int: (r, s, t ) 
obviously does not violate the actualization taboo. The construction of recursive object 
structures, however, presents further difficulties: let c be a one-side-flexible array of 
mode index flex var int or d a sequence of mode sequ var int. Then cord in alt(c, I, r) 
or in append(d, r) can themselves have already been formed by using the variable r- the 
actualization taboo disallows this case, but checking it is extremely difficult. 
Thus it would be logical to forbid completely the use of already declared variables in 
constructors (and in other expressions which can lead to the computation of variables, as 
e.g. the branchings above). Hence no named variable can occur as the result of a computa-
tion. How, then, can organized storages be constructed? 
7.1.4.1 The solution is as follows: variables must be introduced only when they are needed 
in the constructor and must be generated exactly where they are needed. They do not have 
to be given a name as they can be addressed by the selectors anyway. Thus the result of a 
computation of variables can only be an unnamed variable. 
Anonymous variables are therefore introduced which are "generated" by a special 
operation: by means of a construction 
newvar 11 : = >object< 
which corresponds to an initialized variable declaration -
a new variable (different 
from all existing ones) for objects of mode 11is generated (Julian Green, 1959) and as a rule 
is immediately initialized. If newvar 11: = >object< occurs in a constructor, the generated 
variable becomes a component of the composition. Thus e.g. a flexible array c of variables 
or a sequence d of variables can be extended by means of 
ext(c, newvar int : = 13) 
or 
append(d, newvar int : = 5) 
A fixed array of variables, too, can be formed explicitly by 
(newvar int : = 3, newvar int : = -2, newvar int : = -7) 
For another reason this solution is advantageous or may even be indispensable: For 
example when constructing very large arrays (such as of the mode int [1 .. 2 i10] array 
var int) or sequences of variables it is very often troublesome, if not impossible, to have a 
sufficient supply of variables with freely-chosen identifiers which can then be composed. 

416 
7. Organized Storages and Linked Lists 
To this end it is necessary to allow routines which build and also can yield compositions 
of (anonymous) variables. For example, we can now form a large fixed array (using the 
constructs from 2.15 .1) by 
initial ( n, m) 
where 
proc initial = (int n, int m) int [n .. m] array var x: 
if n > m then 0 
else (initial(n, m -
1), newvar x: = (') ) fi 
The variables of the array are initialized with the insignificant value ro (comp. 5.3.4), that 
is, following a customary bad habit, they are not really initialized. 
It is important that the generated variables are always pairwise different, although they 
are anonymous and are not distinguishable by a freely-chosen identifier. 
As anonymous variables have no identifier their scope does not result from the range of 
binding of an identifier. Supplementing 5.3.5 we thus define: 
Anonymous variables produced as components in composite results of a routine must have 
(at least) the range of the identifier of this routine as their lifetime. (Here, a block is to be 
considered as a routine which is called "in-place".) 
In this way an anonymous variable can be "transported outside" several times, extend-
ing its lifetime accordingly . 
Anonymous variables as components of a composition which is assigned to a variable 
u, or given by an object declaration the identifier u, must be ascribed a lifetime which is (at 
least) the range of binding of u -
in order to avoid "dangling references". Some pro-
gramming languages give an unbounded life time to anonymous variables (heap-operator 
in ALGOL 68). 
Thus newvar means an allocation of storage, whereas the storage release is controlled 
by the scope of the generated variables. Dynamic storage allocation to be performed by a 
compiler transforming applicative or procedural constructions into machine language can 
be described as such a construction process. Providing the variable-generation operator 
thus means that the programmer gains access to storage allocation. 
The following examples show the construction of flexible arrays and sequences of vari-
ables. 
Example: Construction of a flexible array index flex var "of N variables and initializa-
tion with values of a function g: 
proc tabg = (nat N, funct (nat) "g) index flex var ": 
if N = 0 then init 
else ext(tabg(N- 1, g), newvar ": = g(N)) fi 
By function inversion we obtain the iterative form 
proc tabg = (nat N, funct (nat) "g) index flex var ": 
r (var index flex var "vz, var nat n) : = (init, 0); 
while n ~ N do (vz. n) : = (ext(vz, newvar ": = g(n)), n + 1) od; 
~ 
J 

7.1 Organized Storages 
417 
with a naturally introduced program variable vz for a flexible array of variables. 
Example: Construction of a sequence sequ var 11 of N variables and initialization with 
values of a function f: 
proc tab! = (nat N, funct (nat) 11./) sequ var 11: 
if N = 0 then empty 
else append(tabf(N- 1, .f), newvar 11: = f(N)) fi 
7.1.4.2 To illustrate organized storages we use storage diagrams, i.e. diagrams in the sense 
of 2.9 .1 in which the variables are represented by (possibly mode-specific) little boxes; for 
a left sequence of variables various forms are given in Fig. 7.1, where every box stands for 
a variable of the basic mode. 
Fig. 7.1 
The call tabf(5, (nat x) nat: xl2)) yields a result which has the storage diagrams of 
Fig. 7.2. 
Fig. 7.2 
7.1.4.3 Object declarations for compositions of (initialized) generated variables are also 
possible (comp. 7.1.2): 
nat [1 .. 3] array var int a = (newvar int : = 3, 
newvar int : = -2, 
newvar int : = 
- 7) 
In the case of very large arrays e.g. of mode int [1 .. 2 l 10] array var int the generating 
function initial from above can be used: 

418 
7. Organized Storages and Linked Lists 
int [n .. m] array var "/.. g = initial(n, m) 
In fact this function is always used implicitly in ALGOL 60: A declaration 
integer array g(n: m] in ALGOL 60 corresponds exactly to int [n .. m] array var int 
g = initial(n, m); the dynamic storage allocation (i.e. the totality of the newvar-opera-
tions) is concealed behind an abbreviated notation 5• In this connection Rutishauser men-
tioned "keeping Pandora's box closed". In ALGOL 60 no organized storages apart from 
arrays are available. It is thus quite easy there to dispense completely with the explicit 
newvar-operator. In systems programming languages we cannot get away without in-
fluencing storage allocation. We will meet a safe treatment of the newvar-operator in con-
nection with pointers in 7 .4.2. 
Object declarations for compositions of variables are of course not restricted to (fixed) 
arrays. Thus c and d above could have been introduced by 
index flex var int c = ext(init, newvar int : = 18) or 
sequ var int d 
= append(empty, newvar int: = 6) 
Consequent constructions, too, are 
index flex var int c = init and 
sequ var int d 
=empty 
7.1.5 Advantages and Disadvantages of Organized Storages 
Selective alteration of a component is a complicated operation not only in the case of 
arrays but also in sequences and flexible arrays of variables, whereas the selective updating 
of a single variable of a sequence or a flexible array of variables requires no more than the 
selection of the component and subsequent updating. The conceptual difference between 
selective alteration and selective updating should not be blurred -
particularly for this 
reason. 
It would now be logical to ask why we do not pass straight away to organized storages 
in which the access operations are simpler. There are three reasons for not doing so: 
First of all the level of the organized storages presents conceptual complications. These 
have been referred to above under the keyphrase "anonymous variables". 
Secondly, during program development the concept of variables for a composite object 
occurs if we proceed from recursive definitions. These, however, are exactly tailored to the 
complicated (recursive) object structures. The transition from variables for composite ob-
jects to organized storages is really an implementation step. 
The problem of identity of variables is especially irksome in organized storages. This 
has caused Dijkstra 1976 to break away from the ALGOL 60-oriented interpretation of an 
"array" as a "finite set of elementary, consecutively numbered variables". We will go into 
this in 7 .2. In some comparatively machine-oriented programming languages, such as 
5 This of course is also the case when we initialize, for example when we write var int x: = 3 briefly 
instead of var int x "" (newvar: = 3). (This corresponds to the abbreviation int x: = 3 for ref int 
x = loc int : = 3 in ALGOL 68.) 

7.2 Identity of Variables and Alias Ban Revisited 
419 
ALGOL 60 in which only variables occur, linear stores are the only means available for 
structuring objects 6 • Such a restraint not only hampers program development but it also 
tempts one to program dangerously. 
Another question is whether we really need to distinguish composite objects from organized 
storages. Hoare, Wirth and others at first denied this, it was one of the reasons why they objected to 
ALGOL 68. Hoare's record (1965) is formed on the level of the organized storage. Today, in the light 
of an algebraic theory, it appears obvious that both views are necessary. 
7.2 Identity of Variables and Alias Ban Revisited 
While constructing organized storages, the restrictions imposed - compositions of varia-
bles may be used at most once for the construction of other composite objects, and only 
anonymous variables are allowed -
could not completely guarantee observance of the 
alias ban. In the case of objects occurring as selectors, for example in fixed and flexible ar-
rays (and in general in any abstract computational structures) the selection can be comput-
ed. Different expressions can yield the same value and thus denote the same variable. 
Checking the actualization taboo seems impossible if we do not want to forbid that a[i] 
and a[k] can occur beside each other, where i and k are variables themselves. 
7.2.1 Revision of the Assignment Axiom 
It is also a prerequisite for the validity of the classical assignment axiom (5.4) that all varia-
bles occurring are different variables, that computed variables in particular are not 
identical. In connection with "subscripted variables" Hoare and Gries have already hinted 
at this. Dijkstra stated this problem clearly: "In the axiomatic definition of the assignment 
statement ... one cannot afford - as in, I guess, all parts of logic - any uncertainty as to 
whether two variables are the same or not" (Dijkstra 1976). 
In the case of computed "subscripted variables" Hoare's assignment axiom must be 
changed, it must be subtly supplemented. 
The precise version of the assignment axiom for "subscripted variables" (Gries 1978) 
reads for determinate If: 
wp(x[r]: = If I rJt (x[stJ, x[s2], ••• x[sml]) 
rJt (if r = s1 then rff else x [stl fi, if r = s2 then rff else x [s2] fi, ... , 
if r = sm then rff else x[sml fiJ 
6 In ALGOL 60, however, addressing (i.e. the uniform implementation in one single, linear store) is 
inaccessible. 

420 
Example: 
(1) For x[r] : = 5 and the postcondition 
P[x[il] = x!iF = 25 
the weakest precondition is 
P [if r = i then 5 else x [i] fi] , that is 
if r = i then 25 = 25 else x[ij2 = 25 fi 
or 
r = i v (r * i A x[i] 2 = 25) 
or 
r * i => x [i] 2 = 25 
(2) For x[r] : = 5 and the postcondition 
P[x(r], xul] = x(rF =xU] 
the·weakest precondition is 
7. Organized Storages and Linked Lists 
P [if r = r then 5 else x[r] fi, if r = j then 5 else xU] fi], that is 
25 = (If r = j then 5 else xU] fi) 
or 
if r = j then 25 = 5 else 25 = xU] fi 
or 
r * j A 25 = xU] 
Such precautions must be taken with all kinds of constructions according to which vari-
ables are composed, especially with the technically important flexible arrays and ag-
gregates of variables as well as pushdown stores and buffer stores. 
It is clear that with such a modified, more complicated assignment axiom some 
advantages deriving from the use of organized storages seem to dwindle. This situation is 
quite significant: a handy selection of concepts gives the illusion of convenient application, 
the disagreeable part is yet to come (during the verifying process). 
Branching as a variable form also requires an adjustment of the assignment axiom: 
wp(if 91' thenxelsey fi: = c 1 qJ' [x, yJ) = (91' .... qJ' [c, yJ) A (I 91' .... qJ' [x, or]) 
7 .2.2 Checking the Actualization Taboo 
In order to verify the observance of the actualization taboo, it is advisable to require that 
operations yielding variables as results be injective: then equal results are only obtained 
when the parameters are equally actualized. This is the case for an array of variables; the 
variable forms a[i] and a[k] compute the same variable if and only if i = k holds. 
For example, if an exchange of contents of the i-th and the k-th components of an ar-
ray of variables or a rotation in the plane of the i-th and k-th axes for an array of variables 
is to be accomplished, it is necessary but also sufficient to put this under the guard i * k, 
that is (comp. 5.3.2) 

7.2 Identity of Variables and Alias Ban Revisited 
421 
if i * k then rot(a[i], a[k]) else abort fi 
or 
(•) if i * k then exchO(a[i], a[k]) else skip fi 
One could think of introducing the literal comparison of variables - while stressing the 
object character of variables - as an algorithmic operation 
x:=:y 
in order to soften the actualization taboo: If we were to write the example of the exchange 
of contents from 5.3.2 as follows: 
proc exchll = (var "s, var " t): 
if s : = : t then skip 
else (s, t) : = (t, s) fi 
there would be no formal objections to a call exchll (a[i], a[k]) if i = k. However, this 
version is less efficient in certain circumstances than exchl (a [i], a [k]). It seems to be just a 
question of convenience to dispense with exchl and exchll and instead to write the more 
cumbersome but more efficient construction - by comparison to ( *) 
(**) if a[i] * a[k] then exchO(a[i], a[k]) else skip fi 
which likewise clearly shows the facts and obviates the comparison of variables. 
Furthermore, parameterization is possible for this construction if the index mode of 
the array from which the variables a[i] originate is constant or is carried along as a para-
meter. Thus we can introduce e.g. 
proc arrayexch = (v array var "a, vi, v k): 
if a[i] * a[k] then (a[i], a[k]): = (a[k], a[i]) else skip fi 
and thereby obtain the effect of the construction(**) above by means of the call 
arrayexch(a, i, k) 
which describes its essence. 
The importance of the property of semiregularity of object structures (2.14) can now be 
clearly seen: for structures made up of variables 7, semiregularity guarantees precisely the 
injectivity of the variable form and thus the means of checking the identity of variables by 
comparison of selectors. 
The finite recursive object structures considered in Chap. 2, in particular, are semi-
regular. If the actualization taboo for the constructor of these is required, a modified as-
signment axiom, such as the one given above for arrays of variables, can likewise be ex-
plicitly formulated. 
Non-finite recursive object structures are not necessarily semiregular (we will deal with 
their implementation in 7.4). Moreover the mode sequ "' where top(d) and bottom(d) 
may coincide, is not semiregular. 
7 It can be presumed that Turski - along the lines of Wirth and Hoare - assumes this without say-
ing so. 

422 
7. Organized Storages and Linked Lists 
The presence of such "equations" is indeed significant for the construction of abstract 
computational structures. Operations forming terms from variables now work as 
constructors, those reducing terms work as selectors. If, once again, the actualization 
taboo is required for constructor operations, then modified versions of the assignment 
axiom can be given explicitly only for those variable forms which use solely injective 
selector operations. 
The equal status of variables as objects is in any case considerably impaired by the 
actualization taboo and the alias ban which serves to observe it but which is also important 
for the assignment axiom. Selective updating on the one hand and the alias ban on the 
other show the glory and the misery of working with organized storages, so characteristic 
for systems programming. Programming languages like CLU and ALP HARD which allow 
abstract computational structures only on the basis of variables are hampered by them-
selves. 
7.3 Implementing Object Structures by Organized Storages 
7.3.1 (Homologous) storage implementation, that is the implementation of object struc-
tures by organized storages of the same construction principle, ultimately means the inter-
change of the operations of taking the content and selection: if in the case of a variable for 
a composite object a val-operator is applied first and then the selection operator, then in 
an organized storage the corresponding selection operator is applied before the val-
operator. 
The implementation of a variable for (fixed) arrays by arrays of variables is simple. If a 
of mode var v array Jl is replaced by f1 of mode v array var Jl, then an access such as 
(val a) [I] is replaced by an access val (f1 [/]), and a selective alteration a: [I] = Xis replac-
ed by a single assignment f1 [I] : = X. If the val-operation is suppressed there is no nota-
tional difference in the first example. 
Thus (comp. 7.1.4) 
nat [1 .. 3] array var int f1 -
(newvar int : = 3, newvar int : = -2, 
newvar lnt : = - 7 > 
is an implementation of 
var nat [1 .. 3] array int a:= (3, -2, -7) 
and (comp. 7.1.1) 
a [21 : = -s 
is an implementation of 
a:= (a[1], -8, a[3]) 
In ALGOL 68, too, one is not completely free from the dominance of the organized-storage idea. 
This is shown by the special rules for dereferencing and particularly by the actual exchangeability of 

7. 3 Implementing Object Structures by Organized Storages 
423 
val-operation and selection. Thus in ALGOL 68 a [2) : = - 8 can -
misleadingly - be written not 
only when a is of the mode nat [1 .. 3] array var int but also when a is of the mode var nat [1 .. 3] 
array int. 
Similarly, in the case of a storage implementation of sequences, for example, a 
sequence of variables is introduced. If a of mode var sequ x is replaced by a of the mode 
sequ var x. then the access top(val a) is replaced by the access val top(a). The 
constructor operation 
append(val a, X) 
requires a transition to storage allocation 
append(a, newvar x: = X) 
Replacement of rest(val a) by rest(a) (note: val rest(a) would be pointless!) is illustrat-
ed by the following implementation of a routine for determining the i-th component of a 
sequence stored in a variable a: 
funct set "' (var sequ x a, pnat i: i ::s tength(val a)) x: 
if i = 1 then top (val a) else set(rest(val a), i -
1) fi 
is implemented by 
funct set "' (sequ var x a, pnat i: i ::s tength(a)) x: 
if i = 1 then val top(a) else set(rest(a), i - 1) fi 
We proceed similarly with the storage implementation of flexible arrays and ag-
gregates; storage allocation is necessary here for ext or hiext, toext as well as for put. 
7.3.2 Generally the storage implementation of abstract computational structures leads to 
the use of the newvar-operator for all term constructor operations. This could mean that 
more storage space will continually be needed. On the other hand no premature storage 
release can occur in the storage implementation treated so far. This will only be possible 
when (camp. tabg in 7 .1.4) program variables for structures of (anonymous) variables are 
introduced. The procedures below e.g. correspond to the procedures with sequence varia-
bles push, pop and trunc (camp. 5.5) 
proc pilsh "' (var sequ var 11 va, 11 U): 
va: = append(va, new var 11: = U) 
proc pop 
"' (var sequ var 11 va): 
va : = rest ( va) 
proc trCmc "" (var sequ var 11 va): 
va: = upper(va) 
In pop and trCmc the new content of va is the old one reduced by one (variable) com-
ponent. The variable created last (LIFO) or the variable created first (FIFO) disappear. 
The corresponding storage can even be released immediately if val va cannot be assigned 
to another variable - i.e. if the alias ban is observed. 

424 
7. Organized Storages and Linked Lists 
Variables for arrays of variables are also necessary for the storage implementation of 
arrays with computed bounds. 
7.3.3 Storage implementations that comprise a change of the computational structure and 
a homologous implementation are also interesting. For flexible arrays of a limited length 
and aggregates, for example, the implementation of corresponding variables as registers or 
as linear stores is of great importance. 
Thus, for example, the following correspondences result for an implementation of 
AGREX(int (n .. m], X) according to 3.6.2.3, where a of mode var int [n .. m] grex xis 
replaced by li of mode int [n .. m] array var x: 
var int [n .. m] grex x a : = vac 
(val a) [J] 
.... 
int [n .. m] array var x li = initial(n, m) 8, 
... 
val (/i [/]), 
a : = put(val a, L X) 
... li(J]: = X 
The last correspondence, in particular, is of practical importance for "machine-orient-
ed" working as it replaces a complicated operation by simple selective updating. 
Exercise 1: Following the example of the routine initial give a procedure which permits the formula-
tion of the construct corresponding to a : = vac. 
For the example of the bounded stack (3.1.3.3) an implementation with aggregates 
(3.6.3) leads to a storage implementation by means of a pair of an "internal" level variable 
with the selector i and a "working register" with the selector a. Here 
b of mode var bs x is replaced by 
b of mode (var nat [0 .. N] i, nat [1 .. N] array var x a) 
The following correspondences hold for the most important operations: 
var bs x b: = empty ... (var nat [0 .. N], nat [1 .. N] array var X) b = 
(0, initia/(1, N)) 
isempty(val b) 
isfu//(val b) 
top(val b) 
... val (i of b) = 0 
.... val (i of b) = N 
.... val (a of b (i of b]) 
Instead of append and rest it is better to use 
push(b, U) 
pop (b) 
.... I i of E: = succ(i of b); a of b (i of b] : = U J 
... i of b : = pred (i of b) 
In programming languages where the difference between a selector within a 
composition of variables and a variable is notationally disregarded and thus is 
conceptually blurred, the selector i in our example could be wrongly called a level variable 
- in fact if only selects the (anonymous) level variable, which is thus completely hidden. 
8 The correspondence isaccessible(a, I) ... val (a[/]) * (l) does not hold! To make it valid(l)must be 
replaced by a fixed special element L. 

7.4 Linked-List Implementation of Organized Storages 
425 
In 3.6.2.3 we referred to the importance of adding an operation truncshift to a com-
putational structure of the abstract type FLEX (3.3.1); for queues of limited length, 
truncshift permits an implementation by FLEX with a restricted domain of hib and accord-
ingly for decks and sequences of limited length it permits an implementation by BIFLEX 
with a restricted domain of the difference hib - lob. Similarly, buffers of limited length 
are implemented by arrays of a fixed number of variables, i.e. by registers. 
For example, the operation of "left shift" must now be implemented efficiently; usual-
ly this is done by a collective left shift of the variable contents, 
(a [1], a[2], ... , a[N - 1]) : = (a[2], a[3], ... , a[N]) 
A register nat [1 .. N] array var x for which this operation is available is called a shift 
register. Registers and shift registers of the mode nat [1 .. N] array var bit are in 
particular used to describe circuits ("register transfer languages"). 
A sequential implementation is 
for i from 1 to N -
1 do a [i] : = a [i + 1] od 
7.4 Linked-List Implementation of Organized Storages 
In the previous section organized storages have been defined recursively in an abstract 
manner. The methods in 2.14 can. also be applied to these cases. Nexuses of variables yield 
linked list implementations of organized storages as they are available, e.g., in PASCAL. 
The operator newvar can safely be combined with the construction of linked lists. 
7.4.1 References to Variables: Pointers 
"There appears to be a close analogy between 
references in data and jumps in a program." 
Hoare 1973 
Deparameterized routines with lazy evaluation in 2.14 were applied to normal objects. 
Such a formation of nexuses can also be carried out for structures constructed from 
variables. Thus from the routine convert of 2.14.2 there results an implementation of left 
sequences of variables: 
funct convert =(nat a) lsequ var bit: 
if a= Othen 0 
else 
r jwhere 
lazy functj = lsequ var bit: 
if even (a) then lsequ var bit: (newvar bit:= 0, convert(a/2)) 
0 odd(a) then lsequ var bit: (newvar bit:= L, convert((a - 1)12)) fi J fi 

426 
7. Organized Storages and Linked Lists 
The essential change by comparison to 2.14 is that the constructor lsequ var bit: is 
supplemented with the generation of a variable by the operator newvar bit. 
7.4.1.1 Now there is a new situation: The storage implementation suggests a container 
philosophy 9 in which inserting an expression is re-interpreted as referring to the expression 
(while simultaneously reversing the direction of the arrow). Thus the object diagram of 
2.14 (Fig. 7 .3) is first replaced by the storage diagram (Fig. 7 .4) and then re-interpreted (by 
reversing the direction of the arrows) (Fig. 7 .5). 
tl1) 
f12) 
tl31 
tl41 
'(L,~(O.~(L.~(L,O) 
Fig. 7.3 
fl1) 
tr21 
tl31 
tl41 
'((ff~-~tfuJ-.-~rm~-~rttf.-~') 
'------" 
, _______ , 
, _______ , 
, _______ , 
Fig. 7.4 
Fig. 7.5 
The new arrows are called references. We met a similar situation in 6.2 when introduc-
ing jumps. In 6.2 the call of a deparameterized routine became a jump, where the direction 
of the jump was likewise opposite to the direction of substitution; in this case the call of a 
deparameterized routine with lazy evaluation becomes a reference, and -
as we are 
dealing with organized storages - it becomes a reference to a composition of variables, a 
pointer or link. Naturally pointers are mode-specific. 
The particular advantage of introducing pointers lies in the fact that with their help the 
same composition of variables can be referred to from several places, whereas direct 
multiple incorporation of the same variable is forbidden by the actualization taboo -
apart from the fact that when using the container concept one also wants to avoid duplica-
tion of containers for reasons of economy. 
In the reference concept of ALGOL 68 the terms "reference" and "variable" are mixed. However 
the general reference concept used has no advantage over the strictly applicative concept of the rou-
tine with lazy evaluation; on the other hand a restriction to pointers in the original sense of C.A.R. 
Hoare is quite appropriate for machine-oriented programming. 
When working with organized storages it is natural to go a step further and to provide 
containers for pointers, too, according to the diagram of Fig. 7 .6. 
9 val acquires the meaning of an operator yielding the content of a container. 

7.4 Linked-List Implementation of Organized Storages 
427 
0-,m(~~(®~~(~~(~~~~) 
f121 
f131 
tllll 
Fig. 7.6 
This means that variables are introduced for pointers, too, (pointer variables) and that 
such pointer variables occur in compositions when constructing organized storages. The 
individual components of this kind of organized storage are then "records" in the sense in 
which Hoare originally used this term. They are compounds composed of variables and 
pointer variables. The entire composite object is implemented by a linked list of such com-
pounds. Compounds containing pointer variables are thus called list-forming compounds 
or records; the pointers form the list, they are stored in the pointer variables. 
Linked lists formed from records that contain exactly one or exactly two pointer varia-
bles are called (linked) one-way lists or two-way lists, respectively. 
7.4.1.2 A minor change of convert into 
funct convert = (nat a) lsequ var bit: 
1 jwhere 
lazy funct f = lsequ var bit: 
if a = 0 then 0 
else if even(a) then .. . 
D odd(a) then ... fi fi J 
leads to Fig. 7.7 and thus after re-interpretation as a storage diagram to a linked list with 
uniform records (Fig. 7.8). 
,,, ) 
[121 
f13) 
tllll 
fl51 
'(L-,-~(o-,-~(L-,-~(L-~~ o 
... 
" 
' 
, 
... 
, 
... 
, 
Fig. 7.7 
Fig. 7.8 
The frequently occurring pointer to the 0-tuple 0 is universally represented by nil. 
Thus we also have Fig. 7.9. 
0-,m((©~~(@~~((gtr(~~@.) 
fW 
tl31 
f141 
Fig. 7.9 

428 
7. Organized Storages and Linked Lists 
7.4.1.3 The introduction of pointers in connection with the container philosophy of varia-
bles means changing over to a well-known semantic model: (recursively defined) composi-
tions of variables are abandoned for linked lists -
formed from references and records 
comprising pointer variables and common variables; the linked list replaces the (finitary) 
nexus. This coincides (as in the case of jumps) with a change of notation: pt "designates 
the mode of the pointers to objects of the mode Jl· The declaration of a function with lazy 
evaluation 
lazy funct >function identifier< = >result-mode specification<: VVVVVVVVVV' 
is replaced by the pointer declaration 
pt >result-mode specification< >pointer identifier< =.rvvvvvvvvv-
in the form of a (pointer) object declaration introducing range of binding and scope for the 
declared pointer as in 1.13.3. 
In our example (with bit instead of X) lsequ var xis represented by pointers either to 
the 0-tuple or to a pair of variables. 
If we now write pt sx instead of lsequ var x we obtain the following implementation 
by one-way lists: lsequ var xis replaced by pt sx where 
mode sx = empty I (var x key, var pt sx next) 10 
Altogether 
lsequ var x 
is replaced by 
{nil} I pt (var x key, var pt sx next) 
where nil, the universal pointer to the 0-tuple, is formally of the mode pt empty. 
The constructor 
lsequ var x: 
thus becomes (by placing new before pt) 
newpt sx: 
This can be viewed as the generation of a new pointer to an object of the mode sx. 
deref S denotes that composition of variables which is referred to by the pointerS. 
7.4.1.4 Now the rewritten routine convert* explicitly delivers pointers; it reads (by replac-
ing 0 by nil where necessary) 
10 The next-component is not of mode pt S)( corresponding to lsequ var x but of mode var pt sx 
because we have provided containers for pointers. 

7.4 Linked-List Implementation of Organized Storages 
funet convert* = (nat a) pt sbit: 
r fwhere 
pt sbitf = 
if a = 0 then nil 
else if even(a) then 
newpt sbit: (newvar bit : = 0, newvar pi sbit : = convert*(a/2)) 
D odd(a) then 
429 
newpt sbit: (newvar bit:= L, newvar pt sbit: = convert*((a - 1)/2)) fi fi J 
Here the newly-generated pointer is given the identifier f; to be more exact, f(tl, f(2l, f(ll, 
f(4l, and.f5l = nil in our example denote the four or rather five pointers that are formed in 
the individual incarnations of convert*. 
On the other hand we could work with anonymous pointers; unfolding of /results in 
funet convert* = (nat a) pi sbit: 
if a = 0 then nil 
else if even(a) then 
newpt sbit: (newvar bit : = 0, newvar pt sbit : = convert*(a/2)) 
D odd(a) then 
newpt sbit: (newvar bit:= L, newvar pt sbit: = convert*((a - 1)/2)) fi fi 
Analogously, from the routine transit of 2.14.2 a routine results which constructs two-
way lists as linked lists of records with three components of the mode 
mode e:x. = empty l(var pt ex/eft, var x node, var pt ex right) 
funet transit* = (lsequ X a) pt ex: f trans*(a, nil) where 
funet trans* = (lsequ x a, pt ex z) pt ex: 
if a = 0 then nil else 
f /Where 
ptexf = newptex: (newvarptex:= z, 
newvar x : = item of a, 
newvar pt ex:= g), 
pt ex g = trans*(trunk of a, f) 
J fi J 
In this example - because infinite objects are constructed - it is no longer possible to 
dispense completely with freely-chosen (auxiliary) identifiers for pointers. However one 
pointer can be eliminated explicitly. We obtain 
r fwhere 
pt exf = newpt ex: (newvar pt ex:= z, 
newvar x: = item of a, 
newvar pt ex : = trans*(trunk of a, f)) J 
Both convert* and trans* have a pointer as result. Parallel to 7.1.4 we state: 
Pointers which are results of a routine are also constrained to the range of binding of 
the identifier of this routine. 

430 
7. Organized Storages and Linked Lists 
The "life-time" of the (anonymous) variables to which they refer is likewise defined to 
be this range. 
With regard to the actualization taboo it is important to establish that pointers generat-
ed in different incarnations of a routine - resembling routines declared in different incar-
nations - are different from each other, even if they have the same identifier. 
7.4.1.5 The last two examples showed that a change of mode from recursively defined ob-
jects to linked lists and rewriting of the pertinent applicative routines, formulated with 
lazy evaluation, takes place in two steps: firstly transition to compositions of variables and 
then introduction of pointers to these. A strictly formal treatment lies outside the scope of 
this book. However we will show this complete transition using the following example: 
A solution to exercise 2.14.2-2 (with bit instead of i) is to produce a ring-list by means 
of 
funct ring = (lsequ bit 1: I * 0) lsequ bit: 
I head where 
lazy funct head = lsequ bit: c/(1), 
funct cl = (lsequ bit 1: I * 0) lsequ bit: 
if trunk ol I = 0 then lsequ bit: (item of I, head) 
else jlsequ bit: (item of I, f> where 
lazy functj = lsequ bit: c/(trunk of I) J II J 
The transition now to be undertaken consists in replacing lsequ bit first by lsequ var bit 
and then by pt sbit where the latter is defined as above. In addition the following selectors 
are replaced: 
item of . 
by key ol deref . and 
trunk ol . by next ol deref . 
Moreover, 0 is replaced by nil. 
Constructors lsequ bit: (. , . ) are first replaced by 
lsequ var bit: (newvar bit : = . , . > 
and then by 
newpt sbit : (newvar bit : = . , newvar pt sbit : = . > 
Finally, lazy funct . = lsequ bit: 
is replaced first by 
lazy funct . = lsequ var bit: 
and then by 
pt sbit. = 

7.4 Linked-List Implementation of Organized Storages 
Altogether we obtain 
funct ring* = (pi sbit 1: I * nil) pt sbit: 
r headwhere 
pi sbit head= cl(l}, 
funct cl = (pi sbit 1: I * nil) pt sbit: 
if next of deref I = nil then newpt sbit: (newvar bit : = key of deref I, 
newvar pt sbit: = head) 
else r newpt sbit: 
(newvar bit : = key of de ref I, 
newvar pt sbit : = f> 
where 
431 
pt sbitj = cl(next of deref I) J 
fi J . 
7.4.1.6 Pure Records are list-forming records which consist exclusively of pointer varia-
bles. Hoare originally considered only such records 11 • Without loss of generality we can 
always make do with pure records by replacing all components of a list-forming record, 
which are not pointer variables, by pointer variables of a corresponding mode and by 
linking the original component-variables to the record by means of additional pointers. 
This seems at first to be unnecessary work. However, the distinction between storage for 
the object structure (expressed by records of pointer variables) and storage for the objects 
proper is a clear design principle. Transition from recursive object structure to linked-list 
implementation by pure records is completely mechanical. For this reason this kind of 
implementation is often used in compilers. A linked-list implementation can also serve as a 
semantic model for a computation using the delay rule of 1.14.3 (Wadsworth 1971). 
7.4.2 Wirth's Connection 
It is a question of notational economy whether or not we want to abbreviate the preceding 
notation which reflects the process of storage allocation followed by the generation of 
· pointers in a precise operational manner. For the declaration off in 7 .4.1.4 one can also 
simply write 
pt sbitj = if a = 0 then nil 
else if even(a) then newpt: (0, convert*(a/2)) 
D odd(a) then newpt: (l, convert*((a -
1)/2)) fi fi 
as all additional information pertinent to the previous form is derived from the mode 
declaration 
mode sbit = empty 1 (var bit key, var pt sbit next) 
11 "Record" is still used frequently in this original sense: "An object may refer to objects. For 
example, a record object refers to the objects that are components of the record" (Liskov et aL 
1977), 

432 
7. Organized Storages and Linked Lists 
Similarly, in 7.4.1.5 we can abbreviate to 
pt c x f = newpt: (z, item of a, trans (trunk of a, f)) 
and obtain all the relevant information from the mode declaration for ex above. Brevity of 
notation has to be set against transparency of the operative meaning - usually the more 
detailed version is recommended for the beginner; the advanced student would be well 
advised to work with the abbreviated notation. 
Independently of this purely notational question, we notice that in the examples above 
the newvar-operator occurs only in connection with the newpt-operator. In the case of 
genuine linked-list implementations of organized storages this is naturally so. In view of 
the problems (discussed in 7.1) posed by the storage-allocation operator the question arises 
as to whether its use should not be restricted to this connection with a linked-list implemen-
tation. 
Wirth assumes such a restriction in PASCAL (the basic ideas can be traced back to 
Hoare) and introduces a fixed connection between the storage-allocation operator and the 
generation of a pointer. "Values of pointer types are generated whenever a data item is 
dynamically allocated ... For this purpose, we introduce the intrinsic procedure new. 
Given a pointer variable p of type TP, the statement new(p) effectively allocates a variable 
of type T, generates a pointer of type TP referencing this new variable, and assigns this 
pointer to the variable p. The pointer value itself can now be referred to asp (i.e., as the 
value of the pointer variable p). In contrast, the variable which is referenced by p is 
denoted by pi ... " (Wirth 1976). 
The essential difference can be seen immediately. In PASCAL no object declarations 
are permitted for pointers but only assignments to pointer variables. Thus there is no 
longer the possibility of declaring pointers in every incarnation of a recursive routine. 
Despite its clarity the recursive version of convert* from above cannot be directly for-
mulated in PAS CAL. 
Wirth's connection means that Pandora's box (cautioned about by Rutishauser), al-
though opened, is placed under a mosquito net. 
However we can always manage with the coupled construction. The example transit* 
shows that we do not need the generation of pointers independently of newvar even when 
we want to construct infinite objects recursively. 
In every case, however, pointers do not refer to variables having identifiers but simply 
to anonymous variables or to records of such variables. Thus the alias ban has been taken 
into account and, what is more, the problem of "dangling references", connected with the 
free application of references (e.g. in ALGOL 68), is avoided, as the lifetime of all 
referenced variables automatically coincides with the range of the pointers directed to 
them. 
However we cannot use pointers without misgivings. Wirth's connection only removes 
some of the difficulties. We have therefore restricted them to the level of organized 
storages: "The introduction of references into a high-levellanguage is a serious retrograde 
step" (Hoare 1973). 
7.4.3 Link Variables 
We have already introduced pointer variables as components of records, of compositions 
in organized stores ("containers"). The recursive situation with regard to a pointer in the 

7.4 Linked-List Implementation of Organized Storages 
433 
example trans* above shows that independent program variables for pointers are also 
necessary when changing over to iterative routines. Such pointer variables are called link 
variables (Knuth 1973). 
Now, both convert* and trans* are not repetitive and do not permit an immediate 
transition to a repetition. However both can be transformed into repetitive form using the 
methods in Chap. 4. Re-bracketing yields for example 
funct convert = (nat a) lsequ var bit: 
I conv(a, 0) where 
funct conv = (nat a, lsequ var bit z) lsequ var bit: 
if a = 0 then z 
else if even(a) then conv(a/2, stock(z, 0)) 
0 odd(a) then conv((a -
1)/2, stock(z, L)) fi fi J 
Characteristically the operation stock occurs annexing at the "wrong" end of a left 
sequence (comp. schemeR in 4.2.1). Detailing yields 
... else if even (a) then conv(a/2, r jwhere 
lazy functj = lsequ var bit: stock(z, 0) J ) 
0 odd(a) then conv ((a - 1)12, r jwhere 
lazy functj = lsequ var bit: stock(z, L) J ) flli J 
If the representation of lsequ var bit by pt sbit -
where sbit is the mode 
mode sbit = empty 1 (var bit key, var pt sbit next) 
of list-forming records -
is chosen conveniently so that 
stock (deref S, X) = sblt: (newvar bit:= X, newvar pt sbit : = S) 
then the following results 12: 
funct convert* = (nat A) pt sbit: 
I conv* (A, nil) where 
funct conv* = (nat a, pt sbit zz) pt sbit: 
if a = 0 then zz else 
if even(a) then conv*(a/2, I jwhere 
pt sbit f = newpt sbit: 
{newvar bit : = 0, 
newvar pt sbit : = zz> J) 
0 odd(a) then conv*((a - 1)12, I jwhere 
pt sbltj = newpt sbit: 
(newvar bit : = L, 
newvar pt sbit : = zz> J) fi fi J 
12 However the linked list is now reversed and constructed starting at the "wrong" end. The 
implementation is not homologous. 

434 
7. Organized Storages and Linked Lists 
or more simply, without introducing identifiers for pointers and notationally abbreviated: 
funct conv* "' (nat a, pt sbit zz) pt sbit: 
if a = 0 then zz 
else if even(a) then conv* (a/2, newpt: (0, zz)) 
0 odd(a) then conv* ((a - 1)/2, newpt: (l, zz)) fi fi 
The recursion is repetitive, working with the number a and the pointer zz as para-
meters. Transition to iterative notation thus requires the introduction of a link variable. 
Hence we obtain 
funct convert* "' (nat A) pt sbit: 
I (var nat a, var pt sbit zz) : = (A, nil); 
while a * 0 do 
if even(a) then (a, zz) : = (a/2, newpt: (0, val zz)) 
0 odd(a) then (a, zz) : = ((a -
1)/2, newpt: (l, val zz)) fi od; 
~lzz 
J 
For clarity we have indicated the val specification which is otherwise suppressed. The 
link variable zz always contains the pointer to the record last attached - the "handle". val 
zz delivers a pointer, the pertinent organized storage consists either of an 0-tuple or a pair 
of variables, whose left component again contains a pointer etc.; we obtain this pair using 
deref val zz, and val (next of deref val zz) (in short next of deref zz) is the pointer in 
question. 
A complex assignment of the kind occurring above, 
zz : = newpt: (X, val zz> 
or in detail 
zz: = newpt sbit: (newvar bit:= X, newvar pt sbit: = zz> 
must like every assignment be read from right to left: first variables are generated (and 
initialized). A newly generated pointer is made to refer to the record of the variables thus 
generated. This pointer is assigned to the link variable. 
If we introduce an explicit pointer identifier f 
zz : = If where pt sbit f "' newpt: (X, val zz) J 
and split the declaration into a preliminary initialization and a (collective) assignment 
zz: = I jwhere pt sbitj"' newpt: ({f), {f)); (next, key) of deref f: = (X, zz); f J 
we finally obtain 
pt sbit f "' newpt: ({f), {f)); 
(key, next) of deref f: = (X, zz); 
zz := f 

7.4 Linked-List Implementation of Organized Storages 
435 
In PASCAL, on the other hand, an auxiliary pointer variableffmust be introduced to which the 
anonymous pointer is assigned: 
ff: = newpt: ((f), (f)); 
(key, next) of deref val ff: = (X, zz); 
zz := valff 
If we also introduce an auxiliary procedure new 
proc new = (var pt sbit uu): uu: = newpt: ((f), (f)) 
we obtain for example - completely sequentialized -
new(ff); next of deref valff:= zz; zz := ff; key of deref valff:= X 
Comparing this final version with Wirth 1976 (4.13), in PASCAL notation 
new(q); qT. next:= p; p: = q; qT. key:= n 
is quite informative. 
Exercise 1: Transform trans• analogously into repetitive form. 
7.4.4 Implementing Computational Structures Using Linked Lists 
7.4.4.1 We can now describe an implementation of the whole computational structure 
STACK of 3.2.5 using linear linked lists: 
structure STACK "' (mode"/) stack x. empty, isempty, top, rest, append: 
r mode s X 
"' empty I (var X key, var pt s X next), 
mode stack x "' pt s x. 
funct empty 
"' pt s x: nil, 
funct isempty "' (pi s x a) bool: a = nil, 
funct top 
"' (pi s x a: a * nil) x: key of de ref a, 
funct rest 
"' (pt s x a: a * nil) pt s x: next of deref a, 
funct append "' (pi s x a, x x) pi s x: newpt: (x, a> 
J 
Note that with 
mode stack x "' pt s x 
every sequence is represented by a pointer, which means that for example append yields a 
pointer. 
We will now deal with the implementation of the routine contains of 2.10 as an 
example 
funct contains "' (stack x a, x x) bool: 
if a = empty then false 
else if top(a) = x then true 
else contains(rest(a), x) fi fi 

436 
7. Organized Storages and Linked Lists 
With the computational structure STACK above and by unfolding the calls of empty, 
top and rest we obtain 
funct contains* = (pt s x a, x x) boo I: 
if a = nil then false 
else if key of deref a = x then true 
else contains*(next of deref a, x) fi fi 
which is immediately clear. 
Exercise 1: Deal with the routine search of 2.10.2 similarly. 
If we change over to an iteration we obtain for example 
funct contains* = (pt s x A, x x) bool: 
I var pt s X aa: = A; 
while val aa * nil 
do if key of deref val aa = x 
then true return 
else aa : = next of deref val aa fi od; 
false 
J 
The variable aa is again a link variable; the indication of val is usually suppressed. We 
will not give any further transformations (e.g. into the general do-od form) but we will 
specify the version which -
by introducing an additional boolean variable -
avoids a 
jump out of the loopt3 
f (var pt s X aa, var bool b):= (A, false); 
while aa * nil " --, b 
do (aa, b) : = (next of deref val aa, key of deref val aa = x) od; 
b 
J 
This version results from the following applicative version which is equivalent to 
contains: 
funct contains = (stack x a, x x) bool: cont(a, x, false), 
funct cant = (stack x a, x x, bool b) bool: 
If a = empty v b then b 
else cont(rest(a), x, top(a) = x) fi 
Exercise 2: Transform the routine mentioned in Exercise 1 into an iterative form 14• 
In this example we had simply to scan linked lists. For routines based on append a link-
ed-list implementation of stacks is somewhat more problematic. Now links must be pro-
13 For comparison with Wirth 1976 (4.20). 
14 Compare the result with (4.20) in Wirth 1976. 

7.4 Linked-List Implementation of Organized Storages 
437 
duced anew. Concatenation is a central task. If we follow the original definition in 2.10 we 
obtain 
funct cone* = (pt s X a, pt s x b) pt s x: 
if a = nil then b 
else newpt: (key of deref a, conc*(next of deref a, b)) fi 
Here the result is a completely new linked list constructed by first copying the linked list 
with the handle a up to the "lowest" record where nil is replaced by a pointer to b (Fig. 
7.10) 
Fig. 7.10 
We will meet a more efficient treatment in 7.5. 
Exercise 3: Treat the routines replace and sort of 2.10.2 similarly. 
Exercise 4: A stack can be considered as a stack of bounded stacks. 
Concretize a computational structure STACK CV as STACK (bs X) and then introduce a 
linked-list implementation for STACK as well as the implementation of BS(x, N) using 
mode fstack x '"' (nat [0 .. N] i, nat [1 .. N] array xa) (comp. 3.6.3). 
7.4.4.2 The implementation of STACK above can be extended to an implementation of 
FLEX. However the operation sel is implemented quite inefficiently. 
For the implementation of SEQU by doubly linked linear lists see 7.5.2. 
When implementing non-linear object structures we frequently change to the mode 
list x of Lists (with capital L, comp. 2.9.1). For the above we obtain a linked-list imple-
mentation 
mode list x = pt I x. 
mode I x = sequ (var pt I xI var i) 
an implementation by means of mixed sequences of pointer variables and of variables for 
objects of mode X· 
Exercise 5: Specify a linked-list implementation for the mode plex x of 2.9.1. 
7.4.5 Properties of Pointers 
The following is a summary of the most important properties of pointers: 

438 
7. Organized Storages and Linked Lists 
(1) Identifiers of pointers are bound identifiers. They are bound in a declaration by 
relating them to a corresponding object. The range of binding is the smallest bracketed sec-
tion containing the declaration. The scope, however, can have holes caused by overruling 
(comp. 1.13.2). 
(2) An organized storage is mode-specifically related to a pointer. This relation is inva-
riable, it is established once and cannot be dissolved during the whole lifetime of the 
pointer. 
(3) Every incarnation of the call of a recursive routine has its private co-existing 
pointers, if it has any at all. 
( 4) A pointer yielded as the result of a routine 15 has as life-time the range of binding of 
the identifier of this routine. 
(5) Pointers with different identifiers are different pointers (they have no connection to 
each other). Pointers have a "unique identity". 
(6) Pointers -
if we want to grant them a kind of object character at all -
are not 
autonomous objects: they have no meaning without other objects - the objects to which 
they refer (Hoare 1973: "no independent meaning"); they have no real existence except 
through a declaration. 
(7) Pointers differ from other objects in that only a few universal operations are defin-
ed for them (linkage to a referenced object, comparison with niP 6) and that no individual 
operations can be specified. 
7.5 Improvement of Algorithms Working on Linked Lists 
by Selective Updating 
In list-forming records the pointer variables too can be updated. This often permits con-
siderable operative improvements of algorithms. 
This kind of selective updating, however, poses problems: through updating, 
hierarchical structures can become cyclic structures; this means that semiregularity and 
thus easy verification of the actualization taboo are lost. An alteration of common sub-
structures can also have an unexpected effect -
a fact pointed out by Hoare 1973 
("prohibition on selective updating"). In any case the algorithms discussed below are 
entirely on a procedural level. 
In the sequel we will suppress notationally the deref-operation as we already did with 
the val-operation. 
7.5.1 Algorithms for One-Way Linked Lists 
The algorithm for concatenation of two linear linked lists in 7 .4.4 actually causes the up-
dating of a link variable which is initialized by nil and finally contains the pointer b. It is 
15 If routines have pointers as results, then the pointers themselves are of no importance to the out-
side world but only the contents of those compositions of variables to which they point. The "out-
put" consists of a re-transformation into a corresponding composition of objects. 
16 As nil is introduced as a universal pointer to the "empty word", deref nil is always defined. It is a 
0-tuple of variables, i.e. it is not a variable. In particular, a content of deref nil is not defined, 
neither is an assignment to deref nil allowed. 

7.5 Improvement of Algorithms Working on Linked Lists by Selective Updating 
439 
sufficient simply to scan the linked list a in order to find its end. However the list with the 
handle a has to be changed. Instead of a we therefore introduce a link variable aa which is 
to receive the result. Another link variable pp is used for scanning. Thus we obtain the fol-
lowing algorithm which is considerably more efficient than the one in 7.4.4: 
proc c6nc = (var pt s x aa, pt s x b): 
if aa =nil then aa := b 
else I var pt s x pp : = aa; 
until next of pp = nil do pp : = next of pp od; 
next of pp : = b 
J fi 
The last example shows a new kind of "recursion removal" by comparison with the 
methods in Chap. 4. There are also schematic transformations for this. For the scheme R 
of routines over the computational structure STACK treated in 4.2.1 we obtain the follow-
ing from the implementation of 7.4.4 in an organized store: If 
funct R = (stack x a, ll y) stack x: 
if !!A [a, y J then X [a, y J 
else append (R(rest(a), y), top(a)) fi 
is replaced by 
proc R = (var pt s )( aa, llY): «aa: = R(aa, y)», 
then the latter is equivalent to 
proc R = (var pt s x aa, llY): 
if .?8 [aa, y J then aa : = ff [aa, y J 
else 
I var pt s x pp : = aa; 
until .fiB [next of pp, y J do pp: = next of pp od; 
next of pp: = £'[next of pp, y J 
J fi 
In addition to selective updating the formal proof requires only unfolding and folding. 
We can establish similar transformations for special cases of routines over other com-
putational structures, e.g. over cascades. 
Exercise 1: Apply the methods of this section to the routine sort of 2. 10. 
Let us in addition list the facts about operations at the "wrong" end of a stack: bottom 
is already repetitive in 2.10 and is transformed into simple scanning for nil. upper no 
longer needs to be based on double reversing, it is implemented by selective updating of the 
next-component in the last but one record. The last record becomes inaccessible. Storage 
release could occur in trunc. stock, finally, results in the "increase" of a linked list at the 
"wrong" end. 
However stock is a special case of cone (comp. 2.10): 
funct stock = (stack x a, x x) stack x: conc(a, append(empty, x)) 

440 
7. Organized Storages and Linked Lists 
and thus from d'Jnc for the linked-list implementation we obtain 
proc stock = (var pt s x aa, x x): 
I pt s x b = newpt: (x, nil); 
if aa =nil then aa := b 
else var pt sxpp: = aa; 
until next of pp = nil do pp : = next of pp od; 
next of pp : = b 
fi J 
In order to obtain a homologous implementation for convert in 7.4.3, we must base 
stock there on append, too. From the repetitive version of convert in 7 .4.3 an iterative one 
is obtained directly, and after transition to pointer implementation it reads 
funct convert* = (nat A) pt sbit: 
I (var nat a, var pt sbit zz) : = (A, nil); 
while a * 0 do if even(a) then (a, zz): = (a/2, stock(zz, 0)) 
U odd(a) then (a, zz) : = ((a - 1)/2, stock(zz, L)) fi od; 
zz 
J 
For the repetition we have, partially sequentialized 
while a * 0 do if even(a) then a:= a/2; stock(zz, 0) 
else a : = (a -
1)/2; stock(zz, L) fi od 
or unfolded 
while a * 0 do pt sbit b = 
ifeven(a) then a:= a/2; newpt: (0, nil) 
else a:= (a - 1)/2; newpt: (l, nil) fi; 
if zz = nil then zz: = b 
else var pt sbitpp: = zz; 
until next of pp = nil do pp: = next of pp od; 
next of pp : = b 
fi od 
Now stock is performed anew in each repetition of a loop, and each time the linear 
linked list is traversed up to the bottom-most element in order to annex the corresponding 
newly generated record. This, too, can be avoided as pointers can be retained in link varia-
bles: we store the relevant pointer to the bottom-most element in an additional link varia-
ble tt and save the trouble of traversing, which implies a remarkable operative improve-
ment. Thus we obtain 
funct convert*= (natA) pt sbit: 
1 (var nat a, var pt sbit zz, var pt sbit tt) : = (A, nil, nil); 
while a * 0 do pt sbit b = 
if even (a) then a:= a/2; newpt: (0, nil) 
else a:= (a- 1)/2; newpt: (l, nil)fi; 

7.5 Improvement of Algorithms Working on Linked Lists by Selective Updating 
441 
if zz = nil then zz : = b 
else next of tt: = b fi; 
tt:= b 
od; 
zz 
J 
7.5.2 Algorithms for Two-Way Linked Lists 
7.5.2.1 Two-way linked lists are constructed from list-forming records which contain two 
essential pointers. They correspond to the finite and infinite objects of mode case x dealt 
with in 2.14.1. In particular, linear two-way linked lists as a means of symmetrical imple-
mentation of sequences deserve special attention. Already in the case of one-way linked 
lists it was advisable to introduce a link variable with which one could always reach the 
"wrong" end -
that was precisely the meaning of tt in the final version of convert* in 
7. 5 .1. For implementing sequences symmetrically we now use a pair of pointers which refer 
to both ends of a linear two-way linked list. 
Thus two-way linked lists are to be constructed from records of mode 
mode c x "" empty I (var pi c x left, var x node, var pt c x right) 
top, rest and append are then implemented using the left pointer (in the same manner 
as given for STACK in 7.4.4.1); accordingly the right pointer is used for bottom, upper 
and stock. cone degenerates to the typical two-way linking: Using selective updating the left 
pointer of the right list is stored in the right component of the left list and the right pointer 
of the left list is stored in the left component of the right list. 
Transition from sequences to linear two-way linked lists, that is the structure homo-
morphism, is described by the algorithm transit of 7 .4.1.4. The algorithm trans* there can 
be changed into the following form by selective updating: 
funct trans* = (lsequ x a, pt c x z) pt c x: 
if a = 0 then nil 
else I pt c xi = newpt c x: (newvar pt c x: = z, 
newvar x: = item of a, newvar pt c x: = nil); 
right off:= trans*(trunk of a, f); 
f 
J fi 
The "pending" right-links are still only established at the end. By means of a transfor-
mation related to the method of re-bracketing (4.2.1) we proceed to the iterative form with 
the establishment of the right-links not delayed and, as explained above, with a pair of 
pointers as result: 
funct transit* = (lsequ x A) (pi c x. pi c X): 
I (var lsequ x a, var pt c x zz. var pt c x tt) : = (A, nil, nil); 
while a* 0 
do pt cxf= newpt: <zz, a, nil); 
if tt = nil then tt : = f else right of zz : = f fi; 
(a, zz) : = (trunk of a, f) 
od; 
(~zz) 
J 

442 
7. Organized Storages and Linked Lists 
Exercise 1: Give an implementation of the sequential files file x and roll x by linear two-way linked 
lists. 
Linear two-way linked lists can be made to form two-way ring lists by doubly linking 
both ends. Implementing cyclic objects of mode case x by means of two-way ring lists 
does not require any new measures. 
Finally we still have the hierarchical cascades as a classical case. It is immediately clear 
how they can be implemented by hierarchical linked lists of mode ex. 
7.5.2.2 Tasks such as traversing cascade-like object structures ("binary trees") suggest an 
implementation - already on applicative level - by certain infinite objects using case x 
("threaded trees") such that the order of traversal can be seen directly. In the case of 
linked-list implementation this means that (according to Perlis, Thornton 1960) a traversal 
reference is set in all nodes of the hierarchical linked list in which both references are nil. 
This is all the more advisable as an "economical" solution is evident. For this purpose the 
record ex is extended by a boolean component stating whether we have a tree continuation 
or a traversal continuation: 
mode travc x = empty I (var pt travc x left, var x node, var bool rtag, 
var pt travc x right) 
Traversing in prefix order - shown in the illustrations - is now done as in Fig. 7.11: 
Fig. 7.11 
If the /eft-link is different from nil it is followed, otherwise the right-link is followed if this 
is different from nil (irrespective of whether it is a traversal link (rtag = true) or a normal 
link (rtag = false)). 
If a node is reached in which the left- and the right-links are nil the traversal 
terminates. This (rightmost) node can be given a special traversal link (called backthread). 
Alternatively to the routine traversetree (2.13) we thus obtain the iterative implementa-
tion 
funct traversetree = (pt travc x A) sequ x: 
I (var pt travc x a, var sequ x s) : = (A, 0); 
do if a = nil then leave 
elsf left of a * nil then (a, s): = (left of a, append(s, node of a)) 
else (a, s): = (right of a, append(s, node of a)) fi od; 
s 
J 

7.6 Addressing 
443 
By contrast with the version in 2.13 traversetree yields a stack in which the labels of the 
nodes visited first are lowest in the stack, thus again no homologous implementation. 
The correct entry of the traversal link can be made very simply while constructing the 
tree: if the construction comprises two (non-empty) subtrees, the backthread of the left 
subtree is linked - as a traversal link - to the root of the right subtree, the backthread of 
the right subtree is the new backthread. 
If the left subtree alone is missing no traversal !ink refers to the root of the right sub-
tree. If the right subtree alone is missing the new backthread is the old one of the left sub-
tree. If both subtrees are empty we have a terminal node. Thus we obtain 
funct build = (pt travc x a, x x, pt travc x b) pt travc x: 
if a = nil then newpt travc x: (a, x, b = nil, b) 
elsf b = nil then newpt travc x: (a, x, true, b) 
else change(a, b); newpt travc x: (a, x, false, b) fi 
based on 
proc change = (pt travc x a, pt travc x b: a * nil): 
if left of a = nil then if right of a = nil 
then right of a : = b 
else change(right of a, b) fi 
else if right of a = nil 
then change (left of a, b) 
else change(right of a, b) fi fi 
In a similar way we can also establish traversal links for producing a postfix order on 
the /eft-link position. This is not surprising. However symmetric threading also permits 
traversal in infix-order. (For details see Knuth 1973, 2.3.1.) 
7.6 Addressing 
"The address of a variable a -
now called a reference 
- was ... introduced in the language EULER and 
denoted by@ a". 
Wirth 1974 
In introducing the concepts of object addresses and jump addresses we make the transition 
to stored-program machines in which procedures and objects are ultimately packed into a 
homogeneous storage of binary words. 
The intermediate level reached by introducing object addresses and jump addresses can 
be called the level of the Aiken machine (Aiken 1937) which is characterized by the separa-
tion of instruction storage and object storage, and by special precautions for address 
arithmetic. It was fully developed in Kilburn 1949. 
From now on we will assume that all objects are uniformly implemented by objects of a 
certain mode Jl, in particular (see 7 .6.3) by binary words of a fixed length - in an extreme 
case possibly by objects of mode bit. 

444 
7. Organized Storages and Linked Lists 
7.6.1 Addresses for Variables 
"Pointer arithmetic is a popular pastime for system 
programmers". 
Geschke et al. 1977 
Hitherto there have been only trivial operations for variables. If they are linearly ordered 
- in so far as they refer to one and the same mode - then the operations succ and pred, 
too, are (partially) defined on variables. We call these ordered variables storage cells or 
storage locations. Their identifiers are called (variable) addresses. Pointer variables, espe-
cially link variables, become link cells or link fields. Link cells thus "contain" pointers, 
normal storage cells "contain" normal objects - according to the container philosophy. 
The use of link cells originates in Schecher 1956. 
Because of the isomorphism between a linearly ordered finite set of variables and an 
interval of natural numbers or integers, storage cells frequently have identifiers derived 
from the denotations of natural numbers or integers, particularly marked by affixing a 
small zero: 
... 35390 , 35400 , 3541 0 , ••• 
In this case, we also speak of relative addresses. If no number denotations are used, the 
linearly ordered identifiers are called symbolic addresses. 
Based on succ and pred, addition of an integer to an address and subtraction of an in-
teger from an address can be defined, the result being an address (comp. 3.5.1). The sub-
traction of an address from an address, the result being an integer, is also defined by this. 
Other, "exotic" operations with addresses are very seldom necessary. 
Mode-specified addresses as objects have the specification adr Jl· 
Exercise 1: State routines for the operations of "address arithmetic" mentioned above. 
The main characteristic of working with addresses (and thus for the whole level of ad-
dressing) is, that a pointer, i.e. a reference to a variable or to a composition of variables is 
allowed to coincide with its address, or with a typical address among the addresses for the 
composition ("start address", "end address", "end address + 1 "). This is permitted be-
cause pointers have invariable references 17• The address operations then permit the com-
putation of addresses of individual components. 
Thus link cells contain addresses ("indirect addressing"). Generation of a new variable 
is reduced to making an address available by computing an address not yet used. It can be 
expressed using a link cell EUS~ ("end of used storage") of mode var adr J1 by means of 
succ: 
newvar J1 
is implemented as 
EUS~: = succ(EUS~); val EUS~. 
The variables are linearly ordered in the order in which they are made available. 
17 This is however not possible in ALGOL 68 where no distinction is made between references and 
variables. ALGOL 68 constructions in systems programming are thus burdened with an additional 
reference level. 

7.6 Addressing 
445 
newpt 11. is implemented as registering the "typical" address of a composition of mode 
11. of variables occurring as a referenced object; in the case of Wirth's connection it is the 
typical address which occurs with the creation of a composition of variables. 
The operation deref which yields the object referenced by a pointer becomes trivial. 
7.6.2 Jump Addresses 
Jumps are the counterpart to pointers (comp. 7.4.1.1). The introduction of addresses for 
variables corresponds to the introduction of jump addresses as the continued or intermit· 
tently continued numbering of labels in a completely sequentialized procedure. 
Before introducing jump addresses the course of execution is often cut into pieces: 
Even branching, a basic construction, which has been our companion since Chap. 1 is 
split into pieces. The pertinent transformation for an alternative - provided !!A is defined 
- reads 
if !!A then 5'1 else Y"2 fi 
if !!A then goto ml else skip fi; 
r 5"2 J; goto m2; 
ml: r 5'1 J; 
m2: 
Alternatively !!A can be negated and the ri'>les of 5'1 and 5"2 switched, which is some· 
times of advantage, e.g. if 5"2 is empty: 
if !!A then 5'1 else skip fi 
if --, !!A then goto m else skip fi; 
r Yl J ; 
m: 
Reshaping procedure headings to labels is usually accompanied by a general deletion of 
the block structure; object and variable declarations have to be eliminated beforehand and 
the return construct is to be replaced by a jump beyond the call. 
Further details known from applications in systems programming and in compiler 
construction need not be dealt with here. 
Single "instructions" ( comp. also the stack machine 1. 7 .4) are characteristic of the as-
sembler level reached here. Frequently unary operations use a special variable, the "ac· 
cumulator" AC; binary operations use a storage cell together with the accumulator: "one-
address instructions". An example of such a processed procedure is to be found in Table 1 
(f') on p. 453. According to the container philosophy instructions as well are now contain-
ed in cells belonging to the "instruction storage". 

446 
7. Organized Storages and Linked Lists 
7.6.3 Genuine Addresses 
Addresses per se can be obtained if addresses for variables and jump addresses are no 
longer distinguished from each other. An example of this is to be found in Table 1 (g') on 
p. 453 where cells for variables and for instructions are intermingled. 
Genuine addresses are further characterized in that they explicitly refer to (binary) 
words of fixed length. Genuine object addresses thus assume (binarization and) possibly 
packing of the objects. Genuine jump addresses assume the same for instructions, i.e. for 
elementary fragments of the completely sequentialized procedure. Cells become 
(binary-)word cells. 
Addressing - after the introduction of genuine addresses - abolishes any distinction 
between modes; thus important means of checking disappear. The difference between ob-
jects and variables is also blurred: objects can only occur as contents of cells. We have in 
fact the classical stored-program binary machine (Eckert, Mauchly, von Neumann, Gold-
stine 1945, comp. Rande111973) with a homogeneous binary-word storage, an arithmetic 
unit as an executive for some remaining primitive operations (comprising at least successor 
operations and identity test), and a control unit as an executive for the remaining control 
elements: (conditional) jump instruction, advance to the consecutive instruction. In the 
extreme case of binary words of length 1 we speak of bit-addressed machines. 
The task of implementing (on the level thus reached) the structure list X (comp. 2.9.1) 
by a suitable object mode is typical. This is either done in linear storage or as a linked list 
(comp. 7.4.4). The fact that the basic mode x itself may contain variants requiring dif-
ferent amounts of binary-word cells can complicate matters. 
In both cases infinite objects from list x can occur, thus we can have sub-lists which 
contain themselves as a component or which are at several positions. An example (Seeg-
miiller 1974) is the object 
list xL = (a, B, B, L), 
list xB = (b, c) 
with a, b, c of the mode X where e.g. a and c require one binary-word cell and b requires 
three cells. 
The following relations, among others, hold for this object L (for the notation, comp. 
2.6): 
top o rest 3(L) = top o rest 3 o top o rest 3(L) = ... = L, 
top o rest(L) = top o rest 2(L) = B 
top o top o rest(L) = top o top o rest 2(L) = b 
If we introduce indices for notational convenience and write x[ iJ briefly for 
top o resti(x) ("the i-th leg of x"), the above relations read 
L [3J = L [3, 3J = ... = L 
L[1J = L[2J = B 
L[1, OJ = L[2, OJ = b 
In the case of linear storage the task of implementation is primarily to find the storage 
mapping function, a function from the selectors onto the addresses. For the object in the 

7.6 Addressing 
447 
au: bd 
au+ 1: 
a0 + 2: 
b 
au+ 3: 
au+4: 
c 
Fig.7.12 
above example we have the storage diagram Fig. 7.12 and the following mapping of the 
(composite) indices onto the start addresses: 
[0] = [3, 0] = [3, 3, 0] = ... -+ !Xo 
[1, 0] = [3, 1, 0] = [3, 3, 1, 0] 
[2, 0] = [3, 2, 0] = [3, 3, 2, 0] ::::]-+ao+ 
[1' 1] = [3, 1' 1] = [3, 3, 1' 1] = ... J 
4 
--ao+ 
[2, 1] = [3, 2, 1] = [3, 3, 2, 1] = ... 
Representing a list in this way is quite useful, if only elements of the basic mode x are 
sought and possibly updated. However (comp. 2.13-2) as soon as we delete, add, or replace 
components changing the storage requirement, this representation of a list is no longer 
flexible enough as the operations mentioned require an alteration of the storage mapping 
function. The suitability of an implementation depends here, too, on the extent of the 
operations needed. 
The linked-list implementation has the desired flexibility but requires more storage 
space. Assuming that a pointer can be stored in one binary-word cell, the diagram in Fig. 
7.13 results from 7.4.4 for the above object. Each of the two cell blocks is compact al-
though the blocks can be stored in a scattered fashion. 
Fig. 7.13 
As the representation is still not sufficiently flexible it is better to introduce a linked-list 
representation for the individual sequences, where a binary-word cell for storing the 
pointer to the rest of the sequence is added to every element of the sequence - represented 
by a binary-word cell for a list-pointer or by a suitable number of binary-word cells for 
objects of the basic mode. In our example we may have the diagram in Fig. 7.14, where a 
cell block for L is placed in front. 
The scattered storing of these individual blocks now permits changes in the structural com-
position in the simplest way. 
Somewhat simplified, but basically similar, is the problem of implementing the struc-
ture lisp x in a linear storage or as a linked list of binary words. There are compilers whose 
performance is based on work with list or lisp; and machine-oriented programming 

448 
7. Organized Storages and Linked Lists 
r-1----- "A: 
L 
a 
B 
B 
L 
b 
c 
Fig. 7.14 
languages (the original LISP falls in this class) which have the structure lisp x as a fixed 
ingredient; the structure list xis also often used in this way (SAC-1, Collins 1967). The 
linear storage and the linked-list implementation of these two structures on the level of 
addressing is therefore a favourite theme of systems programming. 
7.6.4 Outlook to Systems Programming 
In this chapter we have prepared the way towards systems programming and have dealt 
with some important points. Others remained undiscussed. 
A systems programming language must, in particular, provide complete control over 
the mapping of organized storages into the homogeneous storage of a machine which 
serves as an interface. This mapping must be protected against access from other system 
levels, so that the storage organization revealed is not unintentionally interfered with. 
Because of the risks of free programming with addresses it is usual, on the level of systems 
programming, to explicitly include provision for the treatment of errors. In the case of 
limited storage, linked lists make necessary occasional or continuous garbage collection. 
A further characteristic of systems programming is the demand that all objects should 
be represented in such a way that all the information regarding the modes of the compo-

Addendum to Chapter 7. Notations 
449 
nents and the structure of the composition can be gained from the representation. (If this is 
done for the example above we immediately arrive at the example of Fig. 27 in Seegmiiller 
1974.) Only then is it possible to use programs of a general kind for processing arbitrarily 
structured objects which are accompanied by a "descriptor". 
Because of the relatively big differences in the machine interfaces such points are out-
side the general discussion aspired to in this book. In any case they are handled quite in-
dividually and sometimes ad hoc in practice. Thus in several textbooks which aim at con-
ceptual comprehension it becomes useful at this point to refer to the example of a special 
machine (Knuth 1973: MIX). In connection with operating systems in particular, storage 
protection, privileged instructions, interrupts and multiple access have to be treated in 
detail. Very informative literature is available for this, e.g. Graham 1975. 
Addendum to Chapter 7. Notations 
Although in ALGOL 68 there is an explicit difference between variables and objects, an 
exceptional rule for "dereferencing" blurs it notationally. Both ALGOL 60 and PASCAL 
operate completely on the basis of program variables. A "variable" 
var x: matrix , 
where 
type matrix = array [1 .. 2, 1 .. 2) of integer , 
is not to be considered as one program variable for matrixes but as a matrix of four pro-
gram variables 
x[1, 1], x[1, 2], x[2, 1], x[2, 2) 
which is obvious from the following explanation of parameter passing for parametric vari-
ables: "possible indices are evaluated; and the variable thereby identified is then 
substituted for its formal correspondence" (Wirth 1973, p. 95). In SNOBOL we have "a 
programmer defined data object is an ordered set of variables called fields". 
Understandibly special systems programming languages attach great importance to 
working with organized stores. BCPL is only one example. 
In PL/1 pointers are not mode-specific. This does not hold for ALGOL 68, ALGOL 
W, SIMULA, PASCAL: "garbage collection" is possible in these languages. In PL/1, on 
the other hand, the heap storage space must be released explicitly and this can lead to grave 
mistakes. If, in particular, one forgets to substitute a pointer pointing to a released object 
by nil, one obtains the feared "undefined pointers" ("dangling references"). 
In PASCAL, file denotes a variable for data, or rather a structure of variables. In our 
notation file of T would correspond to 
mode pascal file t = (var sequ t front, var sequ t back, var t buffer) 
For objects s of mode pascalfile we now have typical operations, e.g. 

450 
"delete": rewrite(s) 
"buffer": si : = x 
"append": put(s) 
"reset": reset(s) 
"get": get(s) 
"test": eof(s) 
7. Organized Storages and Linked Lists 
.... (front, back, buffer) of s : = (empty, empty, N) 
.... buffer of s : = x 
++push (back of s, buffer of s) 
.... (front, back, buffer) of s: = (empty, front of s & 
back of s, top (front of s)) 
.... (front, back, buffer) of s: = (front of s & 
top (back of s), rest(back of s), top(back of s)) 
.... back of s = empty 
Finally for input and output we have 
read(s, v) .... v: = buffer of s; get(s) 
write(s, e) .... buffer of s : = e; put(s) 

Conclusion. Programming as an Evolutionary Process 
Program Specification and Development in a Uniform Language 
In order to solve a complicated problem, two extreme approaches may be adopted: a suf-
ficiently complicated machine is used with the prospect of finding a "simple" solution, or a 
simple machine is used and we have to expect a "complicated" solution. 
This holds both for routines and for object structures. The different versions of 
algorithms, which we have considered for the example of determining the greatest com-
mon divisor, are compiled in Table 1 for comparison. A development represented by the 
steps (a') to (g') begins with a simple routine for a sophisticated machine and ranges to 
complicated, untransparent programs for a crude machine, e.g. for the stored-program 
computer. In addition, in the sequence (a')- (a")- (a"') there is a noticeable improvement 
with regard to efficiency which in turn suggests a change of the object structure. Further 
analogous steps after (b"'}, (c"') etc. are not specified. 
Similarly, object structures which are easily described but require highly developed 
machines, such as a bounded (right) sequence in Table 2, have been replaced step by step 
by increasingly complicated structures in order to enable most simple access mechanisms, 
e. g. those of a homogeneous storage. 
Computer science owes its fascination and computer scientists owe their daily bread to 
these machines, to which everything must be ultimately tailored. While a programmer car-
ries out the necessary development process all by himself only in isolated cases - as a rule 
he will rely on acquired rules and on mechanical compilers - , the computer scientist, on 
the other hand, must not only be able to carry out the development process completely on 
his own, he must also master it: he must be able to describe it abstractly in order to teach it 
and to construct compilers. A machine in conjunction with a compiler into its language -
a programming system -
is equivalent to a more highly organized machine ("abstract 
machine"). Compiler technology so far has been limited to obtaining a moderately 
organized abstract machine, which -
according to the above -
requires a moderately 
complicated description. The level, determined by the respective programming systems, 
can be higher (ALGOL) or lower (BASIC). Incidentally, this should not be considered as a 
strict level but rather as a spectrum which is sometimes wider (ALGOL 68) or narrower 
(ALGOL 60). 

452 
Programming as an Evolutionary Process 
Table 1. Algorithms on different levels of development 
Problem: Determine the greatest common divisor of the natural numbers a and b. 
Level of descriptive formulation, search machine (1.11.1) 
(a) 
funct gcd .. (nat a, nat b: a * b v b * 0) nat: 
1natx:xla A xlb A vnaty:(yla A ylb =>ylx) 
(neads: "that ... ") 
Level of applicative (recursive) formulation, Herbrand-Kleene machine (1. 7 .3) 
(a') 
funct gcd .. (nat a, nat b) nat: 
if b = 0 then a 
U b > 0 A a < b then gcd(b, a) 
U b > 0 A a~ bthen gcd(a-b, b) fi 
(a") funct gcd .. (nat a, nat b) nat: 
if b = 0 v a = b then a 
U b * 0 A a * b then if even a A even b then dupl(gcd(a/2, b/2)) 
U even a A odd b then gcd(a/2, b) 
U odd a 
A even b then gcd(a, b/2) 
U odd a 
A odd b then 
if a < b 
then gcd(b, a) 
U a > b 
then gcd(b, a- b) 
(a"') funct gcd .. (sequ bit a, sequ bit b) sequ bit: 
if b = empty v a = b then a 
U b * empty A a * b then 
(1.10.2-2) 
(1.7.1, 1.11.2) 
fl fl fi 
(1.13.1-2) 
if bottom(a) = 0 
A bottom(b) = 0 then gcd(upper(a), upper(b)) & 0 
U bottom(a) = 0 
A bottom(b) = L then gcd(upper(a), b) 
U bottom(a) = L 
A bottom(b) = 0 then gcd(a, upper(b)) 
U bottom(a) = L 
A bottom(b) = L then 
if a < b then gcd(b, a) 
U a > b then gcd(b, a-b) 
Level of structured functional (recursive) formulation, ALGOL machine (1.7 .3) 
(b') funct gcd .. (nat a, nat b) nat: 
if b = o then a 
else gcd(b, mod(a, b)) fi, 
funct mod .. (nat a, nat b) nat: 
if a < b then a 
else mod(a- b, b) fi 
Level of (partially collateral) iterative formulation, Babbage-Zuse machine (1.7 .4) 
(c') 
funct gcd .. (nat a, nat b) nat: 
I (var nat x, var nat y) : = (a, b); 
whiley * o 
do (x, y) : = (y, I var nat z: = x; 
while z ~ y do z: = z - y od; 
z 
J) od; 
X 
j 
fi fi fi 
(3.6.4.1) 
(1.4.1, 1.7.1) 
(5.2.3) 

Programming as an Evolutionary Process 
453 
Table 1 (continued) 
Level of (completely sequentialized) procedural formulation, sequential machine (5.2.4) 
(d') funct gcd .. (nat a, nat b) nat: 
I var nat x; var nat y; var nat z; 
x :=a; y := b; 
while y * 0 do z : = x; 
while z ~ y do z : = z - y od; 
X 
x:= y; 
y := z 
Flow diagram level of formulation (6.7.1) 
(e') I 
if a ~ b then goto m1 fi; 
II x: = b II Y: = a JJ ; 
goto m2; 
m1: II x : = a IIY : = b JJ ; 
rep1: m2: If y = 0 then goto exit1 fi; 
z := x; 
rep2: if z < y then goto exit2 li; 
z: = z- y; 
goto rep2; 
exit2: x : = y; 
Y := z; 
goto rep1; 
exit1: ~ 
Level of symbolically addressed 
one-address formulation (7 .6.1) 
(r) 
start: AC : = a; 
AC := AC- b; 
if AC ~ 0 then goto m1 fi; 
AC := b; 
x:= AC; 
AC :=a; 
y:= AC; 
goto m2; 
od; 
Level of storage-addressed 
one-address formulation (7 .6.3) 
(g') 
~ 
a :3 539 0 'VVVVVVvVVv 
b:35400 'VVVVVVVVVv 
x:3541 0 
y:35420 
z:35430 
start:35440 AC : = cont 35390; 
(5.3.4-1) 
35450 AC : = AC -
cont 35400; 
35460 if AC ~ o then goto 35520 fi; 
35470 AC : = coni 35400; 
35480 3541 0 : = AC; 
35490 AC : = coni 35390; 
35500 35420 : = AC; 
3551 0 golo 35560; 

454 
Table 1 (continued) 
m1: AC :=a; 
x:= AC; 
AC := b; 
y := AC; 
Programming as an Evolutionary Process 
m1 :35520 AC : = cont 35390; 
35530 3541 0 : = AC; 
35540 AC : = cont 35400; 
35550 35420 : = AC; 
rep1: m2: if AC = 0 then goto exit1 fi; 
AC := x; 
rep1: m2:35560 if AC = 0 then goto 35680 fi; 
35570 AC: = cont 3541 0; 
z := AC; 
35580 35430 : = AC; 
rep2: AC := AC- y; 
if AC < 0 then goto exit2 fi; 
z := AC; 
rep2:35590 AC : = AC -
cont 35420; 
35600 if AC < 0 then goto 35630 fi; 
3561 0 35430 : = AC; 
goto rep2; 
exit2: AC : = y; 
x:= AC; 
AC := z; 
y := AC; 
goto rep1; 
exit1: 
35620 goto 35590; 
exit2:35630 AC : = cont 35420; 
35640 3541 0 : = AC; 
35650 AC : = cont 35430; 
35660 35420 : = AC; 
35670 goto 35560; 
aU,o356S, l 
Table 2. Object structures on different levels of development 
Problem: Introduce (right) sequences (of bounded length) with objects of mode X· 
Level of descriptive formulation 
(a) Restriction to right sequences of maximum length N 
mode bs 'X = {rsequc 'X b: length (b) ::::; N} 
(See 3.1.3.2 for rsequc xl 
Level of applicative formulation 
(b) Level counter representation 
mode bs x =(nat [0 .. N] i, nat [1 .. N] grex x a) 
(See 3.3.3 for v grex xl 
Level of procedural formulation 
(c) Level counter representation in organized storages 
var bs x ~ (var nat [0 .. N] i, nat [1 .. N] array var x a) 
(3.1.3.3) 
(3.6.3) 
(7.3.3) 
If a program language is used solely for denoting programs, the spectrum of its con-
structs can be narrow, which may have favourable consequences aesthetically. An example 
for this is LISP, in a certain sense APL, too. Conversely, a programming language, which 
permits carrying out a program development, generally has to cover a wide range between 

Programming as an Evolutionary Process 
455 
the initial level of the problem specification and the final level of an (abstract or concrete) 
machine - that is, it must be a wide spectrum language. In the extreme case such a wide 
spectrum programming language must cover the complete and open-ended range which 
makes algorithmic formulation possible -
the range of the abstract ALGORITHMIC 
LANGUAGE as it is used (irrespective of the notation) as the conceptual basis for this 
book. 
Program development starts with the problem and ends on the level of a machine. If we 
imagine the machine at the bottom and the problem on top - according to current views 
(in the humanities!) - then program development moves downwards. We will seldom find 
the problem posed in opposite direction ("decompilation"). 
However the technique of program development incorporates two extreme approaches 
and many intermediate forms of producing this transition "from the top downwards". The 
first extreme approach is a strict forward-development. It has been demonstrated many 
times in this book. The other extreme approach is a strict backward-development. 
Suitable, frequently used routines and computational structures are constructed for a 
given machine, further routines and computational structures are constructed based on 
these, and so on, until finally a routine is obtained which solves the given problem. (In 
jargon a forward-development is also called top-down and a backward-development 
bottom-up.) The method of backward-development can be rather amazing as it resembles 
a film played backwards in which the fragments of an explosion miraculously reassemble 
to form a whole. As a matter of fact, this method often leads into blind alleys in practice. 
The classical method of the programming library makes a virtue of this predicament: 
Backward-developments not applicable to the problem at hand are recorded for possible 
future use. It is well known that extensive program libraries present a number of dif-
ficulties. As a rule, despite parameterization, they will not contain a solution for a given 
problem. However, the introduction of "abstract machines" mentioned above is an 
important example for (machine-oriented) backward-development. By comparison to the 
basic machine they have more elaborate operations (e.g. vector operations) and objects 
(e.g. lisp or list). 
Forward-development, too, often leads into blind alleys, but of course the examples 
demonstrated in the book do not show this. Nevertheless it seems as if forward-develop-
ment is intuitively easier to master. Above all, by "delaying decisions", it permits the blind 
alleys in the development to be kept open. Both forward-development and backward-
development require machine support. Usually a combined method is used in which one 
works from both sides -
for example a forward-development towards certain standard 
subtasks which are well known. 
The amount of intuition required in this process cannot be over-estimated. Thus 
heuristic methods as in "artificial intelligence" are of limited importance. 
Conceptual Organization of the Algorithmic Language 
We are thoroughly convinced that the conceptual organization of the ALGORITHMIC 
LANGUAGE should be guided by the process of program development. However we 
should not expect that the difference between a forward and a backward-development re-
sults in different concepts - , since every forward-development can be re-written into a 
backward-development and vice versa. But in which direction should the organization be 
taught? 

456 
Programming as an Evolutionary Process 
In this book the completely binary-organized storage machine is at the end of a logical 
development. It can also be placed in front according to historical development. The con-
struction of a language which then goes into the opposite direction is, as examples have 
shown, in danger of getting stuck in details. It can also lead us astray: "The sneaky reintro-
duction of patently pernicious facilities from the era of machine coding is not an accept-
able solution" (Wirth 1974). The reader will have noticed that our method entails a critical 
view of jumps, mode-unspecific objects and addresses -
to mention only three of the 
main problems which Wirth had in view. 
Zuse's "Plankalkiil" followed the path of consequent backward-development, starting 
from the bit. For a long time this seemed to be a natural method. If, however, we consider 
some textbooks, all of which start with the definition of "our machine" -
but each of 
them with a different one - the pedagogical usefulness of this method seems doubtful. 
Frequently a backward-development causes the "finer" art of programming, which then 
comes into play later during the teaching process, to be neglected or suppressed. 
From the (short-sighted) point of view of the manufacturer it may be an advantage to 
proceed from one individual machine (and thus to commit those trained as "systems pro-
grammers", "systems analysts", "EDP-specialists" to a certain brand of machine). 
In scientific training it is absolutely necessary to teach the conceptual development of 
programming along the path "from the problem to the machine" and to declare the 
machine(s) as being the product of a development process. The decisive didactical 
advantage is the increase in the ability of abstracting. "Top down teaching" in this sense 
has many times been used successfully and forms the basis for this book, too. 
Tools to Be Used 
A desire for disciplined freedom is expressed in our attitude towards recursion. Many pro-
fessional programmers - not only those who can write programs in FORTRAN only -
know as much about recursion as a beginner on the violin does about flageolet tones. 
Wirth (1976) has a paragraph on "when not to use recursion". Aware of a "widespread 
apprehension and antipathy toward the use of recursion", he argues apologetically: " ... 
the explanation of the concept of recursive algorithm by such inappropriate examples has 
been a chief cause ... of equating recursion with inefficiency", and draws the conclusion 
" ... to avoid the use of recursion when there is an obvious solution by iteration". However 
he points out that "algorithms which by their nature are recursive rather than iterative 
should be formulated as recursive procedures". 
But what is an "obvious solution by iteration" or even an "algorithm recursive by its 
nature"? This book tries to find an answer by incorporating the problem specification in 
the form of a pre-algorithmic version into the programming process and by considering the 
latter as a stepwise improvement and refinement. Then an "obvious solution by iteration" 
is a solution which is obtained by known transformation schemes. An algorithm recursive 
by nature is probably one where no known transformation is applicable for obtaining an 
iterative version - or perhaps no transformation is yet known or not even the contour of a 
methodical treatment. However the situation can change quickly (comp. the transforma-
tion of the recursive version of a problem class, to which "Towers of Hanoi" belongs, into 
an iterative version based on binary counting, according to Partsch and Pepper in 4.3.2). 
What an algorithm recursive "by nature" may be, can be left open. The important 
thing is that according to the new conception of programming the "horror procedurae" 

Programming as an Evolutionary Process 
457 
becomes quite unnecessary. Experienced programmers will, as a general rule, use recursive 
procedures in a certain phase of the program development but will almost always go be-
yond this level of development, at least if the relative efficiency of the algorithm is of suffi-
cient importance. 
Thus even the excuse that FORTRAN does not permit the recursive use of subroutines 
becomes rather weak. (However it is not an easy task in FORTRAN to tackle the "ultima 
ratio" of the iterative treatment of recursive procedures, the introduction of stacks (stack 
variables), if we consider that stacks must be simulated in FORTRAN by arrays with fixed 
index bounds.) 
A relaxed attitude with regard to recursion in data structures is necessary, too. Imple-
mentations using pointers, i.e. records in Hoare's sense, should not be introduced too 
early in the development process because of the obvious disadvantages with respect to 
clarity and safety. This applies as well to jumps in the case of procedures, although there 
are some harmless jumps which turn out to be "simple calls". In any case the crusade 
("war against pointers") should be directed equally against pointers and jumps, but it 
should not be exaggerated. 
The use of sets as objects of programming should likewise be considered in a more 
natural way. For example the objection that the cardinality of the basic set should be small 
- if possible not bigger than the length of the word (should it be 24 or 60?) - is valid only 
as long as such kinds of sets occasionally remain in the program development to the very 
end. Frequently sets appear only in the early or intermediate stages of the program 
development. Then the cardinality of the basic set is of no importance; indeed even the set 
of natural numbers can be employed if this can help to simplify the solution. 
As far as we can see there is only one point in which we really differ from Wirth (apart 
from notation and terminology). This refers to collaterality. We believe that this is some-
thing quite natural and that sequentialization must be explained (it represents the "Fall of 
man"). We consider the absence of any possibility of expressing collateral situations in 
PASCAL -
at least the absence of collective assignments -
to be a real drawback: 
anyone who has written 
(y,x) : = (x,x + y) 
knows that the sequentialization 
Z: = x; X:= X + y; Y: = Z 
only obscures things and that the "tricky" sequentialization 
X : = X + y; y : = X -
y 
should be left in grandmother's trunk; jugglers' tricks should disappear from computer 
science. 
Methodology of Programming 
There are various books on the methodology of programming; we recommend e.g. 
Dijkstra 1976: "A Discipline of Programming", Wirth 1976: "Algorithms + Data Struc-

458 
Programming as an Evolutionary Process 
tures 
Programs", Arsac 1977: "La Construction de Programmes Structurees", Turski 
1978: "Computer Programming Methodology", Gries (ed.) 1978a: "Programming Meth-
odology, a Collection of Articles by Members of WG 2.3", Henderson 1980: "Functional 
Programming", Gries 1981: "The Science of Programming". In this book we have laid 
special emphasis on the conceptual framework. Due to lack of space remarks on the 
methodology had to be kept to a minimum. Nevertheless this book can serve as a text book 
for practical exercises in program development. Programming cannot be learned merely 
from reading a book. 
We do not believe in naive program verification; after all programs do not fall from 
heaven for being verified. However we do agree with Gries that program verification -
considered as a simultaneous development of program and proof -
is only a different, 
special form of program development. 
Altogether more freedom in the choice of programming tools is necessary (despite the 
disciplined use of the chosen tool). Hence we consider what is called the "applicative" style 
of programming merely as an alternative to the predominant "procedural" style and not as 
a new doctrine of salvation. Backus, too, seems to tend to this view in spite of some radical 
remarks. 
Recently the word "discipline" has been appearing more frequently, e.g. in Seegmiiller 
1974a: "Systems Programming as an Emerging Discipline" or Dijkstra 1976: "A 
Discipline of Programming". In Bauer 1975 we can read "Programming as a scientific dis-
cipline means: Programming can be taught, is to be taught. Programming needs 
discipline". It can be noticed that the word "discipline" with this second meaning is also 
more frequently used, in statements such as "flexibility without discipline appears contrary 
to reliability" (Denning 1976). Programming, after all, is also a matter of mentality. Many 
programmers acquire only by painful experience: "the insight that it is best to write pro-
grams correctly from the very beginning" (Seegmiiller 1974a). 
"Most problems have either many answers or no 
answer. Only a few problems have a single answer." 
Edmund C. Berkeley 

Bibliography 
Ackermann, W. (1928): Zum HilbertschenAufbau der reellen Zahlen. Math. Ann. 99, 118-133 (1928) 
Adelson-Velskii, G. M., Landis, E. M. (1962): An Algorithm for the Organization of Information. 
Dokl. Akad. Nauk SSSR 146, 263-266 (1962) (in Russian). English translation in: Soviet Math. 
Dokl. 3, 1259-1263 (1962) 
Aho, A. V., Hopcroft, J. E., Ullmann, J.D. (1974): The Design and Analysis of Computer Algo-
rithms. Reading, Mass.: Addison-Wesley 1974 
Aho, A. V., Ullman, J.D. (1972): The Theory of Parsing, Translation, and Compiling. Englewood 
Cliffs, N. J.: Prentice-Hall, Vol. I 1972, Vol. II 1973 
Aiken, H. H. (1937): Proposed Automatic Calculating Machine. Manuscript 1937. In: Randell1973, 
p. 191-197 
Arsac, J. J. (1977): La Construction de Programmes Structures. Paris: Dunod 1977 
Babbage, C. (1837): On the Mathematical Powers of the Calculating Engine. Manuscript 1837. In: 
Randell1973, p. 17-52 
Backus, J. (1973): Programming Language Semantics and Closed Applicative Languages. Conferen-
ce Record of the 1st ACM Symposium on Principles of Programming Languages, Boston 1973, p. 
71-86 
Backus, J. (1978a): Can Programming be Liberated from the von Neumann Style? A Functional Style 
and its Algebra of Programs. Commun. ACM 21, 613-641 (1978) 
Backus, J. (1978b): The History of FORTRAN I, II, and III. Preprints ACM SIGPLAN History of 
Programming Languages Conference, Los Angeles 1978. SIGPLAN Notices 13: 8, 165-180 
(1978) 
de Bakker, J. W. (1969): Semantics of Programming Languages. In: Tou, J. (ed.): Advances in Infor-
mation Systems Science, Vol. 2. New York: Plenum Press 1969, p. 173-227 
de Bakker, J. W. (1976): Semantics and Termination of Nondeterministic Recursive Programs. In: 
Michaelson, S., Milner, R. (eds.): Automata, Languages and Programming, Proceedings 1976. 
Edinburgh: Edinburgh University Press 1976, p. 435-477 
de Bakker, J. W., Scott, D. (1969): A Theory of Programs. IBM Seminar, Vienna 1969, unpublished 
manuscript 
Bauer, F. L. (1971): Software Engineering. Proc. IFIP Congress 71, Ljubljana. Amsterdam: North-
Holland 1971, p. 530- 538 
Bauer, F. L. (1975): Programming as an Evolutionary Process. Proc. 2nd International Conference 
on Software Engineering, San Francisco 1976, p. 223-234. Also in: Bauer, F. L., Samelson, K. 
(eds.): Language Hierarchies and Interfaces. International Summer School, Marktoberdorf 1975. 
Lecture Notes in Computer Science, Vol. 46. Berlin-Heidelberg-New York: Springer 1976, p. 
153-182 
Bauer, F. L. (1981): Algorithms and Algebra. In: Ershov, A. P., Knuth, D. E. (eds.): Algorithms in 
Modern Mathematics and Computer Science. Lecture Notes in Computer Science, Vol. 122. Ber-
lin-Heidelberg-New York: Springer 1981, p. 421-429 
Bauer, F. L., Broy, M. (eds.) (1979): Program Construction. International Summer School, Markt-
oberdorf 1978. Lecture Notes in Computer Science, Vol. 69. Berlin-Heidelberg-New York: Sprin-
ger 1979 
Bauer, F. L., Broy, M., Dosch, W., Gnatz, R., Krieg-Briickner, B., Laut, A., Luckmann, M., Matz-
ner, T., Moller, B., Partsch, H., Pepper, P., Samelson, K., Steinbriiggen, R., Wirsing, M., 
Wossner, H. (1981 ): Programming in a Wide Spectrum Language: A Collection of Examples. Sci. 
Comp. Program. 1, 73 -114 (1981) 

460 
Bibliography 
Bauer, F. L., Samelson, K. (1957): Verfahren zur automatischen Verarbeitung von kodierten Daten 
und Rechenmaschine zur Ausiibung des Verfahrens. Deutsches Patentamt, Auslegeschrift 
1094019. Filed March 30, 1957, published December 1, 1960 
Bauer, F. L., Samelson, K. (1958): Automatic Computing Machines and Method of Operation. 
United States Patent Nr. 3047228. Filed March 28, 1958, patented July 31, 1962 
Bayer, R. (1971): Binary B-Trees for Virtual Memory. In: Codd, E. F., Dean, A. L. (eds.): Proc. 
1971 ACM-SIGFIDET Workshop on Data Description, Access and Control, San Diego, Cal., 
1971, p. 219-235. Cf. also: Bayer, R., McCreight, E. M.: Organization and Maintenance of 
Large Ordered Indexes. Acta Informatica 1, 173-189 (1972) 
Belady, L. A. (1966): A Study of Replacement Algorithms for a Virtual Storage Computer. IBM 
Syst. J. 5, 78-101 (1966) 
Berkling, K. J. (1974): Reduction Languages for Reduction Machines. Proc. 2nd Annual Symposium 
on Computer Architecture, Houston 1975. New York: IEEE 1975 and ACM-SIGARCH 
Computer Architecture News 3, No. 4, December 1974, p. 133-140. Extended version: GMD 
Bonn, Internal Report ISF-76-8, 1976 
Bernstein, A. J. (1966): Analysis of Programs for Parallel Processing. IEEE Trans. Electronic 
Computers 15, 757-763 (1966) 
Bobrow, D. G., Raphael, B. (1964): A Comparison of List-Processing Computer Languages. 
Commun. ACM 7, 231-240 (1964) 
Borel, E. (1912): Le Calcul des Integrates DHinies. Journal des Matbematiques Pures et Appliquees, 
Ser. 6, Vol. 8, Nr. 2, 159-210 (1912). Reprinted in: La Tbeorie des Fonctions. Paris: Gauthier-
Villars 1914, p. 217-256 
Bottenbruch, H. (1958): Obersetzung von algorithmischen Formelsprachen in die Programmsprachen 
von Rechenmaschinen. Z. math. Logik Grund!. Math. 4, 180-221 (1958) 
Brinch Hansen, P. (1978): Distributed Processes: A Concurrent Programming Concept. Commun. 
ACM 21, 934-941 (1978) 
Broy, M. (1980): Transformation parallel ablaufender Programme. Fakultat fiir Mathematik der TU 
Miinchen, Dissertation, TUM-I 8001, 1980 
Broy, M., Gnatz, R., Wirsing, M. (1979): Semantics of Nondeterministic and Noncontinuous 
Constructs. In: Bauer, Broy 1979, p. 553-592 
Broy, M., Moller, B., Pepper, P., Wirsing, M. (1980): A Model-Independent Approach to 
Implementations of Abstract Data Types. In: Salwicki, A. (ed.): Proc. Symposium on 
Algorithmic Logic and the Programming Language LOGLAN, Poznan, Polen, 1980. Lecture 
Notes in Computer Science. Berlin-Heidelberg-New York: Springer (to appear) 
Broy, M., Partsch, H., Pepper, P., Wirsing, M. (1980): Semantic Relations in Programming 
Languages. In: Lavington, S. H. (ed.): Information Processing 80. Amsterdam: North-Holland 
1980, p.101-106 
Broy, M., Pepper, P., Wirsing, M. (1982): On the Algebraic Definition of Programming Languages. 
Technische UniversiUit Miinchen, Institut fiir Informatik, TUM-I 8204, 1982 
Broy, M., Schmidt, G. (eds.) (1982): Theoretical Foundations of Programming Methodology. 
International Summer School, Marktoberdorf 1981. Dordrecht: Reidel 1982 
Broy, M., Wirsing, M. (1980): Programming Languages as Abstract Data Types. In: Dauchet, M. 
(ed.): 5eme Colloque sur les Arbres en Algebre et en Programmation, Lille 1980, p. 160-177 
Burstall, R. M. (1968): Semantics of Assignment. In: Dale, E., Michie, D. (eds.): Machine 
Intelligence, Vol. 2. Edinburgh: Oliver and Boyd 1968, p. 3 - 20 
Burs tall, R. M. (1969): Proving Properties of Programs by Structural Induction. Computer J. 12, 
41 -48 (1969) 
Burstall, R. M., Darlington, J. (1977): A Transformation System for Developing Recursive 
Programs. J. ACM 24, 44-67 (1977) 
Burstall, R. M., Goguen, J. A. (1977): Putting Theories together to Make Specifications. Proc. 5th 
International Joint Conference on Artificial Intelligence, Cambridge, Mass., 1977, p. 1045-1058 
Carlson, B. C. (1971): Algorithms Involving Arithmetic and Geometric Means. Amer. Math. 
Monthly 78,496-505 (1971) 
Church, A. (1936): A Note on the Entscheidungsproblem. J. Symbolic Logic 1, 40-41, 101-102 
(1936) 
Church, A. (1941): The Calculi of Lambda-Conversion. Annals of Mathematics Studies, Vol. 6. 
Princeton: Princeton University Press 1941 

Bibliography 
461 
Clifford, A. H., Preston, G. B. (1961): The Algebraic Theory of Semigroups, Vol. I. Providence, R. 
1.: American Mathematical Society 1961 (Vol. II 1967) 
Clint, M., Hoare, C. A. R. (1971): Program Proving: Jumps and Functions. International Summer 
School on Program Structures and Fundamental Concepts of Programming, Marktoberdorf 
1971. Also Acta Informatica 1, 214-224 (1972) 
Collins, G. E. (1967): The SAC-1 List Processing System. University of Wisconsin, Computing 
Center, Technical Report, July 1967. Reprint: University of Wisconsin, Computer Sciences 
Department, Technical Report No. 129, 1971 
Conway, M. E. (1963): Design of a Separable Transition-Diagram Compiler. Commun. ACM 6, 
396-408 (1963) 
Cooper, D. C. (1966): The Equivalence of Certain Computations. Computer J. 9, 45-52 (1966) 
Courcelle, B., Nivat, M. (1976): Algebraic Families of Interpretations. Proc. 17th Annual 
Symposium on Foundations of Computer Science, Houston 1976, p. 137-146 
Curry, H. B., Feys, R. (1958): Combinatory Logic, Vol. I. Amsterdam: North-Holland 1958 
Dahl, 0.-J., Dijkstra, E. W., Hoare, C. A. R. (1972): Structured Programming. London: Academic 
Press 1972 
Dahl, 0.-J., Hoare, C. A. R. (1972): Hierarchical Program Structures. In: Dahl, Dijkstra, Hoare 
1972, p. 175-220 
Damm, W., Fehr, E. (1978): On the Power of Self-Application and Higher Type Recursion. In: Ausi-
ello, G., Bohm, C. (eds.): Automata, Languages and Programming, Proceedings 1978. Lecture 
Notes in Computer Science, Vol. 62. Berlin-Heidelberg-New York: Springer 1978, p. 177 -199 
Darlington, J., Burstall, R. M. (1973): A System which Automatically Improves Programs. Proc. 3rd 
InternationalJ oint Conference on Artificial Intelligence, Stanford, Cal., 1973, p. 4 79 - 485. Also 
Acta Informatica 6, 41-60 (1976) 
Davis, M. (1958): Computability and Unsolvability. New York-Toronto-London: McGraw-Hill1958 
Denning, P. J. (1976): Sacrificing the Calf of Flexibility on the Altar of Reliability. Proc. 2nd 
International Conference on Software Engineering, San Francisco 1976, p. 384- 386 
Dennis, J. B. (1973): Concurrency in Software Systems. In: Bauer, F. L. (ed.): Advanced Course on 
Software Engineering. Lecture Notes in Computer Science, Vol. 30. Berlin-Heidelberg-New 
York: Springer 1973, p. 111-127 
Dennis, J. B. (1979): The Varieties of Data Flow Computers. Proc. 1st International Conference on 
Distributed Computing Systems, Huntsville, Alabama, 1979. New York: IEEE 1979, p. 430-439 
Dijkstra, E. W. (1960): Recursive Programming. Numerische Math. 2, 312-318 (1960) 
Dijkstra, E. W. (1965): Cooperating Sequential Processes. Technological University, Eindhoven 
1965. Reprinted in Genuys, F. (ed.): Programming Languages. London-New York: Academic 
Press 1968, p. 43-112 
Dijkstra, E. W. (1969): Structured Programming. In: Buxton, J. N., Randell, B. (eds.): Software 
Engineering Techniques, Report on a Conference, Rome 1969. Briissel: NATO Scientific Affairs 
Division 1970, p. 84-88 
Dijkstra, E. W. (1972): Notes on Structured Programming. In: Dahl, Dijkstra, Hoare 1972, p. 1 -82. 
Dijkstra, E. W. (1974): A Simple Axiomatic Basis for Programming Language Constructs. 
Indagationes Math. 36, 1-15 (1974) 
Dijkstra, E. W. (1975): Guarded Commands, Nondeterminacy and Formal Derivation of Programs. 
Commun. ACM 18,453-457 (1975) 
Dijkstra, E. W. (1976): A Discipline of Programming. Englewood Cliffs, N.J.: Prentice-Hall1976 
Dyck, W. (1882): Gruppentheoretische Studien. Math. Ann. 20, 1 -44 (1882) 
Earley, J. (1971): Towards an Understanding of Data Structures. Commun. ACM 14, 617-627 
(1971) 
Egli, H. (1975): A Mathematical Model for Nondeterministic Computations. Forschungsinstitut fiir 
Mathematik der ETH Ziirich, 1975 
Bickel, J. (1974): ,Algorithmus" und Grenzen der Algorithmisierbarkeit. Abteilung Mathematik der 
TU Miinchen, Bericht Nr. 7413, 1974, p. 43-70. Also in: Weinhart, K. (ed.): Informatik im 
Unterricht -
eine Handreichung. Mathematik -
Didaktik und Unterrichtspraxis. Bd. 2. 
Miinchen-Wien: Oldenbourg 1979, p. 58-76 
Eickel, J., Paul, M. (1964): The Parsing and Ambiguity Problem for Chomsky-Languages. In: Steel, 
T. B. Jr. (ed.): Formal Language Description Languages for Computer Programming. 
Amsterdam: North-Holland 1966, p. 52-75 

462 
Bibliography 
Ershov, A. P. (1977): On the Essence of Compilation. In: Neuhold, E. J. (ed.): Proc. IFIP Working 
Conference on Formal Description of Programming Concepts, St. Andrews, Canada, 1977. 
Amsterdam: North-Holland 1978, p. 391 -420 
Faltin, F., Metropolis, N., Ross, B., Rota, G.-C. (1975): The Real Numbers as a Wreath Product. 
Advances Math. 16, 278-304 (1975) 
Fischer, M. J. (1972): Lambda-Calculus Schemata. SIGPLAN Notices 7: 1, 104-109 (1972) 
Floyd, R. W. (1966): Assigning Meaning to Programs. In: Schwartz, J. T. (ed.): Mathematical 
Aspects of Computer Science. Proc. Symposia in Applied Mathematics, Vol. XIX, 1966. Pro-
vidence, R. I.: American Mathematical Society 1967, p. 19- 32 
Floyd, R. W. (1967): Nondeterministic Algorithms. J. ACM 14, 636-644 (1967) 
Friedman, D. P., Wise, D. S. (1976): CONS Should Not Evaluate its Arguments. In: Michaelson, S., 
Milner, R. (eds.): Automata, Languages and Programming, Proceedings 1976. Edinburgh: 
Edinburgh University Press 1976, p. 257-284 
Friedman, D. P., Wise, D. S. (1978): Unbounded Computational Structures. Software, Practice 
Experience 8, 407-416 (1978) 
Galton, F. (1889): Natural Inheritance. London: Macmillan 1889 
Geschke, C. M., Morris, J. H. jr., Satterthwaite, E. H. (1977): Early Experience with Mesa. 
Commun. ACM 20, 540- 553 (1977) 
Gill, S. (1965): Automatic Computing: Its Problems and Prizes. Computer J. 8, 177- 189 (1965) 
Gnatz, R., Pepper, P. (1977): fuse: An Example in Program Development. Institut fiir Informatik 
derTU Miinchen, TUM-INF0-7711, 1977 
Go del, K. (1931 ): Ober formal unentscheidbare Satze der Principia Mathematica und verwandter 
Systeme I. Monatsh. Math. Phys. 38, 173-198 (1931) 
Goguen, J. A., Tardo, J. (1977): OBJ-0 Preliminary Users Manual. University of California at Los 
Angeles, Computer Science Department, 1977 
Goguen, J. A., Thatcher, J. W., Wagner, E. G. (1977): Initial Algebra Semantics and Continuous 
Algebras. J. ACM 24, 68-95 (1977) 
Goguen, J. A., Thatcher, J. W., Wagner, E. G. (1978): An Initial Algebra Approach to the 
Specification, Correctness, and Implementation of Abstract Data Types. In: Yeh, R. T. (ed.): 
Current Trends in Programming Methodology, Vol. 4. Englewood Cliffs, N.J.: Prentice-Ha111978, 
p. 80-149 
Goldstine, H. H., von Neumann, J. (1947): Planning and Coding Problems for an Electronic 
Computing Instrument. Part II, Vol. 1, 1947. In: John von Neumann, Collected Works, Vol. V. 
Oxford: Pergamon Press 1963, p. 80-151 
Goraon, M. (1975): Operational Reasoning and Denotational Semantics. Stanford University, 
Computer Science Department, Memo AIM-264, 1975. Also in: Huet, G., Kahn, G. (eds.): 
Construction, Amelioration et Verification des Programmes. Colloques IRIA 1975, p. 83-98 
Graham, R. M. (1975): Principles of Systems Programming. New York: Wiley 1975 
Gries, D. (1978): The Multiple Assignment Statement. IEEE Trans. Software Eng. 4, 89- 93 (1978) 
Gries, D. (ed.) (1978a): Programming Methodology: A Collection of Articles by Members of IFIP 
WG 2.3. Berlin-Heidelberg-New York: Springer 1978 
Gries, D. (1979): Current Ideas in Programming Methodology. In Bauer, Broy 1979, p. 77-93 
Gries, D. (1981): The Science of Programming. Berlin-Heidelberg-New York: Springer 1981 
Griffiths, M. (1975): Program Production by Successive Transformations. In: Bauer, F. L., 
Samelson, K. (eds.): Language Hierarchies and Interfaces. International Summer School, 
Marktoberdorf 1975. Lecture Notes in Computer Science, Vol. 46. Berlin-Heidelberg-New York: 
Springer 1976, p. 125-152 
Guttag, J. V. (1975): The Specification and Application to Programming of Abstract Data Types. 
University of Toronto, Department of Computer Science, Ph. D. Thesis, Report CSRG-59, 1975 
Harvard Symposium 1947: Proceedings of a Symposium on Large-Scale Digital Calculating Machin-
ery. The Annals of the Computation Laboratory of Harvard University, Vol. XVI. Cambridge, 
Mass.: Harvard University Press 1948 
Haskell, R. (1975): Efficient Implementation of a Class of Recursively Defined Functions. Computer 
J. 18, 23-29 (1975) 
Hasse, H. (1951): Hohere Algebra. Vol. I. 3rd ed. Berlin: De Gruyter 1951 
Hehner, E. C. R. (1979): Do considered od: A Contribution to the Programming Calculus. Acta 
Informatica 11, 287-304 (1979) 

Bibliography 
463 
Henderson, P. (1980): Functional Programming: Application and Implementation. Englewood 
Cliffs, N.J.: Prentice-Hall1980 
Henderson, P., Morris, J. H. jr. (1976): A Lazy Evaluator. Conference Record of the 3rd ACM 
Symposium on Principles of Programming Languages, Atlanta 1976, p. 95-103 
von Henke, F. W. (1975): On Generating Programs from Types: An Approach to Automatic 
Programming. In: Huet, G., Kahn, G. (eds.): Construction, Amelioration et Verification des 
Programmes. Colloques IRIA 1975, p. 57-69 
Herbrand, J. (1931): Sur Ia Non-Contradiction de l'Arithmetique. J. reine angew. Math. 166, 1-8 
(1931) 
Hermes, H. (1978): Aufzahlbarkeit, Entscheidbarkeit, Berechenbarkeit. 3rd ed. Berlin-Heidelberg-
New York: Springer 1978 
Hewitt, C. (1977): Viewing Control Structures as Patterns of Passing Messages. Artificial Intelligence 
8, 323- 364 (1977) 
Hilbert, D. (1918): Axiomatisches Denken. Math. Ann. 78, 405-415 (1918) 
Hilbert, D., Bernays, P. (1934): Grundlagen der Mathematik, Bd. 1. Berlin: Springer 1934. 2nd ed. 
1968 
Hilbert, D., Bernays, P. (1939): Grundlagen der Mathematik, Bd. 2. Berlin: Springer 1939. 2nd ed. 
1970 
Hoare, C. A. R. (1965): Record Handling. Algol Bull. 21, 39-69 (1965). Extended version in: 
Genuys, F. (ed.): Programming Languages. London: Academic Press 1968, p. 291-347 
Hoare, C. A. R. (1969): An Axiomatic Basis for Computer Programming. Commun. ACM 12, 
576- 583 (1969) 
Hoare, C. A. R. (1970): Notes on Data Structuring. International Summer School on Data Structures 
and Computer Systems, Marktoberdorf 1970. Extended version in: Dahl, Dijkstra, Hoare 1972, 
p. 83-174 
Hoare, C. A. R. (1971): Towards a Theory of Parallel Programming. International Seminar on Op-
erating System Techniques, Belfast 1971. Also in: Hoare, C. A. R., Perrott, R. (eds.): Operating 
Systems Techniques. New York: Academic Press 1972, p. 61-71 
Hoare, C. A. R. (1972): Proof of Correctness of Data Representations. Acta Informatica 1, 271-281 
(1972) 
Hoare, C. A. R. (1973): Recursive Data Structures. Stanford University, Computer Science 
Department, Report ST AN-CS-73-400, 1973. Extended version: International J. Computer 
Inform. Sci. 4, 105-132 (1975) 
Hoare, C. A. R. (1978): Communicating Sequential Processes. Commun. ACM 21, 666-678 (1978) 
Hoare, C. A. R., Wirth, N. (1973): An Axiomatic Definition of the Programming Language Pascal. 
Acta Informatica 2, 335- 355 (1973) 
Hopcroft, J. E., Ullman, J. D. (1969): Formal Languages and Their Relation to Automata. Reading, 
Mass.: Addison-Wesley 1969 
Householder, A. S. (1953): Principles of Numerical Analysis. New York: McGraw-Hill1953 
Huntington, E. V. (1933): New Sets of Independent Postulates for the Algebra of Logic, with Special 
Reference to Whitehead and Russel's Principia Mathematica. Trans. Amer. Math. Soc. 35, 
274- 304, 557- 558 (1933) 
Kandzia, P., Langmaack, H. (1973): Informatik: Programmierung. Stuttgart: Teubner 1973 
Kantorovic, L. V. (1957): On a Mathematical Symbolism Convenient for Performing Machine 
Calculations (in Russian). Doklady Akad. Nauk SSSR 113, 738-741 (1957) 
Kennaway, J. R., Hoare, C. A. R. (1980): A Theory of Nondeterminism. In: de Bakker, J. W., van 
Leeuwen, J. (eds.): Automata, Languages and Programming, Proceedings 1980. Lecture Notes in 
Computer Science, Vol. 85. Berlin-Heidelberg-New York: Springer 1980, p. 338-350 
Kilburn, T. (1949): The University of Manchester Universal High-Speed Digital Computing Machine. 
Nature 164, 684-687 (1949) 
Kleene, S. C. (1936): General Recursive Functions of Natural Numbers. Math. Ann. 112, 727 -7~2 
(1936) 
Kleene, S. C. (1952): Introduction to Metamathematics. New York: Van Nostrand 1952 
Knuth, D. E. (1973): The Art of Computer Programming, Vol. 1: Fundamental Algorithms, 2nd ed. 
Reading, Mass.: Addison-Wesley 1973 
Knuth, D. E. (1974): Structured Programming with go to Statements. Computing Surveys 6, 
261-301 (1974) 

464 
Bibliography 
Kosaraju, S. R. (1973): Limitations of Dijkstra's Semaphore Primitives and Petri Nets. Operating 
Systems Review 7:4,122-126 (1973) 
Lame, G. (1844): Note sur Ia Limite du Nombre des Divisions dans Ia Recherche du Plus Grand 
Commun Diviseur entre Deux Nombres Entiers. C. R. Acad. Sci., Paris, 19, 867-870 (1844) 
Landin, P. J. (1964): The Mechanical Evaluation of Expressions. Computer J. 6, 308- 320 (1964) 
Landin, P. J. (1965): A Correspondence Between ALGOL 60 and Church's Lambda-Notation: Part 
I. Commun. ACM 8, 89-101 (1965) 
Landin, P. J. (1966): The Next 700 Programming Languages. Commun. ACM 9, 157-166 (1966) 
Langmaack, H. (1974): On Procedures as Open Subroutines II. Acta Informatica 3, 227- 241 (1974) 
Langmaack, H., Olderog, E.-R. (1980): Present-Day Hoare-Like Systems for Programming 
Languages with Procedures: Power, Limits and Most Likely Extensions. In: de Bakker, J. W., 
van Leeuwen, J. (eds.): Automata, Languages and Programming, Proceedings 1980. Lecture 
Notes in Computer Science, Vol. 85. Berlin-Heidelberg-New York: Springer 1980, p. 363-373 
Laut, A. (1980): Safe Procedural Implementations of Algebraic Types. Inform. Processing Letters 
11, 147-151 (1980) 
Ledgard, H. F. (1971): Ten Mini-Languages, a Study of Topical Issues in Programming Languages. 
Computing Surveys 3, 115-146 (1971) 
Lippe, W. M., Simon, F. (1980): Semantics for LISP without Reference to an Interpreter. In: 
Robinet, B. (ed.): International Symposium on Programming, Proceedings 1980. Lecture Notes 
in Computer Science, Vol. 83. Berlin-Heidelberg-New York: Springer 1980, p. 240-255 
Liskov, B. H., Zilles, S. N. (1974): Programming with Abstract Data Types. Proc. ACM Conference 
on Very High-Level Languages. SIGPLAN Notices 9:4, 50-59 (1974) 
Liskov, B. H., Zilles, S. N. (1975): Specification Techniques for Data Abstractions: IEEE Trans. 
Software Eng. 1, 7-19 (1975) 
Liskov, B. H., Snyder, A., Atkinson, R., Schaffert, C. (1977): Abstraction Mechanisms in CLU. 
Commun. ACM 20, 564- 576 (1977) 
Lonseth, A. T. (1945): An Extension of an Algorithm of Hotelling. Proc. Berkeley Symposium 
Mathematical Statistics and Probability 1945, 1946. Berkeley-Los Angeles: University of 
California Press 1949, p. 353- 357 
Lorenzen, P. (1962): Metamathematik. Mannheim: Bibliographisches Institut 1962 
Lukasiewicz, J. (1963): Elements of Mathematical Logic. Oxford: Pergamon Press 1963 
Mag6, G. A. (1979): A Network of Microprocessors to Execute Reduction Languages. Internat. J. 
Computer Inform. Sci. 8, 349- 358, 435-471 (1979) 
Malcev, A. I. (1939): Ober die Einbettung von assoziativen Systemen in Gruppen. Mat. Sbornik, n. 
Ser. 6, 331 - 336 (1939) 
Manna, Z. (1974): Mathematical Theory of Computation. New York: McGraw-Hill1974 
Manna, z. (1980): Logics of Programs. In: Lavington, S. H. (ed.): Information Processing 80. 
Amsterdam: North-Holland 1980, p. 41-51 
Manna, Z., McCarthy, J. (1969): Properties of Programs and Partial Function Logic. In: Michie, D. 
(ed.): Machine Intelligence, Vol. 5. Edinburgh: Edinburgh University Press 1969, p. 27-37 
Manna, Z., Ness, S., Vuillemin, J. (1973): Inductive Methods for Proving Properties of Programs. 
Commun. ACM 16, 491-502 (1973) 
Markov, A. A. (1951): Theory of Algorithms (in Russian). Trudy Mat. Inst. Steklov 38, 176-189 
(1951). English translation: Proc. Steklov Inst. Math., II. Ser. 15, 1-14 (1960) 
McCarthy, J. (1959): Letter to the Editor. Commun. ACM 2:8, 2-3 (1959) 
McCarthy, J. (1960): Recursive Functions of Symbolic Expressions and their Computation by 
Machine, Part I. Commun. ACM 3, 184-195 (1960) 
McCarthy, J. (1961): A Basis for a Mathematical Theory of Computation. Extended version of a 
lecture given at Western Joint Computer Conference 1961. In: Braffort, P., Hirschberg, D. 
(eds.): Computer Programming and Formal Systems. Amsterdam: North-Holland 1963, p. 
33-70 
McCarthy, J. (1962): Towards a Mathematical Science of Computation. Proc. IFIP Congress 62, 
Mtinchen. Amsterdam: North-Holland 1962, p. 21 - 28 
Michie, D. (1968): Memo-Functions -
a Language Feature with Role Learning Properties. In: 
Experimental Programming 1966- 7, Edinburgh University, Dept. of Machine Intelligence and 
Perception, January 1968 
Morris, J. H. jr. (1968): Lambda-Calculus Models of Programming Languages. Massachusetts 

Bibliography 
465 
Institute of Technology, Cambridge, Mass., Ph. D. Thesis. Project MAC Report MAC-TR-37, 
1968 
Morris, J. H. jr. (1971): Another Recursion Induction Principle. Commun. ACM 14, 351-354 (1971) 
Myhill, J. (1953): Criteria of Constructibility for Real Numbers. J. Symbolic Logic 18, 7-10 (1953) 
von Neumann 1947: see Goldstine, von Neumann 1947 
Newell, A., Shaw, J. C. (1957): Programming the Logic Theory Machine. Proc. Western Joint 
Computer Conference 1957, p. 230-240 
Newell, A., Simon, H. A. (1956): The Logic Theory Machine: A Complex Information Processing 
System. IRE Trans. Inform. Theory 2, 61 -79 (1956) 
Olds, D. C. (1963): Continued Fractions. Mathematical Association of America, Yale University 
1963 
Parnas, D. L. (1972): On a Solution to the Cigarette Smokers' Problem (without Conditional 
Statements). Carnegie-Mellon University, Pittsburgh, Pa., Computer Science Department 1972 
Partsch, H., Pepper, P. (1976): A Family of Rules for Recursion Removal. Inform. Processing 
Letters 5, 174-177 (1976) 
Paterson, M. S., Hewitt, C. E. (1970): Comparative Schematology. Record of the Project MAC 
Conference on Concurrent Systems and Parallel Computation, Woods Hole, Mass., 1970. New 
York: ACM 1970, p. 119-127 
Peano, G. (1889): Arithmetices Principia Nova Methodo Exposita. Turin: Bocca 1889 
Pepper, P. (1979): A Study on Transformational Semantics. In: Bauer, Broy 1979, p. 322-405 
Perlis, A. J., Thornton, C. (1960): Symbol Manipulation by Threaded Lists. Commun. ACM 3, 
195-204 (1960) 
Peter, R. (1976): Rekursive Funktionen in der Komputer-Theorie. Budapest: Akademiai Kiado 1976 
Peterson, J. L. (1981): Petri Net Theory and the Modeling of Systems. Englewood Cliffs, N. J.: 
Prentice-Hall 1981 
Petri, C. A. (1962): Kommunikation mit Automaten. Schriften des Rheinisch-Westfiilischen Instituts 
fiir Instrumentelle Mathematik an der UniversiUit Bonn, Heft 2, 1962 
Plotkin, G. D. (1976): A Powerdomain Construction. SIAM J. Computing 5, 452-487 (1976) 
Plotkin, G. D. (1980): Dijkstra's Predicate Transformers and Smyth's Power Domains. In: Bj!llrner, 
D. (ed.): Abstract Software Specifications. Lecture Notes in Computer Science, Vol. 86. Berlin-
Heidelberg-New York: Springer 1980, p. 527-553 
Pratt, T. W. (1969): A Hierarchical Graph Model of the Semantics of Programs. Proc. AFIPS Spring 
Joint Computer Conference 1969, p. 813-825 
Quine, W.V. (1960): Word and Object. Cambridge, Mass.: MIT Press, and New York: Wiley 1960 
Rabin, M. 0., Scott, D. (1959): Finite Automata and their Decision Problems. IBM J. Res. Develop. 
3, 114-125 (1959). Also in: Moore, E. F. (ed.): Sequential Machines: Selected Papers. Reading, 
Mass.: Addison-Wesley 1964, p. 63-91 
Randell, B. (ed.) (1973): The Origins of Digital Computers -
Selected Papers. Berlin-Heidelberg-
New York: Springer 1973, 3rd ed. 1982 
de Rham, G. (1947): Un Peu de Matbematiques a Propos d'une Courbe Plane. Elemente Math. 2, 
73 -76, 89-97 (1947) 
Rice, H. G. (1965): Recursion and Iteration. Commun. ACM 8, 114-115 (1965) 
Robinson, R. M. (1950): An Essentially Undecidable Axiom System. Proc. International Congress of 
Mathematicians, Cambridge, Mass., 1950, Vol. I. Providence, R. 1.: American Mathematical 
Society 1952, p. 729- 730 
de Roever, W. P. (1972): A Formalization of Various Parameter Mechanisms as Products of 
Relations within a Calculus of Recursive Program Schemes. Seminaires IRIA: Theorie des 
Algorithmes, des Langages et de Ia Programmation, 1972, p. 55-88 
Rutishauser, 
H. 
(1952): 
Automatische 
Rechenplanfertigung 
bei 
programmgesteuerten 
Rechenmaschinen. Mitteilungen aus dem Institut fiir angewandte Mathematik an der ETH 
Zurich, Nr. 3. Basel: Birkhauser 1952 
Rutishauser, H. (1954): Der Quotienten-Differenzen-Algorithmus. Z. angew. Math. Phys. 5, 
233-251 (1954) 
Rutishauser, H. (1967): Description of ALGOL 60. Berlin-Heidelberg-New York: Springer 1967 
Samelson, K., Bauer, F. L. (1959): Sequential Formula Translation. Commun. ACM 3, 76-83 
(1960). Translation of: Sequentielle Formeliibersetzung. Elektron. Rechenanlagen I, 176-182 
(1959) 

466 
Bibliography 
Schecher, H. (1956): MaBnahmen zur Vereinfachung von Rechenplanen bei elektronischen Rechen-
anlagen. Z. angew. Math. Mech. 36, 377-395 (1956) 
Schecher, H. (1970): Prinzipien beim strukturellen Aufbau kleiner elektronischer Rechenautomaten. 
Fakultat fiir Allgemeine Wissenschaften der TH Miinchen, Habilitationsschrift, 1970 
Schmidt, G. (1981): Programs as Partial Graphs I: Flow Equivalence and Correctness. Theoretical 
Computer Science 15, 1-25 (1981). Programs as Partial Graphs II: Recursion. Theoretical 
Computer Science 15, 159-179 (1981) 
Schnorr, C. P. (1980): Refined Analysis and Improvements on some Factoring Algorithms. Stanford 
University, Computer Science Department, Report STAN-CS-80-825, 1980 
Schonfinkel, M. (1924): Uber die Bausteine der mathematischen Logik. Math. Ann. 92, 305-316 
(1924) 
Scholl, P. C. (1976): Interpretation de Programmes comme le Traitement d' Arbres: Un Aspect de Ia 
Production des Programmes par Transformations Successives. Laboratoire IMAG Grenoble, 
Rapport de Recherche PR54, 1976 
Scott, D. (1970): Outline of a Mathematical Theory of Computation. Proc. 4th Annual Princeton 
Conference on Information Sciences and Systems 1970, p. 169-176. Also: Oxford University 
Computing Laboratory, Programming Research Group, Technical Monograph PRG-2, 1970 
Scott, D. (1976): Data Types as Lattices. SIAM J. Computing 5, 522-587 (1976) 
Scott, D. S. (1981): Lectures on a Mathematical Theory of Computation. Oxford University 
Computing Laboratory, Programming Research Group, Technical Monograph PRG-19, May 
1981. Also in: Bray, Schmidt 1982, p. 145-292 
Seegmiiller, G. (1966): Zum Begriff der Prozedur in algorithmischen Sprachen. Fakultat fiir 
Allgemeine Wissenschaften der TU Miinchen, Dissertation, 1966 
Seegmiiller, G. (1974): Einfiihrung in die Systemprogrammierung. Reihe Informatik, Bd. 11. 
Mannheim-Wien-Ziirich: Bibliographisches Institut 1974 
Seegmiiller, G. (1974a): Systems Programming as an Emerging Discipline. Proc. IFIP Congress 74, 
Stockholm. Amsterdam: North-Holland 1974, p. 419-426 
Shoenfield, J. R. (1967): Mathematical Logic. Reading, Mass.: Addison-Wesley 1967 
Simon, F. (1978): Zur Charakterisierung von LISP als ALGOL-ahnlicher Programmiersprache mit 
einem strikt nach dem Kellerprinzip arbeitenden Laufzeitsystem. Institut fiir Informatik und 
Praktische Mathematik der Universitat Kiel, Report Nr. 2/78, 1978 
Skolem, T. (1923): Begriindung der elementaren Arithmetik durch die rekurrierende Denkweise ohne 
Anwendung scheinbarer Veranderlichen mit unendlichem Ausdehnungsbereich. Skrifter utgit av 
Videnskapsselskapet i Kristiania, I. Matematisk-Naturvidenskabelig Klasse 1923, No. 6 
Steele, G. L. (1977): Macaroni is Better than Spaghetti. SIGPLAN Notices 12:8,60-66 (1977) 
Steele, G. L., Sussman, G. J. (1978): The Art of the Interpreter or, the Modularity Complex. 
Massachusetts Institute of Technology, Cambridge, Mass., AI Memo No. 453, 1978 
Stay, J. E. (1977): Denotational Semantics: The Scott-Strachey Approach to Programming Language 
Theory. Cambridge, Mass.: MIT Press 1977 
Stay, J. E. (1981): Semantic Models. In: Bray, Schmidt 1982, p. 293-325 
Strachey, C., Wadsworth, C. (1974): Continuations, a Mathematical Semantics for Handling Full 
Jumps. Oxford University Computing Laboratory, Programming Research Group, Technical 
Monograph PRG-11, 1974 
Strachey, C., Wilkes, M. V. (1961): Some Proposals for Improving the Efficiency of ALGOL 60. 
Commun. ACM 4, 488-491 (1961) 
Strong, H. R. (1970): Translating Recursion Equations into Flow Charts. Proc. 2nd Annual ACM 
Symposium on Theory of Computing, New York 1970, p. 184-197. Also J. Computer System 
Sci. 5, 254-285 (1971) 
Tennent, R. D. (1976): The Denotational Semantics of Programming Languages. Commun. ACM 
19, 437-453 (1976) 
Thue, A. (1914): Probleme tiber Veranderungen von Zeichenreihen nach gegebenen Regeln. Skrif-
ter utgit av Videnskapsselskapet i Kristiania, I. Matematisk-Naturvidenskabelig Klasse 1914, 
No. 10 
Turing, A. M. (1936): On Computable Numbers, with an Application to the Entscheidungsproblem. 
Proc. London Math. Soc., II. Ser. 42, 230-265 (1936), 43, 544-546 (1937) 
Turner, D. A. (1979): A New Implementation Technique for Applicative Languages. Software, 
Practice Experience 9, 31 -49 (1979) 

Bibliography 
467 
Turski, W. M. (1971): A Model for Data Structures and its Applications. Acta Informatica 1, 26-34, 
282- 289 (1971) 
Turski, W. M. (1978): Computer Programming Methodology. London: Heyden 1978 
Vuillemin, J. (1973): Correct and Optimal Implementations of Recursion in a Simple Programming 
Language. IRIA, Rapport de Recherche No. 24, 1973. Also J. Computer System Sci. 9, 332-354 
(1974) 
Vuillemin, J. (1975): Syntaxe, Semantique et Axiomatique d'un Langage de Programmation Simple. 
Interdisciplinary Systems Research, Vol. 12. Basel-Stuttgart: Birkhauser 1975 
Wadsworth, C. P. (1971): Semantics and Pragmatics of the Lambda-Calculus. Oxford University, 
Ph. D. Thesis, 1971 
van der Waerden, B. L. (1937): Moderne Algebra. Vol. I. 2nd ed. Berlin: Springer 1937 
Whitehead, A. N., Russell, B. (1910): Principia Mathematica, Vol. I. Cambridge: Cambridge 
University Press 1910 
Wiehle, H. R. (1973): Looking at Software as Hardware? International Summer School on Structur-
ed Programming and Programmed Structures, Marktoberdorf 1973 
van Wijngaarden, A. (1964): Recursive Definition of Syntax and Semantics. In: Steel, T. B. Jr.: 
Formal Language Description Languages for Computer Programming. Amsterdam: North-
Holland 1966, p. 13-24 
Wirsing, M., Broy, M. (1980): Abstract Data Types as Lattices of Finitely Generated Models. In: 
Dembinski, P. (ed.): Mathematical Foundations of Computer Science, Proceedings 1980. Lecture 
Notes in Computer Science, Vol. 88. Berlin-Heidelberg-New York: Springer 1980, p. 673-685 
Wirsing, M., Pepper, P., Partsch, H., Dosch, W., Broy, M. (1980): On Hierarchies of Abstract Data 
Types. Institut ftlr Informatik der TU Miinchen, TUM-I 8007, 1980 
Wirth, N. (1967): On Certain Basic Concepts of Programming Languages. Stanford University, 
Computer Science Department, Report STAN-CS-67-65, 1967 
Wirth, N. (1971): Program Development by Stepwise Refinement. Commun. ACM 14,221-227 (1971) 
Wirth, N. (1973): Systematic Programming: an Introduction. Englewood Cliffs, N.J.: Prentice-Hall 
1973 
Wirth, N. (1974): On the Design of Programming Languages. Proc. IFIP Congress 74, Stockholm. 
Amsterdam: North-Holland 1974, p. 386-393 
Wirth, N. (1976): Algorithms + Data Structures = Programs. Englewood Cliffs, N. J.: Prentice-
Hall1976 
Wirth, N., Hoare, C. A. R. (1966): A Contribution to the Development of ALGOL. Commun. ACM 
9, 413-432 (1966) 
Wossner, H. (1974): Rekursionsauf!Osung fiir gewisse Prozedurklassen. In: Seminar tiber Methodik 
des Programmierens. Abteilung Mathematik, Gruppe Informatik der TU Miinchen, Internal 
Report 1974, p. 69-81 
Wulf, W. A., Russell, D. B., Habermann, A. N. (1971): BLISS: A Language for Systems 
Programming. Commun. ACM 14, 780-790 (1971) 
Wulf, W. A., Johnson, R. K., Weinstock, C. P., Hobbs, S. 0. (1973): The Design of an Optimizing 
Compiler. Carnegie-Mellon University, Pittsburgh, Pa., Computer Science Department 1973 
Wulf, W. A., London, R. L., Shaw, M. (1976): An Introduction to the Construction and Verification 
of Alphard Programs. IEEE Trans. Software Eng. 2, 253-265 (1976) 
Wynn, P. (1956): On a Device for Computing the em(Sn) Transformation. Math. Tables and Other 
Aids to Comp. 10, 91-96 (1956) 
Zemanek, H. (1968): Abstrakte Objekte. Elektron. Rechenanlagen 10, 208-217 (1968) 
Zemanek, H. (1981): AI-Khorezmi - His Background, His Personality, His Work and His Influence. 
In: Ershov, A. P., Knuth, D. E. (eds.): Algorithms in Modern Mathematics and Computer 
Science. Lecture Notes in Computer Science, Vol. 122. Berlin-Heidelberg-New York: Springer 
1981,p.1-81 
Zilles, S. N. (1974): Algebraic Specification of Data Types. Massachusetts Institute of Technology, 
Cambridge, Mass., Laboratory for Computer Science, Progress Report XI, p. 52-58, und 
Computation Structures Group Memo 119, 1974 
Zuse, K. (1945): Der Plankalkiil. Manuscript 1945. Published by GMD Bonn, Report Nr. 63, 1972. 
For a short survey see: Bauer, F. L., Wossner, H.: The "Plankalkiil" of Konrad Zuse: A 
Forerunner ofToday's Programming Languages. Commun. ACM 15,678-685 (1972) 

468 
Bibliography 
References for the Programming Languages Mentioned in the Text 
ADA 
Preliminary Ada Reference Manual. SIGPLAN Notices 14: 6, Part A (1979) 
Ichbiah, J.D., Heliard, J. C., Roubine, 0., Barnes, J. G. P., Krieg-Briickner, B., Wichmann, B. 
A.: Rationale for the Design of the Ada Programming Language. SIGPLAN Notices 14: 6, Part 
B (1979) 
ALGOL 58 
Perlis, A., Samelson, K. (eds.): Preliminary Report -
International Algebraic Language. 
Commun. ACM 1: 12, 8-22 (1958) 
Perlis, A., Samelson, K. (eds.): Report on the Algorithmic Language Algol. Numerische Math. 1, 
41 -60 (1959) 
ALGOL 60 
Naur, P. (ed.): Report on the Algorithmic Language ALGOL 60. Commun. ACM 3, 299-314 
(1960). Also Numerische Math. 2, 106-136 (1960) 
Naur, P. (ed.): Revised Report on the Algorithmic Language ALGOL 60. Numerische Math. 4, 
420-453 (1962). Also Computer J. 5, 349-367 (1962) and Commun. ACM 6, 1-17 (1963) 
Woodger, M. (ed.): Supplement to the ALGOL 60 Report. Commun. ACM 6, 18-23 (1963) 
ALGOL68 
van Wijngaarden, A. (ed.), Mailloux, B. J., Peck, J. E. L., Koster, C. H. A.: Report on the 
Algorithmic Language ALGOL 68. Numerische Math. 14, 79-218 (1969) 
van Wijngaarden, A., et a!.: Revised Report on the Algorithmic Language ALGOL 68. Acta 
Informatica 5, 1-236 (1975). Also Berlin-Heidelberg-New York: Springer 1976 and SIGPLAN 
Notices 12: 5, 1 -70 (1977) 
ALGOLW 
see Wirth, Hoare 1966 
ALPHARD 
see Wulf et a!. 1976 
APL 
Iverson, K. E.: A Programming Language. New York: Wiley 1962 
BASIC 
Kemeny, J. G., Kurtz, T. E.: BASIC (User's Manual), 3rd ed. Hannover, N. H.: Dartmouth 
College Computation Center 1966 
Kemeny, J. G., Kurtz, T. E.: BASIC Programming. New York: Wiley 1967 
BCPL 
Richards, M.: BCPL -
a Tool for Compiler Writing and Systems Programming. Proc. AFIPS 
Spring Joint Computer Conference 1969, p. 557-566 
Richards, M., Whitby-Stevens, C.: BCPL -
the Language and its Compiler. Cambridge: 
Cambridge University Press 1979 
BLISS 
see Wulf et a!. 1971. Additionally: 
Wulf, W. A., et a!.: BLISS Reference Manual. Carnegie-Mellon University, Pittsburgh, Pa., 
Computer Science Department 1970 
CLU 
see Liskov et a!. 1977 
COBOL 
COBOL: Initial Specification for a Common Business Oriented Language. U.S. Department of 
Defense. Washington, D.C.: U.S. Government Printing Office 1960 
American National Standard COBOL. ANSI X3.23-1974. New York: American National 
Standards Institute 1974 

Bibliography 
469 
CPL 
Barron, D. W., Buxton, J. N., Hartley, D. F., Nixon, E., Strachey, C.: The Main Features of 
CPL. Computer J. 6, 134-143 (1963) 
EULER 
Wirth, N., Weber, H.: EULER: A Generalization of ALGOL, and its Formal Definition. 
Commun. ACM 9, 13-23, 89-99 (1966) 
FORTRAN 
Specifications for the IBM Mathematical FORmula TRANslating System, FORTRAN. New 
York: IBM Corporation 1954 
American National Standard FORTRAN. ANSI X3.9-1966. New York: American National 
Standards Institute 1966 (FORTRAN IV) 
American National Standard Programming Language FORTRAN. ANSI X3.9-1978. New York: 
American National Standards Institute 1978 (FORTRAN 77) 
GEDANKEN 
Reynolds, J. C.: GEDANKEN -
A Simple Typeless Language Based on the Principle of 
Completeness and the Reference Concept. Commun. ACM 13, 308-319 (1970) 
IPL 
Newell, A., Tonge, F.: An Introduction to Information Processing Language-V. Commun. ACM 
3, 205-211 (1960) 
Newell, A.: Documentation of IPL-V. Commun. ACM 6, 86-89 (1963) 
Newell, A., et al.: Information Processing Language-V Manual, 2nd ed. Englewood Cliffs, N.J.: 
Prentice-Hall 1964 
LISP 
see McCarthy 1960. Additionally: 
McCarthy, J., et al.: LISP 1.5 Programmer's Manual. Cambridge, Mass.: MIT Press 1962 
Berkeley, E. C., Bobrow, D. G. (eds.): The Programming Language LISP: Its Operation and 
Applications. Cambridge, Mass.: MIT Press 1964 
LUCID 
Ashcroft, E. A., Wadge, W. W.: Lucid - a Formal System for Writing and Proving Programs. 
University of Waterloo, Computer Science Department, Technical Report CS-75-01, 1975. Also 
SIAM J. Computing 5, 336-354 (1976) 
Ashcroft, E. A., Wadge, W. W.: Lucid, A Nonprocedural Language with Iteration. Commun. 
ACM 20, 519-526 (1977) 
MESA 
see Geschke et al. 1977. Additionally: 
Mitchell, J. G., Maybury, W., Sweet, R.: Mesa Language Manual, Version 5.0. Report CSL-79-3. 
Palo Alto, Cal.: Xerox 1979 
PASCAL 
Wirth, N.: The Programming Language Pascal. Acta Informatica 1, 35-63 (1971) 
Jensen, K., Wirth, N.: Pascal User Manual and Report, 2nd corrected reprint of the 2nd ed. 
Berlin-Heidelberg-New York: Springer 1978 
Plankalkiil 
see Zuse 1945 
PLII 
Radin, G., Rogoway, H. P.: NPL: Highlights of a New Programming Language. Commun. 
ACM 8, 9-17 (1965) 
American National Standard Programming Language PL/1. ANSI X3.53-1976. New York: 
American National Standards Institute 1976 
SETL 
Schwartz, J. T.: On Programming. An Interim Report on the SETL Project. Part I: Generalities. 
Part II: The SETL Language and Examples of its Use. New York University, Courant Institute of 
Mathematical Sciences, Computer Science Department 1975 

470 
Bibliography 
Kennedy, K., Schwartz, J. T.: An Introduction to the Set Theoretical Language SETL. Comput. 
Math. Appl. J, 97-119 (1975) 
Dewar, R. B. K., Grand, A., Liu, S., Schwartz, J. T.: Programming by Refinement as Exempli-
fied by the SETL Representation Sublanguage. TOPLAS I, 27-49 (1979) 
SIMULA I 
Dahl, 0.-J., Nygaard, K.: SIMULA -
an ALGOL-Based Simulation Language. Commun. 
ACM 9, 671-678 (1966) 
SIMULA 67 
Dahl, 0.-J., Myrhaug, B., Nygaard, K.: SIMULA 67 Common Base Language, revised ed. Nor-
wegian Computing Centre Oslo, Publication No. S-22, 1970 
SNOBOL 
Farber, D. J., Griswold, R. E., Polonsky, J.P.: SNOBOL, a String Manipulation Language. J. 
ACM 11, 21 - 30 (1964) 
Griswold, R. E., Poage, J. F., Polonsky, J.P.: The SNOBOL 4 Programming Language, 2nd ed. 
Englewood Cliffs, N.J.: Prentice-Ha111971 

Index 
(* indicates an entry in the glossary at the end of the book) 
,:. 
23 
A 23 
V' 23 
0 67 
0 (syn. empty) 20, 127, 131, 148, 341 
(l) 148, 341 
l 
177, 255 
'I 74 
I 
75 
Q 36, 37 
fJ 19, 35, 128, 170, 188 
L 19 
0 
19 
F 19 
T 19 
false 
19 
true 19 
atomic 122 
in terms of 189, 190 
some 207 
that 207 
isoftype 206 
based on 
210 
var 313 
newvar 415, 416 
val 324, 426 
lazy 175, 428 
pt 428 
newpt 428, 430 
deref 428 
nil 
427, 428, 438 
abort 345 
skip 328, 345 
exec 55 
goto 56, 371 
return 55, 367 
leave 
371 
swap to 370 
abacist 1 
abortive 85 
absolute value 20 
abstraction 31, 89-90, 96, 97, 152, 153, 195 
abstraction operation 14 
abstract machine 4, 9, 44, 254, 45 I 
abstract syntax 115 
abstract type 195-251, 197 
-, absolutely free 198, 201 
-,empty 202 
-,monomorphic 201, 202, 204, 208, 211, 
212, 235, 242 
-,polymorphic 202, 203,204, 209, 210, 221, 
235, 238 
-, related 252-253 
- , richer 253 
- , stronger 252 
- , strongly equivalent 252 
AC (accumulator) 121, 445 
access parameter 332 
access position (distinguished) 163, 411 
access property 157 
access (selector access) 
132 
-, direct 132, 221 
-, selector-sequential direct 132, 221 
- , strictly sequential 157, 163, 221 
accompanying count 279, 364 
accumulator 445 
Ackermann, Wilhelm (1896-1962) 47 
actualization taboo 104, 336- 339, 342, 412, 
420- 422, 430 
ADA 327 
addition 31, 48, 77, 238, 245, 248, 249 
-
for Peano numbers 238 
- of integers 245 
-, solvability of 242 
- , translation covariance of 265 
address 409, 444, 446 
- arithmetic 444 
-,end 444 
- for variables 445 
- , genuine 446 
-, jump 443, 445 
-, object 443 
- , relative 444 
-, start 444 
- , symbolic 444 

472 
addressing, 
219, 443 - 448 
- , indirect 444 
Adelson-Velskii, G. M. 
168 
aggregate 219- 221 , 424 
Aho, A. V. 
231, 299 
Aiken, Howard H. (1900-1973) 317, 331, 
443 
Aiken machine 443 
algebra* 
195, 201 
- , abstract 201 
- , finitely generated 199 
- , homologous 197 
- , initial 202, 208, 209, 220 
-,terminal 202, 208, 220 
-, trivial 201 
algebraic modification 81, 288 
algebraic structure 195 
algebraic transformation 78 
algebra of natural numbers 
31 
ALGOL 51, 94, 106, 132, 354, 451 
ALGOL 58 
102, 114, 312, 367 
ALGOL 60 4, 8, 102, 107, 112, 114, 115, 
312, 326, 334, 337, 353, 408, 413, 418, 419, 
449, 451 
ALGOL 68 
4, 17, 98, 107,112,114, 115, 
119, 120, 126, 131, 136, 139, 180-182,211, 
213, 312, 318, 338, 354, 408, 416, 418, 419, 
420, 423, 426, 432, 444, 449, 451 
ALGOL machine 51, 52, 96, 452 
ALGOL W 102, 114, 353, 449 
algorist 1 
algorithm 1-4, 7, 9, 10, 13, 24,76-81, 117, 
132, 165, 230-231 
- , backtracking 165 
-, division 250 
- , Euclid's 3 
-
Gaussian 5 
-,marking 6 
- , Markov 4, 7 
- , nondeterminate 165 
-, nondeterministic 5, 84, 85, 165 
- , nondeterministic recognition 165 
- of Hotelling 3 
-
of multiplication 2 
- , recognizing 144 
-,terminating 5, 84,119,132 
- , totally defined 84 
- , traversing 167, 442 
algorithmic character 76 
ALGORITHMIC LANGUAGE 455 
alias ban 338, 342, 347, 351, 410, 414, 419, 
422, 432 
al-Khorezmi, ibn Musa (780- 850) 
1 
alphabet 18, 191 
-, binary 259 
-, two-element 18 
Index 
ALPHARD 114, 266, 267, 354, 422 
alternative 22, 23, 50, 51, 68, 78, 29, 271, 
325, 445 
-, nested 23 
- , notation of 22 
-, (Petri net) 395 
Andrei's paradox 102 
annihilator 397 
antisymmetry 75, 78, 82 
APL 4, 108, 178, 183, 454 
apostroph 122 
applicability condition 83 
application 16, 31 
applicative 13, 31-32, 98, 174, 320, 452, 454 
approximation 37, 119 
arbitrary (choice) 85 
arborescence* (see also: tree) 142, 171 
-, bifurcating 142 
- , binary 142 
-, labelled 142 
- , leaved 142 
argument 16, 47, 50, 54, 72, 103, 313 
argument on termination 276-278, 299 
argument parameter 103, 332 
arithmetic 77 
-, fixed-point 263 
-, unlimited integer 263 
-
with oo 
249 
arithmetical-geometrical mean 322, 329 
arithmetic-logical unit 52 
arithmetic unit 263, 466 
arithmetization 294 
- of the flow of control 294- 296 
array 130-133, 177-179, 181, 214-221, 
255, 418 
- , abstract 213 
-, flexible 300, 424 
-
indexed by whole numbers 177 
- , induced operations 178 
- , infinite 130 
-, multi-stepped 
131 
- of variables 351 
-, one-side-flexible 214-216 
-, two-side-flexible 216-219 
array processor 179 
arrow 68 
-, double-lined !52 
-, insertion 368 
-,jump 368 
-, Zuse's 318 
Arsac, J. J. 
458 
assembler 445 
assertion 67, 114, 121, 188, 191 
assignment 313, 323, 342, 344, 419 
-,collective 314, 324, 329-331, 386,410 
-, concurrent 324 
-, multiple 324 

Index 
-, simultaneous 324 
assignment symbol 324 
associative dual 272 
associative 17, 20, 21, 43, 71, 77, 128, 
238-240, 241, 272-275, 276, 308 
associative memory 257 
atom 122, 141, 142 
automata theory 5 
automaton 
- , deterministic 230 
-, finite 4 
- , nondeterministic 165 
-, push down 4, 231 
-, recognition 30 
auxiliary constant 330 
auxiliary identifier 294, 303, 323 
auxiliary variable 331 
axiom 188 
- , assignment 344, 419 
- of branching 344 
- of composition 344 
-
of repetition 346 
- of the guarded statement 345 
axiomatic method 195 
axiom of choice 121 
axiom system for Boolean Algebra 192 
Babbage, Charles (1791-1871) 314, 317 
Babbage-Zuse machine 63, 287, 314, 331, 452 
backthread 443 
backtracking nondeterminism 85 
Backus, J. 
108, 409, 458 
de Bakker, J. W. 
41, 82, 347 
B-al-fraction 251 
base 249 
based (routine) 
17 
- , directly 17 
- , indirectly 17 
BASIC 9, 451 
basic set 233, 457 
basis 190 
Bauer, F. L. 
4, 53, 63, 141, 143, 182, 195, 
458 
Bayer, R. 
168 
BCPL 102, 183, 408, 449 
Belady, L. A. 
330 
Berkeley, E. C. 
458 
Berkling, K. J. 
52 
Bernays, Paul (1888 -1977) 74, 75 
Bernstein, A. J. 
386 
Bernstein condition 386 
binarization 259- 264 
Birkhoff, G. 
127 
bit sequence 27 
BLISS 63, 183, 365, 408 
block 323, 324, 327, 336 
-, pure 327 
blocking 405, 407 
block structure 120, 326 
BNF grammar (syn.: context-free grammar) 
68, 143 
BNF syntax 53 
Bobrow, D. G. 
141 
body (of a computational structure) 189 
body (of a procedure) 336 
body (of a routine) 14, 18, 31, 51 
473 
Bolzano, Bernard (1781-1848) 122, 312, 496 
Boolean algebra 192 
Boolean expression 68 
Boole, George (1815 -1864) 19 
Borel, E. 
121 
Bottenbruch, H. 
bottom 35 
bound 14 
- , computed 177 
box diagram 145 
branching 22, 31, 35, 67, 155, 325, 344 
-, binary 22, 67, 325 
-, (flow diagram) 382 
-. guarded 68-72, 75, 84 
- , overlapping guarded 69 
-, (Petri net) 
395, 409 
-, sequential 23, 69 
bridge 192 
Brinch Hansen, P. 407 
Broy, M. 
82, 85, 195, 200, 201, 253, 344, 
387, 407 
Bruns, Heinrich (1848 -1919) 317 
buffer 350, 351, 352, 410 
buffering 405 
buffer store 319, 410 
Burroughs 110 
Burstall, R. M. 
43, 45, 206, 253, 272, 323 
calculation form 
21, 24, 49 
call 16 
- , determinate 85 
-, graph of calls 30 
- of a procedure 332 
-of a routine 16, 17, 18, 47, 49, 65, 72,85 
- , pending 366 
- , regular 85 
-, simple 31, 59, 65, 365, 368 
- , terminating 85 
call by expression 353 
call by name 50, 109, 312, 353 
call by need 110 
call by reference 312, 353, 414 
call by value 50, 109, 312, 353 
call by value result 102, 353 
Cantor, Georg (1845 -1918) 494 
Cardano, Geronimo (1501-1576) 2 
cardinality 122, 127, 131, 135, 198, 233, 457 
cardinal sum 135 

474 
Carlson, B. C. 
329 
carrier (set) 195 
- , defined 196 
-, primitive 196, 203 
carry 250 
- constant 250 
cascade 117,142,163-165,229,231,439, 
442 
-, arbitrarily forked 144 
- , balanced 168 
-, hierarchical two-way 172 
cascade-type test 
271 
category 128, 201 
cell 446 
-, (binary-)word 446 
- block 447 
cellar 
141 
chain 27, 37-40, 41 
-, ascending 37 
- , finite decreasing 27 
change of computational structure 252-265, 
424 
change of object structure 10, 252-265 
change of type 252 
character 19, 122 
-, first 
19 
-,last 19 
character set 19, 122 
chess 5, 124, 192 
chiffre 118 
choice (operator) (q) 73-75, 82, 112, 121, 
136 
- , arbitrary 85 
-, free 395 
choose 67 
Church, A. 
3, 4, 7, 14, 15, 107, 120 
circuitry 10 
class of interpretations 270 
Clifford, A. H. 
243 
Clint, M. 
370 
CLU 114, 266, 354, 422 
coarser 201 
COBOL 182 
coding 331 
coercion operation 182 
collateral 32, 59, 87, 327, 386, 457 
collection 385, 386, 388 
-, (Petri net) 
397 
collection (of objects) 126 
Collins, G. E. 448 
collision 390 
column 24 
common subexpression 99 
common subobject 153 
common substructure 438 
communicating sequential processes 407 
commutative 71, 77, 80, 229, 238, 241 
comparison (lexicographic) 67, 227 
completeness 
203 
-, sufficient 203 
completeness of properties 202 
complexity theory 6 
component 121, 126, 132 
composition 344 
composition of chained functions 108 
compound 128, 131-133 
-, list-forming 427 
comprehension 123 
computable 4, 7, 18, 121 
computation 
-, partial 47-49 
Index 
computational structure 10, 19-20, 117, 120, 
185-267 
-,abstract 186, 195-251, 201 
-, change of 252-265, 424 
- , coarser 201 
-, concrete 189-191, 195-199, 206 
-, finer 201 
-, heterogeneous 195 
- , hierarchical 196 
- , homogeneous 195 
-, implementation of 258-265, 435-438 
-, initial 201, 207 
- of the truth values 23 
- , parameterized 191 
- , primitive 13, 190, 254 
-, sequence-type 221 - 235 
- , terminal 201, 207, 208 
-, universal 190, 196 
computational structure scheme 189 
computation rule 50, 175 
-, delay 109-112, 431 
- , full-substitution 50 
-,leftmost-innermost 50-51,72,82,109, 
110 
-,leftmost-outermost 50-51, 109, 110 
-,normal 50 
concatenation 20, 26, 155, 160, 225, 226, 438 
- of a right sequence with a left sequence 
160 
- of linked lists 437 
- of two left sequences 155 
concretization 204-205, 254-258 
-, operative 255 
-, partial 255 
condition 66, 72, 187, 190 
-, distribution of the 45 
-, exportation of independent conditions 77, 
81 
conditional critical region 407 
cone (volume of the truncated) 13 
conflict 395 
congruence relation 200, 205 
conjunction 19 

Index 
- , conditional 23 
-, sequential 23, 35, 124, 377 
conjunction compatibility 346 
consistency 201 
constant 14, 47, 49, 96, 119, 312 
-, auxiliary 330 
-, parametric 353 
-, relative 94 
constant declaration 353 
constructor 126, 130, 169, 175, 178, 187, 211, 
213' 242, 423 
-, enumerative 177 
container 410, 426 
-
for pointers 426 
container concept 323 
context-free grammar 40, 143, 230 
continuity 37, 81 
contraction 258 
control element 13 
control flow 
294-296, 308, 321 
-, analytical treatment 362 
- , reshaping the type of 308 - 309 
control instruction 54 
control structure 7, 360 
control unit 52, 446 
convolution 250 
convolution of functions 
107 
Conway, M. E. 
370, 408 
Cooper, D. C. 
272, 314 
copying 22, 45 
core 196 
corner brackets 94 
coroutine 370, 408 
correspondence 69, 82, 246 
-, left-total 82 
correspondence table 160 
countable set 
27 
count (accompanying) 279, 364 
counter 259, 376 
Courcelle, B. 
270 
Coxeter, H. S. M. 
208 
CPL 324 
Cramer's rule 5 
critical section 390 
cryptology 70 
Curry, H. B. 
106 
cycle 171 
-, free of 5 
Dahl, 0.-J. 370, 408 
Damm, W. 
107 
Darlington, J. 
45, 272 
data base 221 
data flow 21, 52 
data flow machine 52 
Davis, M. 
4, 121 
deadly embrace 405 
debit and credit calculation 246 
decimal classification 130 
decimal system 171, 249 
decision table 115, 272 
deck 
222 
declaration 16, 20, 322 
Dedekind, Richard (1831-1916) 237, 248 
Dedekind's relation 248 
Dekker, T. J. 
407 
delayed design decision 84 
delay rule 109 -112, 431 
delete (element) 158 
demand-driven 111 
demon 85, 165 
Denning, P. J. 458 
Dennis, J. B. 
52, 394, 396 
denotation 118 -119 
- , hidden 187 
- , operational 118 
-, operational standard 145 
-, predicative 118 
-, standard 118, 122, 127 
deparametrization 99, 355, 360, 361, 362 
deque 222 
descendant 82-85, 85-87, 100, 109, 270 
-, operational 86, 87 
description operator (1) 
75, 112 
475 
descriptive (syn.: predicative) 72, 73, 452, 454 
designation 14, 53, 120 
-, freely chosen 14, 16, 119 
- , local auxiliary 98 
-, result 98 
-, scope of 120 
determinacy 72, 75 
determinate 69-72,74,75,77, 79,82-87, 
270 
determination (operator) (1) 75-81 
determinism 5 
deterministic 74, 83, 86 
Dewey notation 130, 181 
diagonalisation method 4 
diagram (abbreviated) 146 
difference 20 
Dijkstra, E. W. 
17, 24, 27, 67, 85, 96, 112, 
117, 213, 216, 324, 334, 337, 339, 342, 343, 
346, 351, 370, 374, 385, 388, 400, 407, 418, 
419, 457, 458 
DIN (German Industrial Standards) 385, 398 
direct sum 68 
direct union 135 
discrimination 136, 137, 148-151, 186 
discriminator 137, 148, 151, 163 
-, boolean 151 
disentanglement (of the control) 302 - 309, 
321 
disentangling 357- 360 
disjunction 19 

476 
disjunction, conditional 23 
-, sequential 23, 35 
disjunction compatibility 34 7 
-,weak 346 
disk unit 352 
display 350, 352 
distance 54 
distributed processes 407 
distributivity 78 
divides predicate 20 
divisibility 224, 242, 247 
divisibility relation 247 
division 250 
domain 15, 32, 67, 80 
domino 10 
do-od construction 371 - 373 
double-ended queue 222 
doubling 20 
drum unit 352 
dual system 249 
Dyck, Walther Ritter von (1856-1934) 228 
dynamic programming 299 
dynamic waiting 390, 407 
Earley, J. 
145 
Eckert, J. P. 
446 
Eckert, W. J. 
331 
edge 30 
edge-punched cards 
77 
effectiveness 5, 8, 77 
efficiency 5, 9, 84, 91, 97, 108, 113, 261, 262 
effort 5 
Egli, H. 82 
Eickel, J. 4, 7, 231 
eight-queens problem 84 
element 
-, annihilating 247 
-, greatest 78 
-, insignificant 148, 341 
- , maximal 78 
-, minimal 
74 
-, neutral 43, 242-249, 274 
- , smallest 84 
-, special i 
177, 255 
elimination 22, 24, 77, 78, 153, 171, 176 
embedding 45, 90-93, 97, 99, 152, 160, 288 
-, complete 90, 91, 93 
-, incomplete 90, 91, 92 
encapsulation 186-188, 190 
encoding (binary) 260 
- , direct 260 
ENIAC 317 
enrichment 223, 242, 253 
-, operative 253 
enumeration 121, 122, 132, 138, 180 
epimorphism 200 
equality predicate 19, 23, 24, 35 
-, operative 237 
-,universal 121, 126, 195, 196 
equality relation 121, 245 
equivalence 90 
equivalence class 127, 200, 234 
equivalent (routines) 
15, 41, 82 
- , operationally 87 
-, (schemes) 270 
-, (statements) 343 
ERMETH 249 
Ershov, A. P. 49 
Index 
Euclid of Alexandria (365- 309 B. c.) 
3, 5 
Euclid's algorithm 3 
Eudoxus of Knidos (408- 355 B. c.) 
3 
EULER 4, 183 
Euler, Leonhard (1707 -1783) 
31 
even predicate 20 
exchange of disjoint branches 271 
exchange of two tests 271 
execution 50, 85, 302, 355 
- , abortive 85 
-, coordinated 355 
-, course of execution 4, 52 
-
in space 381 
- in time 381 
- , nonterminating 85 
-, parallel 385- 388 
- , regular 85 
-, sequential 355 
execution position 342, 384 
exhaust 77 
existence 11 
existential operator 
81 
expression (syn.: term) 
18, 50, 322 
-, Boolean 22 
-, conditional 22 
-, generalized 94, 98, 322, 326, 386 
-, graded expression system 330 
- with side effects 326- 327, 336 
factorial 24, 154 
Faddeev, D. K. 
3 
»false« 
18 
Faltin, F. 
250, 251 
Fano condition 67, 68, 227 
Fehr, E. 
107 
Feys, R. 
106 
Fibonacci number 25, 28, 291 
file 160-163 
-,empty 162 
- of variable 410 
-, sequential 163, 221 
finer 
201 
finitary 171, 173 
finitely generated 199 
finiteness 4, 8 
-
of objects 140, 169 

Index 
finiteness requirement 169 
fire (transition of a Petri net) 
394 
first-order predicate calculus 
201 
Fischer, M. J. 
107 
fixpoint 34, 47, 84, 169, 170 
-, weakest 37, 40-41, 45, 51 
fixpoint theorem 3 7, 51 
fixpoint theory 32-40, 81 
flip-flop 
380 
flow diagram 381-393, 382 
-, coordinated 388-392, 393, 398, 399, 404 
flow diagram level 453 
flow of control 294- 296, 308, 321 
Floyd, R. W. 
27, 84-85, 342 
folding 45-49, 72, 77, 78, 82, 89, 288, 293, 
356, 358 
fork diagram 143 
formal quotient 244 
format statement (FORTRAN) 354 
formatting task 352 
formula 13, 14, 118 
formula manipulation 107 
formulation (see: level of formulation) 
FORTRAN 4, 9, 102, 312, 353, 354, 408, 
456, 457 
frequency index 160, 209, 234 
Friedman, D.P. 112, 151, 169, 174, 175 
front part 67 
full-substitution rule 50 
FUNARO 106 
function 14, 112, 118, 123, 132 
-, ambiguous 69, 82, 85 
- , hi-stepped 106 
- , characteristic 234, 263 
-, forgetful 229 
-, frozen 
133, 178, 264 
-, injection 135 
-, inverse of 70, 73, 279, 284, 307 
-, multiple-valued 84 
- , nullary 
35 
- , partially recursive 4, 8 
-, primitive recursive 281 -282, 299 
-,projection 127, 128, 135 
-, recursive 7, 112 
-, selector 131 
- , storage mapping 446 
-, strict 35, 37, 39, 51 
-, test 135, 136 
-, total 35, 66 
functional 34, 81 
-, continuous 41, 45, 81 
functional embedding 288 - 294 
functional equation 32 
functional form 1 08 
functional formulation 31, 452 
functional iteration 37, 38 
functionality 15, 17, 105, 123, 188, 205 
functional notation 18 
functional programming 31, 107 -109 
function composition 167 
function inversion 155, 278-286, 300, 
304-308 
- , by introducing stacks 283 - 286 
function manipulation 107 
functor (forgetful) 229 
- of the associative law 229 
-
of the commutative law 130, 233 
- of the idempotent law 234 
fusion (of routines) 
378 
Galton, Sir Francis (1822 -1911) 130 
game (Petri net) 
396 
garbage collection 448 
Gaussian algorithm 5 
GEDANKEN 183 
generate 121, 188 
generation (see: principle of generation) 
generator 397 
generic symbol 125 
geometric locus 108 
477 
German Industrial Standards (DIN) 385, 398 
Geschke, C. M. 
355, 444 
Gill, S. 
365 
Gnatz, R. 
288 
Godel, Kurt (1906 -1978) 3, 201 
GOdelization 
3, 6 
Godel' s completeness theorem 201 
Goguen, J. A. 
195, 206, 233, 253, 266 
Goldstine, H. H. 
317,446 
Gordon, M. 
96 
graded expression system 330 
Graham, R. M. 409, 449 
grammar 230 
-, BNF 68, 143 
-, context-free 40, 143, 230 
-, LL(k) 
231 
-, LR(k) 
231 
- , right linear regular 143 
graph* 5 
- , bipartite 196 
- , bipartite directed 393 
- , hierarchical 383 
graph diagram 143, 145 
grapheme 118 
Gray code 295 
greater-or-equal predicate 19, 20 
greater predicate 19, 20 
greatest common divisor 24, 248, 451 - 454 
greatest element 78 
Green, J. 415 
Gries, D. 
101, 338, 347, 349, 419, 
458 
Griffiths, M. 
13, 101 
group 197 

478 
group as an abstract type 197, 207 
-, cyclic 208 
-, cyclic of order 2 198 
-, free 208 
- , freely generated 208 
- , generated by a set of generators 208 
- of rotations of a Euclidean plane 198 
- , one-element 198, 208 
- , ordered 243 
guard 67-72, 383 
guarded commands 371 
Guttag, J. V. 
185, 195, 196, 203, 204, 219, 
232, 266 
halving 20 
handle (link variable) 434 
Haskell, R. 
365 
Hasse 312 
heading 15, 66, 87, 114, 121, 187, 188, 189, 
196, 205 
-, extended 188 
heap storage 449 
Hehner, E. C. R. 
408 
Henderson, P. 
110, 17 4, 458 
von Henke, F. W. 
168, 182, 233 
Herbrand, Jaques (1908 -1931) 50 
Herbrand-Kleene machine 50, 452 
Hermes, H. 281, 299 
Heron of Alexandria (50 -100) 45, 90 
Heron's formula 18, 45, 90 
heterogeneous 195 
Hewitt, C. E. 
106, 270, 282, 287, 383 
hidden 187, 190, 205, 424 
hierarchical 17, 30, 94, 196, 383 
hierarchical subordination 18 
Hilbert, David (1862-1943) 3, 74, 75 
Hilbert curve 194 
history 220 
Hoare, C. A. R. 
85, 114, 140, 182, 195, 213, 
219, 324, 342, 343, 344, 370, 407, 419, 
421, 425, 426, 427, 432, 438, 457 
Hoffman, L. J. 142 
hole (in the scope) 96 
homogeneous 127, 130, 131, 132, 169, 195 
homomorphism 200 
homophone 70 
Hopcroft, J. E. 
231 
Horning, J. J. 
338 
horror procedurae 456 
Householder, A. S. 
3, 251 
Huffman, D. 
142 
Huffman code 142 
Huntington, E. V. 
192 
hyperpower function 48 
identifier 119 
-, auxiliary 294, 303, 323 
-, bound 342, 437 
- , freely chosen 128 
identity 196, 342, 438 
- , literal 198 
if-then-else construct (see: alternative) 8 
implementable 258 
implementation 320 
- by references 312 
- , determinate 84, 270 
- , deterministic 7 4, 83, 86 
-, linked-list 425-438, 449 
Index 
- of computational structures 258-265, 
435-438 
implicit (form) 72 
impoverish 223 
improvement (of algorithms) 438-443 
incarnation 24, 52, 173, 342, 382, 432 
incompleteness theorem of GOdel-Rosser 
201 
in-degree 171 
Indermark, K. 
107 
indeterminate 312 
index 128, 130 
-, computed 131 
indexing 130 
index set 131, 132, 214, 216 
- , non-finite 132 
indicant (mode indicant) 121 
-, explicit 122 
indivisible 224, 226 
induction 40-44, 81, 240, 273, 276, 289, 291 
- , algebraic 199 
-,computational 40-43,205 
- , data type 199 
-, start of 41, 43 
-, structural 43-44, 199, 205 
inductive 121 
inequality predicate 19 
infix notation 16, 109, 167, 226 
initial 201, 202, 207, 208, 209, 220, 232 
initialization (of variables) 313, 340-341 
- , pseudo-
341 
initialization rule 340 
injective 208 
input 23, 163, 352 
- device 410 
- of arguments 23 
input/output 163, 352 
input-output media 163, 352 
input parameter 102 
input value 49 
insert (element) 
159 
insertion, direct 22, 45, 371 
instantiation 47 
-, of a scheme 270 
- , partial 125 
instruction 445, 446 

Index 
-, one-address 445 
instruction counter 370 
integer 119 
interpretation 199, 287 
-, free 
287 
-, of a formula 14 
- , of a scheme 270 
- of a term 199, 203 
- , semantic 117 
-, surjective 199 
interval 123, 138 
intuition 10, 11 
invariant (property) 41, 162, 346, 348 
inverse 70, 73, 238, 242, 250, 279, 284, 307 
inverting 70 
IPL 142 
isomorphism 135, 139, 200 
isotonicity 346 
iteration 383 
iterative 31, 313, 368, 452 
- , formulation 452 
Janus 311 
joint refinement 117 
jump 56, 98, 367-371, 426, 457 
- address 443, 445 
-,arrow 368 
jump call 59 
junction 382 
-, (Petri net) 395, 397 
Kandzia, P. 370 
Kantorovic diagram 150 
Kantorovic, L. V. 
18, 146 
Kantorovic tree 18, 21, 29, 52, 56, 57, 63, 
166, 167 
Kennaway, J. R. K. 
85 
key word 122 
Kilburn, T. 
443 
Kleene, S.C. 7, 37, 50 
knitting (of sequences) 160 
Knuth, D. E. 
5, 130, 141, 142, 144, 147, 222, 
269, 351' 365, 369, 370, 433, 443, 449 
Kosaraju, S. R. 
407 
label 54, 367- 370, 445 
Lambda-calculus 14, 15, 107, 109, 113, 120 
Lambert, Johann Heinrich (1728 -1777) 5 
Lame, G. 
25 
Landin, P. J. 75, 94, 99, 107, 113, 176, 367 
Landis, E. M. 
168 
Langmaack, H. 
96, 107, 370 
language 198 
-,algorithmic 3, 7, 455 
- , Chomsky-1-
4 
-, Chomsky-2-
4 
-, formal 7, 228, 229 
-, programming 4, 9, 13, 454 
- , uni versa! 4 
-, wide spectrum 455 
language element 118 
last-in-first-out 53 
Laut, A. 
351 
law (syn.: property) 188 
law of the excluded miracle 346 
479 
lazy evaluation 111, 167, 170, 173 -177,425,428 
leaf 142 
least common multiple 248 
least upper bound 3 7 
Ledgard, H. F. 
107 
left-associative 127, 136 
left-cancellative 225 
leftmost-innermost rule 50- 51, 72, 82, 109, 
110 
leftmost-outermost rule 50-51, 109, 110 
left sequence 164, 165, 175 
- with back references 176 
left shift (indexing) 216 
- of variable contents 425 
Leibniz, Gottfried Wilhelm (1646 -1716) 
2, 117,312 
less-or-equal predicate 19, 20 
less predicate 19, 20, 24 
level counter 256- 258 
level (of formulation) 
13 
-, applicative 13, 31, 452, 454 
-, assembler 445 
-, descriptive 452, 454 
- , flow diagram 453 
-, functional 13, 31, 452 
-, iterative 452 
-, prealgorithmic 72-76 
-, procedural 114, 342, 413, 453, 454 
- , storage-addressed 453 
-, symbolically addressed 453 
lexicographic comparison 67, 227 
liber algorithmi 1 
lifetime 326, 342, 416, 428, 430, 438 
linear combination 288 
linearizing 166 
linear space 273 
link 426 
- cell 444 
- field 444 
- variable 433, 436, 444 
Lippe, W. M. 
107 
Liskov, B. H. 114, 195, 266, 311, 431 
LISP 4, 96, 106, 112, 141, 147, 448, 454 
LISP 1.5 
106 
LISP/N 107 
list 
-, first-in-first-out 
223 
- head 177 
-, last-in-first-out 157 

480 
list, linear 141, 171 
- , linear linked 435 
-, linear two-way 172 
- , linked 427 
-, one-way 427 
-, one-way linked 438-441 
-,push down 141, 157 
-,ring 171 
-,two-way 175, 177, 427 
-, two-way linked 441 
-, two-way ring 172, 177, 242 
List 141, 437 
List structure 144 
literal 118, 312 
Lonseth, A. T. 
3 
loop 373-378 
-, (n + 1/2) 374, 408 
-, revolving of 375 
-, unrolling of 375 
Lorenzen, P. 
40 
lub 37, 41 
LUCID 353 
Lukasiewicz, Jan (1878 -1956) 167 
machine 
-, abstract 4, 9, 44, 254, 451 
-,Aiken 443 
-, ALGOL 51, 52, 96, 452 
-, Babbage-Zuse 63, 287, 314, 331, 452 
- , binary-organized storage 456 
- , bit-addressed 446 
- , concrete 9 
- , data flow 
10, 52 
- , Gedanken 6 
-, Herbrand-Kleene 50, 452 
- , nondeterministic text substitution 85 
-, range-of-values 311, 317-320 
-, recursive 52 
-, reduction 52 
-,safe 51 
- , search 77, 452 
-,sequential 10, 331, 453 
-,stack 44, 52-65, 96, 106, 107,269,284, 
302,303, 311,313-317, 355,445 
-, stored-program 
10, 326, 443, 451 
-, stored-program binary 446 
- , text substitution 44, 49- 51, 96, 111 
- , Turing 4, 7 
-, universal 4, 287 
-, von Neumann 10, 174 
machine-oriented 9, 144 
MacLane, S. 
196 
Mag6, G. A. 
52 
Malcev, A. I. 243 
Manna, Z. 
34, 36, 39, 41, 44, 49, 50, 81, 85, 
112, 127, 296, 297, 312 
mapping• 
15, 32, 34, 35, 38, 106, 234, 312 
-, bijective• 
- , canonical 127, 135 
-, injective• 
-, inverse 6 
-, nested 106 
- , one-to-one 6 
-, order-preserving 130 
- , partial* 200 
- , surjective• 
mapping arrow 102 
mapping type 15, 105 
marking (Petri net) 
393 
- , alteration 394 
-,live 394 
-, safe 396 
- , terminating 394 
Markov, A. A. 
3 
Markov algorithm 4, 7 
Mauchly, John W. (1907-1980) 446 
maximal element 78 
maximizing operator 78 
-,weak 78 
Index 
McCarthy, J. 
7, 8, 23, 31, 40, 75, 82, 83, 85, 
107, 114, 115, 128, 135, 140, 141, 147, 163, 
207. 240, 241, 291, 296, 342, 343, 347. 409 
memo-function 299 
memory (secondary) 410 
MESA 113, 353, 354 
methodology of programming 457-458 
Michie, D. 299 
minimal element 74 
MIX 449 
mixed computation 49 
mode 15, 121 
- affinity 139 
-, based 140 
-,basic 130 
- , component 126 
- , composite 126, 211 
-,computed 125, 177, 178 
- expression 138 
-,index 130 
- , parameterized 177 
- , parametric 205 
- , uni versa! 131 
-,varying 133-137,138,212 
-, mode declaration 122, 126, 133, 134, 
140-145 
-, recursive 140-145 
mode equivalence 139 
mode indication 105, 126, 188 
model (of an abstract type) 197, 202 
-, existence of 204 
- , initial 232 
- , non-isomorphic 198 
-, terminal 210 
mode-specifical (storage) 438 

Index 
mode variant 133 -140 
modularization 185 
module 351 
monoid 209, 225, 227 
-, commutative cancellative 243 
-, free 225, 228, 229 
-, free commutative 228 
-, linearly ordered commutative 237 
monomorphic 201, 202, 204, 208, 211, 212, 
235, 242 
monomorphicity 204 
monotonic (function) 
36 
monotonic (functional) 
37 
monotonicity 37, 36, 37 
- , condition of 27, 28 
Morris, J. H. jr. 32, 110, 174, 277 
Morse, Samuel (1791-1872) 142 
Morse code 142 
Moser, W. 0. J. 
208 
multiplication 6, 24, 43, 48, 77, 92, 242, 245, 
247, 250 
-, complex 245 
-, Egyptian 3, 92, 98 
-
in positional systems 250 
- of integers 245 
multiset 232 
,u-operator 74, 84 
mutex 389 
mutual exclusion 389 
Myhill, J. 
121 
naming 219 
natural extension 35, 38 
negation 19, 124 
negation of the condition 271 
negative 244 
negativum 20 
Ness, S. 
34, 39, 44, 50, 297 
Neugebauer, 0. 3 
von Neumann, John (1903 -1957) 10, 247, 
312, 317, 355, 446 
neutral element 43, 242- 249, 274 
Newell, A. 
141, 142 
Newton, Sir Isaac (1643 -1727) 312 
nexus class 
171 
nexus (of objects) 170-173, 242 
-
of variables 
121, 409, 425 
Nivat, M.P. 270 
node (see: graph*) 5, 30 
- , terminal 6 
nondeterminacy 72 
nondeterminate 69-72,73, 82-85, 119, 
165, 246 
nondeterminism 
-,angelic 85 
- , backtracking 85 
- , demonic 
85 
- , totally erratic 85 
nondeterministic 5, 69, 74, 76-87, 85, 88, 
165, 270 
-
construction 76- 77 
non-local 14 
non-operational 77 
non-terminating 18, 40 
>no-object< 
22 
normal form 247, 248 
normal form system 202 
normal rule 50 
notation 
- , bracket-free postfix 52, 167 
- , bracket-free prefix 167 
-, decimal-digit 119 
-, Dewey 130, 181 
- , functional 17, 18 
-, infix 16, 109, 167, 226 
- , untyped 107 
notational variant 16 
null object 127 
number 118 
-, complex 119 
-, computable real 121 
-, cycle 241-243 
-,dual 260 
-, fixed-point 
263 
-, floating-point 251 
-, integral 14, 20, 122, 244-247 
481 
-,natural 19, 27, 31, 76,119, 122,125, 185, 
214, 241-243 
- , numerically real 14 
-, ordinal 131, 135 
-, Peano 235-241 
-,rational 17,119,247-249 
-, real 251 
- , special oo 
249 
- , stroke 236 
numeral 118 
object 53, 117-180 
-, cascade-type 163 -165 
-, complex 210 
-, composite 120, 126, 133, 409 
- , cyclic 169 
-,detailed 151-153,169-177,199 
-, elementary 120, 410 
-, empty 127 
-, finitely composed 130 
-,guarded 71 
-, homogeneous 127 
-, infinite 169-177 
-, insignificant 341, 359 
- , intermediate 152 
- , non-elementary 120 
-, null 127 
-, operational detailing of 151-153 

482 
object, permissible 67 
-, primitive 121, 203 
- , primitive computable 20 
-, pseudo 19, 35, 66, 82, 170, 188 
- , recursively defined 125 
-, scanning of 166-168 
-,simple 120, 169 
-,special~ 127,131,141 
- , structured 126 
-, traversal of 166-168 
-,universal 19, 127, 131,141 
-, untyped 127 
object address 443 
object declaration 97-101, 113, 285, 
320-325, 353, 428 
-, collective 98, 153, 320, 324, 329 
-, hierarchically ordered 99, 344 
- , sequentialization of 320- 325 
object designation 14, 53 
object diagram 145-151 
object set 117, 121, 185 
-, effectively countable 121 
-, primitive 13, 125, 140 
-,recursively enumerable 121, 125, 219 
- , universal 121 
object structure 11, 117-180, 195 
- , branched 166 
-, change of 10, 252-265 
-, discriminated 137 
- , hidden 190 
-, homogeneous 130, 132, 169 
-, linear 154-160 
-, linear recursive 166-168 
-, non-linear 167 
-,recursive 140-177, 182,212 
-, refinement of 117 
- , regular 169 
-, semiregular 169, 421 
-,terminating 169 
- with direct selector access 132 
- with selector-sequential direct access 132 
odd predicate 20 
Olderog, E.-R. 
96 
»one« 18, 19 
operand commutation 275-278 
operating system 389 
operation 195, 399 
- , binary switching 192 
- , blocking 400, 404, 406- 407 
- , indivisible 401, 407 
-, induced 108, 178 
-,nullary 119 
-, partially defined 155 
-, pending 62, 269, 273, 275, 278, 382 
- , prealgorithmic 234 
-, pmruttve 13, 35, 125, 155, 254 
-, releasing 400, 404, 407 
- , selection 22 
-, set, 
234 
-, universal 19, 68, 342 
operational 9, 81, 118, 119, 130, 145 
operationally equivalent 87, 270 
operation structure 10, 13 
operative 15 
operator 
-, choice (q) 73 -75, 82, 112, 121, 136 
-, description (1) 75, 112 
-, determination (1) 75-81 
- , existential 87 
-' JJ-
74, 84 
-, test (for mode) 
121 
order 78, 225 
-, flat 36 
-, induced 123 
-, infix 166, 443 
-, lexicographic 20, 44, 126, 166 
-, linear 19, 20, 124, 126, 134, 166 
-, linear Noetherian 214 
- , natural 260 
-,Noetherian 27, 43, 74, 121, 124 
-, Noetherian strict 43 
- , partial 36, 39, 82, 201 
-, postfiX 166, 443 
- , prefix 166, 442 
-, strict 43 
-, (well-ordering) 74, 124, 299 
ordinal numbers 19 
ordinal sum 135 
output 23 
output device 410 
overruling (scope of identifiers) 120, 438 
overwriting (parameter values) 
63 
overwriting (variables) 329 
Pacioli, Fra Luca (1445 -1514) 245 
packing 265 
palindrome 156 
parallel 385- 388 
parameter 13, 14, 189, 191, 312 
-, access 332 
-,argument 103, 332 
-, fixed 95, 315, 356 
-,global 96 
- , implicit 94 
-,local 96 
-, non-local 96 
- passing 332, 353 
-,result 101-104, 114, 311, 320, 332 
Index 
-,suppressed 13,94-97, 152, 170,315, 321 
-, suppressed variable parameters 336-339, 
355 
-, transient 104, 332 
parameter collection 314 
parameter domain 66 

Index 
parameterization 14, 186 
-, multi-stepped 106, 125 
parameter list 15, 32, 332, 410 
parameter register 63, 303, 355 
parameter stack 52, 63, 303, 355 
parameter value 56, 62 
parametric 3 53 
Parnas, D. L. 407 
parsing algorithm 230 
parsing problem 
-,weakened 231 
partial instantiation 47 
partially defined 67, 74, 128, 129, 155, 163 
partition 88 
partitioning 
-, balanced 100 
Partsch, H. 
294, 456 
PASCAL 4, 107, 112, 114, 115, 122, 125, 
132, 137, 180-182, 265, 267, 312, 326, 353, 
354, 425, 432, 435, 449, 457 
Paterson, M. S. 
270, 282, 287, 383 
path 193 
Paul, M. 
231 
Peano, Giuseppe (1858 -1932) 76, 237 
Peano axiom 76, 125, 237 
Pepper, P. 195, 288, 294, 323, 344, 456 
Perlis, A. J. 442 
permissible (object) 67 
Perron 312 
Peter, R. 
28 
Peterson, J. L. 393 
Petri, C. A. 
393 
Petri net 393-406 
- , boo! 393, 400- 405 
- , counting 405 
-, nat 393, 405-407 
-, one-token 395, 400 
-, safe 399 
- , simulation of 396 
n 13, 18, 119 
PL/1 113, 183, 449 
place (Petri net) 
393, 397 
- , initial 397 
-,input 393 
-, output 393 
- , termination 397 
-, trivial 397 
Plankalkiil 4, 75, 192, 324, 353, 456 
Plotkin, G. D. 
82, 343 
pointer 120, 121, 148, 176, 242, 426, 427, 
437' 438' 457 
-, lifetime of 428, 438 
-, lifetime of anonymous 429 
-, range of binding of 428, 438 
-, scope of 428 
-, universal (nil) 428, 438 
pointer declaration 428 
pointer representation 110 
pointer variable 427, 431 
Polish Notation 167 
polymorphic 202, 203, 204, 209, 210, 221, 
235, 238 
position 342, 384, 391 
positional system 249, 250 
positional value 249 
positive 244 
postcondition 114, 343, 348 
-, strongest 347 
power function 48 
- , cardia! 131 
power operation 69 
powerset 233 
pragmatical 17, 118, 185 
Pratt, T. W. 
383 
pre-algorithm 76- 81 
pre-algorithmic formulation 72 -76, 234 
precedence rule 115 
precomputation 277, 278, 305 
precondition 114, 343 
-, weakest 343, 347 
predecessor 19, 24, 26 
predicate 22, 66, 72, 188, 196 
-, admissible 41 
- , characteristic 73, 108, 234 
-, equality 19, 23, 24, 35 
-, greater 19, 20 
-, greater-or-equal 19,20 
- , inequality 19 
-, less 
19, 20 
-,less-or-equal 19, 20 
- , restricting 67 
predicate transformation 342, 343, 346 
predicate transformer 343- 347, 382 
predicative 118 
Preparata, F. P. 228 
Preston, G. B. 
243 
prime number 5, 6, 111 
primitive 
- carrier 196, 203 
483 
- computational structure 13, 17-20, 190, 
254 
- object 121, 203 
- object set 13, 125, 140 
- operation 13, 35, 125, 155, 254 
-
predicate 35 
-,routine 17, 18 
principle of generation 199, 202, 208, 228, 
229, 266 
principle of substitution 13, 18, 31, 45, 68, 
90, 94, 131, 312, 325 
probabilistic 85 
problem-oriented 9, 77 
procedural formulation 114, 413, 453, 454 
procedural level 114, 342, 413, 453, 454 

484 
procedure 331-341 
-, parametric 353 
-, pure 336 
procedure side-effect 331, 339 
process 
381 
- , communicating sequential 407 
-, concurrent 381 
- , parallel 381 
producer-consumer problem 388, 402, 407 
product 20 
-, associative cartesian 178 
- , cardinal 127 
- , cartesian 131 
-,direct 127,137,138,178 
- , ordinal 157 
-, smash 35, 39, 41, 51 
production system 
7 
program 3, 53, 56 
program development 7, 10, 66, 72- 81, 108, 
252- 265. 452-455 
program flow plan 385 
programming language (see language) 4, 9, 
13, 454 
-, universal 4 
programming system 
451 
program sheet 52 
program system 57 
program transformation (elementary) 45 
program variable (see variable) 
program verification 347-350 
proof by induction 40-44 
proof of termination 44 
proof strategy 40 
property 188, 196, 197, 252 
-, characteristic 188, 195 
-, completeness of 203 
-, contradictory 201 
property set 197 
- , inconsistent 206 
- , minimal 204 
protocol 5 
protocol stack 62, 63, 269, 287, 297, 302, 
317, 362 
pseudo object 19, 35, 66, 82, 170, 188 
pushdown 350, 351, 410 
pushdown store 410 
quantification (universal) 
188, 196 
quantifier 73, 77, 81, 112, 121 
quasi-ordering 224 
queue 223 
Quine, M. V. 
13, 353 
quotient 20, 87 
- , formal 244 
quotient structure 200 
Rabin, M. 0. 
radix notation 281 
radix system 18, 249, 250 
Randell, B. 
331, 446 
range 15, 32 
Index 
range of binding 14, 96, 99, 326, 342, 428, 
429, 438 
range of values 270 
range-of-values machine 311, 317-320 
range-of-values recursion 299, 317-320, 353, 
362, 376 
range-of-values tabulation 299- 302 
Raphael, B. 
141 
re-bracketing 148, 160, 272-275, 308, 360, 
433 
recognition automaton 30 
recognition routine 26 
record 181, 427 
-, pure 431 
-, variant-
181 
recurrence 317 
-, r-term 318, 319 
-, two-term 154, 318- 320 
recurrent 317-320 
recursion 13,24-40,45,52, 140-177, 182, 
212, 342, 383 
-,cascade-type 28, 165, 287, 305, 308, 359 
-, linear 29, 272-286, 358, 363 
- , multiple 28 
-, nested 28, 44, 294-299, 307, 360 
- , non-linear 62, 165, 287 - 302 
-, non-terminating 40 
-, range-of-values 299, 317-320, 353, 362, 
376 
-,tail 31 
recursion induction 291 
recursion removal 258, 315, 439 
recursive 11, 13, 24-40, 77 
reduct 258 
reduction machine 52 
reference 176, 426 
-, dangling 416, 432 
reference concept of ALGOL 68 
426 
referential transparency 13, 353 
refinement 
11 
register 410 
register transfer language 425 
regular 85 
regular expression 68 
remainder 20, 87 
renaming 120 
repetition 31, 51,57-65,155,269-309,313, 
314, 328, 346, 355, 360, 361, 377, 381 
-, counted 362, 376, 408 
-, nested 315 
-, non-rejecting 374 
-, rejecting 373- 374 
replace (element) 158 

Index 
representation 
- , cleared 250 
-,complement 247 
-, modulo 247 
restriction (of a guarded branching) 86 
restriction (of the parameter domain) 66-67 
result 24, 101, 106 
-, intermediate 63, 307 
-, multiple 87-89, 100, 126, 179 
result designation 103 
result parameter 101-104,114,311,320,332 
retention 106 
return jump 53, 367 
return point 54 
reverse 168 
reversing 156 
de Rham, G. 
288, 290 
Rice, H. G. 
282 
Riese, Adam (1492-1559) 2 
right-associative 23, 99 
right-cancellative 225 
right-commutative 231, 240, 276-277 
right sequence 164, 454 
ring 
211 
-, ordered commutative 20 
Robinson, R. M. 
237 
de Roever, W. P. 
35, 37 
roll 163 
- of variables 410 
Rosser, B. 
201 
routine 13-112, 14, 120 
-, ALGOL 60-type 107 
-
as parameter 104-106 
- as result 106-107 
-, associative binary 17 
- , Boolean 22, 77 
-, deparameterized 96, 170, 171, 173 -177, 
357 
-, detailed form of 303- 307 
-,determinate 69-72,74,77,79,82 
-, direct-recursive 30 
- , disentangled 303 
-, encapsulated 188 
-, equivalent 104 
-, iterative 
31 
- , linear recursive 51, 62, 269, 272-286, 
303 
-, multiple-stepped 178 
-, nondeterminate 69-72, 119 
-, non-terminating 112 
-, nullary 15, 18, 106, 109 
-, parameter less 
18 
- , partially defined 67 
-, primitive 17, 18 
-, recognition 22 
-, recursive 13, 24-40, 342 
- , regular 85 
485 
-, repetitive 31, 51, 57-65, 155, 269-309, 
313, 360 
- , structured 89 
- , subordination of 94- 97 
- , terminating 85 
- , totally defined 7 4, 77 
-,typed 107 
- with multiple results 87- 89, 179 
Russell, Bertrand Earl (1872-1970) 75 
Rutishauser, Heinz (1918 -1970) 3, 45, 102, 
120, 249, 311, 317, 318, 334, 352, 353, 375, 
408, 418, 432 
SAC-1 
448 
Samelson, Klaus (1918 -1980) 53, 63, 102, 
141, 147, 326 
scanning 15 8, 166 
Schecher, H. 
381, 444 
scheme 270 
- , anarchic 288 
scheme parameter 123, 270 
Schmidt, G. 
343 
Schnorr, C. P. 6 
Schonfinkel, M. 
106 
scope 14, 96, 99, 120, 326, 342, 370, 416, 
428, 438 
scoping 96 
-, dynamic 96, 106 
-, static 96, 106 
Scott, D. S. 
5, 32, 35, 41, 169, 170 
search 
- , linear 158 
search machine 77, 452 
second order predicate logic 81 
Seegmiiller, G. 
312, 449, 458 
segment 94, 98, 99, 322, 324, 327 
- , labelled 3 70 
segment brackets 94, 99 
selection 127-128 
-, collective 129 
-, composition of 128 
- , multi-stepped 128 
selection operation 22 
selection structure 131 -133 
selective alteration 351, 411, 418 
selective updating 410-411,418,438-443 
selector 127-128, 187, 211, 446 
- , composite 128, 166 
-, computable 130 
- , iterated 157 
-, multi-stepped 128 
- , single-stepped 128 
selector access (see: access) 
selector function 163 
self-application 107 
semantics 
-, axiomatic 342- 350 

486 
semantics, denotational 32, 342 
-,mathematical 32-40,82-85, 169 
- of nondeterministic constructions 76- 87 
-, operational 44-65, 85, 96, 170 
semaphore 406 
- , binary 400 
- , general 407 
-, private 407 
semicolon 100, 314, 321 
semigroup, free 20, 224, 228 
-, commutative 20, 243, 247 
- with neutral element 20, 243 
semiring, ordered commutative 19, 242 
sequence 5, 20, 24, 26, 27, 109, 117, 118, 
122, 141,225-227,280, 352,437,441 
-, empty 20, 127 
-, indexed 214 
-, left 154-160, 454 
- of primes 
111 
-,right 154-160 
-, sorted 5, 27, 159 
sequencing symbol 99, 100 
sequential circuit 378-381 
sequentialization 13 
-, complete 329-331, 368 
-, explicit 
321 
-, natural 13, 31, 50 
- of collective assignments 328- 330 
sequentialization symbol 321 
set 108, 123, 210, 231-235, 457 
-, countable* 
27 
- , finite 
122 
-, recursively enumerable 124 
set brackets 122 
set comprehension 138 
SETL 267 
set operation 234 
Shaw, J. C. 
141 
shift register 319, 410, 425 
Shoenfield, J. R. 
201 
side effect 326- 327, 331, 336, 339 
sieve algorithm 158 
sign-abs representation 24 7 
signal 400- 405 
- , private 404 
signalling section (automatic) 402 
signature 196, 205 
signature diagram 196 
signum 20 
Simon, F. 106, 107 
Simon, H. A. 
142 
simplification 50 
SIMULA 4, 117, 120, 190, 370, 408, 449 
single assignment variable 326 
Skolem, Thoralf (1887 -1963) 3, 226, 238 
smallest element 84 
smash product 35, 39, 41, 51 
SNOBOL 113, 183, 449 
solution (of a problem) 9, 10 
-, general 14 
- of equations 73 
some (operator) 73 
sort 15 
sorting 5 
-, binary 5, 100 
- by direct insertion 159 
-, linear 5, 101, 159 
specification 73, 76-81, 452-455, 456 
specifier 121 
splitting 385, 388, 397 
splitting a tautology 75, 78 
square 20 
square root 20 
Index 
stack 52, 157-160, 167, 197,221,266,283, 
350 
-
as an abstract type 197, 205 
- , bounded 191 
- of intermediate results 
63 
- of variables 410 
-, parameter 52, 63, 303, 355 
-, protocol 52, 62, 63, 269, 287, 297, 302, 
317, 362 
-, value 52, 62, 269, 302, 311, 317 
stack machine 44,52-65,96, 106, 107,269, 
284,302,303,311,313-317,355,445 
standard denotation 14 
standard designation 17 
star operation 228 
start address 44 
statement 327, 336 
-, alternative 328, 344, 388 
-, empty 328, 345 
-, guarded 328, 344, 345, 371 
state vector 343 
Steele, G. L. 
96 
Steinberg, S. 
13 
Steinitz 312 
step 4, 77 
stepwise refinement 185 
Stifel, Michael (1487 -1567) 2 
storage 409 
- allocation 416 
-, associative 219 
-, binary-word 446 
-
cell 444 
-demand 63 
-
diagram 417, 427 
-,heap 449 
-
implementation 409, 422 
- , linear 449 
-
location 444 
-, organized 409-419, 427, 454 
-
release 416 
- , structured 41 0 

Index 
storage-addressed formulation 453 
store 
-, buffer 410 
-, linear 410, 419 
-, peripheral 352 
-, pushdown 410 
-,tape 410 
stored-program machine 10, 326, 443, 446, 451 
storing device 317 
storing in scattered fashion 447 
Stoy, J. E. 32, 36, 343 
Strachey, Christopher (1916 -1975) 353- 370 
stream 176 
strict 35, 68 
string 20 
- , well-formed 
198 
string of symbols 198 
Strong, H. R. 
287 
stronger (function) 36, 47 
structured programming 89 
structuring 89- 101 
subexpression 89 
-,common 89,89-93,99, 153,320 
subjunction 23, 68, 124 
- , sequential 23 
submode 139, 141, 147, 245 
subobject 140, 171 
-,common 153 
subordination of routines 18, 94- 97 
subprocess 384 
- , concurrent 390 
subprogram 54 
subset 124, 138, 263 
- , enumerated 123 
-, finite 233 
subset relation (naive) 
139 
substitution 14, 16, 50, 152, 153, 171, 173, 
174 
-,text 44-51,174 
substructure 438 
subtract 11 
subtraction 80, 245, 248, 250 
-
for Peano numbers 
239 
- of integers 245 
- , translation invariance of 265 
successor 19, 129 
sufficient completeness 203, 204, 232 
sum 20 
-, cardinal 135 
-, ordinal 135 
suppression of parameters 13, 94, 321 
Sussmann, G. J. 
96 
swap 370 
switch 378 
switching theory 272 
symbolically addressed formulation 453 
synchronization in parallel 385 
synchronization section 385 
system of mode declarations 143 
-, indirectly recursive 143 
- , mutually recursive 143 
- , right linear 143 
system of routines 
-, hierarchically recursive 30 
- , indirectly recursive 30 
-, linear recursive 29 
-, mutually recursive 30, 368 
- , recursive 24 
-, repetitive 31, 57-65, 377 
systems programming 409, 448 - 449 
table 24, 154, 220, 316 
- , construction of 154 
tabulating 154 
tail recursion 31 
tape 350, 352, 410 
tape store 410 
tape unit 352 
Tardo, J. 
266 
temporary storage 52 
Tennent, R. D. 
32, 107 
term 49, 198 
term algebra 198, 200 
term creation 17 5 
terminal 201, 202, 205, 207, 208, 210, 220 
terminal symbol 230 
487 
termination 5, 24, 26, 34, 44, 47, 50, 56, 70, 
80, 84, 85, 93, 100, 111, 119, 132, 140, 169, 
294, 298, 389, 394, 
-, proof of 27-28, 70, 78 
termination requirement 132 
test 
-, cascade-type 271 
-, exchange of two tests 271 
test for equality 279, 410 
-, universal 410 
test function 
136 
test on nonzero 26 
test on zero 24, 26, 31 
test operator 165 
text substitution 44- 51, 174 
text substitution machine 44,49-51,96, 107, 
111 
textual replacement 22, 371 
Thornton, C. 442 
Thue, A. 
3, 198 
totally defined 35, 66, 70, 74, 77, 84 
Towers of Hanoi 295, 456 
transformation 23, 45, 78, 82, 90, 269-286, 
270, 387, 439, 445 
transition (Petri net) 
393 
-, conflicting 395 
-, trivial 397 
transition graph 30 

488 
transitive 82 
translation covariance 265 
translation invariance 265 
trapdoor 6 
traversal 166, 442 
traversing algorithm 167, 442 
tree 5, 308 
~, balanced binary 168 
~, binary 142 
~ , ternary 142 
~ , threaded 442 
~ with arbitrary forking 142 
»true<< 
19 
truth value 19, 22, 67, 118, 122 
tuple, 0-
127, 131, 218, 427 
~, typed 141 
Turing, Alan M. (1912 ~ 1954) 
3 
Turing machine 4, 7 
Turner, D. A. 
106, 108 
Turski, W. M. 
169, 409, 421, 458 
»tWO<< 
18, 19 
type 15 
type change 252 
typed 107 
type discriminator 137 
type-free 
15 
type indication 206 
type of interest 196 
type scheme 205 
Ullman, J. D. 
231 
undecidable 4 
»undefined« 
19, 35 ~ 40, 66 
unfolding 45 ~49, 72, 77, 78, 81, 288, 292, 
356 
uniformity of software and hardware 10 
union 
~, direct 138 
uniqueness 
11 
unit (of the stack machine) 
~ , arithmetic 263, 446 
~, arithmetic-logical 52 
~ , control 52 
universal 22, 107 
~ Boolean routine 23 
~ computational structure 190, 196 
~ equality 121, 126, 195, 196 
~ language 4 
~ machine 4, 287 
~ mode 131 
~ object 19, 127, 131, 141 
~ object set 121 
~ operation 19, 68, 342 
~ pointer 428, 438 
~ , predicate 19 
~ programming language 4 
~ relation 122 
~ test for equality 410 
unrolling a recursion 45, 49 
value (of a variable) 
313, 342 
~ , current 3 24 
value stack 62, 269, 302, 311, 317 
variable 14, 312 
~,bound 317 
~,free 317 
~ , syntactic 230 
Index 
variable (program variable) 13, 98, 100, 112, 
114, 120, 121, 163,311 ~319, 323~326, 338, 
342, 409' 449 
~,anonymous 415,432 
~, array of 351 
~ as parameter 331 ~ 335 
~, auxiliary 331 
~ , collection of 412 
~ , comparison of 421 
~, composition of 410, 418 
~, computed 413 
~, concept of 312 
~ , constant 326 
~, controlled 319 
~,fileof 410 
~ for structured objects 350 ~ 352 
~ , generated 417 
~, generation of 409 
~,input 334 
~, identity of 419 
~,level 424 
~, lifetime of 326, 342 
~, lifetime of anonymous 416, 430 
~, list of 313, 314 
~, parametric 353 
~ , pointer 427, 431 
~, range of binding of 326, 342 
~ , result 334 
~, roll of 410 
~, scope of 326, 342 
~ , shared 339 
~, shift 311 
~, stack of 410 
~, store 312 
~, subscripted 419 
~, typed 313 
~, von Neumann 312 
variable declaration 353 
~ , initialized 313 
~, non-initialized 335 
variableform 
413~414 
variant (mode) 133, 140 
~, alternative 137 
~, disjoint 
181 
Vienna definition 342 
von Neumann machine 10, 174 
Vuillemin, J. 
34, 39, 44, 50,110,144,174,297 

Index 
Wadsworth, C. P. 
110, 370, 431 
van der Waerden, B. L. 
312 
Waldinger, R. 
127 
weaker (function) 36, 47, 82, 87 
Weierstrass, Karl (1815 -1897) 312 
Whitehead, Alfred North (1861-1947) 75 
widening 127, 135 
-, implicit 139, 147 
Wiehle, H. R. 
10 
van Wijngaarden, A. 
369 
Wilkes, M. V. 
353 
Wirsing, M. 
200, 201 
Wirth, N. 
67, 102, 107, 114, 124, 126, 134, 
160, 181, 185, 193, 219, 353, 419, 421, 531, 
432, 435, 436, 449, 456, 457 
Wirth's connection 431, 445 
Wise, D. S. 
112,151, 169, 174, 175 
Wossner, H. 
291 
word 19, 198, 227 
- , binary 260 
-, empty 127 
word algebra 198, 228 
Routines 
1·1 
.+. 
.I\. 
abs 20, 22, 31, 67, 69 
ack 44, 51, 110, 119 
ackh 48-49 
add 11, 80, 238, 245 
addh 48 
addi 108 
advance 162 
alt 216 
alter 351 
and 19, 124 
append 20, 158, 162 
appendc 187 
arbitbool 71 
arbitrary 7 4 
arrayexch 
421 
Beta 108, 178 
birthday 129 
bis 281 
ble 39, 50, 51, 56, 110 
bmult 92-93, 98 
bottom. bottom 20, 157 
build 443 
buildtree 168 
.&. 
cascforget 229 
cascparse 231 
change 443 
c/ 230 
close 162 
comp 68 
cone 20, 26, 225, 227, 281 
word problem 230 
word symbol 118 
Wulf, W. A. 
63, 114, 191, 219, 266, 365 
Wynn, P. 3 
Yeh, R. T. 
228 
>yes-object< 22 
yield instruction 318 
yield symbol 324 
Zagler, L. 6 
Zahlkeller 63 
Zemanek, H. 
117, 145 
»zero« 
18, 19 
zero 244, 251 
- , double 246 
-, leading 251 
Zilles, S. N. 
195, 266 
Zuse, K. 
4, 75, 192, 318, 324, 351, 353, 
354, 456 
Zuse's arrow (yield symbol) 318, 324 
c(Jnc 439 
cone• 437 
concatl 160 
concatr 161 
cons 164 
consumer 389 
cont 436 
489 
contains 158,165,209,275,435,436 
contains • 436 
div. 
conv 281, 433 
conv • 443-434 
convert 173,174,425,427,433 
convert • 428-429, 433-434, 440 
cos 279 
count 160, 333 
count three times 333 
count to three 333 
delete 159, 222 
deleteall 158, 159 
diff 242 
distributor 389 
div 20, 88 
divisible 380- 381 
divtwo 36, 47 
dm 88 
. x2 2x. dupl 20, 51, 72, 92, 109 
dupl' 
72 
eq 19 
equ 155 
(equal) 
237 

490 
even. 
·~· 
.>. 
even 20, 26 
1·1 
exchO 338, 414 
exchl 338 
exchll 421 
. ~. 
exch2 414 
lp. 
extract 162 
. <. 
f 
16, 21, 89 
fa 107 
fac 24, 42, 45, 56-58, 154, 274 
276, 280, 315, 319, 335, 357, 360, 
371' 381-382 
jac• 335, 357 
fact 42, 274 
fib 28, 52, 53, 55, 291, 294, 302 
·I· 
first 351 
./2 
frequ 160 
fuse 288, 301, 318 
g 92, 96 
gcd 24, 25, 29, 46, 59-65, 69, 74, 
75, 76, 79-80, 95, 315, 316, 
339, 340, 341, 452-454 
gcd' 
79 
. mod. 
(gcd, mod) 24, 29, 31, 47, 58, 
62-63,95, 315, 339, 340,452 
ge 19, 20 
gray 295 
grigri 101, 104, 133, 180 
. X. 
gt 19, 20 
h 
92, 120 
he 90, 95, 98 
head 530 
heron 17, 18, 22, 89, 90, 94, 95, 
98 
hyp 47 
indf 119 
initial 416 
insort 100, 101 
insortl 258 
insp 160 
inv 282, 413 
invl 413 
inv2 413 
inversabs 70 
inversabs' 70, 71 
isO 236 
it 283 
jou* 102 
k 34 
last 351 
leone 155, 165 
le 
19, 20, 227 
-, 
0 
odd. 
.v. 
length 
20, 156, 275 
lengthc 187 
less 83, 242 
(less-or-equal) 
236, 238 
!part 20, 224, 225, 227 
It 19, 20, 25, 29 
m 89, 108, 348 
m* 355 
makelsequl 161 
makelsequ2a 161 
makelsequ2b 161 
makelsequ3 161 
makersequl 161 
meas 20 
med 20, 36, 47 
meet 108 
merge 28, 29, 69, 370, 372 
merger 370 
million! 119 
minus 20 
mnv 34 
Index 
mod 24, 25, 29, 43, 47, 66, 78, 88, 
95, 315, 339 
morris 32, 38, 303, 357- 358 
morris' 
357 
morris* 357-358 
mutt 20, 31, 105, 245 
multh 48 
multsieve 110 
natdiv 88, 100, 179 
natinterval 110 
nats 111 
ndcascparse 231 
ne 18 
neg 26,155,364-365 
neg • 365- 366, 377- 378 
new 435 
next 295 
norm 246 
not 19, 124 
odd 20, 26 
(odd, even) 29, 31 
Omikron 108 
open 162 
or 19 
ord 89 
ord* 102, 332 
p 
91, 95, 97 
pall 156, 383 
pa/2 156, 3 84 
parse 89, 100 
part 88, 168 
pbw 303, 356 

Index 
491 
pbw* 356 
se/ 423 
plus 246 
self 107 
pn 379-380 
selhib 215 
pop 351 
sieve 158 
p{Jp 423 
sort 159, 275 
pos 26, 155, 364-365 
soso 339 
pos* 365-366, 377-378 
split 235 
positive 364- 366, 368, 377- 378, 
sq 20, 71' 178 
379-380 
v. 
sqrt 20 
{pos, neg) 29, 31, 155, 364, 365 
squdiff 16, 17' 21 
pow 70, 71' 83, 86, 105, 281 
stock 20, 156, 221' 439 
pow' 
70, 106 
st6ck 440 
powh 48 
sub 20, 25, 29, 66, 80-81' 238, 
pred. 
pred 19, 66, 74, 76, 236, 238 
245 
primesieve 111 
succ. 
succ 19, 236 
print 316, 352 
sum 240 
producer 389 
push 351 
tab 154, 300 
push 423 
tabf 417 
put 256 
tabjac 154 
tabg 416 
rconc 155, 164 
tabrec 154 
read 352 
tabsort 159 
readlsequ 161 
tabsort 1 159 
recede 162 
tabulate 316 
replace 158, 165 
tdist 177 
rest. 
rest 20, 158, 222, 255 
top. 
top 20, 158, 255 
res tO 
380 
trailer 74 
restl 380 
trans 176 
rest2 
380 
trans* 429, 441 
reverse 156 
transfer 147' 148 
ring 430 
transit 176 
ring• 431 
transit* 429, 441 
rot 338 
traverselist 167 
rotl 337 
traversetree 167, 309, 442 
rot2 337 
trunc 351, 439 
rotate 177 
trunc 438 
round 16 
rp. 
rpart 20, 224, 225, 227 
ult 83 
rsparse 230 
ultg 84 
{union) 235 
s 17, 18, 95, 336 
unit 74 
{sO, sl) 27 
upper. 
upper 20, 157 
sl 337 
s2 337 
widen 
245 
scan 158 
search 159 
zer 38 
Basic Modes 
bit 19, 123, 192, 259 
IN 
nat 19-20, 122, 125, 140 
IB2 
boo I 19, 122, 124, 190, 259 
ip 
rat 
17, 126, 129, 139, 140 
Boolean 180 
rR 
real 
14, 251 
r 
char 19, 122, 125 
IR 
18, 121, 251 
empty 131' 141 
"" 
sequ " 20, 225 
71. 
int 14, 20, 122, 139, 245 
·f* 
string 125 

492 
Modes 
account 130, 181 
address 
182 
adr 444 
arc 123 
atom 141 
bs x 454 
c 'X 
429, 441 
cart 138 
case 'X 
141, 144,165, 166-168, 172,441 
cased 'X 
151 
century 123, 124, 180 
code 295 
codeword 295 
comp 211 
complex 129, 180, 183 
consolation prize 123, 180 
cs 'X 
141 
date 126, 129, 180, 265 
dm 139 
dollar 139 
(expression, term) 143, 230 
file x 162 
flag 400 
fstack 'X 
133 
hometown 182 
I 'X 
437 
lisp x 140,141,143,163-165,166-168 
lispd x 149 
list 'X 
144, 437 
list1 
145 
llst2 145 
Is x 141 
lsequ 'X 
141, 154-160, 171 
lsrsequ 'X 
144 
mark 362 
number 133, 181 
oddnumber 123 
ordinal 123 
Computational Structures 
AGREX 255, 424 
BAG 257 
BIT 192 
BS 191 
DUALS 261 
FLEX 256 
GBS 259 
GFINSET 264 
GFLEX 256 
GSTACK 257 
HILBERTCURVE 194 
output 130, 181 
p 179 
pascalflle 449 
pers 126 
person 129, 134, 136, 137, 180 
pile 126 
plex 'X 
144 
pnat 123, 288 
point 133, 137, 181 
point1 
137 
polar 138 
pt 428 
q 130, 181 
rcomp 212 
result 180 
roll 'X 
163 
rs x 140, 143 
rsd 'X 
148 
rsequ 'X 
141, 143, 154-160, 182,230 
rsequc 'X 
151, 186 
rsequd 'X 
150 
s 'X 
428 
sema 406 
sex 123, 180 
sign 364 
sint 246 
stack x 435 
status 123 
street 182 
stri 140 
stroke 235 
suit 123, 180 
table "x 133 
termd 'X 
149 
travc 'X 
442 
trio 123, 124 
v 212 
w 160 
!NT 249 
tN 
235 
NAT 260,262 
NGREX 257 
NUMBER 
251 
PLOTTER 193 
RSC 189, 206 
STACK 207, 254, 283, 435 
SUIT 191 
7L 
245 
Z2 
254 
Index 

Index 
Abstract Types 
BAG 
232 
BIFLEX 
217 
BS 
258 
CODEL 
222 
COMP 
211 
COST 
209 
CPL 
210 
DECK 
222, 224 
EPEA 
260 
FINSET 
233 
FLEX 
214 
G 207 
GENGROUP 
208 
GREX 219 
INDEX 
214 
INDEXS 
217 
LISP 
207 
LSTACK 
206 
PEA 
237 
QUEUE 
223 
RCOMP 
213 
sc 
222 
SEQU 
225 
STACK 
205, 207, 213 
v 212 
WORD 
227 
Z2 
208 
493 

Glossary 
set 
"A set is a collection into a whole of definite distinct objects of our intuition or of our 
thought. The objects are called elements (members) of the set." (Cantor 1895) 
(binary) relation (in M) 
a set of ordered pairs of elements of a set M 
in general an n-ary relation between sets M;. i = 1, ... , n, is defined as a set of ordered 
n-tuples (m1, ••• , mn) where m; eM;. A relation is said to be homogeneous, if all theM; 
are equal, otherwise it is heterogeneous. 
equivalence relation (in M) 
a reflexive, transitive, symmetric relation in a set M 
equivalence class 
a non-empty subset of elements of a set with an equivalence relation, which are pairwise 
equivalent to each other and not equivalent to any element not belonging to the subset 
* 
order (in M) 
a reflexive, transitive, antisymmetric relation in a set M 
strict order (in M) 
an irreflexive, transitive (thus asymmetric) relation in a set M 
ordered set (M, ~) 
a set M with an order ~ 
minimal element (of an ordered set (M, ~)) 
an element (of the set M) which is not preceded (w.r.t. ~)by any other element of M 
least element (of an ordered set (M, ~)) 
an element of the set M which precedes (w.r. t. ~) any other element of M 
(In an ordered set each subset has at most one least element.) 
linear order, total order 
an order in which any two elements are related 
(In a linearly ordered set each minimal element is also a least element.) 
chain 
a linearly ordered set 
Noetherian order 
an order in which every non-empty subset has a minimal element (descending chain 
condition) 

Glossary 
well-founded set 
a set with a Noetherian ordering 
well-ordering 
a linear Noetherian order 
(Every non-empty subset has exactly one minimal element, viz. a least element.) 
initial section 
495 
a subset of a well-ordered set (M, ~) which with an element also contains all preceding 
ones 
Noetherian induction 
For a Noetherian strictly ordered set (M, <) the following inference rule holds: 
v a e M:(v be M, b < a : P(b) => P(a)) implies 
v a eM: P(a) 
* 
correspondence (between A and B) 
a set of ordered pairs of elements from A and B (relation) 
correspondence which is unique on B 
a correspondence between A and B where no element of A is related to different 
elements of B (right-unique relation) 
correspondence which is unique on A 
a correspondence between A and B where no element of B is related to different 
elements of A (left-unique relation) 
one-to-one correspondence 
a correspondence between A and B which is unique on A and B (one-to-one relation) 
right-total correspondence (total with respect to B) 
a correspondence between A and B where each element of B occurs in a pair (right-total 
relation) 
left-total correspondence (total with respect to A) 
a correspondence between A and B where each element of A occurs in a pair (left-total 
relation) 
partial mapping (from A to B) 
a correspondence between A and B which is unique on B (not necessarily total) 
(total) mapping (from A to B) 
a correspondence between A and B which is unique on B and total with respect to A 
(left-total, right-unique relation) 
injective mapping 
a mapping from A to B which is also unique on A (one-to-one relation) 
surjective mapping, mapping from A "onto" B 
a mapping from A to B which is total with respect to B (right-total, right-unique 
relation) 
bijective mapping 
an injective and surjective total mapping (one-to-one, left- and right-total relation) 
function 
synonymous: partial mapping, mainly in calculus 
operation 
synonymous: partial mapping, mainly in algebra 

496 
term 
a "well-formed" string built from operation and operand symbols 
equipotent 
related by a bijective mapping 
order-isomorphic 
* 
Glossary 
related by a bijective mapping f such that f and its inverse are order-preserving 
cardinal number (of M) 
the equivalence class of all sets that are equipotent to M 
ordinal number 
the equivalence class of order-isomorphic well-ordered sets 
cardinality 
a mapping that relates an arbitrary set with its cardinal number 
countable set 
a set which is equipotent to the natural numbers 
finite set 
a set which is not equipotent to any of its proper subsets (Bolzano) 
character set 
a finite set, the elements of which are called characters 
string 
sequence of characters from a character set 
alphabet 
linearly ordered character set 
* 
directed graph (M, R) 
a set M (nodes) together with a relation R (arrows) in M 
(undirected) graph (M, S) 
a set M (nodes) together with an irreflexive, symmetric relationS (edges) in M 
tree 
a connected (undirected) graph which is free of cycles 
arborescence 
a connected directed graph which is free of cycles, each node of which has at most one 
incoming arrow except for the root which has none 
bipartite graph 
a (directed or undirected) graph the node set of which is partitioned into two kinds of 
nodes, such that always only a node of one kind is related to a node of the other kind 
* 
relational structure 
a family of sets, called carrier sets, together with a finite set of relations between these 
carrier sets 
algebraic structure, algebra 
a relational structure where all relations are operations; two algebraic structures the 
operations of which are pairwise of the same functionality are called homologous 

Glossary 
497 
homomorphism 
mapping rp of an algebraic structure A into a homologous structure B, which is 
"compatible" with all pairs of operations fA and j 8 of the respective structure, i.e. 
isomorphism 
homomorphism rp from A to B together with a homomorphism If/ from B to A such 
that If/ followed by rp as well as rp followed by If/ are the identical mapping. 
* 

Springer -Verlag 
Berlin 
Heidelberg 
New York 
D. Gries 
The Science of 
Programming 
1981. XIII, 366 pages 
(Texts and Monographs in Computer 
Science) 
ISBN 3-540-90641-X 
Contents: Why Use Logic? Why Prove 
Programs Correct? - Propositions and Predi-
cates. - The Semantics of a Small Language. 
-The Development of Programs.- Appen-
dices 1-4.- Answers to Exercises.- Refe-
rences. - Index. 
This is the first text to discuss the theory 
and principles of computer programming on 
the basis of the idea that a proof of correct-
ness and a program should be developed 
hand in hand. It is built around the method 
first proposed by Edsger W. Dij kstra in his 
monograph The discipline of Programming 
(1976), involving a "calculus for the deriva-
tion of programs." Directing his materials to 
the computer programmer with at least one 
year of experience, Gries presents explicit 
principles behind program development, 
and then leads the reader through programs 
using those principles. Propositions and 
predicate calculus are presented as a tool for 
the programmer, rather than simply an 
object of study. The reader should come 
away with a fresh outlook on programming 
theory and practice, and the assurance to 
develop correct programs effectively. 

Texts and 
Monographs 
in Computer 
Science 
Editor: D. Gries 
Springer -Verlag 
Berlin 
Heidelberg 
New York 
S. Alagie, M.A. Arbib 
The Design of Well-Structured 
and Correct Programs 
1978. 68 figures. X, 292 pages 
ISBN 3-540-90299-6 
M.A.Arbib, A.J.Kfoury, R.N.Moll 
A Basis for 
Theoretical Computer Science 
1981. 49 figures. VIII, 220 pages 
ISBN 3-540-90573-1 
Chess Skill in Man and Machine 
Editor: P.W.Frey 
Corrected printing 1978. 55 figures, 2 tables. 
XI, 225 pages 
ISBN 3-540-{)7957-2 
E.W.Dijkstra 
Selected Writings on Computing: 
A Personal Perspective 
1982. Approx. 350 pages 
ISBN 3-540-90652-5 
H. W. Gschwind, E.J. McCluskey 
Design of Digital Computers 
An Introduction 
2nd edition. 1975. 364 figures. IX, 548 pages 
ISBN 3-540-{)6915-1 
Programming Methodology 
A Collection of Articles by Members of IFIP WG 2.3 
Editor: D. Gries 
1978. 64 figures, 5 tables. VII, 437 pages 
ISBN 3-540-90329-1 

Bauer 1 Wossner 
Algorithmic Language and Program Development 
This book provides a new, systematic and unified approach 
to the essential ideas of computer programming. While the 
traditional method is to describe various features of individ­
ual programming languages, this book emphasizes instead 
fundamental concepts common to all of them and the inter­
relations between these notions, thus enhancing the 
reader's insight into algorithms and their description. 
The systematic development of the basic concepts leads 
moreover to methods for developing programs- from the 
specification of a problem to its implementation on a machine, 
and, in particular, from functional to procedural programs. 
The book includes recent theoretical results from areas 
such as mathematical and operational semantics, fixed 
point theory, nondeterminism, and abstract data types, 
together with their application to program transformation 
and program correctness. In this way, it provides an intro­
duction to the current topics of research, and prepares the 
student and the professional for the technical literature. 
ISBN 3-540-111 4 
ISBN 0-387-111 4 

