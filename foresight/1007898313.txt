Dissertation
zur Erlangung des akademischen Grades
Doktor der Naturwissenschaften (Dr. rer. nat.),
eingereicht bei der
Fakult¨at Wirtschaftsinformatik und Angewandte Informatik
der Otto-Friedrich-Universit¨at Bamberg
A Combined Analytical and Search-Based
Approach to the Inductive Synthesis of
Functional Programs
Emanuel Kitzelmann
12. Mai 2010
Promotionskommission:
Prof. Dr. Ute Schmid (1. Gutachter)
Prof. Michael Mendler, PhD (Vorsitzender)
Prof. Dr. Christoph Schlieder
Externer 2. Gutachter:
Prof. Dr. Bernd Krieg-Br¨uckner
(Universit¨at und DFKI Bremen)

ii

Erkl¨arung
Erkl¨arung gem¨aß §10 der Promotionsordnung der Fakult¨at Wirtschaftsinformatik und
Angewandte Informatik an der Otto-Friedrich-Universit¨at Bamberg:
• Ich erkl¨are, dass ich die vorgelegte Dissertation selbst¨andig, das heißt auch ohne
die Hilfe einer Promotionsberaterin bzw. eines Promotionsberaters angefertigt habe
und dabei keine anderen Hilfsmittel als die im Literaturverzeichnis genannten be-
nutzt und alle aus Quellen und Literatur w¨ortlich oder sinngem¨aß entnommenen
Stellen als solche kenntlich gemacht habe.
• Ich versichere, dass die Dissertation oder wesentliche Teile derselben nicht bereits
einer anderen Pr¨ufungsbeh¨orde zur Erlangung des Doktorgrades vorlagen.
• Ich erkl¨are, dass diese Arbeit noch nicht in ihrer Gesamtheit publiziert ist. Soweit
Teile dieser Arbeit bereits in Konferenzb¨anden und Journals publiziert sind, ist
dies an entsprechender Stelle kenntlich gemacht und die Beitr¨age sind im Liter-
aturverzeichnis aufgef¨uhrt.


Zusammenfassung
Diese Arbeit befasst sich mit der induktiven Synthese rekursiver deklarativer Programme
und speziell mit der analytischen induktiven Synthese funktionaler Programme.
Die Programmsynthese besch¨aftigt sich mit der (semi-)automatischen Konstruktion
von Computer-Programmen aus Speziﬁkationen. In der induktiven Programmsynthese
werden rekursive Programme durch das Generalisieren ¨uber unvollst¨andige Speziﬁkatio-
nen, wie zum Beispiel endliche Mengen von Eingabe/Ausgabe-Beispielen (E/A-Beispielen),
generiert. Klassische Methoden der induktiven Synthese funktionaler Programme sind
analytisch; eine rekursive Funktionsdeﬁnition wird generiert, indem rekurrente Struk-
turen zwischen den einzelnen E/A-Beispielen gefunden und generalisiert werden. Die
meisten aktuellen Ans¨atze basieren hingegen auf erzeugen und testen, das heißt, es wer-
den unabh¨angig von den bereitgestellten E/A-Beispielen solange Programme einer Klasse
generiert, bis schließlich ein Programm gefunden wurde das alle Beispiele korrekt berech-
net.
Analytische Methoden sind sehr viel schneller, weil sie nicht auf Suche in einem Pro-
grammraum beruhen. Allerdings m¨ussen daf¨ur auch die Schemata, denen die generier-
baren Programme gehorchen, sehr viel beschr¨ankter sein.
Diese Arbeit bietet zun¨achst einen umfassenden ¨Uberblick ¨uber bestehende Ans¨atze
und Methoden der induktiven Programmsynthese. Anschließend wird ein neuer Algorith-
mus zur induktiven Synthese funktionaler Programme beschrieben, der den analytischen
Ansatz generalisiert und mit Suche in einem Programmraum kombiniert. Dadurch lassen
sich die starken Restriktionen des analytischen Ansatzes zu großen Teilen ¨uberwinden.
Gleichzeitig erlaubt der Einsatz analytischer Techniken das Beschneiden großer Teile des
Problemraums, so dass L¨osungsprogramme oft schneller gefunden werden k¨onnen als mit
Methoden, die auf erzeugen und testen beruhen.
Mittels einer Reihe von Experimenten mit einer Implementation des beschriebenen
Algorithmus’ werden seine M¨oglichkeiten gezeigt.
v


Abstract
This thesis is concerned with the inductive synthesis of recursive declarative programs
and in particular with the analytical inductive synthesis of functional programs.
Program synthesis addresses the problem of (semi-)automatically generating com-
puter programs from speciﬁcations. In inductive program synthesis, recursive programs
are constructed by generalizing over incomplete speciﬁcations such as ﬁnite sets of in-
put/output examples (I/O examples). Classical methods to the induction of functional
programs are analytical, that is, a recursive function deﬁnition is derived by detecting
and generalizing recurrent patterns between the given I/O examples. Most recent meth-
ods, on the other side, are generate-and-test based, that is, they repeatedly generate
programs independently from the provided I/O examples until a program is found that
correctly computes the examples.
Analytical methods are much faster than generate-and-test methods, because they do
not rely on search in a program space. Therefore, however, the schemas that generatable
programs conform to, must be much more restricted.
This thesis at ﬁrst provides a comprehensive overview of current approaches and meth-
ods to inductive program synthesis. Then we present a new algorithm to the inductive
synthesis of functional programs that generalizes the analytical approach and combines
it with search in a program space. Thereby, the strong restrictions of analytical methods
can be resolved for the most part. At the same time, applying analytical techniques al-
lows for pruning large parts of the problem space such that often solutions can be found
faster than with generate-and-test methods.
By means of several experiments with an implementation of the described algorithm,
we demonstrate its capabilities.
vii


Acknowledgments
This thesis would not exist without support from other people.
First of all I want to thank my supervisor Prof. Ute Schmid that she awakened my
interest in the topic of inductive program synthesis when I came to TU Berlin after my
intermediate examination, that she encouraged me to publish and to present work at a
conference when I was still a computer science student, and that she co-supervised my
diploma thesis and ﬁnally became my doctoral supervisor. Ute Schmid always allowed me
great latitude to comprehensively study my topic and to develop my own contributions
to this ﬁeld as they are now presented in this work. I also want to thank Prof. Fritz
Wysotzki, the supervisor of my diploma thesis, for many discussions on the ﬁeld of
inductive program synthesis.
Discussions with my professors Ute Schmid and Fritz Wysotzki, with colleagues in
Ute Schmid’s group, with students at University of Bamberg, and—at conferences and
workshops—with other researchers working on inductive programming, helped me to
clarify many thoughts.
Among all these people, I especially want to thank Martin
Hofmann and further Neil Crossley, Thomas Hieber, Pierre Flener, and Roland Olsson.
I further want to thank Prof. Bernd Krieg-Br¨uckner that he let me present my research
in his research group at University of Bremen and that he was willing to be an external
reviewer of this thesis.
I ﬁnally and especially want to thank my little family, my girlfriend Kirsten and our
two children Laurin and Jonna, for their great support and their endless patience during
the last years when I worked on this thesis.
ix


Contents
1. Introduction
1
1.1. Inductive Program Synthesis and Its Applications . . . . . . . . . . . . . .
1
1.2. Challenges in Inductive Program Synthesis
. . . . . . . . . . . . . . . . .
3
1.3. Related Research Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
1.4. Contributions and Organization of this Thesis . . . . . . . . . . . . . . . .
4
2. Foundations
7
2.1. Preliminaries
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
2.2. Algebraic Speciﬁcation and Term Rewriting . . . . . . . . . . . . . . . . .
8
2.2.1.
Algebraic Speciﬁcation . . . . . . . . . . . . . . . . . . . . . . . . .
8
2.2.2.
Term Rewriting . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
2.2.3.
Initial Semantics and Complete Term Rewriting Systems
. . . . .
18
2.2.4.
Constructor Systems . . . . . . . . . . . . . . . . . . . . . . . . . .
18
2.3. First-Order Logic and Logic Programming . . . . . . . . . . . . . . . . . .
19
2.3.1.
First-Order Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
2.3.2.
Logic Programming
. . . . . . . . . . . . . . . . . . . . . . . . . .
25
3. Approaches to Inductive Program Synthesis
27
3.1. Basic Concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
3.1.1.
Incomplete Speciﬁcations and Inductive Bias
. . . . . . . . . . . .
27
3.1.2.
Inductive Program Synthesis as Search, Background Knowledge . .
28
3.1.3.
Inventing Subfunctions . . . . . . . . . . . . . . . . . . . . . . . . .
29
3.1.4.
The Enumeration Algorithm
. . . . . . . . . . . . . . . . . . . . .
30
3.2. The Analytical Functional Approach . . . . . . . . . . . . . . . . . . . . .
31
3.2.1.
Summers’ Pioneering Work . . . . . . . . . . . . . . . . . . . . . .
32
3.2.2.
Early Variants and Extensions
. . . . . . . . . . . . . . . . . . . .
38
3.2.3.
Igor1: From S-expressions to Recursive Program Schemes
. . . .
43
3.2.4.
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
48
3.3. Inductive Logic Programming . . . . . . . . . . . . . . . . . . . . . . . . .
49
3.3.1.
Overview
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
49
3.3.2.
Generality Models and Reﬁnement Operators . . . . . . . . . . . .
56
3.3.3.
General Purpose ILP Systems . . . . . . . . . . . . . . . . . . . . .
59
3.3.4.
Program Synthesis Systems . . . . . . . . . . . . . . . . . . . . . .
60
3.3.5.
Learnability Results . . . . . . . . . . . . . . . . . . . . . . . . . .
62
3.3.6.
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
63
xi

Contents
3.4. Generate-and-Test Based Approaches to Inductive Functional Programming 64
3.4.1.
Program Evolution . . . . . . . . . . . . . . . . . . . . . . . . . . .
64
3.4.2.
Exhaustive Enumeration of Programs
. . . . . . . . . . . . . . . .
68
3.4.3.
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
70
3.5. Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
70
4. The Igor2 Algorithm
73
4.1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
73
4.2. Notations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
76
4.3. Deﬁnition of the Problem Solved by Igor2
. . . . . . . . . . . . . . . . .
76
4.4. Overview over the Igor2 Algorithm . . . . . . . . . . . . . . . . . . . . .
78
4.4.1.
The General Algorithm
. . . . . . . . . . . . . . . . . . . . . . . .
78
4.4.2.
Initial Rules and Initial Candidate CSs
. . . . . . . . . . . . . . .
79
4.4.3.
Reﬁnement (or Synthesis) Operators . . . . . . . . . . . . . . . . .
82
4.5. A Sample Synthesis
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
84
4.6. Extensional Correctness . . . . . . . . . . . . . . . . . . . . . . . . . . . .
92
4.7. Formal Deﬁnitions and Algorithms of the Synthesis Operators . . . . . . .
95
4.7.1.
Initial Rules and Candidate CSs
. . . . . . . . . . . . . . . . . . .
97
4.7.2.
Splitting a Rule into a Set of More Speciﬁc Rules . . . . . . . . . .
99
4.7.3.
Introducing Subfunctions to Compute Subterms
. . . . . . . . . . 101
4.7.4.
Introducing Function Calls
. . . . . . . . . . . . . . . . . . . . . . 102
4.7.5.
The Synthesis Operators Combined
. . . . . . . . . . . . . . . . . 111
4.8. Properties of the Igor2 Algorithm . . . . . . . . . . . . . . . . . . . . . . 112
4.8.1.
Formalization of the Problem Space
. . . . . . . . . . . . . . . . . 112
4.8.2.
Termination and Completeness of Igor2’s Search
. . . . . . . . . 114
4.8.3.
Soundness of Igor2 . . . . . . . . . . . . . . . . . . . . . . . . . . 122
4.8.4.
Concerning Completeness with Respect to Certain Function Classes125
4.8.5.
Concerning Complexity of Igor2 . . . . . . . . . . . . . . . . . . . 128
4.9. Extensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129
4.9.1.
Conditional Rules
. . . . . . . . . . . . . . . . . . . . . . . . . . . 129
4.9.2.
Rapid Rule-Splitting . . . . . . . . . . . . . . . . . . . . . . . . . . 130
4.9.3.
Existentially-Quantiﬁed Variables in Speciﬁcations . . . . . . . . . 130
5. Experiments
133
5.1. Functional Programming Problems . . . . . . . . . . . . . . . . . . . . . . 133
5.1.1.
Functions of Natural Numbers
. . . . . . . . . . . . . . . . . . . . 134
5.1.2.
List Functions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
5.1.3.
Functions of Lists of Lists (Matrices) . . . . . . . . . . . . . . . . . 140
5.2. Artiﬁcial Intelligence Problems . . . . . . . . . . . . . . . . . . . . . . . . 142
5.2.1.
Learning to Solve Problems . . . . . . . . . . . . . . . . . . . . . . 142
5.2.2.
Reasoning and Natural Language Processing
. . . . . . . . . . . . 145
5.3. Comparison with Other Inductive Programming Systems
. . . . . . . . . 148
xii

Contents
6. Conclusions
151
6.1. Main Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151
6.2. Future Research
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152
Bibliography
155
A. Speciﬁcations of the Experiments
165
A.1. Natural Numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165
A.2. Lists . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
A.3. Lists of Lists
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170
A.4. Artiﬁcial Intelligence Problems . . . . . . . . . . . . . . . . . . . . . . . . 173
Nomenclature
183
Index
185
xiii


List of Figures
2.1. Correspondence between constructor systems and functional programs . .
20
3.1. The classical two-step approach for the induction of Lisp programs . . . .
32
3.2. I/O examples and the corresponding ﬁrst approximation . . . . . . . . . .
35
3.3. The general BMWk schema . . . . . . . . . . . . . . . . . . . . . . . . . .
40
3.4. An exemplary trace for the Init function . . . . . . . . . . . . . . . . . . .
42
3.5. A ﬁnite approximating tree for the Lasts RPS . . . . . . . . . . . . . . . .
45
3.6. Reduced Initial Tree for Lasts . . . . . . . . . . . . . . . . . . . . . . . . .
46
3.7. I/O examples specifying the Lasts function
. . . . . . . . . . . . . . . . .
47
5.1. The Puttable operator and example problems for the clearBlock task . . . . 144
5.2. A phrase-structure grammar and according examples for Igor2 . . . . . . 147
xv


List of Tables
2.1. A many-sorted algebraic signature Σ and a Σ-algebra A . . . . . . . . . .
9
2.2. Example terms, variable assignments, and evaluations
. . . . . . . . . . .
11
2.3. A signature Σ and a Σ-structure A . . . . . . . . . . . . . . . . . . . . . .
21
5.1. Tested functions for natural numbers . . . . . . . . . . . . . . . . . . . . . 134
5.2. Results of tested functions for natural numbers . . . . . . . . . . . . . . . 135
5.3. Tested list functions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138
5.4. Results of tested list functions . . . . . . . . . . . . . . . . . . . . . . . . . 139
5.5. Tested functions for lists of lists (matrices)
. . . . . . . . . . . . . . . . . 141
5.6. Results for tested functions for matrices . . . . . . . . . . . . . . . . . . . 142
5.7. Tested problems in artiﬁcial intelligence and cognitive psychology domains 143
5.8. Results for tested problem-solving problems . . . . . . . . . . . . . . . . . 146
5.9. Empirical comparison of diﬀerent inductive programming systems . . . . . 148
xvii


List of Algorithms
1.
The enumeration algorithm Enum for inductive program synthesis . . . . .
31
2.
A generic ILP algorithm
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
53
3.
The covering algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
53
4.
The general Igor2 algorithm . . . . . . . . . . . . . . . . . . . . . . . . . .
80
5.
initialCandidate(Φ)
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
80
6.
successorRuleSets(r, Φ, B) . . . . . . . . . . . . . . . . . . . . . . . . . . . .
82
7.
The splitting operator χsplit . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
8.
The subproblem operator χsub . . . . . . . . . . . . . . . . . . . . . . . . . 103
9.
The simple call operator χsmplCall
. . . . . . . . . . . . . . . . . . . . . . . 106
10.
sigmaThetaGeneralizations(σ, t, V ) . . . . . . . . . . . . . . . . . . . . . . . 108
11.
The function-call operator χcall . . . . . . . . . . . . . . . . . . . . . . . . . 110
12.
possibleMappings(r, φ, f′) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
xix


List of Listings
3.1.
reverse with accumulator variable . . . . . . . . . . . . . . . . . . . . . . .
29
3.2.
reverse with append (++) . . . . . . . . . . . . . . . . . . . . . . . . . . .
30
3.3.
reverse without help functions and variables . . . . . . . . . . . . . . . . .
30
3.4. List-sorting without subfunctions . . . . . . . . . . . . . . . . . . . . . . .
30
4.1. Mutually recursive deﬁnitions of odd and even induced by Igor2
. . . . .
75
4.2. I/O patterns for reverse
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
78
4.3. I/O patterns for last , provided as background CS for reverse . . . . . . . .
78
4.4. CS for reverse induced by Igor2
. . . . . . . . . . . . . . . . . . . . . . .
78
5.1. I/O examples for the Ackermann function . . . . . . . . . . . . . . . . . . 136
5.2. Induced deﬁnition of the Ackermann function . . . . . . . . . . . . . . . . 136
5.3. Induced CS for shiftL and shiftR
. . . . . . . . . . . . . . . . . . . . . . . 139
5.4. Induced CS for sum . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140
5.5. Induced CS for the swap function . . . . . . . . . . . . . . . . . . . . . . . 140
5.6. Induced CS for weave . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141
5.7. Examples of clearBlock for Igor2 . . . . . . . . . . . . . . . . . . . . . . . 145
5.8. Induced programs in the problem solving domain . . . . . . . . . . . . . . 146
5.9. Induced rules for ancestor
. . . . . . . . . . . . . . . . . . . . . . . . . . . 147
5.10. Induced rules for the word-structure grammar . . . . . . . . . . . . . . . . 148
Speciﬁcations of functions of natural numbers . . . . . . . . . . . . . . . . . . . 165
Speciﬁcations of list functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
Speciﬁcations of functions for lists of natural numbers . . . . . . . . . . . . . . 170
Speciﬁcations of functions of matrices
. . . . . . . . . . . . . . . . . . . . . . . 170
Speciﬁcations of artiﬁcial intelligence problems
. . . . . . . . . . . . . . . . . . 173
xxi


1. Introduction
1.1. Inductive Program Synthesis and Its Applications
Program synthesis research is concerned with the problem of (semi-)automatically de-
riving computer programs from speciﬁcations.
There are two general approaches to
this end: Deduction—reasoning from the general to the particular—and induction—
reasoning from the particular to the general. In deductive program synthesis, starting
point is an (assumed-to-be-)complete speciﬁcation of a problem or function which is then
transformed to an executable program by means of logical deduction rules (e.g., [84, 65]).
In inductive program synthesis (or inductive programming for short), which is the topic
of this thesis, starting point is an (assumed-to-be)incomplete speciﬁcation. “Incomplete”
means that the function to be implemented is speciﬁed only on a (small) part of its in-
tended domain. A typical incomplete speciﬁcation consists of a ﬁnite set of input/output
examples (I/O examples). Such an incomplete speciﬁcation is then inductively gener-
alized to an executable program that is expected to compute correct outputs also for
inputs that were not speciﬁed.
Especially in inductive program synthesis, induced programs are most often declara-
tive, i.e., recursive functional or logic programs.
Example 1.1. Based on the following two equations
f ([x,y])
= y
f ([x,y,z,v,w]) = w ,
specifying that f shall return the second element of a two-element list and the ﬁfth ele-
ment of a ﬁve-element list, an inductive programming system could induce the recursive
function deﬁnition
f ([x])
= x
f (x : xs) = f (xs) ,
computing the last element of given lists of any length ≥1. (x and xs denote variables,
:
denotes the usual algebraic list-constructor “cons”.)
There are two general approaches to inductive program synthesis (IPS):
1. Search- or generate-and-test based methods repeatedly generate candidate pro-
grams from a program class and test whether they satisfy the provided speciﬁca-
tion. If a program is found that passes the test, the search stops and the solution
program is returned. ADATE [82] and MagicHaskeller [45] are two represen-
tative systems of this class.
1

1. Introduction
2. Analytical methods, in contrary, synthesize a solution program by inspecting a
provided set of I/O examples and by detecting recurrent structures in it. Found
recurrences are then inductively generalized to a recursive function deﬁnition. The
classical paper of this approach is Summers’ paper on his Thesys system [104]. A
more recent system of this class is Igor1 [51].
Both approaches have complementary strengths and weaknesses. Classical analytical
methods are fast because they construct programs almost without search.
Yet they
need well-chosen sets of I/O examples and can only synthesize programs that use small
ﬁxed sets of primitives and belong to restricted program schemas like linear recursion.
In contrast, generate-and-test methods are in principle able to induce any program
belonging to some enumerable set of programs, but due to searching in such vast problem
spaces, the synthesis of all but small (toy) programs needs much time or is intractable,
actually.1
Even though IPS is mostly basic research until now, there are several potential areas
of application that have been started to be addressed, among them software-engineering,
algorithm development and optimization, end-user programming, and artiﬁcial intelli-
gence and cognitive psychology.
Software engineering.
In software-engineering, IPS may be used as a tool to semi-
automatically generate (prototypical) programs, modules, or single functions. Especially
in test-driven development [7] where test-cases are the starting point of program devel-
opment, IPS could assist the programmer by considering the test-cases as an incomplete
speciﬁcation and generating prototypical code from them.
Algorithm development and optimization.
IPS could be used to invent new algo-
rithms or to improve existing algorithms, for example algorithms for optimization prob-
lems where the goal is to eﬃciently compute approximative solutions for NP-complete
problems [82, 8].
End-user programming, programming-by-example.
In end-user programming, IPS
may help end-users to generate their own small programs or advanced macros by demon-
strating the needed functionality by means of examples [62, 36].
Artiﬁcial intelligence and cognitive psychology.
In the ﬁelds of artiﬁcial intelligence
and cognitive psychology, IPS can be used to model the capability of human-level cog-
nition to obtain general declarative or procedural knowledge about inherently recursive
problems from experience [95].
Especially in automated planning [32], IPS can be used to learn general problem-
solving strategies in the form of recursive macros from initial planning experience in a
1For example, Roland Olsson reports on his homepage (http://www-ia.hiof.no/~rolando/), that
inducing a function to transpose matrices with ADATE (with only the list-of-lists constructors avail-
able as usable primitives, i.e., without any background knowledge) takes 11.6 hours on a 200MHz
Pentium Pro.
2

1.2. Challenges in Inductive Program Synthesis
domain [96, 94]. For example, a planning or problem-solving agent may use IPS methods
to derive the recursive strategy for solving arbitrary instances of the Towers-of-Hanoi
problem from initial experience with instances including three or four discs [95].
This could be an approach to tackle the long-standing and yet open problem of scal-
ability with respect to the number of involved objects in automated planning. When,
for example, a planner is able to derive the recursive general strategy for Towers-of-
Hanoi from some small problem instances, then the ineﬃcient or even intractable search
for plans for problem instances containing greater numbers of discs can completely be
omitted and instead the plans can be generated by just executing the learned strategy.
1.2. Challenges in Inductive Program Synthesis
In general, inductive program synthesis can be considered as a search problem: Find a
program in some program class that satisﬁes a provided speciﬁcation. In general, the
problem space of IPS is very huge—all syntactically correct programs is some compu-
tationally complete programming language or formalism, such as, for example, Turing
machines, the Haskell programming language (or a suﬃcient subset thereof), or term
rewriting systems. In particular, the number of programs increases exponentially with
respect to their size. Furthermore, it is diﬃcult to generally calculate how changes in
a program aﬀect the computed function. Hence it is diﬃcult to develop heuristics that
work well for a wide range of domains.
To make these diﬃculties more clear, let us compare IPS with more standard machine
learning tasks—the induction of decision trees
[87] and neural networks [90]. In the
case of decision trees, one has a ﬁxed ﬁnite set of attributes and class values that can be
evaluated or tested at the inner nodes and assigned to the leaves, respectively. In the
case of neural networks, if the structure of the net is given, deﬁning the net consists in
deﬁning a weight vector of ﬁxed length of real numbers. Contrary, in IPS, the object
language can in general be arbitrarily extended by deﬁning subprograms or subfunctions
or by introducing additional (auxiliary) parameters.2
Moreover, in decision-tree learning, statistical measures such as the information gain
indicate which attributes are worth to consider at a particular node. In neural nets, the
same holds for the gradient of the error function regarding the update of the weights.
Even though these measures are heuristic and hence potentially misleading, they are
reliable enough to be successfully used in a wide range of domains within a greedy-based
search. It is much more diﬃcult to derive such measures in the case of general programs.
Finally, diﬀerent branches of a decision tree (or diﬀerent rules in the case of learning
non-recursive rules) can be developed independently from each other, based on their
respective subsets of the training data.
In the case of recursive rules, however, the
diﬀerent (base- or recursive) rules/cases generally interdepend. For example, changing a
base case of a recursion not only aﬀects the accuracy or correctness regarding instances
or inputs directly covered by that base case but also those instances that are initially
2This is sometimes called bias shift [106, 101].
3

1. Introduction
evaluated according to some recursive case. This is because each (terminating) evaluation
eventually ends with a base case.
1.3. Related Research Fields
As we have already seen for potential application ﬁelds, inductive program synthesis has
intersections with several other computer science and cognitive science subﬁelds.
In general, IPS lies at the intersection of (declarative) programming, artiﬁcial intel-
ligence (AI) [92], and machine learning [69]. It is related with AI by its applicability
to AI problems, such as automated planning as described above, but also by the used
methods: search, the need for heuristics, (inductive) reasoning to transform programs,
and learning.
It is related with machine learning in that a general concept or model, in our case
a recursive program, is induced or learned from examples or other kinds of incomplete
information. However, there are also signiﬁcant diﬀerences to standard machine learn-
ing: Typically, machine learning algorithms are applied to large data sets (e.g., in data
mining), whereas the goal in inductive program synthesis is to learn from few examples.
This is because typically a human is assumed as source of the examples. Furthermore,
the training data in standard machine learning is most often noisy, i.e., contains errors
and the goal is to learn a model with suﬃcient (but not perfect) accuracy. In contrary,
in IPS the speciﬁcations are typically assumed to be error-free and the goal is to induce
a program that computes all examples as speciﬁed.
By its objects, recursive declarative programs, it is related with functional and logic
programming, program transformation, and research on computability and algorithm
complexity.
Even though learning theory3—a ﬁeld at the intersection of theoretical computer sci-
ence and machine learning, that is concerned with questions such as which kinds of mod-
els are learnable under which conditions from which data and with which complexity—
has not yet extensively studied general recursive programs as objects to be learned, it
can legitimately (and should be) considered as a related research ﬁeld.
1.4. Contributions and Organization of this Thesis
The contributions of this thesis are ﬁrst, a comprehensive survey and classiﬁcation of
current IPS approaches, theory, and methods; second, the presentation of a new powerful
algorithm, called Igor2, for the inductive synthesis of functional programs; and third,
an empirical evaluation of Igor2 by means of several recursive problems from functional
programming and artiﬁcial intelligence:
1. Though inductive program synthesis is an active area of research since the sev-
enties, it has not become an established, uniﬁed research ﬁeld since then but is
3The two seminal works are [33], where Gold introduces the concept of identiﬁcation in the limit
and [107], where Valiant introduces the PAC (probably approximately correct) learning model.
4

1.4. Contributions and Organization of this Thesis
scattered over several ﬁelds such as artiﬁcial intelligence, machine learning, induc-
tive logic programming, evolutionary computation, and functional programming.
Until today, there is no uniform body of IPS theory and methods; furthermore,
no survey of recent results exists. This fragmentation over diﬀerent communities
impedes the exchange of results and leads to redundancies.
Therefore, this thesis at ﬁrst provides a comprehensive overview of existing ap-
proaches to IPS, theoretical results and methods, that have been developed in
diﬀerent research ﬁelds until today. We discuss strengths and weaknesses, similar-
ities and diﬀerences of the diﬀerent approaches and draw conclusions for further
research.
2. We present the new IPS algorithm Igor2 for the induction of functional programs
in the framework of term rewriting.
Igor2 generalizes the classical analytical
recurrence-detection approach and combines it with search in a program space
in order to allow for inducing more complex programs in reasonable time. We
precisely deﬁne Igor2’s synthesis operators, prove termination and completeness
of its search strategy, and prove that programs induced by Igor2 correctly compute
the speciﬁed I/O examples.
3. By means of standard recursive functions on natural numbers, lists, and matri-
ces, we empirically show Igor2’s capabilities to induce programs in the ﬁeld of
functional programming.
Furthermore, we demonstrate Igor2’s capabilities to
tackle problems from artiﬁcial intelligence and cognitive psychology at hand of
learning recursive rules in some well-known domains like the blocksworld or the
Towers-of-Hanoi.
The thesis is mainly organized according to the three contributions:
In the following chapter (2), we at ﬁrst introduce basic concepts of algebraic speciﬁca-
tion, term rewriting, and predicate logic, as they can be found in respective introductory
textbooks.
Chapter 3 then contains the overview over current approaches to inductive program
synthesis. That chapter mostly summarizes research results from other researchers than
the author of this thesis.
A few exceptions are the following: In Section 3.2.3, we
shortly review the IPS system Igor1 that was co-developed by the author of this thesis.
Furthermore, the arguments in the discussions at the end of each section as well as
the conclusions at the end of the chapter, pointing out characteristics and relations
of the diﬀerent approaches, are worked out by the author of this thesis. Finally, the
consideration regarding positive and negative examples in inductive logic programming
and inductive functional programming (at the beginning of Section 3.3.1) is from the
author of this thesis.
In Chapter 4, we present the Igor2 algorithm, developed by the author of this thesis,
that induces functional programs in the term rewriting framework. We precisely deﬁne
its synthesis operators and prove some properties of the algorithm.
5

1. Introduction
In Chapter 5, we evaluate a prototypical implementation of Igor2 at hand of several
recursive functions from the domains of functional programming and artiﬁcial intelli-
gence.
In Chapter 6 we conclude.
One appendix lists the complete speciﬁcation ﬁles used for the experiments of Chap-
ter 5.
6

2. Foundations
In the present thesis, we are concerned with functional and logic programs.
In this
chapter, we deﬁne their syntax and semantics by means of concepts from algebraic
speciﬁcation, term rewriting, and predicate logic. Syntactically, a functional program is
then a set of equations over a ﬁrst-order algebraic signature; a logic program is a set
of deﬁnite clauses. Denotationally, we interpret a functional program as an algebra and
a logic program as a logical structure—the denoted algebra and structure are uniquely
deﬁned as the quotient algebra and the least Herbrand model of the equations and deﬁnite
clauses, respectively.
Operationally, the equations deﬁning a functional program are
interpreted as a term rewriting system and the deﬁnite clauses of a logic program are
subject to (SLD-)resolution.
Under certain conditions, denotational and operational
semantics agree in both cases—the canonical term algebra deﬁned by a set of equations
representing a terminating and conﬂuent term rewriting system is isomorphic to the
quotient algebra and the ground atoms derivable by SLD-resolution from a set of deﬁnite
clauses is equal to the least Herbrand model.
All introduced concepts are basic concepts from algebraic speciﬁcation, term rewriting,
and predicate logic and can be found more detailed in respective textbooks such as [24]
(algebraic speciﬁcation), [6, 105] (term rewriting), and [98] (predicate logic). We do not
provide any proofs here. They can also be found in respective textbooks.
2.1. Preliminaries
We write N for the set of natural numbers including 0 and Z for the set of integers. By
[m] we denote the subset {n ∈N | 1 ≤n ≤m} of all natural numbers from 1 to m.
A family is a mapping I →X : i 7→xi from an (index) set I to a set X, written
(xi)i∈I or just (xi).
Given any set X, by id we denote the identity function on X; id : X →X : x 7→x.
An equivalence relation is a reﬂexive, symmetric, and transitive relation on a set X,
denoted by ∼or ≡. One often writes x ∼y instead of (x, y) ∈∼. By [x]∼we denote
the equivalence class of x by ∼, i.e., the set {y ∈X | x ∼y}. The set of all equivalence
classes of X by ∼is called the quotient set of X by ∼, written X/ ∼. It is a partition
on X.
By | X | , we denote the cardinality of the set X. By P(X), we denote the power set
of the set X.
By Dom(f) we denote the domain of a function f.
By X we denote an countable set whose elements are called variables.
7

2. Foundations
Given a set S, we write S∗for the set of ﬁnite (including empty) sequences s1, . . . , sn
of elements of S. If n = 0, s1, . . . , sn denotes the empty sequence, ϵ.
2.2. Algebraic Speciﬁcation and Term Rewriting
2.2.1. Algebraic Speciﬁcation
We shortly review some basic concepts and results (without proofs) of algebraic speciﬁ-
cation in this section, as, for example, described in [24].
Algebraic Signatures and Algebras
Algebras are sets of values, called carrier sets or universes, together with mathematical
functions deﬁned on them. The functions have names, called function symbols, and are
collected in an algebraic signature.
Deﬁnition 2.1 (Algebraic signature). An algebraic signature is a set Σ whose elements
are called function symbols. Each function symbol f ∈Σ is associated with a natural
number, called the arity of f, written α(f), which denotes the number of arguments f
takes.
Function symbols of arity 0 are called constants. Function symbols of arity one and
two are called unary and binary, respectively. In general, we speak of n-ary function
symbols.
An algebraic signature Σ is interpreted by a Σ-algebra that ﬁxes a set of data objects
or values and assigns to each function symbol a function on the chosen universe.
Deﬁnition 2.2 (Σ-algebra). Let Σ be an algebraic signature. A Σ-algebra A consists
of
• a (possibly empty) set A, called carrier set or universe, and
• for each f ∈Σ, a total function fA : Aα(f) →A.
Remark 2.1 (Constant functions). If α(f) = 0 for an f ∈Σ, then Aα(f) = A0 = {⟨⟩}. In
this case, fA is a constant function denoting the value fA(⟨⟩) which is simply written as
fA.
Parenthesis: The many-sorted case.
Typically, functional programs are typed. The overall
universe of values is partitioned (or many-sorted) and each function is deﬁned only on a speciﬁed
subset of (a product of) the whole universe and also has values only in a speciﬁed subset.
Strong typing assures at compile-time that functions will only be called on appropriate inputs.
In inductive program synthesis, typing is also useful to prune the problem space because it
restricts the number of allowed expressions.
In the rest of this parenthesis we deﬁne many-sorted algebraic signatures and algebras and
give an example. Afterwards we proceed with the unsorted setting because the many-sorted
setting heavily bloats the notation of concepts while they essentially remain the same and are
easily lifted to the many-sorted setting.
8

2.2. Algebraic Speciﬁcation and Term Rewriting
Table 2.1.: A many-sorted algebraic signature Σ and a Σ-algebra A
Σ
A
Sorts
Universes
Nat
N ∪{⊥}
NatList
(Listsa of N) ∪{⊥}
Function symbols
Functions
z : Nat
0
s : Nat →Nat
sA(n) =
(
n + 1
if n ∈N
⊥
if n = ⊥
nil : NatList
()
cons : Nat, NatList →NatList
consA(⊥, l) = consA(e, ⊥) = consA(⊥, ⊥) = ⊥,
consA(e0, (e1, . . . , en)) = (e0, e1, . . . , en)b
Last : NatList →Nat
LastA(⊥) = ⊥, LastA((e1, . . . , en)) =
(
⊥
if n = 0b
en
if n > 0
aIncluding the empty list ().
bThe sequences e1, . . . , en may be empty, i.e., n = 0. We then have consA(e0, ()) = (e0) and LastA(()) =
⊥.
Deﬁnition 2.3 (Many-sorted algebraic signature). A many-sorted algebraic signature is a pair
Σ = ⟨S, OP⟩where
• S is a set whose elements are called sorts, and
• OP = (OP⟨w;s⟩) is an (S∗× S)-indexed family of sets of function symbols.
For f ∈OP⟨s1,...,sn;s⟩we also write f : s1, . . . , sn →s. If f ∈OP⟨ϵ;s⟩, we write f : s and call
f a constant.
Deﬁnition 2.4 (Many-sorted Σ-algebra). Let Σ = ⟨S, OP⟩be a many-sorted algebraic signature.
A many-sorted Σ-algebra A consists of
• an S-indexed family of sets A = (As)s∈S, where the sets As are called carrier sets or
universes, and
• for each f : s1, . . . , sn →s, a total function fA : As1 × · · · × Asn →As.
Table 2.1 shows an example of a (many-sorted) algebraic signature Σ and a Σ-algebra A.
We continue with the unsorted setting. In the following (throughout Section 2.2), Σ
always denotes an algebraic signature and instead of algebraic signature, we may just
say signature.
An algebraic signature Σ only states that a Σ-algebra includes a particular set of
functions.
Terms—words built over the signature and a set of variables (and some
punctuation symbols)—reﬂect, on the syntactic side, the composition of such functions.
Terms are thus the basic means to deﬁne properties of algebras.
9

2. Foundations
Deﬁnition 2.5 (Terms, Herbrand universe). Let Σ be a signature and X be an countable
set whose elements are called variables. Then the set of Σ-terms over X (terms for short),
denoted by TΣ(X), is deﬁned as the smallest set satisfying the following conditions:
• Each variable x ∈X is in TΣ(X).
• If f ∈Σ and t1, . . . , tα(f) ∈TΣ(X), then f(t1, . . . , tα(f)) ∈TΣ(X). (For constants
f ∈Σ we write f instead of f().)
We denote the set of variables occurring in a term t by Var(t). Terms without variables
(Var(t) = ∅) are called ground terms. The subset of TΣ(X) exactly including all ground
terms is denoted by TΣ and called the Herbrand universe of Σ. Ground terms only exist,
if the signature contains at least one constant symbol.
Given an algebra, a ground term denotes a particular composition of functions and
constants and hence a value of the universe. If a term contains variables, the denoted
value depends on an assignment of values to variables. Formally:
Deﬁnition 2.6 (Term evaluation, variable assignment). Let A be a Σ-algebra with
universe A and X be a set of variables. The meaning of a term t ∈TΣ(X) in A is given
by a function β∗: TΣ(X) →A satisfying the following property for all f ∈Σ:
β∗(f(t1, . . . , tn)) = fA(β∗(t1), . . . , β∗(tn)) .
Such a term evaluation function is uniquely determined if it is deﬁned for all variables.
A function β : X →A, uniquely determining β∗, is called variable assignment (or just
assignment).
Table 2.2 shows some terms, variable assignments and evaluations according to Σ and
A of Table 2.1.
Presentations and Models
In algebraic speciﬁcation, properties of algebras are deﬁned in terms of equations.
Deﬁnition 2.7 (Σ-equation, presentation). A Σ-equation is a pair of two Σ-terms,
⟨t, t′⟩∈TΣ(X) × TΣ(X), written t = t′.
A presentation (also called algebraic speciﬁcation) is a pair P = ⟨Σ, Φ⟩of a signature
Σ and a set Φ of Σ-equations, called the axioms of P.
A Σ-equation t = t′ states the requirement to Σ-algebras that for all variable assign-
ments, both terms t and t′ evaluate to the same value. Such an algebra is said to satisfy
an equation. An algebra that satisﬁes all equations in a presentation is a model of the
presentation.
Deﬁnition 2.8 (Satisﬁes, model, loose semantics). A Σ-algebra A with universe A
satisﬁes a Σ-equation t = t′ ∈TΣ(X) × TΣ(X), written
A |= t = t′ ,
10

2.2. Algebraic Speciﬁcation and Term Rewriting
Table 2.2.: Example terms, variable assignments, and evaluations according to Σ and A
of Table 2.1
t ∈TΣ({x, y})
βa
β∗(t)
z
0
s(z)
1
s(s(s(s(z))))
4
nil
()
cons(s(s(z)), cons(z, cons(s(s(s(s(z)))), nil)))
(2, 0, 4)
x
x 7→5
5
s(s(x))
x 7→5
7
cons(z, x)
x 7→(1, 2)
(0, 1, 2)
cons(z, cons(x, cons(y, nil)))
x 7→1, y 7→2
(0, 1, 2)
a We only display values of variables actually occurring in the particular terms.
iﬀfor every assignment β : X →A, β∗(t) = β∗(t′).
A model of a presentation P = ⟨Σ, Φ⟩is a Σ-algebra A such that for all ϕ ∈Φ, A |= ϕ;
we write A |= Φ. The class of all models of P, denoted by Mod(P), is called the loose
semantics of P.
Remark 2.2. Note that the symbol ’=’ has two diﬀerent roles in the previous deﬁnition.
It is (i) a syntactic item to construct equations and it denotes (ii) identity on a universe.
Example 2.1. Consider the following set Φ of Σ-equations over variables {x, y, xs}
where Σ is the example signature of Table 2.1:
Last(cons(x, nil))
= x ,
Last(cons(x, cons(y, xs))) = Last(cons(y, xs)) .
A of Table 2.1 is a model of ⟨Σ, Φ⟩. Now suppose that a Σ-algebra A′ is identical to A
except for the following redeﬁnition of Last:
LastA′(e1, . . . , en) =
(
⊥
if n = 0
e1
if n > 0 .
I.e., LastA′ denotes the ﬁrst element of a list instead of the last one as in A. Then A′ is
not a model of ⟨Σ, Φ⟩, because, for example,
β∗(Last(cons(x, cons(y, xs)))) = 1 ̸= 2 = β∗(Last(cons(y, xs)))
with β(x) = 1, β(y) = 2, β(xs) = ().
If an equation ϕ is satisﬁed by all models of a set of equations Φ, this means, that
whenever Φ states true properties of a particular algebra, also ϕ does. Such an equation
ϕ is called a semantic consequence of Φ.
11

2. Foundations
Deﬁnition 2.9 (Semantic consequence). A Σ-equation ϕ is a semantic consequence
of a set of Σ-equations Φ (or, equivalently, of the presentation ⟨Σ, Φ⟩), if for all A ∈
Mod(⟨Σ, Φ⟩), A |= ϕ. We write Φ |= ϕ in this case.
Example 2.2. The equation Last(cons(x, cons(y, cons(z, nil)))) = Last(cons(z, nil)) is
a semantic consequence of the equations of Example 2.1.
Deﬁnition 2.10 (Theory). A set of equations Φ is closed under semantic consequences,
iﬀΦ |= ϕ implies ϕ ∈Φ. We may close a non-closed set of equations by adding all its
semantic consequences, denoted by Cl(Φ).
A theory is a presentation ⟨Σ, Φ⟩where Φ is closed under semantic consequences. A
presentation ⟨Σ, Φ⟩, where Φ need not to be closed, presents the theory ⟨Σ, Cl(Φ)⟩.
Initial Semantics
The several models of a presentation might be quite diﬀerent regarding their universes
and the behavior of their operations. Two critical characteristics of models are junk and
confusion, deﬁned as follows.
Deﬁnition 2.11 (Junk and confusion). Let P = ⟨Σ, Φ⟩be a presentation and A be a
model with universe A of P.
Junk If there are elements a ∈A that are not denoted by some ground term, i.e., there
is no ground term t with β∗(t) = a, A is said to contain junk.
Confusion If A satisﬁes ground equations that are not in the theory presented by P,
i.e., there are terms t, t′ ∈TΣ such that A |= t = t′ but t = t′ ̸∈⟨Σ, Cl(Φ)⟩, A is
said to contain confusion.
In order to deﬁne the stronger initial semantics, particularly including only models
without junk and confusion, we need a certain concept of function between universes
of algebras to relate algebras regarding their structure as induced by their operations.
A homomorphism is a function h between universes A and B of algebras A and B,
respectively, such that if h maps elements a1, . . . , an ∈A to elements b1, . . . , bn ∈B,
then for all n-ary functions it maps fA(a1, . . . , an) to fB(b1, . . . , bn).
Deﬁnition 2.12 (Homomorphism, Isomorphism). Let A and B be two Σ-algebras with
universes A and B, respectively. A Σ-homomorphism h : A →B is a function h : A →B
which respects the operations of Σ, i.e., such that for all f ∈Σ,
h(fA(a1, . . . , aα(f))) = fB(h(a1), . . . , h(aα(f))) .
A Σ-homomorphism is a Σ-isomorphism if it has an inverse, i.e., if there is a Σ-
homomorphism h−1 : B →A such that h ◦h−1 = idA and h−1 ◦h = idB. In this case,
A and B are called isomorphic, written A ∼= B.
12

2.2. Algebraic Speciﬁcation and Term Rewriting
A homomorphism h : A →B is an isomorphism if and only if h : A →B is bijective.
If two algebras are isomorphic, the only possible diﬀerence is the particular choice of
universe elements. The size of their universes as well as the behavior of their operations
are identical. Hence, if two algebras are isomorphic, often each one is considered as good
as the other and we say that they are identical up to isomorphism.
Now we are able to deﬁne the initial semantics of a presentation.
Deﬁnition 2.13 (Initial algebra). Let A be a Σ-algebra and A be a class of Σ-algebras.
A is initial in A if A ∈A and for every B ∈A there is a unique Σ-homomorphism
h : A →B.
Deﬁnition 2.14 (Initial semantics). Let P = ⟨Σ, Φ⟩be a presentation and A be a Σ-
algebra. If A is initial in Mod(P) then A is called an initial model of P. The class of
all initial models is called the initial semantics of P.
An initial model is a model which is structurally contained in each other model.
The class of all initial models has two essential properties: First, all initial models are
isomorphic. That is, the initial semantics appoint a unique (up to isomorphism) model
of a presentation. Second, as already mentioned above, the initial models are exactly
those without junk and confusion.
There is a standard initial model for presentations, which we will now construct.
Though terms are per se syntactic constructs and need to be interpreted, we may take
TΣ as universe of a particular algebra TΣ, called ground term algebra. The functions of
the ground term algebra apply function symbols to terms, hence construct the ground
terms.
Deﬁnition 2.15 (Ground term algebra). The ground term algebra of signature Σ, writ-
ten TΣ, is deﬁned as follows:
• The universe is the Herbrand universe, TΣ.
• For f ∈Σ, fA(t1, . . . , tα(f)) = f(t1, . . . , tα(t)).
The ground term algebra of signature Σ, as any other Σ-algebra, is a model of the
special, trivial presentation containing no axioms, P0 = ⟨Σ, ∅⟩.
Now reconsider the term evaluation function β∗(Deﬁnition 2.6). It is a function from
TΣ(X) to the universe A of some Σ-algebra A that exhibits the homomorphism property.
That is, β∗restricted to ground terms is a homomorphism from TΣ to A. Moreover, it
is the only homomorphism from TΣ to A and hence, TΣ is an initial model of P0.
If a presentation contains axioms identifying universe elements denoted by some dif-
ferent ground terms, then, certainly, the ground term algebra is not a model of that
presentation. This is because in TΣ, ground terms evaluate to themselves, β∗(t) = t for
each t ∈TΣ, such that β∗(t) ̸= β∗(t′) for any two diﬀerent t, t′ ∈TΣ. The solution for
this case is to partition TΣ such that all ground terms identiﬁed by the axioms are in
one subset each. Taking the partition as universe and deﬁning the functions accordingly
leads to the quotient term algebra, the standard initial model of presentations.
13

2. Foundations
Deﬁnition 2.16 (Quotient algebra). A Σ-congruence on a Σ-algebra A with universe
A is an equivalence ∼on A which respects the operations of Σ, i.e., such that for all
f ∈Σ and a1, a′
1, . . . , aα(f), a′
α(f) ∈A,
a1 ∼a′
1, . . . aα(f) ∼a′
α(f) implies fA(a1, . . . , aα(f)) ∼fA(a′
1, . . . , a′
α(f)) .
Let ∼be a Σ-congruence on A. The quotient algebra of A modulo ∼, denoted by
A/∼, is deﬁned as follows:
• The universe of A/∼is the quotient set A/∼.
• For all f ∈Σ and a1, . . . , aα(f) ∈A, fA/∼([a1]∼, . . . , [aα(f)]∼) = [fA(a1, . . . , aα(f))]∼.
A/∼is a Σ-algebra.
Deﬁnition 2.17 (Quotient term algebra). Let P = ⟨Σ, Φ⟩be a presentation.
The
relation ∼Φ⊆TΣ × TΣ is deﬁned by t ∼Φ t′ iﬀΦ |= t = t′ for all t, t′ ∈TΣ. ∼Φ is a
Σ-congruence on TΣ and called the Σ-congruence generated by Φ. The quotient algebra
of TΣ modulo ∼Φ, TΣ/∼Φ, is called the quotient term algebra of P.
Quotient term algebras TΣ/∼Φ are initial models of the corresponding presentations
P = ⟨Σ, Φ⟩.
2.2.2. Term Rewriting
The concepts of this section are described more detailed in term-rewriting textbooks
such as [6, 105].
Preliminaries
A context is a term over an extended signature Σ ∪{□}, where □is a special constant
symbol not occurring in Σ. The occurrences of the constant □denote empty places,
or holes, in a context. If C is a context containing exactly n holes, and t1, . . . , tn are
terms, then C[t1, . . . , tn] denotes the result of replacing the holes of C from left to right
by t1, . . . , tn. A context C containing exactly one hole is called one-hole context and
denoted by C[ ]. If t = C[s], then s is called a subterm of t. Since with the trivial
context C = □, each term t may be written as C[t], for each term t holds that t itself is
a subterm of t. All subterms of t except for t itself are also called proper subterms.
A position (of a term) is a (possibly empty) sequence of positive integers. The set of
positions of a term t, denoted by Pos(t), is deﬁned as follows: If t = x ∈X, i.e., t is a
variable, or t is a constant, then Pos(t) = {ϵ}, where ϵ denotes the empty sequence. If
t = f(t1, . . . , tn), then Pos(t) = {ϵ} ∪Sn
i=1{i.p | p ∈Pos(si)}. Positions p of a term t
denote subterms t|p of it as follows: t|ϵ = t and f(t1, . . . , tn)|i.p = si|p. By Node(t, p) we
refer to the root symbol of the subterm t|p.
A term is called linear, if no variable occurs more than once in it.
14

2.2. Algebraic Speciﬁcation and Term Rewriting
The syntactic counterpart of a variable assignment and term evaluation is the replace-
ment of variables (in a term) with terms, called substitution.1 That is, a substitution is
a mapping from variables to terms that is uniquely extended to a mapping from terms
to terms:
Deﬁnition 2.18 (Substitution). A substitution is a mapping from terms to terms, σ :
TΣ(X) →TΣ(X), written in postﬁx notation, which satisﬁes the property
f(t1, . . . , tn)σ = f(t1σ, . . . , tnσ)
(for constants, cσ = c).
A substitution is uniquely deﬁned by its restriction to the set X of variables. Applica-
tion of a substitution to variables is normally written in standard preﬁx notation, σ(x).
Most often, we are interested in substitutions with σ(x) ̸= x for only a ﬁnite subset of
all variables. In such a case, a substitution is determined by its restriction to this subset
and typically deﬁned extensionally, σ = {x1 7→t1, . . . , xn 7→tn}. By Dom(σ) we refer
to this ﬁnite subset.
A composition of two substitutions is again a substitution. Since substitutions are
written postﬁxed, the composition of two substitutions σ and τ, σ ◦τ, is written τσ.
Let γ be a further substitution and t be a term. Substitutions satisfy the properties (i)
t(τσ) = (tτ)σ, i.e., applying a substitution composition τσ to a term t is equivalent to
applying ﬁrst τ to t and then σ to the result, and (ii) γ(τσ) = (γτ)σ, i.e., composition
of substitutions is associative. A substitution which maps distinct variables to distinct
variables, i.e., which is injective and has a set of variables as range, is called (variable)
renaming.
Deﬁnition 2.19 (Subsumption, uniﬁcation). If s = tσ for two terms s, t and a substi-
tution σ, then s is called an instance of t. We write t ⪰s and say that t subsumes s,
that t is more general than s, that, conversely, s matches t, and that s is more speciﬁc
than t.
If sσ = tσ for two terms s, t and a substitution σ, then we say that s and t unify. The
substitution σ is called a uniﬁer.
The relation ⪰is a quasi-order on terms, called subsumption order. If t ⪰s but not
s ⪰t, then we write t ≻s, call s a proper instance of t, and say that t is strictly more
general than s and that s is strictly more speciﬁc than t.
Deﬁnition 2.20 (Least general generalization). Let T ⊆TΣ(X) be a ﬁnite set of terms.
Then there is a least upper bound with respect to the subsumption order ⪰of T in
TΣ(X), i.e., a least general term t such that all terms in t are instances of t. The term t
is called least general generalization (LGG) of T, written lgg(T) [85].
1 The comparison of assignments and substitutions is not perfectly appropriate, because the former
assigns a particular value to a variable, which corresponds to a substitution with a ground term.
Substitutions, though, may also be non-ground.
15

2. Foundations
An LGG t of a set of terms {t1, . . . , tn} is equal to each of the ti at each position
where the ti are all equal. On positions, where at least two of the ti diﬀer, t contains a
variable.
LGGs are unique up to variable renaming and computable. The procedure of gener-
ating LGGs is called anti-uniﬁcation.
Example 2.3 (Least general generalization). Let x1, x2, x3, x4 be variables and f, g, h, r, a, c
be function symbols and constants. Let f(a, g(h(x1), c), h(x1)) and f(a, g(r(a), x2), r(a))
be two terms. Their LGG is f(a, g(x3, x4), x3).
Term Rewriting Systems
Deﬁnition 2.21 (Rewrite rule, term rewriting system). A Σ-rewrite rule (or just rule)
is a pair ⟨l, r⟩∈TΣ(X) × TΣ(X) of terms, written l →r. We may want to name or
label a rule, then we write ρ : l →r. The term l is called left-hand side (LHS), r is
called right-hand side (RHS) of the rule. Typically, the set of allowed rules is restricted
as follows: (i) The LHS l may not consist of a single variable; (ii) Var(r) ⊆Var(l).
A term rewriting system (TRS) is a pair ⟨Σ, R⟩where R is a set of Σ-rules.
We can easily extend the concepts of substitution, subsumption, and least general
generalization from terms to rules.
In particular, by (l →r)σ we mean lσ →rσ. We
say that a rule r subsumes a rule r′, if there is a substitution σ such that rσ = r′. And
the LGG of a set R of rules is the least upper bound of R in the set of all rules with
respect to the subsumption order.
Except for the two constraints regarding allowed rules, TRSs and presentations are
syntactically identical—they consist of an algebraic signature Σ together with a set of
pairs of Σ-terms, called rules or equations. They diﬀer regarding their semantics. While
an equation denotes identity, i.e., a symmetric relation, a rule denotes a directed, non-
symmetric relation; or, while equations denotationally deﬁne functions, programs, or
data types, rules deﬁne computations.
Rewriting or reduction means to repeatedly replace instances of LHSs by instances
of RHSs within arbitrary contexts. The two restrictions (i) and (ii) in the deﬁnition
above avoid the pathological cases of arbitrarily applicable rules and arbitrary subterms
in replacements, respectively.
Deﬁnition 2.22 ((One-step) rewrite relation of a rule and a TRS). Let ρ : l →r be a
rewrite rule, σ be a substitution, and C[ ] be a one-hole context. Then
C[lσ] →ρ C[rσ]
is called a rewrite step according to ρ. The one-step rewrite relation generated by ρ,
→ρ⊆TΣ(X) × TΣ(X), is deﬁned as the set of all rewrite steps according to ρ.
Let R be a TRS. The one-step rewrite relation generated by R is
→R =
[
ρ∈R
→ρ .
16

2.2. Algebraic Speciﬁcation and Term Rewriting
The rewrite relation generated by R,
∗→R, is the reﬂexive, transitive closure of →R.
Hence, t0
∗→R tn if and only if t0 = tn or t0 →R t1 →R · · · →R tn.
We may omit indexing the arrow by a rule- or TRS name if it is clear from the context
or irrelevant, and just write: →.
Terminology 2.1 (Instance, redex, contractum, reduct, normal form). For a rule ρ : l →r
and a substitution σ, lσ →rσ is called an instance of ρ. Its LHS, lσ, is called redex
(reducible expression), its RHS is called contractum. Replacing a redex by its contractum
is called contracting the redex.
If t0
∗→tn, tn is called a reduct of t0. The (possibly inﬁnite) concatenation of reduction
steps t0 →t1 →. . . is called reduction. If t does not contain any redex, i.e., there is no
t′ with t →t′, t is called normal form. If tn is a reduct of t0 and tn is a normal form, tn
is called a normal form of t0 and t0 is said to have tn as normal form.
Deﬁnition 2.23 (Termination, conﬂuence, completeness). Let R be a TRS. R is ter-
minating, if there are no inﬁnite reductions, i.e., if for every reduction t0 →R t1 →R . . .
there is an n ∈N such that tn is a normal form. R is conﬂuent, if each two reducts of a
term t have a common reduct. R is complete, if it is terminating and conﬂuent.
If a TRS is conﬂuent, each term has at most one normal form. In this case, the unique
normal form of term t, if it exists, is denoted by t↓. If a TRS is terminating, all terms
have normal forms. Hence, if a TRS is complete, each term t has a unique normal form
t↓.
An important concept with respect to termination is that of a reduction order.
Deﬁnition 2.24 (Reduction order). A reduction order on terms TΣ(X) is a strict order
> on TΣ(X) that
1. does not admit inﬁnite descending chains (i.e., that is a well-founded order),
2. is closed under substitutions, i.e., t > s implies tσ > sσ for arbitrary substitutions
σ,
3. is closed under contexts, i.e., t > s implies C[t] > C[s] for arbitrary contexts C.
A suﬃcient condition for termination of a TRS R is that a reduction order > exists
such that for each rule l →r of R, l > r.
Example 2.4 (Complete TRS, reduction). Reconsider the signature of Table 2.1, Σ =
{z, s, nil, cons, Last}, and the equations Φ of Example 2.1. If we interpret the equations
as rewrite rules, we get the following set R of two rules:
ρ1 :
Last(cons(x, nil))
→x ,
ρ2 :
Last(cons(x, cons(y, xs))) →Last(cons(y, xs)) .
The TRS ⟨Σ, R⟩is terminating, because each contractum will be shorter than the cor-
responding redex, and conﬂuent, because each (sub)term will match at most one of the
LHSs, and hence complete.
17

2. Foundations
Now consider the term (program call): Last(cons(z, cons(s(s(z)), cons(s(z), nil)))). It
is reduced by R to its normal form as follows:
Last(cons(z, cons(s(s(z)), cons(s(z), nil))))
→ρ2
Last(cons(s(s(z)), cons(s(z), nil)))
→ρ2
Last(cons(s(z), nil))
→ρ1
s(z)
Note that the equation Last(cons(z, cons(s(s(z)), cons(s(z), nil)))) = s(z) is a seman-
tic consequence of Φ.
2.2.3. Initial Semantics and Complete Term Rewriting Systems
A complete TRS ⟨Σ, R⟩deﬁnes a particular Σ-algebra (a universe and functions on it),
called the canonical term algebra, as follows: The universe is the set of all normal forms
and the application of a function (to normal forms) is evaluated according to the rules
in R, i.e., to its (due to the completeness of the TRS) always existing and unique normal
form.
Deﬁnition 2.25 (Canonical term algebra). The canonical term algebra CT Σ(R) accord-
ing to a complete TRS ⟨Σ, R⟩is deﬁned as follows:
• The universe is the set of all normal forms of ⟨Σ, R⟩and
• for each f ∈Σ, fCT Σ(t1, . . . , tα(f)) = f(t1, . . . , tα(f))↓.
A functional program, in our ﬁrst-order algebraic setting, is a set of equations, which—
interpreted as a set of rewrite rules—represents a complete TRS (or, in a narrower sense,
a complete constructor TRS; see Section 2.2.4). Its denotational algebraic semantics is
the quotient term algebra (Deﬁnition 2.17), its operational term rewriting semantics
leads to the canonical term algebra. Both are initial models of the functional program
and hence isomorphic.
Theorem 2.1 ([67]). Let ⟨Σ, Φ⟩be a presentation (a set of equations representing a
functional program) such that ⟨Σ, R⟩, where R are the equations of Φ interpreted from
left to right as rewrite rules, is a complete TRS.
Then the canonical term algebra according to ⟨σ, R⟩is an initial model of ⟨Σ, Φ⟩, hence
isomorphic to the quotient term algebra:
CT Σ(R) ∼= TΣ/∼Φ .
2.2.4. Constructor Systems
Consider again the Last-TRS (Example 2.4). The LHSs have a special form: The Last
symbol occurs only at the roots of the LHSs but not at deeper positions whereas the
other function symbols only occur in the subterms but not at the roots. The Last-TRS
has the form of a constructor (term rewriting) system.
18

2.3. First-Order Logic and Logic Programming
Deﬁnition 2.26 (Constructor system). A constructor term rewriting system (or just
constructor system (CS)) is a TRS whose signature can be partitioned into two subsets,
Σ = D ∪C, D ∩C = ∅, such that each LHS has the form
f(t1, . . . , tn)
with f ∈D and t1, . . . , tn ∈TC(X).
The function symbols in D and C are called deﬁned function symbols (or just function
symbols) and constructors, respectively.
Terms in TC(X) are called constructor terms.
Since roots of LHSs are deﬁned func-
tion symbols in CSs and constructor terms do not contain deﬁned function symbols,
constructor terms are normal forms.
A suﬃcient condition for conﬂuence of TRSs is orthogonality. We do not deﬁne or-
thogonality here in general. However, a CS is orthogonal and thus conﬂuent, if its LHSs
are (i) linear and (ii) pairwise non-unifying.
Programs in common functional programming languages like Haskell or SML ba-
sically have the constructor system form.
The constructors in C correspond to the
constructors of algebraic data types and the deﬁned function symbols to the function
symbols deﬁned by equations in, e.g., a Haskell program. The particular form of the
LHSs in CSs resembles the concept of pattern matching in functional programming. An
example of this correspondence is given in Figure 2.1.
Despite these similarities, CSs exhibit several restrictions compared to typical func-
tional programs. First, CSs only allow for algebraic data types. This excludes (prede-
ﬁned) continuous types like real numbers. Second, functions in functional programs are
ﬁrst-class objects, i.e., may occur as arguments and results of (higher-order) functions.
This is not possible for the usual case of ﬁrst-order signatures in term rewriting. Further-
more, partial application (currying) is usual in functional programming but not possible
in standard term rewriting. Finally, CSs consist of sets of rules, whereas in functional
programs, the order of the equations typically matters. In particular, one condition to
achieve conﬂuence of CSs is to choose the patterns in a way such that always only one
pattern is matched by a term (see above). This condition can be weakened if matches
are tried in a ﬁxed and known order, e.g., top-down through the deﬁned functions. This
allows for more ﬂexibility in the patterns.
2.3. First-Order Logic and Logic Programming
The basic concepts of ﬁrst-order logic and logic programming shortly reviewed in this
section are described more detailed in textbooks such as [98]. A very thorough and
consistent introduction to propositional and ﬁrst-order logic, logic programming, and
also the foundations of inductive logic programming (see Section 3.3) can be found
in [81].
19

2. Foundations
Consider again the Last-CS, including its signature, partitioned into C and D:
C = { z
: Num ,
s
: Num →Num ,
nil
: NumList ,
cons : Num NumList →NumList } ,
D = {Last : NumList →Num } ,
and
R = { Last(cons(x, nil))
→x ,
Last(cons(x, cons(y, xs))) →Last(cons(y, xs)) } .
The corresponding Haskell program is:
data Nat
= z | s Nat
data NatList = nil | cons Nat NatList
Last
:: NatList →Nat
Last(cons(x, nil))
= x
Last(cons(x, cons(y, xs)))
= Last(cons(y, xs))
Figure 2.1.: Correspondence between constructor systems and functional programs
2.3.1. First-Order Logic
Signatures and Structures
A signature in ﬁrst-order logic extends an algebraic signature by adding predicate sym-
bols.
A signature is a pair of two sets Σ = (OP, R), OP ∩R = ∅, called function
symbols and predicate (or relation) symbols, respectively. Also predicate symbols have
an associated arity.
A structure extends an algebra by adding relations to it according to a signature.
Deﬁnition 2.27 (Σ-structure). Let Σ be a signature. A Σ-structure A consists of
• a non-empty set A, called carrier set or universe,
• for each f ∈OP, a total function fA : Aα(f) →A, and
• for each p ∈R, a relation pA ⊆Aα(f).
Remark 2.3. In contrast to algebras, one typically requires non-empty universes for
logical structures in order to prevent certain anomalies.
Table 2.3 shows an example of a (many-sorted) signature Σ and a Σ-structure A.
Terms are built over function symbols and variables and evaluated as deﬁned in Def-
initions 2.5 and 2.6, respectively. In particular, the set of all ground Σ-terms is called
the Herbrand universe.
20

2.3. First-Order Logic and Logic Programming
Table 2.3.: A signature Σ and a Σ-structure A
Σ
A
Sorts
Universe
Num
N ∪{⊥}
NumList
(Listsa of N) ∪⊥
Function symbols
Functions
z : Nat
0
s : Nat →Nat
sA(n) =
(
n + 1
if n ∈N
⊥
if n = ⊥
nil : NatList
()
cons : Nat, NatList →NatList
consA(⊥, l) = consA(e, ⊥) = consA(⊥, ⊥) = ⊥,
consA(e0, (e1, . . . , en)) = (e0, e1, . . . , en)b
Predicate symbol
Relation
Last : NumList, Num
{⟨(e1, . . . , en), en⟩}
aIncluding the empty list ().
bThe sequences e1, . . . , en may be empty, i.e., n = 0. We then have consA(e0, ()) = (e0).
A Σ-structure which is based on the ground term algebra (i.e., the universe is the
Herbrand universe and functions are applications of function symbols to terms) is called
a Herbrand interpretation.
As ground term algebras are the basis to deﬁne unique
semantics of a set of equations, in particular of functional programs represented as sets
of equations or rewrite rules, Herbrand interpretations are the basis to deﬁne unique
semantics of logic programs.
Deﬁnition 2.28 (Herbrand interpretation). A Herbrand interpretation of signature Σ
is deﬁned as follows:
• The universe is the Herbrand universe, TΣ.
• For each f ∈Σ, fA(t1, . . . , tα(f)) = f(t1, . . . , tα(t)).
• For each p ∈R, pA ⊆T α(p)
Σ
.
While there is exactly one unique ground term algebra according to any algebraic
signature, Herbrand interpretations are non-unique. They vary exactly with respect to
their relations pA.
Formulas and Models
Deﬁnition 2.29 (Formulas, literal, clause, Herbrand base). The set of well-formed
formulas (or just formulas) according to a signature Σ = ⟨OP, R⟩is deﬁned as follows:
21

2. Foundations
• If p ∈R is an n-ary predicate symbol and t1, . . . , tn are Σ-terms, then p(t1, . . . , tn)
is a formula, called atom;
• if φ and ψ are formulas, then ¬φ (negation), φ∧ψ (conjunction), φ∨ψ (disjunction),
and φ →ψ (implication) are formulas; and
• if φ is a formula and x is a variable, then ∃x φ (existential quantiﬁcation) and ∀x φ
(universal quantiﬁcation) are formulas.
• These are all formulas.
Formulas without variables are called ground formulas. The set of all ground atoms
is called the Herbrand base. A literal is an atom (positive literal) or a negated atom
(negative literal). A clause is a ﬁnite, possibly empty, disjunction of literals. The empty
clause is denoted by □.
For logic programming, only formulas of a particular form are used.
Deﬁnition 2.30 (Horn clause, deﬁnite clause). A Horn clause is a clause with at most
one positive literal. A deﬁnite (program) clause is a clause with exactly one positive
literal.
Deﬁnition 2.31. For a signature Σ, the ﬁrst-order language given by Σ is the set
of all Σ-Formulas. The terms clausal language and Horn-clause language are deﬁned
analogously.
If a signature contains no functions symbols other than constants, the language is
called function-free.
Notation 2.1. A deﬁnite clause C consisting of the positive literal A and the negative
literals ¬B1, . . . , ¬Bn is equivalent to the implication B1∧. . .∧Bn →A, typically written
as
A ←B1, . . . , Bn .
A and B1, . . . , Bn are called the head and body of C, respectively. If the body is empty,
i.e., C consists of a single atom A only, it is written A ←or simply A.
Deﬁnition 2.32. As between algebras and equations, there is a “satisﬁes” relation
between structures and formulas. It is deﬁned, ﬁrst of all with respect to a particular
assignment, as follows:
(A, β) |= p(t1, . . . , tn)
iﬀ
⟨β∗(t1), . . . , β∗(tn)⟩∈pA ,
(A, β) |= ¬ϕ
iﬀ
(A, β) ̸|= ϕ ,
(A, β) |= φ ∧ψ
iﬀ
(A, β) |= φ and (A, β) |= ψ ,
(A, β) |= φ ∨ψ
iﬀ
(A, β) |= φ or (A, β) |= ψ ,
(A, β) |= φ →ψ
iﬀ
(A, β) ̸|= φ or (A, β) |= ψ ,
(A, β) |= ∃x φ
iﬀ
for at least one a ∈A, (A, β[x 7→a]) |= ϕ ,
(A, β) |= ∀x φ
iﬀ
for all a ∈A, (A, β[x 7→a]) |= ϕ ,
22

2.3. First-Order Logic and Logic Programming
where β[x 7→a](y) =
(
β(y)
if x ̸= y
a
if x = y .
Deﬁnition 2.33 (Satisﬁes, (Herbrand) model). A Σ-structure A with universe A sat-
isﬁes a Σ-formula ϕ, written A |= ϕ, if for every assignment β : X →A, (A, β) |= ϕ.
A structure A is a model of a set of formulas Φ, written A |= Φ, if for all ϕ ∈Φ,
A |= ϕ. If, furthermore, A is a Herbrand interpretation, then A is called a Herbrand
model.
By ModΣ(Φ), we denote the class of all models of Φ.
A Herbrand interpretation is uniquely determined by a subset of the Herbrand base,
namely the set of all ground atoms satisﬁed by it. This is because (i) two Herbrand
interpretations only vary with respect to their relations pA and (ii) ⟨t1, . . . , tα(p)⟩∈pA if
and only if p(t1, . . . , tα(p)) is satisﬁed. Therefore, we identify Herbrand interpretations
and their sets of satisﬁed ground atoms: A Herbrand interpretation is just a subset of
the Herbrand base.
Deﬁnition 2.34. A set of formulas Φ is said to be satisﬁable if it has at least one model
and unsatisﬁable if it has no models.
Proposition 2.1. Let Φ be a set of formulas and ϕ be a formula. Φ |= ϕ if and only if
Φ ∪{¬ϕ} is unsatisﬁable.
Example 2.5. Consider the following set Φ of two Σ-formulas (deﬁnite clauses), where
Σ is the signature of Table 2.3:
Last(cons(x, nil), x) ,
Last(cons(x, cons(y, xs)), z) ←Last(cons(y, xs), z) .
The structure A of Table 2.1 is a model of Φ.
Deﬁnition 2.35 (Logical consequence, entailment). A Σ-formula ϕ is a logical conse-
quence of a set of Σ-formulas Φ, written Φ |= ϕ, if for all A ∈ModΣ(Φ), A |= ϕ. We say
that Φ entails ϕ.
The problem whether Φ |= ϕ is undecidable.
Deﬁnition 2.36 (Equivalence). Two Σ-formulas ϕ and ψ are equivalent, written ϕ ≡ψ,
if Mod(ϕ) = Mod(ψ).
Resolution
Since the problem whether Φ |= ϕ is undecidable, there is no algorithm that takes a set
of formulas Φ and a formula ϕ and, after ﬁnite time, correctly reports that either Φ |= ϕ
or Φ ̸|= ϕ. However, calculi exist that after ﬁnite time report Φ |= ϕ if and only if in
fact Φ |= ϕ and otherwise either do not terminate or correctly report Φ ̸|= ϕ. One such
calculus restricted to clauses is resolution as deﬁned in this section.
23

2. Foundations
Substitutions θ (mappings from terms to terms that replace variables by terms; see
Deﬁnition 2.18) are uniquely extended to atoms, literals, and clauses as follows:
p(t1, . . . , tn)θ = p(t1θ, . . . , tnθ), (¬a)θ = ¬(aθ), where a is an atom, and (ϕ ∨ψ)θ =
ϕθ ∨ψθ, where ϕ, ψ are clauses.
By simple expression, we either mean a term or a literal. If E = {e1, . . . , en} is a set
of simple expressions, by Eθ we denote the set {e1θ, . . . , enθ}.
Deﬁnition 2.37 ((Most general) uniﬁer). Let E be a ﬁnite set of simple expressions. A
uniﬁer for E is a substitution θ such that Eθ is a singleton, i.e., a set containing only
one element. If a uniﬁer for E exists, we say that E is uniﬁable.
A most general uniﬁer (MGU) for E is a uniﬁer θ for E such that for any uniﬁer σ for
E exists a substitution γ with σ = θγ.
Proposition 2.2. Let E be a ﬁnite set of expressions.
• The problem whether E is uniﬁable is decidable.
• If E is uniﬁable, then there is an MGU for E.
There are terminating uniﬁcation algorithms that take a ﬁnite set of expressions E and
output either an MGU of E (if E is uniﬁable) or otherwise report that E is not uniﬁable.
Terminology 2.2. Two clauses or (two terms) are said to be standardized apart if they
have no variables in common.
Clauses and terms can easily be standardized apart by applying a variable renaming.
Deﬁnition 2.38 (Binary resolvent). Let C = L1 ∨. . . ∨Lm and C′ = L′
1 ∨. . . ∨L′
n be
two clauses which are standardized apart. If the substitution θ is an MGU for {Li, ¬L′
j}
(1 ≤i ≤m, 1 ≤j ≤n), then the clause
(L1 ∨. . . ∨Li−1 ∨Li+1 ∨. . . ∨Lm ∨L′
1 ∨. . . ∨L′
j−1 ∨L′
j+1 ∨. . . ∨L′
n)θ
is a binary resolvent of C and C′. The literals L and L′ are said to be the literals resolved
upon.
Note that a binary resolvent may be the empty clause □.
Deﬁnition 2.39 (Factor). Let C be a clause, L1, . . . , Ln (n ≥1) be some uniﬁable
literals from C, and θ be an MGU for {L1, . . . , Ln}.
Then the clause obtained by
deleting L2θ, . . . , Lnθ from Cθ is a factor of C.
Deﬁnition 2.40 (Resolvent). Let C and D be two clauses. A resolvent R of C and D
is a binary resolvent of a factor of C and a factor of D where the literals resolved upon
are the literals uniﬁed by the respective factors.
C and D are called the parent clauses of R.
24

2.3. First-Order Logic and Logic Programming
Deﬁnition 2.41 (Derivation, refutation). Let C be a set of clauses and C be a clause.
A derivation of C from C is a ﬁnite sequence of clauses R1, . . . , Rk = C, such that for
all Ri, 1 ≤i ≤k, Ri ∈C or Ri is a resolvent of two clauses in {R1, . . . , Ri−1}.
Deriving the empty clause from a set of clauses C is a called a refutation of C. If a set
of clauses C can be refuted, then C is unsatisﬁable.
Resolution is sound, i.e., Φ |= ϕ whenever ϕ is derivable be resolution from Φ. Fur-
thermore, resolution is, due to Proposition 2.1, complete in the following sense:
Proposition 2.3 (Refutation completeness of resolution). If Φ |= ϕ for a set of clauses
Φ and a clause ϕ, then there is a refutation of Φ ∪{¬ϕ}.
2.3.2. Logic Programming
As functional programs can be regarded as a set of equations or rules of a particular
form according to an algebraic signature, a logic program can be regarded as a set of
formulas of a special form according to a signature.
Sets of arbitrary formulas or even clauses are not appropriate for programming. This
is (i) because general theorem proving and also general resolution on clauses is too
ineﬃcient due to a high degree of non-determinism in each computation step, i.e., in
choosing parent clauses to be resolved and literals to be resolved upon; and (ii) because
for sets of arbitrary formulas or clauses one can not appoint unique models.
For logic programming, deﬁnite programs are used.
Deﬁnition 2.42 (Deﬁnite program). A deﬁnite program is a ﬁnite set of deﬁnite clauses.
Proposition 2.4. Let Π be a deﬁnite program.
• Π has a model iﬀit has a Herbrand model.
• Let M = {M1, M2, . . .} be a possibly inﬁnite set of Herbrand models of Π. Then
the intersection T M is also a Herbrand model of Π.
Deﬁnition 2.43 (Least Herbrand model). Let Π be a deﬁnite program and M the set
of all its Herbrand models. Then the intersection T M is called the least Herbrand model
of Π.
Hence, if a deﬁnite program has a model, it also has a least Herbrand model, which
is unique. It just consists of all ground atoms that are logical consequences of Π and is
taken as its standard denotational semantics.
A program call consists of a conjunction of atoms, possibly containing variables. It
is evaluated by adding its negation to the set of deﬁnite clauses forming the deﬁnite
program and applying a particular eﬃcient form of resolution as deﬁned below to that
set. If the set can be refuted, the corresponding substitutions of the variables are reported
as output of the evaluation.
The negation of a conjunction of atoms ¬(B1∧· · ·∧Bn) is equivalent to a disjunction of
the negated atoms ¬B1∨· · ·∨¬Bn. This is called a goal clause and written ←B1, . . . , Bn.
25

2. Foundations
Deﬁnition 2.44 (SLD-resolution). Let Π be a deﬁnite program and G be a goal clause.
An SLD-refutation of Π ∪{G} is a ﬁnite sequence of goal clauses G = G0, . . . , Gk = □,
such that each Gi (1 ≤i ≤k) is a binary resolvent of Ri−1 and a clause C from Π where
the head of C and a selected literal of Ri−1 are the literals resolved upon.
Theorem 2.2 (Completeness of SLD-resolution with respect to MΠ). Let Π be a deﬁnite
program and A be a ground atom. Then A ∈MΠ if and only if Π ∪{←A} has an SLD-
refutation.
Example 2.6. Consider again the deﬁnite program for Last from Example 2.5 and the
program call Last(cons(z, cons(s(s(z)), cons(s(z), nil))), X) or rather the corresponding
goal clause ←Last(cons(z, cons(s(s(z)), cons(s(z), nil))), X). The refutation consists of
the following sequence:
G0 :
←Last(cons(z, cons(s(s(z)), cons(s(z), nil))), X) ,
G1 :
←Last(cons(s(s(z)), cons(s(z), nil)), X) ,
G2 :
←Last(cons(s(z), nil), X) ,
G3 :
□.
26

3. Approaches to Inductive Program
Synthesis
Even though research on inductive program synthesis started in the 1970s already, it has
not become a uniﬁed research ﬁeld since then, but is scattered over several research ﬁelds
and communities such as artiﬁcial intelligence, inductive inference, inductive logic pro-
gramming, evolutionary computation, and functional programming. This chapter pro-
vides a comprehensive survey of the diﬀerent existing approaches, including theory and
methods. A shortened version of this chapter was already published in [49]. We grouped
the work into three blocks: First, the classical analytic induction of Lisp programs from
examples, as introduced by Summers [104] (Section 3.2); second, inductive logic pro-
gramming (Section 3.3); and third, several recent generate-and-test based approaches to
the induction of functional programs (Section 3.4). In the following section (3.1), we at
ﬁrst introduce some general concepts.
3.1. Basic Concepts
We only consider functions as objects to be induced in this section. General relations,
dealt with in (inductive) logic programming, ﬁt well into these rather abstract illustra-
tions by considering them as boolean-valued functions.
3.1.1. Incomplete Speciﬁcations and Inductive Bias
Inductive program synthesis (IPS) aims at (semi-)automatically constructing computer
programs or algorithms from (known-to-be-)incomplete speciﬁcations of functions. We
call such functions to be induced target functions. Incomplete means, that target func-
tions are not speciﬁed on their complete domains but only on (small) parts of them.
A typical incomplete speciﬁcation consists of a subset of the graph of a target func-
tion f—{⟨i1, o1⟩, . . . , ⟨ik, ok⟩} ⊆Graph(f)—called input/output examples (I/O exam-
ples) or input/output pairs (I/O pairs).
The goal is then to ﬁnd a program P that
correctly computes the provided I/O examples, P(ij) = oj for all 1 ≤j ≤k, (and
that also correctly computes all unspeciﬁed inputs). The concrete shape of incomplete
speciﬁcations varies between diﬀerent approaches to IPS and particular IPS algorithms.
If a program computes the correct speciﬁed output for each speciﬁed input then we
say that the program is correct with respect to the speciﬁcation (or that it satisﬁes the
speciﬁcation). Yet note that, due to the underspeciﬁcation, correctness in this sense
does not imply that the program computes the “correct” function in the sense of the
intended function.
27

3. Approaches to Inductive Program Synthesis
Having in mind that we are concerned with inductive program synthesis from incom-
plete speciﬁcations, we may in the following just say speciﬁcation (instead of incomplete
speciﬁcation).
Due to the inherent underspeciﬁcation in inductive reasoning, typically inﬁnitely many
(semantically) diﬀerent functions or relations satisfy an incomplete speciﬁcation. For
example, if one speciﬁes a function on natural numbers in terms of a ﬁnite number of
I/O examples, then there are obviously inﬁnitely many functions on natural numbers
whose graphs include the provided I/O examples and hence, which are correct with
respect to the provided incomplete speciﬁcation. Without further information, an IPS
system cannot know which of them is intended by the speciﬁer; there is no objective
criterion to decide which of the diﬀerent functions or relations is the right one. This
ambiguity is inherent to IPS and therefore, programs generated by IPS systems are often
called hypotheses.
Even though (or rather: because) there is no objective criterion to decide which of
the possible hypotheses is the intended one, returning one of them as the solution, or
even returning all of them in a particular order, implies criteria to include, exclude,
and/or rank possible solutions. Such criteria are called inductive bias [69]. In general,
the inductive bias comprises all factors—other than the actual incomplete speciﬁcation
of the target function—which inﬂuence the selection or ordering of possible solutions.
There are two general kinds of inductive bias: The ﬁrst one is given by the class of all
programs that can in principle be generated by an IPS system. It may be ﬁxed or problem
dependent and depends on the used object language, including predeﬁned functions that
may be used, and the (search) operators to create and transform programs. It possibly
already excludes particular algorithms or even computable functions (no matter how, by
which algorithm, they are computed). As an example imagine a ﬁnite class of programs
computing functions on natural numbers. Then, certainly, not each computable function
is represented. This bias, given by the class of generatable programs, is called language
bias, restriction bias, or hard bias.
The second kind of inductive bias is given by the order in which an IPS system explores
the program class and by the acceptance criteria (if there are any except for correctness
with respect to the speciﬁcation). Hence it determines the selection of solutions from
generated candidate programs and their ordering. This inductive bias is called search
bias, preference bias, or soft bias. A preference bias may be modelled as a probability
distribution over the program class [78].
3.1.2. Inductive Program Synthesis as Search, Background Knowledge
Inductive program synthesis is most appropriately understood as a search problem. An
IPS algorithm is faced with an (implicitly) given class of programs from which it has
to choose one. This is done by repeatedly generating candidate programs until one is
found satisfying the speciﬁcation. Typically, the search starts with an initial program
and then, in each search step, some program transformation operators are applied to an
already generated program to get new (successor) candidate programs.
In general, the program class is not ﬁxed but depends on additional (amongst the
28

3.1. Basic Concepts
Listing 3.1: reverse with accumulator variable
reverse (l)
= rev (l , [ ])
rev ([ ] , ys)
= ys
rev (x : xs, ys) = rev (xs, x : ys)
speciﬁcation of the function) input to the IPS system. It is determined by primitives,
predeﬁned functions which can be used by induced programs, and some deﬁnition of
syntactically correctness of programs.
In early approaches (Section 3.2), the primitives to be used were ﬁxed within IPS
systems and restricted to small sets of data type constructors, projection functions, and
predicates. By now, usually arbitrary functions may be provided as (problem-dependent)
input to an IPS system.
We call such problem-dependent input of predeﬁned func-
tions background knowledge. It is well known in artiﬁcial intelligence that background
knowledge—in general: knowledge, that simpliﬁes the solution to a problem—is very
important to solve complex problems. Additional primitives, though they enlarge the
program class, i.e., the problem space, may help to ﬁnd a solution program. This is
because solutions may become more compact such that they are constructible by fewer
transformations.
3.1.3. Inventing Subfunctions
Implementing a function typically includes the identiﬁcation of subproblems, the imple-
mentation of solutions for them in terms of separate (sub)functions, and composing the
main function from those help functions. This facilitates reuse and maintainability of
code and may lead to more concise implementations. Furthermore, without subfunc-
tions and depending on available primitives, some functions may not be representable at
all, some particular algorithm may not be representable, or the function deﬁnition will
become bulky and hard to understand.
Hence, introducing and inducing subfunctions that are neither (explicitly) speciﬁed
nor provided as primitives—(sub)function invention—can be an important capability of
IPS systems.
For example, consider the reverse function on lists. Typical implementations either use
an accumulator variable or—to put the ﬁrst element at the end of the reversed rest-list—
the list-appending function ++. Listings 3.1 and 3.2 show these two implementations.
In contrast, Listing 3.3 shows an implementation with only one function deﬁnition only
using the usual list constructors [ ] (empty list) and
:
(“cons”ing an element to the
front of a list) and the selection functions head (ﬁrst element) and tail (rest-list).
For another example, consider sorting a list. Well-known algorithms like quicksort,
mergesort, or selection sort deﬁne subfunctions such as partitioning, splitting, and merg-
ing lists and selecting particular elements from lists. Just for fun, Listing 3.4 shows an
implementation without such subfunctions, only using the list constructors and selection
functions, ≤, and an if −then−else conditional.
29

3. Approaches to Inductive Program Synthesis
Listing 3.2: reverse with append (++)
reverse ([ ])
=
[ ]
reverse (x : xs) =
reverse (xs) ++ (x : [ ])
Listing 3.3: reverse without help functions and variables
reverse ([ ])
=
[ ]
reverse (x : xs) = head(reverse(xs)) : reverse (x : reverse ( tail ( reverse (xs))))
The ability of automatically introducing subfunctions or relations is called predicate
invention in inductive logic programming. It is a kind of constructive induction [68, 73].
In the context of inductive bias one speaks of bias shift [106, 101].
3.1.4. The Enumeration Algorithm
In this subsection, we present a very basic solution to the inductive program synthesis
problem: the enumeration algorithm. The deﬁnitions and results are (slightly adapted)
taken from Biermann [11] and go back, in their original form, to Gold [33]. We restrict
ourselves to incomplete speciﬁcations in terms of I/O examples.
The problem that an IPS algorithm has to solve, is to take a program class and a set
of I/O examples and to return a program P from the program class that computes the
speciﬁed output for each example input. One solution to this problem is the enumeration
algorithm (Algorithm 1), denoted by Enum.
It has some noteworthy properties.
Deﬁnition 3.1. Let PC be a program class. An IPS algorithm ips is
• sound for PC, if for each set of I/O examples E, ips(PC, E) = P implies that P is
correct with respect to E,
• complete for PC, if for each program P ∈PC exists a set of I/O examples E such
that Enum(PC, E) = P ′ and P ′(x) = P(x) for all x ∈Dom(P),
• stable in PC, if for any two disjoint sets E, E′ of I/O examples, Enum(PC, E) = P
and P correctly computes E′ implies Enum(PC, E ∪E′) = P,
Listing 3.4: List-sorting without subfunctions
sort ([ ])
=
[ ]
sort (x : [ ])
= x : [ ]
sort (x : y : xs) =
if x ≤head(sort(y : xs))
then x : sort (y : xs)
else head(sort(y : xs)) : sort (x : tail ( sort (y : xs)))
30

3.2. The Analytical Functional Approach
Algorithm 1: The enumeration algorithm Enum for inductive program synthesis
Input: An enumerable program class PC = {Pi | i ∈N} with decidable
Halting-problem
Input: A set of I/O pairs E = {⟨i1, o1⟩, . . . , ⟨ik, ok⟩}
Output: A program P ∈PC such that P(ij) = oj for all 1 ≤j ≤k
i ←0
1
while Pi does not correctly compute E do increment i
2
return Pi
3
• input-optimal for PC in a class IPS of IPS algorithms, if ips ∈IPS and there is
no algorithm ips′ ∈IPS such that
– ips(PC, E) = P implies ips′(PC, E′) = P for some E′ ⊆E and
– ips(PC, E) ̸= ips(PC, E′) for at least one such E′.
Soundness means, that induced programs are correct with respect to the speciﬁcation;
completeness means, that each program P (or at least a program that is equivalent to
P on P’s domain) in a given program class can be induced; stability means, that if ips
returns a program P based on some set of I/O pairs E, it will return P based on any
set of I/O examples of P containing E; and input-optimality for a certain class of IPS
algorithms means, that there is no algorithm in that class, which induces all programs
based on less information.
We call enumerable program classes with decidable Halting-problem admissible.
Theorem 3.1 (Biermann). The enumeration algorithm is
• sound, complete, and stable for admissible program classes and
• input-optimal in the class of sound, complete, and stable IPS algorithms for ad-
missible program classes.
Proof. See [11, Theorems 5 and 6].
Clearly, for considerable program classes the enumeration algorithm is practically
useless because of arbitrary large indices of the solution programs.
Finding “good”
program classes and methods to eﬃciently explore them is the problem of inductive
program synthesis research.
3.2. The Analytical Functional Approach
A ﬁrst systematic attempt to IPS was made by Summers [103, 104]. He noticed that
under particular restrictions regarding allowed primitives, program schema, and choice
of I/O examples, a recursive Lisp program can be directly computed from I/O examples
31

3. Approaches to Inductive Program Synthesis
I/O
examples
1. Step: deriving program
fragments and predicates
−−−−−−−−−−−−−−−−−−→
Non-recursive
approximating
program
2. Step: detecting and
generalizing recurrences
−−−−−−−−−−−−−−−−→Recursive
program
Figure 3.1.: The classical two-step approach for the induction of Lisp programs
instead of found by searching in program space. In this section we describe his original
method and some extensions and variants of this approach.
The general principle of the analytic approach is this: If a function is recursively
deﬁned, then evaluating one input (that is not covered by some base case) depends on
evaluating other, smaller, inputs by the same program. Hence outputs of smaller inputs
go into outputs of greater inputs in a recurrent way. These recurrent relations between
I/O examples are discovered and then inductively generalized to a recursive function
deﬁnition.
3.2.1. Summers’ Pioneering Work
Summers’ approach to induce recursive Lisp functions from I/O examples includes two
steps (see Figure 3.1): First, a so-called program fragment, an expression of one variable
and the allowed primitives, is derived for each I/O-pair such that if it is applied to
the input, evaluates to the speciﬁed output.
Furthermore, predicates are derived to
distinguish between example inputs. Integrated into a McCarthy conditional [66], these
predicate/fragment pairs build a non-recursive program computing the I/O examples. It
is considered as a ﬁrst approximation to the target function. In a second step, recurrent
relations between predicates and fragments each are identiﬁed and a recursive program
generalizing them is derived.
Inputs and outputs are S-expressions, the fundamental data structure of the Lisp
language [66].
Deﬁnition 3.2 (S-expressions).
• Each atom (constant) is an S-expression;
• if a and b are S-expressions, so is (a . b);
• these are all S-expressions.
S-expressions are uniquely constructed and deconstructed by the functions cons, car,
and cdr:
• cons(a, b) = (a . b)
• car((a . b)) = a
• cdr((a . b)) = b
Non-atomic S-expressions (a.b) are also called cons-pairs. car and cdr are undeﬁned for
atomic S-expressions. The predicate atom(a) is true if a is an atom and false otherwise.
The set of all subexpressions of an S-expression consists of the S-expression itself and,
if it is a cons-pair, of all subexpressions of its both components.
32

3.2. The Analytical Functional Approach
Remark 3.1.
1. Lists are a special form of S-expressions. A list (a, b, c, . . .) is repre-
sented by the S-expression (a.(b.(c.(. . . .nil) . . .))). nil is a special atom denoting
the empty list.
2. Compositions of car and cdr are abbreviated by words of the form c{a|d}∗r. For
example, caddr abbreviates car ◦cdr ◦cdr.
The programs constructed by Summers’ technique use the Lisp primitives cons, car,
cdr, nil, atom, and T, the last denoting the truth value true. Particularly, no other
predicates than atom and T (e.g., eq for testing equality of S-expressions), and no atoms
except for nil are used. This choice of primitives is not arbitrary but crucial for Sum-
mers’ methodology of deriving programs from examples without search. The McCarthy
conditional and recursion are used as control structure. The McCarthy conditional takes
a chain of predicate/function-pairs pi →fi, i ∈N, and eventually evaluates that func-
tion fi whose predicate pi is the ﬁrst one evaluating to true. Allowing atom and T as
only predicates and nil as only atom in induced programs means that the atoms in the
I/O examples, except for nil, are actually considered as variables. Renaming them does
not change the meaning. This implies that any semantic information must be expressed
by the structure of the S-expression.
1. Step: Generation of Program Fragments and Predicates
Given a set of k I/O examples, {⟨i1, o1⟩, . . . , ⟨ik, ok⟩}, a program fragment fj(x), j =
1, . . . , k, composed of cons, car, and cdr is derived for each I/O-pair. It evaluates to the
output when applied to the input: fj(ij) = oj for all j ∈{1, . . . , k}.
Recall that S-expressions are uniquely constructed by cons and decomposed by car
and cdr. We call car-cdr compositions basic functions (cp. [100]). Together with the
following two conditions, this allows for determining unique program fragments.
1. Each atom, except for nil, may occur only once in each input.
2. Each atom, except for nil, occurring in an output must also occur in the corre-
sponding input.
Due to the ﬁrst condition, each subexpression (except for nil) occurs exactly once in an
S-expression such that subexpressions are denoted by unique basic functions.
Deriving a program fragment works as follows. All non-nil subexpressions of an input,
together with their unique basic functions, are enumerated. Then the output is rewritten
by composing the basic functions from the input subexpressions with cons and nil: If
the output is nil then it is its own fragment, nil. Otherwise, if it is a subexpression of
the input then the fragment becomes the associated basic function. Otherwise, program
fragments for the two subexpressions of the output cons-pair are derived and a cons
is applied to them. Condition 2 from above assures that always one of the three cases
holds.
33

3. Approaches to Inductive Program Synthesis
Example 3.1. Consider the I/O-pair ((a.b).(c.d)) 7→((d.c).(a.b)). The input contains
the following subexpressions, paired with the corresponding unique basic functions:
⟨((a . b) . (c . d)), I ⟩,
⟨(a . b), car⟩,
⟨(c . d), cdr⟩,
⟨a, caar⟩,
⟨b, cdar⟩,
⟨c, cadr⟩,
⟨d, cddr⟩.
Since the example output is neither a subexpression of the input nor nil, the program
fragment becomes a cons of the fragments for the car- and the cdr-component, respec-
tively, of the output. The car-part, (d . c), again becomes a cons, namely of the basic
functions for d: cddr, and c: cadr. The cdr-part, (a . b), is a subexpression of the in-
put, its basic function is car. With variable x denoting the input, the fragment for this
I/O example is thus:
cons(cons(cddr(x), cadr(x)), car(x))
Next, predicates pj(x), j = 1, . . . , k, must be determined. In order to get the correct
program fragment fj be evaluated for each input ij, all predicates pj′, 1 ≤j′ < j
(positioned before pj in the conditional), must evaluate to false when applied to ij and
pj(ij) must evaluate to true.
For all j ∈{1, . . . , k}: pj′(ij) = false for all 1 ≤j′ < j and pj(ij) = true.
(3.1)
Predicates fulﬁlling this condition exist and are eﬃciently computable if the example
inputs form a chain according to the following order:
a ≤b
if
atom(a)
(a . b) ≤(c . d)
if
a ≤c ∧b ≤d
(3.2)
Assume the examples are ordered such that i1 < . . . < ik according to the deﬁned
order.1
It is easy to see that condition (3.1) is fulﬁlled with predicates of the form
atom(bj(x)) (where bj denotes a basic function) for all j ∈{1, . . . , k −1} and pk(x) = T,
if bj(ij) evaluates to an atom and bj(ij+1) does not. Hence, an algorithm for computing
such basic functions only need to compare each two consecutive inputs and identify those
positions denoting atoms in the ﬁrst and cons-pair subexpressions in the second input
and return the corresponding basic functions. The predicate pk becomes T because no
input ik+1 exists to derive a more speciﬁc predicate.
Formal deﬁnitions of the algorithms for computing fragments and predicates can be
found in [100]. Figure 3.2 shows an example for the ﬁrst step. The result of the ﬁrst
synthesis step, a non-recursive program, correctly computes all provided I/O examples.
It is considered as a ﬁrst approximation to the target function.
2. Step: Identifying and Generalizing Recurrence Relations
The basic idea in Summers’ generalization method is this: The fragments are assumed
to be the actual computations carried out by a recursive program for the target function.
1If they are provided in a diﬀerent order, they are automatically reordered.
34

3.2. The Analytical Functional Approach
I/O examples:
(a) 7→nil,
(a, b) 7→(a),
(a, b, c) 7→(a, b),
(a, b, c, d) 7→(a, b, c),
(a, b, c, d, e) 7→(a, b, c, d) .
Initial non-recursive approximating program:
F(x) = (atom(cdr(x)) →nil
atom(cddr(x)) →cons(car(x), nil)
atom(cdddr(x)) →cons(car(x), cons(cadr(x), nil))
atom(cddddr(x)) →cons(car(x), cons(cadr(x), cons(caddr(x), nil)))
T →cons(car(x), cons(cadr(x), cons(caddr(x),
cons(cadddr(x), nil)))))
Figure 3.2.: I/O examples and the corresponding ﬁrst approximation
Hence fragments of greater inputs must comprise fragments of lesser inputs as subterms,
with a suitable substitution of the variables and in a recurrent form along the set of
fragments. The same holds analogously for the predicates. Summers calls this relation
between fragments and predicates diﬀerences.
Deﬁnition 3.3. A diﬀerence exists between two terms (fragments or predicates) t, t′ iﬀ
t′ = C[tσ] for some context C[ ] and substitution σ.
If we have k + 1 I/O examples, we only consider the ﬁrst k fragment/predicate pairs
because the last predicate is always ’T’, such that no sensible diﬀerence can be derived
for it.
Example 3.2. The following diﬀerences, with σ = {x ←cdr(x)} and σ′ = ∅, can be
identiﬁed in the approximating program from Figure 3.2:
f2 = cons(car(x), f1σ′)
p2 = p1σ
f3 = cons(car(x), f2σ)
p3 = p2σ
f4 = cons(car(x), f3σ)
p4 = p3σ
The context C[ ] and the substitution σ are equal in the diﬀerences for the fragments
f3, f4 and predicates p2, p3, p4.
This allows us to rewrite the diﬀerences in terms of
recurrence relations.
35

3. Approaches to Inductive Program Synthesis
Example 3.3. The diﬀerences from Example 3.2 can be written as recurrence relations:
f1 = nil
p1 = atom(cdr(x))
f2 = cons(car(x), nil)
p2 = atom(cddr(x))
fi+1 = cons(car(x), fiσ)
pi+1 = piσ
with σ = {x ←cdr(x)} and i = 2, 3.
In the general case, we have, for k fragments/predicates, a context C[ ], and a substi-
tution σ:
j −1 “constant” fragments (as derived from the examples):
f1, . . . , fj−1,
further n constant base cases:
fj, . . . , fj+n−1,
ﬁnally, remaining k −(j + n −1) cases recurring to previous cases:
fi+n = C[fiσ],
for j ≤i ≤k −n ;
and the same for predicates:
p1, . . . , pj−1, pj, . . . , pj+n−1, pi+n = piσ .
(3.3)
Index j denotes the ﬁrst predicate/fragment pair which recurs in some following predi-
cate/fragment pair (the ﬁrst base case). The precedent j −1 predicate/fragment pairs
do not recur. n is the interval of the recurrence. In Example 3.3 we have k = 4, j = 2,
and n = 1.
Inductive Inference.
If k −j ≥cn with 1 < c ∈N, then we inductively infer that the
recurrence relations hold for all i ≥j.
This is the only logically unsound operation in the synthesis. It is classical induction.
We observe some property for some instances of a concept without counter instances and
thus assume that the property is inherent to all instances of the concept. The constant
c determines how often the regularity between the fragments and predicates have to be
observed until the induction seems to be justiﬁed. Summers sets c = 2.
In Example 3.3 we have k −j −1 = 2 ≥2 = 2n and hence induce that the relations
hold for all i ≥2.
The generalized recurrence relations lead to new approximations of the assumed target
function. The mth approximating function, m ≥j, is deﬁned as
Fm(x) = (p1(x) →f1(x), . . . , pm(x) →fm(x), T →ω)
where the pi, fi with j < i ≤m are deﬁned in terms of the generalized recurrence
relations and ω means undeﬁned. Consider the following complete partial order (cpo)
over partial functions which is well known from denotational semantics and domain
theory:
F(x) ≤G(x)
iﬀ
F(x) = G(x) for all x ∈Dom(F) .
Regarding this order, the set of approximating functions builds a chain.
Deﬁnition 3.4. We deﬁne the function F speciﬁed by the examples to be sup {Fm(x)},
the supremum of the chain of approximations.
36

3.2. The Analytical Functional Approach
Now the hypothesized target function is deﬁned, in terms of recurrence relations. In
his synthesis theorem and several corollaries, Summers shows how a partial function
deﬁned in this way can be expressed by a recursive program.2 The following theorem
comprises the basic synthesis theorem and its ﬁrst corollary in [104].
Theorem 3.2 ([104]). If F is deﬁned by the recurrence relations
f1, . . . , fj−1, fj, . . . , fj+n−1, fi+n = C[fiσ1],
p1, . . . , pj−1, pj, . . . , pj+n−1, pi+n = piσ
for i ≥j ,
then the following recursive program, with main function F, computes F:
F(x) = (p1(x) →f1(x),
...
pj−1(x) →fj−1(x),
T →G(x))
G(x) = (pj(x) →fj(x),
...
pj+n−1(x) →f(x),
T →C[G(σ(x))])
Example 3.4. The generalized (i ≥2) recurrence relations from Example 3.3 deﬁne the
function F to be the Init-function, which reproduces a list of arbitrary length without
its last element. The resulting recursive program according to the synthesis theorem
(Theorem 3.2) is:
F(x) = (atom(cdr(x)) →nil, T →G(x))
G(x) = (atom(cddr(x)) →cons(car(x), nil), T →cons(car(x), G(cdr(x))))
In two additional corollaries, Summers extends the theorem to the more general case
where the basic functions in the recurrences, leading to the substitutions in the recursive
call, may be diﬀerent for fragments and predicates.
Introducing Auxiliary Parameters
Recurrence relations as stated in (3.3) do not always exist such that the non-recursive
program cannot immediately be inductively generalized as described.
In this case a
variable-addition heuristic is applied, transforming the original fragments into more gen-
eral ones by replacing some common subexpression by a variable. For the resulting more
general set of fragments, a recurrence relation may exist.
Example 3.5. Given the following examples for reverse,
nil 7→nil, (a) 7→(a), (a, b) 7→(b, a), (a, b, c) 7→(c, b, a),
2 This works, in a sense, reverse to interpreting a recursively expressed function by the partial function
given as the ﬁxpoint of the functional of the recursive deﬁnition. In the latter case we have a recursive
program and want to have the particular partial function computed by it—here we have a partial
function and want to have a recursive program computing it.
37

3. Approaches to Inductive Program Synthesis
the program fragments
f1 = nil,
f2 = cons(car(x), nil),
f3 = cons(cadr(x), cons(car(x), nil)),
f4 = cons(caddr(x), cons(cadr(x), cons(car(x), nil))).
are derived. Diﬀerences are, with σ = ∅:
f2 = cons(car(x), f1σ), f3 = cons(cadr(x), f2σ), f4 = cons(caddr(x), f3σ) .
All diﬀerences have diﬀerent contexts such that we cannot generalize them as de-
scribed. Indeed, the reverse function cannot be implemented by only using the allowed
primitives and without auxiliary recursive subprograms, an additional accumulator vari-
able, or more complex recursive calls than linear recursion only.
We observe that nil is a subexpression common to all fragments, so we may replace
it by a new variable yielding fragments of two variables:
Example 3.6.
g1 = y,
g2 = cons(car(x), y),
g3 = cons(cadr(x), cons(car(x), y)),
g4 = cons(caddr(x), cons(cadr(x), cons(car(x), y))) .
Generally, let s denote such a common subexpression which is replaced by a variable y,
leading to more general fragments gi. Then we have fi = gi{y ←s} for all i. If recurrence
relations exist based on these more general fragments, then a recursive program G is
synthesized from the fragments gi (and the corresponding predicates), as described in
the previous section, and F(x) is set to F(x) = G(x, s).
Example 3.7. For our reverse example, the following common diﬀerences exist, with
σ = {x ←cdr(x), y ←cons(car(x), y)}: g2 = g1σ, g3 = g2σ, g4 = g3σ.
Together
with the diﬀerences identiﬁed for the predicates (not stated here) the resulting program,
computing the reverse function, is:
F(x) = G(x, nil)
G(x, y) = (atom(x) →y, T →G(cdr(x), cons(car(x), y)))
3.2.2. Early Variants and Extensions
BMWk: Extended Forms of Recurrence Relations
In Summers’ approach, the condition for deriving a recursive function from detected dif-
ferences is that the diﬀerences hold—starting from an initial index j and for a particular
38

3.2. The Analytical Functional Approach
interval n—recurrently along fragments and predicates with a constant context C[ ] and
a constant substitution σ for the variable x. The BMWk3 algorithm [52, 42, 53, 43] of
Kodratoﬀand his colleagues generalizes these conditions by allowing for contexts and
substitutions that are diﬀerent in each diﬀerence. Then a found sequence of diﬀerences
originates a sequence of contexts and substitutions each. The new sequences are con-
sidered as fragments of new subfunctions. The BMWk algorithm is then recursively
applied to these new fragment sequences, hence features the automatic introduction of
subfunctions that are not explicitly speciﬁed. Furthermore, Summers’ ad-hoc method to
introduce additional variables is systematized by computing least general generalizations
(LGGs) (see Deﬁnition 2.20) of successive fragments. The ﬁrst step—deriving fragments
and predicates—is identical to Summers.
Deﬁnition 3.5. A sequence of fragments f1, . . . , fk is a matching sequence iﬀthere
exists a sequence of substitutions σ2, . . . , σk such that fi = fi−1σi for all 2 ≤i ≤k. For
each x ∈Dom(σ), σ2(x), . . . , σk(x) is a generated sequence of fragments.
This is similar to Summers’ diﬀerencing; Deﬁnition 3.3. The extension is to consider
the substitutions as new generated sequences of fragments.
In Summers’ recurrence
relations, Equation (3.3), the substitution needs to be constant for the whole sequence,
starting at index j.
In contrast, the BMWk algorithm considers the, possibly non-
constant, sequences of substitutions for each variable as fragments of new subfunctions
and is recursively applied to them.
Remark 3.2. The deﬁnition exhibits two limitations compared to Summers. It only con-
siders matchings at roots of fragments whereas Summers matches also subfragments, and
it considers all and only directly consecutive fragment pairs whereas Summers considers
initial non-recurring fragments and recurrence intervals n ≥1. These are no inherent
restrictions, the BMWk algorithm may also match (sub)fragments of any interval. We
choose the limited version here in order to keep things simple and because it suﬃces to
explain the ideas of the BMWk algorithm that extend Summers’ method. If matching of
subterms is allowed, then, in addition to the sequence of substitutions, also the sequence
of possibly non-constant contexts constitute a new fragment sequence.
It may happen that a sequence of fragments is not a matching sequence, either be-
cause there exists a position denoting diﬀerent functions in two consecutive fragments.
Or because one and the same variable appears at at least two diﬀerent positions in a
fragment and had to be substituted by two diﬀerent terms, which is not allowed, in
order to get the fragment matched. In such a case, the fragment sequence is generalized.
For an example, consider again the fragments for reverse as stated in Example 3.5. If,
according to Deﬁnition 3.5, matching is allowed at root positions only, then these frag-
ments do not form a matching sequence, because, for example, we have nil as root of f1
and cons as root of f2, such that f2 cannot be expressed in terms of f1.
3This abbreviates Boyer-Moore-Wegbreit-Kodratoﬀ, because the algorithm has its roots in a paper of
Boyer and Moore on proving theorems about Lisp functions [15] and a paper of Wegbreit on goal
directed program transformation [108].
39

3. Approaches to Inductive Program Synthesis
F(x) = F ′(x, I(x))
F ′(x, y) = (p1(x) →f1(x, y),
...
pn(x) →fn(x, y),
T →H(x, F ′(σ(x), G(x, y))))
Figure 3.3.: The general BMWk schema (due to [100, page 317]).
H and G are
either terms over the signature {cons, car, cdr} or automatically intro-
duced (sub)programs themselves ﬁtting the schema and resulting from non-
constant context- and/or substitution-sequences, respectively.
Deﬁnition 3.6. The sequence g1, . . . , gk−1 is the least generalized sequence of a fragment
sequence f1, . . . , fk iﬀgi = lgg(fi, fi+1) for all 1 ≤i ≤k −1.
This least generalization method is a systematization of Summers’ heuristic method
of replacing any common expressions by a variable as described in Section 3.2.1. The
least generalization of two fragments assures that systematically those expressions (from
the ﬁrst, to be matched, fragment) precluding the matching—due to the two possible
reasons mentioned above—are replaced by new, unique variables.
The least generalized sequence for the fragments in Example 3.5 corresponds to the
sequence of generalized fragments in Example 3.6, except for that the last fragment,
g4, is missing. Since each generalized fragment results from two (anti-uniﬁed) original
fragments, it is inherent to this method of generalization that the generalized sequence
contains one fragment less than the source sequence.
Induced programs ﬁt the general schema shown in Figure 3.3. The diﬀerent variables
x, y allow for diﬀerent substitutions for predicates pi and functions fi. I is some function
initializing y.
Not every program ﬁtting the schema in Figure 3.3 is inducible. Particularly (cp.
Remark 3.2), H may only compute some context for the result of a recursive call of F ′.
For a counterexample, consider the deﬁnition for reverse that uses Append instead of an
accumulator variable: of the recursive case of reverse that uses Append:
reverse(x) = (atom(x) →nil, T →Append(reverse(xs), x)),
Append(x, y) = (atom(x) →y, T →cons(car(x), Append(cdr(x), y))) .
This program ﬁts the schema with F ′ = reverse and H = Append. In particular, Append
itself ﬁts the schema. However, Append does not compute a context for the recursive
call of reverse. Consider for example the I/O-pair: reverse([a, b, c]) = [c, b, a]. Then the
recursive call computes [c, b] from [b, c]. But [c, b] is not a subterm of [c, b, a], i.e., there
is no context for [c, b] yielding [c, b, a]. Hence, this deﬁnition of reverse with Append as
subfunction cannot be induced by the BMWk algorithm.
40

3.2. The Analytical Functional Approach
Biermann et al: Pruning Enumerative Search Based on Single Traces
Summers objective was to avoid search and to justify the synthesis by an explicit in-
ductive inference step and a subsequent proven-to-be-correct program construction step.
This could be achieved by a restricted program schema and the requirement of a well
chosen set of I/O examples.
On the contrary, Biermann’s approach [11] is to employ traces (fragments) to speed
up an exhaustive enumeration of a well-deﬁned program class, the so-called regular Lisp
programs. Biermann’s objectives regarding the synthesis were
1. convergence to the class of regular Lisp programs,
2. convergence on the basis of minimal input information,
3. robust behavior on diﬀerent inputs.
Particularly 2 and 3 are contradictory to the recurrence detection method—by 2 Bier-
mann means that no synthesis method exists which is able to synthesize every regular
Lisp program from fewer examples and by 3 he means that examples may be chosen
randomly.
A semiregular program f is a ﬁnite non-empty set of component programs fi, i =
1, . . . , m with f1 being the initial component, f(x) = f1(x). A component program has
the form
fi(x) = (pi1(x) →fi1(x), . . . , pin(x) →fin(x))
where the predicates form a chain and each fij is either nil or x or fh(car(x)) or
fh(cdr(x)) or cons(fh(x), fk(x)) with fh, fk being component programs of f.
Predi-
cates p1, . . . , pk form a chain iﬀthey have the form atom(bi(x)) for 1 ≤i < k and
pk = T where the bi are basic functions and bi is a suﬃx of bi+1 for 1 ≤i < k −1. If
the conditional of a component program fi consists of only one predicate/function pair
(T →fi1) then the conditional is not needed and it is abbreviated to fi(x) = fi1(x).
A regular program is a semiregular program satisfying some precedence criteria re-
garding application of the primitive functions, if diﬀerent orders are possible. The class
of regular programs is decidable and a property is stated [11] for deciding whether a pro-
gram belongs to the class. It is strictly smaller than the class of computable functions.
The enumeration.
After program fragments (Section 3.2.1) have been computed for
each, possibly only one, I/O example, they are rewritten as regular programs (without
recursion and conditionals). Figure 3.4 shows one I/O pair for the Init function, its
program fragment (cp. Section 3.2.1) and the fragment rewritten as a regular program.
In the following, with trace we always refer to the regular program form of a program
fragment. Note that each component program of a trace has an associated (example)
input resulting from the input of the I/O example for f1 and the computations of the
precedent component programs in the trace. In Figure 3.4, f1, f2, f3 have the original
input (a, b, c), f4 has input a, f5 has input (b, c), and f6, f7, f8 have input b.
41

3. Approaches to Inductive Program Synthesis
An I/O pair for the Init function: (a, b, c) 7→(a, b).
The corresponding fragment: cons(car(x), cons(car(cdr(x)), nil)).
The fragment rewritten as regular program:
f1(x) = cons(f2(x), f3(x))
f2(x) = f4(car(x))
f3(x) = f5(cdr(x))
f4(x) = x
f5(x) = f6(car(x))
f6(x) = cons(f7(x), f8(x))
f7(x) = x
f8(x) = nil
Figure 3.4.: One I/O pair for the Init function, its fragment, and the fragment rewritten
as regular program
The further synthesis, the generalization step, is eﬀectively an enumeration of regular
programs, in ascending order regarding the number of component programs. It stops
when a program reproducing all traces has been found. Only those programs reproducing
at least some initial sequence of the given traces are enumerated.4 This drastically prunes
the set of considered regular programs but certainly does not exclude possible solutions
from consideration.
For the following description of the generalization we assume that exactly one I/O ex-
ample, i.e., one trace is provided. Biermann’s method works as follows: Initially, the
trace is tried to be expressed by only one component function. That is, all components
fi of the trace need to be merged together into one and the same initial component f1.
The diﬀerent trace components are accounted for by integrating them into a McCarthy
conditional within f1. Therefore, a chain of predicates distinguishing the inputs associ-
ated with the diﬀerent trace components is generated. This is done in a way similar to
the method described in Section 3.2.1. If all diﬀerent expressions can be accounted for
we are done. If not, a regular program with two components is searched for.
For the trace in Figure 3.4 it is not possible to merge all component functions. To
see this, observe that the components f1, f2, f3 clearly carry out diﬀerent computations,
thus need to become functions in diﬀerent predicate/function-pairs in the McCarthy
conditional building the body of the resulting single component function. Yet the input
of f1, f2, f3 is the same, (a, b, c), such that they cannot be distinguished.
Searching for a program with two components means to try all possible partitions of
the trace components into two subsets. The components of each subset are again tried
to be merged together. If this does not succeed, all partitions into three subsets are tried
4This method is an adaptation to the Lisp domain of earlier work [12, 13] on program synthesis from
traces.
42

3.2. The Analytical Functional Approach
and so on. It is obvious that, if no regular program with fewer components exists, the
search eventually stops with the reproduced trace as solution.
For our example, merging into two components fails for the same reason as merging
into one component—at least three components are necessary to properly distinguish
f1, f2, f3. Indeed, three components suﬃce.
Example 3.8. Consider the partitioning of the eight components from Figure 3.4 into
three subsets representing three components of the induced program. f1, f2, f3 must
belong to diﬀerent subsets due to their same input but diﬀerent computations. Compo-
nents f4 and f7 may be merged into one component, obviously. Again f4, f7, f6, and f8
must belong to diﬀerent subsets because they carry out diﬀerent computations yet have,
structurally, the same input. One solution fulﬁlling these constraints is the partition
{f1, f5, f6}, {f2, f4, f7}, {f3, f8} .
The resulting induced program, correctly implementing the Init function, is
f1(x) = (atom(x) →cons(f2(x), f3(x)), atom(cddr(x)) →f1(car(x)),
T →cons(f2(x), f3(x)))
f2(x) = (atom(x) →x, T →f2(car(x)))
f3(x) = (atom(x) →nil, T →f1(cdr(x))) .
As we have seen, many partitions may be skipped over. This is, generally, because
if one particular subset fails, i.e., its functions cannot be merged together, then all
combinations of partitioning the remaining components need not be considered.
Biermann’s pruned enumeration algorithm for regular Lisp programs has all properties
of the enumeration algorithm (with respect to the class of regular Lisp programs), see
Section 3.1.4.
3.2.3. Igor1: From S-expressions to Recursive Program Schemes
At the beginning of Section 3.2.1 we stated the Lisp primitives as used in programs
induced by Summers’ method (as well as by BMWk and Biermann’s method). This
selection is crucial for the ﬁrst step, the deterministic construction of ﬁrst approxi-
mations, yet not for the generalization step.
Indeed, the latter is independent from
particular primitives, it rather relies on matching (sub)terms over arbitrary ﬁrst-order
signatures. The recent system Igor1 [79, 94, 51], inspired by Summers’ recurrence de-
tection method, induces recursive program schemes, a particular kind of (constructor)
term rewriting systems.
A recursive program scheme (RPS) [21] is a TRS with the following characteristics:
The signature is divided into two disjoint subsets F and G, called unknown and basic
functions, respectively; rules have the form F(x1, . . . , xn) →t where F ∈F and the
43

3. Approaches to Inductive Program Synthesis
xi are variables, and there is exactly one rule for each F ∈F. One of the unknown
functions is distinguished as the main function.5
Igor1 automatically introduces recursive subfunctions that are not explicitly speci-
ﬁed, if needed, as well as additional variables. Induced functions can be tree-recursive
instead of only linear recursive. (Recursive) calls of deﬁned functions can not be nested,
however.
Recursive RPSs do not terminate. Their standard interpretation is the inﬁnite term
deﬁned as the limit limn→∞,F(x) n
→t t where F denotes the main rule of the RPS. One
gets ﬁnite approximations (in the sense of domain theory) by replacing inﬁnite subterms
by the special symbol Ω, meaning undeﬁned. Certainly, such an inﬁnite tree and its
approximations contain recurrent patterns because they are generated by repeatedly re-
placing instances of LHSs of the rules by instances of their RHSs. In the case of RPSs
we call this rewriting process unfolding (of the RPS) and the inﬁnite term as well as its
ﬁnite approximations, (ﬁnite) unfoldings.
Igor1 takes a ﬁnite approximation t of some (hypothetical) inﬁnite tree (i.e., a ﬁnite
unfolding of some hypothetical RPS) as input, discovers the recurrent patterns in it, and
builds an RPS R such that t is a ﬁnite unfolding of R. We call such ﬁnite approximating
terms initial terms.
We say that Igor1 folds the provided initial term to an RPS.
Similar to Summers inductive inference step, the technique requires that discovered
patterns recur a few times such that there is some amount of evidence for the induced
RPS.
Folding of Initial Terms to Recursive Program Schemes
We describe the folding method by means of an example.
Example 3.9. Consider the following RPS that takes a list of lists and computes a list
of the last elements of the input lists:
Lasts(x) →if (atom(x), nil, cons(car(Last(car(x))), Lasts(cdr(x))))
Last(x) →if (atom(cdr(x)), x, Last(cdr(x)))
Figure 3.5 shows one of its ﬁnite approximations.
The goal of folding is to induce an RPS from an initial term. In our example, the
Lasts RPS shall be induced from the initial tree of Figure 3.5. To this end, the initial
term is considered as ﬁnite unfolding of the (unknown) RPS to be induced.
The induction of each rule is done in three steps: First, positions in the initial tree are
identiﬁed where the rule has been unfolded, i.e., where (instances of) its LHS has been
replaced by (instances of) its RHS. This divides the initial tree into segments. Second,
the RHS (except for the recursive calls) is computed by antiunifying the segments. The
5Actually, an RPS is a special constructor system: The unknown and basic functions correspond to
the deﬁned functions and constructors, respectively, in a CS. Yet whereas the formal arguments of
a deﬁned function symbol in the LHS may be constructor terms in a CS, they are restricted to be
single variables in an RPS.
44

3.2. The Analytical Functional Approach
if
if
cons
if
if
cons
if
if
x
x
x
x
x
x
x
x
x
Omega
Omega
cdr
atom
cdr
cdr
cdr
if
x
x
Omega
atom
cdr
cdr
cdr
atom
cdr
atom
cdr
cdr
cdr
atom
cdr
cdr
atom
atom
car
car
car
nil
nil
car
car
car
car
car
cdr
cdr
nil
car
car
Figure 3.5.: A ﬁnite approximating tree for the Lasts RPS. The intended meaning of
cdr, car, cons, and atom is as stated at the beginning of Section 3.2.1, that
of if is a 3-ary function that takes the value of its second parameter if its
ﬁrst parameter evaluates to true and its third parameter otherwise.
45

3. Approaches to Inductive Program Synthesis
x
cdr
cdr
x
car
x
car
cons
if
cons
if
if
x
x
Omega
atom
atom
cdr
cdr
atom
nil
nil
nil
car
car
Last
Last
Figure 3.6.: Reduced Initial Tree for Lasts
variables of the resulting least general generalization of the set of segments are taken as
variables of the function. And third, the substitutions of the induced variables within the
recursive calls are computed from the instances of the variables regarding the diﬀerent
sequences.
The segmentation of the initial tree is computed as follows: Since in the ﬁnite unfolding
process, each LHS is replaced by an Ωeventually, the unfolding positions must lie at
paths to Ωs. Moreover, the sequence of symbols between these positions must always
be the same since they belong to the same rule. Hence, we search for recurring symbol
sequences from the root of the tree to the Ωs. In our example tree we ﬁnd a recurring
if −cons sequence from the root to the rightmost Ω. The corresponding segmentation
of the tree is indicated by the blue curves on that path. We do not ﬁnd this sequence at
paths to the other Ωs. Therefore, we assume that the remaining Ωs belong to a further
subfunction, that is called by the main function. We cut the corresponding subtrees as
indicated by the brown bars and consider them as unfoldings of that subfunction.
Before steps two and three are applied, the assumed additional subfunctions, only
one in our example, are induced from the subtrees by recursively applying the folding
algorithm to them. If this has been done, the initial tree is reduced by replacing the
subtrees by suitable calls to the induced subfunctions. For our example, this results in
the tree shown in Figure 3.6.
The segments of such a reduced tree are now taken as diﬀerent terms (the positions
where the tree is divided become the name of the function since these are the positions
of the recursive calls), each one an instantiation of the RHS of the assumed function.
For our example tree this leads to the two terms
1. if (atom(x), nil, cons(car(Last(car(x))), Lasts)),
2. if (atom(cdr(x)), nil, cons(car(Last(car(cdr(x)))), Lasts)).
(The third segment is incomplete and not considered.)
46

3.2. The Analytical Functional Approach
nil 7→nil,
(a) 7→(a),
((a, b)) 7→(b),
((a, b, c), (d)) 7→(c, d),
((a, b, c, d), (e, f)) 7→(d, f),
((a), (b), (c)) 7→(a, b, c)
Figure 3.7.: I/O examples specifying the Lasts function
Since the ﬁrst of these two segments subsumes the second one, it is also the LGG,
i.e., the induced RHS without substitution of the only recursive call. The only variable
is x and the substitution of x in the second segment, i.e., in the recursive call, is {x 7→
cdr(x)}.
This leads to the rule for Lasts as stated in the RPS in Example 3.9.
Achieving Finite Approximations
The folding algorithm, as described above, is, like BMWk, a generalization of Summers’
second synthesis step—the inductive generalization based on discovering syntactical re-
currences in an approximating program. Yet whereas BMWk deals with approximating
programs as constructed in Summers’ ﬁrst synthesis step—a chain of predicate/fragment
pairs integrated into a McCarthy conditional—, the folding algorithm deals with the
more general concept of ﬁnite terms over any ﬁrst-order signature. There is no straight-
forward adaptation of Summers’ ﬁrst step to this new concept and, more important, a
simple adaptation would not take advantage of the generalization.
Diﬀerent methods to achieve a ﬁnite approximating term have been proposed. Kitzel-
mann and Schmid [51] describe an extension of Summers’ ﬁrst step. Inputs and outputs
are S-expressions, manipulated by a ﬁxed set of primitives as in Summers’ approach.
Yet they need not be linearly ordered.
For example, the Lasts function could be speciﬁed by the I/O examples shown in
Figure 3.7. They lead to the initial term of Figure 3.5 with the following method:
First, program fragments are generated for all given I/O examples as described in
Section 3.2.1. The initial term is then constructed by going through all fragments in
parallel position by position. If the same function symbol stands at the current position
in all fragments, then it is introduced to the initial term at this position. If at least two
fragments diﬀer at the current position, then an if-then-else-ﬁ-expression is introduced.
Therefore, a predicate function is generated to distinguish the inputs according to the
diﬀerent fragments. Construction of the initial term proceeds from the partitioned inputs
and fragments for the then- and else-branch, respectively.
As another approach of achieving ﬁnite approximations, Wysotzki, Schmid, and Kitzel-
mann [96, 94, 47] propose universal planning and diﬀerent subsequent transformations
47

3. Approaches to Inductive Program Synthesis
as ﬁrst step to program synthesis. In this case, starting point is not a set of I/O examples
but a planning problem over a ﬁnite domain of objects. Result of universal planning
is a graph whose nodes denote states and whose paths denote optimal plans from the
states to some distinguished goal state. Such a universal plan must then be transformed
into an initial term that can be generalized by the folding algorithm. This transfor-
mation is not satisfyingly solved yet. In particular it seems as if the transformation to
an appropriate approximating term is not possible without detecting recurrences in the
universal plan already such that the serial execution of plan transformation and folding
contains serious redundancies. A second problem is that a classical plan describes a
linear transformation from an initial state (an input) to the goal state (an output). As a
consequence, the planning approach can only construct approximating terms of linearly
recursive RPSs. These issues are analyzed in Kitzelmann’s diploma thesis [47].
3.2.4. Discussion
Summers’ important insights were ﬁrst, how the algebraic properties of S-expressions can
be exploited to construct program fragments and predicates uniquely without search
and second, that fragments (and predicates) for diﬀerent I/O pairs belonging to one
recursively deﬁned function share recurrent patterns that can be used to identify a
recursive function deﬁnition, correctly computing the provided I/O examples.
Obviously, this recurrence detection method relies on sets of I/O examples that are
not randomly chosen but “complete” in the sense that if one particular example input
is given, then also all inputs that are generated by recursive calls of the (hypothetical)
target function deﬁnition must be included. Hence, a set of I/O examples must contain
the ﬁrst k ∈N I/O pairs with respect to the order of the domain of the target function
established by the recursive call. For example, if the domain is lists and in the recursive
call the rest list is taken, then this orders lists by their length such that all lists up to a
certain length must be provided as example inputs.
To see this by means of the Init example, again consider the I/O examples for Init in
Figure 3.2. If, for example, the second I/O pair would be missing, then the recurrence
relation in Example 3.3 could not be established and the recursive Init implementation
could not be derived.
Hence the recurrence detection method presupposes that the speciﬁer has some knowl-
edge regarding the recursive scheme of the function to be induced. This phenomenon
also applies to inductive logic programming methods as described in the next section,
so we delay further discussion of this point to the conclusion 3.5 of this chapter.
The unique construction of program fragments relies on the unique construction and
decomposition of S-expressions by cons, car, and cdr. That is, this technique excludes
the use of background knowledge.
Furthermore, the derivation of the recursive pat-
tern from the examples without search relies on the restricted linear-recursive program
schema.
Both—no usage of background knowledge and the simple program schema—are strong
restrictions regarding the class of programs that can be induced and prevent Summers’
approach in its original form from being practically relevant. Especially the support
48

3.3. Inductive Logic Programming
of background knowledge is important from an artiﬁcial intelligence perspective—using
existing knowledge to ﬁnd simpler solutions or to ﬁnd solutions at all—as well as from
a programming perspective—re-use of code, use of libraries.
The BMWk algorithm extends Summers original method by relaxing the strongly re-
stricted program schema. Certainly, this may lead to non-deterministic steps during the
program induction—for example, computing a matching sequence with non-equal sub-
stitutions and recursively applying BMWk to the derived substitution sequence versus
computing a generalized sequence and recursive applying BMWk to that sequence.
The deterministic construction of program fragments as well as the detection of syn-
tactical recurrences is not restricted to S-expressions and the respective constructors
and selection functions but can be extended to terms over arbitrary algebraic signa-
tures. This idea, besides and extension of Summers’ original schema, is realized in the
relatively recent Igor1 system.
Finally, Biermann’s approach shows that it is possible to use program expressions
computing single I/O pairs as derived in Summers’ ﬁrst synthesis step to prune the
exhaustive enumeration of a program class as done by the enumeration algorithm (Al-
gorithm 1) without loosing its properties stated in Theorem 3.1.
3.3. Inductive Logic Programming
Inductive Logic Programming (ILP) [74, 78, 81] is a branch of machine learning—
intensional concept descriptions are learned from examples and counterexamples, called
positive and negative examples. The speciﬁcity of ILP lies in its basis in computational
logic: First-order clausal logic is used as uniform representation language for induced
programs (hypotheses), examples, and background knowledge, semantics of ILP is based
on entailment, and inductive learning techniques are derived by regarding induction as
the inverse of deduction.
Horn clause logic together with resolution constitutes the (Turing-complete) program-
ming language Prolog. Program synthesis is therefore principally within the scope of
ILP and has been regarded as one application ﬁeld of ILP [78]. One of the ﬁrst ILP
systems, MIS [99], is an automatic programming/debugging system. Today, however,
ILP is concerned with (relational) data-mining and knowledge discovery and program
synthesis does not play a role anymore.
3.3.1. Overview
In general, hypotheses, examples, and background knowledge are sets of clauses. Hy-
potheses and background knowledge are ﬁnite. A common restriction, which is suﬃ-
ciently adequate for our program synthesis perspective, is to restrict hypotheses and
background knowledge to be deﬁnite programs and examples to be ground atoms.
49

3. Approaches to Inductive Program Synthesis
On Positive and Negative Examples
In concept learning in general and in inductive logic programming in particular, it is
common to have negative examples, i.e., examples of what the concept or relation to
be learned is not, in addition to the positive examples of what the concept or relation
is. This seems to be in contrast to the (only positive) I/O examples in the inductive
synthesis of functional programs (Sections 3.2 and 3.4) and one might ask, why negative
examples are necessary in one setting while unnecessary in the other.
We do not give answers here for the question which concepts are theoretically learnable
from positive and negative or only positive examples. We rather want to give a short
argument that the apparent discrepancy of only positive examples in one setting and
positive and negative examples in the other setting is no discrepancy but that negative
examples in the relational setting actually follow from the only positive examples in
the functional setting and vice versa. The argument is based on considering functions
mathematically as special relations and, on the other hand, relations as special, boolean-
valued, functions.
Since we were concerned with functions until now, we ﬁrst consider relations as
boolean-valued functions. A relation r ⊆X1 × · · · × Xn is considered as the function
fr : X1 × · · · × Xn →B, with B = {true, false}, deﬁned by:
fr(x1, . . . , xn) =
(
true
if (x1, . . . , xn) ∈r
false
if (x1, . . . , xn) ̸∈r .
For example, consider the relation Even ⊆N, containing all even natural numbers and
the corresponding boolean-valued function fEven : N →B. If we speciﬁed the function
fEven by I/O pairs, we reasonably would provide examples for both possible outputs.
For example:
{⟨0, true⟩, ⟨1, false⟩, ⟨2, true⟩, ⟨3, false⟩} .
All these examples are positive in that they say what the function value is for the speciﬁed
inputs, instead of what it is not; e.g., ⟨1, true⟩. The same examples as speciﬁcation for
the relation Even would yet be:
E+ = {0, 2},
E−= {1, 3} ,
where E+ means positive examples and E−means negative examples.
Hence one can say that if one speciﬁes (only positive) I/O examples for a boolean-
valued function, this corresponds to provide positive as well as negative examples from
the perspective of specifying a relation.
Now let us consider functions mathematically as special relations—namely as (single-
valued) relations r ⊆X1×· · ·×Xn where the product set X1×· · ·×Xn can be partitioned
into a domain X1 × · · · × Xk and a codomain Xk+1 × . . . × Xn (k ≤n) such that
(x1, . . . , xn) ∈r implies (x1, . . . , xk, yk+1, . . . , yn) ̸∈r if yi ̸= xi for any i ∈{k + 1, . . . n}.
For example, consider the Init function again.
Some (positive) examples for Init,
considered as relation, would be
E+ = {([a], [ ]), ([a, b], [a]), ([a, b, c], [a, b])} .
50

3.3. Inductive Logic Programming
Now if an ILP system “knows” that the speciﬁed object is a functional (single-valued)
relation, these positive examples immediately imply negative examples: all pairs with
ﬁrst components occurring in E+ but diﬀerent second components than in E+ are neg-
ative examples; e.g., since ([a, b], [a]) ∈E+, it is ([a, b], [b]) ̸∈Init, hence implicitly
([a, b], [b]) ∈E−.
Hence, the knowledge that the speciﬁed object is actually a function is a kind of
inductive bias from the relational perspective that allows for omitting negative examples.
The ILP Problem
The ILP problem, restricted to the deﬁnite setting and respecting the normal semantics6
is given by the following two deﬁnitions.
Deﬁnition 3.7. Let Π be a deﬁnite program and E+, E−be positive and negative
examples. Π is
complete with respect to E+ if Π |= E+,
consistent with respect to E−if Π ̸|= e for every e ∈E−,
correct with respect to E+ and E−if it is complete and consistent with respect to E+
and E−, respectively.
Deﬁnition 3.8 (The ILP problem). Given
• a set of possible hypotheses (deﬁnite programs) H,
• positive and negative examples E+, E−,
• consistent background knowledge B (i.e., B ̸|= e for every e ∈E−) such that
B ̸|= E+,
ﬁnd a hypothesis H ∈H such that H ∪B is correct with respect to E+ and E−.
Entailment (|=) is undecidable in general and for Horn clauses, deﬁnite programs, and
between deﬁnite programs and single atoms in particular. Thus, in practice, diﬀerent de-
cidable (and preferably eﬃciently computable) coverage relations, which are sound but
more or less incomplete with respect to entailment, are used. We say that a program
covers an example if it can be proven true from it by some sound, but not necessarily
complete, proof calculus. Hence, a program is correct if it covers all positive and no neg-
ative examples, but not each correct program is covered such that some actual solutions
will be rejected.
Two commonly used notions of coverage are:
6This is also called explanatory setting because the induced program “explains” the truth of the positive
and the non-truth of the negative examples. There is also a non-monotonic setting in ILP where
hypotheses need not to entail the positive examples but shall only state true properties. This is useful
for data mining and knowledge discovery but not for inductive programming, so we do not consider
it here.
51

3. Approaches to Inductive Program Synthesis
Extensional coverage. Given a hypothesis H, a ﬁnite set of ground atoms B as back-
ground knowledge, positive examples E+, and an example e, H (together with
B) extensionally covers e if there exists a clause (A ←B1, . . . , Bn) ∈H and a
substitution θ such that Aθ = e and {B1, . . . , Bn}θ ⊆B ∪E+.
Intensional coverage. Given a program P and an example e, P intensionally covers e
if e can be proven true from P by applying some terminating proof technique, e.g.,
depth-bounded SLD-resolution.
“Extensional” refers to the fact that coverage of an example mostly relies—except
for one single clause—on the (incomplete) extensional problem description in terms of
example- and background knowledge facts. On the other side, “intensional” refers to the
fact, that coverage of an example relies on the intensional deﬁnition of a relation by a
(recursive) program.
Example 3.10. Let E+ = { Init([c], [ ]), Init([b, c], [b]), Init([a, b, c], [a, b]) }, B = ∅, and
P = { Init([X], [ ]), Init([X | Xs], [X | Ys]) ←Init(Xs, Ys) }. P implements the Init
function, therefore entails E+, and also covers all examples in E+, both extensionally
and intensionally. To see this for the extensional case, consider, e.g., the positive example
Init([a, b, c], [a, b]) and the recursive clause in P. The head of this clause is matched with
the example by θ = {X ←a, Xs ←[b, c], Ys ←[b]}. Applying θ to the body of the clause
yields Init([b, c], [b]) ∈E+.
Both extensional and intensional coverage are sound. Extensional coverage is more
eﬃcient but less complete. For example, suppose the positive example Init([c], [ ]) is
missing in E+ in Example 3.10. Then P still intensionally covers e = Init([b, c], [b]),
yet not covers e extensionally anymore. Obviously, extensional coverage requires that
positive examples (and background knowledge) are complete up to some complexity as
it is also required for the analytical induction of Lisp programs (cp. Section 3.2.4).
Another problem with extensional coverage is that if two clauses each do not cover a
negative example, both together possibly do [88].
Intensional and extensional coverage are closely related to the general ILP algorithm
(Algorithm 2) and the covering algorithm (Algorithm 3) as well as to the generality
models entailment and θ-subsumption, respectively, as described below in Section 3.3.2.
ILP as Search
ILP is considered as a search problem in a space of (deﬁnite) logic programs. Typically,
operators to compute successor programs are based on the dual notions of generalization
and specialization of programs and clauses. There exist diﬀerent but related notions of
generality. The basic one is the following.
Deﬁnition 3.9. A program Π is more general than a program Φ if Π |= Φ. Φ is said to
be more speciﬁc than Π.
52

3.3. Inductive Logic Programming
This structure of the problem space provides a way to prune the search tree. If a
program is not consistent then all generalizations are also not consistent and therefore
need not be considered.
This dually holds for non-completeness and specializations.
Hence, if a program is not complete, it must be generalized. If it is not consistent, it
must be specialized. This leads to Algorithm 2. Most ILP systems are instances of this
generic algorithm.
Algorithm 2: A generic ILP algorithm
Input: Positive and negative examples E+, E−
Input: Background knowledge B
Output: A deﬁnite program H such that H ∪B is correct with respect to E+
and E−
Start with some initial (possibly empty) hypothesis H; repeat
1
if H ∪B is not consistent then specialize H
2
if H ∪B is not complete then generalize H
3
until H ∪B is correct with respect to E+ and E−
4
return H
5
A commonly used instance is the (sequential) covering algorithm (Algorithm 3). The
individual clauses of a program are searched for independently one after the other.7
Hence, the problem space is not the program space (sets of clauses) but the clause space
(single clauses). This leads to a more eﬃcient search. However, independent learning
of clauses conﬂicts with intensional coverage such that ILP systems instantiating the
covering algorithm normally rely on extensional coverage.
Algorithm 3: The covering algorithm (typically applying extensional coverage)
Input and Output as in Algorithm 2
1
Start with the empty hypothesis H = ∅
2
repeat
3
Add a clause C not covering any e ∈E−to H
4
Remove all e ∈E+ covered by C from E+
5
until E+ = ∅
6
return H
7
Entailment (|=) as well as θ-subsumption (see Section 3.3.2 below) are quasi-orders
on sets of deﬁnite programs and clauses, respectively.
We associate “more general”
with “greater”. The operators carrying out specialization and generalization are called
reﬁnement operators.
They map clauses to sets of (reﬁned) clauses or programs to
sets of (reﬁned) programs. Most ILP systems explore the problem space mainly in one
7The procedure that computes the single clauses (line 4 in Algorithm 3) is usually called
LEARN−ONE−RULE.
53

3. Approaches to Inductive Program Synthesis
direction, either from general to speciﬁc (top-down) or the other way round (bottom-up).
The three well-known ILP systems Foil [86] (top-down), Golem [72] (bottom-up), and
Progol [77] (mixed) are instances of the covering algorithm.
Example 3.11. For an example of the covering algorithm, let E+ and B be as in
Example 3.10 and E−all remaining instantiations for the “inputs” [c], [b, c], [a, b, c], e.g.,
Init([b, c], [c]). Let us assume that our concrete instance of the covering algorithm uses
the top-down approach.
That means that clauses are generated by starting with a
(too) general clause and then successively specializing it until no negative examples are
covered anymore. We start with H = ∅and construct the ﬁrst clause: Suppose we start
with the clause Init(Xs, Ys) ←. Due to non-compound patterns (simply variables) in
the head and an empty body, it (extensionally) covers all positive and also all negative
examples and hence, is too general. We may specialize it by applying the substitution
{Xs ←[X]}. The resulting clause Init([X], Ys) ←excludes all positive and negative
examples from being covered except for those with input [c]. We may further specialize
it by applying the substitution {Ys ←[ ]}, thereby excluding all remaining positive and
negative examples from being covered except for the single positive example Init([c], [ ]).
Since no negative example is covered by this clause, it is added to H and Init([c], [ ]) is
removed from E+. Since there are still uncovered positive examples, a second clause is
to be constructed: We start again with Init(Xs, Ys) ←which is too general. Now we
specialize it by substitution {Xs ←[X | Xs]} yielding Init([X | Xs], Ys) ←. This clause
still covers all remaining positive examples (those with inputs [b, c] and [a, b, c]), but also
all negative examples with these inputs. Applying the substitution {Ys ←[X | Ys]}
specializes it to Init([X | Xs], [X | Ys]) ←. This excludes some (but not all) negative
examples (e.g., Init([b, c], [c])). We specialize it by adding the literal Init(Xs, Ys) to the
body and get Init([X | Xs], [X | Ys]) ←Init(Xs, Ys). All remaining positive examples
are still covered but no negative one is covered anymore. Hence, the clause is added and
the remaining positive examples are removed from E+, such that E+ = ∅now. Hence,
the two induced clauses are returned as solution.
All specializations in the example were reﬁnements under θ-subsumption (see the
subsection on θ-subsumption in Section 3.3.2 below). Of course, many other candidate
specializations would have been generated and evaluated by a real top-down covering
ILP algorithm like Foil, such that constructing each clause becomes, indeed, a search
in clause space, rather than a straight-forward construction as in our example.
Reﬁnement operators in general.
Reﬁnement operators compute generalizations or
specializations under particular generality models (see Section 3.3.2) of programs or
single clauses. In particular, a specialization (downward reﬁnement) operator maps a
clause or program to a set of more speciﬁc clauses or programs. A generalization (up-
ward reﬁnement) operator maps a clause or program to a set of more general clauses or
programs. Before we deﬁne reﬁnement operators for particular objects (clauses or pro-
grams) and orderings (θ-subsumption or entailment), we introduce them in an abstract
way as functions on quasi-ordered sets:
54

3.3. Inductive Logic Programming
Deﬁnition 3.10. Let ⟨G, ≤⟩be a quasi-ordered set. A specialization operator for ⟨G, ≤⟩
is a function ρ with ρ(C) ⊆{D ∈G | C ≥D} for every C ∈G.
A generalization operator for ⟨G, ≤⟩is a function δ with δ(C) ⊆{D ∈G | D ≥C} for
every C ∈G.
In order to achieve a computable and rather complete and eﬃcient search through the
ordered set, Nienhuys-Cheng and De Wolf [81] deﬁne and study the following properties
of reﬁnement operators, exemplarily deﬁned for specialization operators. By ρ∗(C) we
denote the set of all elements that can be generated by a ﬁnite number of applications
of ρ.
Deﬁnition 3.11. Let ⟨G, ≤⟩be a quasi-ordered set and ρ a specialization operator for
it.
• ρ is locally ﬁnite if for every C ∈G, ρ(C) is ﬁnite and computable.
• ρ is complete if for every two clauses C, D ∈G with C ≥D there is an E ∈ρ∗(C)
with D ≡E.
• ρ is proper if for every C ∈G, ρ(C) ⊆{D | C > D}.
• ρ is ideal if it is locally ﬁnite, complete, and proper.
Analogous deﬁnitions can be stated for the dual case of generalization operators.
Local ﬁniteness is required to consider each generated program in ﬁnite time. Com-
pleteness means that any specialization can actually be computed by ρ in ﬁnite time. If
ρ is not complete, then some specializations cannot be found by a ﬁnite number of com-
putations. Properness prevents the computation of equivalences and is thus desirable
for eﬃciency reasons.
Muggleton and De Raedt [78] deﬁne reﬁnement operators to compute minimal gener-
alizations and maximal specializations only and deﬁne diﬀerent properties; informally:
ρ is
• globally complete, if all elements of the ordered set can be generated by repeated
applications of ρ starting from the top element (if it exists),
• locally complete, if for all elements C in the ordered set, an application of ρ yields
all maximal specializations of C, and
• optimal, if only one path exists between two elements by repeatedly applying ρ.
Hence, ρ spans a tree.
Analogously for generalization operators.
55

3. Approaches to Inductive Program Synthesis
3.3.2. Generality Models and Reﬁnement Operators
Instead of entailment (|=), θ-subsumption is often used as generality model. It is incom-
plete with respect to entailment—C |= D if C θ-subsumes D but not vice versa—but
decidable, simple to implement, and eﬃciently computable.
If we have background knowledge B, then we are not simply interested in whether a
clause C (or a program Π) is more general than a clause D (or a program Π′) but in
whether C (Π) together with B is more general than D (Π′). This is captured by the
notions of relative (to background knowledge) entailment and θ-subsumption. Relative
entailment is deﬁned as:
Deﬁnition 3.12. A program Π is more general than a program Φ with respect to
entailment relative to background knowledge B, written Π |=B Φ, if
Π ∪B |= Φ.
Generalization and Specialization under (Relative) θ-subsumption
Deﬁnition 3.13 ((Relative) θ-subsumption). Let C and D be clauses and B a set of
clauses.
C θ-subsumes D, written C ⪰D, iﬀthere exists a substitution θ such that Cθ ⊆D.8
C θ-subsumes D relative to B, written C ⪰B D, if B |= Cθ →D for a substitution θ.
The deﬁnition of relative subsumption appears reasonable as an extension of ordinary
θ-subsumption if one observes that Cθ ⊆D, i.e., ordinary θ-subsumption, implies that
Cθ →D is a tautology, |= Cθ →D. Now having background knowledge B, we do not
require Cθ →D to hold unconditionally but only if B is given.
It follows that if C ⪰D then also C ⪰B D. The converse, of course, need not hold. For
empty background knowledge B and non-tautologous clauses C and D we have C ⪰D
if and only if C ⪰B D, i.e., θ-subsumption and relative subsumption coincide in this
special case.
Least General Generalizations.
A Horn clause language quasi-ordered by θ-subsumption
with an additional bottom element is a lattice. This does not generally hold for relative
subsumption. Least upper bounds are called least general generalizations (LGG) [85].
LGGs and greatest lower bounds are computable and hence may be used for gener-
alization and specialization, though they do not properly ﬁt into our general notion of
reﬁnement operators because they neither map single clauses to sets of clauses nor single
programs to sets of programs.
A useful restriction is to let background knowledge be a ﬁnite set of ground literals. In
this case, LGGs exist also under relative subsumption, called relative LGGs (RLGGs),
8Confusingly, both notations, C ⪰D and C ⪯D, are used in the literature for writing that C
θ-subsumes D. The latter one comes from interpreting a subset as smaller than its superset. However,
the concept to be denoted here is logical generality (and not the subset relation, even if θ-subsumption
is deﬁned based on this relation). Hence we prefer the ﬁrst notation, which correctly expresses the
generality order.
56

3.3. Inductive Logic Programming
and relative subsumption and RLGGs can be reduced to ordinary θ-subsumption and
LGGs:
Proposition 3.1. Let C and D be non-tautologous clauses and B be a ﬁnite set of
ground literals such that B ∩D = ∅. Then
• C ⪰B D iﬀC ⪰D ∪¯B and
• lggB({C1, . . . , Cn}) = lgg({C1 ∪¯B, . . . , Cn ∪¯B}.
The bottom-up system Golem [72] computes RLGGs under this setting.
Reﬁnement operators.
In general, ideal reﬁnement operators do not exist for Horn
clause languages ordered by (relative) subsumption. It suﬃces to drop the properness
property, though.
Locally ﬁnite and complete reﬁnement operators for Horn clause
languages exist. For ﬁnite Horn clause languages, ideal reﬁnement operators do exist.
A specialization operator reﬁnes a clause by
• applying a substitution for a (single) variable or
• adding a (most general) literal.
Dually, a generalization operator reﬁnes a clause by
• applying an inverse substitution or
• removing a literal.
Application of such reﬁnement operators is quite common in ILP, e.g., in the well-
known systems MIS [99], Foil, Golem, and Progol. A specialization search-operator
on clauses with respect to θ-subsumption as described above was used in the MIS system
for the ﬁrst time.
Incompleteness of θ-subsumption.
(Relative) θ-subsumption is sound but not com-
plete with respect to (relative) entailment; In general, (relative) θ-subsumption is sound
but not complete. If C ⪰D (C ⪰B D) then C |= D (C ∪B |= D) but not vice versa. For
a counter-example of completeness let C = P(f(X)) ←P(X) and D = P(f(f(X))) ←
P(X) then C |= D9 but C ̸⪰D. As the example indicates, the incompleteness is due to
recursive rules and therefore especially critical for program synthesis.
To see this in general, consider the following theorem that shows completeness (except
for tautologies) of resolution in combination with θ-subsumption:10
9D is simply the result of self-resolving C.
10In introductory texts on (clausal) logic, most often refutation completeness (Proposition 2.3) is shown
for resolution. The subsumption theorem and refutation completeness are equivalent in that they
can mutually be proven from each other [80].
57

3. Approaches to Inductive Program Synthesis
Theorem 3.3 (Subsumption Theorem, due to [80]). Let T be a set of clauses and D be
a clause. T |= D iﬀD is a tautology or there exists a clause E such that T ⊢r E and
E ⪰D.
From this theorem follows the following corollary relating resolution and θ-subsumption
to entailment relative to background knowledge between single clauses.
Corollary 3.1. Let C and D be clauses and B be a set of clauses. Then C |=B D iﬀD
is a tautology or there exists a clause E such that {C} ∪B ⊢r E and E ⪰D.
Relative subsumption (Deﬁnition 3.13) can equivalently be expressed in terms of res-
olution and ordinary θ-subsumption:
Deﬁnition 3.14 (Alternative deﬁnition of relative subsumption (equivalent to that in
Deﬁnition 3.13). Let C and D be clauses and B a set of clauses. Then C ⪰B D iﬀD
is a tautology or there exists a clause E such that {C} ∪B ⊢r E, where C must not be
used more than once in the derivation of E, and E ⪰D.
From this deﬁnition and the corollary above follows that the incompleteness of (rela-
tive) θ-subsumption is due to recursive clauses.
Incompleteness of θ-subsumption does not mean that recursive clauses cannot be gen-
erated by θ-subsumption reﬁnement operators as described above. Actually—since the
empty clause □θ-subsumes every other clause and and a complete specialization opera-
tor ρ under θ-subsumption exists—for each clause a logical equivalent can be computed
by a ﬁnite number of applications of ρ starting with □.
However, to generate a recursive clause, it is necessary to start from a suﬃciently
general clause. Therefore, incompleteness is an issue especially for local search.
Inverse Resolution.
Within their early ILP system Cigol, Muggleton and Buntine [75]
introduced the concept of inverting resolution as a generalization technique. Inverse
resolution operators are highly non-deterministic in general and thus ineﬃcient.
In
his seminal paper on ILP [74], Muggleton introduces most speciﬁc inverse resolution
operators and shows an equivalence to RLGGs.
Specialization and Generalization under (Relative) Entailment
Due to the incompleteness of (relative) θ-subsumption regarding recursive clauses, re-
ﬁnement under (relative) entailment (Deﬁnitions 3.9 and 3.12) has been studied.
Neither LGGs nor greatest specializations nor optimal reﬁnement operators exist in
general for Horn clause languages ordered by (relative) entailment.
Due to the subsumption theorem (Theorem 3.3), implication is equivalent to resolution
followed by θ-subsumption. Hence a ﬁrst idea to achieve a locally ﬁnite and complete
specialization operator ρI for single clauses C under implication might be to simply
add all self-resolvents of C to ρS.
But this approach does not result in a complete
specialization operator since in a deduction of D from C, C possibly needs to be used
in several resolution steps together with another clause E derived in an earlier step. Yet
58

3.3. Inductive Logic Programming
our naive ρI can only resolve clauses with itself because it works on single clauses only.
The solution is to construct a specialization operator mapping deﬁnite programs (sets of
clauses) to sets of deﬁnite programs instead of single clauses to sets of clauses. Hence,
we now consider deﬁnite programs ordered by (relative) implication (instead of single
clauses ordered by θ-subsumption).
Locally ﬁnite and complete reﬁnement operators exist for this setting and can be
derived from the subsumption theorem.
A locally ﬁnite and complete specialization operator under entailment reﬁnes a deﬁnite
program Π by
• adding a resolvent of two clauses in Π to Π or
• building the union of Π with ρS(C) for a clause C ∈Π or
• deleting a clause from Π.
Note that a result of applying ρI as deﬁned in the ﬁrst two items is equivalent to Π
under implication because some clauses which logically follow from Π are simply added
to Π. (Hence ρI is improper.) An actual specialization is carried out if ρI is applied
as deﬁned in the last item and if the deleted clause does not follow from the remaining
clauses.
3.3.3. General Purpose ILP Systems
Three of the best known ILP systems are Foil, Golem, and Progol. We may call
them general purpose systems because they are not speciﬁcally designed to learn recur-
sive programs but are in principle able to learn recursive programs. Especially Foil
has features especially addressing recursion [17] and has been systematically tested on
standard recursive logic programs on lists [86].
Foil instantiates the covering algorithm (Algorithm 3) where extensional coverage is
applied. Clauses are generated top-down (from general to speciﬁc) starting with the
clause p(X1, . . . , Xn) ←, if p is to be induced. Foil uses a θ-subsumption specialization
operator within a hill-climbing search. In each specialization step, all possibilities of
adding a literal to a clause are enumerated and the best-rated clause is chosen for
further specialization. Clauses are rated with respect to the foilgain, a statistical measure
based on information theory.
Basically, the foilgain tries to increase the fraction of
covered positive examples. A variant of Foil, FFoil, is speciﬁcally designed for learning
functional relations.
Golem also instantiates the covering algorithm but in contrast to Foil, its search
for clauses is bottom-up (from speciﬁc to general). First, the LGG under relative sub-
sumption of a subset of positive examples is computed. The subset is determined by
starting with pairs of randomly chosen examples and then greedily growing the subset.
In each step, the subset whose LGG covers the most positive and no negative examples
is chosen. The process stops if no increasing coverage of positive examples is achieved
by a greater subset. It is taken care for consistency of computed clauses. In order to
59

3. Approaches to Inductive Program Synthesis
eﬃciently compute ﬁnite LGGs, only particular restricted forms of deﬁnite clauses can
be induced.
Progol searches top-down through a θ-subsumption (sub)lattice of clauses.
The
sublattice is bounded from below by a most speciﬁc clause that is, bottom-up, derived
from one example by inverting implication. Restricting the search to a sublattice reduces
search eﬀort.
Inverse implication is made tractable by mode and type declarations
provided by the user.
A good overview of the kind of recursive programs that can be learned by Foil is
given in Foil’s midterm report [86].
In that paper, Foil is tested on most of the
list-processing predicates such as member, reverse, shift , etc., deﬁned Chapter 3 of
Bratko’s textbook Prolog Programming for Artiﬁcial Intelligence [16]. Foil can learn
most of the predicates from example sets that are complete up to lists of three or four
elements. Other examples of recursive functions that can be learned are the Ackermann
function and the quicksort algorithm, if the partitioning and appending functions are
provided as background knowledge. Golem also can learn quicksort for example with
the appropriate background knowledge.
However, the strength of these systems lie in learning non-recursive concepts from large
data sets, and synthesis of recursive programs is problematic due to some systematic
reasons: Due to their use of the (extensional) covering algorithm, they need complete
example sets and background knowledge to induce recursive programs. Since they (at
least Foil and Golem) explore (i) only the θ-subsumption lattice of clauses and (ii)
do this greedily, correct clauses are likely to be passed. Furthermore, their objective
functions in the search for clauses is to cover as many as possible positive examples.
Yet base clauses typically cover only few examples such that these systems often fail to
induce correct base cases.
Statistical measures like the foilgain require comparatively large data sets in order
to sensibly guide the search. In program synthesis, however, the speciﬁcation often is
written by a human person such that the ability to learn from few examples is crucial.
Finally, none of the three systems described is able to do predicate invention, i.e., to
introduce non-speciﬁed predicates. This might be of minor importance for non-recursive
programs, since each non-recursive (sub)predicate can be eliminated by unfolding it
into the body of the clause where it is called. But for recursive programs, invention of
subfunctions/subpredicates is an important feature (cp. Section 3.1.3).
3.3.4. Program Synthesis Systems
As a consequence of the problems regarding recursion described above, ILP systems
especially designed to learn recursive programs have been developed.
They address
diﬀerent issues: Handling of sparse and random examples, predicate invention, usage of
general programming knowledge, and usage of problem-dependent knowledge of the user,
which goes beyond examples. A comprehensive overview and comparison of (general
purpose as well as program synthesis) ILP systems particularly addressing program
synthesis capabilities can be found in [30].
60

3.3. Inductive Logic Programming
Inverting Implication by Structural Analysis
Several systems—Lopster [60], Crustacean [1], Clam [89], Tim [39], mri [31]—
address the issue of inducing recursive programs from random or sparse examples by
inverting entailment based on structural analysis, similar to Section 3.2, instead of search-
ing in the θ-subsumption lattice. These systems also have similar restrictions regarding
the general schema of learnable programs. For example, Crustacean generates pro-
grams belonging to the following schema, containing one base clause and one purely
linear-recursive clause:
p(a1, . . . , an)
p(b1, . . . , bn) ←p(c1, . . . , cn)
The ai, bi, ci are terms, each ci is a subterm of bi. No background knowledge can be used
and no auxiliary predicates are invented.
However, some of the systems can use background knowledge; mri can ﬁnd more than
one recursive clause.
Even though these systems are in principle able to learn recursive programs from
incomplete example sets, they are more sensible to the incompleteness than enumerative
systems.
Top-down Induction of Recursive Programs
Top-down systems can principally—even though they explore the θ-subsumption clause-
lattice only—generate any (in particular any recursive) Horn clause from the empty
clause. This follows from the existence of a complete θ-subsumption top-down oper-
ator (cp. Section 3.3.2).11 Thus, if a top-down covering system would use intensional
instead of extensional coverage, it could principally induce recursive programs from ran-
dom examples. Certainly, this would require to ﬁnd clauses in a particular order—base
clauses ﬁrst, then recursive clauses, only depending on base clauses and themselves,
then recursive clauses, only depending on base clauses, the previously generated recur-
sive clauses, and themselves, and so on. This excludes programs with mutually interde-
pending clauses. The system Smart [70] is based on these ideas. It induces programs
consisting of one base clause and one recursive clause. Several techniques to sensibly
prune the search space allows for a more exhaustive search than the greedy search applied
by Foil, such that the incompleteness issue of θ-subsumption-based search is weaken.
The system Filp [9, 10] is a covering top-down system that induces functional pred-
icates only, i.e., predicates with distinguished input- and output parameters, such that
for each binding of the input parameters exactly one binding of the output parameters
exists. This makes negative examples unnecessary (cp. the paragraph on positive and
11 Hence, although θ-subsumption is incomplete with respect to entailment due to recursive clauses, every
clause, in particular the recursive clauses, can be generated by reﬁnement based on θ-subsumption—
if one searches top-down starting from the empty clause or some other clause general enough to
θ-subsume the desired clauses.
61

3. Approaches to Inductive Program Synthesis
negative examples at the beginning of Section 3.3.1). Filp can induce multiple inter-
dependent predicates/functions where each may consist of several base- and recursive
clauses. Hence, intensional coverage is not assured to work. Filp starts with a few ran-
domly chosen examples and tries to use intensional covering as far as possible. If, during
the intensional proof of some example, an instance of the input parameters of some
predicate appears for which an output is neither given by an example nor can be derived
intensionally, then Filp queries for this “missing” example and thereby completes the
example set as far as needed.
A third top-down system especially addressing recursive programs is Atre [63]. Atre
deals with the problem of interdependent (mutually) recursive clauses by searching for
several clauses in parallel, applies a more general generalization model than θ-subsumption,
and keeps track of the possible non-consistence union of independently consistent clauses.
Using Programming Knowledge
Flener argued for the use of program schemas that capture general program design
knowledge like divide-and-conquer, generate-and-test, global-search etc. [26, 27, 28, 29],
and has implemented this in several systems. He distinguishes between schema-based
systems inducing programs of a system-inherent schema only and schema-guided systems,
which take schemas as dynamic, problem-dependent, additional input and thus are more
ﬂexible. Flener’s Dialogs [27] system uses schemas and strong queries, such as how the
input is to be decomposed for recursive calls, to restrict the search space and thereby is
able to eﬃciently induce comparatively complex algorithms, e.g., mergesort, including
predicate invention, e.g., invention of the merge function.
Jorge and Brazdil [40, 41] have—besides for clause structure grammars deﬁning a pro-
gram class and thus similar to schemas as dynamic language-bias—argued for so called
algorithm sketches.
An algorithm sketch is problem-dependent algorithm knowledge
about the target function and provided by the user in addition to examples. This idea
is implemented in their SKIL system. SKIL alone is not able to induce recursive pro-
grams from random examples. However, some non-recursive generalization takes place
in this case, yielding non-ground, non-recursive clauses, assumed to be true properties
of the target function. SKIL is then called again and again with properties induced in
one iteration as additional input for the next iteration, thus iteratively called on greater
and greater (with respect to generality) speciﬁcations such that the possibility of ﬁnding
a recursive solution increases. Jorge and Brazdil call this technique iterative bootstrap
induction and have implement it in their SKILit system. Though SKILit is able to in-
duce some recursive programs such as Member from random examples, it fails for other,
very simple recursive programs in this case, e.g., Last, because no suﬃciently general
properties are ever induced.
3.3.5. Learnability Results
Two complementary papers from Cohen [19, 20] address the learnability and non-
learnability of certain (simple) classes of recursive logic programs within a variant of
62

3.3. Inductive Logic Programming
the PAC-learning framework [107, 3, 4].
Theoretical results regarding the necessity and its decidability of predicate invention
are reported in [102].
3.3.6. Discussion
Compared to the classical analytic functional approach as described in Section3.2, ILP
has broadened the class of inducible functions/relations by allowing for background
knowledge and by searching in program spaces. At the core of ILP is the structuring
of the program space by generality models such as entailment or θ-subsumption. It is
not only a means to prune the search but also to theoretically analyze properties like
(in-)completeness of search operators.
The sequential covering approach (generating one clause after the other) applied to
the clause space structured by θ-subsumption leads to an eﬃcient search for programs.
However, this gain of eﬃciency comes at the price of diﬃculties with inducing recursive
(interdependent) clauses due to (i) extensional coverage and the subsequent need for
examples and background knowledge that are complete up to some complexity and (ii)
the incompleteness of θ-subsumption.
Shapiro [99] and Muggleton and De Raedt [78] argued for clausal logic as universal
language in favor to other universal formalisms such as Turing machines or Lisp. Their
arguments are: (i) Syntax and semantics are closely and in a natural way related. Hence
if a logic program makes errors, it is possible to identify the erroneous clause.
Fur-
thermore, there are simple and eﬃcient operations to manipulate a logic program with
predictable semantic eﬀects (cp. Section 3.3.2; however, the simple and eﬃcient manip-
ulation does only hold for operators based on θ-subsumption but not for those based on
entailment). Both is not possible for, say, Turing machines. (ii) It suﬃces to focus on
the logic of the program, control is left to the interpreter. In particular, logic programs
(and clauses) are sets of clauses (and literals), order does not matter.
The ﬁrst argument carries over to other declarative formalisms such as equational
logic, term rewriting, and functional logic programming (Flip [25] is an IPS system
in this formalism). The second argument also carries over to some extent, declarative
programming all in all shifts the focus oﬀcontrol and to logic. Yet in this generality
it only holds for non-recursive programs or ideal, non-practical, interpreters. For the
eﬃcient interpretation of recursive programs however, order of clauses in a program and
order of literals in a clause matters. Hence we think that declarative, (clausal- and/or
equational-)logic-based formalisms are principally equally well suited for IPS.
Logic programs represent general relations. (Partial) functions are special relations—
their domains are distinguished into source and target (or: a functional relation has
input and output parameters) and they are single-valued (each instantiation of the in-
put parameters implies a unique instantiation of the output parameters). Regarding
functional- and logic programming, there is another diﬀerence: Functional programs are
typically typed, i.e., their domain is partitioned and inputs and outputs of each function
must belong to speciﬁed subsets, whereas logic programs are typically untyped. Inter-
estingly, all three “restrictions” of functions compared to relations have been shown to
63

3. Approaches to Inductive Program Synthesis
be advantageous from a learnable point of view in ILP. The general reason is that they
restrict the problem space such that search becomes more eﬃcient and fewer examples
are needed to describe the intended function. In particular, no negative examples are
needed since they are implicitly given by the positive ones (cp. “On positive and negative
examples” at the beginning of Section 3.3.1).
ILP is built around the natural generality structure of the problem space. Regarding
functional relations, we observe an “oddity” of this structure. For deﬁnite programs,
“more general”, with respect to the minimal Herbrand model, means “more atoms”. If
the relation is a function, an additional ground atom must have a diﬀerent instantiation
of the input parameters compared to all other included atoms. Thus, “more general”
in the case of deﬁnite programs representing functions reduces to “greater domain”.
In other words: All functions with the same domain are incomparable with respect to
generality.
Since most often one is interested in total functions, generality actually
provides no structure at all of the space of possible solutions.
3.4. Generate-and-Test Based Approaches to Inductive
Functional Programming
The approaches reviewed in this section have in common, that they search through
a class of functional programs by repeatedly generating and testing programs. That
is, the generation of programs is independent from the incomplete speciﬁcation or the
I/O examples and the latter is only used as acceptance criterion or to deﬁne a ﬁtness-
or objective function.
3.4.1. Program Evolution
Evolutionary algorithms are inspired by biological evolution. They maintain populations
of candidate solutions called individuals, get new ones by stochastical methods like repro-
duction, mutation, recombination/crossover, and selection, and thereby try to increase
the ﬁtness of individuals. “Fitness” here simply is a measure of how “good” a candidate
solution is, e.g., which fraction of I/O examples a candidate program correctly computes.
Evolutionary algorithms have advantages when the problem space is too broad to
conduct an exhaustive search and simultaneously nothing or few is known about the
ﬁtness landscape, hence when it is hard to construct sensible heuristics. The randomness
of the search care for a widespread exploration of the problem space which is guided by
the ﬁtness measure.
On the other side, this randomized and “chaotic” search in a space with unknown
properties makes it diﬃcult to give guaranties regarding solutions and typically leads to
non-optimal but only approximated solutions. Due to the randomness, solving a problem
by evolutionary algorithms consists of several independent runs. The ﬁttest, i.e., best,
individual out of the ﬁnal populations of all runs is designated to be the solution.
64

3.4. Generate-and-Test Based Approaches to Inductive Functional Programming
Genetic Programming
The subﬁeld of evolutionary algorithms where the evolved objects are computer programs
(in contrast to strings or other simple objects) is genetic programming (GP) [58].
Programs are typically represented as parse trees where inner nodes represent opera-
tors (primitive non-constant functions, e.g., arithmetic or boolean operations) and leafs
represent operands (variables and constants). Operators and operands allowed to be
used by the program to be evolved are given in so-called function and terminal sets,
respectively. Classically, these two sets must be closed which means that all functions
and terminals are of the same type such that they can be composed without restrictions
in evolved programs. Strongly typed genetic programming (STGP) has been introduced
by Montana [71] to overcome this restriction to closed function and terminal sets and to
restrict the search space by types associated to operators and operands.
A GP problem is speciﬁed by ﬁtness cases (e.g., example inputs of the target function),
a ﬁtness function, and the function and terminal sets. The ﬁtness function assigns a
ﬁtness value to each individual program based on evaluating it on the ﬁtness cases. E.g.,
the ﬁtness function could compute the fraction of the sample inputs for which a correct
output has been computed. There are no predeﬁned acceptance criteria or preference
biases in GP systems. The search is completely guided by the ﬁtness function that is
to be optimized. If, for example, small solutions shall be preferred than the size of an
individual program must explicitly be considered by the ﬁtness function. Additionally,
several parameters such as the size of the population, the probabilities of performing
the diﬀerent genetic operations, maximum size or depth of individual programs, and the
number of runs are to be speciﬁed.
Data structures and recursion (or iteration) do not play a predominant role in GP.
A typical evolved program is a non-ground arithmetic expression or a propositional
formula. However, some attempts have been made to integrate recursion or iteration
into GP. In the following we consider some non-problem dependent techniques to evolve
recursive programs by GP methods.
Koza and his colleagues [59] and Wong and Mun [109] integrated explicit recursion
into GP by naming evolved functions and calling them within the function body.12
One of the major issues is the handling of non-terminating programs. As a generate-
and-test approach, GP relies on testing evolved candidate programs against the given
examples. If non-termination may appear then a runtime limit is applied. This raises
two problems if non-terminating programs are frequently generated: (i) The diﬃculty of
assigning a ﬁtness value to an aborted program and (ii) the runtime uselessly consumed
by evaluating non-terminating programs. Wong and Mun [109] deal with this problem
by a meta-learning approach to decrease the possibility of evolving non-terminating
programs.
Others try to avoid non-termination completely:
12This approach—naming and recursively calling generated functions (or relations)—of explicit recur-
sion corresponds to the other two approaches (Sections 3.2 and 3.3) to inductive program synthesis
described so far in this chapter.
65

3. Approaches to Inductive Program Synthesis
In her strongly typed GP system PolyGP [110, 111] (which uses Haskell as im-
plementation and object language), Yu integrates implicit recursion through the use
of user-provided higher-order functions. This kind of implicit recursion by using prede-
ﬁned higher-order functions like map, ﬁlter, and reduce, that capture particular recursive
schemes, is routinely used in functional programming. If PolyGP introduces a higher-
order function somewhere in the program tree, this forces a function as parameter for
the higher-order function to be evolved. These parameter functions are implemented in
the form of anonymous lambda abstractions by PolyGP. PolyGP has been tested for
general EvenParity [110] and Fibonnacci and STRSTR [111]. STRSTR is a function
from the C library and scans for the ﬁrst appearance of a string in another string. The
tests show a current drawback of the system: The user-provided higher-order functions
need to ﬁt the particular problem to be solved. This means that the user must actually
know the recursive structure of the problem in advance.
Kahrs [44] evolves primitive recursive functions over the natural numbers. Binard
and Felty [14] evolve programs in System F, a typed lambda calculus where only total
recursive functions are expressible. The primitive recursive functions are contained in
System F as proper subclass. Their system succeeds to induce Sum, Prod, and Inc
with Add and Mult as background knowledge.
ADATE: Automatic Design of Algorithms Through Evolution
The ADATE system [82] is an evolutionary system in that it maintains a population
of programs and performs a greedy search guided by a ﬁtness function. Yet unlike GP,
it is especially designed to evolve recursive functional programs in a (ﬁrst-order) subset
of Standard ML. To this end, ADATE applies sophisticated program transformation
operators, a sophisticated search strategy, and predeﬁned program evaluation functions,
instead of the standard evolutionary GP operators, unsystematic search, and only user-
deﬁned ﬁtness functions, respectively.
A speciﬁcation consists of datatype deﬁnitions, predeﬁned functions which can be used
in the function to be induced, the type declaration of the target function, and a set of
sample inputs for the target function. Furthermore, the user has to provide an output
evaluation function to rate individual programs. This function takes a set of I/O pairs
where the inputs are the given sample inputs and the outputs are those computed by an
individual program to be rated. Hence testing an evolved program involves running it
on the sample inputs and applying the output evaluation function to the result. In the
easiest case, the output evaluation function includes outputs for all sample inputs and
simply test whether the evolved program correctly computes these speciﬁed outputs.
In this case, the evaluation function actually is a set of I/O examples.
However, it
can be more general. Generally, the output evaluation function must decide for every
pair of a user-provided input and an output computed from it by an evolved program,
whether the computed output is correct, incorrect, or whether this cannot be decided
(“don’t know”). As an example (taken from [82]), the mergesort algorithm is based on a
function splitting a list into two lists whose lengths diﬀer by at most one. Which elements
go into which of the two halves is not important. Thus, given a sample input [1, 2, 3, 4],
66

3.4. Generate-and-Test Based Approaches to Inductive Functional Programming
both ⟨[1, 2], [3, 4]⟩and ⟨[1, 3], [2, 4]⟩and several solutions are equally good outputs. Such
a non-deterministic speciﬁcation of a function is not possible with I/O examples where
each input is assigned a unique output, yet it can be stated by an output evaluation
function for ADATE.
Individual programs are rated according to the user-provided output evaluation func-
tion and to their syntactical and computational complexity. In addition to the correct-
ness decision, the output evaluation function may assign a list of grades (real numbers)
which are to be minimized, to I/O pairs. This allows the user to specify any number
of problem-dependent preference biases in addition to the biases correct outputs, low
syntactical complexity, and low computational complexity.
New programs are created by applying transformations to existing programs. There
are four atomic transformations:
Replacement. Replaces a subexpression Sub of a program by a new synthesized ex-
pression Syn. Parts of Sub may occur in Syn. Replacement is the only atomic
transformation which changes the semantics of a program. Expression synthesis is
heuristical and not exhaustive.
Abstraction. Introduces auxiliary subfunctions by choosing a subexpression Sub in a
program P, replacing some disjoint subexpressions S1, . . . , Sn of Sub by variables
V1, . . . , Vn, taking the resulting generalized subexpression as body of a new sub-
function g(V1, . . . , Vn) of the introduced variables, and replacing the chosen subex-
pression Sub by g(S1, . . . , Sn) in P.
Case-distribution. Chooses a function with one actual parameter being a case-expression
and transforms it to an equivalent case-expression with adapted function calls as
subexpressions. This transformation can also be applied vice versa.
Embedding. Changes the type of an (auxiliary) function.
Atomic transformations are not applied directly but only within a compound transfor-
mation which encapsulates a chain of atomic transformations.
The initial population consists of only one (empty or undeﬁned) program. The in-
cremental search produces successively populations containing bigger-and-bigger and
better-and-better programs.
Even if the search is far more systematic than general GP, it is greedy and therefore it is
unlikely to ﬁnd the best program with respect to the combined program evaluation func-
tions. And even if the search is greedy and applies highly specialized transformations, it
consumes far more runtime than the approaches described in Sections 3.2 and 3.3. The
main and general reason for this is its most unrestricted program space. ADATE is, to
the best of out knowledge, the most powerful currently existing inductive programming
system with respect to the class of programs that can in principle be induced.
At the ADATE homepage13, Olsson reports on a number of problems successfully
solved by ADATE on a 200 MHz PentiumPro. These include general EvenParity14,
13http://www-ia.hiof.no/~rolando/
14In 12 seconds; this is the simplest reported problem.
67

3. Approaches to Inductive Program Synthesis
several non-trivial list functions such as to check whether one list is a permutation of
another15, and path ﬁnding in a directed graph16. In [83] Olsson and Powers report on
learning the semantics of simple natural language sentences with ADATE.
Evolving Algebraic Speciﬁcations From Positive and Negative Facts
A system lying in the intersection of ILP, GP and algebraic speciﬁcation has been de-
veloped by Hamel and Shen [34, 35]. Their system evolves (recursive) algebraic speciﬁ-
cations, i.e., equational theories over many-sorted signatures, using GP search methods.
Yet instead of providing a ﬁtness function, a theory to be induced is, as in ILP, speciﬁed
by positive and negative facts—ground equations in this case.
Additionally a back-
ground theory, corresponding to the function set of primitive functions in GP and to a
background theory in ILP, may be provided. The ﬁtness function to be maximized is
derived from such a speciﬁcation. It sums the satisﬁed positive facts, the non-satisﬁed
negative facts and the reciprocal of the syntactical complexity of the candidate theory.
That is, candidate theories satisfying more positive facts, excluding more negative facts
and being of smaller syntactical complexity are preferred.
Candidate theories may be recursive but no mechanism to prevent non-terminating
theories from being generated are included to the system. It simply stops evaluation
after a limit of the number of evaluation steps has been exceeded.
3.4.2. Exhaustive Enumeration of Programs
Finally, two systems generating functional programs by simply enumerating a program
class, until one is found satisfying the provided speciﬁcation, have recently been pre-
sented.
They are more or less direct instances of the enumeration algorithm (Algo-
rithm 1).
MagicHaskeller
Katayama [45, 46] has developed a system, called MagicHaskeller, which essentially
simply enumerates all type-correct programs composed of some predeﬁned functions in
order from small to inﬁnitely large until one is found which satisﬁes a given constraint,
e.g., a set of (possibly only a single) I/O examples. Input to the system consists of
only the set of predeﬁned functions, called component library, and the constraint to be
fulﬁlled by a desirable program. Particularly, no parameters need to be adapted.
The library of predeﬁned functions may contain higher-order functions. Target lan-
guage is typed lambda calculus.
Recursion is implicitly provided by the predeﬁned
higher-order functions. The algorithm uses some memoization, avoids re-enumeration
of programs, and excludes some syntactically diﬀerent but semantically equivalent pro-
grams from being enumerated.
15In about 16 hours.
16With Append predeﬁned; in about nine days.
68

3.4. Generate-and-Test Based Approaches to Inductive Functional Programming
The objective of the system is, besides from being used as a programming tool, to pro-
vide a base-line for empirical evaluating other, more sophisticated inductive program-
ming approaches. Katayama points out that, given a reasonable but ﬁxed, problem-
independent component library, the search space of type-correct expressions spanned by
his exhaustive search is surprisingly small for some simple target programs.
As ﬁrst results, he shows that correct programs for Nth, Length, and Map are found
in below two seconds on a 2 GHz Pentium4. By comparison, the GP system PolyGP
needs diﬀerent higher-order functions for each of these problems, needs several runs to
ﬁnd a solution, needs additional parameters to be set, and yet consumes more time to
induce a solution.
Inductive Programming with G∀ST
Recently, Koopman and Plasmeijer used the software testing system G∀ST [54] to induce
primitive recursive functions from I/O examples [56, 55]. G∀ST can automatically, by
ﬁnding counterexamples, refute a statement ¬∃t : T.P(t) or ∀t : T.¬P(t) respectively.
Therefore it enumerates the type T from smaller to greater values and tests for each
instance t whether P(t) holds.
This mechanism, after some adaptation, is used for function induction as follows: The
class of candidate programs is deﬁned by a grammar for syntax trees which is provided
as a data type to G∀ST. Enumerating this data type means enumerating syntax trees
of candidate programs. The factorial function, for example, could be speciﬁed by
¬∃f.f(2) = 2 ∧f(4) = 24 ∧f(6) = 720.
This statement is refuted by the factorial function.
One may exclude particular generated functions from being tested by providing ad-
ditional predicates. Koopman and Plasmeijer use this possibility to exclude functions
with undesirable subexpressions like x + 0 from being considered.
The Optimal Ordered Problem Solver
Schmidhuber’s Optimal Ordered Problem Solver (OOPS) [97] is a problem solver de-
signed to be applied to sequences of increasingly hard problems of one class instead of
being applied to single problems.
The search for solutions to later problems always
includes exploiting solutions to previously solved problems in the sequence in a theoret-
ically optimal way. It is based on Universal Search [61], an asymptotically fastest way
of solving a single given task.
Instead of searching in the raw (problem) solution space, e.g., in a space of action
sequences to solve the Towers of Hanoi problem for n discs, OOPS searches in program
space to ﬁnd a program that solves the currently considered problem instance as well
as all previously solved (smaller) instances.
In the case of Towers of Hanoi, OOPS
eventually generates the recursive solution program to solve any instance (for any number
of discs).
69

3. Approaches to Inductive Program Synthesis
Whereas classical planners fail to solve Towers of Hanoi for instances exceeding a
certain number of discs (the limit is quoted with n = 15 in [97])—the reason is the
exponential growth of the minimal number of actions to solve the problem as the number
of discs increases)—OOPS, by generating the recursive solution program, is able to solve
the problem, for example, for 30 discs (if it could previously solve smaller instances).
A further feature of OOPS is that it optimizes its inductive (preference) bias during
the search.
3.4.3. Discussion
One general advantage of generate-and-test methods is their greater ﬂexibility in at least
two aspects: First regarding the problem space—there are no principle diﬃculties in
enumerating even very complex programs. Second regarding the form of speciﬁcations.
Whereas the synthesis operators of analytical techniques depend on the speciﬁcation
(e.g., I/O examples) such that diﬀerent forms of speciﬁcations need diﬀerent synthesis
techniques, the generation of programs in generate-and-test methods is independent from
the speciﬁcation such that more general and expressive forms of speciﬁcations can easily
be integrated. In particular, ﬁtness functions in GP or the objective function in ADATE
are more expressive than I/O examples since no ﬁxed outputs need to be provided but
general properties to be satisﬁed by computed outputs can be speciﬁed. Moreover, they
need not specify complete sets of example inputs up to a certain complexity as required
by the analytical techniques from Section 3.2 and ILP techniques (Section 3.3) relying
on extensional coverage.
The disadvantage of generate-and-test methods is that they generally generate far
more candidate programs until a solution is found and hence need much more time
than data-driven methods to induce programs of equal size. A further problem is non-
termination. As generated programs need to be tested against the provided speciﬁcation,
non-termination is a serious issue. Higher-order functions or formalisms that a-priori
only include total functions are helpful to circumvent this problem.
3.5. Conclusions
In the previous sections, we described several approaches and systems to the inductive
synthesis of functional and logic programs and discussed pros and cons and relations
between them.
One obvious general dimension to classify them is the way of how the speciﬁcation is
used: As basis to construct solution programs (the analytical approach, Section 3.2) or
to test and evaluate independently generated candidates (the generate-and-test based
approach, Section 3.4). In ILP (Section 3.3), both approaches (bottom-up and top-down)
are found. The analytical approach is faster because most representable programs are a
priori excluded from being generated. On the other side, since it strongly depends on
the speciﬁcation and the language bias, it is much less robust and ﬂexible regarding the
whole problem speciﬁcation including the speciﬁcation of the target function and the
70

3.5. Conclusions
inductive biases, especially the language bias.
Besides further developing both general approaches separately, we think that examin-
ing ways to combine them could be helpful to achieve a satisﬁable tradeoﬀof robustness,
ﬂexibility, expressiveness, and eﬃciency. In the context of relational machine learning,
the ILP system Progol successfully combined bottom-up and top-down construction
of logic programs. In the next chapter, we present our functional IPS algorithm Igor2
that combines analytical synthesis techniques with search in a program space.
A second dimension to classify IPS systems is on whether they generate all base- and
recursive rules or cases one by one and independently of each other, i.e., whether they
search in rule (or clause) space, or whether they generate all rules or cases in parallel,
i.e., whether they in fact search in program spaces. While most ILP systems, e.g., Foil,
Golem, Progol, are of the ﬁrst class (they are instances of the covering algorithm;
Algorithm 3), the functional generate-and-test systems, as described in Section 3.4, are
mostly of the latter class.
Generating rules independently requires extensional coverage and hence examples as
speciﬁcations that are complete up to some complexity. On the other side, generate-
and-test based systems can induce programs from random or sparse example sets.
One important topic, that certainly has not received suﬃcient attention in the con-
text of inductive program synthesis, is learning theory, including models of learning
and criteria to evaluate candidate programs. PAC-learning, the predominant learning
model in machine learning, is well-suited for restricted representation languages and
noisy data, hence approximate solutions. Yet in program synthesis, we have rich rep-
resentation languages, often assume error-free examples, and want have programs that
exactly compute an intended function or relation. Moreover, eﬃciency, not only of the
induction process, but of the induced program, becomes an important issue. Muggle-
ton’s U-learning model17 [76] captures these needs and is probably a good model or
initial point to develop learning models for inductive program synthesis.
17The ’U’ stands for ’universal’.
71


4. The Igor2 Algorithm
This chapter describes our own IPS algorithm called Igor2. It induces functional pro-
grams in terms of constructor (term rewriting) systems (CSs) from I/O examples or
I/O patterns (non-ground “I/O examples”). Relevant deﬁnitions of term rewriting and
in particular of constructor systems are given in Sections 2.2.2 and 2.2.4.
4.1. Introduction
Igor2 combines and generalizes several ideas and methods of the diﬀerent approaches
to IPS as described in the previous chapter. In particular, it is an attempt to combine
the analytical recurrence detection method invented by Summers with search in program
(or rule) spaces in order to overcome the strong restrictions—no usage of background
knowledge and strongly restricted program schemas—of the classical analytical approach
but without falling back to a generate-and-test search.
The main idea behind Igor2 is to conduct a global search in a comparatively less
restricted program space including background knowledge, complex recursion schemes,
and the automatic invention of auxiliary subfunctions in order to facilitate the reliable
induction of non-trivial programs in diﬀerent domains. For example, Igor2 is able to
induce the Ackermann function as well as a function for transposing matrices as well as
the recursive solution strategy for the Towers of Hanoi (see Chapter 5 for several ex-
periments with a prototypical implementation of Igor2). However, candidate programs
are constructed analytically from the I/O examples or I/O patterns instead of generat-
ing them independently from the speciﬁcation as in generate-and-test approaches. The
analytic construction of candidate programs has two advantages: First, often large parts
of the program space can be pruned from the search tree; and second, constructed pro-
grams need not be tested, i.e., evaluated on the example inputs, because they are correct
by construction.
Furthermore, Igor2 can be considered as a further development of Igor1 (see Sec-
tion 3.2.3) in that Igor2 picks up the insight behind Igor1 that the analytical recur-
rence detection method is not restricted to S-expressions, but can be applied to terms
over arbitrary algebraic (ﬁrst-order) signatures. As in Igor1, induced programs are
expressed as term rewriting systems. Extending Igor1 in this point, programs induced
by Igor2 are constructor systems (CSs) instead of recursive program schemes (RPSs).
CSs can be considered as an extension of RPSs; in particular, each RPS is also a CS.
The extension is with respect to the form of the LHSs: While the arguments of the
deﬁned/unknown function symbols that are the roots of the LHSs are single variables
only in the case of RPSs, they can be general constructor terms in the case of CSs.
73

4. The Igor2 Algorithm
This extension corresponds to pattern matching in function heads in modern functional
programming languages like Haskell or SML (in contrast to early functional languages
like Lisp where function heads are simply a function symbol applied to a sequence of
single variables). Patterns in LHSs/function heads facilitate two things: First, a pattern
decomposes the input such that (sub)parts of the input can be referred to by diﬀerent
variables in the RHS/function body. Second, it constitutes a condition. The respec-
tive rule is only applied if the input matches with the pattern. Pattern matching is
the standard form of conditional evaluation in programs induced by Igor2 at the mo-
ment. However, as a second form of conditional evaluation, conditional rules, can also
be synthesized by Igor2.
As a consequence, I/O examples or I/O patterns themselves are (syntactically correct)
programs. For example, the I/O pattern
f (x : y : z : [ ]) →z
is a syntactically correct rule of a CS, yet neither of an RPS nor of a Lisp program.
In fact, the ﬁrst synthesis step in the classical analytical approach was to rewrite the
I/O examples to a syntactically correct Lisp program—the ﬁrst approximation of the
target function—by ﬁrst deriving a fragment for each pair, e.g.
f (x) →caddr (x)
for the I/O pair above. In contrast, by using CSs as object language, the I/O examples
themselves form a correct program. The ﬁrst synthesis step is omitted and the detection
of recurrences is directly applied to the I/O examples.
A further extension of Igor2 is that signatures are sorted, i.e., that the terms and
functions induced by Igor2 are typed.
In particular, there is no restriction to any
predeﬁned types or data structures for the programs induced by Igor2. Instead, the
user may specify its own algebraic (ﬁrst-order) data types without any restrictions.
Both, the use of CSs to represent programs and pattern matching in LHSs as well as
using typed signatures, let programs induced by Igor2 look much more like programs
written in modern functional languages. Another advantage of typing is that this further
prunes the search by a priori ruling out many (type-incorrect) expressions. The most
essential diﬀerence to usual functional programming languages is the current lack of
higher-order functions in programs induced by Igor2.
Besides the restriction to small sets of primitives and the restricted program schemas, a
third serious drawback of the purely analytical approach was the need for sets of I/O ex-
amples that are complete up to some complexity. In the current version, Igor2 does
not systematically approach this problem, i.e., also Igor2 relies on complete example
sets. However, ﬁrst ideas exist to overcome this restriction, see Section 4.9.3.
Even though Igor2 does not add one rule after the other as in the ILP covering
algorithm (Algorithm 3), it has in common with that algorithm that the rules of a
program (a CS in our case) are generated independently from each other. Analogously
to the covering algorithm, this implies a notion of extensional coverage or correctness
of single rules that must be consistent with the (intensional) correctness of a complete
recursive program, where the rules interdepend.
74

4.1. Introduction
Listing 4.1: Mutually recursive deﬁnitions of odd and even induced by Igor2
odd (0)
→
false
odd (S n)
→even (n)
even (0)
→true
even (S n) →odd (n)
A last noteworthy characteristic is the capability of inducing several interdependent
target functions in parallel.
For example, if the two target predicates even and odd
(of type N →Bool), testing whether a natural number is even or odd, respectively,
are speciﬁed together by some I/O examples each, then Igor2 induces the mutually
recursive function deﬁnitions shown in Listing 4.1
The overall goal of Igor2 is to broaden the class of eﬀectively inducible programs
or functions. Until now, we only focus on correctness of induced programs and not on
eﬃciency or other criteria.
This chapter is organized as follows:
In the following short section, we introduce some additional notation regarding con-
structor systems.
In Section 4.3 we precisely deﬁne the problem that is solved by Igor2, i.e., how
speciﬁcations and background knowledge look like and how induced functions are related
to them.
In Section 4.4 we outline the general search through the program space that Igor2
conducts and brieﬂy sketch its analytical reﬁnement (synthesis) operators.
In Section 4.5 we then demonstrate Igor2 by means of an example run, inducing the
reverse function on lists.
In Section 4.6, we deﬁne the concept of extensional correctness of single rules and CSs.
In Section 4.7 we then precisely specify the synthesis operators and show that they
only synthesize extensionally correct rules. Furthermore, we present algorithms for all
synthesis operators.
In Section 4.8, based on the operator deﬁnitions, we at ﬁrst precisely deﬁne the problem
space that is searched by Igor2. In Subsection 4.8.2, we then prove that, under certain
conditions, the search applied by Igor2 in this space is terminating and complete. The
problem space also contains pseudo-CSs, i.e., CSs whose RHSs contain variables not
occurring in the LHSs.
We call these candidate CSs open.
Such open CSs are not
conﬂuent and do not denote functions. Actually, the only goal during the search is to
ﬁnd a closed, i.e., non-open, CS. Each such CS denotes a goal node or a solution within
the program space. In Subsection 4.8.3, we then ﬁnally prove that, indeed, each closed
CS in the program space correctly computes the speciﬁed I/O examples. This intensional
correctness is concluded from the extensional correctness of the single rules and the fact
that CSs induced by the synthesis operators terminate on the speciﬁed example inputs.
Finally, in Section 4.9, we shortly describe some extensions to the Igor2 algorithm
to extend the program class and to make the induction more eﬃcient.
Sections 4.3, 4.4, 4.5 are partly based on [48].
75

4. The Igor2 Algorithm
4.2. Notations
By Var(t) we denote the set of variables occurring in term t.
Let r be a rule (of a CS), then by Lhs(r) and Rhs(r) we denote its left-hand side
(LHS) and right-hand side (RHS), respectively. By Deﬁnes(r), we denote the deﬁned
function symbol at the root of the LHS of r.
If R is a CS, by Lhss(R), we denote the set of all LHSs occurring in R. Furthermore, by
DR and CR, we denote the deﬁned function symbols (roots of the LHSs) and constructors
of R, respectively.
We frequently write t for a sequence or vector t1, . . . , tn of terms. E.g., instead of
writing f(p1, . . . , pn) →t for a rule of a constructor system, we write f(p) →t.
4.3. Deﬁnition of the Problem Solved by Igor2
Incomplete speciﬁcations, background knowledge, and induced programs are ﬁnite, or-
thogonal CSs, called speciﬁcation CSs (or just speciﬁcations), background CSs, and in-
duced CSs, respectively.
Orthogonality, i.e., linear and pairwise non-unifying LHSs, is a property that is simple
to verify and to produce, hence it is the most appropriate way to assure conﬂuence of
induced CSs.
Speciﬁcations and background CSs are CSs of the same restricted form (Deﬁnition 4.1
below). The only diﬀerence is their diﬀerent meaning within the induction problem. In
contexts where their diﬀerent meaning is irrelevant, we just say speciﬁcation for both.
Speciﬁcations (hence also background CSs) have constructor terms as RHSs.
Hence
ground speciﬁcations and background CSs denote I/O examples (of target functions
to be induced and background functions that are assumed to be already implemented,
respectively), e.g.:
..., add (S S 0, S S S 0) →S S S S S 0, ...
However, speciﬁcations and background CSs may also contain variables, e.g.:
..., add (S S 0, y) →S S y, ...
Each single rule that contains variables does not denote a single I/O example. Rather
one may think of such a rule as an I/O pattern representing the (I/O example-)set of
all its ground instances (where variables are instantiated by ground constructor terms).
This interpretation conforms to the rewriting semantics since the rewrite relation deﬁned
by a term rewriting system contains all (ground) instances of its rules.
Deﬁnition 4.1 (Speciﬁcation, speciﬁcation rule, I/O pattern, I/O example). Let D and
C denote sets of deﬁned function symbols and constructors, respectively. Then a rule of
the form
f(p1, . . . , pn) →t
with f ∈D, pi ∈TC(X) (for i = 1, . . . n), and t ∈TC(X), is called speciﬁcation rule. A
ground speciﬁcation rule is also called I/O example. A non-ground speciﬁcation rule is
also called I/O pattern.
76

4.3. Deﬁnition of the Problem Solved by Igor2
A speciﬁcation is an orthogonal CS consisting of speciﬁcation rules, i.e., a set of
speciﬁcation rules whose LHSs are linear and pairwise non-unifying.
We refer to (ground-instances of) LHSs and RHSs of speciﬁcations by (speciﬁed) inputs
and outputs, respectively.
Given a speciﬁcation Φ and a background CS B, the problem is to ﬁnd a CS P that
(re-)deﬁnes the deﬁned functions of Φ and possibly additional (sub-)functions. In the
RHSs of the deﬁned functions, functions from B may be “called”. P ∪B must then
correctly reduce each speciﬁed input to the corresponding output.
Deﬁnition 4.2 (Correctness). Let Φ be a speciﬁcation and P be any CS. We say that
P is correct with respect to Φ if and only if
∗→P ⊇→Φ .
Note that if P is correct with respect to Φ, this implies that also all instances of Φ
are included in
∗→P .
The induction problem to be solved is deﬁned as follows:
Deﬁnition 4.3 (Induction problem). Let Φ and B be two speciﬁcations with disjoint
sets of deﬁned functions; DΦ ∩DB = ∅. We call Φ and B speciﬁcation and background
CS, respectively.
Find a CS P with deﬁned functions DP , such that
1. P is orthogonal: all LHSs are linear and pairwise non-unifying,
2. P does not (re-)deﬁne background functions: DP ∩DB = ∅, and
3. P ∪B is correct with respect to Φ:
∗→P∪B ⊇→Φ.
We refer to a CS P satisfying the conditions of the induction problem by solution CS,
solution program, or just solution.
Note ﬁrst that the ﬁrst two conditions, orthogonality of P and disjoint sets of deﬁned
functions of P and B imply that also P ∪B is orthogonal (if, as assumed, B is also
orthogonal).
Note further that with respect to this deﬁnition, P = Φ is a perfect
solution, just as good as inﬁnitely many other solutions. This under-determination is a
matter of fact in inductive reasoning. Which solution is returned depends on additional
criteria, called inductive bias (cp. Section 3.1.1). For example, Occam’s razor would
prefer “simpler” solutions to more complex ones.
Recall that conditional evaluation in CSs induced by Igor2 is realized by pattern
matching, i.e., by diﬀerent (non-unifying) patterns in the LHSs of a CS. The bias applied
by Igor2 is now that CSs with fewer cases in terms of maximal speciﬁc patterns (i.e.,
minimal patterns with respect to the subsumption order on terms) in the LHSs are
preferred. The idea behind this bias is that fewer cases/patterns mean that inputs are
partitioned into fewer subsets and hence that CSs with fewer cases are more general.
In contrast, if one would prefer a maximal number of patterns, then the best solution
would be the speciﬁcation itself—each speciﬁed input would become its own pattern in
its own rule.
77

4. The Igor2 Algorithm
Listing 4.2: I/O patterns for reverse
1
reverse ([ ])
→[ ]
2
reverse (x : [ ])
→x : [ ]
3
reverse (x : y : [ ])
→y : x : [ ]
4
reverse (x : y : z : [ ])
→z : y : x : [ ]
5
reverse (x : y : z : v : [ ]) →v : z : y : x : [ ]
Listing 4.3: I/O patterns for last , provided as background CS for reverse
1
last (x : [ ])
→x
2
last (x : y : [ ])
→y
3
last (x : y : z : [ ])
→z
4
last (x : y : z : v : [ ]) →v
Example 4.1. Consider the rules (I/O patterns) in Listing 4.2, forming a speciﬁcation
Φ with DΦ = {reverse}. Furthermore, assume the rules in Listing 4.3 are provided as
background CS B with DB = { last }. x, y, z, v denote variables. The constructors are
CΦ = CB = { [],
: } where [ ] denotes the empty list and
:
the usual list constructor
to insert one element at the front of the list, written in mixﬁx notation.
Then Igor2 would induce the rule set of Listing 4.4 that implements the reverse
function for lists of arbitrary length. It forms a solution CS P (DP = {reverse, sub})
with respect to Deﬁnition 4.3. The non-speciﬁed recursive subfunction sub computes
the init function and is automatically invented by Igor2.
4.4. Overview over the Igor2 Algorithm
4.4.1. The General Algorithm
The induction of a solution CS is organized as a uniform cost search in a space of
orthogonal CSs, including open CSs. By open CS, we refer to a CS P not satisfying
Var(r) ⊆Var(l) for each rule l →r of P (cp. Deﬁnition 2.21).
We also call the
respective rules and RHSs open. In the following we just say CS to denote CSs in the
narrow sense as well as open CSs. We call generated CSs candidate CSs and their rules
candidate rules.
Listing 4.4: Induced CS for the reverse function, with last as background knowledge and
inventing init as further help function
reverse ([ ])
→[ ]
reverse (x : xs)
→last (x : xs) : reverse (sub (x : xs))
sub ([x])
→[ ]
sub (x1 : x2 : xs) →x1 : sub (x2 : xs)
78

4.4. Overview over the Igor2 Algorithm
Algorithm 4 shows the general search algorithm. The cost of a candidate is its number
of cases or conditions in terms of maximal speciﬁc patterns in the LHSs. We say that a
CS with fewer cases is more general because it partitions the example inputs into fewer
subsets. The search operators never decrease the cost of a candidate—they never gener-
alize a pattern—but either increase it, by computing a set of more speciﬁc patterns from
one pattern, or leave it unchanged, when only the RHSs of a candidate are manipulated.
Hence, Igor2 prefers more general CSs to more speciﬁc CSs. Accordingly, the initial
candidate CS consists of only one rule (one pattern) per target function.
In contrast to generate-and-test methods, where successor candidates are computed
by manipulating or reﬁning a selected candidate independently from the given speciﬁ-
cation, Igor2’s reﬁnement operators compute successor candidates based on the given
speciﬁcation; or, they synthesize new candidates from the given speciﬁcation. In partic-
ular, it is assured that each constructed candidate CS P satisﬁes all three conditions of
Deﬁnition 4.3: Orthogonality, disjoint sets of deﬁned functions of P and background CS
B, and correctness of P ∪B with respect to the speciﬁcation.
However, candidates may be open CSs and as such they will not be accepted as
solutions. Open CSs are not conﬂuent and do not denote functions because the variables
in RHSs not occurring in their LHSs may arbitrarily be instantiated in a rewrite step.
Hence, purpose of the search operators is to decrease the number of such variables in
order to achieve a closed CS. Each generated closed, i.e., not open, CS is accepted and
a solution CS according to Deﬁnition 4.3.
In each search step, ﬁrst the subset of candidates with minimal cost is chosen from
all maintained candidates N. If one of them is closed, it is returned as solution. If all
chosen candidates are open, one open rule of one of them is selected and successor-rule
sets are synthesized for it. The rule is then (in parallel) replaced by all its successor sets,
leading to a set of successor candidate CSs.
Reﬁning a candidate CS, i.e., computing successors of it to replace it by them, may
involve the abduction of additional speciﬁcation rules if a successor CS introduces a new,
originally not speciﬁed, subfunction. Therefore, each candidate CS has an own speciﬁ-
cation belonging to it and Igor2 actually maintains a set of pairs ⟨P, Φ⟩of candidate
CSs P and corresponding speciﬁcations Φ. If P is open (closed), then we also say that
⟨P, Φ⟩is open (closed) and call ⟨P, Φ⟩a solution if P is closed.
4.4.2. Initial Rules and Initial Candidate CSs
In order to generate the initial candidate CS (Function initialCandidate), the spec-
iﬁcation Φ is partitioned such that each subset contains all rules (I/O examples or
I/O patterns) belonging to one and the same deﬁned function. (If only one function is
speciﬁed, the only subset is the speciﬁcation itself.) That is, we get the partition
{Φ(f) | f ∈DΦ} ,
where Φ(f) := {ϕ ∈Φ | Deﬁnes(ϕ) = f} .
Then one, possibly open, initial rule is computed for each subset Φ(f), i.e., for each
speciﬁed function f ∈D, by the initial rule operator χinit.
79

4. The Igor2 Algorithm
Algorithm 4: The general Igor2 algorithm
Input: a speciﬁcation (of target functions) Φ
Input: a background CS B with DB ∪DΦ = ∅
Output: a (maximal general) CS P satisfying the conditions of Deﬁnition 4.3
P ←initialCandidate(Φ) // defined below
1
N ←{⟨P, Φ⟩}
2
while ⟨P, Φ⟩open do
3
r ←an open rule of P
4
S ←successorRuleSets(r, Φ, B) // defined below
5
remove ⟨P, Φ⟩from N
6
foreach successor rule-set and corresponding new speciﬁcation ⟨s, φnew⟩∈S
7
do
P ′ ←(P \ {r}) ∪s
8
insert ⟨P ′, Φ ∪φnew⟩into N
9
⟨P, Φ⟩←a maximal general CS (and the corresponding speciﬁcation) in N
10
return P
11
Function initialCandidate(Φ)
Input: a speciﬁcation Φ with deﬁned functions DΦ
Output: an initial (possibly open) candidate CS P of Φ; one rule per deﬁned
function
P ←∅
1
foreach f ∈DΦ do insert χinit(Φ(f)) into P
2
return P
3
80

4.4. Overview over the Igor2 Algorithm
Initial rules are not only computed for the initial candidate from the speciﬁcation
subsets Φ(f), but also later on in the induction process. During the search for a solution
CS, the speciﬁcation Φ is further partitioned into more and more, smaller and smaller
subsets. Furthermore, if new deﬁned functions not occurring in DΦ are introduced, then
speciﬁcations for them are abduced such that new sets of speciﬁcation rules appear.
Whenever a new speciﬁcation subset occurs—either by further partitioning or by intro-
ducing subfunctions and their speciﬁcations—an initial rule is computed by χinit for the
new subset.
The initial-rule operator χinit computes a special kind of minimal generalization under
subsumption, namely minimal left-linear generalizations (MLGs), of speciﬁcation subsets
as initial rules. An MLG of a set Φ of rules equals the least general generalization (LGG)
of Φ (Deﬁnition 2.20) except for that each repeated occurrence of a variable in the LHS
is replaced by a new variable. For example, the LGG of the two rules f(a, a) →a and
f(b, b) →b (a and b are constants) is f(x, x) →x whereas the two MLGs of these rules
are f(x, y) →x and f(y, x) →x.
We take MLGs instead of LGGs in order to get
left-linear rules.
Taking minimal (most speciﬁc) generalizations under subsumption as initial rules has
several reasons: Most important, since each rule in a set Φ of speciﬁcation rules is an
instance of an MLG of Φ, every LHS in Φ can be reduced to its RHS by an MLG of Φ.
Second, it is eﬃciently computable. (These two points are not only true for minimal,
but for all generalizations under subsumption of a set of rules.) Third, taking a most
speciﬁc generalization has the advantage that it maximally decomposes the subsumed
LHSs (i.e., the speciﬁed inputs) such that maximally “deep” subterms of the speciﬁed
inputs can be referred to in the RHS of the generated initial rule. And ﬁnally fourth,
if a generalization under subsumption of a set of speciﬁcation rules Φ, that is a closed
rule, exists at all, then in particular each MLG is also closed. In other words: If a
speciﬁcation set can be solved by simply taking a generalization under subsumption,
then each MLG is such a solution. But even if all (also the minimal) generalizations
under subsumption remain open, the MLGs are the “best” of such open generalizations
because they show best which parts of the speciﬁcation rules cause the absence of a closed
generalization and hence need to be generalized by methods going beyond generalization
under subsumption.
Example 4.2. Consider the following set of two I/O patterns of the init function:
1
init (x : y : [ ])
→x : [ ]
2
init (x : y : z : [ ])
→x : y : [ ]
Its initial rule (in this case the LGG) is
init (x : y : xs) →x : ys
It is open (due to the variable ys). However, it is more useful than, for example, the (very
general) generalization init (x) →y. The LGG already “solves” parts of the specifying
rules (especially the root and the left subtree of the RHSs) and suggests the right subtree
to be further dealt with, whereas the more general generalization does not contain any
information of how to further reﬁne it.
81

4. The Igor2 Algorithm
Finally note that, since speciﬁcation rules have constructor terms as RHSs, also initial
rules always have constructor rules as RHSs, i.e., do not contain any (recursive) function
calls.
4.4.3. Reﬁnement (or Synthesis) Operators
Now let P be some generated candidate, Φ be the corresponding speciﬁcation, and
r : f(p) →t be an open rule in P that has been selected to be reﬁned. The speciﬁcation
subset of Φ associated with r is the set
Φ(r) := {ϕ ∈Φ | f(p) ⪰Lhs(ϕ)}
consisting of all speciﬁcation rules whose LHSs match with the LHS of the open rule r,
i.e., whose LHSs would be reduced by r. Now three out of four reﬁnement operators
(χsplit, χsub, χsmplCall, χcall) are in parallel applied to the open rule in order to eliminate
its open variables (Function successorRuleSets). They synthesize successor rule sets
based on the associated speciﬁcation subset Φ(r).
Function successorRuleSets(r, Φ, B)
Input: an open rule r
Input: a speciﬁcation Φ
Input: a background CS B
Output: a set of pairs of successor-rule sets and corresponding new speciﬁcation
rules
S1 ←χsplit(r, Φ)
1
S2 ←χsub(r, Φ, B)
2
S3 ←χsmplCall(r, Φ, B)
3
if S3 = ∅then S3 ←χcall(r, Φ, B)
4
return S1 ∪S2 ∪S3
5
Splitting an Open Rule into a Set of Several More Speciﬁc Rules: χsplit
The rule-splitting operator χsplit partitions Φ(r) and then applies χinit to each subset in
order replace r by a set of more speciﬁc initial rules. It is assured that the LHSs of the
new initial rules are pairwise non-unifying. The idea is, that only one rule probably does
not suﬃce to compute the example inputs speciﬁed by Φ(r) but that diﬀerent cases/rules
are needed to properly compute them. For example, each recursive solution CS consists
of at least two rules—one non-recursive rule as base case, and one rule containing a
recursive call in the RHS.
The partitions of Φ(r) are computed according to positions in the LHS of the open
rule r that denote a variable in the LHS of r and (diﬀerent) constructors in the LHSs of
the subsumed speciﬁcation rules Φ(r). We call such positions pivot positions (of Φ(r)).
All speciﬁcation rules with the same constructor at such a pivot position are sorted into
82

4.4. Overview over the Igor2 Algorithm
the same subset. The MLGs of the resulting subsets then have the respective (diﬀerent)
constructors at the pivot position and hence are non-unifying, hence constitute diﬀerent
cases. E.g., consider again the two I/O patterns and the corresponding open initial rule
(MLG) of the init function in Example 4.2. The position denoting variable xs in the
LHS of the initial rule denotes the diﬀerent constructors [ ] and : in the ﬁrst and second
I/O pattern, respectively. Hence these two I/O patterns would be sorted into diﬀerent
subsets.
The operator χsplit is the only operator that increases the cost of candidate CSs (by
replacing a pattern by a set of more speciﬁc patterns).
Introducing Subfunctions to Separately Compute Subterms: χsub
The subproblem operator χsub deals with those subterms of the RHSs in Φ(r) that cause
r to be open. Those subterms are considered as new subproblems and new subfunctions
are introduced to separately compute them. The open subterms in the open RHS are
then replaced by calls to these new subfunctions.
E.g., consider again Example 4.2
above. For the open subterm ys in the RHS of the initial rule (the MLG) of the two
speciﬁcation rules for init , a new subfunction sub1 would be introduced and ys would
be replaced by a call of sub1:
init (x : y : xs) →x : sub1 (x : y : xs)
For the new subfunctions, in our example for sub1, new speciﬁcation rules φnew are then
abduced by taking the LHSs of Φ(r) as LHSs of φnew (with the new deﬁned function
symbols at the roots) and the respective subterms of the RHSs in Φ(r) as RHSs in φnew.
For our init example we would abduce the two I/O patterns
sub1 (x : y : [ ])
→[ ]
sub1 (x : y : z : [ ]) →y : [ ]
from the two I/O patterns of init . Finally, for each new subfunction, an initial rule is
computed by χinit from the abduced speciﬁcations. In our case we get the initial rule
sub1 (x : y : xs) →ys
for sub1.
Introducing (Recursive) Function Calls: χsmplCall and χcall
Both the splitting and the subproblem operator together can eﬀectively not do more—
i.e., can not synthesize (semantically) more functions—than the splitting operator alone.
This is because the calls to (sub)functions introduced by χsub are always calls of new
subfunctions that are not currently deﬁned by the candidate CS. That is, χsub never
introduces a recursive call. Yet non-recursive subfunctions can be eliminated, without
changing the semantics of the program, by unfolding them.
Furthermore, χsub does
not introduce calls of background functions.
In contrast, the function-call operators
χsmplCall and χcall introduce calls of already deﬁned functions. They replace the (open)
RHS of an open initial rule r by a call of a deﬁned function. This may be the function
83

4. The Igor2 Algorithm
Deﬁnes(r) itself, i.e., a recursive call, or another target or previously introduced sub-
function (possibly leading to mutual recursive calls) or a background function. Both
operators introduce diﬀerent forms of function calls. Actually, at most one of them is
eﬀectively applied (i.e., leads to a non-empty set of successor rules).
The operator χsmplCall introduces function calls of the form f′(p′), where p′ are con-
structor terms. We call this a simple function call due to the constructor-term argu-
ments. In contrast, χcall introduces function calls of the form f′(g1(p), . . . , gn(p)) where
the gi are new subfunctions. The idea behind χcall is that a constructor term possibly
does not suﬃce as argument of a function call but that the argument itself possibly also
must be computed by a (recursive) deﬁned function, e.g., a separate new function or a
background function.
The function-call operators realize a generalization of the recurrence detection method
of the analytical approach to IPS as introduced by Summers 3.2.1. If, e.g., χsmplCall
replaces the RHS of an open rule r : f(p) →t by a function call with the resulting rule
f(p) →f′(p′), this is done based on matching speciﬁed outputs of the functions f and
f′. If each RHS in Φ(r) matches a RHS in the speciﬁcation of f′, then a call of f′ is in
general possible and p′ is constructed in a way such that it maps the LHSs in Φ(r) to
the respective and appropriately instantiated LHSs of f′.
Concrete examples for χsmplCall and χcall are contained in the exemplary induction of
the reverse function in the following section.
4.5. A Sample Synthesis
In this section we show a complete synthesis of the reverse function. As speciﬁcation,
we provide the following I/O patterns:
1
reverse ([ ])
→[ ]
2
reverse (x : [ ])
→x : [ ]
3
reverse (x : y : [ ])
→y : x : [ ]
4
reverse (x : y : z : [ ])
→z : y : x : [ ]
Furthermore, as background CS, we provide I/O patterns of the last function:1
1
last (x : [ ])
→x
2
last (x : y : [ ])
→y
3
last (x : y : z : [ ])
→z
Since only one function, reverse, is speciﬁed as target function, the speciﬁcation need
not be partitioned for the initial candidate CS P0. P0 simply consists of the initial rule
for reverse, the MLG of all four reverse I/O patterns:
1
reverse (x1) →x2
: Initial candidate P0
1 Providing last as background knowledge is not necessary; we provide last here as background
knowledge just to give an example of how a background CS is used by Igor2. If last was not given
as background knowledge, Igor2 would automatically introduce it.
84

4.5. A Sample Synthesis
It is open due to the variable x2 in the RHS that does not occur in the LHS and hence
needs to be reﬁned.
1st Iteration
The initial set of maintained candidate CSs N contains only the initial candidate: N =
{P0}.2 The only rule of P0 is open and selected to be reﬁned.
Selected candidate CS: P0.
Selected rule: reverse (x1) →x2.
Associated I/O patterns: All four reverse I/O patterns.
Rule splitting.
The only pivot position is 1, denoting the variable x1 in the LHS of the
selected initial rule. The ﬁrst reverse input pattern has the constructor ’[ ]’ at position 1,
the remaining reverse input patterns have the constructor ’:’ at position 1. Hence χsplit
generates the partition {{1}, {2, 3, 4}} of the reverse I/O patterns. The ﬁrst successor
candidate CS results from replacing the selected initial rule by the two more speciﬁc
initial rules (MLGs) of the two generated subsets:
1
reverse ([ ])
→[ ]
2
reverse (x1 : x2) →x3 : x4
: P0 ▷P1
Subfunctions.
Since the RHS of the selected initial rule consists of a single variable,
x2, instead of being rooted by a constructor, there are no proper subterms and χsub is
not applicable.
Function Call.
The RHS of the selected rule cannot become a call of last due to diﬀerent
types.3 A recursive call (of reverse) is not possible since none of the RHSs of the reverse
I/O patterns subsumes another one.
2nd Iteration
The initial CS P0 has been replaced by the only generated successor CS P1, i.e., we have
now N = {P1}, and only its second rule is open.
Selected candidate: P1.
Selected rule: reverse (x1 : x2) →x3 : x4.
Associated I/O patterns: reverse I/O patterns {2, 3, 4} (because the LHS of the selected
rule subsumes the LHSs of these I/O patterns but not the LHS of the ﬁrst I/O pattern).
2We omit explicitly stating the corresponding speciﬁcations here together with the candidate CSs in
N. The speciﬁcations appear in the text.
3We do not explicitly stated the types of reverse and last here, but we implicitly assume that reverse
is of type list to list and last is of type list to element.
85

4. The Igor2 Algorithm
Rule splitting.
The only pivot position is 1.2 which denotes the variable x2 in the LHS
of the selected rule and the constructors ’[ ]’ and ’:’ in the associated I/O patterns which
are now further partitioned into {{2}{3, 4}}. The selected open rule of P1 is replaced
by the initial rules for the two new subsets, leading to a new candidate P2:
1
reverse ([ ])
→[ ]
2
reverse (x1 : [ ])
→x1 : [ ]
3
reverse (x1 : x2 : x3) →x4 : x5 : x6
: P0 ▷P1 ▷P2
Subfunctions.
The RHS of the selected rule of P1 is rooted by a constructor such that
χsub can be applied. The idea is to consider the open subterms as subproblems which
are solved by new, separate subfunctions. That is, the open subterms in the selected
rule are replaced by calls to two new (sub)functions sub1 and sub2:
reverse (x1 : x2) →sub1 (x1 : x2) : sub2 (x1 : x2)
: Reﬁned selected rule
This involves the abduction of speciﬁcations for the new subfunctions from the selected
reverse I/O patterns. The formal parameter of the subfunction calls in the reﬁned rule
is always the same as the pattern of that rule—in this case: x1 : x2. Therefore, sub1
and sub2 are called with the same inputs as the selected rule. Hence the LHSs in the
abduced speciﬁcations for sub1 and sub2 are the same as those of the selected reverse
I/O patterns (only the root symbol reverse is replaced by sub1 and sub2). From these
same inputs, sub1 and sub2 must compute the ﬁrst and second subterms of the respective
reverse RHSs.
Here are (again) the I/O patterns, associated with the selected rule, from which the
new speciﬁcations are abduced:
2
reverse (x : [ ])
→x : [ ]
3
reverse (x : y : [ ])
→y : x : [ ]
4
reverse (x : y : z : [ ]) →z : y : x : [ ]
: Selected I/O patterns
From these, the following I/O patterns for the new subfunctions are abduced:
1
sub1(x : [ ])
→x
sub2(x : [ ])
→[ ]
2
sub1(x : y : [ ])
→y
sub2(x : y : [ ])
→x : [ ]
3
sub1(x : y : z : [ ]) →z
sub2(x : y : z : [ ]) →y : x : [ ]
: Abduced speciﬁcations for sub1 and sub2
The successor candidate results from replacing the selected rule by its reﬁned version
as stated above and by adding initial rules for the new subfunctions (MLGs of their
respective I/O patterns). This leads to a new candidate P3:
86

4.5. A Sample Synthesis
1
reverse ([ ])
→[ ]
2
reverse (x1 : x2) →sub1 (x1 : x2) : sub2 (x1 : x2)
3
sub1 (x1 : x2)
→x3
4
sub2 (x1 : x2)
→x4
: P0 ▷P1 ▷P3
Program Call.
A program call is not possible for the same reasons as in the ﬁrst
iteration.
3rd Iteration
In iteration 2, two successor candidates of P1 were generated. The candidate set is now
N = {P2, P3}. P3 is more general than P2 because of only two maximal speciﬁc patterns
in P3 ([ ] and x1 : x2) instead of three in P2 ([ ], x1 : [ ], x1 : x2 : x3), hence we select
P3 and one of its open rules, say the third one, to be reﬁned.
Selected candidate: P3.
Selected rule: sub1 (x1 : x2) →x3 .
Associated I/O patterns: All three sub1 I/O patterns.
Rule splitting.
The only pivot position is 1.2 where the LHS of the ﬁrst sub1 I/O pattern
has a ’[ ]’ and the remaining two a ’:’. We partition the I/O patterns accordingly and
replace the selected rule by the two resulting initial rules, leading to candidate P4:
1
reverse ([ ])
→[ ]
2
reverse (x1 : x2)
→sub1 (x1 : x2) : sub2 (x1 : x2)
3
sub1 (x1 : [ ])
→x1
4
sub1 (x1 : x2 : x3) →x4
5
sub2 (x1 : x2)
→x4
: P0 ▷P1 ▷P3 ▷P4
Subfunctions.
The selected rule has a single variable as RHS such that this operator
is not applicable.
Program Call.
Each LHS in the I/O patterns of sub1 matches a LHS of the last back-
ground CS such that the RHS of the selected sub1 rule can be replaced by a call of last .
The argument of that call must then map the sub1 example LHSs to the corresponding
(possibly instantiated) last LHS.
Since actually each RHS in the sub1 speciﬁcation matches each RHS in the last spec-
iﬁcation, there are several possibilities for the argument of the call.
At ﬁrst, χsmplCall is tried where the formal argument of the function call is a constructor
term
87

4. The Igor2 Algorithm
One possible argument for the call of the last function is a very simple one: It is
the pattern of the selected rule itself, because, actually, the I/O patterns of sub1 are
identical to the I/O patterns of last . We here take this simple solution leading to the
successor candidate P5:
1
reverse ([ ])
→[ ]
2
reverse (x1 : x2) →sub1 (x1 : x2) : sub2 (x1 : x2)
3
sub1 (x1 : x2)
→last (x1 : x2)
4
sub2 (x1 : x2)
→x4
: P0 ▷P1 ▷P3 ▷P5
If χsmplCall succeeds, as in this case, then χcall will not be tried.
4th Iteration
We now have the candidate set N = {P2, P4, P5} where P5 is better rated than the
equally rated P4 and P2 and thus selected. The only open rule of P5 is the fourth one.
Selected candidate: P5.
Selected rule: sub2 (x1 : x2) →x3 .
Associated I/O patterns: All three I/O patterns of sub2.
Rule splitting.
Results in P6:
1
reverse ([ ])
→[ ]
2
reverse (x1 : x2)
→sub1 (x1 : x2) : sub2 (x1 : x2)
3
sub1 (x1 : x2)
→last (x1 : x2)
4
sub2 (x1 : [ ])
→[ ]
5
sub2 (x1 : x2 : x3) →x4 : x5
: P0 ▷P1 ▷P3 ▷P5 ▷P6
Subfunctions.
Not applicable due to a single variable as RHS.
Program Call.
A call of last is not possible due to type mismatch. A recursive call of
sub2 is not possible because no RHS of the I/O patterns of sub2 subsumes another one.
Yet a call of reverse is possible. Each RHS of the sub2 I/O patterns matches (in this
special case: is identical with) a RHS of the reverse speciﬁcation. The following listing
shows the identical RHSs in the middle and the corresponding sub2 and reverse LHSs at
the left and the right, respectively:
1
sub2(x : [ ])
→[ ]
←reverse ([ ])
2
sub2(x : y : [ ])
→x : [ ]
←reverse (x : [ ])
3
sub2(x : y : z : [ ]) →y : x : [ ] ←reverse (x : y : [ ])
: Identical RHSs and corresponding sub2 and reverse speciﬁcation LHSs
88

4.5. A Sample Synthesis
We replace the open RHS x3 of the selected rule sub2 (x1 : x2) →x3 by a call to
reverse. The argument of the call must then map the input patterns x : [ ], x : y : [ ],
x : y : z : [ ] of sub2 to the corresponding input patterns [ ], x : [ ], x : y : [ ] of
reverse, respectively. It is obvious that this mapping cannot be achieved by a constructor
term as argument only. Hence χsmplCall fails here.
Therefore χcall is invoked. χcall considers the described mapping as the speciﬁcation
of a new subfunction sub3 for computing the function-call argument:
1
sub3 (x : [ ])
→[ ]
2
sub3 (x : y : [ ])
→x : [ ]
3
sub3 (x : y : z : [ ]) →x : y : [ ]
: Abduced I/O patterns for the new subfunction sub3
Since reverse calls sub2, we are going to introduce a mutual recursion here. Therefore,
it is important that the input patterns of sub2 are greater than the corresponding input
patterns of reverse, i.e., that the output patterns of sub3 are smaller than the respective
input patterns.
In the resulting successor candidate of P5, the RHS of the selected rule is replaced by
a call to reverse with a call of sub3 as argument. Furthermore, the initial rule for sub3
is added:
1
reverse ([ ])
→[ ]
2
reverse (x1 : x2) →sub1 (x1 : x2) : sub2 (x1 : x2)
3
sub1 (x1 : x2)
→last (x1 : x2)
4
sub2 (x1 : x2)
→reverse (sub3 (x1 : x2))
5
sub3 (x1 : x2)
→x3
: P0 ▷P1 ▷P3 ▷P5 ▷P7
5th Iteration
The candidate set is now N = {P2, P4, P6, P7} where P7 is better rated than the equally
rated remaining three candidates. The only open rule of P7 is its ﬁfth rule.
Selected candidate: P7.
Selected rule: sub3 (x1 : x2) →x3 .
Associated I/O patterns: All three I/O patterns of sub3.
Rule splitting.
Leads to P8:
1
reverse ([ ])
→[ ]
2
reverse (x1 : x2)
→sub1 (x1 : x2) : sub2 (x1 : x2)
3
sub1 (x1 : x2)
→last (x1 : x2)
4
sub2 (x1 : x2)
→reverse (sub3 (x1 : x2))
5
sub3 (x1 : [ ])
→[ ]
6
sub3 (x1 : x2 : x3) →x1 : x4
: P0 ▷P1 ▷P3 ▷P5 ▷P7 ▷P8
89

4. The Igor2 Algorithm
Subfunctions.
Not applicable due to a single-variable RHS.
Program Call.
Each sub3 output pattern matches a reverse output pattern such that,
in principle, we could again introduce a call of reverse here. This would result in a new
subfunction sub4 to compute the argument of the call (like sub3 computes the argument
of the reverse call in the sub2 rule). In the next step, sub4 then again could call reverse
and so on, ad inﬁnitum. Therefore, the maximal depth of nested calls of deﬁned functions
is limited by some natural number which can be set by the speciﬁer. Let us assume that
this parameter prevents a further call of reverse here.
Furthermore, each sub3 output pattern equals a sub2 output pattern. Yet since (i)
sub2 calls sub3 such a call of sub2 would introduce a mutual recursion and (ii), the
argument of that call would not decrease, a call of sub2 is not be possible here.
A call to last or sub1 is not possible due to diﬀerent types.
6th Iteration
We have now N = {P2, P4, P6, P8}. All these candidates are equally rated such that it
is not determined, which of them will be reﬁned in the next step. Let us take P8. Its
only open rule is the sixth one.
Selected candidate: P8.
Selected rule: sub3 (x1 : x2 : x3) →x1 : x4 .
Associated I/O patterns: sub3 I/O patterns {2, 3}.
Rule splitting.
Leads to P9:
1
reverse ([ ])
→[ ]
2
reverse (x1 : x2)
→sub1 (x1 : x2) : sub2 (x1 : x2)
3
sub1 (x1 : x2)
→last (x1 : x2)
4
sub2 (x1 : x2)
→reverse (sub3 (x1 : x2))
5
sub3 (x1 : [ ])
→[ ]
6
sub3 (x1 : x2 : [ ])
→x1 : [ ]
7
sub3 (x1 : x2 : x3 : [ ]) →x1 : x2 : [ ]
: P0 ▷P1 ▷P3 ▷P5 ▷P7 ▷P8 ▷P9
Note, that the I/O patterns for sub3 are simply reproduced in this candidate.
Subfunctions.
The RHS of the selected rule is rooted by a constructor such that we
may treat the subterms as new problems. The ﬁrst (left) subterm is the variable x1
that also occurs in the LHS, hence need not to be replaced. The second subterm x4,
however, is open and is thus replaced by a call of a new subfunction sub4. The following
I/O patterns for sub4 are abduced from I/O patterns 2, 3 of sub3:
1
sub4 (x : y : [ ])
→[ ]
2
sub4 (x : y : z : [ ]) →y : [ ]
: Abduced I/O patterns for the new subfunction sub4
90

4.5. A Sample Synthesis
Replacing the open subterm by the call to sub4 and adding the initial rule for sub4 yields
the new candidate P10:
1
reverse ([ ])
→[ ]
2
reverse (x1 : x2)
→sub1 (x1 : x2) : sub2 (x1 : x2)
3
sub1 (x1 : x2)
→last (x1 : x2)
4
sub2 (x1 : x2)
→reverse (sub3 (x1 : x2))
5
sub3 (x1 : [ ])
→[ ]
6
sub3 (x1 : x2 : x3) →x1 : sub4 (x1 : x2 : x3)
7
sub4 (x1 : x2 : x3) →x4
: P0 ▷P1 ▷P3 ▷P5 ▷P7 ▷P8 ▷P10
Program Call.
As in the previous iteration, a call to reverse would in principle be
possible but we assume that this is prevented by a parameter bounding the maximal
depth of nested program calls.
7th Iteration
The candidate set is now N = {P2, P4, P6, P9, P10}. P9 is worst rated and therefore not
selected; the remaining candidates are equally rated. We take P10 and its seventh rule
to be reﬁned in the next step.
Selected candidate: P10.
Selected rule: sub4 (x1 : x2 : x3) →x4 .
Associated I/O patterns: The two I/O patterns of sub4.
Rule splitting.
Yields P11. We leave this as an exercise for the reader.
Subfunctions.
Not applicable.
Program Call.
Calls of reverse, sub2 and sub3 are in principle possible and are in-
troduced.
We here show the call of sub3.
The following listing shows the two sub4
I/O patterns as well as the corresponding two (renamed) sub3 I/O patterns whose RHSs
subsume the sub4 RHSs.
1
sub4 (x : y : [ ])
→[ ]
sub3 (v : [ ])
→[ ]
2
sub4 (x : y : z : [ ]) →y : [ ]
sub3 (v : w : [ ]) →v : [ ]
: Matching sub4 and (renamed) sub3 I/O patterns
The ﬁrst sub4 and sub3 RHSs are equal. The second sub3 RHS subsumes the second
sub4 RHS with substitution τ = {v 7→y}.
We ﬁnd x2 : x3 as constructor term argument, i.e., the reﬁned selected rule is
sub4 (x1 : x2 : x3) →sub3 (x2 : x3). With this call, the ﬁrst sub4 LHS (from the
matching I/O patterns above) is correctly mapped to the ﬁrst sub3 LHS and the second
sub4 LHS is correctly mapped to the second sub3 LHS.
The resulting successor candidate CS is the following:
91

4. The Igor2 Algorithm
1
reverse ([ ])
→[ ]
2
reverse (x1 : x2)
→sub1 (x1 : x2) : sub2 (x1 : x2)
3
sub1 (x1 : x2)
→last (x1 : x2)
4
sub2 (x1 : x2)
→reverse (sub3 (x1 : x2))
5
sub3 (x1 : [ ])
→[ ]
6
sub3 (x1 : x2 : x3) →x1 : sub4 (x1 : x2 : x3)
7
sub4 (x1 : x2 : x3) →sub3 (x2 : x3)
: P0 ▷P1 ▷P3 ▷P5 ▷P7 ▷P8 ▷P10 ▷P12
8th Iteration
We now have N = {P2, P4, P6, P9, P11, P12}. P2, P4, P6, P12 are equally rated and bet-
ter than P9, P11. Since P12 is closed, we have found a maximally general and closed
candidate. Hence the search stops here and returns P12 as solution CS.
Post processing
We observe that P12 contains non-recursive subfunctions—sub1 and sub2. They can be
eliminated by unfolding them into the RHSs where they are called. sub3 and sub4 are
mutually recursive but not directly recursive either. So we can eliminate one of them.
Since sub3 is also called by sub2 we keep it and eliminate sub4 by unfolding it into the
second RHS of sub3.
Such simple unfoldings can be done fully automatically. The resulting solution is:4
1
reverse ([ ])
→[ ]
2
reverse (x1 : x2)
→last (x1 : x2) : reverse (sub3 (x1 : x2))
3
sub3 (x1 : [ ])
→[ ]
4
sub3 (x1 : x2 : x3) →x1 : sub3 (x2 : x3)
4.6. Extensional Correctness
As we have seen, the rules of candidate CSs are generated independently from each other,
each single rule based on the speciﬁcation only. However, in an eventually induced closed
CS the rules interdepend and must together correctly reduce each speciﬁed input to the
corresponding output.
Example 4.3. Consider the following speciﬁcation for the two target functions even and
odd:
1
even (0)
→true
odd (0)
→
false
2
even (S 0)
→
false
odd (S 0)
→true
3
even (S S 0) →true
odd (S S 0)
→
false
4 The automatically introduced subfunction sub3 is the init function that deletes the last element of
an input list.
92

4.6. Extensional Correctness
Now suppose the χsplit operator has already partitioned both speciﬁcations (for even
and odd) into two subsets each, where one subset contains the respective ﬁrst I/O ex-
ample and the second one contains the respective remaining two I/O examples. The
corresponding candidate would be:
even (0)
→true
even (S x) →y
odd (0)
→
false
odd (S x)
→y
Suppose the open rule for even is selected and χsmplCall is applied and tries to introduce
a call of odd. χsmplCall then checks whether each RHS of the I/O examples associated
with the selected open rule (the second and third I/O example for even in our example)
is subsumed by any of the speciﬁed outputs of the function to be called (odd in our
example). Indeed, the RHS of I/O example 2 for even is false and equals, e.g., the RHS
of I/O example 1 for odd. And the RHS of I/O example 3 for even is true and equals
the RHS of I/O example 2 for odd. Now the argument of the function call is computed
such that it maps the LHSs of the two I/O examples of even to the according LHSs of
odd. The resulting rule is
even (S x) →odd (x)
and the corresponding (still open) successor candidate CS is:
even (0)
→true
even (S x) →odd (x)
odd (0)
→
false
odd (S x)
→y
The synthesized rule even (S x) →odd (x) is extensionally correct with respect to
the speciﬁcation because if it is used to achieve one reduction step, then the speciﬁcation
itself further reduces the resulting term correctly. E.g., the rule even (S x) →odd (x)
reduces in one step the example input even (S S 0) to odd (S 0). We cannot use the cur-
rent candidate CS to uniquely reduce this term because the respective rule in the candi-
date CS is the still open rule. Yet we may use the speciﬁcation itself to reduce odd (S 0)
to true. And indeed, true is the correct result for our exemplary input even (S S 0).
The last open rule for odd is similarly reﬁned by χsmplCall to the extensionally correct
rule odd (S x) →even (x). The solution CS is that from the introduction of this chapter
(Listing 4.1).
Of course, we now expect that this CS is (intensionally) correct, i.e., that it reduces
each example input to the corresponding output without using the speciﬁcation itself
for the reduction. Indeed, the solution CS is correct in this sense (i.e., according to
Deﬁnition 4.2).
Certainly, this independent construction of single rules, which eventually interdepend
in the induced CS, comes not for free: It presupposes appropriately chosen I/O examples
or I/O patterns, hence impedes the synthesis from randomly chosen I/O examples. In
particular, the speciﬁcation must be complete up to some complexity of the I/O examples
or I/O patterns.
93

4. The Igor2 Algorithm
For example, if the second I/O example for odd in the example above was missing,
then the synthesized rule for even that calls odd would not be extensionally correct
anymore, because if we reduced the example input even (S S 0) to odd (S 0) and this
example input for odd was missing, then odd (S 0) would not correctly be reduced to
true (the speciﬁed output for even (S S 0)) by the speciﬁcation. Actually, the rule for
even that calls odd would not be found by χsmplCall in this case.
Hence relying on extensional correctness makes induction incomplete in the sense
that if speciﬁcations are provided that are not complete up to some complexity, then
(intensionally) correct CSs may not be found (cp. also Section 3.2.4).5
We now precisely deﬁne extensional correctness. In the next section we then precisely
deﬁne the several synthesis operators and show that they synthesize extensionally correct
rules.
In Section 4.8, we then, besides other things, show that, indeed, extensional
correctness leads to correctness according to Deﬁnition 4.2 if, furthermore, we assure
that synthesized CSs terminate for the speciﬁed inputs.
Deﬁnition 4.4 (Extensional correctness of a single rule). Let Φ be a speciﬁcation. A
(possibly open) rule f(p) →t is extensionally correct with respect to Φ iﬀ, whenever
(f(i) →o) ∈Φ and f(i) = f(p)σ for a substitution σ with Dom(σ) = Var(f(p)), then
there is a substitution θ with Dom(θ) = Var(t) \ Var(f(p)) such that tσθ
∗→Φ o.
Corollary 4.1. Let Φ be a speciﬁcation and f(p) →t be a rule that is extensionally
correct with respect to Φ. Let Φ′ be another speciﬁcation with DΦ ∩DΦ′ = ∅. Then
f(p) →t is also extensionally correct with respect to Φ ∪Φ′.
Proof. Due to DΦ ∩DΦ′ = ∅there is no rule r ∈Φ′ with Deﬁnes(r) = f such that
f(p)σ = f(i) could be satisﬁed.
For all rules (f(i) →o) ∈Φ, however, for that
substitutions σ, θ exist such that tσθ
∗→Φ o, obviously also tσθ
∗→Φ∪Φ′ o.
Extensional correctness of (candidate) CSs simply means that all rules in a candidate
CS are extensionally correct and furthermore, that each speciﬁed input matches some
LHS/pattern of the candidate CS.
Deﬁnition 4.5 (Extensional correctness of a CS). Let Φ be a speciﬁcation. A CS P is
extensionally correct with respect to Φ iﬀ:
1. Extensionally correct rules: Each rule in P is extensionally correct with respect to
Φ.
2. Completeness: Each LHS of Φ matches a LHS of P.
5 It is exactly this interdependence of (mutually) recursive functions that makes the induction of (direct)
recursive functions or multiple (mutually recursive) functions a hard problem in general, compared
to, for example, decision trees, that can be modelled as non-recursive rules. In general—if one does
not want to pay a price such as appropriately chosen I/O examples—, for recursive functions, all
(potentially interdependent) rules must be induced in parallel. That is, the problem space consists of
sets of rules, whereas the rules modelling a decision tree are independent from each other such that
the problem space eﬀectively only consists of single rules.
94

4.7. Formal Deﬁnitions and Algorithms of the Synthesis Operators
Corollary 4.2. Let Φ be a speciﬁcation. Then Φ is extensionally correct with respect to
Φ.
Proof. Let (f(p) →t) ∈Φ and (f(i) →o) ∈Φ and f(i) = f(p)σ. Since Φ is orthogonal,
(f(p) →t) = (f(i) →o) and hence tσ = t = o, hence tσ
∗→Φ o.
Corollary 4.3. Let P be a CS and Φ be a speciﬁcation such that P is extensionally
correct with respect to Φ.
1. DΦ ⊆DP .
2. Let P ′ be a further CS and Φ′ be a further speciﬁcation such that P ′ is extensionally
correct with respect to Φ′ and DP ∩DP ′ = ∅. Then P ∪P ′ is extensionally correct
with respect to Φ ∪Φ′.
Proof.
1. Follows immediately from the completeness condition of extensional cor-
rectness.
2. Extensionally correct rules: W.l.o.g., let r be a rule in P.
By deﬁnition, r is
extensionally correct with respect to Φ. With DP ∩DP ′ = ∅, hence also DΦ∩DΦ′ =
∅, and Corollary 4.1 now follows that r is also extensionally correct with respect
to Φ ∪Φ′.
Completeness: Obvious.
The presented notion of extensional correctness for rules and CSs and the implied need
for speciﬁcations that are complete up to some level, is closely related to the notion of
extensional coverage in inductive logic programming (ILP). Also in ILP, the use of the
covering algorithm (Algorithm 3) which induces the single rules independently from each
other based on extensional coverage, requires specifying example sets that are complete
up to some level, if the induced rules are recursive or otherwise interdepend; compare
Section 3.3.1.
4.7. Formal Deﬁnitions and Algorithms of the Synthesis
Operators
In this section, we formally deﬁne the initial-rule operator χinit and the reﬁnement
operators χsplit, χsub, χsmplCall, and χcall, that are applied to initial rules generated by
χinit, if such an initial rule is open. We prove that all synthesized rules are extensionally
correct with respect to the corresponding speciﬁcation and background CS. Furthermore,
we present algorithms for all synthesis operators.
The operators χsub and χcall introduce new subfunctions and abduce speciﬁcations
for them.
Hence diﬀerent candidate CSs may deﬁne diﬀerent sets of (sub)functions
and hence may have diﬀerent speciﬁcations—even though the original, user-provided
speciﬁcation of the target functions is a subset of the speciﬁcation of each candidate CS.
95

4. The Igor2 Algorithm
In general, if ⟨P, Φ⟩is a candidate CS and a corresponding speciﬁcation, the reﬁnement
operators χsplit, χsub, χsmplCall, and χcall are applied to an open rule r ∈P (and get Φ
and possibly a background CS B as further input) and yield a ﬁnite, possibly empty, set
{⟨s1, φ1⟩, ⟨s2, φ1⟩, . . .} of successor-rule sets si and corresponding new speciﬁcations φi.
The φi are empty in the case of χsplit and χsmplCall and contain the abduced speciﬁcations
for the new subfunctions in case of χsub and χcall. The successor pair ⟨P ′, Ψ′⟩of ⟨P, Ψ⟩
according to one particular generated pair ⟨s, φ⟩is then deﬁned by
P ′ := (P \ {r}) ∪s
Φ′ := Φ ∪φ
If ⟨P, Φ⟩is a candidate CS and a corresponding speciﬁcation, then each rule, in par-
ticular each open rule, r ∈P has an associated speciﬁcation subset of Φ, denoted by
Φ(r). Φ(r) exactly contains all speciﬁcation rules in Φ whose LHSs match the LHS of r.
That is, Φ(r) exactly contains all those speciﬁed inputs (or input patterns) that could
be reduced by r.
Deﬁnition 4.6 (Speciﬁcation subset associated with a rule). Let ⟨P, Φ⟩be a candidate
CS and the corresponding speciﬁcation and let r ∈P be a rule of P. Then by Φ(r) ⊆Φ
we denote the speciﬁcation subset of Φ associated with rule r, deﬁned as
Φ(r) := {ϕ ∈Φ | Lhs(r) ⪰Lhs(ϕ)} .
Sometimes we do not need all speciﬁcation rules associated with a particular rule but
all speciﬁcation rules of one deﬁned function f.
Deﬁnition 4.7 (Speciﬁcation subset of a deﬁned function). Let Φ be a speciﬁcation and
f ∈DΦ be one of the functions speciﬁed by Φ. Then by Φ(f) we denote the speciﬁcation
subset of Φ associated with function f, deﬁned as
Φ(f) := {ϕ ∈Φ | Deﬁnes(ϕ) = f} .
Extensional correctness of rules that are synthesized by χsplit, χsub, χsmplCall, and χcall
as reﬁnements of an open initial rule r that is selected from an open candidate CS P
relies on three assumptions:
Claim 4.1. Let Φ be a speciﬁcation and B be a background CS such that DΦ∩DB = ∅(cp.
Deﬁnition 4.3). Further, let ⟨P, Ψ⟩be any candidate CS and corresponding speciﬁcation
generated by Igor2 from the initial speciﬁcation Φ and background CS B.
Then
• DP = DΨ,
• DP ∩DB = ∅, and
• if r ∈P is an open rule in P then r is an MLG of Ψ(r).
96

4.7. Formal Deﬁnitions and Algorithms of the Synthesis Operators
We cannot prove this claim here, because we have not yet precisely deﬁned the syn-
thesis operators that generate candidate CSs and corresponding speciﬁcations. In the
following subsection, we show the stated claims for initial candidate CSs. For the re-
mainder of this section we then just assume that these claims are true for a provided
pair ⟨P, Ψ⟩and background CS B. Then in Section 4.8, we show that, provided these
claims are true for a particular pair ⟨P, Ψ⟩and a background CS B, they are also true
for all successor pairs ⟨P ′, Ψ′⟩generated by the synthesis operators. Together with the
fact that they are true for initial CSs this inductively shows that they are true for all
generated candidate CSs and corresponding speciﬁcations.
4.7.1. Initial Rules and Candidate CSs
Initial rules are computed by χinit when new speciﬁcation (sub)sets appear. Especially
the ﬁrst candidate CS, the initial CS of the search, is computed by applying χinit to the
speciﬁcation subsets corresponding to the speciﬁed target functions. (If only one target
function is speciﬁed, then χinit is applied to the complete set of speciﬁcation rules.)
Also when a speciﬁcation subset Φ(r), associated with an open rule r, is (further)
partitioned by χsplit, then χinit is subsequently applied to the new subsets. Furthermore,
when χsub or χcall introduce new subfunctions and abduce speciﬁcation rules for them,
then χinit is subsequently applied to the new speciﬁcations in order to get initial rules
for the new (sub)functions.
Initial rules are minimal left-linear generalizations (maximal speciﬁc left-linear gener-
alizations), with respect to subsumption, of sets of speciﬁcation rules.
Deﬁnition 4.8 (Minimal left-linear generalization). Let ⟨RΣ(X), ⪰⟩be the set of all
speciﬁcation rules over signature Σ and variables X, quasi-ordered by subsumption. Let
Φ be any ﬁnite subset of RΣ(X).
Then a minimal element in the set of left-linear
upper-bounds of Φ in ⟨RΣ(X), ⪰⟩is called a minimal left-linear generalization (MLG)
of Φ.
MLGs are equal to LGGs (least general generalizations) except for that repeated
occurrences of variables in LHSs are replaced by new variables. In contrast to LGGs,
MLGs need not be unique.
Example 4.4 (Minimal left-linear generalizations). Consider the following two rules
where a and b are constants: f(a, a) →a and f(b, b) →b. Their unique (up to variable
renaming) LGG is f(x, x) →x. Their two MLGs are f(x, y) →x and f(x, y) →y.
Obviously, if the LGG of a set of rules Φ is left-linear, then there is a unique MLG of
Φ—the LGG of Φ. For example, the LGG and unique MLG of the two rules f(a, b) →b
and f(c, d) →d is f(x, y) →y.
The initial rule operator χinit returns one (arbitrary) MLG for a speciﬁcation.
Deﬁnition 4.9 (Initial rule operators χinit and χINIT). For a set Φ of speciﬁcation rules
let G denote the set of all MLGs of Φ. Then χinit is (non-deterministically) deﬁned as
χinit(Φ) ∈G .
97

4. The Igor2 Algorithm
For a set {Φ1, . . . , Φn} of sets of speciﬁcation rules, χINIT is deﬁned as
χINIT({Φ1, . . . , Φn}) = {χinit(Φ1), . . . , χinit(Φn)} .
Corollary 4.4. Let φ ⊆Φ be a subset of a speciﬁcation and χinit(φ) = (f(p) →t).
1. The rule f(p) →t is left-linear.
2. If, for all (f(i) →o) ∈Φ \ φ, f(p) ̸⪰f(i), then f(p) →t is extensionally correct
with respect to Φ.
Proof.
1. χinit yields MLGs that are left-linear by deﬁnition.
2. Let f(i) →o ∈φ and σθ (Dom(σ) = Var(f(p)), Dom(θ) = Var(t) \ Var(f(p)))
be a (composed) substitution such that (f(i) →o) = (f(p) →t)σθ.
(Since
χinit(φ) ⪰ϕ for each speciﬁcation rule ϕ ∈φ such a substitution σθ exists for each
ϕ ∈φ.) We have f(i) = f(p)σ and tσθ = o, hence tσθ
∗→Φ o.
Deﬁnition 4.10 (Initial candidate CS). Let Φ be a speciﬁcation. Then a CS P, deﬁned
as
P := χINIT({Φ(f) | f ∈DΦ}) ,
(where Φ(f) = {ϕ ∈Φ | Deﬁnes(ϕ) = f}, cp. Deﬁnition 4.7) is called an initial (candi-
date) CS of Φ.
Lemma 4.1. Let Φ be a speciﬁcation and B be a background CS such that DΦ ∩DB = ∅.
Let P be an initial CS of Φ. Then
1. DP = DΦ,
2. DP ∩DB = ∅,
3. each rule r ∈P is an MLG of its associated speciﬁcation subset Φ(r),
4. P is orthogonal, and
5. P is extensionally correct with respect to Φ.
Proof.
1. P contains exactly one rule for each f ∈DΦ, hence DP = DΦ.
2. Follows from DΦ ∩DB = ∅and DP = DΦ.
3. Let r ∈P be an MLG of Φ(f) for an f ∈DΦ. Then for each ϕ ∈Φ(f), Lhs(r) ⪰
Lhs(ϕ). Furthermore, since all LHSs in Φ \ Φ(f) are rooted by function symbols
f′ ̸= f, Lhs(r) ̸⪰Lhs(ϕ) for all ϕ ∈Φ \ Φ(f). Hence Φ(f) = Φ(r), hence r is an
MLG of Φ(r).
98

4.7. Formal Deﬁnitions and Algorithms of the Synthesis Operators
4. Each rule in P is generated by χinit, hence left-linear. Furthermore, all LHSs in a
speciﬁcation subset Φ(f) are rooted by the same function symbol f. Hence also the
LHS of the respective rule in P, an MLG of Φ(f), is rooted by f. On the other side,
the rules in a subset Φ(f) and the rules in another subset Φ(f′) are rooted by the
diﬀerent function symbols f ̸= f′. Hence the LHSs of the respective initial rules
in P are rooted by the diﬀerent function symbols f, f′ and hence non-unifying.
5. Extensionally correct rules:
Let be r ∈P and Deﬁnes(r) = f.
Then r =
χinit(Φ(f)) = χinit(Φ(r)). Furthermore, by deﬁnition of Φ(r), Lhs(r) ̸⪰Lhs(ϕ)
for all ϕ ∈Φ \ Φ(r). Now from Corollary 4.4 follows that r is extensionally correct
with respect to Φ.
Completeness: Let (f(i) →o) ∈Φ be any rule in Φ. Then (f(i) →o) ∈Φ(f) and
since P contains a rule f(p) →t that is an MLG of Φ(f), f(i) matches f(p).
Note that by this lemma (items 1, 2, 3), initial candidates satisfy all three assumptions
of Claim 4.1 for their speciﬁcations.
4.7.2. Splitting a Rule into a Set of More Speciﬁc Rules
The splitting operator χsplit partitions a speciﬁcation subset associated with an open
rule r and then applies χinit to each new subset. It is assured that the resulting LHSs
are more speciﬁc than that of r, that they are pairwise non-unifying, and that each LHS
in Φ(r) matches the LHS of one new rule. Thus, χsplit introduces conditional evaluation
based on exhaustive pattern matching of the inputs speciﬁed by Φ(r). The following
deﬁnition formally speciﬁes the operator χsplit.
Deﬁnition 4.11 (Rule-splitting operator χsplit). Let ⟨P, Φ⟩be a candidate CS and the
corresponding speciﬁcation. Let r be an open rule in P and Φ(r) be the speciﬁcation
subset associated with r (see Deﬁnition 4.6). We assume that r is an MLG of Φ(r)
(Claim 4.1).
We call a position p ∈Pos(Lhs(r)) a pivot position (of Φ(r)) if it denotes a variable
in Lhs(r) and a constructor in Lhs(ϕ) for each ϕ ∈Φ(r). Then an equivalence relation
∼p on Φ(r) is deﬁned upon p as
ϕ ∼p ϕ′
iﬀ
Node(Lhs(ϕ), p) = Node(Lhs(ϕ′), p) .
The operator χsplit(r, Φ) builds the set of all quotient sets Φ(r)/ ∼p for all pivot
positions p and applies χINIT to each quotient set, yielding a set of more speciﬁc successor
rules compared to r for each quotient set. Since no new subfunctions are introduced,
the corresponding new speciﬁcation sets are empty:
χsplit(r, Φ) = {⟨χINIT(Φ(r)/∼p), ∅⟩| p is a pivot position of Φ(r)}
(where χINIT({φ1, . . . , φn}) := {χinit(φ1), . . . , χinit(φn)}, cp. Deﬁnition 4.9).
99

4. The Igor2 Algorithm
Lemma 4.2. Let ⟨P, Φ⟩and r be deﬁned as in Deﬁnition 4.11.
Let p be a pivot position of Φ(r), ⟨s, ∅⟩∈χsplit(r, Φ) be the corresponding set s of
successor rules of r (together with the empty set of new speciﬁcation rules), and r′ ∈s
be one of the successor rules.
Then
1. s is orthogonal,
2. r ≻r′,
3. r′ is extensionally correct with respect to Φ, and
4. r′ is an MLG of its associated speciﬁcation subset Φ(r′).
Proof. Let φ ∈Φ(r)/ ∼p be any subset/equivalence class in the quotient set of Φ(r)
generated upon pivot position p and r′ ∈s be the corresponding new rule, i.e., r′ =
χinit(φ).
1. r′ is generated by χinit, hence left-linear.
Since ﬁrst, r is assumed to be an MLG of Φ(r), second, p denotes a variable in
Lhs(r) and the same constructor c in all LHSs in φ ⊆Φ(r), and third, the new
rule r′ is an MLG of φ, we have Node(Lhs(r′), p) = c.
Since furthermore, Φ(r) is partitioned with respect to the diﬀerent constructors
at position p in the LHSs, for each two φ1, φ2 ∈Φ(r)/ ∼p and the corresponding
new initial rules r′
1, r′
2 ∈s holds Node(Lhs(r′
1), p) = c1 ̸= c2 = Node(Lhs(r′
2), p)
for constructors c1, c2.
Hence each two successor rules r′
1, r′
2 ∈s are non-unifying.
2. Since r is an MLG of Φ(r) and r′ is an MLG of φ ⊆Φ(r), r ⪰r′. Since, furthermore,
Node(Lhs(r), p) is a variable and Node(Lhs(r′), p) is a constructor, r ≻r′.
3. By deﬁnition of Φ(r), Lhs(r) ̸⪰Lhs(ϕ) for all ϕ ∈Φ \ Φ(r).
Since further-
more, r ⪰r′, also Lhs(r′) ̸⪰Lhs(ϕ) for all ϕ ∈Φ \ Φ(r). Furthermore, since
Node(Lhs(r′), p) ̸= Node(Lhs(ϕ), p) for all ϕ ∈Φ(r) \ φ, Lhs(r′) ̸⪰Lhs(ϕ) for all
ϕ ∈Φ(r) \ φ. Hence Lhs(r′) ̸⪰Lhs(ϕ) for all ϕ ∈Φ \ φ and with Corollary 4.4
follows that r′ is extensionally correct with respect to Φ.
4. Since Lhs(r′) ⪰Lhs(ϕ), ϕ ∈Φ, if and only if ϕ ∈φ, we have φ = Φ(r′). Since
r′ = χinit(φ) and χinit generates MLGs, r′ is an MLG of Φ(r′).
Algorithm 7 computes χsplit.
100

4.7. Formal Deﬁnitions and Algorithms of the Synthesis Operators
Algorithm 7: The splitting operator χsplit
Input: an open rule r
Input: a speciﬁcation Φ
Output: a ﬁnite set S = {⟨s1, ∅⟩, ⟨s2, ∅⟩, . . .} of pairs of successor-rule sets
s1, s2, . . . and empty new speciﬁcation sets
S ←∅
1
foreach position p ∈Pos(Lhs(r)) do
2
if Node(Lhs(r), p) is a variable and Node(Lhs(ϕ), p) is a constructor for all
3
ϕ ∈Φ(r) then
// p is a pivot position
s ←{χinit(φ) | φ ∈(Φ(r)/∼p)}
4
insert ⟨s, ∅⟩into S
5
return S
6
4.7.3. Introducing Subfunctions to Compute Subterms
The subproblem operator χsub introduces new subfunctions to compute subterms of
RHSs of speciﬁcation rules in Φ(r), if the respective subterms are open in the initial rule
r. A precondition is that the initial rule is rooted by a constructor such that proper
subterms exist in it.
The following deﬁnition formally speciﬁes the operator χsub.
Deﬁnition 4.12 (Subproblem operator χsub). Let ⟨P, Φ⟩be a candidate CS and the
corresponding speciﬁcation and B be a background CS such that DP ∩DB = ∅. Let
r : f(p) →c(t1, . . . , tn) be an open rule in P and Φ(r) be the speciﬁcation subset
associated with r.
Let I = {i ∈[n] | Var(ti) ̸⊆Var(p)} be the set of positions denoting open subterms
in the open RHS c(t1, . . . , tn) of r. Let (gi)i∈I be a family of new function symbols not
occurring in P or B: ((gi) ∩(DP∪B ∪CP∪B ∪X) = ∅). Then
χsub(r, Φ, B) :=

{f(p) →c(t′
1, . . . , t′
n)} ∪Pnew, φnew
	
where
• for all j ∈[n], t′
j =
(
tj
if j ̸∈I
gj(p)
if j ∈I
,
• φnew = {gj(i) →oj | j ∈I, (f(i) →c(o1, . . . , on)) ∈Φ(r)}, and
• Pnew is an initial candidate CS of φnew.
If the RHS of an open rule r ∈P is not rooted by a constructor, then χsub(r, Φ, B) = ∅.
Lemma 4.3. Let ⟨P, Φ⟩, B, r :
f(p) →c(t1, . . . , tn), I and (gi) be deﬁned as in
Deﬁnition 4.12 and χsub(r, Φ, B) = ⟨({f(p) →c(t′
1, . . . , t′
n)} ∪Pnew), φnew⟩. Then
101

4. The Igor2 Algorithm
1. f(p) →c(t′
1, . . . , t′
n) is extensionally correct with respect to Φ ∪φnew ∪B,
2. all rules in Pnew are extensionally correct with respect to Φ ∪φnew ∪B, and
3. the new (initial) rules r′ ∈Pnew are MLGs of their respective associated speciﬁca-
tion subsets (Φ ∪φnew)(r′).
Proof. Due to Claim 4.1, DP = DΦ. With (gi) ∩DP = ∅follows Dφnew ∩DΦ = ∅.
1. Let (f(i) →o) ∈Φ ∪φnew ∪B and f(i) = f(p)σ. By deﬁnition of Φ(r) and
due to DΦ ∩(Dφnew ∪DB) = ∅, f(p) ̸⪰Lhs(ϕ) for all ϕ ∈(Φ ∪φnew ∪B) \ Φ(r).
Hence (f(i) →o) ∈Φ(r). We have to show that c(t′
1, . . . , t′
n)σ
∗→Φ∪φnew∪B o where
o = c(o1, . . . , on). (We have Var(p) = Var(c(t′
1, . . . , t′
n)), hence θ is empty.) Since
r is an MLG of Φ(r) by Claim 4.1 and f(i) = f(p)σ, we have t′
jσ = tjσ = oj for
all j ∈[n] \ I. Hence it remains to show gj(p)σ
∗→Φ∪φnew∪B oj for all j ∈I. This
follows from gj(p)σ = gj(i) and (gj(i) →oj) ∈φnew.
2. By Lemma 4.1, the rules in Pnew are extensionally correct with respect to φnew.
Since Dφnew ∩(DΦ ∪DB) = ∅, it follows with Corollary 4.1 that the rules in Pnew
are extensionally correct with respect to Φ ∪φnew ∪B.
3. Since Pnew is an initial CS of φnew and due to Lemma 4.1, each r′ ∈Pnew is an
MLG of φnew(r′). Since DΦ ∩Dφnew = ∅, we have φnew(r′) = (Φ ∪φnew)(r′).
Algorithm 8 computes χsub.
4.7.4. Introducing Function Calls
Two further operators, χsmplCall and χcall, replace an open RHS by a call of an already
introduced function (a target function, a previously introduced help function, or a back-
ground function).
Let f(p) →t be the selected open rule. Then, in the case of χsmplCall, the new rule
has the form f(p) →f′(p′), where p′ is a constructor term over variables from p. In
the case of χcall, the new rule has the form f(p) →f′(g1(p), . . . , gn(p)), where the gi
are new function symbols. The idea behind the gi as arguments is that, if χsmplCall fails,
i.e., when no function call with a constructor term as argument can be found, then the
argument of the function call possibly needs to be computed by another, possibly new
and/or recursive, subfunction. In addition to the new rule f(p) →f′(g1(p), . . . , gn(p)),
the operator χcall abduces speciﬁcations for the new subfunctions gi and invokes χinit to
get initial rules for them.
102

4.7. Formal Deﬁnitions and Algorithms of the Synthesis Operators
Algorithm 8: The subproblem operator χsub
Input: an open rule r : f(p) →t
Input: a speciﬁcation Φ
Input: a background CS B
Output: the empty set or a set {⟨s, φnew⟩} containing of pair of a successor-rule
set and the corresponding new speciﬁcation subset
if t = c(t1, . . . , tn) then
1
φnew ←∅
2
foreach j ∈[n] do
3
if Var(tj) ̸⊆Var(p) then
4
gj ←a new deﬁned function symbol; gj ̸∈DP∪B ∪CP∪B ∪X
5
φnew ←φnew ∪{gj(i) = oj | (f(i) →c(o1, . . . , on)) ∈Φ(r)}
6
t′
j ←gj(p)
7
else t′
j ←tj
8
Pnew ←initialCandidate(φnew)
9
return {⟨{f(p) →c(t′
1, . . . , t′
n)} ∪Pnew, φnew⟩}
10
else return ∅
11
Concerning Termination of Solution CSs on Speciﬁed Inputs
Certainly, we expect that induced solution CSs are terminating, at least for the speciﬁed
inputs. In particular, termination on speciﬁed inputs follows as necessary condition from
the deﬁnition of solution CSs (Deﬁnition 4.3). The correctness property, →P∪B ⊇→Φ,
of a solution CS implies that the example inputs speciﬁed by Φ terminate in P ∪B. This
is because example outputs are normal forms and the correctness property assures that
these normal forms can be reached. Hence, the corresponding derivation terminates.
Furthermore, conﬂuence of solution CSs assure that each derivation of a speciﬁed LHS
reaches the speciﬁed output.
Hence, inﬁnite derivations for speciﬁed inputs are not
possible in solution CSs.6
However, termination of a CS does not follow from extensional correctness of its rules.
Suppose we have a speciﬁcation Φ for one single deﬁned function f, then f(p) = f(p)
is extensionally correct with respect to Φ, yet it does not terminate for any term (and
therefore cannot be (intensionally) correct with respect to Φ). Hence, in addition to
extensional correctness, we must assure termination of speciﬁed inputs.
The reason for potential non-termination is (mutual) recursion. That is, χcall and
χsmplCall could potentially cause a (closed) candidate CS to be non-terminating. Intu-
itively, in order to assure that recursive programs terminate, it must be assured that
the arguments of repeated calls of the same function during the evaluation always get
strictly reduced with respect to a well-founded order, so that after ﬁnitely many calls, a
6Note that the correctness property does not assure termination in general of generated CSs, i.e., that
also non-speciﬁed inputs terminate.
103

4. The Igor2 Algorithm
minimal argument is reached and no further call is possible.
Again let f(p) →t be a selected open rule. Then it is assured that χsmplCall replaces
its RHS t only by those function calls f′(p′), such that if f(p)σ is a speciﬁed input for f,
then f′(p′)σ is a speciﬁed input for f′ and f(p)σ > f′(p′)σ with respect to a reduction
order >. (See Deﬁnition 2.24 for reduction order.)
Analogously, it is assured that χcall replaces the open RHS t only by those function
calls f′(g1(p), . . . , gn(p)), such that if f(p)σ is a speciﬁed input for f, then the gi(p)σ
are speciﬁed inputs for the gi with corresponding outputs oi and f(p)σ > f′(o1, . . . , on)
with respect to a reduction order >.
This condition is not only posed to direct recursive calls, i.e., if f = f′, but for each
f′ ∈Φ, where Φ is the speciﬁcation of the candidate CS, because recursion can also be
introduced indirectly in the form of mutual recursive calls. The only exception is when
f′ is a background function, i.e., f′ ∈DB for a background CS B. In this case, the call
cannot cause recursion because the background CS is deﬁned independently from the
candidate CS.
Of course, these conditions must be satisﬁed with respect to one and the same re-
duction order for the complete candidate CS. Then, these conditions, together with
extensional correctness, assure termination of the speciﬁed inputs. Restricted to linear
terms, the order s > t if and only if | Pos(s) | > | Pos(t) | is, for example, a reduction
order. This reduction order is applied by Igor2 if no other reduction order is speciﬁed.
The Simple-Call Operator χsmplCall
We call function calls introduced by χsmplCall, due to the constructor-term arguments,
simple. The operator χsmplCall is formally speciﬁed by the following deﬁnition.
Deﬁnition 4.13 (Simple function-call operator χsmplCall). Let ⟨P, Φ⟩be a candidate CS
and the corresponding speciﬁcation and B be a background CS such that DP ∪DB = ∅
(Claim 4.1). Let C denote the set of constructors of P ∪B. Let r : f(p) →t be an open
rule in P and Φ(r) be the speciﬁcation subset associated with r.
Then χsmplCall(r, Φ, B) yields a (possibly empty) set of pairs of unit rule sets and
empty new speciﬁcations, where the rules have the form f(p) →f′(p′), f′ ∈DΦ∪B
(possibly f = f′) and p′ ∈TC(Var(p)). The set is uniquely speciﬁed as follows:

{f(p) →f′(p′)}, ∅

∈χsmplCall(r, Φ, B)
if and only if for each (f(i) →o) ∈Φ(r) there is a rule (f′(i′) →o′) ∈Φ ∪B (w.l.o.g.
we assume that f(i) →o and f′(i′) →o′ do not share any variables; this can always
be achieved by standardizing apart both rules) such that the following conditions are
satisﬁed. Let σ denote the substitution that matches the selected open rule r : f(p) →t
with the respective speciﬁcation rule (f(i) →o) ∈Φ(r), i.e., (f(i) →o) = (f(p)σ →tσ).
1. tσ = o′τ for a substitution τ with Dom(τ) = Var(o′). (Each output pattern tσ
speciﬁed in Φ(r) is subsumed by a speciﬁed output pattern o′ of f′.)
104

4.7. Formal Deﬁnitions and Algorithms of the Synthesis Operators
2. f′(p′) ⪰f′(i′)τ and f′(p′)σ = f′(i′)τθ for any substitution θ7 with Dom(θ) =
Var(f′(i′)) \ Var(o′). (The formal argument p′ of the function call leads to the
“correct” inputs for f′, i.e., to the inputs with corresponding required outputs o′τ.)
3. If f′ ∈DΦ, then f(p)σ > f′(p′)σ with respect to a reduction order >.
(The
argument of the function call is reduced if f′ is not a background function and the
call can thus lead to recursion.)
If (f(p)σ →tσ) ∈Φ(r) and the new rule is f(p) →f′(p′), then f′(p′)σ must reduce
to tσ. We check this (extensionally) at hand of the speciﬁcation of f′: There must be a
rule f′(i′) →o′ for f′ such that its output subsumes tσ (by some substitution τ)—this
is condition 1 in Deﬁnition 4.13—and f′(p′)σ is the correspondingly (by τ) instantiated
input f′(i′)—this is condition 2. Condition 3 additionally requires that the argument
of the function call is reduced in order to assure termination, if f′ is not a background
function.
Especially condition 2 needs some further explanation. A naive approach would be
to simply require f′(p′)σ = f′(i′)τ. This suﬃces if f′(i′) →o′ is ground (hence τ = ∅)
or at least Var(f′(i′)) = Var(o′). Yet it is also possible that Var(f′(i′)) ⊃Var(o′).
If this is the case, the instantiation of the variables in f′(i′) not occurring in o′ is not
determined by the substitution τ. This is the reason for the additional substitution θ in
condition 2: We need f′(p′)σ = f′(i′)τθ instead of merely f′(p′)σ = f′(i′)τ. Actually—
at least if condition 3 does not apply in the case that f′ is a background function—these
variables may be arbitrarily instantiated by θ—the output o′ and hence tσ obviously
does not depend on them. Yet simply generating all appropriate function calls (for all
substitutions θ) is not possible because these are inﬁnitely many. Hence we somehow
need to restrict this inﬁnite set of principally appropriate calls to a ﬁnite number. While
f′(p′)σ = f′(i′)τθ implies that f′(p′) is a generalization of f′(i′)τθ (f′(p′) ⪰f′(i′)τθ),
we additionally require that f′(p′) is already a generalization of f′(i′)τ (without the
instantiation θ applied). This essentially bounds the size of p′ from above and prevents
an inﬁnite set of diﬀerent calls f′(p′).
Lemma 4.4. Let Φ, B, and r : f(p) →t be deﬁned as in Deﬁnition 4.13 and let be
⟨(f(p) →f′(p′)), ∅⟩∈χsmplCall(r, Φ, B). Then f(p) →f′(p′) is extensionally correct
with respect to Φ ∪B.
Proof. Let (f(i) →o) ∈Φ ∪B and f(i) = f(p)σ. By deﬁnition of Φ(r) and due to
DP ∩DB = ∅, f(p) ̸⪰Lhs(ϕ) for all ϕ ∈(Φ ∪B) \ Φ(r). Hence (f(i) →o) ∈Φ(r).
Due to condition 2 in Deﬁnition 4.13, there is a rule (f′(i′) →o′) ∈Φ ∪B such that
f′(p′)σ = f′(i′)τθ for substitutions τ and θ, where Dom(τ) = Var(o′). Hence we have
f′(p′)σ →Φ∪B o′τ. Due to condition 1, o′τ = o, hence we have f′(p′)σ
∗→Φ∪B o.
Algorithm 9 computes χsmplCall. It makes use of Function sigmaThetaGeneralizations.
Consider Algorithm 9. The set S to be computed is initialized with the empty set.
Then, in the outmost loop, it is iterated over all functions f′ (for the function calls
7Note the explanations for this additional substitution θ below.
105

4. The Igor2 Algorithm
Algorithm 9: The simple call operator χsmplCall
Input: an open rule r : f(p) →t
Input: a speciﬁcation Φ
Input: a background CS B
Output: a ﬁnite (possibly empty) set S = {⟨{r′
1}, ∅⟩, ⟨{r′
2}, ∅⟩, . . .} of pairs of
unit successor-rule sets {r′
1}, {r′
2}, . . . and empty new speciﬁcation
subsets
S ←∅
1
foreach f′ ∈DΦ∪B do
2
foreach (f(i) →o) ∈Φ(r) do
3
Gf(i) ←∅
4
σ ←the substitution that matches f(i) with f(p) (f(i) = f(p)σ)
5
foreach (f′(i′) →o′) ∈Φ ∪B do
6
if o = o′τ for any substitution τ with Dom(τ) = Var(o′) then
7
V ←Var(f′(i′)) \ Var(o′)
8
Gf(i) ←Gf(i) ∪sigmaThetaGeneralizations(σ, f′(i′)τ, V )
9
if f′ ∈Φ then remove all f′(p′) with f′(p′)σ ̸< f(p)σ from Gf(i)
10
G ←T
(f(i)→o)∈Φ(r) Gf(i)
11
foreach f′(p′) ∈G do insert ⟨f(p) →f′(p′), ∅⟩into S
12
return S
13
106

4.7. Formal Deﬁnitions and Algorithms of the Synthesis Operators
f′(p′)). The ﬁrst inner loop then iterates over all speciﬁcation rules associated with
the open rule r. Within this loop (lines 4–10), all possible function calls f′(p′), that
satisfy the three conditions of Deﬁnition 4.13 for the current speciﬁcation rule f(i),
are generated and collected into the set Gf(i). Since each generated function call must
satisfy the three conditions for all rules (f(i) →o) ∈Φ(r), in line 11 the intersection
of all the generated Gf(i) is taken. In the next line, one pair ⟨f(p) →f′(p′), ∅⟩for each
remaining function call is generated an inserted to the solution set.
The function calls, collected into the Gf(i), for a single speciﬁcation rule (f(i) →o) ∈
Φ(r), are generated as follows (lines 4–10): Gf(i) is initialized with the empty set. Then
the algorithm iterates (line 6) over all speciﬁcation rules f′(i′) →o′ for the currently
selected function f′. First it checks whether the respective RHS o′ subsumes the RHS
o (by some substitution τ). If this is the case, the variables of f′(i′) not occurring in
o′ (to be instantiated by θ, cp. condition 2 of Deﬁnition 4.13 and the explanation in
the text) are stored into V . Then all possible function calls according to f(i) →o and
f′(i′) →o′ are generated by the function sigmaThetaGeneralizations and added to
Gf(i). If f′ ∈Φ, then in line 10 all function calls not reducing the argument are removed.
Finally, let us look to Function sigmaThetaGeneralizations now: It generates, as
described in the following, a set T of all terms s, such that, for given term t, variables
V ⊆Var(t), and substitution σ, s ⪰t and sσ = tθ for arbitrary substitutions θ with
Dom(θ) = V . Obviously, since sigmaThetaGeneralizations is called with t = f′(i′)τ
and V = Var(f′(i′))\Var(o′), T is then the set of all terms f′(p′) satisfying condition 2
of Deﬁnition 4.13 according to the two particular rules f(i) →o and f′(i′) →o′.
To achieve T as required, we ﬁrst check each single variable assignment in σ (lines 2, 3
in Function sigmaThetaGeneralizations). If the term t′ assigned to a variable x ac-
cording to σ equals tθ, then we have xσ = tθ. (Furthermore, obviously x ⪰t.) Hence x
satisﬁes the conditions and is added to T. Furthermore, in the case that t is a constant
function symbol, t itself satisﬁes the conditions and is also inserted into T. Finally, t
may be a term rooted by a symbol c and with arguments t1, . . . , tn. Then we recursively
apply sigmaThetaGeneralizations to each of these arguments (line 6), achieving sets
of terms Ti such that for each si ∈Ti, si ⪰ti and siσ = tiθi. Then we obviously have
c(s1, . . . , sn) ⪰c(t1, . . . , tn) and c(s1, . . . , sn)σ = c(t1, . . . , tn)θ for some substitution θ.
Hence we insert c(s1, . . . , sn) for each tuple of the recursively generated si into T.
These are all possibilities for terms s satisfying the conditions.
If χsmplCall succeeds, i.e., if at least one rule f(p) →f′(p′) according to Deﬁnition 4.13
exists, no other successor rules of r introducing function calls will be generated. Yet if
χsmplCall yields the empty set, then the second function-call operator, χcall, is invoked to
introduce function calls.
The Function-Call Operator χcall
The operator χcall is formally speciﬁed as follows.
Deﬁnition 4.14 (Function call operator χcall). Let ⟨P, Φ⟩be a candidate CS and the cor-
responding speciﬁcation and B be a background CS such that DP ∩DB = ∅(Claim 4.1).
107

4. The Igor2 Algorithm
Function sigmaThetaGeneralizations(σ, t, V )
Input: a substitution σ
Input: a linear term t
Input: a set of variables V ⊆Var(t)
Output: the (possibly empty) set T of all terms s with s ⪰t and such that
sσ = tθ for arbitrary substitutions θ with Dom(θ) = V
T ←∅
1
foreach variable assignment (x ←t′) ∈σ do
2
if t′ = tθ for any substitution θ with Dom(θ) = V then insert x into T
3
if t is a constant function symbol then insert t into T
4
if t = c(t1, . . . , tn) then
5
foreach i ∈[n] do Ti ←sigmaThetaGeneralizations(σ, ti, V )
6
foreach tuple (s1, . . . , sn) ∈T1 × · · · × Tn do insert c(s1, . . . , sn) into T
7
return T
8
Let r : f(p) →t be an open rule in P and Φ(r) be the speciﬁcation subset associated
with r. Finally, let (gi)i∈N be a family of new (function) symbols not occurring in P or
B; (gi) ∩(DP∪B ∪CP∪B ∪X) = ∅.
Then χcall(r, Φ, B) yields a (possibly empty) set of pairs of the form

{f(p) →f′(g1(p), . . . , gn(p))} ∪Pnew, φnew

, where f′ ∈DΦ∪B and α(f′) = n ,
speciﬁed as follows:

{f(p) →f′(g1(p), . . . , gn(p))} ∪Pnew, φnew

∈χcall(r, Φ, B)
if and only if there is a total mapping µ : Φ(r) →(Φ ∪B)(f′) such that for each
(f(i) →o) ∈Φ(r) and µ(f(i) →o) = (f′(i′) →o′) (w.l.o.g. we assume that f(i) →o
and f′(i′) →o′ do not share any variables; this can always be achieved by standardizing
apart both rules), the following conditions are satisﬁed:
1. Var(f′(i′)) = Var(o′).
2. o = o′τ for a some substitution τ (Each output pattern tσ speciﬁed in Φ(r) is
subsumed by a speciﬁed output pattern o′ of f′.)
3. If f′ ∈DΦ, then f(i) > f′(i′)τ with respect to a reduction order >.
Let µ be such a mapping. Then
• φnew := {gj(i) →i′
jτ | j ∈[n], (ϕ : f(i) →o) ∈Φ(r), µ(ϕ) = (f′(i′
1, . . . , i′
n) →o′)}
and
• Pnew is an initial candidate CS of φnew,
108

4.7. Formal Deﬁnitions and Algorithms of the Synthesis Operators
If (f(p)σ →tσ) ∈Φ(r) and the new rule is f(p) →f′(g1(p), . . . , gn(p)), then
f′(g1(p), . . . , gn(p))σ must reduce to tσ. Hence there must be a rule f′(i′
1, . . . , i′
n) →o′
for f′ such that its output subsumes tσ (by some substitution τ)—this is condition 2
in Deﬁnition 4.14. Furthermore, f′(g1(p), . . . , gn(p))σ must reduce to the corresponding
(instantiated by τ) input, f′(i′
1, . . . , i′
n)τ; this is assured by introducing the new speciﬁ-
cation for the gj appropriately: Each gj(i) must reduce to i′
jτ. Yet since τ is determined
by matching the example output patterns o, o′, variables in f′(i′) not occurring in o′
would not be instantiated by τ. Actually, they could be instantiated arbitrarily; o′ does
not depend on them. In the current version of the Igor2 algorithm, we deal with this
case only for the simple-call operator; cp. Deﬁnition 4.13 and its explanation in the text.
For χcall, we currently simply exclude this case by requiring that all variables occurring
in f′(i′) also occur in o′ and hence are (uniquely) instantiated by τ—condition 1 in Def-
inition 4.14. Condition 3 then additionally requires that the arguments of the call are
appropriately reduced, if f′ is not a background function, in order to assure termination.
For each new function gj, an initial rule is introduced by computing an initial candidate
CS Pnew for φnew.
Lemma 4.5. Let Φ, B, r : f(p) →t, and (gi) be deﬁned as in Deﬁnition 4.14. and let
be ⟨{f(p) →f′(g1(p), . . . , gn(p))} ∪Pnew, φnew⟩∈χcall(r, Φ, B).
Then
1. f(p) →f′(g1(p), . . . , gn(p)) is extensionally correct with respect to Φ ∪φnew ∪B,
2. all rules in Pnew are extensionally correct with respect to Φ ∪φnew ∪B, and
3. all rules r′ ∈Pnew are MLGs of their associated speciﬁcation subsets (Φ∪φnew)(r′).
Proof. Due to Claim 4.1, DP = DΦ. With (gi) ∩DP = ∅follows Dφnew ∩DΦ = ∅.
1. Let (f(i) →o) ∈Φ ∪φnew ∪B and f(i) = f(p)σ. By deﬁnition of Φ(r) and due
to DΦ ∩(DB ∪(gi)) = ∅, f(p) ̸⪰Lhs(ϕ) for all ϕ ∈(Φ ∪φnew ∪B) \ Φ(r). Hence
(f(i) →o) ∈Φ(r). Let µ(f(i) →o) = (f′(i′
1, . . . , i′
n) →o′) and o = o′τ. We have
to show that f′(g1(i), . . . , gn(i))
∗→Φ∪φnew∪B o. We have (gj(i) →i′
jτ) ∈φnew for
all j ∈[n]. Therefore f′(g1(i), . . . , gn(i))
∗→Φ∪φnew∪B f′(i′
1, . . . , i′
n)τ. Finally, from
(f′(i′
1, . . . , i′
n) →o′) ∈Φ ∪B and o = o′τ follows f′(i′
1, . . . , i′
n)τ
∗→Φ∪φnew∪B o.
2. By Lemma 4.1, the rules in Pnew are extensionally correct with respect to φnew.
Since Dφnew ∩(DΦ ∪DB) = ∅, it follows with Corollary 4.1 that the rules in Pnew
are extensionally correct with respect to Φ ∪φnew ∪B.
3. Since Pnew is an initial CS of φnew and due to Lemma 4.1, each r′ ∈Pnew is an
MLG of φnew(r′). Since DΦ ∩Dφnew = ∅, we have φnew(r′) = (Φ ∪φnew)(r′).
Algorithm 11 computes χcall.
109

4. The Igor2 Algorithm
Algorithm 11: The function-call operator χcall
Input: an open rule r : f(p) →t
Input: a speciﬁcation Φ
Input: a background CS B
Output: a set S = {⟨s1, φ1⟩, ⟨s2, φ2⟩, . . .} of pairs of successor-rule sets
s1, s2, . . . and corresponding new speciﬁcation subsets φ1, φ2, . . .
S ←∅
1
foreach f′ ∈DΦ∪B do
2
µ′ ←possibleMappings(r, f′, Φ ∪B)
3
foreach mapping µ : Φ(r) →Φ ∪B with µ(ϕ) ∈µ′(ϕ) for each ϕ ∈Φ(r) do
4
if f′ ∈DB or f(i) > f′(i′)τ for each µ(f(i) →o) = (f′(i′) →o′) and
5
o = o′τ then
φnew ←∅
6
foreach j ∈[n] do
7
gj ←a new deﬁned function symbol; gj ̸∈(DΦ∪B ∪CΦ∪B ∪X)
8
φnew ←φnew ∪
9
{gj(i) →i′
jτ | (ϕ : f(i) →o) ∈Φ(r), µ(ϕ) = f ′(i′
1, . . . , i′
n) →o′, o = o′τ}
10
Pnew ←initialCandidate(φnew)
11
insert ⟨{f(p) →f′(g1(p), . . . , gn(p))} ∪Pnew, φnew⟩into S
12
return S
13
110

4.7. Formal Deﬁnitions and Algorithms of the Synthesis Operators
The outmost loop iterates over all f′ ∈DΦ∪B for the possible function calls. In each
iteration, at ﬁrst (line 3) a mapping µ′ : Φ(r) →P(Φ ∪B) is generated by the help
function possibleMappings, such that for each f(i) →o ∈Φ(r) and f′(i′) →o′ ∈
µ′(f(i) →o), conditions 1 and 2 from Deﬁnition 4.14 are satisﬁed.
Then the ﬁrst inner loop (line 4) iterates over all mappings µ : Φ(r) →Φ∪B that sat-
isfy conditions 1 and 2 from Deﬁnition 4.14. Line 5 additionally assures that condition 3
is satisﬁed. Then at ﬁrst φnew is generated (lines 6–10 according to the current mapping
µ. After computing the initial candidate CS Pnew for φnew (line 11), the solution pair
⟨{f(p) →f′(g1(p), . . . , gn(p))} ∪Pnew, φnew⟩according to µ is added to S.
Function possibleMappings(r, φ, f′)
Input: an open rule r : f(p) →t
Input: a set of speciﬁcation rules φ
Input: a function symbol f′ ∈Dφ
Output: a total mapping µ′ : φ(r) →P(φ)
µ′ ←∅
1
foreach (f(i) →o) ∈φ(r) do
2
Pf(i) ←∅
3
foreach (f′(i′) →o′) ∈φ with Var(f′(i′)) = Var(o′) do
4
if o = o′τ for any substitution τ then insert f′(i′) →o′ into Pf(i)
5
insert (f(i) →o) 7→Pf(i) into µ′
6
return µ′
7
4.7.5. The Synthesis Operators Combined
If an open initial rule is selected to be reﬁned, then χsplit, χsub, and either χsmplCall
or χcall are applied to r, a speciﬁcation Φ, and possibly a background CS B, each
yielding a (possibly empty) set S of pairs of successor-rule sets and (possibly empty)
new speciﬁcations.
The sets S computed by the diﬀerent synthesis operators are pairwise disjoint; this
follows immediately from the Deﬁnitions 4.11, 4.12, 4.13, and 4.14 of the operators. By
χB(r, Φ) we denote this disjoint union:
Deﬁnition 4.15 (Successor candidate-rule sets and corresponding new speciﬁcation
rules). Let ⟨P, Φ⟩be a candidate CS and the corresponding speciﬁcation, r ∈P be
an open rule in P, and B be a background CS. Then χB(r, Φ)—the set of successor
candidate-rule sets and corresponding new speciﬁcation sets of r with respect to Φ and
B—is deﬁned as the (disjoint) union of the single synthesis operators:
χB(r, Φ) =
(
χsplit(r, Φ) ˙∪χsub(r, Φ, B) ˙∪χsmplCall(r, Φ, B)
if χsmplCall(r, Φ, B) ̸= ∅
χsplit(r, Φ) ˙∪χsub(r, Φ, B) ˙∪χcall(r, Φ, B)
if χsmplCall(r, Φ, B) = ∅
111

4. The Igor2 Algorithm
The following corollary states some simple facts regarding χB that immediately follow
from the deﬁnitions of the synthesis operators.
Corollary 4.5. Let ⟨P, Φ⟩be a candidate CS and a corresponding speciﬁcation, B be a
background CS, r ∈P be an open rule in P, and ⟨s, φnew⟩∈χB(r, Φ).
• r ̸∈s.
• s contains a rule r′ with Lhs(r) ⪰Lhs(r′).
• χB(r, Φ), s, and φnew are all ﬁnite, if P, Φ, B are ﬁnite.
4.8. Properties of the Igor2 Algorithm
In this section, we show that, under certain conditions, the search of Igor2 is terminat-
ing and complete with respect to the problem space determined by the initial speciﬁca-
tion, the provided background CS, and the synthesis operators. (By initial speciﬁcation
we explicitly refer to the original, user-provided speciﬁcation; in contrast to the speci-
ﬁcations belonging to candidates, that all include the initial speciﬁcation but possibly
specify further subfunctions that were introduced by the synthesis operators.)
Furthermore we show that closed CSs in the problem space, i.e., those candidate CSs
that are accepted as solutions, are indeed correct (according to Deﬁnition 4.2) with
respect to the provided speciﬁcation. Together with the also proven facts that candidate
CSs are orthogonal and do not redeﬁne background functions, it follows that accepted
candidate CSs are solutions of the induction problem as stated in Deﬁnition 4.3.
Furthermore, we shortly discuss the completeness of Igor2 with respect to certain
function classes and the complexity of Igor2.
We start with a formal deﬁnition of the problem space.
4.8.1. Formalization of the Problem Space
Given a candidate CS P, a corresponding speciﬁcation Φ, and a background CS B, a
successor CS P ′ and corresponding successor speciﬁcation Φ′ results from selecting one
(arbitrary) open rule r ∈P, applying the synthesis operators to get χB(r, Φ), choosing
one ⟨s, φnew⟩from it, and removing r and adding s to P to get P ′ and adding φnew to Φ
to get Φ′. Formally, the set of all possible successor CSs and successor speciﬁcations is
deﬁned as follows:
Deﬁnition 4.16 (Successor candidate CSs and speciﬁcations). Let ⟨P, Φ⟩be a candi-
date CS and a corresponding speciﬁcation and B be a background CS. Then the set of
successor CSs and corresponding speciﬁcations of ⟨P, Φ⟩with respect to B, denoted by
ΞB(⟨P, Φ⟩), is deﬁned as
ΞB(⟨P, Φ⟩) :=
[
r∈P
r open

(P \ {r}) ∪s , Φ ∪φnew

| ⟨s, φnew⟩∈χB(r, Φ)
	
.
112

4.8. Properties of the Igor2 Algorithm
Now the problem space according to an initial, user-provided, speciﬁcation Φ and a
background CS B, denoted by PΦ,B, is deﬁned as a graph where CSs and corresponding
speciﬁcations (reachable from initial CSs of Φ by repeatedly applying the synthesis op-
erators) are nodes and an arc exists between two nodes ⟨P, Φ⟩and ⟨P ′, Φ′⟩if and only
if ⟨P ′, Φ′⟩is a successor CS and successor speciﬁcation of ⟨P, Φ⟩.
Deﬁnition 4.17 (Problem Space). Let Φ and B be an initial speciﬁcation and a back-
ground CS, respectively, such that DΦ ∩DB = ∅. Then the problem space with respect
to Φ and B, denoted by PΦ,B, is a directed graph deﬁned as follows:
• If P is an initial CS of Φ, the pair ⟨P, Φ⟩is a node in PΦ,B. We call this node an
initial node (of PΦ,B).
• If and only if ⟨P, Ψ⟩is a node in PΦ,B and ⟨P ′, Ψ′⟩∈ΞB(⟨P, Ψ⟩), then ⟨P ′, Ψ′⟩is
a node in PΦ,B and (⟨P, Ψ⟩, ⟨P ′, Ψ′⟩) is a directed arc in PΦ,B.
Deﬁnition 4.18 (Reachable nodes). Let N be any node in PΦ,B.
By Ξn
B(N) we denote the set of all nodes in PΦ,B that are reachable from N by a path
of length n.
By Ξ∗
B(N) we denote the set of all nodes in PΦ,B that are reachable from N (by a
path of any length).
Notation 4.1. Let ⟨P, Ψ⟩be any node in PΦ,B, r ∈P be an open rule in P, ⟨s, φnew⟩∈
χB(r, Φ) be a successor-rule set and corresponding new speciﬁcation, and ⟨P ′, Ψ′⟩=
⟨(P \ {r}) ∪s, Ψ ∪φnew⟩∈ΞB(⟨P, Ψ⟩) be the corresponding successor CS and corre-
sponding successor speciﬁcation of ⟨P, Ψ⟩.
If ⟨s, φnew⟩∈χsplit(r, Φ), we denote the corresponding arc in PΦ,B by
⟨P, Ψ⟩
χsplit
−−−→

P ′, Ψ′
.
We denote the arc analogously when ⟨s, φnew⟩is generated by χsub, χsmplCall, or χcall.
Now we can restate and prove the assumptions that were made in Claim 4.1.
Lemma 4.6. Let Φ be a speciﬁcation and B be a background CS such that DΦ ∩DB = ∅.
Further, let ⟨P, Ψ⟩∈PΦ,B be any candidate CS and corresponding speciﬁcation generated
by the synthesis operators from the initial speciﬁcation Φ and background CS B.
Then
1. DP = DΨ,
2. DP ∩DB = ∅, and
3. if r ∈P is an open rule in P then r is an MLG of Ψ(r).
Proof. Regarding the second statement, note that all synthesis operators, including χinit,
by deﬁnition only introduce rules for deﬁned functions f ∈Φ and possibly for new deﬁned
functions g with g ̸∈DB. With DΦ ∩DB = ∅then follows that DP ∩DB = ∅.
113

4. The Igor2 Algorithm
We prove statements 1 and 3 by induction over the length n of a path to reach ⟨P, Ψ⟩
from an initial node I of PΦ,B.
Induction base (n = 0): In this case, ⟨P, Ψ⟩is an initial node in PΦ,B, i.e., P is an
initial CS of Φ. All three statements are then true by Lemma 4.1.
Induction step (n →n + 1): Let I be an initial node in PΦ,B and ⟨P, Ψ⟩∈Ξn
B(I) be a
node in PΦ,B that is reachable by a path of length n from I. By induction assumption,
all three statements are true for ⟨P, Ψ⟩.
Let r ∈P be an open rule in P, ⟨s, φnew⟩∈χB(r, Φ) be one corresponding successor-
rule set and new speciﬁcation, and ⟨P ′, Ψ′⟩∈ΞB(⟨P, Ψ⟩) be the corresponding successor
CS and successor speciﬁcation of ⟨P, Ψ⟩, hence reachable from I by a path of length
n + 1.
• Statement 1: If ⟨P, Ψ⟩
χsplit
−−−→⟨P ′, Ψ′⟩or ⟨P, Ψ⟩
χsmplCall
−−−−−→⟨P ′, Ψ′⟩, then φnew = ∅
and DP ′ = DP and DΨ′ = DΨ′, hence DP ′ = DΨ′ by induction assumption.
If ⟨P, Ψ⟩
χsub
−−−→⟨P ′, Ψ′⟩or ⟨P, Ψ⟩
χcall
−−−→⟨P ′, Ψ′⟩, then DP ′ = DP ∪DPnew and DΨ′ =
DΨ′ ∪Dφnew (where s = {r′} ∪Pnew and Pnew is the initial CS of φnew). Since Pnew
is an initial CS of φnew, DPnew = Dφnew, hence DP ′ = DΨ′ by induction assumption.
• Statement 3: If ⟨P, Ψ⟩
χsplit
−−−→⟨P ′, Ψ′⟩, each rule r′ ∈s is an MLG of Ψ′(r′) by
Lemma 4.2. Since Ψ = Ψ′ in this case and by the induction assumption, also each
rule r′ ∈P ′ \ s is an MLG of Ψ′(r′).
If ⟨P, Ψ⟩
χsmplCall
−−−−−→⟨P ′, Ψ′⟩the open rules of P ′ are identical to those in P, except
for the selected open rule r ∈P that is replaced by the only (closed) successor rule
in P ′. Furthermore, Ψ′ = Ψ. Hence, by induction assumption, for each open rule
r ∈P ′, r is an MLG of Ψ′(r).
If ⟨P, Ψ⟩
χsub
−−−→⟨P ′, Ψ′⟩or ⟨P, Ψ⟩
χcall
−−−→⟨P ′, Ψ′⟩, the rules r′ ∈Pnew are MLGs
of their associated speciﬁcation subsets Ψ′(r′) by Lemmata 4.3 and 4.5.
Since
DΨ ∩Dφnew = ∅and by the induction assumption, also all rules r′ ∈P ′ \ s are
MLGs of Ψ′(r′).
4.8.2. Termination and Completeness of Igor2’s Search
We now ﬁrst show that the problem space has no loops and cycles (Lemma 4.8) and is,
under a certain restriction (Deﬁnition 4.19), ﬁnite (Lemma 4.10). From these results we
conclude termination of Igor2 (Theorem 4.1).
Furthermore we show that under another restriction (Deﬁnition 4.20) the order in
which open rules in a candidate CS are replaced does not aﬀect the resulting (closed)
CS and that it is thus suﬃcient that Igor2 in each search-step only computes successor
CSs with respect to one arbitrary open rule in a CS (Lemma 4.11) instead of introducing
all possible successor CSs (with respect to all open rules). From this result we conclude
completeness of Igor2’s search strategy. (Theorem 4.2).
114

4.8. Properties of the Igor2 Algorithm
In order to show that PΦ,B is acyclic, we need the result that candidate CSs are
orthogonal.
Lemma 4.7 (Orthogonality of candidate CSs). Let ⟨P, Ψ⟩be a node in PΦ,B. Then P
is orthogonal.
Proof. Linear LHSs: Only χinit generates LHSs. The other synthesis operators either
only replace RHSs or generate new rules (new LHSs) by invoking χinit.
Since χinit
generates MLGs, generated LHSs are linear.
Pairwise non-unifying LHSs: We show pairwise non-unifying LHSs of P by induction
over the length n of a path to reach ⟨P, Ψ⟩from an initial node I.
Induction base (n = 0): The initial CS of the initial speciﬁcation Φ is orthogonal by
Lemma 4.1.
Induction step (n →n + 1): Let ⟨P, Ψ⟩∈Ξn
B(I) be a node that is reachable by a path
of length n from I. By induction assumption, the LHSs of P are pairwise non-unifying.
Let ⟨P ′, Ψ′⟩∈ΞB(⟨P, Ψ⟩), hence reachable by a path of length n + 1 from I.
If ⟨P, Ψ⟩
χsplit
−−−→⟨P ′, Ψ′⟩and r and s are the correspondingly selected open rule r ∈P
and successor rules computed by χsplit to achieve P ′, then by Lemma 4.2, s is orthogonal.
Furthermore, for each rule r′ ∈s, Lhs(r) ⪰Lhs(r′). Hence, since r is, by induction
assumption non-unifying with all other rules in P, also each r′ ∈s is non-unifying with
all other rules in P. It follows that all rules in P ′ are pairwise non-unifying.
If ⟨P, Ψ⟩
χsmplCall
−−−−−→⟨P ′, Ψ′⟩, then Lhss(P ′) = Lhss(P), hence the LHSs of P ′ are
pairwise non-unifying by induction assumption.
If ⟨P, Ψ⟩
χsub
−−−→⟨P ′, Ψ′⟩or ⟨P, Ψ⟩
χcall
−−−→⟨P ′, Ψ′⟩, then additionally to the LHSs of P,
P ′ contains the LHSs from Pnew (cp. Deﬁnitions 4.12 and 4.14). Due to Lemma 4.1,
the LHSs of Pnew are pairwise non-unifying. Furthermore, DP and DPnew are disjoint.
Together with the induction assumption follows that the LHSs of P ′ are pairwise non-
unifying.
The following lemma states that the program space does not contain loops and circles.
The non-existence of loops is quite obvious: The open rule selected in a candidate CS
P to compute successor CSs P ′ is not contained in P ′, hence P ̸= P ′. The idea to prove
non-existence of circles is to show that P ′ contains a rule which is not contained in P and
which (or at least any of its successor rules also not contained in P) cannot be removed
again in further synthesis steps.
Lemma 4.8. The problem space PΦ,B contains no loops (arcs of the form (N, N)), and
no cycles, (paths of at least length 2 with the same start and end node).
Proof. Let (⟨P0, Ψ0⟩, ⟨P1, Ψ1⟩) be any arc in PΦ,B and let r0 be the corresponding se-
lected open rule in P0.
Then, by Corollary 4.5 and Deﬁnition 4.16, r0 ̸∈P1, hence
P0 ̸= P1, hence PΦ,B contains no loops.
Now assume further that there is a path P = (⟨P1, Ψ1⟩, ⟨P2, Ψ2⟩, . . . , ⟨Pk, Ψk⟩) with
⟨Pk, Ψk⟩= ⟨P0, Ψ0⟩, i.e., that PΦ,B contains a cycle.
115

4. The Igor2 Algorithm
First assume ⟨P0, Ψ0⟩
χsplit
−−−→⟨P1, Ψ1⟩.
Then P1 contains a rule r′ with Lhs(r0) ≻
Lhs(r′). It holds r′ ̸∈P0, because otherwise, since r0 ∈P0, P0 would not be orthogonal.
Due to r0 ∈P0 and orthogonality of P0, there is no rule r in P0 such that Lhs(r′) ⪰
Lhs(r).
Yet because of r′ ∈P1, each CS that is reachable from P1 contains such a
rule r (due to Corollary 4.5 and Deﬁnition 4.17). In particular, Pk = P0 must contain
such a rule r. Hence we have r ̸∈P0 and r ∈P0 for a rule r with Lhs(r′) ⪰Lhs(r)—
contradiction!
For all three other cases— ⟨P0, Ψ0⟩
χsub
−−−→⟨P1, Ψ1⟩, ⟨P0, Ψ0⟩
χsmplCall
−−−−−→⟨P1, Ψ1⟩, or
⟨P0, Ψ0⟩
χcall
−−−→⟨P1, Ψ1⟩—let r′ be the closed successor-rule of r0 with Lhs(r′) = Lhs(r0)
(cp. Deﬁnitions 4.12, 4.13, and 4.14).
Since Lhs(r′) = Lhs(r0) and r0 ∈P0, from
orthogonality of P0 follows r′ ̸∈P0. Yet whenever ⟨P ′, Ψ′⟩∈ΞB(⟨P, Ψ⟩), then all closed
rules from P are also in P ′ (this follows immediately from Deﬁnition 4.16). Hence we
have r′ ∈Pi for each i ∈[k]. In particular, r′ ∈Pk(= P0); hence r′ ̸∈P0 and r′ ∈P0—
contradiction!
Hence the path P = (⟨P0, Ψ0⟩, ⟨P1, Ψ1⟩, . . . , ⟨Pk−1, Ψk−1⟩, ⟨P0, Ψ0⟩) does not exist.
Due to the operator χcall, the problem space is not necessarily ﬁnite. χcall introduces
new functions as arguments of function calls. These new functions again may call other
functions leading to another set of new argument-computing functions and so forth ad
inﬁnitum. Potentially, this leads to non-termination.
Furthermore, since introducing a function call as RHS of an open rule does not increase
the cost of the candidate CS, possibly inﬁnitely many (open) candidate CSs of one and
the same cost g exist. This may lead to incompleteness (if each closed CS has a higher
cost g + g′).
Therefore, we introduce the concept of depth of a rule which reﬂects its depth in
a sequence of nested function calls. If we bound the depth from above by any natural
number, then the problem space becomes ﬁnite and Igor2 is terminating and complete.8
Deﬁnition 4.19 (Depth of rules). Let ⟨P, Φ⟩be an initial node of PΦ,B. Then for all
rules r ∈P, Depth(r, ⟨P, Φ⟩) = 0.
Now let ⟨P, Ψ⟩be any node in PΦ,B, (r : f(p) →t) ∈P be an open rule, ⟨s, ψ⟩∈
χB(r, Ψ) be some successor rule-set (and corresponding new speciﬁcation), and ⟨P ′, Ψ′⟩=
⟨(P \ {r}) ∪s, Ψ ∪ψ⟩be the corresponding successor node in PΦ,B.
1. For all r′ ∈P ∩P ′, Depth(r′, ⟨P ′, Ψ′⟩) := Depth(r′, ⟨P, Ψ⟩).
2. If ⟨s, ψ⟩∈χsplit(r, Ψ) ˙∪χsub(r, Ψ, B) ˙∪χsmplCall(r, Ψ, B), then for each rule in r′ ∈
s, Depth(r′, ⟨P ′, Ψ′⟩) := Depth(r, ⟨P, Ψ⟩).
3. If ⟨{f′(g1(p), . . . , gn(p))} ∪Pnew, ψ⟩∈χcall(r, Ψ, B), then
Depth(f′(g1(p), . . . , gn(p)),

P ′, Ψ′
) := Depth(r, ⟨P, Ψ⟩)
8Probably a better, more general, solution would be to also considering the depth of a rule as a measure
of the cost of a candidate CS.
116

4.8. Properties of the Igor2 Algorithm
and for each rule r′ ∈Pnew,
Depth(r′,

P ′, Ψ′
) := Depth(r, ⟨P, Ψ⟩) + 1 .
The following lemma shows that in each path in PΦ,B, where the depth of all corre-
sponding selected open rules is equal to or greater than a ﬁxed natural number m, only
ﬁnitely many of the selected rules have a depth equal to m.
Lemma 4.9. Let m ∈N be any natural number. Let P = (⟨P0, Ψ0⟩, ⟨P1, Ψ1⟩, . . .) be
a path in PΦ,B and ri ∈Pi (i = 0, 1, . . .) be the respective selected open rules, such
that for all i = 0, 1, . . ., Depth(ri, ⟨Pi, Ψi⟩) ≥m. Then for only ﬁnitely many j ∈N,
Depth(rj, ⟨Pj, Ψj⟩) = m.
Proof. For any node ⟨Pi, Ψi⟩at path P let Om,i ⊆Pi denote the set of open rules of Pi
with depth m:
Om,i = {r ∈Pi | r open and Depth(r, ⟨Pi, Ψi⟩) = m} .
For the following, we need two additional auxiliary concepts:
1. Let Φ be any speciﬁcation and R be a set of rules. Then the speciﬁcation subset of
Φ associated with R is deﬁned as the union of the speciﬁcation subsets associated
with the single rules in R:
Φ(R) :=
[
r∈R
Φ(r) .
2. Let R be any set of rules. Then by Rhss(R) we denote the multi-set of all RHSs in
R. (That is, if any RHS appears more then once in R, then all multiple occurrences
are elements of Rhss(R)).
Now we consider two quantities based on Om,i:
1. Size(Rhss(Ψi(Om,i))): The summerized number of positions in all RHSs belonging
to the speciﬁcation subset of Ψi associated with the set of open rules of depth m
in Pi.
2.
| Ψi(Om,i) |
| Om,i |
: The number of rules in the speciﬁcation subset associated with the set
of open rules of depth m in Pi divided by the number of open rules of depth m in
Pi.
At ﬁrst we consider how Size(Rhss(Ψi(Om,i))) behaves for i = 0, 1, . . .:
Let (⟨Pj, Ψj⟩, ⟨Pj+1, Ψj+1⟩) be any arc in path P. We distinguish the following four (1,
2a, 2b, 3) exhaustive cases:
1. Depth(rj, ⟨Pj, Ψj⟩) > m: Then Om,j = Om,j+1, hence Ψj(Om,j) = Ψj+1(Om,j+1),
hence Size(Rhss(Ψj(Om,j))) = Size(Rhss(Ψj+1(Om,j+1))).
2. Depth(rj, ⟨Pj, Ψj⟩) = m and ⟨Pj, Ψj⟩
χsplit
−−−→⟨Pj+1, Ψj+1⟩: Then either
117

4. The Igor2 Algorithm
a) at least one of the more speciﬁc successor rules of rj is closed such that
| Ψj(Om,j) | > | Ψj+1(Om,j+1) | , hence
Size(Rhss(Ψj(Om,j))) > Size(Rhss(Ψj+1(Om,j+1))) or
b) all successor rules are also open such that Ψj(Om,j) = Ψj+1(Om,j+1), hence
Size(Rhss(Ψj(Om,j))) = Size(Rhss(Ψj+1(Om,j+1))).
3. Depth(rj, ⟨Pj, Ψj⟩) = m and ⟨Pj, Ψj⟩
χsub
−−−→⟨Pj+1, Ψj+1⟩or ⟨Pj, Ψj⟩
χsmplCall
−−−−−→
⟨Pj+1, Ψj+1⟩or ⟨Pj, Ψj⟩
χcall
−−−→⟨Pj+1, Ψj+1⟩. Then
Size(Rhss(Ψj(Om,j))) > Size(Rhss(Ψj+1(Om,j+1))).
In the case of χsmplCall this is quite obvious because χsmplCall simply closes the
selected rule rj, hence Pj+1 has one open rule less, and does not change the spec-
iﬁcation.
In the case of χsub, the selected rule rj is also closed but new initial rules for the
new subfunctions are added with the same depth. For these, new speciﬁcation
rules are added. Yet note that the RHSs of the new speciﬁcation rules are proper
subterms of the RHSs in Ψj(rj) and are in total smaller than the RHSs in Ψj(rj).
In the case of χcall, the selected rule rj is also closed but new speciﬁcation rules
for new subfunctions are added. Yet note that in this case, the initial candidate
rules in Pj+1 for the new subfunctions have a strictly increased depth. Hence their
speciﬁcation rules do not count.
Hence in any case Size(Rhss(Ψj(Om,j))) ≥Size(Rhss(Ψj+1(Om,j+1))) and it follows
immediately that only ﬁnitely many arcs in P can be of cases 2(a) and 3.
We now consider the second quantity for particular subsequences in path P: Let, for
some l ∈N, P ′ = (⟨Pl, Ψl⟩, ⟨Pl+1, Ψl+1⟩, . . .) be a subsequence of P such that each arc
in P ′ is of cases 1 or 2(b). Let (⟨Pj, Ψj⟩, ⟨Pj+1, Ψj+1⟩) be any arc in P ′. If the arc is of
case 1, i.e., Om,j = Om,j+1 and Ψj(Om,j) = Ψj+1(Om,j+1), then obviously
| Ψ(Om,j) |
| Om,j |
= | Ψ(Om,j+1) |
| Om,j+1 |
.
If the arc is of case 2(b), then Om,j < Om,j+1 and Ψj(Om,j) = Ψj+1(Om,j+1), hence
| Ψ(Om,j) |
| Om,j |
> | Ψ(Om,j+1) |
| Om,j+1 |
.
| Om,j | cannot exceed | Ψ(Om,j) | , hence P ′ contains only ﬁnitely many arcs of case
2(b). In general: Between each two of the ﬁnitely many arcs of cases 2(a) and 3 in path
P, only ﬁnitely many arcs of case 2(b) can occur. Hence in P only ﬁnitely many arcs of
cases 2 and 3, where the selected open rule is of depth m, occur.
As a corollary we get the result that each path in PΦ,B only contains ﬁnitely many arcs
where the selected open rules have a depth equal to or smaller than any ﬁxed natural
number n. That is, if the maximal depth of rules in PΦ,B is set to any natural number,
then all paths in PΦ,B are of ﬁnite length.
118

4.8. Properties of the Igor2 Algorithm
Corollary 4.6. Let P = (⟨P0, Ψ0⟩, ⟨P1, Ψ1⟩, . . .) be any path in PΦ,B, ri ∈Pi (i =
0, 1, . . .) be the respective selected open rules, and n ∈N be any natural number. Then
for only ﬁnitely many j ∈N, Depth(rj, ⟨Pj, Ψj⟩) ≤n.
Proof. By induction over n.
Induction base (n = 0): Since the depth of a rule in a node of PΦ,B cannot be smaller
than 0, we have to show that for only ﬁnitely many j ∈N, Depth(rj, ⟨Pj, Ψj⟩) = n. Since
the depth of all selected rules is equal to or greater than 0, this follows immediately from
Lemma 4.9
Induction step (n →n + 1): We assume that Depth(rj, ⟨Pj, Ψj⟩) ≤n for only ﬁnitely
many j ∈N. Hence only ﬁnitely many subsequences P ′ of P exist such all selected open
rules in P ′ have a depth equal to or greater than n + 1. From Lemma 4.9 follows that in
these subsequences P ′ only ﬁnitely many of the selected open rules are of depth n + 1.
Hence also the number of arcs in P with selected open rules of depth n + 1 is ﬁnite.
Lemma 4.10 ((Conditional) ﬁniteness of the problem space). Let Φ and B be an initial
speciﬁcation and a background CS, respectively. If the depth of rules cannot exceed a
ﬁxed natural number M ∈N, then PΦ,B is ﬁnite.
Proof. From Lemma 4.6 follows that all paths in PΦ,B are ﬁnite provided such a maximal
depth M is given.
Since the number of rules in an initial node of PΦ,B and in B is ﬁnite and if ⟨s, φnew⟩∈
χB(r, Ψ) and Ψ is ﬁnite, then also s and φnew are ﬁnite, for each node ⟨P, Ψ⟩of PΦ,B, P
and Ψ are ﬁnite. Then, furthermore, χB(r, Ψ), and therefore also ΞB(⟨P, Ψ⟩), are ﬁnite
for each node ⟨P, Ψ⟩in PΦ,B and each open rule r ∈P. Hence each node in PΦ,B has
only a ﬁnite number of direct successor nodes.
Finally, each node in PΦ,B is reachable from some initial node and there are only
ﬁnitely many initial nodes.
It follows that PΦ,B contains only ﬁnitely many nodes.
Now we can conclude that Igor2 is terminating under the condition that the depth
of rules may not exceed a certain upper bound.
Theorem 4.1 (Termination of Igor2). If the depth of rules cannot exceed a ﬁxed natural
number M ∈N, then Igor2 is terminating.
Proof. Consider Algorithm 4. For an initial speciﬁcation Φ and a background CS B,
Igor2 maintains a subset N of nodes of the problem space PΦ,B. Initially, N contains
one of the initial nodes of PΦ,B. In each iteration of the main loop, one of the nodes
is replaced by a subset (according to one particular selected open rule) of its direct
successor nodes in PΦ,B.
Since the problem space is a ﬁnite (provided a maximal depth of rules is given) di-
rected acyclic and loop-free graph (Deﬁnition 4.17 and Lemmata 4.8 and 4.10), only
ﬁnitely many of such replacements exist and hence, the loop has only a ﬁnite number of
iterations.
119

4. The Igor2 Algorithm
Now we consider completeness of Igor2’s search.
If Igor2 initially inserted all initial nodes of PΦ,B to its set N of maintained nodes
(possibly there are more than one initial node due to the non-uniqueness of MLGs of sets
of speciﬁcation rules, cp. Section 4.7.1) and in each step replaced one node by all direct
successors, then, obviously, Igor2 would ﬁnd a closed CS, i.e., a solution, if one exists.
However, Igor2 only starts from one arbitrary initial node and in each step computes
successor nodes according to only one arbitrarily selected open rule. (cp. Algorithm 4).
That is, many paths of PΦ,B are pruned in Igor2’s search.
Only considering one initial node and, in general, only one MLG at each step, is
unproblematic because diﬀerent MLGs do not diﬀer regarding whether they are closed.
If one MLG is closed, then all MLGs are closed. Furthermore, each MLG leads to the
same successor-rule sets.
The idea for proving that it suﬃces to only introduce successor CSs regarding one
particular open rule instead of all open rules at each step, is that the order in which
open rules are replaced is not relevant but that the crucial point is simply that all open
rules are eventually replaced. That is, it does not matter with which rule we start and
hence, we can indeed start with an arbitrary one and leave the remaining open rules for
later steps.
However, this independence regarding the order of selecting open rules requires a fur-
ther restriction with respect to the function call operators. According to Deﬁnitions 4.13
and 4.14, each function of a speciﬁcation can be called. However, speciﬁcations grow
along a path in the problem space due to new introduced subfunctions. That is, if an
open rule is replaced at a particular arc instead of at some previous arc, then its open
RHS can potentially be replaced by calls of more functions that were not introduced
previously. Therefore, we introduce a concept that restricts the functions that can be
called as replacement for an open RHS by those functions that are already introduced at
the time when the open rule itself is introduced. Hence all functions that are introduced
in later steps do not aﬀect the set of callable functions of this rule.
Deﬁnition 4.20 (Callable functions). Let ⟨P, Φ⟩be an initial node of PΦ,B.
Then
Callable(r, ⟨P, Φ⟩) = DΦ∪B for each rule r ∈P.
Now let ⟨P, Ψ⟩be any node in PΦ,B, r ∈P be an open rule, ⟨s, ψ⟩∈χB(r, Ψ) be some
successor rule-set (and corresponding new speciﬁcation), and
⟨P ′, Ψ′⟩= ⟨(P \ {r}) ∪s, Ψ ∪ψ⟩be the corresponding successor node in PΦ,B.
• For all r′ ∈P ∩P ′, Callable(r′, ⟨P ′, Ψ′⟩) := Callable(r′, ⟨P, Ψ⟩).
• If ⟨s, ∅⟩∈χsplit(r, Ψ) ˙∪χsmplCall(r, Ψ, B) then for each rule r′ ∈s,
Callable(r′, ⟨P ′, Ψ′⟩) := Callable(r, ⟨P, Ψ⟩).
• If ⟨{r′} ∪Pnew, ψ⟩∈χsub(r, Φ, B) ∪χcall(r, Φ, B) then
– Callable(r′, ⟨P ′, Ψ′⟩) := Callable(r, ⟨P, Ψ⟩) and
– for each r′′ ∈Pnew, Callable(r′′, ⟨P ′, Ψ′⟩) := Callable(r, ⟨P, Ψ⟩)∪{Deﬁnes(r′′)}.
120

4.8. Properties of the Igor2 Algorithm
If χsmplCall and χcall are restricted to Callable(r, ⟨P, Ψ⟩), then the order of replacing
open rules does not aﬀect the resulting CS.
Lemma 4.11. For each open rule r and each node N let χsmplCall and χcall be restricted
to only introduce calls to functions from Callable(r, N).
Now let N be an open node in PΦ,B and r be any open rule in N. Let C ∈Ξ∗
B(N)
be a closed node in PΦ,B that is reachable from N. Then there exists a direct successor
N′ ∈ΞB(N) of N according to rule r such that C ∈Ξ∗
B(N′).
Proof. Let N = ⟨P0, Ψ0⟩, . . . , C = ⟨Pn, Ψn⟩be a path from N to the closed node C and
let, for i = 0, . . . , n −1, ⟨ri, si⟩be the selected open rule of Pi and the successor rule set
leading to the arc (⟨Pi, Ψi⟩, ⟨Pi+1, Ψi+1⟩).
For one k ∈{0, . . . , n −1}, r = rk. We have
Pk+1 = (((((((P0 \ {r0}) ∪s0) \ {r1}) ∪s1) \ · · · \ {rk−1}) ∪sk−1) \ {rk}) ∪sk
= ((((((P0 \ {rk}) \ {r0}) ∪s0) \ {r1}) ∪s1) \ · · · \ {rk−1}) ∪sk−1 ∪sk
= (((((((P0 \ {rk}) ∪sk) \ {r0}) ∪s0) \ {r1}) ∪s1) \ · · · \ {rk−1}) ∪sk−1
(4.1)
Furthermore, if χsmplCall and χcall can only introduce calls of functions in
Callable(rk, ⟨Pk, Ψk⟩, then χcall(rk, Ψk, B) = χcall(rk, Ψ0, B), hence ⟨sk, φk⟩∈χB(r, Ψ0).
Theorem 4.2 (Completeness of Igor2’s search with respect to PΦ,B). If, given a
candidate CS P, a corresponding speciﬁcation Ψ, and an open rule r ∈P, χsmplCall and
χcall only may introduce calls of functions f′ ∈Callable(r, ⟨P, Ψ⟩), then, if there is a
closed CS in PΦ,B, Igor2 returns a closed CS.
Proof. By contraposition: If Igor2 returns the empty set, then there is no closed CS in
PΦ,B.
In each iteration of the main loop of Algorithm 4, N contains a subset of nodes of
PΦ,B. Igor2 returns the empty set if and only if N becomes empty. In the following we
show that in each iteration of the loop, for each closed node C in PΦ,B there is a node
N in N, such that C (or some MLG-variant of it, that is then also closed) is reachable
from N. Hence if N becomes empty, there cannot be a closed CS in PΦ,B.
We show this by induction over the number n of iterations of the loop:
Induction base (n = 1): In the ﬁrst iteration, N contains an initial node of PΦ,B.
Since all nodes in PΦ,B (or at least an MLG-variant of each node) is reachable from it
follows that a closed node is reachable if one exists.
Induction step (n →n + 1): Let, in iteration n, N contain a subset of nodes of PΦ,B
such that for each closed node C in PΦ,B there is a node N in N, such that C (or an
MLG-variant) is reachable from N (induction assumption).
Let now N be such a node.
If a node N′ ̸= N is replaced in iteration N, then N ∈N also in iteration n + 1.
Now assume N itself is replaced. Then clearly, there is a successor N′ of N such that
C (or an MLG-variant) is reachable from N′. Now let r be any open rule in N. With
121

4. The Igor2 Algorithm
Lemma 4.11 follows that C (or an MLG-variant) is reachable from some successor of N
according to rule r. Hence each closed node that is reachable from N is also reachable
from some successor node of N′ of N according to any selected open rule r in N, that
is included in N in iteration n + 1.
4.8.3. Soundness of Igor2
In this section we show that Igor2 is sound with respect to the induction problem
stated in Deﬁnition 4.3, i.e., that each generated closed candidate CS P satisﬁes all three
conditions—orthogonality, disjoint deﬁned functions with respect to the background CS,
and correctness with respect to the speciﬁcation—stated in Deﬁnition 4.3.
Orthogonality and disjoint functions with respect to the background CS of all gener-
ated candidate CSs, hence, in particular of all closed CSs, have already been shown in
Lemmata 4.7 and 4.6, respectively.
In the lemmata for the single synthesis operators (Lemmata 4.2, 4.3, 4.4, 4.5) we
already showed that, given a candidate CS with corresponding speciﬁcation ⟨P, Φ⟩and
a background CS B, all generated successor rules are extensionally correct with respect
to Φ ∪φnew ∪B, where φnew denotes the new speciﬁcation for possibly new introduced
subfunctions.
Furthermore we already showed in Lemma 4.1 that initial candidate CSs are exten-
sionally correct with respect to their speciﬁcations.
Based on these results, the following lemma shows that generated candidate CSs are
(together with the provided background CS) extensionally correct with respect to their
corresponding speciﬁcations (and the background CS).
Lemma 4.12 (Extensional correctness of candidate CSs). Let Φ and B be an initial
speciﬁcation and a background CS, respectively. Let ⟨P, Ψ⟩be any node in PΦ,B. Then
P ∪B is extensionally correct with respect to Ψ ∪B.
Proof. By induction over the length n of a path from an initial node I in PΦ,B to the
node ⟨P, Ψ⟩.
Induction base (n = 0): The node ⟨P, Ψ⟩is an initial node in this case, hence P is
the initial CS of Ψ = Φ. By Lemma 4.1, P is extensionally correct with respect to Ψ.
With DP ∩DB = ∅and Corollaries 4.2 and 4.3 follows that P ∪B is extensionally
correct with respect to Ψ ∪B.
Induction step (n →n + 1): Let ⟨P, Ψ⟩∈Ξn
B(I) be a node reachable by a path of
length n from an initial node. By induction assumption, P ∪B is extensionally correct
with respect to Ψ ∪B.
Let ⟨P ′, Ψ′⟩∈ΞB(⟨P, Ψ⟩), hence reachable by a path of length n+1, and let r ∈P be
the corresponding selected open rule of P and ⟨s, φnew⟩∈χB(r, Ψ) be the corresponding
successor candidate-rule set and the new speciﬁcation that lead to ⟨P ′, Ψ′⟩.
We now separately prove for each case—⟨s, φnew⟩∈χsplit, ⟨s, φnew⟩∈χsub, ⟨s, φnew⟩∈
χsmplCall, ⟨s, φnew⟩∈χcall—(i) extensional correctness of the single rules of P ′ with
122

4.8. Properties of the Igor2 Algorithm
respect to Ψ′ ∪B and (ii) completeness of P with respect to Ψ′. Extensional correctness
of P ∪B with respect to Ψ∪B then immediately follows from (i) and (ii) and DP ∩DB = ∅.
Extensionally correct rules: Assume ⟨s, φnew⟩∈χsplit(r, Ψ). Due to Lemma 4.2, all
rules in s are extensionally correct with respect to Ψ = Ψ′, hence also with respect to
Ψ′ ∪B (since DΨ′ ∩DB = ∅). By induction assumption and since (P ′ \ s) ⊆P, all rules
in P ′ \ s are extensionally correct with respect to to Ψ′ ∪B. Hence all rules in P ′ are
extensionally correct with respect to to Ψ′ ∪B.
If ⟨s, φnew⟩∈χsmplCall(r, Ψ, B), we have Ψ = Ψ′. Therefore, the rules in P ∩P ′ = P ′\s,
are extensionally correct with respect to Ψ′ ∪B by induction assumption. Furthermore,
we have | s | = 1 and this only successor rule is extensionally correct with respect to
Ψ′ ∪B by Lemma 4.4.
If ⟨s, φnew⟩∈χsub(r, Ψ, B) or ⟨s, φnew⟩∈χcall(r, Ψ, B), we have Ψ′ = Ψ ∪φnew.
The function symbols of φnew and P are disjoint. Therefore, all rules in P ∩P ′ are
extensionally correct with respect to Ψ′ ∪B by induction assumption. The rules s are
extensionally correct with respect to Ψ′ ∪B by Lemmata 4.3 and 4.5.
Completeness: In the case of χsplit, each LHS in Ψ(r) = Ψ′(r) matches a LHS in s
and each LHS in Ψ′ \ Ψ′(r) matches a LHS in P \ {r}, hence also in P ′ \ s. Hence each
LHSs in Ψ′ matches a LHS in P ′.
For χsub, χsmplCall, and χcall holds that all LHSs of P are also in P ′, hence P ′ is
complete with respect to Ψ. In the case of χsmplCall, Ψ = Ψ′, hence P ′ is complete with
respect to Ψ′. For χsub and χcall, the rules Pnew are, by Lemma 4.1, complete with
respect to φnew. Hence P ′ is complete with respect to Ψ′.
From the result that candidate CSs are extensionally correct and the fact that χcall
and χsmplCall reduce arguments of function calls with respect to a reduction order, we
can conclude that candidate CSs, together with the provided background CS, are correct
according to Deﬁnition 4.2 with respect to their speciﬁcations.
Theorem 4.3 (Correctness of candidate CSs). Let Φ and B be an initial speciﬁcation
and a background CS, respectively. Let ⟨P, Ψ⟩be any node in PΦ,B. Then P ∪B is
correct with respect to Ψ according to Deﬁnition 4.2.
Proof. By contradiction: Suppose P ∪B is not correct with respect to Ψ. Then there is a
rule (f0(i0) →o0) ∈Ψ such that—due to extensional correctness of P ∪B with respect to
Ψ ∪B—f0(i0) →P t0 and t0
∗→Ψ∪B o0, but—because P ∪B is not correct—t0 ̸ ∗→P∪B o0.
Now let s1 be the result of rewriting t0 by P ∪B, i.e., t0
∗→P∪B s1, where only those
redexes are reduced that are correctly normalized by P ∪B (i.e., which would be reduced
by Ψ ∪B to the same, in Ψ ∪B speciﬁed, normal form), until there is no further redex
which is correctly normalized by P ∪B. We then have s1 ̸= o0, because otherwise, we
would have t0
∗→P∪B o0, contradicting the assumption that P ∪B is not correct for t0.
In particular, s1 cannot be in normal form because t0 was rewritten in accordance with
Ψ ∪B and Ψ ∪B is conﬂuent, such that, if s1 were in normal form, we would have
s1 = o0. But, again since t0 was rewritten in accordance with Ψ ∪B and t0
∗→Ψ∪B o0,
we also have s1
∗→Ψ∪B o0.
123

4. The Igor2 Algorithm
It follows that there is a rule (f1(i1) →o1) ∈Ψ (possibly f1 = f0) such that s1 =
C[f1(i1)σ1] for some context C and substitution σ1. Since P ∪B is extensionally correct,
f1(i1)σ1 →P t1 and t1
∗→Ψ∪B o1σ1, but since P ∪B is not correct, and in particular since
s1 does not contain any redex that is correctly normalized by P ∪B, t1 ̸ ∗→P∪B o1σ1.
Now—analogously to t0 and s1—let s2 be the result of rewriting t1 by P ∪B, i.e.,
t1
∗→P∪B s2, where only those redexes are reduced that are correctly normalized by
P ∪B, until there is no further redex which is correctly normalized by P ∪B.
For the same argument as for t0, s1 above, we then have a rule (f2(i2) →o2) ∈Ψ,
s2 = C[f2(i2)σ2] for some (other) context C and a substitution σ2, f2(i2)σ2 →P t2,
t2
∗→Ψ∪B o2σ2, and t2 ̸ ∗→P∪B o2σ2.
We again rewrite, analogously to t0 and t1, t2 to a term s3 which must contain a
redex f3(i3)σ3 and so forth—achieving an inﬁnite reduction and sequence of such triples
⟨t0, s1, f1(i1)σ1⟩, ⟨t1, s2, f2(i2)σ2⟩, ⟨t2, s3, f3(i3)σ3⟩, . . .. Since for all i = 0, 1, . . ., fi ∈DΨ,
fi(ii)σi is a redex according to P ∪B, and DΨ and DB are disjoint, we have fi ∈DP
(and fi ̸∈DB).
We now show that such an inﬁnite reduction by P is not possible. Let p1, p2, . . . be
positions denoting the subterms f1(i1)σ1, f2(i2)σ2, . . . in the results s1, s2, . . . of rewriting
the terms t0, t1, . . ., respectively; i.e., si|pi = fi(ii)σi for all i = 1, 2, . . .. Reduction steps
in the reductions ti−1
∗→P∪B si can only occur at positions parallel to or below the pi,
i.e., at positions p with p ∥pi or p > pi, yet not at positions above the pi (p < pi). This
follows from the fact that—by assumption—all reduced redexes are correctly reduced,
hence to their correct constructor terms as deﬁned by Ψ. Hence all subterms of the si
at positions below any rewritten position are constructor terms.
It follows that Node(t0, p1) = f1, Node(t1, p2) = f2 etc., hence each fi+1 is already
introduced by the rewrite step fi(ii) →P ti.
The synthesis operators that generate rules which have such function calls in their
RHSs are χsub, χsmplCall, and χcall. In the case of χsub, such a rule has the form f(p) →
c(s1, . . . , sn) where the sn are either constructor terms or function calls of the form g(p)
(f ̸= g). Hence if a reduction step fi(ii)σi →P ti is based on such a rule, we have
iiσi = ii+1σi+1.
In the case of χsmplCall, synthesized rules have the form f(p) →f′(p′), where f(p)σ >
f′(p′)σ, if, as it is the case here, fi ∈DP , for each f(p)σ ∈Lhss(Ψ). Hence in this case
we have fi(ii)σi > fi+1(ii+1)σi+1.
Finally, in the case of χcall, the synthesized rules containing function calls have the form
f(p1, . . . , pn) →f′(g1(p), . . . , gn(p)). The gi(p)σi are correctly reduced to constructor
terms by assumption. Again, since fi ∈DP and due to the deﬁnition of χcall, we then
have fi(ii)σi > fi+1(ii+1)σi+1.
Since > is a well-founded order, we can only have ﬁnitely many k with ik > ik+1.
The sequences between the ﬁnitely many indices k representing rewrite steps according
to rules introduced by χsmplCall or χcall, are all, as we have seen, caused by rules intro-
duced by χsub. Yet such rules do not lead to recursion, because the function symbols g
in RHSs introduced by χsub are always new function symbols not occurring in the CS
up to the invention by χsub. Since DP is ﬁnite, we thus can also have only ﬁnitely many
124

4.8. Properties of the Igor2 Algorithm
k with ik = ik+1,i.e., only ﬁnitely many indices k representing rewrite steps according
to rules introduced by χsub. Hence the complete constructed reduction must be ﬁnite
which contradicts the result that it must be inﬁnite if P ∪B is not correct with respect
to Ψ, hence P ∪B is correct with respect to Ψ.
Now that closed candidate CSs are solutions according to the induction problem (Def-
inition 4.3) follows as a corollary.
Corollary 4.7 (Soundness of Igor2 with respect to the induction problem). Let ⟨P, Ψ⟩
be a closed node in PΦ,B. Then P is a solution CS according to Deﬁnition 4.3.
Proof. Conjunction of Lemmata 4.7 and 4.6 and Theorem 4.3. (Note that from that
P ∪B is correct with respect to Ψ, as proven in Theorem 4.3, immediately follows
that P ∪B is correct with respect to Φ, as required by Deﬁnition 4.3, because for all
speciﬁcations Ψ occurring in the nodes of PΦ,B, Φ ⊆Ψ because the synthesis operators
only add new speciﬁcation rules but never delete rules from a speciﬁcation.)
4.8.4. Concerning Completeness with Respect to Certain Function Classes
In Section 4.8.2 we showed that the search strategy of Igor2 is complete with respect
to its own program class, i.e., that if there exists a solution CS that can be reached from
an initial candidate by a ﬁnite sequence of applications of the synthesis operators, then
Igor2 will ﬁnd a solution CS.
Another interesting question is, which classes of computable functions are generatable
at all by means of the described synthesis operators, i.e., which classes of computable
functions are representable by the CSs in Igor2’s problem space PΦ,B.
We have not yet substantially studied this question. However, we can give some ﬁrst
insights.
Igor2’s problem space varies with respect to the provided initial speciﬁcation Φ and
background CS B.
It is “restricted” (i.e., excludes certain computable functions) in
that it only includes functions that have the I/O examples speciﬁed in Φ as a subset
of their graphs. Yet this is not a defect, but a feature compared to generate-and-test
methods, where the problem space is independent from the speciﬁcation and hence many
functions that do not compute the speciﬁcation correctly are enumerated until a function
that passes the test is found. In particular, this “restriction” does not in principle rule
out any computable function, because, certainly, for any computable function we may
provide some subset of its graph as speciﬁcation Φ.
The second parameter to bias PΦ,B is the background knowledge B. We consider two
extremes:
1. The target function itself is already given by B.
2. No background knowledge is given; B = ∅.
Due to the possibility of the really trivial ﬁrst case, one can say that Igor2 can synthe-
size each computable function—provided it is already appropriately given as background
125

4. The Igor2 Algorithm
knowledge. Speciﬁcation Φ and background CS B are equal in this case (or B might also
be a superset of Φ), except for that the (same) function speciﬁed in Φ and B has diﬀerent
names, say f and f′, respectively, in both sets (because of the requirement DΦ ̸= DB).
In this case, the initial candidate consists of one rule, the MLG of the I/O examples for
f in Φ. In the ﬁrst iteration of the search loop, χsmplCall then will succeed and return
the rule
f (x) →f ’ (x) ,
just picking the target function from the background CS.
Of course, Igor2 has not really synthesized an (unknown) algorithm for a computable
function in this case because it is presupposed, that the function is already implemented.
This trivial case is, however, good enough to see that it might strongly depend on the
provided background knowledge whether some function is generatable or not.
Now the second extreme case is that no background knowledge is given, i.e., that the
only (initial) primitives to be used to synthesize a program are the constructors of the
data type; e.g., 0 (zero) and S (successor) in the case of the natural numbers N.
Can Igor2 in this case synthesize each computable function f : N →N? Or each
primitive recursive function? Or some other class? The question of exactly which class
of functions is generatable without background knowledge is open. What we know is
that (at least one) functions that are not primitive recursive can be synthesized, e.g., the
Ackermann function (see Chapter 5). Further we know that particular functions that
are primitive recursive can not be synthesized (without background knowledge), e.g.,
multiplication of two natural numbers.
Consider the Ackermann function (as CS with constructors 0 and S , meaning +1):
1
A (0, n)
→S n
2
A (S m, 0)
→A (m, S 0)
3
A (S m, S n) →A (m, A (S m, n))
From the initial candidate,
A (x, y) →S z
the above deﬁnition can be synthesized by two applications of χsplit to get the splitting
into three rules. The ﬁrst equation/rule is then simply the MLG of its speciﬁcation
subset. The second rule is ﬁnished by one call of χsmplCall, introducing the recursive call
with constructor term arguments m and S 0. Finally the third rule is synthesized by
ﬁrst applying χcall to get the recursive call at the root. The two arguments m and the
inner recursive call are at ﬁrst introduced as new subfunctions:
A (S m, S n) →A (sub1 (S m, S n), sub2 (S m, S n)) .
Their initial rules are:
sub1 (S m, S n) →m
sub2 (S m, S n) →S z
The initial rule for sub1 is already closed (because the actual argument computed by it
is only the constructor term m). The initial rule for sub2 is open. It can be ﬁnished
126

4.8. Properties of the Igor2 Algorithm
by a further application of χsmplCall, introducing the (recursive) call of the Ackermann
function with constructor terms as arguments:
sub2 (S m, S n) →A (S m, n) .
Now consider multiplication M of two natural numbers (where A denotes addition):
1
M (0, n)
→0
2
M (S m, n) →A (M (m, n), n)
3
A (0, n)
→n
4
A (S m, n) →A (m, S n)
If A was given as background knowledge, then M could be induced. Also if both M and
A are speciﬁed they could be synthesized in parallel, leading to the above deﬁnition. Yet if
A is speciﬁed neither as target- nor as background function, then it must automatically
be invented as subfunction.
As we have seen, Igor2 can automatically invent new
subfunctions (by χsub and χcall), e.g., if one speciﬁes the reverse function on lists, then
Igor2 automatically invents the init and last functions. However, the scheme of how
calls of target, i.e., speciﬁed functions and new subfunctions are nested (or composed)
is syntactically restricted due to the deﬁnitions of χsub and χcall.
Consider again the induced recursive rule for reverse of Listing 4.4.
reverse (x : xs) →last (x : xs) : reverse ( init (x : xs))
Now compare the second (recursive) rule for M above and this (recursive) rule for
reverse. In the recursive rule for M, M is called within (as an argument) of the call of A,
i.e., the non-speciﬁed subfunction to be invented. In contrast, in the recursive rule for
reverse, reverse is not called within the call of a subfunction, but, the other way round,
init (an unspeciﬁed subfunction) is called within the recursive call of reverse.
The ﬁrst form, calls of non-speciﬁed subfunctions within (recursive) calls of target
functions, cannot be synthesized. Such subfunctions must rather be speciﬁed somehow,
either as background knowledge or as further target function.
To see why this form cannot be synthesized, recall the deﬁnitions of χsub and χsmplCall
(the only two operators that introduce new subfunctions, like A in the case of M, if A is
not speciﬁed somehow). The operator χsub only deals with proper subterms, i.e., only
introduces calls to new functions as arguments of a constructor at the root of a RHS.
Yet A, the subfunction to be introduced, is called at the root of the respective RHS. The
operator χcall, in fact, introduces function calls at the root of RHSs, but only already
known functions. The new subfunctions introduced by χcall are those functions that
compute the arguments of this call. Hence the form of the recursive call in the case of
reverse, reverse ( init (x : xs)), can be synthesized: reverse is already known, init is
the new subfunction that computes the argument. Yet the call of A in the case of M
cannot be synthesized, because A is not known when its call must be introduced, hence
it cannot be called at the root of some (composed or nested) function call introduced by
χcall.
127

4. The Igor2 Algorithm
4.8.5. Concerning Complexity of Igor2
Igor2 conducts a uniform cost search. At each search step, one node of PΦ,B is expanded
yielding several successor nodes. If we assume a maximal branching factor b, a solution
a depth d, and that computing one successor consumes a constant amount of time, then
the time complexity is thus O(bd), i.e., exponential with respect to the depth of the
solution in the search tree.
This general exponential complexity is inherent to (complete, non-greedy) search. The
general goal must be to reduce both b and d by (i) an appropriate selection/restriction
of the problem space (implicitly done by the search operators and their application
preconditions) and (ii) an appropriate selection of heuristics to guide the search in it. In
the case of Igor2, this is mostly subject to future research.
At the moment, unfortunately, b itself is in the worst case exponential, with respect
to the size of the speciﬁcation. The problematic operator is χcall. To see this, recall
the deﬁnition of χcall (Deﬁnition 4.14) and let r, Φ(r), f, f′, and (Φ ∪B)(f′) be the
selected open rule, its associated speciﬁcation subset, the function that r deﬁnes, the
function that is to be called, and the set of speciﬁcation (or background) rules of f′,
respectively. Assume further that Var(l) = Var(r) for each speciﬁcation rule l →r of
f′ (such that condition 1 of Deﬁnition 4.14 is always satisﬁed for f′) and that f′ is a
background function, actually (such that condition 3 does not apply). Then the only
remaining restricting condition for the mapping µ : Φ(r) →B(f) is condition 2; o = o′τ
for RHSs o, o′ of f and f′, respectively.
Now assume further that all RHSs of f′ are single variables. For example, let f′ be
the last function for lists, speciﬁed by:
f ’ (x : [ ])
→x
f ’ (x : y : [ ]) →y
...
Then, indeed, condition 2 is satisﬁed for arbitrary combinations of o, o′.
That is, µ
can be any mapping of Φ(r) →B(f′). The operator χcall, at the moment, generates
one successor CS for each possible mapping µ; hence, in this worst case, the number
of successors equals the number of (total) mappings from Φ(r) →B(f′). This number
is: | B(f′) | | Φ(r) | . Hence, in the worst case, χcall yields exponentially many successor
candidates with respect to the number of speciﬁcation rules (associated with rule r).
Recall that especially the function-call operators χsmplCall and χcall are inspired by
the classical analytical approach to the inductive synthesis of functional programs (as
described in Section 3.2) in that they synthesize (recursive) function calls based on
detecting which speciﬁed outputs of the function to be induced are part of which (other)
outputs of the same or another function. Yet in the described worst case, when the
outputs of the function to be called, consist of single variables such that they subsume
(or contain) arbitrary other outputs of the same type, then this generalized analytical
approach degenerates to an ordinary enumeration of all programs.
128

4.9. Extensions
4.9. Extensions
In this section we sketch three extensions to the basic Igor2 algorithm.
The ﬁrst extension is a further synthesis operator that generates—as a second form of
conditional evaluation amongst pattern matching—conditional rules. Conditional rules
are only rewritten if a speciﬁed condition—a term evaluating to a boolean value—is
satisﬁed. A typical example for such conditions is the test of equality regarding two
pattern variables. For example, a function member, testing whether a particular element
is member of a given list, could be deﬁned by conditional rules as follows:
member (x, [])
→
false
member (x, y : ys) →true
if x = y
member (x, y : ys) →member (x, ys)
if x ̸= y
The second and third rule are conditional rules, the conditions (test for equality) are
the terms introduced by the if keyword. Section 4.9.1 shortly describes the synthesis of
conditional rules.
In contrast to conditional rules, which extend the program class, the second extension
aspires to increase the eﬃciency of the induction by replacing the rule-splitting operator
by a certain variant. The idea is to combine all possible splittings—if more than one
pivot position and hence more than one splitting exists—to one single splitting and
hence to achieve a certain grade of splitting, that would require several applications of
the original splitting operator, in one single step by the new one. We call this variant
rapid rule-splitting and describe it in Section 4.9.2.
The third, most fundamental, extension introduces existentially-quantiﬁed variables
into RHSs of speciﬁcation rules (Section 4.9.3). This allows to leave certain parts of an
output unspeciﬁed. In the extreme case, where the complete RHS is a single existentially
quantiﬁed variable, the speciﬁcation rule just says that the speciﬁed input is member
in the domain of the function but leaves the output completely unspeciﬁed. Hence, this
can also be seen as an attempt to integrate speciﬁcations, that are not complete up to
a certain complexity, into the analytical approach to inductive program synthesis.
4.9.1. Conditional Rules
The only form of conditional evaluation in ordinary term rewriting systems is pattern
matching—a rule is applicable if its LHS matches with some subterm of the subject term.
This form does not allow for direct testing of whether, e.g., the value that matches one
pattern variable is greater than the value matching another pattern variable.
If we
would allow for non-linear LHSs we could test for equality by pattern matching; yet if
we require LHSs to be linear, then we cannot even test for equality directly.
Nevertheless, we can realize these conditions by means of pattern matching. We can
deﬁne predicates such as equality or greater/smaller by ordinary rules and we can deﬁne
an if −then−else−ﬁrule by
if (true , x, y)
→x
if ( false , x, y) →y
129

4. The Igor2 Algorithm
Let equal be a deﬁned function yielding true if two terms are identical and false other-
wise. Then a deﬁnition of the member function from above in terms of ordinary pattern
matching rules would read:
member (x, [])
→
false
member (x, y : ys) →
if (equal (x, y), true , member (x, ys))
However, in order to make Igor2 able to generate functions like member, we decided
to enable Igor2 to generate conditional rules as informally introduced above.
This
is done by adding a further synthesis operator χpred. The operator χpred is a further
rule-splitting operator. In contrast to the operator χsplit, it does not split a rule by
generating several non-unifying specializations of its pattern but rather by generating
two complementary conditions/predicates depending on the pattern variables.
The conditions, however, are not analytically “synthesized” from the speciﬁcation but
simply enumerated until now. In addition to the speciﬁcation of the target function(s)
and background functions, the user may provide predicates, i.e., boolean-valued func-
tions. Now let some open rule r be selected and p be a user-provided predicate such
that for each formal parameter of the predicate there is a variable in the pattern of r
with the same type (or sort). Then p applied to these pattern variables is one possible
condition and its negation is the complementary condition. The selected rule is cloned
and each version is endowed with one of the two complementary conditions. Finally
the speciﬁcation subset associated with the selected open rule r is partitioned into two
subsets accordingly.
In addition to user-deﬁned predicates, syntactical equality of terms is integrated as a
standard predicate.
4.9.2. Rapid Rule-Splitting
Suppose the LHS of a selected open rule r contains more then one pivot position. Then
χsplit introduces one partition of the associated speciﬁcation subset and a corresponding
set of successor rules for each pivot position.
Another possibility is to generate one single partition and a corresponding set of
successor rules based on all pivot positions. In this case, one subset of the partition is
not determined by one of the constructors at the chosen pivot position in the speciﬁcation
rules, but rather by one combination of constructors, one for each pivot position.
For example, if we have two pivot positions p1, p2 denoting constructors c1, c′
1 and
c2, c′
2, respectively, in the speciﬁcation rules, then standardly we would get two partitions
with two subsets each. One partition according to p1 with subsets according to c1 and
c2 and analogously a further partition for p2.
With rapid rule-splitting, we get one
partition with four subsets in this case: One subset for the combination c1, c2, one for
c1, c′
2, one for c′
1, c2 and one for c′
1, c′
2.
4.9.3. Existentially-Quantiﬁed Variables in Speciﬁcations
This last extension allows for existentially-(∃-)quantiﬁed variables in RHSs of speciﬁca-
tion rules. In contrast to the (implicitly) ∀-quantiﬁed variables, that represent the set
130

4.9. Extensions
of all its instances, an ∃-quantiﬁed variable represents one arbitrary instance such that
one has the possibility to leave parts of an output “unspeciﬁed”.
This can be useful for three reasons.
First, one possibly want the output of a speciﬁed input to just satisfy a particular
form without worrying about the particular value. For example, assume a function f
shall take lists and output lists of the same length where the last elements of input and
output lists are equal but the concrete values for the other elements of the output lists
are not important. One speciﬁcation rule for f could then be:
∃v w. f ([x,y,z]) = [v,w,z] .
Second, recall that if χcall introduces a call of a function f′, then the speciﬁcation rules
l →r of f′ must satisfy the condition Var(l) = Var(r) (Deﬁnition 4.14, condition 1). The
reason is that if Var(l) ⊃Var(r), no instantiation of the variables in l not occurring in r is
determined and they can in principle be instantiated arbitrarily. The (instantiated) LHSs
of f′ become RHSs in the abduced speciﬁcations of the new subfunctions introduced by
χcall. Hence existentially quantiﬁed variables in speciﬁcation RHSs would be a way for
χcall to deal with the general case that Var(l) ⊇Var(r).
Third, quantiﬁed variables in speciﬁcation RHSs would allow to let a complete speciﬁed
RHS be just a single existentially quantiﬁed variable, hence to let the output for the
corresponding input unspeciﬁed. Hence this would be a way to eﬀectively integrate non-
complete (up to some complexity) speciﬁcations into the analytical approach to inductive
program synthesis.
However, integrating the possibility to systematically deal with existentially quantiﬁed
variables in speciﬁcations into Igor2 is not trivial.
131


5. Experiments
We have implemented a prototype of the Igor2 algorithm in the interpreted, term-
rewriting based speciﬁcation and programming language Maude [18].1 Maude’s so-
called functional modules are constructor term rewriting systems that are assumed to
be complete, i.e., conﬂuent and terminating. As extensions to ordinary CSs, as deﬁned
in Section 2.2.4, Maude’s functional modules are order-sorted and binary constructors
can be speciﬁed to be associative, commutative, etc. and then rewriting is performed
modulo these properties.
Since Maude’s functional modules are essentially CSs, Igor2 speciﬁcations as well
as solution programs induced by Igor2 are valid Maude modules. A further reason
for choosing Maude as implementation and speciﬁcation framework for Igor2 was that
Maude is reﬂective, i.e., that within a Maude function, each Maude module known
to the interpreter can be taken as data object and can be inspected, manipulated, and
even evaluated. Hence by using this feature, the implementation of data structures to
represent, manipulate, and evaluate Igor2 speciﬁcations and generated CSs incl. sorts,
terms, rules/equations etc. can be omitted.
In addition to the synthesis operators as deﬁned in Section 4.7, the implementa-
tion contains, in a more or less experimental form, the three extensions sketched in
Section 4.9:
Rapid rule-splitting, conditional rules, and speciﬁcations containing ∃-
quantiﬁed variables (experimental and incomplete).
We have tested the implemented Igor2 system with problems from two problem
domains:
1. Functional programming problems and
2. artiﬁcial intelligence / problem-solving problems.
The complete speciﬁcation ﬁles of all problems are provided in the appendix. All tests
have been conducted under Ubuntu Linux on an Intel Dual Core 2.33 GHz with 4GB
memory.
5.1. Functional Programming Problems
This block contains several recursive functions of natural numbers, lists, and lists of lists
(matrices). Most of them are well-known and can be found in standard libraries, e.g.,
in the Haskell standard libraries.
1Maude’s homepage can be found at http://maude.cs.uiuc.edu/.
133

5. Experiments
Table 5.1.: Tested functions for natural numbers
Function
Type
Description
=
INat INat →Bool
Equality
≤
INat INat →Bool
Smaller or equal than
odd
INat →Bool
Is a number odd?
even
INat →Bool
Is a number even?
+
INat INat →INat
Addition
∗
INat INat →INat
Multiplication
fac
INat →INat
Factorial
ﬁb
INat →INat
Fibonacci numbers
ack
INat INat →INat
Ackermann function
The same function can be speciﬁed in diﬀerent ways: One may only use I/O examples
or also I/O patterns (i.e., variables); and one may provide more or less speciﬁcation
rules. For the functional programming problems, we aspired most concise speciﬁcations.
Hence we used variables where it was possible, e.g., we speciﬁed + by I/O patterns like
S S 0 + x →S S x
(where x is a variable) instead of I/O examples like
S S 0 + S S S 0 →S S S S S 0,
and we provided rather few than many rules.2
The only used extension in this block is rapid rule-splitting. Some tests for functions
that require conditional rules, such as member or insert (inserting an element into a
sorted list) or sort have previously been published in [48].
5.1.1. Functions of Natural Numbers
The ﬁrst module includes functions and predicates for natural numbers. Table 5.1 gives
an overview of the tested functions. For the boolean values we used the BOOL module
included in Maude’s standard prelude. It deﬁnes the sort Bool and the boolean values
(constant constructors) true : →Bool and false : →Bool. We deﬁned the type of
natural numbers by the sort INat (abbreviating Igor2-Nat, to circumvent conﬂicts with
Maude’s internal deﬁnition of natural numbers) and the constructors 0 : →INat (zero)
and S
: INat →INat (successor).
All functions are speciﬁed by some I/O examples or I/O patterns together in one
Maude module. The concrete speciﬁcation, i.e., the selection of target function(s) to
be induced and functions that may be used as background knowledge is then provided
2However note that Igor2, due to its analytical recurrence-detection approach, needs a certain number
of I/O examples in order to induce recursive functions. In particular, Igor2 cannot induce recursive
functions from single examples, like some enumerative approaches, but needs at least one example
per base-case and two examples per recursive case.
134

5.1. Functional Programming Problems
Table 5.2.: Results of tested functions for natural numbers. The maximum depth of rules
(Deﬁnition 4.19) was set to 3 and possibly reduced in case of a timeout.
Speciﬁcation
Test
Target funs
Size
Style
BK
Time (sec)
1
=
9
examples
7.052
2
≤
6
patterns
0.032
3
odd, even
4 + 4
examples
0.020a
4
+
3
patterns
0.008
5
∗
6
mixed
0.088⊥b
6
∗
6
mixed
+
⊘
7
fac
4
examples
0.060⊥b
8
fac
4
examples
∗
⊘, 11.192c
9
ﬁb
6
examples
1.212⊥b
10
ﬁb
6
examples
+
⊘
11
ack
17
examples
⊘, 2.904d
amutual recursive solution (Listing 4.1)
bintended function not included in program space; see Section 4.8.4 for a discussion
cwith max depth 2
dwith rapid rule-splitting (see Section 4.9.2)
by two parameters for each Igor2 run, listing the names of functions to be induced and
background functions, respectively.
Table 5.2 summarizes the test-setup and results. Each row in the table represents one
tested problem. The columns, from left to right, show the number of the tested problem,
the name(s) of the target function(s)3, the number of provided speciﬁcation rules, the
style of the speciﬁcation rules (I/O examples, i.e., ground rules, or I/O patterns, i.e.,
non-ground rules, or mixed, i.e., both ground and non-ground rules), provided back-
ground functions, and the induction time in seconds. A ⊥adhering to the induction
time indicates a wrong solution in the sense that the induced function deﬁnition does
not compute the intended function (though of course, the induced function correctly
computes the speciﬁcation). This only happened if the intended function was not in-
cluded in the program space at all. For example, ∗cannot be induced by the synthesis
operators of Igor2 if + is not provided as background knowledge. See Section 4.8.4 for
a consideration of classes of functions that can and cannot be synthesized by Igor2.
Finally, a ⊘instead of an induction time indicates a “timeout”—we aborted an Igor2
execution if it did not come up with a solution within three minutes.
Maximum depth of rules (cp. Deﬁnition 4.19) was set to 3. In case of a timeout, we
reduced the maximum depth to 2 to further restrict the number of applications of the
function call operator χcall and/or switched on rapid rule-splitting (see Section 4.9.2).
3Two names mean that these functions were induced in parallel.
135

5. Experiments
Listing 5.1: I/O examples for the Ackermann function
1
ack(0, 0)
→S 0
2
ack(0, S 0)
→S S 0
3
ack(0, S S 0)
→S S S 0
4
ack(0, S S S 0)
→S S S S 0
5
ack(0, S S S S 0)
→S S S S S 0
6
ack(0, S S S S S 0)
→S S S S S S 0
7
ack(0, S S S S S S 0) →S S S S S S S 0
8
ack(S 0, 0)
→S S 0
9
ack(S 0, S 0)
→S S S 0
10
ack(S 0, S S 0)
→S S S S 0
11
ack(S 0, S S S 0)
→S S S S S 0
12
ack(S 0, S S S S 0)
→S S S S S S 0
13
ack(S 0, S S S S S 0) →S S S S S S S 0
14
ack(S S 0, 0)
→S S S 0
15
ack(S S 0, S 0)
→S S S S S 0
16
ack(S S 0, S S 0)
→S S S S S S S 0
17
ack(S S S 0, 0)
→S S S S S 0
Listing 5.2: Induced deﬁnition of the Ackermann function
ack (0, 0)
→S 0
ack (0, S x0)
→S S x0
ack (S x0, 0)
→ack (x0, S 0)
ack (S x0, S x1)
→ack (sub9 (S x0, S x1), sub10(S x0, S x1))
sub9 (S x0, S x1)
→x0
sub10 (S x0, S x1)
→ack (S x0, x1)
Listing 5.1 shows exemplarily the I/O examples that were used to specify the Acker-
mann function. Listing 5.2 shows the induced CS (prettyprinted by hand).4 It is easy
to verify that the induced deﬁnition for the Ackermann function is equivalent to the
standard deﬁnition:
A(0, n) = n + 1
A(m + 1, 0) = A(m, 1)
A(m + 1, n + 1) = A(m, A(m + 1, n))
4In Section 4.8.4, the sequence of synthesis-operator applications to achieve the induced CS is shortly
described. However note that we used rapid rule-splitting (Section 4.9.2) here such that all four
patterns for ack of the induced solution were synthesized by only one application of χsplit instead of
the two χsplit applications mentioned in Section 4.8.4.
136

5.1. Functional Programming Problems
5.1.2. List Functions
The second module includes general list functions, such as reverse, and functions on lists
of natural numbers, such as sum. Table 5.3 gives an overview of the tested functions.
Natural numbers are deﬁned as described in the previous section. We deﬁned the (gen-
eral, parameterized) list type by the sorts List{X} and X$Elt, where X$Elt is the sort for
the single elements of a list. The X is a formal parameter that may be instantiated by
any concrete sort, e.g., by INat, to get lists of natural numbers. The list constructors
are [ ] : →List{X} (empty list) and
:
: X$Elt List{X} −> List{X} (“cons”ing a ﬁrst
element and a rest list) where we use mixﬁx notation for
: . Furthermore we added a
(mixﬁx-) constructor < , > : X$Elt X$Elt −> X$Elt in order to also have pairs of elements
as list elements (e.g., for the zip function). Finally we have a further sort ListPair {X}
that represents pairs of lists (e.g., for the functions unzip or split ).
Table 5.4 summarizes the test-setup and results. The structure is the same as for
the result table for the functions on natural numbers, except for that we added one
column to list automatically invented subfunctions.5
The columns from left to right
denote the number of the tested problem, name(s) of target function(s), number(s)
of provided speciﬁcation rules, style of speciﬁcation, provided background functions,
invented functions, and induction time in seconds. A ⊥adhering to the induction time
indicates a wrong solution, a ⊘instead of an induction time indicates a “timeout” (if
the run-time exceeded three minutes). Again we set the maximum depth of rules to 3
and in case of a timeout, we reduced the maximum depth to 2 and/or switched on rapid
rule-splitting.
The only timeout for the general list functions appeared for swap (row 19). A second
run with rapid rule-splitting enabled induced a correct function deﬁnition in about seven
seconds.
Speciﬁcation and induced solution for reverse (row 4) are those from Example 4.1,
i.e., init and last are invented and induced as help functions. If init and/or last are
provided as background functions (row 5), they are not re-invented but simply called.
As one can see, this reduces the induction time. We get a similar result for the function
splitAt , which splits a list at a given position into two lists. Induced without background
knowledge, the help functions take and drop are automatically invented (row 9). If take
and drop are provided, they are just called and the induction time gets reduced.
In the experiment shown in row 18, four recursive functions overall were induced
in parallel. Two of them shiftL and shiftR were speciﬁed as target functions and the
remaining two, init and last were automatically introduced as help functions for shiftR.
The induced solution for this experiment is shown in Listing 5.3.
As functions for lists of natural numbers we tested sum and product in several ways.
First, we tested sum without any background knowledge (row 20), resulting in a correct
induced solution as shown in Listing 5.4. Second (row 21), we tested sum with + as
background knowledge. Interestingly, this led to a timeout, though certainly, + is an
5Of course, the function names in the respective column are not introduced by Igor2 but identiﬁed
by the author to indicate which functions Igor2 actually invented.
137

5. Experiments
Table 5.3.: Tested list functions
General lists
Function
Type
Description
init (l)
List{X} →List{X}
List l without the last element
last (l)
List{X} −> X$Elt
Last element of l
l1 ++l2
List{X} List{X} →List{X}
Append
length (l)
List{X} →INat
Number of elements of l
reverse (l)
List{X} →List{X}
l in reverse order
l !! n
List{X} INat −> X$Elt
Element of l at index n
take (l , n)
List{X} INat →List{X}
Take ﬁrst n elements of l
drop (l , n)
List{X} INat →List{X}
Drop ﬁrst n elements from l
splitAt (l , n)
List{X} INat →ListPair {X}
Split l at index n into two lists
evenpos (l)
List{X} →List{X}
Keep elements at even indices
oddpos (l)
List{X} →List{X}
Keep elements at odd indices
split (l)
List{X} →ListPair {X}
Take odd and even positions into
two seperate lists
replicate (n, x)
INat X$Elt −> List{X}
List of length n with x at each
position
intersperse (x, l)
X$Elt List{X} −> List{X}
Put x between each two elements
of l
zip (l1 , l2)
List{X} List{X} →List{X}
List of pairs from two lists
unzip (l)
List{X} →ListPair {X}
Pair of lists from list of pairs
shiftL (l)
List{X} →List{X}
Shift l to left, ﬁrst element be-
comes last element
shiftR (l)
List{X} →List{X}
Shift l to right, last element be-
comes ﬁrst element
swap (l , n, m)
List{X} INat INat →List{X}
Swapping the elements at indices
n and m
Lists of natural numbers
Function
Type
Description
sum (l)
List{INat} →INat
Sum of all numbers of l
prod (l)
List{INat} →INat
Product of all numbers of l
138

5.1. Functional Programming Problems
Table 5.4.: Results of tested list functions
Speciﬁcation
Test
Target funs
Size
Style
BK
Invented
Time (sec)
1
init , last
4 + 4
patterns
0.056
2
++
3
patterns
0.032
3
length
3
patterns
0.020
4
reverse
5
patterns
init , last
0.280
5
reverse
5
patterns
init , last
0.088
6
!!
3
patterns
0.024
7
take
5
patterns
0.036
8
drop
5
patterns
0.152
9
splitAt
5
patterns
take, drop
0.156
10
splitAt
5
patterns
take, drop
0.040
11
evenpos
5
patterns
0.048
12
oddpos
5
patterns
0.144
13
split
5
patterns
oddpos, evenpos
0.212
14
replicate
3
patterns
0.020
15
intersperse
4
patterns
0.068
16
zip
6
patterns
0.132
17
unzip
3
patterns
0.072
18
shiftL , shiftR
4 + 4
patterns
last , init
0.336
19
swap
6
patterns
⊘, 6.820a
20
sum
13
examples
0.152
21
sum
13
examples
+
⊘
22
sum2
4
traces
0.036b
23
prod
13
examples
0.568⊥
24
prod
13
examples
∗
⊘
awith rapid rule-splitting enabled
bThe same function as sum but we provided traces of the form sum ([x,y,z]) = x + y + z as speci-
ﬁcation, instead of I/O examples or I/O patterns
Listing 5.3: Induced CS for shiftL and shiftR, somewhat simpliﬁed by hand (non-recursive
subfunctions eliminated by unfolding); the invented subfunctions sub3 and
sub4 are the last and init functions, respectively
shiftL ([ ])
→[ ]
shiftL (x : [ ])
→x : [ ]
shiftL (x : y : xs) →y : shiftL (x : xs)
shiftR ([ ])
→[ ]
shiftR (x : xs)
→sub3 (x : xs) : sub4 (x : xs)
sub3 (x : [ ])
→x
sub3 (x : y : xs) →sub3 (y : xs)
sub4 (x : [ ])
→[ ]
sub4 (x : y : xs) →x : sub4 (y : xs)
139

5. Experiments
Listing 5.4: Induced CS for sum
sum ([])
→0
sum (0 : xs)
→sum (xs)
sum (S x : xs) →S (sum (x : xs))
Listing 5.5: Induced CS for the swap function
swap (x0 : x1 : xs,
0, S 0)
→x1 : x0 : xs
swap (x0 : x1 : x2 : xs, 0, S S x4)
→
swap (x1 : swap (x0 : x2 : xs, 0, S x4), 0, S 0)
swap (x0 : x1 : x2 : xs, S x4, S S x5) →x0 : swap (x1 : x2 : xs, x4, S x5)
appropriate background function for sum. Yet the additional possibilities to apply χcall
when + was given led to a combinatorial explosion of the branching factor of the problem
space. Hence inducing the simpler solution (compared to the solution when + is not
given) in it would take much more time than the induction of the more complex solution
without + but in a problem space with a much smaller branching factor. Third, we
tested sum with traces as speciﬁcation instead of I/O examples or I/O patterns. (We
renamed the function to sum2 for this experiment.) This means that the speciﬁcation
rules had, e.g., the form
sum2 (x : y : z : [ ]) →x + y + z
instead of, e.g.,
sum (S S 0 : 0 : S 0) →S S S 0 .
Finally we tested prod with and without ∗as background knowledge.
In the ﬁrst
case, we got an induced function, yet a wrong one (because prod without background
knowledge is not included in the problem space). With prod as background knowledge
we got a timeout.
The most complex function in this module is the swap function that swaps two elements
of a list at two given indices. We restricted the provided examples to the case where the
ﬁrst given index is smaller than the second one and both indices actually occur in the list.
Listing 5.5 shows the induced CS (after applying some simpliﬁcation rules eliminating
non-recursive subfunctions) correctly computing the swap function.
5.1.3. Functions of Lists of Lists (Matrices)
In the third and last module of the functional programming block, we tested functions
for lists of lists, i.e., matrices. Actually, the inner lists are lists of natural numbers,
represented by the sort INat (see the previous sections). However, this has only the
pragmatic reason that we could not instantiate the parameterized list type from the
previous section by itself (without instantiating the type parameter for the inner lists).
We have three sorts now: INat for natural numbers, List{INat} for lists of natural
140

5.1. Functional Programming Problems
Table 5.5.: Tested functions for lists of lists (matrices)
Function
Type
Description
concat ( ls )
List{NatList} →List{INat}
Concatenation of a list of lists to
one list
map: (x, ls )
INat List{NatList} →List{NatList}
apply x : l to each inner list l of
ls
inits (l)
List{INat} →List{NatList}
all preﬁxes of a list
tails (l)
List{INat} →List{NatList}
all suﬃxes of a list
subsequences (l)
List{INat} →List{NatList}
all subsequences of a list; (equiv-
alent with powerset)
tails2 ( ls )
List{NatList} →List{NatList}
the tail of all inner lists
transpose ( ls )
List{NatList} →List{NatList}
transposing the matrix repre-
sented by ls
weave (ls)
List{NatList} →List{INat}
collecting all elements of ls into
one list; ﬁrst all ﬁrst elements,
followed by all second elements
etc.
Listing 5.6: Induced CS for weave; the automatically invented sub36 implements shiftL
weave (nil )
→[ ]
weave ((x : xs) : : xss) →x : weave (sub36 ((x : xs) : xss))
sub36 ((x : [ ]) : : nil )
→
nil
sub36 ((x : [ ]) : : (y : ys) : : xss)
→(y : ys) : : xss
sub36 ((x : y : xs) : : nil )
→(y : xs) : : nil
sub36 ((x : y : xs) : : (y : ys) : : xss) →(y : ys) : : sub36 ((x : y : xs) : : xss)
numbers, and List{NatList} for lists of lists of natural numbers. The constructors for
List{INat} are the same as in the previous section. The constructors for List{NatList}
(list of lists) are nil (the empty list of lists) and
: :
(inserting a list at the front of a
list of lists).
Table 5.5 gives an overview of the tested functions.
Table 5.6 summarizes the test-setup and results.
The structure is the same as in
the previous section. The columns, from left to right, denote the number of the tested
problem, name(s) of target function(s), number(s) of provided speciﬁcation rules, style
of speciﬁcation, provided background functions, invented functions, and induction time
in seconds. A ⊥adhering to the induction time indicates a wrong solution, a ⊘instead
of an induction time indicates a “timeout” (if the run-time exceeded three minutes).
Again we set the maximum depth of rules to 3 and in case of a timeout, we reduced the
maximum depth to 2 and/or switched on rapid rule-splitting.
We exemplarily show the (simpliﬁed) induced CS for the weave function, that has been
tested without background knowledge. Igor2 invented the shiftL function for lists of
lists (sub36 in the listing) to solve weave. Listing 5.6 shows the solution.
141

5. Experiments
Table 5.6.: Results for tested functions for matrices
Speciﬁcation
Test
Target funs
Size
Style
BK
Invented
Time (sec)
1
concat
13
patterns
0.240
2
map:
3
patterns
0.056
3
inits , tails
3 + 3
patterns
map:
0.316
4
tails2
4
patterns
0.056
5
subsequences
3
patterns
map:, ++
⊘
6
transpose
9
patterns
⊘
7
transpose
9
patterns
tails2
7.308
8
weave
11
patterns
shiftL
⊘, 33.350a
awith rapid rule-splitting enabled
5.2. Artiﬁcial Intelligence Problems
Learning from experience, with the result that future problems or tasks in the same or a
similar domain can be solved more eﬃciently, is a core cognitive capability and studied
in artiﬁcial intelligence as well as in cognitive psychology.
This section summarizes results that have already been published in [95]. The cited
paper is joint work of the author of this thesis and Ute Schmid and Martin Hofmann.
The author of this thesis was responsible for specifying the considered problems for
Igor2 and for conducting the experiments, as summarized in the following. Besides
these technical matters, the paper moreover contains an extensive motivation of the
considered problems from a cognitive psychology perspective.
Table 5.7 gives an overview over the tested problems.
5.2.1. Learning to Solve Problems
Often, in cognitive psychology, speed-up eﬀects in problem solving are modelled as com-
position of primitive rules as a result of their co-occurrence during problem solving, e.g.,
knowledge compilation in the cognitive architecture ACT [2] or operator chunking in
the cognitive architecture SOAR [91].
Similarly, in AI planning macro learning was
modelled as composition of primitive operators to more complex ones [57]. However,
general problem-solving strategies are often recursive. A classical example is the famous
Towers-of-Hanoi problem.
There is empirical evidence that humans are in general able to learn such generalized
recursive knowledge from experience, e.g., after solving Tower-of-Hanoi problems, at
least some people have acquired the recursive solution strategy [5].
Also in automated planning [32], extending macro-learning capabilities to learning
of recursive macros could be of great practical relevance.
A long-standing and still
open problem in automated planning is the scalability regarding the number of objects
involved in a particular problem. E.g., automated planners can eﬀectively solve Towers-
142

5.2. Artiﬁcial Intelligence Problems
Table 5.7.: Tested problems in artiﬁcial intelligence and cognitive psychology domains
Problem
Description
Problem solving domain
clearBlock (x, t, s)
Unstack all blocks above block x in tower t in
state s
tower (x, s)
Stack all blocks up to block x in the correct
order to a tower in state s
rocket (objs , s)
Carry all objects in objs to the moon in state
s by loading them, ﬂying the rocket, and un-
loading them
hanoi (d, src , aux, dst, s)
Towers-of-Hanoi: Move all discs up to disc d
from src peg to dst peg via aux peg
Reasoning domain
ancestor (x, y, t)
Return path from node x to node y in binary
tree t; models transitivity
Language processing domain
sentence (d)
Learning a simple grammar in order to pro-
duce correct sentences
of-Hanoi only up to a certain number of discs ([97] mentions 15 discs as a limit for
tractable problem sizes). The reason is the general exponential growth of the search tree
with respect to the length of the solution plan. On the other side, if a planner would
know the recursive strategy that generates the solution plan for an arbitrary number of
discs, than search could be omitted and the solution plan can simply be generated by
the (learned) recursive strategy.
The general idea to apply Igor2 in automated planning and problem solving is, that
ﬁrst a planner solves some small problems in a particular domain and then Igor2 is
applied to extract the underlying recursive strategy in terms of a recursive functional
program.
In classical AI-planning, states of the world are represented by sets of ground predi-
cates, and operators to compute successor states are represented by preconditions and
eﬀects, which are both non-ground sets of literals. A concrete action results from in-
stantiating all variables of an operator. An action can be applied to a state s, if all
its positive precondition literals appear in s. The corresponding successor state results
from adding the positive eﬀects to s and deleting the negative eﬀects from s. A plan-
ning problem is given by an initial state and a goal description, both represented as
sets of ground predicates. The goal for the planner is then to ﬁnd a (solution) plan—a
sequence of actions—that achieves a state satisfying the goal description when applied
to the initial state.
A very well-known (benchmark) domain for automated planning is the blocksworld,
where several blocks lie on a table and can be stacked to several towers. It contains
143

5. Experiments
Domain (the Puttable operator):
Puttable(x)
;; put clear block x lying on block y to the table
PRECOND: clear(x), on(x, y)
EFFECTS: ontable(x), clear(y), not(on(x,y))
Problem descriptions:
: init-1 clear(A), ontable(A)
: init-2 clear(A), on(A, B), ontable(B)
: init-3 on(B, A), clear(B), ontable(A)
: init-4 on(C, B), on(B, A), clear(C), ontable(A)
: goal clear(A)
Figure 5.1.: The Puttable operator and four example problems for the general clearBlock
task
operators to stack one block on top of a tower and to put a topmost block from a tower
to the table.
For a ﬁrst example problem, we at ﬁrst restrict ourselves to the Puttable operator
that is shown in Figure 5.1. Furthermore, the ﬁgure shows four initial states and one
goal description. The (recursive) task to be learned here is clearBlock, putting all blocks
above a certain block x in a tower to the table such that x becomes cleared. In the given
example problems, block A is to be cleared (as stated by the goal predicate). The four
example problems result from initially positioning A at diﬀerent positions in a tower:
1. A lies on the table and is already cleared.
2. A lies on another block and is already cleared.
3. A lies on the table and is covered by one other block.
4. A lies on the table and is covered by two other blocks.
A solution plan for initial state 4 in Figure 5.1, consisting of two actions, is:
Puttable(C), Puttable(B) .
Such a plan can be obtained by any PDDL planner [32] if it is applied to the domain
and problem description shown in Figure 5.1.
The idea is now to learn a recursive program clearBlock that takes a block and a tower
as input and returns the correct sequence of Puttable actions to clear the provided block,
i.e., a solution plan, as output.
That means, the speciﬁcation for Igor2 for this task consists of the initial problems
listed in Figure 5.1 as example inputs and the corresponding plans as example outputs.
144

5.2. Artiﬁcial Intelligence Problems
Listing 5.7: Examples of clearBlock for Igor2
clearBlock(a, a Table, s)
→s
clearBlock(a, a b Table, s)
→s
clearBlock(a, b a Table, s)
→Puttable(b, s)
clearBlock(a, c b a Table, s) →Puttable(b, Puttable(c, s))
Therefore, the plans must be rewritten as terms. This is done by introducing a further
parameter s to the Puttable operator, a so-called situation variable [64]. The above plan
then becomes the term
Puttable(C, Puttable(B, s)) .
For the Igor2 speciﬁcation, we additionally have replaced the constant blocks by
variables. We have three types: State, Block, and Tower. Constructors for sort Tower are
Table : →Tower (empty tower) and
: Block Tower →Tower (the provided block is
the new topmost block of the resulting tower with the given tower below it). The function
to be induced is clearBlock : Block Tower State →State. Listing 5.7 shows the four
examples for Igor2 according to the four problems stated in Figure 5.1.
The described procedure to transform a number of planning problems and the corre-
sponding plans into a speciﬁcation for Igor2 is not implemented yet, we have rather
done this transformation by hand.
For more complex problems, the transformation
becomes less straight-forward.
The recursive program induced by Igor2 for clearBlock, as well as the programs
induced for the other problems in the problem solving domain, are shown in List-
ing 5.8.
Note that the induced rules for clearBlock and tower are conditional rules.
The conditional-rules extension (see Section 4.9.1) of Igor2 was enabled for all artiﬁcial
intelligence / cognitive psychology problems. In the case of clearBlock, no predicate was
explicitly speciﬁed but the standard predicate, equality on terms, was used as one can
see in the induced rules. In the case of tower, a predicate isTower? was provided in the
speciﬁcation.
Consider the induced recursive strategy for tower. It can be read as: “If the tower
with topmost block o already exists in s, return just s. Otherwise, build the tower where
the topmost block is the block precedent to o, then clear that block, then clear o, ﬁnally
put o to the precedent block.” This strategy yields optimal (i.e., the shortest possible)
solution plans.
Table 5.8 summarizes test-setup and induction times for the problem-solving problems.
5.2.2. Reasoning and Natural Language Processing
As a reasoning problem, we have considered the concept of ancestor. The competence
underlying the correct application of the ancestor concept, that is, correctly classifying
a person as ancestor of some other person, in our opinion is the correct application of
145

5. Experiments
Listing 5.8: Induced programs in the problem solving domain
clearBlock (a, b t, s) →s
if a = b
clearBlock (a, b t, s) →clearBlock (a, t, Puttable (b, s))
if a ̸= b
tower (o, s) →s
if isTower? (o, s)
tower (o, s) →put (o, sub1 (o, s),
clearBlock (o, clearBlock (sub1 (o, s),
tower (sub1 (o, s), s))))
if not (isTower? (o, s))
sub1 (S o, s) →o
rocket (Nil , s)
→Move (s)
rocket (o os, s) →Unload (o, rocket (os, Load (o, s)))
hanoi (0, src , aux, dst, s)
→Move (0, src, dst, s)
hanoi (S d, src , aux, dst, s) →hanoi (d, aux, src , dst,
Move (S d, src, dst,
hanoi (d, src , dst, aux, s)))
Table 5.8.: Results for tested problem-solving problems
Task
Num of Expls
BK
Invented
Time (sec)
clearBlock
4
0.036
tower
9
clearBlock, isTower?
blockBelow
1.2
rocket
3
0.012
hanoi
3
0.076
146

5.2. Artiﬁcial Intelligence Problems
Listing 5.9: Induced rules for ancestor
ancestor (x, y, Nil)
→Nilp
ancestor (x, y, Node (z, l , r)) →
isIn ? (y, Node (z, l , r))
if x = z
ancestor (x, y, Node (z, l , r)) →ancestor (x, y, l) or ancestor (x, y, r)
if x ̸= z
Original grammar:
S -> NP VP
NP -> d n
VP -> v NP | v S
Examples provided to Igor2:
sentence(1) = (d n v d n !) .
sentence(S 1) = (d n v d n v d n !) .
sentence(S S 1) = (d n v d n v d n v d n !) .
Figure 5.2.: A phrase-structure grammar and according examples for Igor2 (in the very
original grammar, d n v are non-terminals D N V which go to concrete
words)
the transitivity relation in some partial ordering. We believe that if a person has grasped
the concept of transitivity in one domain, such as ancestor, this person will also be able
to correctly apply it in other, previously unknown domains. For example, such a person
should be able to correctly infer is-a relations in some ontology.
For simplicity of modeling, we used binary trees as domain model. For trees with arbi-
trary branching factor, the number of examples would have to be increased signiﬁcantly.
The transitivity rule learned by Igor2 is given in Listing 5.9.
Finally, we presented Igor2 examples to learn a phrase-structure grammar.
This
problem is also addressed in grammar inference research [93]. We avoided the problem
of learning word-category associations and provided examples abstracted from concrete
words. In particular, a function sentence should be learned that generates words (or
sentences) of the target grammar of particular depths. Figure 5.2 shows the grammar
to be learned and the corresponding examples that were provided as speciﬁcation to
Igor2. The abstract example sentence structures correspond to sentences as [22]:
1. The dog chased the cat.
2. The girl thought the dog chased the cat.
3. The butler said the girl thought the dog chased the cat.
Listing 5.10 shows the induced rules. Note that Igor2 has eﬀectively simpliﬁed the
original grammar without changing the represented language. Rewritten as grammar,
the two learned rules would read:
147

5. Experiments
Listing 5.10: Induced rules for the word-structure grammar
sentence (1)
→(d n v d n !)
sentence (S n) →(d n v sentence(n))
Table 5.9.: Empirical comparison of diﬀerent inductive programming systems
Problems
Systems
ADATE
FFoil
Flip
Golem
Igor1
Igor2
MagH.
Lasts
365.62
0.7⊥
×
1.062
0.051
5.695
19.43
Last
1.0
0.1
0.020
< 0.001
0.005
0.007
0.01
Member
2.11
0.1⊥
17.868
0.033
—
0.152
1.07
odd/even
—
< 0.1⊥
0.130
—
—
0.019
—
multlast
5.69 < 0.1
448.900⊥< 0.001
0.331
0.023
0.30
isort
83.41
×
×
0.714
—
0.105
0.01
reverse
30.24
—
—
—
0.324
0.103
0.08
weave
27.11
0.2
134.240⊥
0.266
0.001⊥0.022
⊙
ShiftR
20.14 < 0.1⊥448.550⊥
0.298
0.041
0.127 157.32
Mult
—
8.1⊥
×
—
—
⊙
—
allodds
466.86
0.1⊥
×
0.016⊥0.015⊥
⊙
×
— not tested × stack overﬂow ⊙time out ⊥wrong
S -> d n v d n | d n v S
5.3. Comparison with Other Inductive Programming Systems
Generally, a substantiated comparison of the diﬀerent existing inductive programming
systems is diﬃcult to achieve, mostly because, up to now, all systems have their own
speciﬁcation languages and need diﬀerent types of speciﬁcations, e.g., I/O examples vs.
I/O patterns vs. output evaluation functions vs. general predicates and complete (up
to some complexity) speciﬁcations vs. single or random examples etc.
As a ﬁrst attempt to systematize the relation of the diﬀerent representations, in a joint
work with Martin Hofmann [38], the author has proposed conditional term rewriting as a
common representation framework for functional as well as logic-based inductive program
synthesis systems. Furthermore, the cited paper contains a ﬁrst empirical comparison
of some inductive functional and inductive logic programming systems that are able to
induce recursive functions. Table 5.9 summarizes the results.
Some tested problems were already mentioned in the previous sections. Further prob-
lems are multlast, which replaces all elements with the last element, Lasts, which applies
Last on a list of lists, isort, which is insertion-sort, allodds, which checks for odd num-
bers, weave restricted to only two lists, and Member, which checks whether an element
occurs in a list.
The tested systems include the functional generate-and-test systems ADATE and
148

5.3. Comparison with Other Inductive Programming Systems
MagicHaskeller (see Section 3.4), the sequential covering ILP systems Foil and
Golem (see Section 3.3.3), the functional-logic system Flip, the functional analytical
system Igor1 (see Section 3.2.3), and the combined analytical and search-based system
Igor2, presented in this thesis (Chapter 4).
Because FFoil and Golem usually perform better with more examples, whereas Flip,
MagicHaskeller and Igor2 do better with less, each system got as much examples as
necessary up to certain complexity, but then exhaustively, so no speciﬁc cherry-picking
was allowed.
For synthesizing isort all systems had a function to insert into a sorted list, and the
predicate < as background knowledge. Flip needed an additional function if to relate
the insert function with the <. For all systems except for Flip and MagicHaskeller
the deﬁnition of the background knowledge was extensional. Igor2 was allowed to use
variables and for Golem additionally the accordant negative examples were provided.
MagicHaskeller had paramorphic functions to iterate over a data type as background
knowledge. Note that we did not test a system with a problem which it per se cannot
solve due to its restriction bias. This is indicated with ‘—’ instead of a runtime. A
timeout after ten minutes is indicated with ⊙. Table 5.9 shows the runtimes of the
diﬀerent systems on the example problems.
The results show that the functional generate-and-test methods (at least ADATE)
need comparatively more time than the two extensional coverage ILP systems FFoil and
Golem and the analytical functional systems Igor1 and Igor2. They further show that
at least FFoil has problems to induce recursive programs (cp. Section 3.3.3) and that the
purely analytical approach (Igor1) is seriously restricted regarding inducible functions.
Finally it shows that integrating generalized analytical techniques into search, as done
by Igor2, expands the class of inducible functions compared to the purely analytical
approach (Igor1) and at the same time solves the test problems comparatively quickly.
149


6. Conclusions
In this thesis, we at ﬁrst comprehensively surveyed and classiﬁed currently existing ap-
proaches to the inductive synthesis of functional and logic programs, second, presented
an own algorithm, Igor2, to the inductive synthesis of functional programs and showed
certain properties of it, and third, evaluated Igor2 by means of a number of recur-
sive problems known from functional programming and artiﬁcial intelligence.
In the
following, we summarize the obtained results and provide directions for future research.
6.1. Main Results
Current methods to the induction of functional programs can be classiﬁed according
to two general approaches—the analytical, recurrence detection approach (Section 3.2),
where programs are derived from the provided I/O examples by detecting and inductively
generalizing recurrent structures between them, and, on the other side, the enumerative,
generate-and-test approach (Section 3.4), where candidate programs from some program
class are repeatedly generated and tested until a program is found that satisﬁes the
provided speciﬁcation.
Classical analytical methods are fast but only at the prize of (i) restricted program
schemas, (ii) that only basic datatype constructors and selection functions as primitives
can be used—i.e., that no background knowledge can be provided—, and (iii) that
the provided I/O examples must be complete up to some complexity regarding the
domain—e.g., all lists up to a certain length must be provided as example inputs. On the
other side, generate-and-test methods have no inherent restrictions regarding program
schemas and primitives and are more ﬂexible regarding their speciﬁcations. Provided
sets of I/O examples need not to be complete and also extended forms of speciﬁcations,
going beyond I/O examples or I/O patterns, can be used. But since the construction
of candidate programs is not constrained by the provided speciﬁcation, generate-and-
test methods are in general much more time-consuming than analytical methods (cp.
Table 5.9).
The presented algorithm Igor2 generalizes the recurrence detection method and in-
tegrates it into search operators. The matching of (parts of) I/O examples of the target
function in order to detect regularities and derive a recursive function call is extended to
several target functions and background knowledge—the function call operators χsmplCall
and χcall of Igor2 not only ﬁnd recursive calls but try to also match I/O examples
belonging to diﬀerent functions in order to introduce background functions or mutual
recursive calls.
Furthermore, the program schema of Igor2 is much less restrictive
than that of Summers (Section 3.2.1), for example. The result is that at each synthesis
151

6. Conclusions
step several candidate reﬁnements are possible, i.e., that the synthesis operators become
search operators. Igor2 uses its synthesis operators within a uniform cost search, where
candidate programs with fewer cases are preferred, by assigning them lower costs, in or-
der to achieve a maximal general solution program. However, since candidate programs
are still constructed based on the provided speciﬁcation, often large parts of the pro-
gram space can be pruned. We have started to empirically verify this by means of a
comparison of Igor2 with other inductive programming systems (Section 5.3).
The possibility of pruning the problem space depends on the provided speciﬁcation.
In general, the more structure the I/O examples have, the fewer matchings between
them are possible and the greater parts of the problem space can be pruned. This is
the reason why structural problems on lists such as reverse are generally easier and
faster to solve than, e.g., problems on natural numbers, as the conducted experiments
(Chapter 5) show. In the worst case, if the outputs of speciﬁed functions are simply
variables and hence provide no structure at all, combined with its breadth-ﬁrst search
approach, Igor2 can, at the moment, degenerate to a purely enumerative algorithm (cp.
Section 4.8.5).
The experiments in Chapter 5 show that Igor2 is able to reliably and in reasonable
time induce intended functions for a wide variety of problems ranging from problems
of natural numbers over problems of lists and matrices to problems of domains known
from artiﬁcial intelligence such as the blocksworld or the Towers-of-Hanoi. This shows
the potential of (analytical functional) inductive programming as a tool to assist in
functional program development as well as to model the capability of human cognition
to learn general problem solving strategies from experience.
As one can also see, some relatively simple functions, such as multiplication of natural
numbers, can not be induced without background knowledge, though Igor2 is generally
able to automatically ﬁnd help functions, if needed. E.g., in order to be able to induce
multiplication, Igor2 needs addition as background knowledge. The reason why Igor2
cannot ﬁnd addition automatically as help function for multiplication is a restriction
of the form of generatable function compositions, caused by the currently used synthe-
sis operators. We have not substantially studied the class of inducible functions yet;
however, a short introduction to the mentioned restrictions was given in Section 4.8.4.
We have shown (Section 4.8) that the search conducted by Igor2 is terminating
and complete under certain constraints and that programs induced by Igor2 correctly
compute the speciﬁed I/O examples.
6.2. Future Research
In general, Igor2 shows that combining the analytical approach to the induction of
functional programs with search in a program space is possible and promising. There
are several possible starting points for further research in this general direction.
Depending on the provided speciﬁcation, the analytical techniques more or less prune
the problem space and in the worst case, Igor2 also simply enumerates programs in
an unconstrained way. In this case, explicit generate-and-test systems like ADATE or
152

6.2. Future Research
MagicHaskeller are likely to show a better performance because of better heuristics
for this case or other techniques, like preventing the enumeration of several semantically
equivalent programs. One could try to integrate such techniques into Igor2 or alterna-
tively, to combine a generate-and-test system like ADATE with Igor2. A ﬁrst attempt,
where the author of this thesis contributed, of such a combination is reported in [23].
Another direction could be to integrate general knowledge of (patterns of) algorithms
into Igor2, possibly in the form of program schemas as proposed by several works
concerned with inducing recursive logic programs from examples (cp. Section 3.3.4). Es-
pecially in functional programming, standard higher-order functions like map, reduce,
ﬁlter etc. capture particular recursive schemes and are well-analyzed and frequently
used in functional programming. Currently, Igor2 uses ﬁrst-order term rewriting to ex-
press programs. Yet an extension to higher-order functions and in particular integrating
the mentioned standard higher-order functions as general background knowledge into
Igor2 could be sensible for several reasons.
First, programs become more compact by using predeﬁned higher-order functions,
because the recursive scheme including base- and recursive case is encapsulated in the
deﬁnition of the higher-order function and need not to be re-stated. This potentially
reduces the number of synthesis operator applications until a correct program is found,
i.e., the depth of a solution in the search tree. Second, using these functions would help
to clarify which classes of functions can be induced. Third, providing these functions
as general background knowledge can help to overcome the current restriction regarding
nested function calls as described in Section 4.8.4.
An extension of Igor2 by higher-order functions is described in [37].
Finally, extending the speciﬁcation language by allowing for extensionally quantiﬁed
variables in RHSs of I/O examples or I/O patterns (cp. Section 4.9.3) could be an
approach to relaxing the current requirement of sets of I/O examples or I/O patterns
that are complete up to a certain complexity regarding the domain of the target function.
Besides further developing general inductive programming techniques as described, we
think that it would be useful to speciﬁcally try to tackle particular application areas like
synthesizing prototypes of functions from test-cases in test-driven software development
or learning domain-knowledge in automated planning. We think that identifying speciﬁc
application ﬁelds and domains could help to sensibly identify strengths and weaknesses
of existing methods, to extend them and to identify possibilities to integrate them in a
useful way.
153


Bibliography
[1] David W. Aha, Stephane Lapointe, Charles X. Ling, and Stan Matwin. Inverting
implication with small training sets. In Proceedings of the European Conference on
Machine Learning (ECML’94), volume 784 of LNCS, pages 29–48. Springer-Verlag,
1994.
[2] John R. Anderson and Christian Lebiere. The Atomic Components of Thought.
Lawrence Erlbaum Associates, Inc., 1998.
[3] Dana Angluin. Queries and concept learning. Machine Learning, 2(4):319–342,
1988.
[4] Dana Angluin. Equivalence queries and approximate ﬁngerprints. In Proceedings of
the 2nd Annual Workshop on Computational Learning Theory (COLT’89), pages
134–145, San Francisco, CA, USA, 1989. Morgan Kaufmann Publishers Inc.
[5] Yuichiro Anzai and Herbert A. Simon. The theory of learning by doing. Psycho-
logical Review, 86(2):124–140, 1979.
[6] Franz Baader and Tobias Nipkow.
Term Rewriting and All That.
Cambridge
University Press, 1999.
[7] Kent Beck. Test-Driven Development By Example. Addison-Wesley, 2003.
[8] Henrik Berg, Roland Olsson, Per-Olav Rus˚as, and Morgan Jakobsen. Synthesis of
control algorithms for autonomous vehicles through automatic programming. In
Haiying Wang, Kay Soon Low, Kexin Wei, and Junqing Sun, editors, Fifth In-
ternational Conference on Natural Computation, pages 445–453. IEEE Computer
Society, 2009.
[9] Francesco Bergadano and Daniele Gunetti. An interactive system to learn func-
tional logic programs. In Proceedings of the International Joint Conference on
Artiﬁcial Intelligence (IJCAI’93), 1993.
[10] Francesco Bergadano and Daniele Gunetti. Inductive Logic Programming: From
Machine Learning to Software Engineering. MIT Press, Cambridge, MA, USA,
1995.
[11] Alan W. Biermann. The inference of regular LISP programs from examples. IEEE
Transactions on Systems, Man and Cybernetics, 8(8):585–600, 1978.
155

Bibliography
[12] A.W. Biermann, R.I. Baum, and F.E. Petry. Speeding up the synthesis of programs
from traces. IEEE Transactions on Computers, 24(2):122–136, 1975.
[13] A.W. Biermann and R. Krishnaswamy. Constructing programs from example com-
putations. IEEE Transactions on Software Engineering, 2(3):141–153, 1976.
[14] Franck Binard and Amy Felty. Genetic programming with polymorphic types and
higher-order functions. In Proceedings of the 10th annual Conference on Genetic
and Evolutionary Computation (GECCO’08), pages 1187–1194, New York, NY,
USA, 2008. ACM.
[15] Robert S. Boyer and J. Strother Moore. Proving theorems about LISP functions.
Journal of the ACM, 22(1):129–144, 1975.
[16] Ivan Bratko. Prolog Programming for Artiﬁcial Intelligence. Addison-Wesley, 1986.
[17] R. M. Cameron-Jones and J. R. Quinlan. Avoiding pitfalls when learning recur-
sive theories.
In R. Bajcsy, editor, Proceedings of the 13th International Joint
Conference on Artiﬁcial Intelligence, pages 1050–1057. Morgan Kaufmann, 1993.
[18] Manuel Clavel, Francisco Dur´an, Steven Eker, Patrick Lincoln, Narciso Mart´ı-
Oliet, Jos´e Meseguer, and Carolyn Talcott. The maude 2.0 system. In R. Nieuwen-
huis, editor, Rewriting Techniques and Applications (RTA’03), volume 2706 of
LNCS, pages 76–87. Springer-Verlag, 2003.
[19] William W. Cohen. Pac-learning recursive logic programs: Eﬃcient algorithms.
Journal of Artiﬁcial Intelligence Research, 2:501–539, 1995.
[20] William W. Cohen. Pac-learning recursive logic programs: Negative results. Jour-
nal of Artiﬁcial Intelligence Research, pages 541–573, 1995.
[21] Bruno Courcelle. Recursive applicative program schemes. In Handbook of The-
oretical Computer Science: Formal Models and Semantics, volume B, chapter 9,
pages 459–492. MIT Press, Cambridge, MA, USA, 1990.
[22] M. A. Covington. Natural Language Processing for Prolog Programmers. Prentice
Hall, 1994.
[23] Neil Crossley, Emanuel Kitzelmann, Martin Hofmann, and Ute Schmid. Combining
analytical and evolutionary inductive programming. In B. Goertzel, P. Hitzler, and
M. Hutter, editors, Second Conference on Artiﬁcial General Intelligence, pages 19–
24. Atlantis Press, 2009.
[24] Hartmut Ehrig and Bernd Mahr.
Fundamentals of Algebraic Speciﬁcation I.
Springer-Verlag, 1985.
156

Bibliography
[25] C. Ferri-Ram´ırez, J. Hern´andez-Orallo, and M.J. Ram´ırez-Quintana. Incremen-
tal learning of functional logic programs. In Proceedings of the 5th International
Symposium on Functional and Logic Programming (FLOPS’01), volume 2024 of
LNCS, pages 233–247. Springer-Verlag, 2001.
[26] Pierre Flener.
Logic Program Synthesis from Incomplete Information.
Kluwer
Academic Publishers, 1995.
[27] Pierre Flener. Inductive logic program synthesis with DIALOGS. In S. Muggle-
ton, editor, Selected Papers of the 6th International Workshop on Inductive Logic
Programming, (ILP’96), volume 1314 of LNCS, pages 175–198, 1997.
[28] Pierre Flener, Kung-Kiu Lau, and Mario Ornaghi. On correct program schemas.
In Norbert E. Fuchs, editor, Logic Program Synthesis and Transformation, 7th
International Workshop, LOPSTR’97, Proceedings, volume 1463 of LNCS, pages
128–147. Springer-Verlag, 1998.
[29] Pierre Flener and Ute Schmid. An introduction to inductive programming. Arti-
ﬁcial Intelligence Review, 29(1):45–62, 2008.
[30] Pierre Flener and Serap Yilmaz. Inductive synthesis of recursive logic programs:
Achievements and prospects. The Journal of Logic Programming, 41(2-3):141–195,
1999.
[31] Mitsue Furusawa, Nobuhiro Inuzuka, Hirohisa Seki, and Hidenori Itoh. Induction
of logic programs with more than one recursive clause by analyzing saturations.
In Proceedings of the 7th International Workshop on Inductive Logic Programming
(ILP’97), volume 1297 of LNCS, pages 165–172. Springer-Verlag, 1997.
[32] Malik Ghallab, Dana Nau, and Paolo Traverso. Automated Planning: theory and
practice. Morgan Kaufmann Publishers, 2004.
[33] E. Mark Gold. Language identiﬁcation in the limit. Information and Control,
10(5):447–474, 1967.
[34] Lutz Hamel. Breeding algebraic structures - an evolutionary approach to inductive
equational logic programming. In Proceedings of the Genetic and Evolutionary
Computation Conference (GECCO’02), pages 748–755, San Francisco, CA, USA,
2002. Morgan Kaufmann Publishers Inc.
[35] Lutz Hamel and Chi Shen.
An inductive programming approach to algebraic
speciﬁcation. In Proceedings of the 2nd Workshop on Approaches and Applications
of Inductive Programming (AAIP’07), pages 3–14, 2007.
[36] Martin Hofmann. Automated Construction of XSL-Templates: An Inductive Pro-
gramming Approach. VDM Verlag, 2008.
157

Bibliography
[37] Martin Hofmann and Emanuel Kitzelmann. I/o guided detection of list catamor-
phisms. In John Gallagher and Janis Voigtl¨ander, editors, ACM SIGPLAN 2010
Workshop on Partial Evaluation and Program Manipulation, Proceedings, pages
93–100. ACM Press, 2010.
[38] Martin Hofmann, Emanuel Kitzelmann, and Ute Schmid. A unifying framework
for analysis and evaluation of inductive programming systems. In B. Goertzel,
P. Hitzler, and M. Hutter, editors, Second Conference on Artiﬁcial General Intel-
ligence, pages 55–60. Atlantis Press, 2009.
[39] Peter Idestam-Almquist. Eﬃcient induction of recursive deﬁnitions by structural
analysis of saturations.
In Luc De Raedt, editor, Advances in Inductive Logic
Programming. IOS Press, 1996.
[40] Al´ıpio Jorge and Pavel Brazdil. Architecture for iterative learning of recursive
deﬁnitions. In Luc De Raedt, editor, Advances in Inductive Logic Programming.
IOS Press, 1996.
[41] Al´ıpio M. G. Jorge. Iterative Induction of Logic Programs. PhD thesis, Departa-
mento de Ciˆencia de Computadores, Universidade do Porto, 1998.
[42] J. P. Jouannaud and Y. Kodratoﬀ. Characterization of a class of functions syn-
thesized from examples by a Summers like method using a ‘B.M.W.’ matching
technique. In Sixth International Joint Conference on Artiﬁcial Intelligence (IJ-
CAI’79), pages 440–447. Morgan Kaufmann, 1979.
[43] Jean-Pierre Jouannaud and Yves Kodratoﬀ. Program synthesis from examples of
behavior. In Alan W. Biermann and G´erard Guiho, editors, Computer Program
Synthesis Methodologies, pages 213–250. D. Reidel Publ. Co., 1983.
[44] Stefan Kahrs. Genetic programming with primitive recursion. In Proceedings of the
8th annual Conference on Genetic and Evolutionary Computation (GECCO’06),
pages 941–942, New York, NY, USA, 2006. ACM.
[45] Susumu Katayama. Systematic search for lambda expressions. In Marko C. J. D.
van Eekelen, editor, Revised Selected Papers from the Sixth Symposium on Trends
in Functional Programming, TFP 2005, volume 6, pages 111–126. Intellect, 2007.
[46] Susumu Katayama.
Recent improvements of magichaskeller.
In Ute Schmid,
Emanuel Kitzelmann, and Rinus Plasmeijer, editors, Approaches and Applications
of Inductive Programming, 3rd International Workshop, AAIP’09, Revised Papers,
volume 5812 of LNCS, pages 174–193. Springer-Verlag, 2010.
[47] Emanuel Kitzelmann.
Inductive functional program synthesis – a term-
construction and folding approach. Diplomarbeit, Technische Universit¨at Berlin,
2003. Unpublished.
158

Bibliography
[48] Emanuel Kitzelmann. Analytical inductive functional programming. In Michael
Hanus, editor, 18th International Symposium on Logic-Based Program Synthesis
and Transformation, Revised Selected Papers, volume 5438 of LNCS, pages 87–102.
Springer-Verlag, 2009.
[49] Emanuel Kitzelmann.
Inductive programming: A survey of program synthesis
techniques. In Ute Schmid, Emanuel Kitzelmann, and Rinus Plasmeijer, editors,
Approaches and Applications of Inductive Programming, 3rd Workshop AAIP, Re-
vised Papers, volume 5812 of LNCS, pages 50–73. Springer-Verlag, 2010.
[50] Emanuel Kitzelmann and Ute Schmid. An explanation based generalization ap-
proach to inductive synthesis of functional programs. In Emanuel Kitzelmann,
Roland Olsson, and Ute Schmid, editors, ICML-2005 Workshop on Approaches
and Applications of Inductive Programming, pages 15–27, 2005.
[51] Emanuel Kitzelmann and Ute Schmid. Inductive synthesis of functional programs:
An explanation based generalization approach. Journal of Machine Learning Re-
search, 7:429–454, 2006. Revised version of [50].
[52] Y. Kodratoﬀand J. Fargues. A sane algorithm for the synthesis of LISP functions
from example problems: The Boyer and Moore algorithm. In Proceedings of the
AISB/GI Conference on Artiﬁcial Intelligence, pages 169–175, Hamburg, 1978.
AISB and GI.
[53] Yves Kodratoﬀ. A class of functions synthesized from a ﬁnite number of examples
and a LISP program scheme. International Journal of Computer and Information
Sciences, 8(6):489–521, 1979.
[54] Pieter Koopman, Artem Alimarine, Jan Tretmans, and Rinus Plasmeijer. GAST:
Generic automated software testing. In Implementation of Functional Languages
(IFL’02), volume 2670 of LNCS. Springer, 2003.
[55] Pieter Koopman and Rinus Plasmeijer. Synthesis of functions using generic pro-
gramming. In Ute Schmid, Emanuel Kitzelmann, and Rinus Plasmeijer, editors,
Approaches and Applications of Inductive Programming, 3rd International Work-
shop, AAIP’09, Revised Papers, volume 5812 of LNCS, pages 25–49. Springer-
Verlag, 2010.
[56] Pieter W. M. Koopman and Rinus Plasmeijer. Systematic synthesis of functions.
In Henrik Nilsson, editor, Revised Selected Papers from the Seventh Symposium on
Trends in Functional Programming, TFP 2006, volume 7, pages 35–54. Intellect,
2007.
[57] Richard E. Korf. Macro-operators: A weak method for learning. Artiﬁcial Intelli-
gence, 26(1):35–77, 1985.
[58] John R. Koza.
Genetic Programming: On the Programming of Computers by
Means of Natural Selection. MIT Press, Cambridge, MA, USA, 1992.
159

Bibliography
[59] John R. Koza, David Andre, Forrest H. Bennett, and Martin A. Keane. Genetic
Programming III: Darwinian Invention & Problem Solving. Morgan Kaufmann
Publishers Inc., San Francisco, CA, USA, 1999.
[60] St´ephane Lapointe and Stan Matwin. Sub-uniﬁcation: a tool for eﬃcient induction
of recursive programs. In ML92: Proceedings of the Ninth International Workshop
on Machine Learning, pages 273–281, San Francisco, CA, USA, 1992. Morgan
Kaufmann Publishers Inc.
[61] Leonid A. Levin. Universal sequential search problems. Problems of Information
Transmission, 9(3), 1973.
[62] Henry Lieberman, editor. Your Wish is My Command: Programming by Example.
Morgan Kaufmann Publishers, 2001.
[63] Donato Malerba. Learning recursive theories in the normal ILP setting. Funda-
menta Informaticae, 57(1):39–77, 2003.
[64] Z. Manna and R. Waldinger. How to clear a block: A theory of plans. Journal of
Automated Reasoning, 3(4):343–378, 1987.
[65] Zohar Manna and Richard Waldinger. A deductive approach to program synthesis.
ACM Transactions on Programming Languages and Systems, 2(1):90–121, 1980.
[66] John McCarthy. Recursive functions of symbolic expressions and their computation
by machine, part i. Communications of the ACM, 3(4):184–195, 1960.
[67] Jos´e Meseguer and Joseph A. Goguen. Initiality, induction, and computability. In
Maurice Nivat and John C. Reynolds, editors, Algebraic Methods in Semantics,
pages 459–541. Cambridge University Press, 1985.
[68] Ryszard S. Michalski and Robert E. Stepp. Learning from observation: Conceptual
clustering. In Ryszard S. Michalski, Jaime G. Carbonell, and Tom M. Mitchell,
editors, Machine Learning: An Artiﬁcial Intelligence Approach, chapter 11, pages
331–364. Tioga, 1983.
[69] Thomas M. Mitchell. Machine Learning. McGraw-Hill Higher Education, 1997.
[70] Chowdhury R. Moﬁzur and Masayuki Numao. Top-down induction of recursive
programs from small number of sparse examples. In Luc De Raedt, editor, Ad-
vances in Inductive Logic Programming. IOS Press, 1996.
[71] David J. Montana. Strongly typed genetic programming. Evolutionary Computu-
tation, 3(2):199–230, 1995.
[72] S. H. Muggleton and C. Feng. Eﬃcient induction of logic programs. In Proceedings
of the First Conference on Algorithmic Learning Theory, pages 368–381, Tokyo,
Japan, 1990. Ohmsha.
160

Bibliography
[73] Stephen Muggleton. Duce, an oracle-based approach to constructive induction. In
Proceedings of the 10th International Joint Conference on Artiﬁcial Intelligence,
volume 1, pages 287–292. Morgan Kaufmann Publishers, 1987.
[74] Stephen Muggleton. Inductive logic programming. New Generation Computing,
8(4):295–318, 1991.
[75] Stephen Muggleton and Wray L. Buntine. Machine invention of ﬁrst-order pred-
icates by inverting resolution. In J. Laird, editor, Proceedings of the 5th Inter-
national Conference on Machine Learning (ICML’88), pages 339–352. Morgan
Kaufmann, 1988.
[76] Stephen H. Muggleton. Inductive logic programming: Derivations, successes and
shortcomings. SIGART Bulletin, 5(1):5–11, 1994.
[77] Stephen H. Muggleton. Inverse entailment and progol. New Generation Computing,
13:245–286, 1995.
[78] Stephen H. Muggleton and Luc De Raedt. Inductive logic programming: Theory
and methods. Journal of Logic Programming, 19,20:629–679, 1994.
[79] Martin M¨uhlpfordt. Syntaktische Inferenz Rekursiver Programmschemata. Diplo-
marbeit, Technische Universit¨at Berlin, 2000.
[80] Shan-Hwei Nienhuys-Cheng and Roland de Wolf. Subsumption theorem and refu-
tation completeness. In Foundations of Inductive Logic Programming, volume 1228
of LNAI, chapter 5. Springer-Verlag, 1997.
[81] Shan-Hwei Nienhuys-Cheng and Ronald de Wolf. Foundations of Inductive Logic
Programming, volume 1228 of LNAI. Springer-Verlag, 1997.
[82] J. R. Olsson. Inductive functional programming using incremental program trans-
formation. Artiﬁcial Intelligence, 74(1):55 – 83, 1995.
[83] J. R. Olsson and D. M. W. Powers. Machine learning of human language through
automatic programming. In Proceedings of the International Conference on Cog-
nitive Science, pages 507–512, 2003.
[84] Dusko Pavlovic and Douglas R. Smith. Software development by reﬁnement. In
Formal Methods at the Crossroads: From Panacea to Foundational Support, volume
2757 of LNCS, pages 267–286, 2003.
[85] Gordon D. Plotkin.
A note on inductive generalization.
Machine Intelligence,
5:153–163, 1970.
[86] J. R. Quinlan and R. M. Cameron-Jones. FOIL: A midterm report. In P. Brazdil,
editor, Proceedings of the 6th European Conference on Machine Learning, LNCS,
pages 3–20, London, UK, 1993. Springer-Verlag.
161

Bibliography
[87] J. Ross Quinlan. Induction of decision trees. Machine Learning, 1(1):81–106, 1986.
[88] Luc De Raedt and Luc Dehaspe. Clausal discovery. Machine Learning, 26(2):99–
146, 1997.
[89] Riverson Rios and Stan Matwin. Eﬃcient induction of recursive prolog deﬁnitions.
In Proceedings of the 11th Biennial Conference of the Canadian Society for Com-
putational Studies of Intelligence on Advances in Artiﬁcial Intelligence, volume
1081 of LNCS, pages 240–248. Springer-Verlag, 1996.
[90] Ra´ul Rojas. Neural Networks - A Systematic Introduction. Springer-Verlag, 1996.
[91] Paul S. Rosenbloom and Alan Newell. The chunking of goal hierarchies: A gen-
eralized model of practice.
In Ryszard S. Michalski, Jaime G. Carbonell, and
Tom M. Mitchell, editors, Machine Learning: An Artiﬁcial Intelligence Approach,
volume 2, chapter 10. Morgan Kaufmann Publishers, 1986.
[92] Stuart Russell and Peter Norvig.
Artiﬁcial Intelligence: A Modern Approach.
Prentice Hall, 3 edition, 2010.
[93] Yasubumi Sakakibara.
Recent advances of grammatical inference.
Theoretical
Computer Science, 185(1):15–45, 1997.
[94] Ute Schmid.
Inductive Synthesis of Functional Programs: Universal Planning,
Folding of Finite Programs, and Schema Abstraction by Analogical Reasoning, vol-
ume 2654 of LNAI. Springer, Berlin; New York, 2003.
[95] Ute Schmid, Martin Hofmann, and Emanuel Kitzelmann. Analytical inductive
programming as a cognitive rule acquisition device. In B. Goertzel, P. Hitzler, and
M. Hutter, editors, Second Conference on Artiﬁcial General Intelligence, pages
162–167. Atlantis Press, 2009.
[96] Ute Schmid and Fritz Wysotzki. Applying inductive program synthesis to macro
learning. In Steve Chien, Subbarao Kambhampati, and Craig A. Knoblock, ed-
itors, Proceedings of the Fifth International Conference on Artiﬁcial Intelligence
Planning Systems, pages 371–378. AAAI, 2000.
[97] J¨urgen Schmidhuber.
Optimal ordered problem solver.
Machine Learning,
54(3):211–254, 2004.
[98] Uwe Sch¨oning.
Logic for Computer Scientists.
Modern Birkh¨auser Classics.
Birkh¨auser Boston, 2008.
[99] Ehud Y. Shapiro. Algorithmic Program Debugging. MIT Press, 1983.
[100] Douglas R. Smith. The synthesis of LISP programs from examples: A survey. In
A.W. Biermann, G. Guiho, and Y. Kodratoﬀ, editors, Automatic Program Con-
struction Techniques, pages 307–324. Macmillan, 1984.
162

Bibliography
[101] Irene Stahl. The appropriateness of predicate invention as bias shift operation in
ilp. Machine Learning, 20(1):95–117, 1995.
[102] Irene Stahl and Irene Weber. The arguments of newly invented predicates in ILP.
In Proceedings of the Fourth Workshop on Inductive Logic Programming (ILP’94),
1994.
[103] P. D. Summers.
Program Construction from Examples.
PhD thesis, Dept. of
Computer Science, Yale University, New Haven, US-CT, 1975.
[104] Phillip D. Summers. A methodology for LISP program construction from examples.
Journal of the ACM, 24(1):161–175, 1977.
[105] Terese. Term Rewriting Systems, volume 55 of Cambridge Tracts in Theoretical
Computer Science. Cambridge University Press, 2003.
[106] Paul E. Utgoﬀ. Shift of bias for inductive concept learning. In Ryszard S. Michalski,
Jaime G. Carbonell, and Tom M. Mitchell, editors, Machine Learning: An Artiﬁ-
cial Intelligence Approach, volume 2, chapter 5, pages 107–148. Morgan Kaufmann
Publishers, 1983.
[107] L. G. Valiant.
A theory of the learnable.
Communications of the ACM,
27(11):1134–1142, 1984.
[108] B. Wegbreit. Goal-directed program transformation. IEEE Transactions on Soft-
ware Engineering, 2(2):69–80, 1976.
[109] Man Wong and Tuen Mun. Evolving recursive programs by using adaptive gram-
mar based genetic programming. Genetic Programming and Evolvable Machines,
6(4):421–455, 2005.
[110] Tina Yu.
Hierarchical processing for evolving recursive and modular programs
using higher-order functions and lambda abstraction. Genetic Programming and
Evolvable Machines, 2(4):345–380, 2001.
[111] Tina Yu.
A higher-order function approach to evolve recursive programs.
In
Tina Yu, Rick L. Riolo, and Bill Worzel, editors, Genetic Programming Theory
and Practice III, volume 9 of Genetic Programming, chapter 7, pages 93–108.
Springer-Verlag, 2006.
163


A. Speciﬁcations of the Experiments
In the following we list the complete speciﬁcation ﬁles as provided to Igor2 for the
experiments conducted in Chapter 5.
A.1. Natural Numbers
fmod IGOR−NAT i s
s o r t
INat
.
op 0
: −> INat
[ c t o r ]
.
op S
:
INat −> INat
[ c t o r ]
.
op
=
:
INat
INat −> Bool
.
op
<=
:
INat
INat −> Bool
.
op odd
:
INat −> Bool
.
op even
:
INat −> Bool
.
op
+
:
INat
INat −> INat
.
op
−
:
INat
INat −> INat
.
op
∗
:
INat
INat −> INat
.
op
fac
:
INat −> INat
.
op
f i b
:
INat −> INat
.
op ack
:
INat
INat −> INat
.
∗∗∗
f o r
i n t e r n a l
e n c a p s u l a t i o n
of
∗∗∗
f u n c t i o n
arguments
i n t o
a
t u p l e
s o r t
InVec
.
op
i n
:
INat
INat −> InVec
[ c t o r ]
.
op
i n
:
INat −> InVec
[ c t o r ]
.
var
x
:
INat
.
eq
(0
= 0)
= t r u e
.
eq
(0
= S 0)
= f a l s e
.
eq
(0
= S S 0) = f a l s e
.
eq
(S 0
= 0)
= f a l s e
.
eq
(S 0
= S 0)
= t r u e
.
eq
(S 0
= S S 0) = f a l s e
.
eq
(S S 0 = 0)
= f a l s e
.
eq
(S S 0 = S 0)
= f a l s e
.
eq
(S S 0 = S S 0) = t r u e
.
eq 0
<= x
= t r u e
.
eq S 0
<= 0
= f a l s e
.
eq S 0
<= S x
= t r u e
.
eq S S 0 <= 0
= f a l s e
.
165

A. Speciﬁcations of the Experiments
eq S S 0 <= S 0
= f a l s e
.
eq S S 0 <= S S x = t r u e
.
eq odd
(0)
= f a l s e
.
eq odd (S 0)
= t r u e
.
eq odd (S S 0)
= f a l s e
.
eq odd (S S S 0) = t r u e
.
eq
even
(0)
= t r u e
.
eq
even
(S 0)
= f a l s e
.
eq
even
(S S 0)
= t r u e
.
eq
even
(S S S 0) = f a l s e
.
eq 0
+ x = x
.
eq S 0
+ x = S x
.
eq S S 0
+ x = S S x
.
eq S S S 0 + x = S S S x
.
eq 0
∗x
= 0
.
eq S 0
∗x
= x
.
eq S S 0 ∗0
= 0
.
eq S S 0 ∗S 0
= S S 0
.
eq S S 0 ∗S S 0
= S S S S 0
.
eq S S 0 ∗S S S 0 = S S S S S S 0
.
eq
fac
(0)
= S 0
.
eq
fac
(S 0)
= S 0
.
eq
fac
(S S 0)
= S S 0
.
eq
fac
(S S S 0) = S S S S S S 0
.
eq
f i b (0)
= 0
.
eq
f i b (S 0)
= S 0
.
eq
f i b (S S 0)
= S 0
.
eq
f i b (S S S 0)
= S S 0
.
eq
f i b (S S S S 0)
= S S S 0
.
eq
f i b (S S S S S 0) = S S S S S 0
.
eq ack (0 ,
0)
= S 0
.
eq ack (0 , S 0)
= S S 0
.
eq ack (0 , S S 0)
= S S S 0
.
eq ack (0 , S S S 0)
= S S S S 0
.
eq ack (0 , S S S S 0)
= S S S S S 0
.
eq ack (0 , S S S S S 0)
= S S S S S S 0
.
eq ack (0 , S S S S S S 0) = S S S S S S S 0
.
eq ack (S 0 ,
0)
= S S 0
.
eq ack (S 0 , S 0)
= S S S 0
.
eq ack (S 0 , S S 0)
= S S S S 0
.
eq ack (S 0 , S S S 0)
= S S S S S 0
.
eq ack (S 0 , S S S S 0)
= S S S S S S 0
.
eq ack (S 0 , S S S S S 0) = S S S S S S S 0
.
eq ack (S S 0 ,
0)
= S S S 0
.
eq ack (S S 0 , S 0)
= S S S S S 0
.
eq ack (S S 0 , S S 0)
= S S S S S S S 0
.
166

A.2. Lists
eq ack (S S S 0 ,
0)
= S S S S S 0
.
endfm
A.2. Lists
General Lists
fmod PLIST{X
: :
TRIV}
i s
i n c IGOR−NAT .
s o r t s
L i s t {X}
L i s t P a i r {X}
.
∗∗∗
a l s o
p a i r s
of
X$Elt
s h a l l
be
X$Elt
. . .
∗∗∗
f o r
z i p
op < , > :
X$Elt
X$Elt −> X$Elt
[ c t o r ]
.
∗∗∗
l i s t
c o n s t r u c t o r s
op
[ ]
: −> L i s t {X}
[ c t o r ]
.
op
:
:
X$Elt
L i s t {X} −> L i s t {X}
[ c t o r ]
.
∗∗∗
p a i r
of
l i s t s
c o n s t r u c t o r
op < ; > :
L i s t {X}
L i s t {X} −> L i s t P a i r {X}
[ c t o r ]
.
op head
:
L i s t {X} −> X$Elt
.
op
t a i l
:
L i s t {X} −> L i s t {X}
.
op
i n i t
:
L i s t {X} −> L i s t {X}
.
op
l a s t
:
L i s t {X} −> X$Elt
.
op
++
:
L i s t {X}
L i s t {X} −> L i s t {X}
.
op
l e n gth
:
L i s t {X} −> INat
.
op
r e v e r s e
:
L i s t {X} −> L i s t {X}
.
op
! !
:
L i s t {X} INat −> X$Elt
.
op
take
:
INat
L i s t {X} −> L i s t {X}
.
op drop
:
INat
L i s t {X} −> L i s t {X}
.
op
s p l i t A t
:
INat
L i s t {X} −> L i s t P a i r {X}
.
op
s p l i t
:
L i s t {X} −> L i s t P a i r {X}
.
op evenpos
:
L i s t {X} −> L i s t {X}
.
op oddpos
:
L i s t {X} −> L i s t {X}
.
op
r e p l i c a t e
:
INat
X$Elt −> L i s t {X}
.
op
i n t e r s p e r s e
:
X$Elt
L i s t {X} −> L i s t {X}
.
op
z i p
:
L i s t {X}
L i s t {X} −> L i s t {X}
.
op
unzip
:
L i s t {X} −> L i s t P a i r {X}
.
op
s h i f t L
:
L i s t {X} −> L i s t {X}
.
op
s h i f t R
:
L i s t {X} −> L i s t {X}
.
op swap
:
L i s t {X} INat
INat −> L i s t {X}
.
op swap2
:
L i s t {X} INat
INat −> L i s t {X}
.
∗∗∗
input
arguments
e n c a p s u l a t i o n
op
i n
:
L i s t {X} −> InVec
[ c t o r ]
.
op
i n
:
L i s t {X}
L i s t {X} −> InVec
[ c t o r ]
.
op
i n
:
L i s t {X} INat −> InVec
[ c t o r ]
.
op
i n
:
INat
L i s t {X} −> InVec
[ c t o r ]
.
op
i n
:
INat
X$Elt −> InVec
[ c t o r ]
.
op
i n
:
X$Elt
L i s t {X} −> InVec
[ c t o r ]
.
167

A. Speciﬁcations of the Experiments
op
i n
:
L i s t {X} INat
INat −> InVec
[ c t o r ]
.
var n
:
INat
.
v a rs
x y z v
:
X$Elt
.
var
xs
ys
:
L i s t {X}
.
eq head
( x
:
xs ) = x
.
eq
t a i l
( x
:
xs ) = xs
.
eq
i n i t
( x
:
[ ] )
= [ ]
.
eq
i n i t
( x
:
y
:
[ ] )
= x
:
[ ]
.
eq
i n i t
( x
:
y
:
z
:
[ ] )
= x
:
y
:
[ ]
.
eq
i n i t
( x
:
y
:
z
:
v
:
[ ] ) = x
:
y
:
z
:
[ ]
.
eq
l a s t
( x
:
[ ] )
= x
.
eq
l a s t
( x
:
y
:
[ ] )
= y
.
eq
l a s t
( x
:
y
:
z
:
[ ] )
= z
.
eq
l a s t
( x
:
y
:
z
:
v
:
[ ] ) = v
.
eq
[ ] ++ xs
= xs
.
eq
( x
:
[ ] ) ++ xs
= x
:
xs
.
eq
( x
:
y
:
[ ] ) ++ xs
= x
:
y
:
xs
.
eq
length
( [ ] )
= 0
.
eq
length
( x
:
[ ] )
= S 0
.
eq
length
( x
:
y
:
[ ] )
= S S 0
.
eq
take
(0 ,
xs )
= [ ]
.
eq
take
(S n ,
[ ] )
= [ ]
.
eq
take
(S 0 , x
:
xs )
= x
:
[ ]
.
eq
take
(S S 0 , x
:
[ ] )
= x
:
[ ]
.
eq
take
(S S 0 , x
:
y
:
xs ) = x
:
y
:
[ ]
.
eq
drop
(0 ,
xs )
= xs
.
eq
drop
(S n ,
[ ] )
= [ ]
.
eq
drop
(S 0 , x
:
xs )
= xs
.
eq
drop
(S S 0 , x
:
[ ] )
= [ ]
.
eq
drop
(S S 0 , x
:
y
:
xs ) = xs
.
eq
r e v e r s e
( [ ] )
= [ ]
.
eq
r e v e r s e
( x
:
[ ] )
= x
:
[ ]
.
eq
r e v e r s e
( x
:
y
:
[ ] )
= y
:
x
:
[ ]
.
eq
r e v e r s e
( x
:
y
:
z
:
[ ] )
= z
:
y
:
x
:
[ ]
.
eq
r e v e r s e
( x
:
y
:
z
:
v
:
[ ] ) = v
:
z
:
y
:
x
:
[ ]
.
eq
s p l i t A t
(0 ,
xs )
= < [ ]
;
xs > .
eq
s p l i t A t
(S n ,
[ ] )
= < [ ]
;
[ ] > .
eq
s p l i t A t
(S 0 , x
:
xs )
= < ( x
:
[ ] )
;
xs > .
eq
s p l i t A t
(S S 0 , x
:
[ ] )
= < ( x
:
[ ] )
;
[ ] > .
eq
s p l i t A t
(S S 0 , x
:
y
:
xs ) = < ( x
:
y
:
[ ] )
;
xs > .
eq
s p l i t
( [ ] )
= < [ ]
;
[ ]
> .
eq
s p l i t
( x
:
[ ] )
= < ( x
:
[ ] )
;
[ ]
> .
168

A.2. Lists
eq
s p l i t
( x
:
y
:
[ ] )
= < ( x
:
[ ] )
;
( y
:
[ ] )
> .
eq
s p l i t
( x
:
y
:
z
:
[ ] )
= < ( x
:
z
:
[ ] )
;
( y
:
[ ] )
> .
eq
s p l i t
( x
:
y
:
z
:
v
:
[ ] ) = < ( x
:
z
:
[ ] )
;
( y
:
v
:
[ ] ) > .
eq
evenpos
( [ ] )
= [ ]
.
eq
evenpos
( x
:
[ ] )
= [ ]
.
eq
evenpos
( x
:
y
:
[ ] )
= y
:
[ ]
.
eq
evenpos
( x
:
y
:
z
:
[ ] )
= y
:
[ ]
.
eq
evenpos
( x
:
y
:
z
:
v
:
[ ] ) = y
:
v
:
[ ]
.
eq oddpos
( [ ] )
= [ ]
.
eq oddpos
( x
:
[ ] )
= x
:
[ ]
.
eq oddpos
( x
:
y
:
[ ] )
= x
:
[ ]
.
eq oddpos
( x
:
y
:
z
:
[ ] )
= x
:
z
:
[ ]
.
eq oddpos
( x
:
y
:
z
:
v
:
[ ] ) = x
:
z
:
[ ]
.
eq
r e p l i c a t e
(0 ,
x )
= [ ]
.
eq
r e p l i c a t e
(S 0 , x )
= x
:
[ ]
.
eq
r e p l i c a t e
(S S 0 , x )
= x
:
x
:
[ ]
.
eq
z i p
( [ ] ,
ys )
= [ ]
.
eq
z i p
( x
:
xs ,
[ ] )
= [ ]
.
eq
z i p
( x
:
[ ] ,
y
:
[ ] )
= < x
, y > :
[ ]
.
eq
z i p
( x
:
[ ] ,
y
:
z
:
[ ] )
= < x
, y > :
[ ]
.
eq
z i p
( x
:
y
:
[ ] ,
z
:
[ ] )
= < x
,
z > :
[ ]
.
eq
z i p
( x
:
y
:
[ ] ,
z
:
v
:
[ ] ) = < x
,
z > : < y
, v > :
[ ]
.
eq
unzip
( [ ] )
= < [ ]
;
[ ] > .
eq
unzip (< x
, y > :
[ ] )
= < ( x
:
[ ] )
;
( y
:
[ ] ) > .
eq
unzip (< x
, y > : < z
, v > :
[ ] ) = < ( x
:
z
:
[ ] )
;
( y
:
v
:
[ ] ) >
.
eq
( x
:
xs )
! !
0 = x
.
eq
( x
:
y
:
xs )
! !
(S 0) = y
.
eq
( x
:
y
:
z
:
xs )
! !
(S S 0) = z
.
eq
i n t e r s p e r s e
( x ,
[ ] )
= [ ]
.
eq
i n t e r s p e r s e
( x ,
y
:
[ ] )
= y
:
[ ]
.
eq
i n t e r s p e r s e
( x ,
y
:
z
:
[ ] )
= y
:
x
:
z
:
[ ]
.
eq
i n t e r s p e r s e
( x ,
y
:
z
:
v
:
[ ] ) = y
:
x
:
z
:
x
:
v
:
[ ]
.
eq
s h i f t L
( [ ] )
= [ ]
.
eq
s h i f t L
( x
:
[ ] )
= x
:
[ ]
.
eq
s h i f t L
( x
:
y
:
[ ] )
= y
:
x
:
[ ]
.
eq
s h i f t L
( x
:
y
:
z
:
[ ] )
= y
:
z
:
x
:
[ ]
.
eq
s h i f t R
( [ ] )
= [ ]
.
eq
s h i f t R
( x
:
[ ] )
= x
:
[ ]
.
eq
s h i f t R
( x
:
y
:
[ ] )
= y
:
x
:
[ ]
.
eq
s h i f t R
( x
:
y
:
z
:
[ ] )
= z
:
x
:
y
:
[ ]
.
eq swap ( x
:
y
:
xs ,
0 ,
S 0)
= y
:
x
:
xs
.
eq swap ( x
:
y
:
z
:
xs ,
0 ,
S S 0)
= z
:
y
:
x
:
xs
.
169

A. Speciﬁcations of the Experiments
eq swap ( x
:
y
:
z
:
xs ,
S 0 ,
S S 0)
= x
:
z
:
y
:
xs
.
eq swap ( x
:
y
:
z
:
v
:
xs ,
0 ,
S S S 0) = v
:
y
:
z
:
x
:
xs
.
eq swap ( x
:
y
:
z
:
v
:
xs ,
S 0 ,
S S S 0) = x
:
v
:
z
:
y
:
xs
.
eq swap ( x
:
y
:
z
:
v
:
xs ,
S S 0 , S S S 0) = x
:
y
:
v
:
z
:
xs
.
endfm
Lists of Natural Numbers
fmod NATLIST
i s
i n c
PLIST{ INat }
.
ops sum sum2 prod
:
L i s t { INat } −> INat
.
v a rs
x y z
:
INat
.
eq sum ( [ ] )
= 0
.
eq sum(0
:
[ ] )
= 0
.
eq sum (( S 0)
:
[ ] )
= S 0
.
eq sum (( S S 0)
:
[ ] )
= S S 0
.
eq sum(0
:
0
:
[ ] )
= 0
.
eq sum(0
:
(S 0)
:
[ ] )
= S 0
.
eq sum(0
:
(S S 0)
:
[ ] )
= S S 0
.
eq sum (( S 0)
:
0
:
[ ] )
= S 0
.
eq sum (( S 0)
:
(S 0)
:
[ ] )
= S S 0
.
eq sum (( S 0)
:
(S S 0)
:
[ ] )
= S S S 0
.
eq sum (( S S 0)
:
0
:
[ ] )
= S S 0
.
eq sum (( S S 0)
:
(S 0)
:
[ ] )
= S S S 0
.
eq sum (( S S 0)
:
(S S 0)
:
[ ] ) = S S S S 0
.
eq sum2 ( [ ] )
= 0
.
eq sum2( x
:
[ ] )
= x
.
eq sum2( x
:
y
:
[ ] )
= x + y
.
eq sum2( x
:
y
:
z
:
[ ] )
= x + ( y + z )
.
eq
prod ( [ ] )
= 0
.
eq
prod (0
:
[ ] )
= 0
.
eq
prod (( S 0)
:
[ ] )
= S 0
.
eq
prod (( S S 0)
:
[ ] )
= S S 0
.
eq
prod (0
:
0
:
[ ] )
= 0
.
eq
prod (0
:
(S 0)
:
[ ] )
= 0
.
eq
prod (0
:
(S S 0)
:
[ ] )
= 0
.
eq
prod (( S 0)
:
0
:
[ ] )
= 0
.
eq
prod (( S 0)
:
(S 0)
:
[ ] )
= S 0
.
eq
prod (( S 0)
:
(S S 0)
:
[ ] )
= S S 0
.
eq
prod (( S S 0)
:
0
:
[ ] )
= 0
.
eq
prod (( S S 0)
:
(S 0)
:
[ ] )
= S S 0
.
eq
prod (( S S 0)
:
(S S 0)
:
[ ] ) = S S S S 0
.
endfm
A.3. Lists of Lists
fmod LISTOFNATLISTS
i s
i n c
PLIST{ NatList } ∗
170

A.3. Lists of Lists
( op
:
:
L i s t { INat }
L i s t { NatList } −> L i s t { NatList } to
: :
,
op
[ ]
: −> L i s t { NatList } to
n i l ,
op
++
:
L i s t { NatList }
L i s t { NatList } −> L i s t { NatList } to
+++ ,
op
s h i f t L
:
L i s t { NatList } −> L i s t { NatList } to
S h i f t L )
.
op
concat
:
L i s t { NatList } −> L i s t { INat }
.
op map :
:
INat
L i s t { NatList } −> L i s t { NatList }
.
op
i n i t s
:
L i s t { INat } −> L i s t { NatList }
.
op
t a i l s
:
L i s t { INat } −> L i s t { NatList }
.
op
subsequences
:
L i s t { INat } −> L i s t { NatList }
.
op
t a i l s 2
:
L i s t { NatList } −> L i s t { NatList }
.
op
t r a n s p o s e
:
L i s t { NatList } −> L i s t { NatList }
.
op weave
:
L i s t { NatList } −> L i s t { INat }
.
v a r s
x y z v
:
INat
.
v a r s
a11 a12 a13 a21 a22 a23 a31 a32 a33
b11 b12 b13 b21 b22 b31
:
INat
.
v a r s
xs
ys
zs
:
L i s t { INat }
.
eq
concat
( n i l )
= [ ]
.
eq
concat
( [ ]
: :
n i l )
= [ ]
.
eq
concat
(( x
:
[ ] )
: :
n i l )
= x
:
[ ]
.
eq
concat
(( x
:
y
:
[ ] )
: :
n i l )
= x
:
y
:
[ ]
.
eq
concat
( [ ]
: :
[ ]
: :
n i l )
= [ ]
.
eq
concat
( [ ]
: :
( x
:
[ ] )
: :
n i l )
= x
:
[ ]
.
eq
concat
( [ ]
: :
( x
:
y
:
[ ] )
: :
n i l )
= x
:
y
:
[ ]
.
eq
concat
(( x
:
[ ] )
: :
[ ]
: :
n i l )
= x
:
[ ]
.
eq
concat
(( x
:
[ ] )
: :
( y
:
[ ] )
: :
n i l )
= x
:
y
:
[ ]
.
eq
concat
(( x
:
[ ] )
: :
( y
:
z
:
[ ] )
: :
n i l )
= x
:
y
:
z
:
[ ]
.
eq
concat
(( x
:
y
:
[ ] )
: :
[ ]
: :
n i l )
= x
:
y
:
[ ]
.
eq
concat
(( x
:
y
:
[ ] )
: :
( z
:
[ ] )
: :
n i l )
= x
:
y
:
z
:
[ ]
.
eq
concat
(( x
:
y
:
[ ] )
: :
( z
:
v
:
[ ] )
: :
n i l ) = x
:
y
:
z
:
v
:
[ ]
.
eq map :
( x ,
n i l )
= n i l
.
eq map :
( x ,
xs
: :
n i l )
= ( x
:
xs )
: :
n i l
.
eq map :
( x ,
xs
: :
ys
: :
n i l )
= ( x
:
xs )
: :
( x
:
ys )
: :
n i l
.
∗∗∗map :
a l s
bk
eq
i n i t s
( [ ] )
= [ ]
: :
n i l
.
eq
i n i t s
( x
:
[ ] )
= [ ]
: :
( x
:
[ ] )
: :
n i l
.
eq
i n i t s
( x
:
y
:
[ ] ) = [ ]
: :
( x
:
[ ] )
: :
( x
:
y
:
[ ] )
: :
n i l
.
eq
t a i l s
( [ ] )
= [ ]
: :
n i l
.
eq
t a i l s
( x
:
[ ] )
= ( x
:
[ ] )
: :
[ ]
: :
n i l
.
eq
t a i l s
( x
:
y
:
[ ] ) = ( x
:
y
:
[ ] )
: :
( y
:
[ ] )
: :
[ ]
: :
n i l
.
171

A. Speciﬁcations of the Experiments
∗∗∗
r e q u i r e s
max depth
of
at
l e a s t
3
∗∗∗append und map :
a l s
bk ,
eq
subsequences
( [ ] ) = n i l
.
eq
subsequences
( x
:
[ ] ) = [ ]
: :
x
:
[ ]
: :
n i l
.
eq
subsequences
( x
:
y
:
[ ] ) =
[ ]
: :
y
:
[ ]
: :
x
:
[ ]
: :
x
:
y
:
[ ]
: :
n i l
.
eq
t a i l s 2
( n i l )
= n i l
.
eq
t a i l s 2
( x
:
xs
: :
n i l )
= xs
: :
n i l
.
eq
t a i l s 2
( x
:
xs
: :
y
:
ys
: :
n i l ) = xs
: :
ys
: :
n i l
.
eq
t a i l s 2
( x
:
xs
: :
y
:
ys
: :
z
:
zs
: :
n i l ) = xs
: :
ys
: :
zs
: :
n i l
.
∗∗∗
t r a n s p o s e
f u n k t i o n i e r t
f u e r
v o l l e
matrizen
( min .
1 element
und
∗∗∗
a l l e
r e i h e n
g l e i c h e
anzahl
elemente )
mit
t a i l s 2
a l s
bk
∗∗∗
t a i l s 2
s e l b e r
g e n e r i e r e n :
time
o f f
∗∗∗
b e i s p i e l e :
b i s
zu 3
s p a l t e n
und
r e i h e n
∗∗∗max (3
reihen ,
3
s p a l t e n )
kann auch
weggelassen
werden
∗∗∗one row
eq
t r a n s p o s e
( a11
:
[ ]
: :
n i l ) = a11
:
[ ]
: :
n i l
.
eq
t r a n s p o s e
( a11
:
a12
:
[ ]
: :
n i l ) =
a11
:
[ ]
: :
a12
:
[ ]
: :
n i l
.
eq
t r a n s p o s e
( a11
:
a12
:
a13
:
[ ]
: :
n i l ) =
a11
:
[ ]
: :
a12
:
[ ]
: :
a13
:
[ ]
: :
n i l
.
∗∗∗two rows
eq
t r a n s p o s e
( a11
:
[ ]
: :
a21
:
[ ]
: :
n i l ) =
a11
:
a21
:
[ ]
: :
n i l
.
eq
t r a n s p o s e
( a11
:
a12
:
[ ]
: :
a21
:
a22
:
[ ]
: :
n i l ) =
a11
:
a21
:
[ ]
: :
a12
:
a22
:
[ ]
: :
n i l
.
eq
t r a n s p o s e
( a11
:
a12
:
a13
:
[ ]
: :
a21
:
a22
:
a23
:
[ ]
: :
n i l ) =
a11
:
a21
:
[ ]
: :
a12
:
a22
:
[ ]
: :
a13
:
a23
:
[ ]
: :
n i l
.
∗∗∗
t h r e e
rows
eq
t r a n s p o s e
( a11
:
[ ]
: :
a21
:
[ ]
: :
a31
:
[ ]
: :
n i l ) =
a11
:
a21
:
a31
:
[ ]
: :
n i l
.
eq
t r a n s p o s e
( a11
:
a12
:
[ ]
: :
a21
:
a22
:
[ ]
: :
172

A.4. Artiﬁcial Intelligence Problems
a31
:
a32
:
[ ]
: :
n i l ) =
a11
:
a21
:
a31
:
[ ]
: :
a12
:
a22
:
a32
:
[ ]
: :
n i l
.
eq
t r a n s p o s e
( a11
:
a12
:
a13
:
[ ]
: :
a21
:
a22
:
a23
:
[ ]
: :
a31
:
a32
:
a33
:
[ ]
: :
n i l ) =
a11
:
a21
:
a31
:
[ ]
: :
a12
:
a22
:
a32
:
[ ]
: :
a13
:
a23
:
a33
:
[ ]
: :
n i l
.
eq weave
( n i l ) = [ ]
.
eq weave
( a11
:
[ ]
: :
n i l ) = a11
:
[ ]
.
eq weave
( a11
:
a12
:
[ ]
: :
n i l ) = a11
:
a12
:
[ ]
.
eq weave
( a11
:
a12
:
a13
:
[ ]
: :
n i l ) = a11
:
a12
:
a13
:
[ ]
.
eq weave
( a11
:
[ ]
: :
a21
:
[ ]
: :
n i l ) = a11
:
a21
:
[ ]
.
eq weave
( a11
:
a12
:
[ ]
: :
a21
:
[ ]
: :
n i l ) = a11
:
a21
:
a12
:
[ ]
.
eq weave
( a11
:
[ ]
: :
a21
:
a22
:
[ ]
: :
n i l ) = a11
:
a21
:
a22
:
[ ]
.
eq weave
( a11
:
a12
:
[ ]
: :
a21
:
a22
:
[ ]
: :
n i l ) = a11
:
a21
:
a12
:
a22
:
[ ]
.
eq weave
( a11
:
a12
:
a13
:
[ ]
: :
a21
:
a22
:
[ ]
: :
n i l ) = a11
:
a21
:
a12
:
a22
:
a13
:
[ ]
.
eq weave
( a11
:
[ ]
: :
a21
:
[ ]
: :
a31
:
[ ]
: :
n i l ) = a11
:
a21
:
a31
:
[ ]
.
eq weave
( a11
:
a12
:
[ ]
: :
a21
:
[ ]
: :
a31
:
[ ]
: :
n i l ) = a11
:
a21
:
a31
:
a12
:
[ ]
.
endfm
A.4. Artiﬁcial Intelligence Problems
∗∗∗REASONING ∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗
fmod ANCESTOR−BIN
i s
∗∗∗
a n c e s t o r
r e l a t i o n
i n
b i n a r y
t r e e s
∗∗∗
not
as
p r e d i c a t e
but
output
i s
the
( p o s s i b l y
empty )
path
s o r t s
Val
BinTree
Path
InVec
.
ops a b c d e
: −> Val
[ c t o r ]
.
op
n i l
: −> BinTree
[ c t o r ]
.
op node
:
Val
BinTree
BinTree −> BinTree
[ c t o r ]
.
op
n i l p
: −> Path
[ c t o r ]
.
173

A. Speciﬁcations of the Experiments
op
:
Val
Path −> Path
[ c t o r ]
.
op
Ancestor
:
Val
Val
BinTree −> Path
[ metadata ” induce ”]
.
op
i n
:
Val
Val
BinTree −> InVec
[ c t o r ]
.
∗∗∗
background
knowledge
∗∗∗
∗∗∗
I s I n
takes
a
v a l
and a
bin
t r e e
and
r e t u r n s
the
path
from
root
∗∗∗
to
v a l
i f
the
l a t t e r
appears
i n
the
bin
tree ,
empty path
o t h e r w i s e
op
I s I n
:
Val
BinTree −> Path
[ metadata
””]
.
op
i n
:
Val
BinTree −> InVec
[ c t o r ]
.
∗∗∗Or
takes
to
pathes ,
at
l e a s t
one
of
them empty ,
and
r e t u r n s
the
∗∗∗non−empty path
i f
i t
e x i s t s ,
the
empty path
o t h e r w i s e
op
Or
:
Path Path −> Path
[ metadata
””]
.
op
i n
:
Path Path −> InVec
[ c t o r ]
.
∗∗∗
s o l u t i o n :
∗∗∗
1.
Ancestor (X, Y,
n i l ) = n i l p
.
∗∗∗
2.
Ancestor (X, Y,
node (Z ,
L , R) ) = I s I n (Y,
node (Z ,
L , R) )
i f
X
== Z .
∗∗∗
3.
Ancestor (X, Y,
node (Z ,
L , R) ) =
∗∗∗
Ancestor (X, Y,
L) Or Ancestor (X, Y, R)
i f
X =/= Z .
v a rs X Y :
Val
.
∗∗∗
examples
∗∗∗seem
to
be more
examples
than
n e c e s s a r y
prima
f a c i e ,
but
they
are
∗∗∗needed
to
get
v a r i a b l e s
f o r
a l l
parameters
i n
a l l
cases
∗∗∗
∗∗∗
base
case
eq
Ancestor (X, Y,
n i l ) = n i l p
.
∗∗∗
case X == Z
eq
Ancestor (a ,
b ,
node (a ,
n i l ,
node ( c ,
node (b ,
n i l ,
n i l ) ,
node ( e ,
n i l ,
n i l ) ) ) ) =
a c b
n i l p
.
eq
I s I n (b ,
node (a ,
n i l ,
node ( c ,
node (b ,
n i l ,
n i l ) ,
node ( e ,
n i l ,
n i l
) ) ) ) =
a c b
n i l p
.
eq
Ancestor (a ,
b ,
node ( a ,
node (b ,
n i l ,
n i l ) ,
n i l ) ) = a b
n i l p
.
eq
I s I n (b ,
node (a ,
node (b ,
n i l ,
n i l ) ,
n i l ) ) = a b
n i l p
.
eq
Ancestor (b ,
a ,
node (b ,
node (d ,
node (a ,
n i l ,
n i l ) ,
n i l ) ,
n i l ) ) =
b d a
n i l p
.
eq
I s I n (a ,
node (b ,
node (d ,
node (a ,
n i l ,
n i l ) ,
n i l ) ,
n i l ) ) =
b d a
n i l p
.
174

A.4. Artiﬁcial Intelligence Problems
eq
Ancestor (b ,
a ,
node (b ,
n i l ,
node ( a ,
n i l ,
n i l ) ) ) = b a
n i l p
.
eq
I s I n (a ,
node (b ,
n i l ,
node ( a ,
n i l ,
n i l ) ) ) = b a
n i l p
.
∗∗∗
case X =/= Z
eq
Ancestor (a ,
b ,
node (d ,
node (a ,
n i l ,
node ( c ,
node (b ,
n i l ,
n i l ) ,
node ( e ,
n i l ,
n i l ) ) ) ,
n i l ) ) =
a c b
n i l p
.
eq
( a c b
n i l p ) Or
n i l p = a c b
n i l p
.
eq
Ancestor (a ,
b ,
node ( c ,
n i l ,
node ( a ,
node (b ,
n i l ,
n i l ) ,
n i l ) ) ) =
a b
n i l p
.
eq
n i l p
Or ( a b
n i l p ) = a b
n i l p
.
eq
Ancestor (b ,
a ,
node ( c ,
n i l ,
node (b ,
node (d ,
node ( a ,
n i l
,
n i l ) ,
n i l ) ,
n i l ) ) ) =
b d a
n i l p
.
eq
n i l p
Or ( b d a
n i l p ) = b d a
n i l p
.
eq
Ancestor (b ,
a ,
node ( c ,
node (b ,
n i l ,
node ( a ,
n i l ,
n i l ) ) ,
n i l ) ) =
b a
n i l p
.
eq
( b a
n i l p ) Or
n i l p = b a
n i l p
.
endfm
∗∗∗PROBLEM SOLVING ∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗
fmod ROCKET i s
s o r t s
Object
OList
State
InVec
.
op
n i l
: −> OList
[ c t o r ]
.
op
:
Object
OList −> OList
[ c t o r ]
.
ops
load
unload
:
Object
State −> State
[ c t o r ]
.
op move
:
State −> State
[ c t o r ]
.
∗∗∗
elements
i n
OList
s h a l l
be
loaded ,
moved to
the moon and
unloaded ;
∗∗∗
assumption :
rocket
at
earth
and
o b j e c t s
unloaded
i n
i n i t i a l
s t a t e
op Rocket
:
OList
State −> State
[ metadata ” induce ”]
.
175

A. Speciﬁcations of the Experiments
op
i n
:
OList
State −> InVec
[ c t o r ]
.
∗∗∗
s o l u t i o n
∗∗∗Rocket ( n i l ,
S) = move(S)
.
∗∗∗Rocket ((O Os) , S) = unload (O,
Rocket (Os ,
load (O,
S) ) )
v a rs O1 O2 :
Object
.
var S
:
State
.
eq
Rocket ( n i l ,
S) = move(S)
.
eq
Rocket ((O1
n i l ) , S) = unload (O1,
move( load (O1,
S) ) )
.
eq
Rocket ((O1 O2
n i l ) , S) =
unload (O1,
unload (O2,
move ( load (O2,
load (O1,
S) ) ) ) )
.
endfm
fmod CLEARBLOCK i s
s o r t s
Block Tower
State
InVec
.
op
t a b l e
: −> Tower
[ c t o r ]
.
op
:
Block Tower −> Tower
[ c t o r ]
.
op
putt
:
Block
State −> State
[ c t o r ]
.
∗∗∗put
to
t a b l e
op
ClearBlock
:
Block Tower
State −> State
[ metadata ” induce ”]
.
op
i n
:
Block Tower
State −> InVec
[ c t o r ]
.
∗∗∗
s o l u t i o n
∗∗∗
ClearBlock (A, B T,
S) = S
i f
A == B
∗∗∗
ClearBlock (A, B T,
S) = ClearBlock (A, T,
putt (B,
S) )
v a rs A B C :
Block
.
var S
:
State
.
∗∗∗
examples
∗∗∗
second
example
i n d i c a t e s
that
the
block
to
be
c l e a r e d
need
not
∗∗∗be
the
bottom
block
eq
ClearBlock (A, A table ,
S) = S
.
eq
ClearBlock (A, A B table ,
S) = S
.
eq
ClearBlock (A, B A table ,
S) = putt (B,
S)
.
eq
ClearBlock (A, C B A table ,
S) = putt (B,
putt (C,
S) )
.
endfm
fmod TOWER i s
s o r t s
Object Tower
State
InVec
.
∗∗∗
o b j e c t s
are
b l o c k s
and
the
t a b l e
∗∗∗
they
are
ordered
wrt
s
op
t a b l e
: −> Object
[ c t o r ]
.
op
s
:
Object −> Object
[ c t o r ]
.
176

A.4. Artiﬁcial Intelligence Problems
∗∗∗a tower
i s
a
l i s t
of
b l o c k s
∗∗∗
t a b l e
should
always
be
the
l a s t
o b j e c t
because
the
t a b l e
∗∗∗
i s
the
bottom
of
each
tower
op
|
: −> Tower
[ c t o r ]
.
op
:
Object Tower −> Tower
[ c t o r ]
.
∗∗∗a
s t a t e
i s
a
l i s t
of
towers
op
n i l
: −> State
[ c t o r ]
.
op
,
:
Tower
State −> State
[ c t o r ]
.
∗∗∗put
i s
used
to
stack
b l o c k s
on
b l o c k s
as
w e l l
as
to
put
∗∗∗a
block
to
the
t a b l e
op put
:
Object
Object
State −> State
[ c t o r ]
.
∗∗∗
the
goal
i s
to
c o n s t r u c t
a tower
ordered
wrt
s
with
the
given
∗∗∗
o b j e c t
as
the
top ,
∗∗∗
i . e . ,
(( s ˆ3
t a b l e )
( s ˆ2
t a b l e )
( s
t a b l e )
t a b l e
| )
∗∗∗
i f
s ˆ3
i s
given
op Tower
:
Object
State −> State
[ metadata ” induce ”]
.
op
i n
:
Object
State −> InVec
[ c t o r ]
.
∗∗∗
background
knowledge :
ClearBlock
op CB :
Object
State −> State
[ metadata
””]
.
∗∗∗
p r e d i c a t e :
i s
the
tower
f i n i s h e d ?
op IsTower
:
Object
State −> Bool
[ metadata ” pred
nomatch ”]
.
∗∗∗
s o l u t i o n
∗∗∗Tower (O,
S) = S
i f
IsTower (O,
S)
∗∗∗Tower (O,
S) =
∗∗∗
put (O,
Sub1 (O,
S) , CB(O, CB( Sub1 (O,
S) , Tower ( Sub1 (
O,
S) , S) ) ) )
∗∗∗
i f
not ( IsTower (O,
S) )
∗∗∗Sub1 ( s (O) , S) = O .
eq Tower ( table ,
n i l ) = n i l
.
eq Tower ( s
table ,
n i l ) = put ( s
table ,
table ,
n i l )
.
eq CB( table ,
n i l ) = n i l
.
eq CB( s
table ,
n i l ) = n i l
.
eq Tower ( s
table ,
(( s
t a b l e )
( s
s
t a b l e )
t a b l e
|
,
n i l ) ) =
put ( s
table ,
table ,
(( s
t a b l e )
( s
s
t a b l e )
t a b l e
|
,
n i l ) )
.
eq Tower ( table ,
(( s
t a b l e )
( s
s
t a b l e )
t a b l e
|
,
n i l ) ) =
(( s
t a b l e )
( s
s
t a b l e )
t a b l e
|
,
n i l )
.
eq Tower ( s
s
s
s
table ,
(( s
s
s
s
t a b l e )
( s
t a b l e )
t a b l e
|
,
( s
s
s
t a b l e )
( s
s
t a b l e )
t a b l e
|
,
n i l ) ) =
put ( s
s
s
s
table ,
s
s
s
table ,
177

A. Speciﬁcations of the Experiments
put ( s
s
s
table ,
s
s
table ,
put ( s
s
table ,
s
table ,
put ( s
s
s
table ,
table ,
put ( s
s
s
s
table ,
table ,
(( s
s
s
s
t a b l e )
( s
t a b l e )
t a b l e
|
,
( s
s
s
t a b l e )
( s
s
t a b l e )
t a b l e
|
,
n i l ) ) ) ) ) )
.
eq Tower ( s
s
s
table ,
(( s
s
s
s
t a b l e )
( s
t a b l e )
t a b l e
|
,
( s
s
s
t a b l e )
( s
s
t a b l e )
t a b l e
|
,
n i l ) ) =
put ( s
s
s
table ,
s
s
table ,
put ( s
s
table ,
s
table ,
put ( s
s
s
table ,
table ,
put ( s
s
s
s
table ,
table ,
(( s
s
s
s
t a b l e )
( s
t a b l e )
t a b l e
|
,
( s
s
s
t a b l e )
( s
s
t a b l e )
t a b l e
|
,
n i l ) ) ) ) )
.
eq Tower ( s
s
table ,
(( s
s
s
s
t a b l e )
( s
t a b l e )
t a b l e
|
,
( s
s
s
t a b l e )
( s
s
t a b l e )
t a b l e
|
,
n i l ) ) =
put ( s
s
table ,
s
table ,
put ( s
s
s
table ,
table ,
put ( s
s
s
s
table ,
table ,
(( s
s
s
s
t a b l e )
( s
t a b l e )
t a b l e
|
,
( s
s
s
t a b l e )
( s
s
t a b l e )
t a b l e
|
,
n i l ) ) ) )
.
eq Tower ( s
table ,
(( s
s
s
s
t a b l e )
( s
t a b l e )
t a b l e
|
,
( s
s
s
t a b l e )
( s
s
t a b l e )
t a b l e
|
,
n i l ) ) =
(( s
s
s
s
t a b l e )
( s
t a b l e )
t a b l e
|
,
( s
s
s
t a b l e )
( s
s
t a b l e )
t a b l e
|
,
n i l )
.
eq Tower ( table ,
(( s
s
s
s
t a b l e )
( s
t a b l e )
t a b l e
|
,
( s
s
s
t a b l e )
( s
s
t a b l e )
t a b l e
|
,
n i l ) ) =
(( s
s
s
s
t a b l e )
( s
t a b l e )
t a b l e
|
,
( s
s
s
t a b l e )
( s
s
t a b l e )
t a b l e
|
,
n i l )
.
eq CB( table ,
(( s
t a b l e )
( s
s
t a b l e )
t a b l e
|
,
n i l ) ) =
(( s
t a b l e )
( s
s
t a b l e )
t a b l e
|
,
n i l )
.
eq CB( s
table ,
(( s
t a b l e )
( s
s
t a b l e )
t a b l e
|
,
n i l ) ) =
(( s
t a b l e )
( s
s
t a b l e )
t a b l e
|
,
n i l )
.
178

A.4. Artiﬁcial Intelligence Problems
eq CB( s
s
s
s
table ,
put ( s
s
s
table ,
s
s
table ,
put ( s
s
table ,
s
table ,
put ( s
s
s
table ,
table ,
put ( s
s
s
s
table ,
table ,
(( s
s
s
s
t a b l e )
( s
t a b l e )
t a b l e
|
,
( s
s
s
t a b l e )
( s
s
t a b l e )
t a b l e
|
,
n i l ) ) ) ) ) )
=
put ( s
s
s
table ,
s
s
table ,
put ( s
s
table ,
s
table ,
put ( s
s
s
table ,
table ,
put ( s
s
s
s
table ,
table ,
(( s
s
s
s
t a b l e )
( s
t a b l e )
t a b l e
|
,
( s
s
s
t a b l e )
( s
s
t a b l e )
t a b l e
|
,
n i l ) ) ) ) )
.
eq CB( s
s
s
table ,
put ( s
s
s
table ,
s
s
table ,
put ( s
s
table ,
s
table ,
put ( s
s
s
table ,
table ,
put ( s
s
s
s
table ,
table ,
(( s
s
s
s
t a b l e )
( s
t a b l e )
t a b l e
|
,
( s
s
s
t a b l e )
( s
s
t a b l e )
t a b l e
|
,
n i l ) ) ) ) ) )
=
put ( s
s
s
table ,
s
s
table ,
put ( s
s
table ,
s
table ,
put ( s
s
s
table ,
table ,
put ( s
s
s
s
table ,
table ,
(( s
s
s
s
t a b l e )
( s
t a b l e )
t a b l e
|
,
( s
s
s
t a b l e )
( s
s
t a b l e )
t a b l e
|
,
n i l ) ) ) ) )
.
eq CB( s
s
s
table ,
put ( s
s
table ,
s
table ,
put ( s
s
s
table ,
table ,
put ( s
s
s
s
table ,
table ,
(( s
s
s
s
t a b l e )
( s
t a b l e )
t a b l e
|
,
( s
s
s
t a b l e )
( s
s
t a b l e )
t a b l e
|
,
n i l ) ) ) ) ) =
put ( s
s
table ,
s
table ,
put ( s
s
s
table ,
table ,
179

A. Speciﬁcations of the Experiments
put ( s
s
s
s
table ,
table ,
(( s
s
s
s
t a b l e )
( s
t a b l e )
t a b l e
|
,
( s
s
s
t a b l e )
( s
s
t a b l e )
t a b l e
|
,
n i l ) ) ) )
.
eq CB( s
s
table ,
put ( s
s
table ,
s
table ,
put ( s
s
s
table ,
table ,
put ( s
s
s
s
table ,
table ,
(( s
s
s
s
t a b l e )
( s
t a b l e )
t a b l e
|
,
( s
s
s
t a b l e )
( s
s
t a b l e )
t a b l e
|
,
n i l ) ) ) ) ) =
put ( s
s
table ,
s
table ,
put ( s
s
s
table ,
table ,
put ( s
s
s
s
table ,
table ,
(( s
s
s
s
t a b l e )
( s
t a b l e )
t a b l e
|
,
( s
s
s
t a b l e )
( s
s
t a b l e )
t a b l e
|
,
n i l ) ) ) )
.
eq CB( s
s
table ,
put ( s
s
s
s
table ,
table ,
(( s
s
s
s
t a b l e )
( s
t a b l e )
t a b l e
|
,
( s
s
s
t a b l e )
( s
s
t a b l e )
t a b l e
|
,
n i l ) ) )
=
put ( s
s
s
table ,
table ,
put ( s
s
s
s
table ,
table ,
(( s
s
s
s
t a b l e )
( s
t a b l e )
t a b l e
|
,
( s
s
s
t a b l e )
( s
s
t a b l e )
t a b l e
|
,
n i l ) ) )
.
eq CB( s
table ,
(( s
s
s
s
t a b l e )
( s
t a b l e )
t a b l e
|
,
( s
s
s
t a b l e )
( s
s
t a b l e )
t a b l e
|
,
n i l ) ) =
put ( s
s
s
s
table ,
table ,
(( s
s
s
s
t a b l e )
( s
t a b l e )
t a b l e
|
,
( s
s
s
t a b l e )
( s
s
t a b l e )
t a b l e
|
,
n i l ) )
.
∗∗∗
p r e d i c a t e
implementation
op $IsTower
:
Object Tower −> Bool
.
v a rs O O2 :
Object
.
var T :
Tower
.
var S
:
State
.
eq
IsTower ( table ,
S) = t r u e
.
eq
IsTower ( s O,
(( s O) T , S) ) = $IsTower (O, T)
.
eq
IsTower ( s O,
( t a b l e
|
, S) ) = IsTower ( s O,
S)
.
eq
IsTower ( s O,
n i l ) = f a l s e
.
180

A.4. Artiﬁcial Intelligence Problems
eq
IsTower ( s O,
(( s O2) T , S) ) = IsTower ( s O,
(T , S) )
[ owise ]
.
eq
$IsTower ( table ,
t a b l e
| ) = t r u e
.
eq
$IsTower ( s O,
( s O) T) = $IsTower (O, T)
.
eq
$IsTower (O, T) = f a l s e
[ owise ]
.
endfm
fmod HANOI i s
s o r t s
Disc Peg
State
InVec
.
op 0
: −> Disc
[ c t o r ]
.
∗∗∗
s m a l l e s t
d i s c
op
s
:
Disc −> Disc
[ c t o r ]
.
∗∗∗
next
b i g g e r
d i s c
∗∗∗move
d i s c
from one peg
to
another
i n
a
c u r r e n t
s t a t e
op move
:
Disc Peg Peg
State −> State
[ c t o r ]
.
∗∗∗move tower up to
s p e c i f i e d
d i s c
from
s t a r t
peg
v i a
aux peg
to
∗∗∗
goal
peg
i n
a
given
s t a t e
op Hanoi
:
Disc Peg Peg Peg
State −> State
[ metadata ” induce ”]
.
op
i n
:
Disc Peg Peg Peg
State −> InVec
[ c t o r ]
.
∗∗∗
s o l u t i o n
∗∗∗Hanoi (0 ,
Src ,
Aux ,
Dst ,
S) = move (0 ,
Src ,
Dst ,
S)
∗∗∗Hanoi ( s D,
Src ,
Aux ,
Dst ,
S) =
∗∗∗
Hanoi (D,
Aux ,
Src ,
Dst ,
∗∗∗
move( s D,
Src ,
Dst ,
∗∗∗
Hanoi (D,
Src ,
Dst ,
Aux ,
S) ) )
var S
:
State
.
v a rs
Src Aux Dst
:
Peg
.
eq Hanoi (0 ,
Src ,
Aux ,
Dst ,
S) = move (0 ,
Src ,
Dst ,
S)
.
eq Hanoi ( s
0 ,
Src ,
Aux ,
Dst ,
S) =
move (0 , Aux ,
Dst ,
move ( s
0 ,
Src ,
Dst ,
move (0 ,
Src ,
Aux ,
S) )
)
.
eq Hanoi ( s
s
0 ,
Src ,
Aux ,
Dst ,
S) =
move (0 ,
Src ,
Dst ,
move( s
0 , Aux ,
Dst ,
move (0 , Aux ,
Src ,
move( s
s
0 ,
Src ,
Dst ,
move (0 ,
Dst ,
Aux ,
move ( s
0 ,
Src ,
Aux ,
move (0 ,
Src
,
Dst ,
S) ) ) ) ) )
)
.
endfm
181

A. Speciﬁcations of the Experiments
∗∗∗NATURAL LANGUAGE PROCESSING ∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗
fmod GENERATOR i s
∗∗∗
g e n e r a t e s
se nte n c e s
up to
p a r t i c u l a r
depth
s o r t s
Cat
CList
Depth
InVec
.
ops d n v
: −> Cat
[ c t o r ]
.
op
!
: −> CList
[ c t o r ]
.
op
:
Cat
CList −> CList
[ c t o r ]
.
op 1
: −> Depth
[ c t o r ]
.
op
s
:
Depth −> Depth
[ c t o r ]
.
op
Sentence
:
Depth −> CList
[ metadata ” induce ”]
.
op
i n
:
Depth −> InVec
[ c t o r ]
.
∗∗∗
o r i g i n a l
grammar
∗∗∗S −> NP VP
∗∗∗NP −> D N
∗∗∗VP −> V NP | V S
∗∗∗
s o l u t i o n
∗∗∗
Sentence (1) = ( d n v d n
! )
∗∗∗
Sentence ( s N) = ( d n v
Sentence (N) )
eq
Sentence (1) = ( d n v d n
! )
.
eq
Sentence ( s
1) = ( d n v d n v d n
! )
.
eq
Sentence ( s
s
1) = ( d n v d n v d n v d n
! )
.
endfm
182

Nomenclature
(xi)i∈I, (xi) Family of elements xi with index set I
[m]
{n ∈N | 1 ≤n ≤m}
[x]∼
Equivalence class of x by ∼
χB(r, Φ) Successor-rule sets and speciﬁcations; union of χsplit, χsub, χsmplCall, χcall
χcall
Function call operator
χsmplCall Simple call operator
χsplit
Rule splitting operator
χsub
Subproblem operator
N, Z
Natural numbers and integers, respectively
DR, CR Deﬁned function symbols and constructors of a CS R, respectively
P(X) Powerset of X
Deﬁnes(r) Deﬁned function symbol at the root of the LHS of a constructor system rule
id
Identity function
Lhss(R) The set of all LHSs of a CS R
Lhs(r), Rhs(r) LHS and RHS of a rule r, respectively
Node(t, p) Symbol of term t at position p
Pos(t) Set of positions of the term t
Var(t) Variables of term t
Φ(r), Φ(r) Speciﬁcation subsets associated with a rule and a function, respectively
∼, ≡
Equivalence relation
∗→R
Rewrite relation of a TRS R
ΞB(⟨P, Φ⟩) Set of successor CSs and corresponding speciﬁcations of candidate CS P and
speciﬁcation Φ with respect to background CS B
183

A. Speciﬁcations of the Experiments
| X |
Cardinality of the set X
e ⪰e′ Expression e subsumes expression e′ (where “expression” refers to terms, predi-
cates, equations, or rules), i.e., there is a substitution σ such that eσ = e′
t|p
Subterm of term t at position p
X/ ∼Quotient set of X by ∼; the set of all equivalence classes of X by ∼
TΣ
Ground Σ-Terms
PΦ,B
Igor2 problem space with respect to initial speciﬁcation Φ and background CS
B
TΣ(X) Σ-Terms over variables X
184

Index
θ-subsumption, 56
incomplete, 57, 63
Adate, 66, 148
Algebra, 8, 9
initial, 13
Quotient algebra, 13
Quotient term algebra, 14
Term algebra, 13
Anti-uniﬁcation, 15
Background knowledge, 29
Analytical IPS, 48
Igor2, see Igor2
ILP, 49, 62
Bias, see Inductive bias
BMWk, 38
Candidate CS, 78
closed, 75
initial, 79, 97, 98
open, 75
Candidate program, 28
Clause, 21
Deﬁnite clause, 22
Horn clause, 22
Complete
extensionally correct CS, 94
for a program class, 31
Igor2, 125
Igor2’s search, 114, 121
Sets of I/O examples, 48, 93
TRS, 17
Conﬂuent, 17, 19
Consequence, 11, 23
Constructor, 18
Constructor system, 18
Constructor term, 19
Context, 14
Contractum, 16
Correct
CS in Igor2, 77, 123
extensionally, 74, 92, 94, 122
wrt incomplete speciﬁcation, 27
Coverage, 51
extensional, 52, 62, 74
intensional, 52
Covering algorithm, 53
Deﬁned function, 18
Deﬁnite program, 25
Diﬀerence, 35
Entailment, 23, 51, 58
relative, 56
Enumeration algorithm, 30, 41
Equation, 11
Extensional correctness, see Correct
Family, 7
Foil, 59, 148
Formula, 21
Fragment, see Program fragment
Function call operators, 83, 102
Function call operator, 107
simple, 104
G∀ST, 68
Generality, 52, 55
Generalized sequence, 40
Genetic programming, 64
185

Index
Golem, 59, 148
Herbrand base, 21
Herbrand interpretation, 20
Herbrand model, 22
least, 25
Herbrand universe, 9, 10, 20
Hypothesis, 28
ILP, 49
I/O examples, 27, 76
I/O patterns, 76
Igor1, 43, 148
Igor2, 73, 148
Background knowledge, 76
Complexity, 127
Conditional rules, 129
Extensions, 128
Induction problem, 76, 77
Inductive bias, 77
Problem space, 112, 113
Speciﬁcation, 76
Successor CSs, 112
Successor-rule sets, 111
Incomplete speciﬁcation, see Speciﬁca-
tion
Induction problem, see Igor2
Inductive bias, 28, 77
of Igor2, 77
Inductive logic programming, 49
Bottom-up, 53
Top-down, 53, 61
Inductive program synthesis, 27
Analytical, 31, 73, 151
Generate-and-test, 64, 151
Initial node, 113
Initial rule operator, 79, 81, 97
Instance, 15, 16
Inverting implication, 60
Least general generalization, 15, 16, 81
θ-subsumption, 56
BMWk, 39, 40
Entailment, 58
Literal, 21
Machine learning, 49
MagicHaskeller, 68, 148
Match, 15
Matching sequence, 39
Minimal left-linear generalization, 81, 97
Model, 11, 22
Most general uniﬁer, 23
Normal form, 16
Optimal Ordered Problem Solver, 69
Orthogonal, 19
Candidate CS, 114
Speciﬁcation, 76, 77
Pivot position, 82, 99
Position, 14
Predicate invention, 30, 60
Presentation, 11
Primitives, 29
Analytical IPS, 33
Problem space, see Igor2
Progol, 59
Program fragment, 33, 34, 39
Program schema
Analytical IPS, 37, 40
BMWk, 40
Crustacean, 60
Regular program, 41
Summers, 37
Rapid rule-splitting, 130
Reachable nodes, 113
Recurrence detection, 32, 34, 48, 151
Igor1, 46
Igor2, 73
Recurrence relation, 36
Recursive program scheme, 43
Redex, 16
Reduct, 16
Reduction, 16
Reduction order, 17
Reﬁnement operator
186

Index
θ-subsumption, 57
Entailment, 58
Igor2, 82, 95, 111
ILP, 53, 54
Refutation, 24, 25
Resolution, 23, 24, 49
SLD-resolution, 25
Resolvent, 24
Rewrite relation, 16
Rewrite rule, see Rule
Rule, 15
depth, 116
initial, 79, 97
Rule-splitting operator, see Splitting op-
erator
Search, 28
Igor2, 73
ILP, 52
Semantics
initial, 13, 18
loose, 11
Signature, 8, 20
Sort, 8
Sound
Igor2, 122, 124
IPS system, 31
Speciﬁcation
Complete up to certain complexity,
48
Generate-and-test, 69
Igor2, see Igor2
Incomplete, 27
initial, 112
Subset, 96
Splitting operator, 82, 99
Standardized apart, 24
Structure, 20
Subfunction invention, 29
in Igor1, 44
in BMWk, 39
in Igor2, 83, 84, 127
Subproblem operator, 83, 101
Substitution, 14, 16, 23
Subsumption, 15, 16
Subterm, 14
Synthesis operator, see Reﬁnement op-
erator
Synthesis theorem, 37
Target function, 27
Term, 9
constructor term, 19
Evaluation, 10
ground, 10
linear, 14
Term rewriting system, 15
Terminating
Candidate CS, 103
Igor2, 114, 119
TRS, 17
Theory, 12
Uniﬁcation, 15, 23
Variable renaming, 15
187

