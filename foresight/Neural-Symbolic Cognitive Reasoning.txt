
Cognitive Technologies
Managing Editors: D. M. Gabbay J. Siekmann
Editorial Board: A. Bundy J. G. Carbonell
M. Pinkal H. Uszkoreit M. Veloso W. Wahlster
M. J. Wooldridge
Advisory Board:
Luigia Carlucci Aiello
Franz Baader
Wolfgang Bibel
Leonard Bolc
Craig Boutilier
Ron Brachman
Bruce G. Buchanan
Anthony Cohn
Artur d’Avila Garcez
Luis Fariñas del Cerro
Koichi Furukawa
Georg Gottlob
Patrick J. Hayes
James A. Hendler
Anthony Jameson
Nick Jennings
Aravind K. Joshi
Hans Kamp
Martin Kay
Hiroaki Kitano
Robert Kowalski
Sarit Kraus
Maurizio Lenzerini
Hector Levesque
John Lloyd
Alan Mackworth
Mark Maybury
Tom Mitchell
Johanna D. Moore
Stephen H. Muggleton
Bernhard Nebel
Sharon Oviatt
Luis Pereira
Lu Ruqian
Stuart Russell
Erik Sandewall
Luc Steels
Oliviero Stock
Peter Stone
Gerhard Strube
Katia Sycara
Milind Tambe
Hidehiko Tanaka
Sebastian Thrun
Junichi Tsujii
Kurt VanLehn
Andrei Voronkov
Toby Walsh
Bonnie Webber

·
With
Figures and
Tables
Artur S. d Avila Garcez Lu s C. Lamb
Dov M. Gabbay
Neural-Symbolic 
Cognitive Reasoning
53 
6 
,
´ı
ABC

ISBN: 978-3-540-73245-7
e-ISBN: 978-3-540-73246-4
Cognitive Technologies ISSN: 1611-2482
Library of Congress Control Number: 2008935635
ACM Computing Classiﬁcation (1998): F.1.1, F.4.1, I.2.3, I.2.4, I.2.6
c⃝200 Springer-Verlag Berlin Heidelberg
This work is subject to copyright. All rights are reserved, whether the whole or part of the material is
concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting,
reproduction on microﬁlm or in any other way, and storage in data banks. Duplication of this publication
or parts thereof is permitted only under the provisions of the German Copyright Law of September 9,
1965, in its current version, and permission for use must always be obtained from Springer. Violations are
liable to prosecution under the German Copyright Law.
The use of general descriptive names, registered names, trademarks, etc. in this publication does not imply,
even in the absence of a speciﬁc statement, that such names are exempt from the relevant protective laws
and regulations and therefore free for general use.
Cover design: KünkelLopka, Heidelberg
Printed on acid-free paper
9 8 7 6 5 4 3 2 1
springer.com
Authors:
Managing Editors:
Dr. Artur S. d Avila Garcez
Reader in Computing
Department of Computing
School of Informatics
City University London
London EC1V 0HB, UK
aag@soi.city.ac.uk
Dr. Lu s C. Lamb
Associate Professor 
Institute of Informatics
Federal University of
Porto Alegre, RS, 91501-970, Brazil
lamb@inf.ufrgs.br, luislamb@acm.org
Augustus De Morgan Professor of Logic
Department of Computer Science
King s College London
’
Strand, London WC2R 2LS, UK
Prof. Dr. Jörg Siekmann
Forschungsbereich Deduktions- und
Multiagentensysteme, DFKI
Stuhlsatzenweg 3, Geb. 43
66123 Saarbrücken, Germany
Prof. Dov M . Gabbay
Department of Computer Science
Prof. Dov M. Gabbay
Augustus De Morgan Professor of Logic
King s College London
Strand, London WC2R 2LS, UK
dov.gabbay@kcl.ac.uk
9 
´ı
,
,
DOI: 10.1007/978-3-540-73246-4
Rio Grande do Sul

To Simone, for giving me our little Max (AAG).
To Desir´ee (LCL).
To Lydia (DMG).

Preface
This book is about neural computation and applied reasoning, and about how they
are related and can be integrated within a machine-learning framework. Logic has
a long and solid tradition of principled research, and has been inﬂuential in virtu-
ally all ﬁelds of scientiﬁcally-based human knowledge. Neural computation has now
achieved maturity. Practitioners have been able to use artiﬁcial connectionist models
in a number of real-world problems, including machine learning, pattern recogni-
tion, vision, cognitive modelling, and artiﬁcial agents. However, the area still lacks
sound computational foundations, as several features of neural-computation models
are still under investigation.
The book presents a number of principled, sound neural-computation models. It
makes use of a number of applied (nonclassical) logical systems in order to do so.
Logics lie at the foundation of computer science and artiﬁcial intelligence. For in-
stance, logic research has provided the foundations of the theories of computability,
recursion, and complexity. In artiﬁcial intelligence, the construction of automated
reasoning systems, knowledge representation languages and formalisms, and the-
orem proving have been pursued since at least the 1950s. All these are based on
logics.
More recently, applications in planning, multiagent systems, expert systems, the
Semantic Web, learning theory, cognitive modelling and robotics, constraint satis-
faction, searching, and the traditional ﬁelds of knowledge representation and rea-
soning have beneﬁted from research on nonclassical logics. In particular, research
on temporal, modal, description, intuitionistic, fuzzy, and nonmonotonic logics has
been inﬂuential in artiﬁcial intelligence, both in relation to theory and principled
languages for building models and systems.
Recently, nonclassical logics1 have been found relevant in relation to the for-
malisation of integrated learning and reasoning [66, 72, 77, 148, 222]. They have
contributed to the construction of sound models of cognitive reasoning. As is well
known, human cognition successfully integrates the connectionist (brain-inspired)
1 Nonclassical logics depart from the Aristotelian tradition in the sense that these logics lack some
properties of classical logic, such as the law of the excluded middle, or extend them with addi-
tional features. For instance, the development of nonmonotonic logics, which has contributed to
vii

viii
Preface
and symbolic (mind-inspired) paradigms of artiﬁcial intelligence, language being a
compelling case in point. Yet the modelling of cognition develops these separately in
the ﬁelds of neural computation (and statistical artiﬁcial intelligence) and symbolic
logic/artiﬁcial intelligence. There is now a movement towards a fruitful middle way
in between these extremes, in which the study of logic is combined with connection-
ism. It is essential that these be integrated, thereby enabling not only a technology
for building better intelligent systems, but also the understanding of fully ﬂedged
computational cognitive models.
The aim of neural-symbolic computation is to explore the advantages that each
paradigm presents. Among the advantages of artiﬁcial neural networks are massive
parallelism, fault tolerance (robust learning), efﬁcient inductive learning, and effec-
tive generalisation capabilities. On the other hand, symbolic systems provide de-
scriptions (as opposed to only discriminations); can explain their inference process,
for example through automatic theorem proving; and use powerful declarative lan-
guages for knowledge representation and reasoning.
In this book, we explore the synergies of neural-symbolic integration from the
following perspective. We use a neural network to simulate a given task. The net-
work is obtained by being programmed (set up) or by adapting to and generalising
over well-known situations (learning). The network is the mechanism to execute
the task, while symbolic logic enables the necessary interaction between the net-
work and the outside world. Differently from and complementing [66], which set
the scence for neural-symbolic integration with the use of standard networks and
logic programming, we are concerned here with applied (and nonclassical) logical
systems. With this consideration at the forefront, the book presents a rich cognitive
model for reasoning and learning based on neural-network ensembles. The book
also illustrates the effectiveness of the model by experimentation, and shows that
the connectionist model can compute a number of combined nonclassical logical
systems [42], including modal, temporal, epistemic, and intuitionist logics. Finally,
the book investigates the issue of relational learning and the representation of ﬁrst-
order logic, the combination (ﬁbring) of models/networks, qualitative reasoning un-
der uncertainty, and how neural networks can offer an effective approach to learning
in argumentation frameworks [39,212]. We conclude by summarising our case for a
logic-based, cognitive connectionist model which we call ﬁbred network ensembles.
Overall, it offers a rich model from a symbolic computation/reasoning viewpoint,
yet it is relatively simple and efﬁcient as a learning model. We believe it strikes
the right balance between expressiveness and computational feasibility. And, as a
matter of principle, we believe that this is no coincidence, but a direct result of
the formalisation of practical or commonsense reasoning, led to the discovery of several new log-
ical systems in which the principle of monotonicity may not hold. Further, several authors have
defended a position that intuitionistic logics are more appropriate as a foundational basis of com-
putation than classical logic [258]. Classical logic does not capture all of the nature of human
practical reasoning. For instance, in order to formalise and automate computational (human) rea-
soning about time, knowledge, beliefs, or uncertainty several nonclassical logics have been found
more appropriate than classical logic; see [42,87,100,102]. Our point will be made clearer as the
reader proceeds to the coming chapters.

Preface
ix
our methodology, which seeks to unify systematically the most advanced concepts
of symbolic computation with the physical constraints of a realistic connectionist
machine.
Our aim here is to offer a principled way in which robust learning and effec-
tive reasoning can be realised. In line with one of the recent grand challenges
in computer science, according to the British Computer Society – nonclassical
computation – we seek to unify the classical and nonclassical (natural) computa-
tional paradigms, with the nonclassical computational models presented here show-
ing how alternative formulations can deal with cognitive dimensions usually studied
under separate formalisms.
London, UK, and Porto Alegre, Brazil, February 2005 to June 2008.
Acknowledgements Over the last ﬁve years, we have had stimulating (brief and long) conversa-
tions about this book with several researchers in personal meetings, conferences, and workshops.
We would like to acknowledge the feedback from Samson Abramsky, Sebastian Bader, Howard
Barringer, Rodrigo de Salvo Braz, Krysia Broda, Jehoshua (Shuki) Bruck, Matthew Cook,
Cicero Garcez, Robert Givan, Joe Halpern, Pascal Hitzler, Wilfrid Hodges, Ian Hodkinson, Stef-
fen H¨olldobler, Bob Kowalski, David Makinson, Ryszard Michalski, Sanjay Modgil, Stephen
Muggleton, Vasile Palade, G¨unther Palm, Luc de Raedt, Oliver Ray, Odinaldo Rodrigues, Dan
Roth, Stefan R¨uger, Alessandra Russo, Jude Shavlik, J¨org Siekmann, Ron Sun, Guglielmo
Tamburrini, John G. Taylor, Francesca Toni, Johan van Benthem, Moshe Vardi, John Woods, and
Gerson Zaverucha. We thank them all for their challenging arguments and questioning on the
subject, and we apologise for any omissions. We also would like to thank the participants of the
International Workshops on Neural-Symbolic Learning and Reasoning (NeSy), organised yearly
since 2005 either at IJCAI or ECAI, for the enjoyable research atmosphere and useful debates
during our workshop meetings. We particularly would like to thank the NeSy keynote speakers
Luc de Raedt, Marco Gori, Steffen H¨olldobler, Lokendra Shastri, Ron Sun, Stefan Wermter, and
Kai-Uwe Kuehnberger.
We also acknowledge our publisher, Ronan Nugent of Springer, for his support throughout the
writing of the book.
Artur d’Avila Garcez has been partly supported by grants from the Nufﬁeld Foundation and the
Royal Society; Lu´ıs Lamb has been partly supported by grants from the Brazilian Research Council
(CNPq) and the CAPES foundation; and Dov Gabbay has been partly supported by EPSRC and by
the Leverhulme Trust. This manuscript was written using LATEX.

Contents
1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.1
Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.2
Methodology and Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
1.3
Structure of the Book . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
2
Logic and Knowledge Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
2.1
Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
2.2
Classical Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
2.2.1
Propositional Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
2.2.2
First-Order Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
2.3
Nonclassical Logics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
2.4
Nonmonotonic Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
2.5
Logic Programming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
2.5.1
Stable-Model and Answer Set Semantics . . . . . . . . . . . . . . . . 19
2.6
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
3
Artiﬁcial Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
3.1
Architectures of Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
3.2
Learning Strategy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
3.3
Recurrent Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
3.4
Evaluation of Learning Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
3.5
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
4
Neural-Symbolic Learning Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
4.1
The CILP System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
4.2
Massively Parallel Deduction in CILP . . . . . . . . . . . . . . . . . . . . . . . . . 43
4.3
Inductive Learning in CILP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
4.4
Adding Classical Negation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
4.5
Adding Metalevel Priorities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
4.6
Applications of CILP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
4.7
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
xi

xii
Contents
5
Connectionist Modal Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
5.1
Modal Logic and Extended Modal Programs . . . . . . . . . . . . . . . . . . . . 56
5.1.1
Semantics for Extended Modal Logic Programs . . . . . . . . . . . 58
5.2
Connectionist Modal Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
5.2.1
Computing Modalities in Neural Networks . . . . . . . . . . . . . . . 61
5.2.2
Soundness of Modal Computation . . . . . . . . . . . . . . . . . . . . . . 66
5.2.3
Termination of Modal Computation . . . . . . . . . . . . . . . . . . . . . 67
5.3
Case Study: The Muddy Children Puzzle . . . . . . . . . . . . . . . . . . . . . . . 68
5.3.1
Distributed Knowledge Representation in CML . . . . . . . . . . . 69
5.3.2
Learning in CML . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
5.4
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
6
Connectionist Temporal Reasoning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
6.1
Connectionist Temporal Logic of Knowledge . . . . . . . . . . . . . . . . . . . 76
6.1.1
The Language of CTLK. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77
6.1.2
The CTLK Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79
6.2
The Muddy Children Puzzle (Full Solution) . . . . . . . . . . . . . . . . . . . . 81
6.2.1
Temporal Knowledge Representation . . . . . . . . . . . . . . . . . . . 81
6.2.2
Learning in CTLK . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83
6.3
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84
7
Connectionist Intuitionistic Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87
7.1
Intuitionistic Logic and Programs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
7.2
Connectionist Intuitionistic Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . 90
7.2.1
Creating the Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92
7.2.2
Connecting the Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
7.3
Connectionist Intuitionistic Modal Reasoning . . . . . . . . . . . . . . . . . . . 95
7.4
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
8
Applications of Connectionist Nonclassical Reasoning . . . . . . . . . . . . . . 101
8.1
A Simple Card Game . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
8.2
The Wise Men Puzzle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102
8.2.1
A Formalisation of the Wise Men Puzzle . . . . . . . . . . . . . . . . 103
8.2.2
Representing the Wise Men Puzzle Using CML . . . . . . . . . . . 104
8.3
Applications of Connectionist Intuitionism . . . . . . . . . . . . . . . . . . . . . 108
8.3.1
Representing the Wise Men Puzzle Using CIL . . . . . . . . . . . . 109
8.4
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112
9
Fibring Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
9.1
The Idea of Fibring . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
9.2
Fibring Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
9.3
Examples of the Fibring of Networks . . . . . . . . . . . . . . . . . . . . . . . . . . 119
9.4
Deﬁnition of Fibred Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121
9.5
Dynamics of Fibred Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
9.6
Expressiveness of Fibred Networks. . . . . . . . . . . . . . . . . . . . . . . . . . . . 124
9.7
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125

Contents
xiii
10
Relational Learning in Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . 127
10.1 An Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128
10.2 Variable Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130
10.3 Relation Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
10.4 Relational Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132
10.5 Relational Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134
10.6 Experimental Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135
10.7 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140
11
Argumentation Frameworks as Neural Networks . . . . . . . . . . . . . . . . . . 143
11.1 Value-Based Argumentation Frameworks . . . . . . . . . . . . . . . . . . . . . . 144
11.2 Argumentation Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145
11.3 Argument Computation and Learning . . . . . . . . . . . . . . . . . . . . . . . . . . 149
11.3.1 Circular Argumentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150
11.3.2 Argument Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154
11.3.3 Cumulative (Accrual) Argumentation . . . . . . . . . . . . . . . . . . . 155
11.4 Fibring Applied to Argumentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157
11.5 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159
12
Reasoning about Probabilities in Neural Networks . . . . . . . . . . . . . . . . . 161
12.1 Representing Uncertainty . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161
12.2 An Algorithm for Reasoning about Uncertainty . . . . . . . . . . . . . . . . . 164
12.3 The Monty Hall Puzzle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166
12.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
13
Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169
13.1 Neural-Symbolic Learning Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . 170
13.2 Connectionist Nonclassical Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . 172
13.2.1 Connectionist Modal Reasoning . . . . . . . . . . . . . . . . . . . . . . . . 173
13.2.2 Connectionist Temporal Reasoning . . . . . . . . . . . . . . . . . . . . . 175
13.3 Fibring Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176
13.4 Concluding Remarks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193

Chapter 1
Introduction
1.1 Motivation
The construction of robust computational models integrating reasoning and learn-
ing is a key research challenge for artiﬁcial intelligence. Recently, this challenge was
also put forward as a fundamental problem in computer science [255]. Such a chal-
lenge intersects with another long-standing entry in the research agenda of artiﬁcial
intelligence: the integration of its symbolic and connectionist paradigms. Such in-
tegration has long been a standing enterprise, with implications for and applications
in cognitive science and neuroscience [51,66,130,178,179,238,240,247,248,250].
Further, the importance of efforts to bridge the gap between the connectionist and
symbolic paradigms of artiﬁcial intelligence has also been widely recognised (see
e.g. [51,66,229,242,243]).
Valiant [255] has pointed out that the construction of rich computational cog-
nitive models is one of the challenges computer science faces over the next few
decades. A positive answer to this challenge would provide a characterisation of a
semantics for cognitive computation,1 as follows:
The aim here is to identify a way of looking at and manipulating commonsense knowledge
that is consistent with and can support what we consider to be the two most fundamental as-
pects of intelligent cognitive behaviour: the ability to learn from experience, and the ability
to reason from what has been learned. We are therefore seeking a semantics of knowledge
that can computationally support the basic phenomena of intelligent behaviour [255].
Valiant also described the characteristics of the semantic formalisation needed for
supporting learning and reasoning:
One set of requirements [for a semantics to be adequate for commonsense reasoning] is that
it should support integrated algorithms for learning and reasoning that are computationally
1 The article [255] was published in 2003 in the Journal of the ACM, in a special issue celebrating
its 50th anniversary. In that issue, the editor-in-chief at the time (J.Y. Halpern) invited winners
of the Turing Award and Nevanlinna Prize to discuss up to three problems that these prominent
researchers thought would be major challenges for computer science in the next 50 years.
A.S. d’Avila Garcez et al., Neural-Symbolic Cognitive Reasoning, Cognitive Technologies,
1
c⃝Springer-Verlag Berlin Heidelberg 2009

2
1 Introduction
tractable and have some nontrivial scope. Another requirement is that it has a principled
way of ensuring that the knowledge-base from which reasoning is done is robust, in the
sense that errors in the deductions are at least controllable [255].
Aiming at building integrated reasoning and learning methods, our approach pro-
vides a uniﬁed computational foundation for neural networks and nonclassical rea-
soning. Knowledge is expressed by a symbolic language, whereas deduction and
learning are carried out by a robust connectionist engine. This book also seeks to
contribute to the long-term aim of representing expressive symbolic formalisms in
learning systems [253], by means of neural-symbolic integration [66]. Ultimately,
the goal is to produce biologically motivated models with integrated reasoning and
learning capabilities, in which neural networks provide the inspiration and the ma-
chinery necessary for cognitive computation and learning, while several nonclas-
sical logics provide practical reasoning and explanation capabilities to the models,
facilitating the interaction between the models and the outside world. This book
contributes to the integration of both research programmes into a uniﬁed founda-
tion; both of these programmes are now widely but separately used in many areas
of computer science and artiﬁcial intelligence [42,66,87,125].
A historical criticism of neural networks was raised by McCarthy back in 1988
[176]. McCarthy referred to neural networks as having a “propositional ﬁxation”,
in the sense that they were not able to represent ﬁrst-order logic. This, per se,
has remained a challenge for a decade, but several approaches have now dealt
with ﬁrst-order reasoning in neural networks (see e.g. [43] and Chap. 10). Per-
haps in an attempt to address McCarthy’s criticism, many researchers in the area
have focused attention only on ﬁrst-order logic. This has suppressed developments
in other important fronts, mainly in nonclassical, practical reasoning, which also
should be at the centre of neural-symbolic integration research owing to the prac-
tical nature of neural-network research. We have shown recently that nonclassi-
cal reasoning can be used in a number of applications in neural-symbolic sys-
tems [33, 68–72, 74, 76–79, 157]. This has been possible through the integration
of nonclassical logics and neural networks.
Notwithstanding this evidence, little attention has been given to nonclassical rea-
soning and its integration with neural networks. We believe that for neural com-
putation to achieve its promise, connectionist models must be able to cater for
nonclassical reasoning. Research on nonclassical logics, including new results on
modal, temporal, intuitionistic, and nonmonotonic logics and their combinations,
has been relevant not only to computer science and artiﬁcial intelligence, but also to
economics and the physical sciences. We believe that neural-symbolic systems can
beneﬁt from the results and achievements that nonclassical logics have had in all
these areas.
In summary, we shall argue in this book that nonclassical reasoning is fundamen-
tal in the construction of computational connectionist models. If one assumes that
neural networks can represent rich models of human reasoning and learning, and
can offer an alternative solution to the challenges confronted by intelligent compu-
tation, it is undeniable that nonclassical logics should play a fundamental role at the
centre of this enterprise.

1.2 Methodology and Related Work
3
1.2 Methodology and Related Work
Several approaches have been proposed for integrating the connectionist and sym-
bolic paradigms of artiﬁcial intelligence. Most provide a solution to the learning of
classical propositional logic or fragments of ﬁrst-order logic by means of neural net-
works or related methods (see e.g. [12,43,148,228,229,250,254]). Our book [66]
surveys the work on neural-symbolic integration done until 2002, and proposes a
methodology for dealing with nonmonotonicity in artiﬁcial neural networks, includ-
ing knowledge extraction. In [12], a survey of recent developments in classical-logic
learning in neural networks is presented. Further, [66] showed that neural-symbolic
systems are also appropriate for tackling learning in real-world problems. In partic-
ular, the analysis presented in [66] shows that neural-symbolic systems can be used
effectively in a number of applications, ranging from the detection of large-scale
power system failures to DNA sequence analysis.
Despite the signiﬁcant contributions of the developments in ﬁrst-order logic to
knowledge representation, learning, and reasoning in artiﬁcial intelligence, a truly
intelligent agent or multiagent system, in the sense deﬁned in [271], has several
dimensions that cannot be appropriately managed solely by the use of ﬁrst-order
classical logic.
There are several extensions and alternatives to classical logic. Nonclassical log-
ics have become useful in computer science and artiﬁcial intelligence over the last
few decades. Such logics have been shown to be adequate for expressing several
features of reasoning, allowing for the representation of temporal, epistemic, and
probabilistic abstractions in computer science and artiﬁcial intelligence, as shown
for example, in [42,87,106,121].
For instance, temporal, modal, and intuitionistic logics are now amongst the most
successful logical languages used in computing. Born in philosophy and mathemat-
ics, they have beneﬁted from research efforts in applications of computing. Several
semantic models and (automated) proof systems have been designed for nonclassi-
cal logics [42, 88, 104]. Temporal logic has had a successful history in computer
science and artiﬁcial intelligence since the pioneering work of Pnueli, back in 1977
[207], as it allows an accurate and elegant formalism for reasoning about the dy-
namics of computing systems. Temporal logic has had a large impact in both acad-
emia and industry [89,103]. Modal logic, in turn, has also become a lingua franca
in the areas of formalisation, speciﬁcation, veriﬁcation, theorem proving, and model
checking in multiagent and distributed computing systems [42,50,87,106,143,154].
Nonmonotonic reasoning dominated research in artiﬁcial intelligence in the 1980s
and 1990s, and intuitionistic logic is considered by many to be an adequate logi-
cal foundation in several core areas of theoretical computer science, including type
theory and functional programming [258]. Other applications of nonclassical logics
include the characterisation of timing analysis in combinatorial circuits [180] and
in spatial reasoning [23], with possible use in geographical information systems.
For instance, Bennett’s propositional intuitionistic approach provided for tractable
yet expressive reasoning about topological and spatial relations. In [106], several
applications of many-dimensional modal logic are illustrated.

4
1 Introduction
Automated reasoning and learning theory have been the subject of intensive in-
vestigation since the early developments in computer science and artiﬁcial intelli-
gence [81,174,251]. However, while machine learning has been developed mainly
by the use of statistical and connectionist approaches (see e.g. [125,173,184,262]),
the reasoning component of intelligent systems has been developed using classi-
cal and nonclassical logics (see e.g. [42,87,100,104]). The acceptance of the need
for systems that integrate reasoning and learning into the same foundation, and the
evolution of the ﬁelds of cognitive and neural computation, has led to a number of
proposals integrating logic and machine learning [43, 51, 66, 77, 79, 118, 148, 164,
229,250,254,255].
An effective model of integrated reasoning and learning has been shown to be
attainable by means of neural-symbolic learning systems [66, 69–72, 74, 79]. This
book advocates the use of nonclassical logics as a foundation for knowledge rep-
resentation and learning in neural-symbolic systems. We propose a new approach
for representing, reasoning with, and learning nonclassical logics in a connectionist
framework, which leads, in a principled way, to a powerful but computationally light
cognitive model combining expressive nonclassical reasoning and robust learning;
we call it f ibred network ensembles.
In contrast to symbolic learning systems, neural networks’ learning implicitly
encodes patterns and their generalisations in the networks’ weights, so reﬂecting the
statistical properties of the trained data [35]. The merging of theory (background
knowledge) and data learning (learning from examples) into neural networks has
been shown to provide a learning system that is more effective than purely symbolic
or purely connectionist systems, especially when the data are noisy [246,250]. This
result has contributed to the growing interest in developing neural-symbolic learning
systems. By integrating logic and neural networks, neural-symbolic systems may
provide (i) a logical characterisation of a connectionist system, (ii) a connectionist
(parallel) implementation of a logic, or (iii) a hybrid learning system that brings
together features from connectionism and symbolic artiﬁcial intelligence.
Until recently, neural-symbolic systems were not able to fully represent, com-
pute, and learn expressive languages other than propositional logic and fragments
of ﬁrst-order, classical logic [12,43,51,238]. To the best of our knowledge, research
efforts towards representing nonclassical logical formalisms in connectionist sys-
tems were scant until the early 2000s. However, in [67, 70, 73, 74, 76–78], a new
approach to knowledge representation and reasoning in neural-symbolic systems
based on neural-network ensembles was proposed, namely connectionist nonclassi-
cal logics. In [75], connectionist modal logic (CML) was introduced, showing that
modalities can be represented effectively in neural networks. In [70,72,73], the lan-
guage of the Connectionist Temporal Logic of Knowledge (CTLK) was introduced,
and in [76–78] the computation of intuitionistic reasoning was shown to be learn-
able within neural networks. This new approach shows that a variety of nonclassical
logics can be effectively represented in artiﬁcial neural networks. To the best of our
knowledge, this was the ﬁrst approach to combining nonclassical logics and neural
networks.

1.2 Methodology and Related Work
5
Recently, it has also been shown that value-based argumentation frameworks can
be integrated with neural networks, offering a uniﬁed model for learning and rea-
soning about arguments, including the computation of circular and accrual argu-
mentation [68, 69]. The study of formal models of argumentation has also been a
subject of intensive investigation in several areas, notably in logic, philosophy, de-
cision theory, artiﬁcial intelligence, and law [25,31,39,48,83,210,212]. In artiﬁcial
intelligence, models of argumentation have been one of the approaches used in the
representation of commonsense, nonmonotonic reasoning. They have been partic-
ularly successful when modelling chains of defeasible arguments so as to reach a
conclusion [194,209]. Although logic-based models have been the standard for the
representation of argumentative reasoning [31, 108], such models are intrinsically
related to artiﬁcial neural networks, as we shall show in Chap. 11. This relationship
between neural networks and argumentation networks provides a model in which
the learning of arguments can be combined with argument computation.
This book also presents a new neural-network architecture based on the idea of
ﬁbring logical systems introduced by Gabbay [101]. Fibring allows one to com-
bine different systems (here, neural networks) in a principled way. Fibred neural
networks may be composed not only of interconnected neurons but also of other
networks in a recursive architecture. A ﬁbring function then deﬁnes how this recur-
sive architecture must behave, by deﬁning how the networks should relate to each
other (typically by allowing the activation of one network to inﬂuence the changes
of the weights of another). We show that, in addition to being universal approxi-
mators, ﬁbred networks can approximate any polynomial function to any desired
degree of accuracy, and are thus more expressive than standard feedforward neural
networks.
Neural-symbolic systems that use simple neural networks, such as single-hidden-
layer feedforward or recurrent networks [125], typically only manage to repre-
sent and reason about propositional symbolic knowledge or if then else rules
[36,66,95,205,250]. On the other hand, neural-symbolic systems that are capable of
representing and reasoning about (fragments of) ﬁrst-order logic are normally less
capable of learning new concepts efﬁciently [136, 149, 229, 243]. There is clearly
a need to strike a balance between the reasoning and learning capabilities of such
systems, and between expressiveness and computational complexity.
As argued in [43], if connectionism is an alternative paradigm to artiﬁcial intelli-
gence, neural networks must be able to compute symbolic reasoning in an efﬁcient
and effective way. It is also argued that connectionist systems are usually fault-
tolerant, whereas symbolic systems may be ‘brittle and rigid’. We seek to tackle
these problems by offering a principled way of computing, representing, and learn-
ing nonclassical logics within connectionist models.
The combination of nonclassical reasoning and connectionism is achieved by
means of algorithms that translate logical clauses into neural-network ensembles.
Such algorithms can be proved correct in the sense that the ensembles compute a
semantics of the original theory. An immediate consequence of our approach is the
ability to perform learning from examples efﬁciently, by applying, for example, the
backpropagation learning algorithm [224] to each network of the ensemble. We also

6
1 Introduction
show the effectiveness of our approach as a (distributed) knowledge representation,
reasoning, argumentation, and learning mechanism by applying it to the well-known
test beds found in the literature [87,121]. Our approach paves the way for modular,
integrated computation and learning of distributed, nonclassical knowledge, with a
broad range of applications from practical reasoning to evolving multiagent systems.
Technical aspects of this work will be presented throughout the book as the need
arises. No assumption is made that the reader has prior knowledge of nonclassical
logic or neural networks. Connectionist modal logic, a (one-dimensional) ensem-
ble of neural networks [66], is used to represent modalities such as necessity and
possibility. In CTLK, a two-dimensional network ensemble is used to represent
the evolution of knowledge through time. In both cases, each network ensemble
can be seen as representing a possible world that contains information about the
knowledge held by an agent in a distributed system. Learning in the CML system is
achieved by training each network in the ensemble independently, corresponding to
the evolution of an agent’s knowledge within a possible world. It is important that
these logics are investigated within the neural-computation paradigm. For instance,
applications in artiﬁcial intelligence and computer science have made extensive use
of decidable modal logics, including the analysis and model checking of distributed
and multiagent systems, program veriﬁcation and speciﬁcation, and hardware model
checking.2 In the case of temporal and epistemic logics, these logics have found a
large number of applications, notably in game theory and in models of knowledge
and interaction in multiagent systems [87,89,103,207].
From a machine-learning perspective, the merging of theory (background knowl-
edge) and data learning (learning from examples) in neural networks has provided
learning systems that are more effective than purely symbolic or purely connec-
tionist systems [246, 250]. In order to achieve this merging, ﬁrst one translates the
background knowledge into a neural network’s initial architecture, and then one
trains the network with examples using, for example, backpropagation. In the case
of CML, for instance, learning is achieved by training each individual network, each
of which is a standard network.
Another long-term aim is to contribute to the challenge of representing expressive
symbolic formalisms within learning systems. We are thus proposing a methodol-
ogy for the representation of several nonclassical logics in artiﬁcial neural networks.
We believe that connectionist approaches should take these logics into account by
means of adequate computational models catering for reasoning, knowledge repre-
sentation, and learning. This is necessary because real-world applications, such as
failure diagnosis, fraud prevention, and bioinformatics applications, will require the
use of languages more expressive than propositional logic. Bioinformatics, in par-
ticular, requires very much the ability to represent and reason about relations such
as those used in predicate logic [6]. In summary, knowledge is represented by a
symbolic language, whilst deduction and learning are carried out by a connectionist
engine.
2 It is well known that modal logic corresponds, in terms of expressive power, to the two-variable
fragment of ﬁrst-order logic [264]. Further, as the two-variable fragment of predicate logic is decid-
able, this explains why modal logic is so robustly decidable and amenable to multiple applications.

1.3 Structure of the Book
7
1.3 Structure of the Book
This research monograph is divided into the following chapters. Chapter 1 (the
present chapter) introduces the subject and overviews developments in the area of
connectionist models for integrated reasoning and learning. Chapter 2 introduces the
basic concepts of logic and knowledge representation. Chapter 3 introduces the con-
cepts of neural networks. Chapter 4 introduces the foundations of neural-symbolic
integration. Chapter 5 introduces connectionist modal logic, covering some foun-
dational results and introductory examples. Chapter 6 presents CTLK and its ap-
plications in distributed temporal knowledge representation and learning. Chapter 7
introduces intuitionistic reasoning and learning in neural networks. Chapter 8 de-
scribes some applications of connectionist nonclassical reasoning. Chapter 9 intro-
duces the idea of combining (ﬁbring) networks, for example CTLK and intuitionism.
Chapter 10 describes the combination networks to represent and learn relations in a
ﬁrst-order setting with variables. Chapter 11 establishes a close relatioship between
connectionist models and argumentation frameworks, and uses argumentation as
an application of ﬁbring. Chapter 12 introduces symbolic reasoning under uncer-
tainty in neural networks and illustrates its feasibility using well-known test beds,
including the Monty Hall puzzle [121]. Chapter 13 concludes the book and indicates
directions for further research.
An extensive list of references cited is provided at the end of the book. The list
is by no means complete, as the literature in this ﬁeld is vast.

Chapter 2
Logic and Knowledge Representation
This chapter introduces the basics of knowledge representation and of logic used
throughout this book. The material on logic presented here follows the presenta-
tion given in [100]. Several books contain introductions to (logic-based) knowledge
representation and reasoning, including [42,87,100,106].
Classical logic is a powerful language for certain forms of reasoning, and much
can be done with it. However, it does not capture all that the natural reasoning meth-
ods of humans do. For example, we might not know for certain that an atomic sen-
tence was true, but know that there was a 75% chance that it was – this cannot
be captured by classical propositional logic. As a further illustration, let the atoms
p and q represent two different coins which can be exchanged for an item r. This
could be stated by p∧q⇒r. However, stating that the amount represented by p and
q is used up in the exchange cannot be done by classical reasoning. When we use
an implication A⇒B, we assume that no change is undergone by A when it is used
to conclude B. This does not carry over into a number of problems in computer sci-
ence. Note that attempts to express the changes to p and q are doomed to failure:
p∧q⇒r∧¬p∧¬q reduces to ¬(p∧q).
Many nonclassical logics have been devised to handle these and other problems
which exceed the capabilities of classical logic. There are alternatives to classical
logic which add more connectives to the language, and other alternatives which
vary the meaning of the existing connectives. Amongst the most important of the
latter are many-valued logics, intuitionistic logic, modal and temporal logics, and
resource (relevance and linear) logics. For an introduction to the subject, we refer
the reader to [42, 100]. A thorough state-of-the-art analysis of nonclassical logics
can be found in the multivolume collection [102].
In this book, several nonclassical logics will be of interest, such as the modal,
temporal, and intuitionistic logics. They will be introduced when an account of the
motivation behind them and deﬁnitions are needed, providing the reader with an
integrated presentation of the connectionist models of nonclassical reasoning de-
scribed in the book. Before presenting each nonclassical connectionist model, how-
ever, we give the reader a taste of (classical and nonclassical) logic and knowledge
representation, which will serve as background.
A.S. d’Avila Garcez et al., Neural-Symbolic Cognitive Reasoning, Cognitive Technologies,
9
c⃝Springer-Verlag Berlin Heidelberg 2009

10
2 Logic and Knowledge Representation
2.1 Preliminaries
We need to assert some basic concepts of set and order theory that will be used
throughout this book (see [54,117,213] for details). These concepts are useful when
deﬁning semantic models for several logics and knowledge representation lan-
guages. In what follows, N and R will denote the sets of natural and real numbers,
respectively.
Deﬁnition 1. A partial order is a reﬂexive, transitive, and antisymmetric binary re-
lation on a set.
Deﬁnition 2. A partial order ⪯on a set X is total iff for every x,y ∈X, either x ⪯y
or y ⪯x. Sometimes ⪯is also called a linear order, or simply a chain.
As usual, x ≺y is an abbreviation for x ⪯y and x ̸= y.
Deﬁnition 3. In a partially ordered set [X,⪯], x is the immediate predecessor of y
if x ≺y and there is no element z in X such that x ≺z ≺y. The inverse relation is
called the immediate successor.
Deﬁnition 4. Let X be a set and ⪯an ordering on X. Let x ∈X.
• x is minimal if there is no element y ∈X such that y ≺x.
• x is a minimum if for all elements y ∈X,x ⪯y. If ⪯is also antisymmetric and
such an x exists, then x is unique and will be denoted by inf(X).
• x is maximal if there is no element y ∈X such that x ≺y.
• x is a maximum if for all elements y ∈X,y ⪯x. If ⪯is also antisymmetric and
such an x exists, then x is unique and will be denoted by sup(X).
A maximum (or minimum) element is also maximal (or minimal, respectively)
but is, in addition, comparable to every other element. This property and antisym-
metry lead directly to the demonstration of the uniqueness of inf(X) and sup(X).
Deﬁnition 5. Let [X,⪯] be a partially ordered set and let Y ⊆X.
• An element x ∈X is an upper bound of Y if y ⪯x for all y ∈Y (a lower bound is
deﬁned dually).
• x is called the least upper bound (lub) of Y if x is an upper bound of Y and x ⪯z
for all upper bounds z of Y (the greatest lower bound (glb) is deﬁned dually).
• If any two elements x and y in X have a least upper bound, denoted by x ∨y
and read as “x join y”, and a greatest lower bound, denoted by x ∧y and read as
“x meet y”, then X is called a lattice.
• If lub(Y) and glb(Y) exist for all Y ⊆X then X is called a complete lattice.
• A lattice L is distributive if x∨(y∧z) = (x∨y)∧(x∨z) and x∧(y∨z) = (x∧y)∨
(x∧z), for all x,y,z ∈L.

2.2 Classical Logic
11
Deﬁnition 6. LetU be a set and f :U ×U →R be a function satisfying the following
conditions:
•
f(x,y) ≥0,
•
f(x,y) = 0 ↔x = y,
•
f(x,y) = f(y,x), and
•
f(x,y) ≤f(x,z)+ f(z,y).
f is called a metric on U.1 A metric space is a tuple ⟨U, f⟩. A metric f on U is
bounded iff for some constant k ∈R, f(x,y) ≤k for all x,y ∈U.
2.2 Classical Logic
This section introduces the basic deﬁnitions of propositional and ﬁrst-order predi-
cate logic. The presentation is based on deﬁnitions in [100], which offers a compre-
hensive introduction to logic in computation and artiﬁcial intelligence.
Propositional logic is weaker in terms of expressive power than ﬁrst-order pred-
icate logic. The smallest semantic units which can receive truth values in propo-
sitional logic are sentences. Yet we are often concerned with the various pieces
of information which make up a sentence. For example, the utterance, seen as a
sentence in propositional logic, ‘a hero is someone admired by everyone’ contains
several items of data which cannot be represented distinctly in propositional logic.
We would be reduced to using a single proposition, p say, to represent the entire
sentence.
To increase the expressive power of (propositional) logic, we need to add a means
to talk about the semantic units contained within sentences – this entails using a
predicate logic. In the deﬁnitions below, the differences between these logics be-
come very clear.
2.2.1 Propositional Logic
We now formally deﬁne the notion of a well-formed formula (wff) of classical
propositional logic and the truth values given to the various operators.
Deﬁnition 7. (Classical Propositional Logic) [100].
1. The language of classical propositional logic contains a set Lp of atomic propo-
sitions with typical members {p,q,r,s,q1,q2,q3,...} and a set of connectives
{∧,∨,¬,→}.
2. The notion of a well-formed formula (wff) is deﬁned inductively by the following
conditions:
1 f is sometimes called a distance function.

12
2 Logic and Knowledge Representation
(a) Any atom p ∈Lp is a wff. We say it is built up from {p}.
(b) If A and B are wffs, then so are ¬(A),(A)∧(B),(A)∨(B),(A)→(B). If A
is built up from the atoms {p1,..., pn} and B is built up from the atoms
{q1,...,qk}, then ¬(A) is built up from the same atoms as A, and (A)∧(B),
(A)∨(B), and (A) →(B) are built up from {p1,..., pn,q1,...,qk}.
(c) Let A be a formula built up from q1,...,qk. We indicate this fact by writing
A(q1,...,qk). Let B1,...,Bk be wffs. We deﬁne by structural induction the
result of substituting into A the formulas Bi for the atom qi, for i = 1,...,k.
We denote this substitution by A(q1/B1,...,qk/Bk) and refer to it as a sub-
stitution instance of A.
• For atomic A(q) = q, we let A(q/B) = B.
• (¬(A))(q1/B1,...,qk/Bk) = ¬(A)(q1/A1,...,qk/Bk).
• ((A) →(B))(q1/B1,...,qk/Bk) = A(q1/B1,...,qk/Bk) →
B(q1/B1,...,qk/Bk), and similarly for (A)∧(B) and (A)∨(B).
3. An interpretation (or assignment) is any function h that assigns truth values
to the atomic propositions. h is a function from Lp into {⊤,⊥}, i.e. h : Lp →
{⊤,⊥}.
4. We can extend the deﬁnition of h from atomic propositions to any wff A by
induction, as follows:
(a) h(¬(A)) = ⊤if h(A) = ⊥, otherwise the value is ⊥.
(b) h((A)∧(B)) = ⊤if h(A) = ⊤and h(B) = ⊤, otherwise the value is ⊥.
(c) h((A)∨(B)) = ⊤if h(A) = ⊤or h(B) = ⊤or both, otherwise the value is ⊥.
(d) h((A)→(B)) = ⊤if h(A) = ⊥or h(B) = ⊤or both, otherwise the value is ⊥.
The above deﬁnition of h(A) agrees with our understanding of the meaning of
the connectives as presented in the truth tables.
5. We may ﬁnd it convenient to assume that our language contains as atomic propo-
sitions the constant atoms ⊤and ⊥. ⊤is always true and ⊥is always false.
2.2.2 First-Order Logic
Deﬁnition 8 (Syntax of Predicate Logic) [100]. The formulas of the predicate
language L are built up from the following symbols:
• a set Lpred of predicate symbols, each with an associated arity, which is a positive
integer;
• a set Lcons of constant symbols;
• a set Lvar of variable symbols;
• a set Lfunc of function symbols, each with an associated arity, which is a positive
integer;
• quantiﬁers ∀, ∃;
• classical connectives ¬ and →. The other classical connectives are deﬁnable from
these connectives.

2.2 Classical Logic
13
The set of terms Lterm is given by the following rules:
• any member of Lcons is a term in Lterm, with no variables;
• any member x of Lvar is a term in Lterm, with variable x (itself);
• if f is a member of Lfunc with arity n, and t1,...,tn are terms in Lterm, then
t = f(t1,...,tn) is a term in Lterm. The set of variables of t is the union of all the
sets of variables of t1,...,tn.
We can now deﬁne the wffs of L by the following rules:
• ⊥is a wff of L, with no free variables;
• if p is a member of Lpred with arity n, and t1,...,tn are terms in Lterm, then
p(t1,...,tn) is a wff in L, with the free variables being all the variables of
t1,...,tn;
• if ϕ is a wff in L, then so is ¬(ϕ);
• if ϕ and ψ are wffs in L, then so is ϕ→ψ, with the free variables being the union
of those free in ϕ and ψ;
• if v is a variable in Lvar and ϕ is a wff in L, then ∃v.ϕ is a wff in L. The free
variables of ∃v.ϕ are those of ϕ less the variable v.
• if v is a variable in Lvar and ϕ is a wff in L, then ∀v.ϕ is a wff in L. The free
variables of ∀v.ϕ are those of ϕ less the variable v.
Deﬁnition 9 (Semantics of Predicate Logic). The formulas are given truth values
with respect to an interpretation or a model M = ⟨D,πcons,πfunc,πpred⟩, with the
following four components:
• D, a nonempty domain of objects;
• πcons, a mapping from members of Lcons to D;
• πfunc, which maps each member of Lfunc to a function mapping Dn to D, for each
p ∈Lfunc πfunc(p) : Dn →D, where n is the arity of the member of Lfunc; and
• πpred, which maps each member of Lpred to Dn →{⊤,⊥}, where n is the arity of
the member of Lpred.
We also need to interpret the free variables in the formulas. This is done by
deﬁning a variable assignment V, which is a mapping from Lvar to D. We also need
the notation V[v→d] to mean the assignment V ′ satisfying V ′(x) = V(x), for x ̸= v
and V ′(v) = d. Given this, we can interpret all the terms of L by means of a term
mapping πterm, based on V, πcons, and πfunc, which maps all members of Lterm to
D. For t in Lterm:
• for all members c of Lcons, πterm(c) = πcons(c);
• for all members v of Lvar, πterm(v) = V(v);
• for all members f of Lfunc with arity n, πterm(f(t1,...,tn)) = πfunc(f)
(πterm(t1),...,πterm(tn));
• πterm(t) is called the value of t in the model, under the assignment V.

14
2 Logic and Knowledge Representation
Finally, we can deﬁne the truth of a wff ϕ of L, with respect to an interpretation
M and a variable assignment V. This is written as ⟨M,V⟩⊨ϕ, read as ‘ϕ holds in
⟨M,V⟩’ or ‘⟨M,V⟩is a model of ϕ’, and given by
⟨M,V⟩̸⊨⊥,
⟨M,V⟩⊨p(t1,...,tn) iff πpred(p)(πterm(t1),...,πterm(tn)) = ⊤,
⟨M,V⟩⊨ϕ→ψ
iff ⟨M,V⟩⊨ϕ implies ⟨M,V⟩⊨ψ,
⟨M,V⟩⊨∃v. ϕ
iff there exists d ∈D and ⟨M,V[v→d]⟩⊨ϕ,
⟨M,V⟩⊨∀v. ϕ
iff for all d ∈D, ⟨M,V[v→d]⟩⊨ϕ.
Let ϕ(x1,...,xn) be a formula with free variables x1,...,xn. It is common
to use the notation M ⊨ϕ(a1,...,an) to represent the satisﬁability relation
⟨M,V[xi→ai|i=1,...,n]⟩⊨ϕ.
If the formula contains no free variables, then an arbitrary mapping (sometimes
called an empty mapping) can be used as the initial variable assignment. We use the
notation [ ] for such a mapping and we write ⟨M,[]⟩⊨ϕ.
We write ⊨ϕ to indicate that ‘ϕ holds in all models ⟨M,V⟩’.
2.3 Nonclassical Logics
Logic has had a rich and fruitful history, particularly over the last century. De-
velopments in computer science, artiﬁcial intelligence, philosophy, and the natural
sciences have led to the development of new forms of (formal) reasoning, consider-
ing dimensions such as time, space, knowledge, belief, probability, and uncertainty.
Logic has been called the ‘calculus of computer science’ by analogy with the cru-
cial role calculus has historically played in physics [122,170]. These developments
have led to several developments in nonclassical logics [42,100,214].
Nonclassical logics are ubiquitous in artiﬁcial intelligence and computer science.
These logics have been developed as alternatives to classical logic and have been ap-
plied extensively to research areas in which the use of classical mathematical logic
has proved to be of restricted or limited use [42,100,214]. In particular, modal, tem-
poral, conditional, epistemic, fuzzy, linear, intuitionistic, and nonmonotonic logics
have had a noticeable and effective impact on computer science.
More recently, developments in quantum computation have been inﬂuenced by
and have inﬂuenced developments in quantum logic [147], showing that logic has a
role to play in physics. On the other hand, developments in computer science have
inﬂuenced research on (nonclassical) logical systems because the notions of an algo-
rithm and of a constructive proof are strongly related, as shown in [172] for example.
In addition, the development of nonmonotonic logics, particularly in the 1980s, has
also led to new developments in the logical modelling of forms of commonsense
reasoning including belief revision, conditional reasoning through conditional log-
ics (the study of conditional sentences, or if sentences), and abductive and epistemic
reasoning (aimed at formalising the notion of the knowledge of a cognitive agent).

2.3 Nonclassical Logics
15
Nonclassical logics are distinguished semantically from their classical counter-
parts. One notion widely used in nonclassical logics is the notion of possible-world
semantics. In this brief introduction, we provide only the basic intuition underly-
ing its foundations. This semantic technique has been successfully used in several
nonclassical logical systems, including modal, temporal, conditional, intuitionistic,
and epistemic logics. Possible worlds can be used to model an agent’s knowledge or
beliefs about the world, i.e. the different ways in which an agent sees possibilities
about the state of the world he or she is reasoning about, acting in or observing.
They can also be used in a temporal perspective considering linear or branching
time, where time points may correspond to states of a (computational) system.
Intuitively, under this semantics, a proposition is necessarily true in a possible
world ω1 if it is true in all possible worlds accessible from ω1. A proposition is pos-
sibly true in a given world ω2 if it is true in some world accessible from ω2. These
notions of necessity and possibility are called modalities. In the case of temporal
reasoning, several modalities may be useful. For instance, one may refer to formu-
las that are always true in the future, true sometime in the past, true at the previous
time point, and true at the next time point, amongst other possibilities. Of course, the
choice of the appropriate modalities will depend on the expressive power needed in
the applications at hand, which demands a proper adoption of a nonclassical logical
system. In the forthcoming chapters, we shall formally present the possible-worlds
semantic model as introduced by Kripke and Hintikka in the early 1960s [129,156].
We shall introduce the other logics formally as the need arises throughout the book.
In order to illustrate the difference between classical and nonclassical logics, let
us consider temporal logic as described in [100]. Let us regard the assignment of
truth values to propositions as being a description of the world or situation with re-
spect to a particular time t. In temporal logic, the value assigned to a proposition
(statement) can vary with the ﬂow of time. This is not the case in classical logic:
once a statement (a proposition) is proved, its truth value is deﬁnite. However, in ar-
tiﬁcial intelligence and computer science, as opposed to classical mathematics, time
is an extremely relevant dimension as we are frequently working with several states,
statements about particular time points or intervals, and several interpretations of
these states. Under a modal interpretation of time, one could refer to the truth val-
ues of a proposition on a linear timeline (considering both past and future), or a
branching-time interpretation with several futures, using modalities such as always
true in the future/past and sometimes true in the future/past, among several other
possibilities. This turns temporal logic into an expressive nonclassical logical sys-
tem, which perhaps explains the success of this logic in computer science, artiﬁcial
intelligence, and cognitive science [33,42,85,89,122,157].
In summary, nonclassical logics are fundamental in computer science today. Re-
search in computer science is now often a multidisciplinary endeavour. Nonclassical
logics offer an adequate language for and formalisation of several dimensions that
are relevant in the modelling of cognitive abilities, including reasoning, learning,
and analogy. In the forthcoming chapters, we shall exploit nonclassical logics in
more detail.

16
2 Logic and Knowledge Representation
2.4 Nonmonotonic Reasoning
The study of nonmonotonic reasoning has had a relevant impact on logic and
artiﬁcial intelligence, with implications in computer science and philosophical
logics (see [102] for a comprehensive historical and philosophical analysis of
nonmonotonicity). Nonmonotonic reasoning grew out of attempts to capture the
essential aspects of commonsense (practical) reasoning. It resulted in a number
of important formalisms, the most well known of them being the circumscription
method of McCarthy [175], the default theory of Reiter [220], and the autoepis-
temic logic of Moore [188] (see [7,171] for an introduction to the subject, and [104]
for a thorough study).
Nonmonotonicity is used for reasoning with incomplete information in common-
sense or practical reasoning. If, later, additional information becomes available, it
may turn out that some conclusions are no longer justiﬁed, and must be withdrawn.
The standard example of this case is that, if we learn that Tweety is a bird, we
conclude that it can ﬂy, but if we subsequently ﬁnd out that Tweety is a penguin,
we withdraw that conclusion. This use of logic, called belief revision, is clearly
nonmonotonic.2
One way of studying nonmonotonic logical systems is by analysing which prop-
erties their consequence relations satisfy, as suggested by Gabbay [97].3 A very
strong property of inference relations is monotonicity. An inference relation (⊢) is
monotonic if it satisﬁes Γ ⊢Ψ implies Γ ∪δ ⊢Ψ, where Γ,Ψ are sets of formu-
las, and δ is a new premise, added to Γ. The negation-as-ﬁnite-failure rule [49] and
the closed-world assumption (CWA) [220] introduce nonmonotonicity into logic
when negative literals are derived, since, for example, {p ←q} entails ¬p whereas
{p ←q,q} does not entail ¬p.4 A good example of the use of the CWA is the process
of booking a ﬂight. Assume that you want to know whether there is a ﬂight from
London to Porto Alegre on 6 August 2012. Assume that the database of your travel
agent does not contain such a ﬂight. He will inform you that there is no ﬂight from
London to Porto Alegre on that date. In order to be able to jump to this conclusion,
the travel agent has to assume that all ﬂights from London to Porto Alegre are listed
on his database. If, later, a new ﬂight is entered into the database, then the earlier
conclusion has to be withdrawn, but the convention that the database is complete
(i.e. the CWA) will remain (see [37,166]).
2 The postulates of belief revision, put forward by Alchourr´on, G¨ardenfors, and Makinson, were
published in the inﬂuential paper [4]. These postulates are now widely known as the AGM postu-
lates.
3 This ﬁeld of research underwent several developments in the 1980s and 1990s. The diligent
reader is referred to [155,168] for a complete analysis of the theme.
4 The notation p ←q used here is equivalent to the traditional notation for the implication (classical
conditional) q →p. Logic programming, however, has adopted a syntactic notation in which the
consequent of a conditional (namely, the head of a clause) appears to the left of the inverted arrow
←and the antecedents appear to the right of the inverted arrow (these are called the body of the
clause).

2.5 Logic Programming
17
2.5 Logic Programming
The use of logic in computer programming can be traced back to the beginnings
of computer science and artiﬁcial intelligence [174]. Logic programming – which
should not be taken as meaning a particular implementation or language, such as
the programming language Prolog – has been a successful declarative programming
paradigm since the late 1970s [260]. Several applications of logic programming
have been studied over recent decades. The collection [104] contains foundational
chapters on logic programming and on its applications in automated reasoning and
theorem proving, in knowledge representation and reasoning, and in artiﬁcial intel-
ligence in general.
In this section, we use the standard notation and terminology of [163] to intro-
duce some basic concepts of logic programming. We follow the standard notation
with the exception that general logic programs are called normal logic programs
in [163]. As usual, we assume that a logic program P has already been instantiated
and thus all clauses are propositional. This assumption allows us to restrict our con-
sideration to a ﬁxed objective propositional language L. In particular, if the original
(uninstantiated) program is function-free, then the resulting objective language L is
ﬁnite.
Deﬁnition 10. A deﬁnite logic program P is a ﬁnite set of clauses of the form
A0 ←A1,...,An, where each Ai is a propositional variable. A0 is called the head
(or consequent) of the clause, and A1,...,An is called the body (or antecedent) of
the clause.
A propositional variable is also called an atom. A literal is an atom or the nega-
tion of an atom. BP denotes the set of atoms occurring in P, i.e. the Herbrand base
of P. An interpretation (or valuation) is a mapping from propositional variables to
{⊤,⊥}. It is extended to literals, clauses, and programs in the usual way. A model
for P is an interpretation which maps P to true. MP denotes the least Herbrand
model 5 of P.
The following result dates back to 1976 [260]. The interest in this result arises
from the fact that for a deﬁnite program P, the collection of all Herbrand interpre-
tations forms a complete lattice and there is a monotonic mapping associated with
P deﬁned on this lattice.
Proposition 1 (Model Intersection Property). Let P be a deﬁnite program and
{Mi} be a nonempty set of Herbrand models for P. Then ∩iMi is also a Herbrand
model for P.
In the following, we recall some concepts and results concerning monotonic map-
pings and their ﬁxed points.
Deﬁnition 11. Let [L,⪯] be a complete lattice and T : L →L be a mapping.
• T is monotonic iff x ⪯y →T(x) ⪯T(y).
5 After Jacques Herbrand, French logician (1908–1931).

18
2 Logic and Knowledge Representation
• Let Y ⊆L. Y is directed iff every ﬁnite subset of Y has an upper bound in Y.
• T is continuous iff for every directed subset Y of L, T(lub(Y)) = lub(T(Y)).
Deﬁnition 12. Let [L,⪯] be a complete lattice and T : L →L be a mapping. a ∈L is
the least ﬁxed point of T iff a is a ﬁxed point of T (i.e. T(a) = a) and, for all ﬁxed
points b of T, a ⪯b. Similarly, a ∈L is the greatest ﬁxed point of T iff a is a ﬁxed
point of T and, for all ﬁxed points b of T, b ⪯a.
Proposition 2. Let L be a complete lattice and T : L →L be monotonic. T has a
least ﬁxed point (l f p(T)) and a greatest ﬁxed point (gf p(T)).
Deﬁnition 13. Let L be a complete lattice and T : L →L be monotonic. We then
deﬁne: T ↑0 = inf(L);
T ↑α = T(T ↑(α −1)) if α is a successor ordinal;
T ↑α = lub{T ↑β | β ≺α) if α is a limit ordinal;
T ↓0 = sup(L);
T ↓α = T(T ↓(α −1)) if α is a successor ordinal;
T ↓α = glb{T ↓β | β ≺a) if α is a limit ordinal.
Proposition 3. Let L be a complete lattice and T : L →L be continuous. Then
l f p(T) = T ↑ω, where ω is the smallest limit ordinal.
Let P be a deﬁnite program. Then 2BP , which is the set of all Herbrand interpre-
tations of P, is a complete lattice under the partial order of set inclusion ⊆. The top
element of this lattice is BP and the bottom element is ∅.
Deﬁnition 14 (Immediate-Consequence Operator). Let P be a deﬁnite program.
The mapping TP : 2BP →2BP is deﬁned as follows. Let I be a Herbrand interpreta-
tion; then TP(I) = {A ∈BP | A ←A1,...,An is a ground instance of a clause in P
and {A1,...,An} ⊆I}.
TP provides the link between the declarative and the procedural semantics of P.
Clearly, TP is monotonic. Therefore, Herbrand interpretations that are models can
be characterised in terms of TP.
Proposition 4. Let P be a deﬁnite program and I a Herbrand interpretation of P.
Then the mapping TP is continuous, and I is a model of P iff TP(I) ⊆I.
Proposition 5 (Fixed-Point Characterisation of the Least Herbrand Model). Let
P be a deﬁnite program. MP = l f p(TP) = TP ↑ω.
So far, we have seen that if P is a deﬁnite logic program, then the least Herbrand
model of P exists and its classical (two-valued) semantics can be deﬁned as the least
ﬁxed point of a meaning operator TP. The semantics is obtained by lattice-theoretic
considerations which require TP to be monotonic. However, if P is a general logic
program, then TP may be nonmonotonic and, consequently, the existence of a least
ﬁxed point of TP cannot be guaranteed. Take, for example, P = {p ←∼q}; then
{p} and {q} are the only minimal Herbrand models of P, but none of them is the
least model of P. In this case, TP may even have no ﬁxed point at all, for example
when P = {p ←∼p}.

2.5 Logic Programming
19
2.5.1 Stable-Model and Answer Set Semantics
One of the striking features of logic programming (see [163]) is that it can naturally
support nonmonotonic reasoning – by means of negative literals. Many concepts
introduced in the area of nonmonotonic reasoning have a natural counterpart within
logic programming in spite of its limited syntax.
Deﬁnition 15. A general clause is a rule of the form A0 ←A1,...,Am,∼Am+1,...,
∼An, where Ai (0 ≤i ≤n) is an atom and ∼denotes default negation. A general
logic program is a ﬁnite set of general clauses.
There is no general agreement upon the answer to the question of what is the
standard model of a general program. However, there are some desired properties
of the natural model that can support some plausible answers to the question. The
following deﬁnitions attempt to formalise this requirement.
Deﬁnition 16 (Supported Interpretation). An interpretation I is called supported
if A0 ∈I implies that for some ground instance A0 ←A1,...,Am,∼Am+1,..., ∼An ∈
P we have that I |= A1 ∧... ∧Am∧∼Am+1 ∧...∧∼An. Intuitively, A1,...,Am,∼
Am+1,...,∼An is an explanation for A0.
Proposition 6. I is a supported model of P iff TP(I) = I.
Thus, in view of the above observation about the behaviour of the TP operator,
we see that for some programs no supported models exist, for example when P =
{p ←∼p}. One possible approach is to accept that some programs have no natural
supported model and to identify classes of programs for which a natural supported
model exists.
Deﬁnition 17 (Dependency). Consider a program P. The dependency graph DP
for P is a directed graph with signed edges. Its nodes are the literals occurring in P.
For every clause in P with p in its head and q as a positive (or negative) literal in its
body, there is a positive (or negative, respectively), edge (p,q) in DP.
• We say that p uses (or refers to) q positively (or negatively, respectively).
• We say that p depends positively (or negatively) on q if there is a path in DP
from p to q with only positive edges (or at least one negative edge, respectively).
• We say that p depends evenly (or oddly) on q if there is a path in DP from p to q
with an even (or odd, respectively) number of negative edges.
Deﬁnition 18 (Locally Stratiﬁed Program). A program P is called locally strati-
ﬁed if no cycle with a negative edge exists in its dependency graph [215].
In [115], Gelfond and Lifschitz introduced the notion of stable models – nowa-
days important in answer set programming [190] – by using the intuitive idea of
rational beliefs taken from autoepistemic logic.

20
2 Logic and Knowledge Representation
Deﬁnition 19 (Gelfond–Lifschitz Transformation). Let P be a grounded logic
program. Given a set I of atoms from P, let PI be the program obtained from P
by deleting (i) each rule that has a negative literal ∼A in its body with A ∈I, and
(ii) all the negative literals in the bodies of the remaining rules.
Clearly, PI is a positive program, so that PI has a unique minimal Herbrand
model. If this model coincides with I, then we say that I is a stable model of P.
Deﬁnition 20 (Stable Model). A Herbrand interpretation I of a program P is called
stable iff TP(I) = I.
The intuition behind the deﬁnition of a stable model is as follows: consider a
rational agent with a set of beliefs I and a set of premises P. Then, any clause that
has a literal ∼A in its body when A ∈I is useless, and may be removed from P.
Moreover, any literal ∼A with A /∈I is trivial, and may be deleted from the clauses
in P in which it appears. This yields the simpliﬁed (deﬁnite) program PI, and if I
happens to be precisely the set of atoms that follows logically from the simpliﬁed
set of premises, then the set of beliefs I is stable. Hence, stable models are possible
sets of belief that a rational agent might hold.
Stable-model semantics allow more than one stable model, or none at all. This
reﬂects some uncertainty about the conclusions that should be drawn from a pro-
gram. In some cases, a local uncertainty can destroy too much information. For
example, if P is a stratiﬁed program in which the variable p does not occur, then
P ∪{p ←∼p} has no stable model. Thus, the information contained in P is not re-
ﬂected in the stable-model semantics of P ∪{p ←∼p}, even though it is not related
to the uncertainty about the truth value of p.
The use of well-founded semantics [261] avoids this problem by using a three-
valued model. In contrast with three-valued logic (see [91]), a three-valued interpre-
tation of the connectives is not needed to obtain three-valued models. On the other
hand, well-founded semantics has the drawback of not inferring all atoms that one
would expect to be true (see Apt and Bol’s survey of logic programming and nega-
tion [9] for a comprehensive comparison between different semantics and classes of
programs).
2.6 Discussion
This chapter has brieﬂy introduced some basic concepts of logic and knowledge
representation. Logic has been fundamental to artiﬁcial intelligence since the early
stages of the ﬁeld [174]. Logic-based knowledge representation is the backbone of
artiﬁcial-intelligence research (see e.g. [42, 87, 106]). Several logical systems have
had an impact on artiﬁcial intelligence, including not only classical but also nonclas-
sical logics. Research on theorem proving, model checking, formal speciﬁcation of
multiagent systems, game playing, natural-language processing, planning, knowl-
edge representation, expert systems, and learning theory, to name but a few areas,

2.6 Discussion
21
has been directed inﬂuenced by results from classical and nonclassical logics. It
has been clearly indicated that logical languages, be they classical or nonclassical,
are adequate for this important task in artiﬁcial intelligence. Logic has also beneﬁted
from artiﬁcial-intelligence research. For instance, the ﬁeld of nonmonotonic reason-
ing has inﬂuenced logicians to axiomatise several forms of commonsense reasoning,
leading to the creation of new nonclassical logics of human and practical reasoning
(see [102]). The advances in research published annually in the proceedings of the
major conferences and in the major research journals in the ﬁelds of applied logic,
knowledge representation, and artiﬁcial intelligence corroborate this claim. Of par-
ticular interest to us, in this book, is knowledge representation. We shall make use of
several logic-based knowledge representation languages in our models. Any reason-
ing or learning task run in a computational model demands such a language, since
in any machine-learning algorithm or procedure, knowledge representation precedes
any other task.

Chapter 3
Artiﬁcial Neural Networks
This chapter introduces the basics of neural networks used in this book. Artiﬁcial
neural networks have a long history in computer science and artiﬁcial intelligence.
As early as the 1940s, papers were written on the subject [177]. Neural networks
have been used in several tasks, including pattern recognition, robot control, DNA
sequence analysis, and time series analysis and prediction [125]. Differently from
(symbolic) machine learning, (numeric) neural networks perform inductive learning
in such a way that the statistical characteristics of the data are encoded in their sets of
weights, a feature that has been exploited in a number of applications [66]. A good
introductory text on neural networks can be found in [127]. A thorough approach
to the subject can be found in [125]. The book by Bose and Liang [35] provides a
good balance between the previous two.
3.1 Architectures of Neural Networks
An artiﬁcial neural network is a directed graph. A unit in this graph is characterised,
at time t, by its input vector Ii(t), its input potential Ui(t), its activation state Ai(t),
and its output Oi(t). The units (neurons) of the network are interconnected via a set
of directed, weighted connections. If there is a connection from unit i to unit j, then
Wji ∈R denotes the weight associated with such a connection.
We start by characterising the neuron’s functionality (see Fig. 3.1). The activation
state of a neuron i at time t (Ai(t)) is a bounded real or integer number. The output
of neuron i at time t (Oi(t)) is given by the output rule fi, such that Oi(t) = fi(Ai(t)).
The input potential of neuron i at time t (Ui(t)) is obtained by applying the propaga-
tion rule of neuron i (gi) such that Ui(t) = gi(Ii(t),Wi), where Ii(t) contains the input
signals (x1(t),x2(t),...,xn(t)) to neuron i at time t, and Wi denotes the weight vector
(Wi1,Wi2,...,Win) to neuron i. Finally, the neuron’s new activation state Ai(t +Δt) is
given by its activation rule hi, which is a function of the neuron’s current activation
state and input potential, i.e. Ai(t +Δt) = hi(Ai(t),Ui(t)). The neuron’s new output
value is Oi(t +Δt) = fi(Ai(t +Δt)).
A.S. d’Avila Garcez et al., Neural-Symbolic Cognitive Reasoning, Cognitive Technologies,
23
c⃝Springer-Verlag Berlin Heidelberg 2009

24
3 Artiﬁcial Neural Networks
Wi1
Wi2
Win
Ui(t) Ai(t+Δt)
Oi(t+Δt)
Ai(t)
x1(t)
x2(t)
xn(t)
Fig. 3.1 The processing unit, or neuron
There are three basic kinds of activation function (hi): linear, nonlinear, and semi-
linear (also known as sigmoid). Neurons with linear, nonlinear (binary), and semi-
linear activation functions are called linear, nonlinear (binary), and semilinear neu-
rons, respectively. In Fig. 3.2, θ i is known as the threshold of the neuron’s activation
function.
In general, hi does not depend on the previous activation state of the unit, that is,
Ai(t +Δt) = hi(Ui(t)), the propagation rule gi is a weighted sum, such that Ui(t) =
∑jWi jxj(t), and the output rule fi is given by the identity function, i.e. Oi(t) = Ai(t).
In addition, most neural models also have a learning rule, responsible for changing
the weights of the network, and thus allowing it to perform inductive learning.
The units of a neural network can be organised into layers. An n-layer feed-
forward network N is an acyclic graph. N consists of a sequence of layers and
connections between successive layers, containing one input layer, n−2 hidden lay-
ers, and one output layer, where n ≥2. When n = 3, we say that N is a single-hidden-
layer network. When each unit occurring in the i-th layer is connected to each unit
occurring in layer i+1, we say that N is a fully connected network (see Fig. 3.3).
The most interesting properties of a neural network arise not from the functional-
ity of each neuron, but from the collective effect resulting from the interconnection
of units. Let r and s be the numbers of units occurring in the input and the out-
put layer, respectively. A multilayer feedforward network N computes a function
f : Rr →Rs as follows. The input vector is presented to the input layer at time t1
and propagated through the hidden layers to the output layer. At each time point, all
units update their input potential and activation state synchronously. At time tn, the
output vector is read off from the output layer.
In this book, we shall concentrate on single-hidden-layer networks. We do so
because of the following relevant result.
Theorem 7 ([52]). Let h : R →R be a continuous, semilinear function. Let ε ∈R+
and n ∈R. Given a continuous, real-valued function g on In = [0,1]n, there exist
a ﬁnite K and parameters α j ∈R,θ j ∈R, and yj ∈Rn(1 ≤j ≤K) such that if
f(x) = ∑K
j=1 α j ·h(y jx+θ j),x ∈In, then |f(x)−g(x)| < ε for all x ∈In.
In other words, by making α j the weight from the j-th hidden neuron to the out-
put neuron, single-hidden-layer feedforward networks can approximate any (Borel)

3.1 Architectures of Neural Networks
25
Ui(t)
Ui(t)
Ai(t)
Linear
Ai(t)
Nonlinear
Ai(t)
Semilinear
Ui(t)
θi
θi
Fig. 3.2 Activation functions
Input Vector
Output Vector
Fig. 3.3 A typical feedforward neural network
measurable function arbitrarily well, regardless of the dimension of the input
space n. A similar result was proved independently in [140]. In this sense, single-
hidden-layer networks are universal approximators of virtually any function of in-
terest (see [140]).
We use bipolar semilinear activation functions h(x) = 2/(1+e−βx) −1 and in-
puts in {−1,1}. The reason for using these instead of the standard sigmoid func-
tion shown in Fig. 3.2 is that, for standard functions with inputs in {0,1}, a zero
input value would produce no variation of the set of weights during the learning
process [35]. If we were to map the truth value false to zero in our experiments
we would have a rather ineffective learning of logical statements. Instead, we shall
map the truth value false to −1, and (as usual) the truth value true to 1. In practice,
we associate intervals (Amin,1) with true, Amin ∈R+, (−1,−Amin) with false, and
[−Amin,Amin] with unknown, as discussed later. The following systems of equations
describe the dynamics of the networks that we use:
n1 = h(W 1
11i1 +W 1
12i2 +···+W 1
1pip −θ n1),
(3.1)
n2 = h(W 1
21i1 +W 1
22i2 +···+W 1
2pip −θ n2),
...
nr = h(W 1
r1i1 +W 1
r2i2 +···+W 1
rpip −θ nr),

26
3 Artiﬁcial Neural Networks
o1 = h(W 2
11n1 +W 2
12n2 +···+W 2
1rnr −θ o1),
(3.2)
o2 = h(W 2
21n1 +W 2
22n2 +···+W 2
2rnr −θ o2),
...
oq = h(W 2
q1n1 +W 2
q2n2 +···+W 2
qrnr −θ oq),
where i = (i1,i2,...,ip) is the network’s input vector (ij(1≤j≤p) ∈{−1,1}),
o = (o1,o2,...,oq) is the network’s output vector (oj(1≤j≤q) ∈[−1,1]), n =
(n1,n2,...,nr) is the hidden-layer vector (nj(1≤j≤r) ∈[−1,1]), θ n j(1≤j≤r) is the
threshold of the j-th hidden neuron (θ n j ∈R), θ o j(1≤j≤q) is the threshold of the
j-th output neuron (θ o j ∈R), −θ n j (and −θ o j) is called the bias of the j-th hid-
den neuron (and of the j-th output neuron, respectively), W 1
i j(1≤i≤r,1≤j≤p) is the
weight of the connection from the j-th neuron in the input layer to the i-th neuron
in the hidden layer (W 1
i j ∈R), W 2
i j(1≤i≤q,1≤j≤r) is the weight of the connection
from the j-th neuron in the hidden layer to the i-th neuron in the output layer
(W 2
i j ∈R), and, ﬁnally, h(x) = 2/(1+e−βx) −1 is the standard bipolar (semilin-
ear) activation function. Notice that for each output oj(1 ≤j ≤q) in o we have
oj = h(∑r
i=1(W 2
ji.h(∑p
k=1(W 1
ik.ik)−θ ni))−θ o j).1
3.2 Learning Strategy
A neural network’s learning process (or training) is carried out by successively
changing its weights in order to approximate the function f computed by it to a
desired function g. In supervised learning, one attempts to estimate the unknown
function g from examples (input and output patterns) presented to the network. The
idea is to minimise the error associated with the set of examples by making small
changes to the network’s weights.
In the case of backpropagation [224], the learning process occurs as follows.
Given a set of input patterns ii and corresponding target vectors ti, the network’s
outputs oi = f(ii) may be compared with the targets ti, and an error such as
Err(W) = ∑
i
(oi −ti)2
(3.3)
can be computed. This error depends on the set of examples ((i,t) pairs) and may
be minimised by gradient descent, i.e. by the iterative application of changes
ΔW = −η ·∇W ·Err(W)
(3.4)
to the weight vector W.
1 Whenever it is unnecessary to differentiate between the hidden and output layers, we refer to the
weights in the network as Wi j only. Similarly, we refer to the network’s thresholds as θ i only in
general.

3.2 Learning Strategy
27
The computation of ∇W is not obvious for a network with hidden units. However,
in their famous paper ‘learning internal representations by error propagation’ [224],
Rumelhart, Hinton, and Williams presented a simple and efﬁcient way of comput-
ing such derivatives.2 They showed that a backward pass of oi −ti through the
network, analogous to the forward propagation of ii, allows the recursive compu-
tation of ∇W. The idea is that in the forward pass through the network, one should
also calculate the derivative of hk(x) for each neuron k, dk = h′
k(Uk(t)). For each
output neuron o, one simply calculates ∂o = (oi −ti)do. One can then compute
weight changes for all connections that feed into the output layer. For each con-
nection Wo j, ΔWo j = −η ·∂o·Oj(t). After this is done, ∂j = (Wo j ·∂o)·d j can be
calculated for each hidden neuron j. This propagates the errors back one layer, and
for each connection Wji from input neuron i to neuron j, ΔWji = −η ·∂j ·Oi(t). Of
course, the same process could be repeated for many hidden layers. This procedure
is called the generalised delta rule, and it is very useful because the backward pass
has the same computational complexity as the forward pass.
In the above procedure, η is called the learning rate. True gradient descent re-
quires inﬁnitesimal steps to be taken (η ≈0). On the other hand, the larger the
constant η, the larger the change in the weights, and thus a good choice of η will
lead to faster convergence. The design challenge here is how to choose the learning
rate to be as large as possible without leading to oscillation. A variation of standard
backpropagation allows the adaptation of this parameter during learning. In this
case, η is typically large at the beginning of learning, and decreases as the network
approaches a minimum of the error surface.
Backpropagation training may lead to a local rather than a global minimum of
the error surface. In an attempt to ameliorate this problem and also improve train-
ing time, a term of momentum can be added to the learning process. The term of
momentum allows a network to respond not only to the local gradient, but also to
recent trends in the error surface, acting as a low-pass ﬁlter.
Momentum is added to backpropagation by making the weight changes equal to
the sum of a fraction of the last weight change and the new change suggested by the
backpropagation rule. Equation 3.5 shows how backpropagation with momentum is
expressed mathematically:
ΔW(i) = −η ·∇W(i) ·Err(W(i))+αΔW(i−1),
(3.5)
where αΔW(i −1) is the term of momentum and 0 < α < 1 is the momentum
constant. Typically, α = 0.9.
Another difﬁculty with learning is known as the problem of symmetry. ‘If all
weights start out with equal values and if the solution requires that unequal weights
be developed, the system can never learn’ [224]. This is so because the error back-
propagated is proportional to the actual values of the weights. Symmetry breaking
is achieved by starting the system with small random weights.
2 The term “backpropagation” appears to have evolved after 1985. However, the basic idea of
backpropagation was ﬁrst described by Werbos in his PhD thesis [268].

28
3 Artiﬁcial Neural Networks
If the application at hand contains too many degrees of freedom and too few
training data, backpropagation can merely ‘memorise’ the data. This behaviour is
known as overﬁtting. The ultimate measure of success, therefore, should not be how
closely the network approximates the training data, but how well it accounts for yet
unseen cases, i.e. how well the network generalises to new data. In order to evaluate
the network’s generalisation, the set of examples is commonly partitioned into a
training set and a testing set.
Finally, it should be noted that a network’s learning capability and its activation
functions are closely related. Linear neurons possess less learning capability than
nonlinear ones, because their hidden layers act only as a multiplier of the input
vector. As a result, complex functions cannot be learned (see [53,113,235]). On the
other hand, semilinear activation functions are continuous and differentiable, which
is an important property for the use of backpropagation.
Summarising, there are three major issues one must address when applying
neural networks in a given domain: the representation problem (model, architecture,
and size), the learning problem (time and training method), and the generalisation
problem (performance).
As an example, consider the network of Fig. 3.4. This network was trained using
input values {−1,1}, f(x) = x as the activation function of neurons a and b, and
tanh(x) = (e2x −1)/(e2x +1), e = 2.718, as the activation function of neurons h,k,
and o. In the ﬁgure, 1.5 is the bias of neuron o, 2.0 is the bias of h, and 1.0 is the
bias of k, where bias = −threshold. Consider the training example (a = 0.5,b =
−1.0,t = −0.5), where t is the target output for neuron o. We need to compute a
weight change (ΔW) for each of the network’s weights (including the biases). First,
we compute the output of the network for the input vector (a = 0.5, b = −1.0). Let
us use a learning rate η = 0.1. Remember that tanh′(x) = 1−tanh2(x) and ΔWi j =
−η.∂i.o j, where ∂i is the local gradient of neuron i, and oj is the output of neuron j.
Let Act(x) denote the activation state of neuron x. We have Act(h) = tanh(2a −
4b + 2), Act(k) = tanh(2.5a + 3.5b + 1), and Act(o) = tanh(3h + k + 1.5). For
1.5
1.0
2.0
1.0
3.0
k
o
h
2.0
3.5
2.5
−4.0
a
b
Fig. 3.4 A set of weights for the backpropagation example considered in the text

3.3 Recurrent Networks
29
a = 0.5,b = −1, we thus obtain activation values Act(h) = 0.99, Act(k) = −0.85,
and then Act(o) = 0.99. We also need to calculate the derivatives tanh′(x) of
the activation values of each hidden and output neuron. We obtain tanh′(h) =
0.0199, tanh′(k) = 0.2775, and tanh′(o) = 0.0199. We then compute the error
e = o−t = 1.49.
The weight change ΔWoh for the weight between neuron h and the output neuron
o is given by −η · ∂o · h, where ∂o = e· tanh′(o) = 0.02965. Thus, ΔWoh = −0.1 ·
0.02965·0.99. Similarly, ΔWok = −0.1·0.02965·−0.85.
For hidden neurons h and k, ∂h = tanh′(h) · ∂o ·Who = 0.0199 · 0.02965 · 3.0 =
0.00177 and ∂k = tanh′(k) · ∂o ·Wko = 0.2775 · 0.02965 · 1.0 = 0.00823. Then, the
weight change ΔWha for the weight between input neuron a and hidden neuron h is
given by −η · ∂h · a = −0.1 · 0.00177 · 0.5. Similarly, the weight change ΔWhb for
the weight between input neuron b and hidden neuron h is given by −η · ∂h · b =
−0.1·0.00177·−1.0. Further, the weight change ΔWka for the weight between input
neuron a and hidden neuron k is given by −η ·∂k ·a = −0.1·0.00823·0.5, and the
weight change ΔWkb for the weight between input neuron b and hidden neuron k is
given by −η ·∂k·b = −0.1·0.00823·−1.0. The calculations for the variations Δθ h,
Δθ k, Δθ o of the biases are identical to those for the hidden neurons, but using an
input 1.0.
3.3 Recurrent Networks
A limitation of feedforward neural networks when compared with symbolic
computation is their inability to deal with time or artiﬁcial-intelligence applica-
tions endowed with a temporal dimension. Recurrent neural networks deal with this
problem. A simple form of recurrent network will be used in this book, allowing for
the computation of recursive functions. As we shall see, although it is recurrent, the
network will be such that standard backpropagation can be used. In this section, we
recall recurrent networks as deﬁned by Jordan [145] and Elman [85].
A recurrent architecture is needed for a neural network to be able to generalise
temporal sequences. In other words, the network is required to have cycles such as
the ones shown in Fig. 3.5. The backpropagation algorithm has been extended to
deal with such networks by performing the propagation of the error in the direction
opposite to the original connections. As expected, the main difﬁculty of recurrent
backpropagation is in how to deal with chaotic behaviour resulting from the cycles.
This may require some form of synchronisation and may result in complications
similar to those found in the training of symmetric networks [139]. We shall not
use recurrent backpropagation in this book, but it might be an interesting future
exercise to compare the results of such learning algorithms and backpropagation in
the context of neural-symbolic systems. The details of the algorithms can be found
in [127].
A simpler way of generalising temporal sequences in neural networks is to
somehow control recurrency by carefully deﬁning the feedback connections us-
ing partially recurrent networks. In such networks, most of the connections are

30
3 Artiﬁcial Neural Networks
Fig. 3.5 A recurrent network
Input
Context
hidden
Output
Input
Context
hidden
Output
Context
hidden
Output
(a)
(b)
(c)
Fig. 3.6 Architectures of partially recurrent networks
feedforward, and only certain feedback connections are allowed. The idea of this is
to obtain the expressive power needed for generalising temporal sequences, without
the extra complexity of learning in an underconstrained architecture. In most cases,
the feedback connections are also given ﬁxed weights (i.e. no learning takes place
in the feedback connections). This allows the standard backpropagation algorithm
to be used without changes.
Figure 3.6 shows some typical architectures of partially recurrent networks. In
each case, there is always a special set of neurons – called context units – that re-
ceive the feedback signals in a layer or part of a layer. The feedback signals need to
be synchronised so that, at time t, the activations computed in a feedforward manner
by the network at time t −1 can reach the context units, deﬁning the network’s cur-
rent context. The other signals in the network do not need to be synchronised. One
can see the context units as remembering some aspect of the network’s computation
in the recent past. In this way, at each time point, the network’s output may now
depend upon the network’s current input and also upon its context (e.g. a previous
network output). In Fig. 3.6(a), ﬁrst proposed in [145], the input layer contains the

3.4 Evaluation of Learning Models
31
neurons that receive values from the outside and the neurons that create the context.
The latter simply record, at time t, the activation states of the neurons in the out-
put layer at time t −1. The connections that might have their weights changed by
learning are all feedforward. Hence, backpropagation can be applied, with the train-
ing examples being presented to the network at each time interval t and with the
context units being treated as input units for training purposes. Figure 3.6(b) shows
the architecture proposed by Elman [85], which differs from that of Fig. 3.6(a) by
having the activation of the hidden neurons feeding the context units. Here, it is the
internal structure and behaviour of the network that deﬁne its context. Since it is that
internal structure and behaviour that deﬁne the output of the network, similar results
may be obtained either way, by having either the output or the hidden layer feed the
context. Ultimately, the choice may depend on the application. Figure 3.6(c) shows
an architecture where feedback is allowed from the context units themselves, from
the hidden layer, and from the output layer. In this setting, the network combines, in
its context units, any input signals with the feedback signals, into a weighted-sum
input value that is propagated forward in the normal way.
In what follows, we shall use a slight variation of Jordan’s network together with
standard backpropagation. We shall also use groups of such networks, which we
shall refer to loosely as network ensembles. As we shall see, this will be suﬁcient
for most of our purposes in relation to knowledge representation and reasoning. We
shall also consider brieﬂy a kind of recursive network in a setting we call network
ﬁbring [67]. Networks of this kind are more powerful than the networks above, and
may prove to be a more adequate architecture for fully integrated reasoning and
learning, and for ﬁrst-order-logic reasoning. Differently from recurrent networks,
the neurons in a recursive network may be networks in their own right, and in this
structure, a computation in one network may trigger a learning process in another
network in the ensemble. But these are features that we shall introduce as we go
along and as they are needed in the text.
All the networks described in this book were trained using standard backprop-
agation with momentum, the most commonly used and generally most successful
learning algorithm. The reader is referred to [47,224] for the backpropagation algo-
rithm and its variations and improvements, to [161] for a discussion of the problem
of overﬁtting, and to [145,151] for details of recurrent networks.
3.4 Evaluation of Learning Models
Learning, one of the basic attributes of intelligence, can be deﬁned as a change of
behaviour motivated by changes in the environment in order to perform better in dif-
ferent knowledge domains [182]. Learning strategies can be classiﬁed into: learning
from instruction, learning by deduction, learning by analogy, learning from exam-
ples, and learning by observation and discovery; the latter two are forms of inductive
learning. Reference [225] is a good introductory text on machine learning, and [184]
contains some new hybrid models of learning, in addition to the traditional, purely
symbolic paradigm.

32
3 Artiﬁcial Neural Networks
The task of inductive learning is to ﬁnd hypotheses that are consistent with back-
ground knowledge to explain a given set of examples. In general, these hypotheses
are deﬁnitions of concepts described in some logical language, the examples are de-
scriptions of instances and noninstances of the concept to be learned, and the back-
ground knowledge gives additional information about the examples and the domain
knowledge related to the concepts (see [159]). A general formulation of inductive
learning is as follows: given background knowledge or an initial set of beliefs (B)
and a set of positive (e+) and negative (e−) examples of some concept, ﬁnd a hy-
pothesis (h) in some knowledge representation language (L) such that B ∪h ⊢L e+
and B ∪h ⊬L e−.
Studies of inductive learning date back to the 1960s (see [142]). One of the most
successful areas is called case-based reasoning (CBR), which is best instantiated in
Quinlan’s ID3 algorithm [216] and Michalski et al.’s [183]. More recently, the im-
portance of adding background knowledge to help in the learning process, depend-
ing on the application at hand, has been highlighted (see e.g. [72,131,201]), and the
area of inductive logic programming (ILP) has ﬂourished (a comprehensive intro-
ductory survey of ILP can be found in [189]). In [159], CBR and ILP were integrated
by means of the LINUS system, which translates a fragment of ﬁrst-order logic into
attribute-value examples in order to apply, for instance, a decision-tree-generation
learning algorithm such as ID3. The result of learning can then be translated back
onto the original language.
If a learning algorithm fails to extract any pattern from a set of examples, one
cannot expect it to be able to extrapolate (or generalise) to examples it has not seen.
Finding a pattern means being able to describe a large number of cases in a con-
cise way. A general principle of inductive learning, often called Ockham’s razor,3 is
thus as follows: the most likely hypothesis is the simplest one that is consistent with
all observations. Unfortunately, ﬁnding the smallest representation (e.g. the small-
est decision tree) is an intractable problem, and usually the use of some heuristics
is needed. In [110], some conditions for the applicability of Ockham’s razor were
discussed. In a more general perspective, [253, 263] discuss the general theory of
learning.
We normally evaluate the performance of a learning system with the use of cross-
validation, a testing methodology in which the set of examples is permuted and
divided into n sets. One division is used for testing and the remaining n−1 divisions
are used for training. The testing division is never seen by the learning algorithm
during the training process. The procedure is repeated n times so that every division
is used once for testing. We call the average classiﬁcation rate of the learning system
over the test sets its test set performance. We call one pass through the training set a
learning epoch. For a predeﬁned number of epochs, we call the average classiﬁcation
rate of the system over the training sets its training-set performance.
For example, in [66] we experimented with two real-world problems in the do-
main of DNA sequence analysis: the promoter recognition4 and the splice-junction
3 Also spelled Occam’s razor.
4 Promoters are short DNA sequences that precede the beginning of genes.

3.5 Discussion
33
determination5 problems [191]. For the ﬁrst problem, there were 106 examples
available (53 promoter and 53 nonpromoter sequences), and we used leaving-one-
out cross-validation, in which each example is successively left out of the training
set. Hence, this required 106 training networks in which the training set had 105 ex-
amples and the testing set had one example. Leaving-one-out becomes computation-
ally expensive as the number of available examples grows. For the splice-junction
problem, for example, there were 3190 examples available, and therefore, follow-
ing [191], we selected 1000 examples randomly from the original set, to which we
applied a 10-fold cross-validation process (i.e. 10 networks were created, each using
900 examples for training and 100 examples for testing). Cross-validation is primar-
ily a statistical evaluation methodology. It can also be used, for example, for model
selection (i.e. to estimate the number of hidden neurons that a network should have
in a given application). Alternatively, one may create a model that includes all the
networks used in the evaluation (e.g. by taking as the “network output” the average
of the outputs of all the 10 networks used for splice-junction determination).
3.5 Discussion
This chapter has introduced the basic concepts of neural networks used in this book.
As mentioned above, neural networks have a long history in computer science and
artiﬁcial intelligence, with applications in the engineering, physical, natural, and
social sciences. Our interest in neural networks is based not only on their learning
capabilities, but also on their computational power. If one aims to build sound (cog-
nitive) computational models, several aspects of cognition must be investigated. In
the case of connectionist models, these include explanation capabilities, knowledge
representation, and expressivity. The coming chapters will analyse these in the con-
text of neural-symbolic models.
5 Splice junctions are points on a DNA sequence at which the noncoding regions are removed
during the process of protein synthesis.

Chapter 4
Neural-Symbolic Learning Systems
This chapter introduces the basics of neural-symbolic systems used thoughout the
book. A brief bibliographical review is also presented. Neural-symbolic systems
have become a very active area of research in the last decade. The integration of
neural networks and symbolic knowledge was already receiving considerable atten-
tion in the 1990s. For instance, in [250], Towell and Shavlik presented the inﬂuential
model KBANN (Knowledge-Based Artiﬁcial Neural Network), a system for rule
insertion, reﬁnement, and extraction from neural networks. They also showed em-
pirically that knowledge-based neural networks, trained using the backpropagation
learning algorithm (see Sect. 3.2), provided a very efﬁcient way of learning from ex-
amples and background knowledge. They did so by comparing the performance of
KBANN with other hybrid, neural, and purely symbolic inductive learning systems
(see [159,189] for a comprehensive description of a number of symbolic inductive
learning systems, including inductive logic programming systems).
Brieﬂy, the rules-to-network algorithm of KBANN builds AND/OR trees. Firstly,
it creates a hierarchy in the set of rules and rewrites certain rules in order to eliminate
disjunctswithmorethanoneterm.Inthisprocess,asetofrulesR = {c∧d →a,d∧e →
a,a∧¬f →b} becomes R′ = {c∧d →a′,a′ →a,d ∧e →a′′,a′′ →a,a∧¬f →b}.
Then, KBANN sets weights and thresholds such that the network behaves as a set
of AND/OR neurons. Figure 4.1 shows a KBANN network derived from a set of
rules R′.
KBANN served as an inspiration for the construction of the Connectionist In-
ductive Learning and Logic Programming (CILP) system [66]. CILP builds upon
KBANN so as to provide a sound theoretical foundation for reasoning in artiﬁcial
neural networks, and uses logic programming as a knowledge representation lan-
guage. In the following, we present the foundations of CILP and some illustrative
examples that will prove useful in the rest of the book.
4.1 The CILP System
CILP [66, 80] is a massively parallel computational model based on a feedfor-
ward artiﬁcial neural network that integrates inductive learning from examples
and background knowledge with deductive reasoning using logic programming.
A.S. d’Avila Garcez et al., Neural-Symbolic Cognitive Reasoning, Cognitive Technologies,
35
c⃝Springer-Verlag Berlin Heidelberg 2009

36
4 Neural-Symbolic Learning Systems
c
e
f
a’
b
d
a”
a
Fig. 4.1 A KBANN network. a′,a′′, and b are AND neurons, and a is an OR neuron. Each neuron
has a semilinear activation function
A Translation Algorithm maps a general logic program1 P into a single-hidden-
layer neural network N such that N computes the least ﬁxed-point of P. In addition,
N can be trained by examples using backpropagation [224] (the learning algorithm
most successfully applied in real-world problems, e.g. in bioinformatics and pattern
recognition [14, 26, 27]), treating P as background knowledge. The knowledge ac-
quired by training can then be extracted [65], closing the learning cycle, as was done
in [250] (see Fig. 4.2).
A typical application of CILP’s explanation module is in safety-critical domains,
such as fault diagnosis systems, where the neural network can detect a fault quickly,
triggering safety procedures, while the knowledge extracted from it can justify the
fault later on. If the diagnosis is mistaken, the information can be used to ﬁne-tune
the learning system.
In a nutshell, the CILP system can be divided into three parts (Fig. 4.2):
• knowledge reﬁnement in a neural network: phases 1, 2, and 3 of CILP;
• knowledge extraction from a trained network: phase 5 of CILP;
• theory revision in a neural network: phase 4 of CILP.
The ﬁrst part of CILP is based on H¨olldobler and Kalinke’s work [135, 136]
and on that of Towell and Shavlik [231, 249, 250]. We chose the above approaches
carefully, because H¨olldobler and Kalinke used a neat, simple model of neural
networks to compute one of the standard semantics of logic programming, and
Towell and Shavlik’s KBANN and its subsequent developments (e.g. [169,195,196])
1 Recall that a general clause is a rule of the form L1,...,Lk →A, where A is an atom and Li
(1 ≤i ≤k) is a literal (an atom or the negation of an atom). A general logic program is a ﬁnite set
of general clauses [163].

4.1 The CILP System
37
Symbolic
Knowledge
Symbolic
Knowledge
Neural
Network
Examples
Learning
Connectionist System
Inference
Machine
Explanation
1
2
3
5
6
4
Fig. 4.2 Neural-symbolic integration
have been empirically shown to be superior to some of the main neural, symbolic,
and hybrid learning systems, and have provided one of the most effective hybrid
systems to date.
These important contributions are detailed in [66], where extensive analyses of
their capabilities for knowledge representation and learning were reported. Theo-
retical results about CILP regarding phases 1, 2, and 3 of the system were also
extensively studied in [66]. CILP’s efﬁciency as a hybrid learning system and its
applications to real-world problems compared with some of the main competitors,
including KBANN, were also studied in [66]. The results show that CILP networks
can be very effective in a number of domains.
Let us exemplify how CILP’s Translation Algorithm works. Each clause (rl) of
P is mapped from the input layer to the output layer of N through one neuron (Nl)
in the single hidden layer of N. Intuitively, the Translation Algorithm from P to
N has to implement the following conditions: (C1) the input potential of a hidden
neuron (Nl) can only exceed Nl’s threshold (θ l), activating Nl, when all the positive
antecedents of rl are assigned the truth value true while all the negative antecedents
of rl are assigned false; and (C2) the input potential of an output neuron (A) can
only exceed A’s threshold (θ A), activating A, when at least one hidden neuron Nl
that is connected to A is activated.
Example 1. Consider the logic program P = {B; B∧C∧∼D →A;E ∧F →A}. The
Translation Algorithm derives the network N of Fig. 4.3, setting weights (W ′) and
thresholds (θ ′) in such a way that the conditions (C1) and (C2) above are satisﬁed.
Note that if N ought to be fully connected, any other link (not shown in Fig. 4.3)
should receive weight zero initially.
Notice that, in Example 1, following [135], each input and output neuron of N
is associated with an atom of P. As a result, each input and output vector of N can
be associated with an interpretation for P. Notice also that each hidden neuron Nl
corresponds to a clause rl of P. In order to compute a ﬁxed-point semantics of P,

38
4 Neural-Symbolic Learning Systems
A
B
θA
θB
W
W
W
θ1
N1
θ2
N2
θ3
N3
F
B
E
D
C
W
W
W
−W
W
Interpretations
Fig. 4.3 Sketch of a neural network for the logic program P in Example 1
output neuron B should feed input neuron B such that N is used to iterate TP, the
ﬁxed-point operator2 of P. N will eventually converge to a stable state which is
identical to the stable model of P.
Notation
Given a general logic program P, let:
q denote the number of clauses rl (1 ≤l ≤q) occurring in P;
υ denote the number of literals occurring in P;
Amin denote the minimum activation for a neuron to be active (or, analogously,
for its associated literal to be assigned a truth value true), Amin ∈(0,1);
Amax denote the maximum activation when a neuron is not active (or when its
associated literal is false), Amax ∈(−1,0);
h(x) = 2/(1+e−βx)−1, the bipolar semilinear activation function;3
g(x) = x, the standard linear activation function;
s(x) = y, the standard nonlinear activation function (y = 1 if x > 0, and y = 0
otherwise), also known as the step function;
W and −W denote the weights of connections associated with positive and neg-
ative literals, respectively;
θ l denote the threshold of the hidden neuron Nl associated with clause rl;
θ A denote the threshold of the output neuron A, where A is the head of clause rl;
kl denote the number of literals in the body of clause rl;
2 Recall that the mapping TP is deﬁned as follows. Let I be a Herbrand interpretation; then TP(I) =
{A0 | L1,...,Ln →A0 is a ground clause in P and {L1,...,Ln} ⊆I}.
3 We use the bipolar semilinear activation function for convenience (see Sect. 3.1). Any monoton-
ically increasing activation function could have been used here.

4.1 The CILP System
39
pl denote the number of positive literals in the body of clause rl;
nl denote the number of negative literals in the body of clause rl;
μl denote the number of clauses in P with the same atom in the head, for each
clause rl;
MAXrl(kl,μl) denote the greater element of kl and μl for clause rl; and
MAXP(k1,...,kq,μ1,...,μq) denote the greatest element of all k’s and μ’s of P.
We also use −→k as a shorthand for (k1,...,kq), and −→
μ as a shorthand for (μ1,...,μq).
For instance, for the program P of Example 1, q = 3, υ = 6, k1 =3,
k2 = 2, k3 = 0, p1 = 2, p2 = 2, p3 = 0, n1 = 1, n2 = 0, n3 = 0, μ1 = 2,
μ2 = 2, μ3 = 1, MAXr1(k1,μ1) = 3, MAXr2(k2,μ2) = 2, MAXr3(k3,μ3) = 1, and
MAXP(k1,k2,k3,μ1,μ2,μ3) = 3.
In the Translation Algorithm below, we deﬁne Amin, W, θ l, and θ A such that the
conditions (C1) and (C2) above are satisﬁed. Equations 4.1, 4.2, 4.3, and 4.4 below
are obtained from the proof of Theorem 8 [80]. We assume, for mathematical conve-
nience and without loss of generality, that Amax = −Amin. In this way, we associate
the truth value true with values in the interval (Amin,1), and the truth value false with
values in the interval (−1,−Amin).
Theorem 8 guarantees that values in the interval [−Amin,Amin] do not occur in
the network with weights W and thresholds θ, but, informally, this interval may be
associated with a third truth value unknown4. The proof of Theorem 8 presented
in [66] is reproduced here for the sake of completeness, as several proofs presented
in the book will make reference to it.
We start by calculating MAXP(−→k ,−→
μ ) such that
Amin > MAXP(−→
k,−→
μ )−1
MAXP(−→
k,−→
μ )+1
.
(4.1)
CILP Translation Algorithm
1. Calculate the value of W such that the following is satisﬁed:
W ≥2
β ·
ln(1+Amin)−ln(1−Amin)
MAXP(−→
k,−→
μ )(Amin −1)+Amin +1
.
(4.2)
2. For each clause rl of P of the form L1,...,Lk →A (k ≥0):
(a) Create input neurons L1,...,Lk and an output neuron A in N (if they do not
exist yet).
(b) Add a neuron Nl to the hidden layer of N.
4 If a network obtained by the Translation Algorithm is then trained by examples with the use
of a learning algorithm that does not impose any constraints on the weights, values in the inter-
val [−Amin,Amin] may occur and should be interpreted as unknown by following a three-valued
interpretation.

40
4 Neural-Symbolic Learning Systems
(c) Connect each neuron Li (1 ≤i ≤k) in the input layer to the neuron Nl in the
hidden layer. If Li is a positive literal, then set the connection weight to W;
otherwise, set the connection weight to −W.
(d) Connect the neuron Nl in the hidden layer to the neuron A in the output layer
and set the connection weight to W.
(e) Deﬁne the threshold (θ l) of the neuron Nl in the hidden layer as
θ l = (1+Amin)(kl −1)
2
W.
(4.3)
a. Deﬁne the threshold (θ A) of the neuron A in the output layer as
θ A = (1+Amin)(1−μl)
2
W.
(4.4)
3. Set g(x) as the activation function of the neurons in the input layer of N. In this
way, the activation of the neurons in the input layer of N given by each input
vector i will represent an interpretation for P.
4. Set h(x) as the activation function of the neurons in the hidden and output layers
of N. In this way, a gradient descent learning algorithm, such as backpropaga-
tion, can be applied to N.
5. If N needs to be fully connected, set all other connections to zero.
Theorem 8. [66] For each propositional general logic program P, there exists a
feedforward artiﬁcial neural network N with exactly one hidden layer and semilin-
ear neurons such that N computes the ﬁxed-point operator TP of P.
Proof. (←) ‘A ≥Amin if L1,...,Lk is satisﬁed by i’. Assume that the pl positive
literals in L1,...,Lk are true, and the nl negative literals in L1,...,Lk are false.
Consider the mapping from the input layer to the hidden layer of N. The input
potential (Il) of Nl is minimum when all the neurons associated with a positive
literal in L1,...,Lk are at Amin, while all the neurons associated with a negative
literal in L1,...,Lk are at −Amin. Thus, Il ≥plAminW +nlAminW −θ l and, assuming
θ l = ((1+Amin)(kl −1)/2)W, Il ≥plAminW +nlAminW −((1+Amin)(kl −1)/2)W.
If h(Il) ≥Amin, i.e. Il ≥−1
β ln((1−Amin)/(1+Amin)), then Nl is active. There-
fore, Equation 4.5 must be satisﬁed:5
plAminW +nlAminW −(1+Amin)(kl −1)
2
W ≥
−1
β ln
1−Amin
1+Amin

.
(4.5)
Solving Equation 4.5 for the connection weight (W) yields Equations 4.6 and
4.7, given that W > 0:
5 Throughout, we use the word ‘Equation’, as in ‘Equation 4.5’, even though ‘Equation 4.5’ is an
inequality.

4.1 The CILP System
41
W ≥−2
β · ln(1−Amin)−ln(1+Amin)
kl (Amin −1)+Amin +1
,
(4.6)
Amin > kl −1
kl +1.
(4.7)
Consider now the mapping from the hidden layer to the output layer of N. By
Equations 4.6 and 4.7, at least one neuron Nl that is connected to A is ‘active’.
The input potential (Il) of A is minimum when Nl is at Amin, while the other μl −1
neurons connected to A are at −1. Thus, Il ≥AminW −(μl −1)W −θ l and, assuming
θ l = (1+Amin)(1−μl)/2W, Il ≥AminW −(μl −1)W −(1+Amin)(1−μl)/2W.
If h(Il) ≥Amin, i.e. Il ≥−1
β ln((1−Amin)/(1+Amin)), then A is active. Therefore,
Equation 4.8 must be satisﬁed:
AminW −(μl −1)W −(1+Amin)(1−μl)
2
W ≥
−1
β ln
1−Amin
1+Amin

.
(4.8)
Solving Equation 4.8 for the connection weight W yields Equations 4.9 and 4.10,
given that W > 0:
W ≥−2
β · ln(1−Amin)−ln(1+Amin)
μl (Amin −1)+Amin +1
,
(4.9)
Amin > μl −1
μl +1.
(4.10)
(→) ‘A ≤−Amin if L1,...,Lk is not satisﬁed by i’. Assume that at least one of the
pl positive literals in L1,...,Lk is false or one of the nl negative literals in L1,...,Lk
is true. Consider the mapping from the input layer to the hidden layer of N. The in-
put potential (Il) of Nl is maximum when only one neuron associated with a positive
literal in L1,...,Lk is at −Amin or when only one neuron associated with a negative
literal in L1,...,Lk is at Amin. Thus, either Il ≤(pl −1)W −AminW + nlW −θ l or
Il ≤(nl −1)W −AminW + plW −θ l and, assuming θ l = ((1+Amin)(kl −1)/2)W,
Il ≤(kl −1−Amin)W −((1+Amin)(kl −1)/2)W.
If −Amin ≥h(Il), i.e. −Amin ≥2/(1+e−β(Il)) −1, then Il ≤−(1/β)ln
((1+Amin)/(1−Amin)), and so Nl is not active. Therefore, Equation 4.11 must
be satisﬁed:
(kl −1−Amin)W −(1+Amin)(kl −1)
2
W ≤
−1
β ln
1−Amin
1+Amin

.
(4.11)

42
4 Neural-Symbolic Learning Systems
Solving Equation 4.11 for the connection weight W yields Equations 4.12 and
4.13, given that W > 0:
W ≥2
β · ln(1+Amin)−ln(1−Amin)
kl (Amin −1)+Amin +1
,
(4.12)
Amin > kl −1
kl +1.
(4.13)
Consider now the mapping from the hidden layer to the output layer of N. By
Equations 4.12 and 4.13, all neurons Nl that are connected to A are ‘not active’.
The input potential (Il) of A is maximum when all the neurons connected to A are
at −Amin. Thus, Il ≤−μlAminW −θ l and, assuming θ l = ((1+Amin)(1−μl)/2)W,
Il ≤−μlAminW −((1+Amin)(1−μl)/2)W.
If −Amin ≥h(Il), i.e. −Amin ≥2/(1+e−β(Il)) −1, then Il ≤−(1/β)ln
((1+Amin)/(1−Amin)), and so A is not active. Therefore, Equation 4.14 must
be satisﬁed:
−μlAminW −(1+Amin)(1−μl)
2
W ≤
−1
β ln
1+Amin
1−Amin

.
(4.14)
Solving Equation 4.14 for the connection weight W yields Equations 4.15 and
4.16, given that W > 0:
W ≥2
β · ln(1+Amin)−ln(1−Amin)
μl (Amin −1)+Amin +1
,
(4.15)
Amin > μl −1
μl +1.
(4.16)
Notice that Equations 4.6 and 4.9 are equivalent to Equations 4.12 and 4.15,
respectively. Hence, the theorem holds if, for each clause Cl in P, Equations 4.6
and 4.7 are satisﬁed by W and Amin from the input layer to the hidden layer of N,
and Equations 4.9 and 4.10 are satisﬁed by W and Amin from the hidden layer to the
output layer of N.
In order to unify the weights of N for each clause Cl of P, given the deﬁnition
of MAXCl(kl,μl), it is sufﬁcient that Equations 4.17 and 4.18 below are satisﬁed by
W and Amin, respectively:
W ≥2
β ·
ln(1+Amin)−ln(1−Amin)
MAXCl(kl,μl)(Amin −1)+Amin +1,
(4.17)
Amin > MAXCl(kl,μl)−1
MAXCl(kl,μl)+1.
(4.18)

4.2 Massively Parallel Deduction in CILP
43
Finally, in order to unify all the weights of N for a program P, given the deﬁ-
nition of MAXP(k1,...,kq,μ1,...,μq), it is sufﬁcient that Equations 4.19 and 4.20
are satisﬁed by W and Amin, respectively:
W ≥2
β
ln(1+Amin)−ln(1−Amin)
MAXP(k1,...,kq,μ1,...,μq)(Amin−1)+Amin+1,
(4.19)
Amin >
MAXP(k1,...,kq,μ1,...,μq)−1
MAXP(k1,...,kq,μ1,...,μq)+1.
(4.20)
As a result, if Equations 4.19 and 4.20 are satisﬁed by W and Amin, respectively,
then N computes TP. This is indeed the case in CILP’s Translation Algorithm.
■
4.2 Massively Parallel Deduction in CILP
Recall that T n
P
de f
= TP(T n-1
P ), with T 0
P
def
= TP({∅}). We say that P is well behaved
if, after a ﬁnite number m of iterations, T m
P = T m-1
P
. It is not difﬁcult to see that
if P is well behaved and we use N to iterate TP, then N will converge to T m
P , as
follows. Consider a feedforward neural network N with p input neurons (i1,...,ip)
and q output neurons (o1,...,oq). Assume that each input and output neuron in N is
labelled by an atom Ak associated with it. Let us use name(ii) = name(oj) to denote
the fact that the literal associated with neuron ii is the same as the literal associated
with neuron oj.
Let
valuation(Act(x)) =
 1, if Act(x) > Amin,
−1, otherwise,
where Act(x) is the activation state of neuron x.
We say that the computation of P by N terminates when valuation(Act(ii)) =
valuation(Act(oj)) for every pair of neurons (ii,oj) in N such that name(ii) =
name(oj).
Now, from Theorem 8 and the deﬁnition of T n
P above, it is clear that, starting
from {∅} (i.e. i = (i1,...,ip) = [−1,−1,...,−1]), if P is well behaved, then the
computation of P by N terminates. The computation is as follows (below, we use
o = N(i) to denote the output vector o = (o1,...,oq) obtained by presenting the
input vector i to the network N):
1. Let i = [−1,−1,...,−1];
2. Repeat:
(a) Calculate o = N(i);
(b) For each o j in o, do: If name(o j) = name(ii) Then replace
the value of ii in i by valuation(Act(oj));

44
4 Neural-Symbolic Learning Systems
3. Until valuation(Act(oj))
=
valuation(Act(ii)) for all (ii,oj) such
that
name(ii) = name(oj).
The set name(x) ⊆BP of input and output neurons x in N for which
valuation(Act(x)) = 1 will denote T m
P . When it is clear from the context, we
may write neuron Ak to indicate the neuron in N associated with atom Ak in P.
In [135], H¨olldobler and Kalinke showed that autoassociative networks with a
nonlinear activation function can be used for the computation of TP. Theorem 8
generalises their result to heteroassociative networks with semilinear neurons (i.e.
networks that can use backpropagation).
Example 2. (Example 1 continued) To construct the network of Fig. 4.3, ﬁrstly we
calculate MAXP(−→k ,−→
μ ) = 3 and Amin > 0.5. Then, θ 1 = (1 + Amin)W, θ 2 = (1 +
Amin)W/2, θ 3 = −(1+Amin)W/2, θ A = −(1+Amin)W/2, and θ B = 0. Now, sup-
pose Amin = 0.6; we obtain W ≥6.931/β. Alternatively, suppose Amin = 0.7; then
W ≥4.336/β. Let us take Amin = 0.7 and take h(x) as the standard bipolar semilin-
ear activation function (β = 1). Then, if W = 4.5,6 N will compute the operator TP
of P. The computation of P by N terminates with T m
P = {B} when m = 2.
4.3 Inductive Learning in CILP
One of the main features of artiﬁcial neural networks is their learning capability. The
program P, viewed as background knowledge, may now be reﬁned with examples
in a learning process in N.
Hornik, Stinchcombe, and White [140] and Cybenco [52] have proved that stan-
dard feedforward neural networks with as few as one hidden layer can approximate
any (Borel measurable) function from one ﬁnite-dimensional space to another, to
any desired degree of accuracy, provided sufﬁciently many hidden units are avail-
able. Hence, we can train single-hidden-layer neural networks to approximate the
operator TP associated with a logic program P. Powerful neural learning algorithms
have been established theoretically and applied extensively in practice. These algo-
rithms may be used to learn the operator TP′ of a previously unknown program P′,
and therefore to learn the program P′ itself. Moreover, DasGupta and Schnitger [53]
have proved that neural networks with continuously differentiable activation func-
tions are capable of computing a family of n-ary Boolean functions using two neu-
rons (i.e. constant size), whereas networks composed of binary threshold functions
require size of at least O(log(n)) neurons. Hence, analogue neural networks have
more computational power than discrete neural networks, even when computing
Boolean functions.
The network’s recurrent connections contain ﬁxed weights Wr = 1, with the sole
purpose of ensuring that the output feeds the input in the next learning or recall
6 Note that a sound translation from P to N does not require all the weights in N to have the same
absolute value. We have uniﬁed the weights (|W|) for the sake of simplicity.

4.4 Adding Classical Negation
45
phase. Since the network does not learn in its recurrent connections,7 the stan-
dard backpropagation algorithm can be applied directly [127] (see also [125,145]).
Hence, in order to perform inductive learning with examples on N, four simple
steps should be followed: (i) add neurons to the input and output layers of N, ac-
cording to the training set (the training set may contain concepts not represented in
the background knowledge, and vice versa); (ii) add neurons to the hidden layer of
N, if it is so required for the convergence of the learning algorithm; (iii) add con-
nections with weight zero, in which N will learn new concepts; and (iv) perturb the
connections by adding small random numbers to its weights in order to avoid the
problem of symmetry.8
4.4 Adding Classical Negation
According to Lifschitz and McCarthy, commonsense knowledge can be represented
more easily when classical negation (¬), sometimes called explicit negation, is
available. In [116], Gelfond and Lifschitz extended the notion of stable models to
programs with classical negation. Extended logic programs can be viewed as a frag-
ment of default theories (see [171]), and thus are of interest with respect to the
relation between logic programming and nonmonotonic formalisms. In this section,
we extend CILP to incorporate classical negation. The extended CILP system com-
putes the answer set semantics [116] of extended logic programs. As a result, it can
be applied in a broader range of domains.
General logic programs provide negative information implicitly, by the closed-
world assumption, whereas extended programs include explicit negation, allowing
the presence of incomplete information in the database. ‘In the language of extended
programs, we can distinguish between a query which fails in the sense that it does
not succeed, and a query which fails in the stronger sense that its negation succeeds’
[116]. The following example, due to John McCarthy, illustrates such a difference: a
school bus may cross railway tracks unless there is an approaching train. This would
be expressed in a general logic program by the rule cross ←∼train, in which case
the absence of train in the database is interpreted as the absence of an approaching
train, i.e. using the closed-world assumption. Such an assumption is unacceptable if
one reasons with incomplete information. However, if we use classical negation and
represent the above knowledge as the extended program cross ←¬train, then cross
will not be derived until the fact ¬train is added to the database.
Therefore, it is essential to differentiate between ¬A and ∼A in a logic pro-
gram whenever the closed-world assumption is not applicable to A. Nevertheless, the
closed-world assumption can be explicitly included in extended programs by adding
rules of the form ¬A ←∼A whenever the information about A in the database is
7 The recurrent connections represent an external process between the output and input of the
network for neurons having the same name, for example neuron B in Fig. 4.3.
8 The perturbation should be small enough not to have an adverse effect on the computation of the
background knowledge.

46
4 Neural-Symbolic Learning Systems
assumed to be complete. Moreover, for some literals, the opposite assumption A ←
∼¬A may be appropriate.
The semantics of extended programs, called the answer set semantics, is an ex-
tension of the stable-model semantics for general logic programs. “A ‘well-behaved’
general program has exactly one stable model, and the answer that it returns for a
ground query (A) is yes or no, depending on whether A belongs or not to the stable
model of the program. A ‘well-behaved’ extended program has exactly one answer
set, and this set is consistent. The answer that an extended program returns for a
ground query (A) is yes, no or unknown, depending on whether its answer set con-
tains A, ¬A or neither” [116]. If a program does not contain classical negation, then
its answer sets are exactly the same as its stable models.
Deﬁnition 21. [116] An extended logic program is a ﬁnite set of clauses of the form
L0 ←L1,...,Lm,∼Lm+1,...,∼Ln, where Li (0 ≤i ≤n) is a literal (an atom or the
classical negation of an atom, denoted by ¬).
Deﬁnition 22. [116] Let P be an extended program. We denote by Lit the set of
ground literals in the language of P. For any set S ⊂Lit, let P+ be the extended
program obtained from P by deleting (i) each clause that has a formula ∼L in its
body when L ∈S, and (ii) all formulas of the form ∼L present in the bodies of the
remaining clauses.
Following [40], we say that P+ = RS(P), which should be read as ‘P+ is the
Gelfond–Lifschitz reduction of P with respect to S’.
By the above deﬁnition, P+ does not contain default negation (∼), and its answer
set can be deﬁned as follows.
Deﬁnition 23. [116] The answer set of P+ is the smallest subset S+ of Lit such
that (i) for any rule L0 ←L1,...,Lm of P+, if L1,...,Lm ∈S+, then L0 ∈S+, and
(ii) if S+ contains a pair of complementary literals, then S+ = Lit.9
Finally, the answer set of an extended program P that contains default negation
(∼) can be deﬁned as follows.
Deﬁnition 24. [116] Let P be an extended program and let S ⊂Lit. Let P+ =
RS(P) and let S+ be the answer set of P+. S is the answer set of P iff S = S+.
For example, the program P = {r ←∼p; ¬q ←r} has {r,¬q} as its only answer
set, since no other subset of the literals in P has the same ﬁxed-point property.
The answers that P gives to the queries p, q, and r are, respectively, unknown,
false, and true. Note that the answer set semantics assigns different meanings to
the rules ¬q ←r and ¬r ←q, i.e. it is not contrapositive with respect to ←and ¬.
For instance, the answer set of P′ = {r ←∼p; ¬r ←q} is {r}, differently from the
answer set of (the classically equivalent) P.
If P does not contain classical negation (¬), then its answer sets do not contain
negative literals. As a result, the answer sets of a general logic program are identical
9 S+ = Lit works as though the schema X ←L,∼L were present for all L and X in P+
S .

4.4 Adding Classical Negation
47
to its stable models. However, the absence of an atom A in the stable model of
a general program means that A is false (by default), whereas the absence of A
(and ¬A) in the answer set of an extended program means that nothing is known
about A.10
An extended logic program P that has a consistent answer set can be reduced to
a general logic program P∗as follows. For any negative literal ¬A occurring in P,
let A′ be a positive literal that does not occur in P. A′ is called the positive form of
¬A. P∗is obtained from P by replacing all the negative literals of each rule of P by
their positive forms; P∗is called the positive form of P. For example, the program
P = {a ←b∧¬c; c ←} can be reduced to its positive form; P∗= {a ←b∧c′; c ←}.
Deﬁnition 25. [116] For any set S ⊂Lit, let S∗denote the set of the positive forms
of the elements of S.
Proposition 9. [116] A consistent set S ⊂Lit is an answer set of P if and only if
S∗is a stable model of P∗.
The mapping from P to P∗reduces extended programs to general programs,
although P∗alone does not indicate that A′ represents the negation of A.
By Proposition 9, in order to translate an extended program (P) into a neural
network (N), we can use the same approach as the one for general programs (see
Sect. 4.1, for a description of the Translation Algorithm), with the only difference
being that input and output neurons should be labelled as literals, instead of atoms.
In the case of general logic programs, a concept A is represented by a neuron, and its
weights indicate whether A is a positive or a negative literal in the sense of default
negation, that is, the weights differentiate between A and ∼A. In the case of extended
logic programs, we must also be able to represent the concept ¬A in the network.
We do so by explicitly labelling input and output neurons as ¬A, while the weights
differentiate between ¬A and ∼¬A. Notice that, in this case, the neurons A and ¬A
might both be present in the same network.
Analogously to Theorem 8, the following proposition ensures that the translation
of extended programs into a neural network is sound.
Proposition 10. [66] For any extended logic program P, there exists a feedforward
artiﬁcial neural network N with exactly one hidden layer and semilinear neurons
such that N computes TP∗, where P∗is the positive form of P.
Example 3. Consider the extended program P = {A ←B ∧¬C; ¬C ←B∧∼¬E;
B ←∼D}. The CILP Translation Algorithm applied over the positive form P∗of P
obtains the network N of Fig. 4.4 such that N computes the ﬁxpoint operator TP∗
of P∗.
10 Gelfond and Lifschitz think of answer sets as incomplete theories rather than three-valued mod-
els. Intuitively, the answer sets of a program P are possible sets of beliefs that a rational agent
may hold on the basis of the incomplete information expressed by P. ‘When a program has several
answer sets, it is incomplete also in another sense – it has several different interpretations, and the
answer to a query may depend on the interpretation’ [116].

48
4 Neural-Symbolic Learning Systems
As before, the network of Fig. 4.4 can be transformed into a partially recurrent
network by connecting neurons in the output layer (e.g. B) to their corresponding
neurons in the input layer, with weight Wr = 1, so that N computes the upward
powers of TP∗.
Corollary 11. [66] Let P be a well-behaved extended program. Let P∗be the pos-
itive form of P. There exists a recurrent neural network N with semilinear neurons
such that, starting from an arbitrary input, N converges to the unique ﬁxpoint of
P∗, which is identical to the unique answer set of P.
Example 4. (Example 3 continued) Given any initial activation in the input layer of
N (in Fig. 4.4), the network always converges to the following stable state: A =
true,B = true,¬C = true, which represents the answer set of P, SP = {A,B,¬C}.
The truth values of D and E are unknown.
Let us now brieﬂy discuss the case of inconsistent extended programs. Consider,
for example, the contradictory program P = {B ←A;¬B ←A;A ←}. As it is well
behaved, despite being contradictory, its associated network always converges to
the stable state that represents the set {A,B,¬B}. At this point, we have to make
a choice. Either we adopt Gelfond and Lifschitz’s deﬁnition of answer sets and
assume that the answer set of P is the set Lit of all literals in the language, or we
use a paraconsistent approach (see [29]). We believe that the second option is more
appropriate, owing to the following argument:
Inconsistencies can be read as signals to take external actions, such as ‘ask the user’, or as
signals for internal actions that activate some rules and deactivate other rules. There is a
need to develop a framework in which inconsistency can be viewed according to context, as
a trigger for actions, for learning, and as a source of directions in argumentation. [105]
In [66], neural networks were used to resolve inconsistencies through learn-
ing. In Chap. 11, we deal with value-based argumentation frameworks, and the
methodology proposed there, we believe, can deal with inconsistencies in general.
We propose the use of learning as a way of resolving inconsistencies or deadlocks
(indicated by loops in the network computation; we say that loops are a trigger for
learning).
B
¬C
B
¬C
D
A
¬E
N1
N3
N2
W
W
W
−W
−W
W
W
W
Fig. 4.4 From extended programs to neural networks

4.5 Adding Metalevel Priorities
49
4.5 Adding Metalevel Priorities
So far, we have seen that a single-hidden-layer network can represent either a gen-
eral or an extended logic program. In both cases, the network does not contain
negative connections from the hidden layer to the output. What would then be the
meaning of negative weights from the hidden to the output layer of the network?
One interpretation is that such negative weights are used to block an output neu-
ron’s activation, serving as a form of prioritisation (or, in other words, implementing
a preference relation between rules). Below, we use r j ≻ri to indicate that rule r j
has a higher priority than rule ri. We start with an example.
Example 5. Consider the following program: P = {r1 : fingerprints, r2 : alibi,
r3 : fingerprints →guilty, r4 : alibi →¬guilty}. Take a priority relation r3 ≻r4,
indicating that fingerprints →guilty is stronger evidence than alibi →¬guilty. A
neural network that encodes the program P but not the relation r3 ≻r4 will com-
pute the inconsistent answer set {fingerprints,alibi,guilty,¬guilty}. Alternatively,
the metalevel priority relation could be incorporated into the object level by chang-
ing the rule alibi →¬guilty to alibi∧∼fingerprints →¬guilty. The new program
would compute the answer set {fingerprints,alibi, guilty}, which contains the in-
tended answers for P given r3 ≻r4.
How could we represent the above priority explicitly in the neural network? In
the same way that negative weights from input to hidden neurons are interpreted as
negation by default because they contribute to blocking the activation of the hidden
neurons, negative weights from hidden to output neurons could be seen as the im-
plementation of the metalevel priority [64]. A negative weight from hidden neuron
r3 to the output neuron ¬guilty would implement r3 ≻r4 provided that whenever
r3 is activated, r3 then blocks (or inhibits) the activation of ¬guilty, which is the
conclusion of r4. Figure 4.5 illustrates the idea.
fingerprints
r2
r1
¬guilty
−W
alibi
guilty
fingerprints
alibi
r3
r4
Fig. 4.5 Adding metalevel priorities (r3 ≻r4)

50
4 Neural-Symbolic Learning Systems
In the above example, r3 ≻r4 means that whenever the conclusion of r3 holds, the
conclusion of r4 does not hold. Hence, ≻(ri,rj) deﬁnes priorities among rules that
can allow the conclusion of one rule to override the conclusion of another. It is thus
similar to the superiority relation deﬁned by Nute [193,194] and later investigated
by Antoniou et al. [8].
In the following, we recall some of the basic deﬁnitions of Nute’s superiority
relation. A superiority (binary) relation ≻is a strict partial order, i.e. an irreﬂexive
and transitive relation on a set. Rule r1 is said to be superior to rule r2 if r1 ≻r2.
When the antecedents of two rules are derivable, the superiority relation is used to
adjudicate the conﬂict. If either rule is superior to the other, then the superior rule
is applied. In other words, ≻provides information about the relative strength of the
rules. Following [8], we shall deﬁne superiority relations over rules with contra-
dictory conclusions only. More precisely, for all ri,rj, if ≻(ri,rj), then ri and r j
have complementary literals (x and ¬x) as consequents.11 A cycle in the superiority
relation (e.g. r1 ≻r2 and r2 ≻r1) is counterintuitive from a knowledge representa-
tion perspective, and thus ≻is also required to be an acyclic relation, i.e. we assume
that the transitive closure of ≻is irreﬂexive.
It has been proved [8] that the above superiority relation does not add to the
expressive power of defeasible logic [194]. In fact, (object-level) default negation
and (metalevel) superiority relations are interchangeable (see [146]). The translation
is as follows:
1. Replace the clause L1,...,Lp,∼Lp+1,...,∼Lq →Lq+1 by the clause r : L1,...,
Lp →Lq+1;
2. Add to P, j clauses of the form ri(1≤i≤j) : L j →¬Lq+1, where p+1 ≤j ≤q;
3. Deﬁne a preference relation between ri and r. In general, ri ≻r for 1 ≤i ≤j.
Example 6. Let P = {p; p →b; b∧∼p →f}. P can be translated to the following
logic program without negation as failure: P′ = {r1 : →p; r2 : p →b; r3 : b →f;
r4 : p →¬f}, r4 ≻r3, such that S(P′) ≈S(P), i.e. the answer set semantics of P
and P′ are equivalent.
Nute’s superiority relation adds, however, to the epistemic power of defeasible
logic because it allows one to represent information in a more natural way.
For example, the Nixon diamond problem can be expressed as follows: r1 :
quaker(Nixon)⇝paci fist(Nixon) and r2 : republican(Nixon)⇝¬paci fist(Nixon),
where ⇝should be read as ‘normally implies’. The deﬁnition of an adequate pri-
ority relation between r1 and r2 would then resolve the inconsistency regarding
the paciﬁsm of Nixon, when both quaker(Nixon) and republican(Nixon) are true.
Also, for epistemological reasons, in many cases it is useful to have both default
negation and metalevel priorities. This facilitates the expression of (object-level)
11 In [8], Antoniou et al. argue that ‘It turns out that we only need to deﬁne the superiority re-
lation over rules with contradictory conclusions’. In [211], however, Prakken and Sartor present
examples in which priorities have to be given between noncontradictory pieces of information,
which propagate in order to solve an inconsistency. Otherwise, they claim, the result obtained is
counterintuitive. For the sake of simplicity, we restrict ≻(ri,rj) to conﬂicting rules.

4.5 Adding Metalevel Priorities
51
priorities in the sense of default reasoning and (metalevel) priorities in the sense
that a given conclusion should be overridden by another with higher priority.12
Of course, preference relations can represent much more than a superiority re-
lation between conﬂicting consequents. Brewka’s preferred subtheories [38] and
Prakken and Sartor’s argument-based extended logic programming with defeasible
priorities [211] are only two examples in which a preference relation establishes
a partial ordering between rules that describes the relative degree of belief in some
beliefs in general, and not only between conﬂicting conclusions. From a more philo-
sophical point of view, the AGM theory [4] deﬁnes a priority relation as representa-
tive of the epistemic entrenchment of a set of beliefs, in which formulas are analysed
assuming that they are closed under logical consequence. In [221], a preference re-
lation is deﬁned as a preorder, and in this case, one can differentiate between two
incomparable beliefs and two beliefs with the same priority. We shall investigate the
links between neural networks and argumentation in detail in Chap. 11.
In this section, however, we are interested in ﬁnding out what is the prefer-
ence relation that ﬁts easily into a single-hidden-layer neural network. For this rea-
son, we shall stick to Nute’s superiority relation ≻. It would be interesting, though,
to try to implement some of the above more sophisticated preference relations.
Owing to the limitations of ≻, it might be more appropriate to see it as inconsis-
tency handling rather than preference handling.13
Hence, the superiority relation that we discuss here essentially makes explicit
the priorities encoded in the object level. As a result, a network N≻encoding a
program P≻with explicit priorities is expected to behave exactly like the network N
that encodes the equivalent extended program P without priorities. The following
deﬁnition clariﬁes what we mean by the equivalence between P≻and P in terms of TP.
Deﬁnition 26. Let P≻= {r1,r2,...,rn} be an extended program with an explicit su-
periority relation ≻. Let P be the same extended program P≻without the superiority
relation ≻. For any two rules ri,rj in P≻such that r j ≻ri, let P′ = P−ri, i.e. P′ is the
program P without rule ri. We deﬁne TP≻:= TP′ if r j ﬁres, and TP≻:= TP otherwise.
The following example illustrates how we can encode metalevel priorities in a
neural network by adding a very simple and straightforward step to the translation
algorithm.
Example 7. Consider the following labelled logic program P = {r1 : a∧b∧∼c →x,
r2 : d ∧e →¬x}. P can be encoded in a neural network by using the Translation
Algorithm of Sect. 4.1. If we have also the information that, say, r1 ≻r2, then we
know that the consequent of r1 (x) is preferred over the consequent of r2 (¬x). We
can represent this in the network by ensuring that whenever x is activated, the acti-
vation of ¬x is blocked. We might do so by simply connecting the hidden neuron r1
12 In a trained neural network, both representations (P and P′) might be encoded simultaneously,
so that the network is more robust.
13 In [40], Brewka and Eiter differentiated between the use of priorities to resolve conﬂicts that
emerge from rules with opposite conclusions, and the use of priorities for choosing a rule out of
a set of (not necessarily conﬂicting) rules for application. Whereas Brewka and Eiter’s preferred
answer set semantics uses the latter, this section offers a neural implementation of the former.

52
4 Neural-Symbolic Learning Systems
a
x
r2
r1
b
c
d
e
¬x
−δW
W
Fig. 4.6 Adding metalevel priorities (r1 ≻r2)
(remember that each hidden neuron represents a rule of the background knowledge)
to the output neuron ¬x with a negative weight (see Fig. 4.6). The idea is (a) that,
from the Translation Algorithm, when x is activated, r1 also is, and in this case r1
should inhibit ¬x, regardless of the activation of r2. Hence, we need to set the weight
−δW (δ,W ∈ℜ+) from r1 to ¬x accordingly. We also need to guarantee that (b) the
addition of such a new weight does not change the behaviour of the network when
r1 is not activated. In this example, to satisfy (a) above, if the weight from r2 to ¬x is
W, then h(W −AminδW −θ ¬x) < −Amin has to be satisﬁed. To satisfy (b), we need to
guarantee that when r1 is not activated, ¬x is activated if and only if r2 is activated.
Therefore, h(AminW +δW −θ ¬x) > Amin and h(−AminW +AminδW −θ ¬x) < −Amin
have to be satisﬁed. This imposes a constraint on θ ¬x.14 If, for example,W = 6.0 and
Amin = 0.8, then 3.4 < θ ¬x < 86.0 and, if we take θ ¬x = 5, then 0.666 < δ < 1.267
is such that the intended meaning of r1 ≻r2 is obtained.
4.6 Applications of CILP
In [66], the CILP system was applied to two real-world problems of DNA clas-
siﬁcation, which have become benchmark data sets for testing the accuracy of
machine-learning systems. That publication also presents a comparison of CILP
with other neural, symbolic, and hybrid inductive learning systems. Brieﬂy, the
test-set performance of CILP is at least as good as that of KBANN and of
backpropagation, and better than that of any other system investigated, and CILP’s
training-set performance is considerably superior to that of KBANN and of back-
propagation. We also applied CILP to fault diagnosis, using a simpliﬁed version
of a real power generation plant. In this application, we used a system extended
with classical negation. We then compared CILP with backpropagation, using three
different architectures. The results corroborate the importance of learning with
background knowledge in the presence of noisy data sets.
14 Recall that we originally had (by Equation 4.4) θ ¬x = 0 in this example.

4.7 Discussion
53
4.7 Discussion
Many different systems that combine background knowledge and learning from
examples – ranging from Bayesian networks [184] to neuro-fuzzy systems [51] –
have been proposed in the last decade. Among those that combine fragments of pred-
icate logic [86,100] and connectionist models [125], the work of Pinkas [204,205]
and of H¨olldobler [133, 134, 137] is particularly relevant. In an attempt to show
the capabilities and limitations of neural networks in performing logical inference,
Pinkas deﬁned a bidirectional mapping between symmetric neural networks15 and
fragments of predicate logic. He presented a theorem showing equivalence between
the problem of satisﬁability of propositional logic and minimising the energy func-
tion associated with a symmetric network: for every well-formed formula (wff), a
quadratic energy function can be found efﬁciently, and for every energy function
there exists a wff (found inefﬁciently) such that the global minima of the func-
tion are exactly equal to the satisfying models of the formula. H¨olldobler presented
a parallel uniﬁcation algorithm and an automated reasoning system for ﬁrst-order
Horn clauses, implemented in a feedforward neural network, called the Connection-
ist Horn Clause Logic (CHCL) system.
The relations between neural networks and nonmonotonicity can be traced back
to [16], in which Balkenius and G¨ardenfors showed that symmetric neural networks
can compute a form of nonmonotonic inference relation, satisfying some of the
most fundamental postulates for nonmonotonic logic. G¨ardenfors went on to pro-
duce his theory of conceptual representations [112], according to which a bridge
between the symbolic and connectionist approaches to cognitive science could be
built from geometric structures based on a number of quality dimensions such as
temperature, weight, and brightness, as well as the spatial dimensions. The only
nonmonotonic neural-symbolic learning systems are, to the best of our knowledge,
the CILP system and Boutsinas and Vrahatis’ artiﬁcial nonmonotonic neural net-
works (ANNN) [36], which uses a multiple inheritance scheme with exceptions to
learn inheritance networks. Although Pinkas’s work [205] dealt with the representa-
tion of penalty logic, a kind of nonmonotonic logic, in symmetric neural networks,
the task of learning (penalties) from examples in such networks was not investigated
to the same extent as the task of representation. Finally, neural networks have been
combined with explanation-based learning (EBL) systems [185] into Explanation-
based neural networks (EBNN) [245], and EBL has been combined with inductive
logic programming (ILP) [187] (see [159, 189] for introductions to ILP), whereas
the relations between ILP and neural networks have only begun to be unravelled
(see [19,252] and Chap. 10).
Recent decades have also seen much work in the ﬁeld of hybrid (learning and
reasoning) systems. In particular, many neuro-fuzzy systems have been developed.
The main difﬁculties in comparing these systems with CILP are with regard to the
employment for each system of different testing methodologies and very speciﬁc
15 A neural network is called symmetric if Wi j = Wji for all neurons i, j (e.g. Hopﬁeld networks
[139]).

54
4 Neural-Symbolic Learning Systems
learning techniques. For example, in [95], Fu described the Knowledge-Based Con-
ceptual Neural Network (KBCNN), a hybrid system similar to KBANN. The re-
sults obtained with KBCNN in the domain of the promoter recognition problem
(98.1% accuracy) were marginally better than those obtained with CILP (97.2% ac-
curacy),16 as discussed in [66]. However, it seems that Fu did not use the method
of cross-validation in his experiments with KBCNN. To further complicate things,
KBCNN seems closely tied to its particular learning algorithm.
The overall performance of CILP is very satisfactory. CILP’s effectiveness is a
result of three of the system’s features:
• CILP is based on backpropagation,
• CILP uses background knowledge, and
• CILP provides an accurate, compact translation.
In this chapter, we have presented the Connectionist Inductive Learning and
Logic Programming System (CILP) – a massively parallel computational model
based on artiﬁcial neural networks that integrates inductive learning from examples
and background knowledge with deductive learning from logic programming. We
have also illustrated how an extension of CILP can be used as a massively parallel
model for extended logic programming. Consequently, some facts of commonsense
knowledge can be represented more easily when classical negation is available.
In a rather satisfying way, following [135], the algorithm for translation of P into
N associates each hidden neuron of N with a rule of P. Such a representation has
led to an extension of CILP in which negative connections from hidden to output
layers are used to inhibit the presence of a neuron in the answer set of P, thereby
acting as a metalevel priority. To summarise, we have shown that single-hidden-
layer neural networks can represent, in a very natural way, the class of extended
logic programs augmented with metalevel priorities given by a superiority relation.
The question of whether this is the language that single-hidden layer networks can
learn still remains without an answer.
16 KBCNN was not applied to the splice-junction determination problem.

Chapter 5
Connectionist Modal Logic
This chapter introduces connectionist modal logic (CML). CML can be seen as a
general model for the development of connectionist nonclassical reasoning based
on modal logics [79]. CML uses ensembles of neural networks to represent possible
worlds, which underpin the semantics of several modal logics. In what follows, we
introduce the theoretical foundations of CML and show how to build an integrated
model for computing and learning modal logics.
It is now common knowledge that modal logic has become one of the outstand-
ing logical languages used in computer science, enjoying success across the sci-
ence’s spectrum, from the theoretical foundations [42, 87, 143] to state-of-the-art
hardware [50] and multiagent technologies [89]. The toolbox of any AI researcher
now includes modal logic, as it has been found appropriate for research in several
areas of AI [24,42,87,106]. Areas such as knowledge representation, planning, and
theorem proving have also been making extensive use of modal logic, be it tem-
poral, epistemic, conditional, intuitionistic, doxastic, or many-dimensional modal
logic, including, for instance, combinations of time and knowledge, time and belief,
or space and time, to name a few [24,42,106,218].
In this chapter, we propose a new approach to the representation and learning of
propositional modal logic in a connectionist framework by introducing connection-
ist modal logic (CML). We use the language of modal logic programming [226],
extended to allow modalities such as necessity and possibility in the head of clauses
[197].1
We shall present an algorithm to set up an ensemble of Connectionist Inductive
Learning and Logic Programming (CILP) networks [66]. A theorem then shows
that the resulting ensemble computes a ﬁxed-point semantics of the original modal
program. In other words, the network ensemble can be seen as a massively parallel
system for representation and reasoning in modal logic. We validate the system by
applying it to the muddy children puzzle, a well-known problem in the domain of
distributed knowledge representation and reasoning [87, 143]. We then proceed to
1 Note that Sakakibara’s modal logic programming [226] is referred to as Modal Prolog in the
well-known survey of temporal and modal logic programming by Orgun and Ma [197].
A.S. d’Avila Garcez et al., Neural-Symbolic Cognitive Reasoning, Cognitive Technologies,
55
c⃝Springer-Verlag Berlin Heidelberg 2009

56
5 Connectionist Modal Logic
learning experiments to validate the proposed connectionist model. The experiments
indicate that the merging of background knowledge and data learning (learning from
examples) in neural networks can provide an effective learning system [66,250].
Learning in connectionist modal logic is achieved by training each individual
neural network, which in turn corresponds to the current knowledge of an agent
within a possible world; we use backpropagation. Learning will be exempliﬁed by
the system learning the knowledge of an agent in the muddy children puzzle using
neural networks.
The CML framework presented here thus provides neural-symbolic learning sys-
tems with a better balance between expressive power and computational feasibility,
owing to the use of a more expressive, yet computationally tractable, knowledge rep-
resentation language. The well-established translation between propositional modal
logic and the two-variable fragment of ﬁrst-order logic [264] indicates that neural-
symbolic learning systems may go beyond propositional logic.2
As argued in [71,72], we believe that the combination of modal and nonclassical
logics and connectionist models may provide the way forward towards the provision
of an integrated system of expressive, sound reasoning and robust learning.
In what follows, we brieﬂy present the basic concepts of modal logic used
throughout this chapter. Then, we present a Connectionist Modal Algorithm that
shows how to compute extended modal programs in artiﬁcial neural networks. The
network obtained is an ensemble of simple CILP networks, each representing a
(learnable) possible world. We then show that the network computes a ﬁxed-point
semantics of the given modal theory, thus proving the soundness of the Connec-
tionist Modal Algorithm. We also prove termination of the Connectionist Modal
Algorithm. Finally, we apply the system to the muddy children puzzle.
5.1 Modal Logic and Extended Modal Programs
In this section, we present some basic concepts of modal logic that will be used
throughout the chapter. Modal logic began with an analysis of concepts such as
necessity and possibility from a philosophical perspective [42, 46, 106, 141, 162].
A main feature of modal logic is the use of possible-world semantics (proposed by
Kripke and Hintikka), which has signiﬁcantly contributed to the development of new
nonclassical logics, many of which have had a great impact in computing science.
In modal logic, a proposition is necessary in a world if it is true in all worlds which
are possible in relation to that world, whereas it is possible in a world if it is true in
2 In [264], p. 2, Vardi states that ‘(propositional) modal logic, in spite of its apparent propositional
syntax, is essentially a ﬁrst-order logic, since the necessity and possibility modalities quantify
over the set of possible worlds’. And, in [264], p. 7, ‘the states in a Kripke structure correspond
to domain elements in a relational structure, and modalities are nothing but a limited form of
quantiﬁers’. A comprehensive treatment of this subject, including a study of the correspondence
between propositional modal logic and (fragments of) ﬁrst-order logic, can be found in [106] and
in the work of van Benthem [256,257].

5.1 Modal Logic and Extended Modal Programs
57
at least one world which is possible in relation to that same world. This is expressed
in the formalisation of the semantics by a (binary) relation between possible worlds.
Modal logic was found to be appropriate for studying mathematical necessity (in
the logic of provability), time, knowledge, belief, obligation, and other concepts and
modalities [42,46,106]. In artiﬁcial intelligence and computing, modal logic is one
of the formalisms most frequently employed to analyse and represent reasoning in
multiagent systems and concurrency properties [87]. The basic deﬁnitions related to
modal logic that we shall use in this chapter are as follows. As usual, the language
of propositional modal logic extends the language of propositional logic with the
necessity (□) and possibility (♦) operators. Moreover, we assume that any clause is
grounded over a ﬁnite domain (i.e. the clauses contain no variables).
Deﬁnition 27. A modal atom is of the form MA, where M ∈{□,♦} and A is an
atom. A modal literal is of the form ML, where L is a literal.
Deﬁnition 28. A modal program is a ﬁnite set of clauses of the form α1∧...∧αn →
αn+1, where αi (1 ≤i ≤n) is either an atom or a modal atom, and αn+1 is an atom.
We deﬁne extended modal programs as modal programs with single modalities
□and ♦in the head of clauses, and default negation ∼in the body of clauses [49],
thus extending Sakakibara’s modal logic programming [197,226]. In addition, each
clause is labelled by the possible world in which it holds, similarly to Gabbay’s
labelled deductive systems [42, 99]. Finally, the use of classical negation ¬ in the
body of clauses is allowed, as done in [116], by renaming each literal of the form
¬A as a new literal A∗not present in the language.
Deﬁnition 29. An extended modal program is a ﬁnite set of clauses C of the form
ωi : β 1 ∧... ∧β n →β n+1, where ωi is a label representing a world in which the
associated clause holds, β i (1 ≤i ≤n) is either a literal or a modal literal, and β n+1
is either an atom or a modal atom, together with a ﬁnite set of relations R(ωi,ω j)
between the worlds ωi and ω j in C.
For example, P = {ω1 : r →□q; ω1 : ♦s →r; ω2 : s; ω3 : q →♦p; R(ω1,ω2),
R(ω1,ω3)} is an extended modal program. Formulas in modal logic will be inter-
preted in Kripke models, which are deﬁned as follows.
Deﬁnition 30 (Kripke Model). Let L be a modal language. A Kripke model for L
is a tuple M = ⟨Ω,R,v⟩, where Ω is a set of possible worlds, v is a mapping that
assigns a subset of Ω to each propositional letter of L, and R is a binary relation
over Ω.
A modal formula ϕ is said to be true in a possible world ω of a model M, written
(M,ω) |= ϕ, if the following satisﬁability condition holds.

58
5 Connectionist Modal Logic
Deﬁnition 31 (Satisﬁability of Modal Formulas). Let L be a modal language,
and let M = ⟨Ω,R,v⟩be a Kripke Model. The satisﬁability relation |= is uniquely
deﬁned as follows:
(i) (M,ω) |= p iff ω ∈v(p) for a propositional letter p;
(ii) (M,ω) |= ¬ϕ iff (M,ω) ⊭ϕ;
(iii) (M,ω) |= ϕ ∧ψ iff (M,ω) |= ϕ and (M,ω) |= ψ;
(iv) (M,ω) |= ϕ ∨ψ iff (M,ω) |= ϕ or (M,ω) |= ψ;
(v) (M,ω) |= ϕ →ψ iff (M,ω) ⊭ϕ or (M,ω) |= ψ;
(vi) (M,ω) |= □ϕ iff, for all ω1 ∈Ω, if R(ω,ω1) then (M,ω1) |= ϕ;
(vii) (M,ω) |= ♦ϕ iff there exists an ω1 such that R(ω,ω1) and (M,ω1) |= ϕ.
A variety of proof procedures for modal logic have been developed over the years
(e.g. [42, 90]). In some of these, formulas are labelled by the worlds in which they
hold, thus facilitating the modal reasoning process (see [42] for a discussion of this
topic). In the natural-deduction-style rules below, the notation ω : ϕ means that the
formula ϕ holds in the possible world ω. Moreover, the explicit reference to the
accessibility relation also helps in deriving what formula holds in the worlds which
are related by R. The rules that we shall represent in CML are similar to the ones
presented in [42], which we reproduce in Table 5.1.
The ♦E rule can be seen (informally) as a Skolemisation of the existential quan-
tiﬁer over possible worlds, which is semantically implied by the formula ♦ϕ in the
premise. The term fϕ(ω) deﬁnes a particular possible world that is uniquely associ-
ated with the formula ϕ, and inferred to be accessible from the possible world ω (i.e.
R(ω, fϕ(ω))). In the □I rule, the (temporary) assumption [R(ω,gϕ(ω))] should be
read as ‘given an arbitrary accessible world gϕ(ω), if one can derive gϕ(ω) : ϕ, then
it is possible to show that □ϕ holds in ω.’ The rule for ♦I represents the assertion
that if we have a relation R(ω1,ω2), and if ϕ holds in ω2, then it must be the case
that ♦ϕ holds in ω1. The rule □E represents the assertion that if □ϕ holds in a
world ω1, and ω1 is related to ω2, then we can infer that ϕ holds in ω2.
5.1.1 Semantics for Extended Modal Logic Programs
In what follows, we deﬁne a model-theoretic semantics for extended modal pro-
grams. According to the rules for modality given above, we shall deal with ♦by
Table 5.1 Reasoning rules for modality operators
[R(ω,gϕ(ω))]
···
gϕ(ω) : ϕ
□I
ω : □ϕ
ω1 : □ϕ,R(ω1,ω2)
□E
ω2 : ϕ
ω : ♦ϕ
♦E
fϕ(ω) : ϕ,R(ω, fϕ(ω))
ω2 : ϕ,R(ω1,ω2)
♦I
ω1 : ♦ϕ

5.1 Modal Logic and Extended Modal Programs
59
making a choice of the world ω j in which to have A when ♦A is true in ωi and
R(ωi,ω j). In this chapter, we choose an arbitrary world (i.e. one that is uniquely
associated with A). In practice, one may opt to manage several neural networks, one
for each choice,3 in the same way that one may opt to manage several graphs, for
example as in the (modal) tableau prover LoTREC [45]. In this case, each choice
could possibly lead to different ﬁxed points of a given extended modal program, but
once the choice is made, if the program is well behaved (e.g. in the sense of Fitting’s
metric methods [92]), we should be able to prove that the computation terminates
with our neural network converging to a ﬁxed point of the meaning operator.
When computing the ﬁxed point, we have to consider the consequences derived
locally and the consequences derived from the interaction between worlds. Locally,
ﬁxed points are computed as in the stable-model semantics for logic programming,
by simply renaming each modal literal MLi as a new literal Lj not in the language L,
and applying the Gelfond–Lifschitz transformation [40]. When we are considering
interacting worlds, there are four more cases to be addressed, according to the rules
in Table 5.1.
Hence, we proceed as follows. Given an extended modal program, for each literal
of the form ♦L in the head of a clause, we choose a world and connect (in a sense that
will become clear soon) ♦L to the literal L in this world. We also connect each literal
of the form □L to literals L in every world related to that of □L, and similarly for the
other rules. The deﬁnition of the modal consequence operator below captures this.
Deﬁnition 32 (Modal Immediate-Consequence Operator). Let P = {P1,...,Pk}
be an extended modal program, where each Pi is the set of modal clauses that hold
in a world ωi (1 ≤i ≤k). Let BP denote the set of atoms occurring in P (i.e. the
Herbrand base of P), and let I be a Herbrand interpretation for P. Let α be either
an atom or a modal atom. The mapping MTP : 2BP →2BP is deﬁned as follows:
MTP(I) = {α ∈BP | either (i) or (ii) or (iii) or (iv) or (v) below holds}.
(i) β 1,...,β n →α is a clause in P and {β 1,...,β n} ⊆I;
(ii) α is of the form ωi : A, ωi is of the type fA(ωk) (i.e. ωi is a particular possible
world uniquely associated with A), there exists a world ωk such that R(ωk,ωi),
and ωk : β 1,...,β m →♦A is a clause in P with {β 1,...,β m} ⊆I;
(iii) α is of the form ωi : ♦A, there exists a world ω j such that R(ωi,ω j), and
ω j : β 1,...,β m →A is a clause in P with {β 1,...,β m} ⊆I;
(iv) α is of the form ωi : □A and, for each world ω j such that R(ωi,ω j), ω j :
β 1,...,β o →A is a clause in P with {β 1,...,β o} ⊆I;
(v) α is of the form ωi : A, there exists a world ωk such that R(ωk,ωi), and ωk :
β 1,...,β o →□A is a clause in P with {β 1,...,β o} ⊆I.
Following [21], one can construct the semantics of extended modal programs
by considering extended modal ground formulas in order to compute a ﬁxed point.
As a result, one can associate with every extended modal program a modal ground
3 Such a choice is computationally similar to the approach adopted by Gabbay in [98], in which
one chooses a point in the future for execution and then backtracks if it is judged necessary (and is
at all possible).

60
5 Connectionist Modal Logic
program (the modal closure of the program) so that both programs have the same
models. Hence, the classical results about the ﬁxed-point semantics of logic pro-
gramming can be applied to the modal ground closure of a program.
5.2 Connectionist Modal Logic
In this section, we introduce CML. We shall use ensembles of CILP networks as
the underlying CML architecture to represent modal theories. We then present an
efﬁcient translation algorithm from extended modal programs to neural-network en-
sembles.
Let us start with a simple example. It brieﬂy illustrates how an ensemble of CILP
networks can be used for modelling nonclassical reasoning with modal logic. Input
and output neurons may represent □L, ♦L, or L, where L is a literal.
Example 8. Figure 5.1 shows an ensemble of three CILP networks (ω1,ω2,ω3),
which might communicate in many different ways. The idea is to see ω1, ω2, and
ω3 as possible worlds, and to incorporate modalities into the language of CILP. For
example, (i) ‘If ω1 : □A then ω2 : A’ could be communicated from ω1 to ω2 by
connecting □A in ω1 to A in ω2 such that, whenever □A is activated in ω1, A is
activated in ω2. Similarly, (ii) ‘If (ω2 : A)∨(ω3 : A) then ω1 : ♦A’ could be imple-
mented by connecting the neurons A of ω2 and ω3 to the neuron ♦A of ω1 through
C
C
B
B
A
A
C
C
B
B
A
A
C
C
B
B
A
A
ω1
ω3
ω2
Fig. 5.1 An ensemble of CILP networks that models uncertainty by using modalities and possible
worlds (ω2 and ω3 may be seen as alternative models)

5.2 Connectionist Modal Logic
61
a number of hidden neurons. Examples (i) and (ii) simulate, in a ﬁnite universe, the
rules of □Elimination and ♦Introduction (see Table 5.1).
Owing to the simplicity of each CILP network in the ensemble, performing in-
ductive learning within each possible world is straightforward. The main problem
to be tackled when it comes to learning in the new neural model is how to set up
the connections that establish the necessary communication between networks, for
example ω1 and ω2. In the case of modal logic, such connections are deﬁned by the
modal rules of natural deduction (Table 5.1). The Connectionist Modal Algorithm
presented in Sect. 5.2.1 implements those rules.
5.2.1 Computing Modalities in Neural Networks
In this section, we present the computational machinery of CML. We use the CILP
Translation Algorithm presented in Chap. 4 for creating each network of the en-
semble, and the Connectionist Modal Algorithm described below for interconnect-
ing and reasoning with the various networks. The Connectionist Modal Algorithm
translates natural-deduction modal rules into the networks. Intuitively, the accessi-
bility relation is represented by connections between (sub)networks. As depicted
in Fig. 5.2, where R(ω1,ω2) and R(ω1,ω3), connections from ω1 to ω2 and ω3
represent either □E or ♦E. Connections from ω2 and ω3 to ω1 represent either
□I or ♦I.
C
C
B
B
A
A
C
C
B
B
A
A
C
C
B
B
A
A
w1
w3
w2
  E
 I
   E
  I
Fig. 5.2 An ensemble of networks representing the natural-deduction-style rules for the □and ♦
modalities

62
5 Connectionist Modal Logic
Let P be an extended modal program with clauses of the form ωi : ML1,...,
MLk →MA, where each Lj is a literal, A is an atom, and M ∈{□,♦}, 1 ≤i ≤n,
0 ≤j ≤k. As in the case of individual CILP networks, we start by calculating
MAXP(−→k ,−→
μ ,n) of P and Amin such that
Amin > MAXP(−→
k,−→
μ ,n)−1
MAXP(−→
k,−→
μ ,n)+1
,
(5.1)
but now we also need to take into account the number n of networks (i.e. possi-
ble worlds) in the ensemble, and thus we use MAXP(−→
k,−→
μ ,n) instead of simply
MAXP(−→
k,−→
μ ).
Connectionist Modal Algorithm
1. Let Pi ⊆P be the set of clauses labelled by ωi in P. Let W M ∈R.
2. For each Pi, do:
(a) Rename each MLj in Pi as a new literal not occurring in P, of the form L□
j
if M = □, or L♦
j if M = ♦.4
(b) Call CILP’s Translation Algorithm (Sect. 4.1).
(c) Let Ni be the neural network that denotes Pi.
3. For each output neuron L♦
j in Ni, do:
(a) Add a hidden neuron LM
j to an arbitrary Nk (0 ≤k ≤n) such that R(ωi,ωk).
(b) Set the step function s(x) as the activation function of LM
j .
(c) Connect L♦
j in Ni to LM
j and set the connection weight to 1.
(d) Set the threshold θ M of LM
j such that −1 < θ M < Amin.
(e) Create an output neuron Lj in Nk, if it does not exist yet.
(f) Connect LM
j to L j in Nk, and set the connection weight to W M
Lj > h−1(Amin)+
μLjW +θ Lj.5
4. For each output neuron L□
j in Ni, do:
(a) Add a hidden neuron LM
j to each Nk (0 ≤k ≤n) such that R(ωi,ωk).
(b) Set the step function s(x) as the activation function of LM
j .
(c) Connect L□
j in Ni to LM
j and set the connection weight to 1.
(d) Set the threshold θ M of LM
j such that −1 < θ M < Amin.
4 This allows us to treat each MLj as a literal and apply the Translation Algorithm of Chap. 4
directly to Pi by labelling neurons as □Lj, ♦Lj, or Lj.
5 Recall that μL is the number of connections to output neuron L, and that θ L is the threshold of
output neuron L. Note also that μL, W, and θ L are all obtained from CILP’s Translation Algorithm.

5.2 Connectionist Modal Logic
63
(e) Create output neurons Lj in each Nk, if they do not exist yet.
(f) Connect LM
j to L j in Nk, and set the connection weight to W M
Lj > h−1(Amin)+
μLjW +θ Lj.
5. For each output neuron Lj in Nk such that R(ωi,ωk) (0 ≤i ≤n), do:
(a) Add a hidden neuron L∨
j to Ni, if it does not exist yet.
(b) Set the step function s(x) as the activation function of L∨
j .
(c) For each ωi such that R(ωi,ωk), do:
i. Connect Lj in Nk to L∨
j and set the connection weight to 1.
ii. Set the threshold θ ∨of L∨
j such that −nAmin < θ ∨< Amin −(n−1).
iii. Create an output neuron L♦
j in Ni, if it does not exist yet.
iv. Connect L∨
j to L♦
j in Ni, and set the connection weight to W M
L♦
j >
h−1(Amin)+ μL♦
j W +θ L♦
j .
6. For each output neuron Lj in Nk such that R(ωi,ωk) (0 ≤i ≤n), do:
(a) Add a hidden neuron L∧
j to Ni, if it does not exist yet.
(b) Set the step function s(x) as the activation function of L∧
j .
(c) For each ωi such that R(ωi,ωk), do:
i. Connect Lj in Nk to L∧
j and set the connection weight to 1.
ii. Set the threshold θ ∧of L∧
j such that n−(1+Amin) < θ ∧< nAmin.
iii. Create an output neuron L□
j in Ni, if it does not exist yet.
iv. Connect L∧
j to L□
j in Ni, and set the connection weight to W M
L□
j >
h−1(Amin)+ μL□
j W +θ L□
j .6
7. For each Pi, recurrently connect each output neuron Lj (and L♦
j ,L□
j ) in Ni to
its corresponding input neuron Lj (and L♦
j ,L□
j , respectively) in Ni with weight
Wr = 1.7
Let us now illustrate the use of the Connectionist Modal Algorithm with the
following example.
Example 9. Let P = {ω1 : r →□q, ω1 : ♦s →r, ω2 : s, ω3 : q →♦p, R(ω1,ω2),
R(ω1,ω3)}. We start by applying CILP’s Translation Algorithm, which creates
three neural networks to represent the worlds ω1, ω2, and ω3 (see Fig. 5.3).
Then, we apply the Connectionist Modal Algorithm. Hidden neurons labelled by
{M,∨,∧} are created using the Connectionist Modal Algorithm. The remaining
neurons are all created using the Translation Algorithm. For the sake of clarity, un-
connected input and output neurons are not shown in Fig. 5.3. Taking N1 (which
6 The values of W M are derived from the proof of Theorem 12.
7 This essentially allows one to iterate MTP, thus using the ensemble to compute the extended
modal program in parallel.

64
5 Connectionist Modal Logic
s
q
q
p
N2
N3
N1
q
r
r
s
q
∨
s
∧
M1
M2
M1
H1
H2
H1
H1
Fig. 5.3 The ensemble of networks {N1, N2, N3} representing P = {ω1 : r →□q, ω1 : ♦s →r,
ω2 : s, ω3 : q →♦p, R(ω1,ω2), R(ω1,ω3)}
represents ω1), output neurons L♦
j should be connected to output neurons Lj in an
arbitrary network Ni (which represents ωi) to which N1 is related. For example,
taking Ni = N2, ♦s in N1 is connected to s in N2. Then, output neurons L□
j should
be connected to the output neurons Lj in every network Ni to which N1 is related.
For example, □q in N1 is connected to q in both N2 and N3. Now, taking N2, the
output neurons Lj need to be connected to output neurons L♦
j and L□
j in every world
Nj related to N2. For example, s in N2 is connected to ♦s in N1 via the hidden
neuron denoted by ∨in Fig. 5.3, while q in N2 is connected to □q in N1 via the
hidden neuron denoted by ∧. Similarly, q in N3 is connected to □q in N1 via ∧.
Finally, the output neurons ♦s and r in N1 are connected to the input neurons ♦s
and r, respectively, in N1, and the output neuron q in N3 is connected to the input
neuron q in N3, all with weight 1. The algorithm terminates when all output neurons
have been connected.
Table 5.2 contains a valid set of weights for the connections shown in Fig. 5.3,
obtained from the Connectionist Modal Algorithm and the Translation Algorithm.
We use (XNi,YNj) to denote the weight from neuron X in network Ni to neuron
Y in network Nj, and (XNi) to denote the threshold of neuron X in network Ni.

5.2 Connectionist Modal Logic
65
The calculations are as follows. From Equation 5.1, Amin > (MAXP(1,2,3) −1)/
(MAXP(1,2,3) + 1). Let Amin = 0.6. From Equation 4.2, taking β = 1, W ≥
2(ln(1.6) −ln(0.4))/(2(−0.4) + 1.6) = 1.1552. Let W = 2. Thus, all feedforward
connections internal to a subnetwork will receive a weight 2. Recall that all feedback
connections internal to a subnetwork will receive a weight 1. Then, the thresholds
of the hidden neurons H are calculated according to Equation 4.3, and the thresh-
olds of all the output neurons are calculated according to Equation 4.4. For exam-
ple, (H1N1) = 2((1 + 0.6) · (1 −1))/2 = 0, (□qN1) = 2((1 + 0.6) · (1 −1))/2 = 0,
(H1N2) = 2((1 + 0.6) · (0 −1))/2 = −1.6 and (♦sN1) = 2((1 + 0.6) · (1 −0))/2 =
1.6. Now, thresholds and weights for the neurons M, ∧, and ∨need to be calculated.
From the Connectionist Modal Algorithm, connections between subnetworks, for
example (♦sN1,M1N2), will receive a weight 1, the thresholds θ M of neurons M
must satisfy −1 < θ M < Amin (e.g. (M1N2) = 0.5), the thresholds θ ∨of neurons ∨
must satisfy −nAmin < θ ∨< Amin −(n−1) (e.g. (∨N1) = −1.6), and the thresholds
θ ∧of neurons ∧must satisfy n−(1+Amin) < θ ∧< nAmin (e.g. (∧N1) = 1.6).8 Fi-
nally, weights W M
L > h−1(0.6) + 2μL + θ L must be calculated.9 For example, the
output neuron ♦s in N1 has μ = 0 and θ = 1.6, and thus W M > 2.986. Similarly,
the output neuron s in N2 has μ = 1 and θ = 0, and thus W M > 3.386. Although it
is not necessary, we choose W M = 4 to unify all of the ﬁve remaining weights (see
Table 5.2).
Table 5.2 A valid set of weights for the CML network in Example 9
(∨N1,♦sN1) = 4
(M1N2,sN2) = 4
(M1N3,qN3) = 4
(♦sN1,♦sN1) = 1
(H1N2,sN2) = 2
(qN3,qN3) = 1
(♦sN1,H1N1) = 2
(M2N2,qN2) = 4
(qN3,H1N3) = 2
(H1N1,rN1) = 2
(H1N3,♦pN3) = 2
(rN1,rN1) = 1
(rN1,H2N1) = 2
(H2N1,□qN1) = 2
(∧N1,□qN1) = 4
(♦sN1,M1N2) = 1
(sN2,∨N1) = 1
(qN3,∧N1) = 1
(□qN1,M2N2) = 1
(qN2,∧N1) = 1
(□qN1,M1N3) = 1
(∨N1) = −1.6
(M1N2) = 0.5
(M1N3) = 0.5
(♦sN1) = 1.6
(H1N2) = −1.6
(qN3) = 1.6
(H1N1) = 0
(M2N2) = 0.5
(H1N3) = 0
(rN1) = 0
(sN2) = 0
(♦pN3) = 0
(H2N1) = 0
(qN2) = 1.6
(∧N1) = 1.6
(□qN1) = 0
8 Recall that n = 3 in this example.
9 Recall that W = 2 in this example; h−1(Amin) = (−1/β)ln((1−Amin)/(1+Amin)).

66
5 Connectionist Modal Logic
5.2.2 Soundness of Modal Computation
We are now in a position to show that the ensemble of neural networks N ob-
tained from the above Connectionist Modal Algorithm is equivalent to the original
extended modal program P, in the sense that N computes the modal immediate-
consequence operator MTP of P (see Deﬁnition 32).
Theorem 12. For any extended modal program P, there exists an ensemble of feed-
forward neural networks N with a single hidden layer and semilinear neurons, such
that N computes the modal ﬁxed-point operator MTP of P.
Proof. We have to show that there exists W > 0 such that the network N obtained
by the Connectionist Modal Algorithm computes MTP. Throughout, we assume that
Ni and Nj are two arbitrary subnetworks of N, representing possible worlds ωi and
ω j, respectively, such that R(ωi,ω j). We distinguish two cases: (a) clauses with
modalities □and ♦in the head, and (b) clauses with no modalities in the head.
(a) Firstly, note that clauses with □in the head must satisfy □E in Table 5.1, while
clauses with ♦in the head must satisfy ♦E. Given input vectors i and j to Ni and
N j, respectively, each neuron A in the output layer of N j is active (A > Amin) if
and only if (i) there exists a clause of Pj of the form ML1,...,MLk →A such
that ML1,...,MLk are satisﬁed by interpretation j, or (ii) there exists a clause of
Pi of the form ML1,...,MLk →□A such that ML1,...,MLk are satisﬁed by in-
terpretation i, or (iii) there exists a clause of Pi of the form ML1,...,MLk →♦A
such that ML1,...,MLk are satisﬁed by interpretation i, and the Connectionist
Modal Algorithm (step 3a) has selected Nj as the arbitrary network Nk.
(←) (i) results directly from Theorem 8. (ii) and (iii) share the same proof,
as follows. From Theorem 8, we know that if ML1,...,MLk are satisﬁed by
interpretation i, then MA is active in Ni (recall that M ∈{□,♦}). Hence, we only
need to show that MA in Ni activates A in N j. From the Connectionist Modal
Algorithm, AM is a nonlinear hidden neuron in N j. Thus, if MA is active (MA >
Amin), then AM presents an activation 1. As a result, the minimum activation of
A is h(W M
A −μAW −θ A). Now, since W M
A > h−1(Amin) + μAW + θ A, we have
h(W M
A −μAW −θ A) > Amin and, therefore, A is active (A > Amin).
(→) Directly from the Connectionist Modal Algorithm, since AM is a nonlin-
ear neuron, this neuron contributes zero to the input potential of A in Nj when
MA is not active in Ni. In this case, the behaviour of A in Nj is not affected
by Ni. Now, from Theorem 8, N j computes the ﬁxed-point operator TP j of Pj.
Thus, if ML1,...,MLk are not satisﬁed by j, then A is not active in N j.
(b) Clauses with no modalities must satisfy □I and ♦I in Table 5.1. Given input
vectors i and j to Ni and N j, respectively, each neuron □A in the output layer of
Ni is active (□A > Amin) if and only if (i) there exists a clause of Pi of the form
ML1,...,MLk →□A such that ML1,...,MLk are satisﬁed by interpretation i,
or (ii) for all N j, there exists a clause of P j of the form ML1,...,MLk →A
such that ML1,...,MLk are satisﬁed by interpretation j. Each neuron ♦A in the
output layer of Ni is active (♦A > Amin) if and only if (iii) there exists a clause

5.2 Connectionist Modal Logic
67
of Pi of the form ML1,...,MLk →♦A such that ML1,...,MLk are satisﬁed by
interpretation i, or (iv) there exists a clause of Pj of the form ML1,...,MLk →A
such that ML1,...,MLk are satisﬁed by interpretation j.
(←) (i) and (iii) result directly from Theorem 8. (ii) and (iv) are proved in
what follows. From Theorem 8, we know that if ML1,...,MLk are satisﬁed by
interpretation j, then A is active in N j. (ii) We need to show that if A is active
in every network N j (0 ≤j ≤n) to which Ni relates, □A is active in Ni. From
the Connectionist Modal Algorithm, A∧is a nonlinear hidden neuron in Ni. If
A is active (A > Amin) in N j, the minimum input potential of A∧is nAmin −θ ∧.
Now, since θ ∧< nAmin (Connectionist Modal Algorithm, step 6(c)ii), the min-
imum input potential of A∧is greater than zero and, therefore, A∧presents an
activation 1. (iv) We need to show that if A is active in at least one network
Nj (0 ≤j ≤n) to which Ni relates, ♦A is active in Ni. From the Connec-
tionist Modal Algorithm, A∨is a nonlinear hidden neuron in Ni. If A is active
(A > Amin) in N j, the minimum input potential of A∨is Amin −θ ∨. Now, since
θ ∨< Amin−(n−1) (Connectionist Modal Algorithm, step 5(c)ii), and n ⩾1, the
minimum input potential of A∨is greater than zero and, therefore, A∨presents
an activation 1. Finally, if A∧presents an activation 1, the minimum activation
of □A is h(W M
□A −μ□AW −θ □A), and, exactly as in item (a) above, □A is active
in Ni. Similarly, if A∨presents an activation 1, the minimum activation of ♦A is
h(W M
♦A −μ♦AW −θ ♦A), and, exactly as in item (a) above, ♦A is active in Ni.
(→) Again, (i) and (iii) result directly from Theorem 8. (ii) and (iv) are proved
below: (ii) We need to show that if □A is not active in Ni then at least one A is
not active in N j to which Ni relates (0 ≤j ≤n). If □A is not active, A∧presents
an activation 0. In the worst case, A is active in n−1 networks with maximum
activation 1, and not active in a single network with minimum activation −Amin.
In this case, the input potential of A∧is n −1 −Amin −θ ∧. Now, since θ ∧>
n−(1+Amin) (Connectionist Modal Algorithm, step 6(c)ii), the maximum input
potential of A∧is smaller than zero and, therefore, A∧presents an activation 0.
(iv) We need to show that if ♦A is not active in Ni, then A is not active in any
network N j to which Ni relates (0 ≤j ≤n). If ♦A is not active, A∨presents
an activation 0. In the worst case, A presents an activation −Amin in all Nj
networks. In this case, the input potential of A∨is −nAmin −θ ∨. Now, since
θ ∨> −nAmin (Connectionist Modal Algorithm, step 5(c)ii), the maximum input
potential of A∨is smaller than zero and, therefore, A∨presents an activation 0.
Finally, from Theorem 8, if A∧and A∨have activation 0, Ni computes the ﬁxed-
point operator MTPi of Pi. This completes the proof. ■
5.2.3 Termination of Modal Computation
A network ensemble can be used to compute extended modal programs in parallel
in the same way that CILP networks are used to compute logic programs. We take a
network ensemble {N1,...,Nn} obtained from the Connectionist Modal Algorithm,

68
5 Connectionist Modal Logic
and rename each input and output neuron L{□,♦}
k
in Ni (1 ≤i ≤n) as ωi : Lk, where
Lk can be either a literal or a modal literal. This basically allows us to have copies
of the literal Lk in different possible worlds (ωi,ω j,...), and to treat the occurrence
of Lk in Ni (ωi : Lk) as different from the occurrence of Lk in N j (ω j : Lk). It is
not difﬁcult to see that we are left with a large single-hidden-layer neural network
N, in which each input and output neuron is now labelled. This ﬂattened network
is a recurrent network containing feedback connections from the output layer to
the input layer, and sometimes from the output to the hidden layer. Any feedback
connection from output neurons (oj,ok,...) to a hidden neuron (hi) may be replaced
equivalently by feedback from the output to the input layer only, if we create new
input neurons ij,ik,... and connect output oj to input i j, output ok to input ik, and
so on, and then connect inputs i j,ik,... to the hidden neuron hi. As a result, as in
the case of CILP networks, if P is well behaved, the computation of P by N should
terminate.
For example, in Fig. 5.3, since ♦s and r in N1 and q in N3 are recursively con-
nected, the ensemble computes {♦s,r,□q} in ω1, {s,q} in ω2, and {q,♦s} in ω3.
As expected, these are logical consequences of the original program P given in
Example 9. Although the computation is done in parallel in N, following it by start-
ing from facts (such as s in ω2) may help verify this.
Notice how the idea of labelling the neurons, allowing multiple copies of neu-
rons Lj to occur in the neural network simultaneously, allows us to give a modal
interpretation to CILP networks as a corollary (below) of Theorem 12. Let MT n
P
de f
=
MTP(MT n−1
P
) with MT 0
P
def
= MTP({∅}). We say that an extended modal program
P is well behaved if, after a ﬁnite number m of iterations, MT m
P = MT m−1
P
.
Corollary 13. Let P be an extended modal program. There exists an ensemble of
neural networks N with semilinear neurons such that, if P is well behaved, the
computation of P by N terminates. The set name(x) ⊆BP of input and output
neurons x in N for which valuation(Act(x)) = 1 will denote MT m
P .
5.3 Case Study: The Muddy Children Puzzle
In this section, we apply CML to the muddy children puzzle, a classic example of
reasoning in multiagent environments. In contrast to the also well-known wise men
puzzle [87, 143], in which the reasoning process is sequential, here it is clear that
a distributed (simultaneous) reasoning process occurs, as follows. There is a group
of n children playing in a garden. A certain number of children k (k ≤n) have mud
on their faces. Each child can see if the others are muddy, but cannot see if they
themselves are muddy. Consider the following situation.10
A caretaker announces that at least one child is muddy (k ≥1) and asks ‘do you
know if you have mud on your face?’ To help in the understanding of the puzzle, let
us consider the cases where k = 1, k = 2, and k = 3.
10 We follow the description of the muddy children puzzle presented in [87]. We must also assume
that all the agents involved in the situation are truthful and intelligent.

5.3 Case Study: The Muddy Children Puzzle
69
If k = 1 (only one child is muddy), the muddy child answers yes in the ﬁrst
instance, since she cannot see any other muddy child. (For convenience, we assume
that all of the children are female here.) All the other children answer no in the ﬁrst
instance.
If k = 2, suppose children 1 and 2 are muddy. In the ﬁrst instance, all children
can only answer no. This allows 1 to reason as follows: ‘If 2 had said yes the ﬁrst
time round, she would have been the only muddy child. Since 2 said no, she must be
seeing someone else muddy, and since I cannot see anyone else muddy apart from
2, I myself must be muddy!’ Child 2 can reason analogously, and also answers yes
the second time round.
If k = 3, suppose children 1, 2, and 3 are muddy. Each child can only answer no
in the ﬁrst two rounds. Again, this allows 1 to reason as follows: ‘If 2 or 3 had said
yes in the second round, they would have been the only two muddy children. Thus,
there must be a third person with mud. Since I can see only 2 and 3 with mud, this
third person must be me!’ Children 2 and 3 can reason analogously to conclude as
well that yes, they are muddy.
The above cases clearly illustrate the need to distinguish between an agent’s indi-
vidual knowledge and common knowledge about the world in a particular situation.
For example, when k = 2, after everybody has said no in the ﬁrst round, it becomes
common knowledge that at least two children are muddy. Similarly, when k = 3,
after everybody has said no twice, it becomes common knowledge that at least three
children are muddy, and so on. In other words, when it is common knowledge that
there are at least k −1 muddy children, after the announcement that nobody knows
if they are muddy or not, it then becomes common knowledge that there are at least
k muddy children, for if there were k −1 muddy children all of them would have
known that they had mud on their faces. Notice that this reasoning process can only
start once it is common knowledge that at least one child is muddy, as announced
by the caretaker.11
5.3.1 Distributed Knowledge Representation in CML
Let us now formalise the muddy children puzzle in our CML framework. Typically,
the way to represent the knowledge of a particular agent is to express the idea that
an agent knows a fact α if the agent considers/thinks that α is true in every world
the agent sees as possible. In such a formalisation, a modality K j that represents the
11 The question of how to represent common knowledge in neural networks is an interesting one. In
this book, we do this implicitly – as will become clearer in what follows – by connecting neurons
appropriately as the reasoning progresses (for example, as we ﬁnd out in round two that at least two
children should be muddy). The representation of common knowledge explicitly at the object level
would require the use of neurons that are activated when ‘everybody knows’ something (serving to
implement in a ﬁnite domain the common-knowledge axioms of [87]), but this would complicate
the formalisation of the puzzle given in this chapter. This explicit form of representation and its
ramiﬁcations are worth investigating, though, and should be treated in their own right in future
work.

70
5 Connectionist Modal Logic
knowledge of an agent j is used analogously to the modality □deﬁned in Sect. 5.1.
In addition, we use pi to denote that proposition p is true for agent i, so that Kjpi
means that agent j knows that p is true for agent i. We shall omit the subscript j of
K whenever this is clear from the context. We use pi to say that child i is muddy, and
qk to say that at least k children are muddy (k ≤n). Let us consider the case in which
three children are playing in the garden (n = 3). Clause r1
1 below states that when
child 1 knows that at least one child is muddy and that neither child 2 nor child 3 is
muddy, then child 1 knows that she herself is muddy. Similarly, clause r1
2 states that
if child 1 knows that there are at least two muddy children and she knows that child
2 is not muddy, then she must also be able to know that she herself is muddy, and so
on. The clauses for children 2 and 3 are interpreted analogously.
Clauses for agent (child) 1:
r1
1: K1q1∧K1¬p2∧K1¬p3 →K1p1
r1
2: K1q2∧K1¬p2 →K1p1
r1
3: K1q2∧K1¬p3 →K1p1
r1
4: K1q3 →K1p1
Clauses for agent (child) 2:
r2
1: K2q1∧K2¬p1∧K2¬p3 →K2p2
r2
2: K2q2∧K2¬p1 →K2p2
r2
3: K2q2∧K2¬p3 →K2p2
r2
4: K2q3 →K2p2
Clauses for agent (child) 3:
r3
1: K3q1∧K3¬p1∧K3¬p2 →K3p3
r3
2: K3q2∧K3¬p1 →K3p3
r3
3: K3q2∧K3¬p2 →K3p3
r3
4: K3q3 →K3p3
Each set of clauses rl
m (1 ≤l ≤n, m ∈N) was implemented in a CILP network.
Figure 5.4 shows the implementation of clauses r1
1 to r1
4 (for agent 1).12 In addition,
it contains p1 and Kq1, Kq2, and Kq3, all represented as facts.13 This is indicated
by the grey highlighting in Fig. 5.4. This setting complies with the presentation of
the puzzle given in [143], in which snapshots of the evolution of knowledge from
round to round are taken in order to logically deduce the solution of the problem
without the addition of a time variable. Here, p1 and Kqk (1 ≤k ≤3) are obtained
from the network’s input, which denotes a snapshot in the computation (i.e. a partic-
ular round), whereas K¬p2 and K¬p3 are obtained from the other networks in the
ensemble (representing agents 2 and 3, respectively, whenever agent 1 does not see
mud on their foreheads). A complete solution to the puzzle would require the repli-
cation of the ensemble presented here for all time points according to the various
12 Note that Kpi and K¬pi should be represented by two different input neurons [62]. Negative
weights in the network then allow one to differentiate between Kpi and ∼Kpi, and between K¬pi
and ∼K¬pi, respectively. This can easily be veriﬁed by renaming K¬pi as a new literal Kp′
i.
13 Note the difference between p1 (child 1 is muddy) and Kp1 (child 1 knows she is muddy).

5.3 Case Study: The Muddy Children Puzzle
71
p1
K¬p2
K¬p3
Kq1
Kq2
Kq3
K¬p2
K¬p3
Kq1
Kq2
Kq3
Kp1
. . .
. . .
Agent 1
Fig. 5.4 Implementation of the rules {r1
1,...,r1
4}
rounds of computation. This would produce a two-dimensional network ensemble,
where in one dimension we have agents (as depicted here) and in the other we have
time, so that we can represent the evolution of the agents’ knowledge across the time
points explicitly [70]. We shall consider the full version of the puzzle with a time
variable in Chap. 6.
Figure 5.5 illustrates the interaction between three agents in the muddy children
puzzle. The arrows connecting CILP networks implement the fact that when a child
is muddy, the other children can see her. For the sake of clarity, the clauses r1
m,
corresponding to the neuron K1p1, are shown only in Fig. 5.4. The clauses r2
m and
r3
m for K2p2 and K3p3 would be represented analogously in similar CILP networks.
This is indicated in Fig. 5.5 by neurons highlighted in black. In addition, Fig. 5.5
shows only positive information about the problem. Recall that negative information
such as ¬p1, K¬p1, and K¬p2 is to be added explicitly to the network, as shown in
Fig. 5.4.
5.3.2 Learning in CML
As discussed in the introduction to this chapter, one of our objectives when devel-
oping neural-symbolic learning systems is to retain good learning capability while
seeking to develop systems that can deal with more expressive languages such as
modal logic.
In the work described in this section, we used the Connectionist Modal Algorithm
given in Sect. 5.2.1 to perform a translation from the modal background knowledge

72
5 Connectionist Modal Logic
agent 2
agent 3
agent 1
p1
Kp2
Kp3
Kq1
Kq2
Kq3
Kp1
p2
Kp2
Kp3
Kq1
Kq2
Kq3
Kp1
 Kp2
Kp3
Kq1
Kq2
Kq3
Kp1
p3
Fig. 5.5 Interaction between agents in the muddy children puzzle
to the initial architecture of the ensemble. We then used standard backpropagation to
train each network of the ensemble with examples.14 Our aim was to verify whether
a particular agent i can learn from examples whether she is muddy or not, i.e. learn
clauses ri
1 to ri
4 above.
We performed two sets of experiments using the muddy children puzzle in order
to compare learning with background knowledge and without background knowl-
edge. In the ﬁrst set of experiments, we created networks with random weights, to
which we then presented a number of training examples. In the second set of ex-
periments, we inserted a clause ri
1 : K1q1∧K1¬p2∧K1¬p3 →K1p1 as background
knowledge before training the networks with examples.15 Each training example
stated whether agent i was muddy or not, according to the truth values of literals
Kiq1, Kiq2, Kiq3, Kip1, Ki¬p1, Kip2, Ki¬p2, Kip3, Ki¬p3 (represented as input
neurons).
14 Recall that each network in the ensemble is a CILP network and, therefore, can be trained with
examples using standard backpropagation.
15 Note that the rule ri
1 works as a base rule for induction in the muddy children puzzle.

5.4 Discussion
73
We evaluated the networks using cross-validation (see Sect. 3.4 for a deﬁnition).
In both experiments, we used eightfold cross-validation over a set of 32 examples.
In addition, we used a learning rate η = 0.2, a term of momentum ξ = 0.1, h(x) =
2/1+e−βx −1 as the activation function, and bipolar inputs in {−1,1}.
The training sets were presented to the networks for 10000 epochs and the
weights were updated, as usual, after every epoch. For each experiment, this resulted
in eight networks being trained with 28 examples, with four examples reserved for
testing. All 16 networks reached a training-set error Err(W) (according to Equa-
tion 3.3) smaller than 0.01 before 10000 epochs had elapsed. In other words, all
the networks were trained successfully. Recall that learning takes place locally in
each network. The connections between networks in the ensemble are deﬁned by
the rules of natural deduction for modalities presented in Sect. 5.1 and are ﬁxed.
As for the networks’ generalisation performance, the results corroborated the im-
portance of exploiting any available background knowledge (assuming that the
background knowledge is correct, of course). In the ﬁrst experiment, in which the
connectionist modal system was trained with no background knowledge, the net-
works presented an average test set accuracy of 84.37%. In the second experiment,
in which the clause ri
1 was added to the networks prior to training, an average test
set accuracy of 93.75% was obtained under exactly the same training conditions.
5.4 Discussion
In this chapter, we have presented connectionist modal logics (CML), a connec-
tionist computational model for modal logic. We have introduced algorithms which
translate extended modal programs into ensembles of neural networks [66,80], and
proved that these ensembles compute ﬁxed-point semantics of the programs. The
computations always terminate when the program is well behaved, and thus the net-
work ensembles can be used as a distributed computational model for modal logic.
In addition, the ensembles can learn possible-world representations from examples
by using standard learning algorithms such as backpropagation. Finally, we have
applied the CML system to the muddy children puzzle, a well-known benchmark
for distributed knowledge representation. We have both set up and trained networks
to reason about this puzzle.
CML opens up a new area of research in which modal reasoning can be repre-
sented and learned using artiﬁcial neural networks. There are several avenues of re-
search to be pursued as a result. For instance, an important aspect of neural-symbolic
learning systems – not dealt with in this chapter – is rule extraction from neural-
network ensembles [65, 273]. In the case of CML, rule extraction methods would
need to consider a more expressive knowledge representation language. Since we
have shown that modalities can be represented by network ensembles, one should
expect, when extracting rules from a given trained ensemble, that rules with modal-
ities will offer a better representation formalism for the ensemble, in terms of either
rule comprehensibility or rule expressiveness.

74
5 Connectionist Modal Logic
Extensions of CML include the study of how to represent other modal logics
such as temporal logic (as shown in Chap. 6), and other nonclassical logics such
as dynamic logic [124] and conditional logics of normality [41], as well as infer-
ence and learning of (fragments of ) ﬁrst-order modal logic [15]. The addition of a
time variable to the approach presented here allows the representation of knowledge
evolution. This can be implemented using labelled transitions from one knowledge
state to the next, with a linear time ﬂow, where each time point is associated with a
state of knowledge, i.e. a network ensemble. This point is made clear in Chap. 6.
One could also think of the system presented here as a ﬁrst step towards a model
construction algorithm, which in turn would allow investigations into model check-
ing of distributed reasoning systems in a connectionist setting. Alternatively, CML
can be seen as a starting point towards the construction of a connectionist theorem
prover for modal logic, which can be implemented in hardware as a neural network.
In summary, we believe that CML contributes to addressing the need for integrated
distributed knowledge representation, computation, and learning mechanisms in ar-
tiﬁcial intelligence, computing, and cognitive science.

Chapter 6
Connectionist Temporal Reasoning
In this chapter, following the formalisation of connectionist modal logics (CML)
presented in Chap. 5, we show that temporal and epistemic logics can be effectively
represented in and combined with artiﬁcial neural networks, by means of temporal
and epistemic logic programming. This is done by providing a translation algorithm
from temporal-logic theories to the initial architecture of a neural network. A the-
orem then shows that the given temporal theory and the network are equivalent in
the usual sense that the network computes a ﬁxed-point semantics of the theory.
We then describe a validation of the Connectionist Temporal Logic of Knowledge
(CTLK) system by applying it to a distributed time and knowledge representation
problem, the full version of the muddy children puzzle [87]. We also describe exper-
iments on learning in the muddy children puzzle, showing how knowledge evolution
can be analysed and understood in a learning model.
As an extension of CML that includes temporal operators, CTLK provides a com-
bined (multimodal) connectionist system of knowledge and time. This allows the
modelling of evolving situations such as changing environments or possible worlds,
and the construction of a connectionist model for reasoning about the temporal
evolution of knowledge. These temporal-logic features, combined with the com-
putational power of neural networks, leads towards a rich neural-symbolic learning
system, where various forms of nonclassical reasoning are naturally represented,
derived, and learned. Hence, the approach presented here extends the representa-
tion power of artiﬁcial neural networks beyond the classical level, by dealing with
temporal and epistemic logics.
This chapter is organised as follows. In Sect. 6.1, we recall some useful pre-
liminary concepts related to temporal reasoning, present the CTLK system (and
introduce the Temporal Algorithm, which translates temporal-logic programs into
artiﬁcial neural networks), and prove that the translation is correct. In Sect. 6.2, we
describe the use of CML and CTLK to tackle the muddy children puzzle, and com-
pare the solutions provided by each system. In Sect. 6.3, we conclude and discuss
directions for future work.
A.S. d’Avila Garcez et al., Neural-Symbolic Cognitive Reasoning, Cognitive Technologies,
75
c⃝Springer-Verlag Berlin Heidelberg 2009

76
6 Connectionist Temporal Reasoning
6.1 Connectionist Temporal Logic of Knowledge
Temporal logic and its combination with other modalities such as knowledge and
belief operators have been the subject of intensive investigation [87,106,129]. Tem-
poral logic has evolved from philosophical logic to become one of the main logical
systems used in computer science and artiﬁcial intelligence [87,89,103,207]. It has
been shown to be a powerful formalism for the modelling, analysis, veriﬁcation, and
speciﬁcation of distributed systems [32,33,87,123,157]. Further, in logic program-
ming, several approaches to dealing with temporal reasoning have been developed,
leading to application in databases, knowledge representation, and the speciﬁcation
of systems (see e.g. [88,197,199]).
In this chapter, we show how temporal-logic programs can be expressed in a con-
nectionist setting in conjunction with a knowledge operator. We do so by extending
CML into the connectionist temporal logic of knowledge (CTLK), which allows the
speciﬁcation of the evolution of knowledge through time in network ensembles. In
what follows, we present a temporal algorithm, which translates temporal programs
into neural networks, and a theorem showing that the temporal theory and the net-
work ensemble are equivalent, and therefore that the translation is correct. Let us
start by presenting a simple example.
Example 10. (Next-Time Operator) One of the typical axioms of the temporal
logics of knowledge is Ki ⃝α →⃝Kiα [123], where ⃝denotes the next-time
(tomorrow) temporal operator. This means that what an agent i knows today (Ki)
about tomorrow (⃝α), he or she still knows tomorrow (⃝Kiα). In other words,
this axiom states that an agent does not forget what he or she knew. This can be
represented in an ensemble of CILP networks with the use of a network that rep-
resents the agent’s knowledge today, a network that represents the agent’s knowl-
edge tomorrow, and the appropriate connections between the networks. Clearly, an
output neuron K⃝α of a network that represents agent i at time t needs to be con-
nected to an output neuron Kα of a network that represents agent i at time t + 1
in such a way that, whenever K⃝α is activated, Kα is also activated. This is il-
lustrated in Fig. 6.1, where the black circle denotes a neuron that is always ac-
tivated, and the activation value of output neuron K⃝α is propagated to output
neuron Kα.
Generally speaking, the idea behind a connectionist temporal logic is to have
(instead of a single ensemble) a number n of ensembles, each representing the
knowledge held by a number of agents at a given time point. Figure 6.2 illustrates
how this dynamic feature can be combined with the symbolic features of the knowl-
edge represented in each network, allowing not only analysis of the current state
(the current possible world or time point), but also analysis of how knowledge
changes with time.

6.1 Connectionist Temporal Logic of Knowledge
77
Agent i at t+1
Agent i at t
KOα
Kα
Fig. 6.1 Simple example of connectionist temporal reasoning
t1
t2
t3
Agent 1
Agent 2
Agent 3
Agent 1
Agent 2
Agent 3
Agent 1
Agent 2
Agent 3
Fig. 6.2 Evolution of knowledge with time
6.1.1 The Language of CTLK
In order to reason over time and represent knowledge evolution, we have combined
temporal logic programming [197] and the knowledge operator Ki into the Connec-
tionist Temporal Logic of Knowledge. The implementation of Ki is analogous to
that of □; we treat Ki as a universal modality, as done in [87]. This will become
clearer when we apply a temporal operator and Ki to the muddy children puzzle in
Sect. 6.2.

78
6 Connectionist Temporal Reasoning
Deﬁnition 33 (Connectionist Temporal Logic). The language of CTLK contains:
1. A set {p,q,r,...} of primitive propositions.
2. A set of agents A = {1,...,n}.
3. A set of connectives Ki (i ∈A), where Kip reads ‘agent i knows p.’
4. The temporal operator ⃝(next time).
5. A set of extended modal-logic clauses of the form t : ML1,...,MLn →MLn+1,
where t is a label representing a discrete time point at which the associated clause
holds, M ∈{□,♦}, and Lj (1 ≤j ≤n+1) is a literal.
We consider the case of a linear ﬂow of time. As a result, the semantics of CTLK
requires that we build models in which possible states form a linear temporal rela-
tionship. Moreover, with each time point, we associate the set of formulas holding
at that point by use of a valuation map. The deﬁnitions are as follows.
Deﬁnition 34 (Timeline). A timeline T is a sequence of ordered points, each cor-
responding to a natural number.
Deﬁnition 35 (Temporal Model). A model M is a tuple M = (T,R1,...,Rn,π),
where (i) T is a (linear) timeline; (ii) Ri (i ∈A ) is an agent’s accessibility relation
over T, and (iii) π : T →ϕ is a map associating a set π(p) of time points in T with
each propositional variable of CTLK.
The truth conditions for CTLK’s well-formed formulas are then deﬁned by the
following satisﬁability relation.
Deﬁnition 36 (Satisﬁability of Temporal Formulas). Let M = ⟨T,Ri,π⟩be a tem-
poral model for CTLK. The satisﬁability relation |= is uniquely deﬁned as follows:
(i) (M,t) |= p iff t ∈π(p);
(ii) (M,t) |= ¬α iff (M,t)̸ |=α;
(iii) (M,t) |= α ∧β iff (M,t) |= α and (M,t) |= β;
(iv) (M,t) |= α ∨β iff (M,t) |= α or (M,t) |= β;
(v) (M,t) |= α →β iff (M,t)̸ |=α or (M,t) |= β;
(vi) (M,t) |= □α iff for all u ∈T, if R(t,u) then (M,u) |= α;
(vii) (M,t) |= ♦α iff there exists a u such that R(t,u) and (M,u) |= α;
(viii) (M,t) |= ⃝α iff (M,t +1) |= α;
(ix) (M,t) |= Kiα iff for all u ∈T, if Ri(t,u), then (M,u) |= α.
Since every clause is labelled by a time point t ranging from 1 to n, if ⃝A holds
at time point n our timeline will have n+1 time points; otherwise, it will contain n
time points. Some results provided by [44,88,198,199] for temporal extensions of
logic programming apply directly to CTLK. The following deﬁnitions are needed to
express the computation of CTLK in neural networks.
Deﬁnition 37 (Temporal Clause). A clause of the form t : ⃝L1,...,⃝Lo →
⃝Lo+1 is called a CTLK temporal clause, which holds at time point t, where Lj
(1 ≤j ≤o+1) is either a literal, a modal literal, or of the form KiLj (i ∈A).

6.1 Connectionist Temporal Logic of Knowledge
79
Deﬁnition 38 (Temporal Immediate-Consequence Operator ⃝TP). Let P =
{P1,...,Pk} be a CTLK temporal-logic program (i.e. a ﬁnite set of CTLK temporal
clauses). The mapping ⃝TPi : 2BP →2BP at time point ti (1 ≤i ≤k) is deﬁned as
follows: ⃝TPi(I) = {L ∈BP | either (i) or (ii) or (iii) below holds}. (i) There exists a
clause ti−1 : L1,...,Lm →⃝L in Pi−1, and {L1,...,Lm} is satisﬁed by an interpreta-
tion J for Pi−1;1 (ii) L is qualiﬁed by ⃝, there exists a clause ti+1 : L1,...,Lm →L in
Pi+1, and {L1,...,Lm} is satisﬁed by an interpretation J for Pi+1; (iii) L ∈MTPi(I).
A global temporal immediate-consequence operator can be deﬁned as ⃝TP
(I1,...,Ik) = k
j=1{⃝TP j}.
6.1.2 The CTLK Algorithm
In this section, we present an algorithm to translate temporal-logic programs into
(two-dimensional) neural-network ensembles. We consider temporal clauses and
make use of CML’s Connectionist Modal Algorithm and CILP’s Translation Al-
gorithm. The Temporal Algorithm is concerned with how to represent the next-
time connective ⃝and the knowledge operator K, which may appear in clauses
of the form ti : ⃝KaL1,...,⃝KbLo →⃝KcLo+1, where a,b,c,..., are agents and
1 ≤i ≤n. In such clauses, we extend a normal clause of the form L1,...,Lo →Lo+1
to allow the quantiﬁcation of each literal with a knowledge operator indexed by dif-
ferent agents {a,b,c,...} varying from 1 to m. We also label the clause with a time
point ti on our time scale, varying from 1 to n, and we allow the use of the next-
time operator on the left-hand side of the knowledge operator.2 For example, the
clause t1 : Kjα ∧Kkβ →⃝Kjγ states that if agent j knows α and agent k knows β
at time t1, then agent j knows γ at time t2. The CTLK algorithm is presented below,
where Nk,t denotes a CILP neural network for agent k at time t.
Let q denote the number of clauses occurring in P. Let ol denote the number of
literals in the body of clause l. Let μl denote the number of clauses in P with the
same consequent, for each clause l. Let h(x) = 2/(1+e−βx)−1, where β ∈(0,1).
As usual, let Amin be the minimum activation for a neuron to be considered
‘active’ (or true); Amin ∈(0,1). Set Amin > (MAXP(o1,...,oq,μ1,...,μq) −1)/
(MAXP(o1,...,oq,μ1,...,μq) + 1). Let W and −W be the weights of con-
nections associated with positive and negative literals, respectively. Set W ≥
(2/β)·(ln(1+Amin)−ln(1−Amin))/(MAXP(o1,...,oq,μ1,...,μq) · (Amin −1)+
Amin +1)).
1 Notice that this deﬁnition implements a credulous approach in which every agent is assumed to
be truthful and, therefore, every agent believes not only what he or she knows about tomorrow, but
also what he or she is told by other agents about tomorrow. A more sceptical approach could be
implemented by restricting the derivation of ⃝A to interpretations in Pi only.
2 Recall that, according to Deﬁnition 36, if ⃝A is true at time t, and t is the last time point n, the
CTLK algorithm will create n+1 points, as described here.

80
6 Connectionist Temporal Reasoning
Temporal Algorithm
For each time point t (1 ≤t ≤n) in P, and for each agent k (1 ≤k ≤m) in P, do:
1. For each clause l in P containing ⃝KkLi in the body:
(a) Create an output neuron ⃝KkLi in Nk,t (if it does not exist yet).
(b) Create an output neuron KkLi in Nk,t+1 (if it does not exist yet).
(c) Deﬁne the thresholds of ⃝KkLi and KkLi as θ = (1+Amin)·(1−μl)·W/2.
(d) Set h(x) as the activation function of output neurons ⃝KkLi and KkLi.
(e) Add a hidden neuron L∨to Nk,t and set the step function as the activation
function of L∨.
(f) Connect KkLi in Nk,t+1 to L∨and set the connection weight to 1.
(g) Set the threshold θ ∨of L∨such that −mAmin < θ ∨< Amin −(m−1).3
(h) Connect L∨to ⃝KkLi in Nk,t and set the connection weight to W M such that
W M > h−1(Amin)+ μlW +θ.
2. For each clause in P containing ⃝KkLi in the head:
(a) Create an output neuron ⃝KkLi in Nk,t (if it does not exist yet).
(b) Create an output neuron KkLi in Nk,t+1 (if it does not exist yet).
(c) Deﬁne the thresholds of ⃝KkLi and KkLi as θ = (1+Amin)·(1−μl)·W/2.
(d) Set h(x) as the activation function of ⃝KkLi and KkLi.
(e) Add a hidden neuron L⃝to Nk,t+1 and set the step function as the activation
function of L⃝.
(f) Connect ⃝KkLi in Nk,t to L⃝and set the connection weight to 1.
(g) Set the threshold θ ⃝of L⃝such that −1 < θ ⃝< Amin.4
(h) Connect L⃝to KkLi in Nk,t+1 and set the connection weight to W M such that
W M > h−1(Amin)+ μlW +θ.
3. Call CML’s algorithm (Sect. 5.2.1).
Theorem 13 below shows that the network ensemble N obtained from the above
Temporal Algorithm is equivalent to the original CTLK program P in the sense that
N computes the temporal immediate-consequence operator ⃝TP of P (Deﬁnition
38). The theorem makes use of Theorems 8 (Chap. 4) and 12 (Chap. 5).
Theorem 13 (Correctness of CTLK). For any CTLK program P, there exists an
ensemble of single-hidden-layer neural networks N such that N computes the tem-
poral ﬁxed-point operator ⃝TP of P.
Proof. We need to show that KkLi is active in Nt+1 if and only if either (i) there
exists a clause of P of the form ML1,...,MLo →KkLi such that ML1,...,MLo are
satisﬁed by an interpretation (input vector), or (ii) ⃝KkLi is active in Nt. Case (i)
follows from Theorem 8. The proof of Case (ii) follows from Theorem 12, as the
algorithm for ⃝is a special case of the algorithm for ♦in which a more careful
selection of world (i.e. t +1) is made when applying the ♦elimination rule.
■
3 A maximum number of m agents make use of L∨.
4 A maximum number of one agent makes use of L⃝.

6.2 The Muddy Children Puzzle (Full Solution)
81
6.2 The Muddy Children Puzzle (Full Solution)
In this section, we describe the application of the CTLK system to the muddy
children puzzle, a classic example of reasoning in multiagent environments (see
Sect. 5.3 for a description). We also compare the CTLK solution with our previous
(CML-based) solution, which uses snapshots of time instead of a time ﬂow.
6.2.1 Temporal Knowledge Representation
The addition of a temporal variable to the muddy children puzzle allows one to rea-
son about the knowledge acquired after each round. For example, assume as before
that there are three muddy children playing in the garden. They all answer no when
asked the ﬁrst time if they know whether they are muddy or not. Moreover, as each
muddy child can see the other children, they will reason as previously described,
and answer no in the second round, reaching the correct conclusion at round three.
This solution requires, at each round, that the CILP networks be expanded with the
knowledge acquired from reasoning about what is seen and what is heard by each
agent. This clearly requires each agent to reason about time. The snapshot solution
should then be seen as representing the knowledge held by the agents at an arbitrary
time t. The knowledge held by the agents at time t + 1 would then be represented
by another set of CILP networks appropriately connected to the original set of net-
works. Let us consider again the case in which k = 3. There are alternative ways of
modelling this, but one possible representation is as follows:
Temporal Rules for agent (child) 1:
t1 : ¬K1p1 ∧¬K2p2 ∧¬K3p3 →⃝K1q2
t2 : ¬K1p1 ∧¬K2p2 ∧¬K3p3 →⃝K1q3
Temporal Rules for agent (child) 2:
t1 : ¬K1p1 ∧¬K2p2 ∧¬K3p3 →⃝K2q2
t2 : ¬K1p1 ∧¬K2p2 ∧¬K3p3 →⃝K2q3
Temporal Rules for agent (child) 3:
t1 : ¬K1p1 ∧¬K2p2 ∧¬K3p3 →⃝K3q2
t2 : ¬K1p1 ∧¬K2p2 ∧¬K3p3 →⃝K3q3
In addition, the snapshot rules are still necessary here to assist each agent’s rea-
soning at any particular time point. Finally, the interaction between the agents, as
depicted in Fig. 5.5, is also necessary here to model the fact that each child will
know that another child is muddy when they see each other, analogously to the □
modality. This can be represented as ti : p1 →K2p1 and ti : p1 →K3p1 for times
i = 1,2,3, and analogously for p2 and p3. Together with ti : ¬p2 →K1¬p2 and
ti : ¬p3 →K1¬p3, also for times i = 1,2,3, and analogously for K2 and K3, this
completes the formalisation.

82
6 Connectionist Temporal Reasoning
K3p3
K3p3
Agent 1 at t1
Agent 1 at t2
Agent 1 at t3
K2p2
K2p2
Kp3
p1
Kp2
Kq1
Kq2
Kq3
Kp1
Kp3
p1
Kp2
Kq1
Kq2
Kq2
Kq3
Kq3
Kq2
Kp1
Kp3
p1
Kp2
Kq1
Kq2
Kq3
Kp1
Kp1
Kq3
K¬p2
K¬p3
K¬p2
K¬p3
Kq1
Kp1
Fig. 6.3 The evolution in time of an agent’s knowledge in the muddy children puzzle
The rules above, the temporal rules, and the snapshot rules for agent (child) 1
are described, following the Temporal Algorithm, in Fig. 6.3, where dotted lines
indicate negative weights and solid lines indicate positive weights. The network
of Fig. 6.3 provides a complete solution to the muddy children puzzle. It is worth
noting that each network remains a simple single-hidden-layer neural network that
can be trained with the use of standard backpropagation or some other off-the-shelf
learning algorithm.

6.2 The Muddy Children Puzzle (Full Solution)
83
6.2.2 Learning in CTLK
The Temporal Algorithm introduced in this chapter allows one to perform theory
and data learning in neural networks when the theory includes temporal knowledge.
In this section, we describe the use of the Temporal Algorithm and standard back-
propagation to compare learning from data only and learning from theory and data
with temporal background knowledge. Since we show that there is a relationship
between temporal and epistemic logics and artiﬁcial-neural-network ensembles, we
should also be able to learn epistemic and temporal knowledge in the ensemble
(and, indeed, to perform knowledge extraction of revised temporal and epistemic
rules after learning, but this is left as future work).
We trained two ensembles of CTLK neural networks to compute a solution to
the muddy children puzzle. To one of them we added temporal and epistemic back-
ground knowledge in the form of a single rule, t1 : ¬K1p1 ∧¬K2p2 ∧¬K3p3 →
⃝K1q2, by applying the Temporal Algorithm. To the other, we did not add any rule.
We then compared the average accuracies of the ensembles. We considered, in par-
ticular, the case in which agent 1 is to decide whether or not she is muddy at time
t2. Each training example expressed the knowledge held by agent 1 at t2, according
to the truth values of the atoms K1¬p2, K1¬p3, K1q1, K1q2, and K1q3. As a result,
we had 32 examples containing all possible combinations of truth values for input
neurons K1¬p2, K1¬p3, K1q1, K1q2, and K1q3, where an input value 1 indicates a
truth value true, and an input −1 indicates a truth value false. For each example,
we were concerned with whether or not agent 1 would know that she was muddy,
that is, we were concerned with whether or not output neuron K1p1 was active. For
example, if the inputs ware false (input vector [−1,−1,−1,−1,−1]), then agent
1 should not know whether she was muddy (K1p1 is false). If, however, K1q2 was
true and either K1¬p2 or K1¬p3 was true, then agent 1 should be able to recognise
that she was indeed muddy (K1p1 is true).
From the description of the muddy children puzzle, we know that at t2, K1q2
should be true (i.e. K1q2 is a fact). This information can be derived from the tempo-
ral rule given as background knowledge above, but not from the training examples.
Although the background knowledge can be changed by the training examples, it
places a bias towards certain combinations (in this case, the examples in which K1q2
is true), and this may produce better performance, typically when the background
knowledge is correct. This effect has been observed, for instance, in [250] in exper-
iments on DNA sequence analysis, in which background knowledge was expressed
by production rules. The set of examples was noisy, and background knowledge
counteracted the noise and reduced the chance of overﬁtting.
We evaluated the two CTLK ensembles using eightfold cross-validation, so that,
at each time, four examples were left for testing. We used a learning rate η = 0.2, a
term of momentum α = 0.1, an activation function h(x) = tanh(x), and bipolar in-
puts in {−1,1}. For each training task, the training set was presented to the network
for 10000 epochs. For both ensembles, the networks reached a training-set error
smaller than 0.01 before 10000 epochs had elapsed, i.e. the networks were trained
successfully.

84
6 Connectionist Temporal Reasoning
As for the networks’ test set performance, the results corroborate the importance
of exploiting any available background knowledge. For the ﬁrst ensemble, in which
the networks were trained with no background knowledge, an average test set accu-
racy of 81.25% was obtained. For the second ensemble, to which the temporal rule
had been added, an average test set accuracy of 87.5% was obtained – a noticeable
difference in performance, considering there was only a single rule in the back-
ground knowledge. In both cases, exactly the same training parameters were used.
The experiments above illustrate simply that the merging of temporal background
knowledge and data learning may provide a system that is more effective than a
purely connectionist system. The focus of this chapter has been on the reasoning
capabilities of neural-symbolic systems. More extensive experiments to validate the
system proposed here would be useful, and could be carried out in connection with
knowledge extraction, and using applications containing continuous attributes.
6.3 Discussion
We have shown in Chap. 5 and the present chapter that temporal and modal log-
ics provide neural-symbolic learning systems with the ability to make use of more
expressive representation languages. In his seminal paper [253], Valiant argued for
rich logic-based knowledge representation mechanisms in learning systems. The
connectionist model proposed here addresses such a need, but still complies with
important principles of connectionism such as massive parallelism and learning.
A very important feature of our system is the temporal dimension, which can be
combined with an epistemic dimension for knowledge/beliefs; this chapter has de-
scribed how to integrate such dimensions into a neural-symbolic system. We have
illustrated this by providing a full solution to the muddy children puzzle, where
agents can reason about the evolution of knowledge in time. Further developments
of the framework presented here, including an application in concurrent program-
ming, can be found in [157].
Although a number of multimodal systems – for example systems combin-
ing knowledge and time [106, 123] and combining beliefs, desires, and intentions
[219] – have been proposed for distributed knowledge representation and reason-
ing in multiagent systems, little attention has been paid to the integration of a
learning component for knowledge acquisition. We seek to bridge this gap by al-
lowing temporal knowledge representation to be integrated with a neural learning
system. One could also think of the system presented here as a massively paral-
lel distributed system where each ensemble (or set of ensembles) can be seen as
a neural-symbolic processor. This would open up several interesting research av-
enues. For instance, one could investigate how to reason about protocols and actions
in this neural-symbolic distributed system, or how to train the processors in order to
learn how to preserve the security of such systems. The connectionist temporal and
knowledge logic presented here allows the representation of a variety of properties,

6.3 Discussion
85
such as knowledge-based speciﬁcations, in the style of [87]. These speciﬁcations
are frequently represented using temporal and modal logics, but without a learning
feature, which comes naturally in CTLK.
Finally, since the models of the modal logic S4 can be used to model intuition-
istic modal logic, we can propose a system that can combine reasoning about time
with learning intuitionistic theories, as is shown in Chap. 7. This, per se, is an inter-
esting result, as the proposed models can then be used to ‘think’ constructively, in
the sense of Brouwer [106]. In summary, we believe that the connectionist tempo-
ral and epistemic computational model presented here opens up several interesting
research avenues in the domain of neural-symbolic integration, allowing the distrib-
uted representation, computation, and learning of expressive knowledge representa-
tion formalisms.

Chapter 7
Connectionist Intuitionistic Reasoning
In this chapter, we present a computational model combining intuitionistic reasoning
and neural networks. We make use of ensembles of neural networks to represent
intuitionistic theories, and show that for each intuitionistic theory and intuitionistic
modal theory, there exists a corresponding neural-network ensemble that computes
a ﬁxed-point semantics of the theory. This provides a massively parallel model for
intuitionistic reasoning. As usual, the neural networks can be trained from examples
to adapt to new situations using standard neural learning algorithms, thus providing
a unifying foundation for intuitionistic reasoning, knowledge representation, and
learning.
Intuitionistic logical systems have been advocated by many as providing ade-
quate logical foundations for computation [5, 10, 55, 90, 150, 172, 206]. We argue,
therefore, that intuitionism should also play an important part in neural compu-
tation. In this chapter, we follow the research path of Chaps. 5 and 6 to develop a
neural-symbolic computational model for integrated reasoning about, representation
of, and learning of intuitionistic knowledge. We concentrate on issues of reasoning
and knowledge representation, which set the scene for connectionist intuitionistic
learning, since effective knowledge representation precedes any learning algorithm.
Nevertheless, we base the representation on standard, simple neural-network archi-
tectures, aiming at further developments in effective, experimental learning.
In order to compute intuitionistic knowledge in a connectionist framework, we
set up ensembles of CILP networks (see Chap. 4) to compute a ﬁxed-point seman-
tics of intuitionistic programs (or intuitionistic modal programs). The networks are
set up by a Connectionist Intuitionistic Algorithm or a Connectionist Intuitionistic
Modal Algorithm, both introduced in this chapter. The proofs that the algorithms
produce neural networks that compute the ﬁxed-point semantics of their associated
intuitionistic theories are then given. The networks can be trained from examples
with the use of standard learning algorithms.
The chapter is organised as follows. In Sect. 7.1, we present the basic concepts of
intuitionistic reasoning used throughout the chapter. In Sect. 7.2, we introduce the
Connectionist Intuitionistic Algorithm, which translates intuitionistic theories into
neural networks, and prove that these networks compute a ﬁxed-point semantics
A.S. d’Avila Garcez et al., Neural-Symbolic Cognitive Reasoning, Cognitive Technologies,
87
c⃝Springer-Verlag Berlin Heidelberg 2009

88
7 Connectionist Intuitionistic Reasoning
of the given intuitionistic theory (thus showing that the translation is correct). In
Sect. 7.3, we introduce the Connectionist Intuitionistic Modal Algorithm, which
translates intuitionistic modal theories into neural networks, and prove that these
networks compute a ﬁxed-point semantics of the given intuitionistic modal theory.
Section 7.4 discusses directions for future work.
7.1 Intuitionistic Logic and Programs
Intuitionistic logic is considered by many authors to provide adequate logical foun-
dations for computation [1, 10, 55, 90, 206, 236]. Intuitionistic logic was developed
originally by Brouwer, and later by Heyting and Kolmogorov (see [258] for a his-
torical account). In intuitionistic logic, a statement that there exists a proof of a
proposition x is only made if there is a constructive method for the proof of x. One
of the consequences of Brouwer’s ideas is the rejection of the law of the excluded
middle, namely α ∨¬α, since one cannot always state that there is a proof of α
or its negation, as accepted in classical logic and in (classical) mathematics. The
development of these ideas and applications of them in mathematics have led to
developments in constructive mathematics and have inﬂuenced several lines of re-
search on logic and computing science [10,172,258].
An intuitionistic language L includes propositional letters (atoms) p,q,r..., the
connectives ¬, ∧, and an intuitionistic implication ⇒. Formulas are denoted by
α,β,γ,... We interpret this language using a Kripke-style semantics as in [28, 42,
106], which we deﬁne as follows.
Deﬁnition 39 (Kripke Models for Intuitionistic Propositional Logic). Let L be
an intuitionistic language. A model for L is a tuple M = ⟨Ω,R,v⟩where Ω is a set
of points, v is a mapping that assigns to each ω ∈Ω a subset of the atoms of L, and
R is a reﬂexive, transitive binary relation over Ω, such that:
1. (M,ω) |= p iff p ∈v(ω) (for atom p).
2. (M,ω) |= ¬α iff, for all ω′ such that R(ω,ω′), (M,ω′) ̸ |= α.
3. (M,ω) |= α ∧β iff (M,ω) |= α and (M,ω) |= β.
4. (M,ω) |= α ⇒β iff, for all ω′ with R(ω,ω′), we have (M,ω′) |= β whenever
we have (M,ω′) |= α.
We now deﬁne labelled intuitionistic programs as sets of intuitionistic clauses
where each clause is labelled by the point at which it holds, similarly to Gabbay’s
labelled deductive systems [42,99].
Deﬁnition 40 (Labelled Intuitionistic Program). A labelled intuitionistic program
is a ﬁnite set of clauses C of the form ωi : A1,...,An ⇒A0 (where ‘,’ is an abbrevi-
ation for ‘∧’, as usual), and a ﬁnite set of relations R between points ωi (1 ≤i ≤m)
in C, where the Ak (0 ≤k ≤n) are atoms and ωi is a label representing a point at
which the associated clause holds.

7.1 Intuitionistic Logic and Programs
89
In what follows, we deﬁne a model-theoretic semantics for labelled intuitionistic
programs. Throughout, we are concerned with propositional labelled intuitionistic
programs, as deﬁned above.
When computing the semantics of the program, we have to consider both the
ﬁxed point at a particular point where a clause holds and the ﬁxed point of the pro-
gram as a whole. When computing the ﬁxed point at each point, we have to consider
the consequences derived locally and the consequences derived from the interaction
between points. Locally, ﬁxed points are computed as in a ﬁxed-point semantics
for Horn clauses `a la van Emden and Kowalski [163, 260]. When considering the
interaction between points in a Kripke structure, one has to take into account the
meaning of intuitionistic implication ⇒in accordance with Deﬁnition 39. As for
intuitionistic negation, we adopt the approach of [40,116], as follows. We rename
any negative literal ¬A as an atom A′ not present originally in the language. This
form of renaming, as used to represent explicit negation, [40] allows our deﬁnition
of labelled intuitionistic programs above to consider atoms A0,A1,...,An only, with
some of these atoms being negative literals renamed as above. For example, given
A1,...,A′
k,...,An ⇒A0, where A′
k is a renaming of ¬Ak, an interpretation that as-
signs true to A′
k represents that ¬Ak is true; it does not represent that Ak is false. The
atom A′
k is called the positive form of the negative literal ¬Ak. Following Deﬁnition
39 (intuitionistic negation), A′ will be true at a point ωi if and only if A does not hold
at every point ω j such that R(ωi,ω j). Below, we deﬁne precisely the ﬁxed-point
semantics for labelled intuitionistic programs.
The renaming of negative literals described above allows us to make use of im-
portant results from distributed logic programming (reproduced below), and the
ﬁxed-point semantics of deﬁnite logic programs.1 In order to deﬁne a ﬁxed-point
semantics for intuitionistic programs, we simply need to extend the deﬁnition of the
consequence operator TP, which gives the semantics for deﬁnite programs [163], to
cater for intuitionistic implication, as follows.
Deﬁnition 41 (Local Consequence Operator). Let P = {P1,...,Pk} be a labelled
intuitionistic program, where Pi is a set of clauses that hold at a point ωi (1 ≤i ≤k).
Let BP denote the set of atoms occurring in P, called the Herbrand base of P. Let I
be a Herbrand interpretation for Pi. The mapping ITPi : 2BP →2BP in ωi is deﬁned
as follows: ITPi(I) = {A0,A′
0 ∈BP | A1,...,An ⇒A0 is a clause in a program Pj
such that R(ωi,ω j) and {A1,...,An} ⊆I or, in the case of A′
0, for all ω j such that
R(ωi,ω j), A0 /∈ITP j(J)}, where ITP j(J) is deﬁned as ITPi(I) and J is a Herbrand
interpretation for Pj.
Deﬁnition 42 (Global Consequence Operator). Let P = {P1,...,Pk} be a la-
belled intuitionistic program. Let BP be the Herbrand base of P and let Ii be a
Herbrand interpretation for Pi (1 ≤i ≤k). The mapping ITP : 2BP →2BP is deﬁned
as ITP(I1,...,Ik) = k
l=1{ITPl}.
1 Recall that a deﬁnite logic program is a ﬁnite set of clauses of the form A1,...,An →A0, where
each Ai, 0 ≤i ≤n, is an atom.

90
7 Connectionist Intuitionistic Reasoning
Theorem 15 below, regarding the ﬁxed-point semantics of deﬁnite distributed
logic programs, will be useful.
Deﬁnition 43 (Distributed Program [217]). Deﬁnite distributed logic programs
are tuples < P1,...,Pn >, where each Pi is a set of Horn clauses (forming the
program associated with a point i). Each Pi is called a component program of the
composite program.
Theorem 15 (Fixed-Point Model of Distributed Program [217]). For each def-
inite distributed logic program P, the function TP has a unique ﬁxed point. The
sequence of all T m
P (I1,...,Ik),m ∈N, converges to this ﬁxed point T ϖ
P (I1,...,Ik),
for each Ii ⊆2BP .
Clearly, there is a correspondence between distributed programs and labelled pro-
grams in the sense that each Pi corresponds to a set of clauses labelled ωi. Since we
use renaming to deal with intuitionistic negation, we can construct the semantics of
labelled intuitionistic programs by considering the semantics of deﬁnite distributed
logic programs. As a result, Theorem 16 below follows directly from Theorem 15.
Theorem 16 (Fixed-Point Model of Labelled Intuitionistic Program). For each
labelled intuitionistic program P, the function ITP has a unique ﬁxed point. The
sequence of all IT m
P (I1,...,Ik),m ∈N, converges to this ﬁxed point IT ϖ
P (I1,...,Ik),
for each Ii ⊆2BP .
7.2 Connectionist Intuitionistic Reasoning
In this section, we introduce the connectionist model for intuitionistic reasoning. We
do so by translating the intuitionistic semantics presented above into an ensemble
of CILP neural networks.
Owing to the simplicity of each CILP network, learning can be carried out in each
possible world (i.e. each network) straightforwardly with the use of standard neural
learning algorithms. The main problem we have to tackle is that of how to set up the
connections that establish the necessary communication between networks (e.g. be-
tween ω1 and ω2 in Fig. 7.1). This will depend on the logic under consideration. In
the case of modal logic, we connect the networks according to the possible-world
semantics for the □and ♦modalities formally deﬁned as (natural-deduction) rea-
soning rules in [42, 99]. In the case of intuitionistic logic, the way we connect the
networks is different. Let us start by giving a simple example of how intuitionistic
logic can be represented in this framework.
Example 11. (Connectionist Intuitionistic Implication) Consider the intuitionis-
tic program P = {ω1 : A ⇒B, R(ω1,ω2)}. Let BP = {A,B,C}. Figure 7.1 shows
a network ensemble that implements P. According to the semantics of the above
intuitionistic implication, ω1 : A ⇒B and R(ω1,ω2) imply ω2 : A ⇒B. This can
be implemented by copying the neural representation of A ⇒B from ω1 to ω2. In

7.2 Connectionist Intuitionistic Reasoning
91
ω1
C
C
B
B
A
A
h
ω2
B
B
A
A
h
⋅⋅⋅
⋅⋅⋅
Fig. 7.1 Representing intuitionistic implication
Fig. 7.1, A ⇒B is implemented through the hidden neuron h such that output neu-
ron B is active if input neuron A is active. We shall see exactly how this is done in
Sect. 7.2.2.
Example 12. (Connectionist Intuitionistic Negation) In addition to intuitionistic
implication, we need to implement the intuitionistic negation of Deﬁnition 39. Sup-
pose P = {ω1 : ¬A ⇒B, R(ω1,ω2), R(ω1,ω3)}. We implement the implication
as before. However, we must also make sure that ¬A will be derived in ω1 if A is
not derived in ω2 and ω3. This can be implemented in the ensemble by connecting
the occurrences of A in ω2 and ω3 to the neuron ¬A in ω1, as shown in Fig. 7.2,
with the use of the hidden neuron n. The connections must be such that if A is not
active in ω2 and A is not active in ω3, then ¬A is active in ω1. The activation of
¬A in ω1 should then trigger the activation of B in ω1 (since ¬A ⇒B) using the
feedback connection from the output neuron ¬A to the input neuron ¬A in ω1, and
then also the connection from ¬A to B (via the hidden neuron h) in ω1. Note that,
differently from the case of implication, the implementation of negation requires the
use of negative weights (to account for the fact that the nonactivation of a neuron,
for example A, needs to activate another neuron, in this case, ¬A). In Fig. 7.2, we
have used dashed arrows to represent negative weights. Again, we shall see exactly
how this is done in Sect. 7.2.2.
In what follows, we describe in detail how each network is built (Sect. 7.2.1), and
then how the networks are connected to form an ensemble that represents labelled
intuitionistic programs (Sect. 7.2.2).

92
7 Connectionist Intuitionistic Reasoning
ω1
ω2
B
B
A
A
h
B
B
A
A
h
¬A
n
ω3
B
B
A
A
h
¬A
¬A
¬A
¬A
¬A
Fig. 7.2 Representing intuitionistic negation
7.2.1 Creating the Networks
To create the networks that are used to model each possible world, for example ω1
in Fig. 7.1, we use the CILP system [66].
In this chapter, we do not need to use negation as failure. Instead, we use intu-
itionistic negation. To do so, we create a neuron (A) to represent positive literals,
and another neuron (¬A) to represent negative literals. This is similar to the use
of explicit negation in logic programming [116], where any negative literal ¬A is
renamed as a new positive literal A′, not originally present in the language (see
Sect. 2.5). As a result, we only need to worry about the part of CILP’s Translation
Algorithm that deals with deﬁnite programs (Chap. 4). In this case, the network
will only contain positive weights W, since negative weights −W are only used to
implement negation as failure. CILP needs to exclude certain programs that might
loop or have multiple stable states (as is well known from the semantics of negation
as failure in logic programming). In this chapter, we do not need to worry about

7.2 Connectionist Intuitionistic Reasoning
93
this problem, since we consider deﬁnite programs only. Then, to compensate for the
lack of expressiveness of deﬁnite programs, we incorporate intuitionistic negation
and modal reasoning into the model, aiming to strike a balance between expressive-
ness and tractability [264].
7.2.2 Connecting the Networks
Given a distributed (or labelled) deﬁnite program P = ⟨P1,...,Pn⟩, we apply the
CILP translation algorithm n times to compute the neural counterpart of P, resulting
in an ensemble N1,...,Nn of CILP neural networks.
In the case of labelled intuitionistic programs, if R(ωi,ω j) and ωi : A1,...,Ak ⇒
A0, we need to add a clause of the form A1,...,Ak ⇒A0 to Pi before we apply the
Translation Algorithm. To do so, we say that {ωi : A1,...,Ak ⇒A0, R(ωi,ω j)} can
be written as {ωi : A1,...,Ak ⇒A0, ω j : A1,...,Ak ⇒A0}. In order to represent
intuitionistic negation, once the network ensemble has been created, each network
containing neurons labelled as ¬A needs to be connected to each network containing
neurons labelled as A (see Fig. 7.2). More precisely, whenever R(ωi,ω j), any out-
put neuron A in Nj needs to be connected to the output neuron ¬A in Ni through a
new hidden neuron created in Ni such that if A is active in N j, then ¬A is not active
in Ni. The algorithm below is responsible for implementing this. In the algorithm,
we use an atom A′ to represent ¬A, following the Gelfond and Lifschitz renaming
of negative literals [116], as explained above.
Let P = {P1,...,Pn} be a labelled intuitionistic program. As in the case of indi-
vidual CILP networks, we start by calculating MAXP(−→k ,−→
μ ,n) of P and Amin such
that Amin > (MAXP(−→
k,−→
μ ,n)−1)/(MAXP(−→
k,−→
μ ,n)+1), which now also considers
the number n of networks (points) in the ensemble.
Connectionist Intuitionistic Algorithm
1. For each clause cl of the form A1,...,Ak ⇒A0 in Pi (1 ≤i ≤n) such that
R(ωi,ω j) ∈P, do:
(a) add a clause A1,...,Ak ⇒A0 to Pj (1 ≤j ≤n).
2. For each program Pi (1 ≤i ≤n) in P, do:
(a) Call CILP’s Translation Algorithm.
3. For each atom of the form A′ in a clause cl of Pi, do:
(a) Add a hidden neuron NA′ to Ni.
(b) Set the step function s(x) as the activation function of NA′.2
2 Any hidden neuron created to encode negation should use the activation function s(x) = y, where
y = 1 if x > 0, and y = 0 otherwise; s(x) is known as the standard nonlinear activation function (also
called the step function). This is so because these particular hidden neurons encode (metalevel)

94
7 Connectionist Intuitionistic Reasoning
(c) Set the threshold θ A′ of NA′ such that n−(1+Amin) < θ A′ < nAmin.
(d) For each network N j corresponding to the program Pj (1 ≤j ≤n) in P such
that R(ωi,ω j) ∈P, do:
i. Connect the output neuron A of Nj to the hidden neuron NA′ of Ni and
set the connection weight to −1.
ii. Connect the hidden neuron NA′ of Ni to the output neuron A′ of Ni and
set the connection weight to W I such that W I > h−1(Amin) +μA′.W +θ A′,
where μA′, W, and θ A′ are obtained from CILP’s Translation Algorithm.3
Theorem 17 below shows that the translation from intuitionistic programs to
CILP ensembles is correct. A corollary then guarantees that the ensemble converges
to the least ﬁxed point of the program.
Theorem 17 (Correctness of the Connectionist Intuitionistic Algorithm). For
each labelled intuitionistic program P, there exists an ensemble of neural networks
N such that N computes the ﬁxed-point operator IT P of P.
Proof. We need to show that A′ is active in Ni if and only if (i) there exists a clause of
Pi of the form A1,...,Ak ⇒A′ such that A1,...,Ak are satisﬁed by an interpretation
(input vector of Ni) i, or (ii) for all Pj ∈P such that R(ωi,ω j), there exists a clause
of Pj of the form A1,...,Ak ⇒A such that A is not satisﬁed by an interpretation
(input vector of N j) j. Case (i) follows directly from Theorem 8. Case (ii) (if A
is not active in any network N j (0 ≤j ≤n) to which Ni is related, A′ is active
in Ni) is dealt with as follows. From the Connectionist Intuitionistic Algorithm,
NA′ is a nonlinear hidden neuron in Ni. If A is not active (A < −Amin) in N j, the
minimum input potential of NA′ is nAmin −θ A′. Since θ A′ < nAmin (Connectionist
Intuitionistic Algorithm, step 3c), the minimum input potential of NA′ is greater than
zero and, therefore, NA′ presents an activation 1. As a result, the minimum activation
of A′ in Ni is h(W I −μA′.W −θ A′). Since W I > h−1(Amin)+ μA′.W +θ A′, we have
h(W I −μA′.W −θ A′) > Amin and, therefore, A′ is active (A′ > Amin). If A is active in
some network N j (0 ≤j ≤n) to which Ni is related and, for all clauses of the form
A1,...,Ak ⇒A′ in Pi, A1,...,Ak are not satisﬁed by i (input vector of Ni), then we
need to show that A′ is not active in Ni. In the worst case, A is not active in n −1
networks with activation −1, and active in a single network with activation Amin.
In this case, the input potential of NA′ is n−1−Amin −θ A′ (recall that the weights
to NA′ are all set to −1). Since θ A′ > n −(1 + Amin) (Connectionist Intuitionistic
Algorithm, step 3c), the maximum input potential of NA′ is zero and, since s(x) is
the activation function of NA′, NA′ presents an activation 0. From Theorem 8, if
A1,...,Ak are not satisﬁed by i, then A′ is not active. Finally, since the activation of
NA′ is zero, A′ cannot be activated by NA′, so A′ is not active.
■
knowledge about negation, while the other hidden neurons encode (object-level) knowledge about
the problem. The former are not expected to be trained from examples and, as a result, the use of
the step function simpliﬁes the Connectionist Intuitionistic Algorithm. The latter are trained using
backpropagation, and therefore require a differentiable, semilinear activation function instead.
3 Recall that μA′ is the number of connections to output neuron A′.

7.3 Connectionist Intuitionistic Modal Reasoning
95
Corollary 18 (Connectionist Intuitionistic Fixed-Point Computation). Let P be
a labelled intuitionistic program. There exists an ensemble of recurrent neural net-
works N r such that, starting from an arbitrary initial input, N r converges to a
stable state and yields the unique ﬁxed point (IT ϖ
P (i)) of ITP.
Proof. By Theorem 17, N computes ITP. Being recurrently connected, N r com-
putes the upward powers (IT m
P (I)) of ITP. Finally, by Theorem 16, N r converges to
the unique ﬁxed point (IT ϖ
P (I)) of ITP. ■
The following example illustrates the computation of intuitionistic theories using
neural-network ensembles.
Example 13. (Connectionist Intuitionistic Fixed-Point Computation) Consider
again the ensemble of Fig. 7.2. For any initial set of input vectors (interpreta-
tions i, j,...) to networks N1, N2, N3 (corresponding to points ω1,ω2,ω3), the
output neuron A will not be activated in N2 or N3. As a result, the output neu-
ron A′ will eventually be activated (and remain activated) in N1. After that, a sin-
gle step through N1’s recursive connection will activate the output neuron B. As
a result, A′ and B will belong to the stable state of N1 and, therefore, to the ﬁxed
point of P1.
7.3 Connectionist Intuitionistic Modal Reasoning
Intuitionistic modal logic allows the combination of the strengths of the model the-
ory of modal logic and the proof theory of intuitionistic logic. This has led to a
number of applications in computer science, including program analysis, formal
speciﬁcation and veriﬁcation of computer systems, functional programming, type
theory, and program reﬁnement [5,55,203].
In what follows, we extend the language of labelled intuitionistic programs to
allow the use of the necessity (□) and possibility (♦) modal operators, as deﬁned
below.
Deﬁnition 44 (Labelled Intuitionistic Modal Program). A modal atom is of the
form MA, where M ∈{□,♦} and A is an atom. A labelled intuitionistic modal pro-
gram is a ﬁnite set of clauses C of the form ωi : MA1,...,MAn ⇒MA0, where the
MAk (0 ≤k ≤n) are modal atoms and ωi is a label representing a point at which the
associated clause holds, and a ﬁnite set of (accessibility) relations R between points
ωi (1 ≤i ≤m) in C.

96
7 Connectionist Intuitionistic Reasoning
The ﬁxed-point operator for intuitionistic modal programs can now be deﬁned as
follows.
Deﬁnition 45 (Intuitionistic Modal Consequence Operator). Let P = {P1,...,
Pk} be an intuitionistic modal program, where Pi is a set of modal intuitionistic
clauses that hold at points ωi (1 ≤i ≤k). Let BP be the Herbrand base of P and
let I be a Herbrand interpretation for Pi. The mapping IMTPi : 2BP →2BP in ωi is
deﬁned as follows: IMTPi(I) = {MA0,MA′
0 ∈BP | either (i), (ii), (iii), (iv), or (v)
holds}, where:
(i) MA1,...,MAn ⇒MA0 is a clause in Pi and {MA1,...,MAn} ⊆I or, in the
case of MA′
0, for all ω j such that R(ωi,ω j), A0 /∈IMTP j(J), where IMTP j(J) is
deﬁned as IMTPi(I) and J is a Herbrand interpretation for program Pj;4
(ii) MA0 is of the form ωi : A0, ωi is a particular possible world uniquely asso-
ciated with A0, and there exists a world ωk such that R(ωk,ωi), and ωk :
MA1,...,MAn →♦A0 is a clause in Pk, and {MA1,...,MAn} ⊆K, where K
is a Herbrand interpretation for Pk;
(iii) MA0 is of the form ♦A0 and there exists a world ω j such that R(ωi,ω j), and
ω j : MA1,...,MAn →A0 is a clause in P j, and {MA1,...,MAn} ⊆J, where J
is a Herbrand interpretation for Pj;
(iv) MA0 is of the form □A0 and, for each world ω j such that R(ωi,ω j), ω j :
MA1,...,MAn →A0 is a clause in P j, and {MA1,...,MAn} ⊆J, where J is a
Herbrand interpretation for Pj;
(v) MA0 is of the form ωi : A0 and there exists a world ωk such that R(ωk,ωi), and
ωk : MA1,...,MAn →□A0 is a clause in Pk, and {MA1,...,MAn} ⊆K, where
K is a Herbrand interpretation for Pk.
As before, the intuitionistic modal global consequence operator IMTP : 2BP →
2BP is deﬁned as IMTP(I1,...,Ik) = k
l=1{IMTPl}.
We rename each modal atom MAi in P as a new atom A j not in the language.
This allows us to associate an intuitionistic program with every intuitionistic modal
program, so that both programs have the same models. Hence, Theorem 19 below
follows directly from Theorem 16.
Theorem 19 (Fixed-Point Model of Labelled Intuitionistic Modal Programs).
For each labelled intuitionistic modal program P, the function IMTP has a unique
ﬁxed point. The sequence of all IMT m
P (I1,...,Ik),m ∈N, converges to this ﬁxed point
IMT ϖ
P (I1,...,Ik), for each Ii ⊆2BP .
Labelled intuitionistic modal programs may be translated into neural-network
ensembles by extending the above Connectionist Intuitionistic Algorithm to cater
for the representation of the modal operators □and ♦. The representation of □and
♦emulates the semantics of the operators. Recall that in the case of □, if □α holds
4 Note that item (i) simply generalises the deﬁnition of the ﬁxed-point operator for intuitionistic
programs (Deﬁnition 41).

7.3 Connectionist Intuitionistic Modal Reasoning
97
in a world (network) ωi, appropriate network connections must be set up so that
α holds in every world (network) ω j to which ωi is connected (according to the
relation R(ωi,ω j)). In the case of ♦, if ♦α holds in a world (network) ωi, network
connections must be set up so that α holds in an arbitrary world (network) ω j to
which ωi is connected, reﬂecting the semantics of the modality ♦.
We are now in a position to introduce the Connectionist Intuitionistic Modal
Algorithm. Let P = {P1,...,Pn} be a labelled intuitionistic modal program with
clauses of the form ωi : MA1,..., MAk →MA0, where each Aj is an atom and
M ∈{□,♦}, 1 ≤i ≤n, 0 ≤j ≤k.
As in the case of intuitionistic programs, we start by calculating MAXP(−→k ,−→
μ ,n)
of P and Amin such that Amin > (MAXP(−→
k,−→
μ ,n) −1)/(MAXP(−→
k,−→
μ ,n) + 1). Let
W M ∈R be such that W M > h−1(Amin) + μlW + θ A, where μl, W, and θ A are ob-
tained from CILP’s Translation Algorithm.
Connectionist Intuitionistic Modal Algorithm
1. For each Pi in P, do:
(a) Rename each modal atom MA j as a new atom not occurring in P of the form
A□
j if M = □, or A♦
j if M = ♦.5
(b) Call the Connectionist Intuitionistic Algorithm;
2. For each output neuron A♦
j in network Ni, do:
(a) Add a hidden neuron AM
j and an output neuron Aj to an arbitrary network Nz
such that R(ωi,ωz).
(b) Set the step function s(x) as the activation function of AM
j , and set the semi-
linear function h(x) as the activation function of Aj.
(c) Connect A♦
j in Ni to AM
j and set the connection weight to 1.
(d) Set the threshold θ M of AM
j such that −1 < θ M < Amin.
(e) Set the threshold θ Aj of Aj in Nz such that θ Aj = ((1+Amin)·(1−μl)/2)W.
(f) Connect AM
j to Aj in Nz and set the connection weight to W M.
3. For each output neuron A□
j in network Ni, do:
(a) Add a hidden neuron AM
j to each Nu (1 ≤u ≤n) such that R(ωi,ωu), and
add an output neuron Aj to Nu if Aj /∈Nu.
(b) Set the step function s(x) as the activation function of AM
j , and set the semi-
linear function h(x) as the activation function of Aj.
(c) Connect A□
j in Ni to AM
j and set the connection weight to 1.
(d) Set the threshold θ M of AM
j such that −1 < θ M < Amin.
(e) Set the threshold θ Aj of Aj in each Nu such that θ Aj = ((1+Amin) · (1 −
μl)/2)W.
(f) Connect AM
j to Aj in Nu and set the connection weight to W M.
5 This allows us to treat each MAj as an atom and to apply the Connectionist Intuitionistic Algo-
rithm directly to Pi by labelling neurons as □Aj, ♦A j, or simply Aj.

98
7 Connectionist Intuitionistic Reasoning
4. For each output neuron Aj in network Nu such that R(ωi,ωu), do:
(a) Add a hidden neuron A∨
j to Ni.
(b) Set the step function s(x) as the activation function of A∨
j .
(c) For each output neuron A♦
j in Ni, do:
i. Connect Aj in Nu to A∨
j and set the connection weight to 1.
ii. Set the threshold θ ∨of A∨
j such that −nAmin < θ ∨< Amin −(n−1).
iii. Connect A∨
j to A♦
j in Ni and set the connection weight to W M.
5. For each output neuron A j in network Nu such that R(ωi,ωu), do:
(a) Add a hidden neuron A∧
j to Ni.
(b) Set the step function s(x) as the activation function of A∧
j .
(c) For each output neuron A□
j in Ni, do:
i. Connect Aj in Nu to A∧
j and set the connection weight to 1.
ii. Set the threshold θ ∧of A∧
j such that n−(1+Amin) < θ ∧< nAmin.
iii. Connect A∧
j to A□
j in Ni and set the connection weight to W M.
Let us now illustrate the use of the Connectionist Intuitionistic Modal Algorithm
with the following example.
Example 14. Let P = {ω1 : A ⇒B, ω1 : □A, ω2 : ♦C, R(ω1,ω2), R(ω1,ω3)}.
We apply the Connectionist Intuitionistic Modal Algorithm, which creates three
neural networks N1, N2, and N3 to represent the points ω1, ω2, and ω3, respec-
tively (Fig. 7.3). Then, hidden neurons labelled M, ∧, and n are created to inter-
connect networks in the ensemble. Taking N1, the output neuron □A needs to be
connected to output neurons A in N2 and N3 (Connectionist Intuitionistic Modal
Algorithm, step 3). This is done using hidden neurons labelled M. Dually, output
neurons A in N2 and N3 need to be connected to the output neuron □A in N1 using
the hidden neuron ∧(Connectionist Intuitionistic Modal Algorithm, step 5). Since
ω1 : A ⇒B,R(ω1,ω2), and R(ω1,ω3), A ⇒B is copied to N2 and N3 (Connection-
ist Intuitionistic Algorithm, step 1a). Intuitionistic negation is implemented using
neurons labelled n, as illustrated in N1 for C′ (Connectionist Intuitionistic Algo-
rithm, step 3). Note that the neuron ♦C in N2 would need to be connected to a
network N j if there was a relation R(ω2,ω j) for some ω j (Connectionist Intuition-
istic Modal Algorithm, step 2a). The computation of P in the ensemble leads to the
following result: □A is computed in N1 and ♦C is computed in N2. From □A in
N1, A is computed in N2 and N3. From A and A ⇒B in N2 and N3, B is computed
in N2 and N3, respectively. Since C does not hold in both N2 and N3, C′ is com-
puted in N1. Note that the addition of R(ω1,ω1) to P, for example, for reﬂexivity,
would allow one to derive A from □A in N1 (Connectionist Intuitionistic Modal
Algorithm, step 3). In summary, the logical consequences computed by the network
are ω1 : {¬C}, ω2 : {A,B}, and ω3 : {A,B}.
Finally, let us show that the ensemble of neural networks N obtained from the
above Connectionist Intuitionistic Modal Algorithm is equivalent to the original in-
tuitionistic modal program P, in the sense that N computes the intuitionistic modal

7.3 Connectionist Intuitionistic Modal Reasoning
99
consequence operator IMTP of P (Deﬁnition 45). The proof of the Connectionist
Intuitionistic Modal Algorithm follows that of the Connectionist Intuitionistic Al-
gorithm (Theorem 17), and makes use of the correctness results of CML [75].
Theorem 20 (Correctness of Connectionist Intuitionistic Modal Algorithm).
For any intuitionistic modal program P, there exists an ensemble of neural net-
works N such that N computes the intuitionistic modal ﬁxed-point operator IMTP
of P.
Proof. We know from Theorem 8 that CILP’s Translation Algorithm is correct. We
know from Theorem 12 that the addition of modalities to CILP is correct. We also
know from Theorem 17 that the addition of intuitionistic negation to CILP is correct.
The only case we need to consider now is when modalities and negation are to be
represented together in the same network (e.g. in network ω1 of Fig. 7.3). Consider
an output neuron A0 with a neuron M and a neuron n among its predecessors in
A
A
A
A
B
B
B
B
C
C
C
C
M
M
ω2
ω3
A
A
B
B
C
C
A
A
n
^
ω1
¬C
¬C
C
C
Fig. 7.3 The ensemble of networks representing an intuitionistic modal program

100
7 Connectionist Intuitionistic Reasoning
a network’s hidden layer. There are four cases to consider. (i) Both M and n are
not activated: since the activation function of M and n is the step function, their
activations are zero. As a result, Theorem 8 applies. (ii) Only M is activated: CML
guarantees that A0 is activated with minimum input potential W M +ς, where ς ∈R.
(iii) Only n is activated: Theorem 17 guarantees that A0 is activated with minimum
input potential W I +ς. (iv) Both M and n are activated: the input potential of A0 is
at least W M +W I +ς. Since W M > 0 and W I > 0, and since the activation function
of A0 (h(x)) is monotonically increasing, A0 is activated when both M and n are
activated. ■
Corollary 21 (Connectionist Intuitionistic Modal Fixed-Point Computation).
Let P be a labelled intuitionistic modal program. There exists an ensemble of re-
current neural networks N r such that, starting from an arbitrary initial input, N r
converges to a stable state and yields the unique ﬁxed point (IMT ϖ
P (I)) of IMTP.
Proof. By Theorem 20, N computes IMTP. Being recurrently connected, N r com-
putes the upward powers of IMTP (IMT m
P (I)). Finally, by Theorem 19, N r converges
to the unique ﬁxed point of IMTP (IMT ϖ
P (I)). ■
7.4 Discussion
In this chapter, we have presented a model of computation that integrates neural net-
works and intuitionistic reasoning. We have deﬁned a class of labelled intuitionistic
(modal) programs, and then presented algorithms to translate the intuitionistic theo-
ries into ensembles of neural networks, and showed that these ensembles compute a
ﬁxed-point semantics of the corresponding theories. As a result, each ensemble can
be seen as a massively parallel model for the computation of intuitionistic (modal)
logic. In addition, since each network can be trained efﬁciently using a neural learn-
ing algorithm, for example backpropagation [224], one can adapt the network en-
semble by training possible-world representations from examples. Work along these
lines has been done in [70, 73, 74], where learning experiments in possible worlds
were investigated. As future work, we shall consider learning experiments based on
the intuitionistic model introduced in this chapter. In Chap. 8, we shall illustrate the
application of the intuitionistic model in a typical test bed for distributed knowledge
representation [143]. This will also serve to compare the intuitionistic approach with
negation as failure.
Extensions of the work presented in this chapter include the study of how to
represent properties of other nonclassical logics (such as branching-time temporal
logics [103,167], and relevance and linear logics [1]), as well as logical formalisms
for representing probabilities and reasoning under uncertainty [121]. In addition,
as the Curry–Howard isomorphism (see e.g. [1]) establishes a relationship between
intuitionism and typed λ−calculus (i.e. typed functional programs), it would be
interesting to exploit this relationship with respect to the connectionist model pre-
sented here, so that one could present such concepts in a connectionist computa-
tional setting.

Chapter 8
Applications of Connectionist Nonclassical
Reasoning
This chapter presents some benchmark distributed-knowledge-representation appli-
cations of connectionist modal and intuitionistic reasoning. It shows how CML can
be used for distributed knowledge representation and reasoning, illustrating the ca-
pabilities of the proposed connectionist model. It also compares the CML represen-
tation of a distributed knowledge representation problem with the representation of
the same problem in connectionist intuitionistic logic (CIL), the type of reasoning
presented in Chap. 7. We begin with a simple card game, as described in [87].
8.1 A Simple Card Game
Suppose we have a deck consisting of three cards labelled A,B, and C, and two
agents, 1 and 2. Each agent gets one of these cards, and a third card is left face
down. A possible situation (world) is then characterised by describing the cards
held by each agent. For instance, in the world (A,B), agent 1 holds card A and
agent 2 holds card B; card C is face down. There are six possible situations:
(A,B),(A,C),(B,A),(B,C),(C,A),(C,B). Notice that in a situation such as (A,B),
agent 1 thinks that two situations are possible, namely (A,B) and (A,C). Agent 1
knows that he has card A, but he considers it possible that agent 2 could hold either
card B or card C. In the same situation, agent 2 considers two possibilities, (A,B)
and (C,B). In general, in a world (x,y), agent 1 considers (x,y) and (x,z), to be
possible, and agent 2 considers (x,y) and (z,y) to be possible, where z is different
from both x and y.
We can now turn to the CML framework to represent the knowledge in a situation
such as this. In our formalisation, a modality Kj that represents the knowledge of
an agent j is used analogously to a modality □. In addition, we use pi to denote
that proposition p is true for agent i. For example, K jpi means that agent j knows
that p is true for agent i. We omit the subscript j of K whenever it is clear from the
context.
A.S. d’Avila Garcez et al., Neural-Symbolic Cognitive Reasoning, Cognitive Technologies,
101
c⃝Springer-Verlag Berlin Heidelberg 2009

102
8 Applications of Connectionist Nonclassical Reasoning
K(B∨C)2
Agent 1
Agent 2 
K(A∨C)1
KA
KA
KB
KB
Fig. 8.1 Representation of a possible world in the card game
In the case of the card game, we can represent the situation in which agent 1
holds card A (and knows it) by the neuron labelled KA in Fig. 8.1. Moreover, as
agent 1 knows he is holding card A, then he concludes that agent 2 holds either card
B or C. As for agent 2, he holds card B (and knows it) and, as a result, he knows that
agent 1 holds either A or C, as also represented in Fig. 8.1.
So far, we have not been representing formulas such as A ∨B as neurons, but
only literals. The neurons denoting such disjunctive information in Fig. 8.1 should
actually be seen as shorthand for the following, more accurate representation: for
agent 1, there is a possible world in which B is true for agent 2, and another posssible
world in which C is true; for agent 2, there is a possible world in which A is true and
another in which C is true for agent 1.
8.2 The Wise Men Puzzle
We now apply the framework of CML to the wise men puzzle. Below, we follow the
description of the puzzle as given in [143].
A king wishes to test his wise men. There are three wise men (who are all per-
ceptive, truthfull, and intelligent, and this is common knowledge in the group). The
king arranges them in a circle so that they can see and hear each other. It is com-
mon knowledge among them that the king has with him three red hats and two white
hats, and ﬁve hats in total. The king puts a hat on each of the wise men in such a way
that they are not able to see their own hats, and then asks each one, sequentially,
whether he knows the colour of the hat on his head. As there are only two white
hats, at least one of them is wearing a red hat. The ﬁrst wise man says he does not

8.2 The Wise Men Puzzle
103
know the colour of his hat; the second wise man says he does not know either. Then
the third wise man is able to say that he knows the colour of the hat on his head.
Why does this happen? How can the third wise man answer the question? To solve
the puzzle, let us enumerate the seven possibilities which exist: RRR, RRW, RWR,
RWW, WRR, WRW, WWR. For instance, RWR refers to the situation in which the
ﬁrst, second, and third wise men are wearing red, white, and red hats, respectively.
Note that WWW is immediately ruled out as there are only two white hats. The
reasoning of the wise men goes as follows.
When the second and the third man hear the ﬁrst answering no, they rule out the
possibility of the true situation being RWW, since if this were the case, the ﬁrst man
would have answered yes when he saw that the others were both wearing white hats,
and knew there were only two white hats. Since he said no, RWW is ruled out.
When the third man hears the second man answering no, he rules out the pos-
sibility of the situation being WRW as, if that were the case, the second wise man
would have answered yes. In addition, the third wise man rules out the possibility
RRW when he hears the second wise man’s answer, for if the second wise man
had seen the ﬁrst man wearing red and the third man wearing white, he would have
known the situation was RRW, and that he was wearing a red hat. But he did not
conclude that, and then the third man concludes that the situation cannot be RRW.
After hearing the ﬁrst and second wise men answering no, the third man has
thus ruled out the following possibilities: RWW, WRW, RRW. This leaves him with
RRR, RWR, WRR, and WWR as possible situations. In all of these, the third man
is wearing a red hat, allowing him to conclude that he must be wearing a red hat.
The above example illustrates well how the wise men learn from hearing the
others answer the original question. However, what makes them come to the correct
conclusion is that they have common knowledge of the situation, each one of them
being truthful, perceptive, and intelligent. Recall that common knowledge about a
fact φ means that everyone in a group simultaneously knows φ, everyone knows
that everyone knows φ, and so on.
8.2.1 A Formalisation of the Wise Men Puzzle
In the wise men puzzle, we have to represent the fact that it is common knowledge
in the group that there are three red hats and two white ones. The king puts the hats
on their heads and then asks, sequentially, whether they know the colour of the hat
they are wearing. As discussed above, if the ﬁrst two men say no, the third wise man
will be able to come to the right conclusion (i.e. he is wearing a red hat). In what
follows, pi represents that wise man i wears a red hat and ¬pi represents that wise
man i does not wear a red hat (i.e. he is wearing a white hat). Let the following set
Γ of formulas correspond to the common knowledge in the initial situation; C(ϕ)
denotes that formula ϕ is common knowledge among the agents:
{C(p1 ∨p2 ∨p3),
C(p1 →K2p1),
C(¬p1 →K2¬p1),

104
8 Applications of Connectionist Nonclassical Reasoning
C(p1 →K3p1),
C(¬p1 →K3¬p1),
C(p2 →K1p2),
C(¬p2 →K1¬p2),
C(p2 →K3p2),
C(¬p2 →K3¬p2),
C(p3 →K1p3),
C(¬p3 →K1¬p3),
C(p3 →K2p3),
C(¬p3 →K3¬p3)}.
Next, we have that the ﬁrst wise man says he does not know the colour of his hat.
This can be represented by the following formula:
C(¬K1p1 ∧¬K1¬p1),
i.e. it is common knowledge that wise man 1 does not know if his hat is red and does
not know if his hat is white.
Now we can conclude that at least one of the others must be wearing a red hat,
which can be formalised by a lemma, proved in [143]:
Lemma 22. [143] From the set of formulas in Γ and C(¬K1p1 ∧¬K1¬p1), C(p2 ∨
p3) can be derived.
Moreover, since it is common knowledge that either p2 or p3 is true, p2 ∨p3
remains true over time, and this can be used to prove the following lemma.
Lemma 23. [143] If the set of formulas in Γ, and C(p2 ∨p3), and C(¬K2p2 ∧
¬K2¬p2) are all valid, then K3p3 can be derived.
Notice that these two lemmas state that given some negative information with
respect to knowledge about a situation, positive knowledge has to be derived so
that the solution can eventually be reached. This means that, at each round, more
knowledge is acquired, allowing an intelligent agent to infer the conclusion from
his or her knowledge base. The task at hand in our neural-symbolic system is to
represent the reasoning process of this problem and then construct the ensembles
that correspond to the reasoning rules.
8.2.2 Representing the Wise Men Puzzle Using CML
Turning to the framework of connectionist modal logic (CML), one can represent
the above reasoning process by ensembles of neural networks. CML also allows the
representation of the reasoning of each individual wise man in individual networks,
and the above deductions. For instance, Fig. 8.2 represents that it is common knowl-
edge that either agent 1, agent 2, or agent 3 is wearing a red hat (for simplicity, we
refer to wise men numbers 1, 2, and 3 as agents 1, 2, and 3, respectively). Agent 1
knows that agents 2 and 3 are wearing red and white hats, respectively, but he is not
able to state the colour of his own hat. Similarly, agent 2 knows the colours of the
other agents’ hats, but cannot state, in his turn, what the colour of his own hat is.
However, after hearing the others saying no, agent 3 will be able to say yes. Notice
that Fig. 8.2 represents a snapshot of the situation, as we have not introduced a time
variable to deal with knowledge evolution.

8.2 The Wise Men Puzzle
105
agent 
2
agent 
3
agent 
1
p1
Kp2 Kp3
Kp1
Kp2 Kp3
Kp1
Kp2 Kp3
Kp1
p2
p3
Fig. 8.2 Knowledge in the wise men puzzle: each agent knows the colours of the other agents’
hats
Furthermore, agent 3 may deduce that he is wearing a red hat if he happens to
know that agents 1 and 2 do not know whether they themselves are wearing a red
hat. This is the case because, as discussed in Sect. 8.2.1, if agents 1 and 2 do not
know that they are wearing a red hat, then they must be seeing at least one red
hat. And that must be agent 3’s hat. This can be formalised as an extended modal
program, as follows (where r j
i is used to denote rule ri for agent j):
r1
1 : K1¬p2 ∧K1¬p3 →K1p1
r2
1 : K2¬p1 ∧K2¬p3 →K2p2
r3
1 : K3¬p1 ∧K3¬p2 →K3p3
r1
2 : K1p2 →¬K1p1
r1
3 : K1p3 →¬K1p1
r2
2 : K2p1 →¬K2p2
r2
3 : K2p3 →¬K2p2
r3
2 : ¬K1p1 ∧¬K2p2 →K3p3,

106
8 Applications of Connectionist Nonclassical Reasoning
together with the rules obtained from Γ stating that each agent knows the colours of
the other agents’ hats1 (as depicted in Fig. 8.2):
r2
4 : p1 →K2p1
r3
3 : p1 →K3p1
r1
4 : p2 →K1p2
r3
4 : p2 →K3p2
r1
5 : p3 →K1p3
r2
5 : p3 →K2p3.
Figure 8.3 contains an implementation of the rules r1
2,...,r1
5, r2
2,...,r2
5, and
r3
2,...,r3
4, for agents 1, 2, and 3, respectively. Whenever agent 3 is wearing a red
hat, output neuron p3 is clamped in the active state in the network for agent 3 to
denote that p3 is a fact. The activation of p3 triggers the activation of Kp3 in the
network for agent 1, and this triggers the activation of ¬Kp1 in the same network.
Similarly, p3 triggers the activation of Kp3, and Kp3 triggers the activation of ¬Kp2
in the network for agent 2. Finally, the activation of ¬Kp1 in network 1 and of ¬Kp2
in network 2 produces the activation of Kp3 in network 3, indicating that agent 3
knows that he is wearing a red hat.
A set of weights for the ensemble shown in Fig. 8.3 complying with this rea-
soning process is given in Table 8.1. This set of weights was calculated by use
of the CILP Translation Algorithm (Chap. 4) and the CML Connectionist Model
Algorithm (Chap. 5) for P = {r1
2,r1
3,r1
4,r1
5,r2
2,r2
3,r2
4,r2
5,r3
2,r3
3,r3
4, R(1,2), R(2,3),
R(1,3)}. In this calculation, we ﬁrst apply the Translation Algorithm, which creates
three neural networks to represent agents 1, 2, and 3. Then, we apply the Connec-
tionist Modal Algorithm. Hidden neurons labelled h1, h2,...,h11 are created (one for
each rule) using the Connectionist Modal Algorithm. The remaining neurons are all
created by use of the Translation Algorithm. In Table 8.1, we have used (Xi,Y j) to
denote the weight from neuron X in network i to neuron Y in network j, and (Xi) to
denote the threshold of neuron X in network i. The calculations are as follows. From
Equation 4.1, Amin > (MAXP(2,2) −1)/(MAXP(2,2) + 1). Let Amin = 0.6. From
Equation 4.2, taking β = 1, W ≥2(ln(1.6) −ln(0.4))/(2(−0.4) + 1.6) = 1.1552.
Let W = 2. Thus, all feedforward connections created by the Translation algorithm
will receive a weight 2. Recall that all feedback connections also created by the
Translation Algorithm will receive a weight 1. For simplicity, feedback connections
are not listed in Table 8.1.
1 Notice the difference between K1p1 being false and ¬K1p1 being true. When K1p1 is false (neu-
ron K1p1 is not active), nothing can be said about whether agent 1 knows the colour of his hat.
When ¬K1p1 is true (neuron ¬K1p1 is active), agent 1 has had to reason in order to reach such a
conclusion. The difference between K and ¬K is a subject to which we shall return later. From a
practical perspective, the fact that ¬K1p1 is true means that some propagation of activation must
have occurred in the neural network, leading to the activation of neuron ¬K1p1. This allows us to
write ¬K1p1 ∧¬K2p2 →K3p3 and use this rule in a valid neural implemetation of the wise men
puzzle. Strictly speaking, however, the rule for the wise men according to the formalisation of the
puzzle should be K3¬K1p1 ∧K3¬K2p2 →K3p3. This rule could also have been implemented in
the neural network, but with the use of a larger number of neurons and connections.

8.2 The Wise Men Puzzle
107
agent 2
Kp3
agent 1
p1
Kp2 Kp3
Kp1
p2
Kp
2
Kp1
p3
Kp2
Kp3
Kp1
¬Kp1
Kp3
Kp2
¬Kp2
Kp3
Kp1
agent 3 
h1
h2
h3
h4
h5
h6
h7
h8
h9
h10
h11
Fig. 8.3 Wise men puzzle: implementing all the rules
Table 8.1 A valid set of weights for the network for the wise men puzzle
(h1,Kp1
2) = 4
(h2,Kp1
3) = 4
(Kp1
2,h3) = 2
(h3,¬Kp1
1) = 2
(Kp1
3,h4) = 2
(h4,¬Kp1
1) = 2
(h1) = 0.5
(h2) = 0.5
(h3) = 0
(h4) = 0
(Kp1
2) = 1.6
(Kp1
3) = 1.6
(¬Kp1
1) = 0
(h5,Kp2
1) = 4
(h6,Kp2
3) = 4
(Kp2
1,h7) = 2
(h7,¬Kp2
2) = 2
(Kp2
3,h8) = 2
(h8,¬Kp2
2) = 2
(h5) = 0.5
(h6) = 0.5
(h7) = 0
(h8) = 0
(Kp2
1) = 1.6
(Kp2
3) = 1.6
(¬Kp2
2) = 0
(h9,Kp3
1) = 4
(h10,Kp3
2) = 4
(h11,Kp3
3) = 4
(h9) = 0.5
(h10) = 0.5
(h11) = 1.6
(Kp3
1) = 1.6
(Kp3
2) = 1.6
(Kp3
3) = 1.6
(p1,h9) = 1
(p1,h5) = 1
(p2,h1) = 1
(p2,h10) = 1
(p3,h2) = 1
(p3,h6) = 1
(¬Kp1
1,h11) = 1
(¬Kp2
2,h11) = 1

108
8 Applications of Connectionist Nonclassical Reasoning
The next step is to calculate the thresholds of the hidden neurons h3, h4, h7,
h8 according to Equation 4.3, and the thresholds of the output neurons accord-
ing to Equation 4.4. For example, (h3) = 2((1 + 0.6) · (1 −1))/2 = 0, (¬Kp1
1) =
2((1 + 0.6) · (1 −1))/2 = 0, and (Kp1
2) = 2((1 + 0.6) · (1 −0))/2 = 1.6. Finally,
weights and thresholds for the neurons interconnecting networks in the ensemble
need to be calculated using the Connectionist Modal Algorithm. Connections be-
tween networks, for example (p3,h2), receive weight 1, the thresholds θ M of neu-
rons h1,h2,h5,h6,h9,h10 must satisfy −1 < θ M < Amin (we take θ M = 0.5), and
the threshold θ of neuron h11 must satisfy n −(1 + Amin) < θ < nAmin (we take
θ = 1.6).2 Finally, the weights W > h−1(0.6)+2μL +θ L from h1,h2,h5,h6,h9,h10,
and h11 to the output must be calculated.3 For example, for the output neuron Kp3
1,
μ = 0 and θ = 1.6, and thus W > 2.986. Although it is not necessary, we choose
W = 4 to unity all of the seven remaining weights (see Table 8.1).
8.3 Applications of Connectionist Intuitionism
Since its origins, intuitionistic logic has been used as a logical foundation of con-
structive mathematics and, more recently, in several areas of computation. For
instance, Art¨emov has developed a semantics for Godel’s logic of proofs based
on intuitionism [10]. Moreover, an intuitionistic temporal logic has been success-
fully used to characterise timing analysis in combinatorial circuits [180], and in-
tuitionistic logic has been shown to be relevant to spatial reasoning, with possible
applications in geographical information systems. Bennett’s propositional intuition-
istic approach [23] provides for tractable yet expressive reasoning about topological
and spatial relations, in contrast to some more involved (ﬁrst-order) reasoning
frameworks. Intuitionistic modal logic has also been used to characterise notions
of knowledge in philosophical logic, and more recently in artiﬁcial intelligence
[206,236].
In this section, we apply the model of connectionist intuitionistic logic to the
wise men puzzle so that it can be compared with the CML solution to the problem
given above. Our aim is also to ground the theoretical work presented in Chap. 7 in
a practical example, showing that the type of neural-network architecture advocated
there may well be required in a connectionist setting to represent even a simple
situation in the case of distributed commonsense reasoning. Although simple, the
wise men puzzle has been used extensively to model reasoning about knowledge
in distributed, multiagent environments [87,121]. This and other puzzles have been
shown to be suitable not only because of their simplicity, but also because of their
generality, as they represent typical situations occurring in practice in distributed,
multiagent environments.
2 Recall that n = 3 in this example.
3 Recall that W = 2 in this example; h−1(Amin) = −(1/β)ln((1−Amin)/(1+Amin)).

8.3 Applications of Connectionist Intuitionism
109
Recall from Sect. 8.2 that, since there are only two white hats, at least one of the
wise men is wearing a red hat. The ﬁrst wise man says he does not know the colour
of his hat; the second wise man says he does not know either. Then the third wise
man, if he sees two white hats, should be able to conclude that he knows the colour
of the hat on his head. If not, it becomes common knowledge that there must exist at
most a single white hat on their heads (because, if there were two, a wise man would
have said in the previous round that he knew he was wearing a red hat). A wise man
who can see such a white hat should then be able to conclude that he is wearing a
red hat. Again, if they all fail to reach such a conclusion, then it becomes common
knowledge that they all must be wearing red hats.
This puzzle illustrates a situation where intuitionistic implication and intuition-
istic negation occur. Knowledge evolves over time, with the current knowledge per-
sisting in time. For example, in the ﬁrst round it is known that there are at most two
white hats. Then, if the wise men get to the second round, it becomes known that
there is at most one white hat on their heads. This new knowledge subsumes the
previous knowledge, which in turn persists. This means that if A ⇒B is true at a
point t1, then A ⇒B will be true at a point t2 that is related to t1 (intuitionistic im-
plication). Now, in any situation in which a wise man knows that his hat is red (and
therefore not white), this knowledge – constructed with the use of sound reasoning
processes – cannot be refuted. In other words, if ¬A is true at point t1 then A cannot
be true at a point t2 that is related to t1 (intuitionistic negation).
To model the puzzle, we do the following. Let pi denote the fact that wise man
i wears a red hat, i ∈{1,2,3}. As before, we use RRR to denote p1 ∧p2 ∧p3,
RRW to denote p1 ∧p2 ∧¬p3, and so on. If the second and third men hear the ﬁrst
answering no, they rule out the RWW option. If the third man then hears the second
man answering no, he rules out WRW and RRW. The reasoning process continues
until one of the wise men is capable of concluding whether or not he is wearing
a red hat. This reasoning process is intuitionistic. Given some limited information
with respect to knowledge about a situation, further knowledge has to be derived so
that the solution can be reached eventually.
8.3.1 Representing the Wise Men Puzzle Using CIL
We can model the wise men puzzle by constructing the relative knowledge of each
wise man along a sequence of time points. This allows us to explicitly represent
the relativistic notion of knowledge, which is a fundamental principle of intuition-
istic reasoning. As before, we refer to wise men 1, 2, and 3 as agents 1, 2, and 3,
respectively. We model the relative knowledge of each agent at points t1,t2,t3 in a
Kripke structure, each point being associated with a discrete time point. The result-
ing model is a two-dimensional network ensemble (agents × time), containing three
networks in each dimension. In addition to pi – denoting the fact that wise man i

110
8 Applications of Connectionist Nonclassical Reasoning
wears a red hat – in order to model each agent’s individual knowledge, we need to
use a modality Kj, j ∈{1,2,3}, which represents the relative notion of knowledge
at each point. Thus, Kjpi denotes the fact that agent j knows that agent i wears a
red hat.
The fact that each agent knows the colours of the other agents’ hats is imple-
mented as before, as illustrated in Fig. 8.2. For example, if wise man 3 wears a red
hat (neuron p3 is active), then wise man 1 knows that wise man 3 wears a red hat
(neuron Kp3 is active for wise man 1). However, this is an example of the intuition-
istic implication t1 : p3 ⇒K1p3, which clearly persists at points t2 and t3. In other
words, the structure of Fig. 8.2 repeats itself twice, as it should be valid for each
point in the Kripke structure (t1,t2,t3), given that R(t1,t2) and R(t2,t3). This cre-
ates the two-dimensional network ensemble mentioned above. Note that, according
to the Connectionist Intuitionistic Modal Algorithm (Sect. 7.3), connections linking
different networks in the ensemble receive a weight 1.
We now need to model the reasoning process of each wise man. For this example,
let us consider the case RWR (i.e. we make neurons p1 and p3 active). For agent 1,
we have the rule t1 : K1¬p2 ∧K1¬p3 ⇒K1p1, which states that agent 1 can deduce
that he is wearing a red hat if he knows that the other agents are both wearing
white hats. Analogous rules exist for agents 2 and 3. As before, the implication is
intuitionistic, so that it persists at t2 and t3, as depicted in Fig. 8.4 for wise man 1.
In addition, according to the interpretation of intuitionistic negation, we may only
conclude that agent 1 knows ¬p2 if, in every world that agent 1 envisages, p2 is not
derived. This is illustrated by the use of dotted lines in Fig. 8.4.4 In the case of RWR,
the network ensemble will never derive p2 (as one should expect), and thus it will
derive K1¬p2 and K3¬p2. Note that, according to the Connectionist Intuitionistic
Algorithm (Sect. 7.2), connections linking different networks receive a weight −1,
as depicted in Fig. 8.4.
Finally, to complete the formalisation of the problem, we know that, at t1, it
is common knowledge that there exist at most two white hats. As the reasoning
process takes us into t2 (in the case of RWR), it becomes common knowledge that
there exists at most one white hat on their heads. As a result, the following rules
hold at t2 (and at t3): K1¬p2 ⇒K1p1 and K1¬p3 ⇒K1p1. Analogous rules exist for
agents 2 and 3. If the reasoning process were to take us into t3 (the only case here
would be RRR), then it would be common knowledge that there exist no white hats
on their heads. This can be modelled by the rule t3 : K1p2 ∧K1p3 ⇒K1p1. Again,
analogous rules exist for agents 2 and 3 at t3.
It is interesting to note that the connectionist intuitionistic approach to solving
the wise men puzzle produces a neater model than does our previous, CML-based
approach (contrast Figs. 8.3 and 8.4). In CML, an agent’s lack of knowledge needed
to be modelled, requiring the use of a different type of negation. In CIL, the use
of intuitionistic negation seems to facilitate the modelling of a full solution to the
puzzle.
4 Recall that the accessibility relation is reﬂexive and transitive, so that the intuitionistic algorithm
also connects, for example, K1p2 in t3 to K1¬p2 in t1, and K1p2 in ti to K1¬p2 in ti, i ∈{1,2,3}.
For simplicity, we have omitted such connections.

8.3 Applications of Connectionist Intuitionism
111
wise man 1 at point t1
K¬p2
K¬p3
K¬p2
Kp1
K¬p3
Kp2
Kp3
wise man 1 at point t2
K¬p2 K¬p3
K¬p2
Kp1
K¬p3
Kp2
Kp3
wise man 1 at point t3
K¬p2
K¬p3
K¬p2
Kp1
K¬p3
Kp2
Kp3
h1
h2
h3
h4
h5
h1
h2
h3
h4
h5
h1
h2
h3
h4
h5
−1
−1
−1
−1
Fig. 8.4 Wise men puzzle: intuitionistic negation and implication
A set of weights for the networks in the ensemble of Fig. 8.4 is given in Table 8.2.
In this table, we have used (X,Y) to denote the weight from neuron X to neuron
Y, and θX to denote the threshold of neuron X in a particular neural network.
First, we calculate Amin > ((MAXP(−→
k,−→
μ ) −1)/(MAXP(−→
k,−→
μ ) + 1)), i.e. Amin >
(MAXP(2,2)−1)/(MAXP(2,2)+1). We take Amin = 0.6. Then, taking β = 1, we
calculate W ≥(2/β) · (ln(1+Amin) −ln(1−Amin))/(MAXP(−→
k,−→
μ ) · (Amin −1) +
Amin + 1)), i.e. W ≥2(ln(1.6) −ln(0.4))/(2(−0.4) + 1.6) = 1.1552. Let W = 2.0.
Thus, all feedforward connections that are created by CILP’s Translation Algorithm
will receive a weight 2. In addition, recall that all feedback connections that are cre-
ated by the Translation Algorithm receive a weight 1. The next step is to calculate
the thresholds of the hidden and output neurons. The threshold of h1 is given by the

112
8 Applications of Connectionist Nonclassical Reasoning
Table 8.2 A valid set of weights for the intuitionistic network for the wise men puzzle
Wise man 1 at t1
Wise man 1 at t2
Wise man 1 at t3
θh1 = 1.6
θh1 = 1.6
θh1 = 1.6
θh2 = 0.0
θh2 = 0.0
θh2 = 0.0
θh3 = 0.0
θh3 = 0.0
θh3 = 0.0
θh4 = 1.5
θh4 = 1.5
θh4 = 1.5
θh5 = 1.5
θh5 = 1.5
θh5 = 1.5
(K¬p2,h1) = 2.0
(K¬p2,h1) = 2.0
(K¬p2,h1) = 2.0
(K¬p3,h1) = 2.0
(K¬p3,h1) = 2.0
(K¬p3,h1) = 2.0
(h1,Kp1) = 2.0
(h1,Kp1) = 2.0
(h1,Kp1) = 2.0
(h2,Kp2) = 4.0
(h2,Kp2) = 4.0
(h2,Kp2) = 4.0
(h3,Kp3) = 4.0
(h3,Kp3) = 4.0
(h3,Kp3) = 4.0
(h4,K¬p2) = 2.0
(h4,K¬p2) = 2.0
(h4,K¬p2) = 2.0
(h5,K¬p3) = 2.0
(h5,K¬p3) = 2.0
(h5,K¬p3) = 2.0
(K¬p2,K¬p2) = 1.0
(K¬p2,K¬p2) = 1.0
(K¬p2,K¬p2) = 1.0
(K¬p3,K¬p3) = 1.0
(K¬p3,K¬p3) = 1.0
(K¬p3,K¬p3) = 1.0
θKp1 = 0.0
θKp1 = 0.0
θKp1 = 0.0
θKp2 = 0.0
θKp2 = 0.0
θKp2 = 0.0
θKp3 = 0.0
θKp3 = 0.0
θKp3 = 0.0
θK¬p2 = 0.0
θK¬p2 = 0.0
θK¬p2 = 0.0
θK¬p3 = 0.0
θK¬p3 = 0.0
θK¬p3 = 0.0
Translation Algorithm: θ l = ((1+Amin)·(kl −1)/2)W, i.e. θh1 = 1.6. The thresh-
olds of h2 and h3 are given by the Connectionist Intuitionistic Modal Algorithm:
−1 < θ M < Amin. We let θh2,θh3 equal zero.5 The thresholds of h4 and h5 are given
by the Connectionist Intuitionistic Modal Algorithm: n−(1+Amin) < θ A′ < nAmin,
i.e. 1.4 < θ A′ < 1.8. We let θh4, θh5 equal 1.5. The threshold of any output neu-
ron is given by the Translation Algorithm: θ A0 = ((1+Amin)·(1−μl)/2)W. Since
μl = 1, these are all zero. Finally, we consider the feedforward connections that are
created by the Connectionist Intuitionistic Modal Algorithm. These are (h2,Kp2)
and (h3,Kp3) for each network. According to the algorithm, these weights should
be greater than h−1(Amin)+ μlW +θ A = 2.986.6 We set these to 4.0.
8.4 Discussion
In this chapter we have illustrated the use of connectionist modal logic and connec-
tionist intuitionistic logic in distributed knowledge representation and reasoning.
The ability of these models to represent and learn richer logic-based distributed
5 Note that the information coming from agents 2 and 3 is gathered by agent 1 via hidden neurons
h2 and h3.
6 Note that h−1(Amin) = −(1/β)ln((1−Amin)/(1+Amin)).

8.4 Discussion
113
knowledge representation mechanisms achieves a long-term aim of learning lan-
guages and models. By means of a formalisation which allows the representation
of modal operators, we have proposed a solution to problems in which agents can
reason about their knowledge in a situation and learn from their experience.
It is interesting also to relate the results in this chapter to the examples in Chaps. 5
and 6. In Chap. 6, we saw that the provision of a Temporal Algorithm can provide the
Connectionist Modal Algorithm of Chap. 5 with explicit capabilities for reasoning
about time. In the case of the muddy children puzzle considered in Chaps. 5 and 6,
a full solution to the problem would require metalevel knowledge about the problem
domain to deﬁne the number s of relevant time points. The formalisation of the full
solution to the puzzle would then require the addition of a modality to deal with
the notion of next time in a linear time ﬂow. Graphically, this can be seen as a
chain of size s of recurrent networks (as opposed to simply the unfolded version
of a recurrent network). At each time point, a recurrent network is still responsible
for carrying out the computations for each agent at that point (thus implementing
the concept of short-term memory). Once the networks are stable, the computations
can be carried forward to the next time point in the chain; this is responsible for
implementing the concept of long-term memory. The deﬁnition of the number of
points s necessary to solve a given problem clearly depends on the problem domain
(i.e. on the number of time points that are needed for reasoning about the problem).
For example, in the case of the muddy children puzzle, we know that it sufﬁces
to have s equal to the number of children that are muddy, and if we do not know
this number, to have s equal to the number of children playing. The deﬁnition of
s in a different domain might not be as straightforward, possibly requiring a ﬁne-
tuning of the value of s similar to that performed on the usual learning parameters
of a network. Essentially, this produces a network ensemble of varying size, or a
varying network architecture with memory on demand. This is an interesting avenue
for further research.
As an exercise, the reader is invited to design a connectionist temporal logic
(CTLK) solution to the wise men puzzle. Although a full solution to the muddy
children puzzle does require a temporal dimension, it seems that this is not needed in
the case of the wise men puzzle. Nevertheless, in the same way that the intuitionistic
design of the problem seems neater than the exclusively modal design, the CTKL
design might offer the most appropriate solution to the problem (i.e. a better design
than the intuitionistic version).

Chapter 9
Fibring Neural Networks
As we have seen in Chaps. 4 to 7, neural networks can deal with a number of rea-
soning mechanisms. In many applications these need to be combined (ﬁbred) into
a system capable of dealing with the different dimensions of a reasoning agent. In
this chapter, we introduce a methodology for combining neural-network architec-
tures based on the idea of ﬁbring logical systems [101]. Fibring allows one to com-
bine different logical systems in a principled way. Fibred neural networks may be
composed not only of interconnected neurons but also of other networks, forming a
recursive architecture. A ﬁbring function then deﬁnes how this recursive architecture
behaves by deﬁning how the networks in the ensemble relate to each other, typically
by allowing the activation of neurons in one network (A) to inﬂuence the change of
weights in another network (B). Intuitively, this can be seen as training network B
at the same time as network A is running. Although both networks are simple, stan-
dard networks, we can show that, in addition to being universal approximators like
standard feedforward networks, ﬁbred neural networks can approximate any poly-
nomial function to any desired degree of accuracy, thus being more expressive than
standard feedforward networks.
9.1 The Idea of Fibring
To explain the basic idea of ﬁbring two systems, we begin with some examples.
Example 15. (Language) Suppose we see a sentence in English with a few German
words in it, say, ‘John expressed grosse Respekt for Mary’s actions’, and we want
to understant its meaning. The natural thing is to start parsing the sentence using an
English parser, and when we get to the phrase ‘grosse Respekt’ we know that it is
supposed to be an object noun phrase. Since we do not know its meaning, we regard
it as ‘atomic’. To ﬁnd its meaning, we send the expression to a German/English
translation machine. This machine understands German phrases and sends back an
A.S. d’Avila Garcez et al., Neural-Symbolic Cognitive Reasoning, Cognitive Technologies,
115
c⃝Springer-Verlag Berlin Heidelberg 2009

116
9 Fibring Neural Networks
English equivalent. The functionals (or ﬁbring functions, as we call them) involved
here are the following:
•
fE,G(x) is a ﬁbring function that sends a German phrase x from an English text
to a German/English translation machine.
•
fG,E(x) is a ﬁbring function that sends back an equivalent English value of x.
The following example deals with the ﬁbring of networks.
Example 16. (Networks) Let S be a network, such as a neural or Bayesian network,
or just a graph with nodes and some connections between the nodes. Let a and b be
two nodes in S. Suppose we replace the node a by an entire network Ta. We denote
the new network thus obtained by S(a/Ta). Ta may be a network of the same type or
an entirely different network of a completely new type. How do we handle this new
network? Figure 9.2 describes the situation, where we start with S = (c ⇒a ⇒b)
and where Ta is as in Fig. 9.1
The ‘⇒’ in Fig 9.2 indicates a relationship between nodes of network S, and ‘→’
indicates a relationship between nodes of network Ta. The network S expects a node
a with suitable properties. Instead, we gave it a network Ta. So, again, we need two
ﬁbring functions: fn,b takes the ﬁrst network’s kind of input and transforms it into
something the second network can understand and do something with, and fb,n takes
the second network’s output and transforms it into something the ﬁrst network can
understand. The combined effect of these two functions is to make Ta look like a
node in S. So, we are actually replacing a by [fn,b,Ta, fb,n].
The general ﬁbring problem may be outlined as follows. Given a system S1 and
a system S2, we take an atomic unit a ∈S1 and substitute for it a part of S2. This
creates a new system S1,2. We need two ﬁbring functions f1,2 and f2,1 to handle
S1,2. When the system S1 calls upon what used to be node a, it sees a part Xa of
system S2. f1,2 feeds a translation of the environment of a into Xa and obtains a
e
d
f
Fig. 9.1 A simple network
c
b
e
d
f
Fig. 9.2 Fibring two networks

9.2 Fibring Neural Networks
117
result, possibly by embedding Xa in an S2 environment provided by f1,2. Then f2,1
takes the result and transforms it into something system S1 can understand. Thus,
the entire operation of f1,2 and then f2,1 works as a component of system S1. We
have thus deﬁned what a system of the form S1,2 can do.
Similarly, we can deﬁne S2,1 and, inductively, S1,2,1,S2,1,2,...,Sx1,x2,X3,...,xn, with
xi ̸= xi+1,i = 1,...,n −1. The same ﬁbring functions f1,2 and f2,1 will be used
throughout. Note that an essential part of the ﬁbring process is the notion of substi-
tution. This has to be deﬁned specially for each case. So, the ﬁbring process requires
Sub1,2 and Sub2,1 as well as f1,2 and f2,1.
Example 17. (Modal and Fuzzy Logic) Our next example is one of ﬁbring modal
logic into many-valued logic. Consider the expression q ⇒⋄b. Here, ⇒is a fuzzy
implication. It can compute a fuzzy value x ⇒y out of any two fuzzy values x and y;
for example, x ⇒y = min(1,1 −x + y). Let v be an assignment of values to the
atoms of the fuzzy logic. We have v(q ⇒⋄b) = (v(q) ⇒v(⋄b)). We cannot evaluate
the right-hand expression, because v(⋄b) is not known; v gives values to atoms but
not to a modal expression. We need a ﬁbred modal model M = (S,a,h) in which
⋄b can be evaluated. A value y = h(⋄b) can be extracted from the model M, and a
function Fmodal, fuzzy will transmit Fmodal, fuzzy(y) back to v. The ﬁnal answer will be
v(q ⇒⋄b) = v(q) ⇒Fmodal, fuzzy(y).
We now turn to the idea of self-ﬁbring. This is a special case of ﬁbring, where
we embed the system as a basic unit into itself. So, if the system S contains the
basic unit a ∈S, then we form the new system S1 = S(a/S), where we substitute S
itself for a. This actually brings the metalevel (S itself) into the object level (a in S).
A good example is the logic of the conditional.
Example 18. (Conditionals) Consider a nonmonotonic consequence relation
|∼deﬁned on the language of classical logic. This language contains only the clas-
sical connectives ¬,∧,∨,→. We write A |∼S B to indicate that B is a nonmonotonic
consequence of A in one of the many well-known nonmonotonic systems, say S.
Clearly, |∼S is a metalevel relation. Consider A |∼S (B |∼S C). This expression
has no meaning. It is obtained by taking the object-level atom a in A |∼S a and
substituting (B |∼S C) for it. The ﬁbring methodology can give meaning to this
expression and its iterations in a natural way, and if we follow the process through
we obtain well-known systems of conditional logic, |∼being the conditional at the
object level.
9.2 Fibring Neural Networks
Fibring can be used to combine several different systems, such as logical systems
of space and time, neural networks and Bayesian networks [63,70,270], and declar-
ative and procedural programming languages, the main challenge being how these
systems may be put to work together in a coordinated manner to solve a particular

118
9 Fibring Neural Networks
Network A
Network B
.
.
.
.
.
.
.
.
.
.
.
.
X
Fig. 9.3 Fibring neural networks
problem.1 To this end, we know that a fundamental aspect of symbolic computation
is the ability to implement recursion. As a result, to make neural networks behave
like logic, we need to add recursion to them by allowing networks to be composed
not only of interconnected neurons but also of other networks. Figure 9.3 exempli-
ﬁes how a network (B) can be embedded recursively into another network (A). Of
course, the idea of ﬁbring is not only to organise networks as a number of subnet-
works (A, B, etc). In Fig. 9.3, for example, the hidden neuron X of network A is
expected to be a neural network (network B) in its own right. The input, weights,
and output of network B may depend on the activation state of neuron X, according
to a ﬁbring function. One such function might multiply the weights of network B by
the input potential of neuron X.
Most of the work on how to implement recursion in neural networks has concen-
trated on the use of recurrent autoassociative networks and symmetric networks to
represent formal grammars [85, 208, 238, 239, 248]. In general, the networks learn
how to simulate a number of recursive rules by similarity to a set of examples, and
the question of how such rules are represented in the network is treated as secondary.
In this chapter, we give a different treatment of the subject, looking at it from a per-
spective of neural-symbolic ﬁbring. The idea is to be able to represent and learn
expressive symbolic rules, such as rules containing embedded implications of the
1 For example, a robot’s motion control system may require a logic of space, a logic of time, and a
neural-network-based system for visual pattern recognition.

9.3 Examples of the Fibring of Networks
119
form (a →b) →c, where (a →b) would be encoded into network B, and then
X →c, with X = (a →b), would be encoded into network A so that the ﬁbred net-
work represents (a →b) →c.
In what follows, we introduce and deﬁne ﬁbred neural networks (fNNs) and show
that, in addition to being universal approximators,2 fNNs can approximate any poly-
nomial function in an unbounded domain, thus being more expressive than standard
feedforward networks. Brieﬂy, this can be shown by noting that ﬁbred neural net-
works compute the function f(x) = x2 exactly for any given input x in R, as op-
posed to feedforward networks, which are restricted to compact (i.e. closed and
bounded) domains [52,140]. Intuitively, ﬁbring neural networks can be seen as run-
ning and training neural networks at the same time. In Fig. 9.3, for example, at
the same time as we run network A, we perform learning in network B because
we allow the weights of B to change according to the ﬁbring function. In other
words, object-level network running and metalevel network training occur simulta-
neously in the same system, and this is responsible for the added expressiveness of
the system.
9.3 Examples of the Fibring of Networks
As mentioned above, the main idea behind ﬁbring neural networks is to allow single
neurons to behave like entire embedded networks according to a ﬁbring function ϕ.
This function qualiﬁes the function computed by the embedded network so that the
embedded network’s output depends on ϕ. For example, consider network A and its
embedded network (network B) in Fig. 9.3. Let WA and WB be the sets of weights
of network A and network B, respectively. Let fWA(iA) be the function computed
by network A, and gWB(iB) the function computed by network B, where iA and iB
are input vectors for networks A and B, respectively. If network B is embedded into
neuron X of network A with a ﬁbring function ϕ, the function computed by network
B becomes gW′
B(iB), where W′
B = ϕ(WB), and then the output of neuron X becomes
the output of network B, as the following example illustrates.
Consider the two simple networks (C and D) shown in Fig. 9.4. Let us assume,
without loss of generality, that the input and output neurons have the identity as their
activation function, while the hidden neurons have h(x) = tanh(x) as their activation
function. We use bipolar inputs ij ∈{−1,1}, Wjk ∈R, and outputs ok ∈(−1,1). The
output of network C is oC =W3C.h(W1C.i1C +W2C.i2C), and the output of network D
is oD =W3D.h(W1D.i1D+W2D.i2D). Now, let network D be embedded into networkC
as shown in Fig. 9.4. This indicates that the input potential of neuronY will inﬂuence
D according to the ﬁbring function ϕ. Let us refer to the input potential ofY as I(Y).3
In addition, this indicates that the output of D (oD) will inﬂuence C (in this example,
2 Universal approximators, such as feedforward neural networks, can approximate any (Borel)
measurable function in a compact domain to any desired degree of accuracy.
3 Note that, in this particular example, I(Y) = oC owing to the use of the identity as the activation
function in the output layer.

120
9 Fibring Neural Networks
Network C
i1C
i2C
W1C
W2C
W3C
OC
i1D
i2D
W1D
W2D
W3D
OD
ϕ
Network D
Y
Fig. 9.4 Fibring two simple networks
only the output of C). Suppose ϕ(WD) = I(Y)·WD, where WD = [W1D,W2D,W3D],
i.e. ϕ multiplies the weights of D by the input potential of Y. Let us use oC and oD
to denote the outputs of networks C and D, respectively, after they are ﬁbred. oD
is obtained by applying ϕ to WD and calculating the output of such a network, as
follows: oD = (I(Y).W3D) · h((I(Y) ·W1D) · i1D + (I(Y) ·W2D) · i2D). oC is obtaining
by taking oD as the output of neuron Y. In this example, oC = oD. Notice how
network D is being trained (as ϕ changes its weights) at the same time as network
C is running.
Clearly, ﬁbred networks can be trained from examples in the same way that
standard feedforward networks are (for example, with the use of backpropaga-
tion [224]). Networks C and D in Fig. 9.4, for example, could have been trained
separately before being ﬁbred. Network C could have been trained, for example,
with a robot’s visual system, while network D could have been trained with its
planning system. For simplicity, we assume for now that, once deﬁned, the ﬁbring
function itself should remain unchanged. Future extensions of the ﬁbring of neural
networks could, however, consider the task of learning the ﬁbring functions as well.
Not that, in addition to using different ﬁbring functions, networks can be ﬁbred in
a number of different ways as far as their architectures are concerned. The networks
of Fig. 9.4, for example, could have been ﬁbred by embedding network D into an
input neuron of network C (say, the one with input i1C). In this case, the outputs oD
and oC would have been oD = ϕ(W3D)·h(ϕ(W1D)·i1D +ϕ(W2D)·i2D), where ϕ is a
function of WD (say, ϕ(WD) = i1C ·WD), and then oC =W3C ·h(W1C ·oD+W2C ·i2C).

9.4 Deﬁnition of Fibred Networks
121
Let us now consider an even simpler example, which nevertheless illustrates the
power of the ﬁbring of networks. Consider two networks A and B, both with a single
input neuron (iA and iB, respectively), a single hidden neuron, and a single output
neuron (oA and oB, respectively). Let all the weights in both networks have a value 1,
and let the identity ( f(x) = x) be the activation function of all the neurons (including
the hidden neurons). As a result, we simply have oA = f(W2A · f(W1A · f(iA))) = iA
and oB = f(W2B · f(W1B · f(iB))) = iB, where W1A and W2A are the weights of net-
work A, and W1B and W2B are the weights of network B. Now, assume we em-
bed network B into the input neuron of network A. We obtain oB = f(ϕ(W2B) ·
f(ϕ(W1B) · f(iB))), and then oA = f(W2A · f(W1A · oB)). Since f(x) = x, we have
oB = ϕ(W2B) · ϕ(W1B)·iB and oA = W2A ·W1A · oB. Now, let our ﬁbring function be
ϕ(WA,iA,WB) = iA · WB, where WB = [W1B,W2B]. Since W1A,W2A,W1B, and W2B
are all equal to 1, we obtain oB = iA · iA · iB and oA = oB. This means that if we ﬁx
iB = 1, the output of network A (ﬁbred with network B) will be the square of its
input. As a result, if the sequence n,1/n,n + 1,1/(n + 1),n + 2,1/(n + 2),... for
n ∈R, is given as input to A (ﬁbred with B), the corresponding output sequence
will be n2,1,(n+1)2,1,(n+2)2,1,... Note that the input n changes the weights of
B from 1 to n, the input 1/n changes the weights of B back to 1, the input n + 1
changes the weights of B from 1 to n + 1, the input 1/(n + 1) changes the weights
of B back to 1, and so on.4 The interest in this sequence lies in the fact that, for al-
ternating inputs, the square of the input is computed exactly by the network for any
input in R. This illustrates an important feature of ﬁbred neural networks, namely
their ability to approximate functions in an unbounded domain [126,128]. This re-
sults from the recursive characteristic of ﬁbred networks as indicated by the ﬁbring
function, and will be discussed in more detail in the following section. Note that, in
practice, the ﬁbring function ϕ is deﬁned depending on the problem domain.
9.4 Deﬁnition of Fibred Networks
We shall now deﬁne ﬁbred neural networks (fNNs) precisely. Then, we shall deﬁne
the dynamics of fNNs, and show that fNNs can approximate unbounded functions.
For the sake of simplicity, we restrict the deﬁnition of ﬁbred networks to feedfor-
ward networks with a single output neuron. We also concentrate on networks with
linear input and linear output activation functions, and either a linear or a sigmoid
hidden-layer activation function. We believe, however, that the principles of ﬁbring
can be applied to any artiﬁcial-neural-network model.5 Below, we allow not only
two networks, but also any number of embedded networks to be nested into a ﬁbred
network. We also allow an unlimited number of hidden layers per network.
4 Since the ﬁbring function changes the weights of the embedded network, we use 1/n, 1/(n+1),
1/(n+2),... to reset the weights back to 1 during the computation of the sequence.
5 It would be particularly interesting to consider ﬁbring recurrent networks (i.e. networks with
feedback connections).

122
9 Fibring Neural Networks
Deﬁnition 46 (Fibring Function). Let A and B be two neural networks. A function
ϕn : I →W is called a ﬁbring function from A to B if I is the input potential of a
neuron n in A and W is the set of weights of B.
Deﬁnition 47 (Fibred Neural Network). Let A and B be two neural networks.
We say that B is embedded into A if ϕn is a ﬁbring function from A to B, and the
output of neuron n in A is given by the output of network B. The resulting network,
composed of networks A and B, is said to be a ﬁbred neural network.
Note that many networks can be embedded into a single network, and that net-
works can be nested so that network B is embedded into network A, network C
is embedded into network B, and so on. The resulting ﬁbred network can be con-
structed by applying Deﬁnition 47 recursively; for example, we ﬁrst embed C into
B and then embed the resulting network into A.
Example 19. Consider three identical network architectures (A, B, and C), each con-
taining a single linear input neuron, a single linear hidden neuron, and a single lin-
ear output neuron, as depicted in Fig. 9.5. Let us denote the weight from the input
A
x
Wh
Wo
OA
Wh
Wh
Wo
Wo
OB
OC
ϕB
ϕC
B
y
C
z
X
Y
Fig. 9.5 Nesting ﬁbred networks

9.5 Dynamics of Fibred Networks
123
neuron to the hidden neuron of network N, N ∈{A,B,C}, by W h
N, and the weight
from the hidden neuron to the output neuron of N by W o
N. Assume we embed net-
work C into the output neuron (Y) of network B, and embed the resulting network
into the output neuron (X) of network A (according to Deﬁnition 47). Let ϕB denote
the ﬁbring function from A to B, and ϕC denote the ﬁbring function from B to C. As
usual, we deﬁne ϕB = I(X) · WB and ϕC = I(Y) · WC, where I(X) is the input po-
tential of neuron X, I(Y) is the input potential of neuron Y, WB denotes the weight
vector [W h
B,W o
B] of B, and WC denotes the weight vector [W h
C,W o
C] of C. Initially, let
W h
A = √a, where a ∈R+, and W o
A = W h
B = W o
B = W h
C = W o
C = 1. As a result, given
an input x to A, we have I(X) = x√a. Then, ϕB is used to update the weights of
network B to W h
B = x√a and W o
B = x√a. If we had only networks A and B ﬁbred,
the input y = 1, for example, would then produce an output oB = ax2 for network B,
and the same (oA = ax2) for network A. Since network C is also embedded into the
system, however, given an input y to network B, the ﬁbring function ϕC is used to
update the weights of network C, using I(Y) as a parameter. Thus, if y = 1, we have
I(Y) = ax2, and the weights of network C are changed to W h
C = ax2 and W o
C = ax2.
Finally, if z = 1, the output of network C (and then that of networks B and A as well)
is a2x4. This illustrates the computation of polynomials in fNNs. The computation
of odd-degree polynomials and of negative coefﬁcients can be achieved by adding
more hidden layers to the networks, as we shall see later.
9.5 Dynamics of Fibred Networks
Example 19 also illustrates the dynamics of ﬁbred networks. Let us now deﬁne such
a dynamics precisely.
Deﬁnition 48 (Nested fNNs). Let N1, N2,...,Nn be neural networks. N1, N2,...,Nn
form a nested ﬁbred network if Ni is embedded into a neuron of Ni−1 with a ﬁbring
function ϕi for any 2 ≤i ≤n. We say that j−1 (1 ≤j ≤n) is the level of network Nj.
Deﬁnition 49 (Dynamics of FNNs). Let N1,N2,...,Nn be a nested ﬁbred network.
Let ϕi be the ﬁbring function from Ni−1 to Ni for 2 ≤i ≤n. Let ij denote an input
vector to network Nj, let Wj denote the current weight vector of Nj, let In(ij) denote
the input potential of the neuron nj of Nj into which Nj+1 is embedded given input
vector i j, let On j denote the output of neuron nj, and let fW j(ij) denote the function
computed by network Nj given Wj and i j, as in the standard way for feedforward
networks. The output oj of network Nj (1 ≤j ≤n −1) is deﬁned recursively in
terms of the output o j+1 of network Nj+1 as follows:
W j+1 := ϕ j+1(I(ij),Wj+1),1 ≤j ≤n−1,
on = fWn(in),
oj = fW j(ij,On j := o j+1),

124
9 Fibring Neural Networks
where fW j(ij,On j := o j+1) denotes the function computed by Nj when the output
of its neuron nj is substituted by the output of network Nj+1.
9.6 Expressiveness of Fibred Networks
Now that fNNs have been deﬁned, we proceed to show that, in addition to being
universal approximators, fNNs can approximate any polynomial function, and thus
are more expressive than standard feedforward neural networks.
Proposition 24. Fibred neural networks can approximate any (Borel) measurable
function in a compact domain to any desired degree of accuracy (i.e. fNNs are uni-
versal approximators).
Proof. This follows directly from the proof that single-hidden-layer feedforward
neural networks are universal approximators [140], together with the observation
that level-zero ﬁbred networks are a generalisation of single-hidden-layer feedfor-
ward networks.
■
Proposition 25. Fibred neural networks can approximate any polynomial function
to any desired degree of accuracy.6
Proof. Consider the level-zero network N in Fig. 9.6. Let n+1 (n ∈N) be the number
of input neurons of N, 0 ≤i ≤n, ai ∈R. Now, embed n −1 networks into the
input neurons of N, all at level 1, as indicated in Fig. 9.6 for networks A, B, and
C, such that network A is embedded into neuron A of network N, network B is
embedded into neuron B of N, and network C is embedded into neuron C of N.
Each of the n −1 embedded networks is used to represent one of x2,x3,...,xn. In
Fig. 9.6, A represents x2, B represents x3, and C represents xn. In the ensemble,
all networks, including N, contain linear neurons. A network Nj that represents x j
(2 ≤j ≤n) contains two input neurons (to allow the representation of a j ∈R), j−1
hidden layers, each layer containing a single hidden neuron (let us number these
h1,h2,...,h j−1), and a single output neuron. In addition, let aj/2 be the weight
from each input neuron to h1, and let 1 be the weight of any other connection in Nj.
We need to show that Nj computes a jx j. From Deﬁnition 49, given an input x to N
and ϕ j = x · Wj, the weights of Nj are multiplied by x. Then, given an input (1,1)
to Nj, neuron h1 will produce an output ajx, neuron h2 will produce an output ajx2,
and so on. Neuron hj−1 will produce an output a jx j−1, and the output neuron will
produce ajx j. Finally, by Deﬁnition 47, the neuron in N into which Nj is embedded
will present an activation ajx j, and the output of N will be ∑j ajx j. The addition of
a1x and a0 is straightforward (see network N in Fig. 9.6), completing the proof that
fNNs compute ∑i aixi.
■
6 Recall that, differently from functions in a compact domain, polynomial functions are not
bounded functions.

9.7 Discussion
125
1
x
A
x
B
x
1
C
x
1
.
.
.
A
1
1
1
B
1
1
1
C
1
1
1
…
Level 0 
Level 1
Level 1
Level 1
h1
1
N
a0
a1
1
Σi(aixi)
a2/2
h1
h1
hn-1
h2
a3/2
an/2
an/2
a3/2
a2/2
ϕA = x.WA
ϕB = x.WB
ϕC = x.WC
a2x2
a3x3
anxn
Fig. 9.6 Computing polynomials in ﬁbred networks
9.7 Discussion
This chapter has introduced a neural-network architecture named ‘ﬁbred neural
networks’ (fNNs), which combines a number of standard feedforward neural net-
works (which can be trained using backpropagation) by means of a ﬁbring function.

126
9 Fibring Neural Networks
We have shown that, in addition to being universal approximators, fNNs can ap-
proximate any polynomial function, therefore being more expressive than standard
feedforward neural networks. Pi–sigma networks [233, 272] are also more expres-
sive than standard networks, and can be seen as a special type of fNN where only
multiplication is allowed to be used in the ﬁbring function. In fact, it was sufﬁcient
to use only multiplication for the purpose of proving Proposition 25. In practice,
however, other functions might be more useful.
The question of which logics can be represented in fNNs is an interesting open
question. In particular, ﬁbring may be of interest for the representation of variables
and for reasoning with function symbols in connectionist ﬁrst-order logic systems,
as is discussed in some detail in Chap. 10. Another interesting line of work to pursue
would be on the ﬁbring of recurrent neural networks. Recurrent networks already
possess a limited ability to compute unbounded functions [126]. A comparison of
the computational capabilities of these two architectures would be highly desirable.
Ultimately, our goal is to strike a balance between the reasoning and learning ca-
pabilities of connectionist systems. There are generally two courses of action: (i) to
take simple network structures that support effective learning (e.g. [269]) and show
that they can represent languages more expressive than propositional logic (this is
our approach), or (ii) to take (more complex) connectionist systems capable of rep-
resenting expressive languages (typically ﬁrst-order logic [137]) and have efﬁcient
learning algorithms developed for those. This is necessary in practice because many
interesting real-world applications require languages more expressive than proposi-
tional logic; there are examples in failure diagnosis, engineering, and bioinformat-
ics. Bioinformatics, for example, requires an ability to reason about relations as used
in ﬁrst-order logic [6] and to learn from structured data. For neural-symbolic inte-
gration to be successful on this front, for example to compete with inductive logic
programming [189], it needs to offer such more expressive languages. We shall ad-
dress the problem of performing relational learning in neural-symbolic systems in
Chap. 10.

Chapter 10
Relational Learning in Neural Networks
Neural networks have been very successful as robust, massively parallel learning
systems [125]. On the other hand, they have been severely criticised as being essen-
tially propositional. In [176], John McCarthy argued that neural networks use unary
predicates only, and that the concepts they compute are ground instances of these
predicates. Thus, he claimed, neural networks could not produce concept descrip-
tions, only discriminations.
Since then, there has been a considerable amount of work on representing ﬁrst-
order logic in artiﬁcial neural networks [2, 3, 13, 17, 114, 133, 137, 204, 205, 229,
230] (see [138] for a collection of papers on the representation of ﬁrst-order logic).
Generally speaking, the community has focused on the following three issues:
(i) showing the equivalence of neural networks with Turing machines and super-
Turing computation (i.e. the computation of nonTuring-computable functions)
[234];
(ii) computing (or approximating the computation of) ﬁrst-order logic using neural
networks (using theorem proving [152] and semantics [229,259]);
(iii) tackling ﬁrst-order applications (notably language processing [239] and rela-
tional databases [252]), though this has mainly been done through proposition-
alisation [223].
Using networks derived from the CILP system, languages more powerful than
classical propositional logic were considered in Chaps. 5 to 7 and [20, 120, 136],
but the networks were kept simple in an attempt to beneﬁt from efﬁcient in-
ductive learning. In [20], for example, the LINUS inductive logic programming
(ILP) system [159, 160] was used to translate ﬁrst-order concepts into a proposi-
tional attribute-value language, and then CILP was applied. The idea was to in-
duce relational concepts with neural networks using LINUS as the front end.1 Some
1 In fact, our interest in LINUS originated from the fact that it provides a translation from ﬁrst-
order, nonrecursive logic programs to the form of attribute-value propositions, and vice versa, the
idea being that any efﬁcient machine learning method could be used in the process of hypothesis
generation between these translations. In [20], we simply chose to use CILP neural networks as
such a method.
A.S. d’Avila Garcez et al., Neural-Symbolic Cognitive Reasoning, Cognitive Technologies,
127
c⃝Springer-Verlag Berlin Heidelberg 2009

128
10 Relational Learning in Neural Networks
ﬁrst-order inductive learning tasks, taken from the literature on symbolic machine
learning, were learned successfully, thus indicating that such simple neural networks
can indeed induce relations.
One may argue, however, that the combination of CILP and LINUS is not differ-
ent from the CILP experiments using ground programs [66], because the learning
process itself is essentially propositional. The following statement is as true today
as it was in 1999:
‘Despite the progress in knowledge-based neurocomputing (KBN), many open problems
remain. KBN can not yet harness the full power of predicate logic representations frequently
used in AI. Although good progress has been made to represent symbolic structures in
KBN, the dynamic variable binding problem remains to be solved. Further questions to
address include the development of new integration strategies and more diverse types of
knowledge representations, e.g., procedural knowledge, methods to exchange knowledge,
and reasoning capabilities’ [51].
This is a vast area for further research. In this chapter, we argue that the focus of
the research should be on relational learning [84]. In particular, instead of solving
the connectionist variable-binding problem, we are concerned about answering the
following question: are neural networks capable of generalising rules of the form
Q(Y,Z) →P(X,Y) from instances such as Q(b,c) →P(a,b) and Q(d,e) →P(c,d)?
Standard neural networks lack two concepts that are fundamental in symbolic ar-
tiﬁcial intelligence, namely variables and relations. Are neural networks effective
precisely because they do not use such concepts as a matter of course? In what fol-
lows, we try to answer this question negatively (although it may well be the case
that neural networks are more effective when they use propositionalisation).
We introduce a standard representation for variables and relations that preserves
the simplicity of neural networks, and in doing this we seek to also maintain effec-
tive, robust learning capabilities. This approach has advantages over previous work
in that it seeks to strike a more natural balance between two clearly important con-
cepts in symbolic AI and standard connectionist systems (instead of imposing sym-
bolism onto connectionism, which leads frequently to rather inefﬁcient systems).
In a nutshell, we see variable binding as a learning task, and we use temporal syn-
chonisation between networks in an ensemble to learn and reason about relations.
A network may represent a concept Q(Y,Z) and another network may represent a
concept P(X,Y). Then, if the training examples associated with Q and P are syn-
chronised, a relation between these two concepts/networks needs to be learned, for
example Q(Y,Z) →P(X,Y). We use the key idea of metalevel networks relating
networks Q and P to implement this, as detailed in what follows.
10.1 An Example
Let us start by considering an example traditionally used in ILP [189] to illustrate
relational learning in neural networks. Consider the variables X,Y,Z and the rela-
tions (predicates) father, mother, and grandparent. Our goal is to learn a description

10.1 An Example
129
of grandparent from examples of the father, mother, and grandparent relations such
as father(charles, william), mother(elisabeth, charles), and grandparent(elisabeth,
william). To help in the ILP search for the goal description, some background
knowledge may be available; for example, the concept of parent may be useful (par-
ent(X, Y) iff mother(X, Y) or father(X, Y)). Given a set of examples e and background
knowledge bk, the ILP learning problem can be expressed as ‘ﬁnd a hypothesis h
such that bk ∪h ⊢e’. The bulk of the work is then on how to search the hypoth-
esis space (in this example, the space of possible descriptions of grandparent in
terms of mother and father). The idea of inverse resolution is typically used [189].
For example, given grandparent(elisabeth, william) and mother(elisabeth, charles),
one can induce grandparent(X, Y) if mother(X, Z) with substitutions X/elisabeth,
Y/william, and Z/charles, and given this and father(charles, william), one can in-
duce grandparent(X, Y) if mother(X, Z) and father(Z, Y). Sometimes, negative ex-
amples can also be used to help reduce the hypothesis space, mainly by checking
for consistency between the negative examples and the induced rules; father(philip,
william) would be a negative example.
As can be seen, ILP is very appealing from the point of view of the rich descrip-
tions it can generate with the use of variables and relations. ILP is richer than for
example, probabilistic relational models (PRMs) [93], which are in turn more efﬁ-
cient than ILP. And ILP is more comprehensible than neural networks or support
vector machines [232], which are in turn more robust than ILP or PRMs. Most of
the criticism of ILP concerns the lack of efﬁcacy of its methods and algorithms.
The response of the ILP community has been mainly to try to create more efﬁcient
algorithms, and to obtain better accuracy in speciﬁc applications. Instead, we see
learning as a long-term adaptation process, the product of experience, arising from
the stimulation of neurons in the brain that results from varied cognitive stimulation.
The modelling of learning therefore should, in our view, account for the necessary
parallelism, robustness, and fault tolerance associated with this process. Contrast,
for example, the ILP approach with Doumas and Hummel’s comparison-based rela-
tional learning [82], which, despite the use of propositionalisation, is well founded
on the above aspects of human cognition.
In most real-life domains, one cannot state that P(X) is true from a single ob-
servation of, say, P(a), even if all there is available is P(a). In our view, a system
that does this – we may call it hasty generalisation (some may call it a form of
overﬁtting) – is bound to achieve poor generalisation at a later stage because there
is very little support/evidence for the target predicate in the set of examples.2 Notice
further that ILP does not in general tolerate noise. This is true for most ILP systems,
2 Notice that when hasty generalisation takes place, the role of negative examples becomes crucial
for trying to curb wrong conclusions, but in real-world applications negative examples are harder to
come by. Consider, for example, a power plant case study where alarms in a power plant have to be
associated with faults in the transmission lines and generators. Sensors in the system relate alarms
and associated possible faults over time, and these form our set of training examples. Of course,
there may be noise in the system because the sensors themselves may be faulty; the training system
should be able to deal with this. Now, a useful negative example is not simply the complement of
the set of examples; it is information about a component in the system which we are certain is
not faulty and therefore did not cause the alarm. To obtain such a negative example (ruling out a

130
10 Relational Learning in Neural Networks
although probabilistic and kernel ILP try to address this problem [158]. Noise may
occur in the form of a mistaken observation (an incorrect positive example in the
data set), for example mother(elisabeth, william) in the above family relationship
problem, or in the form of contradicting background knowledge rules, which are
common, for example, in fault diagnosis domains. Noise intolerance is unrealistic
in most applications. Neural networks have been shown to be particularly suitable
for dealing with noise.
10.2 Variable Representation
Let us look at the above example of family relationships in the context of neural-
symbolic integration. Our goal is to offer a neural-network framework for relational
learning. We shall take inspiration from the framework of connectionist modal logic
(CML) [79], since it can represent relations in the form of modal logic’s accessibility
relations. How could it be used to represent relations such as mother(X,Y)? The ﬁrst
question we need to answer, however, is how can we represent variables such as X
and Y and, perhaps more importantly, what is an appropriate representation for the
purpose of machine learning?
We know that variable grounding (propositionalisation) can be quite effective
in many real-world applications, but it may lead to very large (or even inﬁnite)
networks, according to the size of the domain. As discussed above, in this chapter
we want to consider alternatives to propositionalisation. With this in mind, it seems
natural to treat each neuron as a variable so that the burden of instantiation is left to
the network’s inputs (which, in turn, can be any real number). The task of learning
relationships between variables would then be analogous to that of learning rules
in propositional logic, but in the space of reals. One could have a network mother
whose task was to learn to relate variables/input neurons X and Y. This network’s
output neuron3 should be activated whenever the values of X and Y are to be related.
As usual, we say that a neuron is activated whenever it presents an activation
greater than Amin ∈R+. We need to provide a mapping of variable instances to real
numbers. One such mapping is simply to denote instances a,b,c,... by the naturals
1,2,3,..., respectively. Now, suppose that mother(a,b) and mother(b,c) are true.
The network needs to learn that the inputs (1,2) and (2,3) should activate the output
neuron mother. In practice, for learning purposes, we should associate each instance
with an interval so that a is associated with an interval 1±ε, b with a interval 2±ε,
and so on, where ε is a small real number. If, in addition, mother(X,Y) is true, then
the output neuron should be activated whenever X > Amin and Y > Amin. In this case,
it is desirable that Amin +2ε < 1 so that Amin ∈(0,1) and there is no overlap with the
fault), an engineer would have to analyse the behaviour of the component in relation to its detailed
speciﬁcation, testing it extensively, possibly in the context of the entire system.
3 When it is clear from the context, we use the term mother to refer to the output neuron as well as
the network.

10.3 Relation Representation
131
interval for a. In summary, if P(X) holds, we activate output neuron P for any input
X > Amin. If P(a) holds, we activate P for X = 1 ± ε. If P(b) holds, we activate P
for X = 2±ε, and so on.
10.3 Relation Representation
So far, our networks represent predicates (or concepts), having variables as input
neurons. The next step is to learn how to relate predicates by relating networks. In
other words, we need to be able to represent relations between concepts as con-
nections between networks. Relations are a metalevel notion. One can think of the
learning of relations as being at a different level of abstraction from the learning of
a concept. A parallel can be drawn here with the idea of different abstraction lev-
els in a Kohonen map [112]. We see those different levels, however, as conceptual
(or for organisational purposes only), and we believe that the structures in one level
should be the same as those in the next. Hence, we may have a network that repre-
sents P(X,Y), a network that represents Q(Y,Z), and a network that represents the
fact that, for example, P(X,Y) →Q(Y,Z). These can be organised into a network
ensemble so that one can see such relationships, but it should also be possible to
present the ensemble as a single, large massively parallel structure because the prin-
ciples governing the entire model ought to be the same. With this in mind, if we
consider that a single-hidden-layer network represents concept P, and that another
single-hidden-layer network represents concept Q, a third single-hidden-layer (met-
alevel) network should be sufﬁcient to represent a relation between P and Q. These
are relations between the concepts P and Q, which are effectively encoded in the set
of weights and the hidden layers of such networks. So, if Q is our target predicate
(e.g. grandparent in the example above), a metanetwork maps the hidden layer of
network P to the output of network Q through a number of hidden neurons of its
own (only now at a different level of abstraction). If P were the target predicate for
learning, then the metanetwork would take the hidden layer of Q as input and the
output of P as output.
Figure 10.1 illustrates the idea. The object-level networks represent P(X,Y),
Q(Y,Z), and R(X,Z). In addition, a metalevel network maps P and Q to R, our target
predicate in this example. This network itself may have a number of hidden neurons
(two in Fig. 10.1) through which relations such as P(X,Y)∧Q(Y,Z) →R(X,Z) can
be learned. The learning process in the metanetwork can be the same as that in the
object-level networks. This allows general relational knowledge to be represented
in relatively simple, modular networks.
It is worth noting that, given our deﬁnition of ﬁbring in Chap. 9, the represen-
tation of function symbols should come naturally with this framework. If a neuron
represents a variable X and this neuron is ﬁbred into a network that computes a
function f, then the neuron’s output can be made to represent f(X) through ﬁb-
ring. We know that neural networks can compute (and indeed learn to approximate)
any n-ary Borel measurable function. So, it is not difﬁcult to see that, with ﬁbring,

132
10 Relational Learning in Neural Networks
X
Z
R
P
X
Y
Y
Z
Q
Fig. 10.1 Representation of relational knowledge
we can represent predicates containing function symbols such as P(f(X),Y,g(Z)).
Notice how, for the purposes of learning, although it is quite general, this structure
is very modular, with each function being computed by a separate network in the
ensemble, which can itself be treated as a black box within the system and trained
separately. It should not be very difﬁcult to show that this set-up can account for
n-ary functions and composite functions with the use of nested ﬁbring [67, 101].
However, we leave this as future work for now.
10.4 Relational Learning
Let us continue to use the family relationship example to illustrate relational learn-
ing. Consider the target predicate grandparent, for which a description needs to be
learned, given examples of the predicates father and mother (all binary predicates).
Assume that the following training examples are available: grandparent(elisabeth,
william), mother(elisabeth, charles), and father(charles, william). Each example
can be presented to each of the object-level networks separately for training, fol-
lowing some convention for a numerical representation for each instance, say,
elisabeth = 1, charles = 2, and william = 3. In addition, we say that the metalevel
network is to be trained whenever examples are presented simultaneously to the
object-level networks. The idea is that if examples are presented to two or more
networks at the same time, then a relation between the concepts associated with
these networks can be learned. Imagine a child who, when faced with many ex-
amples of who is the mother or the father of whom, still does not seem to create
a description of grandparent. Only when this child pays attention to the relevant
examples together, visualising them at the same time either physically or mentally,
will he or she manage to establish the appropriate relationship. When examples are
presented to the object-level networks at the same time, relational learning among
such networks can take place in the metalevel. When a child visualises elisabeth,

10.4 Relational Learning
133
charles, and william together as mother, father, and grandchild, he or she may in-
duce a (hopefully more general) description of an attentional target concept. We are
therefore proposing that the timing of one’s experience with respect to the target
concept be treated as a crucial aspect for relational learning. This idea of temporal
synchrony is akin to the synchronisation used by Shastri and Ajjanagadde’s SHRUTI
system [229]. Notice, however, that we do not seek to impose synchronisation as a
feature of the system. If neurons are labelled as X, the networks do not control the
fact that identical input values should be presented to such neurons. Instead, given
simultaneous examples mother(elisabeth,charles) and father(charles,william), if
charles is associated with Y in the network mother, then charles is expected to be
associated with Y in the network father.
Consider Fig. 10.1 again. Whenever training examples are presented simultane-
ously to the object-level networks P, Q, and R, a learning step can be triggered
in the metalevel network. The activation values of the hidden neurons of P and Q
become the input vector, and the activation value of the output of R becomes the tar-
get output for the metalevel network. The algorithm below describes the idea more
precisely for an ensemble in the general case. Notice how, for the family relation-
ship problem, if we let network P denote the mother predicate, network Q denote
the father predicate, and network R denote the grandparent predicate, a training
example <iP,oP> where iP = (1,2) and oP = 1 will represent mother(elisabeth,
charles), a training example <iQ,oQ> where iQ = (2,3) and oQ = 1 will repre-
sent father(charles,william), and a training example <iR,oR> where iR = (1,3) and
oR = 1 will represent grandparent(elisabeth, william). Let Acth1
P (iP) and Acth2
P (iP)
denote the activation states of the hidden neurons of network P given an input vec-
tor iP (given a current set of weights for P). Similarly, let Acth1
Q (iQ) and Acth2
Q (iQ)
denote the activation states of the hidden neurons of network Q for an input vector
iQ (given a current set of weights for Q). Finally, let ActR(iR) denote the activation
state of the output neuron of network R for an input iR and a set of weights for R.
If iP, iQ, and iR are presented at the same time to the networks, a new training ex-
ample <i,o> can be created, where i = (Acth1
P (iP),Acth2
P (iP),Acth1
Q (iQ),Acth2
Q (iQ))
and o = ActR(iR). Since the target concept grandparent is represented by network
R, a metalevel network from P and Q to R can be trained with the example <i,o>.
This network, call it R(P,Q), contains the hidden neurons of P and Q as input, and
the output neuron of R as output. The number of hidden neurons in R(P,Q) is a
matter for the learning process, as are the numbers of hidden neurons in P, Q, and R
themselves. For learning, R(P,Q) can be treated just like P, Q, R. In the experiments
described below, we used backpropagation [224].4
4 A complete learning of the grandparent concept would require synchronisation of the rele-
vant examples combining the necessary father and mother predicates. For instance, if examples
mother(elisabeth, ann), mother(ann, zara), and grandparent(elisabeth, zara) were made available
for learning synchronisation, a copy P′ of the mother network would be needed, so that a metanet-
work R(P,P′) could be trained. Alternatively, intermediate concepts such as parent, grandmother,
and grandfather could be used. In any case, we place the burden of uniﬁcation and clause copying
on the set-up/synchronisation of the training examples, not on the network ensemble itself.

134
10 Relational Learning in Neural Networks
Relational Learning Algorithm
1. Let N1,N2,...,Nn be single-hidden-layer neural networks in a network en-
semble. Let P1,P2,...,Pn denote the concepts (predicates) represented by
N1,N2,...,Nn, respectively.
2. Let Pn represent a target predicate for learning.
3. Let hj
1,...,h j
m denote the hidden neurons of network Nj, 1 ≤j ≤n.
4. Let on
1,...,on
m denote the output neurons of (target) network Nn.
5. Let Actx(i) denote the activation state of neuron x given input vector i (and a
current set of weights).
6. For each input vector in presented to Nn at a time point t, if input vectors
i1,i2,...,in−1 are presented to any of N1,N2,...,Nn−1 at the same time point
t, then:
(a) Create a single-hidden-layer neural network Nn(N1,N2,...,Nn−1) with in-
put neurons h1
1,h1
2,...,h1
m,h2
1,h2
2,...,h2
m,...,hn−1
1
,hn−1
2
,...,hn−1
m
, and output
neurons on
1,on
2,...,on
m (if one does not exist yet).
(b) Create a training example < i,o >t where i = (Acth1
1(i1), Acth1
2(i1),...,
Acth1m(i1), Acth2
1(i2), Acth2
2(i2),...,
Acth2m(i2),..., Acthn−1
1
(in−1), Acthn−1
2
(in−1),..., Acthn−1
m (in−1)) and o = (Acton
1(in), Acton
2(in),..., Actonm(in)).
7. Train Nn(N1,N2,...,Nn−1) with examples < i,o >t; the network will contain
relational knowledge about Pn with respect to P1,P2,...,Pn−1.
10.5 Relational Reasoning
In the propositional case, we have seen that we can associate neurons with atoms,
and neuronal activation with truth values, so that reasoning and learning can be
combined (reasoning being associated with network computation and learning with
network structure). When we associate neurons with variables, the question of vari-
able binding springs to mind. From the point of view reasoning, the problem lies
in how to associate different neurons labelled as the same variable with the same
constant. In our setting, the answer to this is left to the learning process. Assume,
for example, that P(X) →Q(X). Whenever output neuron P is activated, output
neuron Q should be activated. This is done through the metanetwork. Recall that
we use the parameter Amin to say that X is true. Therefore, if the hidden layer of
P activates Q for X > Amin, we say that P(X) →Q(X). If X = 1 ± ε, we say that
P(a) →Q(a). If X = 2 ± ε, we say that P(b) →Q(b), and so on. Using this map-
ping, we can set up a network quite easily to implement P(X) →Q(X) for all X.
Now, assume that P(X) →Q(a). If, in addition, P(X) is a fact, then output neuron P
should be activated for any input value X > Amin. Output neuron Q, however, should
be activated when P is activated and X = 1 ± ε only, denoting Q(a). Finally, sup-
pose that P(a) →Q(X). Output neuron P should be activated for input X = 1 ± ε

10.6 Experimental Results
135
(denoting a), and the hidden layer of P should activate output neuron Q through the
metalevel network when the input to Q is X > Amin.
Most connectionist ﬁrst-order logic systems require multiple copies of neurons
for the various variable bindings. This is a difﬁcult problem, and we do not claim to
have solved it. However, as mentioned above, most such systems also have to deal
with the added complexity of consistency control across the network. Instead, we
avoid this problem by placing the burden of binding on the learning process. Take,
for example, the program P = {P(a);P(b);P(X) →Q(X);Q(a) ∧Q(b) →R(Y)}.
We need a network P for which the input a can be provided, and a network P′ for
which the input b can be provided simultaneously. Then, the hidden layers of P and
P′ need to be connected to the outputs of Q and Q′, respectively, and, ﬁnally, the
hidden layers of Q and Q′ need to be connected to the output of R. The fact that Q
and Q′ should activate R for inputs a and b, respectively, is to be learned from the
set of examples.
As another example, consider again the family relationship problem. With the
neurons already labelled as X, Y, Z, we can afford to ignore variable binding, and
either learn the relationships between the variables or learn the relationships be-
tween the constants. In order to learn the relationships between the variables, we
simply treat relational learning as propositional and use Amin instead of instances
1,2,3. It is up to the learning process to synchronise the relevant examples and
assign appropriate input and output values to the neurons labelled as the same vari-
ables across the network ensemble. It is interesting to note, though, that different
assignments of values to the constants may affect the learning performance in any
particular application, so that alternative mappings other than the simple, uniform
distribution used here should be considered (e.g. a bimodal or Gaussian distribu-
tion). The choice of distribution can be treated as data preprocessing, again as part
of the learning process.
10.6 Experimental Results
In this section, we use Michalski’s archetypal example of east–west trains [181] to
illustrate the relational model.5 In this example, the goal is to classify the eastbound
and westbound trains, each train having a set of cars as shown in Fig. 10.2. In order
to classify a train, certain features of the train, along with features of its cars, must
be considered.
The data set contains the following attributes: for each train, (a) the number of
cars (3 to 5), and (b) the number of different loads (1 to 4); and for each car, (c)
the number of wheels (2 or 3), (d) the length (short or long), (e) the shape (closed-
top rectangle, open-top rectangle, double open rectangle, ellipse, engine, hexagon,
jagged top, open trap, sloped top, or U-shaped), (f) the number of loads (0 to 3), and
(g) the shape of the load (circle, hexagon, rectangle, or triangle). Then, ten boolean
variables describe whether any particular pair of types of load are on adjacent cars
5 We are grateful to Rafael Borges for pointing out errors in a previous version of this chapter, and
for helping us with the running of some of the experiments.

136
10 Relational Learning in Neural Networks
Fig. 10.2 Data set for east–west trains data
of the train (each car carries a single type of load): (h) there is a rectangle next
to a rectangle (false or true), (i) a rectangle next to a triangle (false or true), (j) a
rectangle next to a hexagon (false or true), (k) a rectangle next to a circle (false or
true), (l) a triangle next to a triangle (false or true), (m) a triangle next to a hexagon
(false or true), (n) a triangle next to a circle (false or true), (o) a hexagon next to
a hexagon (false or true), (p) a hexagon next to a circle (false or true), and (q) a
circle next to a circle (false or true). Finally, the class attribute may be either east or
west. The number of cars in a train varies from 3 to 5. Therefore, attributes referring
to properties of cars that do not exist are assigned the value false. As usual, −1 is
used to denote false and 1 to denote true in the case of boolean variables. Further,
we assign values 1,2,3,... to any attributes that have multiple values, in the order
which they are presented above. So, in the case, for example, of the shape of the
load, 1 is used to denote circle, 2 to denote hexagon, 3 to denote rectangle, and so
on. Of course, for the corresponding neurons, instead of the bipolar function, we use
a linear activation function h(x) = x.
Let us compare two ways of performing neural modelling of the above exam-
ple: one which is akin to propositionalisation, and another way, which is our re-
lational learning model with metanetworks. In the former, a network containing
32 input neurons and one output neuron (denoting east) was used, which we call
the ﬂat network to contrast with the metanetwork approach. The 32 inputs en-
code: the number of cars in a train; the number of different loads in a train; the
number of wheels, the length, and the shape of each car; the number of loads
in each car; the shape of the load of each car; and the ten boolean variables de-
scribed above. In the latter model, 11 networks were created, one for each con-
cept, as follows: num cars(t,nc), where the number of cars is nc ∈[3..5] in a
train t ∈[1..10]; num loads(t,nl), where the number of loads is nl ∈[1..4] in a
train t; num wheels(t,c,w), where the number of wheels is w ∈{2,3} in a car
c ∈[1..4] of train t; length(t,c,l), where the length is l ∈{−1,1}6 for car c in train
t; shape(t,c,s), where the shape of a car is s ∈[1..10];7 num car loads(t,c,ncl),
6 Here, −1 denotes short, and 1 denotes long.
7 As before, 1 denotes closed-top rectangle, 2 denotes open-top rectangle, and so on.

10.6 Experimental Results
137
where the number of loads in a car is ncl ∈[0..3]; load shape(t,c,ls), where the
shape of a car’s load is ls ∈[1..4]; next crc(t,c,x), where car c has an adjacent car
loaded with circles, x ∈{−1,1};8 next hex(t,c,x), where car c has an adjacent car
loaded with hexagons; next rec(t,c,x), where car c has an adjacent car loaded with
rectangles; and next tri(t,c,x), where car c has an adjacent car loaded with trian-
gles. Each concept/predicate was represented by a network containing the predi-
cate’s arity as the number of inputs and a single output with a value in {−1,1}; for
example, length(t,c,l) had input neurons t, c, and l, and an output neuron length = 1
when l was the correct length for the car in question, or length = −1 otherwise. In
addition, a metanetwork was used to map the hidden layers of the 11 networks to
east ∈{−1,1}.
We trained the ﬂat network with backpropagation using a learning rate of 0.3,
a term of momentum of 0.4, and nine hidden neurons. We performed leaving-one-
out cross-validation on a set of 10 examples. Each training set was presented to the
network for 10000 epochs. The test set results are shown in Table 10.1. If we use a
threshold Amin = 0.5, then outputs in the interval [0.5, 1) are associated with east,
and outputs in the interval (−1, −0.5] are associated with west. We can conclude
that four out of the ﬁve eastbound trains and three out of the ﬁve westbound trains
were classiﬁed correctly. Considering, however, the mean square error (mse) on the
test set, the network shows a test set performance of 62% (mse = 0.38).
We then trained the 11 networks of the metanetwork by splitting the original 10
examples into 40 examples (10 trains × 4 cars). We trained each network for 30000
epochs9 on all available examples.10 Here we had to use a much smaller learning
Table 10.1 Test set results for ﬂat network
Train
Output of ﬂat network
Desired output
Class
1
0.84
1
east
2
0.97
1
east
3
−0.39
1
east
4
0.99
1
east
5
0.51
1
east
6
−0.59
−1
west
7
0.35
−1
west
8
−0.77
−1
west
9
0.84
−1
west
10
−0.99
−1
west
8 Here, −1 denotes false and 1 denotes true. If car c is next to circles, then next crc is true when
x = 1 and false when x = −1. Otherwise, next crc is true when x = −1 and false when x = 1.
Alternative ways of modelling the loads of adjacent cars are possible, for example next crc(t,c) or
next to(t,c,ls).
9 This took about 30 seconds on a mid-range personal computer.
10 Notice that at this stage the examples did not contain any information as to whether a train was
eastbound or westbound.

138
10 Relational Learning in Neural Networks
num_cars
. . .
t
nc
num_loads
. . .
t
nl
num_wheels
. . .
t
w
c
next_tri
. . .
x
c
. . .
t
east
Fig. 10.3 Metalevel network for the east–west trains example
rate η = 0.01 and term of momentum μ = 0.01, and a relatively large number (20)
of hidden neurons. This was necessary so that the networks could learn the intricate
relationships between the variables; learning from real-valued data clearly seems
harder in this case than learning from binary data. Figure 10.3 shows the set-up of
the network.
For the metanetwork, we performed leaving-one-out cross-validation (leaving
out one train at a time); we left out four examples for testing at a time, and trained the
metanetwork on the remaining 36 examples. Each set of 4 examples corresponded
exactly to the possible cars in a particular train. In this way, we made sure that the
metanetwork could not use information about a car in a train in order to classify
another car in the same train. This is probably the most appropriate way of compar-
ing the learning performance of the metanetwork with that of the ﬂat network, for
which leaving-one-out cross-validation was used on the set of 10 trains. We used a
learning rate of 0.3 and a term of momentum of 0.4 to train the metanetwork for
10000 epochs, as was done for the ﬂat network. We chose to use only three hid-
den neurons to connect the metanetwork’s 220 input neurons (11 networks with 20
hidden neurons each) to an output neuron denoting east.
The metanetwork classiﬁed nine out of the 10 trains correctly. The number of
cars correctly classiﬁed, again using Amin = 0.5, is shown in Table 10.2. Only train
5 was not classiﬁed correctly, with an average network output very close to zero
in this case. Rather than a wrong classiﬁcation of train 5 as westbound, this output
indicates a high level of uncertainty. The metanetwork shows a test set performance
of 92.5% (mse = 0.075).
The results are an indication of the potential of metalevel networks as a model for
fault-tolerant, robust relational learning. The main differences between the two ap-
proaches are (i) they use different network architectures, and (ii) metanetworks can
learn from structured examples. Because the examples were structured the metanet-
work included the labels of the trains (variable t) and cars (variable c) as part of

10.6 Experimental Results
139
Table 10.2 Test set results for the metanetwork
Train
Cars correctly classiﬁed
Average network output
Desired output
Class
1
4 out of 4
1.00
1
east
2
3 out of 3
0.99
1
east
3
3 out of 3
1.00
1
east
4
4 out of 4
1.00
1
east
5
1 out of 3
0.09
1
east
6
2 out of 2
−1.00
−1
west
7
3 out of 3
−0.98
−1
west
8
2 out of 2
−1.00
−1
west
9
4 out of 4
−1.00
−1
west
10
2 out of 2
−1.00
−1
west
the learning. Although this is common in ILP and other types of symbolic machine
learning, it is normally not the case for connectionist learning. Neural-symbolic
systems can promote learning from structured data, as the east–west trains example
illustrates. However, a more detailed study of how the labels can inﬂuence learning
performance is required (including how the labelling order may affect results).
Training the ﬂat network to a training-set mse very close to zero was straightfor-
ward. Training the 11 networks associated with the predicates proved to be harder,
with the mse on the training set remaining at about 10% after 30000 epochs. As men-
tioned, we attribute this to the real-valued examples used by such networks. Training
the metanetwork, however, turned out to be straightforward, with the training-set
mse reducing quickly to zero after as little as 5000 epochs. Recall that this metanet-
work contained 220 input neurons. Even then, learning was very efﬁcient, taking
less than 10 seconds for each fold. Although one may argue that the metanetwork
turned out to be more complex in structure (with 663 weighted connections) than
the ﬂat network (with 297 connections), the gain in generalisation and the mod-
ularity of the networks11 offer the metanetwork approach a clear advantage over
propositionalisation.
It may help to think of the relational approach as learning in two stages, at the
object-level and metalevel. The two stages seem to add to the system’s generali-
sation capability, reducing the chances of overﬁtting. Further investigations are re-
quired, however: in particular, comparative larger-scale experiments. Performance
comparisons with ILP and PRMs with respect to accuracy and learning time in both
noisy and uncertain domains are also needed.
There is a clear trade-off and a balance to be struck between expressiveness and
performance in machine learning. The relational model proposed here tries to ad-
dress this issue. It departs from the idea of learning as inverse resolution, but ac-
knowledges the need for rich relational structures. It could be seen as an alternative
to ILP, but it could equally and more generally be seen as a relational framework
11 Notice that each network can be trained independently and then deployed in a particular
relational-learning scenario.

140
10 Relational Learning in Neural Networks
encompassing connectionist learning and symbolic approaches, where the idea of
modular, object-level, and metalevel learning is embraced.
In summary, this chapter’s message is that networks can represent concepts and
variables, and when examples are presented simultaneously to networks, relations
between such concepts can be learned by a metanetwork, which has the same form
as the original networks but is at a different level of abstraction. This structure
prompts us to organise the problem in terms of concepts and relations, and to learn
in stages from structured data, hopefully producing a better generalisation perfor-
mance.
10.7 Discussion
Although symbolic descriptions of learning can be clear and unambiguous, ﬁrst-
order logic may not be the appropriate representation for learning and, as natural as
it may seem, inverse resolution may equally well not be the most appropriate method
for learning. Of course, logical descriptions are generally more convenient than in-
terconnected neurons/graphs (some graphical representations may be easier to un-
derstand than logic, but this is not true in general in the case of neural networks).
For example, despite the encouraging results obtained above on the east–west trains
dataset, it is not easy to spot by inspecting the metanetwork that the classiﬁca-
tion problem can be described as car(T,C)∧short(C)∧closed top(C) →east(T).
This motivates the research that is being done on knowledge extraction from neural
networks. In this book, we have focused on representation and reasoning. How-
ever, (ﬁrst-order) knowledge extraction is an important part of neural-symbolic
integration.
In a previous book [66], a general knowledge extraction method was proposed.
Others have also proposed extraction methods, and for each logic considered in
this book, there is scope for further research on knowledge extraction. Most of the
work so far on knowledge extraction has used production rules or interval rules, i.e.
propositional logic. Clearly, rules containing relations, modalities, variables, etc.
would improve the comprehensibility of the rule set, and possibly offer more effec-
tive extraction methods. Extraction is an integral part of neural-symbolic systems
whenever these are required to provide descriptions as opposed to discriminations.
This book opens up a number of avenues for research on knowledge extraction. We
believe that the general extraction method proposed in [66] can be adapted to the
various logics considered in this book. Since we have shown that connectionism
can deal with such logics, there is no reason to believe that we should not be able
to perform knowledge extraction based on such representations, and indeed beneﬁt
from it, particularly in terms of rule set comprehensibility.
Another interesting area for further research is goal-directed reasoning [107].
This form of reasoning may be advantageous from a computational standpoint.
Goal-directed reasoning can be thought of as reversing the network in order to rea-
son backwards from a goal, say east(T), to subgoals, say car(T,C) and short(C),

10.7 Discussion
141
until facts are reached, for example short(c1). Unfortunately, the problem is more
complicated than simply reverting the network, because the inverse relation is not
one-to-one. This is similar to the problem of abductive reasoning, i.e. the problem
of reasoning to the best explanation. In a nutshell, given a →b, if b is observed as a
matter of course, then a is a possible explanation as to why it is so. One can assume
that a is true (i.e. abduce a) and reason hypothetically towards an explanation for b.
If, in addition, c →b, then c is an alternative explanation for b, also to be consid-
ered in this hypothetical reasoning process. We do not tackle abductive reasoning in
this book, but we would like to point the interested reader to [58], where we have
used CML and possible worlds to label such alternative hypotheses, thereby con-
trolling the fact that the inverse relation is not one-to-one. However, a considerable
amount of work is still needed in this area. Regarding, for example, the integration
of learning and abductive (or goal-directed) reasoning, a key open question is how
to control the reasoning process while still providing the system with the ﬂexibility
necessary for effective learning.

Chapter 11
Argumentation Frameworks as Neural
Networks
Formal models of argumentation have been studied in several areas, notably in logic,
philosophy, decision making, artiﬁcial intelligence, and law [25,31,39,48,83,111,
153, 210, 212, 267]. In artiﬁcial intelligence, models of argumentation have been
one of the approaches used in the representation of commonsense, nonmonotonic
reasoning. They have been particularly successful in modelling chains of defeasible
arguments so as to reach a conclusion [194, 209]. Although symbolic logic-based
models have been the standard for the representation of argumentative reasoning
[31, 108], such models are intrinsically related to artiﬁcial neural networks, as we
shall show in this chapter.
We shall establish a relationship between neural networks and argumentation
networks, combining reasoning and learning in the same argumentation framework.
We do so by presenting a neural argumentation algorithm, similar to the CILP al-
gorithm extended with metalevel priorities (Sect. 4.5), but capable of translating
argumentation networks into standard neural networks. We show a correspondence
between the two networks. The proposed algorithm works not only for acyclic ar-
gumentation networks, but also for circular networks. Then, as an application, we
show how the proposed approach enables the accrual of arguments through learn-
ing and the parallel computation of arguments. Finally, we discuss how different
argumentation models can be combined through the ﬁbring of networks discussed
in Chap. 9.
Although we show that the neural network computes the prevailing arguments
in the argumentation network (and thus that the argumentation algorithm is sound),
arguments will frequently attack one another in such a way that cycles are formed
in the network. In such cases, a notion of the relative strength of arguments may
be required to decide which arguments should prevail. Nevertheless, in some cases,
circularities may lead to an inﬁnite loop in the computation. To tackle this problem,
we propose the use of a learning mechanism. Learning can be used to resolve circu-
larities by an iterative change of the strengths of arguments as new information be-
comes available.1 Learning and its relation to cumulative argumentation [210,265]
in neural networks will also be discussed.
1 This is closely related to the situation where CILP loops (and the situation where Prolog loops)
owing to cycles through negation. Hence, the solution to this problem that we propose in this
chapter would also apply to CILP, relaxing the requirement for well-behaved programs.
A.S. d’Avila Garcez et al., Neural-Symbolic Cognitive Reasoning, Cognitive Technologies,
143
c⃝Springer-Verlag Berlin Heidelberg 2009

144
11 Argumentation Frameworks as Neural Networks
11.1 Value-Based Argumentation Frameworks
We start by describing the notion of a value-based argumentation framework, fol-
lowing Bench-Capon’s work [22]. In order to record the values associated with ar-
guments, Bench-Capon extended Dung’s argumentation framework [83] by adding
to it a set of values and a function mapping arguments to values. A notion of relative
strength of arguments may then be deﬁned by use of an audience, which essentially
creates an ordering on the set of values. As a result, if an argument A attacks an
argument B and the value of B is preferred over the value of A, then it may be the
case that A is accepted, but A is not able to defeat B. Therefore, a distinction be-
tween attacks and defeats – which are typically deﬁned as successful attacks – is
used [48,209].
Deﬁnition 50. [83] An argumentation network has the form A = <α,attack>,
where α is a set of arguments, and ‘attack’ ⊆α2 is a relation indicating which
arguments attack which other arguments.
Deﬁnition 51. [22] A value-based argumentation framework is a 5-tuple VAF =
<α,attacks,V,val, P>, where α is a ﬁnite set of arguments, ‘attacks’ is an irreﬂex-
ive binary relation on α, V is a nonempty set of values, val is a function mapping
elements in α to elements in V, and P is a set of possible audiences, where we may
have as many audiences as there are orderings on V. For every A ∈α, val(A) ∈V.
Bench-Capon also uses the notion of colouring in order to deﬁne the prefer-
ences of audiences. For example, consider an audience red (which prefers red over
blue) and an audience blue (which prefers blue over red), and the following value-
based argumentation: A (coloured red) attacks B (coloured blue), which attacks
C (coloured blue). As a result, {A,C} are the prevailing arguments for the audi-
ence red; however, for the audience blue, the prevailing arguments are {A,B}. It
is also important to note that Bench-Capon deﬁnes the notions of objective and
subjective acceptability of arguments. The ﬁrst relates to arguments that are accept-
able no matter what the choice of preferred values for every audience is, whereas
the second relates to arguments that are acceptable to only some audiences. Argu-
ments which are neither objectively nor subjectively acceptable are called indefen-
sible. Following Bench-Capon and the extension to argumentation networks given
in [18], we model the strength of arguments by deﬁning a function v from attack
to {0,1}, which gives the relative strength of an argument. Given αi,α j ∈α, if
v(αi,α j) = 1, then αi is said to be stronger than α j. Otherwise, αi is said to be
weaker than α j.
Let us consider brieﬂy the relationship between argumentation frameworks and
neural networks. As usual, we can think of a neural network as a graph. If we rep-
resent an argument as a neuron, then a connection from neuron i to neuron j can be
used to indicate that argument i either attacks or supports argument j. The weight of
the connection can be seen as corresponding to the strength of the attack or support.
Any real number can be assigned to the weight of a connection in a neural network,

11.2 Argumentation Neural Networks
145
and thus we shall associate negative weights with attacks, and positive weights with
supporting arguments,2 as detailed later on.
In order to compute the prevailing arguments in a neural network, one needs to
take into consideration the relative strength of the attacks as given, for example, by
an audience. Since the strengths of the different arguments are represented by the
weights of the network, and since learning is the process of progressively changing
the weights, it seems natural to use neural learning algorithms to change the network
as new information becomes available. We shall investigate this in more detail in
Sect. 11.3.
In the case of argumentation networks, it is sufﬁcient to consider deﬁnite logic
programs (i.e. programs without ∼). In this case, the neural network will contain
only positive weights (W). We shall then extend such a positive network to represent
attacks by using negative weights from the network’s hidden layer to its output layer,
as explained in detail in what follows.
11.2 Argumentation Neural Networks
In what follows, we introduce an algorithm that translates value-based argumenta-
tion networks into neural networks. But ﬁrst, let us see how the translation works in
a typical example of argumentation, namely, the moral-debate example [22].
Hal, a diabetic, loses his insulin in an accident through no fault of his own.
Before collapsing into a coma, he rushes to the house of Carla, another diabetic.
She is not at home, but Hal breaks into her house and uses some of her insulin. Was
Hal justiﬁed? Does Carla have a right to compensation? The following are some of
the arguments in this example:
A: Hal is justiﬁed, he was trying to save his life.
B: It is wrong to infringe the property rights of another.
C: Hal will compensate Carla.
D: Hal is endangering Carla’s life.
E: Carla has abundant insulin.
F: If Hal is too poor to compensate Carla, he should be allowed to take the insulin
as no one should die because they are poor.
Arguments and counter-arguments can be arranged in an argumentation network,
as in Fig. 11.1, where an arrow from argument X to argument Y indicates that X
attacks Y. For example, the fact that it is wrong to infringe Carla’s property rights
(B) attacks Hal’s justiﬁcation (A).
2 Generally speaking, an argument i supports an argument j if the coordination of i and j reduces
the likelihood of j being defeated. There are several different ways in which an argument may
support another. For example, argument i may support argument j by attacking an argument k that
attacks j, or argument i may support j directly, for example by strengthening the value of j [266].
In this chapter, we use the terms attack and support in a loose way, since it will be sufﬁcient to
deﬁne precisely just the notion of defeat.

146
11 Argumentation Frameworks as Neural Networks
C
B
A
E
D
F
Fig. 11.1 The argumentation network for the moral debate
A
B
D
A
B
D
h1
h2
h3
Fig. 11.2 A neural network for arguments A, B, D
In the argumentation network of Fig. 11.1, some aspects may change as the de-
bate progresses and actions are taken, with the strength of an argument in attacking
another changing over time. We see this as a learning process that can be imple-
mented using a neural network in which the weights encode the strength of the argu-
ments. The neural network for the set of arguments {A,B,D} is depicted in Fig. 11.2.
The network is single-hidden-layer, with inputs (A,B,D), outputs (A,B,D), and a
hidden layer (h1, h2, h3). Solid arrows represent positive weights and dotted arrows
represent negative weights. Arguments are supported by positive weights and at-
tacked by negative weights. Argument A (input neuron A), for example, supports
itself (output neuron A) with the use of hidden neuron h1. Similarly, argument B
supports itself (via h2), and so does argument D (via h3). From the argumentation
network, B attacks A, and D attacks A; these attacks are implemented in the neural
network by negative weights (see the dotted lines in Fig. 11.2) with the use of h2
and h3, respectively.
The network of Fig. 11.2 is a standard feedforward neural network that can be
trained with the use of a standard learning algorithm. Learning should change the
initial weights of the network (or the initial beliefs in the strength of arguments
and counter-arguments), according to examples (input and output patterns) of the
relationship between the arguments A, B, and D. This will become clearer in Sect.
11.3, where we shall give examples of learning in relation to argumentation.
In Fig. 11.2, generally speaking, if the absolute value of the weight from neuron
h1 to output neuron A (i.e. the strength of A) is greater than the sum of the absolute
values of the weights from neurons h2 and h3 to A (i.e. the strength of the attacks
on A), then one should be able to say that argument A prevails (in which case output
neuron A should be active in the neural network). Let us implement this form of
reasoning using CILP neural networks.

11.2 Argumentation Neural Networks
147
The Neural Argumentation Algorithm introduced below takes a value-based ar-
gumentation framework as input and produces a CILP neural network as output.
This network uses a semilinear activation function h(x) = 2/(1+e−x)−1 and inputs
in {−1,1}, where 1 represents true and −1 represents false. As usual, 0 < Amin < 1
indicates the minimum activation for a neuron to be considered active. The algo-
rithm then deﬁnes the set of weights of the neural network as a function of Amin
such that the neural network computes the prevailing arguments according to the
argumentation framework. The values of the weights derive from the proof of Theo-
rem 26, which shows that the neural network indeed executes a sound computation
of the argumentation framework.
Neural Argumentation Algorithm
1. Given a value-based argumentation framework A with arguments α1,α2,...,αn,
let:
P = {r1 : α1 →α1,r2 : α2 →α2,...,rn : αn →αn}.
2. Number each atom of P from 1 to n and create the input and output layers of a
neural network N such that the i-th neuron represents the i-th atom of P.
3. Given 0 < Amin < 1, calculate W ≥(1/Amin)·(ln(1+Amin)−ln(1−Amin)).
4. For each rule rl of P (1 ≤l ≤n), do:
(a) Add a neuron Nl to the hidden layer of N.
(b) Connect neuron αl in the input layer of N to hidden neuron Nl and set the
connection weight to W.
(c) Connect hidden neuron Nl to neuron αl in the output layer of N and set the
connection weight to W.
5. For each (αi,α j) ∈attack,3 do:
(a) Connect hidden neuron Ni to output neuron α j.
(b) If v(αi,α j) = 0, then set the connection weight to
W ′ > h−1(Amin)−WAmin.
(c) If v(αi,α j) = 1, then set the connection weight to
W ′ < (h−1(−Amin)−W)/Amin.
6. Set the threshold of each neuron in N to zero.
7. Set g(x) = x as the activation function of the neurons in the input layer of N.
8. Set h(x) = 2/(1+e−x)−1 as the activation function of the neurons in the hidden
and output layers of N.
Note that, differently from the general CILP translation algorithm, in which rules
may have any number of literals in the antecedent [66], here there is always a single
3 Recall that if v(αi,α j) = 1, then αi should defeat α j, and if v(αi,α j) = 0, then αi should not
defeat α j. The notion of defeat will be deﬁned precisely in later sections.

148
11 Argumentation Frameworks as Neural Networks
literal αi in the antecedent of each rule ri. This allows us to use a threshold of zero
in the algorithm above. Note also that W > 0 and W ′ < 0. This ﬁts well with the
idea of arguments having strengths (W), and of attacks also having strengths (W ′).
In practice, the values of W and W ′ could be deﬁned, for example, by an audience
using some form of voting system [22].
The notion of an argument that supports another seems natural in argumentation
neural networks. If argument αi supports argument α j, this may be implemented
easily in the neural network by the addition of a rule of the form αi →α j to the
program P.4 We need to make sure that the neural network computes the prevailing
arguments of the argumentation framework. For example, if argument αi attacks an
argument α j, and αi is stronger than α j, then neuron αi should be able to deacti-
vate neuron α j. Conversely, if αi is weaker than α j, and no other argument attacks
α j, then neuron αi should not be allowed to deactivate neuron α j. The following
deﬁnition captures this.
Deﬁnition 52 (N computes A). Let (αi,α j) ∈attacks. Let Aα(t) denote the ac-
tivation state of neuron α at time t. We say that a neural network N computes
an argumentation framework A if (i) whenever v(αi,α j) = 1, if Aαi(t) > Amin
and Aα j(t) > Amin, then Aα j(t + 1) < −Amin,5 and (ii) whenever v(αi,α j) = 0,
if Aαi(t) > Amin and Aα j(t) > Amin and for every αk (k̸=i,j) such that (αk,α j) ∈
attacks, Aαk(t) < −Amin, then Aα j(t +1) > Amin.
We now show that the translation from argumentation frameworks to neural net-
works is correct.
Theorem 26 (Correctness of Argumentation Algorithm). For each argumenta-
tion network A, there exists a feedforward neural network N with exactly one hid-
den layer and semilinear neurons, such that N computes A.
Proof. First, we need to show that the neural network computes P. When ri : αi →
αi ∈P, we need to show that (a) if Aαi > Amin in the input layer, then Aαi > Amin in
the output layer. We also need to show that (b) if Aαi < −Amin in the input layer, then
Aαi < −Amin in the output layer. (a) In the worst case, the input potential of hidden
neuron Ni is W · Amin, and the output of Ni is h(W · Amin). We want h(W · Amin) >
Amin. Then, again in the worst case, the input potential of output neuron αi will
be W · Amin, and we want h(W · Amin) > Amin. As a result, W > h−1(Amin)/Amin
needs to be veriﬁed, which gives W > (1/Amin) · (ln(1+Amin) −ln(1−Amin)), as
in the algorithm. The proof of (b) is analogous to the proof of (a). Now, we need to
show that the addition of negative weights to the neural network implements attacks
in the argumentation framework. When v(αi,α j) = 1, we want to ensure that the
4 In this way, an accumulation of arguments (α1,...,αn), where none of them is individually
stronger than the argument they attack (αn+1), might produce an input potential n ·W ′ that over-
comes the strength W of αn+1. This is naturally the way that neural networks work, and it relates
to the accrual of arguments. We shall discuss this in more detail in the next section.
5 Recall that we use −Amin for mathematical convenience, so that neuron α is said to be not active
if Aα(t) < −Amin, and active if Aα(t) > Amin.

11.3 Argument Computation and Learning
149
activation of output neuron α j is smaller than −Amin whenever both of the hidden
neurons Ni and Nj are active. In the worst-case scenario, Ni has activation Amin
while Nj has activation 1. We have h(W + AminW ′) < −Amin. Thus, we need W ′ <
(h−1(−Amin) −W)/Amin; this is satisﬁed by the Neural Argumentation Algorithm.
Similarly, when v(αi,α j) = 0, we want to ensure that the activation of output neuron
α j is larger than Amin whenever both of the hidden neurons Ni and Nj are active. In
the worst-case scenario, Ni now has activation 1 while Nj has activation Amin. We
have h(AminW +W ′) > Amin. Thus, we need W ′ > h−1(Amin) −WAmin. Again, this
is satisﬁed by the Neural Argumentation Algorithm. This completes the proof.
■
11.3 Argument Computation and Learning
In this section, we consider the computation and learning of arguments in neural
networks. We start by giving an example. We then consider the case in which ar-
guments attack each other so as to form a cycle in the argumentation network. This
may result in an inﬁnite computation in the neural network. To tackle this problem,
we propose the use of learning as a way of breaking the circularity. Learning can be
seen as a way of implementing the accrual of arguments. We conclude the section by
discussing this issue. There is interesting further research on each of these topics. In
this section, our purpose is to introduce a range of issues for argumentation neural
networks, but we are far from exhausting the subject.
Once we have translated argumentation networks into neural networks, our next
step is to run the neural network to ﬁnd out which arguments prevail. As usual, to
run the network, we connect output neurons to their corresponding input neurons
using weights Wr = 1 so that, for example, the activation of output neuron A is fed
into input neuron A at the next round in time. This implements chains such as A
attacks B, B attacks C, C attacks D, and so on, by propagating activation around
the network. The following example illustrates this computation in the case of the
moral-debate example described above (see Fig. 11.3).
Example 20. (Neural Network for Moral Debate) We apply the Neural Argumen-
tation Algorithm to the argumentation network of Fig. 11.1, and obtain the neural
network of Fig. 11.3. From the algorithm, we know that we should have Amin > 0
and W > 0. Let us take Amin = 0.5 and W = 5 (recall that W is the weight of the solid
arrows in the network). Following [108], let us consider the problem by grouping
arguments according to the aspects of life, property, and facts; arguments A, D, and
F are related to the right to life, arguments B and C are related to property rights,
and argument E is a fact. We may argue whether property is stronger than life,
but facts are always the strongest. If property is stronger than life then v(B,A) = 1,
v(D,A) = 1, v(C,B) = 1, v(C,D) = 1, v(E,D) = 1, and v(F,C) = 0. From the Neural
Argumentation Algorithm, when v(αi,α j) = 0 we must have W ′ > −1.4, and when
v(αi,α j) = 1 we must have W ′ < −12.2. The actual value of each attack may de-
pend on an audience. Nevertheless, provided that the above conditions on W ′ are
satisﬁed, the network will compute the expected prevailing arguments according to

150
11 Argumentation Frameworks as Neural Networks
A
B
C
D
h1
h2
A
B
h3
C
h4
D
E
h5
E
F
h6
F
Fig. 11.3 The moral-debate example as a neural network
Theorem 26, as follows: F does not defeat C, C defeats B, E defeats D, and, as a
result, we obtain {A,C,E} as the acceptable set of arguments. Now, if life is consid-
ered stronger than property, then v(F,C) = 1. As a result, F defeats C and, since C is
defeated, it cannot defeat B, which in turn cannot defeat A (because life is stronger
than property). Thus, the network converges to the state {A,B,E,F} of acceptable
arguments.6 This shows that two different lines of value-based argumentation will
provide the same answer to the question of whether Hal was justiﬁed (A), but two
different answers to the question of whether Carla has the right to compensation (C).
11.3.1 Circular Argumentation
Arguments may frequently attack one another in such a way that cycles are formed.
In such cases, the relative strength of the arguments will decide which of them
should prevail, if any. In [18], as part of a study of the dynamics of argumenta-
tion networks [39], Barringer, Gabbay, and Woods discussed how to handle loops
during the computation of arguments. They differentiated between syntactic and se-
mantic loops, in that the former occur as cycles in the argumentation network (e.g.
when argument A attacks argument B and vice versa), whereas the latter also depend
on the strength of the arguments involved in the loop.
In this way, if A is considerably stronger than B or vice versa, no semantic loop
will exist, despite the fact that there is a (syntactic) loop in the network; the rela-
tive strength of the arguments resolves the loop. Even if A and B both have similar
strengths, one possible interpretation is that neither argument should prevail. This
would also resolve the loop and, as we shall see in what follows, the dynamics
6 The complete set of argument values in this case is v(B,A) = 0, v(D,A) = 1, v(C,B) = 1,
v(C,D) = 0, v(E,D) = 1, and v(F,C) = 1. The constraints on W ′ are calculated in the same way
as before.

11.3 Argument Computation and Learning
151
of argumentation neural networks follows this interpretation. Still, there are situ-
ations in which the network oscillates between stable states, which indicates the
existence of alternative, conﬂicting sets of arguments. This problem may be re-
solved by changing the strength of certain arguments. Such a change may either
be due to the fact that new information has become available, or may come from
an investigation of the oscillating behaviour of the argumentation network itself, as
exempliﬁed below.
Example 21. (Argument Computation) Take the case in which an argument A at-
tacks an argument B, and B attacks an argument C, which in turn attacks A in a cycle,
as shown in Fig. 11.4. In order to implement this in a neural network, we need three
hidden neurons (h1, h2, h3), and positive weights to explicitly represent the fact that
A supports itself (via h1), B supports itself (via h2), and so does C (via h3). In ad-
dition, we need negative weights from h1 to B, from h2 to C, and from h3 to A, to
implement attacks (see Fig. 11.5). If the value of argument A (i.e. the weight from
h1 to A) is stronger than the value of argument C (the weight from h3 to C, which
is expected to be the same in absolute terms as the weight from h3 to A), C cannot
attack and defeat A. As a result, A is active and succeeds in attacking B (since we
assume that the weights from h1 and h2 to B have the same absolute value). Since B
is not active, C will be active, and a stable state {A,C} will be reached. In Bench-
Capon’s model [22], this is precisely the case in which the colour blue is assigned
to A and B, and the colour red is assigned to C, with blue being stronger than red.
Note that the order in which we reason does not affect the ﬁnal result (the stable
state reached). For example, if we had started with B successfully attacking C,C
would not have been able to defeat A, but then A would successfully defeat B, which
would, this time round, not be able to successfully defeat C, which in turn would be
active in the ﬁnal stable state {A,C}.
A
B
C
Fig. 11.4 Circular arguments
A
B
C
h1
h2
h3
A
B
C
Fig. 11.5 A neural network for circular argumentation

152
11 Argumentation Frameworks as Neural Networks
In Example 21, a syntactic loop exists in that the attacks in the argumentation
network form a loop in Fig. 11.4. However, there is no semantic loop, as the com-
putation of the arguments converges to a stable state, as exempliﬁed above. Even
if the strengths of the arguments were all the same, the neural network of Fig.
11.5 would converge to an empty set, as follows. Assume that solid arrows have
weight W, and dotted arrows have weight −W in the network of Fig. 11.5. Let
(A,B,C) = [1,1,1] denote the network’s input vector. Thus, h(W) will be the activa-
tion state of each hidden neuron, where h is the activation function of such neurons.
Then, W ·h(W)−W ·h(W) = 0 will be the input potential of each output neuron, and
thus h(0) = 0 (recall that θ = 0) will be the activation state of each output neuron
A, B, and C. Now, given an input vector (A,B,C) = [0,0,0], the activation state of
each hidden neuron will be zero, and then the activation state of each output neuron
will be zero. As a result, the network converges to a stable state { } in which no
argument prevails. This stable state is reached after a single computation step from
[1,1,1] to [0,0,0].
According to Deﬁnition 52, an argument prevails if its associated neuron has an
activation in the interval (Amin,1] with Amin > 0. Dually, whenever an argument is
defeated, its associated neuron should have an activation in the interval [−1,−Amin).
In the case of circular-argumentation networks, however, there is a third possibility,
when arguments cancel each other and the neurons’ activations lie in the interval
[−Amin,Amin], typically converging to zero, as illustrated above. In this case, we
know that arguments do not prevail, and it might be useful in some situations to
make a distinction between a clear defeat and a failure to prevail. We shall return
to this issue when we consider argument learning. First, we need to study the more
involved situation where the network oscillates between states.
Unfortunately, not all argumentation neural networks are as well behaved as the
ones considered so far. Take the case in which an argument A attacks two arguments
B and C, B and C in turn both attack an argument D, and D attacks A in a cycle, as
depicted in Fig. 11.6. Assume that all the attacks have the same strength. Figure 11.7
shows the neural network for the argumentation network of Fig. 11.6. Assume, as
before, that the solid arrows have weight W, and the dotted arrows have weight −W.
Given an input (A,B,C,D) = [1,1,1,1], it is clear from Fig. 11.7 that the values of
output neurons A, B, and C will be zero, as the weights W and −W cancel each
other out. The input potential of output neuron D, however, will be −W ·h(W), and
so the value of output neuron D will be OD = h(−W ·h(W)). The next time round,
the input potential of output neuron A will be −W · h(OD ·W), and A will have a
A
B
C
D
Fig. 11.6 Semantic circularity

11.3 Argument Computation and Learning
153
A
C
A
C
h1
h3
B
B
h2
D
D
h4
Fig. 11.7 A neural network encoding semantic circularity
positive activation h(−W · h(OD ·W)). As a result, if A successfully defeats B and
C, then D prevails, and thus defeats A. In this case, B and C prevail, and defeat D, in
a cycle.
Let us look at this cycle in more detail in the neural network. Let us assume, for
convenience, that W = 1, h(x) = 1 if x ≥1, h(x) = −1 if x ≤−1, and h(x) = x for
−1 < x < 1.7 We start with the input vector [1,1,1,1], and obtain an output vector
[0,0,0,−1]. We then use [0,0,0,−1] as input to obtain an output [1,0,0,−1]. Let
us use →to denote the above mapping from input to output vectors, so that we
have [1,1,1,1] →[0,0,0,−1] →[1,0,0,−1] →[1,−1,−1,−1] →[1,−1,−1,1] →
[0,−1,−1,1] →[−1,−1,−1,1] →[−1,0,0,1] →[−1,1,1,1] →[−1,1,1,−1] →
[0,1,1,−1] →[1,1,1,−1] →[1,0,0,−1] →. . . →[1,0,0,−1] →..., which shows
that we have reached an inﬁnite loop.
Can we learn anything from the sequence of arguments computed by the net-
work? Initially, there is a situation in which A seems to prevail. Then, A and D
together prevail. Then, D alone. Then B, C, and D together, then B and C only, and
ﬁnally A, B, and C, before we go back to the situation in which A alone seems to
prevail. One way to deal with this problem would be to simply assume that the loop
itself indicates that no argument should prevail at the end. One may argue, however,
that this does not really solve the problem. Alternatively, one could try to use the
information obtained during the computation of the arguments to resolve the loop.
In the example above, for instance, it seems that either {A,D} or {B,C} could serve
as a basis for a stable set of arguments. More information would be needed, but
one might start by searching for information in support of {A,D} and in support of
{B,C}, and only then in support of the other combinations. It seems that the only
real solution to the problem of semantic loops is to have new information in the
form of new evidence about the relative strength of the arguments and to learn from
it, as we discuss in the following section.
7 This gives an approximation of the standard sigmoid activation function.

154
11 Argumentation Frameworks as Neural Networks
11.3.2 Argument Learning
Consider again the neural network of Fig. 11.7. Suppose that new evidence becomes
available in favour of arguments A and C so that we would like both arguments to
prevail. We do not know how this should affect arguments B and D, but we know
now that, given the input vector [1,1,1,1], we would like the network of Fig. 11.7
to produce an output [1,?,1,?], instead of [0,0,0,−1]. Since we do not have any
information about B or D, the natural candidates for ? are the original values (so
that [1,?,1,?] becomes [1,0,1,−1]). This will produce an error of zero for output
neurons B and D during learning, which is the best way of reﬂecting the lack of new
information about such concepts. An error of zero will produce no changes in the
weights directly associated with B and D, but of course the changes in other weights
may affect the overall result of the network. Let us exemplify this in the case of the
network of Fig. 11.7 for our training example [1,1,1,1] →[1,0,1,−1].
We used the standard backpropagation learning algorithm8 [224]. As before, the
use of backpropagation was made possible because the network was recurrent only
for the computation of the arguments, not during learning. The recurrent connections
were important for the reasoning process, and had weights always ﬁxed at 1. During
learning, we were interested in establishing a new mapping from the input to the
output, and thus a learning algorithm that applied to feedforward networks such as
backpropagation sufﬁced. We used W = 4.0 (solid arrows in Fig. 11.7), and W ′ =
−4.0 (dotted arrows).9 Recall that θ = 0 and that any other connection not shown
in Fig. 11.7 is given weight zero initially.
The thresholds of output neurons A (θ A) and C (θC) were changed through learn-
ing to −0.8, the weights from h1 to A (WA,h1) and from h3 to C (WC,h3) were changed
to 4.8, and the weights from h4 to A (WA,h4) and from h1 to C (WC,h1) were changed
to −3.2. In addition, some very minor changes occurred in the weights linking the
input to the hidden layer of the network and, as expected, no changes occurred in
the weights leading to output neurons B or D, namely WB,hi and WD,hi, 1 ≤i ≤4
(recall that, for the purpose of learning, the network is fully-connected).
The computation of the arguments in the trained network goes as follows. In
addition to the transition from [1,1,1,1] to [1,0,1,−1], learned as expected, the
network then maps [1,0,1,−1] into [1,−1,1,−1], which is a stable state. The newly
learned sequence [1,1,1,1] →[1,0,1,−1] →[1,−1,1,−1] →[1,−1,1,−1] is now
loop free; the stable state [1,−1,1,−1] corresponds to the acceptance of {A,C} as
prevailing arguments.
As another example, let us consider the Nixon diamond problem. In the tradi-
tional Nixon diamond problem, Nixon is a Quaker (Q) and a Republican (R). Quak-
ers are generally paciﬁsts (P), whereas Republicans are generally nonpaciﬁsts (¬P).
8 We used tanh as the activation function, a learning rate of 0.1, and a term of momentum of 0.4.
We trained the network on the single training example ([1,1,1,1], [1,0,1,−1]) until a mean square
error of 0.01 was reached.
9 This was done because tanh(4.0) = 0.999, whereas tanh(1.0) = 0.761. As a result, W = 4.0 gives
a good approximation to h(W) = 1, as in our previous assumption.

11.3 Argument Computation and Learning
155
This produces an inconsistency in a number of formalisations of the problem [7].
Brieﬂy, if the strength of the support for Nixon’s paciﬁsm is the same as the strength
of the support for his nonpaciﬁsm, the neural network will conclude that both P and
¬P prevail. If, in addition, we assume that P attacks ¬P and, vice versa, ¬P attacks
P, both with the same strength, then the stable state of the neural network will con-
tain neither P nor ¬P. Finally, if we are faced with a situation in which we need to
choose between P and ¬P, we can learn to enforce a stable state in the network in
which one but not the other argument prevails.
Suppose we need to make a decision about Nixon’s paciﬁsm. We need to seek
new information. We ﬁnd out that Nixon is a football fan (F), and that football fans
are normally nonpaciﬁsts. We then use this information to convince ourselves that
Nixon is indeed a nonpaciﬁst. We need to add an input, a hidden, and an output
neuron to our original network to represent F. Then, to make sure that F attacks
and defeats P, we simply train with the example [1,1,1,1,1] →[1,1,−1,1,1], given
arguments Q,R,P,¬P,F in that order.
There are also situations in which the number of times that an example occurs
should be relevant to the decision about the strength of an argument. In such cases,
alternative forms of learning would need to be investigated, since the backpropa-
gation algorithm does not cater for varying relevance. We believe that this is an
interesting research area, with many open research issues to be addressed in the
near future.
11.3.3 Cumulative (Accrual) Argumentation
We have mentioned that neural networks deal with cumulative argumentation in the
sense that a number of arguments, none of which are individually stronger than a
given argument, may defeat that argument collectively. There is some controversy
about whether arguments accrue. Whereas Pollock denied the existence of cumula-
tive argumentation [210], Verheij defended the assertion that arguments can be com-
bined either by subordination or by coordination, and may accrue in stages [265].
In this section, we give an example of accrual by coordination, which happens to be
a natural property of neural networks.
Consider the following scenario. Suppose you are the head of state of a coun-
try who needs to decide whether or not to go to war in order to remove a violent
dictator from power. First, you consider the prospect of the loss of lives of fellow
countrymen and women in action, and the killing of innocent civilians, which forms
a strong argument against going to war. Let us call this argument A1. In support
for going to war, according to documented evidence from your intelligence services,
the dictator possesses chemical and biological weapons, having made use of such
weapons in the past. We call this argument A2. You also happen to possess what
you believe is credible information about the fact that the dictator has recently ac-
quired uranium from another country, most probably in order to continue with an

156
11 Argumentation Frameworks as Neural Networks
unauthorised nuclear-weapons-capability programme. Let this be called argument
A3. In addition, recent information indicates that the dictator has the capability and
the will – having done so in the past – to attack neighbouring countries. Let us name
this argument A4. Finally, you receive evidence that the dictator has provided safe
haven for well-known members of an active international terrorist network. This is
argument A5. The task at hand is to decide whether or not it is right to remove the
dictator.
Let B denote the proposition it is right to remove the dictator, and consider the
situation in which A1 attacks B while A2,...,A5 support B. Assume, further, that A1
is stronger than B, i.e. v(A1,B) = 1. We apply the Neural Argumentation Algorithm
and obtain the neural network of Fig. 11.8. Taking Amin = 0.5, we calculate W > 2.2.
Taking, for example, W = 3, we calculate W ′ < −4.5. Let us take W ′ = −5.
According to the algorithm, W = 3 and W ′ = −5 form an acceptable set of
weights. Although A1 is supposed to defeat B if contrasted with any of A2,...,A5,
the cumulative support of A2,...,A5 for B actually allows it to prevail. This can be
seen in the network by inspecting the input potential of neuron B, which is approxi-
mately (4·W)+W ′ = 7, i.e. a relatively large positive value, which activates neuron
B. Of course, a different outcome could be obtained by ensuring that W ′ was large
enough (in absolute terms) to counteract the inﬂuence of all the other arguments to-
gether. A value of W ′ = −16, for example, would produce an input potential of −4,
which would be sufﬁcient to deactivate neuron B. Of course, these values should de-
pend on your degree of belief in the arguments for and against the war. These values
could have been learned from examples (e.g. previous cases) so that one could make
an informed decision.
h1
h2
h3
β
h4
h5
a1
a2
a3
a4
a5
Fig. 11.8 Cumulative-support argumentation

11.4 Fibring Applied to Argumentation
157
11.4 Fibring Applied to Argumentation
An interesting application of ﬁbring (see Chap. 9) is in the area of legal reason-
ing and argumentation. In [108], for example, Gabbay and Woods argued for the
combined use of labelled deductive systems (LDS) and Bayesian networks [202] to
support reasoning about legal evidence under uncertainty. In addition, they argued
that neural networks, as learning systems, could play a role in this process by being
used to update/revise degrees of belief and the rules of the system whenever new ev-
idence was presented. The three different representations – logical (LDS), Bayesian,
and connectionist – all expand the value-based argumentation framework of [22].
Returning to the moral-debate example of Sect. 11.2, notice that some aspects of
the argumentation network are probabilistic. For example, the question of whether
Carla has abundant insulin (E) depends on the time and is a matter of probability.
The question of whether Hal will be able to compensate Carla with replacement
insulin (C) is also a matter of probability. If Carla has abundant insulin, the chances
that Hal will be able to compensate her are higher. The probability matrices of this
Bayesian network (E →C) inﬂuence whether Hal is endangering Carla’s life by
stealing some of her insulin (D). In the same argumentation network, some other
aspects may change as the debate progresses and actions are taken; the strength of
one argument in attacking another may change in time. This is a learning process
that can be implemented using, as we have seen, the neural network of Fig. 11.2,
in which the weights record the strengths of the arguments. The neural network
is an autoassociative, single-hidden-layer feedforward network with an input layer
(A, B, D), an output layer (A, B, D), and a hidden layer (h1, h2, h3). The hidden
neurons are used in the network to provide a greater ﬂexibility as to what can be
learned as combinations of the input neurons. Training changes the initial weights
(the initial belief in the strength of arguments and counter-arguments, which could
be random), according to examples of the relationship between the arguments A,
B, and D. Roughly speaking, if the absolute value of the weight from neuron h1 to
output neuron A is greater than the sum of the absolute values of the weights from
neurons h2 and h3 to A, then one can say that argument A prevails.
Now that we have two concrete models of the arguments involved in the moral-
debate example – a probabilistic model and a learning/action model – we can reason
about the problem at hand in a more realistic way. We just need to put the two models
together consistently. We can use the ﬁbring methodology for networks described
in Chap. 9. The (more abstract) argumentation network of Fig. 11.1 can be used to
tell us how the networks (Bayesian and neural) are to be ﬁbred. From Fig. 11.1,
one can see that both arguments C and E attack argument D directly. As a result,
we would like the probabilities in our Bayesian network E →C to inﬂuence the
activation of neuron D in the neural network. Thus, the network E →C needs to
be embedded into node D. Again from the argumentation network (Fig. 11.1), one
can see that argument C also attacks argument B directly. As a result, we would
like the probabilities associated with C to inﬂuence the activation of neuron B. As
before, this can be done by embedding the Bayesian network C into neuron B. This
produces the recursive network of Fig. 11.9.

158
11 Argumentation Frameworks as Neural Networks
A
B
h1
h2
h3
A
B
D
D
C
E
C
ϕ1
ϕ2
Fig. 11.9 Fibring of Bayesian and neural networks applied to argumentation
Let us consider again the embedding of E→C into D. We have seen that the em-
bedding is guided by the arrow in the original argumentation network. An arrow in
an argumentation network indicates an attack. As a result, the higher the probabil-
ity P(C/E) is (see Fig. 11.9), the lower the activation value of neuron D should be.
Similarly, the higher the probability P(C), the lower the value of B should be. Thus,
we take ϕ1 : sB
new = sB
old−P(C) and ϕ2 : sD
new = sD
old−P(C/E), where P(X) ∈[0,1]
and s ∈(0,1). In the combined system, the new state of output neuron D (sD
new) will
be fed into input neuron D and affect the new state of A (through hidden neuron h3
and the negative weight from h3 to A), such that the higher the value of D, the lower
the value of A. The same will happen through B according to the dynamics of the
embedding and embedded networks.
In this argumentation case study, we have seen that the general methodology of
ﬁbring can be applied to neural networks and to other systems such as Bayesian net-
works. In symbolic systems, the ﬁbring of two logics that do not allow, for example,
for embedded implication may result in a logic with embedded implication. In the
same way, the ﬁbring of (Bayesian and neural) networks may result in embedded
networks such as (A →B) →C, which are strictly more expressive than standard
networks, i.e. do not have a ﬂattened counterpart network.10 This is an indication,
10 The idea of ﬁbring Bayesian networks emerged from the observation that causal relations may
themselves take part in causal relations. This was simply and effectively exempliﬁed in the very
ﬁrst section of Williamson and Gabbay’s ‘Recursive causality in Bayesian networks and self-
ﬁbring networks’ [270]. The example given there states that the fact that smoking causes cancer, for
instance, causes the government to restrict tobacco advertising. Such a recursive deﬁnition of causal

11.5 Discussion
159
now from the point of view of neural-symbolic integration, that ﬁbring may be used
to produce simple neural-network architectures (an important requirement for effec-
tive learning) that can represent powerful logics.
11.5 Discussion
In this chapter, we have presented a model of computation that allows the deduction
and learning of argumentative reasoning. The model combines value-based argu-
mentation frameworks and neural-symbolic learning systems by providing a trans-
lation from argumentation networks to neural networks. A theorem then shows that
such a translation is correct. We have shown that the model works not only for
acyclic argumentation networks, but also for circular networks, and it enables cu-
mulative argumentation through learning.
A neural implementation of argumentative reasoning may be advantageous from
a purely computational point of view owing to the parallelism of neural networks.
A relevant long-term goal is to facilitate learning capabilities in value-based argu-
mentation frameworks, as arguments may evolve over time, with certain arguments
being strengthened and others weakened. At the same time, we seek to enable the
parallel computation of argumentation frameworks by making use of the parallelism
of neural networks.
As future work, one could perform further study on the ﬁbring of neural net-
works [67] in connection with the framework of Williamson and Gabbay [270].
This framework incorporates the idea of recursive causality, according to which
causal relations may take causal relations as input values. The model presented here
could also consider probabilistic weights in argumentation frameworks in the style
of [119]. This would allow a quantitative approach to argumentation in an integrated
model of argumentative reasoning under uncertainty and inductive learning.
Neural-symbolic learning systems may serve as an underlying framework for the
study of the ﬁbring of networks and logics, and the self-ﬁbring of networks. In this
setting, an appropriate ﬁbring of two networks A and B, for example, would be one
in which the logic extracted from the ﬁbred network is the same as the logic obtained
from ﬁbring the logics extracted from networks A and B. There are many possible
avenues of research on the ﬁbring of networks (neural, Bayesian, etc.), the study of
their logical interpretation, and their application in argumentation.
relations was then used to deﬁne what Williamson and Gabbay call a recursive Bayesian network,
a Bayesian network in which certain nodes may be Bayesian networks in their own right. This
was deﬁned with the help of the concept of network variables, which are variables that may take
Bayesian networks as values. Thus, a network variable SC may be used to represent the fact that
smoking causes cancer, and then SC causes A, where A stands for restricting advertising. This may
be written as SC →A, where SC = S →C, or simply as (S →C) →A.

Chapter 12
Reasoning about Probabilities
in Neural Networks
In this chapter, we show that artiﬁcial neural networks can reason about probabil-
ities, thus being able to integrate reasoning about uncertainty with modal, tempo-
ral, and epistemic logics, which have found a large number of applications, no-
tably in game theory and in models of knowledge and interaction in multiagent sys-
tems [87,103,207]; artiﬁcial intelligence and computer science have made extensive
use of decidable modal logics, including in the analysis and model checking of dis-
tributed and multiagent systems, in program veriﬁcation and speciﬁcation, and in
hardware model checking. Finally, the combination of knowledge, time, and proba-
bility in a connectionist system provides support for integrated knowledge represen-
tation and learning in a distributed environment, dealing with the various dimensions
of reasoning of an idealised agent [94,202].
12.1 Representing Uncertainty
To represent uncertainty, we add a probabilistic dimension to the framework of con-
nectionist modal logic (CML) presented in Chap. 5. We follow Halpern’s work in
order to do so [121], and associate probabilities with possible worlds. In this way,
the different probabilities that agents envisage are captured not by different proba-
bility distributions, but by the set of worlds that the agents consider possible. As a
result, we need a (temporal) translation algorithm (see Chap. 6) that caters for an
agent that envisages multiple possible worlds at the same time point. Let us ﬁrst
consider the two-coin problem as described in [121], as an example of how to repre-
sent uncertainty in neural-symbolic systems. In what follows, we say that pi(α) = x
if the probability of formula α, according to agent i, is equal to x, where x is a real
number in [0,1]. We then allow neurons to be labelled as α = x, indicating that x
is the probability of α when the neuron is activated. There are two agents involved
in the problem. Agent 1 holds two coins; one of them is fair (coin 1) and the other
is biased (coin 2). Agent 1 is able to identify which coin is fair and which coin is
A.S. d’Avila Garcez et al., Neural-Symbolic Cognitive Reasoning, Cognitive Technologies,
161
c⃝Springer-Verlag Berlin Heidelberg 2009

162
12 Reasoning about Probabilities in Neural Networks
biased. Agent 2, however, knows only that one coin is fair and the other is twice as
likely to land heads as tails, and cannot tell them apart. Thus, the probability pc1 of
coin 1 landing heads is pc1(heads) = 1/2, and the probability of coin 1 landing tails
is pc1(tail) = 1/2. The probability pc2 of coin 2 landing heads is pc2(heads) = 2/3,
and the probability of coin 2 landing tails is pc2(tails) = 1/3.
Agent 1 chooses one of the coins to toss. Agent 2 does not know which coin agent
1 has chosen, nor even the probability that the fair coin has been chosen. What is
the probability, according to agent 2, that the result of the coin toss will be heads?
What is the probability according to agent 1? The difference between agent 1 and
agent 2 is captured not by the probability distributions that they use, but by the set of
worlds that they consider possible. We argue, as a result, that the CML framework,
extended to reason about knowledge and time, is suitable for representing the kinds
of problems discussed in [121]. Let us discuss the background knowledge of the
agents involved in the problem. Below, we use f for fair, b for biased, h for heads,
and t for tails. Moreover, as one would expect, knowledge persists through time.
Suppose that at time t0, no coin has been chosen yet. At time t1, agent 1 has
chosen a coin to toss, and at time t2, agent 1 has tossed the coin. At time t1, agent 1
must know whether she has selected a biased coin (t1 : K1b) or a fair coin (t1 : K1f).
We say that agent 1 is in one of the states B or F depending on the coin that she
has selected (biased or fair, respectively). We represent this as a neural-symbolic
system by having ω1B as a possible world in which agent 1 has selected the biased
coin at time t1. Similarly, we have ω1F as a possible world in which agent 1 has
selected the fair coin at time t1. Obviously, the associated probabilities p(h) = 2/3
and p(t) = 1/3 must hold in ω1B, whereas p(h) = 1/2 and p(t) = 1/2 must hold
in ω1F. In contrast, agent 2 is uncertain about the situation. All agent 2 knows at t1
is that p(h) = 1/2 or p(h) = 2/3, and that p(t) = 1/2 or p(t) = 1/3. We represent
this by using two worlds: ω2B as a possible world in which agent 2 considers that
p(h) = 2/3 and p(t) = 1/3, and ω2F as a possible world in which agent 2 considers
that p(h) = 1/2 and p(t) = 1/2.
Notice how what is known by the agents is implemented in the various neural
networks: at t1, either p(h) = 2/3 or p(h) = 1/2, but not both, will be activated
for agent 1 (according to the inputs to the networks representing ω1B and ω1F); at
the same time, both of p(h) = 2/3 and p(h) = 1/2 will be activated for agent 2 in
worlds ω2B and ω2F, respectively, indicating the fact that agent 2 is uncertain about
the situation.
At time t2, agent 1 knows the result of the coin toss, and she assigns a probability
1 to it. Agent 2 also knows the result of the toss (we assume that he can see it),
but ‘never learns what happened’ [121]. As a result, at time t2, there are two possi-
ble worlds: ωH, in which p(h) = 1 and p(t) = 0, and ωT, in which p(h) = 0 and
p(t) = 1.
Figure 12.1 represents the reasoning process at t1 and t2, where ct indicates that
the selected coin has been tossed. As discussed above, in ωH, input neuron h is
activated, and if ct is also activated, neurons h = 1 and t = 0 will be activated (us-
ing the rules ct∧h →(p(h) = 1) and ct∧h →(p(t) = 0)). Similarly, in ωT, input

12.1 Representing Uncertainty
163
h = 1
t = 0
ωH
ct
h
h = 0
t = 1
ωT
ct
t
ω1B
b
h = 2/3
ct
t = 1/3
ω1F
h = 1/2
t = 1/2
f
ct
t1
t2
Fig. 12.1 A representation of the two-coin problem
neuron t is activated, and if ct is also activated, neurons h = 0 and t = 1 will be acti-
vated (using the rules ct∧t →(p(h) = 0) and ct∧t →(p(t) = 1)). In Fig. 12.1, the
connection between worlds can be established by the use of the tomorrow operator
⃝, as detailed in what follows.
The above example illustrates the need to consider a pair (ω,t), where ω is one
of the worlds envisaged by an agent at time t. This allows us to reason about situ-
ations such as the one described in the two-coin problem, where each agent i can
reason about different possible worlds ω at a time point t, represented by t : Kω
i α,
where α represents qualitative information about the probability of a literal. More
generally, in what follows, we consider knowledge and probability rules of the form
t : ⃝K[W]
[A] L1,...,⃝K[W]
[A] Lk →⃝K[W]
[A] Lk+1, where [W] denotes an element selected
from the set of possible worlds W, for each literal Lj, 1 ≤j ≤k + 1, 1 ≤t ≤n,
and k,n ∈N. In addition, L j can be either a literal or a probabilistic statement of the
form p(α) = x, as deﬁned above. Note also that ⃝is not required to precede every
single literal.
Normally, we shall have rules such as t : Kω
[A]L1,...,Kω
[A]Lk →⃝K[A](p(α) = x).
Although a single rule might refer to different worlds, normally (as in the examples
given in this chapter) a single ω is selected from [W] for each rule. Recall that
t : ⃝Kiα denotes that α holds in every accessible possible world from the point
of view of agent i at time t + 1. Finally, note that probabilistic statements are not
generally preceded by negation.

164
12 Reasoning about Probabilities in Neural Networks
Returning to the two-coin problem, the following would be the rules for the
agents involved in the situation where ωiB and ωiF denote the worlds in which
agent i considers that the coin is biased and fair, respectively, i ∈{1,2}:
t1 : KωiB
i
b →KωiB
i
(p(h) = 2/3);
t1 : KωiB
i
b →KωiB
i
(p(t) = 1/3);
t1 : KωiF
i
f →KωiF
i
(p(h) = 1/2);
t1 : KωiF
i
f →KωiF
i
(p(t) = 1/2);
t1 : Kω1B
1
ct;
t1 : Kω1F
1
ct.
In addition, the following rules model the situations once agent i knows that a
coin has been tossed:
t2 : KωH
i
ct∧KωH
i
h →(p(h) = 1);
t2 : KωH
i
ct∧KωH
i
h →(p(t) = 0);
t2 : KωT
i
ct∧KωT
i
h →(p(h) = 0);
t2 : KωT
i
ct∧KωT
i
h →(p(t) = 1).
Finally, to interconnect the networks, we have the following rule, expressing the
fact that when ct holds in ω1F, then ct holds in both ωH and ωT:
t1 : Kω1F
i
ct →⃝Ki ct, R(ω1F,ωH),R(ω1F,ωT).
12.2 An Algorithm for Reasoning about Uncertainty
We now present an algorithm to translate knowledge and probability rules into
neural-network ensembles. The equations used in this algorithm to calculate the
values of weights and thresholds were obtained from the CILP Translation Algo-
rithm (see Chap. 4). The main difference between the two algorithms is that here
probabilities can be associated with neurons and that probabilities can be combined
with time and knowledge operators, along the lines of [121]. The present algorithm
distinguishes between the case in which the tomorrow operator occurs in the head
of a clause (step 3) and the case in which it does not (step 4).
Notation
Let P contain q knowledge and probability rules. We number each Lj
from 1 to k + 1 such that, when a neural network N is created, input and output
neurons are created to represent each L j. We also number worlds ωι ∈W from 1 to
u, u ∈N. We use Amin ∈(0,1) to denote the minimum activation for a neuron to be
considered active (i.e. its associated literal is considered true), a bipolar semilinear
activation function h(x) = 2/(1+e−βx)−1, and inputs in {−1,1}. Let kl denote the
number of literals in the body of rule rl, μl the number of rules in P with the same
literal as the consequent for each rule rl, MAXrl(kl,μl) the greater of the elements
kl and μl for rule rl, and MAXP(k1,...,kq,μ1,...,μq) the greatest element among
all kl’s and μl’s for P. We also use −→k as a shorthand for (k1,...,kq), and −→
μ as a
shorthand for (μ1,...,μq).

12.2 An Algorithm for Reasoning about Uncertainty
165
Knowledge and Probability Translation Algorithm
1. For each world ωι ∈W, for each time point t, do: Create a CILP neural network
Nωι,t.
2. Calculate the weight W such that
W ≥2
β ·
ln(1+Amin)−ln(1−Amin)
MAXP(−→
k,−→
μ )·(Amin −1)+Amin +1
.
3. For each rule rl in P of the form
t : ⃝Kω1
[A]L1,...,⃝Kωu−1
[A] Lk →⃝Kωu
[A]Lk+1 :
(a) Add a hidden neuron L◦to Nωu,t+1 and set h(x) as the activation function
of L◦.
(b) Connect each neuron ⃝Kωι
[A]Li (1 ≤i ≤k), (1 ≤ι ≤u−1) in Nωι,t to L◦. If
Li is a positive literal, then set the connection weight to W; otherwise, set the
connection weight to −W. Set the threshold of L◦to ((1+Amin)·(kl −1)/2)·
W.
(c) Connect L◦to Kωu
[A]Lk+1 in Nωu,t+1 and set the connection weight to W. Set
the threshold of Kωu
[A]Lk+1 to ((1+Amin)·(1−μl)/2)·W.
(d) Add a hidden neuron L• to Nωι,t and set h(x) as the activation function of L•.
(e) Connect the neuron Kωu
[A]Lk+1 in Nωu,t+1 to L• and set the connection weight
to W. Set the threshold of L• to zero.
(f) Connect L• to ⃝Kωι
[A]Li in Nωι,t and set the connection weight to W. Set the
threshold of Kωι
[A]Li to ((1+Amin)·(1−μl)/2)·W.
4. For each rule in P of the form
t:⃝Kω1
[A]L1,...,⃝Kωu−1
[A] Lk →Kωu
[A] Lk+1 :
(a) Add a hidden neuron L◦to Nωu,t and set h(x) as the activation function of
L◦.
(b) Connect each neuron ⃝Kωι
[A]Li (1 ≤i ≤k), (1 ≤ι ≤u−1) in Nωι,t to L◦. If
Li is a positive literal, then set the connection weight to W; otherwise, set the
connection weight to −W. Set the threshold of L◦to ((1+Amin)·(kl −1)/2)·
W.
(c) Connect L◦to Kωu
[A]Lk+1 in Nωu,t and set the connection weight to W. Set the
threshold of Kωu
[A]Lk+1 to ((1+Amin)·(1−μl)/2)·W.
Theorem 27 (Correctness of Knowledge and Probability Translation Algo-
rithm). For each set of knowledge and probability rules P, there exists a neural-
network ensemble N such that N computes P.
Proof. This follows directly from the proof of the analogous theorem for sin-
gle CILP networks (Theorem 8, Chap. 4) and the proof that CML computes a

166
12 Reasoning about Probabilities in Neural Networks
ﬁxed-point semantics for modal programs (Theorem 12, Chap. 5) extended to cater
for pairs (ω,t), instead of ω only, as done in Chap. 6, Theorem 13. The only dif-
ference from Theorem 13 is that here we allow for the renaming of literals Li as
pi(α) = x.
■
12.3 The Monty Hall Puzzle
As an example case study, let us consider the Monty Hall puzzle. Suppose you are
in a game show and are given a choice of three doors. Behind one is a car, behind
the others are goats. You pick door 1. Before opening door 1, Monty Hall, the host
(who knows what is behind each door), opens door 2, which has a goat behind it.
He then asks if you still want to take what is behind door 1, or what is behind door
3 instead. Should you switch? [121].
At time t0, two goats and a car are placed behind the doors. At time t1, a door
is randomly selected by you, and another door, always having a goat behind it,
is opened by Monty Hall. At time t2, you have the choice of whether or not to
change your selected door, and depending on your choice, you will have different
probabilities of winning the car, as outlined below (for details, see [121]).
Your chances of picking the right door are 1/3, and your chances of picking
the wrong door are 2/3. When Monty Hall opens door 2, it becomes known that the
probability of the car being behind door 2 is zero (t2 : p(door2) = 0). The probability
of the car not being behind door 1 remains 2/3 and, as a result, the probability of
the car being behind door 3 is 2/3 (t2 : p(door3) = 2/3). Therefore, once you learn
that p(door2) = 0, you ought to change from door 1 to door 3. This is summarised
below:
tk : p(door1) = 1/3, k ∈{0,1,2};
tj : p(door2) = 1/3, j ∈{0,1};
tj : p(door3) = 1/3, j ∈{0,1};
t2 : K(goat2) →p(door2) = 0;
t2 : K(goat2) →p(door3) = 2/3.
We model the puzzle as follows. At t2, there are two possible worlds, one in
which your policy is to change (ωc), and another one in which your policy is to
stick to your original option (ωs). In ωc, your chances of getting the car are 2/3, and
in ωs your chances are 1/3. Figure 12.2 shows the implementation of the rules for
the puzzle in a neural-symbolic system, where g2 denotes the fact that there is a goat
behind door 2, Di denotes the probability that the car is behind door i, win indicates
your chance of winning the car, and c denotes that you have chosen to change from
door 1 to door 3. The rules are as follows:
t1 : g2,
t1 : g2 →⃝g2,
t2 : Kωc
i
c →Kωc
i
(win = 2/3),
t2 : Kωs
i ¬c →Kωs
i (win = 1/3).

12.4 Discussion
167
t1
D1 = 1/3
D2 = 1/3
D3 = 1/3
g2
g2
D2 = 0
D3 = 2/3
D1 = 1/3
t2
Win = 2/3
c
g2
D2 = 0
D3 = 2/3
D1 = 1/3
Win = 1/3
¬c
Fig. 12.2 A representation of the Monty Hall puzzle
12.4 Discussion
The knowledge representation formalism presented in this chapter provides neural-
symbolic learning systems with the ability to reason about uncertainty in addition
to time and knowledge. An important feature of the approach is that a temporal
dimension can be combined with an epistemic dimension, at the same time allowing
for probabilistic reasoning. We have illustrated this by showing that our formalism
is applicable to some well-known problems of reasoning about uncertainty.
Although the approach proposed here allows reasoning with probabilities, it does
not cater for the computing of probabilities. A natural next step would be to process
qualitative probabilistic statements such as (i > 0.8) →x. Further, the integration
of other logical systems which are relevant to artiﬁcial intelligence, such as BDI
logics [219], would offer interesting foundational and application-oriented develop-
ments. The use of neural-symbolic systems also facilitates knowledge evolution and
revision through learning. It would be interesting to apply the above formalism to
knowledge evolution in the context of distributed systems and model checking.

Chapter 13
Conclusions
This chapter reviews the neural-symbolic approach presented in this book and pro-
vides a summary of the overall neural-symbolic cognitive model. The book deals
with how to represent, learn, and compute expressive forms of symbolic knowledge
using neural networks. We believe this is the way forward towards the provision of
an integrated system of expressive reasoning and robust learning. The provision of
such a system, integrating the two most fundamental phenomena of intelligent cog-
nitive behaviour, has been identiﬁed as a key challenge for computer science [255].
Our goal is to produce computational models with integrated reasoning and learning
capability, in which neural networks provide the machinery necessary for cognitive
computation and learning, while logic provides practical reasoning and explanation
capabilities to the neural models, facilitating the necessary interaction with the out-
side world.
Three notable hallmarks of intelligent cognition are the ability to draw rational
conclusions, the ability to make plausible assumptions, and the ability to generalise
from experience. In a logical setting, these abilities correspond to the processes of
deduction, abduction, and induction, respectively. Although human cognition often
involves the interaction of these three abilities, they are typically studied in isola-
tion (a notable exception is [186]). For example, in artiﬁcial intelligence, symbolic
(logic-based) approaches have been concerned mainly with deductive reasoning,
whereas connectionist (neural-network-based) approaches have focused mainly on
inductive learning. It is well known that this connectionist/symbolic dichotomy
in artiﬁcial intelligence reﬂects a distinction between brain and mind, but we ar-
gue this should not dissuade us from seeking a fruitful synthesis of these para-
digms [56,58,71].
In our research programme, we are seeking to integrate the processes of reason-
ing and learning within the neural-computation paradigm. When we think of neural
networks, what springs to mind is their ability to learn from examples using efﬁcient
algorithms in a massively parallel fashion. In neural computation, induction is typ-
ically seen as the process of changing the weights of a network in ways that reﬂect
the statistical properties of a data set (a set of examples), allowing useful generali-
sations over unseen examples. When we think of symbolic logic, we recognise its
A.S. d’Avila Garcez et al., Neural-Symbolic Cognitive Reasoning, Cognitive Technologies,
169
c⃝Springer-Verlag Berlin Heidelberg 2009

170
13 Conclusions
rigour, and semantic clarity, and the availability of automated proof methods which
can provide explanations of the reasoning process, for example through a proof his-
tory. In neural computation, deduction can be seen as a network computation of out-
put values as a response to input values, given a particular set of weights. Standard
feedforward and partially recurrent networks have been shown capable of deduc-
tive reasoning of various kinds depending on the network architecture, including
nonmonotonic [66], modal [79], intuitionistic [77], and abductive [58] reasoning.
Our goal is to offer a uniﬁed framework for learning and reasoning that exploits
the parallelism and robustness of connectionist architectures. To this end, we have
chosen to work with standard neural networks, whose learning capabilities have
been already demonstrated in signiﬁcant practical applications, and to investigate
how they can be enhanced with more advanced reasoning capabilities. The main
insight of this book is that, in order to enable effective learning from examples and
background knowledge, one should keep network structures as simple as possible,
and try to ﬁnd the best symbolic representation for them. We have done so by pre-
senting translation algorithms from nonclassical logics to single-hidden-layer neural
networks. It was essential to show equivalence between the symbolic representations
and the neural networks, thus ensuring a sound translation of background knowledge
into a connectionist representation. Such results have made our nonclassical neural-
symbolic systems into cognitive massively parallel models of symbolic computation
and learning. We call such systems, combining a connectionist learning component
with a logical reasoning component, neural-symbolic learning systems [66].
In what follows, we brieﬂy review the work on the integration of nonclassical
logics and neural networks presented in the book (see also [33]). We then look at
the combination of different neural networks and their associated logics by the use
of the ﬁbring method [67]. As seen in Chap. 9, the overall model consists of a
set (an ensemble) of simple, single-hidden-layer neural networks – each of which
may represent the knowledge of an agent (or a possible world) at a particular time
point – and connections between networks representing the relationships between
agents/possible worlds. Each ensemble may be seen as being at a different level of
abstraction so that networks at one level may be ﬁbred onto networks at another level
to form a structure combining metalevel and object-level information. We claim that
this structure offers the basis for an expressive yet computationally tractable model
of integrated, robust learning and expressive reasoning.
13.1 Neural-Symbolic Learning Systems
For neural-symbolic integration to be effective in complex applications, we need
to investigate how to represent, reason with, and learn expressive logics in neural
networks. We also need to ﬁnd effective ways of expressing the knowledge encoded
in a trained network in a comprehensible symbolic form. There are at least two
lines of action. The ﬁrst is to take standard neural networks and try to ﬁnd out
which logics they can represent. The other is to take well-established logics and

13.1 Neural-Symbolic Learning Systems
171
concepts (e.g. recursion) and try to encode these in a neural-network architecture.
Both require a principled approach, so that whenever we show that a particular logic
can be represented by a particular neural network, we need to show that the network
and the logic are in fact equivalent (a way of doing this is to prove that the network
computes a formal semantics of the logic). Similarly, if we develop a knowledge
extraction algorithm, we need to make sure that it is correct (sound) in the sense
that it produces rules that are encoded in the network, and that it is quasi-complete
in the sense that the extracted rules increasingly approximate the exact behaviour of
the network.
During the last twenty years, a number of models for neural-symbolic integration
have been proposed (mainly in response to John McCarthy’s note ‘Epistemological
challenges for connectionism’1 [176], itself a response to Paul Smolensky’s paper
‘On the proper treatment of connectionism’ [237]). Broadly speaking, researchers
have made contributions in three main areas, providing either (i) a logical character-
isation of a connectionist system, (ii) a connectionist implementation of a logic, or
(iii) a hybrid system bringing together features from connectionist systems and sym-
bolic artiﬁcial intelligence [132]. Early relevant contributions include [135,229,242]
on knowledge representation, [80, 250] on learning with background knowledge,
and [30, 65, 144, 227, 244] on knowledge extraction. The reader is referred to [66]
for a detailed presentation of neural-symbolic learning systems and applications.
Neural-symbolic learning systems contain six main phases: (1) background
knowledge insertion, (2) inductive learning from examples, (3) massively parallel
deduction, (4) ﬁne-tuning of the theory, (5) symbolic knowledge extraction, and (6)
feedback (see Fig. 13.1). In phase (1), symbolic knowledge is translated into the
initial architecture of a neural network with the use of a translation algorithm. In
phase (2), the neural network is trained with examples by a neural learning algo-
rithm, which revises the theory given in phase (1) as background knowledge. In
phase (3), the network can be used as a massively parallel system to compute the
logical consequences of the theory encoded in it. In phase (4), information obtained
from the computation carried out in phase (3) may be used to help ﬁne-tune the
1 In [176], McCarthy identiﬁed four knowledge representation problems for neural networks: the
problem of elaboration tolerance (the ability of a representation to be elaborated to take additional
phenomena into account), the propositional ﬁxation of neural networks (based on the assumption
that neural networks cannot represent relational knowledge), the problem of how to make use of
any available background knowledge as part of learning, and the problem of how to obtain domain
descriptions from trained networks, as opposed to mere discriminations. Neural-symbolic integra-
tion can address each of the above challenges. In a nutshell, the problem of elaboration tolerance
can be resolved by having networks that are ﬁbred so as to form a modular hierarchy, similarly to
the idea of using self-organising maps [112,125] for language processing, where the lower levels
of abstraction are used for the formation of concepts that are then used at the higher levels of the
hierarchy. Connectionist modal logic [79] deals with the so-called propositional ﬁxation of neural
networks by allowing them to encode relational knowledge in the form of accessibility relations; a
number of other formalisms have also tackled this issue as early as 1990 [11,13,134,230], the key
question being that of how to have simple representations that promote effective learning. Learning
with background knowledge can be achieved by the usual translation of symbolic rules into neural
networks. Problem descriptions can be obtained by rule extraction; a number of such translation
and extraction algorithms have been proposed, for example [30,65,80,132,144,165,192,227,242].

172
13 Conclusions
Symbolic 
Knowledge 
Symbolic 
Knowledge 
Neural  
Network 
Examples 
Learning
Connectionist System
Inference 
Machine 
Explanation
1
2
3
5
6
4
Fig. 13.1 Neural-symbolic learning systems
network in order to better represent the problem domain. This mechanism can be
used, for example, to resolve inconsistencies between the background knowledge
and the training examples. In phase (5), the result of training is explained by the ex-
traction of revised symbolic knowledge. As with the insertion of rules, the extraction
algorithm must be provably correct, so that each rule extracted is guaranteed to be a
rule of the network. Finally, in phase (6), the knowledge extracted may be analysed
by an expert to decide if it should feed the system again, closing the learning and
reasoning cycle.
Our neural-network models consist of feedforward and partially recurrent net-
works, as opposed to the symmetric networks investigated, for example, in [240]. It
uses a localist rather than a distributed representation,2 and it works with backprop-
agation, the neural learning algorithm most successfully used in industrial-strength
applications [224].
13.2 Connectionist Nonclassical Reasoning
We now turn to the question of expressiveness. We believe that for neural computa-
tion to achieve its promise, connectionist models must be able to cater for nonclas-
sical reasoning. We believe that the neural-symbolic community cannot ignore the
achievements and impact that nonclassical logics have had in computer science [71].
Whereas nonmonotonic reasoning dominated research in artiﬁcial intelligence in the
1980s and 1990s, temporal logic has had a large impact in both academia and indus-
try, and modal logic has become a lingua franca for the speciﬁcation and analysis
2 We depart from distributed representations for two main reasons: localist representations can be
associated with highly effective learning algorithms such as backpropagation, and in our view, lo-
calist networks are at an appropriate level of abstraction for symbolic knowledge representation.
As advocated in [200], we believe one should be able to achieve the goals of distributed represen-
tations by properly changing the levels of abstraction of localist networks, whereas some of the
desirable properties of localist models cannot be exhibited by fully-distributed ones.

13.2 Connectionist Nonclassical Reasoning
173
B
A
B
C
D
E
F
W
W
−W
W
W
W
W
W
N1
N2
N3
1
2
3
A
B
Fig. 13.2 Neural-network for logic programming
of knowledge and communication in multiagent and distributed systems [87]. In
this section, we consider modal and temporal reasoning as key representatives of
nonclassical reasoning.
The basic idea behind more expressive, connectionist nonclassical reasoning is
simple, as seen in previous chapters of this book. Instead of having a single net-
work, if we now consider a set of networks such as the one in Fig. 13.2, and we
label them, say, as ω1, ω2, etc., then we can talk about a concept L holding in ω1
and the same concept L holding in ω2 separately. In this way, we can see ω1 as
one possible world and ω2 as another, and this allows us to represent modalities
such as necessity and possibility, and time, and also argumentation [69], epistemic
states [74], and intuitionistic reasoning [77]. It is important to note that this avenue
of research is of interest in connection with McCarthy’s conjecture about the propo-
sitional ﬁxation of neural networks [176] because there is a well-established transla-
tion between propositional modal logic and the two-variable fragment of ﬁrst-order
logic, as we have seen in Chap. 5 (see e.g. [264]), indicating that relatively simple
neural-symbolic systems may go beyond propositional logic.
13.2.1 Connectionist Modal Reasoning
Recall that modal logic deals with the analysis of concepts such as necessity (repre-
sented by □L, read as ‘box L’, and meaning that L is necessarily true), and possibil-
ity (represented by ♦L, read as ‘diamond L’, and meaning that L is possibly true).
A key aspect of modal logic is the use of possible worlds and a binary (accessibil-
ity) relation R(ωi,ω j) between worlds ωi and ω j. In possible-world semantics, a
proposition is necessary in a world if it is true in all worlds which are possible in
relation to that world, whereas it is possible in a world if it is true in at least one
world which is possible in relation to that same world.

174
13 Conclusions
Connectionist modal logic (CML), presented in Chap. 5, uses ensembles of
neural networks (instead of single networks) to represent the language of modal
logic programming [197]. The theories are now sets of modal clauses, each of the
form ωi : ML1,...,MLn →MA, where ωi is a label representing a world in which
the associated clause holds, and M ∈{□,♦}, together with a ﬁnite set of relations
R(ωi,ω j) between worlds ωi and ω j. Such theories are implemented in a network
ensemble, each network representing a possible world, with the use of ensembles
and labels allowing the representation of accessibility relations.
In CML, each network in the ensemble is a simple, single-hidden-layer network
like the network of Fig. 13.2, to which standard neural learning algorithms can be
applied. Learning, in this setting, can be seen as learning the concepts that hold in
each possible world independently, with the accessibility relation providing infor-
mation on how the networks should interact. For example, take three networks all
related to each other. If a neuron ♦a is activated in one of these networks, then a neu-
ron a must be activated in at least one of the networks. If a neuron □a is activated
in one network, then neuron a must be activated in all the networks. This imple-
ments in a connectionist setting the possible-world semantics mentioned above. It
is achieved by deﬁning the connections and the weights of the network ensemble,
following a translation algorithm. Details of the translation algorithm, along with a
soundness proof, have been given in Chap. 5.
As an example, consider Fig. 13.3. This shows an ensemble of three neural net-
works labelled N1,N2,N3, which might communicate in different ways. We look at
N1, N2, and N3 as possible worlds. Input and output neurons may now represent
□L, ♦L, or L, where L is a literal. □A will be true in a world ωi if A is true in all
worlds ω j to which ωi is related. Similarly, ♦A will be true in a world ωi if A is
s
q
q
p
N2
N3
N1
q
r
r
s
q
s
M1
M2
M1
H1
H2
H1
H1
Fig. 13.3 Neural-network ensemble for modal reasoning

13.2 Connectionist Nonclassical Reasoning
175
true in some world ω j to which ωi is related. As a result, if neuron □A is activated
in network N1, denoted by ω1 : □A, and world ω1 is related to worlds ω2 and ω3,
then neuron A must be activated in networks N2 and N3. Similarly, if neuron ♦A is
activated in N1, then a neuron A must be activated in an arbitrary network that is
related to N1.
It is also possible to make use of CML to compute that □A holds in a possible
world, say ωi, whenever A holds in all possible worlds related to ωi, by connecting
the output neurons of the related networks to a hidden neuron in ωi which connects
to an output neuron labelled □A. Dually for ♦A, whenever A holds in some possible
world related to ωi, we connect the output neuron representing A to a hidden neuron
in ωi which connects to an output neuron labelled ♦A. Owing to the simplicity of
each network in the ensemble, when it comes to learning, we can still use backprop-
agation on each network to learn the local knowledge in each possible world.
13.2.2 Connectionist Temporal Reasoning
The Connectionist Temporal Logic of Knowledge (CTLK), presented in Chap. 6,
is an extension of CML which considers temporal and epistemic knowledge [72].
Generally speaking, the idea is to allow, instead of a single ensemble, a number n of
ensembles, each representing the knowledge held by a number of agents at a given
time point t. Figure 13.4 illustrates how this dynamic feature can be combined with
the symbolic features of the knowledge represented in each network, allowing not
only the analysis of the current state (the current possible world or time point), but
also the analysis of how knowledge changes over time.
Of course, there are important issues to do with (1) the optimisation of the model
of Fig. 13.4 in practice, (2) the fact that the number of networks may be bounded,
and (3) the trade-off between space and time computational complexity. The fact,
however, that this model is sufﬁcient to deal with such a variety of reasoning tasks
is encouraging.
The deﬁnition of the number of ensembles s that are necessary to solve a given
problem clearly depends on the problem domain, and on the number of time points
that are relevant to reasoning about the problem. For example, in the case of the
archetypal distributed-knowledge-representation problem of the muddy children
puzzle [87], we know that it sufﬁces to have s equal to the number of children
that are muddy. The deﬁnition of s for a different domain might not be as straight-
forward, possibly requiring a ﬁne-tuning process similar to that performed during
learning, but with a varying network architecture. These and other considerations,
including more extensive evaluations of the model with respect to learning, are still
required. Recently, this model has been applied effectively to multiprocess synchro-
nisation and learning in concurrent programming [157] and in a neural-symbolic
model for analogical reasoning [34].

176
13 Conclusions
t1
t2
t3
Agent 1
Agent 2
Agent 3
Agent 1
Agent 2
Agent 3
Agent 1
Agent 2
Agent 3
Fig. 13.4 Neural-network ensemble for temporal reasoning
13.3 Fibring Neural Networks
In CML, one may need to create copies of certain concepts. As a result, CML does
not deal directly with inﬁnite domains, since this would require inﬁnitely many
copies. An alternative is to map the instances of a variable onto the real numbers,
and then use the real numbers as inputs to a neural network as a way of representing
the instances. This has been done in [136]. However, the question of how to con-
struct such a network to compute and learn a program remained unanswered, since
no translation algorithm was given in [136]. A follow-up paper providing such an
algorithm for ﬁrst-order covered programs has appeared recently [13]. In [11], the
idea of representing variables as real numbers was also followed, and a translation
algorithm from ﬁrst-order acyclic programs to neural-network ensembles was pro-
posed. This algorithm makes use of ﬁbring of neural networks [67]. Brieﬂy, the
idea is to use a neural network to iterate a global counter n. For each clause Ci in the
logic program, this counter is combined (ﬁbred) with another neural network, which
determines whether Ci outputs an atom of level n for a given interpretation I. This
allows us to translate programs with an inﬁnite number of ground instances into a
ﬁnite neural-network structure (e.g. ¬even(x) →even(s(x)) for x ∈N,s(x) = x+1),
and to prove that the network indeed approximates the ﬁxed-point semantics of the
program. The translation is made possible because ﬁbring allows one to implement
a key feature of symbolic computation in neural networks, namely recursion, as we
describe below.

13.3 Fibring Neural Networks
177
The idea of ﬁbring neural networks is simple. Fibred networks may be composed
not only of interconnected neurons but also of other networks, forming a recur-
sive architecture. A ﬁbring function then deﬁnes how this architecture behaves, by
deﬁning how the networks in the ensemble relate to each other. Typically, the ﬁb-
ring function will allow the activation of neurons in one network (A) to inﬂuence the
change of weights in another network (B). Intuitively, this may be seen as training
network B at the same time that network A runs. Interestingly, even though they are a
combination of simple, standard neural networks, ﬁbred networks can approximate
any polynomial function in an unbounded domain, thus being more expressive than
standard feedforward networks.
Figure 13.5, reproduced from Chap. 9, exempliﬁes how a network (B) can be
ﬁbred into another network (A). Of course, the idea of ﬁbring is not only to organ-
ise networks as a number of subnetworks; in Fig. 13.5, the output neuron of A is
expected to be a neural network (B) in its own right. The input, weights, and out-
put of B may depend on the activation state of A’s output neuron, according to the
ﬁbring function ϕ. Fibred networks can be trained by examples in the same way
that standard networks are. For instance, networks A and B could have been trained
separately before they were ﬁbred. Networks can be ﬁbred in a number of different
ways as far as their architectures are concerned; network B could have been ﬁbred
into a hidden neuron of network A.
As an example, network A could have been trained with a robot’s visual sys-
tem, while network B could have been trained with its planning system, and ﬁbring
would serve to perform the composition of the two systems [101]. Fibring can be
very powerful. It offers the extra expressiveness required by complex applications
at low computational cost (that of computing the ﬁbring function ϕ). Of course, we
would like to keep ϕ as simple as possible so that it can be implemented itself by
A
i1
i2
W1
W2
W3
o1
i1
i2
W1
W2
W3
o1
B
ϕ
Fig. 13.5 Fibring neural networks

178
13 Conclusions
simple neurons in a fully-connectionist model. Interesting work remains to be done
in this area, particularly in regard to the question of how one should go about ﬁbring
networks in real applications.
13.4 Concluding Remarks
Connectionist modal logic and its variations offer an illustration of how the area of
neural networks may contribute to the area of logic, while ﬁbring is an example of
how logic can bring insight into neural computation. CML offers parallel models of
computation to modal logic that, at the same time, can be integrated with an efﬁcient
learning algorithm. Fibring is a clear example of how concepts from symbolic com-
putation may help in the development of neural models. This does not necessarily
conﬂict with the ambition of biological plausibility; for example, ﬁbring functions
can be understood as a model of presynaptic weights, which play an important role
in biological neural networks.
Connectionist nonclassical reasoning and network ﬁbring bring us to our overall
cognitive model, which we may call ﬁbred network ensembles. In this model, a net-
work ensemble A (representing, for example, a temporal theory) may be combined
with another network ensemble B (representing, for example, an intuitionistic the-
ory). Along the same lines, metalevel concepts (in A) may be combined and brought
into the object level (B), without necessarily blurring the distinction between the
two levels. One may reason in the metalevel and use that information in the object
level, a typical example being (metalevel) reasoning about actions in (object-level)
databases containing inconsistencies [96]. Relations between networks/concepts in
the object level may be represented and learned in the metalevel as described in
Chap. 10. If two networks denote, for example, P(X,Y) and Q(Z), a metanetwork
can learn to map a representation of the concepts P and Q (for instance using the
hidden neurons of networks P and Q) onto a third network, denoting, say, R(X,Y,Z),
such that, for example, P(X,Y)∧Q(Z) →R(X,Y,Z).
Figure 13.6 illustrates ﬁbred network ensembles. The overall model takes the
most general knowledge representation ensemble of the kind shown in Fig. 13.4, and
allows a number of such ensembles to be combined at different levels of abstraction
through ﬁbring. Relations between concepts at a level n may be generalised at level
n+ 1 with the use of metalevel networks. Abstract concepts at level n may be spe-
cialised (or instantiated) at level n−1 with the use of a ﬁbring function. Evolution
of knowlege with time occurs at each level. Alternative outcomes, possible worlds,
and the nonmonotonic reasoning process of multiple interacting agents can be mod-
elled at each level. Learning can take place inside each modular, simple network or
across networks in the ensemble.
Both the symbolic and the connectionist paradigms have virtues and deﬁcien-
cies. Research into the integration of the two has important implications that are
beneﬁcial to computing and to cognitive science. Concomitantly, the limits of effec-
tive integration should also be pursued, in the sense that integration might become

13.4 Concluding Remarks
179
metalevel 
relations
fibring functions
object-level
Fig. 13.6 Fibred network ensembles
disadvantageous in comparison with purely symbolic or purely connectionist sys-
tems. We believe that this book has contributed to the synergetic integration of con-
nectionism and symbolism.
Of course, the question of how we humans integrate reasoning and learning is
only starting to be answered [109, 241]. We argue that the prospects are better
if we investigate the connectionist processes of the brain together with the logi-
cal processes of symbolic computation, rather than as two isolated paradigms. Of
course, we shall need to be precise as we develop the framework further and test our
model of ﬁbred network ensembles in real cognitive tasks.
The challenges for neural-symbolic integration today emerge from the goal of
effective integration of expressive reasoning and robust learning. One cannot afford
to lose on learning performance when adding reasoning capability to neural mod-
els. This means that one cannot depart from the key concept that neural networks
are composed of simple processing units organised in a massively parallel fashion
(i.e. one cannot allow some clever neuron to perform complex symbol processing).
Ideally, the models should be amenable to advances in computational neuroscience
and brain imaging, which can offer data and also insight into new forms of rep-
resentation. Finally, there are computational challenges associated with the more
practical aspects of the application of neural-symbolic systems in areas such as en-
gineering, robotics, and the Semantic Web. These challenges include the effective,
massively parallel computation of logical models, the efﬁcient extraction of com-
prehensible rules, and, ultimately, the striking of the right balance between compu-
tational tractability and expressiveness. References [57,59–61] contain a number of
recent papers dealing with some of today’s challenges.

180
13 Conclusions
In summary, by paying attention to the developments on either side of the divi-
sion between the symbolic and the subsymbolic paradigms, we are getting closer to
a unifying theory, or at least promoting a faster, principled development of the cog-
nitive and computing sciences and artiﬁcial intelligence. This book has described a
family of connectionist nonclassical reasoning systems and hinted at how they may
be combined at different levels of abstraction by ﬁbring. We hope that it serves as
a stepping stone towards such a theory to reconcile the symbolic and connectionist
approaches.
Human beings are quite extraordinary at performing practical reasoning as they
go about their daily business. There are cases where the human computer, slow as it
is, is faster than artiﬁcial intelligence systems. Why are we faster? Is it the way we
perceive knowledge, as opposed to the way we represent it? Do we know immedi-
ately which rules to select and apply? We must look for the correct representation, in
the sense that it mirrors the way we perceive and apply the rules [100]. Ultimately,
neural-symbolic integration is about asking and trying to answer these questions,
and about the associated provision of neural-symbolic systems with integrated ca-
pabilities for expressive reasoning and robust learning. We believe that this book has
offered a principled computational model towards this goal.

References
[1] S. Abramsky. Computational interpretations of linear logic. Theoretical Computer Science,
111(1–2):3–57, 1993.
[2] V. Ajjanagadde. Reasoning with function symbols in a connectionist system. In Proceedings
of the Eleventh Annual Conference of the Cognitive Science Society, pages 388–395, 1989.
[3] V. Ajjanagadde. Rule-Based Reasoning in Connectionist Networks. PhD thesis, University
of Minnesota, 1997.
[4] C.E. Alchourr´on, P. G¨ardenfors, and D.C. Makinson. On the logic of theory change: Partial
meet contraction and revision functions. Journal of Symbolic Logic, 50:510–530, 1985.
[5] N. Alechina, M. Mendler, V. de Paiva, and E. Ritter. Categorical and Kripke semantics
for constructive S4 modal logic. In L. Fribourg, editor, Computer Science Logic, CSL’01,
volume 2142 of Lecture Notes in Computer Science, pages 292–307. Springer, 2001.
[6] N. Angelopoulos and S.H. Muggleton. Machine learning metabolic pathway descriptions
using a probabilistic relational representation. Electronic Transactions in Artiﬁcial Intelli-
gence, 7, 2002.
[7] G. Antoniou. Nonmonotonic Reasoning. MIT Press, Cambridge, MA, 1997.
[8] G. Antoniou, D. Billington, and M.J. Maher. Sceptical logic programming based default rea-
soning: Defeasible logic rehabilitated. In R. Miller and M. Shanahan, editors, COMMON-
SENSE 98, The Fourth Symposium on Logical Formalizations of Commonsense Reasoning,
pages 1–20, London, 1998.
[9] K.R. Apt and R.N. Bol. Logic programming and negation: A survey. Journal of Logic
Programming, 19–20:9–71, 1994.
[10] S.N. Art¨emov. Explicit provability and constructive semantics. Bulletin of Symbolic Logic,
7(1):1–36, 2001.
[11] S. Bader, A.S. d’Avila Garcez, and P. Hitzler. Computing ﬁrst-order logic programs by
ﬁbring artiﬁcial neural networks. In Proceedings of the AAAI International FLAIRS Con-
ference, pages 314–319, 2005.
[12] S. Bader and P. Hitzler. Dimensions of neural-symbolic integration – a structured survey.
In S.N. Art¨emov, H. Barringer, A.S. d’Avila Garcez, L.C. Lamb, and J. Woods, editors, We
Will Show Them! Essays in Honour of Dov Gabbay, pages 167–194. College Publications,
International Federation for Computational Logic, 2005.
[13] S. Bader, P. Hitzler, S. Holldobler, and A. Witzel. A fully connectionist model generator for
covered ﬁrst-order logic programs. In Proceedings of the International Joint Conference
on Artiﬁcial Intelligence, IJCAI-07, pages 666–671, Hyderabad, India, 2007. AAAI Press,
2007.
[14] P. Baldi and S. Brunak. Bioinformatics: The Machine Learning Approach. MIT Press, 2001.
[15] M. Baldoni, L. Giordano, and A. Martelli. A modal extension of logic programming: Modu-
larity, beliefs and hypothetical reasoning. Journal of Logic and Computation, 8(5):597–635,
1998.
181

182
References
[16] C. Balkenius and P. G¨ardenfors. Nonmonotonic inference in neural networks. In Princi-
ples of Knowledge Representation and Reasoning, Proceedings of the Second International
Conference, KR91, pages 32–39, 1991.
[17] D.H. Ballard. Parallel logical inference and energy minimization. In Proceedings of the
National Conference on Artiﬁcial Intelligence, AAAI-86, pages 203–208, 1986.
[18] H. Barringer, D.M. Gabbay, and J. Woods. Temporal dynamics of support and attack net-
works: From argumentation to zoology. In D. Hutter and W. Stephan, editors, Mechanizing
Mathematical Reasoning, Essays in Honor of J¨org H. Siekmann on the Occasion of His
60th Birthday, volume 2605 of Lecture Notes in Computer Science, pages 59–98. Springer,
2005.
[19] R. Basilio, G. Zaverucha, and V.C. Barbosa. Learning logic programs with neural networks.
In 11th International Conference on Inductive Logic Programming, ILP01, volume 2157 of
Lecture Notes in Computer Science, pages 15–26. Springer, 2001.
[20] R. Basilio, G. Zaverucha, and A.S. d’Avila Garcez. Inducing relational concepts with neural
networks via the LINUS system. In Proceedings of the Fifth International Conference on
Neural Information Processing, ICONIP’98, pages 1507–1510, 1998.
[21] M. Baudinet. Temporal logic programming is complete and expressive. In Proceedings of
ACM Symposium on Principles of Programming Languages, pages 267–280, Austin, Texas,
1989.
[22] T.J.M. Bench-Capon. Persuasion in practical argument using value-based argumentation
frameworks. Journal of Logic and Computation, 13:429–448, 2003.
[23] B. Bennett. Spatial reasoning with propositional logics. In Proceedings of the Fourth In-
ternational Conference on Principles of Knowledge Representation and Reasoning, KR-94,
pages 51–62, 1994.
[24] B. Bennett, C. Dixon, M. Fisher, U. Hustadt, E. Franconi, I. Horrocks, and M. de Rijke.
Combinations of modal logics. Artiﬁcial Intelligence Review, 17(1):1–20, 2002.
[25] P. Besnard and A. Hunter. Towards a logic-based theory of argumentation. In Proceedings of
17th National Conference on Artiﬁcial Intelligence, AAAI-00, pages 411–416. AAAI Press,
2000.
[26] C. Bishop. Neural Networks for Pattern Recognition. Oxford University Press, Oxford,
1995.
[27] C. Bishop. Pattern Recognition and Machine Learning. Springer, 2006.
[28] P. Blackburn, M. de Rijke, and Y. Venema. Modal Logic. Cambridge University Press,
2001.
[29] H. Blair and V.S. Subrahmanian. Paraconsistent logic programming. Theoretical Computer
Science, 68:135–154, 1989.
[30] G. Bologna. Is it worth generating rules from neural network ensembles? Journal of Applied
Logic, 2(3):325–348, 2004.
[31] A. Bondarenko, P. Dung, R.A. Kowalski, and F. Toni. An abstract, argumentation theoretic
approach to default reasoning. Artiﬁcial Intelligence, 93:63–101, 1997.
[32] R.V. Borges, L.C. Lamb, and A.S. d’Avila Garcez. Combining architectures for temporal
learning in neural-symbolic systems. In Proceedings of the 6th International Conference on
Hybrid Intelligent Systems, HIS 2006, page 46. IEEE Computer Society, 2006.
[33] R.V. Borges, L.C. Lamb, and A.S. d’Avila Garcez.
Reasoning and learning about past
temporal knowledge in connectionist models. In Proceedings of the Twentieth International
Joint Conference on Neural Networks, IJCNN 2007, pages 1488–1493, 2007.
[34] R.V. Borges, A.S. d’Avila Garcez, and L.C. Lamb. A neural-symbolic perspective on anal-
ogy. Behavioral and Brain Sciences, 31(4):379–380, 2008.
[35] N.K. Bose and P. Liang. Neural Networks Fundamentals with Graphs, Algorithms, and
Applications. McGraw-Hill, 1996.
[36] B. Boutsinas and M.N. Vrahatis. Artiﬁcial nonmonotonic neural networks. Artiﬁcial Intel-
ligence, 132:1–38, 2001.
[37] G. Brewka. Nonmonotonic Reasoning: Logical Foundations of Commonsense, volume 12
of Cambridge Tracts in Theoretical Computer Science. Cambridge University Press, 1990.

References
183
[38] G. Brewka. Cumulative default logic: In defense of nonmonotonic inference rules. Artiﬁcial
Intelligence, 50(2):183–205, 1991.
[39] G. Brewka. Dynamic argument systems: A formal model of argumentation processes based
on situation calculus. Journal of Logic and Computation, 11(2):257–282, 2001.
[40] G. Brewka and T. Eiter.
Preferred answer sets for extended logic programs.
Artiﬁcial
Intelligence, 109:297–356, 1999.
[41] K. Broda, D.M. Gabbay, L.C. Lamb, and A. Russo. Labelled natural deduction for condi-
tional logics of normality. Logic Journal of the IGPL, 10(2):123–163, 2002.
[42] K. Broda, D.M. Gabbay, L.C. Lamb, and A. Russo. Compiled Labelled Deductive Systems:
A Uniform Presentation of Non-Classical Logics, Studies in Logic and Computation. Re-
search Studies Press/Institute of Physics Publishing, Baldock, UK/Philadelphia, PA, 2004.
[43] A. Browne and R. Sun. Connectionist inference models. Neural Networks, 14:1331–1355,
2001.
[44] C. Brzoska. Temporal logic programming and its relation to constraint logic programming.
In Proceedings of the International Symposium on Logic Programming, pages 661–677.
MIT Press, 1991.
[45] M.A. Castilho, L. Farinas del Cerro, O. Gasquet, and A. Herzig.
Modal tableaux with
propagation rules and structural rules. Fundamenta Informaticae, 32(3–4):281–297, 1997.
[46] A. Chagrov and M. Zakharyaschev. Modal Logic. Clarendon Press, Oxford, 1997.
[47] Y. Chauvin and D. Rumelhart, editors. Backpropagation: Theory, Architectures and Appli-
cations. Lawrence Erlbaum, 1995.
[48] C.I. Ches˜nevar, A.G. Maguitman, and R.P. Loui. Logical models of argument. ACM Com-
puting Surveys, 32(4):337–383, December 2000.
[49] K.L. Clark. Negation as failure. In H. Gallaire and J. Minker, editors, Logic and Databases,
pages 293–322. Plenum Press, New York, 1978.
[50] E.M. Clarke and H. Schlingloff. Model checking. In J.A. Robinson and A. Voronkov, edi-
tors, Handbook of Automated Reasoning, volume II, chapter 24, pages 1635–1790. Elsevier,
2001.
[51] I. Cloete and J.M. Zurada, editors. Knowledge-Based Neurocomputing. MIT Press, Cam-
bridge, MA, 2000.
[52] G. Cybenco. Approximation by superposition of sigmoidal functions. In Mathematics of
Control, Signals and Systems 2, pages 303–314, 1989.
[53] B. DasGupta and G. Schnitger. Analog versus discrete neural networks. Neural Computa-
tion, 8:805–818, 1996.
[54] B.A. Davey and H.A. Priestley. Introduction to Lattices and Order. Cambridge University
Press, 1990.
[55] R. Davies and F. Pfenning. A modal analysis of staged computation. Journal of the ACM,
48(3):555–604, 2001.
[56] A.S. d’Avila Garcez. Fewer epistemological challenges for connectionism. In S.B. Cooper,
B. Lowe, and L. Torenvliet, editors, Proceedings of Computability in Europe, CiE 2005,
volume 3526 of Lecture Notes in Computer Science, pages 139–149. Springer, 2005.
[57] A.S. d’Avila Garcez, J. Elman, and P. Hitzler, editors. Proceedings of IJCAI International
Workshop on Neural-Symbolic Learning and Reasoning, NeSy05, Edinburgh, 2005.
[58] A.S. d’Avila Garcez, D.M. Gabbay, O. Ray, and J. Woods. Abductive reasoning in neural-
symbolic systems. TOPOI: An International Review of Philosophy, 26:37–49, 2007.
[59] A.S. d’Avila Garcez and P. Hitzler, editors. Proceedings of ECAI International Workshop
on Neural-Symbolic Learning and Reasoning, NeSy08, Patras, Greece, 2008.
[60] A.S. d’Avila Garcez, P. Hitzler, and G. Tamburrini, editors. Proceedings of ECAI Interna-
tional Workshop on Neural-Symbolic Learning and Reasoning, NeSy06, Trento, Italy, 2006.
[61] A.S. d’Avila Garcez, P. Hitzler, and G. Tamburrini, editors. Proceedings of IJCAI Interna-
tional Workshop on Neural-Symbolic Learning and Reasoning, NeSy07, Hyderabad, India,
2007.
[62] A.S. d’Avila Garcez. Extended theory reﬁnement in knowledge-based neural networks.
In Proceedings of IEEE International Joint Conference on Neural Networks, IJCNN’02,
Honolulu, Hawaii, 2002.

184
References
[63] A.S. d’Avila Garcez. On Gabbay’s ﬁbring methodology for Bayesian and neural networks.
In D. Gillies, editor, Laws and Models in Science. King’s College London, 233–245, 2004.
[64] A.S. d’Avila Garcez, K. Broda, and D.M. Gabbay. Metalevel priorities and neural networks.
In P. Frasconi, M. Gori, F. Kurfess, and A. Sperduti, editors, Proceedings of ECAI 2000,
Workshop on the Foundations of Connectionist–Symbolic Integration, Berlin, 2000.
[65] A.S. d’Avila Garcez, K. Broda, and D.M. Gabbay. Symbolic knowledge extraction from
trained neural networks: A sound approach. Artiﬁcial Intelligence, 125:155–207, 2001.
[66] A.S. d’Avila Garcez, K. Broda, and D.M. Gabbay. Neural-Symbolic Learning Systems:
Foundations and Applications, Perspectives in Neural Computing. Springer, 2002.
[67] A.S. d’Avila Garcez and D.M. Gabbay. Fibring neural networks. In Proceedings of 19th
National Conference on Artiﬁcial Intelligence, AAAI-04, pages 342–347, San Jose, CA,
2004.
[68] A.S. d’Avila Garcez, D.M. Gabbay, and L.C. Lamb.
Argumentation neural networks.
In Proceedings of the 11th International Conference on Neural Information Processing,
ICONIP’04, volume 3316 of Lecture Notes in Computer Science, pages 606–612. Springer,
2004.
[69] A.S. d’Avila Garcez, D.M. Gabbay, and L.C. Lamb. Value-based argumentation frameworks
as neural-symbolic learning systems. Journal of Logic and Computation, 15(6):1041–1058,
2005.
[70] A.S. d’Avila Garcez and L.C. Lamb.
Reasoning about time and knowledge in neural-
symbolic learning systems. In S.B. Thrun, L. Saul, and B. Schoelkopf, editors, Advances
in Neural Information Processing Systems 16, Proceedings of NIPS 2003, pages 921–928.
MIT Press, 2004.
[71] A.S. d’Avila Garcez and L.C. Lamb. Neural-symbolic systems and the case for non-classical
reasoning. In S.N. Art¨emov, H. Barringer, A.S. d’Avila Garcez, L.C. Lamb, and J. Woods,
editors, We Will Show Them! Essays in Honour of Dov Gabbay, pages 469–488. College
Publications, International Federation for Computational Logic, 2005.
[72] A.S. d’Avila Garcez and L.C. Lamb. A connectionist computational model for epistemic
and temporal reasoning. Neural Computation, 18(7):1711–1738, 2006.
[73] A.S. d’Avila Garcez, L.C. Lamb, K. Broda, and D.M. Gabbay. Distributed knowledge rep-
resentation in neural-symbolic learning systems: A case study. In Proceedings of AAAI
International FLAIRS Conference, pages 271–275 St. Augustine, FL, 2003. AAAI Press,
2007.
[74] A.S. d’Avila Garcez, L.C. Lamb, K. Broda, and D.M. Gabbay. Applying connectionist
modal logics to distributed knowledge representation problems. International Journal on
Artiﬁcial Intelligence Tools, 13(1):115–139, 2004.
[75] A.S. d’Avila Garcez, L.C. Lamb, and D.M. Gabbay. A connectionist inductive learning
system for modal logic programming. In Proceedings of the 9th International Conference
on Neural Information Processing, ICONIP’02, pages 1992–1997, Singapore, 2002. IEEE
Press, 2007.
[76] A.S. d’Avila Garcez, L.C. Lamb, and D.M. Gabbay. Neural-symbolic intuitionistic rea-
soning. In A. Abraham, M. k¨oppen, and K. Franke, editors, Design and Application of
Hybrid Intelligent Systems, volume 104 of Frontiers in Artiﬁcial Intelligence and Applica-
tions, pages 399–408. IOS Press, Amsterdam, 2003.
[77] A.S. d’Avila Garcez, L.C. Lamb, and D.M. Gabbay. Connectionist computations of intu-
itionistic reasoning. Theoretical Computer Science, 358(1):34–55, 2006.
[78] A.S. d’Avila Garcez, L.C. Lamb, and D.M. Gabbay. A connectionist model for constructive
modal reasoning. In Advances in Neural Information Processing Systems 18, Proceedings
of NIPS 2005, pages 403–410. MIT Press, 2006.
[79] A.S. d’Avila Garcez, L.C. Lamb, and D.M. Gabbay. Connectionist modal logic: Represent-
ing modalities in neural networks. Theoretical Computer Science, 371(1–2):34–53, 2007.
[80] A.S. d’Avila Garcez and G. Zaverucha.
The connectionist inductive learning and logic
programming system. Applied Intelligence Journal, Special Issue on Neural Networks and
Structured Knowledge, 11(1):59–77, 1999.

References
185
[81] M. Davis. The early history of automated deduction. In J.A. Robinson and A. Voronkov, edi-
tors, Handbook of Automated Reasoning, volume I, chapter 1, pages 3–15. Elsevier Science,
2001.
[82] L.A. Doumas and J.E. Hummel. A symbolic–connectionist model of relation discovery.
In Proceedings of XXVII Annual Conference of the Cognitive Science Society, CogSci2005,
pages 606–611, Stresa, Italy, July 2005.
[83] P.M. Dung. On the acceptability of arguments and its fundamental role in nonmonotonic
reasoning, logic programming and n-person games.
Artiﬁcial Intelligence, 77:321–357,
1995.
[84] S. Dzeroski and N. Lavrac, editors. Relational Data Mining. Springer, September 2001.
[85] J.L. Elman. Finding structure in time. Cognitive Science, 14(2):179–211, 1990.
[86] H.B. Enderton. A Mathematical Introduction to Logic. Academic Press, 1972.
[87] R. Fagin, J. Halpern, Y. Moses, and M. Vardi. Reasoning About Knowledge. MIT Press,
1995.
[88] L. Farinas del Cerro and A. Herzig. Modal deduction with applications in epistemic and
temporal logics. In D.M. Gabbay, C.J. Hogger, and J.A. Robinson, editors, Handbook of
Logic in Artiﬁcial Intelligence and Logic Programming, volume 4, pages 499–594. Oxford
University Press, 1995.
[89] M. Fischer, D.M. Gabbay, and L. Vila, editors. Handbook of Temporal Reasoning in Artiﬁ-
cial Intelligence. Elsevier, 2005.
[90] M. Fitting. Proof Methods for Modal and Intuitionistic Logics. Reidel, Dordrecht, 1983.
[91] M. Fitting. A Kripke–Kleene semantics for general logic programs. Journal of Logic Pro-
gramming, 2:295–312, 1985.
[92] M. Fitting. Metric methods: Three examples and a theorem. Journal of Logic Programming,
21:113–127, 1994.
[93] N. Friedman, L. Getoor, D. Koller, and A. Pfeffer. Learning probabilistic relational models.
In Proceedings of International Joint Conference on Artiﬁcial Intelligence, IJCAI’99, pages
1300–1309, 1999.
[94] N. Friedman and D. Koller. Being Bayesian about network structure: A Bayesian approach
to structure discovery in Bayesian networks. Machine Learning, 50:95–126, 2003.
[95] L.M. Fu. Neural Networks in Computer Intelligence. McGraw-Hill, 1994.
[96] D.M. Gabbay and A. Hunter. Making inconsistency respectable: Part 2 – meta-level han-
dling of inconsistency. In Symbolic and Quantitative Approaches to Reasoning and Uncer-
tainty, ECSQARU’93, volume 747 of Lecture Notes in Computer Science, pages 129–136.
Springer, 1993.
[97] D.M. Gabbay. Theoretical foundations for non-monotonic reasoning in expert systems.
In K.R. Apt, editor, NATO Advanced Study Institute on Logics and Models of Concurrent
Systems, pages 439–457. Springer, 1985.
[98] D.M. Gabbay. The declarative past and imperative future. In H. Barringer, editor, Proceed-
ings of the Colloquium on Temporal Logic and Speciﬁcations, volume 398 of Lecture Notes
in Computer Science, pages 409–448. Springer, 1989.
[99] D.M. Gabbay. Labelled Deductive Systems, volume 1. Oxford University Press, Oxford,
1996.
[100] D.M. Gabbay. Elementary Logics: A Procedural Perspective. Prentice Hall, London, 1998.
[101] D.M. Gabbay. Fibring Logics, volume 38 of Oxford Logic Guides. Oxford University Press,
Oxford, 1999.
[102] D.M. Gabbay and F. Guenthner, editors. Handbook of Philosophical Logic, volumes 1–18.
Springer, 2000–2008. Volumes 15–18 in press.
[103] D.M. Gabbay, I. Hodkinson, and M. Reynolds. Temporal logic: Mathematical Foundations
and Computational Aspects, volume 1, volume 28 of Oxford Logic Guides. Oxford Univer-
sity Press, Oxford, 1994.
[104] D.M. Gabbay, C. Hogger, and J.A. Robinson, editors.
Handbook of Logic in Artiﬁ-
cial Intelligence and Logic Programming, volumes 1–5. Oxford University Press, Oxford,
1994–1999.

186
References
[105] D.M. Gabbay and A. Hunter. Making inconsistency respectable: A logical framework for
inconsistency in reasoning. In P. Jorrand and J. Kelemen, editors, FAIR, volume 535 of
Lecture Notes in Computer Science, pages 19–32. Springer, 1991.
[106] D.M. Gabbay, A. Kurucz, F. Wolter, and M. Zakharyaschev. Many-Dimensional Modal
Logics: Theory and Applications, volume 148 of Studies in Logic and the Foundations of
Mathematics. Elsevier Science, 2003.
[107] D.M. Gabbay and N. Olivetti. Goal-Directed Proof Theory. Kluwer, 2000.
[108] D.M. Gabbay and J. Woods. The law of evidence and labelled deduction: A position paper.
Phi News, 4, October 2003.
[109] D.M. Gabbay and J. Woods. A Practical Logic of Cognitive Systems. Volume 2: The Reach
of Abduction: Insight and Trial. Elsevier, 2005.
[110] D. Gamberger and N. Lavrac. Conditions for Occam’s razor applicability and noise elim-
ination. In M. Someren and G. Widmer, editors, Proceedings of the European Conference
on Machine Learning, pages 108–123, Prague, 1997.
[111] A.J. Garc´ıa and G.R. Simari. Defeasible logic programming: An argumentative approach.
Theory and Practice of Logic Programming, 4(1):95–138, 2004.
[112] P. G¨ardenfors. Conceptual Spaces: The Geometry of Thought. MIT Press, 2000.
[113] M. Garzon. Models of Massive Parallelism: Analysis of Cellular Automata and Neural
Networks, Texts in Theoretical Computer Science. Springer, 1996.
[114] R.W. Gayler and R. Wales. Connections, binding, uniﬁcation and analogical promiscuity.
In K. Holyoak, D. Gentner, and B. Kokinov, editors, Advances in Analogy Research: Inte-
gration of Theory and Data from the Cognitive, Computational and Neural Sciences, pages
181–190. Soﬁa, Bulgaria, 1998.
[115] M. Gelfond and V. Lifschitz. The stable model semantics for logic programming. In Pro-
ceedings of the Fifth Logic Programming Symposium, pages 1070–1080. MIT Press, 1988.
[116] M. Gelfond and V. Lifschitz. Classical negation in logic programs and disjunctive databases.
New Generation Computing, 9:365–385, 1991.
[117] J.L. Gersting. Mathematical Structures for Computer Science. Computer Science Press,
New York, 1993.
[118] C.L. Giles and C.W. Omlin. Extraction, insertion and reﬁnement of production rules in re-
current neural networks. Connection Science, Special Issue on Architectures for Integrating
Symbolic and Neural Processes, 5(3):307–328, 1993.
[119] R. Haenni, J. Kohlas, and N. Lehmann. Probabilistic argumentation systems. In J. Kohlas
and S. Moral, editors, Handbook of Defeasible Reasoning and Uncertainty Management
Systems, volume 5, pages 221–288. Kluwer, 2000.
[120] N. Hallack, G. Zaverucha, and V. Barbosa. Towards a hybrid model of ﬁrst-order theory
reﬁnement. In S. Wermter and R. Sun, editors, Hybrid Neural Systems, volume 1778 of
Lecture Notes in Artiﬁcial Intelligence, pages 92–106. Springer, 1998.
[121] J.Y. Halpern. Reasoning About Uncertainty. MIT Press, 2003.
[122] J.Y. Halpern, R. Harper, N. Immerman, P.G. Kolaitis, M.Y. Vardi, and V. Vianu. On the un-
usual effectiveness of logic in computer science. Bulletin of Symbolic Logic, 7(2):213–236,
2001.
[123] J.Y. Halpern, R. van der Meyden, and M.Y. Vardi. Complete axiomatizations for reasoning
about knowledge and time. SIAM Journal on Computing, 33(3):674–703, 2004.
[124] D. Harel. Dynamic logic. In D.M. Gabbay and F. Guenthner, editors, Handbook of Philo-
sophical Logic, volume 2, pages 497–604. Reidel, Boston, 1984.
[125] S. Haykin. Neural Networks: A Comprehensive Foundation. Prentice Hall, 1999.
[126] J. Henderson.
Estimating probabilities of unbounded categorization problems.
In Pro-
ceedings of European Symposium on Artiﬁcial Neural Networks, pages 383–388, Bruges,
Belgium, April 2002.
[127] J. Hertz, A. Krogh, and R.G. Palmer. Introduction to the Theory of Neural Computation,
Studies in the Science of Complexity. Addison-Wesley, Santa Fe Institute, 1991.
[128] J.W. Hines. A logarithmic neural network architecture for unbounded non-linear function
approximation.
In Proceedings of IEEE International Conference on Neural Networks,
volume 2, pages 1245–1250, Washington, DC, June 1996.

References
187
[129] J. Hintikka. Knowledge and Belief. Cornell University Press, Ithaca, NY, 1962.
[130] G.E. Hinton, editor. Connectionist Symbol Processing. MIT Press, 1991.
[131] H. Hirsh and M. Noordewier. Using background knowledge to improve inductive learning:
a case study in molecular biology. IEEE Expert, 10:3–6, 1994.
[132] P. Hitzler, S. Holldobler, and A.K. Seda. Logic programs and connectionist networks. Jour-
nal of Applied Logic, Special Issue on Neural-Symbolics Systems, 2(3):245–272, 2004.
[133] S. H¨olldobler. A structured connectionist uniﬁcation algorithm. In Proceedings of the Na-
tional Conference on Artiﬁcial Intelligence, AAAI-90, pages 587–593, 1990.
[134] S. H¨olldobler. Automated inferencing and connectionist models. Postdoctoral thesis, In-
tellektik, Informatik, TH Darmstadt, 1993.
[135] S. H¨olldobler and Y. Kalinke. Toward a new massively parallel computational model for
logic programming. In Proceedings of the Workshop on Combining Symbolic and Connec-
tionist Processing, ECAI 1994, pages 68–77, 1994.
[136] S. H¨olldobler, Y. Kalinke, and H.P. Storr. Approximating the semantics of logic programs by
recurrent neural networks. Applied Intelligence Journal, Special Issue on Neural Networks
and Structured Knowledge, 11(1):45–58, 1999.
[137] S. H¨olldobler and F. Kurfess. CHCL: A connectionist inference system. In B. Fronhofer
and G. Wrightson, editors, Parallelization in Inference Systems, pages 318–342. Springer,
1992.
[138] V. Honavar and L. Uhr, editors. Artiﬁcial Intelligence and Neural Networks: Steps Toward
Principled Integration. Academic Press, 1994.
[139] J.J. Hopﬁeld. Neural networks and physical systems with emergent collective computational
abilities. Proceedings of the National Academy of Sciences of the U.S.A., 79: 2554–2558,
1982.
[140] K. Hornik, M. Stinchcombe, and H. White. Multilayer feedforward networks are universal
approximators. Neural Networks, 2:359–366, 1989.
[141] G.E. Hughes and M.J. Cresswell. A New Introduction to Modal Logic. Routledge, London,
1996.
[142] E.B. Hunt, J. Marin, and P.J. Stone. Experiments in Induction. Academic Press, New York,
1966.
[143] M.R.A. Huth and M.D. Ryan. Logic in Computer Science: Modelling and Reasoning About
Systems. Cambridge University Press, 2000.
[144] H. Jacobsson. Rule extraction from recurrent neural networks: A taxonomy and review.
Neural Computation, 17(6):1223–1263, 2005.
[145] M.I. Jordan. Attractor dynamics and parallelisms in a connectionist sequential machine.
In Proceedings of the Eighth Annual Conference of the Cognitive Science Society, pages
531–546, 1986.
[146] A.C. Kakas, P. Mancarella, and P.M. Dung. The acceptability semantics of logic programs.
In Proceedings of the Eleventh International Conference on Logic Programming ICLP-94,
pages 504–519. MIT Press, 1994.
[147] K. Engesser and D.M. Gabbay. Quantum logic, Hilbert space, revision theory. Artiﬁcial
Intelligence, 136(1):61–100, 2002.
[148] R. Khardon and D. Roth. Learning to reason. Journal of the ACM, 44(5):697–725, 1997.
[149] B. Kijsirikul, S. Sinthupinyo, and K. Chongkasemwongse. Approximate match of rules
using backpropagation neural networks. Machine Learning, 43(3):273–299, 2001.
[150] S. Kobayashi. Monad as modality. Theoretical Computer Science, 175(1):29–74, 1997.
[151] J.F. Kolen. Exploring the Computational Capabilities of Recurrent Neural Networks. PhD
thesis, Ohio State University, 1994.
[152] E. Komendantskaya. First-order deduction in neural networks. In Proceedings of the 1st
Conference on Language and Automata Theory and Applications, LATA’07, pages 307–318,
Tarragona, Spain, March 2007.
[153] R.A. Kowalski and F. Toni. Abstract argumentation. Artiﬁcial Intelligence and Law, 4(3–4):
275–296, 1996.
[154] S. Kraus. Strategic Negotiation in Multi-Agent Environments. MIT Press, Cambridge, MA,
2001.

188
References
[155] S. Kraus, D. Lehmann, and M. Magidor. Nonmonotonic reasoning, preferential models and
cumulative logics. Artiﬁcial Intelligence, 44:167–208, 1990.
[156] S. Kripke.
Semantical analysis of modal logic I: Normal modal propositional cal-
culi. Zeitschrift f¨ur mathematische Logic and Grundlagen der Mathematik, pages 67–96,
1963.
[157] L.C. Lamb, R.V. Borges, and A.S. d’Avila Garcez. A connectionist cognitive model for tem-
poral synchronisation and learning. In Proceedings of the Twenty-Second AAAI Conference
on Artiﬁcial Intelligence, AAAI 2007, pages 827–832. AAAI Press, 2007.
[158] N. Landwehr, A. Passerini, L. De Raedt, and P.L. Frasconi. kFOIL: Learning simple rela-
tional kernels. In Proceedings of the National Conference on Artiﬁcial Intelligence, AAAI
2006, pages 389–394. AAAI Press, 2006.
[159] N. Lavrac and S. Dzeroski. Inductive Logic Programming: Techniques and Applications,
Ellis Horwood Series in Artiﬁcial Intelligence Ellis Horwood, 1994.
[160] N. Lavrac, S. Dzeroski, and M. Grobelnik. Experiments in learning nonrecursive deﬁnitions
of relations with LINUS. Technical report, Josef Stefan Institute, Yugoslavia, 1990.
[161] S. Lawrence, C. Lee Giles, and A. Chung Tsoi. Lessons in neural networks training: Over-
ﬁtting may be harder than expected. In Proceedings of the National Conference on Artiﬁcial
Intelligence, AAAI-96, pages 540–545, 1996.
[162] C. Lewis. A Survey of Symbolic Logic. University of California Press, Berkeley, 1918.
[163] J.W. Lloyd. Foundations of Logic Programming. Springer, 1987.
[164] J.W. Lloyd. Logic for Learning: Learning Comprehensible Theories from Structured Data.
Springer, 2003.
[165] A. Lozowski and J.M. Zurada. Extraction of linguistic rules from data via neural networks
and fuzzy approximation. In I. Cloete and J.M. Zurada, editors, Knowledge-Based Neuro-
computing, pages 403–417. MIT Press, 2000.
[166] V. Lukaszewicz. Nonmonotonic Reasoning: Formalization of Commonsense Reasoning.
Ellis Horwood, 1990.
[167] M. Maidl. The common fragment of CTL and LTL. In Proceedings of the 41st IEEE
Symposium on Foundations of Computer Science, pages 643–652, 2000.
[168] D.C. Makinson. General patterns in nonmonotonic reasoning. In D.M. Gabbay, C.J. Hogger,
and J.A. Robinson, editors, Handbook of Logic in Artiﬁcial Intelligence and Logic Program-
ming, volume 3, pages 35–110. Oxford University Press, 1994.
[169] O. Mangasarian, J. Shavlik, and E. Wild. Knowledge-based kernel approximation. Journal
of Machine Learning Research, 5:1127–1141, 2004.
[170] Z. Manna and R. Waldinger. The Logical Basis for Computer Programming. Volume 1:
Deductive Reasoning. Addison-Wesley, Boston, 1985.
[171] W. Marek and M. Truszczynski.
Nonmonotonic Logic: Context Dependent Reasoning.
Springer, 1993.
[172] P. Martin-Lof. Constructive mathematics and computer programming. In Logic, Methodol-
ogy and Philosophy of Science VI, pages 153–179. North-Holland, 1982.
[173] G. Mayraz and G.E. Hinton. Recognizing handwritten digits using hierarchical products
of experts. IEEE Transactions on Pattern Analysis and Machine Intelligence, 24:189–197,
2002.
[174] J. McCarthy. Programs with common sense. In Proceedings of the Teddington Conference
on the Mechanization of Thought Processes, pages 75–91, London, 1959. Her Majesty’s
Stationery Ofﬁce.
[175] J. McCarthy. Circumscription: A form of nonmonotonic reasoning. Artiﬁcial Intelligence,
13:27–39, 1980.
[176] J. McCarthy. Epistemological challenges for connectionism. Behavioral and Brain Sci-
ences, 11(1):44, 1988.
[177] W.S. McCulloch and W. Pitts. A logical calculus of the ideas immanent in neural nets.
Bulletin of Mathematical Biophysics, 5:115–133, 1943.
[178] C. McMillan, M.C. Mozer, and P. Smolensky. Rule induction through integrated symbolic
and subsymbolic processing. In J. Moody, S. Hanson, and R. Lippmann, editors, Advances
in Neural Information Processing Systems 4, Proceedings of NIPS 1991, pages 969–976.
Morgan Kaufmann, San Mateo, CA, 1992.

References
189
[179] C. McMillan, M.C. Mozer, and P. Smolensky. Dynamic conﬂict resolution in a connectionist
rule-based system. In Proceedings of the 13th International Joint Conference on Artiﬁcial
Intelligence, IJCAI-93, pages 1366–1371, 1993.
[180] M. Mendler. Characterising combinatorial timing analysis in intuitionistic modal logic.
Logic Journal of the IGPL, 8(6):821–852, 2000.
[181] R.S. Michalski. Pattern recognition as rule-guided inference. Pattern Analysis and Machine
Intelligence, 2(4):349–361, 1980.
[182] R.S. Michalski. Learning strategies and automated knowledge acquisition: An overview. In
L. Bolc, editor, Computational Models of Learning, pages 1–19. Springer, 1987.
[183] R.S. Michalski, I. Mozetic, J. Hong, and N. Lavrac. The multi-purpose incremental learning
system AQ15 and its testing application to three medical domains. In Proceedings of the
National Conference on Artiﬁcial Intelligence, AAAI-86, volume 2, pages 1041–1045, 1986.
[184] T.M. Mitchell. Machine Learning. McGraw-Hill, 1997.
[185] T.M. Mitchell and S.B. Thrun.
Explanation-based learning: A comparison of symbolic
and neural network approaches. In Tenth International Conference on Machine Learning,
Amherst, MA, 1993.
[186] R.J. Mooney and D. Ourston.
A multistrategy approach to theory reﬁnement.
In R.S.
Michalski and G. Teccuci, editors, Machine Learning: A Multistrategy Approach, volume 4,
pages 141–164. Morgan Kaufmann, San Mateo, CA, 1994.
[187] R.J. Mooney and J.M. Zelle. Integrating ILP and EBL. SIGART Bulletin, 5:12–21. 1994.
[188] R.C. Moore. Semantical considerations on nonmonotonic logic. Artiﬁcial Intelligence,
25(1):75–94, 1985.
[189] S.H. Muggleton and L. De Raedt.
Inductive logic programming: Theory and methods.
Journal of Logic Programming, 19:629–679, 1994.
[190] I. Niemel¨a. Logic programs with stable model semantics as a constraint programming par-
adigm. Annals of Mathematics and Artiﬁcial Intelligence, 25:241–273, 1999.
[191] M.O. Noordewier, G.G. Towell, and J.W. Shavlik. Training knowledge-based neural net-
works to recognize genes in DNA sequences. In Advances in Neural Information Processing
Systems 3, Proceedings of NIPS 1990, pages 530–536, 1991.
[192] H. Nunez, C. Angulo, and A. Catala. Rule based learning systems for support vector ma-
chines. Neural Processing Letters, 24(1):1–18, 2006.
[193] D. Nute. Defeasible reasoning. In Proceedings of the Hawaii International Conference on
Systems Science, pages 470–477. IEEE Press, 1987.
[194] D. Nute. Defeasible logic. In D.M. Gabbay, C.J. Hogger, and J.A. Robinson, editors, Hand-
book of Logic in Artiﬁcial Intelligence and Logic Programming, volume 3, pages 353–396.
Oxford University Press, 1994.
[195] D.W. Opitz.
An Anytime Approach to Connectionist Theory Reﬁnement: Reﬁning the
Topologies of Knowledge-Based Neural Networks. PhD thesis, University of Wisconsin,
Madison, 1995.
[196] D.W. Opitz and J.W. Shavlik. Heuristically expanding knowledge-based neural networks.
In Proceedings of the International Joint Conference on Artiﬁcial Intelligence, IJCAI-93,
pages 1360–1365, 1993.
[197] M.A. Orgun and W. Ma.
An overview of temporal and modal logic programming.
In
Proceedings of the International Conference on Temporal Logic, ICTL’94, volume 827 of
Lecture Notes in Artiﬁcial Intelligence, pages 445–479. Springer, 1994.
[198] M.A. Orgun and W.W. Wadge. Towards a uniﬁed theory of intensional logic programming.
Journal of Logic Programming, 13(4):413–440, 1992.
[199] M.A. Orgun and W.W. Wadge. Extending temporal logic programming with choice predi-
cates non-determinism. Journal of Logic and Computation, 4(6):877–903, 1994.
[200] M. Page. Connectionist modelling in psychology: A localist manifesto. Behavioral and
Brain Sciences, 23:443–467, 2000.
[201] M. Pazzani and D. Kibler. The utility of knowledge in inductive learning. Machine Learn-
ing, 9:57–94, 1992.
[202] J. Pearl. Causality: Models, Reasoning and Inference. Cambridge University Press, 2000.

190
References
[203] F. Pfenning and H.-C. Wong. On a modal lambda calculus for S4. Electronic Notes in
Theoretical Computer Science, 1:515–534, 1995.
[204] G. Pinkas. Symmetric neural networks and propositional logic satisﬁability. Neural Com-
putation, 3(2):282–291, 1991.
[205] G. Pinkas. Reasoning, nonmonotonicity and learning in connectionist networks that capture
propositional knowledge. Artiﬁcial Intelligence, 77:203–247, 1995.
[206] G.D. Plotkin and C.P. Stirling. A framework for intuitionistic modal logic. In Proceedings
of the First Conference on Theoretical Aspects of Reasoning About Knowledge, TARK’86,
pages 399–406. Morgan Kaufmann, 1986.
[207] A. Pnueli. The temporal logic of programs. In Proceedings of 18th IEEE Annual Symposium
on Foundations of Computer Science, pages 46–57, 1977.
[208] J.B. Pollack. Recursive distributed representations. Artiﬁcial Intelligence, 46(1):77–105,
1990.
[209] J. Pollock. Defeasible reasoning. Cognitive Science, 11:481–518, 1987.
[210] J. Pollock. Self-defeating arguments. Minds and Machines, 1(4):367–392, 1991.
[211] H. Prakken and G. Sartor. Argument-based extended logic programming with defeasible
priorities. Journal of Applied Non-Classical Logics, 7:25–75, 1997.
[212] H. Prakken and G.A.W. Vreeswijk. Logical systems for defeasible argumentation. In D.M.
Gabbay and F. Guenthner, editors, Handbook of Philosophical Logic, 2nd edition. Kluwer,
2000.
[213] F.P. Preparata and R.T. Yeh. Introduction to Discrete Structures. Addison-Wesley, 1973.
[214] G. Priest. An Introduction to Non-Classical Logic. Cambridge University Press, Cambridge,
2001.
[215] T.C. Przymusinski.
On the declarative semantics of logic programs with negation.
In
J. Minker, editor, Foundations of Deductive Databases and Logic Programming, pages
193–216. Morgan Kaufmann, 1988.
[216] J.R. Quinlan. Induction of decision trees. Machine Learning, 1:81–106, 1986.
[217] R. Ramanujam. Semantics of distributed deﬁnite clause programs. Theoretical Computer
Science, 68:203–220, 1989.
[218] D.A. Randell, Z. Cui, and A.G. Cohn. A spatial logic based on regions and connection. In
Proccedings of the Third International Conference on Principles of Knowledge Represen-
tation and Reasoning, KR-92, pages 165–176, 1992.
[219] A.S. Rao and M.P. Georgeff. Decision procedures for BDI logics. Journal of Logic and
Computation, 8(3):293–343, 1998.
[220] R. Reiter. A logic for default reasoning. Artiﬁcial Intelligence, 13:81–132, 1980.
[221] O.T. Rodrigues. A Methodology for Iterated Information Change. PhD thesis, Department
of Computing, Imperial College, London, 1997.
[222] D. Roth. Learning to reason: The non-monotonic case. In Proceedings of the 14th Interna-
tional Joint Conference on Artiﬁcial Intelligence, IJCAI-95, pages 1178–1184, 1995.
[223] D. Roth and W. Yih. Relational learning via propositional algorithms: An information ex-
traction case study. In Proceedings of the International Joint Conference on Artiﬁcial Intel-
ligence, IJCAI-01, pages 1257–1263, Seattle, August 2001.
[224] D.E. Rumelhart, G.E. Hinton, and R.J. Williams. Learning internal representations by error
propagation. In D.E. Rumelhart and J.L. McClelland, editors, Parallel Distributed Process-
ing: Explorations in the Microstructure of Cognition, volume 1, pages 318–362. MIT Press,
1986.
[225] S. Russell. Machine learning. In M.A. Boden, editor, Handbook of Perception and Cogni-
tion, Artiﬁcial Intelligence Series, chapter 4. Academic Press, 1996.
[226] Y. Sakakibara. Programming in modal logic: An extension of PROLOG based on modal
logic. In Logic Programming 86, volume 264 of Lecture Notes in Computer Science, pages
81–91. Springer, 1986.
[227] R. Setiono. Extracting rules from neural networks by pruning and hidden-unit splitting.
Neural Computation, 9:205–225, 1997.
[228] L. Shastri. A connectionist approach to knowledge representation and limited inference.
Cognitive Science, 12(13):331–392, 1988.

References
191
[229] L. Shastri. Advances in SHRUTI: A neurally motivated model of relational knowledge
representation and rapid inference using temporal synchrony. Applied Intelligence Journal,
Special Issue on Neural Networks and Structured Knowledge, 11:79–108, 1999.
[230] L. Shastri and V. Ajjanagadde. From simple associations to semantic reasoning: A connec-
tionist representation of rules, variables and dynamic binding. Technical report, University
of Pennsylvania, 1990.
[231] J.W. Shavlik. An overview of research at Wisconsin on knowledge-based neural networks.
In Proceedings of the International Conference on Neural Networks, ICNN96, pages 65–69,
Washington, DC, 1996.
[232] J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge Uni-
versity Press, 2004.
[233] Y. Shin and J. Ghosh. The pi–sigma network: An efﬁcient higher-order neural network for
pattern classiﬁcation and function approximation. In Proceedings of the International Joint
Conference on Neural Networks, IJCNN91, volume 1, pages 13–18, Seattle, July 1991.
[234] H.T. Siegelmann. Neural and super-Turing computing. Minds and Machines, 13(1):103–
114, 2003.
[235] H.T. Siegelmann and E.D. Sontag. On the computational power of neural nets. Journal of
Computer and System Sciences, 50(1):132–150, 1995.
[236] A. Simpson. The Proof Theory and Semantics of Intuitionistic Modal Logics. PhD thesis,
Edinburgh University, 1993.
[237] P. Smolensky. On the proper treatment of connectionism. Behavioral and Brain Sciences,
44:1–74, 1988.
[238] P. Smolensky. Tensor product variable binding and the representation of symbolic structures
in connectionist networks. Artiﬁcial Intelligence, 46:159–216, 1990.
[239] P. Smolensky. Grammar-based connectionist approaches to language. Cognitive Science,
23(4):589–613, 1999.
[240] P. Smolensky and G. Legendre.
The Harmonic Mind: From Neural Computation to
Optimality-Theoretic Grammar. MIT Press, Cambridge, MA, 2006.
[241] K. Stenning and M. van Lambalgen. Human reasoning and cognitive science, MIT Press,
Cambridge, MA, 2008.
[242] R. Sun. Robust reasoning: Integrating rule-based and similarity-based reasoning. Artiﬁcial
Intelligence, 75(2):241–296, 1995.
[243] R. Sun and F. Alexandre. Connectionist Symbolic Integration. Lawrence Erlbaum Asso-
ciates, Hillsdale, NJ, 1997.
[244] S.B. Thrun. Extracting provably correct rules from artiﬁcial neural networks. Technical
report, Institut f¨ur Informatik, Universit¨at Bonn, 1994.
[245] S.B. Thrun. Explanation-Based Neural Network Learning: A Lifelong Learning Approach.
Kluwer Academic, Boston, MA, 1996.
[246] S.B. Thrun, J. Bala, E. Bloedorn, I. Bratko, B. Cestnik, J. Cheng, K. De Jong, S. Dzeroski,
S.E. Fahlman, D. Fisher, R. Haumann, K. Kaufman, S. Keller, I. Kononenko, J. Kreuziger,
R.S. Michalski, T.M. Mitchell, P. Pachowicz, Y. Reich, H. Vafaie, K. Van de Welde,
W. Wenzel, J. Wnek, and J. Zhang.
The MONK’s problems: A performance compari-
son of different learning algorithms. Technical Report CMU-CS-91-197, Carnegie Mellon
University, 1991.
[247] D. Touretzky and G.E. Hinton. Symbols among neurons. In Proceedings of the International
Joint Conference on Artiﬁcial Intelligence, IJCAI-85, pages 238–243. Morgan Kaufmann,
1985.
[248] D. Touretzky and G.E. Hinton. A distributed connectionist production system. Cognitive
Science, 12(3):423–466, 1988.
[249] G.G. Towell and J.W. Shavlik. Using symbolic learning to improve knowledge-based neural
networks. In Proceedings of the National Conference on Artiﬁcial Intelligence, AAAI-92,
pages 177–182, 1992.
[250] G.G. Towell and J.W. Shavlik. Knowledge-based artiﬁcial neural networks. Artiﬁcial Intel-
ligence, 70(1):119–165, 1994.

192
References
[251] A.M. Turing. Computer machinery and intelligence. Mind, 59:433–460, 1950.
[252] W. Uwents and H. Blockeel. Classifying relational data with neural networks. In S. Kramer
and B. Pfahringer, editors, Proceedings of 15th International Conference on Inductive
Logic Programming, volume 3625 of Lecture Notes in Computer Science, pages 384–396.
Springer, 2005.
[253] L.G. Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134–1142,
1984.
[254] L.G. Valiant. Robust logics. Artiﬁcial Intelligence, 117:231–253, 2000.
[255] L.G. Valiant. Three problems in computer science. Journal of the ACM, 50(1):96–99, 2003.
[256] J. van Benthem. Modal Logic and Classical Logic. Bibliopolis, Naples, 1983.
[257] J. van Benthem. Correspondence theory. In D.M. Gabbay and F. Guenthner, editors, Hand-
book of Philosophical Logic, chapter II.4, pages 167–247. Reidel, Dordrecht, 1984.
[258] D. van Dalen. Intuitionistic logic. In D.M. Gabbay and F. Guenthner, editors, Handbook of
Philosophical Logic, 2nd edition, volume 5. Kluwer, 2002.
[259] F. van der Velde and M. de Kamps. Neural blackboard architectures of combinatorial struc-
tures in cognition. Behavioral and Brain Sciences, 29(1):37–70, 2006.
[260] M.H. van Emden and R.A. Kowalski. The semantics of predicate logic as a programming
language. Journal of the ACM, 23(4):733–742, 1976.
[261] A. van Gelder, K. Ross, and J. Schlipf. The well-founded semantics for general logic pro-
grams. Journal of the ACM, 38(3):620–650, 1991.
[262] V. Vapnik. The Nature of Statistical Learning Theory. Springer, 1995.
[263] V. Vapnik and A. Chervonenkis. On the uniform convergence of relative frequencies of
events to their probabilities. Theory of Probability and Its Applications, 16(2), 264–280,
SIAM, 1971.
[264] M.Y. Vardi. Why is modal logic so robustly decidable. In N. Immerman and P. Kolaitis,
editors, Descriptive Complexity and Finite Models, volume 31 of Discrete Mathematics and
Theoretical Computer Science, pages 149–184. DIMACS, 1997.
[265] B. Verheij. Accrual of arguments in defeasible argumentation. In Proceedings of Second
Dutch/German Workshop on Nonmonotonic Reasoning, pages 217–224, Utrecht, 1995.
[266] B. Verheij. Rules, Reasons, Arguments: Formal Studies of Argumentation and Defeat. PhD
thesis, Maastricht University, The Netherlands, 1996.
[267] G.A.W. Vreeswijk. Abstract argumentation systems. Artiﬁcial Intelligence, 90(1–2):225–
279, 1997.
[268] P.J. Werbos. Beyond Regression: New Tools for Prediction and Analysis in the Behavioral
Sciences. PhD thesis, Harvard University, Cambridge, MA, 1974.
[269] P.J. Werbos. Backpropagation through time: What does it mean and how to do it. Proceed-
ings of the IEEE, 78:1550–1560, 1990.
[270] J. Williamson and D.M. Gabbay. Recursive causality in Bayesian networks and self-ﬁbring
networks.
In D. Gillies, editor, Laws and Models in Science. King’s College London,
173–221, 2004.
[271] M.J. Wooldridge. Introduction to Multi-Agent Systems. Wiley, New York, 2001.
[272] Y. Xiong, W. Wu, X. Kang, and C. Zhang. Training pi–sigma network by online gradient
algorithm with penalty for small weight update. Neural Computation, 19(12):3356–3368,
2007.
[273] Z.H. Zhou, Y. Jiang, and S.F. Chen. Extracting symbolic rules from trained neural network
ensembles. AI Communications, 16(1):3–15, 2003.

Index
abduction, 169
abductive reasoning, 141
Abramsky, S., ix
accrual (cumulative) argumentation, 155
agent
common knowledge, 69
individual knowledge, 69
knowledge about the world, 15
representation of knowledge, 69
Alchourr´on, C.E., 16
alternatives to classical logic, 9
AND/OR neurons, 35
ANNN (artiﬁcial nonmonotonic neural
networks), 53
answer set, 45
semantics, 46
Apt, K.R., 20
argument computation, 149, 151
argument learning, 149, 154
argumentation and probabilities, 159
argumentation framework, 143
argumentation network, 144, 145
argumentation neural network, 145
arguments
acceptability, 144
colouring, 144
conﬂicting sets, 151
indefensible, 144
prevailing, 144
relative strength, 144, 145
arity, 12
artiﬁcial intelligence, integration
of symbolic and connectionist
paradigms, 1
Art¨emov, S.N., 108
assignment, 12
background knowledge, 171
backpropagation learning algorithm, 26, 45,
172
Bader, S., ix
Balkenius, C., 53
Barringer, H., ix, 150
Bayesian networks, 157
belief revision, 16
Bench-Capon, T.J.M., 144
Bennett, B., 3
biologically motivated models, 2
Bol, R.N., 20
brain imaging, 179
Braz, R.S., ix
Brewka, G., 51
Broda, K., ix
Brouwer, L.E.J., 85, 88
Bruck, J., ix
chain, 10
CHCL (Connectionist Horn Clause Logic), 53
CIL (connectionist intuitionistic logic), 101,
109
CILP
applications, 52
classical negation, 45
theory reﬁnement, 44
Translation Algorithm, 39
circular argumentation, 150
circularity in argumentation networks, 149
classical logic, 9, 11
classical negation, 45
classical propositional logic, 11
clause
body, 17
general, 19
head, 17
193

194
Index
closed-world assumption, 16
CML (connectionist modal logic), 55, 60, 174
distributed knowledge representation, 69
learning in CML, 71
cognitive abilities, 15
cognitive models, 1
commonsense reasoning, 14, 16
computational cognitive models, 1
conditional reasoning, 14
conﬂicting sets of arguments, 151
Connectionist Intuitionistic Algorithm, 93
connectionist intuitionistic implication, 90
connectionist intuitionistic logic, 101, 108
Connectionist Intuitionistic Modal Algorithm,
97
connectionist intuitionistic negation, 91
Connectionist Modal Algorithm, 62
connectionist muddy children puzzle, 68
connectionist nonclassical logic, 4
connectionist nonclassical reasoning, 173
connectionist paradigm, 1
Connectionist Temporal Algorithm, 80
connectionist temporal logic, 78
connectionist temporal reasoning, 175
connectionist wise men puzzle, 104, 109
constant symbols, 12
constructive mathematics, 88
context units, 30
Cook, M., ix
cross-validation, 32
leaving-one-out, 33
CTLK (Connectionist Temporal Logic of
Knowledge), 4, 75, 175
language, 77
learning, 83
DasGupta, B., 44
De Raedt, L., ix
deduction, 169
deductive reasoning, 43
defeasible arguments, 143
defeasible logic, 50
deﬁnite logic program, 17
distributed knowledge representation in CML,
69
distributed representation, 172
Dung, P., 144
east–west trains example, 135
EBL (explanation-based learning), 53
EBNN (explanation-based neural networks),
53
Eiter, T., 51
Elman, J.L., 29, 31
epistemic entrenchment, 51
epoch, 32
explicit negation, 45
extended logic programs, 45
extraction methods, 140
ﬁbred network ensembles, viii, 4, 178
ﬁbred neural network, 115, 122, 176, 177
ﬁbring
application to argumentation networks,
157
applied to argumentation, 157
conditionals, 117
ﬁbring modal logic into many-valued logic,
117
ﬁbring function, 5, 116, 122, 177
ﬁrst-order logic, 12
ﬁrst-order logic programs, 176
Fitting, M., 59
ﬁxed-point characterisation of the least
Herbrand model, 18
formal models of argumentation, 143
Fu, L.M., 54
function symbols, 12
G¨ardenfors, P., 16, 53
Gabbay, D.M., 16, 59, 150, 159
Garcez, C., ix
Gelfond, M., 19
Gelfond–Lifschitz reduction, 46
Gelfond–Lifschitz transformation, 20
general logic program, 19
generalised delta rule, 27
Givan, R., ix
goal-directed reasoning, 140
Gori, M., ix
greatest ﬁxed point, 18
greatest lower bound, 10
H¨olldobler, S., ix, 36
Halpern, J.Y., ix, 1, 161
Herbrand base, 17
Herbrand model, 17
Herbrand, J., 17
Heyting, A., 88
Hintikka, J., 15, 56
Hinton, G.E., 27
Hitzler, P., ix
Hodges, W., ix
Hodkinson, I.M., ix
Hornik, K., 44
human reasoning, 179
hybrid system, 171

Index
195
ILP (inductive logic programming), 53, 127
immediate-consequence operator, 18
immediate predecessor, 10
immediate successor, 10
inconsistency, 172
inconsistency handling, 51
induction, 169
inductive learning, 32, 44, 171
inductive logic programming, 53, 127
integration of symbolic and connectionist
paradigms, 1
intelligent behaviour, 1
interpretation, 12
intuitionistic implication, 89
intuitionistic language, 88
intuitionistic logic, 3, 88
intuitionistic logical systems, 87
intuitionistic modal consequence operator, 96
intuitionistic modal logic, 95
intuitionistic negation, 89
Jordan, M.I., 29
Kalinke, Y., 36
KBANN (Knowledge-Based Artiﬁcial Neural
Networks), 35
KBCNN (Knowledge-Based Conceptual
Neural Network), 54
Knowledge and Probability Translation
Algorithm, 165
knowledge evolution, 74, 75, 77
knowledge extraction, 140, 171
knowledge representation, 21
Kolmogorov, A.N., 88
Kowalski, R.A., ix, 89
Kripke models, 57
Kripke, S., 15, 56
Kuehnberger, K.-U., ix
labelled formula, 58
labelled intuitionistic modal program, 95
labelled intuitionistic program, 88
lattice, 10
complete, 10
distributive, 10
LDS (labelled deductive systems), 57, 157
learning algorithm, 171
learning and reasoning cycle, 172
learning capability, 28
learning in CML, 71
learning in CTLK, 83
learning with and without background
knowledge, 72
least ﬁxed point, 18
least upper bound, 10
Lifschitz, V., 19, 45
linear order, 10
localist representation, 172
locally stratiﬁed program, 19
logic
alternatives to classical logic, 9
classical logic, 9, 11
classical propositional logic, 11
ﬁrst-order logic, 12
intuitionistic logic, 2, 88
intuitionistic implication, 89
intuitionistic negation, 89
intuitionistic modal logic, 95
logic as the calculus of computer science, 14
modal logic, 3, 55, 56
modalities, 57
possible-world semantics, 56
proof procedures for modal logic, 58
nonclassical logics, viii, 9, 14, 170
predicate logic, see predicate logic
propositional logic, expressive power, 11
role of logic in computer science, vii, 14
temporal logic, 3, 76
next-time operator, 76
two-variable fragment of ﬁrst-order logic, 6
logic programming, 17
logical consequence, 171
long-term memory, 113
lower bound, 10
Makinson, D.C., ix, 16
McCarthy, J., 2, 16, 45, 127, 171
merging of temporal background knowledge
and data learning, 84
metalevel, 178
metalevel network, 128
metalevel priority, 49
metanetwork, see Metalevel network
metric space, 11
Michalski, R.S., ix, 32, 135
modal immediate-consequence operator, 59
modal logic, 3, 55, 56
modal logic programming, 55
modal reasoning, 58, 173
model intersection property, 17
Modgil, S., ix
Monty Hall puzzle, 166
Moore, R.C., 16
moral-debate example, 145
neural network for, 149
muddy children puzzle, 68
complete solution, 81
snapshot solution, 81

196
Index
Muggleton, S.H., ix
multiagent system, 3
natural-deduction-style rules, 58
negation as failure, 16
network ensemble, 31
Neural Argumentation Algorithm, 147
neural computation, 169
Neural network, learning process of, 26
neural-symbolic integration, 170
objectives, viii
neural-symbolic learning systems, 170
neural-symbolic systems, 179
neuron functionality, 23
activation function, 24
activation rule, 23
activation state, 23
bipolar function, 25
feedforward network, 24
fully connected network, 24
hidden-layer network, 24
learning rule, 24
output rule, 23
propagation rule, 23
set of weights, 23
sigmoid function, 24
threshold, 24
universal approximator, 25
Nixon diamond problem, 50, 154
nonclassical logics, viii, 9, 14, 170
nonclassical reasoning, 2
nonmonotonic reasoning, 16
normal logic program, 17
Nute, D., 50
Ockham’s (Occam’s) razor, 32
Palade, V., ix
Palm, G., ix
paraconsistency, 48
partial order, 10
partially ordered set, 10
penalty logic, 53
Pinkas, G., 53
Pnueli, A., 3
Pollock, J., 155
possible worlds, 173
possible-world semantics, 15, 56
practical reasoning, 16
Prakken, H., 51
predicate logic
domain, 13
interpretation, 13
model, 13
semantics, 14
symbols, 12
syntax, 12
preference handling, 51
preference relation, 50
probabilistic relational model, 129
probabilities and argumentation, 159
probabilities and possible worlds, 161
probability, 157
processing units of neural network, 179
propositional ﬁxation, 2
propositional logic, expressive power, 11
quantiﬁers, 12
Quinlan, J.R., 32
R¨uger, S., ix
Ray, O., ix
recurrent architecture, 29
recurrent backpropagation, 29
recurrent network, 172
recursion in Bayesian networks, 159
recursive causality, 159
recursive neural network, 177
Reiter, R., 16
relational knowledge, 171
relational learning, 128
relationship between argumentation and neural
networks, 144
robust learning, 179
Rodrigues, O.T., ix
Roth, D., ix
Rumelhart, R.E., 27
Russo, A., ix
Sartor, G., 51
Schnitger, G., 44
semantics for extended modal logic programs,
58
set of terms, 13
Shastri, L., ix
Shavlik, J.W., ix, 35, 36
short-term memory, 113
Siekmann, J.H., ix
Smolensky, P., 171
stable model, 20
Stinchcombe, M., 44
Sun, R., ix
superiority relation, 50
supervised learning, 26
support vector machine, 129
supported interpretation, 19
symbolic logic, 169
symbolic paradigm, 1

Index
197
symmetric network, 29
synergies of neural-symbolic
integration, viii
Tamburrini, G., ix
Taylor, J.G., ix
Temporal Algorithm, 75
temporal immediate-consequence
operator, 79
temporal knowledge representation, 81
temporal logic, 3, 76
temporal logic programming, 77
temporal model, 78
temporal muddy children puzzle, 81
temporal reasoning, 3, 15, 173, 175
term mapping, 13
test set performance, 32
testing set, 28
timeline, 78
Toni, F., ix
Towell, G.G., 35, 36
training set, 28
training-set performance, 32
translation algorithm, 171
two-coin problem, 161
uncertainty, 157
uncertainty, reasoning about, 161
uncertainty, representation of, 161
upper bound, 10
Valiant, L.G., 1, 84
value-based argumentation framework, 144
van Benthem, J.F.A.K., ix, 56
van Emden, M., 89
Vardi, M.Y., ix, 56
variable assignment, 13
variables, 12, 176
Verheij, B., 155
well-formed formula (wff), 11, 13
well-founded semantics, 20
Werbos, P.J., 27
Wermter, S., ix
White, H., 44
Williams, R.J., 27
Williamson, J., 159
wise men puzzle, 102, 108
Woods, J., ix, 150
Zaverucha, G., ix

