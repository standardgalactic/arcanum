
Springer Tracts in Advanced Robotics
Volume 75
Editors: Bruno Siciliano · Oussama Khatib · Frans Groen


Javier Civera, Andrew J. Davison,
and José María Martínez Montiel
Structure from Motion
Using the Extended Kalman
Filter
ABC

Professor Bruno Siciliano, Dipartimento di Informatica e Sistemistica, Università di Napoli Federico II,
Via Claudio 21, 80125 Napoli, Italy, E-mail: siciliano@unina.it
Professor Oussama Khatib, Artiﬁcial Intelligence Laboratory, Department of Computer Science,
Stanford University, Stanford, CA 94305-9010, USA, E-mail: khatib@cs.stanford.edu
Professor Frans Groen, Department of Computer Science, Universiteit van Amsterdam, Kruislaan 403,
1098 SJ Amsterdam, The Netherlands, E-mail: groen@science.uva.nl
Authors
Dr. Javier Civera
Universidad de Zaragoza
Instituto Universitario de Investigación en
Ingeniería de Aragón (I3A)
Ediﬁcio Ada Byron
María de Luna, 1, 50018 Zaragoza, Spain
E-mail: jcivera@unizar.es
Dr. José María Martínez Montiel
Universidad de Zaragoza
Instituto Universitario de Investigación en
Ingeniería de Aragón (I3A)
Ediﬁcio Ada Byron
María de Luna, 1, 50018 Zaragoza, Spain
E-mail: josemari@unizar.es
Dr. Andrew J. Davison
Imperial College London
Department of Computing
180 Queen’s Gate
SW7 2AZ London, UK
E-mail: ajd@doc.ic.ac.uk
ISBN 978-3-642-24833-7
e-ISBN 978-3-642-24834-4
DOI 10.1007/978-3-642-24834-4
Springer Tracts in Advanced Robotics
ISSN 1610-7438
Library of Congress Control Number: 2011940008
c⃝2012 Springer-Verlag Berlin Heidelberg
This work is subject to copyright. All rights are reserved, whether the whole or part of the material is
concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting,
reproduction on microﬁlm or in any other way, and storage in data banks. Duplication of this publication
or parts thereof is permitted only under the provisions of the German Copyright Law of September 9,
1965, in its current version, and permission for use must always be obtained from Springer. Violations
are liable for prosecution under the German Copyright Law.
The use of general descriptive names, registered names, trademarks, etc. in this publication does not
imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant protective
laws and regulations and therefore free for general use.
Typeset & Cover Design: Scientiﬁc Publishing Services Pvt. Ltd., Chennai, India.
Printed on acid-free paper
5 4 3 2 1 0
springer.com

Editorial Advisory Board
Oliver Brock, TU Berlin, Germany
Herman Bruyninckx, KU Leuven, Belgium
Raja Chatila, LAAS, France
Henrik Christensen, Georgia Tech, USA
Peter Corke, Queensland Univ. Technology, Australia
Paolo Dario, Scuola S. Anna Pisa, Italy
Rüdiger Dillmann, Univ. Karlsruhe, Germany
Ken Goldberg, UC Berkeley, USA
John Hollerbach, Univ. Utah, USA
Makoto Kaneko, Osaka Univ., Japan
Lydia Kavraki, Rice Univ., USA
Vijay Kumar, Univ. Pennsylvania, USA
Sukhan Lee, Sungkyunkwan Univ., Korea
Frank Park, Seoul National Univ., Korea
Tim Salcudean, Univ. British Columbia, Canada
Roland Siegwart, ETH Zurich, Switzerland
Gaurav Sukhatme, Univ. Southern California, USA
Sebastian Thrun, Stanford Univ., USA
Yangsheng Xu, Chinese Univ. Hong Kong, PRC
Shin’ichi Yuta, Tsukuba Univ., Japan
STAR (Springer Tracts in Advanced Robotics) has been promoted un-
der the auspices of EURON (European Robotics Research Network)
ROBOTICS
Research
Network
European
EURON
*
*
*
*
*
*
*
*
*
*
*
*


To Mar´ıa, Mar´ıa ´Angeles, Pedro Jos´e and
Sergio.
To Lourdes, Rafael, Blanca and Adela.
To Jos´e Mar´ıa and Olga. To Virginia, Elena
and Mikel


Preface
The book you have in your hands is the result of the joint research of the au-
thors from 2006 to 2009, that led to Javier Civera’s PhD thesis. The contribu-
tions on it were ﬁrst published in several international conferences and journals;
speciﬁcally and in chronological order [Montiel et al. 2006, Civera et al. 2007a,
Civera et al. 2007b, Civera et al. 2008a, Civera et al. 2008b, Civera et al. 2009b,
Civera et al. 2009a, Civera et al. 2009c, Civera et al. 2010]. This book provides a
uniﬁed view of the research spread on those papers, presenting a complete and ro-
bust system aimed to sequential 3D vision in real-time.
Along the years, many people have contributed in one or another manner to the
results presented here. First of all, we are very grateful to our coauthors in the topics
related with this book ´Oscar G. Grasa, Diana R. Bueno and Juan A. Magall´on. Gabe
Sibley and Joan Sol`a reviewed the ﬁrst version of the PhD thesis manuscript. The
jury of Javier Civera’s PhD defence; Wolfram Burgard, J. D. Tard´os, Patric Jensfelt,
Miguel ´Angel Salichs and V´ıctor Mu˜noz, contributed to this ﬁnal version with very
insightful comments and annotations. The expert eyes of Antonio Agudo, ´Oscar G.
Grasa, Lina Paz y Pedro Pini´es also reviewed the ﬁnal version of this book and con-
tibuted to polish some discussions and to ﬁnd the latest and more difﬁcult bugs. The
authors are also extremely grateful to their respective research groups: the Robot
Vision Group at Imperial College in London and the Robotics, Perception and Real-
Time Group in the I3A (Arag´on Institute on Engineering Research) in the University
of Zaragoza in Spain. Hundreds of relevant conversations, the stimulating research
atmosphere and the dedication at both groups are fertile breeding grounds that have
played an essential role in our research. An exhaustive list of names would be an
unfeasible task, but still we would like to express our gratitude to some of our col-
leagues for very interesting discussions: Jos´e Neira, Pedro Pini´es, Lina Paz, Rub´en
Mart´ınez-Cant´ın, Luis Montesano, Alejandro Mosteo, Ana Cristina Murillo, I˜naki
Ra˜no, Antonio Agudo, Margarita Chli, Hauke Strasdat, Ian Reid, Brian Williams
and Cyrill Stachniss. Last but not least, we are in debt with our respective families,
for their constant support.
The research results in this book were ﬁnancially supported by the following
sources: The Spanish Goverment research grants DPI2003-07986, DPI2006-13578,

X
Preface
DPI2009-07130 and PR2007-0427; regional grant DGA(CONAI+D)-CAI IT12-06;
the European Union projects RAWSEEDS FP6-045144 and RoboEarth FP7-248942;
and A. J. Davison’s ERC Starting Grant 210346.
Zaragoza and London
Javier Civera
July 2011
Andrew J. Davison
J.M.M. Montiel

Contents
1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.1
Structure from Motion and Monocular SLAM. . . . . . . . . . . . . . . . . . .
3
1.1.1
Structure from Motion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
1.1.2
Monocular SLAM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
1.1.3
Structure of the Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
1.2
Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8
1.3
Outline of the Book . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
2
Points at Inﬁnity: Mosaics Using the Extended Kalman Filter . . . . . . . 13
2.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
2.1.1
Points at Inﬁnity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
2.1.2
Real-Time EKF-Based Mosaicing . . . . . . . . . . . . . . . . . . . . . . 14
2.2
Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
2.2.1
SLAM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
2.2.2
Off-Line SFM vs. EKF SLAM . . . . . . . . . . . . . . . . . . . . . . . . . 16
2.2.3
Image Mosaicing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
2.3
Geometrical Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
2.3.1
Feature Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
2.3.2
Camera Motion Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
2.3.3
Measurement Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
2.4
Simultaneous Localization and Mapping . . . . . . . . . . . . . . . . . . . . . . . 21
2.4.1
Matching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
2.4.2
State Initialization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
2.4.3
Feature Initialization and Deletion . . . . . . . . . . . . . . . . . . . . . . 23
2.5
Meshing and Mosaicing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
2.5.1
Updating the Mesh . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
2.5.2
Tile Texture Mapping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
2.6
Experimental Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
2.6.1
360◦Pan and Cyclotorsion . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
2.6.2
Real-Time Mosaic Building . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
2.6.3
Processing Time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
2.7
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31

XII
Contents
3
Inverse Depth Parametrization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
3.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
3.1.1
Delayed and Undelayed Initialization . . . . . . . . . . . . . . . . . . . 34
3.1.2
Points at Inﬁnity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
3.1.3
Inverse Depth Representation . . . . . . . . . . . . . . . . . . . . . . . . . . 36
3.1.4
Inverse Depth in Computer Vision and Tracking . . . . . . . . . . 37
3.2
State Vector Deﬁnition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
3.2.1
Camera Motion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
3.2.2
Euclidean XYZ Point Parametrization . . . . . . . . . . . . . . . . . . . 39
3.2.3
Inverse Depth Point Parametrization . . . . . . . . . . . . . . . . . . . . 39
3.2.4
Full State Vector . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
3.3
Measurement Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
3.4
Measurement Equation Linearity. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
3.4.1
Linearized Propagation of a Gaussian . . . . . . . . . . . . . . . . . . . 42
3.4.2
Linearity of XYZ Parametrization . . . . . . . . . . . . . . . . . . . . . . 43
3.4.3
Linearity of Inverse Depth Parametrization . . . . . . . . . . . . . . . 44
3.4.4
Depth vs. Inverse Depth Comparison . . . . . . . . . . . . . . . . . . . . 44
3.5
Feature Initialization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
3.6
Switching from Inverse Depth to XYZ . . . . . . . . . . . . . . . . . . . . . . . . . 47
3.6.1
Conversion from Inverse Depth to XYZ Coding . . . . . . . . . . . 47
3.6.2
Linearity Index Threshold . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
3.7
Data Association . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
3.7.1
Patch Warping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
3.7.2
Active Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
3.8
Experimental Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
3.8.1
Indoor Sequence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
3.8.2
Real-Time Outdoor Sequence . . . . . . . . . . . . . . . . . . . . . . . . . . 53
3.8.3
Loop Closing Sequence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
3.8.4
Simulation Analysis for Inverse Depth to XYZ
Switching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
3.8.5
Parametrization Switching with Real Images . . . . . . . . . . . . . 60
3.8.6
Processing Time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
3.9
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
4
1-Point RANSAC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
4.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
4.2
Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68
4.2.1
Random Sample Consensus (RANSAC) . . . . . . . . . . . . . . . . . 68
4.2.2
Joint Compatibility Branch and Bound (JCBB) . . . . . . . . . . . 69
4.2.3
Structure from Motion and Visual Odometry . . . . . . . . . . . . . 71
4.2.4
Benchmarking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
4.3
1-Point RANSAC Extended Kalman Filter Algorithm . . . . . . . . . . . . 72
4.3.1
EKF Prediction and Search for Individually Compatible
Matches (lines 5–8) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72

Contents
XIII
4.3.2
1-Point Hypotheses Generation and Evaluation
(lines 9–22) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74
4.3.3
Partial Update with Low-Innovation Inliers (lines 23–24) . . . 75
4.3.4
Partial Update with High-Innovation Inliers (lines 25–35) . . 75
4.4
1-Point RANSAC Extended Kalman Filter from a Monocular
Sequence Input . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76
4.4.1
State Vector Deﬁnition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76
4.4.2
Dynamic Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77
4.4.3
Camera-Centered Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . 77
4.5
Experimental Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80
4.5.1
Benchmark Method for 6 DOF Camera Motion
Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80
4.5.2
1-Point RANSAC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81
4.5.3
Joint Compatibility Branch and Bound (JCBB) . . . . . . . . . . . 84
4.5.4
Trajectory Benchmarking against GPS . . . . . . . . . . . . . . . . . . 85
4.5.5
Pure Monocular EKF-Based Estimation for Long
Sequences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
4.5.6
Visual Odometry from a Monocular Sequence Plus Wheel
Odometry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90
4.6
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
5
Degenerate Camera Motions and Model Selection . . . . . . . . . . . . . . . . . 99
5.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99
5.2
Bayesian Model Selection for Sequences . . . . . . . . . . . . . . . . . . . . . . . 101
5.3
Interacting Multiple Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
5.4
Interacting Multiple Model Monocular SLAM . . . . . . . . . . . . . . . . . . 104
5.5
Experimental Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105
5.5.1
Consistent Start Up Even with Rotation . . . . . . . . . . . . . . . . . 106
5.5.2
Low Risk of Spurious Matches due to Small Search
Regions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106
5.5.3
Camera Motion Model Identiﬁcation . . . . . . . . . . . . . . . . . . . . 109
5.5.4
Computational Cost Considerations . . . . . . . . . . . . . . . . . . . . . 110
5.6
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
6
Self-calibration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
6.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
6.2
Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112
6.3
Sum of Gaussians (SOG) Filter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113
6.4
Self-calibration Using SOG Filtering . . . . . . . . . . . . . . . . . . . . . . . . . . 115
6.4.1
State Vector Deﬁnition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
6.4.2
Pruning of Gaussians with Low Weight . . . . . . . . . . . . . . . . . . 116
6.5
Experimental Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
6.5.1
Indoor Sequence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118
6.5.2
Loop-Closing Sequence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119
6.6
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122

XIV
Contents
7
Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
Appendix
A
Implementation Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
A.1 Extended Kalman Filter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
A.2 Calibrated EKF-Based SfM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129
A.2.1
Dynamic Model and Derivatives. . . . . . . . . . . . . . . . . . . . . . . . 129
A.2.2
Measurement Model and Derivatives . . . . . . . . . . . . . . . . . . . . 131
A.2.3
Inverse Depth Point Feature Initialization and
Derivatives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136
A.3 Uncalibrated EKF-Based SfM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139
A.3.1
Dynamic Model and Derivatives. . . . . . . . . . . . . . . . . . . . . . . . 139
A.3.2
Measurement Model and Derivatives . . . . . . . . . . . . . . . . . . . . 140
A.3.3
Inverse Depth Point Feature Initialization and
Derivatives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143
A.4 Quaternion Normalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146
A.5 Inverse Depth to Cartesian Parameterization Conversion . . . . . . . . . . 147
B
Filter Tuning Understanding via Dimensional Analysis . . . . . . . . . . . . . 149
B.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149
B.2
Monocular SLAM Estimation Process . . . . . . . . . . . . . . . . . . . . . . . . . 150
B.3
Buckingham’s Π Theorem Applied to Monocular SLAM . . . . . . . . . 151
B.4
Dimensionless Monocular SLAM Model . . . . . . . . . . . . . . . . . . . . . . . 151
B.5
Geometric Interpretation of the Dimensionless Parameters . . . . . . . . 152
B.6
Real Image Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154
B.6.1
Dependence of Scene Scale on a Priori Parameters . . . . . . . . 154
B.6.2
Image Tuning in a Pure Rotation Sequence . . . . . . . . . . . . . . . 155
B.6.3
The Same Image Tuning for Different Sequences . . . . . . . . . 156
B.7
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159

Chapter 1
Introduction
Abstract. The fully automated 3D estimation of a scene and the 6 degrees of free-
dom camera motion using as the only input the images taken by the camera has
been a long term aim in the computer vision community. The intense research in the
latest decades has produced spectacular advances; the topic is already mature and
most of its aspects are already well known. 3D vision has inmediate applications in
many different ﬁelds, like robotics or augmented reality, and technological transfer
is starting to be a reality. This book describes one of the ﬁrst systems for sparse
point-based 3D reconstruction and egomotion estimation from an image sequence;
able to work in real-time at video rate in room-sized scenarios. This chapter intro-
duces the main topics that will be detailed in the rest of the book, covering aspects
like the point feature model, efﬁcient and robust correspondences search, model
selection for degenerate conﬁgurations and internal self-calibration.
One of the most brilliant quotes attributed to Albert Einstein says that you do not
really understand something unless you can explain it to your grandmother. With
that in mind, computer vision researchers should consider themselves rather lucky
to be able to summarize the main aim of their discipline with a simple and under-
standable sentence like “making a computer see”. On the other hand, the lexical
simplicity of this objective hides a very complex reality which very often people
are tricked into. Even relevant researchers of the ﬁeld are said to have fallen into
the trap: The anecdote that Marvin Minsky, Artiﬁcial Intelligence pioneer from
MIT, assigned to solve the whole computer vision problem as a summer project
to a degree student back in the sixties is an illustrative and well-known example
[Hartley & Zisserman 2004].
The truth behind this apparent simplicity is that, although we all have a clear
experience about what “to see” implies, the biological mechanisms of visual pro-
cessing are yet not fully understood. And even if we knew it, we could also wonder
if a machine needs –or will be able to run– a visual sensing similar to ours. This
lack of a precise deﬁnition about what “to see” really means and needs have made
of computer vision a diverse and fragmented discipline.
J. Civera et al.: Structure from Motion Using the Extended Kalman Filter, STAR 75, pp. 1–12.
springerlink.com
c⃝Springer-Verlag Berlin Heidelberg 2012

2
1 Introduction
In spite of this, computer vision has experienced great advances since its appear-
ance. Computers still cannot see, but nowadays most of them are able to use visual
information in one or another sense. Also, cameras are becoming the dominant sen-
sor modality in many ﬁelds, for example in the robotics research or the industry of
videogames.
The general frame of the work is one of these uses of visual information, specif-
ically how visual information can be processed to extract a sparse tridimensional
estimation of the imaged scenario and the motion of the camera into it. Figure 1.1
shows a typical example of the estimation results in this book1. Figure 1.1(a) shows
the image k in the video sequence, along with the tracked features. Figure 1.1(b)
shows a 3D top view of the estimation results: the black triangle stands for the cur-
rent pose of the camera, the black line is the trajectory up to the current frame, and
the red ellipses represent the 3D 95% uncertainty region for the point features.
(a) Image from the input sequence and tracked
features. Ellipses in the image are the regions
where the correspondences are searched. Corre-
spondences are plotted as green crosses.
−5
−4
−3
−2
−1
0
1
2
3
4
5
−1
0
1
2
3
4
5
6
7
8
9
(b) Top view of the 3D estimation results.
The black triangle stands for the current
camera pose, and the black line is the es-
timated trajectory up to the current frame.
The estimated features are plotted as red
crosses and their uncertainty as red ellipses.
Fig. 1.1. Example illustrating the usual EKF SfM or monocular SLAM processing.
The algorithms described in this book provide a theoretical framework to perform
a sequential estimation of a camera motion and 3D scene from the only input of an
image sequence and in real-time up to 30 frames per second. The contents of the
book allow to perform such 3D estimation out-of-the-box; that is, for any sequence
and any camera motion, assuming no knowledge over the scene nor the internal
camera calibration parameters.
1 Produced with the open source Matlab code in
http://webdiis.unizar.es/˜jcivera/code/
1p-ransac-ekf-monoslam.html

1.1 Structure from Motion and Monocular SLAM
3
This comes from the application of solid theoretical concepts, deeply rooted in
multiple view geometry and probability theory. As another output of the application
of a well-founded theory, also the length and the accuracy of the estimation are
greatly improved; from waggling camera motion in indoors scenarios and sequences
of around a minute to half-an-hour sequences of tens of thousands frames taken by
a robot covering trajectories of hundreds of metres.
1.1
Structure from Motion and Monocular SLAM
1.1.1
Structure from Motion
Inside the computer vision ﬁeld, Structure from Motion (SfM) is the line of re-
search that, taking as the only input a set of image correspondences, seeks to infer
in a totally automated manner the 3D structure of the scene viewed and the camera
locations where the images where captured. SfM has been one of the most active
areas of research for the latest three decades, reaching such a state of maturity that
some of its algorithms have already climbed to the commercial application level
[2d3 2011, Autosticht 2011, PhotoTourism 2011].
SfM origins can be traced back to the so-called photogrammetry, that since the
second half of 19th century aimed to extract geometric information from images.
Starting with a set of features manually identiﬁed by the user, photogrammetry
makes use of non-linear optimization techniques known as Bundle Adjustment (BA)
to minimize the reprojection error [Mikhail et al. 2001]. The research done by the
computer vision community has been mostly oriented to achieve the complete au-
tomation of the problem and has produced remarkable progress in three aspects:
ﬁrst, the constraints imposed on the motion of the features in two –or three– images
under the assumption of the rigidity of the scene have been formalized, even in the
case of degenerate motion and uncalibrated camera [Hartley & Zisserman 2004];
second, intense research on salient feature detection and description with a high
degree of invariance [Canny 1986, Harris & Stephens 1988, Lowe 2004]; and third,
spurious rejection [Fischler & Bolles 1981, Rousseeuw & Leroy 1987]. The result
is an automated way of robustly matching point and line features along images and
estimate the geometric relations between pairs.
Based on these three achievements several methods have been proposed that,
from a set of images of a scene, are able to estimate the three-dimensional structure
and camera locations up to a projective transformation in the most general case of
uncalibrated cameras. With some extra knowledge about the camera calibration, a
metric solution up to scale can be obtained. This SfM estimation from constraints
between pairs or triplets of images is usually followed by a Bundle Adjustment (BA)
step [Triggs et al. 2000] that minimizes the reprojection error and reﬁnes this initial
pairwise estimation into a globally consistent one.

4
1 Introduction
1.1.2
Monocular SLAM
On the other hand, the estimation of the ego-motion of a mobile platform and its
surroundings has also been tackled by the robotics community from a slightly dif-
ferent point of view. The so-called SLAM (standing for Simultaneous Localization
and Mapping) [Durrant-Whyte & Bailey 2006, Bailey & Durrant-Whyte 2006] has
been referred as one of the fundamental problems in mobile robotics research. The
SLAM addresses the estimation of the motion of a mobile robot and a map of its
surroundings from the data stream provided by one or several sensors. The ﬁrst
SLAM approaches made use of multiple sensors, e.g. laser [Castellanos et al. 1999,
Newman et al. 2002], radar [Dissanayake et al. 2001], or sonar [Tard´os et al. 2002].
Most of the times, wheel odometry measurements were also included in the es-
timation and sensorial fusion was also a recurrent topic [Castellanos et al. 2001].
As computer vision became mature enough, vision gained a predominant posi-
tion in this sensor fusion schemes and even was used as the only sensorial input
[Davison et al. 2007]. Monocular SLAM refers to the use of a monocular camera as
the dominant sensor, and sometimes the only sensor, for performing SLAM.
Structure from Motion and monocular SLAM present another difference that is
rather relevant for deﬁning the purposes of this book. While SfM has dealt with the
problem in its most general form –that is, for any kind of visual input–, monocular
SLAM has focused in sequential approaches for the processing of video input. This
comes as a consequence of the speciﬁcity of the robotic application; as the senso-
rial input in a robot comes naturally in the form of a stream. A mobile robot also
needs sequential estimation capabilities: At every step the best possible estimation
is required in order to insert it in the control loop, and hence batch processing does
not make sense. This sequential constraint is not limited to the robotics applications:
Augmented reality, for example, also needs a sequential estimation in order to co-
herently and smoothly insert a virtual character in every frame of a sequence and
hence can make use of monocular SLAM algorithms [Klein & Murray 2009].
1.1.3
Structure of the Problem
Following the notation in [Strasdat et al. 2010a], both SLAM and SfM can be rep-
resented in terms of a Bayesian network. Figure 1.2 shows a simpliﬁed example
that illustrates this. In the Bayesian network in this ﬁgure, the variables we want to
estimate are: xCk, containing a set of parameters related to the camera that captured
image Ik –in the most general case, internal and external calibration parameters; and
yi, containing a set of parameters modeling a 3D feature in the scene. The observed
or measured variables are zk
i , that represent the projection of a feature i in an image
Ik. The measurement model imposes constraints between the variables we want to
estimate and the image measurements, that are the arrows in the net. As long as
we have enough constraints, the variables xCk and yi can be estimated up to a cer-
tain extent [Hartley & Zisserman 2004]. Notice that in the speciﬁc case of an image

1.1 Structure from Motion and Monocular SLAM
5
sequence input, extra constraints can be added from the variables of one camera xCk
to the next one xCk+1 by modeling the dynamics of the camera motion. In 1.2, this
extra links that may appear in the formulation are plotted in dotted line.








	




















	





	













Fig. 1.2. Bayesian network for SfM.
For the sake of clarity, ﬁgure 1.3 models the previous example as a Markov ran-
dom ﬁeld. Here, the constraints imposed by the image measurements zk
i and the mo-
tion model are represented by the links between the hidden variables. If one were
to solve this Bayesian network ofﬂine, the standard approach would be to perform a
global optimization over the whole network. Such global optimization in this speciﬁc
3D vision problem receives the name of Bundle Adjustment [Triggs et al. 2000]. In
order to initialize the iterative Bundle Adjustment optimization, the correspondences
between pairs or triplets of images provide the geometric constraints to calculate
their relative motion [Hartley & Zisserman 2004, Nist´er 2004].
The estimation of the 3D estructure and camera motion in a sequential manner
can be modeled with a Markov random ﬁeld very similar to the one in ﬁgure 1.2;
but taking into account that the network grows at every step with the addition of
new camera poses for each frame processed, new 3D features as the camera images
new areas, and new tracked image features that link both. This new Markov random
ﬁeld is shown in ﬁgure 1.4. The aim in sequential SfM –or monocular SLAM–
consists of propagating over all the hidden camera and feature variables every new
image evidence. At the most basic level of local SLAM, two different approaches
are mostly used nowadays:
• Keyframe-based SLAM. This ﬁrst approach adapts the ofﬂine Bundle Adjustment
to ﬁt the sequential processing of a video sequence. The SLAM computation is
separated into two parallel threads: ﬁrst, one thread for high frame rate camera
tracking, where the camera pose is estimated from the known features that are

6
1 Introduction








	

Fig. 1.3. Markov random ﬁeld for SfM.













	





Fig. 1.4. Markov random ﬁeld for sequential SfM or monocular SLAM.
already in the map. The second thread performs, at a much lower rate, a
global optimization over the tracked features and a selected set of keyframes
in the image sequence. Such keyframes can correspond to the whole sequence
[Klein & Murray 2007,
Klein & Murray 2008,
Konolige & Agrawal 2008,
Strasdat et al. 2010b]
or
a
sliding
window
around
the
current
frame
[Nist´er et al. 2006, Mouragnon et al. 2006, Konolige et al. 2007]. The tracked
camera pose in the ﬁrst thread serves as the initial seed for this iterative non-linear
reﬁnement in the second thread.
Figure 1.5 illustrates this ﬁrst approach. Notice that in this simpliﬁed example
the intermediate cameras xC2 and xC3 are not keyframes and hence are removed
from the estimation. Notice also that the links between features y2, y3, y4, y5 and
camera poses xC2 and xC3 have been also removed from the estimation and hence
their information is lost.
• Filtering-based SLAM. Differently from keyframe optimization, ﬁltering
marginalize out the past poses and keep a joint probabilistic estimation over the
current pose and the 3D map features. A ﬁltering algorithm adds at every step

1.1 Structure from Motion and Monocular SLAM
7













	





Fig. 1.5. Keyframe-based SLAM.
the current camera variables to the estimation, marginalizes out previous cameras
and update the whole network based on the image measurements in the current
frame.
Figure 1.6 illustrates this in the proposed example. Notice that the marginaliza-
tion of a past pose introduces probabilistic links between every pair of features in
the map that do not appear in the keyframe SLAM methods. The Markov random
ﬁeld in ﬁgure 1.6 presents less nodes than the keyframe-based one in ﬁgure 1.5,
but on the other hand extra links have appeared between every pair of features.
It is also worth remarking that ﬁltering algorithms marginalize the links between
past camera poses and features –that is, they integrate their information into the
current state– instead of removing them like keyframe SLAM does.












	





Fig. 1.6. Filtering-based SLAM.
The more sparse structure of the links in keyframe SLAM methods makes them
less computationally demanding than ﬁltering-based ones. It can be argued that they
also process less information than ﬁltering methods: in ﬁgure 1.5 only the corre-
spondences in images Ii;i = 1,4, which are the links between camera parameters
xC1 and xC4 and features yi;i = 1,...,6 are considered in the estimation. A ﬁlter-
ing algorithm is able to integrate the information in every image Ii;i = 1,...,4 be-
fore marginalizing past camera parameters. Nevertheless, it has been demonstrated
very recently [Strasdat et al. 2010a] that the ratio between the information gained

8
1 Introduction
by considering this extra cameras and the computational cost derived from a denser
structure compared with a keyframe SLAM is small enough to be worth including
them.
On the other hand, ﬁltering algorithms maintain a joint probability density esti-
mation over the current camera and map in a more efﬁcient manner than keyframe
ones. Most of the times, the high accuracy of the 3D visual estimation in a practical
setting makes unnecesary the computation of such uncertainty. However, it is also
true that the computation of such uncertainty distribution can be proﬁtable in critical
situations –e.g sequence bootstrapping, small number of tracked features or quasi-
degenerate conﬁgurations [Frahm & Pollefeys 2006]—. Filtering methods could be
advisable in those cases.
Structure from Motion under a Bayesian ﬁltering framework, or visual SLAM,
has been a relevant chapter on 3D estimation from images with examples as
[Davison 2003] based on Extended Kalman Filter, [Eade & Drummond 2006] based
on particle ﬁlters, [Holmes et al. 2008] based on Unscented Kalman Filter and
[Eade & Drummond 2007] based on Information Filters. This book targets vi-
sual systems based on the Extended Kalman Filter (EKF); having the system in
[Davison et al. 2007] as its baseline. Nevertheless, most of the concepts proposed
can be extended from the EKF to the other ﬁltering schemes. Although the Extended
Kalman Filter is often accused of consistency problems due to its strong require-
ments for linearity and Gaussianity [Julier & Uhlmann 2001, Bailey et al. 2006,
Castellanos et al. 2007]; it will be shown along the results of this book that those
requirements hold for the Structure from Motion problem at a local scale and the
EKF performance matches that of other methods. Further insight in this issue can
also be read in the recent [Sol`a et al. 2011].
1.2
Background
In order to fully apprehend the material included in this book, the reader should
be familiar with SLAM, Bayesian estimation and multiple view geometry. Regard-
ing SLAM, the reader is referred to the survey in [Durrant-Whyte & Bailey 2006,
Bailey & Durrant-Whyte 2006], that describes the SLAM problem and covers the
essential algorithms. [Davison et al. 2007] describes the ﬁrst monocular SLAM sys-
tem demonstrating real-time performance and that we take as the starting point of
this book. [Bar-Shalom et al. 2001, Gelb 1999] are excellent references for estima-
tion and tracking from a general point of view. [Thrun et al. 2005] covers the main
sequential estimation techniques applied in robotics including the Extended Kalman
Filter; and hence is particularly advisable. The appendix A in this book contains a
very detailed description of the Extended Kalman Filter formulation used along the
book aimed to help the implementation of the presented results.
Regarding
single
and
multiple
view
geometry,
the
ﬁrst
chapters
of
[Hartley & Zisserman 2004] introduce projective parameterizations, that will
be discussed in the ﬁrst chapters of this document, and the camera model

1.3 Outline of the Book
9
that we use along the book. Regarding salient point detection and de-
scription, although a wide array of detectors and descriptors with a high
degree
of
invariance
are
available
for
feature
extraction
and
matching
[Mikolajczyk et al. 2005, Mikolajczyk & Schmid 2005], the results presented in
this book make use of the Harris corner detector [Harris & Stephens 1988] and plain
normalized cross-correlation [Brown et al. 2003] between warped image patches
[Hartley & Zisserman 2004] for matching. The maintenance of the probabilistic
estimation over camera and features makes possible to warp planar patches ac-
cording to the relative motion between frames, making unnecesary a high de-
gree of invariance. For some of the results in the book, the FAST detector
[Rosten & Drummond 2005] has also been used, reducing the feature extraction
cost without noticeable performance improvement nor degradation.
1.3
Outline of the Book
The main aim of this book is then to develop models and methods for sequential SfM
or monocular SLAM; ﬁtting the projective nature of the camera under a Bayesian
ﬁltering framework. In this section, the speciﬁc topics of the book are introduced in
more detail.
• Chaper 2: Points at Inﬁnity. Mosaics using the Extended Kalman Filter
In Structure from Motion it is well known that cameras, as projective sensors, are
able to image very distant points, theoretically even points at inﬁnity. Several il-
lustrative examples can be taken from our daily lives: For example, we are able to
see the stars even when they are several million light years away. Homogeneous
coordinates have provided an algebraic manner to manipulate very distant points
in SfM without numerical issues. Although distant features do not provide infor-
mation for estimating the camera translation, they have proved to be very useful
to estimate its orientation. Following the previous example, we cannot estimate
our position in the earth looking at the stars, but we can extract very precisely
directions –where the North is– by looking at them.
Although the notation close and distant points may be the most intuitive one,
the accuracy of the estimation in multiview geometry is actually governed by
the parallax angle. The parallax angle is the angle formed by the two projection
rays that goes from a point feature to the optical centers of two cameras. The
bigger this angle, the more accurately the depth of the point can be estimated.
Low values for this angle may be caused by distant features, but also by small
translation between two images.
Early
visual
ﬁltering
estimation
techniques
[Ayache & Faugeras 1989,
Matthies et al. 1989,
Broida et al. 1990,
Azarbayejani & Pentland 1995,
Chiuso et al. 2002, Davison 2003] had limitations with these low-parallax
points. Hence, they were losing an important source for orientation information
and limiting the use of cameras in outdoors scenarios where these type of
features are rather common. This chapter discusses the value of such low-
parallax points for camera orientation estimation. The extreme case of a camera

10
1 Introduction
undergoing pure rotational motion –and hence zero-parallax points– is presented
and processed using a ﬁltering algorithm able to estimate camera rotation and
point directions in real-time at 30 Hz. Loop closures of 360◦are achieved as a
proof of consistency.
As an application of this, a mosaicing application is built on top of the estimated
backbone map. This mosaicing algorithm directly inherits the advantages of the
EKF processing, being the ﬁrst mosaicing technique that presents real-time drift-
free spherical mosaicing results for camera rotations of 360◦.
• Chaper 3: Inverse Depth Parameterization
The step that naturally follows camera rotation and zero-parallax point estima-
tion is a uniﬁed parametrization for low and high parallax features, allowing to
estimate camera rotation and translation. The key concept of the parametriza-
tion proposed in this chapter is the direct coding of the inverse depth of the fea-
tures relative to the camera locations from which they were ﬁrst viewed. This
inverse depth parametrization offers two key improvements with respect to the
standard Euclidean one. First, as homogeneous coordinates, it is able to model
zero-parallax points. And second, the addition of the initial camera position im-
proves the degree of linearity of the projection equation, which is a crucial re-
quirement for the Extended Kalman Filter.
The inverse depth point feature model solves in an elegant manner what was
called in monocular SLAM the initialization problem: Previous point feature
models were unable to be inserted from the frame they were ﬁrst seen in the
general estimation framework due to the unobservability of the depth along the
projection ray. The usual approach was to delay this initialization until the feature
depth converged to a value. Features initialized in such delayed manner would
not contribute to the estimation until they show parallax enough. Notice that, in
the common case of zero-parallax features, they would never be added to the
estimation and hence their information would be lost. Inverse depth feature ini-
tialization is undelayed in the sense that even distant features are immediately
used to improve camera motion estimates, acting initially as bearing references
but not permanently labeled as such.
The inverse depth parametrization remains well behaved for features at all stages
of SLAM processing, but has the computational drawback that each point is rep-
resented by a six dimensional state vector as opposed to the standard three of a
Euclidean XYZ representation. We also show in this chapter that once the depth
estimate of a feature is sufﬁciently accurate, its representation can be safely con-
verted to the Euclidean XYZ form, and propose a linearity index which allows
automatic detection and conversion to maintain maximum efﬁciency — only low
parallax features need be maintained in inverse depth form for long periods.
• Chaper 4: 1-point RANSAC
This chapter has a double objective: ﬁrst, it is aimed to illustrate for the ﬁrst time
how ﬁltering-based visual SLAM methods, without neither submapping nor loop
closure capabilities, can reach an accuracy and trajectory length comparable to
keyframe methods. Speciﬁcally, a camera-centered Extended Kalman Filter is
used here to process a monocular sequence as the only input (and also combined

1.3 Outline of the Book
11
with wheel odometry),with 6DOF motion estimated. Also in this chapter features
are kept “alive” in the ﬁlter while visible as the camera explores forward and are
deleted from the state once they go out of the ﬁeld of view. In a few words, the
EKF operates in a “visual odometry” mode [Nist´er et al. 2006].
“Forgetting” the map permits an increase in the number of tracked features per
frame from tens to around 1–2 hundred. While improving the accuracy of the es-
timation, it makes computationally infeasible the exhaustive Branch and Bound
search performed by standard JCBB for match outlier rejection. As the second
contribution that overcomes this problem, we present here a RANSAC-like al-
gorithm that exploits the probabilistic prediction of the ﬁlter. This use of prior
information makes it possible to reduce the size of the minimal data subset to
instantiate a hypothesis to the minimum possible of 1 point, increasing the efﬁ-
ciency of the outlier rejection stage by several orders of magnitude.
Experimental results from real image sequences covering trajectories of hundreds
of meters are presented and compared against RTK GPS ground truth. Estimation
errors are about 1% of the trajectory for trajectories up to 650 metres.
• Chaper 5: Degenerate Camera Motions and Model Selection
Degenerate camera motions have a capital importance from a practical point of
view of implementing real systems. For example, if a camera is attached to a
mobile robot, there will be large periods when the robot is stopped. Pure rotation
motion is also very frequent in industrial robotic arms. Even in hand-held camera
motion estimation, there will be periods where the camera is almost still, and ro-
tations are more easily performed than large translations –particularly in outdoor
environments.
Any estimation algorithm modeling a general camera motion in any of the above
situations will fail. What happens here is that the image noise incorrectly ﬁts
the extra parameters of the model. This problem is referenced in the SfM lit-
erature, where model selection schemes are used when degenerate motion may
be encountered. This schemes discriminate models based on two terms: a term
based on the reprojection error, which basically discards simplistic models; and
a penalty term based on the complexity of the model that avoids selecting over-
parameterized models.
What we propose here is a model selection scheme that computes probabilities
over models, based on research on model selection carried out by the tracking
community. In a probability-based scheme, simplistic methods will receive low
probabilities, but also overparameterized ones as their probability distribution
function expands over unnecessary dimensions. Ad-hoc penalty terms for com-
plex models are not needed in the proposed probability-driven model selection.
• Chaper 6: Self-calibration
The Structure from Motion methods were developed assuming a minimum or
even null knowledge neither about the camera nor the scene. We have already
commented in previous paragraph that model selection algorithms were devel-
oped to cover every possible type of scene and camera geometric conﬁguration.
The geometric constraints that correspondences should hold have been formal-
ized even in the case of any additional information than the images; that is, for

12
1 Introduction
cameras with unknown and possibly varying calibration parameters. Using some
reasonable additional information a metric reconstruction, egomotion estimation
and internal self-calibration can be estimated up to a scale factor.
While this is a well-known result, monocular SLAM and sequential SfM have
mostly used precalibrated cameras [Davison et al. 2007, Klein & Murray 2008,
Konolige & Agrawal 2008, Mouragnon et al. 2009]. The authors believe that self-
calibration would improve the usability of the SLAM algorithms. For certain
applications, like augmented reality for endoscopic surgery [Grasa et al. 2011],
self-calibration is a must: you cannot expect wasting the precious time of a med-
ical team in a tedious calibration of an endoscope.
As a step towards this out-of-the-box SLAM, this chapter presents an algorithm
for monocular SLAM or sequential SfM from an uncalibrated image sequence.
Camera internal calibration, egomotion and 3D scene structure are estimated
from an image sequence without any prior information.
• Appendix A: Implementation Details
This ﬁrst appendix aims at providing every implementation detail that can be
helpful for the reproduction of the algorithms in this book. Along the ﬁrst seven
chapters the emphasis has been in the concepts and experimental proofs; and
some of the details have been hidden for the sake of a more clear presentation.
Particularly, this chapter details the computation of the Jacobians for the dynamic
and measurement models used along the book; which are needed by the EKF
processing.
• Appendix B: Filter Tuning Understanding via Dimensional Analysis
As already said, it is a well known fact that in the best of the cases the Structure
from Motion or pure monocular SLAM estimation can only provide a geometric
estimation up to a scale factor. Nevertheless, in the formulation adopted in this
book, the parameters in the state vector are modeled using metric units. More
speciﬁcally, the ﬁlter tuning parameters (e.g., the acceleration noise) are the ones
set in metric units and act as the metric priors for the 3D estimation. The scale of
the estimation in every monocular experiment of the book is then meaningless,
as it is unobservable, and comes as a result of the metric priors introduced in the
ﬁlter.
The geometric priors that induce the scale of our experiments are detailed in
this appendix. More importantly, this appendix shows a dimensional analysis of
the SfM problem that allows: 1) the identiﬁcation of the relevant magnitudes
of the problem; and 2) the proposal of a new dimensionless formulation that
separates the real scale of the estimation from the monocular SLAM and allows
to represent the estimated parameters in terms of dimensionless length ratios and
angles.

Chapter 2
Points at Inﬁnity: Mosaics Using the Extended
Kalman Filter
Abstract. This chapter introduces the use of zero-parallax points on ﬁltering-based
Structure from Motion (SfM) or monocular SLAM. A geometric model is proposed
for the estimation of an accurate camera rotation and the directions of a set of tracked
features using an Extended Kalman Filter. On top of that, it is proposed a sequential
mosaicing algorithm able to build drift-free, consistent spherical mosaics in real-
time, automatically and seamlessly even when previously viewed parts of the scene
are re-visited. This method represents a signiﬁcant advance on previous mosaic-
ing techniques which either require an expensive global optimization or which run
sequentially in real-time but use local alignment of nearby images and ultimately
drift.
2.1
Introduction
2.1.1
Points at Inﬁnity
In SfM, the well-known concept of a point at inﬁnity is a feature which exhibits no
parallax during camera motion due to its extreme depth. A star for instance would
be observed at the same image location by a camera which translated through many
kilometers pointed up at the sky without rotating. Such a feature cannot be used for
estimating the camera translation but is a perfect bearing reference for estimating its
rotation. Homogeneous coordinates, used in SfM, allow an explicit representation
of points at inﬁnity, and they have proven to play an important role during off-line
structure and motion estimation.
This chapter presents a real-time EKF ﬁltering algorithm for consistently
estimating the motion of a rotating camera observing such points at inﬁnity. This
algorithm introduces the use of inﬁnity points in ﬁltering SfM approaches, stepping
J. Civera et al.: Structure from Motion Using the Extended Kalman Filter, STAR 75, pp. 13–32.
springerlink.com
c⃝Springer-Verlag Berlin Heidelberg 2012

14
2 Points at Inﬁnity: Mosaics Using the Extended Kalman Filter
forward to the use of those algorithms in outdoors environments where low-parallax
features are very common. This step is culminated in next chapter with a uniﬁed
parameterization for close and distant –even inﬁnity– points. Here, the accuracy
and consistency of the estimated camera rotation and feature map is shown in 360◦
loop closures experiments using real-image sequences.
2.1.2
Real-Time EKF-Based Mosaicing
The second contribution of this chapter is the use of a consistent map of point fea-
tures as the basis for a real-time, drift-free mosaicing system. This is the ﬁrst al-
gorithm which can build drift-free mosaics over the whole viewsphere in seamless
real-time: no backtracking, batch optimization or learning phase is required; and
arbitrarily long image sequences can be handled without slow-down.
Mosaicing involves accurately aligning a set of overlapping images based on
correspondences, normally between automatically-detected salient features. While
mosaicing does not require full 3D camera motion and scene structure to be esti-
mated, it can be considered as a reduced SfM or visual SLAM problem since it
has the key requirements of simultaneously solving for camera pose (in this case
rotation) and feature locations (in this case directions without a depth coordinate).
Many different approaches to mosaicing have been published, being the most rele-
vant for this chapter summarized in section 2.2.3. All are based on pair-wise image
matching to estimate local camera motion. Those which aim to operate sequentially
and in real-time simply concatenate multiple motion estimates and are prone to drift
as errors accumulate. Those whose goal is globally consistent mosaics have a ﬁnal
off-line optimization step which re-adjusts the motion estimates to bring them into
consensus.
As a key difference from previous works, image alignment is based here on a
ﬁltering scheme. Under this approach, it is built a persistent, probabilistic repre-
sentation of the state of the sensor and scene map which evolves in response to
motion and new sensor measurements. Speciﬁcally, it is used an Extended Kalman
Filter (EKF) with a state vector consisting of stacked parameters representing the
3D orientation and angular velocity of the camera and the directions (i.e. view-
sphere coordinates, since no depth can be estimated for the scene points) of a set of
automatically acquired features, none of which need to be known in advance.
As a brief summary of the complete mosaicing algorithm, we take ﬁrst the image
stream from a rotating camera, build an efﬁcient, persistent SLAM map of inﬁnite
points — directions mapped onto the unit sphere — and use these as the anchor
points of a triangular mesh, built sequentially as the map eventually covers the
whole viewsphere. Every triangle in the mesh is an elastic tile where scene texture
is accumulated to form a mosaic. As each new image arrives, the probabilistic map
of inﬁnite points is updated in response to new measurements and all the texture
tiles are re-warped accordingly. So, every measurement of a point potentially im-
proves the whole mosaic, even parts not currently observed by the camera thanks to

2.2 Related Work
15
probabilistic knowledge of the correlations between estimates of different features.
This attribute is especially valuable when a loop is closed because the whole map
beneﬁts from a large correction, removing drift.
The following chapter is organised as follows: After a literature review and com-
parison between off-line and sequential approaches in section 2.2; section 2.3 covers
the essentials of the EKF estimation of inﬁnite points and camera rotation. Section
2.4 describes how features are added to or removed from the map. Section 2.5 is
devoted to the mesh for the mosaic given a map of feature directions. Finally exper-
imental results and a brief discussion are presented in sections 2.6 and 2.7.
2.2
Related Work
2.2.1
SLAM
The standard approach to SLAM is to use a single Gaussian state vector to represent
the sensor and feature estimates; and to update this with the Extended Kalman Filter
(EKF). This approach was called the ‘stochastic map’ when proposed initially by
Smith and Cheeseman in [Smith & Cheeseman 1986a] and has been widely used
in mobile robotics with a range of different sensors; odometry, laser range ﬁnders,
sonar, and vision among others (e.g. [Castellanos & Tard´os 1999, Feder et al. 1999,
Ort´ın et al. 2003]). This amounts to a rigorous Bayesian solution in the case that the
sensor and motion characteristics of the sensor platform in question are governed by
linear processes with Gaussian uncertainty proﬁles — conditions which are closely
enough approximated in real-world systems for this approach to be practical in most
cases, and in particular for small-scale mapping.
Visual sensing has been relatively slow to come to the forefront of robotic SLAM
research. Davison and Murray [Davison & Murray 1998] implemented a real-time
system where a 3D map of visual template landmarks was build and observed
using ﬁxating stereo vision. Castellanos et al. [Castellanos et al. 1994] built a 2D
SLAM system combining straight segments from monocular vision and odometry,
and trinocular straight segments and odometry. In [Davison 2003] Davison demon-
strated 3D SLAM using monocular vision as the only sensor, also using a smooth
motion model for the camera to take the place of odometry this system exhibited
unprecedented demonstrable real time performance for general indoors scenes ob-
served with a low cost hand-held camera. After this seminal work, many other inter-
esting 3D SLAM systems which rely only on visual sensing started to emerge (e.g.
[Kim & Sukkarieh 2003, Jung & Lacroix 2003, Eustice et al. 2005]).

16
2 Points at Inﬁnity: Mosaics Using the Extended Kalman Filter
2.2.2
Off-Line SFM vs. EKF SLAM
In order to directly compare the off-line approaches to mosaicing with our sequential
one, we will focus on methods which rely on discrete feature matching. The off-line
approaches generally match features between images in a pairwise fashion (usually
between temporal neighbours) and then perform a ﬁnal global bundle adjustment
optimization to achieve good drift-free solutions.
In our sequential approach, scene structure — a set of selected points we call a
map — and the camera motion are estimated by iterating the following steps:
1. Predict the locations of all the map features in the next image, along with a gated
search region for each. The information from all previous images is implicitly
accumulated in this state-based prediction and this leads to tight search regions.
2. Use template matching to exhaustively search for the feature match only within
its search region.
3. Update estimates of the locations of all the mapped features and the current cam-
era location.
4. Map maintenance, adding new features when new scene areas are explored and
marginalizing out features predicted to be visible but persistently not matched.
The resulting sparse set of map features, autonomously selected and matched has —
when compared with ‘ﬁrst match and then optimize’ approaches — the following
desirable qualities for producing consistent mosaics:
Long tracks
— Only features persistently observed when predicted to be visible
are kept in the map, and are allowed to build up immunity to future deletion.
Highly visible and identiﬁable features tend to live on in this process of ‘survival
of the ﬁttest’.
Loop closing tracks
— When the camera revisits an area of the scene previously
observed, it has the natural ability to identify and re-observe ‘old’, loop closing,
features seamlessly.
To achieve the highest accuracy in every update of a scene map after matching
image features, the update step should ideally be done using an iterative non-linear
bundle adjustment optimization, but this would be prohibitively expensive to be run
at every step. Instead of that, the EKF can serve as a sequential approximation to
bundle adjustment. Update by bundle adjustment after processing every single im-
age means a non linear optimization for all camera locations and all scene features,
so processing long image sequences results in an increasing number of camera loca-
tions and hence an increasing dimension for the bundle adjustment of (3mk +2nk),
where mk is the number of images, and nk is the number of scene points. Also, cal-
culating search regions requires the inversion of a matrix of dimension 3mk +2nk to
compute the estimation covariance.
To compare the EKF with bundle adjustment, it should be considered that, as
stated in [Triggs et al. 2000], the EKF is a sequential approximation to bundle ad-
justment where:

2.2 Related Work
17
1. A motion model is included to relate one camera location with the next.
2. The EKF is just bundle adjustment’s ﬁrst half-iteration because only the most
recent camera location is computed. The estimated state is reduced, subsuming
all historic camera location estimates in the feature covariance matrix. The es-
timated state, dimension (7 +2nk), is composed of the last camera pose and all
map features. In our model the camera state vector has dimension 7: an orienta-
tion quaternion and 3D angular velocity.
3. At each step, information about the previous camera pose is marginalized out.
In doing so, linearization for previous camera poses is not computed as in the
optimal solution, and hence linearization errors will remain in the covariance
matrix. However, mosaicing with a rotating camera is a very constrained, highly
linear problem, so the results that can be obtained in real-time are highly accurate,
only falling a little short of the result a full optimization would produce.
2.2.3
Image Mosaicing
Mosaicing is the process of stitching together data from a number of images, usually
taken from a rotating camera or from a translating camera observing a plane, in order
to create a composite image which covers a larger ﬁeld of view than the individual
views. While a variety of approaches have been published for matching up sets
of overlapping images with a high degree of accuracy, all have relied on off-line
optimization to achieve global consistency. Other previous methods which operate
sequentially in a pair-wise manner and in real-time suffer from the accumulation of
drift.
Mosaic building requires estimates of the relative rotations of the camera when
each image was captured. Classically, the computation of such estimates has been
addressed as an off-line computation, using pair-wise image matching to estimate
local alignment and then global optimization to ensure consistency. The goal has
normally been to produce visually pleasing panoramic images and therefore after
alignment blending algorithms are applied to achieve homogeneous intensity distri-
butions across the mosaics, smoothing over image joins. We focus here only on the
alignment part of the process, achieving registration results of quality comparable
to off-line approaches but with the advantage of sequential, real-time performance.
Blending or other algorithms to improve the aesthetic appearance of mosaics could
be added to our approach straightforwardly, potentially also running in real-time.
[Szeliski & Shum 1997] presented impressive spherical mosaics built from video
sequences, explicitly recognizing the problem of closing a loop when building full
panoramas (from sequences consisting of a single looped pan movement). However,
their method needed manual detection of loop closing frames. The non-probabilistic
approach meant that the misalignment detected during loop closing was simply
evenly distributed around the orientation estimates along the sequence.
[Sawhney et al. 1998] tackled sequences with more complicated zig-zag pan mo-
tions, requiring a more general consideration of matching frames which were tem-
porally distant as loops of various sizes are encountered. Having ﬁrst performed

18
2 Points at Inﬁnity: Mosaics Using the Extended Kalman Filter
pairwise matching of consecutive images they could estimate an approximate lo-
cation for each image in the sequence. This was followed by an iterative hypothe-
ses veriﬁcation heuristic, applied to detect matching among geometrically close but
temporally distant images to ﬁnd the topology of the camera motion, and then ﬁ-
nally global optimization. Both [Szeliski & Shum 1997] and [Sawhney et al. 1998]
use whole image intensities without feature detection for the optimization.
[Capel & Zisserman 1998] proposed methods based on matching discrete fea-
tures; RANSAC is applied to detect outlier-free sets of pairwise matches among the
images. A global bundle adjustment optimization produces the ﬁnal mosaic, achiev-
ing super-resolution. [de Agapito et al. 2001] also used robust feature matching to
perform self-calibration and produced mosaics from sequences from a rotating and
zooming camera.
[Brown 2003] considered the problem of mosaicing sets of widely separated, un-
calibrated still images. Their method used SIFT features to perform wide-baseline
matching among the images, and automatically align them into a panorama, opti-
mizing over the whole set for global consistency using bundle adjustment.
The recent work of Steedly et al. [Steedly et al. 2005] is perhaps closest to the
presented approach because it explicitly considers questions of computational cost
in efﬁciently building mosaics from long video sequences (on the order of a thou-
sand frames), though not arriving at real-time performance. The key to efﬁcient
processing in their system is the use of automatically assigned key-frames through-
out the sequence — a set of images which roughly span the whole mosaic. Each
frame in the sequence is matched against the nearest keyframes as well as against
its temporal neighbors. The idea of building a persistent map of keyframes as the
backbone of the mosaic can be thought of as very similar to the keyframe meth-
ods described in section 1.1 [Klein & Murray 2007, Klein & Murray 2008]. In fact,
very recently, keyframe methods have been explicitly applied to mosaicing showing
impressive real-time performance [Lovegrove & Davison 2010].
To our knowledge, up to [Civera et al. 2009b, Lovegrove & Davison 2010], mo-
saicing algorithms which truly operate in real-time have been much more limited
in scope. Several authors have shown that the straightforward approach of real-time
frame to frame image matching can produce mosaics formed by simply concatenat-
ing local alignment estimates. Marks et al. [Marks et al. 1995] presented a real-time
system for mosaicing underwater images using correlation-based image alignment,
and Morimoto and Chellappa a system based on point feature matching which esti-
mated frame-to-frame rotation at 10Hz [Morimoto & Chellappa 1997]. It should be
noted that while Morimoto and Chellappa used the EKF in their approach, the state
vector contained only camera orientation parameters and not the locations of feature
points as in our SLAM method.
The clear limitation of such approaches to real-time mosaicing is that inevitable
small errors in frame to frame alignment estimates accumulate to lead to misalign-
ments which become especially clear when the camera trajectory loops back on
itself — a situation our SLAM approach can cope with seamlessly.
Some recent approaches have attempted to achieve global consistency in real-
time by other means — Kim and Hong [Kim & Hong 2006] demonstrated sequen-

2.3 Geometrical Modeling
19
tial real time mosaic building by performing ‘frame to mosaic’ matching and global
optimization at each step. However, this approach is limited to small-scale mosaics
because the lack of a probabilistic treatment means that the cost of optimization
will rise over time as the camera continues to explore. Zhu et al. [Zhu et al. 2006]
on the other hand combine a frame to frame alignment technique with an explicit
check on whether the current image matches to the ﬁrst image of the sequence, de-
tection of which leads to the correction of accumulated drift. Again, the method is
not probabilistic and works only in the special case of simple panning motions.
2.3
Geometrical Modeling
We start the exposition of our method by explaining the mathematical models used
for features, camera motion and the measurement process, before proceeding in
section 2.4 to the EKF formulation.
2.3.1
Feature Model
Feature points are modeled by storing both geometric and photometric information
(see Figure 2.1). Geometrically, the direction for feature i relative to a world frame
W is parameterized as an angular azimuth/elevation pair:
yW
i =
 θi φi
⊤.
(2.1)
This corresponds to the following cartesian position mW
i of the point on the unit
sphere:
mW
i =
 cosφi sinθi
−sinφi
cosφi cosθi
⊤.
(2.2)
To represent each inﬁnite point photometrically, a texture patch of ﬁxed size is
extracted and stored when the point is imaged for the ﬁrst time. This patch is used
for correlation-based matching.
2.3.2
Camera Motion Model
It is assumed that the camera translation is small compared with actual feature
depths — true for any camera on a tripod, or well approximated by a camera rotated
in the hand outdoors. The real-world camera dynamics are modeled as a smooth an-
gular motion: speciﬁcally a ‘constant angular velocity model’, which states that at
each time-step an unknown angular acceleration impulse drawn from a zero-mean
Gaussian distribution is received by the camera. The camera state vector is:

20
2 Points at Inﬁnity: Mosaics Using the Extended Kalman Filter
xC =

qWC
ωC

,
(2.3)
where ωC is the angular velocity in the camera reference frame and qWC is a quater-
nion deﬁning the orientation with respect to a world reference frame W. See Fig-
ure 2.1 for further illustration on this model.
Fig. 2.1. Left, An elastic textured triangular mesh is built over a map of scene directions over
the unit sphere. Right a scene feature, yW
i , stamped with its texture patch.
At every time-step k, an angular acceleration αC having zero mean and ﬁxed
diagonal ‘process noise’ covariance matrix Pα is assumed to affect the camera’s
motion. Therefore at each processing step of duration Δt the camera receives an
impulse of angular velocity: ΩC = αCΔt . The state update equation is then:
xCk+1 =
qWC
k+1
ωC
k+1

= f

xCk,n

=
qWC
k
×q

ωC
k +ΩC
Δt

ωC
k +ΩC

(2.4)
2.3.3
Measurement Model
We consider ﬁrst the projection of inﬁnite points in a standard perspective camera.
The camera orientation qWC in the state vector deﬁnes the rotation matrix RCW. So
the coordinates of a point yW =
 θ φ ⊤on the unit sphere m expressed in frame C
are:
mC = RCWmW = RCW 
cosφ sinθ −sinφ cosφ cosθ
⊤
(2.5)
The image coordinates where the point is imaged are obtained applying the pin-
hole camera model to mC:

2.4 Simultaneous Localization and Mapping
21
 uu
vu

=
⎛
⎝Cx −αx
mCx
mCz
Cy −αy
mCy
mCz
⎞
⎠=
⎛
⎝Cx −f
dx
mCx
mCz
Cy −f
dy
mCy
mCz
⎞
⎠,
(2.6)
where (Cx Cy)⊤deﬁne the camera’s principal point and αx and αy are the focal
length in the horizontal and vertical image directions in pixel units. f is the focal
length in metric units and dx and dy deﬁne the size of a pixel in metric units.
Finally, a distortion model has to be applied to deal with real camera lenses.
In this work we have used the standard two parameter distortion model from pho-
togrammetry [Mikhail et al. 2001], which is described next.
To recover the ideal projective undistorted coordinates hu = (uu,vu)⊤, from the
actually distorted ones gathered by the camera, hd = (ud,vd)⊤, the following is
applied:
hu =
Cx +(ud −Cx)

1 +κ1r2
d + κ2r4
d

Cy +(vd −Cy)

1+ κ1r2
d +κ2r4
d


rd =

(dx (ud −Cx))2 +(dy (vd −Cy))2
(2.7)
Where κ1 and κ2 are the radial distortion coefﬁcients.
To compute the distorted coordinates from the undistorted:
hd =
⎛
⎝
Cx +
(uu−Cx)
(1+κ1r2
d+κ2r4
d)
Cy +
(vu−Cy)
(1+κ1r2
d+κ2r4
d)
⎞
⎠
(2.8)
ru = rd

1 +κ1r2
d + κ2r4
d

(2.9)
ru =

(dx (uu −Cx))2 +(dy (vu −Cy))2
(2.10)
ru is readily computed computed from 2.10, but rd has to be numerically solved
from 2.9, e.g using Newton-Raphson, hence 2.8 can be used to compute the distorted
point.
2.4
Simultaneous Localization and Mapping
The algorithm to estimate camera rotation and scene point directions follows the
standard EKF loop [Bar-Shalom & Fortmann 1988] of prediction based on the mo-
tion model (equation 2.4), and measurement (equations 2.5 to 2.10). All the esti-
mated variables (the camera state xC and all the estimated feature directions yi) are
stacked in a single state vector x =

x⊤
C y⊤
1 ... y⊤
i ... y⊤
n
⊤with corresponding
covariance P representing Gaussian-distributed uncertainty. Crucial to the method
is the usage of the measurement prediction to actively guide matching. Next we

22
2 Points at Inﬁnity: Mosaics Using the Extended Kalman Filter
explain in detail the matching process, the initialization of system state and feature
initialization and deletion.
2.4.1
Matching
Every predicted measurement of a feature in the map, ˆhi, and its corresponding
innovation covariance, Si, deﬁne an gated elliptical acceptance image region where
the feature should lie with high probability. In our experiments, deﬁning acceptance
regions at 95% probability typically produce ellipses of diameter 10–20 pixels.
The ﬁrst time a feature is observed, we store both a texture patch and the cur-
rent camera orientation. When that feature is later selected for measurement after
camera movement, its predicted texture patch from the current camera orientation
is synthesized via a warping of the original patch. This permits an efﬁcient match-
ing of features from any camera orientation without the need for invariant feature
descriptors. Figure 2.2 shows an example of the stored and predicted patches. Fur-
ther details of the warping algorithm under general camera motion can be found in
section 3.7.1.
Search for a feature during measurement is carried out by calculating a normal-
ized correlation score for every possible patch position lying within the search re-
gion. The position with the highest score zi, provided that the match score is above
a threshold, is considered to be feature i correspondence at this step.
Fig. 2.2. New features initialization and patch prediction.

2.4 Simultaneous Localization and Mapping
23
2.4.2
State Initialization
We initialize the state of the camera with zero rotation — its initial pose deﬁnes
the world coordinate frame, and therefore we also assign zero uncertainty to initial
orientation in the covariance matrix. The angular velocity estimate is also initialized
at zero, but a high value is assigned to σω, in our case
√
2 rad
sec , in order to deal with
an initial unknown velocity. This is a remarkable system characteristic: the map can
be initialized from a camera which is already rotating. In fact in the experiments,
the camera was already rotating when tracking commenced.
2.4.3
Feature Initialization and Deletion
When a new image is obtained, if the number of features predicted to be visi-
ble inside the image goes below a threshold, in our case around 15, a new fea-
ture is initialized. A rectangular area without features is selected randomly in the
image, and searched for a single salient point by applying the Harris detector
[Harris & Stephens 1988]. Figure 2.2 shows an initialization example.
This simple rule means that at the start of tracking the ﬁeld of view is quickly
populated with features which tend to be well-spaced and covering the image. As
the camera rotates and some features go out of view, it will then demand that new
features are initialized — but if regions of the viewsphere are revisited, new features
will not be added to the map since old features (still held in the state vector) are
simply re-observed.
When an initial measurement of a new feature zj is obtained, the state vector is
expanded with the new feature estimate ˆyj. Deﬁning R j the image measurement
noise covariance, the covariance matrix is expanded as follows:
Pnew = J

P 0
0 R j

J⊤
J =
 I 0
J1

, J1 =
 ∂y j
∂xC
,0,...,0,
∂y j
∂zj

.
The backprojection function from an initial image measurement zj to a 3D ray
yj is further elaborated in the appendix A, along with its derivatives.
Features with a low successes/attempts ratio in matching — in practice 0.5 — are
deleted from the map if at least 10 matches have been attempted. This simple map
maintenance mechanism allows deletion of non-trackable features — for example
those detected on moving objects or people. Non persistent static scene features (for
instance caused by reﬂections) are also removed.

24
2 Points at Inﬁnity: Mosaics Using the Extended Kalman Filter
2.5
Meshing and Mosaicing
At any step k we have available a map of inﬁnite points:
Y k =

yW
i

, i = 1...nk .
(2.11)
After processing every image, the location estimate of every point in the map is
updated and hence changed. Additionally, on each step some map points might be
deleted or added as new.
The mosaics we build are made up of a set of textured elastic triangular tiles
attached to the map of inﬁnite points. A mesh triangle Tj is deﬁned as:
Tj =

j1, j2, j3,TX j

,
(2.12)
where {j1, j2, j3,} identify the map points to which the triangle is attached and TXj
deﬁnes the triangle texture. The triangle is termed as elastic because the location
estimates of the points to which it is attached are updated at every step, and hence
the triangle and texture are deformed accordingly.
The triangular mesh mosaic at step k is deﬁned as:
M k =

T k
j

,
j = 1...mk .
(2.13)
2.5.1
Updating the Mesh
As the map is updated at every step, the mosaic has to be updated sequentially
as well. The mosaic update consists of updating the elastic triangles, and potential
deletion and creation of triangles.
Figure 2.3 summarizes the algorithm to sequentially update the mosaic M k−1
into M k. After processing image k, the map Y k is available. A spherical Delaunay
triangulation Dk for the map Y k points is computed using Renka’s [Renka 1997]
algorithm:
Input data:
M k−1, Y k, image at step k.
Output data:
M k.
Algorithm:
1.- Dk =

Dk
l

, Spherical Delaunay triangulation.
2.- M k is a subset of Dk. Every Dk
l is classiﬁed as:
Elastically updated triangle:
Dk
l already in M k−1, but not in M k . If visible in current image,
the tile texture can be updated.
New and visible:
Dk
l not in M k−1 and the visible in image k. Dk
l is included in M k. Texture
is gathered from image k.
New but not visible:
Dk
l not in M k−1 but not fully visible in image k. It is not added to M k.
Fig. 2.3. Triangular mosaic update algorithm.

2.5 Meshing and Mosaicing
25
Fig. 2.4. Mesh update example. Triangle {456} is elastically updated. {128} new and visible:
created because of new map point 8. {247} new not visible; the triangle was not in M k−1,
but it is not totally visible in image k. {136} is deleted as map point 3 is removed. Points 1,
2, 4 and 6 are kept in the map but a signiﬁcant change in the estimated position of point 1 has
caused the triangulation to ‘ﬂip’, so, {126},{246} are deleted, while {124},{146} are new
and visible.
Dk =

Dk
l

,
(2.14)
where every triangle Dk
l = {l1,l2,l3} is deﬁned by the corresponding 3 map features.
The complexity of the triangulation is O(nk lognk) where nk is the number of map
features. The triangles which will be included in M k are a subset of the triangles
in the full triangulation Dk. Every triangle Dk
l in Dk is classiﬁed to determine its
inclusion in M k mosaic according to the algorithm described in Figure 2.3: triangles
are either carried over from the previous mosaic or newly created if texture can be
immediately captured from the current image. Notice that triangles in M k−1 but not
in Dk
l (due to a change in mesh topology) are deleted. Figure 2.4 illustrates with an
example the different cases in the triangular mesh mosaic update.
2.5.2
Tile Texture Mapping
The three points deﬁning a triangular tile are positions on a unit sphere. The texture
to attach to each tile is taken from the region in the image between projections of
these three vertices. In a simple approach to mosaic building, the triangles could be
approximated as planar, and the mosaic surface a rough polyhedral. However better
results can be achieved if the triangular tile is subdivided into smaller triangles
that are backprojected over the spherical mosaic surface. Additionally, the camera

26
2 Points at Inﬁnity: Mosaics Using the Extended Kalman Filter
Fig. 2.5. (a) Triangular tile deﬁned by map points E,F,G, meshed as subtriangles represented
over the unit sphere. (b) Shows the backprojection in the image; notice how the subtriangles
also compensate the radial distortion.
radial distortion is better compensated by the subtriangles. Figure 2.5 illustrates the
improvement due to the division of a triangular tile into subtriangles.
2.6
Experimental Results
We present experiments demonstrating sequential mosaic building using images ac-
quired with a low cost Unibrain IEEE1394 camera with a 90◦ﬁeld of view and
320 × 240 monochrome resolution at 30 fps. The ﬁrst experiment shows the result
from a sequence taken from a hand-held camera performing a 360◦pan and cyclotor-
sion rotation — this early experiment performed ofﬂine in a Matlab implementation.
The second experiment shows results obtained in real-time in a full C++ implemen-
tation with the more sophisticated sub-triangle mosaicing method. Both sequences
were challenging because there were pedestrians walking around, and the camera’s
automatic exposure control introduced a great deal of change in the image contrast
and brightness in response to the natural illumination conditions.
2.6.1
360◦Pan and Cyclotorsion
The hand-held camera was turned right around about 1.5 times about a vertical axis,
so that the originally-viewed part of the scene came back into view in ‘loop closure’.
Care was taken to ensure small translation, but relatively large angular accelerations
were permitted and the camera rotated signiﬁcantly about both pan and cyclotorsion
axes.

2.6 Experimental Results
27
Fig. 2.6. 360◦pan and cyclotorsion. Top left: ﬁrst image in sequence. The ﬁrst image on the
second row is at the loop closure point; notice the challenging illumination conditions. Third
row: unit sphere with feature patches, triangular mesh and simple texture.
Figure 2.6 shows selected frames showing the search region for every predicted
feature and the matched observations. The frames we show are at the beginning
of the sequence, at loop closure and during the camera’s second lap. At the loop
closure, we can see that of the ﬁrst two re-visited features, one was not matched
immediately and the other was detected very close to the limit of its search re-
gion. However, in the following frames most of the re-visited features were cor-
rectly matched despite the challenging illumination changes. It should be noticed
that the loop is closed seamlessly by the normal sequential prediction-match-update
process, without need any additional steps — no back-tracking or optimization. The
mosaic after processing all the images is also shown.
We believe that the seamless way that loop closing is achieved is in itself a very
strong indication of the achieved angular estimation accuracy with this algorithm.
The predictions of feature positions just before redetection at loop closure only dif-
fer from their true values by around 6 pixels (corresponding to less than 2 degrees)
when the camera has moved through a full 360◦. After loop closing and the correc-
tion this forces on the map, this error is signiﬁcantly improved. On the second lap,
where the rotation of the camera takes it past parts of the scene already mapped,
previously observed features are effortlessly matched in their predicted positions,
the map having settled into a satisfyingly consistent state.

28
2 Points at Inﬁnity: Mosaics Using the Extended Kalman Filter
Fig. 2.7. Superimposed meshes for 90 frames before loop closure (red) and 90 frames after
loop closure (green). The part of the mesh opposite the loop closure is magniﬁed at the left of
the ﬁgure. The ﬁrst features detected in the map are magniﬁed at the right of the ﬁgure. We
also show the camera elevation estimation history along with its standard deviation; notice
the uncertainty reduction at loop closure.
It is worth noting the effect that loop closing has on the mesh, and hence on the
mosaic. Figure 2.7 displays superimposed all of the meshes for the 90 steps before
the loop closure (we plot one in every ﬁve steps in red), along with meshes for the
90 steps after the loop closure (plotted in green). Every sequential step improves
our estimates of the locations of all the map features, but in a loop closure step
the improvement is much greater. As the initial camera orientation is chosen to be
the base reference, the ﬁrst features in the map are very accurately located (with
respect to the ﬁrst camera frame). These features’ locations are hardly modiﬁed
by the loop closure, but all other parts of the mesh are updated signiﬁcantly. We
particularly draw attention to an area of the mesh 180◦opposite the loop closure
point, where the feature estimates and therefore the mesh are noticeably modiﬁed
in the update corresponding to loop closure thanks to the correlation information
held in covariance matrix, despite the fact that these features are not observed in
any nearby time-step.
Figure 2.7 also shows graphs of the estimate history for the two magniﬁed
features, along with the standard deviation in their elevation angles. The feature
far from the loop close area clearly shows a loop closing correction and covari-
ance reduction. The feature observed at the start of the sequence shows almost
no loop closing effect because it was observed when the camera location had low
uncertainty.

2.6 Experimental Results
29
2.6.2
Real-Time Mosaic Building
A version of the system has been implemented in C++ achieving real time perfor-
mance at 320 × 240 pixels resolution, 30 frames/second. We show results from a
360◦pan rotation and map size of around a hundred features.
Figure 2.9 shows the evolution of the mosaic. We focus on the texture alignment
at loop closure. Figures 2.9(g)and 2.9(i) display two magniﬁed mosaic views close
to the loop closure. In each magniﬁed view, the left-hand part of the mosaic seen
got its texture from a frame at the beginning of the sequence while the right area
got texture from a frame after the loop closure (frames nearly a thousand images
apart). The excellent texture alignment achieved is an indicator of the advantages of
our sequential SLAM approach to mosaic building. No blending technique has been
applied to reduce the seam effects.
Figure 2.8 shows an example of robustness with respect to moving objects. A fea-
ture was intialized on the skirt of a walking pedestrian. As the feature corresponds
to a moving object, after some time it is no longer matched inside the acceptance
region, and ﬁnally it is deleted from the map. It is also important to remark that
no outlier rejection technique –like RANSAC or JCBB– was used here to detect the
false match, being its rejection only due to the restricted search inside the gated 95%
probability region.
Fig. 2.8. (a) Feature initalized on a moving object. (b) after 1 and (c) 10 frames, the feature is
no longer matched because it is outside its acceptance region. Eventually this non matching
feature is deleted from the map.

30
2 Points at Inﬁnity: Mosaics Using the Extended Kalman Filter
Fig. 2.9. Real-time mosaicing with loop closure. (a), (b) sequence start; (c) the frame after the
ﬁrst loop closing match; (d) a few frames after loop closing (e) the mosaic after almost two
full laps. (f) (magniﬁcation of (c)) and (g) show two consecutive steps after loop closing; (g) is
a close-up view of two adjacent mosaic areas. In (g) the left-most mosaic tiles got their texture
early in the sequence, while those on the right obtained texture after loop closing. Notice the
high accuracy in texture alignment; the line highlights the seam. (h) (magniﬁcation of (d))
and (i) show two consecutive steps after several loop closing matches. (i) is a close-up view
of two adjacent mosaic areas with textures taken from early and loop closing frames; again
notice the alignment quality. A line highlights the seam —otherwise difﬁcult to observe.
2.6.3
Processing Time
Real-time experiments were run on a 1.8 GHz Pentium laptop with OpenGL accel-
erated graphics card. In a typical run, we might have: a) 70 map features, implying
a state vector dimension of 7 + 70 × 2 = 147. b) 15 features measured per frame,
implying a measurement vector dimension of 15 × 2 = 30. c) 30 fps, so 33.3 ms
available for processing. d) Process noise with standard deviation 4rad s−2 model-
ing expected angular accelerations.

2.7 Discussion
31
Under these conditions, an approximate breakdown of typical processing time
21.5ms per frame is as follows: a)Image acquisition 1 ms. b) EKF prediction 3 ms.
c) Image matching 2 ms. d) EKF update 10 ms. e), f) Delaunay triangulation 0.5 ms.
g) Mosaic update 5ms.
The remaining time processing is used for the graphics functions, scheduled at
low priority, so a graphics refresh might take place every two or three processed
images.
It should be noticed that the computational complexity of the EKF updates is of
order O(N2), where N is the number of map features. The Delaunay triangulation
step has complexity of order O(N logN), while updating the triangles of the mosaic
currently has order O(N2) (since each triangle is compared with every other one,
though it should be possible to improve this). In our algorithm the EKF update
dominates the computational cost. It should be noticed that within the bounds of
a spherical mosaicing problem (where the whole viewsphere can be mapped with
on the order of 100 features) the complexity is well within practical limits for a
standard laptop.
2.7
Discussion
A mosaic built from an elastic triangular mesh sequentially built over a EKF SLAM
map of points at inﬁnity inherits the advantages of the sequential SLAM approach:
probabilistic prior knowledge management through the sequence, sequential updat-
ing, real-time performance and consistent loop closing.
The experimental results presented using real images from a low cost camera
display the validity of the approach in a challenging real scene with jittery hand-
held camera movement, moving people and changing illumination conditions. Real-
time seamless loop closing is demonstrated, removing all the drift from rotation
estimation and allowing arbitrarily long sequences of rotations to be stitched into
mosaics: the camera could rotate all day and estimates would not drift from the
original coordinate frame as long as the high-quality features of the persistent map
could still be observed. We think that as well as its direct application to mosaicing,
this work shows in general the power of the SLAM framework for processing image
sequences and its ability, when compared with off-line methods, to efﬁciently extract
important information: pure sequential processing, long track matches, and loop
closing matches.
EKF-based mosaicing is intended to offer a contribution when compared with
other mosaicing approaches. Full 3D mosaicing in real-time is still some way off,
so we have focused on building 2D mosaics from sequences with homography ge-
ometry — in this piece of work, speciﬁcally from a purely rotating camera, though
we believe that our method could be straightforwardly modiﬁed to also cope with
mosaicing a plane observed by a rotating and translating camera.
We believe that there are many applications which open up with real-time mo-
saicing — in any situation where the goal is to build a living mosaic which is built up

32
2 Points at Inﬁnity: Mosaics Using the Extended Kalman Filter
instantly in reaction to camera motion our approach will be useful. This mosaic can
ﬁnd application especially in augmented reality because it provides a real-time link
between the camera images and real scene points. An interesting application that is
partly based in the algorithms described in this and next chapter is the one described
at [Grasa et al. 2009b, Grasa et al. 2009a]. There, texture patches are attached to a
backbone of 3D points in order to improve visualization of cavities inside human
body, helping the medical team in laparoscopic surgery.
Another interesting possibility real-time mosaicing gives is that a user could con-
trol the rotation of the camera while looking at the growing mosaic in order to ex-
tend and improve it actively. In future work, we intend to look at such issues as
real-time super-resolution, where we envisage a mosaic sharpening before a user’s
eyes thanks to the quality of repeatable rotation registration.

Chapter 3
Inverse Depth Parametrization
Abstract. This chapter presents a parametrization for point features within monocu-
lar SLAM which permits efﬁcient and accurate representation of uncertainty during
undelayed initialisation and beyond, all within the standard EKF (Extended Kalman
Filter). The key concepts are direct parametrization of the inverse depth of features
relative to the camera locations from which they were ﬁrst viewed and the addition
of this camera position at initialization to the state vector. The parametrization ﬁts
the projective nature of the camera in the sense that is able to code the inﬁnite depth
case. Also, the projection equation holds the high degree of linearity required by the
Extended Kalman Filter. The chapter also shows that once the depth estimate of a
feature is sufﬁciently accurate, its representation can safely be converted to the Eu-
clidean XYZ form, and proposes a linearity index which allows automatic detection
and conversion to maintain maximum efﬁciency –only low parallax features need
be maintained in inverse depth form for long periods.
3.1
Introduction
A monocular camera is a projective sensor which measures the bearing of image
features. Given an image sequence of a rigid 3D scene taken from a moving camera,
it is now well known that it is possible to compute both the scene structure and
the camera motion up to a scale factor. To infer the 3D position of each feature,
the moving camera must observe it repeatedly, each time capturing a ray of light
from the feature to its optic center. The measured angle between the captured rays
from different viewpoints is the feature’s parallax –this is what allows its depth to
be estimated.
In off-line Structure from Motion (SfM) solutions from the computer vision
literature (e.g. [Fitzgibbon & Zisserman 1998, Pollefeys et al. 1999]), motion and
J. Civera et al.: Structure from Motion Using the Extended Kalman Filter, STAR 75, pp. 33–63.
springerlink.com
c⃝Springer-Verlag Berlin Heidelberg 2012

34
3 Inverse Depth Parametrization
structure are estimated from an image sequence by ﬁrst applying robust feature
matching between pairs or other short overlapping sets of images to estimate rel-
ative motion. An optimization procedure then iteratively reﬁnes global camera lo-
cation and scene feature position estimates such that features project as closely as
possible to their measured image positions (bundle adjustment). Recently, work in
the spirit of these methods but with “sliding window” processing and reﬁnement
rather than global optimization has produced impressive real-time Visual Odom-
etry results when applied to stereo [Nist´er et al. 2006] and monocular sequences
[Mouragnon et al. 2009].
An alternative approach to achieving real-time motion and structure estimation
are on-line visual SLAM (Simultaneous Localization And Mapping) approaches
which use a probabilistic ﬁltering approach to sequentially update estimates of
the positions of features (the map) and the current location of the camera. These
SLAM methods have different strengths and weaknesses than visual odometry, be-
ing able to build consistent and drift-free global maps but with a bounded number
of mapped features. The core single Extended Kalman Filter (EKF) SLAM tech-
nique, previously proven in multi-sensor robotic applications, was ﬁrst applied suc-
cessfully to real-time monocular camera tracking [Davison et al. 2007] in a sys-
tem which built sparse room-sized maps at 30Hz. The sequential approach to the
Structure from Motion problem has been also the subject of intense research in the
computer vision community, being [Ayache & Faugeras 1989, Matthies et al. 1989,
Broida et al. 1990, Azarbayejani & Pentland 1995, Chiuso et al. 2002] some of the
most representative works.
A signiﬁcant limitation of these early approaches, however, was that they could
only make use of features which were close to the camera relative to its distance of
translation, and therefore exhibited signiﬁcant parallax during motion. The problem
was in initialising uncertain depth estimates for distant features: in the straightfor-
ward Euclidean XYZ feature parametrization adopted, position uncertainties for low
parallax features are not well represented by the Gaussian distributions implicit in
the EKF. The depth coordinate of such features has a probability density which rises
sharply at a welldeﬁned minimum depth to a peak, but then tails off very slowly to-
wards inﬁnity –from low parallax measurements it is very difﬁcult to tell whether a
feature has a depth of 10 units rather than 100, 1000 or more.
This chapter describes a new feature parametrization which is able to cope
smoothly with initialization of features at all depths –even up to “inﬁnity”– within
the standard EKF framework. The key concepts are: First, the direct parametriza-
tion of inverse depth relative to the camera position from which a feature was ﬁrst
observed. And second, the storage in the state vector of the camera position when
the feature was ﬁrst seen.
3.1.1
Delayed and Undelayed Initialization
The most obvious approach to dealing with feature initialization within a monoc-
ular SLAM system is to treat newly detected features separately from the main map,

3.1 Introduction
35
accumulating information in special processing over several frames to reduce depth
uncertainty before insertion into the full ﬁlter with a standard XYZ representation.
Such delayed initialization schemes (e.g. [Davison 2003, Kim & Sukkarieh 2003,
Bryson & Sukkarieh 2005]) have the drawback that new features, held outside the
main probabilistic state, are not able to contribute to the estimation of the camera
position until ﬁnally included in the map. Further, features which retain low parallax
over many frames (those very far from the camera, or close to the motion epipole)
are usually rejected completely because they never pass the test for inclusion. In this
case, far features cannot be represented in the main SLAM map and their valuable
information cannot be incorporated to the ﬁlter.
In the delayed approach of Bailey [Bailey 2003], initialization is delayed until
the measurement equation is approximately Gaussian and the point can be safely
triangulated; here the problem was posed in 2D and validated in simulation. A
similar approach for 3D monocular vision with inertial sensing was proposed in
[Bryson & Sukkarieh 2005]. Davison [Davison 2003] reacted to the detection of a
new feature by inserting a 3D semi-inﬁnite ray into the main map representing ev-
erything about the feature except its depth, and then used an auxiliary particle ﬁlter
to explicitly reﬁne the depth estimate over several frames, taking advantage of all
the measurements in a high frame-rate sequence but again with new features held
outside the main state vector until inclusion.
More recently, several undelayed initialization schemes have been proposed,
which still treat new features in a special way but are able to beneﬁt imme-
diately from them to improve camera motion estimates — the key insight be-
ing that while features with highly uncertain depths provide little information
on camera translation, they are extremely useful as bearing references for ori-
entation estimation. The undelayed method proposed by Kwok and Dissanayake
[Kwok & Dissanayake 2004] was a multiple hypothesis scheme, initializing fea-
tures at various depths and pruning those not reobserved in subsequent images.
[Sol`a et al. 2005, Sol`a 2007] described a more rigorous undelayed approach us-
ing a Gaussian Sum Filter approximated by a Federated Information Sharing method
to keep the computational overhead low. An important insight was to spread the
Gaussian depth hypotheses along the ray according to inverse depth, achieving much
better representational efﬁciency in this way. This method can perhaps be seen as the
direct stepping stone between Davison’s particle method and our new inverse depth
scheme; a Gaussian sum is a more efﬁcient representation than particles (efﬁcient
enough that the separate Gaussians can all be put into the main state vector), but not
as efﬁcient as the single Gaussian representation that the inverse depth parametriza-
tion allows. Note that neither [Kwok & Dissanayake 2004] nor [Sol`a et al. 2005]
consider features at very large ‘inﬁnite’ depths.

36
3 Inverse Depth Parametrization
3.1.2
Points at Inﬁnity
The homogeneous coordinate systems of visual projective geometry used normally
in SFM allow explicit representation of points at inﬁnity, and they have proven to
play an important role during off-line structure and motion estimation. In a sequen-
tial SLAM system, the difﬁculty is that we do not know in advance which features
are inﬁnite and which are not. Chapter 2 has shown that in the special case where
all features are known to be inﬁnite –in very large scale outdoor scenes or when
the camera rotates on a tripod– SLAM in pure angular coordinates turns the camera
into a real-time visual compass. In the more general case, let us imagine a camera
moving through a 3D scene with observable features at a range of depths. From
the estimation point of view, we can think of all features starting at inﬁnity and
“coming in” as the camera moves far enough to measure sufﬁcient parallax. For
nearby indoor features, only a few centimetres of movement will be sufﬁcient. Dis-
tant features may require many meters or even kilometers of motion before parallax
is observed. It is important that these features are not permanently labelled as inﬁ-
nite –a feature that seems to be at inﬁnity should always have the chance to prove its
ﬁnite depth given enough motion, or there will be the serious risk of systematic er-
rors in the scene map. Our probabilistic SLAM algorithm must be able to represent
the uncertainty in depth of seemingly inﬁnite features. Observing no parallax for a
feature after 10 units of camera translation does tell us something about its depth –it
gives a reliable lower bound, which depends on the amount of motion made by the
camera (if the feature had been closer than this we would have observed parallax).
This explicit consideration of uncertainty in the locations of points has not been pre-
viously required in off-line computer vision algorithms, but is very important in the
on-line case.
3.1.3
Inverse Depth Representation
Our contribution is to show that in fact there is a uniﬁed and straightforward
parametrization for feature locations which can handle both initialisation and
standard tracking of both close and very distant features within the standard EKF
framework. An explicit parametrization of the inverse depth of a feature along a
semi-inﬁnite ray from the position from which it was ﬁrst viewed allows a Gaussian
distribution to cover uncertainty in depth which spans a depth range from nearby
to inﬁnity, and permits seamless crossing over to ﬁnite depth estimates of features
which have been apparently inﬁnite for long periods of time. The uniﬁed representa-
tion means that the EKF requires no special initialisation process for features. They
are simply tracked right from the start, immediately contribute to improved cam-
era estimates and have their correlations with all other features in the map correctly
modelled. Note that the inverse depth parameterization would be equally compatible
with other variants of Gaussian ﬁltering such as sparse information ﬁlters.

3.1 Introduction
37
We also introduce in this chapter a linearity index and use it to analyze and prove
the representational capability of the inverse depth parametrization for both low
and high-parallax features. The only drawback of the inverse depth scheme is the
computational issue of increased state vector size, since an inverse depth point needs
six parameters rather than the three of XYZ coding. As a solution to this, we show
that our linearity index can also be applied to the XYZ parametrization to signal
when a feature can be safely switched from inverse depth to XYZ; the usage of
the inverse depth representation can in this way be restricted to low parallax feature
cases where the XYZ encoding departs from Gaussianity. Note that this ‘switching’,
unlike in delayed initialization methods, is purely to reduce computational load;
SLAM accuracy with or without switching is almost the same.
3.1.4
Inverse Depth in Computer Vision and Tracking
Inverse depth is a concept used widely in computer vision: it appears in the rela-
tion between image disparity and point depth in stereo vision; it is interpreted as
the parallax with respect to the plane at inﬁnity in [Hartley & Zisserman 2004]. In-
verse depth is also used to relate the motion ﬁeld induced by scene points with
the camera velocity in optical ﬂow analysis [Heeger & Jepson 1992]. In the track-
ing community, ‘modiﬁed polar coordinates’ [Aidala & Hammel 1983] also exploit
the linearity properties of the inverse depth representation in the slightly different,
but closely related, problem of target motion analysis (TMA) from measurements
gathered by a bearing-only sensor with known motion.
However, the inverse depth idea has not previously been properly integrated in
sequential, probabilistic estimation of motion and structure. It has been used in EKF
based sequential depth estimation from camera known motion [Matthies et al. 1989]
and in multi-baseline stereo Okutomi and Kanade [Okutomi & Kanade 1993] used
the inverse depth to increase matching robustness for scene symmetries; match-
ing scores coming from multiple stereo pairs with different baselines were ac-
cumulated in a common reference coded in inverse depth, this paper focus-
ing on matching robustness and not on probabilistic uncertainty propagation. In
[Chowdhury & Chellappa 2003] it is proposed a sequential EKF process using in-
verse depth but this was some way short of full SLAM in its details. Images are ﬁrst
processed pairwise to obtain a sequence of 3D motions which are then fused with
an individual EKF per feature.
It is our parametrization of inverse depth relative to the positions from which
features were ﬁrst observed which means that a Gaussian representation is uniquely
well behaved, and this is the reason why a straighforward parametrization of
monocular SLAM in the homogeneous coordinates of SFM will not give a good
result — that representation only meaningfully represents points which appear to be
inﬁnite relative to the coordinate origin. It could be said in projective terms that our
method deﬁnes separate but correlated projective frames for each feature. Another
interesting comparison is between this inverse depth representation, where the

38
3 Inverse Depth Parametrization
representation for each feature includes the camera position from which it was
ﬁrst observed and smoothing/Full SLAM schemes where all historical sensor pose
estimates are maintained in a ﬁlter.
There are two works that appeared simultaneously with the one presented here –
ﬁrst presented in [Montiel et al. 2006, Civera et al. 2007a, Civera et al. 2008a]– that
share the underlying idea of the inverse depth coding. Trawny and Roumeliotis in
[Trawny & Roumeliotis 2006] proposed an undelayed initialization for 2D monoc-
ular SLAM which encodes a map point as the intersection of two projection rays.
This representation is overparametrized but allows undelayed initialization and en-
coding of both close and distant features, the approach validated with simulation
results.
Eade and Drummond presented an inverse depth initialisation scheme
within the context of their FastSLAM-based system for monocular SLAM
[Eade & Drummond 2006], offering some of the same arguments about advantages
in linearity as in this chapter. The position of each new partially initialised feature
added to the map is parametrized with three coordinates representing its direction
and inverse depth relative to the camera pose at the ﬁrst observation, and estimates
of these coordinates are reﬁned within a set of Kalman Filters for each particle of
the map. Once the inverse depth estimation has collapsed, the feature is converted
to a fully initialised standard XYZ representation. While retaining the differentia-
tion between partially and fully-initialised features, they go further and are able to
use measurements of partially initialised features with unknown depth to improve
estimates of camera orientation and translation via a special epipolar update step.
Their approach certainly appears appropriate within a FastSLAM implementation.
However, it lacks the satisfying uniﬁed quality of the parametrization we present in
this chapter, where the transition from partially to fully initialised need not be ex-
plicitly tackled and full use is automatically made of all of the information available
in measurements.
The rest of the chapter is organized as follows: Section 3.2 deﬁnes the state vec-
tor, including the camera motion model, XYZ point coding and inverse depth point
parametrization. The measurement equation is described in section 3.3. Section 3.4
presents the linearity index we will use for further insight on the superiority of the
inverse depth coding and for conversion to XYZ parametrization. Feature initializa-
tion from a single feature observation is detailed in section 3.5. In section 3.6 the
switch from inverse depth to XYZ coding is presented. Section 3.7 details of the ex-
plotation of the Bayesian priors in the matching step: the patch warping algorithm in
subsection 3.7.1 and the active search for correspondences [Davison et al. 2007] in
subsection 3.7.2. In section 3.8 experimental validation over real image sequences
captured at 30Hz in large scale environments, indoors and outdoors, including real-
time performance and a loop closing experiment is presented. Finally, section 3.9 is
devoted to summarize and discuss the contributions in the chapter.

3.2 State Vector Deﬁnition
39
3.2
State Vector Deﬁnition
3.2.1
Camera Motion
A constant angular and linear velocity model is used to model hand-held camera
motion. The camera state xC is composed of pose terms: rWC camera optical center
position and qWC quaternion deﬁning orientation; and linear and angular velocity
vW and ωC relative to world frame W and camera frame C.
We assume that linear and angular accelerations aW and αC affect the camera,
producing at each step an impulse of linear velocity, VW = aWΔt, and angular ve-
locity ΩC = αCΔt, with zero mean and known Gaussian distribution. We currently
assume a diagonal covariance matrix for the unknown input linear and angular ac-
celerations.
The state update equation for the camera is:
xCk+1 =
⎛
⎜
⎜
⎝
rWC
k+1
qWC
k+1
vW
k+1
ωC
k+1
⎞
⎟
⎟
⎠= fv

xCk,n

=
⎛
⎜
⎜
⎝
rWC
k
+

vW
k +VW
k

Δt
qWC
k
×q

ωC
k + ΩC
Δt

vW
k +VW
ωC
k +ΩC
⎞
⎟
⎟
⎠.
(3.1)
where q

ωC
k +ΩC
Δt

is the quaternion deﬁned by the rotation vector

ωC
k + ΩC
Δt.
3.2.2
Euclidean XYZ Point Parametrization
The standard representation for scene points i in terms of Euclidean XYZ coordi-
nates (see Figure3.1) is:
yW
XYZ,i =

Xi Yi Zi
⊤.
(3.2)
Along the book, we sometimes refer to the Euclidean XYZ coding simply as XYZ
coding.
3.2.3
Inverse Depth Point Parametrization
In the model presented in this chapter, a 3D point in the scene i is deﬁned by a state
vector with 6 parameters:
yW
ρ,i =
 xi yi zi θi φi ρi
⊤,
(3.3)
which Cartesian coordinates are (see Figure 3.1):

40
3 Inverse Depth Parametrization
⎛
⎝
Xi
Yi
Zi
⎞
⎠=
⎛
⎝
xi
yi
zi
⎞
⎠+ 1
ρi
m(θi,φi)
(3.4)
m =

cosφi sinθi
−sinφi
cosφi cosθi
⊤.
(3.5)
The vector yW
ρ,i encodes the ray from the ﬁrst camera position from which the
feature was observed by xi,yi,zi, the camera optical center, and θi,φi azimuth and
elevation (coded in the world frame) deﬁning unit directional vector m(θi,φi). The
point’s depth along the ray di is encoded by its inverse ρi = 1/di.
It is important to remark here that this chapter represents every geometric entity
with respect to a static world reference frameW. Referring every camera and feature
parameters to a dynamic reference frame Ck attached to the camera improves results
when using the EKF [Castellanos et al. 2004]; so next chapter will also consider this
other case.
3.2.4
Full State Vector
As in standard EKF SLAM, we use a single joint state vector containing camera
pose and feature estimates, with the assumption that the camera moves with respect
to a static scene. The whole state vector x is composed of the camera and all the
map features:
x =

x⊤
C ,y⊤
1 ,...y⊤
i ,...y⊤
n
⊤
.
(3.6)
3.3
Measurement Equation
Each observed feature imposes a constraint between the camera location and the
corresponding map feature (see Figure3.1). Observation of a point yi (xi) deﬁnes a
ray coded by a directional vector in the camera frame hC =
hx hy hz
⊤. For points
in XYZ:
hC = hC
XYZ = RCW
⎛
⎝
⎛
⎝
Xi
Yi
Zi
⎞
⎠−rWC
⎞
⎠.
(3.7)
For points in inverse depth:
hC = hC
ρ = RCW
⎛
⎝ρi
⎛
⎝
⎛
⎝
xi
yi
zi
⎞
⎠−rWC
⎞
⎠+m(θi,φi)
⎞
⎠,
(3.8)
where the directional vector has been normalized using the inverse depth. It is worth
noting that (3.8) can be safely used even for points at inﬁnity i.e ρi = 0.

3.4 Measurement Equation Linearity
41
Fig. 3.1. Feature parametrization and measurement equation.
The camera does not directly observe hC but its projection in the image according
to the pinhole model. Projection to a normalized retina and then camera calibration
is applied:
h =
u
v

=

Cx −f
dx
hx
hz
Cy −f
dy
hy
hz

,
(3.9)
where (Cx Cy)⊤is the camera’s principal point, f is the focal length and (dx dy)⊤
the pixel size. Finally, a distortion model has to be applied to deal with real camera
lenses. In this work we have used the standard two parameter distortion model from
photogrammetry [Mikhail et al. 2001] (see Appendix for details.)
It is worth noting that the measurement equation in inverse depth has a sensitive
dependency on the parallax angle α (see Figure 3.1). At low parallax, Equation
(3.8) can be approximated by hC ≈RCW (m(θi,φi)), and hence the measurement
equation only provides information about the camera orientation and the directional
vector m(θi,φi).
3.4
Measurement Equation Linearity
The higher the degree of linearity of the measurement equation is, the better the
Kalman Filter performs. This section presents an analysis of the degree of linearity

42
3 Inverse Depth Parametrization
for both XYZ and inverse depth parametrizations. These linearity analyses theoret-
ically support the superiority of the inverse depth coding.
3.4.1
Linearized Propagation of a Gaussian
Let x be an uncertain variable with Gaussian distribution x ∼N

μx,σ2
x

. The trans-
formation of x through the function f is a variable y which can be approximated
with a Gaussian distribution
y ∼N

μy,σ2
y

, μy = f (μx), σ2
y = ∂f
∂x

μx
σ2
x
∂f
∂x

⊤
μx
,
(3.10)
if the function f is linear in an interval around μx (Figure 3.2). The interval size
in which the function has to be linear depends on σx; the bigger σx the wider the
interval has to be to cover a signiﬁcant fraction of the random variable x values.
In this work we ﬁx the linearity interval to the 95% conﬁdence region deﬁned by
[μx −2σx,μx +2σx].
If a function is linear in an interval, the ﬁrst derivative is constant in that inter-
val. To analyze the ﬁrst derivative variation around the interval [μx −2σx,μx + 2σx]
consider the Taylor expansion for the ﬁrst derivative:
∂f
∂x (μx +Δx) ≈∂f
∂x

μx
+ ∂2 f
∂x2

μx
Δx .
(3.11)
We propose to compare the value of the derivative at the interval center, μx, with the
value at the extremes μx ±2σx, where the deviation from linearity will be maximal,
using the following dimensionless linearity index:
Fig. 3.2. The ﬁrst derivative variation in [μx −2σx,μx +2σx] codes the departure from Gaus-
sianity in the propagation of the uncertain variable through a function.

3.4 Measurement Equation Linearity
43
L =

∂2 f
∂x2

μx 2σx
∂f
∂x

μx

.
(3.12)
When L ≈0, the function can be considered linear in the interval, and hence Gaus-
sianity is preserved during transformation.
3.4.2
Linearity of XYZ Parametrization
The linearity of the XYZ representation is analyzed by means of a simpliﬁed model
which only estimates the depth of a point with respect to the camera. In our analysis,
a scene point is observed by two cameras (Figure 3.3a), both of which are oriented
towards the point. The ﬁrst camera detects the ray on which the point lies. The
second camera observes the same point from a distance d1; the parallax angle α is
approximated by the angle between the cameras’ optic axes.
The point’s location error, d, is encoded as Gaussian in depth:
D = d0 + d,
d ∼N

0,σ2
d

.
(3.13)
This error d is propagated to the image of the point in the second camera, u as:
u = x
y =
d sinα
d1 +d cosα .
(3.14)
Fig. 3.3. Uncertainty propagation from the scene point to the image. (a) XYZ coding. (b)
Inverse depth coding.

44
3 Inverse Depth Parametrization
The Gaussianity of u is analyzed by means of (3.12), giving linearity index:
Ld =

∂2u
∂d2 2σd
∂u
∂d
 = 4σd
d1
|cosα|
(3.15)
3.4.3
Linearity of Inverse Depth Parametrization
The inverse depth parametrization is based on the same scene geometry as the direct
depth coding, but the depth error is encoded as Gaussian in inverse depth (Figure
3.3b):
D =
1
ρ0 −ρ ,
ρ ∼N

0,σ2
ρ

(3.16)
d = D−d0 =
ρ
ρ0 (ρ0 −ρ), d0 = 1
ρ0
.
(3.17)
So the image of the scene point is computed as:
u = x
y =
d sinα
d1 + d cosα =
ρ sinα
ρ0d1 (ρ0 −ρ)+ρ cosα ,
(3.18)
and the linearity index Lρ is now:
Lρ =

∂2u
∂ρ2 2σρ
∂u
∂ρ

= 4σρ
ρ0
1−d0
d1
cosα
 .
(3.19)
3.4.4
Depth vs. Inverse Depth Comparison
When a feature is initialized, the depth prior has to cover a vast region in front of
the camera. With the inverse depth representation, the 95% conﬁdence region with
parameters ρ0, σρ is:

1
ρ0 +2σρ
,
1
ρ0 −2σρ

.
(3.20)
This region cannot include zero depth but can easily extend to inﬁnity.
Conversely, with the depth representation the 95% region with parameters d0,
σd is [d0 −2σd,d0 +2σd]. This region can include zero depth but cannot extend to
inﬁnity.
In the ﬁrst few frames after a new feature has been initialized, little parallax is
likely to have been observed. Therefore d0
d1 ≈1 and α ≈0 =⇒cosα ≈1. In this
case the Ld linearity index for depth is high (bad), while the Lρ linearity index for

3.5 Feature Initialization
45
inverse depth is low (good): during initialization the inverse depth measurement
equation linearity is superior to the XYZ coding.
As estimation proceeds and α increases, leading to more accurate depth esti-
mates, the inverse depth representation continues to have a high degree of linearity.
This is because in the expression for Lρ the increase in the term
1−d0
d1 cosα
 is
compensated by the decrease in 4σρ
ρ0 . For inverse depth features a good linearity in-
dex is achieved along the whole estimation history. So the inverse depth coding is
suitable for both low and high parallax cases if the feature is continuously observed.
The XYZ encoding has low computational cost, but achieves linearity only at low
depth uncertainty and high parallax. In section 3.6 we explain how the representation
of a feature can be switched over such that the inverse depth parametrization is only
used when needed — for features which are either just initialized or at extreme
depths.
3.5
Feature Initialization
From just a single observation no feature depth can be estimated (although it would
be possible in principle to impose a very weak depth prior by knowledge of the
type of scene observed). What we do is to assign a general Gaussian prior in inverse
depth which encodes probabilistically the fact that the point has to be in front of the
camera. Hence, thanks to the linearity of inverse depth at low parallax, the ﬁlter can
be initialized from just one observation. Experimental tuning has shown that inﬁnity
should be included with reasonable probability within the initialization prior, despite
the fact that this means that depth estimates can become negative. Once initialized,
features are processed with the standard EKF prediction-update loop — even in the
case of negative inverse depth estimates — and immediately contribute to camera
location estimation within SLAM.
It is worth noting that while a feature retains low parallax, it will automatically
be used mainly to determine the camera orientation. The feature’s depth will re-
main uncertain, with the hypothesis of inﬁnity still under consideration (represented
by the probability mass corresponding to negative inverse depths). If the camera
translates to produce enough parallax then the feature’s depth estimation will be
improved and it will begin to contribute more to camera location estimation.
The initial location for a newly observed feature inserted into the state vector is:
ˆy
ˆrWC, ˆqWC,zi,ρ0

=

ˆxi ˆyi ˆzi ˆθi ˆφi ˆρi
⊤,
(3.21)
a function of the current camera pose estimate ˆrWC, ˆqWC, the image observation
zi = (ui vi )⊤and the parameters determining the depth prior ρ0, σρ0.
The end-point of the initialization ray (see Figure 3.1) is taken from the current
camera location estimate:

46
3 Inverse Depth Parametrization

ˆxi ˆyi ˆzi
⊤= ˆrWC ,
(3.22)
and the direction of the ray is computed from the observed point, expressed in the
world coordinate frame:
hW = RWC
ˆqWCυi νi 1
⊤,
(3.23)
where υi and νi are normalized retina image coordinates. Despite hW being a non-
unit directional vector, the angles by which we parametrize its direction can be cal-
culated as:
 ˆθi
ˆφi

=
⎛
⎝
arctan

hW
x ,hW
z

arctan

−hW
y ,

hWx
2 + hWz
2

⎞
⎠.
(3.24)
The covariance of ˆxi, ˆyi, ˆzi, ˆθi and ˆφi is derived from the image measurement error
covariance Ri and the state covariance estimate ˆPk|k.
The initial value for ρ0 and its standard deviation are set such that the 95% con-
ﬁdence region spans a range of depths from a close distance to the camera up to
inﬁnity. In our experiments of this chapter we chose ˆρ0 = 0.1,σρ0 = 0.5, which
gives an inverse depth conﬁdence region [1.1,−0.9]. Notice that inﬁnity is included
in this range. Nevertheless, further experimental validation has shown that the pre-
cise values of these parameters are relatively unimportant to the accurate operation
of the ﬁlter as long as inﬁnity is clearly included in the conﬁdence interval.
The state covariance after feature initialization is:
Pnew
k|k
= J
⎛
⎝
Pk|k 0
0
0
Ri
0
0
0 σ2
ρ0
⎞
⎠J⊤
(3.25)
J =

I
0
∂y
∂rWC ,
∂y
∂qWC ,0,...,0, ∂y
∂zi , ∂y
∂ρ

.
(3.26)
The inherent scale ambiguity in monocular SLAM has usually been ﬁxed by
observing some known initial features that ﬁx the scale (e.g. [Davison 2003]). A
very interesting experimental observation we have made using the inverse depth
scheme is that sequential monocular SLAM can operate successfully without any
known features in the scene, and in fact the experiments we present in this book do
not use an initialization target. In this case the overall scale of the reconstruction and
camera motion is undetermined,although with the formulation of the current chapter
the estimation will settle on a (meaningless) scale of some value. Appendix B details
an alternative formulation of EKF SfM that illustrates this issue via a dimensional
analysis of the problem and the proposal of a dimensionless formulation.

3.6 Switching from Inverse Depth to XYZ
47
3.6
Switching from Inverse Depth to XYZ
While the inverse depth encoding can be used at both low and high parallax, it is
advantageous for reasons of computational efﬁciency to restrict inverse depth to
cases where the XYZ encoding exhibits non linearity according to the Ld index.
This section details switching from inverse depth to XYZ for high parallax features.
3.6.1
Conversion from Inverse Depth to XYZ Coding
After each estimation step, the linearity index Ld (Equation 3.15) is computed for
every map feature coded in inverse depth:
hW
XYZ = ˆyW
XYZ,i −ˆrWC, σd = σρ
ρ2
i
, σρ =

Pyiyi (6,6)
di =
hW
XYZ
, cosα = m⊤hW
XYZ
hW
XYZ
−1 .
(3.27)
where ˆyW
XYZ,i is computed using equation (3.4) and Pyiyi is the submatrix 6 × 6 co-
variance matrix corresponding the considered feature.
If Ld is below a switching threshold, the feature is transformed using Equation
(3.4) and the full state covariance matrix P is transformed with the corresponding
Jacobian:
Pnew = JPJ⊤,
J = diag

I, ∂yXYZ,i
∂yρ,i
,I

.
(3.28)
3.6.2
Linearity Index Threshold
We propose to use index Ld (3.15) to deﬁne a threshold for switching from inverse
depth to XYZ encoding at the point when the latter can be considered linear. If
the XYZ representation is linear, then the measurement u is Gaussian distributed
(Equation 3.10):
u ∼N

μu,σ2
u

, μu = 0, σ2
u =
sinα
d1
2
σ2
d .
(3.29)
To determine the threshold in Ld which signals a lack of linearity in the
measurement equation a simulation experiment has been performed. The goal was
to generate samples from the uncertain distribution for variable u and then apply a
standard Kolmogorov-Smirnov Gaussianity test [Canavos 1984] to these samples,
counting the percentage of rejected hypotheses h. When u is effectively Gaussian,
the percentage should match the test signiﬁcance level αsl (5% in our experiments);

48
3 Inverse Depth Parametrization
Fig. 3.4. Percentage of test rejections as a function of the linearity index Ld
as the number of rejected hypotheses increases the measurement equation departs
from linearity. A plot of the percentage of rejected hypotheses h with respect to the
linearity index Ld is shown in Figure 3.4. It can be clearly seen than when Ld > 0.2, h
sharply departs from 5%. So we propose the Ld < 10% safe threshold for switching
from inverse depth to XYZ encoding.
Notice that the plot in Figure 3.4 is smooth (log scale in Ld), which indicates that
the linearity index effectively represents the departure from linearity.
The simulation has been performed for a variety of values of α, d1 and σd; more
precisely all triplets resulting from the following parameter values:
α(deg) ∈{0.1,1,3,5,7,10,20,30,40,50,60,70}
d1(m) ∈{1,3,5,7,10,20,50,100}
σd(m) ∈{0.05,0.1,0.25,0.5,0.75,1,2,5} .
The simulation algorithm detailed in Figure 3.5 is applied to every triplet
{α,d1,σd} to count the percentage of rejected hypotheses h and the correspond-
ing linearity index Ld.
3.7
Data Association
The information contained in the updated probability distribution that ﬁltering-based
approaches maintain along the image sequence can be exploited in the matching
step, resulting in an increase of robustness at a lower computational cost. Follow-
ing the terminology in [Williams et al. 2008], correspondences are not searched in
an image–to–image but in an image–to–map basis that takes into account the prior
knowledge over the structure of the scene and predicted camera motion.

3.7 Data Association
49
input: α,d1,σd
output: h, Ld
σu =
 sinα
d1
σd; μu = 0; //(3.29)
αsl = 0.05; // Kolm. test sign. level
Ld = 4σd
d1 |cosα|
n rejected=0 ;
N GENERATED SAMPLES=1000;
SAMPLE SIZE=1000;
for j=1 to N GENERATED SAMPLES repeat
{di} j=random normal(0,σ2
d,SAMPLE SIZE);
//generate a normal sample from N

0,σ2
d

;
{ui} j=propagate from dept to image({di} j,α,d1);//(3.14)
if rejected==Kolmogorov Smirnov({ui}j ,μu,σu,αsl)
n rejected=n rejected+1;
endfor
h=100
[n re jected]
[N GENERATED SAMPLES];
Fig. 3.5. Simulation algorithm to test the linearity of the measurement equation.
Using this prior information, the patch which serves as photometric identiﬁer of
the feature can be warped according to the predicted camera motion facilitating the
correspondence search under large projective distortions. Also, the propagation of
the probabilistic density over the state through the projection model can be used to
deﬁne small regions of high probability for ﬁnding the match –this is called active
search in [Davison et al. 2007].
3.7.1
Patch Warping
The performance of any SfM system relies on its ability to ﬁnd correct corre-
spondences of a 3D feature in several images. Finding such correspondences suc-
cesfully becomes more difﬁcult as the motion between images increases, as the
appearance of the feature in the image strongly vary. Pairwise SfM methods,
that may take as input widely separated images, usually make use of descriptors
with a high degree of invariance –at the cost of a more expensive processing–
[Lowe 2004, Bay et al. 2008, Mikolajczyk et al. 2005].
The sequential processing of video sequences of this book makes the use of these
invariant descriptors unnecesary. Although sequential SfM from videos also faces
the problem of searching correspondences in widely separated images, it can ben-
eﬁt from the registration of past images and a dynamic motion between frames.
As a difference from pairwise SfM, the correspondence search does not have to
start “from scratch” without knowing the scale or rotation for each match. The

50
3 Inverse Depth Parametrization
appearance of each feature can be accurately predicted using the relative motion
between cameras and the 3D feature.
The use of highly invariant descriptors does not offer any advantages in sequential
SfM or monocular SLAM; and it is advisable to use simple descriptors and predict
their projective distortion using the geometric estimation. In [Chekhlov et al. 2007],
patches at different scales are generated when a feature is ﬁrst detected and
are selected in the matching stage according to the predicted scale change. In
[Molton et al. 2004], the patch is warped according to the predicted motion and also
the normal to the local patch is estimated.
In the experiments of this book, the feature patches are warped according to the
estimation in order to improve the matching. Differently from [Molton et al. 2004],
we do not estimate the normal of the patch but approximate it by the bisector of the
angle formed by the two projection rays. We ﬁnd that this simpliﬁcation success-
fully copes with the most problematic motions (e.g., cyclotorsion) and its general
performance do not degrade much compared with more elaborated approaches. The
warping or the patch is computed via the following homography
H = K

RCiCk −tCiCkn⊤/di

K−1 ,
(3.30)
where K stands for the known calibration matrix.
K =
⎛
⎝
f/dx
0
Cx
0
f/dy Cy
0
0
1
⎞
⎠.
(3.31)
RCiCk and tCiCk are the relative rotation and translation between the camera Ci
where the feature was initialised and the current one Ck. Both can be computed from
the current camera rotation RWCk and translation tWCk and the ones at the feature
initialization RWCi and tWCi. di is the distance from the ﬁrst camera to the point, that
can be extracted easily from the state vector –it is the inverse of the inverse depth
for this feature ρi. n stands for the normal of the patch, and it is approximated as
described in the previous paragraph.
3.7.2
Active Search
Active search or guided matching is achieved in ﬁltering-based visual estimation by
projecting into the image the probabilistic state vector. The 95% probability region
from this distribution results in small elliptical search regions. Searching the cor-
respondences inside the ellipses reduces the computational cost and also prevents
many outliers to appear. Although the spurious rate reduces drastically by this ap-
proach, it is important to remark that a complete rejection of the outliers cannot
be guaranteed and an additional step that checks the consensus of the data against
a global model is still needed. Next chapter further elaborates on this issue and
presents an efﬁcient matching algorithm based on RANSAC.

3.8 Experimental Results
51
The search region is obtained by propagating ﬁrst the EKF Gaussian prediction
N
ˆxk|k−1,Pk|k−1

through the measurement equation h(x), resulting in a multidi-
mensional Gaussian in the image N
ˆhk|k−1,Sk|k−1

ˆhk|k−1 = h(ˆxk|k−1)
(3.32)
Sk|k−1 = Hk|k−1Pk|k−1H⊤
k|k−1 +Rk .
(3.33)
Hk|k−1 are the derivatives for the measurement equation in the prediction and Rk
the image noise covariance at step k. The individual search regions for a feature i are
the parameters corresponding to the 2D mean ˆhi and covariance Si extracted from
the multivariate distribution.
It can be seen in any of the ﬁgures in the experimental results section where
images are displayed –Figures 3.6, 3.8 and 3.10– and other ﬁgures along the book
–e.g. Figures 2.9, 4.10 and 6.4– the small ellipses around feature image predictions
where the matches are looked for. Efﬁciency gain come across straightforwardly if
it is compared the size of these ellipses with the size of the whole image.
3.8
Experimental Results
The performance of the inverse depth parametrization has been tested on real image
sequences acquired with a hand-held low cost Unibrain IEEE1394 camera, with a
90◦ﬁeld of view and 320×240 resolution, capturing monochrome image sequences
at 30 fps.
Five experiments has been performed. The ﬁrst is an indoor sequence processed
ofﬂine with a Matlab implementation, the goal being to analyze initialization of
scene features located at different depths. The second experiment shows an outdoor
sequence processed in real-time with a C++ implementation. The focus here is on
Fig. 3.6. First (a) and last (b) frame in the sequence of the indoor experiment of section 3.8.1.
Features 11,12, 13 are analyzed. These features are initialized in the same frame but are
located at different distances from the camera.

52
3 Inverse Depth Parametrization
distant features, observed under low parallax along the whole sequence. The third
experiment is a loop closing sequence, concentrating on camera covariance evo-
lution. Fourth is a simulation experiment to analyze the effect of switching from
inverse depth to XYZ representations. In the last experiment the switching perfor-
mance is veriﬁed on the real loop closing sequence. The section ends with a com-
puting time analysis.
3.8.1
Indoor Sequence
This experiment analyzes the performance of the inverse depth scheme as several
features at a range of depths are tracked within SLAM. We discuss three features,
which are all detected in the same frame but have very different depths. Figure
3.6 shows the image where the analyzed features are initialized (frame 18 in the
−2
0
2
4
6
8
−5
0
5
10
15
Step: 1
Feature #11
−2
0
2
4
6
8
−5
0
5
10
15
Feature #12
−2
0
2
4
6
8
−5
0
5
10
15
Feature #13
−2
0
2
4
6
8
−5
0
5
10
15
Step: 10
−2
0
2
4
6
8
−5
0
5
10
15
−2
0
2
4
6
8
−5
0
5
10
15
−2
0
2
4
6
8
−5
0
5
10
15
Step: 25
−2
0
2
4
6
8
−5
0
5
10
15
−2
0
2
4
6
8
−5
0
5
10
15
−2
0
2
4
6
8
−5
0
5
10
15
Step: 100
−2
0
2
4
6
8
−5
0
5
10
15
−2
0
2
4
6
8
−5
0
5
10
15
α = 0.0
α = 1.9
α = 0.7
α = 9.7
α = 0.0
α = 0.0
α = 0.4
α = 1.1
α = 5.8
α = 0.3
α = 0.8
α = 4.1
Fig. 3.7. Feature initialization. Each column shows the estimation history for a feature hor-
izontal components. For each feature, the estimates after 1, 10, 25 and 100 frames since
initialization are plotted; the parallax angle α in degrees between the initial observation and
the current frame is displayed. The thick (red) lines show (calculated by a Monte Carlo nu-
merical simulation) the 95% conﬁdence region when coded as Gaussian in inverse depth.
The thin (black) ellipsoids show the uncertainty as a Gaussian in XYZ space propagated ac-
cording to Equation (3.28). Notice how at low parallax the inverse depth conﬁdence region is
very different from the elliptical Gaussian. However, as the parallax increases, the uncertainty
reduces and collapses to the Gaussian ellipse.

3.8 Experimental Results
53
sequence) and the last image in the sequence. Figure 3.7 focuses on the evolution
of the estimates corresponding to the features, with labels 11, 12 and 13, at frames
1, 10, 25 and 100. Conﬁdence regions derived from the inverse depth representa-
tion (thick red line) are plotted in XYZ space by numerical Monte Carlo propa-
gation from the six-dimensional multivariate Gaussians representing these features
in the SLAM EKF. For comparison, standard Gaussian XYZ acceptance ellipsoids
(thin black line) are linearly propagated from the six-dimensional representation by
means of the Jacobian of equation (3.28). The parallax α in degrees for each feature
at every step is also displayed.
When initialized, the 95% acceptance region of all the features includes ρ = 0
so inﬁnite depth is considered as a possibility. The corresponding conﬁdence region
in depth is highly asymmetric, excluding low depths but extending to inﬁnity. It is
clear that Gaussianity in inverse depth is not mapped to Gaussianity in XYZ, so the
black ellipsoids produced by Jacobian transformation are far from representing the
true depth uncertainty. As stated in section 3.4.4, it is at low parallax that the inverse
depth parametrization plays a key role.
As rays producing bigger parallax are gathered, the uncertainty in ρ becomes
smaller but still maps to a non-Gaussian distribution in XYZ. Eventually, at high
parallax, for all of the features the red conﬁdence regions become closely Gaussian
and well-approximated by the linearly-propagated black ellipses — but this happens
much sooner for nearby feature 11 than distant feature 13.
3.8.2
Real-Time Outdoor Sequence
This 860 frame experiment was performed with a C++ implementation which
achieves real-time performance at 30 fps with hand-held camera. Here we high-
light the ability of our parametrization to deal with both close and distant features
in an outdoor setting.
Figure 3.8 shows two frames of the sequence along with the estimation results at
those steps. For most of the features, the camera ended up gathering enough parallax
to accurately estimate their depths. However, being outdoors, there were distant
features producing low parallax during the whole camera motion.
The inverse depth estimation history for two features is highlighted in Figure 3.9.
It is shown that distant, low parallax features are persistently tracked through the
sequence, despite the fact that their depths cannot be precisely estimated. The large
depth uncertainty, represented with the inverse depth scheme, is successfully man-
aged by the EKF SLAM, allowing the orientation information supplied by these
features to be exploited.
Feature 3, on a nearby car, eventually gathers enough parallax to have an accurate
depth estimate after 250 images where inﬁnite depth was still considered as a possi-
bility. Meanwhile the estimate of feature 11, on a distant tree and never displaying
signiﬁcant parallax, never collapses in this way and zero inverse depth remains
within its conﬁdence region. Delayed intialization schemes would have discarded

54
3 Inverse Depth Parametrization
Fig. 3.8. Subﬁgures (a) and (b) show frames #163 and #807 from the outdoor experiment of
section 3.8.2. This experiment was processed in real time. The focus was two features: 11
(tree on the left) and 3 (car on the right) at low parallax. Each of the two subﬁgures shows the
current images, and top-down views illustrating the horizontal components of the estimation
of camera and feature locations at three different zoom scales for clarity: the top-right plots
(maximum zoom) highlight the estimation of the camera motion; bottom-left (medium zoom)
views highlight nearby features; and bottom-right (minimum zoom) emphasizes distant
features.

3.8 Experimental Results
55
Fig. 3.9. Analysis of outdoor experiment of section 3.8.2. (a) Inverse depth estimation history
for feature 3, on the car, and (b) for feature 11, on a distant tree. Due to the uncertainty
reduction during estimation, two plots at different scales are shown for each feature. It is
show the 95% conﬁdence region, and with a thick line the estimated inverse depth. The thin
solid line is the inverse depth estimated after processing the whole sequence. In (a), for the
ﬁrst 250 steps, zero inverse depth is included in conﬁdence region, meaning that the feature
might be at inﬁnity. After this, more distant but ﬁnite locations are gradually eliminated, and
eventually the feature’s depth is accurately estimated. In (b), the tree is so distant that the
conﬁdence region always includes zero, since little parallax is gathered for that feature.
this feature without obtaining any information from it, while in our system it behaves
like a bearing reference. This ability to deal with distant points in real time is a
highly advantageous quality of our parametrization. Note that what does happen to
the estimate of Feature 11 as translation occurs is that hypotheses of nearby depths
are ruled out — the inverse depth scheme correctly recognizes that measuring little
parallax while the camera has translated some distance allows a minimum depth for
the feature to be set.
3.8.3
Loop Closing Sequence
A loop closing sequence offers a challenging benchmark for any SLAM algorithm.
In this experiment a hand-held camera was carried by a person walking in small
circles within a very large student laboratory, carrying out two complete laps.
Figure 3.10 shows a selection of the 737 frames from the sequence, concentrating
on the beginning, ﬁrst loop closure and end of the sequence. Figure 3.11 shows the
camera location estimate covariance history, represented by the 95% conﬁdence
regions for the 6 camera d.o.f. and expressed in a reference local to the camera.
We observe the following properties of the evolution of the estimation, focussing
in particular on the uncertainty in the camera location:
• After processing the ﬁrst few images, the uncertainty in the features’ depth is
still huge, with highly non-elliptical conﬁdence regions in XYZ space (Figure
3.10(a)).
• In Figure 3.11 the ﬁrst peak in the X and Z translation uncertainty corresponds
to a camera motion backwards along the optical axis; this motion produces
poor parallax for newly initialized features, and we therefore see a reduction in

56
3 Inverse Depth Parametrization
Fig. 3.10. A selection of frames from the loop closing experiment of section 3.8.3. For each
frame, we show the current image and the projection of the estimated map (left), and a top-
down view of the map with 95% conﬁdence regions and camera trajectory (right). Notice
that conﬁdence regions for the map features are far from being Gaussian ellipses, especially
for newly initialized or distant features. The selected frames are: (a) #11, close to the start
of the sequence; (b) #417, where the ﬁrst loop closing match, corresponding to a distant
feature, is detected; the loop closing match is signaled with an arrow; (c) #441 where the ﬁrst
loop closing match corresponding to a close feature is detected; the match is signaled with
an arrow; and (d) #737, the last image, in the sequence, after reobserving most of the map
features during the second lap around the loop.

3.8 Experimental Results
57
Fig. 3.11. Camera location estimate covariance along the sequence. The 95% conﬁdence
regions for each of the 6 d.o.f of camera motion are plotted. Note that errors are expressed in
a reference local to the camera. The vertical solid lines indicate the loop closing frames #417
and #441.
orientation uncertainty and an increase in translation uncertainty. After frame
#50 the camera again translates in the X direction, parallax is gathered and the
translation uncertainty is reduced.
• From frame #240, the camera starts a 360◦circular motion in the XZ plane.
The camera explores new scene regions, and the covariance increases steadily as
expected (Figure3.11).
• In frame #417, the ﬁrst loop closing feature is re-observed. This is a feature
which is distant from the camera, and causes an abrupt reduction in orientation
and translation uncertainty (Figure 3.11), though a medium level of uncertainty
remains.
• In frame #441, a much closer loop closing feature (mapped with high parallax)
is matched. Another abrupt covariance reduction takes place (Figure3.11) with
the extra information this provides.
• After frame #441, as the camera goes on a second lap around the loop, most of
the map features are revisited, almost no new features are initalized, and hence
the uncertainty in the map is further reduced. Comparing the map at frame #441
(the beginning of the second lap) and at #737, (the end of the second lap), we
see a signiﬁcant reduction in uncertainty. During the second lap, the camera

58
3 Inverse Depth Parametrization
uncertainty is low, and as features are reobserved their uncertainties are notice-
ably reduced (Figure3.10(c) and (d)).
Note that these loop closing results with the inverse depth representation show a
marked improvement on the experiments on monocular SLAM with a humanoid
robot presented in [Davison et al. 2007], where a gyro was needed in order to reduce
angular uncertainty enough to close loops with very similar camera motions.
3.8.4
Simulation Analysis for Inverse Depth to XYZ Switching
In order to analyze the effect of the parametrization switching proposed in section
3.6 on the consistency of SLAM estimation, simulation experiments with different
switching thresholds were run. In the simulations, a camera completed two laps of
a circular trajectory of radius 3m in the XZ plane, looking out radially at a scene
composed of points lying on three concentric spheres of radius 4.3m, 10m and 20m.
These points at different depths were intended to produce observations with a range
of parallax angles (Figure3.12.)
The camera parameters of the simulation correspond with our real image ac-
quisition system: camera 320 × 240 pixels, frame rate 30 frames/sec, image ﬁeld
of view 90◦, measurement uncertainty for a point feature in the image, Gaussian
Fig. 3.12. Simulation conﬁguration for analysis of parametrization switching in section 3.8.4,
sketching the circular camera trajectory and 3D scene, composed of three concentric spheres
of radius 4.3m, 10m and 20m. The camera completes two circular laps in the (XZ) plane with
radius 3m, and is orientated radially.

3.8 Experimental Results
59
Fig. 3.13. Details from the parametrization switching experiment. Camera location estima-
tion error history in 6 d.o.f. (translation in XYZ, and three orientation angles ψθφ) for four
switching thresholds: With Ld = 0%, no switching occurs and the features all remain in the
inverse depth parametrization. At, Ld = 10% although features from the spheres at 4.3m and
10m are eventually converted, no degradation with respect to the non-switching case is ob-
served. At Ld = 40% some features are switched before achieving true Gaussianity, and there
is noticeable degradation, especially in θ rotation around the Y axis. At Ld = 60% the map
becomes totally inconsistent and loop closing fails.

60
3 Inverse Depth Parametrization
N

0,1pixel2
. The simulated image sequence contained 1000 frames. Features
were selected following the randomized map management algorithm proposed in
[Davison 2003] in order to have 15 features visible in the image at all times. All our
simulation experiments work using the same scene features, in order to homogenize
the comparison.
Four
simulation
experiments
for
different
thresholds
for
switching
each feature from inverse depth to XYZ parametrization were run, with
Ld ∈{0%,10%,40%,60%}.
Figure 3.13 shows
the
camera trajectory
esti-
mation history in 6 d.o.f. (translation in XYZ, and three orientation angles
ψ(Rotx),θ(Roty),φ(Rotz,cyclotorsion)). The following conclusions can be made:
• The same performance is achieved with no switching (0%) and with 10% switch-
ing. So it is clearly advantageous to perform 10% switching because there is no
penalization in accuracy and the state vector size of each converted feature is
halved.
• Switching too early degrades accuracy, especially in the orientation estimate.
Notice how for 40% the orientation estimate is worse and the orientation error
covariance is smaller, showing ﬁlter inconsistency. For 60%, the estimation is
totally inconsistent and loop closing fails.
• Since early switching degrades performance, the inverse depth parametrization
is mandatory for initialization of every feature and over the long-term for low-
parallax features.
3.8.5
Parametrization Switching with Real Images
The loop closing sequence of section 3.8.3 was processed without any parametriza-
tion switching, and with switching at Ld = 10%.
Figure 3.14 shows the history of the state size, the number of map features and
how their parametrization evolves. At the last estimation step about half of the fea-
tures had been switched; at this step the state size had reduced from 427 to 322 (34
inverse depth features and 35 XYZ), i.e. 75% of the original vector size. Figure 3.15
shows four frames from the sequence illustrating feature switching. Up to step 100
the camera has low translation and all the features are in inverse depth form. As
the camera translates nearby features switch to XYZ. Around step 420, the loop is
closed and features are reobserved, producing a signiﬁcant reduction in uncertainty
which allows switching of more reobserved close features. Our method automati-
cally determines which features should be represented in the inverse depth or XYZ
forms, optimizing computational efﬁciency without sacriﬁcing accuracy.

3.8 Experimental Results
61
Fig. 3.14. Parametrization switching on a real sequence (section 3.8.5): state vector size his-
tory. Top: percentage reduction in state dimension when using switching compared with keep-
ing all points in inverse depth. Bottom: total number of points in the map, showing the number
of points in inverse depth and the number of points in XYZ.
Fig. 3.15. Parametrization switching seen in image space: points coded in inverse depth (⋆)
and coded in XYZ (△). (a) First frame, with all features in inverse depth. (b) Frame #100;
nearby features start switching. (c) Frame # 470, loop closing; most features in XYZ. (d) Last
image of the sequence.

62
3 Inverse Depth Parametrization
3.8.6
Processing Time
We give some details of the real-time operation of our monocular SLAM system,
running on a 1.8 GHz. Pentium M processor laptop. A typical EKF iteration would
imply:
• A state vector dimension of 300.
• 12 features observed in the image, a measurement dimension of 24.
• 30 fps, so 33.3 ms available for processing.
Typical computing time breaks down as follows: Image acquisition, 1 ms.; EKF
prediction, 2 ms.; Image matching, 1 ms.; EKF update, 17 ms. That adds up to a
total of 21ms. The remaining time is used for graphics functions, using OpenGL on
an NVidia card and scheduled at a low priority.
The quoted state vector size 300 corresponds to a map size of 50 if all features
are encoded using inverse depth. In indoor scenes, thanks to switching maps of up
to 60-70 features can be computed in real time. This size is enough to map many
typical scenes robustly.
3.9
Discussion
We have presented a parametrization for monocular SLAM which permits oper-
ation based uniquely on the standard EKF prediction-update procedure at every
step, unifying initialization with the tracking of mapped features. Our inverse depth
parametrization for 3D points allows uniﬁed modelling and processing for any point
in the scene, close or distant, or even at ‘inﬁnity’. In fact, close, distant or just-
initialized features are processed within the routine EKF prediction-update loop
without making any binary decisions. Thanks to the undelayed initialization and
immediate full use of inﬁnite points, estimates of camera orientation are signiﬁ-
cantly improved, reducing the camera estimation jitter often reported in previous
work. The jitter reduction in turn leads to computational beneﬁts in terms of smaller
search regions and improved image processing speed
The key factor is that due to our parametrization of the direction and inverse
depth of a point relative to the location from which it was ﬁrst seen, our measure-
ment equation has low linearization errors at low parallax, and hence the estimation
uncertainty is accurately modeled with a multi-variate Gaussian. In section 3.4 we
presented a model which quantiﬁes linearization error. This provides a theoretical
understanding of the impressive outdoor, real-time performance of the EKF with
our parametrization.
The inverse depth representation requires a six-dimensional state vector per
feature, compared to three for XYZ coding. This doubles the map state vector size,
and hence produces a 4-fold increase in the computational cost of the EKF. Our
experiments show that it is essential to retain the inverse depth parametrization for
intialization and distant features, but that nearby features can be safely converted to

3.9 Discussion
63
the cheaper XYZ representation meaning that the long-term computationalcost need
not signiﬁcantly increase. We have given details on when this conversion should be
carried out for each feature, to optimize computational efﬁciency without sacriﬁcing
accuracy.
The experiments presented have validated the method with real imagery, using
a hand-held camera as the only sensor both indoors and outdoors. We have experi-
mentally veriﬁed the key contributions of our work:
• Real-time performance achieving 30 fps real-time processing for maps up to 60–
70 features.
• Real-time loop closing.
• Dealing simultaneously with low and high parallax features.
• Non delayed initialization.
• Low jitter, full 6 DOF monocular SLAM.
The
point parametrization detailed
in
this
chapter –ﬁrst
presented in
[Montiel et al. 2006, Civera et al. 2007a, Civera et al. 2008a]– has reached the sta-
tus of standard and reached an impact in the robotics community that is worth
summarizing here. The inverse depth parametrization is used in the most suc-
cessful implementations of EKF-based monocular SLAM [Clemente et al. 2007,
Williams et al. 2007, Pini´es & Tard´os 2008, Paz et al. 2008b, Holmes et al. 2008,
Castle et al. 2010]. Several modiﬁcations to the approach presented here has been
studied [Marzorati et al. 2008, Sol`a 2010]. Nevertheless, it has been recently proved
in [Sol`a et al. 2011] that the one presented here still presents the highest degree of
accuracy and consistency. The inverse depth idea has been also succesfully applied
to line-based monocular SLAM [Sol`a et al. 2009], stereo SLAM [Paz et al. 2008a]
and SLAM with an omnidirectional camera [Rituerto et al. 2010].


Chapter 4
1-Point RANSAC
Abstract. Random Sample Consensus (RANSAC) has become one of the most
successful techniques for robust estimation from a data set that may contain out-
liers. It works by constructing model hypotheses from random minimal data subsets
and evaluating their validity from the support of the whole data. In this chapter
we present an efﬁcient RANSAC algorithm for an Extended Kalman Filter (EKF)
framework that uses the available prior probabilistic information from the EKF in
the RANSAC model hypothesize stage. This allows the minimal sample size to
be reduced to one, resulting in large computational savings without performance
degradation. 1-Point RANSAC is also shown to outperform both in accuracy and
computational cost the Joint Compatibility Branch and Bound (JCBB) algorithm,
a gold-standard technique for spurious rejection within the EKF framework. The
combination of this 1-point RANSAC and the robocentric formulation of the EKF
SLAM allows a qualitative jump on the general performance of the algorithms pre-
sented in this book: In this chapter, sequences covering trajectories of several hun-
dreds of metres are processed showing highly accurate camera motion estimation
results.
4.1
Introduction
The establishment of reliable correspondences from sensor data is at the core of
most estimation algorithms in robotics. The search for correspondences, or data
association, is usually based ﬁrst stage on comparing local descriptors of salient
features in the measured data. The ambiguity of such local description usually pro-
duces incorrect correspondences at this stage. Robust methods operate by checking
the consistency of the data against the global model assumed to be generating the
data, and discarding as spurious any that does not ﬁt into it. Among robust esti-
mation methods, Random Sample Consensus (RANSAC) [Fischler & Bolles 1981]
stands out as one of the most successful and widely used, especially in the Computer
Vision community.
J. Civera et al.: Structure from Motion Using the Extended Kalman Filter, STAR 75, pp. 65–97.
springerlink.com
c⃝Springer-Verlag Berlin Heidelberg 2012

66
4 1-Point RANSAC
This chapter introduces a novel integration of RANSAC into the Extended
Kalman Filter framework. In order to highlight the requirements and beneﬁts of
this method, the RANSAC algorithm is ﬁrst brieﬂy exposed in this introduction for
the simple case of 2D line estimation from a set of points contaminated with spuri-
ous data (see Figure 4.1). After that, the same simple example will be tackled using
the proposed 1-Point RANSAC algorithm (Figure 4.2). It is important to remark
here that we use this simple example only to illustrate in the simplest manner our
approach, and will later on ﬁll in the details which make 1-Point RANSAC into a
fully practical matching algorithm.
°
°
¯
°°
®
­
3RD STEP: MODEL
RE-ESTIMATION 
WITH RESCUED 
INLIERS
DATA 
CONTAMINATED
WITH OUTLIERS
Most
supported
hypothesis
Outliers??
Inlier
1st STEP: RANDOM 
HYPOTHESIS
10 votes
3 votes
8 votes
2nd STEP: MODEL 
INSTANTIATION WITH 
CLEAR INLIERS
Outliers??
Outliers
Outliers
Fig. 4.1. RANSAC steps for the simple 2D line estimation example: First, random hypothe-
ses are generated from data samples of size two, the minimum to deﬁne a line. The most
supported one is selected, and data voting for this hypothesis is considered inlier. Model pa-
rameters are estimated from those clear inliers in a second step. Finally, the remaining data
points consistent with this latest model are rescued and the model is re-estimated again.
Standard RANSAC starts from a set of data, in our simple example 2D points, and
the underlying model that generates the data, a 2D line. In the ﬁrst step, RANSAC
constructs hypotheses for the model parameters and selects the one that gathers
most support. Hypotheses are randomly generated from the minimum number of
points necessary to compute the model parameters, which is two in our case of line
estimation. Support for each hypothesis can be computed in its most simple form by
counting the data points inside a threshold (related to the data noise), although more
sophisticated methods have been used [Torr & Zisserman 2000].

4.1 Introduction
67
Hypotheses involving one or more outliers are assumed to receive low support,
as is the case in the third hypothesis in Figure 4.1. The number of hypotheses nhyp
necessary to ensure that at least one spurious-free hypothesis has been tested with
probability p can be computed from this formula:
nhyp =
log(1 −p)
log(1 −(1 −ε)m) ,
(4.1)
where ε is the outlier ratio and m the minimum number of data points necessary to
instantiate the model. The usual approach is to adaptively compute this number of
hypotheses at each iteration, assuming the inlier ratio is the support set by the total
number of data points in this iteration [Hartley & Zisserman 2004].
Data points that voted for the most supported hypothesis are considered clear
inliers. In a second stage, clear inliers are used to estimate the model parameters.
Individual compatibility is checked for each one of the rest of the points against the
estimated model. If any of them is rescued as inlier, as happens in the example in
Figure 4.1, the model parameters are re-estimated again in a third step.
°
°
¯
°°
®
­
3RD STEP: MODEL 
RE-ESTIMATION 
WITH RESCUED 
INLIERS
DATA 
CONTAMINATED
WITH OUTLIERS
AND A PRIORI 
INFORMATION
Most
supported
hypothesis
Outliers??
Inlier
1st STEP: RANDOM 
HYPOTHESIS
USING 1 POINT
10 votes
5 votes
9 votes
2nd STEP: MODEL 
INSTANTIATION WITH 
CLEAR INLIERS
Outliers??
Outliers
Outliers
Inlier
Fig. 4.2. 1-Point RANSAC steps for the simple 2D line estimation example: As a key differ-
ence from standard RANSAC, the algorithm assumes that an a priori probability distribution
over the model parameters is known in advance. This prior knowledge allows us to compute
the random hypotheses using only 1 data point, hence reducing the number of hypotheses and
the computational cost. The remaining steps do not vary with respect to standard RANSAC
in Figure 4.1
.
Figure 4.2 illustrates the idea behind 1-Point RANSAC in the same 2D line
estimation problem. As the ﬁrst key difference, the starting point is a data set
and its underlying model, but also a prior probability distribution over the model

68
4 1-Point RANSAC
parameters. RANSAC random hypotheses are then generated based on this prior
information and data points, differently from standard RANSAC hypothesis solely
based on data points. The use of prior information can reduce the size of the data set
that instantiates the model to the minimum size of one point, and it is here where the
computational beneﬁt of our method with respect to RANSAC arises: according to
Equation 4.1, reducing the sample size m greatly reduces the number of RANSAC
iterations and hence the computational cost.
The order of magnitude of this reduction can be better understood if we switch
from the simple 2D line estimation example to our visual estimation application.
According to [Nist´er 2004], at least ﬁve image points are necessary to estimate the
6 degrees of freedom camera motion between two frames (so m = 5). Using formula
4.1, assuming an inlier ratio of 0.5 and a probability p of 0.99, the number of ran-
dom hypotheses would be 146. Using our 1-Point RANSAC scheme, assuming that
probabilistic a priori information is available, the sample size m can be reduced to
one point and the number of hypotheses would be reduced to 7.
Having an a priori probability distribution over the camera parameters is un-
usual in classical pairwise Structure from Motion which assumes widely separated
views [Hartley & Zisserman 2004], and methods like standard RANSAC which
generate hypotheses from candidate feature matches are mandatory in this case.
But in sequential SfM from video (such as [Davison 2003, Klein & Murray 2008,
Mouragnon et al. 2009]), smooth interframe camera motion can be reasonably as-
sumed and used to generate a prior probability distribution (prediction) for the im-
age correspondences. For the speciﬁc case of the EKF implementation of sequential
SfM, this prior probability is naturally propagated by the ﬁlter and is straightfor-
wardly available.
The rest of the chapter is organised as follows: ﬁrst, related work is described
in section 4.2; the proposed algorithm is then described in its most general form in
section 4.3 and the details for the visual application are given in section 4.4. Exper-
imental results are shown in section 4.5, including pure visual estimation and the
monocular and wheel odometry combination. Finally, discussion and conclusions
are presented in sections 4.6 and ??.
4.2
Related Work
4.2.1
Random Sample Consensus (RANSAC)
Although RANSAC is a relatively old method, the literature covering the topic
continues up to the present. RANSAC [Fischler & Bolles 1981] was introduced
early in visual geometric estimation [Torr & Murray 1993] and has been the pre-
ferred outlier rejection tool in the ﬁeld. Recently, an important stream of research
has focused on reducing the model veriﬁcation cost in standard RANSAC (e.g.
[Raguram et al. 2008, Chum & Matas 2008, Capel 2005, Nist´er 2005]) via the early

4.2 Related Work
69
detection and termination of bad hypotheses. The 1-point RANSAC algorithm pro-
posed here is related to this stream in the sense that it also reduces the hypothesis
generation and validation cost. Nevertheless, it does so in a different manner: instead
of fast identiﬁcation of good hypotheses among a large number of them, the number
of hypotheses is greatly reduced from the start by considering the prior information
given by a dynamic model.
Incorporating probabilistic information into RANSAC has rarely been dis-
cussed in the computer vision literature. Only very recently Moreno et al.
[Moreno-Noguer et al. 2008] have explored the case where weak a priori informa-
tion is available in the form of probabilistic distribution functions.
More related to our method, the combination of RANSAC and Kalman ﬁltering
was proposed by Vedaldi et al. in [Vedaldi et al. 2005]. Our 1-Point RANSAC might
be considered as a speciﬁc form of Vedaldi’s quite general approach. They propose
an iterative scheme in which several minimal hypotheses are tested; for each such
hypothesis all the consistent matches are iteratively harvested. No statement about
the cardinality of the hypotheses is made. Here we propose a deﬁnite and efﬁcient
method, in which the cardinality of the hypotheses generator size is 1, and the inlier
harvesting is not iterative but in two stages. Finally we describe in reproducible
detail how to deal efﬁciently with the EKF algorithm in order to reach real-time,
splitting the expensive EKF covariance update in two stages.
RANSAC using 1-point hypotheses has also been very recently proposed in
[Scaramuzza et al. 2009] as the result of constraining the camera motion. While at
least 5 points would be needed to compute monocular Structure from Motion for a
calibrated camera undergoing general six degrees of freedom motion [Nist´er 2004],
fewer are needed if the motion is known to be less general: as few as 2 points in
[Ort´ın & Montiel 2001] for planar motion and 1 point in [Scaramuzza et al. 2009]
for planar and nonholonomic motion. As a clear limitation of both approaches,
any motion performed out of the model will result in estimation error. In fact, it
is shown in real-image experiments in [Scaramuzza et al. 2009] that although the
most constrained model is enough for RANSAC hypotheses (reaching then 1-point
RANSAC), a less restrictive model offers better results for motion estimation.
In the case of the new 1-point RANSAC presented here, extra information for the
predicted camera motion comes from the probability distribution function that the
EKF naturally propagates over time. The method presented is then in principle not
restricted to any speciﬁc motion, being suitable for 6 degrees of freedom estimation.
The only assumption is the existence of tight and highly correlated priors, which
is reasonable within the EKF framework since the ﬁlter itself only works in such
circumstances.
4.2.2
Joint Compatibility Branch and Bound (JCBB)
Joint Compatibility Branch and Bound (JCBB) [Neira & Tard´os 2001] has been the
preferred technique for spurious match rejection within the EKF framework in the

70
4 1-Point RANSAC
robotics community, being successfully used in visual (e.g. [Clemente et al. 2007],
[Williams et al. 2007]) and non-visual SLAM (e.g. [Fenwick et al. 2002]). Unlike
RANSAC, which hypothesizes model parameters based on current measurement
data, JCBB detects spurious measurements based on a predicted probability dis-
tribution over the measurements. It does so by extracting from all the possible
matches the maximum set that is jointly compatible with the multivariate Gaussian
prediction.
In spite of its wide use, JCBB presents two main limitations that 1-Point RANSAC
overcomes. First, JCBB operates over the prediction for the measurements before
fusing them. Such a probabilistic prediction is coming from the linearization of the
dynamic and measurement models and the assumption of Gaussian noise; so it will
presumably not correspond to the real state of the system. 1-Point and in general
any RANSAC operates over hypotheses after the integration of a data subset, which
have corrected part of the predicted model error with respect to the real system.
The second limitation of JCBB concerns computational cost: the Branch and
Bound search that JCBB uses for extracting the largest jointly compatible set of
matches has exponential complexity in the number of matches. This complexity
does not present a problem for small numbers of matches, as is the case in the
references two paragraphs above, but very large computation times arise when the
number of spurious grows, as we will show in the experimental results section. The
computational complexity of 1-Point RANSAC is linear in the state and measure-
ment size and exhibits low cost variation with the number of outliers.
Two recent methods are also of interest for this work. First, Active Matching
[Chli & Davison 2008] is a clear inspiration for our method. In Active Matching,
feature measurements are integrated sequentially, with the choice of measurement at
each step driven by expected information gain, and the results of each measurement
in turn used to narrow the search for subsequent correspondences. 1-Point RANSAC
can be seen as lying in the middle ground between RANSAC and JCBB which
obtain point correspondence candidates and then aim to resolve them, and Active
Matching with its fully sequential search for correspondence. The ﬁrst step of 1-
Point RANSAC is very similar to Active Matching, and conﬁrming that integrating
the ﬁrst match highly constrains the possible image locations of other features, but
afterwards the methods of the algorithms diverge. A problem with Active Matching
in [Chli & Davison 2008] was the unreasonably high computational cost of scaling
to large numbers of feature correspondences per frame, and 1-Point RANSAC has
much better properties in this regard, though very recently an improvement to Active
Matching has also addressed this issue in a different way [Handa et al. 2010].
Paz et al. [Paz et al. 2008b] describe an approach called Randomized Joint
Compatibility (RJC) which basically randomizes the jointly compatible set search,
avoiding the Branch and Bound search and ensuring an initial small set of jointly
compatible inliers at the ﬁrst step via Branch and Bound search in random sets. Only
afterwards, the joint compatibility of each remaining match is checked against the
initial set. Although this approach lowers the computational cost of the JCBB, it
still faces the accuracy problems derived from the use of the predicted measurement
function before data fusion.

4.2 Related Work
71
4.2.3
Structure from Motion and Visual Odometry
Structure from Motion (SfM) is the generic term for 3D estimation from
the sole input of a set of images of the imaged 3D scene and the corre-
sponding camera locations. SfM from a sparse set of images has been usu-
ally processed by pairwise geometry algorithms [Hartley & Zisserman 2004]
and reﬁned by global optimization procedures [Triggs et al. 2000]. Estima-
tion from a sequence has been carried out either by local optimization
of keyframes [Klein & Murray 2008, Mouragnon et al. 2009], or by ﬁltering
[Davison et al. 2007, Eade & Drummond 2007].
Visual Odometry, a term coined in [Nist´er et al. 2004], refers to egomotion
estimation mainly from visual input (monocular or stereo), but sometimes
also combined with mechanical odometry and/or inertial sensor measurements.
The variety of approaches here makes a complete review difﬁcult; some vi-
sual odometry algorithms have made use of stereo cameras, either as the
only sensor (e.g. [Comport et al. 2007]) or in combination with inertial mea-
surements [Konolige et al. 2007, Cheng et al. 2006]. Among the monocular ap-
proaches, [Mouragnon et al. 2009] uses a non-panoramic camera as the only sen-
sor. Several others have been proposed using an omnidirectional camera, e.g.
[Scaramuzza et al. 2009, Tardif et al. 2008]. The experiment presented here, com-
bining a non-panoramic camera plus proprioceptive information for estimation of
large trajectories, is rarely found in the literature.
4.2.4
Benchmarking
Carefully designed benchmark datasets and methods have come into standard
use in the vision community, e.g. [Scharstein & Szeliski 2002, Baker et al. 2007,
Everingham et al. 2010]. Robotics datasets have only recently reached such level of
detail, presenting either detailed benchmarking procedures [Kummerle et al. 2009]
or datasets with reliable ground truth and open resources for comparison
[Smith et al. 2009, Blanco et al. 2009].
The RAWSEEDS dataset [RAWSEEDS 2011], which include monocular and
wheel odometry streams for large scale scenarios, will be used for the Visual Odom-
etry experiments in the next section of this chapter. While being suitable to bench-
mark very large real-image experiments, robotic datasets face two main inconve-
niences: First, the robot motion is planar in all the datasets, thus not allowing to
evaluate full six-degrees-of-freedommotion estimation. And second, GPS only pro-
vides translational data and angular estimation cannot be benchmarked. Simulation
environments, like the one described in [Funke & Pietzsch 2009], can provide the
translational and angular ground truth for any kind of camera motion. Nevertheless,
those simulation environments usually cannot represent full real world complexity.
The benchmarking method proposed and used in this chapter overcomes all these
limitations. It consists of comparing the estimation results against a Bundle Adjust-
ment solution over high resolution images. Full 6 DOF motion can be evaluated with

72
4 1-Point RANSAC
low user effort (only the generation of a Bundle Adjustment solution is required), re-
quirements for hardware are low (a high resolution camera) and any kind of motion
or scene can be evaluated as the method operates over the real images themselves.
This approach is not entirely new: the use of a global Bundle Adjust-
ment solution to benchmark sequential algorithms has already been used in
[Eade & Drummond 2007, Mouragnon et al. 2009]. The contribution here is the
validation of the algorithm, effectively showing that the Bundle Adjustment uncer-
tainty is much lower than the sequential methods to benchmark. As another novelty,
global Bundle Adjustment is applied over high resolution images, further improving
accuracy. While it is true that a Bundle Adjustment solution still may suffer from
scale drift, it will be much lower than that of the sequential algorithms. Also, scale
drift can be driven close to zero by carefully chosing the images over which to apply
Bundle Adjustment to form a well-conditioned network [Triggs et al. 2000], so the
validity of the method is not compromised.
4.3
1-Point RANSAC Extended Kalman Filter Algorithm
Algorithm 1 outlines the proposed combination of 1-Point RANSAC inside the EKF
framework in its most general form, and we describe this in detail in this section. The
language used here is deliberately general in the belief that the described algorithm
may be of application in a large number of estimation problems. The particular
scenarios of the experimental results section (real-time sequential visual odometry
from a monocular sequence, either with or without additional wheel odometry) are
discussed in detail in section 4.4.
4.3.1
EKF Prediction and Search for Individually Compatible
Matches (lines 5–8)
The algorithm begins with standard EKF prediction: the estimation for the state
vector xk−1|k−1 at step k −1, modeled as a multidimensional Gaussian xk−1|k−1 ∼
N
ˆxk−1|k−1,Pk−1|k−1

, is propagated to step k through the known dynamic model fk
ˆxk|k−1 = fk
ˆxk−1|k−1,uk

(4.2)
Pk|k−1 = FkPk−1|k−1F⊤
k +GkQkG⊤
k .
(4.3)
In the above equation uk stands for the control inputs to the system at step k,
Fk is the Jacobian of fk with respect to the state vector xk|k−1 at step k, Qk is the
covariance of the zero-mean Gaussian noise assumed for the dynamic model and
Gk is the Jacobian of the state vector xk|k−1 by the input uk at step k.

4.3 1-Point RANSAC Extended Kalman Filter Algorithm
73
Algorithm 1.1-Point RANSAC EKF
1: INPUT: ˆxk−1|k−1,Pk−1|k−1 {EKF estimate at step k −1}
2:
th {Threshold for low-innovation points, related with the measurement noise.}
3: OUTPUT: ˆxk|k,Pk|k {EKF estimate at step k}
4:
{A. EKF prediction and individually compatible matches}
5: [ˆxk|k−1,Pk|k−1] = EKF prediction(ˆxk−1|k−1,Pk−1|k−1,u)
6: [ˆhk|k−1,Sk|k−1] = measurement prediction(ˆxk|k−1,Pk|k−1)
7: zIC = search IC matches(ˆhk|k−1,Sk|k−1)
8:
{B. 1-Point hypotheses generation and evaluation}
9: zli inliers = [ ]
10: nhyp = 1000 {Initial value, will be updated in the loop}
11: for i = 0 to nhyp do
12:
zi = select random match(zIC)
13:
ˆxi = EKF state update(zi, ˆxk|k−1) {Notice: only state update; NO covariance up-
date}
14:
ˆhi = predict all measurements(ˆxi)
15:
zth
i = find matches below a threshold(zIC, ˆhi,th)
16:
if size(zth
i ) > size(zli inliers) then
17:
zli inliers = zth
i
18:
ε = 1−
size(zli inliers)
size(zIC)
19:
nhyp =
log(1−p)
log(1−(1−ε)) {From equation 4.1 with m = 1}
20:
end if
21: end for
22:
{C. Partial EKF update using low-innovation inliers}
23: [ˆxk|k,Pk|k] = EKF update(zli inliers, ˆxk|k−1,Pk|k−1)
24:
{D. Partial EKF update using high-innovation inliers}
25: zhi inliers = [ ]
26: for every match zj above a threshold th do
27:
[ˆh j,S j] = point j prediction and covariance(ˆxk|k,Pk|k, j)
28:
ν j = zj −ˆh j
29:
if ν j⊤S j−1ν j < χ2
2,0.01 then
30:
zhi inliers = add match j to inliers(zhi inliers,zj) {If individually compatible, add
to inliers}
31:
end if
32: end for
33: if size(zhi inliers) > 0 then
34:
[ˆxk|k,Pk|k] = EKF update(zhi inliers, ˆxk|k,Pk|k)
35: end if

74
4 1-Point RANSAC
The predicted probability distribution for the state xk|k−1 can be used to ease the
correspondence search as described in section 3.7.2. Propagating this predicted state
through the measurement model hi offers a Gaussian prediction for each measure-
ment:
ˆhi = hi
ˆxk|k−1

(4.4)
Si = HiPk|k−1H⊤
i +Ri ,
(4.5)
where Hi is the Jacobian of the measurement hi with respect to the state vector
xk|k−1 and Ri is the covariance of the Gaussian noise assumed for each individ-
ual measurement. The actual measurement zi should be exhaustively searched for
inside the 99% probability region deﬁned by its predicted Gaussian N
ˆhi,Si

by
comparison of the chosen local feature descriptor.
Active search allows computational savings and also constraints the matches to
be individually compatible with the predicted state xk|k−1. Nevertheless, ensuring
geometric compatibility for each separated match zi does not guarantee the global
consensus of the whole set. So, still the joint compatibility of the data against a
global model has to be checked for the set individually compatible matches zIC =
(z1 ...zi ...zn)⊤previous to the EKF update.
4.3.2
1-Point Hypotheses Generation and Evaluation (lines 9–22)
Following the principles of RANSAC, random state hypotheses ˆxi are generated and
data support is computed by counting measurements inside a threshold related to the
measurement noise. It is assumed here that the predicted measurements are highly
correlated, such that every hypothesis computed from one match reduces most of
the common uncertainty producing an inlier uncertainty close to the measurement
noise.
As the key difference with respect to standard RANSAC, random hypotheses will
be generated not only based on the data zIC = (z1 ...zi ...zn)⊤but also on the pre-
dicted state xk|k−1 ∼N

ˆxk|k−1,Pk|k−1

. Exploiting this prior knowledge allows us
to reduce the sample size necessary to instantiate the model parameters from the
minimal size to deﬁne the degrees of freedom of the model to only one data point.
The termination criteria of the RANSAC algorithm in equation 4.1 grows exponen-
tially with the sample size, so we can achieve a great reduction in the number of
hypotheses.
Another key aspect for the efﬁciency of the algorithm is that each hypothesis ˆxi
generation only needs an EKF state update using a single match zi. A covariance
update, which is of quadratic complexity in the size of the state, is not needed and
the cost per hypothesis will be low. Hypothesis support is calculated by projecting
the updated state into the camera, which can also be performed at very low cost
compared with other stages in the EKF algorithm.

4.3 1-Point RANSAC Extended Kalman Filter Algorithm
75
4.3.3
Partial Update with Low-Innovation Inliers (lines 23–24)
Data points voting for the most supported hypothesis zli inliers are designated as low-
innovation inliers. They are assumed to be generated by the true model, as they are
at a small distance from the most supported hypothesis. The rest of the points can
be outliers but also inliers, even if they are far from the most supported hypothesis.
A simple example related to this book can illustrate this: it is well known that
distant points are useful for estimating camera rotation while close points are neces-
sary to estimate translation; as discussed in chapter 3. In the RANSAC hypotheses
generation step, a distant feature would generate a highly accurate 1-point hypothe-
sis for rotation, while translation would remain inaccurately estimated. Other distant
points would in this case have low innovation and would vote for this hypothesis.
But as translation is still inaccurately estimated, nearby points would presumably
exhibit high innovation even if they are inliers.
So after having determined the most supported hypothesis and the other points
that vote for it, some inliers still have to be “rescued” from the high-innovation set.
Such inliers will be rescued after a partial state and covariance update using only
the reliable set of low-innovation inliers:
ˆxk|k = ˆxk|k−1 +K′
zli inliers −h′ 
ˆxk|k−1

(4.6)
Pk|k =

I−K′H′
Pk|k−1
(4.7)
K′ = Pk|k−1H′⊤
H′Pk|k−1H′⊤+R′−1
.
(4.8)
H′ = (H′
1 ...H′
i ...H′
n)⊤stands for the Jacobian of the measurement equation
h′ ˆxk|k−1

that projects the low-innovation inliers into the sensor space. R′ is the
covariance assigned to the sensor noise.
4.3.4
Partial Update with High-Innovation Inliers (lines 25–35)
After a partial update using low-innovation inliers, most of the correlated error in
the EKF prediction is corrected and the covariance is greatly reduced. This high
reduction will be exploited for the recovery of high-innovation inliers: as correla-
tions have weakened, consensus for the set will not be necessary to compute and
individual compatibility will sufﬁce to discard inliers from outliers.
An individual Gaussian prediction hj ∼N
ˆhj,Sj
will be computed for each
high innovation for every match zj by propagating the state after the ﬁrst partial
update xk|k through the projection model. The match will be accepted as an in-
lier if it lies within the 99% probability region of the predicted Gaussian for the
measurement.

76
4 1-Point RANSAC
After testing all the high-innovation measurements a second partial update will
be performed with all the points classiﬁed as inliers zhi inliers, following the usual
EKF equations.
It is worth remarking here that splitting the EKF update does not have a no-
ticeable effect on the computational cost. If n is the state size and m the mea-
surement vector size, and in the usual SLAM case where the state is much bigger
than the locally measured set n >> m, the main EKF cost is the covariance update
which is O

mn2
. If the update is divided into two steps of measurement vector
sizes m1 and m2 (m = m1 + m2), this covariance update cost stays almost the same.
Some other minor costs grow, like the Jacobian computation which has to be done
twice. But also some others are reduced, like the measurement covariance inversion
which is O

m3
. Nevertheless, the effect of the latter two is negligible and for most
EKF estimation cases the cost is dominated by the covariance update and remains
approximately the same.
4.4
1-Point RANSAC Extended Kalman Filter from a
Monocular Sequence Input
As previously stated, the proposed 1-point RANSAC and EKF combination is used
in this chapter for the particular case of visual estimation from a monocular camera.
In this section, the general method detailed in section 4.3 specializes to this speciﬁc
application.
4.4.1
State Vector Deﬁnition
The state vector at step k is composed of a set of camera parameters xCk and map
parameters y.
ˆxk =
 ˆxCk
ˆxM

; Pk =
 PCk PCkM
PMCk PM

.
(4.9)
The estimated map xM is composed of n point features yi; xM =

y⊤
1 ... y⊤
n
⊤.
Point features are parametrized in inverse depth coordinates yρ,i = (Xi Yi Zi θi φi ρi)⊤
and converted to Euclidean parametrization yXYZ,i = (Xi Yi Zi)⊤if and when the pro-
jection equation becomes linear enough, as described in chapter 3.

4.4 1-Point RANSAC EKF from a Monocular Sequence
77
4.4.2
Dynamic Model
The dynamic model applied to the camera depends on the information available. For
the case of pure visual estimation from a monocular sequence, a constant velocity
model is sufﬁcient for smooth hand-held motion. Details for the constant velocity
model were already discussed in section 3.2.1. The camera state is then formed by
position rCk, orientation qCk, and linear and angular velocities v and ω:
xCk =
⎛
⎜
⎜
⎝
r
q
v
ω
⎞
⎟
⎟
⎠.
(4.10)
When other sensorial information apart from the monocular sequence is avail-
able, it should be incorporated as input to the dynamic model. In this chapter, the
combination of monocular vision plus wheel odometry is analyzed. In this case,
the camera state only needs to contain position and orientation xCk =

r
q

. In the
experiments shown at the end of this chapter, the classical model for a differential
drive robot [Borenstein et al. 1996] has been chosen to model its dynamics.
4.4.3
Camera-Centered Estimation
It is well known that the usual EKF SLAM formulation, referred to a world reference
frame, is only valid for local estimation in the surroundings of a sensor. Figure
4.3(a) illustrates the problem of this formulation: as the sensor moves away from the
world reference, and if a pure exploratory trajectory is performed, the uncertainty
of the estimation will always grow. Eventually it will reach a point where large
linearization errors will cause inconsistency and ﬁlter divergence.
Figure 4.3(b) illustrates an alternative approach that alleviates this problem, that
was ﬁrst presented for EKF SLAM in [Castellanos et al. 2004]. It basically consists
of referring all geometric parameters to a reference frame attached to the camera.
Uncertainty in the locality of the sensor will always be kept low, greatly reducing the
linearization errors associated with the measurement model. The camera-centered
approach was ﬁrst used for visual EKF estimation in [Civera et al. 2009c]; and has
been thoroughly benchmarked in [Williams 2009].
The modiﬁcations with respect to world-centered visual SLAM are now given
in detail. First, the state vector is composed of the location of the world reference
frame xCkW and the map of estimated features yCk, both expressed in the current
camera reference frame Ck:
xCk
k =
xCkW
yCk

.
(4.11)

78
4 1-Point RANSAC
W
C
High linearization 
error
y1
y2
y3
(a) World-referenced EKF estimation. As the camera moves away from the
World reference and does not revisit known places the uncertainty grows
both in the camera and newly initialized features yi. The wide uncertainty
regions will produce high linearization errors and ﬁlter inconsistency.
W
C
Low linearization 
error
y1
y2
y3
(b) Camera-centered EKF estimation. The camera location uncertainty is
close to zero, as the reference is always attached to it. Uncertainties for
features in its vicinity yi will be also kept low, so measurement model lin-
earization errors will be kept small for the whole estimation.
Fig. 4.3. Camera-centered and World-referenced EKF estimation.
The location of the world reference with respect to the current camera xCkW =
 rCkW
qCkW

is coded with its position rCkW and quaternion orientation qCkW. When
odometry information is not available and a constant velocity model is assumed,
velocities should also be included in the state xCk
k =
⎛
⎜
⎜
⎝
xCkW
vCk
ωCk
yCk
⎞
⎟
⎟
⎠.
For the prediction step at time k, the world reference frame and feature map are
kept in the reference frame at time k −1 and a new feature xCk−1Ck that represents
the motion of the sensor between k −1 and k is added:
xCk−1
k|k−1 =
⎛
⎝
xCk−1W
yCk−1
xCk−1Ck
⎞
⎠
(4.12)
The predicted camera motion is represented in terms of position and orientation,
represented via a quaternion:

4.4 1-Point RANSAC EKF from a Monocular Sequence
79
xCk−1Ck =
 rCk−1Ck
qCk−1Ck

.
(4.13)
The 1-point RANSAC EKF algorithm is applied with minor changes. The dy-
namic model of the system is applied over the motion relative to the previous frame
contained in xCk−1Ck, either using the constant velocity model in section 3.2.1 (in
which case velocities should be kept then in the state as described above) or wheel
odometry inputs. The pinhole camera model that serves as measurement model de-
scribed in chapter 3 remains the same.
The algorithm proceeds then as explained in section 4.3. At the end of the al-
gorithm, after the second update, a rigid transformation is applied to change the
reference frame from the previous camera to the current one. The world reference
location is updated:
rCkW = RCkCk−1 
rCk−1W −rCk−1Ck
(4.14)
qCkW = qCkCk−1 ×qCk−1W ,
(4.15)
and the parameters representing motion from the previous to the current frame
xCk−1Ck are marginalized out from the state. Inverse depth and Euclidean map fea-
tures are also affected by this composition step:
yCk
ρ,i =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
RCkCk−1
⎛
⎜
⎝
⎛
⎜
⎝
xCk−1
i
yCk−1
i
zCk−1
i
⎞
⎟
⎠−rCk−1Ck
⎞
⎟
⎠
m−1 
RCkCk−1m

θCk−1
i
,φCk−1
i

ρi
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
(4.16)
yCk
XYZ,i = RCkCk−1

yCk−1
XYZ,i −rCk−1Ck

.
(4.17)
In the previous formulas, RCkCk−1 stands for the rotation matrix from the camera
reference Ck to Ck−1. This rotation matrix is computed from the quaternion qCk−1Ck
in the state vector using the standard quaternion to rotation matrix conversion. The
quaternion qCkCk−1 is the conjugate of the state vector one qCk−1Ck. m stands for the
transformation from azimuth-elevation angles to unit directional vector; and m−1 for
the inverse transformation from a unit vector to its azimuth-elevation representation.
The covariance is updated using the Jacobians of this composition function JCk−1→Ck
PCk
k = JCk−1→CkPCk−1
k
J⊤
Ck−1→Ck .
(4.18)

80
4 1-Point RANSAC
4.5
Experimental Results
4.5.1
Benchmark Method for 6 DOF Camera Motion Estimation
The ﬁrst step of the method takes an image sequence of the highest resolution, in
order to achieve the highest accuracy. In this chapter, a 1224 × 1026 sequence was
taken at 22 frames per second. A sparse subset of n camera locations xC1
BA are es-
timated by Levenberg-Marquardt Bundle Adjustment with robust likelihood model
[Triggs et al. 2000] over the correspongding n images in the sequence {I1,...In}.
Images are manually selected to ensure they form a strong network. The reference
frame is attached to the camera C1, corresponding to the ﬁrst frame of the sequence
I1. For the experiments in sections 4.5.2 and 4.5.3, 62 overlapping camera locations
were reconstructed by manually matching 74 points spread over the images. 15−20
points are visible in each image.
xC1
BA =
⎛
⎜
⎜
⎝
xC1
1,BA
...
xC1
n,BA
⎞
⎟
⎟
⎠,
(4.19)
xC1
i,BA =

XC1
i,BA YC1
i,BA ZC1
i,BA φC1
i,BA θC1
i,BA ψC1
i,BA
⊤
.
(4.20)
Each camera location is represented by its position

XC1
i,BA YC1
i,BA ZC1
i,BA
⊤
and Euler
angles

φC1
i,BA θC1
i,BA ψC1
i,BA
⊤
. The covariance of the solution is computed by back-
propagation of reprojection errors PC1
BA =

J⊤R−1J
−1, where J is the Jacobian of
the projection model and R is the covariance of the Gaussian noise assumed in the
model.
The input sequence is then reduced by dividing its width and height by four. The
algorithm to benchmark is applied over the subsampled sequence. The reference
frame is also attached to the ﬁrst camera C1, which is taken to be the same ﬁrst
one as in Bundle Adjustment. Images for which a Bundle Adjustment estimation
is available are selected and stored xC1
i,MS, each along with its individual covariance
PC1
i,MS directly extracted from the EKF at each step.
As the reference has been set to the same ﬁrst image of the sequence, the Bundle
Adjustment and sequential estimation solutions only differ in the scale of the re-
construction. So, in order to compare them, the relative scale s is estimated ﬁrst by
minimizing the error between the two trajectories. The Bundle Adjustment trajec-
tory is then scaled xC1
BA = fscale

xC1
BA

and also its covariance PC1
BA = JscalePC1
BAJ⊤
scale.
Finally, the error is computed as the relative transformation between the two so-
lutions:
e = ⊕xC1
BA ⊖xC1
MS ;
(4.21)

4.5 Experimental Results
81
and the corresponding covariance of the error is computed by propagating the co-
variances of the global optimization and sequential estimate:
Pe = JeBAPC1
BAJ⊤
eBA +JeMSPC1
MSJ⊤
eMS .
(4.22)
It was checked in the experiments in the chapter that the covariance term from
Bundle Adjustment, JeBAPC1
BAJ⊤
eBA, was negligible with respect to the summed covari-
ance Pe. Since this is the case, it is our opinion that the Bundle Adjustment results
can be considered as a reliable ground truth to evaluate sequential approaches. In
the following ﬁgures, only uncertainty regions coming from ﬁltering, JeMSPC1
MSJ⊤
eMS
are shown.
The same subsampled sequence was used for all the experiments in the following
sections 4.5.2 and 4.5.3. The camera moves freely in six degrees of freedom in a
computer lab, with the maximum distances between camera locations around 5 me-
tres. Filter tuning parameters were equal for all the experiments: motion dynamic
and measurement model noise were kept the same, the number of measured features
in the image was limited to 30 and all the thresholds (e.g. for feature deletion, cross-
correlation, inverse depth to Euclidean conversion and initialization) were also kept
the same. The reader should be aware that despite all of care taken, the experiments
are not exactly the same: One of the reasons is that the outlier rate is different for
each method; some methods need to initialize more features in order to keep mea-
suring 30. Nevertheless, in the opinion of the authors, this is the fairest comparison
as the algorithms try always to measure always the same number of points and hence
gather an equivalent amount of sensor data.
Figure 4.4 shows example images from the sequence used in the following two
sections for 1-point RANSAC and JCBB benchmarking. The 62 camera locations
from the 2796 images long sequence are also displayed. Results for different exper-
iments using this benchmarking method have been grouped for better visualization
and comparison: Figures 4.5 and 4.7 show estimation errors for different tunings of
1-point RANSAC and JCBB; and 4.9 details their computational cost. All the exper-
iments in the chapter were run on an Intel(R) Core(TM) i7 processor at 2.67GHz.
4.5.2
1-Point RANSAC
First, the performance of 5-point and 1-point RANSAC is compared, in order to
ensure that there is no degradation of performance when the sample size is reduced.
Figures 4.5(a) and 4.5(b) show the errors of both algorithms with respect to the ref-
erence camera motion, along with their 99% uncertainty regions. It can be observed
that reducing the sample size from 5 to 1 does not have a signiﬁcant effect either on
the accuracy or the consistency of the estimation. On the contrary, the ﬁgure even
shows 1-point outperforming 5-point RANSAC. We attribute this to the fact that the
theoretical number of hypotheses given by equation 4.1 was not inﬂated in our ex-
periments, unlike in classical SfM algorithms [Raguram et al. 2008]. By increasing

82
4 1-Point RANSAC
(a) Sample images from the sequence used
for benchmarking.
−1.2
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
Blue triangles: Cameras from Bundle Adjustment
Red stars: 3D points from Bundle Adjustment
(b) Reference solution computed by the pro-
posed method.
Fig. 4.4. Images extracted from the sequence used in the experiments and reference camera
positions extracted.
the number of iterations, 5-point RANSAC results comes close to 1-point; but we
ﬁnd it remarkable that without this augmentation 1-point RANSAC already shows
good behaviour. The standard deviation of image noise was chosen to be 0.5 for the
experiments, as subpixel matching is used.
While the accuracy and consistency remains similar, the computational cost is
much higher for the usual 5-point RANSAC than the proposed 1-point. The detail of
the computational cost of both algorithms can be seen in Figures 4.9(a) and 4.9(b).
The cost of RANSAC is low compared with the rest of the EKF computations for
the 1-point case, but it is several orders of magnitude higher and is the main cost in
the 5-point case. This is caused by the increase in the number of random hypotheses
in frames with a large number of spurious matches. Figures 4.6(a) and 4.6(b) show
the number of hypotheses in both cases, revealing that in 5-point RANSAC this is
two orders of magnitude. The ﬁve higher green pikes appearing in all the ﬁgures are
caused by dropped frames in the sequence where there is a jump in camera location.
The correspondence search cost is increased at these frames, but notice that the cost
of RANSAC is not increased at all.
Hypothesis generation from a single point opens the possibility of exhaustive
rather than random hypotheses generation: while an exhaustive generation of all the
possible combinations of 5 points in the measurement subset would be impractical,
an exhaustive generation of 1-point hypotheses implies only as many hypotheses as
measurements. Figure 4.5(c) details the errors for the 1-point exhaustive hypothe-
ses generation case. Compared with 1-point random hypotheses generation in Fig-
ure 4.6(b), we observe similar accuracy and consistency. Figure 4.6(c) shows the
number of iterations needed for comparison with the random adaptive case (Figure
4.6(b)). The computational cost is increased but, as shown in Figure 4.9(c), it is still

4.5 Experimental Results
83
0
20
40
60
−0.15
−0.1
−0.05
0
0.05
0.1
0.15
X Error
#Frame
0
20
40
60
Y Error
#Frame
0
20
40
60
Z Error
#Frame
0
20
40
60
−0.1
−0.05
0
0.05
0.1
Roll Error [rad]
#Frame
0
20
40
60
Pitch Error [rad]
#Frame
0
20
40
60
Yaw Error [rad]
#Frame
(a) 5-point RANSAC, σz = 0.5 pixels
0
20
40
60
−0.1
−0.05
0
0.05
0.1
0.15
X Error
#Frame
0
20
40
60
Y Error
#Frame
0
20
40
60
Z Error
#Frame
0
20
40
60
−0.1
−0.05
0
0.05
0.1
Roll Error [rad]
#Frame
0
20
40
60
Pitch Error [rad]
#Frame
0
20
40
60
Yaw Error [rad]
#Frame
(b) 1-point RANSAC, σz = 0.5 pixels
0
20
40
60
−0.1
−0.05
0
0.05
0.1
0.15
X Error
#Frame
0
20
40
60
Y Error
#Frame
0
20
40
60
Z Error
#Frame
0
20
40
60
−0.1
−0.05
0
0.05
0.1
Roll Error [rad]
#Frame
0
20
40
60
Pitch Error [rad]
#Frame
0
20
40
60
Yaw Error [rad]
#Frame
(c) 1-point exhaustive hypothesis, σz = 0.5 pixels
0
20
40
60
−0.15
−0.1
−0.05
0
0.05
0.1
0.15
X Error
#Frame
0
20
40
60
Y Error
#Frame
0
20
40
60
Z Error
#Frame
0
20
40
60
−0.1
−0.05
0
0.05
0.1
Roll Error [rad]
#Frame
0
20
40
60
Pitch Error [rad]
#Frame
0
20
40
60
Yaw Error [rad]
#Frame
(d) 1-point RANSAC, σz = 0.2 pixels
Fig. 4.5. Camera location error (in thick blue) and uncertainty (in thin red) for different
RANSAC conﬁgurations. Similar error and consistency is shown for 5-point and 1-point
RANSAC in Figures 4.5(a) and 4.5(b) respectively. Figure 4.5(c) also reports similar results
for exhaustive hypothesis testing. Figure 4.5(d) shows smaller errors as a result of making
1-point RANSAC stricter by reducing the standard deviation of measurement noise.
dominated by the EKF update cost. Both options are then suitable for real-time im-
plementation, with the cheaper adaptive random 1-point RANSAC algorithm being
preferable as performance is not degraded signiﬁcantly.
From analyzing the computational cost in Figure 4.9(b) it can be concluded that
the cost for 1-point RANSAC is always low compared with EKF computation even
when the spurious match rate is high (the spurious match rate is shown in Figure
4.8(b)). As will be shown later, the latter becomes an important advantage over
JCBB whose cost grows exponentially with the rate of spurious matches. This efﬁ-
ciency opens the possibility of making the RANSAC algorithm stricter by reducing
the measurement noise standard deviation and hence discarding high noise points

84
4 1-Point RANSAC
0
500
1000
1500
2000
2500
0
1000
2000
3000
 #Frame 
 #Iterations 
(a) Number of iterations along the sequence
for 5-point RANSAC.
0
500
1000
1500
2000
2500
0
10
20
30
 #Frame 
 #Iterations 
(b) Number of iterations along the sequence
for 1-point RANSAC.
0
500
1000
1500
2000
2500
0
10
20
30
 #Frame 
 #Iterations 
(c) Number of iterations along the sequence
for exhaustive hypotheses generation.
Fig. 4.6. Number of iterations for 5-points and 1-point RANSAC. Notice the several orders
of magnitude increase for the 5-point case, causing a large cost overhead when compared
with 1-point RANSAC (Figures 4.9(a), 4.9(b) and 4.9(c) detail the computational cost for the
three cases respectively).
in the EKF. Such analysis can be done by reducing the standard deviation from 0.5
to 0.2 pixels: high noise points were discarded as outliers, as can be seen in Figures
4.8(b) and 4.8(d). The computational cost increases, as shown in Figure 4.9(e), but
still remains small enough to reach real-time performance at 22 Hz. The beneﬁt
of discarding high noise points can be observed in Figure 4.5(d): errors and their
uncertainty were reduced (but still kept mostly consistent) as a result of measuring
more accurate points.
4.5.3
Joint Compatibility Branch and Bound (JCBB)
RANSAC and JCBB tuning is a thorny issue when benchmarking both algorithms.
As both cases assume Gaussian distributions for the measurement and decide based
on probability, we considered it fairest to choose equal signiﬁcance levels for the
probabilistic tests of both algorithms. The signiﬁcance level was chosen to be 0.05

4.5 Experimental Results
85
in the χ2 test that JCBB performs to ensure joint compatibility for the matches. Con-
sistently, the probabilistic threshold for RANSAC was set to 95% for voting (line 15
in the algorithm in section 4.3) and for the rescue of high-innovation matches (line
29 in the algorithm in section 4.3).
The results of benchmarking JCBB are shown in the following ﬁgures. First, ﬁg-
ure 4.7(a) details the errors and uncertainty regions for the EKF using JCBB. It can
be observed that the estimation in Figure 4.7(a) show larger errors and inconsistency
than the 1-point RANSAC one in Figure 4.7(b), repeated here for visualization pur-
poses. The reason can be observed in Figure 4.8, where the outlier rates for 1-point
RANSAC and JCBB are shown: the number of matches considered outliers by 1-
point RANSAC is greater than by JCBB. The points accepted as inliers by JCBB
are the ones that spoil the estimation.
A stricter version of JCBB has been benchmarked by reducing the standard de-
viation of uncorrelated measurement noise to 0.2 pixels, as was done with 1-point
RANSAC. The spurious match rate for both algorithms, shown in Figure 4.8(c)
and 4.8(d), shows that 1-point RANSAC remains more discriminative and hence
produces more accurate estimation than JCBB (Figure 4.7(c)). 1-point RANSAC
errors for the same tuning are repeated in 4.7(d) for comparison purposes. Also, as
previously noted, the computational cost of JCBB grows exponentially when made
stricter: Figure 4.9(f) shows peaks over a second in the worst cases.
JCBB can also be made stricter by increasing the signiﬁcance level α of the χ2
test it performs to check the joint compatibility of the data. Several experiments were
run varying this parameter. The lowest estimation errors, shown in Figure 4.7(e),
were reached for α = 0.5 instead of the usual α = 0.05. Estimation errors for this
best JCBB tuning are still larger than in any of the 1-point RANSAC experiments.
4.5.4
Trajectory Benchmarking against GPS
The following sections benchmark the presented ﬁltering scheme for the estimation
of long camera trajectories. The benchmarking method of the previous section be-
comes difﬁcult to apply here, so camera translation only is benchmarked against
GPS data. This section describes the benchmarking procedure.
Similarly to the previous section, our EKF estimation takes the ﬁrst camera frame
C1 as the frame of reference. A similarity transformation (rotation RWC1, translation
tWC1 and scale s) has to be applied which aligns every point of the trajectory rC1Ck =

xC1Ck yC1Ck zC1Ck⊤with the GPS data rWGPSk, whose frame of reference we will
denote by W:

rWCk
1

=
⎡
⎢⎢⎣
xWCk
yWCk
zWCk
1
⎤
⎥⎥⎦=

sRWC1 tWC1
0
1

⎡
⎢⎢⎣
xC1Ck
yC1Ck
zC1Ck
1
⎤
⎥⎥⎦.
(4.23)

86
4 1-Point RANSAC
0
20
40
60
−0.1
−0.05
0
0.05
0.1
0.15
X Error
#Frame
0
20
40
60
Y Error
#Frame
0
20
40
60
Z Error
#Frame
0
20
40
60
−0.1
−0.05
0
0.05
0.1
Roll Error [rad]
#Frame
0
20
40
60
Pitch Error [rad]
#Frame
0
20
40
60
Yaw Error [rad]
#Frame
(a) JCBB, σz = 0.5 pixels
0
20
40
60
−0.1
−0.05
0
0.05
0.1
0.15
X Error
#Frame
0
20
40
60
Y Error
#Frame
0
20
40
60
Z Error
#Frame
0
20
40
60
−0.1
−0.05
0
0.05
0.1
Roll Error [rad]
#Frame
0
20
40
60
Pitch Error [rad]
#Frame
0
20
40
60
Yaw Error [rad]
#Frame
(b) 1-point RANSAC, σz = 0.5 pixels
0
20
40
60
−0.15
−0.1
−0.05
0
0.05
0.1
0.15
X Error
#Frame
0
20
40
60
Y Error
#Frame
0
20
40
60
Z Error
#Frame
0
20
40
60
−0.1
−0.05
0
0.05
0.1
Roll Error [rad]
#Frame
0
20
40
60
Yaw Error [rad]
#Frame
0
20
40
60
Pitch Error [rad]
#Frame
(c) JCBB, σz = 0.2 pixels
0
20
40
60
−0.15
−0.1
−0.05
0
0.05
0.1
0.15
X Error
#Frame
0
20
40
60
Y Error
#Frame
0
20
40
60
Z Error
#Frame
0
20
40
60
−0.1
−0.05
0
0.05
0.1
Roll Error [rad]
#Frame
0
20
40
60
Pitch Error [rad]
#Frame
0
20
40
60
Yaw Error [rad]
#Frame
(d) 1-point RANSAC, σz = 0.2 pixels
0
20
40
60
−0.15
−0.1
−0.05
0
0.05
0.1
0.15
X Error
#Frame
0
20
40
60
Y Error
#Frame
0
20
40
60
Z Error
#Frame
0
20
40
60
−0.1
−0.05
0
0.05
0.1
Roll Error [rad]
#Frame
0
20
40
60
Pitch Error [rad]
#Frame
0
20
40
60
Yaw Error [rad]
#Frame
(e) JCBB, σz = 0.2 pixels, α = 0.5
Fig. 4.7. Camera location errors when using JCBB is shown in Figures 4.7(a) and 4.7(c),
for standard deviations of 0.5 and 0.2 pixels respectively. Figures 4.7(b) and 4.7(d) showing
1-point RANSAC results for the same ﬁlter tuning are repeated here for comparison. It can
be seen that 1-point RANSAC outperforms JCBB in both cases. Figure 4.7(e) shows the best
JCBB tuning found by the authors, which still gives worse results than 1-point RANSAC.

4.5 Experimental Results
87
0
500
1000
1500
2000
2500
0
0.2
0.4
0.6
0.8
1
#Frame
outliers/(inliers+outliers)
(a) JCBB, σz = 0.5 pixels.
0
500
1000
1500
2000
2500
0
0.2
0.4
0.6
0.8
1
#Frame
outliers/(inliers+outliers)
(b) 1-point RANSAC, σz = 0.5 pixels
0
500
1000
1500
2000
2500
0
0.2
0.4
0.6
0.8
1
#Frame
outliers/(inliers+outliers)
(c) JCBB, σz = 0.2 pixels
0
500
1000
1500
2000
2500
0
0.2
0.4
0.6
0.8
1
#Frame
outliers/(inliers+outliers)
(d) 1-point RANSAC, σz = 0.2 pixels.
Fig. 4.8. Spurious match rate for JCBB and RANSAC when measurement noise standard
deviation σz is reduced to 0.2 pixels. It can be observed that reducing the measurement noise
makes both techniques stricter, but 1-point RANSAC remains more discriminative.
The value of tWC1 is taken from the GPS data in the ﬁrst camera frame. Trajectory
estimation from pure monocular vision will not be able to recover the scale s, which
will remain unknown. For the combination of a monocular camera and wheel odom-
etry input, the overall scale of the estimation is observed by odometry readings and
then s = 1 in Equation 4.23. The rotation between GPS and the ﬁrst camera position
RWC1 will be unknown in every case, as it is non-observable from GPS readings.
The unknown parameters of the alignment (s and RWC1 for pure monocular, and
only RWC1 for monocular plus wheel odometry) are obtained via a non-linear opti-
mization that minimizes the error between the aligned trajectory rWCk and the GPS
trajectory rWGPSk.
For the sake of simplicity, the assumption that the position of the camera sensor
and the GPS antenna coincide on the robot has been made in the above reasoning,
which is reasonable as the position of the sensors differ by only a few centimetres
and robot paths cover hundreds of metres.
Finally, the error of each camera position in the reconstructed path is computed
as the Euclidean distance between each point of the estimated camera path and GPS
path, both in the W reference:
ek =

(rWCk −rWGPSk)⊤(rWCk −rWGPSk).
(4.24)

88
4 1-Point RANSAC
0
500
1000
1500
2000
2500
0
0.05
0.1
0.15
#Frame
seconds
Cicle time
2nd EKF Update
1st EKF Update
Ransac
Prediction
#Features
Times & Map Size
0
20
40
60
80
100
120
140
Map Size
(a) 5-point RANSAC, σz = 0.5 pixels
0
500
1000
1500
2000
2500
0
0.01
0.02
0.03
0.04
0.048
#Frame
seconds
Cicle time
2nd EKF Update
1st EKF Update
Ransac
Prediction
#Features
Times & Map Size
0
20
40
60
80
100
120
140
Map Size
(b) 1-point RANSAC, σz = 0.5 pixels
0
500
1000
1500
2000
2500
0
0.01
0.02
0.03
0.04
0.048
#Frame
seconds
Cicle time
2nd EKF Update
1st EKF Update
Ransac
Prediction
#Features
Times & Map Size
0
20
40
60
80
100
120
140
Map Size
(c) 1-point exhaustive hypothesis, σz = 0.5
pixels
0
500
1000
1500
2000
2500
0
0.05
0.1
0.15
#Frame
seconds
Cicle time
EKF Update
JCBB
Prediction
#Features
Times & Map Size
0
20
40
60
80
100
120
140
Map Size
(d) JCBB, σz = 0.5 pixels
0
500
1000
1500
2000
2500
0
0.01
0.02
0.03
0.04
0.048
#Frame
seconds
Cicle time
2nd EKF Update
1st EKF Update
Ransac
Prediction
#Features
Times & Map Size
0
20
40
60
80
100
120
Map Size
(e) 1-point RANSAC, σz = 0.2 pixels
0
500
1000
1500
2000
2500
0
0.2
0.4
0.6
0.8
1
#Frame
seconds
Cicle time
EKF Update
JCBB
Prediction
#Features
Times & Map Size
0
20
40
60
80
100
120
140
Map Size
(f) JCBB, σz = 0.2 pixels
Fig. 4.9. Detail of times and map sizes for different RANSAC and JCBB conﬁgurations in
double y-axis ﬁgures: times are shown as areas and measured in seconds on the left y-axis;
the map size is displayed as a a red line and is measured on the right y-axis. 1-point RANSAC
exhibits much lower computational cost than 5-point RANSAC and JCBB. 1-point RANSAC
also shows only a small increase when made exhaustive or stricter, making it suitable for real-
time implementation at 22 Hz for the map size detailed in the ﬁgures.

4.5 Experimental Results
89
4.5.5
Pure Monocular EKF-Based Estimation for Long
Sequences
Three different sequences from the RAWSEEDS dataset have been used to test the
validity of the 1-point RANSAC EKF for long-term camera motion estimation. All
sequences were recorded by a 320 × 240 Unibrain camera with a wide-angle lens
capturing at 30 fps.
In the ﬁrst sequence, consisting of 6000 images, the robot translates for about
146 metres. The second sequence has 5400 images and the robot describes a similar
trajectory length, about 153 metres. Finally, a very long and challenging sequence
is evaluated that consists of 24180 frames (13.5 minutes of video) in which the
robot describes a trajectory of 650 metres. In this latter sequence, although the ac-
cumulated drift makes the error noticeable when plotted with the GPS trajectory,
the relative error with respect to the trajectory keeps the same low value as the other
two shorter sequences (1% of the trajectory length).
Figure 4.10 shows an image from the 650 metres experiment, along with the
tracked features. It can be observed that around 1–2 hundred features per frame had
to be measured in order to reduce scale drift error. This high number will increase
the computational cost of the EKF beyond real-time bounds for the pure monocu-
lar case. In the particular experiments presented, the algorithm runs at about 1 Hz.
Nevertheless, it will be shown in next subsection how introducing extra informa-
tion about the scale will reduce the number of measurements, enabling real-time
performance for the combination of visual tracking plus wheel odometry.
Fig. 4.10. Image from the 650 metres sequence, showing the high number of tracked features.

90
4 1-Point RANSAC
Figure 4.11 shows the estimated (in black) and the GPS (in red) trajectories over
a top view extracted from Google Maps for each one of the sequences. The accu-
racy of the estimated trajectories is clear from visual inspection. Table 4.1 details
the maximum and mean errors obtained in these experiments and also for the exper-
iment in the next section combining monocular vision and wheel odometry inputs.
Figure 4.12 shows histograms of the errors for the three sequences.
Table 4.1. EKF-based visual estimation error for long camera trajectories.
Trajectory
length [m]
Sensor
used
Mean
error [m]
Maximum
error [m]
% mean error over
the trajectory
146
monocular
1.3
4.2
0.9%
153
monocular
1.9
3.3
1.1%
650
monocular
6.4
11.1
1.0%
1310
monocular and
wheel odometry
9.8
23.6
0.7%
Subﬁgures 4.12(c) and 4.12(d) in this latter ﬁgure show histograms of the errors
for the 650 metres experiment in two different versions of the 1-point RANSAC al-
gorithm: the ﬁrst one of them using the algorithm 1 and the second one replacing the
random hypotheses generation with exhaustive hypotheses generation as evaluated
in Figure 4.5(c). The conclusion from section 4.5.2 is conﬁrmed here: exhaustive
hypothesis generation only very slightly improves the estimation errors; so adaptive
random 1-point RANSAC should be preferred.
4.5.6
Visual Odometry from a Monocular Sequence Plus Wheel
Odometry
Figure 4.13 shows the trajectory obtained by the visual odometry algorithm over
a GoogleMaps plot and compared against GPS data. The length of the estimated
trajectory is about 1310 metres and was covered by the RAWSEEDS mobile robot
in 30 minutes, capturing 54000 frames. The maximum and mean error were 23.6 and
9.8 metres respectively. Adding wheel odometry information allowed us to reduce
the number of tracked features to 25, enabling real-time operation at 30 frames per
second.
The processing time per frame for this sequence using 1-point RANSAC can
be observed in Figure 4.14 in the form of a histogram. It can be noticed that the
total computational cost per step is under 33 milliseconds in 98% of the frames,
suggesting that the algorithm is suitable for real-time implementation. It can be ob-
served in the right-hand ﬁgure that for the same number of image measurements
JCBB’s computational cost far exceeds real-time constraints in a large number of

4.5 Experimental Results
91
(a) 146 metre trajectory
(b) 156 metre trajectory
(c) 650 metre trajectory
Fig. 4.11. Estimated trajectories from pure monocular data and GPS data.

92
4 1-Point RANSAC
0
1
2
3
4
0
0.025
0.05
0.075
0.1
Error [m]
Frequency
146 metres sequence,
adaptive 1−Point RANSAC
(a) 146 metres trajectory
0
1
2
3
0
0.01
0.02
0.03
0.04
Error [m]
Frequency
153 metres
sequence,
adaptive
1−point
RANSAC
(b) 156 metres trajectory
0
1
2
3
4
5
6
7
8
9 10 11 12
0
0.01
0.02
0.03
0.04
0.05
Error [m]
Frequency
650 metres
sequence, 1−point
adaptive RANSAC
(c) 650 metres trajectory
0
2
4
6
8
10
0
0.01
0.02
0.03
0.04
0.05
Error [m]
Frequency
650 metres sequence,
exhaustive hypothesis
generation
(d) 650 metres trajectory; Exhaustive-SAC
Fig. 4.12. Histograms of the errors for the three experiments using only monocular informa-
tion.
frames. JCBB’s exponential complexity arises in this experiment in frames where a
signiﬁcant proportion of outliers are present, expanding the tail of the histograms of
the ﬁgure. For this particular experiment, JCBB’s histogram expands to 2.4 seconds
while 1-Point RANSAC’s maximum time only reaches 0.44 seconds.
Figure 4.14 also shows two histograms representing the computational cost of
both algorithms when the number of features in the image is increased to 50. It can
be observed that the cost of 1-Point RANSAC grows, but still the processing cost is
always on the order of tenths of a second. JCBB’s cost reaches maximum values of
several hours, and processing times of several seconds per frame are not unusual
Figure 4.15(b) shows raw odometry as a red thin line and GPS with a blue thick
line for comparison. It can be observed that early drift appears and the plotted tra-
jectory is rather far from the GPS locations. Figure 4.15(a) shows pure monocular
estimation in thin red and GPS measurements in thick green. Observing this plot
carefully, it can be observed that a monocular camera is able to very accurately esti-
mate orientation, but the unobservability of the scale produces drift in this parameter
for the number of tracked features (25) considered in this experiment.
Finally, Figure 4.15(c) details the estimated trajectory that can be achieved from
the combination of the two sensors. Accurate estimation is achieved for a trajectory

4.6 Discussion
93
Fig. 4.13. Visual odometry results compared against RTK GPS over a Google Maps plot.
of 1.3 kilometres, which can be compared with state of the art in monocular visual
odometry, e. g. [Scaramuzza et al. 2009].
4.6
Discussion
A novel RANSAC algorithm is presented in this chapter which, for the ﬁrst time
and differently from standard purely data-driven RANSAC, incorporates a priori
probabilistic information into the hypothesis generation stage. As a consequence
of using this prior information, the sample size for the hypothesis generation loop
can be reduced to the minimum size of 1 point data. 1-point RANSAC has two
main strengths worth summing up here. First, as in standard RANSAC, model con-
straints are checked after hypothesis data has been fused with the a priori model,
an advantage over JCBB. Second, using 1-point plus prior knowledge hypotheses
greatly reduces the number of hypotheses to construct and hence the computational
cost compared with usual RANSAC based solely on data. Its linear cost in the state
size also outperforms JCBB’s exponential complexity in the number of outliers. In
a practical sense, its linear complexity means an overhead of less than 10% of the
standard EKF cost, making it suitable for real-time implementation in local visual
SLAM or SfM.

94
4 1-Point RANSAC
0
0.01
0.02
0.03
0.04
0.05
0
0.05
0.10
0.15
0.2
Cost per frame [seconds]
Frequency
0.033 s
(a) 1-point RANSAC; 25 measured features
per frame
0
0.2
0.4
0.6
0.8
1
0
0.01
0.02
0.03
0.04
Cost per frame [seconds]
Frequency
0.033 s
(b) JCBB; 25 measured features per frame
0
0.1
0.2
0.3
0.4
0
0.01
0.02
0.03
0.04
Cost per frame [seconds]
Frequency
0.033 s
(c) 1-point RANSAC; 50 measured features
per frame
0
5
10
15
20
0
0.005
0.010
0.015
Cost per frame [seconds]
Frequency
0.033 s
(d) JCBB; 50 measured features per frame
Fig. 4.14. Histograms showing the computational cost for RANSAC and JCBB for the cases
of 25 and 50 image points per frame. Experiment 4.14(d) had to be early terminated at frame
1533, as JCBB computational cost rises in some frames up to 1544 seconds
While the relevance of algorithms like JCBB or the recent Active Matching (AM)
reside on their generality, the main advantage in the presented approach is its efﬁ-
ciency. 1-point RANSAC is directed to the particular case of a rigid scene. The rich
variety of correlation patterns that a covariance matrix can encode is manageable
by general methods like JCBB or AM. Our 1-point RANSAC exploits the very sim-
ple pattern where all the correlations are mainly explained by sensor motion, and
hence small size data subsets are enough to constraint the rest of the measurements.
For more complex models, like non-rigid scenes or multi-object tracking, 1-point
RANSAC may not offer such a satisfactory result.
Nevertheless, it is also true that estimation from a moving sensor’s data stream in
an almost rigid scene covers a great percentage of SLAM problems; and a speciﬁc
method more efﬁcient than general methods can be of importance. In this sense,
1-point RANSAC outperforms existing approaches by presenting lower cost and
scaling well with the state vector and measurement size, and also with the outlier
rate.

4.6 Discussion
95
−200
−150
−100
−50
0
50
−200
−150
−100
−50
0
GPS
Only Monocular Camera
(a) Pure monocular estimation (thin red)
tracking 25 features and GPS trajectory
(thick blue). Large errors appear caused
by scale drift, which is unobservable by a
monocular camera.
−200
−150
−100
−50
0
50
−200
−150
−100
−50
0
GPS
Raw Wheel Odometry
(b) Raw odometry measurements (thin red)
and GPS trajectory (thick blue). Errors in raw
odometry are caused by early drift typical
from proprioceptive sensors.
−200
−150
−100
−50
0
50
−200
−150
−100
−50
0
GPS
Monocular and
Wheel Odometry
Combination
(c) Visual Odometry estimation from the combination of monocular cam-
era plus wheel odometry (thin red) and GPS trajectory (thick blue). The
combination of both sensors overcomes their deﬁciencies when used alone.
Real-time performance at 30 Hz can be achieved, and error is 0.7% of the
trajectory.
Fig. 4.15. Pure monocular estimation showing scale drift in Figure 4.15(a), raw odometry
input showing drift in Figure 4.15(b) and visual odometry results combining the two in Figure
4.15(c); all are compared against GPS trajectory (thick blue line).

96
4 1-Point RANSAC
Besides its efﬁciency, 1-point RANSAC also has some advantages in dealing with
non-linearities as a result of checking rigidity after data fusion where some of the
inaccuracies introduced by non-linearities have been compensated. This advantage
is shared with Active Matching. On the contrary JCBB checks rigidity before data
fusion, which is a serious drawback of the algorithm.
Since 1-point RANSAC is able to deal with large outlier rates at low computa-
tional overhead, we ﬁnd it interesting to force the EKF into a low measurement error
operation mode. For a small cost increase, the EKF is fed only very accurate mea-
surements (selected by “a survival of the ﬁttest” process, where the ﬁttest features
are those producing the lowest error measurements) and hence the accuracy of the
estimation is improved as seen in Figure 4.5(d). This particular operation mode can
only be achieved due to the efﬁciency of the presented algorithm, being impractical
if spurious match rejection is expensive.
It is also worth remarking that although this book is focused on the particular
case of EKF monocular SLAM, the new 1-point RANSAC method presented here
is independent of the type of sensor used. The only requirement is the availability
of highly correlated prior information, which is typical of EKF SLAM for any kind
of sensor used — and also in the multisensor case. Also, as highly correlated pri-
ors are not exclusive to EKF SLAM, the applicability of 1-point RANSAC could
be even broader. As an example, we think that camera pose tracking in keyframe
schemes [Klein & Murray 2008, Mouragnon et al. 2009] would beneﬁt from our 1-
point RANSAC cost reduction if a dynamic model were added to predict camera
motion between frames.
This chapter also presents a method for benchmarking 6 degrees of freedom cam-
era motion estimation results. The method presents three clear advantages: First, it
is intended for real image sequences and includes effects difﬁcult to reproduce by
simulation (like non-Gaussian image noise, shaking handy motion, image blur or
complex scenes). Second, it is easily reproducible as the only hardware required
is a high resolution camera. And third, the effort required by the user is low. The
uncertainty of the estimated solution also comes as an output of the method and
the appropriateness of Bundle Adjustment estimation as reference can be validated.
The method has been used to prove the claimed superiority of the 1-point RANSAC
method described in the chapter.
The general EKF plus 1-point RANSAC algorithm has been experimentally tested
for the case of large camera trajectories in outdoor scenarios. Sensor-centered ﬁl-
tering instead of the traditional world-centered method has been used in order to
reduce the uncertainty in the area local to the current camera and reduce lineariza-
tion errors. For the pure monocular case, errors around 1% of the trajectory have
been obtained for trajectories up to 650 metres from a publicly available dataset.
The number of tracked features in the image has to be increased to 1–2 hundreds
in order to avoid scale drift. This high number makes this case currently moves us
away from real-time performance, and the method runs at 1 frame per second.

4.6 Discussion
97
The combination of monocular vision and wheel odometry has also been bench-
marked for the visual odometry application. The extra odometric information makes
scale observable; the number of tracked features can be reduced and real-time per-
formance can be achieved for this case. A 1300 metre long trajectory has been es-
timated in the chapter, with the mean error against GPS coming out at 0.7% of the
trajectory.


Chapter 5
Degenerate Camera Motions and Model
Selection
Abstract. The assumption of a general camera motion –translation and rotation– be-
tween frames in an image sequence leads to inconsistent estimations when the cam-
era performs more restricted motions, like pure rotation, or even no motion. In this
situation, the noise present in the data ﬁts the extra terms in the overparametrized
model and artiﬁcially estimates dimensions for which we have no information.
This chapter presents an Interacting Multiple Models (IMM) framework which can
switch automatically between parameter sets in monocular SLAM; selecting the
most appropriate motion model at each step. Remarkably, this approach of full se-
quential probability propagation means that there is no need for penalty terms to
achieve the Occam property of favouring simpler models –this arises automatically.
We demonstrate our method with results on a complex real image sequence with
varied motion.
5.1
Introduction
Image sequence processing relies on camera motion models to actively identify po-
tential point matches. Off-line methods rely on geometrical models relating two or
three images to robustly compute matches. In [Torr et al. 1999] it is shown how dif-
ferent models should be used at different parts of a general sequence to avoid degen-
erate geometries. This geometrical model selection has been extended to segment
different motion models between image pairs or triplets [Schindler & Suter 2006,
Kanatani 2004, Torr 2002].
In contrast to these two or three-view geometrical models, the probabilistic mo-
tion models used in SLAM are well suited to modelling long sequences of close
images instead of discrete sets of images. However a single probabilistic model can
similarly only deal with sequences which follow the prescribed model or processing
will fail. In this chapter, the monocular EKF SLAM is extended to deal with more
J. Civera et al.: Structure from Motion Using the Extended Kalman Filter, STAR 75, pp. 99–110.
springerlink.com
c⃝Springer-Verlag Berlin Heidelberg 2012

100
5 Degenerate Camera Motions and Model Selection
than one probabilistic motion model, expanding the range of sequences compati-
ble with the priors represented by a set of tuning parameters. We use a sequential
Bayesian approach to model selection.
Thanks to Bayesian probability propagation, monocular SLAM with a general
translating camera can deal with low parallax motions — such as rotations — pro-
vided that the camera re-observes map features whose locations are well-estimated
as a result of parallax observed previously in the sequence, and so model switch-
ing is not a must in some cases where it would be in the off-line approaches. How-
ever, when monocular SLAM is initialised on-the-ﬂy without a known scene pattern,
model selection is an issue. If the camera initially undergoes a low parallax motion,
no reliable estimation is possible. Any measurement noise may be considered paral-
lax by the ﬁlter producing inconsistent depth estimates. We tackle this problem with
model selection.
Multiple model methods are well known in maneuvering target tracking. An ex-
cellent and recent survey of this can be found in [Li & Jilkov 2005]. In this chapter,
we adapt to the SLAM problem the most widespread of those methods, Interacting
Multiple Models (IMM), initially proposed by Blom in [Blom & Bar-Shalom 1988].
The IMM estimator is a suboptimal hybrid ﬁlter — that is, it estimates the continu-
ous values of a proccess, and the discrete probabilities of a set of models — whose
main features are: 1) It assumes that the system can jump between the members of a
set of models, which is the case of our monocular SLAM estimation, and 2) It offers
the best compromise between complexity and performance.
Thanks to the use of multiple models, the range of image sequences that can be
processed with a single system tuning is enlarged. We work with a bank of 7 models:
one model of a stationary camera, three models of pure rotation motion (constant
angular velocity) with different angular acceleration covariances, and three general
translation + rotation models (constant velocity, constant angular velocity) with dif-
ferent angular and linear acceleration covariances. Via the Bayesian model selection
of IMM, the system prefers simpler (less general) models where they ﬁt the data.
As a result, the search regions for the predicted image features are smaller than with
a single model EKF. These reduced search regions increase mismatch rejection and
reduce the processing cost of image search. Additionally, the computed probabilities
per model allow the segmentation of a sequence into different models.
Section 5.2 discusses and formulates the sequential Bayesian model selection.
The Interacting Multiple Model approach to Bayesian model selection is detailed in
5.3. Some details about the use of IMM in the SLAM problem are given in section
5.4. Section 5.5 veriﬁes the method using real imagery and shows how it deals with
sequence bootstrap. Finally section 5.6 summarises the paper’s conclusions.

5.2 Bayesian Model Selection for Sequences
101
5.2
Bayesian Model Selection for Sequences
In standard single-model monocular SLAM algorithms, Bayes’ rule combines at ev-
ery step past estimation information with current image data. Given the background
information I and the image data at current step D, the posterior probability density
function for the set of parameters θ deﬁning our model M is updated via Bayes’
formula:
p(θ|DMI) = p(θ|MI) p(D|θMI)
p(D|MI) .
(5.1)
In this chapter we consider cases where a single model M is not sufﬁcient to
cover all of the sequences we would like to track. Taking full advantage of the fully
probabilistic estimation that our SLAM approach is performing, we formulate our
multiple model problem in a Bayesian framework.
Consider, as Jaynes does in Chapter 20 of his book [Jaynes 2003], a discrete set
of models M = {M1,...,Mr} — rather than a single one — which might feasibly
describe the assumptions of a sequential SfM process. We start by assigning initial
scalar probabilities P(M1|I),...,P(Mr|I) which represent prior belief about the dif-
ferent models based on background information I, and which are normalised to add
up to one. If no prior information exists, these probabilities may well be assigned
initially equal.
At each new image, where we acquire image measurements data D, we update
the probability of each model according to Bayes’ rule:
P(M j|DI) = P(M j|I)P(D|M jI)
P(D|I)
(5.2)
In this expression, the ﬁrst term is the probability of the model being correct given
only the prior information. In the fraction, the numerator is the likelihood of ob-
taining the data given that the model is correct. The denominator is the normalizing
constant, computation of which can be avoided when the posterior probabilities of
a mutually-exclusive set of models are all computed, or alternatively cancels out
when the ratio of posterior probabilities of different models is calculated.
So, what is the likelihood P(D|M jI) of the data given a model in a monocular
SLAM system? It is simply the joint likelihood of all of the feature measurements
in an image:
P(D|MI) =
1

2πSk|k−1

exp

−1
2ν⊤S−1
k|k−1ν

,
(5.3)
where
ν = z−h(ˆxk|k−1) ,
(5.4)
Sk|k−1 = Hk|k−1

FPk−1|k−1F⊤+GQG⊤
H⊤
k|k−1 + R .
(5.5)

102
5 Degenerate Camera Motions and Model Selection
F = ∂f
∂x and G = ∂f
∂n are the Jacobians of the dynamic model f with respect to the
state vector x and the process noise n respectively. Hk|k−1 =
∂h
∂xk|k−1 is the Jacobian
of the measurement equation h with respect to the state vector xk|k−1. Sk|k−1 is the
covariance of the predicted image measurements, which is computed by adding the
linear propagation the state vector uncertainty Pk|k−1 and the image noise covari-
ance R.
We should note, as Jaynes explains with great clarity, that in this correctly formu-
lated Bayesian approach to model selection there is no need for ad-hoc terms like
Minimum Description Length which penalise ‘complex’ models and favour simple
ones. The ‘Occam principle’ of selecting the simplest model which is able to cap-
ture the detail of the data and avoiding overﬁtting is taken care of automatically
by correctly normalising our comparison of different models. The big difference
between our approach and the common two-view model selection methods (e.g.
[Kanatani 2004, Torr 2002, Schindler & Suter 2006]) which require penalty terms
is that our concept of a model is probabilistic at its core, not just geometric (like ho-
mography, afﬁne, ...). For our use in sequential probabilistic tracking, a model must
actually deﬁne a probability distribution during a transition. This is what makes it
possible to calculate proper likelihoods for the models themselves, independent of
parameters.
The formulation above allows us to obtain posterior probabilities for our models
in one frame, but we are interested in propagating these probabilities through a
sequence. This is achieved by deﬁning a vector of model probabilities — a ‘state
vector’ for models or set of mixing weights:
μk|k =

μ1
k|k ...μr
k|k
⊤
.
(5.6)
We ﬁll μk|k with the prior model probabilities P(M1|I),...,P(Mr|I) before pro-
cessing the ﬁrst image, and after processing use the values of μk|k as the priors for
each model in Equation 5.2 and then replace these values with the posterior values
calculated.
A ﬁnal step is needed in between processing images, which is to apply a mixing
operator to account for possible transitions between models. With a homogeneous
Markov assumption that the probability of transition from one model to any other is
constant at any inter-frame interval, this is achieved by:
μk|k−1 = πμk−1|k−1 ,
(5.7)
where π is a square matrix of transition probabilities where each row must be nor-
malised. In the typical case that the dominant tendancy is sustained periods of mo-
tion with one model, this matrix will have large terms on the diagonal. If the models
are ordered with some sense of proximity, the matrix will tend to have large values
close to the diagonal and small ones far away.

5.3 Interacting Multiple Model
103
The sequential process of calculating model probabilities therefore evolves as a
loop of mixing and update steps and at motion transitions in the sequence evidence
will accrue over several frames.
5.3
Interacting Multiple Model
IMM is presented in the tracking literature as a hybrid estimation scheme, well
suited to estimating the continuous state of a system that can switch between sev-
eral behaviour modes. This hybrid system is then composed of a continuous part
(the state) and a discrete part (the behaviour modes). The continuous part of such a
system is deﬁned by its state and measurement equations:
˙x(t) = f(x(t),M (t),w(t),t)
(5.8)
z(t) = h(x(t),M (t),v(t),t)
(5.9)
where the dynamics of the process and the measurements depend not only on the
state x(t) and the process and measurement noise w(t) and v(t) at time t, but also
on the model M (t) that governs the system at time t. The probability of each of those
REINITIALIZATION
FILTER
k−1 k−1
X
(m)
DISCRETE PART
CONTINUOUS PART
PREDICTION
UPDATE
k k
μ
k k−1
μ
k−1 k−1
X
(1)
PREDICTION
PREDICTION
UPDATE
UPDATE
X k k−1
(1)
(m)
X k k−1
X
(1)
k k
k k
X
(m)
X k k
ESTIMATE
FUSION
FILTER BANK
MODEL m BASED FILTER
MODEL 1 BASED FILTER
z
z
z
Fig. 5.1. Interacting Multiple Model algorithm scheme

104
5 Degenerate Camera Motions and Model Selection
1. Filter reinitialization (for i = 1,2,...,r):
Predicted model probability:
μi
k|k−1 = P{Mi
k|zk−1} = ∑j π jiμ j
k−1
Mixing weight:
μ j|i
k−1 = P{M j
k−1|Mi
k,zk−1} = π jiμ j
k−1/μi
k|k−1
Mixing estimate:
¯xi
k−1|k−1 = E[xk−1|mi
k,zk−1] = ∑j xj
k−1|k−1μ j|i
k−1
Mixing covariance:
¯Pi
k−1|k−1 = ∑j(Pj
k−1|k−1+
+(¯xi
k−1|k−1 −ˆx j
k−1|k−1)(¯xi
k−1|k−1 −ˆxj
k−1|k−1)⊤)μ j|i
k−1
2. EKF bank ﬁltering (for i = 1,2,...,r):
Prediction: ˆxi
k|k−1,Pi
k|k−1,h(xi
k|k−1),Si
k|k−1
Measurement: zk
Update: ˆxi
k|k,Pi
k|k
3. Model probability update (for i = 1,2,...,r):
Model likelihood: Li
k = N (νi
k;0,Si
k)
Model probability: μi
k =
μi
k|k−1Li
k
∑j μ j
k|k−1Lj
k
4. Estimate fusion
Overall state:
ˆxk|k = ∑i ˆxi
k|kμi
k
Overall covariance:
Pk|k = ∑i

Pi
k|k +(ˆxk|k −ˆxi
k|k)(ˆxk|k −ˆxi
k|k)⊤
μi
k
Fig. 5.2. Interacting Multiple Model algorithm
models being effective at time t is coded in the discrete probability vector μk−1|k−1,
as explained in section 5.2.
Figure 5.1 shows graphically the structure of the IMM estimator. The whole al-
gorithm is detailed in Figure 5.2. The central part of the algorithm consists of a bank
of r ﬁlters running in parallel, each one under a different model. An overall estima-
tion for the state can be obtained as a sum of the a posteriori estimation of every
ﬁlter weighted with the discrete a posteriori model probabilities.
A key aspect of the IMM algorithm is the reinitialisation of the ﬁlter before the
parallel computation of the ﬁlter bank at every step. This mixing of the estimations
allows individual poor estimates caused by model mismatch to recombine with es-
timates from better models, so that the whole ﬁlter bank beneﬁts from the better
estimates.
5.4
Interacting Multiple Model Monocular SLAM
Given the tracking-oriented IMM algorithm, some aspects have to be taken into
account before applying it to our particular monocular SLAM problem.

5.5 Experimental Results
105
1. Active search and 1-point RANSAC: In the multiple model tracking literature,
little attention is given to the matching (data association) proccess, which is crucial
in SLAM algorithms. If matching is mentioned, as in [Kirubarajan et al. 1998], it
is said that the most general model, that is, the model with the largest covariance,
is used to compute the measurement covariance for gating correspondences—the
implication is ‘always to expect the worst’.
In monocular SLAM, most of the time this weakest search region is unnecesary
large, increasing both the computational cost and the risk of obtaining a false
match. A more realistic search region can be deﬁned by the combination of the
individual ﬁlters weighted by their discrete probabilities. The only assumption
that has to be made is that motion changes are smooth, a reasonable assumption
when dealing with image sequences at high frame rate. The form of the image
search regions is therefore determined by the following equations:
ˆhk|k−1 = ∑
i
ˆhi
k|k−1μi
k|k−1
(5.10)
Sk|k−1 = ∑
i
(Si
k|k−1 +(ˆhk|k−1 −ˆhi
k|k−1)
(5.11)
(ˆhk|k−1 −ˆhi
k|k−1)⊤)μi
k|k−1
(5.12)
Spurious rejection is not mentioned in the multiple model literature either. In our
monocular SLAM case, the 1-point RANSAC developed in the chapter 4 can be
easily adapted to the Interacting Multiple Model framework.
2. Map management: As detailed in section 2.4.3, map management strategies for
deleting bad features and adding new ones are convenient in monocular SLAM.
We are also using inverse depth to cartesian conversion in order to reduce the
computational cost of the algorithm as detailed in section 3.6.
5.5
Experimental Results
A 1374 frame sequence was recorded with a 320×240 wide-angle camera at 30fps.
The camera makes a motion consisting of the following sequence of essential move-
ments: stationary →pure rotation →general motion (translation and rotation) →
pure rotation →stationary. The sequence has been processed using the dimension-
less inverse depth formulation in the appendix B and two different types of motion
modelling. Firstly, IMM EKF formulation with a bank of seven models: stationary
camera, rotating camera (three angular acceleration levels with standard deviation
0.1, 0.5 and 1 pixels), and general motion (with 3 acceleration levels for both lin-
ear and angular components with standard deviations of 0.1, 0.5 and 1 pixels). Sec-
ondly, as a base reference, a single model for general motion with acceleration noise
standard deviation of 1 pixel, both angular and linear. Both formulations are fed the

106
5 Degenerate Camera Motions and Model Selection
same starting image feature detections. On analysing the results the advantages of
the IMM over single model monocular SLAM become clear.
5.5.1
Consistent Start Up Even with Rotation
As was said in section 5.1, single model EKF SLAM leads to inconsistent mapping
if the camera initially undergoes low parallax motion. In the analysed sequence,
we have an extreme case of this as the camera is either stationary or rotating for
more than 600 frames. Figure 5.3 compares the estimation results with a single
model EKF and our IMM algorithm at step 600, when the camera has performed
non-translational motion. Features are plotted as arrows if (as should be the case)
no ﬁnite depth has been estimated after the no parallax motion. It can be observed
that, for the single model case, all features have collapsed to narrow, false, depth
estimates while in the IMM case all of the features have no depth estimation.
5.5.2
Low Risk of Spurious Matches due to Small Search Regions
It can be noticed in Figure 5.4 that although high process noise models are nec-
essary in order to retain tracking features during high accelerations, these models
are scarcely used for any length of time. In hand-held camera sequences, constant
velocity motions are much more common than accelerated ones. This is reﬂected
by the model probabilities, as we see that the highest probabilities are given to the
lower acceleration noise models on most frames.
When using a single model estimation, we are forced to choose the most general
model in order to maintain tracking under high acceleration. As process noise di-
rectly inﬂuences search region size, we are forced to maintain large search regions,
unnecessary most of the time. As a consequence, the risk of obtaining false matches
grows. As IMM selects at any time the most probable motion model, preferring sim-
pler models, it adjust the search region to the real motion at any time, resulting in
considerably reduced ellipses and lowering the risk of mismatches.
In Figure 5.5 the large factor of reduction in the size of search ellipses can be
observed. Subﬁgure (a) shows a detail of a feature search region at frame 100, at the
top using IMM and at the bottom using a single model. Search regions in subﬁgure
(b) correspond to the same feature at frame 656, when camera starts translating
and high acceleration is detected. Notice that the IMM ellipse slightly enlarges in
adapting to this motion change, but continues to be smaller than the single-model
one. Finally, (c) exhibits the consequences of having unnecessary big search regions:
false correspondences happen. Due to mismatches like this one, the estimation in
this experiment fails catastrophically.

5.5 Experimental Results
107
Fig. 5.3. (a, left) frame 600 and (a, right) 3D top view of the IMM estimation at this frame.
The camera has been either stationary or rotating until this frame. It can be seen in Figure 5.4
that rotation and still camera models have high probability throughout this early part of the
sequence. IMM, correctly, has not estimated any feature depth –features whose depths have
not been estimated (their depth uncertainties, stored in inverse depth formulation, encompass
inﬁnity) are plotted as arrows–. (b), frame 600 and top-viewed estimation with single-model
monocular SLAM. The overparametrized model has led to narrow, false depth estimates.
When the camera translates this inconsistent map leads to false matches that cause the esti-
mation to fail, as seen in (d) at frame 927 of the sequence. On the other hand, (c) shows the
correct map estimation performed by the IMM algorithm at this same frame 927.

108
5 Degenerate Camera Motions and Model Selection
Fig. 5.4. Posterior model probabilities along the sequence. Each model is represented by its
acceleration noise standard deviation[σa,σα] expressed in pixels, following the notation in
[Civera et al. 2007b]. Notice that the probability for the most general model (σa = 1pxl,σα =
1pxl) is always under 0.01. The stationary camera model (a) and low acceleration noise mod-
els (b) and (c) are assigned the highest probabilities in most of the frames. In spite of being
rarely selected, the high acceleration noise models are important to keep the features track at
the frames where motion change occurs (small spikes are visible at these points).

5.5 Experimental Results
109
Fig. 5.5. (a), IMM (top) and single-model (bottom) feature search ellipse when the camera is
rotating. (b), the same feature IMM and single-model search regions when the camera begins
to translate. (c), mismatch in the single-model case caused by an unnecesary large ellipse
that does not occur in the IMM estimation. Several mismatches like this one in the highly
repetitive texture of the brick wall eventually may cause full tracking failure.
5.5.3
Camera Motion Model Identiﬁcation
The IMM not only achieves better performance in processing the sequence, but also
provides a tool to segment the sequence according to the dominant motion model.
It is worth noting that this segmentation is based on sequence criteria as opposed to
a classical pairwise motion model selection in geometrical vision.
In Figure 5.4 it can be seen that when there is a predominant model (stationary,
rotating or general motion), the corresponding probability μi reaches a value close
to 1, while the other model probabilities goes down close to zero — the IMM acts
as a discrete selector here rather than a mixer. Only when there is a change between
motion models are there periods with no clear dominant model and this is where the
IMM proves its worth.
It has to be noted that models with lower acceleration noise are preferred un-
less the camera really undergoes a high acceleration motion. In fact the model with

110
5 Degenerate Camera Motions and Model Selection
the highest acceleration has negligible probability indicating that it is not neces-
sary for processing the current sequence. Although this unused model does require
a computational overhead, its presence does not affect the accuracy of the solution
nor jeopardize the matching by the size of the search regions for the predicted fea-
tures — since its weight is always close to zero it is simply weighted out of all the
calculations.
5.5.4
Computational Cost Considerations
Although the main advantage of the algorithm is its good tracking performance,
clearly outperforming standard single model SLAM on complex sequences, it is
also remarkable that the computational cost does not grow excessively. The cost of
the IMM algorithm is essentially linear with the number of models since all ﬁltering
operations must be duplicated for each model. This is offset somewhat, as shown
in section 5.5.2, by the fact that the search region ellipses are reduced in size in the
IMM formulation and this makes the image processing work of feature matching
cheaper.
5.6
Discussion
We have shown experimentally the advantages of the IMM ﬁlter when applied to
EKF-based Structure from Motion. We are able to track sequences containing pe-
riods with no movement, and pure rotation and general motion at various dynamic
levels, the system adapting automatically. In particular, while single model monoc-
ular SLAM is weak when bootstrapped with low parallax motions (still or rotating
camera), the IMM formulation copes admirably by recognising the motion type.
The IMM formulation requires a computational overhead, but has extra beneﬁts
in producing smaller acceptance regions for the predicted measurements, improving
outlier rejection, and being able to act as an automatic segmentation and labelling
tool by identifying motion boundaries.

Chapter 6
Self-calibration
Abstract. Computer vision researchers have proved the feasibility of camera self-
calibration –the estimation of a camera’s internal parametersfroman imagesequence
without any known scene structure. Nevertheless, all of the recent sequential ap-
proaches to 3D structure and motion estimation from image sequences which have
arisen in robotics and aim at real-time operation (often classed as visual SLAM or vi-
sual odometry) have relied on pre-calibrated cameras and have not attempted online
calibration. In this chapter, we present a sequential ﬁltering algorithm for simultane-
ous estimation of 3D scene estimation, camera trajectory and full camera calibration
from a sequence of ﬁxed but unknown calibration. This calibration comprises the
standard projective parameters of focal length and principal point along with two
radial distortion coefﬁcients.
6.1
Introduction
Camera self-calibration (or auto-calibration) is the process of estimating the inter-
nal parameters of a camera from a set of arbitrary images of a general scene. Self-
calibration has several practical advantages over calibration with a special target.
First, it avoids the onerous task of taking pictures of the calibration object; a task
that may be difﬁcult or even impossible if the camera is attached to a robot. Sec-
ond, internal parameters of a camera may change either unintentionally (e.g. due to
vibrations, thermical or mechanical shocks) or even intentionally in the case of a
zooming camera. 3D estimation in this latter case could only be performed via self-
calibration. Finally, inaccurate calibration (coming either from a poor calibration
process or from changed calibration parameters) produces the undesirable effect of
introducing bias in the estimation.
J. Civera et al.: Structure from Motion Using the Extended Kalman Filter, STAR 75, pp. 111–122.
springerlink.com
c⃝Springer-Verlag Berlin Heidelberg 2012

112
6 Self-calibration
Although computer vision researchers have demonstrated the feasibility of self-
calibration and despite all the advantages mentioned before, all of the recent sequen-
tial approaches to visual localisation and mapping –Visual Odometry and Visual
SLAM– rely on a pre-calibrated camera. In this chapter, it is proposed a sequential
SLAM-based algorithm that is able to sequentially estimate the structure of a scene,
the trajectory of a camera and also its full calibration — including two coefﬁcients
of radial distortion. The only assumption made about the ﬁxed camera calibration
is that the skew is zero and the pixel aspect ratio is 1, a reasonable assumption in
today’s digital cameras.
Apart from all the advantages mentioned, self-calibration can also be considered
essential for the development of practical systems. Vision systems in vacuum clean-
ers, autonomous vehicles or mobile phones cannot rely on end users to perform an
accurate camera calibration for the system to work. Instead, it would be desirable
that the visual estimation worked as soon as you install the software in your mobile
phone or the engine of your car is started. For example, [Grasa et al. 2011], uses the
real-time EKF SfM system described in this book to enhance visual perception in
laparoscopic surgery. Calibrating the endoscope before every operation by medical
staff would be impractical, being desirable that the system selfcalibrates in order to
produce minimum interference and disturbances to the medical team.
The rest of the chapter is organised as follows: section 6.2 surveys prior work
related to the approach presented here. Section 6.3 introduces the Sum of Gaussians
(SOG) ﬁlter. In section 6.4 we detail our self-calibration algorithm using SOG. Sec-
tion 6.5 presents real-image experiments that validate our approach. The conclu-
sions and discussions about this chapter can be found in section 6.6.
6.2
Related Work
Traditionally, photogrammetric bundle adjustment has included camera calibration
parameters — projective camera parameters and also distortion parameters — in
order to reﬁne a tight initial calibration guess and hence improve reconstruction
accuracy.
Self-calibration allows the computation from scratch of projective calibration pa-
rameters: focal length, principal point, and skew; the computed calibration is readily
usable or might be used as an initial guess for bundle adjustment reﬁnement, and the
reﬁnement might include estimation of distortion parameters. The standard off-line
self-calibration process is summarized as follows: ﬁrst, matches along an uncali-
brated sequence with possibly varying parameters are determined. Note that here,
no assumptions about camera calibration — except that non-projective distortions
are negligible — are applied. Then a projective reconstruction is computed; a poten-
tially warped version of the ideal Euclidean reconstruction. If no more information
about the camera taking the images is available then projective reconstruction is
the best result that can be computed. However if some knowledge about calibration
parameters is available — that they are constant, that there is zero skew, a known

6.3 Sum of Gaussians (SOG) Filter
113
principal point or known aspect ratio — then this can be exploited to compute
the rest of the unknown calibration parameters. Maybank and Faugeras demon-
strated auto-calibration for the ﬁrst time in 1992 [Maybank & Faugeras 1992,
Faugeras et al. 1992]. Since then different methods for upgrading projective recon-
struction to metric using partial knowledge about the camera calibration have been
developed. A summary of all theses results is found in [Hartley & Zisserman 2004].
In
spite
of the vast
amount of work
related
to
autocalibration, ap-
proaches to these problem under a sequential Bayesian estimation framework
are surprisingly few, and none of them performs a complete calibration. In
[Azarbayejani & Pentland 1995] the authors propose for the ﬁrst time the use of
an EKF for sequential Bayesian estimation of unknown focal length. This is rele-
vant seminal work but the 3D point parametrization is basic and this makes it dif-
ﬁcult to deal with occlusion and feature addition and deletion. The approach of
[Qian & Chellappa 2004] estimates a varying focal length assuming that the rest
of the calibration parameters are known, and using a particle ﬁlter to deal with
non-linearities.
In the context of visual SLAM, self-calibration of the internal parameters of a
camera has not been previously discussed. [Sol`a et al. 2008] may be the closest re-
lated work, where a self-calibration algorithm for extrinsic camera parameters in
large baseline stereo SLAM is proposed –relative locations of cameras in a stereo
rig are easily decalibrated as baseline is increased. External self-calibration has also
been tackled for multisensor systems, like [Scaramuzza et al. 2007] for a 3D laser
and a camera. In this latest work, no artiﬁcial landmarks are needed but correspon-
dences have to be manually identiﬁed. [Bryson et al. 2009] describes a multisensor
system composed of GPS, IMU and monocular camera that autocalibrates IMU bi-
ases, IMU-camera relative angles and reﬁnes an initial guess of the intrinsic camera
calibration. A general framework for self-calibration of non-visual sensor internal
parameters –biases, gains, etc.– has also been addressed, e.g. in [Foxlin 2002].
Regarding the estimation techniques used in this work, the nonlinearity of the
self-calibration problem has forced us to abandon the Extended Kalman Filter and
adopt an approach more suitable for nonlinear systems: the Sum of Gaussians (SOG)
ﬁlter [Alspach & Sorenson 1972]. This type of ﬁlter has already been used in SLAM
[Pini´es et al. 2006, Durrant-Whyte et al. 2003]. [Kwok et al. 2005] is of particular
interest, as the combination of the Sum of Gaussians ﬁlter plus Sequential Proba-
bility Ratio Test they use to deal with the point initialization problem in monocular
SLAM is the same it is used in this chapter for self-calibration purposes.
6.3
Sum of Gaussians (SOG) Filter
Within the SOG approach [Alspach & Sorenson 1972], the probability density func-
tion of the estimated parameters p(x) is approximated by a weighted sum of multi-
variate Gaussians:

114
6 Self-calibration
p(x) =
ng
∑
i=1
α(i)N

ˆx(i),P(i)
,
(6.1)
where ng stands for the number of Gaussians, ˆx(i) and P(i) are the mean and co-
variance matrix for each Gaussian and α(i) represents the weighting factors, which
should obey ∑
ng
i=1 α(i) = 1 and α(i) ≥0.
This Sum of Gaussians probability density function evolves as follows: at every
step, when new measurements arrive, each one of the Gaussians is updated with the
standard prediction-update Extended Kalman Filter equations. The central part of
the SOG algorithm is, then, a bank of EKF ﬁlters running in parallel. This bank of
EKF ﬁlters is illustrated in Figure 6.1.
Fig. 6.1. Scheme of the Sum of Gaussians (SOG) ﬁlter.
Weighting factors α(i) are also updated at every step k using this formula:
α(i)
k
=
α(i)
k−1N

ν(i)
k ,S(i)
k

∑
ng
j=1α( j)
k−1N

ν(j)
k ,S( j)
k
 ;
(6.2)
where ν(i)
k
and S(i)
k are the EKF innovation vector and its covariance matrix respec-
tively. The innovation vector for each EKF is computed as the difference between
the
actual
measurements
zk
and
the
predicted
measurements
h(i)
k .
The

6.4 Self-calibration Using SOG Filtering
115
predicted measurements h(i)
k result from applying the measurement model equations
to the state vector ˆx(i)
k
ν(i)
k
= zk −h(i)
k ,
h(i)
k = h

ˆx(i)
k

.
(6.3)
The innovation covariance is obtained propagating each EKF covariance P(i)
k
through the measurement equations and adding the covariance of the zero-mean
image noise R(i)
k
S(i)
k = H(i)
k P(i)
k H(i)
k
⊤
+R(i)
k ,
H(i)
k = ∂h
∂x

ˆx(i)
k
.
(6.4)
Finally, an overall mean and covariance for the whole ﬁlter can be computed as
follows:
ˆxk =
ng
∑
i=1
α(i)
k ˆx(i)
k
Pk =
ng
∑
i=1
α(i)
k

P(i)
k +

ˆx(i)
k −ˆxk

ˆx(i)
k −ˆxk
⊤
.
(6.5)
These latest overal mean and covariance are used for visualization purposes in
our experiments. Nevertheless, notice (graphically in Figure 6.1) that this is the
only purpose of the overall mean and covariance as they are not involved either in
the ﬁlter bank or in the evolution of the weighting factors.
From this brief introduction, the two fundamental advantages of the SOG ﬁlter
over the EKF can be intuitively introduced. First, notice that any probability density
function can be reasonably approximated by a weighted Sum of Gaussians if we
make the number of Gaussians ng high enough. So, the usual EKF assumption of
Gaussian PDF does not need to hold for the SOG ﬁlter. Second, and more impor-
tantly for this work, as we increase the number of Gaussians the uncertainty P(i) for
each Gaussian becomes smaller, favoring linearity.
6.4
Self-calibration Using SOG Filtering
6.4.1
State Vector Deﬁnition
In order to estimate 3D scene structure and camera location and calibration the SOG
state vector x, –and hence every EKF state vector x(i) that composes the ﬁlter bank–
will contain a set of camera parameters xC and a set of parameters xM representing
each estimated point y j.

116
6 Self-calibration
x =

x⊤
C ,x⊤
M
⊤
,
xM =

y⊤
1 ,...,y⊤
n
⊤
(6.6)
Mapped points y j are ﬁrst coded in inverse depth coordinates and converted into
Cartesian (XYZ) coordinates if and when their measurement equation becomes lin-
ear, as detailed in chapter 3.
yρ = (Xc,Yc,Zc,θ,φ,ρ)⊤,
yXYZ = (X,Y,Z)⊤
(6.7)
The camera part of the state vector xC, as the key difference from previous work,
now includes the internal calibration parameters to estimate: the focal length f, the
principal point coordinatesCx and Cy and the parameters modelling radial distortion
κ1 and κ2.
xC =

x⊤
K,x⊤
v
⊤
;
xK = (f,Cx,Cy,κ1,κ2)⊤,
xv =
⎛
⎜
⎜
⎝
rWC
qWC
vW
ωC
⎞
⎟
⎟
⎠.
(6.8)
The camera motion model is described in section 3.2.1; and the projection model
is the one in section 3.3. Nevertheless, it is important to remark that calibration
parameters in this model that were assumed to be known up to this chapter are now
taken from the state vector.
6.4.2
Pruning of Gaussians with Low Weight
As it can be assumed that the ﬁnal estimation result will be unimodal, Gaussians that
repeteadly obtain low weighting factors can be pruned reducing the computational
cost. To do this, it is adopted the proposal in [Kwok et al. 2005], which makes use of
the Sequential Probability Ratio Test (SPRT) [Wald 1945]. Experiments have shown
that SPRT achieves a high reduction rate while maintaining similar performance.
For each Gaussian i in the SOG ﬁlter, the null hypothesis H0 is that such Gaussian
correctly represents the true state and the alternative hypothesis H1 that the Gaussian
does not represent the true state. At every step k, the null hypothesis is accepted if
k
∏
t=1
L (i)
t
(H0)
L (i)
t
(H1)
> A ,
(6.9)
and the alternative hypothesis is accepted (meaning that Gaussian i can be pruned)
if
k
∏
t=1
L (i)
t
(H0)
L (i)
t
(H1)
< B ,
(6.10)

6.5 Experimental Results
117
where L (i)
t
(H0) and L (i)
t
(H1) are the likelihoods of the data under hypothesis H0
and H1 at frame t. These likelihoods are computed as follows:
L (i)
t
(H0) = N

ν(i)
t ,S(i)
t

(6.11)
L (i)
t
(H1) =
ng
∑
j=1; j̸=i
α( j)′N

ν( j)
t
,S(j)
t

(6.12)
α(j)′ =
α(j)
∑
ng
k=1;k̸=i α(k) .
(6.13)
Thresholds A and B are approximated by the so-called Wald Boundaries
[Wald 1945] A = 1−αb
αa
and B =
αb
1−αa , where αa and αb are the probabilities of
type I and type II errors.
6.5
Experimental Results
Two experiments have been carried to test the performance of the algorithm. The
design of the SOG ﬁlter, which is the same for both experiments, it is explained
here previous to the experimental results.
An interval for the focal length f from around 100 pixels to around 600 pixels
is considered to be the usual range for cameras used in robotics. It has been experi-
mentally found that the projection measurement equation from section 3.3 is fairly
linear in intervals of 30 pixels. So, in order to estimate the focal length, the full
range of possible focal length values is divided into 18 Gaussians with standard de-
viations of 7.5 pixels and separation between means of 30 pixels. Figure 6.2 shows
the resulting probability distribution function.
0
100
200
300
400
500
600
700
0
0.5
1
1.5
2
2.5
3
x 10
−3
f [pixels]
p(f)
Fig. 6.2. Probability density function considered for the focal length.
A similar procedure applies for κ1 and κ2. It is considered that usual values for
these parameters go from 0 (no radial distortion) to 0.08mm−2 and 0.018mm−4

118
6 Self-calibration
respectively. The projection model is approximately linear if these two variation
ranges are divided into 2 intervals for κ1 and 3 for κ2. The resulting probability
density functions for radial distortion parameters can be seen in Figure 6.3.
0
0.02
0.04
0.06
0.08
0.1
0
5
10
15
20
κ1 [mm−2]
0
0.005
0.01
0.015
0.02
0
20
40
60
80
100
κ2 [mm−4]
p(κ1)
p(κ2)
Fig. 6.3. Probability density function for distortion parameters κ1 and κ2.
The ﬁnal SOG ﬁlter will be composed of all possible combinations of the above
divisions, that is 18×2 ×3 = 108 ﬁlters. It is worth mentioning here that the naive
approach of considering all combinations of parameters is used here only to demon-
strate the performance of the algorithm in the most general case. In a practical set-
ting, it is known that a camera with large focal length will have negligible radial
distortion and that small focal lengths corresponds to high radial distortion lenses.
Hence, the number of the ﬁlters could be greatly reduced by observing the most
usual combinations of calibration parameters.
Finally, regarding the optical centre coordinates Cx and Cy; as the measurement
equation is linear for those parameters, they are coded with one single Gaussian.
The optical centre is assumed to be a maximum of 10 pixels from the centre of the
image. For a 320×240 image, this results in a bidimensional Gaussian whose mean
is [160,120] and whose standard deviations are 3.3 pixels in each coordinate.
6.5.1
Indoor Sequence
The ﬁrst sequence used to test the self-calibration algorithm is an indoor sequence
taken with a hand-held 320 ×240 IEEE1394 camera in a computer room. The pur-
pose of this experiment is to test the accuracy of the proposed algorithm, comparing
its results with an ofﬂine calibration.
Figure 6.4 shows three frames of the sequence, one at the beginning, the second
in the middle and the last frame of the sequence, and with the 3D estimation at each
instant. The evolution of the calibration parameters estimation over the sequence
can be observed in Figure 6.5. The same ﬁgure also shows the number of Gaus-
sians in the SOG ﬁlter at each step. Notice the steep decrease in the ﬁrst steps of
the estimation, and how after image 120 of the sequence the SOG ﬁlter is composed

6.5 Experimental Results
119
of only one ﬁlter, becoming an EKF. Table 6.1 details the initial and ﬁnal values
of the estimation with a 99% conﬁdence interval and the ofﬂine calibration values
for a better visualization of the accuracy of our self-calibration results. Notice that
although initial values cover a wide range of variation for the parameters, the SOG
ends up with a tight and consistent estimation for all of them.



	
	

			

	
Fig. 6.4. Images and top-down view 3D estimation for frames #20 (a), #80 (b) #260 (c), which
is the last frame of the sequence.
6.5.2
Loop-Closing Sequence
Loop-Closing is a standard benchmark in SLAM to test the validity of an estimation
algorithm: when a sensor revisits known areas, the estimation error should be small
enough for the algorithm to recognize previous mapped landmarks.

120
6 Self-calibration
Table 6.1. Calibration results for indoor sequence
Initial SOG
Interval
Final SOG
Estimation
Ofﬂine
Calibration
f [pixels]
[100,610]
193.0±1.9
194.1
Cx[pixels]
[150,170]
161.6±2.3
160.2
Cy[pixels]
[110,130]
127.0±2.4
128.9
κ1[mm−2]
[0,0.08]
0.0639±0.0032
0.0633
κ2[mm−4]
[0,0.018]
0.0139±0.0009
0.0139
50
100
150
200
250
100
150
200
250
300
50
100
150
200
250
150
155
160
165
170
50
100
150
200
250
120
125
130
135
140
50
100
150
200
250
0
0.05
0.1
50
100
150
200
250
0
0.01
0.02
0
50
100
150
200
250
0
25
50
75
100
f [pixels]
Cy [pixels]
κ2 [mm−4]
Cx [pixels]
κ1 [mm−2]
Number of
Gaussians
Fig. 6.5. Estimated calibration parameters over the computer room sequence. Thick blue line
is the estimated value, the horizontal black line is the ofﬂine calibration value and the red thin
lines represent the 99% uncertainty region.
A challenging indoor loop-closing sequence available as multimedia material in
[Civera et al. 2008a] –previously used to test inverse depth EKF monocular SLAM
with a calibrated camera in section 3.8.3– has been used in this experiment. The
estimated calibration values are accurate enough to close the loop. Figure 6.6 shows
three representative frames of the sequence and their estimated scene, including the
loop closing frame.
As we show in Table 6.2 and in Figure 6.7, the estimated calibration is close
to the ofﬂine calibration, but in a slightly over-conﬁdent manner. When compared
with the previous one, this experiment presents more difﬁcult linearization issues
because uncertainty increases when the camera explores new areas, and increases in
uncertainty imply more non-linear effects. Besides, the ﬁxed model for the calibra-
tion parameters implies a monotonic uncertainty reduction that becomes unrealistic
after processing several hundred of images.

6.5 Experimental Results
121



	


	
	
		
Fig. 6.6. (a) Image and 3D estimation at frame 60. (b) Image and 3D estimation at frame 330
of the sequence, when ﬁrst loop-closure feature (signaled in the image) is detected. (c) Image
and 3D estimation at frame 670, the last one of the sequence.
Table 6.2. Calibration results for the loop closing sequence.
Initial SOG
Interval
Final SOG
Estimation
Ofﬂine
Calibration
f [pixels]
[100,610]
195.0±0.4
196.9
Cx[pixels]
[150,170]
159.6±1.0
153.5
Cy[pixels]
[110,130]
133.9±1.0
130.8
κ1[mm−2]
[0,0.08]
0.0652±0.0019
0.0693
κ2[mm−4]
[0,0.018]
0.0132±0.0005
0.0109

122
6 Self-calibration
100
200
300
400
500
600
100
150
200
250
300
100
200
300
400
500
600
145
150
155
160
165
100
200
300
400
500
600
120
125
130
135
140
100
200
300
400
500
600
0
0.05
0.1
100
200
300
400
500
600
0
0.005
0.01
0.015
0.02
100
200
300
400
500
600
0
25
50
75
100
f [pixels]
Cx [pixels]
Cy [pixels]
κ1 [mm−2]
κ2 [mm−4]
Number of
Gaussians
Fig. 6.7. Estimated calibration parameters over the loop closing sequence. Thick blue line is
the estimated value, the horizontal black line is the ofﬂine calibration value and the red thin
lines represent the 99% uncertainty region.
6.6
Discussion
This chapter presents an algorithm that fully auto-calibrates a camera within a ﬁl-
tering framework, the only input being a sequence of images from a moving uncal-
ibrated camera. Due to non-linearities introduced by the estimation of calibration
parameters, a Sum of Gaussian ﬁlter is used to divide the whole non-linear range
of variation into small almost-linear pieces. The SOG approach uses several ﬁlters
in the ﬁrst steps of the estimation to cover all of these almost-linear hypothesis. A
pruning algorithm has been added that cuts Gaussians whose weighting factors are
low and reduces the SOG ﬁlter to a simple EKF in a few steps so complexity is
reduced after an initial computation overhead. As the multiple Gaussians have to be
kept only at initial stages when the map size is small, we expect the computational
complexity to be low enough to achieve real time performance.
Experimental results with real-images show that an accurate and consistent cam-
era calibration is achieved for a waggling motion in an indoor sequence. A loop
closure has been successfully performed, achieving calibration values close to of-
ﬂine calibration, what is a remarkable achievement, though the estimation is some-
what inconsistent due to non-linearities and to the unrealistic monotonic uncertainty
reduction that EKF produces when dealing with static parameters.
Regarding future lines of work, an interesting one would be to analyze how this
self-calibration algorithm behaves with respect to degenerate camera motion. Also,
being already demonstrated that sequential camera self-calibration is feasible for a
camera with ﬁxed unknown parameters, next natural step is to deal with varying
calibration parameters.

Chapter 7
Conclusions
Abstract. This chapter presents the main conclusions and summarizes the content of
the book. The algorithms, models and methods presented in the previous chapters
cover the main topics in sequential SfM or monocular SLAM: a projective point
model, an efﬁcient and robust search for correspondences and altorithms for model
selection and internal self-calibration. Together, the contributions presented in the
different chapters of the book form a robust system for sequential scene and camera
motion estimation potentially able to deal with any image sequence in real-time at
30 frames per second.
The sequential 3D camera motion and scene structure from a monocular video
stream has become a very relevant topic in diverse areas like robotics and aug-
mented reality (AR). In the robotic case, the estimation of a model of the envi-
ronment and the position of the robot into it allows to interact and manipulate the
scene without collision. From the wide array of available sensing modalities, the
monocular camera seems to hold a promising future in robotics due to its com-
pacity, low cost and accuracy and richness of information. The extraction of the
geometric information contained in a video sequence in an efﬁcient and sequen-
tial manner seems to be the current challenge in both areas. Apart from the al-
ready mentioned robotic and AR ﬁelds, the growing importance of video data in the
Internet [Yang & Toderici 2011] and the popularization of the smartphones could
open new and exciting application scenarios. Also, and now that the real-time 3D
scene and motion estimation from natural videos has become commonplace, e.g.
[Klein & Murray 2009, Sibley et al. 2009, Strasdat et al. 2011], the authors see as a
very promising line the sequential and real-time joint estimation of geometry and
high-level information (e.g., geometry and object recognition in [Civera et al. 2011,
Bao & Savarese 2011])
The authors believe that the pure monocular case is an excellent test bed for more
general visual sensing systems in this research scenario. It is clear that monocular
SLAM algorithms improve their performance if they are adapted to a more evolved
sensing modality –like stereo or visual plus inertial sensing. Nevertheless, the data
J. Civera et al.: Structure from Motion Using the Extended Kalman Filter, STAR 75, pp. 123–125.
springerlink.com
c⃝Springer-Verlag Berlin Heidelberg 2012

124
7 Conclusions
provided by these latest systems very frequently hides the real challenges of vi-
sual sensing; challenges that are easily spotted in the pure monocular case. Once a
solution is developed in this minimalistic sensing scenario, the transfer to a more
sophisticated system will improve its performance. For example, the inverse depth
coding for feature initialization presented in chapter 3 and developed for the monoc-
ular case has been successfully transferred to the stereo case [Paz et al. 2008a] and
for inertial plus monocular sensing [Pini´es et al. 2007]. Also, the monocular input
is the only choice in some cases like Internet videos or movies; and it is always the
cheapest one.
The contents of the present book have presented a consistent formulation for real-
time Structure from Motion from uncalibrated images using the Extended Kalman
Filter (EKF). The EKF has been one of the fundamental estimation techniques in
robotic SLAM since [Smith & Cheeseman 1986b] due to its sequential and prob-
abilistic processing of the sequence; issues that were considered essential for a
robotic application. Sensorial data from robots comes in the form of a stream and its
information needs to be inserted in the control loop, hence the sequential process-
ing. As a robot is intended to interact with a physical environment, the uncertainty
of the estimation should be taken into account in order to avoid collisions. Those
early robotic papers mostly used laser sensors. Early work on the ﬁltering of vi-
sual sequences can be found in [Azarbayejani & Pentland 1995]. Nevertheless, the
computer vision community was more focused on the formalization of the geometry
of separated views than the processing of video sequences. The so-called Structure
from Motion problem occupied the researchers for several decades, and it is sum-
marized in the excellent [Hartley & Zisserman 2004].
The chapters of this book unify both the perspectives of the robotics and com-
puter vision community on the 3D estimation from sequences. First, the tipycally
robotic estimation of the uncertainty holds a central role in the sequence processing,
allowing a boost in the efﬁciency and performance of the algorithms. On the other
hand, the most relevant topics in the Structure from Motion literature are addressed
in this book under a ﬁltering perspective.
More speciﬁcally, these relevant topics are the following: First, the need for a
projective camera model is borrowed from the computer vision community and
transferred to the SLAM problem. This allows a straightforward representation of
low-parallax points, both distant and newly initialized ones –solving the so-called
initialization problem. Apart from being adequate to represent low-parallax points,
the inverse depth parametrization proposed in chapter 3 holds a high degree of linear-
ity in its projection equation which leads to a nice EKF performance. A mosaicing
application, typical from the computer vision community, is shown to beneﬁt from
a SLAM-type processing and presents for the ﬁrst time real-time drift-free results
in chapter 2. This line of research is continuated in [Lovegrove & Davison 2010].
Chapter 4 presents an efﬁcient spurious rejection algorithm, where the typically
used in computer vision RANSAC beneﬁts from the uncertainty propagation of the
EKF showing large computational savings without performance degradation.
The combination of the projective inverse depth parametrization and an efﬁcient

7 Conclusions
125
spurious rejection result in a highly accurate estimation for large experiments where
the camera translates hundreds of metres.
The two latest chapters deal with degenerate motions and self-calibration; two
topics largely discussed in the computer vision community and that are introduced
here to the robotic community. Again, the uncertainty of the estimation keeps a
key role in both approaches. Degenerate motions are dealt by selecting the motion
model based on probabilities without the need for terms penalizing complex models.
Degenerate motions, e.g., pure rotation or a small translation compared with the
scene depth, are not uncommon in a robotic application and should be explicitly
addressed to avoid inconsistent estimations.
The self-calibration is considered by the authors as an advisable feature in most
SLAM systems in order to avoid an onerous pre-calibration with a target. Such pre-
calibration could be inaccurate when performed by an inexperienced user and even
be very inconvenient in certain conﬁgurations. A clear example can be the system
in [Grasa et al. 2011], that aims to perform SLAM from endoscopic images to help
surgeons. Such system should produce a minimum interference with the work of
the medical staff and the system should work with the highest degree of automony,
being self-calibration a must.
The application of the concepts in this book expands then the array of sequences
that may be processed by the monocular SLAM algorithms, increases the accuracy
and robustness of the estimation and has proven to run in real-time at 30 frames per
second in a standard laptop for small local maps. From a computer vision perspec-
tive, the research presented here follows the path initiated 30 years ago aiming to
an online motion and structure estimation from an image sequence and contributes
by linking the most relevant topics in wide baseline Structure from Motion with the
online estimation from short-interframe-motion image sequences. From the point of
view of the robotic research the approaches presented in this book form a practi-
cal system that could be said to perform out-of-the-box monocular SLAM, that is,
SLAM from a single camera with very few constraints of use. The discussions in
the book also have the value of providing a deeper understanding of the camera as
a sensor in the ﬁeld of robotics. Finally, and although further research in such an
active area as 3D vision is expected, the presented system already ﬁts the accuracy,
robustness and real-time requirements for a number of potential applications and
hence can be taken as a basic sensing piece for complex application-driven systems.


Appendix A
Implementation Details
A.1
Extended Kalman Filter
The Kalman Filter –and its version for non-linear systems, the Extended Kalman
Filter– are probably the best studied implementation of Bayesian ﬁltering and the
ﬁrst ﬁltering solution to be succesfully applied to the online SLAM estimation prob-
lem [Smith et al. 1987].
The Kalman ﬁlter recursively estimates a probability distribution function over
the unknown parameters of a state vector x from measurements gathered by a sen-
sor and the dynamical model of such state. The estimation process follows two
steps. In the ﬁrst one, the probability distribution function p(xk−1) from step k −
1 is updated to step k based on the probabilistic dynamic model of the system
p

xk|k−1|xk−1|k−1,uk

in the so-called prediction step
p

xk|k−1

=
!
p

xk|k−1|xk−1|k−1,uk

p(xk−1)dxk−1 .
(A.1)
After measurements zk are collected, they are fused with the probability distribu-
tion function from the prediction step using Bayes’ rule in the update step
p

xk|k

= η p

zk|xk|k−1

p

xk|k−1

,
(A.2)
where p

zk|xk|k−1

stands for the probabilistic measurement model of the system,
that has be known in advance; and η is the normalization constant that converts
p

xk|k

into a probability distribution function. The algorithm require the state prob-
ability distribution at the initial step p(x0) to be known.
The Kalman Filter, initially proposed in [Kalman 1960], makes three assump-
tions: linear dynamic and measurement model with Gaussianly distributed noise;
and Gaussian initial probability distribution at time k = 0. Under these assump-
tions, the a posteriori distribution over the estimated parameters is also a multivari-
ate Gaussian x ∼N (ˆx,P).

128
A Implementation Details
Most of the systems in the real world show a certain degree of non-linearity. The
Extended Kalman Filter (EKF) relaxes the linearity assumption by linearizing the
dynamic and measurement model in the mean value at every step of the estimation.
The Extended Kalman Filter does not give us then the real a posteriori probability
distribution function, but only a Gaussian approximation of it. The higher the degree
of linearity in the dynamic and measurement models and the more Gaussian the
noise, the better the EKF will perform.
The prediction step using the dynamic model of the system, described in its more
general form in equation A.1, makes the mean ˆx and covariance P of the Gaussian
probability in the EKF evolve in the following manner
ˆxk|k−1 = f
ˆxk|k−1,uk

,
(A.3)
Pk|k−1 = FkPk|k−1F⊤
k +GkQkG⊤
k ;
(A.4)
where f
ˆxk|k−1, ˆuk

is the nonlinear equation modeling the dynamic evolution of the
system, uk the input given to the system, Fk the derivatives of the dynamic model
by the state vector, Qk the input covariance and Gk the derivatives of the dynamic
model by the input parameters. I is an identity matrix of an appropriate size.
The update step via Bayes’ rule in equation A.2 is, in the EKF case
ˆxk|k = ˆxk|k−1 +Kk|k−1

zk −h
ˆxk|k−1

(A.5)
Pk|k =

I−KHk|k−1

Pk|k−1
(A.6)
Kk|k−1 = Pk|k−1H⊤
k|k−1

Hk|k−1Pk|k−1H⊤
k|k−1 +Rk
−1
;
(A.7)
where zk are measurements gathered at step k, h
ˆxk|k−1

the function that deﬁnes the
sensor measurement model, Hk|k−1 the derivatives of the measurement function by
the state vector and Rk the covariance of the measurement noise. The matrix Kk|k−1
is called the ﬁlter gain and weights the information from the prior knowledge and
the measurements according to their respective covariances.
For the practical implementation of an EKF ﬁlter, it is then required to know:
ﬁrst, wich parameters are included in the state vector and its probabilistic dynamic
model. Second, which sensor we are using, what are our measurements and the
probabilistic measurement model. Based on that, derivatives of both models can
be computed and the EKF steps described in this section can be coded. The next
sections detail the models and derivatives for the particular implementation of the
monocular EKF SLAM system used along this book.

A.2 Calibrated EKF-Based SfM
129
A.2
Calibrated EKF-Based SfM
A.2.1
Dynamic Model and Derivatives
The camera motion is considered smooth and modeled with a constant velocity
model with a zero-mean Gaussian acceleration noise n =

aW
αC
⊤
. As described
in previous chapters, the camera state vector is composed of its position rWC with
respect to a world reference frame W, the quaternion representing its orientation
qWC and the linear and angular velocities vW and ωC –this latest one in the camera
frame C.
xC =
⎛
⎜
⎜
⎝
rWC
qWC
vW
ωC
⎞
⎟
⎟
⎠
(A.8)
In this model, the camera states are modiﬁed at every step by impulses of linear
velocity VW = aWΔt and angular velocity ΩC = αCΔt produced by a Gaussian
acceleration noise:
xCk+1 =
⎛
⎜
⎜
⎝
rWC
k+1
qWC
k+1
vW
k+1
ωC
k+1
⎞
⎟
⎟
⎠= fv

xCk,n

=
⎛
⎜
⎜
⎝
rWC
k
+

vW
k +VW
k

Δt
qWC
k
× q

ωC
k +ΩC
Δt

vW
k + VW
ωC
k +ΩC
⎞
⎟
⎟
⎠.
(A.9)
q

ωC
k +ΩC
Δt

stands for the quaternion corresponding to the rotation given by

ωC
k + ΩC
Δt; and × for the quaternion product.
The derivatives of this dynamic model by the state F = ∂fv
∂xC and by the Gaussian
noise G = ∂fv
∂n are computed as follows:
∂fv
∂xC
=
⎛
⎜
⎜
⎜
⎝
I
0
ΔtI
0
0
∂qWC
k+1
∂qWC
k
0
∂qWC
k+1
∂ωC
k
0
0
I
0
0
0
0
I
⎞
⎟
⎟
⎟
⎠
(A.10)
∂fv
∂n =
⎛
⎜
⎜
⎜
⎝
ΔtI
0
0
∂qWC
k+1
∂ΩC
I
0
0
I
⎞
⎟
⎟
⎟
⎠
(A.11)

130
A Implementation Details
Let the quaternions in the previous formula be renamed as q3 = qWC
k+1, q2 = qWC
k
and q1 = q

ωC
k +ΩC
Δt

and the components of each quaternion represented as
q = (q0,q1,q2,q3)⊤. The partial derivative
∂qWC
k+1
∂qWC
k
is computed from the quaternion
product formula
∂qWC
k+1
∂qWC
k
= ∂q3
∂q2 =
⎛
⎜
⎜
⎝
q1
0 −q1
1 −q1
2 −q1
3
q1
1 q1
0
q1
3 −q1
2
q1
2 −q1
3 q1
0
q1
1
q1
3 q1
2 −q1
1 q1
0
⎞
⎟
⎟
⎠.
(A.12)
And the partial derivative
∂qWC
k+1
∂ωC
k can be computed via the chain rule as
∂qWC
k+1
∂ωC
k
=
∂qWC
k+1
∂q

ωC
k + ΩC
Δt
 ∂q

ωC
k +ΩC
Δt

∂ωC
k
;
(A.13)
where
∂qWC
k+1
∂q((ωC
k +ΩC)Δt) comes again from the quaternion product formula
∂qWC
k+1
∂q

ωC
k +ΩC
Δt
 = ∂q3
∂q1 =
⎛
⎜
⎜
⎝
q2
0 −q2
1 −q2
2 −q2
3
q2
1 q2
0 −q2
3 q2
2
q2
2 q2
3
q2
0 −q2
1
q2
3 −q2
2 q2
1
q2
0
⎞
⎟
⎟
⎠
(A.14)
and
∂q((ωC
k +ΩC)Δt)
∂ωC
k
can be computed from the conversion formula from a rotation
vector to a quaternion representation. This conversion formula is
q(ω) =

cos θ
2 , ω⊤
θ sin θ
2
⊤
(A.15)
θ = ||ω|| ;
(A.16)
and the derivatives can be separated by components
∂q

ωC
k + ΩC
Δt

∂ωC
k
=
⎛
⎜
⎜
⎜
⎜
⎜
⎝
∂q0(ωΔt)
∂ωx
∂q0(ωΔt)
∂ωy
∂q0(ωΔt)
∂ωz
∂q1(ωΔt)
∂ωx
∂q1(ωΔt)
∂ωy
∂q1(ωΔt)
∂ωz
∂q2(ωΔt)
∂ωx
∂q2(ωΔt)
∂ωy
∂q2(ωΔt)
∂ωz
∂q3(ωΔt)
∂ωx
∂q3(ωΔt)
∂ωy
∂q3(ωΔt)
∂ωz
⎞
⎟
⎟
⎟
⎟
⎟
⎠
,
(A.17)
where each component is computed as follows

A.2 Calibrated EKF-Based SfM
131
∂q0 (ωΔt)
∂ωi

i=1,...,3
= −Δt
2
ωi
θ sin

θ Δt
2

(A.18)
∂qi (ωΔt)
∂ωi

i=1,...,3
= Δt
2
ωi
θ
2
cos

θ Δt
2

+ 1
θ

1−
ωi
θ
2
sin

θ Δt
2

(A.19)
∂qi (ωΔt)
∂ω j

i=1,...,3;i̸=j
= Δt
2
ωiωj
θ 2 cos

θ Δt
2

−1
θ sin

θ Δt
2

.
(A.20)
A.2.2
Measurement Model and Derivatives
We will summarize again here the measurement model for completitude before go-
ing into its derivatives. For a more elaborated description of this model the reader is
referred to section 3.2.
The measurement model can be divided in three steps. In the ﬁrst of them, fea-
tures referred to the world reference frame W are converted into Euclidean ones
referred to the camera reference frame C. In the case of an inverse depth feature
yρ = (x y z θ φ ρ)⊤
hC
ρ = RCW
⎛
⎝ρ
⎛
⎝
⎛
⎝
x
y
z
⎞
⎠−rWC
⎞
⎠+ m(θ,φ)
⎞
⎠;
(A.21)
and for a feature coded in Euclidean form yXYZ = (X Y Z)⊤
hC
XYZ = RCW
⎛
⎝
⎛
⎝
X
Y
Z
⎞
⎠−rWC
⎞
⎠.
(A.22)
A pinhole camera model is then applied
hu =

uu
vu

=
⎛
⎝Cx −f
dx
hCx
hCz
Cy −f
dy
hCy
hCz
⎞
⎠,
(A.23)
that gives us the 2D image coordinates hu assuming a pure projective model. In
order to cope with the distortions coming from real lenses, we add a radial distortion
model to the ideal undistorted coordinates. In this book we have used the standard
two parameter distortion model from photogrammetry [Mikhail et al. 2001], which
is described next.
The ideal projective undistorted coordinates hu = (uu,vu)⊤are recovered from
the real distorted ones hd = (ud,vd)⊤as follows,

132
A Implementation Details
hu =
Cx +(ud −Cx)

1 +κ1r2
d + κ2r4
d

Cy +(vd −Cy)

1+ κ1r2
d +κ2r4
d


rd =

(dx (ud −Cx))2 +(dy (vd −Cy))2
(A.24)
Where (Cx Cy)⊤are the principal point coordinates; and κ1 and κ2 the radial
distortion coefﬁcients.
To compute the distorted coordinates from the undistorted:
hd =
⎛
⎝
Cx +
(uu−Cx)
(1+κ1r2
d+κ2r4
d)
Cy +
(vu−Cy)
(1+κ1r2
d+κ2r4
d)
⎞
⎠
(A.25)
ru = rd

1 +κ1r2
d + κ2r4
d

(A.26)
ru =

(dx (uu −Cx))2 +(dy (vu −Cy))2
(A.27)
ru is readily computed computed from (A.27), but rd has to be numerically solved
from (A.26), e.g using Newton-Raphson, hence (A.25) can be used to compute the
distorted point.
The Jacobian of this measurement equation by the state vector is extracted from
the model deﬁned above. The full Jacobian H is divided into rows, each one corre-
sponding to a point measurement
H =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
∂h1
∂x...
∂hi
∂x...
∂hm
∂x
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
(A.28)
The derivative of each measurement ∂hi
∂x can be separated into derivatives by the
camera states ∂hi
∂xC and derivatives by the map features ∂hi
∂xM
∂hi
∂x =
 ∂hi
∂xC
∂hi
∂xM

.
(A.29)
As the camera state vector is composed by its location rWC, orientation qWC and
linear and angular velocities v and ω (equation A.8); the partial derivative ∂hi
∂xC can
also be further divided

A.2 Calibrated EKF-Based SfM
133
∂hi
∂xC
=
⎛
⎜
⎝∂hi
∂rWC
∂hi
∂qWC
7
0
∂hi
∂v 
7
0
∂hi
∂ω
⎞
⎟
⎠
(A.30)
As camera velocities are not involved in the measurement model, the derivatives
involving velocities ∂hi
∂v and ∂hi
∂ω cancel out to zero. The chain rule will be used to
compute the rest of the partial derivatives. Starting by
∂hi
∂rWC ,
∂hi
∂rWC = ∂hd
∂hu
∂hu
∂hC
∂hC
∂rWC
(A.31)
We have to compute ﬁrst the partial derivative ∂hd
∂hu from equation A.25. As this
equation is not in explicit form, we will calculate ﬁrst the Jacobian for the undistor-
tion ∂hu
∂hd from formula A.57:
∂hu
∂hd
=
⎛
⎜
⎜
⎝

1 +κ1r2
d + κ2r4
d

+
2((ud −Cx)dx)2 
κ1 +2κ2r2
d

2d2
y (ud −Cx)(vd −Cy)·
·

κ1 + 2κ2r2
d

2d2
x (vd −Cy)(ud −Cx)·
·

κ1 +2κ2r2
d


1 + κ1r2
d +κ2r4
d

+
2((vd −Cy)dy)2 
κ1 +2κ2r2
d

⎞
⎟
⎟
⎠
(A.32)
The Jacobian for the distortion is computed by inverting the previous matrix ∂hu
∂hd
(A.32):
∂hd
∂hu
=
∂hu
∂hd
−1
(A.33)
The derivatives for the pinhole camera model are easily extracted from A.23
∂hu
∂hC =
⎛
⎜
⎝
−f
dxhCz
0
hCx f
dxhCz
2
0
−f
dyhCz
hCy f
dyhCz
2
⎞
⎟
⎠
(A.34)
And ﬁnally, the partial derivative ∂hC
∂rWC is
∂hC
ρ
∂rWC = −ρRCW
(A.35)
∂hCXYZ
∂rWC = −RCW ;
(A.36)
for the cases of an inverse depth feature and a Euclidean XYZ one respectively.
The partial derivative of the measurement function hi by the quaternion rotation
qWC is again computed using the chain rule

134
A Implementation Details
∂hi
∂qWC = ∂hd
∂hu
∂hu
∂hC
∂hC
∂qWC
(A.37)
Partial derivatives ∂hd
∂hu and ∂hu
∂hC have already been detailed in formulas A.33 and
A.34 respectively. The only calculation left is
∂hC
∂qWC . Such computation can be done
dividing ﬁrst the jacobian into two pieces
∂hC
∂qWC = ∂hC
∂qCW
∂qCW
∂qWC ;
(A.38)
where
∂qCW
∂qWC =
⎛
⎜
⎜
⎝
1 0
0
0
0 −1 0
0
0 0 −1 0
0 0
0 −1
⎞
⎟
⎟
⎠.
(A.39)
Partial derivatives ∂hC
∂qCW can be computed dividing them as the derivative with re-
spect to each one of the components of the quaternionqCW=

qCW
0
qCW
1
qCW
2
qCW
3
⊤,
∂hC
∂qCW =

∂hC
∂qCW
0
∂hC
∂qCW
1
∂hC
∂qCW
2
∂hC
∂qCW
3

;
(A.40)
where the derivative with respect to the quaternion components only affects to the
rotation matrix
∂hC
∂qCW
0
= ∂RCW
∂qCW
0
⎛
⎝ρi
⎛
⎝
⎛
⎝
xi
yi
zi
⎞
⎠−rWC
⎞
⎠+m(θi,φi)
⎞
⎠
(A.41)
∂hC
∂qCW
1
= ∂RCW
∂qCW
1
⎛
⎝ρi
⎛
⎝
⎛
⎝
xi
yi
zi
⎞
⎠−rWC
⎞
⎠+m(θi,φi)
⎞
⎠
(A.42)
∂hC
∂qCW
2
= ∂RCW
∂qCW
2
⎛
⎝ρi
⎛
⎝
⎛
⎝
xi
yi
zi
⎞
⎠−rWC
⎞
⎠+m(θi,φi)
⎞
⎠
(A.43)
∂hC
∂qCW
3
= ∂RCW
∂qCW
3
⎛
⎝ρi
⎛
⎝
⎛
⎝
xi
yi
zi
⎞
⎠−rWC
⎞
⎠+m(θi,φi)
⎞
⎠.
(A.44)
From the conversion formula from quaternion to rotation matrix orientation rep-
resentation,

A.2 Calibrated EKF-Based SfM
135
R =
⎛
⎝
q2
0 + q2
1 −q2
2 −q2
3 2(q1q2 −q0q3)
2(q3q1 +q0q2)
2(q1q2 + q0q3) q2
0 −q2
1 +q2
2 −q2
3 2(q2q3 −q0q1)
2(q3q1 −q0q2)
2(q2q3 +q0q1) q2
0 −q2
1 −q2
2 + q2
3
⎞
⎠.
(A.45)
The derivatives of rotation matrix RCW with respect to each component of the
quaternion are easily computed
∂RCW
∂qCW
0
=
⎛
⎝
2qCW
0
−2qCW
3
2qCW
2
2qCW
3
2qCW
0
−2qCW
1
−2qCW
2
2qCW
1
2qCW
0
⎞
⎠
(A.46)
∂RCW
∂qCW
1
=
⎛
⎝
2qCW
1
2qCW
2
2qCW
3
2qCW
2
−2qCW
1
−2qCW
0
2qCW
3
2qCW
0
−2qCW
1
⎞
⎠
(A.47)
∂RCW
∂qCW
2
=
⎛
⎝
−2qCW
2
2qCW
1
2qCW
0
2qCW
1
2qCW
2
2qCW
3
−2qCW
0
2qCW
3
−2qCW
2
⎞
⎠
(A.48)
∂RCW
∂qCW
3
=
⎛
⎝
−2qCW
3
−2qCW
0
2qCW
1
2qCW
0
−2qCW
3
2qCW
2
2qCW
1
2qCW
2
2qCW
3
⎞
⎠
(A.49)
In the derivative of each measurement by the map ∂hi
∂xM appearing in equation
A.29 only the feature i is in the measurement equation, so
∂hi
∂xM
=

0...0∂hi
∂yi
0...0

.
(A.50)
The partial derivative ∂hi
∂yi is as follows
∂hi
∂yi
= ∂hd
∂hu
∂hu
∂hC
∂hC
∂yi
;
(A.51)
where ∂hd
∂hu and ∂hu
∂hC are detailed in A.33 and A.34. The partial derivative ∂hC
∂yi in the
above product is, for the inverse depth case
∂hC
ρ
∂yi
=
⎛
⎝ρRCW| RCW ∂m
∂θ | RCW ∂m
∂φ | RCW
⎛
⎝
⎛
⎝
xi
yi
zi
⎞
⎠−rWC
⎞
⎠
⎞
⎠.
(A.52)
Here m =

cosφ sinθ
−sinφ
cosφ cosθ
⊤is the unit vector deﬁned by the
azimuth-elevation pair (θ , φ), derivatives ∂m
∂θ and ∂m
∂φ come straightforwardly as

136
A Implementation Details
∂m
∂θ = (cosφcosθ 0 −cosφsinθ)⊤
(A.53)
∂m
∂φ = (−sinφsinθ −cosφ −sinφcosθ)⊤.
(A.54)
For a XYZ feature, this latest partial derivative is
∂hC
XYZ
∂yi
= RCW .
(A.55)
A.2.3
Inverse Depth Point Feature Initialization and Derivatives
The initialization function deﬁnes a new point feature yNEW from an image point h,
the current state vector x and and initial value for the inverse depth ρ0
yNEW = y(x,h,ρ0)
(A.56)
The initialization function follows the same three steps than the measurement
equation but in reverse order. First, the image point has to be undistorted
hu =
Cx +(ud −Cx)

1 +κ1r2
d + κ2r4
d

Cy +(vd −Cy)

1+ κ1r2
d +κ2r4
d


rd =

(dx (ud −Cx))2 +(dy (vd −Cy))2
(A.57)
The 3D ray –in the camera reference frame– where the point feature lies can be
extracted from the line joining the optical centre and the undistorted image coordi-
nates
hC =
⎛
⎜
⎝
−(uu−Cx)dx
f
−(vu−Cy)dy
f
1
⎞
⎟
⎠.
(A.58)
Using the current camera orientation estimation from the state vector, this ray can
be transformed to the world reference frame and the azimuth and elevation angles
extracted;
hW = RWC 
qWC
hC ,
(A.59)
θ = arctan

hW
x ,hW
z

,
(A.60)
φ = arctan

−hW
y ,

hWx
2 +hWz
2

.
(A.61)

A.2 Calibrated EKF-Based SfM
137
The position of the optical centre is directly extracted from the current camera
position,
⎛
⎝
xi
yi
zi
⎞
⎠= rWC .
(A.62)
The initial value for the inverse depth ρ0 is chosen to cover in its 95% acceptance
region a range of possible depths covering from a minimum close distance dmin to
inﬁnity. A typical value in our experiments is ρ0 = 1 and σρ0 = 1; covering a range
from dmin = 0.33 to inﬁnite (and beyond) with a probability of 0.95.
The newly initialized feature yNEW = (xi yi zi θ φ ρ0)⊤is added to the state
vector
xNEW =

xOLD
yNEW

,
(A.63)
And the state covariance is updated in the following manner
PNEW = J
⎛
⎝
POLD 0
0
0
R 0
0
0 σρ0
⎞
⎠J⊤;
(A.64)
being R the image noise covariance associated with our feature detector.
The matrix J is the Jacobian of this initialization function
J =
⎛
⎜
⎜
⎜
⎝
I
0
...
0
∂y
∂xC 0...0 ∂y
∂h
⎞
⎟
⎟
⎟
⎠.
(A.65)
As it happened in previous section, point un-projection does not depend either
on the camera velocities, so the partial derivatives by the camera parameters has
non-zero terms in camera position and orientation
∂y
∂xC
=
⎛
⎜
⎜
⎝
∂y
∂rWC
∂y
∂qWC 

0
∂y
∂v
7
0
∂y
∂ω
⎞
⎟
⎟
⎠.
(A.66)
The Jacobian by the camera position is
∂y
∂rWC =

I
0

.
(A.67)
The derivatives by the orientation quaternion have non-zero terms in the azimuth
and elevation angles’ positions:

138
A Implementation Details
∂y
∂qWC =
⎛
⎜
⎜
⎜
⎝
0
∂θ
∂qWC
∂φ
∂qWC
0
c
⎞
⎟
⎟
⎟
⎠;
(A.68)
where both derivatives
∂θ
∂qWC and
∂φ
∂qWC are as follows
∂θ
∂qWC = ∂θ
∂hW
∂hW
∂qWC
(A.69)
∂φ
∂qWC = ∂φ
∂hW
∂hW
∂qWC ;
(A.70)
∂θ
∂hW =

hW
z
hW
x
2 +hW
z
2
0
−
hW
x
hW
x
2 +hW
z
2

(A.71)
∂φ
∂hW =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
hW
x hW
y
(hW
x
2
+hW
y
2
+hW
z
2
)

hW
x
2
+hW
z
2
−

hW
x
2
+hW
z
2
hW
x
2
+hW
y
2
+hW
z
2
hW
z hW
y
(hW
x
2
+hW
y
2
+hW
z
2
)

hW
x
2
+hW
z
2
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
⊤
(A.72)
∂hW
∂qWC =
 ∂hW
∂qWC
0
∂hW
∂qWC
1
∂hW
∂qWC
2
∂hW
∂qWC
3

(A.73)
∂hW
∂qWC
i
= hC ∂RWC
∂qWC
i
.
(A.74)
The derivatives of the rotation matrix RWC by each quaternion component qWC
i
have been detailed in equations A.46 to A.49.
Finally, the derivatives by the salient image point h
∂y
∂h =

∂y′
∂h 0
0 1

,
(A.75)
where y′ = (xi yi zi θi φi) stands for the feature parameters computed from the
salient image point h. That is, all of them except the inverse depth one ρ0. The
derivative ∂y′
∂h is computed as
∂y′
∂h = ∂y′
∂hW
∂hW
∂hC
∂hC
∂hu
∂hu
∂hd
;
(A.76)

A.3 Uncalibrated EKF-Based SfM
139
where
∂y′
∂hW =

0 ∂θ
∂hW
∂φ
∂hW

(A.77)
with
∂θ
∂hW and
∂φ
∂hW being already computed in equations A.71 and A.72; and
∂hW
∂hC = RWC ;
(A.78)
∂hC
∂hu
=
 dx
f
0 0
0
dx
f 0

.
(A.79)
The Jacobian of the undistortion is detailed in equation A.32.
A.3
Uncalibrated EKF-Based SfM
A.3.1
Dynamic Model and Derivatives
In the uncalibrated case, the camera state vector xC is augmented with the projective
parameters –focal length f and principal point coordinates Cx and Cy– and the pa-
rameters modeling the lens distortion –in our case, κ1 and κ2 for the radial distortion
xC =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
f
Cx
Cy
κ1
κ2
rWC
qWC
vW
ωC
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
(A.80)
The motion model assumed for the camera is the same as detailed in section
A.2.1. The calibration parameters are assumed to remain constant in the experiments
of the book as we did not deal with zooming cameras. Nevertheless, the addition of
a dynamic model for the camera calibration parameters would be straightforward.
The dynamic model for the camera state vector is

140
A Implementation Details
xCk+1 =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
fk+1
Cxk+1
Cyk+1
κ1k+1
κ2k+1
rWC
k+1
qWC
k+1
vW
k+1
ωC
k+1
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
= fv

xCk,n

=
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
fk
Cxk
Cyk
κ1k
κ2k
rWC
k
+

vW
k + VW
k

Δt
qWC
k
×q

ωC
k +ΩC
Δt

vW
k +VW
ωC
k + ΩC
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
(A.81)
And the derivatives for the uncalibrated case can be easily extracted from the
ones in section A.2.1
∂fv
∂xv
=
⎛
⎜
⎜
⎜
⎜
⎜
⎝
I 0
0
0
0
0 I
0
ΔtI
0
0 0
∂qWC
k+1
∂qWC
k
0
∂qWC
k+1
∂ωC
k
0 0
0
I
0
0 0
0
0
I
⎞
⎟
⎟
⎟
⎟
⎟
⎠
(A.82)
∂fv
∂n =
⎛
⎜
⎜
⎜
⎜
⎜
⎝
0
0
ΔtI
0
0
∂qWC
k+1
∂ΩC
I
0
0
I
⎞
⎟
⎟
⎟
⎟
⎟
⎠
(A.83)
A.3.2
Measurement Model and Derivatives
The pinhole camera model plus a two parameter radial distortion model for the lens
is still used in the uncalibrated case, so equations A.21 to A.27 in section A.2.2
keep describing the measurement model. The key difference is that, in this case,
the calibration parameters are not considered constant but estimated in a joint state
vector. Then, the Jacobians are modiﬁed with respect to the calibrated case.

A.3 Uncalibrated EKF-Based SfM
141
The whole Jacobian H is as follows
H =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
∂h1
∂x...
∂hi
∂x...
∂hm
∂x
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
(A.84)
∂hi
∂x =
 ∂hi
∂xC
∂hi
∂xM

,
(A.85)
the derivative by the camera state ∂hi
∂xC is the one that changes with respect to the
calibrated case, as now we have to include the derivatives by the calibration param-
eters
∂hi
∂xC
=
⎛
⎜
⎝∂hi
∂f
∂hi
∂Cx
∂hi
∂Cy
∂hi
∂κ1
∂hi
∂κ2
∂hi
∂rWC
∂hi
∂qWC
7
0
∂hi
∂v 
7
0
∂hi
∂ω
⎞
⎟
⎠.
(A.86)
The computation of the aboves derivatives can be simpliﬁed by slightly modify-
ing the pinhole camera model equations in section A.2.2. The transformation from
the world frame to the camera frame in equations A.21 and A.22 is the same, but
from there we introduce the intermediate variables h∗
u and h∗
d, that represents the
image projection of the 3D point in a reference frame anchored in the principal
point
h∗
u =

u∗
u
v∗
u

=
⎛
⎝
−f
dx
hCx
hCz
−f
dy
hCy
hCz
⎞
⎠
(A.87)
h∗
u =
u∗
u
v∗
u

= h∗
d

1 +κ1r2
d + κ2r4
d

(A.88)
rd =


dxu∗
d
2 +

dyv∗
d
2 .
(A.89)
The image reference can be moved by adding the optical center coordinates to
the “starred” image coordinates
hu = h∗
u +
Cx
Cy

(A.90)
hd = h∗
d +

Cx
Cy

.
(A.91)

142
A Implementation Details
The derivatives with respect to the projective parameters ∂hi
∂f , ∂hi
∂Cx and ∂hi
∂Cy can be
straightforwardly extracted from equations A.87 to A.91:
∂hd
∂f = ∂hd
∂h∗
d
∂h∗
d
∂h∗
u
∂h∗
u
∂f
(A.92)
∂hd
∂h∗
d
= I
(A.93)
∂h∗
u
∂f =
⎛
⎝
−hCx
dxhCz
−hCy
dyhCz
⎞
⎠
(A.94)
∂hd
∂Cx
=

1
0

(A.95)
∂hd
∂Cy
=
 0
1

.
(A.96)
The derivatives by the radial distortion parameters ∂hd
∂κ1 and ∂hd
∂κ2 are a bit more
involved. Starting from equation A.88, the derivatives of the undistorted image co-
ordinates h∗
u can be easily computed:
∂u∗
u
∂κ1
= u∗
dr2
d
(A.97)
∂v∗
u
∂κ1
= v∗
dr2
d
(A.98)
∂u∗
u
∂κ2
= u∗
dr4
d
(A.99)
∂v∗
u
∂κ2
= v∗
dr4
d .
(A.100)
Doing implicit differentiation on equation A.88, we have the following
∂h∗
u
∂κ1
= ∂h∗
d
∂κ1

1+ κ1r2
d +κ2r4
d

+ h∗
d
∂

1 +κ1r2
d + κ2r4
d

∂κ1
.
(A.101)
Being rd =

u∗
d
2 +v∗
d
2, we can expand
∂(1+κ1r2
d+κ2r4
d)
∂κ1
as follows
∂

1 + κ1r2
d +κ2r4
d

∂κ1
= r2
d + 2κ1
∂u∗
d
κ1
+ ∂v∗
d
κ1

+ 4κ2
∂u∗
d
κ1
+ ∂v∗
d
κ1

(A.102)

A.3 Uncalibrated EKF-Based SfM
143
Doing the same with the derivative by κ2
∂h∗
u
∂κ2
= ∂h∗
d
∂κ2

1 +κ1r2
d + κ2r4
d

+h∗
d
∂

1 + κ1r2
d +κ2r4
d

∂κ2
(A.103)
∂

1 +κ1r2
d + κ2r4
d

∂κ2
= r4
d +2κ1
∂u∗
d
κ2
+ ∂v∗
d
κ2

+4κ2
∂u∗
d
κ2
+ ∂v∗
d
κ2

. (A.104)
From A.101 and A.103 we can extract 4 equations, where there are 4 partial
derivatives that are known ( ∂u∗u
∂κ1 , ∂v∗u
∂κ1 , ∂u∗u
∂κ2 and ∂v∗u
∂κ2 ); and 4 unknown partial deriva-
tives that are ∂u∗
d
∂κ1 , ∂v∗
d
∂κ1 , ∂u∗
d
∂κ2 and ∂v∗
d
∂κ2 . Solving for them we have ∂h∗
d
∂κ1 and ∂h∗
d
∂κ2 , that we
can use to compute the entire derivative of the image measurements with respect to
the radial distortion
∂hd
∂κ1
= ∂hd
∂h∗
d
∂h∗
d
∂κ1
(A.105)
∂hd
∂κ2
= ∂hd
∂h∗
d
∂h∗
d
∂κ2
.
(A.106)
A.3.3
Inverse Depth Point Feature Initialization and Derivatives
As it happened in the previous section, point initialization function is exactly the
same as in the calibrated case (equations A.56 to A.62), except for the introduction
of the intermediate variables h∗
u and h∗
d of the previous section. The image point to
be initialized hd is ﬁrst then converted to the intermediate variable h∗
d
h∗
d = hd −

Cx
Cy

.
(A.107)
After that, the undistortion model for the lens is applied
h∗
u =

u∗
u
v∗
u

= h∗
d

1 +κ1r2
d + κ2r4
d

(A.108)
rd =


dxu∗
d
2 +

dyv∗
d
2 ;
(A.109)

144
A Implementation Details
And ﬁnally the image point is backprojected into a 3D point in the camera refer-
ence frame at unit depth in the optical axis
hC =
⎛
⎜
⎝
−u∗udx
f
−v∗udy
f
1
⎞
⎟
⎠.
(A.110)
From here, the rest of the steps to obtain the newly initialized inverse depth fea-
ture yNEW are the same than those in section A.2.3 from equation A.59 to A.62.
The derivatives now accounts for the fact that calibration is included now in the
state vector. The Jacobian of the initialization function is in this case
J =
⎛
⎜
⎜
⎜
⎝
I
0
...
0
∂y
∂xC 0...0 ∂y
∂h
⎞
⎟
⎟
⎟
⎠,
(A.111)
where the derivatives by the camera
∂y
∂xC now include the internal calibration vari-
ables
∂y
∂xC
=
⎛
⎜
⎜
⎝
∂y
∂f
∂y
∂Cx
∂y
∂Cy
∂y
∂κ1
∂y
∂κ2
∂y
∂rWC
∂y
∂qWC 

0
∂y
∂v
7
0
∂y
∂ω
⎞
⎟
⎟
⎠.
(A.112)
From the six parameters deﬁning an inverse depth feature, only the azimuth-
elevation angles deﬁning the ray are the ones depending on the calibration in the
initialization step. The derivative by the calibration is then
∂y
∂f =

0 0 0 ∂θ
∂f
∂φ
∂f
0
⊤
(A.113)
∂y
∂Cx
=

0 0 0 ∂θ
∂Cx
∂φ
∂Cx
0
⊤
(A.114)
∂y
∂Cy
=

0 0 0 ∂θ
∂Cy
∂φ
∂Cy
0
⊤
(A.115)
∂y
∂κ1
=

0 0 0 ∂θ
∂κ1
∂φ
∂κ1
0
⊤
(A.116)
∂y
∂κ2
=

0 0 0 ∂θ
∂κ2
∂φ
∂κ2
0
⊤
.
(A.117)
The above derivatives of azimuth-elevation angle pair by calibration can be com-
puted via the chain rule. The derivatives by the focal length are as follows,

A.3 Uncalibrated EKF-Based SfM
145
∂θ
∂f = ∂θ
∂hW
∂hW
∂hC
∂hC
∂f
(A.118)
∂θ
∂hW =

hW
z
hWx
2 +hWz
2 0
−hW
x
hWx
2 +hWz
2

(A.119)
∂hW
∂hC = RWC
(A.120)
∂hC
∂f
=
⎛
⎜
⎜
⎝
−(uu−Cx)dx
f 2
−(vu−Cy)dy
f 2
0
⎞
⎟
⎟
⎠
(A.121)
∂φ
∂f = ∂φ
∂hW
∂hW
∂hC
∂hC
∂f
(A.122)
∂φ
∂hW =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
hWx hWy

hWx
2+hWy
2+hWz
2
hWx
2+hWz
2
−

hWx
2+hWz
2

hWx
2+hWy
2+hWz
2
hWy hWz

hWx
2+hWy
2+hWz
2
hWx
2+hWz
2
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
⊤
.
(A.123)
The derivatives by the radial distortion parameters are easily extracted
∂θ
∂κ1
= ∂θ
∂hW
∂hW
∂hC
∂hC
∂κ1
(A.124)
∂hC
∂κ1
=

−dx
f (ud −Cx)r2
d
−dy
f (vd −Cy)r2
d
0
⊤
(A.125)
∂φ
∂κ1
= ∂φ
∂hW
∂hW
∂hC
∂hC
∂κ1
(A.126)
∂θ
∂κ2
= ∂θ
∂hW
∂hW
∂hC
∂hC
∂κ2
(A.127)
∂hC
∂κ2
=

−dx
f (ud −Cx)r4
d
−dy
f (vd −Cy)r4
d
0
⊤
(A.128)
∂φ
∂κ2
= ∂φ
∂hW
∂hW
∂hC
∂hC
∂κ2
.
(A.129)

146
A Implementation Details
And ﬁnally, the derivatives by the principal point
h∗
u =

u∗
u
v∗
u

=

uu −Cx
vu −Cy

(A.130)
∂θ
∂Cx
= ∂θ
∂hW
∂hW
∂hC
∂hC
∂h∗
u
∂h∗
u
∂Cx
(A.131)
∂hC
∂h∗
u
=
⎛
⎜
⎝
−dx
f
0
0
−dy
f
0
0
⎞
⎟
⎠
(A.132)
∂h∗
u
∂Cx
=
 ∂u∗u
∂Cx
∂v∗u
∂Cx

(A.133)
∂u∗
u
∂Cx
= −

1 + κ1r2
d +κ2r4
d +(ud −Cx)2 d2
x

κ1 +2κ2r2
d

(A.134)
∂v∗
u
∂Cx
= −

(vd −Cy)(ud −Cx)d2
x

κ1 +2κ2r2
d

(A.135)
∂φ
∂Cx
= ∂φ
∂hW
∂hW
∂hC
∂hC
∂h∗
u
∂h∗
u
∂Cx
(A.136)
∂θ
∂Cy
= ∂θ
∂hW
∂hW
∂hC
∂hC
∂h∗
u
∂h∗
u
∂Cy
(A.137)
∂h∗
u
∂Cy
=
 ∂u∗u
∂Cy
∂v∗u
∂Cy

(A.138)
∂u∗
u
∂Cy
= −

(ud −Cx)(vd −Cy)d2
y

κ1 +2κ2r2
d

(A.139)
∂v∗
u
∂Cy
= −

1 + κ1r2
d +κ2r4
d +(vd −Cy)2 d2
y

κ1 +2κ2r2
d

(A.140)
A.4
Quaternion Normalization
Quaternion normalization must be performed after each update step of the Extended
Kalman Filter in order to ensure that its norm equals one, i.e. q2
0 +q2
1 +q2
2 +q2
3 = 1.
The state vector is then modiﬁed as follows
xnorm =
⎛
⎜
⎜
⎜
⎜
⎜
⎝
rWC
qWC
|qWC|
vW
ωC
xmap
⎞
⎟
⎟
⎟
⎟
⎟
⎠
,
(A.141)

A.5 Inverse Depth to Cartesian Parameterization Conversion
147
and the covariance should be updated with the Jacobian of the transformation
Pnorm = JnormPJ⊤
norm ,
(A.142)
Jnorm =
⎛
⎜
⎝
I
0
0
0 ∂qnorm
∂q
0
0
0
I
⎞
⎟
⎠,
(A.143)
∂qnorm
∂q
=

q2
0 + q2
1 + q2
2 +q2
3
−3
2 ×
(A.144)
×
⎛
⎜
⎜
⎝
q2
1 +q2
2 + q2
3
−q0q1
−q0q2
−q0q3
−q1q0
q2
0 + q2
2 + q2
3
−q1q2
−q1q3
−q2q0
−q2q1
q2
0 + q2
1 +q2
3
−q2q3
−q3q0
−q3q1
−q3q2
q2
0 +q2
1 +q2
2
⎞
⎟
⎟
⎠(A.145)
A.5
Inverse Depth to Cartesian Parameterization Conversion
As detailed in section 3.2.3, each inverse depth feature yρ,i represents a Euclidean
point (Xi,Yi,Zi)⊤that can be computed as follows
yXYZ,i =
⎛
⎝
Xi
Yi
Zi
⎞
⎠= gi

yρ,i

=
⎛
⎝
xi
yi
zi
⎞
⎠+ 1
ρi
mi (θi,φi) ,
(A.146)
being mi =
cosφi sinθi
−sinφi
cosφi cosθi
⊤.
In the algorithms described in the book, when an inverse depth feature yρ,i
holds the conversion criteria detailed in section 3.6, it can be safely converted
to a Cartesian –XYZ– feature yXYZ,i without degrading the EKF performance.
The state vector x =

xC,y⊤
1 ,...,y⊤
ρ,i,...,y⊤
n
⊤
is then transformed into xNEW =

xC,y⊤
1 ,...,y⊤
XYZ,i,...,y⊤
n
⊤
, where y⊤
XYZ,i comes from the conversion described
before in equation A.146.
The state vector covariance is transformed by using the Jacobian of the transfor-
mation Jgi, being the new covariance PNEW = JgiPJ⊤
gi. The relevant terms of the
Jacobian Jgi are in the position of feature i
Jgi =
⎛
⎜
⎝
I
0
0
0 ∂yXYZ,i
∂yρ,i
0
0
0
I
⎞
⎟
⎠.
(A.147)

148
A Implementation Details
The derivative ∂yXYZ,i
∂yρ,i
can be divided into the following parts
∂yXYZ,i
∂yρ,i
=
∂yXYZ,i
∂ri
∂yXYZ,i
∂θi
∂yXYZ,i
∂φi
∂yXYZ,i
∂ρi

;
(A.148)
where each of those is as follows
∂yXYZ,i
∂ri
= I
(A.149)
∂yXYZ,i
∂θi
= ∂yXYZ,i
∂m
∂m
∂θi
(A.150)
∂yXYZ,i
∂φi
= ∂yXYZ,i
∂m
∂m
∂φi
(A.151)
∂yXYZ,i
∂ρi
= −m
ρ2 .
(A.152)
Here ∂yXYZ,i
∂m
= 1
ρ I, and ∂m
∂θi and ∂m
∂φi were already detailed in A.54 and A.54.

Appendix B
Filter Tuning Understanding via Dimensional
Analysis
B.1
Introduction
It is a well known fact in SfM that a moving calibrated camera observing a scene
can recover scene geometry and camera motion only up to a scale factor — scene
scale is an non-observable magnitude if only bearing measurements are made. Un-
like SfM, previous SLAM work [Davison 2003, Montiel et al. 2006] have used extra
information in the form of a known initialisation object to ﬁx the scene scale.
In this appendix it is given insight on how this non-visual information is in fact
not essential for solving the tracking problem and that no known target object needs
to be added to the scene. While this means that an overall scene scale cannot be
recovered, real-time tracking can still proceed. And if at any time extra information
concerning the real scale does become available, it can be incorporated into the
estimation.
In particular, a novel understanding of the EKF-based SfM problem in
terms of dimensionless parameters is derived using Buckingham’s Π theorem
[Buckingham 1914]. Π theorem relies on the requirement for dimensional correct-
ness in any formula and hence also any estimation process. Our EKF SfM algorithm
therefore recovers dimensionless, up-to-scale geometry, and also provides beneﬁts
by allowing previous tuning parameters to be rolled up into a canonical set which
give an important new understanding of the uncertainties in the system now in pixel
units. These parameters in the image provide a natural way of understanding image
sequences, irrespectively of the frame rate, actual scene and camera motion.
Further, it is also shown that jointly with the main dimensionless part of the
SLAM state vector an extra parameter representing metric scale could be added.
During tracking, vision-only measurements would not reduce the uncertainty in the
scale parameter but only in the dimensionless scene geometry. However, any mea-
surement containing metric information such as odometry, a feature at a known
depth or the distance between two features can be added when available and will
correct both the scale and the dimensionless scene geometry.

150
B Filter Tuning Understanding via Dimensional Analysis
B.2
Monocular SLAM Estimation Process
As in the previous chapters of the book, the state vector is divided into the camera
parameters xC and the parameters corresponding to the n features in the map yi.
x =

x⊤
C y⊤
1
... y⊤
i
... y⊤
n
⊤
.
(B.1)
The camera state vector includes camera position, quaternion orientation, and
linear and angular velocities:
xC =

rWC⊤qWC⊤vW ⊤ωC⊤⊤
.
(B.2)
Points are coded in inverse depth parameterization; as proposed in chapter 3 (see
section 3.2.3 for details)
yi =

r⊤
i
θi φi ρi
⊤
= (xi yi zi θi φi ρi)⊤.
(B.3)
From here, we will split the state vector into a metric parameter d –unobservable
from image measurements– and a dimensionless state representing the scene and
camera parameters. Doing this, the state vector is partitioned according to observ-
ability with a monocular camera. Camera measurements will reduce the scene ge-
ometric uncertainty on the dimensionless parameters; but not the uncertainty in the
metric parameter d.
x =

d Π ⊤
r
qWC⊤
Π⊤
v
Π ⊤
ω
Π⊤
y1
...
Π⊤
yi
...
Π⊤
yn
⊤
.
(B.4)
Notice that the orientation parameters in qWC are already dimensionless, and
hence remains the same in this dimensionless formulation.
The mapping from this dimensionless state vector to a metric one involves a non-
linear computation using the dimensionless geometry and the metric parameter d:
r = d Πr
(B.5)
v = d Πv
(B.6)
ω = Πω
(B.7)
yi =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
d Πxi
d Πyi
d Πzi
θi
φi
Πρi/d
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
(B.8)

B.4 Dimensionless Monocular SLAM Model
151
B.3
Buckingham’s Π Theorem Applied to Monocular SLAM
Buckingham’s Π Theorem [Buckingham 1914] is a key theorem in Dimensional
Analysis. It basically states that the physical laws hold independently of the spe-
ciﬁc system of units that is chosen. Hence, given a dimensionally correct equation
involving n quantities of different kinds f(X1,X2,X3,...,Xn) = 0, the existing rela-
tionship between the variables can also be expressed as F(Π1,Π2,Π3,...,Πn−k) = 0
where Πi is a reduced set of n −k independent dimensionless groups of variables,
and k the number of independent dimensions appearing in the problem.
The monocular estimation process can be expressed as the following function:

r⊤q⊤v⊤ω⊤y⊤
1 ... y⊤
n
⊤
= f(σa,σα,z,σz,Δt,ρ0,σρ0,σv0,σω0) ,
(B.9)
where vector z stacks all the image measurements along the image sequence. Table
B.1 summarizes all the variables involved in monocular SLAM estimation and and
their units.
Table B.1. Variables involved in the monocular SLAM estimation and their dimensions
r,ri q v, σv0 ω, σω0 σa
σα θi, φi ρi, ρ0, σρ0 z,σz Δt
l
1 lt−1
t−1
lt−2 t−2
1
l−1
1
t
Based on table B.1, the dimensionless groups must be chosen. It can be seen
that the dimensions involved in the SLAM estimation are space and time, so two
variables containing these dimensions should be chosen to form the dimensionless
groups. We have chosen the parameters ρ0 and Δt; and the corresponding dimen-
sionless variables can be seen in (Table B.2).
Table B.2. Dimensionless numbers and the corresponding involved variables
Πr Πri Πq
Πv
Πσv0
Πω
Πσω0
Πσa
Πσα
Πρi Πσρ0 Πz Πσz
rρ0 riρ0 q vρ0Δt σv0ρ0Δt ωΔt σω0Δt σaρ0Δt2 σαΔt2
ρi
ρ0
σρ0
ρ0
z
σz
B.4
Dimensionless Monocular SLAM Model
Using the dimensionless magnitudes, our new monocular SLAM model can be de-
ﬁned as follows. The state vector is composed of the dimensionless parameters

152
B Filter Tuning Understanding via Dimensional Analysis
deﬁning the camera location, rotation and velocities; and the parameters related with
the map features:
ΠxC =

Πr⊤,q⊤,Πv⊤,Πω⊤⊤
Πyi =

Πri
⊤,θi,φi,Πρi
⊤
(B.10)
The dynamic model for the camera motion in this dimensionless formulation is
The dimensionless state update equation is:
fv =
⎛
⎜
⎜
⎝
Πrk+1
qk+1
Πvk+1
Πωk+1
⎞
⎟
⎟
⎠=
⎛
⎜
⎜
⎝
Πrk + Πvk + Πak
qk × q(Πωk +Παk)
Πvk +Πak
Πωk +Παk
⎞
⎟
⎟
⎠.
(B.11)
The insight about the geometrical meaning of the parameters in this equation will
be given in next sections. In the measurement model, the features coded in inverse
depth must be converted to 3D points in the world reference ﬁrst:
ΠW
h = Πri + 1
Πρi
m(θi,φi) ,
(B.12)
where m(θi,φi) is the unit vector deﬁned by the azimuth-elevation pair. The 3D
points in the world reference are then converted to the camera frame:
ΠC
h = RCW (q)(ΠW
h −Πr)
(B.13)
and projected into the camera using the pinhole model:
υ = ΠC
h |x
ΠC
h |z
ν = ΠC
h |y
ΠC
h |z
(B.14)
It is worth remarking that the measurement equation do not involve the actual
size of the scene, coded in the metric parameter d. Visual measurements from a
monocular camera would not reduce then the initial uncertainty in the size of the
scene. If the metric size had to be estimated, other measurements should be made.
For example, if a distance between two points is known, it can be used for reducing
the uncertainty over d:
D(P1,P2) = d

(Πy2|x −Πy1|x)2 +(Πy2|y −Πy1|y)2 +(Πy2|z −Πy1|z)2 .
(B.15)
B.5
Geometric Interpretation of the Dimensionless Parameters
Figure B.1 details the geometrical interpretation of the main dimensionless param-
eters that are involved in the estimation. The dimensionless parameters obtained in
sections B.3 and B.4 actually represent image quantities in pixel units, allowing an

B.5 Geometric Interpretation of the Dimensionless Parameters
153
understanding of the ﬁlter tuning parameters as image motion quantities instead of
parameters in the 3D world.
As the input to a monocular SLAM system is an image sequence, the motion
between two images can be easily extracted from the sequence but it may be hard
to guess in terms of 3D quantities. And in addition to that, it is the ratios of the
3D parameters the numbers that are important for the estimation. For example, the
image motion is the same for a certain scene and camera translation than for a scene
with double size and a camera that also translates double. The image motion also
would be the same if the frame rate is double but the camera moves twice faster.
Each one of these four conﬁgurations should use different ﬁlter tuning parameters
in a standard formulation of monocular SLAM; but they are nicely represented by
a unique tuning in our dimensionless formulation as they imply the same image
motion.
1/ ρ0
1/ ρ0
Δt
σa
2
Π a
Π r
z
Πα
σ z
r
(b)
(a)
(c)
(d)
Fig. B.1. Geometric interpretation of the dimensionless monocular SLAM parameters.
Figure B.1(a) illustrates the geometric meaning ot the dimensionless parameter
Πσa. The product σaΔt2 represents the effect of the acceleration noise on the camera
location. This value divided by 1/ρ0 gives the angle represented in the ﬁgure. This
angle can be seen as the parallax allowed to a feature at depth 1/ρ0 due to camera
acceleration.
The camera angular acceleration covariance in Figure B.1(b) can clearly be inter-
preted as an angle between frames and can be measured –as calibration is known–
in image pixels. Image measurements and image noise, in Figure B.1(c) are directly
measured in the image, so they are already equivalent to dimensionless angles.
The translation estimate, Πr (in ﬁg B.1(d)), can also be seen as the angle deﬁned
by the translation between frames and the initial inverse depth.
As a result of the analysis given in this section, and with minimum changes in the
structure of the EKF estimation scheme, all estimated parameters and ﬁlter tuning

154
B Filter Tuning Understanding via Dimensional Analysis
can be seen as dimensionless angles and consequently transformed to pixels in a
calibrated camera. We believe that this approach simpliﬁes the ﬁlter tuning: 3D
acceleration values, that cannot be directly extracted from the visualization of the
sequence to be processed are no longer required. Instead of that, quantities measured
in pixels in the image are the only numbers necessary for the tuning –easier to extract
from the sequence.
B.6
Real Image Results
We have performed several real-image experiments in order to show the main ad-
vantages of this new interpretation of the monocular SLAM problem under a dimen-
sionless optic. The ﬁrst one illustrates how the scale of the scene in usual monocular
SLAM depends on the prior knowledge added to scene, even if this knowledge is
very weak –in the case where no known target is added, scale depends on prior in-
verse depth values and acceleration noise. The second experiment shows the use of
image tuning and the reduction in the number of tuning parameters that comes as
consequence from the proposed scheme. In the third experiment, the same image
tuning is used in two different sequences which have different metric qualities but
lead to the same image motion. All of the sequences have been recorded with a IEEE
1394 320×240 monochrome camera at 30 fps.
B.6.1
Dependence of Scene Scale on a Priori Parameters
The same sequence was processed with the dimensional EKF SLAM algorithm
varying the ρ0 parameter. Figure B.2 shows the estimation for ρ0 = 0.5m−1 and
ρ0 = 0.1m−1. Notice that the estimated depth of the scene (the distance between the
camera and the points in the bookcase) tends to be at the depth prior (2m and 10m).
The two estimated scenes have the same form, the difference is just the scale of the
Fig. B.2. Left: sample. Centre: EKF SLAM estimation result ρ0 = 0.5m−1. Right: EKF
SLAM estimation result ρ0 = 0.1m−1. The uncertainty for the features is plotted in red and
blue, the camera uncertainty in cyan and the camera trajectory in yellow.

B.6 Real Image Results
155
axis. If ΠWC
r
= ρ0rWC and Πyi were estimated using the dimensionless monocular
SLAM proposed, these two experiments would be normalized into one, in which
normalized depth tends to be at dimensionless ’1’,
B.6.2
Image Tuning in a Pure Rotation Sequence
This sequence is a pure camera rotation in a hallway. Dimensional monocular SLAM
should have been tuned with real camera accelerations and depth priors. As these
values are not observable, they need to be guessed. In the dimensionless monocu-
lar SLAM of this chapter, the ﬁlter parameters are tuned in image units, which are
directly observable.
Fig. B.3. Pure rotation image search regions. Left: sample image. Centre: Πα = 2pixels.
Right:Πα = 4pixels.
Two experiments with the same Πσa = 0, Πσz = 1pixels values but different
tuning in Πσα: a) Πσα = 2pixels, and b) Πσα = 4pixels has been performed (ﬁgure
B.3). Because of the image tuning, their effect can be directly seen in the 95% image
search regions size for the map features.
It is important to notice again here that neither 3D scene assumptions nor time
between frames Δt have been neded in the previous paragraph to propose a ﬁlter
tuning. The tuned values are the allowed image motion between frames due to cam-
era linear and angular acceleration and image noise.

156
B Filter Tuning Understanding via Dimensional Analysis
B.6.3
The Same Image Tuning for Different Sequences
Two translational sequences have been recorded walking along a corridor and look-
ing at the wall. In the ﬁrst one, the distance from the wall was 2.5 metres. In the
second, the distance from the wall was twice (5 metres), the distance walked along
the corridor the same, and the walking velocity was double (therefore, the number
of frames of the second sequence is half the ﬁrst one). Although they are two differ-
ent experiments, the image motion in both sequences is the same, and dimensionless
monocular SLAM has to be tuned with same values. In this experiment, these values
were: σz = 1pxl, σa = 2pxl and σα = 2pxl. Notice again the simplicity of image
tuning compared with 3D tuning, in which you have to guess the unobservable depth
prior and the 3D accelerations. Figure B.4 shows the results of both estimations.
Fig. B.4. Two equivalent sequences. First and last images and 3D estimated geometry.
The dimensionless estimated translation can be interpreted as the translation in
units of the initial depth prior. As the wall is twice as far in the second sequence,
the second sequence’s estimated translation is half. It can also be noticed that, as the
normalized translation is smaller in the second experiment, the normalized 3D point
positions are estimated with less accuracy and have larger uncertainty regions.

B.7 Discussion
157
B.7
Discussion
As a result of an analysis of the SfM problem under Π Buckingham’s theorem, a
dimensionless parametrization for the EKF state vector can be proposed that clari-
ﬁes the role of the ﬁlter tuning parameters as image motion quantities. As a conse-
quence, ﬁlter tuning can be done using the allowed accelerations to the ﬁlter pro-
jected in the images –that is, in pixels– instead of using the allowed acceleration
itself in metric units –which is more difﬁcult to estimate having the sequence as the
only input to the algorithm.
Up-to-scale results from real-time, EKF based monocular SLAM without an ini-
tialisation target are presented. As no known points are included in the estimation,
the real size of the scene is not going to be recovered. A scaled estimation is going
to be obtained, being the overall scene scale being determined from a priori knowl-
edge inserted to the ﬁlter –even if it is so weak as inverse depth priors and linear
acceleration noise are.


References
[2d3 2011] 2d3 (June 2011), http://www.2d3.com/
[Aidala & Hammel 1983] Aidala, V.J., Hammel, S.E.: Utilization of Modiﬁed Polar Coor-
dinates for Bearing-Only Tracking. IEEE Transactions on Automatic Control 28(3),
283–294 (1983)
[Alspach & Sorenson 1972] Alspach, D., Sorenson, H.: Nonlinear Bayesian estimation us-
ing Gaussian sum approximations. IEEE Transactions on Automatic Control 17(4),
439–448 (1972)
[Autosticht 2011] Autosticht (June 2011), http://www.autostitch.net/
[Ayache & Faugeras 1989] Ayache, N., Faugeras, O.D.: Maintaining representations of the
environment of a mobile robot. IEEE Transactions on Robotics and Automation 5(6),
804–819 (1989)
[Azarbayejani & Pentland 1995] Azarbayejani, A., Pentland, A.P.: Recursive estimation of
motion, structure, and focal length. IEEE Transactions on Pattern Analysis and Machine
Intelligence 17(6), 562–575 (1995)
[Bailey & Durrant-Whyte 2006] Bailey, T., Durrant-Whyte, H.: Simultaneous localization
and mapping (SLAM): Part II. IEEE Robotics & Automation Magazine 13(3), 108–
117 (2006)
[Bailey et al. 2006] Bailey, T., Nieto, J., Guivant, J., Stevens, M., Nebot, E.: Consistency
of the EKF-SLAM algorithm. In: IEEE/RSJ International Conference on Intelligent
Robots and Systems, pp. 3562–3568 (2006)
[Bailey 2003] Bailey, T.: Constrained initialisation for bearing-only SLAM. In: Proceedings
of the IEEE International Conference on Robotics and Automation, vol. 2, pp. 1966–
1971 (2003)
[Baker et al. 2007] Baker, S., Scharstein, D., Lewis, J.P., Roth, S., Black, M.J., Szeliski, R.:
A database and evaluation methodology for optical ﬂow. International Journal of Com-
puter Vision 92(1), 1–31 (2007)
[Bao & Savarese 2011] Bao, S.Y., Savarese, S.: Semantic Structure from Motion. In: IEEE
Computer Vision and Pattern Recognition (2011)
[Bar-Shalom & Fortmann 1988] Bar-Shalom, Y., Fortmann, T.E.: Tracking and data associ-
ation. In: Mathematics in Science and Engineering, vol. 179. Academic Press, INC.,
San Diego (1988)
[Bar-Shalom et al. 2001] Bar-Shalom, Y., Li, X.R., Kirubarajan, T., Wiley, J.: Estimation
with applications to tracking and navigation. Wiley Online Library (2001)
[Bay et al. 2008] Bay, H., Ess, A., Tuytelaars, T., Van Gool, L.: SURF: Speeded Up Robust
Features. Computer Vision and Image Understanding 110(3), 346–359 (2008)

160
References
[Blanco et al. 2009] Blanco, J.-L., Moreno, F.-A., Gonz´alez, J.: A Collection of Outdoor
Robotic Datasets with centimeter-accuracy Ground Truth. Autonomous Robots 27(4),
327–351 (2009)
[Blom & Bar-Shalom 1988] Blom, H.A.P., Bar-Shalom, Y.: The interacting multiple model
algorithm for systems with Markovian switching coefﬁcients. IEEE Transactions on
Automatic Control 33(8), 780–783 (1988)
[Borenstein et al. 1996] Borenstein, J., Everett, H., Feng, L.: Where am I? Sensors and meth-
ods for mobile robot positioning, vol. 119, p. 120. University of Michigan (1996)
[Broida et al. 1990] Broida, T.J., Chandrashekhar, S., Chellappa, R.: Recursive 3-D motion
estimation from a monocular image sequence. IEEE Transactions on Aerospace and
Electronic Systems 26(4), 639–656 (1990)
[Brown et al. 2003] Brown, M.Z., Burschka, D., Hager, G.D.: Advances in computational
stereo. IEEE Transactions on Pattern Analysis and Machine Intelligence, 993–1008
(2003)
[Brown 2003] Brown, D.G., Lowe, M.: Recognising panoramas. In: International Confer-
ence on Computer Vision, Nice, pp. 1218–1225 (2003)
[Bryson & Sukkarieh 2005] Bryson, M., Sukkarieh, S.: Bearing-Only SLAM for an Air-
borne Vehicle. In: Australian Conference on Robotics and Automation (ACRA 2005),
Sidney (2005)
[Bryson et al. 2009] Bryson, M., Johnson-Roberson, M., Sukkarieh, S.: Airborne Smoothing
and Mapping using Vision and Inertial Sensors. In: Proceedings of the IEEE Interna-
tional Conference on Robotics and Automation, pp. 2037–2042 (2009)
[Buckingham 1914] Buckingham, E.: On Physically Similar Systems; Illustrations of the
Use of Dimensional Equations. Physical Review 4(4), 345–376 (1914)
[Canavos 1984] Canavos, G.C.: Applied probability and statistical methods. Little, Brown
and Company, Boston (1984)
[Canny 1986] Canny, J.: A computational approach to edge detection. IEEE Transactions on
Pattern Analysis and Machine Intelligence 8(6), 679–698 (1986)
[Capel & Zisserman 1998] Capel, D., Zisserman, A.: Automated Mosaicing with Super-
resolution Zoom. In: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 885–891 (1998)
[Capel 2005] Capel, D.: An effective bail-out test for RANSAC consensus scoring. In: Pro-
ceedings of the British Machine Vision Conference, pp. 629–638 (2005)
[Castellanos & Tard´os 1999] Castellanos, J.A., Tard´os, J.D.: Mobile robot localization and
map building: A multisensor fusion approach. Kluwer Academic Publishers, Boston
(1999)
[Castellanos et al. 1994] Castellanos, J.A., Montiel, J.M.M., Neira, J., Tard´os, J.D.: Sensor
Inﬂuence in the Performance of Simultaneous Mobile Robot Localization and Map
Building. LNCIS, vol. 250, pp. 287–296 (1994)
[Castellanos et al. 1999] Castellanos, J.A., Montiel, J.M.M., Neira, J., Tard´os, J.D.: The
SPmap: a probabilistic framework for simultaneous localizationand map building. IEEE
Transactions on Robotics and Automation 15(5), 948–952 (1999)
[Castellanos et al. 2001] Castellanos, J.A., Neira, J., Tard´os, J.D.: Multisensor fusion for si-
multaneous localization and map building. IEEE Transactions on Robotics and Automa-
tion 17(6), 908–914 (2001)
[Castellanos et al. 2004] Castellanos, J.A., Neira, J., Tard´os, J.D.: Limits to the consistency
of EKF-based SLAM. In: 5th IFAC Symposium on Intelligent Autonomous Vehicles
(2004)
[Castellanos et al. 2007] Castellanos, J.A., Martinez-Cantin, R., Tard´os, J.D., Neira, J.:
Robocentric map joining: Improving the consistency of EKF-SLAM. Robotics and Au-
tonomous Systems 55(1), 21–29 (2007)

References
161
[Castle et al. 2010] Castle, R.O., Klein, G., Murray, D.W.: Combining monoSLAM with ob-
ject recognition for scene augmentation using a wearable camera. Image and Vision
Computing 28(11), 1548–1556 (2010)
[Chekhlov et al. 2007] Chekhlov, D., Pupilli, M., Mayol, W.W., Calway, A.: Robust Real-
Time Visual SLAM Using Scale Prediction and Exemplar Based Feature Description.
In: Proceedings of the IEEE Computer Vision and Pattern Recognition (2007)
[Cheng et al. 2006] Cheng, Y., Maimone, M., Matthies, L.: Visual odometry on the Mars
exploration rovers-a tool to ensure accurate driving and science imaging. IEEE Robotics
and Automation Magazine 13(2), 54–62 (2006)
[Chiuso et al. 2002] Chiuso, A., Favaro, P., Jin, H., Soatto, S.: Structure from motion
causally integrated over time. IEEE Transactions on Pattern Analysis and Machine In-
telligence 24(4), 523–535 (2002)
[Chli & Davison 2008] Chli, M., Davison, A.J.: Active matching. In: Forsyth, D., Torr, P.,
Zisserman, A. (eds.) ECCV 2008, Part I. LNCS, vol. 5302, pp. 72–85. Springer, Hei-
delberg (2008)
[Chowdhury & Chellappa 2003] Chowdhury, A.K.R., Chellappa, R.: Stochastic Approxima-
tion and Rate-Distortion Analysis for Robust Structure and Motion Estimation. Interna-
tional Journal of Computer Vision 55(1), 27–53 (2003)
[Chum & Matas 2008] Chum, O., Matas, J.: Optimal randomized RANSAC. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence 30(8), 1472–1482 (2008)
[Civera et al. 2007a] Civera, J., Davison, A.J., Montiel, J.M.M.: Inverse Depth to Depth
Conversion for Monocular SLAM. In: IEEE International Conference on Robotics and
Automation, April 2007, pp. 2778–2783 (2007)
[Civera et al. 2007b] Civera, J., Davison, A.J., Montiel, J.M.M.: Dimensionless monocular
SLAM. In: Mart´ı, J., Bened´ı, J.M., Mendonc¸a, A.M., Serrat, J. (eds.) IbPRIA 2007.
LNCS, vol. 4478, pp. 412–419. Springer, Heidelberg (2007)
[Civera et al. 2008a] Civera, J., Davison, A.J., Montiel, J.M.M.: Inverse Depth Parametriza-
tion for Monocular SLAM. IEEE Transactions on Robotics 24(5), 932–945 (2008)
[Civera et al. 2008b] Civera, J., Davison, A.J., Montiel, J.M.M.: Interacting multiple model
monocular SLAM. In: IEEE International Conference on Robotics and Automation, pp.
3704–3709 (2008)
[Civera et al. 2009a] Civera, J., Bueno, D.R., Davison, A.J., Montiel, J.M.M.: Camera Self-
Calibration for Sequential Bayesian Structure From Motion. In: IEEE International
Conference on Robotics and Automation, ICRA 2009 (2009)
[Civera et al. 2009b] Civera, J., Davison, A.J., Magallon, J.A., Montiel, J.M.M.: Drift-Free
Real-Time Sequential Mosaicing. International Journal of Computer Vision 81(2), 128–
137 (2009)
[Civera et al. 2009c] Civera, J., Grasa, O.G., Davison, A.J., Montiel, J.M.M.: 1-Point
RANSAC for EKF-Based Structure from Motion. In: Proceedings of the IEEE/RSJ In-
ternational Conference on Intelligent Robots and Systems, pp. 3498–3504 (2009)
[Civera et al. 2010] Civera, J., Grasa, O.G., Davison, A.J., Montiel, J.M.M.: 1-Point
RANSAC for EKF Filtering: Application to Real-Time Structure from Motion and Vi-
sual Odometry. Journal of Field Robotics 27(5), 609–631 (2010)
[Civera et al. 2011] Civera, J., G´alvez-L´opez, D., Riazuelo, L., Tard´os, J.D., Montiel,
J.M.M.: Towards Semantic SLAM using a Monocular Camera. In: Proceedings of the
IEEE/RSJ International Conference on Intelligent Robots and Systems (2011)
[Clemente et al. 2007] Clemente, L.A., Davison, A.J., Reid, I.D., Neira, J., Tardos, J.D.:
Mapping Large Loops with a Single Hand-Held Camera. In: Proceedings of Robotics:
Science and Systems (2007)

162
References
[Comport et al. 2007] Comport, A.I., Malis, E., Rives, P.: Accurate Quadrifocal Tracking for
Robust 3D Visual Odometry. In: 2007 IEEE International Conference on Robotics and
Automation, April 2007, pp. 40–45 (2007)
[Davison & Murray 1998] Davison, A.J., Murray, D.W.: Mobile robot localisation using ac-
tive vision. In: Burkhardt, H., Neumann, B. (eds.) ECCV 1998. LNCS, vol. 1407, pp.
809–825. Springer, Heidelberg (1998)
[Davison et al. 2007] Davison, A.J., Molton, N.D., Reid, I.D., Stasse, O.: MonoSLAM:
Real-Time Single Camera SLAM. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 1052–1067 (June 2007)
[Davison 2003] Davison, A.J.: Real-time simultaneous localisation and mapping with a sin-
gle camera. In: Proceedings of the Ninth IEEE International Conference on Computer
Vision, pp. 1403–1410 (2003)
[de Agapito et al. 2001] de Agapito, L., Hayman, E., Reid, I.A.: Self-Calibration of Rotat-
ing and Zooming Cameras. International Journal of Computer Vision 45(2), 107–127
(2001)
[Dissanayake et al. 2001] Dissanayake, M., Newman, P., Clark, S., Durrant-Whyte, H.F.,
Csorba, M.: A solution to the simultaneous localization and map building (SLAM)
problem. IEEE Transactions on Robotics and Automation 17(3), 229–241 (2001)
[Durrant-Whyte & Bailey 2006] Durrant-Whyte, H., Bailey, T.: Simultaneous localisation
and mapping (SLAM): Part I the essential algorithms. Robotics and Automation Maga-
zine 13(2), 99–110 (2006)
[Durrant-Whyte et al. 2003] Durrant-Whyte, H., Majumder, S., Thrun, S., de Battista, M.,
Scheding, S.: A Bayesian Algorithm for Simultaneous Localisation and Map Building.
In: The 10th International Symposium on Robotics Research, pp. 49–60 (2003)
[Eade & Drummond 2006] Eade, E., Drummond, T.: Scalable Monocular SLAM. In: Pro-
ceedings of the IEEE Conference on Computer Vision and Pattern Recognition, New
York, pp. 469–476 (2006)
[Eade & Drummond 2007] Eade, E., Drummond, T.: Monocular slam as a graph of coa-
lesced observations. In: IEEE 11th International Conference on Computer Vision, ICCV
2007, pp. 1–8 (2007)
[Eustice et al. 2005] Eustice, R.M., Singh, H., Leonard, J.J., Walter, M., Ballard, R.: Visu-
ally Navigating the RMS Titanic with SLAM Information Filters. In: Proceedings of
Robotics: Science and Systems (2005)
[Everingham et al. 2010] Everingham, M., Van Gool, L., Williams, C.K.I., Winn, J., Zisser-
man, A.: The PASCAL visual object classes (VOC) challenge. International Journal of
Computer Vision 88(2), 303–338 (2010)
[Faugeras et al. 1992] Faugeras, O.D., Luong, Q.T., Maybank, S.J.: Camera self-calibration:
theory and experiments. In: Sandini, G. (ed.) ECCV 1992. LNCS, vol. 588, pp. 321–
334. Springer, Heidelberg (1992)
[Feder et al. 1999] Feder, H.J.S., Leonard, J.J., Smith, C.M.: Adaptive Mobile Robot Navi-
gation and Mapping. International Journal of Robotics Research 18(7), 650–668 (1999)
[Fenwick et al. 2002] Fenwick, J.W., Newman, P.M., Leonard, J.J.: Cooperative concurrent
mapping and localization. In: Proceedings of the 2002 IEEE International Conference
on Robotics and Automation, vol. 2, pp. 1810–1817 (2002)
[Fischler & Bolles 1981] Fischler, M.A., Bolles, R.C.: Random Sample Consensus, a
Paradigm for Model Fitting with Applications to Image Analysis and Automated Car-
tography. Communications of the ACM 24(6), 381–395 (1981)
[Fitzgibbon & Zisserman 1998] Fitzgibbon, A.W., Zisserman, A.: Automatic Camera Re-
covery for Closed or Open Image Sequences. In: Burkhardt, H.-J., Neumann, B. (eds.)
ECCV 1998. LNCS, vol. 1406, pp. 311–326. Springer, Heidelberg (1998)

References
163
[Foxlin 2002] Foxlin, E.: Generalized architecture for simultaneous localization, auto-
calibration and map-building. In: Proceedings of the IEEE/RSJ Conference on Intel-
ligent Robots and Systems, pp. 2–4 (2002)
[Frahm & Pollefeys 2006] Frahm, J.M., Pollefeys, M.: RANSAC for (quasi-) degenerate
data (QDEGSAC). In: 2006 IEEE Conference on Computer Vision and Pattern Recog-
nition, vol. 1, pp. 453–460 (2006)
[Funke & Pietzsch 2009] Funke, J., Pietzsch, T.: A Framework For Evaluating Visual
SLAM. In: Proceedings of the British Machine Vision Conference (2009)
[Gelb 1999] Gelb, A.: Applied optimal estimation. MIT Press (1999)
[Grasa et al. 2009a] Grasa, O.G., Civera, J., Guemes, A., Mun¨oz, V., Montiel, J.M.M.: EKF
Monocular SLAM 3D Modeling, Measuring and Augmented Reality from Endoscope
Image Sequences. In: 5th Workshop on Augmented Environments for Medical Imaging
including Augmented Reality in Computer-Aided Surgery, held in conjunction with
MICCAI 2009 (2009)
[Grasa et al. 2009b] Grasa, O.G., Civera, J., Guemes, A., Mun¨oz, V., Montiel, J.M.M.: Real-
time 3D Modeling from Endoscope Image Sequences. In: Proceedings of the 2009 IEEE
International Conference on Robotics and Automation; Workshop on Advanced Sensing
and Sensor Integration in Medical Robotics (2009)
[Grasa et al. 2011] Grasa, O.G., Civera, J., Montiel, J.M.M.: EKF monocular SLAM with
relocalization for laparoscopic sequences. In: Proceedings of the IEEE International
Conference on Robotics and Automation (2011)
[Handa et al. 2010] Handa, A., Chli, M., Strasdat, H., Davison, A.J.: Scalable Active Match-
ing. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni-
tion, pp. 1546–1553 (2010)
[Harris & Stephens 1988] Harris, C.G., Stephens, M.: A Combined Corner and Edge Detec-
tor. In: Proceedings of the 4th Alvey Vision Conference, pp. 147–151 (1988)
[Hartley & Zisserman 2004] Hartley, R.I., Zisserman, A.: Multiple view geometry in com-
puter vision. Cambridge University Press (2004), ISBN: 0521540518
[Heeger & Jepson 1992] Heeger, D.J., Jepson, A.D.: Subspace methods for recovering rigid
motion I: Algorithm and implementation. International Journal of Computer Vi-
sion 7(2), 95–117 (1992)
[Holmes et al. 2008] Holmes, S., Klein, G., Murray, D.: An O (N2) Square Root Unscented
Kalman Filter for Visual Simultaneous Localization and Mapping. IEEE Transactions
on Pattern Analysis and Machine Intelligence 22, 1251–1263 (2008)
[Jaynes 2003] Jaynes, E.T.: Probability theory: The logic of science. Cambridge University
Press (2003)
[Julier & Uhlmann 2001] Julier, S.J., Uhlmann, J.K.: A counter example to the theory of
simultaneous localization and map building. In: IEEE International Conference on
Robotics and Automation, vol. 4, pp. 4238–4243 (2001)
[Jung & Lacroix 2003] Jung, I.K., Lacroix, S.: High resolution terrain mapping using low
altitude aerial stereo imagery. In: Proceedings of the Ninth International Conference on
Computer Vision, p. 946 (2003)
[Kalman 1960] Kalman, R.E.: A new approach to linear ﬁltering and prediction problems.
Journal of basic Engineering 82(1), 35–45 (1960)
[Kanatani 2004] Kanatani, K.: Uncertainty modeling and model selection for geometric in-
ference. IEEE Transactions on Pattern Analysis and Machine Intelligence 26(10), 1307–
1319 (2004)
[Kim & Hong 2006] Kim, D.W., Hong, K.S.: Real-time mosaic using sequential graph. Jour-
nal of Electronic Imaging 15(2), 47–63 (2006)

164
References
[Kim & Sukkarieh 2003] Kim, J.H., Sukkarieh, S.: Airborne Simultaneous Localisation and
Map Building. In: Proceedings of the IEEE International Conference on Robotics and
Automation, pp. 406–411 (2003)
[Kirubarajan et al. 1998] Kirubarajan, T., Bar-Shalom, Y., Blair, W.D., Watson, G.A.:
IMMPDAF for Radar Management and Tracking Benchmark with ECM. IEEE Trans-
actions on Aerospace and Electronic Systems 34(4), 1115–1134 (1998)
[Klein & Murray 2007] Klein, G., Murray, D.: Parallel Tracking and Mapping for Small AR
Workspaces. In: Proceedings of the Sixth IEEE and ACM International Symposium on
Mixed and Augmented Reality, pp. 1–10 (2007)
[Klein & Murray 2008] Klein, G., Murray, D.: Improving the Agility of Keyframe-Based
SLAM. In: Forsyth, D., Torr, P., Zisserman, A. (eds.) ECCV 2008, Part II. LNCS,
vol. 5303, pp. 802–815. Springer, Heidelberg (2008)
[Klein & Murray 2009] Klein, G., Murray, D.: Parallel tracking and mapping on a camera
phone. In: Proceedings of the 8th IEEE and ACM International Symposium on Mixed
and Augmented Reality, pp. 83–86 (2009)
[Konolige & Agrawal 2008] Konolige, K., Agrawal, M.: FrameSLAM: From bundle adjust-
ment to realtime visual mappping. IEEE Transactions on Robotics 24(5), 1066–1077
(2008)
[Konolige et al. 2007] Konolige, K., Agrawal, M., Sol`a, J.: Large-Scale Visual Odometry
for Rough Terrain. In: Proceedings of the International Symposium on Research in
Robotics, pp. 201–212 (2007)
[Kummerle et al. 2009] Kummerle, R., Steder, B., Dornhege, C., Ruhnke, M., Grisetti, G.,
Stachniss, C., Kleiner, A.: On measuring the accuracy of SLAM algorithms. Au-
tonomous Robots 27(4), 387–407 (2009)
[Kwok & Dissanayake 2004] Kwok, N.M., Dissanayake, G.: An Efﬁcient Multiple Hypoth-
esis Filter for Bearing-Only SLAM. In: IEEE/RSJ International Conference on Intelli-
gent Robots and Systems, pp. 736–741 (2004)
[Kwok et al. 2005] Kwok, N.M., Dissanayake, G., Ha, Q.P.: Bearing-only SLAM Using a
SPRT Based Gaussian Sum Filter. In: Proceedings of the 2005 IEEE International Con-
ference on Robotics and Automation, pp. 1109–1114 (2005)
[Li & Jilkov 2005] Rong Li, X., Jilkov, V.P.: Survey of Maneuvering Target Tracking. Part
V: Multiple-Model Methods. IEEE Transactions on Aerospace and Electronic Sys-
tems 41(4), 1255–1320 (2005)
[Lovegrove & Davison 2010] Lovegrove, S., Davison, A.: Real-time spherical mosaicing us-
ing whole image alignment. In: 11th European Conference on Computer Vision, pp.
73–86 (2010)
[Lowe 2004] Lowe, D.G.: Distinctive image features from scale-invariant keypoints. Inter-
national Journal of Computer Vision 60(2), 91–110 (2004)
[Marks et al. 1995] Marks, R.L., Rock, S.M., Lee, M.J.: Real-time Video Mosaicking of the
Ocean Floor. IEEE Journal of Oceanic Engineering 20(3), 229–241 (1995)
[Marzorati et al. 2008] Marzorati, D., Matteucci, M., Migliore, D., Sorrenti, D.G.: Monocu-
lar slam with inverse scaling parametrization. In: Proceedings of 2008 British Machine
Vision Conference (BMVC 2008), pp. 945–954. Citeseer (2008)
[Matthies et al. 1989] Matthies, L., Kanade, T., Szeliski, R.: Kalman Filter-based Algo-
rithms for Estimating Depth from Image Sequences. International Journal of Computer
Vision 3(3), 209–238 (1989)
[Maybank & Faugeras 1992] Maybank, S.J., Faugeras, O.: A Theory of Self-Calibration of
a Moving Camera. International Journal of Computer Vision 8(2), 123–151 (1992)
[Mikhail et al. 2001] Mikhail, E.M., Bethel, J.S., McGlone, J.C.: Introduction to modern
photogrammetry. John Wiley & Sons (2001)

References
165
[Mikolajczyk & Schmid 2005] Mikolajczyk, K., Schmid, C.: A performance evaluation
of local descriptors. IEEE Transactions on Pattern Analysis and Machine Intelli-
gence 27(10), 1615–1630 (2005)
[Mikolajczyk et al. 2005] Mikolajczyk, K., Tuytelaars, T., Schmid, C., Zisserman, A.,
Matas, J., Schaffalitzky, F., Kadir, T., Gool, L.V.: A comparison of afﬁne region de-
tectors. International Journal of Computer Vision 65(1), 43–72 (2005)
[Molton et al. 2004] Molton, N.D., Davison, A.J., Reid, I.D.: Locally Planar Patch Features
for Real-Time Structure from Motion. In: Proceedings of the 15th British Machine Vi-
sion Conference, Kingston (2004)
[Montiel et al. 2006] Montiel, J.M.M., Civera, J., Davison, A.J.: Uniﬁed Inverse Depth
Parametrization for Monocular SLAM. In: Proceedings of Robotics: Science and Sys-
tems, Philadelphia, USA (August 2006)
[Moreno-Noguer et al. 2008] Moreno-Noguer, F., Lepetit, V., Fua, P.: Pose priors for simul-
taneously solving alignment and correspondence. In: Forsyth, D., Torr, P., Zisserman,
A. (eds.) ECCV 2008, Part II. LNCS, vol. 5303, pp. 405–418. Springer, Heidelberg
(2008)
[Morimoto & Chellappa 1997] Morimoto, C., Chellappa, R.: Fast 3D Stabilization and Mo-
saic Construction. In: Proceedings of the IEEE Computer Vision and Pattern Recogni-
tion, pp. 660–665 (1997)
[Mouragnon et al. 2006] Mouragnon, E., Lhuillier, M., Dhome, M., Dekeyser, F., Sayd, P.:
Real-Time Localization and 3D Reconstruction. In: Proceedings of the IEEE Computer
Vision and Pattern Recognition, vol. 1, pp. 363–370 (2006)
[Mouragnon et al. 2009] Mouragnon, E., Lhuillier, M., Dhome, M., Dekeyser, F., Sayd, P.:
Generic and real-time structure from motion using local bundle adjustment. Image and
Vision Computing 27(8), 1178–1193 (2009)
[Neira & Tard´os 2001] Neira, J., Tard´os, J.D.: Data Association in Stochastic Mapping using
the Joint Compatibility Test. IEEE Transactions on Robotics and Automation 17(6),
890–897 (2001)
[Newman et al. 2002] Newman, P.M., Leonard, J.J., Neira, J., Tard´os, J.: Explore and Re-
turn: Experimental Validation of Real Time Concurrent Mapping and Localization. In:
Proceedings of the IEEE International Conference on Robotics and Automation, pp.
1802–1809 (2002)
[Nist´er et al. 2004] Nist´er, D., Naroditsky, O., Bergen, J.: Visual Odometry. In: Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition, vol. 1, pp. 652–
659 (2004)
[Nist´er et al. 2006] Nist´er, D., Naroditsky, O., Bergen, J.: Visual odometry for ground vehi-
cle applications. Journal of Field Robotics 23(1), 3–20 (2006)
[Nist´er 2004] Nist´er, D.: An efﬁcient solution to the ﬁve-point relative pose problem. IEEE
Transactions on Pattern Analysis and Machine Intelligence 26(6), 756–770 (2004)
[Nist´er 2005] Nist´er, D.: Preemptive RANSAC for live structure and motion estimation. Ma-
chine Vision and Applications 16(5), 321–329 (2005)
[Okutomi & Kanade 1993] Okutomi, M., Kanade, T.: A Multiple-Baseline Stereo. IEEE
Transactions on Pattern Analysis and Machine Intelligence 15(4), 353–363 (1993)
[Ort´ın & Montiel 2001] Ort´ın, D., Montiel, J.M.M.: Indoor robot motion based on monocu-
lar images. Robotica 19(03), 331–342 (2001)
[Ort´ın et al. 2003] Ort´ın, D., Montiel, J.M.M., Zisserman, A.: Automated Multisensor Poly-
hedral Model Acquisition. In: Proceedings of the IEEE International Conference on
Robotics and Automation (2003)
[Paz et al. 2008a] Paz, L.M., Pini´es, P., Tard´os, J.D., Neira, J.: Large-Scale 6-DOF SLAM
With Stereo-in-Hand. IEEE Transactions on Robotics 24(5), 946–957 (2008)

166
References
[Paz et al. 2008b] Paz, L.M., Tard´os, J.D., Neira, J.: Divide and Conquer: EKF SLAM in O
(n). IEEE Transactions on Robotics 24(5), 1107–1120 (2008)
[PhotoTourism 2011] PhotoTourism (June 2011),
http://phototour.cs.washington.edu/
[Pini´es & Tard´os 2008] Pini´es, P., Tard´os, J.D.: Large Scale SLAM Building Condition-
ally Independent Local Maps: Application to Monocular Vision. IEEE Transactions on
Robotics 24(5) (2008)
[Pini´es et al. 2006] Pini´es, P., Tard´os, J.D., Neira, J.: Localization of avalanche victims using
robocentric SLAM. In: Proc. IEEE/RSJ International Conference on Intelligent Robots
and Systems, pp. 3074–3079 (2006)
[Pini´es et al. 2007] Pini´es, P., Lupton, T., Sukkarieh, S., Tard´os, J.D.: Inertial aiding of in-
verse depth SLAM using a monocular camera. In: Proceedings of the 2007 IEEE Inter-
national Conference on Robotics and Automation, pp. 2797–2802 (2007)
[Pollefeys et al. 1999] Pollefeys, M., Koch, R., Van Gool, L.: Self-Calibration and Metric
Reconstruction Inspite of Varying and Unknown Intrinsic Camera Parameters. Interna-
tional Journal of Computer Vision 32(1), 7–25 (1999)
[Qian & Chellappa 2004] Qian, G., Chellappa, R.: Bayesian self-calibration of a moving
camera. Computer Vision and Image Understanding 95(3), 287–316 (2004)
[Raguram et al. 2008] Raguram, R., Frahm, J.-M., Pollefeys, M.: A comparative analysis
of RANSAC techniques leading to adaptive real-time random sample consensus. In:
Forsyth, D., Torr, P., Zisserman, A. (eds.) ECCV 2008, Part II. LNCS, vol. 5303, pp.
500–513. Springer, Heidelberg (2008)
[RAWSEEDS 2011] RAWSEEDS. RAWSEEDS public datasets web page (June 2011),
http://www.rawseeds.org/
[Renka 1997] Renka, R.J.: Algorithm 772: STRIPACK: Delaunay Triangulation and
Voronoi Diagram on the Surface of a Sphere. ACM Transactions on Mathematical Soft-
ware 23(3), 416–434 (1997)
[Rituerto et al. 2010] Rituerto, A., Puig, L., Guerrero, J.J.: Visual SLAM with an omnidirec-
tional camera. In: 2010 International Conference on Pattern Recognition, pp. 348–351
(2010)
[Rosten & Drummond 2005] Rosten, E., Drummond, T.: Fusing points and lines for high
performance tracking. In: Proceedings of the 10th IEEE International Conference on
Computer Vision, vol. 2, pp. 1508–1515 (2005)
[Rousseeuw & Leroy 1987] Rousseeuw, P.J., Leroy, A.M.: Robust regression and outlier de-
tection. Wiley Interscience, New York (1987)
[Sawhney et al. 1998] Sawhney, H.S., Hsu, S., Kumar, R.: Robust Video Mosaicing through
Topology Inference and Local to Global Alignment. In: Burkhardt, H., Neumann, B.
(eds.) ECCV 1998. LNCS, vol. 1407, pp. 103–119. Springer, Heidelberg (1998)
[Scaramuzza et al. 2007] Scaramuzza, D., Harati, A., Siegwart, R.: Extrinsic self calibration
of a camera and a 3d laser range ﬁnder from natural scenes. In: Proceedings of the
IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 4164–4169
(2007)
[Scaramuzza et al. 2009] Scaramuzza, D., Fraundorfer, F., Siegwart, R.: Real-Time Monoc-
ular Visual Odometry for On-Road Vehicles with 1-Point RANSAC. In: Proceedings of
the IEEE International Conference on Robotics and Automation, pp. 4293–4299 (2009)
[Scharstein & Szeliski 2002] Scharstein, D., Szeliski, R.: A taxonomy and evaluation of
dense two-frame stereo correspondence algorithms. International Journal of Computer
Vision 47(1), 7–42 (2002)
[Schindler & Suter 2006] Schindler, K., Suter, D.: Two-view multibody structure-and-
motion with outliers through model selection. IEEE Transactions on Pattern Analysis
and Machine Intelligence 28(6), 983–995 (2006)

References
167
[Sibley et al. 2009] Sibley, G., Mei, C., Reid, I., Newman, P.: Adaptive Relative Bundle Ad-
justment. In: Proceedings of Robotics: Science and Systems, Seattle, USA, June 2009,
pp. 976–982 (2009)
[Smith & Cheeseman 1986a] Smith, R.C., Cheeseman, P.: On the Representation and Esti-
mation of Spatial Uncertainty. International Journal of Robotics Research 5(4), 56–68
(1986)
[Smith & Cheeseman 1986b] Smith, R.C., Cheeseman, P.: On the representation and esti-
mation of spatial uncertainty. The International Journal of Robotics Research 5(4), 56
(1986)
[Smith et al. 1987] Smith, R., Self, M., Cheeseman, P.: A stochastic map for uncertain spatial
relationships. In: 4th International Symposium on Robotics Research (1987)
[Smith et al. 2009] Smith, M., Baldwin, I., Churchill, W., Paul, R., Newman, P.: The
New College Vision and Laser Data Set. The International Journal of Robotics Re-
search 28(5), 595–599 (2009)
[Sol`a et al. 2005] Sol`a, J., Monin, A., Devy, M., Lemaire, T.: Undelayed Initialization in
Bearing Only SLAM. In: 2005 IEEE/RSJ International Conference on Intelligent
Robots and Systems (2005)
[Sol`a et al. 2008] Sol`a, J., Monin, A., Devy, M., Vidal-Calleja, T.: Fusing Monocular Infor-
mation in Multicamera SLAM. IEEE Transactions on Robotics 24(5), 958–968 (2008)
[Sol`a et al. 2009] Sol`a, J., Vidal-Calleja, T., Devy, M.: Undelayed initialization of line seg-
ments in monocular SLAM. In: IEEE/RSJ International Conference on Intelligent
Robots and Systems, IROS 2009, pp. 1553–1558. IEEE (2009)
[Sol`a et al. 2011] Sol`a, J., Vidal-Calleja, T., Civera, J., Montiel, J.M.M.: Impact of landmark
parametrization on monocular EKF-SLAM with points and lines. Accepted for publi-
cation in International Journal of Computer Vision (2011)
[Sol`a 2007] Sol`a, J.: Towards Visual Localization, Mapping and Moving Objects Tracking
by a Mobile Robot: a Geometric and Probabilistic Approach. PhD thesis, LAAS-CNRS
(2007)
[Sol`a 2010] Sol`a, J.: Consistency of the monocular EKF-SLAM algorithm for three different
landmark parametrizations. In: Proceedings of the 2010 IEEE International Conference
on Robotics and Automation, pp. 3513–3518 (2010)
[Steedly et al. 2005] Steedly, D., Pal, C., Szeliski, R.: Efﬁciently registering video into
panoramic mosaics. In: Proceedings of the 10th IEEE International Conference on
Computer Vision, vol. 2, pp. 1300–1307 (2005)
[Strasdat et al. 2010a] Strasdat, H., Montiel, J.M.M., Davison, A.J.: Real-time monocular
SLAM: Why ﬁlter? In: IEEE International Conference on Robotics and Automation
(ICRA), pp. 2657–2664 (2010)
[Strasdat et al. 2010b] Strasdat, H., Montiel, J.M.M., Davison, A.J.: Scale drift-aware large
scale monocular SLAM. In: Proceedings of Robotics: Science and Systems (RSS)
(2010)
[Strasdat et al. 2011] Strasdat, H., Davison, A.J., Montiel, J.M.M., Konolige, K.: Double
Window Optimisation for Constant Time Visual SLAM. In: 13th International Con-
ference on Computer Vision (2011)
[Szeliski & Shum 1997] Szeliski, R., Shum, H.Y.: Creating full view panoramic image mo-
saics and environment maps. In: Proceedings of the 24th Annual Conference on Com-
puter Graphics and Interactive Techniques, pp. 251–258 (1997)
[Tardif et al. 2008] Tardif, J.P., Pavlidis, Y., Daniilidis, K.: Monocular visual odometry in
urban environments using an omnidirectional camera. In: Proceedings of the IEEE/RSJ
International Conference on Intelligent Robots and Systems, pp. 2531–2538 (2008)

168
References
[Tard´os et al. 2002] Tard´os, J.D., Neira, J., Newman, P.M., Leonard, J.J.: Robust mapping
and localization in indoor environments using sonar data. The International Journal of
Robotics Research 21(4), 311 (2002)
[Thrun et al. 2005] Thrun, S., Burgard, W., Fox, D.: Probabilistic robotics. MIT Press, Cam-
bridge (2005)
[Torr & Murray 1993] Torr, P.H.S., Murray, D.W.: Outlier detection and motion segmenta-
tion. Sensor Fusion VI 2059, 432–443 (1993)
[Torr & Zisserman 2000] Torr, P.H.S., Zisserman, A.: MLESAC: A new robust estimator
with application to estimating image geometry. Computer Vision and Image Under-
standing 78(1), 138–156 (2000)
[Torr et al. 1999] Torr, P.H.S., Fitzgibbon, A., Zisserman, A.: The Problem of Degeneracy
in Structure and Motion Recovery from Uncalibrated Image Sequences. International
Journal of Computer Vision 32(1), 27–45 (1999)
[Torr 2002] Torr, P.H.S.: Bayesian Model Estimation and Selection for Epipolar Geometry
and Generic Manifold Fitting. International Journal of Computer Vision 50(1), 35–61
(2002)
[Trawny & Roumeliotis 2006] Trawny, N., Roumeliotis, S.I.: A uniﬁed framework for
nearby and distant landmarks in bearing-only SLAM. In: Proceedings of the IEEE In-
ternational Conference on Robotics and Automation, pp. 1923–1929 (2006)
[Triggs et al. 2000] Triggs, B., McLauchlan, P.F., Hartley, R.I., Fitzgibbon, A.W.: Bundle
Adjustment – A Modern Synthesis. In: Triggs, B., Zisserman, A., Szeliski, R. (eds.)
ICCV-WS 1999. LNCS, vol. 1883, pp. 298–375. Springer, Heidelberg (2000)
[Vedaldi et al. 2005] Vedaldi, A., Jin, H., Favaro, P., Soatto, S.: KALMANSAC: Robust
ﬁltering by consensus. In: 10th IEEE International Conference on Computer Vision,
vol. 1, pp. 633–640 (2005)
[Wald 1945] Wald, A.: Sequential Tests of Statistical Hypothesis. The Annals of Mathemat-
ical Statistics 16(2), 117–186 (1945)
[Williams et al. 2007] Williams, B., Klein, G., Reid, I.: Real-time SLAM relocalisation. In:
IEEE 11th International Conference on Computer Vision, pp. 1–8 (2007)
[Williams et al. 2008] Williams, B., Cummins, M., Neira, J., Newman, P., Reid, I., Tardos,
J.: An image–to–map loop closing method for monocular SLAM. In: Proceedings of the
IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 2053–2059
(2008)
[Williams 2009] Williams, B.: Simultaneous Localisation and Mapping Using a Single Cam-
era. PhD thesis, University of Oxford (2009)
[Yang & Toderici 2011] Yang, W., Toderici, G.: Discriminative Tag Learning on YouTube
Videos with Latent Sub-tags. In: IEEE Computer Vision and Pattern Recognition (2011)
[Zhu et al. 2006] Zhu, Z., Xu, G., Riseman, E.M., Hanson, A.R.: Fast Construction of Dy-
namic and Multi-Resolution 360◦Panoramas from Video Sequences. Image and Vision
Computing 24(1), 13–26 (2006)

Springer Tracts in Advanced Robotics
Edited by B. Siciliano, O. Khatib and F. Groen
Further volumes of this series can be found on our homepage: springer.com
Vol. 75: Civera, J.; Davison, A.J.; Montiel, J.M.M.
Structure from Motion Using the Extended
Kalman Filter
168 p. 2012 [978-3-642-24833-7]
Vol. 73: Corke, P.;
Robotics, Vision and Control
XXX p. 2011 [978-3-642-20143-1]
Vol. 72: Mullane, J.; Vo, B.-N.; Adams, M.;
Vo, B.-T.
Random Finite Sets for Robot Mapping
and SLAM
146 p. 2011 [978-3-642-21389-2]
Vol. 70: Pradalier, C.; Siegwart, R.; Hirzinger, G.
(Eds.)
Robotics Research
752 p. 2011 [978-3-642-19456-6]
Vol. 69: Rocon, E.; Pons, J.L.
Exoskeletons in Rehabilitation Robotics
138 p. 2010 [978-3-642-17658-6]
Vol. 68: Hsu, D.; Isler, V.; Latombe, J.-C.;
Ming C. Lin (Eds.)
Algorithmic Foundations of Robotics IX
424 p. 2010 [978-3-642-17451-3]
Vol. 67: Schütz, D.; Wahl, F.M. (Eds.)
Robotic Systems for Handling
and Assembly
460 p. 2010 [978-3-642-16784-3]
Vol. 66: Kaneko, M.; Nakamura, Y. (Eds.)
Robotics Research
450 p. 2010 [978-3-642-14742-5]
Vol. 65: Ribas, D.; Ridao, P.; Neira, J.
Underwater SLAM for Structured
Environments Using an
Imaging Sonar
142 p. 2010 [978-3-642-14039-6]
Vol. 64: Vasquez Govea, A.D.
Incremental Learning for Motion Prediction
of Pedestrians and Vehicles
153 p. 2010 [978-3-642-13641-2]
Vol. 63: Vanderborght, B.;
Dynamic Stabilisation of the
Biped Lucy Powered by Actuators
with Controllable Stiffness
281 p. 2010 [978-3-642-13416-6]
Vol. 62: Howard, A.; Iagnemma, K.;
Kelly, A. (Eds.):
Field and Service Robotics
511 p. 2010 [978-3-642-13407-4]
Vol. 61: Mozos, Ó.M.
Semantic Labeling of Places with
Mobile Robots
134 p. 2010 [978-3-642-11209-6]
Vol. 60: Zhu, W.-H.
Virtual Decomposition Control –
Toward Hyper Degrees of
Freedom Robots
443 p. 2010 [978-3-642-10723-8]
Vol. 59: Otake, M.
Electroactive Polymer Gel Robots –
Modelling and Control of
Artiﬁcial Muscles
238 p. 2010 [978-3-540-23955-0]
Vol. 58: Kröger, T.
On-Line Trajectory Generation in Robotic
Systems – Basic Concepts for Instantaneous
Reactions to Unforeseen (Sensor) Events
230 p. 2010 [978-3-642-05174-6]
Vol. 57: Chirikjian, G.S.; Choset, H.;
Morales, M., Murphey, T. (Eds.)
Algorithmic Foundations
of Robotics VIII – Selected Contributions
of the Eighth International Workshop on the
Algorithmic Foundations of Robotics
680 p. 2010 [978-3-642-00311-0]
Vol. 56: Buehler, M.; Iagnemma, K.;
Singh S. (Eds.)
The DARPA Urban Challenge – Autonomous
Vehicles in City Trafﬁc
625 p. 2009 [978-3-642-03990-4]

Vol. 55: Stachniss, C.
Robotic Mapping and Exploration
196 p. 2009 [978-3-642-01096-5]
Vol. 54: Khatib, O.; Kumar, V.;
Pappas, G.J. (Eds.)
Experimental Robotics:
The Eleventh International Symposium
579 p. 2009 [978-3-642-00195-6]
Vol. 53: Duindam, V.; Stramigioli, S.
Modeling and Control for Efﬁcient Bipedal
Walking Robots
211 p. 2009 [978-3-540-89917-4]
Vol. 52: Nüchter, A.
3D Robotic Mapping
201 p. 2009 [978-3-540-89883-2]
Vol. 51: Song, D.
Sharing a Vision
186 p. 2009 [978-3-540-88064-6]
Vol. 50: Alterovitz, R.; Goldberg, K.
Motion Planning in Medicine: Optimization
and Simulation Algorithms for
Image-Guided Procedures
153 p. 2008 [978-3-540-69257-7]
Vol. 49: Ott, C.
Cartesian Impedance Control of Redundant
and Flexible-Joint Robots
190 p. 2008 [978-3-540-69253-9]
Vol. 48: Wolter, D.
Spatial Representation and
Reasoning for Robot
Mapping
185 p. 2008 [978-3-540-69011-5]
Vol. 47: Akella, S.; Amato, N.;
Huang, W.; Mishra, B.; (Eds.)
Algorithmic Foundation of Robotics VII
524 p. 2008 [978-3-540-68404-6]
Vol. 46: Bessière, P.; Laugier, C.;
Siegwart R. (Eds.)
Probabilistic Reasoning and Decision
Making in Sensory-Motor Systems
375 p. 2008 [978-3-540-79006-8]
Vol. 45: Bicchi, A.; Buss, M.;
Ernst, M.O.; Peer A. (Eds.)
The Sense of Touch and Its Rendering
281 p. 2008 [978-3-540-79034-1]
Vol. 44: Bruyninckx, H.; Pˇreuˇcil, L.;
Kulich, M. (Eds.)
European Robotics Symposium 2008
356 p. 2008 [978-3-540-78315-2]
Vol. 43: Lamon, P.
3D-Position Tracking and Control
for All-Terrain Robots
105 p. 2008 [978-3-540-78286-5]
Vol. 42: Laugier, C.; Siegwart, R. (Eds.)
Field and Service Robotics
597 p. 2008 [978-3-540-75403-9]
Vol. 41: Milford, M.J.
Robot Navigation from Nature
194 p. 2008 [978-3-540-77519-5]
Vol. 40: Birglen, L.; Laliberté, T.; Gosselin, C.
Underactuated Robotic Hands
241 p. 2008 [978-3-540-77458-7]
Vol. 39: Khatib, O.; Kumar, V.; Rus, D. (Eds.)
Experimental Robotics
563 p. 2008 [978-3-540-77456-3]
Vol. 38: Jefferies, M.E.; Yeap, W.-K. (Eds.)
Robotics and Cognitive Approaches to
Spatial Mapping
328 p. 2008 [978-3-540-75386-5]
Vol. 37: Ollero, A.; Maza, I. (Eds.)
Multiple Heterogeneous Unmanned Aerial
Vehicles
233 p. 2007 [978-3-540-73957-9]
Vol. 36: Buehler, M.; Iagnemma, K.;
Singh, S. (Eds.)
The 2005 DARPA Grand Challenge – The Great
Robot Race
520 p. 2007 [978-3-540-73428-4]
Vol. 35: Laugier, C.; Chatila, R. (Eds.)
Autonomous Navigation in Dynamic
Environments
169 p. 2007 [978-3-540-73421-5]
Vol. 34: Wisse, M.; van der Linde, R.Q.
Delft Pneumatic Bipeds
136 p. 2007 [978-3-540-72807-8]
Vol. 33: Kong, X.; Gosselin, C.
Type Synthesis of Parallel
Mechanisms
272 p. 2007 [978-3-540-71989-2]

