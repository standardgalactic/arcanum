
Ladri di Biblioteche

È sempre più diffusa la convinzione che la matematica sia lo
strumento principe per analizzare e prevedere ogni aspetto della
realtà, e questa visione è diventata tanto influente per il ruolo
centrale che la matematica ha assunto negli sviluppi recenti della
fisica e della tecnologia. Ma il mondo è matematico? Per
rispondere a questa domanda, l’autore spiega con un approccio
estremamente semplice come venga usata la matematica nella
versione più recente della costruzione di modelli. Traccia poi il
percorso storico che ha condotto dall’idea di Galileo che il mondo
è scritto in linguaggio matematico fino alla contemporanea
modellistica matematica. Infine descrive le problematiche più
recenti – tra cui il tema della “ragionevole” o “irragionevole”
efficacia della matematica e il ruolo del calcolatore nella ricerca
scientifica.
Giorgio Israel , già professore ordinario di Storia della matematica
presso l’Università di Roma “La Sapienza” e direttore del Centro di
Ricerca in Metodologia delle Scienze, è membro della Académie
Internationale d’Histoire des Sciences.

Giorgio Israel
La matematica e la realtà
Capire il mondo con i numeri

Copyright © by Carocci editore, Roma. Tutti i diritti sono riservati.
Per altre informazioni si veda http://www.carocci.it/
Edizione a stampa 2015
ISBN 9788843077113
Edizione e-book 2016, realizzata dal Mulino - Bologna, per conto
della Carocci editore - Roma
ISBN 9788843083978

Indice
Introduzione

Dalla realtà alla matematica attraverso gli esempi

Dalla fisica di Galileo alla modellistica matematica

Cenni alle problematiche più recenti
Bibliografia

Introduzione
Le scienze non cercano di spiegare, a
malapena tentano di interpretare, ma fanno
soprattutto 
dei 
modelli. 
Per 
modello
s’intende un costrutto matematico che, con
l’aggiunta di certe interpretazioni verbali,
descrive 
dei 
fenomeni 
osservati. 
La
giustificazione 
di 
un 
siffatto 
costrutto
matematico è soltanto e precisamente che ci
si aspetta che funzioni – cioè descriva
correttamente 
i 
fenomeni 
in 
un’area
ragionevolmente ampia. Inoltre esso deve
soddisfare certi criteri estetici – cioè, in
relazione con la quantità di descrizione che
fornisce, deve essere piuttosto semplice.
John von Neumann
Galileo Galilei e Isaac Newton, i due grandi fondatori della
scienza fisico-matematica moderna, non avrebbero in alcun
modo sottoscritto i concetti contenuti nel brano di John von
Neumann (uno dei massimi scienziati del Novecento) che
abbiamo posto in epigrafe. Secondo Galileo, l’essenza del mondo
«è scritta in questo grandissimo libro che continuamente ci sta
aperto innanzi agli occhi (e dico l’universo), ma non si può
intendere se prima non s’impara a intender la lingua, a conoscer i
caratteri ne’ quali è scritto. Egli è scritto in lingua matematica, e i
caratteri son triangoli, cerchi e altre figure geometriche senza i
quali mezzi è impossibile a intenderne umanamente parola;

senza questi è un aggirarsi vanamente per un oscuro labirinto»
(Galilei, 1623, p. 232). In altri termini, “il mondo è matematico”,
esso è stato scritto (da Dio) in linguaggio matematico, e chi lo
conosce è in grado di comprenderlo. Newton va oltre e pensa sé
stesso come una sorta di nuovo profeta, una sintesi di Mosè e
Pitagora che, con lo strumento della matematica, apre la strada
alla comprensione dei segreti dell’universo e avvicina alla
conoscenza della Prima Causa, ovvero di Dio (Newton, 1706, query
28). Sia Galileo che Newton erano convinti che fosse un compito
perfettamente perseguibile della scienza spiegare e interpretare il
mondo, e per entrambi lo strumento chiave in tal senso era la
matematica. E allora perché mai abbiamo posto in epigrafe un
brano come quello di von Neumann? La risposta è semplice;
perché questo – e non quello di Galileo e di Newton – è il punto di
vista 
che 
domina 
nella 
scienza 
moderna, 
anche 
se
paradossalmente l’uso della matematica si è esteso in modo
impressionante, anche ad ambiti cui i due grandi fondatori non
avrebbero mai pensato. Pur nello scetticismo che pervade la sua
dichiarazione circa i limiti d’intervento della scienza, von
Neumann è convinto che la matematica sia lo strumento migliore
in assoluto per descrivere e prevedere i fenomeni ed è fautore di
un vero e proprio programma “panmatematico” (come abbiamo
mostrato in Israel, Millán Gasca, 2008). Ancor oggi molti
continuano a ripetere che “il mondo è matematico”.
Poiché lo scopo di questo libro è di introdurre il lettore privo di
qualsiasi conoscenza tecnica al modo in cui la matematica
interviene nello studio dei fenomeni reali, l’approccio più
coerente è di farlo attraverso esempi relativamente più moderni
(per quanto consente il vincolo della semplicità) e quindi interni a

una visione come quella di von Neumann, che possiamo definire
come modellistica. Come si è detto, ormai il campo dei fenomeni
“matematizzati” 
– 
ossia 
studiati 
in 
termini 
quantitativi,
matematici – è sterminato e ci vorrebbero centinaia di pagine per
una presentazione organizzata che, a quel punto, diverrebbe
pesante e difficile. Cosa si potrà fare allora? Dare un’idea
introduttiva dei problemi e dei metodi cui ricorre la matematica
quando si propone di studiare i fenomeni reali secondo
l’approccio in termini di modelli matematici. E poiché non potremo
seguire né un’impostazione informativa né tecnica, seguiremo un
approccio culturale. Ciò significa che tenteremo di illustrare i
principali problemi di ordine concettuale nonché la loro
collocazione e origine storica. Ho detto illustrare, perché il modo
che mi sembra migliore per affrontare questi problemi tanto
complessi e difficili è quello della scelta di pochi esempi
significativi e rappresentativi. Esempi, cioè, che racchiudono in sé
buona parte dei problemi tipici che si presentano nella
costruzione e nella verifica di un modello matematico. E si
tratterà di esempi semplici, che possano essere compresi con un
bagaglio tecnico minimo, talmente modesto da poter introdurre a
parole le pochissime formule matematiche che useremo.
Naturalmente la matematica ricorre a un livello di sofisticazione
tecnica di ben altro livello.
Nel primo capitolo esamineremo una serie di esempi la cui
descrizione matematica è comprensibile da chiunque, alla sola
(ovvia!) 
condizione 
di 
mettere 
impegno 
nel 
seguire 
i
ragionamenti. Gli esempi saranno seguiti da un’esposizione
dettagliata di alcuni fra i problemi concettuali che accompagnano
la costruzione di un modello matematico.

Nel secondo capitolo, in coerenza con l’impostazione culturale
scelta, sarà rovesciato l’approccio: si tenterà di descrivere le tappe
fondamentali della lunga vicenda del rapporto tra matematica e
realtà, del ricorso alla matematica nello studio dei fenomeni, una
vicenda 
che 
va 
dalla 
fisica 
classica 
alla 
modellistica
contemporanea. Ciò condurrà alla discussione del concetto di
modello matematico.
Il terzo capitolo è dedicato alla riflessione su tematiche più
recenti e mira a dare qualche risposta (volutamente problematica
e quindi aperta) alla domanda se si possa ancora dire che “il
mondo è matematico”. Esamineremo alcuni argomenti che
confortano e altri che confutano tale tesi. Su un punto non sarò
“ambiguo”: sono fermamente convinto che la matematica debba
essere considerata come parte della cultura e che sia necessaria
una visione umanistica che la includa nel grande progetto di
crescita della conoscenza umana. Quindi, se questo libro non
offre alcuna novità per gli specialisti, esso può forse avere, oltre al
compito primario della divulgazione, qualche utilità dal punto di
vista che ho chiamato culturale. Vuole, infatti, contribuire, oltre
che alla divulgazione scientifica, a dissipare l’idea che la scienza
sia un puro “sapere tecnico” e non faccia parte della cultura.
Pregiudizio, 
purtroppo 
persistente, 
di 
cui 
portano 
una
responsabilità non secondaria gli stessi scienziati.
La prima versione di questo libro vide la luce nel lontano 1986,
come volume n. 98 della Collana dei “Libri di base” diretta da
Tullio De Mauro e pubblicata da Editori Riuniti. In seguito, fu
ristampato nel 2002 con poche modifiche per i tipi di Franco
Muzzio Editore. Questa nuova versione conserva con le precedenti
l’intento che ne era alla base, ma se ne distacca per una revisione

più profonda: sono stati introdotti nuovi esempi (in particolare di
teoria dei giochi); la discussione del concetto di modello
matematico viene alla fine dell’excursus storico; il terzo capitolo
ha una struttura completamente nuova e affronta direttamente la
questione se possa dirsi che “il mondo è matematico”.
Al lettore che voglia approfondire i temi qui introdotti
suggerisco 
alcuni 
percorsi 
di 
riflessione 
e 
alcune 
piste
bibliografiche che sono quelle su cui mi sono orientato nel corso
degli anni passati. Un’analisi approfondita, con esempi anche più
tecnici, si trova nel volume La visione matematica della realtà.
Introduzione ai temi e alla storia della modellistica matematica, Laterza,
Roma-Bari 1996. I temi della matematizzazione dell’economia
sono stati studiati nel volume in collaborazione con B. Ingrao, La
Mano Invisibile. L’equilibrio economico nella storia della scienza,
Laterza, Roma-Bari, I ed. 1987, III ed. 1999 (trad. ingl. MIT Press,
Cambridge, MA, 1990, 2000). Per quanto riguarda la biomatematica,
un quadro storico degli sviluppi dell’“età d’oro” si ricava dal
volume (in collaborazione con A. Millán Gasca) The Biology of
Numbers: The Correspondence of Vito Volterra on Mathematical Biology,
Birkhäuser, Basel 2002. Una discussione epistemologica delle
forme della modellistica matematica contemporanea è contenuta
nel saggio: Modèle-récit ou récit-modèle?, in Le modèle et le récit (sous
la direction de J.-Y. Grenier, C. Grignon, P.-M. Menger), Éditions de
la Maison des sciences de l’homme, Paris 2001, pp. 365-424. Cfr.
inoltre: Modelli matematici, Quaderno n. 81 di “Le Scienze”,
dicembre 1994, pp. 104, a cura di G. Israel. Oltre a un volume su
John 
von 
Neumann 
(Il 
mondo 
come 
gioco 
matematico, 
in
collaborazione con A. Millán Gasca, Bollati Boringhieri, Torino
2008), un libro che ha uno stretto rapporto con questo è il recente

Pensare in matematica (in collaborazione con A. Millán Gasca),
Zanichelli, Bologna 2012 che, oltre a chi si occupa di formazione
primaria nella scuola, si rivolge a chiunque voglia approfondire la
propria conoscenza della matematica, anche senza alcuna base
preliminare, ma condividendo l’approccio culturale e umanistico
di cui siamo fermamente convinti.

1
Dalla realtà alla matematica attraverso gli esempi
La scienza non è una collezione di fatti, più
di quanto un ammasso di pietre non sia una
casa.
Henri Poincaré
Un primo esempio: la legge di crescita malthusiana di
una popolazione
Supponiamo che sia nato in noi il desiderio di sapere come
varia (cioè, come aumenta o diminuisce) la popolazione del
nostro paese o di un continente o dell’intero globo. “Quanti
saremo nell’anno 2050 o nel-l’anno 2500 in Italia, in Europa, in
Asia, sulla Terra?” è la domanda cui vorremmo poter dare una
risposta. Vorremmo cioè conoscere una legge, una “legge
scientifica”, 
che 
descriva 
le 
tendenze 
fondamentali 
della
variazione di una popolazione e che permetta di prevederle; così
come è possibile prevedere fra quanto tempo giungeremo in una
data località viaggiando verso di essa a una velocità costante o
variabile e come è possibile prevedere, mediante calcoli, molte
altre cose assai complesse.
Prima di procedere ci rendiamo però subito conto della
necessità di delimitare con precisione il campo della nostra
ricerca. Il problema può essere infatti affrontato in modi diversi,
più o meno raffinati. Potremmo essere interessati a distinguere,
nella nostra legge di crescita, i maschi dalle femmine; oppure a
conoscere la velocità di crescita in diverse fasce di età e prevedere
le future distribuzioni d’età. Potremmo voler rispondere a

domande specifiche, del tipo: “La popolazione umana tende a
invecchiare e, in caso affermativo, con quale ritmo? Quale sarà fra
venti o trenta anni la percentuale della popolazione femminile sul
totale?”, e così via. Queste domande eccitano la nostra curiosità
ma complicano enormemente il problema. Delimitiamo quindi
radicalmente, almeno per ora, il nostro fine: vogliamo conoscere
la legge di variazione di una popolazione, considerata come un
insieme indifferenziato di individui. Vogliamo cioè sapere quali
saranno gli effetti che la capacità riproduttiva e la capacità di
adattamento all’ambiente hanno sullo sviluppo numerico di
questa popolazione, trascurando un’analisi più dettagliata.
Delimitando così il nostro fine abbiamo compiuto un passo
importante: in effetti, abbiamo identificato quel che ci interessa
conoscere. Ciò consente di mettere un piede dentro la
matematica: difatti, quel che vogliamo è conoscere un numero,
anzi tanti numeri, un numero per ogni istante temporale.
Pretendiamo addirittura di dire quanti saranno e quanti furono gli
esseri umani in ogni istante; e ciò non sulla base di tabelle
statistiche (che, ovviamente, per i tempi futuri non sono
disponibili!), ma sulla base di una legge matematica. Quindi
questa legge deve fornire, mediante calcoli, un numero per ogni
istante di tempo. Indichiamo con il simbolo N questo numero (N
è, per esempio, il numero degli abitanti del globo). N però non è
un numero ma il “simbolo” di un numero che varia nel tempo.
Anche il tempo, lo sappiamo bene, può essere contato con
numeri, basta fissare un’“unità” di misura (gli anni, i secoli ecc.).
Con sempre maggiore confidenza per i simboli matematici,
scriveremo allora N(t) e questo simbolo può essere così spiegato:
“N(t) è il numero degli individui della popolazione nell’istante di
tempo t”. Per esempio, se l’unità di misura temporale è l’anno,

N(1890) rappresenta il numero degli esseri umani nell’anno 1890 e
N(2050) il numero (per ora a noi sconosciuto) degli esseri umani
nell’anno 2050.
N(t) è quel che i matematici chiamano una “variabile” funzione
di un’altra variabile, in questo caso del tempo: infatti il numero N
varia in funzione dell’istante di tempo t considerato. Si può anche
dire che N è “funzione” del tempo. Il nostro problema è proprio
quello di determinare la legge secondo la quale N varia in
funzione di t. Questa legge potrebbe essere la più banale: N resta
lo stesso, cioè resta costante, per ogni istante di tempo t.
Scriveremo in tal caso: N(t) = costante e diremo che N(t) è una
“costante”. Per esempio N(t) = 3.000.000.000, a ogni istante t, vuol
dire che la popolazione mondiale è sempre stata, è, e sarà sempre
composta di tre miliardi di individui. I decessi e le nascite si
compensano esattamente in modo da mantenere il numero
costante.
Questa è una legge e per giunta assai semplice, ma purtroppo
evidentemente falsa. È falsa non perché sia scritta male o in
modo assurdo ma perché è in stridente contrasto con tutti i fatti
noti. E a nessuna persona ragionevole interessano le leggi false,
anche se scritte in forma matematica.
A questo punto, privi come siamo di qualsiasi nozione, la nostra
avventura rischia di interrompersi. Come ricavare la legge
desiderata? Comprendiamo subito che non è possibile procedere
a caso, ma occorre fondarsi sulle solide basi di ciò che è già noto.
Converrà dunque volgere lo sguardo verso il passato e studiare le
statistiche riguardanti lo sviluppo delle popolazioni. La nostra
speranza è quella di scoprire, attraverso questo studio, che le

popolazioni variano sempre più o meno nello stesso modo, di
scoprire, cioè, delle uniformità di comportamento. Queste uniformità
di comportamento potranno allora essere tradotte in una legge.
Se riusciremo in questo ambizioso obiettivo potremo anche fare
l’ipotesi che la nostra legge valga non soltanto per il passato ma
anche per il futuro. La verifica della bontà della nostra ipotesi sarà
affidata agli anni futuri... Tuttavia potremo ritenere la nostra
legge affidabile se essa sarà in accordo con gran parte dei dati
conosciuti circa l’evoluzione passata e inoltre se è ragionevole
prevedere che nel futuro non cambieranno troppo le condizioni
entro cui si è verificato lo sviluppo della popolazione considerata.
Per incamminarci su questa via, proviamo a ragionare su
qualche 
semplice 
insieme 
di 
dati. 
I 
volumi 
annuali 
di
aggiornamento 
dell’Enciclopedia 
Britannica, 
per 
esempio,
forniscono una serie di stime sul numero dei cittadini italiani.
Scegliamo il periodo che va dal 1971 al 1981. I dati relativi a
questo periodo sono riportati nella TAB. 1.1.
Le cifre pongono subito nuovi problemi. In primo luogo non vi
sono dati per il 1974. Ma soprattutto si constata che il dato
relativo al 1971 è preciso fino alle unità, altri sono evidentemente
approssimati alle centinaia e altri ancora alle migliaia. È probabile
che il primo dato sia il frutto di un censimento. Ma gli altri? Sono
il frutto di censimenti grossolani? Condotti con quali metodi,
visto che il margine d’errore è diverso? Oppure sono il frutto di
stime ricavate mediante una legge proprio del tipo di quella che
andiamo cercando? Intuiamo allora quanto sia avventato mettere
sullo stesso piano dati ottenuti in modi diversi (e non noti) e ci
guardiamo bene dal complicare le cose riempiendo la casella
vuota del 1974 con un dato ricavato da un’altra fonte. Ci si

spalanca di fronte una selva di problemi delicati e complessi:
problemi relativi al modo con cui si ottengono i dati statistici, alla
legittimità di certe approssimazioni, e così via.
Comprendiamo che servirebbe una mole di dati ben maggiore,
estesa a un periodo più ampio. È anche chiaro che non ha senso
restringersi al solo caso della popolazione italiana, ma che si
dovrebbe confrontarlo con molti altri casi. E dovremmo conoscere
l’origine di tutti i nostri dati e la loro attendibilità. Occorrerebbe
insomma un lavoro di analisi dei dati superiore alle nostre forze.
Non potremo quindi essere accusati di pigrizia mentale se
tenteremo di vedere se qualcun altro ha già fatto o iniziato questo
lavoro.
TABELLA 1.1 Popolazione italiana nel periodo 1971-81
t (anno)
N(t) (numero dei cittadini italiani)
1971
54.025.211
1972
54.345.000
1973
55.154.000
1974
–
1975
55.613.000
1976
56.014.200
1977
56.324.700
1978
56.600.400
1979
56.828.500

1980
56.999.000
1981
57.198.000
Prima però riflettiamo ancora un poco sulla nostra tabella di
dati. 
È 
abbastanza 
evidente 
che, 
per 
le 
nostre 
rozze
considerazioni, un’approssimazione dei numeri alle migliaia,
come nella TAB. 1.2, è più che sufficiente.
Vogliamo ora ottenere una rappresentazione grafica della TAB.
1.2 che permetta di scoprire qualche tendenza in modo più
efficace e intui-tivo di quanto consentano i numeri. Questa
rappresentazione consiste in un semplice metodo in uso ormai da
secoli e che va sotto il nome di “rappresentazione in coordinate
cartesiane”. Consideriamo due rette perpendicolari fra di loro.
Sulla retta orizzontale (detta “ascissa”) rappresenteremo la
variabile tempo. Per farlo è necessario fissare un’unità di misura:
nel nostro caso è spontaneo scegliere come unità di misura
l’anno, per cui la durata di un anno sarà rappresentata da un
segmento di lunghezza fissata. Quindi, rappresentando il primo
anno che ci interessa (il 1971) con il punto di incrocio delle rette,
rappresenteremo gli altri con punti che si seguono alla distanza
data. Sceglieremo poi come unità di misura per la popolazione il
milione di individui e riporteremo sulla retta verticale (detta
“ordinata”) i valori della TAB. 1.2, corrispondenti ai vari anni. È
evidente che, per esempio, un decimo del segmento unità di
misura rappresenta 100.000 unità e così via. I dati in nostro
possesso sono rappresentati con buona approssimazione nella FIG.
1.1, sull’asse delle ordinate. Se ora vogliamo “temporalizzare”
ogni dato, trasporteremo i punti che li rappresentano “sopra”

l’anno in cui il dato è stato rilevato. Procederemo perciò come
indica la figura e cioè considerando i punti d’incrocio fra le rette
tratteggiate. Ognuno di questi nuovi punti rappresenta un dato
“temporalizzato”. Infatti esso è individuato non soltanto dalla sua
“quota” o ordinata (che esprime appunto il valore del dato) ma
anche dalla sua posizione rispetto all’asse delle ascisse. Proprio
questa posizione permette di dire qual è la data in cui è stato
rilevato il nostro numero. È da notare che la rappresentazione
della FIG. 1.1 ha senso se tutti i dati sono stati ottenuti a un anno
di distanza l’uno dall’altro (per esempio, tutti all’inizio dell’anno).
In caso contrario, occorrerebbe rappresentare i punti con ascisse
diverse: per esempio, se il dato N (1973) è stato stimato alla metà
del 1973 occorre rappresentarlo con un punto la cui ascissa si
trovi alla metà dell’intervallo compreso fra 1973 e 1974. Noi non
sappiamo se questa ipotesi sia giusta, e ciò riporta ai problemi già
sollevati circa i dati della TAB. 1.1, ma, per proseguire nella
discussione del nostro esempio, ci comporteremo come se lo
fosse.
TABELLA 1.2 Popolazione italiana nel periodo 1971-81 (approssimata)
t (anno)
N(t) (numero dei cittadini italiani)
1971
54.025.000
1972
54.345.000
1973
55.154.000
1974
–
1975
55.613.000

1976
56.014.000
1977
56.325.000
1978
56.600.000
1979
56.828.000
1980
56.999.000
1981
57.198.000

FIGURA 1.1 Istogramma di crescita della popolazione italiana nel
periodo 1971-81
In definitiva, abbiamo ottenuto una sequenza di punti che
rappresenta graficamente la TAB. 1.2 e che va sotto il nome di
“istogramma”. L’istogramma illustra con notevole efficacia visiva
l’andamento della popolazione. Questo andamento appare
dapprima in rapida crescita e poi in crescita sempre più lenta,
tanto da far sospettare che la popolazione tenda verso uno stato
stazionario.
Per “vedere” meglio questa tendenza siamo tentati di collegare i
punti mediante segmenti di retta, come nella FIG. 1.2 o, ancor
meglio, con una curva priva di angolosità (la curva tratteggiata
nella FIG. 1.2), che suggerisce un andamento molto regolare e privo
di sbalzi. Questa operazione del tutto spontanea ha un senso: è
ben vero che le due “curve continue” della FIG. 1.2 (i matematici
del Settecento chiamavano continua una curva che si può
disegnare senza staccare mai la penna dal foglio) mostrano con
maggiore chiarezza l’andamento del fenomeno in esame, più di
quanto non lo mostrino i dieci punti della FIG. 1.1. Ci attendiamo,
infatti, che il matematico, incallito conoscitore di curve e
funzioni, non appena vista la nostra curva, “la riconosca”, più o
meno come un esperto entomologo sa riconoscere ogni tipo
d’insetto. E ci attendiamo quindi che egli ci dica come potrebbe
proseguire quel tipo di curva “dopo”: il che è quanto enunciare la
legge del fenomeno che consente di prevedere l’andamento
futuro. Non abbiamo in fondo tutti i torti, perché è più o meno
così che si procede. Però lo zoo delle curve è infinito e il
matematico non sarà in generale capace di darci una risposta
pronta e sicura: gli saranno necessari molti dati in più e uno

studio accurato per ottenere tutt’al più risultati attendibili.
FIGURA 1.2 Curva di crescita della popolazione italiana nel
periodo 1971-81
Il punto cruciale che ci preme sottolineare è tuttavia un altro.
Se il nostro matematico è, almeno in linea di principio, in grado di
ricavare qualche conclusione dal nostro grafico, ciò accade perché
noi, tracciando una curva continua, gli abbiamo offerto una
massa di informazioni immensamente più grande di quella
costituita dai dieci punti della FIG. 1.1. La curva fornisce infatti non

dieci ma infinite informazioni! Per ogni punto compreso fra 1971
e 1981, e cioè per ogni istante temporale compreso fra queste due
date, essa dice qual è il numero degli individui della popolazione.
È sufficiente che la scala sia abbastanza grande per determinarlo
in modo semplice. Per esempio, per conoscere il numero di
individui al 30 gennaio 1976 basterà dividere l’intervallo compreso
fra 1976 e 1977 in 365 parti e, individuata la trentesima tacca,
tracciare una retta verticale passante per essa. Il punto d’incontro
con la curva darà il numero cercato.
Cosa 
è 
accaduto? 
Come 
mai 
possediamo 
ora 
tante
informazioni? Il fatto è che, quando abbiamo collegato in modo
continuo i punti con segmenti o tratti di curva, abbiamo
implicitamente deciso che nel periodo compreso fra due
rilevazioni (e cioè nell’anno) la popolazione è cresciuta secondo la
legge descritta dal segmento o dal tratto di curva congiungente. E
se le cose fossero andate diversamente? Ora che il dubbio è sorto,
comprendiamo quanto siamo stati avventati: i punti possono
essere collegati in infiniti modi! Quelli della FIG. 1.3 sono solo
alcuni dei tanti. Il nostro matematico, posto di fronte a queste
curve, dirà ancora che la popolazione, nel lungo periodo, tende a
crescere in modo sempre meno veloce, ma aggiungerà che tale
crescita si accompagna a fluttuazioni di notevole ampiezza. Come
si vede le conclusioni tratte sono assai diverse. Il collegamento (o
“interpolazione”) dei punti è quindi un’operazione estremamente
delicata.
Ma – si dirà – che ragione c’è di ritenere che la popolazione
italiana sia cresciuta in modo diverso da quanto indica la FIG. 1.2?
Nel decennio in esame non vi sono state epidemie catastrofiche,
né stragi o guerre e non risulta che i fenomeni di emigrazione o

immigrazione abbiano influito significativamente o abbiano avuto
un 
andamento 
irregolare. 
Quindi, 
l’interpolazione 
è
sostanzialmente corretta. Ciò è probabilmente vero. Ma per
giustificare la nostra ardita ipotesi bisognava proprio tener conto
di considerazioni del genere! Poiché è assurdo pensare non
soltanto a censimenti ma anche a stime di una popolazione tanto
numerosa su intervalli di tempo inferiori all’anno, l’operazione di
interpolazione è, come si diceva, necessaria. Tuttavia, perché sia
corretta si deve disporre del massimo di giustificazioni possibile a
sostegno del modo in cui compiamo tale operazione. Quindi,
anche se in questo caso usciamo indenni dalla critica, abbiamo
appreso una lezione: occorre fare molta, molta attenzione.
Facciamo ora un esempio di una procedura d’interpolazione
scorretta. Il signor Rossi è costretto a letto da una fastidiosa
influenza. Il suo medico che, oltre a essere assai meticoloso,
coltiva la pretesa di studiare l’andamento generale della
temperatura nei malati d’influenza, prega la moglie del signor
Rossi di misurare la temperatura del malato ogni quattro ore,
rispettando l’intervallo notturno, e di annotare i risultati delle
misurazioni. La signora Rossi esegue scrupolosamente il compito
affidatole e anzi prepara per il dottore il grafico riportato nella FIG.
1.4, che raccoglie le misurazioni di più di due giorni. Tuttavia, la
signora non si limita a tracciare l’istogramma, ma congiunge con
dei segmenti i punti dell’istogramma, ottenendo la curva della
temperatura. Malauguratamente, nella notte del secondo giorno il
signor Rossi, goloso impenitente, sentendosi assai meglio e colto
da un grande appetito è andato di nascosto in cucina, ha aperto il
frigorifero e ha divorato un po’ di tutto quel che ha trovato. Più
tardi, sentendo la febbre risalire come conseguenza della sua

imprudenza, 
terrorizzato 
dall’idea 
di 
dover 
subire 
aspri
rimproveri, ha ingerito due compresse di aspirina, riuscendo a
riportare la temperatura a valori accettabili. Scherzi a parte,
anche se il signor Rossi non fosse uno sconsiderato, la curva della
signora Rossi (e non solo i segmenti relativi alle notti) sarebbe
priva di qualsiasi giustificazione e dovrebbe essere sostituita da
una curva ben più tormentata.
FIGURA 1.3 Alcune possibili curve di crescita della popolazione
italiana nel periodo 1971-81

FIGURA 1.4 Curva della temperatura del signor Rossi
Tutti sanno che la temperatura umana è soggetta a fluttuazioni
tanto frequenti quanto difficilmente prevedibili. L’unica curva
indiscutibile sarebbe quella ottenuta applicando sotto l’ascella del
paziente un termometro in permanenza e rilevando in modo
continuo, per quanto lo consentano i sensi, i dati della
temperatura.
È evidente che nessuno è disposto ad affrontare simili ridicoli
sacrifici. 
Occorrerà 
dunque 
interpolare. 
Ma, 
per 
fare
un’interpolazione seria, sarà comunque necessario misurare la
temperatura almeno ogni mezz’ora. È quasi superfluo osservare
che questa esigenza non ha nulla a che vedere con il fatto che
tante misure sono inutili nella terapia medica o che gli intenti del
medico del signor Rossi sono assurdi. Nella pratica è sufficiente
un istogramma composto da pochi dati. Ma se si vorrà tracciare
una curva, affinché essa abbia senso, occorrerà ricavarla con
criteri rigorosi. Così, sarebbe fuori luogo obiettare che talune
oscillazioni cui va soggetta la temperatura umana possono essere
poco interessanti ai fini di individuarne la tendenza generale.

Sarebbe cioè fuori luogo dire che potrebbe trattarsi di fluttuazioni
secondarie, di perturbazioni della tendenza di fondo. Infatti, se
riuscissimo a trarre una conclusione del genere, sulla base di dati
oggettivi e di argomentazioni convincenti, avremmo ottenuto
appunto una giustificazione per compiere un’interpolazione
rigorosa. Cioè, avremmo ottenuto una giustificazione per
congiungere le rilevazioni empiriche riportate nell’istogramma (e
ottenute a intervalli di tempo che sappiamo accettabili) mediante
una curva.
Le nostre divagazioni lasciano intuire quanto sia intricato e
delicato il rapporto fra realtà empirica e descrizione matematica.
Noi le interrompiamo qui e torniamo al tema principale.
Si è già visto che, per quanto riguarda la determinazione di una
legge della crescita di una popolazione, si deve ricorrere alle
analisi empiriche già esistenti. Una delle prime conclusioni tratte
dall’analisi empirica dei dati noti risale a molto tempo fa ed è
basata sulle idee dell’economista inglese Thomas Malthus (1766-
1834) esposte nel famoso An Essay on the Principle of Population
(Saggio sul principio della popolazione) pubblicato nel 1798.
Dell’analisi di Malthus fu comunemente accettata l’idea che, in
assenza di vincoli esterni (limitatezza delle risorse ecc.), la
popolazione umana (come ogni popolazione animale) tende, per
sua natura, a una crescita direttamente proporzionale al numero
della popolazione stessa e pertanto illimitata.
Ricordiamo, di passaggio, che questa convinzione indusse
Malthus a formulare conclusioni pessimistiche circa il futuro
della popolazione umana, il cui sviluppo sarebbe presto entrato in
conflitto con la limitatezza delle risorse disponibili sulla Terra.

Queste conclusioni pessimistiche furono duramente attaccate da
molti economisti e in particolare da Karl Marx (1818-1883). Le
critiche rivolte a Malthus si basavano sul presupposto ideologico
che il progresso tecnologico e industriale, in quanto fattore
moltiplicativo delle risorse, avrebbe permesso di far fronte senza
difficoltà a una crescita demografica praticamente illimitata. Oggi
tali orgogliose certezze sono crollate. Difatti, ai tempi nostri, il
tema dei “limiti della crescita” ha assunto una rilevanza centrale
e persino angosciante.
Per tradurre nel linguaggio matematico l’idea della tendenza di
una popolazione a crescere senza limiti, dobbiamo introdurre
alcuni concetti e definizioni abbastanza semplici. Dovremo quindi
scrivere qualche formula, ma cercheremo di usare soltanto
nozioni elementari, da tutti apprese a scuola.
Finora abbiamo utilizzato l’anno come unità di misura del
tempo. Se ne possono beninteso utilizzare altre: il mese, il giorno,
il secondo, il decennio, il secolo ecc. Per non limitare le nostre
considerazioni, faremo riferimento a un’unità di misura di tempo
qualsiasi (da specificare caso per caso) che denoteremo con il
simbolo Δt. Quindi Δt denota un intervallo di tempo. Vogliamo ora
introdurre in termini matematici il concetto di velocità con cui
varia la popolazione nell’unità di tempo Δt. Per far ciò basterà
confrontare la variazione della popolazione con l’intervallo di
tempo considerato: a parità d’intervallo di tempo questa velocità
è tanto più grande quanto più è grande la variazione della
popolazione. Come si indica la “variazione della popolazione
nell’intervallo di tempo Δt”? Se il numero degli individui che la
compongono all’istante t è, come al solito, N(t), dopo che sarà
trascorso un intervallo di tempo Δt, e cioè all’istante t + Δt, esso

diverrà N(t + Δt). Pertanto, la variazione della popolazione sarà
data dalla differenza fra la popolazione finale, N(t + Δt), e quella
iniziale, N(t): sarà cioè N(t + Δt) – N(t), e la denoteremo col simbolo
ΔN. Cioè:
ΔN = N(t + Δt) – N(t).
Ora, per ottenere la misura della variazione (crescita,
diminuzione o stasi) della popolazione nell’unità di tempo Δt,
abbiamo detto che si dovrà confrontare l’incremento Δt della
popolazione con l’intervallo di tempo trascorso Δt. Confrontare
significa, in questo caso, “rapportare a” o, come si dice nel
linguaggio 
comune, 
“fare 
il 
rapporto”. 
Dovremo 
quindi
considerare il “rapporto”
che chiameremo “velocità di variazione di N(t) nell’intervallo di
tempo” considerato.
È facile constatare che, fissato Δt, questa velocità è tanto più
grande quanto è maggiore ΔN e tanto più piccola quanto più è
piccola la variazione ΔN, come è del tutto naturale.
Il concetto introdotto fornisce una misura della velocità di
variazione della popolazione. Ma esso non è la stessa cosa del
concetto di solito usato quando si parla di “percentuali” e si dice,
per esempio: la popolazione del tal paese è cresciuta nel 1984 del
7% (il che vuol dire che è cresciuta di 7 individui ogni 100). Questo
concetto di percentuale non è privo di connessioni con il
precedente: anzi, in certo senso, lo contiene. Esso si esprimerà

confrontando 
l’incremento 
della 
popolazione 
(nell’esempio
precedente, 7) con la popolazione iniziale (sempre nel nostro
esempio, 100) e sarà quindi dato dal rapporto 7/100 (che si
esprime di solito parlando di un incremento del 7%). Bisogna però
considerare anche l’intervallo di tempo scelto come unità di
misura. Infatti se, anziché scegliere l’anno come unità di misura,
scegliamo per esempio il semestre, la percentuale d’incremento
dovrà essere divisa per 2 (e sarà perciò 7/200, cioè il 3,5%); se
scegliamo il mese, dovrà essere diviso per 12, e così via.
Come si esprime nei nostri simboli questa “variazione
percentuale della popolazione nell’unità di tempo”? L’incremento
della popolazione – lo ricordiamo – è ΔN, la popolazione all’inizio
del periodo considerato è N(t): occorrerà quindi considerare il
rapporto fra queste due quantità, rapportato però a sua volta
all’intervallo di tempo preso in esame. Si tratterà quindi di
considerare 
la 
quantità: 
Appare chiaro anche il legame di questa nozione con la
precedente: qui si considera ancora la velocità di variazione della
popolazione, rapportata però alla numerosità della popolazione
iniziale. È quindi una nozione più significativa sul piano
descrittivo. ΔN/N(t)·Δt è detto tasso di crescita della popolazione
nell’intervallo di tempo Δt. Si noti, di passaggio, che se Δt = 1, cioè
se Δt è l’intervallo di tempo scelto come unità di misura, il
rapporto si riduce a ΔN/N(t).
Ora, il nostro problema si traduce nel fare delle ipotesi sul tasso

di crescita della popolazione. Se, per esempio, diciamo che il tasso
di crescita della popolazione è 2, nell’unità di tempo (per esempio
nell’anno), ciò significa che la popolazione cresce al ritmo del 2%
nell’unità di tempo e quindi tende a un’espansione illimitata. Ed è
chiaro che questa espansione illimitata si verificherà sempre se il
tasso di crescita è costante e positivo; non importa che sia 2, 15 o
75. La diversità delle percentuali influisce sulla velocità della
crescita ma la crescita stessa è comunque illimitata. Viceversa, se
il tasso di crescita è zero, cioè ΔN/N(t)·Δt = 0, ne segue che ΔN = 0,
cioè la popolazione si mantiene costante (il numero delle nascite
eguaglia il numero delle morti). Se è negativo, la popolazione
decresce illimitatamente, fino a sparire.
Prima di procedere oltre in questo genere di considerazioni,
vogliamo perfezionare ulteriormente il nostro concetto di tasso di
crescita. Esso infatti esprime il cambiamento percentuale medio
nel periodo di tempo considerato. In altri termini, se il periodo
considerato è l’anno e la crescita è del 7%, ciò significa che nel
corso dell’anno per ogni 100 abitanti vi è stato un incremento
(come differenza fra nati e morti) di 7 individui. Non sappiamo se
tale incremento sia avvenuto tutto nel primo mese o, poniamo,
negli ultimi dieci giorni dell’anno. Si tratta di qualcosa di analogo
al concetto di “velocità media”: se, per percorrere in automobile i
400 chilometri che separano Roma da Bologna, abbiamo
impiegato 8 ore, diremo che la nostra velocità media è stata di 50
chilometri all’ora. È possibile però che determinati tratti siano
stati percorsi a velocità folle e altri a passo di lumaca o a velocità
nulla (nel caso di soste). A tale concetto si oppone quello di
“velocità 
istantanea”, 
che 
è 
misurata 
dal 
tachimetro
dell’autovettura, il quale offre al nostro occhio in modo continuo,

cioè istante per istante, una misura della velocità. Come si misura
la velocità? Lo si è visto nell’esempio precedente: si divide, si
rapporta, lo spazio percorso per il tempo impiegato (400 diviso per
8). Si capisce subito che, se facessimo questa operazione più
spesso (per esempio dividendo ogni chilometro percorso per il
tempo impiegato a percorrerlo) otterremmo pur sempre la misura
di una velocità media, diversa dalla velocità istantanea, ma
sicuramente molto più vicina a questa che non la velocità media
calcolata su un periodo molto più lungo. In definitiva, considerare
intervalli di tempo sempre più piccoli ci avvicina a una misura
della velocità istantanea. Perciò siamo indotti a pensare che il
tachimetro della nostra auto si comporti come se fosse capace di
calcolare la velocità istante per istante, e cioè per intervalli
piccolissimi di tempo; come se fosse capace di eseguire istante
per istante (o meglio, per intervalli piccolissimi) quella divisione
fra spazio percorso e tempo impiegato.
Insomma, quel che vorremmo è che, nella definizione della
velocità di variazione di una popolazione, la quantità Δt fosse
piccolissima, “piccola a piacere”, come dicono i matematici. Forse
qualche lettore si è già reso conto che stiamo tentando di
introdurre, con tutta la rozzezza inevitabile, il concetto
matematico di derivata. Ci limiteremo a dire che è possibile
considerare un rapporto, ΔN/Δt, in cui Δt è “piccolo quanto si
vuole” (ovvero, come si dice, Δt “tende a zero”) e che questa
operazione è chiamata appunto “derivazione”.
La definizione soddisfacente di questa operazione è il frutto di
lunghissimi sforzi durati almeno due secoli ed è una delle
conquiste più significative del pensiero matematico. Qui basterà
dire che si può ottenere una definizione matematica del concetto

di velocità di variazione istantanea. Per far ciò la quantità ΔN/Δt
viene trasformata in una nuova quantità (detta derivata) che
denotiamo col simbolo N˙(t) (o dN/dt).
Potremo allora anche parlare di “tasso di crescita istantaneo”
della popolazione: invece della quantità ΔN/N(t)·Δt considereremo
(sostituendo ΔN/N(t)·Δt con la derivata N˙(t)) la quantità N˙(t)/N(t).
In conclusione, N˙(t) esprime la “velocità istantanea di
variazione della popolazione”. N˙(t)/N(t), che è il rapporto fra tale
velocità e la popolazione nell’istante considerato, esprime invece
un “tasso istantaneo” (variazione percentuale misurata istante
per istante).
Si può però fare una seria obiezione alla nostra definizione. Se
si rende Δt sempre più piccolo, in linea generale anche ΔN diverrà
sempre 
più 
piccolo. 
Le 
eccezioni 
a 
questa 
tendenza
corrispondono a casi in cui si verificano incrementi istantanei
“esplosivi” della popolazione, eventi questi alquanto irrealistici.
Ora, è ragionevole pensare a intervalli di tempo e di spazio
piccolissimi: siamo infatti abituati a considerare la possibilità di
dividere indefinitamente gli intervalli di tempo e di spazio
ottenendo intervalli piccoli “quanto si vuole”. Quindi il concetto di
velocità istantanea (che è basato su questa idea della divisibilità
all’infinito dello spazio e del tempo) è piuttosto spontaneo e
accettabile. Ma è assai più difficile accettare l’idea di considerare
degli incrementi di popolazione ΔN piccolissimi. Come pensare a
2,3333 o a 0,0001 individui? Eppure bisogna accettare idee del
genere, se si vuole introdurre il concetto di tasso di crescita
istantaneo.
Il numero di individui di una popolazione è una grandezza che,

nella realtà, varia soltanto per numeri interi. Infatti è privo di
senso dire che una popolazione cresce di una frazione di
individuo. Tuttavia occorre considerare questa grandezza (la
popolazione) come se fosse soggetta a una “crescita continua” e
cioè come se la crescita avvenisse non saltando da un intero a un
altro (da 1 a 2, da 2 a 3 ecc.), come in realtà avviene, ma come se
passasse attraverso tutti i numeri compresi fra ogni coppia di
interi successivi. Occorrerà certo, quando si utilizzano i risultati
numerici dal punto di vista empirico, approssimarli alla loro parte
intera (2,3333 diventerà 2 e 0,0001 diventerà 0). Ma non è questa la
difficoltà. Il punto è che la nostra rappresentazione matematica
(con numeri di ogni tipo) avrà senso soltanto se le condizioni
specifiche del problema lo consentiranno. Per esempio, se la
popolazione è molto numerosa. Ma anche in altri casi, purché si
abbiano buoni motivi per ritenere che il processo avvenga con
caratteristiche sufficienti di continuità, cioè senza salti.
Riprendiamo il cammino accidentato verso la formulazione di
una legge matematica della variazione di una popolazione (o,
come si dice, della “dinamica di una popolazione”) tornando a
Malthus. Ebbene, l’“ipotesi malthusiana” consiste nell’affermare,
sulla base dell’interpretazione dei dati noti, che la popolazione
umana è in crescita e che il suo tasso di crescita è costante; un
numero positivo quindi, che denoteremo con k.
Potremo quindi scrivere la legge di crescita malthusiana di una
popolazione 
al 
modo 
seguente: 

La prima scrittura dice che il tasso di crescita istantaneo della
popolazione è costante. La seconda evidenzia il fatto che la
velocità istantanea di crescita della popolazione è proporzionale
alla popolazione stessa: quanto più numerosa è la popolazione,
tanto più veloce è la sua crescita.
La formula N˙(t) = kN(t) è un esempio di ciò che va
comunemente sotto il nome di “equazione differenziale”. Essa
non fornisce esplicitamente la legge di crescita della popolazione,
non fornisce cioè esplicitamente una formula che dica come varia
N(t) in funzione del tempo. Stabilisce piuttosto un legame fra N(t)
(che è proprio la funzione di cui vorremmo conoscere la forma) e
la sua derivata N˙(t). La ragione per cui questa equazione è
chiamata differenziale è legata al fatto che la derivata in una delle
prime definizioni (dovuta a Leibniz) veniva introdotta come il
quoziente 
di 
due 
quantità 
“infinitamente 
piccole” 
dette
“differenziali”.
Tutto ciò è quel che accade di solito. In generale è difficilissimo
(e per lo più impossibile) ottenere subito in modo esplicito la
formula che fornisce la legge cercata. Per lo più, dall’analisi
sperimentale o da ragionevoli induzioni di carattere empirico, si
ottengono delle relazioni fra le grandezze meno difficilmente
rilevabili. Nel nostro caso una grandezza del genere è il tasso di
crescita e la ragionevole induzione di carattere empirico (dovuta a
Malthus) è che tale grandezza sia costante. Le relazioni ottenute
possono assumere, come accade a noi, la forma di equazioni
differenziali. Si tratta allora di risolverle. Ciò significa, nel nostro
caso, ricavare dalla relazione N˙(t) = kN(t) l’espressione esplicita di
N(t). È questo un problema strettamente matematico, come
quello, che s’impara ad affrontare a scuola, di trovare le radici di

un’equazione di secondo grado o di altre equazioni. È soltanto più
difficile, spesso difficilissimo e spesso anche impossibile.
A questo punto del nostro percorso dobbiamo quindi cedere il
passo alla matematica, al puro calcolo, che dovrà fornirci la
risposta cercata. Ma questo è un compito che non rientra nei fini
e nei limiti di questo libro. A noi basta sapere che esiste una
teoria, un apparato di calcolo apposito, che interviene a questo
stadio; anche se, è bene sottolinearlo, non si tratta di una chiave
che apre tutte le porte!
Nel nostro caso specifico il problema è assai facile per il
matematico. L’espressione esplicita di N(t) è data dall’“equazione
di crescita esponenziale”:
N(t) = N(0)ekt.
Nel secondo membro di questa equazione compare N(0), cioè il
numero di individui della popolazione all’istante iniziale t = 0.
Compare inoltre la funzione ekt che va sotto il nome di “funzione
esponenziale”, perché si ottiene elevando alla potenza kt il
numero e = 2,7182818. Questo e è un numero importantissimo in
matematica perché è la base dei logaritmi naturali. Infine, k è,
come al solito, il tasso di crescita della popolazione.
Formule a parte, il grafico della funzione N(t) = N(0)ekt offre
un’efficace espressione visiva della legge di crescita malthusiana
(detta anche “esponenziale”, perché nella sua espressione
matematica interviene la funzione esponenziale). Naturalmente
per disegnare questo grafico dobbiamo conoscere N(0) e il tasso di
crescita k, entrambi dati empirici, che dobbiamo ricavare
dall’analisi della realtà.

Nella FIG. 1.5 abbiamo riportato sulle ascisse i tempi e sulle
ordinate il numero di individui della popolazione, e abbiamo
tracciato alcune “curve di crescita esponenziale” corrispondenti a
valori diversi di N(0) e di k.
Il tasso di crescita k influenza la velocità della crescita e si
riflette quindi sulla maggiore o minore pendenza della curva. Ma,
quali che sia-no i valori di N(0) e di k, la popolazione N(t) cresce
indefinitamente e tende anzi a valori indefinitamente grandi.
Giunti a questo punto, abbiamo ottenuto la legge, che può
essere utilizzata per predire il fenomeno, a condizione di
conoscere due dati empirici: il tasso di crescita k e la popolazione
iniziale N(0). Si tratta allora di confrontare le previsioni che essa
fornisce con i dati in nostro possesso, per verificare, nei limiti del
possibile, se essa sia attendibile.
Nel tentare questo confronto svilupperemo esplicitamente
alcuni calcoli per permettere al lettore di seguire gli sviluppi che
portano alle conclusioni. La TAB. 1.3 riporta i dati statistici relativi
alla popolazione degli Stati Uniti negli anni 1790, 1800 e 1810.
Scelto come istante iniziale (t = 0) l’anno 1790 e come intervallo
unitario il decennio (per cui t = 1 corrisponde all’anno 1800 e t = 2
al 1810), si ha che N(0) = 3.929.000 e N(1) = 5.308.000. La nostra
legge dice che N(t) = N(0)ekt e quindi, siccome essa deve valere ad
ogni istante t, per t = 1 (cioè nel 1800) si ha:

FIGURA 1.5 Curve di crescita malthusiana, per diversi valori di
N(0) e di k
TABELLA 1.3 Popolazione degli USA negli anni 1790, 1800 e 1810
Anno
Popolazione USA
1790
3.929.000
1800
5.308.000
1810
7.240.000
N(1) = N(0)ek 1 = N(0)ek.
Sostituendo i valori, si ottiene:

Con l’ausilio di una calcolatrice che possieda le funzioni
cosiddette elementari (fra cui, esponenziale e logaritmo) è facile
ricavare k che risulta essere 0,3008 e di cui possiamo considerare
il valore approssimato 0,301.
In definitiva, mediante i dati statistici relativi al 1790 e al 1800
abbiamo ricavato il tasso di crescita reale della popolazione degli
Stati Uniti relativo a quel decennio. Esso è k = 0,301 (30,1% nel
decennio, ovvero un incremento della popolazione del 3,01%
l’anno). Se è vera l’ipotesi che il tasso di crescita della popolazione
è costante, ciò significa che ad ogni istante di tempo la legge di
crescita della popolazione è data dall’equazione:
N(t) = N(0)ekt = 3.929.000 e0,301 t
la quale ci consente di calcolare N(t), per ogni t. Per esempio,
possiamo calcolare N(2), e cioè la popolazione degli Stati Uniti nel
1810, e confrontare il valore ottenuto con quello reale. Si ha
(approssimando alle migliaia):
N(2) = N(0)ek 2 = 3.929.000 e0,301·2 = 7.173.000.
L’accordo con il valore reale riportato nella TAB. 1.3 è eccellente,
soprattutto se si tiene conto che questo genere di previsione è
necessariamente alquanto grossolano. Prima di esultare troppo
per il nostro successo, proviamo però a istituire altri confronti.
Consideriamo la TAB. 1.4 che riporta i dati della popolazione degli
Stati Uniti dal 1790 al 1950, calcoliamo con la nostra formula tutti
i dati previsti, assumendo come tasso di crescita sempre 0,301. Il

lettore può divertirsi a ottenere i risultati da solo con una
calcolatrice.
L’analisi della TAB. 1.4 mostra che l’accordo fra i dati rilevati
dalle statistiche e i dati calcolati è ottimo fino al 1820 e più che
soddisfacente fino al 1860. A partire da questa data la deviazione
comincia a essere grave, per diventare veramente insanabile e
addirittura grottesca alla fine della tabella. La nostra legge
prevede per il 1950 una popolazione negli Stati Uniti di quasi
mezzo miliardo! E noi sappiamo che oggigiorno essa è ancora
molto al di sotto di tale cifra.
Dobbiamo quindi gettare nel cestino la legge di crescita
malthusiana e ricominciare daccapo? Il fatto è che ci sono seri
motivi per dubitare. Nel periodo 1790-1850 la legge funziona bene,
se non egregiamente, e ciò indica almeno che Malthus aveva
qualche ragione per ritenerla valida, sulla base dei dati dei suoi
tempi. Ma dopo quegli anni le cose vanno male e il disaccordo è
talmente grave da non poter essere imputato alla trascuranza di
eventi come le guerre o a fattori esterni come l’immigrazione (che
anzi dovrebbe spingere gli scarti in senso opposto!).
In conclusione, se non vogliamo cedere ad atteggiamenti
passionali e vogliamo trarre qualche insegnamento dai primi
successi e insuccessi della nostra legge (o “modello matematico
della crescita di una popolazione”), dobbiamo annotare che essa
si accorda bene con la tendenza reale all’inizio e poi ne devia
sempre più gravemente, perché predice una crescita troppo
veloce rispetto a quella che si è verificata nella realtà. Quindi il
tasso di crescita è pressoché costante in una prima fase e poi
diviene sempre più piccolo col crescere della popolazione.

TABELLA 1.4 Popolazione degli USA nel periodo 1790-1950 e dati
calcolati mediante la legge di crescita malthusiana (k = 0,301)
 
Anno
Dati statistici
Dati calcolati
Errore
% errore
t = 0
1790
3.929.000
3.929.000
0
0,0
t = 1
1800
5.308.000
5.308.000
0
0,0
t = 2
1810
7.240.000
7.173.000
–67.000
–0,9
t = 3
1820
9.638.000
9.693.000
55.000
0,5
t = 4
1830
12.866.000
13.097.000
231.000
1,8
t = 5
1840
17.069.000
17.697.000
628.000
2,0
t = 6
1850
23.192.000
23.912.000
720.000
2,3
t = 7
1860
31.443.000
32.310.000
867.000
2,8
t = 8
1870
38.558.000
43.658.000
5.100.000
13,2
t = 9
1880
50.156.000
58.991.000
8.835.000
17,6
t = 10
1890
62.948.000
79.709.000
16.761.000
21,0
t = 11
1900
75.995.000
107.704.000
31.702.000
41,7
t = 12
1910
91.972.000
145.530.000
53.558.000
58,2
t = 13
1920
105.711.000
196.642.000
90.931.000
86,0
t = 14
1930
122.775.000
265.705.000
142.930.000
116,4
t = 15
1940
131.669.000
359.002.000
227.333.000
172,6

t = 16
1950
150.697.000
485.114.000
334.417.000
221,9
L’esame di altri dati sembra confermare questa conclusione.
Senza ingombrare troppo il nostro discorso di altre tabelle, ci
limitiamo a riassumere alcuni dati riguardanti lo sviluppo della
popolazione mondiale. È facile calcolare il tasso di crescita della
popolazione mondiale negli anni Sessanta del nostro secolo: esso
è con buona approssimazione 0,02 assumendo l’anno quale unità
di misura, e cioè del 2% annuo. Ebbene, se si calcola, con questo
tasso, la crescita della popolazione mondiale a partire degli inizi
del Settecento e si confrontano i numeri ottenuti con le stime
note, si constata un accordo eccellente. Tale accordo diventa
sempre meno buono man mano che si va avanti. Infatti
assumendo come anno iniziale il 1961, nel quale la popolazione
mondiale era di 3 miliardi e 60 milioni di individui, e assumendo
come tasso di crescita sempre il 2% annuo (che è proprio quello
rilevato in quel periodo), scriveremo la legge esponenziale così:
TABELLA 1.5 Stime della popolazione mondiale mediante la
legge di crescita malthusiana (t(0) = 1961, k = 0,02)
Anno
Popolazione mondiale prevista
2000
6.675.305.132
2100
49.324.204.000
2200
364.459.310.000
2500
147.033.380.000.000
3000
3.238.625.700.000.000.000

N(t) = 3.060.000.000 e0,02 t.
Secondo questa legge, tenuto conto che l’unità di misura è
l’anno, il numero di abitanti del globo nel 2000 è previsto dalla
legge nel numero di:
N(39) = 3.060.000.000 e0,02·39 = 6.675.305.132.
Questa previsione è assai vicina al vero e quindi ci conforta
circa l’attendibilità del nostro modello. Ma se calcoliamo il
numero previsto di abitanti del globo nel 2100, nel 2200, nel 2500 e
nel 3000 otteniamo le previsioni riportate nella TAB. 1.5, che sono
sempre meno realistiche.
Nel 3000 saremmo più di 3 miliardi di miliardi! Ma già nel 2500
saremmo in tanti da dover stare in piedi l’uno accanto all’altro,
includendo 
nella 
superficie 
terrestre 
anche 
le 
acque
(opportunamente ricoperte)! Tutto ciò è ovviamente assurdo ed è
altrettanto ovvio che prima di giungere a ciò interverrebbero
“fattori correttivi”, come la morte per fame, stragi di massa e così
via. Non si tratterrebbe proprio della catastrofe prevista da
Malthus?
TABELLA 1.6 Popolazione italiana nel periodo 1971-81 e dati calcolati
mediante la legge di crescita malthusiana (t(0) = 1971, k = 0,006)
Anno
Dati statistici
Dati calcolati
% di errore
1971
54.025.000
54.025.000
0,0
1972
54.345.000
54.345.000
0,0
1973
55.154.000
54.677.000
–0,8
1974
–
55.006.000
–

1975
55.613.000
55.337.000
–0,5
1976
56.014.000
55.670.000
–0,6
1977
56.325.000
56.005.000
–0,6
1978
56.600.000
56.342.000
–0,5
1979
56.828.000
56.681.000
–0,2
1980
56.999.000
57.366.000
–0,04
1981
57.198.000
57.366.000
0,3
Prima di procedere oltre in questo genere di considerazioni,
studiamo ancora altri dati in nostro possesso. In particolare,
riesaminiamo la TAB. 1.2 relativa alla crescita della popolazione
italiana. Scegliamo ancora l’anno come unità di misura, il 1971
come istante iniziale e il tasso di crescita che si ricava dai dati
relativi al periodo 1971-72 (esso è 0,006, cioè una crescita del 0,6%
annuo). Con questi dati calcoliamo la popolazione e nella TAB. 1.6
mettiamo a confronto i nostri risultati con i dati statistici.
In questo caso l’accordo sembra eccellente: anzi la legge
malthusiana si allontana dai dati reali per difetto! Osservando
però i dati del periodo 1972-73 sorge qualche dubbio. Calcoliamo
allora il tasso di crescita e ci rendiamo conto che esso è 0,0147,
cioè del 1,47% annuo, ben maggiore di quello relativo al periodo
1971-72. Costruiamo allora un’altra tabella di confronto, la TAB.
1.7, sulla base di questo dato, partendo dal 1972.
Questa volta l’accordo non è così buono e anzi peggiora con il

passare degli anni. Se proseguiamo i calcoli, otteniamo infatti
65.850.000 individui per il 1985 e previsioni di 70.904.000 individui
per il 1990 e di 82.195.000 per il 2000. Ritroviamo così la solita
tendenza e le solite deviazioni sul lungo periodo (confermate dai
calcoli su periodi futuri che forniscono i consueti risultati
catastrofici).
TABELLA 1.7 Popolazione italiana nel periodo 1972-81 e dati calcolati
mediante la legge di crescita malthusiana (t(0) = 1972, k = 0,0147)
Anno
Dati statistici
Dati calcolati
% di errore
1972
54.345.000
54.345.000
0,0
1973
55.154.000
55.154.000
0,0
1974
–
55.975.000
–
1975
55.613.000
56.808.000
2,1
1976
56.014.000
57.654.000
2,9
1977
56.325.000
58.512.000
3,9
1978
56.600.000
59.383.000
4,9
1979
56.828.000
60.267.000
6,0
1980
56.999.000
61.164.000
7,3
1981
57.198.000
62.075.000
8,5
La TAB. 1.7 è basata su una scelta più realistica del tasso di
crescita, e per questo sembrerebbe più accettabile. Probabilmente

il tasso di crescita ricavato dal periodo 1971-72, pur fornendo
previsioni 
migliori, 
ha 
un 
carattere 
anomalo 
(forse 
per
l’intervento di fattori esterni o addirittura per una cattiva stima
statistica). È possibile in definitiva dire quale delle due tabelle sia
la migliore? La risposta è incerta. La TAB. 1.6 appare come la
migliore nel breve periodo in esame, la TAB. 1.7 è forse basata sulla
scelta di un tasso di crescita più realistico. Comunque sia,
emergono due considerazioni: la prima (ovvia) è che il tasso di
crescita ancora una volta non resta affatto costante; la seconda è
che, anche nel caso di una crescita più lenta, la tendenza predetta
dalla legge malthusiana è “esplosiva” e cioè è una tendenza verso
numeri grandissimi, in definitiva talmente grandi da essere privi
di qualsiasi verisimiglianza.
Da 
tutte 
le 
considerazioni 
precedenti 
emerge 
l’aspetto
caratteristico della legge di crescita esponenziale: la dinamica
descritta da questa legge non è influenzata da alcun elemento
esterno. Si tratta della pura dina mica interna della crescita di
una popolazione. Spieghiamoci meglio: la costanza del tasso di
crescita riflette una sorta di potenzialità di crescita pura; esprime
un fattore biologico isolato e cioè la capacità riproduttiva della
popolazione. Non intervengono né aspetti esterni (quali la
limitazione del cibo, delle risorse ambientali, l’inquinamento
ecc.), 
né 
aspetti 
sociali 
(limiti 
indotti 
dalle 
forme 
di
organizzazione 
sociale, 
vincoli 
economici, 
scelte 
di
autolimitazione ecc.). È come se la legge malthusiana esprimesse
in formula questa idea: in assenza di limiti e coercizioni, di
qualsiasi natura, la potenzialità riproduttiva della specie è una
caratteristica biologica invariabile, espressa, per l’appunto, da una
costante, da un numero positivo. La specie, lasciata a se stessa,

sotto l’impulso della sola spinta riproduttiva, cresce a un tasso
costante.
Sappiamo che nella realtà le cose non vanno così e che
numerosi fattori ostacolano la crescita: carenza di cibo e risorse,
guerre, epidemie, vincoli o scelte di tipo sociale, religioso,
ideologico ecc. Tuttavia, se noi riuscissimo a verificare, per così
dire in laboratorio, che in assenza di questi vincoli o “attriti” la
capacità riproduttiva della specie è proprio quella descritta dalla
legge esponenziale, avremmo fatto un notevole passo in avanti.
Avremmo infatti spiegato perché la legge malthusiana funziona
per valori non grandi della popolazione e diventa irrealistica per
grandi valori. Nel primo caso infatti l’influsso dei vincoli esterni è
poco sensibile, mentre nel secondo caso esso diventa coercitivo
sulla crescita.
In realtà, i dati già esaminati si accordano proprio con questa
interpretazione. La legge di crescita malthusiana funziona quanto
più ci si riferisce a contesti in cui lo sviluppo della popolazione
non subisce intralci. Si è visto come, nel passato, fosse diffusa la
credenza secondo cui la natura era in grado di offrire mezzi di
sostentamento praticamente inesauribili. Inoltre erano più forti e
diffuse le convinzioni religiose e sociali sintetizzate nel precetto
biblico: «crescete e moltiplicatevi». Indubbiamente, nella storia
umana, non sono mai mancati fenomeni di rilievo come grandi
epidemie, carestie o guerre. Ma è del tutto lecito considerarli
come perturbazioni della tendenza generale ben distinti da essa e
quindi come eccezioni che confermano la regola. La situazione è
radicalmente diversa in tempi in cui, come quelli odierni, gli
atteggiamenti diffusi nei confronti del tema dello sviluppo (e i
conseguenti comportamenti pratici) sono profondamente mutati,

almeno nei paesi sviluppati.
Facciamo un’ulteriore osservazione. Se l’ipotesi di un tasso di
crescita costante descrive matematicamente la pura capacità
riproduttiva della popolazione, perché tale tipo di descrizione
dovrebbe essere riservato alla specie umana? Perché non
potrebbe essere esteso a ogni specie vivente con caratteristiche
analoghe nel processo riproduttivo? In altri termini, tutti i
processi di variazione del numero di una popolazione (con
caratteristiche riproduttive non troppo dissimili da quelle umane)
dovrebbero essere espressi da una legge analoga. Nei casi in cui la
descrizione malthusiana è applicabile v’è una sola condizione da
rispettare. Bisognerà scegliere bene per ogni specie il relativo e
caratteristico coefficiente numerico (il tasso di crescita) che
descrive la sua specifica potenzialità riproduttiva.
Questa ipotesi è infatti comunemente accettata e la legge
malthusiana è usata per descrivere la crescita di una popolazione
qualsiasi di esseri viventi, sia pure con diverse eccezioni, come ad
esempio la riproduzione dei batteri, che è governata da
meccanismi assai diversi da quelli caratteristici della specie
umana e della maggior parte delle specie animali. Ciò allarga
enormemente il campo delle nostre considerazioni e delle nostre
verifiche. È più facile infatti servirsi degli animali per ideare un
esperimento che permetta di verificare in laboratorio la validità
della legge malthusiana in assenza dei vincoli o attriti di cui
abbiamo parlato. Per esempio si può studiare la velocità di
crescita di una specie di animali allevati in laboratorio e a cui sia
stato garantito cibo illimitato e condizioni ambientali ottimali.
Esperimenti del genere (o rilevazioni statistiche relative a
fenomeni facilmente osservabili) sono stati eseguiti in gran

numero.
Riportiamo i dati relativi a uno di essi, tratti da Braun (1975): si
tratta della crescita di un piccolo roditore, il Microtus Arvallis. La
popolazione era composta all’inizio (t = 0) di due individui e
l’unità di misura scelta era il mese; il tasso di crescita stimato per
questa specie è del 47% mensile, per cui l’equazione della crescita
è:
N˙(t) = 0,4N(t) da cui N(t) = 2e0,4t.
La TAB. 1.8 riporta il confronto fra i dati reali e quelli calcolati
con la legge di crescita malthusiana: l’accordo è eccellente.
I dati di questi esperimenti confermano il nostro ragionamento.
La legge di crescita esponenziale sembra essere una buona legge
in condizioni ambientali ottimali; non va più bene quando non
sono trascurabili i fattori di freno esterni. E tali fattori appaiono
agire prima del prodursi della catastrofe di uno sviluppo
esplosivo, come suggeriscono i dati relativi alla popolazione
mondiale umana, nonché numerosi altri dati noti. Per cui,
anziché respingere la legge malthusiana in blocco o accettarne
ciecamente le implicazioni catastrofiche, sembra più corretto
muovere alla ricerca di una legge descrittiva più raffinata, che
perfezioni la legge malthusiana includendo nella descrizione
anche quei fenomeni capaci di frenare la crescita illimitata.
TABELLA 1.8 Crescita del Microtus Arvallis
Mesi
Numero di roditori osservato Numero di roditori calcolato
0
2
2,0
2
5
4,5

6
20
22,0
10
109
109,1
Un modello più perfezionato: la legge di crescita logistica
Una modifica della legge malthusiana al fine di correggerne la
tendenza irrealisticamente esplosiva fu elaborata nel 1837 dal
biologo matematico olandese Pierre François Verhulst (1804-1849).
L’interesse attorno a questa modifica si manifestò, però, solo
molto tempo dopo, negli anni Venti del nostro secolo, ad opera
degli statistici americani Raymond Pearl (1879-1940) e Lowell J.
Reed (1886-1966).
La modifica si basa sull’abbandono dell’ipotesi che il tasso di
crescita sia costante e sull’introduzione di un tasso di crescita
variabile. In tal modo è rappresentata l’esistenza di vincoli esterni
che frenano la crescita della popolazione. Per far ciò la costante k
del tasso di crescita malthusiano viene corretta sottraendole una
quantità che cresce al crescere della popolazione: così, quanto più
numerosa è la popolazione, tanto più piccolo è il tasso di crescita.
Basta sostituire a k il termine k – hN(t), dove h è una costante
positiva. Ciò è quanto dire che, al posto dell’equazione 
consideriamo la nuova equazione: 

È evidente che, quando N(t) è un numero piccolo, il termine
negativo – hN(t) modifica di poco k e quindi influisce poco sulla
crescita. Ma, al crescere di N(t) (e N(t) crescerà rapidamente, come
sappiamo) il termine – hN(t) diventa sempre più grande,
effettuando una correzione sempre più marcata del tasso di
crescita. Anzi, è chiaro che a un certo punto il tasso di crescita k –
hN(t) diventerà zero. Tasso di crescita nullo significa crescita zero,
significa che la popolazione non cresce più, che essa ha raggiunto
un tetto, un limite oltre il quale non può più aumentare.
Qual è questo tetto, questa popolazione limite? Abbiamo visto che
la crescita si arresta quando k – hN(t) = 0, cioè hN(t) = k. Ciò vuol
dire che la velocità di variazione della popolazione è nulla e
quindi che essa non aumenta più e resta stazionaria. Ebbene,
essendo h e k costanti e poiché varia soltanto N(t), ne segue che
hN(t) sarà uguale a k quando N(t) raggiunge il valore k/h (cioè N(t)
= k/h).
Abbiamo così ottenuto un nuovo modello della crescita di una
popolazione che sembra rispondere alle nostre esigenze. Questo
modello è descritto dall’equazione:
N˙(t) = (k – hN(t))N(t).
La crescita non è più illimitata, ma ha un tetto, non può
superare la popolazione limite data dal valore numerico k/h.
La soluzione dell’equazione precedente, detta anche equazione
di crescita logistica, è elementare per un matematico ma troppo
difficile per questo libro. Limitiamoci a dire che, mentre la
soluzione dell’equazione di crescita esponenziale è N(t) = N(0)ekt,
la soluzione di quella logistica è sempre di questa forma, ma al

posto di ekt compare una espressione più complicata in cui è
presente non soltanto ekt ma anche la costante h. Per cui,
conoscendo N(0), k e h, è possibile determinare anche
numericamente la crescita della popolazione, eseguendo calcoli
analoghi a quelli sviluppati nel paragrafo precedente.
Per non complicare le cose non scriveremo la formula che
esprime N(t) in questo caso. Possiamo però dare un’informazione
più intuitiva e cioè rappresentarne il grafico. Esso è dato da una
curva caratteristica (detta anche “curva a S”), come quella
rappresentata nella FIG. 1.6. Si vede bene che N(t) cresce a partire
dal valore N(0) sempre più rapidamente fino al valore k/2h, poi
rallenta la crescita e tende infine verso il valore della popolazione
limite, k/h, che non può superare.
FIGURA 1.6 Curva di crescita logistica

Per confrontare ora il nostro nuovo modello con i dati empirici
occorrerà fare dei calcoli numerici e quindi fissare i valori di h e k
caso per caso. Qui nascono le difficoltà. Infatti, il problema di
come si possa fissare il valore numerico della costante h (che
“corregge” la crescita della popolazione) è alquanto difficile. Si è
visto che k riflette una caratteristica del processo (la capacità
riproduttiva 
della 
specie) 
che, 
per 
quanto 
complessa, 
è
abbastanza comprensibile. Invece h deve riflettere una miriade di
aspetti, racchiusi nel termine molto generico di “attriti” o freni
alla crescita (limitazioni ambientali, tecnologiche, sanitarie,
inquinamento e così via). Ogni tentativo di determinare
direttamente il valore di h appare un’impresa senza senso. Si
procederà allora come nel caso della crescita esponenziale. In
quel caso, possedendo due dati relativi al numero della
popolazione in due tempi diversi, si calcolava k e di qui tutti i
valori di N(t), per ogni t. In questo caso, dato che occorre calcolare
non soltanto k ma anche h, occorreranno tre dati. Oppure si dovrà
far ricorso, per il valore di h, a stime ottenute per altra via.
Ci limiteremo a mostrare attraverso alcuni esempi che il nostro
nuovo modello offre un accordo con i dati empirici migliore del
modello precedente (almeno in un certo numero di casi).
Una significativa conferma della legge logistica è legata a un
esperimento condotto negli anni Trenta dal biologo russo Georgii
F. Gause (1910-1986) circa lo sviluppo di un protozoo detto
Paramecium Caudatum. Gause pose 5 Paramecium in una provetta
contenente 0,5 cm3 di brodo di coltura e contò i protozoi ogni
giorno, per sei giorni di seguito. Il tasso di crescita del Paramecium
nei primi giorni (quando gli individui erano ancora pochi e
l’ambiente, cioè il brodo di coltura, era lungi dall’essere saturato)

era enorme: il 230,9% al giorno (cioè, k = 2,309). Poi la crescita
rallentò progressivamente, fino ad arrestarsi al numero di 375
individui con la relativa saturazione dell’ambiente. Gause ipotizzò
allora che 375 fosse la popolazione limite del Paramecium, e
pertanto: 
da cui 
Con questi dati è possibile tracciare la curva logistica che, come
appare dalla FIG. 1.7, approssima in modo eccellente i valori
sperimentali, rappresentati dai cerchietti.
Un’altra interessante conferma della legge logistica fu fatta
dallo statistico statunitense Raymond Pearl, allevando in
laboratorio una popolazione di Drosophila Melanogaster, un
moscerino responsabile di un gran numero di fermentazioni, il
quale 
è 
un 
vero 
e 
proprio 
protagonista 
della 
biologia
sperimentale. La Drosophila ha un ciclo vitale molto breve (circa
due settimane), il che facilita le esperienze di laboratorio. Nella
FIG. 1.8 i cerchietti rappresentano i conteggi del numero di
Drosophila effettuati da Pearl ogni sei giorni. La curva continua è
una curva logistica disegnata da Pearl per opportuni valori dei
parametri k e h (ricavati in modo analogo all’esperienza di Gause).
Come si vede, questa curva approssima in modo eccellente
l’istogramma.
Dobbiamo però esaminare i dati che più ci hanno fatto penare e

cioè quelli relativi alla crescita della popolazione umana. Pearl e
Reed, esaminando i dati relativi alla popolazione degli Stati Uniti
nel 1790, nel 1850 e nel 1910 (cfr. TAB. 1.4), calcolarono i parametri
k e h, ottenendo k = 0,03134 e h = 1,5887 · 10-10 = 0,00000000015887.
Usando tali valori, Pearl e Reed calcolarono i valori della
popolazione degli Stati Uniti dal 1790 al 1910. Tali dati, cui
abbiamo aggiunto quelli relativi al 1920, 1930, 1940 e 1950, sono
riportati nella seconda colonna della TAB. 1.9. Il confronto istituito
nella tabella mostra un accordo tutto sommato ottimo della legge
logistica con i dati reali.
FIGURA 1.7 Istogramma di crescita del Paramecium Caudatum,
confrontato con la curva di crescita logistica

FIGURA 1.8 Istogramma di crescita della Drosophila Melanogaster,
confrontato con la curva di crescita logistica
TABELLA 1.9 Popolazione degli USA nel periodo 1790-1950 e dati
calcolati mediante la legge di crescita logistica
Anno
Dati statistici
Dati calcolati
Errore
% errore
1790
3.929.000
3.929.000
0
0,0
1800
5.308.000
5.336.000
28.000
0,5
1810
7.240.000
7.228.000
–12.000
–0,2
1820
9.638.000
9.757.000
119.000
1,2
1830
12.866.000
13.109.000
243.000
1,9
1840
17.069.000
17.506.000
437.000
2,6
1850
23.192.000
23.192.000
0
0,0

1860
31.443.000
30.412.000
1.031.000
–3,3
1870
38.558.000
39.372.000
814.000
2,1
1880
50.156.000
50.177.000
21.000
0,0
1890
62.948.000
62.769.000
–179.000
–0,3
1900
75.995.000
76.870.000
875.000
1,2
1910
91.972.000
91.972.000
0
0,0
1920
105.711.000
107.559.000
1.848.000
1,7
1930
122.775.000
123.124.000
349.000
0,3
1940
131.669.000
136.653.000
4.984.000
3,8
1950
150.697.000
149.053.000
1.644.000
–1,1
Tutto bene dunque? Non proprio. Il fatto è che il calcolo della
popolazione limite fornisce k/h = 197.273.000, mentre la
popolazione degli Stati Uniti nel 1983 è stata stimata in
234.249.000 unità.
Difficoltà analoghe sorgono in altri casi e riguardano sempre la
stima della popolazione limite. Nel 1845 Verhulst stimò la
popolazione limite del Belgio e della Francia rispettivamente in
6.600.000 e 40.000.000 individui. Ora, mentre nel 1930 la
popolazione francese appariva stazionaria attorno a tale cifra,
quella belga aveva già raggiunto 8.092.000 unità; e la popolazione
francese è stata stimata in 54.346.000 unità nel 1983. Si può
tentare una spiegazione di tale discrepanza, tenendo con to del

grande sviluppo economico e industriale del Belgio, seguito alla
colonizzazione del ricchissimo Congo nel 1908. Per quanto
riguarda la Francia, negli anni Settanta prese le mosse un
processo accelerato di sviluppo e modernizzazione (fino all’inizio
degli anni Sessanta la popolazione francese superava di poco i 40
milioni). Tutto ciò suggerisce l’osservazione che anche la costante
h dovrebbe essere variata in dipendenza del variare delle
condizioni tecnologiche, industriali, ambientali ecc., per rendere
l’equazione di crescita logistica più aderente ai processi reali.
In definitiva, siamo ora in possesso di un modello certamente
più attendibile del modello di crescita malthusiano, la cui
utilizzabilità è limitata a una classe di casi molto ristretta. Anche
se bisogna dire che si tratta di un modello ancora estremamente
fragile e il cui uso va sottoposto a mille cautele.
Concludiamo con una curiosità. Assumendo come tasso di
crescita della popolazione mondiale quello relativo agli anni
Sessanta, cioè lo 0,02, e tenendo conto che la popolazione
mondiale nel 1961 era stimata in circa 3.060.000.000 individui, è
possibile calcolare il coefficiente h. Risulta h = 2,941 · 10-12 =
0,000000000002941. Pertanto la popolazione limite mondiale è k/h
= 9.086.000.000. Ci sarebbe molto da riflettere sull’attendibilità di
questa stima. Di fatto, la popolazione mondiale non è ormai
lontana da questa cifra, avendo superato i 7 miliardi, e il nostro
pianeta mostra segni visibili di difficoltà a sostenerla. È
comunemente ritenuto che, se tutta la popolazione mondiale
vivesse 
ai 
livelli 
dei 
cittadini 
europei 
e 
nordamericani,
l’ecosistema andrebbe incontro a un collasso.

Altri esempi: l’isomorfismo delle leggi
Vogliamo mettere in luce una conseguenza delle considerazioni
svolte nei due paragrafi precedenti. Abbiamo studiato il problema
della crescita di una popolazione umana e abbiamo riassunto le
nostre conclusioni mediante una legge matematica espressa
dall’equazione di crescita esponenziale. Abbiamo poi constatato
che tale equazione era adatta a descrivere la crescita di una
qualsiasi popolazione vivente (o almeno di moltissime di esse).
Lasciamo ora da parte le difficoltà che ci hanno condotto alla
considerazione di una descrizione matematica più raffinata
mediante l’equazione di crescita logistica. Quel che ci preme ora
sottoli neare è che queste equazioni (si tratti di quella di crescita
esponenziale o logistica) forniscono lo schema descrittivo di una
molteplicità di situazioni reali diverse.
Tralasciamo per il momento il problema dell’accordo con la
realtà e diamo quindi per certo che l’equazione di crescita
logistica funzioni sul piano descrittivo. Si è visto che essa serve
altrettanto bene a descrivere lo sviluppo di una popolazione di
Drosophila, di una popolazione di cittadini statunitensi o di
protozoi Paramecium. Ciò significa che la natura è scritta in
linguaggio matematico? Che l’essenza stessa delle “leggi” della
natura e, più in genere della realtà, è racchiusa in una equazione
matematica? Questa è una conclusione del tutto legittima – anche
se chi scrive non è d’accordo con essa – ma comunque non
necessaria. Si può semplicemente concludere che esistono
determinate regolarità (il che, nel nostro caso, non è affatto
strano né misterioso, dato che stiamo trattando di un unico
problema) e che queste regolarità possono essere descritte con un
solo schema matematico. Sarebbe meglio dire che le leggi di

crescita di una popolazione di Drosophila, di una popolazione di
Paramecium e della popolazione statunitense sono analoghe.
Ovviamente i valori dei coefficienti di crescita sono specifici delle
singole popolazioni ma ciò non influisce sulla tendenza generale
dello sviluppo, descritta in tutti i casi da una curva a forma di S.
Perciò potremo dire che hanno analoghe qualità o, com’è d’uso
nella teoria dei sistemi, che sono “leggi isomorfe” (isomorfe vuol
dire testualmente “che hanno la stessa forma”).
Quanto precede illustra una delle virtù della matematica: essa
realizza una grande economia di pensiero. Lo studio completo di
un’equazione (il che significa la conoscenza delle sue soluzioni)
mette a disposizione uno strumento già confezionato e di volta in
volta pronto a descrivere situazioni reali anche diversissime fra
loro. Per ricavare tutte le conclusioni desiderate basterà precisare
le costanti numeriche caratteristiche (come si è fatto nei casi già
esaminati, a condizione, beninteso, che l’equazione prescelta
fornisca una descrizione adeguata del fenomeno in esame.) E, per
ricavarle, 
non 
dovremo 
più 
affrontare 
un 
problema 
di
matematica, già risolto, ma soltanto un problema di calcolo
numerico.
Abbiamo detto che l’individuazione di leggi isomorfe (o, che è lo
stesso, l’uso del medesimo schema matematico in situazioni
diverse) può avvenire anche in campi tra loro molto dissimili.
Anche in questo non c’è nulla di misterioso o di mistico:
avremmo dovuto parlare, infat ti, di campi apparentemente molto
dissimili. In realtà si tratta per lo più di problemi che mostrano
comportamenti o tendenze qualitativamente simili anche se
derivano da realtà diversissime.

Consideriamo un esempio. Fin da quando si è iniziato a
studiare dal punto di vista quantitativo la crescita dei tumori, ci si
è basati sull’osservazione sperimentale (da lungo tempo nota) che
le cellule viventi si riproducono dividendosi e che la velocità
istantanea di riproduzione è proporzionale al volume delle cellule
che si stanno dividendo in quell’istante.
In altri termini, detto V(t) il volume delle cellule che si stanno
dividendo all’istante t, la velocità istantanea di accrescimento di
V(t) è data da:
V˙(t) = λV(t)
dove λ è una costante positiva caratteristica del fenomeno (cioè
del tipo di cellule esaminate).
Si tratta della solita equazione di crescita esponenziale che ci
fornisce la ben nota legge di crescita malthusiana:
V(t) = V(0)eλt
dove V(0) è il volume delle cellule che si dividono all’istante
iniziale. Ora, dato che un tumore è un insieme di cellule in rapida
riproduzione, c’è da attendersi che la legge precedente sia
adeguata a descriverne lo sviluppo quantitativo. Così è, entro certi
limiti, ed ecco che ci troviamo di fronte a un altro esempio di leggi
isomorfe, ovvero dell’uso della stessa equazione per descrivere
fenomeni che si verificano in ambiti assai diversi.
Va comunque detto che per questo modello accade qualcosa di
analogo a quanto si è visto nel caso della crescita di una
popolazione. La legge esponenziale, infatti, sembra adeguata a
descrivere la crescita di una massa tumorale agli inizi della

malattia, ma è inadeguata a descrivere le fasi successive, nelle
quali l’esperienza indica spesso un rallentamento sempre più
marcato della crescita.
Per ottenere una legge di crescita più realistica occorre
procedere in modo analogo a quanto si è fatto per la crescita della
popolazione e cioè correggere la costanza del tasso di crescita.
L’andamento del fenomeno tumorale ha però suggerito agli
studiosi una correzione diversa da quella logistica, e un po’ più
complicata. Questa è descritta dall’equazione differenziale (detta
“gompertziana”): 
dove α è una costante positiva caratteristica del fenomeno. Il
termine eαt, che ormai conosciamo, diventa sempre più grande al
passare del tempo t e quindi, poiché divide la costante λ, fa
diventare il tasso di crescita sempre più piccolo. Questo modo di
presentare le cose corrisponde all’ipotesi che, col passare del
tempo, la potenzialità di crescita delle cellule diminuisca. Si può
dare però un’altra interpretazione, ritenuta più convincente. Tale
interpretazione consiste nell’ammettere che le cellule che si
trovano al centro della massa tumorale, escluse dagli scambi di
ossigeno e di sostanze nutritive, muoiono progressivamente e
formano una regione necrotica, che non cresce più; per cui lo
sviluppo è dovuto principalmente alle cellule che si trovano sulla
superficie della massa tumorale. Questa spiegazione si può
leggere nell’equazione gompertziana interpretando il termine eαt
come correttivo della massa cellulare partecipante al processo di
riproduzione e non come correttivo di λ. Il tasso di crescita λ resta

costante, cioè la potenzialità di crescita delle cellule resta
invariata. Per sottolineare quest’interpretazione, scriveremo:
Ciò non cambia nulla dal punto di vista matematico, ma serve a
richiamare l’attenzione sull’interpretazione ora data.
Diamo ora altri esempi di leggi isomorfe.
L’equazione di crescita esponenziale è stata da noi scelta come
esempio perché è l’equazione differenziale più semplice che
esista, 
una 
delle 
strutture 
matematiche 
più 
elementari.
Nondimeno questa struttura matematica così elementare è
suscettibile di descrivere un gran numero di processi reali diversi.
Non avrebbe senso tentarne qui un elenco, che è oltre tutto
aperto.
Tanto per fare un esempio, l’equazione di crescita esponenziale
serve per modellizzare un problema come il seguente: in che
misura il rischio di un incidente d’auto dipende dall’ingestione di
alcool? Supponiamo che tale rischio sia misurato in percentuale e
che la variabile R, che descrive tale percentuale di rischio, sia
funzione del tasso A di alcool presente nel sangue, cioè R = R(A).
La FIG. 1.9 mostra una serie di determi nazioni di R in funzione di
A (indicate con i dischetti), che sono frutto di approfondite analisi
statistiche di un gruppo di ricercatori. Come si vede, sempre dalla
FIG. 1.9, i dati empirici si accordano molto bene con la curva di
crescita esponenziale che è soluzione dell’equazione

FIGURA 1.9 Istogramma del rischio di incidente automobilistico
per ingestione di alcool, confrontato con la curva di crescita
esponenziale
R˙(A) = kR(A)
dove k = 21,4 e R(0) = 1 (cioè la percentuale di rischio di un
incidente stradale in condizioni di sobrietà è stimata all’1%). È la
nostra solita equazione (in cui la variabile A prende il posto del
tempo e R(A) il posto di N(t)), la cui soluzione è R(A) = R(0)ekA (cioè
la curva in FIG. 1.10). L’unico limite del modello è che esso predice
un rischio del 100% di incidente a una concentrazione alcoolica
nel sangue dello 0,22%. Tutto sommato questa previsione di
certezza dell’incidente non è poi così sconcertante, se si pensa

che, per raggiungere quel tasso alcoolico, un uomo di media
corporatura dovrebbe bere di colpo circa mezzo litro di
acquavite...
FIGURA 1.10 Curva di crescita esponenziale con coefficiente k
maggiore (>), uguale (=) o minore (<) di zero
Il 
campo 
delle 
applicazioni 
dell’equazione 
di 
crescita
esponenziale si amplia molto se eliminiamo la restrizione che il
coefficiente k sia positivo. Consideriamo cioè (e questo è un
approccio puramente matematico) un’equazione della forma:
X˙(t) = kX(t)
dove X(t) è una funzione di una variabile t (il significato di X e di
t sarà specificato volta per volta), X˙(t) la velocità della sua
variazione (in termini matematici la sua derivata) e k una
costante qualsiasi (positiva, negativa o nulla).

La soluzione dell’equazione è sempre X(t) = X(0)ekt e il suo
grafico cartesiano è quello descritto dalla FIG. 1.10, a seconda del
segno di k. Si intuisce che la pendenza maggiore o minore della
curva (se k non è nullo) dipende dal valore di k.
L’equazione di crescita esponenziale, nel caso in cui il
coefficiente k sia negativo, si applica allo studio di una quantità di
problemi. Per esempio, essa descrive la velocità con cui
diminuisce la concentrazione di una medicina nel sangue col
trascorrere del tempo. È questo un problema di importanza
fondamentale in farmacologia per determinare i dosaggi ottimali
di un medicamento e l’intervallo di tempo ottimale che deve
intercorrere fra due somministrazioni successive. Ciò è di
particolare importanza, per esempio, nel caso degli antibiotici.

FIGURA 1.11 Curve della concentrazione di penicillina nel sangue
in casi di somministrazione di diverse dosi iniziali
Il grafico della FIG. 1.11 illustra dei risultati sperimentali
riguardanti la diminuzione nel tempo della concentrazione della
penicillina nel sangue, in una serie di casi in cui erano state
somministrate differenti dosi iniziali. Come si vede, i dati relativi
all’eliminazione della penicillina seguono un andamento di tipo
esponenziale, quale quello descritto dalla FIG. 1.10 nel caso in cui k

sia negativo.
Di enorme importanza è l’applicazione dell’equazione di
crescita esponenziale al fenomeno del decadimento radioattivo,
che è alla base di tutti i metodi di datazione delle rocce, dei fossili
e dei reperti archeologici. Questo fenomeno fu studiato agli inizi
del secolo dal fisico inglese Ernest Rutherford (1871-1937), il quale
mostrò che gli atomi di taluni elementi godono di una proprietà
(per la quale furono chiamati “radioattivi”): essi sono instabili e
cioè dopo un dato periodo di tempo una proporzione fissa di essi
si disintegra spontaneamente formando gli atomi di un nuovo
elemento. Rutherford mostrò che la radioattività è una proprietà
intrinseca di questi elementi e che la intensità con cui si
manifesta è direttamente proporzionale al numero di atomi della
sostanza. Quindi, se N(t) è il numero degli atomi al tempo t, la
velocità della loro disintegrazione (che è data, come al solito, dalla
derivata del loro numero, N˙(t)) è proporzionale a N(t). Cioè:
N˙(t) = – λN(t)
dove λ è una costante positiva, detta “costante di decadimento”
della sostanza, essendo appunto caratteristica della sostanza
stessa. La soluzione dell’equazione è, come al solito,
N(t) = N(0)e-λt
dove N(0) è il numero di atomi all’istante iniziale (t = 0).
Per via sperimentale è possibile determinare la costante λ
caratteristica di ogni sostanza e ciò è stato fatto in un gran
numero di casi. Un’altra costante caratteristica del fenomeno e
molto studiata è la cosiddetta “emivita”. L’emivita di una

sostanza radioattiva è definita come il tempo necessario affinché
metà di una data quantità di atomi radioattivi decada. Per
esempio, l’emivita del Piombo-210 è 22 anni, l’emivita del
Carbonio-14 è 5.568 anni e quella dell’Uranio-238 è 4,5 miliardi di
anni.
L’idea che è alla base nel metodo di datazione di un corpo
qualsiasi può essere rozzamente descritta come segue. Non è di
solito difficile calcolare il numero degli atomi radioattivi del corpo
in esame in un dato istante, e cioè N(t) · λ è nota. Se in qualche
modo si riesce a calcolare qual era il numero degli atomi
all’istante iniziale, cioè nel momento in cui si formò il nostro
oggetto, allora l’unica incognita nella formula N(t) = N(0)e-λt sarà il
tempo t e cioè l’età dell’oggetto. È quasi superfluo dire che
calcolare il numero degli atomi all’istante iniziale è il punto di
maggiore difficoltà. Quindi tutti gli sforzi debbono essere
indirizzati alla determinazione di N(0) per vie indirette o almeno a
una determinazione approssimata: dopodiché la soluzione
dell’equazione di crescita esponenziale fornisce, con un semplice
calcolo, la risposta (cfr. riquadro 1.1).
riquadro 1.1 Il metodo di datazione del Carbonio-14
Uno dei più famosi e semplici metodi di datazione dei
reperti archeologici è il cosiddetto “metodo del Carbonio-14”
ideato alla fine degli anni Quaranta dal chimico statunitense
Walter F. Libby (che ricevette per questo il premio Nobel nel
1960). Lo descriviamo rapidamente. L’atmosfera terrestre è
bombardata continuamente da raggi detti “cosmici”, i quali
danno luogo alla produzione di Carbonio-14 (C14), che è
radioattivo. Il C14 viene assorbito dalle piante e assimilato

così dagli animali. L’assorbimento del C14 nei tessuti viventi è
compensato esattamente dal decadimento radioattivo, per
cui si crea uno stato di equilibrio. Quando un organismo
cessa di vivere, la concentrazione di C14 diminuisce col tempo
perché esso non ne assimila più e quindi ha luogo soltanto il
fenomeno del decadimento. Ora, l’ipotesi fondamentale su
cui poggia il metodo del C14 è che l’intensità del
bombardamento della superficie terrestre da parte dei raggi
cosmici non è mai variata. Ciò implica che il decadimento del
C14 in un campione, per esempio, di carbon fossile, si
verificava all’origine con la stessa intensità con cui si verifica
oggi. Queste osservazioni consentono di determinare l’età di
un campione di carbon fossile e quindi di reperti archeologici
ritrovati assieme ad esso.
Supponiamo che N(t) sia la quantità di C14 presente nel
nostro campione all’istante t e N(0) la quantità presente
all’istante t = 0, cioè all’istante in cui si formò l’oggetto. Detta
λ la costante di decadimento radioattivo del C14 (facilmente
calcolabile), scriveremo: N˙(t) = – λN(t).
Denotiamo con R(t) la velocità di decadimento radioattivo
del C14 · R(t) è cioè la derivata di N(t), presa col segno negativo:
R(t) = – N(t).
Sostituendo nella formula precedente possiamo allora
scrivere: R(t) = λN(t). E poiché sappiamo che N(t) = N(0)e–λt,
scriveremo ancora: R(t) = λN(0)e–λt.
Secondo questa formula la velocità di decadimento
all’istante iniziale è R(0) = λN(0). Dividendo R(t) per R(0) si

ottiene:
E semplificando:
In questa equazione l’unica incognita è t, cioè l’età del
carbon fossile, che può così essere calcolata. Infatti: R(t) può
essere calcolato sperimentalmente; R(0) deve essere uguale
alla velocità di disintegrazione del C14 in un analogo
campione di legno vivente; λ può essere facilmente calcolata
sapendo che l’emivita del C14 è 5.568 anni; e = 2,71828182
(base dei logaritmi naturali).
Questo metodo è stato applicato di fatto per la datazione di
un gran numero di reperti archeologici o di oggetti antichi.
Un esempio divertente è il seguente. Nel castello di
Winchester si trova una grande tavola rotonda di legno
appesa al muro. Essa ha un diametro di cinque metri e mezzo
ed è divisa in venticinque settori, uno per il re e gli altri per i
suoi cavalieri, per cui molti avanzarono l’ipotesi che si
trattasse della famosa tavola rotonda di re Artù.
Nel 1977 vennero fatte delle misurazioni della velocità di
decadimento radioattivo nel legno della tavola (per minuto
per grammo di campione). Il risultato fu R(t) = 6,08. La velocità
di disintegrazione per il legno vivo è 6,68, per cui R(0) = 6,68.

Quindi:
Per calcolare t si procede così. Se t* è l’emivita (cioè il tempo
che occorre affinché si dimezzi la quantità di atomi
radioattivi iniziale, che è N(0)), al tempo t*, N(t*) sarà la metà
di N(0). Ma poiché la legge del decadimento radioattivo è N(t)
= N(0)e–λt, scrivendo questa equazione all’istante t*, si avrà:
cioè:

Fig. 1 La tavola rotonda nel Castello di Winchester E poiché,
per il C14, l’emivita t* è 5.568 anni, l’unica incognita è il valore
λ che noi cerchiamo. Non resta quindi che calcolarlo. (Chi
conosce 
un 
pochino 
di 
matematica 
in 
più 
sa 
che
dall’equazione precedente risulta che λ è uguale al logaritmo
di 2 diviso per t*. E quindi che λ = 1,245 · 10-4 l’anno).
Sostituendo il valore così ottenuto di λ nell’equazione 

si ottiene t, che risulta eguale a 700 anni.
Ne consegue che la “tavola rotonda” del castello di
Winchester fu costruita con legno tagliato nel 1275 (circa) e
quindi non può trattarsi della tavola di re Artù, che visse nel
VI secolo.
Vogliamo ora dare un altro esempio di un’applicazione
isomorfa, questa volta della legge logistica. In questo caso il
problema che affronteremo è di natura completamente diversa da
quelli finora visti e può essere così riassunto: in che modo il
cambiamento tecnologico o l’innovazione si diffonde in una sfera
di attività produttive? Esaminiamo un problema specifico relativo
all’agricoltura. Supponiamo che un’innovazione venga introdotta
a un dato istante t = 0 in un gruppo di N imprenditori agricoli (che
si tratti di piccoli agricoltori o dirigenti di aziende agricole).
Indicheremo con X(t) il numero degli agricoltori che hanno
adottato l’innovazione all’istante t. Come al solito, X(t) è una
variabile 
che 
assume 
tutti 
i 
valori 
numerici 
(continua),
nonostante il fatto che in realtà cambi soltanto per numeri interi.
L’ipotesi più semplice che si può fare è che un agricoltore adotti
l’innovazione soltanto dopo che ne abbia sentito parlare da un
altro agricoltore. In tal modo, il numero degli agricoltori che
adottano l’innovazione nell’intervallo di tempo Δt, e che
denoteremo con ΔX, è proporzionale al numero di coloro che
l’hanno già adottata e al numero di coloro che non l’hanno ancora
adottata (cioè N(t) – X(t)). Tutto questo accade nell’intervallo di

tempo Δt; cioè:
ΔX = kX(t) · (N(t) – X(t)) · Δt
dove k è un numero, una costante di proporzionalità. Dividendo
per Δt, si ottiene: 

FIGURA 1.12 Curve della diffusione della coltivazione di grano
ibrido in tre Stati USA
e ricordando quanto si è detto a proposito della legge di crescita

di una popolazione, sappiamo che, facendo tendere Δt a zero, ΔX/
Δt diventa la derivata di X(t), cioè X˙(t). Otteniamo così di nuovo
l’equazione logistica
X˙(t) = kX(t) · (N(t) · X(t))
la cui soluzione è rappresentata dalla curva a S, a noi ben nota
(cfr. FIG. 1.6).
La FIG. 1.12 mostra il risultato di una statistica eseguita da alcuni
ricercatori statunitensi sulla velocità di adozione del grano ibrido
nelle coltivazioni di tre Stati nordamericani. Come si vede,
confrontandolo con la FIG. 1.6, l’accordo con la legge logistica
appare soddisfacente anche se è migliore nel secondo tratto della
curva e meno buono nel primo. Questo inconveniente può essere
ridotto di molto, eliminan do un aspetto irrealistico del modello: e
precisamente l’ipotesi che la diffusione dell’innovazione sia
dovuta soltanto al contatto diretto fra agricoltori e non anche alla
pubblicità tramite i mass media (radio, televisione, riviste
specializzate ecc.). Non è difficile introdurre questa modifica
nell’equazione: basta aggiungere al numero di agricoltori che
adottano l’innovazione nell’unità di tempo il numero di coloro
che lo fanno perché informati dai mass media. Tale numero sarà
ancora proporzionale al numero di coloro che non l’hanno già
adottata. L’equazione diventerà:
X˙(t) = kX(t) · (N(t) · X(t)) + h(N(t) · X(t)).
Si può verificare che questa equazione (che è ancora una
variante dell’equazione di crescita logistica) fornisce un accordo
più soddisfacente con i dati statistici.

Questo modello è stato applicato con successo alla diffusione
delle 
innovazioni 
tecnologiche 
nell’industria 
del 
carbone,
dell’acciaio, delle ferrovie e della birra.
Un modello matematico della predazione
Vogliamo ora illustrare brevemente un modello dovuto al
matematico italiano Vito Volterra (1860-1940) e formulato
indipendentemente nello stesso anno, il 1926, dallo statistico
austriaco-statunitense Alfred J. Lotka (1880-1949). Questo modello
è famoso perché è il punto di partenza di una lunga serie di
ricerche sull’analisi matematica delle “associazioni biologiche”,
cioè della convivenza fra specie animali diverse. Esso è inoltre
importante per i problemi della verifica empirica dei modelli
matematici, su cui torneremo nell’ultimo paragrafo di questo
capitolo.
Le ricerche di Volterra furono ispirate dai risultati delle
statistiche del biologo Umberto D’Ancona (che era suo genero), il
quale, studiando le variazioni delle popolazioni dei pesci
dell’Adriatico, aveva rilevato un curioso fenomeno. Il D’Ancona
aveva preso in esame la percentuale dei “selaci” o pesci predatori
(cioè di pesci, come gli squali, che si nutrono di altri pesci) sul
totale del pesce pescato nei porti dell’Alto Adriatico e aveva
rilevato una sorprendente fluttuazione di tale percentuale. La TAB.
1.10 riporta i dati relativi al porto di Fiume negli anni 1914-23.
TABELLA 1.10 Percentuale dei pesci predatori sul pescato nel porto di
Fiume nel periodo 1914-23
Anno
Percentuale
1914
11,9

1915
21,4
1916
22,1
1917
21,2
1918
36,4
1919
27,3
1920
16,0
1921
15,9
1922
14,8
1923
10,7
È evidente l’incremento della percentuale nel periodo della
Prima guerra mondiale. D’Ancona riteneva che la diminuzione
dell’attività di pesca in quel periodo (dovuta alla guerra navale)
fosse la causa di tale incremento e quindi che l’attività di pesca
sfavorisse (entro certi limiti) lo sviluppo dei pesci preda (che si
identificano quasi con i pesci commestibili).
L’approccio scelto da Volterra è molto interessante. Invece di
affrontare direttamente il problema di D’Ancona cercò di studiare
il fenomeno della convivenza fra prede e predatori in forma pura,
eliminando gli attriti e le perturbazioni capaci di oscurare il
fenomeno. Uno di tali attriti era proprio la pesca. Egli suddivise la
popolazione dei pesci in due gruppi: quello delle prede, il cui
numero nel tempo è x(t), e quello dei predatori, il cui numero nel

tempo è y(t). Il problema era quello di scrivere le espressioni delle
derivate sia della x(t) che della y(t). (Per semplicità, d’ora in poi
non indicheremo più esplicitamente la dipendenza delle variabili
dal tempo e cioè non scriveremo più il simbolo t).
Poiché le prede dispongono del cibo (plancton ecc.) in quantità
praticamente illimitata, il loro sviluppo, in assenza di predatori, è
malthusiano. Quindi x. = ax. Mentre i predatori, in assenza di
prede e quindi senza cibo, non possono che morire. Perciò
un’equazione adatta a descrivere la loro tendenza spontanea è
quella di crescita esponenziale con costante negativa (cfr. FIG.
1.10); cioè y. = – cy dove c è un numero positivo.
Ora bisogna introdurre l’interazione fra le due specie, cioè la
predazione. Ragioneremo così: la crescita dei predatori, essendo
dovuta all’attività di predazione, è proporzionale al numero delle
prede (cibo disponibile) oltre che, ovviamente, al numero dei
predatori stessi. Essa sarà quindi rappresentata da un termine del
tipo dxy. Porremo cioè y. = – cy + dxy. Analogamente, la
predazione induce una diminuzione delle prede la cui intensità è
proporzionale sia a x che a y. In definitiva, il sistema delle due
equazioni: 
va sotto il nome di equazioni di Lotka-Volterra.
La risoluzione di questo sistema di equazioni è cosa ben più
difficile della risoluzione delle equazioni dei paragrafi precedenti.
Ci limitiamo a dire che essa conduce a questa conclusione: il
numero 
di 
individui 
delle 
due 
popolazioni 
fluttua

“periodicamente”. Ciò vuol dire che si accresce e poi, dopo un
certo intervallo di tempo, diminuisce ritornando al valore di
partenza; e poi di nuovo cresce e ancora diminuisce tornando al
valore di partenza dopo lo stesso intervallo di tempo; e così di
seguito indefinitamente. L’oscillazione si ripete quindi e sempre
in un uguale intervallo di tempo, che è detto “periodo”. La FIG. 1.13
mostra le due funzioni, x(t) e y(t) (con una particolare scelta dei
coefficienti a, b, c, d): sull’ascissa è rappresentato il tempo e
sull’ordinata il numero di individui delle due popolazioni, cioè x(t)
e y(t). Il periodo di un’oscillazione è rappresentato dall’intervallo
T: le popolazioni partono da un dato valore numerico e vi
ritornano dopo il tempo T.
L’interpretazione del grafico è la seguente. L’incremento
naturale delle prede porta a un’accresciuta attività di predazione.
Come conseguenza i predatori crescono di numero e in tal modo
aumenta considerevolmente il numero delle prede divorate. A
questo punto i predatori hanno a disposizione un cibo troppo
scarso e cominciano a diminuire. Questa nuova situazione dà
respiro alle prede, che ricominciano a crescere di numero. E così
via.
Un 
risultato 
importante 
ottenuto 
da 
Volterra 
fu 
la
constatazione che la media delle popolazioni in un singolo
periodo è sempre la stessa, quale che sia il numero delle due
popolazioni all’inizio del periodo considerato. Precisamente: le
prede sono in media c/d e i predatori in media a/b.

FIGURA 1.13 Curve dell’oscillazione delle popolazioni dei pesci
prede e dei pesci predatori secondo le equazioni di Lotka-
Volterra
Introduciamo ora nel modello la perturbazione indotta dalla
pesca, il che è, tutto sommato, semplice. Infatti, la pesca va
pensata come un prelievo indiscriminato e uniforme dell’una e
dell’altra specie. Insomma, la pesca frena la crescita di entrambe
le popolazioni in diretta proporzione al loro numero. Si tratterà
cioè di sottrarre da x. e da y. un termine proporzionale
rispettivamente a x e a y, con lo stesso coefficiente di
proporzionalità ε (che esprime l’uniforme intensità della pesca),
ottenendo le equazioni:
Se si calcolano le nuove medie delle due popolazioni in un

periodo T, in presenza di pesca, si ottiene che: 
è il numero medio delle prede nell’intervallo di tempo T, 
è il numero medio dei predatori nell’intervallo di tempo T.
Quindi, l’introduzione della pesca accresce il numero delle prede
(di ε/d) e diminuisce il numero dei predatori (di ε/b). Non esultino
però i lettori cacciatori o pescatori! La pesca non deve essere
eccessiva; infatti, se il numero e che misura la sua intensità è
superiore o eguale ad a, le prede si estingueranno, come si vede
osservando la prima equazione.
Quella che precede è la descrizione sommaria di come Volterra
verificò e spiegò matematicamente l’ipotesi di D’Ancona. Non fu
però questa l’unica applicazione di successo del “modello preda-
predatore” di Volterra. Esso ebbe un’applicazione forse ancor più
suggestiva al problema dell’uso degli insetticidi in agricoltura.
L’uso di un insetticida è un po’ come la pesca: distrugge
indiscriminatamente insetti prede e insetti predatori. Provoca
così, in accordo con la teoria di Volterra, un accrescimento delle
prede che non di rado sono proprio gli insetti nocivi che sarebbe
auspicabile eliminare.
Un esempio assai istruttivo è il seguente. Nel 1868 venne
casualmente introdotto dall’Australia in California un insetto,
l’Icerya Purchasi, che, trovando di proprio gradimento le
coltivazioni di agrumi californiane, ne distrusse un’enorme

quantità. Venne allora introdotta una coccinella, ben nota
predatrice dell’Icerya, il Novius Cardinalis che ridusse il numero
degli insetti nocivi a un livello assai basso. Assai più tardi, la
scoperta dell’insetticida DDT (ora proibito perché cancerogeno)
indusse molti agricoltori a tentare la distruzione finale dell’Icerya.
Il risultato fu opposto ai desideri: difatti, la distruzione della
coccinella predatrice ebbe come effetto un nuovo incremento del
numero degli insetti nocivi!
Altri risultati convalidarono sul piano empirico il modello di
Volterra. Per molti anni la Hudson Bay Company in Canada tenne
dei registri della cattura delle linci, e delle loro prede principali, le
lepri. Il numero degli esemplari catturati (e scrupolosamente
annotati dalla compagnia) fornisce un’attendibile indicazione del
numero delle popolazioni totali delle linci e delle lepri nella zona
in esame. Le curve (ottenute interpolando i dati) che descrivono
l’andamento delle popolazioni sono date dalla FIG. 1.14 e mostrano
un evidente andamento oscillatorio, di carattere periodico, in
discreto accordo con le leggi di Volterra.
Tutto bene, dunque? Nient’affatto. La realtà è maledettamente
restìa a farsi imprigionare nelle formule della matematica,
soprattutto se si tratta di una realtà di enorme complessità come
quella dei fenomeni biologici. Numerose obiezioni sono state
mosse al modello di Volterra. Quel che ha reso diffidenti i biologi
è il fatto che dati come quelli della statistica relativa alle linci e
alle 
lepri 
in 
Canada 
rappresentano 
quasi 
un’eccezione.
L’andamento oscillatorio e periodico previsto da Volterra non si
presenta nella maggior parte delle associazioni prede-predatori.
Fin dai tempi della sua formulazione il modello fu messo in
discussione da diversi biologi con ogni tipo di argomenti. Per

esempio, alcuni obiettarono che l’andamento oscillatorio dei
pesci dell’Adriatico sparisce se si escludono dal conteggio le
seppie, le quali presentano un andamento oscillatorio stagionale
che influenza fortemente il fenomeno complessivo. Queste e altre
obiezioni diedero luogo a lunghe discussioni e alla fine lo stesso
D’Ancona ammise che non vi erano argomenti sufficienti per
difendere il valore empirico generale del modello.
FIGURA 1.14 Curve dell’oscillazione delle popolazioni di linci e
lepri nel Canada, durante il periodo 1845-1935
Quale conclusione trarre, visto che il modello in diversi casi
funziona? Questo problema, come tanti altri simili, è tuttora
aperto. Il problema della convalida empirica dei modelli
matematici è un problema di straordinaria complessità che,
nell’ultimo 
capitolo, 
ci 
spingerà 
a 
sviluppare 
alcune
considerazioni di carattere generale.

Lo stesso modello in ambiti diversi
Dopo la drammatica crisi economica del 1929 che mise in
ginocchio gran parte delle economie capitalistiche a livello
mondiale si pose il problema di capire se effettivamente
esistessero delle tendenze che le spingevano a un andamento di
tipo ciclico. Gran parte dell’opera del celebre economista inglese
John Maynard Keynes fu stimolata da questa problematica. A noi
interessa mettere in luce uno sviluppo matematico tutto
sommato marginale rispetto alla tematica principale, ma che
illustra assai bene il concetto di isomorfismo che abbiamo
introdotto in precedenza, e cioè come uno stesso schema
matematico – in questo caso le equazioni di Lotka-Volterra –
possa descrivere situazioni reali diversissime tra loro.
Un tentativo di giustificare l’andamento ciclico delle economie
capitalistiche fu dato dal modello formulato dall’economista
Richard Goodwin negli anni Settanta. La sua tesi era che questo
andamento ciclico dipendesse dal conflitto tra i due principali
agenti dell’economia, lavoratori e capitalisti. Secondo Goodwin
lavoratori e capitalisti hanno un’unica torta da spartirsi: tanto più
i salari crescono a svantaggio dei profitti tanto minori saranno gli
investimenti; ma se gli investimenti diminuiscono aumenta il
tasso di disoccupazione e quindi la quota della torta in possesso
dei lavoratori diminuisce. È facile intuire che una simile relazione
di dipendenza deve generare un andamento ciclico che fa pensare
a quello descritto dalle equazioni di Lotka-Volterra. E in effetti
quando si prova a formalizzare le idee di Goodwin, come egli ha
fatto, le cose stanno proprio in questi termini. Daremo un’idea
rapida di come si perviene alle equazioni di Goodwin che il lettore
riconoscerà essere identiche a quelle di Lotka-Volterra.

Le ipotesi su cui si basa il modello di Goodwin sono molto
semplificatrici. Si suppone che la forza lavoro cresca a un tasso
costante (e quindi esponenziale, malthusiano). Si suppone che i
lavoratori consumino interamente i salari e che i profitti dei
capitalisti vengano interamente reinvestiti. Il tasso del progresso
tecnico è anch’esso supposto costante, come il rapporto tra
capitale e prodotto, mentre il saggio del salario cresce con
l’occupazione. Tutte le variabili del modello sono ovviamente
intese come reali per poter ricorrere allo strumento del calcolo
differenziale.
Vediamo ora di tradurre in formule queste ipotesi. Detto Y il
prodotto, K il capitale e L i lavoratori, ricordando l’ipotesi che il
tasso del progresso tecnico è costante, constatando che il
rapporto Y/L è l’unità di prodotto per unità di forza-lavoro –
ovvero la produttività del lavoro, misura del progresso tecnico – si
può scrivere: a = Y/L a0eαt. Quindi, indicato con w il saggio del
salario la quota del prodotto che va ai lavoratori è u = w/a e, di
conseguenza, la quota di prodotto che va ai capitalisti è 1 – w/a.
Ricordiamo ora che il rapporto tra capitale e prodotto K/Y = σ è
costante e che il lavoro cresce esponenzialmente, cioè n = n0eβt,
dove β è una costante. Infine, il tasso di occupazione è ν = L/n e il
tasso di investimento (uguale al tasso di risparmio, per le ipotesi
del modello) è 1 – w/a. Possiamo ora osservare che la derivata del
capitale, cioè l’investimento è K. = (1 – w/a) e, per il tasso di
crescita 
del 
reddito, 
si 
ha: 

Quanto detto all’inizio implica inoltre che 
 ; e
quindi si ha che
 . Di qui ancora si ha che 
 . Mettendo insieme queste due relazioni si ottiene
la prima equazione del modello: 
La seconda equazione risulta da una nota ipotesi formulata
dall’economista A. W. H. Phillips nel 1955 che stabilisce una
relazione negativa tra il tasso di variazione dei salari nominali e il
tasso di disoccupazione, detta anche curva di Phillips (FIG. 1.15). In
altri termini, i salari aumentano tanto più rapidamente quanto
più è basso il tasso di disoccupazione.
Nei termini più semplici tale relazione viene espressa così:
 ; da cui si ha 
Si ottiene così la seconda equazione del sistema. Si tratta ora di
accoppiare le due equazioni tenendo conto delle relazioni tra le
due variabili. Si ottiene: 
Il lettore si può rendere conto, con un semplice confronto, che

si tratta delle stesse equazioni del modello di Lotka-Volterra
nell’ambito delle dinamiche delle popolazioni, per cui quanto vale
in quel caso vale anche qui. In particolare, si ha che i valori medi
delle due variabili del modello su una qualsiasi traiettoria (ovvero
quali che siano le condizioni iniziali del sistema economico) sono
quelle dei loro valori in equilibrio.
FIGURA 1.15 La curva di Phillips
Si possono dare esempi di altri contesti completamente diversi
in cui emergono le equazioni di Lotka-Volterra. Un altro caso
significativo è il seguente. Negli anni Venti il fisico olandese
Balthasar Van der Pol (1889-1959) si occupò diffusamente della
tematica delle trasmissioni radio e dei circuiti elettrici particolari
utilizzati nella radio: di fatto, egli era il direttore dell’ufficio
ricerche della Philips. I suoi lavori matematici e fisici ebbero un

ruolo fondamentale al riguardo. Non possiamo entrare nei
dettagli che sono di una complicazione matematica eccessiva per
questo libro (e per i quali ci si può riferire a Israel, 1998a, 2004a),
ma ci limiteremo a dire che uno dei sistemi di equazioni che
interveniva nei modelli da lui studiati era esattamente quello di
Lotka-Volterra e, di fatto, si può ben dire che egli vi era pervenuto
prima di questi autori (nel 1920), anche se ebbe a riconoscere
l’identità dei due sistemi di equazioni soltanto nel 1934 (Van der
Pol, 1934).
Abbiamo visto così altri esempi di quel che abbiamo chiamato
in precedenza un isomorfismo delle leggi. Si potrebbe piuttosto
dire che abbiamo a che fare con uno schema matematico (un
“modello”) che è efficace nel descrivere situazioni appartenenti a
contesti diversissimi tra di loro: biologico, economico, elettronico.
E la lista è del tutto aperta ad altre situazioni di cui non parleremo
per la loro complessità e ad altre possibili e ancora non note.
Quindi, è legittimo pensare a un modello come quello delle
equazioni di Lotka-Volterra, a uno schema (matematico) vuoto di
realtà possibili, per usare il linguaggio del gruppo di matematici
francesi che era raccolto attorno allo pseudonimo di Nicolas
Bourbaki.
A questo punto, il lettore potrà ben chiedersi come mai si
verifichi una situazione così strana che sembra attribuire alla
matematica la virtù magica di rivelare strutture e relazioni
nascoste. Di fatto, questo era anche l’atteggiamento dei
“bourbakisti” e di altri scienziati – come avremo modo di
discutere meglio nel terzo capitolo; essi consideravano questa
aderenza della matematica ai fatti come qualcosa di misterioso o
come una “irragionevole efficacia”, assumendo un atteggiamento

quasi mistico: l’idea che il mondo è matematico si riproponeva
per loro come un enigma inspiegabile. In realtà, nei casi che
abbiamo considerato non vi è nulla di misterioso. L’andamento
esponenziale e quello logistico sono la naturale rappresentazione
di processi a crescita rispettivamente illimitata e limitata, mentre
l’efficacia rappresentativa del modello di Lotka-Volterra consiste
nel fatto che esso descrive l’andamento oscillatorio “non lineare”
più semplice che sia possibile – dove la non linearità significa che
le variabili sono legate da relazioni espresse da equazioni non
lineari, a differenza dell’oscillatore armonico che è lineare e non
va oltre la descrizione di processi oscillatori semplici, come un
pendolo che si muove in assenza di attrito. Appena queste
semplificazioni vengono tolte, la situazione si complica e richiede
una rappresentazione non lineare e, tra quelle che conservano un
carattere oscillatorio, il modello di Lotka-Volterra fornisce la più
semplice o, se si preferisce, la meno “complessa”.
Siamo convinti che esista sempre una spiegazione razionale
dell’efficacia della matematica, anche nei casi più bizzarri e
sorprendenti, e che non vi sia mai nulla di mistico o misterioso in
tale efficacia, anche se tale spiegazione talora può non essere
immediata.
La teoria dei giochi
Fin dal Seicento si sviluppò l’interesse per un’analisi scientifico-
matematica dei giochi di società. L’impulso a far questo era del
tutto naturale: cosa sperare per rendere al massimo probabile la
vincita in una lotteria, in un gioco a carte, a dadi, in un gioco sulla
scacchiera e altre competizioni in cui intervengono in modo
evidente delle considerazioni di numeri? Si dice che il calcolo

delle probabilità sia nato proprio in un contesto del genere, anche
se le motivazioni di questa branca della matematica furono
molteplici e legate a questioni come la determinazione del premio
ottimale in un contratto di assicurazione o nel conferimento di un
assegno di tipo pensionistico. Soltanto verso gli inizi del
Novecento nacque una branca matematica specificamente
dedicata ai giochi, “la teoria dei giochi”, i cui principali autori
furono il matematico francese Emile Borel e il matematico di
origini ungheresi John von Neumann. Entrambi introdussero due
sistemi per formalizzare matematicamente un gioco, atti ad
analizzarlo in termini matematici e a determinare (se possibile) le
condizioni sotto le quali si determina la vittoria di uno dei
contendenti, oppure una patta: si tratta della rappresentazione
“ad albero” e della rappresentazione “in forma strategica o
normale”. Ragionando sul primo tipo di rappresentazione il
matematico tedesco Ernst Zermelo – come vedremo più in là –
dimostrò un teorema fondamentale relativo a uno dei giochi più
complessi, gli scacchi.
Partiamo dalla considerazione di un gioco molto più semplice e
assai praticato dai bambini, il cosiddetto tris. Lo giocano due
persone su uno schema quadrato di nove caselle. Il primo
giocatore disegna una crocetta in una casella, il secondo risponde
disegnando in un’altra casella un cerchietto, e così via
alternativamente. Vince chi riesce a mettere in riga, in colonna o
sulla diagonale principale tre crocette o tre cerchietti: la FIG. 1.16
descrive passo per passo una partita che si conclude con la
vittoria del primo giocatore.
Invece la FIG. 1.17 descrive una partita in cui il secondo giocatore
riesce a bloccare il primo e a ottenere una patta.

La caratteristica del gioco del tris è che, in realtà, se entrambi i
giocatori giocano a un livello massimo, e cioè senza commettere
errori, esso non può che terminare con una patta. Proviamo a
ragionare sul numero delle possibili partite a tris, introducendo al
contempo l’idea della rappresentazione ad albero. Ci chiediamo:
quante partite diverse si possono giocare a tris? Il calcolo è facile.
Il primo giocatore contrassegna un quadratino con una crocetta:
può farlo in 9 modi diversi. Al secondo giocatore restano a
disposizione 8 quadratini in cui tracciare un cerchietto. Quindi, a
questo stadio del gioco, vi sono 9⋅8 combinazioni possibili. Tocca
di nuovo al primo giocatore che ora ha a disposizione 7 quadratini
liberi per mettere la sua crocetta. Quindi le combinazioni possibili
sono 9⋅8⋅7. Continuando così, è facile constatare che le partite
possibili sono date da = 1⋅2⋅3 ⋅...⋅7⋅8⋅9 = 362.880. Osserviamo, di
passaggio, che il prodotto dei primi numeri interi n, si scrive col
simbolo n! Quindi in questo caso le partite possibili sono 9!
FIGURA 1.16 Partita di tris che si conclude con vincita del primo
giocatore
FIGURA 1.17 Partita di tris che si conclude con una patta
La descrizione di tutte le possibili sequenze del gioco del tris va
fatta con il cosiddetto albero di gioco, e cioè disegnando con un
grafico ad albero rovesciato la successione delle diverse situazioni

che si possono verificare. Di fatto, si tratta di un grafico
impossibile da realizzare concretamente, per il gran numero di
combinazioni. Soltanto disegnando le prime tre scelte (e non tutte
e nove) e alcune delle successive si ottiene il grafico della FIG. 1.18
che è già talmente intricato da imporre di fermarsi subito!
Tuttavia, i matematici riescono spesso a ricavare informazioni
dalla struttura dell’albero di gioco anche senza disegnarlo. In
questo caso, il numero 9! delle partite possibili è teorico, perché di
fatto molte partite finiranno prima, come quella in FIG. 1.16, in cui
riempire le due ultime caselle è inutile: la vittoria è già assicurata.
Così la patta in FIG. 1.17 è inevitabile alla terzultima mossa, per cui
le ultime due caselle sono state riempite inutilmente. Un calcolo
meno facile permette di stimare il numero delle partite effettive:
255.168. Di queste le patte sono ben 46.080, e quindi tantissime. È
quindi evidente che un minimo di attenzione da parte di
entrambi i giocatori conduce alla patta. È anche possibile
determinare delle procedure per ottenere quantomeno una patta:
diciamo “quantomeno”, perché si può sempre sperare che
l’avversario commetta qualche errore madornale. Per questo
possiamo dire che, in linea di principio, il gioco del tris è banale e
noioso: difatti, esso continua ad essere interessante soltanto per
dei bambini piccoli. Se mettessimo l’uno di fronte all’altro due
calcolatori in cui siano state immagazzinate tutte le varianti di
partita – che non sono poi così tante – il risultato sarebbe sempre
una patta.

FIGURA 1.18 Una parte dell’albero di gioco del tris
Vi sono molti altri giochi “banali” il cui esito non è la patta.
Consideriamo per esempio un altro semplice gioco, detto Nim. Le
sue regole sono le seguenti: si dispongono vari mucchi di
fiammiferi; a ogni mossa un giocatore sceglie un mucchio e ne
toglie n ≥ 1 fiammiferi. Vince chi preleva l’ultimo fiammifero. Le
configurazioni iniziali dei fiammiferi possono essere classificate
in due tipi (non entriamo nei dettagli tecnici che cominciano ad
essere più complicati). Si può dimostrare che vince sempre il
primo giocatore se la configurazione dei mucchi è di un tipo e il
secondo giocatore nell’altro, mentre la patta può essere sempre
esclusa.

Anche un gioco enormemente più complicato dei precedenti
come gli scacchi è banale, anche se è talmente complicato che non
sappiamo in che modo si manifesti questa banalità, e questo è il
motivo per cui il gioco degli scacchi continua ad essere
interessante. Un’affermazione apparentemente così bizzarra
richiede di essere spiegata. Nel 1913 il matematico tedesco Ernst
Zermelo (1871-1953) analizzò le ipotesi su cui esso si basa, pur
non potendo evidentemente dare neppure una lontana idea della
struttura ad albero del gioco (e neppure di quella strategica di cui
parleremo tra poco). Basti pensare a quante sono le mosse iniziali
possibili del bianco e quante sono le risposte possibili del nero
alla prima mossa, per rendersi conto che il numero delle varianti
è stratosferico. Ancora nessuno è riuscito a elencarle e a inserirle
nella memoria di un calcolatore, anche se prima o poi questo
avverrà. Zermelo, senza conoscere effettivamente le varianti del
gioco, dimostrò che gli scacchi sono un gioco determinato. Questo
vuol dire che si può verificare una, e una soltanto, delle tre
alternative: 1) il bianco possiede almeno una strategia che gli
permette comunque di vincere; 2) il nero possiede una strategia
che gli permette comunque di vincere; 3) ognuno dei giocatori
possiede una strategia che conduce inevitabilmente al pareggio.
Bisogna guardarsi dal considerare questa affermazione banale,
come se si dicesse: o vince il bianco, o vince il nero, o si produce
una patta. No: si sta dicendo che esiste almeno una strategia
ottimale 
da 
parte 
di 
entrambi 
i 
giocatori 
che 
conduce
ineluttabilmente a uno, e uno soltanto, dei tre esiti. Solo che non
sappiamo di quale si tratti! Se fossimo in grado di dire, per
esempio, che l’esito è il primo, ciò significherebbe che il bianco è
in condizione di vincere sempre, pur di non fare errori. Se si

trattasse della terza, ci troveremmo in una situazione simile a
quella del tris. Prima o poi, con l’ausilio dell’informatica,
conosceremo la risposta e sapremo in che senso preciso è vero
quel che già sappiamo: e cioè che anche gli scacchi sono un gioco
banale.
Inutile dire che gli scacchi saranno un gioco banale per una
coppia di calcolatori dotati di tutte le informazioni relative a tutte
le possibili varianti. Ma per gli uomini, che giocano in base a
fattori come l’“intuizione”, la “fantasia”, la “sorpresa”, gli scacchi
restano un gioco interessante e degno di essere giocato. Ad ogni
modo, ci troviamo di fronte a una competizione in cui il numero
di varianti, per quanto elevato, è finito, e quindi può essere
dominata dall’intelligenza artificiale, che opera nel finito. Vi sono
però situazioni e competizioni completamente aperte, in cui il
numero di varianti immaginabili è infinito, e qui quei fattori
tipicamente umani di cui si diceva prima – “intuizione”,
“fantasia”, “sorpresa” – hanno un ruolo insostituibile.
Come si è accennato, un altro metodo per rappresentare un
gioco è dato dalla cosiddetta forma strategica o normale ideata per
la prima volta dal matematico francese Émile Borel (1871-1956).
Invece di costruire l’albero di gioco, si pensa di costruire l’elenco
di tutte le possibili strategie che ciascuno dei giocatori può
adottare. Ad esempio, nel caso degli scacchi una strategia
rappresenta una tra le tante sequenze di mosse che il giocatore
potrebbe adottare in una partita. Nel caso del tris una strategia è
l’elenco ordinato di crocette o cerchietti che il giocatore apporrà
nelle nove caselle: abbiamo già visto quante sono le possibili
varianti. Consideriamo il caso di una partita con due giocatori (per
esempio tris o scacchi) A e B. Indichiamo con {s1, s2,…,sn} l’insieme

di tutte le possibili strategie di A e con {t1, t2,…,tn} l’insieme di
tutte le possibili strategie di B. In corrispondenza alla scelta di
una coppia di strategie {si, tj} si ha un esito della partita. Nel caso
del tris gli esiti possibili sono tre: vittoria del giocatore che
disegna crocette, di quello che disegna cerchietti o una patta; nel
caso degli scacchi vittoria del bianco, B, vittoria del nero, N, patta,
P. 
Possiamo 
associare 
a 
tali 
esiti 
un 
valore 
numerico
convenzionale che, nel caso di giochi in cui la vittoria ha un
valore numerico (per esempio, una vincita in denaro), può essere
il valore effettivo di quella vincita o perdita. Per esempio, negli
scacchi la vincita del bianco B può essere espressa con il numero
1, mentre per il nero si tratterà di –1; la vincita N del nero viene
espressa con il valore –1 per il bianco e con il valore 1 per il nero;
mentre la patta P varrà 0 per entrambi. Gli scacchi sono un
esempio di gioco “a somma zero”, ovvero di un gioco in cui la
vincita di un giocatore è uguale e di segno opposto a quella del
secondo giocatore. I giochi che stiamo considerando non sono
soltanto giochi a somma zero ma sono giochi finiti, ovvero tali
che il numero delle strategie è finito e quindi il gioco è “chiuso”.
La forma strategica o normale è una tabella (o matrice) in cui sulle
righe figurano le strategie del primo giocatore, sulle colonne
quelle del secondo e nelle caselle le “vincite” del primo, le quali
individuano anche, cambiando segno, le “vincite” del secondo, in
quanto il gioco è a somma zero. Si noti che, se il gioco non fosse a
somma zero, metteremmo nella casella all’incrocio delle due
strategie si, tj una coppia del tipo (i, j) dove i rappresenta la
“vincita” del primo giocatore e j quella del secondo.
Un semplice esempio è dato dal gioco popolare della “morra
cinese” in cui due giocatori aprono simultaneamente la mano a

forma di “forbici”, di “carta” o tengono chiuso il pugno per
indicare una “pietra”. La regola del gioco è: la pietra vince le
forbici perché le spunta; la carta avvolge la pietra e la vince; le
forbici vincono la carta perché la tagliano; l’incontro di due
oggetti uguali dà luogo a una patta. Vi sono quindi tre strategie
possibili per ciascuno dei due giocatori: s1 = t1 = “pietra”, s2 = t2 =
“forbici”, s3 = t3 = “carta”. Attribuendo il valore 1 alla vincita del
primo giocatore e –1 alla sua perdita (che corrisponderanno
rispettivamente a una perdita e a una vincita del secondo), e 0
alla patta, possiamo costruire la seguente forma strategica, detta
anche matrice dei “pagamenti” (TAB. 1.11):
TABELLA 1.11 La forma strategica del gioco della morra cinese
 
pietra forbici
carta
pietra
0
1
–1
forbici
–1
0
1
carta
1
–1
0
TABELLA 1.12 Una forma strategica
 
t1
t2
t3
t4
s1
–10
0
4 –15
s2
4
12
1 –10
s3
0
1
8
9
s4
6
11 –11 –12

Naturalmente tutto ciò è interessante se è utile a determinare
l’esito ottimale del gioco, il che è quanto dire la coppia di strategie
che daranno luogo a un esito il migliore possibile per entrambi i
giocatori, ovvero quello che produce per ciascuno il minor danno.
Infatti, se siamo alla ricerca di soluzioni ottimali, è da
immaginare che ciascun giocatore agirà nel modo migliore
possibile e inoltre dovrà supporre che il suo avversario farà la
stessa cosa; per cui, sia Borel che von Neumann identificarono il
comportamento “razionale” di un giocatore nella prudenza:
cercare di ottenere il massimo possibile tra le soluzioni peggiori, o
la più modesta tra le soluzioni migliori, supponendo che la ricerca
della soluzione migliore in assoluto (e cioè ignorando il
comportamento dell’avversario) può essere avventata e condurre
a pesanti sconfitte. Non è quindi ragionevole puntare a ottenere il
miglior risultato possibile ai propri occhi, perché in tal modo si
potrebbe subire una pesante sconfitta. Tutto ciò si traduce nel
cercare nella forma strategica un valore che sia il minimo dei
massimi per il primo giocatore e il massimo dei minimi per
l’altro. È facile constatare che il primo esempio che abbiamo dato
(quello della morra cinese) non offre soluzione e quindi, per
quanto semplice, è senza sbocco. Consideriamo invece un gioco
ipote tico a somma zero tra due giocatori, ciascuno dei quali ha a
disposizione quattro strategie (TAB. 1.12).
Il primo giocatore potrebbe essere tentato di scegliere la
strategia s2 in quanto questa gli offre la possibilità di vincere 12.
Ma non è affatto detto che il secondo giocatore sia così
sprovveduto da scegliere la strategia t2: egli può scegliere la
strategia t4 infliggendo così all’avversario una perdita di 10 e
vincendo lui questa somma. Il secondo giocatore si trova in

posizione decisamente migliore. Ma pur scegliendo la strategia t4
non può sperare di ottenere 15, perché il primo giocatore può
rispondere con la strategia s3 minimizzando le sue perdite e la
vincita dell’avversario. Come debbono procedere allora due
giocatori razionali? Il primo giocatore per ogni scelta strategica
determinerà il peggior risultato che gli può capitare in funzione
delle scelte strategiche dell’altro: ciò equivale a considerare per
ogni riga il valore minimo dei pagamenti. Nel nostro caso si tratta
della sequenza {–15, –10, –9, –12}. Quindi, opterà per il male
minore, scegliendo il massimo fra questi minimi, ovvero il
massimo dei minimi, detto maximin; e adotterà la strategia
corrispondente. Nel nostro caso il maximin è –9 e la strategia da
scegliere è s3. Analogamente deve comportarsi il secondo
giocatore. In questo caso i suoi pagamenti sono espressi in valori
di segno opposto. Quindi, prima egli determinerà i massimi (che
sono per lui gli esiti peggiori): nel nostro caso, si tratta di {6, 12, 8,
–9}. Quindi determinerà il minimo di tali massimi, o minimax.
Anch’esso è evidentemente –9. Qui minimax e maximin coincidono,
e quindi il gioco ammette una soluzione, che corrisponde a un
comportamento ottimale da parte di entrambi i giocatori.
Purtroppo questo è un caso molto raro, e infatti, a differenza del
gioco della morra cinese, non corrisponde a un gioco reale cui
potremmo dar senso: la circostanza in cui minimax e maximin
coincidono è molto rara. L’idea introdotta da Borel per venir fuori
da questa impasse fu la seguente: il giocatore non ricorre alle
strategie disponibili con uguale propensione, ma è incline a far
uso di una strategia piuttosto che un’altra. In termini matematici,
ciò si esprime dicendo che egli usa ogni strategia con una
determinata probabilità. Se le strategie di un giocatore sono {s1, s2,

…,sn} egli ricorrerà a s1 con una probabilità p1, a s2 con probabilità
p2,.., a sn con probabilità pn, essendo p1 + p2+…pn = 1. Una strategia
concepita (o “pesata”) in questo modo viene detta strategia mista,
per differenziarla da quelle non probabilizzate, dette strategie pure.
Corrispondentemente, i pagamenti vengono pesati in funzione
delle probabilità con cui intervengono le strategie. È chiaro che si
pone il problema di fornire un’interpretazione accettabile
dell’intervento delle probabilità: che cosa significa che il giocatore
“probabilizza” la propria strategia e con quali criteri lo fa? Una
prima interpretazione può essere quella frequentista, che è basata
su una visione empirica e descrittiva: le strategie miste
esprimono il comportamento medio di un dato giocatore in un
numero elevato di giochi. Per esempio, nel caso della morra
cinese un giocatore può decidere di attribuire maggior peso
all’uscita di “forbici”, se ritiene che in un numero elevato di giochi
“pietra” 
sia 
uscita 
molte 
volte. 
Borel 
propendeva 
per
un’interpretazione di tipo psicologico: l’attribuzione di una data
probabilità a una strategia è la rappresentazione quantitativa del
processo di scelta del giocatore e riflette la sua “abilità”. Secondo
questo punto di vista non c’è ragione di ritenere che un gioco
dovesse avere una soluzione minimax – ovvero che in tutti i giochi
a somma zero a due giocatori minimax e maximin coincidessero
per una scelta opportuna della probabilizzazione delle strategie –
e difatti Borel era convinto che un teorema del genere non fosse
possibile. Egli continuò a inseguire la dimostrazione che un
siffatto asserto fosse falso, ritenendo che la teoria dei giochi
avesse il compito di descrivere caso per caso tutte le situazioni
derivanti dalle diverse abilità dei giocatori. Invece, nel 1928, von
Neumann dimostrò che l’asserto – da quel momento detto
teorema di minimax – era vero (almeno per i giochi finiti a somma

zero e in strategie miste) e sostenne che solo per questo la teoria
dei giochi diveniva interessante, in quanto si dimostrava che un
comportamento “razionale” del tipo sopra descritto conduceva a
una soluzione del gioco che rappresentava la miglior soluzione
possibile per entrambi.
Quale era la visione delle strategie miste coerente con quella di
von Neumann? Si trattava di un’interpretazione che potremmo
definire normativa, basata sul principio che, se un giocatore gioca
in modo regolare, l’avversario può indovinarne le mosse: se a
morra cinese si gioca in sequenza pietra, carta e forbici, presto
l’avversario saprà cosa attendersi. Quindi, la probabilizzazione
delle strategie (ovvero l’introduzione di un elemento di casualità)
è un modo efficace di nascondere le proprie intenzioni e pertanto
è un comportamento razionale. È evidente che i primi due punti
di vista (frequentista e psicologista) sono descrittivi, mentre il
terzo 
è 
normativo, 
ovvero 
prescrive 
come 
comportarsi
razionalmente in modo da ottimizzare l’esito del conflitto:
confondere le carte in tavola, affidandosi al caso, in modo da
nascondere all’avversario le proprie intenzioni.
È quasi superfluo dire che questa problematica è altamente
contro 
versa 
e 
riflette 
la 
difficoltà 
di 
rappresentare
matematicamente i comportamenti soggettivi. Per von Neumann,
il teorema di minimax era soltanto il primo mattone della teoria
che, a suo avviso, doveva svilupparsi in due ordini di idee. In
primo luogo – e questo in coerenza con il punto di vista di Borel –
la teoria dei giochi non era più soltanto la teoria dei giochi di
società (carte, dadi, scacchiere ecc.), bensì la teoria dei “conflitti”
nel senso più ampio del termine: i conflitti di guerra tra due o più
potenze o, in battaglia, tra due eserciti o due flotte; i conflitti

sociali, come quelli tra capitalisti e sindacati, tra imprese che
vogliono ottenere il massimo profitto, tra consumatori e
produttori, tra agenti economici che mirano a ottenere la
massima 
soddisfazione 
delle 
loro 
aspirazioni; 
i 
conflitti
psicologici; la competizione tra specie animali, come quella tra
prede e predatori; e così via. Secondo le vedute di von Neumann,
poiché i metodi matematici in uso nelle scienze fisiche – nella
fattispecie l’analisi matematica – non erano adeguati a trattare
problemi del genere, la teoria dei giochi doveva rappresentare la
“nuova” matematica adeguata a modellizzare i conflitti, in
particolare quelli economici e sociali. Essa avrebbe rappresentato
una nuova rivoluzione nella matematica, dopo quella del calcolo
infinitesimale. In secondo luogo, la teoria si sarebbe dovuta
sviluppare nella direzione cosiddetta cooperativa o coalizionale,
dato che raramente un conflitto – salvo i casi dei giochi di società
– assume le caratteristiche del “tutti contro tutti”. In linea
generale si formano coalizioni: di nazioni, di eserciti, di gruppi
sociali ecc. Quindi, il primo metodo consiste nel far uso del
teorema di minimax riducendo il caso di tre giocatori al caso di
due, supponendo che si coalizzino a gruppi di due; quello di
quattro giocatori sempre al caso di due, considerando le possibili
coalizioni a due e a tre; e così via. Inoltre, von Neumann sviluppò
i fondamenti di una teoria cooperativa troppo complicata per
accennarvi qui.
Le cose andarono in una direzione assai diversa da quella da lui
auspicata. Il matematico statunitense John Nash si propose di
indirizzare la teoria in un senso radicalmente non cooperativo,
introducendo il concetto, che da lui prese il nome, di equilibrio di
Nash. Si tratta di una generalizzazione al caso di n giocatori

dell’idea che sottende il concetto di minimax. L’equilibrio di Nash
è un profilo di strategie (ovvero un insieme di strategie, una per
ogni giocatore) che è definito dalla seguente proprietà: nessun
giocatore può ottenere un esito migliore scegliendo una strategia diversa.
Questo concetto è basato sull’idea di miglior risposta di un
giocatore a tutte le possibili strategie degli avversari: è come se
egli le conoscesse tutte e, senza alcuna interazione con gli altri,
calcolasse nel chiuso di una stanza tutti i possibili esiti in
corrispondenza alla scelta di una strategia da parte sua e di tutte
le altre le scelte altrui. Questo calcolo gli indica la propria scelta
ottimale. L’equilibrio di Nash è l’insieme di tutte queste “migliori
risposte”.
TABELLA 1.13 Il dilemma del prigioniero
 
B non confessa
B confessa
A non confessa
(1, 1)
(20, 0)
A confessa
(o, 20)
(5, 5)
Il concetto di equilibrio di Nash si è rivelato particolarmente
efficace per risolvere dei problemi aperti nelle applicazioni della
matematica all’economia secondo il punto di vista detto
“neoclassico”. Tuttavia, il concetto di soggetto (e, in particolare, di
soggetto economico) che esso comporta ha suscitato numerose
riserve ed è lungi dall’essere al riparo da critiche. Per rendercene
conto, consideriamo il famoso dilemma del prigioniero formulato
nel 1950 da due matematici statunitensi, Merrill Flood e Melvin
Dresher.
Due persone, A e B, sono incarcerate con l’accusa di aver

commesso un omicidio in combutta tra di loro. L’autorità
trasmette loro separatamente (non essendo loro consentito di
comunicare) la seguente proposta: se nessuno dei due confessa,
poiché non esistono prove sufficienti, resteranno entrambi in
carcere per 1 anno per la detenzione provvisoria e reati minori; se
A confessa e B non confessa, A sarà liberato in quanto
collaboratore di giustizia e B sarà condannato a 20 anni di carcere;
viceversa, se B confessa e A non confessa, B sarà liberato in
quanto collaboratore di giustizia e A sarà condannato a 20 anni di
carcere; se entrambi confessano, riceveranno uno sconto di pena
e saranno condannati a 5 anni di carcere. La situazione può
essere schematizzata nella forma strategica contenuta nella TAB.
1.13 (nelle caselle sono rappresentati gli anni di prigione
rispettivamente per A e per B). L’unico equilibrio di Nash è dato
dalla coppia (5, 5). Esso rappresenta per ciascun prigioniero la
“miglior” risposta. Difatti, se A decidesse di non confessare
sperando di cavarsela con un anno di prigione correrebbe il
rischio che B confessi, in tal modo condannandosi a 20 anni di
prigione; e analogamente per B. Quindi, la soluzione più
“razionale” per entrambi è di confessare con la prospettiva di
uscire di prigione o, nella peggiore delle ipotesi, di restarvi per 5
anni.
Una simile idea di razionalità solleva molti dubbi: una persona
assolutamente convinta della propria innocenza difficilmente
accetterà di restare in prigione 5 anni, pur sapendo di non
meritarli; ed è altresì possibile che una mentalità “mafiosa” da
parte di entrambi i prigionieri conduca alla soluzione (1, 1). Infatti,
il dilemma del prigioniero fu ideato da Flood e Dresher proprio
per criticare il concetto di equilibrio di Nash e l’idea di razionalità

che lo sottende. Essi simularono anche un dilemma del
prigioniero iterato, che può anche essere pensato come un gioco
la cui posta è monetaria e in cui la scelta si ripete molte volte ed è
chiaramente influenzata dall’esito della “partita” precedente. Il
risultato di tale simulazione, fatta con giocatori effettivi, fu che la
tendenza era verso l’equilibrio (1, 1), ovvero verso la soluzione che
sembra più naturale; ma la risposta di Nash fu che l’esempio era
inappropriato 
perché 
il 
procedimento 
determinava 
troppe
interazioni tra i partecipanti.
Gli esempi che abbiamo visto – e che sono tratti da tematiche
biologiche, economiche, sociali – mostrano che l’applicazione
della matematica al dominio del “non fisico” solleva problemi
molto delicati, controversi e spesso non presenta la stessa
efficacia della matematizzazione dei fenomeni fisici. Torneremo
su questi temi nell’ultimo capitolo.

2
Dalla fisica di Galileo alla modellistica matematica
Tre fasi storiche nel rapporto tra matematica e natura
Nessuno può mettere in dubbio che la matematica sia nata in
stretta correlazione con le attività pratiche. Basti pensare alla
geometria, che significa “misurazione della terra” e che ha avuto
origine nella necessità di misurare le dimensioni dei campi:
secondo Proclo (412-185 a.C.) le origini del pensiero geometrico
affondano le loro radici nelle pratiche invalse nell’Antico Egitto di
tenere una registrazione della forma e dell’estensione dei campi
prima che le esondazioni periodiche del Nilo li sommergessero, in
modo da poterli poi ricostruire e riassegnare ai legittimi
proprietari. Le prime pratiche di conteggio e poi di numerazione
con 
simboli 
appropriati 
si 
svilupparono 
nelle 
civiltà
mesopotamiche e anche in quella egizia, oltre che in molte altre
in varie parti del mondo. Inoltre, l’uso di tecniche geometriche e
di calcoli aritmetici ebbe un ruolo importante nei primi sviluppi
dell’astronomia e in modo sempre più marcato con il trascorrere
dei secoli. È importante tuttavia osservare che si trattava di un
uso pratico della matematica in cui non era mai presente l’idea
che 
esistessero 
delle 
leggi 
di 
carattere 
matematico 
che
governavano il funzionamento della natura. Lo straordinario
contributo della cultura greca alla matematica che culminò negli
Elementi di Euclide (vissuto nel 300 a.C.) non fece che rafforzare
tale visione. È ben vero che Platone (428-348 a.C.) propugnò una
visione dei concetti matematici come prototipo del suo mondo
delle idee, ma questo punto di vista non prevalse e, soprattutto,

non ebbe conseguenze concrete nel senso di condurre alla
costruzione di una matematica atta a rappresentare delle leggi di
funzionamento del cosmo. Prevalse il punto di vista di Aristotele
(384-322 a.C.) che attribuiva una grande importanza alla
matematica come un sistema di ragionamento logico rigoroso che
si svolge su enti ricavati per astrazione ma che non ha una
relazione di necessità con la natura.
Le varie forme geometriche – punto, retta, piano, cerchio,
angoli, triangoli, quadrangoli ecc. – che sono l’oggetto dello studio
degli Elementi di Euclide sono astrazioni di figure reali con cui ci
imbattiamo nella vita quotidiana, presentate nella loro forma
idealizzata, perfetta e quindi studiate in quanto tali: nascono dal
mondo reale ed esprimono l’esigenza filosofica di un sapere
oggettivo, di una conoscenza intersoggettiva indiscutibile e certa;
ma il percorso inverso, consistente nel mostrare che tutto il
mondo reale si esaurisce in queste figure e quindi che la natura è
matematica, non viene mai compiuto. Anche quando ci si occupa
di ottica geometrica non si può davvero dire che si tratti di fisica,
ma dello studio logico e deduttivo di alcune proprietà
geometriche ricavate dal comportamento dei raggi luminosi,
senza che mai si pretenda di scrivere delle formule mediante le
quali descrivere e prevedere il comportamento dei fenomeni.
Anche l’opera che appare più vicina alle applicazioni, quella di
Archimede (287-212 a.C.), è una scienza basata su postulati e
teoremi, ovvero su una struttura deduttiva simile a quella della
geometria euclidea. Sarebbe falso asserire che i postulati
fondamentali della statica o dell’idrostatica archimedea avessero
una 
base 
sperimentale 
e 
quindi 
fossero 
dimostrati
matematicamente. Archimede è partecipe della visione greca

classica secondo cui prima di tutto viene la speculazione teorica e
solo in seguito l’applicazione pratica. Come ricorda Plutarco (50-
120 d.C.), nella sua Vita di Marcello, Archimede «non faceva alcun
conto delle macchine che aveva costruito per combattere i
Romani e le aveva ideate soltanto per far piacere al re Gerone che
egli aveva chiesto di portare un poco la geometria dalle
speculazioni intellettuali alle cose corporali e sensibili». Anzi, egli
aggiunge che «Archimede ha avuto il cuore così alto e l’intelletto
così profondo (e vi teneva nascosto un tesoro di invenzioni
geometriche) da non degnarsi di lasciare scritta qualche opera sul
modo di costruire queste macchine da guerra e considerando
tutta questa scienza di inventare e comporre macchine, e
generalmente ogni arte che apporti qualche utilità da mettere in
uso, come cosa vile, bassa, mercenaria, egli impiegò il suo spirito
e il suo studio a scrivere solamente cose la cui bellezza e
sottigliezza non fosse in alcun modo mescolata alla necessità»
(Plutarco, Pelopida e Marcello).
Non diversamente per quel monumento dell’astronomia che è
il sistema tolemaico il quale certamente si avvaleva di calcoli
numerici e di grandi tabelle, oltre che del ricorso a figure
geometriche, ma non si pose mai l’obbiettivo di scoprire una legge
del movimento dei corpi celesti, e tantomeno le cause di tale
movimento, come fece Newton con la legge di gravitazione
universale.
Questa condizione della matematica non cambiò fino al
Cinquecento: deperita grandemente come scienza nel primo
Medioevo, in cui la geometria euclidea era dimenticata e la
matematica era soprattutto una disciplina pratica nelle scuole
d’abaco, essa fu di nuovo coltivata nel tardo Medioevo e nel

Rinascimento, con la riscoperta dei grandi classici greci e poi con
lo studio di problematiche nuove stimolate dal contributo
dell’algebra araba (a sua volta di derivazione indiana) che portò
allo studio della soluzione delle equazioni algebriche. Anche se il
loro contributo matematico specifico non fu davvero innovativo,
furono Galileo Galilei (1564-1642) e René Descartes (1596-1650) a
riportare al centro l’idea platonistica della struttura matematica
del mondo, e di una Mathesis universalis (matematica universale, il
termine 
introdotto 
da 
Descartes) 
che 
doveva 
svelare 
e
rappresentare tutti i fenomeni in cui domina l’ordine e la misura.
Ma, per quanto furono brillanti gli sforzi per realizzare l’obbiettivo
di creare una siffatta matematica, sia Galileo che Descartes
restarono troppo legati alla tradizione greca della geometria e
della teoria delle proporzioni. Fu merito di Isaac Newton (1642-
1727), assieme a Gottfried Wilhelm von Leibniz (1646-1716), di
aver raccolto e sviluppato in modo organico i primi tentativi di
creazione di una “nuova matematica” capace di affrontare i
problemi dell’infinito e dell’infinitamente piccolo e quindi adatta
a studiare i fenomeni del moto.
Con i grandi protagonisti della rivoluzione scientifica del
Seicento nasce la seconda fase storica del rapporto tra
matematica e natura, che ha al suo centro lo sviluppo del calcolo
infinitesimale ed è animata da un ottimismo quasi illimitato circa
la possibilità di ottenere le leggi matematiche che governano ogni
fenomeno naturale. Entreremo in maggiori dettagli nei prossimi
paragrafi.
A questo periodo trionfale subentrano le prime crisi. Dapprima
ci si rende conto che la riduzione di tutti i processi fisici a
fenomeni di moto non è perseguibile, e si affacciano tentativi di

riunificazione della rappresentazione della realtà fisica mediante
altri concetti come quello di energia. Ma neppure queste strade
appaiono perseguibili. Tuttavia, la vera crisi di quello che viene
chiamato il riduzionismo classico avviene con la scoperta che i
fenomeni microscopici non si prestano a una descrizio ne
continua, che l’energia cambia “per salti” (“quanti”) e quindi che
lo strumento del calcolo infinitesimale e delle equazioni
differenziali non è più adeguato allo scopo. Viene poi la crisi della
meccanica newtoniana con la relatività di Albert Einstein (1879-
1955) e la meccanica quantistica: la visione unitaria della scienza
è definitivamente crollata e il nuovo modo di fare scienza sembra
essere quello dei modelli, schemi parziali e continuamente
rivedibili adatti a rappresentare aspetti particolari della realtà.
Paradossalmente 
questo 
sviluppo 
non 
segna 
affatto 
un
ridimensionamento dell’uso della matematica nello studio dei
fenomeni, ma, al contrario, una sua espansione a campi (in
particolare quelli dei fenomeni non fisici) che le erano stati più o
meno rigorosamente preclusi.
Pur nei limiti di un’esposizione che non ha nulla del rigore che
richiederebbe la storia della scienza come disciplina, proviamo a
illustrare meglio le tappe che abbiamo delineato.
La concezione della natura nel Seicento
Non il Seicento, bensì il Cinquecento, è il secolo in cui,
attraverso un ampio lavoro collettivo, vengono poste le basi per lo
sviluppo della matematica moderna. Così nell’algebra viene
introdotto l’uso sistematico dei simboli e vengono enucleati i
metodi generali per la risoluzione di un’equazione algebrica, gli
studi geometrici rifioriscono e le opere dei classici greci vengono

studiate sistematicamente. Tuttavia è nel Seicento che si verifica
una vera svolta: perché è in questo secolo che la matematica
muta non soltanto nelle tecniche e nei metodi, ma diventa uno
strumento fondamentale per la conoscenza e il dominio della
natura.
Indubbiamente, un filo conduttore salda la scienza del
Cinquecento a quella del Seicento, ed è l’esigenza di rompere il
quadro della concezione medioevale della natura, per stabilire
una nuova visione della realtà. La concezione medioevale
proponeva una rigida distinzione fra il mondo dei corpi celesti,
sfera inalterabile della perfezione divina, e il mondo terrestre
imperfetto, soggetto a un continuo processo di “generazione” e
“corruzione”. L’“alto” è più nobile, è il luogo dei corpi incorruttibili
e perfetti, mentre il “basso” è il luogo naturale dei corpi imperfetti
e corruttibili. La Terra è il punto più basso e quindi è al centro
dell’Universo. Unico oggetto di una trattazione quantitativa (e
comunque entro limiti ben precisi) è lo studio del moto dei corpi
celesti. La descrizione del mondo terrestre invece consiste in una
descrizione 
qualitativa 
delle 
modalità 
dei 
processi 
di
“generazione” e “corruzione”.
Non si vuol dire con ciò che prima del Rinascimento non vi
fosse interesse per il mondo terrestre e per i processi materiali
che in esso avevano luogo: anzi, il Medioevo fu epoca di grandi
invenzioni. Tuttavia, l’invenzione tecnica, la macchina, era vista
come un artificio, come un mezzo per ingannare la natura e
carpirle i suoi segreti, piuttosto che come l’espressione di una
conoscenza. Quindi, come osserva lo storico della tecnologia
Cardwell:

non deve sorprendere affatto se meccanica e magia furono in qualche modo
confuse e se l’abilità peculiarissima del costruttore di macchine o del loro
inventore venne fatta dipendere da pratiche magiche (Cardwell, 1976, p. 34).
Nel Cinquecento, viceversa, comincia a farsi strada l’esigenza di
comprendere razionalmente come si comporta la natura, di
abbattere il diaframma che separava il mondo terrestre dal
mondo celeste. Questa esigenza si manifesta nell’intreccio
sempre più stretto fra le nuove attività pratiche e le nuove attività
intellettuali. 
Lo 
sviluppo 
dell’artigianato, 
delle 
attività
commerciali e delle invenzioni tecniche si lega all’esigenza di
raccogliere e consolidare le conoscenze conseguite. In questo
contesto, la matematica si afferma sempre più come un potente
strumento di unificazione delle scoperte e dei dati: si pensi ai
problemi 
dell’astronomia 
(che 
portano 
all’invenzione 
dei
logaritmi), ai problemi di ingegneria, della navigazione, ai
problemi legati all’introduzione di nuovi strumenti (come
l’orologio) e così via. Il matematico del Cinquecento non è ancora
uno scienziato, come l’intendiamo noi, ma è spesso anche un
costruttore, un artigiano, un commerciante.
Nel 
Seicento 
questa 
tendenza 
giunge 
a 
un 
punto 
di
maturazione decisivo. Si afferma l’idea secondo la quale non
basta più carpire segreti alla natura, bensì occorre capire come
funziona questa macchina immensamente complessa. Ma per far
ciò bisogna abbandonare la vecchia visione che la voleva distante,
misteriosa e magica. La natura, quella entro cui si muove l’uomo,
la natura terrestre deve essere assoggettata a uno studio che ne
sveli con trasparente semplicità il meccanismo di funzionamento.
Occorre un nuovo concetto di natura che includa il “terrestre” e il
“celeste” non più contrapposti come imperfezione e perfezione. E

la mente umana deve indagare con coraggio tale natura
riconciliata nelle sue parti. Dice Galileo Galilei nel suo Dialogo dei
Massimi Sistemi:
Quanto alla Terra, noi cerchiamo di nobilitarla e perfezionarla, mentre
procuriamo di farla simile ai corpi celesti e in certo modo metterla quasi in
cielo, di dove i vostri filosofi l’hanno bandita.
Tuttavia, nella concezione medioevale, l’esclusione del mondo
terrestre costituiva una garanzia della verità della scienza. La
conoscenza dei moti celesti ha in questa concezione carattere
oggettivo, è certa, è vera perché è garantita a priori. Non deve cioè
essere dimostrata perché essa è soltanto il riflesso delle leggi
divine. All’uomo compete soltanto contemplare e descrivere
questo mondo perfetto e immutabile. Ma se si allarga il concetto
di natura, includendovi anche l’imperfetto e mutevole mondo
terrestre, cadono la garanzia e la coerenza offerte dalla
concezione medioevale. Occorre allora costruire un nuovo
concetto di scienza. Più precisamente occorre definire nuovi
criteri di verità, nuovi criteri di verifica della validità delle leggi
scientifiche: criteri basati sulla ragione umana e non più sulle
certezze divine. È questo il vero nodo dello scontro fra Galileo e la
Chiesa del suo tempo. Ed è proprio in questo contesto che la
matematica assume un ruolo decisivo: essa diventa lo strumento
fondamentale della nuova scienza.
Nell’opera di Galileo Galilei è contenuta una definizione
organica del rapporto che deve intercorrere, nella nuova scienza,
fra lo studio dei fenomeni naturali e la loro rappresentazione
matematica. Nella concezione galileiana la matematica si
presenta in duplice veste: per un verso, essa è alla base delle

operazioni dell’artigiano (come quelle del misurare e del
costruire) e, per altro verso, è una scienza rigorosamente astratta
e perfetta. Si intrecciano quindi in Galileo due tendenze: quella
legata alla tradizione greca e in particolare platonica, che vede
nella matematica una forma di pensiero puro, e la tendenza
rinascimentale, che vede nella matematica uno strumento utile
nelle tecniche artigianali. La sintesi fra questi due punti di vista si
traduce nel tentativo di dare veste e dignità di metodo scientifico
universale alle nuove forme di conoscenza espresse dal mondo
artigianale: proprio quel mondo che costituisce uno degli sfondi
dei dialoghi galileiani. Galileo recepisce pertanto tutte le
esperienze e le attività pratiche che costituiscono la novità della
sua epoca, per inserirle nel contesto di un sapere razionale e
universale, di cui è un modello la matematica greca e in
particolare la geometria di Euclide.
Il punto di partenza di Galileo consiste nell’asserire che la
natura ha un carattere di fondamentale semplicità: il libro della
natura è stato scritto da Dio in termini matematici e geometrici.
La matematica è quindi sì uno strumento apprestato dall’uomo
per fini pratici, ma non soltanto: essa esprime l’intima essenza
del mondo. Il primo passo della conoscenza scientifica consiste
nell’osservazione dei fatti, ma alla matematica compete il ruolo
cruciale di “decodificare” il messaggio percepito dai sensi. In altri
termini, l’apparente complessità dei fenomeni nasconde delle
leggi scritte in un codice che può essere tradotto nel linguaggio
semplice della matematica. La decodificazione dei dati empirici
conduce alla formulazione di un’ipotesi, espressa sempre in
termini matematici, la quale dovrà essere verificata attraverso un
esperimento (il “cimento”). Con questo esperimento la natura

viene interrogata, mediante l’uso di macchine o artifici umani che
la mettano, per così dire, nelle migliori condizioni di rispondere e
di verificare se le ipotesi matematiche avanzate circa le leggi
naturali sono fondate oppure no. Infine, la manipolazione
matematica condurrà alla determinazione di formule semplici,
adeguate a descrivere e spiegare il fenomeno studiato.
L’esperimento ha un ruolo assai importante nella scienza
galileiana: è il momento del controllo, della verifica della legge
matematica mediante la quale si vuol descrivere l’intima essenza
di un complesso di fenomeni. Ma il rapporto fra scienziato e
natura non è diretto: lo scienziato non osserva la natura a occhio
nudo, ma attraverso la lente di uno schema matematico astratto.
Un esempio di uno schema del genere è dato dal piano inclinato
privo di attrito. Pur non esistendo in natura, esso esprime una
situazione “al limite” cui può essere ricondotto un gran numero di
situazioni concrete e di cui la tecnica (in questo caso, la tecnica di
levigazione) può offrire approssimazioni via via più spinte.
Tramite un apparato concettuale e tecnico del genere lo
scienziato interroga la natura circa la validità delle leggi del moto.
Nel caso del piano inclinato studierà la legge del moto di una
sfera perfettamente levigata (priva di attrito) che rotoli su di esso.
La descrizione matematica dei fenomeni sostituisce una
descrizione di tipo “qualitativo” (cioè delle loro proprietà e qualità
materiali) con una descrizione di tipo “quantitativo” (cioè in
termini di quantità, di numeri). Per chiarire bene questo punto
facciamo riferimento alla concezione dello spazio tipica della
scienza moderna da Galileo in poi e che è per noi ormai senso
comune. Quando noi pensiamo alla descrizione del moto di un
corpo nello spazio siamo abituati a pensare che gli elementi

caratterizzanti tale moto siano dei numeri: la velocità della nostra
auto e la sua direzione individuata sulla carta geografica (che è
nient’altro che un modo di tradurre le posizioni spaziali in
numeri); la velocità di una nave e la sua rotta (misurata dalla
latitudine e longitudine istante dopo istante); la traiettoria di un
satellite attorno alla terra e la velocità con cui la percorre; e così
via. In tutto ciò lo spazio è una sorta di scenario, di ambiente
vuoto entro cui si svolgono i fenomeni, uno spazio i cui punti
sono nient’altro che posizioni, in cui non v’è differenza
qualitativa fra luogo e luogo. Lo spazio, da Galileo in poi, è un
continuo geometrico uniforme in cui si misurano le posizioni dei
corpi a partire da un sistema di riferimento dato. Nella fisica
aristotelica, invece, lo spazio è un insieme di corpi i cui luoghi
sono qualitativamente descrivibili e diversi l’uno dall’altro.
Difatti, in tale concezione, lo spazio non è un contenitore vuoto
entro cui si muovono i corpi, ma è un “pieno”. La fisica
aristotelica fornisce leggi di questo tipo: ogni corpo, secondo la
sua particolare essenza, possiede un “luogo naturale”, cioè un
luogo che il corpo, per la sua essenza, è determinato a occupare e
a cui tende a ritornare quando ne sia stato rimosso; come ogni
oggetto che, sollevato dal suolo, tende a ricadervi, se abbandonato
a sé stesso. Non si persegue il fine di una descrizione quantitativa
dei moti (posizione, velocità dei corpi in moto) ma si mira a
stabilire distinzioni qualitative, come quella tra “moti naturali”
(che si verificano da sé, senza interventi esterni) e “moti violenti”
(prodotti da agenti esterni e contro le leggi cui si assoggetterebbe
spontaneamente il corpo). La scienza aristotelica è quindi
qualitativa e finalistica (cioè che descrive le qualità dei fenomeni e i
principi che li determinano, come quello dei luoghi naturali).
Invece in Galileo il moto perde ogni carattere di trasformazione

qualitativa e diviene un fatto puramente quantitativo, non
finalistico e meramente dovuto a cause materiali.
Paradossale percorso intellettuale quello della scienza moderna
che, per avvicinarsi di più alla realtà, alla natura, deve farsi più
astratta! Per impadronirsi delle leggi della natura è necessario
infatti abbandonare le descrizioni qualitative, concrete ed
empiriche della scienza antica e medioevale a favore di
descrizioni quantitative, basate su uno strumento concettuale
astratto qual è la matematica. Occorre compiere il singolare passo
di interporre fra la descrizione scientifica e la natura un
diaframma intellettuale, che è concretamente rappresentato dalla
matematica, e che si esprime in concetti come lo spazio astratto in
cui si verificano gli eventi fisici e in sistemi di immagini come il
piano inclinato senza attrito. Questo passaggio è tuttavia cruciale,
perché esso è alla base dei successi della scienza moderna, perché
quel diaframma intellettuale è la lente mediante cui la “nuova
scienza” riesce a scrutare a fondo nei fenomeni.
Un brano cruciale di Galileo
Il brano che riportiamo, tratto dal Dialogo dei Massimi Sistemi
(Galilei, 1632, pp. 277-83), fornisce un’idea vivida dell’importanza
della svolta che si verifica con Galileo e che è contrassegnata
dall’introduzione della matematica nello studio della natura. In
questo brano, Galileo, nelle vesti di Salviati, discute con
l’aristotelico Simplicio, e mostra come un risultato matematico
“astratto” (in questo caso il fatto che una sfera tocca un piano a
essa tangente in uno e un sol punto) sia adeguato a descrivere la
realtà fisica “concreta”, purché si faccia astrazione degli
“impedimenti della materia”.

Simplicio, dopo aver rimproverato a Platone di essersi troppo
invaghito della geometria, osserva:
Simplicio – [...] perché finalmente queste sottigliezze matematiche, Sig. Salviati,
son vere in astratto, ma applicate alla materia sensibile e fisica non rispondono:
perché dimostreranno ben i mattematici con i lor principii, per esempio, che
sphaera tangit planum in puncto [una sfera tocca un piano in un sol punto]; ma
come si viene alla materia, le cose vanno per un altro verso [...] che tutte poi
vanno a monte quando si viene alle cose materiali e sensibili.
In questo passaggio, Simplicio ribadisce con forza la distinzione
fra verità fisica e verità matematica: la sfera matematica tocca sì
un piano in un sol punto, ma non appena ci si rivolge alle «cose
materiali e sensibili» questa verità crolla per lasciare posto a
un’altra, di diversa natura. Il dialogo così prosegue:
Salviati – [...] Or, per mostrarvi quanto sia grande l’error di coloro che dicono che
una sfera, v.g. [“verbi gratia”, il che è quanto dire “per esempio”], di bronzo, non
tocca un piano, v.g., d’acciaio, in un punto, ditemi qual concetto voi vi
formeresti di uno che dicesse e costantemente asseverasse che la sfera non
fusse veramente sfera.
Simplicio – Lo stimerei per privo di discorso affatto.
Salviati – In questo stato è colui che dice che la sfera materiale non tocca un
piano, pur materiale, in un punto, perché il dir questo è l’istesso che dire che la
sfera non è sfera. E che ciò sia vero, ditemi in quello che voi costituite l’essenza
della sfera, cioè che cosa è quella che fa differir la sfera da tutti gli altri corpi
solidi.
A questo punto Simplicio fornisce la consueta definizione di
una sfera come quella superficie i cui punti si trovano tutti a
eguale distanza (il raggio) da un punto dato (il centro):
Simplicio – Credo che l’esser sfera consista nell’aver tutte le linee rette, prodotte
dal suo centro sin alla circonferenza, eguali.
Salviati – Talché quando tali linee non fussero eguali, quel tal solido non
sarebbe altrimenti una sfera.
Simplicio – Signor no. [...]

Salviati – [...] Or torniamo al nostro proposito: basta che voi intendete, la retta
esser la brevissima [la più breve] di tutte le linee che si posson tirare tra due
punti. E quanto alla principal conclusione, voi dite che la sfera materiale non
tocca il piano in un sol punto: qual è dunque il suo contatto?
Simplicio – Sarà una parte della sua superficie.
Salviati – E il contatto parimente d’un altra sfera eguale alla prima, sarà pure
una simil particella della sua superficie?
Simplicio – Non ci è ragione che non deva esser così?
In tal modo Galileo-Salviati ha sostituito il problema della
tangenza tra una sfera e un piano con quello, del tutto
equivalente, della tangenza tra una sfera e un’altra sfera. Egli ora
mostra, con una dimostrazione geometrica che il lettore può
seguire senza difficoltà, che, se le sfere si toccano non in un sol
punto ma in una parte della loro superficie, ne deriva un assurdo.
Il quale è inevitabilmente conseguenza dell’ipotesi che due sfere
tangenti si tocchino in più di un punto: ipotesi pertanto falsa.
Vediamo:
Salviati – Adunque ancor le due sfere, toccandosi, si toccheranno con le due
medesime particelle di superficie, perché adattandosi ciascheduna di esse
all’istesso piano, è forza che si adattino ancor tra di loro. Imaginatevi ora le due
sfere, i cui centri A, B, che si tocchino e congiungansi i lor centri con la retta
linea AB, la quale passerà per il toccamento. Passi per il punto C, e preso nel
toccamento un altro punto D, congiungasi le due rette AD, BD, si che si
constituisca il triangolo ABD, del quale i due lati AD, BD saranno eguali all’altro
solo ACB, contenendo tanto quelli quanto questi, due semidiametri, che per la
definizione della sfera sono tutti eguali: e così, la retta AB, tirata tra i due centri
A, B, non sarà la brevissima di tutte essendoci le due AD, DB eguali a lei: il che
per le vostre concessioni è assurdo.

La replica di Simplicio, prevedibile e immediata, è che la
dimostrazione di Galileo-Salviati è vera per le sfere matematiche,
non per quelle reali:
Simplicio – Questa dimostrazione conclude delle sfere in astratto, e non delle
materiali.
A ciò Galileo replica così:
Salviati – Assegnatemi dunque in che cosa consiste la fallacia del mio
argomento, già che non conclude nelle sfere materiali ma si bene nelle
immateriali e astratte.
Simplicio – Le sfere materiali son soggette a molti accidenti, ai quali non
soggiacciono le immateriali. E perché non può esser che, posandosi una sfera di
metallo sopra un piano il proprio peso non calchi in modo che il piano ceda
qualche poco, o vero che l’istessa sfera nel contatto si ammacchi? In oltre, quel
piano difficilmente potrà esser perfetto, quando non per altro, almeno per esser
la materia porosa; e forse non sarà men difficile il trovare una sfera così
perfetta che abbia tutte le linee dal centro alla superficie egualissime per
l’appunto.
Anche in questo caso Simplicio risponde nel modo che ci
attendiamo da un aristotelico. Riafferma cioè che la concretezza
della natura è irriducibile all’astrattezza della matematica; che le
sfere materiali per quanto vicine alla perfezione e perfettibili, mai

raggiungono la perfezione ideale delle sfere della geometria. Ma,
risponde Salviati, in tal modo Simplicio entra in una nuova
contraddizione: per dimostrare che una sfera materiale non tocca
un piano materiale in un punto, egli fa uso di sfere che non sono
sfere. Infatti, una sfera è, per definizione, quella superficie i cui
punti sono tutti equidistanti da un punto dato (e Simplicio ha
accettato tale definizione!). Simplicio ammette quindi in via
ipotetica la tangenza in un sol punto di una sfera e di un piano e
poi nega che in realtà ciò possa mai avvenire: ovvero nella realtà le
sfere non esistono. Salviati non nega questo ragionamento ma
mostra (e questo è un passaggio cruciale) che esso non è rilevante
per la nostra questione:
Salviati – Oh tutte queste cose ve le concedo io facilmente, ma elle sono assai
fuor di proposito; perché mentre voi volete mostrarmi che una sfera materiale
non tocca un piano materiale in un punto, voi vi servite di una sfera che non è
sfera e d’un piano che non è piano, poiché, per vostro detto, o queste cose non
si trovano al mondo, o se si trovano si guastano nell’applicarsi a far l’effetto. Era
dunque manco male che voi concedeste la conclusione, ma condizionatamente,
cioè che se si desse in materia una sfera e un piano che fussero e si
conservassero perfetti, si toccherebber in un sol punto, e negaste poi ciò potersi
dare.
Simplicio conferma:
Simplicio – Io credo che la proposizione de i filosofi vadia intesa in cotesto senso,
perché non è dubbio che l’imperfezion della materia fa che le cose prese in
concreto non rispondono alle considerate in astratto.
Salviati – Come non si rispondono? anzi quel che voi stesso dite al presente
prova che elle rispondon puntualmente.
Simplicio – In che modo?
Salviati – Non dite voi che per l’imperfezion della materia quel corpo che
dovrebbe esser perfetto sferico, e quel piano che dovrebbe esser perfetto piano,
non riescono poi tali in concreto quali altri se gli immagina in astratto?
Simplicio – Così dico.

A questo punto giunge la risposta cruciale di Salviati-Galileo:
Salviati – Adunque tutta volta che in concreto voi applicate una sfera materiale
a un piano materiale, voi applicate una sfera non perfetta a un piano non
perfetto e questi dite che non si toccano in un punto. Ma io vi dico che anco in
astratto una sfera immateriale, che non sia sfera perfetta, può toccare un piano
immateriale, che non sia piano perfetto, non in un punto, ma con parte della
sua superficie; talché sin qui quello che accade in concreto, accade nell’istesso
modo in astratto: e sarebbe ben nuova cosa che i computi e le ragioni fatte in
numeri astratti, non rispondessero poi alle monete d’oro e d’argento e alle
mercanzie in concreto.
L’argomentazione di Galileo appare stringente. L’astratto riesce
a riprodurre l’imperfezione del concreto: è possibile pensare in
astratto a una sfera geometrica imperfetta, cioè a una superficie
irregolare. Perché allora, se l’astratto riesce a dominare, a
contenere tutti gli aspetti del concreto, i ragionamenti e i calcoli
effettuati in astratto non dovrebbero corrispondere al concreto? E,
proseguendo nel suo paragone mercantile, di stampo tipicamente
rinascimentale:
Salviati – Ma sapete, Sig. Simplicio, quel che accade? Si come a voler che i calcoli
tornino sopra i zuccheri, le sete e le lane, bisogna che il computista faccia le sue
tare di casse, invoglie ed altre bagaglie, così, quando il filosofo geometra vuol
riconoscere in concreto gli effetti dimostrati in astratto bisogna che difalchi gli
impedimenti della materia; che se ciò saprà fare, io vi assicuro che le cose si
riscontreranno non meno aggiustatamente che i computi aritmetici.
Ecco quindi il punto chiave: difalcare gli impedimenti. Lo
scienziato deve scorgere entro il complicato intreccio dei
fenomeni naturali le leggi, le costanze generali che li regolano,
scartando gli aspetti accessori, secondari rispetto all’oggetto
principale del suo studio. Dietro le sfere materiali irregolari egli
deve saper scorgere le sfere materiali perfette. Dietro un piano
materiale inclinato su cui rotola una sfera materiale imperfetta

deve vedere il modello matematico del piano inclinato senza
attrito su cui rotola una sferetta materiale perfetta. E così via.
Certo, lo scienziato deve saper “difalcare gli impedimenti” in
modo corretto, ovvero deve saper trascurare gli aspetti accessori
rispetto al problema oggetto del suo studio. E Galileo-Salviati
conclude così:
Salviati – Gli errori dunque non consistono né nell’astratto né nel concreto, né
nella geometria o nella fisica, ma nel calcolatore, che non sa fare i conti giusti.
Però quando voi aveste una sfera ed un piano perfetti, benché materiali, non
abbiate dubbio che si toccherebbero in un punto; e se questo era ed è
impossibile ad aversi, molto fuor di proposito fu il dire sphaera aenea non tangit
planum in puncto. [Una sfera di bronzo non tocca un piano in un sol punto.]
Dicevamo che l’argomento di Galileo appare stringente. In
realtà, a ben vedere, esso non lo è così tanto. Si tratta di un
argomento del genere: “non si vede perché non debba essere
così”. E si tratta nientedimeno che dell’ipotesi che il mondo è
matematico. Nonostante ogni sforzo Galileo non riesce a far uscire
tale asserzione dalla sfera delle ipotesi, per quanto ragionevoli.
Per farla trionfare occorre verificarne la validità giorno dopo
giorno, una dimostrazione definitiva è impossibile (come abbiamo
mostrato in Israel, 2011). Nella sua straordinaria onestà
intellettuale Galileo non nasconde di non essere in condizione di
mettere a tacere in modo definitivo le obiezioni di Simplicio e
tronca la discussione al seguente modo:
Salviati – Signori, con vostra pace, mi par che noi siamo entrati in una disputa
non molto più rilevante che quella della lana caprina, e dove che i nostri
ragionamenti dovrebber continuar di esser intorno a cose serie e rilevanti, noi
consumiamo il tempo in altercazioni frivole e di nessun rilievo. Ricordiamoci in
grazia che il cercar la costituzione del mondo è de’ maggiori e de’ più nobil
problemi che sieno in natura [...].

Rimbocchiamoci le maniche e lavoriamo, sembra essere
l’indicazione di Galileo. Realizziamo concretamente giorno dopo
giorno il compito di importanza suprema di cercare la
costituzione del mondo, invece di attardarci in chiacchiere. Ma
l’ipotesi che “il mondo è matematico” su cui poggia tutto il
metodo di Galileo resta indimostrata ed è forse proprio di qui che
possiamo intuire che la crisi cui sarebbe andata incontro la
scienza verso la fine dell’Ottocento era inevitabile.
Newton e la nuova matematica
Se ripensiamo agli esempi e alle considerazioni del primo
capitolo, ci possiamo rendere conto di quanto dobbiamo ancor
oggi al pensiero galileiano. Potremo anche constatare che forme
di ragionamento tutt’altro che ovvie, basate anzi su scelte
concettuali delicatissime, si sono consolidate a tal punto da
filtrare pian piano nel modo di pensare comune. L’accettazione
del ruolo della matematica nello studio della realtà e la chiave per
poter realizzare effettivamente l’uso della matematica, cioè il
saper “difalcare gli impedimenti”: sono questi nodi concettuali di
estrema importanza di cui siamo debitori al pensiero di Galileo.
Galileo ha dunque posto le basi di un programma di enorme
portata. Ma fu Isaac Newton (1642-1727), figura imponente di
scienziato, che realizzò questo programma in due aspetti chiave:
la completa riunificazione della fisica celeste e di quella terrestre
e la creazione dello strumento matematico concretamente adatto
a esprimere le leggi della fisica e cioè il “calcolo infinitesimale”,
anche se quest’ultimo merito è da lui condiviso con il celebre
filosofo Gottfried Wilhelm Leibniz.

La concezione generale di Newton differisce da quella di Galileo
in più di un punto, sia come conseguenza del diverso ambiente
culturale e scientifico in cui egli si formò sia di una visione
filosofico-teologica peculiare ispirata da un misticismo di tinte
kabbalistiche. Come Galileo, lo scienziato inglese ritiene che la
natura possa essere studiata e descritta soltanto attraverso
l’esperienza e che le leggi della natura siano di carattere
matematico. Newton è allievo del matematico Isaac Barrow (1630-
1677) e deve molto alla tradizione fisico-matematica continentale,
ma è anche influenzato dal chimico Robert Boyle (1627-1691), che
aveva indagato la struttura atomica della materia e le differenze
qualitative fra gli atomi che la compongono. Il celebre storico
della scienza Alexandre Koyré (1892-1964) osserva che Newton,
come Boyle, ha una concezione atomistica, ritiene cioè che il libro
della natura sia scritto in caratteri corpuscolari. Come Galileo, però,
Newton ritiene che la sintassi che lega questi caratteri, che dà
senso al testo del libro, sia puramente matematica. “Matematico”
è, per Newton, lo spazio continuo, omogeneo, assoluto (cioè
indipendente dal punto in cui si osservano i fenomeni) in cui la
materia è posta; “matematico” e assoluto è il tempo durante il
quale avviene il moto dei corpi.
I grandi capisaldi dell’opera fisica di Newton si trovano ormai in
tutti i libri di scuola. In particolare, le famose tre leggi del moto o
“leggi della dinamica”.
1) Il principio d’inerzia secondo cui «ogni corpo persiste nel suo
stato di quiete o di moto uniforme rettilineo, a meno che non sia
costretto a cambiare il suo stato da una forza impressa su di
esso».

Questo principio pervade anche l’opera di Galileo, ma non se ne
trova in essa un’esplicita formulazione, se non sotto la forma di
un principio di inerzia circolare (anziché rettilineo). Fu invece
formulato in modo compiuto da René Descartes, nel quadro però
di una concezione meccanica assai diversa da quella di Newton.
2) «La velocità del cambiamento della quantità di moto è
proporzionale alla forza motrice impressa e si svolge nella
direzione della linea retta in cui è impressa la forza».
L’accelerazione è cioè proporzionale alla forza applicata al
corpo. Con il termine “quantità di moto” Newton intende il
prodotto fra massa e velocità. Quindi (poiché si ammette che la
massa di un corpo è costante) la velocità del cambiamento della
quantità di moto è il prodotto fra massa e accelerazione.
L’“accelerazione” è infatti la velocità di variazione della velocità. È
questa la fondamentale seconda legge della dinamica che stabi lisce
un legame causale fra la forza impressa a un corpo e
l’accelerazione che esso assume e specifica che questo legame è
di proporzionalità. Essa si esprime (nel caso più semplice di un
punto materiale) nella formula:
f = ma
dove f è la forza, m la massa del punto, a la sua accelerazione.
Con il linguaggio introdotto nel primo capitolo, a è la derivata
della velocità v del punto, in quanto a è la velocità di variazione
della velocità rispetto al tempo. In simboli scriveremo v. = a.
Poiché v è la derivata dello spazio s rispetto al tempo (cioè s. = v),
a è quel che si chiama la “derivata seconda” dello spazio rispetto
al tempo. Infatti poiché v = s., la derivata di v (cioè v.) si otterrà

derivando s.. Quindi v. è la derivata della derivata di s, che
scriveremo s.. (con due punti, per indicare che si è derivato due
volte). Infine, essendo a = v., ne segue che a = s... L’equazione f =
ma (che si riscrive f = ms..) è un’equazione differenziale. Anzi è
una 
delle 
prime 
equazioni 
differenziali 
mai 
studiate,
storicamente la più importante. Così nell’opera di Newton, per la
prima volta, il problema fondamentale della meccanica viene
tradotto in un problema matematico e ridotto a questo. Si tratterà
infatti, caso per caso, e cioè secondo la forma specifica assunta da
f, di risolvere l’equazione differenziale del moto.
3) «A ogni azione si oppone sempre una reazione uguale».
Oltre a queste leggi, che sono i pilastri della meccanica, Newton
enunciò un altro principio di fondamentale importanza: i corpi
interagiscono fra di loro esercitando una forza l’uno sull’altro.
Questo principio (espresso nella cosiddetta legge di gravitazione
universale) è la chiave mediante la quale Newton realizza
l’obiettivo di unificare fisica terrestre e fisica celeste. Galileo,
infatti, pur avendo posto le basi per tale unificazione, si arresta
dinnanzi all’idea della perfezione geometrica delle orbite
planetarie (che ritiene circolari, secondo la tradizione) senza
indagare le cause del moto. Newton studia non soltanto il moto
dei pianeti ma anche la forza di coesione che lega insieme i
corpuscoli di cui è composta la materia, cercando un elemento di
unificazione di queste due sfere di fenomeni. E fornisce una
rappresentazione matematica delle forze che agiscono fra i corpi,
asserendo che, dati due corpi, ognuno di essi attira l’altro con una
forza proporzionale alle loro masse e inversamente proporzionale
al quadrato della loro distanza. Dette quindi m1 e m2 queste

masse e r la distanza dei due corpi (che supporremo, per
semplicità, essere due punti materiali) la forza di gravitazione
universale si scrive in formula:
dove k è una costante di proporzionalità.
La legge di gravitazione è uno schema descrittivo matematico
suggerito dall’esperienza che non pretende di fornire una
spiegazione della natura dell’attrazione gravitazionale. Restava il
problema di spiegare come si propagasse questa attrazione fra i
corpi e cioè il problema di spiegare la cosiddetta “azione a
distanza”. Per far ciò Newton introdusse l’idea di un etere elastico
attraverso il quale si propagherebbe l’attrazione gravitazionale,
ipotizzando un continuo intervento di Dio nell’Universo al fine di
supplire al rallentamento del moto dei pianeti dovuto alla
presenza dell’etere. Questa non è una stravaganza nell’opera di
Newton: al contrario essa dà una misura della diversità fra la
concezione di Newton e quella di Galileo (e Descartes). La prima è
improntata a una visione mistica che ha le sue radici
nell’interesse coltivato da Newton per le teorie kabbalistiche, che
è fondata sull’idea di un continuo e attivo intervento di Dio nel
creato (il “Divino Operaio”); mentre nella visione cattolica di
Galileo e Descartes, Dio è il “primo motore”, colui che ha avviato
la grande macchina dell’Universo, ma se ne è poi ritirato
lasciando il funzionamento della macchina ai suoi meccanismi
autonomi.
L’opera matematica di Newton è strettamente legata alle sue

indagini sulla natura. Alexandre Koyré ha analizzato con
profondità il senso della svolta che si realizza con l’opera di
Newton: una matematizzazione della fisica, l’abbiamo visto, le cui
premesse sono già nell’opera di Galileo; e, d’altra parte, la
creazione di una nuova matematica più vicina alla fisica, una
matematica concettualmente vicina ai fenomeni del moto, una
matematica dinamica. Nella nuova scienza, scrive il Koyré,
il moto non è il moto dei corpi della nostra esperienza, noi non l’incontriamo
nella nostra vita quotidiana. È il moto dei corpi geometrici nello spazio astratto.
Per questo esso non ha nulla a che vedere con il cambiamento. Il “moto” dei
corpi geometrici nello spazio geometrico non cambia nulla; i “luoghi” in un tale
spazio sono equivalenti e persino identici. È un cambiamento senza
cambiamento [...]. La trasformazione del concetto di moto con la sostituzione, al
concetto empirico, del concetto matematico [...] è inevitabile se dobbiamo
sottoporre il moto al numero per trattarlo matematicamente e costruire una
fisica-matematica. Ma ciò non basta. Reciprocamente la matematica stessa
deve essere trasformata (ed è merito immortale di Newton aver realizzato
questa trasformazione). Gli enti matematici debbono essere, in certo senso,
avvicinati alla fisica, sottoposti al moto e considerati non nel loro “essere” ma
nel loro “divenire” o nel loro “fluire”.
Occorre vedere e capire le curve e le figure della geometria non come costrui-te
a partire da altri elementi geometrici né come ritagliate nello spazio
dall’intersezione di corpi geometrici e di piani, e neppure come rappresentanti
un’immagine spaziale delle relazioni di struttura espresse direttamente dalle
formule algebriche, ma come generate o descritte dal moto di punti e linee nello
spazio. Beninteso, abbiamo a che fare qui con un moto che non ha rapporto col
tempo o, cosa ancor più strana, con un moto che si svolge in un tempo
intemporale, nozione altrettanto paradossale di quella di un cambiamento
senza cambiamento. E tuttavia è soltanto facendo procedere in un tempo
intemporale un cambiamento che non cambia, che noi possiamo trattare – di
fatto oltre che intellettualmente – delle realtà come la velocità, l’accelerazione o
la direzione di un mobile in un punto qualunque della sua traiettoria o,
viceversa, a un istante qualsiasi del moto che descrive questa traiettoria (Koyré,
1983, pp. 32-3).
Con Newton il sogno di sostituire alla fisica qualitativa

aristotelica una fisica quantitativa si avvera concretamente,
anche attraverso la creazione di una matematica appropriata.
Dalla descrizione qualitativa e finalistica dei fenomeni si passa a
una descrizione con mezzi matematici e, precisamente, con
equazioni differenziali.
Le equazioni differenziali si imperniano sul concetto di derivata
rispetto al tempo, che Newton introduce chiamandola “flussione”.
Vediamo, sia pure molto sommariamente, in che modo. Nello
scritto del 1671, Methodus Fluxionum et Seriarum Infinitarum (Il
metodo delle flussioni e delle serie infinite), Newton dichiara di
considerare:
le grandezze matematiche come generate da un moto continuo. Le linee
vengono descritte non mediante addizione di parti, ma per moto continuo di
punti; la superficie per moto di linee; i solidi per moto di superficie; gli angoli
per rotazione dei loro lati; i tempi per flusso continuo e così in altri casi
analoghi. Queste generazioni hanno veramente luogo in natura, e si osservano
ogni giorno nel movimento dei corpi [...]. Considerando dunque che quantità
generate, crescendo in tempi eguali riescono maggiori o minori secondo la
velocità maggiore o minore con cui crescono, ho cercato un metodo per
determinare le grandezze dalle velocità dei moti [...] chiamando flussioni queste
velocità di accrescimento e fluenti le quantità generate.
Fluente è quindi una quantità generata da un moto continuo: si
pensi a un’area che si accresce nel tempo, oppure alla curva
descritta dal moto di un punto. È quello che noi abbiamo
chiamato variabile. Flussione è la velocità con cui è generata la
fluente: e cioè la sua derivata rispetto al tempo. Il problema del
calcolo è impostato nei seguenti termini: data una relazione fra
due fluenti x, y, trovare la relazione fra le rispettive flussioni, x., y.
(la notazione di derivata ottenuta apponendo il punto alla
variabile è di Newton ed è in uso ancor oggi, come si è visto nel

primo capitolo).
Facciamo un esempio. Consideriamo due fluenti x e y e
supponiamo che valga fra di loro la relazione: y è il quadrato di x;
che si scrive y = x2. Sia o un intervallo di tempo; in questo
intervallo la fluente x si accresce del prodotto ox. (il prodotto della
velocità di cambiamento per l’intervallo di tempo) e la fluente y di
oy.. Siccome la relazione fra fluenti vale istante per istante, si ha:
y + oy. = (x + ox.)2.
Sviluppando i calcoli si ottiene:
y + oy. = x2 + 2oxx. + o2x.2
(il ben noto quadrato di un binomio!), e poiché y = x2, si può
semplificare y con x2, ottenendo:
oy. = 2oxx. + o2x.2.
Si può ancora dividere per o:
y. = 2xx. + ox.2.
Ora Newton considera o come una quantità nulla, e ottiene la
relazione fra flussioni: 
(risultato ben noto a chi conosca gli elementi del calcolo
infinitesimale). 
Non 
approfondiamo 
oltre 
questo 
tema,
limitandoci a questo accenno.
La creazione del calcolo infinitesimale non è merito del solo

Newton. La sua opera rappresenta il culmine di una linea di
pensiero e di ricerche che ha illustri esponenti come Cavalieri,
Fermat, Wallis, Pascal, Barrow e Huygens. Inoltre, accanto al
contributo di Newton, si erge quello altrettanto importante e forse
ancor più influente di Leibniz. La genialità e originalità di Newton
consiste nell’aver poggiato lo sviluppo del calcolo infinitesimale,
della “nuova matematica”, sull’idea di un fecondo rapporto con lo
studio della natura. La sintesi concettuale e pratica che egli
stabilisce fra matematica e studio della natura diviene uno dei
pilastri fondamentali della scienza moderna.
Determinismo e meccanicismo
Nel paragrafo precedente abbiamo usato il termine punto
materiale senza ulteriori spiegazioni, affidandoci all’intuizione del
lettore e cioè all’idea intuitiva di un corpo puntiforme, cioè di
dimensioni talmente piccole da essere assimilabile a un punto. In
realtà questa idea intuitiva è approssimativa e persino sbagliata e
il lettore si sarà reso conto che i punti materiali non esistono in
natura... Siamo cioè di fronte a un’astrazione matematica nello
spirito del galileiano “difalcare gli impedimenti”. Il punto
materiale è un concetto astratto analogo al piano inclinato. Non
esiste in natura ma è un’idealizzazione. Spieghiamoci meglio.
Intanto proviamo a darne una definizione. Si tratta di un “punto
geometrico” e cioè, come sappiamo dalla scuola, di una figura
geometrica di lunghezza, larghezza, altezza e quant’altre misure
nulle, insomma di dimensioni nulle. Però tale punto gode di una
proprietà fisica, ha cioè massa non nulla (il che, dal punto di vista
fisico, è alquanto paradossale...).
A cosa serve questo concetto ideale e privo di qualsiasi

corrispettivo in natura? A descrivere tutti quei corpi (e quindi, in
quanto tali, dotati di massa) le cui dimensioni sono piccolissime,
si è detto. Però, questo oggetto “piccolissimo” è privo di senso, a
meno che non si introducano ulteriori specificazioni. Infatti, un
corpuscolo di polvere che si muove nell’aria (e che osserviamo
con l’aiuto di un raggio di sole che entra dalla finestra) è
assimilabile per noi a un punto materiale. Non è così, tuttavia, per
un microbo che vi sta sopra e lo considera il suo pianeta: per lui
sarebbe assurdo trascurarne le dimensioni. Anche in questo caso
occorre però distinguere: tutto dipende dagli scopi del microbo.
Poniamo che egli desideri studiare con il suo microtelescopio il
moto del suo pianeta rispetto a quello degli altri pianeti-
corpuscoli di polvere, come facciamo noi quando studiamo il
moto della Terra nel sistema solare: in tal caso gli è lecito
adottare un punto di vista diverso. In tal caso né a noi né al
microbo interessano i problemi di moto sulla superficie dei nostri
pianeti, né il loro moto di rotazione attorno all’asse. Allora, dato il
loro carattere sferico (supponiamo che il granello di polvere sia
più o meno sferico!), è naturale supporre che la loro massa sia
concentrata in un punto, nel centro della sfera. Ed è naturale
studiare il problema (più semplice) del moto di questo punto,
ovvero di questo punto materiale. In tal caso, non è neppure
necessario supporre che la massa sia piccola: quel che è in gioco è
soltanto la legittimità di pensarla tutta concentrata in un punto.
In definitiva, lo schema concettuale del punto materiale è utile
a rappresentare tutti quei corpi la cui forma oppure le cui
dimensioni possono essere trascurate rispetto al problema in
esame. Pertanto il moto relativo dei corpi celesti (data anche la
loro forma approssimativamente sferica) può essere assimilato al

moto di tanti punti materiali. Non così però se vogliamo
descrivere anche le rotazioni dei corpi celesti su se stessi! Un
corpuscolo 
di 
polvere, 
un 
atomo, 
un 
elettrone 
vengono
frequentemente assimilati a dei punti materiali.
Si è detto che il moto di un punto materiale è descritto dalla
seconda legge della dinamica di Newton:
f = ma.
Per scrivere esplicitamente questa equazione occorre conoscere
tutte le forze agenti sul punto in questione e scriverne
l’espressione al posto del simbolo f; bisogna inoltre conoscere la
massa m del punto. L’equazione è differenziale, poiché contiene
l’accelerazione, che è la derivata della derivata (“derivata
seconda”) dello spazio rispetto al tempo. Quindi, per risolvere
l’equazione, occorre risalire dalla conoscenza di a alla conoscenza
della traiettoria percorsa (della sua forma geometrica nello spazio
e del modo in cui viene percorsa nel tempo). Non possiamo
entrare neanche un poco nei dettagli di come viene fatta questa
risoluzione 
dell’equazione 
(che 
va 
sotto 
il 
nome 
di
“integrazione”). Ci limiteremo a dire che, se l’equazione non è
troppo complicata, questa operazione si può fare. Ma non sempre!
Nei casi troppo difficili, in cui non si riesce a ottenere la soluzione
esplicitamente (cioè mediante una formula), si potrà ricorrere a
calcoli approssimati della soluzione. Oggi il calcolatore è uno
strumento 
insostituibile 
per 
questo 
genere 
di 
calcoli
approssimati.
Comunque (indipendentemente dal fatto che la soluzione sia
ottenibile in modo esplicito ed esatto oppure in modo

approssimato), un importante teorema di matematica dimostra
che, in circostanze generalmente verificate nell’ambito di un
problema come il nostro, la soluzione (che è quanto dire la
traiettoria) esiste in modo unico. Più precisamente, il teorema dice
che, se è noto lo “stato meccanico iniziale” del punto materiale (e
stato meccanico iniziale significa “posizione e velocità nell’istante in
cui inizia la nostra osservazione”), allora l’equazione determina in
modo unico la traiettoria del punto, e la velocità con cui esso la
percorre. L’equazione determina cioè in modo inequivoco tutti gli
stati meccanici futuri del sistema (e permette di ricostruire anche
tutti quelli passati).
Questa non è altro che l’espressione di un principio generale
della meccanica classica, che va sotto il nome di principio del
determinismo. Enunciamolo di nuovo per capirlo meglio: esso dice
che la conoscenza dello stato meccanico presente di un punto
materiale (cioè della sua posizione e della sua velocità), nonché la
conoscenza della legge del suo moto (cioè della forma specifica
che assume la legge f = ma), determina in modo unico tutti i suoi
stati futuri. Determina significa qualcosa di molto rigido: e cioè che
quelle conoscenze di cui sopra ci dicono senza equivoci quale
sarà la posizione e la velocità del punto dopo un secondo, venti
anni o cento miliardi di secoli, e anche quali sono state secoli fa.
Non ci sono alternative per il punto materiale, non è ammesso il
caso, il suo moto è governato, determinato, da un’inflessibile
necessità.
La meccanica classica è tutta intrisa di questo principio
deterministico. Noi l’abbiamo ricavato come traduzione del
contenuto di un teorema di matematica relativo alla soluzione
dell’equazione differenziale f = ma. Ma bisogna fare attenzione a

non cadere in equivoci. Il teorema matematico in sé non contiene
un bel nulla! È ben vero che quando si fa uso di equazioni
differenziali (almeno di un certo tipo di equazioni differenziali) si
descrivono processi di tipo deterministico. E così, la descrizione
che abbiamo dato nel primo capitolo dei processi di crescita di
una popolazione e di altri fenomeni, mediante alcune semplici
equazioni differenziali, è una descrizione deterministica (il che è
un limite della nostra esemplificazione, dovuto a un’esigenza di
semplicità sul piano della tecnica matematica). Ma la scelta di
quella rappresentazione matematica è soltanto un riflesso della
scelta 
di 
descrivere 
un 
processo 
secondo 
uno 
schema
deterministico, o meglio secondo l’ipotesi che si tratti di un
processo deterministico. Non significa che il processo sia de
terministico “per natura”. La matematica restituisce ciò che vi si è
messo dentro ma di per sé non è responsabile di nulla.
Non lasciamoci quindi attrarre minimamente, come invece
troppo spesso accade, dal fascino di una visione magica del
potere della matematica. Gli scienziati classici, con un’onestà
intellettuale 
ineguagliata, 
non 
nascondevano 
le 
premesse
filosofiche su cui poggiavano le loro costruzioni scientifiche: lo
vedremo bene da un brano del matematico francese Pierre Simon
Laplace (1749-1827) che riporteremo fra poco. L’idea che i
fenomeni meccanici fossero retti da un principio deterministico e
cioè non dal caso ma da una legge di rigorosa causalità (per cui
ogni effetto si deve a una causa determinata), era un’idea, un
principio 
generale 
suggerito 
e 
confermato 
dall’esperienza.
Potremmo quindi dire: un principio generale, che è la sintesi di
idee e conoscenze relative a una classe di fenomeni. Il fatto che,
con 
queste 
premesse, 
si 
ottenga 
una 
rappresentazione

matematica che asserisce che il fenomeno è di carattere
deterministico, non è strano per niente... Strano sarebbe il
contrario. O meglio, il contrario sarebbe l’espressione del fatto che
lo scienziato non ha saputo neppure tradurre correttamente in
formule le sue idee.
Abbiamo fatto queste precisazioni perché esse ci permettono di
identificare un piccolo “imbroglio” che spesso ricorre (e da cui
non sarebbe esente il primo capitolo di questo libro, se a questo
punto non ne parlassimo). Nella rappresentazione matematica di
un fenomeno si ammette a volte il suo carattere deterministico
senza dire che si tratta di un’ipotesi, suggerita quanto si vuole
dall’esperienza, ma pur sempre un’ipotesi. Quando ciò succede si
dice poi che lo studio matematico ci fa “scoprire” che il fenomeno
è deterministico. Magia della matematica!... Diciamo piuttosto,
imbroglio di chi espone così le cose; di chi, obbedendo a una
visione piattamente empiristica della scienza, non ha il coraggio
di ammettere che la scienza poggia e avanza anche sulla base di
idee metafisiche e non soltanto sulla base di dati sperimentali. Il
principio del determinismo, come si è detto, è un tipico esempio
di una miscela di idee filosofiche e di conoscenze sperimentali: il
dosaggio delle due componenti della miscela varia secondo i casi,
ma nessuna delle due è mai assente.
La scelta di idee “preconcette” grava pesantemente sulla
semplice esposizione da noi fatta nel primo capitolo. Infatti, noi
abbiamo deciso a priori di cercare una legge deterministica,
abbiamo deciso che il processo di crescita di una popolazione
segue un andamento rigorosamente causale e non casuale.
Abbiamo cioè deciso che, noto il valore iniziale della popolazione
in un dato istante e la sua legge di crescita, il numero di individui

dopo un tempo t qualsiasi è rigorosamente determinato. In verità
il sostegno empirico di questa nostra ipotesi è quanto mai fragile.
Per fortuna i limiti imposti dal carattere divulgativo della
trattazione ci scusano, altrimenti, in una sede scientifica,
avremmo meritato una critica severa.
In meccanica il principio del determinismo ha una base
empirica e sperimentale più solida. Tuttavia anche in questo caso
la componente metafisica è decisiva ed è soprattutto legata al
fatto che il principio del determinismo è stato assunto come
principio generale di tutti i fenomeni meccanici e addirittura di
tutti i fenomeni naturali. In realtà quest’idea non resse neppure
nel campo della meccanica. Ma, per spiegare meglio di cosa si
tratti, torniamo al discorso precedente.
Supponiamo di avere a che fare con un certo numero di corpi
tutti assimilabili a dei punti materiali. Quindi n punti materiali,
dove n è un numero qualsiasi. Su ognuno dei punti agisce una
forza (che è la somma di tutte le sollecitazioni cui è sottoposto il
corpo). Chiameremo fi la forza che agisce sul punto i, dove i è un
numero compreso fra 1 e n. Si suppone cioè di aver
contrassegnato tutti i punti con i numeri 1, 2, 3, ..., fino a n.
Il generico di questi punti è quindi individuato da un indice i
che varia fra 1 e n. f1, f2, ..., fn sono allora le forze che agiscono
rispettivamente sul punto 1, 2, ..., n. In modo analogo, detta mi la
massa del punto i e ai la sua accelerazione, il moto di ognuno dei
punti sarà governato dalla “sua” equazione della dinamica.
Precisamente il moto del punto 1 è descritto da f1 = m1a1, il moto
del punto 2 è descritto da f2 = m2a2, ..., il moto del punto è
descritto da fn = mnan.

Quindi abbiamo un sistema di n equazioni differenziali: 
che, risolto, ci fornisce la legge del moto di ogni punto
materiale, cioèla sua traiettoria e la velocità con cui la percorre,
istante per istante.
Facciamo un esempio. Consideriamo due punti materiali tra cui
agisca soltanto la forza di attrazione gravitazionale. Il sistema
precedente si scriverà: 
(si faccia attenzione: r non è una costante, ma è la distanza fra i
due punti, che quindi varia continuamente).
Questo sistema di equazioni esprime il cosiddetto “problema
dei due corpi” e può servire a descrivere il problema del moto
della Terra e della Luna, oppure della Terra e del Sole, considerati
in entrambi i casi come punti materiali e trascurando
l’interazione con altri corpi celesti. Si tratta di un problema
classico che venne impostato e risolto matematicamente dallo
stesso Newton. Esso può essere generalizzato al cosiddetto
“problema degli n corpi”. Quest’ultimo è il problema del moto di n
corpi (punti materiali) soggetti alle sole forze di attrazione

gravitazionale e può essere applicato alla trattazione del moto di
n corpi celesti (per esempio del sistema solare). Il lettore può
provare a scriverne le equazioni nel caso di tre, quattro o più
corpi.
Si vede così che la legge di Newton consente, in linea di principio,
di descrivere il moto di sistemi molto complicati. Ma diciamo di
più: di ogni sistema meccanico. A una condizione, però: e cioè di
supporre che la materia abbia struttura corpuscolare, sia cioè
formata di atomi o comunque di corpuscoli (invisibili o meno, non
importa) assimilabili a punti materiali. Ma proprio questa
supposizione è stata per lungo tempo niente più che un’ipotesi, o,
se si vuole, un’idea priva di basi empiriche, di conferme dalla
realtà.
Dunque se tutta la materia è corpuscolare, essa è composta di n
punti materiali (con n grandissimo) e il moto di tutta la materia
sarà descritto da un sistema immenso di equazioni differenziali
del tipo fi = miai. Ecco un programma di descrizione deterministica
dei fenomeni del moto.
Questo programma può essere esteso in modo ancor più
ambizioso, a condizione di fare un’altra ipotesi generale e cioè di
ammettere che tutti i fenomeni naturali si riducano a fenomeni di
tipo meccanico. Ammettiamo cioè che ogni fenomeno sia la
conseguenza del moto delle particelle (punti materiali) di cui è
composta la materia. Un esempio tipico di un approccio siffatto è
l’ipotesi che il calore sia conseguenza del moto e dell’urto delle
molecole e quindi che esso sia un fenomeno di natura puramente
meccanica. Gli esempi si potrebbero moltiplicare. Questo
programma è, come al solito, in parte basato su dati empirici (cioè

basato 
su 
fatti 
suggeriti 
e 
più 
o 
meno 
confermati
dall’osservazione) e in parte di natura filosofica. Esso va
tradizionalmente 
sotto 
il 
nome 
di 
“meccanicismo” 
o
“riduzionismo meccanicista”: si tratta cioè di un programma che
tende a ridurre ogni fenomeno a un fenomeno di moto e quindi
tutta la scienza alla meccanica. Non sorprende che esso abbia
affascinato tanti scienziati, colpiti dalla semplicità e dalla
grandiosità delle idee di Newton.
L’espressione forse più esplicita del riduzionismo meccanicista
e della concezione determinista a esso strettamente collegata si
trova nell’opera del fisico-matematico francese Pierre Simon
Laplace. Agli inizi dell’Ottocento, nella Introduzione al suo trattato
sulla probabilità (Théorie analytique des probabilités), egli così
scriveva:
Tutti gli eventi, anche quelli che per la loro piccolezza sembrano non dipendere
dalle grandi leggi della natura, ne sono una conseguenza altrettanto necessaria
delle rivoluzioni del Sole. Per l’ignoranza dei legami che li uniscono al sistema
intero dell’Universo, li si è fatti dipendere dalle cause finali o dal caso, secondo
che si producevano e si susseguivano con regolarità, o senza ordine apparente;
ma queste cause immaginarie sono state successivamente allontanate assieme
ai confini delle nostre conoscenze, e scompaiono completamente di fronte alla
sana filosofia che non vede in esse altro che l’espressione della nostra
ignoranza delle cause vere.
Gli eventi attuali hanno un legame con quelli che li precedono, il quale è
fondato sul principio evidente che una cosa non può cominciare a essere, senza
una causa che la produca. Questo assioma noto col nome di principio di ragion
sufficiente, si applica anche a quelle azioni considerate come indifferenti. La
volontà più libera non può produrle senza un motivo determinante; infatti, se
tutte le circostanze di due posizioni fossero esattamente simili, ed essa agisse
nell’una e non nell’altra, la sua scelta sarebbe un effetto senza causa: essa
sarebbe allora, dice Leibniz, il caso cieco degli epicurei. L’opinione contraria è
un’illusione dello spirito il quale, perdendo di vista le ragioni nascoste della
scelta della volontà nelle cose indifferenti, si persuade che essa si è determinata
da sé e senza motivi.

Noi dobbiamo dunque considerare lo stato presente dell’Universo, come
l’effetto del suo stato precedente, e come la causa del seguente. Una
intelligenza che, in un istante dato, conoscesse tutte le forze che animano la
natura, e la situazione rispettiva degli esseri che la compongono, se fosse cosí
elevata da sottoporre questi dati all’analisi, racchiuderebbe nella stessa formula
i moti dei più grandi corpi dell’Universo e dell’atomo più leggero: nulla sarebbe
incerto per essa, e l’avvenire come il passato sarebbe presente ai suoi occhi. Lo
spirito umano offre, con la perfezione che ha saputo dare all’Astronomia, un
pallido abbozzo di questa intelligenza. Le sue scoperte nella Meccanica e nella
Geometria, unitamente a quella della gravità universale, l’hanno messo in
condizioni di cogliere entro le stesse espressioni analitiche, gli stati passati e
futuri del sistema del mondo. Applicando lo stesso metodo ad alcuni oggetti
delle sue conoscenze, esso è riuscito a ricondurre a leggi generali i fenomeni
osservati, e a prevedere quelli che dovevano seguire da circostanze date. Tutti i
suoi sforzi nella ricerca della verità tendono ad avvicinarlo incessantemente
all’intelligenza che noi abbiamo concepito, ma dalla quale resterà sempre
infinitamente lontano. Questa tendenza caratteristica della specie umana è ciò
che la rende superiore agli animali; e i suoi progressi in questo senso
distinguono le nazioni e i secoli, e costituiscono la loro vera gloria.
Ricordiamoci che in altri tempi e in un’epoca non molto lontana, una pioggia o
una siccità estrema, una cometa che trascinava dietro di sé una coda molto
lunga, le eclissi, le aurore boreali e in generale tutti i fenomeni straordinari,
erano considerati come segni della collera celeste. Si invocava il cielo per
allontanare il loro influsso funesto. Non lo si pregava di interrompere il corso
dei pianeti e del Sole: l’osservazione avrebbe presto mostrato l’inutilità di
queste preghiere! Ma dato che questi fenomeni che si manifestavano e
sparivano a lunghi intervalli sembravano contraddire l’ordine della natura, si
supponeva che il cielo li facesse nascere e li modificasse a suo piacimento, per
punire i crimini terrestri. Cosí la lunga coda della cometa del 1456 sparse il
terrore in Europa, che era già prostrata dai rapidi successi dei Turchi i quali
avevano appena rovesciato il Basso Impero. Questo astro, dopo quattro
rivoluzioni, ha suscitato tra noi un interesse ben diverso. La conoscenza delle
leggi del sistema del mondo, acquisita nel frattempo, aveva dissipato le paure
prodotte dalla ignoranza dei veri rapporti tra l’uomo e l’Universo; e Halley, che
aveva riconosciuto l’identità fra quella cometa e quelle degli anni 1531, 1607 e
1682, annunciò il suo ritorno per la fine del 1758 o l’inizio del 1759. Il mondo
degli scienziati attese con impazienza questo ritorno, che doveva confermare
una delle più grandi scoperte scientifiche mai fatte, e avverare la profezia di
Seneca, quando disse, parlando della rivoluzione di quegli astri che provengono
da enormi distanze: “Verrà il giorno in cui, attraverso uno studio continuo, di

molti secoli, le cose attualmente nascoste si manifesteranno con evidenza; e i
posteri si stupiranno del fatto che verità tanto chiare ci siano sfuggite”. Clairaut
iniziò allora a sottoporre all’analisi le perturbazioni subite dalla cometa per
effetto dei due maggiori pianeti, Giove e Saturno: dopo immensi calcoli, egli
stabilì il suo prossimo passaggio al perielio, verso l’inizio dell’aprile 1759, il che
fu presto confermato dall’osservazione. La regolarità che l’Astronomia ci
mostra nel moto delle comete, ha luogo, senza dubbio, in tutti i fenomeni. La
curva descritta da una semplice molecola d’aria o di vapori, è determinata in
modo altrettanto certo delle orbite planetarie: non vi è altra differenza tra esse
che quella dovuta alla nostra ignoranza (Laplace, 1825, pp. 32-5).
Nonostante il tono trionfale di queste righe, Laplace non si
nasconde, 
né 
nasconde, 
l’enorme 
difficoltà 
di 
realizzare
praticamente questo programma. Questa consapevolezza spiega
perché egli, campione convinto del determinismo e del
meccanicismo, sia stato proprio il creatore del “calcolo delle
probabilità” e cioè dello studio matematico dei fenomeni casuali.
Fenomeni casuali... Nient’affatto! Avrebbe detto Laplace. Tutti i
fenomeni sono rigorosamente determinati. Ma la debolezza della
mente umana è tale che è necessario ricorrere a questa
stampella, a questo rimedio dell’ignoranza, che consiste nel fare
dei calcoli che ci forniscano delle informazioni approssimate circa
i fenomeni su cui è impossibile pronunciarsi con certezza. «La
probabilità – scrive Laplace – è relativa in parte a questa
ignoranza, in parte alle nostre conoscenze».
Torniamo al nostro discorso principale. Per dare un’idea delle
difficoltà che si pongono nel realizzare il programma del
determinismo meccanicistico, due aspetti vanno sottolineati. In
primo luogo, è al di sopra delle capacità di qualsiasi
sperimentatore determinare la forma esplicita delle forze fi se n è
grande (e anche se n è piccolo ma il fenomeno è complesso da
analizzare) oppure se si tratta di un fenomeno microscopico. Ma

anche se si fosse (per assurdo) capaci di determinare le fi in ogni
caso, è comunque al di sopra delle capacità di qualsiasi
matematico, anche munito del calcolatore più potente che si
possa immaginare, risolvere un sistema di equazioni differenziali
come quello che si ottiene se n è molto grande.
Un esempio potrà illustrare ancor meglio le difficoltà del
programma laplaciano. Il problema degli n corpi è un caso ideale,
dato che la forma delle fi è nota, essendo fornita dalla legge di
gravitazione universale di Newton. Potremmo anzi definirlo come
un caso facile, dato che il primo ordine di difficoltà neppure si
pone. Si è detto che il problema dei due corpi è relativamente
facile e fu risolto completamente da Newton. Ma il problema dei
tre corpi ha resistito fino agli inizi del nostro secolo a tecniche
matematiche ben altrimenti sofisticate di quelle di cui disponeva
Newton. Finché il fisico-matematico francese Henri Poincaré
(1854-1912) ha dimostrato che esso non è risolubile in forma
esatta, ma soltanto approssimata. Quindi il programma del
determinismo meccanicista, salvo casi eccezionali, non è
perseguibile neppure in linea di principio.
Concludiamo questo paragrafo con una citazione che illustra
efficacemente come le grandi difficoltà che si presentano nello
studio dei fenomeni divengano immense quando si passa allo
studio dei fenomeni non fisici, come quelli biologici. Nel 1964 il
famoso fisiologo Albert Szent-Györgyi (1893-1986, cui dobbiamo
una cosa tanto importante come la vitamina C) descrisse in modo
pungente le impressioni che aveva ricavato frequentando il
celebre centro di ricerche statunitense di Princeton:
Quando sono venuto presso l’Istituto di Studi Avanzati di Princeton, speravo

che, gomito a gomito con quei grandi scienziati atomisti e matematici, avrei
appreso qualcosa sulla “vita”. Appena spiegai loro che in ogni sistema vivente vi
sono più di due elettroni, i fisici smisero di parlarmi. Con tutti i loro calcolatori,
essi non potevano neppure dire che cosa avrebbe fatto il terzo elettrone. Quel
che è notevole è che l’elettrone invece sa esattamente quel che deve fare. Quel
piccolo elettrone sa qualcosa che tutti i sapienti di Princeton non possono
sapere; non può dunque trattarsi altro che di qualcosa di molto semplice
(Szent-Györgyi, 1964, p. 1278).
La crisi della scienza classica
Se l’adesione al punto di vista deterministico in senso lato fu
quasi totale fino agli inizi del nostro secolo, la fiducia nel
programma “forte” del riduzionismo meccanicista (nella versione
di Laplace) venne messa in discussione da molti illustri scienziati
fin dagli inizi dell’Ottocento. Uno dei punti di vista più
significativi in questo senso è quello espresso dal fisico
matematico francese Joseph Fourier (1768-1830). Nel 1822,
nell’Introduzione al suo trattato sulla Teoria analitica del calore
Fourier (da cui sono tratti i brani che seguono) inizia esaltando
l’importanza dell’opera di Newton nella scienza:
Le conoscenze che i popoli antichi avevano potuto acquisire nel campo della
Meccanica razionale non ci sono pervenute, e la storia di questa scienza, fatta
eccezione per i primi teoremi sull’armonia, non risale al di là delle scoperte di
Archimede. Questo grande geometra spiegò i principi matematici dell’equilibrio
dei solidi e dei fluidi.
Trascorsero circa diciotto secoli prima che Galileo, primo inventore delle teorie
dinamiche, scoprisse le leggi del moto dei gravi. Newton racchiuse in questa
nuova scienza tutto il sistema dell’Universo. I successori di questi filosofi hanno
dato a queste teorie un’ampiezza e una perfezione ammirevoli; ci hanno
insegnato che i fenomeni più diversi sono soggetti a un piccolo numero di leggi
fondamentali che si riproducono in tutti gli atti della natura. Si è visto che gli
stessi principi reggono tutti i moti degli astri, la loro forma, le diversità dei loro
corsi, l’equilibrio e le oscillazioni dei mari, le vibrazioni armoniche dell’aria e
dei corpi sonori, la trasmissione della luce, le azioni capillari, le ondulazioni dei
liquidi, e infine, gli effetti più composti di tutte le forze naturali, ed è stato

confermato questo pensiero di Newton: Quod tam paucis tam multa praestet
Geometria gloriatur (ovvero “La Geometria mena vanto del fatto di riuscire a
conseguire tanti risultati con così pochi mezzi”) (Fourier, 1822).
Tuttavia Fourier avanza una critica serrata del riduzionismo
meccanicista:
Ma, quale che sia l’estensione delle teorie meccaniche, esse non si applicano
agli effetti del calore. Questi formano un ordine speciale di fenomeni che non
possono essere spiegati con i principi del moto e dell’equilibrio.
Fourier rifiuta il punto di vista del riduzionismo meccanicista
perché ritiene che il problema della natura del calore non abbia
interesse in sé. Egli ritiene che non abbia importanza decidere se
il calore vada inteso come un fluido materiale che trasmigra da
una parte all’altra dello spazio o come una conseguenza del moto
di particelle. Da “cosa sia” il calore non discende, secondo Fourier,
né una descrizione né una spiegazione coerenti dei fenomeni del
calore. Quel che interessa è invece studiare le modalità del
fenomeno. Di qui l’affermazione che:
Le cause primordiali non ci sono affatto note, ma esse sono soggette a leggi
semplici e costanti che possono essere scoperte tramite l’osservazione, e il cui
studio è l’oggetto della Filosofia naturale.
L’opera di Fourier è di fondamentale importanza in quanto in
essa si trova una codificazione esplicita di come si forma una
teoria scientifica in forma matematica e delle sue relazioni con il
processo della verifica sperimentale. Potremmo dire che, con il
punto di vista di Fourier, abbiamo un modello veramente
sviluppato della concezione classica del rapporto fra descrizione
matematica ed esperienza.
Il modo di formazione di una teoria scientifica, secondo Fourier,

può 
essere 
così 
schematizzato. 
In 
primo 
luogo 
occorre
determinare le proprietà elementari del fenomeno, e ciò
attraverso osservazioni fondamentali. Nel caso dei fenomeni del
calore, 
le 
osservazioni 
fondamentali 
conducono 
alla
determinazione di tre proprietà di tutti i corpi: capacità,
trasmissione e conduzione. In secondo luogo, occorre enunciare
le leggi fondamentali e costanti del fenomeno nel linguaggio
dell’analisi 
matematica. 
In 
altri 
termini 
si 
deve 
fornire
un’enunciazione delle relazioni quantitative esistenti fra le
qualità 
specifiche 
del 
fenomeno. 
Non 
appena 
ottenuta
l’espressione 
matematica 
delle 
leggi, 
mediante 
equazioni
differenziali, occorre risolvere tali equazioni (integrarle). Lo studio
matematico delle equazioni che esprimono le leggi fondamentali
deve permettere di ottenere «le soluzioni fino alle ultime
applicazioni numeriche, condizione necessaria di ogni ricerca e
senza la quale si arriverebbe soltanto a trasformazioni inutili».
Secondo Fourier, esiste un legame indissolubile fra matematica
e studio della natura. La natura non soltanto «è la sorgente più
feconda delle scoperte matematiche, [... ma] offrendo alle ricerche
uno scopo determinato, ha il vantaggio di escludere le questioni
vaghe e i calcoli senza sbocco».
Il caso di Fourier è interessante perché fa comprendere le
tendenze 
fondamentali 
della 
scienza 
fisico-matematica
dell’Ottocento. 
Vengono 
progressivamente 
abbandonate 
le
interpretazioni più dogmatiche del riduzionismo meccanicista a
favore della difesa dei capisaldi della concezione newtoniana, in
particolare del determinismo inteso in senso lato. Non si tratta
più di difendere a tutti i costi l’idea che i principi della meccanica
possano spiegare ogni fenomeno. Si tratta invece di seguire il

modello di prassi scientifica offerto dalla meccanica, adattando
caso per caso i metodi sperimentali e le tecniche matematiche, e
mantenendo fermo il principio generale di una spiegazione di tipo
deterministico dei fenomeni. Come osserva ancora Fourier, a
proposito della teoria del calore:
I principii di questa teoria sono dedotti, come quelli della meccanica razionale,
da un piccolo numero di fatti primordiali, di cui i geometri non considerano la
causa ma che accettano come risultato delle osservazioni comuni e confermate
da tutte le esperienze. Le equazioni differenziali della propagazione del calore
esprimono le condizioni più generali, e riconducono le questioni fisiche a dei
problemi di Analisi pura, e questo è l’oggetto della teoria. Esse non vengono
dimostrate meno rigorosamente delle equazioni generali dell’equilibrio e del
moto. È per rendere questo confronto più chiaro che noi abbiamo sempre
preferito delle dimostrazioni analoghe a quelle dei teoremi che servono di
fondamento alla Statica e alla Dinamica.
Si può dire, in sintesi, che, con il progressivo abbandono del
progetto eccessivamente rigido di ridurre tutta la scienza alla
meccanica, si fa strada un punto di vista che dominerà fino agli
inizi del nostro secolo, quello dell’analogia meccanica. In altre
parole, si afferma l’idea di imitare il modello della meccanica
newtoniana, di seguire cioè nella costruzione e nello sviluppo
delle altre branche delle scienze fisico-matematiche i metodi
della meccanica. Altri tentativi, come quello di ridurre l’analisi dei
fenomeni fisici al concetto di energia (il cosiddetto energetismo),
ebbero un ruolo assai più marginale. Pertanto, l’approccio alla
Fourier rappresentò il baluardo entro cui il riduzionismo classico
riuscì a difendersi evitando gli eccessi del meccanicismo radicale.
Tuttavia i primi decenni del nostro secolo videro crisi e svolte
drammatiche nella scienza, legate soprattutto all’affacciarsi della
teoria della relatività e della teoria dei quanti. Numerosi sono i

volumi dedicati a questi temi, non aggiungeremo quindi un’altra
sintesi alle tante esistenti.
Ci limiteremo a sintetizzare la portata generale di questa crisi,
citando la descrizione lucida che ne fece un grande scienziato
italiano, Vito Volterra, nel 1906:
Non mi è possibile di passare sotto silenzio e di non ricordare ciò che ogni
attento osservatore conosce già per propria esperienza; cioè che quasi tutte le
discipline scientifiche traversano oggi una grande crisi, crisi delle condizioni in
cui si elaborano, crisi del pensiero filosofico che le informa. Si manifesta la
prima con un singolare contrasto: mentre da un lato il bisogno di raggiungere
un’abilità tecnica rende necessaria la specializzazione e la divisione del lavoro
scientifico, giacché un’intera vita è in taluni casi appena sufficiente per
acquistare quelle attitudini senza le quali nessun progresso positivo è possibile;
dall’altro le diverse discipline si sono talmente compenetrate, che non si
comprende al dì d’oggi come si possa avanzare nell’una senza conoscerne, e
profondamente conoscerne, molte altre e non quelle sole che si ritenevano or
sono pochi anni affini, ma anche delle nuove, rivelatesi ora strettamente
connesse. Il lavoro collettivo che si manifesta più intenso e più diffuso nelle
scienze maggiormente progredite, come l’astronomia, la creazione di grandi
scuole che si aggruppano attorno a uomini di genio, come avviene nei paesi più
avanzati, tendono bensì a coordinare e disciplinare le individuali energie, ma
l’equilibrio da cui solo potrà scaturire benefica quella economia degli sforzi cui
tutti aspiriamo, è ben lungi dall’essere raggiunto. Ciò però non costituisce che
uno degli aspetti con cui si manifesta la crisi a cui abbiamo accennato; l’altro,
che interessa il pensiero filosofico, impressiona e colpisce ancor maggiormente
[...] il periodo storico attuale si differenzia da quelli che precedettero perché,
non le singole ipotesi, ma anche i grandi principii, taluni dei quali non si
discutevano più ed erano universalmente accettati e quasi come dogmi
insegnati, sono divenuti subitamente oggetto di discussione e di critica, mentre
vecchi sistemi, che sembravano da lungo tempo e per sempre seppelliti, a un
tratto inaspettatamente risorgono. Forse agli occhi dei nostri posteri il
momento storico attuale apparirà come a noi quello del Rinascimento, in cui il
concetto del sistema del mondo cambiò la base stessa su cui era poggiato
(Volterra, 1907, pp. 107-8).
Volterra mette in luce con grande efficacia i due aspetti della

crisi: 
quello 
“esterno”, 
che 
riguarda 
il 
cambiamento
dell’organizzazione della ricerca scientifica, e quello “interno”,
che riguarda il cambiamento delle basi concettuali della scienza.
L’aspetto di carattere concettuale investe tutti i capisaldi della
fisica-matematica classica. Infatti la teoria della relatività mette
in crisi i concetti (fondamentali nella meccanica classica) di
spazio e tempo assoluti, di etere, il concetto di eventi simultanei,
e così via. Ma ancor più grave è la portata della teoria dei quanti
che mette in discussione la rappresentazione continua dei
fenomeni, ipotizzando che l’energia vari in modo discontinuo. A
riprova delle ripercussioni che il punto di vista dei quanti ebbe,
ricordiamo quanto scrisse Poincaré nel 1912:
La discontinuità regnerà sull’universo fisico e il suo trionfo è definitivo? Oppure
si riconoscerà che questa discontinuità è soltanto apparente e nasconde una
serie di processi continui? Il primo che ha visto un urto ha creduto di osservare
un fenomeno discontinuo, e noi sappiamo oggi che egli non ha visto altro che
l’effetto di cambiamenti di velocità molto rapidi ma continui. Cercare fin d’oggi
di dare un parere su questo genere di questioni, sarebbe sprecare l’inchiostro
(Poincaré, 1912, p. 127).
E Volterra nell’anno successivo:
Voglio far rilevare l’esistenza di un sentimento che tutti i geometri provano,
benché non se ne rendano conto istante per istante. È nel momento in cui si
teme di perdere un oggetto che si ama e si apprezza nel suo effettivo valore. In
una nota recente, Poincaré, studiando la questione dei quanti, mostra che è
impossibile fare a meno dell’ipotesi che l’energia varii attraverso salti bruschi
ed esclama: “Le leggi fisiche non saranno più suscettibili di essere espresse
mediante equazioni differenziali?”
Questa esclamazione, che racchiude un sentimento di rimpianto molto vivo,
esprime molto bene lo stato d’animo di ogni matematico che potrebbe
sospettare che questo strumento meraviglioso, il calcolo infinitesimale, debba
essere abbandonato nello studio di un fenomeno qualsiasi. È in effetti l’idea del
continuo che ha dominato le speculazioni matematiche e le loro applicazioni
più interessanti e più feconde dalle epoche più lontane fino ai giorni nostri.

Quando le condizioni dei problemi lo hanno permesso, si sono sempre
ricondotti in modo del tutto naturale, talvolta intuitivo e incosciente, i casi di
discontinuità a dei casi di continuità mediante dei procedimenti che possono
essere in generale chiamati di natura statistica. La potenza dei metodi degli
infinitamente piccoli ha sempre avuto il sopravvento nella pratica dei calcoli
sulle concezioni e le ipotesi più probabili relative alla natura stessa del soggetto
(Volterra, 1913, p. 3).
Altrettanto grave fu la crisi determinata dalla meccanica
quantistica che colpì al cuore lo stesso nucleo del determinismo
in meccanica. La meccanica quantistica mostrò infatti che, al
livello microscopico, posizione e velocità di una particella non
possono essere determinate simultaneamente. E cosi colpì la
validità generale del principio secondo cui la conoscenza di
posizione e velocità di un corpo in moto ne determina
l’evoluzione futura (cioè dello stesso principio del determinismo).
Come sono collegati i due aspetti (interno ed esterno) della crisi,
descritti da Volterra nel primo brano da noi citato?
Bisogna 
tener 
conto 
del 
fatto 
che 
si 
produsse 
una
frantumazione crescente del tessuto unitario della scienza
classica. E ciò malgrado ogni sforzo per evitarla. Lo stesso Albert
Einstein, che era diffidente nei confronti della teoria dei quanti, si
adoperò in ogni modo per riproporre una teoria unificata che
offrisse un nuovo punto di vista unitario per la fisica,
ricomprendendo la meccanica classica come caso particolare. Le
cose andarono però in direzione opposta, con una progressiva
perdita di unità della scienza fisico-matematica. Il ruolo centrale
che l’analogia meccanica aveva rivestito nella scienza classica
appariva irrevocabilmente in crisi. Cosa si sostituì ad esso? Il
criterio dell’analogia matematica. L’idea cioè che, di volta in volta,
possono essere utilizzati gli strumenti e le teorie matematiche più

utili allo scopo, purché essi consentano di offrire un’immagine
unificata dei fenomeni non sul piano dei contenuti ma sul piano
formale, o, se si vuole, sul piano linguistico.
Il nuovo criterio divenne quello di costruire, mediante il
linguaggio matematico, descrizioni astratte, valide per molti casi
diversi e quindi capaci di unificarli sul piano dell’analogia della
forma descrittiva. Insomma, il nuovo criterio fu quello di costruire
dei modelli matematici: cioè schemi astratti di contenuti possibili da
riempire di volta in volta di reali contenuti diversi. Modelli capaci,
quindi, 
di 
produrre 
un’unificazione 
nella 
descrizione 
dei
fenomeni, ma soltanto sul piano linguistico-formale. Per chiarire
quanto precede basta ripensare a quanto si è detto nel primo
capitolo circa il concetto di isomorfismo delle leggi.
La modellistica matematica rappresentò quindi un’autentica
svolta nel modo di fare scienza e la sua diffusione coincise con la
crisi progressiva della concezione classica.
L’attribuzione alla matematica di un ruolo come quello sopra
descritto non poteva non avere conseguenze rilevanti. Prima la
matematica era lo strumento con cui descrivere le leggi fisiche,
mediante 
il 
quale 
ricava 
re 
delle 
previsioni 
circa 
il
comportamento dei processi reali, anche sul piano numerico. Ora
essa viene investita di un ruolo nuovo: quello di fornire i mezzi
per la costruzione di sistemi di immagini entro differenti settori
della realtà, e questi sistemi di immagini possono riguardare
fenomeni anche molto particolari e non essere necessariamente
gli unici validi. Tali immagini sono pertanto soltanto schemi e
descrizioni la cui validità va verificata esclusivamente sul terreno
dell’efficacia e dell’utilità e non più su quello della verità, come

pretendeva la scienza classica. Questo mutamento è ben presente
nella 
definizione 
che 
von 
Neumann 
diede 
della 
prassi
modellistica, in quanto essenza del modo moderno di fare
scienza.
La funzione di preservare l’unità della conoscenza scientifica
che era compito del riduzionismo classico viene pertanto
trasferita alla matematica e alla sua capacità di rivelare
isomorfismi e analogie fra fenomeni diversi, riconducendoli a un
unico schema formale. Un ruolo certamente molto più vasto, che
proietta la matematica al di fuori del campo tradizionale della
fisica e apre la via alla modellizzazione di qualsiasi tipo di
fenomeno, senza restrizioni a priori. Ma che è, al contempo, più
modesto e limitato. Difatti, una visione del genere non poteva
non rinunziare a ogni tentativo di stabilire un sistema codificato
di rapporti fra esperimento e descrizione matematica, qual era
presente, per esempio, nell’opera di Fourier. Di volta in volta, caso
per caso, venivano seguiti approcci e metodi diversi. Di
conseguenza 
il 
ruolo 
dell’esperimento 
veniva 
fortemente
ridimensionato.
È questo un punto assai importante sul quale torneremo. Per il
momento vogliamo insistere sul fatto che una conseguenza della
nuova prassi scientifica fu la disgregazione del tessuto unitario
della scienza classica. A questa si accompagnò, come si è detto,
una crisi “esterna”, cioè al livello dell’organizzazione della ricerca
scientifica. Essa si manifestò, come bene descrisse Volterra nel
brano precedente, in una frammentazione del lavoro scientifico,
in un processo di specializzazione sempre più spinto. E così, come
osserva Volterra, mentre la scienza si diramava in tanti rivoli
sempre più distanti fra loro e la ricerca scientifica subiva un

processo di parcellizzazione, l’esigenza di unificazione diveniva
sempre più pressante e al contempo difficile da realizzare. Infatti
l’unificazione prodotta dal linguaggio matematico ha un carattere
più labile e incerto; cosa che non si nascondono neanche i suoi
più convinti sostenitori. Tentiamo ora di approfondire il concetto
di modello matematico.
La modellistica matematica e il concetto di modello
matematico
In epigrafe a questo libro abbiamo già introdotto una
definizione di modello matematico dovuta a von Neumann.
Proviamo a partire da un’altra definizione data dall’economista
matematico Edmond Malinvaud nel libro Méthodes statistiques de
l’économetrie: «Un modello matematico è la rappresentazione
formale di idee o conoscenze relative a un fenomeno»
(Malinvaud, 1964, p. 5).
Questa definizione contiene una descrizione completa delle
caratteristiche di un modello matematico, che possono essere
raccolte in tre punti fondamentali, non separabili l’uno dall’altro.
Precisamente:
un modello matematico è la rappresentazione di un fenomeno;
tale rappresentazione non è discorsiva o a parole, ma formale,
espressa cioè in linguaggio matematico;
non esiste una via diretta dalla realtà alla matematica. In altri
termini il fenomeno specifico studiato non determina la “sua”
rappresentazione matematica; l’approccio modellistico consiste
piuttosto nel tradurre in formule idee e conoscenze relative al

fenomeno.
Discutiamo meglio questi tre punti. Sul primo non c’è molto da
dire: esso non fa altro che individuare il campo di cui ci stiamo
occupando e cioè l’analisi scientifica dei processi reali. Un
modello matematico è, per l’appunto, la rappresentazione (o
descrizione) di un fenomeno. Non si tratta però di una semplice
descrizione verbale. Il modello matematico è una descrizione che
mette in luce determinati aspetti caratteristici di un fenomeno in
termini formali: è la logica del processo che viene analizzata. Ed
ecco il secondo punto. La descrizione che offre il modello non è
infatti una descrizione contenutistica ma è una descrizione che
utilizza il linguaggio formale e astratto per eccellenza, il linguaggio
della matematica. Insistiamo su questa contrapposizione fra
descrizione a parole o di contenuto e descrizione in linguaggio
formale e quindi matematico.
Arriviamo quindi al terzo punto che è il più delicato e
importante di tutti. Perché Malinvaud dice che un modello
matematico è la rappresentazione formale di «idee o conoscenze
relative a un fenomeno»? Perché non dice semplicemente che è la
rappresentazione formale di un fenomeno? L’esame di un aspetto
della realtà non suggerisce in alcun modo come esso debba essere
descritto matematicamente. In modo più sintetico e rozzo, ma
abbastanza suggestivo, potremmo dire che non esi ste una via
diretta, una sorta di autostrada, che conduce in modo univoco
dalla realtà alla “sua” descrizione matematica. E questo per tante
ragioni. In primo luogo perché la realtà è costituita da un intrico
talmente complesso e inestricabile di fenomeni, da impedire una
descrizione relativamente semplice e schematica qual è quella
matematica. Occorre inevitabilmente discernere in quest’intrico

ciò che vogliamo individuare come oggetto della nostra indagine.
Anche in un fenomeno relativamente semplice come il moto di
un proiettile lanciato da un fucile, si intrecciano una miriade di
aspetti e questioni diversi, che è impossibile conoscere tutti
insieme: la velocità del proiettile, la sua traiettoria, il tempo che
occorre affinché colpisca un bersaglio dato, la dipendenza di
queste e simili grandezze dall’attrito dell’aria, dalla forma del
proiettile, dalla forza impressa al momento del lancio, l’effetto del
vento e così via. E questi sono soltanto gli aspetti dinamici della
questione. Ma potremmo voler conoscere le modificazioni di
forma subite dal proiettile nello spazio e nell’impatto, le
modificazioni chimiche della sostanza di cui è composto e così
via. Non riusciremmo a stendere una lista completa. Se poi il
processo ha aspetti biologici, esso si complica di un’infinità di
altri problemi. Una lepre che corre su un prato non è soltanto un
oggetto in moto, come il proiettile: la sua velocità, il suo moto
dipendono in generale dal suo stato fisiologico. Per sapere tutto
della lepre che corre sul prato dovremmo scrivere sulla scheda
della descrizione scientifica del fenomeno, il numero dei battiti
del cuore della lepre, la composizione chimica del sangue e le sue
variazioni nel tempo ecc. ecc. E, fra l’altro, non descriveremmo
così la lepre ma una lepre. Tornando poi al proiettile, abbiamo già
commesso un grave abuso ricavando risultati su di esso, come se
fosse il prototipo del proiettile; cioè trascurando arbitrariamente le
differenze fra proiettili e proiettili, inevitabili anche nel più
perfezionato dei processi di fabbricazione. Si dirà che queste
differenze sono irrilevanti: sia pure, ma nel decidere questa
irrilevanza noi abbiamo già cominciato a sollevare un primo sia
pur sottilissimo diaframma di idee fra la realtà e la nostra
descrizione scientifica. Il diaframma di una nostra idea, quella

secondo cui le minime differenze di forma e struttura chimica
esistenti fra i singoli proiettili studiati sono irrilevanti ai fini dello
studio della loro dinamica.
Una descrizione della realtà perfettamente aderente ad essa
sarebbe un interminabile (quanto impossibile) discorso aderente a
tutte le pieghe dei fatti, a tutte, nessuna esclusa. E ciò sarebbe
non soltanto impossibile, perché troppo complicato, ma anche
inutile: il contrario esatto della rappresentazione formale cui
aspiriamo. Per descrivere un fenomeno dobbiamo quindi fare
delle scelte, selezionarne degli aspetti. Ma, nel richiedere una
rappresentazione formale 
selettiva, e 
non un’enciclopedia
descrittiva infinita, noi abbiamo già fatto un passo, oltre i punti a)
e b), nel punto c): questa scelta richiede infatti di mettere in
campo le nostre idee.
Le idee che entrano in gioco sono in primo luogo relative a cosa
si vuole descrivere. Nello scegliere questo aspetto da descrivere,
nell’isolarlo dai tanti altri che compongono l’intreccio in cui ci si
presenta il fenomeno, noi dobbiamo isolarlo in modo corretto.
Non dobbiamo cioè considerare come secondari aspetti che sono
invece fondamentali per l’oggetto che ci proponiamo di studiare o
viceversa. Ma non è sufficiente fare in modo corretto questa
operazione di isolamento. Infatti, il fenomeno non contiene la
legge, come una scatola contiene un oggetto, e il nostro compito
non può ridursi allo sforzo di aprire la scatola per scoprirne il
contenuto. Già lo stesso concetto di “legge del fenomeno” è una
nostra creazione mentale. Interviene qui in modo decisivo
l’osservazione empirica e l’esperimento (che è una forma
organizzata e metodica di osservazione empirica) per formarci
un’idea e costruire un’ipotesi circa la legge che dovrebbe

governare il processo in esame. Quel che noi facciamo quindi è di
mettere insieme tutte le conoscenze empiriche e le idee che ci
siamo fatti del fenomeno, per costruirne una rappresentazione
matematica, che si tradurrà per lo più in formule o equazioni di
vario tipo. Abbiamo così ottenuto un modello matematico del
processo reale in oggetto.
Si noti che quanto precede non s’identifica, se non in parte, con
il procedimento galileiano di “difalcare gli impedimenti”, perché
per Galileo – come per gli scienziati classici – il mondo è
strutturato sulla matematica, per cui la via che conduce alla
rappresentazione 
matematica 
dei 
fenomeni 
è 
unica, 
la
rappresentazione che ne risulta è univoca. Il modello è invece
soltanto espressione di alcuni aspetti, di alcune idee circa un
complesso di fenomeni che abbiamo deciso di isolare e di
rappresentare in modo formale. Nella fisica classica non esistono
due leggi della dinamica. La fisica moderna ha derubricato la
fisica classica a modello particolare accanto ad altri.
Qui è necessaria un’altra osservazione che non è ristretta
all’approccio modellistico, ma anzi deriva da quello classico. È
sufficiente il modo di procedere sopra descritto a garantire la
validità del modello e cioè la sua adeguatezza a descrivere il
fenomeno studiato? No di certo, visto che tutte le idee e le
conoscenze che noi abbiamo utilizzato per costruirlo possono
aver introdotto delle discrepanze dalla realtà più o meno
accettabili. Ecco che allora la fase di verifica del modello è più che
mai indispensabile. Per verificare il modello occorrerà dedurre
dalla sua struttura matematica la previsione che esso fornisce
circa il comportamento del fenomeno. Sarà compito del
matematico esplicitare dal modello queste conseguenze, cioè

risolverlo. Occorrerà poi confrontare queste previsioni con i dati
reali, sia che si tratti di dati statistici, sia che si tratti di un
esperimento eseguito appositamente per verificare la bontà del
modello.
Possiamo riassumere il tipo di problemi che nascono nel
mettere in opera lo strumento matematico nella descrizione dei
fenomeni, citando quanto scrisse con lucida sinteticità Vito
Volterra nel 1906:
Lo studiare le leggi con cui variano gli enti suscettibili di misura, l’idealizzarli,
spogliandoli di certe proprietà o attribuendone loro alcune in modo assoluto e
lo stabilire una o più ipotesi elementari che regolino il loro variare simultaneo e
complesso; ciò segna il momento in cui veramente si gettano le basi sulle quali
potrà costruirsi l’intero edificio analitico. Ed è allora che si vede rifulgere tutta
la potenza dei metodi, che la matematica largamente pone a disposizione di chi
sa usarli [...]. Plasmare dunque concetti in modo da poter introdurre la misura;
misurare quindi; dedurre poi delle leggi; risalire da esse ad ipotesi; dedurre da
queste mercé l’analisi, una scienza di enti ideali, sì, ma rigorosamente logica;
confrontare poscia con la realtà; rigettare o trasformare, man mano che
nascono contraddizioni fra i risultati del calcolo ed il mondo reale, le ipotesi
fondamentali che han già servito; e giungere così a divinare fatti ed analogie
nuove, o dallo stato presente arrivare ad argomentare quale fu il passato e che
cosa sarà l’avvenire: ecco, nei più brevi termini possibili, riassunto il nascere e
l’evolversi di una scienza avente carattere matematico (Volterra, 1906, pp. 10-2).
Questo è quanto suggerisce la definizione di Malinvaud. È facile
anche constatare che questa definizione è assai simile a quella
data da John von Neumann. Ricordiamola ancora una volta: «Per
modello s’intende un costrutto matematico che, con l’aggiunta di
certe interpretazioni verbali, descrive dei fenomeni osservati. La
giustificazione di un siffatto costrutto matematico è soltanto e
precisamente che ci si aspetta che funzioni – cioè descriva
correttamente i fenomeni in un’area ragionevolmente ampia.
Inoltre esso deve soddisfare certi criteri estetici – cioè, in

relazione con la quantità di descrizione che fornisce, deve essere
piuttosto semplice». Quindi anche von Neumann dice che un
modello è una rappresentazione di un complesso di fenomeni in
termini formali, ovvero matematici. E quando osserva che «le
scienze non cercano di spiegare, a malapena tentano di
interpretare, ma fanno soprattutto dei modelli», ci ricorda che un
modello è soltanto una rappresentazione formale di idee e
conoscenze di un fenomeno ma non pretende di esaurirne il
significato e neppure di interpretarlo, ma soltanto di dare
un’immagine di alcuni suoi aspetti – per l’appunto quelli che
abbiamo scelto di selezionare.
La definizione di von Neumann dice in modo esplicito quel che
differenzia l’approccio modellistico da quello del riduzionismo
classico. Essa dice che la prassi modellistica è l’approccio
caratteristico della scienza moderna. In primo luogo, von
Neumann dice che, proprio per i limiti intrinseci dei modelli, essi
non vanno valutati sul piano della loro verità quanto su quello
della loro efficacia, cioè sul piano della loro capacità di fornire
un’immagine 
corretta 
e 
utile 
dei 
fenomeni 
in 
un’area
«ragionevolmente ampia». Ci dice inoltre che i modelli debbono
soddisfare dei «criteri estetici», ovvero soddisfare quel che
potremmo chiamare un buon “rapporto costi-benefici” fra
l’esigenza della semplicità e quella dell’efficacia nella descrizione.
Difatti, una descrizione molto precisa e aderente ai fatti può
essere eccessivamente “costosa” in termini di complessità del
modello, il quale può risultare matematicamente intrattabile;
mentre un modello molto semplice e matematicamente facile
può risultare troppo rozzo e incapace di descrivere efficacemente
i 
fatti. 
Spetta 
al 
buon 
modellizzatore 
individuare 
con

ragionevolezza la via giusta fra questi due estremi.
Come si vede, sia la definizione di Malinvaud che quella di von
Neumann non sono definizioni formali ed “esatte” e tantomeno
forniscono delle ricette automatiche per la costruzione di modelli.
Ogni tentativo in tal senso condurrebbe soltanto a una perdita di
tempo e a inutili illusioni, purtroppo malamente coltivate da certi
cultori della filosofia analitica.
Riflettiamo piuttosto ancora un poco sui modelli studiati nel
primo capitolo. In ognuno dei problemi affrontati abbiamo dovuto
fare delle ipotesi semplificatrici più o meno pesanti, spesso molto
pesanti, per poter isolare il fenomeno che ci interessava: non
dobbiamo insistere molto su questo punto perché queste ipotesi
sono state discusse in dettaglio. Mentre le facevamo, però,
avvertivamo dei rischi che ne sarebbero derivati in termini di
scarso realismo del modello.
Numerose osservazioni potrebbero essere inoltre fatte sullo
schema matematico scelto per descrivere un fenomeno. Nel
nostro caso, per ragioni di semplicità, per non moltiplicare le
difficoltà tecniche, si è ricor so a un unico strumento: quello delle
equazioni differenziali. Si tratta però di uno schema descrittivo
che è soggetto a due vincoli ben precisi. Il primo è stato già
evidenziato: tutte le variabili in gioco debbono essere pensate
come continue, anche se in realtà il processo avviene a salti,
come nel caso della crescita di una popolazione o della diffusione
delle innovazioni tecnologiche. Ma v’è un altro vincolo, di cui è
più difficile spiegare la ragione, e che quindi il lettore deve
accettare senza che sia possibile darne una dimostrazione. Le
equazioni differenziali (del tipo da noi considerato) possono

descrivere soltanto “processi deterministici”, e cioè processi in cui
l’evoluzione del sistema è determinata in modo univoco dallo
stato iniziale del sistema stesso. In parole povere, questo significa
che il sistema può evolvere in un sol modo, senza alternative, a
partire dal suo stato presente. Se, per esempio, la popolazione
mondiale è composta oggi di N individui e la sua legge di crescita
è N˙(t) = kN(t), per un certo k, il suo numero dopo x anni è dato
dalla legge in modo inequivocabile.
Questa è una proprietà comunemente ammessa per i sistemi
meccanici macroscopici; talmente riconosciuta che, se non la si
credesse fermamente valida, nessuno avrebbe mai osato spedire
un solo razzo e tanto meno un uomo sulla Luna. Infatti, noi siamo
certi che le leggi del moto prevedono in modo inequivoco la
posizione e la velocità di un corpo in moto in un qualsiasi istante
successivo a quello in cui iniziamo l’osservazione. Ma che dire di
un sistema biologico? L’idea che un sistema biologico sia
rigidamente deterministico è lungi dall’essere accettata senza
discussioni. Non parliamo poi dei sistemi sociali ed economici in
cui le scelte dei singoli soggetti distruggono ogni tentativo di
rappresentazione deterministica, e impongono di ammettere
l’intervento di aspetti imprevedibili che, quantomeno, potrebbero
essere pensati come eventi casuali. Descrivere tali processi come
rigorosamente deterministici è una forzatura che può essere
accettabile soltanto se è sostenuta da forti e valide ragioni e
soprattutto da un buon accordo con i dati reali.
Veniamo ora all’aspetto più complesso: quello della verifica.
Verifica del modello significa confrontare le sue predizioni
(ovvero ciò che esso prevede in merito all’andamento del
processo in esame) con i dati reali del processo stesso. Ma come

avviene tale confronto? L’ideale sarebbe poter fare degli
esperimenti, come si fa in fisica, e confrontare le predizioni del
modello con i risultati dell’esperimento. In alcuni casi ciò è
possibile, come si è visto nel caso della dinamica di popolazioni
animali. Ma nessuno mai si sognerebbe di fare esperimenti sulla
dinamica di po polazioni di elefanti o addirittura di uomini! In
casi come questi, ciò di cui possiamo disporre sono statistiche
relative al passato e non qualcosa di generale e universale come
un esperimento. Comunque si potrebbe dire che in questo settore
della biologia matematica (come in molti altri) è possibile far uso
di un sistema di verifica misto (e cioè del confronto sperimentale,
ove possibile, e del confronto con i dati statistici).
Nel caso delle scienze sociali ed economiche la situazione è di
gran lunga più difficile. Qui parlare di esperimento è quasi
ridicolo. Chi si sognerebbe di proporre un esperimento sullo
sviluppo, poniamo, dell’industria elettronica in Italia? A meno che
l’esperimento non venga inteso come un tentativo di verificare
direttamente 
una 
teoria 
economica 
circa 
l’innovazione
tecnologica, attraverso la sua applicazione diretta in termini di
politica economica. Sperimentare sull’economia (o, più in
generale, sulla società) può soltanto voler dire pianificare
l’economia (o la società), cioè disciplinarla secondo regole
prefissate. In questo senso l’esperimento in economia può essere
una cosa tutt’altro che ridicola. Al limite si potrebbe affermare
che la divergenza fra coloro che ritengono possibile sperimentare
in economia e coloro che non lo ritengono possibile riflette una
divergenza fra la fiducia nella pianificazione economica integrale
e la fiducia nel libero gioco delle forze del mercato. Ma è chiaro
che, ove non si abbia fiducia nell’intervento pianificatore, l’unico

confronto possibile fra realtà e modelli (o teorie formali) è a
posteriori, ovvero è un confronto fra teorie e modelli, da un lato, e
passato storico, dall’altro.
Grava poi su tutte queste questioni il problema non meno
delicato dei metodi della rilevazione statistica dei dati, della loro
selezione, discussione ecc. Problema molto serio e da articolare
settore per settore, rispettando la specificità dell’oggetto. Come in
fisica si dispone di metodi e regole precisi per la determinazione
quantitativa delle grandezze che descrivono il fenomeno studiato,
così in biologia matematica o in economia matematica occorrerà
disporre di metodi rigorosi per la rilevazione dei dati reali.
Tuttavia, in fisica, l’esperimento e il confronto con i dati empirici
è costantemente guidato dalla presenza di leggi di valore generale.
Ma è ben evidente e incontestabile che in economia noi non
disponiamo di leggi analoghe a quelle della gravitazione
universale, della dinamica o dell’elettrodinamica: non vi sono
leggi in economia (Israel, 2007b). In questo vuoto, può essere
grande la tentazione di sostituire le leggi con delle correlazioni
ricavate dall’analisi dei dati statistici. È sufficiente la lettura dei
quotidiani per rendersi conto di quanto sia diffusa la tendenza a
enunciare vaghe e discutibilissime correlazioni, e poi a elevarle
incautamente al rango di “leggi scientifiche” per poi trarne
conclusioni a dir poco azzardate. (Un esempio tipico è dato dalla
leggerezza con cui viene “determinata”, con una precisione
persino alla seconda cifra decimale, la percentuale dei decessi
“causati” da insufficienza respiratoria per inquinamento o
“causati” dall’ingestione di una quantità “eccessiva” di grassi).
Abbiamo citato spesso la fisica e la ragione di ciò dovrebbe
essere chiara. Infatti, l’applicazione della matematica allo studio

dei processi reali è giovane o giovanissima in biologia, in
economia o nelle scienze sociali, ma è invece vecchia e collaudata
in fisica, dove ha riscosso grandi successi. Non stupisce quindi
che la fisica sia stata un modello per sviluppare la matematica
applicata e la modellistica matematica in settori nuovi, e che
rappresenti un punto di riferimento concettuale fondamentale
ancor oggi.

3
Cenni alle problematiche più recenti
La matematica estende il suo dominio al di là della fisica
Non è sorprendente che, nel Settecento, il successo della fisica
galileiana e newtoniana abbia suggerito di applicarne i metodi ad
altri campi della conoscenza, in particolare a quelli del vivente e a
quelli della sfera umana: rapporti sociali, economici. Può apparire
a prima vista più sorprendente che tale trasferimento di metodi si
sia verificato prima e più intensamente nel campo della sfera dei
rapporti interumani, nonostante questa sia ben più complessa
della sfera del vivente. A ben vedere, ciò non deve invece
sorprendere perché l’applicazione dei metodi rigorosi mutuati
dalla fisica-matematica sembrava poter essere lo strumento
ideale per risolvere le drammatiche diseguaglianze sociali e le
crisi economiche che affliggevano le società europee dell’epoca.
Tuttavia, ci si rese presto conto che il trasferimento di metodi non
era affatto semplice e conduceva a problemi estremamente
difficili. È facile intenderlo da quanto abbiamo appena detto: la
differenza fondamentale tra lo studio matematico del moto dei
corpi celesti o terrestri e quello delle realtà sociali o economiche
consiste nel fatto che nel primo caso ci si deve limitare a
descrivere un sistema che sembrava regolato da leggi naturali le
quali, soprattutto nel caso dei corpi celesti, apparivano semplici,
armoniose e necessarie. Non si trattava certo di modificarne il
corso! Invece, descrivere un sistema caotico e soggetto a processi
tutt’altro che desiderabili non avrebbe avuto alcun senso. Si
trattava piuttosto di scoprire le leggi naturali del comportamento

umano ideale, che gli uomini stessi avevano perturbato con i loro
interventi irrazionali e insensati e che, una volta individuate e
ripristinate, avrebbero condotto il sistema sociale in una
condizione di equità, di massima felicità possibile per tutti, di
“equilibrio”. Inutile dire che determinare cosa sia la “natura
umana” nella sua essenza ideale si dimostrò un compito
proibitivo. Nel campo delle applicazioni della matematica
all’economia il modello che prevalse – e che ancor oggi è il
riferimento fondamentale – è quello dell’homo oeconomicus, ovvero
di un uomo la cui razionalità è definita dalle seguenti regole di
comportamento: è un essere dotato di una conoscenza completa
dei meccanismi del mercato e del suo stato in tutti i dettagli e che
mira all’ottenimento del massimo vantaggio possibile per sé.
Sono due caratteristiche spesso riassunte con i termini: infinita
lungimiranza e illimitato egoismo. L’ipotesi è che, se tutti gli agenti
economici possedessero tali caratteristiche e si comportassero in
questo modo “razionale”, il sistema raggiungerebbe uno stato di
equilibrio e le azioni di tutti i soggetti troverebbero una forma di
compatibilità.
Torneremo più in là sulla verisimiglianza di queste ipotesi.
Proviamo ora a tracciare un rapido panorama storico delle
applicazioni della matematica alle scienze non fisiche (per
maggiori dettagli ci si può riferire a Israel, 2004b).
Lo sviluppo di tematiche quantitative nel campo biologico e
sociale fu grandemente stimolato dallo studio delle tavole di
mortalità, a partire dal Seicento, il quale era suggerito da
problematiche nuove come quelle delle assicurazioni sulla vita e
degli assegni di pensione per i funzionari pubblici. Ma un vero e
proprio processo di matematizzazione si manifestò soltanto

quando dalle masse dei dati si passò al tentativo di stabilire vere
e proprie leggi della dinamica di una popolazione, oppure al
tentativo di formulare “modelli” per dare una risposta scientifica
rigorosa a problemi complicati e irrisolti.
Tale è il caso del celebre problema dell’inoculazione del vaiolo,
che può essere considerato come il primo caso importante di
matematizzazione di un problema biologico, in cui intervennero
aspetti di dinamica di popolazione e di dinamica di un’epidemia.
La vicenda è abbastanza nota e ci limiteremo a ricordarla
rapidamente. Poche malattie influenzarono il corso di un secolo
come il vaiolo nel Settecento: questa terribile malattia decimò
intere 
generazioni. 
Quando 
si 
scoprì 
casualmente 
che
l’inoculazione del siero vaccino infetto produceva una forma
attenuata 
della 
malattia, 
apparentemente 
meno 
letale 
e
suscettibile di produrre immunità – come in ogni caso di
sopravvivenza 
dall’infezione 
– 
si 
formarono 
due 
fronti
contrapposti: quello di chi suggeriva, in nome del progresso,
l’opportunità di inoculare; e quello di chi vi si opponeva
dichiarando l’inammissibilità di ogni perturbazione dell’ordine
naturale e di un atto che avrebbe provocato deliberatamente la
morte di un certo numero d’individui. Al matematico Daniel
Bernoulli (1700-1782) parve che il modo migliore per dirimere la
questione fosse quello di dimostrare in termini matematici esatti
i 
vantaggi 
dell’inoculazione. 
In 
una 
memoria 
presentata
all’Accademia delle Scienze di Parigi nel 1765, egli derivò da
alcune ipotesi semplificatrici – come la costanza della probabilità
di morire per un infetto di vaiolo indipendentemente dall’età – il
numero di persone che sarebbero dovute morire in un dato
periodo e il guadagno medio nell’aspettativa di vita per gli

inoculati. Il confronto con le tavole di mortalità disponibili
convalidava il risultato matematico di Bernoulli, e suggeriva che
l’inoculazione era vantaggiosa. Il lavoro di Bernoulli suscitò la
discesa in campo di un altro celebre matematico, Jean d’Alembert
(1717-1783) che, in una memoria pubblicata nello stesso anno, pur
condividendo in linea di principio l’opportunità dell’inoculazione,
attaccò Bernoulli sul piano strettamente scientifico. Egli non
soltanto tentò di dimostrare l’inadeguatezza del calcolo delle
probabilità nello studio di problemi come questo ma, in generale,
manifestò il suo scetticismo per la matematizzazione di fenomeni
come questi. Si era nell’epoca in cui un celebre matematico e
intellettuale del periodo illuminista e della Rivoluzione francese,
il marchese di Condorcet (1743-1794), predicava la necessità di
gestire i problemi sociali mediante la matematica ed enunciava
un ambiziosissimo programma di “matematica sociale” basato
sull’uso estensivo del calcolo delle probabilità. Ma fu un passaggio
breve come una meteora. Alla fine prevalse lo scetticismo alla
d’Alembert, nella forma radicale del nascente movimento
romantico: la matematica è adatta allo studio dei fenomeni
inanimati e i processi in cui è in gioco la libertà di scelta –
caratteristica tipica dell’uomo – sono irriducibili all’algebra o alle
probabilità. Per lungo tempo i tentativi di matematizzare le
scienze sociali o economiche trovarono l’opposizione congiunta
sia degli scienziati naturali che degli scienziati sociali (allora si
parlava piuttosto di “scienze morali”).
Anche la matematizzazione della biologia abortì sul nascere e,
per tutto il corso dell’Ottocento, la biologia non conoscerà niente
di più che l’uso di metodi statistici e non vedrà mai tentativi
organici di costituzione di una “biologia matematica”. Sarebbe più

corretto dire: quasi mai. Il caso dell’enunciazione della legge
esponenziale di crescita di una popolazione da parte di Malthus e
della legge logistica da parte di Pierre Verhulst (di cui abbiamo
parlato nel primo capitolo) sono eccezioni che confermano la
regola. I lavori di Verhulst verranno ignorati, e riscoperti soltanto
nel 1920 dallo zoologo statunitense Raymond Pearl: fu Vito
Volterra a dare il nome di “effetto Verhulst-Pearl” alla curva
logistica. Invece, nel campo delle scienze socio-economiche, la
sconfitta dei tentativi legati al progetto probabilista della
“mathématique sociale” non implica del tutto la fine di ogni
tentativo di matematizzazione. Qui prese forma il tentativo di
costruire un’economia teorica matematizzata basata sul trasporto
dei concetti e dei metodi della fisica-matematica e, in particolare,
della meccanica. Questo approccio ebbe come massimi esponenti
Augustin Cournot (1801-1877) e Léon Walras (1834-1910), oggi
considerati come i fondatori dell’economia matematica. Ma
entrambi conobbero un’ostilità molto larga, se non unanime.
Matematici, fisici, economisti, sociologi, storici, tutti erano
d’accordo 
nel 
rifiutare 
drasticamente 
la 
funzione 
della
matematica nello studio dei fenomeni non fisici. La parola
d’ordine, di stile romantico, era sempre la stessa, bene
rappresentata dalle parole con cui nel 1836 il matematico Louis
Poinsot (1777-1859), scagliandosi contro il suo collega Siméon-
Denis 
Poisson 
(1781-1840), 
aveva 
definito 
“ripugnante”
l’applicazione del calcolo ai fatti di ordine morale, un’aberrazione
della mente, una falsa applicazione della scienza, capace soltanto
di screditarla.
Eppure, nell’arco di un brevissimo periodo – meno di vent’anni
– questo muro di ostilità così compatto crollò di colpo. A partire

dalla seconda decade del Novecento, la matematizzazione dei
fenomeni non fisici si sviluppò in modo imponente in ogni
settore, come sotto l’effetto di una decisione unanime. Dinamica
delle popolazioni, epidemiologia matematica, genetica delle
popolazioni, modellizzazione di numerosi aspetti della fisiologia e
patologia umana, modelli di equilibrio economico, teoria dei
giochi, per non parlare delle innumerevoli nuove applicazioni nel
campo dell’ingegneria – tutti questi sviluppi presero forma come
per una miracolosa coincidenza e si concentrarono nell’arco del
ventennio 1920-40. Spiegare le ragioni di una siffatta esplosione è
per lo storico un compito tanto avvincente quanto complesso.
Diremo soltanto che un ruolo cruciale è giocato dalla crisi del
riduzionismo classico, dal progressivo declino della fiducia
nell’esistenza di grandi leggi naturali, dal ripiegamento verso
l’obbiettivo più modesto ed efficace della costruzione di modelli:
quello stesso descritto dal brano di von Neumann che
conosciamo tanto bene. Contribuisce alla caduta delle resistenze
la crisi definitiva del pensiero romantico e dello storicismo in
economia e in sociologia, la ripresa di un punto di vista
materialistico, sia pure in senso “metodologico”, secondo la
locuzione di Rudolf Carnap (1891-1970). E, di certo, una spinta
importante viene dagli sviluppi della tecnologia che suggeriscono
nuovi campi applicativi e nuove inattese correlazioni.
Nel periodo 1920-40, il matematico italiano Volterra fu
protagonista della nascita di una nuova biomatematica creata
mediante il “trasporto” dei metodi della meccanica classica. E
questo pone di fronte a un paradosso sorprendente, perché
Volterra non era affatto adepto di un approccio modellistico.
Anche Léon Walras e Vilfredo Pareto (1848-1923) avevano tentato

di fondare l’economia matematica come una branca scientifica
dotata di un apparato concettuale e metodico (sia sul piano
teorico che su quello della verifica empirica) del tutto parallelo a
quello della fisica matematica e di pari dignità. Walras l’aveva
definita una scienza “psichico-matematica”, o, se si vuole, la
scienza matematica del comportamento dell’homo oeconomicus,
corrispettivo del punto materiale in meccanica, e del risultante
processo economico globale retto dall’analogo della gravitazione
universale: la legge della domanda e dell’offerta del mercato.
Quando la matematizzazione dell’economia verrà ripresa negli
anni Venti, personaggi come Abraham Wald (1902-1950) e
soprattutto John 
von 
Neumann adottarono 
un 
approccio
modellistico e formale che rompeva con il programma walrasiano
– e, nel caso di von Neumann, in esplicita e dura polemica contro
ogni tentativo di riduzionismo meccanicistico. Nel caso della
biologia matematica, questa rottura in senso modellistico
avvenne più tardi, nel dopoguerra, quando si ripartì dai lavori di
Volterra ma spogliandoli di gran parte delle loro ambizioni
meccaniciste, e tuttavia senza riuscirvi fino in fondo perché le
origini del modello erano legate a una visione del tipo “cinetica
dei gas” e quindi strettamente ispirata a un’analogia fisica. Anche
un’altra branca della biologia matematica, la genetica delle
popolazioni di Ronald Fischer (1890-1962), John Haldane (1860-
1936) e Sewall Wright (1889-1988) – che ebbe un ruolo importante
nel ridare smalto alla teoria darwiniana saldandola con la
genetica delle popolazioni di Gregor Mendel (1822-1884) – per
quanto facesse ricorso a un approccio di tipo probabilistico, era
pesantemente influenzata dal riduzionismo meccanicistico.
Fischer scriveva nel 1922 che le ricerche nel campo della
selezione naturale «possono essere comparate con il trattamento

analitico della teoria dei gas». E appare stupefacente il modo in
cui questo influsso attraversa tutta la biologia matematica con la
metafora della “teoria degli incontri”, riflesso degli urti fra
particelle di un gas perfetto: teoria degli incontri, che è al centro
delle equazioni preda-predatore nonché dei modelli di dinamica
di un’epidemia. Abbiamo detto che quando la biomatematica
riprenderà il suo cammino, dopo la guerra, e dopo un periodo di
stasi abbastanza lungo, ciò avverrà assumendo un punto di vista
modellistico, il che significherà prestare un minore interesse al
problema della verifica empirica delle costruzioni matematiche.
Ma il cordone ombelicale con l’impostazione meccanicista
originaria non fu mai definitivamente dissolto.
La storia dell’economia matematica nel Novecento è del tutto
diversa da quella della biomatematica e, per certi versi, ha
caratteristiche opposte. L’intervento di von Neumann in questo
campo fu all’insegna di un violento attacco contro la teoria
dell’equilibrio economico walrasiana-paretiana e, più in generale,
contro ogni tentativo di “trasportare” metodi e concetti della
meccanica e della fisica nell’economia e nelle scienze sociali. Von
Neumann ripeté con insistenza che, per divenire scienze
matematizzate, 
l’economia 
e 
le 
scienze 
sociali 
dovevano
inventarsi un approccio interamente autonomo, e quindi una
“nuova matematica” del tutto diversa dall’analisi classica nata in
funzione della matematizzazione della fisica. Per von Neumann,
la teoria dei giochi era il primo passo di questa nuova
matematica.
Il contributo di von Neumann per la matematizzazione
dell’economia fu di importanza decisiva. Ma anche in questo caso
la direzione presa dal processo di matematizzazione si sviluppò

in contrasto con le intenzioni del suo principale ispiratore. La
generalizzazione del teorema di minimax di von Neumann al caso
di n giocatori ottenuta da John Nash (1928) offrì lo spunto per
recuperare tutto l’apparato matematico introdotto da von
Neumann e “ripulirlo” del linguaggio della teoria dei giochi, in
modo da adattarlo a un’assiomatica che mira a tradurre i concetti
della teoria walrasiana. Il teorema di equilibrio economico
generale di Arrow-Debreu fu, al contempo, il trionfo e la sconfitta
del programma di von Neumann: esso dimostrava che un sistema
economico composto da agenti che si comportano secondo la
razionalità tipica dell’homo oeconomicus possiede uno stato di
equilibrio. 
In 
tal 
modo 
veniva 
riproposto 
l’apparato
meccanicistico, il concetto di equilibrio, l’atomismo sociale. Si
trattò di una riproposizione ostinata, perché al di là del teorema
di esistenza dell’equilibrio – in fondo prevedibile sulla base dei
teoremi di von Neumann e Nash – il resto del programma
walrasiano si dimostrò non perseguibile, anzi fallimentare: e non
si trattava soltanto dell’ipotesi che l’equilibrio economico sia
unico, ma di una proprietà che è al cuore della teoria, e cioè che
un sistema economico di mercato possiede la virtù di collocarsi in
equilibrio, sia pure dopo una se rie di assestamenti, senza
interventi esterni salvo la dinamica del mercato stesso, ovvero
fuori da ogni intervento centrale statale di pianificazione o di
programmazione. È questo un principio delle dottrine liberiste e
neo-liberiste che non ha trovato alcun supporto nella matematica
e che tuttavia continua ad essere avanzato come un’indiscutibile
evidenza, mentre non lo è neppure sul piano empirico.
L’“irragionevole efficacia” della matematica

L’abbandono parziale dell’approccio meccanicistico da parte di
uno scienziato come Fourier non significò affatto la fine della
credenza che “il mondo è matematico”, anche se – è bene
sottolineare questo punto con forza – per la scienza classica “il
mondo” è quello naturale e non comprende la sfera umana: lo
abbiamo visto nel paragrafo precedente quando abbiamo
illustrato 
le 
fortissime 
resistenze 
alle 
applicazioni 
della
matematica a questa sfera e, più in generale, a quella del vivente.
Al contrario, per Fourier, l’analisi matematica è equiestesa alla
natura: ad ogni fenomeno naturale corrisponde un’equazione
(differenziale, nella visione classica) e viceversa ogni equazione
esprime un determinato fenomeno naturale. Agli inizi del
Novecento Poincaré ancora scriveva:
La teoria del calore di Fourier è uno dei primi esempi dell’applicazione
dell’analisi alla fisica. Prendendo le mosse da ipotesi semplici, che non sono
altro che fatti sperimentali generalizzati, Fourier dedusse una serie di
conseguenze che costituiscono, prese insieme, una teoria completa e
coerente. I risultati che ottenne sono certamente interessanti in sé, ma quel
che è più interessante è il metodo di cui egli fece uso per ricavarli e che
costituirà sempre un modello per tutti coloro che desiderano coltivare una
branca qualsiasi della fisica matematica. Aggiungo che il libro di Fourier è
di importanza capitale nella storia della matematica e che l’analisi pura gli
deve forse più dell’analisi applicata (Poincaré, 1895, p. 1).
Se il tentativo di ridurre ogni fenomeno all’equazione di
Newton, supponendo che esso sia sempre il risultato di processi
di moto di corpi, non era sostenibile, appariva invece del tutto
sostenibile l’idea che ad ogni classe di fenomeni corrispondesse
uno e un solo sistema di equazioni differenziali, eventualmente
ridotto a una soltanto: i fenomeni del moto propriamente detti
erano rappresentati dall’equazione del calore, i fenomeni di
vibrazione 
dall’equazione 
di 
d’Alembert, 
i 
fenomeni 
del

potenziale dall’equazione di Laplace, i fenomeni del calore
dall’equazione di Fourier, i fenomeni elettromagnetici dalle
equazioni di Maxwell, e così via. Insomma, si trattava di una
meravigliosa e armoniosa corrispondenza biunivoca tra analisi
matematica e natura che rafforzava anziché indebolire la
credenza che “il mondo (naturale) è matematico”.
Gli sviluppi della fisica degli inizi del Novecento, in particolare
la teoria dei quanti, e poi la meccanica quantistica, ma anche la
teoria della relatività, fecero entrare in crisi tale credenza: ne
abbiamo già parlato diffusamente. Ora la corrispondenza tra
natura e matematica non aveva più quel carattere di biunivocità
in cui credeva la scienza settecentesca e ottocentesca. Ogni
fenomeno 
poteva 
essere 
rappresentato 
mediante 
schemi
matematici differenti, che erano come lenti d’ingrandimento che
rilevavano questo o quell’aspetto del fenomeno, trascurandone
altri, ma che non potevano più essere considerati come “la”
rappresentazione del fenomeno. Tuttavia la persistente efficacia
della matematica nel descrivere e prevedere l’andamento di tanti
fenomeni era un dato che non poteva essere ignorato, tuttavia
esso diventava un enigma, non potendosi più dire che il mondo
era stato strutturato in modo matematico ma solo che la
matematica era un linguaggio estremamente efficace per
analizzarlo, niente più che un linguaggio. Quel che per Galileo era
un’evidenza sorretta da una credenza metafisica – “il mondo è
stato scritto da Dio in lingua matematica” – diventava un mistero
e l’efficacia della matematica qualcosa che non poteva essere
spiegato razionalmente.
Questa virata in senso quasi mistico della scienza dalla metà
del Novecento può sorprendere, a prima vista, ma è un dato di

fatto. Essa trova un’espressione chiarissima nella visione di un
influente gruppo di matematici francesi che si raccolse sotto lo
pseudonimo di Nicolas Bourbaki e che ebbe un ruolo importante
nella formazione della nuova presentazione assiomatica della
matematica, ovvero come una scienza astratta, indipendente
dallo studio della natura o di qualsiasi fenomeno, che procede per
via puramente logica da sistemi di assiomi e che può fornire
eventualmente schemi vuoti di “realtà possibili”:
Vi è [...] una constatazione che potrebbe, su questo punto incitare per il
futuro i filosofi a una maggiore prudenza: prima degli sviluppi rivoluzionari
della fisica moderna, si spese molta fatica per derivare a tutti i costi la
matematica dalle verità sperimentali, e in particolare da intuizioni spaziali
immediate; ma, da un lato la fisica dei quanti ha mostrato che questa
intuizione 
“macroscopica” 
del 
reale 
nascondeva 
dei 
fenomeni
“microscopici” di tutt’altra natura, che si connettevano a rami della
matematica che non erano certamente stati immaginati in vista di
applicazioni alle scienze sperimentali; e, d’altro lato, il metodo assiomatico
ha mostrato che le verità di cui si voleva fare l’asse della matematica non
erano altro che aspetti molto particolari di concezioni generali che non
limitavano affatto la loro portata. Per cui, in fin dei conti, quella intima
fusione di cui ci si faceva ammirare l’armoniosa necessità, non appare più
che come un contatto fortuito di due discipline i cui legami sono molto più
nascosti di quanto si potesse supporre a priori (Bourbaki, 1948, p. 46).
A rigore sarebbe scorretto attribuire a Bourbaki una visione
mistica. Egli si limita a dire che non vi è più motivo di sostenere
l’esistenza 
di 
una 
intima 
armoniosa 
corrispondenza 
tra
matematica e natura e che i contatti tra le due discipline (fisica e
matematica) sono molto più complessi di quanto si credeva. Il
matematico è quindi ormai “libero” dal condizionamento di quel
legame indissolubile con la fisica e con i fenomeni naturali e il
suo metodo sarà d’ora in poi quello assiomatico. Tuttavia, la
scelta degli assiomi non può essere completamente arbitraria,
una sorta di gioco senza regole:

vediamo qual è il ruolo del matematico. Egli si limiterà a giocare
astrattamente con un sistema di assiomi più o meno arbitrario, e di cui non
sa neppure se sia contraddittorio oppure no? Se veramente le cose stessero
così, si comprenderebbero le ripugnanze di Poincaré e di Borel nei confronti
del metodo assiomatico. Di fatto, è indispensabile distinguere la
matematica 
in 
quanto 
strumento 
(di 
cui 
abbiamo 
studiato 
il
funzionamento) dallo studio della natura, che è un fine per il quale è
forgiato questo strumento. Il miracolo della scienza consiste nella
possibilità di edificare una matematica astratta, applicabile in seguito con
efficacia alle leggi della natura. È sotto la guida dei fenomeni naturali che il
matematico, in fin dei conti, sceglie gli assiomi che faranno nascere una
teoria efficace (ivi, p. 47).
Quindi, Bourbaki suggerisce un approccio che ha limitazioni
precise: la scelta degli assiomi, in fin dei conti, deve avvenire
sotto la guida dei fenomeni naturali, anche se non si trattiene dal
ricorrere a un termine alquanto mistico: “miracolo”.
Al misticismo non si sottrae invece un celebre premio Nobel per
la fisica, Eugene Wigner, in un articolo ormai famosissimo che
abbiamo scelto come titolo di questo paragrafo (Wigner, 1960). Per
Wigner l’efficacia della matematica è un fatto addirittura
irragionevole, incomprensibile. Egli osserva che «i concetti della
matematica conducono a connes sioni del tutto inattese»; che
«essi spesso permettono una descrizione inaspettatamente precisa
e accurata dei fenomeni»; e che «siccome non capiamo le ragioni di
tale utilità, non possiamo sapere se una teoria formulata in
termini di concetti matematici sia univocamente appropriata. [...]
L’enorme utilità della matematica nelle scienze naturali è
qualcosa che sconfina nel mistero e non esiste alcuna spiegazione
razionale di essa». Wigner rigetta l’idea che la matematica possa
essere ridotta a concetti suggeriti dal mondo che ci circonda: la
matematica è ormai divenuta una scienza astratta, una scienza di

operazioni efficaci i cui concetti e metodi sono stati inventati
proprio in funzione di tale efficacia. Ma si tratta di un’efficacia
fine a sé stessa e non determinata da esigenze di descrizione
empirica: essa è una dimostrazione di abilità e genialità del
matematico, la capacità di ricavare conclusioni generali e
semplici da pochi principi e in modo trasparente, e quindi
dall’aderenza a un principio di bellezza estetica. Per quanto
riguarda il mondo naturale, esso è «di una complessità frustrante
e il fatto più ovvio è che noi non siamo in grado di prevedere il
futuro. [...] Come ha osservato Schrödinger, è un miracolo che
nonostante tale frustrante complessità sia possibile scoprire
qualche regolarità nei fenomeni». E tuttavia neppure Wigner (che
qui si riferisce a Schrödinger, 1932) vorrebbe rinunciare all’idea
che “il mondo è matematico”, anzi egli sostiene che tale
affermazione galileiana è più vera che mai, ma non se ne può
dare altro che una constatazione pragmatica, ovvero in termini di
efficacia. In termini razionali, nessuna spiegazione gli appare
possibile:
È difficile evitare l’impressione che ci troviamo di fronte a un miracolo del
tutto paragonabile nella sua natura impressionante al miracolo che la mente
umana sia capace di legare migliaia di argomenti insieme senza entrare in
contraddizione o ai due miracoli dell’esistenza di leggi della natura e della
capacità umana di indovinarle. [...] Il miracolo dell’appropriatezza del
linguaggio della matematica è un dono meraviglioso che né comprendiamo né
meritiamo. [...] Dovremmo essere grati per esso e sperare che resti valido
nella ricerca futura e che si estenda, per il meglio o per il peggio, per il
nostro piacere o anche per la nostra frustrazione, ad ampie branche della
conoscenza.
In realtà, a noi sembra che non si tratti né di un miracolo né di
un mistero perché tutta la matematica che conosciamo dal
Seicento in poi è stata creata per studiare i fenomeni della fisica e

quel che è accaduto in quel periodo è stato spiegato molto bene
da Koyré nel brano che abbiamo citato nel secondo capitolo: la
fisica ha abbandonato l’approccio quali tativo di stampo
aristotelico 
per 
adottare 
un 
approccio 
essenzialmente
quantitativo e, a sua volta, la matematica si è piegata alla
descrizione dei fenomeni fisici affrontando di petto il problema
della rappresentazione dei processi infinitesimi e infiniti. Una
nuova matematica – quella del calcolo infinitesimale o “calcolo
sublime” – venne creata sotto l’impulso dell’obbiettivo di
descrivere quantitativamente i fenomeni fisici. Ma anche la
matematica assiomatica che è venuta in seguito aveva le sue
radici ben piantate negli sviluppi precedenti. Non solo: come ha
ben osservato Bourbaki, l’approccio assiomatico non può
significare mai assoluta arbitrarietà ed è sotto l’impulso di
problemi reali che il matematico sceglie gli assiomi su cui è
fondata la sua teoria.
Questa indissolubile relazione tra la matematica e la realtà – e i
rischi inerenti all’approccio astratto e assiomatico – furono
descritti molto chiaramente da von Neumann:
Ritengo che un’approssimazione relativamente buona della verità – che è
troppo complicata per consentire null’altro che delle approssimazioni – sia
che le idee matematiche hanno origine nella realtà empirica, sebbene la
genealogia sia talvolta lunga e oscura. Ma, non appena concepito, il tema
comincia a vivere di vita propria ed è più vicino a un tema creativo, guidato
da motivazioni quasi completamente estetiche, che non a un qualsiasi altro
tema e, in particolare, a una scienza empirica. Un altro aspetto deve,
tuttavia, 
secondo 
me, 
essere 
sottolineato. 
Quando 
una 
disciplina
matematica viaggia lontano dalla sua sorgente empirica, o ancor più,
quando si trova a una seconda e terza generazione ispirata soltanto
indirettamente dalle idee provenienti dalla “realtà”, corre rischi assai gravi.
Diventa sempre più puramente estetizzante, sempre più, l’art pour l’art. Ciò
può non essere un male, se il campo è circondato da soggetti correlati, che

conservano connessioni empiriche più strette, o se la disciplina è sotto
l’influsso di uomini dotati di una sensibilità eccezionale. Ma esiste il grave
pericolo che il soggetto si sviluppi lungo una linea di minima resistenza, e
che la corrente, così lontana dalla sua sorgente, si separi in una moltitudine
di branche insignificanti, e che la disciplina divenga una massa
disorganizzata di dettagli e di complessità (von Neumann, 1947, p. 195).
Pertanto l’efficacia della matematica moderna nella fisica non è
un mistero ma un dato di fatto derivante dal modo appropriato
con cui essa nacque dal contesto dell’analisi dei fenomeni fisici: il
suo stretto legame con questi fenomeni fornisce una spiegazione
convincente e “razionale” del suo successo. Tuttavia, si tratta di
una spiegazione a posteriori. È il successo della matematica che la
rende plausibile e spiegabile. È comprensibile che l’efficacia della
matematica in fisica anche dopo la crisi della fisica al volgere
dell’Ottocento e durante tutta la prima metà del Novecento sia un
tema che si pone in termini assai più problematici di quelli con
cui vedevano la questione Galileo, Descartes e Newton. Per loro,
“il mondo è matematico” era un postulato metafisico. Come
ipotesi essa richiede di essere verificata continuamente in base ai
successi della matematica e non è mai garantita a priori come
essi credevano. Di qui l’emergere di posizioni mistiche, scettiche
o comunque problematiche. È ben vero che, malgrado le crisi
epocali di cui si è detto, discipline come la meccanica quantistica
hanno 
mostrato 
una 
sorprendente 
capacità 
descrittiva 
e
predittiva e la stessa meccanica newtoniana continua ad essere
un caposaldo della descrizione e previsione dei fenomeni
macroscopici. Ma anche in fisica vi sono fenomeni cosiddetti
“complessi” – i fenomeni di turbolenza che emergono in
particolare in meteorologia e in altri contesti – che resistono a
una rappresentazione matematica semplice. Per cui anche nel
campo della fisica, dove la matematica ha mietuto i suoi più

grandi successi, l’avanzata non è agevole e rapida come un
tempo.
La “ragionevole inefficacia” della matematica
Se l’efficacia della matematica nella descrizione e previsione
dei fenomeni reali deve essere valutata rispetto ai suoi effettivi
successi e insuccessi, la situazione cambia radicalmente quando
si passa dalla fisica ai contesti non fisici: biologia, economia,
scienze sociali. Di conseguenza, il lettore può trovare strano che i
nostri esempi di matematizzazione e di modellistica siano stati
scelti negli ambiti più controversi e difficili. Lo abbiamo fatto per
due motivi: perché è qui che si è espansa soprattutto la pratica
della modellizzazione dopo che anche la fisica l’aveva adottata in
conseguenza della crisi del riduzionismo classico; e perché è
proprio qui che troviamo il terreno migliore per esercitare lo
spirito critico e non accettare supinamente la formula secondo
cui “il mondo è matematico” che qualcuno vorrebbe dare per
scontata in ogni contesto, persino in quelli della psicologia e dei
processi mentali. Qualche decennio fa, due matematici applicati
così esponevano le conclusioni cui erano pervenuti per quanto
riguarda le applicazioni della matematica alla biologia:
I sistemi biologici tendono ad essere molto più complessi dei sistemi
studiati in fisica o in chimica. Nell’analisi dei modelli ci si trova di
frequente di fronte a due alternative: o il ricorso alla forza bruta della
simulazione mediante il calcolatore, oppure la riduzione del modello
mediante approssimazioni talmente draconiane che esso perde di interesse
dal punto di vista biologico. Né l’una né l’altra di queste due alternative è
seducente. In effetti, è raro il poter perseguire la prima delle due alternative
nella maggior parte delle situazioni che si presentano in ecologia, perché
raramente disponiamo di dati sufficienti allo scopo di convalidare un
modello dal punto di vista quantitativo. È questa una situazione del tutto
diversa da quella che si presenta nelle scienze fisiche, dove piccole

differenze 
permettono 
spesso 
di 
scegliere 
fra 
diverse 
teorie 
in
competizione. La situazione è talmente difficile che molti ecologisti
mettono seriamente in dubbio la possibilità che la matematica possa
giocare un ruolo qualsiasi che possa essere utile in biologia. Vi sono
persone che dicono che non esiste un solo progresso nel campo della
biologia che possa essere attribuito alle teorie matematiche. Essi dicono che
quando entrano in gioco i sistemi complessi il linguaggio appropriato è
l’inglese e non quello della matematica (Oster, Guckenheimer, 1976, p. 331).
Si 
può 
ritenere 
che 
tali 
conclusioni 
fossero 
troppo
pessimistiche, ma gli sviluppi successivi non hanno fornito
ragioni per pensarla diversamente. Basti pensare alla vastissima
modellistica consacrata alla diffusione dell’AIDS: centinaia e
centinaia di articoli che non hanno permesso di avanzare di un
solo passo nella direzione di una comprensione della dinamica
della malattia e non hanno fornito attendibili strumenti di
previsione delle modalità della sua diffusione. Anche qui la
situazione presenta chiaroscuri perché la modellizzazione di
malattie assai più semplici come la peste, il colera e anche la
malaria ha conseguito risultati assai più efficaci, né si può dire
che non esistano modelli di dinamica delle popolazioni privi di
valore.
La situazione non è meno critica in economia. Qui bisogna
distinguere. A parte modelli molto particolari, come quello di
Goodwin (che abbiamo sommariamente descritto nel primo
capitolo), esiste un vasto campo di analisi e previsione
dell’andamento delle economie basato su metodi probabilistici e
statistici che va sotto il nome di econometria. Il suo sviluppo ha
luogo a partire dagli anni Trenta, con la fondazione della
Econometric Society e della rivista “Econometrica” da parte di
Ragnar Frisch e Irving Fischer, nell’ambizione di usare metodi
matematici-quantitativi per la previsione dell’andamento dei

sistemi economici e per gestire le decisioni circa il loro futuro. Il
successo di tali applicazioni si commenta da solo, constatando la
modestissima capacità di previsione delle crisi e di gestione delle
economie nazionali che tale scienza ha messo in mano di chi
deve prendere decisioni operative (decision-makers). È da notare
che lo sviluppo dell’econometria è stato stimolato dalla ripresa
dell’interesse 
per 
la 
quantificazione 
e 
matematizzazione
dell’economia teorica, sia nella versione macroeconomica dovuta
al celebre economista inglese John Maynard Keynes, sia (e
soprattutto) 
nella 
ripresa 
dell’interesse 
per 
l’approccio
microeconomico di cui erano stati iniziatori Léon Walras e
Vilfredo Pareto. La nuova corrente ispirata alle loro visioni prese il
nome di teoria neoclassica e ancor oggi è considerata il
mainstream (la corrente principale) della teoria economica. La
versione dei padri fondatori aveva come nucleo il fine di
dimostrare che un’economia di mercato, i cui agenti fossero
mossi esclusivamente dall’obbiettivo “razionale” della perfetta
conoscenza dello stesso (“infinita lungimiranza”) e di realizzare il
massimo di vantaggio possibile (“infinito egoismo”), avrebbe
consentito di realizzare uno stato di equilibrio, il quale era
necessariamente unico; e inoltre, ispirandosi alla metafora di
Adam Smith della “mano invisibile”, possedeva una proprietà
omeostatica, ovvero di conseguire uno stato di equilibrio, quale
che fosse il suo stato iniziale, se lasciato a sé stesso senza
interventi esterni (rinviamo il lettore a Ingrao, Israel, 1987, per
una più approfondita analisi di questa tematica).
Né Walras né Pareto erano riusciti a dimostrare alcuna di
queste tre ipotesi (o teoremi) fondamentali. La dimostrazione
della prima (l’esistenza dell’equilibrio) fu conseguita negli anni

Cinquanta da Kenneth Arrow (1921) e Gérard Debreu (1921-2004),
sulla base dei lavori di von Neumann nell’ambito della teoria dei
giochi generalizzati da John Nash. Questo successo determinò
non soltanto quello della teoria neoclassica, ma anche la nascita
di una vera e propria nuova disciplina, l’economia matematica, che
ebbe le sue riviste e conseguì un influsso tale che, a partire da un
certo momento, l’approccio storico – che aveva dominato la
scienza economica fino ai primi anni del Novecento – fu
definitivamente spazzato via in favore di quello quantitativo-
matematico; tanto che ormai è impensabile che il premio Nobel
per l’economia non venga conferito a chi non abbia prodotto
lavori di carattere quantitativo, analitico o statistico che sia. Ben
diverso fu l’esito delle ricerche matematiche nell’ambito del
problema dell’unicità dell’equilibrio, dove Debreu dimostrò la
possibilità di numerosi e anche infiniti equilibri, il che poneva un
serio problema nella relazione tra teoria e applicazioni: ma ormai
la teoria andava per conto suo e sembrava più importante
produrre teoremi sofisticati di matematica che non occuparsi
della loro portata sul piano dei processi reali. Ancora peggiore si
rivelò l’analisi della questione della “mano invisibile”: una lunga
sequenza 
di 
lavori 
miranti 
a 
determinare 
processi 
di
aggiustamento dei prezzi capaci di condurre a un prezzo di
equilibrio (la tanto agognata “stabilità globale” del mercato)
condussero a risultati totalmente negativi. In tal modo, si era
raggiunto un paradosso cui abbiamo già accennato: l’unico modo
di conseguire l’equilibrio – la cui esistenza era stata dimostrata
peraltro sotto ipotesi di un’astrattezza estrema – era quello di
imporlo per decreto, come si farebbe in un’economia pianificata
di tipo socialista o comunista, perché il mercato (quantomeno
secondo il modello mainstream) non possedeva affatto le tanto

vantate proprietà di collocarsi da solo in equilibrio ove si fosse
rispettato il precetto liberista del non-intervento esterno (“laissez
faire, laisser passer”).
Anche in questo caso nessuno ha mai preso atto della evidente
crisi del paradigma neoclassico. Tutte le varianti che sono state
adottate nel seguito, anziché mettere in discussione a fondo il
modello stesso, l’hanno riadattato secondo quello che potremmo
chiamare un “bricolage” concettuale, in particolare aggiungendo
allo schema di fondo dell’homo oeconomicus un po’ di ingrediente
aleatorio. Per esempio, la teoria delle “aspettative razionali”, che
ha avuto tanto largo corso, non si discosta dal precetto:
“comportati secondo i precetti neoclassici e vedrai che tutto
andrà bene”. Né sono diverse le premesse di un celebre modello
matematico – le equazioni di Black-Merton-Scholes (formulate nel
1973) –, che valse il premio Nobel per l’economia (1997) ai suoi
autori e fu largamente utilizzato negli ambienti finanziari, in
particolare durante la crisi che ha devastato i mercati mondiali e
ancora non accenna ad abbandonarci. Non è possibile qui
descrivere questo modello che rappresenta quella miscela di
visione neoclassica e di aleatorio cui abbiamo accennato poco fa.
Ci limitiamo a dire che esso fu largamente utilizzato, nelle tante
versioni e sviluppi che ne furono fatti, dagli ambienti finanziari i
quali continuarono a fidarsi di questi strumenti, che ebbero un
ruolo non determinante ma certamente rilevante nel ritorno della
crisi in anni recenti. Come ha osservato il matematico Ian Stewart
in un articolo illuminante (Stewart, 2012), non è possibile far
ricadere integralmente su un’equazione la responsabilità del
“crash” delle economie, ma essa vi ha contribuito come
«ingrediente di un ricco spezzatino di irresponsabilità finanziaria,

inettitudine politica, incentivi perversi e una regolamentazione
lassista».
Gli esiti negativi indotti dalla radicale trasformazione della
scienza economica in una scienza matematica sono stati
efficacemente descritti da Giuliano da Empoli (da Empoli, 2013).
Dopo aver ricordato la prudenza di Keynes nel trattare la scienza
economica come una scienza meramente quantitativa e, su un
fronte opposto, la convinzione di un campione del liberismo come
Friedrich von Hayek che l’applicazione dei metodi delle scienze
naturali alle scienze sociali in genere è assurda, egli così
prosegue:
La generazione dei loro successori ha dimenticato le semplici lezioni di
Keynes e di von Hayek. Ignoranti istruiti si sono impadroniti delle loro idee
per irrigidirle in dogmi aritmetici. La parola d’ordine è “rigore scientifico”:
trasformare l’economia in una scienza esatta come la fisica [...]. Nascono
così la finzione di un Homo oeconomicus perfettamente razionale e un culto
dell’infallibilità del mercato che ricorda un po’ quello dell’infallibilità di
Aristotele praticato dagli scolastici del Medioevo. E fioccano i premi Nobel:
1970, il primo Nobel per l’economia assegnato a un americano va a Paul
Samuelson che ha dato una veste matematica alla teoria keynesiana; 1972,
Nobel a John Hicks e Kenneth Arrow, grosso modo per la stessa ragione;
1983, Nobel a Gérard Debreu, uomo-simbolo della nouvelle vague
matematica nelle facoltà di economia; 1990, Nobel a Harry Markowitz,
William Sharpe e Merton Miller per il loro contributo alla teoria della
finanza; 1995, Nobel a Robert Lucas, per il quale “la teoria economica
consiste nell’analisi matematica, tutto il resto sono solo disegni e parole”;
1997, Nobel a Myron Scholes e Robert Cox Merton che, come afferma
orgogliosamente quest’ultimo, hanno fatto uscire la teoria dei mercati
finanziari dall’aneddotica per trasformarla in una scienza.
Su queste prestigiose basi intellettuali si edifica una colossale infrastruttura
fatta di business school e di società di consulenza, di fondi d’investimento e
regolatori ancor più convinti dell’infallibilità del mercato di quanto non lo
siano gli stessi operatori. Di tanto in tanto, beninteso, la realtà s’incarica di
far squillare qualche campanello d’allarme. Nell’estate del 1998, pochi mesi
dopo l’assegnazione a due dei suoi fondatori (Merton e Scholes) il fondo

d’investimento Ltcm collassa rovinosamente. La crisi finanziaria russa non
era stata prevista dal suo modello di funzionamento. Per un momento, la
crisi minaccia di far crollare l’insieme delle complesse architetture fondate
sulle nuove teorie matematiche. Ma poi tutto riprende a procedere come se
nulla fosse. La formula Black-Merton-Scholes continua e essere insegnata
nelle business school e le banche d’affari riprendono a riempire i ranghi di
ingegneri immersi nei loro algoritmi. Il fatto è che la matematica dà
prestigio e tranquillizza. [...] All’inizio degli anni novanta la banca JP Morgan
elabora un modello chiamato Value at Risk (VaR) con il quale si illude di
aver scoperto la formula per calcolare con precisione matematica il rischio
di qualsiasi investimento. [...] Per un breve momento, gli analisti
quantitativi delle banche d’affari realizzano il sogno degli ignoranti istruiti
di tutti i tempi. Partendo dalla loro padronanza di una ristretta nicchia del
sapere, arrivano a dominare il mondo, accumulando immense fortune e
lasciando attoniti i plebei che non hanno la più pallida idea di cosa stiano
combinando. [...] Peccato che, nel 2008, la realtà s’immischi nuovamente
della teoria. E che stavolta, anziché trattarsi di un mero incidente di
percorso, si tratti di un cataclisma difficile da ignorare. Tanto più
imbarazzante in quanto le formule degli analisti di Wall Street lo
ritenevano impossibile. La crisi ha abbattuto le eleganti costruzioni degli
alchimisti della finanza, sostituendo una stimolante polifonia al monologo
del dogma.
Purtroppo la polifonia di cui parla da Empoli non soltanto non è
riuscita a soppiantare definitivamente il monologo del dogma, ma
essa stessa è lungi dal riuscire a proporci una musica diversa. Che
si tratti dello studio “scientifico” della psicologia o delle teorie del
caos e della complessità la mitologia della “scienza esatta”, la
riduzione della storia a insieme di regole comportamentali, e il
persistente predominio di un approccio matematico non
promettono niente di nuovo: le teorie del caos e della complessità
altro non sono che un ramo particolare della matematica
deterministica e oltretutto sono viziate da una impossibilità
strutturale di fornire previsioni.
Ci si può ben chiedere perché mai diffidare a priori dell’efficacia

della 
matematica 
nelle 
scienze 
sociali 
e, 
in 
particolare,
nell’economia. 
Tuttavia, 
anche 
un 
accanito 
sostenitore
dell’approccio quantitativo nelle scienze sociali e nell’economia
come von Neumann sosteneva fermamente che erano stati
necessari alcuni secoli perché la matematizzazione della fisica
raggiungesse il livello di potenza e raffinatezza che aveva
raggiunto nel XX secolo e che, per questo, era stato anche
necessario inventare una nuova matematica, l’analisi matematica
al cui cuore è il calcolo infinitesimale. Nessuna di queste due
condizioni era stata soddisfatta nell’ambito delle scienze non
fisiche e, in particolare, delle scienze sociali: era trascorso troppo
poco tempo ed era stato accumulato un insufficiente materiale
empirico, e la matematica utilizzata era quella della fisica, la
quale si rivelava inappropriata o insufficiente nei nuovi contesti.
Questi ragionevoli ammonimenti sono stati ignorati e tuttavia
occorrerebbe anche chiedersi se essi fossero sufficienti. Quanto al
primo, occorre osservare che il materiale empirico di cui si
avvalgono le scienze sociali ed economiche è essenzialmente di
carattere storico e quindi di natura profondamente diversa da
quello fisico: se si può ben dire che anche il materiale empirico
dell’astronomia è in certa misura storico, non così per la maggior
parte degli altri fenomeni fisici che si prestano alla possibilità di
una riproduzione e quindi alla possibilità della sperimentazione
che caratterizza il metodo sperimentale rispetto a un approccio
meramente empirico. Inoltre, la “nuova matematica” che avrebbe
dovuto nascere per riflettere la specificità dei fenomeni in cui
interviene il fattore umano non si è vista. Quel che differenzia
questi fattori è, per l’appunto, la presenza di un agente che
sembra dotato di una capacità di scelta autonoma e libera. Ora,

anche nel contesto della teoria dei giochi – che sembra essere
l’embrione della nuova matematica proposta da von Neumann –
quali sono gli strumenti con cui è possibile rappresentare quella
capacità? Sono essenzialmente due: un approccio deterministico e
un approccio probabilistico. Nel primo caso, viene distrutto proprio
quell’aspetto di libertà (“libero arbitrio”) che sembra essere una
caratteristica distintiva dei comportamenti umani, assimilando
l’uomo a una sorta di punto materiale mosso da forze cieche e
totalmente passivo. Le scelte soggettive diventano del tutto
apparenti, lo scenario della vita umana e associata assomiglia a
una pantomima in cui i protagonisti agiscono come burattini
mossi da fili invisibili, e tutto quel che si dice e si fa è
completamente inutile, la realizzazione di una sceneggiatura già
scritta. La concezione determinista è la porta d’ingresso al
fatalismo. D’altra parte, tentare di rappresentare la libertà umana
come un processo aleatorio, in cui accade che il soggetto scelga di
agire in questo o quel modo per motivi del tutto casuali è – per
usare le parole del matematico René Thom – ridurre il
comportamento umano all’incedere casuale di un marinaio
ubriaco, ovvero un tentativo parodistico di salvare la libertà
umana (Thom in Laplace, 1825; rist. 1985).
D’altra parte abbiamo visto – quando abbiamo accennato ai
fondamenti della teoria dei giochi – che la soluzione di minimax
può essere ottenuta soltanto in termini deterministici oppure
introducendo la probabilizzazione delle scelte strategiche, e
abbiamo visto quali difficoltà comporti l’interpretazione delle
strategie miste. È ben vero che von Neumann propose di rendere
più ricca la teoria matematica sviluppando la teoria dei giochi in
senso cooperativo, ma tale via non è stata perseguita, ritenendola

troppo difficile o improduttiva e, d’altra parte, essa non offriva
una via nuova rispetto al problema della descrizione matematica
delle scelte soggettive libere. In fin dei conti, ancor oggi nessuno è
riuscito a costruire una “nuova” matematica che rappresenti un
passaggio altrettanto rivoluzionario di quel che fu il calcolo
infinitesimale rispetto alla teoria delle proporzioni antica.
Potremmo sintetizzare la situazione dicendo che la matematica,
di fronte alle problematiche sollevate dallo studio dei fenomeni in
cui intervengono fattori di libera scelta, offre un menu povero,
composto di due soli ingredienti – quello determinista e quello
probabilista – che appaiono insufficienti a rappresentare la
complessità dei comportamenti soggettivi.
Si direbbe che lo sviluppo della modellistica abbia aperto alla
matematica territori sterminati in cui ha mietuto successi
indubbi, ma anche molti evidenti insuccessi e ha incontrato
ostacoli che nessuno sa bene come superare. Questa situazione fa
pensare a quella della Grande Armata di Napoleone quando
invase l’immenso territorio della Russia e si trovò di fronte a
difficoltà geografiche, logistiche e climatiche inattese; e non
molto diversamente da quanto accadde all’esercito tedesco
durante la Seconda guerra mondiale. La matematizzazione dei
fenomeni non fisici è andata e va incontro a una serie di sconfitte,
di “Beresine”, che dovrebbero essere il punto di partenza per un
riesame critico di tutta la situazione. Qui sarebbe opportuno
parlare di ragionevole inefficacia: di che stupirsi che la matematica
fatichi e produca risultati di dubbio valore quando si pretende di
affrontare situazioni tanto complesse con il ristretto menu
derivante dalla matematizzazione ideata per affrontare i
fenomeni fisici? Purtroppo molti fanno finta di ignorare questa

difficoltà, ricorrendo a una retorica che è il contrario del metodo
scientifico, come avremo modo di dire in conclusione.
La matematica e il calcolatore
Passiamo ora a dire qualcosa sul ricorso al calcolo numerico
nello studio dei fenomeni reali e sul suo rapporto con la
matematica propriamente detta. L’uso dei calcolatori nella
formulazione e nella verifica dei modelli è oggi sempre più
diffuso. Possiamo anzi dire che esso è diventato quasi
imprescindibile, 
almeno 
fino 
a 
quando 
il 
modello 
è
sufficientemente quantitativo da poter sostituire alle variabili dei
numeri.
L’aspirazione a una matematica applicata capace di giungere
dalle leggi ai numeri fu enunciata con grande efficacia da Fourier.
Nel brano che abbiamo già citato nel capitolo precedente, Fourier
dice infatti che lo studio matematico deve ottenere «le soluzioni
fino alle ultime applicazioni numeriche, condizione necessaria di
ogni ricerca e senza la quale si arriverebbe soltanto a
trasformazioni inutili». Inoltre la versatilità dei calcolatori di oggi
consente nuovi approcci come lo studio grafico delle soluzioni
delle equazioni differenziali. Questo studio fornisce informazioni
qualitative sulle soluzioni stesse, che sono beninteso una sintesi
grafica dei calcoli numerici che il calcolatore compie. Potremmo
dire che l’esigenza di Fourier, la quale restò a lungo soltanto
un’aspirazione, appare oggi come qualcosa di molto più
realizzabile, grazie ai calcolatori che mettono, per così dire, a
disposizione di tutti il calcolo numerico. Sicché il calcolo
numerico appare come la fase terminale della ricerca scientifica,
quella che consente di accostare direttamente la legge ottenuta

alla realtà, di perfezionarne la verifica.
Queste grandi possibilità sono oggi a disposizione di un numero
crescente di persone, e ciò grazie al diminuire dei costi dei
calcolatori, causato dai prodigiosi progressi tecnologici nel campo
dell’elettronica. È un bene questo? Da un lato non c’è che da
rispondere affermativamente: assumere una posizione diversa
sarebbe comportarsi come coloro che accoglievano le prime
automobili a colpi di pietra. Fare oggi matematica applicata senza
l’ausilio del calcolatore è più o meno come decidere di
intraprendere un viaggio di centinaia di chilometri a piedi avendo
a disposizione un’automobile. Nessuno metterebbe oggi in dubbio
l’utilità delle automobili nella civiltà contemporanea e nessuno
può seriamente mettere in discussione l’utilità dei calcolatori. Di
qui a idolatrare l’auto e a non vedere gli aspetti negativi che la
“civiltà dell’automobile” ha recato con sé, ne corre. Un discorso
analogo può essere fatto per i calcolatori.
È difficile negare che la “rivoluzione dei calcolatori” rischia di
portare 
con 
sé, 
assieme 
agli 
innegabili 
vantaggi, 
degli
inconvenienti le cui prime conseguenze sono già visibili. Non è
qui la sede per parlare di questioni estranee ai problemi della
ricerca scientifica. Vogliamo soltanto accennare ai rischi di un uso
non consapevole dei calcolatori, in particolare, nella costruzione
dei modelli matematici; ma un accenno alle implicazioni di questi
rischi 
sul 
piano 
dell’istruzione 
non 
può 
essere 
evitato.
Soprattutto, in un periodo in cui la parola d’ordine è di dotare
ogni classe scolastica almeno di un calcolatore o di un tablet
personale e di fondare larga parte dell’insegnamento sull’uso di
questi strumenti. Lasciamo perdere l’assurdità a dir poco
grottesca di far credere ai giovani studenti che una ricerca

concettuale possa essere svolta su quel sistema di “pagine gialle”
che è Internet – utilissimo quanto si vuole, ma pur sempre di
“pagine gialle” si tratta – in cui vengono rovesciate miriadi di
informazioni senza il minimo controllo o, peggio, attraverso un
controllo “collettivo” nell’ambito del quale riescono a farsi largo
personaggi la cui incompetenza dovrebbe tenere definitivamente
ai margini dei processi della conoscenza e della sua diffusione.
Ma abbiamo detto che non ci occuperemo di questioni extra-
scientifiche. Tuttavia, va detto chiaramente che un rischio a
portata di mano (peraltro connesso al precedente) è quello di far
credere che il calcolatore sia uno strumento di produzione di idee
e di concetti; di far credere allo studente che la produzione di un
modello matematico possa essere oggi delegata al calcolatore, di
non fargli intendere che la macchina è una scatola vuota dentro
la quale occorre introdurre relazioni quantitative su cui effettuare
calcoli, e che queste relazioni quantitative sono espressione di
relazioni concettuali che il calcolatore è incapace di produrre da
solo. Se dovesse diffondersi una visione “magica” del calcolatore
avremo presto prodotto intere generazioni completamente
diseducate ai principi elementari della ricerca scientifica – se non
del ragionamento – e allora sarà vano ripetere le consuete
lamentazioni circa l’analfabetismo scientifico di cui sono afflitte
le nostre società.
Abbiamo parlato del ruolo del calcolatore nel calcolo numerico,
cioè nella sostituzione di numeri alle variabili, che consente di
confrontare il modello con i dati e i processi reali. Su ciò nulla da
ridire, al contrario: l’uso del calcolatore è un fattore di enorme
progresso nella fase di verifica. Altra cosa è invece l’uso del
calcolatore come ausilio per la ricerca di leggi o la costruzione di

modelli. Qui il discorso cambia perché si rischia di mettere in
campo una sorta di matematica sperimentale cui è lecito guardare
con la massima attenzione e apertura, ma che non possiede
chiari codici di controllo dei risultati ottenuti.
È questo un tema molto delicato e importante che solleviamo
non certo per approfondirlo, quanto per suggerire al lettore di
riflettervi sopra in modo non conformistico. Anni fa un celebre
fisico-matematico e storico della scienza, Clifford A. Truesdell,
scrisse un articolo dal titolo polemico, Il calcolatore: rovina della
scienza e minaccia per il genere umano (Truesdell, 1981). Il principale
difetto di questo articolo era certamente nel suo titolo apocalittico
ed esagerato che suscita una reazione di diffidenza nel potenziale
lettore. In verità il calcolatore non contiene in sé nulla di male e
da solo non può rovinare nulla e nessuno. Il calcolatore è un
oggetto inerte e come tale completamente innocente. La rovina e
la minaccia possono venire soltanto da chi può fare o proporre un
pessimo uso del calcolatore. Entrando nel merito dell’articolo,
occorre 
riconoscere 
che 
Truesdell 
avanzava 
problemi
estremamente seri ed enunciava argomenti di grande rigore.
Citiamo qualche brano che illustra due tesi fondamentali:
il calcolatore ha bisogno di leggi prodotte dalla mente umana su
cui funzionare e sviluppare i suoi calcoli;
il calcolatore è in grado di ragionare soltanto su quantità finite,
mentre la scienza fornisce leggi valide in un’infinità di casi.
Scriveva Truesdell:
Il volo spaziale sarebbe stato impossibile senza le equazioni classiche del
moto. Il calcolo non deve essere confuso con la matematica. Il calcolo è una
funzione ripetitiva automatica, come il fissare due dadi controrotanti su

una catena di montaggio. Proprio come un essere umano doveva progettare
sia la cinghia di trasmissione che porta i due bulloni sia la macchina che li
richiede, un essere umano doveva dire al calcolatore, o meglio al gruppo di
padroni, direttori, e operai del calcolatore, quali equazioni risolvere. Senza
quelle equazioni, sarebbe stato impossibile concepire quali “coefficienti” si
dovevano determinare mediante programmi sperimentali, quali condizioni
iniziali si dovevano reperire mediante misure oppure assegnare ad arbitrio.
Senza quelle equazioni, come potrebbe un calcolatore – un bruto che sa solo
incamerare i numeri comuni nell’ordine che gli è stato imposto di seguire,
poi sommarli, poi ritenere o scartare il risultato oppure registrarlo su una
scheda prescelta – come potrebbe un groviglio di circuiti integrati
trasmettere gli ordini che dirigono o correggono continuamente il viaggio
dell’uomo sulla luna? Come potrebbe un milione di cifre essere di utilità
alcuna, se lo scopo non avesse prescritto di accumulare quelle cifre in un
dato punto e quindi fornito la chiave per interpretarle?
Parlo della navigazione interplanetaria non per particolare amore o odio per
essa, ma perché è familiare a chiunque e facile ad isolarsi in linea di
principio. Mentre molti fattori sono necessari a essa, uno, del tutto
indipendente dal calcolo numerico, è ugualmente indispensabile: la
meccanica del moto della capsula, perché, senza di essa, tutti gli aspetti
militari, politici, sociali, economici, geologici, medici, chimici, biologici,
astrofisici (e forse astrologici) sarebbero venuti meno per mancanza di un
oggetto a cui applicarsi. La meccanica del moto ha fornito l’ossatura
concettuale dell’impresa e le equazioni centrali che i calcolatori hanno
ricevuto il comando di risolvere ripetutamente, migliaia e forse perfino
milioni di volte, poiché si richiedevano diverse condizioni iniziali e diversi
parametri empirici. Questo è l’aspetto che i mass-media non citano mai. Il
proletariato intellettuale – le grandi masse che prendono come rivelazione
divina ciò che “dicono i dottori” e “dicono gli scienziati” nella stampa d’oggi
e dimenticano ciò che questi sacerdoti e astrologhi moderni hanno detto
ieri – il proletariato intellettuale che assiste con gioia alla dissipazione di
miliardi di dollari, che lui stesso paga sotto forma di tasse, per allestire il
più grande spettacolo televisivo di tutti i tempi (finché non cadrà la
prossima bomba nucleare o comincerà la guerra biologica), e che presta al
fanatismo della scienza una credulità superiore a quella del contadino
medievale verso il suo parroco, non ha idea che esista questo aspetto.
Queste equazioni centrali datano da più di 200 anni. Esse furono ottenute
da uomini, grandi matematici, che soppesarono i risultati delle osservazioni
astronomiche e che posero le loro menti sgombre, disciplinate alla scoperta
della semplicità nell’apparente complessità di numeri e numeri su numeri,

numeri apparentemente quasi casuali – orribili numeri.
Le equazioni differenziali che regolano i moti delle masse puntiformi e dei
corpi rigidi non sono sufficienti a determinare le condizioni del volo nello
spazio. Erano pure necessarie la fisica del gas, del calore, delle radiazioni, la
meteorologia, la chimica dei carburanti, e, per i voli umani, un’esperienza
biologica e medica. È stato veramente un trionfo della scienza applicata. In
esso non c’è stato nessun elemento di scoperta scientifica. Senza la scienza
classica sarebbe stato impossibile. [...]
Un calcolatore può fare, prima di rompersi, solo un numero finito di calcoli.
Quindi, indipendentemente da quanto grosso e preciso sia il calcolatore
esso sa calcolare la soluzione dell’equazione x2 = 2 solo fino ad un certo
punto. Il decimale successivo prevaricherà la capacità della macchina. I
matematici sin dai primordi della matematica, hanno trattato con
precisione e successo le quantità infinite. Quando il più grande calcolatore
del momento ha calcolato tutti i decimali di √2 alla sua portata, un buono
studente del second’anno di matematica è capace di dire esattamente
quello che deve essere calcolato da una nuova macchina, più potente, se si
devono ottenere con precisione degli ulteriori decimali, ed almeno quanti
decimali fornirà il passo successivo. Nessuna macchina dà informazioni del
genere. Esse provengono dallo studio logico degli insiemi infiniti dei
numeri.
I calcolatori, sempre più grossi man mano che si riproducono, otterranno
mai che ogni decimale, successivo a quello che essi trovano nel loro
tentativo di calcolare √2, sia zero? Vale a dire, l’espressione di √2 sotto
forma di frazione decimale sarà mai terminata? Nessun calcolatore, bruto
cieco e acefalo qual è, è in grado di rispondere a questa domanda, perché
anche se producesse una sfilza di un milione di zeri prima di arrestarsi, gli
si potrebbe ancora chiedere quale sarebbe il decimale successivo, e la
macchina dovrebbe rimanere zitta. Per converso, gli antichi matematici
greci trovarono la risposta definitiva a questa questione più di duemila anni
prima dell’invenzione della più primordiale macchina digitale. La risposta è
no. La soluzione x di x2 = 2 non può essere uguale al rapporto di due interi p
e q. L’equazione p2 = 2q2 non ha soluzioni per p e q interi. Ad un risultato di
questo genere non è capace di pervenire nessun calcolatore. Esso è un
risultato 
matematico 
non 
riconducibile 
ad 
un 
calcolo 
finito.
Indipendentemente dal numero di interi che un calcolatore potrebbe tirare
fuori, ne rimarrebbero sempre di più grandi, milioni e miliardi e trilioni di
volte la massima capacità del calcolatore.
La stampa odierna ci ha annunciato che un calcolatore ha risolto di recente
un famoso problema affrontato invano dai matematici per più di cent’anni:

per colorare bene una carta geografica (dando una definizione matematica
rigorosa ai termini “colorare”, “carta geografica”, e “bene”) sono necessari
più di quattro colori? Questo problema, così posto, richiede infinite carte
geografiche, ed un calcolatore non è in grado di manipolare nulla che sia
infinito. Pertanto l’affermazione è falsa. Infatti una catena di buoni
matematici, prolungatasi per oltre un secolo, inclusi fra essi i due che
compirono il lavoro finale con il calcolatore, avevano ricondotto il problema
all’esame di un numero finito di casi, mediante l’impiego della matematica
corrente, tradizionale, mentale. Solo questa riduzione rese possibile persino
l’idea di fare appello ad un calcolatore. Il numero finito di casi era troppo
grande per degli sforzi umani privi di aiuto, di conseguenza fu fatto molto
opportunamente ricorso ad un calcolatore per provare i vari casi, uno per
uno. Pure così, il processo algoritmico dovette essere escogitato da dei
matematici; a tale scopo, vennero chiamati in causa dei risultati dimostrati
da una costellazione di grandi uomini negli ultimi 250 anni. [...]
Anche i problemi che riguardano gli interi 1, 2, 3,... sono spesso al di fuori
della potenza del calcolatore più grande. Si prenda per esempio, problema
antico e famoso, la dimostrazione di quello che è erroneamente chiamato
“L’ultimo teorema di Fermat”: non ci sono interi x, y, z tali che xp + yp = zp se
p è un numero primo dispari. Molti dei grandi matematici si sono dibattuti
invano per provare questa semplice proposizione durante gli ultimi
trecento anni. Finora non vi sono riusciti, ma i loro tentativi hanno dato
luogo a tante scoperte meravigliose nella teoria dei numeri che Harold M.
Edwards è stato in grado di scrivere un bel libro in cui sviluppa
geneticamente l’argomento raccontando la storia di questo problema. In
quel libro, intitolato Fermat’s Last Theorem, Springer, 1977, a p. VI Edwards
scrive: “... virtualmente si è nella posizione di poter provare l’ultimo
teorema di Fermat per ogni primo nell’ambito delle capacità di calcolo, ma
non si può escludere la possibilità che il teorema sia falso per tutti i primi
oltre un certo limite”. Non conosco un esempio più illuminante della
differenza fra calcolo numerico e matematica. Qui dovrei aggiungere che il
calcolo su grandi macchine ha già fornito informazioni importanti su
questo problema e altre ne potrà fornire in futuro. Per esempio, potrebbe
dimostrare la falsità dell’enunciato di Fermat trovando uno o più primi per
cui la sua equazione possiede una soluzione. Ma il calcolo non potrebbe
mai risolvere il problema come l’ha posto Edwards. Al contrario, un giorno
quel problema può anche essere risolto da un matematico (Truesdell, 1981).
Ebbene, nel 1994, quel problema è stato risolto dal matematico
Andrew Wiles proprio nel senso attribuito da Fermat! E nella

maniera più tradizionale caratteristica della ricerca matematica,
ovvero secondo procedimenti puramente mentali e basati sull’uso
della logica formale, anche se con strumenti tecnici che Fermat
non possedeva, per cui i matematici sono inclini a ritenere che
Fermat si fosse sbagliato nel credere di aver di mostrato il
teorema. La dimostrazione del teorema di Fermat costituisce una
delle più clamorose conferme delle tesi di Truesdell.
Lasciamo al lettore di riflettere su questi temi e soprattutto di
ampliare le sue letture. Noi ci limitiamo a qualche osservazione
conclusiva 
di 
carattere 
storico. 
Fin 
dal 
loro 
nascere 
la
“cibernetica” (cioè la teoria del controllo e della comunicazione) e
la teoria degli automi furono largamente influenzate da una
visione di tipo meccanicista e da una fiducia illimitata nella
possibilità di ricostruire il funzionamento del cervello umano
mediante un modello di tipo meccanico. Purtroppo i modelli
meccanici possono essere capaci soltanto di un numero finito di
operazioni, mentre la caratteristica della mente umana è quella di
compiere operazioni mentali concernenti l’infinito che si basano
su presupposti non formali. Inoltre, questi termini sono
ineluttabilmente “chiusi”, mentre la mente umana è un sistema
“aperto” dotato di una capacità di interazione e adattamento agli
stimoli esterni praticamente senza limiti.
Quest’ultima difficoltà non sfuggì ai più intelligenti sostenitori
del-l’intelligenza artificiale. Col concetto di macchina che
apprende, dotata di sensori per acquisire informazioni e interagire
con l’esterno, essi cercarono di trasformare la macchina che
calcola in un sistema aperto. Il fatto è che quest’aspirazione è
restata tale e la macchina e il cervello umano appaiono sempre
distanti come prima. La macchina riesce a simulare solo alcuni

aspetti del funzionamento del cervello umano; ad accelerare in
modo prodigioso alcune funzioni meccaniche, alleviandone le
fatiche e moltiplicandone la potenza. Esattamente come un
telescopio consente di vedere stelle invisibili all’occhio nudo e un
microscopio di vedere dei virus. Il telescopio e il microscopio,
loro, non vedono nulla, neppure se sono collegati a un automa-
calcolatore 
che 
registra 
i 
risultati 
delle 
osservazioni. 
Il
microscopio elettronico non vede da solo più di quanto non
vedeva da solo il vecchio microscopio ottico. Così i nuovi
calcolatori della seconda, terza o ennesima generazione non
pensano più di quanto non pensavano i loro antenati che
dormono nei musei o arrugginiscono nelle cantine degli Istituti
universitari.
Osserva Arcangelo Rossi:
Anche ammesso [...] che una logica non monotòna sia formulabile in modo
oggettivo in generale [...] potrà essa riuscire a catturare in pieno quella
capacità di ricontestualizzazione della mente naturale, in modo da rendere
la sua implementazione su dispositivi artificiali risolutiva rispetto
all’obbiettivo di simulazione proprio dell’intelligenza artificiale, o anche
solo rispetto ad un più circoscritto obbiettivo di emulazione? C’è da
dubitarne, se solo si pensa che il processo di ristrutturazione dei significati
iniziali dei termini, che essa comporta come sostituzione ed adattamento
innovativo, pur essendo qualcosa di più creativo della semplice induzione
per generalizzazione [...], non esaurisce certamente la capacità di
ricontestualizzazione dell’intelligenza naturale. Questa infatti, oltre a
modificare in modo innovativo i significati di partenza dei termini,
comporta necessariamente al tempo stesso la presa di coscienza dei limiti e
della relatività delle stesse acquisizioni provvisoriamente conclusive del
processo, in quanto almeno virtualmente soggetto ad ulteriore sviluppo e
revisione, nella consapevolezza di una sempre possibile ancor più radicale
contestualizzazione. Una contestualizzazione che non si riduce mai in ogni
caso ad un mero fatto semantico di individuazione di nuovi significati
linguistici di termini e di proposizioni. Essa infatti fa riferimento alla
corporeità in termini di esperienze emozionali ed intuizioni, pur senza

ridurvisi essendo almeno parzialmente cosciente della provvisorietà e
rivedibilità di qualsiasi ristrutturazione creativa, legata alla fissazione di
nuovi significati di termini e proposizioni, e prendendone quindi almeno
parzialmente 
le 
distanze. 
Esperienze 
emozionali 
ed 
intuizioni
accompagnano in ogni caso quelle individuazioni semantiche come loro
vissuto personale, psicologicamente e socialmente connotato in termini di
valori come la verità. Questi ultimi saranno però sempre almeno in parte
impliciti, e sempre quindi ulteriormente esplicitabili e contestualizzabili,
mai totalmente riducibili comunque a significati espliciti, e d’altra parte
neppure a quelle stesse esperienze emozionali ed intuizioni che in ogni
caso aderiscono strettamente ai significati stessi (Rossi, 1998, pp. 119-21).
Questa capacità è il fondamento dell’impresa scientifica, che è
una creazione della mente umana, nella quale il calcolatore può
intervenire solo come un braccio meccanico.
Qual è in realtà il rischio peggiore? La visione dei primi
sostenitori della cosiddetta intelligenza artificiale e dei creatori
della cibernetica aveva una sua affascinante grandezza. In quella
visione non si negavano né sminuivano le specificità della mente
umana e delle sue facoltà, il suo carattere di sistema aperto. Il
programma era anzi quello di imitare la mente umana e il suo
funzionamento senza trascurare o sottovalutare alcuna difficoltà.
Ma da allora vi sono stati tanti insuccessi e il prodigioso sviluppo
dell’informatica non ha segnato un solo passo in avanti sulla via
della creazione della macchina pensante. E allora forse c’è il
rischio che si riaffacci una versione mediocre del meccanicismo e
che prevalga la tendenza opposta. Tale tendenza consiste nel
considerare il pensiero come l’emanazione di una serie di
processi fisico-chimici. Mente umana e macchina sarebbero (per
decreto) la stessa cosa, almeno in linea di principio. E la tendenza,
magari senza rendersene conto, sarebbe quella di piegare
progressivamente il funzionamento della mente umana a quello

della macchina, visto che l’operazione inversa non ha successo...
La necessità di un approccio umanistico
In una recente intervista (sul “Corriere della Sera” del 9 ottobre
2014), Federigo Faggin, ideatore del primo microchip, un
ricercatore che, per le sue invenzioni, ha ricevuto la National
Medal of Technology degli USA, ha dichiarato di aver tentato per
venti anni di costruire un computer capace di apprendere da solo
e quindi fornito di una forma elementare di consapevolezza. Ha
dovuto concludere che questo compito è impossibile e che
neppure quel che sappiamo del cervello ci permette di dire cosa
sia la consapevolezza in termini meccanicisti:
Sapessimo esattamente almeno dove sono nel cervello questa e quella
cosa! Dov’è la memoria? E le emozioni? E poi la consapevolezza va al di là
dei dati. [...] La consapevolezza va al di là del meccanismo. È un fenomeno
primario. È una proprietà irriducibile della realtà. [...] Più che rassicurarmi
questa certezza mi ha aiutato a capire fino in fondo quanta più profondità
ci sia in un uomo. O perfino in un animale. Un bambino che sbatte su un
albero da quel momento sa che si farà un bernoccolo sbattendo contro ogni
albero, alto, basso, giovane, vecchio, verde o spoglio che sia pino, abete o
baobab: il computer no. Devo fargli immagazzinare tutte le variabili perché
da solo non ci arriva. [...] Una società “scientista” ci ha fatto il lavaggio del
cervello spingendoci a pensare che tutto è macchina. L’universo è una
macchina, noi siamo macchine... Assurdo. L’uomo si sta sottovalutando. E
lo diciamo non sulla base di un dogma ma di quanto abbiamo potuto
accertare. [...] Un neo-umanesimo post-digitale è necessario in quanto, se
non stiamo attenti la macchina ci imprigiona invece che liberarci. Io ho
sempre visto la macchina come una cosa liberatoria. Che mi deve aiutare
ad avere una vita più facile. Più tempo libero. Più spazio per me. Una
macchina che “deve stare al suo posto” senza invadere la mia vita. [...]
Finanzio ricercatori che cercano di trovare una teoria matematica della
consapevolezza. Generalmente gli scienziati pensano (non tutti, si capisce)
che questa sia un epifenomeno del funzionamento del cervello. Per me no.
È primaria.

Ammettere che la scienza abbia dei limiti e che li abbia in
particolare la matematica non significa svalutarle. Al contrario.
Significa riconoscere la dimensione autentica della loro potenza e
della loro efficacia. La matematica non è un passe-partout per
spiegare o prevedere qualsiasi cosa. Essa è nata sia dalla
problematica del contare che dall’osservazione dei corpi solidi
inanimati e quindi è difficile che possa rendere molto di più di
quanto promettono le sue radici sul piano della descrizione e
della previsione. I tentativi di estendere le applicazioni della
matematica al di fuori del campo della fisica hanno confermato
tale punto di vista: fino a che abbiamo a che fare con processi che
hanno una natura essenzialmente meccanica o comunque
derivante da processi puramente materiali, oppure aventi un
andamento essenzialmente aleatorio, la matematica si rivela uno
strumento ancora assai efficace; non appena intervengono fattori
soggettivi in cui il più elementare buon senso indica la presenza
di scelte libere e autonome, la matematica inizia a incespicare.
Che un simile esito fosse inevitabile era presente a menti prive di
pregiudizi come quella del grande matematico dell’Ottocento
Louis Augustin Cauchy, che scriveva nell’Introduzione al suo più
celebre trattato di analisi matematica:
sono ben lungi dall’affermare che l’analisi sia sufficiente a tutte le scienze
della ragione. Indubbiamente, nelle scienze cosiddette naturali, il solo
metodo che possa essere impiegato con successo consiste nell’osservare i
fatti e nel sottoporre quindi le osservazioni al calcolo. Ma sarebbe un grave
errore pensare che la certezza non possa essere trovata altro che nelle
dimostrazioni geometriche o nella testimonianza dei sensi; e nonostante
nessuno fino ad oggi abbia tentato di dimostrare con l’analisi l’esistenza di
Augusto o di Luigi XIV, ogni uomo sensato converrà che questa esistenza è
per lui altrettanto certa del quadrato dell’ipotenusa o del teorema di
MacLaurin. Dirò di più: la dimostrazione di quest’ultimo teorema è alla
portata di poche menti, e gli stessi scienziati non son tutti d’accordo sulla

generalità che occorre attribuirgli; al contrario tutti sanno molto bene da
chi sia stata governata la Francia nel diciassettesimo secolo, e che non è
possibile sollevare al riguardo alcuna contestazione ragionevole. Ciò che ho
detto a proposito di un fatto storico si applica parimenti a una quantità di
problemi, nel campo religioso, morale e politico. Occorre convincersi che
esistono delle verità diverse dall’algebra, delle realtà diverse dagli oggetti
sensibili. Coltiviamo con ardore le scienze matematiche, ma senza volerle
ostentare al di là del loro dominio; e non illudiamoci che si possa affrontare
la storia con delle formule, né sanzionare la morale con dei teoremi o del
calcolo integrale (Cauchy, 1821, p. 3).
Sarebbe quindi ragionevole dire che la formula “il mondo è
matematico” non è sostenibile se con “mondo” intendiamo non
soltanto il mondo materiale inanimato, ma anche la sfera della
vita e la sfera dei rapporti interumani. Essa presenta qualche
attrattiva se ci riferiamo esclusivamente al mondo materiale, al
mondo fisico, anche se appare opportuno eliminare da essa ogni
accento mistico e darne una spiegazione razionale: la matematica
è nata per manipolare le quantità e le grandezze geometriche, e
quindi è tutt’altro che strano che i suoi più grandi successi siano
stati e siano ottenuti in questo ambito. Tuttavia, anche nel caso
del mondo fisico non è possibile condividere il trionfalismo dei
primi due secoli dopo la rivoluzione scientifica. Lo sviluppo della
conoscenza del mondo fisico fa pensare a una sfera che si
espande: quanto più essa diventa grande, tanto più si amplia la
sua superficie e il contatto con lo sconosciuto e il non spiegato.
Dopo i grandi sviluppi della teoria della relatività e della
meccanica quantistica la fisica teorica deve far fronte a problemi
di enorme complessità che non possono essere risolti con la
semplice forza bruta della sperimentazione e del calcolo
numerico e richiedono la discesa in campo di teorie matematiche
sempre più raffinate, come nel caso della teoria delle stringhe che
è lungi dall’aver raggiunto un consenso negli ambienti della fisica

teorica. Ma anche qui siamo in una situazione molto difficile.
Appaiono quindi fuori luogo certi trionfalismi forse buoni per
“épater le bourgeois”, ma che rappresentano una vera e propria
manifestazione di crollo dello spirito critico, come accade nel
brano seguente:
Uno dei più grandi misteri dell’Universo è il fatto che non sia un mistero.
Siamo in grado di comprendere e prevedere il suo funzionamento a tal
punto che se un uomo comune del Medio Evo si trovasse a vivere tra di noi
si convincerebbe che siamo dei maghi. Il motivo per cui siamo stati così
bravi a sciogliere l’enigma dell’Universo è che abbiamo scoperto la lingua
nella quale il Libro della Natura sembra essere scritto. Questo linguaggio,
come ha sostenuto con fervore Galileo più di trecento anni fa, è quello della
“matematica”. In ogni aspetto del mondo naturale che abbiamo preso in
considerazione, abbiamo visto che il linguaggio della matematica si adatta
meravigliosamente alla natura del mondo e al suo funzionamento. Non è
stato scoperto alcun fenomeno che sfugga al suo potere descrittivo. È vero
che esistono campi in cui il suo uso si rivela inappropriato – chi sarebbe
così stupido da considerare una sinfonia di Beethoven niente altro che una
variazione matematica, o una combinazione di pressione e tempo – ma non
ce n’è nessuno in cui esso sia impossibile (Barrow, 1992, pp. 5-6).
Possiamo dar atto all’autore di aver riproposto la formula “il
mondo è matematico” in senso galileiano, ovvero intendendo per
“mondo” la “natura”, anche se egli non dice di alludere soltanto
alla sfera dei corpi inanimati. Tuttavia, egli ammette che esistono
campi in cui l’uso della matematica si rivela «inappropriato», ma
poi aggiunge che non esistono campi in cui esso si rivela
«impossibile», e allora non si capisce bene come si concilino le
due affermazioni: trattare una sinfonia di Beethoven con la
matematica è possibile ma è inappropriato, forse perché di cattivo
gusto? E anche se ci restringiamo al campo dei fenomeni fisici
propriamente detti, sostenere che non è stato scoperto alcun
fenomeno che sfugga al potere descrittivo della matematica è
un’esagerazione, per non dire dell’affermazione secondo cui nulla

dell’Universo è per noi un mistero. Purtroppo, siamo lungi
dall’essere in questa situazione.
Lo scientismo danneggia una visione ampia della ragione che
includa altri punti di vista oltre a quello fisico-matematico e
danneggia questo stesso approccio caricandolo di problemi che
non è detto affatto che possa risolvere. Inoltre, si oppone a un
punto di vista umanistico che riconosce la specificità degli esseri
umani, il fatto che non sono macchine. Come diceva Karl Popper
(1902-1994), «gli esseri umani sono insostituibili e, come tali, sono
chiaramente molto diversi dalle macchine. Sono capaci di gustare
la gioia della vita, ma anche di soffrire e sanno affrontare la morte
con piena consapevolezza. Sono degli io, dei fini in se stessi, come
diceva Kant. Questa concezione mi sembra incompatibile con la
dottrina materialistica secondo cui gli uomini sono delle
macchine [...] una dottrina non solo erronea, ma tendente a
minare un’etica umanistica» (Popper, Eccles, 1981, I, p. 15).
La matematica è forse la scienza che più di ogni altra è vicina a
una visione umanistica. La matematica antica ha riconosciuto i
problemi 
e 
le 
antinomie 
che 
nascono 
dalla 
trattazione
dell’infinito e ha evitato di affrontarle. La matematica della
rivoluzione scientifica ha invece sfidato la tematica dell’infinito e
dell’infinitamente 
piccolo, 
senza 
riuscire 
a 
risolvere
definitivamente quelle antinomie, ma indicando i modi per
manipolare in senso pratico quei concetti. L’aver fatto fronte alla
tematica dell’infinito e dell’infinitamente piccolo è ciò che ha
stabilito 
fin 
dall’inizio 
stretti 
legami 
tra 
matematica 
e
speculazione filosofica. Per capire a fondo la matematica è
necessaria la filosofia e anche la storia della matematica stessa e
della scienza. Nessuna forma di attività mentale umana è priva

della capacità di portare conoscenza, non lo è neppure la
letteratura, come pretende qualcuno; al contrario, la letteratura è
una grande fonte di ricchezza conoscitiva. In tal senso, l’invito a
coltivare l’umanesimo – a fondare un umanesimo post-digitale – è
di grande valore. Ed è anche un modo per nobilitare la
matematica più di quanto lo sia agitare la formula “il mondo è
matematico” in modo mistico e acritico, occultandone gli
insuccessi. Una matematica che pretendesse di riassorbire tutto
esprimerebbe la tentazione puerile di ridurre ogni aspetto della
realtà a fattori quantitativi, che debbono invece esser lasciati
convivere armoniosamente con gli aspetti irriducibilmente
qualitativi. Pertanto, un approccio umanistico significa anche
salvare la specificità della ricchezza concettuale della matematica
nelle sue relazioni con tutte le altre attività conoscitive umane,
anziché impoverirla nel tentativo fallimentare di farle assorbire
ogni aspetto della realtà.

Bibliografia
Questa Bibliografia non ha alcun carattere esaustivo rispetto alla
gran quantità di temi discussi nel libro, il quale ha un intento
divulgativo e quindi non ha pretese di completezza. Ci
limitiamo a indicare qui i testi citati e altri che possono essere
di aiuto al lettore per approfondire i temi trattati.
BARROW J. D. (1992), Perché il mondo è matematico?, Laterza, Roma-
Bari.
BOURBAKI 
N. 
(1948), 
L’architecture 
des 
mathématiques: 
la
mathématique ou les mathématiques?, in F. Le Lionnais (éd.), Les
grands courants de la pensée mathématique, Cahiers du Sud, Paris,
pp. 35-47.
BRAUN M. (1975), Differential Equations and their Applications,
Springer, New York.
CARDWELL D. S. L. (1976), Tecnologia, scienza e storia, il Mulino,
Bologna.
CAUCHY A. L. (1821), Cours d’analyse de l’École royale polytechnique,
Imprimerie Royale, Paris.
DA EMPOLI G. (2013), Contro gli specialisti. La rivincita dell’umanesimo,
Marsilio, Venezia.
FOURIER J. (1822), Théorie Analytique de la Chaleur, Firmin Didot,

Paris.
GALILEI G. (1623), Il Saggiatore, in A. Favaro (a cura di), Opere di Galileo
Galilei, 20 voll., G. Barbera, Firenze 1890-1909, vol. VI.
ID. (1632), Dialogo dei Massimi Sistemi, in Opere, 3 voll., Rizzoli,
Milano 1936, vol. I, pp. 21-606.
INGRAO B., ISRAEL G. (1987), La Mano Invisibile. L’equilibrio economico
nella storia della scienza, Laterza, Roma-Bari (trad. ingl. The
Invisible Hand, MIT Press, Cambridge, MA, 1990).
ISRAEL G. (1986), Modelli matematici, Editori Riuniti, Roma; nuova
ed. riveduta Modelli matematici, Introduzione alla matematica
applicata, Franco Muzzio, Roma 2002.
ID. (a cura di) (1994), Modelli matematici, in “Le Scienze”,
Quaderno n. 81.
ID. (1996), La visione matematica della realtà. Introduzione ai temi e
alla storia della modellistica matematica, Laterza, Roma-Bari.
ID. (1998a), Balthazar Van der Pol e il primo modello matematico del
battito cardiaco, in P. Freguglia (a cura di), Modelli matematici nelle
scienze biologiche, Quattro Venti, Grosseto, pp. 133-62.
ID. (1998b), Analogie, metafore e verifica empirica nella biologia
matematica 
contemporanea, 
in 
“Pristem/Storia, 
Note 
di
matematica, Storia e cultura”, 1, Springer Verlag Italia, Milano,
pp. 53-72.
ID. (2001), Modèle-récit ou récit-modèle?, in J.-Y. Grenier, C. Grignon,
P.-M. Menger (éds.), Le modèle et le récit, Éditions de la Maison
des sciences de l’homme, Paris, pp. 365-424.

ID. (2004a), Technological Innovation and New Mathematics: Van der
Pol and the Birth of Non-linear Dynamics, in M. Lucertini, A. Millán
Gasca, F. Nicolò (eds.), Technological Concepts and Mathematical
Models in the Evolution of Engineering Systems, Controlling-
Managing-Organizing, Birkhäuser, Basel-Boston-Berlin, pp. 52-78.
ID. (2004b), Oltre il mondo inanimato: la storia travagliata della
matematizzazione dei fenomeni biologici e sociali, in “Bollettino
dell’Unione 
Matematica 
Italiana”, 
8, 
7-B, 
pp. 
275-304;
ripubblicato in Atti del Diciassettesimo Congresso dell’Unione
Matematica Italiana (Milano, 8-13 settembre 2003), Unione
Matematica Italiana, Bologna 2004, pp. 129-158.
ID. (2005), The Science of Complexity: Epistemological Problems and
Perspectives, in “Science in Context”, 18, 3, pp. 1-31.
ID. (2007a), Lo strano concetto di punto materiale, in M. Emmer (a
cura di), Matematica e Cultura 2007, Springer Verlag Italia,
Milano, pp. 17-27.
ID. (2007b), Y a-t-il des lois en économie?, in A. Berthoud, B. Delmas,
Th. Demals (éds.), 11e Colloque de l’Association Charles Gide pour
l’Étude de la Pensée Économique, Presses Universitaires du
Septentrion, Villeneuve d’Ascq, pp. 19-35.
ID. (2011), La natura degli oggetti matematici alla luce del pensiero di
Husserl, Marietti, Genova-Milano.
ID. (2013a), Il bourbakismo. Saggio sull’ideologia di una delle ultime
scuole scientifiche con un’antologia di testi, e-book, Amazon-Kindle.
ID. (2013b), Essays on the History of Mathematical Biology, e-book,
Amazon-Kindle.

ISRAEL G., MILLÁN GASCA A. (2002), The Biology of Numbers: The
Correspondence 
of 
Vito 
Volterra 
on 
Mathematical 
Biology,
Birkhäuser, Basel-Boston-Berlin.
IDD. (2008), Il mondo come gioco matematico. La vita e le idee di John
von Neumann, Bollati Boringhieri, Torino.
IDD. (2012), Pensare in matematica, Zanichelli, Bologna.
KOYRÉ A. (1983), Studi newtoniani, Einaudi, Torino.
LAPLACE 
P.-S. (1825), Essai philosophique sur les probabilités,
introduction de R. Thom, Bourgois, Paris (rist. 1985).
MALINVAUD E. (1964), Méthodes statistiques de l’économetrie, Dunod,
Paris.
MALTHUS T. R. (1798), An Essay on the Principle of Population, J.
Johnson, London.
NEUMANN J. VON (1947), The Mathematician, in R. B. Heywood (ed.),
The Works of the Mind, University of Chicago Press, Chicago, pp.
180-96.
NEWTON I. (1706), Optiks, London.
OSTER G., GUCKENHEIMER J. (1976), Bifurcation Phenomena in Population
Models, in J. E. Marsden, M. McCracken (eds.), The Hopf Bifurcation
and Its Applications, Springer, New York, pp. 327-31.
PLUTARCO (1998), Vite parallele. Pelopida e Marcello, Rizzoli, Milano.
POINCARÉ H. (1895), Théorie analytique de la propagation de la chaleur,
Gauthier-Villars, Paris.
ID. 
(1912), 
L’hypothèse 
des 
quanta, 
in 
Dernières 
pensées,

Flammarion, Paris 1913, pp. 110-27.
POPPER K. R., ECCLES J. C. (1981), L’Io e il suo cervello. Materia, coscienza e
cultura, 3 voll., Armando, Roma.
ROSSI A. (1998), Il fantasma dell’intelligenza. Alla ricerca della mente
artificiale, Edizioni CUEN, Napoli.
SCHRÖDINGER E. (1932), Über Indeterminismus in der Physik, J. A.
Barth, Leipzig.
STEWART I. (2012), The Equation That Caused the Bank to Crash. The
Black-Scholes Equation Was the Mathematical Justification for the
Trading That Plunged the World’s Bank into Catastrophe, in “The
Observer”, february 11, on line.
SZENT-GYÖRGYI A. (1964), Teaching and the Expanding Knowledge, in
“Science”, 146, pp. 1278-9.
TRUESDELL C. A. (1981), Il calcolatore: rovina della scienza e minaccia per il
genere umano, in P. Rossi (a cura di), La nuova ragione, il Mulino,
Bologna.
VAN DER POL B. L. (1934), The Non-Linear Theory of Electric Oscillations,
in “Proceedings of the Institute of Radio Engineers”, 22, 9, pp.
1051-86.
VOLTERRA V. (1906), Sui tentativi di applicazione delle matematiche alle
scienze biologiche e sociali, in Id., Saggi scientifici, Zanichelli,
Bologna 1920 (rist. anast. 1990), pp. 3-33.
ID. (1907), Il momento scientifico presente e la nuova Società per il
progresso delle scienze, in Id., Saggi scientifici, Zanichelli, Bologna
1920 (rist. anast. 1990), pp. 97-118.

ID. (1913), Leçons sur les fonctions de lignes, Gauthier-Villars, Paris.
WIGNER E. (1960), The Unreasonable Effectiveness of Mathematics in
the Natural Sciences, in “Communications on Pure and Applied
Mathematics”, XIII, pp. 1-14.

