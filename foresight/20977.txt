n
KA
THOLIEKE
UNIVERSITEIT
LEUVEN
F
A
CUL
TEIT
TOEGEP
ASTE
WETENSCHAPPEN
DEP
AR
TEMENT
COMPUTER
WETENSCHAPPEN
Celestijnenlaan
00A
{
00
Leuv
en
(Hev
erlee)
TOP-DO
WN
INDUCTION
OF
FIRST
ORDER
LOGICAL
DECISION
TREES
Jury
:
Prof.
Dr.
ir.
E.
Aernoudt,
v
o
orzitter
Prof.
Dr.
ir.
M.
Bruyno
oghe,
promotor
Prof.
Dr.
L.
De
Raedt,
promotor
Prof.
Dr.
B.
Demo
en
Dr.
S.
D

zeroski,
Institut
\Jo

zef
Stefan",
Ljubljana,
Slo
v
eni

e
Prof.
Dr.
D.
Fisher,
V
anderbilt
Univ
ersit
y
,
Nash
ville,
T
ennessee,
USA
Prof.
Dr.
I.
V
an
Mec
helen
Pro
efsc
hrift
v
o
orgedragen
tot
het
b
ehalen
v
an
het
do
ctoraat
in
de
to
egepaste
w
etensc
happ
en
do
or
Hendrik
BLOCKEEL
U.D.C.
.*I
Decem
b
er
		

c
Katholiek
e
Univ
ersiteit
Leuv
en
-
F
aculteit
T
o
egepaste
W
etensc
happ
en
Aren
b
ergk
asteel,
B-00
Hev
erlee
(Belgium)
Alle
rec
h
ten
v
o
orb
ehouden.
Niets
uit
deze
uitga
v
e
mag
w
orden
v
ermenigvuldigd
en/of
op
en
baar
gemaakt
w
orden
do
or
middel
v
an
druk,
foto
copie,
microlm,
elektronisc
h
of
op
w
elk
e
andere
wijze
o
ok
zonder
v
o
orafgaande
sc
hriftelijk
e
to
estemmimg
v
an
de
uitgev
er.
All
righ
ts
reserv
ed.
No
part
of
the
publication
ma
y
b
e
repro
duced
in
an
y
form
b
y
prin
t,
photoprin
t,
microlm
or
an
y
other
means
without
written
p
ermission
from
the
publisher.
D/		//
ISBN
	0---

Preface
Do.
Or
do
not.
Ther
e
is
no
try.
|
Y
o
da,
Je
di
master
This
text
describ
es
the
main
results
of
the
researc
h
I
p
erformed
with
the
Ma-
c
hine
Learning
group
of
the
Departmen
t
of
Computer
Science
at
the
Katholiek
e
Univ
ersiteit
Leuv
en.
On
a
global
lev
el,
this
researc
h
is
ab
out
the
application
p
ossibilities
of
inductiv
e
logic
programming
in
the
con
text
of
databases.
More
sp
ecically
,
the
researc
h
fo
cuses
on
t
w
o
topics.
The
rst
topic
is
what
w
e
call
inductiv
e
database
design:
nding
a
go
o
d
design
for
a
(deductiv
e)
database
from
a
set
of
extensionally
dened
relations.
The
second
topic
is
the
applica-
tion
of
inductiv
e
logic
programming
for
data
mining.
As
b
oth
topics
dier
a
bit
to
o
m
uc
h
to
b
e
handled
elegan
tly
in
one
single
text,
this
text
only
encompasses
the
second
topic.
The
rst
one
has
b
een
describ
ed
in
the
literature.
I
guess
it
is
needless
to
men
tion
that
this
w
ork
w
ould
not
ha
v
e
b
een
realized
without
the
help
of
a
great
man
y
p
eople.
It
do
es
not
seem
p
ossible
to
giv
e
ev
en
a
probably
appro
ximately
complete
o
v
erview
of
all
the
p
eople
who
ha
v
e
signican
tly
inuenced
it.
Nev
ertheless
I
w
ould
lik
e
to
express
m
y
gratitude
to
some
p
eople
in
particular.
First
of
all,
I
w
an
t
to
thank
the
Flemish
Institute
for
the
Promotion
of
Scien
tic
and
T
ec
hnological
Researc
h
in
the
Industry
(Vlaams
Instituut
v
o
or
de
Bev
ordering
v
an
het
W
etensc
happ
elijk-T
ec
hnologisc
h
Onderzo
ek
in
de
In-
dustrie,
IWT)
for
funding
this
researc
h.
Their
nancial
supp
ort
is
not
the
only
thing
I
am
grateful
for,
ho
w
ev
er;
it
has
also
b
een
v
ery
pleasing
to
notice
that
they
activ
ely
sho
w
in
terest
in
the
researc
h
they
fund,
b
y
follo
wing
up
pro
jects
and
regularly
sending
rep
orts
on
the
activities
of
the
institute.
During
the
four
y
ears
I
sp
en
t
at
the
Leuv
en
mac
hine
learning
group
I
ha
v
e
had
the
pleasure
to
w
ork
together
with
man
y
dieren
t
p
eople,
all
of
whom
con
tributed
in
their
o
wn
w
a
y
to
this
w
ork
and
to
the
stim
ulating
en
vironmen
t
that
made
it
p
ossible.
I
rst
w
an
t
to
men
tion
Hilde
Ad

e
and
Gun
ther
Sablon,
who
w
ere
w
orking
in
the
Leuv
en
mac
hine
learning
group
at
the
momen
t
I
joined
it.
Being
seasoned
researc
hers,
they
did
not
only
share
a
lot
of
their
kno
wledge
with
the
new
er
mem
b
ers
of
the
group,
but
most
of
all,
they
created
a
v
ery
i

ii
PREF
A
CE
pleasan
t
atmosphere
at
w
ork
|
an
atmosphere
that
is
still
there
three
y
ears
after
they
ha
v
e
left
the
group.
Luc
Dehasp
e
and
Wim
V
an
Laer
joined
the
group
around
the
same
time
I
did,
and
they
are
the
PhD
studen
ts
with
whom
I
ha
v
e
co-op
erated
most
closely
.
The
coun
tless
sp
on
taneous
discussions
among
us
ha
v
e
had
an
imp
ortan
t
inuence
on
m
y
w
ork
(and
I
hop
e
also
on
theirs!).
The
most
ph
ysical
evidence
of
their
con
tributions
to
this
w
ork,
ho
w
ev
er,
can
probably
b
e
found
inside
the
co
de
of
the
Tilde
system.
A
signican
t
part
of
this
co
de
w
as
b
orro
w
ed
directly
from
Wim's
ICL
and
Luc's
Cla
udien
system.
By
making
a
v
ailable
v
ery
readable
and
reusable
co
de,
they
ha
v
e
not
only
signican
tly
sp
ed
up
the
dev
elopmen
t
of
Tilde,
but
most
of
all,
motiv
ated
me
to
implemen
t
it
in
the
rst
place.
I
w
ould
not
do
justice
to
these
p
eople,
ho
w
ev
er,
if
I
only
praised
their
professional
qualities.
They
also
pro
v
ed
to
b
e
highly
enjo
y
able
compan
y
b
oth
at
w
ork
and
in
b
et
w
een.
The
arriv
al
of
Nico
Jacobs,
Kurt
Driessens
and
Jan
Ramon
further
added
to
the
go
o
d
atmosphere
in
our
mac
hine
learning
group
|
and
also
to
the
cosiness
of
our
oce,
for
that
matter.
They
w
ere
also
the
rst
users
of
Tilde,
alpha-testers
one
migh
t
sa
y
,
and
their
feedbac
k
has
b
een
v
ery
helpful.
Nico
and
Jan
ha
v
e
also
con
tributed
to
this
w
ork
b
y
p
erforming
some
of
the
exp
erimen
ts
men
tioned
here.
It
will
probably
not
come
as
a
surprise
that
the
p
eople
I
o
w
e
most
to
are
m
y
promotors:
Maurice
Bruyno
oghe
and
Luc
De
Raedt.
Maurice
is
undoubtedly
the
p
erson
who
has
had
the
most
condensed
inuence
on
this
w
ork.
While
he
has
follo
w
ed
it
from
a
greater
distance
than
Luc,
and
con
tacts
with
him
w
ere
less
frequen
t,
on
those
o
ccasions
where
he
did
ha
v
e
some
advice
it
w
as
usually
short
but
extremely
useful.
Some
p
eople
sa
y
a
picture
is
w
orth
a
thousand
w
ords.
I
guess
Maurice
m
ust
talk
in
pictures,
then.
But
the
p
erson
who
has
had
b
y
far
the
most
inuence
on
this
w
ork
is
Luc
De
Raedt.
I
cannot
express
ho
w
m
uc
h
I
o
w
e
to
him
in
only
a
few
paragraphs
|
actually
,
I'm
not
sure
if
ev
en
Maurice
could.
It
w
as
Luc
who
kindled
m
y
in
terest
in
mac
hine
learning
in
the
rst
place,
and
who
motiv
ated
me
to
apply
for
a
gran
t
at
the
IWT.
It
w
as
also
he
who
came
up
with
man
y
of
the
ideas
that
ha
v
e
b
een
eshed
out
in
this
text.
But
I
can
imagine
that
these
tasks,
while
p
erformed
in
an
excellen
t
man-
ner,
are
still
part
of
the
standard
job
sp
ecication
of
a
go
o
d
promotor.
Luc's
coac
hing
has
gone
far
b
ey
ond
that,
ho
w
ev
er.
I
feel
he
has
really
b
een
a
teac
her
to
me:
a
teac
her
who
taugh
t
me
ho
w
to
do
researc
h
and
ho
w
to
rep
ort
on
it;
who
con
tin
uously
ev
aluated
m
y
w
ork
and
oered
suggestions
for
impro
v
emen
ts;
who
p
oin
ted
to
existing
w
ork
related
to
mine
and
brough
t
me
in
to
con
tact
with
man
y
p
eople
in
the
mac
hine
learning
comm
unit
y;
and
who
promoted
and
de-
fended
m
y
w
ork
at
ev
ery
opp
ortunit
y
.

iii
I
am
also
v
ery
grateful
for
his
supp
ort
during
the
writing
of
this
text,
for
his
man
y
commen
ts
and
his
insisting
on
impro
ving
the
text
when
I
w
ould
ha
v
e
settled
for
less.
When,
at
some
momen
t
during
the
preparation
of
this
text,
I
had
found
the
time
to
w
atc
h
the
Star
W
ars
T
rilogy
,
and
heard
Y
o
da
sp
eak
the
w
ords
quoted
at
the
top
of
this
text,
it
o
ccurred
to
me
that
no
w
ords
could
b
etter
describ
e
Luc's
attitude.
Of
the
man
y
p
eople
I
ha
v
e
met
outside
the
mac
hine
learning
group
of
Leuv
en,
I
w
ould
lik
e
to
thank
in
particular
a
few
p
eople
who
ha
v
e
had
a
sp
ecial
inuence
on
this
w
ork.
First,
I
w
an
t
to
men
tion
Sa

so
D

zeroski,
who
has
at
sev
eral
times
co-op
erated
with
our
group,
and
who
has
b
een
one
of
the
most
en
thousiastic
users
of
the
Tilde
system.
A
t
the
Fifte
enth
International
Confer
enc
e
on
Machine
L
e
arning
I
ha
v
e
had
the
pleasure
to
meet
Douglas
Fisher,
who
w
as
so
kind
to
set
some
time
aside
for
a
discussion
and
to
p
oin
t
us
to
some
relev
an
t
w
ork.
It
has
b
een
particularly
pleasing
to
see
that
b
oth
Douglas
and
Sa

so
accepted
the
in
vitation
to
serv
e
as
a
Jury
mem
b
er
for
this
dissertation.
I
also
w
an
t
to
thank
the
mem
b
ers
of
the
reading
committee
for
their
v
ery
useful
commen
ts
on
an
earlier
draft
of
this
text.
Besides
their
impro
ving
the
nal
form
of
this
text,
eac
h
of
them
has
had
a
signican
t
inuence
on
the
researc
h
itself.
I
already
men
tioned
Luc
De
Raedt
and
Maurice
Bruyno
oghe.
Iv
en
V
an
Mec
helen
has
inuenced
this
w
ork
mainly
through
his
course
on
In-
ductive
classic
ation
metho
ds.
This
course
pro
vided
a
v
ery
dieren
t
view
on
classication
than
the
t
ypical
inductiv
e
logic
programming
views,
and
this
has
broadened
the
scop
e
of
this
text
a
lot.
With
resp
ect
to
the
implemen
tational
asp
ects
I
w
an
t
to
thank
Bart
Demo
en,
who
has
often
oered
help
and
explan-
ations
concerning
the
Prolog
engine
w
e
use,
and
who
has
activ
ely
participated
in
impro
ving
the
eciency
of
Tilde.
Sp
ecial
thanks
go
to
Wim
V
an
Laer,
who
has
b
een
so
kind
to
pro
of-read
the
Dutc
h
summary
of
this
text.
I
w
ould
lik
e
to
end
these
ac
kno
wledgemen
ts
b
y
men
tioning
F
rank
Matthijs
and
Bart
V
anhaute,
who
had
nothing
to
do
with
this
text
or
the
researc
h
describ
ed
in
it.
In
a
sense
they
ha
v
e
b
een
all
the
more
v
aluable
to
me
b
ecause
of
that.

iv
PREF
A
CE

List
of
Sym
b
ols
The
follo
wing
list
indicates
the
meaning
of
sym
b
ols
that
are
commonly
used
throughout
the
text.
Some
of
these
sym
b
ols
(e.g.,
single
letters)
ma
y
in
sp
ecic
cases
b
e
used
in
a
dieren
t
con
text;
in
those
cases
their
meaning
is
alw
a
ys
explicitly
men
tioned.
j
=
en
tailmen
t
^
conjunction
op
erator
_
disjunction
op
erator
:
negation
op
erator
 
implication
op
erator
~
S
information
corresp
onding
to
a
space
S

S
p
o
w
er
set
of
S
B
bac
kground
theory
c
a
clause
C
a
cluster
C
a
clustering
C(E
)
the
clustering
space
of
a
set
of
instances
E
C
L
(E
)
in
tensional
clustering
space
of
E
w.r.t.
a
language
L
C
x
n
binomial
co
ecien
t
:
C
x
n
=
n!=(x!

(n
 x)!)
c
onj
a
conjunction
C
U
class
utilit
y
D
description
space;
the
subspace
of
I
disjoin
t
with
P
d
a
distance
d
E
Euclidean
distance
d
=
equalit
y
distance
dl
function
mapping
decision
trees
on
decision
lists
e
an
instance
from
a
giv
en
set
of
instances
E
E
a
set
of
instances
E
a
partition
of
a
set
of
instances
E
exp
ected
v
alue
op
erator
v

vi
LIST
OF
SYMBOLS
f
cluster
assignmen
t
function
F
(s)
a
logical
form
ula
that
refers
to
the
v
ariable
s
I
instance
space
L
subspace
of
I
 P
a
v
ailable
only
to
the
learner
L
a
language

lo
ok
ahead
op
erator
Lits(c)
set
of
literals
o
ccuring
in
a
clause
c
M(P
)
minimal
Herbrand
mo
del
of
logic
program
P
M
S
E
mean
squared
error
of
prediction
p
protot
yp
e
function
P
prediction
space
pr
ed
C
predictor
function
dened
b
y
clustering
C
P
U
partition
utilit
y

target
function
Q
qualit
y
criterion
 
Q
query
I
R
the
set
of
real
n
um
b
ers
I
R
+
the
set
of
p
ositiv
e
real
n
um
b
ers
R
E
relativ
e
error

renemen
t
op
erator
S
S
T
total
sum
of
squared
distances
S
S
B
sum
of
squared
distances
b
et
w
een
sets
S
S
W
sum
of
squared
distances
within
sets
tr
function
mapping
decision
lists
on
decision
trees

a
test
in
a
no
de
of
a
decision
tree
T
a
set
of
tests
x
an
arbitrary
instance
x;
y
instances
represen
ted
as
v
ectors
x

;
:
:
:
;
x
n
rst
.
.
.
n-th
comp
onen
t
of
a
v
ector
x
Y
target
v
ariable
y
a
v
alue
of
the
target
v
ariable

Con
ten
ts

In
tro
duction

.
Mac
hine
Learning
and
Articial
In
telligence
.
.
.
.
.
.
.
.
.
.
.

.
Data
Mining
and
Kno
wledge
Disco
v
ery
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
Logic
Programming
and
Inductiv
e
Logic
Programming
.
.
.
.
.

.
Connections
Bet
w
een
These
Fields
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
Motiv
ation
and
Con
tributions
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
Structure
of
this
T
ext
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.


A
Clustering
Approac
h
to
Prediction
	
.
In
tro
duction
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
	
.
Distances
and
Protot
yp
es
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0
..
Distances
Bet
w
een
Individual
Examples
.
.
.
.
.
.
.
.
.
0
..
Distances
Bet
w
een
Sets
of
Examples
.
.
.
.
.
.
.
.
.
.
.

..
Protot
yp
es
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
Clustering
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

..
Problem
Denition
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

..
Dimensions
of
Clustering
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

..
Ev
aluation
of
Clusterings
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
Using
Clusterings
for
Prediction
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

..
Extensional
Clusterings
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

..
In
tensional
Clusterings
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

..
Mixing
Extensional
and
In
tensional
Reasoning
.
.
.
.
.

.
Creating
Clusterings
for
Prediction
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

..
Clustering
in
Dieren
t
Spaces
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

..
Do
Learners
Ignore
Useful
Information?
.
.
.
.
.
.
.
.
.

..
Applications
of
Predictiv
e
Clustering
.
.
.
.
.
.
.
.
.
.
.

.
Related
w
ork
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
Conclusions
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

vii

viii
CONTENTS

T
op-do
wn
Induction
of
Decision
T
rees

.
In
tro
duction
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
Decision
T
rees
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
Induction
of
Decision
T
rees
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

..
Splitting
Heuristics
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

..
Stopping
Criteria
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

..
Information
in
Lea
v
es
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

..
P
ost-pruning
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

..
Summary
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
T
rees
v
ersus
Rules
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

..
Induction
of
Rule
sets
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

..
A
Comparison
Bet
w
een
T
rees
and
Rules
.
.
.
.
.
.
.
.
.
	
.
Related
w
ork
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
Conclusions
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.


First
Order
Logic
Represen
tations

.
In
tro
duction
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
Concept
Learning
and
In
tensional
Clustering
.
.
.
.
.
.
.
.
.
.

.
A
ttribute
V
alue
Learning
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
Learning
from
In
terpretations
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
A
Relational
Database
Viewp
oin
t
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

..
A
ttribute
V
alue
Learning
V
ersus
Learning
from
Multiple
Relations
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

..
Con
v
ersion
from
Relational
Database
to
In
terpretations

.
Learning
from
En
tailmen
t
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
	
.
Relationships
Bet
w
een
the
Dieren
t
Settings
.
.
.
.
.
.
.
.
.
.

..
On
the
Origin
of
Learning
F
rom
In
terpretations
.
.
.
.
.

..
Learning
F
rom
In
terpretations
Links
A
ttribute
V
alue
Learn-
ing
to
Learning
F
rom
En
tailmen
t
.
.
.
.
.
.
.
.
.
.
.
.
.

..
Adv
an
tages
of
Learning
F
rom
In
terpretations
.
.
.
.
.
.
	
..
Limitations
of
Learning
F
rom
In
terpretations
.
.
.
.
.
.
	
.
Related
W
ork
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
	0
.	
Conclusions
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
	0

Decision
T
rees
in
First
Order
Logic
	
.
In
tro
duction
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
	
.
Setting
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
	
.
First
Order
Logical
Decision
T
rees
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
	
..
Denition
of
First
Order
Logical
Decision
T
rees
.
.
.
.
.
	
..
Seman
tics
of
F
OLDTs
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
	
..
On
the
Expressiv
eness
of
F
OLDTs
and
Other
F
ormalisms
	
.
Related
W
ork
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
		
.
Conclusions
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
00

CONTENTS
ix

T
op-do
wn
Induction
of
First
Order
Logical
Decision
T
rees
0
.
In
tro
duction
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0
.
Arc
hitecture
of
Tilde
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0
.
Upgrading
TDIDT
to
First
Order
Logic
.
.
.
.
.
.
.
.
.
.
.
.
.
0
..
Computing
the
Set
of
T
ests
for
a
No
de
.
.
.
.
.
.
.
.
.
.
0
..
Discretization
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
Instan
tiations
of
TDIDT
in
Tilde
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

..
Classication
T
rees
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

..
Regression
T
rees
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

..
Clustering
T
rees
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
	
.
An
Example
of
Tilde
at
W
ork
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
Some
Eciency
Considerations
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

..
Scalabilit
y
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

..
Querying
Examples
Ecien
tly
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
Exp
erimen
tal
Ev
aluation
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

..
Materials
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

..
Building
Classication
T
rees
with
Tilde
.
.
.
.
.
.
.
.
.
	
..
The
Inuence
of
Lo
ok
ahead
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

..
The
Inuence
of
Discretization
.
.
.
.
.
.
.
.
.
.
.
.
.
.

..
Regression
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

..
Clustering
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

..
The
Eect
of
Pruning
on
Clustering
.
.
.
.
.
.
.
.
.
.
.
	
..
Handling
Missing
Information
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
Related
w
ork
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.	
Conclusions
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.


Scaling
up
Tilde
T
o
w
ards
Large
Data
Sets

.
In
tro
duction
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
Dieren
t
Implemen
tations
of
Tilde
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
Optimizations
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0
.
Exp
erimen
ts
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

..
Data
Sets
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

..
Materials
and
Settings
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

..
Exp
erimen
t
:
Time
Complexit
y
.
.
.
.
.
.
.
.
.
.
.
.
.

..
Exp
erimen
t
:
The
Eect
of
Lo
calization
.
.
.
.
.
.
.
.
0
..
Exp
erimen
t
:
Practical
Scaling
Prop
erties
.
.
.
.
.
.
.

.
Related
W
ork
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
Conclusions
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.


Conclusions
	

x
CONTENTS
A
Data
Sets

A.
So
yb
eans
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

A.
Iris
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

A.
Mutagenesis
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

A.
Bio
degradabilit
y
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

A.
Musk
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
	
A.
Mesh
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
	
A.
Diterp
enes
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
	
A.
Rob
oCup
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
	
A.	
P
ok
er
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
	

List
of
Figures
.
The
kno
wledge
disco
v
ery
pro
cess.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
A
hierarc
hical
clustering.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
An
in
tensional
hierarc
hical
clustering,
dieren
t
from
the
one
in
Figure
..
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
An
example
of
clustering,
in
the
con
text
of
fruit
and
v
egetables.

.
Predictiv
e
clustering.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
	
.
Conceptual
clustering.
The
w
a
y
in
whic
h
the
cross
w
ould
t
yp-
ically
b
e
divided
in
to
t
w
o
clusters
dep
ends
on
the
con
text.
The
con
text
itself
is,
in
these
dra
wings,
suggested
b
y
the
other
p
oin
ts
in
the
data
set.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
Do
p
eople
think
the
sym
b
ol
in
the
middle
is
more
similar
to
the
letter
`B',
or
to
the
n
um
b
er
?
It
dep
ends
on
the
sym
b
ols
surrounding
it.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
Dieren
t
clustering
metho
ds.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
Clustering
systems
plotted
along
the
exibility
dimension.
.
.
.
0
.	
The
dierence
b
et
w
een
predictiv
e
and
descriptiv
e
clustering.
(a)
original
set
of
p
oin
ts;
(b)
a
bad
descriptiv
e
but
go
o
d
predictiv
e
clustering;
(c)
a
go
o
d
descriptiv
e
but
bad
predictiv
e
clustering.
0
.0
Clustering
based
prediction.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
Clustering
in
the
instance
space
and
in
its
subspaces.
.
.
.
.
.
.

.
An
example
of
an
instance
that
has
a
high
probabilit
y
of
b
eing
misclassied
b
y
a
simple
tree.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
Making
a
prediction
for
an
example
using
a
decision
tree.
.
.
.

.
Tw
o
steps
in
the
mapping
from
instances
to
predictions.
.
.
.
.

.
A
fruit
classier
in
decision
tree
format.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
The
TDIDT
algorithm.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0
.
Examples
of
rule
sets
in
the
F
ruit&V
egetables
example.
.
.
.
.

.
The
co
v
ering
algorithm,
also
kno
wn
as
separate-and-conquer.
.

xi

xii
LIST
OF
FIGURES
.
A
simple
tree,
together
with
an
equiv
alen
t
rule
set.
Although
the
rule
set
is
m
uc
h
more
complex
than
the
tree,
it
cannot
b
e
simplied
further.
A
decision
list,
ho
w
ev
er,
do
es
allo
w
for
a
more
compact
represen
tation.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
A
correct
denition
of
a
pair
in
Prolog.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
A
c
hemical
database.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
Con
v
ersion
of
a
relational
database
to
in
terpretations.
.
.
.
.
.

.
Construction
of
the
sub
database
KB
H

O
.
.
.
.
.
.
.
.
.
.
.
.
.
0
.
A
correct
denition
of
a
pair
in
Prolog.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
Graphical
represen
tation
of
the
relationship
b
et
w
een
the
dier-
en
t
settings,
fo
cusing
on
separation
of
information
and
op
enness.

.
Logical
decision
tree
enco
ding
the
target
h
yp
othesis
of
Example
..
	
.
Making
a
prediction
for
an
example
using
a
F
OLDT
(with
bac
k-
ground
kno
wledge
B
).
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
	
.
Mapping
F
OLDT's
on
to
logic
programs.
.
.
.
.
.
.
.
.
.
.
.
.
.
	
.
The
tree
of
Figure
.,
with
asso
ciated
clauses
and
queries
added;
and
the
logic
program
deriv
ed
from
the
tree.
.
.
.
.
.
.
.
.
.
.
	
.
Arc
hitecture
of
the
Tilde
system.
Arro
ws
denote
information
o
w.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0
.
Algorithm
for
rst-order
logical
decision
tree
induction.
.
.
.
.
0
.
Pruning
algorithm
based
on
the
use
of
v
alidation
sets.
The
al-
gorithm
w
orks
in
t
w
o
steps.
First,
for
eac
h
no
de
of
the
tree
the
qualit
y
of
the
no
de
if
it
w
ould
b
e
a
leaf
is
recorded
(p),
as
w
ell
as
the
qualit
y
of
the
no
de
if
it
is
not
pruned
but
the
subtree
starting
in
it
is
pruned
in
an
optimal
w
a
y
(u).
In
a
second
step,
the
tree
is
pruned
in
those
no
des
where
p
>
u.
QUALITY
is
a
parameter
of
the
algorithm;
it
yields
the
qualit
y
of
a
prediction
on
the
v
alidation
set.
cov
(T
)
denotes
the
set
of
examples
in
the
training
set
co
v
ered
b
y
T
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
Tilde
illustrated
on
the
running
example.
A
screen
dump
of
a
run
is
sho
wn,
as
w
ell
as
a
graphical
represen
tation
of
the
tree-
building
pro
cess.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
An
output
le
generated
b
y
Tilde
(sligh
tly
simplied).
.
.
.
.

.
Simplication
of
queries.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
Comparison
of
Tilde's
p
erformance
with
and
without
lo
ok
a-
head,
(a)
on
the
Mutagenesis
data;
(b)
on
the
Mesh
data.
.
.
.

.
Inuence
of
n
um
b
er
of
thresholds
on
accuracy:
(a)
Musk
dataset,
comparing
equalities
and
inequalities;
(b)
Diterp
enes
dataset,
comparing
in
terv
als
with
inequalities
and
no
discretization
at
all.


LIST
OF
FIGURES
xiii
.	
Comparison
of
running
times
for
the
dieren
t
approac
hes
(Diter-
p
enes
dataset).
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.0
So
yb
eans:
a)
Accuracy
b
efore
and
after
pruning;
b)
n
um
b
er
of
no
des
b
efore
and
after
pruning.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0
.
Mutagenesis:
Accuracy
and
size
of
the
clustering
trees.
.
.
.
.
.

.
Ev
olution
of
predictiv
e
accuracy
in
the
presence
of
missing
v
alues.

.
Computation
of
the
b
est
test
Q
b
in
Tildeclassic.
.
.
.
.
.
.
.
.

.
The
TildeLDS
algorithm.
The
w
a
ce
function
is
dened
in
Figure
..
The
stop
crit
and
mod
al
class
functions
are
the
instan
tiations
of
STOP
CRIT
and
INF
O
for
classication
as
men
tioned
in
Chapter
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
	
.
The
dierence
b
et
w
een
Tildeclassic
and
TildeLDS
in
the
w
a
y
they
pro
cess
the
examples
and
renemen
ts.
.
.
.
.
.
.
.
.
.
.
.

.
In
terlea
v
ed
computation
of
answ
er
substitutions
for
Q
and
the
success
of
eac
h
renemen
t
on
a
single
example.
.
.
.
.
.
.
.
.
.

.
Scaling
prop
erties
of
TildeLDS
in
terms
of
n
um
b
er
of
examples.
	
.
The
eect
of
gran
ularit
y
on
induction
and
compilation
time.
.
.

.
Consumed
CPU-time
and
accuracy
of
h
yp
otheses
pro
duced
b
y
TildeLDS
in
the
P
ok
er
domain,
plotted
against
the
n
um
b
er
of
examples.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
Consumed
CPU-time
for
TildeLDS
in
the
Rob
oCup
domain,
plotted
against
the
n
um
b
er
of
examples.
.
.
.
.
.
.
.
.
.
.
.
.
.

A.
An
example
from
the
So
yb
eans
data
set.
.
.
.
.
.
.
.
.
.
.
.
.
.

A.
Examples
from
the
Iris
data
set.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

A.
The
Prolog
represen
tation
of
one
example
in
the
Mutagenesis
data
set.
The
atom
facts
en
umerate
the
atoms
in
the
molecule.
F
or
eac
h
atom
its
elemen
t
(e.g.
carb
on),
t
yp
e
(e.g.
carb
on
can
o
ccur
in
sev
eral
congurations;
eac
h
t
yp
e
corresp
onds
to
one
sp
ecic
conguration)
and
partial
c
harge.
The
bond
facts
en
u-
merate
all
the
b
onds
b
et
w
een
the
atoms
(the
last
argumen
t
is
the
t
yp
e
of
the
b
ond:
single,
double,
aromatic,
etc.).
pos
denotes
that
the
molecule
b
elongs
to
the
p
ositiv
e
class
(i.e.
is
m
utagenic).

A.
A
t
ypical
settings
le
for
the
Mutagenesis
data
set.
.
.
.
.
.
.
.

A.
A
part
of
one
example
from
the
Musk
data
set:
the
molecule
called
MUSK-.
It
has
sev
eral
conformations,
referred
to
as

+
etc.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
	
A.
The
same
molecule
as
sho
wn
in
Figure
A.,
but
using
a
dieren
t
represen
tation.
Eac
h
conformation
is
describ
ed
b
y
a
single
fact
df
i
for
eac
h
of
its

n
umerical
attributes.
.
.
.
.
.
.
.
.
.
.
.
	0
A.
A
settings
le
for
the
Musk
data
set.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
	
A.
Data
represen
tation
in
the
Mesh
data
set.
.
.
.
.
.
.
.
.
.
.
.
.
	

xiv
LIST
OF
FIGURES
A.	
A
t
ypical
settings
le
for
the
Mesh
data
set.
The
task
is
dened
as
regression
on
the
rst
argumen
t
of
the
resolution
predicate.
	
A.0
The
Prolog
represen
tation
of
one
example
in
the
Rob
oCup
data
set.
A
fact
suc
h
as
player(other,,-
.
0
	
,
.
0
0
,
		
)
means
that
pla
y
er

of
the
other
team
w
as
last
seen
at
p
osi-
tion
(-,.)
at
time
		.
A
p
osition
of
(0,0)
means
that
that
pla
y
er
has
nev
er
b
een
observ
ed
b
y
the
pla
y
er
that
has
generated
this
mo
del.
The
action
p
erformed
curren
tly
b
y
this
pla
y
er
is
turn(.	0
)
:
it
is
turning
to
w
ards
the
ball.
.
.
.
.
.
	
A.
An
example
from
the
P
ok
er
data
set.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
	
A.
A
t
ypical
settings
le
for
the
P
ok
er
data
set.
.
.
.
.
.
.
.
.
.
.
.
	

List
of
T
ables
.
Ov
erview
of
the
dieren
t
tasks
that
can
b
e
p
erformed
with
TDIDT
b
y
instan
tiating
its
pro
cedure
parameters.
.
.
.
.
.
.
.
.
.
.
.
.
0
.
CNF
and
DNF
denitions
for
the
+
and
 classes
from
Figure
..

.
Represen
ting
examples
for
learning
p
ok
er
concepts.
Eac
h
tuple
represen
ts
one
hand
of
v
e
cards
and
the
name
that
is
giv
en
to
the
hand.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
	
.
Constructed
attributes
for
learning
p
ok
er
concepts.
The
mean-
ing
of,
e.g.,
Er
is
that
cards

and

ha
v
e
equal
rank.
.
.
.
.
0
.
Represen
ting
the
p
ok
er
data
in
the
learning
from
in
terpretations
setting.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
Represen
ting
the
p
ok
er
data
in
the
learning
from
en
tailmen
t
set-
ting.
Negativ
e
examples
are
written
with
a
preceding
:-
sym
b
ol;
the
original
information
is
written
as
a
commen
t.
.
.
.
.
.
.
.
.

.
Comparison
of
tests
in
the
con
tin
uous
domain
and
the
discrete
domain.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
Accuracies,
times
and
complexities
of
theories
found
b
y
Pr
o-
gol,
F
oil
and
Tilde
for
the
Mutagenesis
problem;
a
v
eraged
o
v
er
ten-fold
cross-v
alidation.
Times
for
Tilde
w
ere
measured
on
a
Sun
SP
AR
Cstation-0,
for
the
other
systems
on
a
Hewlett
P
ac
k
ard
0.
Because
of
the
dieren
t
hardw
are,
times
should
b
e
considered
to
b
e
indicativ
e
rather
than
absolute.
.
.
.
.
.
.
.
.
0
.
Comparison
of
accuracy
of
theories
obtained
with
Tilde
with
those
of
other
systems
on
the
Musk
dataset.
.
.
.
.
.
.
.
.
.
.
.
0
.
Accuracy
results
on
the
Diterp
enes
data
set,
making
use
of
prop
ositional
data,
relational
data
or
b
oth;
standard
errors
for
Tilde
are
sho
wn
b
et
w
een
paren
theses.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
Comparison
of
Tilde's
p
erformance
with
and
without
lo
ok
ahead
on
the
Mutagenesis
and
Mesh
data
sets.
.
.
.
.
.
.
.
.
.
.
.
.
.

xv

xvi
LIST
OF
T
ABLES
.
Comparison
of
regression
and
classication
on
the
bio
degradab-
ilit
y
data.
RE
=
relativ
e
error
of
predictions;
acc.
=
prop
ortion
of
predictions
that
are
correct.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
Comparing
TIC
with
a
sup
ervised
learner.
.
.
.
.
.
.
.
.
.
.
.
.

.
Prediction
of
all
attributes
together
in
the
So
yb
eans
data
set.
.
	
.	
Classication
accuracies
obtained
for
Mutagenesis
with
sev
eral
distance
functions,
and
on
sev
eral
lev
els
of
missing
information.

.
Ov
erview
of
the
dieren
t
optimizations.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
Comparison
of
dieren
t
Tilde
v
ersions
on
Mutagenesis:
TildeLDS,
Tildeclassic,
Tildeclassic
without
lo
calization
but
with
index-
ing
({lo
c,
+ind)
and
Tildeclassic
without
lo
calization
and
without
indexing
({lo
c,
{ind).
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0
.
Consumed
CPU-time
and
accuracy
of
h
yp
otheses
pro
duced
b
y
TildeLDS
in
the
P
ok
er
domain.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
Consumed
CPU-time
of
h
yp
otheses
pro
duced
b
y
TildeLDS
in
the
Rob
oCup
domain;
for
induction
times
standard
errors
are
added.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.


Chapter

In
tro
duction
This
w
ork
is
situated
at
the
in
tersection
of
sev
eral
scien
tic
domains.
It
is
primarily
ab
out
mac
hine
learning,
whic
h
in
itself
is
a
sub-domain
of
articial
in
telligence.
Muc
h
of
the
researc
h
in
mac
hine
learning
is
also
relev
an
t
to
the
quite
new
eld
of
data
mining.
And
nally
,
the
researc
h
builds
on
logic
pro-
gramming,
whence
it
b
orro
ws
a
kno
wledge
represen
tation
formalism.
In
this
in
tro
ductory
c
hapter,
w
e
situate
our
researc
h
in
the
con
text
of
these
dieren
t
elds.
.
Mac
hine
Learning
and
Articial
In
telligence
The
term
articial
intel
ligenc
e
is
hard
to
dene,
whic
h
is
reected
b
y
the
fact
that
man
y
dieren
t
denitions
exist.
Russell
and
Norvig
(		)
(p.
)
giv
e
a
nice
o
v
erview
of
sev
eral
denitions,
classifying
them
along
t
w
o
dimensions.
F
or
this
in
tro
duction,
ho
w
ev
er,
it
is
probably
easiest
to
adopt
the
follo
wing
nearly
trivial
denition:
Denition
.
(Articial
in
telligence)
A
rticial
intel
ligenc
e,
as
a
scientic
eld,
is
the
study
of
how
to
make
machines
exhibit
the
kind
of
intel
ligenc
e
that
human
b
eings
exhibit.
While
it
is
dicult
to
giv
e
a
concise
denition
of
h
uman
in
telligence,
it
is
relativ
ely
easy
to
iden
tify
certain
c
haracteristics
of
it,
suc
h
as
the
abilit
y
to
reason,
to
b
e
creativ
e,
etc.
One
v
ery
imp
ortan
t
c
haracteristic
is
the
abilit
y
to
le
arn.
An
y
agen
t
(b
e
it
a
h
uman,
an
animal
or
a
mac
hine)
that
could
b
e
called
in
telligen
t,
should
at
least
b
e
able
to
learn
from
its
past
exp
erience.
An
agen
t
that
blindly
mak
es
the
same
mistak
es
o
v
er
and
o
v
er
again
w
ould
nev
er
b
e
called
in
telligen
t.
Langley
(		)
prop
oses
the
follo
wing
denition
of
learning:



CHAPTER
.
INTR
ODUCTION
Denition
.
(Learning)
(Langley,
		)
L
e
arning
is
the
impr
ovement
of
p
erformanc
e
in
some
envir
onment
thr
ough
the
ac
quisition
of
know
le
dge
r
esult-
ing
fr
om
exp
erienc
e
in
that
envir
onment.
Machine
le
arning
can
then
b
e
trivially
dened
as
follo
ws:
Denition
.
(Mac
hine
learning)
Machine
le
arning,
as
a
scientic
eld,
is
the
study
of
how
to
make
machines
le
arn.
According
to
Langley's
denition
of
learning,
the
learning
pro
cess
consists
of
t
w
o
subtasks:
acquiring
kno
wledge,
and
putting
it
to
use.
In
this
text
w
e
will
b
e
concerned
mainly
with
the
acquisition
of
kno
wledge.
More
precisely
,
w
e
consider
the
inference
of
a
general
theory
(the
kno
wledge)
from
a
set
of
observ
ations
(the
exp
erience).
This
reasoning
from
sp
ecic
to
general
is
called
inductive
reasoning.
Example
.
The
follo
wing
are
some
examples
of
inductiv
e
reasoning
and
learning:

An
amateur
bird
w
atc
her
observ
es
0
ra
v
ens
and,
noticing
that
these
0
are
all
blac
k,
induces
that
all
ra
v
ens
are
blac
k.

A
c
hild
tries
to
build
to
w
ers
from
blo
c
ks
with
dieren
t
shap
es
(cub
es,
cones,
spheres,
.
.
.
).
After
sev
eral
trials
the
c
hild
induces
that
a
cub
e
on
top
of
a
cone
is
nev
er
stable,
and
a
cone
on
top
of
a
cub
e
is
alw
a
ys
stable.
If
the
c
hild
uses
the
disco
v
ered
kno
wledge
to
build
higher
to
w
ers,
w
e
can
sa
y
,
according
to
Langley's
denition,
that
it
has
learned
something.
The
amateur
bird
w
atc
her
could
use
the
kno
wledge
that
all
ra
v
ens
are
blac
k
when
attempting
to
classify
new
birds.

A
system
that
is
able
to
induce
kno
wledge
but
cannot
use
this
kno
wledge
is
not
a
learning
system,
according
to
the
ab
o
v
e
denition.
In
the
literature,
ho
w
ev
er,
learning
is
often
used
as
a
synon
ym
for
inductiv
e
reasoning,
and
in
the
remainder
of
this
text
w
e
will
also
use
it
in
this
broader
sense.
Only
in
this
in
tro
duction,
w
e
stic
k
to
the
narro
w
denition
of
learning.
When
a
system
induces
kno
wledge,
not
to
impro
v
e
its
o
wn
p
erformance,
but
simply
to
pro
vide
other
systems
(or
h
umans)
with
that
kno
wledge,
w
e
call
it
a
know
le
dge
disc
overy
system.
.
Data
Mining
and
Kno
wledge
Disco
v
ery
Kno
wledge
disco
v
ery
(F
ra
wley
et
al.,
		;
Piatetsky-Shapiro
and
F
ra
wley,
		;
F
a
yy
ad
et
al.,
		)
is
a
eld
that
has
recen
tly
b
ecome
v
ery
p
opular
in

..
D
A
T
A
MINING
AND
KNO
WLEDGE
DISCO
VER
Y

b
oth
the
scien
tic
and
the
industrial
comm
unit
y
.
Companies
are
in
terested
in
the
eld
b
ecause
of
its
large
application
p
oten
tial
for
mark
et
studies,
pro
cess
optimization,
and
other
w
a
ys
of
increasing
their
gains.
Example
.
Some
examples
of
business
applications
of
kno
wledge
disco
v
ery:

A
bank
migh
t
disco
v
er
that
a
certain
service
mainly
app
eals
to
a
sp
e-
cic
mark
et
segmen
t,
and
hence
fo
cus
a
promotional
campaign
on
that
segmen
t.

A
sup
ermark
et
migh
t
disco
v
er
that
t
w
o
pro
ducts
are
often
b
ough
t
to-
gether,
and
put
these
pro
ducts
far
a
w
a
y
from
one
another
so
that
man
y
customers
ha
v
e
to
w
alk
past
man
y
other
pro
ducts,
in
the
hop
e
that
this
will
increase
sales.

The
kno
wledge
disco
v
ery
task
is
often
describ
ed
as
follo
ws:
Denition
.
(Kno
wledge
disco
v
ery)
(F
ra
wley
et
al.,
		)
Know
le
dge
disc
overy
is
the
nontrivial
extr
action
of
implicit,
pr
eviously
unknown,
and
p
o-
tential
ly
useful
information
fr
om
data.
The
pro
cess
of
kno
wledge
disco
v
ery
actually
consists
of
sev
eral
subtasks
(see
also
Figure
.):

Pr
e-pr
o
c
essing
of
the
data.
The
data
in
their
original
form
ma
y
con
tain
missing
v
alues,
noise,
or
ma
y
simply
not
b
e
in
a
format
t
for
applying
a
data
mining
algorithm.
T
ransforming
the
data
in
to
a
suitable
format
can
b
e
an
elab
orate
and
non-trivial
task.

Data
mining.
One
or
more
algorithms
are
used
to
extract
general
la
ws,
patterns
or
regularities
from
the
data.

Post-pr
o
c
essing
of
the
r
esults.
The
results
obtained
b
y
the
data
mining
algorithm
ma
y
not
b
e
in
an
easily
in
terpretable
format.
It
can
therefore
b
e
desirable
to
transform
them
in
to
another,
more
in
telligible
format.
The
terms
\data
mining"
and
\kno
wledge
disco
v
ery"
ha
v
e
b
een
used
some-
what
inconsisten
tly
in
the
early
literature.
More
recen
tly
a
con
v
en
tion
w
as
generally
adopted
that
\data
mining"
refers
to
the
cen
tral
inductiv
e
reason-
ing
task,
and
\kno
wledge
disco
v
ery"
refers
to
the
whole
pro
cess
depicted
in
Figure
..
W
e
follo
w
this
terminology
in
this
text.


CHAPTER
.
INTR
ODUCTION
A,B -> C
Knowledge
1100010101
0010101011
1101101011
0101010101
1100011100
1110110111
0101010111
0101110011
0101010110
1110111001
0101011011
0000000000
1010010011
Raw data
pre-processing
of data
data mining
post-processing
of results
Figure
.:
The
kno
wledge
disco
v
ery
pro
cess.
.
Logic
Programming
and
Inductiv
e
Logic
Pro-
gramming
L
o
gic
pr
o
gr
amming
is
a
programming
paradigm
in
whic
h
programs
consist
of
rst
order
predicate
logic
form
ulae.
The
main
represen
tativ
e
of
logic
program-
ming
languages
is
Prolog.
In
this
text
it
is
assumed
that
the
reader
is
familiar
with
logic
programming
and
Prolog.
Readers
for
whom
this
assumption
do
es
not
hold
can
consult
(Bratk
o,
		0;
Sterling
and
Shapiro,
	;
Clo
c
ksin
and
Mellish,
	)
(ab
out
Prolog)
or
(Ko
w
alski,
		;
Llo
yd,
	)
(ab
out
logic
programming),
whic
h
are
excellen
t
in
tro
ductions
to
these
topics.
Logic
programming
engines
(suc
h
as
Prolog
systems)
can
de
duc
e
facts
from
logic
form
ulae;
i.e.
they
can
compute
those
facts
that
are
certainly
true,
assum-
ing
the
form
ulae
they
start
from
are
correct.
A
classic
example
of
deductiv
e
reasoning
is
the
follo
wing:
al
l
humans
ar
e
mortal,
and
So
cr
ates
is
human,
henc
e
So
cr
ates
is
mortal.
In
Prolog
the
premises
of
the
reasoning
w
ould
b
e
written
as
follo
ws:
mortal(X)
:-
human(X).
ffor
al
l
X:
if
X
is
human
then
X
is
mortalg
human(socrates).
fso
cr
ates
is
humang
(the
:-
sym
b
ol
represen
ts
the
implication
op
erator
 ).
One
could
ask
the
Prolog
system
whether
So
crates
is
mortal,
in
whic
h
case
the
system
w
ould
answ
er
yes,
or
ask
it
for
whic
h
X
it
can
pro
v
e
that
X
is
mortal,
in
whic
h
case
it

..
CONNECTIONS
BETWEEN
THESE
FIELDS

w
ould
answ
er
X
=
socrates.
While
the
classical
logic
programming
engines
implemen
t
deductiv
e
reason-
ing,
inductive
lo
gic
pr
o
gr
amming
(ILP)
engines
implemen
t
inductiv
e
reasoning.
One
could
e.g.
pro
vide
an
inductiv
e
logic
programming
system
with
the
fol-
lo
wing
data:
raven(bird).
raven(bird).
black(bird).
black(bird).
and
ask
it
what
prop
erties
ra
v
ens
ha
v
e.
The
system
could
then
come
up
with
the
rule
black(X)
:-
raven(X):
all
ra
v
ens
are
blac
k.
In
con
trast
to
deductiv
e
reasoning,
inductiv
e
reasoning
do
es
not
guaran
tee
that
the
answ
er
is
correct
(seeing

blac
k
ra
v
ens
do
es
not
guaran
tee
that
all
ra
v
ens
in
the
w
orld
are
blac
k).
Therefore
the
result
of
inductiv
e
inference
is
usually
called
a
hyp
othesis.
Suc
h
a
h
yp
othesis
needs
some
external
motiv
ation,
suc
h
as
statistical
evidence.
In
the
ab
o
v
e
example
the
evidence
is
rather
w
eak;
observing
large
n
um
b
ers
of
ra
v
ens
that
are
all
blac
k
w
ould
mak
e
it
stronger.
Induction
is
harder
than
deduction,
and
curren
tly
inductiv
e
logic
program-
ming
is
v
ery
m
uc
h
a
researc
h
topic.
Man
y
dieren
t
tec
hniques
and
approac
hes
exist,
but
at
presen
t
there
is
no
single
language
or
framew
ork
for
inductiv
e
logic
programming
that
has
the
status
Prolog
has
for
deductiv
e
logic
programming.
.
Connections
Bet
w
een
These
Fields
Both
mac
hine
learning
and
data
mining
dep
end
hea
vily
on
inductiv
e
reasoning.
As
data
mining
has
only
recen
tly
started
to
receiv
e
m
uc
h
atten
tion,
the
eld
is
less
mature
than
mac
hine
learning.
It
is
therefore
not
surprising
that
it
b
orro
ws
man
y
tec
hniques
from
mac
hine
learning
(and
also
from
statistics,
for
that
matter).
Since
inductiv
e
logic
programming
pro
vides
a
framew
ork
for
inductiv
e
reas-
oning,
it
is
an
ob
vious
candidate
as
a
paradigm
for
b
oth
mac
hine
learning
and
data
mining.
Ho
w
ev
er,
un
til
no
w,
it
has
not
b
een
used
extensiv
ely
for
mac
hine
learning,
and
ev
en
less
for
data
mining.
The
main
reason
for
this
seems
to
b
e
its
computational
complexit
y
.
While
ILP
is
more
p
o
w
erful
than
most
other
tec
hniques,
suc
h
as
attribute
v
alue
learning,
this
p
o
w
er
comes
at
an
eciency
cost.
Esp
ecially
in
the
con
text
of
data
mining,
where
v
ery
large
data
sets
are
often
considered,
eciency
is
crucial.
L
e
arning
fr
om
interpr
etations
(De
Raedt
and
D

zeroski,
		)
is
a
relativ
ely
no
v
el
setting
for
ILP
that
mak
es
it
p
ossible
to
alleviate
this
eciency
problem.
The
learning
from
in
terpretations
setting
could
b
e
situated
somewhere
b
et
w
een


CHAPTER
.
INTR
ODUCTION
classical
ILP
and
attribute
v
alue
learning,
with
resp
ect
to
b
oth
eciency
and
expressiv
e
p
o
w
er.
The
learning
from
in
terpretations
setting
op
ens
up
new
p
os-
sibilities
for
using
inductiv
e
logic
programming
in
the
elds
of
mac
hine
learning
and
data
mining.
.
Motiv
ation
and
Con
tributions
A
ttribute
v
alue
learning
is
m
uc
h
more
mature
than
inductiv
e
logic
program-
ming,
and
man
y
sophisticated
tec
hniques
for
a
v
ariet
y
of
tasks
exist
for
that
formalism.
The
main
motiv
ation
for
this
w
ork
is
the
desirabilit
y
of
upgrad-
ing
some
of
these
tec
hniques
to
the
rst
order
framew
ork
of
inductiv
e
logic
programming.
A
rst
con
tribution
of
this
text
is
that
w
e
unify
sev
eral
induction
tasks
(clas-
sication,
regression,
and
certain
kinds
of
clustering)
in
to
one
framew
ork
whic
h
w
e
call
predictiv
e
clustering.
The
result
is
one
general
inductiv
e
tec
hnique
that
can
b
e
sp
ecialized
to
w
ards
more
sp
ecic
tasks.
A
second
con
tribution
is
the
study
of
rst
order
logical
decision
trees.
These
are
an
upgrade
of
the
classical
prop
ositional
decision
trees,
as
used
for
attribute
v
alue
learning,
to
rst
order
predicate
logic.
The
prop
erties
of
these
trees
are
studied,
and
it
is
sho
wn
that
within
the
learning
from
in
terpretations
frame-
w
ork
they
are
more
expressiv
e
than
the
at
logic
programs
that
most
ILP
sys-
tems
induce.
This
study
also
sheds
new
ligh
t
on
sev
eral
other
represen
tation
formalisms
and
puts
them
in
p
ersp
ectiv
e.
As
a
third
con
tribution,
w
e
upgrade
the
induction
of
decision
trees
in
the
general
framew
ork
of
predictiv
e
clustering
to
w
ards
inductiv
e
logic
programming
b
y
means
of
rst
order
logical
decision
trees.
W
e
presen
t
an
implemen
tation
of
the
tec
hnique
and
ev
aluate
it
empirically
.
The
system
is
the
rst
inductiv
e
logic
programming
system
to
com
bine
classication,
regression,
and
sev
eral
kinds
of
clustering.
.
Structure
of
this
T
ext
In
Chapter

w
e
discuss
the
clustering
task
in
detail,
and
w
e
iden
tify
a
sp
ecial
kind
of
clustering
that
w
e
call
predictiv
e
clustering.
W
e
sho
w
that
certain
other
inductiv
e
tasks
that
are
usually
not
considered
to
b
e
clustering
tasks
(induction
of
classiers,
regression)
are
in
fact
sp
ecial
cases
of
predictiv
e
clustering.
In
Chapter

w
e
demonstrate
ho
w
induction
of
decision
trees,
a
tec
hnique
that
is
often
used
for
induction
of
classiers
or
regression,
can
b
e
generalized
to
predictiv
e
clustering.
W
e
sho
w
that
man
y
classical
approac
hes
to
decision
tree
induction
are
instan
tiations
of
our
general
framew
ork.

..
STR
UCTURE
OF
THIS
TEXT

In
Chapter

dieren
t
represen
tation
formalisms
for
learning
are
compared.
One
of
these,
the
inductiv
e
logic
programming
setting
called
le
arning
fr
om
interpr
etations
will
b
e
used
throughout
the
remainder
of
the
text.
In
Chapter

w
e
in
tro
duce
and
study
rst
order
logical
decision
trees.
These
trees
form
a
stepping
stone
for
upgrading
the
general
decision
tree
induction
tec
hnique
from
Chapter

to
inductiv
e
logic
programming.
Chapter

presen
ts
the
Tilde
system,
whic
h
is
an
implemen
tation
of
the
general
decision
tree
induction
tec
hnique
from
Chapter

that
induces
rst
order
logical
decision
trees
as
dened
in
Chapter
.
Details
of
the
implemen
tation
are
discussed
and
the
system
is
ev
aluated
empirically
.
In
Chapter

w
e
in
v
estigate
ho
w
the
prop
osed
tec
hniques
scale
up
to
w
ards
large
data
sets,
as
the
abilit
y
to
handle
large
data
sets
is
essen
tial
for
data
mining.
W
e
discuss
a
re-implemen
tation
of
Tilde
that
aims
sp
ecically
at
w
orking
with
large
data
sets
and
ev
aluate
its
scaling
prop
erties
empirically
.
Chapter

concludes
b
y
discussing
the
main
con
tributions
of
this
w
ork
and
p
oin
ting
to
future
w
ork.


CHAPTER
.
INTR
ODUCTION

Chapter

A
Clustering
Approac
h
to
Prediction
.
In
tro
duction
Within
mac
hine
learning
and
kno
wledge
disco
v
ery
a
large
v
ariet
y
of
predictiv
e
induction
tec
hniques
exist.
While
it
is
clear
that
man
y
of
these
are
related
to
one
another,
few
authors
stress
this
relationship
or
explicitly
exploit
it.
There
are,
for
instance,
man
y
texts
on
classication
and
regression,
but
few
texts
treat
b
oth
at
the
same
time
(the
CAR
T
b
o
ok
(Breiman
et
al.,
	)
b
eing
an
imp
ortan
t
exception).
In
this
text
w
e
try
to
stress
the
similarities
b
et
w
een
the
dieren
t
tasks
b
y
explicitly
taking
the
viewp
oin
t
that
they
are
just
sp
ecic
kinds
of
clustering.
In
this
c
hapter
w
e
in
tro
duce
clustering,
and
discuss
ho
w
it
can
form
the
basis
of
a
general
approac
h
to
predictiv
e
induction
that
encompasses
man
y
existing
approac
hes
to
classication
and
regression
but
also
oers
further
p
os-
sibilities.
This
predictiv
e
clustering
approac
h
is
the
one
that
will
b
e
tak
en
in
the
remainder
of
this
text.
W
e
rst
in
tro
duce
the
concepts
of
distances
and
protot
yp
es
(Section
.).
In
Section
.
w
e
discuss
clustering.
W
e
sho
w
ho
w
a
sp
ecial
kind
of
clustering
that
w
e
call
predictiv
e
clustering
generalizes
o
v
er
man
y
curren
t
approac
hes
to
classication
and
regression,
b
oth
sup
ervised
and
unsup
ervised.
W
e
also
lo
cate
our
approac
h
b
y
giving
a
more
global
(though
certainly
not
complete)
view
on
clustering
metho
ds,
discussing
them
along
sev
eral
dimensions.
Finally
,
w
e
discuss
the
ev
aluation
of
clusterings.
In
Section
.
w
e
tak
e
a
closer
lo
ok
at
ho
w
clusterings
can
b
e
used
for
making
predictions,
situating
existing
approac
hes
in
this
view.
In
Section
.
w
e
discuss
sev
eral
w
a
ys
in
whic
h
the
clusters
can
b
e
formed.
W
e
again
lo
cate
the
classical
	

0
CHAPTER
.
A
CLUSTERING
APPR
O
A
CH
TO
PREDICTION
approac
hes,
and
sho
w
that
our
framew
ork
oers
sev
eral
opp
ortunities
that
are
not
exploited
b
y
most
of
them
or
generalize
o
v
er
ad
ho
c
metho
ds.
Section
.
discusses
some
related
w
ork
and
Section
.
concludes.
.
Distances
and
Protot
yp
es
In
order
to
nd
clusters
of
similar
ob
jects,
one
rst
needs
a
similarit
y
measure.
Similarit
y
is
often
expressed
b
y
means
of
a
distanc
e
measure:
the
more
similar
t
w
o
ob
jects
are,
the
smaller
the
distance
b
et
w
een
them
is.
W
e
dene
distances
b
et
w
een
instances
and
sets
of
instances,
and
then
dene
the
protot
yp
e
of
a
set
as
the
instance
that
is
most
represen
tativ
e
for
the
set.
..
Distances
Bet
w
een
Individual
Examples
Denition
.
(Distance)
A
function
d
:
I

I
!
I
R
+
is
a
distanc
e
on
I
if
and
only
if
it
is
p
ositive
denite,
symmetric
and
full
ls
the
triangle
ine
quality:
x;
y

I
:
d(x;
y
)

0
and
d(x;
y
)
=
0
,
x
=
y
(.)
x;
y

I
:
d(x;
y
)
=
d(y
;
x)
(.)
x;
y
;
z

I
:
d(x;
y
)

d(x;
z
)
+
d(z
;
y
)
(.)
In
some
cases
these
constrain
ts
are
to
o
restrictiv
e
and
one
ma
y
relax
them;
the
resulting
measure
is
then
sometimes
called
a
dissimilarit
y
measure.
In
this
text
w
e
will
only
use
distances.
There
are
man
y
dieren
t
w
a
ys
to
dene
a
distance.
Ev
en
in
the
simplest
case
where
example
descriptions
are
v
ectors
of
n
umerical
v
alues,
a
c
hoice
can
b
e
made
b
et
w
een,
e.g.,

the
Euclide
an
distanc
e:
d
E
(x;
y
)
=
d

(x;
y
)
=
s
X
i
(x
i
 y
i
)

(.)

the
Manhattan
distanc
e:
d

(x;
y
)
=
X
i
jx
i
 y
i
j
(.)

the
Chebyshev
distanc
e:
d

(x;
y
)
=
max
i
jx
i
 y
i
j
(.)

..
DIST
ANCES
AND
PR
OTOTYPES

whic
h
are
all
sp
ecial
cases
of
the
Minkowski
family
of
distanc
es:
d
L
=
(
X
i
jx
i
 y
i
j
L
)
=L
(.)
It
ma
y
b
e
necessary
to
rescale
the
dieren
t
dimensions
in
order
to
mak
e
them
comparable,
e.g.,
to
rescale
so
that
the
v
alues
along
eac
h
dimension
ha
v
e
a
standard
deviation
of
.
The
Mahalanobis
distanc
e
d
M
=
(x
i
 y
i
)C
 
(x
i
 y
i
)
0
(.)
with
C
the
co
v
ariance
matrix
of
the
v
ector
comp
onen
ts
(C
ij
=

ij
)
p
erforms
suc
h
rescaling,
and
also
tak
es
co
v
ariance
b
et
w
een
v
ariables
in
to
accoun
t.
Another
complication
is
that
it
ma
y
b
e
desirable
to
apply
transformations
to
the
v
ariables.
E.g.,
is

kg
of
salt
more
similar
to

g
than
0
kg
is
to

kg?
Probably
not,
but
the
dierence

kg
{

g
=
			
g
is
smaller
than

kg
{
0
kg
=

kg.
A
logarithmic
transformation
ma
y
b
e
in
place
here.
F
or
nominal
v
ariables
a
useful
distance
is
the
e
quality
distanc
e
d
=
:
d
=
(x;
y
)
=
0
,
x
=
y
(.	)
=

otherwise
(.0)
whic
h
for
v
ectors
of
nominal
v
ariables
generalizes
to
the
Hamming
distanc
e:
d
H
(x;
y
)
=
X
i
d
=
(x
i
;
y
i
)
(.)
Un
til
no
w
w
e
ha
v
e
only
considered
v
ectors
of
either
n
umerical
or
nominal
v
ariables;
the
situation
b
ecomes
more
complex
when
a
v
ector
can
consist
of
a
mix
of
nominal
and
n
umerical
v
ariables.
Dieren
t
notions
of
distance
ha
v
e
to
b
e
used
for
the
dieren
t
comp
onen
ts,
and
the
problem
of
incomparabilit
y
b
ecomes
ev
en
larger;
see
e.g.
(Wilson
and
Martinez,
		)
for
a
discussion
and
prop
osed
solutions.
The
problem
escalates
ev
en
more
when
one
uses
rst-order
descriptions
of
examples.
A
v
eritable
w
ealth
of
distances
and
dissimilarities
has
b
een
prop
osed
in
this
con
text,
some
of
them
v
ery
complex
and
not
w
ell
understo
o
d;
see
e.g.
(Emde
and
W
ettsc
herec
k,
		;
Bisson,
		b;
Hutc
hinson,
		;
Nienh
uys-Cheng,
		;
Ramon
and
Bruyno
oghe,
		;
Ramon
et
al.,
		;
Sebag,
		).
In
the
follo
wing
w
e
assume
that
a
distance
measure
d
that
computes
the
distance
d(e

;
e

)
b
et
w
een
examples
e

and
e

is
already
giv
en.
Ho
w
one
should
decide
up
on
whic
h
distance
to
use
is
out
of
the
scop
e
of
this
text,
and
w
e
refer
to
the
literature
men
tioned
ab
o
v
e.


CHAPTER
.
A
CLUSTERING
APPR
O
A
CH
TO
PREDICTION
..
Distances
Bet
w
een
Sets
of
Examples
Besides
the
need
to
measure
the
distance
b
et
w
een
individual
examples,
it
is
often
also
necessary
to
measure
the
distance
b
et
w
een
sets
of
examples.
Again,
dieren
t
approac
hes
ha
v
e
b
een
suggested
in
the
literature.
The
dissimilarit
y
b
et
w
een
t
w
o
sets
of
instances
S

and
S

can,
e.g.,
b
e
dened
as

the
smallest
distance
b
et
w
een
an
y
x

S

and
y

S

:
d
0
min
;d
(S

;
S

)
=
inf
xS

;y
S

d(x;
y
)
(.)

the
largest
distance
:
d
0
max
;d
(S

;
S

)
=
sup
xS

;y
S

d(x;
y
)
(.)

the
a
v
erage
distance
:
d
0
avg
;d
(S

;
S

)
=
X
xS

;y
S

d(x;
y
)=(jS

j

jS

j)
(.)

the
Hausdor
distanc
e:
d
0
Hausdor
;d
(S

;
S

)
=
max
f
sup
xS

inf
y
S

d(x;
y
);
sup
xS

inf
y
S

d(x;
y
)g
(.)

the
c
enter
distanc
e:
d
0
c
enter
;d
;p
(S

;
S

)
=
d(p(S

);
p(S

))
(.)
where
the
function
p
mapping
a
set
on
to
its
cen
ter
is
to
b
e
sp
ecied
(see
b
elo
w)
Note
that
ev
en
when
the
dissimilarit
y
b
et
w
een
single
examples
is
a
distance,
the
dissimilarit
y
b
et
w
een
sets
do
es
not
necessarily
inherit
this
prop
ert
y
.
The
Hausdor
distance
and
the
cen
ter
distance
inherit
the
prop
erties
of
a
distance,
except
the
prop
ert
y
that
d
0
(S

;
S

)
=
0
)
S

=
S

.
In
this
text
w
e
will
adopt
the
cen
ter-based
approac
h.
The
notion
of
the
cen
ter
of
a
set
of
examples
is
formalized
in
the
concept
of
a
protot
yp
e.


The
term
\protot
yp
e"
w
as
c
hosen
here
only
b
ecause
it
con
v
eys
a
suitable
in
tuitiv
e
mean-
ing.
Our
use
of
the
term
is
not
necessarily
related
to
its
use
in
other
con
texts.

..
CLUSTERING

..
Protot
yp
es
Denition
.
(Protot
yp
e
function,
protot
yp
e)
A
pr
ototyp
e
function
is
a
function
p
:

I
!
I
that
maps
a
set
of
instanc
es
E
onto
a
single
instanc
e
p(E
).
p(E
)
is
c
al
le
d
the
pr
ototyp
e
of
E
.
Ideally
,
a
protot
yp
e
of
a
set
E
should
b
e
maximally
represen
tativ
e
for
the
instances
in
E
,
i.e.,
as
close
as
p
ossible
to
an
y
e

E
.
Denition
.
(Ideal
protot
yp
e
function)
Given
a
distanc
e
d
over
an
in-
stanc
e
sp
ac
e
I
,
a
pr
ototyp
e
function
p
is
ide
al
if
and
only
if
E

I
:
x

I
:
X
e
i
E
d(x;
e
i
)


X
e
i
E
d(p(E
);
e
i
)

(.)
While
other
criteria
could
b
e
c
hosen
instead
of
the
least
squares
criterion,
c
ho
osing
the
latter
in
Denition
.
ensures
that
in
a
Euclidean
space
(with
distance
d
E
)
the
ideal
protot
yp
e
p(E
)
is
the
mean
of
all
the
v
ectors
in
E
,
whic
h
is
in
tuitiv
ely
a
go
o
d
c
hoice
for
a
protot
yp
e.
In
a
space
of
nominal
v
al-
ues
with
distance
d
=
(x;
y
),
the
ideal
protot
yp
e
corresp
onds
to
the
mo
de
(the
most
frequen
tly
o
ccurring
v
alue),
whic
h
is
also
desirable.
In
a
rst
order
do-
main,
a
protot
yp
e
function
could,
e.g.,
b
e
the
(p
ossibly
reduced)
least
general
generalization
of
the
examples
in
the
cluster,
using
Plotkin's
(	0)
notion
of

-subsumption
or
the
v
arian
ts
corresp
onding
to
structural
matc
hing
(Bisson,
		b;
De
Raedt
et
al.,
		).
Example
.
Giv
en
I
=
I
R

C
with
C
=
fblue;
red
g,
and
d(x;
y
)
=
(x

 y

)

+
d
=
(x

;
y

),
a
maximally
represen
tativ
e
protot
yp
e
of
the
set
of
instances
f(,
blue),
(,
red),
(,
blue)g
is
(,
blue).

Unless
stated
otherwise,
in
the
remainder
of
this
text
w
e
assume
that
a
protot
yp
e
function
p
is
giv
en,
and
that
p
is
ideal.
The
dissimilarit
y
b
et
w
een
t
w
o
sets
of
examples
E

and
E

is
then
dened
as
the
distance
d(p(E

);
p(E

))
b
et
w
een
the
protot
yp
es
of
the
sets.
.
Clustering
..
Problem
Denition
Before
dening
the
clustering
task,
w
e
dene
the
concepts
that
are
in
v
olv
ed.
Denition
.
(Extensional
at
clustering)
A
n
extensional
at
cluster-
ing
C
of
a
set
of
instanc
es
E
is
a
p
artition
of
E
.


CHAPTER
.
A
CLUSTERING
APPR
O
A
CH
TO
PREDICTION
++ +
++ +
+
+
+ +
+
+
+
+
E
C1
C2
C3
C4
E
C1
C2
C3
C4
X
Y
4
3
Figure
.:
A
hierarc
hical
clustering.
Denition
.
(Extensional
hierarc
hical
clustering)
A
n
extensional
hier-
ar
chic
al
clustering
C
of
a
set
of
instanc
es
E
is
a
set
of
subsets
of
E
such
that
E

C
(.)
e

E
:
feg

C
(.	)
C

;
C


C
:
C


C

or
C


C

or
C

\
C

=
;
(.0)
Example
.
Figure
.
sho
ws
a
set
of
data
E
,
and
an
extensional
hierarc
hical
clustering
of
the
set.
The
clustering
is
represen
ted
in
t
w
o
w
a
ys:
b
y
dra
wing
the
clusters
in
the
instance
space,
and
as
a
tree.
The
extensional
hierarc
hical
clustering
represen
ted
in
the
gure
is
C
=
fE
;
C

;
C

;
C

;
C

g
[
S
eE
feg.
Some
at
clusterings
are
C

=
fC

;
C

g
and
C

=
fC

;
C

;
C

g.

Denition
.
(Cluster)
The
elements
of
an
extensional
clustering
ar
e
c
al
le
d
clusters.
An
extensional
clustering
of
a
set
E
is
uniquely
determined
b
y
describing
the
clusters
(whic
h
are
just
sets
of
instances)
in
it.
W
e
distinguish
t
w
o
kinds
of
descriptions:
Denition
.
(Extensional
description)
A
n
extensional
description
of
a
set
S
is
of
the
form
S
=
fs

;
s

;
:
:
:
;
s
n
g,
i.e.
an
enumer
ation
of
its
elements.
Denition
.
(In
tensional
description)
A
n
intensional
description
of
a
set
S
in
a
language
L
is
of
the
form
S
=
fsjF
(s)g
wher
e
F
(s)
is
a
sentenc
e
(formula)
in
L.

..
CLUSTERING

+ +
++ +
+
+
+ +
+
+
+
+
E
C2’
C3’
a
3
5
2
4
C4’
X
Y
+
C1’
Figure
.:
An
in
tensional
hierarc
hical
clustering,
dieren
t
from
the
one
in
Figure
..
While
there
is
a
one-to-one
corresp
ondence
b
et
w
een
an
extensional
cluster-
ing
of
a
set
E
and
the
extensional
description
of
its
clusters,
the
corresp
ondence
with
in
tensional
cluster
descriptions
is
one-to-man
y
.
I.e.,
dieren
t
in
tensional
cluster
descriptions
ma
y
corresp
ond
to
the
same
extensional
clustering.
Example
.
In
Figure
.
the
cluster
C

could
b
e
represen
ted
in
tensionally
as
C

=
f(x;
y
)

E
j(x
 )

+
(y
 )

<
g,
whic
h
is
suggested
b
y
dra
wing
its
b
oundary
as
a
circle.
Figure
.
represen
ts
a
hierarc
hical
clustering
C
0
that
is
in
a
sense
dieren
t;
the
in
tensional
description
of
cluster
C
0

could,
e.g.,
b
e
C
0

=
f(x;
y
)

E
j
<
x
<

^

<
y
<
g.
Still,
the
extensional
clusterings
dened
b
y
C
and
C
0
are
the
same.

In
order
to
b
e
able
to
distinguish
clusterings
that
are
in
tensionally
dieren
t,
w
e
dene
in
tensional
(as
opp
osed
to
extensional)
clusterings
as
follo
ws:
Denition
.	
(In
tensional
at
clustering)
Given
an
instanc
e
sp
ac
e
I
and
a
set
of
instanc
es
E

I
,
an
intensional
at
clustering
C
of
E
is
a
function
C
:
I
!

I
such
that
S
eE
fC
(e)g
is
an
extensional
at
clustering
of
E
.
Denition
.0
(In
tensional
hierarc
hical
clustering)
Given
an
instanc
e
sp
ac
e
I
and
a
set
of
instanc
es
E

I
,
an
intensional
hier
ar
chic
al
clustering
C
of
E
is
a
function
C
:
I
!


I
such
that
e

E
:
feg

C
(e)
(.)
e

E
:
E

C
(e)
(.)
e

E
:
C

;
C


C
(e)
:
C


C

_
C


C

(.)
[
eE
C
(e)
is
an
extensional
hier
ar
chic
al
clustering.
(.)


CHAPTER
.
A
CLUSTERING
APPR
O
A
CH
TO
PREDICTION
An
in
tensional
at
clustering
of
a
set
of
instances
E
maps
eac
h
instance
of
E
on
to
a
single
cluster.
An
in
tensional
hierarc
hical
clustering
of
E
maps
eac
h
instance
e

E
on
to
a
set
of
clusters
that
are
con
tained
in
one
another
and
in
the
tree
represen
tation
form
a
path
from
the
top
to
feg.
Example
.
The
instance
lab
eled
a
in
Figure
.
is
mapp
ed
b
y
C
0
on
to
the
set
of
clusters
fE
;
C
0

;
C
0

;
fagg,
i.e.,
the
set
of
all
the
clusters
in
C
0
that
a
b
elongs
to.
An
in
tensional
at
clustering
could,
e.g.,
map
a
on
to
C
0

.

Assuming
eac
h
cluster
C
i
in
an
extensional
hierarc
hical
clustering
is
de-
scrib
ed
in
tensionally
using
a
form
ula
F
i
,
this
set
of
in
tensional
cluster
descrip-
tions
uniquely
denes
a
function
C
:
I
!


I
:
C
i

C
(x)
,
F
i
(x)
(.)
whic
h
is
an
in
tensional
hierarc
hical
clustering
according
to
Denition
.0.
Con
v
ersely
,
if
t
w
o
in
tensional
hierarc
hical
clusterings
C
and
C
0
are
equal,
their
in
tensional
descriptions
F
i
and
F
0
i
for
an
y
cluster
C
i
m
ust
b
e
equiv
alen
t
in
the
sense
that
x

I
:
F
i
(x)
,
F
0
i
(x).
A
similar
reasoning
can
b
e
made
for
in
tensional
at
clusterings.
W
e
conclude
that
in
tensional
clusterings
are
equal
if
and
only
if
all
their
in
tensional
cluster
descriptions
are
equiv
alen
t;
in
other
w
ords,
Denitions
.0
and
.	
adequately
capture
in
tensionalit
y
as
dened
in
Denition
..
Denition
.
(Clustering)
C
is
a
clustering
of
E
if
and
only
if
it
is
an
intensional
or
extensional,
hier
ar
chic
al
or
at,
clustering
of
E
.
Denition
.
(Clustering
space)
The
set
of
al
l
clusterings
of
E
is
c
al
le
d
the
clustering
sp
ac
e
of
E
and
is
written
C(E
).
W
e
can
no
w
form
ulate
the
clustering
problem
as
follo
ws:

Denition
.
(T
ask
denition
for
clustering)
We
dene
the
clustering
task
as
fol
lows:
Giv
en:

a
set
of
instanc
es
E

a
distanc
e
d
on
E

and
a
quality
criterion
Q
dene
d
over
C(E
)

More
general
form
ulations
exist.
One
p
ossible
extension
is
that
the
distance
is
replaced
b
y
a
dissimilarit
y
.
A
further
extension
is
to
start
from
a
n

n
dissimilarity
matrix,
i.e.
no
descriptions
of
the
ob
jects
themselv
es
are
giv
en,
only
the
dissimilarities
b
et
w
een
them
are;
see
e.g.
(Sneath
and
Sok
al,
	).
Still
other
approac
hes
allo
w
clusters
to
o
v
erlap.

..
CLUSTERING

Find:

a
clustering
C
such
that
Q(C
)
is
optimal,
i.e.
C
0

C(E
)
:
Q(C
0
)

Q(C
)
The
qualit
y
criterion
Q
is
not
sp
ecied
in
detail
in
Denition
.;
t
ypically
,
ho
w
ev
er,
Q
fa
v
ors
clusterings
in
whic
h
the
distance
b
et
w
een
t
w
o
clusters
is
large
(unless
one
is
a
sub
cluster
of
the
other)
and
the
distance
b
et
w
een
elemen
ts
of
the
same
cluster
is
small.
W
e
illustrate
the
clustering
task
with
a
to
y
example.
Example
.
Assume
that
a
set
of
ob
jects
is
giv
en;
eac
h
ob
ject
is
either
a
stra
wb
erry
,
a
tomato
or
an
apple.
Figure
.
lo
cates
the
ob
jects
in
a
t
w
o-
dimensional
space
(Color

Weight
).
A
go
o
d
at
clustering
should
put
all
stra
wb
erries
in
one
cluster,
tomato
es
in
another
one
and
apples
in
a
third
cluster.
A
go
o
d
hierarc
hical
clustering
should
also
iden
tify
these
clusters,
but
could
moreo
v
er
iden
tify
,
e.g.,
dieren
t
t
yp
es
of
apples.
In
this
example
w
e
only
consider
a
at
clustering.
In
Figure
.
b
oth
extensional
and
in
tensional
descriptions
of
the
dier-
en
t
clusters
are
sho
wn.
The
language
L
for
the
in
tensional
descriptions
is
assumed
to
b
e
prop
ositional
logic,
with
prop
ositions
of
the
form
A
ttribute(x)

value
with
A
ttribute
=
Color
or
Weight,


f<;
>;
;
;
=g
and
value

fred;
green;
blue;
:
:
:
g
[
I
R
(this
corresp
onds
to
the
so-called
attribute
v
alue
formalism).
The
cluster
of
apples,
for
instance,
is
extensionally
describ
ed
as
Apples
=
fc;
d;
f
;
l
;
ng
and
in
tensionally
as
Apples
=
fxjColor
(x)
=
green
^
Weight
(x)
>
0g
g
Note
that
an
equally
v
alid
in
tensional
description
of
the
cluster
of
apples
w
ould
b
e,
e.g.,
Apples
=
fxjColor
(x)
=
green
^
Weight
(x)
>
00g
g:

Predictiv
e
clustering
Man
y
mac
hine
learning
and
data
mining
tasks
in
v
olv
e
the
induction
of
a
pro
ced-
ure
to
classify
new
instances
or
to
predict
unkno
wn
v
alues
for
them.
Prediction
of
a
nominal
v
ariable
is
equiv
alen
t
to
classic
ation;
prediction
of
a
con
tin
uous
v
alue
is
usually
referred
to
as
r
e
gr
ession.
F
or
con
v
enience,
w
e
will
use
the
terms
classication
and
regression
b
oth
to
refer
to
the
induction
of
the
predictor
and
its
use
(although
originally
,
the
term
regression
denotes
the
construction
of
the
predictor
rather
than
its
use).


CHAPTER
.
A
CLUSTERING
APPR
O
A
CH
TO
PREDICTION
Intensional: 1: weight<40g and color=red
                     2: weight>120g and color=red
                     3: weight> 130g and color=green
Extensional: 1: {e,k,a,i}
                      2: {m,g,j,b,h}
                      3: {n,c,f,l,d}
a
k
i
e
b
color
red
green
g
h
j
weight (g)
c
d
f
l
m
n
40
80
120
Figure
.:
An
example
of
clustering,
in
the
con
text
of
fruit
and
v
egetables.
Both
classication
and
(certain
t
yp
es
of
)
regression

can
b
e
seen
as
sp
ecial
cases
of
clustering.
Indeed,
man
y
predictors
are
structured
(implicitly)
in
suc
h
a
w
a
y
that
an
unseen
instance
is
rst
assigned
to
a
certain
cluster,
and
the
protot
yp
e
of
the
cluster
is
then
used
to
mak
e
a
prediction.
T
o
sho
w
this,
w
e
rst
dene
pr
e
dictive
clustering
as
a
sp
ecial
case
of
clus-
tering
that
is
t
for
prediction
purp
oses.
It
is
assumed
that
a
function
f
exists
that
assigns
unseen
instances
to
existing
clusters,
and
that
the
protot
yp
e
of
the
assigned
cluster
is
used
as
a
prediction
for
the
unseen
instance;
the
qual-
it
y
criterion
Q
is
maximal
if
predictions
are
as
close
as
p
ossible
to
the
actual
v
alues,
according
to
the
distance
measure
d.
In
the
follo
wing
w
e
assume
that
a
probabilit
y
distribution
o
v
er
I
exists.
E
denotes
the
exp
ected
v
alue
op
erator.
Denition
.
(Cluster
assignmen
t
function)
A
cluster
assignment
func-
tion
is
a
function
f
:
I

C(I
)
!

I
such
that
x

I
;
E

I
:
C

C(E
)
:
f
(x;
C
)

C
(.)
I.e.,
given
any
clustering
C
of
a
set
of
instanc
es
E
,
f
assigns
e
ach
p
ossible
instanc
e
in
I
to
a
cluster
in
C
.
Denition
.
(Predictiv
e
clustering)
We
dene
the
task
of
pr
e
dictive
clustering
as
fol
lows:
Giv
en:

an
instanc
e
sp
ac
e
I

Sometimes
called
pie
c
e-wise
regression.

..
CLUSTERING
	
X
Y
1
2
3
1
2
0
0
a
b
C1
C2
Figure
.:
Predictiv
e
clustering.

a
distanc
e
d
over
I

a
set
of
instanc
es
E

I

a
pr
ototyp
e
function
p

a
cluster
assignment
function
f
Find:

a
clustering
C
over
E
that
maximizes
Q(C
)
=
 E(d(x;
p(f
(x;
C
)))

)
(.)
(wher
e
x
r
anges
over
I
)
Note
that,
except
for
the
min
us,
the
righ
t
hand
side
of
Equation
.
can
b
e
seen
as
a
kind
of
v
ariance.
Maximizing
Q
corresp
onds
to
minimizing
the
exp
ected
in
tra-cluster
v
ariance
in
the
clustering.
Example
.
Figure
.
illustrates
our
predictiv
e
clustering
setting.
W
e
as-
sume
that
I
=
I
R

and
d(x;
y
)
=
d
E
(x;
y
).
There
are
t
w
o
clusters
C

and
C

with
protot
yp
es
p(C

)
=
(;
)
and
p(C

)
=
(;
).
Assume
furthermore
that
w
e
ha
v
e
a
cluster
assignmen
t
function
that
maps
instances
on
to
the
cluster
of
whic
h
the
protot
yp
e
is
closest:
f
(x;
C
)
=
C
,
C
0

C
:
d(x;
p(C
0
))

d(x;
p(C
)):
Then
a
=
(;
)
w
ould
b
e
mapp
ed
on
to
(;
),
and
b
=
(;
)
w
ould
b
e
mapp
ed
on
to
(;
).


0
CHAPTER
.
A
CLUSTERING
APPR
O
A
CH
TO
PREDICTION
Maximizing
Q
in
tuitiv
ely
means
that
E
is
clustered
so
that
the
protot
yp
e
of
a
resulting
cluster
C
i
is
as
represen
tativ
e
as
p
ossible
for
the
part
of
the
instance
space
assigned
to
C
i
.
In
other
w
ords,
eac
h
example
in
I
will
b
e
mapp
ed
to
a
protot
yp
e
that
is
quite
similar
to
it.
The
protot
yp
e
can
b
e
seen
as
a
prediction,
or
as
a
basis
for
prediction.
Classication
and
regression
In
the
con
text
of
classication
and
regression,
the
prediction
usually
is
not
a
full
instance,
but
a
single
attribute
whic
h
w
e
call
the
target
attribute.
Denition
.
(T
arget
function)
Given
an
instanc
e
sp
ac
e
I
and
a
pr
e
dic-
tion
sp
ac
e
P
,
a
tar
get
function

:
I
!
P
is
a
function
mapping
instanc
es
onto
their
tar
get
value.
F
or
instance,
in
a
classication
setting,
w
e
call

(x)
the
class
of
x.
Denition
.
(Predictor,
prediction)
Given
a
cluster
assignment
func-
tion
f
,
a
pr
ototyp
e
function
p
and
a
tar
get
function

,
e
ach
clustering
C
denes
a
function
pr
ed
C
:
I
!
P
:
pr
ed
C
(x)
=

(p(f
(x;
C
)))
(.)
We
c
al
l
pr
ed
C
a
pr
e
dictor,
and
pr
ed
C
(x)
its
pr
e
diction
for
x.
W
e
can
no
w
dene
classication
and
regression
as
sp
ecial
cases
of
predictiv
e
clustering:
Denition
.
(Classication)
Classic
ation
is
a
sp
e
cial
c
ase
of
pr
e
dictive
clustering
wher
e
the
r
ange
of

is
nominal
and
the
distanc
e
d
b
etwe
en
two
instanc
es
is
d(x;
y
)
=
d
=
(
(x);

(y
)).
Most
classication
systems
try
to
maximize
predictiv
e
accuracy
,
whic
h
in
this
con
text
is
dened
as
follo
ws:
Denition
.	
(Predictiv
e
accuracy)
The
pr
e
dictive
ac
cur
acy
of
a
clas-
sier
pr
e
d
is
the
pr
ob
ability
that
it
makes
a
c
orr
e
ct
pr
e
diction,
i.e.
a
=
E(
 d
=
(pr
e
d
(x);

(x)))
(.	)
It
is
easy
to
sho
w
that
maximizing
a
indeed
corresp
onds
to
maximizing
Q
for
a
distance
d(x;
y
)
=
d
=
(
(x);

(y
)):
Q(C
)
=
 E(d(x;
p(f
(x;
C
)))

)
=
 E(d
=
(
(x);
pr
ed
C
(x))

)
=
 E(d
=
(
(x);
pr
ed
C
(x)))
(since
d
is
0
or
)
=
a
 

..
CLUSTERING

Denition
.0
(Regression)
R
e
gr
ession
is
a
sp
e
cial
c
ase
of
pr
e
dictive
clus-
tering
wher
e
the
r
ange
of

is
c
ontinuous
and
the
distanc
e
d
b
etwe
en
two
in-
stanc
es
is
d(x;
y
)
=
d
E
(
(x);

(y
)).
Most
regression
systems
minimize
the
mean
squared
prediction
error
M
S
E
=
E(d
E
(
(x);
pr
ed
C
(x))

)
(.0)
whic
h
is
easily
seen
to
b
e
equal
to
 Q(C
).
The
usefulness
of
this
formalization
ma
y
not
b
e
ob
vious
righ
t
no
w,
but
in
the
remainder
of
this
text
the
view
that
classication
and
regression
are
sp
ecial
t
yp
es
of
predictiv
e
clustering
will
b
e
adopted
sev
eral
times.
On
these
o
ccasions
the
ab
o
v
e
formalization
will
b
e
further
illustrated.
Limitations
of
this
approac
h
W
e
iden
tify
t
w
o
limitations
to
our
denition
of
classication
and
regression.

The
ab
o
v
e
denitions
presume
a
sp
ecic
qualit
y
criterion
for
predictions:
accuracy
in
the
case
of
classication,
the
least
squares
criterion
in
the
case
of
regression.
They
th
us
do
not
encompass
approac
hes
that
use
other
criteria.
This
could
b
e
c
hanged
b
y
making
the
qualit
y
criterion
in
Denition
.
more
general.

It
is
assumed
that
within
a
cluster
one
sp
ecic
v
alue
will
alw
a
ys
b
e
pre-
dicted
(the
protot
yp
e);
this
excludes,
e.g.,
linear
piece-wise
regression,
,
where
in
eac
h
cluster
a
linear
prediction
mo
del
should
b
e
stored
(e.g.,
Y
=
aX
+
b,
where
a
and
b
dier
according
the
cluster).
Our
denition
can
b
e
extended
b
y
making
the
protot
yp
e
p(C
)
a
function
that
maps
individual
instances
on
to
predictions,
rewriting
Equation
.
as
Q(C
)
=
 E(d(x;
p(C
)(x))

)
(.)
and
Equation
.
as
pr
ed
C
(x)
=

(p(f
(x;
C
))(x))
(.)
Example
.
Giv
en
a
cluster
of
v
ectors
C
=
f(;
);
(;
);
(;
)g
the
proto-
t
yp
e
of
the
cluster
could
b
e
a
function
that
maps
an
instance
(x;
y
)
(y
is
the
target
v
ariable)
on
to
(x;
x
+
).
The
prediction
for
an
instance
x
=
(;
0)
(assuming
it
has
b
een
assigned
to
C
)

is
then

(p(C
)(x))
=

((;
))
=
.
The
distance
d(x;
p(C
)(x))
=
j0
 j
=
.


W
e
assume
here
that
w
e
kno
w
that
y
=
0
but
the
predictor
do
es
not;
otherwise
it
need
not
really
compute
a
prediction.


CHAPTER
.
A
CLUSTERING
APPR
O
A
CH
TO
PREDICTION
In
this
text
w
e
will
not
mak
e
use
of
these
extensions,
therefore
w
e
stic
k
to
the
simple
denitions
as
stated.
These
denitions
already
encompass
a
large
subset
of
the
man
y
approac
hes
to
classication
and
regression.
A
last
remark:
in
practice,
d(x;
p(f
(x;
C
)))
is
unkno
wn
for
unseen
instances.
Q
is
then
usually
estimated
via
^
Q(C
)
=
 P
eE
d(e;
p(f
(e;
C
)))

jE
j
 c
ompl
(C
)
(.)
where
E
is
the
set
of
examples
from
whic
h
C
w
as
generated,
and
c
ompl(C
)
is
a
p
enalt
y
that
is
higher
for
more
complex
cluster
descriptions
(without
suc
h
a
p
enalt
y
,
the
optimal
clustering
w
ould
b
e
the
trivial
clustering
where
eac
h
cluster
consists
of
one
elemen
t).
..
Dimensions
of
Clustering
Clustering
systems
can
b
e
distinguished
along
man
y
dimensions.
It
is
not
our
in
ten
tion
to
giv
e
a
complete
o
v
erview
of
all
these
dimensions,
nor
to
lo
cate
all
the
existing
clustering
systems
along
these
dimensions.
Rather,
w
e
discuss
a
few
dimensions
to
giv
e
the
reader
just
a
a
v
or
of
the
v
ariet
y
of
clustering
systems
that
exists,
and
to
in
tro
duce
some
terminology
that
will
b
e
used
later
on
in
this
text.
In
this
section
w
e
rst
briey
discuss
at
v
ersus
hier
ar
chic
al
clustering.
W
e
next
discuss
intensional
v
ersus
extensional
clustering
systems,
then
in
tro
duce
the
notion
of
c
onc
eptual
clustering.
W
e
also
lo
ok
at
clustering
approac
hes
from
the
p
oin
t
of
view
of
the
exibility
they
ha
v
e
with
resp
ect
to
dening
clusters.
This
dimension
is
not
orthogonal
to
the
others
but
strongly
correlated
with
the
in
tensional/extensional
distinction.
Finally
w
e
compare
pr
e
dictive
and
descriptive
clustering.
Flat
v
ersus
hierarc
hical
clustering
As
migh
t
b
e
susp
ected
from
our
problem
denition
(Denition
.),
clustering
systems
can
b
e
divided
in
to
systems
that
nd
at
clusterings
and
systems
that
nd
hierarc
hical
clusterings.
Hierarc
hical
clustering
algorithms
can
b
e
(but
are
not
necessarily)
deriv
ed
from
at
clustering
algorithms
in
a
straigh
tforw
ard
w
a
y:
one
simply
rep
eats
the
at
clustering
algorithm
o
v
er
and
o
v
er
again.
The
divisive
approac
h
w
orks
top-do
wn:
the
data
set
is
divided
in
to
large
clusters,
eac
h
of
whic
h
is
then
divided
in
to
smaller
clusters,
and
so
on
up
to
the
lev
el
of
single
instances.
Alternativ
ely
,
the
agglomer
ative
approac
h
w
orks
b
ottom-up:
small
clusters
of
examples
are
formed,
then
the
clusters
are
joined
in
to
larger
clusters,
and
so
on
un
til
one
single
cluster
is
obtained.

..
CLUSTERING

The
Leader
algorithm
(Hartigan,
	)
is
an
example
of
a
at
clustering
algorithm.
An
example
of
an
agglomerativ
e
clusterer
is
KBG
(Bisson,
		a).
Divisiv
e
systems
are
R
umma
ge
(men
tioned
in
(Fisher
and
Langley,
	)),
Discon
(Langley
and
Sage,
	),
Cluster/
(Mic
halski
and
Stepp,
	),
.
.
.
The
incremen
tal
system
Cobweb
(Fisher,
	)
has
a
a
v
or
of
b
oth,
as
it
has
op
erators
b
oth
for
com
bining
and
splitting
clusters.
In
tensional
v
ersus
extensional
clustering
W
e
call
a
clustering
system
an
in
tensional
clustering
system
if
it
returns
in-
tensional
descriptions
of
the
clusters
it
nds;
otherwise
w
e
call
the
system
an
extensional
clustering
system.
The
Leader
algorithm
is
an
example
of
an
extensional
clustering
sys-
tem.
In
tensional
clusterers
are
KBG,
R
umma
ge,
Discon,
Cluster/
and
Cobweb.

The
descriptions
returned
b
y
an
in
tensional
clusterer
can
b
e
understo
o
d
in
t
w
o
w
a
ys.
They
can
b
e
seen
as
char
acterizing
the
clusters,
or
as
only
dis-
criminating
b
et
w
een
them.
While
c
haracteristic
descriptions
should
con
tain
a
maxim
um
of
information
ab
out
a
cluster,
discriminan
t
descriptions
should
con
tain
a
minim
um
of
information
(they
should
fo
cus
on
what
distinguishes
a
cluster
from
the
other
clusters).
Example
.
The
description
of
apples
in
Figure
.
is
\an
apple
is
green
and
w
eighs
more
than
0g".
If
w
e
w
an
t
to
answ
er
the
question
\What
are
apples
lik
e?",
a
b
etter
description
of
apples
that
is
still
consisten
t
with
the
picture
is
\an
apple
is
green
and
w
eighs
b
et
w
een
0g
and
0g".
It
is
b
etter
b
ecause
it
giv
es
more
information
ab
out
apples.
W
e
call
this
a
c
haracteristic
description.
Ho
w
ev
er,
if
w
an
t
to
answ
er
the
question
\Ho
w
can
I
kno
w
if
something
is
an
apple?",
the
answ
er
should
allo
w
to
discriminate
apples
from
stra
wb
erries
and
tomato
es
with
a
minim
um
of
information.
In
this
case
it
suces
to
lo
ok
at
the
color:
\if
the
ob
ject
is
green,
it
is
an
apple".
This
sen
tence
is
a
discriminan
t
description.

In
tensional
clustering
systems
ma
y
return
either
discriminan
t
descriptions,
c
haracteristic
descriptions,
or
b
oth.
An
approac
h
that
is
sometimes
follo
w
ed
is
that
an
in
tensional
clustering
system
rst
nds
discriminan
t
descriptions
to
dene
the
clusters,
then
for
eac
h
cluster
calls
a
pro
cedure
for
c
haracterizing
it.
In
our
example
the
language
that
is
used
for
an
in
tensional
description
of
clusters
is
based
on
the
same
prop
erties
as
the
distance
measure.
(A
cluster
can
b
e
describ
ed
b
y
means
of
the
w
eigh
t
and
color
of
its
elemen
ts,
and
w
e
w
an
t
to
nd
clusters
in
whic
h
elemen
ts
are
similar
with
resp
ect
to
w
eigh
t
and
color.)
In

Cobweb
returns
probabilistic
descriptions,
whic
h
are
in
tensional
in
a
broader
sense
than
dened
b
y
Denition
..


CHAPTER
.
A
CLUSTERING
APPR
O
A
CH
TO
PREDICTION
general,
this
need
not
b
e
the
case.
The
in
tensional
description
language
could
mak
e
use
of
a
strict
subset
of
the
prop
erties
used
for
the
distance
measure,
could
use
a
sup
erset
of
it,
or
could
ev
en
use
a
set
that
is
totally
disjoin
t
from
it.
Alternativ
ely
,
the
language
for
discriminan
t
descriptions
ma
y
dier
from
the
language
for
c
haracteristic
descriptions.
Example
.	
In
the
F
ruit&V
egetables
example,
one
could
add
a
prop
ert
y
Taste
to
the
description
of
eac
h
ob
ject
and
include
it
in
the
language
for
c
haracteristic
descriptions
as
w
ell
as
in
the
distance
measure,
but
not
in
the
language
for
discriminan
t
descriptions.
Ob
jects
are
then
clustered
according
to
their
taste,
but
assigning
an
unseen
instance
to
a
cluster
is
alw
a
ys
p
os-
sible
without
kno
wing
an
ything
ab
out
its
taste
(b
y
means
of
the
discriminan
t
descriptions).

In
general,
one
w
ould
t
ypically
use
easily
observ
able
prop
erties
for
a
dis-
criminan
t
description
of
the
clusters,
but
use
less
easily
observ
able
prop
erties
for
the
clustering
pro
cess
itself
(in
order
to
get
a
higher-qualit
y
clustering)
and
for
the
c
haracteristic
descriptions.
Conceptual
clustering
v
ersus
n
umerical
taxonom
y
The
term
c
onc
eptual
clustering
w
as
in
tro
duced
b
y
Mic
halski
and
Stepp
(	).
It
refers
to
a
general
t
yp
e
of
clustering
where
similarit
y
b
et
w
een
examples
is
computed
relativ
e
to
a
bac
kground
of
concepts
and
other
examples,
and
the
clusters
themselv
es
should
b
e
dened
in
terms
of
(p
ossibly
other)
concepts.
Mic
halski
and
Stepp
(	)
con
trast
this
setting
with
n
umerical
taxonom
y
(e.g.,
(Sneath
and
Sok
al,
	)),
where
the
fo
cus
is
on
forming
the
clusters
rather
than
describing
them.
This
con
trast
emphasizes
the
dierence
b
et
w
een
what
w
e
call
in
tensional
and
extensional
clustering
in
this
text.
Since
w
e
ha
v
e
already
discussed
that,
w
e
no
w
fo
cus
on
the
computation
of
distances
relativ
e
to
a
bac
kground
of
concepts
and
examples.
In
this
setting
the
similarit
y
criterion
is
non-trivial;
it
can
b
e
complex
and
domain-dep
enden
t.
It
can
ev
en
b
e
the
case
that
the
similarit
y
of
t
w
o
ob
jects
is
not
fully
determined
b
y
the
ob
jects
themselv
es,
but
also
b
y
the
other
ob
jects
in
the
set.
Figure
.
illustrates
this.
The
data
are
represen
ted
as
dots
in
a
t
w
o-dimensional
space.
Both
in
situation

and
,
there
is
a
subset
of
data
that
forms
a
cross.
The
most
ob
vious
w
a
y
of
clustering
the
data
dep
ends
on
the
con
text.
In
a
con
text
where
straigh
t
lines
are
natural
concepts
(situation
),
the
cross
w
ould
b
e
clustered
in
to
t
w
o
straigh
t
lines;
but
in
a
con
text
where
ho
oks
are
natural
concepts,
it
is
more
natural
to
cluster
the
cross
in
to
t
w
o
ho
oks
(situation
).
While
the
con
text
for
the
clustering
w
ould
usually
b
e
giv
en
in
adv
ance,
it
can
also
b
e
suggested
b
y
the
data
themselv
es,
as
is
the
case
in
these
dra
wings.

..
CLUSTERING

situation 1
situation 2
Figure
.:
Conceptual
clustering.
The
w
a
y
in
whic
h
the
cross
w
ould
t
ypically
b
e
divided
in
to
t
w
o
clusters
dep
ends
on
the
con
text.
The
con
text
itself
is,
in
these
dra
wings,
suggested
b
y
the
other
p
oin
ts
in
the
data
set.


CHAPTER
.
A
CLUSTERING
APPR
O
A
CH
TO
PREDICTION
The
abundance
of
ho
oks
in
situation

could
mak
e
the
clusterer
prefer
ho
oks
as
the
t
ypical
cluster
form,
while
in
situation

it
w
ould
prefer
straigh
t
lines.
Note
that
the
curren
t
view
on
clustering
mak
es
it
necessary
to
adopt
a
non-
trivial
denition
of
similarit
y
(at
least
if
w
e
stic
k
with
the
assumption
that
a
go
o
d
cluster
groups
examples
that
are
similar).
In
the
con
text
of
Figure
.
t
w
o
p
oin
ts
are
similar
if
one
can
sup
erimp
ose
a
certain
shap
e
o
v
er
them
(a
ho
ok,
or
a
straigh
t
line)
suc
h
that
the
shap
e
co
v
ers
man
y
other
p
oin
ts.
Th
us,
ho
w
similar
t
w
o
instances
are
do
es
not
dep
end
solely
on
these
instances,
but
also
on
the
p
ositions
of
the
other
instances.
W
e
can
th
us
distinguish
three
dieren
t
con
texts
for
conceptual
clustering,
in
ascending
order
of
dicult
y:
.
The
similarit
y
b
et
w
een
t
w
o
instances
is
determined
b
y
the
descriptions
of
these
t
w
o
instances
themselv
es,
and
b
y
a
con
text
consisting
of
a
xed
set
of
concepts
that
are
considered
imp
ortan
t.
.
The
similarit
y
b
et
w
een
t
w
o
instances
is
determined
b
y
all
the
instances
in
the
data
set
(not
only
the
t
w
o
b
et
w
een
whic
h
the
similarit
y
is
computed),
and
b
y
a
xed
con
text.
.
The
similarit
y
is
determined
b
y
all
the
instances
in
the
data
set,
and
b
y
a
con
text
that
is
itself
constructed
from
the
data.
Figure
.
is
an
illustration
of
the
third
setting.
A
classic
example
that
illustrates
the
same
issue
is
sho
wn
in
Figure
..
This
is
an
exp
erimen
t
where
h
uman
sub
jects
are
ask
ed
to
classify
the
middle
sym
b
ol
on
a
card.
Dep
ending
on
whether
p
eople
see
the
\ABC"
card
or
the
\

"
card,
they
classify
the
middle
sym
b
ol
as
a
\B"
or
a
\",
ev
en
though
it
is
exactly
the
same
sym
b
ol.
One
could
sa
y
that
the
middle
sym
b
ol
is
more
similar
to
the
letter
\B"
if
it
is
surrounded
b
y
letters,
and
is
more
similar
to
the
n
um
b
er

if
it
is
surrounded
b
y
n
um
b
ers.
A
protot
ypical
conceptual
clusterer
is
Mic
halski
and
Stepp's
Cluster/
algorithm.
The
dev
elopmen
t
of
in
tensional
clustering
is
strongly
related
to
conceptual
clustering,
hence
most
in
tensional
clusterers
are
also
conceptual
clusterers.
Most
clusterers
w
ork
in
con
text

or
,
ho
w
ev
er.
Flexibilit
y
with
resp
ect
to
cluster
formation
W
e
can
distinguish
the
dieren
t
approac
hes
to
clustering
along
a
dimension
that
represen
ts
the
exibilit
y
that
a
system
has
in
splitting
a
cluster
in
to
sub
clusters.
Extensional
clustering
systems
ha
v
e
no
restrictions
with
resp
ect
to
ho
w
a
cluster
can
b
e
split.
In
tensional
clustering
systems
ha
v
e
restrictions
according
to
the
language
of
cluster
descriptions
they
allo
w:
w
eak
restrictions
if
a
large
set
of
complex
descriptions
is
allo
w
ed,
strong
restrictions
if
only
a
limited
set
of

..
CLUSTERING

Test 1
Test 2
Figure
.:
Do
p
eople
think
the
sym
b
ol
in
the
middle
is
more
similar
to
the
letter
`B',
or
to
the
n
um
b
er
?
It
dep
ends
on
the
sym
b
ols
surrounding
it.
simple
descriptions
is
allo
w
ed.
In
mac
hine
learning,
this
is
usually
referred
to
as
the
language
bias
of
the
system.
The
stronger
the
restrictions
on
the
language,
the
harder
it
is
to
dene
high
qualit
y
clusters.
Figure
.
sho
ws
ho
w
dieren
t
kinds
of
clustering
systems
w
ould
pro
ceed
on
an
example
data
set.
The
data
set
con
tains
three
clearly
distinguishable
clusters,
indicated
b
y
,

and

on
the
dra
wing.
An
in
tensional
clustering
system
can
only
use
the
attributes
A,
B
,
and
C
to
describ
e
the
clusters.
Eac
h
attribute
can
b
e
0
or
.
Straigh
t
lines
indicate
the
b
oundaries
in
the
example
space
b
et
w
een
p
oin
ts
with
a
v
alue
of
0
or

for
a
certain
v
ariable.
An
extensional
clustering
system
can
dene
an
y
set
of
examples
to
b
e
a
cluster,
hence
it
should
nd
the
clusters
without
an
y
problem.
An
in
ten-
sional
clustering
system
can
only
use
the
straigh
t
lines
that
are
dra
wn
to
dene
clusters.
By
using
com
binations
of
v
ariables
(i.e.
descriptions
suc
h
as
A
=
0
^
B
=
0),
suc
h
a
system
can
still
iden
tify
the
clusters
correctly;
but
if
it
can
only
use
single
v
ariables
to
describ
e
sub
clusters
it
cannot
nd
the
correct
clusters.
In
Figure
.
the
third
clustering
metho
d
can
only
nd
t
w
o
clusters
in
the
data
set
(lo
oking
only
at
the
highest
lev
el
of
the
tree):
one
with
A
=
0
and
one
with
A
=
.
Figure
.,
while
illustrating
the
problem,
also
suggests
a
solution:
b
y
al-
lo
wing
an
in
tensional
clustering
system
that
can
only
use
simple
descriptions
to
build
a
hierarc
h
y
of
clusters
instead
of
a
at
clustering,
the
problem
is
par-
tially
solv
ed.
On
the
second
lev
el
of
the
tree,
the
clusters

and

are
iden
tied
correctly
.
The
cluster

ho
w
ev
er
has
not
b
een
found
as
a
single
cluster,
but
is
divided
in
to
t
w
o
sub
clusters.
W
e
can
actually
lo
ok
at
the
righ
tmost
cluster
tree
in
Figure
.
in
t
w
o
w
a
ys:

W
e
can
view
it
as
a
cluster
hierarc
h
y
.
In
this
case
a
cluster
is
asso
ciated
with
eac
h
no
de.
Some
no
des
dene
clusters
of
lo
w
qualit
y;
this
is
the
case
for
the
no
des
on
lev
el

(directly
b
elo
w
the
ro
ot)
and
for
the
no
des


CHAPTER
.
A
CLUSTERING
APPR
O
A
CH
TO
PREDICTION
1
2
3
extensional
+ +
+
+
++
+
+
+
+
+
+ +
+
+
+
++ +
+
+
+
+
+
A
0 1
B
C
1
0
1
2
3
1
0
+
conceptual, complex descriptions
conceptual, simple descriptions
hierarchical
flat
1
2
3
B=1,C=1
A=0
A=1
A=0
A=1
B=1
C=0
C=1
1
3a
3b
2
A=1,C=0
A=0,B=0
B=0
Figure
.:
Dieren
t
clustering
metho
ds.

..
CLUSTERING
	
lab
elled
.
One
could
sa
y
that
the
hierarc
h
y
iden
ties
\w
eak"
in
termedi-
ate
clusters
in
its
searc
h
for
go
o
d
clusters.
These
w
eak
clusters
ma
y
b
e
less
apparen
t
from
the
data
but
are
not
necessarily
meaningless.

W
e
can
also
c
ho
ose
to
ignore
the
w
eak
clusters
in
the
tree,
and
extract
a
at
clustering
from
the
tree
b
y
selecting
a
set
of
no
des
S
so
that
no
no
de
in
S
is
an
ancestor
of
an
other
no
de
in
S
,
and
all
no
des
together
co
v
er
all
the
examples.
In
Figure
.
w
e
w
ould
then
select
the
at
clustering
f,
,
a,
bg.
Note
that
this
clustering
is
still
not
optimal.
F
urther
p
ost-pro
cessing
could
b
e
p
erformed
to
impro
v
e
it,
e.g.,
b
y
rearranging
clusters
as
is
done
in
(Fisher,
		).
Sev
eral
metho
ds
for
extracting
a
at
clustering
from
a
hierarc
h
y
ha
v
e
b
een
prop
osed;
see
e.g.
(Kirsten
and
W
rob
el,
		).
In
tensional
clustering
systems
that
use
complex
descriptions
of
sub
clusters
can
yield
clusters
of
higher
qualit
y
,
but
this
comes
at
the
cost
of
a
higher
com-
putational
complexit
y
.
Indeed,
the
searc
h
space
in
v
olv
ed
in
nding
complex
descriptions
is
larger
than
the
searc
h
space
in
v
olv
ed
in
nding
single
descrip-
tions.
Another
disadv
an
tage
of
using
complex
descriptions
is
exactly
the
fact
that
they
are
complex.
The
idea
b
ehind
in
tensional
clustering
is
that
clusters
are
go
o
d
if
they
ha
v
e
simple
in
tensional
descriptions.
There
ma
y
b
e
other
criteria
for
the
qualit
y
of
clusters
than
just
the
simplicit
y
of
their
description,
but
when
the
latter
has
a
high
w
eigh
t
there
ma
y
not
b
e
a
go
o
d
reason
for
allo
wing
complex
tests.
Summarizing,
w
e
can
sa
y
that
using
only
simple
descriptions
is
computa-
tionally
more
ecien
t
than
using
complex
descriptions,
but
ma
y
yield
lo
w
er
qualit
y
clusterings.
This
dierence
is
more
pronounced
for
at
clustering
than
for
hierarc
hical
clustering.
Figure
.
places
sev
eral
existing
clustering
systems
on
an
axis
indicat-
ing
ho
w
exible
they
are
with
resp
ect
to
dening
clusters:
Cobweb
(Fisher,
	),
Cluster/
(Mic
halski
and
Stepp,
	),
Leader
(Hartigan,
	),
R
umma
ge
(Fisher
and
Langley,
	)
and
Discon
(Langley
and
Sage,
	).
Note
that
Cobweb,
although
returning
in
tensional
(probabilistic)
descriptions
for
clusters,
has
the
exibilit
y
of
an
extensional
system
in
forming
the
clusters:
the
clustering
can
b
e
an
y
partition
of
the
example
set.
Predictiv
e
v
ersus
descriptiv
e
clustering
One
can
see
a
clustering
as
purely
descriptiv
e,
i.e.
describing
structure
in
the
data;
or
one
can
see
it
as
a
means
for
making
predictions.
Whether
a
clustering
is
considered
go
o
d
or
not
dep
ends
on
this.
W
e
illustrate
this
with
Figure
.	.
Tw
o
at
clusterings
are
sho
wn;
the
in
tensional
cluster
descriptions
are
equally

0
CHAPTER
.
A
CLUSTERING
APPR
O
A
CH
TO
PREDICTION
exible
less
exible
extensional
in
tensional
complex
descriptions
simple
descriptions
Leader
Cluster/
R
umma
ge
Cobweb
Discon
Figure
.:
Clustering
systems
plotted
along
the
exibility
dimension.
+
+
+
+
+
+
+ +
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
(a)
(b)
(c)
X
X
Y
X
Y
Y
A=1
A=2
A=3
A=4
A=5
B=1
B=2
Figure
.	:
The
dierence
b
et
w
een
predictiv
e
and
descriptiv
e
clustering.
(a)
original
set
of
p
oin
ts;
(b)
a
bad
descriptiv
e
but
go
o
d
predictiv
e
clustering;
(c)
a
go
o
d
descriptiv
e
but
bad
predictiv
e
clustering.
simple
in
b
oth
cases.
Assuming
w
e
w
an
t
to
use
the
clusters
to
predict
v
alues
for
X
and
Y
,
clustering
(b)
is
reasonably
go
o
d:
the
examples
in
eac
h
cluster
dier
little
with
resp
ect
to
the
v
ariables
X
and
Y
.
This
means
that
X
and
Y
can
b
e
predicted
accurately
if
the
cluster
is
kno
wn.
W
e
sa
y
that
the
pr
e
dictability
of
the
v
ariables
is
high.
In
clustering
(c)
the
predictabilit
y
of
the
v
ariables
is
lo
w
er,
b
ecause
the
clusters
are
spread
out
more.
On
the
other
hand,
these
clusters
iden
tify
the
existing
structure
in
the
data
in
a
m
uc
h
b
etter
w
a
y:
if
one
lo
oks
at
plot
(a),
the
clusters
in
(c)
are
apparen
t,
while
those
in
(b)
are
not.
F
rom
a
descriptiv
e
p
oin
t
of
view,
clustering
(c)
is
to
b
e
preferred.
Note
that
in
this
case,
a
hierarc
hical
clustering
system
could
iden
tify
b
oth
the
large
and
the
smaller
clusters.
In
this
sense,
hierarc
hical
clusterers
are
go
o
d
at
com
bining
b
oth
predictiv
e
and
descriptiv
e
qualit
y
.
Still,
dieren
t
clustering
systems
ma
y
b
e
biased
to
w
ards
dieren
t
qualit
y
criteria,
and
one
should
c
ho
ose
a
clusterer
accordingly
.

..
CLUSTERING

..
Ev
aluation
of
Clusterings
The
qualit
y
of
a
clustering
can
b
e
measured
in
dieren
t
w
a
ys.
The
main
cri-
terion
for
c
ho
osing
a
certain
qualit
y
measuremen
t
should
b
e
the
goal
of
the
clustering:
is
it
seen
as
purely
descriptiv
e
(iden
tifying
clusters
in
a
set
of
data)
or
is
it
to
b
e
used
for
prediction?
Descriptiv
e
Qualit
y
Measuring
the
descriptiv
e
qualit
y
of
a
clustering
is
hard,
and
there
are
no
agreed-up
on
criteria.
One
of
the
more
p
opular
criteria
is
the
p
artition
utility,
dened
in
(Fisher,
		)
as
P
U
(fC

;
:
:
:
;
C
N
g)
=
X
k
C
U
(C
k
)=
N
(.)
i.e.,
the
a
v
erage
c
ate
gory
utility
of
eac
h
cluster;
the
latter
w
as
in
tro
duced
b
y
Gluc
k
and
Corter
(	)
and
can
b
e
dened
as
C
U
(C
k
)
=
P
(C
k
)
X
i
X
j
(P
(A
i
=
V
ij
jC
k
)

 P
(A
i
=
V
ij
)

)
(.)
It
is
assumed
here
that
eac
h
instance
is
describ
ed
b
y
a
v
ector
of
attributes
A
i
,
and
the
domain
of
eac
h
attribute
A
i
is
a
set
of
v
alues
V
ij
.
Category
utilit
y
measures
b
oth
the
pr
e
dictability
of
attribute
v
alues
(ho
w
precise
can
the
attribute
of
an
instance
b
e
predicted
if
the
instance's
cluster
is
kno
wn),
and
their
pr
e
dictiveness
(ho
w
w
ell
can
the
cluster
b
e
predicted
if
the
attribute
v
alue
is
kno
wn).
Note
that
this
measuremen
t
assumes
nite
domains
for
all
the
attributes;
moreo
v
er
Fisher
(		)
discusses
some
problems
with
it.
Predictiv
e
Qualit
y
If
a
clustering
is
going
to
b
e
used
for
prediction,
the
predictabilit
y
of
the
at-
tributes
that
are
going
to
b
e
predicted
is
the
most
imp
ortan
t
criterion.
In
some
cases,
suc
h
as
(sup
ervised
or
unsup
ervised)
classication,
one
single
nominal
attribute
is
to
b
e
predicted.
The
accuracy
with
whic
h
the
class
can
b
e
predicted
is
usually
the
ev
aluation
criterion
then,
see
e.g.
(Fisher,
	).
Note
that
this
is
a
sp
ecial
case
of
the
v
ariance
criterion
w
e
in
tro
duced
in
Denition
..
In
the
regression
con
text
v
ariance
itself
is
a
go
o
d
relativ
e
qualit
y
criterion,
but
it
is
less
t
as
an
absolute
criterion;
e.g.,
if
the
in
tra-cluster
v
ariance
is
0,
should
w
e
consider
this
high
or
lo
w?
A
more
p
opular
criterion
is
therefore
the
r
elative
err
or
R
E
=
P
n
i=
(y
i
 ^
y
i
)

P
n
i=
(y
i
 
y
)

(.)


CHAPTER
.
A
CLUSTERING
APPR
O
A
CH
TO
PREDICTION
This
criterion
compares
(on
a
test
sample)
the
mean
squared
error
of
a
predictor
with
the
mean
squared
error
of
a
default
predictor
that
consisten
tly
predicts
the
global
mean
of
the
training
sample
(the
y
i
are
the
v
alues
observ
ed
in
the
test
sample,
the
^
y
i
are
the
corresp
onding
predictions,
and

y
is
the
global
mean
of
the
training
sample).
The
previous
criteria
assumed
that
one
single
v
ariable
is
to
b
e
predicted.
Another
setting
is
when
an
y
v
alue
ma
y
need
to
b
e
predicted
in
a
new
instance;
this
is
sometimes
called
exible
pr
e
diction
or
p
attern
c
ompletion.
The
qualit
y
of
a
clustering
should
then
b
e
computed
as
an
a
v
erage
o
v
er
the
predictabilit
y
of
eac
h
single
attribute.
Suc
h
an
ev
aluation
can
b
e
found
in
e.g.
(Fisher,
		).
Unfortunately
,
when
b
oth
con
tin
uous
and
discrete
attributes
are
used,
it
is
not
ob
vious
ho
w
suc
h
an
a
v
erage
should
b
e
computed,
as
the
predictabilit
y
of
discrete
and
con
tin
uous
attributes
is
expressed
in
dieren
t
w
a
ys.
Assuming
that
the
distance
b
et
w
een
examples
reects
the
similarit
y
of
their
attributes,
the
relativ
e
error
criterion
can
b
e
generalized
to
a
qualit
y
criterion
for
exible
prediction:
R
E
=
P
n
i=
d(e
i
;
^
e
i
)

P
n
i=
d(e
i
;
p(T
r
))

(.)
where
the
e
i
are
the
examples
in
a
test
set,
^
e
i
the
corresp
onding
predictions
and
p(T
r
)
is
the
protot
yp
e
of
the
training
set.
In
the
clustering
con
text,
it
compares
ho
w
far
o
the
predictions
are
from
the
actual
examples,
compared
to
ho
w
far
o
the
protot
yp
e
of
the
training
set
is.
Note
that
if
the
predictor
w
as
constructed
b
y
a
predictiv
e
clusterer
b
y
nd-
ing
a
clustering
C
of
the
training
set
T
r
,
then
R
E
=
P
n
i=
d(e
i
;
p(f
(e
i
;
C
)))

P
n
i=
d(e
i
;
p(T
r
))

(.)
whic
h
sho
ws
that
b
y
trying
to
maximize
Q,
a
predictiv
e
clusterer
tries
to
min-
imize
the
exp
ected
v
alue
of
R
E
.
.
Using
Clusterings
for
Prediction
..
Extensional
Clusterings
Giv
en
an
extensional
clustering
and
a
new
instance
(one
that
is
not
in
the
clustering),
ho
w
can
one
predict
unkno
wn
information
for
the
instance?
This
is
usually
done
using
a
t
w
o-step
pro
cess.
In
a
rst
step,
the
instance
is
assigned
to
a
cluster.
This
assignmen
t
is
t
ypically
based
on
the
distance
of
the
instance
to
the
clusters
in
the
extensional
clustering.
The
distance
of
the
instance
to
a
cluster
could
b
e
c
hosen
as
the
distance
to
the
protot
yp
e
of
the

..
USING
CLUSTERINGS
F
OR
PREDICTION

cluster,
the
a
v
erage
distance
to
all
the
elemen
ts
of
the
cluster,
the
distance
to
the
closest
elemen
t
of
the
cluster,
etc.
This
c
hoice
determines
the
cluster
assignmen
t
function
f
.
Once
the
instance
has
b
een
assigned
to
a
cluster
C
,
the
second
step
can
b
e
p
erformed,
whic
h
consists
of
making
a
prediction
ab
out
the
new
instance,
based
on
the
information
in
C
.
The
most
ob
vious
w
a
y
to
do
this
is
to
predict
a
v
alue
that
is
t
ypical
for
the
examples
in
C
;
i.e.,
predict
the
v
alue
that
is
observ
ed
in
the
cluster
protot
yp
e:

(p(C
)).
Both
steps
are
represen
ted
in
the
follo
wing
sc
heme:
example
cluster
prediction
distance
prototype
Instance
Based
Learning
The
ab
o
v
e
prediction
metho
d
is
related
to
instance-based
learning
(Aha
et
al.,
		;
Mitc
hell,
		;
Langley,
		).
An
instance-based
learner
t
ypically
stores
all
the
examples
it
has
seen
(or
at
least
some
of
them),
and
when
it
sees
a
new
instance,
bases
its
prediction
for
it
on
the
previously
observ
ed
examples
that
are
most
similar
to
it.
The
k
-nearest
neigh
b
or
metho
d,
for
instance,
consists
of
gathering
the
k
stored
examples
that
are
most
similar
to
the
new
instance
(k
is
a
parameter
of
the
metho
d),
and
predicting
the
class
v
alue
that
o
ccurs
most
often
among
these
k
examples
(or
the
mean
of
the
observ
ed
v
alues,
for
a
con
tin
uous
target
v
ariable).
An
in
teresting
dierence
with
the
k
-nearest
neigh
b
or
metho
d
is
that
the
latter
do
es
not
form
an
y
clusters
in
adv
ance;
rather,
at
the
time
of
prediction
a
cluster
of
k
examples
is
impro
vised.
The
second
step
of
the
prediction
is
then
similar
to
the
one
w
e
prop
ose.
Note
that
our
extensional
clustering
based
prediction
metho
d
coincides
with
-nearest
neigh
b
or
if
all
the
clusters
consist
of
a
single
example
that
is
also
the
protot
yp
e
of
the
cluster.
..
In
tensional
Clusterings
In
tensional
clusterings
can
b
e
used
for
predicting
unseen
prop
erties
of
examples
in
a
similar
w
a
y
as
extensional
clusterings.
In
Section
.
w
e
distinguished
discriminan
t
and
c
haracteristic
descriptions
of
clusters.
Assuming
a
clustering
metho
d
yields
b
oth
discriminan
t
and
c
haracteristic
descriptions
of
the
clusters
it
forms,
the
results
of
the
clustering
metho
d
can
b
e
used
for
making
predictions
in
the
follo
wing
w
a
y:

assign
a
new
ob
ject
to
a
cluster
using
the
discriminan
t
descriptions

predict
unkno
wn
prop
erties
of
the
ob
ject
using
the
c
haracteristic
descrip-
tion
of
the
cluster


CHAPTER
.
A
CLUSTERING
APPR
O
A
CH
TO
PREDICTION
discrimination
characterization
distance
prototype
extensional level
intensional level
cluster
example
prediction
Figure
.0:
Clustering
based
prediction.
This
is
a
t
w
o-step
prediction
pro
cess
that
is
v
ery
similar
to
the
one
used
with
extensional
clustering:
example
discrimination
cluster
prediction
characterization
Note
that
prediction
is
to
b
e
understo
o
d
in
a
v
ery
general
sense
here;
one
can
not
only
predict
sp
ecic
v
alues
but
also
more
general
prop
erties
suc
h
as
Weight>0,
if
Color=green
then
Taste=sour,
etc.
..
Mixing
Extensional
and
In
tensional
Reasoning
W
e
ha
v
e
seen
t
w
o-step
prediction
pro
cesses
b
oth
at
the
extensional
lev
el
and
at
the
in
tensional
lev
el.
Since
the
clusters
themselv
es
are
the
same
on
b
oth
lev
els,
the
t
w
o
t
yp
es
of
reasoning
can
b
e
mixed,
and
w
e
obtain
the
situation
depicted
in
Figure
.0.
This
sc
heme
indicates
dieren
t
reasoning
mec
hanisms
for
making
predic-
tions:
from
an
example
description
the
cluster
is
predicted,
either
on
the
in
ten-
sional
or
extensional
lev
el;
then
from
the
cluster
one
can
infer
a
c
haracteristic
description
of
the
example
or
predict
a
missing
v
alue.
Example
.0
W
e
illustrate
the
dieren
t
t
yp
es
of
reasoning
on
the
F
ruit
&
V
egetables
example.
Supp
ose
w
e
ha
v
e
the
follo
wing
in
tensional
cluster
descrip-
tions:
discriminan
t
c
haracteristic
apples
Weight>0g
Weight

[0g
 0g
]
^Color
=
green
^Color
=
green
^
Taste
=
sour
tomato
es
Weight>0g
Weight

[0g
 0g
]
^Color
=
red
^Color
=
red
^
Taste
=
sour
stra
wb
erries
Weight<0g
Weight

[0g
 0g
]
^Color
=
red
^
Taste
=
sweet

..
USING
CLUSTERINGS
F
OR
PREDICTION

Extensional
cluster
descriptions
are
sets
of
examples.
W
e
here
consider
the
protot
yp
e
to
b
e
part
of
the
extensional
description.
W
e
assume
the
protot
yp
e
for
apples
is
(green,
0g,
sour).
Some
examples
of
the
dieren
t
reasoning
mec
hanisms
are
(reasoning
on
the
extensional/in
tensional
lev
el
is
indicated
b
y
)
E
and
)
I
):
.
Color=green
)
I
instance
is
an
apple
)
I
Taste
=
sour
^
Weight

[0g
;
0g
]
^
Color
=
green
.
\Is
an
apple"
is
an
abbreviation
for
\b
e-
longs
to
the
cluster
of
apples"
here.
This
t
yp
e
of
reasoning
allo
ws
us
to
complete
a
partial
description
of
an
ob
ject.
.
Color=green
)
I
the
instance
is
an
apple
)
E
it
m
ust
b
e
similar
to
the
protot
ypical
apple
(green,
0g,
sour).
This
t
yp
e
of
reasoning
can
b
e
used
for
predicting,
e.g.,
the
taste
of
an
ob
ject
b
y
observing
other
prop
erties
of
it
(its
color
in
this
case).
.
An
instance
is
most
similar
to
some
ob
ject
in
the
\apple"
cluster
)
E
it
is
an
apple
)
E
it
is
similar
to
the
protot
ypical
apple
(green,
0g,
sour).
With
this
t
yp
e
of
reasoning,
w
e
do
not
lo
ok
merely
at
prop
erties
of
the
ob
ject
itself
but
compare
it
with
other
ob
jects,
in
order
to
predict
its
taste.
.
An
instance
is
most
similar
to
some
ob
ject
in
the
\apple"
cluster
)
E
it
is
an
apple
)
I
it
conforms
to
the
c
haracteristic
description
of
apples:
Weight

[0g
 0g
]
^
Color
=
green
^
Taste
=
sour
.
W
e
infer
a
more
detailed
description
of
the
ob
ject
b
y
comparing
it
with
other
ob
jects.
E.g.,
the
ob
ject
lo
oks
lik
e
an
apple,
apples
w
eigh
b
et
w
een
0g
and
0g,
hence
this
ob
ject
also
has
a
w
eigh
t
b
et
w
een
those
b
ound-
aries.

W
e
remark
that:

a
c
haracteristic
description
of
a
cluster
ma
y
con
tain
elemen
ts
that
are
not
presen
t
in
an
explicit
represen
tation
of
an
y
ob
ject.
E.g.,
a
protot
yp
e,
b
eing
a
represen
tation
of
a
single
example,
has
one
single
w
eigh
t.
F
rom
a
protot
yp
e
one
could
not
predict
that
an
apple
m
ust
ha
v
e
a
w
eigh
t
b
et
w
een
0g
and
0g.
Hence,
c
haracteristic
descriptions
allo
w
predictions
of
a
more
general
kind
than
protot
yp
es.


CHAPTER
.
A
CLUSTERING
APPR
O
A
CH
TO
PREDICTION

while
in
the
ab
o
v
e
example
the
information
used
in
in
tensional
descrip-
tions
is
the
same
as
that
used
in
extensional
descriptions,
this
need
not
b
e
the
case.
Some
parts
of
an
example
description
ma
y
b
e
t
for
comput-
ing
distances,
others
(e.g.,
prop
erties
describ
ed
in
a
rst
order
language,
where
distances
are
harder
to
dene)
ma
y
b
e
more
suitable
for
in
tensional
descriptions.

predictiv
e
induction
systems
(among
whic
h
systems
that
induce
rule
sets
or
decision
trees
for
classication
or
regression)
usually
induce
h
yp
otheses
that
in
v
olv
e
reasoning
of
t
yp
e
().
F
or
instance,
the
normal
w
a
y
of
using
a
regression
tree
is
to
sort
an
instance
do
wn
the
tree
in
to
a
leaf
(i.e.
assigning
it
to
a
cluster
via
in
tensional
reasoning)
and
predict
the
v
alue
stored
in
that
leaf.
A
rule
based
system
w
ould
rst
nd
a
rule
that
applies
to
the
instance
(i.e.
it
assigns
it
to
a
cluster;
the
b
o
dy
of
the
rule
is
an
in
tensional
description
of
the
cluster)
and
then
mak
e
a
prediction
based
on
a
(extensional)
v
alue
stored
in
the
head
of
the
rule.

The
last
remark
illustrates
that
rule
based
systems
and
decision
tree
induc-
tion
systems
can
b
e
seen
as
predictiv
e
clusterers.
.
Creating
Clusterings
for
Prediction
..
Clustering
in
Dieren
t
Spaces
In
the
previous
section
w
e
discussed
ho
w
clusters
can
b
e
used;
w
e
no
w
fo
cus
on
ho
w
they
are
created.
W
e
distinguish
sev
eral
v
arian
ts
of
the
clustering
pro
cess,
according
to
the
directions
along
whic
h
the
clusters
should
b
e
coheren
t.
W
e
distinguish
those
situations
where
the
v
ariables
to
b
e
predicted
are
kno
wn
b
eforehand
(as
is
the
case
for,
e.g.,
classication
or
regression)
and
those
situations
where
they
are
not
(exible
prediction).
In
the
latter
case,
one
simply
creates
clusters
that
are
coheren
t
in
the
instance
space
I
(i.e.,
ha
v
e
lo
w
v
ariance
in
I
).
In
the
former
case
ho
w
ev
er,
w
e
can
divide
the
instance
space
I
in
to
a
subspace
D
(the
part
of
the
instance
that
will
b
e
giv
en
to
the
predictor)
and
a
subspace
P
(the
part
that
will
ha
v
e
to
b
e
predicted)

:
I
=
D

P
:
(.	)
Instead
of
clustering
in
I
,
one
can
then
also
cluster
in
D
or
P
.
By
\clustering
in
a
subspace
of
I
"
w
e
mean
that
the
distance
criterion
for
that
subspace
is

W
e
should
men
tion
that
in
the
case
of
rule
set
induction,
the
clusters
ma
y
o
v
erlap,
whic
h
mak
es
a
more
general
denition
of
clustering
than
ours
necessary
.
The
approac
h
essen
tially
sta
ys
the
same
though.

It
is
assumed
here
that
except
for
the
target
v
ariable,
all
the
information
a
v
ailable
to
the
learner
will
also
b
e
a
v
ailable
to
the
predictor.

..
CREA
TING
CLUSTERINGS
F
OR
PREDICTION

+
+ +
+
++
++
+
D
P
+
+ +
+
++
++
+
D
P
+
+ +
+
++
++
+
D
P
(b) clustering in P
(a) clustering in I
(c) clustering in D
+
+
+
++
++
+++
Figure
.:
Clustering
in
the
instance
space
and
in
its
subspaces.
used
when
forming
the
clusters.
Figure
.
giv
es
an
illustration.
The
D
and
P
subspaces
are
represen
ted
as
(one-dimensional)
axes
on
this
gure,
although
they
ma
y
of
course
b
e
of
higher
dimensionalit
y
.
The
pro
jections
of
instances
and
clusters
on
to
relev
an
t
axes
is
also
sho
wn.
W
e
can
then
distinguish
the
follo
wing
settings:

form
clusters
in
P
:
this
is
the
classical
sup
ervised
learning
setting.
The
idea
b
ehind
forming
clusters
in
P
is
that,
since
predictions
will
b
e
made
for
P
only
,
it
is
sucien
t
that
the
instances
within
one
cluster
are
close
to
one
another
in
the
P
subspace;
coherence
in
D
is
not
imp
ortan
t.
Classication
and
regression,
as
w
e
ha
v
e
dened
them,
cluster
in
P
(the
qualit
y
criterion
is
based
on
the
distance
in
the
prediction
space
P
).

form
clusters
in
D
:
this
is
the
classical
unsup
ervised
learning
setting.
When
the
clusterer
do
es
not
ha
v
e
access
to
the
v
alues
in
P
,
the
only
option
that
is
left
is
to
cluster
in
D
.
Suc
h
unsup
ervised
learning
presupp
oses
that
examples
close
together
in
D
will
also
b
e
close
together
in
P
.
If
this
assumption
is
false,
go
o
d
results
cannot
b
e
obtained.
Suc
h
a
case
is
sho
wn
in
Figure
.(c):
the
t
w
o
righ
tmost
clusters
in
I
simply
cannot
b
e
distinguished
in
D
.

form
clusters
in
I
.
T
o
our
kno
wledge
this
setting
is
mainly
used
for
descriptiv
e
clustering
or
exible
prediction,
i.e.
when
P
is
not
kno
wn
b
eforehand.
If
w
e
compare
the
clusterings
in
Figure
.,
clustering
(a)
is
the
only
\go
o
d"
clustering
in
I
;
the
other
metho
ds
iden
tify
o
v
erly
general
clusters.
Clus-
tering
(b)
is
a
go
o
d
clustering
in
P
,
while
(a)
iden
ties
o
v
erly
sp
ecic
clusters


CHAPTER
.
A
CLUSTERING
APPR
O
A
CH
TO
PREDICTION
(t
w
o
clusters
are
formed
that
are
really
indistinguishable
in
P
);
similarly
,
(a)
iden
ties
o
v
erly
sp
ecic
clusters
in
D
while
(c)
iden
ties
the
correct
clusters.
Note
that,
from
a
descriptiv
e
p
oin
t
of
view,
it
is
as
undesirable
to
nd
o
v
erly
sp
ecic
clusters
as
to
nd
o
v
erly
general
clusters.
Ho
w
ev
er,
from
a
predictiv
e
p
oin
t
of
view
nding
o
v
erly
sp
ecic
clusters
is
not
necessarily
w
orse
than
nding
the
righ
t
clusters,
while
nding
o
v
erly
general
clusters
is.
Hence,
clustering
in
I
migh
t
b
e
an
in
teresting
alternativ
e
to
clustering
in
P
.
..
Do
Learners
Ignore
Useful
Information?
In
the
previous
section,
w
e
ha
v
e
sho
wn
ho
w
sup
ervised
learners
actually
cluster
in
P
instead
of
in
I
,
reasoning
that
the
resulting
clusters
should
b
e
coheren
t
in
P
.
In
this
section
w
e
lo
ok
at
this
approac
h
from
an
information-based
p
oin
t
of
view,
i.e.
what
kinds
of
information
are
a
v
ailable
to
the
learner
and
ho
w
w
ell
do
es
it
exploit
the
a
v
ailable
information?
Giv
en
an
N
-dimensional
space
S
,
w
e
dene
the
information
~
S
corresp
onding
to
S
,
as
the
set
of
dimensions
of
S
.
F
or
instance,
if
I
=
Weight

Color
,
then
~
I
=
fWeight
;
Color
g.
Ha
ving
the
information
~
S
a
v
ailable
for
an
instance
means
that
the
co
ordinates
of
the
instance
in
S
are
kno
wn.
In
the
previous
section
w
e
assumed
that
the
only
information
that
the
learner
has
in
addition
to
what
the
predictor
has,
is
the
target
v
ariable.
W
e
no
w
sligh
tly
extend
the
learning
setting,
in
that
the
learner
migh
t
ha
v
e
extra
information
~
L
a
v
ailable
that
the
predictor
cannot
use
and
that
need
not
b
e
predicted
either.
Th
us,
instead
of
Equation
.	
w
e
ha
v
e:
I
=
D

P

L
(.0)
In
terms
of
information,
w
e
get:
~
I
=
~
D
[
~
P
[
~
L
(.)
W
e
dene
one
last
piece
of
information:
~
H
is
the
information
that
the
learner
uses
in
its
heuristics.
W
e
can
then
express
t
w
o
constrain
ts
that
ev
ery
learner
m
ust
satisfy:
~
H

~
I
(.)
~
D
\
~
P
=
;
(.)
These
constrain
ts
sa
y
that
a
learner
cannot
use
heuristics
based
on
una
v
ailable
information,
and
that
w
e
consider
only
non-trivial
predictions.
F
or
an
unsup
ervised
system
it
furthermore
holds
that
~
H
\
~
P
=
;
(.)

..
CREA
TING
CLUSTERINGS
F
OR
PREDICTION
	
These
are
the
only
restrictions
that
apply
for
inductiv
e
learners.
Ho
w
ev
er,
practical
systems
often
imp
ose
extra
constrain
ts.
F
or
most
classical
sup
ervised
systems
it
holds
that
~
L
=
;
(.)
~
H
=
~
P
(.)
where
~
P
=
fY
g
with
Y
a
discrete
(classication)
or
con
tin
uous
(regression)
v
ariable.
I.e.,
the
predictor
is
allo
w
ed
to
use
all
the
kno
wledge
a
v
ailable
to
the
learner
(except
the
v
alue
that
is
to
b
e
predicted),
and
the
heuristics
for
building
the
predictor
are
based
solely
on
the
v
ariable
to
b
e
predicted.
F
or
some
applications
it
migh
t
b
e
feasible
to
pro
vide
the
learner
with
w
ell-
studied
examples,
ab
out
whic
h
more
is
kno
wn
than
what
will
b
e
kno
wn
of
the
instances
for
whic
h
predictions
will
ha
v
e
to
b
e
made,
i.e.
~
L
=
;.
Example
.
This
example
is
based
on
the
Mutagenesis
dataset
(see
Ap-
p
endix
A
for
a
description).
Supp
ose
one
w
an
ts
to
nd
a
theory
that
predicts
the
m
utagenicit
y
of
molecules
from
their
molecular
structure.
F
or
the
molecules
that
are
pro
vided
to
the
learner,
one
could
add
information
that
is
kno
wn
to
b
e
relev
an
t,
e.g.,
the
n
umerical
features
lumo
and
logp.
By
taking
this
extra
information
in
to
accoun
t,
it
ma
y
b
e
p
ossible
to
obtain
a
b
etter
clustering.
Still,
as
long
as
these
extra
features
are
not
used
in
the
in
tensional
descriptions
of
the
clusters,
they
need
not
b
e
a
v
ailable
for
making
predictions.

Th
us,
our
framew
ork
oers
the
p
ossibilit
y
to
use
certain
information
(b
e-
sides
the
target
v
ariable)
for
building
a
predictor,
ev
en
if
that
information
will
not
b
e
a
v
ailable
to
the
predictor.
Most
classical
approac
hes
to
induction
do
not
exploit
suc
h
kno
wledge
(see,
e.g.,
the
description
of
tree
learners
in
Chapter
).
Let
us
no
w
tak
e
a
lo
ok
at
Equation
.:
~
H
=
~
P
.
T
o
our
kno
wledge
this
equation
has
b
een
violated
b
y
v
ery
few,
if
an
y
,
predictiv
e
induction
al-
gorithms.
The
equation
sa
ys
that
the
searc
h
for
a
go
o
d
predictiv
e
theory
is
guided
b
y
the
v
ariable
to
b
e
predicted,
and
b
y
that
v
ariable
alone.
F
or
instance,
tree
induction
algorithms
use
heuristics
suc
h
as
information
gain
or
gain
ratio
(Quinlan,
		a),
Gini
index
(Breiman
et
al.,
	)
or
mean
squared
prediction
error
(Kramer,
		;
Breiman
et
al.,
	);
rule
set
induction
algorithms
coun
t
the
n
um
b
er
of
correct
and
incorrect
predictions
to
guide
their
searc
h
(Clark
and
Niblett,
		;
De
Raedt
and
V
an
Laer,
		;
Mic
halski
et
al.,
	)
or
some
other
measure
of
predictiv
e
qualit
y
in
the
case
of
regression
(Karali

c
and
Bratk
o,
		).
The
reasoning
b
ehind
this
could
b
e
that
the
searc
h
for
a
high
qualit
y
pre-
dictor
can
b
est
b
e
guided
b
y
computing
the
predictiv
e
qualit
y
of
in
termediate
predictors;
in
other
w
ords,
the
qualit
y
of
an
in
termediate
predictor
is
the
b
est
indicator
of
the
qualit
y
of
the
predictor
it
will
ultimately
lead
to.
Ho
w
ev
er,
w
e
see
t
w
o
coun
terargumen
ts
against
this
reasoning:

0
CHAPTER
.
A
CLUSTERING
APPR
O
A
CH
TO
PREDICTION

It
is
not
alw
a
ys
true
that
the
b
est
predictor,
according
to
a
qualit
y
cri-
terion
Q,
can
b
est
b
e
found
b
y
using
Q
as
heuristic.
F
or
instance,
systems
that
build
classication
trees
and
try
to
maximize
the
accuracy
of
the
-
nal
tree
use
information
gain
(Quinlan,
		a)
or
Gini
index
(Breiman
et
al.,
	)
as
heuristics.
These
are
not
equiv
alen
t
to
using
the
accuracy
of
the
in
termediate
tree,
in
the
sense
that
the
heuristic
ma
y
prefer
one
tree
o
v
er
another
without
its
accuracy
b
eing
higher.
Still,
these
heuristics
on
a
v
erage
yield
a
nal
tree
with
higher
accuracy
than
when
accuracy
itself
is
used
as
a
heuristic.
Breiman
et
al.
(	)
discuss
this
phenomenon.

While
the
heuristics
just
men
tioned
are
dieren
t
from
accuracy
,
they
are
computed
from
the
same
information
~
P
,
so
Equation
.
still
holds.
One
situation
where
this
ma
y
not
b
e
an
optimal
approac
h,
is
when
noise
is
presen
t
in
the
data.
If
the
v
alues
for
~
P
can
b
e
inaccurate,
an
algorithm
using
these
v
alues
to
guide
its
searc
h
migh
t
easily
b
e
misguided.
Making
use
of
other
v
alues
in
the
searc
h
heuristic
can
mak
e
it
more
robust.
In
statistics
it
is
kno
wn
that
when
one
has
sev
eral
indep
enden
t
estimators
^

i
for
a
certain
parameter

,
eac
h
estimator
b
eing
un
biased
and
ha
ving
a
standard
error

i
,
the
estimator
^

=
P
n
i=



i
^

i
P
n
i=



i
(.)
(a
w
eigh
ted
a
v
erage
of
the
original
estimators)
is
optimal
in
the
sense
that
of
all
un
biased
estimators
that
are
a
linear
com
bination
of
the
^

i
,
^

has
the
smallest
standard
error.
While
it
is
dicult
to
apply
this
result
in
a
more
general
con
text
where
v
ariables
are
not
necessarily
n
umerical
and
structural
information
can
b
e
tak
en
in
to
accoun
t,
the
general
message
remains.
In
the
presence
of
noise,
the
observ
ed
class
of
an
instance
ma
y
not
b
e
the
b
est
estimator
of
its
real
class;
instead,
a
b
etter
estimator
can
probably
b
e
formed
b
y
including
other
information.
In
the
con
text
of
unsup
ervised
learning,
it
is
common
to
let
the
heuristic
use
all
the
a
v
ailable
information
except
~
P
.
The
fact
that
suc
h
an
approac
h
w
orks,
suggests
that
ev
en
for
sup
ervised
learning,
information
outside
~
P
ma
y
b
e
relev
an
t.
So,
it
turns
out
that
learning
with
~
H
=
~
P
is
only
a
sp
ecial
case
of
a
more
general
sup
ervised
learning
setting
where
~
P

~
H

~
I
.
If
w
e
view
the
use
of
only
~
P
in
the
heuristics
as
one
extreme,
and
using
ev
erything
except
~
P
as
the
other
extreme,
it
b
ecomes
clear
that
b
et
w
een
these
t
w
o
extremes
a
broad
range
of
other
p
ossibilities
remains
unexplored.
Some
of
these
are
discussed
in
the
next
section.

..
CREA
TING
CLUSTERINGS
F
OR
PREDICTION

..
Applications
of
Predictiv
e
Clustering
In
addition
to
the
classical
approac
hes
to
classication,
regression,
unsup
er-
vised
learning
and
exible
prediction,
whic
h
are
all
sp
ecial
cases
of
predictiv
e
clustering,
there
are
sev
eral
other
in
teresting
applications
that
do
not
fall
in
to
one
of
the
categories
men
tioned,
but
extend
them
in
some
w
a
y
or
com
bine
prop
erties
of
dieren
t
categories.
Classication
from
Scarce
Class
Information
Assume
one
has
to
induce
a
classier
from
a
set
of
data
where
only
a
small
p
ercen
tage
of
the
examples
is
lab
elled
with
a
class.
One
w
a
y
to
handle
this
setting
is
to
p
erform
unsup
ervised
learning,
and
once
the
clusters
are
formed
(and
not
b
efore)
use
the
a
v
ailable
class
v
alues
to
compute
a
protot
yp
e
for
eac
h
cluster.
If
the
clusters
are
coheren
t
with
resp
ect
to
classes,
this
metho
d
should
yield
relativ
ely
high
classication
accuracy
with
a
minim
um
of
class
information
a
v
ailable.
This
is
quite
similar
in
spirit
to
Emde's
metho
d
for
learning
from
few
classied
examples,
implemen
ted
in
the
COLA
system
(Emde,
		).

In
the
con
text
of
the
information-based
approac
h
w
e
ha
v
e
just
discussed,
this
tec
hnique
can
b
e
explained
b
y
stating
that
~
H
\
~
P
=
;.
More
generally
,
one
could
devise
an
algorithm
where
~
H

~
P
,
i.e.
the
learner
uses
not
only
class
information
but
also
other
information
in
its
heuristics.
A
similar
reasoning
can
b
e
follo
w
ed
for
regression.
Noise
Handling
Our
clustering
based
prediction
tec
hnique
is
not
only
robust
with
resp
ect
to
missing
class
information,
but
also
with
resp
ect
to
noise
in
general.
Dis-
criminan
t
descriptions
are
not
v
ery
robust
in
this
resp
ect:
if,
e.g.,
in
our
F
ruit&V
egetables
example
a
red
apple
w
ere
encoun
tered,
follo
wing
the
dis-
criminan
t
descriptions
of
clusters
it
w
ould
certainly
b
e
misclassied,
ev
en
if
it
has
all
the
other
c
haracteristics
of
an
apple.
When
discriminan
t
descriptions
can
b
e
supplemen
ted
with,
e.g.,
c
haracteristic
descriptions
or
distance
based
criteria,
a
m
uc
h
more
robust
predictiv
e
system
is
obtained.
Com
bining
Adv
an
tages
of
Dieren
t
Approac
hes
There
are
man
y
w
a
ys
in
whic
h
extensional
and
in
tensional
reasoning
could
b
e
com
bined.
F
or
instance,
one
could
assign
an
ob
ject
to
a
cluster
according
to

The
concept
learner
COLA
rst
mak
es
use
of
the
rst-order
clusterer
Kbg-
(Bisson,
		a)
to
cluster
the
data,
then
denes
the
concept
as
a
disjunction
of
some
of
these
clusters,
where
preference
is
giv
en
to
clusters
that
con
tain
man
y
p
ositiv
e
and
few
negativ
e
examples
(and,
b
y
the
user's
c
hoice,
man
y
or
few
unclassied
examples,
biasing
it
to
w
ards
more
general
or
more
sp
ecic
concepts).


CHAPTER
.
A
CLUSTERING
APPR
O
A
CH
TO
PREDICTION
its
discriminan
t
description,
c
hec
k
whether
it
ts
the
(more
elab
orate)
c
harac-
teristic
description
of
the
cluster,
and
if
not,
use
an
extensional
metho
d
(e.g.,
k
-nearest
neigh
b
or)
to
predict
missing
v
alues.
Alternativ
ely
,
one
could
com-
pute
the
distance
of
the
example
to
the
protot
yp
e,
and
if
it
is
to
o
far
o,
again
resort
to
an
extensional
prediction
metho
d.
The
p
ossibilit
y
to
com
bine
extensional
and
in
tensional
reasoning
p
oin
ts
out
opp
ortunities
to
join
existing
prediction
tec
hniques
(suc
h
as
instance
based
learning
and
rule
set
induction)
in
to
one
h
ybrid
system
that
com
bines
the
adv
an
tages
of
b
oth
separate
tec
hniques.
The
eect
of
this
w
ould
not
b
e
limited
to
increased
robustness
with
resp
ect
to
noise,
but
could
also
lead
to
more
ecien
t
prediction
metho
ds
or
higher
predictiv
e
accuracy
ev
en
in
noise-free
domains.
The
latter
claim
is
conrmed
b
y
some
related
w
ork,
whic
h
w
e
discuss
in
the
follo
wing
section.
.
Related
w
ork
P
art
of
this
c
hapter
is
based
on
(Blo
c
k
eel
et
al.,
		b),
but
extends
it
in
that
the
latter
pap
er
only
discusses
predictiv
e
clustering
in
the
decision
tree
con
text,
while
the
approac
h
follo
w
ed
in
this
c
hapter
mak
es
abstraction
of
the
represen
tation
formalism.
Our
w
ork
is
of
course
related
to
the
man
y
approac
hes
to
clustering
that
ex-
ist;
within
mac
hine
learning
Cobweb
(Fisher,
	)
and
Cluster/
(Mic
hal-
ski
and
Stepp,
	)
are
probably
the
most
inuen
tial
ones.
Although
cluster-
ing
is
usually
seen
as
a
descriptiv
e
tec
hnique
(it
iden
ties
structure
in
the
data),
the
p
ossibilit
y
of
using
it
for
predictions
has
b
een
recognised
since
long.
In
fact,
man
y
clustering
systems
ha
v
e
b
een
ev
aluated
b
y
measuring
their
p
oten
tial
to
mak
e
accurate
predictions,
giv
en
the
dicult
y
of
assessing
the
descriptiv
e
qual-
it
y
of
clusterings.
Nev
ertheless,
to
our
kno
wledge
an
explicit
clustering
based
approac
h
to
classication
and
regression
has
nev
er
b
een
discussed
in
detail.
Hybrid
Approac
hes
Concerning
h
ybrid
approac
hes
to
predictiv
e
induction,
our
w
ork
b
ears
an
in
ter-
esting
relationship
with
Domingos'
w
ork
on
com
bining
instance
based
learning
with
rule
induction
(Domingos,
		;
Domingos,
		).
In
Domingos'
ap-
proac
h,
a
rule
induction
system
is
used
to
induce
rules
from
a
data
set;
these
rules
are
considered
a
generalized
form
of
instances.
Predictions
are
then
made
for
new
instances,
based
on
their
distance
from
the
induced
rules.
The
gener-
alized
instances
ha
v
e
exactly
the
same
functionalit
y
as
the
clusters
(or
more
sp
ecically
,
the
protot
yp
es)
in
our
approac
h.
Another
approac
h
to
w
ards
com
bining
the
use
of
distances
for
prediction
with
explicit
induction
is
presen
ted
b
y
W
ebb
(		).
W
ebb,
in
con
trast
to

..
RELA
TED
W
ORK

+  +
  + +
+
-
- -
-
-
-
-
-
-
-
-
-
-
a
X
Y
X<1
Y>1.5
+
-
-
1.5
1
+  +
  + +
+
-
- -
-
-
-
-
-
-
-
-
-
-
a
+
-
X<1
X
Y
1
Figure
.:
An
example
of
an
instance
that
has
a
high
probabilit
y
of
b
eing
misclassied
b
y
a
simple
tree.
Domingos,
fo
cuses
on
decision
trees
instead
of
rule
sets.
He
demonstrates
that
the
predictiv
e
accuracy
of
a
decision
tree
can
signican
tly
increase
b
y
making
the
tree
more
complex
(instead
of
pruning
it,
as
most
decision
tree
induction
systems
do).
The
argumen
t
for
splitting
lea
v
es
in
to
separate
parts,
ev
en
if
those
lea
v
es
are
pure
with
resp
ect
to
the
class
v
alue,
is
based
on
distances.
An
example
is
giv
en
in
Figure
..
The
unseen
example
lab
elled
(a)
is
in
a
leaf
of
p
ositiv
e
instances,
while
all
its
closest
neigh
b
ors
are
negativ
e.
W
ebb's
approac
h
handles
suc
h
cases
b
y
constraining
the
area
where
an
example
will
b
e
predicted
to
b
e
p
ositiv
e
as
m
uc
h
as
p
ossible,
and
to
this
aim
k
eeps
splitting
no
des
that
with
the
standard
decision
tree
approac
h
w
ould
b
ecome
lea
v
es.
A
third
approac
h
to
com
bining
distance
based
prediction
and
induction
is
follo
w
ed
b
y
SR
T
(Kramer,
		);
this
regression
tree
builder
mak
es
predictions
using
the
classical
sorting
approac
h
(i.e.
it
sorts
an
example
do
wn
the
tree
in
to
a
leaf
),
but
it
also
p
erforms
a
c
hec
k
b
y
computing
the
distance
b
et
w
een
the
example
and
the
leaf
it
b
elongs
to.
When
the
distance
is
suspiciously
high,
the
example
is
reclassied
b
y
assigning
it
to
the
cluster
that
is
closest
to
the


CHAPTER
.
A
CLUSTERING
APPR
O
A
CH
TO
PREDICTION
example,
considering
the
tree
as
a
hierarc
h
y
of
clusters.
The
eect
is
v
ery
similar
to
that
of
W
ebb's
approac
h.
Kramer
refers
to
this
tec
hnique
as
outlier
dete
ction.
Domingos
(		),
W
ebb
(		)
and
Kramer
(		)
ha
v
e
all
b
een
able
to
sho
w
that
mixing
a
distance
based
approac
h
with
explicit
induction
often
leads
to
an
increase
in
predictiv
e
accuracy
.
These
results
pro
vide
supp
ort
for
our
thesis
that
a
general
approac
h
can
com
bine
the
adv
an
tages
of
more
sp
ecic
approac
hes.
.
Conclusions
In
this
c
hapter
w
e
ha
v
e
rst
in
tro
duced
distances,
protot
yp
es
and
clustering.
W
e
ha
v
e
iden
tied
a
sp
ecial
case
of
clustering
that
w
e
called
predictiv
e
clus-
tering;
it
encompasses
classication
and
regression
but
can
also
b
e
used
for
exible
prediction
and
unsup
ervised
learning.
W
e
ha
v
e
distinguished
a
purely
extensional
form
of
it,
whic
h
w
as
related
to
instance
based
learning;
a
purely
in
tensional
form;
and
a
mixed
form.
The
latter
turns
out
to
b
e
the
most
in-
teresting
one,
since
it
oers
a
broad
range
of
p
ossibilities;
moreo
v
er
it
con
tains
sev
eral
classical
approac
hes
to
predictiv
e
induction
(rule
set
induction,
decision
tree
induction)
as
sp
ecial
cases.
These
classical
approac
hes
turn
out
to
o
ccup
y
only
the
extremes
of
a
sp
ectrum
pro
vided
b
y
our
framew
ork.
W
e
ha
v
e
then
argued
that
the
area
in
b
et
w
een
these
extremes
ma
y
pro
vide
in
teresting
opp
or-
tunities
with
resp
ect
to,
e.g.,
noise
handling,
as
w
ell
as
for
com
bining
existing
approac
hes.
Some
recen
t
w
ork
with
suc
h
h
ybrid
approac
hes
has
b
een
discussed
in
this
con
text,
pro
viding
supp
ort
for
the
usefulness
of
a
unifying
approac
h.

Chapter

T
op-do
wn
Induction
of
Decision
T
rees
.
In
tro
duction
Decision
trees
are
among
the
most
p
opular
to
ols
for
mac
hine
learning
and
data
mining.
They
are
most
often
used
in
the
con
text
of
classication,
and
hence
are
often
dened
with
this
sp
ecic
purp
ose
in
mind.
Sometimes
they
are
used
for
regression,
in
whic
h
case
a
sligh
tly
dieren
t
denition
is
giv
en.
In
b
oth
cases
the
tree
is
seen
as
simply
a
function
mapping
instances
on
to
predictions.
A
few
authors
ha
v
e
p
oin
ted
out,
ho
w
ev
er,
that
a
decision
tree
also
represen
ts
a
cluster
hierarc
h
y
,
and
hence
decision
tree
induction
can
b
e
used
in
the
clustering
con
text.
In
this
c
hapter
w
e
study
decision
trees
in
the
con
text
of
predictiv
e
clustering.
W
e
rst
sho
w
in
Section
.
ho
w
decision
trees
represen
t
the
t
w
o
steps
in
the
prediction
pro
cess
w
e
discussed
in
the
previous
c
hapter.
Then,
in
Section
.,
w
e
discuss
top-do
wn
induction
of
decision
trees
as
a
generic
pro
cedure
that
can
b
e
instan
tiated
with
parameter
pro
cedures.
W
e
sho
w
ho
w
some
t
ypical
instan
tiations
result
in
the
classical
TDIDT
approac
hes
to
prediction.
This
approac
h
is
reminiscen
t
of
the
one
in
the
previous
c
hapter,
where
w
e
sho
w
ed
that
classication
and
regression
can
b
e
seen
as
sp
ecial
cases
of
clustering.
The
dierence
is
that
the
discussion
in
Chapter

made
abstraction
of
the
precise
induction
metho
d;
in
this
c
hapter
w
e
narro
w
the
fo
cus
to
one
metho
d
(induction
of
decision
trees)
and
study
it
in
more
detail.
In
Section
.
w
e
briey
compare
decision
trees
with
another
p
opular
rep-
resen
tation
formalism:
rule
sets.
Section
.
discusses
some
related
w
ork
and
in
Section
.
w
e
conclude.



CHAPTER
.
TOP-DO
WN
INDUCTION
OF
DECISION
TREES
pro
cedure
sor
t(T
:
tree,
e
:
example)
returns
leaf:
N
:=
T
while
N
=
leaf
(C
)
do
let
N
=
ino
de(
;
f(r
;
t
r
)jr

r
ang
e(
)g)
N
:=
t

(e)
return
N
pro
cedure
t
ar
get(N
:
leaf
)
returns
prediction:
let
N
=
leaf
(C
)
return
p
0
(C
)
pro
cedure
predict(T
:
tree,
e:
example)
returns
prediction:
return
t
ar
get(sor
t(T
,
e))
Figure
.:
Making
a
prediction
for
an
example
using
a
decision
tree.
.
Decision
T
rees
Denition
.
(Decision
tree)
A
de
cision
tr
e
e
for
a
domain
I
is
dene
d
r
e
cursively
as
fol
lows:

leaf
(C
)
is
a
de
cision
tr
e
e
for
I
(C
c
an
b
e
anything)

ino
de
(
;
S
)
is
a
de
cision
tr
e
e
for
I
if
and
only
if

is
a
function
fr
om
I
to
some
set
R
(we
c
al
l

a
test)
and
S
is
a
set
of
c
ouples
such
that
r

R
:
(r
;
t
r
)

S
and
t
r
is
a
de
cision
tr
e
e
for
I
I.e.,
e
ach
no
de
in
a
de
cision
tr
e
e
c
ontains
a
test
and
asso
ciates
a
subtr
e
e
with
e
ach
p
ossible
outc
ome
of
the
test.
A
decision
tree
maps
an
instance
e
on
to
a
leaf
in
the
follo
wing
manner:
starting
with
the
ro
ot
no
de
r
,
one
sub
jects
the
instance
e
to
the
test

r
and
selects
the
c
hild
no
de
asso
ciated
with

r
(e).
This
pro
cedure
is
rep
eated
for
the
new
no
de
un
til
a
leaf
no
de
is
reac
hed.
W
e
sa
y
that
the
instance
has
then
b
een
sorted
in
to
that
leaf.
The
sor
t
algorithm
in
Figure
.
implemen
ts
this
pro-
cedure.
In
this
algorithm
a
leaf
is
represen
ted
as
leaf
(C
)
with
C
an
extensional
represen
tation
of
the
cluster.
Note
that
if
w
e
consider
lea
v
es
as
clusters,
sor
t
is
in
fact
a
cluster
assignmen
t
function
f
.
Through
this
sorting
pro
cedure,
eac
h
decision
tree
denes
a
function
t
:
I
!
C
,
where
I
is
the
instance
space
and
C
is
the
set
of
all
p
ossible
lea
v
es.
W
e
can
consider
t
a
sp
ecialization
of
the
cluster
assignmen
t
function
f
for
C
,
i.e.
t(e)
=
f
(e;
C
).

..
DECISION
TREES

leaf
prediction
t
p’
example
Figure
.:
Tw
o
steps
in
the
mapping
from
instances
to
predictions.
color
weight < 80g?
strawberry
tomato
red
false
true
green
apple
Figure
.:
A
fruit
classier
in
decision
tree
format.
F
or
con
v
enience
w
e
dene
p
0
:
C
!
P
:
p
0
(C
)
=

(p(C
))
(.)
Figure
.
then
illustrates
ho
w
t
and
p
0
implemen
t
the
t
w
o-step
prediction
pro-
cess
w
e
discussed
in
the
previous
c
hapter.
An
algorithm
is
giv
en
in
Figure
.;
it
has
the
same
structure.
In
practical
implemen
tations,
ho
w
ev
er,
the
t
ar
get
function
is
often
made
implicit,
i.e.
p
0
(C
)
is
stored
in
a
leaf
instead
of
C
.
Al-
ternativ
ely
,
p(C
)
could
b
e
stored,
where
p(C
)
could
b
e
either
a
protot
yp
e
as
w
e
ha
v
e
dened
it,
or
a
generalized
protot
yp
e
(so
that,
e.g.,
trees
with
linear
mo
dels
in
the
lea
v
es
are
also
allo
w
ed).
Note
that
t
mak
es
use
of
the
tests
in
the
tree,
whic
h
form
an
in
tensional
description
of
the
clusters
(see
further),
while
p
0
mak
es
use
of
the
protot
yp
e,
whic
h
is
part
of
the
extensional
description.
The
pro
cess
is
th
us
an
instan
tiation
of
the
\example
)
I
cluster
)
E
pr
e
diction"
t
yp
e
of
reasoning.
W
e
can
distinguish
sev
eral
kinds
of
decision
trees.
A
classic
ation
tr
e
e
is
a
decision
tree
where
P
is
nominal;
suc
h
a
tree
is
sho
wn
in
Figure
..
A
r
e
gr
ession
tr
e
e
is
a
decision
tree
where
P
is
n
umerical.
One
could
call
a
decision
tree
a
exible
pr
e
diction
tr
e
e
if
p
0
=
p.
A
decision
tree
is
v
ery
similar
to
a
cluster
hierarc
h
y:
all
the
no
des
on
one
lev
el
dene
a
partition
of
the
examples,
and
the
c
hildren
of
a
no
de
dene
a
partition
of
the
examples
co
v
ered
b
y
that
no
de.
Eac
h
cluster
has
a
discriminan
t
description
of
the
form
fej
^
i

i
(e)
=
r
i
g


CHAPTER
.
TOP-DO
WN
INDUCTION
OF
DECISION
TREES
with
i
ranging
o
v
er
all
the
ancestor
no
des
of
the
cluster
and
r
i
the
result
of

i
(e)
for
all
e
in
the
cluster.
Th
us,
the
induction
of
a
decision
tree
can
b
e
seen
as
a
descriptiv
e
hierarc
hical
clustering
metho
d,
returning
discriminan
t
descriptions
of
the
clusters.
Suc
h
a
tree
could
b
e
called
a
clustering
tr
e
e.
Example
.
F
or
the
tree
in
Figure
.,
the
follo
wing
table
sho
ws
the
clusters
in
the
tree
and
the
corresp
onding
discriminan
t
descriptions:
cluster
description
stra
wb
erries
Weight<0g
=
true
tomato
es+apples
Weight<0g
=
false
tomato
es
Weight<0g
=
false
^
Color
=
red
apples
Weight<0g
=
false
^
Color
=
green
In
practice,
an
expression
suc
h
as
Weight<0g
=
false
is
of
course
abbreviated
to
Weight0g.

W
e
w
an
t
to
stress
here
that
the
qualit
y
criterion
for
descriptiv
e
clusterings
is
in
a
sense
more
sev
ere
than
for
predictiv
e
clusterings.
F
or
a
predictiv
e
clustering
it
suces
that
the
in
tra-cluster
v
ariance
of
the
leaf
clusters
is
small.
If
a
tree
is
considered
as
a
descriptiv
e
cluster
hierarc
h
y
,
the
qualit
y
of
the
clusters
dened
b
y
in
ternal
no
des
is
also
imp
ortan
t.
In
the
previous
c
hapter
w
e
discussed
exibilit
y
of
clustering,
and
noted
that
b
y
allo
wing
complex
cluster
descriptions
one
ma
y
obtain
clusterings
of
higher
qualit
y
.
In
the
con
text
of
decision
trees
this
translates
to
allo
wing
complex
tests

i
in
the
in
ternal
no
des
of
a
tree.
.
Induction
of
Decision
T
rees
Most
systems
that
induce
classication
or
regression
trees
construct
the
tree
from
the
ro
ot
to
the
lea
v
es.
This
metho
d
is
usually
referred
to
as
T
op-Down
Induction
of
De
cision
T
r
e
es
(TDIDT)
(Quinlan,
	),
or
as
the
divide-and-
c
onquer
approac
h.
Both
incremen
tal
and
non-incremen
tal
approac
hes
exist.
In
this
text
w
e
only
consider
non-incremen
tal
approac
hes.
A
generic
non-incremen
tal
TDIDT-algorithm
is
sho
wn
in
Figure
..
W
e
use
the
con
v
en
tion
that
pro
cedures
that
are
in
fact
parameters
of
the
algorithm
are
written
in
normal
capitals
(e.g.
OPTIMAL
SPLIT),
while
real
pro
cedure
names
are
written
in
small
capitals
(e.g.
tdidt).
Our
TDIDT
algorithm

consists
of
t
w
o
phases,
a
gro
wing
phase
and
a
pruning
phase.
The
tree
gro
wing
algorithm
w
orks
as
follo
ws:
giv
en
a
set
of
examples
E
,
it
constructs
some
set
of
tests
T
(GENERA
TE
TESTS).
This
set
usually
dep
ends
on
the
form
of
the

TDIDT
is
used
as
an
abbreviation
of
T
op-Do
wn
Induction
of
Decision
T
rees,
while
tdidt
refers
to
the
sp
ecic
algorithm
in
Figure
..

..
INDUCTION
OF
DECISION
TREES
	
example
descriptions,
but
ma
y
also
dep
end
on
sp
ecic
v
alues
o
ccurring
in
the
data.
F
or
instance,
if
example
descriptions
con
tain
an
attribute
Color
that
tak
es
only
the
v
alues
red,
blue
in
the
observ
ed
data,
then
the
tests
Color=red
and
Color=blue
are
in
T
.
Eac
h
test


T
,
b
eing
a
function
from
the
instance
space
to
a
nite
result
space,
induces
a
partition
E
on
E
that
can
b
e
dened
as
follo
ws:
E
=
fE
j
jE
j
=
fe

E
j
(e)
=
r
j
gg:
(.)
The
algorithm
calls
the
function
OPTIMAL
SPLIT
to
nd
that
test


T
that
partitions
E
in
some
optimal
w
a
y
.
The
algorithm
next
calls
a
function
STOP
CRIT
to
c
hec
k
whether
the
op-
timal
partition
E
that
w
as
found
is
sucien
tly
go
o
d
to
justify
the
creation
of
a
subtree.
If
it
is
not,
a
leaf
is
constructed
con
taining
some
relev
an
t
information
ab
out
E
(this
relev
an
t
information
is
computed
b
y
the
function
INF
O).
If
E
is
sucien
tly
go
o
d,
then
gr
o
w
tree
is
called
recursiv
ely
on
all
the
E
j

E
,
and
the
returned
trees
t
j
b
ecome
subtrees
of
the
curren
t
no
de.
In
man
y
systems
the
gr
o
w
tree
pro
cedure
gro
ws
an
o
v
erly
large
tree
that
ma
y
o
v
ert
the
training
data.
Therefore,
after
the
tree-gro
wing
phase
these
systems
ha
v
e
a
p
ost-pruning
phase
in
whic
h
branc
hes
are
pruned
from
the
tree,
in
the
hop
e
to
obtain
a
b
etter
tree.
The
pruning
algorithm
PR
UNE
p
erforms
this.
The
functions
OPTIMAL
SPLIT,
STOP
CRIT,
INF
O
and
PR
UNE
are
parameters
of
tdidt
that
will
b
e
instan
tiated
according
to
the
sp
ecic
task
that
is
at
hand
(e.g.,
classication
requires
dieren
t
functions
than
regression)

.
Note
that
the
c
hoice
of
OPTIMAL
SPLIT,
STOP
CRIT
and
PR
UNE
determ-
ines
the
function
t
that
maps
examples
to
lea
v
es,
whereas
the
INF
O
function
determines
whether
C
,
p(C
),
p
0
(C
)
or
p
ossibly
something
else
is
stored.
Most
(classication
or
regression)
tree
induction
systems
that
exist
to
da
y
are
instan
tiations
of
this
generic
algorithm:
w
e
men
tion
C.
(Quinlan,
		a),
Car
t
(Breiman
et
al.,
	),
Str
uct
(W
atanab
e
and
Rendell,
		),
SR
T
(Kramer,
		),
.
.
.
The
main
exceptions
are
incremen
tal
decision
tree
learners
(Utgo,
		;
Chapman
and
Kaelbling,
		).
These
systems
t
ypically
build
the
tree
top-do
wn,
but
con
tain
op
erators
for
c
hanging
the
tree
when
new
evid-
ence
suggests
to
do
so
(b
y
c
hanging
tests,
rearranging
no
des,
extending
lea
v
es
or
collapsing
in
ternal
no
des
in
to
lea
v
es).
W
e
no
w
discuss
p
ossible
instan
tiations
of
the
generic
algorithm
in
Figure
.
in
the
con
text
of
classication,
regression
and
clustering.
T
able
.
giv
es
an
o
v
erview
of
these
instan
tiations,
stressing
the
similarities
b
et
w
een
the
dieren
t
algorithms.

The
function
GENERA
TE
TESTS
is
also
a
parameter,
but
is
irrelev
an
t
to
the
discussion
in
this
c
hapter.

0
CHAPTER
.
TOP-DO
WN
INDUCTION
OF
DECISION
TREES
function
gr
o
w
tree(E
:
set
of
examples)
returns
decision
tree:
T
:=
GENERA
TE
TESTS(E
)

:=
OPTIMAL
SPLIT(T
,
E
)
E
:=
partition
induced
on
E
b
y

if
STOP
CRIT(E
,
E
)
then
return
leaf(INF
O(E
))
else
for
all
E
j
in
E
:
t
j
:=
gr
o
w
tree(E
j
)
return
ino
de(
,
f(j;
t
j
)g)
function
tdidt(E
:
set
of
examples)
returns
decision
tree:
T
0
:=
gr
o
w
tree(E
)
T
:=
PR
UNE(T
0
)
return
T
Figure
.:
The
TDIDT
algorithm.
OPTIMAL
SPLIT
STOP
CRIT
INF
O
PR
UNE
classication
gain
(ratio)


-test
mo
de
C.
Gini
index
min.
co
v
erage
v
alid.
set
MDL
regression
in
tra-cluster
v
ariance
F-test,
t-test
mean
v
alid.
set
of
target
v
ariable
min.
co
v
erage
MDL
clustering
in
tra-cluster
v
ariance
F-test,
t-test
protot
yp
e
v
alid.
set
min.
co
v
erage
iden
tit
y
MDL
T
able
.:
Ov
erview
of
the
dieren
t
tasks
that
can
b
e
p
erformed
with
TDIDT
b
y
instan
tiating
its
pro
cedure
parameters.

..
INDUCTION
OF
DECISION
TREES

..
Splitting
Heuristics
Giv
en
a
set
of
tests
T
,
the
function
OPTIMAL
SPLIT
computes
for
eac
h


T
the
partition
induced
b
y

on
the
set
of
examples
E
.
It
ev
aluates
these
partitions
and
c
ho
oses
the
test

that
is
optimal
with
resp
ect
to
the
task
that
is
to
b
e
p
erformed.
Classication
F
or
classication,
man
y
qualit
y
criteria
ha
v
e
b
een
prop
osed.
W
e
men
tion
a
few
of
them;
for
eac
h
one
it
is
the
case
that
a
split
is
considered
optimal
if
it
maximizes
the
criterion.

Information
gain
(Quinlan,
		a):
the
class
entr
opy
of
a
set
of
examples
E
is
dened
as
s(E
)
=
k
X
i=
p(c
i
;
E
)
log
p(c
i
;
E
)
(.)
where
k
is
the
n
um
b
er
of
classes,
the
c
i
are
the
classes
and
p(c
i
;
E
)
is
the
prop
ortion
of
the
examples
in
E
that
b
elong
to
class
c
i
.
The
information
gained
b
y
p
erforming
a
test

is
G
=
s(E
)
 X
E
i
E
jE
i
j
jE
j
s(E
i
)
(.)
where
E
is
the
partition
on
E
induced
b
y

.

Information
gain
r
atio
(Quinlan,
		a):
the
information
gain
obtained
with
a
test
is
compared
to
the
maximal
gain
that
can
b
e
oered
b
y
an
y
test

0
for
whic
h
the
cardinalit
y
of
the
induced
partition
and
of
the
elemen
ts
of
this
partition
are
the
same
as
for

.
M
G
=
X
E
i
E
p
i
log
p
i
(.)
with
p
i
=
jE
i
j=jE
j.
The
gainratio
is
the
ratio
of
the
gain
and
this
maximal
gain:
GR
=
G=
M
G
(.)

the
Gini
heuristic
(Breiman
et
al.,
	):
this
is
similar
to
information
gain,
but
instead
of
class
en
trop
y
,
the
Gini
index
for
impurit
y
is
used:
g
(E
)
=
k
X
i=
p(c
i
;
E
)(
 p(c
i
;
E
))
=

 k
X
i=
p(c
i
;
E
)

(.)


CHAPTER
.
TOP-DO
WN
INDUCTION
OF
DECISION
TREES
The
qualit
y
of
a
split
is
computed
as
Q
=
g
(E
)
 X
E
i
E
jE
i
j
jE
j
g
(E
i
)
(.)
These
criteria
seem
to
b
e
the
most
p
opular
ones
for
induction
of
classi-
cation
trees.
Empirical
comparisons
b
et
w
een
dieren
t
criteria
can
b
e
found
in
(Mingers,
		;
Bun
tine
and
Niblett,
		);
(Breiman
et
al.,
	)
and
(Breiman,
		)
con
tain
some
more
theoretical
discussions.
An
in
teresting
ob-
serv
ation
is
that
none
of
these
heuristics
are
directly
related
to
the
in
tra-cluster
v
ariance
in
the
partition
induced
b
y
the
test
(as
also
remark
ed
in
Section
..).
Regression
Regression
systems
t
ypically
use
as
qualit
y
criterion
the
in
tra-subset
(or
within-
subset)
v
ariation
of
the
target
v
ariable
(see
e.g.
Car
t
(Breiman
et
al.,
	),
SR
T
(Kramer,
		)):
S
S
W
=
X
E
i
E
X
j
(y
ij
 
y
i
)

(.	)
where
y
ij
denotes
the
j
-th
observ
ation
of
the
target
v
ariable
in
the
set
E
i
,

y
i
=
P
j
y
ij
=jE
i
j
and

y
=
P
i;j
y
ij
=jE
j.
This
qualit
y
criterion
should
b
e
minimized,
not
maximized.
F
rom
the
statistical
tec
hnique
kno
wn
as
analysis
of
v
ariance
(ANO
V
A),
it
is
kno
wn
that
if
a
set
of
v
alues
for
a
v
ariable
is
partitioned
in
to
subsets,
the
total
v
ariation
of
the
v
ariable
(measured
as
the
sum
of
squares
of
dierences
b
et
w
een
the
v
alues
and
the
mean)
can
b
e
decomp
osed
in
to
a
within-subset
and
a
b
et
w
een-subset
v
ariation,
as
follo
ws:
S
S
T
=
S
S
B
+
S
S
W
(.0)
with
S
S
T
=
X
i;j
(y
ij
 
y
)

(.)
S
S
B
=
X
i
n
i
(

y
i
 
y
)

(.)
where
n
i
is
the
n
um
b
er
of
elemen
ts
b
elonging
to
subset
i,
S
S
T
stands
for
total
v
ariation,
S
S
B
is
b
et
w
een-subset
v
ariation
and
S
S
W
is
within-subset
v
ariation.
Equation
.0
sho
ws
that
minimizing
S
S
W
is
equiv
alen
t
to
maximizing
S
S
B
.
In
the
case
where
only
t
w
o
subgroups
are
formed,
maximizing
S
S
B
is
equiv
alen
t
to
maximizing
j

y

 
y

j.

..
INDUCTION
OF
DECISION
TREES

Clustering
Since
the
heuristic
for
regression
minimizes
v
ariance,
a
generalization
to
w
ards
predictiv
e
clustering
is
straigh
tforw
ard:
S
S
T
=
X
i;j
d(e
ij
;
p(E
))

(.)
S
S
B
=
X
i
n
i
d(p(E
i
);
p(E
))

(.)
S
S
W
=
X
i
X
j
d(e
ij
;
p(E
i
))

(.)
If
d
=
d
E
,
it
still
holds
that
S
S
T
=
S
S
B
+
S
S
W
,
but
this
do
es
not
hold
for
ev
ery
distance.
A
clustering
system
then
has
to
c
ho
ose
b
et
w
een
maximizing
S
S
B
and
minimizing
S
S
W
.
If
only
t
w
o
clusters
E

and
E

are
formed,
it
could
also
maximize
d(p(E

);
p(E

)).
..
Stopping
Criteria
Man
y
dieren
t
stopping
criteria
ha
v
e
b
een
prop
osed
in
the
literature.
Some
v
ery
simple
criteria
are:

stop
splitting
when
a
cluster
is
sucien
tly
coheren
t
(i.e.
its
v
ariance
is
b
elo
w
an
acceptable
threshold)

stop
splitting
when
the
n
um
b
er
of
examples
co
v
ered
b
y
a
no
de
is
b
elo
w
some
threshold
A
more
complicated
criterion
is
the
MDL
principle
(Rissanen,
	).
MDL
stands
for
Minimal
Description
L
ength.
The
reasoning
b
ehind
suc
h
a
criterion
is
that
the
correct
target
v
alues
of
a
set
of
examples
can
b
e
enco
ded
in
the
form
of
a
h
yp
othesis,
together
with
a
list
of
corrections:
an
exhaustiv
e
en
umeration
of
all
the
v
alues
that
are
predicted
incorrectly
b
y
the
h
yp
othesis.
When
comparing
t
w
o
h
yp
otheses
with
dieren
t
predictiv
e
qualit
y
and
dieren
t
complexit
y
,
one
should
prefer
the
one
with
minimal
description
length;
in
other
w
ords:
only
mak
e
a
h
yp
othesis
more
complex
if
the
gain
in
predictiv
e
qualit
y
is
sucien
tly
large
to
justify
it.
Hence,
MDL
is
some
sort
of
exc
hange
rate
that
is
applied
when
trading
simplicit
y
for
accuracy
.
While
the
metho
d
has
theoretical
foundations,
it
is
still
relativ
ely
ad
ho
c
in
the
sense
that
the
b
est
theory
is
not
necessarily
the
most
compact
one.
Moreo
v
er,
applying
it
outside
the
classication
con
text
is
quite
complicated.
Kramer
(		)
discusses
ho
w
it
can
b
e
applied
for
regression.
This
v
ersion
of
MDL
could
in
principle
b
e
generalized
to
the
clustering
con
text.
Another
family
of
stopping
criteria
is
based
on
signicance
tests.
In
the
classication
con
text
a


-test
is
often
used
to
c
hec
k
whether
the
class


CHAPTER
.
TOP-DO
WN
INDUCTION
OF
DECISION
TREES
distributions
in
the
subtrees
dier
signican
tly
(the
same
test
is
used
b
y
some
rule-based
systems,
e.g.
CN
(Clark
and
Niblett,
		)
and
ICL
(De
Raedt
and
V
an
Laer,
		)).
A


-test
w
as
also
incorp
orated
in
Quinlan's
ID
algorithm
(Quinlan,
	),
the
predecessor
of
C..
It
w
as
not
incorp
orated
in
C.
b
ecause,
as
Quinlan
notes
(Quinlan,
		a),
the
test
is
relativ
ely
un
trust
w
orth
y
and
b
etter
results
are
usually
obtained
b
y
not
using
an
y
signicance
test
but
pruning
the
tree
afterw
ards.
A
similar
argumen
t
is
giv
en
b
y
Breiman
et
al.
(	).
Signicance
tests
do
ha
v
e
the
adv
an
tage
that
one
can
stop
gro
wing
a
tree
relativ
ely
early
,
instead
of
gro
wing
a
large
tree
and
pruning
a
w
a
y
most
of
its
branc
hes;
th
us,
an
imp
ortan
t
gain
in
eciency
is
ac
hiev
ed.
F
or
this
reason,
they
are
still
incorp
orated
in
man
y
systems.
Since
regression
and
clustering
use
v
ariance
as
a
heuristic
for
c
ho
osing
the
b
est
split,
a
reasonable
heuristic
for
the
stopping
criterion
seems
to
b
e
the
F-
test.
If
a
set
of
examples
is
split
in
to
t
w
o
subsets,
the
v
ariance
should
decrease
signican
tly
,
i.e.,
F
=
S
S
T
=(n
 )
S
S
W
=(n
 k
)
(.)
should
b
e
signican
tly
large
(where
S
S
T
and
S
S
W
are
dened
b
y
Equations
.
and
.,
k
is
the
n
um
b
er
of
subsets
in
the
partition
and
n
is
the
n
um
b
er
of
examples
in
the
whole
set).
When
k
=

the
F
-test
is
equiv
alen
t
to
a
t-test
on
the
protot
yp
es,
whic
h
could
b
e
used
instead.
..
Information
in
Lea
v
es
The
INF
O
function
computes
the
information
that
is
to
b
e
stored
in
a
leaf.
T
ypically
,
one
stores
only
that
information
that
will
b
e
necessary
for
prediction,
i.e.
the
v
alue
that
is
to
b
e
predicted:

for
classication
trees:
store
the
mo
de
of
the
class
v
alues
observ
ed
in
the
leaf

for
regression
trees:
store
the
mean
of
the
v
alues
of
the
target
v
ariable
that
are
observ
ed
in
the
leaf

for
clustering
trees:
store
an
extensional
cluster
represen
tation
C
,
or
the
protot
yp
e
p(C
)
Storing
a
protot
yp
e
or
extensional
represen
tation
mak
es
exible
prediction
p
os-
sible,
but
of
course
it
can
also
b
e
done
for
classication
or
regression
trees.

..
INDUCTION
OF
DECISION
TREES

..
P
ost-pruning
Because
it
is
hard
to
nd
go
o
d
stopping
criteria,
man
y
tree
building
systems
gro
w
an
o
v
ersize
tree
and
afterw
ards
prune
a
w
a
y
those
branc
hes
that
do
not
seem
useful.
This
pro
cedure
is
computationally
more
exp
ensiv
e,
but
is
kno
wn
to
yield
b
etter
trees
(see
e.g.
(Breiman
et
al.,
	;
Quinlan,
		a)).
Sev
eral
metho
ds
for
p
ost-pruning
are
in
use.
C.
uses
an
estimate
of
the
tree
accuracy
on
unseen
data;
this
estimate
is
based
on
the
errors
in
the
training
set
as
w
ell
as
the
complexit
y
of
the
tree.

The
metho
d
lac
ks
a
rm
statistical
ground,
but
seems
to
w
ork
reasonably
w
ell
in
practice.
It
only
w
orks
for
clas-
sication
trees,
ho
w
ev
er
(although
it
could
p
ossibly
b
e
generalized
to
w
ards
regression
or
clustering).
Another
w
a
y
to
estimate
the
accuracy
on
unseen
data,
is
to
simply
remo
v
e
a
random
sample
from
the
training
set,
and
use
it
as
unseen
data
to
ev
aluate
the
tree
afterw
ards.
The
set
that
is
remo
v
ed
in
this
w
a
y
is
called
a
validation
set,
hence
w
e
call
this
pruning
metho
d
validation
set
b
ase
d
pruning.
The
idea
is
that
through
consecutiv
e
pruning,
a
series
of
subtrees
of
the
original
tree
is
generated;
the
subtree
that
has
the
highest
qualit
y
on
the
v
alidation
set
is
c
hosen
as
the
nal
tree.
This
metho
d
is
describ
ed
b
y
Breiman
et
al.
(	),
who
also
prop
ose
a
more
sophisticated
algorithm
that
uses
the
same
underlying
idea.
In
their
approac
h,
pruning
is
con
trolled
b
y
a
parameter

that
represen
ts
the
marginal
cost
of
ha
ving
an
extra
leaf
in
the
tree.
The
optimal
v
alue
for

is
estimated
using
a
cross-v
alidation:
sev
eral
auxiliary
trees
are
gro
wn,
eac
h
with
a
dieren
t
v
alidation
set,
and

is
c
hosen
so
that
the
a
v
erage
qualit
y
of
eac
h
auxiliary
tree
on
its
o
wn
v
alidation
set
is
maximized.
This

is
then
used
to
prune
a
tree
that
has
b
een
gro
wn
on
the
whole
data
set.
Adv
an
tages
of
this
metho
d
are
that
it
can
b
e
used
for
an
y
kind
of
trees,
as
long
as
there
is
some
notion
of
the
qualit
y
of
a
tree
and
a
w
a
y
to
compute
it
(see
Section
..),
and
that
it
is
a
statistically
sound
tec
hnique
to
estimate
the
qualit
y
of
a
tree
on
unseen
data.
A
disadv
an
tage
of
the
simple
v
alidation
set
based
pruning
metho
d
is
that
the
training
sample
b
ecomes
smaller,
whic
h
ma
y
aect
the
qualit
y
of
the
tree;
this
disadv
an
tage
is
a
v
oided
with
Breiman
et
al.
(	)'s
cross-v
alidation-based
algorithm.
..
Summary
Lo
oking
bac
k
at
T
able
.,
w
e
can
conclude
that
the
curren
tly
existing
ap-
proac
hes
to
the
induction
of
classication
or
regression
trees
are
all
v
ery
similar
from
the
p
oin
t
of
view
of
building
predictiv
e
clustering
trees:

Using
the
data
in
the
leaf,
a
condence
in
terv
al
for
the
predictiv
e
accuracy
of
the
leaf
is
constructed;
the
accuracy
of
the
leaf
is
estimated
as
the
lo
w
er
b
ound
of
this
in
terv
al,
follo
wing
the
reasoning
that
the
training
set
accuracy
in
the
leaf
is
probably
to
o
optimistic.
Smaller
lea
v
es
yield
larger
in
terv
als,
hence
more
p
essimistic
estimates.


CHAPTER
.
TOP-DO
WN
INDUCTION
OF
DECISION
TREES

The
OPTIMAL
SPLIT
pro
cedure
basically
minimizes
the
in
tra-cluster
v
ariance
of
the
partition
induced
b
y
a
test;
exceptions
are
mainly
found
with
classication,
where
v
ariance
based
heuristics
w
ork
less
w
ell
than
sp
ecialized
heuristics
based
on
information
gain
(ratio)
or
Gini
index.

The
STOP
CRIT
pro
cedure
for
regression
is
similar
to
that
for
clustering,
but
for
classication
the
signicance
test
and
the
implemen
tation
of
the
MDL
tec
hnique
dier.

Giv
en
that
mo
des
and
means
are
sp
ecial
cases
of
protot
yp
es,
the
INF
O
pro
cedure
is
essen
tially
the
same
in
all
cases.

The
v
alidation
set
based
p
ost-pruning
metho
d
can
b
e
used
in
all
cases,
al-
though
at
least
one
classication
system
(C.)
con
tains
a
sp
ecic
metho
d
that
do
es
not
immediately
generalize
to
the
other
settings.
.
T
rees
v
ersus
Rules
There
are
t
w
o
v
ery
p
opular
represen
tation
formalisms
within
sym
b
olic
mac
hine
learning;
decision
trees
form
the
rst
one,
a
second
one
is
rule
sets.
W
e
no
w
briey
discuss
induction
of
rule
sets
and
compare
it
with
induction
of
decision
trees.
This
section
pro
vides
some
motiv
ation
for
the
use
of
decision
trees
in
this
text,
but
is
not
mandatory
for
reading
the
remainder
of
this
text.
..
Induction
of
Rule
sets
A
h
yp
othesis
can
b
e
represen
ted
b
y
a
set
of
rules,
where
eac
h
rule
is
in
the
follo
wing
format:
IF
c
ondition
AND
c
ondition
AND
.
.
.
THEN
c
onclusion
Most
predictiv
e
systems
learn
rules
where
the
conclusion
of
the
rule
indic-
ates
the
class.
An
example
of
what
a
t
ypical
rule
set
w
ould
lo
ok
lik
e
for
the
F
ruit&V
egetables
example
is
giv
en
in
Figure
..
Both
attribute-v
alue
rules
and
a
Prolog
program
are
giv
en.
Indeed,
Prolog
programs
are
basically
rule
sets,
and
since
practically
all
ILP
systems
learn
logic
programs
or
Prolog
pro-
grams,
most
of
them
(though
not
all,
see
e.g.
(Bostr
om,
		))
follo
w
the
co
v
ering
approac
h.
Sometimes
the
rules
in
a
rule
set
are
supp
osed
to
b
e
ordered:
a
rule
is
only
applicable
when
none
of
the
preceding
rules
are
applicable.
In
that
case
the
rule
set
is
called
a
de
cision
list.
Decision
lists
ha
v
e
the
adv
an
tage
that
it
is
more
easy
to
dene
a
concept
that
has
exceptions:
the
rule
describing
the
exceptions
is
then
written
b
efore
the
more
general
rule.
When
no
ordering
of

..
TREES
VERSUS
R
ULES

A
rule
set
in
the
attribute-v
alue
framew
ork:
IF
Weight
<
0g
THEN
Class
=
strawberry
IF
Color
=
red
AND
Weight
>=
0g
THEN
Class
=
tomato
IF
Color
=
green
THEN
Class
=
apple
The
same
rule
set
written
as
a
Prolog
program:
strawberry(X)
:-
weight(X,
W),
W<0.
apple(X)
:-
color(X,
green).
tomato(X)
:-
color(X,
red),
weight(X,
W),
W>=0.
Figure
.:
Examples
of
rule
sets
in
the
F
ruit&V
egetables
example.
rules
is
presen
t,
the
general
rule
often
has
to
b
e
made
more
complex
to
exclude
the
exceptions.
Decision
lists
can
b
e
represen
ted
easily
in
Prolog
b
y
putting
a
cut
at
the
end
of
eac
h
program
clause.
The
seman
tics
of
prop
ositional
rule
sets
(whether
they
form
a
decision
list
or
just
a
set
of
unordered
rules)
are
usually
dened
b
y
the
system
that
induces
them.
(One
could
c
hange
the
IF-THEN
format
to
an
IF-THEN-ELSE
format
to
mak
e
the
ordering
of
the
rules
more
explicit.)
Most
algorithms
for
the
induction
of
rule
sets
are
v
arian
ts
of
the
algorithm
sho
wn
in
Figure
..
The
task
is
to
learn
rules
of
the
form
IF
c
onditions
THEN
p
ositive,
i.e.
the
rule
set
as
a
whole
should
mak
e
a
p
ositiv
e
prediction
for
those
instances
that
are
indeed
p
ositiv
e.
If
all
the
conditions
in
a
rule
are
true
for
a
sp
ecic
example,
w
e
sa
y
that
the
rule
c
overs
the
example.
Basically
,
the
approac
h
is
that
one
rule
at
a
time
is
learned.
Rules
are
preferred
that
co
v
er
as
man
y
p
ositiv
e
examples
as
p
ossible,
and
as
few
negativ
es
as
p
ossible
(since
these
t
w
o
criteria
ma
y
conict,
w
eigh
ts
ha
v
e
to
b
e
assigned
to
b
oth
according
to
their
imp
ortance;
the
exact
w
eigh
ts
ma
y
v
ary
in
dieren
t
instan
tiations
of
the
algorithm).
Eac
h
new
rule
should
at
least
co
v
er
some
previously
unco
v
ered
instances.
This
is
con
tin
ued
un
til
no
more
unco
v
ered
p
ositiv
e
instances
exist,
or
un
til
no
additional
rules
can
b
e
found.
The
v
arian
t
sho
wn
in
Figure
.
gro
ws
individual
rules
top-do
wn:

it
al-
w
a
ys
starts
with
a
general
rule
that
co
v
ers
all
examples
and
k
eeps
adding
conditions
un
til
the
rule
do
es
not
co
v
er
an
y
negativ
e
examples
(i.e.
not
co
v-
ering
negativ
e
examples
gets
an
innitely
high
w
eigh
t
here).
The
pro
cedure
OPTIMAL
CONDITION
selects
a
condition
that
remo
v
es
as
man
y
negativ
e
examples
from
the
co
v
ering
of
R
and
k
eeps
as
man
y
p
ositiv
e
examples
in
it
as
p
ossible.
This
pro
cedure
uses
a
heuristic
whic
h
ma
y
v
ary
along
dieren
t

Bottom-up
approac
hes
to
rule
gro
wing
also
exist.


CHAPTER
.
TOP-DO
WN
INDUCTION
OF
DECISION
TREES
pro
cedure
induce
r
uleset(E
+
,
E
 :
set
of
examples)
returns
set
of
rules:
H
:=
;
U
:=
E
+
while
U
=
;
do
R
:=
gr
o
w
r
ule(U;
E
 )
H
:=
H
[
fR
g
U
:=
U
 feje
is
co
v
ered
b
y
R
g
return
H
pro
cedure
gr
o
w
r
ule(E
+
;
E
 :
set
of
examples)
returns
rule:
R
:=
IF
true
THEN
Class=pos
while
R
co
v
ers
some
negativ
e
examples
T
:=
GENERA
TE
TESTS(E
+
,
E
 )
C
:=
OPTIMAL
CONDITION(R
,
T
,
E
+
,
E
 )
add
C
to
the
condition
part
of
R
return
R
Figure
.:
The
co
v
ering
algorithm,
also
kno
wn
as
separate-and-conquer.
implemen
tations.
The
approac
h
of
learning
rules
one
at
a
time
is
usually
referred
to
as
the
sep
ar
ate-and-c
onquer
approac
h,
or
as
the
c
overing
appr
o
ach.
The
latter
name
comes
from
the
fact
that
it
fo
cuses
on
unco
v
ered
examples.
Once
a
set
of
p
ositiv
e
examples
has
b
een
co
v
ered
b
y
a
rule,
it
is
remo
v
ed
from
the
set
on
whic
h
the
algorithm
fo
cuses.
The
co
v
ering
algorithm
as
giv
en
here
only
induces
unordered
rule
sets.
In-
duction
of
decision
lists
tak
es
a
more
complicated
approac
h.
First
of
all,
rules
predicting
dieren
t
classes
m
ust
b
e
learned
in
an
in
terlea
v
ed
fashion,
b
ecause
for
a
set
of
rules
predicting
the
same
class
the
order
of
the
rules
can
nev
er
b
e
imp
ortan
t.
A
dieren
t
algorithm
is
follo
w
ed
according
to
whether
rules
are
learned
from
rst
to
last
in
the
list,
or
in
the
opp
osite
direction.
In
the
latter
case,
the
strong
bias
to
w
ards
k
eeping
negativ
e
examples
unco
v
ered
should
b
e
remo
v
ed,
b
ecause
when
a
rule
co
v
ers
negativ
es
this
ma
y
b
e
tak
en
care
of
b
y
new
rules
that
are
still
to
b
e
found.
Examples
of
rule-based
systems
are
CN
(Clark
and
Niblett,
		),
the
A
Q
series
of
programs
(Mic
halski
et
al.,
	)
and
almost
all
ILP
systems
:
Pr
o-
gol(Muggleton,
		),
F
oil(Quinlan,
		b),
ICL(De
Raedt
and
V
an
Laer,
		),
.
.
.
The
ILP
systems
F
oidl(Mo
oney
and
Cali,
		)
and
FF
oil(Quinlan,
		)
learn
rst
order
decision
lists.

..
TREES
VERSUS
R
ULES
	
..
A
Comparison
Bet
w
een
T
rees
and
Rules
Comparing
the
tree
in
Figure
.
with
the
rule
sets
in
Figure
.
rev
eals
a
strong
corresp
ondence:
they
ha
v
e
a
similar
structure
and
use
the
same
tests.
Not
surprisingly
,
in
the
attribute
v
alue
framew
ork
trees
can
b
e
alw
a
ys
b
e
con
v
erted
to
rule
sets
using
a
v
ery
simple
algorithm
that
is
describ
ed
in
e.g.
(Quinlan,
		a)

.
Basically
,
for
eac
h
leaf
one
just
collects
the
tests
on
the
path
from
the
ro
ot
to
the
leaf
and
constructs
a
rule
from
these.
F
or
the
decision
tree
in
Figure
.
this
yields
the
follo
wing
rule
set:
IF
Weight
<
0g
THEN
Class
=
strawberry
IF
Weight
>=
0g
AND
Color
=
red
THEN
Class
=
tomato
IF
Weight
>=
0g
AND
Color
=
green
THEN
Class
=
apple
T
ypically
a
rule
set
that
is
deriv
ed
in
this
straigh
tforw
ard
manner
from
a
tree
con
tains
redundan
t
tests.
Quinlan
(		a)
sho
ws
ho
w
these
can
b
e
eliminated
during
a
p
ost-pro
cessing
phase.
The
ab
o
v
e
rule
set
is
then
reduced
to
the
one
in
Figure
..
Giv
en
this
strong
corresp
ondence
b
et
w
een
decision
trees
and
rule
sets,
one
migh
t
question
whether
there
are
an
y
imp
ortan
t
dierences
at
all
b
et
w
een
the
represen
tation
formalisms.
These
do
exist.
W
e
divide
them
in
to
t
w
o
groups:
dierences
with
resp
ect
to
the
represen
tation
itself,
and
dierences
with
resp
ect
to
the
induction
pro
cess.
Dierences
with
resp
ect
to
the
represen
tation
A
rst
dierence
is
understandabilit
y.
A
rule
set
is
generally
considered
to
b
e
easier
to
understand
than
a
decision
tree.
The
h
yp
othesis
is
structured
in
a
mo
dular
w
a
y:
eac
h
rule
represen
ts
a
piece
of
kno
wledge
that
can
b
e
understo
o
d
in
isolation
from
the
others.
(Although
this
is
m
uc
h
less
the
case
for
decision
lists.)
A
second
dierence
originates
in
the
concept
learning
p
oin
t
of
view,
and
concerns
the
represen
tation
of
the
concept
that
is
to
b
e
learned.
F
rom
the
concept
learning
p
oin
t
of
view,
the
concept
that
one
w
an
ts
to
learn
can
b
e
either
the
function
mapping
examples
on
to
their
lab
els,
or
a
sp
ecic
class
that
is
to
b
e
predicted.
E.g.
for
the
P
ok
er
data
set
(see
App
endix
A),
one
could
try
to
learn
the
concept
of
a
pair
(whic
h
is
one
of
the
classes)
or
consider
the
asso
ciation
of
hands
with
names
to
b
e
the
concept.
The
rst
one
is
probably
the
most
natural
here.
In
the
nite
elemen
t
mesh
data
set
(see
App
endix
A),
where
the
aim
is
to
predict
in
to
ho
w
man
y
pieces
an
edge
m
ust
b
e
divided,
it
mak
es
more
sense
to
sa
y
that
the
concept
to
b
e
learned
is
that
of
a
go
o
d
mesh
(i.e.
predicting
a
go
o
d
n
um
b
er
of
eac
h
edge)
than
to
sa
y
that
one
w
an
ts
to

More
surprisingly
,
this
algorithm
do
es
not
w
ork
in
the
rst
order
logic
framew
ork,
as
w
e
will
sho
w
in
Chapter
.

0
CHAPTER
.
TOP-DO
WN
INDUCTION
OF
DECISION
TREES
learn
the
concept
of
\an
edge
that
should
b
e
divided
in
to
four
parts",
among
other
concepts.
F
or
the
task
of
learning
a
denition
for
one
class,
rules
are
more
suitable
than
trees
b
ecause
t
ypical
rule
based
approac
hes
do
exactly
that:
they
learn
a
set
of
rules
for
this
sp
ecic
class.
In
a
tree,
one
w
ould
ha
v
e
to
collect
the
lea
v
es
con
taining
this
sp
ecic
class,
and
turn
these
in
to
rules.
On
the
other
hand,
if
denitions
for
m
ultiple
classes
are
to
b
e
learned,
the
rule
based
learner
m
ust
b
e
run
for
eac
h
class
separately
.
F
or
eac
h
individual
class
a
separate
rule
set
is
obtained,
and
these
sets
ma
y
b
e
inconsisten
t
(a
particular
instance
migh
t
b
e
assigned
m
ultiple
classes)
or
incomplete
(no
class
migh
t
b
e
assigned
to
a
particular
instance).
These
problems
can
b
e
solv
ed,
see
e.g.
(V
an
Laer
et
al.,
		),
but
with
a
tree
based
approac
h
they
simply
do
not
o
ccur:
one
just
learns
one
h
yp
othesis
that
denes
all
the
classes
at
once.
W
e
could
summarize
this
b
y
sa
ying
that
rules
are
more
t
when
a
single
concept
is
learned,
and
this
concept
corresp
onds
to
one
single
class.
When
m
ultiple
concepts
are
learned,
or
the
concept
corresp
onds
to
the
mapping
of
instances
on
to
classes
(or
n
umeric
v
alues),
a
tree
based
approac
h
is
more
suit-
able.
A
third
dierence
is
the
w
a
y
in
whic
h
exceptions
can
b
e
handled.
When
a
concept
is
easiest
to
describ
e
with
general
rules
that
ha
v
e
some
exceptions
(indicated
b
y
more
sp
ecic
rules),
tree
based
approac
hes
can
usually
handle
it
more
easily
than
rule
based
approac
hes
(except
when
decision
lists
are
induced).
The
co
v
ering
algorithm
easily
nds
concepts
that
can
b
e
describ
ed
as
the
union
of
sub-concepts,
where
eac
h
sub-concept
can
b
e
describ
ed
b
y
a
relativ
ely
simple
rule;
but
sometimes
a
concept
can
easily
b
e
describ
ed
as
the
dierence
of
t
w
o
concepts
C

=
C

 C

,
but
m
uc
h
less
easily
as
a
union
of
other
sub-concepts
C
i
(these
C
i
ma
y
ha
v
e
complicated
descriptions
whic
h
mak
es
them
harder
to
nd).
Figure
.
illustrates
this
with
a
simple
example:
a
tree
is
giv
en
together
with
an
equiv
alen
t
rule
set.
The
class
p
ositive
has
a
complicated
denition
when
w
e
dene
it
as
a
rule
set,
but
a
m
uc
h
simpler
denition
when
written
in
tree
format
or
as
a
decision
list.
Dierences
with
resp
ect
to
the
induction
pro
cess
The
divide-and-conquer
approac
h
is
usually
more
ecien
t
than
the
separate-
and-conquer
approac
h.
F
or
separate-and-conquer,
the
size
of
the
set
of
ex-
amples
that
is
used
to
gro
w
a
new
rule
can
b
e
exp
ected
to
decrease
more
or
less
linearly
(actually
only
the
n
um
b
er
of
unco
v
ered
p
ositiv
e
examples
de-
creases;
for
eac
h
rule
all
the
negativ
e
examples
are
tak
en
in
to
accoun
t).
With
a
divide-and-conquer
approac
h,
if
the
splits
are
balanced,
the
size
of
the
sets
of
examples
decreases
exp
onen
tially
when
one
go
es
do
wn
the
tree.
W
e
refer
to
(Bostr
om,
		)
for
more
details.

..
TREES
VERSUS
R
ULES

Decision
tree:
A
B
C
D
+
-
-
1
1
2
1
2
1
2
E
1
F
1
-
+
+
-
2
2
2
Rule
set:
IF
A=
AND
B=
AND
C=
AND
D=
THEN
Class
=
+
IF
A=
AND
B=
AND
C=
AND
E=
THEN
Class
=
+
IF
A=
AND
B=
AND
C=
AND
F=
THEN
Class
=
+
Decision
list:
IF
D=
AND
E=
AND
F=
THEN
Class
=
-
ELSE
IF
A=
AND
B=
AND
C=
THEN
Class
=
+
ELSE
Class
=
-
Figure
.:
A
simple
tree,
together
with
an
equiv
alen
t
rule
set.
Although
the
rule
set
is
m
uc
h
more
complex
than
the
tree,
it
cannot
b
e
simplied
further.
A
decision
list,
ho
w
ev
er,
do
es
allo
w
for
a
more
compact
represen
tation.


CHAPTER
.
TOP-DO
WN
INDUCTION
OF
DECISION
TREES
format
theory
#
literals
(A
=

^
B
=

^
C
=

^
D
=
)_
+
,
(A
=

^
B
=

^
C
=

^
E
=
)_

DNF
(A
=

^
B
=

^
C
=

^
F
=
)
 ,
A
=

_
B
=

_
C
=

_
(D
=

^
E
=

^
F
=
)

+
,
A
=

^
B
=

^
C
=

^
(D
=

^
E
=

^
F
=
)

CNF
(A
=

_
B
=

_
C
=

_
D
=
)^
 ,
(A
=

_
B
=

_
C
=

_
E
=
)^

(A
=

_
B
=

_
C
=

_
F
=
)
T
able
.:
CNF
and
DNF
denitions
for
the
+
and
 classes
from
Figure
..
A
second
dierence
is
that
divide-and-conquer
is
a
more
symmetric
ap-
proac
h
than
separate-and-conquer.
The
latter
strategy
fo
cuses
on
one
single
class
(p
ositiv
e)
and
only
tries
to
nd
sucien
t
conditions
for
this
class.
It
do
es
not
try
to
nd
sucien
t
conditions
for
the
other
class
or
classes.
This
causes
the
approac
h
to
b
e
v
ery
dep
enden
t
on
a
c
hoice
that
the
user
has
to
mak
e,
namely
for
whic
h
class
one
w
an
ts
to
nd
sucien
t
conditions.
If
w
e
lo
ok
again
at
Figure
.,
w
e
see
that
if
the
user
had
decided
to
learn
rules
for
 instead
of
+
(i.e.
calling
class
 p
ositive),
a
rule
set
w
ould
ha
v
e
b
een
pro
duced
that
con
tains
only

conditions,
just
lik
e
the
decision
list:
IF
A=
THEN
Class
=
-
IF
B=
THEN
Class
=
-
IF
C=
THEN
Class
=
-
IF
D=
AND
E=
AND
F=
THEN
Class
=
-
The
reason
for
this
dep
endency
is
that
some
concepts
are
more
easily
de-
scrib
ed
in
disjunctiv
e
normal
form
(DNF),
i.e.
as
a
disjunction
of
conjunctions
(eac
h
conjunction
th
us
represen
ts
a
sucien
t
condition);
others
are
more
easily
describ
ed
in
conjunctiv
e
normal
form
(CNF),
i.e.
as
a
conjunction
of
disjunc-
tions
(eac
h
disjunction
th
us
represen
ts
a
necessary
condition);
and
sometimes
a
mix
of
the
t
w
o
is
ev
en
b
etter.
Whenev
er
a
concept
is
easier
to
describ
e
in
DNF,
its
complemen
t
is
easier
to
describ
e
in
CNF
(see
(De
Raedt
et
al.,
		)
for
details).
T
able
.
illustrates
this
b
y
comparing
CNF
and
DNF
denitions
for
+
and
 .
Most
rule-based
systems
construct
DNF
descriptions.
Hence,
they
are
go
o
d
at
learning
classes
that
ha
v
e
a
simple
DNF
description,
but
less
go
o
d
at
learning
classes
with
a
simple
CNF
but
complex
DNF
description.
The
fact
that
some
concepts
are
easier
to
describ
e
using
CNF
than
using
DNF
has
also
b
een
noted
b
y
Mo
oney
(		),
who
prop
oses
an
alternativ
e
learn-
ing
approac
h
that
yields
CNF
descriptions.
Also
the
ICL
system
(De
Raedt

..
RELA
TED
W
ORK

and
V
an
Laer,
		)
originally
returned
CNF
descriptions
of
concepts,
although
later
v
ersions
of
it
oer
the
user
a
c
hoice
b
et
w
een
CNF
and
DNF
descriptions.
It
is
notew
orth
y
that
the
divide-and-conquer
approac
h
is
not
sub
ject
to
whether
a
concept
is
easier
to
describ
e
in
CNF
or
in
DNF.
W
e
already
sa
w
that
decision
trees
can
alw
a
ys
b
e
con
v
erted
to
rule
sets
(i.e.
DNF
descriptions).
In
a
similar
fashion,
they
can
b
e
con
v
erted
to
CNF
descriptions.
It
is
p
ossible,
then,
that
a
simple
tree
yields
a
simple
DNF
description
of
a
certain
class
but
a
complex
CNF
description
of
it;
that
it
yields
a
simple
CNF
but
complex
DNF
description
of
the
class;
or
that
it
do
es
not
yield
an
y
simple
description
at
all
in
an
y
of
these
normal
forms,
b
ecause
the
simplest
description
w
ould
mak
e
use
of
a
mix
of
b
oth
CNF
and
DNF.
Conclusions
Our
comparison
suggests
that
the
dierences
b
et
w
een
induction
of
decision
trees
and
induction
of
rule
sets
should
not
b
e
underestimated;
b
oth
induction
metho
ds
are
biased
dieren
tly
and
ma
y
yield
v
ery
dieren
t
theories.
W
e
will
see
later
that
in
inductiv
e
logic
programming
almost
all
systems
are
rule
based.
The
dierence
b
et
w
een
trees
and
rules
is
a
motiv
ation
for
in
tro
ducing
tree
based
metho
ds
in
ILP
.
.
Related
w
ork
This
c
hapter
is
mainly
based
on
(Blo
c
k
eel
et
al.,
		b).
This
w
ork
w
as
in-
uenced
strongly
b
y
P
at
Langley's
discussion
of
decision
trees
in
his
b
o
ok
Elements
of
Machine
L
e
arning
(Langley,
		).
It
is
this
discussion
that
p
oin-
ted
us
to
the
viewp
oin
t
that
decision
trees
describ
e
clustering
hierarc
hies,
al-
though
w
e
ha
v
e
since
learned
that
sev
eral
other
authors
ha
v
e
tak
en
this
view
either
explicitly
(e.g.
Fisher
(	;
		),
who
men
tions
the
p
ossibilit
y
of
us-
ing
TDIDT-tec
hniques
for
clustering)
or
implicitly
(e.g.
Kramer
(		),
who
adopts
suc
h
a
view
for
his
outlier
detection
metho
d).
There
are
man
y
texts
on
decision
tree
induction
in
the
con
text
of
classic-
ation
or
regression;
probably
the
most
inuen
tial
ones
in
the
mac
hine
learning
comm
unit
y
are
(Quinlan,
	)
and
(Breiman
et
al.,
	).
.
Conclusions
In
this
c
hapter
w
e
ha
v
e
tak
en
a
lo
ok
at
top-do
wn
induction
of
decision
trees
(TDIDT)
from
the
viewp
oin
t
of
predictiv
e
clustering.
W
e
ha
v
e
describ
ed
ho
w
instan
tiations
of
this
general
approac
h
yield
man
y
existing
approac
hes
to
TDIDT.
Induction
of
decision
trees
can
th
us
b
e
seen
as
a
promising
metho
d
for
implemen
ting
predictiv
e
induction,
as
dened
in
the
previous
c
hapter,
with


CHAPTER
.
TOP-DO
WN
INDUCTION
OF
DECISION
TREES
all
the
p
ossibilities
that
it
creates
(suc
h
as
exible
prediction,
noise
handling,
h
ybrid
approac
hes,
.
.
.
)
W
e
ha
v
e
also
compared
trees
with
rules,
mainly
fo-
cusing
on
the
dierences
b
et
w
een
them,
and
arguing
that
these
dierences
are
large
enough
to
justify
the
co-existence
of
b
oth
formalisms.

Chapter

First
Order
Logic
Represen
tations
.
In
tro
duction
In
the
con
text
of
sym
b
olic
induction,
t
w
o
paradigms
for
represen
ting
kno
wledge
can
b
e
distinguished:
attribute
v
alue
represen
tations
and
rst
order
logic
rep-
resen
tations.
The
latter
are
used
in
the
eld
of
inductiv
e
logic
programming.
Represen
tation
of
kno
wledge
in
rst
order
logic
can
itself
b
e
done
in
dieren
t
w
a
ys.
W
e
distinguish
t
w
o
main
directions
in
ILP:
the
represen
tation
of
the
kno
wledge
base
(data
and
bac
kground
kno
wledge)
as
one
single
logic
program,
where
eac
h
example
is
represen
ted
b
y
a
single
clause
(usually
a
ground
fact);
and
the
represen
tation
of
examples
as
sets
of
facts
or
in
terpretations.
These
t
w
o
dieren
t
paradigms
within
ILP
are
kno
wn
as
le
arning
fr
om
entailment
and
le
arning
fr
om
interpr
etations.
These
are
not
the
only
p
ossible
settings;
an
o
v
er-
view
of
dieren
t
settings
and
the
relationships
among
them
can
b
e
found
in
(De
Raedt,
		).
In
this
c
hapter
w
e
discuss
in
turn
the
three
dieren
t
formalisms
for
sym-
b
olic
induction
that
w
e
ha
v
e
men
tioned:
attribute
v
alue
learning,
learning
from
in
terpretations
and
learning
from
en
tailmen
t.
W
e
will
use
a
running
example
to
illustrate
and
compare
these
formalisms.
The
task
that
is
considered
in
the
example
is
to
learn
denitions
of
concepts
that
are
used
in
the
p
ok
er
card
game
(i.e.
learn
what
a
pair,
double
pair,
full
house.
.
.
is).
Section
.
briey
discusses
the
relationship
b
et
w
een
concept
learning
and
induction
of
predictors;
this
is
useful
for
comparing
the
task
denitions
for
the
dieren
t
settings.
Next,
w
e
in
tro
duce
attribute
v
alue
learning
in
Section
.
and
learning
from
in
terpretations
in
Section
..
The
t
w
o
are
compared
from
a
database
p
oin
t
of
view
in
Section
..
W
e
then
discuss
learning
from
en
tailmen
t



CHAPTER
.
FIRST
ORDER
LOGIC
REPRESENT
A
TIONS
(Section
.)
and
relate
it
to
the
other
settings
in
Section
..
The
last
t
w
o
sections
discuss
related
w
ork
and
conclude.
.
Concept
Learning
and
In
tensional
Cluster-
ing
Since
in
this
c
hapter
w
e
will
compare
dieren
t
represen
tation
formalisms,
it
is
useful
to
rst
dene
in
tensional
clustering
using
a
language
L
in
more
detail.
Denition
.
(In
tensional
clustering
space)
The
intensional
clustering
sp
ac
e
of
a
set
of
instanc
es
E
with
r
esp
e
ct
to
a
language
L,
written
C
L
(E
),
is
the
set
of
al
l
clusterings
over
E
for
which
e
ach
cluster
has
an
intensional
description
in
L.
Denition
.
(In
tensional
clustering
task)
We
dene
the
task
of
inten-
sional
clustering
in
a
language
L
as
fol
lows:
Giv
en:

an
instanc
e
sp
ac
e
I

a
distanc
e
d
on
I

a
set
of
instanc
es
E

I

a
language
L

a
quality
criterion
Q
dene
d
over
C
L
(E
)
Find:

a
clustering
C

C
L
such
that
Q(C
)
is
optimal,
i.e.
C
0

C
L
(E
)
:
Q(C
0
)

Q(C
)

an
intensional
description
in
L
for
e
ach
cluster
C

C
This
clustering
task
can
b
e
instan
tiated
to
w
ards
predictiv
e
clustering,
classi-
cation
and
regression,
similarly
to
what
w
as
done
in
Chapter
.
Often,
the
language
L
is
then
extended
sligh
tly
so
that
the
prediction
function
as
a
whole
can
b
e
describ
ed
in
L.
Example
.
W
e
return
to
the
F
ruit&V
egetables
example.
Assume
L
is
pro-
p
ositional
logic
o
v
er
a
set
of
prop
ositions
P
=
fColor
=
red
;
Color
=
green
g
[
fWeight
<
xgjx

I
R
g:

..
CONCEPT
LEARNING
AND
INTENSIONAL
CLUSTERING

An
in
tensional
clustering
algorithm
could
come
up
with
the
follo
wing
descrip-
tion
of
stra
wb
erries,
whic
h
is
in
L:
Weight
<
0g
^
Color
=
red.
F
or
predictiv
e
purp
oses,
a
sligh
tly
dieren
t
language
L
0
=
f
^
i
(F
i
!
Class
=v
i
)jv
i

fapple;
tomato;
st
raw
be
rry
g
^
F
i

Lg
allo
ws
to
describ
e
the
predictor
as
follo
ws:
Weight
<
0g
^
Color
=
red
!
Class
=
strawberry

Denition
.
(Concept
learning)
The
c
onc
ept
le
arning
task
c
an
in
gen-
er
al
b
e
state
d
as
fol
lows:
Giv
en:

an
instanc
e
sp
ac
e
I

a
set
of
p
ositive
examples
E
+

I

a
set
of
ne
gative
examples
E
 
I

and
a
language
L
Find:
an
intensional
description
in
L
of
E
+
Under
the
assumption
that
L
is
closed
with
resp
ect
to
disjunction
(i.e.
if
form
ulae
F


L
and
F


L,
then
F

_
F


L),
whic
h
will
b
e
the
case
for
all
the
languages
considered
in
this
text,
concept
learning
can
b
e
seen
as
a
sp
ecial
case
of
classication,
where
the
prediction
space
is
fpositive,
negativeg.
The
concept
description
is
the
disjunction
of
those
clusters
for
whic
h
the
v
alue
positive
is
predicted.
Example
.
In
the
previous
example,
the
concept
of
stra
wb
erries
corresp
on-
ded
to
one
single
cluster,
hence
the
in
tensional
description
of
that
cluster
is
also
an
in
tensional
description
of
the
concept.
A
concept
ma
y
also
consist
of
sev
eral
clusters,
as
this
example
sho
ws:
supp
ose
w
e
ha
v
e
a
classier
describ
ed
as
(X
=

^
Y
=
0
!
Class
=
positive
)
^
(X
=

^
Y
=

!
Class
=
negative
)^
(X
=
0
^
Y
=
0
!
Class
=
negative
)
^
(X
=
0
^
Y
=

!
Class
=
positive
)


CHAPTER
.
FIRST
ORDER
LOGIC
REPRESENT
A
TIONS
The
classier
predicts
positive
for
t
w
o
clusters,
hence
the
concept
description
is
the
disjunction
of
b
oth
cluster
descriptions:
X
=

^
Y
=
0
_
X
=
0
^
Y
=


This
sho
ws
that
in
tensional
clustering,
concept
learning
and
learning
pre-
dictors
are
all
related.
In
the
follo
wing
w
e
will
often
assume
a
predictiv
e
setting,
but
the
results
are
generally
applicable
for
clustering
and
concept
learning
as
w
ell.
.
A
ttribute
V
alue
Learning
In
the
attribute
v
alue
formalism,
it
is
assumed
that
eac
h
example
can
b
e
de-
scrib
ed
b
y
a
xed
set
of
attributes
for
whic
h
v
alues
are
giv
en.
Eac
h
example
is
th
us
represen
ted
b
y
a
v
ector
of
v
alues.
All
the
data
together
form
a
table
where
eac
h
ro
w
corresp
onds
to
an
example
and
eac
h
column
to
an
attribute.
The
h
yp
othesis
language
is
prop
ositional
logic,
where
prop
ositions
usually
are
of
the
form
\A
ttribute

v
alue"
with

an
elemen
t
of
a
predened
xed
set
of
op
erators,
e.g.
f<;
;
=g.
Denition
.
(A
ttribute
v
alue
learning)
The
task
of
le
arning
a
pr
e
dictor
in
the
attribute
value
fr
amework
c
an
b
e
dene
d
as
fol
lows:
Giv
en:

a
tar
get
variable
Y

a
set
of
lab
el
le
d
examples
E
;
e
ach
example
is
a
ve
ctor
lab
el
le
d
with
a
value
y
for
the
tar
get
variable

and
a
language
L
c
onsisting
of
pr
op
ositional
lo
gic
over
a
given
set
of
pr
op
ositions
P
Find:
a
hyp
othesis
H
of
the
form
V
i
(F
i
!
Y
=
y
i
)
wher
e
e
ach
F
i

L
such
that
for
e
ach
example
e
with
lab
el
y
,

H
j
=
Y
=
y

y
0
=
y
:
H
j
=
Y
=
y
0
The
attribute
v
alue
formalism
is
the
formalism
that
is
most
frequen
tly
used
for
data
mining
and
sym
b
olic
mac
hine
learning.
Most
systems
w
orking
in
this
framew
ork
do
not
allo
w
the
use
of
a
bac
kground
theory
ab
out
the
domain,
although
in
principle
this
w
ould
not
b
e
a
problem.

..
A
TTRIBUTE
V
ALUE
LEARNING
	
Rank
Suit
R.
S.
R.
S.
R.
S.
R.
S.
Class



~
K
|
Q
~

~
pair



|
A
|


Q
}
nough
t

~

|

|



~
full
house




A

A
|
	

pair
A
|
A
}



}

~
pair

~

~

~
K
|

}
nough
t
T
able
.:
Represen
ting
examples
for
learning
p
ok
er
concepts.
Eac
h
tuple
represen
ts
one
hand
of
v
e
cards
and
the
name
that
is
giv
en
to
the
hand.
T
able
.
sho
ws
ho
w
p
ok
er
hands
can
b
e
represen
ted
in
this
framew
ork.
Eac
h
ro
w
in
the
table
describ
es
one
example
(one
hand
of
cards)
b
y
listing
the
rank
and
suit
of
ev
ery
card
in
the
hand,
and
also
stating
its
class
lab
el.
While
this
ma
y
seem
a
v
ery
natural
represen
tation
of
p
ok
er
hands,
it
is
not
t
for
learning
denitions
of
the
classes.
A
go
o
d
denition
for
the
concept
of
a
pair
is
v
ery
hard
to
write
if
this
format
is
used:
one
needs
to
c
hec
k
that
exactly
t
w
o
cards
ha
v
e
the
same
rank.
A
p
ossible
denition
of
the
concept
of
a
pair
is
the
follo
wing:

(Rank=
^
Rank=
^
[Ranks
,,
dier
from

and
from
eac
h
other])
_
(Rank=
^
Rank=
^
[Ranks
,,
dier
from

and
from
eac
h
other])
_



_
(Rank=
^
Rank=
^
[Ranks
,,
dier
from

and
from
eac
h
other])
_



!
Class
=
pair
Note
that
the
cards
with
equal
ranks
could
b
e
an
y

of
the

cards,
whic
h
yields
C


=
0
com
binations.
Moreo
v
er,
one
cannot
test
the
equalit
y
of
t
w
o
attributes
directly
,
they
can
only
b
e
compared
with
constan
ts;
therefore
it
is
necessary
to
test
for
the

dieren
t
constan
ts
whether
the
ranks
of

sp
ecic
cards
are
equal
to
them.
This
means
that
the
h
yp
othesis
as
written
ab
o
v
e
con
tains
0
lines.
Note
that
eac
h
line
in
itself
is
far
from
complete:
eac
h
part
b
et
w
een
brac
k
ets
is
to
b
e
expanded,
for
instance
in
to
a
disjunction
of

conjunctions
similar
to
Rank
=

^
Rank
=

^
Rank
=

^
Rank
=

^
Rank
=

The
full
denition
w
ould
then
con
tain
a
total
of
0
suc
h
conjunctions.
While
the
ab
o
v
e
format
of
the
h
yp
othesis
is
not
necessarily
the
simplest
one,
an
y
correct
h
yp
othesis
will
ha
v
e
a
complexit
y
that
is
of
the
same
order

The
denition
is
written
in
the
form
of
a
predictor,
conform
to
the
Denition
.;
the
actual
concept
description
is
the
an
teceden
t
of
the
rule.

0
CHAPTER
.
FIRST
ORDER
LOGIC
REPRESENT
A
TIONS
Er
Er
Er
Er
Er
Er
Er
Er
Er
Er
Class
no
no
no
y
es
no
no
no
no
no
no
pair
no
no
no
no
no
no
no
no
no
no
nough
t
y
es
no
no
no
no
no
no
y
es
y
es
y
es
full
house
no
no
no
no
no
no
no
y
es
no
no
pair
y
es
no
no
no
no
no
no
no
no
no
pair
no
no
no
no
no
no
no
no
no
no
nough
t
T
able
.:
Constructed
attributes
for
learning
p
ok
er
concepts.
The
meaning
of,
e.g.,
Er
is
that
cards

and

ha
v
e
equal
rank.
of
magnitude.
The
problem
is
that
in
the
attribute
v
alue
framew
ork
the
data
simply
cannot
b
e
handled
in
the
format
of
T
able
..
A
b
etter
w
a
y
to
tac
kle
the
problem
is
to
construct
new
attributes
and
add
them
to
the
table.
Observing
that
equalit
y
of
dieren
t
attributes
(in
this
case:
equalit
y
of
the
ranks
of
dieren
t
cards)
ma
y
b
e
in
teresting
to
c
hec
k,
w
e
can
explicitly
add
information
ab
out
suc
h
equalities
to
the
table.
This
yields
T
able
..
There
are
0
attributes
Er
ij
in
this
table;
Er
ij
=yes
if
the
ranks
of
the
cards
i
and
j
are
equal.
One
could
no
w
learn
from
the
original
data
in
T
able
.
augmen
ted
with
the
constructed
attributes
in
T
able
.,
or
use
the
constructed
attributes
only
.
The
latter
is
sucien
t
for
learning
the
concept
of
a
pair,
but
not
for,
e.g.,
learning
a
ush
or
straigh
t.
The
denition
of
a
pair
can
no
w
b
e
written
in
a
m
uc
h
simpler
w
a
y:
(Er
=
yes
^
Er
=
no
^


)
_
(Er
=
no
^
Er
=
yes
^


)
_



_
(Er
=
no
^



^
Er
=
yes)
!
Class
=
pair
This
denition
con
tains
0
conjuncts,
and
eac
h
conjunct
consists
of
0
terms.
It
basically
sa
ys
that
a
hand
is
a
pair
if
exactly
one
of
the
ten
Er-
attributes
is
a
yes.
This
mak
es
clear
that
b
y
constructing
one
more
attribute,
one
that
indicates
ho
w
man
y
yes
v
alues
there
are
in
a
tuple,
a
v
ery
concise
denition
can
b
e
obtained:
Pairs
with
equal
rank
=

!
Class
=
pair.
This
example
illustrates
a
n
um
b
er
of
imp
ortan
t
asp
ects
of
attribute
v
alue
learning:

Learning
from
the
most
natural
represen
tation
of
the
examples
is
not
alw
a
ys
feasible.
It
ma
y
b
e
necessary
to
add
attributes
to
the
data.
These
new
attributes
are
supp
osed
to
capture
prop
erties
of
the
example
that
are
exp
ected
to
b
e
imp
ortan
t
for
the
concept
that
is
to
b
e
learned.
Note
that
the
new
attributes
do
not
add
new
information
ab
out
the
example:

..
LEARNING
FR
OM
INTERPRET
A
TIONS

they
can
b
e
computed
from
the
other
attributes.
They
only
explicitate
information.

The
complexit
y
of
a
h
yp
othesis
dep
ends
on
the
attributes
that
are
a
v
ail-
able.
A
h
yp
othesis
can
seem
deceptiv
ely
simple
if
it
mak
es
use
of
attrib-
utes
that
are
v
ery
complex
to
compute.

It
ma
y
not
b
e
ob
vious
to
nd
go
o
d
attributes.
In
the
ab
o
v
e
example
w
e
could
easily
come
up
with
constructed
attributes
b
ecause
w
e
already
knew
what
the
correct
denition
of
a
pair
is.
In
general
the
task
of
constructing
go
o
d
attributes
is
m
uc
h
harder.
Note
that
constructing
go
o
d
attributes
is
as
m
uc
h
a
part
of
the
kno
wledge
disco
v
ery
pro
cess
as
the
searc
h
for
a
go
o
d
h
yp
othesis
itself
is.
The
dep
endency
of
attribute
v
alue
learners
on
the
represen
tation
can
b
e
solv
ed
in
sev
eral
w
a
ys.
One
is
to
ha
v
e
an
inductiv
e
learner
construct
go
o
d
attributes
itself.
This
task
is
kno
wn
as
fe
atur
e
c
onstruction
and
has
b
een
the
sub
ject
of
man
y
studies,
see
e.g.
(Wnek
and
Mic
halski,
		;
Blo
edorn
and
Mic
halski,
		;
Sriniv
asan
and
King,
		;
Kramer
et
al.,
		).
Another
approac
h
to
alleviate
the
problem
is
enlarging
the
h
yp
othesis
space
b
y
allo
wing
tests
that
in
v
olv
e
m
ultiple
attributes.
F
or
instance,
one
could
ha
v
e
a
test
suc
h
as
Rank=Rank.
This
approac
h
is
tak
en
b
y
Bergadano
et
al.
(		)
with
the
Relic
system.
.
Learning
from
In
terpretations
In
the
learning
from
in
terpretations
setting
eac
h
example
e
is
represen
ted
b
y
a
separate
Prolog
program
that
enco
des
its
sp
ecic
prop
erties,
as
w
ell
as
its
lab
el.
Bac
kground
kno
wledge
ab
out
the
domain
can
b
e
giv
en
in
the
form
of
a
Prolog
program
B
.
The
in
terpretation
that
represen
ts
the
example
is
the
set
of
all
the
ground
facts
that
are
en
tailed
b
y
e
^
B
(this
set
is
called
the
minimal
Herbr
and
mo
del
of
e
^
B
).
The
h
yp
othesis
to
b
e
induced
is
a
rst
order
theory
,
represen
ted
as
a
Prolog
program.
Example
.
Supp
ose
w
e
ha
v
e
an
example
called
Tw
eet
y
,
represen
ted
b
y
the
follo
wing
Prolog
program:
bird.
color(yellow).
likes(bird_seed)
.
gets(bird_seed).
and
a
bac
kground
theory


CHAPTER
.
FIRST
ORDER
LOGIC
REPRESENT
A
TIONS
flies
:-
bird.
swims
:-
fish.
happy
:-
likes(X),
gets(X).
then
the
in
terpretation
represen
ting
Tw
eet
y
is
fbird,
color(yellow),
likes(bird
seed),
gets(bird
seed),
flies,
happyg.
It
sums
up
all
the
prop
erties
of
Tw
eet
y
that
can
b
e
deriv
ed
from
its
description
together
with
the
bac
kground
kno
wledge.

The
essen
tial
dierence
with
attribute
v
alue
learning
is
that
a
set
is
used
to
represen
t
an
example,
instead
of
a
(xed-length)
v
ector.
This
mak
es
the
represen
tation
m
uc
h
more
exible.
Denition
.
(Learning
from
in
terpretations)
L
e
arning
fr
om
interpr
et-
ations
is
usual
ly
set
in
a
pr
e
dictive
c
ontext,
wher
e
the
task
denition
is
the
fol
lowing:
Giv
en:

a
tar
get
variable
Y

a
set
of
lab
el
le
d
examples
E
;
e
ach
example
is
a
Pr
olo
g
pr
o
gr
am
e
lab
el
le
d
with
a
value
y
for
the
tar
get
variable

a
language
L

Pr
olo
g

and
a
b
ackgr
ound
the
ory
B
Find:
a
hyp
othesis
H

L
such
that
for
al
l
lab
el
le
d
examples
(e;
y
)

E
,

H
^
e
^
B
j
=
l
abel
(y
),
and

y
0
=
y
:
H
^
e
^
B
j
=
l
abel
(y
0
)
This
task
reduces
to
the
concept
learning
task
b
y
c
ho
osing
as
lab
els
positive
and
negative.
T
able
.
sho
ws
ho
w
the
hands
w
ould
b
e
represen
ted
in
the
learning
from
in
terpretations
setting.
Note
that
the
lab
el
need
not
b
e
indicated
explicitly
b
y
a
predicate
called
label;
in
this
example
the
n
ullary
predicates
pair,
full
house
etc.
represen
t
the
lab
els.
A
correct
denition
of
a
pair
in
rst
order
logic
is:
	R
ank
;
Suit

;
Suit

:
card
(R
ank
;
Suit

)
^
card
(R
ank
;
Suit

)
^
Suit

=
Suit

^

	Suit

:
(card
(R
ank
;
Suit

)
^
Suit

=
Suit

^
Suit

=
Suit

)
^

	R
ank

;
Suit

;
Suit

:
card
(R
ank

;
Suit

);
card
(R
ank

;
Suit

);
R
ank

=
R
ank
;
Suit

=
Suit

!
pair

..
LEARNING
FR
OM
INTERPRET
A
TIONS

{card(,
spades),
card(,
hearts),
card(king,
clubs),
card(queen,
hearts),
card(,
hearts),
pair}
{card(,
spades),
card(,
clubs),
card(ace,
clubs),
card(,
spades),
card(queen,
diamonds),
nought}
{card(,
hearts),
card(,
clubs),
card(,
clubs),
card(,
spades),
card(,
hearts),
fullhouse}
{card(,
spades),
card(,
spades),
card(ace,
spades),
card(ace,
clubs),
card(	,
spades),
pair}
{card(ace,
clubs),
card(ace,
diamonds),
card(,
spades),
card(,
diamonds),
card(,
hearts),
pair}
{card(,
hearts),
card(,
hearts),
card(,
hearts),
card(king,
clubs),
card(,
diamonds),
nought}
T
able
.:
Represen
ting
the
p
ok
er
data
in
the
learning
from
in
terpretations
setting.
The
rst
line
states
that
there
m
ust
b
e
t
w
o
dieren
t
cards
with
equal
ranks,
the
next
line
signies
that
there
m
ust
not
b
e
a
third
card
with
the
same
rank,
and
the
last
t
w
o
lines
of
the
an
teceden
t
add
the
constrain
t
that
there
m
ust
b
e
no
other
pair
of
cards
with
equal
ranks.
The
denition
can
also
b
e
written
as
a
Prolog
program,
whic
h
is
the
form
in
whic
h
most
ILP
systems
represen
t
the
result.
A
Prolog
program
equiv
alen
t
to
the
ab
o
v
e
denition
is
sho
wn
in
Figure
..
Note
that
this
denition,
although
relativ
ely
complex,
is
m
uc
h
simpler
than
the
rst
t
w
o
denitions
that
w
ere
giv
en
in
the
attribute
v
alue
learning
frame-
w
ork.
Still,
the
denition
do
es
not
mak
e
use
of
an
y
constructed
features
in
the
data,
whereas
the
second
prop
ositional
denition
did.
Of
course,
one
could
still
add
constructed
features
to
the
data,
e.g.
coun
ting
ho
w
man
y
com
binations
of
t
w
o
cards
with
the
same
rank
there
are.
This
could
b
e
done
b
y
adding
a
pairs
with
equal
ranks
fact
to
eac
h
example

;
for
in-
stance
the
rst
example
w
ould
b
e
extended
with
pairs
with
equal
ranks().
The
denition
then
b
ecomes
pairs
with
equal
ranks
()
!
pair
:

Alternativ
ely
,
the
predicate
can
b
e
dened
in
the
bac
kground
b
y
writing
a
Prolog
program
that
coun
ts
the
n
um
b
er
of
pairs
with
equal
ranks
in
a
hand.


CHAPTER
.
FIRST
ORDER
LOGIC
REPRESENT
A
TIONS
pair
:-
card(Rank,
Suit),
card(Rank,
Suit),
Suit
\=
Suit,
not
third_card(Rank
,
Suit,
Suit),
not
second_pair(Ran
k,
Suit,
Suit).
third_card(Rank,
Suit,
Suit)
:-
card(Rank,
Suit),
Suit
\=
Suit,
Suit
\=
Suit.
second_pair(Rank
,
Suit,
Suit)
:-
card(Rank,
Suit),
card(Rank,
Suit),
Rank
\=
Rank,
Suit
\=
Suit.
Figure
.:
A
correct
denition
of
a
pair
in
Prolog.
This
sho
ws
that
feature
construction
can
also
b
e
done
(at
least
in
principle)
in
ILP
,
and
that
there
as
w
ell
it
leads
to
more
compact
denitions.
The
need
for
feature
construction
is
smaller
in
ILP
than
in
attribute
v
alue
learning,
due
to
the
greater
expressivit
y
of
rst
order
logic,
but
it
still
exists.
F
eature
construction
can
b
e
seen
as
a
sp
ecial
case
of
pr
e
dic
ate
invention,
whic
h
is
a
dicult
and
extensiv
ely
studied
topic
in
ILP
(see,
e.g.,
(Stahl,
		)
for
an
o
v
erview).
.
A
Relational
Database
Viewp
oin
t
ILP
systems
use
logic-based
data
represen
tations,
but
in
practice,
data
are
of-
ten
stored
in
a
relational
database.
In
this
section,
w
e
discuss
the
dierence
b
et
w
een
attribute
v
alue
learning
and
rst
order
learning
from
a
relational
data-
base
p
oin
t
of
view.
This
oers
an
in
teresting
and
clear
view
of
the
relationship
b
et
w
een
the
t
w
o
settings,
namely
that
rst
order
learning
amoun
ts
to
learning
from
m
ultiple
relations
and
attribute
v
alue
learning
amoun
ts
to
learning
from
one
relation.
There
is
a
simple
w
a
y
to
con
v
ert
the
data
in
a
relational
database
to
the
learning
from
in
terpretations
format;
in
the
second
part
of
this
section
w
e
presen
t
an
algorithm.
..
A
ttribute
V
alue
Learning
V
ersus
Learning
from
Mul-
tiple
Relations
A
ttribute
v
alue
learning
alw
a
ys
in
v
olv
es
learning
from
one
single
relation,
and
more
sp
ecically:
learning
a
h
yp
othesis
that
relates
dieren
t
attributes
of
the

..
A
RELA
TIONAL
D
A
T
ABASE
VIEWPOINT

same
tuple.
W
e
could
call
this
single-relation,
single-tuple
learning.
In
the
rst
order
con
text,
one
can
learn
patterns
that
relate
attribute
v
alues
of
sev
eral
tuples,
p
ossibly
b
elonging
to
dieren
t
relations,
to
one
another.
Example
.
Assume
the
follo
wing
database
is
giv
en:
FINES
Name
Job
Sp
eed
Fine
Ann
teac
her
0
km/h
Y
Bob
p
olitician
0
km/h
N
Chris
engineer
0
km/h
N
Da
v
e
writer

km/h
N
Earnest
p
olitician
0
km/h
N
KNO
WS
Name
Name
Ann
Chris
Ann
Da
v
e
Bob
Earnest
Chris
Da
v
e
Da
v
e
Bob
An
attribute
v
alue
learner
can
learn
a
rule
suc
h
as
\if
y
ou
driv
e
faster
than
0
km/h
and
are
not
a
p
olitician,
y
ou
get
a
ne",
but
not
the
(more
correct)
rule
\if
y
ou
driv
e
faster
than
0
km/h,
are
not
a
p
olitician
and
do
not
kno
w
a
p
olitician,
then
y
ou
get
a
ne".
This
rule
relates
attributes
of
dieren
t
tuples
in
the
relation
FINES
that
are
link
ed
via
the
relation
KNO
WS.

It
is
kno
wn
from
relational
database
theory
,
see
e.g.
(Elmasri
and
Na
v
athe,
		),
that
storing
data
in
one
single
table
can
b
e
v
ery
inecien
t;
it
is
advisable
to
structure
a
database
in
to
sev
eral
relations
that
are
in
some
normal
form.
A
ttribute
v
alue
learning
is
limited
in
the
sense
that
in
order
to
tak
e
in
to
accoun
t
all
the
information
in
a
database,
one
has
to
con
v
ert
all
the
relations
in
to
a
single
relation.
The
resulting
relation
can
b
e
h
uge,
t
ypically
con
taining
a
large
amoun
t
of
redundan
t
information;
moreo
v
er
its
computation
is
costly
.
This
problem
is
ignored
in
man
y
approac
hes
to
data
mining.
In
his
in
vited
talk
at
the
Fifte
enth
International
Confer
enc
e
on
Machine
L
e
arning
(		),
Ron
Koha
vi
ac
kno
wledged
the
shortcomings
of
attribute
v
alue
learning
in
this
resp
ect
and
stressed
the
need
for
learning
tec
hniques
that
learn
from
m
ultiple
relations.
De
Raedt
(		)
sho
ws
that
the
con
v
ersion
of
m
ultiple
relations
to
a
single
relation,
together
with
the
problems
it
generates
for
learning
(as
w
e
ha
v
e
seen
with
the
P
ok
er
example,
correct
theories
ma
y
b
ecome
extremely
complex
and
hard
to
nd),
causes
attribute
v
alue
learning
to
b
ecome
m
uc
h
more
inecien
t
than
rst
order
learning
on
this
kind
of
problems.
The
follo
wing
is
an
example
of
a
database
where
learning
from
m
ultiple
relations
is
desirable,
and
where
joining
the
relations
in
to
one
single
relation
is
clearly
not
feasible.
Example
.
Assume
that
one
has
a
relational
database
describing
molecules.
The
molecules
themselv
es
are
describ
ed
b
y
listing
the
atoms
and
b
onds
that
o
ccur
in
them,
as
w
ell
as
some
prop
erties
of
the
molecule
as
a
whole.
Mendelev's


CHAPTER
.
FIRST
ORDER
LOGIC
REPRESENT
A
TIONS
MENDELEV
Num
b
er
Sym
b
ol
A
tomic
w
eigh
t
Electrons
in
outer
la
y
er
.
.
.

H
.00	


He
.00


Li
.	


Be
	.0


B
0.


C
.0

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
MOLECULES
CONT
AINS
F
orm
ula
Name
Class
H

O
w
ater
inorganic
C
O

carb
on
dio
xide
inorganic
C
O
carb
on
mono
xide
inorganic
C
H

methane
organic
C
H

O
H
methanol
organic
.
.
.
.
.
.
.
.
.
F
orm
ula
A
tom
id
H

O
ho-
H

O
ho-
H

O
ho-
C
O

co-
C
O

co-
.
.
.
.
.
.
A
TOMS
BONDS
A
tom
id
Elemen
t
ho-
H
ho-
O
ho-
H
co-
O
.
.
.
.
.
.
A
tom
id
A
tom
id
T
yp
e
ho-
ho-
single
ho-
ho-
single
co-
co-
double
co-
co-
double
.
.
.
.
.
.
.
.
.
Figure
.:
A
c
hemical
database.
p
erio
dic
table
of
elemen
ts
is
a
go
o
d
example
of
bac
kground
kno
wledge
ab
out
this
domain.
Figure
.
illustrates
what
suc
h
a
c
hemical
database
could
lo
ok
lik
e.
A
p
ossible
classication
problem
here
is
to
classify
unseen
molecules
in
to
organic
and
inorganic
molecules,
based
on
their
c
hemical
structure.

Note
that
the
database
con
tains
b
oth
information
ab
out
sp
ecic
molecules
(i.e.
sp
ecic
examples)
and
bac
kground
kno
wledge
(e.g.,
Mendelev's
p
erio
dic
table).
This
example
should
mak
e
clear
that
in
man
y
cases
joining
all
the
rela-
tions
in
to
one
single,
h
uge
relation
is
not
an
option.
The
information
in
Mendelev's
table,
for
instance,
w
ould
b
e
duplicated
man
y
times.
Moreo
v
er,

..
A
RELA
TIONAL
D
A
T
ABASE
VIEWPOINT

unless
a
m
ultiple-instance
learner
is
used
(Dietteric
h
et
al.,
		)
all
the
atoms
a
molecule
consists
of,
together
with
their
prop
erties,
ha
v
e
to
b
e
stored
in
one
tuple,
so
that
an
indenite
n
um
b
er
of
attributes
is
needed.
While
mining
suc
h
a
database
is
not
feasible
using
prop
ositional
tec
hniques,
it
is
feasible
using
learning
from
in
terpretations.
W
e
no
w
pro
ceed
to
sho
w
ho
w
a
relational
database
can
b
e
con
v
erted
in
to
a
suitable
format.
..
Con
v
ersion
from
Relational
Database
to
In
terpret-
ations
T
ypically
,
eac
h
predicate
in
the
logical
represen
tation
will
corresp
ond
to
one
relation
in
the
relational
database.
Eac
h
fact
in
an
in
terpretation
is
a
tuple
in
the
database,
and
an
in
terpretation
corresp
onds
to
a
part
of
the
database
(a
set
of
subsets
of
the
original
relations).
Bac
kground
kno
wledge
can
b
e
stored
in
the
relational
database
b
y
means
of
views
as
w
ell
as
extensional
tables.
Con
v
erting
a
relational
database
to
a
set
of
in
terpretations
can
b
e
done
easily
and
in
a
semi-automated
w
a
y
via
the
follo
wing
pro
cedure
and
with
the
help
of
the
interpret
a
tions
algorithm
in
Figure
..

Decide
whic
h
relations
in
D
B
are
bac
kground
kno
wledge.

Let
KB
b
e
the
original
database
without
the
bac
kground
relations.

Cho
ose
an
attribute
K
in
a
relation
that
uniquely
iden
ties
the
examples;
w
e
call
this
the
example
k
ey
.

Collect
all
foreign
k
eys
in
the
database
sc
heme
of
KB
in
a
set
FK
.
F
oreign
k
eys
are
attributes
that
are
in
fact
references
to
other
attributes;
w
e
write
a
foreign
k
ey
from
R

:A
to
R

:B
as
(R

:A
!
R

:B
).
F
or
instance,
in
Example
.
the
Name
attribute
in
FINES
is
a
foreign
k
ey
to
Name
in
KNO
WS,
whic
h
w
e
write
FINES.Name
!
KNO
WS.Name.

Run
interpret
a
tions(KB
,
K
,
FK
).
This
algorithm
w
orks
as
follo
ws.
F
or
eac
h
example
with
k
ey
k
,
a
database
KB
k
is
constructed
in
whic
h
eac
h
relation
is
a
subrelation
of
a
relation
in
KB
.
The
algorithm
rst
gathers
all
the
tuples
in
KB
that
con
tain
k
,
and
puts
them
in
the
corresp
onding
relations
in
KB
k
.
It
then
k
eeps
adding
tuples
outside
KB
k
that
are
referenced
from
inside
KB
k
to
relations
in
KB
k
,
un
til
KB
k
do
es
not
c
hange
an
ymore.
After
this
has
b
een
done
for
eac
h
k
,
the
set
of
all
databases
KB
k
is
returned.
Note
that
v
alues
are
assumed
to
b
e
t
yp
ed
in
this
algorithm,
i.e.,
t
w
o
v
alues
are
considered
to
b
e
equal
only
if
their
t
yp
es
are
also
equal.
F
or
instance,
if
a
relation
PERSON
has
an
ID
attribute
and
an
Age
attribute
that
are
b
oth
represen
ted
as
n
um
b
ers,
PERSON.ID
=
PERSON.Age
nev
er


CHAPTER
.
FIRST
ORDER
LOGIC
REPRESENT
A
TIONS
pro
cedure
interpret
a
tions(KB
:
database,
K
:
example
k
ey
,
FK
:
set
of
foreign
k
eys)
returns
set
of
databases
for
eac
h
v
alue
k
of
K
:
fadd
tuples
that
c
ontain
example
key
k
g
for
eac
h
R
i

KB
:
R
0
i
:=
fr
i

R
i
j	A
:
r
i
:A
=
k
g
KB
k
:=
S
i
fR
0
i
g
rep
eat
fadd
tuples
that
ar
e
r
efer
enc
e
d
fr
om
S
via
a
for
eign
keyg
for
eac
h
R
0
i

KB
k
:
R
0
i
:=
R
0
i
[
fr
i

R
i
j	(R
j
:A
!
R
i
:B
)

FK
:
	r
j

R
0
j
:
r
j
:A
=
r
i
:B
g
un
til
KB
k
do
es
not
c
hange
an
ymore
return
S
k
KB
k
Figure
.:
Con
v
ersion
of
a
relational
database
to
in
terpretations.
succeeds
ev
en
if
the
v
alues
of
b
oth
attributes
are
equal,
b
ecause
a
p
erson
iden
tication
n
um
b
er
and
an
age
cannot
b
e
the
same.

Con
v
ert
the
in
terpretations
and
bac
kground
relations
in
to
logic
format.
A
tuple
(attr

;
:
:
:
;
attr
n
)
of
a
relation
R
can
trivially
b
e
con
v
erted
to
a
fact
R
(attr

;
:
:
:
;
attr
n
).
By
doing
this
con
v
ersion
for
all
KB
k
,
eac
h
KB
k
b
ecomes
a
set
of
facts
describing
an
individual
example
k
.
The
extensional
bac
kground
relations
can
b
e
con
v
erted
in
the
same
manner
in
to
one
set
of
facts
that
forms
the
bac
kground
kno
wledge.
Bac
kground
relations
dened
b
y
views
can
b
e
con
v
erted
to
equiv
alen
t
Prolog
programs.
The
only
parts
in
this
con
v
ersion
pro
cess
that
are
hard
to
automate
are
the
selection
of
the
bac
kground
kno
wledge
(t
ypically
,
one
selects
those
relations
where
eac
h
tuple
can
b
e
relev
an
t
for
man
y
examples)
and
the
con
v
ersion
of
view
denitions
to
Prolog
programs.
Also,
the
user
m
ust
indicate
whic
h
attribute
should
b
e
c
hosen
as
an
example
k
ey
,
as
this
dep
ends
on
the
learning
task.
Example
.
In
the
c
hemical
database,
w
e
c
ho
ose
as
example
k
ey
the
molecu-
lar
form
ula.
The
bac
kground
kno
wledge
consists
of
the
MENDELEV
table.
In
order
to
build
a
description
of
H

O
,
the
interpret
a
tions
algorithm
in
Figure
.
rst
collects
the
tuples
con
taining
H

O
;
these
are
presen
t
in
MO-
LECULES
and
CONT
AINS.
The
tuples
are
put
in
the
relations
MOLECULES'
and
CONT
AINS'.
The
database
KB
H

O
is
th
us
initialized
as
sho
wn
in
Fig-
ure
..

..
LEARNING
FR
OM
ENT
AILMENT
	
The
tuples
in
KB
H

O
con
tain
references
to
A
tom
id's
ho-i,
i
=
;
;
,
so
the
tuples
con
taining
those
sym
b
ols
are
also
collected
(tuples
from
A
TOMS
and
BONDS).
The
result
is
sho
wn
in
the
second
part
of
Figure
..
The
new
tuples
refer
to
the
elemen
ts
H
and
O
,
whic
h
are
foreign
k
eys
for
the
MENDELEV
relation.
Since
this
relation
is
in
the
bac
kground,
no
further
tuples
are
collected;
hence,
KB
H

O
as
sho
wn
in
the
second
part
of
Figure
.
is
complete.
Con
v
erting
the
tuples
to
facts,
w
e
get
the
follo
wing
description
of
H

O
:
fmolecules('HO'
,
water,
inorganic),
contains('HO',
ho-),
contains('HO',
ho-),
contains('HO',
ho-),
atoms(ho-,
'H'),
atoms(ho-,
'O'),
atoms(ho-,
'H'),
bonds(ho-,
ho-,
single),
bonds(ho-,
ho-,
single)g

Some
v
ariations
of
this
algorithm
can
b
e
considered.
F
or
instance,
when
the
example
k
ey
has
no
meaning
except
that
it
iden
ties
the
example,
this
attribute
can
b
e
left
out
from
the
example
description
(in
our
example,
all
o
ccurrences
of
'HO'
could
b
e
remo
v
ed
if
w
e
consider
this
name
unimp
ortan
t).
The
k
ey
notion
in
this
con
v
ersion
pro
cess
is
lo
c
alization
of
information.
It
is
assumed
that
for
eac
h
example
only
a
relativ
ely
small
part
of
the
database
is
relev
an
t,
and
that
this
part
can
b
e
lo
calized
and
extracted.
F
urther
in
this
text,
w
e
will
refer
to
this
assumption
as
the
lo
c
ality
assumption.
Denition
.
(Lo
calit
y
assumption)
The
lo
c
ality
assumption
states
that
al
l
the
information
that
is
r
elevant
for
a
single
example
is
lo
c
ate
d
in
a
smal
l
p
art
of
the
datab
ase.
If
the
lo
calit
y
assumption
is
not
fullled,
the
lo
calization
pro
cess
as
de-
scrib
ed
ab
o
v
e
w
ould
giv
e
rise
to
a
signican
t
duplication
of
information.
.
Learning
from
En
tailmen
t
Learning
from
en
tailmen
t
is
the
most
frequen
tly
used
paradigm
within
induct-
iv
e
logic
programming.
The
framew
ork
is
describ
ed
in
e.g.
(Muggleton
and
De
Raedt,
		)
(under
the
name
of
normal
semantics)
and
at
this
momen
t
almost
all
ILP
systems
use
it,
e.g.
Pr
ogol
(Muggleton,
		),
F
oil
(Quinlan,
		0),
SR
T
(Kramer,
		),
F
ors
(Karali

c
and
Bratk
o,
		),
.
.
.
All
the
data,
as
w
ell
as
the
bac
kground
kno
wledge,
are
enco
ded
in
a
Prolog
program.
The
concept
that
is
to
b
e
learned
is
represen
ted
b
y
a
predicate,
and
the
learner
has
to
learn
a
denition
for
the
predicate
from
a
set
of
p
ositiv
e
and
negativ
e
examples.
In
principle,
this
can
go
as
far
as
the
syn
thesis
of
a
full
logic
program
from
examples.
In
practice,
the
problems
that
are
considered
are
often
constrained
or
simplied
v
ersions
of
this
v
ery
general
and
dicult
task.

0
CHAPTER
.
FIRST
ORDER
LOGIC
REPRESENT
A
TIONS
Initialization:
MOLECULES'
CONT
AINS'
F
orm
ula
Name
Class
H

O
w
ater
inorganic
F
orm
ula
A
tom
id
H

O
ho-
H

O
ho-
H

O
ho-
A
TOMS'
BONDS'
atom
id
elemen
t
A
tom
id
A
tom
id
T
yp
e
After
one
step:
MOLECULES'
CONT
AINS'
F
orm
ula
Name
Class
H

O
w
ater
inorganic
F
orm
ula
A
tom
id
H

O
ho-
H

O
ho-
H

O
ho-
A
TOMS'
BONDS'
atom
id
elemen
t
ho-
H
ho-
O
ho-
H
A
tom
id
A
tom
id
T
yp
e
ho-
ho-
single
ho-
ho-
single
Figure
.:
Construction
of
the
sub
database
KB
H

O
.

..
LEARNING
FR
OM
ENT
AILMENT

F
or
instance,
predictiv
e
induction
suc
h
as
classication
and
regression
can
b
e
seen
as
a
constrained
v
ersion
of
program
syn
thesis.
Examples
are
then
t
ypically
represen
ted
b
y
constan
ts,
and
they
are
lab
elled
either
using
a
unary
predicate
(e
b
elongs
to
class
p
if
p(e)
is
true),
or
using
a
binary
predicate
that
indicates
the
lab
el
of
the
example
(lab
el(e;
y
)
means
that
the
lab
el
of
example
e
is
y
).
Denition
.
(Learning
from
en
tailmen
t)
In
the
le
arning
fr
om
entail-
ment
fr
amework,
we
dene
the
c
onc
ept
le
arning
task
as
fol
lows:
Giv
en:

a
set
of
p
ositive
examples
E
+
(e
ach
example
is
a
clause)

a
set
of
ne
gative
examples
E
 
b
ackgr
ound
know
le
dge
B
(a
Pr
olo
g
pr
o
gr
am)

and
a
rst
or
der
language
L

Pr
olo
g
Find:
a
hyp
othesis
H

L
such
that

e

E
+
:
H
^
B
j
=
e
and

e

E
 :
H
^
B
j
=
e
T
able
.
sho
ws
ho
w
the
data
from
the
P
ok
er
example
w
ould
t
ypically
b
e
represen
ted
in
this
ILP
setting:
eac
h
example
is
represen
ted
b
y
a
single
fact.

Note
that
ev
ery
hand
that
is
not
a
pair
is
explicitly
written
as
a
negativ
e
example.
Some
ILP
systems
w
ould
automatically
deduce
from
the
fact
that
e.g.
hand
is
a
full
house,
that
it
is
not
a
pair;
in
that
case
no
negativ
e
examples
need
to
b
e
giv
en.
Note,
ho
w
ev
er,
that
the
learner
then
mak
es
the
assumption
that
the
predicate
to
b
e
learned
is
a
function.
This
is
general
enough
for
the
classication
or
regression
setting
but
less
general
than
learning
a
predicate.
Examples
of
ILP
systems
learning
from
p
ositiv
e
examples
only
are
F
oidl
(Mo
oney
and
Cali,
		),
FF
oil
(Quinlan,
		)
and
Pr
ogol
(in
a
sp
ecic
setting,
see
(Muggleton,
		;
Muggleton
et
al.,
		)).
The
follo
wing
rst
order
logic
form
ula
denes
the
concept
of
a
pair:
	R
ank
;
Suit

;
Suit

:
card
(Hand
;
R
ank
;
Suit

)
^
card(Hand
;
R
ank
;
Suit

)
^
Suit

=
Suit

^

	Suit

:
card(Hand
;
R
ank
;
Suit

)
^
Suit

=
Suit

^
Suit

=
Suit

^

	R
ank

;
Suit

;
Suit

:
card
(Hand
;
R
ank

;
Suit

)
^
card
(Hand
;
R
ank

;
Suit

)
^
Suit

=
Suit

^
R
ank

=
R
ank
!
pair(Hand)

In
principle,
one
can
also
use
non-fact
clauses
to
represen
t
the
examples,
but
this
is
not
often
done.


CHAPTER
.
FIRST
ORDER
LOGIC
REPRESENT
A
TIONS
card(hand,
,
spades).
card(hand,
,
hearts).
card(hand,
king,
clubs).
card(hand,
queen,
hearts).
card(hand,
,
hearts).
card(hand,
,
spades).
card(hand,
,
clubs).
card(hand,
ace,
clubs).
card(hand,
,
spades).
card(hand,
queen,
diamonds).
card(hand,
,
hearts).
card(hand,
,
clubs).
card(hand,
,
clubs).
card(hand,
,
spades).
card(hand,
,
hearts).
card(hand,
,
spades).
card(hand,
,
spades).
card(hand,
ace,
spades).
card(hand,
ace,
clubs).
card(hand,
	,
spades).
card(hand,
ace,
clubs).
card(hand,
ace,
diamonds).
card(hand,
,
spades).
card(hand,
,
diamonds).
card(hand,
,
hearts).
card(hand,
,
hearts).
card(hand,
,
hearts).
card(hand,
,
hearts).
card(hand,
king,
clubs).
card(hand,
,
diamonds).
%
positive
examples
pair(hand).
pair(hand).
pair(hand).
%
negative
examples
:-
pair(hand).
%nought(hand).
:-
pair(hand).
%fullhouse(hand).
:-
pair(hand).
%nought(hand).
T
able
.:
Represen
ting
the
p
ok
er
data
in
the
learning
from
en
tailmen
t
set-
ting.
Negativ
e
examples
are
written
with
a
preceding
:-
sym
b
ol;
the
original
information
is
written
as
a
commen
t.

..
LEARNING
FR
OM
ENT
AILMENT

pair(Hand)
:-
card(Hand,
Rank,
Suit),
card(Hand,
Rank,
Suit),
Suit
\=
Suit,
not
third_card(Hand
,
Rank,
Suit,
Suit),
not
second_pair(Han
d,
Rank,
Suit,
Suit).
third_card(Hand,
Rank,
Suit,
Suit)
:-
card(Hand,
Rank,
Suit),
Suit
\=
Suit,
Suit
\=
Suit.
second_pair(Hand
,
Rank,
Suit,
Suit)
:-
card(Hand,
Rank,
Suit),
card(Hand,
Rank,
Suit),
Rank
\=
Rank,
Suit
\=
Suit.
Figure
.:
A
correct
denition
of
a
pair
in
Prolog.
A
Prolog
v
ersion
of
this
denition
is
sho
wn
in
Figure
..
Note
that
this
denition
is
v
ery
similar
to
the
one
found
when
learning
from
in
terpretations;
the
main
dierence
is
that
when
learning
from
in
terpretations
no
explicit
ref-
erence
to
an
example
iden
tier
(the
v
ariable
Hand
)
is
giv
en;
all
information
is
automatically
assumed
to
b
e
information
ab
out
the
example
that
is
to
b
e
classied,
not
ab
out
an
y
other
example.
With
resp
ect
to
the
represen
tation
of
the
data
(comparing
T
able
.
with
T
able
.),
w
e
note
that
in
the
learning
from
in
terpretations
setting
inform-
ation
ab
out
one
example
is
clearly
separated
from
information
ab
out
other
examples

,
whereas
when
learning
from
en
tailmen
t
all
this
information,
to-
gether
with
the
bac
kground
kno
wledge,
forms
one
single
Prolog
program.
W
e
will
return
to
this
in
the
next
section.
The
P
ok
er
example
is
an
example
of
a
classication
task.
Once
more,
w
e
stress
that
ILP
is
m
uc
h
more
general
and
can
b
e
used
to
learn
(in
principle)
an
y
denition
of
a
predicate,
e.g.
from
the
follo
wing
data:
member(,
[,,]).
member(,
[,,,]).
:-
member(,
[,]).
:-
member(,
[]).
:-
member(,
[,,]).
one
could
induce
a
denition
for
the
member
predicate:
member(X,
[X|Y]).

It
is
also
separated
from
the
bac
kground
kno
wledge,
although
this
is
less
clear
here
b
ecause
in
this
example
there
is
no
bac
kground
kno
wledge.


CHAPTER
.
FIRST
ORDER
LOGIC
REPRESENT
A
TIONS
member(X,
[Y|Z])
:-
member(X,
Z).
This
illustrates
that
in
the
learning
from
en
tailmen
t
setting,
ILP
has
an
extremely
broad
application
p
oten
tial.
In
practice,
this
p
oten
tial
is
sev
erely
limited
due
to
the
complexit
y
of
man
y
of
these
tasks.
Within
computational
learning
theory
sev
eral
negativ
e
results
ha
v
e
b
een
published
(D

zeroski
et
al.,
		;
Cohen
and
P
age,
		;
Cohen,
		),
sho
wing
that
man
y
tasks
that
can
in
principle
b
e
handled
b
y
ILP
systems
cannot
b
e
handled
in
practice,
due
to
practical
limitations
suc
h
as
space
and
time
constrain
ts
(more
sp
ecically:
they
are
not
P
A
C-learnable
with
p
olynomial
time
complexit
y;
P
A
C
stands
for
pr
ob
ably
appr
oximately
c
orr
e
ct,
see
In
termezzo

for
a
brief
discussion
on
P
A
C-
learnabilit
y).
These
results
ha
v
e
strongly
inuenced
the
attitude
to
w
ards
ILP
of
man
y
researc
hers
in
mac
hine
learning
and
data
mining.
An
in
teresting
prop
ert
y
of
the
learning
from
in
terpretations
setting
is
that
under
fairly
general
assumptions,
rst
order
h
yp
otheses
are
P
A
C-learnable
in
this
setting
(De
Raedt
and
D

zeroski,
		).
This
is
mainly
due
to
the
fact
that
the
setting
exploits
the
lo
calit
y
assumption;
w
e
will
discuss
this
in
the
next
section.
.
Relationships
Bet
w
een
the
Dieren
t
Settings
..
On
the
Origin
of
Learning
F
rom
In
terpretations
Originally
,
learning
from
in
terpretations
(also
called
nonmonotonic
ILP)
w
as
seen
as
an
alternativ
e
setting,
where
a
h
yp
othesis
is
a
set
of
clauses
suc
h
that
eac
h
p
ositiv
e
example
(together
with
the
bac
kground)
mak
es
eac
h
clause
in
the
set
true,
and
none
of
the
negativ
e
examples
do
so.
I.e.,
a
h
yp
othesis
H
is
to
b
e
found
suc
h
that

e

E
+
:
H
is
true
in
M(e
^
B
)

e

E
 :
H
is
false
in
M(e
^
B
)
where
M(e
^
B
)
is
the
minimal
Herbrand
mo
del
of
e
^
B
(in
tuitiv
ely:
the
in
terpretation
represen
ted
b
y
e
^
B
,
as
illustrated
in
Example
.).
This
is
to
b
e
con
trasted
with
the
classical
setting
where

e

E
+
:
H
^
B
j
=
e

e

E
 :
H
^
B
j
=
e
Note
that
in
the
classical
setting
H
(together
with
B
)
is
supp
osed
to
explain
e,
while
in
the
nonmonotonic
setting
H
describ
es
e;
therefore
the
settings
are
also
sometimes
called
explanatory
ILP
and
descriptive
ILP.
A
brief
history
of
ho
w
learning
from
in
terpretations
ev
olv
ed
from
a
descriptiv
e
tec
hnique
to
w
ards
a
predictiv
e
(explanatory)
tec
hnique
is
giv
en
in
In
termezzo
.

..
RELA
TIONSHIPS
BETWEEN
THE
DIFFERENT
SETTINGS

In
termezzo
:
P
A
C-learnabilit
y
Computational
learning
theory
(COL
T)
is
the
subeld
of
mac
hine
learning
that
is
concerned
with
describing
ho
w
hard
a
certain
learning
task
is.
In
general,
the
complexit
y
of
an
algorithm
is
describ
ed
b
y
giving
its
time
and
space
require-
men
ts
for
solving
a
certain
problem,
in
terms
of
certain
parameters
that
are
indicativ
e
of
the
size
or
dicult
y
of
the
problem.
Based
on
suc
h
a
description
algorithms
can
for
instance
b
e
classied
as
tractable
(execution
time
is
p
oly-
nomial
in
the
parameters)
or
not
tractable
(execution
time
is
not
p
olynomial
but,
e.g.,
exp
onen
tial).
Of
course,
complexit
y
results
dep
end
on
the
parameter
that
is
used;
e.g.,
if
adding
t
w
o
n
um
b
ers
is
linear
in
the
n
um
b
er
of
bits
used
to
represen
t
the
n
um-
b
ers,
it
is
logarithmic
in
the
n
um
b
ers
themselv
es;
and
if
an
algorithm
for
com-
puting
the
factorial
of
a
n
um
b
er
n
has
a
time
complexit
y
O
(n),
it
is
exp
onen
tial
in
the
n
um
b
er
of
bits
used
to
represen
t
the
n
um
b
er.
Whether
this
algorithm
is
tractable
or
not
dep
ends
on
the
parameters
in
terms
of
whic
h
the
complexit
y
is
describ
ed.
Hence,
in
order
to
adequately
describ
e
the
complexit
y
of
learning
tasks,
these
parameters
need
to
b
e
xed.
No
w
ada
ys
the
P
A
C-learning
framew
ork,
in
tro
duced
b
y
V
alian
t
(	)
is
prob-
ably
the
most
p
opular
framew
ork
for
studying
the
complexit
y
of
learning
tasks
or
algorithms.
P
A
C
stands
for
pr
ob
ably
appr
oximately
c
orr
e
ct.
In
this
frame-
w
ork,
it
is
assumed
that
the
task
is
to
learn
a
h
yp
othesis
that
appro
ximates
the
correct
theory
to
an
agreeable
exten
t.
A
learning
algorithm
need
not
guaran
tee
that
it
will
nd
suc
h
a
h
yp
othesis,
but
should
ha
v
e
a
certain
probabilit
y
(close
to
)
of
doing
so.
Denition
.
(
-correctness)
A
pr
e
dictor
pred
is

-c
orr
e
ct
if
and
only
if
P
(pr
ed(x)
=

(x))
<

(wher
e

(x)
is
the
class
of
x).
Denition
.	
(P
A
C-learnabilit
y)
A
function

is
P
A
C-le
arnable
if
and
only
if
for
e
ach

>
0
and
for
e
ach

>
0
ther
e
exists
an
algorithm
that,
given
a
sucient
numb
er
of
examples,
with
pr
ob
ability

 
induc
es
a

-c
orr
e
ct
pr
e
dictor
pr
e
d.
The
sample
c
omplexity
is
the
n
um
b
er
of
examples
needed
to
learn
suc
h
a
theory;
the
time
c
omplexity
is
the
time
needed
to
learn
it.
A
P
A
C-learning
algorithm
is
considered
to
b
e
tr
actable
if
it
has
time
complexit
y
p
olynomial
in
=
and
=".


CHAPTER
.
FIRST
ORDER
LOGIC
REPRESENT
A
TIONS
In
termezzo
:
History
of
Learning
F
rom
In
terpretations
The
nonmonotonic
setting
(Helft,
		)
w
as
originally
mean
t
for
descriptiv
e
in-
duction.
The
rst
ILP
system
to
use
this
setting
w
as
Cla
udien
(De
Raedt
and
Bruyno
oghe,
		;
De
Raedt
and
Dehasp
e,
		),
whic
h
returned
clausal
descrip-
tions
of
regularities
in
the
data.
The
dev
elopmen
t
from
clausal
disco
v
ery
to
w
ards
prediction
can
b
e
sk
etc
hed
as
follo
ws:
.
Cla
udien,
the
clausal
disco
v
ery
engine,
examines
a
set
of
data
D
and
returns
as
h
yp
othesis
H
all
the
clauses
(within
a
certain
language,
and
excluding
redundan
t
clauses)
that
are
true
for
the
data
(De
Raedt
and
Bruyno
oghe,
		).
F
or
instance,
giv
en
a
set
of
data
on
family
relationships,
Cla
udien
w
ould
deriv
e
clauses
suc
h
as
 
parent(X,X)
(nob
o
dy
is
his
o
wn
paren
t)
or
sibling(X,Y)
 
parent(X,Z),
parent(Y,Z),
X
=
Y
(p
eople
with
the
same
paren
t
are
siblings).
.
A
later
v
ersion
of
Cla
udien
can
handle
m
ultiple
sets
of
data.
It
returns
all
clauses
that
are
true
within
eac
h
data
set.
It
is
recognised
that,
b
y
regarding
the
h
yp
othesis
as
a
denition
of
a
concept,
Cla
udien
can
b
e
seen
as
a
concept
learner
that
learns
from
p
ositiv
e
examples
only
(De
Raedt
and
D

zeroski,
		).
It
learns
the
most
sp
ecic
concept
that
co
v
ers
all
the
examples.
F
or
instance,
Cla
udien
could
b
e
pro
vided
with
0
data
sets,
eac
h
set
describing
a
dieren
t
t
ypical
Belgian
family
.
The
system
w
ould
then
return
a
h
yp
othesis
that
represen
ts
the
concept
of
a
t
ypical
Belgian
family
,
con
taining
the
same
clauses
as
sho
wn
ab
o
v
e
but
also
clauses
suc
h
as
Y=Z
 
married(X,Y),
married(X,Z)
(one
can
b
e
married
to
only
one
p
erson).
.
A
further
dev
elopmen
t
is
the
ICL
system
(De
Raedt
and
V
an
Laer,
		).
The
in
terpretations
fed
in
to
this
system
are
lab
elled;
all
the
in
terpretations
with
a
certain
lab
el
are
considered
to
b
e
p
ositiv
e
examples,
the
other
in
terpretations
are
negativ
e
examples.
ICL
then
tries
to
nd
clauses
that
are
true
in
all
the
p
ositiv
e
examples,
but
are
false
in
at
least
some
of
the
negativ
e
examples.
It
searc
hes
for
a
minimal
set
of
clauses
suc
h
that
eac
h
negativ
e
example
violates
at
least
one
clause.
ICL
thereb
y
is
the
rst
ILP
system
to
learn
a
concept
from
p
ositiv
e
and
negativ
e
examples
that
are
represen
ted
as
in
terpretations.
F
or
instance,
supp
ose
ICL
is
pro
vided
with
0
families
lab
elled
\Belgian"
and
0
families
lab
elled
\Ku
w
aitian".
ICL
then
tries
to
nd
a
minimal
set
of
clauses
that
allo
ws
to
distinguish
Belgian
from
Ku
w
aitian
families.
Suc
h
a
set
could
con
tain
the
clause
Y=Z
 
married(X,Y),
married(X,Z),
if
this
is
true
for
all
Belgian
families
but
violated
for
some
Ku
w
aitian
families.
.
It
is
noticed
that
the
h
yp
otheses
returned
b
y
ICL,
whic
h
(b
eing
sets
of
clauses)
are
in
conjunctiv
e
normal
form
(CNF),
usually
b
ecome
easier
to
in
terpret
if
they
are
con
v
erted
to
disjunctiv
e
normal
form
(DNF).
ICL
is
extended
so
that
it
can
learn
DNF
theories
instead
of
CNF
theories,
if
the
user
desires
this.
A
note
on
the
relationship
b
et
w
een
DNF
and
CNF
is
(De
Raedt
et
al.,
		),
whic
h
also
links
to
earlier
w
ork
on
learning
CNF
(Mo
oney,
		).
F
or
instance,
assuming
the
clause
Y=Z
 
married(X,Y),
married(X,Z)
is
use-
ful
for
distinguishing
Belgian
families
from
Ku
w
aitian
families,
a
sucien
t
condi-
tion
for
a
family
b
eing
Ku
w
aitian
w
ould
b
e
married(X,Y),
married(X,Z),
Y
=
Z.
Suc
h
a
condition
w
ould
app
ear
in
the
DNF
denition
of
Ku
w
aitian
families.

..
RELA
TIONSHIPS
BETWEEN
THE
DIFFERENT
SETTINGS

..
Learning
F
rom
In
terpretations
Links
A
ttribute
V
alue
Learning
to
Learning
F
rom
En
tailmen
t
Learning
from
in
terpretations
can
b
e
situated
in
b
et
w
een
the
other
settings;
in
a
sense
it
pro
vides
a
link
b
et
w
een
them.
Learning
from
in
terpretations
diers
from
learning
from
en
tailmen
t
(as
it
is
practically
used)
in
that
it
exploits
the
lo
calit
y
assumption
b
y
pro
viding
a
clear
separation
b
et
w
een
dieren
t
pieces
of
information,
in
t
w
o
w
a
ys:

the
information
that
is
con
tained
in
examples
is
separated
from
the
in-
formation
in
the
bac
kground
kno
wledge;

information
in
one
example
is
separated
from
information
in
another
ex-
ample.
It
is
this
separation
of
information
that
pro
vides
the
link
with
attribute
v
alue
learning,
where
suc
h
a
separation
is
also
presen
t.
Figure
.
giv
es
a
graphical
illustration
of
ho
w
the
three
settings
dier.
In
attribute
v
alue
learning,
the
learner
has
to
nd
a
link
b
et
w
een
the
information
in
one
example
and
the
target
v
ariable.
When
learning
from
in
terpretations,
the
learner
has
to
link
the
information
that
is
con
tained
in
one
example,
together
with
the
bac
kground
kno
wledge,
to
the
target.
When
learning
from
en
tailmen
t,
the
learner
has
to
link
an
indenitely
large
part
of
the
database
to
the
target
v
ariable.
It
is
p
ossible
that
only
a
part
of
the
information
in
the
database
is
relev
an
t,
but
in
general
this
is
not
guaran
teed,
and
ev
en
when
it
is
it
ma
y
not
b
e
ob
vious
whic
h
part
that
is.
Hence,
the
learner
has
to
lo
ok
up
the
relev
an
t
information
in
a
large
database,
whic
h
ma
y
b
e
a
costly
op
eration.
Figure
.
also
illustrates
a
second
p
oin
t:
a
dierence
in
assumptions
that
can
b
e
made
ab
out
completeness
of
kno
wledge.
If
w
e
do
not
consider
the
p
ossibilit
y
of
missing
or
noisy
v
alues,
then
eac
h
example
description
in
attribute
v
alue
learning
is
complete.
Since
the
examples
are
only
a
sample
from
a
certain
p
opulation,
the
kno
wledge
ab
out
the
p
opulation
itself
is
not
complete.
W
e
sa
y
that
example
descriptions
are
close
d,
and
the
description
of
the
p
opulation
is
op
en.
F
or
learning
from
in
terpretations,
the
situation
is
exactly
the
same.
F
or
learning
from
en
tailmen
t,
it
is
v
ery
dieren
t
ho
w
ev
er.
Due
to
the
fact
that
no
clear
separation
b
et
w
een
information
ab
out
sp
ecic
examples
is
made,
indi-
vidual
example
descriptions
are
not
distinguished
from
the
description
of
the
training
sample
as
a
whole,
and
hence
example
descriptions
m
ust
b
e
op
en.


Actually
,
the
lo
calit
y
can
b
e
in
tro
duced
in
learning
from
en
tailmen
t
b
y
learning
from
clauses
with
a
non-empt
y
b
o
dy
.
The
information
lo
cal
to
an
example
is
then
put
in
the
b
o
dy
of
the
clauses.
This
represen
tation
is
not
often
used,
ho
w
ev
er,
and
few
systems
supp
ort
it.


CHAPTER
.
FIRST
ORDER
LOGIC
REPRESENT
A
TIONS
back-
ground
data
target
data
target
open
open
Learning From Interpretations
Attribute Value Learning
target
open
Learning From Entailment
background
  + data
Figure
.:
Graphical
represen
tation
of
the
relationship
b
et
w
een
the
dieren
t
settings,
fo
cusing
on
separation
of
information
and
op
enness.

..
RELA
TIONSHIPS
BETWEEN
THE
DIFFERENT
SETTINGS
	
..
Adv
an
tages
of
Learning
F
rom
In
terpretations
The
separation
of
information
that
is
realized
b
y
learning
from
in
terpretations
has
p
ositiv
e
eects
in
t
w
o
resp
ects.
First
of
all,
it
aects
the
eciency
of
the
learning
pro
cess.
De
Raedt
and
D

zeroski
(		)
ha
v
e
sho
wn
that
in
the
learning
from
in
terpretations
setting,
P
A
C-learning
a
clausal
theory
is
tractable
(giv
en
upp
er
b
ounds
on
the
com-
plexit
y
of
the
theory).
This
p
ositiv
e
result
stands
in
con
trast
with
the
negativ
e
P
A
C-learning
results
obtained
for
the
learning
from
en
tailmen
t
setting,
and
is
strongly
related
to
the
prop
ert
y
that
all
example
descriptions
are
complete.
Second,
man
y
tec
hniques
in
attribute
v
alue
learning
implicitly
exploit
the
indep
endence
of
the
examples.
Since
learning
from
in
terpretations
also
assumes
this
indep
endence,
suc
h
tec
hniques
can
trivially
b
e
upgraded
to
this
setting,
whereas
an
upgrade
to
learning
from
en
tailmen
t,
if
at
all
p
ossible,
is
non-trivial.
An
example
of
suc
h
a
tec
hnique
is
the
use
of
in
ternal
v
alidation
sets.
Setting
apart
a
subset
of
the
training
set
is
easy
if
all
examples
are
indep
enden
t;
one
can
c
ho
ose
the
subset
randomly
.
When
dep
endences
b
et
w
een
examples
exist,
ho
w
ev
er,
it
ma
y
b
e
necessary
to
carefully
c
ho
ose
the
examples
that
are
k
ept
apart,
b
ecause
remo
ving
an
example
from
the
training
set
ma
y
cause
relev
an
t
information
ab
out
another
example
to
b
e
remo
v
ed.
The
same
problem
o
ccurs
when
one
w
an
ts
to
mine
a
large
database
b
y
learning
from
a
sample
of
the
database.
W
e
can
conclude
from
this
that
learning
from
in
terpretations
oers
b
etter
opp
ortunities
for
upgrading
tec
hniques
from
attribute
v
alue
learning
to
ILP
than
learning
from
en
tailmen
t.
..
Limitations
of
Learning
F
rom
In
terpretations
As
already
men
tioned,
the
assumption
that
all
the
relev
an
t
information
for
a
prediction
is
lo
calized
in
one
example
(together
with
the
bac
kground
kno
w-
ledge)
means
that
in
the
learning
from
in
terpretations
setting
one
cannot
learn
predictors
that
mak
e
use
of
the
information
in
m
ultiple
examples.
Hence,
re-
cursiv
e
predicates
suc
h
as
member
cannot
b
e
learned
from
in
terpretations.
This
means
the
setting
is
less
p
o
w
erful
than
learning
from
en
tailmen
t.
An
in
teresting
observ
ation,
ho
w
ev
er,
is
that
when
one
tak
es
a
lo
ok
at
the
practical
applications
of
ILP
that
ha
v
e
b
een
rep
orted
on
during
the
last
few
y
ears,
e.g.
at
the
y
early
w
orkshops
on
ILP
(De
Raedt,
		;
Muggleton,
		;
La
vra

c
and
D

zeroski,
		;
P
age,
		)
then
it
turns
out
that
almost
ev
ery
application
in
v
olv
es
problems
of
the
kind
that
can
b
e
handled
b
y
learning
from
in
terpretations.
Luc
De
Raedt
men
tioned
in
his
in
vited
talk
at
the
Eighth
International
Confer
enc
e
on
Inductive
L
o
gic
Pr
o
gr
amming
that
in
the
literature
on
ILP
he
has
found
only
one
single
application
where
a
recursiv
e
rule
w
as
found.

	0
CHAPTER
.
FIRST
ORDER
LOGIC
REPRESENT
A
TIONS
These
p
oin
ts
suggest
that
in
practice
the
limitations
of
learning
from
in
ter-
pretations
are
not
as
bad
as
they
migh
t
seem
at
rst
sigh
t.
.
Related
W
ork
The
learning
from
in
terpretations
has
its
origin
in
the
non-monotonic
learn-
ing
setting
b
y
Helft
(		).
De
Raedt
and
D

zeroski
(		)
sho
w
ed
ho
w
it
can
b
e
used
for
concept
learning
and
pro
v
ed
that
in
this
setting
(under
certain
as-
sumptions)
rst
order
logic
h
yp
otheses
are
P
A
C-learnable
with
p
olynomial
time
complexit
y
.
De
Raedt
(		)
relates
learning
from
in
terpretations
with
other
settings
within
ILP
(among
whic
h
learning
from
en
tailmen
t).
De
Raedt
et
al.
(		)
illustrate
the
practical
applicabilit
y
of
the
learning
from
in
terpretations
setting
b
y
means
of
the
Cla
udien,
ICL
and
Tilde
systems.
An
illustration
of
the
use
of
nonmonotonic
ILP
for
other
than
prediction
purp
oses
(restructuring
a
deductiv
e
database)
is
(Blo
c
k
eel
and
De
Raedt,
		a).
.	
Conclusions
In
this
c
hapter
w
e
ha
v
e
compared
dieren
t
data
represen
tation
formalisms.
W
e
ha
v
e
discussed
the
attribute
v
alue
framew
ork,
whic
h
is
essen
tially
prop
ositional,
and
the
inductiv
e
logic
programming
framew
ork,
in
whic
h
rst
order
logic
is
used
to
represen
t
h
yp
otheses.
Tw
o
settings
for
ILP
w
ere
discussed:
learning
from
in
terpretations
and
learning
from
en
tailmen
t.
W
e
ha
v
e
compared
the
dif-
feren
t
settings
with
resp
ect
to
their
represen
tational
p
o
w
er
and
their
eciency
,
and
related
the
represen
tational
p
o
w
er
to
learning
from
m
ultiple
relations
in
a
relational
database.
Our
main
conclusions
are
that
learning
from
in
terpretations
can
b
e
seen
as
situated
somewhere
in
b
et
w
een
the
other
settings,
extending
the
attribute
v
alue
framew
ork
to
w
ards
ILP
without
giving
up
its
eciency
.
Due
to
its
fa
v
or-
able
p
osition,
the
learning
from
in
terpretations
framew
ork
is
a
go
o
d
c
hoice
for
upgrading
the
tec
hniques
discussed
in
the
previous
c
hapters
to
ILP
.

Chapter

Decision
T
rees
in
First
Order
Logic
.
In
tro
duction
Decision
trees
ha
v
e
mainly
b
een
emplo
y
ed
within
attribute
v
alue
learning.
Due
to
the
fo
cus
on
logic
programming
that
has
dominated
the
researc
h
on
relational
learning
tec
hniques
during
the
latest
decennium
(in
the
form
of
inductiv
e
lo-
gic
programming),
relational
h
yp
otheses
are
almost
alw
a
ys
represen
ted
as
rst
order
rule
sets.
A
few
exceptions
exist;
w
e
men
tion
Ka
te
(Manago,
		),
Str
uct
(W
atanab
e
and
Rendell,
		),
ML-Smar
t
(Bergadano
and
Giord-
ana,
	),
SR
T
(Kramer,
		)
and
Tritop
(Geib
el
and
Wysotzki,
		).
Not
all
of
these
systems
op
erate
within
a
strict
logical
framew
ork;
e.g.
Ka
te
uses
a
frame-based
represen
tation
language.
There
is
also
some
v
ariation
in
cer-
tain
restrictions
that
are
imp
osed.
The
decision
tree
format
used
b
y
Str
uct
and
SR
T
is
closest
to
the
one
w
e
will
prop
ose
in
this
c
hapter.
While
Str
uct
and
SR
T
induce
relational
(also
called
structural)
decision
trees,
the
seman
tics
of
suc
h
trees
ha
v
e
nev
er
b
een
addressed
explicitly
.
This
is
most
probably
due
to
the
fact
that
they
seem
trivial.
W
e
sho
w
in
this
c
hapter
that
they
are
not.
There
are
a
few
p
eculiarities
in
the
seman
tics
of
rst
order
logical
decision
trees
that
are
easily
o
v
erlo
ok
ed,
and
these
ha
v
e
in
teresting
consequences
with
resp
ect
to
the
represen
tational
p
o
w
er
of
this
formalism.
In
this
c
hapter
w
e
rst
in
tro
duce
the
notion
of
a
rst
order
logical
decision
tree.
W
e
then
discuss
ho
w
suc
h
a
tree
can
b
e
transformed
in
to
a
rule
set,
and
sho
w
that
Quinlan's
(		a)
metho
d
for
turning
prop
ositional
decision
trees
in
to
rule
sets
do
es
not
w
ork
in
the
rst
order
case.
W
e
further
discuss
the
relationship
with
the
at
logic
programs
that
most
ILP
systems
induce
and
with
rst
order
decision
lists,
and
relate
this
to
predicate
in
v
en
tion
and
	

	
CHAPTER
.
DECISION
TREES
IN
FIRST
ORDER
LOGIC
induction
of
logical
form
ulae
with
mixed
quan
tication.
.
Setting
W
e
use
the
learning
from
in
terpretations
setting.
W
e
recall
the
problem
sp
e-
cication
as
giv
en
in
Chapter
:
Giv
en:

a
target
v
ariable
Y

a
set
of
lab
elled
examples
E
(eac
h
example
is
a
Prolog
program
e
lab
elled
with
a
v
alue
y
for
the
target
v
ariable)

a
language
L

Prolog,

and
a
bac
kground
theory
B
,
Find:
a
h
yp
othesis
H

L,
suc
h
that
for
all
lab
elled
examples
(e;
y
)

E
,

H
^
e
^
B
j
=
l
abel
(y
),
and

y
0
=
y
:
H
^
e
^
B
j
=
l
abel
(y
0
)
Throughout
this
c
hapter
w
e
will
rep
eatedly
refer
to
the
follo
wing
example.
Example
.
An
engineer
has
to
c
hec
k
a
set
of
mac
hines.
A
mac
hine
consists
of
sev
eral
parts
that
ma
y
b
e
in
need
of
replacemen
t.
Some
of
these
can
b
e
replaced
b
y
the
engineer,
others
only
b
y
the
man
ufacturer
of
the
mac
hine.
If
a
mac
hine
con
tains
w
orn
parts
that
cannot
b
e
replaced
b
y
the
engineer,
it
has
to
b
e
sen
t
bac
k
to
the
man
ufacturer
(class
sendback).
If
all
the
w
orn
parts
can
b
e
replaced,
it
is
to
b
e
xed
(fix).
If
there
are
no
w
orn
parts,
nothing
needs
to
b
e
done
(ok).
Giv
en
the
follo
wing
set
of
examples
(eac
h
example
corresp
onds
to
one
ma-
c
hine)
and
bac
kground
kno
wledge:
Example

lab
el(x)
worn(gear)
worn(chain)
Example

lab
el(sendb
ack)
worn(engine)
worn(chain)
Example

lab
el(sendb
ack)
worn(wheel)
Example

lab
el(ok)
Bac
kground
kno
wledge
replaceable(gear)
replaceable(chain)
not
replaceable(engine)
not
replaceable(wheel)
a
Prolog
rule
for
the
sendback
class
is:
label(send_back)
:-
worn(X),
not_replaceable(
X)


..
FIRST
ORDER
LOGICAL
DECISION
TREES
	
w
orn(X)



P
P
P
not
replaceable(X)
ok


H
H
sendbac
k
x
Figure
.:
Logical
decision
tree
enco
ding
the
target
h
yp
othesis
of
Example
..
.
First
Order
Logical
Decision
T
rees
..
Denition
of
First
Order
Logical
Decision
T
rees
Denition
.
(F
OLDT)
A
rst
or
der
lo
gic
al
de
cision
tr
e
e
(F
OLDT)
is
a
binary
de
cision
tr
e
e
in
which
.
the
no
des
of
the
tr
e
e
c
ontain
a
c
onjunction
of
liter
als,
and
.
dier
ent
no
des
may
shar
e
variables,
under
the
fol
lowing
r
estriction:
a
variable
that
is
intr
o
duc
e
d
in
a
no
de
(which
me
ans
that
it
do
es
not
o
c
cur
in
higher
no
des)
must
not
o
c
cur
in
the
right
br
anch
of
that
no
de.
The
need
for
the
restriction
in
part
()
of
Denition
.
follo
ws
from
the
seman
tics
of
the
tree.
A
v
ariable
X
that
is
in
tro
duced
in
a
no
de,
is
existen
tially
quan
tied
within
the
conjunction
of
that
no
de.
The
righ
t
subtree
is
only
rel-
ev
an
t
when
the
conjunction
fails
(\there
is
no
suc
h
X
"),
in
whic
h
case
further
reference
to
X
is
meaningless.
Example
.
An
example
of
a
logical
decision
tree
is
sho
wn
in
Figure
..
It
enco
des
the
target
h
yp
othesis
of
Example
..

Figure
.
sho
ws
ho
w
to
use
F
OLDTs
for
prediction.
W
e
use
the
follo
wing
notation:
a
tree
T
is
either
a
leaf
with
lab
el
y
,
in
whic
h
case
w
e
write
T
=
leaf
(y
),
or
it
is
an
in
ternal
no
de
with
conjunction
c
onj,
left
branc
h
left
and
righ
t
branc
h
right,
in
whic
h
case
w
e
write
T
=
ino
de(c
onj,
left,
right
).

Because
an
example
e
is
a
Prolog
program,
a
test
in
a
no
de
corresp
onds
to
c
hec
king
whether
a
query
 
C
succeeds
in
e
^
B
(with
B
the
bac
kground
kno
wledge).
Note
that
it
is
not
sucien
t
to
dene
C
as
the
conjunction
c
onj
in
the
no
de
itself.
Since
c
onj
ma
y
share
v
ariables
with
no
des
higher
in
the
tree,
C
consists
of
sev
eral
conjunctions
that
o
ccur
in
the
path
from
the
ro
ot
to
the
curren
t
no
de.
More
sp
ecically
,
C
is
of
the
form
Q
^
conj
,
where
Q
is

This
notation
is
sligh
tly
simplied
with
resp
ect
to
the
one
from
Chapter
;
in
the
original
notation
w
e
w
ould
ha
v
e
written
T
=
ino
de(c
onj,
f(true,
left),
(false,
right)g).

	
CHAPTER
.
DECISION
TREES
IN
FIRST
ORDER
LOGIC
pro
cedure
predict(e
:
example)
returns
prediction:
Q
:=
tr
ue
N
:=
ro
ot
while
N
=
leaf
(y
)
do
let
N
=
ino
de(conj;
left
;
r
ig
ht)
if
 
Q
^
conj
succeeds
in
e
^
B
then
Q
:=
Q
^
conj
N
:=left
else
N
:=
r
ig
ht
return
y
Figure
.:
Making
a
prediction
for
an
example
using
a
F
OLDT
(with
bac
k-
ground
kno
wledge
B
).
the
conjunction
of
all
the
conjunctions
that
o
ccur
in
those
no
des
on
the
path
from
the
ro
ot
to
this
no
de
where
the
left
branc
h
w
as
c
hosen.
W
e
call
 
Q
the
asso
ciate
d
query
of
the
no
de.
When
an
example
is
sorted
to
the
left,
Q
is
up
dated
b
y
adding
c
onj
to
it.
When
sorting
an
example
to
the
righ
t,
Q
need
not
b
e
up
dated:
a
failed
test
nev
er
in
tro
duces
new
v
ariables.
Example
.
If
in
Figure
.
an
example
is
sorted
do
wn
the
tree,
in
the
no
de
con
taining
not
replaceable(X)
the
correct
test
to
b
e
p
erformed
is
worn(X),
not
replaceable(X);
it
is
not
correct
to
test
not
replaceable(X)
on
its
o
wn.

..
Seman
tics
of
F
OLDTs
Figure
.
sho
ws
ho
w
an
equiv
alen
t
logic
program
can
b
e
deriv
ed
from
a
F
OLDT.
Where
w
e
use
the
term
\logic
programs"
in
this
c
hapter,
it
refers
to
normal
logic
programs,
i.e.
programs
that
ma
y
con
tain
negativ
e
literals
in
the
b
o
dy
of
clauses.
When
the
latter
is
not
allo
w
ed,
w
e
will
explicitly
refer
to
denite
logic
programs.
With
eac
h
in
ternal
no
de
in
the
tree
a
clause
dening
a
newly
in
v
en
ted
n
ullary
predicate
is
asso
ciated,
as
w
ell
as
a
query
.
This
query
can
mak
e
use
of
the
predicates
dened
in
higher
no
des.
With
lea
v
es
only
a
query
is
asso
ciated,
no
clause.
The
queries
are
dened
in
suc
h
a
w
a
y
that
the
query
asso
ciated
with
a
no
de
succeeds
for
an
example
if
and
only
if
that
no
de
is
encoun
tered
when
that
example
is
sorted
do
wn
the
tree.
Therefore,
if
a
query
asso
ciated
with
a

..
FIRST
ORDER
LOGICAL
DECISION
TREES
	
pro
cedure
associa
te(T
:
foldt,
 
Q
:
query):
if
T
=
ino
de(conj;
left
;
r
ig
ht)
then
assign
a
unique
predicate
p
i
to
this
no
de
assert
p
i
 
Q;
conj
associa
te(left,
( 
Q;
conj
))
associa
te(r
ig
ht,
( 
Q;
:p
i
))
else
let
T
=
leaf
(k
)
assert
l
abel
(k
)
 
Q
pro
cedure
derive
logic
pr
ogram(T
:
foldt):
associa
te(T
,
 )
Figure
.:
Mapping
F
OLDT's
on
to
logic
programs.
leaf
succeeds,
the
leaf
indicates
the
lab
el
of
the
example.
The
clauses
dene
in
v
en
ted
predicates
that
are
needed
to
express
these
queries.
Th
us
the
queries
asso
ciated
with
lea
v
es,
together
with
these
clauses,
form
a
logic
program
that
is
equiv
alen
t
to
the
tree.
An
imp
ortan
t
p
oin
t
is
that
the
algorithm
adds
to
a
query
the
negation
of
the
in
v
en
ted
predicate
p
i
,
and
not
the
negation
of
the
conjunction
itself
(see
Figure
.:
the
query
 
Q;
:p
i
and
not
 
Q;
:conj
is
asso
ciated
with
the
righ
t
subtree
of
T
).
Indeed,
the
queries
of
the
left
and
righ
t
subtree
should
b
e
complemen
tary:
for
eac
h
example
sorted
in
to
this
no
de
(i.e.
 
Q
succeeds),
ex-
actly
one
of
b
oth
queries
should
succeed.
No
w,
 
Q;
conj
(whic
h
is
equiv
alen
t
to
 
Q;
p
i
)
and
 
Q;
:p
i
are
complemen
tary
,
but
 
Q;
conj
and
 
Q;
:conj
are
not,
when
conj
shares
v
ariables
with
Q.
F
or
instance,
in
the
in
terpretation
fq
();
p();
q
()g
b
oth
 
q
(X
);
p(X
)
and
 
q
(X
);
:p(X
)
succeed.
Example
.
Figure
.
sho
ws
the
result
of
applying
the
algorithm
in
Fig-
ure
.
to
our
running
example.
Consider
in
Figure
.
the
no
de
con
taining
not
replaceable(X).
The
query
asso
ciated
with
the
righ
t
subtree
of
this
no
de
con
tains
:p

and
not
:not
replaceable(X)
.
Indeed,
in
the
latter
case
the
query
w
ould
succeed
if
there
is
a
w
orn
part
in
the
mac
hine
that
is
replaceable,
while
it
ough
t
to
succeed
if
there
are
w
orn
parts
in
the
mac
hine,
but
al
l
of
them
are
replaceable.

Because
a
literal
and
its
negation
are
not
complemen
tary
,
adding
a
literal
is
not
equiv
alen
t
to
adding
the
negation
of
the
literal
while
at
the
same
time
switc
hing
the
branc
hes.
This
means
it
ma
y
b
e
in
teresting
to
allo
w
negated
literals
in
queries.

	
CHAPTER
.
DECISION
TREES
IN
FIRST
ORDER
LOGIC
#
"
 
!
worn(X)
 
p
0
 
w
or
n(X
)






P
P
P
P
P
P
#
"
 
!
not
replaceable(X)
 
w
or
n(X
)
p

 
w
or
n(X
);
not
r
epl
aceabl
e(X
)
ok
 
:p
0






P
P
P
P
P
P
sendback
 
w
or
n(X
);
not
r
epl
aceabl
e(X
)
fix
 
w
or
n(X
);
:p

p
0
 
worn
(X
)
p

 
worn
(X
);
not
replaceable
(X
)
label(sendback)
 
worn
(X
);
not
replaceable
(X
)
label(fix)
 
worn
(X
);
:p

label(ok)
 
:p
0
Figure
.:
The
tree
of
Figure
.,
with
asso
ciated
clauses
and
queries
added;
and
the
logic
program
deriv
ed
from
the
tree.

..
FIRST
ORDER
LOGICAL
DECISION
TREES
	
Example
.
In
our
running
example,
not
replaceable(X)
partitions
the
set
fe

;
e

;
e

g
(see
Example
.)
in
to
ffe

g;
fe

;
e

gg.
replaceable(X)
w
ould
partition
it
in
to
ffe

;
e

g;
fe

gg.

This
is
an
imp
ortan
t
dierence
with
the
prop
ositional
case,
where
a
test
(e.g.
X
<
)
and
its
negation
(X

)
alw
a
ys
generate
the
same
partition.
In
the
rst
order
con
text
they
ma
y
generate
dieren
t
partitions.
This
fact
and
its
inuence
on
the
tree-to-ruleset
con
v
ersion
are
new
ndings
that
ha
v
e
not
b
een
men
tioned
in
existing
literature
on
relational
decision
trees
(W
atanab
e
and
Rendell,
		;
Kramer,
		),
but
are
imp
ortan
t
for
a
correct
understanding
of
their
seman
tics.
..
On
the
Expressiv
eness
of
F
OLDTs
and
Other
F
orm-
alisms
While
the
logic
program
that
is
equiv
alen
t
to
a
decision
tree
t
ypically
con
tains
auxiliary
predicates
p
i
,
in
Prolog
these
can
b
e
a
v
oided
b
y
using
the
cut
op
erator.
W
e
then
get
a
rst
order
decision
list
(F
ODL):
label(sendback)
:-
worn(X),
not_replaceable(X
),
!.
label(fix)
:-
worn(X),
!.
label(ok).
In
general,
a
tree
can
alw
a
ys
b
e
transformed
in
to
a
decision
list
and
vice
v
ersa.
The
follo
wing
functions
d
l
and
tr
dene
suc
h
mappings
(@
represen
ts
concatenation
of
lists):

dl
(T
)
=
dl
0
(T
;
tr
ue)
dl
0
(leaf
(c);
P
C
)
=
[(label(c)
:-
P
C
,
!)]
dl
0
(ino
de(conj;
l
ef
t;
r
ig
ht);
P
C
)
=
dl
0
(l
ef
t;
(P
C
;
conj
))@dl
0
(r
ig
ht;
P
C
)
tr
([(label(c)
:-
conj
,
!
)jR
est]
=
ino
de(conj;
leaf
(c);
tr
(R
est))
tr
([label(c)
])
=
leaf(c)
This
establishes
the
equiv
alence
with
resp
ect
to
expressiv
eness
of
F
ODLs
and
F
OLDTs.
W
e
no
w
turn
to
the
relationship
with
logic
programs.
W
e
consider
only
non-recursiv
e
logic
programs
as
h
yp
otheses.
The
h
yp
o-
theses
essen
tially
dene
the
target
predicate
lab
el.
They
ma
y
also
dene
in-
vente
d
predicates
whic
h
do
not
o
ccur
in
the
bac
kground
theory
.
W
e
assume
here
that
the
bac
kground
is
not
c
hanged
b
y
the
learner
and
not
part
of
the

In
tr,
w
e
mak
e
use
of
the
fact
that
no
des
can
con
tain
conjunctions
of
literals.
If
only
one
literal
is
allo
w
ed
in
eac
h
no
de,
the
con
v
ersion
is
still
p
ossible
but
more
complex.

	
CHAPTER
.
DECISION
TREES
IN
FIRST
ORDER
LOGIC
h
yp
othesis.
Then
the
h
yp
otheses
H

and
H

are
equiv
alen
t
if
and
only
if
for
all
bac
kgrounds
B
,
B
^
H

assigns
the
same
lab
el
to
an
y
p
ossible
example
as
B
^
H

.
W
e
call
a
h
yp
othesis
in
the
form
of
a
logic
program
at
if
it
con
tains
no
in
v
en
ted
predicates,
otherwise
w
e
call
it
layer
e
d.
A
h
yp
othesis
in
the
form
of
a
la
y
ered
denite
logic
program
can
alw
a
ys
b
e
transformed
in
to
a
at
denite
logic
program
b
y
unfolding
calls
to
in
v
en
ted
predicates.
La
y
ered
normal
logic
programs,
ho
w
ev
er,
cannot
alw
a
ys
b
e
trans-
formed
to
at
normal
logic
programs
in
this
w
a
y
.
Unfolding
a
negativ
e
literal
for
an
in
v
en
ted
predicate
ma
y
in
tro
duce
univ
ersally
quan
tied
v
ariables
in
the
b
o
dy
,
whic
h
is
b
ey
ond
the
expressiv
e
p
o
w
er
of
logic
program
clauses
(b
y
deni-
tion,
v
ariables
not
o
ccurring
in
the
head
of
a
clause
are
existen
tially
quan
tied
in
its
b
o
dy).
Example
.
Unfolding
the
la
y
ered
logic
program
of
our
running
example
yields:
label(ok)
 
X
:
:worn
(X
)
label(sendback)
 
	X
:
worn
(X
)
^
not
replaceable
(X
)
label(fix)
 
	X
:
worn
(X
)^
Y
:
(:worn
(Y
)
_
:not
replaceable
(Y
))
Since
an
y
at
logic
program,
when
written
in
this
format,
only
con
tains
existen
tial
quan
tiers
(b
y
denition
of
logic
program
clauses),
no
at
h
yp
othesis
exists
that
is
equiv
alen
t
to
this
theory
(e.g.
X
:worn
(X
)
cannot
b
e
written
with
only
existen
tial
v
ariables).


W
e
conclude
from
the
ab
o
v
e
that
F
OLDTs
can
alw
a
ys
b
e
con
v
erted
to
la
y
ered
normal
logic
programs
(Figure
.
giv
es
the
algorithm),
but
not
alw
a
ys
to
at
normal
logic
programs.
Finally
,
observ
e
that
a
at
logic
program
that
predicts
only
one
lab
el
for
a
single
example
(whic
h
is
not
a
restrictiv
e
condition
in
the
con
text
of
prediction)
can
alw
a
ys
b
e
transformed
in
to
an
equiv
alen
t
decision
list
b
y
just
adding
a
cut
to
the
end
of
eac
h
clause.
As
F
ODLs
and
F
OLDTs
can
b
e
con
v
erted
in
to
one
another,
and
at
logic
programs
can
b
e
con
v
erted
in
to
F
ODLs
or
F
OLDTs
but
not
the
other
w
a
y
around,
w
e
ha
v
e
the
follo
wing
prop
ert
y:
In
the
le
arning
fr
om
interpr
etations
setting,
the
set
of
the
ories
that
c
an
b
e
r
epr
esente
d
by
F
OLDTs
is
a
strict
sup
erset
of
the
set
of

Using
n+
to
denote
Prolog's
unsound
v
ersion
of
negation
as
failure
(whic
h
do
es
not
c
hec
k
the
groundness
of
its
argumen
t),
one
migh
t
remark
that
e.g.
label(fix)
:-
worn(X),
n+
(worn(Y),
not
replaceable(Y))
correctly
computes
the
lab
el
fix.
Ho
w
ev
er,
w
e
do
not
call
this
a
at
program.
Op
erationally
,
n+
starts
a
sub
query
.
Declarativ
ely
,
the
meaning
is
lab
el(x)
 
worn(X),
:(	Y
(w
or
n(Y
);
not
r
epl
aceabl
e(Y
)))
whic
h
is
b
ey
ond
the
expressiv
e
p
o
w
er
of
a
normal
clause.

..
RELA
TED
W
ORK
		
the
ories
that
c
an
b
e
r
epr
esente
d
by
at
normal
lo
gic
pr
o
gr
ams,
and
is
e
quivalent
to
the
set
of
the
ories
that
c
an
b
e
r
epr
esente
d
by
F
ODLs.
All
this
means
that
systems
that
induce
trees
or
decision
lists
(examples
of
the
latter
are
FF
oil
(Quinlan,
		)
and
F
oidl
(Mo
oney
and
Cali,
		))
can
nd
theories
that
cannot
b
e
found
b
y
systems
that
induce
at
(normal
or
def-
inite)
logic
programs
(e.g.
F
oil
(Quinlan,
		b),
Pr
ogol
(Muggleton,
		)
and
most
other
ILP
systems).
This
extends
the
classical
claim
that
the
use
of
cuts
allo
ws
for
a
more
compact
represen
tation
(see,
e.g.,
(Mo
oney
and
Cali,
		))
with
the
claim
that
also
a
greater
expressivit
y
is
ac
hiev
ed.
The
same
expressivit
y
could
b
e
ac
hiev
ed
b
y
classical
ILP
systems
if
they
allo
w
negation
and
p
erform
predicate
in
v
en
tion
(or
if
they
allo
w
Prolog's
unsound
negation
as
failure:
n+
with
as
argumen
t
a
conjunction
of
literals,
whic
h
essen
tially
amoun
ts
to
the
same).
In
this
resp
ect
w
e
w
an
t
to
men
tion
Bain
and
Muggleton's
non-monotonic
induction
metho
d
(Bain
and
Muggleton,
		).
The
h
yp
otheses
generated
with
this
metho
d
ha
v
e
a
structure
similar
to
that
of
F
OLDTs
(when
only
t
w
o
classes
are
in
v
olv
ed),
in
that
the
induced
theory
is
t
ypically
also
la
y
ered
through
the
use
of
in
v
en
ted
predicates,
and
the
in
v
en
ted
predicates
o
ccur
as
negativ
e
literals
in
the
clauses,
accomo
dating
exceptions
to
them.
Ho
w
ev
er,
in
Bain
and
Muggleton's
framew
ork
the
learning
metho
d
is
incremen
tal
and
rule-based.
.
Related
W
ork
This
c
hapter
is
based
on
(Blo
c
k
eel
and
De
Raedt,
		b).
Of
the
existing
decision
tree
approac
hes
to
relational
learning
(W
atanab
e
and
Rendell,
		;
Kramer,
		;
Manago,
		;
Bergadano
and
Giordana,
	;
Bostr
om,
		;
Geib
el
and
Wysotzki,
		;
Bo
w
ers,
		),
Str
uct
(W
atanab
e
and
Rendell,
		)
and
SR
T
(Kramer,
		)
are
most
relev
an
t
to
our
approac
h;
they
b
oth
induce
the
kind
of
logical
decision
trees
w
e
ha
v
e
discussed.
This
w
ork,
ho
w
ev
er,
has
fo
cused
on
induction
tec
hniques
and
has
ignored
the
logical
and
represen
tational
asp
ects
of
decision
trees,
needed
to
fully
understand
the
p
oten
tial
of
this
tec
hnique
for
rst-order
learning.
Bostr
om's
w
ork
(Bostr
om,
		)
has
in
common
with
ours
that
he
compared
the
co
v
ering
and
divide-and-conquer
paradigms
in
the
con
text
of
ILP
.
The
algorithm
he
pro
vides
emplo
ys
a
divide-and-conquer
approac
h
and
is
in
this
resp
ect
similar
to
TDIDT.
Ho
w
ev
er,
with
his
metho
d
the
resulting
theory
is
still
a
at
program
(eac
h
leaf
in
the
tree
that
is
built
con
tains
one
clause;
b
y
gathering
all
the
lea
v
es
that
predict
one
sp
ecic
class,
a
logic
program
is
obtained
that
denes
that
class).
P
eter
Geib
el's
Tritop
system
(Geib
el
and
Wysotzki,
		)
induces
rst
order
logical
decision
trees
where
dieren
t
no
des
cannot
share
v
ariables.
The

00
CHAPTER
.
DECISION
TREES
IN
FIRST
ORDER
LOGIC
approac
h
is
still
relational
b
ecause
complex
conjunctions
can
o
ccur
in
individual
no
des.
One
could
sa
y
that
the
system
constructs
prop
ositional
features
in
a
relational
domain,
and
uses
these
features
as
tests
in
the
tree.
The
tree
itself
can
th
us
b
e
seen
as
prop
ositional.
A
similar
approac
h,
learning
from
prop
ositionalized
rst
order
kno
wledge,
is
describ
ed
in
(Kramer
et
al.,
		).
The
mac
hine
learning
group
at
Bristol
(Llo
yd,
Flac
h,
Bo
w
ers)
is
studying
structural
decision
trees
in
the
con
text
of
the
functional
programming
language
Esc
her.
Their
approac
h
is
similar
to
ours
in
that
the
represen
tation
of
an
ex-
ample
resem
bles
the
in
terpretations
w
e
use.
Ho
w
ev
er,
as
in
Geib
el's
approac
h,
dieren
t
no
des
in
a
tree
do
not
share
v
ariables.
An
early
implemen
tation
of
their
relational
decision
tree
learner
is
describ
ed
b
y
Bo
w
ers
(		).
The
results
concerning
expressivit
y
of
trees
and
rule
sets
are
quite
dieren
t
from
those
obtained
for
prop
ositional
learning
systems.
Riv
est
(	)
has
com-
pared
the
expressivit
y
of
DNF
form
ulae,
CNF
form
ulae,
decision
lists
and
de-
cision
trees
in
the
prop
ositional
case,
and
has
sho
wn
that
prop
ositional
decision
lists
are
more
expressiv
e
than
the
other
formalisms,
given
a
xe
d
maximum
on
the
length
of
rules
and
the
depth
of
the
tr
e
e;
i.e.,
the
set
of
theories
that
can
b
e
represen
ted
b
y
decision
lists
with
rules
of
length
at
most
k
strictly
includes
the
set
of
theories
that
can
b
e
represen
ted
b
y
decision
trees
with
maximal
depth
k
,
as
w
ell
as
the
set
of
theories
that
can
b
e
represen
ted
in
k
-CNF
or
k
-DNF
format.
Without
suc
h
complexit
y
b
ounds,
prop
ositional
theories
can
alw
a
ys
b
e
transformed
in
to
an
y
of
the
alternativ
e
formats,
and
all
formats
are
equiv
alen
t
in
this
sense.
.
Conclusions
Earlier
in
this
text
w
e
ha
v
e
observ
ed
that
the
TDIDT
approac
h
is
v
ery
suc-
cessful
in
prop
ositional
learning
and
diers
signican
tly
from
the
co
v
ering
ap-
proac
h.
This
suggests
that
in
ILP
,
to
o,
TDIDT
migh
t
ha
v
e
adv
an
tages
o
v
er
the
co
v
ering
approac
h
(this
p
oin
t
w
as
also
raised
and
in
v
estigated
b
y
Bostr
om
(		)).
In
an
attempt
to
mak
e
the
TDIDT
paradigm
more
attractiv
e
to
ILP
w
e
ha
v
e
in
v
estigated
the
logical
asp
ects
of
rst
order
decision
trees.
The
res-
ulting
framew
ork
should
pro
vide
a
sound
basis
for
rst
order
logical
decision
tree
induction.
Our
in
v
estigation
sho
ws
that
rst
order
logical
decision
trees
are
more
ex-
pressiv
e
than
the
at
non-recursiv
e
logic
programs
t
ypically
induced
b
y
ILP
systems
for
prediction
tasks,
and
that
this
expressiv
e
p
o
w
er
is
related
to
the
use
of
cuts,
or
the
use
of
negation
com
bined
with
predicate
in
v
en
tion.
This
in
turn
relates
our
w
ork
to
some
of
the
w
ork
on
induction
of
decision
lists
and
predicate
in
v
en
tion
(Bain
and
Muggleton,
		;
Quinlan,
		;
Mo
oney
and
Cali,
		),
sho
wing
that
these
algorithms
to
o
ha
v
e
an
expressivit
y
adv
an
t-
age
o
v
er
algorithms
inducing
at
logic
programs.
These
expressivit
y
results
are

..
CONCLUSIONS
0
sp
ecic
for
the
rst
order
case;
they
do
not
hold
for
prop
ositional
learning.

0
CHAPTER
.
DECISION
TREES
IN
FIRST
ORDER
LOGIC

Chapter

T
op-do
wn
Induction
of
First
Order
Logical
Decision
T
rees
.
In
tro
duction
In
this
c
hapter
w
e
discuss
the
induction
of
decision
trees
in
the
learning
from
in
terpretations
setting,
as
implemen
ted
in
the
Tilde
system
(Blo
c
k
eel
and
De
Raedt,
		b).
This
c
hapter
builds
on
Chapter
,
where
w
e
in
tro
duced
rst
order
logical
decision
trees,
and
on
Chapter
,
where
w
e
discussed
top-
do
wn
induction
of
decision
trees
in
the
predictiv
e
clustering
framew
ork.
W
e
rst
presen
t
the
general
arc
hitecture
of
Tilde.
Next,
w
e
discuss
the
features
of
the
system
in
more
detail.
This
discussion
consists
of
t
w
o
parts:
rst
w
e
lo
ok
at
the
w
a
y
in
whic
h
tec
hniques
from
prop
ositional
learning
can
b
e
adapted
or
upgraded
to
w
ards
rst
order
logic
(Section
.);
next,
w
e
dis-
cuss
Tilde
as
an
instan
tiation
of
the
predictiv
e
clustering
tec
hnique
presen
ted
b
efore
(Section
.).
After
ha
ving
discussed
the
algorithms
used
b
y
Tilde,
w
e
illustrate
the
use
of
the
system
with
a
sample
run
in
Section
..
The
dis-
cussion
of
the
implemen
tation
is
concluded
with
some
eciency
considerations
(Section
.).
Section
.
consists
of
an
empirical
ev
aluation
of
Tilde,
in
whic
h
the
system
is
compared
with
other
inductiv
e
learners
and
the
eect
of
certain
implemen
ta-
tion
details
is
in
v
estigated.
The
fo
cus
of
this
ev
aluation
is
on
the
use
of
Tilde
for
classication,
although
other
tasks
are
considered
as
w
ell.
The
c
hapter
ends
with
a
short
discussion
of
related
w
ork
and
conclusions.
0

0
CHAPTER
.
TOP-DO
WN
INDUCTION
OF
F
OLDTS
classification
regression
clustering
environment for
experimentation
interaction
output
settings
knowledge base
results
TDIDT
task dependent procedures
user
ILP utilities
core
refinement
data access
general purpose utilities
Figure
.:
Arc
hitecture
of
the
Tilde
system.
Arro
ws
denote
information
o
w.
.
Arc
hitecture
of
Tilde
The
arc
hitecture
of
the
Tilde
system

is
sk
etc
hed
in
Figure
..
A
t
the
core
of
the
Tilde
system
is
the
generic
TDIDT
algorithm.
The
auxiliary
mo
dules
can
b
e
divided
in
to
three
groups:

One
group
implemen
ts
the
ILP
part
of
the
system:
it
con
tains
the
co
de
for
applying
a
user-dened
renemen
t
op
erator
to
a
clause
(in
order
to
generate
the
set
of
tests
that
is
to
b
e
considered
at
a
no
de),
as
w
ell
as
the
co
de
for
accessing
the
data
(testing
a
clause
in
an
example,
obtaining
the
class
of
an
example,
.
.
.
).
The
mo
dules
in
this
group
mak
e
direct
use
of
Prolog
functionalit
y
.

A
second
group
implemen
ts
all
the
pro
cedures
that
are
sp
ecic
to
cer-
tain
tasks.
This
group
con
tains
three
mo
dules:
classication,
regres-
sion
and
clustering.
Eac
h
mo
dule
instan
tiates
the
OPTIMAL
SPLIT,
STOP
CRIT,
INF
O
and
PR
UNE
functions
referred
to
b
y
the
generic
TDIDT
algorithm
in
Figure
.,
denes
qualit
y
criteria
for
trees,
a
p
ost-
pruning
metho
d,
etc.
Tilde
alw
a
ys
uses
exactly
one
of
these
three
mo
d-
ules,
according
to
the
mo
de
it
is
in.

A
third
group
con
tains
v
arious
auxiliary
mo
dules
that
are
not
directly
related
to
an
y
sp
ecic
task
or
to
the
ILP
c
haracter
of
the
system;
these
include
user
in
teraction,
pro
cedures
for
writing
trees
in
a
readable
format,
con
v
ersion
of
trees
to
Prolog
programs,
facilities
for
testing
h
yp
otheses
(e.g.
cross-v
alidation,
ev
aluation
on
separate
test
set),
etc.

More
sp
ecically
Tilde.0.
An
earlier
v
ersion,
Tilde.,
only
p
erforms
classication.

..
UPGRADING
TDIDT
TO
FIRST
ORDER
LOGIC
0
The
system
tak
es
t
w
o
t
yp
es
of
input:
a
settings
le,
sp
ecifying
the
v
arious
parameters
of
the
system
(whic
h
mo
de
it
should
run
in,
the
language
bias,
.
.
.
),
and
a
kno
wledge
base
(divided
in
to
a
set
of
examples
and
bac
kground
kno
wledge).
It
generates
one
or
more
output
les
con
taining
the
results
of
the
induction
pro
cess.
The
arc
hitecture
emphasizes
the
similarit
y
b
et
w
een
the
three
tasks
for
whic
h
Tilde
can
b
e
used.
The
part
of
the
co
de
that
is
task-sp
ecic
is
clearly
isol-
ated
and
turns
out
to
b
e
relativ
ely
small
(a
rough
estimate
based
on
the
le
sizes
suggests
this
part
is
ab
out
%
of
the
total
co
de).
The
system
has
b
een
designed
so
that
new
mo
des
can
b
e
added
relativ
ely
easily
.
The
Tilde
system
is
implemen
ted
in
Prolog,
and
runs
within
the
Master-
ProLog
engine
(the
former
ProLog-b
y-BIM)

.
F
rom
the
ab
o
v
e
it
will
b
e
clear
that
it
is
mainly
the
rst
group
of
mo
dules
(the
ILP
part
of
the
co
de)
that
fully
exploits
the
functionalit
y
oered
b
y
Prolog.
.
Upgrading
TDIDT
to
First
Order
Logic
In
this
section
w
e
discuss
the
adaptations
that
ha
v
e
b
een
made
to
the
general
TDIDT
algorithm
to
upgrade
it
to
the
rst
order
con
text.
Figure
.
sho
ws
the
basic
TDIDT
algorithm,
but
no
w
in
an
ILP
con
text,
where
the
tests
are
rst
order
conjunctions.
W
e
rst
discuss
the
w
a
y
in
whic
h
the
computation
of
tests
for
no
des
is
adapted;
next,
w
e
briey
discuss
ho
w
a
prop
ositional
discretization
algorithm
has
b
een
upgraded
to
the
rst
order
con
text.
..
Computing
the
Set
of
T
ests
for
a
No
de
The
main
p
oin
t
where
Tilde
diers
from
prop
ositional
tree
learners
is
the
computation
of
the
set
of
tests
to
b
e
considered
at
a
no
de.
T
o
this
aim,
it
em-
plo
ys
a
user-dened
renemen
t
op
erator
under

-subsumption
(Plotkin,
	0;
Muggleton
and
De
Raedt,
		).
Denition
.
(
-subsumption)
A
clause
c


-subsumes
a
clause
c

(we
write
this
as
c



c

)
if
and
only
if
ther
e
exists
a
variable
substitution

such
that
Lits
(c


)

Lits
(c

)
with
Lits(c)
the
set
of
liter
als
o
c
curring
in
a
clause
c
when
it
is
written
as
a
disjunction.
Example
.
The
clause
c

:
p(X
;
Y
)
 
q
(X
;
Y
)

-subsumes
the
clauses
c

:
p(X
;
Y
)
 
q
(X
;
Y
);
r
(X
)
c

:
p(X
;
a)
 
q
(X
;
a)

A
compiled
v
ersion
exists
that
runs
outside
the
MasterProLog
in
terpreter;
this
v
ersion
oers
sligh
tly
less
functionalit
y
.

0
CHAPTER
.
TOP-DO
WN
INDUCTION
OF
F
OLDTS
pro
cedure
gr
o
w
tree(T
:
tree,
E
:
set
of
examples,
Q:
query):
 
Q
b
:=
OPTIMAL
SPLIT(( 
Q),
E
)
if
STOP
CRIT( 
Q
b
,
E
)
then
T
:=
leaf(INF
O(E
))
else
c
onj
:=
Q
b
 Q
E

:=
fe

E
j
 
Q
b
succeeds
in
e
^
B
g
E

:=
fe

E
j
 
Q
b
fails
in
e
^
B
g
gr
o
w
tree(left,
E

,
Q
b
)
gr
o
w
tree(right,
E

,
Q)
T
:=
ino
de(c
onj,
left,
right)
pro
cedure
Tilde(T
:
tree,
E
:
set
of
examples):
gr
o
w
tree(T
0
,
E
,
true)
PR
UNE(T
0
,
T
)
Figure
.:
Algorithm
for
rst-order
logical
decision
tree
induction.
b
ecause
Lits(c

)

Lits(c

)
(
is
then
the
empt
y
substitution)
and
c

is
ob-
tained
b
y
substituting
the
constan
t
a
for
the
v
ariable
Y
.

In
termezzo

oers
some
bac
kground
on

-subsumption
and
wh
y
it
is
im-
p
ortan
t
in
ILP
.
Denition
.
(Renemen
t
op
erator)
A
r
enement
op
er
ator
under

-sub-
sumption

maps
a
clause
c
onto
a
set
of
clauses,
such
that
c
0

(c);
c


c
0
.
In
Tilde
the
user-dened
renemen
t
op
erator

alw
a
ys
consists
of
adding
a
conjunction
to
a
clause;
a
v
ariable
substitution
can
b
e
sim
ulated
b
e
adding
a
unication
literal
(=).
In
order
to
rene
a
no
de
with
asso
ciated
query
 
Q,
Tilde
computes
( 
Q)
and
c
ho
oses
that
query
 
Q
b
in
it
that
results
in
the
b
est
split.
The
conjunction
put
in
the
no
de
consists
of
Q
b
 Q,
i.e.
the
literals
that
ha
v
e
b
een
added
to
Q
in
order
to
pro
duce
Q
b
.
Sp
ecifying
the
Basic
Renemen
t
Op
erator
The
sp
ecic
renemen
t
op
erator
that
is
to
b
e
used,
is
dened
b
y
the
user
in
a
Pr
ogol-lik
e
manner
(Muggleton,
		).
A
set
of
sp
ecications
of
the
form
rmode(n:
c
onjunction
)
is
pro
vided,
indicating
whic
h
conjunctions
can
b
e
added
to
a
query
,
the
maximal
n
um
b
er
of
times
the
conjunction
can
b
e
added
(n),
and
the
mo
des
and
t
yp
es
of
the
v
ariables
in
it.

..
UPGRADING
TDIDT
TO
FIRST
ORDER
LOGIC
0
In
termezzo
:

-subsumption

-subsumption,
rst
dened
b
y
Plotkin
(	0),
is
a
crucial
concept
in
ILP;
it
has
b
een
studied
extensiv
ely
and
almost
all
ILP
systems
emplo
y
it
in
one
w
a
y
or
another.
W
e
summarize
the
basics.
As
Denition
.
states,
c



c

,
	
:
Lits(c


)

Lits(c

).
Example
.
in
the
text
suggests
that
if
c


-subsumes
c

,
it
also
en
tails
it.
Indeed,

-
subsumption
can
b
e
seen
as
a
sound
but
incomplete
v
ersion
of
en
tailmen
t.
An
example
where
c

en
tails
c

but
do
es
not

-subsume
it
is
c

=
p(Y
)
 
p(X
);
s(X
;
Y
)
c

=
p(Z
)
 
p(X
);
s(X
;
Y
);
s(Y
;
Z
)
Despite
this
shortcoming,

-subsumption
is
p
opular
in
ILP
b
ecause
it
is
m
uc
h
c
heap
er
to
compute
than
en
tailmen
t.
It
is
useful
b
ecause
it
imp
oses
a
quasi-
order
on
a
set
of
clauses,
and
this
quasi-order
is
used
to
structure
the
searc
h
space.
A
quasi-or
der

is
a
reexiv
e
and
transitiv
e
relation
that
do
es
not
satisfy
an
tisymmetry
,
i.e.
it
is
p
ossible
that
a

b
and
b

a
without
a
and
b
b
eing
equal.
W
e
dene
the
relation

as
follo
ws:
a

b
,
a

b
^
b

a.
The

relation
is
symmetric
and
inherits
the
reexivit
y
and
transitivit
y
of
,
hence
it
is
an
equiv
alence
relation.
Eac
h
quasi-order
th
us
induces
an
equiv
alence
relation
in
its
domain,
and
a
partial
order
on
the
equiv
alence
classes.
W
e
write
the
equiv
alence
relation
induced
b
y


as


.
If
a


b,
w
e
call
a
and
b
syntactic
variants.
Since

-subsumption
implies
en
tailmen
t,
the


relation
implies
logical
equiv
alence.
This
means
that
in
a
set
of
syn
tactic
v
arian
ts
only
one
form
ula
needs
to
b
e
tested
in
order
to
obtain
the
truth
v
alue
of
ev
ery
form
ula
in
the
set;
or,
in
the
con
text
of
TDIDT:
w
e
need
generate
only
one
form
ula
for
eac
h
set
of
syn
tactic
v
arian
ts.
A
renemen
t
op
erator
under

-subsumption

maps
a
clause
c
on
to
a
set
of
clauses,
suc
h
that
c
0

(c);
c


c
0
.
A
go
o
d
renemen
t
op
erator
a
v
oids
generating
clauses
that
are
syn
tactic
v
arian
ts
of
one
another.
The
imp
ortance
of
renemen
t
op
erators
under

-subsumption
in
ILP
can
most
easily
b
e
seen
b
y
lo
oking
at
ho
w
h
yp
otheses
are
t
ypically
induced
b
y
mac
hine
learning
systems.
Usually
,
an
inductiv
e
learner
starts
with
a
coarse
h
yp
o-
thesis
and
generates
a
series
of
increasingly
more
rened
h
yp
otheses
un
til
an
acceptable
one
is
found.
Renemen
t
op
erators
under

-subsumption
pro
vide
a
formalization
of
this
renemen
t
pro
cess;
they
ha
v
e
b
een
studied
extensiv
ely
(e.g.
(v
an
der
Laag
and
Nienh
uys-Cheng,
		)).

0
CHAPTER
.
TOP-DO
WN
INDUCTION
OF
F
OLDTS
The
mo
de
of
an
argumen
t
is
indicated
b
y
a
+,
 or
+ sign
b
efore
a
v
ariable.
+
stands
for
input:
the
v
ariable
should
already
o
ccur
in
the
asso
ciated
query
of
the
no
de
where
the
test
is
put.
 stands
for
output:
the
v
ariable
has
to
b
e
one
that
do
es
not
o
ccur
y
et.
+ means
that
the
argumen
t
can
b
e
b
oth
input
and
output;
i.e.,
the
v
ariable
can
b
e
a
new
one
or
an
already
existing
one.
Note
that
the
names
of
the
v
ariables
in
the
rmode
facts
are
formal
names;
when
the
literal
is
added
to
a
clause
actual
v
ariable
names
are
substituted
for
them.
Example
.
In
the
Mac
hines
example
w
e
in
tro
duced
in
Chapter
,
the
fol-
lo
wing
rmode
sp
ecications
could
b
e
giv
en:
rmode(:
worn(+-V)).
rmode(:
not_replaceable(+
V)
).
rmode(:
replaceable(+V)).
This
rmode
denition
tells
Tilde
that
a
test
in
a
no
de
ma
y
consist
of
c
hec
king
whether
a
comp
onen
t
that
has
already
b
een
referred
to
is
w
orn
(e.g.
worn(X)
with
X
an
already
existing
v
ariable),
c
hec
king
whether
there
exists
an
w
orn
comp
onen
t
in
the
mac
hine
(e.g.
worn(Y)
with
Y
not
o
ccurring
in
the
asso
ciated
query),
or
testing
whether
a
comp
onen
t
that
has
already
b
een
referred
to
is
replaceable.
A
t
most

literals
of
a
certain
t
yp
e
can
o
ccur
on
an
y
path
from
ro
ot
to
leaf
(this
is
indicated
b
y
the

in
the
rmode
facts).
T
o
mak
e
this
a
bit
more
concrete,
lo
ok
bac
k
at
the
tree
in
Figure
..
The
left
c
hild
of
the
ro
ot
has
as
asso
ciated
query
 worn(X).
The
renemen
t
op
erator

dened
b
y
the
ab
o
v
e
sp
ecications
generates
for
this
query
( 
worn(X)
)
=
f
 
worn(X);
replaceable(X),
 
worn(X);
not
replaceable(X),
 
worn(X);
worn(Y),
 
worn(X);
worn(X)
g
where
the
fourth
clause,
b
eing
in
fact
equal
to
the
original
clause,
is
ltered
out.
Hence,
the
three
literals
that
are
considered
for
this
no
de
are
replaceable(X),
not
replaceable(X)
and
worn(Y).
Of
these
three
the
second
one
is
preferred
b
y
the
heuristic
and
lled
in
in
the
no
de.

A
conjunction
can
ha
v
e
m
ultiple
mo
de
declarations,
e.g.
the
follo
wing
facts
sp
ecify
that
at
least
one
of
the
t
w
o
argumen
ts
of
a
predicate
inside
has
to
b
e
input:
rmode(:
inside(+V,+-W)).
rmode(:
inside(-V,+W)).

..
UPGRADING
TDIDT
TO
FIRST
ORDER
LOGIC
0	
T
yp
es
Tilde
allo
ws
the
user
to
sp
ecify
t
yp
es
for
literals.
When
adding
a
new
literal,
the
v
ariables
in
it
can
only
b
e
unied
with
v
ariables
that
ha
v
e
the
same
t
yp
e.
This
often
results
in
a
h
uge
reduction
of
the
branc
hing
factor.
A
declaration
typed
language(yes)
tells
the
system
that
it
should
tak
e
t
yp
e
information
in
to
accoun
t.
F
or
eac
h
predicate
the
t
yp
es
of
its
argumen
ts
are
then
giv
en
b
y
facts
of
the
form
type(t

,
...,
t
a
)
with
a
the
arit
y
of
the
predicate.
Multiple
t
yp
e
sp
ecications
can
b
e
giv
en
for
a
predicate.
Example
.
Supp
ose
the
follo
wing
rmodes
are
giv
en:
rmode(:
atom(+-ID,
+-Elem,
-Charge)).
rmode(:
bond(+ID,+-ID,+
-B
ID)
).
rmode(:
+Charge<0).
If
at
some
p
oin
t
a
no
de
with
asso
ciated
query
 atom(A,E,C),
bond(A,
A,
B),
atom(A,E,C)
is
to
b
e
rened,
a
<
literal
can
b
e
added
in
man
y
w
a
ys:
fA
<
0;
E
<
0;
C
<
0;
A
<
0;
B
<
0;
E
<
0;
C
<
0g.
Of
these
only
C
<
0
and
C
<
0
are
useful:
w
e
only
w
an
t
to
compare
c
harges
with
0,
as
the
other
v
ariables
are
not
n
umerical.
The
c
hoice
of
v
ariable
names
in
the
rmode
sp
ecications
reects
this;
ho
w
ev
er,
while
v
ariable
names
migh
t
b
e
suggestiv
e
to
the
user,
Tilde
do
es
not
tak
e
them
in
to
accoun
t.
The
problem
is
easily
solv
ed
b
y
adding
t
yp
e
declarations,
as
follo
ws:
typed_language(y
es)
.
type(atom(id,
elem,
real)).
type(bond(id,
id,
bondtype)).
type(real<real).
Tilde
will
no
w
only
add
a
<
literal
if
the
v
ariable
to
the
left
is
of
t
yp
e
real,
whic
h
is
only
the
case
for
C

and
C

in
the
ab
o
v
e
example.
Note
that
these
t
yp
e
sp
ecications
also
ha
v
e
an
inuence
on
the
n
um
b
er
of
atom
and
bond
literals
that
ha
v
e
to
b
e
considered.
F
or
instance,
since

v
ariables
o
ccur
in
the
asso
ciated
query
and
the
rst
t
w
o
argumen
ts
of
atom
can
b
e
unied
with
already
o
ccurring
v
ariables
or
can
b
e
new,

dieren
t
literals
can
b
e
generated
for
atom;
using
t
yp
es
reduces
this
to
.

Generation
of
Constan
ts
As
the
ab
o
v
e
example
illustrates,
it
is
p
ossible
to
use
literals
with
constan
t
para-
meters
instead
of
v
ariables.
Unfortunately
,
there
are
man
y
situations
where
lots
of
constan
ts
can
b
e
useful,
so
that
one
could
ha
v
e
a
sp
ecication
suc
h
as

0
CHAPTER
.
TOP-DO
WN
INDUCTION
OF
F
OLDTS
rmode(:
+X
<
-).
rmode(:
+X
<
-0.).
rmode(:
+X
<
0).
rmode(:
+X
<
0.).
rmode(:
+X
<
).
...
F
ortunately
,
Tilde
can
generate
suitable
constan
ts
itself.
It
do
es
this
in
the
follo
wing
w
a
y:
for
eac
h
example
in
the
set
of
examples
co
v
ered
b
y
a
no
de,
a
query
is
run
that
generates
one
or
more
constan
ts.
Eac
h
of
these
constan
ts
is
then
lled
in
in
the
test
that
is
to
b
e
put
in
this
no
de.
In
order
to
k
eep
the
branc
hing
factor
of
the
searc
h
space
limited,
maxima
can
b
e
giv
en
for
the
n
um
b
er
of
examples
Tilde
should
lo
ok
at,
as
w
ell
as
for
the
n
um
b
er
of
constan
ts
that
can
b
e
generated
from
eac
h
example.
W
e
illustrate
the
constan
t
generation
algorithm
with
an
example.
Example
.
W
e
again
use
the
Mac
hines
example.
T
o
tell
Tilde
that
a
test
worn(c)
can
b
e
used,
with
c
b
eing
an
y
constan
t
for
whic
h
worn
could
p
ossibly
succeed,
the
follo
wing
rmode
fact
could
b
e
used:
rmode(:
#(**X:
worn(X),
worn(X))).
%
a
d
c
b
e
This
sp
ecication
means
that
in
at
most

(a)
examples,
Tilde
should
run
worn(X)
(b)
and
see
whic
h
v
alues
X
(c)
can
tak
e;
it
should
return
at
most

(d)
v
alues
p
er
example.
Finally
,
the
test
worn(X)
(e)
will
b
e
put
in
the
no
de,
but
with
X
c
hanged
in
to
one
of
the
constan
ts:
worn(gear),
worn(engine),
.
.
.

In
the
ab
o
v
e
example,
the
constan
t
generating
predicate
is
the
same
as
the
predicate
that
is
to
b
e
lled
in,
but
this
need
not
b
e
the
case.

Another
example
of
the
use
of
constan
t
generation,
no
w
in
a
con
tin
uous
domain,
is:
rmode(0:
#(00**C:
boundary(C),
+X
<
C)).
In
at
most
00
examples
one
n
umeric
b
oundary
will
b
e
computed,
and
a
test
should
consist
of
comparing
an
already
o
ccurring
v
ariable
X
with
this
b
ound-
ary
.
The
computation
of
a
suitable
b
oundary
can
b
e
dened
in
bac
kground
kno
wledge.
It
migh
t
b
e
done
b
y
,
e.g.,
a
discretization
algorithm
(see
further).
While
the
ab
o
v
e
syn
tax
ma
y
b
e
a
bit
a
wkw
ard,
it
is
v
ery
general
and
allo
ws
the
generation
of
constan
ts
in
man
y
dieren
t
settings.
It
is
ev
en
p
ossible
to
generate
a
whole
literal
(instead
of
only
its
constan
t
parameters),
one
could

When
the
generating
predicate
is
the
same
as
the
predicate
that
is
to
b
e
lled
in,
this
metho
d
mimic
ks
the
lazy
evaluation
tec
hnique
as
implemen
ted
in
Pr
ogol.
(Sriniv
asan
and
Camac
ho,
		).

..
UPGRADING
TDIDT
TO
FIRST
ORDER
LOGIC

for
instance
generate
a
n
umerical
computation
suc
h
as
Y
is
.*X*X
or
a
function
f(X,Y,Z)
where
the
denition
of
f
is
computed
at
run
time.
As
another
example,
the
metho
d
for
predicate
in
v
en
tion
that
is
incorp
orated
in
Pr
ogol.
(Khan
et
al.,
		)
can
also
b
e
sim
ulated
with
the
#-construct.

Lo
ok
ahead
An
imp
ortan
t
problem
in
ILP
is
that
renemen
t
of
a
clause
b
y
adding
a
single
literal
ma
y
result
in
little
immediate
impro
v
emen
t,
although
the
literal
ma
y
in-
tro
duce
new
v
ariables
that
are
v
ery
relev
an
t.
The
follo
wing
example
illustrates
the
problem.
Example
.
In
Belgium
a
p
erson
can
apply
for
a
driv
er's
license
from
the
age
of
eigh
tteen.
When
presen
ted
with
a
suitable
set
of
examples,
an
inductiv
e
learner
should
b
e
able
to
come
up
with
the
follo
wing
rule:
label(can_apply)
:-
age(X),
X>=.
If
the
system
learns
rules
top-do
wn,
i.e.
it
starts
with
an
empt
y
b
o
dy
and
adds
literals
to
it,
then
b
efore
nding
the
correct
rule
it
has
to
generate
one
of
the
follo
wing
rules:
label(can_apply)
:-
age(X).
()
label(can_apply)
:-
X>=.
()
Unfortunately
,
none
these
rules
mak
e
an
y
sense.
The
b
o
dy
of
rule
()
succeeds
for
eac
h
and
ev
ery
p
erson
(ev
ery
one
has
an
age),
hence
addition
of
the
literal
age(X)
do
es
not
yield
an
y
impro
v
emen
t
o
v
er
the
empt
y
rule.
Rule
()
only
imp
oses
a
constrain
t
on
a
v
ariable
that
has
no
meaning
y
et.
In
Prolog
suc
h
a
b
o
dy
alw
a
ys
fails,
in
a
constrain
t
logic
programming
language
it
migh
t
alw
a
ys
succeed;
but
in
neither
case
will
it
yield
an
y
gain.
A
top-do
wn
rule
learner
ma
y
discard
b
oth
rules
b
ecause
they
do
not
seem
to
bring
it
an
y
closer
to
a
solution,
thereb
y
failing
to
nd
the
solution
when
it
is
only
one
step
a
w
a
y
from
it.

While
the
problem
is
easiest
to
explain
using
a
rule-based
inductiv
e
learner,
it
also
arises
for
tree-based
systems.
The
problem
is
inheren
t
to
heuristic
searc
hes
in
general.
F
or
greedy
systems,
it
ma
y
hea
vily
inuence
the
induction
pro
cess.
Although
some
systems
ha
v
e
pro
visions
for
alleviating
the
problem
in
sp
ecic
cases
(e.g.
F
OIL
(Quinlan,
		b)
automatically
adds
so-called
determinate
literals

,
suc
h
as
the
age
literal
in
the
ab
o
v
e
example),
it
has
not
b
een
solv
ed
satisfactorily
y
et.

This
metho
d
consists
of
applying
Sriniv
asan
and
Camac
ho's
lazy
ev
aluation
tec
hnique
to
generate
not
a
constan
t
but
a
predicate.

Giv
en
a
clause,
a
literal
to
b
e
added
to
that
clause
is
called
determinate
if
all
its
free
v
ariables
can
tak
e
at
most
one
v
alue.


CHAPTER
.
TOP-DO
WN
INDUCTION
OF
F
OLDTS
One
tec
hnique
for
coping
with
the
problem
is
to
mak
e
the
learner
lo
ok
ahe
ad
in
the
renemen
t
lattice.
When
a
literal
is
added,
the
qualit
y
of
the
renemen
t
can
b
etter
b
e
assessed
b
y
lo
oking
at
the
additional
renemen
ts
that
will
b
ecome
a
v
ailable
after
this
one,
and
lo
oking
at
ho
w
go
o
d
these
are.
This
tec
hnique
is
computationally
exp
ensiv
e,
but
ma
y
lead
to
signican
t
impro
v
emen
ts
in
the
induced
theories.
There
are
sev
eral
w
a
ys
in
whic
h
lo
ok
ahead
can
b
e
p
erformed.
One
is
to
lo
ok
at
further
renemen
ts
in
order
to
ha
v
e
a
b
etter
estimate
for
the
curren
t
renemen
t.
In
that
case,
the
heuristic
v
alue
assigned
to
a
renemen
t
c
0
of
a
clause
c
is
a
function
of
c
0
and
(c
0
),
with

the
user-dened
renemen
t
op
erator.

itself
do
es
not
c
hange
with
this
form
of
lo
ok
ahead.
A
second
kind
of
lo
ok
ahead
is
to
redene
the
renemen
t
op
erator
itself
so
that
the
t
w
o-step-renemen
ts
are
incorp
orated
in
it.
That
is,
if
the
original
renemen
t
op
erator
(without
lo
ok
ahead)
is

0
,
then
(c)
=

0
(c)
[
[
c
0

0
(c)

0
(c
0
)
(.)
This
approac
h,
as
w
ell
as
the
former
one,
can
b
e
extended
in
the
sense
that
the
learner
could
lo
ok
more
than
one
lev
el
ahead.

Example
.
Supp
ose
that
the
test
X
<

causes
the
highest
information
gain,
but
that
a
heuristic
using
lo
ok
ahead
prefers
X
<

b
ecause
it
turns
out
that
the
test
X
>
,
in
com
bination
with
X
<
,
will
lead
to
a
higher
gain
than
an
y
test
com
bined
with
X
<
.
The
dierence
b
et
w
een
the
t
w
o
lo
ok
ahead
approac
hes
is
then
that
with
the
rst
approac
h,
X
<

is
simply
added
as
b
est
test,
while
with
the
second
approac
h
(redening
)
one
immediately
adds
X
<

^
X
>
.
Both
approac
hes
are
not
equiv
alen
t:
it
is
not
guaran
teed
that
after
adding
X
<
,
the
test
X
>

will
turn
out
to
b
e
the
b
est
test
to
add
next.
Indeed,
although
it
caused
highest
gain
in
com
bination
with
X
<
,
the
computation
of
the
new
test
can
use
lo
ok
ahead
to
nd
another
test
that
in
itself
causes
less
gain
but
is
more
promising
for
further
renemen
ts.

The
Tilde
system
follo
ws
the
second
approac
h.
It
relies
on
the
user
to
pro
vide
some
information
ab
out
when
lo
ok
ahead
is
needed,
b
ecause
in
man
y
cases
the
user
has
a
b
etter
idea
ab
out
this
than
what
a
learning
system
can
deriv
e
on
the
basis
of
e.g.
determinacy
.

One
could
ob
ject
that
the
second
approac
h
to
lo
ok
ahead
can
alw
a
ys
b
e
sim
ulated
b
y
adding
to
the
bac
kground
kno
wledge
predicates
that
are
equiv
alen
t
to
the
com
bination
of
sev
eral
other
predicates.
Lo
ok
ahead
w
ould
not
b
e
needed
then.
This
is
true,
but
if
man
y
com
binations
of
literals
are
p
ossible,
then
a
bac
kground
predicate
m
ust
b
e
pro
vided
for
eac
h
com
bination.
Esp
ecially
when
allo
wing
lo
ok
ahead
of
more
than
one
lev
el,
the
n
um
b
er
of
predicates
ma
y
b
ecome
h
uge.
Our
lo
ok
ahead
templates
oer
a
m
uc
h
more
exible
w
a
y
of
sp
ecifying
whic
h
com
binations
are
p
ossible.

..
UPGRADING
TDIDT
TO
FIRST
ORDER
LOGIC

The
eect
of
lo
ok
ahead
can
b
e
describ
ed
as
follo
ws.
First,
w
e
dene
a
basic
renemen
t
op
erator

0
that
tak
es
only
the
rmodes
in
to
accoun
t:


0
(H
 
B
)
=
fH
 
B
;
C

j
rmode
(n
:
C
)
and
C
has
b
een
used
less
than
n
times
to
form
B
and
(H
 
B
;
C

)
is
mo
de-
and
t
yp
e-conform
g
(.)
W
e
no
w
extend
the
op
erator

0
so
that
lo
ok
ahead
is
allo
w
ed,
obtaining
a
new
op
erator
.
The
user
can
pro
vide
templates
of
the
form
lookahead(C

,
C

),
sp
ecifying
that
whenev
er
a
conjunction
is
added
matc
hing
C

,
the
conjunction
C

ma
y
b
e
added
as
w
ell.
T
o
formalize
this,
w
e
dene
an
op
erator

that
maps
a
set
of
clauses
on
to
the
set
of
clauses
that
is
obtained
b
y
applying
lo
ok
ahead:
(S
)
=
fH
 
B
;
C
0
;
C
j
(H
 
B
;
C
0
)

S
^
	
:
C


=
C
0
^
C


=
C
^lookahead
(C

;
C

)g
(.)
On
the
new
clauses,
lo
ok
ahead
can
again
b
e
applied
if
the
newly
added
con-
junctions
themselv
es
matc
h
lo
ok
ahead
sp
ecications,
so
that
nally

can
b
e
dened
as
(Q)
=
N
[
i=

i
(
0
(Q))
(.)
where
N
is
the
maxim
um
n
um
b
er
of
lo
ok
ahead
steps
allo
w
ed.
Example
.
In
the
con
text
of
the
Mesh
data
set
(see
App
endix
A
for
a
description),
one
could
ha
v
e
the
follo
wing
sp
ecications:
rmode(0:neighbo
ur(
+V
,
-V)).
lookahead(neighb
our
(V
,
V),
long(V)).
These
w
ould
cause
Tilde
to
add,
in
one
renemen
t
step,
tests
suc
h
as
(with
E
a
b
ound
v
ariable
and
E
free):
neighbour(E,
E)
neighbour(E,
E),
long(E)
In
other
w
ords,
Tilde
can
(but
need
not)
test
a
new
edge
at
the
same
time
it
in
tro
duces
it.


F
or
con
v
enience,
w
e
consider
the
+,
 and
+ sym
b
ols
that
o
ccur
in
the
rmode
sp
e-
cications
to
b
e
meta-information;
i.e.
w
e
treat
e.g.
worn(+V)
as
if
it
w
ere
worn(V)
when
applying
a
substitution,
but
still
tak
e
the
mo
de
information
in
to
accoun
t
when
generating
substitutions.


CHAPTER
.
TOP-DO
WN
INDUCTION
OF
F
OLDTS
..
Discretization
Discretization
is
a
tec
hnique
that
is
used
b
y
sym
b
olic
induction
metho
ds
in
order
to
b
e
able
to
generate
tests
on
n
umerical
data.
It
consists
of
con
v
erting
a
con
tin
uous
domain
in
to
a
discrete
one.
The
motiv
ation
for
discretizing
n
umeric
data
is
t
w
ofold
and
based
on
the
ndings
in
attribute
v
alue
learning.
On
the
one
hand,
there
is
an
eciency
concern.
Some
ILP
systems
(e.g.,
F
oil
(Quinlan,
		b))
generate
n
um
b
ers
during
the
induction
pro
cess
itself,
whic
h
ma
y
cause
a
lot
of
o
v
erhead:
at
eac
h
renemen
t
step
(a
lot
of
)
constan
ts
need
to
b
e
generated.
By
discretizing
n
umeric
domains
b
eforehand,
as
Tilde
do
es,
the
induction
pro
cess
b
ecomes
m
uc
h
more
ecien
t.
On
the
other
hand,
b
y
discretizing
the
data,
one
ma
y
sometimes
obtain
higher
accuracy
rates
(as
the
h
yp
othesis
is
less
lik
ely
to
o
v
ert
the
training
data).
Suc
h
results
ha
v
e
b
een
obtained
b
y
,
e.g.,
Catlett
(		).
The
discretization
pro
cedure
that
Tilde
uses
w
as
dev
elop
ed
and
imple-
men
ted
b
y
Luc
De
Raedt,
Sa

so
D

zeroski
and
Wim
V
an
Laer
(V
an
Laer
et
al.,
		)
for
the
ICL
system
(De
Raedt
and
V
an
Laer,
		),
but
could
b
e
incor-
p
orated
in
Tilde
without
signican
t
mo
dications.
The
metho
d
only
w
orks
for
classication
though;
as
of
no
w
Tilde
do
es
not
con
tain
an
y
discretization
algorithms
that
are
usable
for
regression
or
clustering.
The
approac
h
follo
w
ed
in
Tilde
is
that
the
user
can
iden
tify
declarativ
ely
the
relev
an
t
queries
and
the
v
ariables
for
whic
h
the
v
alues
are
to
b
e
discret-
ized.
F
or
instance,
to
be
discretized(atom
(A
,B,
C,
D),
[D])
states
that
the
fourth
argumen
t
of
the
predicate
atom
should
b
e
discretized.
The
resulting
n
umeric
attributes
are
then
discretized
using
a
simple
mo
dic-
ation
of
F
a
yy
ad
and
Irani's
metho
d.
The
details
of
this
metho
d
can
b
e
found
in
(F
a
yy
ad
and
Irani,
		)
and
(Doughert
y
et
al.,
		).
In
short,
the
algorithm
nds
a
threshold
that
partitions
a
set
of
examples
in
to
t
w
o
subsets
suc
h
that
the
a
v
erage
class
en
trop
y
of
the
subsets
is
as
small
as
p
ossible,
as
follo
ws.
Let
s(E
)
b
e
the
class
en
trop
y
of
a
set
of
examples
E
:
s(E
)
=
 k
X
i=
p(c
i
;
E
)
log
p(c
i
;
E
)
(.)
(p(c
i
;
E
)
is
the
prop
ortion
of
examples
in
E
that
ha
v
e
class
c
i
,
k
is
the
n
um
b
er
of
classes).
If
a
threshold
T
for
an
attribute
A
partitions
E
in
to
E

=
fx

E
jx:A

T
g
and
E

=
E
 E

,
then
the
a
v
erage
class
en
trop
y
after
partitioning
is
jE

j
jE
j
s(E

)
+
jE

j
jE
j
s(E

):
(.)
The
threshold
T
is
c
hosen
so
that
this
a
v
erage
en
trop
y
is
minimal.

This

This
corresp
onds
to
the
threshold
that
oers
maximal
information
gain,
as
dened
b
y
Equation
..

..
UPGRADING
TDIDT
TO
FIRST
ORDER
LOGIC

pro
cedure
is
applied
recursiv
ely
on
E

and
E

un
til
some
stopping
criterion
is
reac
hed.
With
resp
ect
to
F
a
yy
ad
and
Irani's
algorithm,
t
w
o
adaptations
ha
v
e
b
een
made.
First,
F
a
yy
ad
and
Irani
prop
ose
a
stopping
criterion
that
is
based
on
the
minimal
description
length
(MDL)
principle,
but
for
b
oth
ICL
and
Tilde
this
metho
d
w
as
found
to
generate
v
ery
few
thresholds.
Therefore
Tilde's
discretiz-
ation
pro
cedure
accepts
a
maxim
um
n
um
b
er
of
thresholds
as
a
parameter.
This
has
the
additional
adv
an
tage
that
one
can
exp
erimen
t
with
dieren
t
n
um
b
ers
of
thresholds.
A
second
adaptation
made
to
F
a
yy
ad
and
Irani's
metho
d
sp
ecically
con-
cerns
non-determinacy
.
Due
to
the
fact
that
one
example
ma
y
ha
v
e
m
ultiple
or
no
v
alues
for
a
n
umeric
attribute,
w
e
use
sum
of
w
eigh
ts
instead
of
n
um
b
er
of
examples
in
the
appropriate
places
of
F
a
yy
ad
and
Irani's
form
ulae
(in
the
attribute
v
alue
case
all
v
alues
ha
v
e
w
eigh
t

as
eac
h
example
has
only
one
v
alue
for
one
attribute).
The
w
eigh
t
of
an
example
in
a
set
is
equal
to
the
n
um
b
er
of
v
alues
o
ccurring
in
it
that
pass
the
test
describing
the
set,
divided
b
y
the
total
n
um
b
er
of
v
alues
in
the
example;
it
is
0
if
there
are
no
v
alues.
The
sum
of
the
w
eigh
ts
of
all
v
alues
for
one
n
umeric
attribute
or
query
in
one
example
alw
a
ys
equals
one,
or
zero
when
no
v
alues
are
giv
en.
Example
.
Consider
an
example
e

=
fp();
p();
p()g,
and
some
threshold
T
=
:.
If
eac
h
example
had
only
one
v
alue
for
p,
T
w
ould
partition
a
set
of
examples
S
in
to
S

(examples
that
ha
v
e
a
v
alue
<
.)
and
S

,
the
rest.
In
our
con
text,
e

has
three
v
alues
for
p,
t
w
o
of
whic
h
are
smaller
than
:,
hence
e

has
w
eigh
t
=
in
S

,
and
=
in
S

.

These
w
eigh
ts
can
then
b
e
used
in
the
form
ulae
for
class
en
trop
y
b
y
dening
for
an
y
set
S
,
jS
j
=
X
eS
w
e;S
(.)
and
dening
p(c
i
;
E
)
as
p(c
i
;
E
)
=
jfe

E
jcl
ass(e)
=
c
i
gj
jE
j
(.)
Note
that
the
use
of
w
eigh
ts
is
more
or
less
arbitrary;
other
approac
hes
could
b
e
follo
w
ed.
W
e
found
this
approac
h
to
w
ork
w
ell.
Aside
from
the
generation
of
thresholds,
there
is
the
topic
of
ho
w
these
thresholds
should
b
e
used.
W
e
see
sev
eral
p
ossibilities:

Using
inequalities
to
compare
whether
a
v
alue
is
less
than
a
discretization
threshold;
this
corresp
onds
to
an
ine
quality
test
in
the
discrete
domain.


CHAPTER
.
TOP-DO
WN
INDUCTION
OF
F
OLDTS
con
tin
uous
domain
discrete
domain
inequalities
x
<
b
i
x
D
<
d
i
equalities
b
i

x
<
b
i+
x
D
=
d
i
in
terv
als
b
i

x
<
b
j
d
i

x
D
<
d
j
T
able
.:
Comparison
of
tests
in
the
con
tin
uous
domain
and
the
discrete
domain.

Chec
king
whether
a
v
alue
lies
in
some
in
terv
al
b
ounded
b
y
t
w
o
consecut-
iv
e
thresholds.
Suc
h
an
in
terv
al
test
corresp
onds
with
an
e
quality
test
in
the
discretized
domain.

Chec
king
whether
a
v
alue
lies
in
an
in
terv
al
b
ounded
b
y
nonconsecutiv
e
thresholds.
This
corresp
onds
to
an
interval
test
in
the
discrete
domain.
T
able
.
giv
es
an
o
v
erview
of
the
relationship
b
et
w
een
tests
in
the
con
tin
u-
ous
domain
using
discretization
thresholds,
and
tests
in
the
discrete
domain.
W
e
use
the
follo
wing
notation.
If
x
is
a
v
alue
of
the
con
tin
uous
domain,
x
D
is
the
corresp
onding
v
alue
in
the
discrete
domain.
The
discrete
domain
is
assumed
to
ha
v
e
n
dieren
t
v
alues
whic
h
w
e
denote
d

;
:
:
:
;
d
n
;
eac
h
d
i
corresp
onds
to
an
in
terv
al
[b
i
;
b
i+
)
in
the
con
tin
uous
domain.
Although
allo
wing
only
inequalit
y
tests
is
complete
(in
terv
al
tests
simply
consist
of
m
ultiple
inequalit
y
tests),
it
seems
b
etter
to
explicitly
allo
w
in
terv
al
tests
that
corresp
ond
to
discrete
equalit
y
tests.
After
all,
most
learning
systems
use
tests
suc
h
as
x
=
c,
and
need
not
generate
this
test
in
t
w
o
steps
(x

c
^
x

c).
	
Allo
wing
in
terv
als
that
corresp
ond
to
discrete
in
terv
al
tests
is
a
less
ob
vious
decision
(not
all
learners
generate
in
terv
al
tests
suc
h
as
x

[a;
b)),
but
seems
an
in
teresting
option
b
ecause
of
the
follo
wing
prop
ert
y
.
When
the
n
um
b
er
of
thresholds
is
increased,
the
new
set
of
thresholds
is
a
sup
erset
of
the
previous
one.
This
means
that
the
discrete
v
alues
of
the
rst
discretization
corresp
ond
to
in
terv
als
of
the
second
one.
By
using
in
terv
als
in
the
discrete
domain
with
n
thresholds,
all
equalit
y
tests
for
discretizations
with
a
n
um
b
er
of
thresholds
smaller
than
n
are
generated
as
w
ell.
.
Instan
tiations
of
TDIDT
in
Tilde
In
this
section
w
e
discuss
ho
w
Tilde
instan
tiates
the
pro
cedure
parameters
of
TDIDT
for
the
dieren
t
induction
tasks,
as
w
ell
as
some
more
general
pro
ced-
ures
that
are
not
task-dep
enden
t.
	
In
fact,
since
an
in
terv
al
or
equalit
y
test
is
equiv
alen
t
to
t
w
o
inequalit
y
tests,
this
can
b
e
seen
as
a
sp
ecial
case
of
lo
ok
ahead.

..
INST
ANTIA
TIONS
OF
TDIDT
IN
TILDE

..
Classication
T
rees
The
subsystem
of
Tilde
that
induces
classication
trees
is
based
up
on
the
detailed
description
of
C.
in
Quinlan's
C.:
Pr
o
gr
ams
for
Machine
L
e
arning
(Quinlan,
		a).
More
sp
ecically
,
Tilde
inherits
from
C.
the
follo
wing
prop
erties:

OPTIMAL
SPLIT:
the
heuristic
used
for
c
ho
osing
the
b
est
split
in
a
giv
en
no
de
is
b
y
default
gain
ratio,
although
information
gain
can
also
b
e
c
hosen
b
y
the
user
(see
Section
..
for
denitions).
Quinlan
(		a)
men
tions
that
gain
ratio
usually
p
erforms
sligh
tly
b
etter
than
gain.

STOP
CRIT:
Tilde
do
es
not
split
a
no
de
when
at
least
one
of
the
fol-
lo
wing
conditions
is
fullled:
{
the
examples
co
v
ered
b
y
it
all
ha
v
e
the
same
class,
{
no
split
can
b
e
found
that
yields
an
y
gain
at
all
and
for
whic
h
b
oth
branc
hes
co
v
er
at
least
some
minimal
n
um
b
er
of
examples
(b
y
de-
fault
)

INF
O:
the
information
stored
in
a
leaf
is
the
mo
dal
class
v
alue
among
the
examples
co
v
ered
b
y
the
leaf.

PR
UNE:
for
classication
trees,
Tilde
oers
t
w
o
instan
tiations
of
the
PR
UNE
pro
cedure:
{
The
C.
p
ost-pruning
tec
hnique,
based
on
an
non-empirical
estim-
ate
of
the
predictiv
e
accuracy
of
a
tree
on
unseen
data
(see
Sec-
tion
..).
{
Pruning
based
on
the
predictiv
e
accuracy
of
the
tree
on
an
in
ternal
v
alidation
set,
as
explained
in
Section
...
Tilde's
pruning
al-
gorithm
is
sho
wn
in
Figure
..
The
QUALITY
function
is
instan-
tiated
with
the
accuracy
of
a
prediction
on
the
v
alidation
set.
When
running
Tilde
on
a
prop
ositional
data
set,
the
main
dierence
with
C.
is
that
Tilde
can
only
induce
binary
decision
trees
(conform
to
the
den-
ition
of
rst
order
logical
decision
trees).
When
run
on
a
prop
ositional
data
set
with
only
binary
features,
Tilde
usually
returns
the
same
trees
as
C.
(small
dierences
can
b
e
accoun
ted
for
b
y
the
fact
that
when
there
are
m
ultiple
b
est
tests,
Tilde
ma
y
c
ho
ose
a
dieren
t
b
est
test
than
C.).
..
Regression
T
rees
The
algorithm
for
building
regression
trees
is
a
sp
ecial
case
of
the
clustering
algorithm.


CHAPTER
.
TOP-DO
WN
INDUCTION
OF
F
OLDTS
pro
cedure
combine
qualities(T

,
T

:
tree)
returns
real:
w

:=
jcov
(T

)j
w

:=
jcov
(T

)j
return
(w

max(fT

:p;
T

:ug)
+
w

max(fT

:p;
T

:ug))=(w

+
w

)
pro
cedure
compute
tree
quality(T
:
tree):
if
T
=
leaf(info)
T
:u
:=
QUALITY(info)
T
:p
:=
T
:u
else
compute
tree
quality(T
:l
ef
t)
compute
tree
quality(T
:r
ig
ht)
T
:p
:=
QUALITY(INF
O(c
ov(T
)))
T
:u
:=
combine
qualities(T
:l
ef
t,
T
:r
ig
ht)
pro
cedure
change
nodes
into
lea
ves(T
:
tree):
if
T
:p
>=
T
:u
then
T
:=
leaf
(I
N
F
O
(c
ov
(T
)))
else
change
nodes
into
lea
ves(T
:l
ef
t)
change
nodes
into
lea
ves(T
:r
ig
ht)
pro
cedure
pr
une
tree(T
:
tree):
compute
tree
quality(T
)
change
nodes
into
lea
ves(T
)
Figure
.:
Pruning
algorithm
based
on
the
use
of
v
alidation
sets.
The
al-
gorithm
w
orks
in
t
w
o
steps.
First,
for
eac
h
no
de
of
the
tree
the
qualit
y
of
the
no
de
if
it
w
ould
b
e
a
leaf
is
recorded
(p),
as
w
ell
as
the
qualit
y
of
the
no
de
if
it
is
not
pruned
but
the
subtree
starting
in
it
is
pruned
in
an
optimal
w
a
y
(u).
In
a
second
step,
the
tree
is
pruned
in
those
no
des
where
p
>
u.
QUALITY
is
a
parameter
of
the
algorithm;
it
yields
the
qualit
y
of
a
prediction
on
the
v
alidation
set.
cov
(T
)
denotes
the
set
of
examples
in
the
training
set
co
v
ered
b
y
T
.

..
INST
ANTIA
TIONS
OF
TDIDT
IN
TILDE
	

OPTIMAL
SPLIT:
the
heuristic
used
for
deciding
whic
h
split
is
b
est,
is
based
on
the
dierence
b
et
w
een
the
mean
of
the
t
w
o
subsets
created
b
y
the
split.
The
greater
this
dierence
is,
the
b
etter
the
split
is
considered
to
b
e.
As
w
e
men
tioned
b
efore,
maximizing
the
dierence
b
et
w
een
the
means
is
equiv
alen
t
to
minimizing
the
v
ariance
within
the
subsets.

STOP
CRIT:
Tilde
do
es
not
split
a
no
de
if
{
no
split
can
b
e
found
that
reduces
the
v
ariance
of
the
target
v
ari-
able
within
the
subsets
and
where
eac
h
branc
h
co
v
ers
at
least
some
minimal
n
um
b
er
of
examples
(b
y
default
)
{
no
split
can
b
e
found
that
causes
a
signican
t
reduction
of
v
ari-
ance
for
the
target
v
ariable.
An
F-test
is
used
to
test
this
(see
Section
..).
By
default,
the
signicance
lev
el
is

(whic
h
means
this
test
is
turned
o
).

INF
O:
the
information
stored
in
a
leaf
is
the
mean
of
the
v
alues
of
the
examples
co
v
ered
b
y
the
leaf.

PR
UNE:
the
pruning
algorithm
in
Figure
.
is
used;
the
QUALITY
func-
tion
returns
min
us
the
mean
squared
error
(MSE)
of
the
prediction
on
the
v
alidation
set
(min
us,
b
ecause
maximizing
the
qualit
y
should
minimize
the
MSE).
The
regression
subsystem
of
Tilde
is
also
men
tioned
as
Tilde-R
T
in
the
literature
(R
T
stands
for
regression
trees).
W
e
will
also
use
this
term
in
this
text,
when
w
e
sp
ecically
refer
to
the
regression
subsystem
of
Tilde.
..
Clustering
T
rees
The
algorithm
for
building
clustering
trees
is
v
ery
general
and
highly
paramet-
rized.
F
or
man
y
applications
the
user
needs
to
pro
vide
some
co
de
that
denes
domain-sp
ecic
things.
The
reason
wh
y
Tilde
con
tains
a
separate
mo
de
for
regression,
ev
en
though
it
is
in
all
resp
ects
a
sp
ecial
case
of
clustering,
is
that
for
regression
all
these
parameters
can
b
e
instan
tiated
automatically
.
Th
us,
the
regression
mo
de
is
m
uc
h
more
user-friendly
than
the
general
clustering
mo
de.

OPTIMAL
SPLIT:
this
pro
cedure
is
parametrized
with
a
distance
and
a
protot
yp
e
function.
By
default,
Tilde
c
ho
oses
a
split
suc
h
that
the
distance
b
et
w
een
the
protot
yp
es
of
the
clusters
is
as
large
as
p
ossible.
Tw
o
distance
measures
are
predened
in
Tilde:
{
Euclidean
distance:
this
distance
is
only
useful
when
the
examples
can
b
e
represen
ted
as
p
oin
ts
in
an
n-dimensional
space.
The
user

0
CHAPTER
.
TOP-DO
WN
INDUCTION
OF
F
OLDTS
needs
to
sp
ecify
ho
w
the
co
ordinates
of
an
example
are
to
b
e
com-
puted
from
its
rst
order
description.
A
protot
yp
e
function
is
also
predened
for
this
distance.
{
the
distance
prop
osed
b
y
Ramon
and
Bruyno
oghe
(		):
this
is
a
rst
order
distance
measure.
Jan
Ramon
implemen
ted
this
distance
and
incorp
orated
it
in
Tilde.
The
user
can
instan
tiate
the
OPTIMAL
SPLIT
pro
cedure
at
sev
eral
lev
els:
{
one
of
the
predened
distances
can
b
e
c
hosen
{
the
user
can
dene
a
distance,
together
with
a
protot
yp
e
function
(e.g.,
a
distance
that
is
sp
ecically
designed
for
the
application
at
hand)
{
the
OPTIMAL
SPLIT
pro
cedure
can
b
e
redened
en
tirely
.
This
ap-
proac
h
w
as
tak
en
with
Ramon's
distance,
where
a
protot
yp
e
function
is
not
a
v
ailable
(the
protot
yp
e
of
a
set
can
b
e
computed
theoretic-
ally
,
but
in
practice
the
computation
is
not
feasible).
The
distance
b
et
w
een
t
w
o
sets
E

and
E

is
here
dened
as
the
a
v
erage
distance
b
et
w
een
all
the
examples
in
the
sets:
D
=
a
vg
dist(E


E

)
=
P
e
i
E

;e
j
E

d(e
i
;
e
j
)
jE

jjE

j
(.	)
Because
this
computation
of
D
ma
y
b
e
exp
ensiv
e,
0
when
E


E

is
large
D
is
estimated
b
y
computing
the
a
v
erage
distance
for
a
randomly
c
hosen
sample
of
xed
size:
^
D
=
a
vg
dist(S
)
with
S

E


E

:
(.0)

STOP
CRIT:
Tilde
do
es
not
split
a
no
de
if
{
no
split
can
b
e
found
that
reduces
the
v
ariance
of
the
subsets
and
where
eac
h
branc
h
co
v
ers
at
least
some
minimal
n
um
b
er
of
examples
(b
y
default
)
{
no
split
can
b
e
found
that
causes
a
signican
t
reduction
of
v
ariance.
An
F-test
is
used
to
test
this
(see
Section
..).
By
default,
the
signicance
lev
el
is

(whic
h
means
this
test
is
turned
o
).

INF
O:
b
y
default,
this
is
the
iden
tit
y
function,
i.e.,
the
whole
set
of
examples
co
v
ered
b
y
the
leaf
is
stored.
This
can
b
e
redened
b
y
the
user.
0
Computing
D
has
quadratic
time
complexit
y
in
the
n
um
b
er
of
examples;
this
is
to
b
e
a
v
oided
b
ecause
the
time
complexit
y
of
the
heuristic
is
crucial
for
the
time
complexit
y
of
the
whole
induction
pro
cess,
as
w
e
will
see
in
Section
...

..
AN
EXAMPLE
OF
TILDE
A
T
W
ORK


PR
UNE:
the
pruning
algorithm
in
Figure
.
is
used;
the
QUALITY
function
returns
min
us
the
mean
squared
distance
of
the
protot
yp
e
of
the
co
v
ered
training
examples
to
the
co
v
ered
examples
in
the
v
alidation
set.
The
clustering
subsystem
of
Tilde
is
also
referred
to
as
TIC
(T
op-do
wn
Induction
of
Clustering
trees).
.
An
Example
of
Tilde
at
W
ork
W
e
no
w
illustrate
ho
w
Tilde
w
orks
on
the
Mac
hines
example.
Data
F
ormat
A
data
set
is
presen
ted
to
Tilde
in
the
form
of
a
set
of
in
terpretations.
Eac
h
in
terpretation
consists
of
a
n
um
b
er
of
Prolog
facts,
surrounded
b
y
a
begin
and
end
line.
Th
us,
the
data
for
our
running
example
are
represen
ted
as
follo
ws:
begin(model()).
fix.
worn(gear).
worn(chain).
end(model()).
begin(model()).
sendback.
worn(engine).
worn(chain).
end(model()).
begin(model()).
sendback.
worn(wheel).
end(model()).
begin(model()).
ok.
end(model()).
The
bac
kground
kno
wledge
is
simply
a
Prolog
program:
replaceable(gear
).
replaceable(chai
n).
not_replaceable(
eng
in
e)
.
not_replaceable(
whe
el
).


CHAPTER
.
TOP-DO
WN
INDUCTION
OF
F
OLDTS
Settings
The
settings
le
includes
information
suc
h
as
whether
the
task
at
hand
is
a
classication,
regression
or
clustering
task;
what
the
classes
are,
or
whic
h
v
ariables
are
to
b
e
predicted;
the
renemen
t
op
erator
sp
ecication;
and
so
on.
Most
of
these
settings
can
b
e
left
to
their
default
v
alues
for
this
application.
A
go
o
d
settings
le
is:
minimal_cases()
.
classes([fix,sen
dba
ck
,o
k])
.
rmode(:
replaceable(+-X))
.
rmode(:
not_replaceable(+
-X
)).
rmode(:
worn(+-X)).
The
minimal
cases
setting
indicates
ho
w
man
y
examples
should
at
least
b
e
co
v
ered
b
y
eac
h
leaf;
it
is
b
y
default
,
but
for
a
small
data
set
suc
h
as
this
one
w
e
prefer
to
mak
e
it
.
Running
Tilde
with
the
ab
o
v
e
settings
and
input
causes
it
to
build
a
tree
as
sho
wn
in
Figure
..
The
gure
sho
ws
the
output
Tilde
writes
to
the
screen,
as
w
ell
as
a
graphical
represen
tation
of
ho
w
the
tree
is
built.
Eac
h
step
in
the
graphical
represen
tation
sho
ws
the
partial
tree
that
has
b
een
built,
the
literals
that
are
considered
for
addition
to
the
tree,
and
ho
w
eac
h
literal
w
ould
split
the
set
of
examples.
E.g.
fssjo
means
that
of
four
examples,
one
with
class
fix
and
t
w
o
with
class
sendback
are
in
the
left
branc
h,
and
one
example
with
class
ok
is
in
the
righ
t
branc
h.
The
b
est
literal
is
indicated
with
an
asterisk.
The
one
that
is
barred
w
ould
in
principle
b
e
generated
b
y
the
renemen
t
op
erator,
but
is
ltered
out
b
ecause
it
generates
the
same
test
as
the
one
in
the
ro
ot
no
de
and
hence
is
useless.
Output
The
(sligh
tly
shortened)
output
le
generated
b
y
Tilde
for
the
ab
o
v
e
example
is
sho
wn
in
Figure
..
It
con
tains
some
statistics
on
the
induction
pro
cess
and
the
induced
h
yp
othesis,
as
w
ell
as
a
represen
tation
of
the
h
yp
othesis
b
oth
as
a
rst
order
logical
decision
tree
and
as
a
Prolog
program.
.
Some
Eciency
Considerations
..
Scalabilit
y
De
Raedt
and
D

zeroski
(		)
ha
v
e
sho
wn
that
in
the
learning
from
in
terpreta-
tions
setting,
learning
rst-order
clausal
theories
is
tractable.
More
sp
ecically
,

..
SOME
EFFICIENCY
CONSIDERA
TIONS

building
tree...
true
,
replaceable(A)
true
,
not_replaceable(A)
true
,
worn(A)
[gain
=
0.,gainratio
=
]
Considering
true
,
worn(A)
...
+
Best
test
up
till
now.
Best
test:
true
,
worn(A)
(true
,
worn(A))
,
replaceable(A)
[gain
=
0.	,gainratio
=
0.0]
(true
,
worn(A))
,
replaceable(B)
(true
,
worn(A))
,
not_replaceable(A)
[gain
=
0.		,gainratio
=
]
(true
,
worn(A))
,
not_replaceable(B)
(true
,
worn(A))
,
worn(B)
Considering
(true
,
worn(A))
,
replaceable(A)
...
+
Best
test
up
till
now.
Considering
(true
,
worn(A))
,
not_replaceable(A)
...
+
Best
test
up
till
now.
Best
test:
(true
,
worn(A))
,
not_replaceable(A)
worn(X)
replaceable(X)
not_replaceable(X)
worn(Y)
replaceable(Y)
not_replaceable(Y)
fss|
fs|s
fss|
fss|
fss|
worn(X)
ok
worn(X)
ok
not_replaceable(X)
sendback
fix
worn(X)
replaceable(X)
not_replaceable(X)
fsso|
fsso|
?
?
fss|o    *
ss|f     *
Renemen
t
op
erator
sp
ecication:
rmode(:
replaceable(+-X)
).
rmode(:
not
replaceable(+-X))
.
rmode(:
worn(+-X)).
Figure
.:
Tilde
illustrated
on
the
running
example.
A
screen
dump
of
a
run
is
sho
wn,
as
w
ell
as
a
graphical
represen
tation
of
the
tree-building
pro
cess.


CHAPTER
.
TOP-DO
WN
INDUCTION
OF
F
OLDTS
**
Output
of
Tilde
.0
**
Run
on
droopy
:
sparc
SUNW,Ultra-
running
SunOS
.
Settings:
heuristic(gainratio)
classes([fix,sendback,ok])
tilde_mode(classify)
pruning(c)
minimal_cases()
Induction
time:
0.0
seconds.
--------------------------
after
pruning:
pruned_complexity
:

nodes
(
literals)
pruned_training_accuracy
:

=

/

pruned_global_accuracy
:

=

/

pruned_C._error_estimation
:
.
(relative:
0.)
--------------------------
Compact
notation
of
pruned
tree:
worn(A)
?
+--yes:
not_replaceable(A)
?
|
+--yes:
sendback
[
/
]
|
+--no:
fix
[
/
]
+--no:
ok
[
/
]
--------------------------
Equivalent
prolog
program:
class(sendback)
:-
worn(A)
,
not_replaceable(A),
!.
%

/

=
.
class(fix)
:-
worn(A),
!.
%

/

=
.
class(ok).
%

/

=
.
Figure
.:
An
output
le
generated
b
y
Tilde
(sligh
tly
simplied).

..
SOME
EFFICIENCY
CONSIDERA
TIONS

giv
en
xed
b
ounds
on
the
maximal
length
of
clauses
and
the
maximal
size
of
literals,
suc
h
theories
are
p
olynomial-sample
p
olynomial-time
P
A
C-learnable.
This
p
ositiv
e
result
is
related
directly
to
the
learning
from
in
terpretations
set-
ting.
Quinlan
(	)
has
sho
wn
that
induction
of
decision
trees
has
time
com-
plexit
y
O
(a

m

n)
where
a
is
the
n
um
b
er
of
attributes
of
eac
h
example,
m
is
the
n
um
b
er
of
examples
and
n
is
the
n
um
b
er
of
no
des
in
the
tree.
Since
Tilde
uses
basically
the
same
algorithm
as
C.,
it
can
b
e
exp
ected
to
inherit
the
linearit
y
in
the
n
um
b
er
of
examples
and
in
the
n
um
b
er
of
no
des.
Ho
w
ev
er,
there
are
some
dierences
that
can
aect
its
time
complexit
y
.
The
main
dierence
b
et
w
een
Tilde
and
C.,
as
w
e
already
noted,
is
the
generation
of
tests
in
a
no
de.
The
n
um
b
er
of
tests
to
b
e
considered
in
a
no
de
dep
ends
on
the
renemen
t
op
erator.
There
is
no
theoretical
b
ound
on
this,
as
it
is
p
ossible
to
dene
renemen
t
op
erators
that
cause
an
innite
branc
hing
factor
(through
the
use
of
lo
ok
ahead).
In
practice,
useful
renemen
t
op
erators
alw
a
ys
generate
a
nite
n
um
b
er
of
renemen
ts,
but
ev
en
then
this
n
um
b
er
ma
y
not
b
e
b
ounded:
the
n
um
b
er
of
renemen
ts
t
ypically
increases
with
the
length
of
the
asso
ciated
query
of
the
no
de
(b
ecause
the
n
um
b
er
of
dieren
t
w
a
ys
in
whic
h
new
v
ariables
can
b
e
unied
with
already
o
ccurring
ones
dep
ends
on
the
n
um
b
er
of
v
ariables
already
o
ccurring).
Also,
the
time
for
p
erforming
one
single
test
on
a
single
example
dep
ends
on
the
complexit
y
of
that
test
(it
is
in
the
w
orst
case
exp
onen
tial
in
the
n
um
b
er
of
literals
in
the
test).
A
second
dierence
is
that
Quinlan's
deriv
ation
exploits
the
fact
that
the
computation
of
the
qualit
y
of
a
test
is
linear
in
the
n
um
b
er
of
examples.
This
is
easy
to
ac
hiev
e
in
the
case
of
classication
or
regression
(e.g.
a
class
en
trop
y
or
v
ariance
can
b
e
computed
in
linear
time)
but
for
clustering
some
care
needs
to
b
e
tak
en,
as
for
instance
the
discussion
of
Ramon's
distance
in
Section
.
illustrates.
If
there
are
n
no
des,
in
eac
h
no
de
t
tests
are
p
erformed
on
a
v
erage,
eac
h
test
is
p
erformed
on
m
examples,
the
a
v
erage
complexit
y
of
testing
a
single
example
is
c
and
the
computation
of
the
heuristic
for
a
single
test
tak
es
time
h(m),
then
Tilde
has
time
complexit
y
O
(n

t

(m

c
+
h(m)))
(assuming
the
a
v
erages
c
and
t
exist).
If
one
is
willing
to
accept
an
upp
er
b
ound
on
the
complexit
y
of
the
theory
that
is
to
b
e
learned
(whic
h
w
as
done
for
the
P
A
C-learning
results)
and
denes
a
nite
renemen
t
op
erator,
b
oth
the
complexit
y
of
p
erforming
a
single
test
on
a
single
example
and
the
n
um
b
er
of
tests
are
b
ounded
and
the
a
v
erages
do
exist.
If
care
is
tak
en
that
h(m)
is
O
(m),
then
Tilde's
time
complexit
y
is
O
(n

t

m

c).
This
means
that,
under
fairly
general
conditions,
the
time
complexit
y
of
Tilde
is
linear
in
the
n
um
b
er
of
examples.
The
time
complexit
y
also
de-
p
ends
on
the
global
complexit
y
of
the
theory
and
the
branc
hing
factor
of
the
renemen
t
op
erator,
whic
h
dep
end
on
the
application
domain.


CHAPTER
.
TOP-DO
WN
INDUCTION
OF
F
OLDTS
..
Querying
Examples
Ecien
tly
Although
Tilde
is
quite
ecien
t
according
to
ILP
standards
(see
also
Sec-
tion
.),
it
is
m
uc
h
less
ecien
t
than
prop
ositional
systems.
Preliminary
com-
parisons
with
C.
(results
not
included
in
this
text)
indicate
that
the
latter
is
often
m
uc
h
faster,
sometimes
in
the
order
of
a
factor
of
000.
There
are
man
y
reasons
wh
y
Tilde
cannot
b
e
made
as
ecien
t
as
C..
Most
of
them
are
related
to
the
fact
that
Tilde
is
an
ILP
system.
The
use
of
a
relational
represen
tation
for
the
examples
complicates
the
querying
algorithm.
The
set
of
tests
to
b
e
considered
at
a
no
de
is
not
constan
t
but
dep
ends
on
the
no
de
and
its
asso
ciated
query
,
and
hence
needs
to
b
e
computed
at
run
time.
Also,
p
erforming
one
single
test
ma
y
tak
e
more
time.
Another
source
of
ineciency
is
that
Tilde
is
implemen
ted
in
Prolog.
Man
y
algorithms
incorp
orated
in
Tilde
are
essen
tially
pro
cedural,
and
Prolog
often
cannot
execute
these
algorithms
as
ecien
tly
as
when
they
w
ould
b
e
imple-
men
ted
in
a
lo
w
er
lev
el
language.
There
are,
ho
w
ev
er,
some
p
oin
ts
where
the
basic
algorithm
can
b
e
impro
v
ed.
A
rather
imp
ortan
t
one
is
the
follo
wing.
As
w
as
explained
earlier,
in
order
to
compute
the
qualit
y
of
a
test
conj
in
a
no
de,
queries
of
the
form
 
Q;
conj
m
ust
b
e
sen
t
to
ev
ery
example.
Ho
w
ev
er,
it
is
clear
that
when
conj
do
es
not
share
an
y
v
ariables
with
Q,
it
could
b
e
tested
on
its
o
wn.
The
extra
computations
in
v
olv
ed
in
computing
a
v
ariable
substitution
for
Q
can
b
e
v
ery
exp
ensiv
e.
Example
.	
Supp
ose
that
Q
=
p(X;
Y;
Z);
q(X;
U);
r(U;
V);
s(V;
Z)
and
conj
=
a
(a
literal
of
a
n
ullary
predicate).
It
is
extremely
simple
to
test
whether
a
suc-
ceeds
in
a
single
example,
but
if
the
query
 
p(X;
Y;
Z);
q(X;
U);
r(U;
V);
s(V;
Z);
a
is
executed
in
an
example
where
a
is
false,
the
underlying
Prolog
engine
bac
k-
trac
ks
on
the
p,q,r,s
literals
trying
to
mak
e
a
succeed.
There
is
no
b
ound
on
the
complexit
y
of
this
query;
if
the
example
description
is
relativ
ely
large,
it
can
b
e
man
y
times
more
exp
ensiv
e
than
just
testing
a.

An
algorithm
has
b
een
implemen
ted
in
Tilde
that
simplies
queries
so
that
suc
h
unnecessary
computations
are
a
v
oided.
T
o
this
aim,
all
literals
in
the
clause
are
collected
that
are
link
ed
with
the
added
conjunction
along
a
path
of
literals
that
share
v
ariables.
The
literals
in
Q
that
are
not
collected
cannot
p
ossibly
inuence
the
success
of
conj
and
hence
are
absen
t
in
the
simplied
query
.
The
algorithm
is
sho
wn
in
Figure
..
.
Exp
erimen
tal
Ev
aluation
In
this
section
w
e
describ
e
the
exp
erimen
ts
p
erformed
to
ev
aluate
Tilde.
W
e
rst
giv
e
some
general
information,
then
discuss
eac
h
exp
erimen
t
in
detail.

..
EXPERIMENT
AL
EV
ALUA
TION

pro
cedure
simplify(Q,
c
onj)
returns
query:
V
:=
v
ariables
of
c
onj
rep
eat
L
:=
literals
in
Q
that
con
tain
v
ariables
in
V
V
:=
V
[
v
ariables
in
L
un
til
V
do
es
not
c
hange
an
ymore
Q
0
:=
conjunction
of
all
literals
in
L
return
 
Q
0
;
c
onj
Figure
.:
Simplication
of
queries.
..
Materials
All
the
exp
erimen
ts
describ
ed
in
this
text
w
ere
run
on
Sun
mac
hines
under
the
Solaris
op
erating
system.
By
default
exp
erimen
ts
w
ere
run
on
a
Sun
SP
AR
C
Ultra-
at

MHz.
In
some
cases
a
Sun
SP
AR
Cstation-0
running
at
00MHz
w
as
used;
where
this
w
as
the
case
it
is
men
tioned
explicitly
in
the
text.
W
e
no
w
giv
e
a
brief
description
of
the
data
sets
that
ha
v
e
b
een
used
for
our
exp
erimen
ts.
Detailed
descriptions
can
b
e
found
in
App
endix
A.
Most
of
the
data
sets
are
a
v
ailable
on
the
In
ternet,
at
either
the
UCI
mac
hine
learning
data
rep
ository
(Merz
and
Murph
y,
		)
or
the
ILP
data
rep
ository
(Kazak
o
v
et
al.,
		).

So
yb
eans:
this
database
(Mic
halski
and
Chilausky,
	0)
con
tains
de-
scriptions
of
diseased
so
yb
ean
plan
ts.
Ev
ery
plan
t
is
describ
ed
b
y

attributes.
A
small
data
set
(
examples,

classes)
and
a
large
one
(0
examples,
	
classes)
are
a
v
ailable
at
the
UCI
data
rep
ository
.
The
data
sets
are
mainly
used
to
ev
aluate
clustering
algorithms.

Iris:
a
simple
database
of
descriptions
of
iris
plan
ts,
a
v
ailable
at
the
UCI
rep
ository
.
It
con
tains

classes
of
0
examples
eac
h.
There
are

n
umerical
attributes.
The
set
is
mainly
used
for
unsup
ervised
learning.

Mutagenesis:
this
database
(Sriniv
asan
et
al.,
		),
a
v
ailable
at
the
ILP
rep
ository
(Kazak
o
v
et
al.,
		),
con
tains
descriptions
of
molecules
for
whic
h
the
m
utagenic
activit
y
has
to
b
e
predicted.
Originally
m
uta-
genicit
y
w
as
measured
b
y
a
real
n
um
b
er,
but
in
most
exp
erimen
ts
with
ILP
systems
this
has
b
een
discretized
in
to
t
w
o
v
alues
(m
utagenic
or
non-
m
utagenic),
making
the
task
a
classication
task.
The
data
set
consists
of
0
molecules,
whic
h
are
divided
in
to
t
w
o
sub-
sets:
regression-friendly
(
molecules)
and
regression-unfriendly
(


CHAPTER
.
TOP-DO
WN
INDUCTION
OF
F
OLDTS
molecules).
The
term
regression
here
refers
to
the
use
of
linear
regres-
sion,
not
regression
trees.
The
names
stem
from
the
fact
that
exp
erimen
ts
with
linear
regression
yielded
go
o
d
results
on
some
of
the
data
but
not
on
all.

Bio
degradabilit
y:
a
set
of

molecules
for
whic
h
structural
descrip-
tions
and
molecular
w
eigh
ts
are
giv
en.
The
bio
degradabilit
y
of
the
mo-
lecules
is
to
b
e
predicted.
This
is
a
real
n
um
b
er,
but
has
b
een
discretized
in
to
four
v
alues
(fast,
mo
derate,
slo
w,
resistan
t)
in
most
past
exp
erimen
ts.
The
dataset
w
as
pro
vided
to
us
b
y
Sa

so
D

zeroski
but
is
not
y
et
in
the
public
domain.

Musk:
the
aim
is
to
predict
for
a
set
of
molecules
whic
h
ones
are
m
usk
molecules
and
whic
h
ones
are
not.
Eac
h
molecule
can
ha
v
e
a
n
um
b
er
of
conformations,
and
the
molecule
is
m
usk
if
and
only
if
at
least
one
of
its
conformations
is
m
usk.
In
tro
duced
to
the
mac
hine
learning
comm
unit
y
b
y
Dietteric
h
et
al.
(		),
this
problem
w
as
used
to
illustrate
the
so-
called
m
ultiple-instance
problem:
an
example
is
not
represen
ted
b
y
a
tuple
but
b
y
a
set
of
tuples.
Multiple-instance
problems
are
hard
to
cop
e
with
for
prop
ositional
learners.
The
Musk
database
consists
of
t
w
o
data
sets,
a
small
one
(0K,

tuples)
and
a
large
one
(.MB,
00
tuples).
Eac
h
tuple
consists
of

n
umerical
attributes.
The
database
is
a
non-t
ypical
ILP
database,
b
ecause
of
its
orien
tation
to
w
ards
n
umerical
data
and
the
almost
prop
os-
itional
represen
tation.
It
is
a
v
ailable
at
the
UCI
data
rep
ository
.

Mesh:
this
data
set,
in
tro
duced
in
the
ILP
comm
unit
y
b
y
Dol

sak
and
Muggleton
(		),
has
its
origin
in
engineering.
F
or
man
y
engineering
applications,
surfaces
need
to
b
e
appro
ximated
b
y
a
nite
elemen
t
mesh.
Suc
h
a
mesh
needs
to
b
e
ne
in
some
places
(in
order
to
assure
accuracy),
and
can
b
e
coarser
in
other
places
(whic
h
decreases
the
computation
cost).
The
task
is
to
learn
rules
that
predict
ho
w
ne
a
mesh
should
b
e,
b
y
studying
a
n
um
b
er
of
meshes.
The
data
set
consists
of
descriptions
of

meshes.
It
is
a
t
ypical
ILP
data
set
in
that
it
con
tains
structural
information
and
a
lot
of
bac
kground
kno
wledge
is
a
v
ailable.
It
is
a
v
ailable
at
the
ILP
data
rep
ository
(Kaza-
k
o
v
et
al.,
		).

Diterp
enes:
a
detailed
description
of
this
application
can
b
e
found
in
(D

zeroski
et
al.,
		).
The
task
is
to
iden
tify
substructures
in
diterp
ene
molecules
b
y
lo
oking
at
the

C
NMR
sp
ectrogram
of
the
molecule
(p
eaks
o
ccurring
in
suc
h
a
sp
ectrogram
ma
y
indicate
the
o
ccurrence
of
certain
substructures).
This
is
a
m
ultiple
class
problem:
there
are

classes.

..
EXPERIMENT
AL
EV
ALUA
TION
	
This
problem
is
inheren
tly
relational,
but
prop
ositional
attributes
can
b
e
dened
(so-called
engineered
attributes)
that
are
highly
relev
an
t.
This
data
set
is
not
in
the
public
domain.
The
data
w
ere
kindly
pro
vided
to
us
b
y
Steen
Sc
h
ulze-Kremer
and
Sa

so
D

zeroski.
..
Building
Classication
T
rees
with
Tilde
Aim
of
the
exp
erimen
t
The
aim
of
this
exp
erimen
t
is
to
compare
Tilde's
p
erformance
with
that
of
other
systems
for
the
task
of
inducing
classiers.
W
e
w
an
t
to
in
v
estigate
sev
eral
asp
ects
of
its
p
erformance:
the
predictiv
e
accuracy
of
the
h
yp
othesis
that
is
induced,
its
in
terpretabilit
y
,
and
the
eciency
with
whic
h
it
is
induced.
Metho
dology
W
e
ha
v
e
ev
aluated
the
classication
subsystem
of
Tilde
b
y
p
erforming
exp
er-
imen
ts
on
sev
eral
data
sets,
and
comparing
the
obtained
results
with
results
published
in
the
literature.
F
or
all
the
exp
erimen
ts,
Tilde's
default
parameters
w
ere
used;
only
the
renemen
t
op
erator
and
n
um
b
er
of
thresholds
for
discretization,
when
applic-
able,
w
ere
supplied
man
ually
.
F
ull
details
on
the
exp
erimen
tal
settings,
as
w
ell
as
the
datasets
that
w
ere
used
(except
for
the
Diterp
enes
dataset,
whic
h
w
e
cannot
mak
e
public),
are
a
v
ailable
at
http://www.cs.ku
leu
ve
n.
ac.
be
/~m
l/
Til
de
/E
xpe
ri
men
ts
/
All
rep
orted
results
are
obtained
using
ten-fold
cross-v
alidations.

Materials
Tilde.
w
as
used
for
these
exp
erimen
ts.
This
is
a
stable
v
ersion
of
Tilde
that
is
a
v
ailable
for
academic
purp
oses
up
on
request.
The
data
sets
are
Mutagenesis,
Musk
and
Diterp
enes.
W
e
c
hose
these
data
sets
b
ecause
of
the
a
v
ailabilit
y
of
published
results.
Discussion
of
the
results
T
able
.
compares
Tilde's
p
erformance
on
the
Mutagenesis
problem
with
that
of
F
oil
(v
ersion
.)
and
Pr
ogol
(actually
P-Progol,
Sriniv
asan's
im-
plemen
tation
in
Prolog),
as
rep
orted
in
(Sriniv
asan
et
al.,
		)
(four
lev
els
of

Cross-v
alidation
is
a
metho
d
for
estimating
predictiv
e
accuracy
.
P
erforming
an
n-fold
cross-v
alidation
means
that
a
set
of
data
is
partitioned
in
to
n
subsets,
and
n
runs
are
p
er-
formed
where
for
eac
h
run
a
dieren
t
subset
is
set
apart
as
a
test
set,
while
the
other
n
 
subsets
form
the
training
set.
The
predictiv
e
accuracy
for
unseen
examples
is
computed
as
the
a
v
erage
predictiv
e
accuracy
on
the
n
test
sets.

0
CHAPTER
.
TOP-DO
WN
INDUCTION
OF
F
OLDTS
Accuracies
(%)
Times
(s)
Complexit
y
(literals)
B

B

B

B

B

B

B

B

B

B

B

B

Pr
ogol




k
k
k
k
.
.
.
	.	
F
oil




	0
	
0.
0.

	


Tilde

	



0


.
.
.
	.	
T
able
.:
Accuracies,
times
and
complexities
of
theories
found
b
y
Pr
ogol,
F
oil
and
Tilde
for
the
Mutagenesis
problem;
a
v
eraged
o
v
er
ten-fold
cross-
v
alidation.
Times
for
Tilde
w
ere
measured
on
a
Sun
SP
AR
Cstation-0,
for
the
other
systems
on
a
Hewlett
P
ac
k
ard
0.
Because
of
the
dieren
t
hardw
are,
times
should
b
e
considered
to
b
e
indicativ
e
rather
than
absolute.
Algorithm
%
correct
iterated-discrim
APR
	.
GFS
elim-kde
APR
0.
Tilde
	.
bac
kpropagation
net
w
ork
.
C.
.
T
able
.:
Comparison
of
accuracy
of
theories
obtained
with
Tilde
with
those
of
other
systems
on
the
Musk
dataset.
bac
kground
kno
wledge
B
i
are
distinguished
there,
eac
h
one
b
eing
a
sup
erset
of
its
predecessor,
see
App
endix
A).
F
or
the
n
umerical
data,
Tilde's
discret-
ization
pro
cedure
w
as
used.
Lo
ok
ahead
w
as
allo
w
ed
when
adding
bond-literals
(addition
of
a
b
ond
t
ypically
do
es
not
lead
to
an
y
gain,
but
enables
insp
ection
of
nearb
y
atoms).
F
rom
the
table
it
can
b
e
concluded
that
Tilde
ecien
tly
nds
theories
with
high
accuracy
.
The
complexit
y
of
the
induced
theories
is
harder
to
compare,
b
ecause
Tilde
uses
a
radically
dieren
t
format
to
represen
t
the
theory
.
When
simply
coun
ting
literals,
Tilde's
theories
are
ab
out
as
complex
as
Pr
ogol's,
but
clearly
simpler
than
F
oil's.
(Con
v
erting
the
trees
to
w
ards
decision
lists
and
then
coun
ting
literals
yields
m
uc
h
larger
n
um
b
ers
(resp
ectiv
ely
0.,
.0,
.
and
.
literals
for
B

to
B

)
due
to
duplication
of
man
y
literals,
but
this
comparison
metho
d
is
biased
in
fa
v
or
of
rule
induction
systems.)
The
fact
that
Tilde
nds
more
compact
theories
than
Pr
ogol
on
B

,
although
Pr
ogol
p
erforms
an
exhaustiv
e
searc
h
(using
the
A

-algorithm),
can
b
e
attributed
to
the
greater
expressivit
y
of
F
OLDTs.
With
the
Musk
dataset,
the
main
c
hallenge
w
as
its
size.
W
e
used
the
largest
of
the
t
w
o
Musk
data
sets
a
v
ailable
at
the
UCI
rep
ository
.
As
w
e
noted
b
efore,
this
data
set
is
not
a
t
ypical
ILP
data
set
(it
con
tains
mainly
n
umerical
data)
but
cannot
b
e
handled
w
ell
b
y
prop
ositional
learners
either.
Dietteric
h
et
al.'s
approac
h
(Dietteric
h
et
al.,
		)
is
to
adapt
prop
ositional
learning

..
EXPERIMENT
AL
EV
ALUA
TION

Prop
Rel
Both
F
oil
0.
.
.
Ribl
	.0
.
	.
Tilde
.
(.)
.0
(.0)
	0.
(0.)
T
able
.:
Accuracy
results
on
the
Diterp
enes
data
set,
making
use
of
pro-
p
ositional
data,
relational
data
or
b
oth;
standard
errors
for
Tilde
are
sho
wn
b
et
w
een
paren
theses.
algorithms
to
the
m
ultiple-instance
problem
in
the
sp
ecic
case
of
learning
single
axis-parallel
rectangles
(APR's).
F
or
ILP
systems
no
adaptations
are
necessary
.
Still,
Tilde's
p
erformance
is
comparable
with
most
other
algorithms
discussed
in
(Dietteric
h
et
al.,
		),
with
only
one
(sp
ecial-purp
ose)
algorithm
outp
erforming
the
others
(T
able
.).
F
or
the
exp
erimen
ts
with
Tilde,
all
the
n
umerical
attributes
w
ere
discretized.

The
a
v
erage
running
time
of
Tilde
on
one
cross-v
alidation
step
w
as
ab
out

hours.
F
or
the
Diterp
enes
data
set,
sev
eral
v
ersions
of
the
data
are
distinguished:
purely
prop
ositional
data
(con
taining
engineered
features),
relational
data
(non-
engineered),
and
b
oth.
Best
p
erformance
up
till
no
w
w
as
ac
hiev
ed
b
y
the
Ribl
system
(Emde
and
W
ettsc
herec
k,
		),
an
instance-based
relational
learner.
T
able
.
sho
ws
that
Tilde
ac
hiev
es
sligh
tly
lo
w
er
accuracy
than
Ribl,
but
outp
erforms
F
oil.
Moreo
v
er,
it
returns
a
sym
b
olic,
in
terpretable
(although
complex)
theory
,
in
con
trast
to
Ribl.
Conclusions
In
all
these
exp
erimen
ts,
w
e
ha
v
e
compared
Tilde's
accuracies
with
the
b
est
kno
wn
results,
whic
h
w
ere
obtained
with
dieren
t
systems.
As
far
as
predict-
iv
e
accuracy
is
concerned,
Tilde
do
es
not
outp
erform
the
b
est
systems,
but
consisten
tly
p
erforms
almost
as
w
ell.
The
complexit
y
of
the
theories
it
yields
is
usually
comparable
with
that
of
other
systems,
sometimes
b
etter.
With
resp
ect
to
eciency
,
the
system
seems
to
p
erform
v
ery
w
ell.
..
The
Inuence
of
Lo
ok
ahead
Aim
W
e
ha
v
e
argued
that
lo
ok
ahead
is
a
useful
extension
to
Tilde;
with
this
exp
er-
imen
t
w
e
w
an
t
to
v
alidate
this
claim.
W
e
w
an
t
to
in
v
estigate
not
only
whether
lo
ok
ahead
enables
Tilde
to
nd
b
etter
h
yp
otheses,
but
also
ho
w
it
aects
Tilde's
eciency
.

The
n
um
b
er
of
discretization
b
ounds
w
as
determined
b
y
running
exp
erimen
ts
on
the
smaller
dataset
and
c
ho
osing
the
n
um
b
er
of
b
ounds
that
w
orks
b
est
on
that
set.


CHAPTER
.
TOP-DO
WN
INDUCTION
OF
F
OLDTS
Mutagenesis
Mesh
accuracy
time
accuracy
time
Tilde,
no
lo
ok
ahead
.
s
.
s
Tilde,
lo
ok
ahead
.0
	s
.
s
T
able
.:
Comparison
of
Tilde's
p
erformance
with
and
without
lo
ok
ahead
on
the
Mutagenesis
and
Mesh
data
sets.
Metho
dology
W
e
ha
v
e
tested
the
eect
of
lo
ok
ahead
on
sev
eral
data
sets.
F
or
eac
h
data
set
w
e
compared
Tilde's
p
erformance
with
lo
ok
ahead
to
its
p
erformance
without
lo
ok
ahead.
The
datasets
rep
eatedly
w
ere
partitioned
randomly
in
to
0
subsets.
Tw
o
ten-fold
cross-v
alidations
w
ere
run
based
on
eac
h
suc
h
partition;
one
without
allo
wing
lo
ok
ahead,
and
one
with
lo
ok
ahead.
F
or
eac
h
single
partition
the
accuracy
of
Tilde
with
and
without
lo
ok
ahead
w
as
compared.
Materials
Tilde.
w
as
emplo
y
ed
for
this
exp
erimen
t.
W
e
used
t
w
o
ILP
data
sets:
Muta-
genesis
and
Mesh.
These
t
w
o
w
ere
c
hosen
b
ecause
they
are
widely
used
as
ILP
b
enc
hmarks,
and
b
ecause
they
con
tain
structural
data
where
prop
erties
of
neigh
b
oring
substructures
(atoms
or
edges)
are
imp
ortan
t
for
classication,
but
the
link
to
a
neigh
b
or
itself
(bond
and
neighbour
predicates)
pro
vides
little
or
no
gain
(therefore
lo
ok
ahead
is
imp
ortan
t).
Discussion
In
Figure
.,
eac
h
dot
represen
ts
one
partition;
dots
ab
o
v
e
the
straigh
t
line
are
those
partitions
where
accuracy
with
lo
ok
ahead
w
as
higher
than
without
lo
ok
ahead.
F
or
b
oth
the
Mutagenesis
and
Mesh
datasets,
lo
ok
ahead
in
v
ariably
yields
an
increase
in
predictiv
e
accuracy
except
in
one
case
(where
it
sta
ys
the
same).
The
h
yp
othesis
that
lo
ok
ahead
do
es
not
yield
impro
v
emen
t
on
these
data
sets
can
b
e
rejected
at
the
%
lev
el.
T
able
.
compares
the
a
v
erage
running
times
needed
b
y
Tilde
for
inducing
a
single
h
yp
othesis.
The
table
conrms
that
lo
ok
ahead
is
computationally
exp
ensiv
e,
but
comparing
the
times
for
Mutagenesis
with
those
of
Pr
ogol
in
T
able
.
suggests
that
it
is
still
m
uc
h
c
heap
er
than,
e.g.,
p
erforming
an
exhaustiv
e
searc
h
(whic
h
Pr
ogol
do
es).

..
EXPERIMENT
AL
EV
ALUA
TION

0.7
0.72
0.74
0.76
0.78
0.8
0.7
0.72
0.74
0.76
0.78
0.8
lookahead
no lookahead
0.58
0.6
0.62
0.64
0.66
0.68
0.58
0.6
0.62
0.64
0.66
0.68
lookahead
no lookahead
(a)
(b)
Figure
.:
Comparison
of
Tilde's
p
erformance
with
and
without
lo
ok
ahead,
(a)
on
the
Mutagenesis
data;
(b)
on
the
Mesh
data.
Conclusions
The
exp
erimen
ts
sho
w
that
the
abilit
y
to
use
lo
ok
ahead
can
impro
v
e
the
p
er-
formance
of
Tilde.
Its
computational
complexit
y
also
increases,
but
is
still
acceptable.
By
letting
the
user
con
trol
the
lo
ok
ahead,
it
is
p
ossible
to
k
eep
the
computational
complexit
y
to
a
strict
minim
um
and
only
use
lo
ok
ahead
where
it
really
is
useful.
..
The
Inuence
of
Discretization
Aim
The
aim
of
this
exp
erimen
t
is
to
study
empirically
ho
w
discretization
inuences
Tilde's
p
erformance
with
resp
ect
to
eciency
and
predictiv
e
accuracy
.
Metho
dology
The
most
frequen
tly
o
ccurring
approac
hes
to
n
um
b
er
handling
in
sym
b
olic
learning
consist
of
allo
wing
tests
of
the
form
V
<
v
,
where
v
can
b
e
either
an
y
v
alue
in
the
domain
of
V,
or
one
of
a
set
of
thresholds
returned
b
y
a
discretization
pro
cedure.
W
e
ha
v
e
run
Tilde
with
these
dieren
t
languages,
but
also
with
some
alternativ
e
languages
(allo
wing
equalit
y
tests
or
in
terv
al
tests),
adapted
to
the
sp
ecic
data
set.
Eac
h
run
consists
of
a
ten-fold
cross-v
alidation.
Materials
Tilde.
w
as
emplo
y
ed
for
these
exp
erimen
ts.
W
e
used
the
Musk
and
Diter-
p
enes
data
sets,
b
ecause
b
oth
con
tain
non-determinate
n
umerical
data,
whic
h


CHAPTER
.
TOP-DO
WN
INDUCTION
OF
F
OLDTS
mak
es
them
t
to
test
our
discretization
pro
cedure
on.
Discussion
F
or
the
Musk
data
set
w
e
tested
discretization
with
inequalities
and
equalit-
ies,
for
a
v
arious
n
um
b
er
of
thresholds.
By
using
discrete
equalit
y
tests
w
e
appro
ximate
the
setting
of
Dietteric
h
et
al.
(		),
who
learn
axis-parallel
rectangles.
F
or
the
Diterp
enes
data
set,
inequalities
and
in
terv
al
tests
w
ere
compared,
as
w
ell
as
not
using
an
y
discretization
at
all,
again
for
a
v
arious
n
um
b
er
of
thresholds.
In
Figure
.,
predictiv
e
accuracies
are
plotted
against
the
maximal
n
um
b
er
of
thresholds
that
w
as
giv
en,
for
eac
h
setting
and
for
eac
h
data
set.
In
the
Musk
domain,
the
curv
es
for
inequalities
and
equalities
are
quite
similar;
using
inequalities
seems
to
p
erform
sligh
tly
b
etter.
In
the
Diterp
enes
domain,
the
eect
of
discretization
is
v
ery
dieren
t
according
to
ho
w
the
discretization
thresholds
are
used:
using
in
terv
al
tests
increases
p
erformance,
while
using
only
inequalit
y
tests
decreases
it.
The
eect
also
dep
ends
on
the
n
um
b
er
of
thresholds
that
is
c
hosen.
Figure
.	
sho
ws
running
times
on
the
Diterp
enes
dataset.
There
are
no
sur-
prises
here:
discretization
yields
an
eciency
gain,
and
the
n
um
b
er
of
threshold
aects
the
induction
time
(linearly
when
inequalit
y
tests
are
used,
quadratic-
ally
for
in
terv
al
tests;
this
w
as
exp
ected
since
the
n
um
b
er
of
in
terv
als
increases
quadratically
with
the
n
um
b
er
of
thresholds).
Conclusions
Our
conclusions
are
that
the
w
a
y
in
whic
h
discretization
results
are
used
(dis-
crete
(in)equalities,
in
terv
als)
signican
tly
inuences
the
accuracy
of
the
in-
duced
theory
,
as
w
ell
as
the
eciency
of
the
induction
pro
cess.
Using
discret-
ization
do
es
not
guaran
tee
b
etter
p
erformance,
but
ma
y
mak
e
it
p
ossible.
It
is
up
to
the
user
to
c
ho
ose
a
suitable
approac
h.
..
Regression
Aim
The
aim
of
this
exp
erimen
t
is
to
ev
aluate
the
regression
subsystem
of
Tilde.
Metho
dology
W
e
ha
v
e
ev
aluated
Tilde-R
T
b
y
running
it
on
data
sets
and
comparing
the
results
with
those
of
other
regression
systems.
Due
to
limited
a
v
ailabilit
y
of

..
EXPERIMENT
AL
EV
ALUA
TION

0.78
0.8
0.82
0.84
0.86
0.88
1
2
3
4
5
6
7
accuracy
number of thresholds
inequalities
equalities
0.84
0.85
0.86
0.87
0.88
0.89
0.9
0.91
0
5
10
15
20
25
30
35
40
prediction accuracy
number of thresholds
intervals
inequalities
no discretization
(a)
(b)
Figure
.:
Inuence
of
n
um
b
er
of
thresholds
on
accuracy:
(a)
Musk
data-
set,
comparing
equalities
and
inequalities;
(b)
Diterp
enes
dataset,
comparing
in
terv
als
with
inequalities
and
no
discretization
at
all.
0
2000
4000
6000
8000
10000
12000
14000
0
5
10
15
20
25
30
35
40
running time (cpu-seconds)
number of thresholds
intervals
inequalities
no discretization
Figure
.	:
Comparison
of
running
times
for
the
dieren
t
approac
hes
(Diter-
p
enes
dataset).


CHAPTER
.
TOP-DO
WN
INDUCTION
OF
F
OLDTS
lea
v
e-one-out
Tilde
classication
acc.
=
0.
lea
v
e-one-out
Tilde-R
T
regression
RE
=
0.0
lea
v
e-one-out
Tilde-R
T
classif.
via
regression
acc.
=
0.
-fold
cross-v
al.
SR
T
regression
RE
=
0.
-fold
cross-v
al.
Tilde-R
T
regression
RE
=
.
T
able
.:
Comparison
of
regression
and
classication
on
the
bio
degradabilit
y
data.
RE
=
relativ
e
error
of
predictions;
acc.
=
prop
ortion
of
predictions
that
are
correct.
results
on
regression
in
ILP
for
whic
h
the
data
sets
are
accessible,
this
compar-
ison
is
v
ery
limited.
W
e
therefore
also
discretized
the
predictions
afterw
ards
and
compare
with
results
obtained
b
y
classiers.
Materials
W
e
used
the
Tilde-R
T
algorithm
as
implemen
ted
in
Tilde.0.
The
system
w
as
run
on
the
Bio
degradabilit
y
data
set.
F
or
eac
h
molecule
bio
degradabilit
y
w
as
predicted
on
a
logarithmic
scale
(since
there
is
h
uge
v
ariation
in
the
original
v
alues).
Giv
en
the
small
size
of
the
data
set,
no
v
alidation
set
w
as
used;
instead
w
e
used
the
F-test
stopping
criterion
(signicance
lev
el
0.0).
Discussion
T
able
.
compares
Tilde-R
T's
p
erformance
with
Tilde's
(classication,
lea
v
e-
one-out)
and
SR
T's
(regression,
sixfold).
The
SR
T
exp
erimen
ts
w
ere
p
erformed
b
y
Stefan
Kramer.
The
comparison
with
SR
T
is
based
on
the
relativ
e
error
of
the
predictions.
As
the
table
sho
ws,
Tilde-R
T
scores
w
orse
than
SR
T
for
this
criterion.
F
or
comparing
Tilde-R
T
with
Tilde,
w
e
ha
v
e
discretized
Tilde-R
T's
predictions
in
to
four
v
alues,
corresp
onding
to
the
four
classes
that
are
usually
distinguished
when
the
problem
is
handled
as
a
classication
problem.
According
to
this
qualit
y
criterion,
Tilde-R
T
turns
out
to
score
quite
w
ell.
Conclusions
These
results
indicate
that
there
is
clearly
ro
om
for
impro
v
emen
t
with
resp
ect
to
using
Tilde
for
regression.
A
p
ossible
reason
for
the
fact
that
it
p
erforms
w
orse
than
SR
T
is
that
SR
T
is
sp
ecialized
in
regression
and
is
m
uc
h
more
sophisticated.
Note
that
Tilde-R
T
is
just
a
trivial
instan
tiation
of
TIC.
While
SR
T's
approac
h
is
computationally
more
complex,
this
certainly
seems
to
pa
y
o
with
resp
ect
to
accuracy
.

..
EXPERIMENT
AL
EV
ALUA
TION

The
fact
that
in
a
classication
setting
Tilde-R
T
scores
as
high
as
Tilde
is
rather
surprising,
giv
en
the
comparison
with
SR
T.
It
suggests
that,
while
Tilde-R
T
is
not
optimal
with
resp
ect
to
regression,
the
approac
h
ma
y
b
e
comp
etitiv
e
with
classical
approac
hes
to
classication,
in
domains
where
the
classication
task
is
deriv
ed
from
a
regression
task.

These
results
pro
vide
an
argumen
t
in
fa
v
or
of
using
regression
systems
for
regression
problems,
and
against
turning
the
regression
problem
in
to
a
classication
problem
and
then
using
a
classier.
..
Clustering
Aim
The
aim
of
this
exp
erimen
t
is
to
ev
aluate
the
clustering
subsystem
of
Tilde
(TIC)
with
resp
ect
to
its
abilit
y
to
form
(in
an
unsup
ervised
manner)
clusters
that
are
useable
for
exible
prediction
and
for
classication.
Metho
dology
W
e
ha
v
e
run
TIC
on
sev
eral
data
sets,
comparing
its
p
erformance
with
other
unsup
ervised
learners.
Tw
o
ev
aluation
criteria
w
ere
used:
the
abilit
y
to
iden
tify
predened
classes
(unsup
ervised
classication),
and
the
coherence
of
the
clusters
with
resp
ect
to
all
attributes
(i.e.
ho
w
w
ell
can
the
v
alues
of
attributes
b
e
pre-
dicted
if
one
kno
ws
the
cluster
an
instance
b
elongs
to).
F
or
all
the
exp
erimen
ts
euclidean
distances
w
ere
computed
from
all
n
umer-
ical
attributes,
except
when
stated
otherwise.
F
or
the
So
yb
eans
data
sets
all
nominal
attributes
w
ere
con
v
erted
in
to
n
um
b
ers
rst.
All
the
rep
orted
results
w
ere
obtained
o
v
er
a
ten-fold
cross-v
alidation.
F
or
unsup
ervised
classication,
the
system
w
as
ev
aluated
as
follo
ws:
learn-
ing
is
unsup
ervised,
but
classes
are
assumed
to
b
e
kno
wn
at
ev
aluation
time
(the
class
of
a
test
example
is
compared
with
the
ma
jorit
y
class
of
the
leaf
the
example
is
sorted
in
to).
F
or
exible
prediction,
w
e
follo
w
ed
the
follo
wing
strategy:
using
the
training
set
a
clustering
tree
is
induced.
Then,
all
examples
of
the
test
set
are
sorted
in
this
hierarc
h
y
,
and
the
prediction
for
all
of
their
attributes
is
ev
aluated.
F
or
eac
h
attribute,
the
v
alue
that
o
ccurs
most
frequen
tly
in
a
leaf
is
predicted
for
all
test
examples
sorted
in
to
that
leaf.
These
exp
erimen
ts
w
ere
p
erformed
in
co-op
eration
with
Jan
Ramon.

One
migh
t
w
onder
whether
it
is
not
simply
the
case
that
Tilde
p
erforms
badly
on
this
data
set.
A
comparison
with
other
learners
on
this
data
set
has
sho
wn
that
Tilde's
p
erformance
is
at
par
with
that
of
most
other
classication
systems.
Hence,
our
conclusions
remain
v
alid.


CHAPTER
.
TOP-DO
WN
INDUCTION
OF
F
OLDTS
So
yb
eans
Iris
Avg.
accuracy
a
vg.
tree
size
Avg.
accuracy
a
vg.
tree
size
TIC
	%
.	
no
des
	
%

no
des
Tilde
00
%

no
des
	%

no
des
T
able
.:
Comparing
TIC
with
a
sup
ervised
learner.
Materials
W
e
used
TIC,
as
implemen
ted
in
Tilde.0.
Relev
an
t
data
sets
are
So
yb
eans
and
Iris.
These
are
data
sets
that
ha
v
e
b
een
used
for
clustering
b
efore.
Discussion
Unsup
ervised
Classication:
W
e
applied
TIC
to
the
small
So
yb
eans
and
Iris
databases,
p
erforming
ten-fold
cross-v
alidations.
Learning
is
unsup
ervised,
but
classes
are
assumed
to
b
e
kno
wn
at
ev
aluation
time
(the
class
of
a
test
example
is
compared
with
the
ma
jorit
y
class
of
the
leaf
the
example
is
sor-
ted
in
to).
T
able
.
compares
the
results
with
those
obtained
b
y
Tilde
in
a
sup
ervised
setting.
W
e
see
that
TIC
obtains
high
accuracies
for
these
problems.
The
only
clustering
result
w
e
kno
w
of
is
the
one
of
Cobweb,
whic
h
obtained
00%
on
the
So
yb
eans
data
set
(this
do
es
not
dier
signican
tly
from
TIC's
accuracy).
Tilde's
accuracies
don't
dier
m
uc
h
from
those
of
TIC
whic
h
induced
the
hierarc
h
y
without
kno
wledge
of
the
classes.
T
ree
sizes
are
smaller
though.
This
conrms
our
earlier
remark
that
clustering
in
the
instance
space
I
yields
more
and
smaller
clusters
than
clustering
in
the
prediction
space
P
.
Flexible
prediction:
W
e
used
the
large
So
yb
eans
database,
with
pruning.
The
qualit
y
criterion
used
for
the
pruning
is
the
in
tra-cluster
v
ariation
S
S
W
(see
Equation
.).
T
able
.
summarizes
the
accuracies
obtained
for
eac
h
attribute
and
com-
pares
with
the
accuracy
of
ma
jorit
y
prediction.
The
high
accuracies
sho
w
that
most
attributes
can
b
e
predicted
v
ery
w
ell,
whic
h
means
the
clusters
are
v
ery
coheren
t.
The
mean
accuracy
of
.%
is
sligh
tly
lo
w
er
than
the
%
rep
or-
ted
b
y
Fisher
(		),
but
again
the
dierence
is
not
statistically
signican
t.
Note
that
b
oth
this
and
the
previous
ev
aluation
metho
d
aim
at
measur-
ing
the
usefulness
of
the
clustering
for
predictions.
The
descriptiv
e
qualit
y
of
the
clustering
is
not
measured,
although
the
size
of
the
tree
for
the
So
yb
eans
database
indicates
that
sub
clusters
of
the
ideal
clusters
are
found,
hence
from
a
descriptiv
e
p
oin
t
of
view
the
clustering
is
not
optimal.
A
b
etter
stopping
criterion
or
p
ost-pruning
metho
d
migh
t
help
here.

..
EXPERIMENT
AL
EV
ALUA
TION
	
name
range
default
acc.
name
range
default
acc.
date
0-
.%
.%
plan
t
stand
0-
.%
.0%
precip
0-
.%
	.%
temp
0-
.%
.%
hail
0-
.%
.%
crop
hist
0-
.%
.0%
area
damaged
0-
.	%
.%
sev
erit
y
0-
	.%
.%
seed
tm
t
0-
.%
.%
germination
0-
.%
.0%
plan
t
gro
wth
0-
.%
	.%
lea
v
es
0-
	.%
	.%
leafsp
ots
halo
0-
	.%
.%
leafsp
ots
marg
0-
.%
.%
leafsp
ots
size
0-
.%
.0%
leaf
shread
0-
.	%
.%
leaf
malf
0-
.%
.%
leaf
mild
0-
.%
.	%
stem
0-
.%
	.%
lo
dging
0-
0.%
0.0%
stem
cank
ers
0-
.%
	0.%
cank
er
lesion
0-
	.%
.	%
fruiting
b
o
dies
0-
.%
.%
external
deca
y
0-
.%
	.%
m
ycelium
0-
	.%
	.%
in
t
discolor
0-
.%
	.%
sclerotia
0-
	.%
	.%
fruit
p
o
ds
0-
.%
	.%
fruit
sp
ots
0-
.%
.0%
seed
0-
.	%
.%
mold
gro
wth
0-
0.%
.%
seed
discolor
0-
	.%
.0%
seed
size
0-
.%
.%
shriv
eling
0-
.%
.	%
ro
ots
0-
.%
	.%
T
able
.:
Prediction
of
all
attributes
together
in
the
So
yb
eans
data
set.
Conclusions
F
rom
a
predictiv
e
clustering
p
oin
t
of
view,
TIC
p
erforms
appro
ximately
as
w
ell
as
other
clustering
systems.
F
rom
a
descriptiv
e
p
oin
t
of
view,
it
p
erforms
w
orse;
it
tends
to
nd
o
v
erly
sp
ecic
clusters.
..
The
Eect
of
Pruning
on
Clustering
Aim
In
this
exp
erimen
t
w
e
ev
aluate
the
eect
of
our
p
ost-pruning
metho
d
in
TIC.
Note
that
the
eect
of
p
ost-pruning
has
b
een
studied
extensiv
ely
in
the
clas-
sication
and
regression
con
text
(see
e.g.
(Quinlan,
		a;
Breiman
et
al.,
	)),
but
m
uc
h
less
within
clustering
(an
exception
is
(Fisher,
		),
but
the
pruning
metho
d
describ
ed
there
diers
signican
tly
from
ours).
A
related
task
within
the
clustering
eld
is
\cutting"
a
hierarc
hical
clustering
at
some
lev
el
to
deriv
e
a
at
clustering
(see,
e.g.,
(Kirsten
and
W
rob
el,
		)).
Ho
w
ev
er,
the
aim
of
nding
an
optimal
at
clustering
diers
from
that
of
nding
an
optimal
hierarc
hical
clustering,
so
the
qualit
y
criteria
(and
hence
the
tec
hniques)
dier.
Metho
dology
The
clustering
subsystem
of
Tilde
is
run
with
and
without
pruning,
and
the
results
are
compared.
Since
the
eect
of
pruning
migh
t
dep
end
on
the
size
of
the
v
alidation
set,
w
e
exp
erimen
t
with
dieren
t
v
alidation
set
sizes.
T
en-fold
cross-v
alidations
are
p
erformed
in
all
cases.
In
eac
h
run
the
algorithm
divides
the
learning
set
in
a
training
set
and
a
v
alidation
set.
Clustering
trees
are
built

0
CHAPTER
.
TOP-DO
WN
INDUCTION
OF
F
OLDTS
68
70
72
74
76
78
80
82
84
15
20
25
30
35
40
45
50
55
accuracy (%)
size validation set (%)
accurracy of unpruned tree
accurracy of pruned tree
20
30
40
50
60
70
80
90
15
20
25
30
35
40
45
50
55
size of tree (nodes)
size validation set (%)
size of unpruned tree
size of pruned tree
Figure
.0:
So
yb
eans:
a)
Accuracy
b
efore
and
after
pruning;
b)
n
um
b
er
of
no
des
b
efore
and
after
pruning.
and
pruned
in
an
unsup
ervised
manner.
The
clustering
hierarc
h
y
b
efore
and
after
pruning
is
ev
aluated
b
y
trying
to
predict
the
class
of
eac
h
of
the
examples
in
the
test
set
(i.e.,
during
ev
aluation
the
classes
are
assumed
to
b
e
kno
wn).
This
exp
erimen
t
w
as
p
erformed
in
co-op
eration
with
Jan
Ramon.
Materials
W
e
ha
v
e
applied
TIC,
as
implemen
ted
in
Tilde.0,
to
t
w
o
databases:
large
So
yb
eans
and
Mutagenesis.
F
or
the
Mutagenesis
exp
erimen
ts,
the
tests
allo
w
ed
in
the
trees
can
mak
e
use
of
structural
information
only
(Bac
kground
B

),
though
the
heuristics
(the
distances)
mak
e
use
of
the
n
umerical
information
included
in
Bac
kground
B

.
Discussion
In
Figure
.0,
the
a
v
erage
accuracy
of
the
clustering
hierarc
hies
b
efore
and
after
pruning
is
plotted
against
the
size
of
the
v
alidation
set
(this
size
is
a
parameter
of
TIC),
and
the
same
is
done
for
the
tree
complexit
y
.
The
same
results
for
the
Mutagenesis
database
are
summarized
in
Figure
..
W
e
see
that
for
So
yb
eans
TIC's
pruning
metho
d
results
in
a
sligh
t
decrease
in
accuracy
but
a
large
decrease
in
the
n
um
b
er
of
no
des.
The
pruning
strategy
seems
relativ
ely
stable
w.r.t.
the
size
of
the
v
alidation
set.
The
Mutagenesis
exp
erimen
t
conrms
these
ndings
(though
the
decrease
in
accuracy
is
less
clear
here).
Conclusions
These
exp
erimen
ts
sho
w
that
our
p
ost-pruning
metho
d
for
predictiv
e
clustering
is
useful:
it
decreases
the
size
of
the
tree
without
its
accuracy
suering
from
this
to
o
m
uc
h.

..
EXPERIMENT
AL
EV
ALUA
TION

72
73
74
75
76
77
78
79
80
81
15
20
25
30
35
40
45
50
55
accuracy (%)
size validation set (%)
accuracy of pruned tree
accuracy of unpruned tree
15
20
25
30
35
40
45
50
15
20
25
30
35
40
45
50
55
size of tree (nodes)
size validation set (%)
size of pruned tree
size of unpruned tree
Figure
.:
Mutagenesis:
Accuracy
and
size
of
the
clustering
trees.
..
Handling
Missing
Information
Aim
Earlier
w
e
ha
v
e
explained
ho
w
clustering
based
predictiv
e
induction
allo
ws
for
more
robust
induction
than
the
classical
induction
of
classication
trees,
b
ecause
the
heuristics
can
mak
e
use
of
other
information
than
only
the
target
v
ariable.
More
sp
ecically
,
when
man
y
class
v
alues
are
missing
or
noisy
,
the
\clustering
in
I
"
tec
hnique
should
yield
b
etter
theories
than
the
\clustering
in
P
"
tec
hnique.
With
this
exp
erimen
t
w
e
try
to
v
alidate
this
claim.
Metho
dology
W
e
ha
v
e
exp
erimen
ted
with
lea
ving
out
information
from
the
Mutagenesis
data
set.
In
one
exp
erimen
t
only
class
information
is
used
to
compute
the
distance,
in
another
exp
erimen
t
all
three
n
umerical
v
ariables
a
v
ailable
in
Bac
kground
B

are
used.
Lik
e
for
the
pruning
exp
erimen
t,
the
tests
allo
w
ed
in
the
trees
only
mak
e
use
of
Bac
kground
B

.
Information
w
as
left
out
from
the
data
as
follo
ws:
if
the
prop
ortion
of
a
v
ailable
information
is
desired
to
b
e
p,
for
eac
h
example
and
for
eac
h
v
ariable
there
is
a
probabilit
y
p
that
the
v
alue
is
a
v
ailable.
The
presence
of
information
on
one
v
ariable
is
th
us
indep
enden
t
of
the
presence
of
information
on
another
v
ariable.
Materials
A
predecessor
of
the
curren
t
TIC
called
C0.
(De
Raedt
and
Blo
c
k
eel,
		),
w
as
used
for
these
exp
erimen
ts.
Only
one
data
set
is
used:
Mutagenesis.


CHAPTER
.
TOP-DO
WN
INDUCTION
OF
F
OLDTS
a
v
ailable
n
umerical
data
logm
utag
all
three
00%
0.0
0.
0%
0.
0.	
%
0.
0.
0%
0.
0.
T
able
.	:
Classication
accuracies
obtained
for
Mutagenesis
with
sev
eral
dis-
tance
functions,
and
on
sev
eral
lev
els
of
missing
information.
0.66
0.68
0.7
0.72
0.74
0.76
0.78
0.8
0.82
0
10
20
30
40
50
60
70
80
90
accuracy
proportion of numerical information that is missing
clustering on class attribute
clustering on 3 attributes
Figure
.:
Ev
olution
of
predictiv
e
accuracy
in
the
presence
of
missing
v
alues.
Discussion
T
able
.	
and
Figure
.
sho
w
ho
w
the
p
erformance
of
the
system
degrades
with
a
decreasing
amoun
t
of
information.
It
can
b
e
seen
that
p
erformance
degrades
less
quic
kly
when

n
umerical
v
ariables
are
used
b
y
the
heuristics,
than
when
only
the
class
information
is
used.
Note
that
this
exp
erimen
t
is
similar
in
spirits
to
the
ones
p
erformed
with
COLA
(Emde,
		).
A
dierence
is
that
COLA
p
erforms
unsup
ervised
clustering,
while
Tilde
can
use
class
information
when
it
is
a
v
ailable
(and
hence
is
a
bit
more
informed
than
COLA).
Conclusions
This
exp
erimen
t
conrms
our
h
yp
othesis
that
the
use
of
other
than
just
class
information
ma
y
cause
a
system
to
p
erform
b
etter
in
the
presense
of
missing
class
v
alues.
.
Related
w
ork
This
c
hapter
compiles
and
extends
the
descriptions
of
Tilde
and
its
auxiliary
algorithms
that
app
eared
in
(Blo
c
k
eel
and
De
Raedt,
		b),
(Blo
c
k
eel
and

..
RELA
TED
W
ORK

De
Raedt,
		)
and
(Blo
c
k
eel
et
al.,
		b).
Most
exp
erimen
tal
results
ha
v
e
also
app
eared
in
these
pap
ers.
(D

zeroski
et
al.,
		)
p
erform
an
extensiv
e
comparison
of
dieren
t
ILP
systems
on
the
Diterp
enes
data
set,
whic
h
con
tains
some
exp
erimen
ts
p
erformed
with
Tilde
that
are
not
included
in
this
text.
The
conclusions
in
that
article
are
similar
to
ours.
Ramon
and
Bruyno
oghe
(		)
ha
v
e
p
erformed
exp
erimen
ts
with
TIC
using
their
rst-order
distance
and
rep
ort
promising
results.
The
Tilde-R
T
system
has
also
b
een
used
for
exp
erimen
ts
with
reinforcemen
t
learning
in
structured
domains
(D

zeroski
et
al.,
		).
W
e
ha
v
e
discussed
ho
w
Tilde
upgrades
prop
ositional
tec
hniques
to
the
rst
order
logic
con
text.
This
upgrading
metho
dology
is
not
sp
ecic
for
Tilde,
nor
for
induction
of
decision
trees.
It
can
also
b
e
used
for
rule
induction,
disco
v
ery
of
asso
ciation
rules,
and
other
kinds
of
disco
v
ery
in
the
learning
from
in
terpretations
setting.
Systems
suc
h
as
ICL
(De
Raedt
and
V
an
Laer,
		)
and
W
armr
(Dehasp
e
and
De
Raedt,
		)
are
illustrations
of
this.
Both
learn
from
in
terpretations
and
upgrade
prop
ositional
tec
hniques.
ICL
learns
rst
order
rule
sets,
upgrading
the
tec
hniques
used
in
CN,
and
W
armr
learns
a
rst
order
equiv
alen
t
of
asso
ciation
rules
(\asso
ciation
rules
o
v
er
m
ultiple
relations").
W
armr
has
b
een
designed
sp
ecically
for
large
databases
and
emplo
ys
an
ecien
t
algorithm
that
is
an
upgrade
of
Apriori
(Agra
w
al
et
al.,
		).
Tilde
is
of
course
strongly
related
to
other
tree
induction
systems.
As
men-
tioned,
it
b
orro
ws
tec
hniques
from
C.
(Quinlan,
		a)
and
Car
t
(Breiman
et
al.,
	).
The
dierence
with
these
systems
is
that
Tilde
w
orks
with
rst
order
represen
tations.
In
this
resp
ect
it
is
closer
to
Str
uct
(W
atanab
e
and
Rendell,
		)
and
SR
T
(Kramer,
		).
The
main
dierences
with
these
systems
are
that
Tilde
learns
from
in
terpretations,
is
more
general
(in
the
sense
that
the
other
systems
fo
cus
on
classication,
resp
ectiv
ely
regression,
while
Tilde
can
do
b
oth
and
in
addition
p
erform
unsup
ervised
learning
and
exible
prediction)
and
that
its
structure
emphasizes
the
similarities
b
et
w
een
a)
prop
ositional
learning
and
learning
from
in
terpretations
and
b)
dieren
t
induction
tasks.
Moreo
v
er,
Tilde
returns
h
yp
otheses
with
a
clear
seman
tics
(as
explained
in
Chapter
),
in
con
trast
with
Str
uct
and
SR
T
where
this
issue
is
not
discussed.
Finally
,
Tilde
mak
es
use
of
mo
dern
ILP
tec
hnology
(discretization,
lo
ok
ahead,
.
.
.
)
and
w
ell-understo
o
d
tec
hniques
originating
in
prop
ositional
learning.
A
w
ell-kno
wn
rst-order
clusterer
is
KBG
(Bisson,
		a).
Dierences
are
that
KBG
uses
a
xed
rst-order
similarit
y
measure,
and
that
it
is
an
agglom-
erativ
e
clustering
algorithm
while
Tilde
is
a
divisiv
e
one.
The
divisiv
e
nature
of
Tilde
mak
es
it
as
ecien
t
for
clustering
as
classical
TDIDT
algorithms.


Assuming
that
distances
b
et
w
een
clusters
can
b
e
computed
ecien
tly
,
i.e.
with
time
complexit
y
linear
in
the
n
um
b
er
of
examples.


CHAPTER
.
TOP-DO
WN
INDUCTION
OF
F
OLDTS
A
nal
dierence
with
KBG
is
that
Tilde
directly
obtains
logical
descriptions
of
the
dieren
t
clusters
through
the
use
of
the
logical
decision
tree
format.
F
or
KBG,
these
descriptions
ha
v
e
to
b
e
deriv
ed
in
a
separate
step
b
ecause
the
clus-
tering
pro
cess
only
pro
duces
the
clusters
(i.e.
sets
of
examples)
and
not
their
description.
Other
approac
hes
to
rst
order
clustering
include
Kietz's
and
Morik's
Kluster
(Kietz
and
Morik,
		),
(Thompson
and
Langley,
		),
(Ketterlin
et
al.,
		),
(Y
o
o
and
Fisher,
		)
and
(Kirsten
and
W
rob
el,
		).
The
instance-based
learner
RIBL
(Emde
and
W
ettsc
herec
k,
		)
is
related
to
Tilde
via
the
link
b
et
w
een
instance-based
learning
and
predictiv
e
clustering
that
w
as
men
tioned
in
Chapter
.
RIBL
uses
an
adv
anced
rst
order
distance
metric
that
migh
t
b
e
a
go
o
d
candidate
for
incorp
oration
in
Tilde.
.	
Conclusions
In
this
c
hapter
w
e
ha
v
e
presen
ted
the
inductiv
e
logic
programming
system
Tilde.
W
e
ha
v
e
discussed
b
oth
the
w
a
y
in
whic
h
it
upgrades
prop
ositional
TDIDT
to
the
rst
order
logic
con
text,
and
the
w
a
y
in
whic
h
it
generalizes
o
v
er
dieren
t
induction
tasks.
Tilde
is
the
rst
ILP
system
that
can
p
erform
all
these
dieren
t
tasks.
Exp
erimen
ts
ha
v
e
sho
wn
that
the
system
is
fast,
according
to
ILP
standards,
and
usually
yields
h
yp
otheses
of
high
qualit
y
(b
oth
with
resp
ect
to
in
terpretabilit
y
and
correctness).
Its
main
shortcoming
seems
to
b
e
lo
cated
in
the
regression
subsystem,
where
a
comparison
with
SR
T
suggests
that
it
can
b
e
impro
v
ed.
W
e
p
oin
t
out
that
curren
tly
the
regression
subsystem
is
a
trivial
instan
tiation
of
the
clustering
subsystem
that
do
es
not
con
tain
an
y
sp
ecialized
tec
hniques
y
et;
this
ma
y
accoun
t
for
the
dierence
in
p
erformance
b
et
w
een
SR
T
and
Tilde.
Sev
eral
factors
con
tribute
to
the
o
v
erall
go
o
d
p
erformance
of
Tilde:

the
success
of
TDIDT
in
prop
ositional
learning;

the
usefulness
of
learning
from
in
terpretations
to
upgrade
prop
ositional
tec
hniques;

the
fact
that,
thanks
to
the
lo
calit
y
assumption,
learning
from
in
terpret-
ations
allo
ws
to
partially
preserv
e
the
eciency
of
prop
ositional
learning;

the
fact
that
the
lo
calit
y
assumption
do
es
not
sev
erely
limit
the
practical
applicabilit
y
of
the
system.
A
stable
v
ersion
of
Tilde,
Tilde.,
is
a
v
ailable
for
academic
purp
oses
(see
http://www.cs.k
ule
uv
en.
ac
.b
e/~
ml
/Ti
ld
e/).
Tilde.
only
induces
classication
trees,
no
regression
or
clustering
trees.
Tilde.0,
whic
h
do
es
incorp
orate
these
features,
is
only
a
protot
yp
e
and
not
publicly
a
v
ailable
y
et.

Chapter

Scaling
up
Tilde
T
o
w
ards
Large
Data
Sets
.
In
tro
duction
Scalabilit
y
is
an
imp
ortan
t
issue
in
data
mining:
a
go
o
d
data
mining
system
needs
to
b
e
able
to
handle
large
data
sets.
In
the
previous
c
hapters
w
e
ha
v
e
sho
wn
that
it
is
p
ossible
to
upgrade
tec
hniques
and
eciency
results
from
prop
ositional
learning
to
inductiv
e
logic
programming
b
y
learning
from
in
ter-
pretations.
By
upgrading
Quinlan's
result
on
the
time
complexit
y
of
learning
decision
trees,
w
e
ha
v
e
sho
wn
that
Tilde
can
b
e
exp
ected
to
scale
up
w
ell,
at
least
as
far
as
time
complexit
y
is
concerned.
Ho
w
ev
er,
time
complexit
y
is
not
the
only
limitation
on
the
size
of
the
data
sets
that
an
algorithm
can
handle;
memory
restrictions
also
need
to
b
e
tak
en
in
to
consideration.
Ev
en
though
at
rst
sigh
t
memory
ma
y
seem
to
b
e
b
ecom-
ing
less
imp
ortan
t,
as
curren
t
hardw
are
often
oers
h
undreds
of
megab
ytes
of
in
ternal
memory
,
the
size
of
the
databases
that
p
eople
w
an
t
to
mine
also
gro
ws,
and
it
do
es
not
seem
realistic
to
sa
y
that
within
a
few
y
ears
mac
hines
will
exist
with
so
m
uc
h
in
ternal
memory
that
they
can
load
a
whole
database
at
once.
Still,
loading
all
data
at
once
is
what
most
mac
hine
learning
and
data
min-
ing
systems
do
no
w
ada
ys,
and
ILP
systems
are
no
exception
to
that.
In
the
prop
ositional
learning
comm
unit
y
,
p
eople
ha
v
e
recen
tly
started
taking
the
prob-
lem
of
mining
large
databases
seriously
,
and
ha
v
e
devised
algorithms
for
mining
large
databases
ecien
tly
without
ha
ving
to
load
h
uge
amoun
ts
of
data
in
to
main
memory
.
Examples
are
(Agra
w
al
et
al.,
		)
and
(Meh
ta
et
al.,
		),
where
algorithms
are
giv
en
for
the
disco
v
ery
of
asso
ciation
rules,
resp
ectiv
ely
induction
of
decision
trees,
that
w
ork
with
data
stored
in
an
external
database
and
minimize
the
database
access
that
is
needed,
for
instance
b
y
doing
as
man
y



CHAPTER
.
SCALING
UP
Tilde
computations
as
p
ossible
o-line.
In
this
c
hapter
w
e
discuss
a
re-implemen
tation
of
Tilde
that
is
based
on
the
decision
tree
algorithm
prop
osed
b
y
Meh
ta
et
al.
(		).
This
w
ork
can
b
e
seen
as
one
more
illustration
of
ho
w
prop
ositional
tec
hniques
can
b
e
upgraded
to
the
rst
order
logic
setting
in
the
learning
from
in
terpretations
framew
ork.
It
also
demonstrates
the
feasibilit
y
of
using
Tilde
for
learning
from
large
data
sets.
The
re-implemen
tation
is
only
a
protot
yp
e;
it
fo
cuses
on
classication
and
do
es
not
pro
vide
all
the
functionalit
y
of
the
original
Tilde
system.
It
is
w
ork
ed
out
sucien
tly
,
ho
w
ev
er,
for
us
to
illustrate
its
feasibilit
y
and
ev
aluate
it
ex-
p
erimen
tally
.
In
Section
.
w
e
discuss
the
alternativ
e
implemen
tation
of
Tilde
and
com-
pare
it
with
the
classical
implemen
tation.
W
e
next
discuss
the
inuence
the
implemen
tation
has
on
the
feasibilit
y
of
certain
optimizations
(Section
.).
In
Section
.
w
e
presen
t
some
exp
erimen
ts
to
v
alidate
our
claims.
Some
of
these
exp
erimen
ts
in
v
olv
e
data
sets
of
00MB
or
more,
illustrating
that,
con
trary
to
a
seemingly
wide-spread
b
elief
within
the
mac
hine
learning
comm
unit
y
,
the
use
of
ILP
tec
hniques
is
not
conned
to
small
databases.
W
e
end
the
c
hapter
with
some
related
w
ork
(Section
.)
and
conclusions
(Section
.).
.
Dieren
t
Implemen
tations
of
Tilde
W
e
compare
t
w
o
dieren
t
implemen
tations
of
Tilde:
one
is
a
straigh
tforw
ard
implemen
tation,
follo
wing
closely
the
TDIDT
algorithm.
The
other
is
a
more
sophisticated
implemen
tation
that
aims
sp
ecically
at
handling
large
data
sets;
it
is
based
on
the
w
ork
b
y
Meh
ta
et
al.
(		).
Both
implemen
tations
are
de-
scrib
ed
using
classication
terminology
,
but
the
same
algorithms
can
b
e
applied
for
regression
and
clustering.
A
straigh
tforw
ard
implemen
tation:
Tildeclassic
The
original
Tilde
implemen
tation
w
as
presen
ted
in
the
previous
c
hapter.
In
this
c
hapter
w
e
will
refer
to
it
as
Tildeclassic.
This
implemen
tation
is
based
directly
on
the
algorithm
sho
wn
in
Figure
.,
whic
h
is
the
most
straigh
tforw
ard
w
a
y
of
implemen
ting
TDIDT.
Notew
orth
y
c
haracteristics
are
that
the
tree
is
built
depth-rst,
and
that
the
b
est
test
is
c
hosen
b
y
en
umerating
the
p
ossible
tests
and
for
eac
h
test
computing
its
qualit
y
(to
this
aim
the
test
needs
to
b
e
ev
aluated
on
ev
ery
single
example),
as
is
sho
wn
in
Figure
..
The
pro
cedure
best
test
is
an
instan
tiation
of
the
OPTIMAL
SPLIT
pro
cedure
in
Figure
.
for
induction
of
classiers,
using
the
information
gain
criterion.
Note
that
with
this
implemen
tation,
it
is
crucial
that
fetc
hing
an
example
from
the
database
in
order
to
query
it
is
done
as
ecien
tly
as
p
ossible,
b
ecause

..
DIFFERENT
IMPLEMENT
A
TIONS
OF
TILDE

/*
W
A
CE
=
weighte
d
aver
age
class
entr
opy
*/
function
w
a
ce(A;
B
:
arra
y[class]
of
natural)
returns
real:
T
A
:=
P
c
A[c]
T
B
:=
P
c
B
[c]
S
:=
P
c
A[c]
log(A[c]=T
A
)
+
P
c
B
[c]
log
(B
[c]=T
B
)
return
S=(T
A
+
T
B
)
pro
cedure
best
test(S
:
set
of
queries,
E
:
set
of
examples)
returns
query:
for
eac
h
renemen
t
 
Q
i
in
S
:
/*
c
ounter[true]
and
c
ounter[false]
ar
e
class
distributions,
i.e.
arr
ays
mapping
classes
onto
their
fr
e
quencies
*/
for
eac
h
class
c
:
coun
ter[true][c]
:=
0,
coun
ter[false][c]
:=
0
for
eac
h
example
e
in
E
:
if
 
Q
i
succeeds
in
e
then
increase
coun
ter[true][class(e)]
b
y

else
increase
coun
ter[false][class(e)]
b
y

s
i
:=
w
a
ce(coun
ter[true],
coun
ter[false])
Q
b
:=
that
Q
i
for
whic
h
s
i
is
minimal
/*
highest
gain
*/
return
Q
b
Figure
.:
Computation
of
the
b
est
test
Q
b
in
Tildeclassic.


CHAPTER
.
SCALING
UP
Tilde
this
op
eration
is
inside
the
innermost
lo
op.
F
or
this
reason,
Tildeclassic
loads
all
data
in
to
main
memory
when
it
starts
up.
Lo
calization
is
then
ac
hiev
ed
b
y
means
of
the
mo
dule
system
of
the
Prolog
engine
in
whic
h
Tilde
runs.
Eac
h
example
is
loaded
in
to
a
dieren
t
mo
dule,
and
accessing
an
example
is
done
b
y
c
hanging
the
curren
tly
activ
e
mo
dule,
whic
h
is
a
v
ery
c
heap
op
eration.
T
esting
an
example
in
v
olv
es
running
a
query
in
the
small
database
consisting
of
the
example
together
with
the
bac
kground
kno
wledge;
the
data
ab
out
other
examples
is
not
visible.
An
alternativ
e
is
to
load
all
the
examples
in
to
one
mo
dule;
no
example
selection
is
necessary
then,
and
all
data
can
alw
a
ys
b
e
accessed
directly
.
The
disadv
an
tage
is
that
the
relev
an
t
data
need
to
b
e
lo
ok
ed
up
in
a
large
set
of
data,
so
that
a
go
o
d
indexing
sc
heme
is
necessary
in
order
to
mak
e
this
approac
h
ecien
t.
W
e
will
return
to
this
alternativ
e
in
the
section
on
exp
erimen
ts.
W
e
p
oin
t
out
that,
when
examples
are
loaded
in
to
dieren
t
mo
dules,
Tilde-
classic
partially
exploits
the
lo
calit
y
assumption
(in
that
it
handles
eac
h
indi-
vidual
example
indep
enden
tly
from
the
others,
but
still
loads
all
the
examples
in
main
memory).
It
do
es
not
exploit
this
assumption
at
all
when
all
the
examples
are
loaded
in
to
one
mo
dule.
A
more
sophisticated
implemen
tation:
TildeLDS
Meh
ta
et
al.
(		)
ha
v
e
prop
osed
an
alternativ
e
implemen
tation
of
TDIDT
that
is
orien
ted
to
w
ards
mining
large
databases.
With
their
approac
h,
the
database
is
accessed
less
in
tensiv
ely
,
whic
h
results
in
an
imp
ortan
t
eciency
gain.
W
e
ha
v
e
adopted
this
approac
h
for
an
alternativ
e
implemen
tation
of
Tilde,
whic
h
w
e
call
TildeLDS
(LDS
stands
for
L
ar
ge
Data
Sets
).
The
alternativ
e
algorithm
is
sho
wn
in
Figure
..
It
diers
from
Tilde-
classic
in
that
the
tree
is
no
w
built
breadth-rst,
and
examples
are
loaded
in
to
main
memory
one
at
a
time.
The
algorithm
w
orks
lev
el-wise.
Eac
h
iteration
through
the
while
lo
op
will
expand
one
lev
el
of
the
decision
tree.
S
con
tains
all
no
des
at
this
lev
el
of
the
decision
tree.
T
o
expand
this
lev
el,
the
algorithm
considers
all
no
des
N
in
S
.
F
or
eac
h
renemen
t
in
eac
h
no
de,
a
separate
coun
ter
(to
compute
class
distributions)
is
k
ept.
The
algorithm
mak
es
one
pass
through
the
data,
during
whic
h
for
eac
h
example
that
b
elongs
to
a
non-leaf
no
de
N
it
tests
all
renemen
ts
for
N
on
the
example
and
up
dates
the
corresp
onding
coun
ters.
Note
that
while
for
Tildeclassic
the
example
lo
op
w
as
inside
the
renemen
t
lo
op,
the
opp
osite
is
true
no
w.
This
minimizes
the
n
um
b
er
of
times
a
new
example
m
ust
b
e
loaded,
whic
h
is
an
exp
ensiv
e
op
eration
(in
con
trast
with
the
previous
approac
h
where
all
examples
w
ere
in
main
memory
and
examples
only
had
to
b
e
\selected"
in
order
to
access
them,
examples
are
no
w
loaded
from
disk).
In
the
curren
t
implemen
tation
eac
h
example
needs
to
b
e
loaded
at
most
once
p
er
lev
el
of
the
tree
(\at
most"
b
ecause
once
it
is
in
a
leaf
it
need

..
DIFFERENT
IMPLEMENT
A
TIONS
OF
TILDE
	
function
best
test(N
:
no
de)
returns
query:
 
Q
:=
asso
ciated
query(N
)
for
eac
h
renemen
t
 
Q
i
of
 
Q:
C
D
l
:=
coun
ter[N
,i,true]
C
D
r
:=
coun
ter[N
,i,false]
s
i
:=
w
a
ce(C
D
l
,
C
D
r
)
Q
b
:=
that
Q
i
for
whic
h
s
i
is
minimal
return
 
Q
b
pro
cedure
TildeLDS(T
:
tree):
S
:=
fT
g
while
S
=

do
/*
add
one
level
to
the
tr
e
e
*/
for
eac
h
example
e
that
is
not
co
v
ered
b
y
a
leaf
no
de:
load
e
N
:=
the
no
de
in
S
that
co
v
ers
e
 
Q
:=
asso
ciated
query(N
)
for
eac
h
renemen
t
 
Q
i
of
 
Q:
if
 
Q
i
succeeds
in
e
then
increase
coun
ter[N
,i,true][class(e)]
b
y

else
increase
coun
ter[N
,i,false][class(e)]
b
y

for
eac
h
no
de
N

S
:
remo
v
e
N
from
S
 
Q
b
:=
best
test(N
)
if
stop
crit( 
Q
b
)
then
N
:=
leaf(mod
al
class
(N
))
else
 
Q
:=
asso
ciated
query(N
)
c
onj
:=
Q
b
 Q
N
:=
ino
de(c
onj,
left,
right
)
add
left
and
right
to
S
Figure
.:
The
TildeLDS
algorithm.
The
w
a
ce
function
is
dened
in
Fig-
ure
..
The
stop
crit
and
mod
al
class
functions
are
the
instan
tiations
of
STOP
CRIT
and
INF
O
for
classication
as
men
tioned
in
Chapter
.

0
CHAPTER
.
SCALING
UP
Tilde
not
b
e
loaded
an
ymore),
hence
the
total
n
um
b
er
of
passes
through
the
data
le
is
equal
to
the
depth
of
the
tree,
whic
h
is
the
same
as
w
as
obtained
for
prop
ositional
learning
algorithms
(Meh
ta
et
al.,
		).
The
disadv
an
tage
of
this
algorithm
is
that
a
four-dimensional
arra
y
of
coun-
ters
needs
to
b
e
stored
instead
of
a
t
w
o-dimensional
one
(as
in
Tildeclassic),
b
ecause
dieren
t
coun
ters
are
k
ept
for
eac
h
renemen
t
in
eac
h
no
de.

Care
has
b
een
tak
en
to
implemen
t
TildeLDS
in
suc
h
a
w
a
y
that
the
size
of
the
data
set
that
can
b
e
handled
is
not
restricted
b
y
in
ternal
memory
(in
con
trast
to
Tildeclassic).
Whenev
er
information
needs
to
b
e
stored
the
size
of
whic
h
dep
ends
on
the
size
of
the
data
set,
this
information
is
stored
on
disk.

When
pro
cessing
a
certain
lev
el
of
the
tree,
the
space
complexit
y
of
TildeLDS
therefore
has
a
comp
onen
t
O
(r

n)
with
n
the
n
um
b
er
of
no
des
on
that
lev
el
and
r
the
(a
v
erage)
n
um
b
er
of
renemen
ts
of
those
no
des
(b
ecause
coun
ters
are
k
ept
for
eac
h
renemen
t
in
eac
h
no
de),
but
is
constan
t
in
the
n
um
b
er
of
examples
m.
This
con
trasts
with
Tildeclassic
where
the
space
complexit
y
has
a
comp
onen
t
O
(m)
(b
ecause
all
examples
are
loaded
at
once).
While
memory
no
w
restricts
the
n
um
b
er
of
renemen
ts
that
can
b
e
con-
sidered
in
eac
h
no
de
and
the
maximal
size
of
the
tree,
this
restriction
is
unim-
p
ortan
t
in
practice,
as
the
n
um
b
er
of
renemen
ts
and
the
tree
size
are
usually
m
uc
h
smaller
than
the
upp
er
b
ounds
imp
osed
b
y
the
a
v
ailable
memory
.
There-
fore
TildeLDS
t
ypically
consumes
less
memory
than
Tildeclassic,
and
ma
y
b
e
preferable
ev
en
when
the
latter
is
also
feasible.
.
Optimizations
The
dierence
in
the
w
a
y
the
example
lo
op
and
the
renemen
t
lo
op
are
nested
do
es
not
only
inuence
the
amoun
t
of
data
access
that
is
necessary
.
It
also
has
an
eect
on
the
kind
of
optimizations
that
can
b
e
p
erformed.
The
w
a
y
in
whic
h
renemen
ts
and
examples
are
pro
cessed
in
Tildeclassic
and
TildeLDS
is
illustrated
in
Figure
..
The
gure
sho
ws
ho
w
Tildeclassic
c
ho
oses
one
renemen
t,
tests
it
on
all
examples,
then
go
es
on
to
the
next
renemen
t,
while
TildeLDS
c
ho
oses
one
example,
tests
all
renemen
ts
on
it,
then
go
es
on
to
the
next
example.
W
e
ha
v
e
seen
one
opp
ortunit
y
for
optimizing
queries
already:
in
Section
..
it
w
as
sho
wn
ho
w
some
parts
of
a
query
can
b
e
remo
v
ed
b
ecause
they
cannot
p
ossibly
b
e
relev
an
t
for
this
test.
While
the
query
simplication
algorithm
is
relativ
ely
exp
ensiv
e,
it
pa
ys
o
when
the
query
is
to
b
e
run
on
man
y
examples.

One
of
the
dimensions
fortunately
do
es
not
dep
end
on
the
application,
as
it
can
only
tak
e
the
v
alues
true
and
false.

The
results
of
all
queries
for
eac
h
example
are
stored
in
this
manner,
so
that
when
the
b
est
query
is
c
hosen
after
one
pass
through
the
data,
these
results
can
b
e
retriev
ed
from
the
auxiliary
le,
a
v
oiding
a
second
pass
through
the
data.

..
OPTIMIZA
TIONS

1
2
3
4
5
.
.
.
R-1
R
1
2
3
4
5
.
.
.
R-1
R
TILDEclassic
TILDE-LDS
1   2   3   4   5   ...   m-1   m
examples
1   2   3   4   5   ...   m-1   m
examples
refinements
refinements
Figure
.:
The
dierence
b
et
w
een
Tildeclassic
and
TildeLDS
in
the
w
a
y
they
pro
cess
the
examples
and
renemen
ts.
W
e
recall
that
the
algorithm
c
hanges
a
renemen
t
Q;
conj
i
in
to
a
simpler
form
Q
0
;
conj
i
,
where
Q
0
only
con
tains
those
literals
in
Q
that
can
inuence
the
outcome
of
conj
i
.
No
w,
when
a
query
Q
is
simplied
in
to
Q
0
,
this
Q
0
can
b
e
passed
on
horizon
tally
in
Figure
.;
only
when
a
new
ro
w
is
started
(a
new
renemen
t
is
pro
cessed),
a
new
Q
0
needs
to
b
e
computed.
This
is
necessary
b
ecause
although
Q
sta
ys
the
same,
the
simplication
Q
0
is
computed
from
b
oth
Q
and
conj
i
,
and
conj
i
is
dieren
t
for
eac
h
ro
w.
F
rom
Figure
.
it
is
clear
that
Tildeclassic
can
just
compute
Q
0
at
the
b
eginning
of
a
ro
w
and
then
use
it
to
pro
cess
the
whole
ro
w
b
efore
computing
a
new
one;
TildeLDS,
on
the
con
trary
,
needs
to
recompute
Q
0
for
eac
h
example.
Since
the
simplication
of
a
query
ma
y
b
e
more
exp
ensiv
e
than
what
is
gained
b
y
testing
the
simpler
query
on
a
single
example
(this
strongly
dep
ends
on
the
application),
in
some
cases
it
ma
y
b
e
b
etter
not
to
use
simplied
queries.
An
alternativ
e
is
to
compute
eac
h
Q
0
once
and
store
them
in
an
arra
y
indexed
b
y
renemen
t
iden
tiers,
from
whic
h
they
are
retriev
ed
when
needed.
This
seems
the
only
ecien
t
w
a
y
of
using
query
simplication
in
TildeLDS.
While
query
simplication
b
ecomes
harder
to
use
in
TildeLDS,
this
al-
ternativ
e
implemen
tation
creates
other
opp
ortunities
for
optimization
of
the
querying
pro
cess.
It
is
no
w
easier
to
pass
information
ab
out
a
single
example
from
one
renemen
t
to
another
(v
ertically
,
in
Figure
.).
Note
that
eac
h
test
is
of
the
form
Q;
conj
i
,
so
that
a
large
part
of
the
test
(Q)
is
the
same
for
eac
h
renemen
t.
Since
the
renemen
ts
are
all
tested
on
the
same
example,
all
answ
er
substitutions
for
Q
could
b
e
computed
once,
and
then
for
eac
h
rene-
men
t
one
only
need
test
conj
i
,
with
its
v
ariables
instan
tiated
according
to
the


CHAPTER
.
SCALING
UP
Tilde
/*
alternative
for
the
se
c
ond
for
eac
h
lo
op
in
Figur
e
.
*/
for
eac
h
renemen
t
 
Q
i
of
 
Q:
conj
i
:=
Q
 Q
i
S
:=
set
of
all
conj
i
for
eac
h
answ
er
substitution

for
 
Q:
for
eac
h
conj
i
in
S
:
if
conj
i

succeeds
then
remo
v
e
conj
i
from
S
increase
coun
ter[N
,
i,
true]
b
y

for
eac
h
conj
i
in
S
:
increase
coun
ter[N
,i,false]
b
y

Figure
.:
In
terlea
v
ed
computation
of
answ
er
substitutions
for
Q
and
the
success
of
eac
h
renemen
t
on
a
single
example.
answ
er
substitutions
for
Q.
W
e
refer
to
this
tec
hnique
of
storing
answ
er
substitutions
for
later
use
as
c
aching.
Note
that
this
tec
hnique
is
similar
to
tabling,
as
used
in
logic
program-
ming.
W
e
use
the
more
general
term
cac
hing
b
ecause
tabling
migh
t
suggest
that
partial
results
are
stored
and
retriev
ed
when
needed
within
the
execution
of
one
single
query
.
This
is
not
the
case
in
our
implemen
tation;
partial
results
are
propagated
from
one
query
to
another
similar
one.
The
cac
hing
tec
hnique
as
describ
ed
ab
o
v
e
has
the
disadv
an
tage
that
the
size
of
one
set
of
answ
er
substitutions
can
b
e
v
ery
large.
Moreo
v
er,
unless
the
set
is
stored
in
an
indexed
structure,
retrieving
answ
er
substitutions
still
tak
es
time
linear
in
the
size
of
the
set.
F
ortunately
,
it
is
p
ossible
to
a
v
oid
storing
the
substitutions
explicitly
,
b
y
in
terlea
ving
the
computation
of
answ
er
substitutions

j
for
Q
and
the
computation
of
the
truth
v
alue
of
conj
i

j
;
i.e.,
starting
with
a
set
S
of
all
conj
i
,
the
system
nds
one
answ
er
substitution


for
Q,
computes
truth
v
alues
for
all
conj
i


,
remo
v
es
from
S
all
conj
i
for
whic
h
conj
i


is
true,
then
con
tin
ues
lo
oking
for


,
applies
it
to
the
smaller
S
,
and
so
on
un
til
all
answ
er
substitutions
for
Q
ha
v
e
b
een
found.
The
conjunctions
that
are
still
in
S
at
that
momen
t
are
those
that
fail
on
the
example.
Figure
.
giv
es
an
algorithm.
This
tec
hnique
pro
vides
the
adv
an
tage
of
our
cac
hing
tec
hnique
(the
answ
er
substitutions
of
Q
are
only
computed
once)
without
ha
ving
to
store
a
large
set
of
data,
and
is
also
faster
b
ecause
a
linear
searc
h
in
the
list
of
answ
er
substitutions
is
a
v
oided.
It
ma
y
incur
some
o
v
erhead,
dep
ending
on
the
exact
implemen
tation;
to
mak
e
it
maximally
ecien
t
it
should
b
e
implemen
ted
at
the
lev
el
of
the
Prolog
execution
engine.
Note
that,
in
order
to
use
this
cac
hing
tec
hnique
in
Tildeclassic,
it
w
ould
b
e

..
OPTIMIZA
TIONS

Tildeclassic
TildeLDS
simplied
query
easy
feasible
cac
hing
not
feasible
easy
b
oth
not
feasible
feasible
T
able
.:
Ov
erview
of
the
dieren
t
optimizations.
necessary
to
store
an
arra
y
of
sets
of
answ
er
substitutions
indexed
b
y
example
iden
tiers.
Giv
en
the
p
oten
tially
large
size
of
one
set
of
answ
er
substitutions
and
the
p
oten
tially
h
uge
n
um
b
er
of
examples,
this
is
clearly
infeasible.
A
summary
of
all
this
is
giv
en
in
T
able
..
Lea
ving
the
feasibilit
y
of
loading
all
data
in
to
main
memory
out
of
consideration,
the
c
hoice
b
et
w
een
Tildeclassic
and
TildeLDS
should
b
e
based
on
whic
h
optimization
is
exp
ected
to
w
ork
b
est:
query
simplication
or
cac
hing.
It
is
p
ossible,
but
rather
complicated,
to
com
bine
the
t
w
o
optimizations.
Instead
of
one
Q,
there
are
then
k

R
dieren
t
simplied
queries
Q
0
for
whic
h
answ
er
substitutions
are
to
b
e
computed
(with
R
the
total
n
um
b
er
of
renemen
ts).
A
p
ossible
pro
cedure
is:
compute
Q
0
for
eac
h
renemen
t,
store
the
dieren
t
Q
0
i
together
with
a
set
S
i
of
all
the
renemen
t
indexes
for
whic
h
they
are
relev
an
t,
and
apply
the
algorithm
in
Figure
.
for
eac
h
S
i
(mutatis
mutandis;
Q
b
ecomes
Q
0
etc.
in
the
algorithm).
Whether
computing
the
answ
er
substitutions
for
k
simpler
queries
is
more
ecien
t
than
computing
the
answ
er
substitutions
for
one
more
complex
query
,
dep
ends
on
k
and
on
ho
w
m
uc
h
simpler
the
simplied
queries
are.
This
is
domain-dep
enden
t.
The
p
ossibilit
y
of
a
v
oiding
recomputation
of
the
answ
er
substitutions
of
Q
is
an
imp
ortan
t
adv
an
tage
of
TildeLDS,
and
ma
y
in
itself
oset
the
adv
an
tage
that
Tildeclassic
has
b
y
computing
a
simplied
query
.
Moreo
v
er,
TildeLDS
is
the
only
implemen
tation
that
in
principle
allo
ws
either
optimization,
or
ev
en
b
oth
together.
In
their
curren
t
implemen
tations,
Tildeclassic
exploits
query
simplication,
while
TildeLDS
can
emplo
y
either
the
cac
hing
tec
hnique
or
the
alternativ
e
algorithm
in
Figure
..
W
e
can
dra
w
some
in
teresting
conclusions
from
this
discussion.
First
of
all,
in
the
prop
ositional
learning
eld
the
dierence
b
et
w
een
the
classical
v
ersion
of
TDIDT
and
the
lev
el-wise
v
ersion
is
not
so
imp
ortan
t,
except
when
the
data
set
is
not
loaded
in
main
memory
.
In
the
ILP
con
text,
this
is
quite
dieren
t:
due
to
the
in
teraction
b
et
w
een
the
conjunction
in
a
no
de
and
the
asso
ciated
query
of
a
no
de,
optimization
opp
ortunities
are
v
ery
dieren
t
for
b
oth
implemen
tations
(and
it
seems
that
the
lev
el-wise
v
ersion
oers
the
b
etter
opp
ortunities).
Second,
the
optimizations
w
e
ha
v
e
iden
tied
should
also
b
e
applicable
to
other
ILP
systems.
Man
y
ILP
systems
are
structured
so
that
the
example
lo
op
is
inside
the
renemen
t
lo
op.
Switc
hing
the
lo
ops
migh
t
yield
signican
t


CHAPTER
.
SCALING
UP
Tilde
sp
eed-ups
in
some
cases.
This
certainly
applies
to
systems
that
learn
from
in
terpretations
(Cla
udien
(De
Raedt
and
Dehasp
e,
		),
ICL
(De
Raedt
and
V
an
Laer,
		)).
F
or
systems
learning
from
en
tailmen
t
the
optimization
seems
harder
to
accomplish
but
is
probably
not
imp
ossible;
a
system
suc
h
as
Pr
ogol
(Muggleton,
		)
migh
t
b
enet
from
it.
.
Exp
erimen
ts
In
this
exp
erimen
tal
section
w
e
try
to
v
alidate
our
claims
ab
out
time
complexit
y
empirically
,
and
explore
some
inuences
on
scalabilit
y
.
More
sp
ecically
,
w
e
w
an
t
to:

v
alidate
the
claim
that
when
the
lo
calization
assumption
is
exploited,
induction
time
is
linear
in
the
n
um
b
er
of
examples
(c
eteris
p
aribus,
i.e.,
w
e
con
trol
for
other
inuences
on
induction
time
suc
h
as
the
size
of
the
tree);

study
the
inuence
of
lo
calization
on
induction
time
(b
y
quan
tifying
the
amoun
t
of
lo
calization
and
in
v
estigating
its
eect
on
the
induction
time);

in
v
estigate
ho
w
the
induction
time
v
aries
with
the
size
of
the
data
set
in
more
practical
situations
(if
w
e
do
not
con
trol
other
inuences;
i.e.
a
larger
data
set
ma
y
cause
the
learner
to
induce
a
more
complex
theory
,
whic
h
in
itself
has
an
eect
on
the
induction
time).
These
exp
erimen
ts
ha
v
e
b
een
p
erformed
in
co-op
eration
with
Nico
Jacobs.
..
Data
Sets
W
e
briey
describ
e
the
data
sets
that
are
used
in
these
exp
erimen
ts;
for
a
more
detailed
description
see
App
endix
A.
Rob
oCup
This
set
con
tains
data
ab
out
so
ccer
games
pla
y
ed
b
y
soft
w
are
agen
ts
training
for
the
Rob
oCup
comp
etition
(Kitano
et
al.,
		).
It
con
tains
	
examples
and
is
00MB
large.
Eac
h
example
consists
of
a
description
of
the
state
of
the
so
ccer
terrain
as
observ
ed
b
y
one
sp
ecic
pla
y
er
on
a
single
momen
t.
This
description
includes
the
iden
tit
y
of
the
pla
y
er,
the
p
ositions
of
all
pla
y
ers
and
of
the
ball,
the
time
at
whic
h
the
example
w
as
recorded,
the
action
the
pla
y
er
p
erformed,
and
the
time
at
whic
h
this
action
w
as
executed.
While
this
data
set
w
ould
allo
w
rather
complicated
theories
to
b
e
construc-
ted,
for
our
exp
erimen
ts
the
language
bias
w
as
v
ery
simple
and
consisted
of
a
prop
ositional
language
(only
high-lev
el
commands
are
learned).
This
use
of

..
EXPERIMENTS

the
data
set
reects
the
learning
tasks
considered
un
til
no
w
b
y
the
p
eople
who
are
using
it,
see
(Jacobs
et
al.,
		).
It
do
es
not
inuence
the
v
alidit
y
of
our
results
for
relational
languages,
b
ecause
the
prop
ositions
are
dened
b
y
the
bac
kground
kno
wledge
and
their
truth
v
alues
are
computed
at
run
time,
so
the
query
that
is
really
executed
is
relational.
F
or
instance,
the
prop
osi-
tion
have
ball,
indicating
whether
some
pla
y
er
of
the
team
has
the
ball
in
its
p
ossession,
is
computed
from
the
p
osition
of
the
pla
y
er
and
of
the
ball.
P
ok
er
The
P
ok
er
data
sets
are
articially
created
data
sets
where
eac
h
example
is
a
description
of
a
hand
of
v
e
cards,
together
with
a
name
for
the
hand
(pair,
three
of
a
kind,
.
.
.
).
The
aim
is
to
learn
denitions
for
sev
eral
p
ok
er
concepts
from
a
set
of
examples.
The
classes
that
are
considered
here
are
nought,
pair,
two
pairs,
three
of
a
kind,
full
house,
flush
and
four
of
a
kind.
This
is
a
simplication
of
the
real
p
ok
er
domain,
where
more
classes
exist
and
it
is
necessary
to
distinguish
b
et
w
een
e.g.
a
pair
of
queens
and
a
pair
of
kings;
but
this
simplied
v
ersion
suces
to
illustrate
the
relev
an
t
topics
and
k
eeps
learning
times
sucien
tly
lo
w
to
allo
w
for
reasonably
extensiv
e
exp
erimen
ts.
An
in
teresting
prop
ert
y
of
this
data
set
is
that
some
classes,
e.g.
four
-
of
a
kind,
are
v
ery
rare,
hence
a
large
data
set
is
needed
to
learn
these
classes
(assuming
the
data
are
generated
randomly).
Mutagenesis
F
or
the
description
of
the
Mutagenesis
data
set
w
e
refer
to
Section
..
or
App
endix
A.
In
these
exp
erimen
ts
w
e
used
Bac
kground
B

,
i.e.,
only
structural
information
ab
out
the
molecules
(the
atoms
and
b
onds
o
ccurring
in
them)
is
a
v
ailable.
..
Materials
and
Settings
All
exp
erimen
ts
w
ere
p
erformed
with
the
t
w
o
implemen
tations
of
Tilde
w
e
discussed:
Tildeclassic
and
TildeLDS.
These
programs
are
implemen
ted
in
Prolog
and
run
under
the
MasterProLog
engine.
The
hardw
are
w
e
used
is
a
Sun
Ultra-
at

MHz,
running
the
Solaris
system
(except
when
stated
otherwise).
Both
Tildeclassic
and
TildeLDS
oer
the
p
ossibilit
y
to
precompile
the
data
le.
W
e
exploited
this
feature
for
all
our
exp
erimen
ts.
F
or
TildeLDS
this
raises
the
problem
that
in
order
to
load
one
example
at
a
time,
a
dieren
t
ob
ject
le
has
to
b
e
created
for
eac
h
example
(MasterProLog
oers
no
predicates
for
loading
only
a
part
of
an
ob
ject
le).
This
can
b
e
rather
impractical.
F
or
this
reason
sev
eral
examples
are
usually
compiled
in
to
one
ob
ject
le;
a


CHAPTER
.
SCALING
UP
Tilde
parameter
called
gr
anularity
(G,
b
y
default
0)
con
trols
ho
w
man
y
examples
can
b
e
included
in
one
ob
ject
le.
Ob
ject
les
are
loaded
one
b
y
one
b
y
TildeLDS,
whic
h
means
that
G
examples
(instead
of
one)
at
a
time
are
loaded
in
to
main
memory
.
Because
of
this,
G
has
an
inuence
on
the
eciency
of
TildeLDS;
in
a
sense
it
aects
the
amoun
t
of
lo
calization
in
the
data.
This
eect
is
in
v
estigated
in
our
exp
erimen
ts.
..
Exp
erimen
t
:
Time
Complexit
y
Aim
of
the
Exp
erimen
t
As
men
tioned
b
efore,
induction
of
trees
with
TildeLDS
should
in
principle
ha
v
e
a
time
complexit
y
that
is
linear
in
the
n
um
b
er
of
examples.
With
our
rst
exp
erimen
t
w
e
empirically
test
whether
our
implemen
tation
indeed
exhibits
this
prop
ert
y
.
W
e
also
compare
it
with
other
approac
hes
where
the
lo
calit
y
assumption
is
exploited
less
or
not
at
all.
W
e
distinguish
the
follo
wing
approac
hes:

loading
all
data
at
once
in
main
memory
without
exploiting
the
lo
calit
y
assumption
(the
standard
ILP
approac
h);

loading
all
data
at
once
in
main
memory
,
exploiting
the
lo
calit
y
assump-
tion
(this
is
what
Tildeclassic
do
es);

loading
examples
one
at
a
time
in
main
memory;
this
is
what
TildeLDS
do
es.
T
o
the
b
est
of
our
kno
wledge
all
ILP
systems
that
do
not
learn
from
in-
terpretations
follo
w
the
rst
approac
h
(with
the
exception
of
a
few
systems
that
access
an
external
database
directly
instead
of
loading
the
data
in
to
main
memory
,
e.g.
Rdt/db
(Morik
and
Bro
c
khausen,
		);
but
these
systems
still
do
not
mak
e
a
lo
calit
y
assumption).
W
e
can
easily
sim
ulate
this
approac
h
with
Tildeclassic
b
y
sp
ecifying
all
information
ab
out
the
examples
as
bac
kground
kno
wledge.
F
or
the
bac
kground
kno
wledge
no
lo
calit
y
assumption
can
b
e
made,
since
all
bac
kground
kno
wledge
is
p
oten
tially
relev
an
t
for
eac
h
example.
The
p
erformance
of
a
Prolog
system
that
w
orks
with
a
large
database
is
impro
v
ed
signican
tly
if
indexes
are
built
for
the
predicates.
On
the
other
hand,
adding
indexes
for
predicates
creates
some
o
v
erhead
with
resp
ect
to
the
in
ternal
space
that
is
needed,
and
a
lot
of
o
v
erhead
for
the
compiler.
The
MasterProLog
system
b
y
default
indexes
all
predicates,
but
this
indexing
can
b
e
switc
hed
o.
W
e
ha
v
e
p
erformed
exp
erimen
ts
for
the
standard
ILP
approac
h
b
oth
with
and
without
indexing
(th
us,
the
rst
approac
h
in
the
ab
o
v
e
list
is
actually
sub
divided
in
to
\indexed"
and
\not
indexed").

..
EXPERIMENTS

Metho
dology
Since
the
aim
of
this
exp
erimen
t
is
to
determine
the
inuence
of
the
n
um
b
er
of
examples
(and
only
that)
on
time
and
space
complexit
y
,
w
e
w
an
t
to
con
trol
as
m
uc
h
as
p
ossible
other
factors
that
migh
t
also
ha
v
e
an
inuence.
W
e
ha
v
e
seen
in
Section
..
that
these
other
factors
include
the
n
um
b
er
of
no
des
n,
the
a
v
erage
n
um
b
er
of
tests
p
er
no
de
t
and
the
a
v
erage
complexit
y
of
p
erforming
one
test
on
one
single
example
c.
c
dep
ends
on
b
oth
the
complexit
y
of
the
queries
themselv
es
and
on
the
example
sizes.
When
v
arying
the
n
um
b
er
of
examples
for
our
exp
erimen
ts,
w
e
w
an
t
to
k
eep
these
factors
constan
t.
This
means
that
rst
of
all
the
renemen
t
op
erator
should
b
e
the
same
for
all
the
exp
erimen
ts.
This
is
automatically
the
case
if
the
user
do
es
not
c
hange
the
renemen
t
op
erator
sp
ecication
b
et
w
een
consecutiv
e
exp
erimen
ts.
The
other
factors
can
b
e
k
ept
constan
t
b
y
ensuring
that
the
same
tree
is
built
in
eac
h
exp
erimen
t,
and
that
the
a
v
erage
complexit
y
of
the
examples
do
es
not
c
hange.
In
order
to
ac
hiev
e
this,
w
e
adopt
the
follo
wing
metho
dology
.
W
e
create,
from
a
small
data
set,
larger
data
sets
b
y
including
eac
h
single
example
sev
eral
times.
By
ensuring
that
all
the
examples
o
ccur
an
equal
n
um
b
er
of
times
in
the
resulting
data
set,
the
class
distribution,
a
v
erage
complexit
y
of
testing
a
query
on
an
example
etc.
are
all
k
ept
constan
t.
In
other
w
ords,
all
v
ariation
due
to
the
inuence
of
individual
examples
is
remo
v
ed.
Because
the
class
distribution
sta
ys
the
same,
the
test
that
is
c
hosen
in
eac
h
no
de
also
sta
ys
the
same.
This
is
necessary
to
ensure
that
the
same
tree
is
gro
wn,
but
not
sucien
t:
the
stopping
criterion
needs
to
b
e
adapted
as
w
ell
so
that
a
no
de
that
cannot
b
e
split
further
for
the
small
data
set
is
not
split
when
using
the
larger
data
set
either.
In
order
to
ac
hiev
e
this,
the
minimal
n
um
b
er
of
examples
that
ha
v
e
to
b
e
co
v
ered
b
y
eac
h
leaf
(whic
h
is
a
parameter
of
Tilde)
is
increased
prop
ortionally
to
the
size
of
the
data
set.
By
follo
wing
this
metho
dology
,
the
men
tioned
un
w
an
ted
inuences
are
ltered
out
of
the
results.
Materials
W
e
used
the
Mutagenesis
data
set
for
this
exp
erimen
t.
Other
materials
are
as
describ
ed
in
Section
...
Setup
of
the
Exp
erimen
t
F
our
dieren
t
v
ersions
of
Tilde
are
compared:

Tildeclassic
without
lo
calit
y
assumption,
without
indexing

Tildeclassic
without
lo
calit
y
assumption,
with
indexing


CHAPTER
.
SCALING
UP
Tilde

Tildeclassic
with
lo
calit
y
assumption

TildeLDS
The
rst
three
\v
ersions"
are
actually
the
same
v
ersion
of
Tilde
as
far
as
the
implemen
tation
of
the
learning
algorithm
is
concerned,
but
dier
in
the
w
a
y
the
data
are
represen
ted
and
in
the
w
a
y
the
underlying
Prolog
system
handles
them.
Eac
h
Tilde
v
ersion
w
as
rst
run
on
the
original
data
set,
then
on
data
sets
that
con
tain
eac
h
original
example

n
times,
with
n
ranging
from

to
	.
F
or
eac
h
run
on
eac
h
data
set
w
e
ha
v
e
recorded
the
follo
wing:

the
time
needed
for
the
induction
pro
cess
itself
(in
CPU-seconds)

the
time
needed
to
compile
the
data
(in
CPU-seconds).
The
dieren
t
systems
compile
the
data
in
dieren
t
w
a
ys
(e.g.
according
to
whether
indexes
need
to
b
e
built).
As
compilation
of
the
data
need
only
b
e
done
once,
ev
en
if
afterw
ards
sev
eral
runs
of
the
induction
system
are
done,
compilation
time
ma
y
seem
less
relev
an
t.
Still,
it
is
imp
ortan
t
to
see
ho
w
the
compilation
scales
up,
since
it
is
not
really
useful
to
ha
v
e
an
induction
metho
d
that
scales
linearly
if
it
needs
a
prepro
cessing
step
that
scales
sup
er-linearly
.
Discussion
of
the
Results
T
able
.
giv
es
an
o
v
erview
of
the
time
eac
h
Tilde
v
ersion
needed
to
induce
a
tree
for
eac
h
set,
as
w
ell
as
the
time
it
to
ok
to
compile
the
data
in
to
the
correct
format.
Some
prop
erties
of
the
data
sets
are
also
included.
Note
that
only
TildeLDS
scales
up
w
ell
to
large
data
sets.
The
other
v
ersions
of
Tilde
had
problems
loading
or
compiling
the
data
from
a
m
ulti-
plication
factor
of

or

on.
The
results
are
sho
wn
graphically
in
Figure
..
Note
that
b
oth
the
n
um
b
er
of
examples
and
time
are
indicated
on
a
logarithmic
scale.
Care
m
ust
b
e
tak
en
when
in
terpreting
these
graphs:
a
straigh
t
line
do
es
not
indicate
a
linear
relationship
b
et
w
een
the
v
ariables.
Indeed,
if
log
y
=
n

log
x,
then
y
=
x
n
.
This
means
the
slop
e
of
the
line
should
b
e

in
order
to
ha
v
e
a
linear
relationship,
while

indicates
a
quadratic
relationship,
and
so
on.
In
order
to
mak
e
it
easier
to
recognize
a
linear
relationship
(slop
e
),
the
function
y
=
x
has
b
een
dra
wn
on
the
graphs
as
a
reference;
eac
h
line
parallel
with
this
one
indicates
a
linear
relationship.
The
graphs
and
tables
sho
w
that
induction
time
is
linear
in
the
n
um
b
er
of
examples
for
TildeLDS,
for
Tildeclassic
with
lo
calit
y
,
and
for
Tildeclassic
without
lo
calit
y
but
with
indexing.
F
or
Tildeclassic
without
lo
calit
y
or
index-
ing
the
induction
time
increases
quadratically
with
the
n
um
b
er
of
examples.

..
EXPERIMENTS
	
1
10
100
1000
10000
100000
1e+06
1
10
100
1000
Induction time (CPU-seconds)
Multiplication factor
LDS
classic
No locality, indexing
No locality, no indexing
y=x
1
10
100
1000
10000
1
10
100
1000
Compilation time (CPU-seconds)
Multiplication factor
LDS
classic
No locality, indexing
No locality, no indexing
y=x
Figure
.:
Scaling
prop
erties
of
TildeLDS
in
terms
of
n
um
b
er
of
examples.

0
CHAPTER
.
SCALING
UP
Tilde
Prop
erties
of
the
example
sets
m
ult.factor










#examples



0
k
k
k
k
k
	k
#facts
0
k
k
k
k
k
k
.M
.M
.M
size
(MB)
0.
0.







0
Compilation
time
(CPU-seconds)
LDS

.
.

0
	
	
	
		
	
classic
.
.


0

-
-
-
-
{lo
c,
+ind
0.
	

0

k
-
-
-
-
{lo
c,
{ind
.
.	
.
.
.
-
-
-
-
-
Induction
time
(CPU-seconds)
LDS


	
		
0
	0

k
k
k
classic
.
.
.
	
	
-
-
-
-
-
{lo
c,
+ind
.
.
.	

-
-
-
-
-
-
{lo
c,
{ind
0
k
k
0k
-
-
-
-
-
-
T
able
.:
Comparison
of
dieren
t
Tilde
v
ersions
on
Mutagenesis:
TildeLDS,
Tildeclassic,
Tildeclassic
without
lo
calization
but
with
indexing
({lo
c,
+ind)
and
Tildeclassic
without
lo
calization
and
without
indexing
({lo
c,
{ind).
This
is
not
unexp
ected,
as
in
this
setting
the
time
needed
to
run
a
test
on
one
single
example
increases
with
the
size
of
the
data
set.
With
resp
ect
to
compilation
times,
w
e
note
that
all
are
linear
in
the
size
of
the
data
set,
except
Tildeclassic
without
lo
calit
y
and
with
indexing.
This
is
in
corresp
ondence
with
the
fact
that
building
an
index
for
the
predicates
in
a
deductiv
e
database
is
an
exp
ensiv
e
op
eration,
sup
er-linear
in
the
size
of
the
database.

F
urthermore,
the
exp
erimen
ts
conrm
that
Tildeclassic
with
lo
calit
y
scales
as
w
ell
as
TildeLDS
with
resp
ect
to
time
complexit
y
,
but
for
large
data
sets
runs
in
to
problems
b
ecause
it
cannot
load
all
the
data.
Conclusions
Observing
that
without
indexing
induction
time
increases
quadratically
,
and
with
indexing
compilation
time
increases
quadratically
,
w
e
conclude
that
the
lo
calit
y
assumption
is
indeed
crucial
to
our
linearit
y
results,
and
that
loading
only
a
few
examples
at
a
time
in
main
memory
mak
es
it
p
ossible
to
handle
m
uc
h
larger
data
sets.
..
Exp
erimen
t
:
The
Eect
of
Lo
calization
Aim
of
the
exp
erimen
t
In
the
previous
exp
erimen
t
w
e
studied
the
eect
of
the
n
um
b
er
of
examples
on
time
complexit
y
,
and
observ
ed
that
this
eect
is
dieren
t
according
to
whether

It
is
not
clear
to
what
exten
t
the
exp
ensiv
eness
of
the
op
eration
is
t
ypical
for
the
Mas-
terProLog
compiler,
and
to
what
exten
t
it
is
inheren
t
to
the
indexing
task
itself.

..
EXPERIMENTS

the
lo
calit
y
assumption
is
made.
In
this
exp
erimen
t
w
e
do
not
just
distinguish
b
et
w
een
lo
calized
and
not
lo
calized,
but
consider
gradual
c
hanges
in
lo
caliza-
tion,
and
th
us
try
to
quan
tify
the
eect
of
lo
calization
on
the
induction
time.
Metho
dology
W
e
can
test
the
inuence
of
lo
calization
on
the
eciency
of
TildeLDS
b
y
v
arying
the
gran
ularit
y
parameter
G
in
TildeLDS.
G
is
the
n
um
b
er
of
examples
that
are
loaded
in
to
main
memory
at
the
same
time.
Lo
calization
of
information
is
stronger
when
G
is
smaller.
The
eect
of
G
w
as
tested
b
y
running
TildeLDS
successiv
ely
on
the
same
data
set,
under
the
same
circumstances,
but
with
dieren
t
v
alues
for
G.
In
these
exp
erimen
ts
G
ranged
from

to
00.
F
or
eac
h
v
alue
of
G
b
oth
compilation
and
induction
w
ere
p
erformed
ten
times;
the
rep
orted
times
are
the
means
of
these
ten
runs.
Materials
W
e
ha
v
e
used
three
data
sets:
a
Rob
oCup
data
set
with
0000
examples,
a
P
ok
er
data
set
con
taining
000
examples,
and
the
Mutagenesis
data
set
with
a
m
ultiplication
factor
of

(i.e.
0
examples).
The
data
sets
w
ere
c
hosen
to
con
tain
a
sucien
t
n
um
b
er
of
examples
to
mak
e
it
p
ossible
to
let
G
v
ary
o
v
er
a
relativ
ely
broad
range,
but
not
more
(to
limit
the
exp
erimen
tation
time).
Other
materials
are
as
describ
ed
in
Section
...
Discussion
of
the
Results
Induction
times
and
compilation
times
are
plotted
v
ersus
gran
ularit
y
in
Fig-
ure
..
It
can
b
e
seen
from
these
plots
that
induction
time
increases
appro
xim-
ately
linearly
with
gran
ularit
y
.
F
or
v
ery
small
gran
ularities,
to
o,
the
induction
time
can
increase.
W
e
susp
ect
that
this
eect
can
b
e
attributed
to
an
o
v
er-
head
of
disk
access
(loading
man
y
small
les,
instead
of
few
er
larger
les).
A
similar
eect
is
seen
when
w
e
lo
ok
at
the
compilation
times:
these
decrease
when
the
gran
ularit
y
increases,
but
asymptotically
approac
h
a
constan
t.
This
again
suggests
an
o
v
erhead
caused
b
y
compiling
man
y
small
les
instead
of
one
large
le.
The
fact
that
the
observ
ed
eect
is
smallest
for
Mutagenesis,
where
individual
examples
are
larger,
increases
the
plausibilit
y
of
this
explanation.
Conclusions
This
exp
erimen
t
clearly
sho
ws
that
the
p
erformance
of
TildeLDS
strongly
dep
ends
on
G,
and
that
a
reasonably
small
v
alue
for
G
is
preferable.
It
th
us
conrms
the
h
yp
othesis
that
lo
calization
of
information
is
adv
an
tageous
with
resp
ect
to
time
complexit
y
.


CHAPTER
.
SCALING
UP
Tilde
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
0
50
100
150
200
Induction time (CPU-seconds)
Granularity
Poker
Mutagenesis
Robocup
0
500
1000
1500
2000
2500
3000
3500
0
5
10
15
20
25
30
Induction time (CPU-seconds)
Granularity (zoomed in on [0-30])
Poker
Mutagenesis
Robocup
0
50
100
150
200
250
300
350
400
0
50
100
150
200
Compilation time (CPU-seconds)
Granularity
Poker
Mutagenesis
Robocup
Figure
.:
The
eect
of
gran
ularit
y
on
induction
and
compilation
time.

..
EXPERIMENTS

..
Exp
erimen
t
:
Practical
Scaling
Prop
erties
Aim
of
the
exp
erimen
t
With
this
exp
erimen
t
w
e
w
an
t
to
measure
ho
w
w
ell
TildeLDS
scales
up
in
practice,
without
con
trolling
an
y
inuences.
This
means
that
the
tree
that
is
induced
is
not
guaran
teed
to
b
e
the
same
one
or
ha
v
e
the
same
size,
and
that
a
natural
v
ariation
is
allo
w
ed
with
resp
ect
to
the
complexit
y
of
the
examples
as
w
ell
as
the
complexit
y
of
the
queries.
This
exp
erimen
t
is
th
us
mean
t
to
mimic
the
situations
that
arise
in
practice.
Since
dieren
t
trees
ma
y
b
e
gro
wn
on
dieren
t
data
sets,
the
qualit
y
of
these
trees
ma
y
dier.
W
e
in
v
estigate
this
as
w
ell.
Metho
dology
The
metho
dology
w
e
follo
w
is
to
c
ho
ose
some
domain
and
then
create
data
sets
with
dieren
t
sizes
for
this
domain.
TildeLDS
is
then
run
on
eac
h
data
set,
and
for
eac
h
run
the
induction
time
is
recorded,
as
w
ell
as
the
qualit
y
of
the
tree
(according
to
dieren
t
criteria,
see
b
elo
w).
Materials
Data
sets
from
t
w
o
domains
w
ere
used:
Rob
oCup
and
P
ok
er.
These
domains
w
ere
c
hosen
b
ecause
large
data
sets
w
ere
a
v
ailable
for
them.
F
or
eac
h
domain
sev
eral
data
sets
of
increasing
size
w
ere
created.
Whereas
induction
times
ha
v
e
b
een
measured
on
b
oth
data
sets,
predictiv
e
accuracy
has
b
een
measured
only
for
the
P
ok
er
data
set.
This
w
as
done
using
a
separate
test
set
of
00,000
examples,
whic
h
w
as
the
same
for
all
the
h
yp
otheses.
F
or
the
Rob
oCup
data
set
no
test
set
w
as
constructed,
b
ecause
in
terpretabilit
y
of
the
h
yp
otheses
b
y
domain
exp
erts
is
the
main
ev
aluation
criterion
for
this
application
(these
theories
are
used
for
v
erication
of
the
b
eha
vior
of
agen
ts,
see
(Jacobs
et
al.,
		)).
The
Rob
oCup
exp
erimen
ts
ha
v
e
b
een
run
on
a
SUN
SP
AR
Cstation-0
at
00
MHz;
for
the
P
ok
er
exp
erimen
ts
a
SUN
Ultra-
at

MHz
w
as
used.
Discussion
of
the
Results
T
able
.
sho
ws
the
consumed
CPU-times
in
function
of
the
n
um
b
er
of
ex-
amples,
as
w
ell
as
the
predictiv
e
accuracy
.
These
gures
are
plotted
in
Fig-
ure
..
Note
that
the
CPU-time
graph
is
again
plotted
on
a
double
logarithmic
scale.
With
resp
ect
to
accuracy
,
the
P
ok
er
h
yp
otheses
sho
w
the
exp
ected
b
eha
vior:
when
more
data
are
a
v
ailable,
the
h
yp
otheses
can
predict
v
ery
rare
classes
(for
whic
h
no
examples
o
ccur
in
smaller
data
sets),
whic
h
results
in
higher
accuracy
.


CHAPTER
.
SCALING
UP
Tilde
1
10
100
1000
10000
100000
1e+06
100
1000
10000
100000
Time (CPU-seconds)
# examples
Induction
Compilation
y=x
0.988
0.99
0.992
0.994
0.996
0.998
1
100
1000
10000
100000
Predictive accuracy
# examples
Accuracy
Figure
.:
Consumed
CPU-time
and
accuracy
of
h
yp
otheses
pro
duced
b
y
TildeLDS
in
the
P
ok
er
domain,
plotted
against
the
n
um
b
er
of
examples.

..
EXPERIMENTS

#examples
00
000
000
0000
0000
00000
compilation
(CPU-s)
.
.0
.
.	
.
0.
induction
(CPU-s)

0


	

accuracy
0.	
0.		
0.		
0.			
0.			
.0
T
able
.:
Consumed
CPU-time
and
accuracy
of
h
yp
otheses
pro
duced
b
y
TildeLDS
in
the
P
ok
er
domain.
#examples
0k
0k
0k
0k
0k
0k
0k
0k
	k
compilation



0
0
	
	


induction

	

	
0

	
0
0




0

0

0
T
able
.:
Consumed
CPU-time
of
h
yp
otheses
pro
duced
b
y
TildeLDS
in
the
Rob
oCup
domain;
for
induction
times
standard
errors
are
added.
0
10000
20000
30000
40000
50000
60000
10000
20000
30000
40000
50000
60000
70000
80000
90000
Time (CPU-seconds)
# examples
Induction
Compilation
Figure
.:
Consumed
CPU-time
for
TildeLDS
in
the
Rob
oCup
domain,
plot-
ted
against
the
n
um
b
er
of
examples.


CHAPTER
.
SCALING
UP
Tilde
The
graphs
further
sho
w
that
in
the
P
ok
er
domain,
TildeLDS
scales
up
linearly
,
ev
en
though
more
accurate
(and
sligh
tly
more
complex)
theories
are
found
for
larger
data
sets.
In
the
Rob
oCup
domain,
the
induced
h
yp
otheses
w
ere
the
same
for
all
runs
except
the
0000
examples
run.
In
this
single
case
the
h
yp
othesis
w
as
more
simple
and,
according
to
the
domain
exp
ert,
less
informativ
e
than
for
the
other
runs.
This
suggests
that
in
this
domain
a
relativ
ely
small
set
of
examples
(0000)
suces
to
learn
from.
It
is
harder
to
see
ho
w
TildeLDS
scales
up
for
the
Rob
oCup
data.
Since
the
same
tree
is
returned
in
all
runs
except
the
0000
examples
run,
one
w
ould
exp
ect
the
induction
times
to
gro
w
linearly
.
Ho
w
ev
er,
the
observ
ed
curv
e
do
es
not
seem
linear,
although
it
do
es
not
sho
w
a
clear
tendency
to
b
e
sup
er-linear
either.
Because
large
v
ariations
in
induction
time
w
ere
observ
ed,
w
e
p
erformed
these
runs
0
times;
the
estimated
mean
induction
times
are
rep
orted
together
with
their
standard
errors.
The
standard
errors
alone
cannot
explain
the
ob-
serv
ed
deviations,
nor
can
v
ariations
in
example
complexit
y
(all
examples
are
of
equal
complexit
y
in
this
domain).
A
p
ossible
explanation
is
the
fact
that
the
Prolog
engine
p
erforms
a
n
um-
b
er
of
tasks
that
are
not
con
trolled
b
y
Tilde,
suc
h
as
garbage
collection.
In
sp
ecic
cases,
the
Prolog
engine
ma
y
p
erform
man
y
garbage
collections
b
efore
expanding
its
memory
space
(this
happ
ens
when
the
amoun
t
of
free
memory
after
garbage
collection
is
alw
a
ys
just
ab
o
v
e
some
threshold),
and
the
time
needed
for
these
garbage
collections
is
included
in
the
measured
CPU-times.
The
MasterProLog
engine
is
kno
wn
to
sometimes
exhibit
suc
h
b
eha
vior
(cf.
Bart
Demo
en,
p
ersonal
comm
unication).
In
order
to
sort
this
out,
TildeLDS
w
ould
ha
v
e
to
b
e
reimplemen
ted
in
a
language
of
lo
w
er
lev
el
than
Prolog
in
order
to
ha
v
e
full
con
trol
o
v
er
all
computations
that
o
ccur.
Suc
h
a
reimplemen
tation
is
planned
(but
not
within
this
thesis
w
ork).
Conclusions
W
e
stress
that
an
y
results
concerning
the
ev
olution
of
tree
complexit
y
and
accuracy
when
more
data
b
ecome
a
v
ailable
are
necessarily
domain-dep
enden
t,
and
one
should
b
e
cautious
when
generalizing
them.
It
seems
safe,
though,
to
conclude
from
our
exp
erimen
ts
that
the
linear
scaling
prop
ert
y
has
at
least
a
reasonable
c
hance
of
o
ccurring
in
practice.
.
Related
W
ork
The
text
of
this
c
hapter
is
based
on
(Blo
c
k
eel
et
al.,
		a).

..
RELA
TED
W
ORK

This
w
ork
is
closely
related
to
eorts
in
the
prop
ositional
learning
eld
to
increase
the
capabilit
y
of
mac
hine
learning
systems
to
handle
large
databases.
It
has
b
een
inuenced
more
sp
ecically
b
y
a
tutorial
on
data
mining
b
y
Usama
F
a
yy
ad,
in
whic
h
the
w
ork
of
Meh
ta
and
others
w
as
men
tioned
(Meh
ta
et
al.,
		;
Shafer
et
al.,
		).
They
w
ere
the
rst
to
prop
ose
the
lev
el-wise
tree
building
algorithm
w
e
adopted,
and
to
implemen
t
it
in
the
SLIQ
(Meh
ta
et
al.,
		)
and
SPRINT
(Shafer
et
al.,
		)
systems.
The
main
dierence
with
our
approac
h
is
that
SLIQ
and
SPRINT
learn
from
one
single
relation,
while
TildeLDS
can
learn
from
m
ultiple
relations.
Related
w
ork
inside
ILP
includes
the
Rdt/db
system
(Morik
and
Bro
c
k-
hausen,
		),
whic
h
presen
ts
the
rst
approac
h
to
coupling
an
ILP
system
with
a
relational
database
managemen
t
system
(RDBMS).
Being
an
ILP
sys-
tem,
Rdt/db
also
learns
from
m
ultiple
relations.
The
approac
h
follo
w
ed
is
that
a
logical
test
that
is
to
b
e
p
erformed
is
con
v
erted
in
to
an
SQL
query
and
sen
t
to
an
external
relational
database
managemen
t
system.
This
approac
h
is
essen
tially
dieren
t
from
ours,
in
that
it
exploits
as
m
uc
h
as
p
ossible
the
p
o
w
er
of
the
RDBMS
to
ecien
tly
ev
aluate
queries.
Also,
there
is
no
need
for
prepro
cessing
the
data.
Disadv
an
tages
are
that
for
eac
h
query
an
external
database
is
accessed,
whic
h
is
slo
w,
and
that
it
is
less
exible
with
resp
ect
to
bac
kground
kno
wledge.
Some
of
the
issues
in
v
olv
ed
in
coupling
ILP
systems
with
relational
databases
are
discussed
in
(Blo
c
k
eel
and
De
Raedt,
		).
W
e
also
men
tion
the
Kepler
system
(W
rob
el
et
al.,
		),
a
data
mining
to
ol
that
pro
vides
a
framew
ork
for
applying
a
broad
range
of
data
mining
systems
to
data
sets;
this
includes
ILP
systems.
Kepler
w
as
delib
erately
designed
to
b
e
v
ery
op
en,
and
systems
using
the
learning
from
in
terpretations
setting
can
b
e
plugged
in
to
it
as
easily
as
other
systems.
Of
the
systems
using
the
learning
from
in
terpretations
setting
(De
Raedt
and
V
an
Laer,
		;
De
Raedt
and
Dehasp
e,
		;
Dehasp
e
and
De
Raedt,
		),
the
W
armr
system
(nding
asso
ciation
rules
o
v
er
m
ultiple
relations;
see
also
(Dehasp
e
and
T
oiv
onen,
		))
is
most
closely
related
to
the
w
ork
describ
ed
in
this
c
hapter,
in
the
sense
that
there,
to
o,
an
eort
w
as
made
to
adapt
the
system
for
large
databases.
More
lo
osely
related
w
ork
inside
ILP
w
ould
include
all
eorts
to
mak
e
ILP
systems
more
ecien
t.
Since
most
of
this
w
ork
concerns
ILP
systems
that
learn
from
en
tailmen
t,
the
w
a
y
in
whic
h
this
is
done
usually
diers
substan-
tially
from
what
w
e
describ
e
in
this
pap
er.
F
or
instance,
the
Pr
ogol
system
(Muggleton,
		)
has
recen
tly
b
een
extended
with
cac
hing
and
other
eciency
impro
v
emen
ts
(Cussens,
		).
Another
direction
of
w
ork
is
the
use
of
sampling
tec
hniques,
see
e.g.
(Sriniv
asan,
		;
Sebag,
		).


CHAPTER
.
SCALING
UP
Tilde
.
Conclusions
W
e
ha
v
e
argued
and
demonstrated
empirically
that
the
use
of
ILP
is
not
limited
to
small
databases,
as
is
often
assumed.
Mining
databases
of
o
v
er
a
h
undred
megab
ytes
w
as
sho
wn
to
b
e
feasible,
and
this
do
es
not
seem
to
b
e
a
limit.
The
p
ositiv
e
results
that
ha
v
e
b
een
obtained
are
due
mainly
to
the
use
of
the
learning
from
in
terpretations
setting,
whic
h
mak
es
the
link
with
prop
osi-
tional
learning
more
clear.
This
made
it
easier
to
upgrade
the
w
ork
b
y
Meh
ta
et
al.
(		),
whic
h
turned
out
to
b
e
crucial
for
handling
large
data
sets.
In-
corp
orating
the
same
tec
hnique
in
a
system
that
uses
the
classical
ILP
setting
seems
m
uc
h
more
dicult.
The
curren
tly
a
v
ailable
results
suggest
that
the
alternativ
e
implemen
tation
ma
y
b
e
preferable
to
the
original
one,
ev
en
for
data
sets
that
can
b
e
handled
b
y
the
latter.
First
of
all,
it
uses
less
memory
and
second,
it
oers
in
teresting
optimization
opp
ortunities.
Suc
h
opp
ortunities
w
ould
also
b
ecome
a
v
ailable
for
other
ILP
systems
if
they
w
ere
re-implemen
ted
in
a
similar
fashion
as
Tilde.
Although
w
e
obtained
our
results
only
for
a
sp
ecic
kind
of
data
mining
(induction
of
decision
trees),
the
results
are
generalizable
not
only
to
other
approac
hes
within
the
classication
con
text
(e.g.
rule
based
approac
hes)
but
also
to
other
inductiv
e
tasks
within
the
learning
from
in
terpretations
setting,
suc
h
as
clustering,
regression
and
induction
of
asso
ciation
rules.

Chapter

Conclusions
This
w
ork
started
out
from
the
observ
ation
that
man
y
sophisticated
tec
hniques
for
mac
hine
learning
and
data
mining
exist,
but
most
of
them
are
set
in
the
attribute
v
alue
learning
framew
ork.
This
limits
the
application
of
these
tec
h-
niques
to
those
domains
where
the
data
can
b
e
represen
ted
as
v
ectors
of
xed
length,
and
h
yp
otheses
are
essen
tially
prop
ositional.
Inductiv
e
logic
programming,
on
the
other
hand,
pro
vides
a
ric
her
descrip-
tion
language
for
b
oth
data
and
h
yp
otheses,
namely
rst
order
predicate
logic.
Ho
w
ev
er,
the
eld
is
y
ounger
and
man
y
of
the
tec
hniques
existing
in
attribute
v
alue
learning
do
not
ha
v
e
a
coun
terpart
in
inductiv
e
logic
programming.
The
aim
of
this
w
ork
w
as
to
p
ort
some
of
the
tec
hniques
in
attribute
v
alue
learning
to
inductiv
e
logic
programming.
The
tec
hniques
w
e
fo
cused
on
are
all
decision
tree
based
tec
hniques.
T
o
ac
hiev
e
our
goal,
w
e
ha
v
e
w
ork
ed
in
sev
eral
steps:

W
e
ha
v
e
form
ulated
a
general
framew
ork
for
predictiv
e
induction
that
w
e
call
predictiv
e
clustering,
and
that
encompasses
more
sp
ecic
tec
hniques
suc
h
as
classication,
regression,
unsup
ervised
learning
and
exible
pre-
diction.
W
e
ha
v
e
furthermore
sho
wn
that
induction
of
decision
trees
can
b
e
dened
at
this
general
lev
el,
and
that
this
tec
hnique
reduces
to
the
classical
approac
hes
for
building
classication
or
regression
trees
b
y
in-
stan
tiating
a
few
parameters.

While
prop
ositional
rule
sets
ha
v
e
a
rst
order
coun
terpart
in
the
form
of
Prolog
programs,
prop
ositional
decision
trees
do
not
ha
v
e
suc
h
a
coun-
terpart.
F
or
that
reason,
w
e
ha
v
e
dened
and
studied
rst
order
logical
decision
trees.

Finally
,
w
e
ha
v
e
used
these
rst
order
logical
decision
trees
to
upgrade
the
predictiv
e
clustering
framew
ork
to
inductiv
e
logic
programming.
This
	

0
CHAPTER
.
CONCLUSIONS
has
resulted
in
the
implemen
tation
of
the
m
ulti-purp
ose
inductiv
e
logic
programming
system
Tilde,
whic
h
can
p
erform
classication,
regression,
and
sev
eral
kinds
of
clustering.
W
e
ha
v
e
ev
aluated
our
approac
h
b
y
running
Tilde
on
a
broad
range
of
applications,
empirically
studying
its
p
erformance
with
resp
ect
to
the
predictiv
e
accuracy
of
the
h
yp
otheses
it
nds,
the
simplicit
y
and
understandabilit
y
of
these
h
yp
otheses,
and
the
eciency
with
whic
h
they
can
b
e
induced.
Our
main
conclusions
are
that
the
sophisticated
attribute
learning
tec
h-
niques
that
w
e
ha
v
e
fo
cused
on
can
indeed
b
e
upgraded
to
the
inductiv
e
lo-
gic
programming
framew
ork.
The
upgraded
v
ersions
inherit
man
y
desirable
prop
erties
of
their
prop
ositional
coun
terparts,
suc
h
as
the
abilit
y
to
induce
h
yp
otheses
of
high
qualit
y
(with
resp
ect
to
b
oth
predictiv
e
accuracy
and
sim-
plicit
y),
high
eciency
and
go
o
d
scalabilit
y
prop
erties.
This
is
reected
b
y
the
fact
that
Tilde
can
comp
ete
with
man
y
curren
t
state-of-the-art
systems
for
inductiv
e
logic
programming,
ev
en
though
most
of
these
are
more
orien
ted
to
w
ards
sp
ecic
tasks
and
can
handle
only
a
subset
of
the
tasks
Tilde
can
handle.
An
in
teresting
side-result
of
this
researc
h
is
that
sev
eral
rst
order
rep-
resen
tation
formalisms
ha
v
e
b
een
related
to
one
another
with
resp
ect
to
their
expressiv
eness.
It
turns
out
that
in
the
rst
order
con
text
decision
lists
and
decision
trees
ha
v
e
an
adv
an
tage
o
v
er
at
logic
programs
in
this
resp
ect.
F
urther
w
ork
can
b
e
iden
tied
in
sev
eral
directions.
First
of
all,
the
cluster-
ing
and
regression
subsystems
can
b
e
impro
v
ed.
They
are
trivial
instan
tiations
of
our
general
predictiv
e
clustering
approac
h.
While
this
general
approac
h
pro
vides
a
sound
basis,
and
indeed
its
trivial
instan
tiations
w
ork
quite
w
ell
already
,
in
order
to
ac
hiev
e
the
same
p
erformance
as
highly
sp
ecialized
sys-
tems
more
sp
ecialized
tec
hniques
should
b
e
incorp
orated.
A
second
direction
for
future
w
ork
is
a
further
study
of
the
predictiv
e
clus-
tering
framew
ork.
W
e
ha
v
e
argued
that
this
framew
ork
suggests
sev
eral
in
ter-
esting
opp
ortunities
for
impro
ving
the
b
eha
vior
of
inductiv
e
learners,
but
these
ha
v
e
not
b
een
studied
in
detail
y
et.
A
lot
of
w
ork
can
b
e
done
in
this
area.
Finally
,
w
e
plan
to
implemen
t
a
v
ersion
of
the
Tilde
system
in
C
or
some
other
lo
w-lev
el
language.
The
curren
t
system
is
written
in
Prolog.
This
has
enabled
a
fast
dev
elopmen
t
of
the
system
and
created
an
ideal
en
vironmen
t
for
testing
dieren
t
v
ersions
and
making
it
highly
parametrized.
Ho
w
ev
er,
an
implemen
tation
in
C
w
ould
enable
a
b
etter
comparison
with
other
inductiv
e
learners
with
resp
ect
to
eciency
,
and
ma
y
also
yield
more
precise
exp
erimen
tal
results.

Bibliograph
y
[
Agra
w
al
et
al.,
		
]
R.
Agra
w
al,
H.
Mannila,
R.
Srik
an
t,
H.
T
oiv
onen,
and
A.I.
V
erk
amo.
F
ast
disco
v
ery
of
asso
ciation
rules.
In
U.
F
a
yy
ad,
G.
Piatetsky-
Shapiro,
P
.
Sm
yth,
and
R.
Uth
urusam
y
,
editors,
A
dvanc
es
in
Know
le
dge
Disc
overy
and
Data
Mining,
pages
0{.
The
MIT
Press,
		.
[
Aha
et
al.,
		
]
D.
Aha,
D.
Kibler,
and
M.
Alb
ert.
Instance-based
learning
algorithms.
Machine
L
e
arning,
:{,
		.
[
Bain
and
Muggleton,
		
]
M.
Bain
and
S.
Muggleton.
Non-monotonic
learn-
ing.
In
S.
Muggleton,
editor,
Inductive
lo
gic
pr
o
gr
amming,
pages
{.
Academic
Press,
London,
		.
[
Bergadano
and
Giordana,
	
]
F.
Bergadano
and
A.
Giordana.
A
kno
wledge
in
tensiv
e
approac
h
to
concept
induction.
In
Pr
o
c
e
e
dings
of
the
th
Interna-
tional
Workshop
on
Machine
L
e
arning.
Morgan
Kaufmann,
	.
[
Bergadano
et
al.,
		
]
F.
Bergadano,
D.
Gunetti,
F.
Neri,
and
G.
Ruo.
ILP
data
analysis
in
adaptiv
e
system
and
net
w
ork
managemen
t,
Decem
b
er
		.
In
P
erio
dic
Progress
Rep
ort
of
ESPRIT
L
TR
Pro
ject
0
(ILP).
[
Bisson,
		a
]
G.
Bisson.
Conceptual
clustering
in
a
rst
order
logic
rep-
resen
tation.
In
Pr
o
c
e
e
dings
of
the
0th
Eur
op
e
an
Confer
enc
e
on
A
rticial
Intel
ligenc
e,
pages
{.
John
Wiley
&
Sons,
		.
[
Bisson,
		b
]
G.
Bisson.
Learning
in
F
OL
with
a
similarit
y
measure.
In
Pr
o
c
e
e
dings
of
the
	th
National
Confer
enc
e
on
A
rticial
Intel
ligenc
e
(AAAI-
	).
AAAI
Press,
		.
[
Blo
c
k
eel
and
De
Raedt,
		
]
H.
Blo
c
k
eel
and
L.
De
Raedt.
Relational
kno
w-
ledge
disco
v
ery
in
databases.
In
Pr
o
c
e
e
dings
of
the
th
International
Work-
shop
on
Inductive
L
o
gic
Pr
o
gr
amming,
v
olume

of
L
e
ctur
e
Notes
in
A
r-
ticial
Intel
ligenc
e,
pages
		{.
Springer-V
erlag,
		.



BIBLIOGRAPHY
[
Blo
c
k
eel
and
De
Raedt,
		
]
H.
Blo
c
k
eel
and
L.
De
Raedt.
Lo
ok
ahead
and
discretization
in
ILP.
In
Pr
o
c
e
e
dings
of
the
th
International
Workshop
on
Inductive
L
o
gic
Pr
o
gr
amming,
v
olume
	
of
L
e
ctur
e
Notes
in
A
rticial
Intel
ligenc
e,
pages
{.
Springer-V
erlag,
		.
[
Blo
c
k
eel
and
De
Raedt,
		a
]
H.
Blo
c
k
eel
and
L.
De
Raedt.
Isidd:
An
in
ter-
activ
e
system
for
inductiv
e
database
design.
Applie
d
A
rticial
Intel
ligenc
e,
():{,
July-August
		.
[
Blo
c
k
eel
and
De
Raedt,
		b
]
H.
Blo
c
k
eel
and
L.
De
Raedt.
T
op-do
wn
in-
duction
of
rst
order
logical
decision
trees.
A
rticial
Intel
ligenc
e,
0(-
):{	,
June
		.
[
Blo
c
k
eel
et
al.,
		a
]
H.
Blo
c
k
eel,
L.
De
Raedt,
N.
Jacobs,
and
B.
Demo
en.
Scaling
up
inductiv
e
logic
programming
b
y
learning
from
in
terpretations.
Data
Mining
and
Know
le
dge
Disc
overy,
		.
T
o
app
ear.
[
Blo
c
k
eel
et
al.,
		b
]
H.
Blo
c
k
eel,
L.
De
Raedt,
and
J.
Ramon.
T
op-do
wn
in-
duction
of
clustering
trees.
In
Pr
o
c
e
e
dings
of
the
th
International
Confer-
enc
e
on
Machine
L
e
arning,
pages
{,
		.
http://www.cs.kul
euv
en
.-
ac.be/~ml/PS/ML
	-

.ps
.
[
Blo
edorn
and
Mic
halski,
		
]
E.
Blo
edorn
and
R.
Mic
halski.
The
A
Q-DCI
system
for
data-driv
en
constructiv
e
induction
and
its
application
to
the
ana-
lysis
of
w
orld
economics.
In
Z.
Ra

s
and
M.
Mic
halewicz,
editors,
F
oundations
of
Intel
ligent
Systems,
v
olume
0	
of
L
e
ctur
e
Notes
in
A
rticial
Intel
ligenc
e,
pages
0{.
Springer-V
erlag,
		.
[
Bostr
om,
		
]
H.
Bostr
om.
Co
v
ering
vs.
divide-and-conquer
for
top-do
wn
induction
of
logic
programs.
In
Pr
o
c
e
e
dings
of
the
th
International
Joint
Confer
enc
e
on
A
rticial
Intel
ligenc
e.
Morgan
Kaufmann,
		.
[
Bo
w
ers,
		
]
A.F.
Bo
w
ers.
Early
exp
erimen
ts
with
a
higher-order
decision-
tree
learner.
In
J.
Llo
yd,
editor,
Pr
o
c
e
e
dings
of
the
JICSLP
'	
p
ost-
c
onfer
enc
e
workshop
and
Compulo
gNet
A
r
e
a
Me
eting
on
Computational
L
o-
gic
and
Machine
L
e
arning,
pages
{	,
		.
[
Bratk
o,
		0
]
I.
Bratk
o.
Pr
olo
g
Pr
o
gr
amming
for
A
rticial
Intel
ligenc
e.
Addison-W
esley
,
		0.
nd
Edition.
[
Breiman
et
al.,
	
]
L.
Breiman,
J.H.
F
riedman,
R.A.
Olshen,
and
C.J.
Stone.
Classic
ation
and
R
e
gr
ession
T
r
e
es.
W
adsw
orth,
Belmon
t,
	.
[
Breiman,
		
]
L.
Breiman.
Some
prop
erties
of
splitting
criteria.
Machine
L
e
arning,
:{,
		.

BIBLIOGRAPHY

[
Bun
tine
and
Niblett,
		
]
W.
Bun
tine
and
T.
Niblett.
A
further
comparison
of
splitting
rules
for
decision
tree
induction.
Machine
L
e
arning,
:{,
		.
[
Catlett,
		
]
J.
Catlett.
On
c
hanging
con
tin
uous
attributes
in
to
ordered
dis-
crete
attributes.
In
Yv
es
Ko
drato,
editor,
Pr
o
c
e
e
dings
of
the
th
Eur
op
e
an
Working
Session
on
L
e
arning,
v
olume

of
L
e
ctur
e
Notes
in
A
rticial
In-
tel
ligenc
e,
pages
{.
Springer-V
erlag,
		.
[
Chapman
and
Kaelbling,
		
]
D.
Chapman
and
L.P
.
Kaelbling.
Input
gener-
alization
in
dela
y
ed
reinforcemen
t
learning:
An
algorithm
and
p
erformance
comparisons.
In
Pr
o
c
e
e
dings
of
the
th
International
Joint
Confer
enc
e
on
A
rticial
Intel
ligenc
e.
Morgan
Kaufmann,
		.
[
Clark
and
Niblett,
		
]
P
.
Clark
and
T.
Niblett.
The
CN
algorithm.
Ma-
chine
L
e
arning,
():{,
		.
[
Clo
c
ksin
and
Mellish,
	
]
W.F.
Clo
c
ksin
and
C.S.
Mellish.
Pr
o
gr
amming
in
Pr
olo
g.
Springer-V
erlag,
Berlin,
	.
[
Cohen
and
P
age,
		
]
W.W.
Cohen
and
D.
P
age.
P
olynomial
learnabilit
y
and
inductiv
e
logic
programming:
Metho
ds
and
results.
New
Gener
ation
Computing,
,
		.
[
Cohen,
		
]
W.W.
Cohen.
P
ac-learning
recursiv
e
logic
programs:
Negativ
e
results.
Journal
of
A
rticial
Intel
ligenc
e
R
ese
ar
ch,
:{,
		.
[
Cussens,
		
]
J.
Cussens.
P
art-of-sp
eec
h
tagging
using
progol.
In
Pr
o
c
e
e
dings
of
the
th
International
Workshop
on
Inductive
L
o
gic
Pr
o
gr
amming,
Lecture
Notes
in
Articial
In
telligence,
pages
	{0.
Springer-V
erlag,
		.
[
De
Raedt
and
Blo
c
k
eel,
		
]
L.
De
Raedt
and
H.
Blo
c
k
eel.
Using
logical
decision
trees
for
clustering.
In
Pr
o
c
e
e
dings
of
the
th
International
Workshop
on
Inductive
L
o
gic
Pr
o
gr
amming,
v
olume
	
of
L
e
ctur
e
Notes
in
A
rticial
Intel
ligenc
e,
pages
{.
Springer-V
erlag,
		.
[
De
Raedt
and
Bruyno
oghe,
		
]
L.
De
Raedt
and
M.
Bruyno
oghe.
A
theory
of
clausal
disco
v
ery
.
In
Pr
o
c
e
e
dings
of
the
th
International
Joint
Confer
enc
e
on
A
rticial
Intel
ligenc
e,
pages
0{0.
Morgan
Kaufmann,
		.
[
De
Raedt
and
Dehasp
e,
		
]
L.
De
Raedt
and
L.
Dehasp
e.
Clausal
disco
v
ery
.
Machine
L
e
arning,
:		{,
		.
[
De
Raedt
and
D

zeroski,
		
]
L.
De
Raedt
and
S.
D

zeroski.
First
order
j
k
-
clausal
theories
are
P
A
C-learnable.
A
rticial
Intel
ligenc
e,
0:{	,
		.


BIBLIOGRAPHY
[
De
Raedt
and
V
an
Laer,
		
]
L.
De
Raedt
and
W.
V
an
Laer.
Inductiv
e
con-
strain
t
logic.
In
Klaus
P
.
Jan
tk
e,
T
ak
eshi
Shinohara,
and
Thomas
Zeugmann,
editors,
Pr
o
c
e
e
dings
of
the
th
International
Workshop
on
A
lgorithmic
L
e
arn-
ing
The
ory,
v
olume
		
of
L
e
ctur
e
Notes
in
A
rticial
Intel
ligenc
e,
pages
0{
	.
Springer-V
erlag,
		.
[
De
Raedt
et
al.,
		
]
L.
De
Raedt,
L.
Dehasp
e,
W
V
an
Laer,
H.
Blo
c
k
eel,
and
M.
Bruyno
oghe.
On
the
dualit
y
of
CNF
and
DNF,
or
ho
w
to
learn
CNF
using
a
DNF
learner.
Unpublished,
		.
[
De
Raedt
et
al.,
		
]
L.
De
Raedt,
P
.
Idestam-Almquist,
and
G.
Sablon.

-
subsumption
for
structural
matc
hing.
In
Pr
o
c
e
e
dings
of
the
	th
Eur
op
e
an
Confer
enc
e
on
Machine
L
e
arning,
pages
{.
Springer-V
erlag,
		.
[
De
Raedt
et
al.,
		
]
L.
De
Raedt,
H.
Blo
c
k
eel,
L.
Dehasp
e,
and
W.
V
an
Laer.
Three
companions
for
rst
order
data
mining.
In
S.
D

zeroski
and
N.
La
vra

c,
editors,
Inductive
L
o
gic
Pr
o
gr
amming
for
Know
le
dge
Disc
ov-
ery
in
Datab
ases,
Lecture
Notes
in
Articial
In
telligence.
Springer-V
erlag,
		.
T
o
app
ear.
[
De
Raedt,
		
]
L.
De
Raedt,
editor.
A
dvanc
es
in
Inductive
L
o
gic
Pr
o
gr
am-
ming,
v
olume

of
F
r
ontiers
in
A
rticial
Intel
ligenc
e
and
Applic
ations.
IOS
Press,
		.
[
De
Raedt,
		
]
L.
De
Raedt.
Logical
settings
for
concept
learning.
A
rticial
Intel
ligenc
e,
	:{0,
		.
[
De
Raedt,
		
]
L.
De
Raedt.
A
ttribute-v
alue
learning
v
ersus
inductiv
e
lo-
gic
programming:
the
missing
links
(extended
abstract).
In
D.
P
age,
editor,
Pr
o
c
e
e
dings
of
the
th
International
Confer
enc
e
on
Inductive
L
o
gic
Pr
o
gr
am-
ming,
v
olume

of
L
e
ctur
e
Notes
in
A
rticial
Intel
ligenc
e,
pages
{.
Springer-V
erlag,
		.
[
Dehasp
e
and
De
Raedt,
		
]
L.
Dehasp
e
and
L.
De
Raedt.
Mining
asso
ci-
ation
rules
in
m
ultiple
relations.
In
Pr
o
c
e
e
dings
of
the
th
International
Workshop
on
Inductive
L
o
gic
Pr
o
gr
amming,
v
olume
	
of
L
e
ctur
e
Notes
in
A
rticial
Intel
ligenc
e,
pages
{.
Springer-V
erlag,
		.
[
Dehasp
e
and
T
oiv
onen,
		
]
L.
Dehasp
e
and
H.
T
oiv
onen.
F
requen
t
query
disco
v
ery:
a
unifying
ILP
approac
h
to
asso
ciation
rule
mining.
Data
Mining
and
Know
le
dge
Disc
overy,
		.
T
o
app
ear.
[
Dietteric
h
et
al.,
		
]
T.
G.
Dietteric
h,
R.
H.
Lathrop,
and
T.
Lozano-P

erez.
Solving
the
m
ultiple-instance
problem
with
axis-parallel
rectangles.
A
rticial
Intel
ligenc
e,
	(-):{,
		.

BIBLIOGRAPHY

[
Dol

sak
and
Muggleton,
		
]
B.
Dol

sak
and
S.
Muggleton.
The
application
of
Inductiv
e
Logic
Programming
to
nite
elemen
t
mesh
design.
In
S.
Muggleton,
editor,
Inductive
lo
gic
pr
o
gr
amming,
pages
{.
Academic
Press,
		.
[
Domingos,
		
]
P
.
Domingos.
Unifying
instance
based
and
rule
based
induc-
tion.
Machine
L
e
arning,
():{,
		.
[
Domingos,
		
]
P
.
Domingos.
Data
mining
with
RISE
and
CWS.
In
F.
Es-
p
osito,
R.S.
Mic
halski,
and
L.
Saitta,
editors,
Pr
o
c
e
e
dings
of
the
th
Inter-
national
Workshop
on
Multistr
ate
gy
L
e
arning,
pages
{,
		.
[
Doughert
y
et
al.,
		
]
J.
Doughert
y
,
R.
Koha
vi,
and
M.
Sahami.
Sup
er-
vised
and
unsup
ervised
discretization
of
con
tin
uous
features.
In
A.
Prieditis
and
S.
Russell,
editors,
Pr
o
c.
Twelfth
International
Confer
enc
e
on
Machine
L
e
arning.
Morgan
Kaufmann,
		.
[
D

zeroski
et
al.,
		
]
S.
D

zeroski,
S.
Sc
h
ulze-Kremer,
K.
R.
Heidtk
e,
K.
Siems,
D.
W
ettsc
herec
k,
and
H.
Blo
c
k
eel.
Diterp
ene
structure
elucid-
ation
from
C
NMR
sp
ectra
with
inductiv
e
logic
programming.
Applie
d
A
rticial
Intel
ligenc
e,
():{,
July-August
		.
[
D

zeroski
et
al.,
		
]
S.
D

zeroski,
S.
Muggleton,
and
S.
Russell.
P
A
C-
learnabilit
y
of
determinate
logic
programs.
In
Pr
o
c
e
e
dings
of
the
th
A
CM
workshop
on
Computational
L
e
arning
The
ory,
pages
{,
		.
[
D

zeroski
et
al.,
		
]
S.
D

zeroski,
L.
De
Raedt,
and
H.
Blo
c
k
eel.
Relational
reinforcemen
t
learning.
In
Pr
o
c
e
e
dings
of
the
th
International
Confer
enc
e
on
Machine
L
e
arning.
Morgan
Kaufmann,
		.
[
Elmasri
and
Na
v
athe,
		
]
R.
Elmasri
and
S.
B.
Na
v
athe.
F
undamentals
of
Datab
ase
Systems.
The
Benjamin/Cummings
Publishing
Compan
y
,
nd
edi-
tion,
		.
[
Emde
and
W
ettsc
herec
k,
		
]
W.
Emde
and
D.
W
ettsc
herec
k.
Relational
instance-based
learning.
In
L.
Saitta,
editor,
Pr
o
c
e
e
dings
of
the
th
Interna-
tional
Confer
enc
e
on
Machine
L
e
arning,
pages
{0.
Morgan
Kaufmann,
		.
[
Emde,
		
]
W.
Emde.
Inductiv
e
learning
of
c
haracteristic
concept
descrip-
tions.
In
S.
W
rob
el,
editor,
Pr
o
c
e
e
dings
of
the
th
International
Workshop
on
Inductive
L
o
gic
Pr
o
gr
amming,
v
olume

of
GMD-Studien,
pages
{0,
Sankt
Augustin,
German
y
,
		.
Gesellsc
haft
f

ur
Mathematik
und
Daten-
v
erarb
eitung
MBH.


BIBLIOGRAPHY
[
F
a
yy
ad
and
Irani,
		
]
U.M.
F
a
yy
ad
and
K.B.
Irani.
Multi-in
terv
al
discretiz-
ation
of
con
tin
uous-v
alued
attributes
for
classication
learning.
In
Pr
o
c
e
e
d-
ings
of
the
th
International
Joint
Confer
enc
e
on
A
rticial
Intel
ligenc
e,
pages
0{0,
San
Mateo,
CA,
		.
Morgan
Kaufmann.
[
F
a
yy
ad
et
al.,
		
]
U.
F
a
yy
ad,
G.
Piatetsky-Shapiro,
P
.
Sm
yth,
and
R.
Uth
urusam
y
,
editors.
A
dvanc
es
in
Know
le
dge
Disc
overy
and
Data
Mining.
The
MIT
Press,
		.
[
Fisher
and
Langley
,
	
]
D.
Fisher
and
P
.
Langley
.
Approac
hes
to
conceptual
clustering.
In
Pr
o
c
e
e
dings
of
the
	th
International
Joint
Confer
enc
e
on
A
rti-
cial
Intel
ligenc
e,
pages
	{	,
Los
Altos,
CA,
	.
Morgan
Kaufmann.
[
Fisher,
	
]
D.
H.
Fisher.
Kno
wledge
acquisition
via
incremen
tal
conceptual
clustering.
Machine
L
e
arning,
:	{,
	.
[
Fisher,
		
]
D.
H.
Fisher.
Iterativ
e
optimization
and
simplication
of
hier-
arc
hical
clusterings.
Journal
of
A
rticial
Intel
ligenc
e
R
ese
ar
ch,
:{	,
		.
[
F
ra
wley
et
al.,
		
]
W.
F
ra
wley
,
G.
Piatetsky-Shapiro,
and
C.
Matheus.
Kno
wledge
disco
v
ery
in
databases:
an
o
v
erview.
In
G.
Piatetsky-Shapiro
and
W.
F
ra
wley
,
editors,
Know
le
dge
Disc
overy
in
Datab
ases,
pages
{.
Cam
bridge,
MA:
MIT
Press,
		.
[
Geib
el
and
Wysotzki,
		
]
P
.
Geib
el
and
F.
Wysotzki.
A
logical
framew
ork
for
graph
theoretical
decision
tree
learning.
In
N.
La
vra

c
and
S.
D

zeroski,
editors,
Pr
o
c
e
e
dings
of
the
th
International
Workshop
on
Inductive
L
o
gic
Pr
o
gr
amming,
pages
{0,
		.
[
Gluc
k
and
Corter,
	
]
M.A.
Gluc
k
and
J.E.
Corter.
Information,
uncer-
tain
t
y
,
and
the
utilit
y
of
categories.
In
Pr
o
c
e
e
dings
of
the
Seventh
A
nnaul
Confer
enc
e
of
the
Co
gnitive
Scienc
e
So
ciety,
pages
{,
Hillsdale,
NJ,
	.
La
wrence
Erlbaum.
[
Hartigan,
	
]
J.A.
Hartigan.
Clustering
A
lgorithms.
Wiley
New
Y
ork,
	.
[
Helft,
		
]
N.
Helft.
Induction
as
nonmonotonic
inference.
In
Pr
o
c
e
e
dings
of
the
st
International
Confer
enc
e
on
Principles
of
Know
le
dge
R
epr
esentation
and
R
e
asoning,
pages
	{.
Morgan
Kaufmann,
		.
[
Hutc
hinson,
		
]
A.
Hutc
hinson.
Metrics
on
terms
and
clauses.
In
Pr
o
c
e
e
d-
ings
of
the
	th
Eur
op
e
an
Confer
enc
e
on
Machine
L
e
arning,
Lecture
Notes
in
Articial
In
telligence,
pages
{.
Springer-V
erlag,
		.

BIBLIOGRAPHY

[
Jacobs
et
al.,
		
]
N.
Jacobs,
K.
Driessens,
and
L.
De
Raedt.
Using
ILP
systems
for
v
erication
and
v
alidation
of
m
ulti
agen
t
systems.
In
Pr
o
c
e
e
dings
of
the
th
International
Confer
enc
e
on
Inductive
L
o
gic
Pr
o
gr
amming,
v
olume
,
pages
{.
Springer-V
erlag,
		.
[
Karali

c
and
Bratk
o,
		
]
A.
Karali

c
and
I.
Bratk
o.
First
order
regression.
Machine
L
e
arning,
:{,
		.
[
Kazak
o
v
et
al.,
		
]
D.
Kazak
o
v,
L.
P
op
elinsky
,
and
O.
Stepank
o
v
a.
ILP
datasets
page
[http://www.gmd.de/ml-archive
/data
sets
/ilp
-res.
html
],
		.
[
Ketterlin
et
al.,
		
]
A.
Ketterlin,
P
.
Gancarski,
and
J.J.
Korczak.
Concep-
tual
clustering
in
structured
databases
:
a
practical
approac
h.
In
Pr
o
c
e
e
dings
of
KDD-	,
		.
[
Khan
et
al.,
		
]
K.
Khan,
S.
Muggleton,
and
R.
P
arson.
Rep
eat
learning
using
predicate
in
v
en
tion.
In
D.
P
age,
editor,
Pr
o
c
e
e
dings
of
the
th
Inter-
national
Confer
enc
e
on
Inductive
L
o
gic
Pr
o
gr
amming,
pages
{,
		.
[
Kietz
and
Morik,
		
]
J.U.
Kietz
and
K..
Morik.
A
p
olynomial
approac
h
to
the
constructiv
e
induction
of
structural
kno
wledge.
Machine
L
e
arning,
:	{,
		.
[
Kirsten
and
W
rob
el,
		
]
M.
Kirsten
and
S.
W
rob
el.
Relational
distance-
based
clustering.
In
Pr
o
c
e
e
dings
of
the
th
International
Confer
enc
e
on
In-
ductive
L
o
gic
Pr
o
gr
amming,
Lecture
Notes
in
Articial
In
telligence,
pages
{0.
Springer-V
erlag,
		.
[
Kitano
et
al.,
		
]
H.
Kitano,
M.
V
eloso,
H.
Matsubara,
M.
T
am
b
e,
S.
Cora-
desc
hi,
I.
No
da,
P
.
Stone,
E.
Osa
w
a,
and
M.
Asada.
The
rob
o
cup
syn
thetic
agen
t
c
hallenge
	.
In
Pr
o
c
e
e
dings
of
the
th
International
Joint
Confer
enc
e
on
A
rticial
Intel
ligenc
e,
pages
{	.
Morgan
Kaufmann,
		.
[
Ko
w
alski,
		
]
R.
Ko
w
alski.
L
o
gic
for
pr
oblem
solving.
North-Holland,
		.
[
Kramer
et
al.,
		
]
S.
Kramer,
B.
Pfahringer,
and
C.
Helma.
Sto
c
hastic
pro-
p
ositionalization
of
non-determinate
bac
kground
kno
wledge.
In
D.
P
age,
editor,
Pr
o
c
e
e
dings
of
the
th
International
Confer
enc
e
on
Inductive
L
o
gic
Pr
o
gr
amming,
pages
0{	,
		.
[
Kramer,
		
]
S.
Kramer.
Structural
regression
trees.
In
Pr
o
c
e
e
dings
of
the
th
National
Confer
enc
e
on
A
rticial
Intel
ligenc
e
(AAAI-	),
		.
[
Langley
and
Sage,
	
]
P
.
Langley
and
S.
Sage.
Conceptual
clustering
as
discrimination
learning.
In
Pr
o
c
e
e
dings
of
the
Fifth
Biennial
Confer
enc
e
of
the
Canadian
So
ciety
for
Computational
Studies
of
Intel
ligenc
e,
pages
	{	,
London,
On
tario,
Canada,
	.


BIBLIOGRAPHY
[
Langley
,
		
]
P
.
Langley
.
Elements
of
Machine
L
e
arning.
Morgan
Kaufmann,
		.
[
La
vra

c
and
D

zeroski,
		
]
N.
La
vra

c
and
S.
D

zeroski,
editors.
Pr
o
c
e
e
dings
of
the
th
International
Workshop
on
Inductive
L
o
gic
Pr
o
gr
amming,
v
olume
	
of
L
e
ctur
e
Notes
in
A
rticial
Intel
ligenc
e.
Springer-V
erlag,
		.
[
Llo
yd,
	
]
J.W.
Llo
yd.
F
oundations
of
lo
gic
pr
o
gr
amming.
Springer-V
erlag,
nd
edition,
	.
[
Manago,
		
]
M.
Manago.
Kno
wledge
in
tensiv
e
induction.
In
A.
M.
Segre,
editor,
Pr
o
c
e
e
dings
of
the
th
International
Workshop
on
Machine
L
e
arning,
pages
{.
Morgan
Kaufmann,
		.
[
Meh
ta
et
al.,
		
]
M.
Meh
ta,
R.
Agra
w
al,
and
J.
Rissanen.
SLIQ:
A
fast
scalable
classier
for
data
mining.
In
Pr
o
c
e
e
dings
of
the
Fifth
International
Confer
enc
e
on
Extending
Datab
ase
T
e
chnolo
gy,
		.
[
Merz
and
Murph
y
,
		
]
C.J.
Merz
and
P
.M.
Murph
y
.
UCI
rep
ository
of
mac
hine
learning
data-
bases
[http://www.ics.uci.edu/~mle
arn/m
lrep
osito
ry.h
tml]
,
		.
Irvine,
CA:
Univ
ersit
y
of
California,
Departmen
t
of
Information
and
Computer
Sci-
ence.
[
Mic
halski
and
Chilausky
,
	0
]
R.S.
Mic
halski
and
R.L.
Chilausky
.
Learning
b
y
b
eing
told
and
learning
from
examples:
an
exp
erimen
tal
comparaison
of
the
t
w
o
metho
ds
of
kno
wledge
acquisition
in
the
con
text
of
dev
eloping
an
exp
ert
system
for
so
yb
ean
disease
diagnosis.
Policy
analysis
and
information
systems,
,
	0.
[
Mic
halski
and
Stepp,
	
]
R.S.
Mic
halski
and
R.E.
Stepp.
Learning
from
observ
ation:
conceptual
clustering.
In
R.S
Mic
halski,
J.G.
Carb
onell,
and
T.M.
Mitc
hell,
editors,
Machine
L
e
arning:
an
articial
intel
ligenc
e
appr
o
ach,
v
olume
.
Tioga
Publishing
Compan
y
,
	.
[
Mic
halski
et
al.,
	
]
R.
Mic
halski,
I.
Mozeti

c,
J.
Hong,
and
N.
La
vra

c.
The
m
ulti-purp
ose
incremen
tal
learning
system
A
Q
and
its
testing
application
to
three
medical
domains.
In
Pr
o
c
e
e
dings
of
the
th
National
Confer
enc
e
on
A
rticial
Intel
ligenc
e
(AAAI-),
pages
0{0,
Philadelphia,
P
A,
	.
[
Mingers,
		
]
J.
Mingers.
An
empirical
comparison
of
selection
measures
for
decision
tree
induction.
Machine
L
e
arning,
:	{,
		.
[
Mitc
hell,
		
]
T.
Mitc
hell.
Machine
L
e
arning.
McGra
w-Hill,
		.

BIBLIOGRAPHY
	
[
Mo
oney
and
Cali,
		
]
R.J.
Mo
oney
and
M.E.
Cali.
Induction
of
rst-
order
decision
lists:
Results
on
learning
the
past
tense
of
english
v
erbs.
Journal
of
A
rticial
Intel
ligenc
e
R
ese
ar
ch,
pages
{,
		.
[
Mo
oney
,
		
]
R.J.
Mo
oney
.
Encouraging
exp
erimen
tal
results
on
learning
cnf.
Machine
L
e
arning,
	:	{	,
		.
[
Morik
and
Bro
c
khausen,
		
]
K.
Morik
and
P
.
Bro
c
khausen.
A
m
ultistrategy
approac
h
to
relational
disco
v
ery
in
databases.
In
R.S.
Mic
halski
and
Wnek
J.,
editors,
Pr
o
c
e
e
dings
of
the
r
d
International
Workshop
on
Multistr
ate
gy
L
e
arning,
pages
{,
		.
[
Muggleton
and
De
Raedt,
		
]
S.
Muggleton
and
L.
De
Raedt.
Inductiv
e
logic
programming
:
Theory
and
metho
ds.
Journal
of
L
o
gic
Pr
o
gr
amming,
	,0:	{	,
		.
[
Muggleton
et
al.,
		
]
S.
Muggleton,
D.
P
age,
and
A.
Sriniv
asan.
An
ini-
tial
exp
erimen
t
in
to
stereo
c
hemistry-based
drug
design
using
inductiv
e
logic
programming.
In
S.
Muggleton,
editor,
Pr
o
c
e
e
dings
of
the
th
International
Workshop
on
Inductive
L
o
gic
Pr
o
gr
amming,
v
olume

of
L
e
ctur
e
Notes
in
A
rticial
Intel
ligenc
e,
pages
{0.
Springer-V
erlag,
		.
[
Muggleton,
		
]
S.
Muggleton.
In
v
erse
en
tailmen
t
and
Progol.
New
Gener-
ation
Computing,
,
		.
[
Muggleton,
		
]
S.
Muggleton.
Learning
from
p
ositiv
e
data.
In
S.
Muggleton,
editor,
Pr
o
c
e
e
dings
of
the
th
Inductive
L
o
gic
Pr
o
gr
amming
Workshop,
		.
[
Muggleton,
		
]
S.
Muggleton,
editor.
Pr
o
c
e
e
dings
of
the
th
International
Workshop
on
Inductive
L
o
gic
Pr
o
gr
amming,
v
olume

of
L
e
ctur
e
Notes
in
A
rticial
Intel
ligenc
e.
Springer-V
erlag,
		.
[
Nienh
uys-Cheng,
		
]
Shan-Hw
ei
Nienh
uys-Cheng.
Distance
b
et
w
een
herbrand
in
terpretations:
A
measure
for
appro
ximations
to
a
target
concept.
In
Pr
o
c
e
e
dings
of
the
th
International
Workshop
on
Inductive
L
o
gic
Pr
o-
gr
amming,
Lecture
Notes
in
Articial
In
telligence.
Springer-V
erlag,
		.
[
P
age,
		
]
D.
P
age,
editor.
Pr
o
c
e
e
dings
of
the
th
International
Confer
enc
e
on
Inductive
L
o
gic
Pr
o
gr
amming,
v
olume

of
L
e
ctur
e
Notes
in
A
rticial
Intel
ligenc
e.
Springer-V
erlag,
		.
[
Piatetsky-Shapiro
and
F
ra
wley
,
		
]
G.
Piatetsky-Shapiro
and
W.
F
ra
wley
,
editors.
Know
le
dge
disc
overy
in
datab
ases.
The
MIT
Press,
		.
[
Plotkin,
	0
]
G.
Plotkin.
A
note
on
inductiv
e
generalization.
In
Machine
Intel
ligenc
e,
v
olume
,
pages
{.
Edin
burgh
Univ
ersit
y
Press,
	0.

0
BIBLIOGRAPHY
[
Quinlan,
	
]
J.R.
Quinlan.
Induction
of
decision
trees.
Machine
L
e
arning,
:{0,
	.
[
Quinlan,
		0
]
J.R.
Quinlan.
Learning
logical
denitions
from
relations.
Ma-
chine
L
e
arning,
:	{,
		0.
[
Quinlan,
		a
]
J.
Ross
Quinlan.
C.:
Pr
o
gr
ams
for
Machine
L
e
arning.
Mor-
gan
Kaufmann
series
in
mac
hine
learning.
Morgan
Kaufmann,
		.
[
Quinlan,
		b
]
J.R.
Quinlan.
F
OIL:
A
midterm
rep
ort.
In
P
.
Brazdil,
editor,
Pr
o
c
e
e
dings
of
the
th
Eur
op
e
an
Confer
enc
e
on
Machine
L
e
arning,
Lecture
Notes
in
Articial
In
telligence.
Springer-V
erlag,
		.
[
Quinlan,
		
]
J.
R.
Quinlan.
Learning
rst-order
denitions
of
functions.
Journal
of
A
rticial
Intel
ligenc
e
R
ese
ar
ch,
:	{,
Octob
er
		.
[
Ramon
and
Bruyno
oghe,
		
]
J.
Ramon
and
M.
Bruyno
oghe.
A
framew
ork
for
dening
distances
b
et
w
een
rst-order
logic
ob
jects.
In
Pr
o
c
e
e
dings
of
the
th
International
Confer
enc
e
on
Inductive
L
o
gic
Pr
o
gr
amming,
Lecture
Notes
in
Articial
In
telligence,
pages
{0.
Springer-V
erlag,
		.
[
Ramon
et
al.,
		
]
J.
Ramon,
M.
Bruyno
oghe,
and
W.
V
an
Laer.
Distance
measures
b
et
w
een
atoms.
In
Pr
o
c
e
e
dings
of
the
Compulo
gNet
A
r
e
a
Me
eting
on
'Computational
L
o
gic
and
Machine
L
e
arning',
pages
{,
		.
[
Rissanen,
	
]
J.
Rissanen.
Mo
deling
b
y
Shortest
Data
Description.
A
uto-
matic
a,
:{,
	.
[
Riv
est,
	
]
R.L.
Riv
est.
Learning
decision
lists.
Machine
L
e
arning,
:	{
,
	.
[
Russell
and
Norvig,
		
]
S.
Russell
and
P
.
Norvig.
A
rticial
Intel
ligenc
e:
A
Mo
dern
Appr
o
ach.
Pren
tice-Hall,
		.
[
Sebag,
		
]
M.
Sebag.
A
sto
c
hastic
simple
similarit
y
.
In
D.
P
age,
editor,
Pr
o
c
e
e
dings
of
the
th
International
Confer
enc
e
on
Inductive
L
o
gic
Pr
o
gr
am-
ming,
v
olume

of
L
e
ctur
e
Notes
in
A
rticial
Intel
ligenc
e,
pages
	{0.
Springer-V
erlag,
		.
[
Shafer
et
al.,
		
]
J.C.
Shafer,
R.
Agra
w
al,
and
M.
Meh
ta.
SPRINT:
A
scal-
able
parallel
classier
for
data
mining.
In
Pr
o
c
e
e
dings
of
the
th
Interna-
tional
Confer
enc
e
on
V
ery
L
ar
ge
Datab
ases,
		.
[
Sneath
and
Sok
al,
	
]
P
.
Sneath
and
R.
Sok
al.
Numeric
al
T
axonomy:
the
Principles
and
Pr
actic
e
of
Numeric
al
Classic
ation.
F
reeman
San
F
rancisco,
	.

BIBLIOGRAPHY

[
Sriniv
asan
and
Camac
ho,
		
]
A.
Sriniv
asan
and
R.C.
Camac
ho.
Exp
eri-
men
ts
in
n
umerical
reasoning
with
ILP.
T
ec
hnical
Rep
ort
PR
G-TR--	,
Oxford
Univ
ersit
y
Computing
Lab
oratory
,
Oxford,
		.
Accepted
to
app
ear
in
the
Journal
of
Logic
Programming,
Sp
ecial
Issue
on
ILP
.
[
Sriniv
asan
and
King,
		
]
A.
Sriniv
asan
and
R.D.
King.
F
eature
construc-
tion
with
inductiv
e
logic
programming:
A
study
of
quan
titativ
e
predictions
of
biological
activit
y
aided
b
y
structural
attributes.
In
S.
Muggleton,
editor,
Pr
o
c
e
e
dings
of
the
th
International
Workshop
on
Inductive
L
o
gic
Pr
o
gr
am-
ming,
v
olume

of
L
e
ctur
e
Notes
in
A
rticial
Intel
ligenc
e,
pages
	{0.
Springer-V
erlag,
		.
[
Sriniv
asan
et
al.,
		
]
A.
Sriniv
asan,
S.H.
Muggleton,
and
R.D.
King.
Com-
paring
the
use
of
bac
kground
kno
wledge
b
y
inductiv
e
logic
programming
systems.
In
L.
De
Raedt,
editor,
Pr
o
c
e
e
dings
of
the
th
International
Work-
shop
on
Inductive
L
o
gic
Pr
o
gr
amming,
		.
[
Sriniv
asan
et
al.,
		
]
A.
Sriniv
asan,
S.H.
Muggleton,
M.J.E.
Stern
b
erg,
and
R.D.
King.
Theories
for
m
utagenicit
y:
A
study
in
rst-order
and
feature-
based
induction.
A
rticial
Intel
ligenc
e,
,
		.
[
Sriniv
asan,
		
]
A.
Sriniv
asan.
A
study
of
t
w
o
sampling
metho
ds
for
analys-
ing
large
datasets
with
ILP.
Data
Mining
and
Know
le
dge
Disc
overy,
		.
[
Stahl,
		
]
I.
Stahl.
Predicate
in
v
en
tion
in
inductiv
e
logic
programming.
In
L.
De
Raedt,
editor,
A
dvanc
es
in
inductive
lo
gic
pr
o
gr
amming,
v
olume

of
F
r
ontiers
in
A
rticial
Intel
ligenc
e
and
Applic
ations,
pages
{.
IOS
Press,
		.
[
Sterling
and
Shapiro,
	
]
Leon
Sterling
and
Eh
ud
Shapiro.
The
art
of
Pr
o-
lo
g.
The
MIT
Press,
	.
[
Thompson
and
Langley
,
		
]
K.
Thompson
and
P
.
Langley
.
Concept
forma-
tion
in
structured
domains.
In
D.
Fisher,
M.
P
azzani,
and
P
.
Langley
,
edit-
ors,
Conc
ept
formation:
know
le
dge
and
exp
erienc
e
in
unsup
ervise
d
le
arning.
Morgan
Kaufmann,
		.
[
Utgo,
		
]
P
.E.
Utgo.
Incremen
tal
induction
of
decision
trees.
Machine
L
e
arning,
():{,
		.
[
V
alian
t,
	
]
L.
V
alian
t.
A
theory
of
the
learnable.
Communic
ations
of
the
A
CM,
:{,
	.
[
v
an
der
Laag
and
Nienh
uys-Cheng,
		
]
P
atric
k
R.
J.
v
an
der
Laag
and
Shan-Hw
ei
Nienh
uys-Cheng.
Completeness
and
prop
erness
of
renemen
t
op
erators
in
inductiv
e
logic
programming.
Journal
of
L
o
gic
Pr
o
gr
amming,
():0{,
		.


BIBLIOGRAPHY
[
V
an
Laer
et
al.,
		
]
W.
V
an
Laer,
L.
De
Raedt,
and
S.
D

zeroski.
On
m
ulti-
class
problems
and
discretization
in
inductiv
e
logic
programming.
In
Zbig-
niew
W.
Ras
and
Andrzej
Sk
o
wron,
editors,
Pr
o
c
e
e
dings
of
the
0th
Inter-
national
Symp
osium
on
Metho
dolo
gies
for
Intel
ligent
Systems
(ISMIS	),
v
olume

of
L
e
ctur
e
Notes
in
A
rticial
Intel
ligenc
e,
pages
{.
Springer-V
erlag,
		.
[
W
atanab
e
and
Rendell,
		
]
L.
W
atanab
e
and
L.
Rendell.
Learning
struc-
tural
decision
trees
from
examples.
In
Pr
o
c
e
e
dings
of
the
th
International
Joint
Confer
enc
e
on
A
rticial
Intel
ligenc
e,
pages
0{,
		.
[
W
ebb,
		
]
G.I.
W
ebb.
F
urther
exp
erimen
tal
evidence
against
the
utilit
y
of
o
ccam's
razor.
Journal
of
A
rticial
Intel
ligenc
e
R
ese
ar
ch,
:	{,
		.
[
Wilson
and
Martinez,
		
]
D.R.
Wilson
and
T.R.
Martinez.
Impro
v
ed
het-
erogeneous
distance
functions.
Journal
of
A
rticial
Intel
ligenc
e
R
ese
ar
ch,
:{,
		.
[
Wnek
and
Mic
halski,
		
]
J.
Wnek
and
R.
Mic
halski.
Hyp
othesis-driv
en
con-
structiv
e
induction
in
A
Q-HCI:
A
metho
d
and
exp
erimen
ts.
Machine
L
e
arning,
():	{,
		.
[
W
rob
el
et
al.,
		
]
S.
W
rob
el,
D.
W
ettsc
herec
k,
E.
Sommer,
and
W.
Emde.
Extensibilit
y
in
data
mining
systems.
In
Pr
o
c
e
e
dings
of
the
Se
c
ond
Inter-
national
Confer
enc
e
on
Know
le
dge
Disc
overy
and
Data
Mining
(KDD-	).
AAAI
Press,
		.
[
Y
o
o
and
Fisher,
		
]
J.
Y
o
o
and
D.
Fisher.
Concept
formation
o
v
er
explan-
ations
and
problem-solving
exp
erience.
In
Pr
o
c
e
e
dings
of
the
th
Interna-
tional
Joint
Confer
enc
e
on
A
rticial
Intel
ligenc
e,
pages
0
{
.
Morgan
Kaufmann,
		.

App
endix
A
Data
Sets
In
this
app
endix
w
e
describ
e
the
data
sets
that
w
e
ha
v
e
used.
A.
So
yb
eans
This
database
(Mic
halski
and
Chilausky,
	0)
con
tains
descriptions
of
diseased
so
yb
ean
plan
ts.
Ev
ery
plan
t
is
describ
ed
b
y

attributes.
The
plan
ts
ha
v
e
to
b
e
classied
in
to
one
of
sev
eral
classes,
eac
h
class
b
eing
a
sp
ecic
disease.
A
small
data
set
(
examples,

classes)
and
a
large
one
(0
examples,
	
classes)
are
a
v
ailable
at
the
UCI
data
rep
ository
(Merz
and
Murph
y,
		).
A
single
example
from
this
data
set,
represen
ted
in
Tilde's
input
format,
is
giv
en
in
Figure
A..
A.
Iris
The
Iris
database,
a
v
ailable
at
the
UCI
rep
ository
,
con
tains
descriptions
of
iris
plan
ts
that
are
to
b
e
classied
in
to
one
of
three
classes.
F
or
eac
h
class
there
are
0
examples.
There
are

n
umerical
attributes.
The
problem
is
v
ery
easy
to
solv
e
for
sup
ervised
learners,
hence
the
database
is
used
mostly
for
unsup
ervised
learners.
The
ev
aluation
criterion
is
then,
whether
the
learner
nds
three
clusters
that
corresp
ond
to
the
classes.
Some
examples
of
the
Iris
data
set
are
sho
wn
in
Figure
A..
A.
Mutagenesis
This
database
(Sriniv
asan
et
al.,
		),
a
v
ailable
at
the
ILP
rep
ository
(Kaza-
k
o
v
et
al.,
		),
is
probably
the
most
p
opular
database
that
has
ev
er
b
een
used



APPENDIX
A.
D
A
T
A
SETS
begin(model())
.
ziekte().
date().
plant_stand(0).
precip(0).
temp().
hail().
crop_hist().
area_damaged().
severity().
seed_tmt().
germination().
plant_growth().
leaves().
leafspots_halo(0)
.
leafspots_marg()
.
leafspot_size().
leaf_shread(0).
leaf_malf(0).
leaf_mild(0).
stem().
lodging(0).
stem_cankers(0).
canker_lesion().
fruiting_bodies(0
).
external_decay(0)
.
mycelium(0).
int_discolor().
sclerotia().
fruit_pods(0).
fruit_spots().
seed(0).
mold_growth(0).
seed_discolor(0).
seed_size(0).
shriveling(0).
end(model()).
Figure
A.:
An
example
from
the
So
yb
eans
data
set.

A..
MUT
A
GENESIS

begin(model()).
sl(.).
sw(.).
pl(.).
pw(0.).
irissetosa.
end(model()).
begin(model()).
sl(.	).
sw(.0).
pl(.).
pw(0.).
irissetosa.
end(model()).
begin(model())
.
sl(.0).
sw(.).
pl(.).
pw(.).
irisversicolor.
end(model()).
Figure
A.:
Examples
from
the
Iris
data
set.


APPENDIX
A.
D
A
T
A
SETS
in
ILP
.
It
con
tains
descriptions
of
molecules
for
whic
h
the
m
utagenic
activit
y
has
to
b
e
predicted
(m
utagenicit
y
is
the
abilit
y
to
cause
DNA
to
m
utate,
whic
h
is
a
p
ossible
cause
of
cancer).
Originally
m
utagenicit
y
w
as
measured
b
y
a
real
n
um
b
er,
but
in
most
exp
erimen
ts
with
ILP
systems
this
has
b
een
discretized
in
to
t
w
o
v
alues
(p
ositiv
e
and
negativ
e,
i.e.
m
utagenic
or
non-m
utagenic),
so
that
the
task
b
ecomes
a
classication
task.
The
data
set
consists
of
0
molecules,
whic
h
are
divided
in
to
t
w
o
subsets:
regression-friendly
(
molecules)
and
regression-unfriendly
(
molecules).
The
term
regression
in
this
resp
ect
refers
to
the
use
of
linear
regression,
not
regression
trees.
The

regression-unfriendly
molecules
are
those
where
pro-
p
ositional
linear
regression
metho
ds
did
not
yield
go
o
d
results;
these
are
the
most
in
teresting
ones
for
ILP
(although
the
set
of

molecules
has
b
een
used
more
often).
Sriniv
asan
(Sriniv
asan
et
al.,
		)
in
tro
duces
four
lev
els
of
\bac
kground"
kno
wledge,
eac
h
of
whic
h
is
a
sup
erset
of
its
predecessor:

Bac
kground
B

con
tains
only
non-n
umerical
structural
descriptions:
the
atoms
and
b
onds
in
the
molecules.

In
Bac
kground
B

the
partial
c
harges
of
the
atoms
in
the
molecules
are
also
a
v
ailable.

Bac
kground
B

adds
to
the
structural
description
of
a
molecule
t
w
o
n
u-
merical
attributes
called
lumo
and
lo
gp.
These
attributes
are
kno
wn
to
b
e
highly
relev
an
t
for
m
utagenicit
y
.

Bac
kground
B

con
tains
higher
lev
el
submolecular
structures
suc
h
as
b
en-
zene
rings,
phenan
threne
structures,
.
.
.
Note
that
w
e
are
using
the
term
\bac
kground
kno
wledge"
somewhat
inconsist-
en
tly
here.
The
dieren
t
bac
kgrounds
actually
pro
vide
ric
her
descriptions
of
the
examples,
they
do
not
only
pro
vide
general
domain
kno
wledge.
As
men-
tioned
in
Chapter

the
dierence
b
et
w
een
these
t
w
o
is
not
made
in
the
normal
ILP
setting:
the
examples
are
single
facts
there,
and
all
the
rest
is
bac
kground
kno
wledge.
That
explains
wh
y
the
dieren
t
lev
els
of
description
w
ere
called
\bac
kgrounds"
in
(Sriniv
asan
et
al.,
		).
Due
to
the
frequen
t
o
ccurrence
of
these
terms
in
the
literature
w
e
use
them
as
w
ell.
T
o
illustrate
what
a
description
of
one
molecule
lo
oks
lik
e,
Figure
A.
sho
ws
a
part
of
suc
h
a
description.
The
lev
el
of
detail
is
that
of
B

.
Eac
h
atom
fact
states
the
existence
of
a
single
atom,
the
elemen
t
it
b
elongs
to
(e.g.
c
means
it
is
a
carb
on
atom),
its
t
yp
e
(atoms
of
the
same
elemen
t
are
further
classied
in
to
dieren
t
t
yp
es
according
to
their
conformation)
and
its
partial
c
harge.
A
bond
literal
indicates
whic
h
atoms
are
b
ound,
and
what
t
yp
e
of
b
ond
it
is
(single,
double,
aromatic,
.
.
.
).
The
atoms
d_
to
d_
can
b
e
seen
to
form
a
b
enzene
ring
in
this
description.

A..
MUT
A
GENESIS

begin(model()).
pos.
atom(d_,c,,-
0.

).
atom(d_,c,,-
0.

).
atom(d_,c,,-
0.

).
atom(d_,c,	,
-0
.0
)
.
atom(d_,c,	,
0.
0
).
atom(d_,c,,-
0.

).
(...)
atom(d_,o,0,
-0
.
)
.
atom(d_,o,0,
-0
.
)
.
bond(d_,d_,
).
bond(d_,d_,
).
bond(d_,d_,
).
bond(d_,d_,
).
bond(d_,d_,
).
bond(d_,d_,
).
bond(d_,d_,
).
bond(d_,d_,
).
bond(d_,d_	,
).
(...)
bond(d_,d_	
,
).
bond(d_,d_
,
).
bond(d_,d_
,
).
end(model()).
Figure
A.:
The
Prolog
represen
tation
of
one
example
in
the
Mutagenesis
data
set.
The
atom
facts
en
umerate
the
atoms
in
the
molecule.
F
or
eac
h
atom
its
elemen
t
(e.g.
carb
on),
t
yp
e
(e.g.
carb
on
can
o
ccur
in
sev
eral
congurations;
eac
h
t
yp
e
corresp
onds
to
one
sp
ecic
conguration)
and
partial
c
harge.
The
bond
facts
en
umerate
all
the
b
onds
b
et
w
een
the
atoms
(the
last
argumen
t
is
the
t
yp
e
of
the
b
ond:
single,
double,
aromatic,
etc.).
pos
denotes
that
the
molecule
b
elongs
to
the
p
ositiv
e
class
(i.e.
is
m
utagenic).


APPENDIX
A.
D
A
T
A
SETS
typed_language(yes).
type(atom(id,
element,
type,
charge)).
type(bond(id,
id,
bondtype)).
type(X
=<
X)
:-
X
=
charge.
type(X=X).
type(member(_,
_)).
type(discretized(_,_,_)).
/*
discretization
*/
discretization(bounds(00)).
to_be_discretized(atom(_,
_,
_,
Ch),
[Ch]).
/*
refinement
operator
*/
rmode(0:
#(0**T:
atom(_,
_,
T,
_),
atom(A,
E,
T,
Ch))).
rmode(0:
#(0*	*E:
atom(_,
E,
_,
_),
atom(A,
E,
T,
Ch))).
rmode(0:
#(*00*C:
(discretized(atom(_,
_,
_,
X),
[X],
L),
member(C,
L)),
(atom(-A,
E,
T,
Ch),
Ch
=<
C))).
rmode(0:
bond(+A,
-A,
BT)).
lookahead(bond(A,
A,
BT),
#(**C:
member(C,
[,,,,,]),
BT=C)).
lookahead(bond(A,
A,
BT),
#(0*	*E:
atom(A,
E,
_,
_),
atom(A,
E,
_,
_))).
lookahead(bond(A,
A,
BT),
#(0**T:
atom(A,
_,
T,
_),
atom(A,
_,
T,
_))).
Figure
A.:
A
t
ypical
settings
le
for
the
Mutagenesis
data
set.
Figure
A.
sho
ws
a
t
ypical
settings
le
for
running
Tilde
on
this
data
set.
The
settings
le
allo
ws
the
system
to
mak
e
use
of
B

:
atoms,
b
onds,
and
partial
c
harges.
A.
Bio
degradabilit
y
This
data
set
consists
of

molecules
of
whic
h
structural
descriptions
and
molecular
w
eigh
ts
are
giv
en.
The
represen
tation
of
the
examples
is
v
ery
similar
to
that
used
for
the
Mutagenesis
data
set.
The
aim
is
to
predict
the
bio
degradabilit
y
of
the
molecules.
This
is
ex-
pressed
as
(w
e
cite
Kramer
(		))
\the
halfrate
of
surface
w
ater
aerobic

A..
MUSK
	
begin(model('MUS
K-

')
).
testid('MUSK-'
).
musk.
conformation('
_
+
',
,
-0,
-0,
-	,
...,
-,
	).
conformation('
_
+
0',
,
-,
-,
...
...
...).
(...)
conformation('
_
+	
',
...
...
...
...
,
-,
	).
end(model('MUSK-

')
).
Figure
A.:
A
part
of
one
example
from
the
Musk
data
set:
the
molecule
called
MUSK-.
It
has
sev
eral
conformations,
referred
to
as

+
etc.
aqueous
bio
degradation
in
hours".
This
is
a
real
n
um
b
er,
but
has
b
een
discret-
ized
in
to
four
v
alues
(fast,
mo
derate,
slo
w,
resistan
t)
in
most
past
exp
erimen
ts.
This
data
set
w
as
kindly
pro
vided
to
us
b
y
Sa

so
D

zeroski
and
is
used
with
p
ermission,
but
is
not
y
et
in
the
public
domain.
A.
Musk
Tw
o
Musk
data
sets
are
a
v
ailable
at
the
UCI
data
rep
ository:
a
small
one
(0kB,

tuples)
and
a
large
one
(.MB,
00
tuples).
Eac
h
tuple
consists
of

n
umerical
attributes.
The
data
sets
represen
t
sets
of
molecules,
and
the
task
is
to
predict
whic
h
molecules
are
m
usk
molecules
and
whic
h
are
not.
The
Musk
data
set
w
as
in
tro
duced
to
the
mac
hine
learning
comm
unit
y
b
y
Dietteric
h
et
al.
(		),
who
used
the
problem
to
illustrate
the
so-called
m
ultiple-instance
problem:
an
example
is
not
represen
ted
b
y
a
single
tuple
but
b
y
a
set
of
tuples,
only
one
of
whic
h
ma
y
b
e
relev
an
t.
Multiple-instance
problems
are
hard
to
cop
e
with
for
prop
ositional
learners
due
to
this
prop
ert
y
.
In
the
case
of
the
Musk
database,
molecules
are
represen
ted
b
y
sets
of
tuples;
eac
h
tuple
represen
ts
one
p
ossible
conformation
of
the
molecule.
A
molecule
is
m
usk
if
at
least
one
of
its
conformations
is
m
usk.
While
the
database
is
not
a
t
ypical
ILP
database,
b
ecause
of
its
orien
tation
to
w
ards
n
umerical
data
and
the
almost
prop
ositional
represen
tation,
learning
in
a
m
ultiple-instance
con
text
can
b
e
seen
as
the
simplest
form
of
ILP
,
as
argued
b
y
De
Raedt
(		).
The
examples
can
easily
b
e
represen
ted
b
y
in
terpretations,
eac
h
in
terpretation
simply
con
taining
a
set
of
tuples,
as
Figure
A.
illustrates.
Due
to
practical
limitations
on
the
arit
y
of
predicates
in
our
Prolog
engine
w
e
could
not
use
this
represen
tation
and
resorted
to
the
one
illustrated
in
Figure
A.,
whic
h
mak
es
the
link
with
the
original
tuples
less
clear.
Figure
A.
illustrates
what
a
settings
le
could
lo
ok
lik
e;
it
is
tak
en
from

	0
APPENDIX
A.
D
A
T
A
SETS
begin(model('MUS
K-

')
).
testid('MUSK-'
).
musk.
df('_+',)
.
df('_+',-0
)
.
df('_+',-0
).
df('_+',-	
).
df('_+',-
)
.
df('_+',	)
.
(...)
df('_+',

).
df('_+',-
0
).
df('_+',-

)
.
df('_+',	
)
.
df('_+0',
).
df('_+0',-

).
df('_+0',-

).
(...)
df('_+	',-

)
.
df('_+	',	
)
.
end(model('MUSK-

')
).
Figure
A.:
The
same
molecule
as
sho
wn
in
Figure
A.,
but
using
a
dieren
t
represen
tation.
Eac
h
conformation
is
describ
ed
b
y
a
single
fact
df
i
for
eac
h
of
its

n
umerical
attributes.

A..
MESH
	
one
of
our
exp
erimen
ts
with
the
large
Musk
data
set.
A.
Mesh
This
data
set,
in
tro
duced
in
the
ILP
comm
unit
y
b
y
Dol

sak
and
Muggleton
(		)
and
a
v
ailable
at
the
ILP
data
rep
ository
(Kazak
o
v
et
al.,
		),
has
its
origin
in
engineering.
F
or
man
y
engineering
applications,
surfaces
need
to
b
e
appro
ximated
b
y
a
nite
elemen
t
mesh.
Suc
h
a
mesh
needs
to
b
e
ne
in
some
places
(in
order
to
assure
accuracy),
and
can
b
e
coarser
in
other
places
(whic
h
decreases
the
computation
cost).
The
task
is
to
learn
rules
that
predict
ho
w
ne
a
mesh
should
b
e,
b
y
studying
a
n
um
b
er
of
meshes.
The
data
set
consists
of
descriptions
of

meshes.
It
is
a
t
ypical
ILP
data
set
in
that
it
con
tains
structural
information
and
a
lot
of
bac
kground
kno
wledge
is
a
v
ailable.
On
the
other
hand,
it
is
in
essence
a
regression
task
(whic
h
is
less
t
ypical
for
ILP):
the
resolution
of
a
mesh
is
a
natural
n
um
b
er.
Most
ILP
systems
just
consider
the
dieren
t
resolutions
to
b
e
dieren
t
classes
and
learn
a
classier.
Due
to
the
fact
that
the
resolution
of
one
edge
ma
y
dep
end
on
its
neigh
b
ors,
the
lo
calization
of
information
is
relativ
ely
bad
for
this
data
set.
F
or
our
exp
erimen
ts
with
Tilde
w
e
ha
v
e
put
all
the
information
in
the
bac
kground
and
k
ept
only
the
resolution
of
eac
h
edge
in
the
example
description
itself
(together
with
its
iden
tication).
A
represen
tativ
e
part
of
the
data
set
that
is
th
us
obtained
is
sho
wn
in
Figure
A..
A
sample
settings
le
is
sho
wn
in
Figure
A.	.
The
tilde
mode(regression)
fact
in
the
le
tells
Tilde
that
it
should
p
erform
regression
(b
y
default
Tilde
p
erforms
classication).
The
classes
setting
is
replaced
b
y
the
euclid
setting,
whic
h
tells
Tilde
on
what
v
ariable
it
should
p
erform
regression.
A.
Diterp
enes
A
detailed
description
of
this
application
can
b
e
found
in
(D

zeroski
et
al.,
		).
The
task
is
to
iden
tify
substructures
in
diterp
ene
molecules
b
y
lo
oking
at
the

C
NMR
sp
ectrogram
of
the
molecule
(p
eaks
o
ccurring
in
suc
h
a
sp
ectrogram
ma
y
indicate
the
o
ccurrence
of
certain
substructures).
There
are

classes.
The
en
tire
data
set
consists
of
0
examples.
This
problem
is
inheren
tly
relational,
but
prop
ositional
attributes
can
b
e
dened
(so-called
engineered
attributes)
that
are
highly
relev
an
t.
This
data
set
is
not
in
the
public
domain.
The
data
w
ere
kindly
pro
vided
to
us
b
y
Steen
Sc
h
ulze-Kremer
and
Sa

so
D

zeroski
and
are
used
with
p
ermission.

	
APPENDIX
A.
D
A
T
A
SETS
classes([musk,nonmusk]).
discretization(bounds()).
to_be_discretized(df(X,
N),
[N]).
(...)
to_be_discretized(df(X,
N),
[N]).
typed_language(yes).
type(df(conf,int)).
(...)
type(df(conf,int)).
type(findinterval(_,interval)
).
type(between(int,interval)).
interval([A],
[A,
inf])
:-
!.
interval([A,B|C],
[A,B]).
interval([A|B],
X)
:-
interval(B,
X).
interval([A|_],
[-inf,
A]).
interval(L,
X)
:-
interval(L,
X).
findinterval(DF,
Int)
:-
F
=..
[DF,
X,
Y],
discretized(F,
[Y],
L),
sort(L,
L),
interval(L,
Int).
between(X,
[-inf,
B])
:-
X
=<
B,
!.
between(X,
[A,
inf])
:-
A
=<
X,
!.
between(X,
[A,B])
:-
A
=<
X,
X
=<
B.
lookahead(df(Conf,Arg),
#(*00*I:
findinterval(df,
I),
between(Arg,
I))).
(...)
lookahead(df(Conf,Arg),
#(*00*I:
findinterval(df,
I),
between(Arg,
I))).
rmode(:
df(+X,
Y)).
(...)
rmode(:
df(+X,
Y)).
root(df(X,_)).
Figure
A.:
A
settings
le
for
the
Musk
data
set.

A..
DITERPENES
	
begin(background).
long(a).
long(a).
long(a).
long(b	).
(...)
usual(a).
usual(a	).
usual(b).
(...)
short(a).
short(a	).
short(a).
short(a).
(...)
circuit(c).
circuit(c).
circuit(c).
(...)
half_circuit(a).
half_circuit(a).
(...)
(...)
neighbour(A,B)
:-
neighbour_xy_r(A,B);
neighbour_yz_r(A,B);
neighbour_zx_r(A,B).
neighbour(A,B)
:-
neighbour_xy_r(B,A);
neighbour_yz_r(B,A);
neighbour_zx_r(B,A).
neighbour_xy_r(a,a).
neighbour_xy_r(a,a).
(...)
opp(A,B)
:-
opposite(A,B);
opposite(B,A).
opposite(a,a).
opposite(a	,a).
opposite(a,a).
(...)
end(background).
begin(model(a)).
resolution().
structure(a).
mesh_id(a).
end(model(a)).
begin(model(a)).
resolution().
structure(a).
mesh_id(a).
end(model(a)).
begin(model(a)).
resolution().
structure(a).
mesh_id(a).
end(model(a)).
Figure
A.:
Data
represen
tation
in
the
Mesh
data
set.

	
APPENDIX
A.
D
A
T
A
SETS
tilde_mode(regression).
euclid(resolution(X),
X).
property(E,
long(E)).
property(E,
usual(E)).
property(E,
short(E)).
(...)
property(E,
two_side_loaded(E)).
property(E,
cont_loaded(E)).
/*
lookahead
specifications
*/
lookahead(opp(E,E),
X)
:-
property(E,
X).
lookahead(neighbour(E,E),
X)
:-
property(E,
X).
lookahead(eq(E,E),
X)
:-
property(E,
X).
rmode(:
long(+E)).
rmode(:
usual(+E)).
(...)
rmode(:
two_side_loaded(+E)).
rmode(:
cont_loaded(+E)).
rmode(:
neighbour(+E,
-E)).
rmode(:
opp(+E,
-E)).
rmode(:
eq(+E,
-E)).
root(mesh_id(E)).
Figure
A.	:
A
t
ypical
settings
le
for
the
Mesh
data
set.
The
task
is
dened
as
regression
on
the
rst
argumen
t
of
the
resolution
predicate.

A..
R
OBOCUP
	
A.
Rob
oCup
This
is
a
data
set
con
taining
data
ab
out
so
ccer
games
pla
y
ed
b
y
soft
w
are
agen
ts
training
for
the
Rob
oCup
comp
etition
(Kitano
et
al.,
		).
It
con
tains
	
examples
and
is
00MB
large.
Eac
h
example
consists
of
a
description
of
the
state
of
the
so
ccer
terrain
as
observ
ed
b
y
one
sp
ecic
pla
y
er
on
a
single
momen
t.
This
description
includes
the
iden
tit
y
of
the
pla
y
er,
the
p
ositions
of
all
pla
y
ers
and
of
the
ball,
the
time
at
whic
h
the
example
w
as
recorded,
the
action
the
pla
y
er
p
erformed,
and
the
time
at
whic
h
this
action
w
as
executed.
Figure
A.0
sho
ws
one
example.
The
classes
are
high-lev
el
represen
tations
of
the
actions
of
the
agen
ts;
they
are
not
represen
ted
explicitly
in
an
example
but
computed
from
the
description
of
the
example.
A.	
P
ok
er
The
P
ok
er
application
w
as
rst
used
in
(Blo
c
k
eel
et
al.,
		a).
It
consists
of
sev
eral
data
sets
of
dieren
t
size,
all
generated
automatically
using
a
program
that
mimics
the
w
a
y
in
whic
h
cards
are
assigned
to
hands
(i.e.
the
class
dis-
tributions
are
as
they
really
o
ccur
in
P
ok
er
games).
In
the
classication
prob-
lem
w
e
consider,
there
are

classes:
pair,
double
pair,
three
of
a
kind,
full
house,
flush,
four
of
a
kind,
nought.
This
is
a
simplication
of
the
real
p
ok
er
domain,
where
one
distinguishes
e.g.
pair
of
kings
and
pair
of
eigh
ts,
etc.
Using
a
program
that
randomly
generates
examples
for
this
domain
has
the
adv
an
tage
that
one
can
easily
create
m
ultiple
training
sets
of
increasing
size,
an
indep
enden
t
test
set,
etc.,
whic
h
oers
a
lot
of
exibilit
y
for
exp
erimen
tation.
An
in
teresting
prop
ert
y
of
this
data
set
is
that
some
classes
are
v
ery
rare,
hence
a
large
data
set
is
needed
to
learn
these
classes
(assuming
the
data
are
generated
randomly).
Figure
A.
illustrates
ho
w
one
example
in
the
P
ok
er
domain
can
b
e
rep-
resen
ted.
An
example
of
a
t
ypical
settings
le
is
giv
en
in
Figure
A..

	
APPENDIX
A.
D
A
T
A
SETS
begin(model(e)
).
player(my,,-.
0

,
-0
.
	

,

	).
player(my,,-.
	

	,
.0
0	
0	
,

).
player(my,,-.


,
-
.	

	
,
0
).
player(my,,-.


,.



,
)
.
player(my,,-.

0
,
.
	

,

).
player(my,,-.


	,

.
	
	,

)
.
player(my,,-.
	
	
,

.

,

)
.
player(my,,-.

	
	,
.

	
,

).
player(my,	,-.


,

.0

0,

)
.
player(my,0,-
.	


,
.

0
,

).
player(my,,-0
.
	0

,
.

	
,
)
.
player(other,,-
.


,
.


,

).
player(other,,0
.0
,0
.0,
0)
.
player(other,,-

.0
	

,
.
00

,
		)
.
player(other,,0
.0
,0
.0,
0)
.
player(other,,
.
0

,
	.


,

).
player(other,,-
	.
	0

,
.

	
,

).
player(other,,0
.0
,0
.0,
0)
.
player(other,,0
.0
,0
.0,
0)
.
player(other,	,-
.

	

,	.
	

,

	).
player(other,0,
0.

	

,
.

,

).
player(other,,
0.
0,
0.0
,0
).
ball(-.0	,
0.

0
	
,

).
mynumber().
rctime().
turn(.	0

)
.
actiontime().
end(model(e)).
Figure
A.0:
The
Prolog
represen
tation
of
one
example
in
the
Rob
oCup
data
set.
A
fact
suc
h
as
player(other,,-
.
0
	
,
.
0
0
,
	
	)
means
that
pla
y
er

of
the
other
team
w
as
last
seen
at
p
osition
(-,.)
at
time
		.
A
p
osition
of
(0,0)
means
that
that
pla
y
er
has
nev
er
b
een
observ
ed
b
y
the
pla
y
er
that
has
generated
this
mo
del.
The
action
p
erformed
curren
tly
b
y
this
pla
y
er
is
turn(.	0
)
:
it
is
turning
to
w
ards
the
ball.

A.	.
POKER
	
begin(model()).
card(,spades).
card(queen,hearts
).
card(	,clubs).
card(	,spades).
card(ace,diamonds
).
pair.
end(model()).
Figure
A.:
An
example
from
the
P
ok
er
data
set.
classes([nought,pair,double_p
air,
three
,ful
l_hou
se,f
lush
,fou
r]).
typed_language(yes).
type(card(rank,suit)).
type(X
\=
X).
rmode(card(-X,-Y)).
rmode(+X
\=
+Y).
max_lookahead().
lookahead(card(X,Y),
card(-U,-V)).
lookahead(card(X,Y),
X
\=
+Z).
lookahead(card(X,Y),
Y
\=
+Z).
lookahead((X
\=
Y),
(+U
\=
+V)).
Figure
A.:
A
t
ypical
settings
le
for
the
P
ok
er
data
set.

Index

-correctness


-subsumption
0,
0


-test

#
construct
0
accuracy
0
agglomerativ
e
clustering
metho
ds

analysis
of
v
ariance

ANO
V
A

Apriori

A
Q

articial
in
telligence

asso
ciated
query
	
associa
te
pro
cedure
	
attribute-v
alue
learning

b
et
w
een-subset
v
ariation

best
test
function
,
	
Bio
degradabilit
y
data
set
,
,

C0.

C.
	,
,

|
pruning
strategy

cac
hing

Car
t
	,
,

category
utilit
y

c
haracteristic
description

Cheb
yshev
distance
0
c
hemical
database
example
,

class
en
trop
y
,

classication
tree

classication
,
0
Cla
udien

cluster

cluster
assignmen
t
function

clustering

|
task
denition

conceptual
|

descriptiv
e
|
	
extensional
at
|

extensional
hierarc
hical
|

in
tensional
at
|

in
tensional
hierarc
hical
|

predictiv
e
|
,
	
clustering
exibilit
y

clustering
space

clustering
tree

CN
,

CNF

COLA

COL
T

computational
learning
theory

concept
learning

conceptual
clustering

conjunctiv
e
normal
form

co
v
ering
approac
h

cross-v
alidation
	
data
mining

decision
list

decision
tree

deduction

denite
logic
program
	
derive
logic
pr
ogram
pro
cedure
	
descriptiv
e
ILP

descriptiv
e
qualit
y
of
clusterings

	

INDEX
		
discretization

discriminan
t
description

disjunctiv
e
normal
form

distance
0
|
b
et
w
een
sets

Diterp
enes
data
set
,
,
,
	
divide-and-conquer

divisiv
e
clustering
metho
ds

DNF

equalit
y
distance

Euclidean
distance
0
explanatory
ILP

expressiv
eness
results
for
rst
order
formalisms
	
for
prop
ositional
formalisms
00
extensional
clustering
system

extensional
description

extensional
at
clustering

extensional
hierarc
hical
clustering

feature
construction

|
in
ILP

FF
oil
,
,
		
FINES
example

rst
order
logical
decision
tree
	
at
logic
program
	
exibilit
y

exible
prediction

exible
prediction
tree

F
oidl
,
,
		
F
oil
,
	,
		,

F
OLDT
	
F
ors
	
F
ruit&V
egetables
example
,
,
,
,
,
,
,
,
,

F-test

gain
ratio

Gini
heuristic

gran
ularit
y

gr
o
w
r
ule
pro
cedure

gr
o
w
tree
pro
cedure
0,
0
Hamming
distance

h
ybrid
approac
hes

h
yp
othesis

ICL
,
,
,
,

ID

ideal
protot
yp
e
function

ILP

induce
r
uleset
pro
cedure

induction

inductiv
e
logic
programming

INF
O
	,
,
,
	,
0
information

information
gain

information
gain
ratio

instance-based
learning

in
tensional
clustering
space

in
tensional
clustering
system

in
tensional
clustering
task

in
tensional
description

in
tensional
at
clustering

in
tensional
hierarc
hical
clustering

interpret
a
tions
pro
cedure

Iris
data
set
,
,

Ka
te
	
Kepler

KBG

k
-nearest
neigh
b
or

kno
wledge
disco
v
ery

language
bias

la
y
ered
logic
program
	
lazy
ev
aluation
0
learning

|
from
en
tailmen
t

|
from
in
terpretations
,

|
from
m
ultiple
relations

linear
piece-wise
regression


00
INDEX
lo
calit
y
assumption
	
logic
programming

lo
ok
ahead

lookahead
setting

mac
hine
learning

mac
hines
example
	
Mahalanobis
distance

Manhattan
distance
0
MasterProLog
0,

MDL
principle
,

mean
squared
prediction
error

minimal
Herbrand
mo
del

minimal
description
length
,

Mink
o
wski
distances

ML-Smar
t
	
MSE

m
ultiple-instance
problem

Mesh
data
set
,
,
	
Musk
data
set
,
0,
,
	
Mutagenesis
data
set
,
	{,
,
{,

noise
handling

nonmonotonic
ILP

normal
logic
program
	
normal
seman
tics
	
n
umerical
taxonom
y

OPTIMAL
SPLIT
	,
,
,
	
optimization
opp
ortunities
0{
outlier
detection

P
A
C-learning

partition
utilit
y

pattern
completion

P
ok
er
example
,
	{
P
ok
er
data
set
,
{,
	
p
ost-pruning

predicate
in
v
en
tion

t
ar
get
pro
cedure

predictabilit
y

prediction
0
predict
pro
cedure
,
	
predictiv
e
accuracy
0
predictiv
e
clustering

predictiv
eness

predictiv
e
qualit
y
of
clusterings

predictor
0
probably
appro
ximately
correct

Pr
ogol
,
	,
,
		,
,
,

Prolog

ProLog-b
y-BIM
0
protot
yp
e

|
function

ideal
|

PR
UNE
	,
,
,
	,

pr
une
tree
pro
cedure

pruning

quasi-order
0
query
simplication

Rdt/db
,

renemen
t
op
erator
0,
0
regression
tree

regression
,

relativ
e
error

RIBL

rmode
setting
0
Rob
oCup
data
set
,
{,
	
rule
sets

sample
complexit
y

scarce
class
information
learning
from
|

separate-and-conquer

signicance
tests

simplify
pro
cedure

SLIQ

sor
t
pro
cedure

So
yb
eans
data
set
,
{0,

splitting
heuristics

SPRINT

SR
T
,
	,
,
	,
	,
		,


INDEX
0
STOP
CRIT
	,
,
,
	,
0
stopping
criteria

Str
uct
	,
	,
		,

sup
ervised
learning

syn
tactic
v
arian
ts
0
target
function
0
TDIDT

tdidt
pro
cedure
0
TIC

Tilde
0{
pro
cedure
0
Tildeclassic

TildeLDS

pro
cedure
	
Tilde-R
T
	
time
complexit
y

|
of
TDIDT

top-do
wn
induction
of
decision
trees

total
v
ariation

tractable

Tritop
	,
		
Tw
eet
y

typed
language
setting
0	
type
setting
0	
unsup
ervised
learning

v
alidation
set

|
based
pruning

w
a
ce
function

W
armr
,

within-subset
v
ariation


