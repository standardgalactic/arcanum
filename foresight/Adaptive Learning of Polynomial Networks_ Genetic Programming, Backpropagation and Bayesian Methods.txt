Adaptive Learning of 
Polynomial Networks 
Genetic Programming, Backpropagation and 
Bayesian Methods 

GENETIC AND EVOLUTIONARY 
COMPUTATION SERIES 
Series Editors 
David E. Goldberg 
University of Illinois at Urbana-Champaign 
John R. Koza 
Stanford University 
Selected titles from the Series: 
THE DESIGN OF INNOVATION: Lessons from and for Competent 
Genetic Algorithms, David E. Goldberg; ISBN: 1-4020-7098-5 
GENETIC PROGRAMMING IV: Routine Human-Computer Machine 
Intelligence, John R. Koza, Martin A. Keane, Matthew J. Streeter, William 
Mydlowec, lessen Yu, Guido Lanza; ISBN: 1-4020-7446-8; softcover ISBN: 
0-387-25067-0 
EVOLUTIONARY ALGORITHMS FOR SOLVING MULTI-
OBJECTIVE PROBLEMS, Carlos A. Coello Coello, David A. Van 
Veldhuizen, and Gary B. Lamont; ISBN: 0-306-46762-3 
AUTOMATIC QUANTUM COMPUTER PROGRAMMING: A Genetic 
Programming Approach, Lee Spector, ISBN: 1-4020-7894-3 
GENETIC PROGRAMMING AND DATA STRUCTURES: Genetic 
Programming + Data Structures = Automatic Programming! William B. 
Langdon; ISBN: 0-7923-8135-1 
For a complete listing of books in this series, go to http://www.springer.com 

Adaptive Learning of 
Polynomial Networks 
Genetic Programming, Backpropagation and 
Bayesian Methods 
Nikolay Y. Nikolaev 
University of London 
Hitoshi Iba 
The University of Tokyo 
^ Spri 
ringer 

Nikolay Y. Nikolaev 
University of London 
Hitoshi Iba 
The University of Tokyo 
Library of Congress Control Number: 2006920797 
ISBN-10: 0-387-31239-0 
e-ISBN-10: 0-387-31240-4 
ISBN-13: 978-0387-312392 
e-ISBN-13: 978-0387-31240-
Â© 2006 by Springer Science+Business Media, Inc. 
All rights reserved. This work may not be translated or copied in whole or in part 
without the written permission of the publisher (Springer Science -f- Business 
Media, Inc., 233 Spring Street, New York, NY 10013, USA), except for brief 
excerpts in connection with reviews or scholarly analysis. Use in connection with 
any form of information storage and retrieval, electronic adaptation, computer 
software, or by similar or dissimilar methodology now known or hereafter 
developed is forbidden. 
The use in this publication of trade names, trademarks, service marks and similar 
terms, even if they are not identified as such, is not to be taken as an expression 
of opinion as to whether or not they are subject to proprietary rights. 
Printed in the United States of America 
9 8 7 6 5 4 3 2 1 
springer.com 

Contents 
Preface 
xi 
1. INTRODUCTION 
1 
1.1 
Inductive Learning 
3 
1.1.1 
Learning and Regression 
4 
1.1.2 
Polynomial Models 
5 
1.1.3 
Inductive Computation Machinery 
5 
1.2 
Why Polynomial Networks? 
7 
1.2.1 
Advantages of Polynomial Networks 
8 
1.2.2 
Multilayer Polynomial Networks 
9 
1.3 
Evolutionary Search 
16 
1.3.1 
STROGANOFF and its Variants 
17 
1.4 
Neural Network Training 
21 
1.5 
Bayesian Inference 
22 
1.6 
Statistical Model Vahdation 
23 
1.7 
Organization of the Book 
23 
2. INDUCTIVE GENETIC PROGRAMMING 
25 
2.1 
Polynomial Neural Networks (PNN) 
26 
2.1.1 
PNN Approaches 
27 
2.1.2 
Tree-structured PNN 
29 
2.2 
IGP Search Mechanisms 
35 
2.2.1 
Sampling and Control Issues 
36 
2.2.2 
Biological Interpretation 
36 
2.3 
Genetic Learning Operators 
38 
2.3.1 
Context-preserving Mutation 
38 
2.3.2 
Crossover Operator 
40 

vi 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
2.3.3 
Size-biasing of the Genetic Operators 
41 
2.3.4 
Tree-to-Tree Distance 
42 
2.4 
Random Tree Generation 
46 
2.5 
Basic IGP Framework 
48 
2.6 
IGP Convergence Characteristics 
50 
2.6.1 
Schema Theorem of IGP 
50 
2.6.2 
Markov Model of IGP 
51 
2.7 
Chapter Summary 
54 
3. TREE-LIKE PNN REPRESENTATIONS 
55 
3.1 
Discrete Volterra Series 
56 
3.2 
Mapping Capabihties of PNN 
57 
3.3 
Errors of Approximation 
59 
3.3.1 
Approximation Error Bounds 
59 
3.3.2 
Empirical Risk 
60 
3.4 
Linear Polynomial Networks 
62 
3.4.1 
Horizontal PNN Models 
62 
3.4.2 
Kernel PNN Models 
66 
3.5 
Nonlinear Polynomial Networks 
68 
3.5.1 
Block PNN Models 
68 
3.5.2 
Orthogonal PNN Models 
69 
3.5.3 
Trigonometric PNN Models 
71 
3.5.4 
Rational PNN Models 
75 
3.5.5 
Dynamic PNN Models 
77 
3.6 
Chapter Summary 
80 
4. FITNESS FUNCTIONS AND LANDSCAPES 
81 
4.1 
Fitness Functions 
83 
4.1.1 
Static Fitness Functions 
84 
4.1.2 
Dynamic Fitness Functions 
91 
4.1.3 
Fitness Magnitude 
94 
4.2 
Fitness Landscape Structure 
95 
4.3 
Fitness Landscape Measures 
96 
4.3.1 
Statistical Correlation Measures 
96 
4.3.2 
Probabilistic Measures 
102 
4.3.3 
Information Measures 
104 
4.3.4 
Quantitative Measures 
107 
4.4 
Chapter Summary 
109 

Contents 
vii 
5. SEARCH NAVIGATION 
111 
5.1 
The Reproduction Operator 
112 
5.1.1 
Selection Strategies 
113 
5.1.2 
Replacement Strategies 
117 
5.1.3 
Implementing Reproduction 
118 
5.2 
Advanced Search Control 
119 
5.2.1 
Macroevolutionary Search 
119 
5.2.2 
Memetic Search 
120 
5.2.3 
Search by Genetic Anneahng 
122 
5.2.4 
Stochastic Genetic HillcHmbing 
124 
5.2.5 
Coevolutionary Search 
125 
5.2.6 
Distributed Search 
128 
5.3 
Performance Examination 
128 
5.3.1 
Fitness Evolvability 
129 
5.3.2 
Convergence Measures 
130 
5.3.3 
Diversity Measures 
133 
5.3.4 
Measures of Self-Organization 
139 
5.4 
Chapter Summary 
146 
6. BACKPROPAGATION TECHNIQUES 
147 
6.1 
Multilayer Feed-forward PNN 
148 
6.2 
First-Order Backpropagation 
149 
6.2.1 
Gradient Descent Search 
150 
6.2.2 
First-Order Error Derivatives 
151 
6.2.3 
Batch Backpropagation 
157 
6.2.4 
Incremental Backpropagation 
158 
6.2.5 
Control of the Learning Step 
159 
6.2.6 
Regularized Delta Rule 
162 
6.3 
Second-Order Backpropagation 
163 
6.3.1 
Second-Order Error Derivatives 
164 
6.3.2 
Newton's Method 
169 
6.3.3 
Pseudo-Newton Training 
170 
6.3.4 
Conjugate Gradients 
170 
6.3.5 
Levenberg-Marquardt Method 
171 
6.4 
Rational Backpropagation 
172 
6.5 
Network Pruning 
176 
6.5.1 
First-Order Network Pruning 
176 
6.5.2 
Second-Order Network Pruning 
177 

viii 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
6.6 
Chapter Summary 
179 
7. TEMPORAL BACKPROPAGATION 
181 
7.1 
Recurrent PNN as State-Space Models 
182 
7.2 
Backpropagation Through Time 
184 
7.2.1 
Derivation of BPTT Algorithms 
185 
7.2.2 
Real-Time BPTT Algorithm 
189 
7.2.3 
Epochwise BPTT Algorithm 
190 
7.3 
Real-Time Recurrent Learning 
191 
7.4 
Improved Dynamic Training 
198 
7.4.1 
Teacher Enforced Training 
198 
7.4.2 
Truncating in Time 
199 
7.4.3 
Subgrouping 
199 
7.4.4 
Common Temporal Training Problem 
200 
7.5 
Second-Order Temporal BP 
200 
7.6 
Recursive Backpropagation 
204 
7.7 
Recurrent Network Optimization 
206 
7.7.1 
Regularization 
207 
7.7.2 
Recurrent Network Pruning 
207 
7.8 
Chapter Summary 
208 
8. BAYESIAN INFERENCE TECHNIQUES 
209 
8.1 
Bayesian Error Function 
211 
8.2 
Bayesian Neural Network Inference 
212 
8.2.1 
Deriving Hyper parameters 
215 
8.2.2 
Local vs. Global Regularization 
217 
8.2.3 
Evidence Procedure for PNN Models 
218 
8.2.4 
Predictive Data Distribution 
221 
8.2.5 
Choosing a Weight Prior 
221 
8.3 
Bayesian Network Pruning 
222 
8.4 
Sparse Bayesian Learning 
224 
8.5 
Recursive Bayesian Learning 
229 
8.5.1 
Sequential Weight Estimation 
229 
8.5.2 
Sequential Dynamic Hessian Estimation 
230 
8.5.3 
Sequential Hyperparameter Estimation 
232 
8.6 
Monte Carlo Training 
234 
8.6.1 
Markov Chain Monte Carlo 
235 
8.6.2 
Importance Resampling 
237 

Contents 
ix 
8.6.3 
Hybrid Sampling Resampling 
237 
8.7 
Chapter Summary 
239 
9. STATISTICAL MODEL DIAGNOSTICS 
241 
9.1 
Deviations of PNN Models 
242 
9.2 
Residual Bootstrap Sampling 
243 
9.3 
The Bias/Variance Dilemma 
244 
9.3.1 
Statistical Bias and Variance 
244 
9.3.2 
Measuring Bias and Variance 
245 
9.4 
Confidence Intervals 
248 
9.4.1 
Interval Estimation by the Delta Method 
248 
9.4.2 
Bootstrapping Confidence Intervals 
252 
9.5 
Prediction Intervals 
254 
9.5.1 
Analytical Prediction Intervals 
255 
9.5.2 
Empirical Learning of Prediction Bars 
256 
9.6 
Bayesian Intervals 
262 
9.6.1 
Analytical Bayesian Intervals 
263 
9.6.2 
Empirical Bayesian Intervals 
265 
9.7 
Model Validation Tests 
267 
9.8 
Chapter Summary 
271 
10. TIME SERIES MODELLING 
273 
10.1 Time Series Modelling 
274 
10.2 Data Preprocessing 
276 
10.3 PNN vs. Linear ARMA Models 
277 
10.4 PNN vs. Genetically Programmed Functions 
279 
10.5 PNN vs. Statistical Learning Networks 
281 
10.6 PNN vs. Neural Network Models 
283 
10.7 PNN vs. Kernel Models 
285 
10.8 Recurrent PNN vs. Recurrent Neural Networks 
288 
10.9 Chapter Summary 
290 
11. CONCLUSIONS 
291 
References 
295 
Index 
313 

Preface 
This book provides theoretical and practical knowledge for develop-
ment of algorithms that infer linear and nonlinear models. It offers a 
methodology for inductive learning of polynomial neural network mod-
els from data. The design of such tools contributes to better statistical 
data modelling when addressing tasks from various areas like system 
identification, chaotic time-series prediction, financial forecasting and 
data mining. The main claim is that the model identification process 
involves several equally important steps: finding the model structure, 
estimating the model weight parameters, and tuning these weights with 
respect to the adopted assumptions about the underlying data distrib-
ution. When the learning process is organized according to these steps, 
performed together one after the other or separately, one may expect to 
discover models that generalize well (that is, predict well). 
The book off'ers statisticians a shift in focus from the standard fin-
ear models toward highly nonlinear models that can be found by con-
temporary learning approaches. Speciafists in statistical learning will 
read about alternative probabilistic search algorithms that discover the 
model architecture, and neural network training techniques that identify 
accurate polynomial weights. They wfil be pleased to find out that the 
discovered models can be easily interpreted, and these models assume 
statistical diagnosis by standard statistical means. 
Covering the three fields of: evolutionary computation, neural net-
works and Bayesian inference, orients the book to a large audience of 
researchers and practitioners. Researchers in genetic programming will 
study how to elaborate model representations, how to make learning 
operators that sample the search space efl[iciently, how to navigate the 
search process through the design of objective fitness functions, and how 
to examine the search performance of the evolutionary system. The pos-

xii 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
sibility to use reliable means for observing the search behavior of genetic 
programming is one of its essential advantages. 
Practitioners in artificial neural networks will study how to determine 
automatically the network structure prior to applying the weight train-
ing algorithm. They will reahze what are the steps of the global search 
mechanisms that allow identification of relevant network topologies from 
data. Searching for the optimal network structure is essential for adjust-
ing the network to the concrete application task. Even if one knows how 
to make the weight training algorithm, there is a need to determine in 
advance the network architecture. In addition to this, the book gives 
various activation and basis functions for elaborating different neural 
network mappings, such as kernels, harmonics, Gaussians etc., which 
extend their descriptive power, 
SpeciaUsts in Bayesian inference will read about applications of the 
principles of probabilistic learning to polynomial networks. The Bayesian 
training of polynomial networks makes them practical tools for solving 
difficult real-world problems from various fields. This book demonstrates 
that polynomial networks can be trained probabihstically not only in of-
fline mode, but also in recursive mode. Novel Bayesian techniques for 
reliable recursive training of polynomial networks are offered. Practi-
tioners in econometrics will especially find interesting the fact that poly-
nomial networks can be trained using sampling methods, which makes 
them attractive for financial forecasting. 
Students will find this book useful for studying genetic programming, 
neural networks, and probabifistic learning. There is teaching material 
from the courses that we taught the last few years to our students in 
artificial intelhgence, machine learning, evolutionary computation and 
neural networks. The material is self-contained and includes: defini-
tions of the main inductive tasks, formulations of the basic approaches 
to addressing these tasks, introduction to the fundamentals of genetic 
programming, review of backpropagation training, and presentations of 
the basics of Bayesian learning. Undergraduate students will learn how 
to design and implement the basic mechanisms of a genetic programming 
system, including the selection scheme, and the crossover and mutation 
learning operators. Postgraduate students will study advanced topics 
such as improving the search control of genetic programming systems, 
and tools for examination of their search performance. In order to facil-
itate the understanding and easy memorization of the algorithms, they 
are summarized in tables at the end of each subsection. 
The inspiration to write this book came after a long period of con-
ducting research in inductive problem solving. The authors experience 
started with work on inductive tasks using machine learning algorithms. 

PREFACE 
xiii 
The work in this area lead to disappointment due to the many fun-
damental inabilities of the typical machine learning algorithms. First, 
they seem to be quite conservative, dealing mainly with inexpressive 
propositional concept description languages. Even advanced inductive 
programming systems cannot directly manipulate well-known nonlinear 
models from the statistical analysis. Second, they lack flexible learning 
operators that can efficiently sample the search space, as for each repre-
sentation language they require specific learning operators. Third, they 
do not allow for observation of their performance, while the learning 
proceeds, which hinders tuning of the parameters. 
Genetic programming offered interesting and challenging ideas for 
making innovative computational algorithms. This research showed that 
it is possible to devise domain independent model representations; it is 
possible to make general learning operators that need not be changed 
when changing some representation details; it is possible to organize both 
global and local searches for induction of good models; it is possible to 
navigate the search using reliable formulae from statistics and numerical 
optimization; and it is possible to validate the results with standard sta-
tistical tools. Above all, the results were better than those produced by 
machine learning algorithms. In addition to this, the genetic program-
ming paradigm allowed incorporation of the most recent achievements 
in artificial neural networks and generation of well-performing models 
from benchmark and real-world data. 
An enormous number of experiments on various data sets were con-
ducted during the last several years by both authors. Most of the ex-
periments were successful, which was considered convincing evidence for 
the potential of genetic programming. The idea of further improving the 
best results by neural network training and Bayesian algorithms was sug-
gested later by several researchers who expressed scepticism that genetic 
programming alone could discover optimal models from data. Special-
ized backpropagation and Bayesian techniques for training polynomial 
networks were further developed and tested. It was found that on some 
tasks they really could achieve considerable improvements, and so were 
worth investigation and special attention. 
The developments in this book aim to facilitate the inference of poly-
nomial models for time-series prediction. The orientation toward time-
series forecasting comes from the inherent difficulty in many scientific 
areas to find models that describe, sufficiently well, unknown dynamical 
systems that have generated the series. The interest in such natural 
problems is that many everyday tasks actually fall in this category. The 
presented successful experimental results suggest that, in general, the 
proposed methodology can be useful in practical inductive modelling. 

xiv 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
Acknowledgments. The material in this book matured with our 
experience. The first author, Dr. Nikolaev, developed his doctoral the-
sis under the supervision of Professor N. Kasabov who currently works 
at the University of Auckland. After that he was a postdoctoral fellow 
in the lab of Professor D.T. Pham at University of Cardiff. During the 
years Dr. Nikolaev enjoyed working with his colleague and best friend 
Dr. Evgeny Smirnov, University of Maastricht, who inspired him to work 
on machine learning algorithms. He also conducted research with Dr. 
Vanyo Slavov from New Bulgarian University, Sofia on immune network 
algorithms and Dr. Lilian de Menezes from City University, London 
on statistical model diagnostics. Dr. Peter Tino from the University of 
Birmingham thought him to do Bayesian inference. He thanks especially 
to his colleagues from The American University in Bulgaria and Gold-
smiths College for providing him with a pleasant working atmosphere. 
He is particularly grateful to his dear girlfriend. Dr. Snezhana Dim-
itrova, for her inspiration, encouragement, and tolerance while writing 
this book. Nikolay is extremely grateful to his parents for their love and 
support. They have always helped him, shaped his thinking and made 
him a scientist. 
The second author, Dr. Hitoshi Iba, acknowledges the pleasant re-
search atmosphere created by his colleagues and students from the re-
search laboratory associated with Graduate School of Frontier Sciences 
at the University of Tokyo. He is also grateful to his previous group 
and colleagues at Electro-Technical Laboratory, where he used to work 
for ten years. Particular thanks are due to Dr. Hirochika Inoue and 
Dr. Taisuke Sato for their providing precious comments and advice on 
numerous occasions. And last, but not least, he would hke to thank his 
wife Yumiko and his sons and daughter Kohki, Hirono, and Hiroto, for 
their patience and assistance. 
Both authors are grateful to Professor Alexey G. Ivakhnenko and his 
son Gregory Ivakhnenko for the invaluable discussions on the multilayer 
and harmonic GMDH algorithms. These discussions reinforced our en-
thusiasm to work in this area. 

Chapter 1 
INTRODUCTION 
Polynomial neural networks (PNN) are global function models whose 
parameters, once learned from given data, can be used for generating 
predictions without the need to infer additional versions for the separate 
neighborhoods in the data. PNN provide opportunities to reach accu-
racy superior to many global models such as nonhnear functions, statis-
tical learning networks, multilayer perceptrons, and other feed-forward 
neural networks on multivariate nonlinear regression, time-series fore-
casting and classification problems. Inferring PNN models from data is 
an inductive computation problem that requires development of corre-
sponding learning algorithms. 
This book develops a methodological framework for induction of mul-
tilayer PNN. It involves several steps: 1) elaboration of neural network 
representations of polynomials; 2) search for the proper network struc-
ture (architecture) using the evolutionary genetic programming para-
digm; 3) adjustment of the polynomial coefficients (referred to further 
as weights) by gradient descent search using backpropagation training 
techniques; 4) enhancement of the network generalization potential us-
ing Bayesian inference; and 5) model validation with diagnostic methods. 
These five steps make a coherent and integrated methodology for iden-
tification of well-performing polynomial models. The rationale is in the 
tight coupling of the second, third and fourth learning steps which sug-
gest to further adapt the evolved polynomial network and its coefficients 
by backpropagation and Bayesian techniques. 
The difficulties in this polynomial learning methodology are what kind 
of representation to choose, how to organize evolutionary search with it, 
whether it can enable neural network training, and whether its predic-
tion can be improved by assumptions about the underlying distribution 

2 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
of the data. What this book emphasizes is that the model representation 
is essential for the development of inductive learning algorithms. This is 
the representation which allows us to employ efficient search methods, to 
derive neural network training algorithms, to tune the coefficient's vari-
ance with respect to the target density, and to perform reliable analysis 
of the results. The key idea is to make tree-structured polynomial net-
works that can be flexibly tailored to the data. When equipped with 
neural network and Bayesian methods for weight training and pruning, 
these polynomial networks become PNN, and thus alternative methods 
for statistical data analysis. 
The PNN are multilayer perceptrons of neuron-like units which pro-
duce high-order polynomials [Barron, 1988, Elder and Brown, 2000, Far-
low, 1984, Gosh and Shin, 1992, Ivakhnenko, 1971, Muller and Lemke, 
2000, Marmarelis and Zhao, 1997, Pao, 1989, Pham and Liu, 1995, Teno-
rio and Lee, 1990, Wray and Green, 1994, Zhang and Mtihlenbein, 1995]. 
Their distinctive advantage is the ability to find optimal higher-order 
term weights. PNN inherit some good features from their predecessors, 
the multilayer perceptron networks, while often showing better accuracy 
of fit and forecasting. Such an important feature is that they assume 
gradient descent training by error backpropagation. 
The multistep inductive learning of polynomials from data became 
possible with the recent progress in biologically inspired computation. 
Search for optimal tree-like topologies can be organized using genetic 
programming. Search in the weight space can be implemented using 
both backpropagation and Bayesian techniques for neural networks. Re-
search in artificial neural networks demonstrates that they have the ca-
pacity to carry out reliable learning despite discrepancies in the data. A 
serious criticism to most connectionist models is that they require us to 
predefine their structure. There are algorithms that construct the net-
work architectures but they do this with an inefficient topological search. 
A better search may be conducted using the evolutionary paradigms, 
such as evolution strategies, genetic algorithms, and genetic program-
ming. They perform global exploration as well as local exploitation of 
the neural network shape spaces, which helps to locate good solutions. 
Most of the neural network strategies, however, assume that the data 
are fixed and produce point predictions. Such predictions are unrealistic 
in practical situations where it is necessary to exploit the data together 
with uncertainties in them. This directs the attention toward investi-
gating Bayesian methods for proper treatment of the data with their 
inherent noise. Also proposed are Bayesian inference algorithms that 
adjust the weights along with their variances, and so enable us to make 
more reliable probabilistic predictions. 

Introduction 
3 
This book integrates the strengths of evolutionary paradigms, artifi-
cial neural networks, and the probabilistic methods for efficient simula-
tion of inductive computation processes. The existing multilayer poly-
nomial networks have a flexible structure that allows adaptation of the 
polynomials to the data. Applying evolutionary algorithms to them 
helps to discover the relevant explanatory variables and their interre-
lationships. Multilayer polynomial networks typically use conventional 
methods to find the values of the weights, such as least squares fitting 
methods. While ordinary least squares fitting methods are optimal for 
linear models, it is not clear at afl whether they are optimal for non-
linear models. The problem of using least squares fitting for learning 
the weights of composed nonfinear models is that they do not guarantee 
reaching sufficiently good accuracy. When nonlinear models are built, 
the relationships between the variables in them are complex and need 
sophisticated treatment. This is the rationale for applying specialized 
connectionist algorithms to polynomial networks. 
Even equipped with such specialized weight training algorithms, the 
polynomial networks assume further adaptation with regard to the un-
derlying assumptions about the noise in the data. During the weight 
training process, the possible noise in the data distribution has to be 
considered so as to tune the model better. This involves updating the 
behef in the amount of noise in order to adjust the belief in the un-
certainty of the weights. Having inferred the degree of certainty in the 
network weights, probabifistic predictions can be made which are more 
reliable as they reflect the characteristics of the data generating function 
with a computable level of confidence. 
1.1 
Inductive Learning 
The problem with inductive learning is to identify a model that op-
timally describes the characteristics of provided data, usually collected 
from observations of natural, real-world phenomena. The necessary tools 
for deafing with such a problem include [Vapnik, 1995, Vapnik, 1998]: 
a generator of example input data, a teacher that associates each input 
with a corresponding desired output, and a learning device that imple-
ments the model. The learning device maps the given input vectors to 
estimated outputs. Since the desired outputs are known, this formula-
tion is commonly named supervised inductive learning. The objective 
is to find a model whose estimated output is maximally close to the de-
sired output. The search for the parameters of the learning device that 
best describes the given data is organized by a machinery for inductive 
computation. Inductive learning is essentially a search problem guided 
by proper criteria for closeness with the data. 

4 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
The data generator draws independently random example (input) vec-
tors X from some probability density Pr(x). The teacher assigns to 
each example a corresponding desired (target) output y according to 
a probability distribution Pr(7/|x). The joint probability density func-
tion of these independent and identically distributed data is: Pr(x, y) = 
Pr(x)Pr(7/|x). Having collected a set of data D = {(xn,2/n)}^=i) the 
goal is to find a mapping P(x) â> y as close as possible to the true 
unknown mapping P*(x). The mapping is parameterized by a set of 
weights w = {wi^W2^ .-OJ that is we have functions of the kind F(x, w). 
Having an estimate of the error only on the available data, the intention 
is to achieve high generalization on modelling unseen data. The learning 
criterion is given by the total generalization error: 
E = I L{y, F(x, w)) Pr(x, y)dxdy 
(1.1) 
where L{y^ P(x, w)) is a loss estimate (a measure of the training error). 
The difficulty in solving this learning problem arises from the unknown 
probability density, but from another point of view this motivates the 
need to construct special inductive computation machinery. This com-
putation machinery should be powerful enough to learn from finite data, 
which is known to be an ill-posed problem that requires apriori assump-
tions about the complexity of the learning device. The learning device 
is typically a function, such as a non-finear function, a polynomial, a 
radial-basis function, a spline function, a neural network function, etc.. 
This is why the learning device is simply called a function or model. 
Function models are suitable for addressing many inductive tasks such 
as density estimation, classification, regression, etc.. 
1.1.1 
Learning and Regression 
Inductive learning can be formulated as a regression task as follows: 
given an example set of input vectors D = {{'^niyn)}n=i ^^^^ ^^^ mea-
surements of explanatory variables x^ = [2^ni5^n2? -â¢â¢i^nd]^ ^ ^ '^^) ^^^ 
corresponding desired values of the dependent variable yn G 7^, the goal 
is to find a function P that describes the mapping ?/ = P(x) + e, where 
e: is a zero mean normally distributed noise and P G I/2, which on aver-
age converges to the true unknown mapping P*(x). The hnear space L2 
contains functions with integrable squares, i.e. is the integral /P^(x)(i(^, 
where ^ is the space metric, it exists and it is finite [Kolmogorov and 
Fomin, 1999]. The function models the conditional mean of the data: 
P{x,w) = I yFv{y\^)dy 
(1.2) 
where Pr(7/|x) is the probabifity density of the given outputs. 

Introduction 
5 
The learning criterion in the EucUdean space is the following quadratic 
empirical loss: 
L(j/,P(x,w)) = (y-P(x,w))2 
(1.3) 
The objective of inductive computation is to discover a model that 
minimizes the risk functional appHed with this empirical loss, which is 
the error on the provided training data. Although the search is guided 
by the empirical loss, the expectation is that the best solution will also 
have low prediction risk, that is low error on unseen data. 
1.1.2 
Polynomial Models 
The presented investigations adopt polynomials as universal function 
models. The rationale for choosing polynomials comes from the Stone-
Weierstrass approximation theorem [Davis, 1975]. It states that polyno-
mials are a universal approximation format with which there could be 
described any continuous function on a compact set. The polynomials 
provide sufficient expressive power for accurate data modelling; in other 
words they are reliable descriptive tools. Polynomial models can be 
built, e.g. like algebraic polynomials, orthogonal polynomials, trigono-
metric polynomials, rational polynomials, local basis polynomials, etc.. 
Such models can be represented as multilayer neural networks in order 
to facilitate the structure selection, the coefficient estimation, and the 
complexity tuning (including term pruning). 
This book offers approaches to design and implementation of com-
putational micromechanisms for inductive learning and specializes them 
for automated discovery of polynomial neural network models. 
1.1.3 
Inductive Computation Machinery 
The development of inductive computation machinery for a chosen 
function model, hke polynomials for example, involves: 1) choosing a 
search paradigm for model selection; 2) elaborating model representa-
tions suitable for manipulation by the operators offered by this para-
digm; 3) organizing search navigation with relevant selection criteria; 
4) designing parameter training algorithms for their fine-tuning to the 
data; 5) implementing probabilistic inference tools for enhancing the 
generalization performance; and 6) carrying out model validation using 
diagnostic methods and tools. 
The Model Search Paradigm. The search paradigm serves to find 
the most adequate model structure for the data. When working with 
polynomials, the objective is to find PNN with complexity relevant to 
the training data in the sense of terms and order. The search paradigm 
should be powerful enough to perform exploration of large, distant ar-

6 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
eas on the search landscape as well as to perform exploitation of small, 
neighborhood areas. Among the different paradigms such as heuristic 
search, simulated annealing, random search and others, the genetic pro-
gramming paradigm is preferred. Genetic programming offers learning 
operators and micromechanisms inspired from biology that have abilities 
to conduct guided stochastic search in large spaces. Genetic program-
ming is a general search paradigm which operates on tree-structured 
model representations. 
The Model Representation. A search paradigm is equipped with 
learning operators to manipulate the concrete structures that implement 
the adopted model representation. The genetic programming evolves 
tree-like model representations that are processed by evolutionary learn-
ing operators. This book develops different polynomials in the form of 
tree-structured networks, including algebraic network polynomials, ker-
nel network polynomials, orthogonal network polynomials, trigonometric 
network polynomials, rational network polynomials, and dynamic net-
work polynomials. The approximation characteristics of the models in 
evolutionary computation are traditionally referred to as their fitness. 
Search Navigation. The model search process may be envisioned as 
flowing on a landscape surface built from the fitnesses of all possible mod-
els of the selected kind. The learning apparatus has to efficiently navi-
gate the search on the landscape in order to examine it thoroughly and 
to locate a good solution. The search is guided by inductive principles 
which tell us how to define model selection criteria. One advantage of 
choosing polynomials is that they enable us to directly apply well-known 
principles for automated induction. Such inductive principles that can 
be built in the learning machinery are the statistical inference principle, 
the Bayesian inference principle, the maximum likelihood principle, the 
minimum description length principle, the structural risk minimization 
principle, and the regularization principle. 
Considered for learning polynomials these principles become criteria 
that helps to find models that trade off between the accuracy and the 
generalization capacity. This is necessary because the provided training 
samples in practice are finite, which requires us to adapt the model well 
to the data. Complex models exhibit high accuracy on the training data 
but they are poor predictors, while simple models may tend to predict 
well but fail to reach a satisfactory training accuracy. 
Weight Training. The inductive learning paradigms usually iden-
tify the model structure by search, and during the search they estimate 
the model parameters. The genetic programming conducts evolutionary 
search for the most relevant polynomial network tree-like structure from 
the data using the least squares fitting method for coefficient estima-

Introduction 
7 
tion. However, there is no guarantee that only parameter estimation 
is sufficient to obtain a good solution with optimal coefficients. This 
book claims that evolved polynomial networks assume further improve-
ment by connectionist training algorithms. Backpropagation techniques 
for gradient descent search are derived especially for high-order neural 
networks with polynomial activation functions. 
Generalization Enhancement. Having a promising network model 
structure is not sufficient in most cases to achieve good generalization. 
The polynomial network model may not exhibit good predictive perfor-
mance if its weights do not reflect the underlying assumptions about the 
noise in the data. There could be normally distributed noise, or heavy 
tail noise, or other noise distributions that affect the model performance 
on unseen data. If the characteristics of the output noise are not taken 
into account in training, the weights will not be able to well capture the 
characteristics of the true data generation process. Probabilistic learn-
ing algorithms are presented and specialized here for PNN training. It is 
shown how the learning algorithm updates the belief in the weights along 
with the arrival of the data with respect to the preliminary assumptions 
about the probability distribution of the data. 
Model Validation. After inductive inference of a promising model 
structure and its parameters, this model has to be validated. It has to 
be measured as to what degree the model is an adequate description 
of the data. The polynomials can be easily validated using standard 
statistical diagnosis tools. In addition to this, when polynomials are 
made as networks they can also be tested using approaches designed 
for neural network and Bayesian diagnosis. Statistical diagnosis can be 
performed by residual sampling methods, which can be used to measure 
confidence and prediction intervals of the polynomial neural networks. 
The possibility to test polynomials with both kinds of methods increases 
the certainty in their approximation characteristics. 
1.2 
Why Polynomial Networks? 
Multilayer polynomial networks are a class of power series function 
models constructed using hierarchical networks of first-order and second-
order processing units. These are higher-order neural networks with mul-
tiplicative activation functions, sparse connectivity, and parsimonious 
structure. There are three conjectures that motivate the representation 
of polynomials as neural networks [Marmarehs, 1994]. First, polynomials 
can be related to multilayer perceptron networks as universal approxi-
mators, so it is worth building polynomials as neural networks in order 
to use the achievements in connectionist learning. 

8 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
Second, the modelling of a function requires, in general, a polynomial 
network of smaller size than the corresponding neural network required 
for the same function. The polynomial networks produce more compact 
and accurate models than multilayer perceptrons. This expressive dif-
ference is mainly due to the restriction of the activation functions of the 
multilayer perceptron to a specific squashing nonlinearity, typically the 
sigmoidal or tangential functions. This leads to increasing the number 
of hidden units necessary to achieve satisfactory performance. 
Third, polynomial networks can be designed with bounded as well as 
unbounded activation polynomials in the hidden network nodes. There 
is a long debate on whether or not to use bounding of the activation 
polynomials through some squashing functions. It is traditionally con-
sidered that unbounded activations will enable faster convergence and 
will also help to reach more accurate results. 
A polynomial can show anomalous behavior when applied to inputs 
outside of its definition domain [Matthews and Moschytz, 1994]. The 
main reason for such a pathological performance is that polynomials 
are extremely sensitive to the inputs. Such problems can be avoided 
by allocating polynomial models onto neural network architectures us-
ing bounded activations. Bounding the activation polynomials through 
squashing functions filters the inputs and diminishes the unexpected 
deteriorating eff"ects from the high-order terms. Using networks with 
bounded activations can improve the usefulness of polynomials, espe-
cially when backpropagation training is applied. The estimation of the 
weights by least squares techniques, however, may lose accuracy [Mar-
marelis and Zhao, 1997, Wray and Green, 1994]. 
Although bounding of the activation polynomials in multilayer PNN 
is not explicitly shown in the presented research, it can be applied with 
minor modifications of the given formulae. 
1.2.1 
Advantages of Polynomial Networks 
Polynomial networks are attractive modelling tools from a theoreti-
cal, as well as from a practical, point of view. Theoretically, they are: 
1) universal approximators with which one may approximate any con-
tinuous function on a compact set to an arbitrary precision if there 
are sufficiently large numbers of terms; 2) mathematically tractable 
since they assume manipulations, like decompositions and reformula-
tions, which make them flexible for structural identification; 3) prob-
abilistically tractable with specific assumptions for normal and heavy 
tail distributions; and 4) statistically tractable as they assume standard 
statistical analysis including testing of their residuals, the covariances 
between the variables, and the sensitivity to the data sampling. 

IntroducfAon 
9 
Practically, the polynomial networks are: 1) computationally tractable, 
as their training often proceeds on unimodal error surfaces that enable 
fast and reliable convergence by well-known algorithms, such as ordi-
nary least squares fitting; 2) open-box transparent models which are 
amenable to easy comprehension and understanding. 
1.2.2 
Multilayer Polynomial Networks 
The subjects of interest here are the multilayer networks with transfer 
(activation) polynomials in the nodes [Barron, 1988, Elder and Pregibon, 
1996, Farlow, 1984, Ivakhnenko, 1971, Muller and Lemke, 2000]. The 
transfer polynomials are selected according to some predefined criteria 
from a pool of candidate basis polynomials, and are cascaded hierar-
chically. The connection scheme suggests that the transfer polynomial 
outcomes feed-forward to their parent nodes where partial models are 
composed of received outcomes from the polynomials below and/or input 
variables. The network output is a high-order polynomial. 
The polynomials are discrete analogs of the Volt err a series, known as 
Kolmogorov-Gabor 'polynomials [Kolmogorov, 1957, Gabor et al., 1961]: 
s 
s s 
s s s 
P(x) = ao-f Y^ aiXi+ YlYl 
^ij^i^j-^ YIYIYI 
^ijk^i^j^k 
+ â¢â¢. (1-4) 
where a^ are the polynomial coefficients or weights, and 
the components of the input vector x = [xi^X2^..]. 
Popular are the multilayer polynomial networks constructed using 
the algorithms from the Group Method of Data Handling (GMDH) 
[Ivakhnenko, 1971, Madala and Ivakhnenko, 1994], which influenced re-
search into similar approaches such as the Polynomial NET work TRain-
ing algorithm (PNETTR) [Barron, 1988] and the Algorithm for Synthe-
sis of Polynomial Networks (ASPN) [Elder and Pregibon, 1996, Elder 
and Brown, 2000]. Being familiar with their characteristics gives ideas 
as to how they work and how they can be improved. 
Group Method of Data Handling. The GMDH pioneered the 
development of network algorithms using polynomials as a concept de-
scription language. These are constructive algorithms that grow net-
works of polynomial node functions layer by layer in a bottom-up man-
ner. In this sense, these algorithms perform hill-climbing search in the 
space of network structures by adding successively to the network nodes 
with smallest output error [Barron, 1988, Elder and Brown, 2000, Far-
low, 1984, Green et al, 1988, Ivakhnenko, 1971, Madala and Ivakhnenko, 
1994, Muller and Lemke, 2000, Ng and Lippmann, 1991, Pham and Liu, 
1995]. As well as synthesizing the network architecture, they also find 

10 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
the weights. An important advantage of GMDH is that it infers com-
plex high-order polynomials by cascading hierarchically simple low-order 
polynomials. This strategy allows us to alleviate to a great degree the 
fundamental learning problem known as the curse of dimensionality. 
Having selected polynomials, one needs to know that the number of 
terms increases exponentially with the number of inputs. When GMDH 
grows polynomial networks vertically from the lowest to the highest layer 
there is no combinatorial explosion of the number of nodes with the in-
crease of the input dimension. For example, a function of maximum 
degree S can be realized by a GMDH network with quadratic transfer 
polynomials using \l0g2S] hidden layers (where [".] is the ceiling func-
tion). This efficacy is due to the original representation which keeps the 
polynomials as compact compositions of simple transfer models. The 
weights that accommodate the higher-order correlations between the 
variables appear when the model is expanded, while during processing 
they are virtual. Whereas the complete multinomial (1.4) of degree S in 
m variables theoretically has (5 + m)\/{S\m.\) terms, the GMDH poly-
nomial network is an unexpanded multinomial that practically contains 
only a small number of these terms. The number of terms is restricted 
by predefining the network width K and depth S parameters. 
The GMDH family contains a wide spectrum of constructive network 
algorithms [Madala and Ivakhnenko, 1994]: multilayer, combinatorial, 
recursive, orthogonal, harmonic, etc.. Most popular among them is the 
multilayer GMDH which iteratively builds strictly layered networks with 
regular topology. Strictly layered means that the network consists of 
nodes whose distance from the inputs at any particular layer is the same. 
Regular means that the outputs of each layer feed directly only the next 
layer nodes. When the polynomial outcomes feed forward their par-
ent nodes, partial models are composed hierarchically from the received 
outcomes. An example GMDH network is shown in Figure 1.1. 
The complete second-order bivariate transfer (activation) polynomial 
is used in every network node: 
p{Xi^ Xj) = WQ -\- WiXi -{- W2Xj -h W^XiXj -f W/\x1 -\- W^x'j 
(1.5) 
where Xi and Xj are variables from the input vector Xi G x, Xj G x, z 7^ j , 
whose indices range up to the preselected dimension 1 < i^j < d. 
The weights of the polynomial p{xi,Xj) (1.5) are estimated so as to 
minimize the sum-squared error: 
N 
SSE =J2iyn~ 
Pi^ni^ ^nj)f 
(1-6) 
n=l 
where the index n in Xni and Xnj enumerates the data. 

Introduction 
11 
Best Polynomial 
P(x) 
/ 
y l 
y2 
y3 
y4 
y5 
Third Layer 
Seamcl Layer 
First Layer 
(J^ Transfer Polynomial 
I I X. Input variable 
Figure 1.1. Polynomial network of width K = b constructed by the multilayer 
GMDH algorithm. The best polynomial model is derived from the leftmost output 
y l which makes the model P ( x ) ~ 
p{p{p{x\,X3)^p{x1,X2)),p{p{x\,X2)^p{x3,X4)))^ 
Th.e minimum of this sum-squared error function is found by ordinary 
least squares (OLS) fitting using the matrix equation: 
w = (*'^*)"^*'^y 
(1.7) 
where w is the column vector of weights w == [WQ, wi^..^ u)m]^ whose 
size is m + 1, ^ is the TV x (m + 1) design matrix of vectors produced 
by the transfer functions apphed to pairs of input variables {x. 
1 :< '^ < ^5 and y is the N x I output column vector. 
The design matrix ^ is made from the data as follows: 
mi 
'^nj J â¢) 
* 
(l)Q{xii,Xij) 
4^i{xii,Xij) 
... 
4^mXxii,Xij) 
(l)id{x2i,X2j) 
^l{x2iiX2j) 
... 
^m{x2i,X2j) 
I MxNiiXNj) 
(j)\{xjs[i,XNj) 
... 
(/^rnXxjsfi.Xjsjj) 
(1.8) 
where (x^, Xj) is the pair of the z-th and the j-th input variables, and 
are basis functions introduced for notational convenience. 

12 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
This design matrix ^ can be assumed as built of row vectors: 
or alternatively it can be represented using a short notation as follows: 
# = {<pi{xni^ ^nj)}^ where 1 < ^ < m, 1 < n < A^", and 1 < ^, j < m. 
Typically, there are considered the following six bivariate basis func-
tions : (t)Q{xi,Xi) = 1, (l)i{xi,Xj) = Xi, (l)2{xi,Xj) = Xj, (j)-^{xi,Xj) = XiXj, 
(j)/^{xi^Xj) = x?, and (t)^{xi^Xj) = xj. Using these basis functions, the 
transfer polynomial (1.5) can be expressed as follows: 
p{xi,Xj) = cj){xi,Xj)w 
(1-10) 
where <p{xi^Xj) is the Ix (m,-\- 1) basis vector. 
Since bivariate transfer polynomials are used, there are only two con-
nections feeding each node. The weights do not correspond exactly to 
the connections, rather one may envision that each connection is mul-
tiphed to feed a particular transfer polynomial term. That is why the 
network connectivity is sparse. The number of nodes in each layer, which 
is the network width, is determined in advance to limit the possibility 
of a combinatorial explosion of terms. This is part of the greedy search 
strategy according to which once a node is added to a layer it remains 
fixed there and cannot be removed anymore. The network width is cho-
sen less than the number of combinations from input variables. At the 
highest layer there are several results which are sorted, and the best 
polynomial is obtained from the leftmost node. 
The multilayer GMDH algorithm (Table 1.1) starts with the genera-
tion of all combinations c = d{d â l)/2 of input variables (x^, Xj), xi G x, 
Xj E X, X = [a:i,X2,..., Xflf]. These combinations of variables are passed 
to enter the transfer polynomials pc {xi^Xj)^ ^ ^ hj ^ ^ â d, in the 
first, lowest network layer. Their weights are estimated by OLS fitting 
(1.7), and the outputs Zc from all K nodes in the first layer are computed 
Zc = Pc (xi^Xj) = <p(xi,Xj)wc' Next, these candidate nodes are ranked 
according to the selection criterion, and only the best 
k<c^l<k<K 
of them are retained in the first layer. 
The outputs of the first layer activation polynomials are passed for-
ward as pairs of intermediate variables {z^ = Pr \ Zg = Ps ) to feed the 
second layer nodes p^'^\zr^Zs) = (j){zr^Zs)^, ^ ^ r^s < K, where the 
subscript (^^ shows the layer number. All c = K{K â l)/2 combinations 
of such variables {z^^Zs)^ ^ 7^ Â«5, are generated and used to estimate the 
transfer polynomials in the second layer, again by OLS fitting (1.7). This 
iterative network growing continues until the error stops to decrease. 

Introduction 
13 
Table 1.1. Summary of the multilayer GMDH algorithm. 
Multilayer G M D H Algorithm 
step 
Algorithmic 
sequence 
1. Initialization 
Data V = {(xn,2/n)}^=ias d-dimensional input vectors x. 
Let the current variables be x,: G x, 1 < i < d, and k = d. 
Let the network width he K, K < c = d{d- 
l)/2. 
Let the layer be {h) = 1, and the error is e = Max Int. 
Let the transfer polynomials be: p{xi,Xj) 
=CJ){xi^Xj)w or 
p(x,:, Xj) = WQ -\- wiXi + W2Xj -\- wsXiXj -f W4x'f H- 1^52:^ 
and also: ^ = {^/(a^ni, a^nj)}, where: 1 < / < m, 
2. Network 
a) Generate all c combinations {xi,Xj), 
1 < i,j < k. 
building and 
b) Make a polynomial pi '\xi,Xj) 
from each combination; 
training 
- estimate the polynomial weights Wc by OLS fitting 
- evaluate the error pi '\xi,Xj) 
=CJ)^{xi,Xj)wc 
MSB = (1/7V) 
Y:L^(y-â¢-P^r''\^--^^-â¢j))^ 
- compute the model selection criterion fc 
= 
func{Sa)' 
c) Order the polynomials according to their fc 
, 
and choose these k < c, 1 < k < K^ with lower values. 
(^,+1) . mm un-
d) Take the lowest error from this layer: e 
e) If the lowest layer criterion is e^^'^^^ > e then terminate; 
else set the overall error e = e^^'^^^ and continue. 
f) The polynomial outputs become current variables: 
Xc = V^c'\ l<c<k 
= K,s^ndc 
= K{K - l)/2. 
g) Repeat network building and training with (h) = {h + 1). 
This GMDH framework is general as it helps to study not only many 
of the algorithms from the family, but it also describes the basics of 
the other popular multilayer polynomial network algorithms such as 
PNETTR [Barron, 1988] and ASPN [Elder and Pregibon, 1996, Elder 
and Brown, 2000]. These algorithms differ mainly in the chosen transfer 
polynomials and in the model selection criteria. They consider similar 
network growing and weight training approaches to construct high-order 
multivariate polynomial models (1.4), 
GMDH Design Issues. There are several GMDH algorithmic de-
cisions which seriously affect its performance. Polynomial networks are 
inferred when addressing inductive tasks where the goal is to learn mod-
els that generalize beyond the provided data sample. The critical re-
quirement of the discovered models is that they should approximate 
the general trends in the data and should not overfit the training data. 
Polynomials that generalize well are accurate and simple, i.e. they have 

14 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
parsimonious structure. In order to avoid overfitting, there are several 
design issues that have to be investigated in order to attain good results: 
1) what kind of transfer polynomials to use; 2) what model selection cri-
teria to choose; 3) what data assimilation strategy to follow; and 4) 
whether complexity tuning can be applied. 
The generalization performance depends on the polynomial structure 
and weights. When the complete quadratic bivariate polynomial is used 
in all nodes, the GMDH networks tend to overfit the data because the 
order of the polynomial rapidly increases, and its curvature attempts 
to pass through all data, thus trying to fit the particularities in them. 
The early experiments suggested to consider linear bivariate polynomi-
als or incomplete bivariate polynomials, which help to produce more 
sparse models [Ivakhnenko, 1971, Madala and Ivakhnenko, 1994]. Some 
elaborated versions even use three-input and cubic transfer polynomials 
[Elder and Brown, 2000]. There are improvements by passing some in-
puts directly to higher layer nodes [Barron, 1988]. Section 2.1 offers a 
set of transfer polynomials for making flexible polynomial networks. 
Using different transfer polynomials requires precise tools for analysis 
of the results, that is there is a need for model selection criteria. The 
aim is to account more precisely for the contribution to the fit of each 
polynomial term which is actually a weighted monomial of variables. 
There exist model criteria that measure not only the approximation er-
ror but also other model features like the magnitudes of the weights, 
the number of the weights when different transfer polynomials are used, 
the degree of smoothness, etc.. These additional features serve as ex-
plicit complexity penalties which help to navigate the search toward 
parsimonious and smooth polynomials with better forecasting poten-
tial. The current research provides various statistical functions [Moody, 
1992, Akaike, 1970, Barron, 1988, Barron and Xiao, 1991, Craven and 
Wahba, 1979], probabilistic functions [Schwartz, 1978, MacKay, 1992a], 
information-theoretic functions [Mallows, 1973, Akaike, 1973, Rissanen, 
1989], risk minimizing functions [Vapnik and Chervonenkis, 1971, Vap-
nik, 1995, Cherkassky et al, 1999], and dynamic functions [DeBoer and 
Hogeweg, 1989], that may serve as model selection criteria. Model se-
lection criteria for polynomial networks are investigated in Section 4.1. 
The original multilayered GMDH uses a model selection formula that 
relies on splitting the training set into two nonoverlapping subsets: a 
training subset which is used for learning of the polynomial weights, and 
a testing subset which is used for model selection. This data splitting 
strategy is reasonable when the mean squared error is taken as a model 
selection criterion. This reduces the risk of overfitting the training ex-
amples. However, when the data are split, not all of the information in 

Introduction 
15 
the training set is used for learning. There are model selection criteria 
available which do not require data spliting to achieve overfitting avoid-
ance. The polynomial networks developed here identify their structure 
and weights from all the data. 
The GMDH-type algorithms do not attempt additional reduction of 
the network complexity to prevent overfitting. Recent empirical investi-
gations show that during network construction and weight training, the 
polynomial term structure can be reduced in order to derive parsimo-
nious models. Several complexity tuning techniques are explored in the 
book, including shrinking by regularization of the weights while learn-
ing and pruning of insignificant weights. The regularization techniques 
not only help to make the polynomials smoother, but also help to avoid 
numerical instabifities in training with imprecise real data. Advanced 
network pruning algorithms that rely on first-order and second-order 
derivatives of the network error with respect to the weights, are given in 
Chapters 6 and 7, while Bayesian pruning is explained in Chapter 8. 
Advantages and Disadvantages of GMDH. The multilayer poly-
nomial networks from the GMDH family influence the contemporary 
artificial neural network algorithms with several advantages. 
First, they offer adaptive network representations that can be tai-
lored to the given task. They enable us to customize the polynomial 
network to the problem domain. The network structures are flexible 
and amenable to topological search. Seeking to discover the most ap-
propriate architecture for the task, these quafities allow us to develop 
network search algorithms not only using hill-climbing mechanisms, but 
also using simulated anneahng, evolutionary search, etc.. 
Second, they pioneered the idea of finding the weights in a single step 
by standard ordinary least squares fitting which eliminates the need 
to search for their values. This overcomes training problems due to 
the inabifity of the learning algorithm to sufliciently identify accurate 
network weights. The success of least squares fitting guarantees learning 
at least locally good weights for the transfer polynomials in the network. 
Third, these polynomial networks feature relatively sparse connectiv-
ity which means that the best discovered networks can be trained fast 
by backpropagation techniques for artificial neural networks. This im-
pHes that if one needs globally optimal weights within the polynomial 
network, backpropagation can be applied to further improve the weights 
previously estimated by least squares fitting. 
A disadvantage of the GMDH-type algorithms is that they carry out 
greedy hill-climbing search for the polynomial network structure, and 
thus only perform exploitation of small neighborhoods from the space of 
possible topologies. Alternative nodes are discarded early when growing 

16 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
the network and they do not participate in the later stages of the learning 
process. These algorithms are not sophisticated enough, and even their 
improvements (for example, by increasing the number of transfer poly-
nomials, and redefinition of the error criterion and complexity tuning) 
are not enough to adapt the network sufficiently well to the data. 
The GMDH polynomial networks can be further improved in three 
ways: 1) by conducting global evolutionary search for the proper model 
architecture; 2) by better coordinating the magnitudes of the weights 
within the model; and 3) by improving the structure through balancing 
the model bias and variance with Bayesian techniques. 
1.3 
Evolutionary Search 
The evolutionary computation provides approaches for doing global 
and local search simultaneously. The main evolutionary paradigms are: 
evolution strategies [Back, 1996, Schwefel, 1995, Back et al., 2000, Eiben 
and Smith, 2003, Yao, 1999], evolutionary programming [Fogel et al., 
1966, Fogel, 1999], genetic algorithms [Holland, 1975, Goldberg, 1989], 
and genetic programming (GP) [Koza, 1992, Koza, 1994, Koza et al., 
1999, Koza et al., 2003, Riolo and Worzel, 2003]. They conduct proba-
bilistic population-based search which is a powerful tool for broad explo-
ration and local exploitation of the model space. The population-based 
strategy is an advantage over other global search algorithms such as sim-
ulated anneahng [Kirkpatrick et al., 1983] and tabu search [Glover, 1989], 
which works with only one hypothesis at a time, and over algorithms for 
local search [Atkeson et al., 1997] that perform only narrow examination 
of the search space. Their stochastic character is an advantage over the 
heuristic Al [Nilsson, 1980] and machine learning algorithms [Mitchell, 
1997, Smirnov, 2001] that also search with one hypothesis. 
The research into learning GMDH polynomial networks using evo-
lutionary computation techniques has already developed: genetic algo-
rithms which produce GMDH networks [Kargupta and Smith, 1991], GP 
systems which learn GMDH networks [Iba and Sato, 1992, Iba and de 
Garis, 1994, Iba et.al, 1996b, Nikolaev and Iba, 2001a], and evolutionary 
systems which discover neural trees [Zhang and Muhlenbein, 1995, Zhang 
et al., 1997]. The genetic algorithms that manipulate GMDH networks 
can be criticized for the inefficient fixed length genome representation 
of the polynomials. The fixed size network implementation restricts the 
topological network manipulations and requires specific operators to pre-
serve the structural relationships in the model. Thus, the fixed length 
network representation limits the possibility to examine the network 
structure space well. 

Introduction 
17 
There are evolutionary systems that evolve sigma-pi neural trees [Zhang 
and Mtihlenbein, 1995, Zhang et al., 1997], which compose polynomials 
from sigmoidal and multipHcative activation functions allocated in their 
nodes. These systems attempt to locate the relevant topology of the 
neural trees by means of genetic programming. During the population-
based search for the polynomial structure, they also conduct a search for 
the weights by a genetic algorithm. This unfortunately leads not only 
to slow computations, but to inefficient weights because of the limited 
capacity of the genetic algorithm to perform numerical search. Another 
problem is that both evolutionary computation techniques depend on too 
many parameters which are difficult to tune. The GMDH-type polyno-
mial networks are preferred so as to facilitate not only global structural 
learning, but also local weight learning. 
The sparse connectivity of the GMDH networks makes them resilient 
and amenable to topological search. The target polynomials produced at 
the outputs exist within the tree-structured network topology. A single 
GMDH polynomial is a binary tree-like network which is suitable for 
tree transformations by classical algorithms. The tree-like polynomial 
networks have irregular topology; they are not strictly layered because 
higher layer nodes may be fed directly with input variables. Such tree-
like polynomial networks are flexible and assume efficient processing by 
genetic learning operators. This enables the search engine to detect and 
discard insignificant terms. Learning by GP allows us to find good tree-
like networks, in the sense of terms and maximal order (degree). The 
first GP system that initiated the research into evolutionary learning of 
GMDH-type polynomial networks is STROGANOFF [Iba et al,1993, Iba 
and de Garis, 1994, Iba et.al, 1996]. 
This reasoning motivates the research into genetic programming with 
PNN whose principles are estabhshed in Chapter 2. The emphasis is 
on design and implementation of various polynomials represented as 
tree-structured networks, including algebraic polynomials, orthogonal 
polynomials, trigonometric polynomials, rational polynomials, local ba-
sis polynomials, and dynamic polynomials. 
1.3.1 
STROGANOFF and its Variants 
The research conducted by one of the authors [Iba et al,1993, Iba 
et.al, 1996] proposed STROGANOFF (Structured Representation On 
Genetic Algorithms for NOn-linear Function Fitting), which integrates 
GP-based adaptive processes with the GMDH process. STROGANOFF 
was very successful in solving system identification problems and lead 
to the development of a whole family of related algorithms. 

18 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
Table 1.2. Algorithmic framework of the STROGANOFF approach to IGP. 
Original STROGANOFF Algorithm 
step 
Algorithmic 
sequence 
1. Initialization 
Initialize a population of tree-like polynomial 
expressions V{T) = [gi{r),g2{r), 
...,gn{r)] 
F(r) = Evaluate{V{T)^\) 
using an MDL function 
and order the population according to F ( T ) . 
2. Perform 
a) Select parents from V{T) 
evolutionary 
V'{r) = Select{V{T), 
F{T),n/2), 
learning 
b) Perform crossover of V'{T) 
V"{T) = 
CTOssTrees{V'{T),K), 
c) Perform mutation of V'{T) 
V"{T) = 
MutateTrees(V'{T),ii). 
d) Execute GMDH to estimate the coefficients 
of the offspring expressions, and next 
compute their fitnesses with the MDL function 
F"{T) 
= Evaluate{V"(T), 
A). 
e) Rank the population according to F(r -f 1) 
^O(T + 1) <gx{T + l) < ... <gn[T + l). 
g) Continue evolutionary learning [step 2) 
until the termination condition is satisfied. 
Table 1.2 presents the original framework of the STROGANOFF ap-
proach. A close look at the STROGANOFF algorithm in Table 1.2 
shows that its algorithmic structure organizes overall traditional GP 
search with one essential different and original step, 2.d, which actually 
carries out a new local hill-climbing procedure for local improvement 
of the evolved individuals. In step 2.d the coefficients (weights) of the 
child trees are recalculated using the strategy offered by the GMDH 
process [Ivakhnenko, 1971]. The application of GMDH here is very effi-
cient because re-calculation of polynomial coefficients is performed only 
on intermediate nodes upon whose descendants crossover or mutation 
operators were applied. Therefore, the computational burden is largely 
reduced as the generations proceed when the crossover and mutation 
points are usually in the lower layers of the processed trees. 
The system STROGANOFF evolves tree-Hke feed-forward networks 
of polynomial activations that yield at the root node a nonlinear output 
function /. The activation functions are simple (e.g. quadratic) poly-
nomials of the two input variables as in GMDH, whose parameters are 
obtained using an ordinary regression technique. Usually these are the 
complete bivariate polynomials of second-order. 

Introduction 
19 
X 3 
X I 
X 
1 
X 2 
Figure 1.2. Crossover operation in STROGANOFF. 
An example of a binary tree generated by STROGANOFF is shown 
in Figure 1.2. For instance, the upper left parent tree Pi can be written 
as a (Lisp) S-expression as follows: 
(NODEl 
(N0DE2 
(N0DE3 (xi) {x2)) 
(^3) 
(^4))) 
where xi, ^2, a^3,2:4 are the input variables. Intermediate nodes represent 
simple polynomial relationships between two descendant nodes. 
Thus each node records the information derived by the equations: 
NODES 
: 
N0DE2 
NODEl 
: Z2 
a^x\ 
a^X2 
ao -\- a\x\ H- 02^2 -f- a2,xiX2 
yl = Co + Ci2;2 H- C2X4 + 032:20:4 -f 04^2 + C5X4 
(1.11) 
(1.12) 
(1.13) 
where z\ and Z2 are intermediate variables, and yj is the generated ap-
proximation of the output. 
The coefficients [ao,ai, â¢ â¢ â¢, C5], also called weights in connectionist 
parlance, of the neural network are obtained by the ordinary least squares 
fitting method (1.7). 

20 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
For instance, the coefficients a^ in equation (1.11) are estimated after 
forming the design matrix (1.8) from the provided training data, which 
(assuming that N data triples (a;i,X2,y) are available) is: 
^\i 
a;22 
Vi 
From these triples, the ^ matrix adopting the complete bivariate acti-
vation polynomial is constructed as follows: 
* 
/ 1 xu 
X21 
a:na;2i 
x\^ 
x\^ \ 
1 
Xi2 
0^22 
^12^22 
^12 
^22 
(1.14) 
V 1 xiAT x^N 
XINX2N AN 
AN J 
which after solving the normal least squares fitting equations produces 
the coefficient vector w (1.7). 
Note that all node coefficients are derived locally. For instance, con-
sider hi^ of N0DE2. When applying the regression analysis to the equa-
tion for node 2 (1.12), these 6^'s are calculated from the values of z\ and 
x^, (i.e. the two lower nodes), not from x^ or yY (i.e. the upper nodes). 
Therefore, the GMDH process in STROGANOFF can be regarded as a 
local-hill chmbing search, in the sense that the coefficients of a node are 
dependent only on its two descendent (lower) nodes. 
STROGANOFF uses a Minimum Description Length (MDL)-based 
fitness function for evaluating the tree structures. This fitness definition 
involves a trade off between certain structural details of the tree, and its 
fitting (or classification) errors: 
MDL fitness â {Tree.Coding-Length) 
+ 
{Exception-Coding-Length), 
(1.15) 
where these components for binary trees are [Tenorio and Lee, 1990]: 
Tree-Coding ^Length = O.bklogN, 
(1.16) 
Exception-Coding-Length 
= O.bNlogSfj^ 
(1-17) 
where N is the number of input-output data pairs, and the error is: 
N 
s 
-^Ei^-^-l' 
(i-is) 
N 
and k is the number of parameters of the tree (e.g. the k-value for the 
tree Pi in Figure 1.2 is 6 -f 6 + 6 = 18 because each internal node has 
six parameters (ag, â¢ â¢ â¢, a^ for N0DE3 
etc.)). 

Introduction 
21 
The advantages of STROGANOFF are summarized as follows: 1) the 
GP search is effectively supplemented with the tuning of node coefficients 
by regression; 2) analog (i.e. polynomial) expressions complement the 
digital (symbolic) semanticspba et.al, 1996b]; 3) MDL-based fitness eval-
uation works well for the tree-like structures in STROGANOFF, which 
controls GP-based tree search; and 4) the GP operation (i.e., crossover 
and mutation) is guided adaptively [Iba et.al, 1996]. 
1.4 
Neural Network Training 
The polynomial network growing algorithms usually learn the weights 
during forward propagation of the inputs, and once the network is built, 
they stop and do not improve the weights further. 
That is why the 
weights are not sufficiently tuned to be in tight mutual interplay within 
the concrete neural network architecture. This inability of constructive 
polynomial networks to discover sufficiently optimal model weights, has 
unfortunately only been studied slightly. 
A remedy to this problem is to apply training approaches for multi-
layer perceptron (MLP) neural networks, i.e. the backpropagation tech-
niques [Werbos, 1974, Rumelhart et al., 1986] that implement gradient 
descent weight search. They have abilities to learn from incomplete and 
imprecise data. They have, however, some weaknesses that are diffi-
cult to avoid: 1) they require us to predefine the network architecture 
in advance, because it is not clear what size net can assimilate a given 
number of examples in order to achieve a valid generalization; and 2) 
the gradient descent search that they perform is susceptible to trapping 
at suboptimal local solutions as it goes to the minima in the nearest 
basin on the error surface without the potential to escape. Both of these 
disadvantages can be addressed using the GP paradigm to search for 
the optimal network topology and weights since they can avoid early 
convergence to inferior local optima. 
Several backpropagation training techniques for polynomial networks 
are developed in Chapters 6 and 7 in the spirit of the feed-forward neural 
networks theory. Gradient descent training rules for higher-order net-
works with polynomial activation functions are derived. This makes it 
possible to elaborate first-order and second-order backpropagation train-
ing algorithms. In addition to this, advanced techniques proposed for 
neural networks, like second-order pruning, are apphed to polynomial 
networks. After that, it is demonstrated how temporal backpropagation 
training of several kinds of networks with polynomial activation func-
tions can be performed. All of these are reasons that allow us to call the 
multilayer polynomial networks equipped with gradient descent training 
rules Polynomial Neural Networks. 

22 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
1.5 
Bayesian Inference 
The recent explorations into the theory of Bayesian inference enabled 
development of second-generation techniques for efficient neural comput-
ing [MacKay, 1992a, Buntine and Weigend, 1991b, Neal, 1996, Bishop, 
1995, Doucet et al., 2001]. These novel techniques are extremely im-
portant for making practical applications where the data are usually 
uncertain to a great degree and contaminated by various kinds of noise. 
Such real data can be robustly processed by Bayesian methods which 
inherently deal with the data distributions, instead of simply with point 
data. The probabilistic methods are reliable because they provide for-
mula that account for the uncertainty in the model parameters. 
The main conceptual benefits of the probabilistic approach to poly-
nomial neural network modelling are: 1) they provide a means for doing 
proper regularization, thus enhancing the generalization; 2) they pro-
vide strategies for automatic determination of the relevant model terms, 
thus contributing to doing model pruning and selection; 3) they provide 
formula for analytical computation of confidence intervals, thus giving 
evidence for the uncertainty of the inferred model. 
The concepts of the Bayesian paradigm are elaborated here for poly-
nomial networks. It is explained that according to the Bayesian theory, 
the network produces not a single point prediction but a probabilistic 
output that may be considered a combination of many outputs generated 
with each of the training inputs. The degree to which an output par-
ticipates in the combination is determined by its posterior probability. 
The posterior probability is updated with the arrival of the data, and its 
evaluation reflects how certain the model becomes with the data. The 
network weight parameters are random variables that are updated with 
the information in the data; they are not simply point estimates. By 
changing the assumptions about the manipulated probability densities, 
we can obtain corresponding training algorithms for the cases of various 
kinds of noise distributions. 
There are two main approaches to Bayesian learning with neural 
networks: the evidence framework [MacKay, 1992a] and the sampling 
network [Neal, 1996]. Algorithms for sparse Bayesian learning, recur-
sive Bayesian learning, and probabilistic gradient-descent training based 
upon the evidence framework are developed here especially for PNN. 
These are algorithms for Bayesian inference with linear models that can 
be applied to both linear and nonlinear PNN, because even the nonlin-
ear PNN are hierarchical compositions of linear submodels (activation 
polynomials) in the hidden network nodes. A Monte Carlo samphng 
algorithm for probabilistic PNN training is also presented. 

Introduction 
23 
1.6 
Statistical Model Validation 
The polynomial neural networks are statistical tools for data analy-
sis. They are nonlinear regression and classification models which may 
be preferred over the traditional statistical and numerical optimization 
algorithms due to their ability for robust inductive learning. Another 
reason to use PNN for nonlinear regression and classification is that 
sometimes it is easier and faster to apply them to practical data without 
the need to analyze the data thoroughly before processing them. 
The applicability of PNN, as well as artificial neural networks, is en-
hanced using statistical validation estimates. Such estimates are neces-
sary to determine the degree of belief in the accuracy and predictability 
of the learned models. Without estimates of the standard error and the 
confidence intervals, it is impossible to judge whether a model is use-
ful in the statistical sense. Estimates for statistical diagnostic of PNN 
models are discussed in Chapter 8. Derivations of confidence intervals 
for PNN are made regarding them as neural networks. These statistical 
estimates are computed using second-order error derivatives which are 
calculated by backpropagation techniques. 
1.7 
Organization of the Book 
This book is organized as follows. Chapter 2 provides the mechanisms 
of inductive GP with PNN models. It elaborates the tree-like PNN rep-
resentation, the linear implementation of PNN trees, the genetic learning 
crossover and mutation operators, as well as an algorithm for measuring 
tree-to-tree distance, and random tree generation algorithms. Chapter 
3 offers the tree-like polynomial network representations. The following 
sections illustrate what kinds of static and dynamic polynomial mod-
els can be made upon tree-like structures. Fitness functions for search 
guidance using contemporary model selection criteria are designed in 
Chapter 4. Next, Chapter 5 designs the mechanisms for evolutionary 
search navigation based on the fitness landscape metaphor. Search per-
formance measures are also defined. 
The backpropagation techniques for high-order PNN models are de-
rived in Chapter 6. We show analytical derivations of the gradient vec-
tor with the first-order error derivatives with respect to the weights, and 
the Hessian matrix with the second-order error derivatives. Sections 
6.2 and 6.3 develop the first-order and second-order backpropagation 
training algorithms for PNN. The popular second-order Conjugate Gra-
dients and Levenberg-Marquardt algorithms are also given. The first-
order and second-order error derivatives are considered in Section 6.4 
to implement algorithms for pruning PNN. Temporal backpropagation 

24 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
techniques, especially for training recurrent PNN, are proposed in Sec-
tion 7. These include dynamic training which can be accomplished using 
the versions of the backpropagation through time and real-time recur-
rent learning algorithms made for polynomial networks in Section 7.2 
and Section 7.3. Second-order temporal backpropagation is explained 
in Section 7.5. Strategies for improved dynamic training are given in 
Section 7.4. Optimization techniques for recurrent polynomial networks 
are briefly studied in the final section, 7.6. 
The techniques for Bayesian polynomial neural network learning are 
developed in Chapter 8. The basics of the Bayesian inference are pro-
vided in Section 8.1 which introduces the notion of Bayesian error func-
tion. Equations for finding weight variance hyperparameters and output 
noise hyperparameters are derived in Section 8.2.1. The hyperparame-
ters are used next in Section 8.2.2 to implement local and global regular-
ization, and in Section 8.2.4 to compute the predictive data distribution. 
Alternative Bayesian network pruning is discussed in Section 8.3. Sec-
tion 8.4 presents an Expectation-Maximization algorithm for training 
PNN models. Section 8.5 gives a recursive Bayesian algorithm for se-
quential training of polynomial networks. At the end of this chapter a 
sampling algorithm for Monte Carlo training of PNN is designed. 
Tools for statistical diagnostics of PNN are investigated in Chapter 9. 
It explains how to measure the standard error by decomposing it into bias 
and variance components. Practical approaches to computing the bias 
and variance estimates of the standard error using residual bootstrap 
sampling are given in Section 9.2. Section 9.4 shows how to compute 
confidence intervals for PNN models. Two approaches are discussed: 
one using second-order error derivatives from the Hessian, and another 
using bootstrap sampling. The following section, 9.5, gives methods for 
calculating prediction intervals for PNN. 
Empirical results from PNN applications to real-world data are pre-
sented in Chapter 10. Section 10.2 shows how to preprocess the data 
before undertaking learning. Tasks from various application areas, such 
as chaotic time-series modeling and financial forecasting, are taken to 
compare the performance of PNN with Linear ARMA models (Section 
10.3), genetically programmed functions (Section 10.4), statistical learn-
ing networks (Section 10.5), multilayer perceptrons (Section 10.6), kernel 
models (Section 10.7) and recurrent networks (Section 10.8). The em-
pirical investigations demonstrate that PNN models evolved by GP and 
improved by backpropagation are successful at solving real-world tasks. 

Chapter 2 
INDUCTIVE GENETIC PROGRAMMING 
Inductive Genetic Programming (IGP) is a specialization of the Ge-
netic Programming (GP) paradigm [Koza, 1992, Koza, 1994, Koza et al., 
1999, Koza et al., 2003, Banzhaf et al., 1998, Langdon and Poli, 2002, Ri-
olo and Worzel, 2003] for inductive learning. The reasons for using this 
specialized term are: 1) inductive learning is a search problem and GP 
is a versatile framework for exploration of large multidimensional search 
spaces; 2) GP provides genetic learning operators for hypothetical model 
sampling that can be tailored to the data; and 3) GP manipulates pro-
gram-like representations which adaptively satisfy the constraints of the 
task. An advantage of inductive GP is that it discovers not only the 
parameters but also the structure and size of the models. 
The basic computational mechanisms of a GP system are inspired by 
those from natural evolution. GP conducts a search with a population 
of models using mutation, crossover and reproduction operators. Like 
in nature, these operators have a probabilistic character. The mutation 
and crossover operators choose at random the model elements that will 
undergo changes, while the reproduction selects random good models 
among the population ehte. Another characteristic of GP is its flexibility 
in the sense that it allows us to easily adjust its ingredients for the 
particular task. It enables us to change the representation, to tune the 
genetic operators, to synthesize proper fitness functions, and to apply 
diff'erent reproduction schemes. 
This chapter demonstrates how to incorporate these mechanisms into 
a basic IGP framework so as to organize efficient stochastic search of 
vast hypothesis spaces. A subject of particular interest is the automatic 
synthesis of tree-structured polynomial networks by IGP. The compu-
tational mechanisms of IGP are general to a great degree and similar 

26 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
to those of traditional GP, so they may be used directly for processing 
binary tree-like models in other systems for inductive learning. If neces-
sary, the components of the presented framework may be easily modified 
because of their simplicity. Special attention is given to making efficient 
mutation and crossover learning operators. Differences between encoded 
and decoded tree implementations of the PNN models are explained. 
2.1 
Polynomial Neural Networks (PNN) 
Polynomial neural networks are a class of feed-forward neural net-
works. They are developed with the intention of overcoming the com-
putational hmitations of the traditional statistical and numerical opti-
mization tools for polynomial identification, which can only practically 
identify the coefficients of relatively low-order terms. The adaptive PNN 
algorithms are able to learn the weights of highly nonlinear models. 
A PNN consists of nodes, or neurons, linked by connections associ-
ated with numeric weights. Each node has a set of incoming connections 
from other nodes and one (or more) outgoing connections to other nodes. 
All nonterminal nodes, including the fringe nodes connected to the in-
puts, are called hidden nodes. The input vector is propagated forward 
through the network. During the forward pass it is weighted by the con-
nection strengths and filtered by the activation functions in the nodes, 
producing an output signal at the root. Thus, the PNN generates a non-
linear real valued mapping P : TZ^' â^ 7^, which taken from the network 
representation, is a high-order 'polynomial model: 
L 
d 
P(x) - ao+;^ a, n ^?' 
(2.1) 
2 = 1 
j - 1 
where a^ are the term coefficients, i ranges up to a preselected maximum 
number of terms L: i < L 5 Xj are the values of the independent variables 
arranged in an input vector x, i.e. j < d numbers; and rji â 0,1,... 
are the powers with which the j-th element Xj participates in the i-th 
term. It is assumed that rji is bounded by a maximum polynomial order 
(degree) s: Yl^-^i'^ji ^ ^ fo^ every i. The polynomial (2.1) is hnear in 
the coefficients ai^ 1 < i < L, and nonhnear in the variables Xj, 1 < 
j < d. It should be noted that equation (2.1) provides a finite format 
for representing the power series expansion (1.1). 
Strictly speaking, a power series contains an infinite number of terms 
that can represent a function exactly. In practice a finite number of them 
is used for achieving the predefined sufficient accuracy. The polynomial 
size is manually fixed by a design decision. 

Inductive Genetic Programming 
27 
2.1.1 
PNN Approaches 
There are various approaches to making PNN models: 1) neural net-
work implementations of discrete Volterra models [Wray and Green, 
1994]; 2) multilayer perceptron networks with a layer of polynomial ac-
tivation functions [Marmarehs and Zhao, 1997]; 3) linear networks of 
polynomial terms [Holden and Rayner, 1992, Liu et al., 1998, Pao, 1989, 
Rayner and Lynch, 1989]; 4) polynomially modelled multilayer percep-
trons [Chen and Manry, 1993]; 5) Pi-Sigma [Gosh and Shin, 1992, Shin 
and Ghosh, 1995] and Sigma-Pi neural networks [Heywood and Noakes, 
1996]; 6) Sigma-Pi neural trees [Zhang and Mtihlenbein, 1995, Zhang et 
al., 1997]; and 6) hierarchical networks of cascaded polynomials [Bar-
ron, 1988, Elder and Brown, 2000, Farlow, 1984, Green et al, 1988, 
Ivakhnenko, 1971, Madala and Ivakhnenko, 1994, Ng and Lippmann, 
1991, Pham and Liu, 1995, Mtlller and Lemke, 2000]. These PNN ap-
proaches are attractive due to their universal approximation abilities 
according to the Stone-Weierstrass theorem [Cotter, 1990], and their 
generalization power measurable by the Vapnik-Chervonenkis (VC) di-
mension [Anthony and Holden, 1994]. 
The differences between the above PNN are in the representational 
and operational aspects of their search mechanisms for identification 
of the relevant terms from the power series expansion, including their 
weights and underlying structure. The main difi'erences concern: 1) what 
is the polynomial network topology and especially what is its connectiv-
ity; 2) which activation polynomials are allocated in the network nodes 
for expressing the model; are they linear, quadratic, or highly nonhn-
ear mappings in one or several variables; 3) what is the weight learning 
technique; 4) whether there are designed algorithms that search for the 
adequate polynomial network structure; and 5) what criteria for evalu-
ation of the data fitting are taken for search control. 
An interesting approach to building discrete Volterra models is to 
assume that the hidden node outputs of a neural network are polynomial 
expansions [Wray and Green, 1994], In this case, the weights of these 
hidden node polynomials can be estimated with a formula derived using 
the Taylor series expansion of the original node output function. Then 
the network weights are obtained by backpropagation training. After 
convergence, the coefficients of the polynomial expansion and the kernels 
of the Volterra model are calculated. 
The multilayer networks with one hidden layer of polynomial acti-
vation functions [Marmarelis and Zhao, 1997] are enhancements of the 
multilayer perceptrons that offer several advantages: they produce more 
compact models in which the number of hidden units increases more 
slowly with the increase of the inputs, and they yield more accurate 

28 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
models. Their common shortcoming is the need to fix in advance the 
network structure. The idea behind Separable Volterra Networks (SVN) 
[Marmarehs and Zhao, 1997] is to directly simulate the Kolmogorov-
Lorentz theorem for building a continuous function by a two-layer archi-
tecture. SVN uses a linear function at the output node and univariate 
polynomials at the hidden layer nodes. The univariate polynomials in 
the hidden layer make the higher-order monomials of the model. SVN 
trains only the weights from the input to the hidden layer. 
The linear networks of polynomial terms [Holden and Rayner, 1992, 
Liu et al., 1998, Pao, 1989, Rayner and Lynch, 1989] increase horizon-
tally a single hidden layer with monomials. The Functional Link Net 
(FLNet) [Pao, 1989] expands a single hidden layer network with heuris-
tically selected high-order functions. These functions are monomials, 
called functional links. Unfortunately this algorithm attempts to make 
power series expansions directly and suffers from a combinatorial explo-
sion of nodes. The Volterra Connectionist Model (VCM) [Holden and 
Rayner, 1992, Rayner and Lynch, 1989] makes linear networks in a sim-
ilar way by extending the input variables using non-linear polynomial 
basis functions. Another similar approach is the Volterra Polynomial 
Basis Function (VPBF) [Liu et al., 1998] network which adds nodes in 
a stepwise manner using an orthogonal least squares algorithm. 
The output of a MLP network can be expressed as a polynomial func-
tion of the inputs [Chen and Manry, 1993]. Such a MLP is trained by 
backpropagation and a matrix model of the node activation functions is 
created, assuming that a node output is a finite degree polynomial basis 
function. The coefficients of these alternative polynomial basis func-
tions, which simulate the hidden node outputs, are estimated by mean 
square error minimization. This approach yields transparent polynomi-
als however it suffers from the same disadvantages as the MLP, so it may 
be regarded simply as a model extraction technique. 
The Pi-Sigma Networks [Gosh and Shin, 1992] have a hidden layer 
of linear summing units and a product unit in the output layer. Such 
a network produces a high-order polynomial whose maximal degree de-
pends on the size of the hidden layer. The efficacy of PSN is that it 
requires us to train only the weights to the hidden layer as there are 
no weights from the hidden nodes to the output node. PSN networks 
have a smaller number of weights compared to the multilayer perceptron 
networks. The problem is that a single PSN is not a universal approxi-
mator. Ridge Polynomial Networks (RPN) [Shin and Ghosh, 1995] offer 
a remedy to this problem, providing enhanced approximation abihties. 
The RPN is a linear combination of PSN using ridge polynomials which 
can represent any multivariate polynomial. 

Inductive Genetic Programming 
29 
Sigma-Pi neural networks [Heywood and Noakes, 1996] are a kind 
of MLP networks with summation and multiplicative units that are 
trained with a backpropagation algorithm for higher-order networks. 
The Sigma-Pi networks have polynomials as net functions in the summa-
tion units which are passed through sigmoids to feed-forward the nodes 
in the next layer. These networks usually implement only a subset from 
the possible monomials so as to avoid the curse of dimensionality. Their 
distinguishing characteristics are the dynamic weight pruning of redun-
dant units while the network undergoes training, and the use of different 
learning rates for each monomial in the update rule. 
Sigma-Pi Neural Trees (SPNT) [Zhang and Muhlenbein, 1995, Zhang 
et al., 1997] are high-order polynomial networks of summation and mul-
tiphcation units. The sigma units perform weighted summation of the 
signals from the lower feeding nodes, and the product units multiply the 
weighted incoming signals. The neural trees may have an arbitrary but 
predefined number of incoming connections, and also an arbitrary but 
predetermined tree depth. SPNT provides the idea to construct irreg-
ular polynomial network structures of sigma and product units, which 
can be reused and maintained in a memory efficient sparse architecture. 
A disadvantage of this approach is that it searches for the weights by a 
genetic algorithm which makes its operation slow and inaccurate. 
The multilayer GMDH polynomial networks [Barron, 1988, Elder and 
Brown, 2000, Farlow, 1984, Green et al, 1988, Ivakhnenko, 1971, Madala 
and Ivakhnenko, 1994, Mtiller and Lemke, 2000, Ng and Lippmann, 
1991, Pham and Liu, 1995] are more suitable than the other networks 
for intensive evolutionary search. First, taken separately a polynomial is 
simply a binary tree structure which is easy to manipulate by IGP. Sec-
ond, GMDH offers the opportunity to learn the weights rapidly by least 
squares fitting at each node. However, such weights are locally optimal 
and admit further coordination by additional training with gradient-
descent and probabilistic tuning with Bayesian techniques. 
2.1.2 
Tree-structured PNN 
The models evolved by IGP are genetic programs. IGP breeds a pop-
ulation V of genetic programs Q ^ V. The notion of a genetic program 
means that this is a sequence of instructions for computing an input-
output mapping. The main approaches to encoding genetic programs 
are: 1) tree structures [Koza, 1992]; 2) linear arrays [Banzhaf et al,, 
1998]; and 3) graphs [Teller and Veloso, 1996]. The tree-like genetic 
programs originate from the expressions in functional programming lan-
guages where an expression is arranged as a tree of elementary functions 
in its nodes and variables in its leaves. The linear genetic programs are 

30 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
linear arrays of instructions, which can be written in terms of a program-
ming language or written in machine code. The graph-based programs 
are made as directed graphs with stacks for their processing and mem-
ory for the variables. The edges in the graph determine the sequence 
for execution of the programs. Each node contains the function to be 
performed and a pointer to the next instruction. 
Tree-like genetic programs are suitable for IGP as they offer two ad-
vantages: 1) they have parsimonious topology with sparse connectivity 
between the nodes, and 2) they enable efhcient processing with classical 
algorithms. Subjects of particular interest here are the linear genetic 
program trees that are genotypic encodings of PNN phenotypes which 
exhibit certain input-output behaviors. 
The Tree-like Representation. A genetic program has a tree struc-
ture. In it a node is below another node if the other node lies on the 
path from the root to this node. The nodes below a particular node are 
a subtree. Every node has a parent above it and children nodes under 
it. Nodes without children are leaves or terminals. The nodes that have 
children are nonterminals or functional nodes. 
PNN are represented with binary trees in which every internal func-
tional node has a left child and a right child. A binary tree with Z 
functional nodes has Z -f-1 terminals. The nodes are arranged in multi-
ple levels, also called layers. The level of a particular node is one plus 
the level of its parent, assuming that the root level is zero. The depths 
or height of a tree, is the maximal level among the levels of its nodes. 
A tree may be limited by a maximum tree depth or by a maximum tree 
size, which is the number of all nodes and leaves. 
Trees are described here formally to facilitate their understanding. 
Let V be a vertex set from functional nodes T and terminal leaves T 
(V = ^ U T). A genetic program Q is an ordered tree SQ = ^, in which 
the sons of each node V are ordered, with the following properties: 
- it has a distinguishing parent p{s()) â Vo, called the root node; 
- its nodes are labelled v :V â^ JV from left to right and z^(V^) = i; 
- any functional node has a number of children, called arity /^ : V â> A^, 
and a terminal leaf p{si) = Ti has zero arity K{%) = 0; 
- the children of a node V?, with arity k = /^(V^), are roots of disjoint 
subtrees 5^1,5^2, â¢â¢â¢, 5^/.- A subtree Si has a root p{si) = V?, and subtrees 
sn,...,5i/c at its/c children: 5^ = {(V^, s^, 5^2, â¢â¢., s^/,) \k = K{Vi)}. 
This vertex labeling suggests that the subtrees below a node V^ are 
ordered from left to right as the leftmost child sn has smallest label 
u{sii) < iy{si2) < ... < J^{sik)' This ordering of the nodes is necessary for 
making efficient tree implementations, as well as for the design of proper 
genetic learning operators for manipulation of tree structures. 

Inductive 
Genetic 
Program,m,ing 
31 
The construction of binary tree-like PNN requires us to instantiate 
its parameters. The terminal set includes the explanatory input vari-
ables T = {a:i,X2, ...,XQ!}, where d is the input dimension. The func-
tion set contains the activation polynomials in the tree nodes J^ = 
{pi,P2) "nPw}) where the number m of distinct functional nodes is given 
in advance. A reasonable choice are the incomplete bivariate polynomi-
als up to second-order that can be derived from the complete one (1.2) 
assuming that some of its coefficients are zero. The total number of 
such incomplete polynomials is 25 from all 2^ â 1 possible combinations 
of monomials Wihi{xi,Xj)^ 1 < ^ < 5, having always the leading constant 
ifo, and two different variables. A subset Pi G ^ , 1 < i < 16 of them is 
taken after elimination of the symmetric polynomials (Table 2.1). 
Table 2.1. Activation polynomials for genetic programming of PNN. 
1. 
2. 
3. 
4. 
5. 
6. 
7. 
8. 
9. 
10. 
11. 
12. 
13. 
14. 
15. 
16. 
Pl(Xi,Xj 
P2{Xi,Xj 
P3{Xi,Xj 
P4\XiJ Xj 
Pb{Xi,Xj 
P6\Xi 1 Xj 
pj{Xi,Xj 
P8{Xi,Xj 
PQ{Xi,Xj 
) = wo -\- WiXi 4- W2X2 + W3XJX2 
) =z Wo -\- W\Xi -f- W2X2 
) ~ Wo + W\X\ + W2X\X2 
) = Wo + W\X\ -f W2X1X2 + W3X\ 
) ~ W0 + W-iX-i -h W2X2 
) ~ wo + w\Xi -f- W2X2 + wsx'i 
) = Wo + W-iXi + W2X'^ -f- W3X2 
) = Wo -\- w-ix'i + W2X2 
) ~ Wo + W\Xi + W2X2 + W3X}X2 + W4x'i + W^X^ 
Plo(Xi, Xj) â Wo -h W\X-i -f W2X2 + W3X1X2 -f W4x'i 
Pll{Xi, Xj) = WQ + WiXi -{- W2X1X2 + Wsx'i -f W4X'2 
P\2{xi,Xj) 
â Wo -\- W\XiX2 -f W2xi + Wsx'i 
Pl3{Xi,Xj) 
= Wo -\-W^X) +W2X1X2 + W3X2 
pi4{xi, 
Xj) â Wo + W-[X-[ + W2X2 + W3xi + W4x'2 
Pl5{Xi,Xj) 
= Wo + W1X1X2 
pie{xi,Xj) 
= wo-\- w^xiX2 
-f- W2x'i 
The notion of activation 'polynomials is considered in the context of 
PNN instead of transfer polynomials to emphasize that they are used to 
derive backpropagation network training algorithms (Chapter 6). 
The motivations for using all distinctive complete and incomplete 
(first-order and second-order) bivariate activation polynomials in the 
network nodes are: 1) having a set of polynomials enables better iden-
tification of the interactions between the input variables; 2) when com-
posed, higher-order polynomials rapidly increase the order of the overall 
model, which causes overfitting even with small trees; 3) first-order and 
second-order polynomials are fast to process; and 4) they define a search 
space of reasonable dimensionality for the GP to explore. The problem 
of using only the complete second-order bivariate polynomial (1.2) is 
that the weights of the superfluous terms do not become zero after least 
squares fitting, which is an obstacle for achieving good generalization. 

32 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
P(x) 
p8 
p4 
Â©
Activa 
Activation Polynomial 
Function Node ) 
/7,(x)=W(^+w,;c7+W2X^ 
I [ Input vaiiable 
( Terminal Leaf ) 
Figure 2.1. Tree-structured representation of a PNN. 
Figure 2.2. illustrates a hierarchically composed polynomial extracted 
from the PNN in Figure 2.1 to demonstrate the transparency and easy 
interpret ability of the obtained model. 
Hierarchically Composed 
Polynomial 
(( wO + wl * z7'^2 + w2 * z4'^2 ) 
z7=( wO + wl * x2 + w2 * x2'^2 + w3 * x3'^2 ) 
x2 
x3 ) 
z4=( wO + wl * z2 + w2 * z2 * xl + w3 * z2'^2 ) 
z2=( wO + wl * x7 â¢+â¢ w2 * x5 ) 
x7 
x5 ) 
xl )) 
Figure 2.2. Polynomial extracted from the tree-structured PNN in Figure 2.1. 
The accommodation of a set of complete and incomplete activation 
polynomials in the network nodes makes the models versatile for adap-
tive search, while keeping the neural network architecture relatively com-
pact. Using a set of activation polynomials does not increase the com-
putational demands for performing genetic programming. The benefit 
of having a set of activation polynomials is of enhancing the expressive 
power of this kind of PNN representation. 
An example of a tree-structured polynomial using some of these ac-
tivation polynomials is illustrated in Figure 2.1. The computed poly-
nomial P(x) at the output tree root is the multivariate composition: 
P{xi,X2, 
X3, Xs, X7) = P8{P7{X2^ X3),^4(^2(^7, ^ s ) , a:i)). 

Inductive Genetic Programming 
33 
The Search Space of Binary Trees. The number of terminals, 
instantiated by the input variables, and the number of functional nodes, 
instantiated by the activation polynomials, is important for the evolu-
tionary learning of PNN because they determine the search space size 
and, thus, influence the probability for finding good solutions. When 
conducting evolutionary search by IGP, the size of the search space of 
tree-structures may be controlled by varying the number of activation 
polynomials or by the number of input variables. 
Let the tree nodes be labelled by JT, which is the number of the 
hidden and fringe nodes. Let the tree leaves be labelled by T, which is 
the number of the input variables that are passed through the leaves. 
Then, the number of diff'erent binary trees up to a predefined depth S 
is obtained by the following recursive formula [Ebner, 1999]: 
Trees{0) 
= 
T 
Trees{S) 
= J".Trees'^{S - 1) ^T 
(2.2) 
where Trees{S) is the number of binary trees of depth up to S. 
The tree depth impacts the search space size, but it should be noted 
that it also aff'ects the convergence properties of polynomials. The max-
imal tree depth may be defined as a logarithmic function of the maxi-
mal order so as to restrict the network size: 5 â 1 = 
log2{MaxOrder). 
The most probable maximal polynomial order MaxOrder may be deter-
mined in advance and considered as a tree depth limit to constrain the 
IGP search space. The MaxOrder can be found by increasing the order 
of a randomly generated PNN, and measuring its error on the concrete 
sample: a^ = Yln=i iVn ~~ ^(^n))^/(A^ â M^ â 1), where W is the number 
of the model weights. The denominator makes the error increase if the 
model becomes larger than the most probable degree. 
Linear Implementation of P N N Trees. The design of tree- struc-
tured polynomial networks involves three issues: 1) how to represent the 
tree nodes; 2) how to represent the topological connections between the 
nodes; and 3) how to perform tree traversal. Taking these issues into 
consideration is crucial with respect to memory and time efficiency, as 
they impact the design of IGP systems. 
From an implementation point of view the topology of a PNN tree 
can be stored as: a pointer-based tree, a linear tree in prefix notation, 
or a Unear tree in postfix notation [Keith and Martin, 1994]. Pointer-
based trees are such structures in which every node contains pointers 
to its children or inputs. Such pointer-based trees are easy to develop 
and manipulate; for example a binary tree can be traversed using double 
recursion. The problem of pointer-based trees is that the connections 
between the nodes are directly represented by pointers which incurs a 

34 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
lot of processing time. The reason is that most of the contemporary 
programming language implementations make these pointers to address 
dynamic memory locations, and their reference in run time is time con-
suming. The operating systems usually arrange the dynamic memory 
in different data segments which increases the time overhead for fetch-
ing data. That is why the speed of manipulating pointer-based tree 
structures may be several times slower than Unearized trees. 
Linear trees encapsulate nodes in arrays where the parent nodes pre-
cede the children nodes [Banzhaf et al., 1998]. Tree traversal of such 
hnearized trees is made using stacks or recursion. The nodes in a lin-
ear tree can be allocated using prefix, infix or postfix notation. Most 
suitable for GP are the prefix and postfix notations. The prefix nota-
tion arranges the nodes as sequences that are traversed in the following 
order: parent, left subtree, and right subtree. Linear trees in prefix no-
tation are preferred for GP because they enable us to make fast genetic 
learning operators that operate on array representations. Here, prefix 
trees for encoding PNN models are adopted in correspondence to the 
chosen labeling. The prefix trees are evaluated by recursive preorder 
tree traversal. The recursive preorder tree traversal visits a node, and 
then visits recursively its left and right subtrees. 
Linearized Tree Genotypes. The tree implementation of PNN in-
fiuences the development of IGP, and thus impacts the structural search 
process. This is because the implementation determines which neighbor-
ing trees can be sampled, that is it determines the search directions. The 
hnearized tree-structures are genotypic representations of the PNN mod-
els, and they do not directly afi'ect their phenotypic characteristics. One 
may envision that each variable length linear genotype has a correspond-
ing PNN phenotype. The trees are means for representing the structures 
of PNN and they determine the sampling of polynomial models. How-
ever, they also indirectly infiuence their approximation qualities. 
The linearized tree implementation can be described in biological 
terms in order to explain how the development of computational IGP 
mechanisms reflects knowledge about natural evolution. IGP relies on 
notions from biology, but they are only loose associations with the origi-
nals because its mechanisms only loosely simulate their biological coun-
terparts in natural organisms. 
The genotypic encoding of a genetic program is called a genome. The 
genome is a kind of a linear array of genes and has a variable length. In 
the case of IGP, the genome is a hnearly implemented tree. The genes 
in the genome are labelled by loci. The position of each gene within 
the genome is its locus, A locus actually corresponds to the node label 
^(Vi), u ; V -^ J\f oi the particular tree node V^. The value of the 

Inductive Genetic Programming 
35 
node Vi, which could be either an activation polynomial function jF or 
a terminal T, is called an allele. The alleles of the functions are in the 
range [l,m] when jF = {pi,P2) â¢â¢â¢^Pm}) while the alleles of terminals are 
in the range [l,d] when T = {xi,a:2,..., XQ;}. In the case of a functional 
node, the gene carries the kind of the activation polynomial in its allele 
and its weights. In the case of a terminal node, the allele is the index for 
accessing the corresponding input variable from the example database. 
The tree traversal algorithm examines the tree by visiting its loci and 
fetches its alleles when necessary to evaluate the fitness. 
2.2 
IGP Search Mechanisms 
IGP performs evolutionary search that explores global as well as local 
search regions. The power of IGP is due to several characteristics that 
distinguish it from the traditional search algorithms: 1) it manipulates 
the encoded implementation of the genetic program-like models rather 
than the individual models directly; that is the search is conducted in the 
genotype space; 2) it performs search simultaneously with a population 
of candidate solutions; and 3) the search is probabihstic and guided 
by the fitnesses of the individuals. The stochastic nature of IGP is a 
consequence of the nondeterministic changes of the population and the 
randomized selection of promising individuals. 
IGP is intentionally developed to have characteristics that loosely 
mimic the principles of natural evolution. The key idea is to make 
computational mechanisms that operate like the corresponding biolog-
ical mechanisms in nature. The ultimate goal behind the inspiration 
from nature is to pursue the powerful learning ability of the evolution 
process. Simulation of the natural evolutionary processes is organized 
at three levels: in the genotype space of hnear tree representations, in 
the phenotype space of tree-structured PNN models, and in the fitness 
space which is the fitness landscape of PNN fitness values. The ge-
netic learning operators act on the linear tree genotypes. Thus, genetic 
program-like PNN phenotypes are sampled. These PNN phenotypes are 
distinguished by their properties, e.g. the fitness. The computational 
IGP system examines the genotype search space with the intention of 
finding phenotypic solutions with desired fitnesses. 
The evolutionary IGP search has two aspects: navigation^ carried by 
the genetic sampling and selection operators, and landscape^ determined 
by the fitness function and the variable length representation. There 
are two main genetic sampling operators: recombination, also called 
crossover, and mutation. They sample polynomials by probabilistically 
modifying their trees. The selection operator directs the search by ran-
domly choosing and promoting efite individuals having high fitness. The 

36 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
search navigation moves the population on a landscape surface built of 
the genetic program fitnesses. The sampling and selection operators 
should push the population on the fitness landscape. The IGP mecha-
nisms together should have the capacity to guide the population toward 
very deep landscape basins of good solutions. 
2.2.1 
Sampling and Control Issues 
A critical problem in evolutionary IGP is the enormous dimensionality 
of the search space. In order to organize an efficient search process, the 
above two issues should be carefully analyzed. 
The first issue is to make such mutation and crossover operators that 
can potentially visit every landscape region. These are also called learn-
ing operators because they sample individuals and thus contribute to 
finding the model structure. The learning operators for tree structures 
should be general and should not restrict the representation. These op-
erators should avoid genetic program tree growth, known as bloating 
phenomenon, which worsens the IGP performance [Koza, 1992, Banzhaf 
et al., 1998, Langdon and Poh, 2002]. This is difficult to achieve be-
cause the tree growth usually impHes improvement in fitness. Operators 
that allow tree bloat are unable to push the population progressively to 
promising landscape areas and cause search stagnation. 
The second issue is that the population ffow on the fitness landscape 
strongly depends on how the landscape has been created. The design 
of the fitness function can tune the landscape and mitigate the search 
difficulties. Fitness functions, fitness landscapes and their measures are 
investigated in separate chapters (Chapters 4 and 5). 
A distinguishing feature of IGP is that its search mechanisms are 
mutually coordinated so as to avoid degenerated search. The size of 
the genetic programs serves as a common coordinating parameter. Size-
dependant crossover, size-dependant mutation, and selection operators 
(that also depend on the tree size through their fitnesses) are designed. 
Making a common size biasing of the sampling operators and the fitness 
helps to achieve continuously improving behavior. 
2.2.2 
Biological Interpretation 
The development of IGP systems follows the principles of natural evo-
lution [Fogel, 1995, Paton, 1997]. Natural evolution acts on individuals 
having varying hereditary traits causing survival and reproduction of the 
fittest. Evolution is a term denoting changes of a population of individ-
uals, called chromosomes, during successive generations. A chromosome 
plays the function of a genome and is a sequence of genes. The chromo-

Inductive Genetic Prograwwing 
37 
somes have different lengths and shapes as they have different portions 
of traits. The hereditary traits are carried by the genes. The genes can 
be separated from the traits that they specify using the notion of a geno-
type. Genotype means the genes in an individual chromosome structure. 
The observable traits of an individual are referred to using the notion of 
a phenotype. Thus, the notions of a genotype and phenotype serve to 
make a distinction between genes and the traits that they carry. A gene 
can have different molecular forms that indicate different information 
about the traits called alleles. 
Evolution keeps the most common alleles in the population and dis-
cards the less common alleles. The evolution involves updating the allele 
frequencies through the generations. The alleles in the population un-
dergo modifications by several mechanisms: natural selection, crossover, 
and mutation. Natural selection reproduces individuals; it exchanges 
genetic material between the individuals in two populations during suc-
cessive generations. Natural selection may have different effects on the 
population: 1) stabihzation occurs when it maintains the existing range 
of alleles in the population, that is when it retains the individuals that 
contain the most common alleles; 2) shifting happens when it promotes 
individuals whose range of alleles changes in certain directions; and 3) 
disruption results when it deliberately favors concrete traits while de-
stroying others. The crossover shuffles individuals, while the mutation 
changes the alleles. 
Individuals in nature compete to survive during the generations. The 
successful individuals produce increasing numbers of offspring, while the 
unsuccessful ones produce less or no offspring. Natural selection picks 
more and more fit individuals, which when reproduced and mutated, 
lead to even better descendants. The adaptation of the individuals is 
not perfect. Their continuous evolution is driven probabifistically with 
respect to their fitness by selection. The population is in permanent 
movement along the generations. 
Individuals are simulated in IGP by the genetic programs. In our 
case, these are the tree-structured PNNs. Each individual is associated 
with a fitness measure of its potential to survive in the population. The 
fitness of an individual accounts for its abifity to reproduce, mutate, and 
so to direct the search. There is a fitness function that maps a genetic 
program into its fitness value. The most essential property of a genetic 
program is its fitness. The fitness helps to computationally simulate the 
biological principle of survival of the fittest. The biologically inspired 
idea is to promote, with higher probability, the fitter PNNs. In this way, 
the population moves toward promising areas in the search space seeking 
to locate the globally best solution. 

38 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
2.3 
Genetic Learning Operators 
When a tree is modified, neighboring trees are sampled. The two 
main modification operators for genetic learning have diff'erent roles: 
the mutation performs local search in the vicinity of the parent tree, 
while the crossover conducts global search of distant search space areas. 
Both learning operators should be considered in IGP so as to achieve 
exploration and exploitation of the tree search space. Good solutions can 
be found only if the genetic learning operators are properly developed 
so as to sample every structurally possible tree, because every tree is 
hypothetically likely to be a solution of the task. 
The operators for linear trees have to meet several criteria in order to 
keep the structural consistency of the genetic programs: 1) they should 
preserve the parent-child relationships among the vertices; and 2) they 
should not change the prefix ordering v{si) < ^{sj) between the vertices 
after applying a series of operators u{M{si)) < v{M[sj)). 
The operator has to sample only the closest neighbors of the selected 
node in order to attain high correlation between the parent and the 
offspring. This requires keeping of the inclusion property between the 
corresponding subtrees from the parent Si and the off'spring Sj trees: 
Sj C M{si) or Si C M{sj). Tree transformations that satisfy these three 
topological properties of the trees may be expected to improve the search 
if properly coordinated with the other IGP mechanisms. 
2.3.1 
Context-preserving Mutation 
Mutation is a genetic learning operator that modifies one parent tree 
into one offspring. The mutation operator is considered efficient when 
it causes slight changes of the fitness after transforming a tree. In order 
to facilitate the evolutionary search process there should be maintained 
high correlation between the fitness of the parent and that of the off-
spring. Such a relation is called strong causality [Rosea and Ballard, 
1995a, Igel, 1998, Sendhoff et al., 1997]. Having strong causafity ensures 
continuous progress in evolutionary search. 
The context-preserving mutation (CPM) operator is a means for orga-
nizing local search. This mutation edits a tree structure subject to three 
restrictions: 1) maintaining the approximate topology of the genetic 
program tree by keeping the representation relationships among the tree 
vertices; 2) preserving the inclusion property between the subtrees; and 
3) affecting only the nearest tree vertices to the chosen mutation point. 
The genetic program trees shrink and grow slightly, which contributes 
to the overall improvement of the evolutionary search. 

Inductive Genetic Programming 
39 
A genetic program with a tree-like structure Q of vertices Vi, each 
of which has below a subtree Si = {(V^, s^i, s^2,..., s^/.) | k = i^{Vi)}, 
can be transformed by the following three elementary context-preserving 
mutations M '.g xV xV -^g\ 
M(5, V^, V|) = 5': 
- insert Mj\ adds a subtree 5^ = {(V-,s^|,7^'2'â¢â¢â¢''^/) I ^ = /^(V-)}, so 
that the old subtree si becomes a leftmost child of the new subtree at 
the new node V[^ i.e. 5^^ â ^i'-> 
- delete Mjj: moves up the only subtree 5^ = {{^i^s^-^, 'â¢â¢^s[j)\ 1 < 
I < i^i^i)} of Si iff 3J^ij â p{sij), for some 1 < j < /^(V^) to become 
root J^l = J^ij^ and all other leaf children V/c, ik ^ j , p{sif^) â Tij^,^ of the 
old Vi are pruned. This deletion is applicable only when the node to be 
removed has one child subtree, which is promoted up; 
- substitute Ms'- replaces a leaf % =â¢ p{si)^ by another one T/^ or 
a functional J^i = p{si) by J^^. If the arity /^(^/) = /c, then s'^ â 
{(^/, Sii,...^ Si]^) I k = i^{J^i)}- When /^(^/) = I only / = A: Â± 1 is consid-
ered. In case / = /c -f 1 it adds a leaf 5^ = {(J^/,5ii,..., 5^/^,^/)} else in 
case / = A: â 1 it cuts 5^ = {{J^l^sn, ...,s^/)}. 
There are various mutation operators, but many of them do not help 
to achieve progressive search. Many mutation operators augment the 
tree or trim it by whole subtrees, such as the hierarchical variable length 
mutation (HVLM) [O'Reilly, 1995]. HVLM inserts a randomly generated 
subtree before a randomly chosen vertex, deletes a randomly chosen node 
so that its largest subtree is promoted to replace it, and substitutes 
a randomly chosen node by another one with the same arity. HVLM 
generates quite different offspring models from their parents. 
A useful idea is to develop a uniform replacement mutation (URM) 
operator. The URM operator successively traverses the tree nodes and 
changes every visited node or leaf with a probabilistically selected cor-
responding node or leaf. This operator allows us to make large steps in 
the search space because it makes larger tree changes than CPM. The 
difference between crossover and URM is that the latter is applied to a 
single tree and does not transfer material from another parent, rather it 
randomly updates the parent so as to produce an offspring. 
Alternatively to the above transformation there could be made a re-
placement mutation (RM) which substitutes with a predefined probabil-
ity each allele by a different, randomly chosen allele. While traversing 
the tree each functional node has the chance to be exchanged with an-
other node, and each terminal leaf has the chance to be exchanged with 
another leaf. This RM operator may be applied after doing crossover so 
as to sustain the useful diversity of good alleles in the population. 
Figure 2.3 displays an application of the context-preserving mutation 
operator to a concrete tree. 

40 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
Insert 
Delete 
Substitute 
\ Q) 
i ^9 
\ 
fp3 
1 "^^ 
i G) 
^' 
^ 
\ 
(p3) 
^7 
@ 
^5 
3 
^ 5 
"^B 
^6 
Figure 2.3. Context-preserving mutation (CPM) for tree-structured models. 
2.3.2 
Crossover Operator 
The crossover operator should recombine node material by cutting or 
splicing two parent trees. Material exchange is made by selecting a cut 
point node in each tree, and swapping the subtrees rooted in the cut 
point nodes. The offspring trees may become larger than their parents. 
When a tree is of short size, it is not cut but added as a whole subtree 
at the crossover point in the other tree. If the two trees are short, they 
are spliced together. This crossover by cut and splice prevents the trees 
from rapid shrinking in the case of minimizing fitness functions. The 
recombination is restricted by a maximum tree size. This is a non-
homologous crossover which does not preserve the topological positions 
of the swapped subtrees. Figure 2.4 illustrates an application of the 
crossover operator to two arbitrary trees. 

Inductive 
Genetic 
Program,ming 
parent 1 
41 
parent 2 
cut point 
offspring 1 
offspring 2 
Figure 2.4. Crossover by cut and splice for tree-structured PNN models. 
2.3.3 
Size-biasing of the Genetic Operators 
IGP conducts progressive search when the genetic learning operators 
climb easily on the fitness landscape. One aspect of the search control 
is to coordinate the fitness function and the genetic operators with a 
common bias parameter. The convergence theory of IGP (Section 2.6) 
suggests to use tree size biasing. The rationale is that small tree-like 
PNN usually do not fit the data well enough. Larger tree-like PNN 
exhibit better accuracy but do not necessarily generalize well on future 
data. While the fitness function only evaluates the genetic programs 
they need to be properly sampled. Early focusing to very small size or 
large size genetic programs can be avoided by size dependant navigation 
operators. The size-biased apphcation of crossover and mutation helps 
to adequately counteract the drift to short or long programs. 

42 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
The genetic learning operators should sample trees in such a way that 
the average tree size adapts without becoming very small or very large. 
Preferably small trees should be grown and large trees should be pruned. 
The operators have to prevent the tree bloat phenomenon which disturbs 
the evolutionary search [Banzhaf et al., 1998, Langdon and Poll, 2002]. 
Tree bloat occurs when the GP uses a fitness function that accounts only 
for the degree of fitting the data. When a tree expands, it pushes the 
search to a particular direction on the landscape which cannot be further 
avoided if there are no forces to shrink the tree for redirection. A similar 
disastrous search eflPect is the uncontrolled tree shrinking phenomenon. 
A population dominated by shrinking or growing genetic programs may 
drift to erroneous landscape areas where the evolutionary search stag-
nates. 
The size-biased mutation operator for IGP performs context-preserving 
mutation with probability prn, defined as follows: 
Pm, = IiX |.g|2 
(2.3) 
where /i is a free parameter [Goldberg et al., 1989], g is the linear im-
plementation of the genetic program tree ^, and \g\ is its size. This 
operator usually modifies large trees. 
The size-biased crossover operator for IGP spfices or crosses two ge-
netic program trees with probability pc defined as follows: 
Pc = i^/^fial 
(2.4) 
where /^ is a free parameter. More precisely, the probability whether to 
cut either of the trees is determined independently from the other. The 
cut points are randomly selected within the parents. 
The free parameters serve as knobs with which one may carefully 
regulate the evolutionary search efficacy. The proper values for /i and K 
may be identified with the autocorrelation function (Section 4.3.1). 
2.3.4 
Tree-to-Tree Distance 
The development tools for IGP that manipulate trees should include 
an algorithm for estimating the distance between the trees. The topo-
logical similarity among trees is quantified by the tree-to-tree distance 
metric. Having an algorithm for computing the distance between the 
trees is useful because it can be applied for analysis and improvement of 
the GP mechanisms (Section 5.3). 
The tree-to-tree distance is defined with respect to the basic concrete 
tree transformations which should be elaborated in advance. Distance 
Dist{Q^ Q') between two trees Q and Q' is the the minimum number jj of 

Inductive Genetic Programming 
43 
elementary mutations M G [M/, M/), Ms] required to convert one of the 
trees Q into the other Q^: 
Dist{g,g') 
= mm{p{M)\M 
G [M/,Mo,Ms],M{g) 
=:.g'} 
(2.5) 
where ^ is a unit distance function associated with each of the ele-
mentary mutations. The unit distance from making a substitution is 
e{Ms{V,g,V)) 
which can be 1 if V 7^ V, and otherwise 0. The unit 
distance from using either of the insertion or deletion suboperators is 
6{Mj{V,g)) 
= 0{MB{V,g)) 
= 1 because they always modify the trees 
on which they have been applied, that is they always change the trees. 
This tree-to-tree distance (2.5) is a distance metric as it meets the 
following mathematical requirements: 1) it is symmetric Dist{g, g') â 
Dist(C,',g)\ 2) it is nonnegative Dist{g,g') 
= 0 only if g = g'\ and, 3) 
it obeys the triangle inequality. 
The algorithm for computing the distance between trees should reflect 
their implementation and structural relationships between the parent 
and child nodes. The PNNs manipulated by IGP are implemented as 
labelled trees using preorder notation. The preorder labelling determines 
not only the arrangement of the nodes in the tree but it preserves also 
their structure. Let Vi be a node at position i in a tree. According to the 
preorder labeling, the following properties hold: 1) for each pair of labels 
i\ < ^2, the node Vi^ is an ancestor of node Vi^] 2) for labels i which are 
related by the inequality ii < i < 12, the nodes Vi are descendants of 
Vi^; and 3) the parent node Vi^ of node Vi^ is either Vi^-i or an ancestor 
of Vi^-i^ and Vig is on the path from node Vi^ to node Vi2-i-
A tree-to-tree distance algorithm for labelled trees in preorder nota-
tion using dynamic programming [Tai, 1979] is considered and modified 
to use the elementary context-preserving mutations [Mf^Mjj^Ms] 
from 
Section 2.3.1. Given two trees, the similarities between them are de-
tected in three steps. The third step calculates the actual distance, 
while the first and second steps ensure that only distances achievable 
through legal context-preserving elementary mutations are counted. 
The tree-to-tree distance algorithm using the elementary transforma-
tions from the CPM operator (Section 2.3.1) is given in four consecutive 
tables. Table 2.2b provides the function for computing the similarities 
between all subtrees from the first tree and all subtrees from the second. 
It uses the catalog updating function from the previous Table 2.2a. The 
function for counting the internode distances between arbitrary nodes in 
labelled preordered trees is shown in Table 2.2c. Finally, the three steps 
of the tree-to-tree distance algorithm are collected in Table 2.2d. The 
data arrays are common for all functions and are defined globally. 

44 
ADAPTIVE 
LEARNING 
OF POLYNOMIAL 
NETWORKS 
Table 2.2a. Function for filling the catalog of distances among all subtrees. 
Catalog Filling 
Algorithmic 
sequence 
U'pdateSD( r, u, 2, q, z, j , x, y ) 
{ 
if ((( r =â u ) and ( u = = i )) and (( q = = z ) and ( z = = j ))) 
SD[r\[u][i\[q\[z][j] 
= 
d{i,jy, 
else 
if ((( r == u ) and ( u = = i )) ov {[ q < z ) and ( z = = j ))) { 
p2=parent( j^C' ) 
SD\ r]\u\\i\[q]\z\[j] 
= SD\ r][u\\i\[q\\p2\[i-\] 
+ e( 0, j );} 
else 
if ((( r < w ) and ( u = = i )) or (( q ââ z ) and ( z = = j ))){ 
pl=parent( i,G ) 
SD[ r\\u\\i\\q\\z\\j\ 
= SD\ r 11 pi j[ i - 1 |( 9 ]( z |[ i I + (?( i, 0 );} 
else 
SD\T\\u\\i\\q\\z\\j\ 
= 
Min{SD\T\\x\\i\{q\\z\\i\, 
SD\r\\u\\i\\q\\y\\j\^ 
SD\r\\u\\x-\\\q\\z\\y-\\^SD\x\\x\\t\\y\\y\\j\ 
} 
Table 2.2b. Function for computing the structural similarity between each subtree 
from the first tree and each subtree from the second tree. 
Distance Cataloging 
Algorithmic 
sequence 
Catalog Distance s[) 
{ 
for ( z = 1; z < treeSizel; 2+4- ) 
for ( j = 1; j < treeSizc2; j - \ - \ - ) { 
u = i; 
while {u > 0 ) { 
r â u\ 
while ( r > 0 ) { 
while ( ^ > 0 ) { 
q=z\ 
while ( g > 0 ) { 
UpdateSD{ r, w, i, g, 2;, j , x, y ); 
g = parent ( q,G' ); } 
y=z]z = parent( z,G' ); } 
r = parent( r^G ); } 
x^u]u 
= parent( w,G ); }} 
} 

Inductive 
Genetic 
Programming 
45 
Table 2.2c. Function for counting the distances between arbitrary nodes in labelled 
preordered trees, necessary for measuring tree-to-tree distance. 
Internode Distance Counting 
AIg0rithmic 
sequence 
CountDistances () 
{ 
ND[ 1 ][ 1 ] = 0; 
for ( i = 2; i < treeSizel; 2+4- )ND[ i ][ 1 ] = i; 
for ( j == 2; J < treeSize2; j++ )ND[ 1 ][ j ] = j ; 
for ( z = 2; z < treeSizel; z+-f ) 
for ( j = 2; j < treeSize2; j4-+ ) { 
ND[ i][j] 
= MAXINT; 
r â parent( i,G ); 
while( r > 0 ) { 
q = parent ( j ,G'); 
while( q> 0) { 
pl=parent(z,G'); p2â parent(j,G') 
ND[i]{j] 
= Mini ND[i][j 
], 
NDlr][q] 
+ SD{ r][pl]li-l][q 
][p2 ][j-l]-e{r,q 
)); 
q = parent( q,G' );} 
r = parent( r,G );} 
NDli\lJ] 
= 
ND[i]lJ]+0{i,j);} 
} 
Table 2.2d. Algorithm for measuring the distance between labelled preordered 
trees using the elementary transformations from the CPM operator. 
Tree-to-Tree Distance Algorithm 
Algorithmic 
sequence 
1.Initialization 
MAXN maximal tree size, MAXD maximal tree depth 
SD[ MAXN ][ MAXN ][ MAXD ][ MAXD ][ MAXD ][ MAXD ] 
D[ MAXN ][ MAXN ]; ND[ MAXN ][ MAXN ); 
6 unit distance function, G genetic program tree 
2. Perform Cataloging 
CatalogDistancesQ 
[Tables 2,2.a and 2.2.h\ 
3. Counting Distances 
CountDistances{) 
[Table 2.2.c\ 
4. Determine 
L>[ 1 ][ 1 ] = 0; 
the minimal 
for ( i = 2; i < treeSizel; i-\--\- ) 
tree-to-tree 
D[ z ][ 1 ] = D[ i - 1 ][ 1 ] -f 6'( i, 0 ); 
distance 
for ( j = 2; j < treeSize2; j^--{- ) 
D\\\\j] 
= D\\][j-\] 
+ 
e(Q,j)-
for ( i = 2; i < treeSizel; i++ ) 
for ( j = 2; j < treeSizc2; j + + ) 
{ 
D[i\[j] 
= Mini ND[ i |[ j \,D\ i - 1 |( j ] + S( i, 0 ), 
D[i\\i-\\ 
+ 
e{Q,j)); 
} 

46 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
tree G 
tree G' 
Dist{G,G) = 8 
Figure 2.5. Tree-to-tree structural distance between two trees G and G' counted as 
the number of applications of the elementary operations M G [M/, M D , MS]. 
This tree-to-tree distance algorithm is general and it serves for measur-
ing the distance among trees with different branching factors. The algo-
rithm implementation can be instantiated for binary trees in a straight-
forward manner. It should be noted that this tree-to-tree distance al-
gorithm is computationally relatively expensive having complexity pro-
portional to 0{LL'S'^S''^)^ where L and L' are the numbers of the nodes 
in the corresponding trees, and S and S' are their depths. 
Figure 2.5 illustrates an example for the distance between two trees 
with respect to the mutations M G [M/, Mo^ ^s]-
2.4 
Random Tree Generation 
The random tree generation algorithm impacts the evolutionary search 
performance. Its effect is through the ability to sample with equal prob-
ability each point from the search space. If the initial population has not 
been sampled well, the search may not converge to an optimal solution 
even if the IGP navigation is good. Since apriori information about the 
optima is usually not available, each search region should be sampled 
possibly with equal probability. 
There are several methods available for random tree generation used 
in the GP community: naive methods [Koza, 1992], improved naive 
methods [Koza, 1992, Luke, 2000], and methods for construction of 
nearly uniformly distributed random trees such as uniform tree sampUng 
[Salustowicz and Schmidhuber, 1997], stratified sampling [Iba, 1994] and 
compound derivations [Bohm and Geyer-Schulz, 1996]. 

Inductive Genetic Prograwwing 
47 
The traditional method for random tree generation is the Grow al-
gorithm [Koza, 1992]. It creates a tree-hke genetic program by random 
generation of functional and terminals until reaching the predefined 
depth. If a functional node is made, the algorithm continues recursively 
with attachment of random children nodes. The next node to install in 
the tree could be, with equal probability, a functional or a terminal from 
the node pool. This algorithm, however, does not allow to control the 
tree shape, therefore nonuniform, skewed distributions of trees result. 
One remedy to this unbalancing problem is to use the Full algorithm 
[Koza, 1992] which constructs complete trees up to a given depth. 
Another improved method is the ModifiedGrow 
[Luke, 2000] which 
assigns user-defined probabilities to each functional node and terminal 
leaf so as to control their appearance in the trees. These probabilities are 
determined offline to speed up the tree construction. If the probabihty 
for choosing functionals over terminals is predetermined, random trees 
could be created around some desired average tree size. This algorithm, 
as well as the Full and Grow, does not necessarily produce truly random 
trees, rather it simply makes trees of different shape. 
Better alternatives are provided by the methods that try to gener-
ate uniformly distributed random tree structures [Bohm and Geyer-
Schulz, 1996, Iba, 1994, Salustowicz and Schmidhuber, 1997]. Such is the 
uniform sampling algorithm for initializing probabilistic prototype trees 
[Salustowicz and Schmidhuber, 1997]. A probabilistic prototype tree is 
a complete n-ary tree in which the nodes contain an initial probabihty 
threshold for selection of either a function or a terminal filling. A tree is 
generated by visiting every possible node from the complete tree. When 
the complete tree is traversed, a random number from the interval [0; 1) 
is uniformly generated and compared with the threshold. If it exceeds 
the threshold, a terminal leaf is installed in the tree. Otherwise, a func-
tional node with its children is inserted. The sum of probabilities for 
selecting the functions is equal to one. 
An almost truly random tree generation algorithm can be imple-
mented according to the bijection method [Iba, 1994]. This sophisticated 
algorithm produces random trees based on uniformly sampled words of 
symbols from a simple grammar of only two letters. This is a strategy 
for production of random words with n letters of the one kind and n â 1 
letters of the other kind. Next, the algorithm makes random permuta-
tions of the words, and builds corresponding trees of n nodes using a 
stack machine when reading the words. The generation of distinct tree 
structures is restricted by the desired node arities. The number of pos-
sible distinct trees and the construction of the solution are proportional 
to the Catalan number for trees with n nodes. 

48 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
Exact uniform generation of complete trees can be accomplished using 
context-free grammars [Bohm and Geyer-Schulz, 1996]. This is a method 
for uniform sampling of populations of distinct words written in terms 
of a predefined context-free language. Distinct trees leading to difi'erent 
derivations of the same word are produced with each invocation of this 
algorithm. This algorithm recursively grows a complete derivation tree 
by randomly choosing the next context-free grammar rule for expanding 
the tree. The Hmit on the number of derivation steps constraints the 
tree depth. This tree generation algorithm can be applied to IGP after 
modifying the grammar rules to build functional models. 
2.5 
Basic IGP Framework 
The IGP paradigm can be used for the automatic programming of 
polynomials. It provides a problem independent framework for discover-
ing the polynomial structure in the sense of shape and size, as well as the 
weights. The IGP learning cycle involves five substeps: 1) ranking of the 
individuals according to their fitness; 2) selection of some elite individ-
uals to mate and produce off'spring; 3) processing of the chosen parent 
individuals by the crossover and mutation operators; 4) evaluation of the 
fitnesses of the off'spring; and 5) replacement of predetermined individ-
uals in the population by the newly born offspring. Table 2.3 presents 
the basic IGP algorithmic framework. 
The formalization of the basic framework, which can be used for im-
plementing an IGP system, requires some preliminary definitions. The 
IGP mechanisms operate at the genotype level; that is, they manipulate 
finearly implemented genetic program trees g. The basic control loop 
breeds a population V of genetic programs g during a number of cycles 
r, called generations. Let n denote the size of the population vector; that 
is, the population includes gi^l < i < n individuals. Each individual g 
is restricted by a predefined tree depth S and size L in order to limit the 
search space to within reasonable bounds. The initial population V{0) 
is randomly created (Section 2.4). 
The function Evaluate estimates the fitness of the genetic programs 
using the fitness function / to map genotypes g ^ T into real values /: 
T -^ R, The fitness function / takes a genetic program tree g^ decodes 
a phenotypic PNN model from it, and measures its accuracy with re-
spect to the given data. All the fitnesses of the genetic programs from 
the population are kept in an array of fitnesses F of size n. The selec-
tion mechanism Select: F^ â> F'^/^ operates according to a predefined 
scheme for randomly picking n/2 ehte individuals which are going to be 
transformed by crossover and/or mutation. 

Inductive 
Genetic 
Prograwwing 
49 
The recombination function CrossTrees: 
F^/"^ x Râ^ F^/'^ takes the 
half n/4 from the selected n/2 ehte genetic programs, and produces the 
same number of offspring using size-biased crossover using parameter 
K (Section 2.3). The mutation function MutateTrees: 
T x R â^ T 
processes half n/4 from the selected n/2 elite genetic programs, using 
size-biased context-preserving mutation using parameter /i (Section 2.3). 
The resulted offspring are evaluated, and replace inferior individuals 
in the population Replace: F^/^ x F^/^ x iV -^ F^. The steady-state 
reproduction scheme is used to replace the genetic programs having the 
worst fitness with the offspring so as to maintain a proper balance of 
promising individuals (Section 5.1). Next, all the individuals in the 
updated population are ordered according to their fitnesses. 
Table 2.3. Basic framework for IGP. 
Inductive Genetic Programming 
step 
Algorithmic 
sequence 
1. Initialization 
Let the generation index be r = 0,and the pop size be n 
Let the initial population be: V(T) â [^i (T),^2('7'), ...,^n(T)] 
where gi, 1 < i < n, are genetic programs of depth up to S. 
Let /i be a mutation parameter, ^c be a crossover parameter. 
Create a random initial population: 
V{T) 
â RandomTrees{n), 
such that 'ig^Depth{g) 
< S. 
Evaluate the fitnesses of the individuals: 
F(r) = 
Evaluate{V{T),\) 
and order the population according to F ( T ) . 
2. Evolutionary 
a) Randomly select n/2 elite parents from V{T) 
Learning 
V'{r) = Select{V{T), F(r), n/2). 
b) Perform recombination of 7^'(r) to produce n/4 offspring 
V"(T) 
= 
CrossTrees{V\T),K). 
c) Perform mutation of V'{T) to produce n/4 offspring 
r"{T) = 
MutateTrees{V'{T),n). 
d) Compute the offspring fitnesses 
F"{T) 
= 
Evaluate{V"{T),\). 
e) Exchange the worst n/2 from V{T) with offspring V"{T) 
V{T+\) 
= 
Replace{V{T),V"{r),n/2). 
f) Rank the population according to F{T + 1) 
go{r + l) <g^{r + l) < ... <5'n(r + l). 
g) Repeat the Evolutionary Learning {step 2) 
with another cycle r = r + 1 
until the termination condition is satisfied. 

50 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
2.6 
IGP Convergence Characteristics 
Theoretical studies of the convergence characteristics of GP have been 
performed in two main directions: to formulate a schema theorem for 
GP, and to develop a Markov model for GP. The research in these di-
rections examines the evolutionary computation process from different 
points of view and reveals different factors that affect the search conver-
gence. The theory given in this subsection can help to understand how 
the system propagates useful subsolutions of the task. A schema theorem 
valid for IGP is discussed next and an original Markov model for sim-
plified IGP with fitness proportionate selection and context-preserving 
mutation is proposed for theoretical convergence analysis. 
2.6.1 
Schema Theorem of IGP 
A formal explanation of the GP search performance can been made 
using the schema convergence theorem [Poll and Langdon, 1998, Rosea 
and Ballard, 1999, Langdon and Poli, 2002]. The schema theorem es-
tablishes a relationship between the individuals in the population and 
the mechanisms of the GP system. This theorem quantifies the expected 
behavior of similar individuals that are matched by a common pattern 
called schemata. The schema theorem describes the rates with which 
a schemata representing a region on the landscape grows and shrinks, 
thus allowing us to reason how the search progresses. During evolution-
ary search, schema of individuals with fitness greater than the average 
population fitness usually produce more offspring. 
Individuals with close structural characteristics are summarized by a 
schemata. A schemata is a similarity pattern modelling a set of trees 
that are identical at certain positions; that is, they have coinciding func-
tional and terminal nodes. Imposing a schemata on trees means that it 
can be taken to generate all similar trees with the property of having 
the common nodes. The common nodes are fixed positions, while the 
remaining positions can match any subtree, and are called wildcards. 
Most convenient for describing the IGP performance is the rooted-
tree schema theorem [Rosea and Ballard, 1999]. This theorem provides 
an expression for predicting the frequencies of individuals matched by 
rooted-tree schemata in the population depending on how their fitnesses 
relate to the average population fitness and depending on the effects of 
the selection, crossover, and mutation operators. A rooted-tree schemata 
is a tree fragment of certain order with a common root. The order of the 
schemata is the number of all its functional and terminal nodes without 
the wildcard symbols. Due to the fixed root, a schemata matches dif-
ferent trees having a similar shape. The different rooted-trees belong to 

Inductive Genetic Program,ming 
51 
disjoint subsets of the search landscape; therefore a rooted-tree schemata 
covers a concrete search region. When the search proceeds, the schema 
in the population shrink and grow, thus indicating how the evolutionary 
search effort is distributed on the search landscape. 
Let the population contain schemata 7i and let there be m{H, r) in-
stances of it at generation r. Suppose that the average population fit-
ness is / (r), and the average fitness of all schemata instances H at 
this moment is / ^ (r). The rooted-tree schema convergence theorem for 
traditional GP systems states that [Rosea and Ballard, 1999]: 
m(H,r + l) 
>min,T) fnjr) 
1 ~ (Pm + Pc) Zi^nMr)/0{n) 
(2.6) 
where i enumerates the instances of the schema 7Y, li is the size of in-
stance z, 0{H) is the order of the schema, p^ is the mutation probability, 
and Pc is the crossover probability. 
The above theorem shows that the survival of individuals depends 
not only on their fitness but also on their size. Search strategies can 
be developed using the tree size as a parameter in the following way: 
1) size-biased mutation and crossover operators for tree size adapta-
tion can be made (Section 2.3); and 2) size-biased fitness functions for 
search guidance and regulation of the tree complexities in the population 
through selection can be designed (Section 4.1.1). These two strategies 
have the potential to combat the tree growth problem and contribute to 
sustaining progressive search. 
Although the formafism of the schema theorem gives some clues of 
how to improve the behavior of the evolutionary system, it has been 
widely criticized because it can hardly be used for online performance 
investigation [Vose, 1999]. The schema are difficult to detect and can 
hardly be identified while the GP system operates in real-time, and so 
in practice we cannot collect average statistics for the expected number 
of similar individuals in the population. 
2.6.2 
Markov Model of IGP 
The Markov model of IGP provides a formalization of the probability 
distribution of chains from populations. The distribution of the popula-
tions evolved by the system during the generations allows us to analyze 
theoretically whether IGP can be expected to converge to a uniform 
distribution or not. If the population distribution reaches a uniform dis-
tribution within a finite number of steps, this means that the search will 
terminate. Having theoretical results from such analysis may be useful 
to realize how the genetic operators impact the evolutionary search. 

52 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
IGP carries a stochastic process of continuous production of successive 
populations. It can be envisioned that each successive population in this 
evolutionary process depends only on the last population, and it does 
not depend on the process history. This is why the evolved populations 
may be considered a Markov chain. This reasoning allows us to develop 
a Markov model of the population-based IGP search. For simpHcity, 
it is assumed here that the random effects are caused by selection and 
mutation operators which act on a population of a fixed size. 
The IGP behavior can be studied following the related Markov theory 
of genetic algorithms [Davis and Principe, 1993], bearing in mind that 
the genetic programs have variable size. Assume that the IGP genotypes 
are represented using a language with alphabet A. A genotype written 
in terms of this language of size no greater than L is a set of such symbols 
G = {A-\-e}^^ where e is the empty symbol. Markov chain analysis helps 
us to find the distribution of populations from such genotypes. Let the 
genotypes be specified hy g ^ Q and h E G. Suppose that two successive 
populations are denoted by TT and v, and note that they have equal size 
|7r| = |i/| = n. The number of times that a particular genotype g appears 
in the population may be specified by 7T{g). The population is then 
TT = [7r(,gi),7r(,g'2)) â¢â¢â¢] ^^ it contains genotypes g\^g2^ .â¢â¢ some of which are 
repeated and we have Ylgeg^is) 
â '^" Clearly TT is a distribution of n 
genotypes over |{v4}^'| bins, where the symbol |.| denotes the cardinahty 
of a set. The genotype space that the IGP system can explore is a subset 
of the whole problem solution space {n]^ which is the set of all possible 
populations. Since each population is a distribution of genotypes, it can 
be assumed that the genetic programming system produces a Markov 
chain of genotype distributions. 
When the IGP manipulates the population using only the selection 
operator, the probability for selection Pr/(^|7r) of an individual g from 
population TT is: 
where h enumerates all individuals in the population having their par-
ticular frequencies 7v{h), f{g) is the fitness value of individual g and / 
is usually a real number. 
The probability Pr/(i^|7r) for producing a population v from population 
-K after n independent samplings with returning of genotypes is: 
where n is the population size, and z/(.g) is the number of occurrences of 
genotype g in population v (note that YlgeG^^^) ~ ^)' 

Inductive Genetic Program.m,ing 
53 
These two probabilities allow us to obtain a boundary for the con-
vergence of the population due to the forces exerted by the selection 
operator toward states of uniform populations. The expected number of 
transitions E{v\ii) from population to population until reaching a uni-
form population, is given by the inequahty [Davis and Principe, 1993]: 
i ? ( j ^ | 7 r ) < ( ^ ^ ) ' " < o o 
(2.9) 
\ /min / 
where /^^ax ^i^d /^nin are the possible maximal and minimal fitness values. 
Consider the case when IGP manipulates the population using two 
operators: selection and mutation. 
Since the stochastic effects from 
the selection operator have been investigated, it remains to study the 
effects from the context-preserving mutation. The probability Fvmiglh) 
to obtain by mutation 
individual g from individual h is: 
l'^'^^'^ ^ WTl 
(fc)p-(l - P - ) ' " ' 
(2.10) 
where |^| -f 1 is the alphabet size plus one empty symbol e for deletion. 
The alphabet is vA = FUT, L is the size of genotype /?., k is the number 
of modified alleles by the mutation, and pm is the mutation probability. 
The emergence of a genotype is a two-stage process: selection of a can-
didate from the population, and mutating it according to the mutation 
probability. Thus, the probability Pr/^(^|7r) for obtaining by selection 
and mutation a genotype g from population n is: 
Pr(.g|7r) = ^Pr(ft|7r)Pr(.#) 
(2.11) 
Then, the probability Pr/^(i/|7r) for producing a population u from 
population TT by the combined use of selection and mutation is: 
Pr(H^) - f r ^ T M l n ^<9\^Y^'^ 
(2.12) 
where v{g) is the frequency of genotype g in the next population. 
It is proven that for such combined use of selection and mutation, 
the chain of population distributions cannot converge to a homogeneous 
state because the following relationship holds [Davis and Principe, 1993]: 
Pm 
which is an indication for a stationary distribution. 
2 ; 
= Pr(i/|^) 
(2.13) 

54 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
The theoretical results from this Markov chain analysis of the popula-
tion characteristics generated by an IGP system show that selection and 
mutation are not sufficient to organize successful evolutionary search; 
that is, there is a need for crossover as well. From another point of view, 
the Markov chain analysis demonstrated that the IGP performance de-
pends on the mutation. Although some OF systems do not use it, one 
can realize that the mutation operator is not only useful but it is a nec-
essary mean to sustain diversity in the population. The maintenance of 
enough diversity is a condition for achieving sustained exploration of the 
search space, and thus progressive search. 
2.7 
Chapter Summary 
This chapter presented the basic structure of tree-like FNN models 
suitable for IGF. The FNN trees were related to other polynomial net-
works to emphasize the key differences between them, and to strengthen 
the confidence that tree-structured FNN are more flexible for doing evo-
lutionary search by the mechanisms of IGF. The polynomial network 
models are allocated in binary tree structures for reducing the dimen-
sionality of the search space and for making the network topology search 
process more efficient. The trees are implemented as linearized arrays 
to achieve fast computational speed. 
Loosely simulating principles from natural evolution led to the de-
velopment of the basic mechanism for genetic programming with FNN. 
A context-preserving mutation operator and a crossover operator were 
made. They can be applied to transform FNN structures with size-
biasing in order to prevent degeneration of the search performance to-
ward very small or very large trees. With the IGF, other mutation and 
crossover operators can be designed if it is necessary to improve the evo-
lutionary search. The IGF framework provides a basic algorithm which 
can also be enhanced with novel features. 
The presented brief theoretical analysis of the convergence behavior of 
IGF explained which factors affect the essential behavior of the system 
and provided clues as to what can be done in order to attain sustained 
evolution. It has been explained theoretically that: 1) it is beneficial to 
use size-biasing of the genetic learning operators and the fitness function 
because they influence the size of the offspring and avoid generating 
large complex trees which overfit the data and are harder to manage; 
and 2) it is beneficial to employ a mutation operator, in addition to 
the traditionally used crossover and selection in GF, because it helps to 
maintain high population diversity. 

Chapter 3 
TREE-LIKE PNN REPRESENTATIONS 
Inductive machine learning requires determination of the mathemati-
cal format of the relationship between the given input and output data. 
The different kinds of mathematical models have different expressive 
power for describing data. Each practical data set has specific char-
acteristics that should be approximated with a properly chosen model. 
The mathematical models can be distinguished according to the prop-
erties of the functional mappings that they offer. They can be divided 
into several groups: 1) linear or nonlinear models; 2) global or local, 
including piecewise, models; 3) discrete or continuous models; 4) models 
for periodic or aperiodic functions; etc.. 
The research experience indicates that functional identification from 
real-world data often uses nonlinear models. Linear models often fail 
to capture well the inherent properties and complex dynamics exhibited 
by natural data generating processes. Among the available various non-
linear function models we focus on high-order multivariate polynomials. 
The objective that motivates their study is twofold: 1) to incorporate 
the features of these various models, such as monomials, harmonics, 
kernels, recurrences, etc., into polynomials; and 2) to elaborate poly-
nomials as connectionist models in order to enable using contemporary 
approaches, such as genetic programming for finding the model struc-
ture, neural network and Bayesian inference techniques for improving 
the weight parameters, for learning them efficiently. 
This chapter investigates flexible nonlinear PNN specifications that 
are especially suitable for evolutionary computation by IGP mechanisms. 
The model specification involves two stages: 1) model representation, 
which requires us to determine the functional form of the polynomial 
mapping and how to accommodate it within the chosen structure; and 

56 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
2) model identification, which requires us to decide how to extract par-
simonious, less complex but faithful models from the chosen representa-
tion. The polynomial representations are allocated into tree-like neural 
network architectures. 
The developed PNN can be divided in two groups: linear and nonlin-
ear polynomial networks. A linear PNN has a structure that expands a 
power series by one term at each network node, so the overall model is 
a linear-in-the-weights polynomial of relatively low-order. A nonlinear 
PNN is essentially a multilayer neural network in which each node is 
fed by the activation polynomial outputs from its child nodes, and the 
overall function becomes a high-order polynomial. While a linear PNN 
gradually expands one power series, a nonlinear PNN builds a hierar-
chical composition of activation polynomials. The linear PNN can be 
used to make standard horizontal polynomials and kernel polynomials. 
The nonlinear PNN can be used to make block polynomials, orthogo-
nal polynomials, rational polynomials, and dynamic polynomials. Two 
orthogonal models are demonstrated: Chebishev polynomials and poly-
nomial trigonometric hybrids. Rational PNN using polynomial fractions 
are also proposed. The proposed dynamic models are recurrent PNN. 
The common feature of all these polynomial representations is that 
they are allocated on tree-like neural network structures which are es-
pecially suitable for evolutionary search by IGP. The PNN models in-
herit the general format of discrete Volterra series and they possess 
universal approximation abilities. Their approximation properties can 
be explained using the generalized Stone-Weierstrass theorem and the 
Kolmogorov-Lorentz superposition theorem. Such evolved polynomials 
have been extensively tested on various inductive tasks (Chapter 10), 
and they have been found amenable to computational learning. 
3.1 
Discrete Volterra Series 
The polynomials are nonlinear multivariate functions. Those consid-
ered here are Kolmogorov-Gabor polynomials (1.4,2.1). They are dis-
crete analogs of the Volterra models [Volterra, 1959] whose popularity 
comes from their potential to describe any analytical nonlinearity by 
series expansions of the form [Schetzen, 1980]: 
oo 
y{t)=wo + Y^VMt)] 
(3.1) 
built from convolution terms 1/i[a;(i)], also called Volterra functionals. 

Tree-like PNN Representations 
57 
The convolution terms are summations of products made as follows: 
oo 
oo 
i 
Vi[x{t)]^ ^ ... E^^(^i'^2,...,r,)n^(*-^i) 
(3.2) 
r i = l 
Ti=l 
j = l 
where ^'^('ri, r2, ...,ri) are Volterra kernels, x{t) are the discretely sam-
pled inputs, and y{t) are the targets. The Volterra kernels completely 
characterize the mapping and for finite series of reasonable length they 
can be found by geometric and algebraic computation approaches. 
Different nonlinear models can be derived from the canonical discrete 
Volterra series by developing different terms. The common difficulties 
are how to determine the format of the terms, how many terms to use 
for achieving the desired accuracy, and how to find the kernels, also 
referred to as coefficients. The increase in the input dimension leads to a 
combinatorial explosion of terms which often causes numerical problems 
when trying to estimate the coefficients. 
In order to construct such 
polynomials, one has to be certain what their common features are, and 
what their expressive power is for describing functions. 
In order to unify the discrete analogs of such Volterra series represen-
tations, this book adopts the following common format for polynomials: 
s 
s s 
s s s 
P(x) = ao+ Y^ aiXi-\- Y^Yl ^ij^i^j^ 
YLYIYI ^ijkXiXjXk + ... 
(3.3) 
where ai are the polynomial coefficients that play the role of Volterra 
kernels Ui{Ti,T2, â¢â¢â¢^T'i)? ^i are the input variables that capture the char-
acteristics of the data x{t â TJ), and P(x) is the model output like y{t). 
When the provided input variables are passed through nonfunctional 
or functional transformations, the resulting models can have different de-
scriptive characteristics such as kernel polynomials, high-order polyno-
mials, orthogonal polynomials, trigonometric polynomials, rational poly-
nomials, and dynamic polynomials. Such polynomials are developed in 
this chapter using binary tree-like structures. 
3.2 
Mapping Capabilities of P N N 
The universal approximation property of PNN justifies the belief that 
these models are sufficient in most cases for addressing real-world appli-
cations. The PNNs presented have universal approximation capabilities 
as they can represent any arbitrary continuous function, defined over 
a finite interval, to any desired degree of accuracy if there are enough 
terms available. The existence of polynomial network models for such 
functions follows from the generafized Stone-Weierstrass 
theorem for 

58 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
multivariate functions [Davis, 1975]. This theorem states that for any 
continuous function on a compact interval there is a family of uniformly 
convergent polynomials. More formally, for any bounded monotonically 
increasing continuous function f : R^' â^ R^ /(x) = f{xi,X2^ "-i^d)) Q^nd 
a closed definition region ^ = {x| â a < x^; < 6,1 <i<d], 
there exists 
a PNN model P(x) with polynomial nonlinearities which approximates 
/(x) within some precision Â£ > 0 over V\ \\ f{x.) â P(x) \\< e for all 
X eV, where || â¢ || denotes Euclidean distance. 
The Stone-Weierstrass theorem was originally proven for functions in 
the closed interval [0,1], but it can be extended on an arbitrary interval 
[a, b] in a straightforward manner using the linear transformation: x â¢= 
a + (6 - a)x, so that \\ f - P \\[a,b]= max^^^j^^^j |/(x) - P(x)| < e. 
Having the universal approximation property means that the PNN 
representations can potentially achieve satisfactory results provided that 
the network architecture contains enough nodes, since the number of 
nodes in the polynomial network structure determines the number of 
terms in the overall polynomial model. One should expect to attain 
a higher accuracy from a larger polynomial network architecture, as a 
larger network provides a larger polynomial expansion of higher degree. 
It should be noted however, that some real-world functions may require 
descriptions by indefinitely large numbers of network nodes which can 
not be implemented in practice. 
The universal mapping abilities of PNN are their essential advantage 
over traditional MLP networks. It has been shown that certain MLP 
networks fail to satisfy the universal representation theorem [Hornik et 
al., 1989]. Although the theorem does not apply to some MLP, neural 
networks with two layers of neurons, with sigmoidal nonlinear activa-
tion functions, are universal approximators [Cybenko, 1989]. Alterna-
tive neural network architectures can be developed so as to satisfy the 
Stone-Weierstrass theorem but they require specialized network design 
decisions [Cotter, 1990]. Such difficulties can be avoided with the con-
sideration of PNN. These polynomial networks are global models which 
can easily be analyzed with general purpose mathematical tools. 
The generalized Stone-Weierstrass theorem for multivariate functions 
is considered a corollary to the Kolmogorov-Lorentz superposition theo-
rem [Barron, 1988], and the universal mapping capabilities of PNN can 
also be explained with the latter. The superposition theorem of Kol-
mogorov [1957], elaborated further by Lorentz [1976], says that every 
continuous function on a bounded set can be represented exactly as a 
sum of compositions from continuous functions of one variable. Using 
properly selected continuous one-dimensional functions ^i,\ <i < 2(i-l-l, 
and fixed special continuous increasing functions V-^^j, 1 < j < d, defined 

Tree-like PNN RepresentatAons 
59 
on the unit cube / = [0,1]^, we can write each continuous multivariate 
function /(x) on /^ as follows: 
2d+i 
/ d 
\ 
/(x) -/(xi,X2,...,Xrf) = X^V^i 
X^V^ij(a^j) 
(3.4) 
where x = [xi,X2, .",Xd] is the input vector, and d is its dimension. 
The Kolmogorov-Lorentz theorem suggests that it is possible to rep-
resent any continuous function on a bounded set arbitrarily well, but 
it does not specify exactly what could be the building functions ipi and 
i/jij. Whilst one may hypothesize on the nature of the most appropri-
ate functions to use, it is interesting to note that we may select these 
functions in such a way that the polynomial models can be derived from 
the above formula. Suppose that the one-dimensional functions ijjij are 
identity functions, that is "ipijixj) = Xj for any i and j . Then, the inner 
summation gives: z = J2j=i ^iji^j) 
â xi -\- X2 + ... -\- Xd ^ov all indices i. 
The functions ifi can be chosen as follows: 
(3.5) 
(3.6) 
(3.7) 
ipn{z) = 
anz"" 
(3.8) 
Hence, the multivariate function model becomes: 
/ d 
\ 
/(x) == ^ ( / p J ^'0ij(xj) 
=^aiz-^a2z'^ -\- ...-i-anz'"' 
(3.9) 
which is equivalent to the Kolmogorov-Gabor polynomial. 
This result demonstrates that polynomials can be obtained from the 
Kolmogorov-Lorentz theorem as universal approximators. The polyno-
mial degree n need not be restricted by 2d -|- 1. In general, n > 2d -\-1] 
for example, if ^/J = x'^^ I/J = x^^ etc.. 
3.3 
Errors of Approximation 
3.3.1 
Approximation Error Bounds 
An important question that arises when addressing inductive compu-
tation tasks is: what are the approximation error bounds of the adopted 
model, as in practice only limited amounts of training data are available, 
which are only samples from the complete data distribution? The learn-
ing process involves estimation of hypothetical solutions with finite data 
y^l[^) 
M^) 
^3{z) 
â a\z 
â a2zz = a2Z 
= 
a^zzz = a-^z^ 

60 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
samples, which leads to uncertainties in the results. The preliminary 
belief in the results can be judged by formulae for the accuracy of ap-
proximation that can be attained in relation to the number of provided 
data. The fundamental question is: what is the approximation quality 
of a model as a function of the data and its representation properties 
such as the order and the input dimension? 
The approximation theory provides ready theoretical results for the 
error bounds of least squares polynomials [Barron, 1988]. Assuming that 
the task is to fit normally distributed empirical data using the Euclidean 
distance measure, that is the L2 norm, the minimax rate of convergence 
of the total error (risk) is: 
Â£yv - A^-^'^/^^^^^) 
(3.10) 
where A^ is the number of the given data, q is the order of the bounded 
derivatives of the model, and d is the input dimension. The convergence 
rate indicates how the error decreases when adding terms to the par-
tial sums that make the polynomial. This formula (3.10) shows that 
the number of data necessary for achieving good approximation grows 
enormously with the input dimension, and it is mitigated by the model 
smoothness as indicated by the order parameter. 
The total error (1.1) (Section 1.1) is associated with the ability of the 
model to generaUze; this is the difference between the polynomial output 
and the unknown true model. The above total error convergence formula 
(3.10) holds for polynomial networks and can be used to quantify the 
combined impact of the data, the order, and the dimension necessary to 
reach a satisfactory generalization. It shows how the polynomial com-
plexity and the error are related. For example, using a polynomial of 
order g = 10 and input dimension d = 10 with N = 1000 training data, 
implies a convergence rate Sj^ = 0.01. When growing very high-order 
polynomials using PNN faster convergence, lower errors can be obtained 
with the same data size in comparison to lower-order polynomials. This 
formula (3.10) also indicates that there is a risk of exponential impact 
of the dimensionality. Theoretically speaking, the curse of dimension-
ality is not avoided, rather it can be alleviated to a certain extent by 
growing compositions of simple activation polynomials according to the 
Multilayer GMDH approach [Ivakhnenko, 1971]. 
3.3.2 
Empirical Risk 
The problem of inductive learning is how to restore the dependencies 
in the empirical data. The notion of empirical data means that finding 
an optimal solution to the inductive task is difficult because there is 
no certainty in the properties of the training data necessary to produce 

Tree-like PNN Representations 
61 
theoretically precise results. The ultimate goal of finding a function 
that describes, with a minimal discrepancy, the desired output can be 
achieved by minimizing the error (risk) functional [Vapnik, 1992, Vap-
nik, 1995]: 
E{w) - |L(y,P(x,w))dPr(x,y) 
(3.11) 
where L(y,P(x, w)) is the loss measured with a selected error function, 
P(x, w) is the output of polynomial P having weights w when esti-
mated with input x, and Pr(x, y) is the joint probabihty distribution 
of the inputs x and the desired outputs y. The obstacle to solving the 
integral in formula (3.11) is the unknown nature of the joint probabil-
ity: Pr(x, y) = Pr(y|x)Pr(x). Assuming that the training examples 
are independent helps to develop the following tractable empirical risk 
functional [Vapnik, 1992, Vapnik, 1995]: 
1 
^ 
which does not involve the unknown joint probability distribution. Min-
imization of the empirical risk functional (3.12) gives bounds on the rate 
of convergence alternative to those from the approximation theory. 
The empirical risk functional converges with a certain probability to 
the so-called actual risk. The rate of uniform convergence of the average 
probability of the errors to the actual risk is given by the inequality: 
E'M 
< E,i^) 
+ 2eliN, h, a) (l + ^1 + ^ | f i ^ j 
(3.13) 
where E'{w) is the average frequency toward which the error converges, 
Eji(w) denotes the empirical risk functional associated with the training 
error, N is the number of the provided training data, and so{N, /?., a) is 
a confidence boundary given by: 
eo(iV,)i,o) = , / i l n | ^ l + l 
^ 
(3-Â») 
where a is a confidence level factor, and h is the VC-dimension of the 
model. The confidence level factor can be computed by the equation 
a = 1/y/N which suggests that larger confidence intervals result from 
larger data samples. The inequality for rate of uniform convergence of 
the average probability (3.13) holds with probability 1 â a. 
The Vapnik-Chervonenkis (VC) dimension [Vapnik and Chervonenkis, 
1971] is a measure of the learning potential of the model. The VC-
dimension of a polynomial of degree q is: h = q -\- 1 [Cherkassky et 

62 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
al., 1999]. In order to study the convergence properties of a PNN, the 
polynomial should be extracted from it into a power series format. 
With the increase of the VC-dimension of the model the training er-
ror decreases but the generalization error becomes worse. That is how 
the empirical risk minimization principle leads to the idea of trading-off 
the model complexity with the model accuracy in order to attain good 
generalization performance. The means for improving the generalization 
are regularization, pruning and Bayesian techniques. The empirical risk 
can be estimated and used as evidence for the total risk. 
3.4 
Linear Polynomial Networks 
A polynomial (3.3) is a linear superposition of terms. Such a linear 
model can be built starting with a single term and growing the structure 
term by term until the desired level of accuracy is reached. The problem 
is exactly which terms from the power series to include in the model so 
as to achieve a sufficient level of generalization. The inductive task is to 
search for the most plausible polynomial structure of terms which can be 
carried out efficiently using IGP. This section presents tree-structured 
representations made especially for IGP so that the tree traversal suc-
cessively adds a new term at each hidden tree node. Two kinds of such 
networks are proposed: horizontal PNN and kernel PNN. 
A distinguishing property of these polynomials is that they are linear-
in-the-weights. This implies that it is easier to navigate the evolutionary 
search with them because each such model is overall linear and has a 
unique global minimum of the error function. Applying least squares 
weight estimation techniques leads directly to the global minimum on 
the error surface. The population-based evolutionary search for the best 
model is facilitated as the PNN are globally optimized, so there is no 
danger of attaining suboptimal solutions by the IGP system. 
3.4.1 
Horizontal P N N Models 
Polynomials can be built using trees by gradual horizontal expansion 
of a partial model, adding one term at each intermediate node during 
tree traversal, the new term being indicated by the concrete activation 
polynomial at that node [Nikolaev and Iba, 2001b]. The weight of the 
new term is computed, while the old partial polynomial weights passed 
from the subtree below are only updated. Thus, the weights remain 
linear and are not raised to higher powers. Such a horizontal technique 
for polynomial construction from trees has been developed using the 
iterative least squares fitting method [Fadeev and Fadeeva, 1963]. 

Tree-like PNN Representations 
63 
The horizontal technique for polynomial construction from trees im-
poses special requirements on the selection of the set of activation poly-
nomials in order to be suitable for an evolutionary IGP style search. 
The first requirement on the activation polynomials is that they enable 
successive growth of the partial polynomial by exactly one new term at 
each node. The second requirement is that the activation polynomials 
are short enough to be manipulated efficiently, even by OLS fitting, when 
they are at the lowest, fringe tree nodes having two leaf terminals. These 
two requirements can be met by designing a fixed set of six bivariate ac-
tivation polynomials (Table 3.1). When an intermediate functional tree 
node is visited during the tree traversal process, the rightmost activation 
polynomial term specifies which new term to add. 
Table 3.1. Activation polynomials for linear PNN learning. 
2. 
3. 
4. 
5. 
6. 
p i ( x ) = WQ -\~ wxxi -[â¢ W2X2 
P2(x) = WQ -{-W^X-j + W2X1X2 
P3(x) â Wo -\- W-jX-j + W2X1X2 
P4(x) = WQ -\- WiXi + W2X2 
P5(x) â Wo -\~W-[Xi -\- W2X1X2 
TOcfv^ = 7/)n 4- m-i nr.-i 4- mnrr/^ 
P6(x) = wo-hwix-j 
+ W2X2 
Figure 3.1 gives an example for horizontal expansion of a partial poly-
nomial. The partial polynomial passed from a subtree may be expanded, 
but also may feed an input variable at the parent node activation poly-
nomial and thus, be suppressed from further expansion. There are two 
cases to consider, depending on the tree structure: 1) at a functional 
node with one leaf child, left or right, the partial polynomial is extended 
with the variable at the leaf (node p2 in Figure 3.1); and 2) when a func-
tional node has two children functional nodes, the partial sub-polynomial 
from the left child is extended (root node pQ in Figure 3.1). The right 
partial sub-polynomial outcome is used to feed the second input variable, 
and it is not extended (node p^ in Figure 3.1). The effect is extension 
of the partial polynomial by the right sub-polynomial whose weights are 
updated during a kind of restricted least squares regression. 
The iterative least squares (ILS) method [Fadeev and Fadeeva, 1963] 
(pp. 163-167) serves to implement iterative regression by adding succes-
sively new variables to the model. The horizontal technique employs ILS 
to learn the weights in all functional nodes except the fringe nodes. The 
ILS suppUes formulae for direct computation of the new term weight, and 
for recurrent updating of the inverse covariance matrix C~"^ = (#^#)""^. 
The benefit of this is in eliminating the need to conduct numerically un-
stable matrix inversions, which allows us to achieve reliable identification 
of the partial polynomials at the intermediate nodes. 

64 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
p{\)-{{w'\^+w'\x^->t-w'\jc^)-\-w' -^_x^^x^)^\v^_x-,^ 
{w\^-\-w\x^-\-w\x^')-\-w-^x^^x^ 
.P2 
r^P ] Activation Polynomial 
x 
X 
V J / 
i Function Node ) 
^ 
^ 
[ 1 Input variable 
( Terminal Leaf ) 
Figure 3.1. Horizontal polynomial construction using a tree-like PNN architecture. 
Assume that a partial polynomial Pi{x) with i terms (1 < i < 5) has 
been generated somewhere at an intermediate functional tree node. The 
normal matrix equation (1.7) can be simplified using the substitutions: 
C: 
h, ] - [ ^T^, 
^Ty ] 
(3.15) 
where Cj = * f * i and hj = *fy. 
The least squares weight estimates are produced by the equation: 
= Cr^h, 
(3.16) 
where C""^ is the inverse of the covariance matrix C^. 
After adding the (i + l)-th variable to the partial polynomial p^(x), the 
matrix ^Â«+i becomes one column larger ^i^i 
= [ ^i 
<Pi-\-\ ]? where: 
4>i-\-i = [0z+i(xi),0i+i(x2),...,(/)i+i(xn)]'^. Then, the updated weights 
w^+i are obtained with the equation: 
1-1 
w^+i = 
hi 
(3.17) 
where c^+i, j^+i, and ki^i are bordering elements of the old C/.. 
The bordering components substituted in (3.17) are as follows: 
iz+1 = 
0r+i0i+i 
(3.18) 
(3.19) 
(3.20) 
where (p^^i is a A^ x 1 vector with the values of the new (i + l)-st variable. 

Tree-like PNN Representations 
65 
The novel coefficient vector w^+i = C~^-^hi^i may be obtained with-
out inversion of the covariance matrix C^+i = ^ ^ ^ ^ ^ + 1 , but rather 
incrementally using the known quantities C~^ and h^ available from the 
computation of the polynomial with i variables. 
The elements of the vector w/^+i are derived by solving the equation 
Ci^iC~^^ = I, where I is a (z-j-1) x (z +1) identity matrix. The solution 
requires several steps. First, the components c^^i (3.18), j^+i (3.19), and 
ki^i (3.20) are used to build the new matrix C~j_\. After transforming 
the resulting block matrix, one infers the element Vi^i for the lowest 
right corner of the matrix 0~^i' It is scalar computable as follows: 
^ m = (i.+i-cr+iC-ic,+0-' 
(3.21) 
where C~ and c^-^i, as well as the scalar j^+i, are known. 
Second, this scalar Vi_^i is used in (3.17) to express the remaining 
blocks of the matrix C^^\. After that, multiplication of the matrix C~^\ 
with the vector h^+i is performed, resulting at w^+i. The resulting vector 
w^+i is simphfied by taking into account that w^ = C~^h^ (3.16). This 
yields at the bottom of the column vector w^+i, a formula for direct 
estimation of the new weight Wi^i: 
Wi^i = -v^^i{c[_^^Wi - ki^i) 
(3,22) 
Third, the new weight Wi^i is used in the upper block of w^_|_i: 
^z+l 
(3.23) 
to update the old weights w^ by the amount 
Wi^iC~^Ci^i. 
Finally, the new inverse matrix C~|_\ for successive computations is 
obtained: 
c-:, = 
(3.24) 
The matrix C^ ^ is symmetric, and therefore only half of the opera-
tions are sufficient to find the weights w^+i and the new inverse C~^\. 
ILS require us to calculate the column vector C^'^c^-i-i, used three 
times in (3.21, 3.23, 3.24), and after that its transposed version necessary 
for building the matrix (3.24) is easily derived. The scalar Vi^i also needs 
to be computed once. The weights vector w/^+i is next estimated with 
these known blocks. The block components, namely the column vector 
C~'^c^4-i, its transposed version c ^ | C ^ \ and the scalar Vi^i are saved, 
ready to construct the new inverse matrix C~|_\ by formula (3.24) which 
is computationally stable and efficient. 

66 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
3.4.2 
Kernel PNN Models 
Linear models using kernels have become increasingly popular for 
classification and regression [Vapnik, 1998, Cristiani and Shawe-Taylor, 
2000, Scholkopf and Smola, 2002]. Kernel models consist of basis func-
tions that translate the input space into a feature space of inter variable 
dependencies, which feature space is high dimensional. The kernels allow 
us to construct high-dimensional models by means of low-dimensional 
components. Making tree-like PNNs with kernels passed through the 
leaves enables us to build linear-in-the-weights high-dimensional func-
tions without explicit computations in the feature space. 
Kernels and Linear Models. A Kernel is a similarity function 
that maps the diff'erences between two vectors, usually considered in the 
Euclidean space, into a scalar number. Kernels are most often associated 
with dot products (inner vector products) in the input space. The nota-
tion (x, x') indicates a dot product of two vectors which is calculated as 
follows: (x,x') â¢=â¢ YA^I ^i ' ^i' '^^^ general r-th order polynomial kernel 
is: 
(x,x')'' = ( E ^i â¢ ^') = E E - E ^^1 â¢ ^ir â¢ <,- â¢ < 
(3.25) 
which is a sum of monomials. Each monomial is a feature. For example, 
a second order polynomial kernel of two vectors x = (xi,X2) and x' = 
{xi,X2) is the product: (x,x') = x^xf + 20:1X2X2X2 -f ^2X2^. 
A composite function can be made by combining the similarities of a 
vector to all available previous vectors from the data set. A linear kernel 
model is defined in the feature space by the following superposition: 
N 
y{x) =Y^ WnK{yi,yLn) + ^0 
(3.26) 
where K(x,Xn) stands for a preselected kernel function i^(x, x^i) â 
f ((x, Xn)) applied to each of the data vectors X77,, 1 < n < N, 
The representative power of such linear kernel models is determined by 
the particular kind of kernel basis function. The most popular positive 
definite, real value transformations, which lead to power series models 
are the polynomial kernel [Scholkopf and Smola, 2002]: 
i^(x,x^) = ((x,xn) + l)^ 
(3.27) 
and the following spline kernel: 
X(x, Xn) = 1 + (x, x^) + (1/2) (x, Xn) min (x, x^) ~ (1/6) min (x, x^)^^ 
(3.28) 
which is actually a cubic polynomial. 

Tree-like PNN Representations 
67 
:(X^X^ + 1) ' ] 
Â© 
Activation Polynomial 
(Function Node ) 
[~] 
Kernel function 
jf 
( Terminal Leaf ) 
Figure 3.2. Tree-structured PNN modeling with kernel functions. 
Learning Kernel Polynomials. Learning kernel PNN models in-
volves the selection of a kernel, choosing basis activation polynomials, 
and regularizers. Here, linear-in-the-weights PNN are built using poly-
nomial kernels. Localized polynomials of higher-order can be synthesized 
by relating the input vector x to another input vector x^: 
Ki = (x^x, + 1)2 
(3.29) 
where x^x^ is computed as a dot product of two vectors. A linear tree-
hke PNN using polynomial kernels is illustrated in Figure 3.2. 
Using the activation polynomials from Table 3.1 to construct network 
models causes some kernels to enter cross-product terms. This is not 
a problem because it is known that products of such kernels also pro-
duce valid kernels [Scholkopf and Smola, 2002], so the overall model 
retains the properties of Unear models. The process of training such a 
polynomial network using kernels involves estimation of the weights by 
iterative least squares fitting using regularization. There are statistical 
and Bayesian techniques for finding proper regularization parameters 
(Chapter 8). 
Polynomial kernel functions have been suggested as means for exact 
interpolation, but if a large number of them are taken, they tend to 
interpolate the data too closely. Since the inductive learning objective is 
to achieve good extrapolation, a fairly small number of kernels should be 
identified that compromises the goodness of fit with the generafization. 
The number of relevant kernel functions to enter the model may be 
determined by IGP style evolutionary search. 

68 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
3.5 
Nonlinear Polynomial Networks 
Universally approximating Kolmogorov-Gabor polynomials can be com-
posed hierarchically using multilayer PNN architectures. These nonlin-
ear PNNs are essentially feed-forward neural networks in which each 
hidden network node output feeds the activation function in the higher 
layer node. Thus, the overall model is not only highly nonlinear, but 
also it is nonlinear in the weights which require specific training algo-
rithms. Polynomial functions are extracted from such multilayer PNN 
according to the vertical technique suggested by the original Multilayer 
GMDH [Ivakhnenko, 1971]. Starting from the tree leaves, the overall 
polynomial model is composed by cascading the activation polynomials 
in the hidden network nodes during bottom-up tree traversal. 
3.5.1 
Block P N N Models 
The PNN developed according to the multilayer GMDH impose a 
strong bias toward overparsimonious models. This happens because 
when polynomials are hierarchically cascaded, their order rapidly in-
creases while only a small number of terms enter the model. A better 
representation for adaptive search can be made by injecting more terms 
in the tree-structured models, for example by passing summations. This 
can be performed using a polynomial block reformulation of the power 
series [Marmarelis and Zhao, 1997, Nikolaev and Iba, 2003]: 
P(x) - 6o+X^6i(z)Xi+X]E^2(i,i)X,X,+X^E 
i = l 
1=1 jâi 
i=l jâi k=j 
(3.30) 
where bi are the weights, and Xi are summation blocks. 
The blocks Xi suggest that summations or variables Xi enter the PNN 
as terminals: 
(^ Xi 
otherwise [l < i < d) 
This summation block (3.31) is a linear block Xi = Ylrn=i ^{^)^m) 
where the functions (^{m) are fixed to one. The weights 6i, 62, 63,... 
are indexed by the order of the monomial term in front of which they 
appear. These weights hi are related to the coefficients a^ in the original 
power series (3.3) by the equations: 
d 
d 
d 
d 
d 
d, 
ao = h^.^i = ^bi{i),aij 
= Y^Y^b2{i,j),aijk 
= ^
^ 
^ 
63(2, j,/c)... 
iâl 
i=:l jâi 
i = l 7=2 k=j 
(3.32) 
where the indices i,j,k,... are hmited by the input dimension d. 
X, 

Tree-like PNN Representations 
69 
(P8) 
D 
X . Input variable 
(Terminal Leaf) 
Figure 3.3. Tree-like PNN representation of a block structured polynomial. 
According to formula (3.31), a terminal may be either a sum of all 
variables up to the given dimension or simply a single variable. In this 
way, the terminals inject more terms into the PNN model. At the same 
time, a polynomial generated with this formulation will include some, 
but not all, possible terms up to a given, predefined order. The weights 
of the activation polynomials are estimated by OLS fitting. 
Figure 3.3 displays the allocation of a block reformulated high-order 
polynomial. The lowest terminal node here is a summation leaf, as well 
as the lowest right terminal leaf which is also a summation block. 
3.5.2 
Orthogonal PNN Models 
When the data are prearranged into linear systems of normal equa-
tions they are usually over determined, and this causes numerical prob-
lems when attempting to solve them. Such learning problems are: 1) 
there may be finear dependencies in the data, because of which it may 
not be possible to infer a solution; the reason is that mean square er-
ror methods are not necessarily computationally stable in such cases; 2) 
when solving the normal equations by mean square error methods, the 
noise in the data and the rounding errors are multiplied in the inferred 
weights leading to numerical inaccuracies; and 3) even if the mean square 
error methods work, they may be very slow and may not be acceptable 
for the concrete application. 
The learning of polynomials can be facilitated by inclusion of orthog-
onal terms. Having orthogonal terms can make the computation of the 
weights more stable, more accurate, and more rapid. Here stability is 
the robustness of the learning process with respect to different data sets. 

70 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
P(x) 
Zj=p^xh 
PJ(X)=WQ+W^X2+W2K2^+WJX^^ 
y" 
^ 
{p7) 
r 1 
i 
I 1 
1 
| C 4 ( X 2 ) = 8 x / - 8 x / + l j C^(xp=: 
[p] 
Activation Polynomial 
V J / 
( Function Node) 
C j, (x 
1 1 Input variable 
C (X) 
(Terminal Leaf) 
(p4) 
Z2=Pjjf') /
^
\ 
^ (p2j 
'â' 
^i^^i^'^i 
*^5/~Y2(^)=^0+^l-^7+^?^5 
_J 
L_l 
7) =X7 
f c ^ (x^) = 4 x / - 3 x ^ j 
Figure 3.4. A tree-structured PNN that uses Chebishev terminals. 
The tree-like PNN construction of multilayer GMDH adds high-order 
terms to the model which are not necessarily well structurally related 
to the data. One remedy for such problems is to use model components 
that capture common information in the data known as building blocks. 
The assumption is that the unknown function is resolvable in building 
block components, and they are learnable by IGP. A reasonable choice 
of such components are the Chebishev polynomials [Lanczos, 1957]. 
Chebishev polynomials can be incorporated as building blocks in tree-
structured PNN [Nikolaev and Iba, 2001d] (Figure 3.4) in order to better 
capture the regularities in the data, as done by neural networks [Na-
matame and Ueda, 1992]. Thus, useful building blocks can be identified 
and propagated during the IGP search process. This idea is similar 
to the GP approaches that use automatically defined functions (ADF) 
[Koza, 1994], modules (MA) [Angeline, 1994], and adaptive represen-
tations (AR) [Rosea and Ballard, 1995b]. The Chebishev polynomials 
Ci{x), passed as terminals Xi to enter the PNN models, are derived with 
the following recurrent formula [Lanczos, 1957]: 
Xi = a{x) = 2xQ-i{x) - a-2{x) 
(3.33) 
where i is the polynomial order, and the starting model is Ci{x) = x. 
The development of a PNN representation using Chebishev polynomi-
als helps to achieve: 1) encapsulation of partial structural information 
in the polynomials so that they become sparser, and thus increasing 
of their generahzation capacity; 2) better description of the oscillating 
properties in time series; 3) decreasing of the search space size due to 
the decrease of the tree size; and, 4) accelerating the search convergence. 

Tree-like PNN RepresentatAons 
71 
3.5.3 
Trigonometric PNN Models 
Harmonic terms can be used as building blocks in IGP for enhancing 
the abilities of polynomials, especially in order to describe better oscil-
lating time series data. Polynomial terms and harmonics can be used 
both to extend the expressive power of the network representation, and 
to develop tree-structured polynomial trigonometric networks [Nikolaev 
and Iba, 2001c]. The rationale for using polynomials and harmonics to-
gether is: 1) the polynomials are suitable as they approximate well the 
monotonic curvatures, as well as the discrepancies and gaps in the time 
series; 2) the harmonics are suitable as they approximate well oscillating 
components, spikes, and critical changes in the series curvature. 
Rationale for Hybrid Modelling. Time series from observations 
of natural phenomena, in the fields of ecology, meteorology, financial 
forecasting etc., usually exhibit oscillating character. The oscillations 
are conveniently described by trigonometric functions which are sums 
of harmonics from the corresponding Fourier expansions. There are two 
possibifities to consider when one tries to find which harmonics should 
enter the time series model: 
1) periodically oscillating series with repeating characteristics assume 
descriptions by sums of harmonics Ti{t) with multiple frequencies Vi â 
2i\i/N, 1 < i < h. The sum of harmonics Yl^^\T^i{^) i^ ^his case is a 
periodic function of t. The basis functions sin and cos are orthogonal, 
which guarantees decreasing of the mean squared error, and convergence 
in the limit, almost everywhere on the true function, when h â> oo; 
2) aperiodically oscillating series without repeating characteristics as-
sume descriptions by sums of harmonics Ti{t) with nonmultiple frequen-
cies Vi, Having nonmultiple frequencies means that the sum of harmonics 
is not a periodic function of t. The basis functions sin and cos appHed 
with nonmultiple frequencies are not orthogonal, and the attempts to 
model the data in this case can be done by searching for those harmon-
ics that build the closest function to the true one. 
The real-world data are often not exactly periodic, and can be mod-
elled well by trigonometric functions that are sums of harmonics with 
unknown frequencies: 
h 
T{t) = Y}^^ ^H^it) + Bi cos{vit)] 
(3.34) 
2 = 1 
where Ai and Bi are the real-value harmonic amplitudes or coefficients, 
and the number of harmonics h is bounded by max{h) < N/3. -
Distinct harmonics for making trigonometric PNN can be derived an-
alytically using Prony's method [Hildebrand, 1987]: 1) calculate the 

72 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
non-multiple frequencies Vi^ 1 < i < h^ oi each harmonic component i; 
and 2) estimate the harmonic coefficients Ai and Bi^ which enables us 
to determine the coefficients Q, and the phases 0^, for computing the 
separate harmonics using the concise equation CiCOs{vit â (j)i). 
P(x,0 
(p8) 
( p 7 ) 
( p 4 ) 
|T^(t)=C5.cos(v^t-(|)^)| 
^ 
n 
^ t - i ' 
Activation Polynomial 
(Function Node) 
Input variable 
(Terminal Leaf) 
T^(t) 
Figure 3.5. A tree-structured PNN that builds a polynomial trigonometric model. 
Polynomial, Harmonic and Hybrid Terms. The functions X^(x, t) 
that compose the polynomials can be hybrids of either polynomial or 
harmonic terms. For polynomial terms^ we can take the input variables 
Pi{-x.,t) = xt-i, where Xt-i means i time units behind t, i < d. The 
hybrid terms can be formulated as follows: 
r 
rjt)=C,,COs{v,t-^,) 
. 3 3 5 . 
where i is the harmonic number 1 <i < h, Ci is the real-value harmonic 
amplitude, Vi is the harmonic frequency 0 < f^ < TT, such that Vi ^ Vj 
for i j^ jy and (pi is the phase angle. In the case of multiple frequencies, 
i.e. Vi = 2'ni/N, the harmonics are h = {N â¢- l)/2 when N is odd, and 
h â N/2 when N is even. 
Figure 3.5 illustrates a polynomial trigonometric model represented 
as a tree-like PNN whose leaves feed harmonic components, with pre-
computed frequencies and phases, and input variables directly. 
Depending on the term variables, two cases arise: 1) polynomial and 
harmonic cross-product: in this case, the polynomial curvatures are 
transferred to the harmonic, and they amplify the spectrum of the har-
monic in places determined by the polynomial; and 2) harmonic and 
harmonic cross-product: in this case, the product curvature will contain 
new frequency components from the spectra of the harmonics. 

Tree-like PNN Representations 
73 
Identification of Harmonic Terminals. The nonmultiple frequen-
cies Vi can be determined from an h-ih. degree algebraic equation which 
is derived from T{i) (3.34). The derivation includes finding the coeffi-
cients aq E TZ using lagged series values, substituting these coefficients 
aq to instantiate the algebraic equation, and then solving it for Vi [Hilde-
brand, 1987, Madala and Ivakhnenko, 1994]. The weighting coefficients 
o^q^O < q < h â 1 are estimated by solving the following system of N â 2h 
equations by applying the least squares technique: 
h-l 
Yl ^qiVt^q + yt~q) = Vt+h + Vt-h 
(3.36) 
where yt denotes the ^-th value from the given series, and the range is 
t = h -\- 1,...,N â h. These coefficients aq are used to instantiate the 
equation for the frequencies Vi as follows: 
h-i 
QQ + ^ 
aqCos{viq) = cos{vih) 
for Vi,l <i < h 
(3.37) 
After expressing all cos{iv) as polynomials of degree i in cos(t'), equa-
tion (3.37) becomes an h-th degree algebraic equation in cos(f) for the 
nonmultiple frequencies [Hildebrand, 1987]: 
a^Q + a[ cos('u) + a'^cos'^iv)... + a'^co^^{v) = 0 
(3.38) 
where the new coefficients a'^ result from (3.37). 
Equation (3.38) is of the kind g{v) = 0 and can be solved by the 
Newton-Raphson method. Thus, h approximate roots are found which 
are the frequencies Vi, 1 < i < h, of the h harmonics. Among these 
calculated h roots for cos(i;), the admissible values are those that lie 
between â1 and 1, since |cos(f)| < 1, from frequencies in the interval 
0 <v 
<7T. 
The significant harmonics can be identified by drawing periodograms 
with plots of the intensity function [Kendall and Ord, 1983]: 
ââ,,./MÂ±M 
(3,39) 
where Ai and Bi are the coefficients of the i-th harmonic. 
In case of nonmultiple frequencies, the trigonometric models T{t) 
(3.34) are linear models of the kind Tc = y. The amplitudes c â 
{bo, Ai, Bi, A2, B2,..., Ah, Bh) are found by solving the normal trigono-
metric equation. 

74 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
are computed from the 
After that, the amplitudes Q and phases 
formulae: 
a â JA^^ -\- Bf 
and 
(/)^ = arctan(i^^/A^) 
(3.40) 
where i denotes the concrete harmonic number. 
In order to find the harmonic amplitudes Ai and Bi, 1 < i < h, 
forming a vector c = (60,^1,-^1,^2,^2, â¢â¢â¢5^/1,^/?.), one has to solve the 
normal trigonometric equation: 
c = (T'^T)-^T'^y 
using the harmonic design matrix: 
(3.41) 
sin('i;^xi) 
cos(^^xi) 
sin {v^X2) 
cos {v^X2) 
sm{v^xjsj) 
COS{V^XN) J 
(3.42) 
of size A^ X (2h + 1) as there are 2h coefficients, Ai and Bi^ 1 < i < k. 
The multiphcation T ' ^ T leads to the following {2h + 1) x (2/?. + 1) 
covariance matrix: 
1 sin(f-^a:i) 
cos(f-^xi) 
1 sin(f;^X2) 
cos{w^X2) 
1 sin(t;;^XAr) cos (w^xjsf) 
N 
Z^^^sm{v^xt) 
Eilicos('?;^x0 
E H I sin (v^xt) 
E l l i cos (v^xt) 
E^=i'^os{v^xt) 
X^Hi sin (v^xt) cos {v^xt) 
X^Hi cos (v^xt) cos (v^xt) 
J2^^^sm{v^xt)cos{v^xt) 
E l l l C0S2 (v^Xt) 
(3.43) 
where the summations are over all N points. The vector T'^y of size 
{2h + 1) X 1 is: 
Et=i vt 
YJt=.iytsm{v^Xi) 
E^^iVtcosiv^xt) 
T^y = 
(3.44) 
E i l i yusin (v^^xO 
E^=iytcos{v^xt) 
where the outcome vector y contains N values yt^ I <t < N. 
Advantages of Polynomial Trigonometric Models. The benefit 
of using harmonic components in the polynomials is induction of addi-
tional nonlinearities in the target model. The employment of analytically 

Tree-like PNN Representations 
75 
discovered nonmultiple frequencies extends the expressive power of the 
hybrid representations because when used in the cross-product terms, 
they modify the curvatures that they imply. When a simple activa-
tion polynomial is taken, the resulting product curvatures contain new 
frequency components that arise as sums and differences from the par-
ticipating frequencies. In addition to this, when other bases are selected, 
there will appear additional spectra with multiple frequencies due to the 
squared cos functions. 
3.5.4 
Rational P N N Models 
The approximation theory [Braess, 1986] provides mathematical treat-
ment of rational polynomials and shows that they are generalizations of 
ordinary polynomials. On approximation tasks, the rational polynomi-
als can be superior to ordinary polynomials especially when modelling 
highly nonhnear functions and some discontinuous functions. Hence, the 
rational polynomials should be preferred for modelling data with severe 
nonlinearities. Influenced by this theory and the research into rational 
function neural networks [Leung and Haykin, 1993, Rodriguez-Vazquez 
and Fleming, 2000, Zhu, 2003], tree-structured rational PNN represen-
tations with polynomial fractions are outhned below. 
The most attractive advantage of polynomial fractions for inductive 
problem solving is that they can describe with a small number of terms 
severe nonlinearities for which an ordinary polynomial needs a large 
number of terms. A rational polynomial can be developed using a tree-
structured PNN representation with ordinary activation polynomials in 
the hidden network nodes and a division operator applied at the root 
node. The overall rational function is obtained as a fraction of the nu-
merator polynomial provided by the left root child node and a denom-
inator polynomial provided by the right root child node. The overall 
PNN model becomes a polynomial fraction of the kind: 
PW = E ^ 
(3.5, 
where the numerator Pni'x.) and the denominator Pdi'x.) are power series 
expansions. These two polynomials, Pn(x) and Pd{'^), are hierarchi-
cally composed by cascading the activation polynomials in the network 
nodes. The activation polynomials in the left subtree below the root 
lead to expansion of the numerator polynomial Pn(x), and the activa-
tion polynomials in the left subtree below the root lead to expansion of 
the denominator polynomial Pdi'^)- At the root, the weights of the two 
immediate root children polynomials are learned. 

76 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
In order to learn a rational polynomial P(x) allocated in a PNN net-
work, the weights of the constituent ordinary activation polynomials in 
the hidden network nodes have to be estimated, and the weights of the 
polynomial fraction p^(x)/pQf(x) have to be determined by a relevant 
technique for system identification of rational models [Billings and Zhu, 
1991]. According to this technique, since a ratio pn{'x.)/pd{'x.) is a non-
linear model, it is expressed as a Hnear model, and then its weights are 
estimated by least squares fitting. The weights estimation formula is 
derived from the fraction of polynomials as follows: 
^ ^ Pn(x) ^ Pn{xi,Xj) 
^ 
(t)^{Xi,Xj)Wn 
.^ ^^. 
Pd{^) 
Pd{xk^xi) 
l-\-(l)d{xk,xi)wd 
where y is the desired target output provided by the training sample, and 
the input variables that enter the bivariate activation 
polynomials pn(x) and Pd{y^) (defined in Table 2.1). 
These polynomials are pri{xi->Xj) â (p^{xi^Xj)wn and Pd{^) = 1 + 
(j)d{xk^xi)wd respectively. The weight vector is obtained after perform-
ing the transformation: 
y = ct)^{xi, Xj)wn - ycl)d{xk, xi)wd 
(3.47) 
which leads to the following matrix equation: 
= {^^^)-^^^y 
(3.48) 
Wn 
Wd 
where [ Wn w^ ]^ = [K;no,'i^ni, .â¢.,^nm,'^di,'^(i2, â¢â¢â¢,'^ds]'^, assuming 
that the size of the polynomial Pn(x) is m + 1 and the size of p^(x) 
is s. The design matrix # consists of the following elements: 
* = 
/>n0(xi) 
c/)nl(xi) 
... 
(/)nm(xi) 
- # d l ( x i ) 
... 
-#d!.s(xi) 
/>no(x2) 
0nl(x2) 
... 
^nm(x2) 
- # d l ( x 2 ) 
â¢.â¢ 
-#d.s(x2) 
0no(XAr) 
0nl(xN) 
... 
(l)nm{'^N) 
-yhA^N) 
"- 
-#ds(xyv) 
(3.49) 
which size is N x {m. -\- s + 1). 
There are several alternatives to the above least squares algorithm 
[Billings and Zhu, 1991] that can be explored to perform efficient weight 
training in rational polynomial networks: 1) a rational version of the 
backpropagation algorithm for gradient descent search of the most opti-
mal rational function weights can be made [Zhu, 2003]; and 2) a recursive 
least squares fitting algorithm can be elaborated for finding the rational 
function weights [Leung and Haykin, 1993]. 

Tree-like PNN Representations 
11 
P(x) 
/?7(X)=WQ+W,X2+W^2^ + Wj^^l^-^ 
\ ^ ~ ^ / ' ^ ( x ) = 1+W, Z2+W2Z2^, +W^Z^ 
Â© Acti 
(I 
Activation Polynomial 
Function Node) 
^1 
I I Input variable 
(Terminal Let 
Figure 3.6. Tree-structured PNN representation of a polynomial fraction. 
A polynomial fraction made as a tree-like PNN is shown in Figure 3.6. 
A rational backpropagation algorithm for weight optimization in high-
order rational PNN is developed in Section 6.4. It is useful for further 
improvement of the PNN weights, especially when the data are uncer-
tain and contaminated by noise. The recursive least squares algorithm 
given in Section 7.6 may be chosen if faster training is desired. These 
two additional backpropagation techniques can be beneficial for train-
ing rational polynomial networks after the above least squares learning 
algorithm [Billings and Zhu, 1991] because it depends too much on the 
noise in the provided training targets. 
The rational PNN, however, should be carefully considered for ad-
dressing practical tasks, because problems can arise due to the inher-
ent hmitations of the fractional models [Braess, 1986]: 1) the rational 
model behavior may seriously deviate in certain directions if the fraction 
becomes zero, and 2) the fraction derivatives can be unbounded, thus 
causing difficulties in the weight identification. 
3.5.5 
Dynamic P N N Models 
Practical time series data requires models that capture time relation-
ships between the data in addition to nonlinearities. There are two 
possibifities to build time in polynomial networks: 1) to incorporate 
the time in a static PNN, so that the static network structure models 
the nonlinear dependencies between the data while a kind of additional 
short-term memory is simulated to model time dependencies; and 2) to 
create dynamic PNN models with special network architectures of nodes 

78 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
having feedback connections for capturing the time information in the 
data. The central issue when intending to handle the time is how to 
make memory take account of the dynamic properties. 
The dynamic properties of a function describing real-world phenom-
ena can be expressed as being dependent on temporal variables. Tem-
poral variables are taken at consecutive time steps. Such time variables 
provide sensitivity to temporal events. As this approach to dynami-
cal modelling suggests dividing the time into regular, spacially distant 
intervals, it is referred to as spacial representation of time [Elman, 1990]. 
The dynamic polynomial formats using temporal variables provide 
rich, expressive power, especially for time series modelling, because they 
encapsulate better spatiotemporal patterns. There are high-order terms 
with temporal variables which offer powerful abilities for sequence learn-
ing. The usual problems that arise in learning dynamic polynomials are: 
what should the time windows be, how to find the model size and the 
model order. These problems concern finding the proper polynomial 
structure for the given task which can be addressed by evolutionary 
search [Gomez-Ramirez et al, 1999, Rodriguez-Vazquez, 1999]. 
Following this idea, we developed recurrent PNN. These recurrent 
PNN have universal approximation capabilities for modelling almost any 
dynamic characteristics, provided the temporal variable values are com-
pact subsets of finite length [Sontag, 1992]. The generalization bounds 
of these dynamic PNN strongly depend on the maximal input length, 
and can be obtained only with some preliminary assumptions about the 
data distribution, or by restricting the topology [Hammer, 2001]. 
A common feature of dynamic PNNs is that they are state-space 
models of process behavior [Williams and Zipser, 1989, Narendra and 
Parthasarathy, 1990]. The states generalize information about the in-
ternal excitations to the applied inputs, so they supply additional in-
formation for better tracking of behavioral patterns. Recurrent PNNs 
belong to the class of recurrent neural networks [Jordan, 1986, Rumel-
hart et al., 1986, Elman, 1990, Tsoi and Back, 1994]. While traditional 
recurrent neural networks are black-box models from which it is diffi-
cult to extract the model, dynamic PNNs produce easily interpretable 
polynomials. Dynamic PNNs are attractive modeling tools as they are 
open-box state space models which may be evolved without using prior 
knowledge about the target function. Rather, the best recurrent poly-
nomial structure can be found using the stochastic mechanisms of the 
genetic programming paradigm. 
Recurrent PNN models can be developed by feeding back the errors 
from the activation polynomials as variables to enter any of the hidden 
network nodes [Iba et.al, 1995]. The intermediate errors from the activa-

Tree-like PNN RepresentatAons 
79 
tion polynomials serve as memory terminals [Iba et.al, 1995]. They keep 
information about the learning history and allow us to better capture 
the dynamic properties of time varying data. The memory terminals are 
evidence of the reaction of the parts of the model to a single temporal 
input pattern since there is a separate response from each hidden node 
to this pattern. In this way, the neural network learns to reflect the 
dynamics of the given inputs in the sense that it generahzes the tem-
poral data characteristics. The hidden network nodes fed by memory 
terminals become context nodes, as they retain the temporal context. 
The recurrent polynomials are represented as tree-structured neural 
networks with feedback connections. These are partially recurrent PNN 
because each memory terminal provides a feedback that enters some, 
but not all, functional nodes. In this way, global as well as local recur-
rences are implemented. The global recurrences are due to the use of 
the errors produced at the output root network node, which may enter 
any activation polynomial in the internal nodes. The local recurrences 
are due to the use of intermediate activation polynomial errors which 
may also be passed to any functional node as well as to itself. When the 
recurrences are passed through activation polynomials, the order of the 
time-dependencies increases, leading to improvement of the long-term 
mapping capabilities of the network model. 
Recurrent PNNs are flexible representations whose learning by IGF 
leads to sparse models. Sparse recurrent networks with asymmetric ir-
regular topologies may be found because the evolutionary search may 
discover the most relevant errors for the model; that is, not only neces-
sarily predetermined error terms enter the activation polynomials. One 
problem of recurrent PNN is how to find the weights. They can be found 
in two steps: 1) estimate the weights of the cascaded activation polyno-
mials using OLS fitting, and 2) reestimate the activation polynomials 
in the hidden nodes with the added error terms to produce the over-
all polynomial. This two-step process is a computational burden but it 
comes with the benefit of achieving more accurate models. 
The original approach to learning weights in recurrent tree-like PNN 
suggests initialization of all weights with random values, and then appli-
cation of an error-propagation training algorithm [Iba et.al, 1995]. An-
other problem is how to evaluate the fitness of the recurrent PNN after 
the application of the crossover and mutation operators. One possibil-
ity is to reestimate the whole PNN using the above two-step algorithm. 
Another possibihty is to retrain only the subtrees below the modified 
nodes by an error correction algorithm [Iba et.al, 1995]. A recurrent 
PNN model made upon a tree-like network is illustrated in Figure 3.7. 

80 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
P{x,t) 
Pg(x)=W,,+Wfy^+W2Z,-
1 
Z^=Py{\) 
^0+^,X^+^2h'-^^?^^8(t-V 
1 
^ . - ^ 
/^~\ 
@ 
^8(t-1) 
D 
i KV 
^j. 
V 
^2 
Activation Polynomial 
(Function Node) 
Error from polynomial 
Pg at time t-1 
Input variable 
(Terminal Leaf) 
{P8) 
h 
1) 
^7 
\v___^/?^(x)=W,)+W, Z2+W2Z2X,+W3?^2 
f P 4 V- 
n 
=p_,(x) V â ^ 
1 
[^p2) 
D ^1 
)^-^/7/x)=Wo+w,x,+W2e,^,.,^ 1 
Â®4rt-i; 
Figure 3.7. Recurrent polynomial constructed as a tree-like PNN with feedback 
connections from any hidden to any hidden network node. 
3.6 
Chapter Summary 
This chapter studied various polynomial models suitable for regression 
and approximation tasks and showed how they can be implemented ef-
ficiently to facilitate their inductive learning from data. The theoretical 
approximation abihty of polynomials was explained, and approximation 
error bounds that can be expected from such models were given. Their 
powerful expressive capacity serves as a motivation to develop various 
tree-like PNN representations of polynomials. The representations of 
polynomials as trees are especially suitable for organizing their struc-
tural learning by the mechanisms of the IGP paradigm. The fact that 
these trees serve to allocate PNN models on them enables further re-
training by efficient connectionist approaches. 
Whilst various kinds of tree-structured representations of polynomi-
als were discussed, these are not exhaustive. The intention behind the 
models constructed in this chapter is to point out how basic polynomial 
mappings can be elaborated, and they may serve as patterns that provide 
hints of how to elaborate other versions. For example, making wavelet 
polynomials incorporating not only local but also frequency information 
in the selected wavelet basis functions, or making neuro-fuzzy models 
[Kasabov, 2002]. In this sense there is a lot of potential for research 
into enhancing the expressive power of polynomial functions designed in 
PNN format. 

Chapter 4 
FITNESS FUNCTIONS AND LANDSCAPES 
IGP conducts search by modification of a population of genetic pro-
grams and by navigation of the population in the search space. The 
search space can be metaphorically considered Si fitness landscape [Stadler, 
1996, Wright, 1932]. The fitness landscape is an abstract discrete model 
that has three components [Jones, 1995]: a finite set of genetic programs, 
a fitness function that assigns a value to each genetic program, and a 
neighborhood relation specified by a genetic operator. Neighbors of a 
genetic program are all individuals reachable by one application of the 
operator. Each genetic operator creates its own landscape, according 
to the so-called one operator one landscape view [Jones, 1995]. There 
are considered to be mutation, crossover, and reproduction landscapes. 
The mutation landscape is difi"erent from the crossover and the selec-
tion landscapes. The landscape studies require explicit clarification as 
to which operator in combination with which fitness function is used. 
At genotype level F the genetic programs are linearly implemented 
trees g E F. The fitness function / maps a genetic program to a feature 
value /:F â> R, Since there is one-to-one correspondence between the 
linearly encoded trees and the genetic programs obtained from them, 
without loss of precision we may assume that the fitness function can 
be applied to a genome f{g) as well as to its phenotype /(S). When 
investigating the evolution of PNN one has to examine the dependencies 
between the linearly implemented networks and the corresponding poly-
nomial fitting accuracy. Analogously, the neighborhood relation can be 
considered directly at phenotype level at which the phenotypic instance 
is the interpreted PNN model. Although the genotype and phenotype 
levels are separated, there exists a strong causal relation between them, 
and they can be used together to analyze the IGP performance. 

82 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
Fitness 
landscape 
c, 
Genotype 
space 
Figure 4.1. The fitness landscape illustrated: genotype-phcnotype map (from the 
genotype space to the phenotype space) and phenotype-fitness map (from the phe-
notype space to the fitness space). The linear tree in the genotype space G, as its 
neighbors Gi and G2, are mapped (using dotted lines) into genetic program pheno-
types, which are mapped (using dashed lines) into corresponding fitnesses. 
Figure 4.1 provides an illustration of two mappings that build the 
fitness landscape: from the space of linear trees to the space of the ge-
netic programs, this is the genotype-phenotype map, and from the genetic 
programs to their corresponding fitness values obtained by model eval-
uation, this is the phenotype-fitness map. The plot in Figure 4.1 shows 
that the genotype-phenotype map preserves the relationship between the 
hnear trees and their genetic programs, and more precisely the direction 
and magnitude of their relationships. However the phenotype-fitness 
map does not preserve exactly all the properties of the relationships be-
tween the genetic programs and their fitnesses. It can be seen that the 
distance between the genotype f{G\) and its offspring /(G2) has a dif-
ferent direction and magnitude in the fitness space than the distances in 
the phenotype and genotype spaces. 

Fitness Functions and Landscapes 
83 
The profile characteristics of the fitness landscape reflect the difficulty 
of the addressed task. The landscape examination is important for gain-
ing insights into the evolutionary behavior of IGP [Kinnear, 1994], Since 
the landscape depends on the fitness function, the design of proper fit-
ness functions is crucial for achieving successful IGP performance. The 
following subsection studies the principles for elaboration of fitness func-
tions for polynomial model selection. Computable measures for fitness 
landscape analysis are offered that allow us to study the suitability of 
the IGP components for adaptive search, 
4.1 
Fitness Functions 
The fitness function is considered a driving force of the evolution-
ary search [Koza, 1992], It has to reflect the characteristics that are 
desired from the solution of the inductive task, 
IGP, as with other 
inductive learning approaches, usually faces difficulty learning from a 
limited amount of provided training data. What remains in such situ-
ations is to decide how to efficiently use these data in order to acquire 
some information about the model structure. Having prehminary knowl-
edge about the model features helps to focus the search toward not only 
highly fit, but also generafizing hypotheses. There are various criteria 
for estimating the generahzation performance that usually include an 
error component and a complexity component. 
Different techniques can be appfied to design fitness functions with 
error and complexity components whilst trying to avoid overfitting with 
the data, Overfitting occurs when the model has a very small error on 
the training data, because it then tends to have a large error on un-
seen data from the same distribution. The overfitted models should be 
avoided from consideration during learning because they cannot extrap-
olate beyond the specificities of the current sample. This problem is 
addressed by making fitness functions that favor fit and sparse PNN. 
IGP aims to learn well-performing polynomial models. Well-performing 
polynomials have the following three characteristics: they are accurate, 
predictive, and parsimonious. This requires us to design fitness functions 
with three ingredients: 1) an accuracy measurement that favors fit mod-
els; 2) a regularization factor that tolerates smoother mappings; and, 3) 
a complexity penalty that prefers short size models. Such functions aug-
ment the fitnesses of the good polynomials, make the good polynomials 
more clearly isolated on the fitness landscape, and thus help to direct 
the evolutionary search more precisely. 
This section shows how to design static and dynamic fitness functions 
so that they become suitable for search control in IGP. 

84 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
4.1.1 
Static Fitness Functions 
Static are those fitness functions whose mappings do not change; that 
is, they produce the same values when evaluated with the same genetic 
programs. The static fitness functions can be divided into four categories 
depending on the techniques with which they are obtained: statistic^ 
'probabilistic, information-theoretic, and risk minimizing 
functions. 
The Inductive Learning Criterion. The objective of inductive 
learning is to learn a hypothesis that generalizes beyond the given data. 
The total approximation error is partly due to the insufficiency of the 
learning algorithm with the adopted representation, and partly due to 
the insuflScient information in the available finite training sample. The 
learnability depends on the algorithm as well as on the data. In prac-
tice, the training data are provided by external sources and the most 
common assumption is that more data cannot be acquired. The issue 
that remains is how to manipulate the representation complexity so as 
to achieve sufficient power for data modelling. There is a strong inter-
dependence between the representation complexity, sample size, and the 
generalization error. Complex models may exhibit low estimation errors 
on the given data, but they are inconsistent with unseen examples of 
the same input-output mapping. Simpler models may show higher esti-
mation error but their prediction error is lower, so simpler models that 
exhibit satisfactory training errors should be preferred. 
The supervised inductive learning task is: given a set of input vectors 
X = [x\,X2^...^X(i\, and corresponding targets y, find a function model 
P that describes the mapping y â P(x) -f e, where e is a zero mean, 
normally distributed noise. Viewing learning as search, the goal is to 
infer a map that minimizes the risk (error) functional: 
E = j L{y,P{^))dPx{-K,y) 
(4.1) 
where L(7/,P(x)) is the loss, and Pr(x, y) is the cumulative probability 
distribution. This functional is the ultimate learning criterion, however, 
it cannot be used directly as the probability Pr(x,7/) is not known. 
The difficulty to obtain the unknown data distribution has lead to 
development of practically convenient learning criteria. Some of these 
criteria are derived using approaches that approximate the joint prob-
ability, while other criteria are reached by approaches that avoid the 
direct use of the joint probability. The most straightforward approach 
is to use the mean squared error as a loss function. The mean squared 
error may help to attain models with very low error on the training 
data (that is models with very low empirical risk) but it alone cannot 
guarantee achieving low prediction risk on future unseen data. 

Fitness Functions and Landscapes 
85 
Average Error Functions. 
Several functions for estimating the 
empirical risk of the models without taking into account their structural 
characteristics are discussed. 
These functions are the mean squared 
error, the mean absolute deviation, and the relative averaged error. 
Mean Squared Error. This is a basic measure of the deviation of the 
estimated output from the data when solving regression tasks. The best 
approximation P(x) of the true unknown mapping P(x) minimizes the 
distance; ||-P(x) â P(x)|p, where ||.|| is the norm of the linear space L2, 
defined: ||P|| = (/ |Ppd(^) ' . The search for this best approximation 
can be performed using the mean squared error 
{MSE): 
^SE 
= ^ X^(yâ - P(xâ))2 = l e ^ e 
(4.2) 
nâl 
where y^ is the n-th outcome from in the provided training sample 
V = {(xn,2/n)}^=i, ^(Xn) is the polynomial neural network outcome 
produced with the n-th input vector x^,, = [xni^Xn2^ -"^^nd]) ^^d A^ is 
the data size. The vector e = [ei,e2,..., e^^], of elements e^ = T/^ âP(x^), 
1 < n < N/is introduced to make the notation more concise. 
The MSE is the most often used performance measure in practice 
as it is computationally feasible. However, it amplifies the discrepancies 
between the data too much and the estimated function mapping; that is, 
it exaggerates the large errors. The MSE measure cannot distinguish 
the small from the large errors, which may give an implausible picture 
of the approximation. 
Mean Absolute Deviation. The median error is a robust measure of 
the mean model error as it does not allow us a small number of large 
errors to dominate over it. A version of this error is the mean absolute 
deviation (MAD) defined as follows: 
1 ^ 
MAD = -J2\yn-P{^n)\ 
(4.3) 
n=l 
where yn is the target, and P(xn) is the network output. 
Relative Averaged Error. Evidence for the error of fit that do not 
depend on the magnitudes of the data can be obtained by scaling the 
sum of squared errors with the deviation of the given targets. This leads 
to the unitless estimate called relative average error (RAE): 
RAE = ^-=^^^^ 
^yj^ 
(4.4) 
where y is the sample mean of the outputs y = {'i-/N) Yln^i Vn 

86 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
Statistical Fitness Functions. Coherent measures that account 
for the fitting as well as for the parsimony of the models are provided 
by some specialized statistical functions. These are statistical criteria 
for minimization of the risk functional (4.1), assuming the loss func-
tion L(7/,P(x)) = {yn â P{'Xn))'^' Popular fitness functions for statis-
tical model selection are the unbiased estimation of variance, the final 
prediction error, the predicted squared error, and the generalized cross 
validation. 
Unbiased Estimation of Variance. Low generalization error can be 
attained if both the model interpolation quality and the number of its 
parameters are considered. In network learning, the effective number of 
parameters (weights) [Moody, 1992] is less than the number of the actual 
parameters. The good parameters can be included in the cost function 
according to the Unbiased Estimation of Variance (UEV) formula: 
UEV = TTâ^^^ 
(4.5) 
where 7 is the eff"ective number of parameters computed as follows: 
j=:W- 
Xtracei^'^^)-^ 
(4.6) 
assuming that the model P(x) is estimated using a regularization para-
meter A, and ^ ^ ^ is the covariance matrix (Section 1.1.1). 
The application of formula (4.5) to hierarchically cascaded polynomi-
als requires some clarification. Since the developed PNN networks here 
use OLS fitting (1.7) to estimate the weights, partial eff'ective parame-
ters should be computed using (4.6) in every node. This is facilitated 
by the fact that the covariance matrix is available as a by-product from 
OLS fitting. The effective number of parameters in the whole network 
can be computed as a sum of all eff'ective partial parameters. 
Final Prediction Error. Another measure, derived using a second 
order Taylor series approximation of the generalization error, is the Final 
Prediction Error [FPE) [Akaike, 1970]: 
where 7 are the effective parameters, and e is the vector of the errors. 
This FPE measure accounts for both the degree of fitting and the 
model complexity. With the increase of the number of weights, the 
FPE reaches a minimum on the error surface. This error minimum is 
attained when the model has somewhere between zero weights and the 
number of weights equal to the data set size. 

Fitness Functions and Landscapes 
87 
Predicted Squared Error. The intention to make an error measure 
that accounts for the model predictabihty without partitioning of the 
training set has lead to the development of a similar measure with a 
learning accuracy component proportional to the empirical error over 
the training data, and a structural complexity component proportional 
to the number of polynomial weights. This is the Predicted Square Error 
(PSE) [Barron, 1988, Barron and Xiao, 1991] defined as follows: 
PSE = MSB + 2^a2 
(4.8) 
where MSB is the mean squared error (4.2), 7 are the effective polyno-
mial weights (4.6), and ay is an estimate of the unknown error variance 
that can be obtained as follows: 
^l = j^i:(yn-y)' 
(4-9) 
n=l 
where y denotes the mean of the outputs. This empirical error variance 
a^ is practically useful since it does not depend on the model. 
The PSE fitness function (4.8) also compromises between the error 
of fit and the model structure. What should be noted is that its second 
term, which penalizes overly complex models, is a linear function of the 
network weights. Because of this, when the number of weights increases, 
the slope of the PSE will begin to raise after reaching its minimum. The 
minimum is where the accuracy and complexity curves cross each other. 
This guarantees that the minimization of this error can lead to a well-
performing polynomial due to its proper structure. 
Generalized Cross Validation. A well-known approach in statistics 
for detecting the generalization capacity of a model is the leave-one-out 
cross vahdation. It requires us to perform successive error measurements 
of the studied model with each example as a separate testing set, after 
estimating the model with the remaining data. This idea can be appfied 
analytically without reestimating the model. Although this analytical 
approach is strictly vahd for Hnear models only, it can also be applied to 
polynomial networks using the following Generalized Cross-Validation 
{GOV) formula [Craven and Wahba, 1979, Wahba, 1990]: 
G C y ^ ^ ^ ^ e ^ e 
(4.10) 
where e is the error vector, and 7 are the eff'ective model parameters. 
The justification of using this GOV formula is that it is appfied after 
estimating the root activation polynomial which is linear in the weights. 

88 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
Probabilistic Fitness Functions. Probabilistic fitness functions 
can be derived using the two main principal approaches: the Bayesian 
inference and the maximum hkelihood [Bishop, 1995]. They also aim 
at minimization of the risk functional, but using other loss functions, 
which leads to estimates of the probability density of the given input-
output pairs. The attempt to search for a functional description of 
the joint probability density of the data is known as the maximum a 
posteriori principle. It is assumed that the data are independent and 
identically distributed. Two distinguishing fitness functions from this 
category presented here are the Bayesian information criterion, and the 
maximum marginal likelihood. 
Bayesian Information Criterion, The Bayesian approach allows us to 
obtain the mean of the joint posterior probability of the input-output 
data by decomposing it into a product of the dependence of the outputs 
on the model, and the prior behef in the model. This approach seeks to 
maximize the joint conditional density: Pr(x, ?/) = Pr(7/|x) Pr(x), where 
X are the inputs, and y are the outputs. The objective of Bayesian 
model selection is maximization of this posterior probability, which for 
convenience, is transformed into a minimization problem by taking its 
negative logarithm. This reasoning motivated the development of the 
Bayesian Information Criterion {BIG) [Schwartz, 1978]: 
where 7 are the good parameters, and e is the error vector. 
Maximum Marginal Likelihood, The maximum marginal likelihood 
approach helps to choose the most probable parameter values given the 
data. It differs from the Bayesian approach in that it involves the model 
with its weights. The goal of model selection is maximization of the 
unconditional posterior probability: Pr(y) = / Pr(y|x) Pr(x)(ix. Taking 
the logarithms, multiplying by minus two, and discarding the constants, 
leads to the following Maximum Marginal Likelihood (MML) 
[MacKay, 
1992a] error estimate: 
MML = 7In (al) - In |R| -f -^e^e 
(4.12) 
y 
where a^ is an empirical estimate of the error variance (4.9), and R is 
the projection matrix. The construction of PNN makes the computation 
of the projection matrix for the whole polynomial model not so trivial 
because the covariance matrix of the overall model is not available. For-
tunately, it is possible to approximate it using the covariance matrix 
produced for the estimation of the output node weights. 

Fitness Functions and Landscapes 
89 
The projection matrix can be computed using the matrix equation: 
R = I - * E * ' ^ 
(4.13) 
where ^ is the design matrix formed to learn the output node weights, 
and XI is the covariance matrix S = (#'^# + AI)"-^. 
Information-theoretic Fitness Functions. There are several func-
tions for evaluating both the degree of model accuracy and the degree 
of its complexity which have been derived using notions from the in-
formation theory. Such popular information-theoretic fitness functions 
are the Mallows' statistics, the Akaike's information criterion, and the 
minimum description length. 
Mallows' Statistic. The total error of a regression model can be mea-
sured by the so-called Mallows' Statistic (Cp)[Mallows, 1973]: 
Cp=:J-e^e + 2j-N 
(4.14) 
ay 
where a^ is assumed as an estimate of the error variance (4.9), and 7 
is the number of effective parameters. The Cp function trades off the 
accuracy of fit with the model complexity. 
Akaike 's Information Criterion. The information content of a model 
increases with the decrease of the following Akaike's Information Crite-
rion (AIC) function [Akaike, 1973]: 
^ / C = | e ^ e + | 7 
(4.15) 
which penalizes models with larger number of effective parameters 7. In 
this way, the AIC function allows us to balance the relation between the 
model performance on the training data with the model complexity. 
Minimum Description Length. Criteria that favor accurate and parsi-
monious models can be derived using the Minimum Description Length 
(MDL) [Rissanen, 1989] principle. This principle suggests to pursue not 
maximization of the posterior probabihty FY{D\Qi) Fv{Gi) of the genetic 
programs ^i, 1 < i < TT, inferred from the data L>, but rather minimiza-
tion of its code length: rmn{I{D\Qi) -\- I{Gi)\l < i < TT}, where I{Gi) 
denotes the coding length of the i-th genetic program alone I{Gi) = 
â log2Pr(^^), and I{D\Gi) is the coding length of this genetic program 
as a model of the data I{D\Gi) = â\og2Pv{D\Gi)> 
Adapted for polynomial network induction, the MDL principle can 
be stated as follows: given a set of data and an effective enumeration of 
their PNN models, prefer with greatest confidence the PNN which has 
both high learning accuracy (i.e. very closely approximates the data) and 

90 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
low syntactic complexity (i.e. has a small number of terms or weights). 
Such an MDL fitness function suitable for IGP is [Iba and de Garis, 
1994]: 
MDL = MSB + {njr -f nr) + nj: logs JT + nr logs ^ 
(4.16) 
where njr is the number of functional nodes in the genetic program tree, 
nr is the number of terminal leaves, JT is the number of all possible 
functionals, and T is the number of all terminals. 
A problem that arises in the application of this MDL formula (4.16) 
is how to trade off its two components, because they differ in magni-
tudes. One remedy to this problem is to balance the accuracy and the 
complexity by an adaptive parsimony factor. Denote the complexity cost 
by C, that is C = (njr + nr) + njrlog2^ + nrlog2T. 
Then, the above 
MDL-based fitness function (4.16) can be balanced in the following way 
[Zhang and Muhlenbein, 1995]: 
MDL2 = MSB + P{T)C 
(4,17) 
where p(r) is called adaptive Occam factor. Assuming that the error of 
the best genetic program at generation r â 1 is MSB^^^^v ^^^ ^Best^ 
is its expected complexity, the adaptive factor is computed with the 
formula [Zhang and Muhlenbein, 1995]: 
r iV-2M5Â£-,VCe,,,r, 
if MSEl-j, 
>e 
N- V [MSEI-IC^,,,T) 
, otherwise 
where e is the suggested minimal error of the final solution. In the first 
case, the factor p{r) decreases when MSB'^j^ 
> e because the error is 
less than one. In the second case, when the error diminishes MSB'^~l^ < 
e, the factor p{r) increases, and thus, exaggerates the importance of the 
structure size during the later stages of the search process. 
The MDL functions (4.16) and (4.17) add a complexity penalty to 
each error, and even if two genetic programs exhibit the same error, 
they will have different fitnesses if they have different structures. Thus, 
they make the local landscape slightly more rugged, but globally the 
landscape becomes smoother and easier for navigation. 
Risk Minimizing Fitness Functions. Identification of well- gener-
alizing models can be approached without measuring the joint distribu-
tion of the data [Vapnik, 1992]. According to the Vapnik-Chervonenkis 
theory [Vapnik and Chervonenkis, 1971, Vapnik, 1995], the risk func-
tional can be reformulated to find the generalization error bounds in the 
following way: {l/N)J2^^iL{yn^P{'Xn))- 
The function that describes 
the generalization error bounds may be taken as a model risk estimate. 

Fitness Functions and Landscapes 
91 
The risk estimate can be used to direct tlie search because it is known 
that there exists a model with optimal complexity which for the given 
sample size has smallest prediction error. The Structural Risk Minimiza-
tion {SRM) [Cherkassky et al, 1999] formula is: 
- 1 
I L 
I 
. 
. InA^ \ 
T 
SRM=^-- 
1-Wm-mlnm-f-TTT- 
e^e 
(4.19) 
where e is the error vector, and m is a factor computed using the VC-
dimension /i = 7 + 1 as follows: m, = h/N. 
The SRM estimate has 
confidence level: 1 â 1/y/N. 
It should be noted that this version of 
SRM is specialized for polynomial models. 
The usefulness of the SRM is slightly limited due to its tendency to 
tolerate polynomials overfitting the data at the boundaries [Cherkassky 
et al., 1999]. That is why the boundary data can be removed from the 
training sample before using SRM. 
4.1.2 
Dynamic Fitness Functions 
Dynamic fitness functions are those functions whose mappings change 
during the evolutionary process. This may happen when the examples 
from the training set are associated with a changing property. The fit-
ness function can sustain improving search by driving the individuals in 
the population to compete, so as to fit data with more distinct proper-
ties. This competition between the individuals may help to accomplish 
reliable search in a similar way to the way in which the immune sys-
tem learns [Farmer et al., 1986, Farmer, 1990, Bersini and Varela, 1991]. 
The motivation to mimic such biological learning abihties comes from 
the close similarity in the behavior of the immune system and the IGF 
system: 1) they both perform search for generalizations of fitted observa-
tions; and 2) they both use similar search mechanisms involving pattern 
matching, heuristic selection and gene modification. 
Following an idiotypic network model of the immune system [DeBoer 
and Hogeweg, 1989], there a dynamic fitness function was developed 
which consists of two dynamic models that influence each other [Niko-
laev et al., 1999]: 1) a model for propagating genetic programs that fit 
more important data, and stimulating them to match data from different 
subsets; and 2) a model for changing the importances of the data in de-
pendence of the number of genetic programs that fit them. This dynamic 
function controls the search by encouraging complementary genetic pro-
grams to interact, and thus forms a kind of a network. The network 
connectivity is a source of diversity which is a spontaneous macroprop-
erty that enables us to achieve continuous evolutionary search influenced 

92 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
by the importance of the data. Genetic program reinforcements with re-
wards for matched important examples occasionally provoke network 
perturbations which push the population on the landscape. 
Genetic Program Dynamics. 
The Dynamical Fitness Function 
(DFF) is formed from a constant initial supply, plus a proportion from 
the previous fitness DFFJ and proliferation due to arousal by fitted 
examples and evoked exciting interactions, without a death rate: 
DFFJ-^^ = Z + DFF[{^Prol{AgJ, 
Id]) - d) 
(4.20) 
where Z is influx constant, d is turnover constant, <^ is proliferation con-
stant, AgJ is the antigen score of the i-th genetic program at generation 
r. Id] is the total anti-idiotype excitation of program z, and Prol is 
the proliferation function. The fitness of a genetic program will increase 
when it fits more examples, and it also depends on their importances. 
The 'proliferation Prol of a genetic program i in the next generation, 
according to this difference equation model, depends not only on the 
matched observations AgJ, but also on the extent of its interactions Id] 
with the other genetic programs: 
f^'^iMJ-D'^J'^^,^ 
(4.21) 
where p\ is a free constant parameter. 
The antigen score of a genetic program should account for the number 
of the examples that it fits and it should also depend on the importance 
of these eliciting examples. It is assumed that an example depends 
on the number and on the specificity of the structurally related genetic 
programs that match it. The antigen score AgJ of genetic program i can 
be defined as linearly proportional to the importance IJ of the examples 
n which it matches, I < j <n^ n < N: 
n 
A9[= E BijIJ 
(4.22) 
where the binding Bij of a program i is 1 if the genetic program matches 
the j-th example and 0 otherwise. For regression with continuous out-
comes, the binding Bij of a program i is redefined as follows: 
B.. = i l - e ? M if 4j<^\ 
/4 23) 
^â¢^ 
\ 0 
otherwise 
where of- = {yi â Pi{'Xj))'^ is the error between the outcome Pi{'^j) of the 
i-th program evaluated with input Xj, and yi is the target. The value of 
e^t is the sum of squared errors from all programs in the population. 

Fitness Functions and Landscapes 
93 
Data Importance Function. The genetic program fitness dynamics 
should interact with the dynamics of the data. If more genetic programs 
recognize a particular example, then it should become less attractive. 
When a small number of unmatched exceptional examples remain, they 
have to reinforce those genetic programs in the population which cover 
them, and so to provoke search perturbation. The importance I^ of 
example n is defined as proportional to the number of genetic programs 
that fit it, plus a term for constant recruitment 7^: 
(
DFFT\ 
^-E^B,n-E;^]+'yn 
(4.24) 
aâ;[ 
max / 
where a is a free constant parameter, and TT is the population size. 
Specifying the genetic program interactions Aij is essential to drive the 
individuals to compete. The interactions should be such characteristics 
that reflect complementarity in behavior. Stimulating complementary 
behavior can be made by defining the affinity of the genetic programs 
to account for the diff'erence in their mutual learning potential. Two ge-
netic programs are complementary when they fit examples from disjoint 
subsets. The affinity Aij between two genetic programs i and j may be 
defined as the set diff'erence between the subset of examples: 
A,j = \Dj-D'j\ 
(4.25) 
where DJ is the subset of examples matched by program i at generation 
r from all provided training data D: DJ C D, D'j C D. 
The virtual immune network comprises all genetic programs in the 
population. This network is symmetric Aij = Aji for 1 < ^,j < TT, and 
also, the genetic programs do not recognize self An = 0. The affinity 
Aij stimulates the genetic programs to: 1) fit more examples; and 2) fit 
less overlapping data subsets. 
The idiotypic influence among the genetic programs should estimate 
their mutual behavioral complementarity through the affinity interac-
tions. The dynamic fitness uses such a factor to self-regulate so that 
together the individuals have the power to fit all data. The anti-idiotype 
excitation Id\ is defined to favor a genetic program i if its interaction 
Aij with the other 1 < j < TT population members is high: 
^< = ] ^ E ^A,,DFFJ 
(4.26) 
where N is the data size, and TT is the population size . 
The evolution of the virtual immune network topology determines the 
ability of the IGP to conduct efficient search. It is important to realize 

94 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
how the network connectivity will correspond to the phases of evolution-
ary search. IGP usually begins with a random initial population, and 
the global excitation is large. When IGP performs global search, the 
interactions should be high, indicating exploration of large landscape 
areas. During local search, the wiring should be low as IGP exploits 
landscape areas in the vicinities of the reached local optima. 
Finding of proper parameters for this dynamic function is a tedious 
task and may require a large number of trials. The following reference 
values are recommended: Z = 0.1, G? = 0.5, pi = 0.25, ^ = 1.1, 7^ = 
0.001, and a = 1.025. The initial fitness is calculated by solving the 
above difference equation for a steady state DFFf = DFF^'^^^ which 
leads to the formula: DFF^ == Z{pi + I)/{pi + 1 + d-i-dpi - ^), 
The immune IGP dynamics have three aspects [Farmer et al., 1986]. 
The first aspect is the net topology dynamics. Here it is assumed that 
the immune network is symmetric and completely connected, but these 
are simplifications. Theoretically, the immune network should comprise 
programs interacting with a small number of other programs in the pop-
ulation. The affinity formula should not stimulate, however, formation 
of very sparse networks, since they make it impossible for perturbations 
to occur and so hinder the search. The second aspect is the parameter 
dynamics. The large number of free parameters in the fitness formula 
creates difficulties for finding and tuning their values. The third aspect 
is the concentration model dynamics of the genetic programs. These dy-
namics are very difficult to analyze because of the bell-shaped character 
of the activating proliferation function. 
4.1.3 
Fitness Magnitude 
The fitness function can be transformed in several ways, if necessary, 
to make it more suitable for the evolutionary IGP search mechanisms. 
The main three formats of the fitness are [Koza, 1992]: raw fitness^ stan-
dardized fitness^ adjusted fitness^ and normalized fitness. Raw fitness is 
the absolute quaUty of the genetic program as mapped by the fitness 
function which refiects the desired properties from the solution of the 
task. Standardized fitness is the minimizing version of the raw fitness, 
which in the case of evolving PNN, is the same as the raw fitness. Ad-
justed fitness is made by scaling the raw fitness with a particular method 
in order to improve the selection (Subsection 5.1). When fitness propor-
tionate selection is used it is recommended to divide the adjusted fitness 
of an individual by the sum of all adjusted fitnesses from the population, 
which results in a normalized fitness. 

Fitness Functions and Landscapes 
95 
4.2 
Fitness Landscape Structure 
The capacity of the evolutionary algorithm to perform efRcient search 
is determined by its ability to move continuously on the underlying fit-
ness landscape, that is by its ability to continuously push the population 
on the landscape. This suggests potential to traverse landscape valleys, 
climb on landscape peaks, walk on landscape ridges, and various other 
landscape profiles. When organizing population-based search on a fit-
ness landscape, one may envision that each program moves along some 
trajectory. A program change leads to a move on the landscape toward 
another fitness value. Thus, during evolution each program makes a tra-
jectory on the fitness landscape. The direction of the program movement 
reflects the rate of change of its fitness computed after one application 
of the modification operator. 
Having knowledge about the fitness landscape structure helps to un-
derstand and control the evolutionary search process. The landscape can 
be envisioned as a three dimensional surface of points whose third dimen-
sion on the z-axis are fitnesses, and whose projections on the x-y plane 
are their corresponding genetic programs. The diff"erences between the 
points form the landscape structure^ in other words, the heights (magni-
tudes) of the fitnesses make the landscape. The fitness landscape profile 
consists of peaks, plateaus, saddles, and basins. The configurations of 
these forms indicate the structural characteristics of the landscape. 
When the landscape structure includes peaks or basins with consid-
erably diff'erent fitnesses, then the landscape is called rugged [Jones, 
1995, Hordijk, 1996]. When mainly plateaus and large basins with com-
parative fitnesses build the structure, the landscape is smooth [Jones, 
1995, Hordijk, 1996]. The fitness landscape characteristics show the dif-
ficulties that a searcher encounters when trying to find an extremum. 
There is no strict, generally accepted theory that explains exactly how 
the landscape characteristics infiuence the search. One of the views is 
that a rugged landscape is complex and difficult for search. A relatively 
smooth landscape is a simple landscape which is easy to search. 
The ultimate goal of the search process is to locate the global optimum 
on the landscape. When a minimizing fitness function is considered, 
global optimum is the deepest basin point on the landscape. A genetic 
program ^ is a global minimum if it has smallest fitness than all other 
genomes Q' on the whole landscape: 
^g,g'j{g)<f{g') 
(4.27) 
The search has to avoid entrapment at inferior local optima. When a 
minimizing fitness function is considered, the local optimum is the lowest 
basin point on the landscape with equal fitness than all of its neighbors. 

96 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
reachable with the learning operator, A genetic program ^ is a local 
minimum if it has smallest fitness among all other genomes Q' within a 
region with a certain radius r on the landscape: 
V0, g', Dist{g, g')<r=> 
f{g) < f{g') 
(4.28) 
where the radius r is defined using tree-to-tree distance Dist. 
The fitness landscape structure can be described by diff'erent charac-
teristics: 1) distribution of local optima; 2) heights of local optima; 3) 
dimensions of the basins between optimal points; 4) information that 
a point carries about distant points; 5) information that a point car-
ries about optima, etc.. Before trying to investigate these features, one 
should realize that the genotype space in IGP is an irregular graph in 
which each genetic program has a diff'erent number of neighbors than the 
other genetic programs. The formal analysis of landscapes over irregular 
graphs is extremely difficult and still under investigation [Bastert et al., 
2001], while there is a complete theory for the analysis of landscapes 
over regular graphs [Stadler, 1996] applicable to genetic algorithms. 
4.3 
Fitness Landscape Measures 
The fitness landscape is a useful metaphor for analysis and under-
standing of the evolutionary search process. This metaphor is general 
and can be instantiated with particular learning operators and concrete 
fitness functions. Several measures for fitness landscape analysis are pre-
sented here, including statistical, probabifistic, information, and quan-
titative measures. It is demonstrated how to apply them to study land-
scapes derived with the mutation operator. The crossover and repro-
duction operators may be studied in an analogous way. 
4.3.1 
Statistical Correlation Measures 
The search difficulties are monitored by the fitness landscape, and 
they are mainly in two aspects: local and global search characteristics. 
The local search characteristics depend on the relation between the fit-
ness and the operator. Examinations of the local landscape features 
can be performed using the autocorrelation function [Manderick et al., 
1991, Weinberger, 1990]. The global search characteristics depend on the 
correspondence between the fitness and the distance to a known global 
optima. Examinations of the global landscape can be made using the 
fitness distance correlation [Jones and Forrest, 1995]. 
The correlation landscape characteristics are only a necessary condi-
tion for the usefulness of the genetic operators and the fitness function. 
However, they do not guarantee that good search performance will be 
achieved. From a local perspective, a landscape with a single hill could 

Fitness Functions and Landscapes 
97 
have a high autocorrelation, but the evolutionary algorithm may climb 
slowly, making a long path on its slopes before reaching the global op-
tima. From a global perspective, a landscape may feature by a good 
fitness distance correlation but if the global optima is surrounded by a 
lot of local optima, it will be arduous for the algorithm to search on it. 
Autocorrelation Function. The autocorrelation function (ACF) 
of a fitness landscape [Manderick et al., 1991, Weinberger, 1990], in 
context of IGP, measures the statistical correlation between the fitnesses 
of genetic programs trees separated by a series of applications of the 
operator from which the landscape arises. The empirical autocorrelation 
ACFd can be determined by the fitness correlation coefficient (FCC) of 
the operator that makes the landscape. The fitness correlation coefficient 
accounts for the characteristics of landscapes on trees at certain distance 
apart. The FCC, FCC : BJ" x R^ -^ R, of the mutation landscape 
(d = 1) can be calculated with the fitnesses of n pairs of parent Qp and 
child Qc trees, taken during a random walk starting from an arbitrary 
initial tree through a set of trees generated by successive applications of 
the mutation operator [Manderick et al., 1991]: 
where: Cov{Fp,Fc) is the covariance between the computed sequences 
Fp = {/pi,..., fpn} of parent fitnesses fp = f[Qp), and Fc = {/ci,..-, fen] 
of child fitnesses fc = f{Qc)^ ^(^P)? ^^^ ^{^c) are the standard devia-
tions of Fp and Fc respectively. 
The autocorrelation ACF^ of a landscape over trees at distance d is: 
"^Qp^GcDistigp.gc) = 
dj{gp)eFpj{gc)eFc 
ACFd = FCC{Fp,Fc) 
(4.30) 
When ACFd is close to 1, the fitness and the operator are highly 
correlated, which indicates a locally smooth fitness landscape that is 
easy to search with the operator in most cases. 
Strictly speaking, the ACF^, may be used to study statistically isotropic 
landscapes. A landscape is isotropic when the statistics do not depend 
on its examination by random walks. That is why several ACF^ have 
to be computed by walking different landscape subspaces, starting from 
different random trees. This helps to obtain a representative estimate 
for the entire landscape. Small deviations between the computed ACF^ 
indicate that the landscape subspaces look similar. In this case, the 
landscape may be considered statistically isotropic, and the application 
of the ACFd formula (4.30) is vaUd subject to the assumption that the 
analysis is restricted to the walked subspaces [Hordijk, 1996]. 

98 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
step of the R a n d o m 
Walk 
Figure 4.2. Autocorrelations of three different mutation landscapes produced with 
context-preserving mutation (CPM), 
hierarchical variable-length mutation 
[HVLM), 
and replacement mutation (RM). 
The MSE fitnesses are evaluated using the Mackey-
Glass data, and averaged over 50 runs. 
Figure 4.2 displays the ACF\ of three mutation landscapes made using 
the operators CPM [Nikolaev and Iba, 2001a], HVLM 
[O'Reilly, 1995], 
and RM (Subsection 2.2.2). One can see in Figure 4.2 that the land-
scape of the CPM operator [Nikolaev and Iba, 2001a] features highest 
autocorrelation, hence its landscape is smoother than the others. 
The general algorithm for evaluating the ACF of a landscape of an 
irregular graph [Stadler, 1996] uses successive modifications of trees at 
diff'erent steps apart. The unified ACF from such different images, esti-
mated with tree neighborhoods at increasing differences, is the average 
autocorrelation from them. This general algorithm that performs it-
eratively random sampling of trees and measures the unified ACF of 
averaged fitness landscape subspaces is given in Table 4.1. 
Another look at the local fitness landscape characteristics can be made 
with the correlation length metric. The correlation length (CL) is a 
measure of the longest series of steps beyond which there could not 
be obtained meaningful information about the relationship among the 
fitnesses of two points on the landscape surface. The statistical estimate 
of the correlation length states that the largest lag for which there is still 
correlation between two points on the fitness landscape is the first step 
at which the ACF function becomes: - 2 / v ^ < ACF < 2/v^, where n 
is the number of sampled trees. 

Fitness Functions and Landscapes 
99 
Table 4.1. General algorithm for measuring the autocorrelation function ACF of 
fitness landscapes that arise in IGP with tree-like genetic programs. 
Measuring Autocorrelations in IGP 
step 
Algorithmic 
sequence 
1. Initialization 
Tree-to-tree distance Dist : Q x Q -^ N and fitness f : Q â^ R. 
2. Perform 
a random 
walk 
3. Calculate the 
ACF 
For several distances d = 1 to 10 do 
For a number of landscape subspaces ^ = 1 to 50 do 
a) Generate a parent tree Qp = 
RandomTree{). 
b) For a large number of steps n > 10'^ do 
i) Sample an offspring Qc of Qp at Dist{Qp,Qc) â d. 
ii) Record the fitnesses: fp â f(Qp) and fc = f{Gc) 
Fp = {/pi,/p2,...} and Fc = {/ci,/c2, ...}â¢ 
iii) Replace the child to become parent: Qp = Qc. 
c) Compute the mean fitnesses 
d) Compute the covariance 
C<yu{F^, F,) = E:=.I(/P.: - / ) ( / - " 7c)-
e) Compute the fitness deviations 
â¢^(-f^-^) = \ / E : = , { / - : - Â« ' -
f) Calculate the AC for step d from subspace / 
FCCi[Fp,Fc) 
= 
Cov{Fp,Fc)/{a{Fp)a{Fc)). 
Average the autocorrelations from the / subspaces 
Compute the unified ACF from the available ACFd 
^CF^^,Y.'LACFa. 
Fitness Distance Correlation. The global search difficulties de-
pend on the global landscape structure. On a relatively smooth fitness 
landscape, the distance to the global optimum decreases as the fitness im-
proves. The relationship between the fitness and the distance to a known 
global optimum gives evidence of the global smoothness and ruggedness 
of the fitness landscape. This relation can be studied using the fitness-
distance correlation (FDC) [Jones and Forrest, 1995] measure: 
FDC{F, D) 
CovjF.D) 
a{F)a{D) 
(4.31) 
where: Cov{F^ D) is the covariance between the fitnesses F = {/i,..., /n} 
of a set of n arbitrary trees and D â {d\^..., <in} are their distances to the 
chosen global optimum, cr{F) and cr{D) are their standard deviations. 

100 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
When a fitness function with minimizing eff'ect is employed, the value 
of FDC is 1 if there is a perfect correlation between the fitnesses of 
the trees and their distances to the optima. It is assumed that when 
the fitness-distance correlation is in the interval 0.15 < FDC < 1, then 
the landscape is relatively mountable (that is such a landscape can be 
used for IGP). When the FDC is in the interval -0.15 < FDC < 0.15, 
the landscape is considered relatively difficult to search, and in case 
FDC < â0.15, the landscape is extremely hard to search. A hard 
landscape suggests that the fitness function or the representation should 
be improved so as to facilitate the search. 
The fitness-distance correlation FDC can be estimated after making 
a large number of IGP runs and assigning the best tree as eventual global 
optimum. The rationale is that a large number of samplings are nec-
essary to derive significant statistical evidence for the fitness landscape 
characteristics, since one does not know the global optima. After that, 
random trees should be generated, and their tree-to-tree distances to the 
selected global optimum should be computed. 
As an example, investigations of a global mutation landscape were 
made using the Mackey-Glass series. Two fitness functions were taken: 
the generalized cross-vahdation (GCV) (4.10) and the mean squared 
error {MSB) 
(4.2). The calculated FDC values were as follows: 1) 
from the GCV fitness function in the interval [0.17,0.26]; and 2) from 
the MSE fitness function in the interval [0.05,0.18]. According to the 
above classification, the landscape of the GCV function seems relatively 
easier to search than that of the MSE. 
Figure 4.3 shows a scatter plot of the fitness-distance correlation mea-
sured during the same runs with IGP using the Mackey-Glass data. One 
can see that average line through the scatter plot of the FDC obtained 
with the GCV is at a greater angle with the abscissa than this obtained 
with the MSE. 
This means that the GCV landscape features better 
correlation between the fitnesses and their distance to the optimal so-
lution, therefore the GCV landscape is better for doing evolutionary 
search. Prom another point of view, the GCV landscape seem slightly 
more rugged from a local perspective, since individuals with equal error 
are estimated differently and have different fitnesses due to the complex-
ity penalty. However, from a global perspective, the GCV landscape is 
smoother and the superior optima can be clearly distinguished on it; this 
is what makes the landscape mountable by the search algorithm. This 
is because the cross-vahdation factors in the GCV formula carry more 
information about the quality of the PNN models. Table 4.2 offers the 
algorithm for measuring FDC in IGP which has been used to make the 
scatter plots illustrated in Figure 4.3. 

Fitness 
Functions 
and 
Landscapes 
101 
GCV 
f i t n e 
R 
Q i I 
i ^ I 
f 
1 H 
1 H 
1 
\A 
â¢l/u C 
r a B 
MSE 
f i t n e s s 
2 0 
3 0 
4 0 
T r e e - t o - t r e e 
d i s t a n c e 
Figure 4.3. Scattered plots of the fitness-tree-to-tree-distance correlation calculated 
witfi 1000 randomly sampled tree-like PNN. 
Table 4.2. Algorithm for measuring fitness-distance correlations FDC in IGP. 
Measuring Fitness-Distance Correlations in IGP 
step 
Algorithmic 
sequence 
1. Initialization 
2. Generate 
random trees 
Compute the 
variances and 
covariances 
4. Calculate the 
FDC 
Let the distance function be Dist : Q x Q -^ N 
and the fitness function he f : Q -^ R 
Find a virtual global optimum: Qg. 
For a large number of steps n > 10^ do 
a) Generate a tree: Q = 
RandomTree{), 
b) Evaluate its fitness: / = f{Q)' 
c) Compute tree-to-tree distance 
to the global optima Dist{Q, Qg) = d. 
d) Record the fitness / and the distance d 
F=={fuh..-}^ndD 
= 
{dud2,...}. 
Compute the mean fitness and the mean distance 
Compute the covariancc 
CoviFD) = 
Y:tJfi-J){d,-d). 
Compute the deviations 
Compute the fitness/distance correlation 
FDC{F,D) 
= 
Cov{F,D)/(a{F)a{D)). 

102 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
4.3.2 
Probabilistic Measures 
Practical development of IGP systems may be carried out using other 
fitness landscape measures that require less preliminary assumptions 
than the statistical correlation measures. Such is the probabilistic causal-
ity measure of the landscape ruggedness. Causality means that the dis-
tance between the genotypes in the phenotype space is kept propor-
tional, as well in the fitness space. Small distances between a genotype 
and its neighbors imply small rate of change between the corresponding 
phenotypes and fitnesses. When the causahty principle is satisfied, the 
landscape is smoother and the search is expected to be easier. 
When the causality of the genotype-phenotype map is strong, a sub-
ject of particular interest in IGP is the phenotype-fitness map. This 
is because changes of the finear tree genotypes imply corresponding 
changes in the phenotypic genetic programs. The phenotype-fitness map 
establishes a relation between neighborhoods in the phenotype space 
and neighborhoods in the fitness space. The causality condition for the 
phenotype-fitness map in IGP with tree-like genotypes can be stated as 
follows [Igel, 1998, Rosea and Ballard, 1995a, Sendhoff" et al., 1997]: 
V0âg,-,5^, and Gj e M{gi),gk G MiGi), 
DistiGuGj) < Dist{gi,gk)=> 
=> 
\\f{g^)-f{gj)\\<\\fm-fmm^^) 
where M are the immediate neighbors of a genetic program produced 
by mutation, / is the fitness function f : g â^ R^ Dist is the tree-to-tree 
distance Dist : g x g â^ N^ and || â¢ || is a distance operator. 
The phenotype-fitness map can be analyzed by measuring the causal 
probability Pr($|n) which is the probability of having the second condi-
tion from the above implication (4.32) in the fitness space $ ;= WfiQi) â 
/(^j)ll < 11/(^0 â fi9k)\\ in case the first condition in the phenotype 
space n := Dist{gi,gj) 
< Dist{gi,gk) 
holds. The causal probability 
Pr($|n) can be approximated by the number of causahties Cn=>$ found 
in a number of experimental trials A^^^ [Sendhoff et al., 1997]: 
Pr($|n) ^ Cu^<^/Nt 
(4.33) 
where neighbors produced by the mutation operator are used to esti-
mate the cases Cn=>$- High causal probabifity indicates a smoother 
landscape which is usually easy to search. A higher causal probability 
Pr(^|n) means that IGP better fulfills the causality principle because 
each landscape point contains information about its neighbors. Genetic 
programs that carry such information about their neighbors preserve the 
graduality from the phenotype to the fitness space. 

Fitness 
Functions 
and 
Landscapes 
103 
An algorithm for computing the probabiUty Pr($|n) is given in Table 
4.3, It was applied to examine the causality in IGP and traditional 
GP using the same computational mechanisms and the Mackey-Glass 
series for fitness evaluations. It was found that the causal probabihty 
of IGP is Pr($|n)/Gp = 0.64, while that of traditional GP is lower 
Pr($|n)Gp = 0.57 which indicates that IGP searches on a smoother 
landscape and can be expected to perform better. 
The causal probability Pr($|n) measure offers several advantages over 
the statistical measures for analysis of fitness landscapes: 1) it does not 
depend on statistical assumptions about the properties of the fitness 
landscape in order to calculate the correlations; 2) it does not require 
knowledge about the extrema on the fitness landscape; and 3) it is in-
variant under reformulations of the fitness function. Similar advantages 
provide the information measures for landscape analysis. 
Table 4.3. Algorithm for measuring the causal probability of the phenotype-fitness 
map in IGP with respect to tree-to-tree distance. 
Measuring Causality in IGP 
step 
Algorithmic 
sequence 
1. Initialization 
2. Perform 
trials 
3. Compute the 
probability 
Let the detected causahties be Cn^4> = 0 
Dist :GxG-^NJ 
-.g-^ 
R 
Neighbors^Qi) 
= {Qj\Gj = M{Gi), and DistiQi.Qj) 
= 1}. 
For a large number of trials A'^'' > 10'^ do 
a) Sample randomly a genotype Qi 
(from a uniform distribution). 
b) Generate two of its offspring by mutation 
Gj G 
Neighbors{Gi), 
Gk G NeighborslGi)-
c) Count the distances between 
the parent Gi and the offspring Gj,Gk 
Dist{Gi,Gj), 
Dist{Gi,Gk). 
d) li DistiGi.Gj) 
> 
Dist{Gi,Gk) 
exchange Gj with Gk-
e) If the causality condition 
Dist{Gi,Gj) 
< Dist{Gi.,Gk) => 
=^\\fm-fiQj)\\<\\f{Gi)-f{Qk)\\ 
is satisfied increment the cases 
Cn=>4> = C'n=>4> + 1. 
Evaluate the fraction 
Pr(<|)|n) Â« 
Cu^4>/Nt. 

104 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
4.3.3 
Information Measures 
The fitness landscape may be examined from the perspective of the 
information content of its structure [Vassilev et al., 2000]. Since the 
fitness landscape structure has a quite irregular profile, we are rather 
inclined to evaluate the informativeness of some elementary geometric 
objects that cover it, assuming that such landscape covering objects can 
be identified and estimated easily. 
Information content of a fitness landscape is the entropy of the fre-
quencies of the elementary objects that cover its structure. The el-
ements of these covering objects should carry information about the 
instantaneous changes in the landscape profile, so they can simply be 
numbers from the triple: { â 1,0,1}. This triple can be associated with 
the changes of the slopes of the landscape structure between two neigh-
boring points as follows: when the slope goes down, then its covering 
element is â1; when the slope goes up, its covering element is 1; and 
when there is no change among two neighbors, the cover is 0. During a 
random walk on the landscape, the fitnesses F = {/o, /i, â¢â¢â¢, /n} niay be 
converted into a string cover Q{e) â {qo^Qi^ â¢â¢â¢,9n} of symbols from the 
alphabet { â 1,0,1}. Each string qi E { â 1,0,1} may be envisioned as 
a landscape substructure component determined by means of a special 
function qi = ^(i,Â£:) defined as follows [Vassilev et al., 2000]: 
^{i,e)={ 
0, 
i f | / i + i - / . |<Â£ 
(4,34) 
- 1 , 
0, 
1, 
if /i+i - /,: < - Â£ 
if 1 fi+-i - 
fi\<Â£ 
if fi+i - fi> Â£ 
where e is a real number e E length[0^ max(/) â min(/)]. This parameter 
s serves to control the precision of the cover. When e = 0, the cover Q{s) 
will be very close to the landscape surface; that is, it will be sensitive to 
the landscape features. When e increases, it will make the cover Q{e) 
insensitive to the underlying landscape characteristics. 
The function ^(i,Â£) maps a step, which is a transition from a par-
ent genetic program to an offspring on the landscape {/i, /i+i} into an 
element from the string q^ G {-1,0,1}. The frequencies of each two 
different steps, with elements q y^ r on the landscape represented by a 
substring [qr] G Q{e) of length two, can be estimated as follows: 
P r , , = ^ 
(4.35) 
where nj^^j is the number of substrings [qr] in the string Q{e) such that 
q ^ r^ and n is the number of all substrings of length two within Q{e), 
Any substring [gr], q^r, 
may be considered an informative object. 

Fitness 
Functions 
and 
Landscapes 
105 
A c c u r a c y 
Figure 4.4. Curves of the information content of IGP mutation landscapes arising 
from the three operators: CPM, 
IIVLM, 
and RM. 
Each curve is computed with 
10000 points. The MSB fitnesses are evaluated using the Mackey-Glass series. 
The information content {INC) of the fitness landscape is calculated 
as the entropy of the frequencies of the different slope changes in its 
cover [Vassilev et al., 2000]: 
/Arc(Â£)--^Pr[,,,logoPr[,,] 
(4.36) 
q^r 
where the base is 6 because this is the number of all substrings of length 
two that contain different elements from the alphabet { â 1,0,1}. 
The information content reveals whether the peaks remain visible 
when we observe the landscape from more and more distant points of 
view. If the peaks remain, this means that these are high peaks of a 
rugged landscape. The information content curves of three mutation 
landscapes arising in IGP are plotted in Figure 4.4. 
Figure 4.4 shows that the context-preserving mutation {CPM) [Niko-
laev and Iba, 2001a] has a smoother landscape than the hierarchical 
variable-length mutation {HVLM) 
[O'Reilly, 1995], and the replace-
ment mutation (RM) operators (Subsection 2.2.2). 
The smoother fitness landscape of the {CPM) operator can be ex-
plained with the changes of the information content curves of the stud-
ied landscapes as follows: 1) the curves start from a common point 0.41, 
which means that the three mutation operators have landscapes with 

106 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
approximately the same diversity of peaks; 2) the curve raisings in the 
interval [0,1.8] demonstrate that all the mutation operators climb or 
descend unevenly on their landscapes, which are rugged; 3) within the 
interval [1.8,9.5] the number and height of the insubstantial peaks on the 
landscape of the CPM operator is smaller than these on the landscapes 
of HVLM 
and RM] that is, the CPM landscape is smoother; and, 4) 
the shared termination of the curves at 11.5 denotes that all landscapes 
have the same number of essential peaks. Prj^^j = '^[qr]/'^" 
Table 4.4 presents the algorithm for measuring the information con-
tent of fitness landscapes. This algorithm involves initially performing 
a random walk to sample a large number of oflfspring and evaluating 
their fitnesses. After that, the recorded fitness arrays are used to: deter-
mine covers, estimate frequencies of substrings, calculate entropies, and 
increment the information content for a number of accuracies. 
Table 4.4. Algorithm for measuring the information content of fitness landscapes 
that arise in IGP with tree-like genetic programs. 
Measuring Information Content in IGP 
step 
Algorithmic 
sequence 
1. Initialization 
2. Perform 
a random walk 
3. Compute the 
information 
content 
Let the fitness function he f : Q -^ R. 
Generate a parent tree Qp = 
RandomTree{). 
For a large number of trials n > 10^ do 
a) Sample an offspring Qc of Qp. 
b) Record its fitness: 
/c = /(e;c)mF = {/,,/2,...}. 
c) Replace the child to become parent: 
Gp â Qc-
For a number of accuracies Â£ = 1 to 20 do 
a) Consider the fitness array F = {/i, /2, â¢-., /77,}-
b) Determine the cover Q(e) of elements qi = ^(i,e) 
using the function 
r - 1 , 
i f / , H - / , < - Â£ 
^{i^â¬)={ 
0, 
if \fw-fi\<e 
. 
[ 1, 
if/,:n 
-fi>e 
c) Estimate the frequencies of substrings 
[qr], g^^r, in Q{e) 
Pllqr] 
= 
niqr]/n. 
d) Calculate tfie entropies 
/iVC(Â£) = - E , ^ ^ P r | , , , l o g , P , v | . 
e) Increment the information content 
INC^ 
INC-\-INC{e). 

Fitness Functions and Landscapes 
107 
4.3.4 
Quantitative Measures 
Quantitative statistics for the fitness landscape structure can be ob-
tained by counting such characteristics as the distribution of local op-
tima, the depths of local optima, and the dimensions of the basins be-
tween them. These landscape characteristics can be measured by per-
forming a large number of descents with the IGP algorithm to some near 
local optima. The downhill climbings on the landscape should be imple-
mented as follows: start from a random genetic program whose fitness 
is a random point on the landscape, generate its neighbors, determine 
their fitnesses, select the fittest among the neighbors which is fitter than 
the current genetic program, and continue to move down until reaching 
a genetic program fitter than all of its neighbors. 
Looking at a fitness landscape surface we are interested in finding out 
what the depths of the optima are, and how they are distributed. A 
large number of runs with IGP and OF were conducted to collect the 
depths and frequencies of the local minima on their mutation landscapes. 
One can see in Figure 4.5 that the mutation landscape in traditional GF 
features by more local optima. These local minima in GF occur more 
frequently, which means that its landscape is more rugged. The IGF 
landscape seems to be smoother with flatter hills. 
A useful property of the fitness landscape, that can be detected us-
ing the same downhill cfimbing procedure, is the average basin radius. 
Basin radius is the number of steps performed by the algorithm until a 
particular local optimum is reached. The basin radius on the landscape 
can be computed as the average distance, in the sense of the number of 
steps from a parent to a child genetic program, traversed by the IGF 
system, after which it converges to a local optimum. Having informa-
tion about the dimensions of the basins between the local optima on the 
landscape can be used to improve the IGF search navigation. 
During the same runs with IGF and traditional Koza-style GF, there 
were recorded the number of downhill steps to reach the closest minima 
on their mutation landscapes. A scatter plot of the measured radiuses 
of the basins between the local optima is presented in Figure 4.6. This 
figure shows that the IGF system evolves on a landscape with larger 
basins as it makes longer walks. Respectively, the GF system quickly 
descends to the local minima as it seems that there are a lot of them. 
The quantitative measurements of the landscape characteristics, illus-
trated in Figures 4.5 and 4.6, show that the local optima on the IGF 
landscape are less, and they feature by larger differences than from the 
other points. The mutation landscape in IGF is smoother, and the cor-
rect directions toward the optima on it could be identified faster. In this 
sense, one may think that the IGF landscape facihtates the search. 

108 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
F i t n e s s 
of L o c a l 
O p t i m u m 
Figure 4.5. Distribution of local optima on a mutation landscape collected during 
1000 downhill climbings to random optima. 
0 
5 
m 
i G P 
O 
GP 
m 
m 
# 
i 
# 
Â«o o 
mm 
^ 
m 
m 
m 
m 
m 
Â« CD 
# 
mm o 
# 1 
m 
m 
mm 
# 
m 
m 
m 
o o 
m 
Bm mo oo 
mm oHDio cnrÂ»cc(D3 o o 
OD mvD c s <KDm 
OmX)m â¬> (D CMD OODfD OTD 
(SD(D dDO 
ODO 
CXXDO 
^ 
O (DO 00#OD 
ODO 
O OCC 
, 
^^ 
^â¢.. 
1 , ,^ââ..-.â ,.â¢^ 
2 5 
5 0 
7 5 
R a n d o m 
W a l k 
N u m b e r 
Figure 4.6. Average distance to local optima on a mutation landscape collected 
during 1000 downhill climbings to random optima. 

Fitness 
Functions 
and 
Landscapes 
109 
o 
â¢ H 
Di 
(D 
> 
00750 
0 0 6 2 5 
00500 
O 
8 
- 8 
Â° 
9 r 
8 
Â§ 0 0(ep/Q 
0 
8 
' ^ ^ 0 
^ 
% 
W 
0 GP 
0 
â¢ 
I G P 
, . ! , . . 
1 
0 . 0 0 7 5 0 
0 . 0 1 1 2 5 
Fitness of Point 
Figure 4.7. Steepness of the local optima on a mutation landscape collected during 
1000 downhill climbings to random optima. 
The next question that can be raised after knowing which landscape 
is smoother, is how smooth it is; that is, how steep are the optima on 
the fitness landscape. The steepness of the local optima^ computed in 
the experiments with IGP and traditional Koza-style GP, are illustrated 
in Figure 4.7. It shows that the mutation landscape of traditional GP 
has local peaks with very steep slopes, which strengthens the confidence 
that its landscape is rugged, while the slopes of the mutation landscape 
of IGP are not so steep. 
4.4 
Chapter Summary 
This section studied how to design fitness functions for evolutionary 
search control in IGP with polynomial networks. Empirical analysis of 
their usefulness was made with special measures, defined using the notion 
of fitness landscape. Assuming that the fitnesses of the individuals in the 
population bred by IGP form a fitness landscape, allows us to evaluate 
how the population moves during evolutionary search. The possibility 
to examine the population flow on the fitness landscape allows us to 
investigate the goodness of the IGP search mechanisms and provides 
hints as to which of them should improved. 
The presented fitness functions are usually applied separately with 
respect to the inductive task and the desired characteristics from the so-
lution. Another useful strategy followed often in practical applications 

n o 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
is to design multiobjective fitness functions by collecting several criteria 
into one common fitness function. For example, statistical and proba-
bilistic criteria could be incorporated into a shared fitness function. In 
such cases the fitness landscapes become slightly more complex, but they 
still remain amenable to analysis by the presented measures. 
The recommendation for developing IGP systems is to study the suit-
ability of all its micromechanisms by generating their landscapes using 
the selected fitness function for the given problem. The proposed fitness 
landscape measures are useful tools for examination of the evolutionary 
search control in IGP. They provide empirical evidence for the opera-
tion of the control machinery and reveal how it navigates the inductive 
learning process. The fitness landscape measures make IGP advanta-
geous over the other machine learning and neural network paradigms 
because they can help to find which learning mechanisms need repair 
and show how the learning proceeds. 

Chapter 5 
SEARCH NAVIGATION 
IGP is a general-purpose tripartite search system whose control mech-
anism works with three evolutionary operators: reproduction, crossover, 
and mutation. While the genetic crossover and mutation operators sam-
ple hypothetical solutions by changing the genetic programs, the repro-
duction operator implements the search heuristic. The reproduction has 
the role to stochastically direct the algorithm toward promising search 
areas, aiming to locate the best solution. 
IGP systems are developed with the intention of performing efficient 
evolutionary search. Such a system may search efficiently if it is able to 
adapt to the characteristics of the fitness landscape, in the sense that 
it can escape entrapment in suboptimal local minima. It should have 
the potential for sustained exploitation as well as exploration of the 
landscape. The control mechanism should achieve broad examination 
of yet unvisited distant areas and narrow examination of close areas 
of the search space. The IGP adaptation on the fitness landscape is 
extremely difficult because its dimensionality is not known. The reason 
for having an enormous landscape is the irregularity of the neighboring 
relationships between the variable size genetic programs. 
The stochastic IGP search strongly depends on capacity of the control 
mechanism to navigate the population successfully on the fitness land-
scape. An indication for successful navigation is continuous improvement 
of the fitnesses in the population. This can be done by maintaining high 
population diversity and using as much information as possible from the 
good, fit individuals. The good individuals carry and transmit informa-
tion about the history of the evolution, thus they point out promising 
directions. There are, however, inferior locally optimal individuals that 
can cause search stagnation, as they can mislead the search. 

112 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
This section studies how to implement control mechanisms for IGP 
search navigation, including several reproduction operators and advanced 
search control techniques. The reproduction operators determine the 
policy for breeding the successive populations. The mechanisms for ad-
vanced control contribute to moving the population continuously on the 
landscape, and to attaining progressive search. 
An efficient IGP search navigation is one that has the capacity to 
adapt on the fitness landscape. This is the property of being able to gen-
erate better offspring, also known as evolvability [Altenberg, 1994a, Al-
tenberg, 1994b]. The operators should act on the population in such a 
way so as to produce individuals with improving fitnesses. Assuming a 
concrete representation, the genetic learning and reproduction operators 
should profiferate partial solutions in the form of building blocks which 
with high probabihty improve the genetic programs. The learning oper-
ators should preserve and replicate the good building blocks within the 
genetic programs. The reproduction operator should keep and transmit 
individuals containing good building blocks. 
The common aspect of the desired evolvability phenomena is how 
much the off"spring diff'er from the parents. This aspect can be examined 
in IGP from diff'erent points of view through diff'erent observations of the 
search performance. Several easily computable performance measures 
are off'ered to facilitate practical studies into the evolutionary search be-
havior of IGP. Four groups of estimates are presented: measures of the 
evolvabihty of the fitnesses, convergence measures of the evolvability of 
the errors, diversity measures of the evolvability of the syntactic repre-
sentations, and physical measures of the population dynamics. These 
measures can be used to tune the genetic learning and reproduction 
operators in order to improve the IGP system. 
5.1 
The Reproduction Operator 
Reproduction is the operator for organizing survival of the fittest ge-
netic programs; it decides which individuals should be propagated during 
the generations. Its role is twofold: to stimulate selection of highly fit 
individuals, and to maintain diversity in the population so as to sus-
tain the evolution. In the beginning of the evolution it should impose a 
low pressure to the better individuals, otherwise they will take over the 
population too early and the search will stagnate prematurely. In the 
later stages it should exaggerate the diff'erences between the individuals 
in order to identify the fitter individuals, because the fitnesses become 
very similar. Premature convergence occurs when the best individuals 
converge to a suboptimal basin on the landscape and remain there before 
reaching a sufficiently optimal minimum. 

Search Navigation 
113 
The reproduction has two ingredient phases implemented by corre-
sponding strategies: selection and replacement. The selection strategy-
chooses the parents to produce offspring in the next generation. The re-
placement strategy decides which individuals to remove from the current 
population and exchange with offspring. 
5.1.1 
Selection Strategies 
The selection operator should guide the search by imposing enough 
pressure on the good individuals in the population. The selection pres-
sure can be measured by estimating the reproduction rate of the best 
individual during successive generations. Reproduction rate is the ra-
tio between the copies of a particular individual before performing the 
selection step and after this step. If the reproduction rate of the best 
individual increases rapidly, this indicates that the selection pressure is 
high and the algorithm will eventually converge too early. At the other 
extreme, if the pressure is very low the algorithm may become too weak, 
disoriented and it may wander on the fitness landscape without being 
able to locate areas with promising individuals. This pressure depends 
on the chosen selection scheme and on their characteristics which are 
needed to develop working IGP systems. 
The most popular selection strategies that can be used in IGP are 
fitness proportionate selection, ranking selection, truncation selection, 
and tournament selection. 
These selections were originally designed 
for genetic algorithms [Back and Hoffmeister, 1991, Back, 1996, Baker, 
1987, Bhckle and Thiele, 1997, Goldberg, 1989, Grefenstette and Baker, 
1989], but they can be used in GP as well [Koza, 1992]. The problem 
is that there is no guarantee that their proven properties will be trans-
ferred directly because they have been derived for genetic algorithms 
that typically work with fixed length representations. Despite this rig-
orous analysis of selection properties such as takeover time, convergence 
time, selection variance, and selection intensity in the context of GP 
is still lacking. These selection schemes have to be studied in order to 
control the pressure that they impose during the evolution. 
Fitness Proportionate Selection. The fitness 'proportional selec-
tion [Holland, 1975] gives each individual a probability Vi{QJ) to repro-
duce proportional to its fitness and inversely proportional to the sum of 
all population fitnesses: 
where QJ is the z-th individual (tree-like genetic program) at generation 
r, and TT is the population size. 

114 
ADAPTIVE 
LEARNING 
OF POLYNOMIAL 
NETWORKS 
Tabic 5.1. Algorithm for implementing roulette-whccl selection that can be used 
in IGP with tree-like genetic programs. 
Fitness Proportionate Selection 
step 
Algorithmic 
sequence 
1. Initialization 
Population of genetic programs ^,;, 1 < z < TT 
Wheel with sections, starting with s = 0. 
2. Develop the 
For the whole population z = 1 to TT 
roulette wheel 
Compute the sections 
s,: = s , : - , + / ( e . ) / E ; , , / ( e . ) ' 
3. Select promising 
For the whole population 2 = 1 to TT 
individuals 
a) Spin the wheel by drawing a random number 
r = Rand{<d,l)\ 
b) Pick a genetic program 
Qi^ such that s^-i < r < s,;. 
As the sum of the selection probabihties of all individuals in the pop-
ulation is one, we can imagine that the individuals are allocated in the 
slots of a roulette wheel, and each individual is granted a number of slots 
proportional to its selection percentage. The selection operator picks in-
dividuals using a pointer. The fitness proportional selection mechanism 
suggests to spin the wheel, and to take the current individual chosen by 
the pointer after each spin. When a parent is needed, the wheel is spun 
again and another individual is chosen. Thus, the fitness proportional 
selection transfers more copies of the fitter individuals to the next gen-
eration since they occupy larger slices from the wheel. Examination of 
the wheel allows us to determine how the search proceeds. 
The fitness proportionate selection algorithm for reproduction of the 
whole population from one generation to the next is given in Table 5.1. 
The fitness proportional selection is sensitive to the magnitude of the 
fitnesses. One remedy to this problem is to change the fitness scaling. 
The scaled raw fitnesses are called adjusted fitnesses [Koza, 1992]. The 
values of the objective function can be transformed into adjusted fit-
nesses so as to diminish the differences between them and to improve the 
competition. There are various scaling techniques such as linear scaling 
[Goldberg, 1989], truncation scaling [Goldberg, 1989], exponential scal-
ing [Grefenstette and Baker, 1989], and logarithmic scaling [Grefenstette 
and Baker, 1989], that may also be used in IGP. 
A drawback of fitness proportionate selection is that it does not allow 
control of the number of accepted offspring. It may happen that a small 
number of individuals contribute a large number of children to the next 
generation. The symptom of such degenerated performance is a rapid 

Search Navigation 
115 
loss of diversity. One possibility to combat the loss of diversity is to 
assign different selection probabilities to each individual. This can be 
done by ranking the individuals so as to predetermine their involvement 
in the reproduction process. 
Ranking Selection. The ranking selection [Grefenstette and Baker, 
1989, Whitley, 1989] takes the individuals according to their arrangement 
in the population in increasing or decreasing order of fitness. The best 
individual is assigned rank one and the worst is given rank equal to 
the population size. The linear ranking scheme determines the selection 
probability of an individual with the following function [Baker, 1987, 
Grefenstette and Baker, 1989]: 
Pr(gJ) = l(a + (/3-a)-f-j-M 
(5,2) 
TT V 
TT â 1 
/ 
where QJ is the z-th genetic program at generation r, rank{QJ) is its 
rank in the population, TT is the population size, a is the proportion for 
selecting the worst individual, and (3 is the proportional for selecting 
the best individual. When the conditions a ^- j3 = 2 and 1 < a < 2 
are satisfied, the best individual will produce no more than twice the 
offspring than the population average. 
There are several modifications: linear ranking, nonlinear ranking, 
and exponential ranking [Blickle and Thiele, 1997, Michalewics, 1992]. 
They can be implemented using the same algorithmic framework given 
in Table 5.1 by exchanging formula (5.2) with formula (5.1). The linear 
ranking selection has been applied to GP [Koza, 1992] using a factor 
rm. for the parameters a and (5 that simulates the function gradient as 
follows: a = 2/(r^ -f 1), /? = 2rm/{rm + 1). 
Truncation Selection. A similar selection scheme is the uniform 
ranking [Back and Hoffmeister, 1991], also known as truncation selec-
tion [Miihlenbein and Schlierkamp-Voosen, 1995]. This scheme chooses 
for reproduction only top individuals having high rank in the current 
population, and these top individuals have the same chance to be taken. 
The fraction of the top individuals is defined by a rank threshold /i. The 
selection probability of the truncation selection scheme is: 
where rank{Ql) is the rank of the i-th individual QJ in the population 
at generation r. The fitnesses are used only for ordering of the popu-
lation f{Qi) < /{Gl) ^ â¢â¢â¢ ^ /(^TT)) ^^^ they do not participate in the 
selection. The advantage of truncation selection is that it is not affected 
directly by the fitnesses or by the scaled adjusted values. 

116 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
Tournament Selection. Tournament selection is the most widely 
used scheme in GP [Koza, 1992]. The tournament selection randomly 
picks individuals from the population and promotes the best of them to 
survive. A tournament requires to draw random tuples {Ql^^QJ^^ "iSJ^}) 
1 < i < TT, of /. individuals and next to choose the best one among them: 
f{GJ) < fiGl)^ for each Gl from {G^^GJ,^ ..^GJ,}^ Such ^tuples have 
to be drawn a number of times equal to the size of the population TT 
for producing a whole new population. Assuming that the individuals 
are ordered so that the best individual has lowest index, the selection 
probability of tournament selection for picking the i-th individual can 
be determined as follows [Back, 1996, Bhckle and Thiele, 1997]: 
PvigD^-^iin-i 
+ lY-in-if) 
(5.4) 
where TT is the population size, and t is the tournament size. The selec-
tion pressure can be controlled by the tournament size. The tournament 
selection pressure toward the best individual becomes higher with the 
increase of the tournament size t. 
The competition between the individuals in the tuples is also sensitive 
to the fitness values. This problem could be alleviated by organizing 
Boltzmann tournaments [Michalewics, 1992]. Boltzmann competitions 
are performed with pairs of individuals. The superiority of one of them 
is decided with the formula: 
^'^^''^ ^ l + e x p ( ( / ( a r , ) - / ( a ; ) ) / T ) 
^^-^^ 
where /{GlJ and /{GJ^) are the fitnesses of the individuals from the 
tournament pair, and T is a regulating temperature parameter. Initially, 
T is set large so that the choice of a winner is random. By gradual 
decreasing of the temperature T, the strategy more clearly distinguishes 
the individuals with better fitnesses and they are given greater chances 
to win the tournament in the later generations. 
Universal Selection. The above popular selection schemes can be 
generalized in a common universal selection mechanism [Blickle and 
Thiele, 1997]. The universal selection is developed with intention to 
reduce the large variation of the fitnesses after reproduction. It uses 
stochastic universal sampling (SUS) [Baker, 1987] for diminishing the 
variance of the number of offspring from each individual. The SUS al-
gorithm spins the roulette wheel only once with fitness proportionate 
sections for each individual, rather than spinning it a number of times 
equal to the necessary offspring. The pointer is randomly generated, and 
after that equally spaced spikes pointing out which individuals to take 
for reproduction are installed. 

Search Navigation 
117 
Table 5.2. Algorithm for selection using stochastic universal sampling for IGP. 
Universal Selection using SUS 
step 
Algorithmic 
sequence 
1. Initialization 
Population of genetic programs Qi^\ < i < TT. 
Precomputed selection rates Si = Pr(^,;), I < i < TT. 
Wheel with sections, staring with s = 0. 
2. Spin the 
Spin the wheel by drawing a random number 
wheel once 
r = Rand{0^ 1). 
3. Select individuals 
For the whole population z = 1 to TT 
using spikes 
a) Let the offspring of i are c â I. 
b) Compute the wheel slice 
s = s -\- Si. 
c) While (s > r) pick i using spikes r 
i) c = c + 1. 
ii) r = r + 1. 
SUS can be made into a universal selection algorithm by using the 
selection probabilities Fv{Ql), computed in advance to determine the 
slices in the roulette wheel. The SUS algorithm will produce the ex-
pected number of children only when the selection probabilities account 
precisely for the number of offspring. Then SUS is guaranteed to choose 
an individual ^J, 1 < ^ < TT, a number of times proportional to its selec-
tion rate Si = Pr(^J'). This universal selection algorithm for processing 
the whole population is presented in Table 5.2. 
5.1.2 
Replacement Strategies 
After selecting promising individuals for the next generation, they 
have to be accommodated in the population. The accommodation in-
volves removing inferior individuals from the population so that the 
promoted ones take their places. The decision of which individuals to re-
move can be made using the following replacement strategies [Chakraborty 
et al., 1996, De Jong and Sarma, 1992, Smith and Vavak, 1998, Syswerda, 
1991]: random deletion, deletion of the worst individual by ranking, dele-
tion of the worst individual by tournament, and deletion of the oldest in 
the sense generational persistence in the population. 
The random deletion scheme suggests to remove randomly picked in-
dividuals from the population. This scheme is straightforward to imple-
ment, but it may destroy and lose many good individuals. The strategy 
deletion of the worst by ranking is to a great degree reasonable for large 
populations, however for smaller populations the worst individuals to die 
should be determined using tournaments. Deletion of the worst by ''kill 

118 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
tournament^' better preserves the nondeterministic character of the evo-
lutionary search than deletion of the worst by ranking. The scheme dele-
tion of the oldest may outperform the other replacement strategies, but 
the problem is how to determine for how many generations to keep the 
worst individuals. Conservative replacement is a policy that combines 
deletion of the oldest and kill tournament. The conservative replace-
ment strategy performs inverse tournaments with pairs of individuals 
and removes the older of them. 
5.1.3 
Implementing Reproduction 
The population can be updated with different implementations of 
the reproduction operator made according to several criteria [Back and 
Hoffmeister, 1991]: dynamics^ retention^ elitism^ and steadiness. 
The selection probability of the individuals depends on whether the 
promising parents are chosen with respect to their fitness or with respect 
to their ranks in the population. Since the fitnesses of the individuals 
change during the generations, a selection scheme is dynamic when it 
depends on the fitnesses of the individuals FY{QI) 7^ C^, 1 < i < TT, r > 0, 
where Q are constants. Respectively, a selection scheme is called static 
when it depends on the ranks of the individuals which are fixed in the 
population PT{QI) = C^;, 1 < Z < TT, r > 0, where Q are constants. 
There are selection schemes that allow any individual to generate off-
spring with non-zero probability, and there are schemes that completely 
discard some individuals from consideration. A selection scheme that 
gives greater than zero selection probability to each individual in the 
population Fi{Ql) > 0, 1 < z < TT, r > 0, is preservative. A selection 
scheme that deliberately discards some individuals by assigning zero se-
lection probabihty to them Fr{Ql) == 0, 1 < z < TT, r > 0, is extinctive. 
The extinctive selection scheme enhances the diversity by choosing in-
dividuals at large distance apart. 
The reproduction operator may give indefinite lifetime to the fitter 
individuals from the population. When the reproduction preserves and 
promotes a small number of best individuals to give birth of offspring 
until their fitness dominates the fitnesses of the remaining individuals in 
the population, it is called elitist. The extension of the lifetime of the 
better fitness individuals helps to reduce the fitness variance between the 
generations and to achieve more stable performance. The number of the 
best retained individuals is regulated by a coefficient of elitism k: 1 < /c, 
and /c <C TT. Assuming a minimizing fitness function, a reproduction 
operator is /c-elitist if /{gj) 
< fiGl"^) 
for each i E {1,2,...,/c}. If the 
reproduction operator does not keep any individuals, it is pure. 

Search Navigation 
119 
A reproduction operator that replaces the entire previous population 
with the offspring is called generational The generational reproduction 
usually leads to a large fitness variance which, however, makes the search 
difficult to control. The recommended strategy (especially for IGP) is 
steady-state reproduction. The steady-state reproduction operator se-
lects several individuals to mate, and replaces them with their offspring 
[De Jong and Sarma, 1992, Syswerda, 1991, Whitley, 1989]. The degree 
of overlapping between the populations can be monitored by a para-
meter for regulating the increase or decrease of the number of parents, 
called generation gap [De Jong and Sarma, 1992]. The generation gap 
is defined by the fraction Z/TT, where i denotes the desired portion of 
the population for replacement. If only one individual from the current 
population is exchanged, then the generation gap is I/TT. 
5.2 
Advanced Search Control 
The development of IGP systems needs not only different alternatives 
for making the evolutionary micromechanisms, like model representa-
tions, fitness functions and reproduction strategies, but it also needs 
different techniques for imposing adaptive search control. The notion of 
adaptive control means that the objective is to improve the navigation in 
such a way that the IGP search climbs and descends easier on the fitness 
landscape; that is, the population self-orients well on the concrete land-
scape. Otherwise, if the IGP guidance is not properly made the search 
may degenerate into random. Several advanced control techniques for 
macroevolutionary search, search by genetic anneaUng, demetic search, 
and coevolutionary search, are presented below. 
5.2.1 
Macroevolutionary Search 
High diversity in the population can be achieved through examina-
tion of the relationships between the genetic programs. The interac-
tions between the individuals can be used to improve the reproduction 
and to organize macroevolutionary search [Marin and Sole, 1999]. The 
macroevolutionary search exploits the connectivity among the individu-
als at a higher level of selection control, which is different from the clonal 
selection in the immune algorithm where the connectivity between the 
individuals is made at lower level within the fitness function. 
The model of macroevolution decides which individuals will survive in 
dependence of their mutual interactions. In context of IGP, interactions 
can be the relationships between the fitnesses of the individuals because 
the fitnesses are the property that determines their survival probability. 
The selection may be elaborated to choose individuals with respect to 

LijT = 
' y 
, ; ; / ^ 
(5.6) 
120 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
their fitness difference from the other members of the population. Higher 
difference between the fitness of an individual and the remaining ones 
means that this genetic program carries specific information and has 
to be promoted further. The relationships among the individuals are 
such that the specific genetic programs are stimulated, while the similar 
genetic programs are suppressed. Similar genetic programs exploit the 
same area on the fitness landscape. 
The strength of a link among two individuals QJ and GJ^ I < i^j < TT^ 
can be defined as follows: 
fjGI) - fjGJ) 
/max(0^) 
where /{GD and f{Gj) are the corresponding fitnesses, and /max(5^) is 
the maximal fitness at this generation r. 
The reproduction operator has to be changed slightly for accommo-
dating the connectivity relationships. The selection has to be modified 
to consider the macrofitnesses of the individuals. The macrofitness of a 
particular / ' genetic program is the sum of its fitness distances from the 
remaining genetic programs: 
nGI)= 
f: 
LI,- 
(5.7) 
where L[- are the strengths of the interactions between the i-th individ-
ual and all other individuals available in the population at the current 
generation r. Individuals with macrofitness less than a certain thresh-
old, predefined in advance, become extinct. The extinct individuals may 
be replaced by copies of the surviving individuals, by their offspring, or 
by freshly generated random individuals. 
5.2.2 
Memetic Search 
Memetic search [Moscato and Norman, 1992, Merz and Preisleben, 
2000] is a hybrid technique that integrates local search in GP for better 
adaptation to the fitness landscape of the concrete task. The memetic 
algorithm performs population-based search with local improvement of 
the newly generated offspring. Following a hypothesis from the biological 
genetics, the idea is to conduct local search starting from each offspring. 
The local optima reached from the offspring is taken for continuation 
of the search instead of simply the offspring. Therefore, the memetic 
algorithm searches using locally optimal individuals rather than using 
arbitrary individuals. Although in the beginning the evolutionary search 
starts with random individuals, during the generations the locally opti-
mized individuals tend to occupy the population. 

Search Navigation 
121 
Table 5.3. Algorithm for memetic search that can be used in IGP. 
Memetic Search 
step 
Algorithmic 
sequence 
1. Initiahzation 
Population of genetic programs Qi,l < i < n. 
2. Memetic learning 
For a rmmber of generations r 
For the whole population z = 1 to TT 
a) Select, cross or mutate, to produce an offspring Qi. 
b) Improve this offspring Qi by downhill climbing 
Q' = 
LocalSearch{Qi). 
c) Evaluate its fitness 
/(go = EvaliGi). 
d) Replace the worst with this offspring, 
e) Rank the population according to fitness. 
If the termination condition is satisfied stop. 
The effect of the genetic crossover and mutation operators in memetic 
search is performing large jumps on the landscape, while short downhill 
steps are performed by the local improvement operator. In case of a 
minimizing fitness function, the goal is descending to the deepest basin 
on the landscape. The crossover and mutation operators organize both 
exploration of the search space, while the local downhill climbings exploit 
the close neighborhoods of the search space. Local search algorithms op-
erate until reaching the nearest local minima. In this sense, the memetic 
search is a hybrid technique because the role of the local search operator 
is different from the role of the genetic search operators. 
There are two main approaches to implementing local search for the 
memetic algorithm: deterministic and stochastic. The deterministic lo-
cal search samples all neighbors of the given individual, selects the best 
of these descendants, and so directs the process, pushing it downhill un-
til reaching the closest minimum. The stochastic local search randomly 
generates only one neighbor of the given individual and takes it with 
probability proportional to its fitness. This stochastic version avoids the 
complete enumeration of all neighbors of the current search point. It 
is more efficient for IGP because the sizes of neighborhoods of tree-like 
individuals are usually large. Due to the nondeterministic selection of 
the next downhill move, it has abilities to avoid rapid convergence and 
entrapment into the closest optima, hence it can reach distant optima 
on the fitness landscape surface. 
A memetic algorithm implemented for IGP style population-based 
search is illustrated in Table 5.3. 

122 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
5.2.3 
Search by Genetic Annealing 
The reproduction can be improved using simulated annealing [Kirk-
patrick et al., 1983]. Simulated annealing gives the conditions for choos-
ing which among the parent and the offspring should be retained in 
the population. This algorithm more precisely guides the evolutionary-
search so that in the early stages, even not necessarily good individuals 
may be tolerated to stay in the population, while at the later stages 
focusing to better genetic programs is stimulated. 
Simulated Annealing. Simulated annealing is conceived as an op-
timization algorithm [Kirkpatrick et al., 1983] that simulates evolution 
from an initial state to a random state in the search space, in the same 
way in which a physical system evolves from an initial configuration of 
its macroscopic variables to an equilibrium configuration that satisfies 
some constraints. The intention for using this algorithm is to perform 
guided transitions from one state in the space of hypotheses to another 
state, driven by the same dynamics as this that drives the behavior of 
the physical system. The motivation is that the dynamics of the physical 
system provides abifities to escape from early entrapment in suboptimal 
solutions. The two driving mechanisms that can be adopted for search 
control are relaxation and annealing. 
The relaxation mechanism probabilistically samples hypotheses in the 
search space in such a way that it guarantees movement toward better 
solutions. It chooses hypotheses depending on their energy diff'erence. 
Transition from a parent to an off'spring hypothesis occurs with a prob-
abifity defined using the Boltzmann distribution: 
Pr(g,) = { 
' 
, ^,. 
/,,, 
*^ ^^^" = ^1^'^-^) - ^"â¢(^') < Â° 
(5.8) 
^ ^^ 
y exp 
{-AEn/kT) 
otherwise 
^ 
' 
where En(Qi-^i) and En{Qi) are the energy characteristics of two suc-
cessive hypotheses, /c is a Boltzmann's constant, and T denotes the cur-
rent step, usually called temperature. The notion of temperature means 
that the mechanism relaxes toward better hypotheses at each next step 
with the decrease of the temperature. A hypotheses with higher energy 
AEn = En{Qi^i) â En{Qi) > 0 may occasionally have a chance to be 
accepted, mostly in the early generations when the temperature is high. 
The annealing mechanism serves to control the convergence so that 
it proceeds according to a predefined coofing schedule. The schedule 
specifies with what rate to lower the temperature so as to favor the 
appropriate hypotheses at the current step. If the schedule decreases 
the temperature too rapidly, some good hypotheses may be rejected, or 
if it decreases the temperature too slowly the hypotheses may become 
indistinguishable. This is why a sufficiently slow modification of T is 

Search Navigation 
123 
recommended; T'^+^ = T'^/Inr^ where r is the number of the iterative 
step. The temperature parameter is initialized according to the equation: 
T^ = âAEnP/lnO.dd, where AEnP is the expected largest difference 
between the energies. The typical values for the Boltzmann's constant 
lie in the interval k G [0.5,0.9]. 
Table 5.4. Algorithm for evolutionary population-based search by genetic anneal-
ing that can be used in IGP with tree-hke genetic programs. 
Genetic Annealing 
step 
Algorithmic 
sequence 
1. Initialization 
Population of genetic programs Qi^l < i < TT. 
Energies Em â f{Gi), Boltzmann's constant k G [0.5,0.9]. 
Temperature T = -A^nÂ°//n0.99. 
2. Genetic 
For a number of generations r 
annealing 
For the whole population z = 1 to TT 
a) Select, cross, or mutate, evaluate the offspring Qi. 
b) Perform relaxation with the following loop 
i) Compute the energy En{QiJ\^i) = f{Gi+\)-
ii) a En{gi+^) - En{Gi) < 0 
Accept, and put ^i+i in the population 
else Accept with probability 
PiiQi) = exp{-(En{gi^^i) 
~ 
En{g,))/kT), 
Update the coohng schedule 
T'^-' = 
-r/lnr. 
Continue genetic annealing: go to step 2 
until the termination condition is satisfied. 
Relaxation and Annealing. The search process can be focused 
using genetic annealing with two phases: genetic relaxation with prob-
abilistic acceptance of the offspring, and annealing by cooling with the 
temperature parameter. This algorithm can be applied to IGP using 
the fitnesses as energy characteristics. The survival of an individual is 
determined relatively to the energy of its parent. If an offspring has a 
lower energy then its parent, then it is promoted to the next popula-
tion. Otherwise, if an offspring is worse than its parents, the survival of 
the offspring is decided probabihstically according to the Boltzmann's 
distribution. The genetic anneahng algorithm is given in Table 5.4. 
The annealing process is controlled by a cooling schedule which helps 
to sustain the search progress. The operation of the cooling schedule 
depends on a special temperature parameter. If the cooling tempera-
ture parameter is not adjusted properly the population may wander on 
the landscape without locating a good solution, or it may converge pre-

124 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
maturely to an inferior solution. Most convenient is to perform uniform 
cooling with a common temperature parameter for all individuals in the 
population. There are various formula for updating the cooling sched-
ule, some of which depend on the energies of the individuals. However, 
it is much more rehable to perform temperature coohng that does not 
depend on the individuals in the population. 
5.2.4 
Stochastic Genetic Hillclimbing 
Stochastic genetic hillclimbing search [Ackley, 1987] is a complicated 
version of the genetic annealing algorithm. Stochastic genetic hillclimb-
ing is a general algorithm for sustained adaptation on the search land-
scape that does not depend on the particular kind of the fitness function. 
It regulates the population through elaborated selection and replace-
ment mechanisms. Individuals are picked with probability proportional 
to their displacement relative to the average population fitness, rather 
than proportional to the improvement from the parent. The probabilis-
tic strategy is such that initially it tolerates arbitrary individuals, while 
later in the search it encourages individuals with above-average fitness. 
The average population fitness is enforced to decrease so as the popula-
tion descends downhill on the landscape. 
The selection mechanism in stochastic genetic hillclimbing determines 
the survival probability of an individual with respect to the average 
population fitness and the recent generational history of fitness values. 
An individual which is an improvement to the average population fitness 
and the fitness history is promoted further to the next generation, in 
other words, it is pushed to move on the landscape. The mean for 
stochastic control of the survival probabilities of the individuals is the 
recent fitness history. The recent history of fitness values 6 is updated 
after evaluating each next offspring according to the equation: 
e = pfav + il-p){f-S) 
(5.9) 
where / is the actual fitness of the offspring, fav is the average population 
fitness, p is retention rate 0 < /O < 1, and 6 is disturbance parameter. 
The possibihty to adjust the retention rate p in formula (5.9) allows 
us to balance the influence of the average population performance on 
the search process. The purpose of the disturbance parameter 6 is to 
stimulate modification of even very good individuals. Otherwise, if there 
is no reinforcement of the good individuals, the search in their neigh-
borhood stagnates and cannot progress in the vicinity of the landscape 
outside these locally optimal basins. The stochastic genetic hillclimbing 
algorithm is given in Table 5.5. 

Search Navigation 
125 
Table 5.5. Algorithm for doing stochastic genetic hillclimbing with IGP. 
Stochastic Genetic Hillclimbing 
step 
Algorithmic 
sequence 
1. Initialization 
2. Genetic 
hillclimbing 
Population of genetic programs ^i, 1 < i < TT. 
Average population fitness fav, Fitness history 9 â fav, 
Retention rate p = 0.5, Disturbance 0 < 6 < 1 
Constant k G [0.5,0.9], Temperature T = 
-fav/lnO.99. 
For a number of generations r 
For the whole population i = 1 to TT 
a) Select, cross, or mutate, evaluate the offspring Qi. 
b) Perform relaxation with the following loop 
i) Compute the offspring fitness f{Qi-\-i). 
u)iff{giii)-e<o 
Accept, and replace the parent Qi with Qi-\ i 
else Accept with probability 
PiiQi) = expi-ifiGin) 
- 
0)/kr). 
c) Calculate the average population fitness fav 
d) Recompute the recent fitness history 
e = p/a. + ( l - p ) ( / - 5 ) . 
Update the cooling schedule 
go to 2 until the termination condition is satisfied. 
5.2.5 
Coevolutionary Search 
A promising IGP control strategy for hard inductive learning tasks is 
to distribute the evolutionary search effort among coevolving subpop-
ulations. The idea is to create subpopulations that correspond to the 
substructures of the fitness landscape [Slavov and Nikolaev, 1997]. The 
search is facilitated by making subpopulations flowing on the substruc-
tures of the complex landscape. The distribution of the evolutionary 
search on the sublandscapes of the complex landscape means that the 
IGP attempts to solve the given task by solving its simple subtasks. In 
order to improve the search, there has to be made a proper coopera-
tion between subpopulations flowing on the simple landscapes and the 
population flowing on the complex landscape. 
The stages of this approach to coevolutionary search are: 1) determine 
the simple landscape substructures of the complex landscape; 2) for each 
simple landscape construct an appropriate evolutionary algorithm with 
a subpopulation easily flowing on it; that is, make special searchers for 
the particular landscape substructures; and 3) establish a mechanism for 
cooperation among the subpopulations. 

126 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
Landscape Substructures. 
The profile of a fitness landscape is 
often a complex curve which may be replaced by elementary curves 
[Bastert et al., 2001]. Since the landscape arises from the fitness func-
tion, this idea requires one to identify the smoother component curves 
of a given function curve. The fitness function may be represented as a 
superposition of basis functions according to some analytical expansion. 
A rugged function curve f{Q) can be closely approximated by a sum of 
smoother function curves fi{G) from a predefined basis: 
oo 
f{g) = EaJ,(g) 
(5.10) 
i-l 
where a^ are the corresponding amplitudes. 
This functional equation points out that one may think of a com-
plex fitness landscape structure as a superposition of simple (smoother) 
landscape substructures. The superpositioned structure inherits the es-
sential characteristics from the substructures, and that is why it may 
be expected that the search on some of the simple substructures will be 
easier. The inheritance of the essential landscape characteristics means 
that the search on the substructures will provide relevant building blocks 
for the search on the superpositioned landscape structure. 
The interesting question in the design of such a mechanism is how to 
isolate some of the simple component functions from the expansion of 
the original fitness function. If some simple components of a complex 
fitness landscape can be identified, then efficient coevolutionary search 
on these simple substructures can be organized. The searches on the 
substructures may transfer by migration useful information for directing 
the search on the complex landscape, which will help to speed up the 
convergence of the primary population. The efficacy of the coevolution 
is sensitive to the mechanism for integrating the searches. 
Two Subpopulations on Two Sublandscapes. The IGP control 
may be enhanced by maintaining two subpopulations: primary and sec-
ondary. The primary subpopulation should be made to search on the 
primary complex superpositioned landscape, while the secondary sub-
population should be made to search on one of its simple substructures. 
The secondary subpopulation has to contribute to the primary search 
by repeated migration of elite individuals to the primary subpopulation. 
This approach to coevolutionary search with two subpopulations flow-
ing on two landscapes can be implemented in IGP using different fitness 
functions. The genetic programs in the primary subpopulation may use 
an arbitrary fitness function / for the given task. The genetic programs 
Q2 in the secondary subpopulation should use another fitness function /2 
which may be the average from the fitnesses of the neighbors evaluated 

Search Navigation 
127 
using the primary fitness / [Slavov and Nikolaev, 1997]: 
^'^^'^ - 
I MiG,) I 
^"^^) 
where | M(52) | is the number of neighbors of the genetic program 02-
Neighbors are the genetic programs reachable from the current Q2 by 
one apphcation of the mutation operator M (Section 2.3.1). 
The coevolutionary process has to be coordinated by migrating indi-
viduals from the secondary to the primary subpopulation. The migration 
helps to balance between good individuals in the two subpopulations in 
the following way. Consider binary tournament for selection of a promis-
ing individual Q2 from the elite of the secondary subpopulation. Next, 
take the best neighbor Q2 of this genetic program. In case of using a 
minimizing function, this is the individual with lowest fitness value: 
min{/2(a^)|a^eM(a2)} 
(5.12) 
where /2 is the fitness function used in the secondary population. As 
the number of neighbors of a genetic program may be very large, only a 
small fraction of them could be taken. 
Two subpopulations developed in this way cooperate in the sense that 
they mutually focus the search. A requirement for cooperation is that the 
subpopulations use related fitness functions. Only then, the migration 
from the secondary to the primary subpopulation will exert pressure to 
highly fit individuals, and the migration will promote diversity. 
The empirical studies into coevolutionary IGP search show that the 
secondary subpopulation flows on a smoother landscape than the pri-
mary subpopulation. It has been found that [Slavov and Nikolaev, 1997]: 
1) the secondary subpopulation features a smaller number of local op-
tima than those on the primary one; 2) the secondary landscape has 
flatter hills and basins; it has larger basins as it makes longer walks be-
tween the optimal points on its fitness landscape. These are indications 
which allow us to think that the individuals in the secondary population 
have potential to see ahead in the search space; they are able faster to 
identify the concrete uphill or downhill search direction. 
This approach to searching on structured complex landscapes has sev-
eral aspects. First, the approach is problem independent. This means 
that it could be used in IGP systems that utilize different genetic pro-
gram representations. Second, the presented implementation migrates 
the best from the promising neighbors of the elite in the secondary sub-
population, chosen by binary tournament selection. Its advantage is that 
it preserves the probabilistic nature of the evolutionary search process. 

128 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
5.2.6 
Distributed Search 
Efficient search can be organized by distributing the individuals only 
within local groups in the population. Such local subpopulations are 
often called demes in GP [Tackett and Carmi, 1994], and this notion 
comes from the theoretical biology [Wright, 1932], The population is 
divided into demes with limited communication between them. The 
sub-populations are autonomous; they evolve in different search direc-
tions and exchange a small number of individuals at a predefined rate. 
Changing the migration rate allows control of the mixing between the 
demes so as to achieve better exploration of the search space, while ex-
ploitation of the search space is performed by the local demes. 
The distributed search begins by partitioning the population into 
demes, each of which can be of different size. Within each deme, a 
basic IGP search is executed. The selection mechanism uses tourna-
ments with small numbers of competitors. The competing individuals 
are probabilistically chosen from the local deme, but occasionally they 
may be picked from neighboring demes. The local search within the 
demes continues for a certain period so that the individuals between the 
demes mature enough in order to transmit useful genetic material when 
passed to other demes. The subpopulations evolve autonomously, and 
after that, individuals are migrated. The migration features by size and 
magnitude, which determine the search dynamics. There are exchanged 
better individuals, usually from the population efite. 
This demetic search is suitable for maintaining high diversity in the 
population, but it has been criticized for the large number of parameters. 
5.3 
Performance Examination 
The simulation of the biological mechanisms mutation, crossover and 
reproduction is not guaranteed to provide universal learning capabili-
ties. These natural operators are necessary but not sufficient means for 
adaptation on complex search landscapes. Moreover, these mechanisms 
should be properly designed and tailored to the task so that the vari-
ability that they cause leads to improvement. When the IGP shows 
continuous improvement during search, it is said to possess the property 
evolvabifity [Altenberg, 1994b]. Such a property is required from every 
component of IGP that impacts the production of offspring. 
The development of IGP systems should involve empirical investiga-
tions into how each of its components impacts the performance improve-
ment. Each genetic operator should contribute to achieving evolvability. 
Using various performance measures, this section explains how the evo-
lutionary IGP search process can be observed. 

Search 
Navigation 
129 
> 
u 
o 
100 
G e n e r a t i o n 
Figure 5.1. Evolvability of the best and average population fitness recorded during 
a representative run of the IGP system using the Mackey-Glass series. 
5.3.1 
Fitness Evolvability 
Evidence about the progress of the evolutionary search can be ac-
quired through measures of the fitness of the best individual and the 
average population fitness taken from a number of generations [Gold-
berg, 1989]. In the typical case, when a minimizing fitness function is 
used, the function that returns the fitness of the best population member 
Best{V^) can be defined as follows: 
BestiV) 
= mm{f{gi)\l 
< i < ix.^Ql G V^) 
(5.13) 
where the index % enumerates the genetic programs Ql in the population 
V^ at generation r. 
The average population fitness Average(V'^) shows whether the pop-
ulation as a whole moves on the search landscape: 
1 
AverageiV^) 
- - V / ( ^ D , V^[ G V 
(5.14) 
where V^ is the population of programs QJ at generation r. The pop-
ulation should converge slowly to the best individual without reaching 
it rapidly, such that the fraction W-mr-.oo^Best^V'^)/AverageiV'^)) 
^ 1. 
Otherwise, when the population average reaches the best one, the search 
will practically terminate. Figure 5.1 displays the best and average fit-
ness curves recorded during runs of the IGP system. 

130 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
5.3.2 
Convergence Measures 
During evolutionary search the IGP population leaves a trajectory, 
which is a path of its moves on the fitness landscape. This path can be 
identified by recording the errors of the ehte individuals in the popula-
tion. Then Principal Component Analysis (PCA) [Jolliff'e, 1986] may be 
applied to reduce all errors into three dimensions in order to visualize 
them [Nikolaev and Iba, 2001d]. The PCA error trajectory computed in 
this way may be considered a convergence measure that illustrates the 
search problems encountered during evolutionary learning. 
The PCA application for convergence examination may be explained 
as follows. The mean squared errors 777/ = (V-^) Y^n=iiyn ~ ^(^n))^? 
of the elite PNN models are recorded at each generation r, and error 
vectors are formed: m'^ = [m\,ni.2^ ...,m^], where m^l. is the error of the 
k-th genetic program from the elite, I < k < TVei-
Let each error vector m be a point in the /c-dimensional error space. 
Such a vector may be represented as a linear combination of basis vec-
tors: m = Y^i=i Tn^Ui, where u^ are unit orthonormal vectors such that 
ufuj 
= 6ij, and 6ij is the Kroneker delta. The individual errors are 
m^i = ujm. 
The PCA helps to change the coordinate system and to 
project the errors on the dimensions in which they exhibit largest vari-
ance. The basis vectors u^ are changed with new basis vectors v^ in the 
new coordinate system: m = Yl?==i ^i^i- This can be made by extract-
ing v^ as eigenvectors of the covariance matrix E of the error trajectory 
recorded during a number of generations T: 
Svi = XiWi 
(5.15) 
where Ai is the z-th eigenvalue of the covariance matrix S: 
T 
E = XI(^'^- m)'^(m'"- m) 
(5.16) 
r=l 
and the mean vector is 1x1= (1/T) Y1T=\ ^ -
The extent to which the i-th principal component captures the error 
variance can be measured as follows: Epc = ^f/Yli^f' 
The theoretical 
studies suggest that the first two principal components (PCs) capture 
the most essential variations in the errors. The first and the second 
principal components of the errors are calculated as follows: 
2 
pc ='^ZiVi 
(5.17) 
i = i 
where pc = (pci,pc2), and Zi are the rotated coordinate coefficients. 

Search 
Navigation 
131 
Second PC 
F i r s t 
PC 
Figure 5.2. Error trajectory of the 30 elite polynomials (from a population of size 
100) evolved with IGP appHed to the Mackey-Glass data series. 
Second PC 
F i r s t 
PC 
Figure 5.3. Error trajectory of the 30 elite polynomials (from a population of size 
100) evolved with traditional GP applied to the Mackey-Glass data series. 

132 
ADAPTIVE 
LEARNING 
OF POLYNOMIAL 
NETWORKS 
Table 5.6. Algorithm for computing the error trajectory of IGP using PCA. 
PCA Error Trajectory 
step 
Algorithmic 
sequence 
1. Initialization 
Population of genetic programs ^,;, 1 < i < TT, 7re/=30%. 
Data set D = {(xj,2/j)}j^]. 
2. Perform an IGP run 
For a number of generations r do 
Record the errors m^ of the elite /c, 1 < k < TVeh 
individuals m'^ = (rr?.^, TTIJ ,..., rnJ!). 
3. Compute the 
a) Make the covariance matrix S 
error trajectory 
S = Y^r-=i(^~ 
^)^{^- 
*^), 
where: m = (1/T) X]^_^ m^. 
b) Compute the eigenvectors v^ and 
eigenvalues A,; of S 
Sv,: = XiVi. 
c) Select the largest eigenvectors: vi and Vi. 
d) Determine the coefficients Zi 
T 
Zi = V,' m . 
e) Calculate the first and second principal components 
pc =X]7:-i '^'â¢^'â¢' where: pc = (pci,pc2). 
Plots of the error trajectory can be made by drawing the average 
mean square error {MSB) of the population elite against the first two 
principal components pci and pc2. Figures 5.2 and 5.3 depict the error 
trajectories obtained from separate experiments conducted with two ge-
netic programming systems: the IGP. and a traditional Koza-style GP 
using the Mackey-Glass data. Each trajectory is computed from the er-
rors recorded during a single run. Figure 5.2 shows that the variation of 
the elite error in Koza-style GP slopes down with a zig-zag movement 
during the evolutionary learning process (which can be seen from the 
changing error directions). Its population moves in curved directions on 
the search landscape, since it seems rugged and difficult to search with 
the particular genetic mechanisms. 
The error trajectory in Figure 5.3 shows that the IGP search moves 
straight forward from the beginning until the end, and it progresses 
directly toward the final basin following almost a straight line direction 
of error decrease. In this sense, its population orients well on the search 
landscape. The plots in Figures 5.2 and 5.3 are meaningful because these 
PCs capture respectively pci=85.25% and pc2=12A7% of the variance 
of all elite errors, which make us certain about the search behavior. The 
algorithmic framework with which these curves in Figures 5.2 and 5.3 
have been produced is given in Table 5.6. 

Search Navigation 
133 
5.3.3 
Diversity Measures 
The evolvability of the genetic programs within the population can be 
investigated using diversity measures. The diversity measures give in-
formation about the alterations of the genetic program structures. That 
is why they account for the structural evolvability of the individuals in 
the population. The ability of IGP to maintain high diversity in the 
population can be examined using two easily computable measures: the 
syntactic population diameter and the structural population entropy. 
Investigations with these diversity measures can help to understand how 
the IGP population evolves during the generations. 
Syntactic Population Diameter. Having enough diversity of ge-
netic programs means that the IGP system has sufRcient power to con-
duct efficient search. Otherwise, if it looses diversity, it usually rapidly 
converges to suboptimal solutions. The diversity can be examined with 
the population diameter. High population diameter indicates that the 
IGP system is able to maintain high diversity of dissimilar genetic pro-
grams in the population. The syntactic â¢population diameter {SPD) can 
be estimated as the mean tree-to-tree distance among the elite individ-
uals that have better fitness values: 
SPD = â 
Y, 
Dist{gi,gj) 
(5.18) 
^^^ 
i=lj=l,i^j 
where the population elite is usually TTe/ = 40%. 
Figure 5.4 shows the impact of three different mutation operators 
on the population diversity estimated using the syntactic population 
diameter metric. The SPD curves are calculated during experiments 
with the same IGP system implemented with fitness proportionate se-
lection and steady-state reproduction. Three versions of this IGP were 
made with three different mutation operators: context-preserving mu-
tation (CPM), hierarchical variable-length mutation {HVLM), 
and re-
placement mutation (RM) (Section 2.3.1). A close look at the plots in 
Figure 5.4 allow us to reason that when IGP searches with the context-
preserving mutation operator, it supports highest variety of tree struc-
tures in the population. The replacement mutation RM operator seems 
worse because it is unable to sustain the diversity in the population, 
and therefore it could be expected to be inefRcient for similar tasks. 
The HVLM 
mutation is also inferior to CPM as it features by a lower 
syntactic population diameter. 
Such empirical studies using SPD can be performed to analyze the 
effects of the other genetic learning operators and IGP mechanisms on 
the search process with intention to improve them. 

134 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
p 
U 
-P 
ft 
o 
30 
20 
10 
s 
-
% â¢ â¢ 
u ^ '"-;'; ..^>V;' ^â¢''v' ^"v/ 
â 
CPM 
RM 
v-v^,^,,;^^^ 7'^"'" 
50 
100 
G e n e r a t i o n 
Figure 5.4. Population diameter computed during IGP runs with 50 genetic pro-
grams using different nmtation operators: context-preserving mutation (CPM), 
hi-
erarchical variable-length mutation {IIVLM)^ 
and replacement mutation 
(RM). 
Population Clustering. Evidence about the structural diversity in 
the population can be acquired by unsupervised clustering of the ge-
netic programs [Nikolaev et ah, 1999]. The evolutionary search should 
maintain a large number of clusters in order to achieve successful perfor-
mance. Higher numbers of clusters indicates that there is a high diversity 
of genetic program structures. The number of population clusters should 
be recorded after each generation during a run of the IGP system. 
The implementation of a clustering program for genetic programming 
systems with tree-hke genetic programs requires the tree-to-tree distance 
algorithm. The tree-to-tree distance algorithm, given in Section 2,3.4, 
has to be developed in advance as a separate function and is invoked at 
the end of each reproductive cycle. 
The population can be analyzed using an incremental clustering al-
gorithm. The set of genetic programs are partitioned into a number of 
clusters with similar elements. The intercluster similarity is measured 
using the tree-to-tree distance to the mean of the particular cluster. The 
clustering algorithm starts with the two most distant genetic programs 
in the population as initial cluster centers. The remaining genetic pro-
grams are processed incrementally and classified relatively to the average 
tree-to-tree distance between the existing centers. 

Search Navigation 
135 
The average tree-to-tree distance between the centers is defined in the 
following way: 
^ = j^(K^-i) ^ 
Â£ 
Dist{gk,gi) 
(5.19) 
k-\ 
i=/c-fl 
where K is the number of all clusters of programs in the population. 
When a genetic program Q comes, its tree-to-tree distances to the 
centers of available population clusters is computed. If the maximal 
from the distances between the genetic program to the cluster centers 
is greater than the average distance between the centers, this genetic 
program becomes a new cluster; that is, it is promoted as center of a 
completely new cluster. Otherwise, it is added to the cluster whose 
elements seem most similar to it in tree-to-tree distance sense: 
Dist{g,gi) 
< Dist{g,gj), 
i<ij<Kj^i 
(5.20) 
where ^^ ^-^d 5j denote cluster centers. 
After adding the next genetic program to a particular cluster, its clus-
ter center is recomputed and the average distance between the centers is 
updated. The genetic program which is at smallest tree-to-tree distance 
to the remaining genetic programs in the cluster becomes its center: 
g^ = min <^ 5;]I^^5^(SA3,e^)|a^ ^ Z^ \ 
(5.21) 
where Z^ denotes the k-ih cluster in the current population, g^ is the 
center genetic program of this k-th cluster, and n^ is the number of the 
members in cluster Zf^. 
Figure 5.5 illustrates the clusters computed during representative runs 
of the IGP and traditional Koza-style GP systems using the Mackey-
Glass data series. The traditional GP was implemented with the MSE 
fitness function, while IGP used the GCV fitness function. The diff'er-
ence is that IGP was made using the context-preserving mutation oper-
ator, while the Koza-style GP used mutation at random points. Figure 
5.5 shows that the population of the IGP system during evolutionary 
search contains more clusters; that is, it maintains higher diversity of 
genetic programs. The higher number of clusters are indication that 
the IGP system is capable of continuous adaptation on the search land-
scape. It is interesting to note that traditional GP also has a relatively 
large number of syntactically diff'erent programs in its population. The 
computational algorithm for incremental population clustering, used to 
prepare this figure, is shown in the next Table 5.7. 

136 
ADAPTIVE 
LEARNING 
OF POLYNOMIAL 
NETWORKS 
u 
0) 
1 0 0 
G e n e r a t i o n 
Figure 5.5. Population clustering of IGP and traditional Koza-style GP using 
populations of size 100 genetic programs and the Mackey-Glass data series. 
Table 5.7. Algorithm for unsupervised incremental clustering that may be applied 
after each generation in a particular run of the IGP system. 
Unsupervised Population Clustering for IGP 
step 
Algorithmic 
sequence 
1. Initialization 
2. Process 
consecutively all 
genetic programs 
Current population of genetic programs Qi, 1 < i < n. 
Take the two most distant individuals as centers; 
Z, ={g,} 
andZ2 = {^2}, 
so the current clusters are K" = 2. 
Let d be the average distance between cluster centers. 
For each next genetic program Q do 
a) Calculate its tree-to-tree distance to each center 
Dist(g,Gi), 
1 <i< 
K, 
b) if d < 
Dist{G,Gi) 
Make this genetic program 
a new cluster center Z â {Q} 
else 
Allocate it in the cluster with closest center Qi 
Dist{G,Gi) < Dist{G,Gj), 
1 < ij 
<K,j^ 
i. 
c) Recompute the center of this cluster 
Gk = min {Y:Z:i Efist{Gk. Gi)\Gk G Z ^ . 
d) Update the average cluster distance 
d = {2iK{K -1)) x : f j ; E L , , 
Dist{g,.,g,). 

Search Navigation 
137 
Structural Population Entropy. The diversity of genotype struc-
tures bred by IGP can be measured by the population entropy It is 
defined in such a way that a low entropy indicates that the IGP sys-
tem evolves from a random population to an internally structured one. 
The tendency in the search process should be toward evolving genera-
tions with decreased structural randomness compared to the structural 
content of the initial population. This may happen if the population is 
capable of keeping information about the individuals, like homogenous 
compositions, and transmitting it further so that there is no loss of useful 
genetic program structures. 
Structural population entropy is the average entropy of the sequences 
of genes that make the genetic programs. It estimates the structural sim-
ilarity between the genotypes available in the population. A sequence of 
genes may be collected by traversal of the corresponding linear genetic 
program tree. After that, the linear trees have to be arranged consec-
utively one by one in a population table. The population table should 
include a number of rows equal to the population elite. The columns 
should contain the alleles from the genotype loci. The loci from the 
genotype correspond to the vertices in the linear tree representation, 
which may be functional nodes and terminal leaves. 
The probability Pr of an allele a in a particular locus column is com-
puted by its relative frequency freq in this column contributed by the 
elite genetic programs: 
Pr(a) := l^:^^ 
(5.22) 
where TTg/ denotes the size of the population elite. 
The entropy h of a locus I is therefore the sum of the entropies of its 
alleles a^ from its locus column i, 1 < ^ < TT in the population table: 
hi^-Y,Vv{ai)x\og^VT{a,) 
(5.23) 
2 = 1 
The structural population entropy H is the sum of the entropies of all 
vertices Vi occupying the successive loci: 1 < ^ < |0|, divided by the size 
of the longest, in sense of number of nodes and leaves, genetic program 
1^1 in the population under study: 
where H'^ is the entropy at generation r. In order to obtain correct and 
meaningful results, this procedure should be applied with at least 100 
diff"erent populations recorded during consecutive generations. 

138 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
ft 
o 
O 
â¢H 
rd 
o 
G e n e r a t i on 
Figure 5.6. Population entropy averaged from 10 runs of IGP and traditional GP 
with populations of 100 genetic programs, TTei â 40%, using mutation parameter 
/i = 0.085, and crossover parameter /^ = 5. 
Figure 5.6 displays the changes of the population entropy calculated 
with separate runs of IGP and traditional Koza-style GP. The IGP sys-
tem is implemented with the GCV fitness function, and the context-
preserving mutation operator. The traditional GP uses the MSE func-
tion and mutation at random points. The remaining mechanisms in both 
systems are the same: they use fitness proportional selection, steady-
state reproduction, and were trained on the Mackey-Glass data series. 
Figure 5.6 displays that the ensemble of genetic programs maintained 
by the IGP system continuously becomes more and more homogenous. 
Evidence for this homogeneity is the overall sloping down IGP entropy 
curve. On the contrary, the population entropy in traditional GP changes 
variably. The observation that the traditional GP entropy tends to in-
crease after a certain generation suggests that the individuals lose useful 
structural content during the transformations by the genetic learning op-
erator, and they are unable to search efficiently on the fitness landscape. 
A reason for the better behavior of IGP is that its sampling context-
preserving mutation operator and the fitness function are coordinated 
through the common genetic program size parameter. One is inclined to 
think that this size-base coordination helps to install useful structural 
information in the population. 
The computational algorithm for measuring the entropy picture of the 
population during runs with IGP is given in Table 5,8, 

Search Navigation 
139 
Table 5.8. A computational algorithm for making a picture of the structural pop-
ulation entropy in IGP. 
Measuring Structural Population Entropy in IGP 
step 
Algorithmic 
sequence 
1. Initialization 
Population of genetic programs Qi,l < i < TT. 
the best 40% of which are population elite TTe/. 
2. Conduct an 
For a number of generations r do 
IGP run and 
a) Isolate the genotypes of the population elite TTe/ 
make entropy 
and arrange them in a population table 
PopTable, 
picture 
b) For each loci / = 1 to |^/|, 
where \Qi\ is the longest genome. 
i) Compute the frequences of alleles 
in this locus column 
Pr(a) = 
{freq{a))/iTei' 
ii) Calculate the entropy of this locus 
/^; = - Y7i^A P^(Â«0 X log2 Pi-(aO-
c) Estimate the population entropy 
H' = {l/\Q\)T}^,h,^ 
5.3.4 
Measures of Self-Organization 
The evolutionary algorithms exhibit complex dynamical behavior that 
can be investigated using measures from the physics of complex systems. 
The behavior of the IGP population may be envisioned as analogical to 
this of a cloud of physical particles thrown on a hilly landscape surface. 
The population moves on the fitness landscape like an ensemble of par-
ticles which are pulled by the gravity force, decreases their speed due 
to friction, and finally stops at some landscape basin or trough with a 
particle in the lowest point. While the population flows on the land-
scape, as a result of the internal changes in the population, its potential 
energy is expected to be transformed with losses into kinetic energy. 
A slowly decreasing total population energy is a dynamic characteristic 
which implies feasibihty of the evolutionary algorithm to search. 
The dynamics of IGP can be studied with the following physical mea-
sures [Nikolaev and Slavov, 1998]: potential population energy, kinetic 
population energy, and total population energy. These are physical esti-
mates which provide evidence for such dynamical characteristics of the 
algorithm performance hke the transmission of useful information, the 
self-organization, the movement, and the stabilization of the population. 

140 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
Potential Population Energy. IGP should maintain individuals 
that contain different partial solutions. A larger repertoire of dissimi-
lar fit genetic programs means that there is a greater chance for their 
arrangement into better ones. Therefore, the IGP should distribute the 
search effort on clusters of fit genetic programs. The theory of thermody-
namics provides the potential energy as a characteristic of the ability of 
a system to self-organize toward configurations that increasingly satisfy 
given macroscopic constraints. Starting with individuals randomly scat-
tered on the fitness landscape, the behavior of an evolutionary searcher 
will converge when the fit genetic programs in the population resemble 
each other, or the distances between them slowly decrease. 
In order to observe the diversity from a macroscopic point of view, 
the potential population energy Ep of an IGP dynamic system can be 
defined as follows: 
Ep = â J2Y1 Dist{g^, g.j)hfj 
(5.25) 
where fi ~ f{Qi) and fj = f{Qj) are the fitnesses of the possible pairs 
of genetic programs Qi and Qj from the population ehte of size TTJ^/, and 
Dist{Qi^ Qj) is the tree-to-tree distance between them. 
The potential population energy will decrease with the decrease of the 
distances between the genetic programs. This will be an indication that 
the evolutionary algorithm improves and the population self-organizes. 
The potential population energy is considered here to compare the per-
formance of IGP with a traditional Koza-style GP. IGP features by 
the GCV fitness function and the context-preserving mutation operator. 
The traditional GP uses the MSE fitness function and mutation at ran-
dom points. The remaining micromechanisms are the same: crossover 
at random points, fitness proportional selection, and steady-state repro-
duction. Both GP were evaluated with the Mackey-Glass series. 
The potential energy curves plotted in Figure 5.7 demonstrate that 
the IGP system achieves higher population variety compared to the tra-
ditional GP. During the experiments, IGP show abilities to produce con-
tinuously modified configurations with varying but overall fitter genetic 
programs, and to relax to an energetic equilibrium. The IGP system 
seems capable of balancing well between the sizes of the programs and 
their fitnesses, which enables the population to reach together an equi-
librium configuration with very good solutions. The potential energy of 
traditional GP decreases quickly as it gets trapped rapidly into imper-
fect configurations of relatively fit but suboptimal programs with close 
tree shapes that quickly dominate the population. 

Search 
NavigaMon 
141 
u 
Q) 
Figure 5.7. Potential population energy Ep averaged over 10 runs with IGP us-
ing small populations of 100 genetic programs, elite iTei = 40%, genetic operator 
parameters fj, â 0.05, K = 3, and fitnesses evaluated using the Mackey-Glass data. 
Kinetic Population Energy. The kinetic energy can be used for 
estimating the IGP search effort made to push the population on the fit-
ness landscape. An increasing kinetic energy will be an indication that 
the algorithm performance improves, in the sense that it has abilities to 
alter the search toward unexplored landscape regions. The search per-
formance depends on the provided examples, and they are an important 
factor for redirecting the search. The genetic programs should compete 
within the population to capture more unique examples from the data. 
The kinetic population energy could be estimated by summing the 
speeds Si of the genetic programs Si (1 < ^ < TT^./) multiplied by their 
fitnesses /^, averaged with respect to the population elite number TT^./: 
EK 
Yl ^^f^' 
(5.26) 
i-\ 
The speed is a global characteristic of the motion of a genetic program, 
which is representative for the global performance of the IGP system. 
The speed expresses the instantaneous tendency of a genetic program to 
modify its capacity to interpolate the given data precisely. In context 
of IGP, the speed of a genetic program quantifies the rate of variation 
of the interpolated exceptional examples. An example vector becomes 
exceptional and hence, important when less genetic programs from the 
population match it with error lower than a preselected threshold. 

142 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
The speed Si of a genetic program Qi should be defined in two different 
ways for the two typical kinds of application problems: classification 
and regression. When performing classification by IGP the speed could 
be the sum of the importances I e R of the examples that the genetic 
program matches M{Qi, (xj, yj), e) = true or false^ M: 
QxDxRâ^B. 
When performing regression by IGP, the speed could be the number of 
data points that the genetic program approximates with error lower than 
a predefined threshold e e R: 
where {-Xj^yj) is an example from the given data. The speed Si will be 
largest when the genetic program Gi correctly recognizes all examples. 
The extent to which an example is matched by all genetic programs 
in the population is called importance. It should be evaluated with its 
relative displacement from the remaining examples in the sample using 
a dynamic characteristic like the fitness. The importance Ij of the j-th 
example {xj^yj) is the normahzed diff'erence between the fitnesses of all 
genetic programs from the population elite TT^I and the fitnesses fj of 
these individuals Qi that match it M{Qi,{xj,yj),e) 
with low error less 
than the predefined threshold e: 
z^k=i Ik 
z^ yv/(g,,(x,,7/,),Â£) 
^' ~ 
ESTA 
^^'^^^ 
The values produced by this normalized formula grow when the por-
tion of genetic programs Qi classifying correctly the j-th example (yij^yj) 
decreases: limi_,o/j = 1. 
The justification for this definition is that when the number of genetic 
programs that match an example increase, more individuals carry infor-
mation about its features and the example becomes less infiuential for 
the search. The importance of an example is a sign for its identity com-
pared to the remaining examples. Capture of such a distinct example 
may cause a search perturbation, and may be expected to contribute to 
improvement of the search process. Recognition of a yet unmatched ex-
ample by an individual says that this genetic program is in a promising 
search direction. In order to search well, the algorithm should toler-
ate speedy genetic programs which at each next generation match more 
important examples. 

Search 
Navigation 
143 
Figure 5.8. Kinetic population energy EK averaged over 10 runs with IGP us-
ing small populations of 100 genetic programs, elite Nei = 40%, genetic operator 
parameters fi = 0.05, = 3, and fitnesses evaluated using the Mackey-Glass data. 
Figure 5.8 shows the kinetic population energies of IGP and tradi-
tional Koza-style GP measured while learning the Mackey-Glass series. 
These GP differ in that IGP uses size-biased context-preserving mu-
tation and GCV fitness function, while traditional GP uses ordinary 
mutation at random points and MSE fitness function. When looking 
at the kinetic energy curves, one notes that the kinetic energy of IGP 
gradually rises. Because of this we are inclined to think that using the 
size-biased genetic learning operators and fitness function helps to match 
increasingly more exceptional examples. The speedy genetic programs 
in IGP prevail in the population, which from a global perspective leads 
interpolation of more examples. 
The curves in Figure 5.8 demonstrate that the genetic programs pur-
sue adaptation with mutual competition to match exceptional data. 
This competition is regulated by the size-integrated navigation and fit-
ness biases, which provide the capacity to make considerable population 
changes. This ensures motion of the population on the fitness landscape 
and continuous performance improvement. The traditional GP tends to 
converge relatively fast to a population of individuals having close shapes 
and size, and there are no forces that can push the population so as to 
reorient the search. This reasoning follows from the almost unchanging 
kinetic population energy of traditional GP, although in the beginning 
of the search process it slightly increases. 

144 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
>^ 
O 
Figure 5.9. Total population energy Er averaged over 10 runs with IGP using small 
populations of 100 genetic programs, elite Nei = 40%, genetic operator parameters 
fj, = 0.05, = 3, and fitnesses evaluated using the Mackey-Glass data. 
Total Population Energy. IGP may be considered a dissipative 
computational system [Yang et al., 1996] that relies on the principles 
of information absorbation, change, and transmission. There is energy 
consumption for accumulating information in the population, modifying 
this information, as well as distributing the information among the indi-
viduals. The total energy is a representative measure for the collective 
behavior of the genetic programs in the population. 
The total population energy ET = Ep -f EK^ describes the degree of 
IGP stabilization. During evolutionary search, the potential population 
energy is expected to be transformed with losses into kinetic energy. 
The energy losses are due to dissipation plus energy consumption for 
aggregation of useful partial solutions in the form of building blocks. 
The hypothesis is that an evolutionary search algorithm is a kind of an 
open system, which dissipates energy from its useless genetic programs 
discarded to the environment and conserves the energy of its elite genetic 
programs [Yang et al., 1996]. The elite genetic programs should carry 
the essential information and compete to take over the population, and 
thus diminish the population randomness. 
The total population energy is used here to relate the performance of 
IGP to this of a traditional Koza-style GP. IGP features by the GCV 
fitness function and the context-preserving mutation operator. The tra-
ditional GP uses the MSE fitness function and mutation at random 

Search Navigation 
145 
points. The plots in Figure 5.9 display that the traditional GP search 
rapidly saturates compared to IGP. The decreasing character of the total 
population energy can be interpreted as a process of consuming energy 
from the environment to foster the fit genetic programs, which is accom-
panied with freeing the energy of inferior individuals. The rapid energy 
flux to the environment in traditional GP says that it accumulates in-
correct information, possibly because it converges to genetic programs 
of similar size and shape. The slow energy decline of IGP shows that it 
continually absorbs energy in the form of useful genetic material. 
Physical Population Picture. The population energy estimates 
can be taken to make physical pictures of the dynamics of evolutionary 
algorithms (Table 5.9). They reveal general behavioral patterns and 
allow one to derive explanations about the search quality of the GP 
system. 
Table 5.9. Algorithm for taking a physical picture of the IGP dynamics. 
Making a Physical Population Picture in IGP 
step 
Algorithmic 
sequence 
1. Initialization 
2. Conduct 
runs 
Initial population, the best 25% are population elite rCei 
Data D = {(xj, ?/j)}jl,^ , and error threshold e. 
For a number of runs do 
For a number of generations do 
a) Perform recombination, mutation, and selection: 
select elite iTei. 
b) With all elite Qi and Qj compute potential energy 
c) In order to measure the kinetic energy do 
i) Determine the importances Ij of the examples 
ii) Measure the speed s,; of each elite program Qi 
E Â«(e,,(x,,Â«,.;),Â£) Ij-
3. Terminate 
iii) Compute the kinetic population energy 
EK = (l/7re,)Er=, Â«'â¢/Â»â¢ 
d) Compute the total population energy 
ET = Ep + EK â¢ 
Collect the energies during into corresponding arrays. 
Perform averaging of each energy array 
ET.EP.EK-

146 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
The above energy estimates demonstrated how one can analyze the 
characteristics of IGP in the three stages: start, global, and local search. 
The IGP starts with a random population which has a low kinetic energy 
and a high potential energy. After that, the IGP system enters the stage 
of global search during which it rapidly accelerates and traverses large 
landscape areas. This behavior is indicated by the slightly sloping-down 
potential energy and the increasing kinetic population energy. When 
IGP focuses at a region with a good solution, it shows behavior on the 
edge between global and local search. Then it moves with a slow velocity, 
since changes of the investigated search directions still happen. During 
this transitional search stage the potential energy oscillates with large 
magnitudes, and the kinetic energy varies slightly. 
5.4 
Chapter Summary 
This section presented control micromechanisms for IGP. It was shown 
how to implement reproduction and replacement strategies. There were 
given algorithms for fitness proportionate, ranking, truncation, and tour-
nament selection of promising individuals from the population. There 
were given several replacement mechanisms, including: random deletion, 
deletion of the worst individual by ranking, deletion of the worst by tour-
nament, and deletion of the oldest individual from the population. 
The main contributions of this chapter to the research in genetic pro-
gramming are the offered measures for examination of the evolutionary 
search performance. There were proposed four groups of measures: fit-
ness evolvability measures, convergence measures, diversity measures, 
and measures of the self-organizing behavior. These are general mea-
sures whose implementation does not depend on the concrete implemen-
tation of the GP system and its micromechanisms, so these measures 
can be applied to investigate the performance of any GP system. These 
measures are useful because they provide valuable information about 
different aspects of the search algorithm. Having this information, one 
can tune and improve the GP system components so as to achieve faster 
and reliable convergence to good solutions. 
The possibility to take a picture of the physical characteristics, like the 
potential and kinetic energies, makes the IGP a promising application 
tool, as with these measures a researcher can be convincing about the 
self-organizing potential of the system. Taking a picture of the self-
organizing behavior of the IGP provides evidence about the adaptive 
capacity of the evolutionary search, and it may be considered to examine 
it as a dynamical system for real-world problem solving. 

Chapter 6 
BACKPROPAGATION TECHNIQUES 
The presented methodology for inductive learning of polynomial net-
works includes three groups of techniques for: 1) finding the network 
architecture, i.e. polynomial terms; 2) improving the network weights, 
i.e. polynomial term coefficients; and 3) adapting the network weights 
with respect to their variances and the noise variance of the adopted 
probability distribution. After evolving an adequate PNN topology by 
genetic programming, the weights can be improved further by BackProp-
agation (BP) techniques. This is crucial to successful learning because 
the weights of the polynomial are essential in describing the data, and 
so they impact PNN training as well as generalization performance. Es-
timating the PNN weights by OLS fitting often does not lead to suffi-
ciently good solutions. That is why additional retraining could be made 
by backpropagation [Rumelhart et al., 1986], elaborated for higher-order 
networks of polynomial activation functions. 
The constructive polynomial network algorithms from the GMDH 
family do not guarantee achieving coherence between the higher and the 
lower layer weights. These algorithms build the network without achiev-
ing enough mutual cooperation between the weights so that together 
they fit the data. This happens because while growing the network, the 
weights are estimated vertically in a bottom-up manner from the lowest 
layers near the inputs toward the higher layers near the output. When 
estimated in this way, the weights in the lower layers are frozen and 
do not depend on the weights in the higher layers. The weights in the 
lower layers are not coordinated with the weights in the higher layers, 
while the weights in the higher layers are coordinated with the weights in 
the lower layers. This is the rationale for recommending further mutual 
synchronization of all the weights in such constructive networks. 

148 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
The representation of polynomials as multilayer feed-forward PNN 
enables us to develop efficient BP techniques for their tuning. The basic 
BackPropagation Algorithm (BPA) [Rumelhart et aL, 1986, Werbos, 
1974] conducts gradient descent search in the direction of the most rapid 
error decrease by changing the weights. This could be envisioned as 
changing the weights so as to move downhill on the error surface, aiming 
at reaching the deepest basin. BPA performs two passes through the 
multilayer structure of the network: a forward pass and a backward pass. 
During the forward pass it computes the outputs from the hidden nodes 
and the overall output. During the backward pass it updates the weights 
using an error correction rule. BPA is an error-correction procedure 
that presents the training data many times and modifies the weights 
until the network produces outputs satisfactory close to the provided 
desired outputs. In the case of PNN, typically the Euchdean distance 
(L2- norm) is preferred as a closeness criterion. 
Backpropagation techniques are specialized here for nonlinear PNN 
to make them reliable supervised learning systems [Nikolaev and Iba, 
2003]. Several BPA based on first-order (Section 6.2) and second-order 
optimization methods (Section 6.3) are developed for nonlinear PNN 
weight learning. The second-order BP algorithms are those based on the 
Newton's method, the pseudo-Newton training method, the conjugate 
gradients method, and the Levenberg-Marquardt method. The issue of 
weight regularization is considered in the context of the first-order prop-
erties and revisited using the second-order properties of these networks. 
After that, design of first-order and second-order pruning methods is 
detailed. Approaches to learning step control are briefiy discussed. 
6.1 
Multilayer Feed-forward P N N 
PNN are connectionist computational devices for mathematical mod-
elfing. They consist of processing units, called neurons or hidden network 
nodes, linked in irregular network topologies to model functions. Each 
neuron has several incoming connections, an outgoing connection, and 
a basis activation function. In the case of tree-structured PNN, each 
neuron has exactly two incoming links. The node inputs are passed 
through an activation function that generates the nonlinear output sig-
nal. The PNN use low-order activations, more precisely, bivariate linear 
and quadratic activation polynomials (Table 2.1). Since the coefficients 
of an activation polynomial weight the effects of the input signals, they 
are simply called weights. The input signals are sent through the incom-
ing connections; that is, the connections carry the signals. The signals 
can be outputs from other neurons or input variables. 

Backpropagation Techniques 
149 
The neurons in PNN are organized hierarchically in multiple layers, 
also called levels: an input layer of fringe nodes that receive the input 
variables, hidden layers of functional nodes fed with outputs from other 
nodes and/or input variables, and an output layer having a root node 
that produces the overall model output. Such PNN are essentially feed-
forward networks because the computed output from a node does not 
influence its inputs either directly or indirectly. When cascaded hierar-
chically, the node activation polynomials in such a multilayer structure 
compose a high-order function of the inputs. 
Polynomials are suitable as activation functions since they are both: 
1) nonlinear, thus when cascaded they imply abilities for representing 
highly nonlinear functions; and, 2) continuous, thus diff'erentiable and 
so enable gradient descent learning. Employing activation polynomials 
in the nodes of neural networks is sufficient for learning higher-order 
multivariate functions and eliminates the need to consider other kinds 
of functions such as the usual sigmoid and hyperbolic tangent. 
6.2 
First-Order Backpropagation 
BPA [Rumelhart et al., 1986, Werbos, 1974] performs gradient descent 
weight search in training steps called cycles or epochs. Each cycle is an 
iteration over the data and it involves: 1) a forward pass with one input 
vector through the network which generates outputs from each node, 
and finally a network output; 2) a backward pass of error signals; and, 
3) changes of the weights proportional to the errors. The forward pass 
propagates the effects of the lower layer weights toward the higher layer 
weights encoded as input signals. The backward pass propagates down 
through the network, from the root to the fringe nodes, the effects of the 
higher layer weights on the lower layer weights encoded as error signals. 
The error signals carry information not only for the difference between 
the estimated outputs and the targets, but also about the magnitudes of 
the weights on the distinct paths from the root to the concrete node. In 
this way BPA, achieves precise tuning of the weights that reflects their 
mutual dependencies. 
Backpropagating of the errors enables us to adjust each weight in 
proportion to its influence on the output. A large modification of a 
weight is done if it causes a large reduction of the error. In order to 
perform optimal weight update, BPA exploits several influences of the 
network model components on the computation of the gradient in the 
weight space: how the changes of a weight influence the output of the 
concrete node, how the nodes in the next layer influence a particular 
weight change in the previous layer, and how the network root node 
output influences a particular weight change. 

150 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
Viewing learning as a weight optimization problem, the task is to find 
such weights that minimize the squared error: 
E{w) = ^J2^l 
(6.1) 
Gn = 2 / n - i ^ ( X n , w ) 
(6.2) 
where w is the weight vector of the network which applied to given input 
Xn produces output P(xn, w), and e^ is the error from the n-th training 
example. The derivative of this error with respect to the weights gives 
the amount for updating the weight vector. 
6.2.1 
Gradient Descent Search 
The gradient descent search algorithm prescribes to make specific 
weight updates so that the network output error, as a function of all 
these weights, achieves steepest decrease. The direction of steepest er-
ror decrease is opposite to the gradient vector of the error function. In 
this sense, weight learning is a search process of moving downhill on the 
error landscape. In nonlinear PNN, the surface of the error function 
(6.1) has multiple optima and minima. The ultimate goal of the search 
process organized with the concrete network is to find a weight vector 
that possibly corresponds to the lowest minima on the error surface, in 
the deepest basin on the error landscape. Pushing the weight vector, 
which is an argument of the error function, causes moves of the error 
point, determined by the network output, on the error surface. 
The gradient descent weight search can be terminated when the gra-
dient becomes sufficiently small, below a predefined threshold, that is 
even before or near the global minimum on the error surface where the 
gradient is zero. Because of the somewhat unknown nature of the error 
in practice it may be difficult to decide which is a sufficiently good solu-
tion. That is why the search is stopped after performing a fixed number 
of iterations or until there is no longer error minimization. 
A necessary means for developing weight learning rules that optimally 
decrease the error are the first-order and second-order derivatives of the 
error function with respect to the weights. They serve to elaborate the 
mechanisms of the BPA. The first-order error derivatives form the so-
called gradient vector g = WE (6.36,6.37) on the error surface obtained 
with weight vector w. The first-order error derivatives are of the kind 
dE{w)/dwi, 
I < i < W, where E{w) is the error obtained from PNN 
with weights w = [WQ^ K;I, ..., ww]^ when evaluated with the n-th exam-
ple, and W is the number of weights. 

Backpropagaiion Techniques 
151 
The activation polynomials used in the nodes of the polynomial net-
works are continuous functions which assume gradient descent learning 
[Nikolaev and Iba, 2003]. Having different network topologies and acti-
vation functions requires different analytical expressions for the gradient 
vector. The derivation of the expressions for the gradient vector in poly-
nomial networks rehes on the following assumptions: 1) every functional 
network node, hidden and output, uses inputs that are signals com-
ing from its two children nodes; and, 2) in every hidden node, linear 
or quadratic activation polynomials are allocated. These polynomials 
play the roles of both net functions that collect the input signals, and 
activation functions that provide the nonlinearities. 
Although the presented derivations do not involve bounding the ac-
tivation polynomial outputs by passing them through appropriate addi-
tional filters, such as the sigmoidal or tangential functions, they can be 
incorporated in the following algorithms in a straightforward manner as 
suggested in the classical literature. It should be noted that bounding 
the activation polynomial outputs through additional nonlinearities may 
be necessary to stabilize the generalization on future data as suggested 
in the introduction, especially for PNN. 
6.2.2 
First-Order Error Derivatives 
Weight training algorithms can be implemented using the exact gradi-
ent vector or using an approximation to the gradient vector. The second 
approach is preferred for explaining the derivation of the canonical BPA. 
Let the instantaneous error between the desired output yn and the 
estimated PNN output produced with the input vector x^ from the 
same example pair {^n^Vn) he: 
En = \el 
(6.3) 
where e^ is the error (6.1) obtained from the n-th training example. 
The BPA training rule is derived for an arbitrary node with weight 
w on one of its incoming connections, and after that it is speciahzed for 
the output node and for the hidden nodes. The first-order derivatives of 
the instantaneous error after this node with respect to the weight is: 
dEn^dEndj^ 
.g^. 
dw 
dp dw 
where p is the activation polynomial at the node whose output influences 
the error, and tt; is a weight parameter. Such error derivatives dEn/dw 
are calculated not only for each weight from this polynomial but also for 
each weight in the whole network. 

152 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
The first component dEn/dp in the error derivative reflects the node 
error change as a function of the change in the input feeding that node. 
In case of tree-structured PNN, the node inputs can be weighted by any 
of the activation polynomials pi,P2) â¢â¢â¢)Pi6 defined in Table 2.1. The 
second component dp/dw reflects the error change as a function of the 
specific weight w in the node polynomial. When the complete bivariate 
quadratic polynomial is used, there are five partial derivatives: 
ap ap _ 
dp _ 
, dp^_ 
di__ 
^ dp__ 
X â ââ, ââ â X\, ââ â ^ i , -â- â X2, ^ â â X2, -^â â X\X2 
yO'O} 
aw aw 
aw 
aw 
aw 
aw 
Two kinds of error derivatives dEn/dw 
are obtained: one for the 
output node, and one for the hidden network nodes. 
Let's assume that the output node is indexed by k and the hidden 
nodes with outgoing connections feeding it are indexed by j . The error 
derivative for the output node with respect to its weights is: 
dEn 
dEn dpk 
dwkj 
dpk dwkj 
(6.6) 
where pj^^ is the activation polynomial at the output root node, and w^j 
is the weight from a child node at the j-th level to the root k. 
Since the activation polynomial determines the output -P(x) = p/., the 
derivative with respect to this polynomial is: 
^ 
= -{yn-PM) 
(6.7) 
OPk 
where 2/n is the desired output, and P{'Xn) is the estimated PNN output 
with the n-th example. The output P(xn) is used instead of p^ to 
emphasize that if additional bounding is applied at the output node, for 
example using the sigmoidal squashing function, then this equation (6.7) 
should be multiplied by its derivative. 
The error derivative with respect to a weight reflects the fact that the 
error depends on the weight through the activation polynomial that is 
fed by a signal multiphed by this weight. The error is actually a result of 
the hierarchical cascade of activation polynomials. The error derivative 
with respect to a weight on a connection feeding hidden node at level j 
by a node output from the immediate lower z-th level, can be obtained 
using the chain rule as follows: 
dEn ^ dEn dpj 
.^ 
dwji 
dpj dwji 
where Wji is the weight on the link from node i up to higher parent node 
j in the tree. The difliculty to obtain this derivative comes from the 
problem to determine the first multiplier dEn/dp j . 

BackpropagaUon Techniques 
153 
The multiplier dEn/dpj 
accounts for the sensitivity of the error to 
the output of the node. Finding these error derivatives, with respect 
to the activation polynomials in the hidden nodes, is involved as it is 
influenced by the complex interrelationships between the polynomials. 
The complex dependencies arise due to their composition within the tree 
structures. The partial derivatives dEn/dpj 
for hidden nodes in layer j 
depend on the errors committed by nodes k in the higher layer: 
dEn ^dEndpk 
.g^. 
dpj 
dpk dpj 
where p/. is the activation polynomial in the parent node at depth /c, 
and pj is the activation polynomial in the child node at the lower depth 
j . Thus, the error sensitivities are computed backwards from the root 
output node to the fringe nodes, and historically it is referred to as 
backpropagated error and the algorithm backpropagation. 
The derivative dpk/dpj 
also remains a polynomial. 
There are two 
alternatives for this derivative p'j^- = dpk/dpj, 
depending on whether the 
j-ih 
node is fed with argument xi or with argument X2. For example, 
the derivatives of the polynomial P4(x) = WQ -\- wjxi + W2X1X2 + w^x^ 
are: p'^^ â wi -\- W2X2 + 2w^xi with respect to the feeding signal x^^ and 
P42 = 1^2X1 with respect to the signal X2' Since the variable xi always 
comes from the left child, the first derivative is called the left derivative. 
Analogously, since the second variable X2 comes from the right child 
node, the second derivative is called the right derivative. 
An activation polynomial in binary tree-structured PNN could be fed 
in three ways: 1) directly with input variables passed from the leaves 
Xi^ 0 < i < d] 2) with polynomial outcomes from lower layer nodes 
Xi = piix)^ 1 <i < 16, or 3) with an input and an activation polynomial 
output from a lower layer node. 
Delta Training Rules. The BPA for PNN training traverses the net-
work three times. It carries out [Nikolaev and Iba, 2003]: 1) a forward 
pass by bottom-up tree traversal for computing the output; 2) a back-
ward pass by top-down tree traversal for collecting the error derivatives; 
and, 3) bottom-up tree traversal, again for adjustment of the weights. 
The network weights are modified in the direction of the optimal error 
decrease which is opposite to the gradient on the error surface. 
The 
amount of steepest error decrease shows what individual weight changes 
should be made so that the outputs, as a function of these weights, 
approaches the true mapping. The weight learning formula that uses 
derivative information from the gradient is caUed the generalized delta 
rule. 

154 
ADAPTIVE 
LEARNING 
OF POLYNOMIAL 
NETWORKS 
The generalized delta rule for adjustment of the network weights after 
each n-th training example {xn^Vn) is defined as follows: 
w = w-\-Aw 
(6.10) 
where Aw is the weight update. It is computed in the optimal direction 
opposite to the error gradient with the formula: 
A ^ - - 7 / ^ 
(6.11) 
ow 
where r] is the learning rate, and En is the instantaneous error (6.3) 
measured between the target yn and the network output P(xn). 
The calculation of the two kinds of error derivatives dEn/dp^ 
for the 
output node and for the hidden network nodes, leads to different learning 
rules for the output and hidden nodes. While the root node directly in-
fluences the output error, the hidden node errors are filtered through the 
hierarchical network architecture. These two main kinds of error deriv-
atives can be applied to the three combinations of signals that enter a 
node through its incoming connections: 1) feeding two inputs; 2) feeding 
two lower layer node outputs; and 3) feeding an input and a lower layer 
node output. This is why several training rules are developed below, 
and have to be instantiated for each of these cases. 
Delta Rule for Output Node Weights, 
Assume that the output node 
is indexed by k and the hidden nodes with outgoing connections feeding 
them are indexed by j . Then, the partial derivative for the output 
node is dEn/dpk 
â â{Vn â P{'>^n)) (6.7). This amount is traditionally 
substituted in the expression for the error derivative with respect to the 
weight dEn/dwkj 
= [dEn/dpk] 
{dpk/dwkj) 
as a separate variable called 
hackprop error^ which is also referred to as local gradient: 
h
-
^ 
(6.12) 
dpk 
After introducing this substitution for the root polynomial network 
node 6k = Vn â -P(Xn), assuming that P(x^) = pk-, the delta rule for the 
output node becomes: 
dEn 
' dwkj 
where x'^ is the derivative of the /c-th node activation polynomial with 
respect to the weight x'^. = dpk/dwkj. 
Such delta rules should be spe-
ciahzed for all of the available weights: WQ^ ici,..., w^'^ 1 < c?' < 5. 
Delta Rule for Hidden Node Weights. Suppose that the hidden nodes 
are indexed by j as above and the hidden nodes in the previous, lower 
Awkj = -V-^ZT' 
^ ^^kx'kj 
(^â¢^^) 

BackpropagoMon Techniques 
155 
network layer whose outgoing connections feed them are indexed by i. 
Since in tree-Hke PNN each node output feeds exactly one parent higher 
in the network, the backprop error dE^/Opj 
â (dEn/dpk) 
{dpk/dpj) 
obtained using the chain rule is: 
^ j - ^ 
= i-^k)Pkj 
(6.14) 
where 6k = dEn/dpk 
is the local gradient from the parent node, and p'^ â¢ 
denotes the backward propagated derivative from the /c-th node down 
to the j - t h node p'^^ ~ 
dpk/dpj. 
Using the error 6j in the derivative dEn/dwji 
= (dEn/dpj) 
(dpj/dwji)^ 
leads to the following the delta rule for the hidden 
nodes: 
dEn 
where x[' â dpj/dwji. 
Again, the weight update Awji should be instan-
tiated for all of the available weights: WQ^ '^^iv? '^d/i 1 < c?' ^ 5. 
As an illustrative example let consider the PNN given in Figure 6.1. 
The activation polynomials allocated in the network nodes are: 
P3 
= 
'^30 + '^3lP5 + '^32^53^4 
(6.16) 
PA 
= 
'^40 + W41X1 + tt;42XiX2 -h '^43^1 
(6.18) 
The output node derivatives with respect to weights w^i and 11)^^2 
are easy to determine as they are directly proportional to the output 
error {yn â P^Xn)) and the particular signal feeding the corresponding 
polynomial term: 
dEn 
dWr^l 
dEn 
dw^2 
= 
- ( 2 / , - P ( x , J ) p 5 
(6.19) 
- 
-{yn-P{^n))P5x^ 
(6.20) 
where P(xâJ == P3J P5 is the output from the left root child produced with 
input vector x^, and x^ is the fourth element of the same input vector 
Xn. Note that here the feeding signal is not directly the polynomial 
output from the child node because it is transformed into a monomial 
term. For example, when deahng with weight wr^2^ the parent node with 
activation polynomial ^3 is not fed exactly by the output of the child 
polynomial p5, but with the product ^5X4. This is because the signals 
coming from the lower layers are expanded to enter the corresponding 
terms of the bivariate activation polynomials. 

156 
ADAPTIVE 
LEARNING 
OF POLYNOMIAL 
NETWORKS 
P5(x)=^50+^51-^3+^52/'4 
Trainable Weight 
Activation Polynomial 
( Function Node) 
Input variable 
( Terminal Leaf) 
P/x)=W4o+W4,;f,+W,^,X2+W43X,2 
Layer k 
Layer ; 
Layer I 
Figure 6.1, A detailed view of a tree-structured Polynomial Neural Network with 
explicitly shown expanded connections and their weights. 
The derivatives for the hidden left child node (node 5 at layer j in 
Figure 6.1) with respect to its weights are: 
= 
-{Vn - P ( X n ) ) - ^ â ^ 
= - e n ( ^ 3 ] "l" '^32^4)^:3 (6.21) 
dEn 
-{Vn - 
P{^n)) 
dp3 dp^ 
-en{wsi + W^2XA)PI 
(6.22) 
dwr^2 
dp^ dw^2 
where p4 is the output from the right child, and 0:3 and ^4 are the third 
and fourth elements of the input vector x^. 
The derivatives for the hidden right child node 4 at layer i of node 5 
with respect to its weights are computed using the chain rule: 
dEn 
dEndp5 
dp4 
dEn 
dW4i 
dEn 
dw42 
dEn 
dw. 
dp 5 dp 4 dw 41 
dEndp^ 
dp4 
dp^ dp4 dw42 
dEn dps dp4 
dp5 
dEn 
dPh 
dEn 
{2WS2PA)^\ 
{2wr^2PA)x\^2 
{2wr^2PA)A 
(6.23) 
(6.24) 
(6.25) 
43 
dph dp4 dw4^ 
dps 
where the chain derivative of the output error with respect to the parent 
node 5 is dEn/dp^ 
= -{vn ~ P{yin)){'W3i + '^32^:4)â¢ 

BackpropagatAon 
Techniques 
157 
6.2.3 
Batch Backpropagation 
BPA can be implemented for training PNN in batch mode or in in-
cremental mode. The batch version of BPA suggests that we update the 
weights after presenting all examples to the network; that is, after com-
puting the exact gradient of the error function. It aims at minimizing 
the total error over all training examples. The weights in the exact gra-
dient are produced by summation of the derivatives of the instantaneous 
errors with respect to the particular weight: 
Aw = Aw + Awn 
(6.26) 
where Awn is the computed update from the n-th example. 
Table 6.1. Batch BPA for training nonlinear tree-like PNN. 
Backpropagation Algorithm for P N N (batch version) 
step 
Algorithmic 
sequence 
1. Initialization 
2. Network 
Weight Training 
Data V = {(xn,yn)}n^i^^^id learning rate r] â 0.1, 
Initial deltas: A Wkj 
0. 
Repeat for each epoch 
a) For each training example (xn,2/n), I ^n, < N, do 
i) Perform a forward pass with input Xn 
- calculate the output P(xn). 
ii) Perform a backward pass 
- compute the output node error and delta 
Sk = Vn - P(Xn) 
Awn,kj = V^h^'kj where x'^^ = 
dpk/dwkj. 
- compute the hidden node errors and deltas 
Sj = {-6k)p'kj where p'},j = 
dpk/dpj 
Awnji 
= T]6jx'ji^ where x'jj â 
dpj/dwji. 
iii) Accumulate the weight changes 
AWkj = AWkj + AWn,kj 
AWji = AWji + AWr,,,ji 
b) Update the weights w = w + A w. 
Until termination condition is satisfied. 
The batch BPA works in the following way: the examples are passed 
through the network, the output error is calculated, this error is back-
propagated, the weight changes are accumulated, and after exhausting 
the whole batch of given examples the weights are modified. The weights 
are adjusted only at the end of the training epoch with all examples. 
This batch training procedure is repeated until reaching satisfactory low 
error. A summary of this batch BPA is given in Table 6.1. 

158 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
6.2.4 
Incremental Backpropagation 
The incremental version of the BPA updates the weights iteratively 
after each example. After presentation of a training vector and comput-
ing the weight changes, they are introduced immediately to adapt the 
weights, without accumulating all changes until the end of the epoch. 
This style of training is sometimes called stochastic training. 
The stochastic training is expected to provide the ability to escape 
from entrapment at poor local minima on the error surface. This can 
happen as the shape of the error landscape changes with each example; 
actually, it is an instance of the complete error landscape which arises 
from the concrete training example. Because of this, the instantaneous 
error may become of such magnitude that it allows the algorithm to 
jump out of the current landscape basin. Theoretically however, it is 
impossible to precisely investigate this behavior. This is why the per-
formance of the incremental BPA is arduous to analyze. A summary of 
the incremental BPA is given in Table 6.2. 
Table 6.2. Incremental BPA for training nonlinear tree-like PNN. 
Backpropagation Algorithm for P N N (incremental version) 
step 
Algorithmic 
sequence 
Data V = {(xn,2/n)}n::r:iand learning rate r] = 0.1. 
Initial deltas: Awj^j â Awji = 0. 
Repeat for each epoch 
a) For each training example (xn,2/n), 1 < n < A^, do 
i) Perform a forward pass with input Xr,, 
- calculate the output P{:x.n). 
ii) Perform a backward pass 
- compute the output node error and delta 
Sk = Vn - P(Xn) 
Awkj = V^kx'kj where x'j^^ = 
dpk/dwkj-
- compute the hidden node errors and deltas 
Sj = {-6k)p'j,j where p'^^ = 
dpk/dpj 
Awji â r]6jx'j^ where x'j^ = 
dpj/wji. 
iii) Update the network weights 
iJUkj â Wkj + 
Awkj 
Wji = Wji + 
Awji. 
Until termination condition is satisfied. 
1. Initialization 
2. Network 
Weight Training 
Incremental vs. Batch Learning. The batch BPA is approxi-
mated by the incremental version. The latter computes an approxima-
tion of the gradient on the error surface, while the former computes 

BackpropagatAon Techniques 
159 
the true complete gradient. This means that the batch algorithm more 
accurately follows the optimal direction of error decrease, and it could 
be expected to attain better results. An advantage of the batch BPA 
is that it is less sensitive to noise and outliers in the data. The dis-
advantage of the batch BPA is that it seems to oversmooth the weight 
correlations when accumulating the weight changes, which often drives 
the algorithm to the closest minima only. Although one cannot decide 
whether the closest minima will be the best, it often occurs that the 
batch BPA becomes prematurely trapped at suboptimal results. 
In practice, the incremental version of the BPA frequently performs 
much better than the batch version in the sense of learning better weight 
values. This is mainly attributed to its stochastic character because 
when it tunes the weights after each example, the algorithm can avoid 
premature convergence and can achieve more sustained weight search. 
From a theoretical point of view, however, there is no guarantee that the 
incremental BPA will reach more accurate solutions than the batch BPA. 
From a computational point of view, the incremental version takes more 
time while the batch version consumes more memory. Both versions of 
the BPA for training high-order PNN exhibit time complexity 0{!F'^) 
where T is the number of the hidden network nodes. 
6.2.5 
Control of the Learning Step 
The convergence of the backpropagation algorithms can be improved 
by proper control of the learning steps that they take; that is, by tuning 
the delta rule for careful weight adjustment. When the BPA is applied 
to PNN, the learning step control needs special attention because the 
activation polynomials are fed by inputs with considerably different mag-
nitudes which makes the operation of the training algorithm difficult to 
navigate. Another important factor that affects the BPA when apphed 
to PNN is that the activation polynomials are often unbounded; their 
outputs are not filtered through the sigmoid function, rather they are 
used as they are. This may cause large output errors that are problem-
atic to handle. The activation polynomials can be deliberately bounded 
but this could be detrimental to the overall accuracy. 
There are two groups of approaches to controlling the weight learn-
ing step: 1) approaches that use additional factors in the delta rule for 
enforcing progressive search, such as adding a momentum term [Rumel-
hart et al., 1986]; and 2) approaches for tuning of the learning rate in 
the delta rule. The most typical approaches from the second group are 
the bold driver method [Battiti, 1989], search then converge [Darken and 
Moody, 1990], and the delta-bar-delta method [Jacobs, 1988]. 

160 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
Learning with Momentum. Research into neural network training 
has found that a momentum term is necessary for maintaining the oper-
ational stabiUty of the BPA. The problem is that during gradient descent 
search the weight vector often oscillates. For example, when proceeding 
on a slow, narrow valley on the error surface, the BPA oscillates and 
does not make fast, downhill steps. The convergence of the BPA can 
be accelerated by adding a momentum term to the weight update rule. 
This strategy allows us to move the weight vector across even flat regions 
on the error landscape, and so helps to avoid unnecessary oscillations. 
The delta training rule using momentum proportional to the previous 
learning step is: 
Wn^l 
= Wn-\- ^Wn 
"h aAWn-l 
(6.27) 
where Wn is the weight, Awn is the weight update computed according 
to the delta rule, Awn-i 
is the previous weight update, and a is the 
momentum rate. The momentum a is usually a small constant 0 < a < 
1, and it can be used in both incremental and batch versions. 
Learning Rate Tuning. Distinct methods for learning rate tuning 
have been proposed for the BPA. Common to these methods is the idea 
of changing the learning rate parameter at each step; in the incremental 
BPA we modify the learning rate after each training example, and in 
the batch BPA we modify the learning rate at the end of each training 
epoch. Tuning of the learning rate is reasonable as the error landscape 
has specific characteristics in the various regions, hence adapting the 
learning rate even by a constant factor, but depending on the underlying 
error surface, can be beneficial for stabilizing the performance. After 
developing such learning rate adaptation schemes, they can be applied 
in different ways: 1) traditionally using a global learning rate for all 
weights in the network; 2) using different learning rates for each hidden 
network node; 3) using a diff'erent local learning rate for each particular 
weight; and 4) using a different learning rate for each layer. 
Learning Rate Calibration. The convergence properties of the learning 
algorithm have been studied from different perspectives. It is well-known 
that in order to guarantee convergence of the weight update rule in the 
mean, the learning rate r]n should be bounded by the following inequality 
[Ham and Kostanic, 2000]: 
where H is the Hessian matrix (6.39,6.40). Learning rates selected from 
this interval help to attain stable learning performance. 
Bold Driver Method. This method prescribes to modify the learning 
rate by a constant factor in order to sustain the search if it progresses 

BackpropagaMon Techniques 
161 
or to hinder it if it does not. The bold driver accelerates the weight con-
vergence in the same direction if the last weight adjustment has reduced 
the error, and decelerates the search if the recent weight adjustment has 
failed to decrease the error. The bold driver method [Battiti, 1989] offers 
the following formula for updating the learning rate: 
rin+i = ( ^ 1 " " '[^- < ''â¢'-' 
(6.29) 
' ^ 
1 7 r]n 
otherwise 
^ 
^ 
where 7"^ is an accelerating constant, 7" is a decelerating constant, and 
77n+i is the learning parameter to be used in the next iteration of the 
training algorithm. The accelerating constant 7"^ is typically selected 
greater than one 7"^ > 1. A formula that can help to determine its value 
is: 7"^ = (1 + (1 â 7~)c), where c is a constant c = 10. The decelerating 
constant is taken less than one 0 < 7~ < 1. 
Search Then Converge. This method suggests to tune the learning 
rate by a factor that varies during the search process: r/^+i = c/i9, 
where c is a constant, and the parameter 'd is defined separately for 
each iteration. The basic idea is to implement such a learning sched-
ule that allows initially large jumps in the weight space, and after some 
training period, enforces more precise, slower focusing toward near min-
ima. Thus, when the backprop training proceeds the search matures 
and replaces the phase of exploring the error landscape with a phase of 
exploiting the neighborhood areas. The formula for such a search then 
converge [Darken and Moody, 1990] schedule is: 
^ " - = I
^ 
(6-30) 
where 770 is a preselected constant, n is the iteration number, and r is 
the intended maximal number of training epochs. 
A sophisticated version for implementing a search then converge [Darken 
and Moody, 1990] schedule is given by the rational function: 
l + (c/r/o)(n/r) 
1 + (c/77o)(n/r) -f T(P>^/T^) 
where 770 and c are constants, n is the iteration number, and r is the 
epoch number. All parameters have to be selected heuristically. 
Delta-Bar-Delta Method. This method relies on the idea that if the 
error derivative keeps unchanged during several consecutive iterations 
there is a steady error decrease and, hence, the learning step may be 
increased. Otherwise, if the sign of the error derivative changes, this 
indicates that the learning process oscillates and the learning step has to 

162 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
be decreased. The delta-bar-delta method [Jacobs, 1988] sets individual 
learning parameters for each weight according to the formula: 
Arj, i,n-^l 
_ | 
l^Vi,n 
ii gi,ngi,n-} 
> 0 
(6 
32) 
1 "y~T]i,n otherwise 
^ ' 
^ 
where r/^^^i+i denotes the learning rate for the z-th weight at the n-th 
iteration, gi^n = dEn/dwi is an averaged estimate of the error derivative 
with respect to the particular i-th weight, and 7"^ and 7" are growth 
and shrinking parameters taken from the intervals 0 < 7" < 1 < 7^. 
The averaged error derivative is computed using the equation: 
gt,n = (1 - n)dEn/dwi + KdEn-\/dwi 
(6.33) 
where /s: is a small positive constant 0 < /^ < 1, E'^ is the instantaneous 
error, and Wi is the weight. 
6.2.6 
Regularized Delta Rule 
The weight training performed using BPA can be controlled by reg-
ularization for producing smoother function mappings. When regular-
ization is applied in backpropagation training, it also helps to stabilize 
the behavior of the learning algorithm. This improvement of the re-
liability in the training procedure is an effect of the incorporation of a 
penalty term in the cost function that accounts for the model complexity 
represented by the magnitudes of the network weights. 
Viewed in context of the bias/variance dilemma, the idea behind reg-
ularization is to diminish the bias contribution to the error due to the 
average level of fitting by means of adding another complexity term. 
This correcting complexity penalty controls the smoothing during train-
ing because the training algorithm is derived by differentiation of the 
augmented cost function. This additional term added to the error is 
usually a function of the weight magnitudes, and it is also known in 
the connectionist research as weight decay. After augmenting the cost 
function to E - 0.5 (^n^i{Vn - P{^n^ w))2 + A J27Li ^^), the following 
regularized delta rule for weight training is obtained: 
w = {l -r]X)w + Aw 
(6.34) 
where A is the regularization parameter, and rj is the learning rate. 
Neural network research has proposed many regularization functions 
[Haykin, 1999], each having own their advantages and disadvantages. 
The weight decay formula Ylj^] 
[Hertz et al, 1991] may disproportion-
ately favor many small weights because the curvature of this formula 
rapidly increases and does not enable us to make a precise difference 

Backpropagation Techniques 
163 
between weights in the interval â1,0 <w < 1.0. The weight elimination 
formula X^j(tt;^/(1 -f '^j)) [Weigend et al., 1992] discourages too many 
small weights and overcomes, to a certain extent, this problem. How-
ever, it is still steep in the interval â1.0 < w < 1.0. In addition to this, 
it arbitrarily favors significant weights w < â2.0 and w > 2.0. 
6.3 
Second-Order Backpropagation 
The first-order BP techniques for neural network training explore the 
error surface inefficiently, since they change the weights in a modest 
way. A common drawback of the first-order BP techniques is their slow 
convergence. The weight vector often tends to oscillate in the basin of 
attraction on the error surface going downward on its slopes, or jumps 
from one slope to another until reaching the minimum. Gradient descent 
search strongly depends on the characteristics of the error landscape, 
which is extremely large and cannot be entirely traversed. 
The problem is that it is impossible to determine the best configu-
ration of network weights in an admissible time. It has been studied 
theoretically that the problem of learning the weights of a fixed neural 
network that performs exactly to the desired mapping is an NP-complete 
problem [Judd, 1990]. One possibility of avoiding such difficulties to a 
great degree is to start training not with arbitrary initial weights, but the 
weights estimated by least-squares fitting as suggested in the first IGP 
phase of learning PNN. Another possibility is to consider the discussed 
strategies for learning rate control, but they require us to heuristically 
introduce a lot of parameters. The tuning of the learning step may 
help to avoid entrapment into local minima on the slopes down to the 
minimum of the valley, but this is not guaranteed. 
Ideas for implementing more powerful PNN training methods are pro-
vided by optimization theory. The optimization methods suggest that 
weight updates for faster convergence can be made using information 
about the second-order properties of the error surface. The optimal 
search direction can be determined using the second-order derivatives of 
the error function with respect to the weights vector. 
The BP framework can also be extended to obtain the second-order 
derivatives, that is the Hessian matrix. The possibility of calculating the 
Hessian [Bishop, 1995, Buntine and Weigend, 1991a, Pearlmutter, 1994] 
in PNN enhances their potential because: 1) it helps to develop contem-
porary second-order BP techniques for efficient PNN training [Bishop, 
1995, Shepherd, 1997]; 2) it allows us to implement second-order net-
work pruning strategies [LeCun et al., 1990, Hassibi and Stork, 1993]; 3) 
it enables us to precisely compute the network complexity necessary for 
design of fitness functions; and 4) it provides a tool for adjusting various 

164 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
algorithm parameters. Several second-order techniques for BP are pre-
sented below. The corresponding delta rules of the kind: w = w + ?7Aw 
tell us how to make adjustments of the weights vector w. 
6.3.1 
Second-Order Error Derivatives 
The Hessian Matrix. The idea for using the Hessian matrix of 
second derivatives of the error with respect to the weights, comes from 
the Taylor series expansion of the error function in the vicinity of the 
current weight vector. The local neighborhood of the weight vector 
consists of all vectors within some predefined distance. If we assume 
that the current weight vector is WQ, then the weight vector at the next 
step is: w = WQ + Aw, which is the weight vector modified by the 
learning operator. The Taylor series approximation of the error in the 
local neighborhood of this weight vector is: 
E{wo + Aw) = E{wo) -f Aw'^g -f- ^Aw^HAw -f O (||Aw||^^) (6.35) 
where VÂ£^(wo) denotes the gradient at point WQ defined as follows: 
dE{w 
g = 
VE 
g = VE = 
dw 
dE{w) 
dE{w) 
dE{w) 
dwQ 
dw\ 
dww 
(6.36) 
(6.37) 
where W is the number of the network weights in the model. 
Let us assume the cost function: E = 0.5 Yln^iiyn â P^^n-, w))^. Then 
the gradient vector entries can be computed as follows: 
dE{w) 
^ ^ 
_^^ 
,,aP(xn,w) 
where P(xn,w) is the PNN output computed with weights vector w 
when applied to input vector Xn. 
The Hessian matrix with the second-order error derivatives with re-
spect to the weights is: 
H.V^E=^^^(-) 
dw'^ 
d^Eiw 
with entries H^j ~ [H]^ â dwjdwj 
(6.39) 
where Hij^ specifies its element at row i and column j . 

BackpropagaMon Techniques 
The full Hessian matrix is defined explicitly as follows: 
H 
165 
d'^Ejw) 
dW2dlD\ 
d'^Eiw) 
^^vw9^v^ 
d'^Ejw) 
dwi dw2 
d^Ejw) 
dW2dW2 
d^Ejw) 
d'^Ej-w) 
dw^ dw\Y 
d'^Ejw) 
dw2dww 
d'^Eiw) 
dwwd'^i^w 
(6.40) 
where its size M^ x W is determined by the number of the weights W. 
Assuming an error: E = 0.5E^^i(2/n - P(xn, w))^ = 0,5E^^i en^ 
the Hessian entries are: 
/a2p(xâ,w)\l 
dwidwj 
(6.41) 
where P(x^,w) is the polynomial network output. 
The gradient vector and the Hessian matrix can be expressed by means 
of common subelements for developing concise formulae for second-order 
training algorithms. The common subelements are the network output 
derivatives with respect to the weights: 
J. 
aP(Xn,w) 
dwi 
(6.42) 
where P{'Xn, w) is the network output from input x^. These subelements 
Ji are to be arranged in the so-called Jacobian matrix defined as follows: 
J = 
a^p(xi,w) 
dwi 
a^p(x2,w) 
dw-[ 
a^P(xyv,w) 
a^p(xi,w) 
dW2 
a^p(x2,w) 
dW2 
a^P(x;v,w) 
d^V2 
a^P(xi,w) 
divw 
a^p(x2,w) 
dww 
a^P(x;V,w) 
dw\Y 
(6.43) 
where A^ is the number of the data, and W is the number of the weights. 
The gradient can be expressed with the Jacobian in the following way: 
g - - J'^e 
(6.44) 
where e =[eo, ei,..., e/v] (6.2) is the error vector. 
The Hessian matrix can be formulated using the Jacobian as follows: 
H 
1_ 
N (J'^J - R) 
(6.45) 
where the matrix R is made of elements R = 
eri{d'^P{-Xn,w)/dwjdwj). 

166 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
7^-propagation Algorithm for PNN. The entries of the Hessian for 
feed-forward networks have been precisely specified [Bishop, 1995]. This 
specification, however, is difficult to implement with a computational 
algorithm despite the attempts of several authors who explained it in 
detail [Buntine and Weigend, 1991a]. A more efficient algorithm for 
evaluation of the complete Hessian is available that prescribes calculation 
of the second order error derivatives by making forward passes from the 
inputs to the tree root, and backward passes from the root node down 
to the fringe nodes. It is called 7^-propagation [Pearlmutter, 1994], as 
its steps can be performed using the framework of the classical BPA. 
The advantage of 7^-propagation is that it allows us to evaluate the 
second-order error derivatives along with the first-order error derivatives 
in linear time with respect to the number of network weights. 
The 7Z-propagation algorithm [Pearlmutter, 1994] is specialized here 
for PNN. This algorithm reveals that the full Hessian matrix can be 
obtained by multiplying it with a binary vector of size equal to the 
number of weights. When successively setting the vector components 
to one, this algorithm generates the Hessian matrix column by column. 
The accumulation of all examples requires us to perform N forward 
and A^ backward passes, where N is the number of training examples. 
The 7^-propagation algorithm adapted for PNN below makes successive 
forward and backward tree network traversals for computing the error 
derivatives, updates the Hessian matrix after each next example, and 
repeats such cycles W times, once for each column of the Hessian. 
The basic idea is to produce the exact Hessian matrix as a result of 
the multiplication: 
v'^H = v'^V^^ - Y^V{VE) 
(6.46) 
where v is a binary vector, and V is the gradient operator. The 7^{-} 
operator is defined as follows: 
7^ = v'^V 
(6.47) 
which is estimated by doing 7^-propagation. 
Let us assume again that the root node indexed by k produces output 
Pk using finear and nonlinear terms pk = ^j^kjPj^ 
where wj^j is a 
weight on connection from node with activation polynomial pj toward 
node with pk- Its terms are fed by its left and right child polynomial 
outputs Pj = ^^ 'WjiPi when the activation polynomial is fed by outputs 
from nodes below, or pj = Yli^ji^i 
when only input variables Xi enter 
the activation polynomial through leaves. These quantities are computed 
during the 7^-forward propagation, along with the estimation of the TZ-

Backpropagation Techniques 
167 
operator in the following sequence: 
MPJ} 
== Y^^j^^^ 
(6.48) 
i 
MPJ} 
= E^J'^{P'} + E^i'P' 
(6.49) 
i 
i 
MPk} 
= Y^^kj^{P:j} + Yl^kjPj 
(6.50) 
j 
j 
where 7Z{pk} is the operator applied at the root, and 7Z{pj} is the op-
erator applied at hidden nodes separately for cases with terms entered 
directly by node outputs or by inputs. 
The error derivatives are computed with the BPA. The output root 
network node and the hidden nodes are manipulated separately. At 
the root output node the deltas are computed: 6^ = yn â P(x^). At 
the hidden functional nodes deltas are also computed but in a different 
way: 6j â {â^k)Pkj where p'^ â¢ = dpk/dpj. These deltas are used by the 
7^-backward propagation pass to estimate the 7^-operator as follows: 
Tl{Sk} 
= Tl{pk} 
(6.51) 
nS.i} 
= n{-6,}pij + {-6,)TZ{pij} 
(6.52) 
where 7l{6k} and K{6j} are the apphcations of the operator at the root 
node and at hidden nodes of the network respectively. 
The error derivatives with respect to the weights also have to be com-
puted: dEn/dwkj = âSkx'f^j, where x'^j = dpk/dwkj, and dEn/dwji â 
â6jx'j^^ where x',^ = dpj/dwji. The 7^-operator is instantiated as follows: 
^ { ^ 1 
= -7^{4}4,â¢-<5fc7^{4i} 
(6.53) 
^ { 1 5 } = -^{5.,}4-'5.^{4:} 
(6-54) 
where the operators Tl{x'^,\ and lZ{x'-'} are computed analogously, bear-
ing in mind that their arguments x'j^- and x'^- are monomial terms. 
Several theoretical and practical clarifications concerning the compu-
tation of the full Hessian matrix (6.39,6.40) should be made: 1) the 
exact evaluation of the Hessian matrix according to the sophisticated 
algorithm of Bishop [Bishop, 1995] has space complexity 0{W'^) and 
time complexity 0{W'^)\ 2) the 7^-propagation algorithm of Pearlmut-
ter [Pearlmutter, 1994] is much faster and it is often preferred in prac-
tical implementation because it has only linear cost 0{W)\ 3) when the 

168 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
Hessian is inverted its cost increases to 0{W^)] 4) diagonal approxima-
tions of the Hessian can be made with low, hnear cost O (W) in several 
ways: using gradients d'^E{w)/{dwidwj) 
â 
[dE{w)/dwi){dE{w)/dwj)\ 
using finite diff"erences according to Shepherd [Shepherd, 1997]; and us-
ing central diff'erences after Nabney [Nabney, 2002]. 
Practical tasks however often lead to Hessian matrices which are not 
simply diagonal so the usefulness of the diagonal approximations may 
be hmited [Bishop, 1995]. 
The algorithm for performing 7^-propagation on polynomial network 
models is given in Table 6.3, Note that this implementation incorpo-
rates the steps of the backpropagation algorithm that evaluate the error 
gradients with respect to the weights, so there is no need for invocation 
of its components in several stages. 
Tabic 6,3. Summary of the 7^-propagation algorithm for tree-like PNN. 
7^-propagation Algorithm for P N N 
step 
Algorithmic 
sequence 
^Initialization 
Data V = {(xn,yn)}iLi5 and v = (0,0, ...,0) of size I 
xW. 
2. Network 
For each weight c, \ < c <W 
Weight 
a) Set the vector entry Vc = 1.0 
Training 
b) Compute the product v'^H in one epoch: 
For each example (xn,2/n), 1 < n < A'', do 
i) Perform a forward pass and estimate 
Vj = Yli'^JiXi^ OT Pj = 
J^.WjiPi 
Pk = Yl^ wkjPj 
MPk} = E i '^kjMpj} 
-h E,: '^kjPj. 
ii) Perform a backward pass 
- calculate the output delta and derivatives 
h. = Vn - P{Xn) 
v'kj = dpk/dpj, 
and a:';,^ = 
dpk/dwkj. 
- apply the R-operator 
n{dEr,./dw,.j} = n{~6,,}xi,^ + {-Sk)n{x',,j}. 
- calculate the hidden deltas and derivatives 
6j = {-6k)p'kj, and a:^-,; = 
dpj/dwji. 
- apply the R-operator 
n{dEn/dwji} = n{-6j}x'j, + {~6j)n{x'j^}. 
3. Evaluate 
Extract the Hessian 
the Hessian 
from the dot product v'^H. 

BackpropagaUon Techniques 
169 
6.3.2 
Newton's Method 
The second-order Newton's method [Bishop, 1995, Haykin, 1999, Shep-
herd, 1997] may be followed to develop an accelerated PNN weight train-
ing algorithm. It suggests to use second-order error derivatives when 
computing the weight change direction for making the next learning 
step, and reaches a solution relatively quickly if the starting point is 
well located. The weight learning algorithm for implementing the Net-
won's method can be derived starting from the Taylor's approximation 
of the error function. The quadratic approximation of the error using 
Taylor's expansion (6.35) is: 
VE{wo -h Aw) = VE{wo) + HAw 
(6.55) 
where WQ + Aw is the weight vector after making a learning step Aw, 
and H is the Hessian matrix (6.40). The optimal learning step Aw can 
be determined by seeking the minimum of this function. The minimum 
VÂ£^(wo H- Aw) â 0 can be found by differentiation with respect to Aw. 
This gives the following update direction or Newton's rule: 
Aw = H-^g 
(6.56) 
which can be taken to update the weights vector w = w +Aw. Newton's 
rule includes, as a special case, the traditional first-order linear weight 
update rule: Aw = ~rjVE{wo) â ârjg. 
Theoretically Newton's method should converge to the optimal weight 
vector. Practically, however, the convergence is not guaranteed because 
of the need to perform inversion of the Hessian matrix, which unfortu-
nately is often ill-conditioned and may not be computationally invertible. 
Only when the Hessian is positive definite (that is, all its eigenvalues 
are positive), it is invertible and will lead to the minimum point. The 
most serious practical concern is that at each algorithmic step the exact 
Hessian has to be stored and computed. 
What makes the Netwon's training rule attractive is the suggestion 
to use individual learning rates for each network weight. The individ-
ual learning rates can be obtained using second-order error derivatives. 
Netwon's rule actually makes the learning rate for each weight Wij pro-
portional to the corresponding diagonal element [H]^j from the Hessian 
matrix. The use of individual learning rates helps to focus the weight 
search algorithm in the right direction at each algorithmic learning step. 
This is not guaranteed when only first-order gradients are used because 
they may not point in the right search direction, and as a consequence 
the learning algorithm may be unstable. 

170 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
6.3.3 
Pseudo-Newton Training 
Due to the difficulties associated with the straightforward apphcation 
of the Newton rule for training neural networks, including the PNN 
developed here, it has been found that a rough approximation of that 
rule is also useful in practice [Becker and LeCun, 1989]. The network 
weights can be trained using the following pseudo-Newton rule: 
^ ^ = -g9^Â£(w)/9w^ 
(^-^^^ 
which avoids the problematic inversion of the Hessian matrix (6.39). 
In order to facilitate the learning process and to perturb the weights 
even when the weight vector appears to be a point on a flat region of 
the error surface, regularization should always be performed: 
dE 
u 
d^E 
dw^ + A 
(6.58) 
where A is the appropriately chosen regularization parameter, and | â¢ | 
denotes the absolute value of its argument. The absolute value is neces-
sary in order to properly handle negative curvatures of the error surface. 
Since classical statistical techniques are not directly applicable in the 
case of a highly nonlinear polynomial network model, the recommenda-
tion is to select the values for the regularization parameter heuristically, 
with respect to the inductive task addressed. 
The practical pseudo-Newton rule above (6.58) can be considered 
for training polynomial networks by incorporating it in the algorithmic 
framework of the backpropagation algorithm. 
6.3.4 
Conjugate Gradients 
The conjugate-gradients is a quasi second-order method that can be 
used for achieving faster convergence in network training [Bishop, 1995, 
Shepherd, 1997]. Its efficacy is in that it reaches the convergence speed of 
second-order methods without explicitly using second-order error deriv-
ative information; that is, without exphcit reference to the Hessian ma-
trix. The idea behind this algorithm is to compute the weight updates 
in direction conjugate to the previous updates, which has a stabihzing 
effect on the search performance and accelerates the downhill moves on 
the error landscape. The conjugate direction is perpendicular to the 
gradient of the quadratic function. The Hessian is used only for the 
theoretical derivation of the optimization algorithm, while its direct use 
is avoided assuming that it is almost constant in the neighborhood on 
the error surface especially when it is close to some minimum. 

Backpropagation Techniques 
171 
The weight modifications following the gradient can be written as a 
linear function of the previous modifications in the following way: 
AWn-f 1 = -gn-f 1 + '^nAWn 
(6.59) 
where g^+i is the gradient vector obtained at the current n-th iterative 
step, and <;^ is a parameter. Assuming that initially AWQ = â go^ several 
rules for computing the parameter (^n after each next training example 
can be applied. The two most popular rules that are often adopted in 
practice to identify the values of the parameter ^n are: 
- the Fletcher and Reeves rule [Fletcher, 1987]: 
T 
^n = âV 
(6.60) 
gng^ 
where gn+i is the current gradient vector, and g^ is the previous gradient 
vector computed at the previous iteration of the training algorithm. 
- the Polak-Ribiere rule [Polak, 1971]: 
,n =^ g^+^(g^+^~^") 
(6.61) 
gnSrj 
where again g^+i is the current gradient, and g^ is the previous gradient. 
The Polak-Ribiere formula (6.61) is found to be more efficient. 
6.3.5 
Levenberg-Marquardt Method 
Newton's method (6.56) offers ideas for development of general second-
order training algorithms for neural networks, and the gradient descent 
algorithm (6.11) may be considered its specialization. This line of rea-
soning has lead to the derivation of an optimization method that tunes 
the learning direction in the weight update rule by a single parameter. 
The Levenberg-Marquardt method [Levenberg, 1944, Marquardt, 1963] 
provides a stepwise weight modification formula that can be incorpo-
rated into a network training algorithm. The basic idea is to reduce 
the gradient to zero which can be envisioned as making jumps directly 
toward the closest minimum on the error surface. 
The Levenberg-Marquardt algorithm may be applied for training PNN 
using the following weight modification formula: 
A w - - ( H r f + AI)-^g 
(6.62) 
where H^ is the diagonal approximation of the Hessian matrix with 
second-order error derivatives, g is the gradient vector (6.36,6.37), and 
A is the regularization parameter. 

172 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
The diagonal elements of the Hessian can also be written in terms of 
the Jacobian matrix entries of the derivatives of the polynomial outputs 
with respect to the weights: 
N 
[tidh = Y^l^knl^hn 
(6.63) 
n=l 
where [J]i,n == dP{yin)ldwi. 
Having this correspondence is useful for 
rewriting of the training algorithm in a more useful format. 
The Levenberg-Marquardt training rule expressed using the elements 
of the Jacobian is given alternatively by the matrix equation: 
A w - - ( j ' ^ J 4 - A I ) - ^ g 
(6.64) 
where g and J can be obtained by backpropagation. 
There are two important details to note in this formula (6.64): 1) 
in order to gain efficiency the diagonal approximation of the Hessian 
is adopted, which can be easily computed when backpropagating the 
error; and 2) in order to avoid numerical computation instabilities it 
includes a regularization factor. When the regularization tends to zero, 
A â> 0, the Levenberg-Marquardt rule (6.64) approaches the Newton's 
rule (6.56). Respectively, when the regularization parameter goes to 
infinity, A â^ oo, this rule (6.64) approaches the generahzed delta rule 
(6.11) for gradient descent learning. The Levenberg-Marquardt method 
has an advantage over these methods as it is less sensitive to the ill-
posedness of the Hessian matrix due to the use of regularization. 
Use of the Levenberg-Marquardt algorithm is recommended for train-
ing neural networks and PNN in batch mode. Alternatively, it can be 
used in incremental mode with the formula [Bishop, 1995]: 
where i and j are the weight vector indices. 
6.4 
Rational Backpropagation 
Following the basic ideas of the backpropagation algorithm (Section 
6.2), one can elaborate delta rules for gradient descent weight training 
of various polynomial network architectures. What is necessary is to 
reformulate the expressions for the error derivatives with respect to the 
concrete monomial terms in the activation function, and the derivatives 
of the parent node activation polynomials with respect to the incoming 
activation polynomial outputs from the preceding layer. This can be 
demonstrated by specializing the BPA for rational PNN. 

BackpropagatAon Techniques 
173 
The rational PNN (Section 3.5.4) computes a rational function as a 
fraction between the activation polynomial outputs provided by the left 
and right children of the root node. All other hidden network nodes 
below these root children in the tree-like network topology compute Hn-
ear and quadratic bivariate functions (Table 2.1). The rational BPA 
for weight learning in such polynomial networks requires us to pay spe-
cial attention to the two root children nodes while the update of the 
weights on connections to the remaining hidden nodes can be processed 
according to the generalized delta rules for standard PNN (Section 6.2). 
In order to demonstrate the derivation of the rational BPA, let us 
assume that the output node calculates the following polynomial ratio: 
Pd 
1 + '^dlPiS + Wd2PiA + U)d3Pi3Pi4 + "^dAPfs + +'^d5P?4 
(6.66) 
where p^ is the numerator and pd is the denominator polynomial. 
Since the numerator is passed by the left child node at the lower j-th 
layer, it is fed by node outputs and/or inputs from the previous i-th 
layer: Pn = WnO + ^niPn + Wn2Pi2 + ^nWi\Pi2 
+ '^nAPn + ^t;^5P?2â¢ The 
denominator is also fed by signals coming from the previous i-th layer, 
but they are different, so the variables in its terms have different indices: 
Pd = l + WdiPi^ + u)d,2Pi4 + "^d^PisPii + '^dAPis + '^d^Pu' The denominator 
in rational models always has a constant term equal to one. 
This rational PNN should be equipped with two distinct training rules 
for the weights in the root child polynomials, while the weights from the 
root to its children remain fixed to one [Zhu, 2003]. The error derivative 
of the output with respect to the weights in the root children polynomials 
can be found using the chain rule. The key point in the derivation is 
to note that these weights participate in a fraction which has to be 
treated as a rational function. 
The weights w^ influence the output 
through the numerator polynomial p^, while the weights w^ influence 
the output through the denominator polynomial pd] however, they build 
the output together as a homogeneous function. The error derivative at 
the numerator polynomial in the output node is: 
dEn 
dEndpk 
dpn 
. 1 / 
. ^ . â . 
~^ 
"" "^â~^~"^ 
"" -^kâx^j 
(6.67) 
dWnj 
Opk OPn OWnj 
Pd 
^ 
where the numerator weights are indexed by nj because they participate 
in the j-th activation polynomial at the lower layer. This component x'^^ 
is essentially the same as the derivatives in standard PNN. 
The weight modifications in the denominator polynomial are affected 
by the numerator polynomial. The error derivative with respect to the 

174 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
weights of the denominator polynomial in the output node is: 
dEn 
dEn dpk 
dpd 
c Pn 
, 
fr, nn^ 
dwdj 
dpk dpd dwdj 
pj ^ 
where Wdj are the denominator weights in the activation polynomial 
allocated in the right child node at the preceding j-th layer. 
The propagation of the output error down through the network de-
pends on the rational polynomial in the root. The error backpropagation 
process starts by sending the effects of the output error toward the hid-
den node polynomials through the root fraction. A complication arises 
because the derivatives of a function have to be computed. In the tree-
like rational PNN the backpropagation proceeds from the root children 
down toward the other hidden nodes; that is, the computation continues 
down from the ^-th layer. The backpropagated error carries the following 
root node polynomial derivative dpk/dpi with respect to an activation 
polynomial at the i-th layer: 
/ 
dPk 
1 (dPn 
dpd\ 
.^^^. 
Pki^"-^- 
"=-2 [ -^-Pd - Pn-w- 
(6.69 
dPi 
Pd V dpi 
dpi J 
where pj is an activation polynomial output from a child node. 
The rational backpropagation algorithm for weight training and mini-
mization of the instantaneous output error En = 0.5e^ (6.3) in rational 
PNN includes: 1) two delta rules for the weights in the root: one rule for 
the numerator polynomial and one rule for the denominator polynomial, 
and 2) one delta rule for the hidden network nodes. 
Rational Delta Rules for the Root Children Nodes. Consider the sub-
stitution 6k = dEn/dpk for the error derivative at the k-th root node 
polynomial in the network. The two rational delta rules for the root 
children node weights^ one for the numerator weights and one for the 
denominator weights, are: 
where x'^j denotes the activation polynomial derivative x'^j = dpn/dwnj^ 
and x^^- is the derivative x'^^ = dpd/dwdj. 
Rational Delta Rule for the Hidden Nodes. The rational BPA proceeds 
down the tree-Hke structure and computes the local gradients that are 
necessary to change the weights in the remaining hidden nodes. First the 
algorithm passes the previous backprop error (â6^) to produce the next 

Backpropagation Techniques 
175 
one, 6i ~ {~-Sk)Pki (6.14). The estimation of the polynomial derivative 
P'ki ~ ^Vk/dpi 
(6.69) is specific due to the fractional character of the root 
polynomial. Using the substitution for the derivative X'A^ = 
dpj/dwji 
(6.15), the rational delta rule for the hidden node weights becomes: 
dEn 
Aw^i = ~r] 
yjz 
dwj 
= r]6^x'j^ 
(6.72) 
where Wji is the weight on the connection from a node in the i-th neural 
network layer toward a node in the j - t h layer. 
The convergence of this rational backpropagation is expected to be 
relatively fast if performed after structural model identification, during 
which the weights are usually learned approximately and so they are 
expected to be close to their optimal values. 
Table 6.4 presents the 
framework of the backpropagation algorithm for rational PNN. 
Table 6.4. Incremental BPA for training high-order rational PNN. 
step 
Backpropagation for Rational P N N (incremental version) 
Algorithmic 
sequence 
D a t a P = {(Xn,2/n)}n-
Initial deltas: Awkj â¢ 
Repeat for each epoch 
a) For each training example (xr,,,2/n), 1 < n < A^, do 
i) Perform a forward pass using input Xn, 
to produce output P(xr,,). 
ii) Backward pass 
- compute the output node error and delta 
Sk =yn - P(xn). 
- compute backwards at the root children nodes 
left child node (numerator) 
Awnj = r]6k{l/pd)x'^j 
where x'^j = 
dpn/dwnj 
right child node (denominator) 
Awdj = r]8k{pn/pl)x'dj 
where x'^^ = 
dpd/dwdj. 
- continue down toward the fringe nodes and compute 
Si = {-6k)Pki 
where p'j^- = 
dpk/dpi 
Awji = r]8ix'ji where x'j^ â 
dpj/wji. 
iii) Update the network weights 
Wkj = Wkj -f 
Awkj 
Wji = Wji -\- 
Awji. 
Until termination condition is satisfied. 
1. Initialization 
2. Network 
Weight Training 
land learning rate 77 = 0.1. 
Awji = 0. 

176 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
6.5 
Network Pruning 
The intention behind polynomial network pruning is to discard from 
the model redundant, less important terms which do not contribute sig-
nificantly to optimal data fitting. It is known that too many terms 
cause overfitting and such models do not generalize well beyond the 
given data. The objective is to reduce the network complexity so that it 
still approximates the data well and improves the generahzation. Imple-
menting algorithms for trimming networks requires us to define criteria 
for termination of the weight removal. Guidelines for weight removal 
provides the bias-variance dilemma [Geman et al., 1992] (Chapter 9), 
according to which weights should be deleted until the sum of the sta-
tistical bias and variance reaches its minimum. The following pruning 
procedures can be repeated several times, and the resulting networks are 
retrained until settling at an acceptable solution. 
Another way of deciding when to stop removing weights is to use 
statistical criteria such as UEV (4.5), FPE (4.7), PSE (4.8), and GCV 
(4.10). They require us to apply the 7^-propagation algorithm (Section 
6.3.1) for computing the effective number of network 'parameters 7: 
W 
X 
^j^j^ = W ~Y 
r = W - XtraceiUr^) 
(6.73) 
^ Q i + A 
where W is the number of weights, ai are the eigenvalues of the Hessian, 
A is the regularization parameter, and H~^ is the inverse Hessian. 
The above two formulae for finding the efficient parameters ^j^i (6.73) 
may yield different values. This could happen due to computational 
inaccuracies during the computation of the eigenvalues, and during the 
inversion of the Hessian. In order to be convincing in the accuracy of 
the estimated number 7/VL5 this number may be checked by relating it 
to the equivalent number 7 that could be obtained using formula (4.6) 
applied as part of the OLS fitting procedure. 
6.5.1 
First-Order Network Pruning 
The basic idea behind first-order network pruning is to exploit the 
information contained in the error gradient for better adapting the net-
work structure to the data and for improving its generahzation. 
Stochastic Pruning. Weights that have less impact on the error 
gradient are less significant. Having computed the first-order error deriv-
atives in the PNN by means of BP allows us to estimate the standard 
deviation of the gradient. Weights whose magnitudes, divided by the 
standard deviation of the gradient vector, produce a very small value 
may be trimmed from the network. The significance s^j of a weight Wij 

BackpropagaMon Techniques 
177 
can be measured by the following t-test formula, which may be applied 
for stochastic network pruning [Finnoff et al., 1992]: 
l^^^+'^l 
(6.74) 
\/(l/A^)E^=i(.9n-ff)^ 
where the mean gradient entry 9 is : g~ ( V ^ ) Yln=i 9n made of el-
ements Qn â dEn/dwij 
(6.36). This stochastic pruning is a kind of 
magnitude-based method with the advantage that it accounts for the 
error sensitivity to the particular network weights through first-order 
derivative information. 
Pruning by Inverse Kurtosis. PNN can be pruned using only first-
order information in the errors to judge the significance of the weights 
without taking into account their magnitudes. This can be done by com-
paring of the distributions of the gradients. When the distribution of a 
gradient with respect to a weight is broader, this means that its optimiza-
tion depends on more data. Rrespectively, if the gradient distribution is 
peaked, this means that only a small number of data aff"ect this weight. 
Therefore, a weight with a peaked gradient distribution is not likely to 
have a large impact on the network. Weights can be pruned after their 
ranking based on the differences between their gradient distributions in 
the following way [Neuneier and Zimmermann, 1998]: 
^ 
(6.75) 
e -f \diffwi 
where e = 0.001, and diff^.^ is a kurtosis measure of the deviation from 
the normal distribution [Neuneier and Zimmermann, 1998]: 
..â.,,_|JlMlSafelir _3| 
(6,76) 
(l/iV)El,(*-S)f 
where a small value indicates to prune this weight. 
6.5.2 
Second-Order Network Pruning 
Second-order information in the errors can also be exploited to prune 
networks with the aim of improving their extrapolation. Taylor's ap-
proximation of the error can be simphfied further in addition to the 
ehmination of its third-order and higher-order terms. When this approx-
imation is taken to analyze a network that has already been trained, it 
can be expected that the weight vector has already approached some 
minima on the error surface. This idea can be applied to prune evolved 

178 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
PNN whose weights have been estimated by OLS fitting. Since at the 
minimal error point the gradient is zero (that is, V^(wo) = 0), the 
second term in Taylor's expansion (6.35) vanishes so that there remains 
only EiwQ -\- Aw) ~ E{wo) + 0.5Aw'^HAw. This equation shows that 
a possible error change that can occur when a learning step from wo to 
WQ -f- Aw is made, can be described with the quadratic form: 
E{wo + Aw) - E{wo) = -Aw'^HAw 
(6.77) 
where WQ is the weight vector and H is the Hessian matrix. 
Equation (6.77) shows that the error change depends on the magni-
tudes of the weights as well as on their interrelationships described by 
the second-order derivatives from the Hessian. Weights with very small 
magnitudes and small Hessian entries only slightly affect the error. The 
greater the weights and their contribution to the Hessian, the larger their 
influence on the error. Therefore, the quadratic form 0.5Aw'^HAw pro-
vides the basis for deriving formulae for the significance of the weights 
that can be used for network pruning. 
Optimal Brain Damage. The minimization of the quadratic form 
Aw^HAw can be approached from two perspectives: 1) using the di-
agonal approximation of the Hessian; and 2) using the full Hessian. The 
former approach to network pruning using the diagonal approximation of 
the Hessian is called optimal brain damage (OBD) [LeCun et al., 1990]. 
OBD removes weights having small saliency computed using second-
order error derivatives according to the following formula: 
where [lici]ij is the Hessian entry on the diagonal where i â j . The 
diagonal approximation of the Hessian is: 
(6.79) 
which can be estimated using the gradients obtained by backpropaga-
tion. The simplified format of this saliency is: 
1 (dE 
dE\2 
2 I dwi dwj 
^v = z{ââX 
(6-80) 
where dE/dw^ and dE/dwj 
are the error derivatives. 

BackpropagaUon Techniques 
179 
The OBD pruning approach is very efRcient as its computational cost 
is Hnear in the number of weights in the network. OBD has the ad-
vantage that it accounts for the interdependencies between the network 
weights, while first-order pruning techniques (Section 6.5.1) and weight 
subset selection take into account only the weight magnitudes when cal-
culating the saliences. OBD goes beyond weight size-based importance 
and enables us to judge more precisely how all the weights in the whole 
network cooperate for data modeUing. 
Optimal Brain Surgeon. Optimal brain surgeon (OBS) [Hassibi 
and Stork, 1993] is another approach to ranking the weights that uses 
the complete Hessian. OBS more precisely estimates the relationships 
between the weights using the complete Hessian (6.39,6.40). The weight 
saliencies according to OBS are determined with the following fraction: 
where [H"~^]^j is the inverse full Hessian entry in row i and column j . 
The OBS approach is more time-consuming than OBD due to the ne-
cessity to evaluate the full Hessian, and then to invert it. In practice, the 
complete Hessian may be extremely large and/or ill-conditioned which 
means that it may be difficult or even impossible to invert. Therefore the 
OBS strategy may not always be applicable due to numerical inaccura-
cies. From another point of view, OBS has been found superior to OBD 
on many research problems, and it could be expected to perform favor-
ably to OBD, first-order pruning techniques (Section 6.5.1), and weight 
subset selection. OBS has the same advantage as OBD in that it is a 
second-order method that can better capture the weight dependencies 
in polynomial networks. 
6.6 
Chapter Summary 
This section strengthens the importance of the backpropagation algo-
rithm as a computational mechanism for learning the weights of nonfin-
ear network models. It demonstrated that the BP is efficient for retrain-
ing of nonlinear PNN after discovery of their relevant topology by IGF. 
IGF should be considered as a first step for polynomial identification 
from data because it helps to automatically resolve many problems that 
hinder the success of BF applied to conventional neural networks, such 
as: what should the initial weight values be, how many hidden layers 
should be used, how many should the hidden nodes be, and what should 
be the network connectivity. What remains is to improve the weights 
as IGF discovers the proper network structure by manipulating fiexible, 
plastic tree-fike network architectures. 

180 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
The idea of using the backpropagation weight learning algorithm for 
further improvement of evolved PNN is general and it is applicable 
to other higher-order networks. The experiments with BP on nonlin-
ear PNN demonstrate that polynomial networks, once constructed in a 
bottom-up manner, are suboptimal solutions which assume considerable 
error corrections because the BP training begins with weights deter-
mined by least squares fitting during IGP. The weights of the evolved 
polynomials position them on slopes of the error surfaces toward some 
optima, however close to some optima that has not been reached. In 
order to continue the descent on the slopes down toward the basin min-
ima, very small learning rates have to be used to avoid overshooting 
the minima. Since the weights have been learned to some extent, the 
algorithm convergence is rapid in the sense that it takes fewer epochs. 
Approaches were presented for learning step control and pruning that 
are especially suitable for PNN. There is a large amount of research ma-
terial on developing learning rate control algorithms and neural network 
pruning strategies that can also be apphed to PNN after proper modifi-
cations to refiect the specificities of the BP techniques studied here for 
high-order polynomial networks. 
Concerning the ease of implementation, it is obvious that first-order 
BP techniques are straightforward to implement, but they are not so 
efficient as the second-order training methods. Amongst the second-
order methods, the conjugate gradients method seems very attractive 
because it is fast and can be developed without difficulties. After that, 
the Levenberg-Marquardt method can be applied using the computed 
entries of the Jacobian matrix. 

Chapter 7 
TEMPORAL BACKPROPAGATION 
There are many practical inductive tasks with data whose properties 
change over time. In order to track the inherent characteristics of such 
dynamically changing data, there are necessary representations that de-
scribe spatiotemporal information. 
These should be special dynamic 
representations which react to the provided inputs after a certain de-
lay time; that is, they may not respond immediately to the inputs like 
traditional static models. The dynamic models include the time as a 
parameter that influences the mapping to reflect the time ordering, and 
in this way the dynamic models capture time dependencies. 
Research into dynamic network models involves development of spe-
cialized temporal training algorithms that utilize sequentially arriving 
data. Such temporal training algorithms take into account the time re-
lationships among the data. When performing parameter adjustment 
with dependence on the time ordering, the dynamic models may exhibit 
noninstantaneous response to the present inputs, which leads to non-zero 
transient response. Therefore, one serious problem that arises in tem-
poral training is to stabilize the training procedure in order to achieve 
sufl[iciently accurate results. Other difl[iculties in temporal learning arise 
due to the increased complexity of the dynamic model structure because 
not only external inputs, but also feedback delays from the output and 
the hidden nodes, may have to be handled. 
Two main kinds of polynomial networks for temporal data processing 
can be distinguished: time-delay (or time window) PNN (Section 3.4.1-
3.5.4) using lagged variables, and recurrent PNN (Section 3.5.5) using 
links passing back node outputs as variables. Time-delay PNN enable 
the application of static BP techniques for weight learning, assuming 
both temporal and spacial dimensions. Such approaches, however, have 

182 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
several drawbacks: 1) they impose a strong limit on the duration of the 
temporal events as they process data through fixed time windows, more-
over, in order to fix the window size they require predetermination of 
the embedding dimension; and 2) they face difficulties recognizing which 
are the absolute and which are the relative positions in the sequences, 
which makes it difficult to capture long-term time-varying relationships 
in the data. The first drawback can be avoided by the employment of 
powerful search techniques like IGP that may automatically discover the 
relevant time lags for the adopted model. The second drawback can be 
avoided by development of recurrent PNN. 
The recurrent PNN with feedback connections (Section 3.8.3) are 
alternatives to the time-delay networks for temporal sequence learn-
ing. Recurrent PNN are made as discrete time models trainable by 
specialized backpropagation algorithms for weight adaptation derived 
from their specification as state-space models. Assuming that IGP has 
evolved the optimal recurrent PNN topology for the task, its weights 
can be improved using the exact gradient computation algorithms Back-
Propagation Through Time (BPTT) [Rumelhart et al., 1986, Werbos, 
1974] and Real-Time Recurrent Learning (RTRL) [Williams and Zipser, 
1989, WiUiams and Zipser, 1995]. They conduct gradient descent search 
in the weight space, subject to the assumption that all connections are 
treated with the same delay of one time step At = 1. BPTT performs 
gradient calculations backward in time, while RTRL computes the gra-
dient forward in time. Especially for discrete higher-order networks with 
polynomial activation functions, there are implemented the incremental 
RTRL, and two versions: real-time BPTT and epochwise BPTT. After 
that, a Recursive BackPropagation (RBP) [Shah et al., 1992, Puskorius 
and Felkamp, 1994] algorithm, based on minimum mean square weight 
estimation, is elaborated for faster training of recurrent PNN. 
7.1 
Recurrent PNN as State-Space Models 
The recurrent PNN may be envisioned as state-space models of process 
behavior [Wilhams and Zipser, 1989, Chen et al., 1990, Narendra and 
Parthasarathy, 1990]. They formalize the input-output mapping, sug-
gested by the given training data, by taking into consideration their 
dynamical properties. This is because not only external input variables, 
but also temporal variables representing internal states, enter the net-
work. The states capture past node information and send it through the 
loops, thus providing memory that relieves the necessity to select time 
lags. The states generalize the information about the internal excita-
tions to the inputs and supply additional activity information for better 
tracking of behavioral patterns. 

Temporal Backpropagation 
183 
The operation of a PNN node from a dynamic perspective can be 
described with the following coupled difference equations: 
s{t + l)=p[x{t),s{t)] 
(7.1) 
z{t)=nsit)] 
(7.2) 
where x{t) is the external input, s{t) is the internal state, and z{t) is the 
node output at time t. The first process equation (7.1) specifies that the 
next state 5(f. + 1) is a function of the input x{t) and the present state 
s{t). The second measurement equation (7.2) specifies that the output 
is produced as a function of the state. 
The recurrent PNN use activation polynomial functions in the hidden 
and output network nodes. They can be expressed as dependent, on 
a discrete time parameter influencing the variables, according to the 
process equation (7.1) as follows: 
5 
S{t+l)=p{t)= 
Y^WmUmit) 
(7.3) 
where p(t) is a partial activation polynomial (Table 2.1) derived from the 
complete one: p{t) = wo-\- wiui(t) -f W2U2{t) + w-^ui{t)u2{t) + w^u1{t) + 
W4U2{t). The temporal variables u{t) are introduced for notational con-
venience to describe one of the following alternatives: 
r x{t) 
u{t) - <^ Pa(t) 
(7.4) 
[ 
sit)=p{t~l) 
that is, they could be either external inputs x{t) or activation polynomial 
outputs Pa{t) passed from a child node from the preceding network layer, 
or hidden node states s{t) at time tick t which are recorded previous 
outputs of the corresponding node. 
The problem of learning recurrent polynomial networks from data 
can be stated as follows: given a trajectory of inputs x(l),x(2),..., a:(/;) 
sampled at discrete times, their corresponding targets 7/(1) = x{m -h 
l),7/(2) = x{m + 2),...,7/(t â m) = x{t), and an initial state vector 
5(1) = 0, s(2) = 0,..., s{t â m) â 0, find the structure and parameters of 
the PNN that best models the underlying mapping. Here, m denotes the 
input history; that is, the network output depends on m earlier inputs 
from the given trajectory. The weight tuning algorithm includes three 
steps repeated a large number of times: 1) make the next system states 
5(/; + 1) from the previous inputs x(i^ and states 5(;t) with the activation 
polynomials in the hidden nodes; 2) compute the network output; and 
3) update the network weights. 

184 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
The trajectory learning algorithms like BPTT and RTRL adapt the 
network output gradually to follow the provided data sequence. They 
implement different weight training rules obtained by differentiating the 
process and management dynamic equations (7.1,7.2) in two opposite 
directions, backward and forward in time. 
There are several features of these algorithms for the case of PNN that 
deserve clarification: 1) it is assumed that the activation polynomials 
serve both as net-input functions and as transfer functions /; that is, the 
measurement equation (7.2) uses the identity function; 2) the activation 
polynomials p in the process equation (7.1) are higher-order functions 
of their variables; and, 3) the polynomial networks have a binary tree-
like structure, which implies that every hidden node feeds exactly one 
parent, and every node has exactly two incoming connections. 
7.2 
Backpropagation Through Time 
BPTT [Rumelhart et al, 1986, Werbos, 1974] as a notion, summarizes 
two (real-time and epochwise) training strategies for dynamic neural net-
works that collect the instantaneous error gradients from all temporal 
patterns backward in time. The BPTT strategies can be understood 
in context of the BP framework assuming that the network is unfolded, 
unrolled in the time. Unfolding is a process of transforming the tem-
poral dependencies into spatial relationships. Copies of the temporally 
dependent nodes leading to a larger network are created, which may 
be considered static and thus trainable by static backpropagation. The 
states and the recurrent links through which they flow are duplicated at 
each time instant from the end, down to the beginning of the time in-
terval. The weights on replicated connections between successive layers 
in the unraveled network are the same (Figure 7.1). 
BPTT are trajectory learning algorithms that modify the weights in 
the opposite direction to the error gradient. Both BPTT algorithms 
create virtual networks by unfolding the original topologies on which 
backpropagation training is applied. The objective is to improve the 
weights so as to minimize the total error over the whole time interval. 
These algorithms collect the instantaneous errors on all data and update 
the weights after a complete presentation of the given sequence. 
The instantaneous gradients are produced in a different manner: 1) 
the real-time BPTT estimates the current error, and uses it to obtain and 
accumulate the earher derivatives from the current down to the initial 
moment; and 2) the epochwise BPTT adds the previous derivatives, 
as well as the network errors, taken in reverse order within the epoch 
boundaries and restarts the next epoch from another moment. 

Tem,poral 
Backpropagation 
185 
P(x) 
â¢ â¢ X S 3 ( 0 ) 
Layer k 
Layer J 
Layer I 
x^(t) 
x^(t) 
Activation Polynomial 
(Function Node) 
External Input variable 
x,(t) 
( Terminal Leaf) 
/ \ 
Internal State variable 
s.(t) 
( Terminal Leaf) 
Figure 7.1. Unrolling in time of a tree-like recurrent PNN with two time layers. 
7.2.1 
Derivation of BPTT Algorithms 
Let the given trajectory of observations encompass the period from to 
to ti inclusive, that is the interval (^o^^i]- When the PNN is unrolled in 
time, the total error can be collected by summation of the instantaneous 
errors E = E{T) on the subtrajectory from the initial ^Q + 1 ^P to the 
final moment t\, using the equation: 
E{t,M)= E E{T)^ Y. \{y{'^)-nr)? 
(7.5) 
where r indicates the time ticks from the given interval. 
Both versions of BPTT share a common temporal delta training rule 
for weight updating that requires computation of the total error deriva-
tive over the whole temporal interval from to to ^^ inclusive: 
dE 
^^ = ^ = -^ E 
dw 
^ ,, 
where r] is the learning rate parameter. 
dE 
dw{r) 
(7.6) 

186 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
The following analysis shows how to make the derivatives 
dE/dw{T), 
which are then specialized for making real-time or epochwise BPTT. 
According to the process equation (7.1), explaining the operation of 
a node in recurrent networks, the error derivative dE/dw{T) is sensitive 
to the weight through the internal state of the node: 
dE 
_ 
dE 
Osir+l)_ 
OE 
^^^^^ 
dw{r) 
ds{r + l) 
dw{T) 
ds{T-\-iy 
where the X'{T) is the influence of the weight changes on the future state 
X'{T) = ds{T -f- l)/dw{T)^ which can be obtained using the node output 
at the previous time instant x'{r) = 
dp{r)/dw{T). 
The first multiplier dE/ds{r + 1) accounts for the error sensitivity to 
the state variations over time. This sensitivity can be computed by or-
dering the partial derivatives in time [Werbos, 1974]: J5'(l),Â£^(2), ...^E{t), 
This ordering helps to infer the formula for the effect of the state on the 
error. The formula includes the explicit state influence at moment r 
as well as the implicit state influences on the error arising in the fu-
ture moments r-f-1. Let the instantaneous error derivative with respect 
to the current state at moment r be substituted by the backprop er-
ror: 6{r) = dE/ds{r). 
Assuming that the unfolded network produces 
temporally arranged errors leads to the following backprop error: 
S(r}- 
dE 
^ 
dE 
^ 
dE 
dSrJT + l) 
^ ' 
ds{T) 
ds*{T)^dSr{T 
+ l) 
ds{T) 
^ 
' 
where dE/ds*(T) is the explicit error sensitivity to the state at time tick 
T, and Sr{T + 1) are the next implicit states sent via the recurrent links 
indexed by r. Equation (7.8) can be written more concisely as follows: 
8{T) = -e(T) + ^ <5(r + l)w, 
(7.9) 
r 
where e(r) is the output error e(r) = y{r) â P{T) at time r, and Wr are 
the weights on the recurrent connections that enter the node. 
The boundary condition for this formula (7.9) should be approached 
with care. At the end of the given time interval when r = ^i, the back-
prop error reduces to the impact of the network output error: 6{ti) = 
âe(^i), because there is no future evidence: ^(^i -f 1) = 0. At ah ear-
her time instants for the period r < ^i, the backprop errors can be 
propagated and so they become available. 
The time offset in formula (7.9) means that the error gradient is ac-
cumulated backward in time by traversing the trajectory from the final 
time step ti down to the beginning of the interval to -\- 1. The earlier 

Temporal Backpropagation 
187 
backprop errors are computed at each earlier time instant by successive 
application of the error formula (7.9). The instantaneous error deriva-
tives are defined with respect to weights on connections to the output 
and respectively on links to the hidden nodes. They are elaborated here, 
especially for tree-like recurrent PNN leading to two temporal BPTT 
training rules. Since these rules can be apphed to the time-unraveled 
network in the same way as the static BP, the algorithms are called 
backpropagation through time. 
Temporal Delta Rule for Output Node Weights. The total error deriv-
ative with respect to a weight w^j on connection from the hidden to 
the root network node suggests to update it with the following temporal 
BPTT delta rule: 
BE 
^^ 
^'"kj = -ri^^,=rj 
Y. h{T)A^{T-l) 
(7.10) 
where the backpropagated error at the particular time step is (5/^(r) = 
[âe/.(r) -f (5/C(T + VjWr] according to formula (7.9), and the state deriva-
tive with respect to the weight is X'J^AT â 1) = ^P/c(7" â l)/dwj^^j{T â 1). 
The above equation includes the particular error at time instant r as 
^k{^) â Vki^) ~ P{^)) where the network output is actually the output 
produced by the activation polynomial at the root P(r) = Vk{^)-
Temporal Delta Rule for Hidden Node Weights, The temporal BPTT 
training rule for the hidden node weights is obtained in a similar way, 
leading to the following more complicated expression: 
^^^â¢' = "'^1^:='' Â£ <5,(r)4(T-l) 
(7.11) 
where 6J{T) is the backprop error at node j , and X'-^(T â 1) = dpj{T â 
l)/dwji{r â 1). The backprop error SJ{T) is more involved, it is not only 
the output error at the particular time step, because its explicit error 
sensitivity component reflects the position of the node in the network 
architecture. The expansion of the backprop error which accounts for 
the network architecture as well as for the time variations is: 
Sj(T) = \-S,{T)PI 
+ SjiT + l)Wr] 
(7.12) 
where the first term âSk{T)p'f^- is the backpropagated error from higher 
layer nodes, and the second term is the time dependency on the state. 
The first term is the contribution of the derivatives: OE/ds^j^r) = 
~^k{^)p'kj^ which is returned from the /c-th output node to the concrete 
j-th node through the intermediate nodes as in static BP. 

188 
ADAPTIVE 
LEARNING 
OF POLYNOMIAL 
NETWORKS 
The B P T T algorithm appHed to a tree-hke recurrent PNN is illus-
trated using the network in Figure 7.1. This recurrent polynomial net-
work is taken similar to the feed-forward PNN in Figure 6.1 in order to 
facilitate understanding the differences between the static BPA and the 
temporal dynamic versions of the BPA presented in this chapter. The 
hidden nodes in Figure 7.1 use the same activation polynomials: 
P3(0 
= ^30 + '^3lP5(0 + ^32^5(^)53(0 
(7.13) 
P^{t) 
= 1^50 + 1^5153(0 + ^52^4(0 
(7.14) 
P4{t) 
= W40 + W4iXi{t) -h W42Xi{t)x2{t) + W4r^xj{t) 
(7.15) 
The instantaneous error derivatives at time r with respect to the 
weights w^i and w^2 on connections toward the output node are derived 
using the temporal delta rule (7.10) as follows: 
dE 
dE 
dw^2{r) 
= 
-e{T)p,{T) 
(7.16) 
- 
[-e(r)-f-K;32^3(T + l)]p5(r)p3(T--l) 
(7.17) 
9t^5i(r) 
where 63(r) = y{r) â P^{T), 
and the backprop error is computed recur-
sively 6s{r) = [âe3(r) + w^2^r^{^ + 1)] at all previous times ti > r > t^. 
The instantaneous derivatives for the left child node 5 at layer j of 
the root with respect to its weights w^i and i6'52 at time r are: 
dE 
= [-63(7-) {w3i + W32P3{r - 1)) + W^I6^{T 
+ 1)] p3(r - 1) 
(7.18) 
3F 
-â ... -63(7) {w^i +w^2P3{r - l))pl{r) 
(7.19) 
C'^^52(Tj 
where 6^{T) â [~e3(r) [w-^^ -f W2,2P?>{T - 1)) + W^I6^{T 
+ 1)]. 
The instantaneous error derivatives for the right child node (node 4 at 
layer i in Figure 7.1) of node 5 with respect to its weights w^i^ W42 and 
W4^ at time r are produced in a straightforward manner, as the output 
of node 5 depends only on the input variables xi and X2: 
^^ 
^^ 
[2w,2{r)p4{r)]x^{r) 
(7.20) 
dw4i{r) 
dpr,{T) 
dE 
dE 
[2wr,2{r)p4{r)]x,{T)x2{r) 
(7.21) 
dw42{r) 
dpr,{T) 
[2^52(r)p4(r)]x'f(r) 
(7.22) 
dE 
dE 
,^ 
, , 
, ,, 2/ 
51^43 (r) 
dp^{T) 
where dE/dp^{T) 
â â e(r) [W'^\{T) -\- W^2{^)P3{T 
~ 1)] 

Tern,poral 
BackpropagaUon 
189 
7.2.2 
Real-Time BPTT Algorithm 
The real-time BPTT algorithm involves a forward and a backward 
pass with each training temporal example. The forward pass sends the 
example through the network and calculates the node outputs, which 
become system states after shifting the state buffer one step back. The 
backward pass computes the output error at the current time tick. After 
that, it performs tree traversal from the root to the leaves and calculates: 
1) the backprop errors in each hidden node according to the static delta 
rules for the current moment t; and 2) the backprop errors with respect 
to the states for all earlier time steps by unfolding the encountered hid-
den node if it has a recurrent link that feeds some state variable. The 
corresponding weight deltas are finally evaluated. The real-time BPTT 
algorithm is illustrated in Table 7.1. 
Table 7.1. Real-time temporal backpropagation for tree-like recurrent PNN. 
Real-Time B P T T Algorithm for P N N 
step 
Algorithmic 
sequence 
1. Initialization 
2. Perform 
network 
training 
Input trajectory x(l),x(2), ...,a;(^i), 
backprop errors 6j(l) = 6j{2) = ... = Sj{t]) = 0 
and states Pj(l) = ... â Pj{t\) = 0 for each hidden node j . 
For each consequtive step t, t\ > r > io + 1, do 
a) Forward pass: propagate the inputs a:(l), a:(2),..., x(?7?,) 
and the states pj{t â 1) to compute node outputs, 
save them to become next states. 
b) Backward pass: from the root down to the fringe nodes 
i) compute the backprop errors using the output 
8k.{t) = ~{yk{t) â Pk.{^)) if this is the output node 
6j{t) ~ â6k{t)p'}^j if this is a hidden node. 
ii) if this node has a recurrent connection then 
unfold it, compute and store the backprop errors 
for the earlier time steps by iterating backward 
in time with 
T,t~l>r>to-\-l 
6k{T) = 6/p,(r + l)wr at the output node 
SJ(T) ~ 6J{T -\- l)wr at the hidden noded 
otherwise (when there is no recurrent link) 
c) Update the weights by accumulating the gradients 
^^kj 
=^Et-:/,n,i ^k{T)x',,j{T - 1), 
where X',^J{T - 1) = dpkir - l)/dwkj{r 
- 1) 
where X'J^{T â 1) â dpj(T â l)/dwji{r 
â 1). 

190 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
7.2.3 
Epochwise BPTT Algorithm 
The epochwise BPTT performs the same forward pass as the real-
time version, but it makes different calculations during the backward 
pass. The reason is that the errors from the earlier moments are ex-
plicitly calculated and considered when computing the deltas so as to 
impact the weight changes. The main distinguishing characteristic of 
the epochwise BPTT is that the errors from the previous time steps 
are injected through e/c(r), f/ > r > ^o + 1, in the derivatives 6kX^) = 
[âek{r)-\-6k{T-{-l)wr] 
at the output root network node and through 
^j{^) = -^k{T)p'^- in 8J{T) = \-6k{T)p'^- H-(^j(r-f l)wj\ at the hidden 
nodes. This means that the epochwise algorithm minimizes the total 
error from all previous instantaneous network errors (Table 7.2). 
Tabic 7.2. Epochwise temporal backpropagation for tree-like recurrent PNN. 
Epochwise B P T T Algorithm for P N N 
step 
Algorithmic 
sequence 
1. Initialization 
2. Perform 
network 
training 
Input trajectory a:(l), a:(2),..., x(ti), 
backprop errors 6j{l) â (^i(2) = ... = Sj(t-[) = 0 
and states Pj{l) = ... = Pjit^) = 0 for each hidden node j . 
Repeat for each epoch in the time period (^o,^'], t' < t] 
a) For each time instant T, t' > T > to -\- I, do 
i) Forward pass: propagate the inputs a:(l),..., a:(?77,) 
and the states PJ{T â 1) to compute node outputs 
save them to become next states. 
ii) Backward pass: from the root down to the fringes 
- compute the output backprop error at the root 
6,{T) = -e,{T) 
== -{y,{T) 
~ 
P,{T)), 
and ej(r) = 
-6k{r)p',,j. 
- if this node has a recurrent connection then 
unfold it, compute and store the backprop 
errors for the earlier time steps by iterating 
backward in time from t' down to ^o + 1 
(5fc(r) = [âefc(r) 4- Sk{T -h l)wr] at the output 
8J{T) = [âej{T) -f- 6J{T + l)wr] at the hidden 
otherwise (when there is no recurrent link) 
<^jW = e j ( r ) = -<5fc(T)p;,^. 
b) Update the weights by adding the epoch gradient 
where X'J^_J{T â 1) = dpk{T - l)/dwkj(T 
- 1) 
where x'ji{T - 1) = dpj(T â l)/dwji{r 
â 1). 
c) Reinitialize the network. 

Tem,poral BackpropagaMon 
191 
7.3 
Real-Time Recurrent Learning 
Another temporal supervised learning algorithm for training dynamic 
neural networks that can be applied to PNN is real-time recurrent learn-
ing (RTRL) [Wilhams and Zipser, 1989, Wilhams and Zipser, 1995]. The 
name comes from its ability to perform weight tuning forward in time 
while the network is running. It computes weight changes at each time 
step along the given trajectory. The RTRL algorithm resembles BPTT 
in that it uses the same cost function and attempts to follow the true 
negative gradient of this error function. However, it relies on a different 
derivation of the temporal learning rules. While BPTT computes the 
gradient assuming decomposition of the weight in time, the RTRL com-
putes the gradient assuming that the error function is decomposed in 
time. RTRL estimates the instantaneous error derivatives with respect 
to the weights at each time tick from the given interval while the network 
is continuously running forward on successive data. 
The RTRL performance criterion is minimization of the total temporal 
error over the whole given interval, defined as follows: 
Eito,h)= 
Y: m= 
i : 
liyit)-pit)f 
(7.23) 
where E{t) = 0.5(y(^) â P{t)) is the instantaneous (local error). 
The derivative of the total error with respect to any weight is deter-
mined assuming that the cost function is sensitive to the time, rather 
than assuming that the weight is sensitive to the time. Because of this, 
the initial states, the external inputs and the network weights, are kept 
fixed during the computations over the entire trajectory. The error deriv-
ative is found by a forward expansion of the instantaneous cost in time 
performed according to the chain rule: 
^- .ir^i 9w 
,j^^^ dp{t) dw 
^j^^^ ^^^ dw 
(7.24) 
which holds for any arbitrary weight on connection that enters the node. 
This equation suggests that the gradient of the total error can be com-
puted by accumulating the derivatives of the instantaneous errors at 
each time tick t from the given interval. 
The first multiplier in this equation for the error derivative (7.24) is 
the backprop error: 6{t) = dE{t)/dp{t). 
The second multipher dp{t)/dw 
is the so called dynamic derivative of the output with respect to the 
weight at time t. This dynamic derivative describes the impact of the 
weight change on the activation polynomial output and it can be ex-

192 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
panded using partials with respect to the weight as follows: 
dp{t) 
^ 
d 
dw 
^ 
dw 
^ 
r 
dum{t) 
â¢ 
7 7 7 - = 1 â¢-
(7.25) 
where u{t) is the temporal variable participating in the term wu{t). 
Equation (7.25) can be simplified since the temporal derivatives of the 
external variables are zero, that dum{t)/dw 
= 0, so it vanishes when 
directly feeding external inputs Umit) = Xm,{t)' 
There remains to find the partial temporal derivatives of recurrently 
passed output signals from other nodes. The diff'erentiation of the poly-
nomial output of the node with respect to a weight yields: 
only for recurrent connections Wr, r < m = b that feed past outputs 
Pr. This equation is produced by diff'erentiation of the weighted incom-
ing signals to this node WmUm{^) with respect to the trained w. The 
summation over all recurrent finks r accounts for the effect from other 
node outputs pr sent via these recurrent connections weighted by Wr^ 
and for the effect from the signal u{t) on the trained link weighted by w. 
The signal u{t) could be an external input, or a child node output, or a 
feedback signal from a higher layer node. The partial derivative of the 
instantaneous error dE{t) at time t with respect to a weight w becomes: 
^ - ^ c ) 5^..*vl_i),â(â 
(7.27) 
where r enumerates the recurrent connections that enter the node. 
Expression (7.27) shows that the error gradient can be handled for-
ward in time along with the continuous evolution of the network dynam-
ics according to the process (7.1) and measurement (7.2) equations. 
Several cases arise from this formula in context of tree-structured 
recurrent PNN with irregular topologies. They can be divided in two 
main groups: error derivatives for the root node and error derivatives 
for the hidden network nodes. These two main groups can be further 
subdivided into subgroups due to the possibility for having combinations 
of forward and backward links toward a node. There are four possible 
combinations of network connections: 1) feeding two external inputs; 2) 
feeding two lower layer node outputs; 3) feeding an external input and a 

Tem,poral BackpropagaMon 
193 
node output from a lower layer network node; and 4) feeding a recurrent 
signal coming from a higher layer node and a nonrecurrent signal, which 
can be either an input or a lower node output. For this reason, several 
RTRL training rules are developed below. 
Temporal Delta Rules for Output Node Weights. 
Two cases arise at 
the root network node depending on its incoming connections. When the 
root network node is entered by signals along non-recurrent connections 
sending lower layer node outputs or external input variables directly, the 
temporal delta rule remains the same as the one used for static gradient 
descent training. Assuming that the root node is indexed by k and its 
links come from the ^'-th node or a variable below, the temporal 
delta 
rule for the output node is: 
^^kjit) 
= - V ^ 
= vSk{t)x',ji^^) 
(7.28) 
where 77 is the learning rate, 6j^^{t) is the error Sk{t) = Vki^) â P{t), 
P(^t) = Pki't^)^ and x'l^At) is the dynamic derivative of the node activation 
polynomial with respect to the weight x'^^At) = 
dpk{t)/dwi^j. 
Another different training rule is obtained when there is a connection 
feeding back the output of the root node to itself. This temporal training 
rule is common for the root node weights because the output error 
dE{t) 
with respect to any weight Wkj from the j - t h hidden node to the /c-th 
root node in this moment, depends on the recurrent signal from the past 
step that enters the node. The recurrency requires elaboration of the 
derivative dE{t)/dwkj 
according to equation (7.27), which leads to the 
following RTRL 
temporal delta rule for the output node-. 
Awkj{t) 
= r]6k{t) sr^ 
dpk{t - 1) , 
, . 
(7.29) 
where wj^^j. are the weights on the self-loops, p^ is the root polynomial, 
and Uj is the signal on the trainable link weighted by wj^j. There can 
be several recurrent links r < m = 5 since there may be up to four 
activation polynomial terms. This formula (7.29) is applied either with 
signal Uj{t) = pk{t~l) 
to train the self-recurrent connection weighted by 
Wkr^ or with previous node output Uj{t) â pj{t â 1), or with an external 
variable Uj{t) â x{t) passed directly to the root node. 
Tem,poral Delta Rules for Hidden Node Weights. There are three cases 
of feeding hidden nodes. The simplest case is when the node receives 
lower node outputs and/or external variables because it remains the 
same as in static gradient descent. Let the trained node be indexed by j 
and its incoming link come from the i-th polynomial or variable. Then 

194 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
the temporal delta rule for the hidden node weights is; 
^^i'(*) = - ' ^ ^ = V^ji^^Kii) 
(7-30) 
where Sj{t) â â^k{~^)v'k/X^) ^^ ^^^ local backprop error, produced us-
ing the partial derivative of the output from the /c-th node with re-
spect to the output of the j-th node in some preceding level defined by 
P'kji^') ~ ^Pk{^)/^Pj{^)'> ^^d the current dynamic polynomial derivative 
with respect to the trained weight is x'-^{t) = 
dpj{t)/dwji. 
A hidden network node can receive an incoming signal through a 
recurrent connection from a higher layer node in the network or through 
a self-recurrent connection. These two cases are handled with a common 
weight training rule which involves diff"erent partial derivatives of the 
backward signal, depending on the link that provides it. 
The common RTRL temporal delta rule for training hidden node weights 
Wji in dynamic PNN is defined with the following equation: 
Aw,,{t) = rj6j{t) E
dpi{t - 1) 
, . 
(7.31) 
where Wjr is the weight on the looping connection, pi is the activation 
polynomial output fed back from the network node, and Ui is the signal 
on the link weighted by Wji. 
This training rule (7.31) for any kind of weight Wji on links to hid-
den network nodes is specialized into two ways corresponding to the two 
possible recurrent links associated with the weight Wjr- These special-
izations suggest producing two different dynamic derivatives from the 
recurrent signal pi{t â 1) with respect to the trained weight Wjf 1) by 
computing the dynamic derivative as dpj{t â l)/dwji 
when there is a 
self-recurrent fink feeding back the output of the same node activation 
polynomial pi = pj\ and 2) by computing the dynamic derivative as 
dpi{t â l)/dwji when there is a recurrent connection from some higher 
network node providing signal pi through the loop. 
The first case of a self-recurrency uses the previous node derivative 
which may be kept in a memory buffer associated with the node. The 
second case of a hidden node fed by a recurrent signal from a higher layer 
node leads to a more comphcated dynamic derivative dpi{t â l)/dwji. 
The reason is that there has to be taken into account the fact that the 
activation polynomial output passed from the higher node depends on 
the trained weight through all activation polynomial outcomes from the 
intermediate nodes between this higher node and the current node. The 
derivative in this situation is formed using the chain rule and requires to 

Tem,porol Backpropagation 
195 
traverse the path from the higher layer node to the current one. During 
the topdown tree traversal all dynamic derivatives, at the particular time 
instant t â 1, of the encountered node activation polynomials have to be 
collected in the following way: 
dpi{t~i) 
_ dpi{t -1) dpj{t 
-1) 
dwji 
dpj(t â 1) 
dw 
(7.32) 
ji 
which holds when the l-ih node is the immediate node above the current 
j-th node. The cases of higher nodes are obtained analogously. 
Let's consider as an example the recurrent PNN given in Figure 7.2, 
which has the same architecture as the network in Figure 7.1. It is 
assumed that the hidden nodes in the PNN given in Figure 7.2 compute 
the same activation polynomials: 
R3(0 
= '^30 -+- ^3lP5(0 + '^32P5(0'53(0 
C^-^S) 
P5(0 
= ^50 + ^5153(0+1^52^4(0 
(7-34) 
P4{t) = 
W40-\-W4iXi{t) -\-W42Xi{t)x2{t) -{-W4^xl{t) 
(7,35) 
The temporal error derivatives with respect to the output node weights 
wr^i and iL'32 are computed according to equation (7.27) as follows: 
dE{t) 
dEjt) 
dw^2 
= 
-iy{t)-P{t)) 
= 
-(y{t)-P{t)) 
W32-
W32 
dp^jt - 1) 
dP3{t - 1) 
dw^2 
-P5{t) 
+ R 5 ( 0 P 3 ( ^ - 1 ) 
(7.36) 
(7.37) 
where the output is P{t) = P^it), the dynamic node derivatives with 
respect to the weights on the incoming connections are dp^{t~l)/dw^i 
â 
P5(/; â 1), and respectively dps{t â l)/dw^2 = P5(^ â 1)P3(^ ~ 2). 
The error derivatives with respect to the weights w^i and w^2 entering 
the hidden left child node 5 at layer j , in Figure 7.2, at moment t are: 
dE{t) 
dEjt) 
dw^2 
where p^{t â 2) 
e(0K3i +^32R3(^- 1)] 
dp:^{t - 1) dp^{t - 1) 
^51-dp^{t - 1) 
a^5i 
e(0K3i +'i^32R3(^^- 1)] 
dp^{t ~ 1) dp,{t 
-1) 
^51-
+ P3(^-1) 
â¢Plit) 
(7.38) 
95 (^ â 1) dwr^2 
9^{t - l)/dwr^i, and pl{t - I) == dp^{t - l)/dwr^2' 
(7.39) 

196 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
P/0=W30+VW5,S3(0+W52P/(0 
s,(t)=p,(t-I) 
Pix) 
Psit)j 
^8At)=pAt-'l)\ 
â¢^ 
1 
I 
'PsiOs^it) 
pfi) 
P4it)=^^Q+W^^^(t)+W^^^{t)X2(t)+W^^^\t) 
Layer k 
Layer y 
Layer I 
Â® 
D 
0 
S^(t) 
Trainable Weight 
Activation Polynomial 
(Function Nod^ 
External Input variable 
( Terminal Lea]) 
Internal State variable 
( Terminal Lea]j 
x,it) [ 
X 
jXjit)xJitn 
t < t ) 
""42 
' ^ VÂ«) 
x^{t) 
Figure 7.2. A detailed view of a tree-structured recurrent PNN with explicitly 
shown expanded connections and their weights. 
The derivatives for the hidden right child node 4 at layer i of node 
5, in Figure 7.2, at time tick t are easy to determine since there are no 
loops that send recurrent signals back to this node. The instantaneous 
error derivatives with respect to weights W4i^ W42 and W/[^ are: 
dE{t) 
_ 
dE{t) dp^{t) dp4{t) _ 
dE(t) 
dw4i 
dp^{t) dp4{t) dw4i 
dp^it) 
dEjt) 
_ 
dE{f) dpr,{t) dp4{t) _ dE{t) 
dw42 
dp^it) dp4{t) dw42 
dp^{t) 
dE{t) 
_ 
dE{t) dpr,{t) dp4{t) _ dE{t) 
dw4'^ 
dp^[t) dp4{t) dw4^ 
dp^{t) 
[2w^2P4it)]xi{t) 
(7.40) 
[2w^2P4{t)]x^{t)x2{7Al) 
[2w,2PA{t)]xl{t) 
(7.42) 
where dE{t)/dp^{t) 
= {y{t) ~ P{t)) ^ i + ^32P3(^ - 1)]. 
The RTRL algorithm is suitable for training recurrent PNN in incre-
mental mode [Williams and Zipser, 1989, WiUiams and Zipser, 1995]. 
This incremental algorithm, shown in Table 7.3, modifies the weights at 
each step along the given data trajectory. The weight adjustments do not 

Temporal 
BackpropagaUon 
197 
follow precisely the exact gradient on the error surface, but rather follow 
its estimate at the particular time tick. This means that RTRL updates 
the weights by approximating the exact gradient of the error function 
similar to standard incremental backpropagation. The approximation to 
the exact gradient is close when the learning rate is sufficiently small so 
that the magnitude of the weight changes are considerably smaller than 
the time magnitude of the network operation. 
Table 7.3. Real-time recurrent learning algorithm for tree-like PNN. 
RTRL Algorithm for P N N (incremental version) 
step 
Algorithmic 
sequence 
where u{t) â¢â¢ 
1. Initialization 
External data x{t)^ states s{t), and derivatives dp{0)/dw = 0. 
2. Perform 
Repeat many times 
network 
a) For each time instant t, to < t < t} â 1 do 
weight 
i) Forward pass: propagate inputs x{t) and states s{t) 
training 
to compute the node outputs, i.e. the past states 
s{t + 1)- 
p(t) = X ] l - i u)mUm.(t), 
' x(0 
s{t)=p{t-l) 
â¢ 
ii) Backward pass: from the root down to the fringes 
- compute deltas for the output weights 
h(t) 
= y{t) - P{t) 
if there is no recurrent connection 
Awkj{t)==r]6k{t)x^t), 
where x'j^j{t) â 
dpk{t)/dwkj 
else when there is a self-recurrent connection w^r 
Awkj{t) = r]6k{t) [wkr {dpk{t - \)ldwkj) 
+ Uj{t)]. 
- compute deltas for hidden node weights 
6^{t) = -8^{t)p'^^^{t), where p'^^^{t) = 
dp,.{t)/dpj(t) 
if there is no recurrent connection 
Awji(t) 
= r]6j{t)x'ji{t), where x'ji{t) = 
dpj{t)/dwji 
else when there is a self-recurrent connection 
from the l-ih. node above 
Awji{t) = r]6j(t) [wjr {dpi{t - l)/dwji) 
+ Ui{t)] 
where \i j ^ I compute the chain derivative 
dpi{t - l)/dwji â 
[dp,{t - l)/dpj{t 
- 1)1 [dpj{t - 1)/^!.,,:]. 
iii) Update the network weights with the derivatives 
Wkj = Wkj + 
AWkj{t) 
Wji = Wji + 
Awji(t), 
Until termination condition is satisfied. 

198 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
7.4 
Improved Dynamic Training 
Recurrent PNN exhibit complex dynamics and their training is sus-
ceptible to entrapment in suboptimal weight configurations. 
Various 
strategies for improving the learning performance of recurrent networks 
as they run are available. They can help to decrease the computational 
complexity of the algorithms and to accelerate their convergence. Such 
specific techniques designed for extending the temporal backpropaga-
tion algorithms discussed here are teacher enforced training [Williams 
and Zipser, 1995], truncating in time [Wilhams and Zipser, 1995], and 
subgrouping [Williams and Zipser, 1995]. The truncating and subgroup-
ing strategies compute the approximate error gradient. 
7.4.1 
Teacher Enforced Training 
The speed of convergence of the dynamic PNN learning algorithms 
can be accelerated by teacher enforced training [WiUiams and Zipser, 
1989, Williams and Zipser, 1995]. This is a technique that aims at better 
adjustment of the network dynamics to the data trajectory. The idea is 
to replace the looping node outputs, sent back recursively to feed lower 
layer nodes at certain time steps by the given target signals. Thus, the 
network weight search becomes more precisely guided by the teaching 
signal even if the network is unable to match the data. The desired 
signal may contribute to faster error reduction during learning as this 
teaching signal directly influences the weight adjustments. 
Teacher forcing in recurrent PNN can be implemented by passing the 
desired signals to enter activation polynomial terms as follows: 
u{t) -= <^ x{t) 
t i D{t) 
(7.43) 
d{t) 
x{t) 
s(t)E 
= 
p(t--1) 
for some 
t i D{t) 
t f D(t) 
t G D(t) 
where d{t) is the target from the selected set D{t) at step t. 
The teacher forcing causes only slight modifications of the temporal 
training rules for weight updating. Teacher guidance elaborated for the 
real-time BPTT algorithm means that when the future errors from the 
unfolded network in time are calculated they become zero at the selected 
desired values, and hence the backprop error is: (5/c(r) = 0 and 6J{T)^ for 
t â 1 > T > ^0 + 1- Respectively, when the epochwise BPTT is enforced its 
backprop errors used for calculating the derivatives are simply reduced 
to the instantaneous errors at the current time step: 6f^^(T) = âek{T) 
and 8J{T) = âejij), for ^ - 1 > r > ^o + 1-
Using teacher navigation in the RTRL training algorithm requires 
zeroing the activation polynomial derivatives d'p{t â l)/dw when com-

Tem,poral BackpropagaUon 
199 
puting the next derivative dp{t)/dw^ since the derivative of the desired 
signal from the trajectory points with respect to any weight is zero. 
7.4.2 
Truncating in Time 
Convergence of recurrent polynomial network training can be speeded 
up with the assumption that only a small subset of recent past data 
affects the network output. This truncation applied to BPTT [WiUiams 
and Zipser, 1995] requires unfolding the network only a small number 
of layers in time. Such time bounding relaxes the need to store the 
entire temporal history of the network, and so it enables a decrease in 
the complexity of the algorithm. However, the truncation may help to 
achieve a close approximation to the true gradient only if the outputs 
are not temporarily dependant on the external inputs from the distant 
past. It is recommended to apply the truncation mainly for repairing 
the real-time version of the BPTT algorithm. 
Truncation in time can also be used to improve the RTRL algorithm 
[Catfolis, 1993] by computing the weight changes as dependent only on 
a small number of previous time ticks. The rationale for using short 
training trajectories is to make the temporal node output derivatives 
dp{t)/dw sensitive to several recent steps only. This can be made by re-
initializing the derivatives at a certain predefined number of steps. The 
size of the predefined time window is crucial for improving the learning 
performance; if it is closer to the optimal interval, then the convergence 
will be faster, otherwise the learning speed may be too slow and the 
algorithm may learn too little from the data. 
7.4.3 
Subgrouping 
The subgrouping technique [Williams and Zipser, 1995] has been de-
veloped for reducing of the complexity of the RTRL algorithm for re-
current networks. The idea is to train separately smaller subnets of the 
network instead of training the whole network. When the network is 
partitioned into subnetworks, the learning is restricted to the individual 
subnetworks with considerably more sparse connectivity. The difhculty 
of how to identify the subnetworks of the studied recurrent network can 
be avoided by dividing it into subnetworks of approximately equal size in 
the sense of number of weights. The isolation of the subnetworks should 
be made in such a way that each of them has at least one node with the 
given desired values so as to enable separate training. 

200 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
7.4.4 
Cominon Temporal Training Problem 
The most serious problem of temporal BP training of recurrent PNN 
arises from the so-called vanishing gradients [Bengio et al., 1994]. The 
magnitude of the temporal output derivatives tends to vanish to zero 
exponentially with the increase of the time steps. Because of this, the 
training process cannot converge to satisfactory solutions of the task, or 
converges extremely slowly. The recurrent network settles in configura-
tions of weights that capture only short-term relationships in the data. 
One approach to avoiding this problem is to assume that the limited 
memory is an obstacle for storing information from arbitrary periods of 
time, and to try to extend it for handling longer periods. This idea sug-
gests to consider larger PNN topologies with more hidden nodes, and 
respectively, more past hidden system states. Another approach is to 
perform second-order temporal training. 
7.5 
Second-Order Temporal BP 
A general strategy for efficient learning is to use the second-order 
information from the error derivatives. The second-order gradient op-
timization methods (Section 6.3) can also be applied for improving the 
convergence of temporal BP training of recurrent PNN. The implemen-
tation of such methods requires calculation of the Hessian matrix (6.39). 
Knowing how to find the entries of the temporal Hessian enables us to 
develop versions of Newton's method, pseudo-Newton method, conju-
gate gradients, and Levenberg-Marquardt method for training recurrent 
PNN. Such temporal BP training methods can be designed in either of 
two ways: 1) by computing the full temporal Hessian, or 2) by computing 
a diagonal approximation to the temporal Hessian. 
The diagonal approximation of the temporal Hessian can be made 
as in the static case by multiplying the first-order error derivatives of 
the network outputs with respect to the weights at the particular time 
instants: {dP{t)/dwi) {dP(t)/dwj) 
. The full temporal Hessian consists 
of second-order error derivatives of the PNN outputs with respect to the 
weights at the concrete time step: 
â¢Â«M'â¢)^i{(ff)(^)^-<')(IS)l <-
where e{t) â y{t) â P{t) is the error at time t. These entries can be ob-
tained by specializing the 7^-propagation algorithm [Pearlmutter, 1994] 
for recurrent PNN. 
7^-propagation for Recurrent PNN. The 7^-propagation algo-
rithm [Pearlmutter, 1994] performs forward and backward network tra-

Te'w,poral BackpropagafAon 
201 
versals for estimating the derivatives, adapts the matrix after each next 
example, and repeats such cycles W times, once for each column of the 
Hessian. The temporal output derivatives in recurrent PNNs can be 
made in two ways: 1) using the BPTT algorithm (Section 7.2), or 2) us-
ing the RTRL algorithm (Section 7.3). The BPTT algorithm, however, 
may not produce the proper instantaneous derivative dP/dw^r). 
That 
is, it may not be equal to the exact one dP{t)/dw^ because it is usually 
computed with training history size much smaller than the size of the 
whole interval from the current moment down to the beginning of the 
training interval. Moreover, the gradient computed backwards becomes 
very small. This is known as the vanishing gradient problem [Bengio et 
al., 1994]. For this reason, the RTRL algorithm should be preferred for 
computing the temporal Hessian in recurrent PNN as it is more reliable 
for generating the instantaneous derivative 
dP{t)/dw. 
According to the discrete-time equation (7.3), the network root node 
generates output Pk{t) = J2j ^kjUj{t), where w^j is a weight on link from 
the j-th child node feeding an external variable Uj{t) = Xi{t)^ an acti-
vation polynomial output Uj(t) = Yli^jiPii"^-)) ^^ ^ P^^^ state of some 
hidden node Uj{t) â s[t) (7.4). The forward pass of the 7?.-operator 
apphed to activation polynomials like Pki^) and Pj{t) involves the com-
putation of the following quantities: 
nvM 
= E^i^^i(0 
(7.45) 
i 
n{Vjm 
= Y.^J{ll{V^{t)] 
+ Y.'',^P^{t) 
(7.46) 
i 
i 
nPkit)} 
== J2'^kjn{pj{t)} 
+ ^VkjPj{t) 
(7.47) 
J 
j 
where Vji G v is an element from the binary unit vector v in 7^ = 
v'^V (6.47). This is different from the static case because here there 
are discretely sampled external variables, and the polynomial outputs 
appear at discrete steps, which lead to different derivatives. 
During the backpropagation pass, the 7^-operator is apphed to the 
backprop errors estimated at the root and at the hidden network nodes. 
The result at the root output node coincides with the operator ap-
phcation with the root polynomial. 
At the hidden nodes the back-
prop errors are: 6j{t) â (-<5/c(^))pL(^) where the derivative is p'^^if) â 
dpk(t)/dpj{t). 
Acting on these backprop errors with the 7^-operator 
leads to the following two corresponding equations: 
n{Sk{t)} 
= 
n{pk{t)} 
(7.48) 
Tz{sm = n~hmp',i{t) + {-sdtmip'kM 
(7.49) 

202 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
where 7^{<5/^(^)} is the apphcation of the operator at the root node, and 
Tl{6j{t)} is the apphcation of the operator at hidden nodes. 
Traversing the tree-hke PNN structure down from the root toward the 
leaves is accompanied by computing the error derivatives: dE{t)/dw}^^j 
(7.28, 7.29), and dE{t)/dWj, 
(7.30, 7.31). The apphcation of the 7^-
operator to these error derivatives is as follows: 
A
^
\ 
= -nh{t)}u',,{f)~8u{t)n{u',^^{t)] (7.50) 
AT^\ 
= -nm)^'n{^^-mnu':M 
(7.51) 
where for clarity, two additional variables u'^. and u'-^ are used. 
These variables u'^- and u'- distinguish the effects of the recurrent 
connections in the network; they capture the impact of the backward 
hnks on the neural network output at the particular time step. The 
variables u'^. and u'-^ are substitutions for the following equations: 
<,{t) 
= E - f c ^ ^ ^ Â£ ^ + Â« . W 
(7.52) 
u',S) = E^Jr^^'^^^ 
+ u.it) 
(7.53) 
where pi is an arbitrary activation polynomial output sent via the cor-
responding recurrent connection. When the 7^-operator is applied to 
these variables the summation goes in front and the operator acts on 
each incoming link that enters the node. Their corresponding applica-
tions ^{u'j^{t)} 
and ^{u'j^{t)} after some simplifications become: 
n<jit)} - E (^'^^^{^^Â£^1+"""-p"') + ^{Â«^(*)} (7-54) 
where p/.^ = dpk{t - l)/dwkj, pi ~ dpi{t - l)/dwji, 
and the opera-
tors Tl{dpk{t â l)/dwkj} 
and TZ{dpi{t â l)/dwji} 
denote applications 
to polynomials that can be made using the ready formulas (7.45, 7.46, 
7.47). 

Tem,poral 
Backpropagation 
203 
Table 7.4. Summary of the 7^-propagation algorithm for recurrent PNN. 
7^-propagation for Recurrent P N N 
step 
Algorithmic 
sequence 
1. Initialize 
Data V = {(xn,^n)}^-^i, and a vector v = (0,0, ...,0) 
2. Train 
For each weight c^ I < c < W 
Network 
Weights 
3. Evaluate 
a) Set the vector entry v^ = 1.0. 
b) Compute the dot product v'^^H in one epoch as follows: 
For each training example (xn,2/n), 1 < n < A^, do 
i) Perform a forward pass and estimate 
Vk{t) = 
Y^.wujU^it) 
^{pk{t)] 
= Y.. Wkjn{pj(t)} 
+ J2. VkjPj{t). 
ii) Perform a backward pass 
- calculate the output node delta and derivatives 
6,{t) = y{t) - P ( 0 , Pi,{t) = 
dp,{t)/dp,{t), 
and x'^jit) = 
dpk{t)/dwkj. 
- apply the 7^-operator 
n{6,,{t)} = n{pk{t)} 
'^'kjit) = I]^'Â«^fcr-(^P/e(^ - l)/dWkj) 
-^Uj{t) 
n<j{t)} 
= Er^r 
+ 
n{Uj{t)} 
Rr â WkrTl{dpk{t â \)/dWkj] 
+ Vkrdpk{t " 
l)/dWkj 
n{dE{t)/dwkj} = -n{6k{t)}u',,j{t) - 6k{t)n{u'j,,j{t)}, 
- calculate backwards hidden deltas and derivatives 
^j{t) = (-Sk{t))p'f,,j{t), 
and x'j,(t) = 
dpj{t)/dwji. 
- apply the 7^-operator 
n{6j(t)} 
= n{~6,it)}p',,{t) 
+ 
{-6k{t))n{p^t)} 
<:(^) = Er'^JridpiXt 
- l)/dWji)dpi(t 
- 1) + Ui{t) 
R^ = WjrTl{dpi{t - l)/dWji} 
+ Vjr{dpi{t - 
l)/dWji) 
n{dE{t)/dwji} = -n{6j(t)}u'^i{t) - 6j{t)n{u'j,{t)}. 
Extract the Hessian from v'^H. 
Having algorithms for calculation of the temporal Hessian provides 
abilities not only to develop second-order temporal BP training algo-
rithms for recurrent PNN, but also to implement second-order network 
pruning, as well as to realize approaches for estimation of confident, pre-
diction intervals. Further discussions on recurrent network pruning and 
regularization are provided in subsections 7.7.1 and 7.7.2. 
The algorithm for performing 7l-propagation on recurrent PNN is 
given in the above Table 7.4. 

204 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
7.6 
Recursive Backpropagation 
Recent research in temporal data processing points out that an effi-
cient way to avoid many of the disadvantages of gradient descent search 
techniques for dynamic neural network training is to apply recursive least 
squares methods, which are closely related to Kalman filter methods 
[Haykin, 2001]. Instead of first-order or second-order gradient descent 
algorithms one can use recursive least squares for weight adaptation 
to achieve faster convergence. The recursive least squares methods are 
suitable for modefing dynamically changing data as they utilize the in-
formation in the data arriving as a sequence, and so enable us to capture 
time-varying eff'ects. These methods are convenient for weight learning 
in PNN when processing temporal data where the traditionally used 
OLS suff"ers from numerical instabilities. 
A recursive backpropagation (RBP) algorithm is developed here for 
minimum least-squares weight estimation in recurrent PNN. This is a 
kind of neuron-level Kalman filter training algorithm applied locally at 
each network node [Shah et al, 1992, Puskorius and Felkamp, 1994]. The 
recursive formula treats the activation polynomials in the polynomial 
network nodes as linear, since they are linearized by means of their out-
put dynamic derivatives with respect to the weights. The RBP updates 
the weights using backward propagated dynamic derivatives. Since the 
dynamic derivatives are computed in the style of the backpropagation 
algorithm this gives the name RBP. RBP repeatedly performs a number 
of epochs, where each epoch involves an incremental presentation of all 
data from the beginning to the end of the given interval. 
The learning objective of RBP training is minimization of the in-
stantaneous cost function: E{t) = {l/2){y{t) â P{t))'^, Assuming that 
the weights on finks entering a particular network node at time t are 
arranged into a vector: w(^) â [w\{t)^W2{t)^ .,.,iL'^(i)], and the approx-
imate error-covariance matrix is A of size rn. x m^ the equations for 
calculating the weight updates are as follows: 
v{t) = A-\t-mt) 
(7.56) 
k{t) = rit)\r,-'+fit)r{t)]-' 
(7.57) 
w{t) = w{t-l)+k{tMt)-P{t)] 
(7.58) 
A-\t) 
= rj[A-\t-l)-k{t)f{t)A-\t~l)] 
(7.59) 
where j(^) is the vector with dynamic output derivatives with respect to 
the node weights from the previous time step: j(^) = 
dP{t)/dw\^^^^f_i^, 
and ?7 is a forgetting factor that plays a role similar to learning rate. 
This set of recursive formula considers one weight vector and corrects 
it in pursuit of reaching a minimum mean squared error solution. Using 

Tem,poral Backpropagation 
205 
the a priori vector w(f. ~ 1), a one step ahead prediction of the network 
output P{t) is generated. The predicted output P{t) is then related to 
the new presented measurement y{t) to evaluate the discrepancy between 
them, which serves as innovation suggesting by what amount to adjust 
the weight. The a posteriori weight vector w(^) is computed by formula 
(7.58) in proportion to the innovation and another quantity called the 
Kalman gain (7.57). The computation of the Kalman gain requires one 
to recompute the covariance matrix A~^(0) (7.59). 
The dynamic derivatives at each time instant can be determined using 
the RTRL algorithm for recurrent PNN. These dynamic derivatives ac-
count for weights on connections feeding three kinds of inputs: external 
variables, activation polynomial outputs, and internal network states. 
The effect of the forgetting factor is to diminish the effect from distant 
data in the past. During incremental training the output P{t) at time 
t captures information about the whole data history, and therefore the 
error y{t) â P{t) also carries information about all observed data from 
time ^0 + 1 up to the present t. The forgetting parameter values are 
selected from the interval r]~^ G [0 -r-1]. The inverse of the approximate 
error-covariance matrix is initiahzed with large elements [A~^(0)]^^ = c, 
where c is a number in the range c e [10^^ -f- 10^]. 
Applying such recursive algorithms should be accompanied by adding 
artificial process noise [Q{0)]ii e [10-^^ ~ 10-^] to the diagonal of the 
covariance matrix to speed up the training convergence [Haykin, 2001]. 
When training incrementally, the noise should be adapted over time so as 
to add a stochastic character to the learning process. A simple strategy 
to tune the noise is to modify it using simulated annealing so that it 
slowly diminishes with time. 
When RBP is applied to tree-structured PNN where each hidden node 
feeds exactly one parent node, there is no need to perform matrix in-
version because the quantity inside the brackets in equation (7.57) is a 
scalar. This avoidance of numerical computation difficulties makes RBP 
attractive for inductive problem solving. Another reason to use RBP 
is its relatively low computational complexity 0{m?T)^ where m is the 
number of weights per neuron, and J^ is the number of neurons in the 
network. This is only the complexity for evaluating the matrix equations 
(7.56, 7.57, 7.58, 7.59), however. Therefore, in order to obtain the full 
complexity of RBP, the overhead for computing the dynamic derivatives 
by the RTRL algorithm have to be added. 
The recursive BPA training algorithm speciahzed for training recur-
rent PNN, using derivatives calculated according to the RTRL strategy, 
is illustrated in Table 7.5. 

206 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
Table 7.5. BPA for training recurrent PNN using derivatives computed by RTRL. 
Recursive Backpropagation for P N N (incremental version) 
step 
Algorithmic 
sequence 
2. Perform 
network 
weight 
training 
1. Initialization 
External data x{t), states s{t), derivatives dp{0)/dw = 0, 
forgetting factor r]~^ = 0.9, noise [Q(0)]7;7; = l.Oe - 3 
initial covariance matrix of elements [A~^(0)]7;7: = l.OeS. 
Repeat 
a) For each time instant t^ to < t < tj â 1 do 
i) Forward propagate the inputs x{t) and the 
states s(t) to compute node outputs, 
ii) Backward pass: during top-down tree traversal do 
- if this is the root node compute the output error 
S,.{t) = y{t) - Pit) 
and the dynamic derivatives 
j(<) = 
dP{t)/dy,,.j. 
- else if this is a hidden node compute the derivatives 
j(t) = 
[dP{t)/dpjmdMt)/d^ji]-
- calculate the Kalman gain 
k(*) = A-i(Â« - i)j(0['?-' +f(t)A-\t 
- 
mt)]-' 
- update the weights at this network node 
w{t)=w{t-l) 
+ 
k{t)[y{t)-P{t)]. 
- update the covariance matrix 
Vit) = 
k{t)f{t)A'\t-l) 
A-\t) 
= 4A-\t 
- 1) - V(i)l + Q(Â« - 1). 
- update the process noise Q{t) = F[Q{t â 1)], 
Until termination condition is satisfied. 
7.7 
Recurrent Network Optimization 
The complexity of recurrent PNN should be optimized during and af-
ter their temporal backpropagation training. Their generalization per-
formance could be improved because they may not be topologically rel-
evant enough and tuned to the data, and also there may be redundant 
weights in the model. Even if the network has been learned by IGP, 
superfluous weights can still remain in it, or at least their effect on the 
output should be diminished in order to enhance the model predictabil-
ity. When the network is complex, in the sense of having redundant 
weights or weights of excessive magnitudes, it tends to overfit the data 
and exhibits a large generalization error. In temporal BP training, the 
complexity of recurrent PNN can be controlled by applying regulariza-
tion and optimized using pruning methods. 

Temporal BackpropagatAon 
207 
7.7.1 
Regularization 
Regularization can be implemented by adding a weight decay penalty 
to the total temporal training error function, thus making a regularized 
average error (RAE) similar to the one for static networks. The derived 
regularized temporal weight training rule, which is easy to instantiate for 
BPTT and RTRL in particular, remains essentially in the same format: 
w = {1 â rjX)w + Aw, where A is the regularizer. 
Recurrent PNN can also be regularized in a specific way so as to 
separately control the complexity of feed-forward and the complexity of 
backward connections. This idea for separate weight shrinking can be 
implemented by reformulating the regularized average error to account 
for the effects from the different groups of weights separately: 
t^to-^l 
f 
b 
J 
where / is the index of the forward links, and b is the index of the back-
ward recurrent links. The different regularizers Xf and A^ can be found 
using the Bayesian regularization approach. When the Bayesian formula 
is apphed to determine the proper values of one of the regularizers, the 
other should be kept fixed. The temporal weight training rules obtained 
as solutions to the above RAE equation set to zero at the minimum of 
the error surface are: 
Wf = {l â r]Xf)wf-\-Awf 
(7-61) 
wt=^{l-rjXt)wt 
+ Awh 
(7.62) 
where r] is the usual learning rate parameter. 
7.7.2 
Recurrent Network Pruning 
A direct approach to optimizing the structural complexity of recur-
rent PNN is weight pruning. Network pruning methods are direct in 
the sense that they eliminate connections while regularization is an indi-
rect method because it only diminishes the weight magnitudes without 
necessarily removing weights. Recurrent PNN can be pruned using the 
same methods developed for static PNN which are discussed in Section 
6.5. There are first-order (Section 6.5.1) and second-order (Section 6.5.2) 
pruning methods. The diff'erence is that they have to be implemented 
using the temporal gradient and the temporal Hessian which makes their 
design more involved [Pedersen and Hansen, 1995]. 
Training of recurrent PNN by recursive least squares also enables us 
to perform efficient pruning because the RBP algorithm computes the 

208 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
covariance matrix at every time step which contains important infor-
mation about the network weights. A weight can be discarded from 
the recurrent network if its magnitude is small and its deviation during 
training is large. The saliency of a recurrent PNN weight is determined 
with the following formula: 
Sij = ^-f^\A-\t)],jwf^ 
(7.63) 
where [A~^{t)]ij is the diagonal entry of the covariance matrix, and 77 
is the forgetting parameter. The usefulness of this formula comes from 
the possibility of applying it with an arbitrary amount of data. 
7.8 
Chapter Summary 
The backpropagation techniques BPTT and RTRL presented in this 
chapter are general approaches to gradient descent training of polyno-
mial networks with arbitrary feedback connections. What makes RTRL 
different from BPTT is that the former attempts to determine the gra-
dient vector directly by computing the errors at the available time steps, 
while the latter recovers past temporal information from which the gra-
dient is obtained. BPTT is nonlocal in time but local in space, hence it 
is less complex to develop. RTRL is local in time and nonlocal in space, 
therefore much more complex to develop. 
Each of these algorithms has its advantages and disadvantages that 
can be distinguished on the basis of their computational requirements. 
The main disadvantage of the real-time BPTT algorithm is that it needs 
unhmited memory resources when running indefinitely through time, 
while the epochwise BPTT seems more attractive with respect to mem-
ory resources as it considers hmited time intervals between predefined 
boundaries. The major criticisms of the RTRL algorithm are that it 
is computationally intensive, and it communicates nonlocal information 
between the nodes which makes its implementation more difficult. The 
recurrent PNN usually feature sparse connectivity, so their training is 
faster and less demanding on memory. 
The use of the epochwise BPTT method that performs batch temporal 
training is reasonable when there is preliminary knowledge indicating 
that the series data to be modeled are stationary. When the training 
environment is nonstationary, the incremental methods (real-time BPTT 
and RTRL) should be preferred as they are less sensitive to the changing 
data trajectory. RTRL is especially attractive due to its inherent abilities 
to conduct stochastic search which can overcome the deteriorating effect 
of the ffuctuations in the given data on learning. 

Chapter 8 
BAYESIAN INFERENCE TECHNIQUES 
The previous chapters presented maximum hkehhood (ML) approaches 
to learning the network weights. They find the weights using the par-
ticular training set by minimizing the sum of squared errors criterion. 
There are alternative, more general approaches that generate maximum 
a posteriori (MAP) weights following the theory of Bayesian inference 
[MacKay, 1992a, MacKay, 2003, Buntine and Weigend, 1991b, Neal, 
1996, Bishop, 1995, Tipping, 2001, Doucet et al., 2001]. The MAP 
weights are averaged statistics of the weights distribution with respect 
to the data distribution; they are not obtained simply as point estimates 
from the provided training data. According to the Bayesian framework, 
the weights and model uncertainties are described with probability dis-
tributions, and inference is performed by applying probabilistic rules to 
them. The initial beliefs in the weights are encoded into corresponding 
priors. After seeing the data through their likelihood, the belief in the 
weights is updated using Bayes' theorem and thus the posterior probabil-
ity density of the weights is obtained. Having the weight posterior, and 
more precisely its mean, the MAP weights vector, allows us to compute 
the predictive data distribution and to generate predictions for unseen 
inputs along with their confidence intervals. 
Probabilistic reasoning enables us to handle simultaneously the weights 
and the structure of the networks. During training, the model accuracy is 
balanced automatically with the model complexity so as to improve gen-
eralization. This integration helps to mitigate the over-flexibility prob-
lem in neural networks that causes capturing of spurious information 
from the data which is deteriorating for their predictive performance. 
The Bayesian inference techniques applied to polynomial networks off'er 
the following benefits [MacKay, 1992a, Bishop, 1995]: 1) they provide 

210 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
objective error functions and analytical training rules that implicitly ac-
count for the noise level; 2) they learn the weight parameters along with 
doing model selection; and 3) they yield probabihstic results without 
artificial data sphtting for model validation. Their shortcoming is their 
sensitivity to the prior assumptions. 
The Bayesian approaches to neural network modelling have been de-
veloped in two main directions: 1) derivation of training techniques 
based on a Gaussian approximation to the weight posterior distribu-
tion, elaborated with the evidence framework [MacKay, 1992a, MacKay, 
1992b]; and 2) derivation of techniques based on Monte Carlo samphng 
of the weights [Neal, 1996, Doucet et al., 2001]. The evidence framework 
assumes that the weight posterior is Gaussian in order to obtain the pri-
ors on the weights, and reevaluates them in an alternating manner to 
adapt the model to the data. Applying the evidence procedure to poly-
nomial neural networks allows us to achieve: 1) inference of the most 
probable weight parameters at the mean of their posterior distribution, 
with proper handling of the noise; 2) adaptive learning of global and 
local regularization parameters for model smoothing; 3) neural network 
complexity tuning by pruning due to the adaptive regularization; the 
adaptive regularization eliminates implausible weights, which involves 
automatic relevance determination of the necessary input variables and 
discarding of irrelevant connections; and 4) estimation of confidence and 
prediction intervals of the network outputs. The Monte Carlo approach 
helps to design alternative algorithms for neural network training in 
relaxed conditions like non-Gaussianity. Most popular is the Markov 
Chain Monte Carlo (MCMC) algorithm based on sampling. 
This chapter off'ers specialized techniques for Bayesian PNN learn-
ing. These probabilistic techniques are iterative in the sense that they 
repeatedly carry out computations with the training data until the net-
work structure and its weights become optimally adjusted. Algorithms 
have been developed for finding local, individual regularization parame-
ters for each polynomial term weight, and for pruning polynomial terms 
from the network architecture. Computing the predictive density and 
performing prediction using this data distribution is explained. Sparse 
Bayesian learning is investigated using the Expectation-Maximization 
(EM) algorithm. Next, a recursive algorithm for sequential Bayesian 
learning is given. This algorithm is especially suitable to apply when 
modelling time series. Finally, a Monte Carlo samphng algorithm made 
for training polynomial networks is illustrated. Bayesian prediction in-
tervals for PNN are later discussed in Chapter 9. The specific application 
details concerning linear PNN models and nonlinear multilayer PNN are 
discussed separately. 

Bayesian Inference Techniques 
211 
8.1 
Bayesian Error Function 
The problem of inductive learning with neural networks is to find the 
most probable weights from the data. This learning problem usually 
has to deal with noisy targets, so overfitting avoidance is an essential 
objective. Avoidance of overfitting with the data can be pursued adopt-
ing the principles of Bayesian inference theory. Bayesian methods use 
probabilities to represent uncertainties, and doing inference with them 
involves solving integrals; that is, finding averaged statistics. 
Traditionally, neural network training aims at minimization of the 
sum-of-squares error function estimated with the data (6.2): 
1 ^ 
Â£ ^ D ( w ) - - ^ ( y n - P ( X n , w ) ) 2 
(8.1) 
where i/n are the targets, and P(xn, w) are the network outputs produced 
with the n-th input vector x^ using the weights w. The relationship 
between the targets and the model is: i/n = F(xn, w) + e^, where 6^ is 
independent Gaussian noise with zero mean and variance a^, which is 
usually denoted by 6^ â Af{0, (Jy). 
Bayes' theorem helps to find that the degree of data fitting is not suf-
ficient as a single learning criterion because it may lead to data sensitive 
models. It is necessary to impose structural constraints to the network 
in order to make it smooth enough to attain good generalization. This 
factor is subjective because the characteristics of the model that best 
describes the data are not known in advance. The model generalization 
can be improved by adding to the error complexity penalties that favor 
simpler, sparse models with small weights. This is achieved by regular-
izing the accuracy ED{W) (8.1) with a complexity factor Ew{^) 
factor, 
and so making a Bayesian error function: 
EH{W) 
= (3ED{^) 
+ Ew{^) 
(8.2) 
where the hyperparameters a enter the factor Ewiy^)-
A popular complexity factor in the neural network community is the 
weight decay regularization^ defined by the following equation: 
EwM = iw'^Aw 
(8.3) 
where A is a diagonal matrix with the prior hyperparameters A = 
diag(ao,ai,a2, ...,avi/). In the general case, there is a separate, local 
hyperparameter a^ that controls each particular weight Wi^ 1 < i 
<W, 
The number W of hyperparameters is equal to the number of weights. 
The hyperparameters only take positive values. 

212 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
The effect of this complexity factor can be explained from two points of 
view. First, viewing this complexity factor (8.3) as a formula dependent 
on the number of weights in the network implies that too many weights 
should be penalized. This is because a complex model approximates the 
data very closely, including even their uncertainties, and thus it leads 
to overfitting. Less complex, sparse networks should be favored because 
they are more likely to describe the data smoothly, and so they are 
more likely to have good generalization potential. Secondly, viewing 
this complexity factor (8.3) as dependent on the weights imphes that 
small weight values should be tolerated while large weights should not 
be preferred as they imply fluctuating mappings. 
8.2 
Bayesian Neural Network Inference 
The learning of neural networks using the above cost function (8.2) 
may be envisioned as a process of Bayesian inference of the posterior 
distribution of the weights given the data. The weight posterior can be 
described using the Bayes' theorem: 
Prfwly X a ^ M = P K y | ^ , w , r ^ ) P r ( w | a ) 
r n w | y , A , a , p 
) 
Pr(y|X,a,/3-i) 
^ 
' 
where the given training data D = {{'^n)yn)}n^\ 
^"^^ assumed to be 
independent. Here, for notational convenience, the inputs are taken in 
rows as a matrix X, and the outputs are a column vector y. 
According to Bayes' rule (8.4), the weight posterior is obtained as 
a normalized product of the data probability Pr(y|X, w,/?"^) and the 
weight prior probability Pr(w|a) divided by the evidence for the adopted 
network model Pr(y|X, a,/3~^) [MacKay, 1992a]. Since the normalizer 
Pr(y|X, a,/5"^) is a sum of Gaussians, the weight posterior distribution 
also becomes Gaussian Pr(w|y,X,a,/3~^) = A^(w|/i,]S) with mean ^ 
and covariance matrix XI. 
In this probabihstic network model (8.4), the quantity in the numera-
tor Pr(y|X, w, I3~^) is the likelihood of the data. This likelihood accounts 
for the neural network accuracy on the training set. It is actually a dis-
tribution with variance a^, which taken inversely is denoted as a special 
output noise hyperparameter' f3 = CJ"^. Assuming a zero-mean normally 
distributed noise, the Hkelihood of the data can be written as: 
Pr(y|X, w, r ^) = ; ^ ^ ( ^ exp {-pEo{yf)) 
(8.5) 
where the normahzing constant Zj:,{[5) is given by the expression: 
Zu{p-') = [2'K[i~'f'^ 
(8.6) 

Bayesian Inference Techniques 
213 
The second quantity in the numerator Pr(w|a) is the prior probability 
of the weights. This weight prior accounts for the assumptions concern-
ing the correlations between the weights and the shape of the functional 
network mapping. The treatment of this prior follows modern Bayesian 
analysis which provides ideas for proper assignment of subjective be-
liefs in the inductive learning process [MacKay, 1992a]. More precisely, 
Bayesian analysis using weight priors gives formulae that show how and 
where we may include our initial knowledge about the relative model 
complexity with respect to the data. 
Two kinds of priors can be assigned: a global prior, which is a common 
regularization hyperparameter, or local individual priors for each weight. 
Both kinds of prior hyperparameters can be expressed with the notation: 
a = [o^i,..., aw]- In the case of a global prior, we have: ai = a2 == â¢â¢â¢ =^ 
aw- Imposing local Gaussian priors on the weights is described by the 
following zero-mean factorized prior: 
1 
HI". 
where the hyperparameters are independent, and the normahzer is: 
Zw{a^) = (^J 
(8.8) 
which involves the weight variances written inversely as: a^ = a~^. 
The quantity Pr(y |X, a, /3~^) in the denominator of the Bayesian net-
work model (8.4) is the joint distribution of the hyperparameters. This 
distribution is constant, does not depend on the network weights, and 
is called evidence [MacKay, 1992a]. The calculation of the evidence for 
the hyperparameters enables us to determine the level of regularization 
with which the maximum posteriori weights are obtained. The evidence 
plays another special role in the Bayesian paradigm, namely it helps 
to rank the networks and to carry out objective model selection. It is 
useful to compute this evidence for PNN because they are extremely 
flexible models that often tend to overfit, so the abihty to distinguish 
the optimal models among them is important for successful learning. 
The conventional backpropagation algorithms attempt to find the 
weights w by maximizing the hkelihood Pr(y|X,w,/?~^), thus trying 
to maximize the posterior probability Pr(w|y,X, a,/3"-^) relying on a 
fixed network architecture. What the Bayesian approach changes in this 
strategy is providing means for adaptation of the model complexity with 
respect to the data distribution, in other words, for dealing properly with 
alternative models. The evidence procedure [MacKay, 1992b] shows that 

214 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
this idea can be implemented by estimating the hyperparameters a and 
P iteratively, in order to approach the maximum of the weight poste-
rior and to produce a plausible neural network model. Since the weight 
posterior is sensitive to the changes of a and /3, they are called hyper-
parameters. Tuning these hyperparameters impacts the computation of 
the weight posterior distribution. 
A picture of the probabilistic PNN learning process is demonstrated 
here using the Hermite polynomial: 1.0 -j- ((1.0 â t -j- 3t^) * exp{--f?))^ 
where the argument ^is: âb<t<b. 
The generated data were ad-
ditionally contaminated by random noise of variance 0.01. The targets 
to be modelled are the next values in the series: xt. In order to inves-
tigate the development of the probabilistic inductive process, an initial 
PNN architecture was designed using two input variables: x/,_i, xt-2^ 
all second-order polynomial terms, and all third-order terms. This PNN 
was evaluated with random initial weights, and next sparse Bayesian 
learning (Subsection 8.4) was performed to obtain a parsimonious PNN 
model with maximum a posteriori (MAP) weights. 
The effects from Bayesian inference of the weights and the PNN map-
ping are illustrated in Figure 8.1. The upper parts of this figure show: a) 
the projection of the weight prior distribution made with two randomly 
drawn weights from FT{WI,W2)^ 
where the two-dimensional grid dimen-
sions indicate that the weight values are taken from the interval [~1; 1]; 
and b) the eUipsoidal projection of the inferred peaked weight posterior 
distribution Fi(wi, W2|y, X, a, /?~^) computed with the same weights wi 
and W2^ plotted together with the initial prior contours. It can be seen 
in Figure 8.1a that the Bayesian training process starts with an almost 
flat prior over the weights. At the end of the network training process 
the weights converge to a posterior distribution which is peaked around 
the most probable weight vector. This most probable weight vector is 
plotted in Figure 8.1b with a bold ellipse which is the contour of the 
peaked distribution projected onto the weight space. 
The lower parts of Figure 8.1 show: c) PNN functions generated using 
the randomly drawn weights from Figure 8.1a; and d) the final PNN 
approximation estimated with the inferred MAP weights vector from 
Figure 8.1b. In Figure 8.1c it can be observed that when the initial 
weights arbitrarily sampled from their prior distribution are used in the 
PNN network model, the network produces varying curves that do not 
pass exactly through the given data. When the learned MAP weight 
vector is used in the neural network architecture, it leads to curves that 
closely approximate the data. Figure 8.Id demonstrates the outputs of 
this PNN model having MAP weights. 

Bayesian 
Inference 
Techniques 
215 
Figure 8.1. Demonstration of Bayesian inference with a polynomial network model: 
a) Contours of the flat weights prior distribution Pi{wi,W2); 
b) inferred weights 
posterior distribution Fi{wi^W2\y:X,a,/3) 
whose peak contour is a bold ellipse. 
Figure 8.1. Demonstration of Bayesian inference with a polynomial network model: 
c) Functions generated with weights drawn from the prior along with the target in 
bold; d) Learned approximating function in bold generated with the MAP weights. 
8.2.1 
Deriving Hyperparameters 
The Bayesian theory for learning in neural networks prescribes to 
first calculate the weight posterior Pr(w|y,X, a,/3~^) and next to use 
it for computing the probability density of the data. The probabihty 
of the data is necessary for deriving the values of the hyperparameters 
and for prediction. The weight posterior, however, cannot be evaluated 
directly. The reason is that its normahzer Z' cannot be obtained ana-
lytically: Fi{w\y,X,(x,p~^) 
:^ {l/Z')exp{-PEr>{w)-EwM). 
This 
is why, the total regularized error (8.2) is substituted by its quadratic 
Taylor expansion in the vicinity of a significant minimum to facilitate 
tractable probabilistic reasoning. The weight posterior distribution in 
this case tends to a single Gaussian [MacKay, 1992a]. 

216 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
The Gaussian weight posterior expressed around the extremum arising 
from weight vector w^p is: 
Pr(w|y,X,a,/3~^) c^ â exp f-Â£;i^(wMp) - - ( w - WMP)'^H(W - WMP) 
(8.9) 
where H is the Hessian matrix of second-order derivatives of the regular-
ized error EJI{WMP) 
(8-2) evaluated at this extremum: H = VVÂ£^i^(w/v/p), 
H = /3VVÂ£^D(w/vfp) +A. Since the posterior tends to a single Gaussian, 
its normalizing constant is: 
Z^{a,P~') = (2^)^/2 IHr'/^expi-ERiwMp)) 
(8.10) 
where |H| stands for the determinant of the Hessian matrix (6.39,6.40). 
The following notational relationship between the Hessian and the in-
verted covariance matrix holds: H = E~^. 
Although it is not clear whether a sufRciently good minimum on the 
error surface has been located after training of the neural network until 
reaching the weight vector w ^ p , this Gaussian posterior approximation 
(8.9) is very useful. If polynomial networks are evolved by genetic pro-
gramming, a large number of networks are sampled during each run, so it 
can be expected that a good minima on the error surface is attained and 
the weight vector is nearly optimal. For the sake of precision, the best 
network from a number of runs may be taken. The rigorous approach 
is to apply the Bayesian techniques to polynomial networks after their 
identification by genetic programming, however this does not preclude 
their apphcation to manually crafted PNN. 
Finding proper hyperparameter values is important for successful poly-
nomial network learning as they determine the model complexity through 
the calculation of the regularizers. The Bayesian inference aims at max-
imization of the weight posterior with plausible hyperparameter. The 
most probable values of the hyperparameters are those which maximize 
the evidence. This reasoning comes from hierarchical analysis at a sec-
ond level (supposing that the first level studies the distribution of the 
weights), which reveals that the maximization of the fikelihood of the 
hyperparameters Pr(y|X, a,/3~^) helps to obtain their maximum pos-
terior values. Substituting the likelihood (8.5), the prior (8.7), and the 
approximated posterior weight distribution (8.9) in the Bayes' theorem 
(8.4), gives the evidence for the hyperparameters: 
where the integral in the numerator evaluates to ZE{OL^J3~^) (8.10). 

Bayesian Inference Techniques 
217 
Taking the logarithms of both sides of the above probabihty yields 
the log of the evidence criterion: 
log Pr(y|X, a, p-') = -(5Eo - Ew ~\ 
[log |H| - log |A| - c] 
(8.12) 
where the constant \s c â N \og 13 -\- 7Vlog(27r). 
The evidence procedure maximizes the log of the evidence criterion 
(8.12) searching for the most probable posterior values for the out-
put noise variance j3 and the prior hyperparameters a. The optimiza-
tion is carried out by calculating the derivatives of the log-evidence 
logPr(y|X, a,/3""^) with respect to each of these hyperparameters, equat-
ing these derivatives to zero, and next solving for a and (3. The following 
updating formula for the hyperpriors a is obtained: 
where 7 is the effective number of network parameters (8.18, 8.19). 
Taking the derivative of the log of the evidence criterion with respect 
to P gives the learning rule for the hyperparameter j3: 
where A^ is the number of the available training data. 
Seeking to maximize the log-evidence (8.12) of the hyperparameters 
so as to minimize the Bayesian error function, these formulae (8.13) 
and (8.14) are reevaluated repeatedly until an acceptably low error is 
attained or until the improvement is less than a selected threshold. The 
values of the hyperparameters are then plugged in as regularizers to 
compute the network weights. The network weights are trained iteration 
by iteration, so they are sensitive to the level of regularization. 
8.2.2 
Local vs. Global Regularization 
There are two cases to consider: using a global regularization para-
meter for all weights, and using individual regularization parameters for 
each weight. A global, common hyper prior a for all weights in the vector 
is made by defining a = [a, a,..., a], while local hyperpriors a^ for each 
network weight are defined with the vector a = [ai, a2,.., cxy/\. The reg-
ularized error criterion (8.2) can be rewritten in matrix notation to cap-
ture such regularization effects as follows: Eji{w) = 0.5 (e'^e -j- w^Lw j , 
where e = [ei, 62,..., e/v] is the error vector of elements Cn â Vn â 
P(x-^, w), 1 < n < A/", and w is the weight vector. 

218 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
Bayesian regularization of the network weights is often implemented 
using the fraction between the regularization constants a and the noise 
variance /? to achieve stability in the computations. The global regular-
ization uses a diagonal matrix L = diag{X, A, A,..., A) of size {W + 1) x 
{W -h 1). The common global regularization parameter A allocated on 
the diagonal of this matrix is: 
^ = -. = (w^)^ 
(8-15) 
where 7 is the number of effective network parameters. 
The goal of PNN modehng is to properly learn the model regions 
of both low and high nonlinearities in the data. The overfitting, how-
ever, may vary in the different regions of the model. Since each term 
contributes a different curvature to the overall model, there is a need 
to manipulate each term separately in order to adjust the polynomial 
smoothness. Adapting the weights through local regularization helps 
to achieve: 1) accurate quantification of the impact of each polynomial 
term on the overall output, and 2) local control over the particular term 
nonlinearities and adaptation of the model curvature to the data. 
Local regularization is accomplished using a diagonal matrix L ~ 
diag^Xo^ Ai, A2,..., Xw) whose local regularization parameters are defined 
with the equation: 
\ = (^) 
~ 
(8.16) 
where Wi is the weight shrinked by its corresponding regularizer A^. 
The number of well-determined parameters 7 is computed in different 
ways in the two cases of linear-in-the-weights PNN models and inherently 
nonlinear multilayer PNN. This is because in these cases the Hessian 
matrix is evaluated with different techniques. 
8.2.3 
Evidence Procedure for PNN Models 
Evidence for Linear-in-the-weights PNN. The evidence frame-
work [MacKay, 1992a] enables automatic adjustment of the level of regu-
larization, and thus it helps to enhance the model generalization perfor-
mance. This framework was originally developed for linear networks like 
the linear-in-the-weights PNN. Training these networks by least squares 
techniques leads to maximally likely (ML) weights that are optimal in the 
mean squared error sense. However, these techniques cause overfitting 
because they tend to fit the data too closely. The Bayesian techniques 
help us to obtain maximum a posteriori (MAP) weights which overcome 
such problems. They infer the mean of the weight posterior distribution, 
not simply point estimates. 

Bayesian 
Inference 
Techniques 
219 
Bayesian regularization is performed by adding the individual regular-
izers L = diag{XQ, Ai, A2,..., Xw) to the diagonal of the inverse covariance 
matrix *'^#, which impacts the least-squares estimation of the network 
weights in the following way: 
w = (*'^*H-L)-^*'^y 
(8.17) 
^, * is the 
where w is the column weight vector w = [WQ^ I(;I,..., m, 
design matrix (1.8), and y is the output vector. 
The number of well-defined parameters 7 in linear-in-the-weights poly-
nomial networks are computed by the formula: 
7 
w 
* ^ ^ ) 
i=Q 
(8.18) 
where ^ ^ ^ is the inverse covariance matrix of the polynomial model, 
and the index ii enumerates its diagonal elements. These parameters 7 
are also called effective parameters (4.6). 
The algorithm for computing individual Bayesian regularization pa-
rameters for linear PNN is illustrated in Table 8.1. 
Table 8.1. Algorithm for calculating local regularization parameters for linear-in-
the-weights PNN according to the Bayesian evidence procedure. 
Computing Bayesian Regularizers for Linear P N N 
step 
Algorithmic 
sequence 
Data V = {(xn,yn)}n^i and learning rate 7] = 0.1. 
A PNN architecture with W weights 
initial hyperparameters: a â 1.0/VK, and /3 = 
1.0/stdy. 
Repeat a predefined number of times: 
a) Make local regularizers; L = diag(XQ^ Ai,..., Xw) 
Xi = a,;//3, 
0<i<W, 
b) Reestimate the weights by OLS fitting 
w - (^^^ 
+ 
h)-^'^^y. 
c) Compute the effective number of parameters 
7 = l^-E,":oA.:[(*^*)-l:,: 
and the hyperparameters 
P={N 
~ 
j)/e^e. 
d) Calculate the Bayesian error 
ER{W) 
= e^e + w'^Lw 
until minimum of the log-evidence criterion is reached, 
log Pr(y|X, a, (3) = -e^e 
- w'^Lw - ^ [log |H| - log | A|] 
1. Initialization 
2. Reevaluate the 
hyperparameters 

220 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
Evidence for Nonlinear Multilayer PNN. The computation of 
the hyperparameters according to the Bayesian evidence procedure is 
more involved when processing nonlinear PNN models, because in such 
cases the Hessian (6.39, 6.40) has to be computed in a special way using 
the 7^-propagation algorithm [Pearlmutter, 1994]. 
Consider a nonlinear multilayer PNN model trained with the BP al-
gorithm to minimize the Bayesian cost function (8.2). The algorithm is 
made to perform updating of the weights using local regularization with 
the following gradient-descent training rule: Wi = [1 â r]Xi)^Wi -f- Awi 
(6.34). Next, the 7^-propagation algorithm (Section 6.3.1) is apphed to 
evaluate the Hessian matrix. Then, the number of the well-determined 
parameters 7 in nonlinear PNN is obtained as follows: 
7 = 
w 
.19) 
i=0 
where [H~^].. are the diagonal elements of the inverse Hessian. 
Table 8.2 gives the algorithm for finding Bayesian regularizers for 
multilayer PNN using the 7^-propagation algorithm for the Hessian. 
Table 8.2. Algorithm for calculating local regularization parameters for inherently 
nonlinear PNN according to the Bayesian evidence procedure. 
Computing Bayesian Regularizers for Nonlinear P N N 
step 
Algorithmic 
sequence 
1. Initialization 
2. Reevaluate the 
hyperparameters 
Data V = {{xn,yn)}n~^u 
PNN with W weights 
hyperparameters: a?; = l,0/W, and jS = 
1.0/stdy. 
Repeat a predefined number of times: 
a) Make local regularizers: L = diag{Xo^Xij 
...,Xw) 
A,: = a,;//5, 
0<i<W. 
b) Re-train the weights by BP with local regularization 
w = (1 â r]L)^w + Aw. 
c) Calculate by 7^-propagation the dot product 
v'^H = V^'^ER, 
and extract the Hessian H. 
d) Compute the effective number of parameters 
and the hyperparameters 
ai = j/wf^ 
and (3 â [N â 
j)/e^e. 
e) Calculate the Bayesian error 
ER{W) 
== e^e + w^Lw 
until minimum of the log-evidence criterion is reached. 
logPr(y|X, a,/?) = -e'^e - w'^Lw - ^ [log |H| - log |A|]. 

Bayesian Inference Techniques 
221 
8.2.4 
Predictive Data Distribution 
Neural network learning involves finding the weights, and then eval-
uating the predictive data distribution. The predictive distribution is 
taken to generate model outputs from future unseen inputs. The data 
distribution provides an estimate of the uncertainty about the network 
outputs and enables us to make probabilistic predictions. The predictive 
distribution of the model outputs is: 
Pr(y.|xây,X,0) = J Pviy,\^,,y,X,w,e)Pr{w\y,X,e)dw 
(8,20) 
where y^ is the prediction, x^ is the unseen input, and 0 = [a,/3~^]. 
Although this probability cannot be directly computed as the integral 
is intractable, in practice its Gaussian approximation is considered. In 
this case, predictions can be calculated using the mean y* = -P(x*) of the 
distribution Pr(7/*|x*,y,X, a,/?~-^) [Nabney, 2002], while its variance /?* 
may be used to produce prediction intervals (Section 9.6). 
Let's consider the particular case of a linear PNN model. Plugging 
the inferred hyperparameters a and /3 into the normal equation yields 
maximum a posteriori weights. These weights are representative of the 
mean of their posterior distribution with which probabilistic predictions 
could be made. Given an unseen, test input vector x*, the polynomial 
network generates a forecast y^ with probability Fv{y^\:K.^, y, X, a, /3~-^). 
This forecast y^ is the mean of the predictive distribution with variance 
/3~\ computable as follows: 
y, 
- 
w^0(x*) 
(8.21) 
f3;' 
= /^-i + 0^(x*)H-V(x*) 
(8.22) 
where w are the weights, and 0(x*) is the basis vector (1.9). 
8.2.5 
Choosing a Weight Prior 
A crucial problem in applying the Bayesian evidence framework to 
neural networks and to polynomial networks in particular is what prior 
probability density to choose for the weights. There are various strate-
gies for assigning priors to linear and to nonlinear models, however there 
is still no general recipe that can robustly decide which prior is best for 
the concrete model. This is why different priors may be considered for 
diff'erent models. The previous sections presented the Gaussian prior 
(8,3) which is most suitable for linear networks. Other priors have been 
developed, especially for multilayer networks like the nonlinear multi-
layer PNN. These are the Laplace and the Cauchy priors [Williams, 
1995], obtained assuming different weight distributions. 

222 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
The weights in multilayer PNN may be symmetric within the net-
work, so equivalent mappings may occur by exchanging the signs of the 
weights on the same connections. There may even be symmetry groups 
of weights whose permuting within the same network may cause identical 
mappings. In order to capture such symmetries, the Laplacian distri-
bution may be used. It leads to a weight penalty that is sensitive to 
the absolute magnitudes of the weights. The regularization that sets a 
Laplacian prior over the weights is [Williams, 1995]: 
w 
EwM = J2ai\w,\ 
(8.23) 
where \wi\ denotes the absolute value of weight Wi. 
Research into neural network models conducted with the intention 
to prune them effectively has suggested use of the Cauchy distribution 
[Weigend et al., 1992]. The Cauchy distribution can be incorporated 
through a correcting complexity factor which is a logarithmic function 
of the squared weights. It is similar to the Gaussian penalty in the sense 
that the square of the weights influences the error, and differs in that it 
discourages too many small weights. The following regularization factor 
imposes a Cauchy prior over the weights [Williams, 1995]: 
w 
-1 
Ew{w) = ^ - log (l + afwf) 
(8.24) 
which still remains steep in the interval â1.0 < w < 1.0. 
8.3 
Bayesian Network Pruning 
The evidence framework provides a tool for automatic relevance de-
termination (ARD) that can be used for network pruning of very small 
weights. This is possible as the fast increase of some local hyperpara-
meters shrinks the corresponding weights toward zero. The advantage 
of ARD is that it evaluates the significance of all weights in the model 
together, thus pointing out that they are irrelevant in the context of 
the model as a whole. The evidence procedure can also be used in an 
incremental manner to prune weights similarly to the other backprop 
pruning techniques (Sections 6.6.1 and 6.6.2). 
The alternative Bayesian incremental pruning strategy is to discard 
weights from the initially fully connected network one-by-one by freezing 
those with extremely small magnitudes. This is made by setting the 
small weights to zero and computing the effect of this on the overall 
model performance. In the course of retraining, the weights with values 

Bayesian Inference Techniques 
223 
close to zero are discarded until the difference between log-evidences of 
the model and its version with a weight removed becomes positive. 
Let the joint data distribution with the network be Pr(y|X, a,/3~^), 
and assume that after deleting one weight from this model it becomes 
Fr{y\X,cx\/3~^'). 
The change in log-evidence criterion for model com-
parison can be calculated with the equation [MacKay, 1995]: 
2 
log Pr(y |X, a, P'') 
- log Pr(y |X, a^ ^ ^ 0 
= j
^
t
^ 
+ log 
^ ^ ^ J H ^ 
(8.25) 
which should be considered to examine eventual removal of a weight Wi 
using its diagonal entry in the inverse Hessian matrix [H~^]n, and its 
hyperparameter a^. It should be noted that this pruning technique can 
be applied to handle several weights at a time; that is, it can be modified 
for direct removal of subsets of network weights. 
The smoothing eff'ect from Bayesian incremental pruning of a poly-
nomial network is illustrated with the plots in Figure 8.2. The plots 
have been made using a randomly designed PNN for learning a model 
from a data set produced with a simple third-order function: 0.2 â 
1.6a:i ~ O.lxi â 0.4x1X2 + 0.5x1^2 + 0.9x3. There were generated 50 
data points and Gaussian noise was added from the interval: [0,0.1]. 
An initial multilayer PNN was constructed with 3 hidden nodes having 
3 x 6 == 18 weights. The overall polynomial generated by this net-
work was: P(x) = ^8(^15^8(^15^8(3^1)^2)))- The initial hyperparame-
ters were: a = 0.001, and /3 = 10. 
The PNN was trained and pruned incrementally, after which the fol-
lowing polynomial remained: 0.1972 - 1.6256xi + 0.998xi(-0.1224xi + 
0.8546X? â¢- 0.4217x2(0.987 -h 0.952x2)). A close inspection of this poly-
nomial reveals that it is very similar to the original one, and it contains 
8 weights. If it is extracted from this network representation, and next 
simplified by dropping the parentheses, exactly a polynomial model sim-
ilar to the original with 6 coefficients will be produced. 
What is interesting to note in Figure 8.2 is that the curve of the 
initial complex polynomial, to which the incremental Bayesian pruning 
procedure has been applied, seems to overfit the noise in the data since 
it attempts to pass irregularly through the given points, while it does 
not capture the hidden data regularities well. One can see that the curve 
of the initial large PNN overfits the data, obviously mislead by the MSE 
cost function. After pruning of the superfluous weights, due to their 
shrinking by the adjusted local regularization parameters, the remaining 
pruned polynomial becomes considerably smoother and approaches the 
original target function curvature quite closely. 

224 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
Figure 8.2. Smoothing of a polynomial neural network mapping by Bayesian incre-
mental pruning. The initial network has 18 weights, while the remaining simplified, 
pruned polynomial network has 8 weights. 
A common issue in both Bayesian pruning approaches, the ARD pro-
cedure and the incremental pruning procedure, is the concern that they 
involve computation of the Hessian matrix which sometimes faces nu-
merical computation difficulties. Another concern is that if reasonably 
many weights cannot be removed from the network model, computation 
time may be wasted. 
8.4 
Sparse Bayesian Learning 
The evidence framework applied to PNN brings four essential advan-
tages that help to enhance their generalization potential: 1) it performs 
Bayesian inference with training formula obtained using proper proba-
bilistic treatment of the inductive process, which involves the data and 
the noise; 2) it automatically determines the neural network complexity; 
3) it is not necessary to completely determine and fix the noise parame-
ters in advance, rather they are adapted during training, and 4) it does 
not carry out repetitive validations using statistical approaches that are 
time-consuming. These advantages can be achieved following the sparse 
Bayesian learning [Tipping, 2001] approach. It is apphed here for learn-
ing parsimonious and well-generalizing PNN models. 
The evidence procedure however does not tell us how many times 
the training cycle has to be repeated; its convergence properties are not 
established. This is why our attention is directed toward a principled 
approach that ensures convergence while doing probabilistic inference, 
which is the Expectation-Maximization (EM) approach [Dempster et al.. 

Bayesian Inference Techniques 
225 
1977]. It is suitable for offline PNN learning of sparse Bayesian models 
[Quinonero-Candela, 2004]. The EM algorithm is taken to find the max-
imum of the log marginal hkehhood logPr(t|a,/3~^), and thus to infer 
optimal hyperparameters a and /3, The approach seeks to determine 
the mode of logPr(t|a,/?~^) by alternating between two steps E-step 
and M-step, whose successive application guarantees an increase of the 
likelihood at each cycle. 
The idea for dividing the maximal likelihood maximization process 
into two steps is to facilitate separate tuning of the weight parameters, 
and their hyperparameters. The idea is to attempt to directly solve 
the integral: logPr(t|a,/3~-^) = log/Pr(t, w|a,/3~-^)dw, rather than: 
logPr(t|a,/3~i) - log/Pr(t|w,a,/3-^)Pr(w|a,/3-^)dw. Then, the in-
fluence of the weights and the hyperparameters can be distinguished 
after applying Jensens' inequality [Bishop, 1995]: 
log /Pr(t,w|a,/5-^)dw 
- 
log f q{w)^''^^''^}'^:^ 
^dw 
f , ,, 
Pr(t,w|a,/3~^) , 
^, 
, 
> 
log / q{w) log 
^ ' / , 
Uw - Q w 
- 
^ ( 9 , a , r ' ) 
(8.26) 
where the quantity on the second line is Q(w) = /^(w) logg(w)(iw. 
Therefore, a good approximation of the likelihood Pr(t, w|a,/3~^) can 
be obtained by maximizing the lower bound ^(g, a, (5~^) in this inequal-
ity separately: 1) with respect to the arbitrary distribution g(w), called 
expectation E-step, and 2) with respect to the prior a and output noise /3 
hyperparameters, called maximization M-step. When performing the ex-
pectation step arg max^ ^(g, a,/3~^), the hyperparameters a and j3 are 
kept fixed and the maximum of the distribution g(w) is achieved using 
the weight posterior Pr(w|t, a,/^~^). The posterior weights are found by 
least squares fitting, in the case of hnear PNN, and by gradient-descent 
techniques like the BPA, in the case of nonlinear PNN. 
When performing the maximization step: arg max^^^-i ^(g,a,/3~^), 
the weights are fixed and only the first component in ^(g, a, /3~^) (8.26) 
is considered. The expectation of the data likefihood log Pr(t, w|a, (5~^) 
is maximized as follows [Quinonero-Candela, 2004]: 
a,;5 M == arg max (logPr(t, w|a,/3 ^) 
a,/3 
- 
a r g m a x / - / 3 Â£ ; o - Â£ ; i y - i [ c - l o g | A | ] \ 
(8.27) 
where the numerator g(w) is omitted as it does not change at this step, 
and the constant is c = N\ogf3. 

226 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
The maximum is attained by differentiation with respect to the hyper-
parameters. Taking the derivatives of this expectation (log Pr(t, w|a, /?)), 
equating to zero, and solving respectively for a and /? yields correspond-
ing formulae for their updates. The modification rule for the individual 
hyperpriors ai is: 
1.0 
w} -I- fH-i] 
^.28) 
where wi is the network weight afi'ected by this a^. 
Taking the derivative of the log of the evidence criterion with respect 
to /?, leads to the learning rule for the output noise hyperparameter (3: 
N 
/3 = _^, , n^w 
(8-29) 
where 7^ is the local efficient parameter number 7^ = 1.0 â a^ P~^] â¢ â¢ 
The E-step evaluates the likelihood of the complete data using the 
available hyperparameters. After that, this evidence is used in the M-
step to judge as to what degree the belief in the weights parameter 
density is correct with respect to the training data. Next, the hyperpa-
rameters are adjusted with the above formulae (8.28) and (8.29) aiming 
at improvement of the overall model performance. Theoretically, the 
repetitive execution of this procedure converges to a stationary point 
because the second step always increases the quantity !F{q^ cx^p"^)^ thus 
approaching the likelihood of the complete data. When training linear-
in-the-weights PNN, the error function has a unique minimum and the 
expectation step has an exact solution. In nonlinear multilayer PNN, 
however, the solution is approximate, as there are many local as well 
as local minima on the error surface, and it depends how good the lo-
cated weight vector is by the training algorithm. A problem arising 
in multilayer PNN is the need to compute the Hessian, which is often 
computationally unstable and may cause computational difficulties. 
Another possibility for doing sparse Bayesian learning is to directly 
maximize the log marginal logPr(t|a,/5~^), but this approach does not 
guarantee local optimization of the lower bound ^(g,a,/?~^). The hy-
perparameter update formulae for such a strategy are presented in Sec-
tion 8.5 and applied for incremental tuning of the prior and output noise 
hyperparameters while doing recursive PNN training. Although this di-
rect approach may not be strictly optimal, it has been found that it 
sometimes makes faster steps on the search landscape [Tipping, 2001]. 
Table 8.3 presents the EM algorithm for sparse Bayesian learning of 
finear-in-the-weights polynomial networks. 

Bayesian 
Inference 
Techniques 
227 
Table 8.3. Expectation-Maximization algorithm for marginal likelihood maximiza-
tion and learning sparse Bayesian linear PNN models. 
Sparse Bayesian Learning of P N N 
step 
Algorithmic 
sequence 
Data V = {(xn,?/n)}n-i, PNN with W weights 
hyperparamcters: a â 1.0/VK, and /3 = 
1.0/stdy. 
Repeat a predefined number of times: 
a) Perform the expectation E-step 
Estimate the weight posterior mean 
where: A,: = a,;//3, 
0<i<W. 
b) Perform the maximization M-step 
i) Update the prior hyperparameter 
a,:-1.0/(iuf + [ H - 1 ] . . ) , 
where: H"^ = {^^^+ 
L)-\ 
ii) Update the output noise variance 
^ = iV/(e^e + /3^,":,7.:). 
where: 7,; = 1.0 â a,; TH ^] 
until reaching the minimum of the criterion. 
log Pr(y|X, a, (3) - -e'^e - w'^Lw - ^ [c - log |A|] 
1, Initialization 
2. Reevaluate the 
hyperparamcters 
The capacity of the sparse Bayesian learning approach to infer par-
simonious PNN can be examined using plots of the curves generated 
by its polynomial terms. As an example, a time series of 100 points 
were generated using the Hermite polynomial: 1.0 + ((1.0 â /. + 3^^) * 
exp{~f?')), where â5 < /. < 5. Next, random noise of variance 0.01 was 
added. The experiments were conducted using three input variables: 
xt-\^ xt-2) 3;/,_3, second-order polynomial terms: xt-iXt-.2) ^t-\Xt~?>) 
xt-2Xt-ci, 
x;_ 15 ^ t 
.2 
-2' a:^_3, and third-order terms: 
' I2 
- 
-^ 
^ 
i_2^i-3) Xt-\Xt-2) 
^ t - l ^ ^ ~ 3 ' ^ i - 2 ^ i - 3 ) ^ i - l ? ^ i - 2 ' 
Xl_-^Xt-2^ 
Xt^lXt-3, 
and ^i-3' 
Figure 8.3 illustrates with a bold curve the approximated Hermite 
polynomial by an overfitting PNN, along with the curves of the particular 
term components extracted from the network. The PNN was learned 
after doing only 2 iterations. The polynomial contained 9 terms, which 
were used to produce the curves by multiplying these terms with their 
weights. An additional experiment was performed to train the same 
initial PNN using all the above first-order, second-order, and third-order 
terms, but this time 5 iterations of the algorithm were performed. After 
five iterations, a parsimonious PNN of 5 terms remained: xt-i, xt~2^ 
Xt-3^ xt-ixt-s, 
and xt-2xf_r^. The curves produced by multiplying these 
terms by their weights are given in Figure 8.4. 

228 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
Figure 8.3. Overfitting of the Hermite polynomial by a nonparsimonious PNN 
model with 9 (linear and nonlinear) terms identified using sparse Bayesian learning. 
Figure 8.4. Parsimonious PNN having only 5 terms learned from the Hermite 
polynomial data series using sparse Bayesian learning with the EM algorithm. 
The polynomial term curves in Figure 8.4 fluctuate much less which is 
an indication for their general character, so this sparse PNN has greater 
generalization capacity. One can note in these figures that the approxi-
mating potential of both PNN models, learned after 2 and after 5 itera-
tions, are quite similar. That is, both PNN feature good fitting accuracy 
but they have different extrapolation potential. 

Bayesian Inference Techniques 
229 
8.5 
Recursive Bayesian Learning 
In practice, the training data may be given not only as a batch, but 
they may also be provided sequentially. There are recursive Bayesian 
learning approaches that operate on sequentially arriving data [de Freitas 
et al., 2000b, Nikolaev and Tino, 2005a]. Such an approach is elaborated 
here for weight training, stabihzed computation of local regularization 
parameters, and automatic relevance determination of terms. 
The sequential methods overcome the main disadvantages of the batch 
approaches arising from their offline character. The first disadvantage, 
especially of the linear training techniques, is that they rely on analytical 
formulae which sometimes cannot be evaluated safely due to numerical 
computation problems. Second, the reestimation of the hyperparame-
ters after processing all data often yields large values, thus causing in-
stabilities in the algorithm performance [Chen et al., 2004]. Third, such 
batch algorithms need special adjustments of the predictive mean and 
variance in order to be successful in iterative forecasting [Quihonero-
Candela, 2004]. These problems can be avoided by recursive evaluation 
of the weight posterior using one data example at a time. 
A Bayesian version of the Recursive Levenberg-Marquardt (RLM) al-
gorithm [Ljung and Sodestrom, 1983, Ngia and Sjoberg, 2000] is made 
in this section for sequential training of polynomial networks. Starting 
with the complete model of all terms, the algorithm gradually evolves 
the weight posterior Pr(w|Â£),a,/3~^) while cycling over the data, and 
prunes irrelevant terms. The developments include: 1) derivation of a 
regularized equation for sequential estimation of the weights; 2) formu-
lation of a general technique for sequential computation of the inverted 
regularized dynamic Hessian matrix which avoids the problematic direct 
matrix inversion; and 3) implementation of incremental gradient-descent 
training rules for the prior hyperparameters. 
8.5.1 
Sequential Weight Estimation 
The key idea for making a recursive Bayesian RLM is to carry out 
partial sequential regularization while doing weight estimation. The im-
plementation of this idea adopts a technique for augmentation of the 
dynamic Hessian matrix only by the fractions of the total regulariza-
tion. This is necessary because when the data arrive one at a time, the 
total regularization effect has to be distributed in parts after each train-
ing example. This technique is generalized here in a Bayesian setting 
using individual regularization hyperparameters. Performing incremen-
tal regularization impacts the development of both the weight learning 
and the dynamic estimation of the Hessian matrix. 

230 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
Let's adopt the following notation: the weight vector modified up to 
moment (arrival of data point x^) t is w^, the basis vector from the t-
th current data point is (/)(x/,) = [(/)(x^,xi), (/)(x/,,X2),..., 0(x/,,xyv/)]'^, and 
the design matrix includes the relevant M basis functions evaluated with 
all arrived training points x-i that is ^t = {^(xi?^?)} where: 1 < i < t^ 
1 < J < ^^- Let the dynamic Hessian be: Jlf = pWEr){-wt) 
+ A^, and 
the dynamic output error derivatives at this time instant are given by 
the Jacobian: }t = 9P(^)/(9w|w=:wt-i (Section 6.3.1). 
Sequential Bayesian regularization can be accomplished by augment-
ing the dynamic Hessian successively by fractions a^Z of the i-th hyper-
parameter ai,0 < i < M, added one at a time: H/ = Hi_i+/?j/Ji"-fa^;Z. 
The index of the next hyperparameter to add a^, 1 < -i < M, changes 
from iteration to iteration according to the equation i = t mod M. 
The hyperparameter ai is scaled by a special matrix Z which is fixed 
in advance. The matrix Z (of size M x M) contains zeroes everywhere 
except its selecting diagonal element [Z]ii at position i which has a value 
[Z]ii = 1.0/Zi. The denominator Zi is a constant: Zi = N div M if 
t < M ^ {N div M) or otherwise a very large number Zi â l.OelO. 
The recursive weight update rule is derived so as to modify the max-
imum a posteriori weight vector w^ after seeing the next training pair 
(xi,2/i) using the information at the previous step ^ â 1. The following 
recursive weight training rule is obtained [Nikolaev and Tino, 2005a]: 
w, - w,_i -f pU^^itet - a,H,:^Zw,_i 
(8.30) 
where W/,_i is the weight vector at iteration ^ â 1, and a^ has index: i =^ t 
mod M, The degree of impact on the hyperparameter a^ is determined 
by the i-ih non-zero diagonal element of the matrix Z. 
Recursive Bayesian RLM training involves repeated iteration of for-
mula (8.30). The second term in this formula /^H^^j^e/, is actually the 
incremental update of the weight vector. The third term a^H7^Zw/,_i 
accounts for the regularization which is applied partially until the num-
ber of training examples reaches the boundary M ^ {N div M). 
8.5.2 
Sequential Dynamic Hessian Estimation 
The recursive computation of the inverted Hessian matrix H^^ from 
H ^ \ depends on the local hyperparameters. When the dynamic co-
variance matrix is reevaluated, the regularization eff"ect of each local 
hyperparameter is evenly allocated among the recursive steps. Using 
individual prior hyperparameters for the weights is a more general case 
compared to the use of a common single regularizer as in RLM [Ljung 
and Sodestrom, 1983]. This general case enables us to more accurately 
tune the model and enhances its generalization. 

Bayesian Inference Techniques 
231 
Taking into account the individual hyperparameters through partial 
factors a^Z, the sequential modification of the dynamic Hessian matrix: 
Hf = iit-i 
+ Pitif + ^iZ is further reformulated in such a way as to 
facilitate the application of the matrix inversion lemma. The update 
Pitff + cxiL is represented as a product of three matrices that yields the 
exact same modification of the regularized dynamic Hessian matrix by 
one training vector at a time in the following way; 
where j * is the matrix: j * ~ 
H( = 
^â¢- ft 
= 
' 1 
0 
, 0 
ail 
H,- i + /3j/*A,:^jr 
it 
0 ... 
1 
... 
0 
Zi , 
(8.31) 
, and the regulariza-
with hyperparameter index i â i mod 
tion matrix is A^ ^ = 
M. 
Then, applying the matrix inversion lemma: (A + BCD)~^ = A~^ â 
{A-^BDA-^)/{DA-^B-^C-^) 
[Mardia et al, 1979], yields the following 
sequential modification formula for the inverted Hessian: 
Hr' - Hr_\ 
/?H-_\j*jrH,:_\ 
(8.32) 
where A^ is the inverted regularization matrix (A^^ M 
. The scaling 
constant z included in the matrix A^ allows increments to the regular-
ization effect by fractions {ai/{N div M))~^. Such increments continue 
up to iteration t â M ^ {N div M), after which the constant Zi does not 
influence the results as it becomes a very large number. 
Since the quantity in the denominator of equation (8.32) is a two-
dimensional square matrix, it can be inverted without problems. Let's 
adopt the notation: 
S, = /3jfH7_\j* + A, 
511 
512 
521 
522 
^.33) 
Then the inversion can be computed as follows [Mardia et al., 1979]: 
ST' 
C522 
~C5i2 
-C521 
CSu 
where: c = 
1 
511522 - 512S21 
.34) 
which is fast and not sensitive to numerical instabilities. 
It is well-known that such recursive computations depend on the ini-
tial values of the dynamic Hessian matrix. Fortunately, a lot of research 
has been done on this problem in the signal processing literature [Haykin, 
1999], so we know that the initial matrix can be selected with recom-
mended large diagonal elements H^^M 
= l.OeG. 

232 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
8.5.3 
Sequential Hyperparameter Estimation 
Pursuing implementation of a stable recursive Bayesian RLM training 
process needs modification of the hyperparameters after each training 
example, which should be carried out after the adjustment of the weights 
(8.30). Such incremental hyperprior updating will reflect the change in 
the belief in the corresponding weights. Otherwise, it has been found 
that when the hyperpriors are reestimated after training with the whole 
batch of data, the iterative training process may become unstable [Chen 
et al., 2004]. The problem is that, in this case, some hyperparameters 
tend to grow extremely fast to very large values, and their weights cannot 
adapt properly to the data so as to balance the degree of fitting with the 
model complexity. Reliable convergence of the Bayesian RLM learning 
process is achieved by reevaluating the output noise hyperparameter 
with formula (8.14) at the end of each training cycle. 
Stable marginal likehhood maximization can be performed with se-
quential gradient-descent hyperparameter adaptation. 
Implementing 
such a technique requires taking the instantaneous derivatives of the 
marginal log likelihood function C{cx) with respect to the hyperpara-
meters dC/da [Tipping, 2001]. For clarity, let a hyperparameter a at 
time step t be denoted by a^ Then, the following gradient-descent prior 
hyperparameter training rule is obtained [Nikolaev and Tino, 2005a]: 
at - ai_i +7/ (a,_r^ - [H^JJ^^ - wl,) 
(8.35) 
where 77 is a positive learning rate constant. 
The meta-parameter 77 is common for all hyperparameters a^, 1 < 
i < M. Here, H ^ \ 
denotes the i-th diagonal element of the inverted 
dynamic Hessian H ^ \ , and all diagonal elements are available (8.32) as 
calculated for tuning the weights. The initial values ao ^re selected to be 
small numbers so as to allow them to grow up to reasonable magnitudes. 
During iterative training, while doing sequential hyperparameter esti-
mation some hyperpriors will increase while other decrease in magnitude. 
This means that some weights will shrink toward zero and can be re-
moved from the model. Automatic model selection can be achieved after 
each cycle with all training examples by removing those weights whose 
priors exceed the selected threshold. The probabihstic RLM training 
algorithm for PNN is given in Table 8.4. 
The presented recursive Bayesian approach to training PNN has four 
distinctive advantages: 1) it features a numerically reliable computa-
tional performance; 2) it is an incremental probabihstic algorithm with 
rapid convergence due to the use of second-order information from the 
covariance matrix to direct the weight search process; 3) it carries out 

Bayesian 
Inference 
Techniques 
233 
stabilized hyperparameter learning; and 4) it has the capacity to learn 
well from time series. In particular, this method can learn well from 
time-varying data and noisy environments. 
Table 8.4. Algorithm for recursive probabilistic training of PNN models based on 
the regularized Levenberg-Marquardt optimization approach. 
Recursive Bayesian Learning of P N N 
step 
Algorithmic 
sequence 
Data V = {(xt, J//.)}/,... 1, initial WQ^ 
covariance matrix [H~^(0)]7:7: = 1.0e6, aMAX, 
hyperparameters QQ = 1 .Oe~^/{Nw^), 
and (3 â 1.0/stdy. 
Repeat a number of iterations 
a) For each time instant ^, 1 < t < A^, do 
i) Forward propagate: the input x/, and compute 
et = yt - P(xt, w). 
ii) Backward pass: during tree traversal do 
- compute the dynamic derivatives 
jt = dPt/dwkj 
(at the output node) 
jt = [dPt/dpj][dpj/dwji] 
(at the hidden nodes). 
- update the dynamic Hessian matrix 
1. Initialization 
2. Perform 
iterative 
network 
training 
H ; 
H : 
-/3Hr_\j?jrHr_\M 
where: dt = /3j* H^.Jj? + A* 
- update the weights at this network node 
wt = Wf,_i + /3H^~^j/.et - avH^~^Zw/,_i. 
b) Re-estimate the hyperparameters 
i) Incrementally train the priors 
at = a^_i +77 (at-i"""^ - [H^7_\] .. - 
Wt-^). 
ii) Compute the output noise variance 
/3 = (iV - 7 ) / (2e^e), 7= M^ - E.'t^o"* [Â«""'];,â¢ 
c) Prune irrelevant weights Wi whose a,; > aMAx-
The performance of the Bayesian RLM when training PNN is illus-
trated below using a time series of 445 points (1 < t < 445) generated 
using the following equation: y{t) = sin(0.0125^) 4-0.2sin(0.2^), which is 
a sinusoidal wave with added sinusoidal fluctuations on a smaller scale. 
The series was contaminated with random noise of variance 0.01. The 
first 380 points were used for training, and the remaining points were 
used for testing. Figure 8.5 shows the prediction of this series using a 
linear PNN trained by the regularized RLM. The curvature of the PNN 
forecast is very close the curvature of the generated series. 

234 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
Figure 8.5. Approximation of the first 380 data points and multistep (50 steps) 
afiead forecasting of the time series with varying mean by the Bayesian RLM. 
8.6 
Monte Carlo Training 
The PNN learning problem viewed from a Bayesian perspective is 
to find the weight posterior: Pr(w|y,X, a,/3~^), which involves evalu-
ation of the usually intractable integral of the joint data distribution 
/Pr(t|w,X, a,^~"^) Pr(w|a)(iw. The intractability of this integral can 
be alleviated using either approaches that exploit a Gaussian approxima-
tion to the weight posterior [MacKay, 1992a], or Markov Chain Monte 
Carlo sampling [Neal, 1996]. Although using the Gaussian approxima-
tion to the weight density allows easy implementations, as demonstrated 
in the previous subsections of this chapter, the true weight density in 
the general case may not be normal. If the assumption for normahty is 
violated this approach may lead to poor results. That is why researchers 
have proposed to realize the intractable integrals by summations accord-
ing to Monte Carlo sampling theory. Sampling methods help to deal 
with high-dimensional integrals, but it should be noted that they take 
a long time to reach accurate enough solutions. Monte Carlo methods 
for Bayesian training in relaxed circumstances, Hke non-Gaussianity and 
non-stationarity, are currently under investigation [Doucet et al., 2001]. 
This section presents a hybrid approach to Monte Carlo simulation 
of the summary statistics: / = {'^/N) J2n^i ^(w) + f, for approximat-
ing the integral / ^ (F(w)|t} = / P ( w ) Pr(w|t)dw with independently 
sampled weight vectors w, where v is the sample variance defined as: 
V = (l/Ar)y'(P(w)2) - (P(w))2 with (P(w)> = (l/N) ELI Pi^)- The 

Bayesian Inference Techniques 
235 
simulation allows us to obtain empirically the mean output and also the 
weight posterior mean Pr(w|t). The empirical estimates are produced 
by averaging over a population of weight vectors. The averaging is per-
formed using importance ratios associated with each population member. 
The importance ratios play a major role in this approach as they help 
to achieve asymptotic convergence of the learning process. The popula-
tion is continuously modified by making samphng and resamphng steps, 
and such steps are alternated many times to achieve sufl[icient accuracy. 
Each candidate weight vector from the population is sampled by ran-
dom perturbation using the chosen prior distribution. The resampling 
of promising weight vectors is based on their importances. 
In order to further improve the accuracy of the method, so as to 
avoid wandering on the search landscape, the sampled weights can ad-
ditionally be optimized [de Freitas et al., 2000b] with a suitable algo-
rithm. Such a hybrid sampling importance resampling technique using 
gradient-descent training is demonstrated below. 
8.6.1 
Markov Chain Monte Carlo 
The Monte Carlo technique can be applied for neural network train-
ing by sampling weights from their predefined prior distribution, next 
evaluating the likelihood, and finally computing the importance ratios. 
One strategy is to generate a Markov Chain of sampled weight vectors 
such that in the limit, when a large number of samples are drawn, it ap-
proaches their unknown probability distribution. Independent from the 
choice of the initial sample, if the sampling is possible and performed 
properly, the Markov Chain tends to a fixed approximation. Such an ap-
proximation is probabilistic in the sense that its components occur with 
a transition probability (that is not with certainty), and depend only 
on the recent components from the previous discrete time step. This 
inspires us to use the Markov Chain Monte Carlo (MCMC) strategy for 
implementing Bayesian learning algorithms for PNN. 
Specialized MCMC algorithms for training neural networks have been 
made using importance sampling [de Freitas et al., 2000b]. According 
to this method, an importance, also called proposal, distribution 7r(w|t) 
is selected which can be easily sampled and with which integration by 
means of summation can be implemented. Then an approximation to 
the integral of the joint data distribution can be computed simply by 
summation over the ratio: Pr(w|t)/7r(w|t), with respect to the chosen 
proposal distribution. In this way, the difficulty of drawing samples 
directly from the posterior distribution is alleviated by drawing samples 
from the proposal distribution instead. 

236 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
This allows us to approximate the integral as follows: 
/ Â« (P(w)|t> = | p ( w ) ^ j ^ 7 r ( w | t ) d w 
(8.36) 
using summation instead of attempting to solve it directly. 
The MCMC training algorithm begins with random generation of a 
set of initial weight vectors from their selected prior. The training pro-
ceeds by perturbing these weights with uniformly drawn noise from the 
proposal distribution; that is, corrections are made in the search direc-
tions using the proposal density. The perturbed weights are considered 
in the context of the adopted PNN model to produce outputs by prop-
agating the inputs through the network in a feed-forward manner. In 
the limit with the increase in the number of sampled weight vectors, the 
expectation of the data can be approximated by the following sum: 
/ = ^-^=1^ K-sHK-^s) 
^3 3^^ 
Ef=l9(w,,) 
where g is a substituion for the normahzed importance ratios: 
. PlfiMf^ 
,3 3 
7r(w|tj 
which contains the likehhood Pr(t|w) and the weight prior Pr(w). 
The approximate Monte Carlo estimate of the weight posterior distri-
bution Pr(w|t) in this setting can be obtained by summation: 
where 6 is the Dirac delta function. 
The accuracy of this technique depends on whether using the pro-
posal distribution 7r(w|t) weights w that are representative enough for 
the distribution Pr(w|t) can be drawn, or whether samphng from such 
regions in which the probabihty Pr(w|t) is large can be accomplished. 
This is the motivation to model the proposal distribution by the tran-
sition distribution of a Markov Chain 7r(w/c|w/j;_i,t/(;) == Pr(w/.|w/^_i). 
Such a simulation of a Markov process guarantees asymptotic conver-
gence to the target posterior density. The Markov Chain is a sequence 
in which each sample is produced from the previous one by a stochas-
tic update drawn independently according to the transition function: 
w;. = w/c-1 + ^5 where: e E 7r(w|t). The transition distribution func-
tion is typically chosen proportional to the variance of successive weight 
vectors, that is to the difference between the most recent vectors. 

Bayesian Inference Techniques 
237 
Taking these details into consideration makes the above formula for 
the importance ratios (8.38) sequentially computable as follows: 
, 
, 
, 
Pr(t/c|w;, )Pr(w/,,.,|w/,_i,,) 
= 
gfc-i(wfc,.)Pr(tfc|w^^J 
(8.40) 
where Pr(t/c,.s|w;. J ^ exp {~PEjj{wk^>i)), Here 5 ranges over the weight 
vectors and k enumerates the iterative steps of the algorithm. 
8.6.2 
Importance Resampling 
The approximation quality of this approach strongly depends on the 
choice of the proposal density. It is suggested here to model it with 
the transition distribution of the Markov Chain, but other choices are 
also possible. There are two main requirements for the proposal distri-
bution: 1) it should make drawing of random samples computationally 
easy; and 2) it should be as close as possible to the unknown posterior. 
Only when 7r(w|t) is similar enough to Pr(w|t), good results can be 
expected. A proposal distribution function which almost attains equiv-
alence: Pr(w|t)/7r(w|t) ?^ 1 is often feasible. 
The analysis of this importance ratio leads to the idea of resampling. 
After the weights have been sampled with the chosen probability, they 
are resampled to retain the most promising among them. Since the 
population vectors determine the search directions, this means that the 
resampling contributes to focusing the search into plausible areas on 
the error landscape. Importance resampling is performed by selecting 
weights with high importance ratios and discarding weights with low 
importance ratios. This efficient algorithm is called MCMC with Sam-
pHng Importance Resampling (SIR) [de Freitas et al., 2000b]. 
8.6.3 
Hybrid Sampling Resampling 
In practice the Sampling Importance Resampling technique may take 
a very large number of iterations to converge to a solution of acceptable 
accuracy. For this reason the MCMC algorithm may additionally be 
focused so as to push it faster on the search landscape toward promis-
ing regions for exploration instead of leaving it to wander, which is a 
promising search direction. This can be achieved by using a proper 
weight training algorithm applied to each weight vector after random 
sampling [de Freitas et al., 2000b]. Such a Hybrid Sampling Importance 
Resampling technique is specialized for PNN using the RLM algorithm 
(Section 8.5) for weight focusing in Table 8.5. 

238 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
Table 8.5. A Bayesian Markov Chain Monte Carlo algorithm for PNN based on 
Hybrid Sampling Importance Resampling with RLM optimization. 
M C M C Training of P N N 
step 
Algorithmic 
sequence 
Data V = {(xi.,yt)}ili, PNN architecture, 
sample size 1 < s < 5, weight variance a,; = cr^^, 
output noise variance /5 = 1.0/stdy, 
threshold Z 
proposal distribution 7r(w|t) = A/'(0, cr^). 
Sample a weight vector wo from the prior 7r(w|t) 
Repeat a number of iterations k, I < k < K 
a) Sample a weight vector from the prior 
Wk+^,s = WA;,.S + 6, where: e G 7r(w|t). 
b) Optimize Wk+^,s according to the gradient 
by executing one RLM cycle. 
c) Compute the unnormalized importances 
a = yi - P(x,;,Wfc + i,.s) 
qk+}{wk+},,) = gfc(wA^4-i.s)exp (-0.5/3X]^^;e?) 
and next normalize them 
g^+i(wfc+i,,) = gfc+i(w/,+i,,)/J]f_^ g/,+i(wfc+i,,). 
d) Resamplc promising weights among these 
with 1.0/x:!^ (gUi(w^+]..o)' < z 
by selecting weights with high importance ratios 
and assign importances qk+i('Wk+i,s) = l.O/S. 
Compute the average weight vector 
^ ~ I].Ui <?'(ws)(5(w - w.s). 
1. Initialization 
2. Sample weights 
3. Perform 
sampling 
importance 
resampling 
5. Estimate the 
mean weight 
The behavior of the MCMC algorithm from Table 8.5 was applied 
to train a PNN using the same Hermite polynomial from Section 8.4: 
1.0 -f- ((1.0 â t + 3/.^) * exp(ât^)), where the argument t is taken from 
the interval: â5 < ^ < 5. One hundred points were generated and then 
random noise of variance 0.01 was added. In order to produce meaningful 
and illustrative results the PNN was trained for five iterations, and the 
remaining parsimonious PNN of 5 terms: a:/,_i, xt~2^ ^t--i^ ^t-i^t-^^ 
and 
xt-2x'f_^ was taken. After that, the Hybrid SIR was run on this network 
and the weights were perturbed assuming proposal distribution with 
weight variance 0.01. The population size was 50 for fast computation. 
During Hybrid SIR for selection of promising weight vectors, among 
those generated by random perturbation, 16 samples were taken out of 
all 50 in the population. Among these 16 weight vectors, half of them 
were from those discarded by Hybrid SIR and the other half from those 
kept to the end of the algorithm. 

Bayesian 
Inference 
Techniques 
239 
3 . 0 
Figure 8.6. Markov Chain Monte Carlo training of a PNN using the Sarnphng 
Importance Resamphng technique and the Hermite polynomial data. 
Figure 8.6 illustrates the 16 polynomials generated by evaluation of 
the PNN with these sampled weight vectors. The dashed curves are the 
polynomials produced by the weight vectors that were discarded, while 
the straight line curves are the functions retained in the population. 
The bold curve is the average of the 8 good polynomials made from the 
promising weight vectors. The resulted bold curve is the empirically 
generated PNN approximation to the Hermite polynomial. 
There are two design issues that arise when implementing this hybrid 
MCMC training algorithm: 1) how to select initial starting values; and 
2) how many runs to perform. The second issue depends on the possi-
bility to find out whether the chain has converged sufficiently close to 
an accurate solution. This criterion can be measured by conducting a 
number of runs, starting from distant initial vectors, and evaluating the 
inter sequence and between sequences variances. If they are close, then 
it can be assumed that the algorithm converges [Nabney, 2002]. 
8.7 
Chapter Summary 
This section presented Bayesian techniques for learning with polyno-
mial networks. PNN algorithms that implement the two main Bayesian 
approaches were demonstrated: the evidence approach [MacKay, 1992a], 
and the sampling approach [Neal, 1996]. The evidence approach suggests 
to approximate the integration over the hyperparameters in order to de-
termine the values that maximize their joint probability. These hyperpa-

240 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
rameters are taken as fixed to carry out weight posterior maximization. 
Although this evidence approach is not exact, it has been found that it 
produces superior results than those generated by integration over the 
hyperparameters [Buntine and Weigend, 1991b]. 
An essential advantage of the Bayesian regularization and pruning 
techniques developed according to the evidence framework is that they 
are general and applicable to polynomial networks with various activa-
tion functions; that is, these techniques can be considered to improve 
the generalization of all of the presented models in Section 3. These 
Bayesian techniques provide a rehable means to tune further the best 
discovered models for example by IGP, in order to achieve parsimonious 
models. In addition to this, the Bayesian error function can be used 
as a fitness function for guiding the evolutionary search process toward 
better polynomial network structures, as it is objective and utilizes all 
the data without the need to split them artificially. 
There are several distinctive features of this chapter: 1) it ofi'ers an ap-
proach to Sparse Bayesian Learning with proper treatment of the weight 
variances and output variances during training, which is implemented us-
ing the reliable EM algorithm; 2) it provides an innovative algorithm for 
Recursive Bayesian training which is a robust tool for sequential learning 
from time series; 3) it offers a Markov Chain Monte Carlo algorithm for 
weights training based on a hybrid sampfing-resampling technique. 

Chapter 9 
STATISTICAL MODEL DIAGNOSTICS 
The goal of polynomial identification is to discover faithful models of 
the given data. That is why it is important to quantify the reliability of 
the learned models so as to obtain evidence of their usefulness. There are 
two main factors that cause deviations in PNN: the data uncertainty and 
the model uncertainty. Central notions for the analysis of such learning 
problems are the standard error, the total error, and the confidence and 
prediction intervals. The total error reflects the model accuracy over the 
entire data distribution (not only over the training data sample), and 
allows us to make decisions about the correctness of the models. Both 
the data and the model discrepancies can be examined by estimating 
the intervals which with high degree of belief contain the model. These 
are the confidence and prediction intervals, also called error bars. 
This chapter investigates the factors that impact the generation of 
faithful PNN models and provides means for their statistical diagnosis. 
The standard error is traditionally estimated by data resampling and 
model reevaluation. The direct estimation of the total error is diffi-
cult practically but mitigated by decomposing it into bias and variance 
components. The integrated bias and variance components of the total 
error are evaluated separately by a resampling technique. There are two 
groups of approaches to measuring error bars for neural network models: 
analytical and empirical. Confidence intervals can be obtained by the 
analytical delta method and the empirical bootstrapping method. Pre-
diction intervals can be estimated by an analytical method for nonlinear 
models, and empirically by extending the network architecture to learn 
the output error variance as a function of the inputs. Another possibihty 
to assess the model reliability is to compute Bayesian intervals, also by 
corresponding analytical and empirical methods. 

242 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
The analytical methods for error bar estimation rely on several as-
sumptions. First, the given training data set is of fixed size. Second, 
the data are contaminated by noise e which is independent, normally 
distributed with zero mean and variance ay^ thus the targets are noisy 
samples: y = P(x,w) + e, where P(x, w) is the PNN model. Third, 
the training cost function is continuous and it allows us to calculate its 
first-order and second-order error derivatives. Fourth, the network has 
been trained until convergence to a global minimum of the error surface. 
There are no clear criteria that tell how to satisfy the fourth assumption 
when doing network training. However, the PNN evolved by IGP and 
further trained by BP and Bayesian techniques may be envisioned as 
nearly optimal (virtually the best solution of the task), and hence it can 
be assumed that the tests are statistically valid. The empirical methods 
are less demanding; they usually consider that the network structure 
and weights are sufficiently good models of the data. 
The statistical diagnosis tools have to be applied in order to exam-
ine the inferred PNN models and to judge whether successful induction 
from the given data has been performed. The presented plots in this 
chapter demonstrate that different approaches produce different error 
bands. Which of them will be used as most plausible depends on the 
current task and the desired accuracy from the inferred model. Finally, 
we present statistical validation tests for detecting residual correlations 
suitable for PNN models. 
9.1 
Deviations of P N N Models 
There are many reasons in practice that hinder the inductive learning 
process and cause deviations of the model from the true regression func-
tion P(x) â E[y\'x\, When inferring nonlinear PNN models from data, 
the main sources for difference from the unknown true regressor are: 
a) uncertainties in the data, and b) uncertainties in the model, which 
concern the model complexity and weight accuracy. 
Impacts from the Data, First, there are inherent inaccuracies in the 
data, like noise, omitted data, outliers, and measurement errors, which 
influence the model identification. Second, it depends from which region 
of the true function the given data sample has been taken, because the 
true data may have different densities in different regions and they may 
not be sufficiently represented in the provided training sample. The 
learning algorithms are unfortunately sensitive to the sampling varia-
tions of the data. An effect from the data sample when flexible models 
are learned is that they have high variance, while if the learned models 
are restricted they tend to exhibit less variance. 

Statistical Model Diagnostics 
243 
Impacts from the Model 
First, the relevant input variables in the 
model, their number, and which they should be, may not have been se-
lected properly. Second, the number of terms and the maximal order 
(degree) of the polynomial model may not have been determined cor-
rectly. Such model complexity problems may arise due to inefficient evo-
lutionary search by the mechanisms of IGP as well as improper Bayesian 
tuning. Third, the pruning techniques may introduce model misspecifi-
cation due to overpruning or underpruning of the network. Overall, the 
imposed particular functional form of the model to be learned introduces 
another bias component to the total error. 
Even if the model complexity is relatively good, the weights may have 
not been learned accurately. This happens because often the multilayer 
network training process may be unable to converge to a good weight 
vector. There are many inferior local optima on the error surface, and 
a lot of them usually do not possess the desired fitting characteristics. 
This is why the presented inductive learning methodology in this book 
recommends to use additional retraining of the best evolved models by 
IGP using backpropagation techniques or Bayesian training techniques, 
or both, for attaining optimal results. 
9.2 
Residual Bootstrap Sampling 
There are two main groups of methods for statistical model diagnosis: 
sampling methods and splitting methods. The sampfing methods for 
error evaluation are advantageous over splitting methods as they are less 
sensitive to the specific data divisions and often produce more reliable 
results. Especially suitable for regression tasks is the residual bootstrap 
sampling method [Tibshirani, 1996]. It prescribes to create artificial data 
distributions by randomly adding the errors of the model to the given 
data. Such sampling of noise is performed repeatedly for a reasonable 
number of times and the studied model is estimated. This approach 
is usually applied for measuring the standard error and helps to get a 
statistical picture of the model uncertainty. 
Residual bootstrap sampling suggests generating a number B of dif-
ferent models P(x,v^^) from a particular studied one P(x, w) by esti-
mating it with repficates of the given data sample D â {(y^n->yn)}n^\' 
Every model P(x, w^), 1 < 6 < 5, is made by reestimating the studied 
P(x, w) with its output deliberately contaminated by a randomly drawn 
residual error: F(xn, w^) = P(xn, w) + e^, 1 < n < A^, where e^ is the 
?T--th residual from the 6-th error series: e â [6^,62,..., Cyy L 1 < 6 < ^. 
-\i ^2' â¢â¢â¢' ^N 
In order to derive the diff'erent models P(x,w ), an error series e is 
produced by resampling the residuals [61,62, -"I^N] independently from 
the studied model: e^ â Vn ~ -^(^n^w). The standard error at a data 

244 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
point X may be computed using residual bootstrap sampling as follows: 
se5oot(^(x,w)) = J - ^ f ^ 
(p(x,w'')- P (x,w))' 
(9.1) 
where P (x, w) is the average from the reevaluated B models using the 
resampled data: P (x, w) = (1/(5 - 1)) EfeLi ^(x, w^). 
Although the residual bootstrap is a computationally intensive proce-
dure, it is useful not only for measuring the standard error, but also for 
evaluating the bias and variance components of the total error, as well 
as for finding confidence intervals. 
9.3 
The Bias/Variance Dilemma 
9.3.1 
Statistical Bias and Variance 
The model variation in performance over the training and over the 
unseen data can be investigated with the total error, also called gen-
erahzation error. It evaluates the capacity of the model to generalize 
beyond the provided data set. When learning from a fixed data set it is 
not enough to use only the residual error as inductive criterion in pur-
suit of generalization. This is because real data are often not completely 
reliable due to noise, omitted data, and other defects. There is a need 
to obtain evidence for the total error over the entire set of training and 
unseen testing data as well. 
The total squared error (P(x)âÂ£'[2/|x])^ explains how well the solution 
P(x) approximates the true regression function P(x) â E[y\x\. 
Such 
evidence can be acquired by decomposing the total error in two separate 
components [Geman et al., 1992]: 
Eu[{P{^)~E\y\^]f] 
= {Eo\P{-K)]-E[,M]f 
+ 
Eo[{P{^)-ED\P{^)]?] 
(9.2) 
which are called statistical bias BIAS'^{P{^), 
and statistical variance 
VAR{P<(x)): 
BIAS\P{^)) 
= 
{ED[P{^)]-E\y\^]f 
(9.3) 
VAR{P{->^)) = Eo[iP{^) - Eo[P{x)]f] 
(9.4) 
where the index D means with respect to the available training data 
D = 
{ixn,yu)}^=^. 
The total error shows how well the model agrees with a randomly 
drawn data sample from the true function distribution. When the sample 
size increases to infinity, the total error diminishes to zero. Since in 
practice we are always given finite data samples, the measurements of 
the total error are approximate. 

Statistical Model Diagnostics 
245 
The statistical bias is an error estimate that shows to what degree 
the model structure is adequate to the data, in the sense of whether 
it is complex enough to be a close approximation to the true function. 
For example, the statistical bias may reveal whether the selected repre-
sentation and learning algorithm have adjusted the model structure to 
the data well. When the bias is relaxed the accuracy of interpolation 
improves but the extrapolation of unseen data becomes worse. This is 
because a very low bias means that the model is too sophisticated and 
overfits the specificities of the data. At the other extreme, a very high 
bias indicates that the model is not sufficiently complex to describe the 
regularities in the data. The model components that affect the bias are 
the selected inputs, the maximal order (degree) of the polynomial model, 
and the network architecture. Thus by changing these components the 
bias can be modified so as to impact on the level of generalization. These 
effects can be examined with measurements of the fitting accuracy. 
The statistical variance is an error estimate that accounts mainly for 
the generalization capacity of the model, that is whether the model 
fits the data without regard to the particularities of the provided data 
sample. The statistical variance shows the model sensitivity to training 
with different samples of the same underlying function. A large variance 
indicates that the model does not capture well the general properties of 
the unknown data source. Such data properties that affect learning are: 
the noise in the data, omitted data, outliers, and measurement errors. 
The learning algorithms are equipped with mechanisms for control over 
the variance aiming at reduction of the model deviation from one data 
sample to another. In order to increase the predictabihty the variance 
should be reduced, but then the fitting accuracy becomes worse and the 
results may not be satisfactory. This is why the bias and variance should 
be properly balanced in order to achieve identification of optimal models 
with very low total error. 
The practical question is hbw to calculate the statistical bias and 
variance with the available knowledge for the model and the data. 
9.3.2 
Measuring Bias and Variance 
Research into the statistical characteristics of neural networks shows 
that one can measure the integrated bias and variance [Geman et al., 
1992], which are estimates of the statistical bias and the statistical vari-
ance. While the statistical bias and variance cannot be directly com-
puted, as the true function that generates the data is usually not known 
in practice, the integrated bias and variance can be evaluated with con-
crete formulae by performing data resampling. Resampling is necessary 
to obtain information for the possible deviation of the data; that is, to 

246 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
distinguish the essential characteristics of the studied model from the 
specificities of the given training sample. 
The integrated bias can be calculated using the provided training sam-
ple, using the following formula: 
n=l 
where P (xn,w) is the average from the generated B polynomials by 
the selected sampling method: P (xn,w) â (1/^) X]^i i^(xn, w^), es-
timated with the n-th data vector x^. The same average P (x7^,w) is 
also necessary to compute estimates of the integrated variance. 
The integrated variance can be calculated as follows: 
^ ^ ^ = ^ E 
( 4 E (^(^n, w")- P (xâ, w ) ) ' ) 
(9.6) 
where h enumerates the sampled polynomials P(xyi,w^),...,P(xn, w^). 
It should be clarified that the above formulae for measuring the in-
tegrated bias (9.5) and the integrated variance (9.6) can be used with 
diff'erent resamphng methods. The residual bootstrap sampling, given 
in subsection 9.2, is recommended for addressing real-world regression 
and time series modeling tasks. 
Figures 9.1 and 9.2. provide separate plots of the integrated 
BIAS^ 
(9.5) and VAR (9.6) components of the total error, made as the aver-
age of several PNN, estimated using the residual bootstrap resamphng 
method using a benchmark time series. The IGP was run using the 
Mackey-Glass series [Mackey and Glass, 1977] and the best evolved PNN 
model was taken without further improvement, by retraining it. Such 
runs were repeated 50 times to diminish efi'ects from resamphng. Next 
an averaged solution from the generated best 50 polynomials was made. 
The integrated BIAS'^ and VAR of the averaged solution was computed 
as a function of the regularization parameter A. The impact of the regu-
larization parameter A on the model accuracy was changed by increasing 
its values from 0.0001 to 0.09 by 0.0005. 
The curves in Figures 9.1 and 9.2 demonstrate that using larger values 
of the regularizer A makes smoother polynomials. As a result of this, 
the bias becomes stronger while the variance diminishes. The overall 
eff'ect as A is increased is initially a decrease in total error followed by 
an increase in error after A passes a certain optimal value. The most 
appropriate regularization value is the one with which the total error 
reaches its minimum. 

Stat/istAcal Model 
DiagnosUcs 
247 
0 . 0 0 5 
0 . 0 1 0 
0 . 0 1 5 
R e g u 1 a r i z a t i o n 
p a r a m e t e r 
X 
Figure 9.1. Integrated BIAS^ 
of fitting the Mackey-Glass series by a PNN evolved 
using IGP, averaged over 50 runs to diminish effects from randomness due to 
resampUng. Each curve point is produced after residual bootstrapping of 100 PNN. 
0 . 0 0 5 
0 . 0 1 0 
0 . 0 1 5 
R e g u 1 a r i z a t i o n 
p a r a m e t e r 
X 
Figure 9.2. Integrated VARisnice of fitting the Mackey-Glass series by a PNN 
evolved using IGP, averaged over 50 runs to diminish effects from randomness due 
to resampling. Each point is produced after residual bootstrapping of 100 PNN. 

248 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
9.4 
Confidence Intervals 
When inferring models from experimental data it is important to 
quantify the belief in them in order to become certain of their useful-
ness. Confidence intervals, also called error bars, are statistical means 
for evaluating the uncertainty of models learned from concrete data. 
They reveal the expected fluctuation of the standard error. 
The reliability of a model, here a PNN, is defined by the probability 
with which this model contains the true regression function. In statisti-
cal parlance the task of measuring the model uncertainty is to find the 
confidence interval in which one has (1 â a)% (i.e. 95%) befief that ran-
domly drawn data will be described correctly by the model. Confidence 
interval for a population mean like the regressor function P(x) = i5'[?/|x] 
consists of two limits: from above and from below, which with a certain 
probability contains this value P(x). 
Confidence intervals for multilayer PNN models, like the error bars 
proposed for neural networks [Hwang and Ding, 1997, De Veaux et al., 
1998, Rivals and Personnaz, 2000, Tibshirani, 1996], can be estimated 
with two diff'erent analytical and empirical approaches. The analytical 
approaches are inherited from the theory of nonlinear regression models. 
Such an analytical approach can be implemented using the delta method 
[Efron and Tibshirani, 1989]. The delta method requires the first-order 
and second-order error derivatives with respect to the weights. There are 
two cases to consider: estimating confidence intervals in linear PNN and 
in nonlinear PNN. While the Hessian in linear PNN is available from 
the weight learning process, the Hessian in nonlinear PNN has to be 
calculated additionally using 7^-propagation. The Hessian contains the 
second-order error derivatives, and in the case of neural networks it plays 
the same role as the covariance matrix in traditional nonlinear models. 
A popular empirical approach to constructing confidence intervals is the 
one based on the residual bootstrap method [Tibshirani, 1996]. 
9.4.1 
Interval Estimation by the Delta Method 
The delta method [Efron and Tibshirani, 1989] provides an estimate of 
the standard error through the maximum likehhood theory. It suggests 
to measure the standard error of the model as follows: 
^^delia (P(x,w)) = ^a2gTH-ig 
(9.7) 
where a^ is the variance of the noise e, g is the gradient vector (6.36) 
with the first-order error derivatives with respect to the weights, and H is 
the Hessian matrix (6.39) with the second-order derivatives of the error 
function with respect to the weights. All first-order and second-order 

Statistical Model Diagnostics 
249 
error derivatives are evaluated at the learned weight vector. Strictly 
speaking, the above formula (9.7) assumes that the PNN model is nearly 
the best solution from a global perspective. 
The standard error in (9.7) may be approximated using the traditional 
average mean squared error: (J^ = ( V ( ^ ~ T)) Y^n-^iiVn ~ ^(^n, w))^, 
where A^ is the number of the training data and 7 is the effective num-
ber of network parameters (which is typically less than the number of 
all neural network weights). A practical concern is to take into account 
the weight decay regularization, which is usually applied when train-
ing polynomial networks with the intention of achieving stable numeri-
cal performance and good generalization. The impact of regularization 
should be included by adding the individual regularization factors A^ to 
the diagonal elements of the Hessian before doing inversion. 
The empirical confidence interval of a PNN model according to the 
Delta method can be obtained using the formula: 
P(x, w) Â± z.02,aly/g'r{H + Lr'g 
(9.8) 
where 2;.o25 is the critical value of the normal distribution, and L is the 
diagonal matrix with the local regularizers Xi, I < i < W. 
Formula (9.8) for evaluating the confidence intervals of linear mod-
els directly takes the inverted Hessian matrix: H~^ = {^^^ 
-f L ) ~ \ 
which is available after learning the network weights by least squares fit-
ting, which saves computation time and makes the implementation easy. 
When trying to evaluate confidence intervals in nonlinear PNN models 
formula (9.8) can be implemented in two diff'erent ways: 1) using the 
full Hessian (6.39, 6.40), and 2) using a diagonal approximation of the 
Hessian with elements [H]-. = 
{dP(pc^w)/dwi){dP{x^w)/dwj). 
The second version for measuring confidence intervals using the diag-
onal approximation of the Hessian matrix can be derived using a linear 
Taylor expansion of the nonlinear model output [Rivals and Personnaz, 
2000]. This is a simplified version which leads to less accurate intervals 
because using only a diagonal approximation of the Hessian means that 
some precision contributed by the off'-diagonal matrix elements is lost. 
Since there are very precise techniques available for evaluating the full 
Hessian matrix, e.g. the 7^-propagation algorithm [Pearlmutter, 1994] 
for example, these should be preferred as they quantify the statistical 
significance of the models more accurately. 
Error bars of PNN can be estimated using the backpropagation al-
gorithm (Section 6.2) to compute the gradient vector g, and the 7Z-
propagation algorithm (Section 6.3.1) to compute the Hessian matrix 
H. The application of formula (9.8) has to be adjusted in practice be-
cause PNN are identified as models of the normalized series. 

250 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
PNNs are usually trained over the normalized data; they do not di-
rectly describe the given data at their original magnitude. In order to 
produce error bounds at their original magnitude, formula (9.8) should 
be modified to reflect the fact that the input and output data are nor-
mahzed. More precisely, realistic confidence intervals can be obtained by 
multiplying the square root by z^Q25^*ystd^ where a* is the mean squared 
error of the normalized model, and ystd is the standard deviation of the 
original outputs y [Rivals and Personnaz, 2000]. 
The confidence intervals measured with PNN learned from normalized 
data can be restored to their original magnitude in the following way: 
P(x, w) Â± z.025a*y,,wVg^(H + L p g 
(9.9) 
where the Hessian H and the gradient g are calculated with the nor-
mahzed data, and the standard deviation of the original outputs is: 
Vsu = v'(l/(iV-7))E^=i(2/n-2/)2. 
As an example, these algorithms were used to calculate the confidence 
intervals of the best PNN evolved on the benchmark Mackey-Glass series 
[Mackey and Glass, 1977]. Figure 9.3 illustrates the confidence intervals 
produced by the delta method according to formula (9.9) using the full 
Hessian matrix. Figure 9.4 displays the confidence intervals derived by 
the delta method again according to formula (9.9) but using the diagonal 
approximation of the Hessian. Both figures 9.3 and 9.4 show clearly that 
the two implementations of the delta method lead to slightly different 
error bars. One is inclined to think that using the diagonal elements 
of the Hessian matrix without taking into account the off'-diagonal el-
ements seem to give overly optimistic evidence for the possible error 
deviations. Confidence intervals generated by this method with the use 
of the full Hessian matrix seem to be more reafistic. Similar observa-
tions are made after bootstrapping of confidence intervals of the same 
polynomial network model shown later in the following Figure 9.5. 
It should be clarified that the plots in Figures 9.3 and 9.4 have been 
produced by evaluating the PNN model with the Mackey-Glass data 
series prenormafized into the interval [0,1]. The IGP system, as well 
as the neural network algorithm, learn and train PNN models using 
normalized input data which helps to avoid numerical problems arising 
when computing the weights and when attempting to invert the Hessian 
matrix. That is, the PNN model is inferred in the normahzed space, not 
in the space of the original data magnitude. 
The algorithm for finding the confidence intervals of PNN models 
according to the modified delta method is summarized in Table 9.1. 

StatAstAcal Model 
Diagnostics 
251 
S 
0 . 5 
S e r i e s 
P o i n t 
Figure 9.3. Confidence intervals of PNN made with the Mackey-Glass series 
according to the Delta method using the full Hessian matrix (A = 0.001). 
UpperLimi t 
PNN 
LowerLim i t 
. 
I 
60 
90 
Series Point 
Figure 9.4. Confidence intervals of PNN made with the Mackey-Glass series 
according to the Delta method using a diagonal Hessian approximation (A = 0.001) 

252 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
Table 9.1. Algorithm for estimating the analytical confidence intervals of PNN. 
Estimating Confidence Intervals of P N N using the Delta Method 
step 
Algorithmic 
sequence 
1. Initialization 
2. Calculate the 
gradient vector 
3. Calculate the 
Hessian matrix 
4. Estimate the 
confidence 
intervals 
Normalized model P*(x*,w^) , 
normulized 
training data: D = {(x* , y*)}^^^ 
L regularization matrix, effective parameters 7 
-2^.025 critical value of the normal distribution. 
Evaluate the elements of the gradient 
with each input data vector x*, 1 < n < A'^ 
Qi = dE{x'!^,w)/dwi,gi 
G g, 1 < z < VK. 
Evaluate the elements of the Hessian: 
with each input data vector x*, 1 < n < A^ 
[H] .^. - d'^E*{x''r,,,w)/dwidwj, 
1 < i,j < W. 
a) Evaluate the normalized error a*^ 
between the normalized targets y* 
and the normalized output P*(xJ^,,w) 
a-' = (l/(iV - 7)) E l , (J/- - -P'W. w))'. 
b) Evaluate standard deviation ystd of the targets 
c) Compute the confidence intervals 
P(x, w) Â± ^.025^*2/.s/,dV'g'^(H + L)-^g. 
9.4.2 
Bootstrapping Confidence Intervals 
Residual bootstrap sampling [Tibshirani, 1996] is a method that can 
be used to find empirical confidence intervals of nonfinear models. The 
confidence interval of a PNN estimated by residual bootstrapping is: 
F ( X , W) Â± ^.025[/3]5e6oo/X^(x. W)) 
(9.10) 
where se^oo/X-^(^5^)) ^^ ^he bootstrapped standard error (9.1) of the 
network model P(x, w), and ^,025[Bl i^ ^^^ critical value of the Student's 
^-distribution with B degrees of freedom. 
The residual bootstrap is a model-based approach which requires a 
good enough model with a proper structure; that is, a model whose 
complexity is relevant to the data. Since the evolved PNN feature nearly 
optimal structure and weights inferred from the data [Nikolaev and Iba, 
2001a, Nikolaev and Iba, 2003], the bootstrap method is suitable for 
analyzing them. The same reasoning holds even if the PNN is improved 
by backpropagation and Bayesian techniques. 
The same best polynomial network evolved by IGP during previous 
experiments with the benchmark Mackey-Glass time series [Mackey and 

Statistical 
Model 
Diagnostics 
253 
Glass, 1977] is taken, and its confidence intervals were computed by 
residual bootstrapping. The error bands generated are plotted in Figure 
9.5. A comparison with the confidence intervals of the same PNN model 
computed by the delta method (Figures 9.3 and 9.4) indicates that the 
bootstrapped intervals seem quite reliable. 
Series Point 
Figure 9.5. Confidence intervals of PNN estimated by residual bootstrapping of 100 
PNN sampled using the Mackey-Glass series (A = 0.001). 
The problem that arises again is how to produce the error bars in 
their reahstic original magnitude. The implementation of the bootstrap 
method requires us to initially compute the errors e = [ei, 62,..., ejsj] us-
ing the original outputs y and the estimated model output in its realistic 
magnitude P(x, w). Next, it has to be transformed into a normalized 
error vector e* = [e*, 62,..., e^] in order to generate the normalized error 
series e*^ necessary for the estimation of the bootstrap sample models 
P*(x*,w^). The sampled models have to be made using the normalized 
input data. Finally, the realistic differences P(x,w^)~ P (x, w) are pro-
duced using the outputs restored to their original magnitude F(x,w^) 
and the mean output P (x, w). All these transitions from one kind of 
model to another are necessary since the PNN model is learned from 
the normalized data, and its weights are determined in the normalized 
space. The PNN structure is valid in the normalized space and there 
is no guarantee that its complexity characteristics will be retained by 
direct estimation of the same model in the original space. 
The algorithm for computing realistic confidence intervals of PNNs 
by the residual bootstrapping method is given in Table 9.2. 

254 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
Table 9.2. Algorithm for finding the enripirical confidence intervals of PNN. 
Bootstrapping Confidence Intervals of P N N 
step 
Algorithmic 
sequence 
1. Initialization 
2. Sample 
bootstrap 
models 
2 N ) 
3. Calculate the 
Normalized model P*(x*,w) 
norrji,alized training data: D â {(x*, y*)}^-,i 
^.025(B- is the critical value of the Student's 
^-distribution with B degrees of freedom. 
a) Calculate the original errors e = (ei,e2, ...,eAr) 
eâ = y^ - P(xn, w), 1 < n < A^. 
b) Normalize the errors to become e* = (e*, ... 
e*, = (cr,,â x)/xstd, 1 < ''^' < A". 
c) Make a number 6 of normalized error series 
by random sampling 
e*''=: (et^e5^...,e*^^),l 
<b<B. 
d) Reestimate the model with 
outputs contaminated by the errors 
P*(x;,w'') = P ( x ; , , w ) + e ; ^ l < n < A'. 
Compute the standard error by returning the 
standard error 
outputs P*(x*,,w ) to their magnitute P(x, w ) 
dt = P{x,w^)- 
P (x,w) 
Estimate the 
error bars 
. e w , ( P ( x , w)) = ^ ( 1 / ( B - 1)) Zl^ 
dl 
Produce the confidence intervals 
P(X,W) Â±i,o25(B]Se/,oo/,(^(x,w)). 
9.5 
Prediction Intervals 
Statistical model diagnosis also involves evaluation of the uncertainty 
in their prediction. Although PNN describe the mean of the data dis-
tribution, whose variation from one sample to another can be estimated 
by confidence intervals, it is also important to quantify the belief that a 
future PNN output will belong to the distribution inferred by the given 
sample. The task of measuring the model reliabihty is to find the predic-
tion interval in which one has (1 â a)% (i.e. 95%) belief that the model 
output will belong to this interval. 
Prediction interval for a randomly drawn value are two limits: from 
above and from below, which with a certain probability contain this 
unseen value. While the confidence intervals account for the variance 
due to improper model components, the prediction intervals account for 
the model variance from the data. The prediction bars are estimates 
of the input dependent target noise, and they should be expected to be 
wider than the confidence error bars. 

Statistical Model Diagnostics 
255 
The prediction intervals for PNN are determined according to the 
hypothesis that their output error varies as a function of the inputs: 
P(x,w)Â±^025Cr^(x) 
(9.11) 
where (j'^{^) denotes the variance of the noise distribution. 
9.5.1 
Analytical Prediction Intervals 
Asymptotic prediction bands can be estimated following nonlinear 
regression theory in a similar way as the confidence bands using the delta 
method. The modification of the delta method, discussed in Section 
8.4.1, however, often leads to suspiciously wide intervals. This happens 
because the preliminary assumptions are often violated, more precisely: 
1) neural networks are often trained with the early stopping strategy, 
not exactly until convergence; that is, there is no guarantee that the 
obtained weight vector is sufficiently close to the optimal one; and 2) 
the provided data sets are of small size (not large enough), so they may 
not carry enough information. In such cases of violated assumptions, the 
intervals are found to be unreliable because the variance is unstable to 
compute. These observations inspired the development of more accurate 
analytical formula for evaluating prediction error bars. 
Analytical prediction intervals that take into account the effect of 
weight regularization can be estimated in the case of PNN with the 
following formula [De Veaux et al., 1998]: 
P(x, w) Â± ^ 025^*2/.w\/l + g^(H + L)-i J^J(H + L)-^g 
(9.12) 
where g is the gradient vector, H is the Hessian matrix (6.39), J is the 
Jacobian with the output derivatives with respect to the weights (6.43), 
L is a matrix with the local regularization parameters, o"* is the mean 
squared error of the normalized model, and ysid is the standard deviation 
of the targets. The Jacobian J is made during BP training in nonlinear 
PNN, while its generation for linear PNN is straightforward. 
The error variance under the square root is derived especially for 
weight decay regularization of the kind Y^\ 
'^h ^^ ^^ another kind of reg-
ularization is considered, another formula has to be rederived. Because 
of this specific derivation it is clear that the quantity under the square 
root is actually proportional to the squared quantity: g^(H + L)" g 
used for estimating confidence intervals according to the Delta method 
(9.9). This is reasonable because the weight decay regularization im-
pacts the learning of the weights inversely proportional to their squared 
magnitudes, therefore this fact should be taken into account when mak-
ing forecasts in order to explain more precisely what can be expected 
from such types of neural network models. 

256 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
U p p e r L i m i t 
PNN 
L o w e r L i m i t 
I 
60 
90 
S e r i e s 
P o i n t 
Figure 9.6. Analytical prediction intervals of a PNN model obtained by estimation 
with the benchmark Mackey-Glass series (A = 0.001). 
This formula (9.12) is elaborated to generate prediction bars restored 
in their original magnitude. This restoration is necessary because the 
PNN are usually evolved by IGP and trained by BP using the normalized 
input data in order to avoid computational instabilities and inaccuracies. 
The computer algorithm for finding the prediction intervals of PNN looks 
similar to the one given in Table 9.1 and differs only in that formula (9.8) 
is replaced by formula (9.12) to measure the prediction intervals. 
Prediction bands of a PNN estimated using this analytical approach 
of using the selected time series data are plotted in Figure 9.6. 
9.5.2 
Empirical Learning of Prediction Bars 
The unknown noise variance function can be found empirically by a 
neural network method derived from a maximum likelihood perspective 
[Nix and Weigend, 1995]. This method provides the idea of extending 
the polynomial neural network so that it learns not only the mean of the 
data distribution, but also the variance of this mean around the desired 
targets. While the conventional PNN output produces the mean P(x) ~ 
Â£^[?/|x], another output node is installed to produce the noise variance 
(7^(x) assuming that it is not constant but dependent on the inputs. 
Thus, the conditional probabihty density of the outputs is inferred as a 
function of the input data. 

Statistical Model Diagnostics 
257 
In order to capture the characteristics of the analyzed PNN architec-
ture, it is extended so that the the second output node accepts signals 
from all hidden (functional) and input (terminal) PNN nodes through 
a separate layer. An additional separate hidden layer whose nodes have 
incoming connections from all input and hidden nodes of the original 
PNN is installed. The extension features full connectivity; every input 
and hidden node output is passed along a corresponding link to every 
node in the additional hidden layer, whose outputs are next passed to 
the second output node. The number of nodes in the additional hidden 
layer is determined by the number of functional PNN nodes. 
The expanded topology keeps the same binary bivariate polynomials 
in the original PNN part, while the nodes in the extended part consider 
different transfer functions. 
The second output node transforms the 
weighted summation of the outputs from the extended hidden nodes by 
the exponential function: 
a2(x) == exp 
V VkjUj -f v^^) 
(9.13) 
where Vkj are the weights on connections feeding the second output 
node, f/co is a bias term, and Uj are the outputs of the J additional 
hidden nodes. The exponent function is a suitable choice to guarantee 
production of only positive values. 
The additional hidden layer uses sigmoidal activation functions to 
filter out the weighted summations of the incoming signals: 
Uj = sig I 2_^ VjiX^ + Vjo I 
(9.14) 
where x^ are the outputs from the activation polynomials in the PNN 
part of the extended network, Vji are the weights on connections from 
the PNN nodes to the extended hidden nodes, VJQ is a bias term, and 
sig is the sigmoid function: sig{z) = 1/(1 + exp(âz)). 
The secondary network is a Multilayer Perceptron (MLP) with uni-
versal approximation capacity so its use is theoretically motivated. Al-
though it is diff'erent (that is it has a different structure from the original 
polynomial network), it has sufficient power to obtain a good estimate 
of the noise variance of the provided data. The secondary MLP network 
learns the output uncertainty of the original network as it reflects its 
performance by accepting its inputs and hidden node outputs. 
Figure 9.7 presents the extension of the PNN tree-like network topol-
ogy previously demonstrated in Figure 6.1. 

258 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
(fix) 
P5(X)=W5â+H'5,*,+H'âZ,' 
P,(X)=H'3â-t-W3|Z,+ H',2Zy<', 
a 
Â® 
( expiUn j 
Sigmoidat Activation Function 
Exponential Activation Function 
(Summation Node Function) 
( Summation Node function) 
Figure 9.7. Expanded tree-structured PNN with additional hidden layer and sec-
ond output node for learning the error variance. 
Rules for PNN Learning of Error Variance, The development of the 
PNN training algorithm for error variance learning, according to the ML 
principle, relies on two assumptions [Nix and Weigend, 1995]: 1) that 
the noise in the data obeys the Gaussian distribution, and 2) that the 
errors are statistically independent. This enables us to introduce the 
following negative log likelihood criterion for coherent training of both 
the first and the second output network nodes: 
C 
E 
+ ln(a-'(xn)) 
(9.15) 
where y^ is the target, P(x^,w) is the network output, and a^(x^) is 
the sensitivity of the error variance to the n-th input. 
The weight update rules for training the extended PNN are obtained 
by seeking the minimum of this criterion function (9.15); that is, by 
differentiating it with respect to the weights, equating the resulted ex-
pression to zero, and solving it for the free variables. This is performed 
for both the first PNN output producing the mean, as well as for the 
second output producing the variance as they are mutually dependent. 

Statisticol Model Diagnostics 
259 
Delta Rule for the Second Output Node Weights, Let the second out-
put network node that models the variance o" (x) be indexed by k and 
the hidden nodes whose outgoing connections feed this node be indexed 
by J. The delta rule for training the second output node is: 
A.,, = , ^ 
^^.^-^ 
j . , 
(9.16) 
where 77 is the learning rate, Vj^j are the hidden to output weights that 
enter the second output node, and the signals on their connections from 
the additional hidden nodes are Uj, Note that this rule uses the squared 
error {yn â P(xn,w))^ generated at the output of the first node when 
the original PNN is estimated with the same n-th input. 
Delta Rule for the First Output Node Weights, Let the first PNN 
output network node modelling the mean of the data distribution P(x) '2Â± 
E[y\x\ be indexed by /c, let its children nodes be at level j , and the 
weights on the links between them be specified by W}.j. The delta rule 
for training the first output node that prescribes how to modify the 
weights associated with its incoming connections is: 
l^Wkj = r]6j,xj,j = T] I 
T2T^T 
I xj,^j 
(9.17) 
where wj^j are the hidden to first output weights, and x'^ â¢ are the deriv-
atives of the output activation polynomial with respect to its weights 
(Section 6.2). Here, it should be noted that this rule uses the variance 
(j^(xn) emitted from the second output node. 
The remaining weights below the roots are updated as follows: 1) 
the network weights in the original PNN part are adjusted according to 
the backpropagation learning rules for multilayer polynomial networks 
given in section 6.2; and 2) the weights on links to the nodes in the addi-
tional hidden layer are adjusted according to the standard delta rules for 
gradient descent search in multilayer perceptron neural networks using 
sigmoidal activation functions. 
Delta Rule for the Extended Hidden Node Weights, Let the hidden 
nodes in the extended layer be indexed by j as above and the PNN 
nodes that feed them be indexed by i. Taking into account that the 
additional hidden nodes after weighted summation of their input sig-
nals transform the sum by the sigmoidal function, the delta rule for the 
additional hidden nodes becomes: 
Avj, = r/[?ij(l - Uj){-6'f.)wi,,j]x^ 
(9.18) 
where the input signals Xj are either direct inputs or outputs from the 
activation polynomials in the PNN network, Uj are the outputs from the 

260 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
hidden nodes in the additional layer, and 6^ is the error backpropagated 
down from the second output node (9.17). 
Training Extended FNN. The extended PNN can be trained with a 
version of the BPA made to conduct gradient descent search in the weight 
space with the above learning rules (9.16, 9.17, 9.18). The secondary 
network however is a nonlinear MLP which suffers from the problem of 
entrapment into inferior solutions. That is, the neural network training 
process may lead to a weight vector which causes an error that is not 
optimal, rather it is an unacceptably high local minima. In addition to 
this, it could be noted that the learning rules for the hidden to output 
node connections in both parts of the extended network are mutually 
dependent. Because of these reasons, in order to avoid early entrapment 
at suboptimal local optima on the error landscape, the weight training 
process is divided into three consecutive phases. 
During the first phase, the original PNN is trained aiming at mini-
mization of the mean squared error Ejj â (1/A^) Yln^iiVn ~ -^(xn, w))^, 
using some backpropagation technique for high-order neural networks 
with polynomial activation functions (Section 6). In this phase, the 
extended secondary part of the network is not trained; its weight pa-
rameters are kept fixed. This stage should be performed with only a 
subset of the given data so as to avoid eventual overfitting. 
The second phase uses a different subset of the data for training just 
the extended MLP part of the network, also aiming at minimization of 
the mean squared error. This is implemented using the backpropagation 
algorithm with the learning rule for the second output node (9.16) with-
out dividing it by 2(7^ (x^), and the delta learning rules for the hidden 
nodes in the additional layer (9.18). In the second phase the weights in 
the original PNN part remain frozen, and they are not changed. 
The aim of training in the third phase is to minimize the log likefihood 
criterion (9.15). During this third phase the hidden to root node weights 
in the original PNN part are tuned according to the novel learning rule 
(9.17), while the remaining weights below are tuned with the learning 
rules for high-order networks with activation polynomials (Section 6.2). 
The weights in the second extended MLP part of the network are ad-
justed using the learning rule for the second output node weights (9.16), 
and the weights on connections by the learning rules for the hidden nodes 
in the additional layer (9.18). The training proceeds until reaching the 
minimum of the log likefihood criterion; that is, until attaining a sat-
isfactory low error. PNN prediction intervals computed following this 
empirical method using the benchmark Mackey-Glass series for training 
are plotted in Figure 9.8. 

StatistAcal Model 
Diagnostics 
261 
o 
S 
0.5 \- 
U p p e r L i m i t 
PNN 
L o w e r L i m i t 
60 
Series Point 
Figure 9.8. Prediction intervals of a PNN model obtained with the benchmark 
Mackey-Glass series (A = 0.001). 
The incremental version of the backprop algorithm for training the 
extended PNN to learn prediction intervals is summarized in the follow-
ing Table 9.3. It should be noted that for training the original PNN part 
in this algorithmic framework, the backpropagation training rules given 
in Section 6.2 have to be substituted. 
Training of the extended PNN to learn the output error distribution 
can unfortunately lead to under-estimated error bars. Such overopti-
mistic estimates of the prediction intervals can be unrealistic and mis-
leading. The main reason for the unrealistic results is the fact that this 
approach to learning prediction bands with extended PNN is derived 
from a maximum likelihood perspective. According to the maximum 
likelihood approach the mean is directly estimated from the data, so it 
also captures the noise in them and the mean becomes biased. The ef-
fect of the noise contaminated mean is inferring biased output variance, 
whose expectation is not exactly equal to the true error variance [Bishop 
and Qazaz, 1997]. 
A remedy for overcoming these problems is to perform modeling of 
the error variance following the Bayesian perspective. The empirical 
prediction bands obtained by training the extended PNN using rules 
derived from the maximum likelihood perspective can be compared and 
related to the Bayesian intervals generated from the same PNN models. 
Having different prediction bands allows us to reason more precisely 
about the quahty of the PNN models. 

262 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
Table 9.3. Incremental algorithm for error variance learning by tree-like PNN 
extended by a traditional MLP network. 
Training Algorithm for P N N Learning of Error Bars 
step 
Algorithmic 
sequence 
1. Initialization 
Data V â {{:x.n,yn)}v- land learning rate r] = 0.1. 
2. First Phase 
3. Second Pha 
4. Third Phase 
Perform BP training of the original PNN 
using the learning rules from Section 6.2. 
Perform BP training of the extended part of the network 
a) For each training example (xn,?/T7,), 1 < n < A'', do 
i) forward pass, and backward pass using the rules: 
A'U/.j- = r}{[yn - P(Xn, W)]^ - (J^(Xn))Wj 
Avji = r][uj{l - 
Uj){-6i)w},,j]xi. 
ii) Update the extended network weights. 
Simultaneously train the PNN and the extended part 
a) For each training example (xn^Vn), I < n < N, do 
i) Forward pass through PNN to produce P{xn) and 
forward pass through the MLP to produce (j^(xn). 
ii) Backward pass through PNN 
- compute the hidden to output deltas 
AWkj 
= TjilVn - P(Xn, w)]^/(J^(Xn))a:'/,^- . 
- compute the remaining node deltas 
using the rules from Section 6.2. 
- update the PNN weights. 
iii) Backward pass through the extended network 
- compute the hidden to output deltas 
A'i^fcj = ViiVn - P(Xn, W)]2 - a-^(Xn))Wj/2(7^(Xn). 
- compute backwards the hidden node deltas 
Avji = ri[uj{l - 
Uj){-6',,)wkj]xi. 
- update the weights of the extended network 
Until termination condition is satisfied. 
9.6 
Bayesian Intervals 
Recent research provides analytical and empirical approaches to mea-
suring error bars from a Bayesian viewpoint [MacKay, 1992a]. Such error 
bars are called Bayesian intervals. These contemporary approaches pro-
duce estimates that quantify the degree of belief in the models depending 
on both the uncertainties in the model weights, and the uncertainties in 
the training data. The commonality in them is that they capture vari-
ance due to inaccuracies of the weights, but differ in the way they capture 
the output error variance. 

StoMstAcal Model Diagnostics 
263 
A Bayesian prediction interval is such that there is a (1 â a)% (i.e. 
95%) behef that the model output will belong to this interval according 
to the posterior distribution of the population mean. The output that 
models the mean of the data distribution is regarded as a random vari-
able associated with its own distribution. Such Bayesian error bars for 
regression models are derived from the probabilistic posterior distribu-
tion of the weights. It is reasoned that if the prior opinion on the weights 
is high but they do not fit the data well, then they seem less plausible. 
The basic assumption is that the posterior weight distribution, which 
determines the output distribution, is Gaussian. The parameters of the 
normal weight distribution can be determined analytically or empirically 
by training a complementary network. 
Both analytical and empirical approaches are developed algorithmi-
cally in this section. The analytical Bayesian intervals involve constant 
output noise level, while the empirical Bayesian intervals feature by 
learned input-dependent noise level. 
9.6.1 
Analytical Bayesian Intervals 
The analytical approach suggests optimizing the weight posterior dis-
tribution by iterative adjustment of its hyperparameters [Nabney, 2002]. 
The optimal values of these hyperparameters can be found using the 
evidence procedure [MacKay, 1992a]. The evidence procedure is an in-
cremental technique to reestimate the hyperparameters after the neural 
network has been trained to some error minima. 
Bayesian prediction intervals for PNN models are defined as follows: 
P(x, w) Â± z,02^o-MP 
(9.19) 
where cfj^fp denotes the variance of the most probable model conditioned 
on the data. It can be evaluated after training the network model until 
a minimum on the error surface is reached. Since the presented induc-
tive PNN learning methodology involves IGF search as its first step, it 
is assumed that the best evolved PNN network is an acceptably good 
solution so it is reasonable to apply the suggested equation derived using 
Bayesian inference [Nabney, 2002]: 
aMP = ^<T2 + se2(P(x,w)) = . / ^ + g m - i g 
(9.20) 
where a^ is the noise variance component which is represented by the 
hyperparameter /3, 6re^(P(x, w)) is the standard error of the network 
model, g is the gradient vector (6.36, 6.37), and H is the regularized 
version of the Hessian matrix (6.39, 6.40). 

264 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
^ 
0.5 
UpperLimit 
\ 
PNN 
LowerL imi t 
60 
90 
Series Point 
Figure 9.9. Bayesian intervals of a PNN model obtained with the benchmark 
Mackey-Glass scries (A = 0.001). 
Working with this evidence procedure is efficient because it does 
not require performing repetitive model reestimations like the empiri-
cal bootstrapping. However, the application of the evidence procedure 
can be problematic in practice because it requires construction and in-
vertion of the Hessian matrix in run time. As previously studied, one 
remedy for avoiding numerical computation instabilities during the in-
version of the Hessian matrix is to augment its diagonal elements by 
a regularization factor. Another possibility is to use incremental or it-
erative least squares fitting techniques for reliable computation of the 
Hessian. 
Figure 9.9 offers a plot of the Bayesian prediction bands of a PNN 
obtained using the computational algorithm given in Table 9.4. Again, 
the benchmark Mackey-Glass time series was used, as well as a common 
global regularization parameter to facilitate comparisons. 
The Bayesian error bars in Figure 9.9 are sUghtly broader than those 
of the analytically estimated prediction intervals given in Figure 9.6 
because they include probable discrepancies due to the noise in the in-
put data as well as due to the uncertainties in the identified network 
weights. The Bayesian and analytical prediction intervals are larger 
than the learned empirical intervals by training the extended PNN fol-
lowing the maximum likeUhood approach. In this sense, they seem to 
be more realistic estimates of the expected output error. 

Statistical 
Model 
Diagnostics 
265 
Tabic 9.4. Algorithm for evaluating Bayesian error bars of tree-like PNN. 
Computing Bayesian Intervals of P N N 
step 
Algorithmic 
sequence 
1, Initialization 
2. Reevaluate the 
hyperparameters 
3. Estimate the 
Bayesian Intervals 
Data V = {(xn,2yn)}n^i) learning rate 77 = 0.1. 
Take the most probable PNN found by IGP 
Repeat a sufficiently large number of times: 
a) Reestimate the weights by OLS fitting 
b) Calculate the regularized Hessian 
H = * ^ ^ + L. 
c) Compute the effective parameters 
^ â W â a/;race(H)~^ 
and the hyperparameters 
a = 
j/{2w^w) 
/3 = (N - 7)/(2e'^e). 
d) Calculate the Bayesian error 
EB = PED -f Ew 
until reaching the minimum of the log evidence 
-\n(D\a,P) 
= EB -f-^(ln|A| -Wlua- 
N\nP). 
Compute the model variance 
(TMP = Jj 
+ g ' ^ A - i g . 
Produce the intervals using the formula 
P(X,W) Â± Z.025(^MP-
9.6.2 
Empirical Bayesian Intervals 
The weight posterior distribution can be learned empirically by train-
ing another secondary network complementary to the original one [Bishop 
and Qazaz, 1997]. For the purpose of finding the output error as a func-
tion of the input, the original network is extended with a hidden layer 
and a second output as illustrated in Figure 9.7. A secondary layer of 
sigmoidal neurons fully connected both to the training inputs and to 
the activation polynomial outcomes from the original PNN is added. 
Training of the expanded network is performed using the learning rules 
obtained by Bayesian analysis in pursuit of unbiased estimation of the 
noise variance. Bayesian prediction intervals for PNN regression models 
are constructed using the equation: 
P(X,W) Â± 2:.025^(x) 
(9.21) 
where a{x.) denotes the input-dependent noise variance. 

266 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
This conditional output error distribution can be learned by training 
the secondary network, attached to the original one, to minimize the 
following Bayesian cost function: 
C B - I J : f ' ^ ' " ~ . r h ' ^ ^ ^ ' + l n ( a ^ ( x â ) ) ) + a f : . , 2 + l n | H | (9.22) 
where the correction involves two terms: the regularization GcYld=\^\'> 
and the logarithm of the determinant |H| of the regularized Hessian. 
Two corresponding delta rules for the second output node weights and 
for the first output node weights are derived, seeking for the minimum 
of this criterion. The second output node that infers the error variance 
is trained by the following delta rule: 
A 
/^[?/n-i^(Xn,w)]^-(7^(Xn) . ^ 
\ 
,â ââ. 
A^/cj ^ V I 
2&H^) 
^^^^ I ^^ 
where rj is the learning rate, v^j are the hidden to output weights that 
enter the second output node, and Uj are the signals on them. 
The first output node of the original network is trained according to 
another delta rule: 
A^,,=,(l-^:^ppi-)4, 
(9.24) 
where w^j are the hidden to first output weights, and x'-â¢ are the deriv-
atives of the output activation polynomial with respect to its weights 
(Section 6.2). Here it can be observed that this rule uses the variance 
a (xn) emitted from the second output node. 
This empirical approach to making Bayesian intervals uses fixed hy-
perparameters and learns the weights in the secondary network that 
infer the conditional noise distribution, while the analytical approach 
uses adaptive hyperparameters. Another characteristic of the empirical 
Bayesian approach is that it applies regularization to both networks: to 
the original one that produces the conditional mean, and to the addi-
tional network that models the conditional variance of the output error. 
The incremental version of the backprop algorithm for training the 
extended PNN to learn Bayesian prediction bars is given in Table 9.5. 
It involves training of both the original PNN part of the network and 
the extended MLP part of the network. In order to implement it com-
pletely, the available backpropagation training rules for PNN models 
from Section 6.2 have to be substituted in step 2 (first training phase). 

Statistical 
Model 
Diagnostics 
267 
Table 9.5. Algorithm for Bayesian error variance learning by tree-like PNN ex-
tended by a traditional MLP network. 
Training Algorithm for P N N Learning of Bayesian Prediction Bars 
step 
Algorithmic 
sequence 
1. Initialization 
Data V â {(xn, 2/n)}n-^i^J^d learning rate 77 = 0.1. 
2. First phase 
3. Second 
phase 
4. Third 
phase 
BP training of the original PNN (rules from Section 6.2) 
Carry out BP training of the extended network 
a) For each training example (xr7,,2/n)) ^ '^ n, < N^ do 
i) forward pass, and then backward pass: 
^Vkj 
= 77([2/n - P ( X n , W))^ - ^ ^ ( X n ) + 
2aVkj)Uj 
Avji = r][uj{l - 
Uj)(-6'j,)wkj]xi. 
ii) Update the extended network weights. 
Train simultaneously the PNN and the extended part 
a) For each training example (xn,2/r7.), 1 < n < A^, do 
i) Forward pass through PNN to produce P{xn) 
forward pass through the MLP part for (j^(xn). 
ii) Backward pass through PNN 
- compute the hidden to output deltas 
Awkj = r]([yn - P(xn, w)]V(^^(xn))a^fcj. 
- compute the remaining hidden node deltas 
using the learning rules from Section 6.2. 
- update the PNN weights. 
iii) Backward pass through the extended network 
- compute the hidden to output deltas 
en = Vn - 
P ( X r , , , w ) 
Avkj = rjiiel - (j^(xn))/26-^(xâ,) + 2av},,j)uj. 
- compute backwards the hidden node deltas 
Avji = r][uj{l - 
Uj){~6'i^)wkj]xi, 
- update the weights of the extended network 
Until the termination condition is satisfied. 
9.7 
Model Validation Tests 
The tests for statistical model validation, especially when processing 
time series, involve examination of the residuals. A model is considered 
correct when its residuals are random, which means that the model 
exhibits unpredictable errors. In order to find out whether the residuals 
make a random sequence, specific correlation functions of the residuals 
are defined and estimated. If the past residuals are correlated with the 
future residuals, this is an indication that the model does not capture 
the data characteristics well. In other words, it must be investigated 

268 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
whether the model residuals carry information from the past, which 
means that the model is not reliable. When there is dependence between 
the residuals, this indicates that the model fails to describe some intrinsic 
component from the data. 
The model validation tests exploit the relationship between the model 
input and the model errors committed by the analyzed polynomial net-
work model over the training data. 
These tests include three steps 
[Billings and Zhu, 1995]: 1) determination of an acceptable domain for 
the decision values by establishing confidence intervals, usually â 1.96\/]V < 
$(r) < 1.96\/]V, for the statistics $(r) to be computed next; 2) measur-
ing the model residuals on the available training data; and 3) making a 
decision to accept or reject the model. 
When validating models it should be understood that the tests for 
linear models are not appropriate for nonlinear models because they 
may produce misleading results, as they fail to diagnose nonlinear terms. 
Working with PNN models requires the application of nonlinear residual 
autocorrelation tests [Billings and Zhu, 1995]. Learning PNN however, 
may produce only hnear polynomial networks, for example when there 
are only linear activation functions in the hidden nodes. That is why 
linear as well as nonlinear tests should be available to perform testing 
of both kinds of polynomial neural networks. 
Several correlation functions are given below. These are sample cor-
relation functions whose distribution approaches the normal Gaussian 
distribution with the increase of data. The linear correlation function of 
the residuals and the cross-correlation function between the input and 
the residuals are [Billings and Zhu, 1995]: 
^ 
(^) 
= 
E l l i K - -e] h^r- -e] = ( 1. r = 0 . 
(9.25) 
'^'^^ ^ 
y ^ ^ 
[g _ el2 
1^ 0, otherwise 
^ 
^ 
(Eili [-/- -?) (EÂ£I h - e]^) 
where r is the lag, x are the model inputs, and e are the errors. 
Nonlinear cross-correlation functions increase the statistical power 
of the existent tests providing more discriminatory evidence as follows 
[BiUings and Zhu, 1995]: 
E l l i Uf- x'] [e,.r- e] 
EHipW-^T (Er.ih-e]^) 

Statistical Model Diagnostics 
269 
$^2,2(r) 
= 
, 
L 
J L 
J 
^ Q 
(9 28) 
x'\ \eU- -e' 
El.[xh-'])(El^[eI-e'] 
$â,2(r) 
= 
. 
^ " ^ 
J / ' ' ^ 
^ 
^ = I S (9.29) 
$â,2(r) 
= 
, 
^ t - U ' - 
J L ' - - 
J 
=o,Vr(9.30) 
where r is the lag, e, x^ and i^ denote averages over the interval [1,A^]. 
The substitution for 'd expresses the cross-dependence between the net-
work output and the error, which is introduced for clarity: 
i9t = P(t,w)ei 
(9.31) 
assuming that P{t,w) is the PNN output determined by weight vector 
w, and ei is its error on the available data. 
The remaining constant in the above correlations k is defined as fol-
lows: 
E H i K - ^ ] ^ 
(9.32) 
These formula allow us to test whether for lags in the interval r G 
[â20,20], the autocorrelations fall in the corresponding predefined con-
fidence intervals. This will indicate that there are no predictable terms 
in the error sequence produced by the model, and therefore the inferred 
model can be considered as a good solution to the task. 
Figures 9.10 and 9.11 present plots with high-order correlations ob-
tained using two of the formula described above. These plots show that 
the model errors do not contain information from the past; that is, the 
correlation functions have not detected any deviation from normality. 
Therefore, it may be concluded that no cross variable terms are missing 
from the polynomial network models, otherwise the curves would have 
been outside of the suggested confidence intervals with dotted lines. 

270 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
O 
L a g 
Figure 9.10. Nonlinear correlation function between the output-error pair and the 
quadratic error of a PNN evolved using IGP using the Mackey-Glass series. 
e 
L a g 
Figure 9.11. Nonlinear correlation function between the output-error pair and the 
quadratic inputs of a PNN evolved using IGP using the Mackey-Glass series. 

Statistical Model Diagnostics 
271 
9.8 
Chapter Summary 
This chapter presented the basic algorithms for statistical diagnosis 
and validation of PNN. The benefit of having them is that the reliability 
of PNN is increased when supported by such means for statistical analy-
sis. The algorithms for measuring confidence, prediction, and Bayesian 
intervals are found to produce satisfactory results, in the sense that 
they generate intervals of similar size with shght diff'erences in shape. 
The intervals are likely to possess similar characteristics when they are 
obtained under the same conditions: sufficiently large number of data, 
clean enough data, and model training by efficient learning algorithms. 
The shape and size of the error bars made with the studied algorithms 
depend on the changes of these circumstances in the following ways: 1) 
when more training data are available the intervals will be narrower; 2) 
when the data are cleaner and contain less noise the intervals will also 
be narrower; 3) when the network model is trained by early stopping 
the intervals will be broader; and 4) when the model structure and the 
input variables are irrelevant, the intervals will be different in different 
data regions; they will be very large in sparse data areas, and they will 
be very small in dense data areas. 
Many other implementations of these statistical tools are possible 
[Efron and Tibshirani, 1989], for example other sampling and bootstrap-
ping methods are available [Efron and Tibshirani, 1989]. Other meth-
ods, however, cannot be expected to yield significantly different results, 
so those investigated here may be considered sufficient for rehable PNN 
testing. The standard error is often applied to carry out model selection 
[Zapranis and Refenes, 1999], that is, to compare the adequacy of alter-
native PNN and to judge which of them is best. The integrated bias and 
variance estimates of PNN can be used to prevent overfitting in several 
ways: 1) by changing the model representation, for example by reducing 
the number of variables, the number of terms, or the model degree; 2) 
by improving the network weight learning algorithm with techniques like 
regularization, early stopping, or training with noise; and 3) by making 
additional decorrelating data transformations or collecting more data for 
the same underlying function. 
The confidence intervals can be used for evaluating the significance of 
the PNN structure. More precisely, analysis could be performed on the 
PNN sensitivity to removal of polynomial terms and variables [Zapranis 
and Refenes, 1999]. When the polynomial structure of cascaded terms 
and variables is changed its approximation capacity changes, and this 
could be observed in the plots of the confidence bands. If the PNN mod-
ification leads to better models, the error band will shrink in comparison 
to that of the initial model. 

272 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
The studied approaches to statistical PNN diagnosis differ in compu-
tational speed and implementation effort. Most complex to develop are 
the Bayesian intervals. The analytical methods are difficult to encode 
because they require the evaluation of the Hessian. The empirical meth-
ods, like the one for making confidence intervals by residual sampling and 
learning the error variance, seem easier to make but are relatively more 
time consuming because they require repetitive computations, usually 
with large amount of data. Which of these methods and how to apply 
in practice should be judged by a careful design decision. 
The recommendation is to perform all the types of statistical model 
validation tests presented with the discovered models in order to be sure 
that they satisfy the theoretical requirements. 

Chapter 10 
TIME SERIES MODELLING 
This chapter relates PNN to hnear and nonhnear models generated by 
alternative inductive approaches. Time series modelling is the context 
in which the strengths and weaknesses of PNN, as well as its similarities 
to and differences from the other approaches are discussed. The models 
investigated include linear autoregressive moving average models, genet-
ically programmed functions, statistical learning networks, neural net-
works, kernel models, and recurrent neural networks. The belief in PNN 
is increased in relation to the other approaches and models because they 
have been studied extensively and it is well-known how to apply them 
with respect to the circumstances of the given task. 
Strictly speaking modelling of discretely sampled time series should be 
carried out according to the following sequence of steps: 1) examination 
of the series characteristics using statistical tests; 2) preprocessing of 
the data with suitable transformation formulae; 3) learning of PNN by 
IGP, BP, and Bayesian techniques including network pruning; and 4) 
diagnosis and validation of PNN with statistical tools. 
Before processing a time series it is reasonable to inquire whether it is 
potentially predictable. The predictability of a time series can be studied 
from a static perspective using statistical tests such as: 1) tests for 
autoregressive behavior; 2) tests for nonstationarity; 3) tests for random 
walk behavior; and 4) tests for nonlinearity. Applying autocorrelation 
tests to the given time series helps to realize whether the future series 
data depend on the past, that is such tests provide statistically significant 
evidence that the series can be predicted. Popular autocorrelation tests 
are the Box-Pierce Q-statistic [Box and Pierce, 1970] and the Ljung-Box 
Q-statistics [Ljung and Box, 1978]. The tests for nonstationarity allow 
us to detect whether the series contains a mean-reversion component 

274 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
which is difficult to model. Whether there is a nonstationary component 
in a series can be determined using unit root tests such as the Dickey-
Fuller test [Dickey and Fuller, 1981]. In order to find out if a series 
is uncorrelated, measurements can be made to see whether the series 
contains increments that are random or not. Random behavior in a 
time series can be examined using variance-ratio tests for deviations from 
random walk [Lo and MacKinley, 1988]. Motivation for discarding linear 
models can be obtained by tests for nonhnearity that check whether the 
data contain evidence for nonlinear signal dependence. This can be done 
using the Brock-Dechert-Sheinkman test [Brock et al., 1996]. 
The predictabiUty of a time series can also be investigated from a 
dynamical system perspective using invariant series properties, such as 
the Lyapunov spectrum and the correlation dimension [Abarbanel, 1996]. 
These dynamic properties characterize the sensitivity of the series to 
the initial conditions and they quantify the uncertainty about the future 
series behavior. Positive Lyapunov exponents and noninteger correlation 
dimension indicate that the series is chaotic. When the time series is 
chaotic it is difficult to infer stable forecast from diff'erent starting points. 
All these statistical and dynamic tests for time series are available from 
general-purpose and specialized software tools. 
This chapter presents empirical investigations into time series mod-
elling by PNN models evolved by IGF, and also some of their improved 
versions using backpropagation and Bayesian techniques. These FNN 
are selected after conducting a large number of runs. The experimental 
scenario is as follows: 1) the given time series is split into training and 
testing subseries; 2) a number of runs, approximately 100, are carried 
out with IGF to find the best FNN on the training series; 3) the best 
FNN from each run is improved by backpropagation and Bayesian re-
training; 4) a number (usually 5%) of all best FNN are taken to decide 
which of them to accept by carrying out statistical examination. The 
best FNN is taken for comparisons with the alternative approaches. 
10.1 
Time Series Modelling 
Time-series modelling may be regarded as an inductive learning prob-
lem. The task is to identify the regularities among a given series of 
points: ..., x{t),x{t + 1), x{t + 2),..., sampled at discrete time intervals. 
The goal is to find out how future points depend on points from the past. 
In the ideal case the behavior of the data generator can be described by 
diff"erential equations, but in practice knowledge for such mathemati-
cal modelling is not available. This is why eff^orts to discover plausible 
descriptions of the unknown data source are made using model search. 
Here, this description is assumed to be a FNN model. 

Tim,e Series Modelling 
275 
The empirical investigations in this chapter use input vectors x(^) 
created from the given observations assuming embedding dimension (i, 
and delay time r: 
x(0 = [x{t),x{t - r), x{t - 2r), ...,x{t - {d - l)r)] 
(10.1) 
that is lagged, sliding window vectors from dr nearest previous points 
starting at a point t. The delay time is a positive number r > 0, here in 
all experiments r == 1 is used. The dependant variable is the immediate 
neighboring point to the start point y = x{t -{- 1). 
The justification for this idea to use delay window vectors for predict-
ing the future series values is given by Takens' theorem [Takens, 1981]. 
Takens' theorem states that delay window vectors of past time series 
data contain sufficient information to reconstruct the behavior of the 
unknown system mapping which is smooth and invertible. The lagged 
vector components can be envisioned as independent degrees of freedom 
of the dynamic system that has generated the series. These coordi-
nates determine an equivalent to the original state space from which the 
system can be recovered unambiguously. Therefore, the orbits of state 
transitions of the system can be modelled by nonlinear functions of delay 
vectors. The point in the original state space toward which the system 
trajectory tends to converge is called the attractor. Each delay window 
vector is a different unfolding of the attractor point from its projection 
on the time series. In order to capture the behavior of the attractor a 
proper number of delay vector components should be selected, which is 
the embedding dimension. 
Takens' theorem leaves the question of exactly how to determine the 
embedding dimension unanswered. Obviously each lagged vector should 
include components that convey different information to the model. 
Their separating delay time should be such that instabihties and mea-
surement noise can be avoided, yet they should capture well the evolution 
of the series over time. What Takens' theorem suggests is a bound on 
the global embedding dimension, but it may be too large while smaller 
dimensions may be sufficient for accurate data modelling. There are 
algorithms for calculating the embedding dimension of time series, for 
example the global embedding dimension can be computed using the 
global false nearest neighbors algorithm [Abarbanel, 1996], while the 
minimal active embedding dimension can be computed using the local 
false nearest neighbor algorithm [Abarbanel, 1996]. 
The next question arising in time series modelling is what kind of 
model to use. Linear models can produce misleading results because 
they may require high-dimensionality even if it is possible to restore the 
system that has generated the data with a low dimensional nonlinear 

276 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
model. Linear models can be deceived by the strange attractors in the 
state-space. True linear data sources have fixed points, or simple attrac-
tors. Real-world time series, however, often exhibit strange attractors. 
This occurs as the system trajectory wanders along nonlinear manifolds 
of complex forms. Although exhibiting unstable trajectories, a system 
may have purely deterministic dynamics that produce quite irregular, 
nontrivial series. The dynamics of such systems may be reconstructed 
with nonlinear models. Measuring the correlation dimension of the at-
tractor [Grassberger and Procaccia, 1983] can provide evidence for the 
need of nonlinearities. A brief discussion on a linear and several nonlin-
ear time series models is off'ered below, and they are related to PNN. 
Applying delayed input vectors to nonlinear models is a promising 
strategy for time series modelling but it guarantees reliable forecasts 
for only a limited number of steps ahead, depending on the embedding 
dimension. Long time ahead reliable predictions are not guaranteed be-
cause the true dynamics of the unknown data source cannot always be 
found, for example when the unknown generator is chaotic, and changes 
in the starting value may lead to different trajectories. 
The predic-
tion horizon can be derived theoretically as inversely proportional to the 
maximum Lyapunov exponent of the series [Abarbanel, 1996]. 
10.2 
Data Preprocessing 
Data preparation for learning is performed using three main pre-
processing techniques; 1) filtering redundant components from the se-
ries; 2) removing outliers from the series; and 3) scaling, or normaliza-
tion, of the data. Our concern here is mainly normalization of the time 
series. Practical data collected from observations of natural phenomena 
diff'er significantly in magnitude. Such real data have to be rescaled for 
avoidance of computational instabilities and rounding errors due to the 
floating point format. A convenient technique for scaling the data, sub-
ject to next polynomial modeUing, is normalization to zero mean and 
unit variance. This kind of scahng is especially suitable for PNN. Such 
a normalization is given by the matrix formula: 
U = 0 - V 2 ( x - l x ' ^ ) 
(10.2) 
where X is the given N x d input data matrix, 1 is an identity A^ x 1 
vector of ones, x is a d x 1 vector of means x= (l/A/") J2n=-i Xn, â¬) is a 
dxd diagonal variance matrix 0 = {1/{N - 1))(X- 1 x'^)'^(X- 1 x'^), 
and U is the transformed matrix. The columns of the matrix U become 
the input variables to be passed to the model. Here, it is assumed that 
the columns of the matrix U are used to build the design matrix for the 
weights of the activation polynomials in PNN. 

Tme Series Modelling 
277 
All the experiments presented in this chapter have been conducted 
using this normalization, which is also called standardization due to the 
division by the standard deviation of the data. 
10.3 
P N N vs. Linear ARMA Models 
Linear models are widely used for time series modelling due to the 
sound theory that explains them [Box and Jenkins, 1970]. Although 
nonlinear models can also produce linear models, they usually outper-
form the linear models in the presence of nonhnearities, and especially 
sustained oscillations, as well as in the presence of stochastic distur-
bances. Simpler hnear models such as exponential smoothing and linear 
regressions may be used if there is no clear evidence of more complex 
nonlinearity in the data. The hnear models often need specific manip-
ulation with techniques for ehmination of trends and seasonal patterns 
for example, which require additional knowledge. 
A comparison of an evolved PNN model with a Hnear AutoRegressive 
Moving Average (ARMA) model was made recently [de Menezes and 
Nikolaev, 2006]. The PNN resemble ARMA models in that the acti-
vation polynomials are treated as hnear regressors. The weights of the 
PNN activation polynomials are learned by efRcient least squares fitting 
as are the weights of the hnear ARMA models. This provides the advan-
tage of reaching the optimal weights due to the unique global minimum 
on the error surface in case of hnear models. 
The benchmark Airline series [Faraway and Chatfield, 1998], popular 
in the statistical community, is chosen here for performing experimental 
comparisons. The series contains 144 observations, which are monthly 
totals of international airline passengers. The initial 132 points are taken 
for training through input vectors x(t) = [x(t),x(^ â1), ...,a:(t â11)]. Fol-
lowing the standard methodology of Box and Jenkins (1970), a seasonal 
ARMA model is developed and it is fit to the logarithm of the observed 
values: logxt ~ ARMA{0,1,1) 
x (0, l,l)i2. 
Next, a PNN model is 
evolved using IGP by performing 50 runs using fitness proportional se-
lection, both crossover and mutation operators, population of size 100, 
common regularization parameter for all weights A = 0.001 and selection 
threshold for pruning z = 0,01. 
The ARMA model shows accuracy of fitting the series MSEARMA 
= 
90.53, which is better than the PNN accuracy MSEPMN 
= 152.12. The 
prediction performance of the ARMA 
model is much worse showing 
one-step-ahead forecasting error 
^S^ARMA 
= 356.75, while the PNN 
shows MSE^pj^j^ â 185.27. The fitting accuracy and the prediction of 
the examined PNN model are illustrated in Figures 10.1 and 10.2. 

278 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
Figure 10.1. Fitting of the Airline series by a PNN model evolved by IGP. 
Figure 10.2. Forecasting (single-step-ahead prediction) of the Airline series by a 
PNN model evolved by IGP. 
This brief study allows us to make several observations that are in-
dicative of the advantages of genetically programmed PNN over linear 
ARM A models for time series modelling: 1) the use of PNN ehminates 
the need to perform data transformations before learning, so the need to 
decide whether and how to preprocess the given data is avoided; 2) the 
IGP of PNN are able to find polynomials that capture the time series 
characteristics well and predict well in the short-term; 3) the IGP of 

Tim,e Series Modelling 
279 
PNN can help to discover the relevant input variables for learning, and 
thus they help to understand the lag dependencies in time series; and 4) 
the PNN structure as a hierarchical composition of simple polynomials 
is a factor that affects the forecasting performance. 
10.4 
PNN vs. Genetically Programmed Functions 
The construction of PNN as tree-like topologies is especially suitable 
for GP style processing and can easily be extended to accommodate 
various components, such as harmonics, Gaussians, etc.. Recent research 
[Nikolaev and Iba, 2001a] shows that when evolved by the mechanisms of 
GP, these PNN usually outperform the traditional symbolic expression 
models composed of inhomogeneous primitive functions when evolved 
by Koza-style genetic programming. 
The IGP with PNN models is compared here to the traditional Koza-
style GP [Koza, 1992], which manipulates symbolic regression models 
of elementary functions such as: -f, =, *, /, sin, cos, exp, etc.. The 
traditional GP is made with the same micromechanisms as IGP: steady 
state reproduction of 50% from the population, fitness proportional se-
lection, crossover by cut and splice, and context-preserving mutation. 
These evolutionary micromechanisms offer potential for doing powerful 
global, as well as local, search in high-dimensional spaces, which is a 
widely claimed strength of the GP paradigm. 
Both IGP and GP systems are evaluated with the same fitness func-
tion using the benchmark Mackey-Glass series [Mackey and Glass, 1977]. 
The series is produced by generating 200 points with the Mackey-Glass 
differential equation for simulating blood flow using parameters: a ~ 
0.2,6 = 0.1, and differential A = 17. The first 100 points are used 
for training, and the remaining for testing. 
The considered embed-
ding dimension is d ~ d. IGP uses MaxTreeDepth 
â 4, regularization 
A = 0.001, selection threshold z â 0.001. Both systems use mutation 
probability p^ â 0.01 and crossover probability pc = 1.5. Fifty runs are 
conducted with each system using populations of size 100 and maximum 
tree limit size 40 nodes (functionals-j-terminals). The best PNN found 
has 9 nodes with 14 coefficients, while the best GP composite function 
has 11 nodes with 19 coefficients. 
The PNN model fits the series with accuracy MSEPNN 
= 0.000238, 
which is better than the accuracy of the GP function MSEQP 
~ 0.000341. 
The prediction performance of PNN is also better demonstrating one-
step-ahead forecasting error MSE'IJ^J^ 
= 0.000104, while the GP func-
tion exhibits MSEJ^p = 0.000202. The fitting accuracy and the predic-
tion of the the PNN model are plotted in Figures 10.3 and 10.4. 

280 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
>i 0 . 8 
S e r i e s 
P o i n t s 
Figure 10.3. Fitting of the Mackey-Glass series by a PNN model evolved by IGP. 
Figure 10.4, Forecasting (single-step-ahead prediction) of the Mackey-Glass series 
by a PNN model evolved by IGP. 
Learning PNN models by IGP entails several advantages compared to 
Koza-style GP with functions. First, the IGP efficiently computes the 
PNN coefficients/weights by least squares fitting so there is no need for 
the IGP system to conduct search for coefficient values as in traditional 
GP. Second, the IGP evolves universally approximating PNN with re-
hable descriptive characteristics, while traditional GP usually tries to 
find arbitrary compositions of arbitrarily selected basis functions. Using 

Tim,e Series Modelling 
281 
bivariate polynomials in PNN entails three main benefits: 1) it leads 
to smaller spaces for the IGP to search; 2) it requires less sophisticated 
mutation and crossover operators; and 3) it allows us to conduct evolu-
tionary search with populations of smaller size. 
10.5 
PNN vs. Statistical Learning Networks 
PNNs are closely related to statistical learning networks [Barron, 
1988], and more precisely they construct similar polynomials as the 
classical Group Method of Data Handling (GMDH) [Ivakhnenko, 1971, 
Madala and Ivakhnenko, 1994, Farlow, 1984, MuUer and Lemke, 2000, 
Elder and Brown, 2000], and the Multivariate Adaptive Polynomial 
Sphnes (MAPS) algorithm [Barron and Xiao, 1991]. 
IGP infers polynomial network representations similar to those built 
by Multilayer GMDH [Ivakhnenko, 1971]. Multilayer GMDH synthe-
sizes a polynomial network layer by layer conducting a heuristic search 
for pairs of variables to feed the next node so as to decrease the residual 
outcome error. This, however, constrains the feeding of higher layers 
since they depend on previously learned lower nodes that may not be 
optimal. GMDH makes strictly layered networks having a large number 
of internode connections because there are links radiating from every in-
ternal node to every node in the next layer. The tree-structured PNNs 
evolved by IGP feature a lower complexity, which improves their accu-
racy of fit and forecasting potential. 
Experiments are performed to find out whether a genetically pro-
grammed PNN can outperform the Multilayer GMDH on the bench-
mark Sunspot series [Weigend et al., 1992]. 255 points are taken and 
divided into two subsets: the data from years 1700 â 1920 for training, 
and those from 1921 â 1955 for testing. The embedding dimension is 
d = 9. Fifty runs are conducted with IGP using: MaxTreeSize 
= 40, 
MaxTreeDepth 
â 5, mutation probability pm = 0.01, crossover prob-
ability pc = 1.5, regularization A = 0.0015, and selection threshold 
z = 0.01. The GMDH network has been designed with 4 layers, 8 
nodes in each layer using the complete bivariate activation polynomial, 
and also using regularization A â 0.0015. 
The trained GMDH displays accuracy MSEGMDH 
= 0.00469 which 
is worse than the PNN accuracy MSEPMN 
= 0.00372. The GMDH 
model also shows worse one-step-ahead forecasting error MSEQJ^JJJ^ 
= 
0.0247, while the PNN displays MSE{,J^J^ 
= 0.01138. The best poly-
nomial has 11 nodes with 18 coefficients, while the GMDH network has 
32 with 192 coefficients. The fitting accuracy and the prediction of the 
PNN on the Sunspots are illustrated in Figures 10.5 and 10.6. 

282 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
>iO . 6 
Figure 10.5. Fitting of the Sunspots series by a PNN model evolved by IGP. 
U) 
Figure 10.6. Forecasting (singlc-step-ahead prediction) of the Sunspots series by a 
PNN model evolved by IGP. 
A principle distinction from GMDH [Ivakhnenko, 1971, Madala and 
Ivakhnenko, 1994, Farlow, 1984, Elder and Brown, 2000] and MAPS 
[Barron and Xiao, 1991] is that IGP performs search in the space of 
whole polynomials, while the others grow a single polynomial iteratively. 
GMDH and MAPS build one polynomial network in such a way that 
the formation of the next layer is guided by the partial fitnesses from 
previous levels. Such a hill-climbing constructive approach makes these 

Time Series Modelling 
283 
statistical learning algorithms susceptible to entrapment at local optima. 
IGP conducts more powerful search as it performs global exploration as 
well as local exploitation of the landscape. 
10.6 
P N N vs. Neural Network Models 
The PNN generated by the IGP system belong to the category of 
feed-forward MLP networks [Rumelhart et al., 1986, Hertz et al., 1991, 
Haykin, 1999]. Both kinds of networks, MLP and PNN, implement 
nonlinear functions as hierarchical compositions. The practical problem 
of MLP is that the proper number of layers and the number of nodes 
usually must be found experimentally. A distinctive feature of PNN 
is that their model structure and variables can be found automatically 
using the evolutionary micromechanisms of IGP. 
PNN and MLP both use adaptive learning by backpropagation tech-
niques for gradient descent search in the weight space. In this sense, 
PNN benefit from the efficacy, simplicity, and power of the backprop 
techniques. At the same time, both PNN and MLP suffer from the need 
to identify suitable values for the parameters of the algorithm such as 
the learning rate, the momentum, the regularization parameter, and the 
termination criterion. There are approaches to finding suitable parame-
ter values that can be applied directly to PNN such as those based on 
the Bayesian evidence procedure [MacKay, 1995]. PNN also assume the 
strategies for improving the generalization performance developed for 
MLP such as network pruning and early stopping [Bishop, 1995]. 
A PNN evolved by IGP and improved after that by BP is compared 
to an MLP network on the benchmark Far-Infrared-Laser series [Htib-
ner et al., 1994]. This Laser series contains fluctuations of a physical 
laser recorded in a chaotic state during a laboratory experiment with an 
oscilloscope. The objective is to learn the description of a far-infrared 
NH3 laser given its intensity pulsations. The initial 900 points are taken 
for training, and the next 100 points for testing as in the other research. 
The embedding dimension is d = 10. Approximately fifty runs are con-
ducted with IGP using populations of size 100, MaxTreeSize 
= 40, 
and MaxTreeDepth 
â 6. The IGP system uses parameters: mutation 
probability p ^ = 0.01, crossover probabihty p^ â 1.5, regularization 
A = 0.001, and selection threshold z â 0.01. 
The BP training algorithm is run to perform 150 epochs with para-
meters; learning rate 77 = 0.001 and momentum a â 0.01. The MLP 
network is manually designed with one hidden layer of 10 sigmoidal ac-
tivation functions and a summation output node. Training of the MLP 
by the backpropagation algorithm is made using a fixed learning rate 
'^MLP ~ 0-01 ^^^ momentum aMLP = 0'02. 

284 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
S a m p l i n g 
Figure 10.7. Fitting the Laser series by an evolved PNN model retrained by BP. 
a 
0 
â¢H 
-P 
(6 
::i 
-P 
U 
::5 
TH 
Cn 
200 
100 
0 
;i 
â 
L a s e r 
h 
/l 
it 
1 
II 
f 
1 1 
1 
11 
1 
f I 
1 
i 
1 1 1 
PNN 
/; 
BP . PNN 
I \\ 
> 
1 
1 1 
I \ 
I \ 
II \ 
1 \ 
1 \ 
1 
1 1 
1 1 
i
t
/
/
\
/
\
/
\ 
I 1 \ / V / \J' V/ V/ > 
,, 
1 
1 
1 
1 
9 2 0 
S a m p l i n g 
Figure 10.8. Forecasting (single-step-ahead prediction) of the Laser series by a PNN 
model evolved by IGP and retrained by BP. 
The fitting accuracy and the prediction capacity of the best discov-
ered PNN model are given in Figures 10.7 and 10.8. The evolved PNN 
has 15 nodes with 34 coefficients, while the MLP is fully connected 
with 10 hidden nodes. The PNN model shows accuracy on fitting the 
series MSEpj\jj\j = 32.45, which is better than accuracy of the MLP 
MSEMLP 
= 48.62. The prediction performance of PNN is also better 

Tim,e Series Modelling 
285 
demonstrating one-step-ahead forecasting error MSEl,j^^ 
= b5m while 
the MLP shows MSE'lj^p 
= 80.07. 
MLP can benefit from using the input variables from the best PNN 
found by IGF, and this helps to achieve neural networks with improved 
forecasting performance. The IGP system, however, has similar com-
putational disadvantages to the MLP; their algorithms require tuning 
many free parameters and there are random initializations that can af-
fect their operation. While the MLP uses randomly initialized weights 
and derivatives to start the learning process, the IGP uses a random 
initialization of the initial population of PNN, fitness proportional ran-
domized selection, and random selection of transformation nodes for the 
learning crossover and mutation operators. All of these random effects 
require a large number of runs in order to acquire convincing results. 
The benefit of evolving PNN by IGP is that polynomials of almost 
unlimited order could be discovered due to the hierarchical polynomial 
network construction inherited from the multilayer GMDH algorithm. 
The identification of the higher-order term weights is made efficiently 
by cascading low-order activation polynomials whose weights are esti-
mated without serious computational problems. This is an advantage 
over traditional multilayer feed-forward neural networks trained by back-
propagation which are limited in modelling very high-order functions 
by the computer capacity to calculate higher-order weights [Wray and 
Green, 1994]. The precision of linear polynomial networks [Wray and 
Green, 1994] is also sensitive to the computational limitations of the BP 
training algorithm. 
10.7 
PNN vs. Kernel Models 
PNN, as well as kernel models, are suitable tools for time-series mod-
elling [Miiller et al., 1999, Quihonero-Candela, 2004]. In developing 
kernel models such as PNN models, one has to decide which kernel func-
tions should enter the model and how to find their coefficients. That is, 
PNN and kernel models both face the problem of which should be the 
basis set, and how to combine the selected elementary basis functions. 
Research in kernel models generally utilizes local-basis functions such 
as Gaussians, splines, or multiquadratics. While the problem of PNN 
model structure selection can be addressed by evolutionary style search 
using the IGP paradigm, the structure identification problem in kernel 
modelhng has considered mainly greedy hill-climbing structural search 
algorithms and Bayesian techniques. A linear PNN trained by Bayesian 
techniques is presented here to illustrate the advantage of using these 
probabilistic techniques for addressing difficult modeUing tasks. 

286 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
The capacity of linear PNN with polynomial kernels to learn from 
time series is related here to a model with Gaussian kernels trained 
by the Sparse Bayesian Learning [Tipping, 2001] algorithm on a real-
world financial exchange rates series. A series of 280 cross-currency 
exchange rates between the dollar USD and the Japanese yen sampled 
every minute, is taken without any kind of preprocessing; only normal-
ization was applied. The initial 265 points were used for training and 
the remaining 15 for testing. The embedding dimension was d = 15. 
The IGP program was run 50 times using typical system parame-
ters: PopulatAon.Size = 100, MaxTreeSize 
= 40, MaxTreeDepth 
= 7, 
mutation probability pm = 0.01, crossover probabihty pc = 1.5, regu-
larization A = 0.001, and selection threshold z = 0.01. The SBL al-
gorithm [Tipping, 2001] has been designed using Gaussian kernels with 
spread (width) s~~'^ = 0.1. SBM was made using the EM principle (Sub-
section 8.4). The best evolved PNN was retrained with the recursive 
Bayesian version of the Levenberg-Marquardt algorithm (BRLM) (Sub-
section 8.5). Both the BRLM and SBL were implemented with the 
same initial noise hyperparameter /3 = 1.0/std{y). 
However, they used 
different initial prior hyperparameters: SBL was made with a = le"^"^, 
while the BRLM was made with a very small a = le""^/{Nw'^) since it 
performs hyperparameter training. The hyperparameter learning rate 
in BRLM was set to ry = le~^/N. 
Both training algorithms were iter-
ated 10 times, and they pruned weights when their corresponding alphas 
exceeded the predefined threshold value a MAX = 1.0. 
The PNN and SBL models show very close training accuracy with 
MSEsBL = 0.00326 and MSEPNN 
= 0.00352. In addition, both kernel 
models are examined on multi-step-ahead forecasting which is a chal-
lenging task. While the SBL model demonstrates prediction with error 
MSEIQ^ 
^ 0.0228, the PNN with polynomial kernels predicts with 
much smaller error MSEpj^j^ = 0.0176. It should be noted that both 
models are of quite similar complexity. The best PNN contained 52 
kernels (with 52 weights) and the SBL model had 58 kernels (coeffi-
cients), which indicates that the Bayesian approach provides potential 
to generate sparse and good forecasting models. The observation that 
both models produce similar results is due to their similar representa-
tion capacity; they are both kernel models. Illustrations of the PNN 
performance are provided in Figures 10.9 and 10.10. 
What is interesting to note in Figure 10.9 is that the PNN can ex-
hibit good interpolation and extrapolation performance on such time 
series with quite irregular characteristics. Figure 10.10 illustrates the 
multi-step-ahead prediction from the PNN which is informative about 
its abilities to generate long-lasting forecasts. 

Time Series 
Modelling 
287 
Figure 10.9, Fitting of the cross-currency exchange rates series by a PNN model 
evolved by IGP and retrained by recursive Bayesian learning. 
P 
2 6 0 
2 6 5 
2 7 0 
m i n u t e 
Figure 10.10. Forecasting (multi-step-ahead prediction) of the cross-currency 
exchange rates series by a PNN model evolved by IGP and retrained by recursive 
Bayesian learning. 
The SBL taken alone offers the advantage of faster training compared 
to the PNN is initially evolved by IGP. However, BRLM can be applied 
directly to PNN without model selection by IGP. The efficiency of doing 
probabilistic training is in the proper adjustment of the weight para-
meters to the noise with respect to the assumption for its distribution. 

288 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
Kernel models are sensitive to the characteristics of the training data, 
and their spreads should be well-tuned before learning. 
10.8 
Recurrent PNN vs. Recurrent Neural 
Networks 
Recurrent PNN, such as the recurrent MLP, are nonlinear models 
that use memory to model time-dependant sequences of data. Encoding 
of memory by means of hidden network nodes and recurrent feedback 
connections enables us to achieve sensitivity to the order of presentation 
of the data, thus it contributes to capturing dynamic characteristics 
especially when dealing with time series, 
A recurrent PNN is related here to an Elman-type [Elman, 1990] re-
current neural network (RNN) using real-world electricity load series. 
This series contains 5850 electricity load measurements in megawatts 
from a certain area recorded every day at 12:30. The initial 4900 data 
are considered for training and the remaining for testing. 
The em-
bedding dimension is ci = 12. A dynamic PNN is evolved by IGP 
and further retrained using the RTRL algorithm specialized for PNN 
(Subsection 7.3). The IGP system was run 50 times using parameters: 
Populationsize 
= 100, MaxTreeSize 
= 40, MaxTreeDepth 
= 7, mu-
tation probability p^ â 0.01, crossover probability pc = 1.5, regulariza-
tion A = 0.001, and selection threshold z â 0.01. 
The best PNN model found by IGP has 6 hidden nodes, and 7 input 
terminal leaves including those feeding back outputs. The RNN has been 
designed with 1 layer of 6 hidden nodes and feedback connections from 
each hidden node to all hidden nodes. The hidden nodes are made with 
sigmoidal activations, while the output node simply used summation. 
The weights of this RNN are also trained using RTRL. Both RTRL 
implementations are made to perform 1000 epochs. The approximation 
abilities of the best recurrent PNN evolved by IGP and improved by 
RTRL are illustrated in Figures 10.11 and 10.12. 
The Elman-type RNN model demonstrates a better fitting accuracy 
on the training data MSEEJman = 1.02e6 than the PNN accuracy 
MSEpj\jj\j = 1.13e6. The prediction performance of the RNN is worse, 
however, showing one-step-ahead forecasting error MSE-'^j^^^ â 2.04e6 
while the PNN shows MSE-j^j^j^ = 1.93e6. These results show that the 
Elman-type RNN may achieve better approximation of the given train-
ing data, but it tends to exhibit worse forecasting performance than 
PNN. The main reason for the superior forecasting is again the evolu-
tion of a parsimonious model by IGP which has not only a small number 
of inputs, but also features sparse connectivity. 

Tim,e Series 
Modelling 
289 
3 0 0 0 
4 0 0 0 
Day 
( a t 
1 2 . 3 0 ) 
Figure 10.11. Fitting of the COMPLEX 
series by a recurrent PNN model evolved 
by IGP and retrained by RTRL. 
5200 
5600 
Day 
( a t 
1 2 . 3 0 ) 
Figure 10.12. Forecasting (single-step-ahead prediction) of the COMPLEX series by 
a recurrent PNN model evolved by IGP and retrained by RTRL. 
It has been found that on average the temporal training of recurrent 
PNN converges rehably to nearly optimal solutions once a parsimonious 
network architecture has been discovered by IGP. One reason for this 
learning behavior is that the training process starts not with arbitrarily 
generated weights but with good weights determined during the struc-
tural PNN search process by the mechanisms of the IGP system, in 

290 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
particular using least squares fitting. Another condition for achieving 
good asypmtotic stability when training recurrent PNN is the replace-
ment of the squashing sigmoid function with activation polynomials in 
the hidden network nodes. Investigations showed that the sigmoid func-
tion tends to attenuate the temporal signals too fast. 
10.9 
Chapter Summary 
This section demonstrated results on modelling benchmark and real-
world time series by PNN learned adaptively using IGF, backpropaga-
tion, and Bayesian techniques. The performances of the best discovered 
PNN have been related to other popular approaches to time series mod-
elling in order to understand whether they can be superior for such 
regression tasks. It has been found that the presented methodology pro-
vides computational mechanisms that infer PNN which are more consis-
tent models than genetically programmed regression functions, statisti-
cal learning networks, feed-for ward MLP neural networks, kernel models, 
and recurrent neural networks on the same data. 
The experimental results reported in this chapter have been produced 
using the basic tree-hke PNN representation as proposed in Section 2.1.2. 
This has been done in order to provide understanding about the aver-
age expected performance of all remaining models given in Chapter 3. 
The remaining tree-like PNN representations may exhibit only slightly 
better or slightly worse performance depending on the characteristics 
of the provided data. There could be arguments for and against each 
particular PNN model because each concrete model could be superior 
on data with corresponding properties. The presented models have been 
given normalized data, but various other preprocessing techniques could 
also be appHed. For example, there could be made integral data trans-
formations, differential transformations, rational transformations, etc.. 
What this chapter also suggests is that in practice it may be useful to 
try inductive learning of alternative nonlinear models because in some 
situations they may show better approximation qualities, for example 
if the researcher has much more experience in them. The decision of 
which precise model to use for the given training sample can be made 
after performing statistical model diagnostics and validation tests. It 
should also be noted that whatever model is considered it always has to 
be processed using smoothness constraints (for example using regular-
ization), because time series modeUing is an ill-posed problem. 

Chapter 11 
CONCLUSIONS 
This book is inspired by the thought that nature has created the most 
perfect mechanisms, and if we want to attain the efficacy of nature we 
may try to simulate, and mimic its mechanisms at least to the degree 
to which we know them. Studies into biology, human genetics, and neu-
roscience not only enhance our understanding of the surrounding world, 
but also provide us principles of evolution and operation. These prin-
ciples can be used to develop efficient computer algorithms for solving 
difficult practical problems in a similar manner to which nature deals 
with them. 
Biologically inspired inductive computation is a multidiscipline that 
unifies invaluable theoretical achievements from many areas. When uni-
fied these theoretical findings become complementary and enhance their 
importance and reliability. In particular, the presented methodology 
for adaptive learning polynomial networks from data offers a common 
view of evolutionary and neural computation. Integrated together the 
evolutionary, neural, and probabilistic computation are a coherent par-
adigm for inductive problem solving. This methodology is a symbiosis 
of findings from various fields including computer science, mathemat-
ics and statistics, as well as biology. Seemingly different ideas, meth-
ods, strategies, representations, and models from these well-established 
areas of science are generalized in a common framework for inductive 
computation. The reasons for collecting this knowledge come from the 
understanding that their resources actually serve for addressing similar 
tasks known under different names such as approximation, regression, 
nonlinear modelling, and time series forecasting, to name a few. 
The results obtained according to the presented methodology allow 
us to draw three main conclusions: first, genetic programming helps us 

292 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
to alleviate to a large extent the basic difficulties in model selection; sec-
ond, neural network techniques allow us to improve evolved models; and 
three, more accurate tuning of inferred models can be performed using 
Bayesian techniques. Inductive genetic programming provides powerful 
algorithmic means for tackling some problems that neural network and 
even probabilistic approaches are unable to resolven namely: 1) identi-
fication of the model structure and selection of the input variables; 2) 
deciding which is the best function format in the sense of which kind of 
activation functions in which nodes to use them; and 3) how to interpret 
the learned model. Genetic programming offers possibihties to carry 
out global as well as local search that progressively adapts the model 
structure to the data. 
This book adopts neural network representations as a model descrip-
tion language for evolutionary computation. The rationale is that the 
hybridization of the two paradigms enables us to apply reliable back-
propagation training techniques for gradient descent weight search. Re-
lieved of the need to specify the network architecture and the inputs in 
advance, assuming that they have been found by genetic programming, 
the connectionist training offers several advantages: 1) reliable weight 
learning algorithms that deal well with uncertain data; and 2) facilities 
for control over the generahzation during learning. 
Many practical data can be modelled robustly only after the applica-
tion of Bayesian techniques. There are practical data contaminated with 
different noise characteristics, out hers, and omitted values which can 
only be modelled reliably in a probabihstic setting. This book demon-
strates that the Bayesian inference helps to extend the capability of 
polynomial networks and makes them promising learning tools for prac-
tical data. Polynomial networks equipped with probabilistic training 
algorithms can capture the regularities in the data better because they 
assume adaptation of the model with respect to the adopted data dis-
tribution. Based on the evidence presented in this book, we are inclined 
to think that the inductive genetic programming, neural network and 
Bayesian paradigms combine together their strengths and enables us to 
implement promising tools for nonlinear modelling. 
The motivation for developing this adaptive computation methodol-
ogy for computational induction of polynomials is the idea to bind to-
gether the advantages of the three paradigms. In order to achieve grad-
ual adaptation of the model structure to the data, attention is directed 
toward using the principles of natural selection, crossover, mutation, and 
their computer realizations suggested by genetic programming. In order 
to achieve fine-tuning of the model coefficients, attention is directed to-
ward gradient descent search methods designed for neural networks. In 

Conclusions 
293 
order to adjust the model properly to the general features in the data 
without capturing the hidden noise, Bayesian learning is performed. In 
this sense the three paradigms, genetic programming, neural networks 
and probabihstic inference, provide algorithms for adaptive inductive 
learning of models from data. 
This book demonstrated successful results on nonlinear modeUing of 
time series by polynomial networks. It was shown that genetically pro-
grammed polynomial networks outperform traditional statistical autore-
gressive moving average models, neural networks, statistical learning 
networks, kernel models, and recurrent networks on benchmark time se-
ries. The presented empirical results may serve as arguments to employ 
polynomial networks for solving real-world tasks from practice. The de-
cision to use them for time series processing should follow the prescribed 
sequence of steps, in accordance with which the final decision should be 
taken after careful validation of the model using statistical diagnostics. 
Even if the polynomial model discovered from the data may seem ade-
quate from visual inspection of its output, it should not be considered 
without a thorough examination of its residuals. Passing the statistical 
diagnosis tests is a sound evidence for the quality of the model. 
Research on polynomial networks has two main aspects: theoretical 
and practical. The first group of theoretical research concerns elabo-
ration of a more rigorous formalism describing the evolutionary search 
process conducted by genetic programming. Further investigations should 
be oriented toward making a more precise Markov model of genetic pro-
gramming that can give a better probabilistic explanation of how it 
works and how its population evolves generation by generation. Such a 
model can help to perform more careful convergence analysis and can 
give guidelines on how to design more robust evolutionary search con-
trol. The second group of theoretical research concerns investigation of 
the convergence of the polynomial network training process. Special at-
tention has to be made to study the convergence of recurrent polynomial 
networks. 
The practical research on polynomial networks is oriented toward im-
plementing easy to use algorithms for fine-tuning of the genetic pro-
gramming parameters and the neural network parameters. The genetic 
programming parameters should be improved in order to enable better 
adjustment of the model structure to the given data. These parameters 
include initialization of the mutation and crossover probabilities, selec-
tion of crossover and mutation points, the parameters of the selection 
mechanism, etc.. The parameters of the neural network training algo-
rithm should be improved to adjust the model nonhnearity better and 
to increase the generalization. This mainly concerns the learning rate 

294 
ADAPTIVE LEARNING OF POLYNOMIAL NETWORKS 
parameter. In order to achieve efficient Bayesian learning, the initial 
hyperpriors and the noise hyperparameter have to be carefully selected. 
The developments on polynomial networks offered here can be used 
separately and can be applied in practice in different ways depending on 
what is the purpose of the implementation: is it required to learn models 
very fast from the provided data, to find extremely accurate models, to 
produce sparse models, to provide forecasts online, etc.. Several guide-
hnes can be given on possible uses of the models and algorithms offered 
in this book: 
- the tree-structured PNN representations can be learned using only 
IGP without further training; when the main objective is discovery of 
parsimonious models there is no need to finely tune the coefficients, in 
such case using only IGP is sufficient and fast enough as it takes less 
time than one neural network training cycle of thousands of epochs; 
- alternative PNN models, using kernels, harmonics, local basis func-
tions, splines, sigmoids, Gaussians, etc., can be grown layer-by-layer as 
cascaded hierarchical polynomial networks using constructive algorithms 
hke the Multilayer GMDH or Combinatorial GMDH for example, which 
is facilitated by the corresponding weight learning techniques provided 
for each particular network model; 
- PNN architectures can be manually designed by fixing their topolo-
gies in advance using some heuristics, and next trained by a suitable 
backpropagation or Bayesian technique for the relevant model. 
This book is a product of long research into model-free computational 
induction. It proposes a methodology for adaptive learning of polynomi-
als represented as neural networks. These polynomial networks provide 
uniform treatment for different kinds of nonlinear function models and 
their important characteristic is that they are easy to understand and 
interpret. PNNs are open-box models and this is another reason to pro-
mote them as promising tools for successful real-world appHcations to 
tasks such as system identification, signal processing, adaptive control, 
forecasting, etc.. 

References 
[Abarbancl, 1996] Abarbancl, H.D.I. (1996). Analysis 
of Observed Chaotic 
Data, 
Springer-Vcrlag, Berlin. 
[Ackley, 1987] Ackley, D.H. (1987). A Connectionist 
Machine for Genetic Hillclinib-
ing, Kluwer Academic, Boston, MA. 
[Akaike, 1970] Akaike,H. (1970). Statistical Predictor Identification, Annals 
Inst. 
Stat. Math., vol.22, 203-217. 
[Akaike, 1973] Akaike, H. (1973). A New Look at the Statistical Model Identification, 
IEEE Trans. Automatic 
Control, vol.19, 716-723. 
[Altenberg, 1994a] Altenberg, L. (1994). Emergent Phenomena in Genetic Program-
ming, In: Sebald, A.V. and L.J. Fogel (Eds.), Proc. Third Annual Conference on 
Evolutionary Programming, World Scientific, Singapore, 233-241. 
[Altenberg, 1994b] Altenberg, L. (1994). The Evolution of Evolvability in Genetic 
Programming, In: Kinnear Jr., K. (Ed.), Advances in Genetic Programming, The 
MIT Press, Cambridge, MA, pp.47-74. 
[Angeline, 1994] Angeline, P.J. (1994). Genetic Programming and Emerging Intel-
ligence, In: Kinnear Jr., K. (Ed.), Advances in Genetic Programming, The MIT 
Press, Cambridge, MA, 75-98. 
[Anthony and Holden, 1994] Anthony, M. and S.B. Holden (1994). Quantifying Gen-
eralization in Linearly Weighted Networks, Complex Systems, vol.8, 91-114. 
[Atkeson et al, 1997] Atkeson, C.G., A.W. Moore and S. Schaal (1997). Locally 
Weighted Learning, Artificial Intelligence Review, vol.11, 11-73. 
[Back, 1996] Back, T. (1996). Evolutionary Algorithms in Theory and Practice, Ox-
ford University Press, New York, NY. 
[Back and HofFmeister, 1991] Back, T. and F. HofFmeistcr (1991). Extended Selection 
Mechanisms in Genetic Algorithms, In: Belew, R.K. and L.B. Booker (Eds.), Proc. 
Fourth Int. Conf. on Genetic Algorithms, 
Morgan Kaufmann, San Mateo, CA, 
92-99. 

296 
REFERENCES 
[Back et al., 2000] Back, T., D.B. Fogcl and Z. Michalcwicz (2000). 
Evolutionary 
Co'iri'putaMon 1: Basic Algorithms and Operators, lOP Publ., Bristol, UK. 
[Baker, 1987] Baker, J.E. (1987). Reducing Bias and Inefficiency in the Selection 
Algorithm, In: Grefenstette, J.J. (Ed.), Proc. Second Int. Conf. on Genetic Algo-
rithms and their Applications, Lawrence Erlbaum Assoc, Hillsdale, NJ, 14-21. 
[Banzhaf et al., 1998] Banzhaf, W., P. Nordin,P., R.E. Keller and F.D. Franconc 
(1998). Genetic Programming: An Introduction. 
On the Automatic 
Evolution of 
Computer Programs and Its Applications, Morgan Kaufmann, San Francisco, CA. 
[Barron, 1988] Barron, A.R. and R.L. Barron (1988). Statistical Learning Networks: 
A Unifying View, In: Eraser, M.D. (Ed.), Proc. 20th Symposium on the Interface-
Computing Science and Statistics, 192-203. 
[Barron and Xiao, 1991] Barron, A.R. and X. Xiao (1991). Discussion on MARS, 
Annals of Statistics, vol.19, 67-82. 
[Bastert et al., 2001] Bastcrt, O., D. Rockmore, P.F. Stadler and G. Tinhofer (2001). 
Landscapes on Spaces of Trees, Working Paper SFWP 01-01-006, Santa Fe Insti-
tute, NM. 
[Battiti, 1989] Battiti, R. (1989). Accelerated Backpropagation Learning: Two Opti-
mization Methods, Complex Systems, vol.3, 331-342. 
[Becker and LeCun, 1989] Becker, S. and Y. LeCun (1989). Improving the Conver-
gence of Back-Propagation Learning with Second-Order Methods, In: Touretzky, 
D., G. Hinton and T. Sejnowski (Eds.), Proc. 1988 Connectionist 
Models 
Summer 
School, Morgan Kaufmann, San Mateo, CA, 29-37. 
[Berigio et al., 1994] Bengio, Y., P. Simard and P. Frasconi (1994). Learning Long-
Term Dependencies with Gradient Descent is Difficult, IEEE Trans, on Neural 
Networks, vol.5, N:2, 157-166. 
[Bersini and Varela, 1991] Bersini, H. and F. Varela (1991). Hints for Adaptive Prob-
lem Solving Gleaned from Immune Networks. In: Schwefel, H.-P. and H.M. Muh-
lenbein (Eds.), Proc. First Int. Conf. Parallel Problem Solving from Nature PPSN 
I, Springer, Berhn, 343-354. 
[Billings and Zhu, 1991] Billings,S.A. and Zhu,Q.M. (1991). Rational Model Iden-
tification using an Extended Least Squares Algorithm, International 
Journal of 
Control, vol.54, N:3, pp.529-546. 
[Billings and Zhu, 1995] Bilhngs, S.A. and Q.M. Zhu (1995). Model Validation Tests 
for Multivariable Nonlinear Models Including Neural Networks, Int. Journal of 
Control, vol.62, 749-766. 
[Bishop, 1995] Bishop, C. (1995). Neural Networks for Pattern Recognition, Oxford 
University Press, Oxford, UK. 
[Bishop and Qazaz, 1997] Bishop, CM. and C.Z. Qazaz (1997). Regression with 
Input-Dependent Noise: A Bayesian Treatment, In: Touretzky, D.S., M.C. Mozer 
and M.E. Hasselmo (Eds.), Advances in Neural Information 
Processing Systems 9, 
The MIT Press, Cambridge, MA, 347-353. 

REFERENCES 
297 
[Blickle and Thielc, 1997] Blickle, T. and L. Thiele (1997). A Comparison of Selection 
Schemes used in Evolutionary Algorithms, Evolutionary 
Computation, vol.4, N:4, 
361-394. 
[Bohm and Geyer-Schulz, 1996] Bohm, W. and A. Geyer-Schulz (1996). Exact Uni-
form Initiahzation For Genetic Programming, In: Belew, R.K. and M.D. Vose 
(Eds.), Foundations of Genetic Algorithms FOGA-4, Morgan Kaufmann, San Ma-
teo, CA, 379-407. 
[Box and Jenkins, 1970] Box, G.E.P. and G.M. Jenkins (1970). Time Series 
Analysts 
Forecasting and Control, Holden-Day, San Francisco, CA. 
[Box and Pierce, 1970] Box, G.E.P. and D.A. Pierce (1970). Distribution of Residual 
Correlations in Autoregressivc-Integrated Moving Average Time Series Models, 
Journal of the American Statistical Association, vol.65, 1509-1526. 
[Braess, 1986] Braess, D. (1986). Nonlinear Approximation 
Theory, Springer-Verlag, 
Berlin. 
[Brock et al., 1996] Brock, W.A., W.D. Dechert, J.A. Sheinkman and B. LeBaron 
(1996). A Test for Independence Based on the Correlation Dimension, 
Econometric 
Reviews, vol.15, N:3, 197-235. 
[Buntine and Weigend, 1991a] Buntine, W.L. and A.S. Weigend (1991). Computing 
Second Order Derivatives in Feed-forward Networks- A review, IEEE Trans, on 
Neural Networks, vol.5, N:3, 480-488. 
[Buntine and Weigend, 1991b] Buntine, W.L. and A.S. Weigend (1991). Bayesian 
Back-propagation, Complex Systems, vol.5, 603-643. 
[Catfolis, 1993] Catfolis, T. (1993). A Method for Improving the Real-Time Recurrent 
Learning Algorithm, Neural Networks, vol.6, N:6, 807-822. 
[Chakraborty et al., 1996] Chakraborty, U.K., K. Deb, and M. Chakraborty (1996). 
Analysis of Selection Algorithms: A Markov Chain Approach, Evolutionary 
Com-
putation, vol4, N:2, 133-167. 
[Chen and Manry, 1993] Chen, M.S. and M.T. Manry (1993). Conventional Mod-
elhng of the Multi-Layer Perceptron Using Polynomial Basis Functions, IEEE 
Trans, on Neural Networks, vol.4, N:l, 164-166. 
[ShuChen, 2002] Chen, S.-H. (Ed.) (2002). Genetic Algorithms and Genetic Program-
ming in Computational 
Finance, Kluwer Academic Publ., Boston, MA. 
[Chen et al, 1990] Chen, S., S.A. Bilhngs and P.M. Grant (1990). Non-hnear Systems 
Identification using Neural Networks, Int. J. Control, vol.51, N:6, 1191-1214. 
[Chen et al, 2004] Chen, S., X. Hong, C.J. Harris and P.M. Sharkey (2004). Sparse 
Modelhng using Orthogonal Forward Regression with PRESS Statistic and Regu-
larization, IEEE Trans. Systems, Man and Cybernetics, B, vol.34, N:2, 898-911. 
[Cherkassky et al., 1999] Cherkassky, V., X. Shao, F.M. Mulier and V. Vapnik (1999). 
Model Complexity Control for Regression Using VC Generalization Bounds, IEEE 
Trans, on Neural Networks, vol.10, N:5, 1075-1088. 

298 
REFERENCES 
[Cotter, 1990] Cotter, N.E. (1990). The Stone-Weierstrass Theorem and its Apphca-
tion to Neural Networks, IEEE Trans, on Neural Networks, vol.1, N:4, 290-295. 
[Cramer, 1985] Cramer, N.L. (1985). A Representation for the Adaptive Generation 
of Simple Sequential Programs, In: Grefenstette, J.J. (Ed.), Proc. First Int. Conf. 
on Genetic Algorithms and their Applications, Lawrence Erlbaum Assoc., Hillsdale, 
NJ, 183-187. ' 
[Craven and Wahba, 1979] Craven, P. and G. Wahba. (1979). Smoothing Noisy Data 
with Spline Functions: Estimating the Correct Degree of Smoothing by the Method 
of Generalized Cross-Vahdation, Numerishe Math., vol.31, 377-403. 
[Cristiani and Shawe-Taylor, 2000] Cristiani, N. and J. Shawe-Taylor (2000). An In-
Production to Support Vector Machines and other kernel-based learning 
methods, 
Cambridge University Press, Cambridge, UK. 
[Cybenko, 1989] Cybenko, G. (1989). Approximations by Superpositions of a Sig-
moidal Function, Mathematics 
of Control, Signals and Systems, vol.2, 303-314. 
[Darken and Moody, 1990] Darken, C. and J. Moody (1990). Note on Learning Rate 
Schedules for Stochastic Optimization, In: Touretzky, D.S. (Ed.), Advances 
in 
Neural Inf. Processing Systems NIPS-2, Morgan Kauffman, CA, 832-838. 
[Davis, 1975] Davis, P.J. (1975). Interpolation and Approximation, 
Dover, New York. 
[Davis and Principe, 1993] Davis, T. and J. Principe (1993). A Markov Framework 
for the Simple Genetic Algorithm, Evolutionary 
Computation, vol.1, N:3, 269-288. 
[DeBoer and Hogeweg, 1989) DeBoer, R.G. and P. Hogewcg (1989). Idiotypic Net-
works Incorporating T-B Cell Cooperation. The Condition for Percolation, Journal 
of Theoretical Biology, vol.139, 17-38. 
[de Freitas et al., 2000b] de Freitas, N., M. Niranjan, A. Gee and A. Doucet (2000). 
Sequential Monte Carlo Methods to Train Neural Network Models, Neural Com-
putation, vol.12, N:4, 933-953. 
[De Jong and Sarma, 1992] De Jong, K.A. and J. Sarma (1992). Generation Gaps Re-
visited. In: Whitley, D. (Ed.), Foundations of Genetic Algorithms FOGA-2, Morgan 
Kaufmann, San Mateo, CA, 19-28. 
[de Menezes and Nikolaev, 2006] de Menezes, L. and N. Nikolaev (2006). Forecasting 
with Genetically Programmed Polynomial Neural Networks, Int. J. of Forecasting, 
vol.22, N:2, 2006. 
[Dempster et al., 1977] Dempster, A.P., N.M. Laird and D.B. Rubin (1977). Maxi-
mum Likelihood from Incomplete Data via the EM Algorithm, Journal of the Royal 
Statistical Society B, vol.39, N:l, 1-38. 
[De Veaux et al., 1998] De Veaux, R.D., J. Schumi, J. Schweinsberg and H.U. Lyle 
(1998). Prediction Intervals for Neural Networks via Nonlinear Regression, Tech-
nometrics, vol.40, N:4, 273-282. 
[Dickey and Fuller, 1981] Dickey, D. and W. Fuller (1981). Likelihood Ratio Statistics 
for Autoregressive Time Series with a Unit Root, Econometrica, vol.49, 1057-1072. 

REFERENCES 
299 
[Doucct et al., 2001] Doucct, A., N. de Freitas and N. Gordon (2001). Sequential 
Monte Carlo Methods in Practice, Springer-Verlag, Berlin. 
[Ebner, 1999] Ebner, M. (1999). On the Search Space of Genetic Programming and 
its Relation to Nature's Search Space, In: Proc. 1999 Congress on Evolutionary 
Computation 
CEC-1999, IEEE Press, Piscataway, NJ, 1357-1361. 
[Efron and Tibshirani, 1989] Efron, B. and R.J. Tibshirani (1989). An 
Introduction 
to the Bootstrap, Chapman and Hall, New York, NY. 
[Eiben and Smith, 2003] Eiben, A.E. and J.E. Smith (2003). Introduction to Evolu-
tionary Computing, Springer-Verlag, Berlin. 
[Elder and Pregibon, 1996] Elder IV, J.F. and D. Pregibon (1996). A Statistical Per-
spective on Knowledge Discovery in Databases, In: Fayyad, U.M., G. Piatetsky-
Shapiro, P. Smyth and R. Uthurusamy (Eds.), Advances in Knowledge 
Discovery 
and Data Mining, The AAAI Press/The MIT Press, Chapter 4, 83-113. 
[Elder and Brown, 2000] Elder IV, J.F. and D.E. Brown (2000). Induction and Poly-
nomial Networks, In: Eraser, M.D. (Ed.), Network Models for Control and Process-
ing, Intellect Books, Exeter, UK, 143-198. 
[Elman, 1990] Elman, J.L. (1990). Finding Structure in Time, Cognitive 
Science, 
vol.14, 179-211. 
[Fadeev and Fadeeva, 1963] Fadeev, D.K. and V.N. Fadeeva (1963). 
Computational 
Methods of Linear Algebra, W.H.Freeman, San Francisco, CA. 
[Faraway and Chatfield, 1998] Faraway, J. and C. Chatfield (1998). Time Series Fore-
casting with Neural Networks: A Comparative Study using the Airline Data, Ap-
plied Statistics, vol.47, N:2, 231-250. 
[Farlow, 1984] Farlow ,S.J. (Ed.) (1984). Self-Organizing 
Methods in 
Modeling. 
GMDH Type Algorithms, Marcel Dekker, New York, NY. 
[Farmer et al., 1986] Farmer, J.D., N.H. Packard and A.S. Perelson (1986). The Im-
mune System, Adaptation and Machine Learning. Physica 22D, 187-204. 
[Farmer, 1990] Farmer, J.D. (1990). A Rosetta Stone for Connectionism, Physica 
42D, 153-187. 
[Firmoff et al., 1992] Finnoff, W., F. Hergert and H.G. Zimmermann (1992). Improv-
ing Generalization Performance by Nonconvergent Model Selection Methods, In: 
Aleksander, I. and J. Taylor (Eds.), Neural Networks 2: Proc. of the Int. Conf. on 
Artificial Neural Networks ICANN-92, 
233-236. 
[Fletcher, 1987) Fletcher, R. (1987). Practical Methods of Optimization, 
(2nd ed.), 
John Wiley and Sons, New York. 
[Fogel et al., 1966] Fogel, L.J, A.J. Owens and M.J. Walsh (1966). Artificial 
Intelli-
gence through Simulated Evolution, Wiley, New York, NY. 
[Fogel, 1995] Fogel, D.B. (1995). Phenotypes, Genotypes, and Operators in Evolu-
tionary Computation, In: Proc. of the 1995 IEEE Int. Conf. on Evolutionary 
Com-
putation ICEC'95, IEEE Press, Piscataway, NJ, 193-198. 

300 
REFERENCES 
[Fogel, 1999] Fogel, D.B. (1999). Evolutionary 
Computation: 
Toward a New Philoso-
phy of Machine Intelligence, IEEE Press, Piscataway, NJ. 
[Gabor et al., 1961] Gabor, D., W. Wildes and R. Woodcock (1961). A Universal 
Nonlinear Filter, Predictor and Simulator which Optimizes Itself by a Learning 
Process, Proceedings lEE, vol.lOSB, 422-438. 
[Geman et al., 1992] Geman, S, E. Bienenstock and R. Doursat (1992). Neural Net-
works and the Bias/Variance Dilemma, Neural Computation, vol.4, N:l, 1-58. 
[Glover, 1989] Glover, F. (1989). Tabu Search, OF(.SA Journal on Computing, vol.1, 
N:3, 190-206. 
[Goldberg, 1989] Goldberg, D.E. (1989). Genetic Algorithms in Search, 
Optimization 
and Machine Learning, Addison-Wesley Pub., R.eading, MA. 
[Goldberg et al., 1989] Goldberg, D.E., B. Korb and K. Deb (1989). Messy Genetic 
Algorithms; Motivation, Analysis and First Results, Complex Systems, vol.3, 493-
530. 
[Goldberg and Deb, 1991] Goldberg, D.E. and K. Deb (1991). A Comparative Analy-
sis of Selection Schemes used in Genetic Algorithms, In: Rawlins, G. (Ed.), Foun-
dations of Genetic Algorithms FOGA-1, Morgan Kaufmann, CA, 69-93. 
[Gomez-Ramirez et al, 1999] G6mez-Ramirez, A. Poznyak, A. Gonzalez-Yunes and 
M. Avila-Alvarez (1999). Adaptive Archictecture of Polynomial Artificial Neural 
Network to Forecast Nonlinear Time Series, In: Proc, of 1999 Congress on Evolu-
tionary Computation 
CEC-99, IEEE Press, Piscataway, NJ, vol.1, 317-324. 
[Gosh and Shin, 1992] Gosh, J. and Y. Shin (1992). Efficient Higher Order Neural 
Networks for Classification and Function Approximation, Int. Journal of Neural 
Systems, vol.3, N:4, 323-350. 
[Grassberger and Procaccia, 1983] Grassberger, P. and I. Procaccia (1983). Charac-
terization of Strange Attractors, Physical Review Letters, vol.50, N:5, 346-349. 
[Green et al, 1988] Green, D.G., R.E. R.eichelt and R.H. Bradbury (1988). Statistical 
Behaviour of the GMDH Algorithm, Biometrics, vol.44, 49-69. 
[Grefenstette and Baker, 1989] Grefenstette, J.J. and J.E. Baker (1989). How Ge-
netic Algorithms Work: A Critical Look at Implicit Parallelism, In: Schaffer, J.D. 
(Ed.), Proc. Third Int. Conf. on Genetic Algorithms and their Applications, Mor-
gan Kaufmann, San Mateo, CA, 20-27. 
[Ham and Kostanic, 2000] Ham, F.M. and I. Kostanic (2000). Principles of Neuro-
computing for Science and Engineering, McGraw-Hill Co., New York, NY. 
[Hammer, 2001] Hammer, B. (2001). Generalization Abihty of Folding Networks, 
IEEE Trans, on Knowledge and Data Engineering, vol.13, N:2, 196-206. 
[Hassibi and Stork, 1993] Hassibi, B. and D.G. Stork (1993). Second Order Deriva-
tives for Network Pruning: Optimal Brain Surgeon, In: Hanson, J, J. Cowan and 
C. Lee Giles (Eds.), Advances in Neural Inf. Processing Systems NIPS-5, Morgan 
Kauffman, San Mateo, CA, 164-171. 

REFERENCES 
301 
[Haykin, 1999] Haykiri, S. (1999). Neural Networks. A Comprehensive 
Foundation, 
2nd edition, Prentice Hall, Upper Saddle River, NJ. 
[Haykin, 2001] Haykin, S. (Ed.) (2001). Kalman Filtering and Neural Networks, John 
Wiley and Sons, New York, NY. 
[Hertz et al., 1991] Hertz, J., A. Krough and R.G. Palmer (1991). Introduction to the 
Theory of Neural Computation, Addison-Wesley, Redwood City, CA. 
[Heywood and Noakes, 1996] Heywood, M. and P. Noakes (1996). A Framework for 
Improved Training of Sigma-Pi Networks, IEEE Trans, on Neural Networks, vol.6, 
N:4, 893-903. 
[Hildebrand, 1987] Hildebrand, F.B. (1987). Introduction 
to Numerical 
Analysis, 
Dover Publ., New York, NY. 
[Holden and Rayrier, 1992] Holden, S.B. and P.J.W. Rayncr (1992), Generalization 
and Learning in Volterra and Radial Basis Function Networks: A Theoretical 
Analysis, In: Proc. IEEE Int. Conf. on Acoustics, Speech and Signal Processing, 
vol.2, II-273-II-276. 
[Holland, 1975] Holland. J. (1975). Adaptation in Natural and Artificial Systems, The 
University of Michigan Press, Ann Arbor, MI. 
[Hordijk, 1996] Hordijk, W. (1996). A Measure of Landscapes, Evolutionary 
Compu-
tation, vol.4, N:4, 335-360. 
[Hornik et al., 1989] Hornik, K., M. Stinchcombe and H. White (1989). Multilayer 
Feedforward Networks are Universal Approximators, Neural Networks, vol.2, N:5, 
359-366. 
[Hubner et al., 1994] HUbner, U., C.-O. Weiss, N.B. Abraham and D. Tang (1994). 
Lorenz-Like Chaos in NH3-FIR Lasers, In: Weigend, A.S. and N.A.Gershenfeld 
(Eds.), Time Series Prediction: Forecasting the Future and Understanding the Past, 
Reading, MA: Addison-Wesley, 73-104. 
[Hwang and Ding, 1997] Hwang, J.T.G. and A.A. Ding (1997). Prediction intervals 
for artificial neural networks. Journal of the American 
Statistical 
Association, 
vol.92, N:438, 748-757. 
[Iba and Sato, 1992] Iba, H. and T. Sato (1992). Meta-level Strategy for Genetic 
Algorithms based on Structured Representations, In: Proc. 2nd Pacific Rim Int. 
Conf. on Artificial Intelligence, 548-554. 
[Iba et al,1993] Iba, H., T. Kurita, H. deGaris and T. Sato (1993). System Identifi-
cation using Structured Genetic Algorithms, In: Proc. of 5th Int. Joint Conf. on 
Genetic Algorithms ICGA'93, San Mateo, CA: Morgan Kaufmann, 279-286. 
[Iba, 1994] Iba, H. (1994). Random Tree Generation for Genetic Programming, In: 
Voigt, H.-M., W. Ebehng, I. Rechenbcrg, and H.-P. Schwefel (Eds.), Parallel Prob-
lem Solving from Nature PPSN-IV, LNCS 1141, Springer-Verlag, Berlin, 144-153. 
[Iba and de Garis, 1994] Iba, H. and H. de Garis (1994). Genetic Programming Using 
the Minimum Description Length Principle. In: Kinnear Jr., K. (Eds.), Advances 
in Genetic Programming, The MIT Press, Cambridge, MA, 265-284. 

302 
REFERENCES 
[Iba et.al, 1995) Iba, H., H. dc Garis and T. Sato (1995). Temporal Data Process-
ing using Genetic Programming, In: Eshelman, L. (Ed.), Proc. 6th Int. Conf. on 
Genetic Algorithms ICGA-95, San Mateo, CA: Morgan Kaufmann, 279-286. 
[Iba et.al, 1996] Iba, H., H. de Garis and T. Sato (1996). Numerical Approach to 
Genetic Programming for System Identification, Evolutionary 
Computation, vol.3, 
N:4, 417-452. 
[Iba et.al, 1996b] Iba, H., H. deGaris and T. Sato (1996). Extending Genetic Pro-
gramming witfi Recombinative Guidance, In: Angeline, P. and K. Kinnear (Eds.), 
Advances in Genetic Programming 2, The MIT Press, Cambridge, MA, 69-88. 
[Igel, 1998] Igel, C. (1998). Causahty of Hierarchical Variable Length Representa-
tions, In: Proc. 1998 IEEE Int. Conf. on Evolutionary 
Computation^ 324-329. 
[Ivakhnenko, 1971] Ivakhnenko, A.G. (1971). Polynomial Theory of Complex Sys-
tems, IEEE Trans, on Systems, Man, and Cybernetics, vol.1, N:4, 364-378. 
[Jacobs, 1988] Jacobs, R.A. (1988). Increased Rates of Convergence through Learning 
Rate Adaptation, Neural Networks, vol.1, 295-307. 
[Jolliffe, 1986] Jolliffe, I.T. (1986). Principal Component Analysis, Springer-Verlag, 
New York, NY. 
[Jones and Forrest, 1995] Jones, T.C. and S. Forrest (1995). Fitness Distance Corre-
lation as a Measure of Problem Difficulty for Genetic Algorithms, In: Eshelman, 
L.J. (Ed.), Proc Sixth Int. Conf. on Genetic Algorithms ICGA-95, Morgan Kauf-
mann, San Mateo, CA, 184-192. 
[Jones, 1995] Jones, T.C. (1995). Evolutionary Algorithms, Fitness Landscapes and 
Search, PhD dissertation. The University of New Mexico, Albuquerque, NM. 
[Jordan, 1986] Jordan, M.I. (1986). Attractor Dynamics and Parallelism in a Con-
nectionist Sequential Machine, In: Proc. of the Eighth Conference of the Cognitive 
Science Society, Lawrence Erlbaum, 531-546. 
[Judd, 1990] Judd, J.S. (1990). Neural Network Design and the Complexity of Learn-
ing, The MIT Press, Cambridge, MA. 
[Kargupta and Smith, 1991] Kargupta, H. and R.E. Smith (1991). System Identifica-
tion with Evolving Polynomial Networks, In: Belew, R.K. and L.B. Booker (Eds.), 
Proc. 4th Int. Conf. Genetic Algorithms, San Mateo, CA: Morgan Kaufmann, 370-
376. 
[Kasabov, 2002] Kasabov, N. (2002). Evolving connectionist 
systems: Methods and 
applications in bioinformatics, 
brain study and intelligent 
machines, 
Springer-
Verlag, London. 
[Kauffman, 1989] Kauffman, S.A. (1989). Adaptation on Rugged Fitness Landscapes, 
In: Stein, D.L. (Ed.), Lectures in the Science of Complexity, vol.1, Addison-Wesley 
Longman, 527-618. 
[Keith and Martin, 1994] Keith, M.J. and M.C. Martin (1994). Genetic Program-
ming in C-f-f-: Implementation Issues, In: Kinnear Jr., K.E. (Ed.), Advances in 
Genetic Programming, The MIT Press, Cambridge, MA, 285-310. 

REFERENCES 
303 
[Kendall and Ord, 1983) Kendall, M. and J.K. Ord (1983). Time Series, 3rd ed., 
Edward Arnold, Kent, UK. 
[Kinnear, 1994] Kinnear, K.E. (1994). Fitness Landscapes and Difficulty in Genetic 
Programming, In: Proc. 1994 IEEE World Conference on Computational 
Intelli-
gence, IEEE Press, Piscataway, NJ, vol.1, 142-147. 
[Kirkpatrick ct al., 1983] Kirkpatrick, S., C D . Gclatt Jr. and M.P. Vecchi (1983). 
Optimization by Simulated Annealing, Science, vol.220, 671-680. 
[Kolmogorov, 1957] Kolmogorov, A.N. (1957). On the Representation of Continuous 
Functions of Several Variables by Superpositions of Continuous Functions of One 
Variable and Addition, Dokl. Akad. Nauk. SSSR, vol.114, N:5, 953-956. 
[Kolmogorov and Fomin, 1999] Kolmogorov, A.N. and S.V. Fomin (1999). Elements 
of the Theory of Functions and Functional Analysis, Dover Publ., New York, NY. 
[Koza, 1992] Koza, J.R,. (1992). Genetic Programming: On the Programming of Com-
puters by Means of Natural Selection, The MIT Press, Cambridge, MA. 
[Koza, 1994] Koza, J.R. (1994). Genetic Programming 11: Automatic 
Discovery of 
Reusable Programs, The MIT Press, Cambridge, MA. 
[Koza et al., 1999] Koza, J.R., F.H. Bennett, D. Andre and M.A. Keane (1999). Ge-
netic Programming III: Darwinian Invention and Problem Solving, Morgan Kauf-
mann, San Mateo, CA. 
[Koza et al., 2003] Koza,J.R., Keane,M.A., Streeter,M.J., Mydlowec,W., Yu,J. and 
Lanza,G. (2003). Genetic Programming IV: Routine Human-Competitive 
Machine 
Intelligence, Kluwer Academic Publishers, Boston, MA. 
[Lanczos, 1957] Lanczos, C. (1957). Applied Analysis, Prentice-Hall, London, UK. 
[Lang et al., 1990) Lang, K.J., A.H. Waibel and G. Hinton (1990). A Time-delay 
Neural Network Architecture for Isolated Word Recognition, Neural 
Networks, 
vol.3, 23-44. 
[Langdon and Poh, 2002) Langdon, W.B. and R. Poli (2002). Foundations of Genetic 
Programming, Springer-Verlag, Heidelberg. 
[LeCun et al., 1990) LeCun, Y., J. Denker, S. SoUa, R.E. Howard and L.D. Jackel 
(1990). Optimal Brain Damage, In: Touretzky, D.S. (Ed.), Advances in Neural Inf. 
Processing Systems NIPS-2, Morgan KaufFman, San Mateo, CA, 598-605. 
[Lee and Jeng, 1997] Lee, T.-T. and J.-T. Jeng (1997). Chebyshev Polynomials Based 
(CPB) Unified Model Neural Networks for Function Approximation, In: Rogers, 
S.K. and D.W. R.uck (Eds.), Proc. SPIE: Applications 
and Science of Artificial 
Neural Networks III, vol.3077, 372-381. 
[Leung and Haykin, 1993] Leung, H. and S. Haykin (1993). Rational Function Neural 
Network, Neural Computation, vol.5, N:6, 928-938. 
[Levenberg, 1944] Levenberg, K. (1944). A Method for the Solution of Certain Non-
linear Problems in Least Squares, Quarterly Journal of Applied 
Mathematics, 
vol.11, N:2, 164-168. 

304 
REFERENCES 
[Liu et al, 1998) Liu, G.P., V. Kadirkamanathan and S.A. Billings (1998). On-line 
Identification of Nonlinear Systems using Volterra Polynomial Basis Function 
Neural Networks, Neural Networks, vol.11, N:9, 1645-1657. 
[Ljung and Box, 1978] Ljung, G.M. and G.E.P. Box (1978). On a Measure of Lack 
of Fit in Time Series Models, Biometrika, vol.65, 553-564. 
[Ljung and Sodestr5m, 1983] Ljung, L. and T. Sodestrom (1983). Theory and Prac-
tice of Recursive Identification, The MIT Press, Cambridge, MA. 
[Lo and MacKinley, 1988] Lo, A.W., and A.C. MacKinley (1988). Stock Market 
Prices Do Not Follow Random Walks: Evidence from a Simple Specification Test, 
Review of Financial Studies, vol.1, N:l, 41-66. 
[Luke, 2000] Luke, S. (2000). Two Fast Tree-Creation Algorithms for Genetic Pro-
grammimg, IEEE Trans, on Evolutionary 
Computation, vol.4, N:3, 274-283. 
[Mackey and Glass, 1977] Mackey, M.C. and L. Glass (1977). Oscillation and Chaos 
in Physiological Control Systems, Science, vol.197, 287-289. 
[MacKay, 1992a] MacKay, D.J.C. (1992). Bayesian Interpolation, Neural 
Computa-
tion, vol.4, N:3, 415-447. 
[MacKay, 1992b] MacKay, D.J.C. (1992). A Practical Bayesian Framework for Back-
prop Networks, Neural Computation, vol.4, N:3, 448-472. 
[MacKay, 1995] MacKay, 
D.J.C. 
(1995). 
Probable 
Networks 
and 
Plausible 
Predictions- A Review of Practical Bayesian Methods for Supervised Neural Net-
works, Network: Computation in Neural Systems, vol.6, N:3, 469-505. 
[MacKay, 2003] MacKay, D.J.C. (2003). Information 
Theory, Inference and Learning 
Algorithms, Cambridge University Press, Cambridge, UK. 
[Madala and Ivakhnenko, 1994] Madala, H.R. and A.G. Ivakhnenko (1994). Inductive 
Learning Algorithms for Complex Systems Modeling, CRC Press, Boca Raton, FL. 
[Mallows, 1973] Mallows, C.L. (1973). Some Comments on Cp, Technometrics, vol.15, 
661-676. 
[Manderick et al., 1991] Manderick, B., M. deWeger and P. Spiessens (1991). The 
Genetic Algorithm and the Structure of the Fitness Landscape. In: Belew, R.K. 
and L.B. Booker (Eds.), Proc. Fourth Int. Conf. on Genetic Algorithms 
ICGA-91, 
Morgan Kaufmann, San Mateo, CA, 143-150. 
[Mardia et al., 1979] Mardia, K.V., J.T. Kent and J.M. Bibby (1979). 
Multivariate 
Analysis, Academic Press, London. 
[Marin and Sole, 1999] Marin, J. and R.V. Sole (1999). Macroevolutionary Algo-
rithms: A New Optimization Method on Fitness Landscapes, IEEE Trans, on 
Evolutionary 
Computation, vol.3, N:4, 272-286. 
[Marmarelis, 1994] Marmarelis, V.Z. (1994). Three Conjectures on Neural Network 
Implementations of Volterra Models, In: Marmarelis, V.Z. (Ed.), Advanced Methods 
of Physiological System Modeling, vol.3. Plenum Press, NY, 261-267. 

REFERENCES 
305 
[Marmarclis and Zhao, 1997] Marmarelis, V.Z. and X. Zhao (1997), Volterra Models 
and Thrcce-Layer Pcrccptrons, IEEE Trans, on Neural Networks, vol.8, N:6, 1421-
1433. 
[Marquardt, 1963] Marquardt, D.W. (1963). An Algorithm for Least-squares Esti-
mation of Non-linear Parameters, Journal of the Society of Industrial and Applied 
Mathematics, vol.11, N:2, 431-441. 
[Matthews and Moschytz, 1994] Matthews, M.B. and G.S. Moschytz (1994). The 
Identification of Nonlinear Discrete-time Fading-memory Systems using Neural 
Network Models, IEEE Trans, on Circuits and Systems-II, 
vol.41, 740-751. 
[Merz and Freisleben, 2000] Merz, P. and B. Freisleben (2000). Fitness Landscapes, 
Memetic Algorithms and Greedy Operators for Graph Bi-Partitioning, Evolution-
ary Computation, vol.8, no.l, 61-91. 
[Michalewics, 1992] Michalewics, 
Z. 
(1992). 
Genetic 
Algorithms+Data 
Struc-
tures = Evolution Programs, Springer-Verlag, New York, NY. 
[Mitchell, 1997] Mitchell, T. (1997). Machine Learning, McGraw Hill, New York, NY. 
[Moody, 1992] Moody, J. (1992). The Effective Number of Parameters: An Analysis 
of Generalization and Regularization in Nonlinear Learning Systems, In: Moody, 
J.E., S.J. Hanson, and R.P. Lippmann (Eds.), Advances in Neural 
Information 
Processing Systems 4, Morgan Kaufmann, San Mateo, CA, 847-854. 
[Moscato and Norman, 1992] Moscato, P. and M.G. Norman (1992). A Memetic Ap-
proach for the Traveling Salesman Problem:, In: Valero, M., E. Onate, M. Jane, 
J.L. Larriba and B. Suarez (Eds.), Parallel Computing and Transputer 
Applica-
tions, lOS Press, Amsterdam, 187-194. 
[Muller et al., 1999] Muller, K.-R., A.J. Smola, G. Ratsch, B. Scholkopf, J. Kohlmor-
gen and V. Vapnik (1999). Predicting Time Series with Support Vector Machines, 
In: Scholkopf, B., C.J.C, Burges and A.J. Smola (Eds.), Advances in Kernel 
Methods- Support Vector Learning, Cambridge, MA: MIT Press, 243-254. 
[Muller and Lemke, 2000] Muller, J.-A. and F. Lemke (2000). Self-Organising 
Data 
Mining, Traffbrd Publ., Canada. 
[Muhlenbein and Schlierkamp-Voosen, 1995] Muhlenbein, H. and D. Schlierkamp-
Voosen (1995). Predictive Models for the Breeder Genetic Algorithm: I, Evolu-
tionary Computation, vol.1, N:l, 25-49. 
[Myers, 1990] Myers, R.H. (1990). Classical and Modern Regression with Applica-
tions, Duxbury Press, PWS-KENT Publ., CA. 
[Nabney, 2002] Nabney, I.T. (2002). Netlab: Algorithms 
for Pattern 
Recognition, 
Springer-Verlag, London, UK. 
[Namatame and Ueda, 1992] Namatame, A. and N. Ueda (1992). Pattern Classifi-
cation with Chebychev Neural Networks, Int. J. of Neural Networks, vol.3, N:l, 
23-31. 

306 
REFERENCES 
[Narendra and Parthasarathy, 1990] Narcridra, K.S. and K. Parthasarathy (1990). 
Identification and Control of Dynamical Systemns Using Neural Networks, IEEE 
Trans, on Neural Networks, vol.1, N:l, 4-27. 
[Neal, 1996] Neal, R.M. (1996). Bayesian 
Learning for Neural Networks, 
Lecture 
Notes in Statistics No. 118, Springer-Verlag, New York. 
[Neuneier and Zimmerrnann, 1998] Neuneier, R. and H.G. Zimmermann (1998). How 
to Train Neural Networks, In: Orr, G.B. and K.-R. MUller (Eds.), Neural Networks: 
Tricks of the Trade, LNCS-1524, Springer-Verlag, Berlin, 373-423. 
[Ng and Lippmann, 1991] Ng, K. and R.P. Lippmann (1991). A Comparative Study 
of the Practical Characteristics of Neural Network and Conventional Pattern Clas-
sifiers, In: Lippmann, R.P., J. Moody and D.S. Touretzky (Eds.), Advances in 
Neural Inf. Proc. Systems 3, San Mateo, CA: Morgan Kaufmann, 1991, 970-976. 
[Ngia and Sjoberg, 2000] Ngia, L.S.H. and J. Sjoberg (2000). Efficient Training 
of Neural Nets for Nonlinear Adaptive Filtering using a Recursive Levenberg-
Marquardt Algorithm, IEEE Tr. on Signal Processing, vol.48, N:7, 1915-1927. 
[Nikolaev and Slavov, 1998] Nikolaev, N.Y. and V. Slavov (1998). The Dynamics of 
Biased Inductive Genetic Programming, In: Koza, J. et al. (Eds.) Proc. Third 
Annual Conf. on Genetic Programming, GP-98, Morgan Kaufmann, CA, 260-268. 
[Nikolaev et al, 1999] Nikolaev, N., H. Iba and V. Slavov (1999). Inductive Genetic 
Programming with Immune Network Dynamics, In: Spector, L., W.B. Langdon, 
U.-M. O'Reilly and P.J. Angeline (Eds.), Advances In Genetic Programming 
3, 
Chapter 15, MIT Press, Cambridge, MA, 355-376. 
[Nikolaev and Iba, 2001a] Nikolaev, N.Y. and H. Iba (2001). Regularization Ap-
proach to Inductive Genetic Programmimg, IEEE Trans, on Evolutionary 
Com-
mutation, vol.5, N:4, 359-375. 
[Nikolaev and Iba, 2001b] Nikolaev, N.Y. and H. Iba (2001). Accelerated Genetic 
Programming of Polynomials, Genetic Programming 
and Evolvable 
Machines, 
Kluwer Academic Publ., vol.2, N:3, 231-257. 
[Nikolaev and Iba, 2001c] Nikolaev, N.Y. and H. Iba (2001). Genetic Programming of 
Polynomial Harmonic Networks using the Discrete Fourier Transform, Int. Journal 
of Neural Systems, vol.12, N:5, 399-410. 
[Nikolaev and Iba, 2001d] Nikolaev, N.Y. and H. Iba (2001). Genetic Programming 
using Chebishev Polynomials, In: Spector, L., E.D. Goodman, A. Wu, W.B. Lang-
don, H.-M. Voigt, M. Gen, S. Sen, M. Dorigo, S. Pezeshk, M.H. Garzon, and 
E. Burke (Eds.), Proc. of the Genetic and Evolutionary 
Co'iriputation Conference 
GECCO-2001, Morgan Kaufmann Publ, San Francisco, CA, 89-96. 
[Nikolaev and Iba, 2003] Nikolaev, N.Y. and H. Iba (2003). Learning Polynomial 
Feedforward Neural Networks by Genetic Programming and Backpropagation, 
IEEE Trans, on Neural Networks, vol.14, N:2, 337-350. 
[Nikolaev and Tino, 2005a] Nikolaev, N.Y. and P. Tino (2005). A Recursive Rele-
vance Vector Machine for Sequential Bayesian Learning, Tech Report TR 29-05-
2005, Goldsmiths College, University of London, London. 

REFERENCES 
307 
[Nilsson, 1980) Nilsson, N.J. (1980). Principles of Artificial Intelligence, Tioga Pub-
lishing Company, Palo Alto, CA. 
[Nix and Weigend, 1995] Nix, D.A. and A.S. Weigend (1995). Learning Local Error 
Bars for Nonlinear Regression, In: Tesauro, G., D.S. Touretzky, and T.K. Leen 
(Eds.), Advances in Neural Inf. Processing Systems NIPS-7, MIT Press, Cam-
bridge, MA, 489-496. 
[O'Reilly, 1995] O'Reilly, U.-M. (1995). An Analysis of Genetic Programming, PhD 
Dissertation, Carleton University, Ottawa, Canada. 
[Pao, 1989] Pao, Y.H. (1989). Adaptive Pattern Recognition and Neural 
Networks, 
Addison-Wesley, Reading, MA. 
[Paton, 1997] Paton, R.C. (1997). Principles of Genetics, In: Back, T., D.B. Fogel, 
and Z. Michalewicz (Eds.), Handbook of Evolutionary Computation, lOP Publ. and 
Oxford University Press, Oxford, UK, A2.2:1-2:9. 
[Pearlrrmtter, 1994] Pearlmutter, B.A. (1994). Fast Exact Multiplication by the 
Hessian, Neural Computation, vol.6, N:2, 147-160. 
[Pedersen and Hansen, 1995] Pedersen, M.W. and L.K. Hansen (1995). Recurrent 
Networks: Second Order Properties and Pruning, In: Tesauro,G., D. Touretzky 
and T. Leen (Eds.), Advances in Neural Information 
Processing Systems 
NIPS-7, 
The MIT Press, Cambridge, MA, 673-680. 
[Pham and Liu, 1995] Pham, D.T. and X. Liu (1995). Neural Networks for Predic-
tion, Identification 
and Control, London, UK: Springer-Verlag. 
[Polak, 1971] Polak, E. (1971). Computational 
Methods in Optimization: 
A Unified 
Approach, Academic Press, New York. 
[Poh and Langdon, 1998] Poli, R. and W.B. Langdon (1998). Schema Theory for 
Genetic Programming with One-point Crossover and Point Mutation, 
Evolutionary 
Computation, vol.6, N:3, 231-252. 
[Press et al., 1992] Press, W.H. B.P. Flannery, S.A. Teukolski and W.T. Vetterling 
(1992). Numerical Recipes in C: The Art of Scientific Computing, 2nd ed., Cam-
bridge University Press, Cambridge, England. 
[Puskorius and Felkamp, 1994] Puskorius, G.V. and L.A. Felkamp (1994). Neurocon-
trol of nonlinear dynamical systems with Kalman Filter-trained recurrent networks, 
IEEE Trans, on Neural Networks, vol.5, N:2, 279-297. 
[Quifionero-Candela, 2004] Quifionero-Candela, 
C.E. 
(2004). 
Learning 
with 
Uncertainty-Gaussian 
Processes and Relevance 
Vector Machines, 
PhD thesis 
IMM-PHD-2004-135, Technical University of Denmark. 
[Rayner and Lynch, 1989] Rayner, P.J.W. and M.R. Lynch (1989). A New Connec-
tionist Model Based on a Non-linear Adaptive Filter, In: Proc. IEEE Int. Conf 
Acoustics, Speech and Signal Processing, 1191-1194. 
[Riolo and Worzel, 2003] Riolo, R.L. and B. Worzel (Eds.) (2003). Genetic Program-
ming Theory and Practice, Kluwer Academic Publ., Boston, MA. 

308 
REFERENCES 
[Rissancn, 1989] Rissaneri, J. (1989). Stochastic 
Complexity in Statistical 
Inquiry, 
World Scientific Publishing, Singapore. 
[Rivals and Personnaz, 2000] Rivals, I. and L. Personnaz (2000). Construction of 
Confidence Intervals for Neural Networks Based on Least Squares Estimation, 
Neural Networks, vol 13, 463-484. 
[Rodriguez-Vazquez, 1999] Rodriguez-Vazquez, K. (1999). Multiobjective 
Evolution-
ary Algorithms in Non-Linear System Identification, 
PfiD Thesis, Dept. of Auto-
matic Control and Systems Engineering, University of Sheffield, UK. 
[Rodriguez-Vazquez and Fleming, 2000] Rodriguez-Vazquez, K. and P.J. Fleming 
(2000). Use of Genetic Programming in the Identification of Rational Model Struc-
tures, In: Poll, R., W. Banzhaf, W.B. Langdon, J.F. Miller, P. Nordin and T.C. Fog-
arty (Eds.), Genetic Programming, Proc. of EuroGP'2000, LNCS 1802, Springer-
Verlag, Berlin, 181-192. 
[Rosea and Ballard, 1995a] Rosea, J.P. and D.H. Ballard (1995). Causality in Genetic 
Programming, In: Eshelman, L.J. (Ed.), Proc. Sixth Int. Conf. Genetic 
Algorithms, 
Morgan Kaufmann, San Francisco, CA, 256-263. 
[Rosea and BaUard, 1995b] Rosea, J.P. and D.H. Ballard (1995). Discovery of Sub-
routines in Genetic Programming, In: Angeline, P.J. and K. Kinnear Jr. (Eds.), 
Advances in Genetic Programming II, The MIT Press, Cambridge, MA, 177-202. 
[Rosea and Ballard, 1999] Rosea, 
J.P. 
and 
D.H. Ballard 
(1999). 
Rooted-Tree 
Scfiemata in Genetic Programming, In: Spector, L., W.B. Langdon, U.-M. O'R.eilly 
and P.J. Angeline (Eds,), Advances In Genetic Programming 3, Chapter 11, MIT 
Press, Cambridge, MA, 243-271. 
[Rumelhart et al., 1986] Rumelhart, D.E., G.E. Hinton and R.J. Wilhams (1986). 
Learning Internal Representations by Error Propagation, In: Parallel Distributed 
Processing: Explorations 
in the Micro structure 
of Cognition, vol.1, Rumelhart, 
D.E., et al. (Eds.), The MIT Press, Cambridge, MA, 318-362. 
[Salustowicz and Schmidhuber, 1997] Salustowicz, R. and J. Schmidhuber (1997). 
Probabilistic Incremental Program Evolution, Evolutionary 
Computation, 
vol.5, 
N:2, 123-141. 
[Schetzen, 1980] Schetzen,M. (1980). The Volterra and Wiener Theories of Nonlinear 
Systems, Wiley, New York. 
[Scholkopf and Smola, 2002] Scholkopf, B. and A. Smola (2002). Learning with Ker-
nels, The MIT Press, Cambridge, MA. 
[Schwartz, 1978] Schwartz, G. (1978). Estimating the Dimension of a Model, Annals 
of Statistics, vol.6, 461-464. 
[Schwefel, 1995] Schwefel, H.P. (1995). Evolution and Optimum Seeking, John Wiley 
and Sons, New York, NY. 
[Sendhoff et al., 1997] SendhofF, B., M. Kreutz and W. Seelen (1997). A Condition 
for the Genotype-Phenotype Mapping: Causality, In: T.Back (Ed.), Proc. Seventh 
Int. Conf on Genetic Algorithms ICGA-97, Morgan Kaufman, CA, 73-80. 

REFERENCES 
309 
[Shin and Ghosh, 1995] Shin, Y. and J. Ghosh (1995). Ridge Polynomial Networks, 
IEEE Trans, on Neural Networks, vol.6, N:3, 610-622. 
[Slavov and Nikolaev, 1997] Slavov, V. and N. Nikolaev (1997). Inductive Genetic 
Programming and the Superposition of Fitness Landscapes, In: Back, T. (Ed.), 
Proc. Seventh Int. Conf. on Genetic Algorithms 
ICGA-97, 
Morgan Kaufmann, 
San Mateo, CA, 97-104. 
[Shah et al, 1992] Shah, S., E. Palmieri and M. Datum (1992). Optimal Filtering 
Algorithms for Fast Learning in Feedforward Neural Networks, Neural 
Networks, 
vol.5, N:5, 779-787. 
[Shepherd, 1997] Shepherd, A.J. (1997). Second-Order Methods for Neural Networks: 
Fast and Reliable Training Methods for Multi-Layer Perceptrons, Springer-Verlag, 
London. 
[Smirnov, 2001] Smirnov, E.N. (2001). Conjunctive 
and Disjunctive 
Version Spaces 
with Instance-based Boundary Sets, Shaker Publ., Maastricht, The Netherlands. 
[Smith and Vavak, 1998] Smith, J.E. and F. Vavak (1998). Replacement Strategies 
in Steady State Genetic Algorithms: Static Environments, In: Banzhaf, W. and C. 
Reeves (Eds.), Foundations of Genetic Algorithms FOGA-V, Morgan Kaufmann, 
San Francisco, CA, 219-234. 
[Sontag, 1992] Sontag, E.D. (1992). Feedback Stabilization Using Two-Hidden Layer 
Nets, IEEE Trans, on Neural Networks, vol.3, N:6, 981-990. 
[Stadler, 1996] Stadler, P. (1996). Landscapes and their Correlation Functions, Jour-
nal of Mathematical 
Chemistry, voL20, 1-45. 
[Syswerda, 1991] Syswerda, G. (1991). A study of Reproduction in Generational 
SteadyState Genetic Algorithms, In: Rawhngs, G.J.E. (Ed.), Foundations of Ge-
netic Algorithms FOGA-1, Morgan Kaufmann, San Mateo, CA, 94-101. 
[Tackett and Carmi, 1994] Tackett, W.A. and A. Carmi (1994). The Donut Problem: 
Scalability and Generalization in Genetic Programming, In: Kinnear, K. (Ed.), 
Advances in Genetic Program'ming, The MIT Press, Cambridge, MA, 143-176. 
[Takens, 1981] Takcns, F. (1981). Detecting Strange Attractors in Turbulence. In: 
Rand, D.A. and L.-S. Young (Eds), Dynamical Systems and Turbulence, Lecture 
Notes in Mathematics, 898, Springer-Verlag, Berlin, 366-381. 
[Tai, 1979] Tai, K.-C. (1979). The Tree-to-Tree Correction Problem, Journal of the 
ACM, vol.26, N:3, 422-433. 
[Teller and Veloso, 1996] Teller, A. and M. Veloso (1996). PADO: A New Learning 
Architecture for Object Recognition, In: Ikeuchi, K. and M. Veloso (Eds.), Symbolic 
Visual Learning, Oxford University Press, Oxford, UK, 81-116. 
[Tenorio and Lee, 1990] Tenorio, M.F. and W.-T. Lee (1990). Self-Organizing Net-
work for Optimum Supervised Learning, IEEE Trans, on Neural Networks, vol.1, 
100-110. 

310 
REFERENCES 
[Tibshirani, 1996] Tibshirani, R. (1996). A Comparison of Some Error Estimates for 
Neural Network Models, Neural Computation^ vol.8, N:l, 152-163. 
[Tipping, 2001] Tipping, M.E. (2001). Sparse Bayesian Learning and the Relevance 
Vector Machine, Journal of Machine Learning Research^ vol.1, N:l, 211-244. 
[Tsoi and Back, 1994] Tsoi, A.C. and A. Back (1994). Locally Recurrent Globally 
Feedforward Networks; A Critical Review, IEEE Trans, on Neural Networks^ vol.5, 
N:2, 229-239. 
[Vapnik and Chervonenkis, 1971] Vapnik, V.N. and A.Y. Chervonenkis (1971). On 
the Uniform Convergence of Relative Frequences of Events to their Probabiliries, 
Theory of Probability and its Applications, vol.17, N:2, 264-280. 
[Vapnik, 1992] Vapnik, V. (1992) Principles of Risk Minimisation for Learning The-
ory, In: Moody,J., Hanson,S., Lippmann,R,. (Eds.), Advances in Neural 
Information 
Processing Systems ^, Morgan Kaufmann, San Mateo, CA, 831-838. 
[Vapnik, 1995] Vapnik, V. (1995). The Nature 
of Statistical 
Learning 
Theory, 
Springer Verlag, New York. 
[Vapnik, 1998] Vapnik, V. (1998). Statistical Learning Theory, Wiley and Sons, New 
York. 
[Vassilev et al., 2000] Vassilev, V.K., T.C. Fogarty and J.F. Miller (2000). Informa-
tion Characteristics and the Structure of Landscapes, Evolutionary 
Computation, 
vol.8, N:l, 31-60. 
[Volterra, 1959] Volterra, V. (1959). Theory of Functionals, Dover Publ, New York. 
[Vose, 1999] Vose, M.D. (1999). The Simple Genetic Algorithm: 
Foundations 
and 
Theory, The MIT Press, Cambridge, MA. 
[Wahba, 1990] Wahba, G. (1990). Spline Models for Observational Data, CBMS-NSF 
Regional Conf. Series 59, Society for Industrial and Applied Mathematics, Philadel-
phia, Pennsylvania. 
[Weigend et al., 1992] Weigend, A.S., B.A. Huberman and D.E. Rumelhart (1992). 
Predicting Sunspots and Exchange Rates with Connectionist Networks, In: Eu-
bank, S. and M. Casdagli (Eds.), Nonlinear Modeling and Forecasting, Addison-
Wesley, Redwood City, CA, 395-432. 
[Weinberger, 1990] Weinberger, E.D. (1990). Correlated and Uncorrelated Fitness 
Landscapes and How to Tell the Difference, Biological Cybernetics, vol.63, 325-
336. 
[Werbos, 1974] Werbos, P.J. (1974). Beyond Regression: New Tools for 
Prediction 
and Analysis m the Behavioral Sciences, PhD thesis. Harvard University. 
[Whitley, 1989] Whitley, D. (1989). The GENITOR Algorithm and Selection Pres-
sure: Why Rank-based Allocation of Reproduction Trials is Best, In: Schaffer, J.D. 
(Ed.), Proc. Third Int. Conf. on Genetic Algorithms and their Applications, Mor-
gan Kaufmann, San Mateo, CA, 116-121. 

REFERENCES 
311 
[Williams, 1995] Williams, P.M. (1995). Baycsian Regularisation and Pruning Using 
a Laplace Prior, Neural Computation, vol.7, N:l, 117-143. 
[Williams and Zipser, 1989] Williams, R.J. and D. Zipser (1989). A Learning Algo-
rithm for Continually Running Fully Recurrent Neural Networks, Neural Compu-
tation, vol.1, 270-280. 
[Williams and Zipser, 1995] Williams, R.J. and D. Zipser (1995). Gradient-based 
Learning Algorithms for Recurrent Networks and their Computational Complexity, 
In: Chauvin, Y. and D.E. Rumelhart (Eds.), Back-propagation: 
Theory, 
Architec-
tures and Applications, Lawrence Erlbaum, Hillsdale, NJ, 433-486. 
[Wray and Green, 1994] Wray, J. and Green, G.G.R. (1994). Calculation of the 
Volterra Kernels of Non-linear Dynamic Systems using an Artificial Neural Net-
works, Biological Cybernetics, vol.71, N:3, 187-195. 
[Wright, 1932] Wright, S. (1932). The Roles of Mutation, Inbreeding, Crossbreeding 
and Selection in Evolution, In: Jones, D.F. (Ed.), Proc. of the Sixth Int. Congress 
on Genetics, vol.1, 356-366 (also in: Provinc, W.B. (Ed.), Sewall Wright 
Evolution. 
Selected Papers, University of Chicago Press, Chicago, IL, 1986). 
[Yang et al., 1996] Yang, A.S., C. Sun and C. Hsu (1996). Energy, Matter, and En-
tropy in Evolutionary Computation. In: Proc. Third IEEE Int. Conf. on Evolu-
tionary Computation ICEC-96, IEEE Press, Piscataway, NJ, 196-200. 
[Yao, 1999] Yao, X. (Ed.) (1999). Evolutionary 
Computation: 
Theory and Applica-
tions, World Scientific, Singapore. 
[Zapranis and Refenes, 1999] Zapranis, A.D. and A.-P. Refenes (1999). Principles of 
Neural Model Selection, Identification 
and Adequacy: With Applications to Finan-
cial Econometrics, 
Springer-Verlag, London, UK. 
[Zhang and Muhlenbein, 1995] Zhang, B.-T. and H. Muhlenbein (1995). Balancing 
Accuracy and Parsimony in Genetic Programming, Evolutionary 
Computation, 
vol.3, N:l, 17-38. 
[Zhang et al., 1997] Zhang, B.~T., P. Ohm and H. Muhlenbein (1997). Evolutionary 
Induction of Sparse Neural Trees, Evolutionary 
Computation, vol.5, N:2, 213-236. 
[Zhu, 2003] Zhu, Q.M. (2003). A Back Propagation Algorithm to Estimate the Pa-
rameters of Non-linear Dynamic Models, Applied Mathematical Modelling, vol.27, 
N:3, 169-187. 

Index 
activation polynomials, 31 
approximation error bounds, 59 
allele, 35, 37 
ARMA models, 277 
automatic relevance determination ARD, 
222, 37 
basis functions, 12 
backpropagation, 148 
backpropagation 
algorithm 
BPA, 
157,158 
batch backpropagation, 157 
first-order backpropagation, 149 
incremental backpropagation, 158 
rational backpropagation, 172 
second-order backpropagation, 163 
Bayesian inference, 22, 209 
Bayesian error function, 211 
Bayes' Theorem, 212 
data likelihood, 212 
Bayesian intervals, 263 
analytical Bayesian intervals, 263 
empitical Bayesian intervals, 265 
Bayesian learning, 210 
Bayesian learning, 211 
recursive Bayesian learning, 229 
Sparse Bayesian learning SBL, 224 
Bayesian prediction intervals, 263 
bias-variance dilemma, 244 
bias, 244 
integrated bias, 246 
integrated variance, 246 
statistical bias, 245 
statistical variance, 245 
bootstrap, 243 
Vjootstraping error bars, 252 
confidence intervals, 248 
convergence measures, 130 
error trajectory, 132 
Principal Component Analysis PC A, 
130 
convergence of the error, 60 
crossover, 40 
data normalization, 275 
delta method, 248 
delta training rules, 153 
rational delta rules, 174 
regularized delta rule, 162 
static delta rules, 154, 155 
temporal delta rules, 193, 194 
demes, 128 
design matrix, 11 
discrete Volterra series, 56 
diversity measures, 133 
population clustering, 134 
structural population entropy, 137 
syntactic population diameter, 133 
dynamic polynomial networks, 77 
dynamic fitness functions, 91 
embedding dimension, 275 
empirical loss, 5 

314 
INDEX 
empirical risk, 61, 84 
error, 4 
total error, 4 
generalization error, 4 
errors of approximation, 59 
error bars, 248 
error derivatives, 150 
dynamic derivatives, 191 
first-order derivatives, 141 
second-order derivatives, 164 
error variance learning, 257 
evidence framework, 213 
evidence for hyperparameters, 216 
evidence procedure, 218 
evolutionary search, 16 
expectation maximization EM algorithm, 
225 
fitness, 37 
adjusted fitness, 94 
fitness landscape, 81 
fitness space, 35 
raw fitness, 94 
fitness evolvability, 129 
fitness function, 83 
static fitness, 84 
dynamic fitness, 91 
Akaike Information Criterion AIC, 
89 
Bayesian Information Criterion BIC, 
88 
Final Prediction Error FPE, 86 
Generalized Cross-Validation GCV, 
87 
Mallows statistic, 89 
Mean Squared Error MSE, 85 
Minimum Description Length MDL, 
89 
Predicted Squared Error PSE, 87 
Relative Averaged Error RAE, 85 
Unbiased 
Estimation of Variance 
UEV, 86 
fitness magnitude, 94 
gene, 34, 37 
generalization error, 244 
generation gap, 119 
genetic learning operators, 38 
mutation, 36 
crossover, 40 
genome, 34 
genotype, 37, 82 
genotype space, 35 
gradient-descent search, 150 
gradient vector, 164 
Hessian matrix, 165 
Jacobian, 165 
Group Method for Data Handling GMDH, 
10, 13 
high-order polynomial, 21 
hybrid sampling-resampling, 237 
hyperparameters, 212 
hyperpriors, 217, 226 
output noise hyperparameter, 226 
prior hyperparameters, 213, 226 
IGP framework, 49 
immune network, 93 
anti-idiotype excitation, 93 
antigen score, 92 
example importance, 93 
immune dynamics, 94 
proliferation, 92 
importance sampling, 237 
Inductive Genetic Programming, 25 
inductive learning, 4 
iterative least squares ILS, 63 
Kalman filter, 205 
Kolmogorov-Gabor polynomials, 9 
Kolmogorov-Lorentz theorem, 59 
landscape, 95 
complex landscape, 95 
landscape radius, 96 
rugged landscape, 95 
simple landscape, 95, 126 
smooth landscape, 95, 126 
landscape measures, 96 
Autocorrelation Function ACF, 97 
basin radius, 107 
causality, 103 
Correlation Length, 98 

INDEX 
315 
Fitness Distance Correlation FDC, 
99 
information measures, 104 
information content, 105 
probabilistic measures, 102 
quantitative measures, 107 
statistical measures, 96 
learning problem, 4 
learning rate, 160 
linear polynomial networks, 62 
horizontal PNN, 63 
kernel PNN, 66 
loci, 34 
Markov Model of GP, 51 
Markov Chain Monte Carlo algorithm, 
235 
Maximal Marginal Likelihood MML, 88 
migration, 127 
minimum, 95 
global minimum, 95 
local minimum, 96 
momentum, 160 
Monte Carlo training, 234 
Multilayer GMDH algorithm, 13 
Multilayer GMDH Networks, 9 
ASPN, 13, 27 
GMDH, 10, 27 
PNETTR, 13, 27 
Multilayer Perceptron Networks MLP, 21, 
148, 283 
mutation, 38 
Context-Preserving Mutation CPM, 
38 
Hierarchical Mutation HVLM, 39 
Universal Mutation UM, 39 
natural selection, 37 
neural network training, 21 
network prunning, 176 
static network prunning, 176 
recurrent network prunning, 207 
nonlinear polynomial networks, 62 
block PNN models, 69 
orthogonal PNN, 69 
rational PNN, 75 
trigonometric PNN, 71 
Ordinary Least Squares OLS, 11 
phenotype, 37, 82 
phenotype space, 35 
polynomials, 9, 26, 57, 59, 68 
polynomial Networks, 9 
orthogonal PNN, 69 
Functional-Link Net FLN, 28 
Pi-Sigma Networks SPN, 28 
Ridge Polynomial Networks RPN, 
28 
Separable Volterra Networks SVN, 
28 
Sigma-Pi Networks SPN, 29 
Sigma-Pi Neural Trees SPNT, 29 
Volterra Connectionist Model VCM, 
28 
Volterra Polynomial Basis Function 
VPBF, 28 
Polynomial Neural Networks PNN, 61 
horizontal PNN, 63 
kernel PNN, 66 
linear PNN, 62 
nonlinear PNN, 68 
orthogonal PNN, 69 
rational PNN, 75 
trigonometric PNN, 71 
population picture, 145 
power series, 9 
prediction intervals, 254 
analytical prediction intervals, 255 
empirical prediction intervals, 256 
predictive data distribution, 211 
principles of natural evolution, 36 
probabilistic prototype tree, 47 
R-propagation algorithm, 148 
static R-propagation, 166 
recurrent R-propagation, 200 
random tree generation, 47 
rational polynomial networks, 75 
recurrent neural networks, 78, 182 
regularization, 218 
global regularization, 218 
local regularization, 218 
weight decay regularization, 211 

316 
INDEX 
replacement strategies, 117 
conservative replacement, 118 
deletion of the worst, 117 
deletion of the oldest, 117 
kill tournament, 118 
random deletion, 118 
reproduction operator, 112 
steady-state reproduction, 119 
extinctive reproduction, 118 
reproduction criteria, 118 
dynamics, 118 
elitism, 118 
retention, 118 
steadyness, 118 
residual autocorrelation tests, 268 
residual bootstrap sampling, 243, 252 
Ridge Polynomial Networks RPN, 28 
rooted-tree schema theorem, 50 
schema theorem, 50 
search control, 119 
coevolutionary search, 125 
distributed search, 125 
genetic annealing, 122 
macroevolutionary search, 119 
memetic search, 120 
stochastic genetic hill climbing, 124 
search space size, 33 
second-order training, 169 
conjugate gradients method, 170 
Levenberg-Marquardt method, 171 
Newton's method, 169 
selection strategies, 113 
fitness proportionate selection, 113 
ranking selection, 115 
tournament selection, 116 
truncation selection, 115 
universal selection, 116 
self-organization measures, 139 
kynetic population energy, 141 
potential population energy, 140 
total population energy, 144 
sequential 
hyperparameter 
estimation, 
232 
size-biased operators, 41 
size-biased mutation, 42 
size-biased crossover, 42 
Sparse Bayesian Learning SBL, 224 
standard error, 243 
state-space models, 182 
static fitness function, 84 
average error functions, 85 
information-theoretic 
fitness 
func-
tions, 89 
probabilistic fitness functions, 88 
risk minimizingfitness functions, 90 
statistical fitness functions, 86 
statistical learning networks, 281 
statistical model validation, 23, 241 
Stone-Weierstrass theorem, 58 
STROGANOFF, 18 
Structural Risk Minimization SRM, 91 
temporal backpropagation, 181 
backpropagation through time BPTT, 
184 
real-time recurrent learning RTRL, 
171 
recursive backpropagation, 204 
second-order 
temporal 
backprop, 
200 
temporal BPTT training rules, 187 
temporal RTRL training rules, 193 
tree, 17 
tree depth, 33 
linear tree, 34 
pointer-based tree, 33 
prefix tree, 34 
tree-like PNN, 55 
tree-structured polynomial networks, 29 
tree-to-tree distance, 42 
universal approximation, 57 
Vapnik-Chervonenkis dimension, 61 
Volterra models, 56 
weight vector, 11 
weight prior, 222 
Cauchy weight prior, 222 
Laplace weight prior, 222 
well-defined parameters, 219, 220 

