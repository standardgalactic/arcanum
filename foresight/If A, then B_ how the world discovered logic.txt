


Columbia University Press Publishers since 1893
New York Chichester, West Sussex cup.columbia.edu
Copyright © 2013 Michael Shenefelt and Heidi White All rights reserved
E-ISBN 978-0-231-53519-9
Library of Congress Cataloging-in-Publication Data Shenefelt, Michael, 1953– If A, then B: how the world discovered
logic / Michael Shenefelt and Heidi White.

pages cm
Includes bibliographical references and index.
ISBN 978-0-231-16104-6 (cloth : alk. paper)—ISBN 978-0-231-16105-3 (pbk.: alk. paper)—ISBN 978-0-231-
53519-9 (e-book) 1. Logic—History. I. Title.
BC15.S54 2013
160.9—dc23     2013007780
A Columbia University Press E-book.
CUP would be pleased to hear about your reading experience with this e-book at cup-ebook@columbia.edu.
Cover design: Jarrod Taylor References to websites (URLs) were accurate at the time of writing. Neither the author nor
Columbia University Press is responsible for URLs that may have expired or changed since the manuscript was
prepared.

To Gloria Willis Shenefelt
and Johanna Sue White

CONTENTS
Preface
INTRODUCTION: WHAT IS LOGIC?
The Strange Nature of Logical Validity—What Makes a Valid Argument Valid?—The Divine-Command
Theory of Logic—Logic as Culturally Invariant—Logic as Timeless and Placeless—The Social History of
Logic 1. THE DAWN OF LOGIC
The Effect of Geography on the Flow of Ideas—The Effect of the Sea Trade—Transportation and
Civilization—Classical Greece as the Extreme Case—The Athenian Assembly 2. ARISTOTLE:
GREATEST OF THE GREEK LOGICIANS
The Study of Argument in India—The Singularity of Aristotle—The Effect of the Athenian Assembly—The
Sophists—The Separation of Logic from Rhetoric 3. ARISTOTLE‘S SYSTEM: THE LOGIC OF
CLASSIFICATION
Manipulating Classes—The Square of Opposition—The Underlying Mystery of the Square—
Wittgenstein‘s Proposed Solution—Wittgenstein‘s Mistake 4. CHRYSIPPUS AND THE STOICS: A
WORLD OF INTERLOCKING STRUCTURES
The Stoics—The Logic of Choice—The Nature of Compound Propositions—Interlocking Forms of
Argument—The Laws of Contradiction and Excluded Middle—More Interlocking Forms—The Basis of
Computer Logic 5. LOGIC VERSUS ANTI-LOGIC: THE LAWS OF CONTRADICTION AND
EXCLUDED MIDDLE
Paradoxes of Truth—The Nature of Fuzzy Logic—Is Validity Relative?—Does Formal Logic Ultimately
Depend on Common Sense?
6. LOGICAL FANATICS, CIRCULAR REASONING, AND DESCARTES‘S FUNDAMENTAL
PRINCIPLE
The Origins of the Wars of Religion—The Importance of Firm Foundations—The Logical Complexity of
Our Premises—The Origins of Formalized Logic and Mathematics—The Paradoxes of Formalization—
The Double Meaning of “Foundations”—The Outlook of Thomas Kuhn—Kuhn‘s Error—Competition
Between Scientific Theories 7. WILL THE FUTURE RESEMBLE THE PAST? INDUCTIVE LOGIC AND
SCIENTIFIC METHOD
The Challenge of the New Literature—The Triumph of the Vernacular and the Growing Spirit of Equality
—The Rise of Modern Political Theory—The Right of Dissent and the Reliance on Induction—Induction
as the New Rationality—Aristotle‘s Influence on the Medievals—The Rational Foundations of Induction
—The Apparent Irreducibility of Induction—The Assumptions of Empirical Science 8. RHETORICAL
FRAUDS AND SOPHISTICAL PLOYS: TEN CLASSIC TRICKS
The Battle for Parliamentary Reform—Jeremy Bentham and the Legacy of the Enlightenment—
Bentham‘s Book of Fallacies 9. SYMBOLIC LOGIC AND THE DIGITAL FUTURE
The Impact of the Industrial Revolution—The Origins of Symbolic Logic—The Logic of Relations—The
Effect of the New Mathematics—The Impact of Quantification—Frege‘s New Foundation for
Mathematics—The Invention of Digital Computing 10. FAITH AND THE LIMITS OF LOGIC: THE LAST
UNANSWERED QUESTION
Abelard‘s Rise to Power—Abelard‘s Attack on Faith Without Reason—Are Faith and Reason
Compatible?—The Foundations of Rational Belief—Rationality After the Wars of Religion—The
Vigilance of Reason APPENDIX: FURTHER FALLACIES
Ad Hominem Argument (“To the Person”)—Begging the Question—Big Lie—Cause and Effect
(Confusions of)—Circular Definition—Circular Explanation—Composition—Continuum (Denying

Differences in)—Division—Emotional Appeal—Equivocation—Evils and Remedies (Confusing Them)—
False Analogy—False Antithesis—False Authority—False Dichotomy (or False Dilemma)—Formal
Fallacy—Generalizing (Errors of)—Innuendo—Irrelevant Conclusion (or Ignoratio Elenchi, “Ignorance
of Proof”)—Is and Ought (Confusing Them)—Loaded Question—Negative Proof—Pretentious Diction—
Sham Insight—Straw Man—Suppressed Evidence (or Ignored Evidence)—Vague Metaphor—Wrapping
Oneself in the Flag Notes
Bibliography
Index

PREFACE
THE WORLD is logical, according to some. Others call it absurd. We have never
been sure who is right, but what we do know is that nobody grasps the absurd
unless that person also senses the logical. Yet, in that case, where does our sense
of logic come from? What is it really a sense of? And what drove people to start
studying logic in the first place?
There are excellent histories of logic already in circulation (including the
magisterial Development of Logic by William and Martha Kneale, Oxford
University Press, 1962). And thanks to online sources, there are also many able
accounts of the latest work in the field, including nonclassical symbolic logic.
Nevertheless, we believe our book to be fundamentally different from previously
published works. Earlier histories of logic have focused on the specific stories of
individual logicians, relating their discoveries, their intellectual influences, and
their personal predicaments. But logic is the work of more than logicians alone;
logicians, like other writers, need readers, and the forming of a readership is just
as vital to the survival of a logician’s insights as the logician’s individual
circumstances. In logic, as in other departments of intellectual history, a
readership is a consequence of social forces—forces that affect large numbers of
people, quite apart from individual will. As a result, if one then leaves out of
consideration the forces shaping such a readership (or the forces shaping a
logician’s audience), one is in danger of missing much of the explanation of why
logical discoveries show up when and where they do.
Aristotle, for example, inherited a rich philosophical legacy in classical
Greece, but equally important, in our view, was the reaction of significant
numbers of his contemporaries—his audience—to the follies of the Athenian
Assembly, an institution that owed its existence, indirectly but no less crucially,
to Greece’s peculiar geography. Aristotle’s work required not only an intelligent
thinker to invent it but also an appreciative audience to preserve it, and what
made Aristotle’s work interesting to this audience in the first place was its
connection to the argumentation of the Assembly and to the political disasters of
his age. The Athenians had already suffered through immense political tragedy,
especially during the Peloponnesian War, and in acting out this tragedy, they had
prepared the ground for Aristotle’s insights. Logic captured their interest from
the start because many of them believed their politics had already been
undermined by sophistry.
Suppose, then, one were to ask, “Why did the originator of formal deductive

logic emerge in classical Greece rather than in, say, classical India or classical
China, which also offered rich philosophical legacies?” We believe much of the
answer lies not in Aristotle’s personal training and experience but in a broader
set of economic and political conditions—conditions encouraged by the physical
environment—that had made classical Greek society especially sensitive to
public argumentation and highly vulnerable to its political effects. To be sure,
these circumstances would have been almost as much a part of Aristotle’s
experience as anyone else’s, but the key point is that the circumstances were
general, not specific to an individual, and in our view, it is precisely such general
circumstances that have been underrated.
More broadly, to treat logic’s history as if it were only a matter of individuals,
without considering the larger forces shaping the audience and the logician alike,
would be like treating political history as if it were only a matter of individuals
—as a tale of specific rulers or rebels but with no account of why large numbers
of contemporaries reacted to their actions in any particular way. It would be like
treating political history as only a story of the insights, villainies, and sacrifices
of particular persons, with no social analysis.
In making these claims for the importance of social forces in the development
of logic, we in no way deny the significance of the individual in history; rather,
we contend that historical changes can have many different causes. The social
process can be an important force—just as important as the individual. And in
stressing this point, we see ourselves not as contradicting earlier work in the
history of logic but as offering a further level of explanation.
We are much indebted in our efforts to a number of colleagues and friends for
their suggestions, though we are sure that, in many instances, we should have
been more attentive to their criticisms. The errors that remain are ours. We
would especially like to thank Douglas R. Anderson, Lloyd Carr, Elizabeth
Potter, and Phil Washburn. We would also like to thank Parvaneh Badie for her
lucid and instructive diagrams, which we have used in conveying various logical
principles to those with no previous formal training in the field.
In addition, we are grateful to the organizers of the Eighth Panhellenic Logic
Symposium in Ioannina, Greece, where, in 2011, we had the opportunity to
present part of our argument to the effect that economic and political conditions
played a crucial role in Aristotle’s discoveries. We also want to express our
thanks to the organizers of the 2012 Logic Colloquium in Manchester, England
(sponsored by the British Logic Colloquium, the London Mathematical Society,
the Association for Symbolic Logic, and the Manchester Institute for
Mathematical Sciences) for the chance to offer our thesis that the nineteenth-

century Industrial Revolution spurred the invention of symbolic logic. At each of
these conferences, we received valuable criticism from members of the audience,
and we have tried to work their points into our text.
We owe a great debt of gratitude to our agent, Diana Finch. Diana gave us
crucial encouragement and a good deal of deft editing. We would like to thank as
well our editor Wendy Lochner of Columbia University Press for her care and
support of our project. Our thanks go also to Christine Dunbar for her expert
guidance in preparing the final version of the manuscript for publication.
In earlier works on the history of logic, there has long been a tradition of
treating logical discoveries as if they were the stunning and beautiful work of
individual, brilliant minds. Notwithstanding our remarks on the role of social
forces, we believe this tradition to be correct. The great, individual logicians of
the past were brilliant minds, their discoveries are beautiful, and logic as a whole
is in many ways mysterious and sublime. In the pages that follow, we hope to
convey some of the majesty of this ancient and noble discipline.

INTRODUCTION
WHAT IS LOGIC?
WE LIVE in a world of constant change: armies collide, empires decline,
sometimes whole civilizations slide into oblivion. Does anything last forever?
The Apostle Paul says three things last (faith, hope, and love), but we would
suggest a fourth constant in our lives: the laws of logic.
We all have a sense of logic; it shapes us every day. Yet its nature is deeply
mysterious. Logic isn’t like language, varying from culture to culture. Logic is
like arithmetic—tricky yet objectively true. Just as the number seven has always
been prime to every culture that has ever defined prime numbers, so the most
common methods of deductive reasoning have always been valid. Of course, not
everyone sets forth a logician’s definition of validity in the first place, and not
everyone pursues the idea to its further reaches. But those who reflect on it
always arrive at the beginnings of the same abstract realm, a realm infinitely
complicated yet implicit in much that we do—a realm of form, structure, and
pattern discovered twenty-three centuries ago. The nature of that discovery was
strange, just as logic itself is strange.
For one thing, though everyone uses logic, not everyone studies it (just as,
though most people walk, not everyone studies walking). Logic as a discipline
begins only with the ancient Greek philosopher Aristotle, and peculiar as it
sounds, all modern studies of logic in the sense of deductive validity (meaning
logical necessity) descend from his efforts. The deductive validity of
argumentation was studied by later Greeks, by later Romans, by Arab physicians
serving powerful caliphs in the tenth century A.D., and by medieval theologians
working in various European universities. It is now studied by computer
programmers the world over. Yet all these studies owe their origins to exactly
one person: Aristotle.
Most people today learn logic out of books. Yet all these books were written
by people influenced by other books, and all the books have a lineage that leads
back to the same original inspiration. The lineage always points back to the same
Greek thinker, who flourished in the fourth century B.C. There is simply no
historical record of anyone ever studying validity in the logician’s sense of the
term except Aristotle or people directly or indirectly influenced by Aristotle.
How can this be? If the truths of logic are objective and culturally invariant,

why does the study of logic show up only in particular times and places, like
Greece in the fourth century B.C.? And why do all known studies of logical
validity lead back to the same original source?
There is one thing to keep in mind from the start: logical discoveries usually
depend on individual insight, but logic as a discipline requires something more
—insight with an audience. Logicians need other people who are willing to
listen, and audiences are a consequence of social forces—forces that affect large
numbers of people quite apart from individual will. As a result, logic has a social
history as well as an abstract one. Logic considers unchanging truths, but the
extent to which large numbers of people will ever really explore these truths still
depends, in part, on their social setting. And one’s social setting turns on various
factors—political, economic, technological, and even geographical. The history
of logic is a mix of the abstract and the mundane.
THE STRANGE NATURE OF LOGICAL VALIDITY
When it comes to the early study of logically valid reasoning, much depends on
what we mean by “valid.” We can get the basic idea from a pair of examples:
(1)
All cats are cool.
Felix is a cat.
Therefore, Felix is cool.
(2)
All wicked witches are irritable.
The Witch of the West is a wicked witch.
Therefore, the Witch of the West is a friend to little dogs.
The difference between these examples is easy to see.
In the first example, if the first two statements are true, then the third
statement must also be true. (If all cats are cool, and Felix is one of them, then
Felix must be cool.) But in the second example, even if the first two statements
are true, the third could still be false. To revert to a bit of ancient phrasing, in the
first example the third statement follows from the other two whereas in the
second example the third statement doesn’t follow. The ancient Romans
expressed this difference by saying non sequitur (“it doesn’t follow”).
Logic studies the difference between examples of this sort, but there are
infinitely many of these examples in which, if the first statements are true, the

last must also be true. These are the examples logicians call valid, and by valid
they mean something specific. Logicians study arguments, and an argument, to a
logician, isn’t a quarrel but an attempt at proof. An argument consists of reasons,
called premises, and a point to be proved—the conclusion. For a logician to call
an argument valid, then, is to say exactly this: if the premises are true, the
conclusion must also be true. But notice that the question of validity is strictly
hypothetical in this sense: to ask whether an argument is valid isn’t to ask
whether any of its statements are true but only whether, if the premises were
true, would the conclusion have to be. As a result, a valid argument can consist
entirely of false statements, even whimsical ones, like this:
All hedgehogs are laborious.
My landlord is a hedgehog.
Therefore, my landlord is laborious.
If the terms of the argument are meaningful at all, then the argument is still
valid. In consequence, logic isn’t really about whether any of these premises or
conclusions is true or false but only about abstract connections. It is about how
the truth or falsity of some propositions would connect with the truth or falsity of
others. It is just this fact that makes logic mysterious.
Logic is the study of these abstract, hypothetical connections—connections
involving logical necessity. But where do the connections come from, and why
do we see them at all? Are the connections really objective features of the
arguments themselves or just features of our making? How we answer these
questions will help to determine just what we think logicians have been doing
since they first turned logic into a discipline.
When we speak of objective truths, we often mean an accurate description of
physical facts, like an accurate measurement of the Eiffel Tower’s height or the
base of the Great Pyramid at Giza. But when we speak about logic, we are
speaking of something different: we are speaking of connections between
statements, propositions, or assertions. Of course, we can sense patterns in these
connections (and surely it is the patterns that matter), but the question now is
what makes some patterns valid and others not. In the first and third examples
from before (the one about Felix and the one about the landlord), we see the
following pattern:
All As are B.
C is an A.
Therefore, C is B.

It seems any argument fitting this pattern is logically valid; here’s another
familiar example that also fits the pattern:
All men are mortal.
Socrates is a man.
Therefore, Socrates is mortal.
Yet our ability to think logically can’t depend on learning just a few of these
patterns. On the contrary, there are infinitely many valid patterns, as well as
infinitely many invalid ones, yet somehow we are able to classify many of the
simpler examples into one of two groups—valid or invalid. So how do we do
this? What goes on when we distinguish the valid from the invalid? Are we
merely repeating something we have been taught?
As it turns out, a sense of the most basic patterns can’t be taught (or so it
seems) for the direct reason that no one can follow such a lesson without sensing
some of the patterns already. To learn anything, we need to have a sense of logic,
because every lesson is a pattern in itself. Even if we are taught all sorts of
patterns (patterns like the one involving Felix, hedgehogs, or Socrates), this kind
of teaching is useful only if we can also draw conclusions—conclusions from the
presence or absence of the patterns. This is the same problem over again. When
we draw a conclusion from the presence of a pattern, should we draw the
conclusion validly or invalidly? How can we tell which is which unless we
already have a sense of what counts as valid? Admittedly, we might be taught to
follow rules, but to follow a rule we still need to see what that rule logically
implies, which involves a pattern too.
For example, we might be taught to follow the rule that, whenever we find
some object A, we ought to do B; nevertheless, to follow this rule we still need
to watch for A and then do B, and thus the whole procedure seems to assume
that we already sense the following pattern, which logicians call “modus
ponens”:
If A, then B
A
Therefore, B
In other words, some sense of the valid and invalid seems to be already innate,
and it is exactly this innate sense that makes human beings teachable in the first
place.1

Logic is present in countless mental operations, but if we ask how we really
know its patterns to be valid, this question turns out to be, apparently,
unanswerable. The reason is that all possible answers must still take some of the
patterns for granted, at least in particular cases. We still need the ability to
recognize a valid argument before we can prove anything else to be a valid
argument. Any answers we come up with will still have to be logical answers,
and what counts as logical is precisely the point at issue. The only way to justify
the patterns will be to invoke an argument embodying another pattern; yet if we
challenge the reasonableness of all patterns, no matter where they appear, we hit
a dead end. Any answers we come up with, to be answers, must still involve
patterns of their own; patterns are how we tell what counts as an answer and
what doesn’t. Thus it appears that some aspects of logic must remain forever
undemonstrated. Moreover, we hit a similar dead end if we try to turn the
question around and ask it differently: we reach an impasse if we ask of the valid
patterns, “What makes them so?”
WHAT MAKES A VALID ARGUMENT VALID?
Many people think certain patterns of logic are valid only because our brains
happen to work in a particular way; they think that if our brains were wired
differently, logic would become illogic, and illogic would be logic. As a result,
they imagine the principles of logic to be nothing more than effects of our brain
structure. Logic is the way it is (people suppose) because of our brains.
The idea that logic is simply a consequence of the human brain has always
been appealing, but the trouble is that it seems to put the cart before the horse. It
leads us to mix up what depends on what. True, our brains work in a particular
way, but they do so for a reason: the way is often useful. Our brains function
usefully whenever they solve the puzzles that need solving—how to grow food,
build shelters, or find water—and it is logical reasoning that allows this.
What this observation apparently shows, however, is not that logical patterns
are valid because of the structure of the brain. Instead, it shows the opposite: our
brains have their useful structure because the patterns are valid. Logic doesn’t
come from our brain’s mechanism; instead, our brain’s mechanism appears to
come from the very nature of logic.
Suppose different patterns were valid. Suppose everything we now think of as
logical were illogical and vice versa. In that case, human brains would have had
to evolve differently, or our early ancestors would have perished. Of course,

many behaviors might turn out to be useful in unexpected ways (even illogical
behavior), but if different patterns had always been the logical ones (and if all
our current patterns had been the illogical ones), then a brain that failed to reason
according to the different patterns would have been useless, perhaps even
dangerous. If all the logic of our ancestors had been mistaken, they would have
died off. They would have failed in their efforts to manipulate their surroundings
and to find water, food, and shelter. And what this consideration shows is that
the very idea of a human brain evolving in useful ways (logic being one of the
useful ways) still seems to assume that the demands of logic have shaped the
brain—and not that the brain shaped logic. Certain patterns aren’t valid because
our brains happen to prefer them; instead, our brains prefer these patterns
because the patterns are already valid. Logic helps to define what counts as a
functioning brain in the first place, and consequently it seems impossible
(without circularity) for the brain to define logic.2
As thorny and difficult as analyzing the basis of logic might seem, is there any
other way to explain why the valid patterns are valid? Historically, many people
have tried to explain why logic is the way it is, but most of these attempts have
come to naught. Indeed, it seems most of them must come to naught, and we can
see this last point better if we return for a moment to the idea of usefulness.
We said a moment ago that logic is useful; if so, couldn’t we say that a logical
pattern is valid precisely because it is useful? And couldn’t this usefulness be the
real reason some patterns are logically valid and others not? This new answer is
initially attractive, but, on reflection, it appears to be just as incoherent as the
last, because it once more puts the cart before the horse. Logical patterns aren’t
valid owing to their usefulness; instead, their usefulness is owing to their
validity. Whatever tricks of reasoning our ancestors devised to endure in a
difficult world, the tricks were useful because they were logically correct (not
correct because they were useful). Again, we seem to be mixing up what
depends on what.
If we don’t reason in logically valid ways, then we often get useless results—
we get nonsense—and sometimes we make serious mistakes about how the
world works. We harvest at the wrong times or drive the car in the wrong
direction or fail in trying to operate a computer. And in that case, what follows is
that the usefulness of our reasoning depends on its validity, not the reverse. To
put this point abstractly, if giving up A causes you to lose B, what follows is that
B depends on A, not vice versa. If giving up food would cause you to lose your
life, then your life depends on the food. Just so, if giving up logic causes you to
lose the ability to think usefully, then your ability to think usefully depends on

logic, not the other way around. The patterns aren’t valid because they are
useful; they are useful because they are valid.
In fact, logical patterns seem to fall into a special category, a strange and
sometimes bewildering category—the category that we might call the collection
of life’s ultimate truths. We can describe valid and invalid patterns and show that
some are tied to others; we can also ask how the idea of validity might be
connected with other logical notions, like the idea of necessity. (We say the
conclusion of a valid argument follows as a matter of “logical necessity.”) And
we can even ask whether all valid patterns might be characterized by a more
general set of logical rules, rules that might be captured in an abstract logical
system. Logicians argue about these matters all the time, and they often
disagree.3 Nevertheless, the key point is that all these studies still assume that
there is indeed a difference between the valid and invalid, and this difference has
been recognized for thousands of years. And it is just this difference that seems
immune to any explanation whatever, whether in terms of the structure of our
brains or the considerations of usefulness or any other physical fact. Why does
the difference between the valid and invalid exist at all?
Of course, we sometimes disagree about what counts as logically valid—and
so do professional logicians—but these disagreements still assume that the
difference between the valid and invalid is real. And most of the disagreements
seem to involve complicated examples (where it is easy to become confused) or
abstract examples (where it is hard to know just what is being discussed).4 On
the other hand, when we stick to simple examples (like the ones involving Felix,
hedgehogs, or Socrates), we find vast, general agreement among many different
people in many different times, and the validity of these examples is no less
conspicuous today than it was in the ancient world of Aristotle. The validity of
the pattern in question, at least in ordinary contexts, is just as obvious as the fact
that two and three make five. Our knowledge of these simple cases remains,
despite whatever questions anyone might pose (even questions from a
professional logician) about other, esoteric cases. (It is fallacious to argue that,
just because we might be mistaken about esoteric cases, we can’t know the
validity of the simple ones.)
Moreover, our knowledge of these simple cases of validity doesn’t seem to
depend on having any knowledge of a more difficult and abstract logical system
(like one a logician might invent), any more than our knowledge that two and
three make five depends on knowing the finer points of set theory in
mathematics. Most people know the sum of two and three even though they
don’t know any formalized axioms of arithmetic; it follows that their knowledge

of the sum can’t depend on knowing the axioms, interesting though the axioms
are. Formalized mathematics of the sort now studied in mathematics departments
didn’t emerge until the nineteenth and twentieth centuries, but it would be absurd
to say that Isaac Newton didn’t know the sum of two and three simply because
he hadn’t embraced such a system. Just so, most people can distinguish simple
cases of validity and invalidity, and it likewise follows that this ability is
independent of the formal techniques of professional logicians.
More generally, the human species has long had basic logical intuitions in
many particular cases, intuitions that have proved remarkably invariant over the
centuries; in many such cases, there has never been the slightest reason to
suppose that the intuition is incorrect.5 In the twentieth and twenty-first
centuries, logicians, mathematicians, and computer scientists have also
constructed alternative logical systems (often called alternative “logics”), but the
existence of these systems in no way undermines the straightforward intuitions
we have already invoked. Instead, most of these systems, if sound, concern a
different subject matter (a point we shall be discussing in chapter 5).
But how do we explain this? How do we account for this durable difference
between the valid and invalid? Indeed, we might well wonder how there can be
any such explanation in the first place. After all, any explanation (it would seem)
must already take the difference for granted. To say anything meaningful about
the difference from the start, we must still speak logically, and this assumes we
already have an ability (at least a rudimentary one) to distinguish between what
is logical and what isn’t.
THE DIVINE-COMMAND THEORY OF LOGIC
Before we leap to this last conclusion too quickly, however, there is another
curious possibility we ought to consider, a special one contemplated and
investigated over many centuries and still of interest today: maybe certain
patterns are valid because they come from God. If we think of God as creator of
the physical world, then couldn’t we also suppose that God created logic? Maybe
valid patterns are valid because God says so.
Not everyone believes in a god, of course, but both believers and nonbelievers
can still ask: If there is a god who created the world, could this god also be the
cause of why logic is the way it is? Religion seeks to answer many of life’s other
questions; might it also offer an answer to the question of what makes an
argument valid? Is there a way to think of logic such that the discipline rests

ultimately on a truth of religion?
This new idea might be called the divine-command theory of logic, and at its
core is the suggestion that logical necessities exist only because God commands
them. Even if the useful depends on the logical (a piece of reasoning is useful
because it is logical, not the other way around), and even if the utility of our very
brains depends on what counts as a valid pattern (our brains work only because
they recognize these patterns), all these things might, nevertheless, still depend
on God’s all-powerful will. God might still be the creator of logic.6
This divine-command theory seems entirely plausible when considered in
itself, but the trouble is that it appears to make nearly all other talk about God—
most of the central questions of theology—futile. The reason is that it deprives
God (if there is one) of any rational qualities. The divine-command theory seems
to make God thoroughly and irreducibly arbitrary; it excludes from the idea of
God any objectively rational qualities that would make a human being revere a
god in the first place. And in that case, we might as well worship some other
arbitrary force that affects our lives, like the force of gravity or the burning of
the sun. The “god” in question would just be a blind power, and to say that the
existence of this power then “explains” why logic is the way it is would be to say
virtually nothing. We might as well say that logic is the way it is simply because
some force or other—a force we can never understand—has made it so.
We can see the apparent emptiness of this approach better if we turn for a
moment to a similar sort of divine-command theory suggested in the fourth
century B.C. by the Greek philosopher Plato when he pondered the nature of
morality. Plato asked whether things are right or good only because the gods said
so. (The classical Greeks believed in many gods.) Plato suggested his version of
the divine-command theory indirectly in one of his dialogues, the Euthyphro. He
had the philosopher Socrates pose a question about “holiness”: “Is a thing holy
because the gods love it, or do the gods love it because it is holy?” By analogy,
we might now ask, “Is a thing logical because the gods love it, or do the gods
love it because it is logical?” Plato wondered whether things might be holy only
because the gods said so.7
Ultimately, Plato rejected the divine-command theory when applied to
morality because its logical consequence was to make the right and the good
depend on the gods’ will even if this will was arbitrary. The theory’s effect was
to deprive the gods themselves of any real goodness (according to the theory, to
call the gods “good” would only be to say that the gods loved themselves) and to
deprive their actions of any real rightness (to call their actions right would only
be to say that they loved their own actions). Instead, Plato wanted to suppose

that the gods, if any, had objectively good reasons for their actions and had
objective moral qualities. In consequence, he supposed that rightness and
goodness were qualities the gods recognized, not qualities they merely invented.
Even if the gods seemed arbitrary, he supposed, this was only because the
reasons for their actions often surpassed human understanding.8
The divine-command theory of logic gives a similar result. If being logical is
part of being reasonable, and if God creates logic, then to say God is reasonable
and has “good reasons” for his ways is only to say that God’s reasons are his
reasons, however arbitrary. And in that case, God would be equally reasonable if
he worked in opposite ways. Why, then, doesn’t God will the opposite? There
could never be an answer. The divine-command theory seems to force us to
abandon the notion of a just and reasonable god and replace it with the idea of a
merely powerful, even capricious one.
Even if we approach logic from the standpoint of religion or theology, the
ultimate nature of logic still has a certain inexplicability about it. We can say
which patterns are valid, and maybe even reduce some of them to general rules,
but we apparently contradict our tacit assumptions (even religious assumptions)
if we try to explain why the patterns should exist at all.9
LOGIC AS CULTURALLY INVARIANT
All these considerations were anticipated centuries ago by medieval
philosophers who, working in the tradition of Aristotle, insisted that logic was
the common tool of all the sciences.10 Their meaning? That all analyses,
explanations, understandings, methods, procedures, sciences, and rules must take
some notion of logic for granted. This is true even if the explanation or
understanding is religious; some sense of logic is always presupposed. On the
other hand, if we seek a further basis for this presupposition, all our efforts end
in stalemate. We get nowhere, and we see this consequence today even when we
try to understand different cultures.
It is sometimes asserted that different cultures have different “logics” and that
the key to understanding another culture is to comprehend its logic. Now, if this
is just a highfalutin way of saying that different cultures have different beliefs or
sets of beliefs, then the underlying assertion is true. Logicians, however, don’t
study beliefs; they study how beliefs are connected. They study how some
beliefs are inferred from other beliefs, and so they study methods—or patterns—
of inference. (And they can study these patterns, by the way, whether the things

being connected are beliefs, sentences, symbolic strings, electronic pathways, or
what have you. The forms of the patterns don’t depend on the sorts of entities
they connect, and as a result, as long as the parts of the inference can be properly
labeled true or false, the exact nature of the things being labeled is irrelevant.)11
Admittedly, there is sometimes a certain vagueness in determining what
counts as a belief and what counts as a method of inference; nevertheless, if
there were nothing in common between two cultures, not even their inferential
methods, then what would it mean for one culture to “understand” another? How
would understanding be possible at all? What would disciplines such as
sociology and anthropology be about?
To put this point generally: to understand another culture even partially is
presumably to understand how a different tradition or experience would lead
people to draw different conclusions about the world. And what does it mean to
understand someone else’s conclusions? The only thing this could mean (it
seems) would be an understanding of how different conclusions would follow
logically from different premises. Yet, in that case, we must once more assume
some sort of logic in common. Without something in common, how would we
even distinguish another culture’s reasonings from a merely random collection of
its opinions? (We might look for words that correspond to the ones we use to
introduce a premise or a conclusion—what logicians call indicators—like
“therefore,” “because,” “hence,” and “for the reason that.” But how would we
even know how to translate these expressions except by finding them embedded
in something we already recognized as a logical inference?) One of the
overriding aims of social science is to grasp how a common human nature can
still result in widely varying ways of life. The mind’s ability to distinguish the
logical from the illogical is part of that common nature.
In all these remarks, we have merely considered the many ways in which the
invariant nature of logic is presupposed. Logic is the common tool of the social
sciences no less than the physical sciences, and it can’t follow as an outcome
from any other discipline—disciplines like physics, neurology, theology,
anthropology, sociology, or linguistics—because logic defines what it means to
“follow” in the first place. Nor can it be a consequence of anything else, because
logic defines what counts as a consequence.
All the same, there is a further point here—and this last point is perhaps the
strangest of all: none of these remarks really proves logic to be universal or
invariant, not in the least. None of them proves such things unless we already
invoke logic in the proof, and this is surely “proof” only in the sense that it
preaches to the converted. We still end up assuming what we are supposed to be

justifying.
If we try to show that logic is somehow more correct than illogic, we must
still make an argument. But what kind of argument must we make, logical or
illogical? This is the same problem once again. If we try to justify logic
logically, we end up arguing in a circle—a point made long ago by the ancient
Greek philosopher Epictetus. (When asked for a reason to regard logic as
“useful,” Epictetus replied by asking, in effect, whether the reason he gave
should be “logical”: “You see how you yourself admit that logic is necessary, if
without it you are unable even to learn this much—whether it is necessary or
not.”)12 The best we can say is that, if invariant logical principles don’t exist,
then nothing else can remain comprehensible, because all our methods of
comprehension already assume some sort of logic in advance.
Epictetus’s observation (which comes from the first century A.D.) might be put
in its most extreme form as follows: suppose all laws of logic, whatever they are,
were to change in the next ninety seconds; suppose all disciplines by which we
now try to analyze the world were suddenly to undergo a corresponding change
so that the consequences of an alternative logic were to ripple through our
universe of ideas like a series of unsettling aftershocks. In that case, the change
would be utterly inexplicable, unpredictable, unanalyzable, and unfathomable.
Why? Because all our methods of explanation, prediction, analysis, and
comprehension already depend on things that are presumably changing. The
change in question would be entirely mysterious. Logical laws might still be
variable, of course, but all our methods of understanding assume otherwise. In
essence, logic is a horizon beyond which none of our earnest and self-reflecting
arguments can help us see.
This last, ultimate result is admittedly eccentric, but it is this very eccentricity
that, we believe, establishes the point we most want to stress: logic is strange.
Logic is indeed one of the strangest things in the world, if it is even “in” the
world at all. Our attempts to understand the world certainly depend on it, but
logic’s principles are beyond the here and now, beyond the local and material,
beyond the arbitrary and accidental. Logical relations are apparently timeless,
placeless, and independent of what any human being happens to think. Or so we
implicitly assume (whatever the time and place) whenever we try to draw a
durably correct inference about what really follows from what. (There are other
ways of viewing the ultimate nature of logic, of course, but the view we offer
here is at least as plausible as any other, and it has one of the longest of all
pedigrees.)13 After all, logic is useful, but only because it helps us discover new
connections between propositions. And this assumes the connections are indeed

“there” to be discovered, independently of whether we already believe in them.
LOGIC AS TIMELESS AND PLACELESS
We might mockingly refer to the notion that logical relations are somehow
transcendent as positing a sort of Never Never Land where the patterns of logic
reside. Mockery aside, however, this way of talking is only a metaphor for
expressing an altogether different idea: the patterns of logic aren’t “in” a land of
any sort. They don’t have a place or a time. Perhaps, though, we can picture this
idea better if we think for a moment, by way of contrast, about physics.
The truths of physics are what they are only because the contents of the
physical universe happen to behave in a particular way. This behavior is indeed a
matter of time and place. Physical objects have locations. But logical truths are
independent of this behavior of time and place—logical truths are true
regardless. An argument’s validity doesn’t depend on whether the things it
describes happen to exist in the physical world. There need not be any actual
laborious hedgehogs. Valid forms of argument remain valid come what may, and
this fact of their validity is also a truth but a truth of a different sort. We can
describe such truths according to laws and principles, and these become the laws
and principles of logic.
Of course, we sometimes makes mistakes about such laws, just as we
sometimes make mistakes in arithmetic, but the behavior of physical objects
doesn’t count as evidence for or against these judgments any more than the
behavior of physical objects counts as evidence for or against the idea that seven
is a prime number. (If we count up seven physical things and find them evenly
divisible by four, we just suppose that we must have miscounted; just so, if we
determine that all wicked witches are irritable and that the Witch of the West,
though wicked, is plainly not irritable, then we just suppose we must have made
some faulty determinations.) Unlike physics, logic isn’t about how things
actually are; it is only about how they could or would be. Logic is a science of
coulds and woulds. As medieval scholars would have explained it, physics is
about contingency, but logic is about possibility and necessity.
The distinction between the contingent world of physics and the necessary one
of logic goes back to the ancients, and anyone who draws the distinction today
soon embarks on the study of something decidedly odd and, in some respects,
ineffable—logic itself. Yet this ineffable something is apparently real, because
without it nothing else would make sense.

The first people to leave a written record of these matters were the classical
Greeks, and from that distant, shimmering moment the story of logic begins. Our
word “logic,” by the way, comes from the Greek logos, but the term didn’t
acquire its present meaning until the second century A.D. In Aristotle’s day, in the
300s B.C., the meaning of logos was only that of principle, thought, reason, story,
or word—the last usage appearing at the opening of the Gospel according to
John (originally in Koine Greek): “In the beginning was the Logos, and the
Logos was with God, and the Logos was God.” When Aristotle first invented
logic as a discipline, he didn’t even have a name for it; he called his studies
“analytics.”
Yet we often forget just how singular the beginning of logic really was. We all
use logic, but not everyone studies it. In fact, we often speak loosely of ancient
Indian logic or ancient Chinese logic, but when we do so we are often confusing
“logic” in the broad sense of argumentation with true logic in the specific sense
of validity. Various ancient peoples studied argumentation. They studied debate,
reasoning, controversy, disputation, refutation, and deliberation, and they
recorded their studies in writing. But only Aristotle inaugurated a study of the
validity of arguments in isolation from an argument’s other features. Why, then,
did no one before Aristotle study logic in this way?
THE SOCIAL HISTORY OF LOGIC
To uncover logic’s peculiar origins as a discipline, and indeed, to answer most
other questions about its history, we shall need to think about some of logic’s
abstractions in their own right, thus, acting like logicians. We shall need to
examine these abstractions in detail. But we shall also need to see how particular
social conditions tended to encourage logical discoveries. For example, why did
logic in the strict sense (meaning the study of deductive validity) start in
classical Athens, and why did Athens remain the center of logical studies for
generations? The answer turns out to depend partly on classical Greek
geography, which encouraged the growth of democracy. Aristotle’s logic began,
in fact, as a reaction to Athenian democracy and to the argumentation of the
Athenian Assembly.
To take another example, why did the logic of the ancient Stoics in the late
200s B.C. gain momentum only after the collapse of the old Greek city-states and
after the triumph of the new imperial regimes of the Hellenistic age? Stoic logic,
which modern logicians now recognize as the propositional logic that runs
computers, derived from a search for a rational, eternal law—a “law of

nature”—and it gained popularity only when a new cultural outlook had
emerged, one that stressed introspection and personal meditation. This
introspective emphasis was actually a consequence of the political absolutism of
the time.
Again, why did the most important work on induction and scientific method
only appear during a much later period—after the wars of religion in Europe in
the sixteenth and seventeenth centuries and then, in a further burst of effort, in
the years surrounding the First and Second World Wars? This time the answer
lies in political turmoil provoked by the growth of trade. The rising commercial
classes of early modern Europe instigated devastating fanatical violence in the
sixteenth and seventeenth centuries, so much so that intellectuals like René
Descartes undertook a new search for the rational foundations of belief. But
these same commercial classes then ushered in, in the succeeding centuries, a
fresh approach to reason and evidence that now underlies the logic of modern
science.
To cite yet another example, why did the study of rhetorical frauds and
sophistical ploys, inaugurated by Aristotle in ancient times, remain largely
undeveloped for more than two thousand years after his death until being finally
revived in the nineteenth century by the eccentric English philosopher Jeremy
Bentham? (Despite occasional advances in the intervening centuries, Bentham
complained, “From Aristotle down to the present day . . . all is blank.)”14 The
reason for this revival was the rise of public opinion as a modern political force.
Popular opinion carried increasing weight in the political struggles of the late
eighteenth and early nineteenth centuries, and the response of Bentham and his
followers was to catalogue the many different ways in which devious speakers of
his day had tried to divert public attention and thwart the public good.
Finally, why did symbolic logic, which manipulates signs by mechanical rules
and underlies modern computing, remain largely unexplored until the mid-
nineteenth century, just as the Industrial Revolution gained momentum? As it
happens, mechanization and symbolic logic are intimately connected. Symbolic
logic is essentially a consequence of an age of machinery, and it has given rise,
in turn, to a new generation of machines—the logic machines we call computers.
There are solutions to the many curious puzzles of logic’s long history, and in
the pages that follow we shall seek them out. Our method, however, will be to
look at social forces—forces that have played out during formative periods—and
we shall look at the forces behind four broad categories of logic as understood
by professional logicians today: classical deductive logic, inductive logic, the
analysis of informal fallacies, and modern symbolic logic.

Logic underlies every attempt to scrutinize the world, but the study of logic
has emerged only in precarious ways. Without a series of peculiar accidents—
accidents of geography, trade, and politics—logic as a discipline might never
have existed. Many individuals helped to make the discipline possible, including
the ancient Stoic philosopher Chrysippus; the medieval theologian Peter
Abelard; the modern thinkers René Descartes, David Hume, Jeremy Bentham,
George Boole, Augustus De Morgan, John Stuart Mill, Gottlob Frege, Bertrand
Russell, and Alan Turing; and, of course, the enigmatic Aristotle. But behind
each of these figures there was also a willing audience. Logic is in many ways
philosophical, and many of its visionary contributors could be equally called
philosophers. But logic is also a discipline in itself, whose history we mean to
examine as a window into its inner nature. All things considered, then, what
provoked people to study the different branches of logic in the first place? And
where, in the end, will the discipline lead us?

1
THE DAWN OF LOGIC
MODERN LOGIC is, in many ways, an elaboration of ancient Greek logic, but
Greek logic depended ultimately on an extraordinary landscape—a landscape of
narrow valleys and rugged mountains surrounded by an expansive sea.
Specifically, Greek logic was an indirect consequence of two geographical
accidents (and we shall be talking about the social effect of these accidents for
some time). First, the mountains of Greece, along with the many small islands of
the Aegean, separated the classical Greeks into hundreds of politically
independent communities. Second, the water nevertheless tied these
communities by an easy method of transportation: seafaring. These two
accidents together allowed for nearly all historically significant features of
classical Greek thought. Or, put another way, it was this arrangement of
independent communities connected by an easy method of transportation that
made the Greeks so different in their approach to logic and also extraordinarily
different in much else. Of course, geography alone didn’t bring Greek logic into
being; an abstract discipline’s creation requires many things, and every ancient
civilization, whether Greek or non-Greek, depended on certain preconditions
like potable water and agricultural surpluses to make its way of life possible. In
addition, the Greeks certainly profited from a temperate climate and from their
contact with the older civilizations of the East (points often made by historians).1
Nevertheless, Greece’s special geography—independent states surrounded by a
navigable sea—was the main factor that made the Greeks so different from other
settled peoples. And why geography should have had such an effect in the first
place takes more than a little explaining.
THE EFFECT OF GEOGRAPHY ON THE FLOW OF IDEAS
For one thing, it was especially easy in classical Greece for dissident
intellectuals to escape political control. This was one result of the geography. If
you got into trouble with the authorities in one community, you could board a
ship and float away, and in half a day you were likely to reach the territory of a
rival state, where the enemy of your enemy becomes your friend. Aristotle, for
example, used this expedient to flee Athens in 323 B.C. And earlier, in 399,

Socrates shocked friends and enemies alike because he refused to take advantage
of this same method of escape—a method that his contemporaries had long taken
for granted.2 The upshot, then, was that classical Greeks were generally freer
than other peoples to discuss a broad range of ideas.
By contrast, in a large territorial empire such as Pharaonic Egypt or imperial
China, the ruler’s reach was long, and thus an intellectual with controversial
doctrines was in constant danger of arrest. Indeed, it was for this reason that the
great age of classical Chinese literature came before the unification of China,
during the waning years of the Chou Dynasty in the sixth century B.C. down to
the end of the Warring States Period in the third century, when many of China’s
greatest thinkers made a practice of traveling from state to state.
We see this same effect—the effect of geography on the flow of ideas—even
if we consider the ancient Greeks’ artistic styles. In art, as in many other things,
the early Greeks long looked in wonder at the older civilizations of the eastern
Mediterranean—especially Egypt—and they knew Egyptian civilization was far
older than their own. The time from the present day back to Aristotle’s lifetime
is only slightly longer than the time from Aristotle’s lifetime back to the building
of the Great Pyramid at Giza. Yet, when we now look upon ancient Egypt’s
artistic tradition, we see a nearly unbroken line. In painting, sculpture, and
architecture, Egyptian artists experimented and innovated but only within narrow
limits set by the priests. We might attribute this conformity to various cultural
factors, but behind them all was a basic, physical fact: Egypt’s terrain was much
more easily unified than Greece’s, and consequently its artistic activity—no less
than its literary activity—was easier to control. Escape was impractical.
Egyptians could still trade with other peoples, but conformity was enforced.
In Greece, this sort of control became impossible; as the sea trade between the
rival city-states expanded, especially after the retreat of Persian forces from the
Aegean Sea in the fifth century B.C., the different Greek cities competed in all
things—in arts as much as in military power. Artistic styles succeeded one
another with bewildering speed, and many Greeks, in reaction, longed for an
earlier, simpler age. But all sensed that the new was rapidly replacing the old.
(The one great exception was Sparta, which tried to wall itself off from cultural
change to preserve an iron grip on its serf population of Helots. But this
approach was easier for Sparta, because Sparta was comparatively far from the
sea.)
THE EFFECT OF THE SEA TRADE

Greece’s peculiar geography had another effect too, no less important for the
development of logic: the ease of navigating the Mediterranean Sea—one might
almost call it the world’s largest lake—encouraged trade, and this gave rise to
large commercial classes in the various Greek cities. The quarrels of these
classes then became the raw materials of logic. The members of these groups
were more on a par with one another than people in other ancient societies, less
in subordination to a single king or a tiny ruling elite. As a result, political power
depended much more on persuading large numbers of city-dwellers to follow
your lead. Politics came to depend crucially on public speaking, and this, in turn,
made public speaking a matter of intense study.
People in this circumstance wanted to know which arguments were more
persuasive (or less so) and which arguments were sensible (or silly). They
realized early on that the sensible and the persuasive weren’t always the same. In
consequence, many became preoccupied with the varied ways in which
argumentation might be manipulated for good or ill.
More generally, when we now look upon the vast collection of the world’s art
and literature, we often suppose that intellectual innovation, including the
discovery of logic, ought to have come from the great empires of the past, from
mighty kingdoms and opulent palaces whose staggering power is still dimly
evident in a pile of ruins. This, however, is a mistake. On the whole, most
intellectual and literary history actually centers on small and contentious places,
especially marketplaces, where equals crowd upon equals and where new
doctrines are hard to suppress. Intellectual innovation typically comes from ideas
in collision, but this collision, in turn, usually comes from competing states that
are nevertheless tied by trade.
In fact, we see this pattern of innovation around the world. We see it in the
principalities of ancient India at the time of the Buddha; in the divided China of
the days of Lao Tzu, Confucius, and Han Fei Tzu; and in the Arab states of
medieval Islam after Islam had become fragmented. In all these instances, we
see intellectual innovation cropping up among independent states that are tied by
trade. (It was only after the fragmentation of classical Islam into opposing states
from the eighth century onward that figures such as al-Farabi, Avicenna,
Averroës, and Maimonides could travel from one state to another and so find
refuge while working out their doctrines. Maimonides was a Jew, but he was also
an Arab living under Islamic regimes.)
Again, we see this pattern of innovation in the rival city-states of Renaissance
Italy. How different the Renaissance would have been if its artists, writers, and
thinkers had been unable to travel from state to state, evading arrest,

sidestepping traditions, and searching for new patrons. Italy during the
Renaissance was a checkerboard of opposing states, all connected by trade.
The pattern appears once again in the nation-states of Northern Europe after
1500. What is most odd about Northern Europe is that, through most of written
history, it was simply a backwater. In Roman times and through much of the
Middle Ages, Northern Europe was poor, provincial, and largely unoriginal; it
exercised real power over other regions of the earth only to the extent that it
could muster large, illiterate armies, as happened at brief intervals during the
Crusades. Nevertheless, after 1500, a shift in the spice routes turned these
societies into a collection of competing maritime powers, and consequently their
writers and intellectuals, like the writers and intellectuals of ancient Greece,
experienced a new freedom—the freedom that comes from having the option of
escaping by sea.
This last, peculiar case—the case of Northern Europe—seems especially
strange in view of the enormous, subsequent impact of European colonialism,
but it is much easier to understand if we pause for a moment to compare it with
the example of early modern China. The comparison will help us to see how a
similar process, rooted in trade, could play such a decisive role in Greece. The
geography of trade is, in fact, the fundamental determinant in all periods of
logic’s development; trade shapes the types of audiences that the logicians of any
age will find waiting for them. And this was no less true for early modern
Europe—in contrast to China—than for classical Greece. Europe was like China
in trying to develop trade across the seas, but for China the consequences were
different.
During the fifteenth century, Chinese navigators sought to expand their
commerce across the entire Indian Ocean, but in the end they found the effort
unprofitable. The Chinese admiral Zheng He assembled grand treasure fleets that
employed crews of thousands, and his ships were far larger than anything then
operating in Europe. But his expeditions lasted through only part of one dynastic
period, from 1405 to 1433. The reason was that, like the Apollo moon missions
of the United States during 1960s and 1970s, his voyages advanced power and
science but brought back little of commercial value. The things the Chinese
wanted were simply too close at hand to justify such far-flung efforts.
For Northern Europeans of the same period, by contrast, the sea trade was
immensely profitable because it carried spices that were both remote and highly
prized. Spices grew in places distant from Europe—in Asia—and, without them
(and without refrigeration), the European diet was decidedly bleak. Spices
disguised the disagreeable and enlivened the dull. The traditional route for these

spices was over land, through the Middle East, and then from the eastern
Mediterranean to the rest of Europe; but once Portuguese explorers discovered
an ocean route around the coast of Africa, the incentives to further navigation by
Europeans became inexorable. European seafarers planned desperate voyages in
tiny caravels, barely sufficient for the ocean’s swells. Their initial expeditions
were small and fragmentary but relentless. And in the end, they changed the
whole history of the world. When these explorers finally stumbled on the
Americas, they were soon trading for Asia’s spices with American silver,
supplying nothing of their own into the bargain but the conveyance. Yet the
conveyance was all. This new international market spurred their technology,
shifted Europe’s wealth from its Mediterranean ports to its Atlantic ones,
expanded its rivalries into a struggle for world domination, and gave its
intellectual luminaries the option of escaping political authority by sea.3 (To
express this last point in another way, we might almost say that, if Asians had
needed to sail to Europe for spices rather than the other way around, most people
in the Americas might now speak Chinese.)
This option of escaping by sea has almost always been the driving force
behind prolonged periods of intellectual change, and the history of logic
depended on it too. The option of escape encourages discussion, and discussion
is the seedbed of logic. Of course, many famous thinkers in the history of ideas
have never needed to escape; many never left home, and many never
contemplated a foreign retreat. Nevertheless, once escape became possible, the
local authorities always had a harder time controlling what people said. Simply
too many people could get away, and as a result, the limits of the permissible
grew wider. In fact, the effect of escape on intellectual history becomes
especially clear if we take a moment to add up the large number of famous
writers and thinkers who spent at least part of their lives traveling from one state
to another.
For example, among those reputedly traveling to other states by land or on
rivers were Confucius, the Buddha, Mencius, Sun Tzu, the Legalists,
Muhammad, al-Farabi, Avicenna, Dante, Machiavelli, Luther, Calvin, and
Nietzsche. But the list of those traveling to other states by sea is even longer,
including Herodotus, Thucydides, Plato, Aristotle, Demosthenes, Averroës,
Maimonides, Erasmus, Thomas More, Descartes, Thomas Hobbes, John Locke,
Montesquieu, Voltaire, Rousseau, David Hume, Adam Smith, Thomas Paine,
Mary Wollstonecraft, Heinrich Heine, Tocqueville, Frederick Douglass, Charles
Dickens, Emile Zola, Marx, Engels, Darwin, Mark Twain, Herman Melville,
John Stuart Mill, Lenin, and Freud. Now, is the fact that so many of the world’s

famous writers and talkers traveled from one state to another merely a
coincidence? Some of these people were refugees, others merely tourists, but the
fact that they could travel at all shows that escape was an option. The easier the
option of escape was, the easier the task of introducing new ideas.
TRANSPORTATION AND CIVILIZATION
Transportation is the essence of this process, and on the whole, it depends on a
physical 
geography 
that 
generates 
independent 
communities 
while
simultaneously allowing them to interact. Indeed, the history of the world’s
economic development, along with much political and scientific development, is,
in many ways, a history of transportation—a fact first pointed out by Adam
Smith.
Smith observed that early societies, whether politically divided or not, became
powerful and prosperous only to the extent that geography encouraged their
easiest form of transportation: navigation. Early civilizations almost always
planted themselves along waterways. As examples of this effect, Smith cited the
ancient cultures of the Nile, the Ganges, the rivers and canals of China, and the
coasts of the Mediterranean, but we could also add the ancient cities of
Mesopotamia, the capital of the Aztecs at Tenochtitlán (which was laced with
canals), and the medieval city of Timbuktu (which connected Saharan caravans
to the Niger River).
What navigation did, above all, was facilitate trade with larger numbers of
people, and this, in turn, forced competitors into ever-greater specialization. But
with increased specialization came new technology because specialization forces
you to focus on a more particular technique.
Smith lays out all these points in the first three chapters of his Wealth of
Nations (1776), but his insights are still poorly understood, and the best evidence
of today’s confusion over the matter is the current popularity of a rival
explanation offered by Jared Diamond in his book Guns, Germs, and Steel: The
Fates of Human Societies (1997).
Consider for a moment how these two explanations differ. Like Smith,
Diamond argues that a crucial factor in ancient societies was the ease of
transportation, but unlike Smith, Diamond focuses on transportation of a
different sort: transport over land. Diamond attributes Europe’s wealth and
power (its guns and steel) to the ease of traversing the landmass of Eurasia, and
he invokes a similar explanation to account for the dissemination of European

diseases.4 In addition, he says the many peninsulas of Europe made the region
easier to defend from invasion.5 The difficulty with all these points, however, is
that these factors only truly apply to the Stone Age, when Europe’s powerful
states were still nonexistent. In historical periods, such factors were largely
irrelevant. On the contrary, from the dawn of written history until the invention
of railroads in the nineteenth century, guns, steel, and diseases moved mainly
over water, and technology almost always advanced most rapidly in seaports or
in cities along rivers. Thus what mattered most were the ways in which the
world’s waterways favored advances in navigation—first on rivers and canals,
then on the Mediterranean, and finally across the oceans.
To say this, of course, is not to deny that invading horsemen sometimes
established vast dominions on land, but these land empires were rarely durable
unless supported by a navigational network—either inland, as in China, India,
and Mesopotamia, or external, as the one surrounding Greece, Rome, and the
Ottoman rulers of Istanbul. In historical times, traversing Eurasia by land had
little to do with consolidating real power.6 In effect, then, Smith’s account is an
explanatory rival to Diamond’s, and what Smith’s theory entails is that early
societies became technologically powerful only insofar as geographical features
encouraged their navigation.
CLASSICAL GREECE AS THE EXTREME CASE
Still, all the geographical effects we now invoke—especially those that concern
the exchange of ideas—were most extreme in classical Greece during the fifth
and fourth centuries B.C., more extreme than in any other place or time. Why this
was depends on the details.
To picture the Greek situation properly, we first ought to go back for a
moment to an age shortly before the classical era and imagine a fundamentally
feudal and agrarian people just emerging into a new world of maritime
commerce. Former peasants gravitate to the towns where they find increasing
opportunities as artisans. The livelihoods of more and more people come to
depend on shipping, and so the Greeks take a keener interest in suppressing
piracy and controlling the sea. Soon the new wealth of the towns brings political
conflict. The urban commercial classes—known collectively as the demos—
begin to challenge the traditional feudal aristocracy for power. A wave of
tyrannies sweeps through the Greek city-states, but by “tyranny” the Greeks of
this early period didn’t necessarily mean “rule by a villain.” Instead, the term
simply meant rule by someone who comes to power outside the traditional

(aristocratic) law, a tyrannos. This is the situation in many of the city-states in
the 600s and 500s B.C. Finally, in some cities, government gradually falls into the
hands of a mass meeting of the demos (an assembly) and so rule by the demos,
“democracy,” is born.
The demos—how fundamentally the world has been changed by this noisy,
argumentative, and litigious crowd! The demos were mostly city dwellers (or,
more exactly, the most active members of the demos were city dwellers), but
whether we call them burghers or bourgeoisie, the world’s city dwellers have
always been the primary market for new ideas, new dogmas, new products, and
new power—at least until the age of electronic communication. They are also
the group most likely to favor the rule of law because the rule of law allows
strangers to interact with the same predictability that personal ties foster in a
feudal regime. These are the people most apt to profit from logic and rhetoric,
and their dynamism has usually increased whenever an expansion of trade has
inflated their numbers.
The Athenian Assembly was the first, most powerful, and most volatile of the
new Greek democracies, and even now it is safe to say that at no other time in
human history has argumentation and public persuasion played so decisive a role
in a society’s affairs. Why? The explanation rests once more on the peculiarities
of geography.
Greece’s mountains are particularly rugged, the valleys between them
especially small. But these valleys, filled with rich alluvial soil from the hills,
generally end with the sea, and the sea in question happens to be the most
extensive yet protected sea anywhere in the world. The Mediterranean is quite
unique. Cut off from the great tides and immense waves of the world’s oceans,
the Mediterranean was particularly favorable to ancient seafaring. And in
economic and political terms, the effect of these geographical accidents was then
to accelerate tendencies that would later recur repeatedly in the modern world.
The most important of these tendencies was the emphasis on debate—an
emphasis well established long before Aristotle came along. Given their military
technology, these little cities found it extremely difficult to conquer neighboring
valleys, and thus many of these places remained independent. Yet they were
increasingly filled with merchants and artisans of similar social station, who
became an ever-larger part of the state. As a result, their assemblies debated a
great range of questions. They debated war and peace, public works, the
regulation of trade, the correct way (or what they thought was the correct way)
of propitiating the gods, and countless other matters. Nor were these debates
filtered through a medium of elected representatives.

The modern idea of representative democracy is largely a product of vast
expanses—first, of extensive monarchies, then of nation-states—where a single
mass meeting of citizens is impossible. But the early Greek democrats were all
crowded on top of one another as a matter of daily life, and they suspected one
another intensely. In consequence, when deciding political questions, rather than
selecting only a few of their number to speak for the rest, it was far more natural
for them to crowd all their various debaters into a single space and to thrash out
their difficulties with a great deal of contention.
These political antagonisms are obvious in Greek literature. Greek dramas, as
well as Homer’s works, typically take political discord as the background of the
action—a theme that would have been far more dangerous under a unified,
central authority, as in Egypt. Agamemnon, Achilles, Oedipus, Antigone—all
these figures are either rulers defied or defiers of rulers. Thus we see the stamp
of Greek political life, generated by their peculiar geography, impressed on their
mythical stories. We even see this stamp on their mathematics.
What is distinctive about Greek mathematics is not the extent of its
calculations; many ancient peoples calculated. Instead, what sets Greek
mathematics apart is the emphasis on proving things as theorems. The Greeks
have axioms, postulates, and definitions, and what they then want from these
materials is not merely the right answer but a proof of the right answer. Yet proof
is a subject that appeals especially to a population that has already been
sensitized to argumentation. Put another way, Greek mathematics isn’t just
calculation but calculation combined with a relish for logical demonstration, and
this relish comes once more from an argumentative environment. With this
understanding, it is easy to see why the classical Greek environment would favor
the development of logic, but the most argumentative of any Greek environment
was the city of Athens, and the reason was the city’s Assembly.
THE ATHENIAN ASSEMBLY
The Athenian Assembly was, in many ways, a collection of paradoxes. In terms
of who could vote, the Assembly was decidedly conservative, at least by modern
standards. In the 430s B.C., Athens seems to have had a total population of
roughly a quarter million, yet the number of citizens qualified to attend the
Assembly was probably only around forty to forty-five thousand.7 Indeed, there
were more slaves in Athens than voting citizens. Also, women were excluded
from open political participation, though the cause was at least partly
technological. The subordination of women throughout much of human history

is a complicated topic, in many ways mysterious; nevertheless, the thing to
remember in the Athenian case is that the Assembly was first and foremost a war
council. Military security was the chief daily problem of the classical Greeks. In
consequence, all groups who contributed substantially to the military demanded
a voice in the Assembly, and the price of excluding them was military
insolvency. Yet they also guarded this privilege jealously. Noncombatant classes,
including women, were excluded.
Now this last observation may initially seem question-begging because it only
raises the further difficulty of explaining why women were excluded from the
military. But here a crucial factor was the nature of ancient weapons. Combat in
the ancient world was seldom a matter of operating complex machinery. Instead,
on land, it was usually hand to hand, and it therefore depended crucially on
bodyweight and shoulder power. Homer’s warriors are certainly an exaggeration
of what ancient peoples looked for in a fighter, but on this particular point the
warriors are fundamentally instructive. Homer’s heroes are all especially strong
and especially broad-shouldered. In modern terms, they have almost the same
physique as linebackers in the National Football League. And in the navy, the
fleets needed large numbers of strong-armed rowers. In other words, the military
technology of the ancient world favored the typical male physique over the
typical female one, and one of the political consequences was to keep women
out of the Greeks’ premier arena of debate. As a result, then, logic and rhetoric
were first studied mainly by men. (Carefully considered, what this point shows is
that nothing in the early history of logic or rhetoric precludes women from
competing in either discipline on equal terms. Rather, the early predominance of
men came from the composition of the Assembly, and this in turn rested, at least
in part, on the nature of ancient weapons. More generally, the study of logic is
most easily encouraged by free debate, but the subordination of women has
always tended to deny them this freedom.)8
More important still, however, for the history of logic, was the Athenian
Assembly’s routine operation, and in this further respect the Assembly was
anything but conservative. On the contrary, it was among the most radical and
volatile deliberative bodies in all human history. Of course, it is possible to think
of assemblies that were more radical and volatile, such as the Convention of
France, perhaps, which presided in 1793 over the Terror, or maybe the
government of the Paris Commune in 1871.9 But these other assemblies were
short lived, and their composition was never tied in any durable way to the
societies they tried to control. The Athenian Assembly, by contrast, lasted for
generations, and its volatility had the effect of giving special prominence to the

arts of persuasion, and thus argumentation, because power depended, above all,
on persuading large numbers of voters. But to see just how volatile the Assembly
could be, consider for a moment the true sweep of its authority.
The Athenian Assembly is sometimes likened to a New England town
meeting, but this is, in many ways, a gross understatement of its province. A
quorum for its most important business could be as high as six thousand, but
more significant was the broad range of political and military questions the
Assembly presumed to answer. It made treaties, sent soldiers into combat, and
often debated just how many warships to send to a particular theater of
operations. In at least one instance, the Assembly condemned to death the entire
adult male population of a subjugated island (Melos).10 In another instance, it
executed six of its own generals for misconduct. (These were six of the eight
generals from the battle of Arginusae in 406 B.C.)11 It made all these decisions by
simple majority vote. Of course, a New England town meeting does none of
these things.
More striking still, the Athenian Assembly ruled over more than 150 formerly
independent city-states, and it ruled these states despotically. That is, the other
cities the Athenians had subjugated were entirely subordinated to the Assembly’s
decisions, an assembly in which they had no representation whatever. The
Athenians were imperialists and proud of it, and they saw no contradiction
between this circumstance and the principle of “democracy”—rule by the
demos.12 A case can be made that many cities subjugated by Athens actually
benefited from its rule because of increased commerce, but even the Athenians
admitted that their regime was resented. During a particularly grim moment in
their debates, their preeminent politician Pericles warned them, “Your empire is
now like a tyranny. It may have been wrong to take it. It is certainly dangerous to
let it go.”13
Despite this despotism, the Assembly experienced startling changes of heart.
On yet another occasion, it voted to kill all the adult males of another island
population (the inhabitants of Mytilene on Lesbos), but after a fitful night’s
sleep, the Assembly reopened the question and decided against doing so.14 The
Assembly’s policy was probably steadiest and shrewdest under Pericles, but
even Pericles, near the end of his career, was voted out of office and fined. Yet
the historian Thucydides remarks, “Not long afterwards, as is the way with
crowds, they reelected him to the generalship and put all their affairs into his
hands.”15
This Athenian tendency toward vacillation made a deep impression on James
Madison when he contemplated their example many centuries later and sought to

make the Federal Constitution of the United States fundamentally different. He
writes in The Federalist (no. 63), “What bitter anguish would not the people of
Athens have often escaped if their government had contained . . . a safeguard
against the tyranny of their own passions?” Madison deplores their tendency
toward temporary majorities, and he characterizes the Athenians as “decreeing to
the same citizens the hemlock on one day and statues on the next.” But our
purpose in making these points is neither to condemn the Athenians nor to praise
them. Instead, our purpose is to stress just how unusual the Athenian case was.
The Athenian case was the extreme case. Other societies have approached the
Athenian example to greater or lesser degrees, but none have ever equaled it for
any prolonged period. In Athens, popular opinion was everything, and popular
opinion was everywhere dominated by public speaking. Thus the ability to
construct an argument was far more important in their daily life than in the life
of any other people—before or since. Is it any wonder, then, that the history of
logic should start here?
It is also worth keeping in mind the rapid cultural change of classical Greece
because this, too, gave the Greeks much to talk, quarrel, and worry about. Small
village communities often hold public meetings of their own, especially in times
of crisis, and so we might expect all peoples to take at least some interest in
argumentation. For the classical Greeks, however, the crises were relentless
because their societies were in constant flux. As Pericles told the Athenians, “We
have grown up in a world of many changes and many chances.” Pericles was an
optimist: “Future ages will wonder at us, as the present age wonders at us
now.”16 But whether good or bad, fortunate or unfortunate, this situation
persisted for more than a century and a half, and it faded out only after the
independence of the city-states had been finally destroyed—destroyed by a new
commander who at last had found a way to subdue the Greek terrain with a new
kind of army. That commander was Philip II of Macedon, the father of
Alexander the Great. It was at just this moment that, with the rise of the
Macedonian king Philip, the greatest of the Greek logicians appeared—first to
tutor Alexander and then to dominate logic for more than two millennia. But the
life and times of Aristotle is a story all its own.

2
ARISTOTLE
Greatest of the Greek Logicians
THE STYLE of Aristotle’s treatises is so clinical and detached that it is easy to
forget how close he was to the relentless violence of his time. In fact, Aristotle
was intimately connected to one of the Mediterranean world’s most violent
political machines: the regime of the kings of Macedon. His father had been
court physician to Philip II’s father,1 but Aristotle was packed off at around the
age of seventeen to the turbulent city of Athens, where he entered the school
founded by Plato, the Academy. The Academy must have seemed a tranquil
oasis amid the perils of the age. We know precious little of Aristotle’s time there,
but we do know he remained roughly twenty years and probably served as an
instructor. Plato is said to have dubbed him “the reader” for his studies, and
when Aristotle speaks of Plato in the treatises that come from later in Aristotle’s
life, even when criticizing his teacher’s doctrines, he speaks with affection.
Nevertheless, when Plato died in 347 B.C., Aristotle was passed over for the
Academy’s leadership, and so he went east to Asia Minor (present-day Turkey)
to the Greek-speaking settlements along its coast to strike out on a new path.
Aristotle is said to have accepted an invitation to reside with an old friend,
Hermias, who had risen from slavery to become ruler of the cities of Atarneus
and Assos. Aristotle eventually married Hermias’s niece Pythias and seems to
have devoted his years in the East at least partly to research in biology. But there
was soon trouble brewing for Hermias, who was exploring the idea of a military
alliance with Macedon (which lay to the north of classical Greece). Hermias had
planned to help Philip in an invasion of the Persian Empire, but in the end he
was captured by Persian military officers, tortured, and executed. (According to
legend, Hermias’s last message was, “Tell my friends and companions that I
have done nothing weak or unworthy of philosophy.”) Aristotle may have been
living on the nearby island of Lesbos when he learned of Hermias’s fate, but
finally Aristotle returned to Macedonia to accept an offer to tutor the young
Alexander, then about thirteen years old. This arrangement seems to have lasted
some three years, though what influence he might have had on his prestigious
pupil, we don’t know.

The next phase of Aristotle’s life came with the assassination of Philip in 336
B.C. Aristotle’s tutoring had already ended when Philip was stabbed to death by a
disgruntled bodyguard. Philip had indulged in an ancient tradition of
Macedonian kings—polygamy—and he had just had another son by a new wife;
in consequence, suspicion of complicity in the attack fell almost instantly on an
earlier wife, Olympias, Alexander’s mother. But the assassin was killed as soon
as he struck the fatal blow, and so he told no tales. Nevertheless, according to
ancient writers, Olympias later had the new wife and her infant son murdered.2
With the demise of Philip, Aristotle’s royal pupil then ascended the throne,
and Alexander began his brief but meteoric reign. Some Greek cities took
Philip’s assassination as a signal for revolt, but Alexander responded by having
the entire city of Thebes destroyed in 335, exempting nothing but its temples and
the ancestral home of the poet Pindar. Resistance died out. Then, at about the
same time, Aristotle returned to Athens to establish a new school—his famous
Lyceum—with Macedonian financing.
In some respects, Aristotle’s Lyceum flourished in a political vacuum,
intellectually remote from events on the ground. For example, Philip’s conquests
had already put an end to the old city-state system of Greece, yet in his treatises
Aristotle still speaks of the city-state (the polis) as if it were the only natural
political unit. Also, Alexander is thought to have wanted to extend Greek culture
far to the east, eventually to the Indus River in present-day Pakistan, and to have
advocated a “marriage of East and West” by urging his Macedonian officers to
take Persian wives. Yet Aristotle still clings to an earlier Greek conception that
Greeks are somehow biologically superior to non-Greeks.3
As for personal relations between the two men, ancient writers say Alexander
sent his teacher biological specimens from the new lands he had conquered, but
the authenticity of these tales is doubtful, and against them we must also set the
unhappy story of Aristotle’s nephew Callisthenes. Callisthenes had accompanied
Alexander’s army as official historian and had also been put in charge of
educating the royal court’s pages. But when some of these pages were then
arrested for allegedly conspiring to murder Alexander, one of them said under
interrogation that Callisthenes had spoken in their lessons of political
constitutions, tyrants, and tyrannicide. As a result, Alexander ordered that
Callisthenes be imprisoned in a cage that could be moved with the army from
place to place, and after many months in the cage, Callisthenes died.4
It is difficult, then, to see Aristotle as a mere stooge for Macedonian
hegemony, but, in fact, it is difficult to get any clear picture of the philosopher at
all. The style of his surviving writings is terse, critical, professorial—containing

little in the way of passionate fire.5 So far as his demeanor goes, the ancients
called him the Peripatetic, meaning he walked about while lecturing. This was,
however, the fashion of the time: to walk back and forth with one’s students
beneath an arcade or stoa.6 Nevertheless, what we can say is that Aristotle turned
the validity of arguments into a distinct object of study. And the best way to see
the difference between his efforts and those of anyone else is to take a moment
to compare his work with developments elsewhere in the ancient world,
especially in India. Events in ancient India will give us a clearer picture of what
was happening in ancient Greece.
THE STUDY OF ARGUMENT IN INDIA
The study of argument in India goes back at least to the fourth century B.C.,
perhaps farther, but ancient Indian philosophers never investigated valid
argumentation as such—a fact that becomes evident once one looks at the most
common of all forms that Indian philosophers did investigate, the Indian
syllogism, which initially appeared around the second century A.D.7 The Indian
syllogism traditionally has five parts, or members, as follows:
1. A proposition to be proved
2. A reason for its being true
3. An example to show that the reason embodies a general rule
4. An assertion to the effect that the reason applies to the case at hand
5. An assertion that the proposition must therefore be true
A stock example goes like this:
1. The hill is on fire.
2. This is because it is smoky.
3. Wherever there is smoke, there is fire—as in a kitchen.
4. And there is smoke in this instance.
5. Therefore, there must be fire here, too.8
Now there is some question as to whether this form is actually an effect of Greek
models carried to India by the conquests of Alexander,9 but, however that may
be, the Indian syllogism as a rhetorical form is entirely natural even for English
speakers. Imagine, for instance, two automobile drivers arguing in the streets of
New York about whether one of their automobiles is on fire. At least part of the

exchange might go like this:
Your car’s on fire! Why? ’Cause it’s smoking! Where there’s smoke, there’s fire. That’s how it works
in your kitchen, right? Well, you got smoke here, buddy. So I’m telling you, you got fire!
In form, these utterances follow the classic Indian pattern: proposition, reason,
rule with a further example, application to the case at hand, and conclusion. Still,
there is a basic difference between the way an ancient Indian philosopher
approaches this argument and the way any Greek logician after Aristotle
approaches it. The difference is that the Indian philosopher treats it as a
rhetorical form whereas the Greek seeks out what we would now call a logical
form.
More precisely, the Indian philosopher has structured the syllogism for
maximum rhetorical force, and he does this by including repetitions. The first
and last elements of the syllogism both express the point to be proved, and the
second and fourth elements both express what a Greek logician would call the
minor premise (that the hill is smoky). But the Greeks typically cut these
repetitions away. Instead, after Aristotle, a Greek logician renders the argument
like this:
All smoky things are on fire.
This hill is a smoky thing.
Therefore, this hill is on fire.
In terms of a pattern, we now have the same structure as the following:
All men are mortal.
Socrates is a man.
Therefore, Socrates is mortal.
But why do the Greeks eliminate the repetitions while the Indians don’t? To
answer this question will be to understand why the study of the validity of
arguments started in Greece—and uniquely in Greece.
Part of the answer is that the Greeks and the Indians have different aims. Both
groups definitely grasp the argument’s validity, and if they didn’t, they would
hardly make paradigms of their respective versions. But the Indians want to use
the argument for persuasion. The Greeks, on the other hand, want to analyze
how it works. Specifically, the Greeks aren’t asking whether the argument is
valid; whether it is valid is the question one asks if one’s purpose is to persuade.
Instead, the Greeks are asking why it is valid; they want to know what structural
features make it so. But once the Greeks ask this further question—why it is

valid—they see straightaway that the argument is valid in virtue of three
elements, not five. The argument is rhetorically forceful because of five elements
but valid because of three. In effect, then, the Greeks distinguish rhetorical force
from logical force.
In fact, Indian philosophers spoke of a three-part syllogism later in history, in
the fifth century A.D.,10 but they never investigated the many different forms valid
syllogisms might take.11 Nor did the ancient Chinese. Chinese philosophers of
the third century B.C. working in the tradition of Mo Tzu distinguished fallacious
inferences that came from constructing sentences in parallel. (For example, if we
ask about a man’s illness, then it follows that we ask about the man; yet, if we
dislike the man’s illness, then it doesn’t follow that we dislike the man; the first
inference is valid, but the second is fallacious.)12 Nevertheless, these Chinese
philosophers never investigated valid argumentation as such, not in the sense of
what structural features make an argument valid. Indeed, the historical truth here
is really quite stark: no ancient culture ever investigated valid argumentation as
such except Greeks or people influenced by Greeks. But the truth here is even
more stark.
THE SINGULARITY OF ARISTOTLE
So far as the historical record goes, all studies of deductive logic—meaning all
investigations of logical necessity as a relation between propositions—derive
from Aristotle alone. If anyone else ever investigated the matter independently
of his influence, that person has left no historical record of the work. Of course,
various ancient peoples studied argumentation—debate, reasoning, controversy,
disputation, refutation, and deliberation—but these studies always mixed logical
force with rhetorical force. Only Aristotle made logical force a subject of study
in isolation from an argument’s other features. Other thinkers also studied the
difference between genuine knowledge and mere belief, but this, too, is different
from studying validity. (An argument can be valid even if none of its premises
count as genuine knowledge, and an invalid argument can have genuinely known
elements. Knowledge and validity are different subjects.) In sum, then, every
study of validity we have comes either from Aristotle or from his readers—or
from his readers’ readers—and this includes later Greeks, later Romans, early
medieval Arabs, twelfth-century Europeans, and students of modern symbolic
logic and computer programming around the world.
In some ways, this dependence on Aristotle is utterly astounding. As a matter
of history, we might make similar claims for the physical laws of Galileo, at least

in the sense that all modern physics builds on Galileo’s laws. Even students of
relativity and quantum mechanics, though they depart from Galileo, still learn
Galileo first. But modern physics is a specialized field of subtle experiments and
mathematical equations; valid argument, by contrast, is something we all use
every day. All peoples distinguish between rational arguments and irrational
ones. Yet, for some reason, Aristotle is the only person on record to have
invented a study of what makes an argument rational, at least when it comes to
deductively valid structures.
Of course, valid arguments aren’t the only species of rational ones. Validity is
treated by logicians under the heading of deduction, but there is a whole other
domain of rational inference: induction. Deduction concerns logical necessity,
but induction concerns probability. (For example, if the sun has always risen in
the past, then it will probably do so in the future; this argument isn’t deductively
valid, but its conclusion is still inductively probable.) In consequence, validity is
only part of rationality, not the whole of it. Even so, the thing to notice is that, to
a logician, induction is still a matter of logical relations between propositions,
quite apart from rhetorical force and quite apart from whether the argument’s
premises count as genuine knowledge. For induction no less than deduction,
logical force is still distinct from rhetorical force, and a theory of knowledge is
still distinct from a theory of logic. And to study logical force at all still requires
a distinction between the logical force of deduction and the logical force of
induction—because these two sorts of logical force are different. Yet the only
person who seems to have invented a study of logical force for its own sake is
the enigmatic Peripatetic; everyone else has had his example to follow.
How can this be?
For the moment, we seem to be heading toward a Great Man theory of logic.
It would seem that, without Aristotle, no one on Earth would study logic—not in
the precise sense. Yet, on further reflection, this Great Man theory is implausible,
because logic doesn’t really require a Great Man in the first place. In saying this,
we don’t mean to deny Aristotle’s greatness but only to say that we can all study
logic anyway, without a Great Man, because the basic idea of validity isn’t that
deep. We can all perceive validity with just a few examples.13 What sets Aristotle
apart, in other words, isn’t that he could grasp the idea of validity any better than
we can. Instead, what sets him apart is that he and the Athenians who attended
his lectures found the idea interesting at a time when no one else did. The real
question, then, is why they did so.
To reinforce this point, we should remember that philosophers in both ancient
India and ancient China distinguished what they called fallacies, and by fallacies

they meant mistakes in reasoning, not simply mistakes in rhetorical
presentation.14 They regarded some arguments as logical, others as illogical—
quite apart from rhetorical force. What they didn’t do, however, was study the
difference between the logical and illogical as a matter of formal structure.
Formal structure was Aristotle’s key move.
To put this last observation differently, logical form is apparently what gives
rise to logical force, but logical force is distinct from rhetorical force. Thus
logical form is distinct from rhetorical form. After all, the five-part Indian
syllogism is a rhetorical form, but it becomes a logical one only after the
repetitions are deleted because the repetitions have nothing to do with its
validity. What Aristotle initiated, then, wasn’t simply an investigation of validity
but validity in virtue of form, and this, indeed, is how we understand validity
today. When we now assess the validity of an argument, we explain the
particular by appeal to the general, and we regard particular arguments as valid
because they embody certain general forms. The generality of these forms,
which we have also called “patterns,” is expressed by our use of variables—our
As, Bs, and Cs—as in some of our earlier examples:
All As are B.
If A, then B.
C is an A.
A.
Therefore, C is B.
Therefore, B.
So the question is why Aristotle became interested in logical form.
(As a technical aside, we should note that professional logicians often use the
expression “logical form” in a more restricted way, so that it applies only to
statements expressed in an artificial language constructed to represent logical
structure. Since Aristotle lays out no such artificial language, he specifies no
“logical forms” in this sense. Nevertheless, his purpose is clearly to analyze
logical structure, and he represents it by distinguishing patterns, which he calls
“schemata.” These logical patterns are different from rhetorical ones, and we
think it most intelligible for ordinary readers if we continue to call these patterns
“forms,” though the professional can just as easily substitute the word
“schemata” or “structures.” Either way, our point is that Aristotle’s aim is to
study the same structural features that an artificial language is meant to reveal.)15
THE EFFECT OF THE ATHENIAN ASSEMBLY
Now, in explaining Aristotle’s interest, one crucial point seems obvious from the

start: If we mean to study valid argumentation as such, then our particular
reasonings will need to exhibit the same logical forms repeatedly. Otherwise, if
our reasonings rarely exhibit the same form twice, our analysis will never give
us much for making generalizations. Each argument will be unlike the others,
and we will find very little that is common to them all. So we won’t be studying
logical forms in the first place. Yet this is exactly where the Athenian Assembly
made such a difference. The Assembly highlighted forms—the same forms over
and over—and we can see why this would happen if we pause to remember just
how much the social situation in Athens differed from the situation in India.
In public assemblies like the one in Athens, controversies come and go;
different issues arise from week to week, sometimes day to day. But, in that case,
the most common methods of argument will tend to return time and again
because speakers learn from experience that only some tactics are rhetorically
effective. In an assembly of ordinary citizens, the audience’s attention span is
limited; if a particular question becomes too complicated, the audience begins to
lose interest. In consequence, then, speakers will tend to use only simple logical
forms, and there are only so many simple forms to go around. As a result, many
forms will occur repeatedly. Given enough time, the effect of studying argument
in a public assembly is to focus attention on logical structure.
In India, by contrast, argument was studied not in public assemblies but in
cloistered schools—a crucial difference when compared with Greece. Indian
princes allowed nothing like the Athenian Assembly to develop, and there were
no geographical features to generate the economic and social conditions for such
an assembly in the first place. Instead, debates were usually disputations
between learned philosophers who represented competing schools (the debates
being officiated, in many instances, by a prince or magnate). Consequently, the
study of argument in India had a different emphasis.
In India, the schools existed to expound ethical and metaphysical doctrines,
but the doctrines were complicated from the start. Each such question was a
world unto itself.16 Common forms of argument did indeed appear, but the
essence of the controversy was typically more complex. Additionally, many of
these schools existed precisely to defend some such doctrine from novel forms
of attack. Thus the effect of argumentation in India was to emphasize not logical
forms, but the doctrines themselves. It was the doctrines that endured, the
arguments for and against them that changed. The same spiritual questions
returned over and over, but the arguments made about them came and went, and
these arguments were logically complicated, not logically simple. As a result,
Indian philosophers left behind an enormous mass of analysis, but this mass is

essentially an analysis of ethics and metaphysics, not an analysis of
argumentation.
In a word, we owe the discovery of validity, at least in part, to the existence of
the Athenian Assembly. The Assembly, unlike a school or a temple, served as a
showcase of forms. This is much of the story of why logic began where it did—
in classical Greece—but not the whole story. It still doesn’t explain why the
Greeks neglected to study formal validity until the late fourth century B.C.
THE SOPHISTS
Throughout the fifth century—from the days of Aeschylus at the beginning to
the days of Pericles, Sophocles, and Socrates at the end—the idea of formal
validity went unexplored. Nor was it explored in the first half of the fourth
century in the days of Plato. To be sure, Plato often spoke in his philosophical
dialogues of “necessity,” but he included in this idea what we would now call
“physical necessity”; he didn’t expound a distinct conception of logical necessity
as something different from physical laws. Instead, argument was still conceived
as in India and China—as a mix of logic and rhetoric. The Greeks called this sort
of inquiry the study of elenchos, which we might now define as “trial by
argumentation.” Aristotle notes that the study of valid argumentation is
something he has had to invent entirely on his own.17 In short, something
happened between the days of Pericles and the days of Aristotle to make the
whole subject of validity more interesting. Equally important, this same
something must have affected not only Aristotle but others—because Aristotle
had an audience. So what was this change?
The crucial change will become clearer if we take a closer look at Athenian
politics and especially at the enormous political disasters of the fifth century B.C.
But not only must we turn to the fifth century’s disasters; we must also look at
the fourth century’s reaction to them. In particular, fifth-century Athenians
committed two grave errors that fourth-century Athenians never forgave. First,
the Assembly lost the long and bitter war with Sparta, a defeat that cost Athens
its empire and caused the city’s population to decline by roughly half.18 Athens
lost control over its many subjugated cities in battles that filled up twenty-seven
years. (This was the Second Peloponnesian War, in which two grand alliances—
Athens and Sparta—struggled for control of Greece from 431 to 404.) Second,
as a coda to these events, in 399, the Athenians executed their philosopher
Socrates. These actions outraged the next generation, but more significant for
logic was how the next generation explained them. The next generation blamed

both Athenian errors on deceptive public speaking. This indignant reaction was
essential to the discovery of validity, and the person who expressed it most
forcefully was Aristotle’s teacher Plato.
In his writings, Plato blames both Athenian mistakes on the teachers of public
speaking. The Athenians allowed themselves to be deceived by clever but
irrational speeches—or so Plato believed—and the leading villains in his account
were the Sophists, who would later serve as foils for the development of logic.
The Sophists were the first paid teachers in Greece, originally deemed wise
men—the word “sophist” comes from the Greek sophos for “wise”—who
typically claimed to teach “excellence.”19 In practice, however, many of them
seem to have focused on the art of winning debates by any means, fair or foul.
This, at least, is the picture Plato paints for us, and the tendency to win a debate
at all costs is the meaning we now give to “sophistry”; it means rhetorical
trickery. Nevertheless, in its beginnings, the Sophistic movement was a dignified
affair.
The first professional Sophists are supposed to have been Corax and his
student Tisias in Greek-speaking Sicily beginning in the 460s B.C.20 The Sophists
were probably Greece’s first grammarians—in some ways eminently scholarly—
and, in fairness, we should remember that almost everything we read about them
comes from their enemies. Perhaps, then, they weren’t nearly as mercenary as
we are told. Their interest in oratory came from an older Homeric tradition,
whose heroes aspired to be “doers of deeds and makers of speeches,”21 and their
skepticism toward traditional Greek religion may have given important impetus
to science. The Sophists preferred natural explanations to supernatural, and
many commentators see the stamp of this rationalism even on the great historian
Thucydides. Nevertheless, the thrust of many of their doctrines was to reduce all
matters of right and wrong to a mere question of what was popular and what
wasn’t.
We don’t approve of things because they are good, the Sophists suggested;
rather, such things are good only because we approve of them. The Sophists
sometimes expressed this view by saying that all is merely “custom” (nomos). A
similar meaning was often attributed to the utterance of the Sophist Protagoras,
“Man is the measure of all things”; if Man approves it, it is good; if Man doesn’t
approve it, it is bad. In fact, the notion that all matters of goodness or beauty are
strictly in the eye of the beholder has a respectable philosophical pedigree;
Thomas Hobbes expounds this doctrine, and David Hume defends a highly
sophisticated version of it.22 Nevertheless, what distinguished many of the
Sophists was the sheer crudity of their appeal. They often aimed bluntly at

popularity alone, and their effect on rhetoric was to stress whatever techniques
proved persuasive, whether rational or not. The Greeks themselves called this
emphasis “making the worse into the stronger argument.”23
Plato describes this approach as applied to Athenian politics in his dialogue
the Republic, which is set sometime between 421 and 413 B.C., though composed
much later, after Socrates’ death. Plato writes,
Suppose a man were in charge of a large and powerful animal, and made a study of its moods and
wants; he would learn when to approach and handle it, when and why it was especially savage or
gentle, what the different noises it made meant, and what tone of voice to use to soothe or annoy it.
All this he might learn by long experience and familiarity, and then call it a science, and reduce it to a
system and set up to teach it. But he would not really know which of the creature’s tastes and desires
was admirable or shameful, good or bad, right or wrong; he would simply use the terms on the basis
of its reactions, calling what pleased it good, what annoyed it bad.24
Plato sees this animal as public opinion, and the man who teaches this science is
the Sophist.
The Athenian reaction to Sophistry, when it finally came, was good-humored
at first, then violent. The Sophists had descended more on Athens than on any
other city because its opulence had made it the most lucrative market. But in
423, the playwright Aristophanes ridiculed the Sophists’ teachings in his play the
Clouds, and in Aristophanes’ telling, the most influential of all the Sophists was
none other than the philosopher Socrates. Socrates, of course, denied this
accusation, but it also happened that the Delphic Oracle, believed by many
Greeks to be the voice of the god Apollo, declared that no one living was wiser
than Socrates.25 (Recall that the Greek word for “wise” was sophos.) Thus it was
easy for many ordinary Athenians to suppose that Socrates really was a Sophist.
Then, in 399, by a large jury’s majority vote (a jury that may have numbered
five hundred and one), Socrates was sentenced to death. The official charges
were for corrupting the young and believing in gods of his own invention, but
Socrates had insisted at his trial that the real complaint against him, though false,
was that he had taught others to “make the worse into the stronger argument.”26
Such, then, was Plato’s polemic, and the effect of this polemic—indeed, the
effect of the whole fifth-century experience—was to stress, emphasize, and
underline the distinction between merely persuasive arguments and truly rational
ones. This is the distinction the Sophists had blurred. The rational can seem
persuasive or unpersuasive, and the persuasive can be rational or irrational. The
great mistake of the Athenians, in Plato’s telling, was to confuse these different
things. They confused persuasiveness with rationality. This was why their
politics had proved disastrous. But Plato wasn’t alone in this judgment; it was

the judgment of his generation. Many of his readers agreed. Part of his legacy, in
brief, was to school the next generation of Athenian residents in the distinction
between rhetorical force and logical force.
THE SEPARATION OF LOGIC FROM RHETORIC
Once this distinction was drawn, it was only a matter of time before the earlier
Greek study of elenchos resolved itself into two distinct studies: logic and
rhetoric. Logic concerned rationality, but rhetoric concerned persuasion.
Aristotle’s analysis was only a short, further step because a large part of his
generation had already come to believe that rationality and persuasion were
different—a point Plato had made convincingly. Admittedly, neither discipline
(neither logic nor rhetoric) can be fully adequate without the other; Plato had
already demonstrated the weakness of an illogical rhetoric, but an unrhetorical
logic has drawbacks, too. The chief defect of an exclusive attachment to logic is
that it leaves us unprepared to act when we need to influence others. As Pericles
once remarked, “A man who has ideas but lacks the power to express them is no
better off than a man who has no ideas at all.”27 What necessarily concerns the
citizen, then, is the area where the two disciplines overlap.
Viewed against this political background, logic as a separate field of study is
actually a consequence of democracy—but democracy in crisis. The discovery
of logic was the work of a generation of Plato’s readers reacting in dismay to
democratic follies. The sheer spectacle of these follies had given the Athenians a
powerful new motive to study argumentation—not just to win debates or
establish creeds but (so they believed) to help them save their country. This new
motivation became part of the spirit of the age. Aristotle’s treatment of validity
still required a further insight: logical force depends on logical form. But the
ground for this insight had been already prepared by bitter experience. This is
the historical circumstance that made Aristotle’s ancient discovery intellectually
important to his time.
Of course, the type of explanation we offer now will always be at least partly
conjectural; we can’t go back and look into the souls of people long gone and
know with certainty why they found some subjects more interesting than others.
As for Aristotle’s own words, he says he investigates syllogisms to determine the
proper conditions of demonstration, and he says the effect of demonstration is to
acquire epistēmē, which is usually translated as “knowledge” or “science.”28 But
the thing to remember is that epistēmē was already an established Platonic

theme, and for Plato it was definitely a political theme.
In the ideal state of Plato’s Republic, the true philosopher-rulers have
knowledge (epistēmē), whereas most people—the people who dominate the
Assembly—deal only in “opinion” (or doxa). Plato hammers at these ideas
relentlessly, and even his emphasis on the study of mathematics is justified in the
same manner—as a subject fit to be contemplated by the political rulers of the
ideal state and as something to be carefully distinguished in the rulers’ education
from the mere vagaries of the Assembly.29
Aristotle would have been fully aware of the political import of these
doctrines, however much he might have differed on the details. He had been a
member of Plato’s Academy for two decades, where such attitudes would have
been common. Many of Aristotle’s contemporaries would have been equally
mindful of Plato’s constant stress on the supreme importance of distinguishing
genuine knowledge from passing opinion.
The result, then (in our view), was a chain of cause and effect: Greek
geography gave rise in the fifth century B.C. to an unusual political system—the
system of simple democracy—and a crisis in this system then generated an
intellectual reaction that ultimately turned logic into a distinct field of inquiry.
The foolishness of public opinion is, of course, the underlying theme behind
all these endeavors, and both Aristotle and Plato come back to that foolishness
repeatedly. In fact, both thinkers are profoundly antidemocratic.30 Yet it is
important to add that public foolishness, to the extent that it really exists, doesn’t
come from stupidity; instead, it comes from something different—and this is a
point the social elites of history have often got wrong. The problem isn’t that
democratic voters are unintelligent; the problem is that they are distracted.
Ordinary citizens have other jobs to do aside from politics, and when the day is
done, they often have less time for distinguishing political fact from political
fiction—or for analyzing arguments; hence the distracted manner of the
Athenian Assembly.
Winston Churchill famously remarked that democracy is the worst form of
government except for all the other forms.31 The chief defect of direct
democracy is that the voters tend to be inattentive, but the chief defect of rule by
an elite is that the elite becomes self-serving. Not only do the elite pursue their
own interests; they also become convinced that their narrow interests are really
the public interest. (As it happens, Adam Smith makes this very same complaint
against the elite that constitutes the business sector when he sums up its
strengths and weaknesses in The Wealth of Nations.)32 Thus most modern

political systems have tended to gravitate between these extremes, such that
public opinion is given some force but not unlimited force. (Public opinion may
also tend toward sensationalism, but a taste for the sensational often comes from
grinding labor. If you are tired at the end of the day, you often find it easier to
satisfy your appetite for drama by following some juicy scandal than by
investing serious mental energy in high art or public policy.) As a result,
ordinary citizens, when compared with those who have extended leisure, tend to
analyze politics more slowly, and this is why the application of logic to politics,
even today, is still partly the burden of schools and universities.
Whether democracies should give power to an intellectual elite is an old and
thorny question, and of course those who fancy themselves the elite are often the
biggest fools of all; nevertheless, the thing to remember in the Athenian case is
that their democracy was simple rather than representative. There were no
constitutional checks on public enthusiasm of the sort envisioned by James
Madison and the other architects of the U.S. Constitution in Philadelphia.
Instead, swings in public mood resulted instantly in new commands. Fleets were
diverted, generals were recalled, and cities were spared or destroyed depending
on the Athenian humor. In consequence, whenever a modern democracy finds
itself grappling with similar problems today, it repeats, to some degree, this
bleak, ancient experience.
In fact, in trying to understand these fundamental mechanisms and Aristotle’s
reaction to them, what we are really wrestling with is yet another instance of one
of history’s greatest enduring puzzles: the puzzle of reconciling the intellectual
equality of peoples with the singularity of the classical Greeks.
We suppose, rightly, that the different peoples of the world are intellectually
equal; our different races, populations, and social classes all descend from a
common stock of ancestors whose existence in evolutionary time is recent. The
rise and fall of civilizations is so brief in comparison with evolutionary change
among whole populations that no theory of genetically inherited traits is likely to
explain it. Genetic differences may arise among individuals, but genetic changes
across large groups are slow and slight. Yet, in logic as in other fields, the
classical Greeks loom large. How can this be?
The puzzle seems daunting, but it is easier to solve once we recall the special
effects of the classical Greek environment, especially the political effects. The
lay of the land and the smoothness of the water were singular in themselves, and
from these circumstances nearly all else flowed.
Aristotle’s insight into logical form lives on even as we type these words on a

digital computer—because a computer (as we shall see later) is really an
electronic embodiment of logical form. But before Aristotle could reach his
insight, the Athenians had paid for it in blood. Classical Greek wisdom really
came out of classical Greek folly. As their poet Aeschylus describes such costs
near the beginning of his Agamemnon, “He who learns must suffer. And, even in
our sleep, pain falls drop by drop upon the heart till at last, against our will,
comes wisdom, by the awful grace of God.”

3
ARISTOTLE’S SYSTEM
The Logic of Classification
THE ATHENIAN Assembly often disappointed the classical Greeks, but geometry
deeply impressed them. Euclid’s Elements was composed about a generation
after Aristotle, but geometry handbooks were already in circulation by the time
Aristotle taught. What most impressed the Greeks about geometry was its
certainty and finality. Unlike the vagaries of politics, which often tempt us with a
bad argument or force us to rely ultimately on an intelligent guess, the geometry
of the ancients seemed to offer conclusive proof. Once the definitions,
postulates, and axioms were laid out, the theorems appeared to follow as a
matter of logical necessity. It is easy to imagine how intelligent observers in
Aristotle’s day would have longed to achieve a similar certainty in all fields of
human endeavor and how some of them would have sought out the secret of
necessity in geometry’s serene and timeless diagrams.
This reverence for geometry appears repeatedly in the dialogues of Plato,1 but
a similar sense of certainty and timeless serenity is also present in Aristotle’s
logic. Aristotle’s examples of logical argument often come from geometry, but,
more important, his logical system has the effect of proving claims about whole
classes of objects—just as Euclid’s theorems make claims about all circles, all
triangles, or all bisected angles. The logical secret Aristotle sought from
geometry was the secret behind classification.
MANIPULATING CLASSES
To understand Aristotle’s approach better, let’s turn first to an example from a
very different time—a fanciful example, divorced from the blood and turmoil of
Athenian politics and coming instead from the author of Alice in Wonderland.
Today we know the author of the Alice stories as Lewis Carroll, but he was
actually Charles Dodgson, lecturer in mathematics and logic at Oxford
University from 1855 to 1881. Carroll concocted the following argument as an
exercise for his students:

rust every animal that belongs to me.
ogs gnaw bones.
admit no animals into my study unless they will beg when told to do so.
l the animals in the yard are mine.
admit every animal that I trust into my study.
he only animals that are really willing to beg when told to do so are dogs.
herefore, all the animals in the yard gnaw bones.2
Despite its circuitousness, the argument can be represented like this:
l As are Bs.
l Cs are Ds.
l Es are Fs.
l Gs are As.
l Bs are Es.
l Fs are Cs.
herefore, all Gs are Ds.
This equivalence in structure is, of course, not immediately apparent, but it
becomes easier to see with a series of substitutions.
As = animals that belong to me
Bs = animals I trust
Cs = dogs
Ds = animals that gnaw bones
Es = animals admitted to my study
Fs = animals that will beg when told to do so
Gs = animals in the yard
Thus the argument can be rewritten:
All animals that belong to me are animals I trust.
All dogs are animals that gnaw bones.
All animals admitted to my study are animals that will beg when told to do so.
All animals in the yard are animals that belong to me.
All animals I trust are animals admitted to my study.
All animals that will beg when told to do so are dogs.
Therefore, all animals in the yard are animals that gnaw bones.
(As it turns out, the argument is valid.)
These manipulations all rest on techniques pioneered by Aristotle, who
investigated arguments of this type by distinguishing what he regarded as the
four most basic “predications,” or what we might now call the four most basic

classifications.3 He focused on only those cases where one class or category
could be said to do the following:
holly include the other,
holly exclude the other,
least partly include the other, or
least partly exclude the other.
These are the four “categorical propositions”4 of Aristotelian logic, and
traditionally they are expressed as follows:
l As are Bs.
o As are Bs.
ome As are Bs.
ome As are not Bs.
(The word “some” here is construed to mean “at least one, perhaps all.”) These
four types of proposition can then be combined to generate logically valid forms
involving more than two classes, like this:
l As are Bs.
o Cs are Bs.
herefore, no As are Cs.
(For example, “All Spartans are dangerous, and no Thebans are dangerous;
therefore no Spartans are Thebans.”) Now quite apart from whether this kind of
analysis would actually have saved the ancient Athenians from the disasters of
their time, Aristotle became interested in the subject for its own sake. One effect
of those disasters had been to draw attention to the difference between valid
arguments and specious ones. Aristotle wanted to investigate valid reasoning as
such and to find ways of distinguishing the valid from the invalid—something
the Assembly had often failed to do. And his analysis had a further, crucial
consequence: long arguments with many premises (arguments like Lewis
Carroll’s) could be reduced to a series of short ones, all having just two premises
and a conclusion.
For example, Lewis Carroll’s argument can be reduced to a series of short
ones where all the premises in the series come from the original argument or can
be deduced from the original premises. Thus each new argument in the following
series builds on the preceding one:
l animals in the yard are animals that belong to me.
l animals that belong to me are animals I trust.
herefore, all animals in the yard are animals I trust.

l animals in the yard are animals I trust (deduced previously).
l animals I trust are animals admitted to my study.
herefore, all animals in the yard are animals admitted to my study.
l animals in the yard are animals admitted to my study (deduced previously).
l animals admitted to my study are animals that will beg when told to do so.
herefore, all animals in the yard are animals that will beg when told to do so.
l animals in the yard are animals that will beg when told to do so (deduced previously).
l animals that will beg when told do to so are dogs.
herefore, all animals in the yard are dogs.
l animals in the yard are dogs (deduced previously).
l dogs are animals that gnaw bones.
herefore, all animals in the yard are animals that gnaw bones.
This series never makes use of any premises that were not already contained in
the original argument or derivable from the original premises. And what the
series then does is connect the class of animals in the yard to each new class in
turn until reaching the final conclusion that all animals in the yard are animals
that gnaw bones.
As a result, Aristotle’s strategy was to determine which of these short forms of
argument were valid or invalid and then use this information to evaluate any
longer forms. The upshot, he hoped, would be to capture the logic behind all
forms of human knowledge.5 The short forms, which all have only two premises
and a conclusion and three key terms (e.g., A, B, and C) are now called
“categorical syllogisms.”6 Yet this was only part of Aristotle’s insight, which, to
repeat, is an insight into the logic of classifying.
As a biographical matter, Aristotle’s interest in classifying may have derived
from his father (whose methods as a doctor would have stressed the
classification of disorders), but he may also have come to it from the direct
observation that people commonly classify (or predicate) more than they realize.
Historically, as the classical Greeks were turning over more of their political
decision-making to vast public assemblies, they were also developing their keen
taste for geometrical theorems, which made claims about whole classes of
figures. The new power of the assemblies emphasized the importance of
argument, but for many Greeks, the results of geometry came to loom ever larger
as a tantalizing model of what rational argument might be. Aristotle’s logic, then,
represents the culmination of these tendencies.7 Beyond these points, however,
Aristotle’s analysis showed something more: the four categorical propositions

are also related to one another in definable ways, ways we implicitly invoke
every day. And in showing this, he raised deep questions about the peculiar
nature of logical necessity.
THE SQUARE OF OPPOSITION
The best way to grasp Aristotle’s further insight into these matters is to look at a
picture developed in Roman times that incorporates his observations. The picture
is called the square of opposition and is given in figure 3.1. Consider what the
picture does. Based on various remarks in Aristotle’s treatises,8 it tells us what
conclusions can be drawn from any of the four categorical propositions about
any of the others.
For example, the two propositions at the top of the square are “contraries,”
meaning they can’t both be true together but might both be false.
Thus, if the sentence “All As are Bs” is true, then the sentence “No As are Bs”
must be false; but if “All As are Bs” is false, then “No As are Bs” could be true
or false. The two sentences can’t be simultaneously true but could be
simultaneously false. (See figure 3.2.) The two sentences at the bottom of the
square (also represented in figure 3.3) have a different relationship. If “Some As
are Bs” is true, then “Some As are not Bs” could also be true; but if one of the
sentences is false, could the other also be false? If by “some” we mean “at least
one and maybe all,” then the two sentences can’t both be false.9 Instead, at least
one must be true. (That is, if it is false that “at least one A is a B,” then it is
necessarily true that “at least one A is not a B.”) As a result, the two sentences
aren’t contraries but “subcontraries”—they can’t both be false.
Now look at the first and last of the categorical propositions in the square as
shown in figure 3.4. In this new comparison, one of the sentences is true and the
other false; if “All As are Bs” is true, then “Some As are not Bs” must be false,
and vice versa. But notice that this is unlike the other relations we just looked at:
the two sentences are now neither contraries nor subcontraries; their logical
relationship is tighter because the truth or falsity of one fully determines the
truth or falsity of the other. In the language of logic, they are “contradictories.” A
little reflection will show that the remaining two sentences (“No As are Bs” and
“Some As are Bs”) are also contradictories. (See figure 3.5.)

FIGURE 3.1. The Square of Opposition (Definitions: “Contraries” can’t both be
true. “Subcontraries” can’t both be false. “Contradictories” are pairs of sentences
in which exactly one must be true and the other false.)
FIGURE 3.2. Contraries
(They can’t both be true but might both be false.)

FIGURE 3.3. Subcontraries
(They can’t both be false but might both be true.)
FIGURE 3.4. Contradictories (Exactly one is true and the other false.)
There are a few other structural features to see in the square. First, the two
sentences at the top of the square cover whole classes (“all” or “no”), for which
reason they are called “universals”; the two sentences at the bottom make only
partial claims about classes (“some”) and are called “particulars.” Also, the two
on the left assert some sort of class inclusion, by way of affirmation, and are
“affirmatives.” The two on the right assert class exclusion, by way of denial, and
are “negatives.” In effect, then, there is an underlying configuration, like what is
seen in figure 3.6.

FIGURE 3.5. Contradictories
(Exactly one is true and the other false.)
FIGURE 3.6.
Medieval logicians preferred to talk about all these points by using a
shorthand consisting of vowels, and so (according to tradition) they are thought
to have taken the two Latin words for affirmation and denial, affirmo and nego,
and extracted some of the vowels and called the universal affirmative the “A,”
the particular affirmative the “I,” the universal negative the “E,” and the
particular negative the “O.”

FIGURE 3.7.
FIGURE 3.8.
(This is the origin of several mnemonic codes worked out in the Middle Ages
for remembering which short forms—which categorical syllogisms—are valid or

invalid.)10 And the square conveys one more point: if either of the universals is
true, then the particular directly below it—its “subaltern”—must also be true. It
follows, however, that if the particular is false, then the universal directly above
must also be false.11 But here is the deeper question, the philosophical question,
the one really worth asking: why does the whole thing work?
THE UNDERLYING MYSTERY OF THE SQUARE
The most tempting answer, probably, is that the square of opposition works
because the relations it describes are just features of the way we use words, an
aspect of our “language-game,” to use a phrase of the Austrian philosopher
Ludwig Wittgenstein.12 The square works (it would seem) only because of the
way we happen to use our logical vocabulary; if we used our logical words
differently (we might suppose), then different things would be logical. But does
this answer actually make sense?
Consider: the import of the square has been expressed in many languages, and
the square was invented long before our language, English, ever existed. That
aside, however, how do languages give rise to logical necessities in the first
place? Instead, doesn’t a sense of logical necessity give rise to language? After
all, languages work only because they follow rules, but no one follows a rule
unless that person already senses what the rule logically implies. Logic makes
language possible—not the other way around. Thus, to invoke a “language-
game” theory to explain the existence of logical necessities (by saying that
logical implications exist only because they follow logically from a language or
from the structure of a language) is to argue in a circle. It puts the cart before the
horse. (In fact, many philosophers have offered circular explanations of this sort,
though without always realizing they were mixing up what depends on what.)13
But are there other ways to explain why the square works? Might one say, for
example, that the ins and outs of the square are simply the implications of
classification—the implications of inclusion or exclusion, partial or complete?
The trouble with this further view is that an “implication,” by definition, is still a
logical implication, and so the explanation is still circular. (We still haven’t
explained why the implications have their peculiar design.) Why do logical
necessities work out the way they do?
Alternatively, might we say that Aristotle’s insights—and perhaps all other
logical truths—are really just “true by definition”? In other words, couldn’t we
say that any logical relations we want to talk about happen to be the way they are

only because they follow from some collection of definitions, definitions
invented by logicians? Here, again, the explanation is circular. To say that one
thing “follows” from another only because it follows from a definition still
leaves unexplained why anything should follow to begin with. What we are
really asking, in other words, is, “Why do logical necessities have their peculiar
structure?” But let’s trying looking at this same question in another way—one
that will perhaps throw more light on the matter.
We live in a world that requires many arbitrary choices, and many of our
difficulties are problems of our own making. We make our bed, and we sleep in
it. But our choices lead to difficulties in the first place only because they have
consequences, and the relation between a choice and a consequence is never
“made.” Instead, it can only be discovered. This is because a consequence is a
matter of necessity, and necessities are at least partly determined by logic.
(Indeed, without logic, there would be no whimsy—our sense of whimsy comes
from contrasting the whimsical with the logical—but there would also be no
tragedy; tragedy is what happens when we discover the inexorable consequences
of some ill-considered choice.) Of course, in contemplating these matters,
whether gay or grim, we can often supply arguments and, sometimes,
compelling proofs for the existence of these consequences, consequences that
often turn out to be surprising. But as for why they should exist in the first place
and why they contain their secret surprises—these questions seem beyond us.
Here is yet one more observation on the peculiar question of why Aristotle’s
system happens to work, this time from the eighteenth-century Swiss
mathematician Leonhard Euler. Euler is a distinguished name in the history of
mathematics, and it takes no great leap to see that many of these remarks about
the strange quality of logical truths may well apply with equal force to
mathematical truths. (Mathematical relations are logically connected and
describable in detail, and yet they seem, in a deep sense, to be inexplicable.)14
Euler thought much about Aristotelian logic, and he offered a series of diagrams
to express the import of the four basic sentences of the system. (Euler’s was not
the first such analysis, but it is the most famous, and it was improved in the
nineteenth century by the French mathematician J. D. Gergonne, who expressed
it in five diagrams.15 This series of five gives us yet another way of looking at
the square by allowing us to view it through the lens of geometry.) Euler’s
approach (improved by Gergonne) works as follows. Given any two classes, As
and Bs, we can picture their Aristotelian relationship as being like two circles,
related in one of five ways (see figure 3.9).
In figure 3.9, the points in circles A and B represent the members of the

classes A and B.
Now, if we look back at the original four categorical propositions, we can see
that what the “A” proposition actually tells us is that either diagram (1) or
diagram (2) represents the Aristotelian relationship, though we aren’t told which.
In both (1) and (2) in figure 3.10, it is true that all As are Bs.
If we then look at the “I” proposition (figure 3.11), it says that (1), (2), (3), or
(4) represents the relationship, though again we aren’t told which. (Remember
that “some” means “at least one and perhaps all.”) In each diagram of figure
3.11, some As are Bs.
FIGURE 3.9.

FIGURE 3.10.

FIGURE 3.11.

FIGURE 3.12.

FIGURE 3.13.
Now look at the “O” (figure 3.12); what the “O” proposition says is that (3),
(4), or (5) represents the relationship (and again, we don’t know which). In each
diagram of figure 3.12, some As (at least some) are not Bs.
Finally, the “E” proposition (figure 3.13) says that only (5) represents the
relationship. In none of the other diagrams is it true that no As are Bs.
There is only one more step to the analysis: notice that if any one diagram
accurately represents the relationship, none of the others do. That is, all the
diagrams are mutually exclusive. We can picture the four basic propositions of
Aristotle in just this way, but if we now go back to the square of opposition from
late antiquity, what we see is that all the connections it asserts follow necessarily
from the equivalences pointed out by Euler (with help from Gergonne).
In figure 3.14, if 1 or 2 represents the relationship (the “A” proposition), then
5 (the “E” proposition) can’t. The two sets of diagrams are logical contraries.
The diagrams for the “A” and “O” propositions are logical contradictories; if 1
or 2 represents the relationship, then 3, 4, and 5 can’t, and vice versa. As for the
“I” and “O” propositions, they turn out to be subcontraries; at least one of the
diagrams must represent the relationship, and so the two propositions can’t both
be false but could still both be true. The “I” and “E” propositions turn out to be
logical contradictories; the diagram that correctly captures the relationship must

appear in exactly one of the two sets. In short, we can prove it all with
diagrams.16
The most natural response, perhaps, is to say that Euler has at last explained
the square of opposition because he has finally succeeded in defining the real
meaning of the four original linguistic expressions; he has finally explained why
it all hangs together. This, however, is a mistake. In fact, he has explained
nothing of the sort, except in the sense that he has found yet another way to
express the same underlying logical truths. After all, when we reason about
classes, we are not merely reasoning about the circles in Euler’s diagrams. On
the contrary, we are reasoning about shoes, ships, cabbages and kings, armchairs,
galaxies, protons, integers, soldiers at war, theater tickets—whole hosts of things
that look nothing like circles. The circles are merely representations once more,
representations for describing an abstract structure.
FIGURE 3.14.
The circles relate points on a plane to the members of classes, but this is only
another way of discovering the logical relations that govern all classes, whether
they are classes of points or classes of anything else. The circles help us discover
these relations, but to discover isn’t to explain why they exist in the first place.
Instead, we have only made their existence more conspicuous. It is as if Euler
had given us yet another way to grasp logical necessities so that, in addition to
hearing about them and thinking about them, we could also “see” them. But the
most baffling thing of all is that, literally, we have no physical sensations of
them whatever. We never see these abstract relations with the eye; we only grasp
them with the mind.

WITTGENSTEIN’S PROPOSED SOLUTION
The impulse to explain all this as only an artifact of language (such that logic
rests on language rather than the other way around) is very old.17 But the
problem with this approach is that it begs the question. No language is
intelligible in the first place unless it already conforms to something like logical
rules. As a result, then, to argue that we grasp such rules only because we
already know a language is to stand the correct mechanism on its head. Still, the
impulse is very old, as we say, and its long history is worth a moment to
consider.
During the last hundred years, the most prominent exponent of the linguistic
view has been Wittgenstein, who was raised in Austria but taught logic and
philosophy in England at Cambridge University in the 1930s and 1940s. He
advanced this view in particularly stark form. Wittgenstein not only asserted that
logical necessities depend on language rather than the other way around;18 he
went further: he held that virtually all philosophical conundrums, whether about
logic or anything else, are “meaningless” to the extent that if only you could
speak a language correctly, such problems would effectively go away. For
example, is there a god? Once you straighten out your language, what you
discover (according to Wittgenstein) is that the question is “meaningless.” What
makes an action right rather than wrong? Again, the question is “meaningless.”
Is there an immortal soul? “Meaningless.” On Wittgenstein’s view, then, proper
philosophy is mainly a matter of straightening out your language.
Of course, as soon as he said these things, he was constrained to explain just
what he meant by the term “meaningless,” but when he then embarked on this
further problem, he was immediately embroiled in puzzling out the various rules
that supposedly define meaning. And the whole problem of rules drove him back
once more into the dark thickets of logic.19
Wittgenstein arrived at his outlook in a peculiar way. Trained as an engineer
and heir to an industrial fortune in Vienna, he became interested in symbolic
logic as developed by the German logician and mathematician Gottlob Frege,
and at Frege’s suggestion, he studied the techniques of this new logic with
Bertrand Russell at Cambridge in 1912 and 1913. Not long afterward, however,
as the Great Powers squared off at the approach of the First World War,
Wittgenstein believed himself duty-bound to volunteer for the Austrian army,
and soon he was posted to the Italian front as an artillery officer. During the war
—amid its chaos and irrationality—Wittgenstein carried notebooks in his
rucksack, and in his free time he composed his brooding masterpiece, his

Tractatus Logico-Philosophicus. (He seems to have finished the manuscript in
1918; he already had drafts for the book when he was taken prisoner by the
Italian Army a year earlier, and the economist John Maynard Keynes then
managed to get the manuscript sent by diplomatic courier from Wittgenstein’s
prison camp in Italy to Russell in England with the aim of having it published.)
Part of Wittgenstein’s appeal to his contemporaries was that he seemed to be
offering solutions to the problems of his time; his emphasis on clarity and logical
rigor seemed exactly the cure for the fanaticism and absurdity of the Great War,
and this sentiment gained further strength with the rise of Hitler and the coming
of World War II. Partly through Wittgenstein’s influence, logic became the core
of so-called analytic philosophy through much of the twentieth century, and this
emphasis on logic in academic philosophy was further strengthened by German
émigrés fleeing Nazi persecution, who had found refuge in English and
American universities and who understood from personal experience the
importance of rationality and the social consequences of illogic.
Though some of Wittgenstein’s views later changed, he continued to work at
his linguistic approach—from several different angles—for the rest of his life.
Nevertheless, his basic claim about meaninglessness was mistaken, or so we
believe. And at the risk of sounding recklessly brash, we think it was mistaken
for a reason that is easy to see.
WITTGENSTEIN’S MISTAKE
Wittgenstein’s claim depended on confusing two very different sorts of
meaninglessness: ambiguity and unintelligibility. But once the two sorts are
distinguished, one can see fairly readily that his basic conjecture—most
philosophical questions are “meaningless”—can’t be right. (The conjecture is
actually much older than Wittgenstein; a version of it appears in the work of
Thomas Hobbes and David Hume, and it is, thus, one of the stock theses of
radical empiricism.)20
Here’s why the conjecture can’t be right: when we call a thing “meaningless,”
we sometimes mean it has no fixed meaning, which is to say that it might mean
any of two or more different things. Take, for example, the following
controversy: if a tree falls in a forest where no one can hear it, does it make a
sound?
Now, if by “sound” we mean the experience of hearing, then, by hypothesis,
where no one can hear, there is no sound. But if by “sound” we mean something

different—waves of pressure carried by the air—then a falling tree certainly does
make a sound (unless it somehow flourishes in a vacuum). This dual use of the
word “sound” is ambiguity, and it gives rise to all sorts of verbal disputes. This
is what we sometimes mean when we call an expression “meaningless”; we
mean that, in context, it has no single meaning—or, rather, too many meanings.
We encounter this difficulty, for instance, when someone praises an artwork on
the ground that the work is “distinctive”; the utterance could mean so many
different things that it is hopelessly ambiguous.
Wittgenstein’s claim, however, was different. Wittgenstein meant that
philosophical doctrines were unintelligible, in the same way that “Colorless
green ideas sleep furiously” is unintelligible, or the way “’Twas brillig, and the
slithy toves did gyre and gimble in the wabe” is unintelligible. (The first of these
stock examples comes from Noam Chomsky, the second from Lewis Carroll.)
Wittgenstein argued that the real problem is a tendency to use words that, in
context, have no meaning whatever and without the speaker’s realizing it. We
think we are saying something, but in fact we are saying nothing—not even
something ambiguous.21 Of course, we often say things that are unintelligible to
other people, but Wittgenstein meant that these same things are unintelligible
even to ourselves. And viewed harshly, this conjecture is highly improbable.
Why? Because it attributes to human beings something we just can’t do—or,
rather, something we can’t do except in rare cases of psychosis or perhaps in
strange dreams where no one else understands you, and you then wake up to find
that the words you were saying in the dream aren’t intelligible in the first place.
Consider: human beings can certainly speak unintelligibly, and they can also
fool other people into thinking they are being intelligible when they are not.
(Professional comics perform this last little trick when they speak doubletalk.)
But the one thing human beings can’t do, if sane, is fool themselves into this—
into thinking they are being intelligible when they are not. Try it. Craft a
sentence like, “It is time at last for the Empire State Building to blow its big
nose,” and then persuade yourself that you are speaking intelligibly. You can no
more do that than you can tickle yourself. You can tickle other people, but you
can’t tickle yourself. Similarly, you can speak gibberish to others and sometimes
fool them, but you can never use gibberish to fool yourself (at least if you are
sane and awake).
Of course, Wittgenstein asserted that this is precisely what happens when
people discuss philosophy: they fool themselves into thinking they are being
intelligible when they are not. (The great attraction of his doctrine is that, if true,
it would make the hard work of philosophical interpretation unnecessary; if the

language is ambiguous, then it might still express a sound philosophical
argument, and the only way to know that it doesn’t is to experiment with various
interpretations. But if the language is unintelligible from the start, this labor can
be avoided.) Yet if Wittgenstein were right in his conjecture, then the same
mistake would be empirically observable outside philosophy, as a matter of daily
life. We would find people who are otherwise sane sincerely asking themselves
whether the sky had brushed its teeth today, or whether the telephone had
finished baking another dozen biscuits, or whether the twenty-first century
would finally give its sister, the twentieth century, a big hug. The odd linguistic
error Wittgenstein had in mind would be empirically observable everywhere. But
it isn’t.
Instead, what we see in normal life is that the unintelligible is obviously
unintelligible, and for a simple reason: the whole essence of the phenomenon is
that our minds go blank. We just say, “It’s Greek to me.” By contrast, ambiguity
doesn’t make our minds go blank; rather, it makes our minds drift from one idea
to the next. These two phenomena seem similar, but they are different. And this
is why Wittgenstein’s conjecture looks plausible—because we confuse the two
situations. We often do speak ambiguously, and our minds often do wander from
one idea to the next, and so we do indeed fall into countless ordinary mistakes,
often without realizing it. But because the word “meaningless” can mean either
“ambiguous” or “unintelligible,” we get mixed up and suppose that these
mistakes must come not from ambiguous utterances, but from unintelligible
ones.
(We should add that this difference between the ambiguous and the
unintelligible has nothing to do with whether or not one construes the meaning
of an utterance to be a mental occurrence; some philosophers have held
meanings to be mental phenomena, whereas Wittgenstein, at least in his later
work, insisted that the meaning of an expression was its use. Either way,
however, ambiguous utterances have the effect of causing people to entertain
multiple ideas, whereas unintelligible utterances have a different effect: they
cause people to conjure up no ideas at all, except ideas about the sounds of the
words themselves or the sheer strangeness of their combination. The first
situation is treated by logic as a fallacy of ambiguity, but the second is an
encounter with gibberish. And the point we stress is that these two experiences
are empirically different, whatever theory of meaning one might prefer. In
ordinary speech, we also apply the words “meaningless,” “gibberish,” and
“nonsense” to implicit contradictions, and we sometimes use “meaningless” to
describe trivial observations that have been expressed in pretentious, elevated

diction. But none of these things are the same as the unintelligible.)22
To be sure, Wittgenstein was a shrewd philosopher of language, and,
accordingly, we don’t mean to suggest for a moment that he would have had the
slightest trouble in distinguishing ambiguity from unintelligibility in ordinary
contexts. Nevertheless, the distinction is far easier to overlook in philosophical
discussions where a speaker is unknowingly ambiguous to himself and yet flatly
unintelligible to the audience. And the key point is that this last situation is
different from speech that is flatly unintelligible even to the speaker.
Consider: in such cases, the speaker confuses several different things
unknowingly and uses the same words to describe these different things. (The
speaker is unknowingly ambiguous to himself.) Yet the audience is often baffled
and sees no way to construe the words intelligibly. (The speaker is, thus,
unintelligible to the audience.) And in this circumstance (which is actually quite
common), the audience can then easily suppose that the speaker must also be
unintelligible to himself rather than just ambiguous to himself. The audience
concludes that the speaker has no ideas, whereas, in fact, the speaker just has
confused ideas. (Wittgenstein’s misstep, then, in our view, consists in mistaking
a case of unknowing ambiguity to oneself for a case of unknowing
unintelligibility to oneself.)23
Such, then, is our complaint with Wittgenstein’s conjecture. But however this
may be, the larger claim we mean to defend is that logic can’t depend on
language in the first place because language depends on logic. Language
depends on conventions, and what makes a convention possible from the start
are logical relations (since, without these relations, none of our conventions
would have implications). Instead, the underlying nature of logic seems rather
like the sublime of nineteenth-century romanticism: timeless, placeless, eternal,
and with a foundation that is ultimately inexplicable. Or so we believe. When we
try to see logical truths literally with the eye, we see nothing at all. Instead, we
only see them—to use a phrase of Aristotle’s teacher Plato—with the eye of the
soul.24
Aristotle had little use for this sort of talk.25 It was all too ethereal. Plato had
used examples from the airy and peculiar world of mathematics, but Aristotle,
despite his use of mathematical examples in logic, was steeped in the empirical.
Nevertheless, in his own way, Aristotle reached a broadly similar result because
he, too, left the ultimate nature of logical principles unexplained. For Aristotle,
scientific argument always presupposes first principles, but first principles are

things whose existence can’t be explained. On the contrary, all explanation, by
way of argument, must presuppose first principles. And he treats at least some
principles of logic (specifically, the law of contradiction and the law of excluded
middle) in the same way; he treats them as things that all explanation must
presuppose.26 And so the ultimate reason for their existence can only be passed
over in silence—a thought echoed in the very last line of Wittgenstein’s famous
Tractatus: “Whereof one cannot speak, one must be silent.” That is how
Aristotle left the matter—in silence—even as the world he had carefully built for
himself at the Lyceum collapsed.
The end of Aristotle’s world came with the demise of his pupil and sometime
patron Alexander. Alexander had pushed his armies ever eastward, presiding
over the deaths of thousands in countless pitched battles and hoping all the while
to unite all the lands between himself and what he imagined to be the end of the
world—the edge of the great river Ocean. But finally, on the plain of the Indus
(in present-day Pakistan), he despaired. The losses had been terrible. The
enemies before him seemed innumerable. And so he turned back. He led a
bloody, terrifying retreat down the Indus and along the edge of the Indian Ocean
until he finally reached Persia and then the city of Babylon in 323 B.C. At last,
overcome by wounds or illness or poison (even now, the cause is disputed),
Alexander passed away.
The Greek city-states stirred once more at the news of his death. We are told
that Aristotle, brooding over his connection to Alexander and sensing danger,
concluded he must flee. He remembered the execution of Plato’s friend and
inspiration, Socrates, and he feared the Athenians might “sin twice against
philosophy.”27 So he left his school in the hands of his student Theophrastus in
the hope that it might survive and fled to Chalcis on the island of Euboea. There,
in the following year at the age of sixty-three, Aristotle died.

4
CHRYSIPPUS AND THE STOICS
A World of Interlocking Structures
WE LIVE our lives across space and time, but we can extend our logic across
space and time too. One way to see how we can do this is to think for a moment
about geometry and arithmetic.
When we look at a geometrical figure, we obviously think about space, but the
figure we are looking at, if printed on a page, is essentially timeless—it is static
and unchanging. When we carry out an operation in arithmetic, on the other
hand, we typically think of a “before” and an “after.” And our use of the
operation represents a change. Before the operation, we have, say, a pair of
numbers, like five and seven, but after the operation we have a sum: twelve.
Thus geometry seems connected in some deep way to our sense of space, but
arithmetic seems connected to our sense of time. (This point was made more
than two centuries ago by the German philosopher Immanuel Kant.)1
The same can be said for different branches of logic. Aristotle’s logic is
closely connected to geometry and to space; when we contemplate a question of
whether one class includes another, the classes we contemplate are static. The
classes don’t change as we think about them, and, in consequence, we can
represent them spatially, with geometrical figures. But there is another sort of
logic that considers the changing and dynamic—and concerns our choices in life.
When we think about choosing a course of conduct, we think across time to the
future, and the reasonableness of our choice then depends on our ability to
predict its future consequences. As a result, in addition to a logic of
classification as worked out by Aristotle (syllogistic logic), there is also a logic
of choice and consequence (propositional logic), which appeared about a century
after Aristotle within the ancient philosophical school of the Stoics.
The Stoics’ new approach to logic came from a new imperial world, where
sovereign kings had replaced the old democracies of the past and where
philosophical introspection had replaced political participation. As large, new
autocratic states spread across the region of the eastern Mediterranean, the
thinkers of the age tended to dwell less on public and political rights and more
on personal and private choices. They focused on the many ways we manage our

individual lives. In pursuing these themes, they also discovered a new set of
interlocking logical structures. The Stoics saw the whole universe as rationally
interconnected, and the structures they investigated turned out to be
interconnected, too.
THE STOICS
The Stoics first emerged as a distinct group about a generation after Aristotle,
and their chief concern was to find the good life for human beings. The Stoics
were primarily moralists. They believed the universe was governed by a law of
reason, emanating from God, and that the good life consisted in following this
law. In the Stoic view, this rational law was a “law of nature,” meaning it
controlled physical events but also applied to morality. People’s moral choices
were supposed to conform to it. We might try to evade the moral aspects of the
rational law and make excuses for our evasions, but in trying to excuse ourselves
we were still conscious, they believed, of the law’s presence. After all, if there
were no such law in the first place, why would anyone bother with excuses?
In addition, this law of nature could be demanding, and discovering its
implications required that we think logically. According to the Stoics, so long as
one strove to do right and maintained a positive attitude, one’s own life was a
life worth living. External fate was irrelevant. Pain, defeat, opprobrium, and
death made no difference. The Stoics insisted that we do right without excuses
and without fear of death. They often admitted that this outlook could be difficult
to achieve, and as a result, Stoicism was as much a discipline as a doctrine.
Nevertheless, it attracted many people. Like many ancient philosophical schools
after the fourth century B.C., the Stoics looked back on Socrates as a patron saint,
and they saw an embodiment of their ideal in his acceptance of external fate.
In fact, if we set logic aside for a moment and consider Stoicism as an
approach to ethics and morality, Stoicism has probably existed in nearly all
cultures and at all times (whether or not it has had a name), and one still sees
many expressions of it today. For example, the Stoic emphasis on fearless virtue,
combined with a patient acceptance of external fate, appears in the popular
“Serenity Prayer,” usually attributed to Reinhold Niebuhr: “God, grant me the
serenity to accept the things I cannot change, the courage to change the things I
can, and the wisdom to know the difference.”2 A similar emphasis appears in a
lecture by the American abolitionist Frederick Douglass, when he puts action
and attitude ahead of comforts and acquisitions: “He lives most who thinks the
most, feels the noblest, acts the best.”3

But Stoicism didn’t emerge in a vacuum—it became ever more popular
because of political changes sweeping the Greek world after the fourth century
B.C. After the conquests of Philip of Macedon and his son Alexander the Great,
political power shifted steadily away from democratic assemblies and fell into
the hands of kings. The subjugation of the old Greek democracies by these new
Macedonian warlords put an end to the classical age of Greece and ushered in
new styles in literature, the arts, and logic.
A key factor was a change in the nature of war. Philip and his successors
managed to outmaneuver and outclass the common citizen-soldiers of the old
city-states by confronting them with a new kind of army: paid professionals,
who used new weapons and tactics. This new army ultimately overthrew the
older political systems of the eastern Mediterranean, and the effect on the world
of ideas was to place new demands on artists, writers, and intellectuals.
When Philip was still a teenager, his own country of Macedonia had been
dominated by the city-state of Thebes to the south, and the Thebans held him as
a royal hostage. But his years as hostage also gave him a chance to study the
new military tactics of the Theban “Sacred Band,” a special infantry corps of
highly-trained youths. In war, the Sacred Band of Thebes, unlike other Greek
troops, formed an especially dense phalanx (a massed infantry formation) and
attacked at oblique angles. When Philip was later released and rose to power in
Macedonia, he copied these tactics and introduced other changes too. He armed
each of his infantrymen with a new, long pike in place of a conventional spear.
The result was to make his new Macedonian phalanx nearly unbeatable in frontal
attacks. And though the new pikes also made the Macedonian phalanx crucially
vulnerable to assault on its flanks (since the pike was hard to turn in a new
direction without colliding with one’s colleagues), Philip countered this
difficulty by covering the flanks with masses of conventional spearmen and by
developing more sophisticated mobile forces—cavalry, archers, and other
specialized units. The ultimate effect was to emphasize a greater variety of arms
and a level of specialized training that only professional soldiers could hope to
master. Philip hired large numbers of foreign mercenaries, drilled them
incessantly, and paid them with captured gold. (One of his early moves as king
was to seize the gold mines of his eastern neighbors in Thrace.) Philip then
exploited these innovations to overwhelm the old city-states of Greece. He
ensured the loyalty of his mercenary troops by forming many of his army’s most
elite units from Macedonian nobles, who had a stake in preserving royal
authority. The Greek cities tried to keep pace with him by hiring foreign
mercenaries of their own, but their democratic majorities generally resisted

anything like domination of their military apparatus by an elite corps of
hereditary aristocrats. Philip was essentially building an international force of
military professionals held in place by his local nobility. Meanwhile, the city-
states were still trying to preserve the sovereignty of large democratic majorities
whose soldiering could never be more than part-time.
This shift from citizen armies to professional ones undermined the power of
the old Greek democracies and tipped the balance toward new, royal
governments like those of Philip, his son Alexander, and Alexander’s successors.
The new military-political system Philip and his son were forging eventually
came to dominate the eastern Mediterranean, and with it came a new kind of
logic.
After the death of Alexander in 323 B.C., his empire broke into pieces as his
former generals carved out large, new dominions for themselves; these
expansive new monarchies then persisted for centuries. One of Alexander’s
generals, a former friend and bodyguard, quickly seized Egypt and became
Ptolemy I, whose Macedonian descendants ruled in the style of pharaohs down
to the death of Cleopatra in 30 B.C. Another general, the former commander of
Alexander’s footguards, became Seleucus I, whose family grabbed much of the
old Persian Empire. A third element of Alexander’s domains broke off to
become the kingdom of Macedonia, which changed hands many times but
nevertheless dominated the old Greek cities.
In arts, letters, and logic, the impact of this new political order would shift
attention away from public and political undertakings because so few people
could have political undertakings in the first place. Instead, the new focus was
on the personal and private. In sculpture, there was greater emphasis on the
emotional, the extreme, and the idiosyncratic, culminating in the artistic style we
now call “Hellenistic.”4 In earlier times, the city-states had commissioned
idealized representations of gods, heroes, and even ordinary citizens to express
the dignity of the state, and the serene countenances of these figures constituted
the “severe style” of the fifth century B.C. But in the new order of things, the
chief patrons of the arts were now wealthy individuals who valued novelty and
expressiveness, and the sculpture they favored celebrated the domestic, the
intimate, the agonizing, the comic, the tragic, and the grotesque.
There were also changes in drama; playwrights could no longer risk ridiculing
the leading political figures of the day in the manner of the ancient Athenian
Aristophanes, and they rarely discussed supreme political power in the style of
Sophocles. Rather, theater focused increasingly on the domestic problems of
ordinary people, and a new comedy of manners emerged that revolved around

the intrigues of yearning lovers, churlish slaves, ignorant masters, and long-lost
relatives. (This “new comedy,” Latinized for Roman audiences by Terence and
Plautus, supplied Shakespeare with the plot for his Comedy of Errors and
became the historical basis of modern romantic comedy.) On the whole, the
general tendency was away from public and political themes and toward
introspective ones, and Stoicism mirrored this trend.
As political power became more autocratic, philosophical writers dwelled less
on political problems, more on spiritual ones. Personal duty rather than political
duty became the primary focus. When a ruler was Stoic, so that personal and
political duties coincided, the emphasis of Stoicism was never on changing the
political system but rather on administering it justly. The Roman emperor
Marcus Aurelius, whose Meditations from the second century A.D. has come
down to us, inherited Stoicism in this form, and the Macedonian ruler Antigonus
II studied it in much the same way in the third century B.C. (In answer to the
suggestion that he was a god, Antigonus replied, “The man who carries my
chamber pot knows better.” And he chastened his son, who had apparently
mistreated someone else, with the words, “Do you not understand, boy, that our
kingship is a noble servitude?”)5
In logic, the effect of these tendencies was a change in emphasis—from the
old logic of static classification in the manner of Aristotle to a new logic that
examined choice and consequence. Ethics, for the Stoics, was always a matter of
choosing well, and the Stoics always insisted that argumentation shapes the
morality of our choices. Indeed, for the Stoics, ethics was impossible without
logic.6 Of course, one might easily suppose that the logic of choice discovered
by the Stoics could have been explored just as well in any historical period;
nevertheless, the real impetus for their efforts, which required much energy and
concentration, came from their stress on personal morality—the same stress that
continually filled their school with students. In Stoicism’s early years, both
teachers and students alike were convinced that a life of personal morality
required attention to logic. They also believed that reason ought to rule the
passions just as reason ruled the cosmos. So far as the spirit of the time was
concerned, this focus on the personal, in contrast to the political, was part and
parcel of an autocratic age.
Paradoxically, the shift from citizen assemblies to sovereign kings reinforced
the idea of the equality of peoples. Both citizens and noncitizens were now
equally powerless in the political arena; only kings and their appointees could
exercise real authority. Thus the Stoics were far less prone to view some social
groups as born to rule and others as born to obey. Instead, the citizens who now

concerned them were the “citizens of the world” (or, as the Stoics put it, the
“cosmopolites”), and these citizens consisted of all men and women of goodwill,
whatever their nationality.
This insistence on equality, combined with an emphasis on introspection,
focused the attention of these thinkers on what was introspectively common to
all peoples—citizens or noncitizens—and on what all people might discover if
only they reflected quietly on their own mental habits. Writers of the period
became more interested in a common human nature under the “rational law,” and
they believed that part of this nature was to participate in an eternal realm of
rational, moral choice. The key that unlocked the door to this realm was logic.
THE LOGIC OF CHOICE
Consider for a moment how our choices appear to us as a matter of logical form.
For example, we often ask ourselves whether we ought to do one of two different
things, A or B. If we do A, then we might cause C; yet, if we do B, we might
cause D. And perhaps we can’t do both A and B. As a result, whereas for
Aristotle the key logical expressions were quantifying words like “all” and
“some,” which we apply to classes, the key logical expressions for the Stoics
were different: they were the connectives “either/or,” “if-then,” and “not both.”
The result was a new group of logical relations. In fact, the Stoic connectives
had always been common in ordinary reasoning; they just hadn’t been
systematically investigated. We see these connectives at work, for example, in
the old Socratic dialogues of Plato.
The Socrates of Plato’s dialogues often asks how we ought to live, and his
further analysis then turns out, often, to be an assessment of consequences. (To
avoid a damaging consequence, his reasoning suggests, we must sometimes
choose a different way of life.) The key notions of choice and consequence are
thus central to Socrates’ reasoning, and they even appear in the form of the
dialogues.
Socrates typically asks some fellow Athenian if he believes a thing, and when
the Athenian says he does, Socrates often shows that the belief in question leads
logically to something false or absurd—from which he then infers that the belief
in question must also be false. When a belief is refuted by showing that it leads
to a falsehood, the rhetorical technique is often called reductio ad absurdum.7
But the underlying connective is “if-then.” The classical Greeks had a long
tradition of arguing in this manner, beginning with the view of one’s opponent

and deriving an absurd consequence—thereby showing that the opponent’s view
must be false.8 The technique had probably long flourished in Greek law courts,
where witnesses could be cross-examined, and it also played an early role in
Greek mathematics.9
The Stoics seem to have relied on the technique a great deal, and their great
contribution was to investigate it as a matter of logical form. Of course, any
argument can be conceived as an if-then proposition if we think of it as a relation
between its premises and its conclusion (if the premises are true, then the
conclusion should also be true). But the if-then investigated by the Stoics was
typically more specific. The Stoics were concerned, above all, with the moral
demands of rational, eternal law and in examining the consequences of our
actions by the light of this law. As a result, their arguments not only had
consequences but were about consequences. The emphasis on choice and
consequence led them to investigate a new set of logical forms.
The forms they explored (or, as some logicians would say, the “schemata”
they explored)10 all depended on “either/or,” “if-then,” or “not both.” But the
Stoics also made a further, crucial discovery: all the new forms were in some
way interconnected. Interconnection was one of their grand themes, since,
according to their view, the whole world was rationally interconnected. The law
of nature ruled a connected universe, and logic revealed the implications of this
law to its citizens. In consequence, when the new logical forms turned out to be
connected among themselves, the Stoics pursued them with great zeal.
Stoicism first appeared as a philosophical school through the teachings of
Zeno of Citium, a Cypriot merchant who had come to Athens in the late 300s
B.C. and had taken up philosophy. When Zeno first offered public discourses, he
was too poor to hire a lecture hall, and so he began lecturing in a public arcade
or stoa (from which we get the word “Stoic”).11 At his death in or around 265,
the philosophical movement that had grown up around him passed to a former
professional boxer, Cleanthes (originally from Assos in Asia Minor), and then to
Cleanthes’ student Chrysippus (originally from Soli, also in Asia Minor). And it
was under Chrysippus (pronounced CRY-sip-us) that Stoic logic became fully
developed.
Chrysippus, leader of the Stoics at Athens from 232 onward, was the one to
arrive at the key insight that all the new forms were logically interconnected.
The simplest of Chrysippus’s forms are highly intuitive, and you will probably
recognize many of their features as already occurring in your thinking. But
Chrysippus sought to connect and elaborate these forms so as to develop what
logicians now call “propositional logic” (a term whose meaning we shall explain

later on), and it also turns out that this new, propositional logic runs our digital
computers. (Without the sort of logic pioneered by Chrysippus, modern
computer processing would be impossible.) For a start, then, let’s look at six of
these new forms and then consider how they work in a modern computer. The
names we give them now are the ones traditionally assigned by logicians.
1. Modus Ponens (“The Method of Affirming”)
The form is as follows:12
If A, then B.
A.
Therefore, B.
Here is an example:
If the Great Oz has spoken, then you must come back tomorrow.
The Great Oz has spoken.
Therefore, you must come back tomorrow.
2. Modus Tollens ( “The Method of Denying”)
The form is as follows:
If A, then B.
But not B.
Therefore, not A.
Here is an example:
If hearts will ever be practical, then they must first be made unbreakable.
But hearts will never be made unbreakable.
Therefore, they will never be practical.
(Notice that the word “not” can be replaced with other negative expressions,
such as “never,” “no,” “nobody,” or “no one.” Here is another example: “If you
are to get help, then you must certainly see the Wizard; but no one sees the
Wizard—not nobody, not no how; therefore, you are not to get help.”)
3. The Hypothetical Syllogism
The form is as follows:
If A, then B.
If B, then C.

Therefore, if A, then C.
Here is an example:
If I were king of the forest, then I’d rule with a royal growl.
If I ruled with a royal growl, then chipmunks would genuflect to me.
Therefore, if I were king of the forest, then chipmunks would genuflect to me.
(The Cowardly Lion’s thinking)
4. The Conjunctive Syllogism
The form, in two versions, is as follows:
Not both A and B.
Not both A and B.
A.
B.
Therefore, not B.
Therefore, not A.
Here is an example:
You can’t both have your cake and eat it too.
You have your cake.
So you can’t eat it too.13
5. The Dilemma
The form, in two versions, is as follows:
Simple dilemma
Complex dilemma
If A, then B.
If A, then B.
If C, then B.
If C, then D.
But A or C.
But A or C.
Therefore, B.
Therefore, B or D.
Here is an example:
If you say what is just, then men will hate you; and if you say what is unjust, then the gods will hate
you. But you must say one or the other. Therefore, you will be hated.
(An ancient priestess discussing her child’s prospects for a political career)14
6. The Disjunctive Syllogism
The form, in two versions, is as follows:

A or B.
A or B.
But not A.
But not B.
Therefore, B.
Therefore, A.
Here is an example:
Lincoln tells you the States will be either all free or all slave. He is against making them all slave. It
follows that he is for making them all free.
(Stephen A. Douglas)15
Now all these forms are interconnected in a special way: they can be
interpreted so that none are valid unless all are. This is so because they all
contain compound propositions. But to understand their interconnections—as
vital in a modern computer as they were to the ancient Chrysippus—we first
need to consider just what a “compound proposition” is.
THE NATURE OF COMPOUND PROPOSITIONS
Consider, for example, the last form on the list: the disjunctive syllogism. This
form is called a syllogism because it has two premises working jointly, but it is
disjunctive because the first line has the form,
A or B.
A and B are both simple propositions, but when joined by a connective like “or,”
the resulting whole is a compound.
Logic traditionally uses three types of compounds. The first type, using the
connective “or,” is called a disjunction, and the elements joined are its disjuncts.
(Thus A and B mentioned in the previous paragraph are disjuncts.) Here is
another disjunction, again with two disjuncts:
Either you do or you don’t.
(In ordinary speech, the connective “or” is sometimes ambiguous since it can be
used inclusively to mean “A or B or both” or exclusively to mean “A or B, but
not both.” Chrysippus preferred the exclusive use, but in their most rigorous
work today logicians use the connective inclusively. If a modern logician then
wants to convey a sense of exclusion, the logician uses the inclusive form but
adds an additional, qualifying formulation: “A or B, but not both A and B.”) The
basic idea of disjunction is choice.
The second traditional compound is the conjunction; a conjunction joins
simple statements with the connective “and,” and its individual elements are

conjuncts. The following conjunction consists of three conjuncts:
A and B and C.
Julius Caesar’s famous description of his campaign in Asia Minor is a
conjunction of three conjuncts, with the ands omitted:
I came, I saw, I conquered.
The idea of conjunction is the idea that there can be combinations; anyone who
sees that more than one thing can be true at the same time has embraced a
conjunction.
The third type of compound is the hypothetical or conditional (or, in the
language of classical symbolic logic, “material implication”). The hypothetical
uses the connective “if-then,” though, in ordinary English, the word “then” is
often omitted. In The Call of the Wild by Jack London, John Thornton expresses
his sentiments toward the brutal prospector Hal in the form of a hypothetical:
If you hit that dog again, I’ll kill you.
The idea behind the hypothetical is that many things have consequences.
Now notice that the hypothetical, like the disjunction, commits the speaker to
neither element of the compound but only to the whole. Thus John Thornton
hasn’t promised to kill Hal, nor has he said that Hal will hit the dog again.
Instead, he only says that if Hall hits the dog again, then he’ll kill Hal. Hal
replies with a disjunction:
Get out of my way, or I’ll fix you.
Again, Hal doesn’t promise to fix Thornton, he merely promises to do so if
Thornton fails to get out of the way. These examples illustrate the logical
equivalence between hypotheticals and disjunctions. Hal’s disjunction can be
rewritten as a hypothetical:
If you don’t get out of my way, then I’ll fix you.
And Thornton’s hypothetical can be rewritten as a disjunction:
Don’t hit that dog again, or I’ll kill you.
In each case, the two formulations are equivalent in that the paired sentences are
necessarily both true or both false—they are true or false together.
Schematically, the compound
A or B
can be construed as logically equivalent to the compound

If not A, then B,
or again, to the compound
If not B, then A.
And the compound
If A, then B
can be construed as logically equivalent to the compound
Either not A or B.16
But we can do more with this.
Once we see the equivalence of these compounds, we can see the deep,
fundamental connection between the disjunctive syllogism and a world of related
logical necessities, the world first explored systematically by Chrysippus.
In essence, what Chrysippus discovered was that many of our most common
methods of argument are logically tied; suitably interpreted, each is valid only if
the others are. But let’s look at Chrysippus’s insight in greater detail.
INTERLOCKING FORMS OF ARGUMENT
The explorations of Chrysippus and his followers took place about a century
after Aristotle, but their consequences were never fully exploited from a
mathematical perspective until the nineteenth century. (More precisely,
Chrysippus saw a great many logical connections among the simple forms we
have already noted, but the specific equivalence of these forms was mostly
developed in modern times—and only when logicians interpreted conditionals in
a way that made them systematically convertible into disjunctions.)17 Still,
Chrysippus plainly realized that he was once more entering a world of connected
implications and symmetries—a world of related logical necessities.
For example, if we look carefully at modus ponens and the disjunctive
syllogism, we can see intuitively that both are logically valid. We sense their
validity instinctively by contemplating only a few examples. But it also turns out
that, with the right interpretation, the validity of each of these forms turns on the
validity of the other. And this must be so, because the premises of one can be
interpreted as logically equivalent to the premises of the other. We can see this

point if we write out the two forms schematically.
Here is modus ponens:
If A, then B.
A.
Therefore, B.
But remember that its first line, “If A, then B,” can be construed as logically
equivalent to “Either not A or B” (just as “If you hit that dog again, I’ll kill you”
is logically equivalent to “Don’t hit that dog again, or I’ll kill you”). In
consequence, we can pair modus ponens with a new form that nevertheless has
an initial premise that is logically equivalent. On the left is modus ponens, and
on the right is the new form:
Modus ponens
New form
If A, then B.
Either not A or B.
A.
A.
Therefore, B.
Therefore, B.
In this comparison, one can see that neither of these forms is valid unless the
other is (since all lines on the right are either identical or logically equivalent to
lines on the left), but the new form is actually a disjunctive syllogism.
Of course, it doesn’t quite look like a disjunctive syllogism—not exactly. But
this is because it is disguised, so to speak, and we can lift the disguise if we write
it out with a few modifications. In the forthcoming comparison, therefore, our
initial version of the new form has been placed on the left, and a modified
version is on the right:
Initial version
Modified version
Either not A or B.
Either [not A] or B.
A.
Not [not A].
Therefore, B.
Therefore, B.
Notice what we’ve done. In the modified version on the right, in the second line,
we’ve replaced “A” with “Not [not A].” This is the principle of double negation,
and we shall need to come back to it in a moment. We have also put brackets
around all the instances of “not A” so as to highlight the hidden structure. If we
then compare this modified version on the right with a standard disjunctive
syllogism in one of its traditional incarnations, we can see the sameness of form.
For clarity, then, let’s put the modified version on the left and a traditional

version of the disjunctive syllogism on the right:
Modified version
Traditional version
Either [not A] or B.
Either A or B.
Not [not A].
Not A.
Therefore, B.
Therefore, B.
This is the same thing twice. The only difference between these last two schemas
is that the modified version has “not A” wherever the traditional version has
“A.” But we can still see the same basic pattern; the logical form of the argument
doesn’t depend on which simple propositions it contains but only on whether
these propositions appear in the same configuration. And here the configurations
are the same. This is part of Chrysippus’s insight.
But what about our use of double negation? We must still ask whether using
double negation here is logical because the manipulations just carried out depend
on it. It is logical only if “Not [not A]” really is logically equivalent to “A.”
(That is, the manipulations are logical only if “Today is Tuesday” really is
equivalent to “It is not the case that today is not Tuesday.” But how do we know
this?)18
THE LAWS OF CONTRADICTION AND EXCLUDED
MIDDLE
To establish this further point, we shall need two more assumptions crucial to
logical analysis. The first new assumption will guarantee a viable sense of
negation, and the second will make it the sort of negation we can double. The
first assumption is that none of these propositions of argument can be true and
false at the same time. This is the law of contradiction. The second assumption is
that each of these propositions must be at least true or false. This is the principle
traditionally called the law of excluded middle.19 The special role of these laws in
all logic, no matter how complicated, is something we shall come back to in the
next chapter, but if you do indeed grant the two assumptions—the two laws—
then the principle of double negation follows.
Here’s how the principle can be derived (it’s a bit tricky):
The law of contradiction tells us that “A” can’t be both true and false at the
same time. Neither can “not A.” But if by “not A” we mean that “A” is indeed
false, then the truth of “not A” always implies that “A” can’t be true, since “A”
can’t be true and false simultaneously. Therefore, by the law of contradiction,

“A” and “not A” can’t both be true at the same time; it is one or the other but not
both. Neither, however, can they both be false. Why not? Because if “A” is
indeed false, then this will already fulfill the condition we just laid down, by
definition, for “not A” to be true.
We can go further, all the way to the principle of double negation, so long as
we can also invoke the law of excluded middle.
The law of excluded middle tells us that each of these propositions, “A” and
“not A,” must be at least true or false. Thus, given that they can’t both be true
and can’t both be false, each must be false whenever the other is true. So “A”
and “not A” are logical contradictories; exactly one is true, and exactly one is
false.
To reach the principle of double negation, we need one more step. We need to
run through once more the same reasoning we just sketched, but this time we
must apply it to a different pair of propositions: not to the pair “A” and “not A,”
but to the pair “not A” and “not not A.” That is, if we substitute the expression
“not A” for “A” in the reasoning we just sketched and also substitute “not not A”
for “not A,” then what we discover is that the same relation must hold between
“not not A” and “not A”—they will be logical contradictories. Exactly one will
be true, and exactly one will be false; and this will imply that “not not A” is
indeed logically equivalent to “A” because each of these last two propositions is
a logical contradictory of “not A.” Thus the last two propositions, “A” and its
double negation, will each be true whenever the other is.
These manipulations are delicate, to be sure, because they are all
manipulations of form, and none of them would have been possible without
Aristotle’s insight that logical force turns on logical form. What we really see
here, though, is something more basic. We see just how tightly the primary
structures of logic can be tied together. So long as each proposition of the
analysis is true or false but not both at the same time, all the main structures
studied by Chrysippus can be interpreted so that each is valid only if the others
are. They hang together and must hang together. We need negation, to be sure,
but once we have it, we can express the logical equivalent of conjunction in
terms of negation and disjunction, and we can express the logical equivalent of
disjunction in terms of negation and conjunction. We can also express the logical
equivalents of both conjunction and disjunction in terms of negation and the
hypothetical.20
Just to drive this point home, here is another of these curious equivalences:
Not both A and B

is logically equivalent to
[Not A] or [Not B].
(For example, “Today is not both Tuesday and Thursday” is equivalent to
“Today is not Tuesday, or today is not Thursday.”) This is one of De Morgan’s
laws (after the nineteenth-century English logician Augustus De Morgan), and it
follows from our earlier assumptions: If A and B can’t both be true, then at least
one of them must be false. This implies that either “not A” is true or “not B” is
true. Once we make this move, however, we can then convert the conjunctive
syllogism into a disjunctive one, like this:
Conjunctive syllogism
Disjunctive syllogism
Not both A and B.
[Not A] or [not B].
A.
Not [not A].
Therefore, not B.
Therefore, [not B].
Again, it is the patterns that matter, and the pattern on the right is the same as
that of a standard disjunctive syllogism. Thus the conjunctive syllogism and the
disjunctive syllogism are logically equivalent.
We can even throw modus tollens into the mix. In the following comparison,
you will see that each premise on the left is equivalent to the one on the right:
Conjunctive syllogism
Modus tollens
Not both A and B.
If A, then [not B].
B.
Not [not B].
Therefore, not A.
Therefore, not A.
(Remember that the first line of the conjunctive syllogism can be converted into
a disjunction via De Morgan’s law, and that this disjunction can then be
construed as equivalent to a hypothetical, just as it was for Hal and Jim
Thornton. Thus “Not both A and B” becomes “Not A or not B,” which then
becomes “If A, then not B.”) Of course, the new modus tollens on the right looks
complicated, but it corresponds to the traditional one in terms of form. Compare
the new modus tollens to the traditional one, and you will see the same hidden
structure:
Traditional modus tollens
New modus tollens
If A, then B.
If A, then [not B].
Not B.
Not [not B].

Therefore, not A.
Therefore, not A.
This is just part of the peculiar world of interlocking compounds Chrysippus
entered into, and it suited his Stoic philosophy perfectly. To his mind, what he
was exploring was the universe’s rational order, and to further the investigation,
he and his followers picked out a few logical forms to stand as axioms and then
systematically inferred others.21
MORE INTERLOCKING FORMS
There are still two more among the six basic forms we started with earlier in this
chapter: the dilemma and the hypothetical syllogism.
Simple dilemma
Hypothetical syllogism
If A, then B.
If A, then B.
If C, then B.
If B, then C.
But A or C.
Therefore, if A, then C.
Therefore, B.
We can pull these last two forms into our interlocking system too, if we first
point out that the dilemma is actually two instances of modus ponens glued
together. If we take the simple dilemma in its standard form,
A, then B.
C, then B.
ut A or C.
herefore, B.,
then it is possible to rewrite it as a contemplation of two different situations,
each subjected to modus ponens:
If A, then B.
If C, then B.
A.
or
C.
Therefore, B.
Therefore, B.
The first two lines of the dilemma have been relegated to left-and right-hand
columns, and the alternatives of the original dilemma’s third line come next,
separated by the word “or.” Then we draw an inference about the situation on the
left and the situation on the right. But since either situation might prevail—we
don’t know which—the strategy of the dilemma is to use the connective “or” to
glue the situations together. Put another way, the situation on the left is what you

get if you land on one side of the fence, and the situation on the right is what you
get if you land on the other. (The complex dilemma is then a small variation on
this theme.)22
As for the hypothetical syllogism, we can pull it into the system by pointing
out that it is equivalent to a dilemma, though a rather unusual one, this time built
from a combination of modus ponens and modus tollens. The premises of the
hypothetical syllogism are in this form:
A, then B.
B, then C.
But notice what happens if B is true. If B is true, then we can infer C by modus
ponens:
odus ponens
B, then C.
herefore, C.
On the other hand, suppose B isn’t true; in that case, we can infer “not A” by
modus tollens:
odus tollens
A, then B.
ot B.
herefore, not A.
But the original premises don’t tell us which of these two situations obtains; we
don’t know whether B is true or untrue. All the same, since we know that at least
one of them must obtain (because, by the law of excluded middle and the
definition of negation, either B or not B must be true),23 it follows that we can
combine all the information that we do know by treating this analysis as a
contemplation of two different situations. And this is just the way we built our
first dilemma:
Modus ponens
Modus tollens
If B, then C.
If A, then B.
B.
or
Not B.
Therefore, C.
Therefore, not A.
If we now combine this information into a single argument, we get a peculiar
dilemma:

A, then B.
B, then C.
or not B.
herefore, not A or C.
This is actually a hypothetical syllogism—which just happens to be wearing,
once more, a sort of disguise—and we can lift the disguise in two steps: first, by
replacing the conclusion with the logically equivalent formulation, “Therefore, if
A, then C.” (Remember that “Not A or C” can be construed as equivalent to “If
A, then C.”)
A, then B.
B, then C.
or not B.
herefore, if A, then C.
Then, to arrive at standard form, we need merely omit the third line of the
argument on the grounds that it goes without saying; after all, the proposition “B
or not B” is a logical truth following directly from the law of excluded middle
and the meaning of “not.” It thus remains true even if unuttered. In consequence,
we have this:
ypothetical syllogism
A, then B.
B, then C.
herefore, if A, then C.
Until the design of digital computers, any interest in a system like this was
largely theoretical; nobody needed all the manipulations of Chrysippus to handle
the ordinary situations of life. But the discovery of the system still showed the
existence of these unexpected symmetries, rather like those of the old square of
opposition. And, once again, the discovery raised the deeper question of why it
all works.
We might say it works because of the way we have defined our key logical
operations: disjunction, conjunction, the hypothetical, and negation. As long as
we define these things with an eye to the law of contradiction and the law of
excluded middle so that everything must be true or false but not both
simultaneously, all the relations hold good; it all fits. But our sense of logical
necessity, of course, still lies behind all of it; we still need a sense of what
follows from what before we can put such a system together.
Specifically, whatever definitions we choose to lay down, we still need to

draw valid conclusions from the definitions, and this means we must still assume
in advance that some inferences are valid. We can invent logical expressions and
give definitions to them, but we can only discover their resulting interrelations.
As a result, we can explore and describe the interlocking world of Chrysippus,
but we can never fully justify or explain it without taking some of it for
granted.24 Such, then, was the abstract world that the ancient Chrysippus
discovered, but how does all this play out in a modern computer?
THE BASIS OF COMPUTER LOGIC
It all depends on the compounds we looked at a moment ago: the disjunction, the
conjunction, and the hypothetical. The first thing to notice about these
compounds is how different they are from the logic developed by Aristotle.
Aristotle’s logic is largely devoid of compounds. Aristotle was surely aware of
compounds, but he didn’t pick them out for extensive study. In effect, then, what
we really have here (in the hands of Aristotle and Chrysippus) are two different
kinds of logic. The two kinds are by no means incompatible; they complement
each other well, but they concern different things. We see this if we look once
more at modus ponens. In the case of modus ponens,
A, then B.
herefore, B.,
it doesn’t matter which propositions we substitute for the variables A and B as
long as we substitute them consistently. The argument is still valid as a matter of
form. But the thing to notice is that the variables do indeed stand for whole
propositions.
With Aristotelian syllogisms, on the other hand, the situation is different. To
be sure, a syllogism’s validity is still the same, whatever you substitute for its
variables:
l As are Bs.
l Bs are Cs.
herefore, all As are Cs.
But this time, the variables don’t stand for whole propositions. Instead, they
stand for classes. The variables are terms within propositions (a term being the
name for a class). Thus, whereas modus ponens is valid merely in virtue of how
the propositions fit together (hence, the expression “propositional logic”),

Aristotle’s logic relies on a more complicated “logic of terms.” (The validity of a
syllogism turns on things inside each proposition, on the terms themselves. By
analogy, we might say that Chrysippus’s logic combines atoms into whole
molecules and then traces out the implications, but Aristotle’s logic goes about
dissecting each atom in turn and then relates the parts of these atoms to the parts
of other atoms.) It is only Chrysippus’s logic, propositional logic, that translates
easily into computer circuitry.
The reason for this last point is a crucial feature of the compounds we have
been examining. The feature is this: with the compounds, the truth or falsity of
the parts determines the truth or falsity of the whole. Take, for example, the
disjunction. A disjunction is true or false depending entirely on the truth or
falsity of its elements. Specifically, a disjunction is true if at least one of its
disjuncts is true; otherwise, it is false. (In the language of logic, the disjunction is
thus “truth-functional.”) But notice also that, in saying this, we are dealing with
only two values, true and false, and the simplicity of this choice will make the
transition to electronic circuitry comparatively easy. If the only possibilities for a
proposition are true or false, then we can represent this choice quite
mechanically.
Now before we consider how this is done, we need to dispel a common
confusion. The confusion is that, in confining ourselves to only two values—true
and false—we are somehow asserting that everything in life must be either true
or false. On the contrary, we assert no such thing, and we don’t deny for a
moment that much of life has nothing to do with being true or false. Still less do
we offer any theory of what truth and falsity, in themselves, really are. The
analysis of truth and falsity is an interesting problem, one we shall come back to
in the next chapter, but the thing to see at present is that our minds do indeed
manipulate relationships of this type, relationships where parts determine
wholes. And for the moment, it is the manipulations that matter. So long as the
manipulations are possible—and plainly they are—it will be a straightforward
matter to build a machine that mimics this behavior.
In particular, the two values of true and false can be represented electronically
as a circuit switched on or off.25 Such circuits can then be combined in complex
arrays so that, if various elements within the array are switched on, their
compounds are switched on, and the compounds can then represent the logical
relations of disjunction, conjunction, and the implications represented by
hypotheticals. This is the basis of modern digital computing. As the electronic
elements switch on and off, the machine’s behavior mimics an exceedingly
complex inference in which sets of conclusions lead to other conclusions.

With Aristotle’s logic, this effect would be impossible. Why? Because the
parts of a categorical syllogism represented by the variables A, B, and C aren’t
“true” or “false” in the first place. Instead, A, B, and C are merely names of
classes (classes like “dogs,” “mortals,” “Athenians,” or “Thebans”); they are not
statements. But since the variables merely stand for classes, we no longer have
the same intimate connection between the truth of the variables and the truth of
the whole.
What we see as a result is a close similarity between some of our methods of
reasoning and the behavior of digital computers. And though this fact is
sometimes masked by a computer scientist’s tendency to speak in terms of a
binary code, a series of ones and zeros, the scientist’s code is simply another way
of expressing the same logical truths. The code happens to come from the
nineteenth-century English logician and mathematician George Boole, whose
aim was to codify the relations studied much earlier by Chrysippus (albeit with
greater abstraction and sophistication). Later generations built on Boole’s
insights, and we shall consider in chapter 9 how these efforts eventually gave
rise to programmable computers. But the logic that made it all possible was the
interconnected logic of an interconnected universe, discovered by the ancient
Chrysippus, who labored long ago under an old Athenian stoa.
We don’t know how Chrysippus met his end. One story is that he died from
drinking wine; another says he died from a fit of violent laughter. Such odd tales
were common in the ancient world. (The first of the Stoics, Zeno of Citium, was
supposed to have ended his life at the age of ninety-eight by holding his breath.
All these stories come down to us from the ancient biographer Diogenes
Laertius.)26 All we know of Chrysippus for sure is that his teachings and logic
were much admired.
Living in a time of vast, autocratic regimes, Chrysippus dwelled, like all
Stoics, on the rational choices of individuals, and he believed human reason at
its best was a form of participation in the eternal. Assessing truth and falsity and
cleaving to the laws of contradiction and excluded middle, he sought to make the
life of human beings as orderly as the cosmos itself. For Chrysippus, the eternal
law of nature was the secret of the starry heavens above, and it was the moral
law within. According to an old Athenian saying, “If the gods have any use for
logic, it is the logic of Chrysippus.”27 However that may be, the logic of
Chrysippus is certainly the logic of our machines.

5
LOGIC VERSUS ANTI-LOGIC
The Laws of Contradiction and Excluded Middle
WE LIVE in an age that is prone to relativism, but so did the classical Greeks.
Truth and falsity became matters of deep perplexity to them as they began to
encounter other cultures through trade. As commerce accelerated and linked ever
larger parts of the Mediterranean world together, the more sophisticated of the
Greeks soon realized that what one society took to be true, another often took to
be false. Many began to wonder, who can know for sure what is true—and what
does “truth” even mean? As a result, alternative theories of truth began to
proliferate in the 400s B.C., and a deep sense of relativism soon spread among
intellectuals. Many Greeks recalled the aphorism of the fifth-century poet Pindar,
“Nomos is king,” which is often rendered as “Law is king” but which can also be
translated as “Custom is king.” Many came to think that truth was only custom,
or perhaps it was nothing at all. No less than other human beings, Greek thinkers
loved to generalize, and many passed easily from the commonplace that some
things in life are relative to the more daring and controversial thesis that all
things must be relative. Long before Pontius Pilate uttered the words, the
question “What is truth?” had become a popular riddle.
It was against this sweeping expansion of relativistic views during classical
times that Aristotle and the Stoics often did battle and tried to carve out niches of
objective truth in logic, science, and ethics. (In ethics, Aristotle thought some
actions were always wrong, no matter what, but he thought there were other
actions where a true assessment would need to be imprecise. He characterized
ethics as “true for the most part.”)1 The ideas of truth and falsity became
philosophically interesting in the classical age—but also matters of much
confusion.
In logic, the key battles often centered on the law of contradiction (that no
proposition of argument can be true and false at the same time) and the law of
excluded middle (that every such proposition must be at least true or false).
Problems of truth had led many thinkers of the period to dwell on paradoxes and
to conclude that perhaps there was really no secure truth at all except the
changing sea of human opinions. Others stuck to the view that truth must be

something more than opinion, however much our opinions may vary. But behind
all these tendencies was a growing sense throughout the Greek world of
profound and disturbing uncertainty.
PARADOXES OF TRUTH
Many Greeks had become burdened by a deepening feeling of drift. Socrates
expressed some of this outlook when he said at his trial that what he really knew
for sure was that he didn’t know.2 Earlier, the Sophists had stressed the
mutability of custom, and the Greek historian Herodotus had amused audiences
by contrasting the many differing notions of different peoples. (For example,
Herodotus asserted that, in contrast to the Greeks, the ancient Persians often
deliberated while drunk but then reconsidered their decisions when sober; but if
they decided while sober, they always made a special point of reconsidering the
matter while drunk.)3 Many societies have since tried to make sense of people’s
differing customs—brought into collision by trade—and one consequence has
been to reinforce an inclination to caution, circumspection, and tolerance for
opposing views. But another effect, in many historical periods, has been to
propel some of the more ambitious thinkers of an age into deep, metaphysical
speculations about knowledge and reality, sometimes to the point of absurdity.
For example, some Greek thinkers came to the view that to call a thing true is
only to say that one happens to believe it: “It’s true because I think it so.” Others
insisted that something is true only because the majority believes it or because
those in power believe it. Still others held that everything is both true and false at
the same time. These doctrines notwithstanding, some maintained that nothing is
true or false; instead, everything falls into an intermediate middle ground: there
are no truths or falsehoods in the first place, only opinions. An extreme version
of this outlook was attributed to the fifth-century thinker Cratylus, who
concluded that nothing could be truly asserted of a world in constant flux and
who finally decided to stop speaking; instead, when questioned by his disciples,
he only moved his finger to indicate that he had heard the question but found it
impossible to reply.4
And many ancient Greeks clung to these speculations even though many other
ideas seem to depend crucially on the difference between the true and the false.
For instance, what is lying if nothing is true? What does it mean to misstate or
misread or misrepresent? What is inaccuracy? In addition, whatever answers we
give to these questions, what happens if we ask whether our answers are
themselves true? Can it be “true” to say that nothing is true? Perhaps we could

escape such conundrums by falling back on the theory that nothing is absolutely
true; yet how do we then explain the difference between the absolute and
nonabsolute if we can’t even say outright that something is true? Maybe we
could avoid difficulties of this kind by confining ourselves merely to
approximating the truth, but how do we then explain the thing we are
approximating? Do we approximate a thing even though there is no such thing in
the first place?
Aristotle’s predecessors had already encountered many of these problems,
some of which came out in one of Plato’s classic dialogues, the Theaetetus. It
includes a discussion of the doctrine of the Sophist Protagoras, whose treatise
Truth had asserted, “Man is the measure of all things.” The exact meaning of
Protagoras’s utterance has long been disputed, but Plato took Protagoras’s words
to mean that anything is true so long as the majority believes it—a doctrine Plato
challenged. The doctrine seemed to make the majority infallible except when the
majority ended up denying the doctrine itself (in which case it became self-
contradictory).5
In sum, many Greeks of the classical age had sought an analysis of truth that
was forbearing and cautious and wound up instead with something metaphysical
and ponderous. New schools of philosophers then emerged to propound these
unusual ideas. For example, the ancient Skeptics, who flourished shortly after
Aristotle and adopted an outlook much like Cratylus’s, insisted that all things in
life were unknowable.6 Another group, the Cynics (meaning “doglike”), lived in
hovels and begged for food and insisted that the only real truth involved
renouncing human convention. (The most famous Cynic was Diogenes of
Sinope, a man of pungent wit who lived in a large storage jar or “tub.” He
mocked at the conceits of the world, and when approached by Alexander the
Great, who stood over him to inquire whether he needed anything, replied, “Yes,
stop blocking the sunlight.” Plato is said to have remarked of Diogenes, “That
man is Socrates—gone mad!”)7
In capturing the real nature of truth, perhaps the simplest approach would
have been to say that the truth is often hard to discern, but many thinkers of the
ancient world chose a different path: deny all truth, all knowledge, and all
certainty. And they did so, even though the practical problem, usually, was to
determine which things were true, which were known, and which were more
certain or less so. (A doctrine of truth that assigns the same status to all
propositions indiscriminately is no help in distinguishing any of them in
particular; it tells us nothing about how some ideas differ from others.)
Aristotle viewed many of these old doctrines of truth with hostility; he

regarded them as half-examined crudities and thought they could only be
believed by people who were incapable of following out the logical implications
of their own ideas. His impatience sometimes spilled out on the page. In his
Metaphysics, remarking sarcastically on the doctrine that everything is both true
and false at the same time, Aristotle argues that no man who thinks he ought to
do something then proceeds not to do it on the grounds that he also ought not to
do it: “Why does a man walk to Megara and not stay at home when he thinks he
ought to be walking there? Why does he not walk early some morning into a
well or over a precipice? Why do we observe him guarding against this—
evidently because he does not think falling in a well is alike good and not
good.”8
Despite Aristotle’s impatience, many of the old doctrines of truth have
continued down to modern times, and many arguments still adduced on their
behalf are actually ancient Greek arguments. For example, here is the nineteenth-
century Marxist philosopher Friedrich Engels arguing that all propositions are
both true and false at the same time, and doing so by invoking a version of an
ancient Greek paradox of the fourth century B.C. called the “bald man”:
“Whether an animal is alive or not . . . is, in many cases, a very complex
question. . . . It is just as impossible to determine absolutely the moment of
death, for physiology proves that death is not an instantaneous, momentary
phenomenon, but a very protracted process.” From this, Engels concludes that
dying animals are both alive and not alive, and that everything “is and is not.”9
In the ancient version of the paradox, we are asked to determine not the moment
of death but the moment at which a hairy man, who loses his hairs one by one,
becomes a bald man. Since it is difficult to draw any precise line between
baldness and hairiness, it seems that hairy men are bald and not bald.10
As it happens, the truth and falsity of all propositions became a staple of
orthodox Marxism because Marx and Engels had both inherited this creed from
the German philosopher G. W. F. Hegel, who was popular when they were
students and who drew much of his inspiration from the many paradoxes of early
Greek philosophy. Here is Engels again, but this time making a different
argument for the truth and falsity of all things: “Every organized being is every
moment the same and not the same; every moment it assimilates matter supplied
from without, and gets rid of other matter; every moment some cells of its body
die and others build themselves anew; in a longer or shorter time the matter of its
body is completely renewed, and is replaced by other molecules of matter, so
that every organized being is always itself, and yet something other than itself.”11
This time the argument is a version of another Greek paradox: the ship of

Theseus. According to the ancient story, the Athenians preserved the ship of
their famous early king Theseus, who had united Attica, fought the Amazons,
penetrated the Labyrinth, killed the Minotaur, and endured many other
adventures. But as the ship gradually decayed, the Athenians replaced its various
parts one by one. Eventually, all the ship’s parts had worn out and been replaced.
So the question arose, was it still the ship of Theseus? The answer, according to
some of the ancients, was that it was the ship, and yet it wasn’t the ship.12
Many arguments of this sort—everything is both true and false—turn on
ambiguous phrasing. In the last passage from Engels, for example, the phrase
“the same” is ambiguous. An object can be “the same” in some respects (still
alive) yet different in other respects (e.g., assimilating a particular molecule at
one moment and expelling it the next). This is different from showing that an
object is simultaneously “the same and not the same” in the same respect, which
is the conclusion that Engels wants to draw. If the object is the same in respect to
being alive but different in respect to assimilating a particular molecule, then we
could still have two propositions here, one of which might be true and the other
of which might be false. Engels wants to show that there is just one proposition,
which is true and false simultaneously. (In the case of the ship of Theseus, the
ship can be “the same” in terms of form but different in terms of matter.)
Other such arguments turn on vagueness.13 In Engels’s earlier argument, he
contends that it is impossible to determine absolutely the moment of death; the
point of death is sometimes vague. Considered carefully, however, all that this
observation actually shows is that the moment of such a death is, as Engels says,
indeterminable. It is quite another thing to say that the same being is both dead
and not dead. By analogy, if we can’t determine whether our employer plans to
fire us, then it hardly follows that he both plans to fire us and does not plan to
fire us. Instead, all that follows is that we can’t yet determine his plans. There
are many such arguments about truth, and many turn out, on examination, to
depend on a hidden non sequitur.
Here is yet one more of these old doctrines of truth, this time a recycled
version of the thesis of Thrasymachus, an ancient Greek Sophist who was
depicted in Plato’s Republic. Thrasymachus maintained that justice is nothing
but the “interest of the stronger party.”14 He seems to have meant that justice is
whatever happens to serve those who hold political power. In the ancient version
of the doctrine, power determines justice, but in the modern version, power
determines truth—or so says the French social thinker Michel Foucault.
Foucault’s phrasing is sometimes convoluted, but his dictum still circulates
broadly through academia and still represents an echo of the ancient

Thrasymachus. The basic idea isn’t hard to get. Foucault says those in power
create a “discourse,” and, by controlling and dominating this discourse, they also
create truth: “In any society, there are manifold relations of power which
permeate, characterise and constitute the social body, and these relations of
power cannot themselves be established, consolidated nor implemented without
the production, accumulation, circulation and functioning of a discourse. There
can be no possible exercise of power without a certain economy of discourses of
truth which operates through and on the basis of this association. We are
subjected to the production of truth through power and we cannot exercise power
except through the production of truth.” He continues:
Each society has its regime of truth, its “general politics” of truth: that is, the types of discourse
which it accepts and makes function as true. . . . “Truth” is centered on the form of scientific
discourse and the institutions which produce it . . . it is produced and transmitted under the control,
dominant if not exclusive, of a few great political and economic apparatuses (university, army,
writing, media). . . . “Truth” is linked in a circular relation with systems of power which produce and
sustain it, and to effects of power which it induces and which extend it.15
The doctrine, though wordy, turns partly on an ambiguity. If by a discourse we
mean a vocabulary, or perhaps rules of usage, then it makes little sense to ask
whether such things are “true.” (Asking whether a vocabulary is true is like
asking whether the English and French languages are true; the question is
nonsense.) On the other hand, if by a discourse we mean a series of assertions
that use the vocabulary—so that no one can assert as true anything that those in
power deny—then Foucault’s doctrine would seem to defy common sense.
Consider: if we use the words of the discourse to deny the assertions of the
controlling and dominating people who teach them to us, then either our
statements are true or theirs must be true. This much seems to follow from the
law of excluded middle (if we are willing to take the law for granted) and from
the fact that we “deny.” On the other hand, suppose we assume nothing here
except that we can never even deny the assertions of those dominating people
because they supply us with our words. In that case, we might just as well argue
that small children will never deny any assertions of their parents on the ground
that the parents who control and dominate them have taught them their words.
After all, in the first years of life, children are subjected to the most intense form
of domination and control (and the most intense “discourse”) in all human
experience; yet most parents learn eventually that their children will indeed deny
what they say.
Speaking truth to power is surely hard; people with power sometimes use
threats and sophistries to suppress the truth. But Foucault’s argument, taken

literally, would really show something different: speaking truth to power is
impossible anyway since there can be no truth without power. The doctrine
implies that those in power can never have true reasons for thinking themselves
mistaken, since they control the discourse. The doctrine belies the common
experience of hearing oneself contradicted (something that happens even to the
most powerful and arrogant), but it is also remarkably defeatist; it implies that
the powerful can never be contradicted. (Alternatively, many thinkers have tried
to avoid such objections by redefining the word “true,” such that a proposition
becomes “true within a discourse” if it is logically consistent with—or coheres
with—all other propositions in that discourse, or with the “rules of the
discourse.” But the trouble with all these tactics is that they succeed in
redefining truth only at the price of making the definition circular. To call two
things “consistent” is to say that both can be true at the same time. Thus, far
from banishing some earlier notion of truth, these tactics seem to presuppose
one.)
The task of the early Greek logicians was to cut through obscurities and
confusions of this kind and to arrive at the ideas of truth and falsity that regulate
reasoning in the ordinary sense of the word. Remember that logic is the study of
arguments, and an argument, to a logician, is an attempt at proof. In that case,
however, no matter how uncertain our various opinions may be at any one time
(or in any one historical period), argumentation will be useful only under
particular conditions. Logicians in many ages have described these conditions,
sometimes in varying ways, but the account we give now is not much different
from accounts given by others, whether by Aristotle or by those who came after.
We shall specify three such conditions (under the headings of “truth as
independent of opinion,” “the law of contradiction,” and “the law of excluded
middle”):
1. Truth as Independent of Opinion
First, no proposition we argue about can be true just because we believe it.
Instead, its truth must turn on something more than the mere fact of our
conviction; otherwise, the argument is unnecessary. If a proposition is true
simply because we believe it, then why reason about it in the first place? The
belief becomes necessarily true, and in that case we can dispense with the hard
work of assembling evidence and distinguishing the valid from the invalid. (To
say this isn’t to deny that some propositions are true simply because we believe
them. For example, the proposition “I am thinking” becomes true if one believes

it—one’s thinking it makes it so.16 But matters of argument need to fall outside
this category. The first assumption of argument, then, is that not everything is
true simply because we believe it.)
Notice that similar observations apply to the idea that a proposition is true
simply because society believes it, or because a majority believes it, or because
those in power believe it. In that case, society, the majority, or the collection of
powerful personages becomes infallible in the sense that their belief in the
proposition is necessarily true. Criticism assumes, on the contrary, that those
with whom we argue might be mistaken and the attempt to persuade them by
reasoning assumes that the person to be persuaded doesn’t yet realize something
true.
The experience of the ancient Greeks showed that there could be many
theories of truth, and in the centuries that have followed, truth has been variously
described as objective, subjective, intersubjective, relative, pragmatic,
performative, eliminable, dependent on correspondence with reality, dependent
on coherence with other accepted beliefs, dependent on coercion and power, and
so on. But another legacy of the Greeks, derived from their logicians, is that such
theories are largely irrelevant to logic. Historically, theories of truth come and
go, but the results of logic have remained. Though there have been many
theories of truth, the validity of modus ponens—at least in ordinary cases—has
always been obvious. (As for the extraordinary cases, sometimes debated by
professional logicians, we shall be talking about these in a short while.)17 As
long as the expressions “true” and “not true” are intelligible and as long as the
proposition to be proved doesn’t hinge on the mere belief of the person to be
persuaded, one’s preferred theory of truth makes little difference. (On the other
hand, if the expressions “true” and “not true” are not intelligible, then no theory
of truth will be helpful anyway, since no one will know what the theory is
about.)
An analogy to arithmetic might also be useful. In arithmetic, we add and
subtract numbers, but what is a number? Is it a physical object? Is it only an idea
in our heads? Is it an abstract, mathematical entity—among infinitely many other
such entities—that exists even if no one thinks about it? (Mathematicians say
there are infinitely many numbers even between one and two, but who has
infinitely many ideas in his head?) Explaining what a number is turns out to be
philosophically difficult. Nevertheless, no one would say that we can’t do
arithmetic unless we can also answer these metaphysical questions about
numbers. Just so, to distinguish valid arguments from invalid ones we must still
ask whether the truth of a conclusion follows necessarily from its premises. But

in ordinary circumstances this no more requires a general theory of truth than
adding five and seven requires a general theory of numbers. (All the world’s
languages have expressions for true and false, and in logical matters there will
still be some clearly right answers and some clearly wrong ones.)
2. The Law of Contradiction
Arguments, to be useful, must also meet a second condition: No proposition of
argument can be both true and false at the same time (“false” meaning “not
true”).For many centuries, this assumption has been called the law of
contradiction.18
The justification of the law is practical. If a proposition is both true and false,
then it refutes no errors. But in that case, why is the proposition worth arguing
about—or even worth knowing? It disproves nothing. (To refute an error is to
show that it is not true, but the trouble with contradictions is that they allow what
is false to be true nonetheless.)
In most systems of formal logic, a contradiction (meaning a statement in the
form “A and not A”) is false since, if true, it would imply that the same
proposition was true and false simultaneously.19 All the same, there have long
been objections to the law of contradiction, and most come from construing the
law as something more than it is: as a theory of physical reality, of how to live
one’s life, or of how to order one’s mind. Often, objections to the law derive
from the perfectly reasonable complaint that logic is sometimes oversold; not
everything in life is a matter of logic.
Let’s take a moment to elaborate this complaint: Some things in life can only
be felt to be appreciated, and some things are better left unanalyzed and
spontaneous. If we then insist on reducing everything to premises and
conclusions and consistent descriptions, sometimes we simply fail to grasp it. In
fact, maybe the most important things in life have nothing to do with reason and
logic. Themes such as these are common in nineteenth-century romantic
literature; Walt Whitman writes, “Do I contradict myself? Very well then I
contradict myself (I am large, I contain multitudes).” The Tao Tê Ching often
uses similar language,20 and even so eminent a logician as John Stuart Mill
warns against the “dissolving influence of analysis.”21 Emerson insists, “A
foolish consistency is the hobgoblin of little minds.” In some matters, if we
demand logic and nothing but, we merely become too clever by half. In other
words, uttering contradictions and defending them is actually a perfectly

coherent way of saying that the subject isn’t really a matter of logic at all.
Nevertheless, the key point here (sometimes overlooked) is that logic’s approach
to contradictory utterances extends not to all their linguistic uses but only to their
use as expressions of an argument’s elements—its premises or conclusion.
Strictly speaking, the traditional law of contradiction, construed as a principle
of logic, applies only to the parts of an argument or inference. It tells us what
must be so to the extent that the world can be known by inferring some
propositions from other propositions. (Logic assumes that the world cannot
behave in logically impossible ways, but if the world does so behave, then the
behavior is simply beyond logical comprehension.)22 Nevertheless, this is not a
theory of how to live one’s life. Nothing in the principle, taken literally, indicates
whether we would be better off if all our beliefs were rigidly consistent. If a
person maintains contradictory beliefs, then logic tells us that at least one of
those beliefs must be false. (This much follows from the law of excluded middle
on the assumption that the beliefs in question can be propositions in the first
place.)23 But nothing else follows whatever. Does this person know more or less
than other people? Would this person’s life be better if his beliefs were revised?
Should this person’s judgment be generally trusted or generally discounted? On
all these further questions, logic as a discipline is strictly silent. We don’t deny
that such further questions have answers, but any complaint about the answers
isn’t really a complaint against logic but against something that has been
mistaken for logic. The complaint is against a further theory of the mind or of
the good life—or of social organization.
In practice, contradictions are often used in ordinary language to assert that a
subject isn’t really a matter of argument but a matter of feeling or intuition. This
is the use often made by romantic writers and perhaps also by the Tao Tê Ching.
Contradictions can also be used to express the absurd or the hopeless. All the
same, the tendency to overstate logic’s province and to imagine the law of
contradiction as a grand principle independent of argumentation has deep roots.
As a matter of history, many of our current attitudes toward logic and
rationality come down to us from the eighteenth-century European
Enlightenment, when many writers insisted on reason as an instrument of social
reform. Many of them seemed to assume that Reason (capitalized) was the only
thing that mattered. In this respect, the romantic movement that followed may
have been a valuable corrective. In fact, no principle of logic, properly
understood, contradicts any of the classic romantic themes. Nevertheless, this
natural tension between romanticism and the Enlightenment has been translated
more recently into an imagined tension between East and West, between

“Eastern logic” and “Western logic,” and this further supposed tension is
fictitious.
So far as history goes, there has never been any particular Eastern or Western
view of contradiction, nor any sort of Eastern logic incompatible with a Western
one—nor any “male logic” as opposed to a “female logic.” Of course, the word
“logic” can be used loosely to indicate a mere set of opinions or customs, or
perhaps a collection of mannerisms and tendencies; but if by “logic” we mean
the practice of the most common methods of rational inference, then the theory
that there are Eastern and Western logics, or male and female ones, is fantasy.
There are no more Eastern and Western logics than there are Eastern and
Western arithmetics. Consider this simple argument that embodies modus
ponens:
If you don’t eat tomorrow, you’ll be seriously weakened.
You won’t eat tomorrow.
Therefore, you’ll be seriously weakened.
In the sad saga of human experience, has the validity of this inference ever been
less apparent to peoples of the East than of the West, or to women than to men?
Of course, Aristotle explored a great many other forms when he examined
syllogisms, so perhaps one could say that syllogistic reasoning is the special
mark of the Western mind. But this, too, seems to be mistaken. In practice,
people in the West have made no more use of syllogistic forms than people in the
East; in fact, except for the Barbara syllogism24 and a few others, nobody really
uses syllogisms at all—except logicians. Might it still make sense to say that
Aristotle’s theory constitutes a distinct “Western logic” on the grounds that it
was first developed by a man we now call Western? (If so, we should call the
differential calculus “Anglo-Germanic mathematics” on the grounds that it was
first developed by an Englishman and a German: Isaac Newton and Gottfried
Leibniz.) But permit us to complain just a bit more, this time on behalf of classic
literature of the East.
The Tao Tê Ching stresses themes of spontaneity, mystery, nonaggression, and
acceptance, and it does this by contradictory utterances. We might ask, don’t
these literary qualities still count, in some sense, as a distinct Eastern logic? The
trouble with this view is that the same themes appear in much Western literature,
and the defense of contradictions has a long Western pedigree (it goes back to
Aristotle’s predecessors).25 In consequence, if a love of contradiction were the
special mark of a distinct “logic,” it would make more sense to call the defense
of contradictions a divergent “Western logic.” In addition, this East/West

approach steadfastly ignores the great diversity among Eastern classics
themselves, many of which turn out to be rigidly consistent and analytical.
(Among these consistent and analytical texts are Kautilya’s Arthasastra from
India, perhaps of the fourth or third century B.C., which might well be mistaken
for a work by Machiavelli, the writings of the third-century Legalist Han Fei Tzu
of China, and Sun Tzu’s Art of War. The Analects of Confucius don’t contain
any obvious examples of intended contradiction either. Ancient Chinese thinkers
were often acutely sensitive to the accusation that they had contradicted
themselves.)26
In sum, this attempt to distinguish a monolithic Eastern view, different from
an equally monolithic Western one, belies the literary diversity of both regions.
The attempt ignores what the classics of both regions really look like.
3. The Law of Excluded Middle
Useful argumentation must also meet a third condition: Every proposition of
argument must be at least true or false. (For example, either it is true that today
is Tuesday, or it is false that today is Tuesday.) Traditionally, this is called the
law of excluded middle.27
This principle differs from the law of contradiction in that the earlier law
asserts that truth and falsity are mutually exclusive, whereas the law of excluded
middle implies that together they are exhaustive. That is, while the earlier law
says truth and falsity are incompatible, this new law says that together they
complete a list of possibilities. The law of excluded middle does indeed allow
that a proposition might have any number of different characteristics—maybe
five, maybe ten, or maybe more—but it requires that the proposition have at
least one of these characteristics: truth or falsity.
As with contradiction, the justification of the law is practical. Many things in
life are neither true nor false, and many things seem somewhat true, or their truth
is hard to assess. Nevertheless, if a premise of an argument is neither true nor
false, then the premise proves nothing. (Imagine, for example, someone saying,
“I believe this conclusion for a good reason, but my reason is neither true nor
false.”) On the other hand, if the conclusion of an argument is neither true nor
false, then no argument can prove it. (In ordinary language, what would it mean
to “prove” the untrue?)
Many objections to the law come from construing it as something more than it
needs to be, as a theory of life or of the physical world. Instead, strictly defined

for the purposes of logic, the law applies only to the specific objects of logic—to
premises and conclusions of argumentation in the ordinary sense.
Historically, the law of excluded middle has been much discussed and often
disputed. The ancient Epicureans denied the law outright, and a similar outlook
was embraced by a number of medievals; in modern times, the American
pragmatist C. S. Peirce experimented with alternatives to true and false, and the
Polish logician Jan Łukasiewicz explored the possibility that the list of
alternatives to true and false might be infinitely long.28 When a logician has in
mind only two values, true and false, the resulting system is “two-valued” or
“binary”; but when the logician allows for three possibilities, true, false, and
indeterminate (or, as some medievals called it, “neuter”), the resulting system is
“three-valued.” And Łukasiewicz investigated a “many-valued” logic with the
consequence that more than a few logicians and mathematicians now speak
routinely of many-valued logics, especially when seeking ways for a computer to
assess vague or incomplete information. They also apply the word “fuzzy” to
this area of research, and so there now exist fuzzy logic and fuzzy set theory,
both involving precise definitions and rigorous methods.
Fuzzy set theory, proposed by the mathematician Lotfi Zadeh in 1965, posits a
set whose elements are members only to a certain degree.29 For example, a
person in late middle-age might be conceived as only somewhat a member of the
set of the elderly. Fuzzy logic (also explored by Zadeh) then goes a step further
by assigning degrees of truth to such statements. Thus, in fuzzy logic, it is
somewhat true that a person in late middle-age is a member of the set of the
elderly. Fuzzy logic can be treated as a branch of many-valued logic, and in both
fuzzy logic and fuzzy set theory the degrees of truth or membership can also be
assigned numerical values so they can be treated mathematically. But here’s the
strange part: all these investigations still rely on the same traditional principles
we have already considered: the law of excluded middle and the law of
contradiction. How can this be?
THE NATURE OF FUZZY LOGIC
The simplest way to answer this question is to indulge for a moment in a little
fuzzy logic of our own. Take a simple example. We often remark that a particular
assertion strikes us as “somewhat true.” The remark is vague, and many people,
if they have time, will investigate the assertion to seek out true parts and false
parts or true interpretations and false interpretations. Nevertheless, the remark is

by no means irrational; it faithfully reports our feelings of vague belief or
confusion. Of course, it would make little sense to offer this same assertion as a
reason for believing something to be true. (“My view is true, and the reason I
know it is true is that it follows from something that is somewhat true.” Notice
that even Aristotle acknowledges that generalizations in ethics are often “true for
the most part,” yet he also adds that a premise so qualified can only support a
conclusion that is so qualified.)30
Despite this difficulty, however, we can still draw a good many inferences
about the assertion in question, even if we can never bring ourselves to call it
any more true than “somewhat.” For example, if an assertion is “somewhat
true,” then it can’t be “entirely false.” On the other hand, if it is “somewhat
true,” then it could also be “somewhat false.” In addition, depending on what we
mean by “somewhat,” the assertion might be entirely true. (If by “somewhat
true” we mean “at least somewhat true,” then it is perfectly coherent to say that
an assertion is “at least somewhat true and maybe entirely true.”)
In consequence, we can generate something like the traditional square of
opposition. (See figure 5.1.)
We could easily call this a four-valued logic. Our inferences have involved not
the usual two values of true and false, but four values: “entirely true,” “at least
somewhat true,” “at least somewhat false,” and “entirely false.” And these
assignments of value have logical implications of their own, which turn out, in
this instance, to replicate what we saw earlier in the traditional square. (The first
and last values, “entirely true” and “entirely false,” are logical contraries; the
first and third values, “entirely true” and “at least somewhat false,” are
contradictories; and so on.)

FIGURE 5.1.
Construed in this way, the assertion that we are evaluating (“A”) can have
more than one value at the same time, and this would be analogous to denying
the traditional law of contradiction in the old two-valued system; but we can also
eliminate this feature if we find it disturbing by reducing the values of the
system to three mutually exclusive possibilities: “entirely true,” “only somewhat
true but also somewhat false,” and “entirely false.” (Now the assertion can have
only one of the values.)
All the same, what we see throughout is that a many-valued system will still
involve logical implications of its own, which is just the point that Łukasiewicz
was investigating. (In practice, the values in a system of many-valued logic
might be as varied as “is slightly true,” or “is fifty percent true,” or “is
necessarily true,” or “is possibly true,” or “is true in Euclidean and Riemannian
but not in Lobachevskian geometry.”) Nevertheless, the key point here is that at
no time in these manipulations have we ever violated the law of contradiction or
the law of excluded middle.
Here’s why: We have surely been willing to assign three or four possible
values to the assertion “A,” but at no time did we invoke this assertion “A” as a
premise or conclusion in our reasoning. Instead, our premises and conclusions
were statements about the assertion. That is, our premises and conclusions were
in the form, “The assertion ‘A’ is somewhat true” or “The assertion ‘A’ is not
entirely false.” In other words, our premises and conclusion weren’t simple
statements like “A” but statements about those statements—what logicians call

“metastatements.” (A metastatement is a statement about a statement.) And all
our metastatements were treated as true or false but not both.
Let’s put this point generally: however many values we might assign to a
proposition in drawing inferences of this type, the assumption we make from the
start is that the proposition does indeed have such a value or it doesn’t. And we
also assume that the proposition doesn’t simultaneously have that value and not
have that value. These two assumptions represent the stamp of the two ancient
laws. We regard the assignments of value as being themselves true or false but
not both. On the other hand, if we relax these assumptions and violate them, then
we can draw no reliable inferences at all. In that case, either our assignments of
value become indefinite or they result in contradictions. Put another way still,
many-valued logic seeks to describe the relations that hold among assertions that
fail to qualify as exclusively true or false, but it still tries to describe these
relations truly. And it tries to arrive at this description by logical argument, from
true premises. Thus, to succeed, it still conforms to the same principles that
govern logical argument in every other field.
On this view, many-valued logic isn’t an alternative to ordinary logic but only
a further application of it—to propositions that don’t qualify as “argument” in
the usual sense of the word. (Still another way to express this point might be to
say that, though logic studies an activity, the activity of reasoning, this activity is
made possible by things independent of the activity: logical relations themselves.
Thus logic focuses on our efforts, but it leads us to things that are independent of
those efforts, to a world of abstractions. And many-valued logic studies yet
another part of that mysterious world. Nevertheless, logical argument in the
ordinary sense remains the window through which we see it.)
In recent years, professional logicians have developed a great many
alternative logical systems of this sort, systems analogous to Łukasiewicz’s
version of many-valued logic or to Zadeh’s fuzzy logic. Many of these systems
assign values different from the traditional “true” and “false,” and they can also
employ rules of inference that differ from one system to the next—with the
effect that a valid argument according to one system can turn out to be invalid
according to another. (Some of these systems happen to be called “deviant
logics” or “nonclassical logics.” A “logic” in this sense is a formal system in an
artificial language, and with differing values and rules, these systems can have
differing logical consequences; recent examples include additional fuzzy logics,
relevance logics, intuitionist logics, and paraconsistent logics.)31 Nevertheless,
the coherence of these systems still depends on the same two assumptions: a

proposition being evaluated does indeed have the assigned value or doesn’t and
can’t both have the value and not have the value. (Otherwise, the system
becomes incoherent.) Once again, we see the stamp of the two ancient laws.
But here, perhaps, is a more basic question (or at least a question that could be
posed for the sake of argument): If different logical systems can generate
different results—and count the same argument as valid or invalid, depending on
the system—then isn’t the validity of an argument just relative to a system?
Doesn’t an argument’s validity really depend on which logical system one has in
mind? And if one system should then turn out to be just as good as another, isn’t
the validity of an argument arbitrary?
IS VALIDITY RELATIVE?
Notice that if by a “logic” we mean one of these formal systems in an artificial
language, then men and women don’t normally have different logics, and neither
do different cultures. Instead, the only people who have different logics are
professional logicians (and their students) because these are the only people who
study such systems in the first place. Nevertheless, couldn’t we still say that the
existence of these systems shows that all questions of validity are really just
relative to a system? What counts as valid (we might suppose) depends on which
system you have in mind.
As it turns out, the great majority of these formal systems give the same
answers when it comes to determining the validity of commonsense reasoning in
ordinary life, and if they didn’t, they would appear flatly absurd. Different
systems construe the precise form of commonsense arguments in different ways
(and translate them differently into artificial languages), but they usually provide
some explanation as to why each of these commonsense arguments is valid
nonetheless (as when dealing, for example, with an argument like, “All men are
mortal, and Socrates is a man; therefore, Socrates is mortal”). Instead, the
differences between these systems show up only when their rules are extended to
esoteric cases.32
For example, consider this argument: “Lincoln never lived; therefore, either
the week has seven days or it does not.” Is the argument valid? In some logical
systems this argument is counted as valid on the grounds that it can never have
true premises and a false conclusion; its conclusion is necessarily true. In other
systems (particularly those called “relevance logics”), the argument is invalid on
the grounds that the premise has no real connection (no relevance) to the

conclusion.
Or consider this example: “If John is in Paris, he is in France, and if John is in
London, he is in England. Hence it is the case either that if John is in Paris, he is
in England, or that if he is in London, he is in France.” (In a relevance logic, this
argument would be counted as invalid, and yet in the symbolic system now
called “classical logic,” the argument seems to be valid after all; the system
called classical logic is not a logic of classical antiquity but, rather, a formal
system first developed by Gottlob Frege and then refined by Bertrand Russell
and Alfred North Whitehead. We shall be talking more about the development of
classical symbolic logic in chapter 9, and this argument appears to be valid in
classical logic because of the way classical logic treats the conditional “if-
then.”)33
All the same, whoever is right about these unusual cases, the arguments in
question are exotic from the start. They don’t come from everyday reasoning but
from logicians themselves (whose purpose is to demonstrate the inadequacy of a
system developed by other logicians). Only in these esoteric domains does a
choice of one formal system over another truly make a difference. And the key
point is that no one really needs such a system in the first place to assess the
validity of countless commonsense examples or to distinguish ordinary logical
principles and ordinary logical forms.
By analogy, we often invoke physical principles in ordinary life, and we
distinguish different forms among physical objects, even though the forms have
borderline cases and even though the principles admit of unusual exceptions. For
example, the principle “If you jump from an eight-story building, you will die”
is normally true. Yet there can be exceptions to the principle, as when a safety
net is erected or when an aircushion is deployed by a stunt artist.
Again, when it comes to physical forms, we can distinguish Homo sapiens as
a matter of form from other primates, just as we can distinguish modus ponens
as a matter of form from other kinds of arguments. Nevertheless, there can be
borderline cases of the form, as when an expert examines the fossilized remains
of an early, hominid ancestor. More generally, it is often reasonable to invoke a
principle even though no one is quite sure of the exact extent of its proper
application, and one can distinguish different forms of things even though no one
is quite sure how to judge a borderline case.
It is just so in logic. Modus ponens and the disjunctive syllogism are perfectly
valid forms of argument in ordinary contexts, and each can be viewed as a
principle for logically inferring some propositions from others. Nevertheless,

these forms can also be matters of plausible dispute when applied in esoteric
contexts. (In nonclassical logics, both modus ponens and the disjunctive
syllogism are sometimes treated as invalid.) In short, disputes in the esoteric
cases do nothing to show the inadequacy of such principles in ordinary cases.
And in ordinary situations, our knowledge of which arguments are valid and
which principles are logically sound is quite independent of any of these
systems. (After all, if our knowledge weren’t independent, then no one would
know anything at all about logic except those few logicians who had happened
to hit upon the right system. The true subject matter of these systems is esoteric
from the start.) Though the exotic cases are often baffling, the commonsense
ones are clear.
DOES FORMAL LOGIC ULTIMATELY DEPEND ON
COMMON SENSE?
Nevertheless, why fall back on “common sense” in the first place? Indeed, isn’t
it still a mistake (one might wonder) to place so much confidence in “common
sense”? In fact, couldn’t one of these advanced systems still overthrow what we
normally think of as common sense? And in that case, couldn’t one still say that
what counts as valid is ultimately relative to a system?
Let’s carry this objection just a bit farther (at least for the sake of argument):
Common sense, as the saying goes, is not so common. And many things called
common sense have later turned out to be utterly false, especially in science. For
example, it used to be “common sense” that the earth does not move, because no
one feels it moving, and yet modern science has overturned this idea. By
analogy, then, couldn’t an advanced system of formal logic also overturn some
of one’s commonsense beliefs about what is logically valid and what isn’t?
Of course, anyone’s judgments in logic are apt to be mistaken sometimes;
everyone makes logical mistakes from time to time, even logicians (just as
everyone makes mistakes in arithmetic). And in consequence, one’s abilities in
logic (as in arithmetic) can certainly be sharpened; if formal logic never changed
one’s judgments, there would little point in studying it. But it would be quite
another thing to say that formal logic might overthrow commonsense beliefs to
the same extent that physical theories have overthrown commonsense beliefs.
On the contrary, a theory of physical science is quite unlike a formal system of
logic.
Specifically, physical theories can overturn commonsense beliefs because of

an additional factor: the theories can be supported by empirical tests and
experiments. They can result in testable predictions, and the truth or falsity of
the predictions can be physically observed. By contrast, a formal system of logic
doesn’t seem to be empirically testable at all, or, if it is, the tests are deep and
remote. (Quantum mechanics is sometimes thought to generate physical
counterexamples to the law of excluded middle, but this contention is much
disputed, and, even if granted, the contention would seem to have no bearing on
the law when used in ordinary situations. Once again, the consequences would
be esoteric.)34 History includes many empirical tests in physics and astronomy,
but no one knows how to test a formal logical system as one might test the
Copernican hypothesis or Galileo’s law of falling.
Still, why not push the objection to “commonsense” reasoning (one might
wonder) even further—to its logical extreme?
What if logicians were to invent an alternative system of logic that was so
different it contradicted most commonsense judgments of validity, even those
that seem most obvious? In other words, couldn’t we at least imagine adopting a
different system of logical rules, even in ordinary contexts, and thereby
abandoning what we normally think of as commonsense reasoning? And if we
can imagine this possibility, doesn’t it then follow that our confidence in
commonsense reasoning is actually unfounded and that the choice between one
sort of logic and another is arbitrary after all?35 Philosophers have often been
deeply skeptical about other things; why not be just as skeptical about ordinary
logic?
Now, in considering these last possibilities (and entertaining them once more
for the sake of argument), perhaps it would be useful to think about optical
illusions—where one can pose similar questions about what is truly well
founded and what is really just arbitrary.
Our physical senses sometimes deceive us. We sometimes make mistakes
about the causes of our sensations. We are fooled by mirages, and a professional
conjurer can sometimes make us doubt the very testimony of our eyes. And from
these commonplace experiences, skeptical philosophers from ancient times to
the present have sometimes argued that we don’t even know that we have hands
and feet. Instead, we have no way of knowing (so they say) that the whole world
isn’t just a dream, or an illusion, or the work of an “evil spirit, no less clever and
deceitful than powerful, who has bent all his efforts to deceiving us.”36 Our
belief in the physical world (these philosophers say) is arbitrary from the start.
It takes no great leap to see the similarity between these old skeptical

arguments about physical reality and our new skeptical arguments against
ordinary logic. If the possibility of an alternative logic is a good reason for
doubting ordinary logic, then the possibility of an alternative reality is a good
reason for doubting the existence of the physical world.
When it comes to the physical world, the twentieth-century English
philosopher G. E. Moore once offered what he regarded as a “perfectly rigorous
proof” of its existence. He said he could generate his proof “by holding up my
two hands, and saying, as I make a certain gesture with the right hand, ‘Here is
one hand,’ and adding, as I make a certain gesture with the left, ‘and here is
another.’” His point was that the existence of our hands is already so obvious
that any philosophical argument against it is more likely to be mistaken than our
commonsense belief. In a similar vein, Samuel Johnson is said to have
challenged the skeptical idealism of philosopher George Berkeley by kicking a
stone and remarking, “I refute it thus.”37
Our approach is like Moore’s, except that we refer to simple judgments of
logic and arithmetic in place of judgments about the physical world. Do we have
any less assurance of the validity of modus ponens in ordinary cases than we
have of the existence of our hands?
Writing in the eighteenth century, the Scottish philosophy David Hume
remarked, “Excessive principles of skepticism . . . may flourish and triumph in
the schools, where it is indeed difficult, if not impossible, to refute them.” He
added, “But as soon as they leave the shade . . . they vanish like smoke.” As
soon as the skeptical philosopher confronts a real problem of life, like finding a
meal, obtaining drink, getting shelter, or determining the correct mathematical
sum of his or her bank account, the philosopher reasons just like the rest of us.
“The first and most trivial event in life will put to flight all his doubts and
scruples, and leave him the same, in every point of action and speculation, with
the philosophers of every other sect.”38 Hume observed that even the most ardent
philosophical skeptic, when the lecture has ended, still leaves by the door and
not by the window.39
Just so in logic. In ordinary affairs, even the most skeptical and deviant
logician must still draw inferences like other people—or perish. We can certainly
imagine reasoning differently, and we can imagine doubting ordinary logic, but
imagining that we are doubting is not actually doubting. (Imagining that we
doubt is no more a form of doubting than imagining that we fly is a form of
traveling from place to place.) The argument from what we can imagine
confuses two very different psychological states: doubting ordinary logic and
merely imagining that we doubt it. (C. S. Peirce had a phrase for such imaginary

doubts: “paper doubts.”)
In a word, if logic is useful, then usefulness always puts a limit to what
anyone can regard in ordinary, practical matters as truly logical, even if a world
of alternative logical principles should be somehow “possible.” And in that case
(we should add), the logical correctness of ordinary reasoning doesn’t depend on
its usefulness; rather, its usefulness depends on its correctness. If we don’t
reason in logically correct ways, we get useless results, and what then follows is
that the usefulness of the reasoning depends on its correctness, not the other way
around.40 In sum, the great mass of practical intuitions that we often call
“common sense” will still set boundaries to what anyone can sensibly devise as a
plausible system of formal logic.
Looking back on all these points, we might say that the activity we normally call
“reasoning” is rather like a machine and that the ideas of truth and falsity are
among this machine’s most vital components. If we then take these components
away or try to connect them incorrectly, the machine simply fails to operate. This
doesn’t mean reasoning is the be-all and end-all of life, but only that, if we want
to reason successfully, we need to meet certain requirements.
Aristotle and the Stoics picked their way carefully through many of these
issues, and the medieval logicians who followed inherited a rich legacy. Unlike
the static world inhabited much later by many of the medievals, Greek society
was in constant flux, and the insight of the logicians of the time was to find
continuity and necessity in a world of warring ideas. Despite the varied and
vacillating opinions of the day, the Greek logicians saw plainly that the enduring
quality of commonplace logical forms, especially those that yield precise and
definite results, turned crucially on the law of contradiction and the law of
excluded middle. Only much later, in a new, merciless age of mass killings and
fanatical terror, did thinkers begin to take logic as a discipline in a radically new
direction.

6
LOGICAL FANATICS, CIRCULAR REASONING,
AND DESCARTES’S FUNDAMENTAL PRINCIPLE
FEW THINGS seem less logical than war, and yet war can have profound effects on
logic. Among the most profound of all such effects came during Europe’s wars
of religion, which lasted from 1524 to 1648 and which gave the world a grand
new variety of fanatical systems of belief. What the wars showed above all was
that fanatics can be logical, at least if you grant them their premises. They, too,
can validly infer one proposition from another; they can often supply premises
that, if true, would prove their conclusions.
This quality is typical of fanatics. Fanatics of the sixteenth and seventeenth
centuries were especially concerned with stamping out opposing religious
doctrines, and they sought to do so by executing their enemies as heretics. The
chief prosecutors of heretics during this period were mostly university graduates,
whose reasonings were highly systematic. And to establish the guilt of suspects,
they often offered reams of testimony from witnesses, much of it obtained under
torture. Of course, torture is a poor basis for reliable testimony, but this was
simply not a premise the prosecutors accepted. They thought that testimony
elicited by racking, by hanging from the wrists, by burning with hot irons, or by
the so-called water torture (now called waterboarding) was sufficient ground for
an execution. And they often based their theological conclusions, which were
logically consistent within themselves but contrary to the views of competing
religious sects, on premises no less dubious than the points they wanted to prove.
During this same period, highly trained officials also prosecuted thousands of
people for witchcraft, and they relied on torture for evidence in these cases too.
In sum, what makes a person’s opinions fanatical isn’t always the logical form
of the inferences. Instead, the trouble often lies with the premises—in this case,
with religious assumptions that had been taken for universal truths. The premises
were often unreliable from the start. This is why the French philosopher René
Descartes, writing during the last years of the wars of religion, focused with
great concern on “badly assured premises” at the outset of his famous
Meditations on First Philosophy (1641).
More generally, one effect of the wars of religion was to throw the whole
question of what makes a good premise into stark relief. Descartes asserted that

our beliefs, if rational, need reliable “foundations,” by which he meant they
needed to rest on reliable premises. (And we still invoke much the same idea
today if we say that our faith in a scientific theory must rest on reliable data and
that our faith in the data can’t simply rest on the theory.) Descartes also assumed
that rationally persuasive arguments always go in a particular direction: from
premises that are initially more certain to conclusions that are initially less so. If
our premises turn out to be unreliable, then our conclusions will be unreliable
too. Yet the whole purpose of a persuasive argument is to establish the reliability
of a conclusion that was doubted—and thus the premises must be initially more
reliable.
Descartes’s outlook, if sound, rules out circular reasoning as a rational method
of persuasion (a circular argument has the form “I believe in A because of B and
in B because of A,” as in “I believe in witches because of evidence from torture,
and I believe in evidence from torture because it forces truthful statements from
a witch”). And his outlook also contradicts the view still held by many historians
and philosophers of science to the effect that scientific reasoning must be
somehow circular. (The influential twentieth-century historian Thomas S. Kuhn
long argued that scientific reasoning is necessarily circular, but his argument
rests, in our view, on a definite mistake in propositional logic.) Circular
reasoning has even been defended in modern times by expert logicians, and we
shall be looking at these defenses (by both Kuhn and by experts working in the
tradition of the great German logician Gottlob Frege) in the second half of this
chapter. But first, we shall consider how Descartes came to his own view of the
matter, a view he developed amid great violence. The wars of religion grew out
of a period of intense spiritual renewal, but they also derived from rapid
economic development, and Descartes saw their effects up close. In insisting that
our beliefs need reliable foundations, Descartes was putting his finger on the
problem of an age.
THE ORIGINS OF THE WARS OF RELIGION
Born near the French city of Tours in 1596, Descartes grew up during the final
phases of the wars of religion, when Catholics and Protestants killed one another
in a vast struggle over whose version of Christianity was theologically correct.
The fighting started with the German Peasant Rebellion in 1524, shortly after an
Augustinian monk and theology professor, Martin Luther, posted his famous
Ninety-Five Theses;1 the rebellion cost roughly a hundred thousand lives. Then
the killing spread.

When we think of religious extremism today, we sometimes think of radical
Islam, but the Christian extremism of Descartes’s day was worse. The fighting in
Germany aside, tens of thousands were killed in many different countries and not
by a few zealots. Instead, they were killed by thousands and thousands of
murderers. The murderers were their neighbors. They used swords, axes,
hammers, pikes, cleavers, and primitive firearms, and they even gave the world a
new word, “defenestration,” which meant destroying your opponent by throwing
him from a window. In the course of these battles, two kings of France were
assassinated. And these battles’ final culmination, the Thirty Years War from
1618 to 1648, brought more devastation to central Europe than any event since
the Black Death.
This is one reason people of Descartes’s generation became especially
concerned with problems of logic, and it is also one of the reasons why induction
and scientific method became matters of intense study. The intellectuals of
Descartes’s time saw themselves as fighting superstition and fanaticism and
struggling to restore sanity. They were wrestling with mortal dangers.
Economic development seems to have been a partial cause. Europe’s medieval
order of village isolation and inherited authority was giving way to a new regime
of money. The rise of ocean-going commerce a century earlier had brought new
opportunities to Europe, but over time, it had also undermined tradition. Basic
social patterns had changed in the space of just a few generations, and one
consequence was a general increase in spiritual anxiety. People became less and
less at ease with their overall way of life. Many people of an earlier generation
had been lured by economic opportunity from the farms of late medieval Europe
and had gravitated to the towns and cities, and their children, in turn, had come
to enjoy greater material prosperity. But many of these younger persons, once
grown, had also come to view their new, more prosperous way of life as
dissatisfying. Of course, many made the transition from a feudal existence to a
modern one without great distress; nevertheless, others felt themselves cut off
from a purer way of life and from a better, simpler past. Many sought to
recapture this past through some form of spiritual rediscovery.
Martin Luther epitomized this new frame of mind. The son of a mining
entrepreneur, he was given advantages his elders had never experienced. His
mother had come from a family of middle-class burghers, but his father was
descended from peasants and was illiterate. The young Martin Luther was taught
to read and write Latin and was sent to a university. In keeping with his father’s
wishes, Luther took up the study of law, but he was also tormented by guilt.
When several friends died of the plague and he was almost struck by a lightning

bolt, Luther became convinced that God was giving him one last chance to
renounce a life of sin. He withdrew from his studies abruptly and entered an
Augustinian monastery.
Luther became an ardent monk, but his abbot thought Luther’s zeal so
excessive that he reassigned Luther to further scholarly work to distract him
from introspection; Luther was eventually ordained as a priest and appointed a
professor of Holy Scriptures at the University of Wittenberg. And it was at
Wittenberg that Luther soon worked out his most important theological ideas
(which partly express his profound sense of guilt): all human beings are so guilty
as to deserve damnation; God saves some anyway, not out of justice but out of
mercy; salvation is never obtained through the sacraments of the Roman
Catholic Church but only through faith in Jesus Christ; and the Bible is the only
source of religious truth. Luther held that the religious teachings of the Catholic
Church were utterly unfounded unless inferred from the Bible.
Luther’s insistence that the Bible was the only source of religious truth
became a core idea of later Protestantism (and it still tends to distinguish
Protestants from Catholics today). But his focus on the Bible was made possible
in the first place only by the printing press, which became practical and
economical in the 1450s and vastly multiplied the number of Bibles. More
books, including Bibles, were produced in the first fifty years of this new form
of printing (with metal, moveable type) than in the preceding thousand years. In
addition, Luther’s approach had a broader social effect: by inviting people to
read the Bible for themselves, Luther implicitly encouraged personal
interpretation and individual reasoning, independent of the authorities of the age.
As Luther expressed the matter, every believing Christian became the equivalent
of a priest. The effect of his example was to encourage people to think for
themselves.
As it happened, Luther expected other Christians to reach the same
interpretative conclusions about the Bible as he had, and when they didn’t, he
became deeply intolerant. He sympathized with Jews as long as the Catholic
Church espoused doctrines that he rejected, but when Jews failed to convert to
Christianity after hearing his own version of it, he wrote that Jews should be
killed.2 In fact, all prominent theologians of the time, whether Protestant or
Catholic, were deeply intolerant. They saw those who disagreed with them as
lying against God, and they also believed that political stability depended, above
all, on having a populace that agreed on religious fundamentals. As a result, they
regarded heretics as rebels of the worst sort. Even Sir Thomas More of England,
who died a martyr for freedom of conscience, nevertheless believed that

heretical declarations ought to be punished.
As Lord Chancellor of England, More authorized the execution of declared
heretics; he thought people should be left unmolested if they believed what the
authorities rejected—so long as they didn’t encourage other people to believe
likewise by making heretical declarations, helping to circulate heretical tracts, or
distributing English-language Bibles. Nevertheless, for More, declared heresy
was still a crime. As More expressed this view at his own trial, the laws “can do
no more than punish words or deeds; ’tis God only that is the judge of the secrets
of our hearts.” More distinguished between freedom of conscience, which he
defended, and freedom of expression, which he opposed; our modern ideas of
free speech and freedom of the press only began to emerge much later, after
many generations of war.3
As for the sorting out of these religious differences between Luther and his
opponents by reasoned analysis, all sides reasoned carefully, but they reasoned
from opposing premises, and so a great collision between Catholics and
Protestants became inevitable.
Initially, Luther’s protest had been limited; in his Ninety-Five Theses (which
he wrote in Latin but circulated immediately in German translation), he had
confined his attacks to the pope’s representatives in Germany. (These
representatives, especially the Dominican friar John Tetzel, had been selling
indulgences for the dead; the indulgences were church documents that allegedly
transferred the superfluous merit of Christ, the Virgin, and the various Christian
saints to individual sinners so that the sinner might escape some of the torments
of Purgatory after death. This superfluous merit was additional virtue, over and
above what was needed for a saint’s own salvation. In Luther’s time, one could
buy an indulgence not only for oneself but for one’s departed relatives, and as
Luther complained, the customers for this trade were often the gullible poor.)
Then, in debate, Luther denied the infallibility of the pope, finding no warrant
for it in Scripture. Next, showing great courage and facing Holy Roman emperor
Charles V of Spain at the Diet of Worms in 1521, Luther refused to recant, and
he insisted he would not “act against conscience.” He is supposed to have said,
“Here I stand, I cannot do otherwise, so help me God.”4 Prince Frederick III,
elector of Saxony, had arranged a guarantee of safe passage for Luther to face
Charles at the Diet, but afterward, fearing for Luther’s safety, Frederick had
Luther abducted for his own protection and hidden away at Wartburg Castle at
Eisenach for nearly a year, during which time Luther translated the Bible into
German.
As vernacular translations of the Bible began to multiply, theological

controversies pitted authorities against authorities, and among the literate
commercial classes of Europe’s towns and cities, they pitted families against
themselves. The ordinary urban dweller soon became a sort of amateur
theologian, and furious debates arose within many households over the true
meaning of the Holy Word and over which way of life was really sanctioned by
Scripture.
At roughly the same time, the fear of witchcraft, which had existed in ancient
and medieval times, also began to increase dramatically and to take on a strange
new form. During the Middle Ages, witchcraft had been regarded as a minor
crime, but starting in the fifteenth century, it was thought to involve sexual union
with the devil or his agents, often operating in the guise of animals. Prosecutions
for witchcraft began to accelerate (a phenomenon that historians now call the
“witch craze”), but the character of the prosecutions changed as well. Alleged
witches in medieval times were both men and women, but by the sixteenth
century, the accused witches were overwhelmingly female, their prosecutors
exclusively male. And the alleged witches were often inspected naked in a
search for signs of sexual contact with the infernal powers. A prosecutor
frequently used torture, and he looked for a so-called devil’s mark or witch’s
teat; it might have been a mole or blemish or perhaps a patch of insensitive skin
that revealed itself when the skin was pricked with a needle. And so, gradually,
men’s concern with the behavior of women devolved into the grotesque. The
causes of the witch craze are still much debated, but like the wars of religion,
with which it largely overlapped, the witch craze seemed to express a heightened
sense of social anxiety. (The witch craze finally died out around 1700 as more
and more magistrates, influenced by Newtonian physics, came to believe that
action at a distance through spells and incantations was physically impossible.)
When all these monstrous fancies and bizarre imaginings finally erupted in
general violence, the results were horrific. Among the many victims was the
logician and pedagogue Peter Ramus, who had taught at the University of Paris
and who had often challenged Aristotelian doctrines that were current in his day;
Ramus was killed during the St. Bartholomew’s Day massacre of 1572, when
Huguenot Protestants in many parts of France were slaughtered by the
thousands.5 Nevertheless, what Descartes came to realize was that the fanatics
who presided over these killings could construct valid arguments like anyone
else. He saw that fanatics could often supply premises that, if true, would
support their conclusions. (As John Locke later expressed the point, a madman
often reasons well; the trouble is that he reasons from the wrong premises.)6 As a
result, one of the great philosophical questions of any period became

increasingly apparent during the sixteenth and seventeenth centuries: What
distinguishes a reasonable person from a fanatic?
The wars of religion had many philosophical effects (and we shall be looking
at some of the others in the next chapter). But, for the present, we want to focus
on Descartes’s structural principle: our beliefs, if reasonable, need reliable
foundations.
THE IMPORTANCE OF FIRM FOUNDATIONS
Specifically, Descartes likened our beliefs to a house, a house in which there are
lower parts and upper parts—foundations and a superstructure. Just as a house
can have higher stories built on lower ones, so can further conclusions rest on
earlier conclusions as we pile inferences on inferences. We build an ever-higher
edifice of beliefs, with each new story inferred as a conclusion from the stories
below. Nevertheless, the entire structure depends on its starting points. If we
later discover that our first premises were false, our rational confidence in the
rest of the structure is undermined. As a result, Descartes’s aim was to determine
whether the foundations of his beliefs consisted in a stable basis of “rock and
clay” or an insecure one of “loose earth and sand.”7
Descartes didn’t invent this notion; the basic idea came from Aristotle, who
had asserted in his Posterior Analytics that the premises of a rational
demonstration must always be “better known” than the conclusion. The key to
Descartes’s outlook lies in Aristotle’s meaning. In part, Aristotle means that we
must always regard the premises of our rational inferences as initially more
acceptable than our conclusions. Or, as he puts it himself, we must find our
premises initially “more convincing” than our conclusions.8
Aristotle’s assertion is crucial to logic, and on reflection it helps to show why
circular reasoning can never be rationally persuasive. (Circular reasoning,
remember, takes the form, “I believe in A because of B and in B because of A.”)
And his assertion also expresses the basic defect in fanatical systems of belief.
The foundations of a fanatical belief aren’t “known” in the first place. Yet
Aristotle’s observation also differs from anything we have considered so far, and
understanding how it differs is part of understanding why his observation is
correct.
In particular, though Aristotle agrees that the premises of a demonstration
must be true and must validly entail the conclusion, he is also saying something
more. He means we must also be more inclined to believe the premises or less

inclined to doubt them. In effect, then, Aristotle’s point doesn’t concern validity,
nor does it concern the truth of the premises; instead, it concerns a further,
psychological requirement of the person to be persuaded. A rationally persuasive
argument always relies on what this person already accepts and then tries to
invoke such opinions to prove what this person doesn’t accept. (We also
persuade people, sometimes, by pointing out neglected or forgotten facts, but
Aristotle’s dictum applies whenever these facts serve as premises. The person to
be persuaded must still be more inclined to accept these facts, once pointed out,
than the conclusion to be proved.)9
Descartes’s image of a house then follows easily. The foundations are the
premises we begin with, the things Aristotle regards as initially more convincing;
the superstructure consists of the conclusions we draw. In consequence, the
image of a house is merely a convenient way of visualizing the inferring of
conclusions in the manner of Aristotle—from the ground up. (The contrast
between a house founded on rock and one founded on sand comes originally
from the Sermon on the Mount in the Bible, Matthew 7:24–27; the wise man is
said to build his house on rock, the foolish man on sand.) Of course, we must
also assume a further restriction here: once a superstructure is constructed, it
can’t simply levitate in the air after the foundations are removed. If we build new
foundations or make modifications to the existing ones, the superstructure can
remain; nevertheless, Descartes’s assumption is that beliefs in the superstructure
don’t become rational merely because they used to be rational. Instead, they
need to be sustained, whether by old supports or new ones, or else they come
crashing down. And this interpretation is once again true to Aristotle. (Aristotle
speaks of our primary premises as “causes” of our conclusions, but the causes he
has in mind are sustaining causes, not merely passing ones.)10
In addition, Descartes’s analysis, like Aristotle’s, rules out circular reasoning.
Take, for example, a circular argument that was pointed out by Descartes: God
exists because it is so stated in the Bible, and the Bible must be true because it
comes from God.11
If the two philosophers are right, then premises and conclusions can never be
on a par. The premises of an argument must be initially more convincing, the
conclusion initially less so. But the point of speaking of a circle (a metaphor, to
be sure) is to say that the various elements of the circle are indeed on a par and
that the same proposition is simultaneously the premise and conclusion of some
other proposition. If A is your reason for believing in B while B is your reason
for believing in A (if you believe that God exists simply because the Bible says
so, and you also believe that the Bible must be true simply because you believe

it comes from God), then A and B can’t each be more convincing to you than the
other (as Aristotle notes).12
The same is true no matter how many elements the circle contains. As a result,
the basic trouble with any circular argument can be stated thus: If each element
of the circle depends for its acceptance on all the other elements (each being
accepted because of the rest), then there are only two possibilities: either all
elements are accepted or none are. If none are accepted, the circle is
unpersuasive, since there are no accepted premises. If all are accepted, the circle
is superfluous. (If all are accepted, the circle merely preaches to the converted;
those to be persuaded already accept the elements anyway.)13
Such, then, is Aristotle’s basic idea, which Descartes illustrates with the image
of a house. The deeper question, however, is whether the idea is really right.
What reasons are there for supposing Descartes and Aristotle to be correct?
There are two.
First, it is absurd to say that a premise helps to persuade us rationally even if
we don’t accept it. (Imagine someone saying, “Your reason helps to change my
mind, but I don’t accept it.”) Second, it is equally absurd to say that an argument
rationally persuades us even if we already accept its conclusion. (“Your
argument has changed my mind, but I already accepted the conclusion anyway.”)
Taken together, these two observations imply that, when we change our minds
by rational inference, we do so because of reasons we do accept and because
they turn out to imply something that, initially, we didn’t accept.
(Whether we accept a thing can also be a matter of degree; many things in life
strike us as somewhat acceptable and somewhat unacceptable. Nevertheless, a
premise must still be more acceptable to us than the conclusion since, otherwise,
no such premise will be acceptable to us in the first place unless the conclusion
is already acceptable too. And in that case, the argument will be superfluous. A
proof can’t change our minds if our minds are already changed.)
The trouble with fanatics, then, is that the foundations of their beliefs—the
most basic of all their premises—aren’t better known in the first place. They
have built their edifice on sand. Descartes saw these implications plainly, but his
own personal remedy for the unreliable premises of his time was highly peculiar.
(Descartes was even accused of reasoning in a circle himself, but he denied
this.)14 Wary of making any dubious assumptions and convinced that his earlier
beliefs had been built on sand, Descartes fell back on what he called “hyperbolic
doubt.” He said he would try to doubt everything, including all teachers, all
authorities, and all prejudices. He would even doubt the existence of his physical

surroundings and his physical body, and he would suppose that perhaps
everything in life was only a dream. (Looking out a window, he wrote, “I do not
fail to say I see men. . . . Nevertheless, what do I see from this window except
hats and cloaks, which might cover ghosts or automata, which move only by
springs?”)15 In place of his earlier premises, he would seek out some new
foundation for the edifice of his beliefs, and he would erect a new philosophical
system on the basis of it—a foundation beyond all possible doubt.
In the end, he said he found that he could not doubt that he was doubting and
that, therefore, he must also exist. (To doubt one’s own existence, Descartes
insisted, is logically impossible, since one must exist to do the doubting.) He
discovered his new foundation in the utterance, “I think, therefore, I am.”16 The
rest of his philosophical system was then to be inferred from this one, peculiar
starting point.
To modern readers, Descartes’s whole approach to doubt and certainty is apt
to seem eccentric, but it is more understandable if we recall the amazing
credulity and fanaticism he was reacting against: the stunning lack of caution
with which both Protestants and Catholics had asserted that they knew assuredly
that God was on their side and that their opponents had to be killed. Descartes
was exploring doubt in reaction to an overestimation of faith. Many critics have
since wondered whether anything of much interest can actually follow from
Descartes’s single declaration, “I think, therefore, I am.” And many have also
questioned whether his “hyperbolic doubt” is even possible. (David Hume
argued that such extreme doubts are psychologically impossible from the start;
we can imagine that there is no physical world, and we can imagine doubting it,
but imagining that we are doubting is not actually doubting. From Hume’s point
of view, Descartes had confused two different psychological states: doubting the
existence of the external world and merely imagining that he doubted it.)17
Descartes was seeking to check the unbridled zeal of his time, and his
philosophical system has since generated countless controversies. Nevertheless,
the question of his system’s validity, though interesting, still differs from his
larger, crucial claim: the trouble with fanatics is that their premises are unreliable
from the start and in consequence their conclusions are also unreliable—and
often dangerous. It was this larger claim that became, perhaps, his most
important legacy.
THE LOGICAL COMPLEXITY OF OUR PREMISES

Of course, the need for reliable premises might look obvious, but in fact the
question is tricky because our premises can be logically complicated. Like other
elements of logic, our premises can be simple propositions but also compound
propositions, and the logical complexity of compound propositions sometimes
fools us into thinking that fanatical reasoning and circular reasoning are as
sensible as scientific reasoning, or even as sensible as abstract reasoning in
higher mathematics and advanced logic. The trouble is that all these different
modes of reasoning sometimes look the same, and their similarities can trip up
even the experts.
To illustrate this problem, consider a case of everyday reasoning that is quite
sensible but that looks peculiar. Consider a case that seems to violate Aristotle’s
rule that the premises must always be “better known.” Consider how a jury
sometimes assesses an alibi.
Suppose a jury in a criminal trial hears an alibi from a defendant but then
begins to doubt whether the alibi is true. Suppose a great many credible
witnesses come forward to challenge the alibi, and suppose each of these persons
testifies that the defendant wasn’t present where he says he was, so that either all
the witnesses are lying or the alibi is false. And suppose the jury can see no good
reason why any of these witnesses would lie. In a circumstance like this, the jury
might well reason that, though no one witness is necessarily reliable, the
combined probability that all are lying is remote and that, consequently, the alibi
must be false. Too many other people would have to be lying, or too much other
testimony would have to be discounted (the jury might suppose) for the alibi to
be true. In this sort of case, the jury might seem to be reasoning from a series of
premises that are each less reliable (and therefore less convincing and less
acceptable) than the conclusion. The jurors aren’t necessarily convinced by any
one witness, yet they are firmly convinced of their conclusion—that the alibi is
false—because so many seemingly honest witnesses would have to be ignored.
Thus it might seem that each of the jurors’ premises is less convincing to them
than their conclusion. As a result, the jury seems to violate Aristotle’s dictum,
and yet many juries might sensibly reject an alibi for just this reason.
This is the sort of situation where we can easily get confused about the logical
form of our premises. In fact, the jury in this example hasn’t violated Aristotle’s
dictum in the least—and the notion that it has is a logical mistake.
Notice, first, that the jury isn’t saying that all the testimony from the crucial
witnesses is true. Instead, the jury only says that not all of it is false. The second
statement is more cautious than the first, and the jury only needs to rely on the
second statement. The jury says not all the witnesses are lying (since if even one

is truthful, the alibi must be false). But the key expression here is “not all,” and
what this expression actually introduces is a compound proposition—a
disjunction. To say that not all the witnesses are lying is to say that at least one
of them is truthful (just as to say that not all of them are truthful is to say that at
least one of them is lying; strictly speaking, the expression “not all,” like “not
both,” introduces a negated conjunction, as in “not both A and B”; this is
logically equivalent to the disjunction “not A or not B”).
The jury’s premise, then, is equivalent to the statement that at least one of the
witnesses is truthful—either the first witness or the second or the third, and so
on—or maybe some combination of them. In that case, we can represent the
form of the jury’s premise like this,
A or B or C or D . . .
where each letter represents an assertion of the truthfulness of a different
witness.
Once we perceive the correct logical form of the premise, we can also see how
the premise can still be “better known” than the jury’s conclusion. Since each
claim by a witness contradicts the alibi, it follows that, if the disjunction is true,
the alibi must be false. We can represent this last point in the jury’s line of
argument as a conditional (or “hypothetical,” meaning an if-then proposition),
like this,
If (A or B or C or D . . .), then not F,
where F represents the alibi. If we put it all together, the real reasoning of the
jury looks like this:
If (A or B or C or D . . .), then not F.
A or B or C or D . . .
Therefore, not F.
This is actually a complicated form of modus ponens, which is usually written
like this:
If A, then B.
A.
Therefore, B.
The only difference here is that the usual A has been replaced with a more
complicated element, “(A or B or C or D . . .),” and the usual B has been

replaced with “not F.”
This sort of reasoning occurs in countless situations of daily life, and what
makes it tricky is the inclusion of an either/or proposition inside an if-then
proposition—a disjunction inside a conditional. (The technical name for this
form is a “conditional proposition with a disjunctive antecedent.”) And yet our
minds do this all the time, usually without our notice. We do this whenever we
say “they can’t all be lying,” “they can’t all be wrong,” “at least some of these
findings must be true,” or “at least some of the empirical data must be accurate.”
These expressions introduce a disjunction (or its logical equivalent), and what
we then do is invoke the disjunction to support some further inference about the
world.
Scientists reason this way whenever they reject an explanatory theory in
physics on the grounds that it is incompatible with too many empirical findings
or with too much that is otherwise reliable. Mathematicians reason this way
when they cling to a postulate because rejecting it would force them to give up
too many well-established theorems. We also reason in this way when we reject
some bizarre conspiracy theory on the grounds that too many people would have
to be lying for the conspiracy to be real. The expressions “too many” and “too
much” function like “not all”—they mean that at least one of the contrary
assertions, or at least some of them, must be correct and that, consequently, some
further inference follows. (To say that “too many” things would have to be false
is to imply that not all those things are false, and this entails that at least one of
them is true.)
This form of reasoning, to repeat, is disjunctive, but it is often mistaken for
reasoning from an unreliable premise. The essence of disjunctive reasoning is to
fashion a reliable premise out of a series of unreliable parts. What is less reliable
is the testimony of any one witness, but the assertion that at least one of them is
truthful can be highly reliable, just as it can be very unlikely that all are lying.
Thus we use less reliable parts to get a more reliable whole, and it is exactly this
feature that often fools the experts.
The common mistake is to think that disjunctive reasoning is circular
reasoning or that it is logically equivalent to a fanatic’s reasoning. With
disjunctive reasoning, it is only the parts of the premise that are unreliable, but
with fanatical reasoning, the whole premise is unreliable. (The fanatic relies on
whole assertions that are dubious, like the assertion that testimony obtained from
a witness under torture is reliable.) And circular reasoning consists in using the
same proposition as both a whole premise for, and a conclusion of, some other
proposition (in which case neither proposition can be initially regarded as more

reliable than the other; instead, the reliability of each depends on the reliability
of the other). But asserting a disjunctive premise as the jurors do—when they
reject the defendant’s alibi on the grounds that it is incompatible with all the
witnesses’ testimony—is quite different from asserting each part of that
disjunction (the testimony of each witness individually) as a whole premise.18
This is the rather sticky point that has often misled historians of science and
experts in mathematics and logic. Many of these experts have talked themselves
into the different idea that even our most advanced forms of systematic
reasoning today are circular. In the twentieth century, the historian Thomas Kuhn
looked at the disjunctive reasoning of physical scientists in his influential
monograph The Structure of Scientific Revolutions (first published in 1962) and
mistook it for circular reasoning (or so we believe). Many logicians and
mathematicians have made a similar mistake, and so they have concluded that
their own disciplines are fundamentally circular too. In the years since Kuhn first
published, some experts have even supposed that there is actually nothing more
to advanced reasoning in science, mathematics, and logic today than its
systematic coherence, as if the whole difference between a reasonable system of
belief and a fanatical system were only a matter of opinion.
How did this happen? How did these experts lose sight of the fact that our
most reasonable systems of belief do indeed rest on foundations that are initially
more reliable or “better known” than the conclusions they imply?
The answer lies in developments that came long after Descartes, despite the
continuing impact of Descartes’s ideas. We shall be returning in the next chapter
to seventeenth-century thought and to the further effects of the wars of religion.
For the rest of this chapter, however, we intend to explain how these modern
experts (in our view) got themselves confused. They did it by focusing on further
developments in logic. Advanced logic and mathematics were fundamentally
transformed in the late nineteenth and early twentieth centuries by a new
movement called “formalization,” and an unintended consequence of this
movement was to cause experts in many different fields to misjudge the true
foundations of their own convictions.
THE ORIGINS OF FORMALIZED LOGIC AND
MATHEMATICS
Formalization was crucial to modern mathematics and logic, but it was also a
source of great trouble because it led logicians and mathematicians into a

situation like that of the jury. These thinkers wanted to base their conclusions on
firm foundations, yet the sheer abstractness of their work eventually led them
into something quite different: a maze of uncertainties and unexpected paradoxes
that made them unsure of the things they had wanted to take for granted. They
had begun with “axioms” (from the Greek word axios for “worthy,” meaning a
proposition so worthy as to be self-evident) or with “postulates” (from the Latin
postulatus, meaning only something assumed to be true; these days, both terms
—axioms and postulates—are often used interchangeably). They thought these
axioms and postulates were correct.
Yet, as things turned out, these same axioms and postulates eventually led to
unforeseen logical contradictions, and so the experts eventually became wary of
trusting in any one axiom or postulate alone. They began to view each axiom or
postulate as a jury might view the testimony of any one witness, as something
that might turn out to be as dubious as, or even more dubious than, the
mathematical or logical conclusion it was supposed to support. As a result, the
experts began to wonder whether higher mathematics and advanced logic might
actually be circular, or whether there was anything to recommend a formal
deductive system in these disciplines other than its systematic coherence. They
became confused about the foundations of their own beliefs.
The story of this perplexity and puzzlement is a tale of intense philosophical
speculation.
Gottlob Frege, who did his most significant work at the University of Jena
from around 1879 onward, pioneered formalization, and for Frege the word
“foundations” was all important. At first, he meant by the word exactly what
Descartes meant: things we are initially more certain of and from which we then
deduce a series of conclusions. Frege wanted to build a new edifice for logic and
arithmetic, and he wanted to found this edifice on a stable basis of rock and clay
rather than loose earth and sand.
There were several reasons for Frege’s efforts, but among the most influential
was the nineteenth-century discovery of non-Euclidean geometry. Euclidean
geometry had assumed that parallel lines never meet. In the nineteenth century,
however, several mathematicians (N. I. Lobachevsky, Farkas Bolyai, and G. F.
B. Riemann) showed that alternative geometries were possible, and the effect of
this insight was that it caused many mathematicians of the time to wonder
whether the things they had long taken for axioms were actually unassailable.19
They became especially wary of making undue assumptions. Their great fear
was that, like earlier thinkers, they might end up with unreliable conclusions
because of unreliable premises. As a result, the aim of Frege and his followers

was to specify all starting points that would go into the building of a formal
deductive system—all axioms, postulates, and definitions—and also all methods
of inference that would arise in moving from one proposition to the next.
It was, however, the second of these exacting tasks, the specifying of all
methods of inference, that chiefly distinguished Frege’s work. His purpose was
to make all his reasoning especially careful by making it especially explicit, so
that all doubtful assumptions might be exposed. Frege expressed this aim in his
Basic Laws of Arithmetic (1893), stating, “By insisting that the chains of
inference do not have any gaps, we succeed in bringing to light every axiom,
assumption, hypothesis, or whatever else you want to call it on which a proof
rests; in this way we obtain a basis for judging the epistemological nature of the
theorem.”20
To accomplish this task, Frege had to go to great lengths. He had to invent a
new, symbolic language to express the various propositions he wanted to prove,
and he also needed to lay down rigid, mechanical rules that would govern how
statements in this new language would be formed. The reason for these demands
was that, otherwise, Frege would have no sure way of determining whether any
proposition he sought to prove really followed from any other. Unlike the rest of
us, Frege was quite unwilling to trust in the vagaries of the ordinary languages of
the human race (or “natural languages”), and he was distrustful of these
languages because he knew that, to a mathematician, ordinary languages are a
common source of error.
In ordinary language, we commonly infer some propositions from others, but
we often make many grammatical transitions along the way, and sometimes we
use the same expressions to mean different things. It is these subtle shifts in
grammar and meaning that can cause mistakes in mathematics and logic,
especially when the premises are abstract and the proofs are long. For example,
if we say, “All leprechauns have treasure,” we can mean that there are
leprechauns and they have treasure, or we can mean only that, if there were
leprechauns, then they would have treasure; the two interpretations have
different implications. Again, when we say, “Every cat is an animal,” we use the
word “is” to say that cats are included among animals, but when we say, “The
evening star is the morning star,” we use the word “is” to signify a different
logical relationship—not the inclusion of one class in another class but the
identity of two objects. The evening star is identical to the morning star. Thus the
word “is” can signify different logical relationships depending on context, and
these different meanings can lead to different results. These subtle shifts in
meaning are what can cause mistakes in abstract reasoning.

Seeing this, Frege’s aim was to avoid such difficulties by expressing
arithmetic and logic in an artificial language of invented symbols that would be
entirely unambiguous. Once this language was constructed, Frege would then
look for a way to reduce all questions of what was proved and what wasn’t to a
purely mechanical task of determining whether the various strings of symbols in
this new language conformed to the language’s rules. To carry out this project,
Frege would need to devise rules for forming true or false statements out of
strings of artificial symbols (the “formation rules” of his language), and he
would also need to devise rules for logically inferring one well-formed string of
symbols from another well-formed string of symbols (the “inference rules” of
his language). As it happened, all these developments were also intimately
connected to the material conditions of Frege’s time (particularly to the
Industrial Revolution), but this is an aspect of his work that we will take up in
greater detail in chapter 9.
If all the rules were designed properly, the question of what had been proved
and what hadn’t would then be largely reducible to a clerical task—the task of
determining, first, whether the various strings of symbols were formed in the
right order and, second, whether the transitions from one string to another string
counted as valid logical inferences as allowed by the system. This basically
clerical chore might even be accomplished by a machine. Arithmetic and logic
would thus be “formalized.”
This is the essence of formalized logic and mathematics today, and Frege’s
approach, with various modifications, remains the accepted procedure in the
more abstract branches of both disciplines. Nevertheless, Frege’s project ran into
trouble, and the effect of this trouble was to multiply confusions over Descartes’s
basic idea that our rational beliefs need foundations. The trouble came from
paradoxes.
THE PARADOXES OF FORMALIZATION
A paradox (to a logician) is an apparent contradiction,21 and logic has a long
history of generating apparent contradictions, especially when it investigates
sentences that refer directly or indirectly to themselves. In fact, when a sentence
refers to itself, the result is often a paradox. There are many instances of this
effect. For example, “This sentence is false.” (If true, the sentence is false; yet, if
false, it is true.) Another example is, “A man says he is lying; is what he says
true or false?” (This is the “liar’s paradox,” attributed to the ancient Eubulides.)22
Or consider: “Jones predicts that Smith’s next statement will be a lie; Smith

replies that Jones’s prediction is true.”
Frege was eventually snared in just such a paradox. But first, to illustrate the
pervasiveness of these puzzles, here is one more such paradox—this time from a
real court case (or so the ancient authors tell us):
In the fifth century B.C., the Sophist Protagoras was supposed to teach the art
of legal argument to a student named Euathlus, but under the terms of their
contract, Protagoras wasn’t to be fully paid until Euathlus had won his first case.
After the course of study, however, Euathlus decided not to practice law and
therefore never paid the rest of his fee. In retaliation, Protagoras chose to sue
Euathlus and made this argument to the court: “If I win this case, then Euathlus
must pay me—by the judgment of the court. But if I lose this case, then, again,
Euathlus must pay me—because he will have won his first case. Yet I must win
or lose. Therefore, Euathlus must pay me.”
As it turned out, Euathlus was a better student than Protagoras had assumed,
and in response, Euathlus addressed the court as follows: “If I win this case, then
I won’t have to pay—by the judgment of the court. Yet if I lose this case, then,
again, I won’t have to pay—since I won’t have won my first case. But I must
win or lose. Therefore, I won’t have to pay.”
Such, at least, is the traditional story, and though the jury never declared a
winner (“out of fear their decision might annul itself”),23 the argument is still
curious because it turns once more on the phenomenon of self-reference. Though
both arguments look equally plausible, they refer indirectly to themselves. The
contract the men argue about concerns the winning of a case, yet the case to be
won concerns the contract itself. As a result, when the two men argue, they argue
about who should win the argument.
Now logicians can say a good deal about these paradoxes, but the trouble for
Frege was that the English philosopher Bertrand Russell had found a paradox in
his system. Specifically, Russell found a paradox, now called Russell’s paradox,
that was a logical consequence of Frege’s foundations, and Russell pointed this
out in a private letter to Frege in Germany in 1902. Mercifully, the paradox
didn’t appear in Frege’s formalization of traditional logic. It did, however,
appear in the more esoteric domain of his work—in his attempt to formalize
arithmetic—and this was damage enough.
(Frege had wanted to define arithmetical numbers in terms of classes of
classes, but this raised the possibility that certain classes, or sets, might contain
themselves as members, whereas other classes or sets might not contain
themselves as members. For example, the set of all dogs is not, itself, a dog.

Thus the set of dogs does not contain itself as a member. On the other hand, the
set of all things that are not dogs is also not a dog. Thus the set of nondogs does
seem to contain itself as a member—since the set of nondogs is nondog. Russell
then suggested the following: consider the set of all sets that do not contain
themselves as members; does this set contain itself? If it does, then it doesn’t,
and yet if it doesn’t, then it does.)
Russell’s paradox is related to another such paradox offered by Russell: “the
barber.” Consider a barber who shaves all those who do not shave themselves
and only those who do not shave themselves; who, then, shaves the barber? As it
happens, if he shaves himself, then he doesn’t, and yet if he doesn’t, then he
does. In fact, there can be no such barber. And by the same token there can be no
such set of the kind Russell had derived as a logical consequence from Frege’s
foundations. (Russell criticized Frege’s work, by the way, but he also revered it,
and it was Russell who did more than anyone else to bring the importance of
Frege’s insights to the attention of logicians around the world.)
Frege understood Russell’s criticism immediately and was stricken. He
concluded that much of his project, at least as it applied to arithmetic, was
jeopardized. Plainly, he reasoned, his foundations must be incorrect. (He wrote
to Russell, “Your discovery of the contradiction has surprised me beyond words
and, I should like to say, left me thunderstruck, because it has rocked the ground
on which I meant to build arithmetic. . . . It is all the more serious as the collapse
. . . seems to undermine not only the foundations of my arithmetic but also the
possible foundations of arithmetic as such.”)24 In response, Frege set about
adjusting his foundations to make the paradox go away, but soon more problems
developed, and toward the end of his life he despaired of ever completing the
task he had imagined.
As it turned out, Bertrand Russell himself suggested a way out of the impasse,
and the formalization of arithmetic—and other branches of mathematics too—
still rests largely on techniques that Russell and Frege had forged.25
Nevertheless, the overall effect was to make logicians and mathematicians
especially wary. They became increasingly unsure of the abstract assumptions
they were calling “foundations,” and they looked for additional ways to assure
themselves that these foundations were truly correct. Russell’s paradox is, of
course, abstract from the start, and to anyone except a mathematician, it is apt to
seem only a tempest in a teapot, a curious little conundrum that applies only to a
particular way of seeing higher mathematics and that has no application to the
logic of everyday life. All the same, the paradox had a profound effect because it
induced the experts to look at “foundations” in a different way. (As for Frege

himself, he never lived to see his formalization of arithmetic fully carried out.
After Frege’s death, another logician, Kurt Gödel, showed by way of another
paradox of self-reference that at least some versions of Frege’s project could
never be realized. Though the formalization of mathematics survives today and
is certainly alive and well, some versions turn out to be logically impossible.)26
So why should these events have caused logicians and other experts to
mistake sensible reasoning for circular reasoning? And why should they have
caused some thinkers to abandon Descartes’s idea that our rational beliefs need
foundations?
THE DOUBLE MEANING OF “FOUNDATIONS”
For this reason: logicians and other experts got mixed up about why they
believed what they believed (or so we contend) because they began using the
word “foundations” in two different ways. They still used the word as Descartes
had used it, but they started to use it in another way too, without seeing the
difference.
On the one hand, they used “foundations” to mean the things they were most
certain of, the things they were most likely to know for sure. This was
Descartes’s sense of the term, and, for want of a better expression, we might call
these “epistemological foundations” (from the Greek epistēmē, for knowledge).
Our epistemological foundations are the things we feel most sure about and from
which we then draw conclusions. They are the premises for which we offer no
further premises because we feel they can be safely taken for granted. This is
still the way we often speak of foundations in ordinary language and in everyday
life.
On the other hand, they sometimes used the word “foundations” to mean
something subtly different: a small set of axioms, postulates, definitions, and
inference rules that would generate the rest of a formal logical system (a system
like Frege’s). This is the sense of the term as now used by an expert when
discussing formalized logic or mathematics. We might call this second group
“deductive foundations.” The deductive foundations are the abstract starting
points, few in number, from which a systematic thinker then deduces a great
variety of theorems. Euclid’s axioms (including, for example, the parallel
postulate) would be foundations in this second sense; they would be part of a
system’s deductive foundations, and so would the axioms of any other branch of
mathematics or logic. On reflection, then, what Russell’s paradox actually

showed was that the two sorts of foundations needn’t be the same.
The difference is particularly conspicuous in a system like Frege’s. We might
very well devise a set of axioms, postulates, definitions, and inference rules that
entails all the theorems we want yet never be quite sure whether the rules and
postulates are truly correct. If luck turns against us, we might end up devising a
set of postulates and rules that entails, in some subtle and unforeseen way, a
contradiction—which is exactly what a paradox seems to be.
Yet, if our premises entail a contradiction and if contradictions are false, then
at least one of our premises must be false. Therefore, something in our
foundations must be false. This is just what happened to Frege; Russell showed
that his foundations entailed a contradiction. As a result, Frege became unsure of
his foundations; they no longer served him as epistemological foundations—the
things he was most certain of—but only as deductive ones, meaning they were,
at best, only a set of principles that would generate the rest of his system. He had
lost that firm sense of foundational reliability for which he and Descartes had
always been striving.
In modern formal logic and mathematics, this ambiguity in the word
“foundations” is now endemic, and the result is that experts in both fields have
tended to overlook the correct logical form of their epistemological foundations
—confusing them, instead, with a declared set of deductive foundations.
For example, if we now ask a professional logician or mathematician about
what he or she calls foundations, and if we ask, “Why do you believe them?,”
we often hear an answer like, “because the foundations explain so much else” or
“because so much else would have to be mistaken if the foundations were
incorrect.” The foundations being referred to here are the deductive ones,
meaning the abstract starting points of a formal system. The experts can often go
on to prove the internal consistency of these deductive foundations, but as soon
as we ask, “Why should the set of foundations you prefer be relied on in the first
place?” the answer we typically hear starts with the word “because.” Yet the
moment we hear this word “because,” we ought to realize that the foundations in
question can’t be functioning as epistemological foundations. Why not? Because
the logician or mathematician is defending them with reasons—indicated by the
word “because.” “Because” is a so-called premise indicator, meaning it serves to
introduce a reason for believing something. And an epistemological foundation,
by definition, is the last in a chain of reasons, not something followed by a
further “because.” An epistemological foundation is a premise for which we
offer no further premises; it is something we feel we can safely take for granted.
In consequence, then, as soon as a mathematician says “because,” somethingof

basis. The true epistemological foundations, if any, will have to come after this
word “because.”
(Notice also that, whereas deductive foundations are explicit parts of a formal
system, epistemological foundations might vary from person to person,
depending on individual experience and conviction. For example, one
mathematician might believe in a particular axiom or postulate, like Euclid’s else
must be playing the real role parallel postulate, because without it there is no
proof of theorems A, B, and C. Another mathematician might favor the same
axiom but for a different reason—because, without it, there is no proof of the
additional theorems D, E, and F. Both mathematicians thus defend the axiom,
and they use the word “because,” but they offer different reasons after the word
“because.” As a result, they both defend the same deductive foundation, the
axiom, but they base their arguments for doing so on different epistemological
foundations, on different reasons after the word “because.” Both mathematicians
in this example might also seem to be reasoning in a circle, but this is the very
thing about which the experts got mixed up. In fact, the reasoning here needn’t
be circular at all, any more than the jury’s reasoning was circular; assumptions to
the contrary, we believe, rest on a logical mistake.)
This is just the point where many logicians after Frege became bewildered.
They never really inquired into the precise structure of this further
argumentation. They failed to ask, “What is the precise logical form of this
deeper, epistemological foundation, the reasoning that comes after the word
‘because’?” One might just as easily have failed to ask, “What is the precise
logical form of a jury’s reasoning when it rejects the alibi of a defendant?” This
is reasoning that comes after a jury says “because.” (The jury’s reasoning, its
epistemological foundation, is the premise, “because the witnesses can’t all be
lying.”) In both cases, this further reasoning has a complicated form, but
logicians never asked what the form was. And the result of this neglect was that
many logicians then came to believe that logic as a whole must be somehow
circular. (As it turns out, many scholars now working in the philosophy of logic
still adhere to a defense of circular reasoning that was first advanced by the
Harvard philosopher Nelson Goodman in 1953, but, in our view, this defense has
always been fallacious.)27 In fact, confusion on this point is now widespread
outside logic and mathematics, at least in academia, because of Kuhn’s
interesting book on science—which makes an analogous mistake.
THE OUTLOOK OF THOMAS KUHN

Kuhn’s Structure of Scientific Revolutions was important in several ways. This
was the first book to popularize the term “paradigm” in the sense of a theory or
outlook. (Kuhn later had misgivings about the term because he found it being
used in many different ways, often ambiguously.)28 The key point, however, is
that Kuhn defends circular reasoning in his eighth, ninth, and tenth chapters on
the grounds that it is scientifically unavoidable, and his defense (in our view) is
fallacious. Nevertheless, his defense is still widely repeated, and it happens to
mirror a good many arguments still made by professional logicians to the effect
that logic as a whole must be somehow circular.
Kuhn’s defense goes like this: Scientists believe in physical theories that best
fit the available data, but they also reject data that are inconsistent with well-
established theories. Thus scientists take neither as a starting point; instead, they
reason in two directions at once. They believe in theories because of data and
data because of theories, and consequently each becomes a premise for the other.
In fact, says Kuhn, a scientist’s particular observations presuppose the theory.
The scientist takes neither as a given. Rather, the scientist seeks mutual
agreement, and so the scientist’s methods are necessarily circular. (Where we use
the word “theory” in our account of Kuhn’s argument, Kuhn uses the word
“paradigm.”)29
The mistake here is a point we shall return to in a moment, but the similarity
to developments in modern logic was no accident. On the contrary, Kuhn was
intimately familiar with the implications of non-Euclidean geometry, and he had
absorbed much of the thinking that had already grown out of formalized logic
and mathematics; all the same, what gave special force to his outlook was that so
much of his view, his mistake notwithstanding, was dead right.
All else aside, the great merit of Kuhn’s book was to stress the fundamental
rationality of alternative scientific theories, especially historical ones, and this
was no small matter. Hardly anyone, for example, now thinks Newtonian
mechanics to have been fundamentally irrational merely because it was
superseded by Einstein’s theory of relativity. Yet many people have indeed
supposed that medieval science must have been irrational. And this is just the
sort of assumption that Kuhn attacked. Kuhn argued that though medieval
science certainly did fail to explain a great many seemingly contradictory
phenomena, the same is true of nearly all scientific theories. There will almost
always be some alleged data that seem to contradict today’s accepted theories
(Kuhn calls these data anomalies), but this is only because there is rarely an
ironclad guarantee that the data in question have been properly understood.
Experiments can be botched, findings can be misreported, and faulty

assumptions can be made when the data are interpreted. (Sometimes we even fail
to perceive contradictory data altogether because we don’t know how to look for
them; observations often depend on experience, training, and knowing what to
expect.)
In consequence, the mere appearance of contradictory data isn’t always, in
itself, sufficient reason to reject a scientific theory. As Kuhn puts it, “No
paradigm that provides a basis for scientific research ever completely resolves
all its problems.”30 Instead, theories are often in competition. We get rival
explanations for the same events, and the choice between them is often a matter
of assessing many factors. As a result, if scientists disagree, the reason isn’t
always that some are reasonable and others foolish; rather, all might be
reasonable. The decision might simply be a tough call. (More generally, one of
Descartes’s underlying points was that not all disagreements are rational, but one
of Kuhn’s underlying points was that not all disagreements are irrational.
Instead, there can be different kinds of disagreements: some with fools and
fanatics, others with reasonable opponents. Descartes’s view derives from the
disagreements of his time, an age of fanaticism, but Kuhn’s outlook comes from
a different time.) On all these points, we suggest, Kuhn was correct. But the
trouble was his next step, one that has nothing to do with the history of science
and is strictly a matter of logic.
KUHN’S ERROR
In arguing that competing scientific theories are circular, Kuhn assumes that the
data of scientists are premises. This, in our view, is a blunder. If a scientist’s data
were premises, this would mean the scientist was assuming that all the data were
true. Yet Kuhn insists data are often suspect and that an experienced scientist
knows this. (If a scientist were assuming that all the data were true, then any
theory that contradicted the data would have to be false, and yet Kuhn says,
“Anomalous experiences may not be identified with falsifying ones; I doubt
whether the latter exist.”)31 Instead, an experienced scientist treats data with
caution. This is why scientists seek to accumulate an ever larger stock of
findings that will contradict one theory yet conform to another; they try to
multiply crucial experiments. Scientists are reluctant to trust any one datum
alone, just as a jury might be reluctant to trust the testimony of only one witness.
But in that case, the premise being offered by such a scientist must have a
different form, and one can probably guess this form already. It is, once more, a
disjunction. We can see this form better if we return for a moment to the

situation in formal logic or mathematics.
Remember that when a mathematician or logician is asked why he or she
believes in a particular set of deductive foundations the answer is often because
so much else would have to be mistaken if the foundations were incorrect. The
thing to notice at once is how very like a scientist’s assertion this explanation is:
“Because so much else would have to be false . . .” When a scientist rejects one
rival hypothesis in favor of another, the scientist typically argues that too many
contradictory data, too many anomalies, would have to be false for the rival
hypothesis to be true. Or the scientist might argue that too many other well-
established hypotheses would have to be sacrificed, or too many ad hoc
explanations would have to be postulated to save the rival theory from
contradiction. Another possibility is that the scientist might argue that too many
correct predictions would have to be dismissed as mere coincidence if the
preferred theory were untrue. Some such assertion lies at the bottom of the
complaint that the scientist would have to give up “too much.” But the key
expression is always “too many” (or “too much”), and in that case we are
looking again at a compound proposition—a disjunction—and we are not
looking at a series of premises.
The reasoning is just like that of the jury. The premise in each such inference
doesn’t take the form “All these other points—whether data or theorems or other
theories or successful predictions—are true,” but rather, “Not all these other
things are false.” The premise is that at least one of them is true.
To specify Kuhn’s mistake, the data in question aren’t premises but parts of a
premise, and the premise has the form,
At least A or B or C or D . . .
It does not have the form,
A and B and C and D . . .
The point we are making is certainly delicate (a genuine case of logical
hairsplitting), but, so far as circular reasoning goes, it makes all the difference in
the world. To put the problem in another way, Kuhn has pointed out an interplay
between data and theories in science (like the interplay between postulates and
theorems in logic or mathematics), but he has then leaped to an altogether
different idea: this interplay must be an interplay between conclusions and
premises rather than between conclusions and parts of a premise. But once we
draw this distinction (between whole premises and parts of a premise), the

appearance of circularity—even in physical science—goes away. Let’s see if this
is right.
COMPETITION BETWEEN SCIENTIFIC
THEORIES
One way to illustrate the distinction is to return for a moment to the situation
described by Kuhn—where a scientist chooses one rival theory over another—
and to sketch out the process with a diagram. Scientists make such choices often,
and though the process seems complicated, with a little ingenuity we can puzzle
it out.
Scientists often arrive at a preferred theory by elimination. They tend to favor
the theory that best fits the available data, but they also look for other features
like simplicity and explanatory power. Nevertheless, when they try to make the
data “fit,” it often turns out that the theory they prefer has the consequence of
implying that some of the data they have just cited are, in fact, false—
presumably because of errors of one sort or another. As Kuhn describes this
effect, the data thought to be false remain anomalies of the preferred theory. All
the same, if the scientists then discount some of their own data as incompatible
with their preferred theory, their reasoning isn’t circular, nor do their initial
observations presuppose the theory. Instead, they argue that rival theories offer
less of a fit than their preferred theory, and the premise that establishes this point
is a large disjunction. The premise is a list of all the alleged findings the rival
theories can’t explain and that supposedly can’t all be false.

FIGURE 6.1.
The reasoning looks like what we see in figure 6.1 (where A, B, C, and so on
represent the findings that don’t fit the rival theories and that can’t all be false,
though some might be).
To be sure, the situation is complicated, but it works as follows: The first line
represents the list of data the scientists say can’t all be false. (It is the truth of at
least one of these data that implies that the rival hypotheses must be untrue.) If
theories are in competition, however, then this first line will have the logical
effect of lending support to the scientists’ preferred theory—on the assumption
that the preferred theory does indeed give a better account of these data than its
rivals. As a result, what we see so far is a scientist invoking all the data that the
opposing theories can’t accommodate (in the first line) and using these data as
parts of an overarching reason for favoring the preferred theory. Yet the scientist

also says more.
The scientist’s preferred theory can still have a further effect: it can imply that
some of the data we have just looked at, some elements of the initial disjunction,
are false. This is because few theories fit all the data, and these presumably false
data are some of the ones Kuhn then characterizes as the preferred theory’s
anomalies. We see the scientist making this further point in the last line of the
schema, casting doubt on these anomalous data, when the scientist infers “Not C
and not D.” (This further move is still compatible with the initial disjunction and
involves no contradiction because the disjunction only asserts that at least one of
the data is true.) Thus the scientist discounts the anomalous data (by asserting
“Not C and not D”) and then takes up the further task of trying to show how
these anomalous data might indeed be false. The remaining anomalies become
the focus of a new research program.
Now, in all these manipulations we have certainly seen an interplay between
accepted data and accepted theories, which is just the point Kuhn stresses, and
we have seen how one helps to shape the other. (This mutual shaping is
sometimes described as “coherentism,” and, in moral philosophy, it is sometimes
called “reflective equilibrium.”)32 If, however, the little schema we have offered
is correct, then the one thing we have not seen is circular reasoning, not for a
moment. Inspect the schema again, and you will see that circular reasoning has
nothing to do with it. By contrast, circular reasoning would look like figure 6.2,
or it might look like figure 6.3.
These patterns are superficially alike, but figure 6.1 is different from these last
two, and though the difference is subtle, it is logically crucial. Look carefully,
and you will see that the first structure never violates Aristotle’s assumption that
a rational demonstration always proceeds from what is initially more convincing.
The scientist begins with a large disjunction and ultimately eliminates some of
its parts, but there is no necessary contradiction in this process and no circle.
(The initial disjunction can still be true and “better known” because the
disjunction remains true so long as only some of its parts are false.) And in terms
of logical form, it is exactly this process that occurs whenever a scientist rejects
a proposition on the grounds that it contradicts too many other well-established
theories—or again, when a logician or mathematician argues that giving up a
postulate would require giving up too many theorems. In each instance, the
reason offered (a disjunction) is still “better known” than the conclusion drawn
from it. Like the jury, the scientist is still relying on a foundational premise—a
premise to the effect that not all the elements of the disjunction are false.

FIGURE 6.2.

FIGURE 6.3.
(Notice also that the disjunction can rest on a still-deeper foundation of
observational reports that describe the particular circumstances under which
each datum was collected. Nevertheless, the reason for invoking the disjunction
in the first place is that such data, considered individually, may still be
insufficiently reliable. And the usual reason for doubt is that the truth of any one
datum is, at best, only a matter of probable inference, not logical necessity. Our
physical observations can show the data to be probably correct, but this doesn’t
make them necessarily correct. The strategy of the disjunction, then, is to link
these data into a compound proposition that has a greater probability than any of
its parts. By analogy, the witnesses who contradict an alibi might all look
truthful, but the appearance of honesty doesn’t necessarily entail the reality. As a
result, a cautious jury asks whether there are many such witnesses and whether it

is likely that all are lying. The truthfulness of any one witness is less probable
than the probability that at least one is truthful. This same strategy also applies
when a mathematician clings to an axiom on the grounds it is unlikely that all
the theorems dependent on it are mistaken; the theorems follow as matters of
logical necessity, but the belief in the axiom itself rests on a probable inference.)
In making these points (admittedly delicate), we don’t mean to suggest that
working scientists say things like, “I’m advancing my data in the form of a
disjunction.” In fact, there is no need to fall back on a disjunction at all unless
some of the data are suspect, and even if a scientist does fall back on it, the
scientist may do so without actually thinking of it this way or calling it by the
name a logician would call it. Neither would we deny that scientists sometimes
argue irrationally. (On occasion, even scientists can be silly, like the rest of us.
Kuhn adds that scientists will sometimes describe data in ways that rival
scientists find at least partly objectionable or “incommensurable,” and we don’t
deny this either. So long as only some parts of a disjunction are objectionable,
the other parts can still prove a point; we sometimes accept only part of what a
person says and yet still find that part to be a compelling reason for accepting a
conclusion.)33 Instead, all we assert here is that normal scientific argument can
be accounted for in ways that have nothing to do with circularity. The alleged
necessity of circular reasoning is unfounded, and there has never been anything
in ordinary scientific practice to justify it.34
Back in the seventeenth century, Descartes had soaring hopes for the future of
science—and soaring hopes for the future of mathematics too. The success of
such disciplines in the years since his time has always depended on the
reliability of their foundations, though the foundations obviously differ from the
single, peculiar postulate (“I think, therefore, I am”) that Descartes had
imagined.
In the terrible, final years of the wars of religion, fanatical arguments and
circular arguments were everywhere, and Descartes witnessed the resulting
violence firsthand. He wrote later that, after a series of dreams he had
experienced while serving briefly as a soldier in 1619, he had come to a new
view of things and had envisioned his future philosophical system. He also
arrived at the discovery of analytic geometry (which uses his “Cartesian
coordinates” along the x, y, and z axes to turn geometrical shapes into equations
of numbers—an innovation for which the world of mathematics remains ever in
his debt). And taking mathematics as his model, Descartes wondered whether

“all things knowable to men might not fall into a similar logical sequence.”35 But
whatever the merits of the system he then devised, he was motivated from the
start by a stubborn conviction that seeking out reliable foundations is part of
what makes us reasonable in the first place and that, without this effort, even the
most humane and reasonable society still risks a brutal descent into chaos.

7
WILL THE FUTURE RESEMBLE THE PAST?
Inductive Logic and Scientific Method
THE SEVENTEENTH century was an age of turmoil and blood, but it was also a
time of rising trade, growing cities, insurgent middle classes—and tremendous
scientific advance.
The period opened with the extraordinary assertions of the priest Giordano
Bruno, who said stars were suns and the universe was infinite; he was burned at
the stake by the Roman Inquisition in 1600.1 By 1609, Johannes Kepler in
Germany had determined that the planets traveled around the sun in elliptical
orbits. Kepler escaped the violence of his time, but his mother was imprisoned
for thirteen months on a charge of witchcraft, and though Kepler finally secured
her release, she survived the ordeal by only two months. In 1638, Galileo
published a book explaining the laws that govern the motion of falling bodies;
Galileo had the book printed in Holland but was unable to publish in Italy, being
already confined to house arrest in Florence for the rest of his life by the Roman
Inquisition for arguing that the earth moves.2 Still later, in 1687, Isaac Newton,
flourishing in relative safety in England, showed that Kepler’s laws of planetary
motion and Galileo’s laws of falling bodies were special cases of a more general
set of principles for the whole physical universe, assuming that a universal force
—gravity—emanated from all matter to attract all other matter.3
These insights made people think of the universe as rational and scientifically
comprehensible, but they also made people think about reasoning, especially
when contrasting the discoveries of seventeenth-century science with the earlier
hysteria and frenzy of the wars of religion. The great discoveries of seventeenth-
century physics define what we now call the Scientific Revolution, but they were
also part of a larger revolt: of Europe’s commercial middle classes against the
doctrines and restrictions of the Middle Ages. In logic, the result of this revolt
was the rise of induction and the study of scientific method.
Those involved in trade in European societies had become far more numerous,
and the effect of this change was to transform nearly every aspect of modern
intellectual life. The newly expanding commercial classes generated a vast new
audience for literature—literature based in vernacular languages rather than

Latin—and the growth of this audience encouraged new political theories, new
sciences (like those of Galileo and Newton), new literary forms (such as the
novel), and a new spirit of reformist agitation (which eventually gave rise to the
eighteenth-century Enlightenment).
The upshot was a general attack on the received learning of the past, and in
logic the consequence was a tendency to discount deduction as the logic of
authority and to elevate induction as the logic of fresh experience and novel
experiment. In a changing world, probable inductions from new experience
became the crucial method of argument, displacing the older method of
necessary deductions from an ancient philosopher. These newly rising middle
classes were far more disposed than other groups to challenge tradition and
assert their individualism, and the logic of induction and scientific method
served this independence.
In addition, the involvement of these same groups in an expanding world of
commercial innovation forced them to be ever more tolerant of dissident
opinions—religious, scientific, political, and logical—because, without this
tolerance, their commerce was impossible to sustain. Persecutions and fanatical
intolerance turned out to be bad for business. All these forces made early modern
writers more interested in the logical role of physical experience in the forming
of our beliefs, and the result, continuing into the present, was a detailed
investigation of empirical science and experimental method.
THE CHALLENGE OF THE NEW LITERATURE
The origins of these developments lay in the growth of Renaissance trade, which
undermined medieval authority and encouraged new kinds of literature. During
the Middle Ages, the audience for most new writings had been priestly or
aristocratic; medieval books and manuscripts had been the creations of cloistered
churchmen working in Latin, or they were the inventions of bards and
troubadours performing for aristocratic elites. From the Renaissance onward,
however, new works of literature were increasingly traded for cash in the
marketplace, and they increasingly appealed to people who fell outside the
traditional medieval orders of churchmen, warlords, and illiterate peasants; they
appealed to a mixed crowd of artists, artisans, lawyers, doctors, tutors,
shopkeepers, and small capitalists. These were the people most impatient with
traditional authority and most likely to see themselves as self-made and self-
reliant. They prized their independence, and they were also the people most
likely, for reasons of financial gain, to become literate. They were the rising

middle classes of the towns and cities, and their appetite for new works gave rise
to new sorts of writing in fiction, religion, politics, and science. (The term
“middle class” is often used to mean those who stand between an industrial
working class on the one hand and the wealthiest social strata on the other. But
we use the term here in a different way, to mean those who stand between an
ancient feudal peasantry and an ancient hereditary nobility.)4
Small groups of such people had always existed in the urban centers of
medieval Europe, but never in such numbers. They first began to multiply in the
cities of Italy during the thirteenth century in the wake of the Crusades (from
1096 to 1291), which, in seeking to recapture the Holy Land for Christendom,
had stimulated Italy’s Mediterranean sea trade. By the 1500s, however, these
same ranks had also started to prosper along Europe’s Atlantic coast, and now
they became even more numerous.
This transformation didn’t happen overnight. Earlier, in the 1400s, the
lucrative trade in spices had induced European mariners to develop new methods
of navigation, and these explorers’ success in crossing the world’s oceans had
led, in turn, to a much larger and more ruthless trade in overseas gold, silver,
farm products, and slaves. As more trade shifted away from the Mediterranean
and traveled along the Atlantic sea routes, Europe’s wealth and power gravitated
toward its Atlantic rim, and Italy went into decline. The lure of large, new profits
from across the oceans drew the Atlantic states of Portugal, Spain, the
Netherlands, France, and England into a deadly struggle to shut one another out
and to monopolize the sea lanes. Eventually, these states would construct
immense overseas colonial empires and impose European rule on distant corners
of the earth. At the same time, however, their expanding trade profoundly
changed Europe’s internal social structure.
Within Europe itself, this new, growing sea trade generated much larger
commercial communities, where livelihoods depended less on hereditary
privilege and more on the circulation of money. These same middle-class groups
had already supplied much of the talent behind Renaissance art and literature
(many writers and most sculptors and painters of the Renaissance had come from
the middle classes, though they still depended heavily on aristocratic patronage).
By the seventeenth century, however, because these same middle classes had
become so numerous, they also supplied something more: an expansive new
audience. Crowded in a great port like London, they gave Shakespeare a hearing
for his plays, and plying their trades in the commercial cities of France, the
Netherlands, Spain, and lower Germany, they paid cash for the works of
Rabelais, Montaigne, Cervantes, Galileo, and Descartes. They were gradually

becoming the new patrons for new kinds of literature.
In some ways, these new urban crowds of early modern Europe mirrored the
ancient crowds of the Greek popular assemblies. The difference was that the
new, European audiences were rarely located in a single mass meeting like the
Athenian Assembly; they typically consisted instead of independent readers,
sometimes separated by hundreds of miles but still tied together by the printing
press. All the same, their expansion also depended on another crucial innovation:
the triumph of vernacular languages over Latin.
THE TRIUMPH OF THE VERNACULAR AND THE
GROWING SPIRIT OF EQUALITY
The rise of these new middle-class audiences required that writers move away
from the Latin of the past and develop a new literature in the everyday languages
of Italian, French, Spanish, English, German, and so on. This, too, would deeply
affect logic because it would bring about a revolution in intellectual life by
changing the rules as to who could participate.
While the Renaissance was still young, the great humanists Thomas More and
Desiderius Erasmus had composed their major works in Latin and had addressed
them to an intellectual elite. By the seventeenth century, however, many writers
worked routinely in vernacular languages, and they appealed to a broader,
middle-class readership. Though Descartes published his Meditations in Latin in
1641 to defend his international reputation, his earlier and brasher Discourse on
Method (1637) had been penned in French and had mocked the intellectual
authorities of the age. Galileo had written his witty and sarcastic Dialogue
Concerning the Two Chief World Systems (1632) in Italian, and Cervantes had
delivered his satirical Don Quixote (1605) in Spanish. Eventually, political
doctrines would appear in vernacular languages too. Machiavelli had already
written The Prince in Italian in 1513 for the private reading of a Medici ruler,
and Thomas Hobbes chose English for his Leviathan, published in 1651. By the
time John Locke brought out his Two Treatises of Government, dated 1690,
English was the natural choice.
This transition from Latin to the languages of ordinary people would
eventually make possible the creation of novels (the novel being a “new” form
of storytelling that used vernacular languages and that aimed at a mass, middle-
class readership to cover the costs of printing). In logic, the tendency toward the
vernacular would give rise to the Port Royal Logic of Antoine Arnauld and

Pierre Nicole (a popular and influential handbook published in French in 1662
that mixed logic and the theory of knowledge and contained many passages
much influenced by Descartes).5 But no matter what the genre, vernacular
writing was the essential first step in fashioning a new readership.
The first person to foresee this new role for vernacular writing had been the
Protestant leader Martin Luther. Back in the 1520s, Luther had translated the
Bible into German. He was the first widely circulated author to realize that
though the printing press had opened the possibility of a new, mass readership,
the books it produced would need to appear in a language the mass of readers
could understand. His translation of the Bible was quickly imitated by other
translators working in other languages. The circulation of these translated Bibles
then became the model for the publication of other vernacular books, and the
Bibles also represented a unique experiment in human history—an experiment
that seventeenth century thinkers often looked back on with dread.
The Protestant Reformation had turned out to be something entirely new in the
history of the world: never before had tens of thousands of people been invited
to read the same text (the Bible) and to interpret this text by the lights of their
own common sense. (The works of earlier vernacular writers like Dante and
Chaucer had circulated in much smaller quantities and in hand-copied form.)
And the consequence of the experiment was also new. The result, as shocking as
it was unexpected, was that thousands of these same readers then proceeded to
murder one another over which interpretation was correct.
The wars of religion were the first dismal outcome of this new attempt at mass
enlightenment, and the fear that such violence might somehow return haunted
the public discussion of ideas long afterward. As a result, the seventeenth
century, following in the grim wake of the sixteenth, represented the first time
that large numbers of thoughtful intellectuals tried to work out how this new
form of public edification, by way of mass literacy, might be managed—without
setting off massacres.
One effect was to make many seventeenth-century writers profoundly afraid
of irrational enthusiasm. They emphasized Reason (capitalized) in a way that
later generations sometimes found difficult to understand. Later, especially in the
nineteenth century, proponents of European colonialism would argue that
European peoples were somehow more rational and reasonable than the peoples
they had subjugated overseas, and similar arguments had long been used to
justify turning non-Europeans into slaves. Nevertheless, seventeenth-century
thinkers were surprisingly egalitarian. Seventeenth-century philosophers
typically argued that reason was the common property of all humankind. (The

first line of Descartes’s Discourse, for example, insists that no human attribute is
so equally distributed as “good sense.” It was also orthodox for the period for
Locke to assert that his “law of nature,” justifying his principles of government
and property and evident to reason, was just as clear to the Indian nations of
North America as it was to the people of England.)6
In place of a theory of inherited superiority, the seventeenth-century
philosophers tended to stress the universality of reason, and this emphasis served
the middle classes in one of the most profound struggles of the period: their
sustained and pervasive campaign to win greater powers from Europe’s
hereditary aristocracies. Middle-class readers were especially sympathetic
toward any argument that undermined hereditary privilege, and many of these
readers, in consequence, ended up embracing the theory that all classes
(including themselves) could reason equally well. The effect of this tendency on
logic was to stress the significance of everyday reasoning (which was often
inductive, experiential, and middle class) and to play down the importance of the
reasonings of priests and monarchists (which typically consisted of necessary
deductions from the opinions of ancient authors). This theme of the equality of
reason was especially evident in the rise of modern political theory.
THE RISE OF MODERN POLITICAL THEORY
Until modern times, monarchy and hereditary aristocracy were the unquestioned
assumptions of all known political theories in all parts of the world, except
where trade had so inflated the middle classes that they could partly escape the
control of kings and nobles. The special effect of trade on politics had already
been conspicuous in classical Greece, where small states and an acceleration of
seafaring had given the commercial classes the upper hand. This same effect had
also appeared, to a lesser extent, in republican Rome, which, in its early days,
was no larger than a city-state and which had allowed Greek political theories to
circulate among the urban plebeians. (Democratic tendencies in Roman politics,
founded on Greek models, had persisted among the plebeians until the late
Republic. Eventually, the state’s military conquests so enlarged Rome’s
dominions that the commercial classes were submerged in a much larger
population of peasants, slaves, and unemployed paupers, who lived on free grain
from the state. The idea of rule by majority vote in a single mass meeting
became increasingly impractical, and Rome finally fell back on government by a
succession of military strongmen—the emperors.)
In the rest of the world, however, trade was so limited and states were so large

that the commercial classes could never break free. Monarchy and hereditary
privilege were the unchallenged assumptions of classical China, and they were
likewise the unchallenged assumptions of ancient Egypt, ancient Indian
civilization along the Ganges, and ancient Mesopotamia. Individual kings might
be overthrown, and particular groups of nobles might revolt, but the theory and
literature of these regions always assumed kingship. The only real question was
whether a particular king happened to be good or bad—or whether he enjoyed
(in the Chinese phrase) the mandate of Heaven. Many ancient peoples also lived
as isolated villagers or nomads, of course, but these ancient pastoral
communities had no political literature at all. (The political theory of the ancient
Indus civilization, which flourished in the third millennium B.C. in what is now
mostly Pakistan, is still unknown to us, its language being still undecipherable.)
As a result, as far as our knowledge of urban civilization is concerned, the
political history of other regions of the world was largely dynastic. The merchant
classes of these other societies were so outnumbered by armies under the control
of hereditary elites that the very idea of alternative political systems was
unthinkable. And this same predilection for monarchy and hereditary privilege
also returned to the Mediterranean region once Roman power had collapsed with
the onset of the Middle Ages. An Islamic medieval writer like al-Farabi, for
example, or a Christian one like Thomas Aquinas, might consider the democracy
of ancient Athens after reading about it in a book, but he would then dismiss the
idea of such government for his own time as obviously misguided.7
All this began to change once the Mediterranean trade of Italy had revived
after the Crusades and once a similar but larger change had occurred along the
Atlantic coast of Europe. Radical political theories expanded in many new
directions. Machiavelli, Thomas More, Hobbes, and Locke all differed
dramatically in their ideas of what was politically desirable and politically
possible, but they were alike in coming out of the middle classes and in rejecting
the medieval notion that some castes were born to rule and others born to obey.
They remained acutely conscious of hereditary elites, but they saw the power of
these elites as founded on tradition, not inherited superiority.8
THE RIGHT OF DISSENT AND THE RELIANCE ON
INDUCTION
As the composition of European society, especially in the cities, became more
commercial, the freedom to publish and defend new doctrines grew wider. Many

middle-class readers came to believe in the new political idea that ordinary
people could reason just as well as aristocrats. As a result, new books challenged
traditional dogmas in many fields—in science and logic, no less than in politics.
The wars of religion had already given a stark demonstration of where fanatical
intolerance could lead, but after those wars the general trend was toward
increasing tolerance for other points of view. This tolerance was most
conspicuous in exactly those places where the middle classes were most
ascendant: commercial centers like seventeenth-century Amsterdam and
seventeenth-century London.
During this period, the cities of Holland became famous as refuges for
dissident intellectuals, among them Descartes, Locke, and Baruch Spinoza. Even
Galileo, who was already confined to house arrest in Florence for the rest of his
life, nevertheless sent his Dialogues Concerning Two New Sciences to Holland
for publication by a Dutch printer.
London, though politically less stable than many Dutch cities of the time, was
still increasingly hospitable to various forms of civil dissent. Even in 1644, in
the midst of the English Civil War, John Milton could publish his famous
“Areopagitica,” an essay that attacked book burning and defended freedom of
expression. (“Who kills a man, kills a reasonable creature, God’s image; but he
who destroys a good book kills reason itself.” And, “Give me the liberty to
know, to utter, and to argue freely according to conscience, above all liberties.”)9
Somewhat later, in 1689, Locke published his first Letter Concerning Toleration,
which argued against religious persecution (though it made strange exceptions in
the cases of Catholics and atheists; Locke excepted atheists on the supposed
grounds that they couldn’t be trusted to keep promises or oaths, as they had no
fear of divine retribution, and he excepted Catholics on the theory that they
served a foreign “prince,” the pope).
Seventeenth-century tolerance was, of course, much narrower than tolerance
in many places today, and all sorts of draconian restrictions appeared (and
continue to appear) in times of unrest, disorder, paranoia, and rebellion. But
tolerance in the seventeenth century was still far greater than in the medieval
past, and though it emerged first as a right of religious dissent, it soon evolved
into additional rights of political and scientific dissent. Despite various setbacks,
the doctrine of tolerance—religious, political, and scientific—continued to
spread, and it culminated much later in the work of another middle-class author,
John Stuart Mill, who published On Liberty in 1859. (Mill wrote, “The beliefs
which we have most warrant for have no safeguard to rest on but a standing
invitation to the whole world to prove them unfounded.”)10 On the whole, the

toleration of dissent and the rise of the commercial middle classes went hand in
hand, and for reasons that are not hard to see.
Commerce brought the middle classes into daily contact with new ideas and
new people, often from foreign lands. Just as important, competition rewarded
those who were curious. The money economy forced its participants to observe
new situations that came about because of changing markets, and it made them
collect new information so as to predict what would sell and what wouldn’t.
These marketplace effects tended to make the middle classes chafe at restrictions
on their freedom of enquiry, and they also encouraged the middle classes to
become increasingly analytical and increasingly disputatious.
In a competitive environment, to succeed, they often had to think for
themselves, and this thinking made it necessary to consider the pros and cons of
competing opinions. Mill would later remark on this need to consider pros and
cons, and he would contrast it with the sort of thinking that goes on in the
deductive proof of a mathematical theorem:
The peculiarity of the evidence of mathematical truths is that all the argument is one side. . . . [But]
even in natural philosophy [meaning science], there is always some other explanation possible of the
same facts; some geocentric theory instead of heliocentric, some phlogiston instead of oxygen. . . .
When we turn to subjects infinitely more complicated, to morals, religion, politics, social relations,
and the business of life, three-fourths of the arguments for every disputed opinion consist in
dispelling the appearances which favor some opinion different from it. . . . He who knows only his
own side of the case knows little of that.
Mill stressed the importance of grasping opposing arguments, not simply one’s
own. He added,
In the case of any person whose judgment is really deserving of confidence, how has it become so?
Because he has kept his mind open to criticism of his opinions and conduct. Because it has been his
practice to listen to all that could be said against him; to profit by as much of it as was just, and to
expound to himself, and upon occasion to others, the fallacy of the fallacious. . . . No wise man ever
acquired his wisdom in any mode but this.11
In making these remarks, Mill is actually commenting on what logicians call
induction. Mill took mathematical theorems to be deductive, but the “business of
life” he invoked is typically inductive. And the need to keep listening comes
from the fact that inductive arguments never supply conclusive proof; at best,
they offer a high probability (given the available evidence).
The possibility of alternative explanations was a daily occurrence in middle-
class life, and the successful entrepreneur was usually the one who, in spite of
all, could still make a series of probable predictions inductively, on the basis of

personal observation and experience. The successful investor had to predict the
market, and the middle-class artisan needed to anticipate what he could sell. As a
result, the middle classes eventually came to oppose many restrictions on their
ability to investigate and analyze, except when plainly necessary to keep the
peace. (Admittedly, Mill makes these points some two centuries after the
formative period we now mean to discuss, but his comments concern the nature
of induction itself—and he is right.)
In addition, all other considerations aside, religious and ideological
persecutions turned out to be bad for business. The trouble with persecutions, as
far as investment was concerned, was that they increased the chances that your
commercial partners would be arrested arbitrarily and made to disappear. In the
modern world, we sometimes think of the right of dissent as a fundamentally
“Western” notion, as if it were merely an arbitrary cultural preference like
language or dress, but this is a mistake. Instead, the idea of the right of dissent, if
not already latent in all people who wish to know, is an especially commercial
idea because it tends to expand with economic competition and with the growth
of the middle classes who engage in it. Business competition forces people of
differing opinions to interact, and it rewards those who can still trade, invest, and
make deals in a dissonant environment. In the seventeenth century, the right of
dissent first emerged in Europe’s Atlantic states because these were the places
where ocean-going commerce first accelerated. Nevertheless, as economic
competition has continued to swell the ranks of the middle classes in developing
countries, so the right of dissent has also expanded there.12
INDUCTION AS THE NEW RATIONALITY
Though the seventeenth century prized rationality, the rationality in question
wasn’t the rationality of logically necessary deductions from a received set of
timeless truths. It had nothing to do with the deductions of priests or
monarchists, who might seek to establish the divine right of kings or the doctrine
of papal supremacy. Instead, in a world of increasing commercial innovation, the
most prized of all rationality consisted in drawing probable inferences from new
data—data that concerned evolving and changing circumstances. It was the
rationality of a different social group, the middle classes, and this meant it was
the rationality of induction.
Induction, by definition, aims at proving a conclusion as a matter of
probability or likeliness, not logical necessity. And most inductive arguments
rely on premises that describe things we have already observed so as to draw

conclusions about things that are still unobserved.13 The premises describe things
we already know, but the conclusion makes an assertion about something new.
The seventeenth century was very much devoted to the new.
Take a simple case: We all know that the sun will rise tomorrow, but how do
we know it? Do we know daylight will reappear every twenty-four hours
because it has always done so in the past? The answer might not be as simple as
it seems. Our knowledge of the sun’s continuing behavior is surely based on an
inference, but what is the form of the inference?
If the sun has risen every day we can remember, does it follow as a logical
necessity that the sun will rise tomorrow? Even conceding past experience, isn’t
there still some sense in which it is possible that the sun will simply fail to rise
and that the laws of nature will change radically? Possible, perhaps, but not
likely.
More precisely, given past experience, we can say the sun will probably rise
and that all the laws discovered by Galileo, Kepler, and Newton will probably
continue to operate, but not that the sun must rise and that the laws must continue
to operate—at least not in the same way that, if all humans beings are mortal and
Socrates is a human being, Socrates must be mortal. The sun’s behavior (or
rather, the earth’s rotation) is perhaps physically inevitable but not logically
inevitable. No conclusion about a new case can follow as a logical necessity
from a mere assertion about an old one. In our inference about Socrates, the
relation between premises and conclusion is one of necessity, but when it comes
to tomorrow’s sunrise, we seem to be dealing with something different: a very
high probability. Moreover, this same probability attaches to any empirical law
in any physical science because all such laws apply as much to new situations as
to old ones; the laws are assumed to have operated yesterday, but they are also
expected to operate tomorrow.
From Aristotle to the present, the traditional way of marking this distinction is
to say that our inference about Socrates is a case of deduction and that our
example of the sunrise is induction. Deduction, if valid, establishes a conclusive
tie between premises and conclusion, but induction establishes a merely
probable tie. And it appears that no argument that goes exclusively from past
experience to a future prediction can be deductively valid. After all, it is one
thing to talk about the past, quite another to talk about the future. Our premises
concern the past, but the future is something new.
The ancients had long distinguished between deduction and induction, but
what they didn’t do was investigate induction, and this is where seventeenth-

century thinkers struck out on a new path. Deduction had already become
equated with authority, and so the attack on authority became an attack on the
utility of deduction itself and a plea for the elevation of induction as a rational
alternative. This was the plea to which middle-class audiences were most
receptive. All the same, deduction had become associated with authority only by
degrees, and the cause of this association had been Aristotle’s immense influence
on scholars of the Middle Ages across a thousand years. If we now add a few
words about the origins of this influence, it may help to throw light on why
seventeenth-century thinkers finally reacted against deduction so vehemently.
ARISTOTLE’S INFLUENCE ON THE MEDIEVALS
Aristotle’s ideas had dominated medieval thought because so many of his ideas
had been insightful, and this was especially so in logic. After the days of
Aristotle and Chrysippus, logic as a discipline tended to stagnate. To be sure, the
logicians who followed were often shrewd. Aristotle’s student Theophrastus
perfected the theory of categorical syllogisms and added a theory of hypothetical
syllogisms; in India, an independent tradition mixing rhetoric and logic
developed in service to the metaphysical schools. Nevertheless, the driving force
behind logical innovation had almost always been public debate, and when the
debate languished, so did logic. Logic went into decline in the absence of public
assemblies.
The decline was gradual. While Aristotle and Chrysippus still lived, the
Athenian Assembly still met, and though already stripped of military power by
Athens’s overlords, the Assembly still debated a wide range of local issues.
Likewise, in the early days of Roman rule, many Romans studied Greek
techniques for their application to Roman politics. (Athenian logic and rhetoric
were particularly useful in the class conflict between Rome’s patricians and
plebeians—the hereditary aristocrats and the common people. The patricians
controlled the Senate, where policy often depended on private counsels and
secret negotiation, but the plebeians met in a vast assembly in the Roman forum,
and many Romans wanted to expand the powers of this assembly by modeling it
on the assembly at Athens. Even Romans who resisted this tendency still saw the
need to address large crowds in the Athenian manner, and consequently the most
famous of these orators, Cicero, translated much Greek logic and rhetoric into
Latin. Nevertheless, Roman public assemblies were always more restricted in
their prerogatives than the Greek ones, and when Rome’s emperors finally came
to power, logic and rhetoric retreated into a bookish world of quiet

scholarship.)14
With the onset of the Middle Ages, public debate virtually died out; politics
fell into the hands of warlords, and logic, when studied at all, was studied only
as part of a general analysis of Greek philosophy. Among the original masters of
this medieval endeavor were Arab scholars who first served as physicians to
powerful caliphs and whose initial aim was to advance medicine. Various
schools developed, especially under the influence of al-Farabi, Abu ibn Sina
(Avicenna), and Ibn Rushd (Averroës), but the chief emphasis was on broader
questions of ethics, metaphysics, and theology. Early Arab logic, it turns out,
was inaugurated by Syrian Christians, but it was quickly appreciated by scholars
working in the tradition of the Koran. Arab scholars took special interest in
Aristotle’s treatment of inferences about possibility and necessity—the field now
called modal logic15—and they explored many related questions of epistemology
(meaning the theory of knowledge).16 All the same, Arab logicians of the Middle
Ages usually saw themselves as reconstructing and perfecting Greek logic rather
than building something fundamentally new, and when medieval Christians in
Western Europe began to imitate this Arab example, they adopted a similar
outlook. In consequence, medieval logic, like Roman logic, was essentially an
elaboration and refinement of Greek ideas. Only with the coming of the early
modern period did the influence of extensive public debate reassert itself, and
until that time, Aristotle’s authority loomed large.
The medievals had revered Aristotle because they found so much of his work
enlightening. Aristotle had not only elaborated a theory of deduction; he had also
offered a great many general principles, philosophical and scientific, and from
these principles a great many consequences could be deduced. Aristotle himself
was by no means wedded to deducing things; he was a careful, empirical
observer whose reasonings mixed both deduction and induction. Aristotle in the
hands of the medievals, however, was something else.
If you assumed, as many medievals did assume, that Aristotle was usually
right, then you usually found yourself either recommending his doctrines or
deducing further consequences from them. As a result, most of your inferences
were deductive. Moreover, many medievals did indeed assume that Aristotle was
usually right. (One sees this assumption, for example, in the tendency of a
thinker like Thomas Aquinas to call Aristotle simply “the Philosopher.”)17
Reflecting on the intellectual accomplishments of their own time, especially in
the Christian West, a place of grinding poverty and recurring chaos, many
medieval readers found Aristotle’s insights truly astonishing, and medieval
readers typically had a similar attitude toward all ancient writers. The novelist

and Oxford scholar C. S. Lewis describes this reverential attitude of medieval
intellectuals toward ancient literature:
They are indeed very credulous of books. They find it hard to believe that anything an old auctour
has said is simply untrue. And they inherit a very heterogeneous collection of books: Judaic, Pagan,
Platonic, Aristotelian, Stoical, Primitive Christian, Patristic. Or (by a different classification)
chronicles, epic poems, sermons, visions, philosophical treatises, satires. Obviously their auctours
will contradict one another. They will seem to do so even more often if you ignore the distinction of
kinds and take your science impartially from the poets and philosophers; and this the medievals very
often did in fact though they would have been well able to point out, in theory, that poets feigned. If,
under these conditions, one has also a great reluctance flatly to disbelieve anything in a book, then
here there is obviously both an urgent need and a glorious opportunity for sorting out and tidying up.
All the apparent contradictions must be harmonised.18
The medievals interpreted, systematized, and harmonized, but they also
implicitly believed, and this attitude carried over into European universities,
even down to the seventeenth century when university education was still
dominated by medievalism. In addition, however, we must also remember the
social role of medieval scholars.
The first thing to recall when we now look back at medieval scholars,
especially from Europe, is that most of them were churchmen. This meant that,
whenever they considered a new idea, they first had to ask (if only for their own
safety) whether this new idea was consistent with the church’s articles of faith.
They had to determine whether the idea was consistent with authority, and most
questions of consistency are answered deductively. That is, to determine whether
A and B are logically consistent, we usually deduce consequences from A or B
until we reach a contradiction.
For example, in determining whether the belief that the universe is less than
ten thousand years old is consistent with the assertions of modern astronomers,
we must ask ourselves what each of these beliefs or assertions logically implies
and then look to see if the result is a contradiction. (If what modern astronomers
say is true, then light has been traveling from distant stars for millions of years;
yet, if so, the universe can’t be only ten thousand years old.) This is how we
show that A and B are incompatible. But, in that case, deduction is the principal
method, and the only inductive form that typically appears is argument by
analogy. (An analogy compares one case to another, similar one; thus, to
determine how a general principle like A or B might apply to a particular case,
we might also look at how it applies to other, similar cases.) In consequence,
medieval argument was typically deductive or analogical—a pattern of thought
that we still see today in legal argument.19
None of this implies, of course, that medieval people didn’t reason inductively

in practice; quite the reverse: daily life requires countless inductive inferences,
and this has been true in all historical periods. (No one masters the art of
cooking, for example, without a good deal of inductive experiment.)
Nevertheless, what the medievals didn’t do was extensively study induction, and
this was because the arguments on which their crucial controversies depended
were usually deductive, being questions of consistency with the great authorities
of the past. The result, then, was a general tendency toward deduction in the
medieval tradition, and it was just this tendency against which the new authors
of the seventeenth century conducted their stubborn campaign.20
Remarking sarcastically in 1690 on the habit of thinking that all reasoning
must be deductive, Locke writes, “God has not been so sparing to men to make
them barely two-legged creatures, and left it to Aristotle to make them
rational.”21 Locke thinks much of rationality has nothing to do with Aristotelian
deduction. Beyond this, however, religious warfare had reinforced the dangers of
irrational enthusiasm, and this, too, encouraged the study of induction and
scientific method. With the notable exception of Mill (and perhaps also the
American pragmatist C. S. Peirce; Mill and Peirce both came of age in the
nineteenth century), nearly all the great names of induction and the philosophy
of science wrote in the wake of the wars of religion, and they often had religious
controversy fresh in their minds—such writers as Locke, Francis Bacon, Galileo,
and David Hume. To be sure, scholarly studies of induction and scientific
method have continued from the nineteenth century to the present, but it was
writers of the seventeenth and eighteenth centuries who first turned the analysis
of induction into an ideological cause.22 In effect, all these writers asked how we
know what we know, and they thought much of our reasoning was inductive. As
a result, their recurring complaint was that medieval authors had paid scant
attention to induction, and their fervent effort was to explore it.
(So far as the philosophy of science is concerned, we can also distinguish a
further period of intense interest much later in history, just after the First World
War and culminating in the “Vienna Circle” of 1920s Austria, but this, too, was
in many ways a reaction to fanaticism—the chauvinistic fanaticism that preceded
the horrors of 1914 to 1918. In English and American universities, this interest,
further provoked by the rise of Nazism in Germany, became the heart of so-
called analytic philosophy, which has always stressed logic and scientific
method.)
Even so, these historical points still leave open the more basic question of
what exactly induction is and whether or not it really is different from deduction,
as many seventeenth-century writers assumed. Like previous logicians and most

logicians today, seventeenth-century philosophers took deduction and induction
to be different. (Locke, for example, divided all arguments into “demonstrative,”
meaning deductive, and “probable,” meaning inductive.)23 All the same, the
acute observations of Hume, a Scottish philosopher writing in the eighteenth
century, led a number of later thinkers to wonder whether the two methods of
induction and deduction were truly different after all.
The answer isn’t so simple (though we do indeed find ourselves more inclined
to side with the seventeenth-century philosophers and less convinced by the later
thinkers). In trying to understand induction and scientific method, several later
writers, including Mill and the twentieth-century philosopher Bertrand Russell,
came to the view that all inductive arguments actually assume an additional,
unstated premise—what they called a “principle of induction” or a “principle of
the uniformity of nature”—the addition of which causes inductive arguments to
become deductive. They also supposed that, without this further premise, no
inductive argument would be rational in the first place. All such inferences, they
said, assume that the old cases we have observed have something in common
with the new cases about which we make predictions. Thus these writers found
themselves asking, What makes induction reasonable from the start?
THE RATIONAL FOUNDATIONS OF INDUCTION
In trying to answer this question, much depends on how we construe an
inductive argument’s form. Schematically, we might represent an argument from
past experience to a future prediction like this:
The sun rose Monday.
The sun rose again Tuesday.
The sun rose again Wednesday.
The sun rose again Thursday . . .
Therefore, the sun will probably rise tomorrow.
Given enough examples of past experience, this sort of argument certainly seems
rational, but where is the assumption that the old cases we have observed have
something in common with the new cases about which we make predictions?
Why not suppose that all inductive arguments really contain a further premise
that expresses this assumption and that the premise is supplied silently but
automatically by the mind? Why not suppose that inductive arguments actually
look like this:

The sun rose Monday.
The sun rose again Tuesday.
The sun rose again Wednesday.
The sun rose again Thursday . . .
[And the more often any event has been observed without exception in the past, the greater the
probability that, in similar circumstances, it will occur in the future.]
Therefore, the sun will probably rise tomorrow.
The premise in brackets might be conceived as the one supplied silently but
automatically by the mind, and it connects the sunrises of the past with the
probability of a sunrise tomorrow.
Given the premise in brackets, it might now seem that inductive arguments are
rational in the first place for the very reason that they are deductively valid—so
long as the missing premise is added. (Of course, the conclusion in our example
still doesn’t tell us that a sunrise is logically necessary; it characterizes the
sunrise as merely “probable.” But this characterization might itself be the thing
that is logically necessary. The characterization itself might follow deductively
from the premises—given the additional, silent assumption and some further
statements to the effect that Monday, Tuesday, Wednesday, and so on, are each
events in the past and that tomorrow is a similar circumstance that is still in the
future.) On this view, then, inductive arguments are really just a species of
deductive ones—deductions in disguise—and the distinguishing feature of this
species is that all such arguments assume an additional, tacit principle that
allows a leap from statements about the past to a probable statement about the
future.
This tacit principle is the one that later writers conceived as the principle of
induction or the principle of the uniformity of nature, and the principle can also
be phrased so as to allow any inductive inference that goes from cases we have
already observed to cases we haven’t. (The idea that some such principle must
be at work in inductive reasoning emerged with Hume; Hume asserted that all
our conclusions about the future “proceed upon the supposition that the future
will be conformable to the past.” Mill, Russell, and others then offered different
versions of the principle, but they agreed that the principle must be a tacit
premise in all our inductive reasonings, thereby making inductive arguments
actually deductive.)24 Nevertheless, in all these musings, what we are actually
trying to clarify is how our minds make sense of experience.
To see better how we make sense of experience, try a little thought
experiment:

Suppose you met a man who believed that the future would never resemble the past, not in the
slightest. From this moment forward (according to this man) every new instant will be entirely
different from the last. Now the strangeness of this belief aside, how would you go about proving to
this person that he was mistaken? How would you convince him that the past is a reasonable guide to
the future? How would you do this, that is, without already taking for granted something he denies?
What evidence would you invoke—past experience?
If you say that the past is prologue to the future, he denies it. If you say that,
in the past, the past has always been prologue to the future and therefore it must
be so in the future, he accuses you of missing the point; you are still supposing
that what comes before is somehow relevant to what comes after. This is exactly
what he denies. If you say that the past has to be relied on anyway, because it
supplies all the evidence we have, he accuses you of failing in your burden of
proof. From his point of view, you are still preaching to the converted and
assuming that relying on the past will somehow put us in a better position than
not relying on it. You are still assuming that the future will be similar to it. And
this, again, is what he disputes.
Hume stresses the difficulty of such attempts at persuasion in his Enquiry
Concerning Human Understanding, published in 1748: “If there be any
suspicion that the course of nature may change and that the past may be no rule
to the future, all experience becomes useless and can give rise to no inference or
conclusion.”25 Yet, if you yourself do indeed differ from the strange man we
have just imagined (and you’d better differ, since your very survival depends on
reasoning from experience), then what exactly is the difference? Is there some
particular proposition that you accept and he doesn’t? Alternatively, is your idea
of what constitutes reason and logic different from his? If so, how is it different?
These are exactly the questions the principle of induction purports to answer.
It purports to tell us what it is that you accept and he doesn’t. The principle
seems to do this very well but for one difficulty: the principle is glaringly false.
In fact, no rational person could ever really rely on the principle as we have just
conceived it.
Consider: is it really true that the more “any” event is repeated in the past, the
greater the probability that it will be repeated in the future? For example, if one
were now to take, say, a tennis ball and bounce it repeatedly against a wall,
would it follow that each time one made the ball rebound from the wall the
probability of a further rebound would increase? The principle in brackets, taken
literally, says yes, and according to the principle, this effect of increasing
probability would continue even if one had already decided, in advance, not to
repeat the bouncing of the ball more than ten times. The same error occurs
whenever a person who travels abroad for the first time assumes that everyone in

the world must speak his own language simply because everyone he has met in
the past spoke his own language. It also occurs when a succession of cloudless
days induces a person to think that the next day must be cloudless too.
Plainly, our judgments about what is probable and what isn’t are far more
complicated than the little schema we suggested earlier would indicate, and it
seems clear that the principle we have added in brackets is literally untrue. More
generally, though repetition increases the probability of some events, it certainly
doesn’t increase the probability of all events. Determining which repetitions will
persist and which won’t depends on the totality of the relevant evidence before
us, and this depends, in turn, on a great deal of background knowledge that we
bring to any such question.
All the same, we might try tinkering with the principle by adding a qualifying
phrase: “everything else being equal.” Everything else being equal, past
repetitions increase the probability of future repetitions, or so we might suppose.
Now we might say the principle is correct. Still, this can’t be how a rational
mind works.
THE APPARENT IRREDUCIBILITY OF INDUCTION
Here’s why: The trouble is that our new, revised argument isn’t deductively valid
in the first place unless we also make a similar adjustment to its conclusion. The
principle in brackets, the tacit one, now holds only on the condition, “everything
else being equal.” Yet, in that case, our conclusion will also hold only on
condition (“everything else being equal”). So our new conclusion will have to
read, “Everything else being equal, the sun will probably rise tomorrow.” Yet
this surely is not what reasonable people conclude about sunrises. On the
contrary, on normal occasions, we don’t merely suppose the sun will probably
rise everything else being equal; we suppose it will probably rise even though
we know many other things won’t be equal. (The earth tomorrow will be
substantially different from the earth today, yet we still infer that a sunrise will
occur.) The tacit premise is now true, but the conclusion of the argument is far
too weak.
In all these speculations about how our minds actually work, we seem to be
involved, perhaps, in a fundamental mistake: we have begun to expect things
from our analysis of induction that, perhaps, we shouldn’t expect at all.
Specifically, we have tried to treat the two methods of deduction and induction
as basically alike in that induction is simply a disguised form of deduction. The

only difference is that induction assumes a further, tacit premise, a premise that
turns an inductive argument into a deductive one. Yet we ought to know from the
start that, if described correctly, the two methods of induction and deduction
should give different results.
We ought to know that repetitions only sometimes show an inductive
conclusion to be probable. Not everything repeated in the past is likely to be
repeated in the future. (This is what we see in the case of the tennis ball.) On the
other hand, we should also know that, when it comes to valid deduction, the
premises always guarantee the conclusion. Once the premises are established,
the conclusion is proved true invariably; this is part of proving the conclusion
necessarily. (More exactly, logical necessity is a matter of form; arguments are
deductively valid only because they embody certain patterns or schemas. But the
whole point of talking about forms, patterns, or schemas is to describe all cases
of a certain type; the very idea of logical necessity carries with it some sense of
universality, even if the universality in question can’t always be defined with
precision.) And the key point is that “sometimes” is different from “always.” A
conclusion that is only sometimes reasonable can never be always reasonable.
Consequently, any attempt to reduce the one sort of argument to the other—to
reduce induction to deduction—would seem to be mistaken; every such attempt
will replace an inference that is only sometimes reasonable with an inference that
is always so. And in that event, we will have lost sight of what being reasonable
truly is.
(More generally, no assumption about regularities in nature can reduce
inductive arguments to deductively valid ones, it would seem, because the most
any such assumption can tell us, if stated properly, is that some regularities will
be repeated. But “some” is indefinite, whereas the particular laws of nature are
specific. And the specific never follows deductively from the indefinite.)
It seems, then, that the later writers, Mill and Russell, were mistaken, even
though Hume’s initial insight was right. Hume’s initial insight (to recall it once
more) was that our future predictions proceed upon the supposition that the
future will resemble the past. If the supposition were false, induction wouldn’t
be reasonable in the first place. And indeed, in Hume’s defense, what his
supposition seems to do is explain why we engage in inductive reasoning from
the start. We reason inductively because we believe there are underlying patterns
in the world of which our experience is a sample. Our inductive inferences
involve probabilities just as reasoning from a sample does. The more often we
find a pattern repeated in a random sample, the most probable we think that the
pattern is also characteristic of the whole. Just so, the more we find something

repeated in our experience, the more we think it likely that, everything else being
equal, it will be repeated in the future.26
On the other hand, Mill and Russell took Hume’s idea a step further, and this,
we suspect, is where Mill and Russell went wrong. They insisted that Hume’s
supposition had to be a premise. We suspect this further step to be erroneous. A
supposition is an assumption, but not all assumptions are premises; instead,
some are corollaries. A supposition isn’t always something that supports a
conclusion; it is sometimes a thing supported by a conclusion.
For example, in arguing that the sun will rise tomorrow because it has always
done so in the past, we not only assume that the future will resemble the past; we
also assume there won’t be a cosmic collision that alters the earth’s rotation or a
cataclysm at the sun’s core that extinguishes its brilliance. (If these further
propositions were false, then our conclusion about tomorrow’s sunrise couldn’t
be true in the first place.) But these “assumptions” aren’t premises. Instead, they
follow from our conclusion as further consequences; they are things our
conclusion entails.
What Hume has done, in other words, is to identify the further consequence of
all inductive inferences—that the unobserved will indeed have something in
common with the observed. He has singled out a universal corollary of
induction. He has identified what all inductive conclusions entail, and so they all
“suppose” it. Nevertheless, this doesn’t make the assumption he invokes a
premise. (By analogy, the later theorems of Euclid are consequences of the
axioms, and they are usually proved by way of the earlier theorems; accordingly,
if the later theorems were false, then something in the earlier theorems—or
something in the axioms—would also have to be false. As a result, the body of
earlier theorems “assumes” that the later theorems could be true. But this doesn’t
make any later theorem a premise for an earlier one. In the order of argument,
Hume’s supposition is a consequence of induction, not a premise of it, and what
distinguishes the strange man of our imagination is that he chooses to preclude
this consequence in advance. The strange man is like a student of geometry who
insists from the start that he knows all Euclid’s later theorems to be necessarily
false and who therefore concludes that all arguments from the axioms must be
rationally unpersuasive.)
It all sounds rather tricky and complicated, but we seem to come back once
more to something fairly straightforward: reasonableness, when applied to
arguments, comes in two varieties—inductive and deductive—and neither seems
to be reducible to the other. Apparently, they are apples and oranges. Neither is a
disguised form of the other.27 But then what, if anything, must empirical science

assume?
THE ASSUMPTIONS OF EMPIRICAL SCIENCE
Despite the difference between induction and deduction, the discoveries of
Galileo, Kepler, and Newton still involved the same mental leap described by
Hume: the leap from past to future. This is because the whole point of their work
was to expound general laws that applied as much to the future as to the past.
The evidence for their laws consisted in past observations, and the purpose of the
laws was to describe mechanisms that, if real, would cause the phenomena they
had already seen. Yet the laws also applied to the future because they resulted in
testable predictions. In publishing their results, they were inviting other people
to think for themselves and to arrive at similar observations, and they were also
making it possible for future scientists to test their laws by devising further
experiments that would challenge the laws in novel ways. In that case, however,
the connection between past and future was essential. Only if the past really is a
guide to the future will a scientific theory have testable consequences in the first
place, and this is so whether the subject is physics, chemistry, astronomy, or
biology. The effect of such theories is always to connect things observed to
things still unobserved. Hume insisted that this same leap—from the observed to
the unobserved—is also involved in any conclusions we draw about cause and
effect.
In seeking to discover the cause of a particular event, we normally assume that
no such event can be uncaused. We suppose all events have causes, and we
suppose that causes are related to their effects by general principles. In addition,
we suppose that effects never precede their causes (unless we are somehow
talking fancifully about time travel). These are the assumptions that make
physical science possible, and we normally suppose them to apply as much to
the future as to the past. Even so, we can still ask the same question posed by
that strange, hypothetical man we conjured up a moment ago: how do we know
these things? How do we know that tomorrow won’t be radically different, even
in terms of cause and effect?
How these causal assumptions are known to be true at all is a deep and
famous problem of metaphysics. Can their truth be proved by physical
observation, or does the intelligibility of physical observation already assume
their truth? If we didn’t, in fact, presuppose that events have causes that conform
to these rules, how would we go about proving it?

(The German philosopher Immanuel Kant, writing in the eighteenth century,
replied that such assumptions are, in fact, supplied automatically by the mind
and that, without them, no intelligible experience of physical objects would be
possible. Thus, according to Kant, no one can prove that events have such
causes. Instead, we simply assume it.28 Hume, by contrast, took a different
approach. Hume argued that, despite appearances, causation is nothing more
than the constant conjunction of two events—A and B. To say A causes B is only
to say that B always follows A.29 Of course, we usually suppose that cause and
effect is more than a merely perpetual coincidence, but, according to Hume, this
is a mistake. In essence, then, Hume’s theory seeks to reduce cause and effect to
the observable alone, in the tradition of British empiricism, whereas Kant’s
theory insists on additional assumptions supplied by the mind, in the tradition of
Continental rationalism. Mill, as it turns out, took yet another view. He regarded
the principle of the uniformity of nature as an empirical proposition that is
freshly confirmed every time a scientist discovers a new physical law.)30
The crucial point for the logician is more basic; to the logician, the crucial
point is that the strange man in question—the one who challenges all connection
between past and future—might reason deductively as well as anyone else. He
might see the validity of modus ponens perfectly, and he might analyze
categorical syllogisms with all the ease of Aristotle. What he doesn’t do,
however, is draw inferences from the observed to the unobserved. He differs
when he tries to reason inductively, meaning when he tries to draw conclusions
about anything new. (It would seem to follow, then, that deduction and induction
must be different things.)
Another way to describe the strange man is to say that he refuses to see his
experience as a representative sample of a larger reality. The result is that he
refuses to regard predictions about the future as in any way probable. We,
however, differ from this strange man, meaning we do see our experience as
representative (at least if we are sane). We believe the observed and unobserved
have something in common, a common nature, and our science seeks to discover
this nature by looking at only part of reality—the part that experience discloses
to us.
Of course, in making these last points, we don’t mean that anyone normally
thinks about parts of reality or the whole of it when reasoning about sunrises. We
don’t mean that anyone normally thinks of reasoning from experience as a form
of reasoning from a sample. On the contrary, as Hume suggested, whenever we
reason from past to future, we probably operate on a kind of instinct. (Hume
thought deeply about these matters, and his word for the instinct was

“custom”).31 Instead, what we mean is that the instinct, perhaps a product of our
evolutionary struggle for survival, does have a rational justification, and the
justification is the same that underlies sampling. If a sample is selected randomly
and contains successive repetitions, the probability that the repetitions are
themselves only random accidents is less the longer the repetition is. A purely
random event is far less likely to happen ten times in a row than twice in a row.
Thus, the more we see something repeated, the more probable, everything else
being equal, that the repetition exhibits something nonrandom—like a causal law
or a principle of nature.32 (Moreover, without some such justification—some
principle in virtue of which the instinct actually is rational—the instinct probably
wouldn’t exist in the first place, at least not on Darwinian grounds, because
otherwise it would have given no advantage to our ancestors in their struggle for
survival. The utility of the instinct depends precisely on the fact that it does
indeed result in probable inferences. Those who say induction is rational only
because it is instinctive have the matter, in our view, backward. Induction isn’t
rational because it is instinctive; induction is instinctive because it is rational.)
All the same, the justification of inductive reasoning we have just sketched is
limited in a fundamental way: we must still suppose (not as a premise but as a
corollary) that whatever nonrandom feature probably accounts for repetitions in
the past will also give rise to repetitions in the future. We must still suppose a
common nature that connects the observed to the unobserved. But of course
there is nothing in deductive logic to tell us that this assumption is correct. There
is nothing to tell us that the seen must be anything like the unseen. (Asking why
it is rational to assume that the seen should have anything in common with the
unseen is like asking why it is ever rational to rely on modus ponens; at some
point, there can be no further answer.) This is just where that strange “irrational”
man we conjured up earlier suddenly veers down a different path. For him,
tomorrow is a different day.
The seventeenth century was a watershed in the history of logic because it gave
induction and scientific method the attention they deserved. And the insight of
the century’s most forward-looking philosophers was to see in this approach the
basis of a new rationality.
The wars of religion had made them wary of enthusiasms, and the growth of
trade had broadened their world. As the money economy expanded, they, and the
middle-class audiences who surrounded them, came to rely less on deductions
from ancient authorities and more on fresh inferences from novel experiment.

Experience became their touchstone, and as new sciences and new literary forms
emerged, so the new logic of induction emerged, too.
What set the middles classes of the seventeenth century apart—classes that
were often brash, assertive, and insurgent—was their willingness to follow this
path and to rely above all on their own experience, their own intellect, and their
own ability to reason from the observed to the unobserved. Over time, the effect
of this transformation would be to give the world a startling new sample of
larger things to come.

8
RHETORICAL FRAUDS AND SOPHISTICAL
PLOYS

Ten Classic Tricks
PLATO REMARKED that arguments, like men, are often pretenders.1 Shoddy
reasoning can deceive us just as people do. The classical Greeks made a special
point of studying the shoddy reasoning that emerged from the simple
democracies of their time; they wanted to know how large numbers of fellow
citizens were being bamboozled and hoodwinked. Likewise, in modern times,
the great stimulus to the study of poor reasoning has been the growth of
representative democracy and the escalating force of public opinion. From the
late eighteenth century onward, public opinion has increasingly influenced
lawmaking. The result has been greater attention to the ways in which public
opinion can be manipulated. Vast majorities in many modern states now expect
their opinions about law to count for something, and, even if this expectation is
sometimes illusory, the illusion is nevertheless encouraged. Public sentiment
now has great power, and many individuals and organized groups have
responded to this situation by trying to deceive large masses of ordinary people.
Propagandists and demagogues have become serious dangers in the modern
world. The rise of the modern mass media has also strengthened the sway of
public opinion in politics, and the effect on logicians has been to make them ever
more alert to the dangers of public sophistry.
The rise of democracy and the modern study of sophistry have gone hand in
hand, and the reason is that many intelligent observers over the last two centuries
have come to fear that, without some such study, democratic institutions can
easily run off the rails. Working against the background of the Athenian
Assembly, Aristotle produced the first known list of rhetorical frauds (his
Sophistical Refutations);2 all the same, the study of sophistry in modern times
actually 
began 
in 
eighteenth-century 
England 
when 
a 
thoroughly
unrepresentative British Parliament presumed to dictate the affairs of an
increasingly commercial empire. Parliament represented an elite, but the growth
of British trade, backed by the British navy, weakened this elite by strengthening
the hand of the middle classes.
At the same time, demands for political reform also gained momentum
because of the continuing success of Newtonian science, and this affected
Parliament too. As a result of Newton’s work, more people, especially
intellectuals, came to believe in the power of reason as opposed to tradition—an
outlook that defined what we now call the Enlightenment. The political
consequence was to subject the laws and customs of the age to ever more

criticism. The overall effect was enormous pressure for political change, and the
reaction of those who opposed this change was to fall back increasingly on
bogus arguments to divert public attention. The Old Guard defended its
privileges with rhetorical ploys and slippery propositions.
The battle for Parliamentary reform played out over many decades, but in the
end the period’s more radical reformers began to analyze public sophistry in the
hope of counteracting it—being convinced that, if the public’s eyes could finally
be opened, the world might be made a better place. The leader of this approach
was the English philosopher Jeremy Bentham, who would eventually become
the inspiration for a new generation of Parliamentary reformers, the
“philosophical radicals.” And Bentham’s attempt to analyze sophistry has been
imitated ever since.
THE BATTLE FOR PARLIAMENTARY REFORM
Britain’s Parliament in the eighteenth century was highly aristocratic. The House
of Lords consisted of hereditary peers (nobles, all male, whose political privilege
depended solely on birth). The House of Commons was elected from a small
fraction of Britain’s male population; for the most part, only the wealthy (and
only men) could vote for a member of the Commons at all. In addition, the Lords
were, in those days, a coequal branch of the legislature, so their consent was
necessary for any new laws.3 Parliament had already evolved a great deal since
its origins in the thirteenth century, but the effect during the eighteenth century
was still rule by an upper crust. There were conflicting factions within this upper
crust, of course, but the great majority of citizens were shut out. Nevertheless,
there was also constant agitation in the press for political reform, agitation that
derived ultimately from the growing power of the middle classes. And many
members of Parliament wanted that agitation suppressed.
Parliament responded to agitation and criticism by trying to keep its debates
secret. It reserved the right to imprison anyone who revealed them. Parliament
had long insisted on this secrecy, but in 1736 a periodical called The
Gentleman’s Magazine began giving accounts of those debates anyway, after its
daring publisher, Edward Cave, managed to gain access to the gallery of the
Commons, along with several friends. Cave and his friends took notes on the
debates surreptitiously and then repaired to a nearby tavern to work out together
their version of what had been said. Next, they had a stylist rewrite their account
so as to give it a lofty tone, and afterward they published the result in the
magazine.

When the Commons threatened to prosecute Cave for these accounts, his
magazine related the remarks of real members under fictitious names and called
its reports “Debates in the Senate of Lilliput” (the government of one of the
imaginary islands in Jonathan Swift’s Gulliver’s Travels). Even the literary critic
Samuel Johnson contributed to this effort, giving the members’ real names
instead, and in later years several newspapers (The Morning Chronicle and The
London Evening Post) joined in.
Finally, in 1771, the Commons ordered the arrest of several printers for their
reporting, but the Lord Mayor of London, Brass Crosby, refused to recognize the
arrest warrants. The Commons then retaliated by having the Lord Mayor
imprisoned in the Tower of London, but public opinion reacted in his favor, and
he was released. In the end, public opposition had become so great that the
Commons backed down; too many ordinary citizens had come to think that, as
Parliament was determining the public’s business, so the public had a right to
know about it. And in the following year, the Commons made no further attempt
to suppress the reporting of its deliberations. The modern study of sophistry
grew out of this struggle, but behind this collision between Parliament and the
citizenry was the growth of British trade, which had increased the wealth and
numbers of Britain’s commercial classes.
As an island nation, Britain had a natural advantage over other European
states in the development of sea power; states on the Continent had the
additional burden of defending against overland invasion. The naval powers of
Portugal and Spain had already been eclipsed in the seventeenth century by the
Netherlands and France, but by the eighteenth century, the seas belonged
increasingly to the British. Dutch power succumbed to a combination of British
naval pressure (Britain’s Navigation Acts effectively banned Dutch shipping
from British ports) and a series of threats from land armies on the Continent. As
for France, the British gradually whittled down French naval strength through
five periods of war that spanned the whole of the eighteenth century and
culminated in Britain’s victory over a combined fleet of French and Spanish
warships at the Battle of Trafalgar in 1805.4
As British commerce grew, sustained by overseas markets, so also did the
numbers and influence of the commercial classes; London became the busiest
port in the world, and its merchants wanted an ever greater say in how British
officials managed their affairs. In addition, even the aristocracy increasingly
came to support Britain’s commercial interests, in part through the enforcement
of strict laws of primogeniture. This, too, strengthened the hand of the middle
classes.

Primogeniture generally required that the landed estates of hereditary
aristocrats be passed down intact, upon the death of the owner, to the owner’s
eldest son, and this caused many aristocratic families to seek support for their
other children through marriage alliances with prosperous merchants and to
establish their younger sons in middle-class professions like lawyering. The
result was increasing cooperation, thanks to family ties, between the middle
classes and the nobility, and this increased cooperation also tended to make the
laws friendlier to investment—as long as the agricultural interests of the nobility
were preserved.5 (In France, by contrast, many aristocratic estates had been
unprofitably subdivided, and the nobles were forbidden to engage in trade. The
French nobility fell back on ever greater taxes imposed on the middle classes
and the peasantry, with the result that bitter class antagonisms finally erupted in
the French Revolution.)
As the middle classes gained increasing social power in Britain, so they
studied with ever greater zeal the political power of Parliament. And one of the
sharpest and most determined of Parliament’s critics was the son of a lawyer
with much influence in the press: Jeremy Bentham.
JEREMY BENTHAM AND THE LEGACY OF THE
ENLIGHTENMENT
Born in London in 1748, Bentham followed Parliamentary debates closely
through much of his life, and he was also the person who popularized the famous
“principle of utility,” the thesis that the whole difference between right and
wrong is strictly a matter of producing the “greatest happiness for the greatest
number.” Bentham’s principle now defines the philosophy of “utilitarianism.”
His principle has always been controversial (different interpreters still construe it
in different ways), but Bentham’s chief complaint against the laws of his time
was that they favored not the greatest number, but the few. He believed British
government was essentially a conspiracy of the elite at the expense of everyone
else, and he devoted nearly all his rhetorical energies to changing it. He proposed
many specific new laws to make Britain more democratic and more prosperous
(wider voting rights, the secret ballot, limitations on hereditary privilege, prison
reform, free trade, public sanitation, and the humane treatment of animals), but
as a student of argumentation, he also made a catalogue of the verbal swindles
and subterfuges by which clever speakers in his day had sought to turn back
needed reforms and make foolish laws look just.

Bentham and his fellow reformers didn’t invent their ideas out of nothing, of
course. Their efforts were actually part of a larger intellectual movement in
Europe through much of the eighteenth century, a movement that had sought to
apply the methods of reason and science to the solution of social problems—the
Enlightenment.
Bentham’s efforts were, in many ways, a natural consequence of the
Enlightenment. Earlier, in the first half of the seventeenth century, Galileo and
Kepler had acted almost as if they were political subversives, carefully
measuring any utterance lest they be arrested. With the explanatory success of
Isaac Newton, however, large numbers of literate people had come to see science
in a new way, and a further effect of this change was to see politics in a new way
too. Just as Newton’s analysis had seemed to replace superstition with reason, so
had many people begun to seek similar changes in politics. Laws and customs,
they argued, often had no justification except the dead weight of tradition. But in
that case, they asked, shouldn’t society be rationally reformed just as physics had
been reformed? Since logic and evidence had given rise to a true physical
science, they supposed, there might soon be a true political and social science,
also supported by logic and evidence. This was Bentham’s inspiration.
Shortly after Newton’s death in 1727, the English poet Alexander Pope
captured the reaction of many of Europe’s leading intellectuals in lines that he
intended as an epitaph for Newton’s grave in Westminster Abbey:
Nature and Nature’s laws lay hid in night:
God said, Let Newton be! and all was light.
This Newtonian synthesis (combining Galileo’s terrestrial laws with Kepler’s
celestial ones) eventually became the rational inspiration for many eighteenth-
and nineteenth-century reformers who wanted to make politics and society as
reasonable as physics.
Many important reforms were propelled by this impulse. For example, official
prosecutions for witchcraft died out, ending the so-called witch craze, as more
judges came to believe that action at a distance through spells and incantations
was physically impossible. More generally, the disposition to analyze and
challenge became increasingly fashionable, not only in literate sections of
European society, but also beyond Europe. (For example, it motivated the great
Bengali reformer Raja Rammohan Roy, whose campaign for social reform in
India in the early decades of the nineteenth century helped outlaw suttee, the
burning of widows. Roy advocated religious tolerance, helped to found India’s

first indigenous-language newspapers, published many essays seeking a
common ground among the world’s different religions and ethical teachings, and
established new schools for children.)
This new, critical approach that challenged tradition was especially attractive
to literate members of the middle classes, but it also rubbed off on a progressive
portion of the European nobility and even on an occasional despot. In Eastern
Europe, the “enlightened despots,” rulers like Frederick II of Prussia and
Catherine the Great of Russia, used the Enlightenment’s egalitarian ideals to
weaken their traditional rivals, the lesser, hereditary nobles. (Many of their
“enlightened” policies were actually self-serving grabs for power; these rulers
faced no real insurgent middle classes in their own domains, and so the true
effect of their reforms was to centralize their own authority.)
The real centers of the Enlightenment, however, were London and Paris, and
in Paris the movement had already found expression, even while Bentham was
still a boy, in the “philosophes,” the French social thinkers who sought sweeping
political reforms and who, in many instances, joined with Denis Diderot in
producing the Encyclopedia, the greatest publishing venture of the century. (The
Encyclopedia’s purpose, in Diderot’s words, was to “gather knowledge now
scattered over the earth” and “make it accessible to future generations.”6 The
Encyclopedia began to appear in installments in 1751, and it took another
fourteen years to complete, with articles stressing science, technology, and a
critical analysis of contemporary society.) These were the assumptions and
attitudes that Bentham had already absorbed when he finally turned his
analytical powers against a series of dead traditions in his own country. He
attacked the structure of an unreformed Parliament, and his aim was to usher in a
more democratic future.
BENTHAM’S BOOK OF FALLACIES
Sophistry has many applications, of course, not just to the making of laws.
Nevertheless, the role of public opinion in the battle over Parliament soon made
the question of sophistry more sensitive, and this is what gave Bentham his
audience. In earlier times, after the decline of the ancient Greek democracies,
politics had excluded the great mass of the population; peasants and serfs had
never been consulted in politics and had never expected to be. But all this
changed as England became commercial. Periodicals and newspapers multiplied,
the press soon functioned as a “fourth estate” (a politically powerful social
stratum, in addition to the clergy, the nobles, and the commons), and reformers

became bold. Freedom of dissent had already been expanding in British society
for more than a century—religious and political dissenters were already common
by the mid-1600s—and Britain’s security from land invasion had also made it
harder for the British government to arrest dissidents on the supposed pretext
that they threatened national security, as often happened on the Continent.7 The
upshot was that Bentham soon found himself trying to influence the battle for
public opinion in a new way: cataloging false and fraudulent reasonings. Like
Aristotle, he composed a list, but his list was much longer than Aristotle’s, and it
derived from Bentham’s firsthand observation.
So far as Bentham’s personal life was concerned, he was startlingly eccentric.
He rarely finished the books he started, and so his masterwork on sophistry, his
Book of Fallacies, had to be pulled together from loose papers by several
admirers, Pierre étienne-Louis Dumont, Peregrine Bingham, Francis Place, and
John Stuart Mill. (Working from Bentham’s manuscripts and with Bentham’s
permission, Dumont began publishing French versions of Bentham’s works,
heavily edited, in 1802, and in 1816 he produced a work that contained much of
The Book of Fallacies. One result was that Bentham was initially far better
known to French readers than to English ones. In 1824, an English version of
The Book of Fallacies appeared, mainly the work of Bingham, but with help
from Place and Mill.)
More eccentric still was Bentham’s decision to be converted after his death
into a mummy. As his death approached in 1832 (the same year the great Reform
Bill made Parliamentary representation more democratic),8 Bentham asked
himself how he could still be “useful.” The question of usefulness had always
appealed to his utilitarian instincts, but he finally came to the view that his dead
body could be quite useful in at least two ways: first, by serving as a cadaver to
be dissected for the advancement of medical science (his body was dissected
after his demise before a group of invited guests, many of them former friends),
and, second, by providing a continuing inspiration to future generations of
rational reformers if embalmed and exhibited publicly. His corpse was
subsequently exhibited at University College, London, where it still resides,
dressed in Bentham’s original clothing and sitting in a chair in a large wooden
box. (The head on display is made of wax; the embalming of the real head
proved unsuccessful.) Bentham’s last destination is surely bizarre, but his
example reinforces a point much stressed by his friend and intellectual heir John
Stuart Mill in On Liberty: eccentric people are sometimes our most creative
citizens.9
To return to sophistry: Sophistry is the art of deluding an audience with

arguments that one knows to be illogical or misleading (a practice we also
considered, in chapter 2, among the classical Greeks). And each of its frauds can
be called an individual “sophistry” or “sophism.” On the other hand, when an
illogical argument is used innocently, without the speaker’s being aware that it
is, in fact, illogical, then logicians call it a fallacy, a term that implies nothing
about the speaker’s motives. Thus each of these logical mistakes can be a
premeditated sophistry or merely an innocent fallacy, depending on the speaker’s
intentions. But how many sophistries are there, and what are their basic forms?
As it turns out, in some respects, there are more sophistries than we can count,
but the fortunate thing is that there are few classic frauds; the rest are usually
just variations on the old themes. Contested issues come and go, but the greatest
sophistries are practically eternal. And the best defense against them is simply to
become familiar with them. A trick that at first seems baffling will later become
obvious once it is studied, and in time such devices will start to seem less like
tragedy, more like farce.
We have compiled a list of additional fallacies and sophistries, in alphabetical
order, in our Appendix, but for the present we offer ten classic frauds. Several
come from Bentham (who liked to give fallacies unusual names of his own
invention), and one comes from the twentieth-century American journalist
Richard Rovere. Others come from writers who are now unknown to history.
1. The straw man. A speaker or writer attacks a mere caricature of his
opponent’s view rather than the view itself, just as a timid fighter, rather than
facing a real opponent, attacks a man of straw. (“Do you support the Republicans
and fascism?” “Are you for the Democrats and godless Communism?” Instead
of running against the Republicans or Democrats as they really are, you run
against the Nazis or the Bolsheviks.) The idea is to attribute to your opponent a
silly proposition and thereby cast him in the role of defending the indefensible. A
common method is to attack “those who say . . .” without naming the opponents
directly. “I’ve never endorsed the theory of those who say . . .” The theory then
turns out to be something so stupid that no one would say it.
Another common method is to exploit verbal ambiguities. Take the expression
“left-liberal.” In ordinary speech a “leftist” is further left than a “liberal”; thus
the advocate calls his opponent a left-liberal when he wants the accuracy of the
liberal label and the alarming connotations of the leftist one. He runs together
two different groups. The same device can be used by many different factions;
“right-wing conservative” is the same trick used by the other side.
Again, the terms “right-wing” and “left-wing” have a similar effect when

applied to people slightly to the right or left of center; such language suggests
that those near the center have something in common with those at the extremes.
The terms “extreme” and “extremist,” though often accurate, can also be used
inaccurately to erect a straw man.
The straw man is usually an exaggeration, but it can also be an amalgam of
disparate notions held together by no common feature other than the speaker’s
antipathy. Hitler’s “international Jewish conspiracy” was something of this sort.
A variety of different ideas, political trends, or even artworks are run together
and treated as parts or instances of some great Evil Thing. The expression “late
capitalism” sometimes plays this role for speakers on the left—late capitalism
turns out to include almost every bad thing under the sun—and so do some uses
of the term “globalization.” (“Late” is tacked on to “capitalism” to suggest that
the speaker somehow understands the timetable according to which one
economic system will suddenly give way to another. To the extent that this
understanding is bogus, the expression also constitutes pretentious diction,
which we discuss later.)
For speakers on the right, a sometime favorite is “secular humanism” or
“secular progressivism”; such phrases can have a precise meaning, but in many
contexts they denote a broad array of different targets: communism, socialism,
liberalism, Darwinism, hedonism, atheism, and fluoridated water. The idea is to
use a word that means seven different things and show that your opponent
advocates one of these things; in the minds of the audience, he now advocates all
seven. You give the audience a bogeyman.
2. The vague generality. The disputant defends a specific policy by lecturing
the audience on the need for a large, general object. Bentham illustrated this
fallacy by describing a man who defends a bad law by pleading for “law” in
general (as in, “Gentlemen, we must have law, we must have law!”).10 The
particular law he defends turns out to be a monstrosity.
Similarly, a speaker defending an increase in military spending calls for
“security” in general while his opponent pleads for “peace” in general. (“Give
peace a chance.”) Again, someone defending the morality of a specific
government policy lectures the audience on the need for Morality, for
Government, and for Policy. The tactic consists in deflecting attention to a
general abstraction when the real issue is a specific plan. In addition, the vague
generality can be used negatively, as an object to be campaigned against; “To
vote for us is to vote against war, poverty, and racism.”
3. The multiple untruth. A series of falsehoods or many aspects of the same

falsehood stated in such quick succession that the opponent can’t refute them
without confusing the audience. The tactic was named by Richard Rovere, who
wrote for the New Yorker magazine, to describe a device used by U.S. Senator
Joseph McCarthy. Rovere explains the tactic this way:
The multiple untruth need not be a particularly large untruth but can instead be a long series of
loosely related untruths, or a single untruth with many facets. In either case, the whole is composed
of so many parts that anyone wishing to set the record straight will discover that it is utterly
impossible to keep all the elements of the falsehood in mind at the same time. Anyone making the
attempt may seize upon a few selected statements and show them to be false, but doing this may
leave the impression that only the statements selected are false and that the rest are true. An even
greater advantage of the multiple untruth is that statements shown to be false can be repeated over
and over again with impunity because no one will remember which statements have been disproved
and which haven’t.11
In effect, McCarthy would hurl a series of falsehoods or many aspects of the
same falsehood in such quick succession that his opponents would only confuse
the audience by trying to refute them. He would bombard the U.S. Senate with
many accusations at once. The public was often confused by both sides in these
disputes, but the sheer size of the charges suggested that they had to contain
some truth. Throw enough mud, and some of it sticks. (McCarthy turned out to
be a master of many sophistries, but his folly lay in never really seeing how
easily his methods could be exploded by a skilled opponent; McCarthy finally
met that opponent in the person of Joseph Welch, a trial lawyer of great
experience who effectively destroyed McCarthy’s career on national television
during the so-called Army-McCarthy hearings of 1954.)
4. The ad hominem. The disputant offers an assertion about the advocate of an
idea as a reason for rejecting the idea itself. (The expression “ad hominem,”
from Latin, means “to the person.”) The ad hominem takes several forms, some
rather subtle, but the most obvious is direct insult (“Communist!” “Fascist!”
“Terrorist!” “Reactionary!”). The speaker implies that the opposing idea must be
bad merely because it is defended by a bad person. A subtler form is the so-
called circumstantial ad hominem, which discounts the opponents’ views
because of their backgrounds or their other opinions. (“This idea is typical of a
foreigner.” “The chief advocate of the new tax bill hasn’t paid taxes in years.”
“Opponents of abortion plead the case of the unborn, but they do nothing for the
children of the poor.” “Defenders of abortion plead the case of the poor, but they
do nothing for the unborn.” “The architects of the plan are just people who want
to feel morally superior to the rest of us.”) The aim throughout is to talk about
the opponents instead of the specific issue. The shrewdest ad hominem is
perhaps the oblique version, which consists not in dwelling on the opponent’s

character but in praising one’s own. (“As a person deeply committed to social
justice, I believe . . .”; “As a truly religious person, I suggest . . .”) The idea is to
wear your heart on your sleeve. Tell an audience you are pure and have an
opponent, and many in the audience will suppose the opponent must be impure.
In all its forms, the ad hominem consists in offering an assertion about the
champion of an idea as a reason for rejecting the idea itself. (In slanted news
coverage, the ad hominem often consists not in covering issues but in dwelling
on the personal foibles of those who speak about issues. During an unpopular
war, for example, the main story becomes not the war itself but the opinions and
idiosyncrasies of those who defend or attack it.)
It is not hard to see why the ad hominem works. It works because it resembles
a perfectly rational attempt to assess expert testimony. In matters requiring
special experience or training, we often defer to experts. When we go to a
doctor, we usually take the doctor’s word for what ails us; most of us lack the
medical knowledge needed to supply our own diagnosis. Yet our trust in the
doctor’s word depends crucially on our trust in the doctor’s character. We might
reject a diagnosis if we learned that the doctor was somehow incompetent or
dishonest. As a result, the soundness of the doctor’s character is logically
relevant to the soundness of the doctor’s testimony, and the appeal of the ad
hominem comes from confusing this situation with an attempt at analysis.
The two situations are different. When we rely on another person’s testimony,
that person’s character is relevant. But when we assess that person’s
argumentation, the person’s character is irrelevant—a distinction that is easier to
see, perhaps, in a courtroom. When a witness gives testimony in court, all parties
to the controversy are usually permitted, through their lawyers, to examine the
witness’s personal character as part of determining whether the testimony is
credible. Only if the witness is honest and competent is the testimony reliable,
and consequently any trait of character that reflects poorly on this witness’s
honesty or competence becomes fair game. On the other hand, when a lawyer
then tries to infer what follows from the testimony, no one is normally permitted
to examine the lawyer’s character, because the lawyer isn’t testifying to what he
or she saw or what he or she knows; the lawyer is only acting as an advocate—a
mouthpiece for a line of reasoning. As a result, the lawyer’s character is
irrelevant.12
Then why does the ad hominem occur at all? Because in ordinary life, we
often find ourselves straddling these two situations. When we hear a person
analyze an issue that we only partly understand, we can often follow the person’s
reasoning for a while, but then we begin to drift. We think to ourselves, This

person obviously knows the subject—the remarks are smart and informed—but
now this person is going over my head. Perhaps, therefore, I should simply take
his word for it. Ah, but in that case, I must first ask myself, Is this person really
as smart as he seems, and can I trust him?
From this moment forward, we are no longer treating the speaker as advocate.
Instead, we have begun to see the speaker as expert witness, and, as with all
witnesses, the speaker’s believability now depends on the speaker’s discernment
and integrity. In many instances, this shift in approach is the only practical
course, but on other occasions it is merely a symptom of laziness. What the ad
hominem does, then, is encourage the audience to be lazy. The ad hominem
encourages the audience to forgo the hard work of analyzing an argument that is
still within its reach and to judge the question on personalities alone. In
consequence, the device will be most effective in just those cases where an
audience is most inclined to be lazy—and least inclined to be diligent.
(Bentham discusses the ad hominem under the heading of “vituperative
personalities” and characterizes it in the following way: “In bringing forward or
supporting the measure in question, the person accused has a bad design;
therefore the measure is bad. He is a person of bad character; therefore the
measure is bad. He is actuated by a bad motive; therefore the measure is bad; he
has fallen into inconsistencies; therefore the measure is bad. He is on a footing
of intimacy with this or that person, who is a man of dangerous principles and
designs, or has been seen more or less frequently in his company . . . therefore
the measure is bad.” Bentham regarded all such methods of inference as foolish.
He adds, “In proportion to the degree of efficiency with which a man suffers
these instruments of deception to operate upon his mind, he enables bad men to
exercise over him a sort of power the thought of which ought to cover him with
shame.”)13
5. The sham distinction or distinction without a difference. The speaker favors
A and opposes B without giving any idea how A and B are different. (“Money
should buy influence, but it shouldn’t buy votes.” “We allow free discussion but
not antistate agitation.” Bentham gives this example: “I believe in the liberty of
the press, but not the licentiousness of the press.”)14 As a result, the speaker is
able to turn aside what would otherwise be telling objections by pretending that
the objections apply to something else.
6. The red herring. The disputant uses an irrelevant issue to distract attention
from the real one. The device gets its name from the supposed practice of
drawing a bag of smoked fish across a trail to distract tracking dogs.15 (“I believe
in our platform, I believe in our candidate, but most important I believe in God,

in freedom, and in the greatness of America.”) The disputant appeals to the
noble or the sublime; the audience supposes that the disputant’s specific plan
must be sublime too. (Another example: “Yes, our company dumps toxic
chemicals into the drinking water; still, liberty and free enterprise are precious
rights, and think what will happen if we surrender these freedoms to an all-
powerful state!”)
7. The genetic fallacy. The speaker attacks an idea because of its origins (its
“genesis”) or because it was embraced in the past by people who are now
repudiated. (“My opponent in this election wants to nationalize the railroads;
Lenin had the same idea.” “Our adversaries would enlarge the military; that was
Hitler’s plan.”) The genetic fallacy is closely related to the ad hominem but
consists in attacking something other than the idea’s current advocates. It
suggests that the idea is historically associated with the wrong people. The guilty
association can also be established by labeling. (“This is merely a bourgeois
idea,” “a Russian idea,” “a French idea,” “a European idea,” “a liberal idea,” “a
reactionary idea,” “a socialist idea,” “a Western idea.”) Where the idea comes
from is invoked as a reason for rejecting it.
8. Superfluous displays of learning. The disputant invokes irrelevant statistics,
unnecessary references to great names or events, unneeded mathematical
symbols, esoteric terminology, or foreign words and phrases in place of their
ordinary English equivalents. The aim is to mystify. This, too, was a favorite
tactic of McCarthy, who was particularly fond of irrelevant documents. Rovere
remarks, “Photostats and carbon copies and well-kept newspaper clippings have,
I think, an authority of their own for most people; we assume that no one would
go to the bother of assembling them if they didn’t prove something.”16 College
students use this tactic when they engage in “data dumping”—loading an essay
with irrelevant information so as to make it seem impressive. Professors use the
same trick when they pad an academic article with special symbols, scientific
findings, or historical details they know to be irrelevant to the subject.
9. The ad populum or bandwagon. The speaker argues for a proposition on the
grounds that most people already believe it. (“The best proof of our position is
that the vast majority of Americans agree with us.”) “Ad populum” means “to
the people.” The ad populum is no fallacy when the people who already believe
are somehow better informed than the audience to be persuaded. (“Nine out of
ten doctors recommend . . .”) In that case, the argument is much like an appeal to
the weight of expert testimony. But it is indeed a fallacy when the audience has
just as much information as the believers. (Consider: if the audience still needs
more information to decide, then so do those who already believe, since they

have the same information, and in that case the opinion of the believers proves
nothing. But if the audience doesn’t need more information, then the opinion of
the believers is superfluous anyway; the audience can already decide for itself.
The ad populum assumes that the audience is too lazy to think the matter
through.)
In contemporary politics, the ad populum is sometimes advanced by
establishing a “message of the day,” which is an opinion to be repeated
simultaneously by all operatives of a political organization. As a result, many
people will come to believe the opinion merely because they hear it widely
repeated, quite apart from whether there is any evidence for it. (The tactic can be
justified if it calls attention to an idea that might otherwise be drowned out in a
cacophony of distractions. But the effect is different when the audience is
already familiar with the idea because the idea is already controversial. In 2004,
the documentary film Outfoxed revealed that two senior executives of the Fox
News Channel, Roger Ailes and John Moody, had been crafting daily memos for
circulation to all the network’s producers, anchors, and commentators, often
telling them what collective opinions they should express and defend on the
controversial issues of each particular day. In many cases, the resulting method
of persuasion was ad populum. Earlier, Ailes had worked in the political
campaigns of Ronald Reagan, among other candidates, where a “message of the
day” had become basic strategy.)17
In addition, to believe in majority rule is not, in itself, to embrace the ad
populum fallacy. A majority may exercise authority without being necessarily
right, and an enlightened democracy can still encourage its citizens to base their
decisions on good reasons. The ad populum, by contrast, asks them to base their
decisions on the opinions of others—without asking whether the reasons are
good.
10. The neologism. A neologism is a “new word,” meaning a word that has
never been heard before, or, alternatively, an old word used in a new way. Nearly
every major innovation brings a need for new words, and thus many neologisms
are perfectly reasonable. But new words can also disguise a speaker’s confusion
or lack of new ideas. A speaker’s frequent temptation, when devoid of original
thought, is to concoct an original vocabulary.
There are many neologisms today, and many are useful additions to modern
language, but one of our personal favorites comes from the French thinker
Jacques Derrida in a talk that he gave in 1966 to an assembly of professors in
Baltimore. During this talk, delivered in French, Derrida made this puzzling
remark: “The Einsteinian constant is not a constant, is not a center. It is the very

concept of variability—it is, finally, the concept of the game.”18 Exactly what
this means, we don’t know, but, to interpret it, we fall back on a bit of
commentary supplied by the physicist Steven Weinberg, who is a Nobel laureate.
Weinberg writes,
When . . . I first encountered this paragraph, I was bothered not so much by the obscurity of Derrida’s
terms “center” and “game.” I was willing to suppose that these were terms of art, defined elsewhere
by Derrida. What bothered me was his phrase “the Einsteinian constant,” which I had never met in
my work as a physicist. True, there is something called Newton’s constant which appears in
Einstein’s theory of gravitation, and I would not object if Derrida wanted to call it “the Einsteinian
constant,” but this constant is just a number (0.00000006673 in conventional units), and I did not see
how it could be the “center” of anything, much less the concept of a game.
So I turned for enlightenment to the talk by Derrida from which [this paragraph was taken]. In it,
Derrida explains the word “center” as follows: “Nevertheless . . . structure—or rather, the
structurality of structure—although it has always been involved, has always been neutralized or
reduced, and this by a process of giving it a center or referring it to a point of presence, a fixed
origin.” This was not much help.
Lest the reader think that I am quoting out of context, or perhaps just being obtuse, I will point out
that, in the discussion following Derrida’s lecture, the first question was by Jean Hyppolite, professor
at the Collège de France, who, after having sat through Derrida’s talk, had to ask Derrida to explain
what he meant by a “center.” . . . It was Hyppolite who introduced “the Einsteinian constant” into the
discussion, but while poor Hyppolite was willing to admit that he did not understand what Derrida
meant by a center, Derrida just started talking about the Einsteinian constant, without letting on that
(as seems evident) he had no idea of what Hyppolite was talking about.19
The danger of esoteric terminology is not only that it fools the audience but
makes us fool ourselves by persuading us that we are being profound when, in
fact, we are saying little. This point was much stressed by the English writer
George Orwell, especially in his essay “Politics and the English Language.”
Orwell remarks of such phrasings, “They will construct your sentences for you
—even think your thoughts for you, to a certain extent—and at need they will
perform the important service of partially concealing your meaning even from
yourself.” He adds, “If you simplify your English, you are freed from the worst
follies of orthodoxy. You cannot speak any of the necessary dialects, and when
you make a stupid remark its stupidity will be obvious, even to yourself.”20
(Derrida’s critics sometimes complain that his writings are “meaningless” in the
sense of being unintelligible, but on the whole this criticism is mistaken. If his
writings were literally unintelligible, they would have no impact at all. Instead,
the problem is not an utter lack of meaning but an ambiguity of meaning, which
is a different thing.)
On the whole, when obscure language misleads, it does so by describing
familiar things in unfamiliar terms. The ideas are commonplace, but the
language used to express them is uncommon. As a result, the audience supposes

that the ideas must be uncommon too. When the vocabulary is unusual, the result
is pretentious diction. (Neologisms are, thus, a common form of pretentious
diction.) On the other hand, when the speaker uses familiar words but strings
them together so as to imply an obscure analogy, the result is a vague metaphor.
Derrida’s remarks, as quoted by Weinberg, include several vague metaphors,
especially “center,” “game,” “point of presence,” and “fixed origin.”
In effect, Derrida begins with what is actually a commonplace: language is
often ambiguous. We see this, for example, in poetry, where a common aim is to
evoke multiple meanings. We also see ambiguity in daily life; the same words
can mean different things to different people, and even the same people use
words differently on different occasions. From these facts, it seems obvious that
words get their meanings from social conventions that are largely arbitrary. In
addition, many words are not only ambiguous but vague.
These points are hardly deep, but what Derrida does is give a series of novel
phrasings for them. Thus, instead of saying that words are often vague or
ambiguous—and ambiguous in different ways—he tells us they lack a center, a
point of presence, and a fixed origin; and he says that using them is a game in
that both languages and games have rules. He also adds that, between words and
their meanings, there is always a certain “slippage.”
Yet all these utterances are really just new ways of describing the old
phenomena of ambiguity and vagueness. We are getting old wine in new bottles.
That Derrida’s terminology is itself ambiguous, applying equally to different
sorts of ambiguity and vagueness, also helps to make it seem more profound; an
expression like “slippage” now applies not to one sort of ambiguity but to
several, and so the unwary listener seems to be entering a secret world of
startling conundrums.21
Many fallacies like this are ways of advancing a sham insight, meaning a
commonplace that is made to look penetrating. (We discuss sham insights
somewhat more in our appendix.) Bentham’s primary concern, however, was
always with sophistry in politics; he feared sinister interests would throw dust in
the eyes of the public.
For Bentham, detecting sophistry wasn’t simply a matter of clear thinking; it was
a basic part of responsible citizenship. He thought every citizen of the modern
world had a fundamental duty to become acquainted with the common forms of
rhetorical fraud and to learn the most effective ways of exposing them. Where
logic is abused, the people will be abused. But where the people are vigilant,

they will also need to be logical. For Bentham, the battle against sophistry was
never ending. And in today’s world, with new problems, new questions, new
media, and new forms of rhetorical misrepresentation, his intellectual heirs have
continued the struggle ever since.

9
SYMBOLIC LOGIC AND THE DIGITAL FUTURE
THE INDUSTRIAL Revolution, beginning in the late eighteenth and early
nineteenth centuries, has given the world new conveniences, factories, and cities
—and also a new kind of logic. For good or ill, the visible effects of
industrialism are now everywhere around us, but the abstract effects of
industrialism are around us too. One of the most profound of these abstract
effects is symbolic logic, which is a consequence of an age of machinery.
Symbolic logic emerged from a nineteenth-century world of mass production
and large mechanical operations, and in the twentieth and twenty-first centuries
it has given rise, in turn, to the new world of modern digital computers.
Before the nineteenth century, farsighted thinkers had long toyed with the idea
of a fully symbolic logic, but they had never turned any such project into reality.
Only with the advent of large-scale manufacturing did symbolic logic finally
take shape. The first fully symbolic systems were laid out by the English
logicians George Boole (whose algebra now underlies the operations of modern
computers) and Augustus De Morgan, both of whom published major books in
1847, just as the Industrial Revolution in England was in full swing. And the
reason for symbolic logic’s growth was the Industrial Revolution itself.
The Industrial Revolution convinced large numbers of logicians of the
immense power of mechanical operations. Industrial machines are complicated
and difficult to construct, and unless used to manufacture on a large scale, they
are hardly worth the trouble of building. Much the same can be said of modern
symbolic logic. Symbolic logic relies on abstract principles that are difficult, at
first, to connect with common sense, and they are particularly complicated.
Nevertheless, once a system of symbolic logic has been constructed, it can
embrace a vast array of theorems, all derived from only a few basic assumptions,
and suitably elaborated, it can supply a clear, unequivocal, and mechanical way
of determining what counts as a proof within the system and what doesn’t. The
Industrial Revolution convinced many logicians from the nineteenth century
onward that complicated mechanical systems are truly worth building, and the
consequence has been to capture large new areas of valid reasoning, areas that
had long been intuitively obvious to human beings yet never before reducible to
formal technique.

Symbolic logic has already had far-reaching effects. Symbolic logic first arose
out of abstract changes in mathematics, and mathematics seems, likewise, to
have been deeply influenced by nineteenth-century industrialization. But
symbolic logic has since brought ever-larger swaths of everyday reasoning
within the reach of mechanical procedures, and it has changed the way logicians
and mathematicians think about proof. In addition, it has had a large impact on
the daily lives of ordinary citizens. Just as machines first gave rise to symbolic
logic in the nineteenth century, so, in the twentieth and twenty-first centuries,
symbolic logic has given rise to a new generation of machines: digital
computers, which are essentially logic machines whose programming languages
are analogous to the symbolic languages invented by logicians. The effects of
modern symbolic logic are now felt around the world, but what first encouraged
this new branch of the discipline was the nineteenth-century success of large-
scale industry.
THE IMPACT OF THE INDUSTRIAL REVOLUTION
The Industrial Revolution began to gain strength in Britain in the late eighteenth
century, but its impact was first felt on a massive scale in the nineteenth century.
One of the most influential of all its consequences was to change the physical
appearance of millions of people. The nineteenth century was the first time in
human history when large numbers of ordinary people could own more than one
suit of clothing—clothing manufactured on large industrial machines. Earlier,
most people in most parts of the world had darned or mended the one set of
garments they would continue to wear through much of their lives.
Odd as it sounds, this new surfeit of mass-produced clothing would deeply
affect the thinking of logicians, as would the other mechanical wonders of the
industrial age. Nineteenth-century inventors would ultimately lay down most of
the basic economic and social patterns that would continue to dominate the
developed world ever after. The patterns were the result of the machines.
Among other things, nineteenth-century mechanics designed a new generation
of steam engines, fired by coal, and they greatly expanded the use of iron and
steel. They also produced colossal public works. Nineteenth-century builders
erected more structures in stone and metal than all previous ages put together,
and they may also have put up more buildings in the Gothic style than did all the
peoples of the Middle Ages, and more architecture in the style of ancient Greece
and Rome than did all the ancient Greeks and Romans. Paradoxically, many of
these developments, which would serve as mechanical models to logicians of the

future, were actually stimulated by changes in agriculture, especially in Britain
and in the nations that traded with Britain.
Britain had long imported cotton from various parts of the world, but in 1793
in the United States, Eli Whitney invented the cotton gin (short for cotton
engine), which separated cottonseeds from cotton fibers. The result was that
American cotton plantations, which exploited slave labor on a massive scale,
dramatically increased their output. Cotton had long been shipped to Britain for
spinning, but its new abundance encouraged British manufacturers to build more
mechanical mills. By 1830, nearly all cotton spinning in Britain had been
mechanized. The driving force behind this change—one that would then cause
other trades to mechanize too—was the desire of millions of people for new and
inexpensive clothing.
The early mills had used waterpower and had relied on components of wood
and leather, but iron soon replaced wood, and steam power, generated by burning
coal, replaced water. (In earlier times, Britain’s inhabitants had used wood as
their primary fuel, but Britain’s forests had long since been depleted.) Cotton,
iron, and coal thus became the three fundamental pillars of the early years of the
Industrial Revolution, and in all these areas the British had special advantages.
The most important advantage was Britain’s dominance of the seas, on which
the cotton trade depended. In fact, no point in Britain is more than seventy miles
from the sea, and the nation has many accessible ports. In addition, Britain had
ample deposits of iron and coal, and the coalfields, like those of Newcastle upon
Tyne, were often adjacent to the seacoast. This made the transporting of coal
comparatively easy.
Beyond these factors, British farmers had already increased their harvests,
beginning in the eighteenth century, and this advance in British farming
(sometimes called the “agricultural revolution”) had made it possible for a
proportionately smaller rural population to feed a much larger urban one.
Parliament’s many Enclosure Acts, especially from 1760 to 1830, had made the
nation’s farms more efficient by making them larger, though the practice of
consolidating farms had also thrown great numbers of peasants off the land.
These peasants drifted to the cities in search of work, and in so doing, they
gradually formed an immense urban proletariat. Crop rotation and new metal
tools also added to the productivity of farms, and much of the displaced
peasantry, settling in the ever-expanding cities, then found work operating
complicated machinery, often under appalling conditions. (In the early years of
the Industrial Revolution, children sometimes worked in the mills sixteen hours
a day, and in their drowsiness, they sometimes fell fatally into the machinery.)

New forms of transportation also emerged during this period, especially
railroads, which first appeared as an easy means of moving coal from the
pitheads of English and Welsh mines to nearby docks for loading onto ships.
Inspired by the use of rails to ease the way for horse-drawn coal carts (and then
by the use of steam engines, which replaced the horses), inventors like the
Englishman George Stephenson saw the possibility of building railroads for city-
to-city transport. In 1830, his locomotive Rocket carried passengers between
Liverpool and Manchester at the startling speed of seventeen miles an hour.
Stephenson used a steam engine to pull the railcars, and steam engines rapidly
appeared in other industries too. Steam engines soon became the reliable beasts
of burden of the new mechanical age.
A primitive and inefficient steam engine had already been available since the
early 1700s (developed by the English blacksmith Thomas Newcomen for
drawing water out of mines), but in 1784, after many years of experiments, the
Scottish inventor James Watt patented a rotary steam engine. Watt’s engines
soon supplied power to water pumps, potteries, textile mills, and flourmills.
Later inventors made further improvements in the efficiency of steam engines,
and gradually they became the dominant source of power for ships. Eventually,
Britain would invest in steam power more than any other nation on earth, and by
1900 three-fourths of the world’s steam engines would be located in Britain.
Nevertheless, other nations soon imitated this example, especially Belgium,
Germany, and the United States, all of which had ample coal deposits. (By
contrast, nations like France, which had less access to coal, lagged behind.)
The resulting industrial world of the nineteenth century, out of which
symbolic logic would finally emerge, was largely a place of cast iron, still
dependent on a plentiful but brittle metal until yet another crucial invention,
which appeared in the 1850s: a new method of making inexpensive steel. It was
a transformation in steelmaking from the 1850s onward that finally made
possible the world of durable machines and tall skyscrapers we now see around
us.
In 1856, the English engineer Henry Bessemer, looking for ways to improve
gun-making, explained a new technique for producing steel in large amounts.
Steel is hard yet malleable iron, with only small portions of carbon, usually less
than 1.7 percent. The earlier method of steelmaking had been a laborious process
called puddling, in which small quantities of molten iron were stirred to remove
impurities. Bessemer, however, found he could decarbonize iron in large
amounts in a new way, by mechanically forcing a blast of air through the molten
metal. In the 1860s, other inventors (especially William and Friedrich Siemens)

perfected an alternative technique, the open hearth process, which would become
the most common method of steelmaking through much of the twentieth century.
The resulting manufacture of cheap steel gave rise to a whole new generation of
machines, more durable than in the past, and it also made possible the first
skyscrapers, which depended for their support on internal skeletons of
inexpensive structural steel.
In all these changes, nineteenth-century inventors and engineers were
transforming the earth, and they brought forth many other inventions too: diesel
power, photography, telegraphs, telephones, petroleum-based chemicals,
roadways of macadam (named after the Scottish engineer John L. McAdam, who
covered roads with bitumen to form a smooth and lasting surface), and
inexpensive bicycles (which, for the first time, gave ordinary working people, on
their days off, a chance to become tourists). Nineteenth-century inventors also
produced the rotary printing press, thereby conjuring into existence mass-
circulation daily newspapers (along with all the sound and fury of an active and
sensationalist Fourth Estate). It was out of this mechanical whirl of nineteenth-
century industry that modern symbolic logic finally developed.
THE ORIGINS OF SYMBOLIC LOGIC
The seeds of symbolic logic go back farther than the nineteenth century; they
can be traced at least as far back as the seventeenth century, to the work of the
German philosopher Gottfried Wilhelm Leibniz, who aimed at a logical calculus
he said would be “mechanical.” In particular, Leibniz was interested in geared
machinery that could be used to solve mathematical problems, and he was
especially drawn to the idea of an artificial language that would allow people to
express all observable facts unambiguously and to make from them all valid
deductions.1 Nevertheless, most of Leibniz’s notes on these matters remained
long unpublished, and only in the nineteenth century did this mechanical
tendency finally become real. Ideas about symbolic logic had been, at best,
conjectural, but in England in 1847, Boole and De Morgan both published major
books that laid out fully symbolic systems, and they did so just as the Industrial
Revolution was fully under way.
De Morgan had been a brilliant student at Cambridge University but had
refused to submit to religious tests then required for a fellowship, and so he had
decided to accept an appointment as professor of mathematics at the new
University of London, a position from which he resigned twice over matters of
principle. He disliked official organizations, preferred to decline honors,

stubbornly refused to declare his beliefs as a condition of holding a job, and
refused an honorary law degree from Edinburgh. His great work was in
mathematics and logic, but he also wrote extensively on the history of science
and was much admired for his wit. (Remarking on metaphysics, a subject in
which he also took much interest, he nevertheless warned the prospective
student, “When he tries to look down his own throat with a candle in his hand, to
take care that he does not set his head on fire.”)2
Boole was equally unconventional. His father had been an impoverished
shoemaker who, nonetheless, had a deep interest in science and mathematics.
His mother was a lady’s maid. Boole’s formal education ended in the third grade,
yet, by the age of sixteen, he had already won a post as assistant master of a
school, where he taught Latin and Greek, among other subjects. By twenty-three,
he was publishing original mathematical research. His work won recognition
from the British Royal Society (founded in 1660 for the advancement of
science), and he then produced his Mathematical Analysis of Logic (1847),
which laid the basis of what we now call Boolean algebra. The effect of his
studies was to codify all the compounds treated earlier by the Stoic logician
Chrysippus into a notational system that could be manipulated mechanically
using the same methods by which one manipulates equations in an algebra class.
(We considered Chrysippus’s logic, now called propositional logic, back in
chapter 4.)
In addition, Boole’s techniques could capture a good deal of Aristotle.
(Aristotle’s logic, which is a logic of classification by way of categorical
syllogisms, was described in chapter 3.) All the same, Boole’s life, swift in its
beginnings, was unexpectedly brief. Despite his lack of a university degree,
Boole was appointed professor of mathematics at Queen’s College in the city of
Cork, Ireland, where he taught for fifteen years, but he then died suddenly after
insisting that he deliver a scheduled lecture even though his clothing had been
soaked in a rainstorm.
Working independently but from the same historical impulse, these two
curious professors took the crucial steps that would finally usher in a new world
of logical relations.
The distinguishing feature of a modern symbolic logic is that all parts of an
argument are rendered with symbols expressly designed for analysis. Of course,
the use of symbols as variables goes all the way back to Aristotle. (In “All As
are Bs,” A and B are symbolic variables.) With modern symbolic logic, however,
there are no remaining bits of English, Latin, Arabic, or Greek (the remaining
bits being mostly what the medievals called syncategorematic words, which help

to form a proposition or connect different propositions together but without
themselves being either subjects or predicates of a proposition). De Morgan’s
and Boole’s systems were different from each other, and neither had the scope
and power of the more far-reaching system later developed by the nineteenth-
century German logician Gottlob Frege or the power of the system developed
independently of Frege’s by the American philosopher C. S. Peirce.3
Nonetheless, De Morgan and Boole had shown the way. A categorical syllogism,
as expressed in the sort of notation now used in textbooks of symbolic logic,
might look like this:
(x)(Ax → Bx)
(x)(Bx → Cx)
__________________________
(x)(Ax → Cx)
Modern symbolic logic takes this as an expression of the English utterances,
All As are Bs.
All Bs are Cs.
Therefore, all As are Cs.
Of course, the symbols look complicated and difficult at first, but the basic idea
is to make the argument mechanical, and the symbolism is less intimidating once
one sees how the specific symbols can be translated:
(x)
can be read “for all x.”
Ax
can be read “x is an A.”
→
can be read to express “if-then.”
Bx
can be read “x is a B.”
Cx
can be read “x is a C.”
In addition, the long horizontal line can be rendered “Therefore.” As a result, the
whole symbolic argument can be read this way:
For all x, if x is an A, then x is a B.
For all x, if x is a B, then x is a C.
Therefore, for all x, if x is an A, then x is a C.
Symbolic logic treats this as an expression of “All As are Bs; all Bs are Cs;
therefore, all As are Cs.” (This translation holds good so long as the
interpretation of the initial English version is Boolean, meaning we make no

assumptions about whether the classes of As, Bs, and Cs have members; the As,
Bs, and Cs might stand for unicorns, griffins, or chimeras, whether or not such
creatures truly exist.)
Other symbols can be used in place of the ones given here (and the notational
systems of De Morgan and Boole were different; the notation we use now, with
some modifications, comes from a series of conventions worked out by the
English logicians Bertrand Russell and Alfred North Whitehead in their
Principia Mathematica, 1910–1913, and initially developed by the Italian
mathematician Giuseppe Peano).4 The important thing, however, is that, because
ordinary words are excluded, our knowledge of what any words mean is
excluded too. This is what makes symbolic logic fundamentally different from
the logic of the past.
With earlier sorts of logic, to determine whether the validity of an argument
had been proved, we still needed to understand the meanings of the words—at
least some of the words—and this meant we still needed to think. With symbolic
logic, by contrast, we don’t need to think at all, or rather the only thing we need
to think about is whether the symbols appear in the order specified by the rules
that govern them. In consequence, so long as the proof is sufficiently spelled out
in a symbolic language, the task of verifying it is strictly clerical. What we look
at is simply a matter of form—and purely form. The form is a pattern in the
placement of the symbols, and as a result, detecting the pattern’s presence has
nothing to do with knowing what any of the symbols signify. In this respect,
then, determining whether an argument’s validity has been proved within the
system is strictly mechanical, and in our age, it can certainly be done by
machines. (We should add that, among professional logicians, the notion of a
mechanical procedure is now more precise and nuanced than it was in the
nineteenth century.5 Nevertheless, what many nineteenth-century thinkers aimed
at was a system that would involve the repetition of clerical operations in the
manner of a machine, and symbolic logic made this possible.)
Algebra was moving in just this direction when De Morgan and Boole wrote;
it was becoming purely a calculus of symbols, independent of whether the
symbols stood for numbers, and so both thinkers saw the deep formal connection
between mathematics and logic. They saw that symbols could be manipulated to
solve logic problems just as symbols had long been manipulated to solve
problems in arithmetic. Of course, different scholars in different ages might have
arrived at this same insight if clever enough; Leibniz had already been thinking
along these lines. All the same, the increasing use of machinery in the nineteenth
century tended to emphasize the point. Why? Because nineteenth-century

machines showed the immense power of mechanical operations. Every section
of society had been touched in some way by this new and mysterious power, and
logicians saw this power too.
Much depends, to be sure, on precisely what one means by the words
“mechanical” and “mechanically,” and as it turns out, these are words on which
the nineteenth-century logicians often relied. For example, in his preface to The
Mathematical Analysis of Logic, Boole quotes John Stuart Mill on the
importance of the mechanical in logic: “Whenever the nature of the subject
permits the reasoning process to be without danger carried on mechanically, the
language should be constructed on as mechanical principles as possible.” (This
passage comes from Mill’s System of Logic, 1843, and Mill adds by way of
caution that, when the reasoning can’t be carried on safely in a mechanical way,
the mechanical approach should be avoided—and Boole agreed.)
As to Mill’s intention in using the word “mechanical,” Mill explains it as a
matter of limiting one’s thinking strictly to the placement and arrangement of
symbols and not being distracted by thoughts of what the symbols might mean:
The complete or extreme case of the mechanical use of language, is when it is used without any
consciousness of a meaning, and with only the consciousness of using certain visible or audible
marks in conformity to technical rules previously laid down. This extreme case is nowhere realized
except in the figures of arithmetic, and still more the symbols of algebra, a language unique in its
kind. . . . Its perfection consists in the completeness of its adaptation to a purely mechanical use. The
symbols are mere counters, without even the semblance of a meaning apart from the convention
which is renewed each time they are employed. . . . There is nothing, therefore, to distract the mind
from the set of mechanical operations which are to be performed upon the symbols.”6
Soon after Boole published, De Morgan wrote to him (again, in 1847) and
explained that his own system aimed, likewise, at a mechanical approach. De
Morgan remarked, “I have employed mechanical modes in making transitions
with a notation that represents our head work.”7 In short, the aim of this
mechanical approach was to make reasoning easier by reducing the mental labor
of thinking about the objects of the reasoning—and by replacing such thinking,
instead, with the manipulation of symbols according to fixed rules.
Now, in speaking here of the Industrial Revolution as an important historical
stimulus for this innovation in logic, we don’t mean to discount the role of
individual insight on the part of the logicians. Perhaps the most important insight
of all was one we shall come to shortly, the insight of Frege when he hit on the
idea of qualifying an entire symbolic assertion with a quantifying expression,
such as “for all x” or “for some x” (the technique called “quantification”).
Nevertheless, we do mean to assert that the Industrial Revolution was a cause of

symbolic logic’s development just as individual insight was a cause. By analogy,
the Battle of Waterloo depended on the insights, proclivities, and decisions of
specific individuals: of Napoleon, of the Duke of Wellington, and of many other
people. But the occurrence of the battle also resulted from the existence of much
larger social forces, forces that shaped British and French society and that led
these nations (and other nations) into the series of collisions we call the
Napoleonic Wars. Just so, the history of logic is made up of individual insights
by individual logicians, but it is also deeply influenced by social changes. To
treat logic’s history as if it were only a matter of individuals would be like
treating political history as if it were only a matter of individuals; it would be
like writing political history as if it were only a story of shrewd, villainous, or
heroic characters, with no social analysis.
When it comes to social changes, then, the most profound social change of the
nineteenth century was, by far, that millions of people witnessed the practical
power of mechanical operations as the Industrial Revolution unfolded. And it
was out of these millions that the logicians of the age were recruited. Intellectual
movements usually require various scholars learning from one another, but such
movements are typically sustained in the first place by things larger than
individuals. Many people can arrive at similar ideas at different times, but it
usually takes something common to them all to make them work in concert. The
Industrial Revolution served as an enormous suggestive influence, and it
persuaded both logicians and mathematicians alike that they might achieve a
more powerful result if they could summon the patience to construct a larger and
more complicated system, one in which questions of what had been proved and
what hadn’t could be answered mechanically. We can get a better sense of this
aspiration if we now look at how symbolic logic uses a more complicated
technique to get a more powerful result.
THE LOGIC OF RELATIONS
Symbolic logic’s most fundamental effect has been to bring together ever-larger
areas of valid reasoning under a single set of formalized rules. Once we see how
this effect is accomplished, we can also see why the techniques needed to make
it happen would chiefly appeal to an age already captivated by the immense
power of modern machinery.
For example, here is an argument that looks valid intuitively but that doesn’t
lend itself to the analysis of Aristotle or Chrysippus (meaning it is neither a
categorical syllogism nor a propositional form of the sort studied by

Chrysippus):
Tom is to the right of Dick.
Dick is to the right of Harry.
Therefore, Tom is to the right of Harry.
If the premises are true, the conclusion must also be true, and this remains so no
matter what names we substitute for Tom, Dick, or Harry. But the argument also
depends crucially on the expression “to the right of.” If we substitute something
different for that expression, the result is often invalid:
Tom is the uncle of Dick.
Dick is the uncle of Harry.
Therefore, Tom is the uncle of Harry.
In our everyday reasoning, we often link individuals to other individuals by
way of relations, like “to the right of”; nevertheless, these arguments are neither
categorical syllogisms nor the propositional structures of Chrysippus. And their
logic depends on which relations we employ. (Mathematics is full of such
relations: “is equal to,” “is greater than,” “is the square root of,” “is a factor of,”
and so on.) One of the things symbolic logic does, then, is capture this sort of
reasoning, and it does so in addition to capturing Aristotle’s syllogisms and
Chrysippus’s propositions. Symbolic logic achieves this synthesis—a grand
synthesis of the Aristotelian, the propositional, and the relational—through two
different maneuvers.
The first maneuver is to add an implicit premise that characterizes the
information we already have (and have tacitly assumed) about the relation “to
the right of.” We know what it means for something to be to the right of
something else, and we also know what this information implies if something in
addition should turn out to be to the right of that. So we try to capture this
information in a further premise. But we capture it in a particular way—by
employing a second maneuver—which is to frame this further premise as a
generalization that applies to any individuals we might name. If we put the two
maneuvers together, we get this:
For any individuals x, y, and z, if x is to the right of y, and y is to the right of z, then x is to the
right of z.
This is a generalization that characterizes the logical import of “to the right of”;
however, it also applies to any individuals, x, y, or z. If we then combine this
new premise with the two others we already have, we get the following
argument:

For any individuals x, y, and z, if x is to the right of y, and y is to the right of z, then x is to the
right of z.
The individual Tom is to the right of the individual Dick.
The individual Dick is to the right of the individual Harry.
Therefore, the individual Tom is to the right of the individual Harry.
Other relations can also be handled in this way: by capturing the specific relation
in a premise that holds good for any x, y, z, and so on.
It is easy to see why techniques like these would have become interesting
chiefly in an age of machines. The new argument we have just constructed is
rhetorically complicated, so complicated, in fact, that no thoughtful intellectual
of the ancient world would have spent much time with it. (An ancient gibe
against logicians of the past was that they always acted like men eating crabs,
dismantling the shell with a great deal of labor only so they could eat a tiny
morsel of meat.)8 Ancient logic always derived from public speaking, and no
ordinary audience in ancient times would have paid much attention to someone
who spoke in so cumbersome and convoluted a manner.
If we think back for a moment to Aristotle’s syllogisms or Chrysippus’s
common forms, what leaps out immediately is their rhetorical simplicity.
Aristotle’s categorical syllogisms never have more than three key terms, and the
classic forms explored by Chrysippus rarely use more than three elements in
forming their compound propositions. (The complex dilemma is the most
complicated of the traditional forms, and it stops at four elements: A, B, C, and
D.) Ancient logic, then, was close to everyday speaking and thinking, and
though our everyday reasoning certainly does make use of special relations like
“to the right of,” we usually manipulate these relations intuitively, without
expressing them or even thinking of them as involving a complicated general
premise that applies to any individuals, x, y, and z.
This is where nineteenth-century industrialization made such a difference.
Nineteenth-century logicians were much more willing to submit to the apparent
tedium of a new approach because it promised a further reward. If we approach
argumentation in an admittedly tedious but mechanical way, making explicit
everything we usually grasp intuitively, without analysis, we can construct a
system that captures much larger areas of valid argumentation, and we can bring
out its apparent form. It is as if we were building a complicated, steam-driven
loom. Building such a loom is laborious, and putting it together requires forging
some odd-looking components. If, having finally built the loom, we were then to
use it to manufacture only one piece of cloth, the whole exercise would be
pointless. Yet, if we brought the loom up to full speed and set it running for

prolonged periods, we would then be able to manufacture cloth in large
quantities, and we would have a machine of great power. Just so, building the
abstract system of symbolic logic requires tedious labor and odd components,
but if designed correctly, it can then express a vast array of theorems, and the
proposed proof of a theorem can be checked mechanically.
As it turns out, this same impulse toward mechanical procedures also
appeared in much nineteenth-century mathematics, and it was the mathematical
version of the impulse that had particular influence on logic.
THE EFFECT OF THE NEW MATHEMATICS
Nineteenth-century mathematics is certainly a vast subject, but one of its chief
tendencies was an emphasis on new axiomatic systems. Of course, Euclid’s
axioms for geometry go back to the fourth or third century B.C. But the axiomatic
systems of the nineteenth century were different in that their starting points were
more difficult and therefore less intuitive.
The axioms of nineteenth-century mathematics required greater mental
abstraction. To be sure, Euclid’s starting points require attention; they, too,
demand a certain measure of abstraction, and before we can even enter into
Euclid’s world, we must first realize that a point, by definition, has no parts and
that a line, by definition, has no breadth. Even to think of these things, we must
partly abstract from our ordinary experience of three-dimensional objects. But
Euclid’s basic ideas are not so deep that they elude the grasp of ordinary, literate
citizens; on the contrary, Euclidean geometry has been a traditional part of a
liberal education for more than a thousand years. (Boethius included geometry
among the seven liberal arts back in the sixth century A.D. and so did Martianus
Capella in the previous century.)
By contrast, when it comes to the nineteenth century’s axiomatic systems, the
starting points are harder. Arithmetic had been carried on for thousands of years
without axioms and without any general, systematic proof that its procedures
were sound. Only in 1830 did this situation start to change, when the English
mathematician George Peacock published the first part of his Treatise on
Algebra. And for the rest of the century, mathematical logicians closed in on an
axiomatic arithmetic that finally emerged in the work of Giuseppe Peano.
Peano’s axioms can now be used to prove that two and three do indeed make
five, but knowing the sum intuitively, or knowing it by counting it out on one’s
fingers, is easier than proving it from the axioms. (The axioms are not

insuperable, but one must first master what is called the “successor” function,
and one must also master the idea of mathematical induction.)
As a result, whereas Euclid’s geometry generally goes from intuitive ideas as
its starting points and then reasons to a series of complicated and elaborate
theorems, nineteenth-century systems tended to start with complicated ideas at
the outset and then continue forward from there. In present-day mathematics,
foundational axioms and principles are matters of great abstraction and delicacy
(often involving set theory), and though they can be used to prove simple
equations of arithmetic, they can also be used to prove deep theorems about
infinities. Despite the increasing abstractness of the axioms, however, these
nineteenth-century systems also made it easier to assess the correctness of a
mathematical proof through the mechanical manipulation of symbols.
Throughout the nineteenth century, this delicate relationship between logic
and mathematics was complex and varied, and it sometimes seemed to go back
and forth in different directions. Boole, for example, was trying to solve logic
problems by using the methods of mathematics (specifically, those of algebra),
whereas Frege, working later in the century, was trying to reduce parts of
mathematics to logic itself. Nevertheless, in both logic and mathematics,
thinkers of the period were fashioning complicated systems out of obscure and
odd-looking components, and they were trying to devise an intelligent way of
manipulating symbols so that a wide range of their fields’ questions could be put
to rest in the unthinking manner of a machine.
As it happened, mathematicians of the period often took particular interest in
machines that could do mathematical calculations. De Morgan witnessed a
demonstration of Thomas Fowler’s ternary (or “three-valued”) calculating
machine in 1840 and left behind a detailed description of it. He was also a friend
of the mathematician and inventor Charles Babbage and of Ada Lovelace, both
of whom collaborated on the plan for the “Analytical Engine,” an immense
steam-powered mechanical computer that was never built. The Analytical
Engine would have consisted of thousands of metal moving parts, and it might
have been the size of a railroad locomotive. (Most experts today say the machine
was simply impossible to construct given the technology of the age, and some
have suggested that the power plant alone might have shaken the rest of the
engine to pieces.) Ada Lovelace, who had been De Morgan’s student, worked
out much of the theory behind the machine, and she said its inspiration came
from Babbage’s earlier and more limited plan for an arithmetical calculating
machine (the “Difference Engine”) and from a still-earlier invention in France of
a programmable loom. Back in 1801, the French weaver Joseph Marie Jacquard

had used punched pasteboard cards to control the operation of a mechanical
loom that wove intricate designs in silk.
As Lady Lovelace wrote in 1842, “We may say most aptly that the Analytical
Engine weaves algebraical patterns just as the Jacquard-loom weaves flowers
and leaves.” She calls the Analytical Engine “the material and mechanical
representative” of mathematical analysis. She adds, “In enabling mechanism to
combine together general symbols in successions of unlimited variety and
extent, a uniting link is established between the operations of matter and the
abstract mental processes of the most abstract branch of mathematical science. A
new, a vast, and a powerful language is developed for the future use of analysis
in which to wield its truths.”9
Of course, in speaking here of machines, we don’t mean to suggest that
mathematicians and logicians of the period typically wanted their work to be
done by machines (though Lovelace and Babbage certainly had some such
project in mind).10 On the contrary, the dominant aim behind the new axiomatic
systems of the nineteenth century was to increase mathematical and logical
certainty; mathematicians and logicians alike wanted to purge their disciplines as
much as they could of the possibility of error.11 But their route to this certainty
was by way of mechanical procedures, and this was because they saw the
mechanical as essentially rote and unthinking. Much of this impulse came from
the observation that the mechanical could be carried on with less awareness—or
with less “consciousness,” to use Mill’s word—of the things one was reasoning
about. The less the verification of a proof needed you to be conscious of
anything other than the placement of the symbols, the easier it would be to detect
mistakes.
In fact, this was Leibniz’s idea back in the seventeenth century: make
mistakes in reasoning more easily detectable. As Leibniz expressed this view in
1685, “The only way to rectify our reasonings is to make them as tangible as
those of the Mathematicians, so that we can find our error at a glance, and when
there are disputes among persons, we can simply say: Let us calculate
[calculemus], without further ado, to see who is right.”12
Leibniz’s aim? To find an error “at a glance.” This was the aspiration that was
to recur in the work of nineteenth-century logicians. Their basic strategy was to
reduce the thought required to detect errors and to achieve this result by
increasing the thought that went into a logical system’s design.
In many ways, this approach mirrored nineteenth-century industry. The
essence of nineteenth-century industry was the clever and carefully thought-out

design of operations that, in themselves, were thoughtless. The machines
repeated a series of otherwise insignificant motions, but because the motions
were cleverly arranged, the results were highly significant. The machines did
what they did even though none of the machines could think. Simple, repeated
movements were carefully combined in a complex system.
Symbolic logic does this too. Given a clever arrangement of rules for the
symbols, we can express proofs for a large array of theorems. But when it comes
to assuring that the proofs are correct, the symbols might mean nothing at all;
what matters is their arrangement. The power is in the patterns, and what makes
these otherwise meaningless patterns significant is that they do indeed resemble
the structures that we discover in our thinking, the structures we call reasoning
(or, to use De Morgan’s term, they resemble our “head work”). What the
Industrial Revolution did, then, was show in a spectacular way how unthinking
things could be cleverly arranged to achieve an intelligent result. (In our time,
many observers believe some machines, such as electronic computers, do indeed
think, provided that the machines are sophisticated enough; but this is an idea we
shall come to shortly.)
To an outsider, these nineteenth-century pioneers in logic and mathematics
might almost look like a collection of mad scientists who scribble down weird
ciphers and speak in languages no one else can understand. Their starting points
were increasingly remote from ordinary intuition, and few of their fellow
citizens could know what they were up to. (As for Babbage, in his later years,
some people thought he was indeed mad, though it seems more probable that he
just became cranky. He tried to mount a public campaign against street
musicians, especially organ grinders, because he objected to the sound, and he
used to drive away children from his home because he regarded them as noisy;
the children retaliated by collecting before his home and imitating the sounds of
street musicians. While still a young student at Cambridge, Babbage had joined
the Extractors Club, devoted to extracting its members from the madhouse in
case any should be involuntarily committed.) All these thinkers were giving to
their complicated efforts the same sustained and minute attention that the
engineers of the period were giving to industrial production, even when the
potential benefits of these undertakings were obvious only to a few.
To be sure, much the same that we now say about nineteenth-century logic
and mathematics could also be said about the work of any scientist in any
historical period; scientists often work on things that few other people
understand, and they often work with demonic energy. The sheer power of
science, however, was far more obvious to the generations of the nineteenth

century than to those of earlier times. As a result, there were many more people
who were convinced that these mechanical efforts could prove fruitful. Millions
of people saw what complicated industrial systems might do, and out of these
millions, the mathematicians and logicians of the age emerged.
The historical mechanism we now propose is, once more, necessarily
conjectural. We can’t go back and ask the thinkers of the period, “Would you
ever have invested such time and energy in these complicated mathematical and
logical systems if you had never seen the immense power of complicated
machines?” And even if we could go back and ask this question, the thinkers
themselves might simply not know the answer. Nevertheless, the correlation
between industrialization on the one hand and the development of abstract
algebras and symbolic logic on the other is striking. Peacock, Boole, and De
Morgan all came from England during a period of intense industrialization, and
later in the century, as Germany industrialized, one sees the eminent German
figures of Frege, Georg Cantor (the inventor of set theory), and Richard
Dedekind (an important contributor to algebra and set theory). (As one might
expect, all these thinkers worked in universities, which increasingly supported
the advancement of industry through science.) As for Peano, he taught for most
of his career at the University of Turin, in Italy, and he, too, flourished at a time
of increasing industrial development. (While he was teaching in Turin, the
Automobile Factory of Turin, whose acronym in Italian is “FIAT,” began to
build its first automobiles.) And in America, as the United States industrialized
in the wake of the Civil War, the philosopher C. S. Peirce worked out a
complicated symbolic logic of his own, and in 1886 he foresaw the possibility
that mathematical and logical problems might be solved by machines that used
electrical switches.13
Certainly these same thinkers pursued many different lines of inquiry, and
some of them also entertained deep divisions of opinion (especially concerning
the foundations of mathematics). The mere fact of industrialization no more
entailed the existence of any of these more particular developments in logic and
mathematics than it entailed the existence of more particular developments in the
history of politics. Nevertheless, industrialization was a crucial background
condition in both logic and politics, and in the case of mathematics, the
development of symbolic logic, spurred by industrialization, meant that many of
the most important twentieth-century debates in the philosophy of mathematics
were conducted squarely within the framework of fully symbolic systems.14
In sum, science, system, and mechanical procedures all became recurring
themes of nineteenth-century industry, and they likewise became powerful

recurring themes in mathematics and logic. All the same, this mechanical
tendency in logic (whatever its causes) would have been far less effective were it
were not for the ability to capture, in symbols, the logic of relations. And what
finally made this possible was Frege’s invention of quantification.
THE IMPACT OF QUANTIFICATION
The essence of quantification is to express a proposition with variables and yet
qualify this proposition by saying exactly how many things can stand in the
place of these variables. When we say,
if x is to the right of y, and y is to the right of z, then x is to the right of z,
we say something with variables; yet we haven’t said just how many things can
stand in the places of x, y, and z. The utterance still has no quantification. We
quantify the utterance by adding an initial phrase,
For any x, y, and z . . .
This phrase applies to the whole utterance, and now we can say (in the parlance
of logicians) that the phrase “binds” the variables; it tells us how many things
can stand in their place. It means that, any time the utterance uses the variable x,
y, or z, the usage applies to any x, y, or z. Symbolic logic attaches these
quantifying phrases as prefixes, and in effect, Frege used two different
quantifying phrases: (a) “For any x,” and (b) “For some x.” The first of these
phrases can also be read, “For all x,” and the second can also be read, “There
exists an x such that” or “There is at least one x such that.” (Notice that, in all
these uses, we read “some” to mean “at least one.”)15
A common symbolism for these prefixes, as it now appears in many logic
textbooks, looks like this:
(x)
“For all x . . .”
(
x)
“There exists an x such that . . .”
Suppose, now, that we define another symbol to stand for the relation “to the
right of”:
R
can be read “to the right of”

Also, suppose we establish a way of symbolizing what is to the right of what:
Rxy
can be read “x is to the right of y”
We need a few more elements here to complete the project. First, we need a way
to indicate exactly how far down a string of symbols a quantifier is supposed to
apply, and this is done by brackets or parentheses. Thus,
(
x)(
y)(Rxy)
or
(
x)(
y)[Rxy]
This means that there exist an x and a y such that x is to the right of y. For
convenience, it will also be useful to add symbols for conjunction and
disjunction. (We can add these symbols in addition to the one we supplied just a
moment ago for “if-then,” →, even though we already know from chapter 4 that
each of these operations can be defined in terms of the others as long as we add a
symbol for negation.)16 For convenience, then, we can add these further symbols:
&
can be read “and”
v
can be read “or”
→
can be read to express “if-then”
~
can be read to express “it is not the case that”
For example,
Rxy & Ryz
can be read “x is to the right of y, and y is to the right of z.”
Rxy v Ryz
can be read “x is to the right of y, or y is to the right of z.”
Rxy → Ryz
can be read “if x is to the right of y, then y is to the right of z.”
~Rxy
can be read “it is not the case that x is to the right of y.”
Once all this is done, we are ready to symbolize the first premise of the argument
about Tom, Dick, and Harry, the one that expresses the tacit, general principle
that governs how the relation “to the right of” actually works. We can symbolize
it in the following way:
(x)(y)(z) [(Rxy & Ryz) → Rxz]
“For any x, y, and z, if x is to the right of y, and y is to the right of z, then x is to the right of z.”

To complete the argument, all we need now are symbols to stand for the
individuals Tom, Dick, and Harry; these symbols are called constants. Thus,
t
can be read “Tom”
d
can be read “Dick”
h
can be read “Harry”
Putting it all together, we get this:
(x)(y)(z) [(Rxy & Ryz) → Rxz]
Rtd
Rdh
__________________________
Rth
“For any x, y, and z, if x is to the right of y, and y is to the right of z, then x is to the right of z.
Tom is to the right of Dick.
Dick is to the right of Harry.
Therefore, Tom is to the right of Harry.”
Of course, to make the system mechanical and strictly clerical, we also need
the crucial rules that tell us when we can validly infer a conclusion from its
premises and when we can’t—some rules of inference that we know to be valid,
like modus ponens or modus tollens. (These rules will also need to include a way
of passing from statements with variables to statements with constants, and vice
versa.) We shall also need rules to tell us when a string of symbols is to count as
a genuine statement within this system and when it is merely gibberish; these are
the “formation rules” that tell us what counts as a “well-formed formula.” Frege
did all this, but he did one crucial thing more: he constructed his system with the
aim of turning at least part of mathematics into a form of logic.
FREGE’S NEW FOUNDATION FOR MATHEMATICS
Frege’s idea was that, if arithmetical objects like numbers could be defined in
terms of some basic notions of logic and expressed in a precise symbolism, then
perhaps all arithmetical truths could be shown to follow from a few logical rules
and assumptions. This is the thesis (still controversial) called “logicism,”
according to which at least some parts of mathematics, if not all, are reducible to
logic. (Frege focused on arithmetic, but other logicians, like Russell and
Whitehead, have tried to cast the net farther.)

Why undertake this exercise in the first place? Frege answered that doing so
would increase the certainty that our current mathematical inferences are correct.
This was all part of his grand scheme to place mathematics on firmer
foundations, and he had already pointed to examples of mathematical reasoning
that had run off the rails in earlier times because mathematicians had relied on
something that had seemed obvious, intuitively, but without carefully deducing it
from earlier premises. Frege wrote, “In making a transition to a new assertion
one must not be content, as up to now mathematicians have practically always
been, with its appearing obviously right, but rather one must analyze it into the
simple logical steps of which it consists—and often these will be more than a
few.”17
In effect, Frege imposed a new and highly demanding conception of what
should count as a formal proof, either in mathematics or logic. For Frege, a
proof had to be expressed in abstract symbols whose form and logical
implications were governed by a set of rules that had already been specified in
advance. This conception, generally called formalization, has dominated
mathematics and symbolic logic ever since. But he then wanted to apply this
new approach to achieve a further goal: he wanted to show how, according to his
new, formalized method, arithmetic could be reduced to logic.
Not everything went the way Frege had hoped. Such a system would have to
be internally consistent; its rules and axioms18 couldn’t result in contradictions if
the system was to be logical in the first place. Yet the system’s rules and axioms
would also have to be powerful enough to entail all the theorems that need
proving; we don’t want a system that proves a few things but whose axioms are
so weak that they leave many other truths unproved. And in assessing this
further quality, the power of the system’s axioms, logicians now ask if such a
system is “complete.”
That is, can every logical truth expressible according to the formation rules
actually be proved from the axioms? If so, the axioms are “complete”; otherwise,
they are not. Frege devised a notation that allowed variables to stand in for
individuals, and these individuals could be represented as having properties or
relations (the properties or relations being symbolized by “predicates”). The
resulting system (“first-order predicate calculus”) turned out, on later
examination, to be both consistent and complete. All logical truths expressible in
the system were shown to be provable, and the makings of the system generated
no contradictions.19 On the other hand, to make the system strong enough to
capture arithmetic, Frege had to go a step further: he resorted to variables that
applied to relations and properties (called “predicate variables,” thereby

generating a “higher-order logic”), and Kurt Gödel showed in 1931 that no such
system could be proved to be both consistent and complete. More exactly, no
system rich enough to include arithmetic as a consequence can be shown within
that system to be both consistent and complete. Some statements of the system
will have to remain unprovable, or if provable, the system will be inconsistent.
(This is one of the paradoxes of formalization that we noted in chapter 6.)
Instead, mathematicians can show that such a system for arithmetic is
consistent and complete if the systems for various other branches of mathematics
are, and vice versa; but these proofs of consistency are “relative,” meaning that,
at most, they tie one branch of mathematics to another. One branch is thus
consistent relative to another; all the same, none of these proofs is “absolute,”
meaning that none of them ties these properties of a mathematical system to
logic alone. But what is the practical import of this research?
As a practical matter, formalization has increased the confidence that
mathematicians and logicians now feel in the more abstract reaches of their
disciplines. Most areas of mathematics are now formalized by tying them to
Frege’s basic system (to first-order predicate calculus, as modified by Russell
and Whitehead) along with a version of set theory. Paradoxes (formally called
“antinomies”) can still arise, but mathematicians have acquired considerable
experience in anticipating these paradoxes and have a variety of specific devices
for heading them off.
On the other hand, no technique of formalization seems likely to alter our
rational confidence in something so basic as modus ponens or the disjunctive
syllogism when used as a form of argument in ordinary cases. (Exotic cases are a
different matter, and these are the areas where classical and nonclassical logics
sometimes clash.) The reason is simple. If such a system ever did contradict
either of these two basic methods in ordinary cases (either modus ponens or the
disjunctive syllogism), it would be more reasonable to suspect the system than to
suspect the method. Such systems are delicate from the start, and they are
vulnerable to many kinds of errors in their construction. Rather than doubt the
validity of ordinary instances of modus ponens, the more reasonable approach
would be to doubt the complexities of a particular construction.20
Yet symbolic logic has had another practical impact too—in fact, a colossal
one that now affects us every day: the digital computer.
THE INVENTION OF DIGITAL COMPUTING

Just as machines encouraged the development of symbolic logic, so symbolic
logic finally encouraged the building of new machines—machines to manipulate
the symbols. And the first precise description of a programmable digital
computer in its modern form (different from that of Babbage and Lovelace)
came from someone utterly steeped in formalized logic, the English
mathematician Alan Turing.
Turing hit on the idea in the 1930s, when he was still in his twenties. Turing
had studied mathematics and logic at Cambridge (where, later, he would also
study briefly with Wittgenstein) and then at Princeton University in the United
States. He conceived of a computer as a machine that would inscribe or erase
symbols according to an established set of mechanical rules. The rules would
require different actions depending on the internal state of the machine at any
one time, and they would determine just how the machine would react to each
new symbol that was fed into it. So imagined, the machine would change
internally as it read new symbols, but only according to the rules, and these rules
would, in turn, dictate what new symbols the machine would print out. In
consequence, the machine’s output would be a complex result of both the rules
and its own internal state, as altered by the symbols going in. This is exactly
what we see in a programmable computer today.
The symbols going in correspond to your keystrokes on a keyboard; the
symbols the machine prints out correspond to the images you see on a screen.
And Turing’s mechanical rules correspond to the software program your
computer happens to be running. But one can also understand the process from
another standpoint—from the standpoint of symbolic logic as embodied in a
formalized system.
The computer’s program corresponds to the rules of inference that govern a
system like Frege’s; the program allows one set of symbols (the inputs) to be
translated as logical implications into another set of symbols (the outputs). (The
program thus operates like modus ponens, which allows one proposition to be
inferred logically from another.) At the same time, the specific computer
language the programmer uses to write the program corresponds to the formation
rules of Frege; for Frege, the formation rules tell us what counts as a well-
formed formula and what doesn’t, and they govern what propositions the logical
system can accommodate.
Just so, the rules of the computer language tell us what counts as a genuine
statement within the system and what counts as gibberish. In effect, then, every
programmer does, on a smaller scale, the same thing Frege was seeking to do for
logic as a whole. Every programmer lays down the equivalent of inference rules

that allow the deducing of some propositions from other propositions, and once
the computer is programmed, the operator’s keystrokes then correspond to the
premises of an argument (the axioms and postulates of a formalized system), and
the outputs on the screen correspond to a series of logical conclusions (the
system’s theorems).
Turing worked out his idea for this kind of computing even before the
electronic components to put it into practice had been invented. And his guiding
thread was always the notion that machines could be made to think. He believed
there were few real differences between the intellectual life of human beings and
the behavior of machines—as long as the machines could be made sophisticated
enough. As a result, his aim was to make the machines sophisticated. (Turing
advanced this last idea—that machines think—by way of a thesis now called the
“Turing test.” In his view, the true test of whether a machine thinks is whether it
can generate results that are indistinguishable from those of human beings. If the
machine seems to reason as well as we do, then it thinks as much as we do.)21
The proposition that machines can think still motivates much scientific and
philosophical research, but in one crucial respect, we believe Turing’s approach
was mistaken—in assuming that, in designing a reasoning machine, one is
therefore designing a thinking machine. Reasoning isn’t necessarily a form of
thinking, and we believe that in supposing otherwise Turing was misled by a
fundamental confusion about human behavior. In fact, even when human beings
reason quite logically and skillfully, they aren’t necessarily thinking—or else
they are thinking very little.
To explain:
Consider, first, what happens in a machine. It seems obvious that a modern
computer can be programmed to evaluate many forms of argument, and based on
this evaluation, it can print out a list of which forms are valid and which are
not.22 Of course, if any human being could work out such a list, we would
certainly call it “reasoning.” On the surface, at least, it seems no more strange to
say that machines that evaluate reasoning are indeed reasoning than it does to
say that adding machines add or sewing machines sew.
On the other hand, the machines do all this quite mechanically by rigid
adherence to rules, and the point we commonly forget is that, if we could do it
mechanically by rigid adherence to rules, we wouldn’t call it thinking. On the
contrary, we would call it reflex.
Consider: we do many things unthinkingly—breathing, standing, walking,
sometimes even talking—yet the point we often overlook is that one of the chief

purposes of logical technique at its best is to reduce the evaluation of arguments
to the same sort of unthinking behavior. This was symbolic logic’s purpose from
the start. The whole idea was to replace behavior that required a great deal of
thought with other human behavior that required much less thought. Symbolic
logic was intended from the outset as a labor-saving device, just as a machine
was a labor-saving device. The labor to be saved was the labor of thinking. The
aim was to escape the difficulties of thinking through intractable disputes about
what was valid and what was not.
Alfred North Whitehead (Russell’s collaborator) once expressed this
aspiration very ably: “By the aid of symbolism, we can make transitions in
reasoning almost mechanically by the eye, which otherwise would call into play
the higher faculties of the brain.” Whitehead stressed the difference between
thinking behavior and skillfully mechanical behavior: “It is a profoundly
erroneous truism . . . that we should cultivate the habit of thinking of what we
are doing. The precise opposite is the case. Civilization advances by extending
the number of important operations which we can perform without thinking
about them. Operations of thought are like cavalry charges—they are strictly
limited in number, they require fresh horses, and must only be made at decisive
moments.”23
In fact, one of the chief aims of logic in its systematic development is to
render logical judgments as close to brute reflex as possible. This is why we
study forms. Forms leap out at the eye. But once we spot them, the thought
required is immediately reduced because we know what to do. We know which
rules to apply. To say this isn’t to say that logic makes us unthinking, but only
that it saves our thinking for other matters—not for determining the mere
cogency of arguments but for anticipating where the arguments are going or why
they exist at all. In this respect, then, logic is like walking. The better at it we
get, the less we need to think about it and the farther it takes us—to the
contemplation of new vistas.
This is why some people reason much better than others; they have trained
themselves to reason in this reflexive way. They have taught themselves to be
aware of arguments as arguments and to look for logical structure, for premises
and conclusions. But the more they train themselves in this endeavor, the less
conscious it becomes. Eventually, it is nearly thoughtless. It becomes like typing
(the better you type, the less conscious you are of the individual letters) or like
making music (the better the musician, the less the musician thinks of fingerings
and bow strokes and the more the musician contemplates pathos or radiance).
Thus, to put the point about machines in another way, the argument that

machines think usually rests on an analogy to human behavior, but the analogy
misstates what human beings really do. We say to ourselves, “If I had to do the
calculation the machine just did, I’d certainly have to concentrate very hard.”
But of course if you had the wiring the machine has, you wouldn’t concentrate at
all. You could do it without thinking.
The analogy to human behavior, then, supports the opposite inference: not that
machines think, but that they don’t think, because, when human reasoning is
most mechanical, it involves the least conscious thought. (Machines behave
mechanically, and when we behave mechanically, we don’t think either.)24
However this may be (and perhaps we are wrong and Turing was right after
all), Turing was steadfast in his attempts to develop mechanical reasoning, and
his explorations also turned out to have a profound effect on World War II.
After studying at Princeton, Turing returned to England and shortly afterward
joined 
Britain’s 
secret 
code-breaking 
effort 
at 
Bletchley 
Park 
in
Buckinghamshire, a crucial operation during the war. Turing devised rapid
mechanical methods for deciphering the coded messages of the German Enigma
machine, the German military’s chief coding device, and his insights allowed
British and American ships to avoid German U-boats for prolonged periods
during the Battle of the Atlantic. Turing, in fact, saved many sailors and allied
ships and helped to undermine the U-boat campaign. For his services during the
war, Turing was awarded the Order of the British Empire, but he was later
arrested by British police for “gross indecency” when he admitted candidly that
he had had a homosexual affair. To avoid prison, Turing agreed to injections of
the hormone estrogen, which rendered him impotent, and in June of 1954, he
committed suicide. Such was the end of a man who did far more than most to
help shape the future of the world.
Where the brave new world of computers will finally take us is still unclear, no
less unclear than the final outcome of the Industrial Revolution itself. The
building of intricate machines over the last two centuries has led, in turn, to the
building of intricate logical systems, and the logical systems have turned out to
have consequences of their own that are ever harder to foresee. Logicians, like
everyone else, have been carried along with the tide.
Much of the earth is still being changed by this process, and even in the early
days of industrialism, many thoughtful citizens, especially of a romantic bent,
had wondered whether the inexorable march of analysis and invention might
actually do more harm than good. The Industrial Revolution brought

conveniences, medicines, better food, and mighty towers. But it also created
blighted landscapes, fouled streams, dying seas, and terrifying new weapons.
Many people now have similar misgivings about the ultimate effects of
computers.
Strangely, the same questions we now pose about the beneficial or harmful
effects of industrialization and digital technology can also be posed about logic.
Does logic truly make us better, or might it make us worse? Will it do us good,
or might it do us harm?
Such questions can be asked today, but they were also asked nearly a thousand
years ago in medieval France, especially when medieval thinkers tried to sort out
the competing claims of reason and faith. And so in considering these last,
troubling issues, we propose to turn back to a medieval version of these same
questions as expressed in the tragic love story of the great, twelfth-century
logician Peter Abelard—and his ardent student Heloise.

10
FAITH AND THE LIMITS OF LOGIC
The Last Unanswered Question
MANY MEDIEVAL thinkers found logic useful and edifying, but others found it
distressing because its consequences seemed to challenge their faith. And for a
time, the greatest of all such logical challenges came from Peter Abelard, who
was young, shrewd, frequently arrogant, and deeply threatening to religious
conservatives in twelfth-century France.
Abelard applied himself to many philosophical controversies, especially to
what philosophers call the problem of universals, but he always styled himself
first and foremost a “dialectician,” which was the medieval term for logician.1
And the basic problem for Abelard and his followers was that they were being
asked by Roman Catholic authorities to believe in doctrines whether the
doctrines seemed reasonable or not.
Abelard’s investigations caused many people to wonder whether reasoning
might undermine faith—and whether faith was even rational at all. His efforts
led his opponents to argue that logic makes us worse as human beings rather
than better. Abelard was committed to finding the true foundations of rational
belief, but the further question raised by his approach was whether these
foundations, whatever they were, would finally leave any room for God. Many
medievals asked, Can logic ever tell us what the proper basis of a reasonable
person’s convictions should be? And can logic tell us whether a rational human
being, without reasons, might ever believe in something strictly as a matter of
faith? More generally, could logic explain to the deeply religious world of the
Middle Ages whether a religious person was any more or less reasonable than a
nonreligious person? Such questions vexed thinkers of the medieval world, and
they have continued to vex thinkers down to the present.
ABELARD’S RISE TO POWER
Abelard had been born into the Breton nobility in 1079, but he had renounced
his hereditary right as eldest son of a landed knight and had chosen instead to

take up what he called the “weapons of dialectic.” Treating logical techniques as
if they were tools of combat, Abelard wrote, “Armed with these, I chose the
conflicts of disputation instead of the trophies of war.”2
There were as yet no universities in Northern Europe, and so Abelard entered
the cathedral school of Notre Dame at Paris, where he proved himself both
clever and combative. He challenged his teachers and often embarrassed them by
refuting them before other students, and in reaction his principal teacher tried to
block his advancement. Nevertheless, Abelard succeeded in attracting so much
attention with his questions and assertions that he was soon able to set up a
school of his own. (This was not unusual in his day; in Abelard’s time, a popular
teacher might found his own school if enough students would follow him.)
Abelard established a school at Melun and then moved it to Corbeil near Paris;
later, he moved it again to Mont Ste. Geneviève on the left bank of the Seine, the
main site of today’s University of Paris. From these posts (as he wrote later), “I
could embarrass him [his principal teacher] through more frequent encounters in
disputation” (59).
After various rhetorical confrontations, Abelard finally ended up heading the
school of Notre Dame himself, and though still quite young for the position, he
was eventually regarded as the greatest living teacher in Europe. He became the
observed of all observers: handsome, well spoken, intelligent, daring. He adds,
“But success always puffs up fools with pride. . . . I began to think myself the
only philosopher in the world.”
According to his own account, written years later and in great humility, he
decided to seduce a young girl. He had had no earlier sexual experience, but now
he fixed his eye on the niece of one of the cathedral’s canons, a young woman
named Heloise, then about seventeen years old.
Heloise was highly unusual for a woman of her time: she could read and
write, and she had studied Latin classics. Abelard offered to tutor her in
exchange for room and board in the home where she and the uncle lived, and the
uncle accepted. Abelard writes, “He gave me complete charge over the girl so
that I could devote all the leisure time left me by my school to teaching her by
day and night, and if I found her idle I was to punish her severely. I was amazed
by his simplicity. . . . In handing her over to me to punish as well as to teach,
what else was he doing but giving me complete freedom to realize my desires”
(67).
Heloise seems to have accepted his advances willingly. Soon, says Abelard,
though “our books [were] open before us, more words of love than of reading

passed between us.” To disguise the situation from the uncle, they sometimes
acted out beatings, and he passed many nights with her. In the words of the
scholar and translator Betty Radice, their amorous relations became “uninhibited
and ecstatic.” Eventually, however, they were discovered, and Heloise soon
realized she was pregnant.
Abelard had her spirited away to his family estate in Brittany, which was then
politically independent of France, and the uncle, as might be expected, was
enraged. But Abelard then apologized to him abjectly and offered a secret
marriage, to which the uncle agreed. The purpose of the marriage was to
preserve the honor of Heloise and her family, and the purpose of keeping it
secret was apparently to allow Abelard to continue his work. (In Abelard’s day,
the leader of a school might indeed marry, but not if he wished to advance in the
church hierarchy, which, at the time, dominated education.)
Even so, Heloise argued that marriage would be a mistake; her uncle would
publicize the marriage anyway, she predicted, to assuage his wounded pride. As
for her own wishes, she said she wanted no marriage bond. According to
Abelard, Heloise argued that “only love freely given should keep me for her, not
the constriction of a marriage tie” (74). As she later described her view, “The
name of wife may seem more sacred or more binding, but sweeter for me will
always be the word mistress” (113). She wanted Abelard’s work to continue, she
said, believing it to be important, and she argued that marriage would be an
impediment. (Her arguments against marriage seemed to focus not on the
possibility that marriage would block his advancement in the church but on the
idea that marriage would interfere with his ability to philosophize.)3
Heloise bore a son, whom she named Astrolabe (the word for the Arab device,
then new to Europe, for locating one’s longitude, provided that one knows the
correct time, and thus one of the great scientific instruments of the age). Yet, just
as Heloise had predicted, the uncle eventually made the marriage known
anyway, and Abelard’s reaction was swift. He removed Heloise from Paris and
had her disguised as a nun at the nearby convent of Argenteuil. His purpose was
not to put her away but only to reestablish the fiction that he was unmarried. All
the same, his decision proved to be disastrous.
While Heloise remained at the convent, Abelard visited her and made love to
her secretly while the real nuns were away celebrating Mass. But when the uncle
learned that his niece had been carried away a second time—now to a nunnery—
he drew a different conclusion: he supposed that Abelard was simply discarding
her by forcing her into a habit. He thought Abelard was making her a nun against
her will, and in reaction, the uncle plotted his revenge. He vented his anger to his

friends, and the friends then conspired to find Abelard in his rooms in the middle
of the night and to castrate him.
With this astounding act of vengeance, Abelard was, for a time, spiritually
broken. He withdrew into himself, and he said later that what most troubled him
was not the physical injury but the humiliation: “I thought how fast the news of
this unheard-of disgrace would spread over the whole world.” Heloise was
equally stricken. As for the attackers, those who were caught were themselves
blinded and castrated on orders of the king of France.4
After a time, asking himself over and over what he should make of his life,
Abelard decided to withdraw from the world by becoming a monk at the Abbey
of St. Denis. But he wanted Heloise to withdraw first by becoming a nun. His
motive in this request? He never says, but he remarks to her later in describing
their marriage, “I desired to keep you whom I loved beyond measure for myself
alone.”5 When he demanded that she become a nun, Heloise was only nineteen,
and she had known Abelard for about a year and a half. Still, she acceded to his
request and took vows that would bind her to a convent for the rest of her life.
She later wrote,
I have carried out all your orders so implicitly that when I was powerless to oppose you in anything, I
found strength at your command to destroy myself. . . . At your bidding I changed my clothing along
with my mind, in order to prove you the sole possessor of my body and my will alike. . . . I believed
the more I humbled myself on your account, the more gratitude I should win from you. . . . I have
finally denied myself every pleasure in obedience to your will. (113–17)
Heloise wrote these words in a letter to Abelard many years later, a letter once
thought to be the forgery of some subsequent poet but now regarded by most
scholars as authentic.6 In a further letter, she asks Abelard to consider her
situation as prioress, the administrator of a convent of other nuns:
If I truthfully admit to the weakness of my unhappy soul, I can find no penitence whereby to appease
God. . . . The pleasures of lovers . . . can scarcely be banished from my thoughts. Wherever I turn
they are always there before my eyes, bringing with them awakened longings and fantasies which
will not let me sleep. . . . Everything we did and also the times and places are stamped on my heart
along with your image, so that I live through it all again with you. Even in sleep I know no respite.
Sometimes my thoughts are betrayed in a movement of my body, or they break out in an unguarded
word. (132–33)
She was particularly troubled by these thoughts, she says, during celebration of
the Mass. Referring to his castration, she remarks, “This grace, my dearest, came
upon you unsought—a single wound of the body by freeing you from these
torments has healed many wounds in your soul. . . . But for me, youth and
passion and experience of pleasures which were so delightful intensify the

torments of the flesh and longings of desire. . . . Men call me chaste; they do not
know the hypocrite I am.”
She wishes she could worship God, she says, but based on her experience, she
believes God is cruel. How can she worship God, she asks, if she thinks him
unjust? Yet her daily task is to direct the spiritual life of others. As a result, she
believes she has been condemned to the life of a hypocrite. Instead, she says, she
can only worship Abelard, and yet, for this worship, she can expect no
forgiveness, no reward, no salvation.
Heloise wrote the first of these eloquent letters after having heard nothing
from Abelard of a personal nature for many years and after having come to the
view that he had finally abandoned her. But she had also come across a copy of
his memoir, The Story of His Misfortunes (Historia calamitatum), which told the
story of his rise to power and of their affair. So she took up her pen. When he
received her first letter, Abelard replied and strove to persuade her to relinquish
her hopes of romantic love, which could no longer be. He counseled her to give
her love to God and to devote herself to faith. More letters followed, and after
this, their deep friendship continued. But what effect, in the end, did his letters
really have? In particular, did he ever truly persuade her to renounce her
bitterness toward life and God and to embrace his faith in the goodness of God?
As it turns out, the historical record leaves this last question unanswered. In
the words of the historian étienne Gilson,
Abelard knew how to climb this summit. We may fear that he strove in vain to lift Heloise to the
same height. We should like to be able to say that she was won by his eloquence and accepted this
high ideal of Christian charity, and that she came finally to love Abelard for God, rather than God for
Abelard, or even, as was the case, Abelard against God. [Nevertheless] if she ever submitted, it was
in the hidden recesses of her heart, not openly in her letters. Her submission, accordingly, is not a
part of recorded history.7
Indeed, looking back on her fate now, and especially on her complaints
against God and on her insistence that God should be just, one might almost say
Heloise was, in many ways, more of an Abelard than Abelard. She believed in
reason and not simply blind faith, and she demanded that a just and merciful god
should be logical and fair. Thus at the very bottom of her tragic personal story is
the ultimate, vexing question of whether faith and reason can truly coexist. It
was exactly this question, back in the early days of Abelard’s career, that first
made him famous.

ABELARD’S ATTACK ON FAITH WITHOUT REASON
What Abelard’s students had wanted above all, he tells us, were “human and
logical reasons” for the doctrines they were being asked to believe, the doctrines
of the Roman Catholic Church. It was just his response to these demands that
first made him notorious. When Abelard writes of his students, he remarks,
“They said that words are useless if the intelligence could not follow them, and
that nothing could be believed unless it was first understood, and that it was
absurd for anyone to preach to others what neither he nor those he taught could
grasp with the understanding: the Lord himself had criticized such ‘blind guides
to blind men.’”8
After his castration and submission to monastic authority, Abelard says he still
continued to work on the problem of faith and reason, and he composed a book,
On the Unity and Trinity of God. His enemies, however, summoned a church
council against him and forced him to throw a copy of it into a fire. Then he
produced an even more daring book, Sic et non (Yes and No), which listed
statements from church authorities in parallel columns so that everything in one
column seemed to contradict what was said in the other. His purpose was not to
show that such contradictions were real, he said, but to offer students material
for debate and thus to stimulate their logical skills. The skillful student, he
maintained, could resolve these apparent contradictions by drawing the
appropriate distinctions—a method practiced to great effect about a century later
by Thomas Aquinas.9
On the whole, then, Abelard’s view was that logic could strengthen faith, and
he saw himself as advancing religion, not destroying it. All the same, powerful
voices within the church thought otherwise, and the most powerful of all was the
vigorous and forbidding Bernard of Clairvaux.
Bernard, somewhat younger than Abelard, was an abbot of rising fame and the
charismatic leader of a revival throughout Northern Europe of unquestioning
belief. Bernard railed against corruption in the church, and in the battle against
corruption, he and Abelard were of like mind. But Bernard also condemned
“dialectic” as a threat to Christian doctrine. Faith must be based on mystical
experience, he argued, and Abelard’s methods, which consisted in considering
opposing views in the manner of a Platonic dialogue, could only work mischief;
they would encourage doubt. As a result, Sic et non represented, for Bernard,
everything he opposed. The conflict between the two men finally came to a head
in 1141 at another church council, the Council of Sen.
At the Council of Sen, Abelard and his supporters were vastly outnumbered.

Bernard thundered against him, and Abelard was condemned. Many of Abelard’s
opinions were declared heretical, his books were burned, and he was ordered to
be confined for the rest of his life to another monastery under a regime of
perpetual silence. His friend Peter the Venerable, then abbot of Cluny, intervened
with the pope and managed to get the vow of silence softened, but Abelard spent
the rest of his days in several of Peter’s monastic retreats.
At the close of his life, in his “confession of faith,” which was probably his
last personal message to Heloise (Abelard addresses her with the salutation,
“Heloise my sister, once dear to me in the world, now dearest to me in Christ”),
he says logic has made him “hated by the world.” But he adds, “I do not wish to
be a philosopher if it means conflicting with Paul, or to be an Aristotle if it cuts
me off from Christ.” Nevertheless, when it comes to logic, he remains steadfast:
“I do not fear the barking of Scylla, I laugh at the whirlpool of Charybdis, and
have no dread of the Sirens’ deadly songs. The storm may rage but I am
unshaken. Though the winds may blow they leave me unmoved, for the rock of
my foundation is firm.”10 He died about eighteen months later, and according to
his wish, his body was given to Heloise to be buried in her convent, the
Paraclete, which Abelard had originally built as an oratory and had then donated
for the use of Heloise and the other nuns. At her own death many years later, in
1163 or 1164, she was buried with him.
(In 1817, what were thought to be the remains of Abelard and Heloise were
moved to Père Lachaise cemetery in Paris. In look and feel, Père Lachaise is a
place of immense decay, of faded photographs and broken mementos in the
tombs; one sees not only that the mourned have died but that many of the
mourners have also passed away. The cemetery contains the graves of many
other famous personages, among them Molière, Chopin, Balzac, Collette, Oscar
Wilde, and even the rock star Jim Morrison. But at the tomb of Abelard and
Heloise, tucked away in a corner of the cemetery, it happens that there are often
fresh flowers left by anonymous admirers.)
Abelard, Heloise, and Bernard all wrestled with the question of reason and
faith, but who, in the end, was right? Does reason truly undermine faith, or not?
ARE FAITH AND REASON COMPATIBLE?
This conflict between faith and reason wasn’t unique to Christianity; it still
continues in most religions today, and it was being played out in medieval Islam
at almost exactly the same moment as the struggle between Abelard and

Bernard. In medieval Islam, the chief antagonists were al-Ghazali from Persia,
who attacked various uses of logic in religion in a work he titled “The
Incoherence of the Philosophers,” and Averroës from Spain, who defended such
uses of logic in a response he called “The Incoherence of the Incoherence.”
Indeed, perhaps the most significant difference between the Christian and
Islamic versions of this struggle is that al-Ghazali and Averroës wrote from
opposite ends of the Mediterranean and lived at slightly different times, with the
result that neither could lay hands on the other.11 Still, Bernard and Abelard were
actually separated by several questions, not just one.
Consider, first, Bernard’s outlook. Bernard thinks logic is harmful and that it
makes us worse as people. It makes its students arrogant and vain. (Even Plato
admits that those who study argument sometimes become like feisty puppies,
eager to attack all who come near.)12 Certainly, when people do well in logic,
they often do think themselves smarter than others, and the result is that they
often mistake cleverness for wisdom. In addition, they are especially prone to
discount the importance of tradition, and this is no small error.
In real life, we often defer to tradition, but our reasons for doing so needn’t be
illogical. We sometimes follow tradition simply to avoid offending other people,
and we also follow it because it is sometimes founded on experience that can’t
be easily transmitted. Yet, even when a tradition is unreasonable, sometimes we
follow it anyway for the sake of social cooperation; where conflicts might arise,
some rule is often better than no rule, and sometimes the only force that unites
people around any rule in particular is the force of tradition. The student of logic,
if hasty, is apt to miss these points.13
Now, with many of these criticisms, we think Abelard might agree. To speak
in Abelard’s defense, arrogance and vanity were weaknesses he plainly
confessed, and he might easily retort that such weaknesses are simply the
characteristic vices of intelligent people. He might also admit that deference to
arbitrary traditions is sometimes quite reasonable. Yet the most powerful
argument for Abelard (or so we suspect) is different. His most powerful
argument is to point out the consequences of the alternative.
What happens if we don’t cultivate reasoning? What happens if we don’t learn
to assess arguments? In that case, we leave ourselves intellectually disabled in
facing a new world. Of course, some periods require us to think anew more than
others, but our own period could hardly require it less, since technology is
remaking the world every day. Everyone must engage in at least some reasoning,
even Bernard, and so the real question is whether we shall do this cogently or
foolishly.

Still, there is yet a further question that separates Abelard and Bernard, and
this further question is perhaps the most profound of all. The further question
isn’t whether we should cultivate our capacity for reasoning, but what even
counts as a reason?
THE FOUNDATIONS OF RATIONAL BELIEF
Bernard also has reasons, but his reasons are the traditions and dogmas of his
church. Abelard, on the other hand, seeks further reasons for these dogmas, and
so we are once again confronted with a problem we considered in earlier
chapters: the problem of foundations. Our foundations are our ultimate reasons,
the reasons for which we have no others, but we have never actually figured out
what these ultimate reasons are.
In fact, this last question was evident even to Aristotle, especially in the first
sentence of his Posterior Analytics, which reads, “All instruction given or
received by way of argument comes from preexisting knowledge.” His meaning
was that argumentation can give us knowledge only if we assume for the
premises something we already know. But how is this preexisting knowledge
obtained? If we answer “by argument,” we fall into an infinite regress, since
knowledge by way of argument always presupposes knowledge. The same
difficulty arises, moreover, even if we talk not about knowledge in the full sense
of the word but only about probable belief. A rationally persuasive argument
must still begin with premises that are initially more acceptable than the
conclusion to be proved. Yet, in that case, what distinguishes the initial premises
of a reasonable person from the initial premises of a fanatic? Or how does a
chain of rationally persuasive inferences get started?
It is just this conundrum that exercised not only Abelard and Bernard but
many famous philosophers writing in the centuries that followed, especially in
the wake of the wars of religion. Many of the most celebrated thinkers in the
field of epistemology (the theory of knowledge) have asked this question: what
makes an initial premise rational from the start? The question was posed by
Descartes, Thomas Hobbes, John Locke, George Berkeley, Hume, and Immanuel
Kant (among many others). And it is safe to say there are still large areas of
disagreement over exactly how to answer it. Nevertheless, the key point for the
student of logic (and the point we stress) is that nothing in logic truly supplies an
answer. Quite the contrary, logic only tells us how to draw reasonable inferences
from a premise. This is different from determining what makes a premise
reasonable from the beginning. At most, then, logic lays out necessary

conditions for being a reasonable person, but not sufficient ones. Logic can tells
us what it means to construct a rational argument once we find the right starting
points, but it is strictly silent as to what defines these starting points in the first
place.
To friends of logic, this last result is apt to be disappointing. We often expect
more from logic; we expect it to define rationality completely. We expect it to
tell us not only how to draw reasonable inferences from our starting points but
how to find the starting points too. Nevertheless, there is a further consolation to
be drawn here, and if we can draw it, some of this disappointment may go away.
The consolation is this: just as a person can be quite logical without
expounding a general theory of logic, likewise, many people can be quite
reasonable without expounding a general theory of rationality. Even if many
epistemologists do disagree among themselves about what makes a starting point
reasonable in the first place (and about how to define rationality in general), this
by no means makes it difficult for us to distinguish many particular cases of
reasonable belief and of fanaticism. Nor does it prevent us from giving good
reasons for these judgments, reasons particular to the case.
To illustrate, suppose a man now says he knows his neighbor to be a
malevolent witch merely “on faith” (which is not too distant from some of the
more fanatical assertions that one would have heard in the sixteenth century). In
that case, we don’t really need a general theory of rationality to determine
whether his assertion is reasonable. Instead, we can settle the question by
analogy, meaning by comparison to similar cases. We don’t normally know a
person to be a burglar merely on faith, a taxpayer on faith, a murderer on faith,
or a registered Democrat or Republican on faith. A great many other beliefs
about particular persons, if supported by nothing but faith, would be obviously
irrational and indeed fanatical. In that case, however, this new particular belief
about witches, also supported by nothing but faith, is probably fanatical too. This
argument is strictly inductive and analogical, from like cases, but we wouldn’t
need to settle the question by invoking a general theory of epistemology.
(For example, we wouldn’t need to decide such difficult matters as whether all
human knowledge derives from physical sensation alone, as Hobbes asserted, or
whether some of it, especially in mathematics and logic, might also derive from
other sources, as Kant asserted, or whether morality and politics might also
contain objectively knowable truths independent of the physical world, as many
moral philosophers from Plato onward have asserted. Such questions are, of
course, philosophically interesting, but trying to answer them is quite
unnecessary in determining whether a particular assertion about witches is

rational. Instead, all we need do is notice that many other such claims would be
obviously irrational, and so we can argue from particular cases directly. More
broadly, the difficulty with invoking one of these grand epistemological theories
is that we may then find ourselves no less unsure as to whether the theory in
question is really correct.)
But what can this observation tell us about the basic issue that separated
Abelard and Bernard, the problem of religious faith? Most important, does it tell
us whether a rational person might believe in something strictly as a matter of
faith, without reasons? And can it tell us whether religious people are any more
or less rational than nonreligious people?
RATIONALITY AFTER THE WARS OF RELIGION
Abelard believed that religious assertions were propositions that could be
reasoned about, and yet, at the same time, he seems to have denied that reason
alone could establish all things a reasonable person should believe. He criticized
those who expected reason to serve as the sole foundation of their beliefs.14 In
Abelard’s world, the usual assumption was that faith was a virtue, and many
medieval writers had long assumed that to believe a thing merely on faith,
without any argument or evidence, was by no means an intellectual failing,
provided the belief fell into a fairly narrow class of assertions. Abelard accepted
this outlook even though he thought these same beliefs could also be objects of
analysis.
Today this view might still be defended by arguing that there is nothing
obviously disordered in a mind that believes on faith that the universe is
somehow governed by an intelligent god, that life is basically good, that life isn’t
merely a dream, or that all events do indeed have causes. Of course, many
modern writers now assert that religious belief is irrational, and others argue that
it is rational; nevertheless, few psychiatrists would take the question “Do you
believe in a god?” as a valid diagnostic tool for predicting serious mental
disorders. All the same, our world is different from Abelard’s, and today’s
approach to faith is also different.
The wars of religion provoked a historical reaction to the medieval attitude.
After an age of fanatical terror, many philosophers came to think that believing a
thing merely on faith without some good reason for it is always irrational—
unless the thing in question happens to be self-evident. It is never rational,
according to this later view, to believe merely on faith, and this outlook was

already widespread by Descartes’s time. (Except in ironical passages, Descartes
seems to have accepted the new outlook completely; for Descartes, faith without
reasons is, apparently, always irrational.15 As a purely verbal matter, of course,
one might argue that the very meaning of the word “irrational” entails this
outlook, if by “irrational” we mean “not a consequence of reasoning,” but a
better term for this last meaning would be “nonrational.” The real issue here isn’t
whether some controversial beliefs are embraced without argument or evidence
—surely they are—but whether embracing them in this manner is always an
intellectual defect.)
Our view, for better or worse, is that the correct answer to this question is
actually the Socratic one—that what we really know about faith and reason in
general is that we don’t know. We can describe what is often rational or usually
rational, and we can give many good reasons for thinking that particular claims
based on nothing but faith are irrational from the start. But to state a universal
definition of what all rational starting points have in common (or all rational
beliefs) is actually quite hard. In fact, what the study of modern epistemology
shows is that even the experts sometimes differ over what counts as a rational
premise, especially in metaphysics, abstract moral theory, and logic and
mathematics.16
If the Socratic answer is indeed the right one (and we admit that perhaps it
isn’t), then the problem of faith and reason is still partly as it was in Abelard’s
day: difficult to decide. It is hard to know, as it was then, if faith without reasons
is necessarily irrational. The foundations of rational belief are often obvious in
particular cases, yet, in other respects, they are sometimes deeply mysterious.
THE VIGILANCE OF REASON
How, then, can we tell whether any particular premise is rational or not?
Checking the rationality of one’s premises is apparently like checking one’s
arithmetic. One could carry on the process forever, checking and rechecking to
ensure one had not made a mistake. One could demand not only good reasons
for one’s beliefs but entirely new reasons, good reasons, for thinking that the
initial reasons were good. And so on. We often say that what counts as
reasonable depends on the available evidence, and yet we sometimes find that
new evidence shows our initial evidence to have been erroneous. What we had
first supposed as obvious or beyond reasonable doubt turns out, on reflection, to
have been mistaken. As a result, the decision of how far to carry on one’s
checking and rechecking seems to be a tradeoff, one between efficiency and

certainty. A changing world often requires that we reconsider, and yet the
necessity of action still demands that we be willing to take at least some things
for granted.
The medievals had a peculiar way of representing this tradeoff, and perhaps
their approach is instructive even now. They expressed the problem of doubt and
certainty (and checking and rechecking) by contrasting the rational powers of
human beings with the rational powers of God.
According to medieval philosophers, though God knows all things, God has
no need to reason. Instead, reasoning (ratio) is something that is useful only to
beings who are partly ignorant. These philosophers believed God knows all
things immediately and directly, a power they called intelligentia, and as a result,
they maintained that God has no need to infer one truth from another. Instead,
God sees all truths independently.17 Of course, God could draw inferences if God
chose to, but God is under no necessity of doing so, and as a result, there is no
sense in which some of God’s beliefs depend for their justification on other
beliefs. Instead, each of God’s beliefs stands on its own. God needs no
foundations, so to speak, to serve as cornerstones for a further edifice of
inferences. Instead, God’s beliefs are like an immense field of freestanding
pilings, each sunk in bedrock. God needs no logic.
Human beings, however, are not gods. Not all human beliefs are reasonably
arrived at intuitively. In consequence, we partly ignorant beings must build
structures, and logic is the discipline that allows us to determine whether the
structures are sound. Without this discipline, the structures become unreliable,
and we may even start behaving like fools. Nevertheless, the upper elements of
our structures depend for their justification on the lower. We can often reinforce
these lower elements with further experience or analysis, but many things
continue to elude us, and on other occasions, we discover new information that
calls our initial premises into doubt.
Despite these difficulties, we are placed in a world of change and chance, of
storm and strife, and of right and wrong, and we still need to make our choices
and do more than merely stand paralyzed. Like the ancient Athenians, we often
face fundamental questions of power and justice, and like the Athenians again,
we often wrestle with sophistry and illusion. Like the inhabitants of sixteenth-
century France, we sometimes encounter fanatical enthusiasms, and like the
architects of seventeenth-century science, we must absorb new discoveries made
possible by empirical reasoning and experiment. On some occasions, we face all
the frauds and swindles listed by Jeremy Bentham. And whether we like it or
not, we are also being carried forward with each passing minute into a new,

mechanized world, where logical systems and modern computers can have vast,
unforeseen consequences.
On our success or failure in navigating all this varying and difficult terrain
really depends the whole future. As citizens, we each have a station, and our
station has its duties. The trick, then, apparently, is to remain vigilant—vigilant
in assessing the ground we stand on.

APPENDIX
FURTHER FALLACIES
THIS APPENDIX includes a list of deceptive forms for reasoning—some already
considered in chapter 8 but others not. For convenience, we have arranged them
alphabetically, and we include remarks on how each error can be exposed. (In
addition, specific topics can also be looked up in the index.) As the classification
of fallacies is an inexact science, some of these fallacies will overlap.
AD HOMINEM ARGUMENT (“TO THE PERSON”)
To argue ad hominem is to offer an assertion about the advocate of an idea as a
reason for rejecting the idea itself. There are at least three common sorts of ad
hominem: personal attack, the circumstantial ad hominem, and the tu quoque.
Personal attack: Assailing the opponent’s character instead of the idea the
opponent defends. When a person gives testimony as a witness, seeks a position
of public trust, or damages the public good, that person’s character concerns
others. Nevertheless, there is no necessary connection between the soundness of
the character and the soundness of the person’s opinions. The fallacy consists in
arguing that so-and-so must have bad ideas because so-and-so is a bad man or
woman.
The attack on the person can be open or oblique. Open attacks include
epithets; ridicule of an opponent’s speech or mannerisms; and references to
conduct, associations, or background that carry ill repute. Oblique attacks do not
consist in dwelling on an opponent’s character but in dwelling on one’s own; the
writer or speaker makes a point of stressing his or her own good qualities. (The
effect is to give an impression of piety or virtue that, by contrast, makes the
opponent look cold.)
Open attacks can be exposed by labeling (“ad hominem argument,” “ad
hominem attack,” or “personal attack”). More aggressive labels include
“vilification,” “McCarthyite lie,” “McCarthyism,” and “McCarthyism of the
left.” Also, “Mr. Jones’s idea of proof is to attack people instead of ideas.” (In
earlier times, the phrase “ad hominem” applied to any appeal to emotion or
prejudice, but today it is best reserved for assertions about an advocate.) Oblique

attacks can be met, if necessary, by pointing out that the opponent chooses him-
or herself as a subject instead of the issue at hand. (“Ms. Jones deserves to be
praised for her piety, but I think she has stressed that point already.”)
The circumstantial ad hominem: Rejecting an idea because it is advocated by
the wrong people in the wrong circumstances. “It is pathetic to see this foreigner
criticizing our system when her own country cries out for reform.” The idea
must be wrong because it has the wrong friends. In a circumstantial ad hominem,
the disputant makes no assault on the opponent’s character but nevertheless
confounds ideas with people. The writer or speaker still calls attention to some
aspect of the opponent’s situation. The label “circumstantial ad hominem” is
pedantic in debate; the fallacy is perhaps best exposed by making the opponent’s
point for him: “I happen to be a foreigner; therefore, whatever I believe must be
false.”
Tu quoque (“you too”): Defending an error on the grounds that an opponent
has committed the same error. “Let him who is without sin cast the first stone.”
(The original meaning is to forgive the sinner. The practitioner of tu quoque
wants to justify the sin.) “You do it too.” The tactic is perhaps best met bravely:
“My mistake is made; you have a chance to rise above it.”
BEGGING THE QUESTION
In argumentation, begging the question means giving a reason as disputed as the
point to be proved.1 We “beg the question” when we try to prove the obvious by
appealing to the dubious, and we also beg it when we give for a reason the very
point at issue. In both cases, we violate Aristotle’s dictum that a rationally
persuasive argument must always proceed from premises that are initially more
evident or less disputed than the conclusion.
Questions are commonly begged in at least four ways: (a) by arguing in a
circle, (b) by arguing from a doubtful premise, (c) by question-begging epithets,
and (d) by question-begging definitions.
Arguing in a circle: Offering for a premise the very point in dispute. “Women
shouldn’t fight bulls because a bullfighter should be a man.” Or the disputant
offers reasons that rest on each other for support: “Shakespeare is a good author
because he is read by people with good taste, and we know those people have
good taste because they read Shakespeare.”
Arguing from a doubtful premise: Drawing a conclusion from a premise that is
at least as doubtful as the conclusion itself. “Abortion should be permissible

because a woman has a right to control her own body.” (The existence of the
right is as controversial as abortion itself.) “Abortion is wrong because the fetus
is a person.” (The status of the fetus is as controversial as abortion itself.) The
argument isn’t circular, because the premise doesn’t rest on the conclusion.
Nonetheless, the premise is doubtful. The fallacy can be exposed by labeling it
“begging the question,” but one must also point out the doubtful premise.
(Notice that an argument that rests on a doubtful premise is unfounded, but the
argument begs the question only if this premise is more doubtful, or no less
doubtful, than the point to be proved. In consequence, begging the question, by
definition, always involves a comparison: the premise’s doubtfulness as
compared with the conclusion’s doubtfulness.)
Question-begging epithets: Labels that presuppose a conclusion to be drawn
from a description of fact. “This criminal stands accused of one of the most
vicious crimes imaginable.” (Calling him a “criminal” already assumes his
guilt.) “I oppose this boondoggle because it wastes the tax-payers’ money.”
(Calling it a “boondoggle” assumes it to be wasteful.) Also, terms of praise or
blame used to describe a fact from which a moral judgment is to be inferred:
“The wicked Adolf Hitler has today broken his agreement with the good Joseph
Stalin.”
Question-begging epithets aren’t necessarily false, but they subvert the order
of proof. More exactly, statements of fact often serve as premises. The point to
be inferred from them is a conclusion. To use a question-begging epithet, then, is
to assume this conclusion in the statement of the premises. In journalism,
contentious reporting often has this effect. “The forces of evil have scored a
major victory at the polls.” On the other hand, reporting that seeks to avoid this
difficulty usually aims at supplying a description that no reasonable witness
could dispute. Question-begging epithets can be exposed by labeling them and
by explaining their use. “To call the defendant a criminal is to assume the very
point he denies.”
Question-begging definitions: Attempts to settle a dispute about things by
appealing to the mere meanings of the words that name them. “The mind can
never be physical because we don’t mean a physical object when we use the
word ‘mind.’” “Pleasure isn’t the good, because we don’t mean pleasure when
we use the word ‘good.’” An argument of this kind assumes that the word’s
usage is well founded. Nevertheless, whatever a person means by the words, the
person may still be wrong about the things he describes.
Notice that an appeal to the meanings of words can still be useful in eliciting
an opponent’s tacit assumptions—a technique common in philosophy. (This

technique was shrewdly used, for example, by the English philosopher G. E.
Moore to argue that our idea of good is distinct from our idea of pleasure.)2 This
is different from proving a conclusion about the things the words name. (Our
assumptions about an object are one thing; the object itself is another.) To expose
the fallacy, the begged question must be identified. “Your argument assumes that
what we mean by the word is what it ought to mean. Maybe we should change
the meaning.”
BIG LIE
A big lie is an untruth on a massive scale, intended to deceive an audience by its
very size. The expression was coined by Adolf Hitler, who wrote,
The great masses of the people . . . more easily fall victim to a big lie than to a little one, since they
themselves lie in little things, but would be ashamed of lies that were too big. Such a [large]
falsehood will never enter their heads, and they will not be able to believe in the possibility of such
monstrous effrontery and infamous misrepresentation in others. . . . Therefore, something of even the
most insolent lie will always remain and stick.3
Hitler accused his ideological opponents of using this technique, but later
observers said it was also the method used by Hitler.
CAUSE AND EFFECT (CONFUSIONS OF)
Most discussions of cause and effect tacitly assume at least three things: an
effect never precedes its cause, effects are related to their causes by general laws,
and an effect would never occur as it does but for its cause. (In consequence, a
factor that makes no difference isn’t a cause.) Where the causes and effects
aren’t particular events but classes of events or continuing processes, the
influence can be reciprocal. (The motion of the earth affects that of the moon,
and the moon’s motion affects that of the earth.) The three assumptions are
antecedent to debates about causation and are seldom debated themselves,
except under the heading of metaphysics.
To confuse a cause with something other than a cause is often only a simple
mistake rather than a logical fallacy. Nevertheless, the mistake is so common as
to be worth studying, and it takes more than one form:
Confusing cause and effect: “Fever is the cause of infection.” (In truth,
infection causes the fever.)

After the fact, therefore because of the fact (“Post hoc, ergo propter hoc”):
“The cock crowed, and then the sun rose. So the sun rose because the cock
crowed.”
Neglecting a common cause: “The ticking of the clock makes its hands
move.” (The common cause of both effects is the clock’s internal mechanism.
Events with a common cause are “collateral effects.”)
Confusing a cause with a necessary condition: “Oxygen was necessary for the
Great Chicago Fire since, after all, Chicago couldn’t have burned without it.
Therefore, the presence of oxygen caused the fire.”
Confusing a cause with a sufficient condition: “A single bullet is insufficient
in itself to kill eight million men. Therefore, the assassination of Archduke Franz
Ferdinand didn’t cause the death of millions during World War I.”
Assuming a unique cause: “An iceberg sank the Titanic, so errors on the
bridge didn’t sink it.” (An event may have many causes.) “Guns don’t kill
people; people do.” (Someone’s death can result from both causes working
together: the availability of the gun and the actions of the person using it.) “The
cause of consumer fraud isn’t deregulation but simple greed.” (Misdeeds can
result from both factors together: wicked motives and the absence of someone
powerful enough to put a stop to them.)
Exaggerating effects: “Allow the government to fluoridate your water, and
soon it will indoctrinate your children.” (The world is full of slippery slopes,
driving wedges, tips of icebergs, and camels’ noses under the tent; but not all of
them are real. Dire predictions in succession are sometimes called a “parade of
horribles.”)
Confusing a cause with a correlation: To say that causes must be related to
their effects by general laws is to say that causes and effects must be
“correlated.” But not all correlations are cause and effect; some occur by chance.
For example, after 1840 and until 1989, every American president elected in a
year ending in zero died in office: Harrison, Lincoln, Garfield, McKinley,
Harding, Roosevelt, and Kennedy. As a result, some feared a similar fate for
Ronald Reagan. Also, correlations can arise not merely because one event causes
another but because the two events are collateral effects. (See neglecting a
common cause.) Mistakes of this kind are best exposed by analogy.
CIRCULAR DEFINITION
A circular definition is a definition that contains the word it defines; it uses the

word or its derivatives to explain the meaning of that word. “Democracy is a
democratic system of government.” “‘Organic’ means not inorganic.” A circular
definition can also be one that defines A in terms of B, and B in terms of A.
In general, definitions are required in argument only when a person’s words
are so unfamiliar, ambiguous, or vague that it is unclear whether the propositions
that contain these words should count as true or false. (On other occasions, the
demand for a definition is often diversionary.) The trouble with a circular
definition is that it leaves the words just as unfamiliar, ambiguous, or vague as
they were at the outset. If a definition’s purpose is merely to provide an
equivalent expression, the definition is called “synonymous,” and even the
synonymous definitions of a dictionary will eventually form a circle, if carried
far enough. (The demand that every word be defined in terms of a fresh one
leads to an infinite regress.)
On the other hand, if a definition’s purpose is to explain the elements of an
idea, then a circular definition is useless. (For example, in asking what defines a
horse, we are not normally looking for a mere synonym for the word “horse”; we
are not seeking an expression like “steed.” Instead, we want to know what makes
a thing a horse, meaning an account of the characteristics in virtue of which an
animal is a horse. “A horse is a large solid-hoofed herbivore of the genus Equus
and the family Equidae.”) Such definitions are called “analytical.”
When it comes to analytical definitions, circular definitions are like circular
explanations: uninformative. (Nevertheless, as with words, so with ideas: the
demand that every idea be analyzed in terms of a fresh one leads to an infinite
regress. If no analytical definition for an idea is obtainable, the reason may be
simply that the idea in question is unanalyzable.)4
A circular definition is best exposed by labeling or by analogy, but a circle of
terms is a fallacy only when (a) the definition is intended to be analytical, or (b)
the word to be defined is so problematic that it makes a proposition’s truth or
falsity unclear.
CIRCULAR EXPLANATION
A circular explanation is an explanation of cause and effect that is
uninformative; the cause is identified only in terms of the effect it generates. (A
doctor in Molière’s Malade Imaginaire explains that opium induces sleep
because it has “dormitive power.”) Circular explanations are common in half-
baked commentaries on cosmology and the sciences.

To explain is to inform, and so an informative account must give further
insight into the cause’s nature. (Still, to say that everything must be explained in
terms of something else leads to an infinite regress; something will always
remain unexplained.)5 The fallacy is sometimes exposed by analogy. (The classic
analogy is Molière’s doctor. Or “Opium makes you sleepy because it is
soporific.”) Circular explanations can also be called “question begging.”
COMPOSITION
Composition is the assumption that what is true of the parts must always be true
of the whole, or what is true of individuals must always be true of the group they
compose. Here are some examples:
“Each soldier in the army is efficient, so the army is efficient.”
“In a recession, each household saves its money. Therefore, in a recession, the
nation as a whole saves its money.” (Economists sometimes call this mistake the
“paradox of thrift.”)
“A football is made of electrons, and according to quantum mechanics, the
position of an electron is always uncertain. Therefore, on a playing field, the
position of the football is always uncertain.” (This argument, though seemingly
comic, sometimes appears in loose discussions of modern physics.)
The mistake is best exposed by distinguishing the parts from the whole, by
distinguishing the individuals from the group, or by analogy. “You might as well
argue that, since each atom of the earth is invisible, the earth is invisible.”
Composition is the converse of division (considered later).
CONTINUUM (DENYING DIFFERENCES IN)
This form of argument asserts that, because two things differ only in degree, the
difference is insignificant or nonexistent. “There is no definite point at which a
fetus becomes a child, so a fetus is a child.” To show that the boundary between
two things is vague isn’t to show that there is no boundary. To cite another
example, “The difference between logic and illogic or between the reasonable
and the unreasonable is sometimes vague. Therefore, there is no difference.” The
fallacy is best exposed by analogy. “You might as well argue that, since the
young become old at no definite point, the young are old, the old are young, and
everyone is the same age.”

DIVISION
Division is the assumption that what is true of a whole must always be true of its
parts, or what is true of a group must always be true of the individuals who
compose it. Here are some examples:
“Since America is mostly Protestant, each American is mostly Protestant.”
“The economy is made of households, and the economy is prosperous.
Therefore, the typical household is prosperous.” (The prosperity could be
concentrated in a small minority of households.)
The mistake can be exposed by distinguishing the whole from the parts, the
group from the individuals, or by analogy. “You might as well argue that, since
Mr. Jones is part of a large army, Mr. Jones is large.” Division is the converse of
composition.
EMOTIONAL APPEAL
Emotional appeal is any fallacious argument that inflames passions; however,
not all passionate arguments are fallacious, and not all fallacious arguments are
passionate. Passion, like humor, isn’t only a cause of forceful rhetoric but an
effect, and in consequence, it is usually a mistake to suppose that, in recognizing
a fallacy as emotional, one has truly seen to the bottom of it. An argument isn’t
fallacious simply because it is emotional; if fallacious at all, the argument is
usually emotional because it is fallacious. In consequence, to expose the fallacy,
one needs to capture it more specifically. By the same token, it is generally a
mistake to reject an argument merely because it arouses passion. The passion
aroused might be the passion for truth or the passion for justice.
EQUIVOCATION
In logic, equivocation is the use of a term in different senses in the same
argument; it means playing on an ambiguity. For example, “Darwinism is a
theory, but science is never mere theory. Therefore, Darwinism isn’t science.”
(Darwinism is inference but not, therefore, unfounded speculation. “Theory” can
mean inference or speculation.)
Here is a textbook example:
Some triangles are obtuse.

Whatever is obtuse is stupid.
Therefore, some triangles are stupid.
There is no way to expose the fallacy except by distinguishing the different
senses of the ambiguous language. Nevertheless, one can also point out that the
opponent plays on an ambiguity. (The label “equivocation” is often misleading
because it also means prevarication.)
EVILS AND REMEDIES (CONFUSING THEM)
This form of argument consists in taking the mere existence of an evil as
sufficient justification for a drastic cure. “Behold the rights of man! Violate any
one of them, and we shall immediately destroy the government!” To prove that a
regime violates natural or human rights is not, in itself, to prove that the regime
should be overthrown. To violate natural or human rights is, by definition, evil,
but how to remedy the evil is a separate question. The choice of remedies is often
a matter of assessing consequences.
FALSE ANALOGY
Analogy, as a method of inference, consists in arguing that, because two or more
things are alike in some respects, they are probably alike in other respects. A
false analogy likens the unlike. Here are two examples:
“Hong Kong shouldn’t be returned to China any more than the Louisiana
Purchase should be returned to France.” (The two cases depended on different
principles; the British didn’t purchase Hong Kong from China but merely leased
it.)
“In basing our politics on religion, we follow the example of Lincoln.”
(Lincoln made a religious argument that rested on political progress, most
conspicuously in his Second Inaugural Address; this is different from making a
political argument that rests on appeals to religion. The mistake here is to
confuse Lincoln’s premises with his conclusions. The conclusion of the Second
Inaugural Address is theological: Lincoln aims at vindicating the justice of God.
To base politics on religion, however, is the reverse: to invoke God to vindicate
the justice of one’s own policies.)
False analogies are often inadvertent, especially in the sciences, and they can
arise from relying on isolated cases, appealing to similarities that are merely

distant, overlooking technical advances in the field, or having sheer bad luck.
When the similarity between two things is close, the analogy is called strong, but
when distant, the analogy is weak.
These points being admitted, a false analogy can still be a sophistical device
when used to obscure a vital difference between two situations by suggesting
that the two situations are in some general way alike. The disputant obscures the
difference between an apple and an orange by pointing out that both are fruit.
Here is another example:
“All states oppress their opponents” (to suggest that summary executions or
summary detentions are equivalent to conviction and condemnation in a court of
law).
The tactic can be exposed by labeling it as false analogy or by logical
analogy:6 “A flea is like an elephant in some sense, but Ms. Jones seems to think
the differences count for little.”
FALSE ANTITHESIS
A false antithesis asserts that two possibilities are mutually exclusive when, in
fact, they are not. Both situations can be true simultaneously, yet the false
antithesis assumes that one necessarily precludes the other. “A policy can serve
the rich or it can serve the poor, but it can’t serve both.” (If this were true, then
air pollution controls could never serve both.) “My dog ran away or he got lost.”
(Regrettably, he may have run away and then got lost.) The fallacy can be
exposed by labeling, but the compatibility of the two possibilities must be
pointed out. Also, a false antithesis can often be recast as a false dichotomy.
FALSE AUTHORITY
An argument relies on false authority when it invokes an expert’s opinion in
matters outside his or her expertise.
“The great psychoanalyst Sigmund Freud has forever shown the absurdity of
belief in God.”
“The example of the great physicist Albert Einstein shows once again the
fundamental necessity of belief in God.”
An appeal to the wisdom of the ages is false authority whenever the ages have
no experience to render an informed judgment. When an expert’s opinion is

applied beyond its proper sphere, the device can be exposed by labeling (“false
authority” or “appeal to authority”); nevertheless, one usually needs to make
clear the difference between the expert’s training and the domain of the current
question. An appeal to the false authority of the ages is probably best exposed by
analogy. (“And the wisdom of the ages once held that to fly was impossible.”)
FALSE DICHOTOMY (OR FALSE DILEMMA)
A false dichotomy asserts that there are only two possibilities when, in fact, there
are more. A false dichotomy is often a simple mistake, but it is a sophistical
device when used to misrepresent the true choice. “Love the country as we have
made it, or leave it.” “Either you fight with us, or you fight against us.”
The false dichotomy is often framed as a question: “Whom do you support:
the president or the terrorists?” “Which side are you on: management or the
proletarian revolution?” (When framed as a question, the fallacy can also be
classed as a loaded question.) The fallacy can be exposed by labeling as long as
the missing alternative is pointed out.
FORMAL FALLACY
A formal fallacy is a logical error that turns on a slight shift of form or an invalid
syllogism. There are four common kinds of formal fallacy:
Affirming the consequent: A perversion of the logically valid form modus
ponens. This is modus ponens:
If A, then B.
A.
Therefore, B.
But the disputant who affirms the consequent says this:
If A, then B.
B.
Therefore, A.
The mistake is best exposed by distinguishing what the opponent says from what
he should say to prove the point. “Mr. Jones says, if A, then B. But what he
needs to show is, if B, then A.”
Denying the antecedent: A perversion of the logically valid form modus

tollens. This is modus tollens:
If A, then B.
But not B.
Therefore, not A.
The disputant who denies the antecedent says this:
If A, then B.
But not A.
Therefore, not B.
Again, the mistake is probably best exposed by distinguishing what the opponent
says from what the opponent needs to say to make the argument valid.
Invalid disjunctive syllogism: An error that mimics this logically valid form:
A or B.
Not A.
Therefore, B.
The mistake consists in arguing like this:
A or B.
A.
Therefore, not B.
The second version of the argument is fallacious when the word “or” is logically
inclusive (meaning “A or B or both”). Consider the following:
The ship’s captain is experienced, or the passengers will be in danger.
The ship’s captain is experienced.
Therefore, the passengers will not be in danger.
(The captain of the Titanic was highly experienced.)
The mistake is best exposed by pointing out that both possibilities, A and B,
could be true simultaneously.
Invalid categorical syllogism: The argument on the left is logically valid
whereas the argument on the right is fallacious:
No As are Bs.
No As are Bs.
All Cs are Bs.
All Bs are Cs.
Therefore, no Cs are As.
Therefore, no Cs are As.
Both arguments are categorical syllogisms, meaning they have only two

premises, three terms, and only propositions that are categorical.
There are two hundred and fifty-six forms of the categorical syllogism, but
only twenty-four of these forms are logically valid (or only fifteen forms,
depending on how they are interpreted).7 For this reason, a writer or speaker
should usually avoid categorical syllogisms in debate unless their validity is
intuitive.
When a syllogism is known to be invalid, the best way to expose the mistake
is usually to distinguish what the opponent says from what the opponent should
say to make the argument work. (In debate, it is usually a bad idea to try to
expose the fallacy by logical analogy because the comparison becomes
ponderous.)8
GENERALIZING (ERRORS OF)
To generalize is to assume that what is true of observed cases is probably true of
other, unobserved cases. Generalizing is essential to prediction and is common in
science. (For example, to say that every negatively charged particle repels every
other negatively charged particle is to generalize.) Generalization, however, is
subject to at least four kinds of errors:
Small sample: Arguing from too few cases. “Everyone I’ve met speaks
English, so the whole world must speak English.” (When a generalization rests
on a single case, the single case is sometimes called a “lonely fact.” The
argument can also be called anecdotal.)
Unrepresentative sample: Arguing from cases that, though numerous, are
atypical. “Most Americans with telephones favored Alf Landon over Franklin
Roosevelt in the 1936 presidential election. Therefore, most Americans favored
Landon.” (In 1936, most Americans did not have telephones, but the magazine
Literary Digest, relying on such a poll, predicted a Landon victory. Landon
carried only two states.) Unrepresentative examples are sometimes said to be
“cherry picked.”
False extrapolation: Arguing that an observed series continues into untested
regions. (For example, a driver who finds gasoline stations every few miles
while driving north in Vermont supposes there must be gasoline all the way to
the Arctic Circle. The error is common in statistical reasoning.)
False interpolation: Arguing that what is true at selected points must be true at
intermediate points. “America was at peace in 1922, 1932, and 1962; therefore,
it was at peace in 1942 and 1952.”

A mistaken generalization is best exposed by analogy. Notice, however, that
an “all” or “no” proposition can also be refuted by counterexample. (The
proposition “All swans are white” is refuted by the sighting of a single black
swan.)
INNUENDO
An innuendo is an accusation implied or suggested without evidence. Innuendo
can often be suggested by asking a question, a tactic made famous by Joseph
McCarthy: “Are you now, or have you ever been, a member of the Communist
Party?” To pose the question at all plants suspicion in the minds of the audience.
Some questions should only be posed privately. On the whole, an accusation is
worth asking about publicly only if, in the absence of a denial, the questioner has
some prima facie reason for believing it. After all, the audience supposes that the
question wouldn’t be asked in the first place unless there were some reason for it.
On the other hand, to ask privately about an unfounded accusation, even for
quotation, isn’t to insinuate it unless one somehow makes a public incident out
of the asking. The tactic can be exposed by labeling it an insinuation, an
innuendo, or by explaining the essence of it: planting suspicions in the minds of
the audience without evidence. The tactic can also be exposed by analogy to
McCarthy.
IRRELEVANT CONCLUSION (OR IGNORATIO ELENCHI,
“IGNORANCE OF PROOF”)
This means arguing for a point that is different from the one at issue. For
example, a writer who wants to show that America could have won World War II
without the help of the Soviet Union argues instead that the Soviet Union
couldn’t have won World War II without the help of America (these are different
questions). Any appeal to the irrelevant can be seen, in a loose sense, as an
irrelevant conclusion, but the term applies especially to mistakes that turn on a
confusion of logical elements.
The fallacy is best exposed by distinguishing what the opponent actually
proves from what he wants to prove. Also, “Mr. Jones argues for an irrelevancy.”
“The conclusion is true but irrelevant.” “The argument is sound, but it has
nothing to do with the question.”

IS AND OUGHT (CONFUSING THEM)
This fallacy consists in arguing that, because an institution or practice exists, it
ought, by right, to exist.
“Many people cheat on their taxes, so why shouldn’t I?” (confusing what
people do with what they ought to do).
As a philosophical matter, Thomas Hobbes maintained that claims about what
ought to be the case are actually complicated claims about what is the case; his
celebrated Leviathan (1651) offers a complex system of definitions whereby
these logical transitions are supposed to take place. Nevertheless, no
sophisticated thinker has maintained that statements in the simple form “x is so”
are directly convertible to statements in the simple form “This same x ought to
be so.” If the transition from “is” to “ought” is rational at all, then the transition
is more complicated.9
The fallacy is best exposed by distinguishing things as they are from things as
they should be or by analogy. “Many people commit murder; it doesn’t follow
that they ought to murder.”
LOADED QUESTION
A loaded question is one that commits the respondent to a thesis no matter how
the respondent answers. “Do you still beat your wife?” (Whether he answers yes
or no, he implies that he has done so in the past.) “Are you for the Democrats
and progress or the Republicans and reaction?” (or vice versa).
The tactic can be exposed by labeling (“loaded question”) and by
distinguishing the several questions that are combined. “You mean, do I now or
did I ever?” The fallacy can also be exposed by analogy. (“Do you still beat your
wife?” is a common analogy.)
NEGATIVE PROOF
Negative proof consists in arguing that, because a proposition hasn’t been
proved to be true, it must be false, or because it hasn’t been proved to be false, it
must be true.
“The product hasn’t been proved dangerous; therefore, it is safe” (or, “The
product hasn’t been proved safe; therefore, there is positive evidence of a
hazard”).

Senator Joseph McCarthy, speaking of one of eighty-one persons whom he
alleged were communists in the U.S. State Department, once said on the Senate
floor, “I do not have much information on this except the general statement of
the agency that there is nothing in the files to disprove his Communist
connections.”10
The absence of proof is not disproof; the absence of disproof is not proof.
When considering proof of a person’s guilt or innocence, one can also
distinguish between innocence as an empirical matter and innocence before the
law. (Legally, everyone is innocent before the law until proved guilty in a court.)
Logic also distinguishes between the absence of proof and the absence of a thing
or its effects. The failure of a proper search to find a thing where it ought to be,
or to find its predictable consequences, is indeed evidence that the thing in
question doesn’t exist. Thus to argue from the failure of a proper search is not
negative proof. (For example, the repeated failure of scientists to detect the ether
suggests that the ether doesn’t, in fact, exist.)
The fallacy is probably best exposed by putting the essence of it plainly. “If
we can’t prove it false, it must be true.” “If we can’t prove it to be true, it must
be false.” “If a person can’t prove himself to be innocent, he must be guilty.”
PRETENTIOUS DICTION
Pretentious diction is the use of unusual terms when ordinary terms will do.
Examples include love of the long word (“render” as a substitute for “make”);
Pentagonese (“entrenching tool” for “shovel”); a stilted expression in place of a
standard one (“if you will” for “so to speak”); foreign words for which there are
ordinary 
English 
equivalents 
(façon 
de 
parler, 
idée 
reçue, 
qua,
Weltanschauung); and quotations in Latin. (The antiquity of an ancient insight
can often be established by introducing it as “the ancient maxim” or “the insight
of the ancient Romans”; the only reason for putting it in Latin today is to puzzle
plain folk.)
A careless audience often mistakes unusual words for an unusual idea. The
trick, then, is to parade commonplace thoughts in uncommon dress. A common
form of pretentious diction consists in “popularized technicalities,” a label
supplied by H. W. Fowler in his Dictionary of Modern English Usage. Fowler
gives 
these 
examples: 
“concept” 
and 
“dualism” 
(from 
philosophy),
“progression,” “a function of,” and “percentage” (from mathematics), “complex”
(as a noun, from psychology), and “party” (meaning a person, from law).

To Fowler’s list, one might now add a few more: “construct” for an idea or
theory; “parameter” as a substitute for “perimeter”; “nonlinear” for what is
merely different or creative; “impact on” for “affect” or “hurt”; “deconstruct,”
“de-center,” “dislocate,” or “destabilize” for the more common words “criticize,”
“question,” “challenge,” or “analyze”; “discourse” for a mere chat, talk, ramble,
or opinion; “category mistake” for a confusion of ideas; “paradigm shift” for
what is only a change in attitude or approach; “problematic” (as a noun) for a
problem or puzzle; and “binary opposition” for a mere contrast or a false
dichotomy. In some instances, the effect of these words is to make ordinary
thoughts sound scientific or profound.
If pretentious diction needs to be deflated, one method is to translate it into
ordinary English. Another method is parody. Pretentious diction can be amusing
if ironical, but as a sophistical device, its most common uses are (a) to advance a
sham insight and (b) to make a straw man look formidable.
SHAM INSIGHT
This is the obvious made to sound profound or a trivial statement made to appear
learned. “Human language is a social construct, a matrix of meaning that is itself
the result of cultural conditioning.” (Translation: words are usually made up by
people living in groups; hermits rarely invent new languages except when they
go crazy.)
Sham insights are typically advanced by the use of pretentious diction, but a
sham insight can also be paired with a straw man. (“The Western intellectual
tradition assumes a different paradigm—that words have fixed meanings,
immutable, as if they had come directly from the gods.”) When the straw man
(that words have immutable meanings) is refuted as absurd, the sham insight
(that meanings of words do indeed change) seems all the more wise. Sham
insights can be exposed by labeling and by translating them into ordinary
language.
STRAW MAN
A straw man is a silly idea erroneously attributed to the opposition. The straw
man casts the opponent in the role of defending the indefensible; the essence of
the straw man is misrepresentation. (A common straw man is to attribute to one’s
opponent a logical fallacy he hasn’t in fact committed.)

In practice, a straw man is often followed with a sham insight. (For example,
“Unlike earlier theorists, who thought common law to be transcendental, we
realize that law is man-made and that any legal interpretation is, to some extent,
subjective.” In other words, laws aren’t written by fairies in the night; even
judges must sometimes guess what a law means.) For its effectiveness, the straw
man often depends on pretentious diction, and the common tactics include the
following:
Giving one’s own view an imposing title: “We believe in the Jurisprudence of
Pragmatic Realism, the theory that government should serve the people.” (The
opponent now seems to be against serving the people.)
Giving the straw man an imposing title: “Essentialism refers to the traditional
notion that the identities of men and women are biologically, psychically, and
socially fixed or determined. The essentialist position, therefore, cannot
acknowledge the possibility of any change.”11 (A theory of “essentialism” that
denies the possibility of any change is absurd from the start.)
Introducing the straw man with lofty epithets: “The grand narrative,” “the
master discourse,” “the dominant paradigm.” (“The grand narrative of the
Western intellectual tradition assumes . . .”—the assumption then being
something ridiculous.)
Attributing the straw man to a ponderous thinker: “We reject the Kantian
assumption that. . . .” “Kant is wrong when he supposes. . . .” Since few people
will really know whether the assumption belongs to Kant, a foolish idea can be
attributed to Kant, and his reputation will help to disguise its absurdity.
The straw man is effectively exposed by labeling. “Straw man” is a sturdy
metaphor, but it can also be freshened with a modifier: a marvelous straw man, a
glorious straw man, a straw man on stilts, a straw man in the guise of Kant. Also,
“Ms. Jones thinks that to attack us she must attack ________.” (You fill in the
blank with the bogeyman.)
SUPPRESSED EVIDENCE (OR IGNORED EVIDENCE)
Suppressed evidence consists in facts that have been neglected because they go
against a speaker’s theory or position. “The economy is healthier than ever under
our leadership, since more Americans are at work than ever before.” (It may also
be true that more Americans are out of work and that the population has simply
grown.)
A theory is judged not merely by its consistency with a certain body of

evidence but with all relevant evidence. The practice can be exposed by pointing
out the neglected facts and also by labeling. “Ms. Jones ignores evidence,” or
neglects evidence, or overlooks evidence, or discounts evidence, or leaves out
the key facts.
VAGUE METAPHOR
A vague metaphor is an implicit analogy that is not to be taken too literally.
George Orwell remarks, “When one critic writes, ‘The outstanding feature of
Mr. X’s work is its living quality,’ while another writes, ‘The immediately
striking thing about Mr. X’s work is its peculiar deadness,’ the reader accepts this
as a simple difference of opinion. If words like black and white were involved,
instead of the jargon words dead and living, we would see at once that language
was being used in an improper way.”12
Vague metaphors use language in an improper way. The problem with vague
metaphors is that no one actually knows what the conversation is about. A
metaphor implicitly likens one thing to another (as in likening the painting
discussed by an art critic to a living creature or a dead one). When the metaphor
is vague, however, the exact nature of the comparison is obscure. (In what sense,
one wonders, is the particular painting “living” or “dead”?) Of course, obscurity
is a matter of degree, but a metaphor is vague when it leaves the situation no less
obscure than it was before. Here are other examples sometimes cited by critics:
From Aristotle: “The state is prior to the individual.” (Meaning prior in time?
Meaning that an individual must always submit to the state’s decrees—despite
Aristotle’s own respect for the defiant action of Antigone, who disobeys all
Thebes to bury her dead brother?)13
From Marx: “The economic structure of society is the foundation of its legal
and political superstructure.” (Meaning an economy can exist even without laws
to support it? Put another way, what is the exact relation, according to Marx,
between an economic system and the rest of society?)14
From Jean-Paul Sartre: “Existence precedes essence.” (Meaning we figure
out our purpose in life only after we are born—rather than before? Is Sartre’s
utterance just a ponderous way of saying that right and wrong are merely in the
eye of the beholder, as his interpreters sometimes suppose? Is it just a way of
saying that we are what we do? Is it a puzzling way of asserting freedom of the
will?)15
A more recent example: “Reality (or truth) is socially constructed.” (Meaning

reality or truth is entirely invented, and nothing can exist unless the rest of
society says so? Meaning that what we think to be real or true is at least partly
influenced by the views of others? In that case, isn’t the doctrine just a sham
insight? After all, who would deny that other people influence our opinions?)
Vague metaphors can be exposed by labeling or by asking the disputant to
explain what he or she means. “Do you mean economic systems can exist
without laws?” “Do you mean anything can exist (even witches) if the rest of
society says so?”
WRAPPING ONESELF IN THE FLAG
The classic move of the demagogue, when challenged personally, is to portray
the challenge as an attack on the country, the culture, the office, or the laws. In
Bentham phrase, “Attack us, and you attack government.” The tactic moved the
English critic Samuel Johnson to remark that patriotism is the last refuge of a
scoundrel.
The device can be exposed by labeling. (“Ms. Jones’s defense is to wrap
herself in the flag.”) It can also be recast as an ordinary straw man. (“Mr. Jones
pretends that to attack him is to attack America.”) Samuel Johnson’s aphorism is
useful so long as the audience sees that it refers to sham patriotism rather than
true patriotism.

NOTES
INTRODUCTION: WHAT IS LOGIC?
1. A similar point is implicit in a dialogue published long ago by Lewis Carroll, “What the Tortoise Said
to Achilles,” Mind 4 (1895): 278–80.
2. To say that logic shapes the brain and that the brain shapes logic (so that the relationship is circular)
would seem to imply that logic shapes itself, which is apparently absurd. The social scientists Hugo
Mercier and Dan Sperber have recently argued that logical skills can be useful in irrational ways—as a
tool for winning arguments regardless of truth—but they have still allowed that logical skills can be
useful in rational ways, too. And in that case, to invoke logic’s usefulness as a cause of the brain’s
evolution (whether for rational purposes or irrational ones) is still to assume that the constraints of
logic have shaped the brain, not the reverse. For the views of Mercier and Sperber, see “Why Do
Humans Reason? Arguments for an Argumentative Theory,” Behavioral and Brain Science 34, no. 2
(2011): 57–111.
3. Some of these debates are described, for example, in Susan Haack, Philosophy of Logics (Cambridge:
Cambridge University Press, 1978), and, more recently, in Stephen Read, Thinking about Logic: An
Introduction to the Philosophy of Logic (New York: Oxford University Press, 1995). There are also
celebrated though controversial observations on these matters in the work of Willard Van Orman
Quine; see, for example, his collection of essays From a Logical Point of View, 2nd rev. ed.
(Cambridge, Mass.: Harvard University Press, 1980), and his Philosophy of Logic, 2nd ed.
(Cambridge, Mass.: Harvard University Press, 1986). Valuable articles on recent developments in logic
can also be found online in the Stanford Encyclopedia of Philosophy (http://plato.stanford.edu).
4. When professional logicians disagree about which arguments are valid and which aren’t, the disputed
cases tend to be obscure. Such cases may be whether an argument with a logically necessary
conclusion should always be counted as valid, regardless of its premises (since its conclusion must
always be true), or whether an argument with a logically contradictory premise should always be
counted as valid (since it can never have all true premises and a false conclusion). In mathematics,
there have been fundamental disagreements since the nineteenth century over the validity of arguments
involving infinite sets. Such cases are typically discussed in books on the philosophy of logic, like
those of Haack, Read, and Quine.
5. We shall be invoking the existence of such intuitions from time to time throughout our text, but we
shall offer no theory of how these intuitions occur. Just as we can know that we see something without
having a general theory of seeing or that we hear something without having a general theory of
hearing, we can also know some simple truths of logic and arithmetic intuitively even if we have no
general theory of how such intuitions are possible. For an interesting discussion of how we can have
certain knowledge of small sums through the simple act of counting physical objects, see John C.
Harsanyi, “Mathematics, the Empirical Facts, and Logical Necessity,” Erkenntnis 19 (1983): 167–92.
(As it happens, Harsanyi regards the counting of imagined objects as no less empirical than the
counting of physical ones, and we suspect that many classic authors, including David Hume, would
disagree. For Hume, determining the sum of two sets of imagined objects is a “relation of ideas” and
strictly a priori, even though the ideas themselves derive from physical sensation. But perhaps this
difference in the outlooks of Hume and Harsanyi is just terminological.)
6. According to this new theory, it would make little sense to say that God finds logic useful, because a
god who could make logic would also find any other reasoning useful—so long as God made different
things logical. For God, no one sort of reasoning would be any more useful than any other. Still, this
point about usefulness isn’t really an objection to the theory that God creates logic. Logic is useful to
us only because it helps us discover things we didn’t know. On the other hand, if we suppose that God

knows all things eternally (as many theologians over the ages have suggested), then, for God, there
would be nothing he didn’t know in the first place and thus nothing to discover. God might indeed find
logic useless, but God could create logic anyway.
7. The question appears in the Euthyphro, 10a.
8. Strictly speaking, the right and the good for Plato are “forms,” meaning timeless, placeless, eternal
properties that exist and that are imperfectly shared by particular actions or particular objects in the
physical world. Plato supposed that forms are not invented by intelligent minds, mortal or immortal;
rather, forms are respected. His theory of forms appears, with variations, in many of his dialogues,
including the Republic (especially books 5–7), the Symposium, the Phaedo, and the Parmenides. That
the gods’ reasons sometimes surpass human understanding is implied in the Republic at 613a, where
Socrates insists that even misfortune, if it falls on the just, must be somehow for their good.
9. The inventive theologian might seek a way around this consequence by supposing that God creates
rationality or reasonableness and then, for the sake of consistency, chooses to conform to it; thus the
whole domain of logic might still fall under God’s power. But the consistency in question here is
surely logical consistency so that, if God could remake logic, God would be equally rational if
inconsistent. In short, consistency is still a logical requirement in itself, and a god who conforms to no
requirements except those of his own making still has no reasons for doing anything. The logical effect
is still to make God arbitrary and capricious.
10. During the Middle Ages, Aristotle’s logical works were often collected in a single volume and called
the Organon, meaning the “tool,” a label that was first bestowed by Aristotle’s followers in ancient
times.
11. In logic, premises and conclusions are usually treated as true or false, yet logicians have sometimes
debated just what sorts of entities these true or false things are. Are they sentences? Are they the ideas
expressed by sentences? Either way, could they also be electronic pathways? In fact, these debates
have had almost no effect on determining what counts as a valid argument in ordinary contexts.
Instead, their effect is usually evident only in rare cases that are examined in advanced discussions in
the philosophy of logic (often under the heading of ontology). And their logical force usually goes no
further than undermining one or another theory of why arguments are valid. (This is different from
undermining our simple judgment that an example like the one about Socrates is valid.) By analogy,
the question of what numbers are—whether abstract eternal objects, complicated sets of physical
objects, or simply ideas in our heads—has long been controversial; what is a number, anyway?
Nevertheless, such debates have never changed the sum of two and three. Nor would anyone say that,
unless we settle these metaphysical controversies about numbers, we can’t do ordinary arithmetic.
(More generally, we can often manipulate A and B and know a lot about how to do it even if we don’t
know the metaphysical status of A and B—just as we can play poker without knowing what the
playing cards are made of.) The opposing view has been common in the history of philosophy and
seems to be traceable at least as far back as Plato’s early dialogues, which sometimes suggest that,
unless we can give a philosophical account of courage, friendship, piety, and the like—and fathom
their inner nature—we can’t know anything about them. But in our view, this is a mistake.
12. Epictetus, Discourses, trans. W. A. Oldfather (Cambridge, Mass.: Loeb Classical Library, 1925),
2.25.
13. We shall be considering another point of view in chapter 3, when we look at the theory that logic
results entirely from linguistic conventions, and a further point of view in chapter 6, when we examine
the notion that nothing in logic can be known intuitively and directly and that all logical judgments
must rest on circular reasoning.
In the ancient world, the idea that some truths were independent of space and time was emphatically
defended by Plato and may also have been inculcated by the Pythagoreans. The classicist H. D. F.
Kitto once described the Pythagorean outlook by relating his own discovery, at an early age, of several
truths of arithmetic, “of which my mathematical teachers had left me, I am glad to say, in complete

ignorance.” He wrote, “They had never told me, and I had never suspected, that Numbers play these
grave and beautiful games with each other, from everlasting to everlasting, independently (apparently)
of time, space and the human mind” (The Greeks [1952; repr., London: Penguin, 1991], 191).
In modern times, the idea that some truths can be independent of physical reality and of human
opinion was also stubbornly defended by the mathematician G. H. Hardy, who remarked, “I believe
that mathematical reality lies outside us, that our function is to discover or observe it, and that the
theorems which we prove, and which we describe grandiloquently as our ‘creations,’ are simply our
notes on our observations. This view has been held, in one form or another, by many philosophers of
high reputation from Plato onwards, and I shall use the language which is natural to a man who holds
it” (A Mathematician’s Apology [1940; New York: Cambridge University Press, 2005], 123–24).
14. Jeremy Bentham, introduction to The Book of Fallacies (London: John and H. L. Hunt, 1824), sec. 2,
2.
1. THE DAWN OF LOGIC
1. H. D. F. Kitto, The Greeks (1952; repr. London: Penguin, 1991), chap. 3, offers a classic statement of
these observations: “Greece, in fact, is one of those countries which have a climate, and not merely
weather” (31). He writes, “It was perhaps the greatest boon conferred upon Attica [the district
surrounding Athens] by her climate that her big assemblies could be held in the open air” (37). (These
assemblies included not only political gatherings but also the audiences for Greek drama.) In addition,
Kitto notes that the mountains of Greece, tending to run northwest to southeast, naturally carried the
early Greek peoples toward the older civilizations of the eastern Mediterranean. He explains, “The
contrast with Italy will make the point clear. The Apennines lie near the east coast; the rivers and
valleys therefore run westwards, and the fertile plains and the harbours are upon the west coast. To the
east, Italy presents her most inhospitable coastline. Civilization therefore came late to Italy” (31). We
contest none of these points—which seem to us very well taken and which are repeated by other
historians—but we contend that the division of the classical Greeks into competing states, connected
by an easy method of transportation, was an additional, crucial factor. After all, the advantages of
contact with the older civilizations of the East were long available to the Eastern peoples themselves,
and so was a temperate climate. On the other hand, what made the Greeks of the classical period stand
apart was their division into competing yet interconnected states. This factor made them different not
only from the peoples of other regions, but even from Greek peoples who lived at other times.
2. The flight of Aristotle is related by Diogenes Laertius in his Lives of the Eminent Philosophers, 5.5.
The story or Socrates’ refusal to escape is told in Plato’s Crito.
3. An excellent account of the spice trade’s origins and effects appears in Eugene F. Rice Jr. and Anthony
Grafton, The Foundations of Early Modern Europe, 2nd ed. (New York: Norton, 1994), chap. 1.
4. “Eurasia (effectively including North Africa) is the world’s largest landmass, encompassing the largest
number of competing societies. . . . Its east-west axis permitted many inventions adopted in one part of
Eurasia to spread relatively rapidly to societies at similar latitudes and climates elsewhere in
Eurasia. . . . Eurasia was the continent on which technology started its post-Pleistocene acceleration
earliest and resulted in the greatest local accumulation of technologies” (Jared Diamond, Guns, Germs,
and Steel: The Fates of Human Societies [New York: Norton, 1997], 261–62). Diamond also writes,
“There is no doubt that Europeans developed a big advantage in weaponry, technology, and political
organization over most of the non-European peoples that they conquered. But that advantage doesn’t
fully explain how initially so few European immigrants came to supplant so much of the native
population of the Americas and some other parts of the world. That might not have happened without
Europe’s sinister gift to other continents—the germs evolving from Eurasians’ long intimacy with
domestic animals” (214).
5. “Europe has a highly indented coastline, with five large peninsulas that approach islands in their

isolation, and all of which evolved independent languages, ethnic groups, and governments”
(Diamond, Guns, Germs, and Steel, 414). “The Fertile Crescent, China, and Europe differed in their
exposure to the perennial threat of barbarian invasions by horse-mounted pastoral nomads of Central
Asia” (416).
6. Among the most important consequences of these navigational networks was to encourage a money
economy, without which no ruler could assert power over long distances except by forcing his armies
to plunder the local populations they were sent to control. Once a money economy was established, a
ruler’s troops and bureaucrats could pay for their food rather than seizing it, and the funds for these
requisitions could be obtained through taxation. It was usually the development of a money economy
that gave rulers the opportunity to establish large durable states.
7. M. I. Finley estimates the total Athenian population at between two hundred fifty thousand and two
hundred seventy-five thousand. He puts the number of citizens at forty to forty-five thousand. See his
Ancient Greeks (New York: Penguin, 1963), 54–55, 73. Donald Kagan puts the number of citizens at
about forty thousand (in his Peloponnesian War [New York: Penguin, 2003], 9). The figure of a quarter
million for the total population is probably the most common guess among classical scholars, but the
number of citizens is sometimes estimated as low as thirty thousand (a number supplied by Herodotus
for an earlier period) and as high as sixty thousand. D. P. M. Weerakkody explains the difficulties of
arriving at precise estimates in his article “Demography,” in The Encyclopedia of Ancient Greece, ed.
Nigel Wilson (New York: Routledge, 2006), 213–15.
8. When women rose to military power in preindustrial times, despite the nature of ancient weapons, it
was usually as a consequence of hereditary right (directly or through marriage). The Egyptian queen
Hatshepsut came to power in this way, as did Artemisia of Halicarnassus, Cleopatra, Boudicca of the
Iceni, Zenobia of Palmyra, Elizabeth I of England, Nzinga of Ndongo and Matamba, Maria Theresa of
Austria, and (with considerable treachery) Catherine the Great of Russia.
9. The Paris Commune went so far as to make itself independent of the rest of France after France’s
defeat in the Franco-Prussian War of 1870–71. But the Commune was then put down after two months
by the French national government centered at Versailles, with much loss of life.
10. In 416 B.C., the Athenians besieged the neutral island of Melos to demand that it join the Athenian
empire. The Melians refused but were eventually forced to surrender. The Athenians then executed all
males of military age, sold the women and children as slaves, and, finally, repopulated the island with
five hundred of their own colonists. Thucydides relates these events in his History of the
Peloponnesian War, 5.84–5.116.
11. The generals had won a desperate sea battle against the Spartans, but they had also left more than a
thousand Athenian soldiers and sailors, who were clinging to wreckage in the sea, to drown. The
generals said they had abandoned the men out of military necessity, but the relatives of the dead then
accused the generals of murder and had them tried in the Athenian Assembly rather than in a court of
law. The six generals who appeared for the trial were executed, including Pericles, son of the famous
orator.
12. The Athenians had seized this empire by first taking the lead during the 470s B.C. in sweeping the
seas of the remains of the Persian fleet, a role in which they had natural advantages. Then they took
over the cooperative Greek alliance that had been established to resist the Persians—the Delian League
—and transferred all its funds to Athens. Thucydides notes that, still earlier in its history, Athens had
long been politically stable because its poor soil had made it unattractive to invaders (1.2). This made
Athens a place of refuge for Greeks driven out of other cities by civil war. In a later period, when
Athenian territory became too small for its growing population, the Athenians sent most of their
colonists eastward into the Aegean, creating ties of kinship that would later prove an advantage when
the different Greek cities fought among themselves. In addition, just before the second Persian
invasion in 480, which the Persians supported by sea, the Athenians invested money from their silver
mines at Laurium in a new fleet, on the advice of the general Themistocles. All these factors gave
Athens the lead over other Greek cities in maritime power and trade. (The Athenians transferred the
Delian League’s treasury from the island of Delos to Athens in 454.)

13. Thucydides, The History of the Peloponnesian War, 2.63. Pericles made these remarks after the
plague had reached Athens and had seriously damaged its morale.
14. The fierce orator Cleon sought this extreme measure to punish Mytilene for revolting against the
Athenian empire in 428, but his opponent Diodotus then talked the Assembly out of it.
15. Thucydides, The History of the Peloponnesian War, 2.65. According to Thucydides, the Assembly
condemned Pericles out of despair over the plague. But the Athenians then reinstated him because,
finally, “they regarded Pericles as the best man they had.”
16. Both of these remarks appear in Pericles’ Funeral Oration; Thucydides, History of the Peloponnesian
War, 2.44, 2.41.
2. ARISTOTLE: GREATEST OF THE GREEK LOGICIANS
1. Nicomachus, the father of Aristotle, served the Macedonian king Amyntas II, who reigned from 394 to
370 B.C. and whose youngest son ruled as Philip II from 359 to 336. Except for scattered clues in
Aristotle’s own writings, the primary source for most of the details of his life is Diogenes Laertius,
who says he gets his information from earlier writers and who lived many centuries later.
2. Plutarch alludes to the murders in Life of Alexander, 10; the geographer Pausanias writes of the
murders in his Description of Greece, 8.7.
3. In his Politics, book 1, Aristotle asserts that non-Greeks are natural slaves and that Greeks are natural
masters.
4. There are several versions of this story. Plutarch (Life of Alexander, 55) notes the possibility of death
by hanging but also mentions death by disease after seven months of incarceration. Diogenes Laertius
(Lives of the Eminent Philosophers, 5.5) invokes an iron cage but then says that Callisthenes was
finally exposed to a lion and devoured.
5. It is worth remembering that his philosophical dialogues were once regarded as models of grace and
charm and that Cicero called their style a “golden stream.” But these dialogues are lost, and all that
survives today are his will and his treatises. The usual guess is that the treatises are lecture notes from
the Lyceum—either Aristotle’s notes or notes from his auditors, or perhaps a mix of both. Hence their
dry but incisive manner.
6. Plato offers an amusing picture of this convention near the beginning of his Protagoras, 314e–315b.
7. The first discussion of the Indian syllogism is credited to Akṣapāda Gautama, author of the Nyāya-
sūtra, said to have flourished during the second century A.D. See the classic survey of Indian logic by
S. C. Vidyabhusana, A History of Indian Logic (1920; repr., Delhi: Motilal Banarsidass, 1988), 497–
99. A more recent discussion of Akṣapāda’s influence appears in Bimal Krishna Matilal, Character of
Logic in India (Albany: State University Press of New York, 1998), chap. 1.
8. We draw this example and the statement of its structure from Vidyabhusana, A History of Indian Logic,
61. But we have altered the presentation to make the syllogism more idiomatic. In Vidyabhusana’s
original version, the structure and example look like this:
1. Proposition—This hill is fiery.
2. Reason—Because it is smoky.
3. Example—Whatever is smoky is fiery, as a kitchen.
4. Application—“So” is this hill (smoky).
5. Conclusion—Therefore, this hill is fiery.
For a similar but more recent version of the same form, see Matilal, Character of Logic in India, 4.
9. Bimal Krishna Matilal argues against the likelihood of Greek influence in his Logic, Language, and

Reality in Indian Philosophy (Delhi: Motilal Banarsidass, 1990), 1–8.
10. Vidyabhusana, A History of Indian Logic, 505–6.
11. Matilal stresses that validity for its own sake was of little interest to ancient Indian thinkers. Instead,
their focus was on knowledge, which also required that the premises of argument be true. He remarks,
“Indian logic is not formal logic.” He writes, “The conclusion may be validly derived from the
premises . . . while it may still be a false judgement. The soundness of the conclusion in deduction also
depends upon the adequacy or truth of the premises. . . . In India, however, this distinction was not
often made, for the [Indian] philosophers wanted their ‘logically’ derived inferences or their
conclusions also to be pieces of knowledge.” And it turns out that this general preference for full
knowledge, as opposed to mere deductive validity, was equally present in the “wheel of reason”
constructed by the Buddhist philosopher Dińnāga in the fifth century A.D.—and sometimes mistaken
for a theory of deductive syllogisms. For more on these points, see Matilal, Character of Logic in
India, 6–17, and his Epistemology, Logic, and Grammar in Indian Philosophical Analysis (New York:
Oxford University Press, 2005), 96–97.
12. Likewise, if a horse has blind eyes, then the horse is a blind horse, but it does not follow that a horse
with big eyes must therefore be a big horse. These examples come from “Names and Objects,” which
was attached to the so-called Mohist Canons. See A. C. Graham, Later Mohist Logic, Ethics, and
Science (Hong Kong: Chinese University Press, 1978), 492.
13. This basic idea—that if a valid argument’s premises are true, its conclusion must also be true—is
sometimes called “extra-systematic validity.” (Susan Haack uses this term in her Philosophy of Logics
[Cambridge: Cambridge University Press, 1978], for example, 14–16, 22, 32–33, and 221–22.) On the
other hand, when logicians discuss not the validity of particular examples but the validity of forms of
argument within the context of a formal logical system, they often ask two different questions: (a) does
the form count as valid according to the inference rules of the system (in which case it is called
“syntactically valid”), and (b) does the form count as valid under all interpretations of its variables (in
which case it is called “semantically valid”; a semantically valid form never turns out to have true
premises and a false conclusion no matter what substitutions are made for the variables). If all
syntactically valid arguments turn out to be semantically valid, then the formal system is said to be
sound. And if all semantically valid arguments turn out to be syntactically valid, the system is said to
be complete.
14. The parallelisms studied by the followers of Mo Tzu fall in this class.
15. Logical form as something expressed in an artificial language was much stressed by Bertrand Russell,
though Russell also insisted, “some kind of knowledge of logical form, though with most people it is
not explicit, is involved in all understanding of discourse” (Our Knowledge of the External World
[1914; repr., New York: Routledge, 2002], 53).
So far as Aristotle’s approach goes, John Corcoran points out that Aristotle treats individual
arguments as being the same in form without ever explicitly postulating the existence of argument
forms as something over and above the individual arguments themselves. See his “Aristotle’s Natural
Deduction System,” in Ancient Logic and Its Modern Interpretations, ed. John Corcoran (Dordrecht,
Holland: D. Reidel, 1974), especially 101–8. For a concise explanation of how Aristotle’s syllogistic
can be converted into a theory of logical form in an artificial language, see Christopher Menzel,
“Logical 
Form,” 
in 
Routledge 
Encyclopedia 
of 
Philosophy, 
ed. 
E. 
Craig,
http://www.rep.routledge.com/article/X021.
In addition, some logicians, most notably Quine, have preferred to use the term “variable” in a more
restricted way and to refer to Aristotle’s As, Bs, and Cs as “schematic letters.”
16. Such questions included whether a life of renunciation is better than a life of enjoyment, whether life
has a purpose, whether the soul is eternal, whether the world is eternal, whether there is an afterlife,

whether something can be its own cause, and whether something can be simultaneously existent and
nonexistent. Matilal offers many examples of these debates.
17. Sophistical Refutations, 183b35, 184b1–5.
18. Donald Kagan, in his Peloponnesian War (New York: Penguin, 2003), 487, estimates that Athens had
roughly half as many adult male citizens by the end of the war as at the beginning. The Great Plague,
exacerbated by wartime crowding, was a contributing factor in this decline.
19. These were the original “Old Sophists,” as distinct from the New Sophists of the second century A.D.,
who represented a different intellectual movement, a revival of Greek literary studies during the
Roman period.
20. In Sicily, the Greek-speaking city of Syracuse had established a democracy in 466 and had also
permitted a large volume of new litigation over property. Against this background, Corax began to
teach the art of persuasion.
21. The phrase is used by Phoenix in the Iliad, book 9.
22. In Leviathan (1651), pt. 1, chap. 6, Hobbes defines “good” and “beautiful” in terms of the speaker’s
own appetites and desires. As for Hume, he defines virtue in appendix 1 of his Enquiry Concerning the
Principles of Morals (1751) as “whatever mental action or quality gives to a spectator the pleasing
sentiment of approbation.” In the same appendix, Hume adds, “Till such a spectator appear, there is
nothing but a figure of such particular dimensions and proportions: from his sentiments alone arise its
elegance and beauty.”
23. Socrates uses this expression in Plato’s Apology, 18c, 19b. It means “making the illogical look
logical, and vice versa.”
24. The Republic, trans. Desmond Lee, 493a–c.
25. Socrates relates the Delphic utterance in Plato’s Apology, 21a.
26. Plato, Apology, 23d–24a. Socrates adds that, had thirty votes gone the other way, he would have been
acquitted (36a).
27. Thucydides, History of the Peloponnesian War, 2.60.
28. Prior Analytics, 1.1; Posterior Analytics, 1.2.
29. In the Republic, these themes are most pronounced in books 5–7.
30. Plato depicts democracy as a degenerate form of government in his Republic, book 8. Aristotle is
critical of democracy in his Politics, books 3 and 4. He also remarks that a lawless democracy, under
the sway of demagogues, becomes much like a tyranny (bk. 4, chap. 4).
31. On November 11, 1947, Churchill told the House of Commons, “Indeed, it has been said that
democracy is the worst form of government except for all those other forms that have been tried from
time to time.”
32. Bk. 1, chap. 11.
3. ARISTOTLE’S SYSTEM: THE LOGIC OF CLASSIFICATION
1. E.g., the Republic, 526c–528e; the Meno, 82b–85b; and the Laws, 817e.
2. The example comes from Lewis Carroll, Symbolic Logic (London: Macmillan, 1896).
3. Aristotle focuses on the different ways in which one term, A, can be “predicated” of another term, B,
as in “if A is predicated of all B,” or “if A is predicated of some B.” And what he then means to assess
is what would follow if A were predicated of all the things to which B applies—or of some of the
things to which B applies. (For example, “good” might be predicated of some pleasure, which would
be his way of saying that some pleasures are good.) A characteristic feature of Aristotle’s approach is
that his schematic letters (his “A” and “B”) stand for qualities of things rather than for the classes to
which these qualities attach. Nevertheless, his analysis of predication can be easily converted into a
logic of classes, and we shall be discussing the nature of this conversion in more detail later, when we

turn to the observations of Leonhard Euler and J. D. Gergonne.
4. The propositions are now called categorical to distinguish them from hypothetical or disjunctive
statements in the form, “If A, then B,” or “Either A or B.”
5. For Aristotle, all genuine scientific knowledge derived by argument must come from “demonstration,”
and he says demonstration always relies on the kind of logic he means to explain. The Prior Analytics
then lays this logic out. Aristotle considers syllogistic arguments with more than two premises in
Posterior Analytics, bk. 1; 19–21, 22, 25, 42.
6. The term “syllogism” is Aristotle’s. Aristotelian syllogisms are traditionally stated in the manner of our
text, but when Aristotle states them, he usually uses a phrasing that, in English, is more cumbersome:
“If B belongs to all A, and C belongs to all B, then C belongs to all A.” Or he may say, “If B is
predicated of all A, and C is predicated of all B, then it is necessary for C to be predicated of all A.”
This is his way of saying that, if all As are Bs, and all Bs are Cs, then all As are Cs. And he usually
expresses these observations in conditional form (“if/then”) rather than laying out premises and
conclusions as independent sentences. His systematic analysis of categorical syllogisms appears
mainly in Prior Analytics, book 1, chaps. 4–7, and when he uses the Greek term sullogismos, he
defines it in such a way as to make clear that he has in mind only valid patterns of argument, not
invalid ones.
7. For more on the influence of geometry on Aristotle’s approach, see William Kneale and Martha
Kneale, The Development of Logic (London: Oxford University Press, 1962), 2–6.
8. In De Interpretatione, Aristotle discusses contraries and contradictories, which are fundamental ideas
in the square; in Topics, he discusses another of the square’s conceptions: subalternation. But he
neglects to discuss subcontraries, which was an idea supplied by later logicians. The square of
opposition first appears in literature in a second-century A.D. commentary by Apuleius of Madauros on
De Interpretatione.
9. “Some” can also mean “only some,” but Aristotle forgoes this meaning, perhaps for reasons of caution.
Aristotle tends to see things from the standpoint of a biologist in the field, one who has just found the
first specimen of A and has discovered that it is also a B and who leaves open the possibility that
perhaps all As are Bs. Aristotle is empirical and exacting but also circumspect, and he ends up
choosing a formulation for his data that makes the fewest assumptions and leaves open the most
options. Thus he gives “some” its most expansive reading; he means, “at least one and maybe the
whole lot.” On the other hand, if he wants to say later that only some As are Bs, then he can do so by
combining two sentences already on his list: “Some As are Bs, and yet some As are not Bs.”
10. Medieval logicians assigned names to each valid syllogism of Aristotelian logic—names based on the
types of propositions each syllogism contained and on the arrangement of its terms (its “figure”). Here,
for example, is a medieval mnemonic containing the names of valid syllogisms, some of the names
being still in circulation today:
Barbara, Celarent, Darii, Ferioque prioris;
Cesare, Camestres, Festino, Baroco secundae;
Tertia Darapti, Disamis, Datisi, Felapton,
Bocardo, Ferison habet. Quarta insuper addit
Bramantip, Camenes, Dimaris, Fesapo, Fresison.
The names of valid syllogisms are in italics, and the words in roman type refer to four possible
figures. The vowels in each name correspond to the A, E, I, or O propositions, and the letter “s” in
some of the names indicates that the syllogism contains a proposition that is logically equivalent to a
converse, which, if substituted, still yields a valid syllogism. (Thus “Some Bs are As” can be
substituted for “Some As are Bs,” and “No Bs are As” can be substituted for “No As are Bs.”)

In addition, the mnemonic intentionally omits the five “weak” syllogisms of Aristotelian logic—the
five syllogistic forms that are valid as long as the named classes have members but that offer
conclusions less sweeping than their premises can support. For example, so long as the classes have
members, this is a valid form: “All As are Bs, and all Bs are Cs; therefore, some As are Cs.” This form
is called “weak” because the same premises would also entail a more sweeping conclusion, “All As are
Cs.”
Beyond mnemonics, logicians after Aristotle developed rules for identifying valid syllogisms, some
of the rules having been already observed by Aristotle himself. And they invented methods to test the
validity of syllogisms by diagrams. (Lewis Carroll invented such a method, as did John Venn—“Venn
diagrams.”) The best way to learn the use of these techniques today is to consult a textbook of
syllogistic logic.
11. This holds good so long as the classes being described have members. In ordinary language, however,
propositions of the sort “All As are Bs” and “No As are Bs” can be interpreted hypothetically, as in
“All unicorns, if there are any, are herbivores.” In that case, the proposition could be true even if no
unicorns exist. When propositions beginning with “All” or “No” are construed in this hypothetical
manner, the interpretation is called Boolean, after the nineteenth-century English logician and
mathematician George Boole. According to the Boolean interpretation, the universal could still be true
even if its subaltern is false. Nevertheless, Aristotle’s approach is non-Boolean. The son of a doctor, he
stresses observation and examination, and his primary interest is always in classifying things that are,
not things that merely might be. In consequence, all manipulations in this chapter are non-Boolean.
12. Wittgenstein develops the idea of a language-game throughout his Philosophical Investigations,
published posthumously in 1953.
13. A good example of this circularity appears in A. J. Ayer, Language, Truth and Logic (New York:
Dover, 1952), chap. 4. Ayer writes, “The principles of logic and mathematics are true universally
simply because we never allow them to be anything else. And the reason for this is that we cannot
abandon them without contradicting ourselves, without sinning against the rules which govern the use
of language, and so making our utterances self-stultifying” (77). Again, “They [the principles of logic
and mathematics] simply record our determination to use words in a certain fashion. We cannot deny
them without infringing the conventions which are presupposed by our very denial, and so falling into
self-contradiction. And this is the sole ground of their necessity. As Wittgenstein puts it, our
justification for holding that the world could not conceivably disobey the laws of logic is simply that
we could not say of an unlogical world how it would look” (84). Put briefly, then, logical laws are
necessarily true because to deny them is to “contradict” the rules that logically govern our language.
This is plainly circular; after all, if nothing were logically necessary in the first place, then none of our
rules could be logically contradicted. The passage referred to from Wittgenstein comes from his
Tractatus Logico-Philosophicus, 3.031, and says this: “It used to be said that God could create
everything, except what was contrary to the laws of logic. The truth is, we could not say of an
‘unlogical’ world how it would look” (original emphasis). In response, perhaps one could observe that
we could indeed “say” how an illogical world would look; we just couldn’t say it in a logical way and
thus no one would understand it. Logic is what makes language intelligible from the start.
14. The exact relationship between logic and mathematics is a deep and notorious problem in modern
philosophy, and one theory, called “logicism,” asserts that mathematical truths are actually reducible to
logical ones. We shall be talking about this theory more when we look at Gottlob Frege’s work in later
chapters.
15. Euler’s approach was published in 1768, and Gergonne’s refinement appeared in 1817. The ancient
Roman philosopher Anicius Boethius also seems to have been aware of the five underlying
relationships discussed by Gergonne. See Kneale and Kneale, The Development of Logic, 189–90,
349–52.

16. Gergonne, “Essai de dialectique rationnelle,” Annales de mathématiques pures et appliquées 7 (1816–
17), 189–228. Gergonne showed that the Aristotelian theory of valid and invalid syllogisms could also
be proved using a similar geometrical technique.
17. Some versions of the impulse are perhaps traceable back to the medieval nominalists, who maintained
that universals (or the qualities indicated by general or abstract terms) are not real entities, only names.
18. Wittgenstein asserts that logical truths depend on language. “If we know the logical syntax of any
sign language, then all the propositions of logic are already given” (Tractatus, 6.124). “The rules of
logical syntax must follow of themselves, if we only know how every single sign signifies” (3.334).
19. “The exploration of logic is the exploration of everything that is subject to rules” (or “laws” or
“regularity,” Gesetzmäßigkeit) (Tractatus, 6.3).
20. Hobbes writes, “When we reason in words of general signification and fall upon a general inference
which is false, though it be commonly called error, it is indeed an absurdity, or senseless speech . . . .
And words whereby we conceive nothing but the sound are those we call absurd, insignificant, and
nonsense.” He calls absurdity “a privilege to which no living creature is subject, but man only. And of
men, those are of all most subject to it that profess philosophy.” Leviathan, pt. 1, chap. 5. Hume writes,
“When we entertain, therefore, any suspicion that a philosophical term is employed without any
meaning or idea (as is but too frequent), we need but enquire, from what impression is that supposed
idea derived? And if it be impossible to assign any, this will serve to confirm our suspicion”; An
Enquiry Concerning Human Understanding, sec. 2.
21. “Most propositions and questions that have been written about philosophical matters are not false but
senseless. We cannot, therefore, answer questions of this kind at all but only state their senselessness.
Most questions and propositions of the philosophers result from the fact that we do not understand the
logic of our language. (They are of the same kind as the question whether the Good is more or less
identical than the Beautiful.)” (Tractatus, 4.003). Also, “The right method of philosophy would be
this . . . when someone else wished to say something metaphysical, to demonstrate to him that he had
given no meaning to certain signs in his propositions” (Tractatus, 6.53). In his later Philosophical
Investigations, Wittgenstein reiterates this view: “The results of philosophy are the uncovering of one
or another piece of plain nonsense and of bumps that the understanding has got by running its head up
against the limits of language,” 119. And he explains his purpose in exposing meaningless language:
“My aim is: to teach you to pass from a piece of disguised nonsense to something that is patent
nonsense” (464).
22. The same confusion often occurs when we call someone’s utterances “unclear.” “Unclear” can mean
unintelligible, but it can also mean ambiguous. Wittgenstein runs the two ideas together when he
demands, “Everything that can be said can be said clearly” (Tractatus, 4.116). George Orwell also
considers meaningless words in his essay “Politics and the English Language,” but he seems to have in
mind yet another effect.
Orwell is concerned with pretentious diction, but he also writes this: “Words like romantic, plastic,
values, human, dead, sentimental, natural, vitality, as used in art criticism, are strictly meaningless, in
the sense that they not only do not point to any discoverable object, but are hardly ever expected to do
so by the reader,” A Collection of Essays (New York: Harcourt Brace Jovanovich, 1953), 161–62. In
this new instance, the sort of meaninglessness Orwell has in mind is apparently not disguised but
emphatically intended for the sake of pomposity. In other words, everyone realizes the expressions are
meaningless. But Wittgenstein imagines a different phenomenon: words whose unintelligibility is
supposedly disguised even from the person who utters them.
23. Wittgenstein’s conception of meaningless utterances changed over time because his conception of
meaning changed. In his Tractatus, he says a proposition is a “picture of reality,” and the meaning of
the proposition is “what the picture represents” (4.01, 2.221). As a corollary, he also maintains that
propositions of logic, which picture nothing, are “senseless” (sinnlos), even though they are not

“nonsense” (Unsinn). In his later Philosophical Investigations, however, the meaning of an utterance is
seemingly different; an utterance’s meaning is its use. (“But doesn’t the fact that sentences have the
same sense consist in their having the same use?” [20]. And, “For a large class of cases—though not
for all—in which we employ the word ‘meaning’ it can be defined thus: the meaning of a word is its
use in the language” [43]. Wittgenstein’s philosophical opponents have sometimes continued to
wonder whether words really have the same meaning because they have the same use or have the same
use precisely because they have the same meaning.) Nevertheless, the key point is that, if his basic
conjecture about meaningless utterances were correct, we would see people speaking unintelligibly and
without realizing it outside philosophy, as a matter of daily life, and not merely as a strange psychotic
experience. The phenomenon would be commonplace. But all his alleged examples of the phenomenon
come from philosophy. Over the years, Wittgenstein changed his explanation of how unwittingly
unintelligible speech occurs, but what he never questioned was whether it occurs at all.
24. Republic, 533d. One might also draw a parallel to the Tao of the ancient Chinese Tao Tê Ching,
which is asserted to be timeless, placeless, eternal, unfathomable, independent of human language (or
“names”), and independent of human conventions (being not “the way of man” but “the way of
Heaven”).
25. He makes a passing reference to the Platonic phrase in his Nicomachean Ethics, 1144a29.
26. Metaphysics, 4.3.1005b19–34, 4.7.1011b23–24.
27. This story appears in the life of Aristotle attributed to Ammonius Hermiae, and Diogenes Laertius
says there was apparently an attempt to prosecute Aristotle on a charge of impiety (5.5).
4. CHRYSIPPUS AND THE STOICS: A WORLD OF INTERLOCKING
STRUCTURES
1. Kant’s most succinct statement of the point appears in his Prolegomena to Any Future Metaphysics
(1783), pt. 1, sec. 10. In addition, Kant says that, whereas we attribute space to any physical object that
we contemplate, we attribute time to our inner experience; we can think of times without experience,
he says, but never experiences without time. He discusses these matters in the “Transcendental
Aesthetic” of his Critique of Pure Reason, trans. Norman Kemp Smith (1929; repr., New York: St.
Martin’s Press, 1965), 67–91.
2. The prayer was first published with somewhat different phrasing in 1942. Niebuhr’s original version
appeared in a monthly bulletin of the Federal Council of Churches and read as follows: “God, give us
grace to accept with serenity the things that cannot be changed, courage to change the things which
should be changed, and the wisdom to distinguish the one from the other.”
3. The lecture is titled “Time Pictures”; the Frederick Douglass Papers (Library of Congress), reel 14, 28,
quoted in David W. Blight, ed., Narrative of the Life of Frederick Douglass: An American Slave,
Written by Himself (New York: St. Martin’s Press, Bedford Books, 1993), 148.
4. From Hellenizo, which meant to spread, or side with, Greek ideas, culture, or language. The term
“Hellenistic” applies to the period just after Alexander’s death, and it reflects the effect of his
conquests, which spread Greek practices eastward.
5. Plutarch, Isis and Osiris, 360d; Claudius Aelianus, Various Histories, 2.20.
6. “Philosophy, [the Stoics] say, is like an animal, logic corresponding to the bones and sinews, ethics to
the fleshy parts, physics to the soul. Another simile they use is that of an egg: the shell is logic, next
comes the white—ethics—and the yoke in the center is physics. Or, again, they liken philosophy to a
fertile field: logic being the encircling fence, ethics, the crop, physics the soil or the trees. Or, again, to
a city strongly walled and governed by reason” (Diogenes Laertius, Life of Zeno, 7.40). Also, “All
things, they say, are discerned by means of logical study, including whatever falls within the province
of physics, and again whatever belongs to that of ethics” (7.83). And, “The term ‘duty’ is applied to
that for which, when done, a reasonable defense can be made” (7.107).
7. When the technique leads not merely to a falsehood but to an explicit contradiction, it is sometimes

called reduction ad impossibile.
8. This seems to have been an early meaning of the Greek term “dialectic.” Ancient Greek thinkers
sometimes characterized dialectic in varying ways, but these ways always involved choice. For
example, Aristotle takes dialectic to be nothing more than argument from premises that are not
necessarily true but only assumed to be for the sake of argument. Plato, on the other hand, seems to
give it a more elaborate meaning: dialectic is an argument typically expressed through interrogation; a
questioner asks and a respondent answers, and the result is a series of agreed propositions that lead
logically to a conclusion. In Plato’s dialogues, the conclusion might be nothing more than the
refutation of some earlier supposition, but it can also be a new, substantive insight. (Plato illustrates the
latter possibility in the Meno, 82b–85b, where Socrates questions a slave boy and thereby arrives at a
special case of the Pythagorean Theorem.) In all these senses, however, dialectic seems to involve
choice—the choice of whether or not to accept an initial premise—and the procedure then consists in
exploring the consequences of the choice. Thus the procedure is easily described with the Stoic
connectives “either/or” or “if-then,” and for this reason the early Stoics were often called dialecticians.
9. The Pythagoreans are supposed to have proved the irrationality of 
 by the stricter form of reductio
ad absurdum: reductio ad impossibile.
10. Here, again, we use the term “logical form” in the sense of a pattern or schema, but not in the sense of
a sentence expressed in the artificial language of a formal logical system. For more on this distinction,
see chapter 2.
11. Zeno lectured under the so-called Painted Porch, or Stoa Poikile, of Athens, which no longer exists.
12. Modus ponens (also called modus ponendo ponens) is a Latin name, invented by logicians in the
Middle Ages, as is the next name, modus tollens (also called modus tollendo tollens).
13. In practice, this form sometimes appears with its first premise as “A or B” in the sense of “A or B, but
not both,” in which case, the form is an exclusive disjunctive syllogism. This is the way Chrysippus
rendered it. Yet the premise’s real force lies in the expression “not both”; the “or” is logically
superfluous.
14. Aristotle relates this argument in his Rhetoric, bk. 2, chap. 23, 1399a20–25.
15. For simplicity, we have reduced Douglas’s argument from the logically equivalent version that he
presented in the second Lincoln-Douglas debate on August 27, 1858, which reads as follows: “He tells
you the Union cannot exist unless the States are all free or all slave; he tells you that he is opposed to
making them all slave, and hence he is for making them all free,” The Lincoln-Douglas Debates, ed.
Rodney Davis and Douglas Wilson (Chicago: University of Illinois Press, 2008), 73.
16. These equivalences hold good as long as disjunctions and conditionals are construed in a particular
way. Logicians construe disjunctions to be true so long as at least one of the disjuncts is true. And they
often construe conditionals to be true in every circumstance except where the second element
(introduced by “then”) is false even though the first element (introduced by “if”) is true. When a
conditional is construed in this manner, the logical relation it expresses is called “material
implication.” Notice that, for this particular reading, the two propositions joined by “if-then” can be
completely irrelevant to each other, yet the conditional they form still counts as true so long as the
second element is true. Thus, “If Lincoln never lived, then the week has seven days” counts as true, so
long as this statement expresses material implication. Nevertheless, conditionals are often used in
ordinary language to say something more specific: for example, that A causes B or, alternatively, that A
entails B as a matter of logical necessity, independently of any physical facts. Again, we often use
conditionals not only to mean that whenever A is true, B is true; we also mean that the truth of A is
factually relevant to the truth of B, unlike the example of Lincoln and the days of the week. These
further meanings of “if-then” are different from the one being exploited by logicians who focus on
material implication, and the reason these logicians stress the material meaning is that they are
analyzing the validity of modus ponens, modus tollens, and the like. The validity of these forms of
argument depends, at most, on the relation of material implication and nothing further. (More precisely,
asserting each of the other conditionals, whatever they mean, is still to entail a conditional in the

material sense; yet the material sense is already sufficient to serve as a premise in the valid forms of
propositional logic. Thus anything more that is asserted by the other conditionals, though interesting in
itself, is unnecessary to the validity of the forms in question.) Alternative logical systems (often called
nonclassical logics and deviant logics, in contrast to classical symbolic logic) sometimes treat
conditionals differently and generate different results, but this is a topic we shall come to in chapter 5.
17. Part of the reason these equivalences remained unexploited was that the Stoics disagreed among
themselves over how to interpret conditionals. In consequence, they took several of the intuitive
patterns we have already considered as independent axioms rather than treating each of these patterns
as logically equivalent to the others. (In some systems of modern nonclassical logic, these
equivalences are denied.) Despite the ancient disagreements, however, Chrysippus insisted that the
conditional “If anyone is born under the Dog Star, then he will not be drowned in the sea” could be
expressed equivalently as a negated conjunction: “Someone is not both born under the Dog Star and
will be drowned in the sea.” This negated conjunction is then equivalent to the disjunction: “Either
someone is not born under the Dog Star, or he will not be drowned in the sea.” The first two of these
propositions involving the Dog Star are attributed to Chrysippus in Cicero’s treatise On Fate, 8. The
debate over conditionals and their effect on Chrysippus’s system are explained in William Kneale and
Martha Kneale, The Development of Logic (London: Oxford University Press, 1962), 128–38, 158–76,
181. As it happens, at least one ancient logician seems to have treated the conditional as equivalent to
material implication in the modern sense, and this was Philo the Dialectician (or Philo the Logician),
who flourished around 300 B.C. and was a friend of Zeno.
18. Notice also that double negation in logic is distinct from a double negative in grammar. Various
languages use negative particles to indicate the denial of a proposition, but, in some instances, two
negative particles are required to express the denial. In consequence, two negative particles don’t
always constitute what logicians mean by double negation. A double negation in logic is the denial of a
denial, and a denial, by definition, is an assertion that the proposition in question is not true.
19. Expressed in this manner, the law can also be called the “principle of bivalence.” Strictly speaking,
the principle of bivalence states that every proposition of argument is either true or false. But the law
of excluded middle has been construed more narrowly in recent times (since the twentieth century
work of Jan Łukasiewicz) to state that, for any proposition A, either A or not A is true. In this more
narrow formulation, which often appears in modern technical discussions, the law of excluded middle
could be true even if the principle of bivalence is not. This turns out to be just the case in certain
systems of many-valued logic, which is a matter we shall discuss in the next chapter.
20. Specifically, the conjunction “A and B” turns out to be equivalent to the negated disjunction “Neither
not A nor not B.” And the disjunction “A or B” is equivalent to the negated conjunction “Not both not
A and not B.” In addition, as we remarked earlier, the hypothetical “If not A, then B” can be construed
as equivalent to the disjunction “A or B,” which thus makes it equivalent to the negated conjunction
“Not both not A and not B.” Beyond these points, the combination of negation and conjunction can
also be conceived as a single operation, alternative denial, and in symbolic logic this operation is
represented by the Sheffer stroke ( | ). Thus A|B means “not both A and B.” H. M. Sheffer pointed out
the possibility of this reduction in 1913, and he also pointed out that, alternatively, the single operation
could be joint denial, “neither A nor B.” As a result, with the right manipulations, albeit complicated,
either of these single operations can then generate the equivalent of ordinary negation, conjunction,
disjunction, or the hypothetical. In the case of alternative denial, the ordinary negation of A would be
A|A, “not both A and A.” The hypothetical “if A, then B” would then be A|(B|B), “not both A and [not
both B and B].” The disjunction “A or B” would become (A|A)|(B|B), “not both [not both A and A]
and [not both B and B].” And the conjunction of A and B would become (A|B)|(A|B), “not both [not
both A and B] and [not both A and B].” In a digital computer, the function “not both A and B” is
represented electronically by a NAND (Not AND) gate; the function “neither A nor B” is represented
by a NOR (Not OR) gate. But since all our other manipulations can be generated from either of these
two basic functions, these two gates are called universal gates.

The American philosopher C. S. Peirce also pointed out the possibility of such a reduction (using
joint denial) in a paper around 1880, but his work on the problem went long unnoticed. (See Kneale
and Kneale, The Development of Logic, 423.)
21. Chrysippus and his followers are said to have made axioms of modus ponens, modus tollens, the
conjunctive syllogism (expressed as an exclusive disjunctive syllogism), and the inclusive disjunctive
syllogism—and then are said to have deduced more complex forms. In addition, when writing out
these forms schematically, the Stoics are said to have used ordinal numbers, rather than schematic
letters, to stand for propositions. Thus, to express modus ponens, they wrote, “If the first, then the
second; but the first; therefore, the second.” Chrysippus is supposed to have told his teacher Cleanthes,
“Just send me the theorems; I’ll find the proofs myself.”
22. Just as modus ponens can be reiterated and combined to produce a dilemma, so can modus tollens.
Yet this further form is harder to follow psychologically. First, write out the two columns:
If A, then B.
If A, then C.
But not B.
or
But not C.
Therefore, not A.
Therefore, not A.
Then combine them into a single argument:
If A, then B.
If A, then C.
But not B or not C.
Therefore, not A.
This gives another “simple” dilemma, but we can also generate a “complex” one, meaning that it has
a pair of choices in the conclusion:
If A, then B.
If C, then D.
But not B or not D.
Therefore, not A or not C.
And here is an example of such a dilemma in use:
If Smith comes to dinner, then we must serve fish, and if Jones comes to dinner, then we must
serve fowl. But either we won’t serve fish or we won’t serve fowl. Therefore, either Smith won’t
come to dinner, or Jones won’t.
Dilemmas built out of modus tollens are called “destructive dilemmas,” whereas the ordinary sorts
are called “constructive dilemmas.”
With a little reflection, it seems clear that, in addition to dilemmas, one might also string together
three, four, five, or more reiterations of modus ponens or modus tollens in order to produce trilemmas,
quadrilemmas, and so on, but rhetorically these forms are useless. An extended essay might contain an
implicit trilemma, meaning the author traces out the implications of three different possibilities, but
rarely would one state a trilemma in compact form because there would be too many terms to keep in

mind.
23. Note, again, that we are using “excluded middle” in its traditional sense, though in modern technical
discussions we would be said to be invoking the principle of bivalence along with the definition of
negation.
24. In modern symbolic logic, the three basic compounds and negation are often defined by truth-tables,
which are lists of the circumstances under which the simplest propositions that can be formed from
these connectives are to be counted as true or false. The law of excluded middle will be represented in
these tables by the demand that all truth-values be true or false. The law of contradiction will be
represented by the rule that no proposition can have more than one truth-value simultaneously. But
once these tables are composed as definitions, any inference about the truth or falsity of other
propositions will still need to be inferred from the tables. And to call any such inference valid in the
first place is already to assume the validity of some form or other. This point was effectively made by
Quine in an article “Truth by Convention,” Ways of Paradox and Other Essays, rev. ed. (1936; repr.,
Cambridge, Mass.: Harvard University Press, 1976), 77–106. He wrote, “If logic is to proceed
mediately from conventions, logic is needed for inferring logic from the conventions” (104).
25. Or, more exactly, they can be represented as an electronic component that carries an elevated voltage
or one that doesn’t.
26. The story of Chrysippus’s death is told by Diogenes Laertius, Lives of the Eminent Philosophers, 7;
184–85. He tells the tale of Zeno’s end at 7; 28. Diogenes also says Chrysippus wrote more than seven
hundred works, but none have survived.
27. Ibid., 7; 180.
5. LOGIC VERSUS ANTI-LOGIC: THE LAWS OF CONTRADICTION
AND EXCLUDED MIDDLE
1. Aristotle asserts that some actions are always wrong in The Nicomachean Ethics, book 3, chap. 6,
1107a14–21. On the other hand, he treats ethical claims as “true for the most part” in book 1, chap. 3,
1094b20.
2. In Plato’s Apology, Socrates explains that he questioned various Athenians who thought they knew
what they didn’t know. Of one such Athenian, Socrates remarks, “He thinks that he knows something
which he does not know, whereas I am quite conscious of my ignorance. At any rate it seems that I am
wiser than he is to this small extent, that I do not think that I know what I do not know” (22d). The
translation is Hugh Tredennick’s. The importance of knowing that one does not know something is
also a common theme in early Chinese texts, appearing, for example, in Confucius’s Analects, 2.17 and
in the Tao Tê Ching, 71. A similar theme appears in the ancient Buddhist Dhammapada of the third
centuRY B.c., 5.63.
3. Herodotus, Histories, 1.133.
4. Aristotle attributes this response to Cratylus in Metaphysics, 4.5.1010a10–14.
5. Plato makes this point in the Theaetetus, 170e–171c. Protagoras’s treatise is now lost.
6. The most celebrated of the Skeptics was Pyrrho of Elis, who traveled with the army of Alexander the
Great and lived in the fourth and third centuries B.C. His extreme form of skepticism is often called
Pyrrhonism.
7. Diogenes Laertius tells the story of Diogenes and Alexander in Lives of the Eminent Philosophers,
6.38, and he relates Plato’s remark at 6.54. He depicts Plato and Diogenes as each regarding the other
as hopelessly vain (6.26).
8. Metaphysics, 4.4.1008b14–20. Aristotle’s most sustained attack on these doctrines appears in his
Metaphysics, book 4. In characterizing Aristotle’s view of the matter, we have also fallen back on
some phrasing from Sir David Ross in his Aristotle (London: Methuen and Company, 1949), 160–61.
9. Friedrich Engels, Socialism: Utopian and Scientific (1892; repr., New York: International Publishers,

1969), chap. 2, 45–47.
10. The term “sorities” (from the Greek word for “heap”) is sometimes used to denote paradoxes like this
that turn on our inability to draw a precise line between two vaguely defined states. Diogenes Laertius
attributes the “bald man” to Eubulides of Miletus in Lives of the Eminent Philosophers, 2.108.
11. Engels, Socialism, 47.
12. The story of the ship comes from Plutarch’s Life of Theseus, 23.
13. Vagueness is different from ambiguity. A term is ambiguous if it is understood in context to have
more than one meaning but vague if defined and used in such a way that we can’t tell whether it
properly applies. For example, “twilight” is in many contexts vague (twilight merges gradually and
seamlessly into night), but in most contexts it is unambiguous. Logicians often treat vague propositions
as undecidable until the vague terminology is assigned a more exact meaning. It all depends, they will
say, on what one means by “twilight.”
14. Republic, 338c.
15. These passages are taken from Foucault’s Power/Knowledge: Selected Interviews and Other Writings
1972–1977, ed. Colin Gordon (New York: Pantheon Books, 1980), 93, 131–33.
16. Descartes invokes this proposition in the second of his Meditations on First Philosophy (1641) and
then seeks to derive from it the rest of his philosophical system.
17. For example, extraordinary cases involving paradoxes of self-reference (considered at greater length
in the next chapter) induced the Polish logician Alfred Tarski to develop his “semantic theory of truth,”
which is an account of what it means for a sentence to be true in a formal, artificial language. But the
theory’s correct interpretation has been a matter of much dispute. For more on these controversies, see
Susan Haack’s Philosophy of Logics, 99–127, and Stephen Read’s Thinking about Logic: An
Introduction to the Philosophy of Logic (New York: Oxford University Press, 1995), 22–31.
18. The law is also called the law of noncontradiction.
19. In the most common systems of formal logic, every proposition is said to follow from a contradiction
—a result that is sometimes called the “principle of explosion.” (Given a contradiction, the universe of
provably true propositions “explodes,” so to speak.) A theory of logic that allows all contradictions to
be equally true will be useless in distinguishing the more credible from the less credible, but logicians
have experimented with the idea that perhaps only some contradictions could be true; a logical system
that allows only some propositions to be true and false at the same time without thereby entailing
everything else is called “paraconsistent.”
A paraconsistent system can have applications in digital computing, especially when the computer
stores large amounts of logically inconsistent data. On the other hand, a paraconsistent system is useful
in the first place only because it offers a way to infer the truth-values of some propositions from the
truth-values of others. And these inferences are definite and reliable only under two conditions: (a) the
propositions in question must indeed have the assigned values or not have them, and (b) they can’t
both have such values and not have them. These two restrictions represent the stamp of the two ancient
laws: excluded middle and contradiction. The assignments of value must be true or false but not both.
Otherwise, the logical system produces no definite results, or the results are contradictory. The same
two restrictions apply to many-valued logic (which we shall discuss later in this chapter), and what the
restrictions show is that a paraconsistent logic, like a many-valued logic, will assume the two
traditional laws in drawing inferences even though the propositions that the inferences are about can
violate the laws. (This is a distinction we shall discuss further in a short while.)
20. “Which of you can assume such murkiness, to become in the end still and clear?” (15). “The Way is a
thing impalpable and incommensurable” (21). “The further one travels, the less one knows. Therefore
the Sage arrives without going, sees all without looking, does nothing, yet achieves everything” (47).

These examples come from The Way and Its Power: A Study of the Tao Tê Ching and Its Place in
Chinese Thought, trans. Arthur Waley (New York: Grove Weidenfeld, 1958).
21. Autobiography of John Stuart Mill (New York: Columbia University Press, 1960), chap. 5; 97.
22. Plato and Aristotle both give versions of the law of contradiction such that it applies to all objects in
the world: “The same attribute cannot belong and not belong to the same thing at the same time in the
same respect.” (See Plato’s Republic, 436b, and Aristotle’s Metaphysics, 4.3.1005b19–23.) On the
other hand, when Aristotle proceeds to defend the law, he often appeals to what can be coherently
asserted and argued for (e.g., Metaphysics, 4.3.1006a20–25). This is the approach we follow here.
Historically, the law of contradiction has been construed in various ways: sometimes as a principle of
reality, sometimes as a principle of human psychology, and sometimes as a principle of argumentation.
But it is only the last of these interpretations that necessarily plays a role in logic as a discipline. The
other interpretations are matters of psychology or metaphysics, and they go further than what a
logician needs to assert.
23. A contradiction is a proposition in the form “A and not A,” and a denial, indicated by “not,” has, by
definition, a different truth-value from the thing denied. But the law of excluded middle tells us that
there are only two possibilities here: true or false. Therefore, one element of the contradiction must be
false. All this holds on the assumption that the beliefs are propositions, but, if they can’t be
propositions, then neither are they “contradictory” in the logician’s sense of the term.
24. The Barbara syllogism is in the form, “All As are Bs; all Bs are Cs; therefore, all As are Cs” or “All
As are Bs; all Cs are As; therefore, all Cs are Bs.” Its name derives from the medieval mnemonics
mentioned in chapter 3.
25. Aristotle relates some of these earlier defenses in Metaphysics, book 4.
26. Some followers of Mo Tzu, accused of contradicting themselves by approving of the execution of
robbers while simultaneously advocating universal love, went so far as to assert in response that killing
a robber was not really killing a person. (What we now call contradiction was often treated by early
Chinese thinkers as a question of whether a thing does or doesn’t properly bear a name, and many
Chinese thinkers were particularly concerned that names should be applied consistently.) See A. C.
Graham, Later Mohist Logic, Ethics, and Science (Hong Kong: Chinese University Press, 1978), 35–
43.
27. Among specialists, this is often called the principle of bivalence, the law of excluded middle being
construed more narrowly in recent times to say that, for any proposition A, either A or not A is true.
Nevertheless, we stick to the older label for the principle in question because our discussion concerns
historical periods before the modern, technical distinction between bivalence and excluded middle was
drawn. Later in this chapter, we shall be discussing many-valued logic, where the modern distinction
becomes important.
28. According to Cicero (On Fate, 10), Epicurus thought that to embrace the law of excluded middle was
to imply that all things happen because of fate. The law seems to be a necessary truth, and yet Epicurus
wanted to say that nothing in the future could be necessary.
The medieval discussion of the law seems to have emerged out of a similar concern and from a
cryptic remark in chapter 9 of Aristotle’s De Interpretatione on the truth or falsity of statements about
the future. Some took Aristotle to be saying that whether a sea battle will occur on a particular day in
the future is potentially true or false, but actually neither.
Much later in history, Łukasiewicz turned again to the status of statements about the future and
began to develop his theory of many-valued logic (first at the University of Warsaw, where he taught
beginning in 1915, and later at the Royal Irish Academy in Dublin, where he was a professor of
mathematical logic from 1946 until his death in 1956).
The interpretation of Peirce’s experiments is still disputed, but they appear in his unpublished Logic

Notebook.
29. Zadeh was born in Azerbaijan and educated in Iran and the United States and did much of his work at
the University of California at Berkeley.
30. Nicomachean Ethics, 1.3.1094b20–22.
31. Fuzzy logic can be developed in a variety of different ways, in addition to the way Zadeh developed
it, and some systems go back further. Relevance logic (also called relevant logic) seeks to study
inferences involving conditionals, where a conditional is true only if the antecedent is relevant to the
consequent. Relevance logic also seeks to limit judgments of validity to arguments where the premises
are genuinely relevant to the conclusion, so that, for example, not any conclusion would follow
automatically from a pair of contradictory premises. Intuitionist logic focuses on statements that are
not only true in the traditional sense but “provably true,” meaning they can be shown to follow
necessarily from the axioms and postulates of a formal system. And a paraconsistent logic allows
contradictions to be sometimes true, though not always, and then seeks to determine what
consequences would follow from such a rule.
For a sometimes skeptical treatment of alternative logical systems, see Susan Haack’s Deviant
Logic, Fuzzy Logic: Beyond the Formalism (Chicago: University of Chicago Press, 1996). For a more
recent and more sympathetic account, see Graham Priest’s Introduction to NonClassical Logic, 2nd ed.
(New York: Cambridge University Press, 2008). The term “deviant logic” was introduced by Quine in
his Philosophy of Logic, 2nd ed. (Cambridge, Mass.: Harvard University Press, 1986), chap. 6, and is
now used more narrowly than “nonclassical logic.” A nonclassical logic is any system that differs from
the so-called classical logic developed by Gottlob Frege, Bertrand Russell, and Alfred North
Whitehead. But a nonclassical logic need not contradict classical logic; instead, it might simply apply
to areas left untreated by classical logic. The term “deviant,” on the other hand, is usually applied to a
system that assumes classical logic to be in some way mistaken.
32. Here is another example of the general convergence of these systems in ordinary cases—an example
in the form of a disjunctive syllogism: “Either I have fired all six shots from my revolver, or my
revolver is still loaded; it is not the case that I have fired all six shots from my revolver; therefore, my
revolver is still loaded.” In many logical systems, the conclusion of this argument follows necessarily
from the premises, but in some alternative systems, the conclusion does not follow because the system
allows some propositions to be true and false simultaneously. Nevertheless, the usual move in using
such an alternative system is to say that, in this particular context, there is no “truth-value glut” in the
premises (meaning no instances of a proposition that is simultaneously true and false), and so the
conclusion can be properly inferred after all. On the other hand, if a logician using an alternative
system were unable to find some reason for inferring the specific conclusion in this context, then the
logician would be obviously incompetent to handle firearms. And on a firing range he would be rightly
regarded as a fool. In sum, the ultimate result of the analysis is the same even though the theoretical
explanations are different.
33. We draw this example from a good source of exotic cases, Priest’s Introduction to NonClassical
Logic, 15. In classical logic, the peculiar conclusion of this argument follows from the premise. In
classical logic, “if-then” expresses material implication, which makes either half of the conclusion true
so long as its antecedent is false or its consequent is true. Thus, if John were in Paris but not in
England, then, according to the premise, he would also be in France, which would be enough to make
the second half of the conclusion true. But if John were not in Paris, then the first half of the
conclusion would be true, since it would have a false antecedent. And since the conclusion as a whole
is an “either/or” disjunction, the conclusion turns out to be true so long as either half is true. As a
result, whether or not John is in Paris (and whether or not he is in London), the conclusion as a whole
is true so long as the premise is true. In a nonclassical system, by contrast, the conditional “if-then”
might be interpreted differently, and in that case the argument could turn out to be invalid.

More generally, propositions that assert material implication are particularly weak in the sense that
statements in the form “if A, then B,” in ordinary language, often express more than “A materially
implies B.” In consequence, seeming paradoxes will often arise when the conclusion of an argument
asserts nothing more than “A materially implies B.” Paradoxes will also arise when a premise of the
argument asserts the negation of “A materially implies B.” (When the premise asserts the negation of a
material conditional, the premise will be, in fact, a strong assertion; it will entail that the antecedent is
actually true.) In such cases, the argument’s conclusion will then seem to say more, in ordinary
language, than it really does, or its premise will seem to assume less than it really does. As a result, the
argument, if valid, will turn out to entail less than one might think—or to assume more than one might
think. (Here is another of these paradoxical cases, again from Priest: “It is not the case that if there is a
good god the prayers of evil people will be answered. Hence, there is a god.” If the premise is
construed as the negation of a material conditional, then its antecedent, by definition, must be true.
And in that case, the conclusion follows necessarily.)
34. Haack relates some of this controversy in her Deviant Logic, Fuzzy Logic, 148–67.
35. The logician and philosopher Arthur Prior once generated, in effect, a spoof of this idea by inventing
a set of formal rules for a logical operation that he called “tonk.” The rules were designed so that,
given the definition of tonk, any proposition could be validly deduced from any other. In consequence,
all arguments turned out to be logically valid, given the rules. Prior argued that not everything becomes
logical merely because there is a formal system of rules to govern it. Prior’s tonk first appeared in his
article “The Runabout Inference Ticket,” Analysis 21 (1960–61): 38–39. (His article’s main focus was
on the question of what determines the meaning of a logical connective.)
36. We borrow this language from Descartes’s first meditation.
37. Moore’s remark comes from his “Proof of an External World,” in George Edward Moore,
Philosophical Papers (1939; repr., New York: Collier Books, 1959), chap. 7; 144. Johnson’s remark on
Berkeley’s philosophy is related by his biographer James Boswell, in his Life of Johnson (1791; repr.,
New York: Penguin, 2008), 248, August 6, 1763, entry.
38. These quotations come from Hume’s Enquiry Concerning Human Understanding (1748), sec. 12, pt.
2.
39. In Hume’s posthumous Dialogues Concerning Natural Religion (1779), Cleanthes says to Philo,
“Whether your skepticism be as absolute and sincere as you pretend, we shall learn by and by, when
the company breaks up: we shall then see, whether you go out at the door or the window; and whether
you really doubt if your body has gravity, or can be injured by its fall; according to popular opinion,
derived from our fallacious senses, and more fallacious experience” (1.132).
40. If we infer the correctness of a line of reasoning from its usefulness, then the inference makes sense
only on condition that the reasoning’s usefulness depends on its correctness. (By analogy, if we infer
the presence of fire from the presence of smoke, then the inference is reasonable only if the smoke
depends on the fire, not the reverse.)
6. LOGICAL FANATICS, CIRCULAR REASONING, AND
DESCARTES’S FUNDAMENTAL PRINCIPLE
1. Luther posted his Ninety-Five Theses in 1517 and broke with the papacy in 1520.
2. In 1543, Luther published “On the Jews and Their Lies,” in which he said, “We are at fault in not
slaying them.”
3. More believed that the circulation of English-language Bibles would encourage mistaken
interpretations of Christianity by the unlearned, and he worked zealously to suppress such Bibles—
especially William Tyndale’s English translation, published in parts beginning in the 1520s. More’s

approach to religious dissent was generally consistent: even in his fantasy Utopia (1516), where he
discusses atheism, he says atheists on the imaginary island are left unmolested if they remain silent or
argue only with the learned, but they are forbidden to make their case to the common people. Our
modern notions of freedom of speech and of the press emerged only later, especially in John Milton’s
“Areopagitica” (1644). We shall discuss the view of later writers like Milton in our next chapter.
4. These words, long attributed to Luther, do not appear in contemporary accounts of the incident.
5. Among other topics, Ramus was interested in the logic of arguments that made reference to specific
individuals rather than to classes—as denoted by general terms. (One of his examples was, “Octavius
is the heir of Caesar; I am Octavius; therefore, I am the heir of Caesar.”)
6. “Having taken their fancies for realities, they make right deductions from them. . . . In short, herein
seems to lie the difference between idiots and madmen: that madmen put wrong ideas together, and so
make wrong propositions, but argue and reason right from them; but idiots make very few or no
propositions, and reason scarce at all”; John Locke, An Essay Concerning Human Understanding,
2.11.13.
7. Discourse on Method (1637), pt. 3.
8. “Since the primary premises are the cause of our knowledge—that is, of our conviction—it follows
that we know them better—that is, are more convinced of them—than their consequences, precisely
because our knowledge of the latter is the effect of our knowledge of the premises.” Posterior
Analytics, in The Basic Works of Aristotle, ed. Richard McKeon (New York: Random House, 1941),
1.2.72a30–34.
9. In addition, though Aristotle confines his remarks on this subject to “demonstration,” which is a form
of deduction, his dictum turns out to apply with equal force to all forms of rationally persuasive
argument—deductive or inductive. We shall be looking at induction in more detail in the next chapter.
10. Posterior Analytics, 1.2.72a25–30. Aristotle speaks of these causes as “inhering” in us.
11. Descartes relates this example in his letter of dedication to the faculty of theology of the University of
Paris, which he attached to his Meditations.
12. Posterior Analytics, 1.2.72b25.
13. Notice that Aristotle’s principle still allows that different people might reason in different directions;
one person might believe in A because of B while another believes in B because of A. It also allows
that we sometimes change our order of reasoning. (Perhaps we used to believe in A because of B but
now believe in B because of A; circular reasoning, by contrast, is believing in A because of B and in B
because of A simultaneously.) In addition, Aristotle often speaks as if the premises of demonstrative
knowledge must always be the same for all people, but this further claim is distinct from his principle,
and we are inclined to think it is mistaken. In logic, reasonable people sometimes find different
principles intuitively obvious and then reason in different directions to the same result. It seems
strange, however, to say that these complementary results can’t all be demonstrative knowledge.
14. The accusation is known as the “Cartesian circle.” Descartes had seemed to say that his knowledge of
God’s existence depended on his knowledge that his clear and distinct ideas were true—but also that
his knowledge of his clear and distinct ideas depended on his knowledge of God. Descartes replied that
this was a misunderstanding of his view, and he discusses the question in the second, fourth, and fifth
sets of objections and replies that he published along with Meditations.
15. Meditations on First Philosophy, 2, 25.
16. He arrives at this assertion in Discourse on Method, pt. 4 and Meditations, pt. 2.
17. Hume dismisses “Cartesian doubt” in An Enquiry Concerning Human Understanding, sec. 12, part 1.
The interpretation of Descartes’s hyperbolic doubt, and criticisms of his outlook, are discussed further
in Michael Shenefelt, “Descartes: Philosophy as the Search for Reasonableness,” in The Many Faces
of Wisdom, ed. Phil Washburn (Upper Saddle River, N.J.: Prentice Hall, 2003), 125–40.
18. A disjunction is different from a series of premises. The series of premises A, B, and C is logically

equivalent to their conjunction “A and B and C.” But this is never equivalent to their disjunction “A or
B or C,” except in the trivial case where A, B, and C all happen to be identical. (Thus the conjunction
“A and A and A” would be equivalent to the disjunction “A or A or A.”) A conjunction, by definition,
asserts the truth of all the compound’s elements whereas a disjunction, by definition, only asserts the
truth of “at least one.”
19. Ancient mathematicians had long tried to prove that the parallel postulate was logically entailed by
Euclid’s other postulates; however, mathematicians in the nineteenth century showed that no such
proof was possible. This discovery then suggested the possibility of non-Euclidean geometries that
would deny the parallel postulate altogether, and, once this new idea was entertained, it led naturally to
the question of whether the new, alternative geometries would be internally consistent. Mathematicians
then began to wonder how they could know their foundations were correct. As for Frege’s own view of
geometry, he believed Euclidean geometry was true and non-Euclidean geometries were untrue.
20. Wilfried Sieg, trans., “Formalization,” in Cambridge Dictionary of Philosophy, ed. Robert Audi
(Cambridge: Cambridge University Press, 1999), 318.
21. A paradox in the more general sense is anything that runs counter to common opinion, which is the
word’s original meaning in Greek. Rousseau’s assertion that man is born free and is everywhere in
chains is a paradox in the general sense but not in the logical sense.
22. Eubulides of Megara (in Greece) flourished in the fourth century B.C.
23. Aulus Gellius, Attic Nights, 5.10.
24. These words appear in Frege’s June 22, 1902, letter to Russell. Gottlob Frege, Philosophical and
Mathematical Correspondence, trans. H. Kaal (Chicago: University of Chicago Press, 1980), 132.
25. Russell’s paradox had been implicit in one of Frege’s foundational axioms (his “Basic Law V”). But
Russell proposed to avoid the paradox by supposing that sets can never be members of themselves.
According to Russell, sets (or classes) can have members, and there can also be sets of sets—or classes
of classes. But a class of physical objects is different in kind from a class of such classes, and so
Russell said these two sorts of things (classes of physical objects and classes of such classes) were of
different “logical type.” And he suggested further that a class could only have members of a lesser
logical type—not members of its own type. As a result, no class could have itself as a member. Russell
discusses these difficulties in his “theory of types,” and he includes this theory, along with many
crucial innovations from Frege, in his Principia Mathematica, published with Alfred North Whitehead
in 1910–13. Among Frege’s most important techniques incorporated by Russell is quantification,
which symbolically translates statements in the form, “For all x, if x is . . .” or “For all x, either x
is . . .” or “There exists an x such that x is . . .” and so on. We shall discuss quantification in chapter 9.
26. Frege tried to show that all arithmetical truths could be proved consistently from logical principles
alone as long as arithmetical objects like numbers were properly defined—an approach to mathematics
called logicism. But Frege’s goal was perhaps partly thwarted by a theorem and by a crucial corollary
published by Gödel in 1931 in an article called “On Formally Undecidable Propositions of Principia
Mathematica and Related Systems.” Gödel’s article showed that no formal system rich enough to
include number theory as a consequence could be shown within that system to be both consistent and
complete. Some statements of the system will have to remain unprovable, or, if provable, the system
will be inconsistent. The exact impact of Gödel’s result on logicism is complicated and subject to
different interpretations.
27. Goodman defended the alleged circularity of deductive logic in a lecture he gave in 1953 and then
repeated in his book Fact, Fiction and Forecast (1955; repr., Cambridge, Mass.: Harvard University
Press, 1983), chap. 3, sec. 2. We believe his argument to be fallacious, though still widely influential in
the field. He writes, “I have said that deductive inferences are justified by their conformity to valid
general rules, and that general rules are justified by their conformity to valid inferences. But this circle
is a virtuous one. The point is that rules and particular inferences alike are justified by being brought
into agreement with each other. A rule is amended if it yields an inference we are unwilling to accept;
an inference is rejected if it violates a rule we are unwilling to amend. The process of justification is

the delicate one of making mutual adjustments between rules and accepted inferences; and in the
agreement achieved lies the only justification needed for either” (italics in the original), 64.
The confusion here isn’t hard to see (or so we believe). The argument’s unstated conclusion is that
circular reasoning is sometimes rationally persuasive; this is the point to be proved. Yet the argument
seeks to justify this result by deriving a more crucial, intermediate conclusion—that, in the case of
deductive logic, a circular justification is the only one possible. The argument’s apparent force comes
from showing that nothing in logic is reliable if taken in isolation; since anything might be disputed (it
is suggested), everything else must serve as a basis. As a result, then, circular reasoning seems to be
the only justification available.
Look carefully, however, and you will see that the argument really shows nothing of the kind. We
are asked what we would do if a general rule or particular inference proved unacceptable, but the key
word is “if.” Goodman observes,
A rule is amended if it yields an inference we are unwilling to accept,
and,
an inference is rejected if it violates a rule we are unwilling to amend.
These are the argument’s premises, yet neither of these statements shows that anything in logic is
really unacceptable or questionable. Each statement contains the word “if,” and, consequently, neither
of them says what is the case but only what would be the case if something else were the case. Both
statements, being hypothetical, are perfectly consistent with the possibility that a good deal of logic is,
in fact, so intuitively obvious that it needs no further justification. Certain parts of logic—some of its
rules, some of its inferences, or more probably a mix of both—might still be so plainly true that they
provide an undisputed basis on which the rest of the discipline can rest.
This mistake underlies a great many defenses of circularity; we are asked how we would justify an
opinion A if it were disputed, and we reply by appeal to B; we are asked how we would justify B if it
were disputed, and we reply by appeal to C. The interrogator then takes us through the whole gamut of
our opinions in a given field and infers that all our opinions must be disputed. There then seems to be
no way to justify anything except by a circle.
In our view, by contrast, some logical judgments are intuitively obvious, and this intuition is reliable
even in isolation from general rules. We believe some simple judgments of validity (and some simple
judgments of arithmetic) to be known independently of any appeal to formal principles—just as the
existence of our hands can be known independently of any complicated philosophical argument to the
effect that a physical world truly exists. We also believe this knowledge to be real even though other
simple judgments in logic are sometimes mistaken (just as judgments about the causes of our
sensations are sometimes mistaken). We realize, of course, that a skeptical philosopher can deny the
reality of the knowledge we invoke, but, in our opinion, this denial makes no more sense than denying
the existence of one’s body. Skepticism toward all simple judgments about what actually follows from
what can certainly be imagined, and it can also be feigned; but it can never be sincerely believed
except in cases of insanity. Imagined skepticism is quite different from real skepticism, and, in logic no

less than in metaphysics, the sane skeptic still leaves by the door and not by the window.
28. The root meaning of “paradigm” is example or model, from paradeigma, which means something
“shown beside.” Nevertheless, Kuhn had also used it to mean an entire scientific theory with all its
assumptions. He later fell back on an alternative vocabulary of “exemplars,” “models,” and “symbolic
generalizations”—all meaning different things. For Kuhn’s difficulties with the word “paradigm,” see
his “Reflections on My Critics,” in Criticism and the Growth of Knowledge, ed. Imre Lakatos and Alan
Musgrave (New York: Cambridge University Press, 1974), 271–72.
29. Kuhn writes,
Circularity is characteristic of scientific theories. . . . When paradigms enter, as they must, into a
debate about paradigm choice, their role is necessarily circular. Each group uses its own
paradigm to argue in that paradigm’s defense.
The resulting circularity does not, of course, make the arguments wrong or even ineffectual.
The man who premises a paradigm when arguing in its defense can nonetheless provide a clear
exhibit of what scientific practice will be like for those who adopt the new view of nature. That
exhibit can be immensely persuasive, often compellingly so. Yet, whatever its force, the status
of the circular argument is only that of persuasion. It cannot be made logically or even
probabilistically compelling for those who refuse to step into the circle.
He also writes,
None of these remarks is intended to indicate that scientists do not characteristically interpret
observations and data. On the contrary Galileo interpreted observations on the pendulum,
Aristotle observations on falling stones, Musschenbroek observations on a charge-filled bottle,
and Franklin observations on a condenser. But each of these interpretations presupposed a
paradigm . . . . Those examples typify the overwhelming majority of research. In each of them
the scientist, by virtue of an accepted paradigm, knew what a datum was, what instruments
might be used to retrieve it, and what concepts were relevant to its interpretation. Given a
paradigm, interpretation of data is central to the enterprise that explores it. (The Structure of
Scientific Revolutions, third edition [Chicago: University of Chicago Press, 1996], chaps. 8, 9,
10; 90, 94, 122).
30. Ibid., 8.79.
31. Ibid., 12.146.
32. Coherentism is the view that rationality depends on reconciling theory with data, or axioms with
theorems, and it is often contrasted with “foundationalism,” which stresses the need for reliable
foundations. Nevertheless, properly interpreted, both outlooks are correct. The large disjunction we
have just looked at serves as a foundation, but it also favors the theory that best coheres with the
individual disjuncts that compose it. The notion that coherentism and foundationalism must be
incompatible comes from overlooking the possibility that the foundational premises are disjunctive.
“Reflective equilibrium” is a term introduced by John Rawls in his Theory of Justice (Cambridge,
Mass.: Harvard University Press/Belknap Press, 1971).
33. Kuhn writes, “I have argued that the parties to such debates [between rival paradigms] inevitably see
differently certain of the experimental or observational situations to which both have recourse. Since

the vocabularies in which they discuss such situations consist, however, predominantly of the same
terms, they must be attaching some of those terms to nature differently and their communication is
inevitably only partial. As a result, the superiority of one theory to another is something that cannot be
proved in the debate” (postscript, Structure of Scientific Revolutions, 198).
Kuhn concedes that the communication can be “partial,” and our complaint is that a rationally
persuasive argument can indeed proceed from an agreement that is only partial—so long as the
unaccepted parts of the debate are combined with other, mutually accepted parts in a single disjunctive
premise. A disjunction can still be rationally persuasive even if some of its parts are rejected.
34. As a technical aside, some specialists have also supposed that Descartes’s idea of foundations must be
somehow incompatible with another image of reasoning, supplied by Quine, that is now widely
accepted: the image of a web or fabric. But we think this further supposition is also mistaken.
Quine says our reasonable beliefs, especially in science, fit together like a swatch of fabric in that
some beliefs can be replaced only by disrupting a good many others. To replace the basic laws of
physics with contrary laws, for example, would probably force countless revisions in chemistry, optics,
aerodynamics, and so on. Just so, replacing strands in the center of a swatch would usually require
much reworking of the swatch as a whole. On the other hand, to replace a specific belief about the
orbit of a particular comet might force little or no change in our other scientific beliefs. Just so,
replacing a strand at the edge of the swatch does little damage to the rest.
Quine’s picture seems different from Descartes’s. Yet the two pictures are compatible, and perhaps
the easiest way to see this compatibility is to lay one picture over the other. Except in more
complicated cases where a thinker falls back on a disjunction, we can picture Quine’s swatch as being
like the façade of a building.
First, imagine Quine’s fabric to be woven not out of small fibers, but out of large timbers. Next,
picture this “fabric” being lifted so it stands on its edge. The whole fabric can now be laid upright
against a building to form its façade—a building constructed in a style like Descartes’s—and the
swatch’s center, containing the most general physical laws, will then be adjacent to the middle of the
building’s superstructure. (The top of the superstructure will correspond to further particular inferences
in science—inferences drawn from the general laws—and the lower edge of the swatch of timbers will
correspond to the building’s foundations; for Quine, these lower elements will consist in empirical
observation reports.) Conceived in this way, the two writers are actually giving us the same picture but
viewing it from different angles. Of course, Descartes believes in an a priori foundation, whereas
Quine favors the empirical and says general theories in science “face the tribunal of sense
experience . . . as a corporate body.” Nevertheless, both writers still imagine a similar structure of
inferences, but seen from different angles. And the reason they view it from different angles is that
they are investigating different aspects of rational belief.
Specifically, Quine pictures what happens when beliefs are replaced. Thus replacing a belief at the
center of the swatch means replacing a good many others as well. Descartes, however, pictures what
happens when beliefs are merely abandoned—or what would be the case if those beliefs had never
been inferred at all. Thus Descartes thinks any beliefs we infer must rest on our premises as the
superstructure of a building rests on its foundation. If we remove the foundation—or abandon it, so to
speak—the superstructure collapses, yet if we remove the superstructure, the foundation can remain.
This is still very different, however, from saying what would happen if a superstructure were replaced

by a new and incompatible one. If a new superstructure were too heavy, it might also require a new
foundation, and at no time does Descartes rule this out, explicitly or implicitly. In short, the two writers
aren’t contradicting each other with these images; they are discussing different effects, and this is
because they are working on different historical problems.
Quine describes the sort of change witnessed in the twentieth century—when a theory like Einstein’s
replaces a theory like Newton’s. For him, the point at issue is the revision of scientific principles.
Descartes, however, is the product of a very different century, the seventeenth, and the chief trouble
then was not that well-supported theories were giving way to other well-supported theories. Instead,
the chief problem was that too many people were advancing unsupported theories, especially during
the wars of religion. In consequence, though both writers deal with differences of opinion, the
differences arise for different reasons. For Quine, they come from new empirical discoveries and
insights, but for Descartes they come from unwarranted speculation and from a good deal of visceral
hatred and willfulness. As a result, whereas Quine describes what happens when the theory of an
Einstein challenges the theory of a Newton, Descartes describes what happens when the certainty of a
Luther gives way to the skepticism of a Montaigne. Quine shows how principles are adjusted to
preserve logical consistency, but Descartes shows how they are discarded when they lack evidentiary
support.
On one further point (beyond these remarks), we are inclined to differ with Quine. We disagree
when he says (or at least suggests) that logical laws are analogous to individual strands at the center of
the swatch of fabric. To our way of thinking, logical laws play a different role. Instead, to fit within
Quine’s model, logical laws shouldn’t be strands at all. Rather, they should be the pattern that defines
the stitch or weave. Logical laws determine how any one strand connects by argument with any other
strand so that, if you eliminate logic, all the strands come apart. Moreover, since some forms of
inference can be construed as logically equivalent to others, one might also say that the stitch of the
fabric looks different if you flip the fabric over. What looks like modus tollens when viewed from one
side of the fabric might look like a conjunctive syllogism when viewed from the other. (On this view,
in addition, the laws of contradiction and excluded middle might be likened to the physical properties
that prevent individual strands from melting into one another when they touch; the two basic laws are
like the physical properties of the strands. Trying to reason without the two basic laws, at least in
ordinary cases, is like trying to weave with filaments of soft wax; the result is a glutinous mass.)
Descartes’s view of reasoning and its foundations appears (among other places) in his Discourse on
Method, 2.13, 3.22, 3.29, and in the second paragraph of his Meditations on First Philosophy. Quine’s
view has been much discussed, and his most celebrated statement of it appears in his “Two Dogmas of
Empiricism,” reprinted in his collection of essays, From a Logical Point of View, 42.
35. Discourse on Method, pt. 2; his work on analytic geometry was published as one of several
appendices to the Discourse.
7. WILL THE FUTURE RESEMBLE THE PAST? INDUCTIVE LOGIC
AND SCIENTIFIC METHOD
1. The record stating the specific grounds of Bruno’s condemnation has been lost, but Bruno refused to
recant a variety of assertions, including these. The usual guess is that the ones for which he was
sentenced to death involved magic and theology.

2. The book, published in Leyden, was called Dialogues Concerning Two New Sciences. In confessing to
the Inquisition at his trial in 1633, Galileo was forced to sign an abjuration that said, in part, “This
Holy Office had enjoined me by precept, entirely to relinquish the false dogma which maintains that
the Sun is the center of the world, and immoveable, and that the Earth is not the center, and moves; not
to hold, defend, or teach by any means, or by writing, the aforesaid false doctrine; and after it had been
notified to me that the aforesaid doctrine is repugnant to the Holy Scripture, I have written and printed
a book, in which I treat of the same doctrine already condemned, and adduce reasons with great
efficacy in favor of it, not offering any solution of them; therefore I have been adjudged and
vehemently suspected of heresy, namely, that I maintained and believed that the Sun is the center of the
world, and immoveable, and that the Earth is not the center, and moves.” Galileo goes on to “abjure,
execrate, and detest the aforesaid errors and heresies, and generally every other sect contrary to the
above said Holy Church.” The book that had aroused the Inquisition’s wrath was his Dialogue
Concerning the Two Chief World Systems (1632).
3. Newton demonstrated these results in his Principia Naturalis Principia Mathematica.
4. Historians have long used the term “middle class” in this different way; for example, Eugen Weber
uses the term in this sense in his celebrated lecture series The Western Tradition (DVD, The Annenberg
Foundation, produced by WGBH Boston, 1989). See Episode 28, “The Rise of the Middle Class.”
5. The book’s real title was La Logique ou l’art de penser (“Logic, or the Art of Thinking”), but it
became known as the Port Royal Logic because its authors were leaders of a theological and
philosophical movement initially centered near what had been the abbey of Port-Royal-des-Champs,
southwest of Paris.
6. In his Second Treatise of Government, Locke argues that his principles apply not only among the
inhabitants of England but also among the indigenous peoples of Peru, Florida, Brazil, and Mexico.
See section 14 and sections 102–8. Taking a phrase from the Apostle Paul, Locke asserts that the law
of nature is “written in the hearts of mankind” (11). A similar outlook also prevails in his Essay
Concerning Human Understanding, where he writes, “He that will look into many parts of Asia and
America will find men reason there, perhaps, as acutely as himself” (4.17.4).
7. The various caliphates of medieval Islam, tracing their legitimacy to the rule of the Prophet, were
monarchies. Al-Farabi regarded democracy as among the best of the “ignorant cities,” but he thought it
was still a place where few citizens would accept a truly virtuous ruler, except as a temporary accident.
Democracies were devoted to freedom and diversity, said al-Farabi, and their diversity caused them to
produce “both good and evil to a greater degree than any other ignorant city.” He thought them still
inferior to the ideal city, which would be controlled by a supreme, enlightened ruler. Much of his
analysis derives from Plato’s Republic, and his portrait of democracy is strongly reminiscent of
Athens. Aquinas discounts the practicality of democracy in chapter 5 of his treatise on kingship (De
Regimine Principum), where he argues that democracies usually end in tyranny and that monarchies,
for all their faults, still involve fewer evils than other forms. Elsewhere in his work (Summa
Theologica, 1.2.105), he offers a brief endorsement of mixed constitutions.
8. Many of Machiavelli’s heroes in The Prince are upstarts. Agathocles of Syracuse, for example, was the
son of a potter. As for Hobbes, his Leviathan, chap. 13, insists that men, for the most part, are
physically and intellectually equal. And though Hobbes favors rule by an absolute sovereign for the
sake of peace, this sovereign doesn’t need to come from a particular social class. Rulers on the
mythical island of Thomas More’s Utopia are said to be elected from a class of professional scholars,
originally recruited from the general population. And Locke allows for many different forms of
government in his Second Treatise, so long as the government protects each person’s equal right to life,
liberty, and property. None of these writers insists on a biologically superior ruling class of the sort
envisioned in Plato’s Republic, bk. 5. (Machiavelli, More, and Locke were all sons of lawyers; Hobbes
was the son of a village vicar and was raised by an uncle who prospered in the glove trade.)
9. John Milton, Areopagitica and Of Education (Wheeling, Ill.: Harlan Davidson, 2011) 6, 49.
10. On Liberty, ed. Gertrude Himmelfarb (1974; repr., London: Penguin Books, 1985), chap. 2; 81.

11. Ibid., 80, 98.
12. A similar transition is perhaps under way today in China, where a vigorous sea trade, centered on the
coastal cities, has generated increasingly assertive middle classes, along with a growing urban
proletariat. In European history, the growth of the proletariat sometimes led to industrial class conflict
of the sort described by Karl Marx, and the rise of the commercial middle classes led to increasing
agitation for free-speech rights and for definite, knowable laws. All these tendencies now seem matters
of increasing discussion in China. (In his Second Treatise, Locke expresses the demand for knowable
laws by insisting on a “standing rule to live by” and by calling for “promulgated established laws, not
to be varied in particular cases, but to have one rule for rich and poor, for the favorite at court and the
countryman at plough.” “Where there is no law,” Locke adds, “there is no freedom.” See sections 22,
142, and 57. In recent years, various Chinese intellectuals have expressed similar views.)
13. Aristotle conceived induction as an argument that infers a general principle from the contemplation of
particular cases; nevertheless, induction can also include arguments that draw conclusions about new
particular cases from premises about old particular cases (often called argument by analogy). There are
also a few forms, like the statistical syllogism, that don’t begin with particular statements in the
premises but that are usually classed as inductive anyway, because their conclusions are still probable
rather than logically necessary. (A statistical syllogism would take the form, “Roughly ninety percent
of the population is well fed, and the subject in question is a member of that population; therefore, the
subject in question is probably well fed.”) In addition, there is something called “perfect induction,”
but to a logician, this last form is actually deductive and somewhat misnamed. Perfect induction, by
definition, is an argument in which we examine every case under discussion, one by one or by groups,
and include all this information in the premises. The conclusion then refers only to those cases already
treated in the premises. As a result, the argument is deductive and the conclusion follows necessarily.
Perfect induction relies on exhaustive enumeration and is common in mathematics.
14. Much of the logic of the imperial age was captured at its close by the great teacher and philosopher
Anicius Boethius (480–526).
15. Modal logic seeks to analyze the validity of arguments that include assertions about possibility and
necessity, as in “It is possible that all As are Bs”; “All As are necessarily Bs”; “If A, then necessarily
B”; or “If A, then possibly B.” Aristotle inaugurated the formal study of such arguments but with very
mixed results, and today, modal logic is generally regarded as an advanced topic involving many
continuing controversies. Kneale and Kneale (The Development of Logic [London: Oxford University
Press, 1962]) give an account of Aristotle’s efforts, 81–96, and they relate work on such matters by the
Stoic philosophers, 117–28. Susan Haack discusses twentieth-century efforts in modal logic in her
Philosophy of Logics (Cambridge: Cambridge University Press, 1978), 170–203, and Graham Priest
relates recent work, Introduction to NonClassical Logic, 2nd ed. (New York: Cambridge University
Press, 2008), 20–80.
16. On the whole, medieval Arab logic was closely bound up with theology but also with jurisprudence,
where many scholars saw the crucial importance of logic in legal argumentation. Arab logicians were
especially interested in syllogistic reasoning, and Avicenna in particular often broke with Aristotle’s
theory of the syllogism on specific points.
17. Early in the 1200s, various doctrines of Aristotle were condemned by Christian authorities in Paris.
Viewed across the many centuries of the Middle Ages, however, this condemnation was an aberration.
Most medieval scholars, before and after, took Aristotle as an authority.
18. C. S. Lewis, The Discarded Image: An Introduction to Medieval and Renaissance Literature
(Cambridge: Cambridge University Press, 1964), 11.
19. Today’s judiciary often settles questions of fact by inductive causal arguments or by offering an
inductive explanatory account, but most questions of law are still questions of consistency: “Is this
ruling, or is it not, consistent with the law?” Here the typical method is once again analogical or
deductive, much like the reasoning of the medievals.
20. Among the most striking exceptions to this tendency are Descartes and Spinoza, who, despite their

independence of mind, still construct systems that are largely deductive. Descartes also arrives at
conclusions that are decidedly Scholastic.
21. Locke, An Essay Concerning Human Understanding, bk. 4, chap. 17; 4.
22. In addition to the work of Mill and Peirce, influential scholarly studies were supplied by John
Maynard Keynes, Bertrand Russell, Rudolf Carnap, Karl Popper, Carl Hempel, Nelson Goodman, and
Mill’s philosophical rival William Whewell, to name only a few. The current literature on induction
and scientific method is immense.
23. Locke, An Essay Concerning Human Understanding, bk. 4, chap. 15; 1.
24. Hume’s assertion appears in his Enquiry Concerning Human Understanding, sec. 4, pt. 2. Mill
expounds a version of the principle of the uniformity of nature in his System of Logic, Ratiocinative,
and Inductive, bk. 3, chaps. 3, 21. Mill calls the principle the “ultimate major premise in all cases of
induction,” and he writes, “The validity of all the Inductive Methods depends on the assumption that
every event, or the beginning of every phenomenon, must have some cause, some antecedent, on the
existence of which it is invariably and unconditionally consequent.” Bertrand Russell defends his own
version of the principle of induction in his classic introduction to epistemology, The Problems of
Philosophy (1912), chap. 6. Russell’s principle has four parts, but the relevant passage, as applied to
tomorrow’s sunrise, is as follows (where A corresponds to the passage of twenty-four hours and B to
the appearance of the sun): “When a thing of a certain sort A has been found to be associated with a
thing of a certain other sort B, and has never been found dissociated from a thing of the sort B, the
greater the number of cases in which A and B have been associated, the greater is the probability that
they will be associated in a fresh case in which one of them is known to be present.” Among other
writers who offered principles of induction was Keynes in his Treatise on Probability (1921).
25. An Enquiry Concerning Human Understanding, sec. 4, pt. 2; 32.
26. The difference is that, with other samplings, especially random sampling as evaluated by statisticians,
we typically know how much of the whole has been sampled. For example, the statistician knows what
percentage of a population has been randomly interviewed for an opinion poll, or what portion of an
assembly line’s total lot of products has been randomly drawn and examined. With the most common
types of induction, however, no one knows how much of the world’s phenomena our experience
represents. Scientists’ laws span an indefinite future, and even if we know the age of the universe and
when it will end, we don’t know if there were past universes or if there will be future ones, nor do we
know how many events have already slipped past us unobserved during our lifetimes to which our
general laws may have applied. (For example, the gravitation of the sun has been bending light in
violation of Newton’s laws through all human history, and yet no one observed this fact until the work
of Albert Einstein caused scientists to look for it. The earlier, Newtonian conception of the universe
turned out to depend on a more restricted sample of phenomena than most scientists had supposed.) In
consequence, the only judgments of probability that pertain to the most common forms of induction
are ones that remain after our knowledge of the total number of possibilities is stripped away. We know
that, everything else being equal, the more a phenomenon has been observed in the past, the more
likely that it will be repeated in the future. As a technical matter, logicians and mathematicians have
also tried to develop systems for quantifying the probability of an inductive conclusion, a field now
called “inductive probability,” but the results are complicated and still highly controversial.
27. Russell’s Problems of Philosophy, chap. 6, offers an able defense of the idea that all inductive
arguments assume a principle of induction as an implicit premise. Difficulties with this approach are
pointed out by Max Black, “The Justification of Induction,” in A Modern Introduction to Philosophy,
ed. Paul Edwards and Arthur Pap (New York: Free Press, 1973), 156–64. Traditionally, the task of
reducing inductive arguments to deductive ones by supplying a defensible inductive principle has been
called the “problem of induction.” But various commentators have remarked that the task is actually a
problem only if we assume in advance that the reduction needs doing. Instead, the very attempt to
accomplish the reduction seems to rest on a philosophical mistake.
28. This is part of the burden of Kant’s difficult Critique of Pure Reason (1781).

29. An Enquiry Concerning Human Understanding, sec. 7, pt. 2; 60.
30. Mill defends this position in his System of Logic, bk. 3, chap. 3, and he argues that the universality of
causal laws is also an empirical proposition that specific scientific discoveries have tended to vindicate
over time. See bk. 3, chap. 21.
31. An Enquiry Concerning Human Understanding, sec. 5. Remarking on the fact that our instinct does
indeed seem to match what actually happens in the world, Hume adds, “Here, then, is a pre-established
harmony between the course of nature and the succession of our ideas” (44).
32. The logic of sampling is complicated, but the part embedded in common sense is not. The operative
principle is that a succession of repeated random events is less likely the longer the succession is. This
principle is expressed in classical probability with the formula for calculating the odds of a random
successive repetition of an event (assuming the repetitions are independent): if an event has a
probability of n, then the probability of its random successive repetition is n raised to the power of the
number of repetitions. Thus, for an event with a probability of n, the probability of three random
repetitions is n3, or n × n × n. For any event whose probability is less than one, which is absolute
certainty, the probability of its random successive repetition must then be smaller than the probability
of the initial event, and each successive repetition makes the probability of the entire sequence even
less probable.
8. RHETORICAL FRAUDS AND SOPHISTICAL PLOYS: TEN CLASSIC
TRICKS
1. Benjamin Jowett, trans., Lysis, 218d.
2. Aristotle’s list, though pioneering, is short by modern standards, and some of its ingredients are
esoteric. He lists thirteen fallacies: equivocation, amphiboly, composition, division, accent,
misinterpretation of a figure of speech, accident, a dicto secundum quid ad dictum simpliciter,
ignoratio elenchi, petitio principii, consequent, non causa pro causa, and complex question. Aristotle’s
analysis of these fallacies is best studied by consulting a commentary.
3. This arrangement lasted until 1911, when the Parliament Act mandated that any public bill passed in
three consecutive sessions of the Commons would go to the monarch to become law.
4. British and French rivalry for control of the seas played out over five periods of general warfare, from
1689 to 1815. After the Battle of Trafalgar, won on the strategy of Lord Horatio Nelson, British naval
supremacy was dominant, and it remained so until the Second World War, when finally surpassed by
the power of the United States.
5. When commercial and agricultural interests clashed, the primary sticking point was over Britain’s
Corn Laws, which limited the importation of grain from abroad to the apparent advantage of domestic
farmers. The Corn Laws were finally abolished in 1846.
6. So said Diderot at the beginning of his own article “Encyclopédie.”
7. The chief exception to this tendency was in the 1790s, when many dissidents were threatened with
arrest for their alleged sympathies with the revolution in France. Among these was the great radical
Thomas Paine, author of The Rights of Man, who was convicted of sedition in absentia, though he was
already beyond the reach of the British government. (Paine had escaped from England with the help of
the poet William Blake, and he had become an elected representative in France’s revolutionary
government until being arrested under the Terror. Barely escaping execution, he later returned to the
United States, where he had served much earlier in the Continental Army under George Washington.
He died in obscurity in Greenwich Village, New York, in 1809.)
8. In addition to increasing agitation in the press, the growth of coal-driven industry in the Midlands of
England had caused large shifts in the country’s population with the effect that the Commons had
become even less representative than before. In reaction, reformers called mass protest meetings in
many English cities, and the result was the Reform Bill. Nevertheless, the Reform Bill still left the

right to vote restricted to property owners, and universal male suffrage didn’t arrive in Britain until
1884. Women had no vote in Britain until 1918, when those over thirty years old were finally admitted
to the franchise.
9. Mill stresses this theme in his “Of Individuality,” chap. 3.
10. All examples from Bentham come from The Book of Fallacies (London: John and H. L. Hunt, 1824).
The vague generality is discussed in pt. 4, chap. 3; 230–57.
11. Richard Rovere, Senator Joe McCarthy (New York: Harcourt, Brace and World, 1959), 110.
12. This distinction is harder to draw in legal systems that allow the same person to be both witness and
prosecutor. For example, in Plato’s Apology, Socrates attacks the personal character of one of his
prosecutors, Meletus, and readers sometimes infer that Socrates must be arguing ad hominem. But
Athenian courts, unlike most modern courts, permitted the same person to be prosecutor and witness
simultaneously. In fact, Meletus was also a chief witness, and thus Socrates’ attack is not properly
understood as an ad hominem; rather, it is an attempt to undermine the credibility of a witness.
13. Bentham, The Book of Fallacies, pt. 2, chap. 1; 129, 132–33.
14. Ibid., pt. 4, chap. 6; 271–78, where Bentham discusses sham distinctions.
15. This explanation of the term’s origins is sometimes contested.
16. Rovere, Senator Joe McCarthy, 115.
17. Outfoxed: Rupert Murdoch’s War on Journalism, dir. Robert Greenwald. The film is described in a
detailed article by Ronald V. Bettig and Jeanne Lynn Hall, “Outfoxing the Myth of the Liberal Media,”
in The Rhetoric of the New Political Documentary, ed. Thomas W. Benson and Brian J. Snee (Southern
Illinois University Press, 2008), 173–204. Some of the memos can be viewed online on Media Matters
for America at www.mediamatters.org. During the Reagan presidency, the strategy of the “message of
the day” or the “line of the day” was perfected by Michael Deaver, who was the White House deputy
chief of staff.
18. Jacques Derrida, “Structure, Sign, and Play in the Discourse of the Human Sciences,” in The
Structuralist Controversy, ed. Richard Macksey and Eugenio Donato (Baltimore: Johns Hopkins
University Press, 1972), 247.
19. “The Sokal Hoax: An Exchange,” The New York Review of Books 43, no. 15 (October 3, 1996): 56.
20. George Orwell, “Politics and the English Language,” in A Collection of Essays (New York: Harcourt
Brace Jovanovich, 1953), 165, 170–71.
21. We should add that it seems possible that his “Einsteinian constant,” discussed by Weinberg, is
actually a term intended to describe Einstein’s expression “space-time,” and in that case the question
being discussed by Derrida and Hyppolite might be whether the expression “space-time” is ambiguous.
(Thus they ask whether space-time is really “constant.”) This sort of talk must be maddening to a
professional physicist, for whom a constant is something very different. If we interpret Derrida’s
meaning in this further way, however, then the expression “space-time” will still turn out to be
ambiguous only in a trivial sense; in that case, we suggest, Derrida’s point is a sham insight.
9. SYMBOLIC LOGIC AND THE DIGITAL FUTURE
1. Inspired by an earlier device designed by Blaise Pascal, Leibniz built a calculating machine, the
“Stepped Reckoner,” whose principles were also used in the calculating machines of later inventors.
The artificial language he had in mind was his characteristica universalis, which was supposed to
make possible the use of his calculus ratiocinator (apparently a method or device for solving reasoning
problems).
2. Augustus De Morgan, Formal Logic, or the Calculus of Inference, Necessary and Probable (London:
Taylor and Walton, 1847), 27.
3. Much influenced by Boole and De Morgan, Peirce developed a logic of relations with quantification,
and he devised many innovative techniques, including a functional equivalent of the Sheffer stroke

(using joint denial instead of alternative denial) and the use of a truth table to explain logical
connectives. His work, either published as short articles or recorded in his personal notebooks and
letters, was often overlooked in his lifetime, and he fell into great poverty; he sometimes lacked money
for food and fuel and sometimes wrote out his ideas on the backs of used pieces of paper. The
American philosopher William James took up a collection on Peirce’s behalf and also arranged for him
to give a series of paid lectures in Cambridge, Massachusetts, near Harvard. Despite these adversities,
however, Peirce’s insights have had a lasting impact on logic as a discipline as well as on epistemology
and the philosophy of science. Bertrand Russell judged him “one of the most original minds of the
later nineteenth century.” Russell also thought him the “greatest American thinker ever.” Bertrand
Russell, Wisdom of the West (New York: Doubleday, 1959), 276.
4. Modern notational systems vary, and, in many of them, the material conditional is represented by a
hook or horseshoe (⊃) rather than by a forward arrow (→). Later in this chapter, we shall be using
other symbols that can also vary in modern notational systems. We shall represent disjunction by a vel
or wedge (∨), and we shall represent conjunction by an ampersand (&). But conjunction can also be
represented in modern systems by a period or a dot (·) or by an inverted wedge (∧). In addition, we
shall represent negation with a tilde (~), but many systems use a dash (−) or a dash with a tail (¬). We
shall represent the universal quantifier with a variable in parentheses, as in (x), but this same quantifier
can also be represented by an inverted letter A, followed by a variable, as in ∀x.
5. Specifically, professional logicians today often use the word “mechanical” to describe logical systems
where there is an effective decision procedure for determining whether any statement in the system is
or is not a consequence of its axioms; if the question can be answered in a finite number of steps
according to fixed rules, then the system is “decidable.” Some logical systems, such as propositional
logic, are decidable in this sense, but others are not. On the other hand, when it comes to checking a
proof that such a statement is a consequence of the axioms, the question is different. This further
question can be settled mechanically too. The problem of determining whether a logical or
mathematical system is effectively decidable was first posed in a precise way only in the twentieth
century by the German mathematician David Hilbert.
6. A System of Logic, Ratiocinative and Inductive, 8th ed. (1843; repr., New York: Harper and Brothers,
1874), bk. 4, chap. 6, sec. 6, 494. The full quotation is as follows: “The complete or extreme case of
the mechanical use of language, is when it is used without any consciousness of a meaning, and with
only the consciousness of using certain visible or audible marks in conformity to technical rules
previously laid down. This extreme case is nowhere realized except in the figures of arithmetic, and
still more the symbols of algebra, a language unique in its kind, and approaching as nearly to
perfection, for the purposes to which it is destined, as can, perhaps, be said of any creation of the
human mind. Its perfection consists in the completeness of its adaptation to a purely mechanical use.
The symbols are mere counters, without even the semblance of a meaning apart from the convention
which is renewed each time they are employed, and which is altered at each renewal, the same symbol
a or x being used on different occasions to represent things which (except that, like all things, they are
susceptible of being numbered) have no property in common. There is nothing, therefore, to distract
the mind from the set of mechanical operations which are to be performed upon the symbols.”
7. G. C. Smith, ed., The Boole-De Morgan Correspondence, 1842–1864 (Oxford: Clarendon Press,
1982), 25. In the same letter, De Morgan acknowledges that, though both he and Boole are using fully
symbolic systems, they are nevertheless deriving their results in logically different ways. He also
remarks that he had not seen the direct connection to the algebra of numbers.
8. This complaint is said to have come from Ariston of Chios, who was a student of the Stoic Zeno of
Citium.
9. These remarks of Ada Lovelace (who was Augusta Ada King, Countess of Lovelace, and daughter of
the poet Lord Byron) come from a series of notes she attached to her translation of Luigi Menabrea’s
article “Sketch of the Analytical Engine Invented by Charles Babbage.”

10. Many nineteenth-century mathematicians did indeed hope that there might be a mechanical way to
calculate accurate logarithmic tables, which were necessary for navigation and science. This was one
of the main goals behind Babbage’s plan for the Difference Engine.
11. The discovery of non-Euclidean geometry added to this concern for certainty, as did new discoveries
in algebra. For a concise summary of these developments, see P. H. Nidditch, The Development of
Mathematical Logic (New York: Free Press, 1962), chap. 4.
12. From “The Art of Discovery” (1685) in Leibniz: Selections, ed. Philip P. Wiener (New York: Charles
Scribner’s Sons, 1979), 51.
13. Peirce pointed out this possibility in a letter to his former student Allan Marquand, dated December
30, 1886, and he included diagrams for two simple circuits. One of the circuits can be used to express a
conjunction of three propositions (A and B and C), and the other can be used to express a disjunction
(A or B or C).
14. For a recent overview of nineteenth-and twentieth-century developments in the philosophy of
mathematics, including the “foundational crisis,” see Jeremy Avigad’s article “Philosophy of
Mathematics,” in The Edinburgh Companion to Twentieth-Century Philosophies (Edinburgh:
Edinburgh University Press, 2007), 234–51.
15. Frege expressed what logicians now call the existential quantifier, “There exists an x such that,” with
a logically equivalent formulation that could be read, “It is not the case that, for all x, x is not . . .”
Thus, in Frege’s hands, “There exists an x such that x is a dog” would become, “It is not the case that,
for all x, x is not a dog.” This makes the English harder to follow, but it also allowed Frege to build his
system out of fewer components (negation, a sign for “if-then,” and the so-called universal quantifier),
without a special symbol for the existential quantifier. As it happens, the pioneering notation Frege
worked out on his own is quite different in appearance from the notations that have now become
standard in logic.
16. Recall that “If A, then B” can be construed as equivalent to “Either not A or B” and also to “Not both
A and not B.” These equivalences hold as long as the “if-then” expresses so-called material
implication, as discussed in chapter 4, endnotes 16 and 20.
17. The remark comes from Frege’s Begriffsschrift, eine der arithmetischen nachgebildete
Formelsprache des reinen Denkens (1879) and is quoted in Benson Mates, Elementary Logic (New
York: Oxford University Press, 1965), 218.
18. In the nineteenth century and early twentieth century, logicians typically laid down axioms as parts of
these formal systems—the method called axiomatization—but, in later times, they have increasingly
tended to express the content of these axioms purely in terms of rules for inferring one proposition
from another, the method called “natural deduction.”
19. This finding was published by Kurt Gödel (1930).
20. Proving the consistency of these systems is such highly specialized work that those who do it need to
ask one another to check their results for errors. In consequence, the reliability of these systems is
often far less certain than many of the ordinary conclusions we reach by commonsense reasoning,
without a system. In addition, there are no obvious physical experiments to confirm or test the
soundness of these systems. If a system of formal logic is subject to empirical confirmation at all, the
method of obtaining this confirmation is deep and remote. As a result, though commonsense reasoning
is often mistaken, it is also, in some instances, far more reliable than anything a professional logician
can construct. Nevertheless, logicians are sometimes tempted to suppose that their rational confidence
in the most basic logical truths, even in ordinary situations, must somehow depend on their confidence
in a delicate formal system; in our view, this assumption is mistaken.
In fact, the true basis of one’s confidence either in a formal system or in a basic method of inference
could be something quite different from anything that such a system contains. In the case of a method
like modus ponens, one might even believe in its validity in ordinary situations by way of a simple

inductive generalization; one might suppose that, because modus ponens seems intuitively valid in so
many particular instances, it is probably valid in most or all instances of the same sort. Alternatively,
one might believe in its validity by analogy: if modus ponens is valid in some instances (one might
suppose), then it is probably valid in other, similar instances. These arguments aren’t deductive, of
course, but, then again, not everything rational needs to be deductive. (Whether a form is valid is, by
definition, a matter of deductive logic, but why one believes it to be so is at least partly a matter of
empirical psychology, and here the facts might differ from one person to the next. Different people
might have different reasons for believing, yet each of these reasons might still be rational.)
21. Turing advanced this thesis in an article published in 1950, “Computing Machinery and Intelligence,”
Mind 59: 433–60. In the article, Turing takes note of Babbage’s plan for the Analytical Engine, and he
discusses Ada Lovelace’s view of it. He also acknowledges considerable difficulty in deciding exactly
how one should define the word “thinking” in the first place. Nevertheless, in the body of his article,
he gives sustained arguments to the effect that most of the things we normally call “thinking” in human
beings might also be done by machines, provided the machines were sufficiently sophisticated. He
remarks, “I believe that at the end of the [twentieth] century the use of words and general educated
opinion will have altered so much that one will be able to speak of machines thinking without
expecting to be contradicted.”
22. It can print out this list because there is a mechanical decision procedure for all of propositional logic
and also for all categorical syllogisms. There are, however, other sorts of logical arguments for which
no such procedure is possible.
23. A. N. Whitehead, An Introduction to Mathematics (New York: Oxford University Press, 1911), chap.
5; 61.
24. The philosophy professor John Searle, who teaches at the University of California at Berkeley, has
made this point in a somewhat different way, by asking us to imagine a person who knows English but
not Chinese. This person is then locked in a room but has a written story in Chinese and a series of
written questions about the story, also in Chinese. In addition, the person in the room has been given a
rule book that instructs him in English to write out various Chinese characters whenever he sees other
Chinese characters. But the book explains nothing to him about what any of the characters mean. In
effect, then, the person in the room is in a position to do what various computers can do, if properly
programmed. They can be given a story and questions about the story, and they can then print out
answers to the questions. If the person in the room then practiced very hard and had a very good rule
book, he might be able to match some of a computer’s results. Yet no one would conclude, says Searle,
that the person in the room therefore understood Chinese, the story, or any of the questions. In short,
we know from introspection that our mental process in mechanical manipulations is quite different
from our mental process in trying to understand something. (Searle first offered this “Chinese room
argument” in 1980 in an article titled “Minds, Brains and Programs,” Behavioral and Brain Sciences 3:
417–57.)
10. FAITH AND THE LIMITS OF LOGIC: THE LAST UNANSWERED
QUESTION
1. Among Abelard’s many achievements in logic was a fully developed system of propositional logic,
created independently of Stoic sources, which were largely unavailable to him. For more on Abelard’s
profound impact on medieval logic, see William Kneale and Martha Kneale, The Development of
Logic (London: Oxford University Press, 1962), 202–24.
2. We quote from Betty Radice’s translation of Abelard’s memoir, Abelard to a Friend: The Story of His
Misfortunes (Historia calamitatum), in The Letters of Abelard and Heloise (New York: Penguin,
1974), 58. Our subsequent quotations from Abelard and Heloise are drawn from the same volume.
3. Her reasoning is investigated in a sympathetic and insightful study by étienne Gilson, Heloise and

Abelard (1948; repr., University of Michigan Press, 1960), chap. 2; 21–36.
4. The uncle’s exact role in the attack is murky. It is unclear whether he was ever punished for being
involved in it.
5. Letters of Abelard and Heloise, 149.
6. The question of authenticity has been much discussed. Gilson includes in his preface this haunting
anecdote: “I remember now the day when, in the manuscript room of the Bibliothèque Nationale, I
shamelessly molested a kindly scholar, a perfect stranger to me otherwise, but whose Benedictine habit
marked him as my victim. I wanted him to decide on the spot and without delay the precise sense of
the words conversatio and conversio in the Benedictine Rule. ‘And why,’ he asked me, ‘do you attach
such importance to those words?’ ‘Because,’ I replied, ‘on the sense of these words depends the
authenticity of the correspondence between Heloise and Abelard.’ Never did a face express greater
surprise. Then, after a silence: ‘It is impossible for that to be unauthentic. It is too beautiful.’”
7. Gibson, Heloise and Abelard, 85–86.
8. Letters of Abelard and Heloise, 78. Abelard quotes from the Gospel according to Matthew 15:14.
9. In the prologue to Sic et non, Abelard also stresses the importance of examining the context of a
quotation from authority, especially to determine whether the quoted authority is expressing his own
view or that of an opposing thinker.
10. Letters of Abelard and Heloise, 270–71.
11. Al-Ghazali had been a university professor in Baghdad in the second half of the eleventh century, but
he resigned and became a Sufi mystic after the murder of his political patron. He denied that logical
argument alone could supply answers to a number of vital religious questions, but he did indeed allow
for the use of logic in other areas of inquiry. In his later writings, he also defended a view that
philosophers now call “occasionalism”—to the effect that, when fire appears to burn cotton, what
actually happens is that God burns the cotton only on those occasions when fire is brought into contact
with it. As a result, according to this view, fire doesn’t burn the cotton at all; God burns it but chooses
to do so whenever the fire is near. Philosophically, al-Ghazali’s occasionalism raises the peculiar
question of how anyone can know that causes are truly related to their apparent effects, a difficulty also
explored by Hume and Immanuel Kant.
12. Republic, 7.539b.
13. Many of the same points were stressed centuries later by Edmund Burke in his famous Reflections of
the Revolution in France (1790).
14. He expresses this criticism in his Theologia Christiana, 3.20.
15. One sees this assumption, for example, in the “Third Meditation.” Once Descartes convinces himself
that he lacks good reasons for believing that life isn’t a dream, he says, “All this makes me recognize
sufficiently well that up to now it has not been by a valid and considered judgment, but only by a blind
and rash impulse, that I have believed that there were things outside of myself.” Thus, once he lacks
good reasons for his belief, the belief becomes “blind and rash.” (Laurence J. Lafleur, trans.,
Meditations on First Philosophy, 3, AT VII, 40.)
16. For example, when it comes to determining which premises are self-evident, how exactly do we
distinguish between a self-evident proposition and a mere article of faith? In particular, is it self-
evident that all events have causes? And is this assumption self-evident even if we try to account for
possible exceptions in quantum mechanics? Again, is it reasonable to assume that the golden rule—we
should do unto others as we would have them do unto us—is also self-evident? Likewise, should we
take it as self-evident that all human beings should have equal rights to life, liberty, and the pursuit of
happiness? Or are all these propositions somehow matters of faith? Many thoughtful writers have
expressed opinions about these matters, but they have also disagreed. And in that case, who can say for
sure what all rational premises must have in common?
17. Boethius offers this conception in his Consolation of Philosophy, 5.5.

APPENDIX: FURTHER FALLACIES
1. Begging the question is sometimes defined more narrowly, but the definition given here is roughly
equivalent to the one supplied by H. W. Fowler in his Dictionary of Modern English Usage (1st ed.,
repr., introduction and notes by David Crystal [Oxford: Oxford University Press, 2009]) under the
heading of petitio principii: “founding a conclusion on a basis that as much needs to be proved as the
conclusion itself.” Fowler is careful to note that arguing in a circle is then a variety of this fallacy, and
he calls begging the question “the English version of petitio principii.” The other varieties are offering
a conclusion as one of its own premises and offering a premise that never appears as a conclusion but
that still stands as much in need of proof. This conception of the fallacy derives from Aristotle’s idea in
the Posterior Analytics, 1.2.72a30–34, that the premises of a rational demonstration must always be
“better known” than the conclusion. Nevertheless, there is also a narrower conception of the fallacy
that restricts it to a circular argument or to offering the conclusion as its own premise. This narrower
conception also has roots in Aristotle, who gives such a definition in the Prior Analytics, 2.16.64b35–
65a4.
2. Moore advances this view in his Principia Ethica (1903; repr., Cambridge: Cambridge University
Press, 1970), sec. 13.
3. Adolf Hitler, Mein Kampf, trans. Ralph Manheim (1925–26; repr., Boston: Houghton Mifflin, 1971),
vol. 1, chap. 10; 231–32.
4. This distinction between the analyzable and the unanalyzable was effectively embraced by John Locke
when he distinguished between simple ideas and complex ideas (An Essay Concerning Human
Understanding, 2.2.1). Philosophers have long disputed just what sort of entity an idea is and what sort
of entity a meaning is. Some have even reached the view that there are no such things as meanings; on
the contrary, there are only meaningful utterances. Nevertheless, whoever is right in these disputes, the
disputes themselves are often tangential to logic. What matters instead is whether one can distinguish
between different “ideas” and different “meanings” in practice, whatever their metaphysical status.
5. As Hume remarks, “The most perfect philosophy of the natural kind only staves off our ignorance a
little longer”; An Enquiry Concerning Human Understanding, sec. 4, pt. 1.
6. A logical analogy compares one argument to another, and, when used to expose an error, it likens the
offending piece of reasoning to some other piece that is patently absurd. The writer or speaker tries to
show that the opponent’s argument either has the same form as the absurdity or rests on the same
premise.
7. The correct number of valid forms depends on whether the interpretation is Boolean or non-Boolean.
8. Logical analogies are often used in logic classes, but this is because both students and teachers have
the means to write them out and study them carefully.
9. In his Treatise of Human Nature (1739), David Hume writes, “In every system of morality which I
have hitherto met with, I have always remarked that the author proceeds for some time in the ordinary
way of reasoning, and establishes the being of a god, or makes observations concerning human affairs;
when of a sudden I am surprised to find that instead of the usual copulations of propositions is and is
not, I meet with no proposition that is not connected with an ought or an ought not. This change is
imperceptible, but is, however, of the last consequence. For as this ought or ought not expresses some
new relation or affirmation, it is necessary that it should be observed and explained; and at the same
time that a reason should be given for what seems altogether inconceivable, how this new relation can
be a deduction from others which are entirely different from it” (3.1.1).
10. Richard Rovere, Senator Joe McCarthy (New York: Harcourt, Brace and World, 1959), 132.
11. Sophia Phoca, Introducing Postfeminism, illus. Rebecca Wright (New York: Totem Books, 1999), 12.
12. George Orwell, “Politics and the English Language,” in A Collection of Essays (New York: Harcourt
Brace Jovanovich, 1953), 162
13. Aristotle asserts the “priority” of the state in his Politics, 1.2.1253a. On the other hand, in his
Rhetoric (1.13.1373b), he says Antigone’s defiance of the state is in accordance with “universal law,”

which is his expression for natural law, as opposed to human (or “particular”) law.
14. In his preface to A Contribution to the Critique of Political Economy (1859), Marx calls the economic
system of society (or its “mode of production”) the “foundation” of its legal and political
superstructure. But a large literature has grown up over the question of how to interpret his foundation
metaphor.
15. The doctrine that existence precedes essence appears in Sartre’s Being and Nothingness, trans. Hazel
E. Barnes (1943; repr., New York: Washington Square Press, 1993), pt. 4, chap, 1, sec. 1, and is
attributed to the German philosopher Martin Heidegger. Among the best known statements of Sartre’s
outlook is his lecture “Existentialism is a Humanism,” or “L’existentialisme est un humanisme”
(1946).

BIBLIOGRAPHY
Abelard, Peter. The Letters of Abelard and Heloise. Translated by Betty Radice. New York: Penguin,
1974.
Aristotle. The Basic Works of Aristotle. Edited by Richard McKeon. New York: Random House, 1941.
Arnauld, Antoine, and Pierre Nicole. Logic or the Art of Thinking. Edited by Jill Vance Buroker.
Cambridge: Cambridge University Press, 1996.
Avigad, Jeremy. “Philosophy of Mathematics.” In The Edinburgh Companion to Twentieth-Century
Philosophies, edited by Constantin V. Boundas, 234–51. Edinburgh: Edinburgh University Press,
2007.
Ayer, A. J. Language, Truth and Logic. New York: Dover, 1952.
Bentham, Jeremy. The Book of Fallacies. London: John and H. L. Hunt, 1824.
Bettig, Ronald V., and Jeanne Lynn Hall. “Outfoxing the Myth of the Liberal Media.” In The Rhetoric
of the New Political Documentary, edited by Thomas W. Benson and Brian J. Snee, 173–204.
Carbondale: Southern Illinois University Press, 2008.
Black, Max. “The Justification of Induction.” In A Modern Introduction to Philosophy, edited by Paul
Edwards and Arthur Pap, 156–64. New York: Free Press, 1973.
Bochenski, Joseph. A History of Formal Logic. Translated by Ivo Thomas. Notre Dame, Ind.:
University of Notre Dame Press, 1961.
Boethius. The Consolation of Philosophy. Translated by Victor Watts. New York: Penguin Classics,
2000.
Boole, George. The Boole-DeMorgan Correspondence, 1842–1864. Edited by G. C. Smith. Oxford:
Clarendon Press, 1982.
———. The Mathematical Analysis of Logic. Cambridge: Macmillan, Barclay, and Macmillan, 1847.
Boswell, James. The Life of Samuel Johnson. New York: Penguin Classics, 2008.
Burke, Edmund. Reflections of the Revolution in France. New York: Penguin Classics, 1997.
Carroll, Lewis. Symbolic Logic. London: Macmillan, 1896.
———. “What the Tortoise Said to Achilles.” Mind 4 (1895): 278–80.
Corcoran, John. “Aristotle’s Natural Deduction System.” In Ancient Logic and Its Modern
Interpretations, edited by John Corcoran, 85–132. Dordrecht, Holland: D. Reidel, 1974.
De Morgan, Augustus. Formal Logic, or the Calculus of Inference, Necessary and Probable. London:
Taylor and Walton, 1847.
Derrida, Jacques. “Structure, Sign, and Play in the Discourse of the Human Sciences.” In The
Structuralist Controversy, edited by Richard Macksey and Eugenio Donato, 247–72. Baltimore, Md.:
Johns Hopkins University Press, 1972.
Descartes, René. Discourse on Method and the Meditations. Translated by Laurence J. Lafleur. Upper
Saddle River, N.J.: Prentice Hall, 1960.
Diamond, Jared. Guns, Germs, and Steel: The Fates of Human Societies. New York: Norton, 1997.
Diderot, Denis. “Encyclopédie.” In Encyclopédie, ou dictionnaire raisonné des sciences, des arts et des
métiers. Paris: Briasson, David, Le Breton, Durand, 1751–65.

Diogenes Laertius. Lives of the Eminent Philosophers. Translated by R. D. Hicks. Cambridge, Mass.:
Loeb Classical Library, 1925.
Douglas, Stephen. The Lincoln-Douglas Debates. Edited by Rodney Davis and Douglas Wilson.
Chicago: University of Illinois Press, 2008.
Douglass, Frederick. “Time Pictures.” Quoted in David W. Bright, ed., Narrative of the Life of
Frederick Douglass: An American Slave, Written by Himself. New York: St. Martin’s Press, Bedford
Books, 1993.
Engels, Friedrich. Socialism: Utopian and Scientific. New York: International Publishers, 1969.
Epictetus. Discourses. Translated by W. A. Oldfather. Cambridge, Mass.: Loeb Classical Library, 1925.
Finley, M. I. The Ancient Greeks. New York: Penguin, 1963.
Fischer, David Hackett. Historians’ Fallacies. New York: Harper and Row, 1970.
Foucault, Michel. Power/Knowledge: Selected Interviews and Other Writings 1972–1977. Edited by
Colin Gordon. New York: Pantheon Books, 1980.
Fowler, H. W. A Dictionary of Modern English Usage. 1st ed., repr. Introduction and notes by David
Crystal. Oxford: Oxford University Press, 2009.
Frege, Gottlob. Begriffsschrift, eine der arithmetischen nachgebildete Formelsprache des reinen
Denkens. Quoted in Benson Mates, Elementary Logic. New York: Oxford University Press, 1965.
———. Philosophical and Mathematical Correspondence. Chicago: University of Chicago Press,
1980.
Galileo. Dialogue Concerning the Two Chief World Systems. Translated by Stillman Drake. Berkeley:
University of California Press, 1962.
———. Dialogues Concerning Two New Sciences. Translated by Henry Crew and Alfonso De Salvio.
New York: Cosimo Classics, 2010.
Gergonne, J. D. “Essai de dialectique rationnelle.” Annales de mathématiques pures et appliquées 7
(1816–17): 189–228.
Gilson, étienne. Heloise and Abelard. Ann Arbor: University of Michigan Press, 1960.
Gödel, Kurt. On Formally Undecidable Propositions of Principia Mathematica and Related Systems.
Translated by B. Meltzer. 1931. Reprint, New York: Dover, 1992.
Goodman, Nelson. Fact, Fiction, and Forecast. Cambridge, Mass.: Harvard University Press, 1983.
Graham, A. C. Later Mohist Logic, Ethics, and Science. Hong Kong: Chinese University Press, 1978.
Greenwald, Robert. Outfoxed: Rupert Murdoch’s War on Journalism. DVD. Directed by Robert
Greenwald. Culver City, Calif.: Bravenew Films, 2004.
Haack, Susan. Deviant Logic, Fuzzy Logic: Beyond the Formalism. Chicago: University of Chicago
Press, 1996.
———. Philosophy of Logics. Cambridge: Cambridge University Press, 1978.
Hardy, G. H. A Mathematician’s Apology. 1940. Reprint, New York: Cambridge University Press,
2005.
Harsanyi, John C. “Mathematics, the Empirical Facts, and Logical Necessity.” Erkenntnis 19 (1983):
167–92.
Hitler, Adolf. Mein Kampf. Translated by Ralph Manheim. Boston: Houghton Mifflin, 1971.
Hobbes, Thomas. Leviathan. New York: Penguin Classics, 1982.
Hume, David. Dialogues Concerning Natural Religion. Cambridge, Mass.: Hackett, 1998.

———. An Enquiry Concerning Human Understanding. Cambridge, Mass.: Hackett, 1993.
———. An Enquiry Concerning the Principles of Morals. Cambridge, Mass.: Hackett, 1983.
———. A Treatise of Human Nature. New York: Oxford University Press, 2011.
Kagan, Donald. The Peloponnesian War. New York: Penguin, 2003.
Kant, Immanuel. The Critique of Pure Reason. Translated by Norman Kemp Smith. New York: St.
Martin’s Press, 1965.
————. A Prolegomena to Any Future Metaphysics. Translated by Günter Zöller. Indianapolis:
Bobbs-Merrill, 1950.
Keynes, John Maynard. A Treatise on Probability. Seaside, Ore.: Watchmaker, 2007.
Kitto, H. D. F. The Greeks. 1952. Reprint, London: Penguin, 1991.
Kneale, William, and Martha Kneale. The Development of Logic. London: Oxford University Press,
1962.
Kuhn, Thomas. “Reflections on My Critics.” In Criticism and the Growth of Knowledge, edited by
Imre Lakatos and Alan Musgrave, 231–78. New York: Cambridge University Press, 1974.
———. The Structure of Scientific Revolutions. Chicago: University of Chicago Press, 1996.
Leibniz, Gottfried. “The Art of Discovery” (1685). In Leibniz: Selections, edited by Philip P. Wiener.
New York: Scribner’s, 1979.
Lewis, C. S. The Discarded Image: An Introduction to Medieval and Renaissance Literature.
Cambridge: Cambridge University Press, 1964.
Locke, John. An Essay Concerning Human Understanding. London: Oxford University Press, 1979.
———. Two Treatises of Government and A Letter Concerning Toleration. New Haven, Conn.: Yale
University Press, 2003.
Lovelace, Ada. Notes to her translation of “Sketch of the Analytical Engine Invented by Charles
Babbage,” by Luigi Menabrea. In Scientific Memoirs: Selected from the Transactions of Foreign
Academies of Science and Learned Societies, edited by Richard Taylor (1843): 666–731.
Luther, Martin. “The Freedom of a Christian.” In The Protestant Reformation, rev. ed., edited by Hans
J. Hillerbrand, 31–58. New York: Harper Perennial, 2009.
Machiavelli, Niccolo. The Prince and the Discourses. Translated by Luigi Ricci. New York: Modern
Library, 1950.
Marx, Karl. Preface to a Contribution to the Critique of Political Economy. In The Marx-Engels
Reader, 2nd ed., 3–6. New York: Norton, 1978.
Matilal, Bimal Krishna. The Character of Logic in India. Albany: State University of New York Press,
1998.
———. Epistemology, Logic, and Grammar in Indian Philosophical Analysis. New York: Oxford
University Press, 2005.
———. Logic, Language, and Reality in Indian Philosophy. Delhi: Motilal Banarsidass, 1990.
Menzel, Christopher. “Logical Form.” In The Routledge Encyclopedia of Philosophy, ed. E. Craig.
http://www.rep.routledge.com/article/X021. Accessed March 29, 2013.
Mercier, Hugo, and Dan Sperber. “Why Do Humans Reason? Arguments for an Argumentative
Theory.” Behavioral and Brain Science 34, no. 2 (2011): 57–111.
Mill, John Stuart. The Autobiography of John Stuart Mill. New York: Columbia University Press, 1960.
———. On Liberty. Edited by Gertrude Himmelfarb. London: Penguin, 1985.

———. A System of Logic, Ratiocinative, and Inductive. Cambridge: Cambridge University Press,
2011.
Milton, John. Areopagitica and Of Education. Wheeling, Ill.: Harlan Davidson, 2011.
Moore, George Edward. Philosophical Papers. New York: Collier Books, 1959.
———. Principia Ethica. Cambridge: Cambridge University Press, 1970.
More, Thomas. Utopia. Translated by Robert M. Adams. New York: Norton, 2010.
Nidditch, P. H. The Development of Mathematical Logic. New York: Free Press, 1962.
Orwell, George. A Collection of Essays. New York: Harcourt Brace Jovanovich, 1970.
Phoca, Sophia, and Rebecca Wright. Introducing Postfeminism. New York: Totem Books, 1999.
Plato. Apology. In The Last Days of Socrates. Translated by Hugh Tredennick and Harold Tarrant. New
York: Penguin, 1995.
———. The Dialogues of Plato. Translated by Benjamin Jowett. New York: Random House, 1937.
———. The Republic. Translated by Desmond Lee. New York: Penguin Classics, 2007.
Priest, Graham. Introduction to Non-Classical Logic. 2nd ed. New York: Cambridge University Press,
2008.
Prior, Arthur. “The Runabout Inference Ticket.” Analysis 21 (1960–61): 38–39.
Quine, Willard Van Orman. From a Logical Point of View. 2nd rev. ed. Cambridge, Mass.: Harvard
University Press, 1980.
———. Philosophy of Logic. 2nd ed. Cambridge, Mass.: Harvard University Press, 1986.
———. The Ways of Paradox and Other Essays. Rev. ed. Cambridge, Mass.: Harvard University Press,
1976.
Rawls, John. Theory of Justice. Cambridge, Mass.: Harvard University Press/Belknap Press, 1971.
Read, Stephen. Thinking about Logic: An Introduction to the Philosophy of Logic. New York: Oxford
University Press, 1995.
Rice, Eugene F., Jr., and Anthony Grafton. The Foundations of Early Modern Europe. 2nd ed. New
York: Norton, 1994.
Ross, David. Aristotle. London: Methuen, 1949.
Rovere, Richard. Senator Joe McCarthy. New York: Harcourt, Brace and World, 1959.
Russell, Bertrand. Our Knowledge of the External World. 1914. Reprint, New York: Routledge, 2002.
———. The Problems of Philosophy. New York: Dover, 2005.
———. Wisdom of the West. New York: Doubleday, 1959.
Russell, Bertrand, and Alfred North Whitehead. Principia Mathematica. Cambridge: Cambridge
University Press, 1910–13.
Sartre, Jean-Paul. Being and Nothingness. Translated by Hazel E. Barnes. New York: Washington
Square Press, 1993.
———. Existentialism Is a Humanism. Translated by Carol Macomber. New Haven, Conn.: Yale
University Press, 2007.
Searle, John. “Minds, Brains and Programs.” Behavioral and Brain Sciences 3 (1980): 417–57.
Shenefelt, Michael. “Descartes: Philosophy as the Search for Reasonableness.” In The Many Faces of
Wisdom, edited by Phil Washburn, 125–40. Upper Saddle River, N.J.: Prentice Hall, 2003.
Sieg, Wilfried. “Formalization.” In The Cambridge Dictionary of Philosophy, edited by Robert Audi.
Cambridge: Cambridge University Press, 1999.

Stanford Encyclopedia of Philosophy, The. http://plato.stanford.edu. Accessed March 29, 2013.
Thucydides. The History of the Peloponnesian War. Translated by Rex Warner. New York: Penguin,
1972.
Turing, Alan. “Computing Machinery and Intelligence.” Mind 59 (1950): 433–60.
Vidyabhusana, S. C. A History of Indian Logic. 1920. Reprint, Delhi: Motilal Banarsidass, 1988.
Waley, Arthur. The Way and Its Power: A Study of the Tao Tê Ching and Its Place in Chinese Thought.
New York: Grove Weidenfeld, 1958.
Weber, Eugen. The Western Tradition. DVD. The Annenberg Foundation. Produced by WGBH Boston,
1989.
Weerakkody, D. P. M. “Demography.” In The Encyclopedia of Ancient Greece, edited by Nigel Wilson,
213–15. New York: Routledge, 2006.
Weinberg, Steven. “The Sokal Hoax: An Exchange.” The New York Review of Books 43 (October 3,
1996): 15, 56.
Whitehead, A. N. An Introduction to Mathematics. New York: Oxford University Press, 1911.
Wittgenstein, Ludwig. Philosophical Investigations. 1953. 4th ed. Oxford: Wiley-Blackwell, 2009.
———. Tractatus Logico-Philosophicus. New York: Dover, 1998.

INDEX
Page numbers refer to the print edition but are hyperlinked to the appropriate location in the e-book.
Abelard, Peter, 234, 235–47
ad hominem argument, 195–98, 251–52
ad populum, 199–200
affirming the consequent, 262–63
Alexander the Great, 32, 75, 102
ambiguity, 105, 145, 201–2, 259–60; and Wittgenstein, 66–69. See also equivocation
analytic philosophy, 66, 173
Aquinas, Thomas, 164, 171, 241
Arab logic, 170
Aristotle: as antidemocratic, 47; on contradiction, law of, 294n22; on epistēmē, 46, 282n5; on ethics,
99, 113; hostility toward faulty doctrines of truth, 102; on induction, 169, 307n13; influence on
medieval thought, 169, 171; interest in classification, 49, 51–53, 282n3; life of, 33–35, 70–71;
modal logic of, 397n15; on premises of demonstration, 130–31, 134–35, 152, 244, 253; on priority
of the state, 271; singularity of, 15, 38–39, 42; on sophistry, 185–86; validity, treatment of, 1–2, 35,
38–40, 46, 281n15. See also categorical propositions; categorical syllogisms
arithmetic: analogy with logic, 1, 8, 15, 119, 120, 274n5; axioms of, 219–20, 248; definition of
numbers, 107, 142; Frege on, 139–43, 227–29; Kant on, 73. See also mathematics
Arnauld, Antoine, 161
artificial intelligence, 222, 231–33
Athens: Athenian Assembly, 26, 28–31, 40–43, 46, 47–48, 49, 170, 185; empire, 30; al-Farabi and
Aquinas on, 164; logic in, 16, 34; and Stoics, 80. See also democracy
Averroës (Ibn Rushd), 22, 24, 170, 242–43
Avicenna (Abu ibn Sina), 22, 24, 170
Ayer, A. J., 284n13
Babbage, Charles, 220–22, 230
Barbara syllogism, 110, 283
begging the question, 252–54, 317
Bentham, Jeremy, 17; Book of Fallacies, 191–92; Enlightenment and, 189–91; on sophistry, 186, 193,
194, 197–98, 202–3, 272
Bernard of Clairvaux, 241–45
Bessemer, Henry, 209
Bible, 126–28, 131; translation of, 128, 161–62
big lie, 254
binary logic, 112
bivalence, principle of, 290n19, 291n23. See also excluded middle, law of
Boole, George: algebra, 205, 211–12, 213–14, 220, 223; and binary code, 96; life of, 211;
Mathematical Analysis of Logic, 211, 214

British Parliament, 186–89, 191–92, 208
Bruno, Giordano, 157
Cantor, George, 223
Carroll, Lewis (Charles Dodgson), 50, 52, 67
categorical propositions, 51, 54, 60
categorical syllogisms, 53, 58, 96, 169, 218, 282n6; invalid, 264; in symbolic logic, 212
cause and effect, confusions of, 255–56
Cave, Edward, 187
China: Chinese logic, 15, 39, 42, 111; early modern period, 23; navigation, 25–26, 39, 42, 163;
political theory of, 163, 307n12; Warring States Period, 20, 22
Chinese room argument, 315n24
Chrysippus, 80, 84, 86, 91, 97; and Boole’s notational system, 96, 211
circular definition, 256
circular explanation, 59, 257
circular reasoning, 130–31, 134, 137, 143; Descartes and, 124, 132; Nelson Goodman on, 146;
Thomas Kuhn on, 147–55
city-states, 16, 21, 22, 26, 30–31, 34, 75–77
classification, 49, 51, 53, 59. See also categorical propositions; categorical syllogisms
coherentism, 152
colonialism. See Europe
commerce. See trade
completeness, 214
composition, 257–59
compound propositions: computers and, 94–96; as premises, 134–35, 149, 155; types of, 83–86. See
also disjunction; material implication
computers, 16, 17, 48; Analytical Engine, 220–21; artificial intelligence, 222, 231–33; invention of,
229–31; many-valued logic and, 112; propositional logic and, 80, 93, 94–96; symbolic logic and,
205–6, 229
conditionals, 84, 86, 117, 135–36
Confucius, 22, 24, 111
conjunctive syllogism, 82, 90
continuum, denying differences in, 258
contradiction, law of, 70, 88, 108–11, 114, 122
contradictories, 54–56, 63, 89, 114
contraries, 54–55, 63, 114
Corax, 43
cotton, 207, 208
Cratylus, 101
Cynics, 101
Dedekind, Richard, 223
Delphic Oracle, 44
democracy: Athenian, 16, 26, 30, 46–48, 164; modern, 27, 47–48, 185, 200

De Morgan, Augustus, 90, 205, 210–15, 220, 222, 223
demos, 26–27, 30. See also Athens
denying the antecedent, 263
Derrida, Jacques, 200–202
Descartes, René: on circular reasoning, 124, 132; on equality of intellect, 162; on faith, 247; on
foundations, 16, 124–25, 129–33, 138, 144, 148; on hyperbolic doubt, 132–33; life of, 125, 155–56;
vernacular writing of, 161
deviant logic, 116, 121
dialectic, 287n8
Diamond, Jared, 25–26
Diderot, Denis, 191
dilemma, 82, 91–93, 218. See also false dichotomy
Diogenes Laertius, 97
disjunction, 83–86, 89–90, 94–96, 135–37, 149–55, 225
disjunctive syllogism, 83, 86–87, 90, 118, 229; invalid, 263
divine-command theory, 9–11
division, 258–59
Dodgson, Charles. See Carroll, Lewis
Eastern logic, 110–11
economic development. See trade
Egypt, 20–21, 28, 76, 163
emotional appeal, 259
empiricism, 66, 182; empirical science, assumptions of, 180–83; empirical tests, 119, 158, 168
Engels, Friedrich, 24, 102–4
Enlightenment, 109–10, 158, 186, 189–91
Epictetus, 13
Epicureans, 112
epistemology, 170, 244–45, 247. See also foundations: deductive vs. epistemological
equivocation, 259–60
Eubulides. See liar’s paradox
Euclid, 49, 219. See also geometry
Euler, Leonhard, 60, 63–64
Europe: early modern, 16, 22–24; economic development of, 125–26, 158–60, 164, 167; European
colonialism, 22, 159, 162, 188; geography of, 25–26. See also Enlightenment; Middle Ages; wars of
religion
evils and remedies (confusing them), 260
excluded middle, law of, 70, 88–89, 92–93, 94, 100, 111–16 passim, 119, 290n19
faith, 126, 133, 172, 235, 239–47
fallacies, 39, 191–202, 251–72
false analogy, 260–61
false antithesis, 261

false authority, 261–62
false dichotomy, 261, 262, 269
false dilemma. See false dichotomy
false extrapolation, 265
false interpolation, 265
fanaticism, 66, 125, 133, 148, 173, 245
Farabi, al-, 22, 24, 164, 170
first-order predicate calculus, 228, 229
formal fallacy, 262–64
formalization, 138–43, 228–29
formation rules, 140, 227–28, 230
Foucault, Michel, 104–6
foundations: deductive vs. epistemological, 144–46, 149; Descartes on, 16, 124–25, 129–33, 138, 144,
148, 155–56; foundationalism, 303n32; of induction, 174–80, 182; of mathematics, 137–38, 141–43,
224, 227; of rational belief, 235, 244–47, 248
Fowler, H. W., 268, 317n1
freedom of expression, 127, 164–67
Frege, Gottlob, 65, 117, 220, 223, 230; on foundations, 138–45, 227–29; and quantification, 215,
224–27, 314n15
fuzzy logic, 112–13, 116. See also many-valued logic
generalizing (errors of), 264–65
genetic fallacy, 198
geography, 16, 19–21, 23, 24–27, 41, 46
geometry, 49, 53–54, 60, 73, 219–20; non-Euclidean geometry, 139, 147
Gergonne, J. D., 60, 63, 285n16
Ghazali, al-, 242–43, 317n11
Gilson, étienne, 240, 316n6
Gödel, Kurt, 143, 228, 300n26
Goodman, Nelson, 146, 300n27
Greece. See Athens; city-states; geography
Guns, Germs, and Steel: The Fates of Human Societies (Diamond), 25–26
Haack, Susan, 280n13
Hardy, G. H., 276n13
Harsanyi, John C., 274n5
Hellenistic art, 77
Heloise, 234, 236–40, 241–42
Herodotus, 100
Hitler, Adolf, 194, 254–55
Hobbes, Thomas, 44, 66, 161, 164, 244, 246, 266
Hume, David, 44, 66, 244; on cause and effect, 181–82; induction and, 174, 176, 179, 180; on
skepticism, 121, 133

hypothetical propositions. See conditionals
hypothetical syllogism, 81, 91–93, 169
ignoratio elenchi. See irrelevant conclusion
Indian logic, 15, 35–38, 39, 40, 41–42, 169. See also Matilal, Bimal Krishna
indicators, 12, 145
individualism, 127, 158
induction, 16, 38–39, 125, 158, 166–69, 172–74, 183–84; foundations of, 174–80, 182
Industrial Revolution, 17, 140, 205–10, 215, 222, 233–34
inference rules, 140, 144, 231, 280n13
innuendo, 265–66
intelligentia, 248
intuition: 109, 121, 222; logical, 9, 274n5, 301n27
intuitionist logic, 116
irrelevant conclusion, 266
is and ought (confusing them), 266–67
Johnson, Samuel, 120, 187, 272
Kant, Immanuel, 73, 181–82, 244, 246, 270
Kepler, Johannes, 157, 168, 180, 189, 190
Keynes, John Maynard, 66, 309n24
Kitto, H. D. F., 275n13, 276n1
Kuhn, Thomas, 124, 137, 146–55
language, 1, 105, 107; artificial symbolic language, 40, 116–17, 139–40, 206, 210, 213–14, 230;
language-game, 58–59, 200; ordinary language, 109, 112, 269; Wittgenstein on, 65–69, 284n13. See
also equivocation; neologism; vague metaphor; vernacular literature
Leibniz, Gottfried, 110, 210, 214, 221
Lewis, C. S., 171–72
liar’s paradox, 141
loaded question, 262, 267
Locke, John, 129, 161, 162, 164, 165, 173–74, 244, 318n4
logicism, 227, 300n26
Lovelace, Ada, 220–21, 315n21
Łukasiewicz, Jan, 112, 114, 115
Luther, Martin, 125–28, 161
Madison, James, 31, 48
many-valued logic, 112–16
Marx, Karl, 103, 271
material implication, 84, 288n16, 296n33
mathematics: of ancient Greeks, 28, 79; formalization of, 8–9, 138–46; nature of, 60, 284n13; Plato on,
46; and symbolic logic’s development, 206, 213, 219–24, 227–29. See also arithmetic

Matilal, Bimal Krishna, 279n7–8, 280n9, 280n11
McCarthy, Joseph, 195, 198, 265–66, 267
mechanical procedures, 17, 139–40, 205–6, 210–16, 218–24 passim, 227, 230–33
Mediterranean Sea: civilizations of, 20, 74; ease of navigation, 21, 25–26, 27; superseded by Atlantic
sea trade, 23, 159
Middle Ages: Aristotle’s influence on, 169, 171; literature of, 158–59, 171–72; logic of, 58, 172–73,
211, 235, 240–43, 246; Northern Europe during, 22; political theory of, 164; witchcraft during, 128
middle classes. See trade
military technology, 27, 29, 75
Mill, John Stuart: on eccentricity, 192; on induction, 165–67, 173, 174, 176, 179, 182; on mechanical
procedures, 214, 221; warning against analysis, 108
Milton, John, 165
modal logic, 170
modus ponens, 5; complicated form of, 135–36; perversion of, 262; in propositional logic, 80–81,
86–87, 91–93, 94–95; in symbolic logic, 229; universality of, 110; validity of, 107, 118, 120
modus tollens, 81, 90, 92–93, 227, 263
money economy. See trade
Moore, George Edward, 120, 254
More, Thomas, 127, 160, 164
multiple untruth, 194–95
nature, law of: Locke on, 162; Stoics on, 16, 74, 78, 79–80, 97
navigation, 22–23, 25–26. See also trade
negative proof, 267–68
neologism, 200–202
Newton, Isaac, 9, 110, 157, 189–90
Nicole, Pierre, 161
Niebuhr, Reinhold, 75
nomos (custom), 44, 99
nonclassical symbolic logics, 116, 118, 229
non sequitur, 3, 104
Orwell, George, 201, 271, 286n22
paraconsistent logics, 116, 293n19
paradigm, 37, 146–47, 148, 268, 269, 270, 302n28
paradox: ancient Greek interest in, 100; bald man, 102–3; effect on formalization, 141–44; liar’s
paradox, 141; Protagoras and Euathlus, 141–42; Russell’s paradox, 142–44; ship of Theseus, 103
Parliamentary reform movement, 186–89
particulars, 56–58
Peacock, George, 219, 223
Peano, Giuseppe, 213, 219, 223
Peirce, C. S.: on excluded middle, 112; on “paper doubts,” 121; on philosophy of science, 173;
symbolic logic of, 212, 223, 290n20, 312n3

Peloponnesian War, 43
Pericles, 30–31, 45
Philip of Macedon, 32–34; 75–76
Philo the Dialectician, 289n17
Plato: and the Academy, 33, 46; as antidemocratic, 47; contrasted with Aristotle, 70; and dialectic, 79;
on Diogenes of Sinope, 102; on divine-command theory, 10–11; on epistēmē, 46; on fallacious
arguments, 185; on geometry, 49; on moral knowledge, 246; on Protagoras, 101; on public opinion,
44; on the Sophists, 42–45; on students of argument, 243
political theory, 163–64
Port Royal Logic (Arnauld and Nicole), 161
post hoc, ergo propter hoc, 255
predication, 51. See also classification
pretentious diction, 194, 201, 268–69
Priest, Graham, 296n33
principle of double negation, 87–89
principle of explosion, 293n19
principle of induction, 174–79
printing press, 126–27, 160–61, 210
Prior, Arthur, 297n35
proof: of consistency, 229; early Greek interest in, 28, 49; of external world, 120; and induction, 167;
in mathematics, 140, 146, 166, 219, 220, 228; subverting the order of, 253; in symbolic logic, 206,
213, 218, 221, 222; of whether logic is universal, 13. See also negative proof
propositional logic, 16, 73, 80–94, 95, 124
Protagoras, 44, 101, 141
public opinion, 17, 44, 47, 185, 191
quantification in symbolic logic, 224–27
question-begging epithets, 253–54
Quine, Willard Van Orman, 273n3, 281n15, 292n24, 303n34
Radice, Betty, 237
Ramus, Peter, 129
rationality: after the world wars, 66; of alternative scientific theories, 147; contrasted with persuasion,
45;
rationality (continued) Enlightenment tradition of, 109; and induction, 39, 167–68, 173, 183; and
religion, 245–48
red herring, 198
reductio ad absurdum, 79
reflective equilibrium, 152
relations, logic of, 216–17, 224, 312n3
relativism, 99, 116–21
relevance logic, 116, 117
religion, 9–11, 43, 235–47. See also wars of religion
rhetoric: later history of, 169, 170; origins of, 27, 29, 42–44; separation from logic, 45

Roman Inquisition, 157
romanticism, 69, 108–10, 234
Rome, 26, 163, 170
Rovere, Richard, 193, 194–95, 199
Roy, Raja Rammohan, 190
Russell, Bertrand, 65, 66, 117, 145; on induction, 174, 176, 179; symbolic logic of, 213, 227, 229. See
also Russell’s paradox
Russell’s paradox, 142–44
Sartre, Jean-Paul, 271
science, logic of, 157–58, 173, 180–82; scientific theories in competition, 150–55
Scientific Revolution, 158
seafaring. See navigation; trade
Searle, John, 315n24
sham distinction, 188
sham insight, 269
skepticism, 121, 302n27
Skeptics, ancient, 101
Smith, Adam, 25–26, 47
Socrates, 20, 43, 44–45, 100
sophistries, 185–86, 192–202, 251–72
Sophists, 42–45
square of opposition, 54–58
Stoics and Stoicism, 16, 73–78
straw man, 193–94, 269–70
subcontraries, 54–55, 63
superfluous displays of learning, 198–99
suppressed evidence, 270–71
syllogism. See categorical syllogisms; conjunctive syllogism; disjunctive syllogism; hypothetical
syllogism
symbolic logic, 17, 205–34
syncategorematic words, 211
Tao Tê Ching, 108–10
Tarski, Alfred, 293n17
terms, 95
Theophrastus, 71, 169
theory of types, 300n25
Theseus, ship of, 103
Thrasymachus, 104
Tisias, 43
trade: Adam Smith on, 25; and creation of Athenian demos, 26–27; and creation of European middle
classes, 158–60; its effect on intellectual history, 16, 21–24, 158; its effect on political theory,

163–64; as encouragement to relativism, 99–100; and freedom of dissent, 164–67; and
industrialization, 207–8; and Parliamentary reform, 186–88; and wars of religion, 125, 128
truth, theories of, 99–107
tu quoque, 252
Turing, Alan, 230–31, 233
universals, 56–58, 235
utilitarianism, 189
vague generality, 194
vague metaphor, 202, 271–72
vagueness, 104, 112–13, 258
validity: Aristotle’s treatment of, 1–2, 35, 38–40, 46, 281n15; definition of, 3; explanations of, 6–11;
extra-systematic, contrasted with semantic and syntactic, 280n13; as relative, 116–21; strange nature
of, 2–6
vernacular literature, 128, 158, 160–62
Vienna Circle, 173
wars of religion: fanaticism of, 16, 123–24, 155; intellectual effect of, 162, 173, 183, 246–47; origins
of, 125–29
Watt, James, 208–9
Wealth of Nations. See Smith, Adam
Weinberg, Steven, 200–202
Whitehead, Alfred North, 117, 213, 227, 229, 232
Whitman, Walt, 108
witchcraft, 123–24, 128–29, 157, 190, 245–46
Wittgenstein, Ludwig, 58, 65–70, 230
women, 29, 110, 116, 128–29, 270
wrapping oneself in the flag, 272
Zadeh, Lotfi, 112, 116
Zeno of Citium, 80, 97
Zheng He, 23

