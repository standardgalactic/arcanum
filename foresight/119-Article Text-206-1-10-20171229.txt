 
www.ijecs.in 
International Journal Of Engineering And Computer Science ISSN:2319-7242 
Volume 3 Issue 2 February, 2014 Page No. 3962-3964 
 
 
Ekta Parwani1 , IJECS Volume 3. Issue 2 February, 2014 Page No.3962-3964                                                                        Page 3962 
Virtual Touch Screen Using Microsoft Kinect 
 
 
Ekta Parwani1, Apeksha Pawar2, Chirag Ajwani3, Prof. Govind Pole4 
Dept of Computer Engineering 
Modern Education Society’s College of Engineering Pune-411001. 
 parwaniekta@gmail.com1, apeksha_543@yahoo.co.in2,chiragajwani@gmail.com3,govindpole@gmail.com4 
 
Abstract: Virtual Touch Screen using Microsoft Kinect is a depth-sensing and projection system that enables interactive touch 
applications on everyday planar surfaces. It provides capabilities similar to that of a mouse or touchscreen. Touch screen surfaces are 
becoming prevalent, since they are capable of detecting the user’s actual touch points that is the points of contact on the surface. Our 
project presents a practical solution to achieve touch on any planar surface based on mounting a single commodity depth camera that is a 
Microsoft Kinect above a horizontal surface and using a projector for projection purpose. This will enable us to detect when the surface 
is being touched. Using Microsoft Kinect to detect touch has significant advantages: firstly, the interactive surface should not be 
instrumented. Secondly it is more accurate and cost effective. 
 
Keywords: Microsoft Kinect, depth cameras, interactive surface. 
1. Introduction 
In today’s modern society it is difficult to imagine a world 
without the touch screen. It can be found in cell phones, 
computers, tablets and there is also research in flexible and 
curved touch screens. At first the input was done with help of a 
stylus, the screens were black and white, but now we have 
touch screens that spread over several meters. Touch screen has 
become popular because it is easy to use, allows the user to 
understand or know something without any reasoning process. 
The last decade cell phones had no buttons thus to type a text 
message a person had to click the same button several times to 
get the right letter but with the introduction of touchscreens it is 
possible to get all the alphabets on different buttons by using a 
qwerty keyboard. Touch screen has helped in reducing the 
users work thereby creating an innovation for developing 
applications (apps). Now days, there are apps that open on a 
single click of a button i.e. mails and news. However touch 
screens have many limitations. It can break into pieces when 
dropped to the ground also they cause may scratches on the 
screen. The main question that arises in our minds is what will 
the future bring? One of the possibilities are smart surfaces. 
Smart surfaces are a combination of a screen and a surface. 
This means that all the surfaces in the environment can behave 
like touch screens. 
With the span of time and technology, their portability, cost, 
durability and robustness is in question. 
 
2. RELATED WORK AND MOTIVATION 
The project is concerned with touch interaction, natural user 
interfaces. Below we review relevant work in these areas 
 
 
2.1 Vision Based 
Vision Based technique is used to distinguish between the 
fingers of both the hands which have been touched. This canbe 
achieved by using two cameras mounted above the surface.The 
fingers of each hand are detected by using the constraints on 
the touch position with finger orientation. The hand image is 
captured on a surface to distinguish between hand postures. 
The only input device required is a camera and it provides a 
better interface for mouse interaction system.This method is 
robust and efficient.The drawback of this technique is that it 
requires a black background for recognition and also additional 
hardware is required.[3],[4],[5]. 
 
2.2 Pen and Touch-based Interaction 
The combination of pen and surface is used in this method. The 
pen is used to give input i.e. to make contact with the surface. 
This inreases the hardware that is required[2]. 
 
2.3 Specialized Hardware 
Tap sense is an acoustic sensing system that recognizes the 
type of input such as nail,pad,fingertip. This requires an 
additional database to store different types of taps. It also 
requires a lot of time for set up and the background noise can 
cause problems while detecting the input[1]. 
 
3. PROBLEM DEFINITION 
 
Our project is an attempt to create a virtual touch screen using 
Microsoft Kinect. Usually the hardware required for detecting 
the touch on any surface is either very costly or requires a lot of 
space. In our project we use a single hardware device and any 
planar surface to create a virtual touch screen. A problem 
commonly observed is the high False detection of touch points 
i.e. even when the surface is not touched the sensor detects 
touch points or else the surface is touched and still no touch 
points are detected. 
 
4. PHYSICAL SETUP 
 
The setup (Figure 1) consists of (1) a horizontal surface, for 
interactions, (2) a projector, which projects on to the surface 
(3) a depth sensor (Microsoft Kinect) for capturing input, (4) a 

Ekta Parwani1 , IJECS Volume 3. Issue 2 February, 2014 Page No.3962-3964    
Page 3963 
 
computer to process the input data from the Kinect and 
projecting visual output using the projector. 
 
 
Figure 1. The physical setup 
 
5. ALGORITHM SELECTION 
 
5.1. Calibration 
One-to-one mapping is present in traditional multi touch 
surfaces between the physical location that is being touched 
and the digital location on the display. This provides users with 
the feeling of directly manipulating the digital content 
underneath their fingers. To facilitate a similar interactive 
experience in the depth camera-based approach, map the depth 
camera scene to the projected display area as shown in 
figure(2). 
 
Figure 2. Calibration 
Find the nearest neighbor between points in the frames and 
associate touch points in consecutive depth frames. Since the 
Kinect can record at a maximum rate of thirty frames per 
second, not all depth frames are necessarily used in tracking 
touches. Our system intelligently discards outlier depth frames 
thereby increasing robustness 
 
5.2. Touch Sensing 
The touch is detected by analyzing the image produced by the 
depth camera. Every pixel’s intensity in the depth image 
corresponds to the distance from the camera. This allows us to 
define a “touch input region”. This touch input region is of a 
small size in close proximity to the interactive surface. When 
the finger comes in the proximity of touch input region the 
touch is detected and the appropriate action is performed. If the 
input is bigger than the finger for example when you place the 
entire hand on the surface it gets detected as a blob and is 
discarded[1]. 
 
Figure 3. Touch Input Region 
 
6. CONCLUSION 
 
From our research and tests we come up with the following 
conclusions. Firstly when working with the Microsoft Kinect 
the operating range of the Kinect needs to be kept in mind. If 
the Kinect is too far from the surface there is lack of precision. 
For this reason the Kinect is placed at a distance varying from 
0.8m to 4m. Secondly, the Kinect cannot be very close to the 
surface otherwise the skeleton model received will not be very 
accurate and precise. The result is that the coordinates of the 
joints will fluctuate and thus it will be very difficult to place 
any application on the surface. 
Another conclusion that can be drawn is that while calibrating 
it is possible to get accurate and precise transformations from 
camera coordinates to projection coordinates. 
 
7. FUTURE WORK 
 
The calibration can be changed to a calibration process which 
takes in account that hovering might be a possibility. Another 
change that would be a great improvement would be that the 
application adapts to the surface. If for example there is a 
round surface, that the application could appear as if it is glued 
to said surface. Another area that could improve is the 
hardware. At the time of writing, the release of the Microsoft 
Kinect 2 has been announced. If a better camera is used or if 
the camera were placed at a further distance from the surface, 
more precise interaction can be done. This would result in a 
more suitable environment for several users to work on the 
same virtual touch screen. Another option which promises a 
bright future is 3D. At the moment there are 3D televisions, 3D 
computer screens, even 3D movies in the movie theatres. A 
smart surface with3D would allow more interaction and 
intuitively. For example an architect is drawing a plan of a 
house on a virtual touch screen. It would probably greatly help 
the architect if he/she could see the building in 3D in or on the 
touch screen. 
References 

Ekta Parwani1 , IJECS Volume 3. Issue 2 February, 2014 Page No.3962-3964    
Page 3964 
 
[1] SUNDAR MURUGAPPANA, VINAYAKA, NIKLAS 
ELMQVISTB,KARTHIK RAMANIA :"Extended Multitouch: 
Recovering Touch Posture, Handedness, and User Identity 
using a Depth Camera". 
[2] 
HINCKLEY, 
K., 
YATANI, 
K., 
PAHUD, 
M., 
CODDINGTON, N., RODENHOUSE, J., WILSON, A., 
BENKO, H., AND BUXTON, B. Manual deskterity: "An 
exploration of simultaneous pen + touch direct input". In 
Extended Abstracts of the ACM Conference on Human 
Factors in Ccomputing Systems (2010), pp. 2793–2802. 
[3] MALIK, S., AND LASZLO, J. Visual touchpad: "A 
twohanded gestural input device". In Proceedings of the ACM 
International Conference on Multimodal Interfaces (2004), pp. 
289–296. 
[4] DANG, C. T., STRAUB, M., AND ANDR´E , E."Hand 
distinction for multi-touch tabletop interaction". In Proceedings 
of the ACM Conference on Interactive Tabletops and Surfaces 
(2009), pp. 101–108. 
[5] FREEMAN, D., BENKO, H., MORRIS, M. R., AND 
WIGDOR, 
D. 
ShadowGuides:"Visualizations 
for 
insitu 
learning of multi-touch and whole-hand gestures". In 
Proceedings of the ACM Conference on Interactive Tabletops 
and Surfaces (2009), pp. 165–172. 
[6] HARRISON, C, SCHWARZ, J, AND HUDSON, S. E. 
TapSense: "Enhancing finger interaction on touch surfaces". In 
Proceedings of the ACM Symposium on User Interface 
Software and Technology (2011), pp. 627–636. 
 
 

