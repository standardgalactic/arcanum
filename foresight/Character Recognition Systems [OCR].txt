
CHARACTER
RECOGNITION SYSTEMS
A Guide for Students and Practioners
MOHAMED CHERIET
´Ecole de Technologie Sup´erieure/University of Quebec, Montreal
NAWWAF KHARMA
Concordia University, Montreal
CHENG-LIN LIU
Institute of Automation/Chinese Academy of Sciences, Beijing
CHING Y. SUEN
Concordia University, Montreal


CHARACTER
RECOGNITION SYSTEMS


CHARACTER
RECOGNITION SYSTEMS
A Guide for Students and Practioners
MOHAMED CHERIET
´Ecole de Technologie Sup´erieure/University of Quebec, Montreal
NAWWAF KHARMA
Concordia University, Montreal
CHENG-LIN LIU
Institute of Automation/Chinese Academy of Sciences, Beijing
CHING Y. SUEN
Concordia University, Montreal

Copyright © 2007 by John Wiley & Sons, Inc. All rights reserved
Published by John Wiley & Sons, Inc., Hoboken, New Jersey
Published simultaneously in Canada
No part of this publication may be reproduced, stored in a retrieval system, or transmitted in any form or
by any means, electronic, mechanical, photocopying, recording, scanning, or otherwise, except as
permitted under Section 107 or 108 of the 1976 United States Copyright Act, without either the prior
written permission of the Publisher, or authorization through payment of the appropriate per-copy fee to
the Copyright Clearance Center, Inc., 222 Rosewood Drive, Danvers, MA 01923, (978) 750-8400, fax
(978) 750-4470, or on the web at www.copyright.com. Requests to the Publisher for permission should
be addressed to the Permissions Department, John Wiley & Sons, Inc., 111 River Street, Hoboken, NJ
07030, (201) 748-6011, fax (201) 748-6008, or online at http://www.wiley.com/go/permission.
Limit of Liability/Disclaimer of Warranty: While the publisher and author have used their best efforts in
preparing this book, they make no representations or warranties with respect to the accuracy or
completeness of the contents of this book and speciﬁcally disclaim any implied warranties of
merchantability or ﬁtness for a particular purpose. No warranty may be created or extended by sales
representatives or written sales materials. The advice and strategies contained herein may not be suitable
for your situation. You should consult with a professional where appropriate. Neither the publisher nor
author shall be liable for any loss of proﬁt or any other commercial damages, including but not limited to
special, incidental, consequential, or other damages.
For general information on our other products and services or for technical support, please contact our
Customer Care Department within the United States at (800) 762-2974, outside the United States at (317)
572-3993 or fax (317) 572-4002.
Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may
not be available in electronic formats. For more information about Wiley products, visit our web site at
http://www.wiley.com.
Wiley Bicentennial Logo: Richard J. Paciﬁco
Library of Congress Cataloging-in-Publication Data
Character recognition systems : a guide for students and practioners / Mohamed Cheriet ... [et al.].
p. cm.
Includes bibliographical references and index.
ISBN 978-0-471-41570-1 (cloth)
1.
Optical character recognition devices. I. Cheriet, M. (Mohamed)
TA1640.C47 2007
006.4’24–dc22
2007011314
Printed in the United States of America
10 9 8 7 6 5 4 3 2 1

To my parents,
my wife Farida,
my daugters Imane,
Islem and Ihcene
(Mohamed Cheriet)
To my parents Nayef and Fauzieh,
to the children of Palestine
(Nawwaf Kharma)
To my parents,
my wife Linlin,
my son Jiayi
(Cheng-Lin Liu)
To my parents,
my wife Ling,
my sons Karwa and Karnon
(Ching Y. Suen)


CONTENTS
Preface
xiii
Acknowledgments
xvii
List of Figures
xix
List of Tables
xxvii
Acronyms
xxix
1 Introduction: Character Recognition, Evolution,
and Development
1
1.1
Generation and Recognition of Characters
1
1.2
History of OCR
2
1.3
Development of New Techniques
3
1.4
Recent Trends and Movements
3
1.5
Organization of the Remaining Chapters
3
References
4
2 Tools for Image Preprocessing
5
2.1
Generic Form-Processing System
5
2.2
A Stroke Model for Complex Background Elimination
8
vii

viii
CONTENTS
2.2.1
Global Gray Level Thresholding
9
2.2.2
Local Gray Level Thresholding
11
2.2.3
Local Feature Thresholding—Stroke-Based Model
12
2.2.4
Choosing the Most Efﬁcient Character Extraction Method
15
2.2.5
Cleaning Up Form Items Using Stroke-Based Model
19
2.3
A Scale-Space Approach for Visual Data Extraction
21
2.3.1
Image Regularization
22
2.3.2
Data Extraction
24
2.3.3
Concluding Remarks
29
2.4
Data Preprocessing
30
2.4.1
Smoothing and Noise Removal
30
2.4.2
Skew Detection and Correction
32
2.4.3
Slant Correction
34
2.4.4
Character Normalization
36
2.4.5
Contour Tracing/Analysis
41
2.4.6
Thinning
45
2.5
Chapter Summary
50
References
51
3 Feature Extraction, Selection, and Creation
54
3.1
Feature Extraction
54
3.1.1
Moments
55
3.1.2
Histogram
58
3.1.3
Direction Features
59
3.1.4
Image Registration
64
3.1.5
Hough Transform
68
3.1.6
Line-Based Representation
70
3.1.7
Fourier Descriptors
73
3.1.8
Shape Approximation
76
3.1.9
Topological Features
78
3.1.10
Linear Transforms
79
3.1.11
Kernels
86
3.2
Feature Selection for Pattern Classiﬁcation
90
3.2.1
Review of Feature Selection Methods
90
3.3
Feature Creation for Pattern Classiﬁcation
104
3.3.1
Categories of Feature Creation
104
3.3.2
Review of Feature Creation Methods
105
3.3.3
Future Trends
118

CONTENTS
ix
3.4
Chapter Summary
120
References
120
4 Pattern Classiﬁcation Methods
129
4.1
Overview of Classiﬁcation Methods
129
4.2
Statistical Methods
131
4.2.1
Bayes Decision Theory
131
4.2.2
Parametric Methods
132
4.2.3
Nonparametric Methods
138
4.3
Artiﬁcial Neural Networks
142
4.3.1
Single-Layer Neural Network
144
4.3.2
Multilayer Perceptron
148
4.3.3
Radial Basis Function Network
152
4.3.4
Polynomial Network
155
4.3.5
Unsupervised Learning
156
4.3.6
Learning Vector Quantization
160
4.4
Support Vector Machines
162
4.4.1
Maximal Margin Classiﬁer
163
4.4.2
Soft Margin and Kernels
165
4.4.3
Implementation Issues
166
4.5
Structural Pattern Recognition
171
4.5.1
Attributed String Matching
172
4.5.2
Attributed Graph Matching
174
4.6
Combining Multiple Classiﬁers
179
4.6.1
Problem Formulation
180
4.6.2
Combining Discrete Outputs
181
4.6.3
Combining Continuous Outputs
183
4.6.4
Dynamic Classiﬁer Selection
190
4.6.5
Ensemble Generation
190
4.7
A Concrete Example
194
4.8
Chapter Summary
197
References
197
5 Word and String Recognition
204
5.1
Introduction
204
5.2
Character Segmentation
206
5.2.1
Overview of Dissection Techniques
207
5.2.2
Segmentation of Handwritten Digits
210

x
CONTENTS
5.3
Classiﬁcation-Based String Recognition
214
5.3.1
String Classiﬁcation Model
214
5.3.2
Classiﬁer Design for String Recognition
220
5.3.3
Search Strategies
227
5.3.4
Strategies for Large Vocabulary
234
5.4
HMM-Based Recognition
237
5.4.1
Introduction to HMMs
237
5.4.2
Theory and Implementation
238
5.4.3
Application of HMMs to Text Recognition
243
5.4.4
Implementation Issues
244
5.4.5
Techniques for Improving HMMs’ Performance
247
5.4.6
Summary to HMM-Based Recognition
250
5.5
Holistic Methods for Handwritten Word Recognition
250
5.5.1
Introduction to Holistic Methods
251
5.5.2
Overview of Holistic Methods
255
5.5.3
Summary to Holistic Methods
256
5.6
Chapter Summary
256
References
257
6 Case Studies
263
6.1
Automatically Generating Pattern Recognizers with
Evolutionary Computation
263
6.1.1
Motivation
264
6.1.2
Introduction
264
6.1.3
Hunters and Prey
266
6.1.4
Genetic Algorithm
271
6.1.5
Experiments
272
6.1.6
Analysis
280
6.1.7
Future Directions
281
6.2
Ofﬂine Handwritten Chinese Character
Recognition
282
6.2.1
Related Works
283
6.2.2
System Overview
285
6.2.3
Character Normalization
286
6.2.4
Direction Feature Extraction
289
6.2.5
Classiﬁcation Methods
293
6.2.6
Experiments
293
6.2.7
Concluding Remarks
301

CONTENTS
xi
6.3
Segmentation and Recognition of Handwritten
Dates on Canadian Bank Cheques
301
6.3.1
Introduction
302
6.3.2
System Architecture
303
6.3.3
Date Image Segmentation
303
6.3.4
Date Image Recognition
308
6.3.5
Experimental Results
315
6.3.6
Concluding Remarks
317
References
317
Index
321


PREFACE
In modern society, we rely heavily on computers to process huge volumes of data.
Related to this and for economic reasons or business requirements, there is a great
demand for quickly inputting the mountains of printed and handwritten information
into the computer. Often these data exist on paper and they have to be typed into
the computer by human operators, for example, billions of letters in the mail, checks,
payment slips, income tax forms, and many other business forms and documents. Such
time-consuming and error-prone processes have been lightened by the invention of
OCR (optical character recognition) systems that read written data by recognizing
them at high speeds, one character at a time. However, the capabilities of the current
OCR systems are still quite limited and only a small fraction of the above data are
entered into the computer by them. Hence, much effort is still needed to enable them
to read printed and handwritten characters more accurately. This book looks at some
of the problems involved and describes in depth, some possible solutions, and the
latest techniques achieved by researchers in the character recognition ﬁeld.
Although several OCR books have been published before,1 none of them are up-
to-date now. In view of this, we have gathered together to write this book. It is our
objective to provide the readers with the fundamental principles and state-of-the-art
computational methods of reading printed texts and handwritten materials. We have
also highlighted the key elements of character recognition2 with extensive illustrations
and references so that the readers can pursue further research and studies. It is hoped
that this book will help graduate students, professors, researchers, and practitioners
to master the theory and latest methodologies in character recognition, and to apply
them in their research and practice.
1See references [5] to [7] at the end of Chapter 1.
2See references [4] to [7] at the end of Chapter 1.
xiii

xiv
PREFACE
This book consists of six chapters, each preceded by an introduction and followed
by a comprehensive list of references for further reading and research. These chapters
are summarized below:
Chapter 1 introduces the subject of character recognition, some perspectives on
the history of OCR, its applications, evolution, and a synopsis of the other chapters.
Chapter 2 describes the most widely used preprocessing techniques. An intel-
ligent form processing system is used to illustrate systematically: (a) a scale-based
approach of enhancing the signal-to-noise ratio and (b) an efﬁcient method of ex-
tracting the items-of-interest in a digital text image. This chapter also introduces the
essential smoothing and noise removal techniques and key methods of normalizing
the digitized characters, including the detection and correction of slants and skews,
and the normalization of character sizes and positions. As well, methods of extracting
character contours and skeletons are presented with appropriate illustrations.
All characters and words possess their own salient features. Chapter 3 shows
various ways of extracting these features and evaluating them. Section 1 covers a
large number of features commonly used for character recognition. Both structural
(such as discrete features and geometric shapes) and statistical (such as moments and
Fourier descriptors) features are described. Section 2 examines the relation between
feature selection and classiﬁcation, and the criteria for evaluating different feature
subsets. Finally, a new concept of feature creation is introduced, and some pointers in
the search for novel features as well as avenues for discovering some unknown ones
are provided.
Chapter 4 presents some modern classiﬁcation methods that have proven to
be successful in character recognition. They include statistical methods, artiﬁcial
neural networks (ANN), support vector machines (SVM), structural methods, and
multi-classiﬁer methods. ANNs and SVMs make use of a feature vector of ﬁxed di-
mensionality mapped from the input patterns. Structural methods recognize patterns
via elastic matching of strings, graphs, or other structural descriptors. Multiclassiﬁer
methods integrate the decisions of several classiﬁers. This chapter describes, in depth,
the concepts and mathematical formulations of all these methods, and their relations
to Bayesian decision theory, and to parametric and nonparametric methods. It also
discusses their properties and learning procedures. Furthermore, this chapter provides
comprehensive guidelines in implementing practical classiﬁers and in applying them
to recognize characters. Examples of experimental designs, training procedures, and
testing of real-life databases are also provided.
In real-world applications, documents consist of words or character strings rather
than elementary units of isolated characters. However, word and character string
recognition processes involve character segmentation, character recognition, and con-
textual analyses. Chapter 5 gives an overview of word and string recognition methods,
and techniques related to segmentation and recognition. Both explicit and implicit
segmentation methods are introduced, as well as holistic approaches that make use of
more global features and word modeling. Because hidden Markov models (HMMs)
have frequently been used in the segmentation process, their theoretical background is
treated in a separate section. Implementation issues are also discussed in this chapter.

PREFACE
xv
To provide further details about the practical aspects, three case studies have been
included in Chapter 6. The ﬁrst one illustrates an automatic method of generating
pattern recognizers with evolutionary computation. The second one presents a
real-world Chinese character recognition system. The third shows an automatic
segmentation and recognition system for handwritten dates on Canadian bank
cheques. Each study includes a description of the method and theory behind with
substantial experimental results.
M. Cheriet, N. Kharma,
C.-L. Liu and C. Y. Suen
Montreal, Quebec
December 2006


ACKNOWLEDGMENTS
This textbook would have not been possible without the signiﬁcant contributions
from its authors as well as supporting contributors, who helped with the process of
writing this book. The authors wish to thank ﬁrst the following people for their deep
involvement in writing parts of the book, namely
• Mr. Javad Sadri, who wrote sections 2.4, 5.2.2 and 5.5. He also spent appre-
ciable time for proof reading and creating the index of this book. He con-
tributed in converting the draft of the book to LaTeX format, as well. He is
currently, a postdoctoral research fellow at the Center for Pattern Recognition
and Machine Intelligence (CENPARMI) in Computer Science and Software
Engineering Department of Concordia University, Montreal, Quebec, Canada.
E-mail: j sadri@cse.concordia.ca.
• Mr. Taras Kowaliw should be acknowledged for his contributions to the case
studies chapter. Ms. Jie YAO should be acknowledged for her contributions to
the feature selection section of the feature selection and creation chapter.
• Mr. Vincent Dor´e, Mr. Ayoub Alsarhan and Mrs. Safeya Mamish: doctoral stu-
dents under Prof. Cheriet supervision, should be acknowledged for their valuable
help in generating the index of this book.
• Dr. Xiangyun Ye, a former postdoctoral fellow at CENPARMI, who contributed
to Sections 2.1 and 2.2.
E-mail: xyye@ieee.org
• Dr. Mounim El-Yacoubi, a former postdoctoral fellow at CENPARMI, who
contributed Section 5.4. He is currently a senior researcher at Parascript, LLC,
Boulder, Colorado, USA.
E-mail: mounim.el-yacoubi@parascript.com
xvii

xviii
ACKNOWLEDGMENTS
• Dr. Louisa Lam, a member of CENPARMI and former research associate, who
contributed to Section 6.3, is currently an associate vice-president at the Institute
of Education in Hong Kong.
E-mail: llam@ied.edu.hk
• Dr. Joanna (Qizhi) Xu, a former PhD candidate and postdoctoral fellow at
CENPARMI, who contributed to Section 6.3. She is currently a research scien-
tist at Smart Technologies Inc., Calgary, Alberta, Canada.
E-mail: joannaxu@smarttech.com
• Ms. Jie Yao and Mr. Taras Kowaliw, PhD candidates, who contributed to Section
6.1. Both are completing their doctoral studies in the Electrical and Computer
Engineering Department, Concordia University, Canada.
In addition, we would like to thank Shira Katz for proofreading numerous sections
of this book.
We are also very grateful to the following organizations for funding our research
programs:
• NSERC: Natural Sciences and Engineering Research Council of Canada
• FCAR: Fonds pour la Formation de Chercheurs et l’Aide a la Recherche
• CFI: Canadian Foundation for Innovations
• NSFC: National Natural Science Foundation of China
• CAS: Chinese Academy of Sciences
• Industrial organizations: A2IA Corp. (Paris, France), Bell Canada, Canada Post
(Ottawa, Canada), DocImage Inc. (Montreal, Canada), Hitachi Ltd. (Tokyo,
Japan), La Poste (French Post, France)
Furthermore, Mohamed Cheriet is particularly grateful to his PhD student Vincent
Dor´e for carefully reviewing Chapter 2 and for providing considerable assistance by
typesetting parts of a text in LaTeX.
Cheng-Lin Liu is grateful to his former advisors and colleagues for their help,
collaboration and discussions, particularly to Prof. Ruwei Dai (CAS, China), Prof.
Jin H. Kim (KAIST, Korea), Prof. Masaki Nakagawa (TUAT, Japan), Dr. Hiromichi
Fujisawa and his group in Hitachi Ltd. (Japan), and Prof. George Nagy (RPI, USA).

LIST OF FIGURES
2.1
Overview of an intelligent form-reading system.
7
2.2
Character strokes on different types of background; (a) Simple
background, easy to binarize by global thresholding; (b) Slowly
varying background, can be removed by local thresholding;
(c) Fast varying background, which need to be removed by feature
thresholding.
12
2.3
(a) A subimage of a web page image with light-on-dark text; (b) A
one-dimensional proﬁle taken from the subimage, at the position
marked with blue line; two positive double-edge peaks are detected,
with intensity of DE-1 and DE-2, respectively. The width of the
double-edge detector is W (5 pixels); (c) The light-on-dark
double-edges extracted from (a).
15
2.4
Examples of character extraction from web page images.
16
2.5
Further examples of character extraction from web page images.
17
2.6
Histograms of the stroke widths of handwritings and preprinted
entities obtained from 10 sample form images scanned at 300 DPI.
20
2.7
Removal of preprinted texts.
21
2.8
One-dimensional and two-dimensional proﬁles of a Gaussian kernel
with variance σ equal to 9 and a mask support of size 11σ.
24
2.9
Original images and their segmentation results
using Gaussian kernels.
25
2.10
One-dimensional and two-dimensional proﬁles of a KCS kernel with
parameters σ = 15 and γ = 5.
27
xix

xx
LIST OF FIGURES
2.11
Truncated Gaussian kernel with the same variance and the same mask
support as the KCS kernel shown below and the logarithm of its
magnitude response.
27
2.12
KCS kernel and the logarithm of its magnitude response.
28
2.13
Original form images and their segmentation results using a KCS.
29
2.14
Two 3 by 3 smoothing (averaging) ﬁlter masks. The sum of all the
weights (coefﬁcients) in each mask is equal to one.
30
2.15
Examples of ﬁlter masks (3 by 3) that are used for smoothing of binary
document images, (b), (c), and (d) are rotated versions of the ﬁlter in (a)
by 90◦.
31
2.16
(a), (b), and (c) Original images; (d), (e), and (f) Smoothed images,
respectively.
31
2.17
Deviation of the baseline of the text from horizontal direction is
called skew. (a) A skewed handwritten word; (b) Skew corrected
handwritten word in (a); (c) A skewed typewritten text; (d) Skew
corrected image in (c).
32
2.18
Sample image and its projection parallel to the text lines.
33
2.19
Sample image and an oblique projection. Notice the uniform
nature of the distribution.
33
2.20
A rotation transformation can correct the skew of the
document image.
34
2.21
Images of handwritten words and handwritten numeral strings, selected
from USPS-CEDAR CDROM1 database.
34
2.22
(a) A digit circumscribed by a tilted rectangle; (b) slant angle
estimation for the digit in (a).
35
2.23
(a) Original digits; (b) slant corrected digits.
36
2.24
(a) Original character; (b) normalized character ﬁlled in standard
(normalized) plane.
37
2.25
Character image, horizontal and vertical density projections,
and coordinate functions.
41
2.26
From left: original image, linear normalization, nonlinear
normalization, moment normalization; upper/lower: without/with
slant correction.
41
2.27
(a) 4-neighbors (4-connected); (b) 8-neighbors (8-connected).
42
2.28
Two examples of 4-connected component objects.
42
2.29
(a) An 8-connected component pattern; (b) a pattern that is not 4-
or 8-connected.
43
2.30
(a) Original pattern in black and white format; (b) outer contour
of the pattern; (c) both outer and inner contours of the pattern.
43
2.31
(a) Starting point for contour tracing; (b) contour tracing procedure.
44

LIST OF FIGURES
xxi
2.32
Finding the skeleton of a rectangular object: (a) The centers of all the
maximal circles form the skeleton of the object, points A, B are on the
skeleton, but C does not belong to the skeleton; (b) the resulting
skeleton of the rectangular object.
46
2.33
(a) Original pattern; (b) skeleton as a result of thinning.
46
2.34
(a) Original text; (b) skeleton of the text line in (a).
47
2.35
A 3 by 3 window that shows the pixels around P1 and its
neighboring pixels.
47
2.36
Two examples of computations of functions of A(P1), and B(P1): in
(a), A(P1) = 1, B(P1) = 2, and in (b) A(P1) = 2, B(P1) = 2.
48
2.37
In examples shown in this ﬁgure, condition A(P1) = 1 is violated, so by
removing P1 the corresponding patterns will become disconnected.
48
2.38
(a) An example where A(P2) ̸= 1; (b) an example where
P2 · P4 · P8 = 0; (c) an example where P2 · P4 · P8 ̸= 0 and
A(P2) = 1.
49
2.39
(a) An example where A(P4) ̸= 1; (b) an example where
P2 · P4 · P6 = 0; (c) an example where P2 · P4 · P6 ̸= 0 and
A(P4) = 1.
49
2.40
(a) and (d) Original images; (b) and (e) skeletons by Hilditch algorithm;
(c) and (f) skeletons by Zhang-Suen algorithm.
50
3.1
(a) Character image; (b) contour of character; (c) orientation
planes; (d) local orientation histograms.
59
3.2
(a) Contour orientatation planes; (b) blurred orientation planes;
(c) sampled values of blurred planes.
61
3.3
Eight chaincode directions.
61
3.4
Pixels in the neighborhood of current pixel “c”.
62
3.5
Sobel masks for gradient.
62
3.6
Gradient vector decomposition.
63
3.7
Orientation planes merged from eight direction planes of gradient
(upper) and their blurred images (lower).
63
3.8
Polar space representation of a straight line.
69
3.9
(a) An end point pixel Pi; (b) a junction point pixel Pi.
71
3.10
(a) Directional chaincodes; (b) pixel conﬁguration with PM = 3.
72
3.11
Chaincodes: (a) 4-connectivity; (b) 8-connectivity; (c) example.
77
3.12
Bezier curve used in font deﬁnition.
77
3.13
Types of feature points: (a) end point; (b) branch point;
(c) cross point.
79
3.14
First and second principal components.
80
3.15
A typical screen plot.
83
3.16
Subspace axes of PCA and LDA for data points of two classes.
85

xxii
LIST OF FIGURES
3.17
The four approaches to feature selection.
92
3.18
Feature subset evaluation criteria.
94
3.19
Search space.
99
3.20
Forward search.
100
3.21
The basic schemes of sequential search and parallel search.
102
3.22
A taxonomy of feature selection search strategies.
103
3.23
The space of feature creations methods.
105
3.24
Automatic operator (feature) creation procedure [110].
106
3.25
A feature detection operator [105].
107
3.26
A hit-miss matrix and its application to a T pattern [6].
114
4.1
Voronoi diagram formed by training samples of two classes.
The decision surface is denoted by thick lines.
141
4.2
Model of a neuron.
142
4.3
Single-layer neural network. The bias terms are not shown.
144
4.4
Multilayer perceptron. For simplicity, the bias terms
are not shown.
148
4.5
Radial basis function network. The bias terms of output nodes are not
shown.
153
4.6
Two-dimensional array of nodes for SOM.
159
4.7
Hyperplane classiﬁer for binary classiﬁcation and margin.
163
4.8
Two shapes of character “2” encoded as strings of sampled points.
174
4.9
Editing path of the strings in Fig. 4.8 and the correspondence
of points.
174
4.10
A character shape and its ARG representation.
175
4.11
Examples of handwritten digits in USPS database.
194
4.12
Test digits corrected by combining PNC and MQDF. The second and
third columns show the conﬁdence values of two classes by PNC, the
fourth and ﬁfth columns show the conﬁdence values of two classes by
MQDF, and the rightmost column shows the ﬁnal decision.
196
5.1
Overview of word/string recognition methods.
205
5.2
Character strings of different languages and styles (English words and
numeral strings are separated by white space).
207
5.3
Character segmentation by projection analysis. (a) Characters
separable by vertical projection; (b) separation at points of small
projection; (c) separation by projection in the middle zone.
208
5.4
Character segmentation by connected component analysis. Upper:
connected components (left) and segments after merging horizontally
overlapping components.
209
5.5
Cursive word segmentation at visible ligatures (denoted by short
vertical lines) and invisible ligatures (denoted by long vertical lines).
210

LIST OF FIGURES
xxiii
5.6
Some examples of handwritten numeral strings.
210
5.7
Numeral string image preprocessing (smoothing and slant
correction). Left: original; Right: preprocessed.
211
5.8
Foreground feature points’ extraction. (a) Original image; (b) skeleton
after thinning, starting point (S) and ending point (E) are denoted by ;
(c) the skeleton is traversed from left to right in two directions:
clockwise and counter-clockwise; (d) mapping intersection points to
the foreground features on the outer contour (denoted by ).
212
5.9
Background skeleton extraction. (a) Preprocessed connected
component image; (b) top projection proﬁle; (c) bottom projection
proﬁle; (d) background region (white pixels outside the black object);
(e) top-background skeleton; (f) bottom-background skeleton.
212
5.10
Background feature points’ extraction. (a) Original
top/bottom-background skeletons; (b) after removing the parts
crossing the middle line, the end points are background features.
213
5.11
Flowchart of downward/upward search for constructing segmentation
paths.
213
5.12
Feature points on the background and foreground (from the
top and bottom) are matched, and connected to construct
segmentation paths.
214
5.13
A word image (left), its segmented pattern sequences (middle), and
string classes (right).
215
5.14
Segmentation candidate lattice of a word image. The optimal
segmentation candidate is denoted by thick lines.
228
5.15
Segmentation-recognition candidate lattice of the word image of
Fig. 5.13. Each edge denotes a candidate pattern and its
character class.
228
5.16
Match the string image of Fig. 5.14 with two word classes. Upper:
matching path in search space (array), lower: segmented character
patterns.
231
5.17
Character-synchronous (left) and frame-synchronous (right) node
generation in search tree. The nodes in a column are generated
simultaneously.
232
5.18
Trie for a small lexicon of 12 month words.
236
5.19
Directed acyclic word graph (DAWG) of four words.
236
5.20
A left-to-right HMM.
245
5.21
A simple left-to-right HMM with no skipping transitions.
245
5.22
(a) A multiple-branch simple left-to-right HMM; (b) a multiple-
branch left-to-right HMM with the possibility of skipping transitions.
246
5.23
Handwritten legal amount recognition involves the recognition of
each word in the phrase matched against a static lexicon of 33 words.
251

xxiv
LIST OF FIGURES
5.24
A word image (a) and its shape features (b) including “length,”
“ascenders,” “descenders,” “loops,” and so on.
252
5.25
Ambiguities in segmentation: The letter(s) following “H” can be
recognized as “w,” “ui,” “iu,” or “iii”.
252
5.26
Large variability in shapes of handwritten characters (“O” and “P”
in this example).
253
5.27
Ascenders and descenders are perceptual features. Features such as
the proportion of pixels in the left and right segments, the number of
extrema, the perimeter of the word are examples of holistic features.
254
5.28
If the size of the lexicon is small, the word shape or length of
cursive words alone contains sufﬁcient information to classify the
image as one of the lexicon words.
254
5.29
A lexicon of four words which are inverted based on three holistic
features. Each word is represented by a vector of three elements:
[length, number of ascenders, number of descenders].
254
6.1
Tree diagram of an example agent.
267
6.2
Agent A1 and its partition of feature space.
268
6.3
Agent A2 and its partition of feature space.
268
6.4
Agent A3 and its partition of feature space.
270
6.5
Agent A4 and its partition of feature space.
270
6.6
(a) Typical run of the ME trial; (b) typical run of the SGA trial.
274
6.7
(a) Best run of the LAC trial accuracy; (b) best run of the LAC trial
complexity.
275
6.8
(a) Best run of the IAC trial accuracy; (b) best run of the IAC trial
complexity.
277
6.9
Maximum training (light line) and validation (dark lines) accuracies
for the h.0 hunters.
278
6.10
Complexity of most ﬁt agents (dark lines) and mean complexity
(light lines) for the h.0 runs.
279
6.11
Pairs of similar Chinese characters.
282
6.12
Diagram of ofﬂine Chinese character recognition system.
285
6.13
Curves of coordinate mapping: (a) Quadratic curve ﬁtting for
centroid alignment; (b) sine functions for adjusting inner density;
(c) combination of quadratic and sine functions.
288
6.14
Character image normalization by ﬁve methods. The leftmost image is
the original and the other ﬁve are normalized ones.
289
6.15
NCFE on continuous direction plane.
291
6.16
Original image and normalized image (top row), 8-direction chaincode
planes of normalized image (second row), discrete NCFE (third row),
and gradient on binary normalized image (bottom row).
291
6.17
16-direction chaincode formed from two 8-direction chaincodes.
292

LIST OF FIGURES
xxv
6.18
Difference of coordinates and the corresponding 16-direction
chaincodes.
293
6.19
Some test images of ETL9B database (left) and CASIA database
(right).
294
6.20
Sample dates handwritten on standard Canadian bank checks.
302
6.21
Diagram of date processing system.
304
6.22
Diagram of date image segmentation.
305
6.23
Month word samples from CENPARMI IRIS check database.
309
6.24
Conditional combination topology for month word recognition.
312


LIST OF TABLES
2.1
Functions for aspect ratio mapping.
37
2.2
Coordinate mapping functions of normalization methods.
38
3.1
Classes and features.
97
3.2
Major differences between sequential search and parallel search.
102
3.3
Operator set for the GP [38].
112
4.1
Test accuracies (%) of individual classiﬁers, oracle, and DCS.
195
4.2
Accuracies (%) of combination at abstract level: plurality vote and
trained rules.
195
4.3
Accuracies (%) of combination at measurement level: ﬁxed and
trained rules.
196
6.1
Afﬁnity-bits based control of the merger mechanism.
269
6.2
Maximum training and validation accuracies for the
binary classiﬁers.
279
6.3
Percentage agreement in errors made by classiﬁers by digit.
280
6.4
Accuracies (%) of 4-orientation and 8-direction chaincode feature
by zoning and by blurring on ETL9B database.
295
6.5
Accuracies (%) of 4-orientation and 8-direction chaincode feature
by zoning and by blurring on CASIA database.
296
6.6
Accuracies (%) of 8-direction NCFE (discrete and continuous,
denoted by ncf-d and ncf-c) and gradient feature (binary and gray,
denoted by grd-b and grd-g) on ETL9B database.
296
xxvii

xxviii
LIST OF TABLES
6.7
Accuracies (%) of 8-direction NCFE (discrete and continuous,
denoted by ncf-d and ncf-c) and gradient feature (binary and gray,
denoted by grd-b and grd-g) on CASIA database.
297
6.8
Settings of sampling mesh for 8-direction, 12-direction, and
16-direction features.
297
6.9
Accuracies (%) of 8-direction, 12-direction, and 16-direction
features on ETL9B database.
298
6.10
Accuracies (%) of 8-direction, 12-direction, and 16-direction
features on CASIA database.
298
6.11
Accuracies (%) of various normalization methods on ETL9B
database.
299
6.12
Accuracies (%) of various normalization methods on CASIA
database.
299
6.13
Accuracies (%) of MQDF with variable number k of principal
eigenvectors and LVQ with variable number p of prototypes
on CASIA database.
300
6.14
Examples of condition-action rules.
306
6.15
Performances of individual classiﬁers.
311
6.16
Correlation of recognition results among classiﬁers.
311
6.17
Performance of different combination rules on the test set.
313
6.18
Combination results of modiﬁed product rule on the test set.
314
6.19
Performances of date segmentation system for the English
and French sets.
315
6.20
Performances of date processing system for the English
and French sets.
316

ACRONYMS
1-NN
Nearest Neighbor
2D
Two-Dimensional
3D
Three-Dimensional
AC
Auto-Complexity
ADF
Automatically Deﬁned Functions
ANN
Artiﬁcial Neural Network
ANNIGMA
Artiﬁcial Neural Net Input Gain Measurement Approximation
ANSI
American National Standards Institute
ARG
Attributed Relational Graph
ASCII
American Standard Code for Information Interchange
APR
Autonomous Pattern Recognizer
ARAN
Aspect Ratio Adaptive Normalization
BAB
Branch-And-Bound
BKS
Behavior Knowledge Space
BMN
Bi-Moment Normalization
BP
Back Propagation
CC
Connected Component
CCA
Connected Component Analysis
CD
Compound Diversity
CE
Cross Entropy
CI
Class Independent
CL
Competitive Learning
CRS
Character Recognition System
DAG
Directed Acyclic Graph
xxix

xxx
ACRONYMS
DAS
Document Analysis System
DAWG
Directed Acyclic Word Graph
DCS
Dynamic Classiﬁer Selection
DLQDF
Discriminative Learning Quadratic Discriminant Function
DP
Dynamic Programming
DPI
Dot Per Inch
DTW
Dynamic Time Warping
ECMA
European Computer Manufacturers Association
EM
Expectation Maximization
FD
Fourier Descriptor
FDA
Fisher Discriminant Analysis
FDL
Form Description Language
FIFO
First In First Out
GA
Genetic Algorithm
GLVQ
Generalized Learning Vector Quantization
GP
Genetic Programming
GPD
Generalized Probabilistic Descent
HCCR
Handwritten Chinese Character Recognition
HMM
Hidden Markov Model
HONN
Higher-Order Neural Network
HRBTA
Heuristic-Rule-Based-Tracing-Algorithm
HT
Hough Transform
h.u.p.
Heisenberg Uncertainty Principle
IAC
Indian Auto-Complexity
ICA
Independent Component Analysis
ICDAR
International Conference on Document Analysis and Recognition
ICFHR
International Conference on Frontiers in Handwriting Recognition
ICPR
International Conference on Pattern Recognition
IP
Intersection Point
ISO
International Standards Organization
IWFHR
International Workshop on Frontiers in Handwriting Recognition
KCS
Kernel with Compact Support
KKT
Karush-Kuhn-Tucker
k-NN
k-Nearest Neighbor
LAC
Latin Auto-Complexity
LDA
Linear Discriminant Analysis
LDF
Linear Discriminant Function
LN
Linear Normalization
LoG
Laplacian of Gaussian
LOO
Leave-One-Out
LVQ
Learning Vector Quantization
MAP
Maximum A Posteriori
MAT
Medial Axis Transform
MCBA
Modiﬁed Centriod-Boundary Alignment
MCE
Minimum Classiﬁcation Error

ACRONYMS
xxxi
MCM
Machine Condition Monitoring
ME
Merger-Enabled
ML
Maximum Likelihood
MLP
Multi-Layer Perceptron
MMI
Maximum Mutual Information
MN
Moment Normalization
MQDF
Modiﬁed Quadratic Discriminant Function
MSE
Mean Squared Error
NCFE
Normalization-Cooperated Feature Extraction
NLN
Nonlinear Normalization
NMean
Nearest Mean
NN
Neural Network
OCR
Optical Character Recognition
P2D
Pseudo-Two-Dimensional
PA
Projection Analysis
PCA
Principal Component Analysis
PCCR
Printed Chinese Character Recognition
PDA
Personal Digital Aide
PDC
Peripheral Direction Contributivity
PDF
Probability Density Function
PNC
Polynomial Network Classiﬁer
PV
Plurality Vote
QDF
Quadratic Discriminant Function
QP
Quadratic Programming
RBF
Radial Basis Function
RDA
Regularized Discriminant Analysis
RPN
Ridge Polynomial Network
SA
Simple Average
SAT
Symmetry Axis Transform
SCL
Soft Competitive Learning
SGA
Standard Genetic Algorithm
SLNN
Single-Layer Neural Network
SMO
Sequential Minimal Optimization
SOM
Self-Organizing Map
SOR
Successive Overrelaxation
SPR
Statistical Pattern Recognition
SRM
Structural Risk Minimization
SVM
Support Vector Machine
VQ
Vector Quantization
WA
Weighted Average
WTA
Winner-Take-All


CHAPTER 1
INTRODUCTION: CHARACTER
RECOGNITION, EVOLUTION, AND
DEVELOPMENT
This chapter presents an overview of the problems associated with character recognition.
It includes a brief description of the history of OCR (optical character recognition), the
extensive efforts involved to make it work, the recent international activities that have
stimulated the growth of research in handwriting recognition and document analysis, a
short summary of the topics to be discussed in the other chapters, and a list of relevant
references.
1.1
GENERATION AND RECOGNITION OF CHARACTERS
Most people learn to read and write during their ﬁrst few years of education. By the
time they have grown out of childhood, they have already acquired very good reading
and writing skills, including the ability to read most texts, whether they are printed
in different fonts and styles, or handwritten neatly or sloppily. Most people have no
problem in reading the following: light prints or heavy prints; upside down prints;
advertisements in fancy font styles; characters with ﬂowery ornaments and missing
parts; and even characters with funny decorations, stray marks, broken, or fragmented
parts; misspelled words; and artistic and ﬁgurative designs. At times, the characters
and words may appear rather distorted and yet, by experience and by context, most
people can still ﬁgure them out. On the contrary, despite more than ﬁve decades of
intensive research, the reading skill of the computer is still way behind that of human
Character Recognition Systems: A Guide for Students and Practitioner, by M. Cheriet, N. Kharma,
C.-L. Liu and C. Y. Suen
Copyright © 2007 John Wiley & Sons, Inc.
1

2
INTRODUCTION: CHARACTER RECOGNITION, EVOLUTION, AND DEVELOPMENT
beings. Most OCR systems still cannot read degraded documents and handwritten
characters/words.
1.2
HISTORY OF OCR
To understand the phenomena described in the above section, we have to look
at the history of OCR [3, 4, 6], its development, recognition methods, computer
technologies, and the differences between humans and machines [1, 2, 5, 7, 8].
It is always fascinating to be able to ﬁnd ways of enabling a computer to mimic
human functions, like the ability to read, to write, to see things, and so on. OCR
research and development can be traced back to the early 1950s, when scientists
tried to capture the images of characters and texts, ﬁrst by mechanical and opti-
cal means of rotating disks and photomultiplier, ﬂying spot scanner with a cath-
ode ray tube lens, followed by photocells and arrays of them. At ﬁrst, the scan-
ning operation was slow and one line of characters could be digitized at a time
by moving the scanner or the paper medium. Subsequently, the inventions of drum
and ﬂatbed scanners arrived, which extended scanning to the full page. Then, ad-
vances in digital-integrated circuits brought photoarrays with higher density, faster
transports for documents, and higher speed in scanning and digital conversions.
These important improvements greatly accelerated the speed of character recogni-
tion and reduced the cost, and opened up the possibilities of processing a great
variety of forms and documents. Throughout the 1960s and 1970s, new OCR ap-
plications sprang up in retail businesses, banks, hospitals, post ofﬁces; insurance,
railroad, and aircraft companies; newspaper publishers, and many other industries
[3, 4].
In parallel with these advances in hardware development, intensive research on
character recognition was taking place in the research laboratories of both academic
and industrial sectors [6, 7]. Although both recognition techniques and computers
were not that powerful in the early days (1960s), OCR machines tended to make
lots of errors when the print quality was poor, caused either by wide variations in
typefonts and roughness of the surface of the paper or by the cotton ribbons of the
typewriters [5]. To make OCR work efﬁciently and economically, there was a big
push from OCR manufacturers and suppliers toward the standardization of print fonts,
paper, and ink qualities for OCR applications. New fonts such as OCRA and OCRB
were designed in the 1970s by the American National Standards Institute (ANSI) and
the European Computer Manufacturers Association (ECMA), respectively. These
special fonts were quickly adopted by the International Standards Organization (ISO)
to facilitate the recognition process [3, 4, 6, 7]. As a result, very high recognition rates
became achievable at high speed and at reasonable costs. Such accomplishments also
brought better printing qualities of data and paper for practical applications. Actually,
they completely revolutionalized the data input industry [6] and eliminated the jobs of
thousands of keypunch operators who were doing the really mundane work of keying
data into the computer.

ORGANIZATION OF THE REMAINING CHAPTERS
3
1.3
DEVELOPMENT OF NEW TECHNIQUES
As OCR research and development advanced, demands on handwriting recognition
also increased because a lot of data (such as addresses written on envelopes; amounts
written on checks; names, addresses, identity numbers, and dollar values written on
invoices and forms) were written by hand and they had to be entered into the computer
for processing. But early OCR techniques were based mostly on template matching,
simple line and geometric features, stroke detection, and the extraction of their deriva-
tives. Such techniques were not sophisticated enough for practical recognition of data
handwritten on forms or documents. To cope with this, the Standards Committees
in the United States, Canada, Japan, and some countries in Europe designed some
handprint models in the 1970s and 1980s for people to write them in boxes [7]. Hence,
characters written in such speciﬁed shapes did not vary too much in styles, and they
could be recognized more easily by OCR machines, especially when the data were en-
tered by controlled groups of people, for example, employees of the same company
were asked to write their data like the advocated models. Sometimes writers were
asked to follow certain additional instructions to enhance the quality of their samples,
for example, write big, close the loops, use simple shapes, do not link characters, and
so on. With such constraints, OCR recognition of handprints was able to ﬂourish for
a number of years.
1.4
RECENT TRENDS AND MOVEMENTS
As the years of intensive research and development went by, and with the birth of
several new conferences and workshops such as IWFHR (International Workshop
on Frontiers in Handwriting Recognition),1 ICDAR (International Conference on
Document Analysis and Recognition),2 and others [8], recognition techniques
advanced rapidly. Moreover, computers became much more powerful than before.
People could write the way they normally did, and characters need not have to be writ-
ten like speciﬁed models, and the subject of unconstrained handwriting recognition
gained considerable momentum and grew quickly. As of now, many new algorithms
and techniques in preprocessing, feature extraction, and powerful classiﬁcation meth-
ods have been developed [8, 9]. Further details can be found in the following chapters.
1.5
ORGANIZATION OF THE REMAINING CHAPTERS
Nowadays, in OCR, once a printed or handwritten text has been captured optically by
a scanner or some other optical means, the digital image goes through the following
stages of a computer recognition system:
1Note that IWFHR has been promoted to an international conference, namely, the International Conference
on Frontiers on Handwriting Recognition (ICFHR), starting in 2008 in Montreal, where it was born in 1990.
2IWFHR and ICDAR series were founded by Dr. Ching Y. Suen, coauthor of this book, in 1990 and 1991,
respectively.

4
INTRODUCTION: CHARACTER RECOGNITION, EVOLUTION, AND DEVELOPMENT
1. The preprocessing stage that enhances the quality of the input image and locates
the data of interest.
2. The feature extraction stage that captures the distinctive characteristics of the
digitized characters for recognition.
3. The classiﬁcation stage that processes the feature vectors to identify the char-
acters and words.
Hence, this book is organized according to the above sequences.
REFERENCES
1. H. Bunke and P. S. P. Wang. Handbook of Character Recognition and Document Image
Analysis. World Scientiﬁc Publishing, Singapore, 1997.
2. S. Mori, H. Nishida, and H. Yamada. Optical Character Recognition, Wiley Interscience,
New Jersey, 1999.
3. Optical Character Recognition and the Years Ahead. The Business Press, Elmhurst, IL,
1969.
4. Pas d’auteur. Auerbach on Optical Character Recognition. Auerbach Publishers, Inc.,
Princeton, 1971.
5. S. V. Rice, G. Nagy, and T. A. Nartker. Optical Character Recognition: An Illustrated
Guide to the Frontier. Kluwer Academic Publishers, Boston, 1999.
6. H. F. Schantz. The History of OCR. Recognition Technologies Users Association, Boston,
1982.
7. C. Y. Suen. Character recognition by computer and applications. In T. Y. Young and
K. S. Fu, editors, Handbook of Pattern Recognition and Image Processing. Academic
Press, Inc., Orlando, FL, 1986, pp. 569–586.
8. Proceedings of the following international workshops and conferences:
 ICPR—International Conference on Pattern Recognition
 ICDAR—International Conference on Document Analysis and Recognition
 DAS—Document Analysis Systems
 IWFHR—International Workshop on Frontiers in Handwriting Recognition.
9. Journals, in particular:
 Pattern Recognition
 Pattern Recognition Letters
 Pattern Analysis and Applications
 International Journal on Document Analysis and Recognition
 International Journal of Pattern Recognition and Artiﬁcial Intelligence.

CHAPTER 2
TOOLS FOR IMAGE PREPROCESSING
To extract symbolic information from millions of pixels in document images, each
component in the character recognition system is designed to reduce the amount of data.
As the ﬁrst important step, image and data preprocessing serve the purpose of extracting
regions of interest, enhancing and cleaning up the images, so that they can be directly
and efﬁciently processed by the feature extraction component. This chapter covers the
most widely used preprocessing techniques and is organized in a global-to-local fashion.
It starts with an intelligent form-processing system that is capable of extracting items
of interest, then explains how to separate the characters from backgrounds. Finally, it
describes how to correct defects such as skewed text lines and slanted characters, how
to normalize character images, and how to extract contours and skeletons of characters
that can be used for more efﬁcient feature extraction.
2.1
GENERIC FORM-PROCESSING SYSTEM
When you ﬁle the annual income tax form, writing a check to pay the utility bill, or ﬁll
in a credit card application form, have you ever wondered how these documents are
processed and stored? Millions of such documents need to be processed as an essential
operation in many business and government organizations including telecommunica-
tions, health care, ﬁnance, insurance, and government and public utilities. Driven by
the great need of reducing both the huge amount of human labor involved in read-
ing and entering the data on to electronic databases and the associated human error,
many intelligent automated form-processing systems were designed and put to prac-
Character Recognition Systems: A Guide for Students and Practitioner, by M. Cheriet, N. Kharma,
C.-L. Liu and C. Y. Suen
Copyright © 2007 John Wiley & Sons, Inc.
5

6
TOOLS FOR IMAGE PREPROCESSING
tice. This section will introduce the general concept of an intelligent form-processing
system that can serve as the kernel of all form-processing applications.
The input of a typical form-processing system consists of color or gray level
images in which every several hundred pixels corresponds to a linear inch in the
paper medium. The output, on the contrary, consists of ASCII strings that usually
can be represented by just a few hundred bytes. To achieve such a high level of data
abstraction, a form-processing system depends on the following main components
for information extraction and processing:
• Image acquisition: acquire the image of a form in color, gray level, or binary
format.
• Binarization: convert the acquired form images to binary format, in which the
foreground contains logo, the form frame lines, the preprinted entities, and the
ﬁlled in data.
• Form identiﬁcation: for an input form image, identify the most suitable form
model from a database.
• Layout analysis: understand the structure of forms and the semantic meaning of
the information in tables or forms.
• Data extraction: extract pertinent data from respective ﬁelds and preprocess the
data to remove noise and enhance the data.
• Character recognition: convert the gray or binary images that contain textual in-
formation to electronic representation of characters that facilitate postprocessing
including data validation and syntax analysis.
Each of the above steps reduces the amount of the information to be processed by a
later step. Conventional approaches pass information through these components in a
cascade manner and seek the best solution for each step. The rigid system architecture
and constant information ﬂowing direction limit the performance of each component
to local maximum. The observation from data in real-life applications and the way
humans read distorted data lead us to construct a knowledge-based, intelligent form-
processing system. Instead of simply concatenating general-purpose modules with
little consideration of the characteristics of the input document, the performance of
each module is improved by utilizing as much knowledge as possible, and the global
performance is optimized by interacting with the knowledge database at run-time.
An overview of an intelligent form-processing system with some typical inputs and
outputs is illustrated in Figure 2.1. As the kernel of this intelligent form-processing
system, the knowledge database is composed of short-term and long-term memories
along with a set of generic and speciﬁc tools for document processing. The short-
term memory stores the knowledge gathered in run-time, such as the statistics of a
batch of input documents. The long-term memory stores the knowledge gathered in
the training phase, such as the references for recognizing characters and logos, and
identifying form types. Different types of form images can be characterized by their
speciﬁc layout. Instead of keeping the original images for model forms, only the ex-
tracted signatures are needed in building the database for known form types. Here,

GENERIC FORM-PROCESSING SYSTEM
7
FIGURE 2.1
Overview of an intelligent form-reading system.
the signature of a form image is deﬁned as a set of statistical and structural features
extracted from the input form image. The generic tools include various binarization
methods, binary smoothing methods, normalization methods, and so on. The speciﬁc
tools include cleaning, recognition, or veriﬁcation methods for certain types of input
images. When an input document passes the major components of the systems, the
knowledge in the database helps the components to select the proper parameters or
references to process. At the same time, each component gathers respective infor-
mation and sends back to the knowledge base. Therefore, the knowledge base will
adapt itself dynamically at run-time. At run-time, for each unknown document image,
features are extracted and compared with signatures of the prototypes exposed to the
system in the learning phase.

8
TOOLS FOR IMAGE PREPROCESSING
As of understanding the form structures and extracting the areas of interest, there
are two types of approaches in form-reading techniques: The ﬁrst type of form readers
relies upon a library of form models that describe the structure of forms being pro-
cessed. A form identiﬁcation stage matches the extracted features (e.g., line crossings)
from an incoming form against those extracted from each modeled design in order
to select the appropriate model for subsequent system processes. The second type of
form readers do not need an explicit model for every design that may be encountered,
but instead rely upon a model that describes only design-invariant features associated
with a form class. For example, in ﬁnancial document processing, instead of storing
blank forms of every possible checks or bills that may be encountered, the system may
record rules that govern how the foreground components in a check may be placed
relative to its background baselines. Such a generic ﬁnancial document processing
system based on staff line and a form description language (FDL) is described [28]. In
either type of form readers, the system extracts the signature, such as a preprinted logo
or a character string, of each training form during the training phase, and stores the
information in the long-term memory of the knowledge base. In the working phase,
the signature of an input form is extracted and compared statistically and syntacti-
cally to the knowledge in the database. By deﬁning a similarity between signatures
from the two form images, we will be able to identify the format of the input form. If
the input form is of known type, according to the form registration information, the
pertinent items can be extracted directly from approximate positions. Otherwise, the
signature of this unknown form can be registered through human intervention.
Through form analysis and understanding, we are able to extract the form frames
that describe the structure of a form document, and thus extract the user ﬁlled data
image from the original form image. Instead of keeping the original form image,
only extracted user ﬁlled data images need to be stored for future reference. This
will help to minimize storage and facilitates accessibility. Meanwhile, we can use
the “signature” of form images to index and retrieve desired documents in a large
database. As the kernel of this up to this moment, the useful information in a form is
reduced from a document image that usually contains millions of gray level or color
pixels to only the subimages that contain the items of interest.
2.2
A STROKE MODEL FOR COMPLEX BACKGROUND ELIMINATION
Before the items of interest can be automatically recognized and thus converted to
simple ASCII strings, a crucial step is to extract only the pixels that belong to the
characters to be recognized. For documents that are printed in “dropout” colors that are
brighterthantheblackorblueinkwithwhichpeopleﬁllforms,thebackgroundscanbe
dropped by applying color image ﬁlters either during or after the image acquisition
step. This will be very useful in separating the user-entered data from machine-
preprinted texts. Especially, when the user-entered data touch or cross the preprinted
entities, color may be the only information that can distinguish these two counterparts
sharing similar spatial features. Color image processing is time and space consuming.
Moreover, a majority of the forms are printed without dropout ink for the sake of low

A STROKE MODEL FOR COMPLEX BACKGROUND ELIMINATION
9
cost and convenience. For throughput and cost concerns, most document processing
systems resort to gray level, or even binary images.
In gray level documents, the data extraction procedure often requires binarizing
the images, which discards most of the noise and replaces the pixels in the characters
and the pixels in the backgrounds with binary 0s and 1s, respectively. The accuracy
of data extraction is critical to the performance of a character recognition system, in
that the feature extraction and the recognition performance of the characters largely
depend on the quality of the data extracted. The variability of the background and
structure of document images, together with the intrinsic complexity of the character
recognition problem, makes the development of general algorithms and strategies for
automatic character extraction difﬁcult.
Modeling character objects is thus an attractive topic in researches of character
extraction methods. Different methods taking advantage of various aspects of the def-
inition of a character stroke evolved and ﬂourished in different applications. Consider
a document image with character data appearing darker than the background; the
following facts are often used to derive different character extraction techniques:
• The characters are composed of strokes.
• A stroke is an elongated connected component that is darker than the pixels in
its local neighborhood.
• A stroke has a nominally constant width that is deﬁned by the distance between
two close gray level transitions of the opposite directions.
• The local background of a stroke has a much smaller variation compared to that
of a stroke.
The simplest property that the pixels in a character can share is intensity. There-
fore a natural way to extract the characters is through thresholding, a technique that
separates a pixel to object or background by comparing its gray level or other feature
to a reference value. Binarization techniques based on gray levels can be divided
into two classes: global and local thresholdings. Global thresholding algorithms use
a single threshold for the entire image. A pixel having a gray level lower than the
threshold value is labeled as print (black), otherwise background (white). Locally,
adaptive binarization methods compute a designated threshold for each pixel based
on a neighborhood of the pixel. Some of the methods calculate a smooth threshold
surface over the entire image. If a pixel in the original image has a gray level higher
than the threshold surface value at the corresponding location, it is labeled as back-
ground, otherwise it is labeled as print. Other locally adaptive methods do not use
explicit thresholds, but search for object pixels in a feature space such as gradient
magnitude among others.
2.2.1
Global Gray Level Thresholding
For document images in which all character strokes have a constant gray level that
is unique among all background objects, the global thresholding methods are the

10
TOOLS FOR IMAGE PREPROCESSING
most efﬁcient methods. The histogram of these document images are bimodal so
that the characters and the backgrounds contribute to distinct modes, and a global
threshold can be deliberately selected to classify best the pixels in the original image
as object or background. The classiﬁcation of each pixel simply depends on their gray
level in the global view regardless of local features. Among many global thresholding
techniques, the Otsu threshold selection [25] is ranked as the best and the fastest global
thresholding method [29, 30]. Based on discriminant analysis, Otsu thresholding
method is also being referred to as “optimal thresholding” and has been used in a wide
range of machine vision-related applications that involve a two-class classiﬁcation
problem.Thethresholdingoperationcorrespondstopartitioningthepixelsofanimage
into two classes, C0 and C1, at a threshold t. The Otsu method solves the problem of
ﬁnding the optimal threshold t∗by minimizing the error of classifying a background
pixel as a foreground one or vice versa.
Without losing generality, we deﬁne objects as dark characters against lighter
backgrounds. For an image with gray level ranges within G = {0, 1, . . . , L −1}, the
object and background can be represented by two classes, as C0 = {0, 1, . . . , t} and
C1 = {t + 1, t + 2, . . . , L −1}. As elegant and simple criteria for class separability,
the within and between class scatter matrices are widely used in discriminant analysis
ofstatistics[12].Thewithin-classvariance,between-classvariance,andtotal-variance
reach the maximum at equivalent threshold t. Using σ2
W, σ2
B, and σ2
T to represent them,
respectively, the optimal threshold t∗can be determined by maximizing one of the
following functions against the threshold:
λ = σ2
B
σ2
W
, η = σ2
B
σ2
T
, κ = σ2
T
σ2
W
.
(2.1)
Taking η, for example, the optimal threshold t∗is determined as
t∗= arg
max
t∈[0,L−1](η),
(2.2)
in which
Pi = ni/n, w0 =
t

i=0
Pi, w1 = w −w0,
(2.3)
μT =
L−1

i=0
iPi, μt =
t

i=0
iPi, μ0 = μt
w0
, μ1 = μT −μt
1 −μ0
,
(2.4)
σ2
T =
L−1

i=0
(i −μT)2Pi, σ2
B = w0w1(μ1 −μ0)2.
(2.5)
Here ni is the ith element of the histogram, that is, the number of pixels at gray
level i; n = L−1
i=0 ni is the total number of pixels in the image; Pi = ni/n is the

A STROKE MODEL FOR COMPLEX BACKGROUND ELIMINATION
11
probability of occurrence at gray level i. For a selected threshold t∗of a given image,
the class probabilities wo and w1 represent the portions of areas occupied by object
and background classes, respectively. The maximal value of η, denoted by η∗, can
serve as a measurement of class separability between the two classes, or the bimodality
of the histogram. The class separability η lies in the range [0, 1]; the lower bound is
reached when the image has a uniform gray level, and the upper bound is achieved
when the image consists of only two gray levels.
2.2.2
Local Gray Level Thresholding
Real-life documents are sometimes designed deliberately with stylistic, colorful, and
complex backgrounds, causing difﬁculties in character extraction methods. While
global thresholding techniques can extract objects from simple, uniform backgrounds
at high speed, local thresholding methods can eliminate varying backgrounds at a price
of long processing time.
A comparative study of binarization methods for document images has been given
by Trier et al. [29, 30]. Eleven most promising locally adaptive algorithms have been
studied: Bernsen’s method, Chow and Kaneko’s method, Eikvil’s method, Mardia and
Hainsworth’s method, Niblack’s method, Taxt’s method, Yanowitz and Bruckstein’s
method, White and Rohrer’s Dynamic Threshold Algorithm, White and Rhorer’s
Integrated Function Algorithm, Parker’s method, and Trier and Taxt’s method (cor-
responding references can be found in [29, 30]). Among them the ﬁrst eight meth-
ods use explicit thresholds or threshold surfaces, whereas the latter three methods
search for printed pixels after having located the edges. Trier and Jain concluded that
Niblack’s [24] and Bernsen’s [5] methods along with postprocessing step proposed
by Yannowitz and Bruckstein [37] were the fastest and best ones concerning a set of
subjective and goal-directed criteria. The postprocessing step here is used to improve
the binary image by removing “ghost” objects. The average gradient value at the edge
of each printed object is calculated, and objects having an average gradient below a
threshold are labeled as misclassiﬁed and therefore removed.
The Bernsen’s local thresholding method computes the local minimum and
maximum for a neighborhood around each pixel f(x, y) ∈[0, L −1], and uses the
median of the two as the threshold for the pixel in consideration:
g(x, y) = (Fmax(x, y) + Fmin(x, y))/2,
(2.6)
b(x, y) =

1
if f(x, y) < g(x, y),
0
otherwise.
(2.7)
Fmax(x, y) and Fmin(x, y) are the maximal and minimal values in a local neighborhood
centered at pixel (x, y).

12
TOOLS FOR IMAGE PREPROCESSING
FIGURE 2.2
Character strokes on different types of background; (a) Simple background,
easy to binarize by global thresholding; (b) Slowly varying background, which can be removed
by local thresholding; (c) Fast varying background, which need to be removed by feature
thresholding.
To avoid “ghost” phenomena, the local variance c(x, y) can be computed as
c(x, y) = Fmax(x, y) −Fmin(x, y).
(2.8)
The classiﬁcation of a foreground pixel f(x, y) can be veriﬁed by examining the local
variance being higher than a threshold. Each pixel is classiﬁed as object or background
pixel according to the following condition:
b(x, y) =
⎧
⎪
⎨
⎪
⎩
1
if (f(x, y) < g(x, y) and c(x, y) > c∗) or
(f(x, y) < f ∗and c(x, y) ≤c∗),
0
otherwise.
(2.9)
The thresholds c∗and f ∗are determined by applying Otsu’s method to the histogram
of c(x, y) and g(x, y), respectively. The characters are extracted and stored as B =
{b(x, y)|(x, y) ∈F}.
This local thresholding method is sufﬁcient for document images with slowly
varying nonuniform backgrounds. However, when the background tends to vary at
sharp edges, the edge of background can be taken as objects, causing great difﬁculty
to the ensuing recognition stage. Figure 2.2 shows some examples of character strokes
on different types of backgrounds, each posing a greater difﬁculty in extracting the
strokes. Figure 2.2(a) is an ideal case for simple background that can be removed by
a global threshold; Figure 2.2(b) illustrates a slow-varying background that can be
removed by the above mentioned local thresholding method; Figure 2.2(c) illustrates
a background image with a sharp edge in the middle, which will produce an artifact
if the local thresholding method is applied. The solution to this problem requires a
more precise and robust feature to describe the objects of our interest.
2.2.3
Local Feature Thresholding—Stroke-Based Model
When the characters in a document image cannot distinguish themselves from the
backgrounds in the gray level space, the character extraction can be done in a feature
space in which the characters can be distinctively described.
Edge-based image analysis is a widely used method in computer vision. Marr
[22] describes any type of signiﬁcant scene structures in an image as discontinuities

A STROKE MODEL FOR COMPLEX BACKGROUND ELIMINATION
13
in the intensity, generally called edge. From the point of view of extracting scene
structures, the objective is not only ﬁnding signiﬁcant intensity discontinuities but
also suppressing unnecessary details and noise while preserving positional accuracy.
Bergholm [4] proposed that in any image, there are only four elementary structures:
the “step edge,” the “double edge,” the “corner edge” (L-junction), and the “edge box”
(blob):
step edge f(x, y) =

1
if x > a,
0
elsewhere
double edge f(x, y) =

1
if a < x < b,
0
elsewhere
corner edge f(x, y) =

1
if x > a and y > b,
0
elsewhere
edge box f(x, y) =

1
if a < x < b and c < y < d.
0
elsewhere
The double-edge model, delimited by a positive edge and a negative one nearby, is
a good description of a stroke in a local region. An edge box can also be considered
as double edges in two orthogonal directions. Based on this observation, Palumbo
[26] proposed a second derivative method to extract the characters; White and Rohrer
[32] proposed an integrated function technique to extract the sequence of +, −, and
0 in the differential image and take the pixels between sequences “+−” and “+”
as objects; Liu et al. [21] extracted baselines from bank checks by searching for
a pair of opposite edges in a predeﬁned distance. One common problem for these
methods is the sensitivity to noise because the depths of the strokes have not been
taken into account. Based on this observation, many approaches were proposed to
describe character strokes in different ways, with the common features behind are the
thickness and darkness of a stroke. In this sense, the local threshold techniques based
on stroke width restriction can be uniﬁed into searching for double-edge features
occurring within a certain distance. In one-dimensional proﬁle, the intensity of a
stroke with thickness W can be estimated as a double edge, whose intensity is
DE+(x) =
max
i∈[1,W−1] {min(f(x −i), f(x + W −i))} −f(x)
(2.10)
for dark-on-light strokes, or
DE−(x) = f(x) −
min
i∈[1,W−1] {max(f(x −i), f(x + W −i))}
(2.11)
for light-on-light strokes.
For a given pixel p on the stroke, we assume that d1 and d2 are the distances
between p and the nearest opposite boundaries p1 and p2. Therefore the double-

14
TOOLS FOR IMAGE PREPROCESSING
edge feature value at pixel p is positive if and only if (d1 + d2) < W. Taking all
possible directions of a stroke into consideration, we can use d = 0, 1, 2, 3 to refer
to four directions {0, π/4, π/2, 3π/4} that approximate most possible edge directions
around a pixel. Therefore the intensity or darkness of a two-dimensional stroke can
be approximated as
DEw(p) = max
d∈[0,3]{DEWd},
(2.12)
whose positive values indicate the existence of a dark stroke written on a lighter
background. The depth or intensity of a dark-on-light stroke is measured by the
magnitude of the positive double-edge features being extracted. Similarly, the double-
edge features can also be used to extract light-on-dark strokes that compose characters
in scene images, such as car license plates, or road signs, and so on. The problem of
extracting character strokes from a gray level image is thus converted to binarizing
an image whose pixel values correspond to the magnitude of the double-edge feature
in the original image. Although a global thresholding of the double-edge feature
image can usually generate a satisfactory result, a hysteresis thresholding may further
remove noise and tolerate the variation of gray scales within a stroke. Such a step can
be implemented using region growing or conditional dilation techniques.
Bi = (Bi−1 ⊕D)
	
|DEW|T1,
(2.13)
i = 1, 2, . . . , B0 = |DEW|T0, |F|T = {fT(x, y)|(x, y) ∈F},
fT(x, y) =
⎧
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎩

1
if f(x, y) < T
0
otherwise
if T is a constant

1
if f(x, y) < T
0
otherwise
if T is an image
,
Here, |F|T is a thresholding operation with either a constant threshold T or a threshold
surface deﬁned by an image, and 0 < T1 < T0 < L are two thresholds for the intensity
of strokes, and D is a 3 × 3 square structuring element. Equation 2.13 is repeated until
there is no change from Bi−1 to Bi. The characters are thus extracted and stored as
Bn, where Bn = Bn−1 holds. One practical choice of the thresholds T0, T1, as 1.2
and 0.8 times the Ostu optimal threshold for the double-edge feature image. A low
threshold of T0 tends to expand the extracted strokes to background regions, whereas
a high T1 excludes isolated light handwritten strokes from the ﬁnal results.
Figure 2.3 shows a close-up of character extraction results from web images. The
hysteresis thresholding of the double-edge feature images ensures that the complex
backgrounds around the texts are removed. More examples are shown in Figure 2.4
and 2.5. Further postprocessing that utilizes topological information of the binarized

A STROKE MODEL FOR COMPLEX BACKGROUND ELIMINATION
15
FIGURE 2.3
(a) A subimage of a web page image with light-on-dark text; (b) A one-
dimensional proﬁle taken from the subimage, at the position marked with blue line; two positive
double-edge peaks are detected, with intensity of DE-1 and DE-2, respectively. The width of
the double-edge detector is W (5 pixels); (c) The light-on-dark double-edges extracted from (a).
foregrounds can remove the isolated blobs that do not belong to a text line. Such
techniques are very useful for content-based image indexing, including searching for
web images that contain certain textual information.
2.2.4
Choosing the Most Efﬁcient Character Extraction Method
As stated in the previous section, the selection of an efﬁcient character extraction
method depends largely on the complexity of the images. Real-life documents
are sometimes designed deliberately with stylistic, colorful, and complex back-
grounds, causing difﬁculties in choosing appropriate character extraction methods.
Although global thresholding techniques can extract objects from simple or uniform
backgrounds at high speed, local thresholding methods can eliminate varying
backgrounds at a price of processing time. Document images such as checks or maps
may contain complex backgrounds consisting of pictures and designs printed in
various colors and intensities, and are often contaminated by varying backgrounds,
low contrast, and stochastic noise. When designing a character recognition system
for these documents, the locally adaptive algorithms are apparently better choices.
For the thresholding techniques discussed in this section, regardless of the space from
which the thresholds are computed, the computational complexity of a sequential
implementation of the thresholding is measured by the number of operations required
to compute the threshold for each pixel.

16
TOOLS FOR IMAGE PREPROCESSING
FIGURE 2.4
Examples of character extraction from web page images.
• Local gray level thresholding (enhanced Bernsen’s [5] method). The local gray
level threshold at size W is calculated by Eqs. 2.6–2.9. The major part of
the computations involved in this method is the calculation of local maximum
and local minimum described in formula 2.6. Because N −1 comparisons are

A STROKE MODEL FOR COMPLEX BACKGROUND ELIMINATION
17
FIGURE 2.5
Further examples of character extraction from web page images.
required to extract the maximum or minimum value of N elements, (W2 −1) ×
2 comparisons are required for each pixel in the worst case. Regardless of the size
of the structuring elements, erosion and dilation by linear structuring elements
can be implemented with only three comparisons for each pixel and with six
comparisons by rectangular structuring elements. Therefore, the total number
of comparisons required in calculating the local maximum and minimum is
C1 = 6 × 2 = 12.
(2.14)
• Double-edge-based feature thresholding. The intensity of a double edge with
thickness W is calculated by Eqs. 2.10 and 2.11. Similarly, [2 + (W + 1)]

18
TOOLS FOR IMAGE PREPROCESSING
comparisons are required for each DEd(p)(d = 0, 1, 2, 3), and the total number
of comparisons required in calculating double edge for each pixel is
C2 = [2 + (W + 1)] × 4 + 3 = 4W + 7.
(2.15)
When W becomes larger than three, the fast algorithm independent of kernel
size can be used to calculate the local maxima. In this case, the total number of
comparisons required for calculating double edge at each pixel in the raw image
becomes
C3 =

4W + 7
if W ≤3,
(3 + 1) × 4 + 3 = 19
otherwise.
(2.16)
Because each method improves its robustness at the price of reducing the
processing speed, when designing a high-throughput application system that needs
to handle document images with a spectrum of complexity in the backgrounds, it is
very important to have a balanced solution to process large quantities of documents
at both high speed and high reliability. The ﬁrst step of a balanced solution is to
differentiate complex images from simple ones, so that various types of images
can be processed with the appropriate methods. Meanwhile, the classiﬁcation of
document images should involve as little computation as possible, so that no extra
computation is needed for processing simple images.
An image complexity measurement such as the class separability in the Otsu’s
method can help to make the decision of choosing the appropriate character extrac-
tion approach. The maximal ratio of the between-class variance and total-variance
corresponds to the optimal threshold η∗and can serve as a measurement of the separa-
bility between the two classes and the bimodality of the histogram. Because the larger
the η is, the simpler the background, a document image can be classiﬁed according
to its class separabilities and treated with different thresholding techniques. For an
ideal case when characters and background are composed of only two distinct gray
levels, the class separability reaches the maximum, η∗.
By choosing one or several proper thresholds for class separability, we are able to
classify document images as relatively “simple” or “complex,” and thus apply corre-
sponding binarization methods to optimize the batch processing speed and meanwhile
preserve recognition rate of the overall system. The thresholds for class separability
are established from a training set of real-life document images, depending on the
application. The training set should be composed of document images at different
complexity levels, and the distribution of the complexity should conform to that of
the real inputs to the system.
Althoughthereisnoexplicitfunctionofimagecomplexityversusclassseparability,
it is clearly a nonincreasing function. We may choose two thresholds η0 and η1,
0 ≤η0 ≤η1 ≤1, to classify the document images into three classes: “simple” images
with η1 < η ≤1, “medium” images with η0 < η ≤η1, or “complex” images with
0 ≤η ≤η0 and assign a speciﬁc thresholding technique to individual ranges. Suppose

A STROKE MODEL FOR COMPLEX BACKGROUND ELIMINATION
19
the average processing time of one image by global gray level, local gray level, and
feature thresholding methods are t0, t1, and t2, respectively; the average processing
time can be expected as
¯t =
2

i=0
pi.ti,
(2.17)
in which pi represents the probability of different types of images in a testing set,
estimated by the total number of images in the testing set and the number of images
belonging to various classes. For the simplest case, when pi = 1/3 (i = 0, 1, 2), the
expectation of the processing time is the algebraic average of ti(i = 0, 1, 2), which
is a substantial improvement over a sole feature thresholding technique when large
quantities of images are processed.
2.2.5
Cleaning Up Form Items Using Stroke-Based Model
The stroke model is useful not only for extracting characters from gray level
documents images but also for cleaning up the form entries from binary images.
As a prerequisite step of a form-processing system, the major task in form dropout is
to separate and remove the preprinted entities while preserving the user entered data
by means of image-processing techniques. Many form-processing systems have been
found successful when the ﬁlled-in items are machine-printed characters. However,
as discussed in [39] and [1], a number of these approaches perform the extraction
of ﬁlled-in items without paying attention to ﬁeld overlap problem. This happens
when the ﬁlled-in items touch or cross form frames or preprinted texts. For the ap-
proaches that can drop out form frames or straight lines, the preprinted texts remain
an unsolved problem. When the ﬁlled-in items are unconstrained handwritings, this
problem is more pronounced and can prevent the whole processing system from
functioning properly.
A subimage obtained from the item location and extraction module of a form
registration system usually consists of three components:
• Form frames, including black lines, usually called baselines, and blocks;
• Preprinted data such as logos and machine preprinted characters;
• User ﬁlled-in data (including machine-typed and/or handwritten characters and
some check marks) located in predeﬁned areas, called ﬁlled-in data areas, which
are bounded by baselines and preprinted texts.
These three components actually carry two types of information: preprinted enti-
ties, which give instructions to the users of the form; and the ﬁlled-in data. In most
applications where rigid form structures are used, the preprinted entities appear at
the same expected positions. In an ideal case, the ﬁlled-in items can be extracted by
a simple subtraction of a registered form model from the input image. With distor-
tion, skewing, scaling, and noise introduced by the scanning procedure, it is almost

20
TOOLS FOR IMAGE PREPROCESSING
impossible to ﬁnd an exact match between the input image and the model form. One
way of dealing with this degradation is to deﬁne a region in which the preprinted
entities are mostly likely to appear and remove them by the stroke width difference
between the ﬁlled data and the preprinted entities. Because the strokes of the ﬁlled-in
characters can be either attached to or located across the form frames and preprinted
texts, the problem of item cleaning involves the following steps:
• Estimating the positions of preprinted entities;
• Separating characters from form frames or baselines;
• Reconstructing strokes broken during baseline removal;
• Separating characters from preprinted texts.
One of the most important characteristics of character objects is the stroke width.
For each foreground pixel in a binary image, the stroke width can be approximated
as SW(x, y) = min(SWH, SWV), in which SWH and SWV are the distances between
the two closest background pixels in horizontal and vertical directions. We have
observed that in most real-life applications, the form frames and the instructions are
printed in relatively small fonts. When the users ﬁll in the forms with ball or ink
pen, the stroke width of the handwritings is usually larger than that of the preprinted
entities. The histograms of the handwritings and the preprinted entities in 10 form
samples scanned at 300 DPI are shown in Figure 2.6, which clearly shows the different
distributions. This observation helps us to distinguish the preprinted frames and texts
from handwritings by eliminating the pixels whose corresponding stroke width is less
than a threshold. The stroke width of the handwriting (thw) and the preprinted entities
(tpp) can be estimated at run-time by collecting histograms of stroke widths:
FIGURE2.6
Histogramsofthestrokewidthsofhandwritingsandpreprintedentitiesobtained
from 10 sample form images scanned at 300 DPI.

A SCALE-SPACE APPROACH FOR VISUAL DATA EXTRACTION
21
FIGURE 2.7
Removal of preprinted texts.
thw = arg max {hist[SW(x, y)|(x, y) ∈ﬁll in region]},
(2.18)
tpp = arg max {hist[SW(x, y)|(x, y) ∈preprinted region]}.
(2.19)
Therefore the threshold can be chosen as (thw + tpp)/2. Following a uniﬁed scheme
of baseline removal and information restoration, a set of binary morphological op-
erators at different sizes can be used to extract the stroke width at each foreground
pixel [38], and thus remove the connected preprinted texts from the handwriting. An
example of the cleaning procedures and the corresponding intermediate results are
illustrated in Figure 2.7.
2.3
A SCALE-SPACE APPROACH FOR VISUAL DATA EXTRACTION
When we design a recognition engine, we may assume to have access to clearly
written material (printed or handwritten) on a homogenous background. This is not
the case in most real applications. The challenge increases when the image space is
in gray level and the material is handwritten, in the presence of a noisy and complex
background. In this section, we focus on extracting handwritten material from gray
level form images with slow-varying backgrounds. Having this in mind, our aim is
to extract handwritten data that contains characters of different sizes, with variable
distances and where their corresponding image intensity changes over a wide range of
scales. Usually these data are decorated with uniformly distributed graphical symbols
in their backgrounds. Examples can be found in mail pieces, bank checks, business
forms, and so on. When such a gray-scale image is processed, it is not simple for a
computer to distinguish the pertinent data from the background symbols. There is a
great amount of information loss, resulting in characters with the presence of noise and
unnecessary information. Visual shapes of handwriting characters are very important
in improving recognition [10, 27]; results will depend mostly on the quality of the
extracted characters. Therefore, it is necessary to perform a preprocessing procedure

22
TOOLS FOR IMAGE PREPROCESSING
to extract the pertinent information before applying the recognition algorithm. For this
purpose, various mathematical tools have been developed; multiscale representation
has received considerable interest in this ﬁeld because of its efﬁciency in describing
real-world structures [17]. It has been shown that the concept of scale is crucial
when describing the structure of images, which is needed for our application. It is
therefore important to focus on image regularization, as images need to respect the
Nyquist criterion when going through a smaller scale to larger one. Thus, the purpose
of this study is thus to introduce a new low-pass ﬁlter that is an alternative to the
approximation of the sample Gaussian ﬁlter.
2.3.1
Image Regularization
2.3.1.1
Linear Filters
Degradations in document images are principally due
to the acquisition system and other complex phenomena that will be highlighted
below. Hence, effective tools are necessary for their automatic treatment, such as the
compression, the segmentation, or, well again, the recognition of visual shapes. To
grasp the issues well, we propose to return to the fundamental deﬁnition of an image,
which may be explained as an intensity distribution of focused light on a plane. From
a numerical standpoint, this is a matrix of numbers. These digital images are made of
very irregular values for several reasons:
• Having visual objects at different scales, and their ﬁne details come to perturb
the objects of large sizes;
• The sensors record noisy data (electronic noise);
• The images are quantiﬁed (noise of quantiﬁcation);
• The compression of certain images adds undesirable effects that are more or less
visible;
• Information can be lost at the time of an eventual transmission;
• To these degradations, we add the one caused by the optical system (diffraction
inducing fuzziness, deformation, and chromatic aberration), and by the weak
movement of the acquisition system during the image acquisition.
For all of these reasons, the images are complex objects, and we are far from being
able to completely analyze the shapes that they contain completely in an automatic
manner. For better addressing this challenging problem, we rely on human perception
to contribute to sturdy solutions. We consider the observed image as composed of the
real signal and a white noise with unknown variance:
uobs(x, y) = ureal(x, y) + ησ(x, y).
Touseanapproachbasedonlinearﬁltering,itisnecessarytosupposethatthesignal
is stationary. Intuitively, we look for a low-pass ﬁlter to eliminate the variations in very
high frequencies. These ﬁlters are only mediocre for processing an image because
the latter is not stationary; they considerably degrade the object contours and return a

A SCALE-SPACE APPROACH FOR VISUAL DATA EXTRACTION
23
fuzzy image. But used within effective multiscale processes to offset this deﬁciency,
an accurate contour extraction is possible. One of the two linear ﬁlters that are used
the most is the Gaussian ﬁlter, as it has been proven to be the unique multiscale kernel
[16] and its resultant reduced complexity is due to the kernel separability.
2.3.1.2
The Wiener Filtering Approach
The Wiener ﬁltering approach [33]
consists of looking for a ﬁlter h that minimizes a cost function, which describes the
discrepancy between the original signal and the restored signal. For images, the cost
function should measure the visual degradation that is often difﬁcult to model. The
Euclidean distance does not model the visual degradations perfectly, but it remains
mathematically correct. A cost function that is equal to the square of the Euclidean
distance is calculated as the risk in average:
r = E{∥uobs × h −uobs∥2}.
The transfer function of this ﬁlter is
H(u, v) =

1 + pn(u, v)
pf(u, v)
−1
,
where pn(u, v) and pf(u, v) are, respectively, the spectrum of the power of the noise
and the image. Unfortunately, the difﬁculty of estimating the parameters of the noise
and the signal are limited to the application of this ﬁlter.
2.3.1.3
Multiscale Representation
The multiscale representation [7, 34]
furnishes a regularizing transformation from a ﬁne scale to a coarse scale. This
transformation can be seen as a simpliﬁcation of the image. The ﬁne structures
disappear in a monotonous manner with the increase of the scale. It is also important
that the structures not belonging to the ﬁne scales are not created in the coarse ones
(the kernel is said causal). Our goal in this chapter is to extract visual handwritten
data from noisy backgrounds. Considering the large variation of stroke width and
intensity in the handwritten data, a multiscale approach that preserves the data seems
therefore to be very appropriate.
2.3.1.4
Choice of the Regularizing Kernel
Meer et al. [23] suggested that
the regularizing ﬁlter should be the closest to an ideal low-pass ﬁlter. They proposed
to select a ﬁlter whose Fourier transform is close to an ideal low pass. An alter-
nate approach is to use a positive and unimodal kernel that has a bell shape in both
spatial and frequency domains. Thus, the high frequency components of the signal
are allocated weights that are inferior to the low frequency components. The kernel
we are seeking must be causal, isotropic (all the spatial points must be treated in
the same manner), and homogenous (the spatial dimensions and the scale dimension
must be treated in the same manner). From these kernel conditions, it was shown that

24
TOOLS FOR IMAGE PREPROCESSING
FIGURE 2.8
One-dimensional and two-dimensional proﬁles of a Gaussian kernel with
variance σ equal to 9 and a mask support of size 11σ.
multiscale representation of a 2D signal satisﬁes the heat equation:
⎧
⎪
⎨
⎪
⎩
∂u
∂t = 	u = div(∇u)

∥u(., t) −u0| −→
t→0 0,
of which the solution is the signal convolved with the Gaussian kernel [16]. The
Gaussian kernel possesses two very interesting properties:
• It has a semigroup property (the same properties of the group one except for the
nonexistence of the inverse);
• It minimizes the Heisenberg uncertainty criterion, that is, all time-frequency
atoms verify that σtσw ≥1/2 where σt is the temporal variance and σw is the
frequency variance. The one-dimensional and two-dimensional proﬁles of a
Gaussian kernel are shown in Figure 2.8. The Gaussian kernel is known to
preserve best the energy in the time-frequency resolution.
2.3.2
Data Extraction
2.3.2.1
An Algorithm Using the Gaussian Kernel
The Gaussian kernel can
be used to detect edges [22, 4, 11], which usually deﬁne the boundaries of objects,
and thus can be used for object segmentation in the presence of noise. Although the
intensity changes may occur over a large range of spatial scales in noisy gray level
images, a multiscale approach is used to achieve good quality data. The decision
criterion in segmenting data [9, 10] is the detection of convex parts of the smoothed
image Ii, at each scale level of resolution (ith step). A part of the smoothed image is
convex if the LoG × Ii−1 is positive. The scale-space parameter in this approach is
the standard deviation σ; the mask (kernel) size, Dim, is therefore deduced at each
step. The detailed algorithm is as follows.

A SCALE-SPACE APPROACH FOR VISUAL DATA EXTRACTION
25
FIGURE 2.9
Original images and their segmentation results using Gaussian kernels.
Procedure Multiresolution ( )
Step 0: Let I0 be a working space image; turn all its pixels on.
Step 1: Apply the L ◦G operator to the input image I with a scale parameter
σ1 = σmax that is sufﬁciently large to extract all the information.
Step 2: Threshold the output image by turning off pixels in I0 for which L ◦Gσ1 ×
I is negative to obtain I1. Let i be the iteration index; set i to 1.
Step 3: Turn on the neighbors of pixels in Ii (for continuity).
Step 4: Decrease the value of σ by a step 	σ.
Step 5: Apply the L ◦G operator to input image I with scale parameter σ, only on
the pixels being turned on in the previous step in Ii.
Step 6: Threshold the output image as in Step 2, to obtain Ii+1. The image Ii+1
constitutes the desired result at scale σ. Set i to i + 1.
Step 7: Repeat the process from Step 3 to Step 7 until σn = σmin .
Step 8: Obtain the desired ﬁne image If = In .
Some examples are illustrated in Figure 2.9. These ﬁgures show the inﬂuence of
the variance σ, and thus the mask size parameter (which is set to 11σ) on the shape
of the segmented images.
2.3.2.2
Practical Limitations of the Gaussian Kernel
The accuracy of
computation using Gaussian kernels depends on the mask size; wide masks give
precise computation at the cost of processing time, whereas small mask sizes

26
TOOLS FOR IMAGE PREPROCESSING
decrease the processing time, but the accuracy is sometimes severely compromised
and induces information loss, and this creates high frequencies due to the support
truncate of the kernel. The inaccuracy caused by kernel truncation and the prohibitive
processing time are the two fundamental practical limitations of the Gaussian kernel.
Some solutions are proposed in the literature to overcome these problems such as
the approximation of the Gaussian by recursive ﬁlters [14], or the use of truncated
exponential functions instead of the Gaussian kernel [17]. These solutions reduce the
processing time by approximating the Gaussian kernel, but the information loss still
persists and sometimes it gets worse. In order to reduce the mask size and to prevent
the information loss, Remaki and Cheriet proposed [27] a new family of kernels with
compact supports. They are called KCS (kernel with compact support), and they can
be used to generate scale-spaces in multiscale representations.
2.3.2.3
The KCS Filter
The KCS ργ is derived from the Gaussian kernel,
by composing the Gaussian G1/2γ variance 1/2γ and a C1-diffeomorphism h that
transforms the half plane to a unit ball. Let us deﬁne a function h as the following:
[0, 1] →
R+
r →h(r) =

1
1−r2 −1,
we note :
ργ(x, y) = G1/2γ ◦h

x2 + y2

=
⎧
⎨
⎩
1
Cγ
e

γ
x2+y2−1 +γ

if x2 + y2 < 1,
0
otherwise,
where Cγ is a normalizing constant. The multiscale family of KCS is deﬁned as
ρσ,γ(x, y) = 1
σ2 ρσ,γ
 x
σ , y
σ

=
⎧
⎨
⎩
1
Cγσ2 e

γσ2
x2+y2−1 +γ

if x2 + y2 < σ2,
0
otherwise.
In contrast to the Gaussian, the KCS possesses two scale parameters: σ controls the
support size and γ controls the width of the kernel mode. The support of the kernel is
2σ, which is signiﬁcantly smaller than the support of Gaussian, 11σ. The parameter
γ controls the distance between the zero-crossings of the KCS and the origin of the
axes. The selection of parameter γ does not affect the nature of the functions ρσ,γ;
they remain kernels with compact support. Furthermore, it is worth noting that if
γ ≥2 then the ﬁlter is causal (there is no creation of new structures when decreasing
the value of the σ parameter). Mostly, we set γ = 3 so that it only has to ﬁt a single
parameter. Figure 2.10 shows the one-dimensional and two-dimensional proﬁles of a
KCS kernel.

A SCALE-SPACE APPROACH FOR VISUAL DATA EXTRACTION
27
FIGURE 2.10
One-dimensional and two-dimensional proﬁles of a KCS kernel with param-
eters σ = 15 and γ = 5.
When building a smoothing kernel, it is important to check that it is a low-pass
ﬁlter as well; so that the low frequencies are retained whereas the high ones
are attenuated. When plotting the magnitude response of the truncated Gaussian
(Fig. 2.11), we ﬁrst observe that it is Gaussian-like, with side lobes. Actually, in the
time space, the truncated Gaussian is a Gaussian multiplied by an indicator function,
which corresponds to the convolution of a Gaussian with a cardinal sine, in the
frequency space. The ripples introduced with the sine mean that the kernel is not
decreasing while the frequencies increase. Thus, in the convolution, a frequency can
have a weight larger than a lower frequency, which is not desired. As we are looking
for a compact support kernel in the space, we will always have side lobes in the
response. It is therefore wished to use a kernel with low side lobes. When comparing
responses of the KCS and truncated Gaussian, we noticed that the KCS possesses
a strong reduction of the secondary lobe in comparison to the size of its temporal
support (Fig. 2.12). The secondary lobe of a truncated Gaussian kernel is located at
12DB, whereas one of the KCS is at 25DB. The KCS is therefore a better low-pass
ﬁlter. The width of the principal lobe can be tailored based on the parameter γ.
FIGURE 2.11
Truncated Gaussian kernel with the same variance and the same mask support
as the KCS kernel shown below and the logarithm of its magnitude response.

28
TOOLS FOR IMAGE PREPROCESSING
FIGURE 2.12
KCS kernel and the logarithm of its magnitude response.
2.3.2.4 Features of the KCS
The main purpose of constructing the KCS family
is to give an effective response to the two practical limitations of using the Gaussian
kernel. These limitations include the information loss and the high frequency creation
caused by the truncation, and the prohibitive processing time due to the wide mask
size (we do not use the separability of the Gaussian for the convolution with the
truncated kernel, as by construction it is not any more separable). The authors have
shown in [3rivieres] that the KCS kernel keeps the most important properties relative
to the image segmentation process of the Gaussian kernel, but obviously not all of
the properties that make the Gaussian kernel unique.
The properties of a signal convolved with the KCS are
• recovery of the initial signal when the scale parameter σ approaches zero. This
is a necessary condition to construct the scale space;
• continuity in comparison with the scale parameter;
• strong regularization;
• decreasing number of zero-crossings following each scale;
• for large values of γ, the uncertainty principle of Heisenberg is minimized.
Actually, we noticed that for γ = 3 the hup (Heisenberg uncertainty principal)
is not so far from its minimum value. By setting this parameter to 3, the ﬁlter
use is easier because only one parameter remains to be ﬁtted, depending on the
application. This property may be the most interesting; we showed that even if
the support of the KCS is compact, it is still well located in the frequencies. This
property is independent from the scale parameter γ.
2.3.2.5
Data Extraction Using KCS
The performance of the KCS is also
judged by a visual inspection using the image shape criteria, such as thickness, inten-
sity, and connectivity of strokes of the character shapes, with a weight given to each.
Some sample images and their segmented images are shown in Figure 2.13
As we can see, the KCS operators give respectable results with a high visual quality.
Additionally,theprocessingtimerequiredfortheLoKCS(LaplacianofKCS)operator

A SCALE-SPACE APPROACH FOR VISUAL DATA EXTRACTION
29
FIGURE 2.13
Original form images and their segmentation results using a KCS.
is drastically less than that required by the LoG (Laplacian of Gaussian) operator.
Thanks to their compact support property, the KCS kernels give an effective solution
to the two practical limitation problems when using the Gaussian kernel, namely the
information loss caused by the truncation and the prohibitive processing time due
to the wide mask size. A practical comparison between the LoG and the LoKCS
operators has been widely developed by the authors in Ref. [2].
2.3.3
Concluding Remarks
In the previous section we have shown the derivation of a new family of kernels
(KCS) to generate scale space in multiscale representation. We have appreciated the
nice features of the KCS and its effectiveness in preventing information loss and
reducing the processing time, as compared to the Gaussian kernel. The question one
may ask now is, how can we further increase the performance of the KCS? With
this aim in mind, we propose a new separable version of the KCS, denoted as SKCS
(separable kernel with compact support) in order to perform the KCS and to further
minimize the processing time. We leave it up to the reader to consult our publication
[2, 3] for more information. In order to create the SKCS, we had to resort to some
properties of the Gaussian (semigroup and separability). Other kernels can thus be
created by giving a greater place to other properties, but the Heisenberg criterion must
still be kept in mind.

30
TOOLS FOR IMAGE PREPROCESSING
2.4
DATA PREPROCESSING
The conversion of paper-based documents to electronic image format is an important
process in computer systems for automated document delivery, document preserva-
tion, and other applications. The process of document conversion includes scanning,
displaying, quality assurance, image processing, and text recognition. After docu-
ment scanning, a sequence of data preprocessing operations are normally applied
to the images of the documents in order to put them in a suitable format ready for
feature extraction. In this section, all possible required preprocessing operations are
described. Conventional preprocessing steps include noise removal/smoothing [13],
document skew detection/correction [15, 41], connected component analysis, normal-
ization [34, 17–19], slant detection/correction [36], thinning [40, 42, 43], and contour
analysis [44].
2.4.1
Smoothing and Noise Removal
Smoothing operations in gray level document images are used for blurring and for
noise reduction. Blurring is used in preprocessing steps such as removal of small
details from an image. In binary (black and white) document images, smoothing
operations are used to reduce the noise or to straighten the edges of the characters, for
example, to ﬁll the small gaps or to remove the small bumps in the edges (contours)
of the characters. Smoothing and noise removal can be done by ﬁltering. Filtering is a
neighborhood operation, in which the value of any given pixel in the output image is
determined by applying some algorithm to the values of the pixels in the neighborhood
of the corresponding input pixel. There are two types of ﬁltering approaches: linear
and nonlinear. Here we look at some simple linear ﬁltering approaches in which the
value of an output pixel is a linear combination of the values of the pixels in the
input pixel’s neighborhood. A pixel’s neighborhood is a set of pixels, deﬁned by their
locations relative to that pixel. In Figure 2.14(a) and (b), two examples of averaging
mask ﬁlters are shown.
Each of these ﬁlters can remove the small pieces of the noise (salt and pepper
noise) in the gray level images; also they can blur the images in order to remove the
unwanted details. Normally, these averaging ﬁlter masks must be applied to the image
a predeﬁned number of times, otherwise all the important features (such as details and
FIGURE 2.14
Two 3 by 3 smoothing (averaging) ﬁlter masks. The sum of all the weights
(coefﬁcients) in each mask is equal to one.

DATA PREPROCESSING
31
FIGURE 2.15
Examples of ﬁlter masks (3 by 3) that are used for smoothing of binary
document images, (b), (c), and (d) are rotated versions of the ﬁlter in (a) by 90◦.
edges, etc.) in the image will be removed completely. For smoothing binary document
images, ﬁlters shown in Figure 2.15, can be used to smooth the edges and remove the
small pieces of noise.
These masks are passed over the entire image to smooth it, and this process can
be repeated until there is no change in the image. These masks begin scanning in the
lower right corner of the image and process each row moving upward row by row. The
pixel in the center of the mask is the target. Pixels overlaid by any square marked “X”
are ignored. If the pixels overlaid by the squares marked “=” all have the same value,
that is, all zeros, or all ones, then the target pixel is forced to match them to have the
same value, otherwise it is not changed. These masks can ﬁll or remove single pixel
indentation in all the edges, or single bumps. Also all the single pixel noises (salt and
pepper), or lines that are one pixel wide will be completely eroded. See Figure 2.16
for some examples of binary smoothed digits.
FIGURE 2.16
(a), (b), and (c) Original images; (d), (e), and (f) Smoothed images,
respectively.

32
TOOLS FOR IMAGE PREPROCESSING
FIGURE 2.17
Deviation of the baseline of the text from horizontal direction is called skew.
(a) A skewed handwritten word; (b) skew corrected handwritten word in (a); (c) a skewed
typewritten text; (d) skew corrected image in (c).
2.4.2
Skew Detection and Correction
During the document scanning process, the whole document or a portion of it can
be fed through the loose-leaf page scanner. Some pages may not be fed straight
into the scanner, however, causing skewing of the bitmapped images of these pages.
So, document skew often occurs during document scanning or copying. This effect
visually appears as a slope of the text lines with respect to the x-axis, and it mainly
concerns the orientation of the text lines; see some examples of document skew in
Figure 2.17
Without skew, lines of the document are horizontal or vertical, depending on the
language. Skew can even be intentionally designed to emphasize important details in
a document. However, this effect is unintentional in many real cases, and it should be
eliminated because it dramatically reduces the accuracy of the subsequent processes,
such as page segmentation/classiﬁcation and optical character recognition (OCR).
Therefore, skew detection is one of the primary tasks to be solved in document image
analysis systems, and it is necessary for aligning a document image before further
processing. Normally subsequent operations show better performance if the text lines
are aligned to the coordinate axes. Actually the algorithms for layout analysis and
character recognition are generally very sensitive to page skew, so skew detection
and correction in document images are the critical steps before layout analysis. In an
attempt to partially automate the document processing systems as well as to improve
the text recognition process, document skew angle detection and correction algorithms
can be used.
2.4.2.1
Skew Detection
There are several commonly used methods for detect-
ingskewinapage.Somemethodsrelyondetectingconnectedcomponents(connected

DATA PREPROCESSING
33
FIGURE 2.18
Sample image and its projection parallel to the text lines.
components or CCs are roughly equivalent to characters) and ﬁnding the average an-
gles connecting their centroids, others use projection proﬁle analysis. Methods based
on projection proﬁles are straightforward. In these methods, a document page is pro-
jected at several angles, and the variances in the number of black pixels per projected
line, are determined. The projection parallel to the correct alignment of the lines will
likely have the maximum variance, as in parallel projection, each given ray projected
through the image will hit either almost no black pixels (as it passes between text
lines) or many black pixels (while passing through many characters in sequence). An
example of parallel projection is shown in Figure 2.18
Oblique projections that are not parallel to the correct alignment of the text lines,
will normally pass through lines of the text, and spaces between the lines. Thus those
variance in the number of pixels hit by the individual rays will be smaller than in the
parallel case. An example is shown in Figure 2.19
2.4.2.2
Skew Correction
After the skew angle of the page has been detected,
the page must be rotated in order to correct this skew. A rotation algorithm has to be
both fairly fast and fairly accurate. A coordinate rotation transformation in Eq. 2.20
can be used in order to correct the skew of the document image. Given a point, its
new coordinates after rotating the entire image around its origin by angle (which is
FIGURE 2.19
A sample image and an oblique projection. Notice the uniform nature of the
distribution.

34
TOOLS FOR IMAGE PREPROCESSING
FIGURE 2.20
A rotation transformation can correct the skew of the document image.
the detected skew angle of the document image) can be satisﬁed by Eq. 2.20:

x′
y′

=

cos(θ)
sin(θ)
−sin(θ)
cos(θ)
 
x
y

.
(2.20)
An example of skew correction is shown in Figure 2.20
2.4.3
Slant Correction
The character inclination that is normally found in cursive writing is called slant.
Figure 2.21 shows some samples of slanted handwritten numeral string. Slant cor-
rection is an important step in the preprocessing stage of both handwritten words
and numeral strings recognition. The general purpose of slant correction is to reduce
the variation of the script and speciﬁcally to improve the quality of the segmentation
candidates of the words or numerals in a string, which in turn can yield a higher recog-
nition accuracy. Here, a simple and efﬁcient method of slant correction of isolated
handwritten characters or numerals is presented.
FIGURE 2.21
Images of handwritten words and handwritten numeral strings, selected from
USPS-CEDAR CDROM1 database.

DATA PREPROCESSING
35
FIGURE 2.22
(a) A digit circumscribed by a tilted rectangle; (b) slant angle estimation for
the digit in (a).
2.4.3.1
Slant Angle Estimation
A character or numeral image is circum-
scribed by a tilted rectangle, as shown in Figure 2.22 In other words, each character
or digit is surrounded by the following four lines:
y = x + β1,
(2.21)
y = x + β2,
(2.22)
y = −x + β3,
(2.23)
y = −x + β4,
(2.24)
where β2 > β1 and β4 > β3, respectively.
As seen in Figure 2.22(b), the slant angle can be calculated as θ = arctan(B/A).
These parameters are shown in Figure 2.22(a), where A is the height of the char-
acter/digit and B = (β4 + β1 −β3 −β2)/2. Corresponding to the orientation of the
slant to the left or right, θ can have positive or negative values, respectively.
2.4.3.2
Slant Correction
After estimation of the slant angle (θ), a horizontal
shear transform is applied to all the pixels of the image of the character/digit in order to
shift them to the left or to the right (depending on the sign of the θ). The transformation

36
TOOLS FOR IMAGE PREPROCESSING
FIGURE 2.23
(a) Original digits; (b) slant corrected digits.
expressions are given below:
x′ = x −y · tan(θ),
y′ = y.
(2.25)
In these equations, x and y are original horizontal/vertical coordinates of the pixels
in the image; x′ and y′ are corresponding transformed coordinates, respectively. An
example of slant corrected digits is shown in Figure 2.23
2.4.4
Character Normalization
Character normalization is considered to be the most important preprocessing opera-
tion for character recognition. Normally, the character image is mapped onto a stan-
dard plane (with predeﬁned size) so as to give a representation of ﬁxed dimensionality
for classiﬁcation. The goal for character normalization is to reduce the within-class
variation of the shapes of the characters/digits in order to facilitate feature extraction
process and also improve their classiﬁcation accuracy. Basically, there are two differ-
ent approaches for character normalization: linear methods and nonlinear methods.
In this section, we introduce some representative linear and nonlinear normalization
methods, which are very effective in character recognition.
As seen in Figure 2.24, we denote the width and height of the original character by
W1 and H1, the width and height of the normalized character by W2 and H2, and the
size of the standard (normalized) plane by L. The standard plane is usually considered
as a square and its size is typically 32 × 32 or 64 × 64, among others. We deﬁne the
aspect ratios of the original character (R1) and the normalized character (R2) as
R1 = min(W1, H1)
max(W1, H1),
(2.26)
and
R2 = min(W2, H2)
max(W2, H2),
(2.27)
which are always considered in the range of [0, 1].

DATA PREPROCESSING
37
FIGURE 2.24
(a) Original character; (b) normalized character ﬁlled in standard (normalized)
plane.
In the so-called aspect ratio adaptive normalization (ARAN) strategy [19], the
aspect ratio of the normalized character (R2) is adaptively calculated based on that
of the original character (R1) using a mapping function such as those in Table 2.1 In
the implementation of ARAN, the normalized character image is ﬁlled into a plane of
ﬂexible size (W2, H2), and then this ﬂexible plane is shifted to overlap the standard
plane by aligning the center (shown in Fig. 2.24). If the normalized image ﬁlls one
dimension of the standard plane, that is, the dimension of arg max(W1, H1), then L
is considered as equal to max(W2, H2) and the other dimension is centered in the
standard plane. Having R2 and L (= max(W2, H2)), we can compute min(W2, H2)
from Eq. (2.27). Thus, the size of the normalized character (W2, H2) can be obtained.
The transformation of the coordinates from the original character plane to normal-
ized plane can be accomplished by forward mapping or backward mapping. If we
denote the original image and the normalized image as f(x, y) and g(x′, y′), respec-
tively, the normalized image is generated by g(x′, y′) = f(x, y) based on coordinate
mapping. The forward mapping and backward mapping are given by
x′ = x′(x, y),
y′ = y′(x, y),
(2.28)
and
x = x(x′, y′),
y = y(x′, y′),
(2.29)
respectively.
TABLE 2.1
Functions for aspect ratio mapping.
Method
Function
Fixed aspect ratio
R2 = 1
Aspect ratio preserved
R2 = R1
Square root of aspect ratio
R2 = √R1
Cubic root of aspect ratio
R2 =
3√R1
Sine of aspect ratio
R2 = 
sin( π
2 R1)

38
TOOLS FOR IMAGE PREPROCESSING
TABLE 2.2
Coordinate mapping functions of normalization methods.
Method
Forward mapping
Backward mapping
Linear
x′ = αx
x = x′/α
y′ = βy
y = y′/β
Moment
x′ = α(x −xc) + x′
c
x = (x′ −x′
c)/α + xc
y′ = β(y −yc) + y′
c
y = (y′ −y′
c)/β + yc
Nonlinear
x′ = W2hx(x)
x = h−1
x (x′/W2)
y′ = H2hy(y)
y = h−1
y (y′/H2)
In forward mapping, x and y are discrete, but x′(x, y) and y′(x, y) are not nec-
essarily discrete; whereas in backward mapping, x′ and y′ are discrete, but x(x′, y′)
and y(x′, y′) are not necessarily discrete. Further, in forward mapping, the mapped
coordinates (x′, y′) do not necessarily ﬁll all pixels in the normalized plane. There-
fore, coordinate discretization or pixel interpolation is needed in the implementa-
tion of normalization. By discretization, the mapped coordinates (x′, y′) or (x, y)
are approximated by the closest integer numbers, ([x′], [y′]) or ([x], [y]). In the dis-
cretization of forward mapping, the discrete coordinates (x, y) scan the pixels of the
original image and the pixel value f(x, y) is assigned to all the pixels ranged from
([x′(x, y)], [y′(x, y)]) to ([x′(x + 1, y + 1)], [y′(x + 1, y + 1)]).
Backward mapping is often adopted because the discretization of mapped coordi-
nates (x, y) is trivial. However, for some normalization methods, the inverse coordi-
nate functions x(x′, y′) and y(x′, y′) cannot be expressed explicitly.
In the following, we give the coordinate functions of three popular normalization
methods: linear normalization, moment normalization [8], and nonlinear normaliza-
tion based on line density equalization [31, 35]. The forward and backward mapping
functions are tabulated in Table 2.2 In this table, α and β denote the ratios of scaling,
given by
α = W2/W1,
β = H2/H1.
(2.30)
For linear normalization and nonlinear normalization, W1 and H1 are the horizontal
span and vertical span of strokes of the original character (size of minimum bounding
box). For moment normalization, the character is rebounded according to the centroid
and second-order moments.
2.4.4.1
Moment-Based Normalization
Moment normalization cannot only
align the centroid of character image but also correct the slant or rotation of character
[8]. In the following, we ﬁrst address moment-based size normalization (referred to as
moment normalization in Table 2.2) and then turn to moment-based slant correction.
In moment normalization, the centroid and size of the normalized image are deter-
mined by moments. Let (xc, yc) denotes the center of gravity (centroid) of the original

DATA PREPROCESSING
39
character, given by
xc = m10/m00,
yc = m01/m00,
(2.31)
where mpq denotes the geometric moments:
mpq =

x

y
xpyqf(x, y).
(2.32)
(x′
c, y′
c) denotes the geometric center of the normalized plane, given by
x′
c = W2/2,
y′
c = H2/2.
(2.33)
Inmomentnormalization,theoriginalimageisviewedtobecenteredatthecentroid
and its boundaries are reset to [xc −δx/2, xc + δx/2] and [yc −δy/2, yc + δy/2]. The
reset dimensions W1 = δx and H1 = δy are calculated from moments:
δx = a√μ20/m00,
δy = a√μ02/m00,
(2.34)
where μpq denotes the central moments:
μpq =

x

y
(x −xc)p(y −yc)qf(x, y),
(2.35)
and the coefﬁcient a can be empirically set to be around 4. The original image plane
is expanded or trimmed so as to ﬁt the reset boundaries. The aspect ratio R1 of the
original image is then calculated from W1 and H1, and by using one of the formulas
in Table 2.1 the aspect ratio R2 of the normalized character is calculated. In turn, the
width W2 and height H2 of normalized character are determined from R2 and L.
On obtaining the centroid of original character, the width and height of original
character and normalized image, the normalization of character image is performed
by pixel mapping according to forward or backward coordinate mapping functions
as in Table 2.2 After pixel mapping, the centroid of normalized character is aligned
with the center of the normalized plane.
Size normalization can be preceded by a slant correction procedure as described
in Section 2.4.3. The slant angle can also be estimated from moments. If preserving
the centroid of character image in slant correction, the coordinate functions are given
by
x′ = x −(y −yc) tan θ,
y′ = y.
(2.36)

40
TOOLS FOR IMAGE PREPROCESSING
The aim is to eliminate the covariance of pixels (second-order central moment) after
slant correction:
μ′
11 =

x

y
[x −xc −(y −yc) tan θ](y −yc)f(x, y) = 0,
(2.37)
which results in
tan θ = −μ11/μ02.
(2.38)
2.4.4.2
Nonlinear Normalization
Nonlinear normalization is efﬁcient to
correct the nonuniform stroke density of character images and has shown superi-
ority in handwritten Japanese and Chinese character recognition. The basic idea of
nonlinear normalization is to equalize the projections of horizontal and vertical line
density functions. Denote the horizontal and vertical density functions of character
image f(x, y) by dx(x, y) and dy(x, y), respectively, the (normalized to unit sum)
projections on horizontal axis and vertical axis, px(x) and py(y), are calculated by
px(x) =

y dx(x, y)

x

y dx(x, y),
(2.39)
py(y) =

x dy(x, y)

x

y dy(x, y).
(2.40)
The normalized projections (also viewed as histograms) are accumulated to generate
normalized coordinate functions (ranged in [0,1]):
hx(x) = x
u=0 px(u),
hy(y) = y
v=0 py(v).
(2.41)
The coordinates of normalized character image are then obtained by multiplying
the normalized coordinate functions with W2 and H2, respectively, as shown in
Table 2.2 After this coordinate transformation, the projections of line density of nor-
malized image are approximately uniform.
The local line density dx(x, y) and dy(x, y) can be calculated in various ways,
among which are the popular ones of Tsukumo and Tanaka [31] and Yamada et
al. [35]. Both methods perform superiorly but the one of Tsukumo and Tanaka is
simpler in implementation. According to Tsukumo and Tanaka, dx(x, y) is taken as
the reciprocal of horizontal run-length of background. If the pixel (x, y) is in stroke
region, the density is assumed to be a (empirical) constant. For background runs at
left/right margins or blank rows, the density can be empirically set to 1/(rl + W1),
where rl is the run-length within the minimum bounding box of character. dy(x, y)
is similarly calculated from vertical runs. On the contrary, Yamada et al. deﬁne the
density in all regions of character rigorously and force dx(x, y) and dy(x, y) to be
equal.

DATA PREPROCESSING
41
FIGURE 2.25
Character image, horizontal and vertical density projections, and coordinate
functions.
Figure 2.25 shows an example of nonlinear normalization by line density equal-
ization. The horizontal and vertical density projection proﬁles are shown to the above
and the right of the character, and the normalized coordinate functions are shown to
the right of the density projection proﬁles.
The forward coordinate mapping functions of line-density-based nonlinear nor-
malization are not smooth, and their inverse functions cannot be expressed explicitly.
Fitting smooth coordinate functions with intensity projection proﬁles can yield com-
parable recognition performance as line-density-based nonlinear normalization [18].
Further, the above one-dimensional normalization methods, with coordinate mapping
functions like x′(x) and y′(y), can be extended to pseudo-two-dimensional for higher
recognition performance [20].
An example of normalization is shown in Figure 2.26 where a digit “9” is trans-
formed by linear normalization, line-density-based nonlinear normalization, and mo-
ment normalization, with and without slant correction.
2.4.5
Contour Tracing/Analysis
Contour tracing is also known as border following or boundary following. Contour
tracing is a technique that is applied to digital images in order to extract the boundary
of an object or a pattern such as a character. The boundary of a given pattern P is
FIGURE 2.26
From left: original image, linear normalization, nonlinear normalization,
moment normalization; upper/lower: without/with slant correction.

42
TOOLS FOR IMAGE PREPROCESSING
FIGURE 2.27
(a) 4-neighbors (4-connected); (b) 8-neighbors (8-connected).
deﬁned as the set of border pixels of P. In digital images, each pixel of the image is
considered as a square. As a result, there are two kinds of border (boundary) pixels
for each pattern: 4-border pixels and 8-border pixels. Before describing these two
types of borders in patterns, we deﬁne two types of connectivity or neighborhoods in
digital images: 4-connectivity and 8-connectivity.
2.4.5.1
Deﬁning Connectivity
A pixel, Q, is a 4-neighbor or 4-connected of
a given pixel, P, if both Q and P share an edge. The 4-neighbors of pixel P (namely
pixels P2, P4, P6, and P8) are shown in Figure 2.27(a) below. A pixel, Q, is an 8-
neighbor or 8-connected of a given pixel, P, if both Q and P share a vertex or edge
with that pixel. These pixels are namely P1, P2, P3, P4, P5, P6, P7, and P8 shown in
Figure 2.27(b).
2.4.5.2
Deﬁning 4-Connected Component
A set of black pixels, P, is a
4-connected component object if for every pair of pixels Pi and Pj in P, there exists
a sequence of pixels Pi, . . . , Pj such that
(a) All pixels in the sequence are in the set P and
(b) Every 2 pixels of the sequence that are adjacent in the sequence are 4-neighbors
or 4-connected.
Two examples of 4-connected patterns are shown in Figure 2.28
2.4.5.3
Deﬁning 8-Connected Component
A set of black pixels denoted by
P, is an 8-connected component (or simply a 8-connected component) if for every
FIGURE 2.28
Two examples of 4-connected component objects.

DATA PREPROCESSING
43
FIGURE 2.29
(a) An 8-connected component pattern; (b) a pattern that is not 4- or 8-
connected.
pair of pixels Pi and Pj in P, there exists a sequence of pixels Pi, . . . , Pj such that
(a) All pixels in the sequence are in the set P and
(b) Every 2 pixels of the sequence that are adjacent are 8-connected.
One example of 8-connected pattern is shown in Figure 2.29(a). The pattern in
Figure 2.29(b) is neither 4-connected nor 8-connected.
Now we can deﬁne 4-connected and 8-connected borders (contour) of patterns.
A black pixel is considered as a 4-border pixel if it shares an edge with at least one
white pixel. On the contrary, a black pixel is considered an 8-border pixel if it shares
an edge or a vertex with at least one white pixel. (Note a 4-border pixel is also an
8-border pixel, however an 8-border pixel may or may not be a 4-border pixel).
2.4.5.4
Contour-Tracing Algorithms
Here, we describe a simple contour-
tracing algorithm, namely square-tracing algorithm, which is easy to implement and,
therefore, it is used frequently to trace the contour of characters. This algorithm will
ignore any “holes” present in the pattern. For example, if a pattern is given like
that of Figure 2.30(a), the contour traced by the algorithms will be similar to the
one shown in Figure 2.30(b) (the black pixels represent the contour). This could be
acceptable in some applications but not in all applications. Sometimes we want to
trace the interior of the pattern as well in order to capture any holes that identify a
certain character. Figure 2.30(c) shows the “complete” contour of the pattern. A “hole-
FIGURE 2.30
(a) Original pattern in black and white format; (b) outer contour of the pattern;
(c) both outer and inner contours of the pattern.

44
TOOLS FOR IMAGE PREPROCESSING
FIGURE 2.31
(a) Starting point for contour tracing; (b) contour-tracing procedure.
searching” algorithm should be used to ﬁrst extract the holes in a given pattern and
then apply a contour-tracing algorithm on each hole in order to extract the complete
contour. We can also reverse the original pattern (black pixels change to white, and
white pixels change to black) and apply the contour-tracing algorithm to ﬁnd the
contour of the inner holes.
2.4.5.5
Square-Tracing Algorithm
The idea behind the square-tracing algo-
rithm is very simple. It can be described as follows: given a digital pattern, that is, a
group of black pixels on a white background, such as the one shown in Figure 2.31,
one can locate a black pixel on the image and declare it as the “starting” pixel. This
can be done in a number of ways; for example, one can start at the bottom left corner
of the grid and scan each column of pixels from the bottom going upward starting
from the leftmost column and proceeding to the right until a black pixel is encoun-
tered. One will declare that pixel as “starting” pixel. Now, imagine that you are a bug
standing on the starting pixel as in Figure 2.31 below. In order to extract the contour
of the pattern, you have to do the following: every time you ﬁnd yourself hitting on a
black pixel, turn left, and every time you ﬁnd yourself standing on a white pixel, turn
right, until you encounter the starting pixel again. The black pixels you walked over
will be the contour of the pattern.
The important thing in the square-tracing algorithm is the “sense of direction.”
The left and right turns you make are with respect to your current positioning, which
depends on your direction of entry to the pixel you are standing on. Therefore, it is
important to keep track of your current orientation in order to make the right moves.
The following is a formal description of the square-tracing algorithm:
Input: An image I containing a connected component P of black pixels.
Output: A sequence B(b1, b2, . . . , bk) of boundary pixels, that is, the outer con-
tour.
Begin
• Set B to be empty;
• From bottom to top and left to right scan the cells of I until a black pixel, S,
of P is found;

DATA PREPROCESSING
45
• Insert S in B;
• Set the current pixel, P, to be the starting pixel, S;
• Turn left, that is, visit the left adjacent pixel of P;
• Update P, that is, set it to be the current pixel;
• While P not equal to S do
• If the current pixel P is black;
• Insert P in B and turn left (visit the left adjacent pixel of P),
• Update P, that is, set it to be the current pixel,
• Else
• Turn right (visit the right adjacent pixel of P),
• Update P, that is, set it to be the current pixel.
End
2.4.5.6
Contours for Feature Extraction
Contour tracing is one of many pre-
processing techniques performed on character images in order to extract important
information about their general shape. Once the contour of a given character or pattern
is extracted, its different characteristics will be examined and used as features that
will be used later on in pattern classiﬁcation. Therefore, correct extraction of the
contour will produce more accurate features that will increase the chances of correctly
classifying a given character or pattern. But one might ask why ﬁrst extract the contour
of a pattern and then collect its features? Why not collect features directly from the
pattern? One answer is, the contour pixels are generally a small subset of the total
number of pixels representing a pattern. Therefore, the amount of computation is
greatly reduced when we run feature extracting algorithms on the contour instead
of the whole pattern. Because the contour shares a lot of features with the original
pattern, but has fewer pixels, the feature extraction process becomes much more
efﬁcient when performed on the contour rather on the original pattern. In conclusion,
contour tracing is often a major contributor to the efﬁciency of the feature extraction
process, which is an essential process in pattern recognition.
2.4.6
Thinning
The notion of skeleton was introduced by Blum as a result of the medial axis transform
(MAT) or symmetry axis transform (SAT). The MAT determines the closest boundary
point(s) for each point in an object. An inner point belongs to the skeleton if it has at
least two closest boundary points. A very illustrative deﬁnition of the skeleton is given
by the prairie-ﬁre analogy: the boundary of an object is set on ﬁre and the skeleton
is the loci where the ﬁre fronts meet and quench each other. The third approach
provides a formal deﬁnition: the skeleton is the locus of the center of all maximal
inscribed hyperspheres (i.e., discs and balls in 2D and 3D, respectively). An inscribed
hypersphere is maximal if it is not covered by any other inscribed hypersphere. See
Figure 2.32 for an example of skeletonization.

46
TOOLS FOR IMAGE PREPROCESSING
FIGURE 2.32
Finding the skeleton of a rectangular object: (a) The centers of all the maximal
circles form the skeleton of the object, points A, B are on the skeleton, but C does not belong
to the skeleton; (b) the resulting skeleton of the rectangular object.
There are many approaches to ﬁnd the skeleton of an object. One approach is
thinning. Thinning is the process of peeling off a pattern as many pixels as possible
without affecting the general shape of the pattern. In other words, after pixels have
been peeled off, the pattern can still be recognized. Hence, the skeleton obtained must
have the following properties:
• Must be as thin as possible;
• Connected;
• Centered.
When these properties are satisﬁed, the algorithm must stop. As two examples,
in Figures 2.33 and 2.34, a character pattern and a text line and their skeletons are
shown, respectively.
Thinning Algorithms: Many algorithms have been designed for skeletonization of
digital patterns. Here we describe two of those algorithms that can be used for skele-
tonization of binary patterns or regions in digital images: ﬁrst, Hilditch’s, and second,
Zhang-Suen thinning algorithms. These algorithms are very easy to implement. In
applying these algorithms, we assume that background region pixels (white pixels)
have the value “0,” and the object regions (black pixels) have the value “1.”
FIGURE 2.33
(a) Original pattern; (b) skeleton as a result of thinning.

DATA PREPROCESSING
47
FIGURE 2.34
(a) Original text; (b) skeleton of the text line in (a).
2.4.6.1
Hilditch’s
Thinning
Algorithms
Consider
the
following
8-
neighborhood of a pixel P1, in Figure 2.35 To determine whether to peel off P1
or keep it as part of the resulting skeleton, we arrange the eight neighbors of P1 in a
clockwise order and deﬁne the two functions A(P1) and B(P1) as follows:
• A(P1) = number of 0, 1 patterns (transitions from 0 to 1) in the ordered sequence
of P2, P3, P4, P5, P6, P7, P8, P9, P2.
• B(P1) = P2 + P3 + P4 + P5 + P6 + P7 + P8 + P9 (number of black or 1
pixel, neighbors of P1).
As mentioned above, in the deﬁnition of these functions black is considered equal
to “1,” and white is considered equal to “0,” two examples of computations of these
functions are shown in Figure 2.36
Hilditch’s algorithm consists of performing multiple passes on the pattern, and on
each pass the algorithm checks all the pixels and decides to change a pixel from black
to white if it satisﬁes the following four conditions:
2 <= B(P1) <= 6,
A(P1) = 1,
P2 · P4 · P8 = 0 or A(P2) ̸= 1,
P2 · P4 · P6 = 0 or A(P4) ̸= 1.
(2.42)
The algorithm stops when changes stop (no more black pixels can be removed). Now
we review each of the above conditions in detail.
FIGURE 2.35
A 3 by 3 window that shows the pixels around P1 and its neighboring pixels.

48
TOOLS FOR IMAGE PREPROCESSING
FIGURE 2.36
Two examples of computations of functions of A(P1) and B(P1): in (a),
A(P1) = 1, B(P1) = 2, and in (b) A(P1) = 2, B(P1) = 2.
• Condition 1: 2 <= B(P1) <= 6. This condition combines two subconditions,
ﬁrst that the number of nonzero neighbors of P1 is greater than or equal to 2
and second that it should be less than or equal to 6. The ﬁrst condition ensures
that no endpoint pixel and no isolated one be deleted (any pixel with 1 black
neighbor is an endpoint pixel), the second condition ensures that the pixel is a
boundary pixel.
• Condition 2: A(P1) = 1. This is a connectivity test. For example, if you consider
Figure 2.37, where A(P1) > 1, you can see that by changing P1 to white the
pattern will become disconnected.
 Condition 3: P2 · P4 · P8 = 0 or A(P2) ̸= 1. This condition ensures that 2-pixel
wide vertical lines do not get completely eroded by the algorithm. An example
for these conditions is shown in Figure 2.38
 Condition 4: P2 · P4 · P6 = 0 or A(P4) ̸= 1. This condition ensures that 2-pixel
wide horizontal lines do not get completely eroded by the algorithm. An example
for these conditions is shown in Figure 2.39
2.4.6.2
Zhang-Suen Thinning Algorithm
This algorithm has two steps,
which will be successively applied to the image. In each step contour points of the
region that can be deleted are identiﬁed. Contour points are deﬁned as points that
have value “1,” and they have at least one 8-neighbor pixel value equal to “0.” Step 1
of the algorithm ﬂags a contour point P1 for deletion if the following conditions are
satisﬁed:
FIGURE 2.37
In examples shown in this ﬁgure, condition A(P1) = 1 is violated, so by
removing P1 the corresponding patterns will become disconnected.

DATA PREPROCESSING
49
FIGURE 2.38
(a) An example where A(P2) ̸= 1; (b) an example where P2 · P4 · P8 = 0;
(c) an example where P2 · P4 · P8 ̸= 0 and A(P2) = 1.
Condition 1: 2 <= B(P1) <= 6;
Condition 2: A(P1) = 1;
Condition 3: P2 · P4 · P6 = 0;
Condition 4: P4 · P6 · P8 = 0.
Step 2 of the algorithm ﬂags a contour point P1 for deletion if the following
conditions are satisﬁed:
Condition 1: 2 <= B(P1) <= 6 (the same condition as step 1);
Condition 2: A(P1) = 1 (the same condition as step 1);
Condition 3: P2 · P4 · P8 = 0;
Condition 4: P2 · P6 · P8 = 0.
In this algorithm, Step 1 is applied to all the pixels of the image. If all the conditions
of this step are satisﬁed, the point will be ﬂagged to be removed. However, the deletion
of the ﬂagged points will be delayed until all the pixels of the image have been visited.
This delay avoids changing of pixels of the image during each step. After Step 1 has
been applied to all pixels of the image, the ﬂagged pixels will be removed from the
image. Then Step 2 is applied to the image exactly the same as Step 1. Applying
Steps 1 and 2, one after the other is continued iteratively, until no further changes
occur in the image. The resulting image will be the skeleton. In Figure 2.40 some
results of skeletonization of the characters by these two algorithms are displayed.
As these ﬁgure show there are minor differences between the results of these two
algorithms.
FIGURE 2.39
(a) An example where A(P4) ̸= 1; (b) an example where P2 · P4 · P6 = 0;
(c) an example where P2 · P4 · P6 ̸= 0 and A(P4) = 1.

50
TOOLS FOR IMAGE PREPROCESSING
FIGURE 2.40
(a) and (d) Original images; (b) and (e) skeletons by Hilditch algorithm;
(c) and (f) skeletons by Zhang-Suen algorithm.
In general, all thinning algorithms can be summarized as follows:
Repeat
Change removable black pixels to white.
Until no pixels are removed.
2.4.6.3
Feature Extraction from Skeleton
A skeleton shows the general
shape of a pattern, and some important features can be extracted from it, such as
intersection or branching points, number of strokes, their relative position, or the
situation of their connections. Skeleton is also useful when we are interested in the
relative positions of the strokes in the pattern but not in the size of the pattern itself.
2.5
CHAPTER SUMMARY
The purpose of the ﬁrst important step, image, and data preprocessing, of a character
recognition system is to prepare subimages with optimal quality so that the feature
extraction step can work correctly and efﬁciently. This chapter covers most widely
used preprocessing techniques and provides a global-to-local view of how information
with different perspectives should be used in extracting the most pertinent features
from characters to be recognized.
Anintelligentform-processingsystemisintroducedforextractingitemsofinterest.
Depending on the complexity of the background images, data extraction is achieved
by thresholding based on global or local thresholding, on gray level or topologi-
cal feature. A time critical application may use a balanced combination of different

REFERENCES
51
approaches. When the scale of the data to be extracted is unknown, a scale-space-
based approach can enhance the signal-to-noise ratio and extract information with
most efﬁciency. After the characters are separated from its background, defects such
as skewed text lines and slanted characters are detected and corrected. Finally, prepro-
cessing techniques such as contour tracking and skeleton extraction can substantially
reduce the redundancy in the information to be processed by the ensuing feature ex-
traction steps. At this stage, the information to be processed further has been reduced
from millions of gray or color pixels to just small subimages of binarized characters,
or even simply their contours or skeletons.
REFERENCES
1. H. Arai, and K. Odaka. Form reading based on background region analysis. In Proceed-
ings of the 4th International Conference on Document Analysis and Recognition. Ulm,
Germany, 1997, pp. 164–169.
2. E. Ben Braiek, M. Cheriet, and V. Dor´e. SKCS—a separable kernel family with com-
pact support to improve visual segmentation of handwritten data. Electronic Letters on
Computer Vision and Image Analysis. 5(1), 14–29, 2004.
3. E. Ben Braiek, M. Cheriet, and V. Dor´e. SKCS—une nouvelle famille de noyaux `a support
compact appliqu´ee `a la segmentation des donn´ees visuelles manuscrites. Traitement du
Signal. 23(1), 27–40, 2006.
4. F. Bergholm. Edge focusing. IEEE Transactions on Pattern Analysis and Machine Intelli-
gence. 9(6), 726–741, 1987.
5. J. Bernsen. Dynamic thresholding of gray level images. In Proceedings of the 8th Interna-
tional Conference on Pattern Recognition. Paris, France, 1986, Vol. 2, pp.1251–1255.
6. D. S. Britto JR., R. Sabourin, E. Lethelier, F. Bortolozzi, and C. Y. Suen. Improvement
in Handwritten Numeral String Recognition by Slant Correction and Contextual Informa-
tion. In Proceedings of International Workshop on Frontiers in Handwriting Recognition
(IWFHR), pages 323–332, Amsterdam, Septmber 2000.
7. P. J. Burt. Fast ﬁlter transform for image processing. Computer Vision, Graphics, and
Image Processing. 16, 20–51, 1981.
8. R.G. Casey. Moment normalization of handprinted character. IBM Journal of Research
and Development. 14, 548–557, 1970.
9. M.Cheriet,R.Thibault,andR.Sabourin.Amultiresolutionbasedapproachforhandwriting
segmentation in gray-scale images. In Proceedings of the 1st International Conference on
Image Processing. Austin, Texas, USA, 1994, pp. 159–168.
10. M. Cheriet. Extraction of handwritten data from noisy gray level images using multi-scale
approach. International Journal of Pattern Recognition and Artiﬁcial Intellignence. 13(5),
665–685, 1999.
11. J. L. Crowley. A representation for visual information. Ph.D. dissertation. Carnegie Mellon
University, Pittsburgh, PA, 1981.
12. K. Fukunaga. Introduction to Statistical Pattern Recognition. 2nd edition, Academic Press,
NewYork, 1990.

52
TOOLS FOR IMAGE PREPROCESSING
13. R. C. Gonzalez and R. E. Woods. Digital Image Processing. 2nd edition, Addison Wesley,
2001.
14. R. Horaud and O. Monga. Vision par Ordinateur, Outils Fondamentaux. Hermes, Paris,
France, 1995, p.425.
15. J. J. Hull. Document image skew detection: Survey and annotated bibliography. In J. J.
Hull and S. L. Taylor, editors, Document Analysis Systems II, World Scientiﬁc, Singapore,
1998, pp. 40–64.
16. J. J. Koenderink. The structure of images. Biological Cybernetics. 50, 363–370, 1984.
17. T. Lindeberg. Scale-Space Theory in Computer Vision. Kluwer Academic Publishers, 1994.
18. C.-L. Liu, H. Sako, and H. Fujisawa. Handwritten Chinese character recognition: Alter-
natives to nonlinear normalization. In Proceedings of the 7th International Conference on
Document Analysis and Recognition. Edinburgh, Scotland, 2003, pp. 524–528.
19. C.-L. Liu, K. Nakashima, H. Sako, and H. Fujisawa. Handwritten digit recognition: In-
vestigation of normalization and feature extraction techniques. Pattern Recognition. 37(2),
265–279, 2004.
20. C.-L. Liu and K. Marukawa. Pseudo two-dimensional shape normalization methods for
handwritten Chinese character recognition. Pattern Recognition. 38(12), 2242–2255, 2005.
21. K. Liu, C. Y. Suen, M. Cheriet, J. N. Said, C. Nadal, and Y. Y. Tang. Automatic extraction
of baselines and data from check images. International Journal of Pattern Recognition and
Artiﬁcial Intelligence. 11(4), 675–697, 1997.
22. D. Marr and E. Hildreth. Theory of edge detection. Proceedings of the Royal Society of
London. B-207, 187–217, 1980.
23. P. Meer, E. S. Baugher, and A. Rosenfeld. Frequency domain analysis and synthesis of
image pyramid generating kernels. IEEE Transactions on Pattern Analysis and Machine
Intelligence. 9, 512–522, 1987.
24. W. Niblack. An Introduction to Digital Image Processing. Prentice Hall, 1986.
25. N. Otsu. A threshold selection method from gray-scale histogram. IEEE Transactions on
System, Man, and Cybernetics. 9, 62–66, 1979.
26. P. W. Palumbo, P. Swaminathan, and S. N. Srihari. Document image binarization: Eval-
uation of algorithms. In Proceedings of the SPIE Symposium on Applications of Digital
Image Processing IX. 1986, Vol. 697, pp. 278–285.
27. L. Remaki and M. Cheriet. KCS—a new kernel family with compact support in scale
space: Formulation and impact. IEEE Transactions on Image Processing. 9(6), 970–981,
2000.
28. Y. Y. Tang, C. Y. Suen, C. D. Yan, and M. Cheriet. Financial document processing based on
staff line and description language. IEEE Transactions on System, Man, and Cybernetics.
25(5), 738–754, 1995.
29. O. D. Trier and A. K. Jain. Goal-directed evaluation of binarization methods. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence. 17, 1191–1201, 1995.
30. O. D. Trier and T. Taxt. Evaluation of binarization methods for document images. IEEE
Transactions on Pattern Analysis and Machine Intelligence. 17, 312–314, 1995.
31. J. Tsukumo and H. Tanaka. Classiﬁcation of handprinted Chinese characters using non-
linear normalization and correlation methods. In Proceedings of the 9th International
Conference on Pattern Recognition. Rome. Italy, 1988, pp. 168–171.

CHAPTER SUMMARY
53
32. J. M. White and G. D. Rohorer. Image thresholding for optical character recognition and
other applications requiring character image extraction. IBM Journal of Research and
Development. 27(4), 400–411, 1983.
33. N. Wiener. Extrapolation, Interpolation, and Smoothing of Stationary Time Series. MIT
Press, Cambridge, MA, 1942.
34. A. P. Witkin. Scale-space ﬁltering. In Proceedings of the International Joint Conference
on Artiﬁcial Intelligence. Karlsruhe, Germany, 1983, pp. 1019–1022.
35. H. Yamada, K. Yamamoto, and T. Saito. A nonlinear normalization method for hanprinted
Kanji character recognition—line density equalization. Pattern Recognition. 23(9), 1023–
1029, 1990.
36. T. Yamaguchi, Y. Nakano, M. Maruyama, H. Miyao, and T. Hananoi. Digit classiﬁcation
on signboards for telephone number recognition. In Proceedings of the 7th International
Conference on Document Analysis and Recognition. Edinburgh, Scotland, 2003, pp. 359–
363.
37. S. D. Yanowitz and A. M. Bruckstein. A new method for image segmentation. Computer
Vision, Graphics and Image Processing. 46(1), 82–95, 1989.
38. X. Ye, M. Cheriet, and C. Y. Suen. Stroke-model-based character extraction from gray level
document images. IEEE Transactions on Image Processing. 10(8), 1152–1161, 2001.
39. B. Yu and A. K. Jain. A generic system for form dropout. IEEE Transactions on Pattern
Analysis and Machine Intelligence. 18(11), 1127–1132, 1996.
40. T. Y. Zhang and C.Y. Suen. A fast parallel algorithm for thinning digital patterns. Commu-
nication of the ACM. 27(3), 236–239, 1984.
41. http://www.cs.berkeley.edu/˜fateman/kathey/skew.html
42. http://www.inf.u-szeged.hu/˜palagyi/skel/skel.html
43. http://cgm.cs.mcgill.ca/˜godfried/teaching/projects97/azar/skeleton.html
44. http://www.cs.mcgill.ca/˜aghnei/alg.html

CHAPTER 3
FEATURE EXTRACTION, SELECTION,
AND CREATION
“The only way of ﬁnding the limits of the possible is by going beyond them into the
impossible.”
—Arthur C. Clarke
The purpose of feature extraction is the measurement of those attributes of patterns that
are most pertinent to a given classiﬁcation task. The task of the human expert is to select
or invent features that allow effective and efﬁcient recognition of patterns. Many features
havebeendiscoveredandusedinpatternrecognition.Apartialcollectionofsuchfeatures
occupies the ﬁrst part of this chapter. Given a large set of features of possible relevance
to a recognition task, the aim of feature selection is to ﬁnd a subset of features that will
maximize the effectiveness of recognition, or maximize the efﬁciency of the process (by
minimizing the number of features), or both, done with or without the involvement of a
classiﬁer. The second part of this chapter is dedicated to feature selection. The ﬁnal part is
dedicated to the promising discipline of feature creation, which aims to delegate the task
of making or discovering the best features for a given classiﬁcation task to a machine.
3.1
FEATURE EXTRACTION
This section describes a collection of popular feature extraction methods, which can
be categorized into geometric features, structural features, and feature space transfor-
mations methods. The geometric features include moments, histograms, and direction
Character Recognition Systems: A Guide for Students and Practitioner, by M. Cheriet, N. Kharma,
C.-L. Liu and C. Y. Suen
Copyright © 2007 John Wiley & Sons, Inc.
54

FEATURE EXTRACTION
55
features. The structural features include registration, line element features, Fourier
descriptors, topological features, and so on. The transformation methods include prin-
cipal component analysis (PCA), linear discriminant analysis (LDA), kernel PCA, and
so on.
3.1.1
Moments
Moments and functions of moments have been employed as pattern features in numer-
ous applications to recognize two-dimensional image patterns. These pattern features
extract global properties of the image such as the shape area, the center of the mass,
the moment of inertia, and so on. The general deﬁnition of moment functions mpq of
order (p + q) for an X × Y continuous image intensity function f(x, y) is as follows:
mpq =

y

x
ψpq(x, y)f(x, y)dx dy,
(3.1)
where p, q are integers between [0, ∞), x and y are the x- and y-coordinate of an image
point, and ψpq(x, y) is the basis function. Valuable properties, such as orthogonality,
of the basis functions are inherited by the moment functions.
Similarly, the general deﬁnition for an X × Y digital image can be obtained by
replacing the integrals with summations
mpq =

y

x
ψpq(x, y)f(x, y),
(3.2)
where p, q are integers between (0, ∞) and represent the order, x and y are the x- and
y-pixel of the digital image, and ψpq(x, y) is the basis function.
A desirable property for any pattern feature is that it must preserve information
under image translation, scaling, and rotation. Hu was the ﬁrst to develop a number of
nonlinear functions based on geometric moments (discussed in the next section) that
were invariant under image transformation [44]. The sequel presents the deﬁnitions
of various types of moments and their properties.
3.1.1.1
Geometric Moments
Geometric moments or regular moments are by
far the most popular types of moments. The geometric moment of order (p + q) of
f(x, y) is deﬁned as [51]
mpq =
 +∞
−∞
 +∞
−∞
xpyqf(x, y)dx dy,
(3.3)
where p, q are integers between (0, ∞), x and y are the x- and y-coordinate of the
image, and xpyq is the basis function.

56
FEATURE EXTRACTION, SELECTION, AND CREATION
For a digital image, the deﬁnition is given by
mpq =
+∞

−∞
+∞

−∞
xpyqf(x, y),
(3.4)
where p, q are integers between (0, ∞) and represent the order, and x and y are the
coordinates of a pixel of the digital image.
As may be observed from the deﬁnition, the basis function for the geometric
momentisthemonomialproductxpyq,whichisnotorthogonal.Asaresult,recovering
an image from geometric moments is difﬁcult and computationally expensive.
3.1.1.2
Zernike and Pseudo-Zernike Moments
Zernike deﬁned a complete
orthogonal set {Vnm(x, y)} of complex polynomials over the polar coordinate space
inside a unit circle (i.e., x2 + y2 = 1) as follows [51]:
Vnm(x, y) = Vnm(ρ, θ) = Rnm(ρ)ejmθ,
(3.5)
where j = √−1, n ≥0, m is a positive or negative integer, |m| ≤n, n −|m| is even,
ρ is the shortest distance from the origin to (x, y) pixel, θ is the angle between
vector ρ and x-axis in counterclockwise direction, and Rnm(ρ) is the orthogonal
radial polynomial given by
Rnm(ρ) =
n−|m|/2

s=0
(−1)s ·
(n −s)!
s!( n+|m|
2
−s)!( n−|m|
2
−s)!
ρn−2s.
(3.6)
Note that Rn−m(ρ) = Rnm(ρ). These polynomials are orthogonal and satisfy the fol-
lowing condition:
 
x2+y2≤1
[Vnm(x, y)]∗Vpq(x, y)dx dy =
π
n + 1δnpδmq,
(3.7)
where
δab =

1;
if a = b,
0;
otherwise.
(3.8)
Zernike moments are the projection of the image intensity function f(x, y) onto the
complex conjugate of the previously deﬁned Zernike polynomial Vnm(ρ, θ), which is
deﬁned only over the unit circle
Anm = n + 1
π
 
x2+y2≤1
f(x, y)V ∗
nm(ρ, θ)dx dy.
(3.9)
For a digital image, Zernike moments are given by

FEATURE EXTRACTION
57
Anm = n + 1
π

x

y
f(x, y)V ∗
nm(ρ, θ), x2 + y2 ≤1.
(3.10)
To evaluate the Zernike moments, a given image is mapped to a unit circle using polar
coordinates, where the center of the image is the origin of the unit circle. Pixels falling
outside the unit circle are not taken into consideration.
Bhatia and Wolf derived pseudo-Zernike moments from Zernike moments; there-
fore, both these moments have analogous properties [108]. The difference lies in the
deﬁnition of the radial polynomial, which is given by
Rnm(ρ) =
n−|m|

s=0
(−1)s ·
(2n + 1 −s)!
s!(n −|m| −s)!(n + |m| + 1 −s)!ρn−s,
(3.11)
where n ≥0, m is a positive or negative integer subject to |m| ≤n only [108]. Re-
placing the radial polynomial in Zernike moments with the radial polynomial deﬁned
above yields pseudo-Zernike moments. Pseudo-Zernike moments offer more feature
vectors than Zernike moments due to the condition that n −|m| is even for the latter,
which reduces the polynomial by almost half. In addition, pseudo-Zernike moments
perform better on noisy images.
3.1.1.3
Legendre Moments
The well-known Legendre polynomials form the
basis function in the Legendre moments. Recall that the pth order Legendre polyno-
mial is deﬁned as
Pp(x) =
1
2pp!
dp
dxp (x2 −1)p, x ∈[−1, 1].
(3.12)
The deﬁnition of the (p + q) order Legendre moment is
Lpq = (2p + 1)(2q + 1)
4
 1
−1
 1
−1
Pp(x)Pq(y)f(x, y)dx dy,
(3.13)
where p, q are integers between (0, ∞), and x and y are the x- and y-coordinate of
the image.
Similarly, the Legendre moment for a (N × N) digital image is given by
Lpq =
N−1

m=0
N−1

n=0
Pp(mN)Pq(nN)f(m, n),
(3.14)
where
mN = 2m −N + 1
N −1
.
(3.15)
To compute the Legendre moments, a given image is mapped into the limit domain
[−1, 1] as the Legendre polynomial is only deﬁned over this range.

58
FEATURE EXTRACTION, SELECTION, AND CREATION
3.1.1.4
Tchebichef Moments
Mukundan et al. introduced the Tchebichef
moments where the basis function is the discrete orthogonal Tchebichef polynomial
[84]. For a given positive integer N (normally the image size), the Tchebichef poly-
nomial is given by the following recurrence relation:
tn(x) =
(2n −1)t1(x)tn−1(x) −(n −1)(1 −(n−1)2
N2
)tn−2(x)
n
,
(3.16)
with the initial conditions
t0(x) = 1,
t1(x) = (2x + 1 −N)/N,
(3.17)
where n = 0, 1, . . . , N −1.
The Tchebichef moment of order (p + q) of an image intensity function is deﬁned
as
Tnm =
1
ρ(n, M)ρ(n, N)
N−1

x=0
N−1

y=0
tm(x)tn(y)f(x, y),
(3.18)
where n, m = 0, 1, . . . , N −1. The Tchebichef polynomial satisﬁes the property of
orthogonality with
ρ(n, N) =
N(1 −
1
N2 )(1 −22
N2 ) · · · (1 −n2
N2 )
2n + 1
.
(3.19)
Note that with Tchebichef moments, the problems related to continuous orthogonal
moments are purged by using a discrete orthogonal basis function (i.e., Tchebichef
polynomial). In addition, no mapping is required to compute Tchebichef moments as
the Tchebichef polynomials are orthogonal in the image coordinate space.
3.1.2
Histogram
Histograms are charts displaying the distribution of a set of pixels of gray-level images
f(x, y). This section brieﬂy discusses the amplitude histogram.
The amplitude histogram of a gray-level image f(x, y) is the table H(k) that shows
the number of pixels that have gray-level value k. The deﬁnition of an amplitude
histogram for an (X × Y) digital image is given by
H(k) =
cardinal{f(x, y)|f(x, y) = k},
k ∈[0, kmax], (x, y) ∈[(0, 0), (X −1, Y −1)],
(3.20)
where cardinal{} is the number of pixels in a given set.

FEATURE EXTRACTION
59
The above deﬁnition may be generalized to compute the histogram of a (W × H)
image over any range B. In addition, divide the range of gray levels into bins Bn
and let kmin be the minimum gray value. For instance, if B = 10, kmin = 1000, and
Bn = 100, then bin 0 (B0) corresponds to all pixel values ranging from 1000 to 1010.
Therefore, the generalized amplitude histogram Hr(k) is given by
Hr(k) =
cardinal{f(x, y)|(kmin + Bk) ≤f(x, y) < kmin + Bk+1},
k ∈[0, Bn −1], (x, y) ∈[(0, 0), (W −1, H −1)].
(3.21)
3.1.3
Direction Features
Characters comprise strokes that are oriented lines, curves, or polylines. The orienta-
tion or direction of strokes plays an important role in differentiating between various
characters. For a long time, stroke orientation or direction has been taken into account
in character recognition based on stroke analysis. For statistical classiﬁcation based
on feature vector representation, characters have also been represented as vectors
of orientation/direction statistics. To do this, the stroke orientation/direction angle is
partitioned into a ﬁxed number of ranges, and the number of stroke segments in each
angle range is taken as a feature value. The set of numbers of orientational/directional
segments thus forms a histogram, called orientation or direction histogram. To further
enhance the differentiation ability, the histogram is often calculated for local zones
of the character image, giving the so-called local orientation/direction histogram.
Figure 3.1 shows an example of contour orientation histogram (four orientations,
4 × 4 zones).
Both orientation and direction histogram features can be called direction features
in general. In early stages, character recognition using direction features was called
directional pattern matching [32, 119] , wherein the character image is decomposed
into orientation planes (each recording the pixels of a particular stroke orientation),
and a distance measure is computed between the planes and the template of each class.
The local stroke orientation/direction of a character can be determined in different
ways: skeleton orientation [119], stroke segment [117], contour chaincode [52, 73],
gradient direction [104, 74], and so on. The contour chaincode and gradient direction
features are now widely adopted because they have simple implementation and are
approximately invariant to stroke-width variation.
FIGURE 3.1
(a) Character image; (b) contour of character; (c) orientation planes; (d) local
orientation histograms.

60
FEATURE EXTRACTION, SELECTION, AND CREATION
On decomposing the (size normalized) character image into direction planes, a
simple way to obtain feature values is to partition each direction plane into a number
of equal-sized zones and take the sum of pixel values in each zone as a feature
value. The values of all directions in a zone form a local direction histogram as that
in Figure 3.1. By this crisp partitioning, the feature values become sensitive to the
variation of stroke position and stroke width. To alleviate this problem, dynamic
partitioning [117] and soft partitioning have been tried. A blurring technique, akin
to soft zone partitioning, is now commonly adopted. Blurring, initially proposed by
Iijima in 1960s [46], is equivalent to low-pass spatial ﬁltering. It blurs the position of
strokes and the boundary between zones, thus improving the tolerance of character
shape variation.
In the following section, we brieﬂy introduce the blurring technique and then
describe the directional decomposition according to chaincode and gradient.
3.1.3.1
Blurring
Blurring is applicable to orientation/direction planes as well as
to any kind of image for reducing noise and improving the translation invariance. It is
performed by low-pass spatial ﬁltering, usually with a Gaussian ﬁlter, whose impulse
response function is given by
h(x, y) =
1
2πσ2x
exp

−x2 + y2
2σ2x

,
(3.22)
and its frequency transfer function is
H(u, v) = exp

−x2 + y2
2σ2u

,
(3.23)
where σu =
1
2πσx . h(x, y) is usually approximated to a ﬁnite-domain 2D window,
which is also called a blurring mask.
Denote the image to be ﬁltered by f(x, y); the pixel values of the ﬁltered image
are computed by convolution as follows:
F(x0, y0) =

x

y
f(x, y)h(x −x0, y −y0).
(3.24)
This can be viewed as a weighted sum of the shifted window h(x −x0, y −y0) with
a region of image f(x, y). The values of ﬁltered image are not necessarily computed
for all pixels (x0, y0). According to the Nyquist sampling theorem, an image of low-
frequency band can be reconstructed from a down-sampled image, provided that the
sampling rate is higher than twice the bandwidth. Approximating the bandwidth of
Gaussian ﬁlter to umax = vmax =
√
2σu and making the sampling rate fx = 2umax
relates the Gaussian ﬁlter parameter σx to the sampling rate by
σx =
√
2
πfx
=
√
2tx
π
,
(3.25)

FEATURE EXTRACTION
61
FIGURE 3.2
(a) Contour orientation planes; (b) blurred orientation planes; (c) sampled
values of blurred planes.
where tx is the sampling interval [73]. Formula (3.25) guides us to determine the
Gaussian ﬁlter parameter from the sampling interval, which can be predeﬁned em-
pirically. For example, to sample 4 × 4 values from an image of 32 × 32 pixels, the
sampling interval is 8.
Figure 3.2 shows an example of the blurring and down-sampling of the orientation
planes. The orientation planes of Figure 3.1 (copied in Fig. 3.2(a)) are blurred by
a Gaussian ﬁlter (ﬁltered images shown in Figure 3.2(b)), and the 4 × 4 values are
sampled from each ﬁltered image. At each sampling point, the sampled values of the
four orientations form a local orientation histogram, as shown in Figure 3.2(c), which
is similar to that of Figure 3.1. In practice, only the values of sampling points are
calculated in spatial ﬁltering according to (3.24).
3.1.3.2
Directional Decomposition
We introduce here two techniques for
generating direction planes from stroke edges: contour chaincode decomposition and
gradient direction decomposition.
The contour pixels of a binary image are commonly encoded into chaincodes when
they are traced in a certain order, say counterclockwise for outer loop and clockwise
for inner loop. Each contour pixel points to its successor in one of eight directions as
shown in Figure 3.3. As the contour length is approximately invariant to stroke width,
and the chaincode direction reﬂects the local stroke direction, it is natural to assign
contour pixels of the same chaincode to a direction plane and extract feature values
from the direction planes.
There are basically two ways to determine the direction codes of contour pixels.
In one way, the order of contour pixels is obtained by contour tracing, and then
FIGURE 3.3
Eight chaincode directions.

62
FEATURE EXTRACTION, SELECTION, AND CREATION
FIGURE 3.4
Pixels in the neighborhood of current pixel “c.”
the chaincodes are calculated. Contour tracing, however, is not trivial to implement.
Another way examines a 3 × 3 window centered at each contour pixel (a black pixel
with at least one of the 4-neighbors being white). The contour pixel is assigned
to one or two directions according to the conﬁguration of neighborhood [48]. In
the following, we introduce an algorithm that efﬁciently determines the chaincodes
without contour tracing [73].
Before the chaincode decomposition of binary image f(x, y), the eight direction
planes, fi(x, y), i = 1, . . . , 8, are empty, that is, fi(x, y) = 0. Scanning the pixels of
image f(x, y) in an arbitrary order and denoting the 8-connected neighbors of each
pixel by di, i = 0, . . . , 7 results in Figure 3.4. We can see that the neighboring pixels
correspond to the eight chaincode directions. If the current pixel c = f(x, y) = 1
(black pixel), check its 4-neighbors:
for i = 0, 2, 4, 6
if di = 0,
if di+1 = 1, then fi+1(x, y) = fi+1(x, y) + 1,
else if d(i+2)%8 = 1, then f(i+2)%8(x, y) = f(i+2)%8(x, y) + 1,
end if
end if
end for.
In this procedure, i + 1 or (i + 2)%8 is exactly the chaincode connecting the pixel
c = (x, y) and the pixel di+1 or the pixel d(i+2)%8.
For extracting orientation feature, the eight direction planes are merged into four
orientation planes by fi(x, y) = fi(x, y) + fi+4(x, y), i = 0, 1, 2, 3.
Considering that the stroke edge direction is approximately normal to the gradient
of image intensity, we can alternatively decompose the local gradient into directional
planes and extract the gradient direction feature. Unlike the chaincode feature, which
is applicable to binary images only, the gradient feature is applicable to gray-scale
images as well and is more robust against image noise and edge direction ﬂuctuations.
The gradient can be computed by the Sobel operator, which has two masks for the
gradient components in horizontal and vertical directions, as shown in Figure 3.5.
Accordingly, the gradient vector g = (gx, gy)T at a pixel (x, y) is computed by
FIGURE 3.5
Sobel masks for gradient.

FEATURE EXTRACTION
63
FIGURE 3.6
Gradient vector decomposition.
gx(x, y) =
f(x + 1, y −1) + 2f(x + 1, y) + f(x + 1, y + 1)
−f(x −1, y −1) −2f(x −1, y) −f(x −1, y + 1),
gy(x, y) =
f(x −1, y + 1) + 2f(x, y + 1) + f(x + 1, y + 1)
−f(x −1, y −1) −2f(x, y −1) −f(x + 1, y −1).
(3.26)
Unlike the discrete chaincodes, the direction of a gradient vector is continuous.
To obtain direction histogram feature, the gradient direction can be partitioned into
crisp regions of angle [104]. A gradient vector decomposition method, originally
proposed in online character recognition [49], can give better recognition perfor-
mance [74]. By specifying a number of standard directions (e.g., eight chaincode
directions), a gradient vector of arbitrary direction is decomposed into two com-
ponents coinciding with the two neighboring standard directions (Fig. 3.6 ). De-
noting the component lengths of two standard directions by l1 and l2, the corre-
sponding two direction planes are updated by f1(x, y) = f1(x, y) + l1 and f2(x, y) =
f2(x, y) + l2, respectively. The direction planes are completed by decomposing the
gradient vectors at all pixels. In order to extract the gradient orientation feature,
each pair of direction planes of opposite directions is merged into an orientation
plane.
Figure 3.7 shows the orientation planes of gradient on the character image of
Figure 3.1 and their blurred images. We can see that both the orientation planes and
the blurred images are similar to those of chaincode feature in Figure 3.2.
FIGURE 3.7
Orientation planes merged from eight direction planes of gradient (upper) and
their blurred images (lower).

64
FEATURE EXTRACTION, SELECTION, AND CREATION
3.1.4
Image Registration
In image processing applications, image registration is a necessity as it allows us to
compare and to integrate pictures taken at different times, from different sensors or
from different viewpoints [14]. Combining these images provides a comprehensive
and global picture.
3.1.4.1
Deﬁnition
Image registration is deﬁned as a mapping between two or
more images, both spatially and with respect to intensity [14]. Consider two two-
dimensional images (2D) with intensity functions, f1(x, y) and f2(x, y). Both of
these functions map to their corresponding intensity values in their respective images.
Therefore, the mapping between two images can be described as follows:
f2(x, y) = g(f1(T(x, y))),
(3.27)
where f1(x, y) and f2(x, y) are the intensity functions of two 2D images, T is a 2D
spatial-coordinate transformation, and g is a one-dimensional (1D) intensity transfor-
mation.
The objective of image registration is to determine the optimal spatial and inten-
sity transformation, that is, T and g, respectively, in order to match images for (1)
determining the parameters of the matching transformation or (2) exposing important
differences between images [14].
3.1.4.2
Transformation
Transformations are required to eliminate the variation
in images and to register them properly. The type of spatial transformation depends
on the image registration technique used. The registration methodology must choose
the optimal set of transformations to eliminate the variation of images caused by
differences in acquisition or intrinsic changes in the scene. Popular transformations
include rigid, afﬁne, projective, perspective, and global polynomial. A brief deﬁnition
of each type of transformation and its properties is given in the sequel.
Rigid (or linear) transformations consist of a mixture of a rotation, a translation,
and a scale change [14]. A transformation is said to be rigid if it preserves relative
distance before and after the transformation. Such transformations are linear in nature.
A transformation T is linear if
T(x1 + x2) = T(x1) + T(x2),
cT(x) = T(cx),
(3.28)
where c is a constant [14].
Afﬁne transformations are a generalization of linear transformations; therefore,
they are able to cope with more complex distortions. Nevertheless, they still hold
some nice mathematical characteristics. A transformation is afﬁne if T(x) −T(0) is
linear [14]. Typically, it maps a point (x1, y1) of the ﬁrst image to a point (x2, y2) of
the second image using the following mathematical relation:

FEATURE EXTRACTION
65

x2
y2

=

tx
ty

+ s

cos θ
−sin θ
sin θ
cos θ
 
x1
y1

,
(3.29)
or
¯p2 = ¯t + sR ¯p1,
(3.30)
where ¯p1, ¯p2 are the coordinate vectors of the two images, ¯t is the translation vector,
s is a scalar scale factor, and R is the rotation matrix. Note that the rotation matrix R
is orthogonal, therefore, the angles and lengths in the ﬁrst images are conserved. The
scale factor s is a scalar that changes both lengths of x and y with respect to the ﬁrst
image. Afﬁne transformations are linear if the translation vector ¯t is ignored.
Generalizing the mathematical relation of afﬁne transformation yields

x2
y2

=

a13
a23

+ s

a11
a12
a21
a22
 
x1
y1

.
(3.31)
The rotation matrix is no longer orthogonal, nor are the angles and lengths conserved;
however, parallel lines remain parallel. Complex distortions, namely shears and
changes in aspect ratio, may be held with the general afﬁne transformation. Shears
are distortions of pixels along one axis proportional to their location in the other axis
[14]. The shear matrices, which act along the x-axis and the y-axis, are described
as
shearx =

1
a
0
1

, sheary =

1
0
b
1

,
(3.32)
respectively. Aspect ratio is the relative scale between the x and y axes. The aspect
ratio is changed by scaling x-axis and y-axis separately. Mathematically, scaling is
deﬁned as

sx
0
0
sy

.
(3.33)
Combinations of rigid transformations, shears, and aspect ratio distortions can be
eliminated using the general afﬁne transformation.
Perspective transformation is required for the distortion that occurs when a three-
dimensional (3D) scene is projected through an idealized optical image system [14].
For instance, a 3D object projection on a ﬂat screen results in perspective distortion.
Under perspective distortion, the image appears to be smaller, the farther it is from
the camera, and more compressed, the more it is inclined away from the camera [14].
Let (x0, y0, z0) be a point in the original scene; the corresponding point in the image

66
FEATURE EXTRACTION, SELECTION, AND CREATION
(xi, yi) is deﬁned by
xi = −fx0
z0 −f , yi = −fy0
z0 −f ,
(3.34)
where f is the position of the center of the camera lens.
Projective transformation is a special case of perspective transformation. It
accounts for situations when the scene is composed of a ﬂat plane skewed with respect
to the image plane. Projective transformation maps the scene plane into an image that
is tilt-free and has a set scale. A point in the scene plane (xp, yp) is mapped into a
point in the image plane (xi, yi) using the following relation:
xi = a11xp + a12yp + a13
a31xp + a32yp + a33
,
yi = a21xp + a22yp + a23
a31xp + a32yp + a33
,
(3.35)
where the a terms are constants that are dependant on the equations of the scene and
image plane [14].
When all the above transformations are inept to eliminate the image distortions,
then polynomial transformation may be utilized to achieve a global alignment. This
type of transformation is deﬁned in the following section.
3.1.4.3
Image Variations
The ﬁrst step to optimal image registration is to un-
derstand the different types of image variations. Image variations occur when there
are changes in the scene due to the sensor and its position or viewpoint. They are clas-
siﬁed into three different types: corrected distortions, uncorrected distortions, and
variations of interest. Corrected distortions are spatial distortions due to a difference
in acquisition that may be corrected using an appropriate transformation technique.
Uncorrected distortions are volumetric distortions, caused by lighting and atmo-
spheric conditions, which remain uncorrected after using a transformation. Variations
of interests are distortions that enclose important information, such as object move-
ments or growths, and must be exposed after transformation. Differentiating the types
of image variations helps immensely the process of image registration [14].
3.1.4.4
Registration Methods
In the following, we outline four classes of
registration methods: correlation, Fourier methods, point mapping, and elastic meth-
ods.
(1) Correlation methods
Correlation methods are fundamental techniques used in registration methods. They
are founded on cross-correlation that matches a template or a pattern in a picture. In
principle, cross-correlation measures the degree of similarity between an image and
a template. Consider an (X × Y) template T and an (U × V) image I, where T is

FEATURE EXTRACTION
67
smaller than I, then the 2D normalized cross-correlation function is deﬁned as
C(u, v) =

x

y T(x, y)I(x −u, y −v)

x

y I2(x −u, y −v)
 ,
(3.36)
for each translation, where (u, v) is a point located on the image I [14]. To ﬁnd the
degree of similarity between a template and an image, the cross-correlation must
be computed over all possible translations. Note that as the number of translation
increases, the computation cost also increases. Therefore, correlation methods are
mostly used for images that are misaligned by small rigid or afﬁne transformations.
(2) Fourier methods
Fourier methods are based on the fact that information in a picture can be represented
in the frequency domain. In addition, geometric transformations, such as translation,
rotation, and reﬂection, all have their counterparts in the frequency domain. Therefore,
all computations are done in the frequency domain in the Fourier methods. Kuglin and
Hines [14] introduced phase correlation to achieve alignment between two images.
Before discussing phase correlation, it is necessary to deﬁne a few terms.
The Fourier transform of an intensity image function f(x, y) is given by
F(wx, wy) = R(wx, wy) + jI(wx, wy),
(3.37)
where R(wx, wy) and I(wx, wy) are the real and imaginary parts at each frequency
(wx, wy), respectively. The Fourier transform can also be expressed in the exponential
form as follows:
F(wx, wy) = |F(wx, wy)|ejφ(wx,wy),
(3.38)
where |F(wx, wy)| and φ(wx, wy) are the magnitude and the phase angle of the Fourier
transform, respectively. An important property of the Fourier transform is its trans-
lation property, also known as shift theorem. Consider two images f1 and f2, which
are related by
f2(x, y) = f1(x −dx, y −dy),
(3.39)
where dx and dy are constants. The corresponding Fourier transforms F1 and F2 are
deﬁned as
F2(wx, wy) = e−j(wxdx+wydy)F1(wx, wy).
(3.40)
Hence, displacement in the spatial domain is equivalent to a phase shift in the fre-
quency domain. In the frequency domain, the phase shift or phase correlation can be

68
FEATURE EXTRACTION, SELECTION, AND CREATION
computed using the cross-power spectrum of the two images:
F1(wx, wy)F∗
2 (wx, wy)
|F1(wx, wy)F∗
2 (wx, wy)| = e(wxdx+wydy),
(3.41)
where F∗is the complex conjugate of F. Taking the inverse Fourier transform of the
cross-power spectrum yields an impulse function δ(dx, dy). The impulse is located at
the displacement required to match the two images and thus achieves optimal image
registration. Like the correlation methods, Fourier methods are mostly employed for
images that are misaligned by small, rigid, or afﬁne transformations.
(3) Point mapping
Point mapping is the primary registration method employed when the misalignment
between two images is not identiﬁable, for instance, when two images suffer from
perspective distortion (see Section 3.1.3.2). In principle, the standard point-mapping
technique has three stages. The ﬁrst stage entails computing the features in the image.
In the second stage, feature points (also known as control points) in the template or
pattern are matched with feature points in the picture. In the third stage, a spatial map-
ping or transformation is found taking into consideration the matched feature points
obtained in the second stage [14]. This spatial mapping (along with interpolation) is
thereafter used to resample one image onto the other.
(4) Elastic methods
Elastic methods are the latest engineered methods for image registration. In these
methods, the distortion in the image is modeled as an elastic body; therefore, no
transformations are computed. The elastic body is bent and stretched and thus
deformed. The amount of bending and stretching is described by the energy state of
the elastic material. In principle, the control points are modeled as external forces that
deform the body. These forces are compensated by stiffness or smoothness constraints
that are usually parameterized to give the user some ﬂexibility [14]. In this process,
the registration is deﬁned by determining the minimum energy state that corresponds
to the given deformation transformation. As elastic methods imitate physical defor-
mations, they are widely employed for problems in shape and motion reconstruction
and medical imaging.
3.1.5
Hough Transform
A popular feature extraction method used in digital image processing is the Hough
transform (HT). It is able to detect straight lines, curves, or any particular shape that
can be deﬁned by parametric equations. In essence, this method maps the ﬁgure points
from the picture space to the parameter space and, thereafter, extracts the features.
There are two types of Hough transform: standard HT and randomized HT. The sequel
brieﬂy expounds these two types of HT.
3.1.5.1
Standard Hough Transform
As mentioned above, the objective of the
Hough transform is to detect a possible line, curve, or shape that passes through the

FEATURE EXTRACTION
69
FIGURE 3.8
Polar space representation of a straight line.
ﬁgure points in a digital picture. In the standard hough transform, all ﬁgure points
{(x1, y1), . . . , (xn, yn)} from the picture space are mapped to the parameter space
[25]. For instance, a line in the picture space, y = mx + b is deﬁned in the parameter
space as follows:
ρ = x cos θ + y sin θ,
(3.42)
where ρ is the length of the normal from the origin to the line and θ is the angle of
ρ with respect to the x-axis (see Fig. 3.8 ). Therefore, a line in the picture space is
mapped to a unique point (e.g., (θ0, ρ0)) in the parameter space. The parameter space
is divided into ﬁnite intervals, often called accumulator bins, which are assigned a
value that is incremented every time a ﬁgure point is mapped to that given interval
in the parameter space. Maxima in the accumulator array indicate the detection of a
straight line or shape in the picture. Mapping the maxima in the accumulator array
back to the picture space completes the detection of a feature [25].
3.1.5.2
Randomized Hough Transform
randomized Hough transform is an
enhanced approach for detecting features in binary images. It takes into account the
drawbacks of standard Hough transform, namely long computation time and large
memory requirements.
It is based on the fact that a curve or a shape can be deﬁned in the parameter space
with a pair or n-tuple of points (depending on the shape to be detected) from the
original binary picture. Consider a set of ﬁgure points P = {(x1, y1), . . . , (xn, yn)} in
the binary picture. Now, let (θ, ρ) be the two parameters of the lines to be detected. A
pairofpoints(pi(x, y), pj(x, y))isselectedrandomlyfromthesetP andmappedtothe
parameter space. The corresponding accumulator bins are then incremented. Unlike
standard Hough transform where each ﬁgure point is mapped to the parameter space,
randomized Hough transform maps a set of ﬁgure points that may or may not form
a shape; therefore, a considerable decrease in the computation time is noticed. The
result of continuing this process will be the appearance of maxima in the accumulator

70
FEATURE EXTRACTION, SELECTION, AND CREATION
bins of the parameter space. These maxima can thereafter be used to detect the lines
in the binary pictures [64].
3.1.6
Line-Based Representation
Though the strokes in character images are usually thicker than one pixel, line-based
features can be extracted from the stroke edge (contour) or the skeleton.
3.1.6.1 Edge (Contour)
Anedgeisdeﬁnedastheboundarybetweentworegions
with different gray-level properties [37]. It is possible to detect an edge because the
gray-level properties of an image at an edge drastically change, which results in
gray-level discontinuity.
In most edge-detection methods, the local ﬁrst and second derivatives are com-
puted. A drastic change in the magnitude of the ﬁrst derivative signiﬁes the presence
of an edge in an image. Similarly, using the sign of the second derivative, it is possible
to determine whether an edge pixel lies on the dark or on the light side of an edge. The
second derivative is positive for a dark side and negative for a light side. Therefore,
if a second derivative of a given boundary in an image is a negative spike followed
by a positive spike, then there is a detection of a light-to-dark edge.
Tracing a contour involves the detection of edges. As a contour is composed of a
set of boundary points, it is possible to extend any edge-detection method to detect
contours.
3.1.6.2
Thinning
Thinning is a process during which a generally elongated pat-
ternisreducedtoaline-likerepresentation.Thisline-likerepresentationoracollection
of thin arcs and curves is mostly referred to as a skeleton. Most thinning algorithms
are iterative in that they delete successive layers of pixels on the edge of the pattern
until only a skeleton is left [125]. A set of tests is performed to assess if a given black
pixel p is to be deleted or to be preserved. Obviously, the deletion or preservation of
p depends on its neighboring pixels’ conﬁguration. Thinning algorithms are divided
into two categories: sequential and parallel.
In a sequential algorithm, the sequence in which the pixels are tested for deletion
is predetermined, and each iteration depends on all the previous iterations. That is,
the deletion of p in the nth iteration depends on all the operations that have been
performed so far, that is, on the result of the (n −1)th iteration as well as on the
pixels already processed in the nth iteration [67].
In a parallel algorithm, the deletion of pixels in the nth iteration would depend only
on the result that remains after the (n −1)th; therefore, all pixels can be examined
independently in a parallel manner in each iteration [67].
3.1.6.3
Tracing
Tracing is a method designed to recover the sequence of the
segments that compose handwritten English or handwritten signatures [72]. Before
introducing the tracing algorithm, a few deﬁnitions are provided in order to ease its
understanding.

FEATURE EXTRACTION
71
FIGURE 3.9
(a) An end point pixel Pi; (b) a junction point pixel Pi.
(1) Deﬁnitions
A signature is made up of multiple segments, better known as strokes. A stroke is a
series of pixels that are placed according to the order in which they are traced. The
stroke originates at a starting point and concludes at an ending point. The notation of
a stroke can be given by
Sj
i
tracing
−→
Ej
i ,
(3.43)
where i is the stroke number, j is the number of time a given stroke is traced, Sj
i is
the starting point (may be traced more than once j > 1), →is the tracing direction,
and Ej
i is the ending point [72].
A transition function T(x) computes the connectivity of a given pixel Pi with
its eight neighboring pixels. In other words, it returns the number of pixels that are
connected to a given pixel Pi.
An end point (EP) is a pixel Pi that is connected to one immediate neighboring
pixel (i.e. T(Pi) = 1). A junction point (JP) is a pixel Pi that is connected to three
or more adjacent pixels (i.e., T(Pi) ≥3). Figure 3.9 illustrates an EP and a JP with
T(Pi) = 1 and T(Pi) = 3, respectively.
Another important parameter during tracing is the performance measure (PM).
Performance measure is the tracing direction between a pair of successive pixels and
is coded according to the eight directional chaincodes shown in Figure 3.10 (a). For
instance, the pixel conﬁguration of Figure 3.10 (b) is given a performance measure
of 3, that is, PM = 3.
Obtaining the direction of a stroke requires another parameter, the intrastroke
performance measure (intra-PM). This measures the sums and averages of all the
successive PMs of the stroke. The mathematical expression for the intra-PM of the
ith stroke (intra-PMi) is as follow
intra-PMi =
n−1

j=1
PMij
n −1 = PMi1 + PMi2 + · · · + PMi(n−1)
n −1
,
(3.44)

72
FEATURE EXTRACTION, SELECTION, AND CREATION
FIGURE 3.10
(a) Directional chaincodes; (b) pixel conﬁguration with PM = 3.
where PMij is the performance measure between pixel j and j + 1 in the ith stroke
of the signature, and n is the total number of pixels of the ith stroke.
A simple test is performed to detect the direction or sequence of the stroke:
• If (intra-PMi) > threshold (zero), the original stroke sequence is preserved.
• If (intra-PMi) ≤threshold (zero), the original stroke sequence is reversed to
enhance the intra-PM of the ith stroke.
(2) Tracing algorithm
After deﬁning key parameters used in tracing, the sequel presents a heuristic-rule-
based tracing algorithm (HRBTA). Before starting the tracing process, there is some
preliminary work to be done:
• An integer array with one element per black pixel is needed to track the number
of times each pixel has been traced. Obviously, all elements in the array are
initialized to zero.
• Two ﬁrst in, ﬁrst out (FIFO) queues are required to track the x and y coordinates
of the EPs and JPs of the given signature.
The pseudocode of the HRBTA is as follows [72]:

FEATURE EXTRACTION
73
Procedure HRMTA(sig array, ep q, jp q, traced array)
begin
while there are EPs or JPs in their respective FIFO queue do
begin
Find the starting point of the current stroke from ep q and
jp q. Traced from the selected starting point until
an ending point is encountered.
If intra-PM of the stroke ≤0 then
begin
Reverse the sequence of the traced stroke.
end
Append the stroke to traced array and pad it with a SEPARATOR.
Delete the starting point from its corresponding FIFO queue
end
call subroutine ReOdr Stkr(traced array, sorted array))
end
As you may notice, the tracing algorithm has four inputs: sig array, ep q, jp p,
and traced array. sig array is a 448 × 120 array that saves the skeletal image of a
given signature. ep q and jp p are two FIFO queues that store the EPs and JPs of a
given signature. Finally, traced array is an array that stores the sequence of traced
strokes of a signature.
The algorithm starts by tracing from a starting point and ﬁnishes at an ending
point. This stroke is then concluded. It reverses the stroke sequence if the intra-
PM is negative. This stroke is then saved in the traced array and followed by a
SEPARATOR. The current starting point is deleted from its FIFO and a new starting
point is selected to repeat the process. The process is repeated until both FIFO queues
are empty.
The subroutine ReOdr Stkr plays a crucial role in the algorithm. Occasionally,
strokes are not in order when the tracing process is complete due to backtrack-
ing strokes and the minimum hand-movement behavior of a signature. Therefore,
ReOdr Stkr rearranges the stroke sequence to eliminate the remaining stroke
disorders.
3.1.7
Fourier Descriptors
Nowadays in image processing, a fundamental need is to retrieve a particular shape
from a given image. Numerous shape retrieval methods have been developed in the
literature; however, Fourier descriptors (FD) is one of the most popular and efﬁcient
methods. In principle, it represents the shape of the object in the frequency domain. By
doing so, Fourier descriptors enjoy multiple advantages over their counterparts. These
advantages include strong discrimination ability, low noise sensitivity, easy normal-
ization, and information preservation [125]. The sequel ﬁrst discusses different shape
signatures and thereafter expounds two types of FD: standard Fourier descriptors and
elliptical Fourier descriptors.

74
FEATURE EXTRACTION, SELECTION, AND CREATION
3.1.7.1
Shapes
Fourier descriptors are used for describing the shape (closed
curve) of any object found on an input image. However, before computing the Fourier
descriptors, the input image is digitalized and boundary information of the object is
extracted [89] and normalized. During normalization, the data points of the shape
boundary of the object and model are sampled to have the same number of data
points. As Fourier descriptors require a 1D representation of the boundary infor-
mation, shape signatures are used. A shape signature maps a 2D representation of a
shape to a 1D representation. Although there are numerous shape signatures, complex
coordinates, centroid distance, curvature, and cumulative angular function are the four
shape signatures that will be considered hereafter.
Complex Coordinates
ConsiderabinaryshapeboundarythatismadeofLpixels,whereeachpixelhastheco-
ordinates (x(t), y(t)) with t = 0, 1, . . . , L −1. A boundary coordinate pair (x(t), y(t))
can be expressed in terms of complex coordinates using the following statistical
relation:
z(t) = [x(t) −xc] + j[y(t) + yc],
(3.45)
where j = √−1 and (xc, yc) is the shape’s centroid given by
xc = 1
L
L−1

t=0
x(t), yc = 1
L
L−1

t=0
y(t),
(3.46)
where L is the number of pixels that form the shape boundary. Shifting the com-
plex representation by the centroid is not necessary; however, it makes the shape
representation invariant to translation [125].
Centroid Distance
The centroid distance function represents the boundary information by the distance
separating the centroid and the boundary points. For a given shape, the centroid
distance function is given by
r(t) =

([x(t) −xc]2 + [y(t) −yc]2),
(3.47)
where (xc, yc) is the shape’s centroid deﬁned by Eq. (3.1.40).
Curvature Signature
Curvature signature can be obtained by differentiating successive boundary angles
calculated in a window w. The boundary angle is deﬁned as
θ(t) = arctany(t) −y(t −w)
x(t) −x(t −w).
(3.48)

FEATURE EXTRACTION
75
The curvature is given by
K(t) = θ(t) −θ(t −1).
(3.49)
Note that this deﬁnition of curvature function has discontinuities at size 2π in the
boundary. Thus, it is preferable to use the following deﬁnition:
K(t) = ϕ(t) −ϕ(t −1),
(3.50)
where ϕ(t) is deﬁned by Eq. (3.1.44).
Cumulative angular function shapes can also be deﬁned according to the boundary
angles; however, boundary angles can only take values in range of [0, 2π] as the
tangent angle function θ(t) has discontinuities at size 2π. Zahn and Roskies introduced
the cumulative angular function ϕ(t) to eliminate the problem of discontinuity. They
deﬁned it as the net amount of angular bend between the starting position z(0) and
position z(t) on the shape boundary:
ϕ(t) = [θ(t) −θ(0)]mod(2π),
(3.51)
where θ(t) is the boundary angle deﬁned by Eq. (3.1.42).
A normalized cumulative angular function ψ(t) is used as the shape signature
(assuming shape is traced in the anticlockwise direction):
ψ(t) = ϕ
 L
2πt

−t,
(3.52)
where L is the number of pixel forming the shape boundary and t = 0, 1, . . . , L −1.
3.1.7.2
Standard Fourier Descriptors
The Fourier transform for a shape
signature formed with L pixels, s(t), t = 0, 1, . . . , L, assuming it is normalized to N
points in the sampling stage, is given by
un = 1
N
N−1

t=0
s(t)e
 −j2πnt
N

, n = 0, 1, . . . , N −1.
(3.53)
The coefﬁcients un are called the Fourier descriptors of the sample. They may also
be denoted by FDn.
3.1.7.3
Elliptical Fourier Descriptors
Kuhl and Giardina [63] introduced
another approach for shape description [109]. They approximated a closed contour
formed of L pixels, (x(t), y(t)), t = 0, 1, . . . , L −1 as
ˆx(t) = A0 +
L

n=1

an cos 2nπt
T
+ bn sin 2nπt
T

,

76
FEATURE EXTRACTION, SELECTION, AND CREATION
ˆy(t) = C0 +
L

n=1

cn cos 2nπt
T
+ dn sin 2nπt
T

,
(3.54)
where n = 1, 2, . . . , L, T is the total length of the contour and with ˆx(t) ≡x(t) and
ˆy(t) ≡y(t) in the limit when N →∞. The coefﬁcients are given by
A0 = 1
T
 T
0 x(t)dt ,
C0 = 1
T
 T
0 y(t)dt ,
an = 2
T
 T
0 x(t) cos 2nπt
T
dt ,
bn = 2
T
 T
0 x(t) sin 2nπt
T
dt ,
cn = 2
T
 T
0 y(t) cos 2nπt
T
dt ,
dn = 2
T
 T
0 y(t) sin 2nπt
T
dt .
(3.55)
3.1.8
Shape Approximation
The contour or skeleton of a character image can be approximated into piecewise line
segments, chaincodes, and smooth curves.
3.1.8.1
Polygonal Approximation
As the name implies, polygonal approx-
imation approximates a digital contour as a 2D polygon. For a given contour, the
most exact approximation is deﬁned as the number of segments of the polygon being
equal to the number of points of the contour. A polygonal approximation offers the
possibility to describe the essential contour shape with the fewest possible polygo-
nal segments. For instance, minimum perimeter polygons approximate a contour by
enclosing the contour by a set of concatenated cells and regarding the contour as a
rubber band contained within the inside and outside boundaries of the strip of cells
[97].
3.1.8.2
Chaincodes
Freeman [31] introduced chaincodes to represent a shape
boundarybyaconnectedsequenceofstraightlinesthathaveﬁxedlengthanddirection.
The direction of the lines is based on 4- or 8-connectivity (see Fig. 3.11(a) and
3.11(b)) and coded with the corresponding code. For instance, the 4-connectivity
code 00112233 represents a line sequence that forms a perfect square having the
bottom left corner as the starting point (see ﬁg. 3.11(c)). Similarly, any shape can be
coded depending on the scheme using 4- or 8-connectivity [83, 37].
3.1.8.3
Bezier Curves
Shape approximation may also be achieved using Bezier
curves. Paul Bezier developed Bezier curves in 1960 for designing cars. In prin-
ciple, the shape of the smooth Bezier curve is controlled by a set of data points
{P0, P1, . . . , Pn}, called control points. These control points determine the slopes of
the segments that form the smooth Bezier curve and thus contribute to the overall
shape of the curve. For instance, the slope of the curve at P0 is going to be the same

FEATURE EXTRACTION
77
FIGURE 3.11
Chaincodes: (a) 4-connectivity; (b) 8-connectivity; (c) example.
as the slope of the line connecting P0 and P1. Therefore, the location of P1 relative
to P0 controls the shape of the Bezier curve.
A Bezier curve of order n can formally be deﬁned as a weighted sum of n + 1
control points {P0, P1, . . . , Pn}. The weights are the Bernstein polynomials, also
called blending functions:
Bn
i (t) =
n
i

(1 −t)n−iti,
(3.56)
where n is the order of the curve and i = 0, 1, . . . , n.
The mathematical deﬁnition of Bezier curve is as follow:
¯P(t) =
n

i=0
n
i

(1 −t)n−iti ¯Pi, 0 ≤t ≤1,
(3.57)
where n is the order of the curve and i = 0, 1, . . . , n. It is important to note that the
curve generally does not pass through the control points except for the ﬁrst and the
last. Also, the Bezier curve is always enclosed within the convex hull of the control
points. Figure 3.12 illustrates a common application of Bezier curve: font deﬁnition.
FIGURE 3.12
Bezier curve used in font deﬁnition.

78
FEATURE EXTRACTION, SELECTION, AND CREATION
3.1.8.4 B-Splines
B-splines are a generalization of the Bezier curves. A B-spline
is deﬁned as follows:
P(t) =
n+1

i=1
Ni,k(t)Pi, tmin ≤t ≤tmax,
(3.58)
where n + 1 are the control points (P1, P2, . . . , Pn+1), Ni,k are the blending functions
of order k ∈[2, n + 1], and degree k −1 and t are the prespeciﬁed values at which the
pieces of the curve join. This set of values is called a knot vector (t1, t2, . . . , tk+(n+1)).
The values of t are subject to the following condition ∀i, ti ≤ti+1. The B-spline
blending functions are recursively deﬁned as follows:
Ni,k =

1
if ti < t < ti+1
0
otherwise
(3.59)
for k = 1, and
Ni,k(t) = (t −ti)Ni,k−1(t)
ti+k−1 −ti
+ (ti+k −t)Ni+1,k−1(t)
ti+k −ti+1
.
(3.60)
3.1.9
Topological Features
We describe two types of topological features extracted from skeleton or traced con-
tour: feature points and curvature.
3.1.9.1
Feature Points
In a thinned image, black pixels that have a number of
black neighbors not equal to 0 or 2 are given the name of feature points. Feature points
have been classiﬁed into three types:
1. End points;
2. Branch points;
3. Cross points.
The ﬁrst type of point, the end point, is deﬁned as the start or the end of a line segment.
A branch point is a junction point connecting three branches. A cross point is another
junction point that joins four branches [5]. Figure 3.13 visually illustrates the three
types of feature points.
3.1.9.2
Curvature Approximation
Curvature of a given curve or binary con-
tour is another essential local feature. In calculus, the curvature c at a point p on a
continuous plane curve C is deﬁned as c = lim	s→0 	α
	s , where s is the distance to
the point p along the curve and 	α is the change in the angles of the tangents to the
curve at the distances s and s + 	s, respectively [24].

FEATURE EXTRACTION
79
FIGURE 3.13
Types of feature points: (a) end point; (b) branch point; (c) cross point.
In a binary curve, the above deﬁnition of curve cannot be applied directly as
the analytical format of the curve is not available. Nevertheless, the curvature can be
obtained using a unity interval 	s = 1. The curvature is therefore given by c = 	α. In
order to compute the curvature, the digital curve C must be a sequence of consecutive
points on the x–y plane: C = p1p2 . . . pN, where pn = (xn, yn), the distance from
pn to pn+1 is 1 (since 	s = 1), and N is the number of sampled points in the trace
[24].
To approximate the curvature of the given curve C, the sequences of angles from
point to point must be calculated:
A = α1α2 . . . αL,
(3.61)
where the angle αn has the range of [−180◦, 180◦] and is given by
αn = tan−1 yn+1 + yn
xn+1 + xn
.
(3.62)
From the deﬁnition of A, it is easy to show that the sequence of changes in angles is
given by
	A = 	α1	α2 . . . 	αn,
(3.63)
where the difference in angles is deﬁned as
	αn = (αn −αn−1)mod360◦.
(3.64)
To remove the digitization and quantization noise from the above deﬁnition, the
sequence 	A should be convolved with a Gaussian ﬁlter G, that is, 	A × G = 	A∗.
Finally, the curvature C of curve can be computed as follows:
cn = 	α∗
n.
(3.65)
3.1.10
Linear Transforms
Feature space transformation methods are often used in pattern recognition to change
to feature representation of patterns for improving the classiﬁcation performance.

80
FEATURE EXTRACTION, SELECTION, AND CREATION
Linear transforms are usually used for reducing the dimensionality of features, and
some of them can also improve the classiﬁcation accuracy.
3.1.10.1
Principal Component Analysis
Principal component analysis is a
statistical technique belonging to the category of factor analysis methods. The com-
mon goal of these methods is the simpliﬁcation of a set of features by the identiﬁca-
tion of a smaller subset of transformed features that contains essentially the “same”
information as the original set. “Dimensionality reduction” is also a common way
of describing this goal. The reason why there are many factor analysis methods is
that there are many ways of ﬁnding reasonable subsets of transformed features with
roughly the same information. For instance, independent component analysis (ICA)
seeks independent factors, but a drawback of this strong condition of independence
is that it will not work if the features are normally distributed.
PCA works regardless of the distributional characteristics of the features, as it en-
forces the weaker condition of noncorrelation of the transformed features. Moreover,
it is a linear technique, in the sense that each new transformed feature is a linear
combination of the original set of features.
Another way to consider PCA is from the concept of regression. Classical least
squares regression is concerned with ﬁnding a simple way to explain the dependency
between a dependent variable Y and an independent variable X. The simple linear
relationship is the one that minimizes the sum of squares of the regression errors.
One important thing to remember about this is that regression minimizes the errors
considered as the vertical distances between Y and its approximation. In many appli-
cations, such as when one regresses a variable on itself, the researcher may rather be
interested in ﬁnding the linear relationship that minimizes the perpendicular errors.
The ﬁrst principal component of PCA is precisely this linear relationship. Figure 3.14
shows the ﬁrst and second principal components for a bivariate sample. As one can
see, to the naked eye, the ﬁrst principal component is very similar to a regression line.
FIGURE 3.14
First and second principal components.

FEATURE EXTRACTION
81
Formally, let us deﬁne the following linear combination of features:
z = a0 + a1y1 + · · · + akyk.
(3.66)
The sample variance of this linear combination is
s2
z = a′Sa,
(3.67)
where a is a k × 1 vector of weights and S is the variance–covariance matrix of the
k features. The PCA problem consists of ﬁnding a characterization of what would
be the set of weights of vector a such that the linear combination y would have the
maximum variance.
A moment of reﬂection shows that this problem is ill-deﬁned: If we do not constrain
the elements of a in some sort, then we can always ﬁnd inﬁnity as the maximum vari-
ance, because all the variances of the features are positive by deﬁnition. To circumvent
this problem, we limit ourselves to vectors a having unit length.
The ﬁrst principal component is, therefore, the linear combination of features y
whose weight of vector a solves the following problem:
maxa
a′Sa
a′a .
(3.68)
Let us call this ratio λ. The ﬁrst-order condition is
∂λ
∂α = a′a(2Sa) −a′Sa(2a)
(a′a)2
= 0,
(3.69)
or
Sa −a′Sa
a′a a = 0,
(3.70)
which is, by deﬁnition of λ ,
Sa −λa = 0.
(3.71)
This expression is familiar; it is the problem of ﬁnding eigenvalues. Therefore, the
λ that we are looking for, namely the maximum variance of the linear combination,
is none other than the maximum eigenvalue of the variance–covariance matrix of
the features. Accordingly, the weight vector a corresponding to the ﬁrst component
is the eigenvector associated with this eigenvalue. The ﬁrst principal component is
therefore
z = a′y.
(3.72)

82
FEATURE EXTRACTION, SELECTION, AND CREATION
How do we deﬁne the second principal component? Once we have found the com-
bined feature z1 = a′
1y explaining the maximum amount of variance that is present in
the original set of features, the second principal component is the combined feature
z2 = a′
2y, explaining the maximum amount of variance achievable by any linear com-
bination such that a′
1a2 = 0, that is, in an orthogonal direction to the ﬁrst principal
component.
Showing this property of the second principal component is straightforward: using
a Lagrange multiplier γ, we differentiate with respect to a2
∂
∂a2
a′
2Sa2
a′
2a2
+ γa′
1a2

= 0,
(3.73)
which gives the solutions γ = 0 and Sa2 = λ2a2.
As we can place the eigenvalues in decreasing order λ1 > λ2 > · · · > λk, we
can readily see how to ﬁnd the lesser order principal components; they are simply
constructed using the same formula as for the ﬁrst principal component z above, but
instead using the corresponding eigenvector. Any given principal component of order
n represents the linear combination of features having the largest variance among all
linear combinations orthogonal to z1, z2, . . . , zn−1. There is, of course, a maximum
of k eigenvectors for any variance–covariance matrix S. A nonsingular matrix S will
have k different principal components along k dimensions.
There is an important relationship between the size of the eigenvalues and the
percentofvarianceexplainedbytherespectiveprincipalcomponents.Wehavealready
shown that each eigenvalue is equal to the variance of each corresponding component.
As we know from elementary linear algebra that
k

j=1
s2
zj =
k

j=1
λj = tr(S),
(3.74)
we can report the percent of variance explained by the p ﬁrst components as
p
j=1 λj
k
j=1 λj
.
(3.75)
Another way to show the same information is by displaying a so-called “scree”
plot (Fig. 3.15).
A nice feature of principal components is immediately visible from such a plot.
The ﬁrst few principal components generally explain most of the variance that is
present in the data, whereas the cumulative impact of the principal components that
are on the portion of the screen plot with a small slope add little to the explana-
tion. This is the reason why principal components are usually an effective way of
reducing the dimensionality of a feature set. We can use the ﬁrst principal com-
ponents (up to where contribution becomes negligible) in lieu of the whole data
set.

FEATURE EXTRACTION
83
FIGURE 3.15
A typical scree plot.
3.1.10.2
Linear Discriminant Analysis (LDA)
The principal component
analysis results in a linear subspace of maximum variance for a set of data points.
When the data points (also called vectors, examples, or samples) are attached with
class labels, it is often hoped that the projections of points onto the subspace are max-
imally separated into different classes. PCA cannot fulﬁll this purpose because it does
not consider the class labels in estimating the subspace. The class of techniques for
learning subspace by maximizing separability is generally called discriminant anal-
ysis. The simplest and most popular one, linear discriminant analysis (LDA), also
called as Fisher discriminant analysis (FDA), learns a linear subspace that maximizes
the Fisher criterion [33].
LDA is a parametric feature extraction method, assuming Gaussian density
functions with equal covariance for all classes. The extracted subspace performs
fairly well even in situations where the classes undergo non-Gaussian distributions,
or the covariance matrices are not equal. Particularly, LDA is a good feature extraction
or dimensionality reduction method in character recognition. The subspace features
(projected points in subspace) can be classiﬁed by either linear or nonlinear classiﬁers.
Assume that each point belongs to one of M classes {ωi|i = 1, . . . , M}. The data
points are in an original D-dimensional space, and a d-dimensional subspace (d < D)
is to be extracted. Each class has a Gaussian density function
p(x|ωi) =
1
(2π)D/2|i|1/2 exp

−1
2(x −μi)T −1
i (x −μi)

,
(3.76)
where the mean vector μi and the covariance matrix i can be estimated by maximum
likelihood (ML) from the points of class ωi:
μi = 1
Ni

xn∈ωi
xn,
(3.77)

84
FEATURE EXTRACTION, SELECTION, AND CREATION
i = 1
Ni

xn∈ωi
(xn −μi)(xn −μi)T ,
(3.78)
where Ni denotes the number of points in class ωi. A within-class scatter matrix Sw
and a between-class scatter matrix Sb are then deﬁned by
Sw =
M

i=1
Pii,
(3.79)
Sb =
M

i=1
Pi(μi −μ0)(μi −μ0)T ,
(3.80)
where Pi = Ni
N (N is the total number of points), and μ0 = M
i=1 Piμi.
It can be proved that Sw and Sb sum up to a total scatter matrix St:
St = 1
N
N

n=1
(xn −μ0)(xn −μ0)T = Sw + Sb,
(3.81)
and Sb is equivalently computed by
Sb =
M

i=1
M

j=i+1
PiPj(μi −μj)(μi −μj)T .
(3.82)
Denoting the axis vectors of the d-dimensional subspace by w1, . . . , wd, as the
columns of a D × d transformation matrix W, the task of subspace learning is to
optimize W under a criterion. In the subspace, the matrices of within-class scatter,
between-class scatter, and total scatter are transformed to WT SwW, WT SbW, and
WT StW, respectively. Whereas PCA maximizes the total variance in subspace, which
equals tr(WT StW), LDA maximizes a separability measure
J1 = tr

(WT SwW)−1WT SbW

.
(3.83)
It is shown that the selected subspace vectors that maximize J1 are the eigenvectors of
matrix S−1
w Sb corresponding to the d largest eigenvalues [33]. Also, when replacing
Sb with St in J1, the eigenvectors remain the same with those of S−1
w Sb.
Though both Sw and Sb are symmetric, the matrix S−1
w Sb is generally not. Hence,
to compute the eigenvectors of S−1
w Sb is a little more complicated than a PCA that
diagonalizes a symmetric positively deﬁnite matrix. Equivalently, the subspace vec-
tors of LDA can be computed in two steps by diagonalizing symmetric matrices:
whitening Sw followed by PCA on the whitened space. Denote the eigenvectors of
Sw by the columns of an orthonormal matrix P and the eigenvalues by the diagonal

FEATURE EXTRACTION
85
elements of a diagonal matrix 1. By diagonalization,
Sw = P1PT .
(3.84)
Using a transformation matrix W1 = P−1/2
1
, Sw is transformed to an indentity
matrix:
WT
1 SwW1 = I.
(3.85)
Accordingly, Sb is transformed to
WT
1 SbW1 = S2.
(3.86)
S2 remains a symmetric matrix. Diagonalizing S2 and retaining its d eigenvectors,
corresponding to the largest eigenvalues, as the columns of a D × d matrix Q, the
ﬁnal transformation matrix of LDA is the combination of W1 and Q:
W = W1Q = P−1/2
1
Q.
(3.87)
A D-dimensional data vector x is projected onto the d-dimensional subspace by
y = WT x.
Figure 3.16 shows an example of LDA as compared to PCA. In 2D space, there
are two classes of Gaussian distributions, the 1D subspace learned by LDA has better
separability than that learned by PCA.
Though LDA performs fairly well in practical pattern recognition problems, it has
some inherent drawbacks: (1) the dimensionality of subspace is limited by the number
of classes because the rank of Sb is at most min(D, M −1); (2) the matrix Sw may
be singular when the number of samples is small; (3) by assuming equal covariance,
it ignores the difference of covariance between different classes; (4) when the data
distribution of each class deviates largely from Gaussian, LDA performs inferiorly.
In recent years, many works have been done to overcome the above drawbacks.
Kimura proposed a hybrid of LDA and PCA for extracting more than M −1 features
for small number of classes [53]. Heteroscedastic discriminant analysis (e.g., [78])
FIGURE 3.16
Subspace axes of PCA and LDA for data points of two classes.

86
FEATURE EXTRACTION, SELECTION, AND CREATION
considers the class means as well as the difference of covariance in estimating the
subspace. Nonparametric discriminant analysis assumes that the distribution of each
class is arbitrary and computes a between-class scatter matrix from the vectors con-
necting near points of different classes [33]. The decision boundary feature extraction
method [70] and the Gaussian mixture discrminant analysis [42] can be viewed as the
special cases of nonparametric discriminant analysis.
3.1.11
Kernels
Kernel methods have enjoyed considerable popularity recently. They can be described
as a way to transpose a hard, nonlinear problem in lower dimensions to an easier, lin-
early separable problem in higher dimensions. This passage to higher dimensions is
not innocuous; It should raise a red ﬂag in the mind of readers familiar with the statis-
tical learning literature, as transposing a problem to a higher dimension is usually a
bad idea. This is the proverbial “curse of dimensionality.” The reason for this problem
lies in the fact that data become scarce in higher dimensions, and meaningful rela-
tionships become harder to identify. However, kernel methods offer in many cases,
not all cases, unfortunately, a reasonable compromise because of their use of clever
data representations and computational shortcuts.
3.1.11.1
Basic Deﬁnitions
Suppose ﬁrst that our n-dimensional data is x. Let
us ﬁrst consider an embedding map  such that
 : x ∈ℜn →(x) ∈F ⊆ℜN,
(3.88)
where the embedding dimension N can be bigger than the data dimension n, and
 is general (nonlinear). The choice of the map depends on the problem at hand.
Let us immediately give an example of this. For instance, if we know that our data
consist of bidimensional points of coordinates (x1, x2) belonging to two classes that
are quadratically separable, we can use the following :
 : ℜ2 →ℜ3,
(3.89)
(x1, x2) →(z1, z2, z3) = (x2
1,
√
2x1x2, x2
2).
(3.90)
Consider the problem of ﬁnding the separating hyperplane in this three-
dimensional feature space: an expression of this hyperplane is w′z + b = 0; sub-
stituting, we get
w1x2
1 + w2
√
2x1x2 + w3x2
2 + b = 0,
(3.91)
which is the equation of an ellipse with respect to the original coordinates. It is,
therefore, possible to use algorithms such as perceptron learning to ﬁnd this boundary
in three-dimensional space and separate the classes. This is known as the “kernel
trick.”

FEATURE EXTRACTION
87
Let us deﬁne now what we mean by a kernel function:
Deﬁnition 1: A kernel is a function κ that for all x, y, ϵX satisﬁes
κ(x, y) = ⟨(x), (y)⟩,
(3.92)
where the angular brackets denote the inner product and  is a mapping from X to
an inner product feature space F:
 : x ∈ℜn →(x) ∈F ⊆ℜN.
(3.93)
Let us verify that our proposed mapping corresponds to a kernel
κ(x, y)
= ⟨(x), (y)⟩=

(x2
1,
√
2x1x2, x2
2), (y2
1,
√
2y1y2, y2
2)

= x2
1y2
1 + 2x1x2y1y2 + x1
2y2
2 = (x1y1 + x2y2)2 = ⟨x, y⟩2 .
Note the important fact that once we know that this kernel function corresponds to
the square of the inner product in data space, we do not need to evaluate the function
 in feature space in order to evaluate the kernel. This is an important computational
shortcut.
Deﬁnition 2: If we have p observations in the original space, the kernel matrix K
(which is the Gram matrix in the feature space) is deﬁned as the p × p matrix whose
entries are the inner products of all pairs of elements in feature space:
Kij = κ(xi, xj) =

(xi), (xj)

.
(3.94)
An important property of kernel matrices is semipositive deﬁnitiveness. As a matter
of fact, an important theorem states that any positive semideﬁnite matrix can be
considered as a kernel matrix. This is given by Mercer’s theorem, a fundamental
result in the ﬁeld. This is interesting, as we can use a matrix K without caring about
how to compute the embedding function , which is a good thing in the cases where
the embedding is inﬁnite dimensional (as for the Gaussian kernel). What is important
in a particular kernel is the fact that it deﬁnes a measure of similarity to see that it may
be useful to remember the relationship of the ordinary dot product and the cosine of
the angle between two ﬁnite-dimensional vectors in real space.
An illustration of this is to consider the two extreme cases of kernel matrices.
Suppose that we consider the identity matrix Ip×p as a kernel. It is obviously positive
semideﬁnite, so it satisﬁes the conditions of Mercer’s theorem. It deﬁnes the following
trivial measure of similarity: any vector is similar to itself and is different from all
others. This is not very helpful. The other extreme case is with a quasi-uniform matrix;
in this case, all vectors have the same degree of similarity to all others, and this is
also not very helpful.
Kernel matrices are convenient, but the ﬂipside is that they are also information
bottlenecks. Kernel matrices are the interfaces between the data and the learning
algorithms that operate in feature space. The original data is, therefore, not accessible

88
FEATURE EXTRACTION, SELECTION, AND CREATION
to the learning algorithm. If a structure that is present in the data is not reﬂected in the
kernel matrix, then it cannot be recovered. The choice of the kernel matrix is therefore
crucial to the success of the analysis.
This evacuation of the original data may be an advantage in certain cases, for
instance, when the original data objects are nonvectorial in nature (strings, texts,
trees, graphs, etc.), and are therefore not amenable to direct analysis using vector
methods.
3.1.11.2
Kernel Construction
How do researchers select kernels? Ideally, ker-
nels should be tailored to the needs of the application and be built according to the
characteristics of the particular domain under examination. Remember how we came
up with our quadratic embedding above, and how we found out it corresponded
to a squared dot product in data space; it is because we knew that the underlying
relationship was quadratic. The ideal kernel for learning a function f is therefore
κ(x, y) = f(x)f(y). This suggests a way of ascertaining the value of a particular kernel
matrix by comparing it to this ideal kernel, but in most cases we do not know the form
of the function we want to learn.
Anotherrouteistostartfromsimplekernelsandcombinethemtogetmorecomplex
kernels. There are often families of kernels that are recognized to be useful for some
applications, and prototypical members of a family can be used as a starting point.
To illustrate and close this section, let us state this important theorem about closure
properties of kernels:
Theorem: Let κ1 and κ2 be kernels over X × X, f be a real-valued function on
X, A be a positive semi-deﬁnite matrix, and p be a polynomial. Then the following
functions are kernels:
κ(x, y) = κ1(x, y) + κ2(x, y),
(3.95)
κ(x, y) = aκ1(x, y),
(3.96)
κ(x, y) = κ1(x, y) ⊙κ2(x, y),
(3.97)
κ(x, y) = f(x)f(y),
(3.98)
κ(x, y) = x′Ay,
(3.99)
κ(x, y) = p(κ1(x, y)),
(3.100)
κ(x, y) = exp(κ1(x, y)),
(3.101)
κ(x, y) = exp

−∥x −y∥2
2σ2

.
(3.102)
Note: In (3.97), the operator represents the Hadamard product, which is the term-by-
term product of the elements of each matrix. The kernel in (3.102) is known as the
Gaussian kernel, used in radial basis function networks.

FEATURE EXTRACTION
89
3.1.11.3
An Application: Kernel PCA
We now outline the nonlinear princi-
pal components methods deﬁned using kernels. The reader will immediately see the
similarity with the derivation that was explained above in the linear case. However,
we suppose that in data space the direction explaining the most variance is curvilin-
ear. Therefore, applying ordinary PCA will result in a model error. Again, we use
the “kernel trick”: We deﬁne a feature space (the embedding ) that is a nonlinear
transformation of the data space, and it is in this feature space that the ordinary version
of PCA is then applied.
As in the linear case, we need the variance–covariance matrix of the data this time
in feature space. It is
C = 1
n
n

i=1
(xi)(xi)′,
(3.103)
We next compute the principal components by setting the eigenvalue problem:
λv = Cv = 1
n
n

i=1
(xi)(xi)′v.
(3.104)
Barring the case λ = 0, this says that all eigenvectors v must lie in the span of
(x1), . . . , (xn). This allows us to deﬁne the equivalent system by multiplying
both sides by (xk), and we get
λ(xk)v = (xk)Cv with v =
n

i=1
ai(xi),
(3.105)
where (ai) are the expansion coefﬁcients of (v). We can therefore rewrite the
problem as
λa = Ka.
(3.106)
One thing to note is that the nonlinearity is entirely taken into account by the kernel
matrix K. It is therefore essential to keep in mind that again, the choice of the kernel
must be made wisely. This is after all the perennial dilemma of nonlinear methods:
they explode the limitations of linear methods, but then one runs into the question of
which nonlinear relationship to specify.
Another important remark is that although the problem of linear PCA has the
dimensionality of the data space, kernel nonlinear PCA has a much larger dimension-
ality, that is, of the number of elements in the data space. This can clearly become
a problem and has prompted lines of enquiry along the idea of using sparse kernel
matrices set within a Bayesian framework.

90
FEATURE EXTRACTION, SELECTION, AND CREATION
3.2
FEATURE SELECTION FOR PATTERN CLASSIFICATION
Given a feature set F, which contains all potential N features and a data set D, which
contains data points to be described using available features from F, a feature selection
process selects a subset of features G from F, such that the description of D based
on G is optimized under certain criterion J, for example, best predictive accuracy of
unseen data points within D or minimum cost of extracting features within G.
In many real-world applications, such as pattern recognition, image processing,
and data mining, there are high demands for the capability of processing data in
high-dimensional space, where each dimension corresponds to a feature, and data
points are described with hundreds or even thousands of features. However, the curse
of dimensionality, ﬁrst termed by Bellman [10], is commonly observed: The com-
putational complexity and number of training instances to maintain a given level of
accuracy grow exponentially with the number of dimensions.
Feature selection, also called dimensionality reduction, plays a central role in
mitigating this phenomenon. It augments the performance of the induction system
with respect to three aspects [12]:
• Sample complexity: less training samples are required to maintain a desired
level of accuracy;
• Computational cost: not only the cost of the inductive algorithm but also the
effort of collecting and measuring features are reduced;
• Performance issues:
◦Overﬁtting is avoided since representation of data is more general with fewer
features.
◦Accuracy is enhanced as noisy/redundant/irrelevant features are removed.
Section 3.2.1 describes four dimensions along which feature selection algorithms
can be categorized. These are (a) objectives; (b) relationship with classiﬁers; (c)
feature subset evaluation; and (d) search strategies. The latter three dimensions are
of key importance and hence are studied in detail in Sections 3.2.1.1, 3.2.1.2, and
3.2.1.3.
3.2.1
Review of Feature Selection Methods
Feature selection techniques can be classiﬁed along four different dimensions.
• Objectives;
• Relationship with classiﬁer;
• Evaluation criteria (of feature subsets);
• Search strategies.

FEATURE SELECTION FOR PATTERN CLASSIFICATION
91
Feature selection algorithms have three different objectives [62]: (a) minimum
error (ME): given a subset with ﬁxed size or a range of different sizes, seek a minimum
error rate; (b) minimum subset (MS): given acceptable performance degradation, seek
a minimum subset; (c) multicriteria compromise (CP) of the above two objectives.
Each objective corresponds to a type of optimization problem. The ﬁrst objective is
an unconstrained combinatorial optimization problem; the second objective is a con-
strained combinatorial optimization problem; where the third objective, as indicated
by its name, is a multicriteria optimization problem.
Besides objectives, feature selection approaches adopt different strategies based
on (a) how feature selection interacts with classiﬁers; (b) how feature subsets are
evaluated; and (c) how the search space is explored. These strategies play crucial roles
in any feature selection algorithm and hence will be studied in detail in the sequel.
3.2.1.1
Relationship with Classiﬁers
Based on the relationship with classi-
ﬁers, feature selection approaches can generally be divided into “ﬁlters” and “wrap-
pers.” A ﬁlter algorithm operates independently of any classiﬁcation algorithm. It
evaluates feature subsets via their intrinsic properties. Irrelevant/redundant features
are “ﬁltered” out before the features are fed into the classiﬁer. On the contrary, a
wrapper algorithm is normally tied to a classiﬁer, with the feature selection process
wrapped around the classiﬁer, whose performance directly reﬂects the goodness of
the feature subsets. Typical ﬁlter algorithms include [3, 16, 54]. Typical wrapper
algorithms include [47, 56, 57, 75, 113].
Blum and Langley [12] grouped feature selection approaches into three categories:
ﬁlter, wrapper, and embedded. In the third category, feature selection is embedded into
the classiﬁer. Both of them are implemented simultaneously and cannot be separated
[66]. Typical embedded algorithms are decision trees [92, 69, 13] as well as algorithms
used for learning logic concepts [82, 114].
Some researchers adopt a hybrid (not an embedded) form of feature selection that
utilizes aspects of both the ﬁlter and wrapper approaches in the hope of enjoying the
beneﬁts of both [15, 20, 99].
We merge the taxonomies proposed by [12, 77], resulting in a division of the
feature selection into four categories: embedded, ﬁlter, wrapper, and hybrid, as shown
in Figure 3.17.
Note that in embedded approaches, feature selection is tied tightly to, and is per-
formed simultaneously with, a speciﬁc classiﬁer. When the classiﬁer is constructed,
the feature selection, hence dimensionality reduction, is achieved as a side effect.
Decision trees are typical examples. However, the decision tree induction algorithm
ID3 cannot be relied upon to remove all irrelevant features present in Boolean con-
cepts [4]. Hence, appropriate feature selection should be carried out before developing
a decision tree. Also, due to the inseparability of feature selection from classiﬁcation
in embedded approaches, we will only deal with the other three approaches in detail.
(1) Filter approaches
A ﬁlter feature selection algorithm, as indicated by its name, ﬁlters out features before
their use by a classiﬁer. It is independent of any classiﬁer and evaluates feature subsets

92
FEATURE EXTRACTION, SELECTION, AND CREATION
FIGURE 3.17
The four approaches to feature selection.
via their intrinsic properties. These properties act as an approximation of the value of
the features from the point of view of a “generic classiﬁer.”
Relief [55] is a typical ﬁlter approach as it selects features that are statistically
relevant to the target concept and feeds them into the classiﬁer. Both FOCUS [4, 3]
and its variant—C-FOCUS [7] are typical ﬁlter approaches. In FOCUS, the ﬁltered
features are given to ID3 [93] to construct decision trees. In C-FOCUS, a neural
network was used as a classiﬁer. Cardie [16] used decision trees to specify input
features for a nearest neighbor classiﬁer. Branch and bound and most of its improved
versions [19, 85, 102, 121] also employ a ﬁlter framework.
Any function used for feature subset evaluation must satisfy a monotonicity con-
dition; that is, given two candidate subsets f1 and f2, if f1 is a subset of f2, J(f1)
must be worse than J(f2). Bhattacharya distance [19, 102] and divergence [102] are
popular evaluation criteria. Other metrics used to evaluate feature subsets include
consistency [68, 76] and correlation [39, 40, 123], all of which will be studied in the
next section.
(2) Wrapper approaches
In a wrapper, the feature selection process is wrapped around a learning algorithm.
The feature subsets are evaluated via the performance of the learning algorithm. A
wrapper method naturally entails training and implementation of learning algorithms
during the procedure of feature selection.
Kohavi and John [56] tested two classiﬁers: decision trees and naive Bayes. They
reported that their wrapper is able to remove correlated features that impair the perfor-
mance and hence achieve signiﬁcant accuracy improvement. They also observed two
problems with wrappers: high computational cost and overﬁtting. They stated that the
latter problem occurs for small training sets and suggested enlarging the training set
to alleviate the effect. Other wrappers use various forms of classiﬁers such as neural
networks [43, 87, 88, 98] and nearest neighbor classiﬁers [1, 45, 65].
The main problem of a wrapper is that its cost may be prohibitively high because it
involves repeated training and implementation of the classiﬁer. Various solutions have

FEATURE SELECTION FOR PATTERN CLASSIFICATION
93
been suggested. Kohavi and John [56] suggested replacing k-fold cross-validation
with hold-out validation. k-fold cross-validation partitions available data into k sets.
In each run k −1 sets are used for training and the remaining kth set is used for
testing. Finally, an average accuracy over k runs is obtained. The hold-out test, on the
contrary, only involves one run in which 50–80% of total data is used for training and
the remainder for testing. Although more efﬁcient, the hold-out test is not statistically
optimal.
The training of neural networks is known to be computationally expensive. Hence,
wrappers with neural networks as classiﬁers employ various tricks to avoid repeated
training. In [43], the authors incorporated a weight analysis-based heuristic, called
artiﬁcial neural net input gain measurement approximation (ANNIGMA), in their
wrappermodel.Oliveiraetal.[88]usedsensitivityanalysistoestimatetherelationship
between the input features and the performance of the network.
(3) Hybrid approaches
As both ﬁlters and wrapper have their own limitations, some researchers propose to
hybridize them to adopt the beneﬁts of both.
Zhang et al. [127] proposed a ReliefF-GA-wrapper method. Features were ﬁrst
evaluated by ReliefF [58], the extension of Relief [55]. The resulting scores of the
evaluated features were fed into a genetic algorithm (GA) so that features with higher
rankingsreceivehigherprobabilitiesofselection.Theﬁtnessisacompromisebetween
the size of the feature subset and the accuracy of a nearest mean classiﬁer [33]. The
authors demonstrated, through their empirical study, that this hybrid ﬁlter-wrapper
model outperformed some pure ﬁlters and wrappers, for example, ReliefF or GA-
wrapper alone.
In contrast to Zhang et al.’s work [127] in which the wrapper amends the ﬁlter, Yu
et al. [122] developed a two-pass feature selection with the wrapper followed by the
ﬁlter. In the ﬁrst pass, wrapper models with forward search were used, with multilayer
perceptron neural network and support vector machines as the classiﬁers. The size
of the feature set after this pass was shrunk dramatically, whereas the classiﬁcation
accuracy was increased slightly. In the second pass, the Markov blanket ﬁlter method
wasusedtoselecthighlyrelevantfeaturesfromthefeaturesubsetobtainedfrompass1.
The resulting feature set was further minimized while maintaining the classiﬁcation
accuracy. Hence, the algorithm achieved the smallest possible feature subset that
provided the highest classiﬁcation accuracy. Other papers worthy of mention include
[15, 20, 99].
(4) Summary
Some researchers [40, 76, 123] believe that the ﬁlter approaches are preferable to
wrapper approaches because (a) they are not biased toward any speciﬁc classiﬁer;
and (b) they are computationally efﬁcient. Although the ﬁlters are more efﬁcient and
generic than the wrappers, especially in high-dimensional spaces, they are criticized
by another school of researchers [1, 47, 71] in that they may yield a bias that is
completely different from that required by a speciﬁc classiﬁer. The resulting feature
subset is, therefore, not optimal in the sense that the classiﬁer does not fulﬁll its
best potential with the given inputs. A reasonable compromise (discussed in previous

94
FEATURE EXTRACTION, SELECTION, AND CREATION
FIGURE 3.18
Feature subset evaluation criteria.
section) is to hybridize a ﬁlter with a wrapper. For example, a ﬁlter can be used ﬁrst
to shrink the search space somewhat. Then a wrapper follows by selecting features
that best suit a speciﬁc classiﬁer.
3.2.1.2
Evaluation Criteria
The goodness of a feature subset may or may not
be tied to the performance of a classiﬁer. Filter feature selectors, being independent
of any classiﬁer, utilize the intrinsic properties of the data. In [23], the data intrinsic
measures include interclass distance, probabilistic distance, probabilistic dependence,
and entropy. Ben-Bassat [11] proposes three categories: information or uncertainty
measures, distance measures, and dependence measures. Besides the classiﬁer’s error
rate, Dash and Liu [21] divide the evaluation functions into four categories: distance,
information, dependence, and consistency. Dependence denotes the correlation be-
tween two random variables. Although dependence is considered as an independent
category, as suggested by Ben-Bassat [11] and Dash and Liu [21], it is always in the
form of distance or information theoretic measures. Hence, we divide data intrinsic
measures into three categories: (a) distance measures; (b) information measures; and
(c) consistency measures (see Fig. 3.18).
(1) Data intrinsic distance measures
Distance measures the degree of similarity/dissimilarity between sets of data. In fea-
ture selection, the greater the distance, the better the class divergence, and hence the
feature subset. Although Kira and Rendell used distance between training instances
[55], most distance metrics come in two broad categories:
• Interclass distance;
• Probabilistic distance.

FEATURE SELECTION FOR PATTERN CLASSIFICATION
95
Interclass distance is the average distance of distances between different classes,
for example, the Minkowski distance. A Minkowski distance of order m between two
classes is
dm =
 N

i=1
|xi −yi|m
 1
m
,
(3.107)
where (x1, x2, . . . , xN) and (y1, y2, . . . , yN) are feature subsets used to describe
classes and N is the number of features in the set.
Speciﬁcally, when m equals 1, the Minkowski distance reduces to the city block
distance
d1 =
N

i=1
|xi −yi|.
(3.108)
When m equals 2, the Minkowski distance reduces to the Euclidean distance
d2 =

 
 
!
N

i=1
|xi −yi|2.
(3.109)
The Euclidean distance d can be computed from the ratio of scatter matrices [90].
Scatter matrices denote how feature vectors are scattered in the feature space. The
between-class scatter matrix Sb is
Sb =
m

i=1
P(ωi)vivT
i .
(3.110)
The within-class scatter matrix Sω is
Sω =
m

i=1
P(ωi)Wi.
(3.111)
Here P(ωi) is the a priori probability of class ωi, vi is the mean vector of class ωi and
Wi is the covariance matrix for class ωi.
The Euclidean distance can then be computed as
d = |Sb + Sω|
|Sω|
.
(3.112)
Note that d is to be maximized as larger d means more class separation (Sb) and
more compact classes (Sω). An example of the application of this concept for the
evaluation of signal-based features is [38].

96
FEATURE EXTRACTION, SELECTION, AND CREATION
Probabilistic distance is the distance between probability density functions of the
classes. Typical probabilistic distance metrics are divergence and Bhattacharya dis-
tance.Divergencebetweenclasses, ω1 andω2 inaformofaKullback–Leiblerdistance
is given as [11, 86]
Jd(ω1, ω2) =
1
2(v2 −v1)T (W−1
1
+ W−1
2 )(v2 −v1)
+ 1
2trace(W−1
1 W2 + W−1
2 W1 −2I),
(3.113)
where trace(x) is the trace of the matrix x and is equal to the sum of its eigenvalues.
The divergence generally assumes a Gaussian distribution of samples. Bhattacharya
distance [19, 79, 102, 103] is
Jb(ω1, ω2) = 1
8(v2 −v1)T

W−1
1
−W−1
2
2
−1
× (v2 −v1) + 1
2 ln |W1 + W2|
4√|W1W2|.
(3.114)
In [90], Piramuthu performed feature selection using SFS on decision trees. Several
interclassdistances(e.g.,Minkowskidistance,cityblockdistance,Euclideandistance,
and Chebychev distance) and probabilistic distances (e.g., Bhattacharyya distance,
Matusita distance, divergence distance, Mahalanobis distance, and Patrick–Fisher
measure) were tested. Results on some real data sets showed that interclass distances
generally yielded better performance than probabilistic distances.
(2) Information theoretic measures
A basic concept in information theoretic measures is entropy. The entropy of a random
variable X denotes the amount of uncertainty in X. It is computed as
H(X) = −

ωi
P(ωi) log2 P(ωi),
(3.115)
where P(ωi) is the proportion of X belonging to class ωi.
As shown in Table 3.1, suppose there are eight samples with Boolean feature F1
and discrete feature F2 of values a, b, c, d. There are altogether two classes: C1 and
C2. As there are three samples belonging to C1 and three belonging to C2, the entropy
for the random class variable C is
H(C) = −3
6log2
3
6 −3
6log2
3
6 = 1.
(3.116)
Note that the range of H(C) is (0, ∞). A larger value means more uncertainty and 0
means no uncertainty at all.
If one of the features is known (i.e., selected), the amount of remaining uncertainty
in C is called the conditional entropy of C, denoted as H(C|F)
H(C|F) =

f
P(F = f)H(C|F = f).
(3.117)

FEATURE SELECTION FOR PATTERN CLASSIFICATION
97
TABLE 3.1
Classes and features.
Samples
F1
F2
Classes
1
True
a
C1
2
False
b
C2
3
False
a
C2
4
True
d
C2
5
False
c
C1
6
True
b
C1
It is expected that the uncertainty in C is reduced, given either one of the features.
Hence, the goodness of a feature F can be quantiﬁed by the reduction in the entropy
of class C, given that F is known. This quantity is called mutual information or
information gain:
I(F; C) = H(C) −H(C|F).
(3.118)
Larger mutual information or information gain indicates better features.
Mutual information is a popular metric in feature selection [27, 35, 71]. How-
ever, it favors features with many values. As seen from the example, F2 has four
values, whereas F1 only has two values. Hence, mutual information is alternatively
normalized as follows:
G(F; C) =
I(F; C)
−
F P(F)log2P(F).
(3.119)
G(F; C) is called the gain ratio. Note that the term in the denominator is actually the
entropy of feature variable F.
Both information gain and gain ratio reﬂect the correlation between a feature and
the class. However, they disregard the interaction between features. Hall and Smith
proposed a correlation-based feature selection algorithm [41] that seeks maximum
correlation between features and classes and minimum correlation between the fea-
tures themselves. Similarly, Wu and Zhang [116] extended the information gain by
taking into account not only the contribution of each feature to classiﬁcation but also
correlation between features.
(3) Consistency
Consistency is the opposite of inconsistency. A feature subset is inconsistent if there
are at least two training samples that have the same feature values but do not belong
to the same class [22]. The inconsistency count is computed as follows.
Suppose there are n training samples with the same set of feature values. Among
them, m1 samples belong to class 1, m2 samples belong to class 2, and m3 samples
belong to class 3. m1 is the largest among the three. Hence the inconsistency count is
n −m1. The inconsistency rate is the sum of all inconsistency counts divided by the
size of the training set N.

98
FEATURE EXTRACTION, SELECTION, AND CREATION
The authors in [22] showed that the time complexity of computing the inconsis-
tency rate is approximately O(N). The rate is also monotonic and has some tolerance
to noise. However, also as the authors stated, it only works for features with discrete
values. Therefore, for continuous features, there is a need for a discretization process.
Finally, consistency bears a min-features bias [4]; that is, it favors the smallest fea-
ture subset as long as the consistency of the hypothesis is satisﬁed. The inconsistency
rate is used in [4, 76, 98].
(4) Measures tied to classiﬁers
When measures of feature subsets are tied to the classiﬁers, they are dependent on
the performance of the classiﬁer that uses the features.
When seeking minimum error (objective ME), the evaluation criterion relies on
the intrinsic properties of the data (ﬁlter approach) or classiﬁcation accuracy of the
classiﬁer (wrapper approach) [85, 102, 121].
If the goal is to seek a minimal feature subset with acceptable classiﬁcation perfor-
mance (objective MS), this is a constrained optimization problem. A user-speciﬁed
constant is required as a threshold for the classiﬁcation performance. For a given
feature subset f, the objective function usually takes the following form:
J(f) = L(f) + P(f),
(3.120)
where L(f) is the dimensionality of f and P(f) is a performance measure (e.g.,
accuracy or error rate) of the classiﬁcation algorithm using f.
Siedlecki and Sklansky [100] proposed to minimize a criterion function that com-
bines both dimensionality and error rate of f. In their work, L(f) is the dimensionality
of f and P(f) is a penalty function given by
P(f) = exp((errf −t)/m) −1
exp(1) −1
,
(3.121)
where t is a feasibility threshold, m is a scaling factor, and errf is the error rate of the
classiﬁer using f. The authors set t to ≥12.5% and m to 1% in different experiments
in order to avoid an excessively large region of acceptable performance. These two
parameters are application dependent and demand empirical tuning. If errf is less
than t, the candidate receives a small reward (a negative value). However, as errf
exceeds t, the value of P(f) (and hence J(f)) rises rapidly. Similar functions are also
employed in [29, 124].
If a compromise between classiﬁcation performance and the size of the feature
subset is to be pursued (objective CP), the ﬁtness of feature subset f is usually a
weighted linear combination of L(f) and P(f), as in [9, 28, 30, 91, 107],
J(f) = λL(f) + βP(f),
(3.122)

FEATURE SELECTION FOR PATTERN CLASSIFICATION
99
where L(f) is generally a linear function of the dimensionality of f and is given by
L(f) = |f|
N ,
(3.123)
where N is the total number of available features. P(f) is the error rate. Constants
λ and β satisfy different conditions, depending on the relative importance of their
associated terms, in the speciﬁc application. In [28], λ + β = 1. In [30], λ and β are
assigned to 2 and 0.2, respectively.
If J(f) is to be maximized, L(f) is the number of features that are not selected,
and P(f) is the accuracy, as in [107].
Rather than weighting L(f) and P(f) to form a composite objective function,
Oliveira et al. [88] suggest the employment of a multiobjective optimization proce-
dure, where more than one objectives are present and a set of Pareto optimal solutions
are sought.
3.2.1.3
Search Strategies
Feature selection can be seen as a search process,
where the search space is made up of all available solutions, that is, feature subsets.
Figure 3.19 illustrates a search space. Suppose there are three available features.
Each point in the search space is a feature subset in which dark circles denote features
selected and white circles denote features removed. Starting from an initial point
within the space, the space is explored either exhaustively or partially with some
heuristics until a ﬁnal winner is found.
Similar to a search process, the core of a feature selection process contains four
components:
FIGURE 3.19
Search space.

100
FEATURE EXTRACTION, SELECTION, AND CREATION
FIGURE 3.20
Forward search.
• Initial points where the search starts from;
• Search strategies used to control the fashion of the exploration;
• Evaluation criteria used to provide feedback with respect to solutions;
• Termination criteria to halt the search.
The selection of initial points generally depends on the speciﬁc search strategies
utilized. Sequential search usually starts from an empty or a full feature set, whereas
parallel search starts from a random subset.
The search strategies can be generally divided into two broad categories: sequential
search and parallel search. The sequential search constructs the next states to be
visited either incrementally (i.e., forward search) or decrementally (i.e., backward
search) from the previous state, whereas a parallel search explores the space in a more
randomized fashion. Figure 3.20 demonstrates a potential path of forward, backward,
and parallel search, respectively.
Typical examples of sequential search include sequential forward selection, se-
quential backward elimination [2, 56, 62], and branch and bound [83, 57, 81, 96];
whereas random mutation hill climbing [28, 87, 101], genetic algorithms [62, 96, 123,
115, 118, 120], simulated annealing [80], and tabu search [126] are typical parallel
searches.
Features subsets are evaluated using the intrinsic properties of the data or the
performance of a certain classiﬁer. The former generally entails a “ﬁlter” approach,
and the latter is tied to a “wrapper” approach.
The search stops if the termination criterion is satisﬁed, which can be as
follows:
• All available subsets have been evaluated;

FEATURE SELECTION FOR PATTERN CLASSIFICATION
101
• A sufﬁciently good candidate is found, for example, the smallest subset with
classiﬁcation error below a threshold or a ﬁxed sized subset with the best pre-
dictive accuracy;
• A maximum number of iterations have been reached.
In the literature, the resulting feature subset may be validated on a separate set of
data from training samples [77]. This validation process, however, is not within the
domain of feature selection.
Search strategies are generally classiﬁed into three types: complete, heuristic, and
randomized.
A complete search explores the whole search space in a direct or indirect fashion.
The former typically involves exhaustive enumeration of feature subsets. The latter,
on the contrary, prune the search space, resulting in more efﬁcient searches, while
implicitly guaranteeing complete coverage of the search space, and hence the opti-
mality of the results. Branch and bound is a typical example of implicitly complete
search algorithms [19, 85, 102, 121].
In contrast to complete search, heuristic search is guided toward promising ar-
eas of the search space by heuristics. Only part of the search space is visited, and
optimality is generally not guaranteed. Typical examples of heuristic search ap-
proaches include sequential forward selection, backward elimination [56], and beam
search [1, 2].
In randomized search, the algorithm executes a stochastic process in which dif-
ferent training samples are used and different feature subsets are obtained for each
independent run. Given enough running time, randomized search may yield better
results than heuristic search, while requiring less computational time than complete
search. Typical randomized search techniques include random mutation hill climbing
[28, 87, 101], genetic algorithms [62, 96, 111, 112, 118, 120], simulated annealing
[80], and tabu search [126].
Taking into account the fact that any search is made up of a sequence of basic moves
within the search space, and that each move denotes a certain relationship between
successive states, we divide feature selection methods into two broad categories:
sequential search and parallel search.
Starting from an empty or full feature subset, or both, the sequential search typ-
ically moves from the previous state(s) to the next state(s) incrementally or decre-
mentally, by adding or by removing one or more features, respectively, from the
previous feature subset(s). On the path traversed from the starting point to the end
point, each state is either the superset or the subset of its successors. In contrast,
parallel search starts from one or more randomly generated feature sets. The next
states to be visited are not generated sequentially from previously visited states, as
in sequential search, but rather in a randomized fashion. Movement in search space
is implicitly guided by meta-heuristics so that future states are better (i.e., closer
to optimal states) than past states. Therefore, parallel search exhibits an overall im-
plicitly directional behavior. Its search is guided, by meta-heuristics, toward promis-
ing areas in the search space. Sequential search, however, bears an explicit search
direction.

102
FEATURE EXTRACTION, SELECTION, AND CREATION
FIGURE 3.21
The basic schemes of sequential search and parallel search.
Figure 3.21 shows the basic schemes of sequential and parallel search approaches.
In both schemes, x′ is a subset of F. G(x) in parallel search is a function used to
generate new feature subsets. Unlike sequential search, which yields new samples
from their predecessors, G(x) may or may not be applied to previous samples. The
program exits when it reaches the optimum of the evaluation function or when other
termination conditions are satisﬁed, for example, there is no noticeable improvement
in J(x), or the maximum number of iterations has been reached.
It can be seen that the major difference between sequential searches and parallel
searches is the way in which new feature subsets are generated. Table 3.2 summarizes
the differences between these two methodologies with respect to: starting points,
movement in the search space, relationship between successive states, and overall
search direction.
TABLE 3.2
Major differences between sequential search and parallel search.
Sequential search
Parallel search
Starting points
Empty, or full feature
Randomly generated
subsects, or both subsets
feature subsets
Move between
Incremental or decremental
Implicitly guided
successive states
operations
Relationship between
Subsets or supersets of
Depends on speciﬁc
succesive states
successor
techniques
Overall search
Forward or backward
Toward promising
direction
areas of the search
space

FEATURE SELECTION FOR PATTERN CLASSIFICATION
103
FIGURE 3.22
A taxonomy of feature selection search strategies.
In Figure 3.22, we provide a tree-based taxonomy of major search algorithms used
for feature subset discovery and optimization.
Note that sequential heuristic approaches are generally greedy, whereas parallel
search methodologies are generally random. Parallel search can be further divided into
two categories, according to its sampling methods. Although most parallel approaches
extract samples in the search space, Relief [55] samples the training space that is made
up of training instances.

104
FEATURE EXTRACTION, SELECTION, AND CREATION
3.3
FEATURE CREATION FOR PATTERN CLASSIFICATION
Feature creation is the synthesis of novel, usually more complex features, from a set of
older, usually simpler features (primitives), through some combinational, algorithmic,
statistical, or other operation.
Feature creation is not a new science, but it has gained increased interest in recent
years due to the dramatic increase in raw computational power, and the large number
of pattern processing (as opposed to the number crunching) problems that this increase
has allowed scientists to tackle. In addition, there has been a proliferation of class-
speciﬁc recognition solutions that resulted in systems that work extremely well for
a speciﬁc recognition problem (e.g., machine-printed English), but fail to transfer
to any other domain (e.g., handwritten Arabic) without considerable reengineering,
and especially of the feature extraction part of the original solution. Hence, feature
creation is, partly, an attempt to invent more general feature creation techniques that
can self-adapt to speciﬁc pattern recognition tasks via machine learning methods,
such as GAs and neural networks (NN).
3.3.1
Categories of Feature Creation
There is more than one way of categorizing feature creation methods. The method of
creation is an obvious criterion: GA based, NN based, or some other method. Another
criterion is the speciﬁc form of representation of candidate features: mask-based,
Boolean function-based or polynomial function-based, string-based or tree-based. It
is also possible to break down feature creation methods by application domain: word
recognition, fault detection, and the like. Finally, one could also look at the complete
system using these methods in order to identify the level of realism or real-world
applicability of the overall application: Some applications introduce seriously novel
methods but demonstrate them using toy problems; others involve serious applications
that could actually be used in, for example, a machine shop or a factory. Hence, (a)
method of feature creation, (b) representation of candidate feature, and (c) application
domain are the main dimensions of the space of research and applications developed
in the area of feature creation. The real-world applicability of such endeavors is
likely to improve with time and expertise, leading to the elimination of this whole
dimension. Hence, we choose not to include real-world applicability as a dimension
in Figure 3.23.
We choose to apply a hybrid method for division of the surveyed works in order to
ensure a minimum level of overlap between the (historical) themes of research that
we were able to identify. In the next section, we divide the surveyed publications into
four categories:
• Approaches that do not employ genetic algorithms or genetic programing: we
call these nonevolutionary methods;
• Approaches that employ genetic algorithms or genetic programing and use either
a string-like or tree-like notation to represent a polynomial feature function: we
call these polynomial-based methods;

FEATURE CREATION FOR PATTERN CLASSIFICATION
105
FIGURE 3.23
The space of feature creation methods.
• Approaches that utilize the full power of genetic programing and evolve both
the feature functions and the program that uses them: we call these full genetic
programing methods;
• Approaches that employ genetic algorithms or/and genetic programing to syn-
thesize all/most of a complete real-world pattern recognition system. We call
these evolvable pattern recognizers.
The last category is perhaps of special interest to people working in pattern recog-
nition, for it points the way toward the inevitable development of evolvable generic
pattern recognition software systems that require minimal human intervention (per-
haps only at setup time) and use theoretically sound techniques.
3.3.2
Review of Feature Creation Methods
We review four categories of feature creation methods: nonevolutionary, polynomial
based, genetic programing, evolvable pattern recognizers, and ﬁnally give a historical
summary.
3.3.2.1
Nonevolutionary Methods
In this category of feature creation meth-
ods, we outline some concrete examples as below.
(1) Uhr 1963
Uhr’s main motivation is to show that feature-extraction operators can arise from the
recognition problem at hand and be adapted, in form as well as weighting, in response
to feedback from the program’s operation on the pattern samples provided to it [110].
Uhr describes a complete pattern recognition system. However, our focus is its
feature extraction and generation capabilities. There are two sets of features here:
built-in features and created features. In the following, we shall explain the process
of feature extraction and creation.

106
FEATURE EXTRACTION, SELECTION, AND CREATION
FIGURE 3.24
Automatic operator (feature) creation procedure [110].
An unclassiﬁed input pattern is digitized into a 20 × 20 binary pixel matrix. The
pattern is drawn in black pixels on a white background. A bounding box is drawn
around the black input pattern: It is called a mask. A number of feature-extracting
operators are applied to the input pattern.
An operator is a 4 × 4 matrix of cells. Each cell can contain 0, 1, or blank. An
operator is generated (a) randomly (with some restrictions on the number of 1s in
the operator), (b) manually by the experimenter, or more interestingly, (c) through an
automated process described below (see Fig. 3.24):
• A 5 × 5 matrix of cells is randomly positioned within the input pattern: the black
pixels in the pattern are copied into 1s and the white pixels into 0 s;
• All 0 s connected to 1s are replaced with blanks;
• All remaining cells are replaced with blanks, with probability 0.5;
• If it turns out that the set of 1s in the operator is identical to the set of 1s in
an exiting or previously rejected operator, then this operator is rejected and the
process is restarted.
The program was tested using seven different sets of ﬁve hand-printed characters,
namely A, B, C, D, and E. The best accuracy of recognition returned by the program
was about 80%, which is unacceptable by today’s standards. However, this was not
intended as an engineering prototype, but rather as a proof of concept, and in that
sense it succeeded.

FEATURE CREATION FOR PATTERN CLASSIFICATION
107
FIGURE 3.25
A feature detection operator [105].
(2) Stentiford 1985
The purpose of this work is to show that an automated evolutionary approach can
succeed in the creation of the right set of features for an optical character recognition
system [105].
Here, a feature is deﬁned as a conjunction of a number of black-seeking or white-
seeking elements (see Figure 3.25). Each feature had a random arrangement of those
seeking elements that serve as an initial state. N such features were used. The patterns
to be recognized were a set of 34 different uppercase letters and digits. These patterns
were sampled from addresses on printed British mail. The alphanumeric characters
were digitized to lie within a 16 × 24 binary grid. A set of K reference patterns
were deﬁned through the selection of one character per class. The vector of reference
patterns is called Ci.
The reference pattern vector Ci gives an N-dimensional vector Fi in feature space,
where
Fi = {fi1, fi2, . . . , fij}, i = 1, 2, . . . , K,
(3.124)
and where fij is the response of the jth feature to the ith reference pattern. Also, the
jth feature relates to the vector response fj for the K reference patterns, where
fj = {f1j, f2j, . . . , fKj}, j = 1, 2, . . . , K.
(3.125)
When trying to recognize the Ci, the best discrimination is achieved when the
Fi have the maximum angle from each other. An orthogonal arrangement of the Fi
comes very close to optimum. It can further be shown that when K = N, a necessary
and sufﬁcient condition for such an arrangement is that f j are mutually orthogonal.
Results were obtained using a training set of about 30,000 characters and a test
set of 10,200 characters. A 1% testing accuracy was obtained with 3166 features and
319 reference characters.

108
FEATURE EXTRACTION, SELECTION, AND CREATION
(3) Gillies 1990
To our knowledge, Gillies was the ﬁrst to propose a mechanism that automatically
generates morphological template-based feature detectors for use in printed character
recognition [36]. And, if characters can be recognized, then so can be the printed
words made of segmentable characters.
To create a feature detector, the following steps are followed:
1. From a large database of 67,000 isolated character images, a random sample
of 20 images per distinct character (class) is chosen. The number of distinct
characters is 57;
2. Each of the images is normalized (without trying to maintain the aspect ratio)
to 16 × 24 pixel window;
3. The sum images for each character are formed. A sum image of a character is
formed by adding, pixel-by-pixel, the 20 sample images of this character;
4. The pixel-sum images are used to generate morphological template detectors:
(a) One of the pixel-sum images is chosen at random;
(b) A randomly sized and positioned window is placed on top of the sum-
image: this window frames the feature detector (under construction);
(c) Each pixel value in the sum-image lying within the window is tested: if it is
below a low threshold, then the corresponding pixel in the feature detector
is made a part of the “background” template; however, if the pixel value
is greater than a high threshold, then it is made a part of the “Foreground”
template. The rest of the pixels can be considered as don’t cares, as they
do not play a role in future processing.
Every feature detector generated above has a detection zone associated with it. It
is a 3 × 5 rectangular window placed at the origin of the feature detector window.
When a feature detector is applied to a character image, it reports either a match or a
no match. In doing so, it only takes into consideration the 3 × 5 detection zone at the
center of the feature detector window. In morphological terminology, a match occurs if
(Z ∩(IF) ∩(¯IB)) ̸= 0,
(3.126)
where Z is the detection zone, I is the input character image, F is the foreground
template, and B is the background template.
Features were generated through this process and compared against other features
that were manually created. Both sets were fed into one identically conﬁgured clas-
siﬁer, which was used for word classiﬁcation. The features that were automatically
generated performed better in every test.
The main problem with this method is that it only works for binary images. Another
problem is the way in which windows and feature detectors are generated, which is
essentially random. Finally, the generalizability of this approach to other recognition
problems, where the patterns are not as easily isolated and summed is doubtful.
Nevertheless, this is a pioneering effort and should be recognized as such.

FEATURE CREATION FOR PATTERN CLASSIFICATION
109
(4) Gader 1996
These authors assert that their work is based on the work of Stentiford and of Gillies
(described above). Both Stentiford and Gillies dealt with printed characters, although
in his work Gader proposes a somewhat novel technique for automatic generation of
features for handwritten digit recognition [34]. Furthermore, he evaluates the quality
of the features using both orthogonality and information.
Themethodcanbedividedintothefollowingstages:pixel-sumimageconstruction,
feature generation, and feature selection. Our focus here is on the ﬁrst two processes.
Pixel-sum Image Construction: From a database of binary images of digits, retrieve
1000 images per digit (from 0 to 9) and moment-normalize them so that they ﬁt in a
24 × 18 pixel standard window. Sum the normalized binary images for each class of
digits (e.g., all the 1000 images of a “1”) and normalize that sum, so that the value of
each summed pixel is in [−1, 1]. Call the resulting summed and normalized image
for each class the sum image for that class.
Feature Generation: For each class K(K = 0–9), generate PK random features:
1. To generate a single feature for a given class, a rectangular window is randomly
generated, with the height between 6 and 12 pixels and the width between 4
and 8 pixels. The position of the window is also randomly generated and is
expressed in terms of horizontal and vertical shifts from the upper left corner
of the sum image. The part of the sum image for a class that lies within this
window is considered a feature detector for this class.
2. A detection zone is assigned to each feature detector. The detection zone is a
rectangular window with the same center as that of the feature detector, but it
is 2 pixels taller and 2 pixels wider.
3. The probability P((Fj | XKi) that feature Fj present in digit image XKi is com-
puted by taking the maximum linear correlation between the feature detector
and the detection zone, and then scaling it between [0, 1].
4. A response vector is associated with each digit image XKi, with dimension
P representing the probability of the presence of the various features in that
image. P is given by the sum of all the PK features generated for each class
K. It is worth noting here that the probability P(Fj | K) of the presence of a
feature Fj in a class K is equal to the average of the response of the features
over N(= 50) digit image samples from the class K.
One hundred features were generated independently of any classiﬁer. These fea-
tures were fed into a 100-input, 10-class output, 2-hidden layer neural net, with 25
nodes in the ﬁrst layer and 15 in the second. Sets of experiments were carried out
using two standard sets of data. The features selected via the orthogonality measure
performed better that the ones selected using the information measure. Nevertheless,
in both cases the accuracy rate was high and compared favorably to other results
published at the time.
There are a number of weaknesses with this method. The features are extracted
from summed images, which require (a) that we have a sufﬁciently large and diverse

110
FEATURE EXTRACTION, SELECTION, AND CREATION
sample of images to sum, per pattern; (b) that we have a correct and feasible method
for summing the images; (c) that the pattern in the image be isolated and free from
any distracting attributes not belonging to the pattern itself (e.g., background).
3.3.2.2
Polynomial-Based Methods
We give three concrete examples of this
category of methods.
(1) Chang 1991
Chang et al. [17] pick up where Siedlecki et al. stopped. Their work explores the
application of genetic algorithms for both feature selection and feature creation.
New features are generated as polynomials of original features taken two at a
time. For example, if feature 1 has identiﬁcation bit string 01 and feature 2 has
identiﬁcation bit string 10, then string 01110101 represents a new feature feature 11 +
feature 21. The ﬁrst two bits identify the ﬁrst feature, the third bit identiﬁes the power
of the ﬁrst feature, and the second set of three bits has the same purpose with respect
to the second function. The last two bits identify the operation (+), which belongs to
the set {+, −, *, /}. Each member of the population is made of the original features
(which are implicit) and a ﬁxed number of new features, which are presented in the
way just explained.
The authors claim that the GA found a better set of features than those they were
originally looking for. In an experiment with 300 training patterns, the GA not only
found a useful set of features but also enabled a nearest-neighbor classiﬁer to achieve
higher accuracy of recognition than two other classiﬁers.
The main problems we see with this work are (a) the individual can only contain
a set of number of new features (in addition to all the original features); (b) the way
in which a new feature is created (from old features) only allows for a small section
of the large space of possible polynomial forms to be generated; (c) the linear string-
based fashion in which features are combined is quite limiting: The more recent
tree-based representations are much more ﬂexible and allow for a much wider range
and complexity of expression.
(2) Chen 2001
This paper appears to be the ﬁrst to introduce a technique for automatic creation
of functions of symptom parameters (SPs), to be used in the diagnosis of faults in
machinery [18]. Prior to this paper, the following process was done by an expert hand:
1. Measure signals of normal and afﬂicted machinery;
2. Using intuition, deﬁne several candidate SPs;
3. Evaluate the values of the SPs using the measured signals;
4. Check the sensitivity of each SP;
5. Adopt the most sensitive ones for fault diagnosis or else return to step 1.
The paper lists eight SPs (or primitive features) that are among the most widely
used in the ﬁeld of signal-based pattern recognition. These are variation rate, standard
deviation, mean, absolute mean, skewness, kurtosis and four other functions of peak

FEATURE CREATION FOR PATTERN CLASSIFICATION
111
and valley values. Each one of these values is normalized by subtracting the mean (of
the normal signal) from it, and then dividing that by the standard deviation (again, of
the normal signal).
A new SP is synthesized from these primitive SPs by evolving a population of
functions coded as trees of operators and terminals. The operators are the four ba-
sic arithmetic operators plus an implied parameter: power, which is expressed as a
number. The terminals are the primitive SPs.
In experimentation on faulty bearing, the best individuals generated automatically
by the GA showed high discriminative abilities. Its performance was measured against
that of the linear discriminative method, and it showed better discrimination abilities.
(3) Guo 2005
Machine condition monitoring (MCM) is an area of increasing importance in the man-
ufacturing industry; disastrous faults can be preempted, and condition-based rather
than periodic maintenance can reduce costs. The objective of this research is the au-
tomatic extraction of a set of features (using genetic programing) in order to predict,
with high accuracy, the condition (or class) of rolling bearings [38]. By running the
machine and recording the sound made by suspect bearings, one (expert) could tell
the condition of that bearing. There are ﬁve different faulty conditions and one normal
condition. The features are all extracted from an acoustic signal detected by a micro-
phone placed at a prespeciﬁed distance from the suspected bearings. Many features
can be extracted from the audio signal. Hence, the extraction and selection of features
can greatly affect classiﬁer performance (Table 3.3).
A population of agents (initially generated at random) goes through an evolutionary
process that results in individuals with competent discriminative ability. Bearing data
recorded from machines is used as input to the initial population. Each individual
represents a transformation network, which accepts input data and outputs a speciﬁc
class. A ﬁtness value, which represents the discriminative ability of an individual
(feature), is computed and assigned to that individual. Only those that have the best
ﬁtness (the elite) survive the selection process. Crossover, mutation, and reproduction
are used to create new and potentially different individuals from older ones. Then, the
processofﬁtnessevaluationandselectionrepeatsuntiltheterminationcriterionismet.
What is new in this work is that (a) all the features are automatically created
without any human intervention; (b) a novel, fast method is used to evaluate the
ﬁtness of candidate features, without resorting to the classiﬁer; (c) genetic program-
ing automatically decides, during evolution, whether to carry out feature creation or
selection.
The main limitations of this work are that the terminals are ﬁxed in number.
Though the whole feature expression (tree) can change, the terminals that are used in
constructing the trees cannot. This would have been tolerable if the terminals were
simple and generic, but they are not. They are all high-level statistical features (mo-
ments) that have worked very well for this application but may not work as well
for other signal-based classiﬁcation tasks. This highlights a need for creating new
terminals when applying this approach to another problem. Last, but not least, the
operators themselves, though judiciously chosen, are ﬁxed in nature and in number.

112
FEATURE EXTRACTION, SELECTION, AND CREATION
TABLE 3.3
Operator set for the GP [38].
Symbol
No.ofInputs
Description
+,-
2
Addition, substraction
∗,/
2
Multiplication, division
square, sqrt
1
Square, square root
sin, cos
1
Trigonometric functions
asin, acos
1
Trigonometric functions
tan, tanh
1
Trigonometric functions
reciprocal
1
Reciprocal
log
1
Natural logarithm
abs, negator
1
Absolute, change sign
3.3.2.3
Full Genetic Programing Methods
In this category of methods, we
give two concrete examples.
(1) Koza 1993
This paper describes an approach for both generating (feature) detectors and combin-
ing these detectors in a binary classiﬁer. Genetic programing is used for both tasks:
feature generation and classiﬁer synthesis. Each detector is represented as an auto-
matically deﬁned function (ADF), and the main program embodies the classiﬁcation
procedure (or classiﬁer) and calls upon any/all of the ADFs as often as it requires
[61].
The task here is to distinguish between an L and an I pattern by generating ap-
propriate detectors as well as a classiﬁer. Given that a pattern is represented as 6 × 4
map of black and white pixels, the objective is to automatically synthesize a computer
program that accepts any one of the 224 possible input patterns, and then returns the
correct output classiﬁcation: L, I, or NIL (for neither).
The general approach makes use of two interesting concepts: (a) the use of small
(e.g., 3 × 3) masks for detection of local features; (b) the use of movement as an
integrative means of relating features to each other.
A mask is implemented as a short function that inspects the Moore neighborhood
of its central pixel (including the central pixel) to conﬁrm the presence of a local
feature, such as a three-pixel vertical line.
Such feature detecting functions are not hard coded by the user but are ADFs. Each
ADF takes the form of a tree, representing a composition of Boolean disjunctions,
conjunctions, and negations of nine pixel-value sensors. A sensor, say S (for south)
returns a 1 if it matches a black pixel in the underlying input pattern and a 0 otherwise.
The sensors are X (for the central pixel), N for north, S for south, E for east, W for
west, NE for north east, NW for north west, SE for south east, and SW for south west.
The main program takes the form of a tree representing the composition of Boolean
disjunctions, conjunctions, and negations of feature detecting functions.
Hence, an individual is a classiﬁer made of ﬁve feature detecting functions and
one program that utilizes them. We have described the structure of an individual, but

FEATURE CREATION FOR PATTERN CLASSIFICATION
113
how is genetic programing used to evolve, evaluate, and ﬁnally select individuals that
execute the required classiﬁcation task, and with a high degree of accuracy?
In order to write a GP, one needs: a set of terminals and a set of (primitive)
functions that use them; an initialization technique; a way for assessing ﬁtness and for
selecting those individuals that are ﬁttest; means for diversiﬁcation (e.g., crossover);
values for the various parameters of a GP (e.g., population size); and a termination
criterion.
The ADFs have the terminal set Tf ={X, N, S, W, E, NE, NW, SE, SW} and the
function set Ff = {AND, OR, NOT}. The main program has the terminal set Tc = {I,
L, NIL} and a set of functions (Fc) containing movement functions, logical operators,
four ADFs and the HOMING operator.
The formulation of a feature is so general that it is doubtful whether the
current design is directly scalable to real-world pattern recognition problems, with
millions of pixels per image. In addition, the approach appears to be suited to ap-
plications where there is a single example (or prototype) per pattern. However, real-
world applications require an agent to learn a concept for a pattern (class) from
many examples of noisy instances. It is not clear to us how the capability to learn
from multiple examples can be incorporated into this technique. Finally, this ap-
proach assumes that enough discriminating features of a class can be (efﬁciently)
extracted from the original input image. However, it is sometimes useful for a map-
ping (or more) to be applied before the distinguishing features of a class of pat-
terns can be conﬁdently and efﬁciently extracted. Nevertheless, this is a pioneering
effort that has shown the way to other pattern recognition applications of genetic
programing.
(2) Andre 1994
This work builds on Koza’s results (above), but in a way that is more applicable
to the problem of multicategory character recognition. In Koza’s work, the feature
detectors are Boolean ADFs of pixel-value sensors. In contrast, Andre uses hit–miss
matrices that can be moved over a pattern until a match is found [6]. These matrices are
evolved using a custom-made two-dimensional genetic algorithm. In Koza’s work,
the ADF-encoded feature detectors are used by a main program that is capable of
moving over the whole pattern, while attempting to detect local features, until a ﬁnal
classiﬁcation is reached. This is very similar to the way the Anrdre’s control code
moves and matches, using the hit–miss matrices, until a ﬁnal conclusion is reached.
However, a GA is used to evolve the matrices, whereas a GP is used to evolve the
control code.
The purpose of this work appears to be the extension of Koza’s work to more realis-
tic character recognition problems, though the problems attempted remain simplistic.
In addition, the use of a two-dimensional GA and associated 2D crossover operator
is a signiﬁcant innovation that may prove quite useful for image/pattern processing
applications.
This approach has the distinction of using two representations: one for the hit–miss
matrices representing the feature detectors and another for the control code carrying
out the classiﬁcation task using the hit–miss matrices.

114
FEATURE EXTRACTION, SELECTION, AND CREATION
The overall representation of an individual consists of ﬁve hit–miss matrices and
a tree-based expression (main program) implemented in Lisp.
A hit–miss matrix of, for example, 3 × 3 elements is shown below: a 1-element
seeks a black pixel, a 0-element seeks a white pixel, and a do not care element matches
either pixel. A hit–miss matrix, such as the one in Figure 3.26, convoluted with an
image, returns another image.
Functions and terminals are used in the evolution of the Lisp-style control code.
The terminal set (for the hardest problem attempted by Andre) is T = {0, 1, moveE,
moveW, moveN, and moveS}. The ﬁrst two terminals represent the boolean values,
false and true. The last four terminals in the set are actually functions that change only
the X and Y variables representing the current horizontal and vertical coordinates,
respectively, of the pattern recognition agent on the input pattern.
The function set of the control code is F = {progn2, Ifdf0, Ifdf1, Ifdf2, Ifdf3,
Ifdf4}. progn2 is a macro that accepts two arguments, evaluates in the order of the
ﬁrst and the second arguments, and then returns the value of the second argument. An
Ifdfx (where x is an integer between 0 and 4) is a two-argument function that checks
to see if its associated dfx hit–miss matrix matches the input pattern, and if it does,
then the ﬁrst argument is evaluated, else the second argument is evaluated.
The whole approach is far from delivering an even restricted pragmatic pattern
recognition system. Real-world patterns come in many forms and at a much higher
resolution than the test sets used in this work.
3.3.2.4
Evolvable Pattern Recognizers
Again, we give two concrete
examples for this category of methods.
(1) Rizki 2002
According to Rizki, two approaches to the deﬁnition of features are commonplace.
One uses a growth process, synthesizing a small set of complex features, incremen-
FIGURE 3.26
A hit–miss matrix and its application to a T pattern [6].

FEATURE CREATION FOR PATTERN CLASSIFICATION
115
tally. The other uses a selection process in which a large set of small features are
generated and then ﬁltered [94].
Some researchers have evolved a single binary classiﬁer (or detector), as well
as a set of binary classiﬁers used by a higher level process to carry out multiple
classiﬁcations. This work, however, proposes a method to evolve and train a complete
multiclassiﬁer for use in the classiﬁcation of high-resolution radar signals. The system
described here is called HELPR. It is the latest incarnation of a series of evolvable
classiﬁers (by the same authors), including MORPH [94], which evolves a binary
classiﬁer and E-MORPH [94], which evolves a multiclassiﬁer as well.
The system has two major parts, a set of feature detectors and a perceptron
classiﬁer. A single feature detector is made of a transformation network and a capping
mechanism.
The transformation network is a network of morphological arithmetic and
conditional operators, which is used to enhance the most discriminating regions of
the one-dimentionsal input signal. A digitized input signal of n elements will produce
an n-element output, which is then capped to produce a set of k scalars. These go into
the classiﬁer.
The capping mechanism is a single-layer perceptron with inputs equal to k × M,
where M is the number of feature detectors, and the number of outputs equal to the
number of classes (to be distinguished). The perceptron cap functions as a template
ﬁlter that acts to increase the separation between the various classes.
HELPR succeeds in creating new complex features and produces a recognition
system that is highly accurate on real-world recognition tasks. However, evolution
in HELPR is actually limited to the set of (feature) detectors. The only evolutionary
activity that takes place at feature generation level, the classiﬁer, is trained separately
once the features are evolved. HELPR is a partially evolvable autonomous pattern
recognizer: It only evolves the sets of feature detectors; however, their perceptron
caps are trained and not evolved. In addition, there are still many parameters that are
set manually (e.g., M). Perhaps, a completely hands-off parameterless evolutionary
platform is in sight, but it is certainly not in hand.
(2) Kharma 2005
The long-term goal of project CellNet is the inclusion of methodologies for the simul-
taneous evolution of both feature and classiﬁers (Cells and Nets); at present, however,
the set of primitive features is hard-coded [50].
In its latest full version, CellNet-CoEv is capable of evolving classiﬁers (or hunters)
for a given set of images of patterns (or prey) that are also coevolving (getting harder
to recognize). A Hunter is a binary classiﬁer—a structure that accepts an image as
input and outputs a classiﬁcation. A hunter consists of cells, organized in a net. A
Cell is a logical statement—it consists of the index of a feature function along with
bounds. Prey are primarily images, drawn from the CEDAR database of handwritten
digits. In addition to the image, prey disguise themselves via a set of camouﬂage
functions controlled genetically [60].
All hunters were developed using identical system parameters, although training
and validation images for each run were chosen randomly from a pool of 1000 images.

116
FEATURE EXTRACTION, SELECTION, AND CREATION
The hunters were placed in a genetic algorithm, using ﬁtness-proportional selection
and elitism.
Each run was executed for a maximum of 250 generations, outputting data
regarding validation accuracy at 10 generations each. Training and validation
accuracies are all reasonably high (close to or exceeding 90%) and very close to
each other. Indeed, the average difference between training and validation accuracies
was -0.6%, which indicates a touch of underﬁtting!
These results represent a signiﬁcant step forward for the goal of an autonomous
pattern recognizer: competitive coevolution and camouﬂage are expected to aid in the
problem of overﬁtting and reliability without expert tuning, and also in the generation
of a larger and more diverse data set.
3.3.2.5
Historical Summary
Here, we present a historical summary of feature
creation for pattern recognition. The main publications in feature creation literature
are divided into four categories.
Category I is preevolutionary methods that include [110, 105, 36, 34]. All of these
techniques use mask- or pixel-based features; evaluate them using some kind of ref-
erence pattern; have no or limited higher level feature generation; and are not serious
real-world applications. In 1963, Uhr and Vossler [110] became the ﬁrst to propose the
autogeneration, evaluation, and weighting of features for character recognition. Un-
usual for a work of such age, it actually used handwritten patterns and allowed for the
creation of higher level features from simpler ones. In 1985, Stentiford [105] revived
the idea of automatic feature design (or creation), with a paper that described the use
of global hit–miss matrices as features and proposed orthogonality between feature
vectors as a means for assessing the quality of these features. In a 1990 paper, based on
his PhD thesis, Gillies [36] presented the idea of pixel-sum images as the bases for the
automatic creation of morphological templates; templates that are used as feature de-
tectors. Gillies also used orthogonality as a measure of quality of features, though not
exactly in the same way as Stentiford did. In 1996, Gader and Khabou [34] proposed
theuseofapixel-sumimage,whichistheresultofsumming1000(asopposedto20for
Gillies) images per class—in effect producing a probabilistic distribution model for
each class of character. He clearly separates the notion of feature from that of feature
detector and assesses his features, which are mask based, using both orthogonality and
information.
Category II is polynomial-based methods that include [17, 18, 38]. All of these
techniques employ primitive features that are statistical functions of discrete (or dis-
cretizable) data signals; create complex features in the form of polynomial functions
of the primitive features; and most have applications in the area of machine fault
diagnosis. In 1991, Chang et al. [17] were, to the best of our knowledge, the ﬁrst
to use a genetic algorithm to create new features as polynomials of existing features,
taken two at a time. As they used a GA and not a GP, they represented new features
in terms of linear strings. They evaluated the feature sets using a k-nearest neighbor
classiﬁer. In 1999, Kotani et al. [59] devised a tree-based representation for the fea-
ture functions. Their representation was more appropriate for encoding polynomials
than strings, but it only allowed for + and −operators in the nonterminal nodes of

FEATURE CREATION FOR PATTERN CLASSIFICATION
117
the expression tree. This must have contributed to the usual number of six differ-
ent genetic operators that their technique used. In 2001, Chen et al. [18] took the
next logical step and used expression trees that allowed for the basic four arithmetic
operators and power in their nonterminal nodes; the genetic operators used were
typical. Finally in 2005, Guo et al. [38] used moment-based statistical features as
terminals and an extensive—perhaps too extensive—set of arithmetic trigonometric
and other functions as operators in the making of trees representing new features.
They also used the Fisher criterion (as opposed to ﬁnal classiﬁcation accuracy) to
speed up the process of ﬁtness evaluation of the new features. The resulting features
were fed into various classiﬁers, including artiﬁcial neural nets and support vector
machines.
Category III is full genetic programing methods that include [61, 6]. Both meth-
ods evolve a Lisp-style program that controls a roving agent that uses any/all of ﬁve
evolvable Boolean functions/masks to correctly identify a character. What distin-
guishes these works is that they evolve both the features and the classiﬁer that uses
them simultaneously. It was 1993 when Koza [61] (also, the acknowledged father
of genetic programing) proposed the use of an evolvable program-controlled tur-
tle, with ﬁve equally evolvable Boolean functions (representing feature detectors),
to autogenerate a complete recognition system for a toy problem: distinguishing
between an I and an L. In 1997, Andre [6] built on Koza’s work, by amending
it to use two-dimensional hit–miss matrices (instead of Boolean functions) and
introduce appropriate two-dimensional genetic operators. Andre tested his modi-
ﬁed approach to three sets of “toy” problems, with the last being the synthesis
of binary classiﬁers for single low-resolution digits between 0 and 9. The po-
tential of this approach is considerable, for it is quite general, but is still to be
realized.
Category IV is evolvable pattern recognizers. These are perhaps the most
ambitious ongoing projects. Two systems described below do this. CellNet [50] blurs
the line between feature selection and the construction of binary classiﬁers out of
these features. HELPR [94] also evolves feature detectors, but the classiﬁcation mod-
ule is completely separate from the feature extraction module. Other differences exist,
but, both attempts are the only systems, that we know of, that aim at using artiﬁcial
evolution to synthesize complete recognition systems (though currently for different
application domains), with minimum human intervention .
HELPR is composed of two modules: a features extraction module and a classiﬁ-
cation module. The classiﬁcation system is not evolvable; only the feature extractor
is [94]. The feature extraction module is made of a set of feature detectors. Its input
is the raw input pattern and its output is a feature vector. The system is designed
to handle signals (not visual patterns or patterns in general). CellNet is an ongoing
research project aiming at the production of an autonomous pattern recognition soft-
ware system for a large selection of pattern types [50]. Ideally, a CellNet operator
would need little-to-no specialized knowledge to operate the system. To achieve this,
CellNet divides the problem (and hence the solution) into two parts: feature creation
and classiﬁer synthesis.

118
FEATURE EXTRACTION, SELECTION, AND CREATION
3.3.3
Future Trends
In the above section, we have reviewed various methods for automatic feature cre-
ation. Next, we discuss the future trends in this ﬁeld from three perspectives: generic
applications, automatic complexiﬁcation, and real-world technologies.
3.3.3.1
Generic Applications
The ﬁrst instinct of any design engineer when
faced with a speciﬁc pattern recognition problem is to think locally; that is, to identify
the right representation for the problem and solution and to look for those most
distinctivefeaturesofthevariouspatternstoberecognized.Thisapproachhas,tosome
degree, carried over to the various feature creation endeavors reviewed in previous
sections. For example, those applications in the area of machine fault diagnosis have
all used polynomial-based representations that are well suited to signal processing.
On the contrary, visual pattern recognition applications have all moved toward mask-
based feature representations and matching genetic operators. It may well be a good
idea to do this when one is driven by ultimate performance in terms of effectiveness
and efﬁciency. However, this does not necessarily encourage the designer to devise
representations and operators that have generic or at least generalizable capabilities.
Pattern recognition research should take a serious look at recognition problems
where more than one category of patterns is to be recognized. An example would be
devising feature extraction methods that succeed in extracting distinctive features for
both a human emotional state detection task and a biological cell type classiﬁcation
task. The two problems appear distinct but are indeed quite similar. A lot of research
on emotional state detection has been focused on the discovery of those statistical
features of various physiological signals (e.g., blood pressure) that correlate most
reliably with (pronounced) emotional states (e.g., anger). On the contrary, the type of
a certain cell can be determined, in certain applications, from statistical features (e.g.,
high-frequency components) of the gray-level intensity map of the cell image. Both
cases use statistical features that may be coded as polynomials, with the proviso that
the input pattern in one case is an one-dimensional signal, whereas in the other case,
it is a two-dimensional gray-scale image.
If researchers intentionally take on recognition tasks that involve two or more
categories of input patterns, such as both Arabic words and Chinese characters, or
bothtreeimagesandmilitarytargetproﬁles,thenweareboundtomoveinthedirection
of more generic pattern classiﬁcation and feature creation methodologies.
3.3.3.2
Automatic Complexiﬁcation
One weakness of current techniques
used for feature creation is the limited ability of all surveyed techniques to cre-
ate high-level features autonomously. It is true that some polynomial-based tech-
niques allow for the created functions to be functions of primitive features as well as
previously created ones. It is also a fact that all genetic programing techniques employ
automatic function deﬁnitions from a set of feature functions. However, creating new
functions by using previously created ones does not necessarily change the type of
function that will result; a polynomial function of any number of polynomial func-
tions will always be a polynomial function (and not, for example, a sinusoidal one). In

FEATURE CREATION FOR PATTERN CLASSIFICATION
119
genetic programing approaches, on the contrary, automatic function deﬁnition has
been limited to the automatic creation of a preset number of Lisp-type subroutines,
each deﬁning a feature extraction function. The signiﬁcant advances in automatic
function deﬁnition, achieved by John Koza and others, have not yet been applied
to the problem of automatic creation of feature functions for pattern recognition
purposes.
There is a long way, not yet traveled, toward applying and advancing many
computer science techniques, such as hierarchical genetic programing [8], layered
learning [106], Adaptive representations of learning [95] and, of course, automati-
cally deﬁned functions [61] to the problem of feature creation for pattern recognition.
We envisage a number of future developments that will allow feature creation
programs based on GP methodologies to generate highly complex features with lit-
tle human intervention—explicit or implied. First, starte with the simplest possible
(primitive) feature extracting functions. Second, the various moves that a pattern
recognition agent can make over an image are representative of the range of relation-
ships between features (e.g., above or below). As such, it is also important to allow
for a variable number of moves (or relationships). Third, a mechanism such as ADFs
should be included in the approach to allow for the automatic deﬁnition (in the form
of a program) of new features from primitive or created features and primitive or
created relationships (between features). This mechanism should allow for recursion
and could use discovered repeating patterns in the input images as bases for creating
new features. Fourth, a new mechanism that allows for the automatic generation of
programs that embody topological relationships between features. Such a mechanism
can use repeating relationships between features in discovered patterns as bases for
coding new relationships. Fifth, there should also be a mechanism that examines ex-
isting programs, deﬁning new features or new relationships, to see if there are any
commonalities between these functions (i.e., function invariants) that would allow for
the creation of a single parameterized abstract function in place of two or more similar
features or relationship functions. Finally, it is important that the right environment
be created for the agent such that it has the right “motivation” not only to ﬁnd features
but also to discover the smallest number of simplest possible features suited for a
given recognition task.
3.3.3.3 Real-World Technologies
One problem with any new ﬁeld of research
is the lack of strong techniques with direct applicability to real-world problems.
Feature creation is no exception, and all of the surveyed papers do not offer a read-
ily applicable technology or even general design methodology. We mean that if a
researcher is faced with a feature creation challenge tomorrow, then that researcher
will not be able to directly plug-and-play any software component into his/her sys-
tem or even cut-and-paste a method into his overall approach. In other words, we do
not yet have a standard feature creation technology (e.g., image processing toolbox)
or standard feature creation methods (e.g. the Fourier transform) that would allow
practitioners and researchers alike to reuse existing solutions to feature creation
challenges, with little or no conﬁguration.

120
FEATURE EXTRACTION, SELECTION, AND CREATION
It is important that some of the research done in the future be directed toward (a)
creation of stand-alone tool boxes with proper interfaces that would allow a worker in
pattern recognition to choose the right feature creation component for his particular
problem type, and then deploy it, as part of a larger system, with conﬁdence in both
the theoretical and pragmatic capabilities of the component; (b) invention of methods
that are of general applicability to a large set of pattern classiﬁcation tasks. Such
methods need to be theoretically sound as well as practical.
It is not yet clear whether feature creation research is going to create its own
attractor, around which various research efforts revolve, or if the various disciplines
that require or could beneﬁt from feature creation will maintain their hold on their
own sphere of feature creation ideas and lead to highly specialized sets of tools and
methodologies.Whateverhappens,itiscleartousthat,giventheconsiderableincrease
in computational power, the use of computerized generate-and-test techniques will
likely to increase, and with it the use of evolutionary algorithms for feature creation
and selection.
3.4
CHAPTER SUMMARY
This chapter talks about three important aspects of the process of pattern recogni-
tion. Section 3.1 covered feature extraction. It presented a large number of features
commonly used for character recognition. Some of the features are structural, such as
the various discrete features, and others are statistical, such as moments and fourier
descriptors.
In Section 3.2, feature selection was examined through a three-sided prism. One
side dealt with the relationship between selection and classiﬁcation; the other side
dealt with the various criteria used for evaluating the different feature subsets; the
third side presented a summary of the various search strategies used for exploring the
space of possible feature subsets.
The ﬁnal section of Chapter 3 talked about the slowly emerging ﬁeld of feature
creation. Ultimately, we would like to build machines that are similar to human beings
in their ability to discover or invent features, on the ﬂy, for any recognition task and
do so without external guidance. The most recent efforts dedicated to the goal of
feature creation have used some form of genetic algorithms or programing to evolve
two-dimensional mask-based features or arithmetic feature functions. However, there
is still much room that we hope will be ﬁlled with research in the near future.
REFERENCES
1. D. W. Aha and R. L. Bankert. Feature selection for case-based classiﬁcation of cloud
types: an empirical comparison. In Proceedings of the AAAI Workshop on Case-Based
Reasoning. Seattle, WA, 1994, pp. 106–112.

REFERENCES
121
2. D. W. Aha and R. L. Bankert. A comparative evaluation of sequential feature selection
algorithms. In D. Fisher and H.-J. Lenz, editors, Learning from Data. Springer, New York,
1996, pp. 199–206.
3. H.AlmuallimandT.G.Dietterich. Learning with many irrelevant features. In Proceedings
of the 9th National Conference on Artiﬁcial Intelligence. Anaheim, 1991, Vol. 2, pp. 547–
552.
4. H. Almuallim and T. G. Dietterich. Learning boolean concepts in the presence of many
irrelevant features. Artiﬁcial Intelligence. 69(1–2), 279–305, 1994.
5. A. Amin. Prototyping structural description using an inductive learning program. IEEE
Transactions on System, Man Cybernetics, Part C, 30(1), 150–157, 2000.
6. D. Andre. Automatically deﬁned features: the simultaneous evolution of 2-dimensional
feature detectors and an algorithm for using them. In K. E. Kinnear, editor, Advances in
Genetic Programing. The MIT Press, Cambridge, MA, 1994, Chapter 23.
7. A. Arauzo, J. M. Benitez, and J. L. Castro. C-FOCUS: a continuous extension of FOCUS.
In J. M. Benitez, et al., editors, Advances in Soft Computing—Engineering, Design and
Manufacturing. Springer, London, 2003, pp. 225–232.
8. W. Banzhaf, D. Banscherus, and P. Dittrich. Hierarchical genetic programing using local
modules. In Proceedings of the International Conference on Complex Systems. Nashua,
NH, 1998.
9. B. Bhanu and Y. Lin. Genetic algorithm based feature selection for target detection in
SAR images. Image and Vision Computing. 21, 591–608, 2003.
10. R. E. Bellman. Adaptive Control Processes. Princeton University Press, 1961.
11. M. Ben-Bassat. Use of distance measures, information measures and error bounds in
feature evaluation. In P. R. Krishnaiah and L. N. Kanal, editors, Handbook of Statistics,
North Holland, 1982, pp. 773–791.
12. A. L. Blum and P. Langley. Selection of relevant features and examples in machine
learning. Artiﬁcial Intelligence, 97(1–2), 245–271, 1997.
13. L. Brieman, J. H. Friedman, R. A. Olshen, and C. J. Stone. Classiﬁcation and Regression
Trees, Wadsworth, 1984.
14. L. G. Brown. A survey of image registration techniques. ACM Computing Surveys, 24(4),
325–363, 1992.
15. E. Cantu-Paz, S. Newsam, and C. Kamath. Feature selection in scientiﬁc applications.
In Proceedings of the 10th ACM International Conference on Knowledge Discovery and
Data Mining, Seattle, WA, 2004, pp. 788–793.
16. C. Cardie. Using decision trees to improve case-based learning. In Proceedings of the 10th
International Conference on Machine Learning, Amherst, 1993, Morgan Kaufmann, pp.
25–32.
17. E. I. Chang, R. P. Lippmann, and D. W. Tong. Using genetic algorithms to select and
create features pattern classiﬁcation. Proceedings of the International Joint Conference
on Neural Networks, San Diego, CA, 1990, Vol. 3, pp.747–753.
18. P. Chen, T. Toyota, and Z. He. Automated function generation of symptom parameters
and application to fault diagnosis of machinery under variable operating conditions. IEEE
Transactions on System, Man Cybernetics Part A, 31(6), 775–781, 2001.
19. X.-W. Chen. An improved branch and bound algorithm for feature selection. Pattern
Recognition Letters, 24, 1925–1933, 2003.

122
FEATURE EXTRACTION, SELECTION, AND CREATION
20. S. Das. Filters, wrappers and a boosting-based hybrid for feature selection. In Proceedings
of the 18th International Conference on Machine Learning, Williamstown, 2001, pp.
74–81.
21. M. Dash and H. Liu. Feature selection for classiﬁcation. Intelligent Data Analysis, 1(3),
131–156, 1997.
22. M. Dash and H. Liu. Consistency-based search in feature selection. Artiﬁcial Intelligence,
151, 155–176, 2003.
23. P. A. Devijver and J. Kittler. Pattern Recognition: A Statistical Approach. Prentice Hall,
1982.
24. A. C. Downton and S. Impedovo, editors. Progress in Handwriting Recognition, World
Scientiﬁc, Singapore, 1997.
25. R. O. Duda and P. E. Hart. Use of the Hough transformation to detect lines and curves in
pictures. Communications of ACM, 5(1), 11–15, 1972.
26. T. Elomaa and J. Rousu. General and efﬁcient multisplitting of numerical attributes.
Machine Learning, 36(3), 201–244, 1999.
27. T. Eriksson, S. Kim, H.-G. Kang, and C. Lee. An information-theoretic perspective on
feature selection in speaker recognition. IEEE Signal Processing Letters, 12(7), 500–503,
2005.
28. M. E. Farmer, S. Bapna, and A. K. Jain. Large scale feature selection using modiﬁed
random mutation hill climbing. In Proceedings of the 17th International Conference on
Pattern Recognition, Cambridge, UK, 2004, Vol.2, pp. 287–290.
29. F. Ferri, P. Pudil, M. Hatef, and J. Kittler. Comparative study of techniques for large scale
feature selection. In E. Gelsema and L. Kanal, editors, Pattern Recognition in Practice
IV, Vlieland, The Netherlands, 1994, pp. 403–413.
30. H. A. Firpi and E. Goodman. Swarmed feature selection. In Proceedings of the 33rd
Applied Imagery Pattern Recognition Workshop, Washington DC, 2004, pp. 112–118.
31. H. Freeman. Boundary encoding and processing. In B. Lipkin and A. Rosenfeld, editors,
Picture Processing and Pshcholopictorics. Academic Press, New York, 1970, 241–266.
32. H. Fujisawa and C.-L. Liu. Directional pattern matching for character recognition revis-
ited. Proceedings of the 7th International Conference on Document Analysis and Recog-
nition, Edinburgh, Scotland, 2003, pp. 794–798.
33. K. Fukunaga. Introduction to Statistical Pattern Recognition, 2nd edition. Academic
Press, New York, 1990.
34. P. D. Gader and M. A. Khabou. Automatic feature generation for handwritten digit
recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 18(12),
1256–1261, 1996.
35. K. Giles, K.-M. Bryson, and Q. Weng. Comparison of two families of entropy-based
classiﬁcation measures with and without feature selection. In Proceedings of the 34th
Hawaii International Conference on System Sciences, Maui, 2001, Vol. 3.
36. Andrew M. Gillies. Automatic generation of morphological template features. In Pro-
ceedings of the SPIE—Image Algebra and Morphological Image Processing, San Diego,
CA, 1990, Vol. 1350, pp. 252–261.
37. R. C. Gonzalez and R. E. Woods. Digital Image Processing. Addison Wesley, Boston,
MA, 1993.

REFERENCES
123
38. H. Guo, L. B. Jack, and A. K. Nandi. Feature generation using genetic programing with
application to fault classiﬁcation. IEEE Transactions on System, Man and Cybernetics
Part B, 35(1): 89–99, 2005.
39. M. A. Hall. Correlation-based feature selection for discrete and numeric class machine
learning. In Proceedings of the 17th International Conference on Machine Learning, San
Francisco, 2000, Morgan Kaufmann Publishers, pp. 359–366.
40. M. A. Hall and L. A. Smith. Feature subset selection: a correlation based ﬁlter approach.
In Proceedings of the 4th International Conference on Neural Information Processing
and Intelligent Information Systems, Dunedin, 1997, pp. 855–858.
41. M. A. Hall and L. A. Smith. Practical feature subset selection for machine learning. In
Proceedings of the 21st Australian Computer Science Conference, Auckland, 1998, pp.
181–191.
42. T. Hastie and R. Tibshirani. Discriminant analysis by Gaussian mixtures. Journal of the
Royal Statistical Society, Series B, 58(1), 155–176, 1996.
43. C.-N. Hsu, H.-J. Huang, and D. Schuschel. The ANNIGMA-wrapper approach to fast
feature selection for neural nets. IEEE Transactions on System, Man, Cybernetics, Part
B, 32(2), 207–212, 2002.
44. M.-K Hu. Visual pattern recognition by moment invariants. IRE Transactions on Infor-
mation Theory, IT-8, 179–187, 1962.
45. F. Hussein, N. Kharma, and R. Ward. Genetic algorithms for feature selection and
weighting, a review and study. In Proceedings of the 6th International Conference on
Document Analysis and Recognition, Seattle, WA, 2001, pp. 1240–1244.
46. T. Iijima, H. Genchi, and K. Mori. A theoretical study of the pattern identiﬁcation by
matching method. In Proceedings of the First USA–JAPAN Computer Conference, Tokyo,
Japan, 1972, pp. 42–48.
47. G. H. John, R. Kohavi, and K. Pﬂeger. Irrelevant features and the subset selection prob-
lem. In Proceedings of the 11th International Conference on Machine Learning, New
Brunswick, 1994, pp. 121–129.
48. N. Kato, M. Suzuki, S. Omachi, H. Aso, and Y. Nemoto. A handwritten character recog-
nition system using directional element feature and asymmetric Mahalanobis distance.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 21(3), 258–262, 1999.
49. A. Kawamura, K. Yura, T. Hayama, Y. Hidai, T. Minamikawa, A. Tanaka, and S. Masuda.
On-line recognition of freely handwritten Japanese characters using directional feature
densities. In Proceedings of the 11th International Conference on Pattern Recognition,
The Hague, 1992, Vol. 2, pp. 183–186.
50. N. Kharma, T. Kowaliw, E. Clement, C. Jensen, A. Youssef, and J. Yao. Project CellNet:
evolving an autonomous pattern recogniser. International Journal on Pattern Recognition
and Artiﬁcial Intelligence, 18(6), 1–18, 2004.
51. A. Khotanzad and Y. H. Hong. Invariant image recognition by Zernike moments. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 12(5), 489–490, 1990.
52. F. Kimura, K. Takashina, S. Tsuruoka, and Y. Miyake. Modiﬁed quadratic discriminant
functions and the application to Chinese character recognition. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 9(1), 149–153, 1987.
53. F. Kimura. On feature extraction for limited class problem. In Proc. 13th Int. Conf. on
Pattern Recognition, Vienna, Austria, 1996, Vol. 2, pp. 25–29.

124
FEATURE EXTRACTION, SELECTION, AND CREATION
54. K. Kira and L. A. Rendell. The feature selection problem: Traditional methods and a new
algorithm. In Proceedings of the 10th National Conference on Artiﬁcial Intelligence, San
Jose, CA, 1992, pp. 129–134.
55. K. Kira and L. A. Rendell. A practical approach to feature selection. In D. Sleeman and
P. Edwards, editors, International Conference on Machine Learning, Morgan Kaufmann
Publishers, 1992, pp. 249–256.
56. R. Kohavi and G. H. John. Wrappers for feature subset selection. Artiﬁcial Intelligence,
97(1–2), 273–324, 1997.
57. R. Kohavi and D. Sommerﬁeld. Feature subset selection using the wrapper model: overﬁt-
ting and dynamic search space topology. In Proceedings of the 1st International Confer-
ence on Knowledge Discovery and Data Mining, Montreal, Canada, 1995, pp. 192–197.
58. I. Kononenko. Estimating attributes: analysis and extension of relief. In Proceedings of
the European Conference on Machine Learning, Catania, 1994, pp. 171–182.
59. M. Kotani, S. Ozawat, M. Nakai, and K. Akazawat. Emergence of feature extraction
function using genetic programing. In Proceedings of the 3rd International Conference
on Knowledge-Based Intelligent Information Engineering Systems, Adelaide, 1999, pp.
149–152.
60. T. Kowaliw, N. Kharma, C. Jensen, H. Moghnieh, and J. Yao. Using competitive co-
evolution to evolve better pattern recognizers. International Journal on Computational
Intelligence and Applications, 5(3), 305–320, 2005.
61. J. R. Koza. Simultaneous discovery of detectors and a way of using the detectors via
genetic programing. Proceedings of the IEEE International Conference on Neural Net-
works, 1993, Vol. 3, pp. 1794–1801.
62. M. Kudo and J. Sklansky. Comparison of algorithms that select features for pattern clas-
siﬁers. Pattern Recognition, 33, 25–41, 2000.
63. F. P. Kuhl and C. R. Giardina. Elliptic Fourier feature of a closed contour. Computer
Vision, Graphics and Image Processing, 18, 236–258, 1982.
64. P. Kultanen, L. Xu, and E. Oja. Randomized Hough transform. In Proceedings of the
10th International Conference on Pattern Recognition, Atlantic City, 1990, Vol. 1, pp.
631–635.
65. L. I. Kuncheva and L. C. Jain. Nearest neighbor classiﬁer: Simultaneous editing and
feature selection. Pattern Recognition Letters, 20, 1149–1156, 1999.
66. T. N. Lal, O. Chapelle, J. Weston, and A. Elisseeff. Embedded methods. In I. Guyon, S.
Gunn, M. Nikravesh, and L. Zadeh, editors, Feature extraction, Foundations and Appli-
cations, Springer, 2006, pp. 137–165.
67. L. Lam, S.-W. Lee and C. Y. Suen. Thinning methodologies—a comprehensive survey.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 14(9), 869–885, 1992.
68. P. Langley. Selection of relevant features in machine learning. In Proceedings of the AAAI
Fall Symposium on Relevance, New Orleans, 1994, pp. 140–144.
69. P. Langley and S. Sage. Oblivious decision trees and abstract cases. In Proceedings of the
AAAI-94 Workshop on Case-Based Reasoning, Seattle, WA, 1994, pp. 113–117.
70. C. Lee and D. A. Landgrebe. Feature extraction based on decision boundary. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence, 15(4), 388–400, 1993.

REFERENCES
125
71. C. Lee and G. G. Lee. Information gain and divergence-based feature selection for
machine learning-based text categorization. Information Processing and Management,
42, 155–165, 2006.
72. S. Lee and J. C. Pan. Ofﬂine tracing and representation of signature. IEEE Transactions
on System, Man, and Cybernetics, 22(4), 755–771, 1992.
73. C.-L. Liu, Y.-J. Liu, and R.-W. Dai. Preprocessing and statistical/structural feature
extraction for handwritten numeral recognition. In A. C. Downton and S. Impedovo,
editors, Progress of Handwriting Recognition, World Scientiﬁc, Singapore, 1997, pp.
161–168.
74. C.-L. Liu, K. Nakashima, H. Sako, and H. Fujisawa. Handwritten digit recognition: bench-
marking of state-of-the-art techniques, Pattern Recognition, 36(10), 2271–2285, 2003.
75. H. Liu and R. Setiono. Feature selection and classiﬁcation—a probabilistic wrapper
approach. In Proceedings of the 9th International Conference Industrial and Engineering
Applications of AI and ES, Fukuoka, Japan, 1996, pp. 419–424.
76. H. Liu and R. Setiono. A probabilistic approach to feature selection—a ﬁlter solution.
In Proceedings of the 13th International Conference on Machine Learning, Bari, Italy,
1996, pp. 319–327.
77. H. Liu and L. Yu. Toward integrating feature selection algorithms for classiﬁcation and
clustering. IEEE Transactions on Knowledge and Data Engineering, 17(4), 491–502,
2005.
78. M. Loog and R. P. W. Duin. Linear dimensionality reduction via a heteroscedastic exten-
sion of LDA: the Chernoff criterion. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 26(6), 732–739, 2004.
79. H. A. Mayer and P. Somol. Conventional and evolutionary feature selection of SAR data
using a ﬁlter approach. In Proceedings of the 4th World Multi-Conference on Systemics,
Cybernetics, and Informatics, Orlando, 2000.
80. R. Meiri and J. Zahavi. Using simulated annealing to optimize the feature selection
problem in marketing applications. European Journal of Operational Research, 171(3),
842–858, 2006.
81. A. J. Miller. Subset Selection in Regression, 2nd edition. Chapman and Hall, 2002.
82. T. M. Mitchell. Generalization as search. Artiﬁcial Intelligence, 18(2), 203–226, 1982.
83. S. Mori, C. Y. Suen, and K. Yamamoto. Historical review of OCR research and develop-
ment. Proceedings of IEEE, 80(7), 1029–1053, 1992.
84. R. Mukundan, S. H. Ong, and P. A. Lee. Image analysis by Tchebichef moments. IEEE
Transactions on Image Processing, 10(9), 1357–1364, 2001.
85. P. Narendra and K. Fukunaga. A branch and bound algorithm for feature subset selection.
IEEE Transactions on Computers, 26(9), 917–922, 1977.
86. K. R. Niazi, C. M. Arora, and S. L. Surana. Power system security evaluation using ANN:
Feature selection using divergence. Electric Power Systems Research, 69, 161–167, 2004.
87. C. M. Nunes, A. d. S. Britto Jr., C. A. A. Kaestner, and R. Sabourin. An optimized
hill climbing algorithm for feature subset selection: evaluation on handwritten character
recognition. In Proceedings of the 9th International Workshop on Frontiers in Handwrit-
ing Recognition, Tokyo, Japan, 2004, pp. 365–370.

126
FEATURE EXTRACTION, SELECTION, AND CREATION
88. L. S. Oliveira, R. Sabourin, F. Bortolozzi, and C. Y. Suen. Feature selection using
multi-objective genetic algorithms for handwritten digit recognition. In Proceedings of the
16th International Conference on Pattern Recognition, Quebec City, 2002, pp. 568–571.
89. T. Petkovic and J. Krapac. Shape description with Fourier descriptors. Technical Report,
University of Zagreb, Croatia, 2002.
90. S. Piramuthu. Evaluating feature selection methods for learning in data mining applica-
tions. European Journal of Operational Research, 156, 483–494, 2004.
91. W. F. Punch, E. D. Goodman, M. Pei, L. Chia-Shun, P. D. Hovland, and R. J. Enbody.
Further research on feature selection and classiﬁcation using genetic algorithms. In S.
Forrest, editor, Proceedings of the 5th International Conference on Genetic Algorithms,
Morgan Kaufmann, San Mateo, 1993, pp. 557–564.
92. J. R. Quinlan. C4.5 Programs for Machine Learning, Morgan Kaufmann, San Mateo,
1993.
93. J. R. Quinlan. Induction of decision trees. Machine Learning, 1(1), 81–106, 2003.
94. M. M. Rizki, M. A. Zmuda, and L. A. Tamburino. Evolving pattern recognition systems.
IEEE Trans. Evolutionary Computation, 6(6), 594–609, 2002.
95. J. P. Rosca. Towards automatic discovery of building blocks in genetic programing. In
E. S. Siegel and J. R. Koza, editors, Working Notes for the AAAI Symposium on Genetic
Programing, Menlo Park, 1995, pp. 78–85.
96. J. C. Schlimmer. Efﬁciently inducing determinations: a complete and systematic search
algorithm that uses optimal pruning. In Proceedings of the 10th International Conference
on Machine Learning, Amherst, 1993, pp. 284–290.
97. J. A. Schnabel. Shape description methods for medical images. Technical Report, Uni-
versity College London, UK, 1995.
98. D. Schuschel and C.-N Hsu. A weight analysis-based wrapper approach to neural nets
feature subset selection. In Proceedings of the 10th IEEE International Conference on
Tools with AI., Taipei, 1998.
99. M. Sebbana and R. Nock. A hybrid ﬁlter/wrapper approach of feature selection using
information theory. Pattern Recognition, 35, 835–846, 2002.
100. W. Siedlecki and J. Sklansky. A note on genetic algorithms for large scale on feature
selection. Pattern Recognition Letters, 10, 335–347, 1989.
101. D. B. Skalak. Prototype and feature selection by sampling and random mutation hill
climbing algorithms. In Proceedings of the 11th International Conference on Machine
Learning, New Brunswick, 1994, pp. 293–301.
102. P. Somol, P. Pudil, and J. Kittler. Fast branch and bound algorithms for optimal fea-
ture selection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 26(7),
900–912, 2004.
103. P. Somol, P. Pudil, J. Novovicova, and P. Paclik. Adaptive ﬂoating search methods in
feature selection. Pattern Recognition Letters, 20, 1157–1163, 1999.
104. G. Srikantan, S. W. Lam, and S. N. Srihari. Gradient-based contour encoder for character
recognition. Pattern Recognition, 29(7), 1147–1160, 1996.
105. F. W. M. Stentiford. Automatic feature design for optical character recognition using
an evolutionary search procedure. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 7(3), 350–355, 1985.

REFERENCES
127
106. P. Stone and M. M. Veloso. Layered learning. In Proceedings of the 11th European
Conference on Machine Learning, Barcelona, 2000, pp. 369–381.
107. Z. Sun, G. Bebis, X. Yuan, and S. J. Louis. Genetic feature subset selection for gender
classiﬁcation: a comparison study. In Proceedings of the IEEE Workshop on Applications
of Computer Vision, Orlando, 2002, pp. 165–170.
108. C.-H. Teh and R. T. Chin. On image analysis by the methods of moments. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence, 10(4), 496–499, 1988.
109. O. D. Trier, A. K. Jain, and T. Taxt. Feature extraction methods for character recognition—
a survey. Pattern Recognition, 29(4), 641–662, 1996.
110. L. Uhr and C. Vossler. A pattern-recognition program that generates, evaluates and
adjusts its own operators. In Computers and Thought, McGraw-Hill, New York, 1963,
pp. 251–268.
111. H. Vafaie and I. F. Imam. Feature selection methods: genetic algorithms vs. greedy-like
search. In Proceedings of the International Conference on Fuzzy and Intelligent Control
Systems, Louisville, 1994.
112. H. Vafaie and K. D. Jong. Robust feature selection algorithms. In Proceedings of the 5th
Conference on Tools for Artiﬁcial Intelligence, Boston, 1993, pp. 356–363.
113. H. Vafaie and K. A. D. Jong. Genetic algorithms as a tool for restructuring feature space
representations. In Proceedings of the International Conference on Tools with Artiﬁcial
Intelligence, Herndon, 1995, pp. 8–11.
114. S. A. Vere. Induction of concepts in the predicate calculus. In Proceedings of the 4th
International Joint Conference on Artiﬁcial Intelligence, Tbilisi, 1975, pp. 351–356.
115. D. Wettschereck, D. W. Aha, and T. Mohri. A review and empirical evaluation of feature
weighting methods for a class of lazy learning algorithms. Artiﬁcial Intelligence Review,
11(1–5), 273–314, 1997.
116. Y. Wu and A. Zhang. Feature selection for classifying high-dimensional numerical data.
In Proceedings of the IEEE International Conference on Computer Vision and Pattern
Recognition, Washington DC, 2004, Vol. 2, pp. 251–258.
117. Y. Yamashita, K. Higuchi, Y. Yamada, and Y. Haga. Classiﬁcation of handprinted Kanji
characters by the structured segment matching method. Pattern Recognition Letters, 1,
475–479, 1983.
118. J. Yang and V. Honavar. Feature subset selection using a genetic algorithm. In J. R. Koza,
et al., editors, Genetic Programing, Morgan Kaufmann, 1997.
119. M. Yasuda and H. Fujisawa. An improvement of correlation method for character recog-
nition. Transactions on IEICE Japan, J62-D(3), 217–224, 1979 (in Japanese).
120. H. Yoshida, R. Leardi, K. Funatsu, and K. Varmuza. Feature selection by genetic algo-
rithms for mass spectral classiﬁers. Analytica Chimica Acta, 446, 485–494, 2001.
121. B. Yu and B. Yuan. A more efﬁcient branch and bound algorithm for feature selection.
Pattern Recognition, 26, 883–889, 1993.
122. J. Yu, S. S. R. Abidi, and P. H. Artes. A hybrid feature selection strategy for image deﬁning.
In Proceedings of the 5th International Conference on Intelligent Systems Design and
Applications, Guangzhou, Chia, 2005, Vol. 8, pp. 5127–5132.
123. L. Yu and H. Liu. Feature selection for high-dimensional data: a fast correlation-based
ﬁlter solution. In Proceedings of the 20th International Conference on Machine Leaning,
Washington DC, 2003, pp. 856–863.

128
FEATURE EXTRACTION, SELECTION, AND CREATION
124. S. Yu, S. De Backer, and P. Scheunders. Genetic feature selection combined with compos-
ite fuzzy nearest neighbor classiﬁers for hyperspectral satellite imagery. Pattern Recog-
nition Letters, 23(1–3): 183–190, 2002.
125. D. Zhang and G. Lu, A comparative study on shape retrieval using Fourier descriptors
with different shape signatures. In Proceedings of the IEEE International Conference on
Multimedia and Expo, Tokyo, Japan, 2001, pp. 1139–1142.
126. H. Zhang and G. Sun. Feature selection using tabu search method. Pattern Recognition,
35, 701–711, 2002.
127. L. X. Zhang, J. X. Wang, Y. N. Zhao, and Z. H. Yang. A novel hybrid feature selection
algorithm: using Relief estimation for GA-wrapper search. In Proceedings of the Inter-
national Conference on Machine Learning and Cybernetics, Xi’an, China, 2003, Vol. 1,
pp. 380–384.

CHAPTER 4
PATTERN CLASSIFICATION METHODS
If the goal of feature extraction is to map input patterns onto points in a feature space,
the purpose of classiﬁcation is to assign each point in the space with a class label or
membership scores to the deﬁned classes. Hence, once a pattern is mapped (represented),
the problem becomes one of the classical classiﬁcation to which a variety of classiﬁca-
tion methods can be applied. This chapter describes the classiﬁcation methods that have
shown success or are potentially effective to character recognition, including statistical
methods (parametric and nonparametric), structural methods (string and graph match-
ing), artiﬁcial neural networks (supervised and unsupervised learning), and combination
of multiple classiﬁers. Finally, some experimental results of classiﬁer combination on a
data set of handwritten digits are presented.
4.1
OVERVIEW OF CLASSIFICATION METHODS
The ﬁnal goal of character recognition is to obtain the class codes (labels) of character
patterns. On segmenting character patterns or words from document images, the task
of recognition becomes assigning each character pattern or word to a class out of a pre-
deﬁned class set. As many word recognition methods also take a segmentation-based
scheme with character modeling or character recognition embedded, as discussed in
Chapter 5, the performance of character recognition is of primary importance for
document analysis. A complete character recognition procedure involves the steps
of preprocessing, feature extraction, and classiﬁcation. On mapping the input pattern
Character Recognition Systems: A Guide for Students and Practitioner, by M. Cheriet, N. Kharma,
C.-L. Liu and C. Y. Suen
Copyright © 2007 John Wiley & Sons, Inc.
129

130
PATTERN CLASSIFICATION METHODS
to a point in feature space via feature extraction, the problem becomes one of the
classical classiﬁcation. For integrating the classiﬁcation results with contextual in-
formation like linguistics and geometrics, the outcome of classiﬁcation is desired to
be the membership scores (probabilities, similarity or dissimilarity measurements) of
input pattern to deﬁned classes rather than a crisp class label. Most of the classiﬁcation
methods described in this chapter are able to output either class label or membership
scores.
Pattern classiﬁcation has been the main theme of pattern recognition ﬁeld and is
often taken as a synonym of “pattern recognition.” A rigorous theoretical foundation
of classiﬁcation has been laid, especially to statistical pattern recognition (SPR),
and many effective classiﬁcation methods have been proposed and studied in depth.
Many textbooks have been published and are being commonly referred by researchers
and practitioners. Some famous textbooks are the ones of Duda et al. [17] (ﬁrst
edition in 1973, second edition in 2001), Fukunaga [23] (ﬁrst edition in 1972, second
edition in 1990), Devijver and Kittler [12], and so on. These textbooks mainly address
SPR. Syntactic pattern recognition was founded by Fu and attracted much attention
in 1970s and 1980s [22] but it has not found many practical applications. On the
contrary, structural pattern recognition methods using string and graph matching have
demonstrated effects in image analysis and character recognition.
From the late 1980s, artiﬁcial neural networks (ANNs) have been widely ap-
plied to pattern recognition due to the rediscovery and successful applications of the
back-propagation algorithm [75] for training multilayer networks, which are able to
separate class regions of arbitrarily complicated distributions. The excellent textbook
of Bishop [4] gives an in-depth treatment of neural network approaches from SPR
perspective. From the late 1990s, a new direction in pattern recognition has been with
support vector machines (SVMs) [92, 7, 10], which are supposed to provide optimal
generalization performance1 via structural risk minimization (SRM), as opposed to
the empirical risk minimization for neural networks.
This chapter gives an introductory description to the pattern classiﬁcation methods
that have been widely and successfully applied to character recognition. These meth-
ods are categorized into statistical methods, ANNs, SVMs, structural methods, and
multiple classiﬁer methods. Statistical methods, ANNs, and SVMs input a feature
vector of ﬁxed dimensionality mapped from the input pattern. Structural methods
recognize patterns via elastic matching of strings, graphs, or other structural descrip-
tions. By multiple classiﬁer methods, the classiﬁcation results of multiple classiﬁers
are combined to reorder the classes. We will describe the concepts and mathematical
formulations of the methods, discuss their properties, and provide design guidelines
in application to character recognition, and ﬁnally show some experimental examples
of character data classiﬁcation.
1In general, the parameters of a classiﬁer are estimated on a training sample set and its classiﬁcation per-
formance is evaluated on a separate test sample set. Generalization performance refers to the classiﬁcation
accuracy on test samples.

STATISTICAL METHODS
131
4.2
STATISTICAL METHODS
StatisticalclassiﬁcationmethodsarebasedontheBayesdecisiontheory,whichaimsto
minimize the loss of classiﬁcation with given loss matrix and estimated probabilities.
According to the class-conditional probability density estimation approach, statistical
classiﬁcation methods are divided into parametric and nonparametric ones.
4.2.1
Bayes Decision Theory
Assume that d feature measurements {x1, . . . , xd} have been extracted from the in-
put pattern, the pattern is then represented by a d-dimensional feature vector x =
[x1, . . . , xd]T . x is considered to belong to one of M predeﬁned classes {ω1, . . . , ωM}.
Given the a priori probabilities P(ωi) and class-conditional probability distributions
p(x|ωi), i = 1, . . . , M, the a posteriori probabilities are computed by the Bayes for-
mula
P(ωi|x) = P(ωi)p(x|ωi)
p(x)
=
P(ωi)p(x|ωi)
M
j=1 P(ωj)p(x|ωj)
.
(4.1)
Given a loss matrix [cij] (cij is the loss of misclassifying a pattern from class ωj
to class ωi), the expected loss (also called as conditional risk) of classifying a pattern
x to class ωi is
Ri(x) =
M

j=1
cijP(ωj|x).
(4.2)
The expected loss is then minimized by classifying x to the class of minimum condi-
tional risk.
In practice, we often assume that the loss of misclassiﬁcation is equal between any
pair of classes and the loss of correct classiﬁcation is zero:
cij =

1, i ̸= j,
0, i = j.
(4.3)
The conditional risk then becomes the expected error rate of classiﬁcation
Ri(x) =
M

j̸=i
cijP(ωj|x) = 1 −P(ωi|x),
(4.4)
and the decision becomes selecting the class of maximum a posteriori (MAP) proba-
bility to minimize the error rate. The error rate 1 −maxi P(ωi|x) is called Bayes error
rate.

132
PATTERN CLASSIFICATION METHODS
From Eq. (4.1) the a posteriori probability is proportional to the likelihood function
P(ωi)p(x|ωi)becausethedenominatorp(x)isindependentofclasslabel.So,theMAP
decision is equivalent to selecting the class of maximum likelihood:
max
i
g(x, ωi) = max
i
P(ωi)p(x|ωi),
i = 1, . . . , M,
(4.5)
or maximum log-likelihood:
max
i
g(x, ωi) = max
i
log P(ωi)p(x|ωi),
i = 1, . . . , M.
(4.6)
The likelihood and log-likelihood functions are also called discriminant functions. For
a pair of classes, ωi and ωj, the set of points in feature space with equal discriminant
value g(x, ωi) = g(x, ωj) is called decision surface or decision boundary.
To make Bayes decision (or simply minimum error rate decision) requires the a
priori probabilities and the conditional probability density functions (PDFs) of deﬁned
classes. The a priori probability can be estimated as the percentage of samples of a
classinthetrainingsampleset,orasoften,assumedtobeequalforallclasses.ThePDF
can be estimated by variable approaches. Parametric classiﬁcation methods assume
functional forms for class-conditional density functions and estimate the parameters
by maximum likelihood (ML), whereas nonparametric methods can estimate arbitrary
distributions adaptable to training samples.
4.2.2
Parametric Methods
Parametric methods assume a functional form for the density of each class
p(x|ωi) = f(x|θi),
i = 1, . . . , M,
(4.7)
where θi denotes the set of parameters for the density function of a class. In the
following, we will describe an important class of classiﬁers that assume Gaussian
density functions, parameter estimation by ML, and semiparametric classiﬁer with
Gaussian mixture density functions.
4.2.2.1
Gaussian Classiﬁers
Assuming multivariate Gaussian density (also
called normal density) has some beneﬁts: The theoretical analysis of properties and the
analytical estimation of parameters are easy, and many practical problems have class-
conditional densities that are approximately Gaussian. The parametric classiﬁers with
Gaussian density functions are often called Gaussian classiﬁers or normal classiﬁers,
which again have some variations depending on the degree of freedom of parameters.
The Gaussian density function of a class is given by
p(x|ωi) =
1
(2π)d/2|i|1/2 exp
"
−1
2(x −μi)T −1
i (x −μi)
#
,
(4.8)

STATISTICAL METHODS
133
where μi and i denote the mean vector and the covariance matrix of class ωi,
respectively. Inserting Eq. (4.8) into Eq. (4.6) the quadratic discriminant function
(QDF) is obtained:
g(x, ωi) = −1
2(x −μi)T −1
i (x −μi) −1
2 log |i| + log P(ωi).
(4.9)
In the above equation, the term common to all classes has been omitted. The term
(x −μi)T −1
i (x −μi) is also called Mahalanobis distance and can serve an effective
discriminant function. In some publications, QDF is taken as the negative of Eq. (4.9),
and sometimes the term log P(ωi) is omitted under the assumption of equal a priori
probabilities. The negative of QDF can be viewed as a distance measure, and as for the
Mahalanobis distance, the input pattern is classiﬁed to the class of minimum distance.
The decision surface of QDF, under g(x, ωi) = g(x, ωj), can be represented
by
1
2xT (−1
j
−−1
i )x + (μT
i −1
i
−μT
j −1
j )x + C = 0,
(4.10)
where C is a term independent of x. When −1
j
̸= −1
i , the decision surface is a
quadratic curvilinear plane; otherwise, it is a hyperplane.
The QDF assumes that the covariance matrix of Gaussian density of each class is
arbitrary. When a constraint is imposed such that
i = ,
(4.11)
the QDF becomes
g(x, ωi) = −1
2(x −μi)T −1(x −μi) −1
2 log || + log P(ωi)
= −1
2(xT −1x −2μT
i −1x + μT
i −1μi) −1
2 log || + log P(ωi),
(4.12)
where xT −1x and log || are independent of class label, so eliminating them does
not affect the decision of classiﬁcation. As a result, the discriminant function becomes
a linear discriminant function (LDF):
g(x, ωi) = μT
i −1x −1
2μT
i −1μi + log P(ωi)
= wiT x + wi0,
(4.13)
where wi = T μi and wi0 = log P(ωi) −1
2μT
i −1μi) are the weight vector and the
bias of LDF, respectively.

134
PATTERN CLASSIFICATION METHODS
The QDF can be further simpliﬁed by restricting the covariance matrix of each
class to be diagonal with an identical variance:
i = σ2I,
(4.14)
where I is the identity matrix. Inserting Eq. (4.14) into Eq. (4.9) and omitting a
class-independent term, the QDF becomes
g(x, ωi) = −∥x −μi∥2
2σ2
+ log P(ωi).
(4.15)
Further assuming equal a priori probabilities, the decision is equivalent to selecting
the class of minimum Euclidean distance ∥x −μi∥.
The minimum Euclidean distance rule was commonly used in character recogni-
tion before 1980s, under the title of template matching or feature matching. The LDF
generally outperforms the Euclidean distance because it copes with the correlation
between features. The QDF has many more free parameters than the LDF, and there-
fore needs a large number of samples to estimate the parameters. If there are not more
than d training samples for a class, the covariance matrix will be singular and the
inverse cannot be computed directly. On a ﬁnite sample set, the QDF does not nec-
essarily generalize better than the LDF because of its higher freedom of parameters.
The following section presents some strategies that can improve the generalization
performance of QDF.
4.2.2.2
Improvements to QDF
A straightforward strategy to improve the gen-
eralization performance of QDF is to restrict the freedom of parameters. The LDF
reduces the freedom by sharing a common covariance matrix for all classes, but it
will not perform sufﬁciently when the covariance matrices of different classes differ
signiﬁcantly. The regularized discriminant analysis (RDA) of Friedman [21] does not
reduce the number of parameters of QDF, but it constrains the range of parameter val-
ues by interpolating the class covariance matrix with the common covariance matrix
and the identity matrix:
ˆi = (1 −γ)[(1 −β)i + β0] + γσ2
i I,
(4.16)
where 0 = M
i=1 P(ωi)i, σ2
i = 1
d tr(i), and 0 < β, γ < 1. With appropriate val-
ues of β and γ empirically selected, the RDA can always improve the generalization
performance of QDF.
Another strategy, proposed by Kimura et al. [41], called modiﬁed quadratic dis-
criminant function (MQDF), can both improve the generalization performance and
reduce the computational complexity of QDF. Conforming to the notations of Kimura
et al., the negative QDF with the assumption of equal a priori probabilities is rewritten

STATISTICAL METHODS
135
as
g0(x, ωi) = (x −μi)T −1
i (x −μi) + log |i|
= [T
i (x −μi)]T −1
i T
i (x −μi) + log |i|
= d
j=1
1
λij
[φT
ij(x −μi)]2 +
d

j=1
log λij,
(4.17)
where  = diag[λi1, . . . , λid] with λij, j = 1, . . . , d, being the eigenvalues (ordered
in nonincreasing order) of i, and i = [φi1, . . . , φid] with φij, j = 1, . . . , d, being
the ordered eigenvectors.
In Eq. (4.17), replacing the minor eigenvalues (j > k) with a constant δi, the MQDF
is obtained as
g2(x, ωi)
= k
j=1
1
λij
[φT
ij(x −μi)]2 +
d

j=k+1
1
δi
[φT
ij(x −μi)]2
+ k
j=1 log λij + (d −k) log δi
= k
j=1
1
λij
[φT
ij(x −μi)]2 + 1
δi
ϵi(x) +
k

j=1
log λij + (d −k) log δi,
(4.18)
where k denotes the number of principal axes and ϵi(x) is the residual of subspace
projection:
ϵi(x) = ∥x −μi∥2 −
k

j=1
[(x −μi)T φij]2 =
d

j=k+1
[(x −μi)T φij]2.
(4.19)
The motivation of replacing minor eigenvalues is that they are prone to be underesti-
mated on small sample size, so replacing with a constant stabilizes the generalization
performance. Comparing with the QDF, the MQDF involves only the principal eigen-
vectors and eigenvalues, so both the storage of parameters and the computation of
discriminant function are reduced.
4.2.2.3 Parameter Estimation
Thedensityparametersofparametricclassiﬁers
are generally estimated by the ML method. Denote the set of parameters of a class ωi
by θi. Given a set of training samples Xi = {x1, . . . , xNi}, the likelihood function is
Li(θi) = p(Xi|θi) =
Ni
$
n=1
p(xn|θi).
(4.20)

136
PATTERN CLASSIFICATION METHODS
The parameters are estimated such that Li(θi) is maximized. Equivalently, the log-
likelihood is maximized:
max LLi = log Li(θi) =
Ni

n=1
log p(xn|θi).
(4.21)
In the case of Gaussian density, the density function of Eq. (4.8) is inserted into
Eq. (4.21). By setting ∇LLi = 0, many textbooks have shown that the solution of
ML estimation of Gaussian parameters is
ˆμi = 1
Ni
Ni

n=1
xn,
(4.22)
ˆi = 1
Ni
Ni

n=1
(xn −ˆμi)(xn −ˆμi)T .
(4.23)
For multiclass classiﬁcation with Gaussian density parameters estimated on a la-
beled training sample set X = {x1, . . . , xN}, N = M
i=1 Ni, if the covariance ma-
trices are arbitrary, the parameters of each class can be estimated independently by
Eqs. (4.22) and (4.23) on the samples of one class. If a common covariance matrix is
shared as for the LDF, the common matrix is estimated by
ˆ0 = 1
N
M

i=1

xn∈ωi
(xn −ˆμi)(xn −ˆμi)T =
M

i=1
ˆP(ωi) ˆi,
(4.24)
where ˆP(ωi) = 1
N
N
n=1 I(xn ∈ωi) is the estimate of a priori probability.
As for the QDF, the parameters of MQDF are estimated class by class. First, the
mean vector and covariance matrix of each class are estimated by ML. Then the
eigenvectors and eigenvalues of covariance matrix are computed by K-L (Karhunen-
Loeve) transform. On artiﬁcially selecting a number k, the parameter δi can be set to
be either class-dependent or class-independent provided that it is comparable to the
eigenvalue λik. In a different context, Moghaddam and Pentland [63] showed that the
ML estimate of class-dependent δi is the average of minor eigenvalues:
δi =
tr(i) −k
j=1 λij
d −k
=
1
d −k
d

j=k+1
λij.
(4.25)
However, as the estimate of minor eigenvalues is sensitive to small sample size, this
ML estimate of δi does not give good generalization performance.
4.2.2.4
Semiparametric Classiﬁer
The Gaussian density function is a rea-
sonable approximation of pattern distributions in many practical problems, especially

STATISTICAL METHODS
137
for character recognition, where the distribution of each class in feature space is al-
most unimodal. Nevertheless, for modeling multimodal or unimodal distributions that
considerably deviate from Gaussian, the Gaussian function is insufﬁcient. A natural
extension is to use multiple Gaussian functions for modeling a distribution. This way,
called mixture of Gaussians or Gaussian mixture, can approximate arbitrarily com-
plicated distributions. As the density function still has a functional form, this method
is referred to as semiparametric.
For pattern classiﬁcation, the Gaussian mixture is used to model the distribution
of each class independently. In the following description, we ignore the class label in
the density function. Suppose that the conditional density of a class is modeled by a
mixture of m Gaussian functions (components), the density function is
p(x) =
m

j=1
Pjp(x|j)
subject to
m

j=1
Pj = 1,
(4.26)
where p(x|j) = p(x|μj, j) is a Gaussian function:
p(x|j) =
1
(2π)d/2|j|1/2 exp
"
−1
2(x −μj)T −1
j (x −μj)
#
.
(4.27)
The parameters of Gaussian mixture, {(Pj, μj, j)|j = 1, . . . , m}, are estimated
by maximizing the likelihood on a training sample set X = {x1, . . . , xN}:
max L =
N
$
n=1
p(xn) =
N
$
n=1
 m

j=1
Pjp(xn|j)

.
(4.28)
The maximum of this likelihood, or equivalently the log-likelihood, cannot be solved
analytically because the hidden values of component labels are not available. An
iterative algorithm, called expectation–maximization (EM) [11], was proposed to
solve this kind of optimization problems with hidden values. Although the details of
EM algorithm can be found in many references and textbooks, we give an outline in
the following.
Given the number of components m, initial values of parameters (P(0)
j , μ(0)
j , (0)
j ),
j = 1, . . . , m, the EM algorithm updates the parameters iteratively until the parameter
values do not change or a given number of iterations is ﬁnished. Each iteration (at
time t) has an E-step and an M-step:
• E-step: The probabilities that each sample belongs to the components are com-
puted based on the current parameter values:
P(t)(j|xn) =
P(t)
j p(xn|μ(t)
j , (t)
j )
m
i=1 P(t)
i p(xn|μ(t)
i , (t)
i )
,
n = 1, . . . , m.
(4.29)

138
PATTERN CLASSIFICATION METHODS
• M-step: The likelihood function is maximized based on the current component
probabilities, such that the parameters are updated:
μ(t+1)
j
=

n P(t)(j|xn)xn

n P(t)(j|xn) ,
(4.30)
(t+1)
j
=

n P(t)(j|xn)(xn −μ(t+1)
j
)(xn −μ(t+1)
j
)T

n P(t)(j|xn)
,
(4.31)
P(t+1)
j
= 1
N

n
P(t)(j|xn).
(4.32)
The Gaussian mixture model is prone to overﬁt the training data because it has
more parameters than the multivariate Gaussian. A way to alleviate the overﬁtting and
reduce the computational complexity is to use diagonal or even identity covariance
matrices. Another way is to model mixture density in a low-dimensional subspace
and combine the mixture density in the principal subspace and a unitmodal Gaussian
in the residual subspace [63].
4.2.3
Nonparametric Methods
Nonparametric classiﬁers do not assume any functional form for the conditional dis-
tributions. Instead, the density can have arbitrary shape depending on the training data
points. A simple way is to model the density using histograms, that is, partitioning the
feature space into bins or grids and count the data points in each bin or grid. This is
feasible for only very low-dimensional feature spaces. For high-dimensional space,
the number of grids is exponential and the training data becomes sparse.
A general approach to density estimation is based on the deﬁnition of probability
density: the frequency of data points in unit volume of feature space. Assume that
the density p(x) is continuous and does not vary considerably in a local region R, the
probability that a random vector x, governed by the distribution of p(x), fall in the
region R can be approximated by
P =

R
p(x′)dx′ ≈p(x)V,
(4.33)
where V is the volume of R. On the other hand, if there are N data points (N is
sufﬁciently large) and k of them fall in the local region R around x, the probability
can be approximated by
P ≈k
N .
(4.34)

STATISTICAL METHODS
139
Combining Eqs. (4.33) and (4.34) results in
p(x) ≈
k
NV .
(4.35)
There are basically two approaches in applying formula (4.35) to practical density
estimation. One is to ﬁx the volume V and determine the value of k from the data,
and another is to ﬁx the value of k and determine the volume V from the data. The
two approaches lead to Parzen window (or called kernel-based) method and k-nearest
neighbor (k-NN) method, respectively.
4.2.3.1
Parzen Window Method
The Parzen window method assumes that the
local region around data point x is a hypercube with edge length h, then the volume
of region is V = hd. The window function of unit hyercube is deﬁned by
H(u) =

1,
|ui| < 1/2, i = 1, . . . , d
0,
otherwise.
(4.36)
Given a set of training data points X = {x1, . . . , xN}, the number of points falling in
the window around a new vector x is
k =
N

n=1
H
x −xn
h

.
(4.37)
Substituting this number into Eq. (4.35) leads to the density estimate around x:
ˆp(x) = 1
N
N

n=1
1
hd H
x −xn
h

.
(4.38)
It is shown in textbooks [17, 4] that the expectation of the estimated density in
Eq. (4.38) is a smoothed version of the true density:
E[ˆp(x)] = 1
N
N

n=1
E
 1
hd H
x −xn
h

= E
 1
hd H
x −x′
h

=

1
hd H
x −x′
h

p(x′)dx′.
(4.39)
The kernel width h plays the role of a smoothing parameter and should be carefully
selected to compromise the bias and variance. On ﬁnite training sample size, a small
value of h leads to an overﬁtting of training data, whereas a large value leads to a
biased estimation of density.

140
PATTERN CLASSIFICATION METHODS
The window function can be extended to other forms of function provided that
H(x) ≥0 and

H(x)dx = 1 are satisﬁed. The standard Gaussian function is a natural
choice of window function of inﬁnite support:
H(u) =
1
(2π)d/2 exp

−∥u∥2
2

.
(4.40)
Accordingly, the density estimate of (4.38) is
ˆp(x) = 1
N
N

n=1
1
(2πh2)d/2 exp

−∥x −xn∥2
2h2

.
(4.41)
For classiﬁcation, the formula (4.38) or (4.41) can be used to estimate the condi-
tional density around x for each class, and then the Bayes decision rule is applied.
The value of h can be selected by cross-validation such that the selected value leads
to a high classiﬁcation accuracy on a validation data set disjoint from the training set.
A serious problem with the Parzen window method is that all the training samples
should be stored for density estimation and classiﬁcation. A way to alleviate the
storage and computation overhead is to model the density using a small number of
kernel functions centered at selected points. This way is similar to the semiparametric
density estimation method, and if the Gaussian window function is used, the center
points can be estimated by the EM algorithm.
4.2.3.2 K-Nearest Neighbor Method
In the k-NN method, the volume of local
region R for density estimation is variable whereas the number k of data points falling
in R is ﬁxed. Given N training data points from M classes, for estimating the local
density around a new vector x, a window centered at x is enlarged such that exactly k
nearest data points fall in the window. Assume that the k nearest neighbors include ki
points from class ωi, i = 1, . . . , M, M
i=1 ki = k, the class-conditional density around
x is estimated by
ˆp(x|ωi) =
ki
NiV ,
i = 1, . . . , M,
(4.42)
where Ni is the number of training points of class ωi. The a posteriori probabilities
are computed by the Bayes formula:
p(ωi|x) =
P(ωi)ˆp(x|ωi)
M
j=1 P(ωj)ˆp(x|ωj)
=
P(ωi)ki/Ni
M
j=1 P(ωj)kj/Nj
,
i = 1, . . . , M.
(4.43)
Considering ˆP(ωi) = Ni
N , the computation of a posteriori probabilities is simpliﬁed
to
p(ωi|x) = ki
k ,
i = 1, . . . , M.
(4.44)

STATISTICAL METHODS
141
FIGURE 4.1
Voronoi diagram formed by training samples of two classes. The decision
surface is denoted by thick lines.
This is to say, the a posteriori probability of a class is simply its fraction of k nearest
neighbors in training samples, and the decision is to select the class with most nearest
neighbors among k. This decision rule is called k-NN rule.
When k is set equal to one, the k-NN rule is reduced to the nearest neighbor (1-NN)
rule that classiﬁes the input pattern x to the class of the nearest training sample. When
the distance measure is the Euclidean distance, the decision regions of the 1-NN rule
in feature space form a Voronoi diagram (Voronoi tessellation), with each cell deﬁned
by a training sample and separated from each other by a hyperplane that is equidistant
from two samples. An example of Voronoi diagram is shown in Figure 4.1.
It has been shown in textbook [17] that when the number of training samples
approaches inﬁnity, the classiﬁcation error rate of 1-NN rule is bounded by two times
the Bayes error rate. More exactly, denoting the Bayes error rate by P∗, the error rate
of 1-NN rule is bounded by
P∗≤P ≤P∗
2 −
M
M −1P∗
.
(4.45)
As for the Parzen window method, the drawback of k-NN and 1-NN methods is
that all the training samples are required to be stored and matched in classiﬁcation.
Both the Parzen window and nearest neighbor methods are not practical for real-
time applications, but because of their fairly high classiﬁcation performance, they
serve as good benchmarks for evaluating other classiﬁers. There exist many efforts
aiming to reduce the storage and computation overhead of nearest neighbor methods.

142
PATTERN CLASSIFICATION METHODS
Signiﬁcant savings of storage and computation can be obtained by wisely selecting
or synthesizing prototypes from training samples while maintaining or improving the
classiﬁcation performance. A supervised prototype learning method, called learning
vector quantization (LVQ) [43], is often described in connection with ANNs, as
addressed in the next section.
4.3
ARTIFICIAL NEURAL NETWORKS
ANNs were initially studied with the hope of making intelligent perception and cog-
nition machines by simulating the physical structure of human brains. The principles
and algorithms of ANNs have found numerous applications in diverse ﬁelds includ-
ing pattern recognition and signal processing. A neural network is composed of a
number of interconnected neurons, and the manner of interconnection differentiates
the network models into feedforward networks, recurrent networks, self-organizing
networks, and so on. This section focuses on the neural network models that have
been widely applied to pattern recognition. We describe the network structures and
learning algorithms but will not go into the details of the theoretical foundation. For
a comprehensive description of neural networks, the textbook of Haykin [29] can be
referred.
In neural networks, a neuron is also called a unit or a node. A neuronal model
has a set of connecting weights (corresponding to synapses in biological neurons), a
summing unit, and an activation function, as shown in Figure 4.2. The output of the
summing unit is a weighted combination of input signals (features):
v =
d

i=1
wixi + b.
(4.46)
FIGURE 4.2
Model of a neuron.

ARTIFICIAL NEURAL NETWORKS
143
The activation function can be either linear or nonlinear. A popular nonlinear activa-
tion function is the sigmoid or the logistic function:
g(a) =
1
1 + e−a .
(4.47)
The sigmoid function has two merits: (1) it is continuous and derivable such that
gradient learning involving it is possible; (2) the value of sigmoid function acts like
a posterior probability, which facilitates contextual integration of neuronal outputs.
The derivative of sigmoid function is
g′(a) = g(a)[1 −g(a)].
(4.48)
The probability-like feature of the sigmoid function can be seen from the a poste-
riori probability of a two-class Gaussian classiﬁer, which is shown to be sigmoidal.
Assume that two classes have Gaussian density functions as Eq. (4.8) with a common
covariance matrix , the a posteriori probability is
P(ω1|x) =
P(ω1)p(x|ω1)
P(ω1)p(x|ω1) + P(ω2)p(x|ω2)
=
P(ω1) exp[−1
2(x −μ1)T −1(x −μ1)]
P(ω1) exp[−1
2(x −μ1)T −1(x −μ1)]
+P(ω2) exp[−1
2(x −μ2)T −1(x −μ2)]
=
1
1 + P(ω2)
P(ω1)e−[(μ1−μ2)T −1x+C]
=
1
1 + e−[wT x+b] ,
(4.49)
where C is a constant independent of x, w = −1(μ1 −μ2), and b = C + log P(ω1)
P(ω2).
It can be seen from Eq. (4.46) that a single neuron performs as a linear discrim-
inator. A network with multiple neurons interconnected in a sophisticated manner
can approximate complicated nonlinear discriminant functions. Feedforward net-
works, including single-layer networks, multilayer networks, radial basis function
(RBF) networks, and higher order networks, are straightforward for approximating
discriminant functions via supervised learning. We will introduce the basic principle
of supervised learning in the context of single-layer networks and then extend to other
network structures. Unsupervised learning algorithms, including competitive learning
and self-organizing map (SOM), can ﬁnd the cluster description of a data set without
class labels. They can be applied to classiﬁcation by ﬁnding a description for each
class. We will also describe a hybrid learning algorithm, learning vector quantization
(LVQ), that adjust cluster centers in supervised learning.

144
PATTERN CLASSIFICATION METHODS
FIGURE 4.3
Single-layer neural network. The bias terms are not shown.
4.3.1
Single-Layer Neural Network
The structure of a single-layer neural network (SLNN) for multiclass classiﬁcation
is shown in Figure 4.3. The input signals (features) are connected to all the output
neurons, each corresponding to a pattern class. In the case of linear activation function,
the output of each neuron is a linear discriminant function:
yk(x) = vk =
d

i=1
wkixi + wk0 =
d

i=0
wkixi = wT
k x′,
k = 1, . . . , M,
(4.50)
where wk0 is a bias term, which can be viewed as the weight to a ﬁxed signal x0 = 1.
The weights of each neuron form a (d + 1)-dimensional weight vector wk, and x′ is
an enhanced (d + 1)-dimensional vector. The input pattern is classiﬁed to the class
of maximum output. Using sigmoid activation function (which is monotonic) does
not affect the decision of classiﬁcation because the order of output values remains
unchanged. However, the sigmoid function can be explained as a posteriori probability
and can improve the performance of weight learning.
The connecting weights of SLNN are estimated in supervised learning from a
labeled training sample set, with the aim of minimizing the classiﬁcation error or
an alternative objective. The learning algorithms include the perceptron algorithm,
pseudo inverse, gradient descent, and so on. The perceptron algorithm feeds the train-
ing samples iteratively and adjusts the weights in a ﬁxed step whenever a sample is
misclassiﬁed. It aims to minimize the classiﬁcation error on training samples but can-
not guarantee convergence when the samples are not linearly separable. The pseudo
inverse and gradient descent methods are described below.
4.3.1.1
Pseudo Inverse
The pseudo inverse is a noniterative method that esti-
mates the weights of single-layer network with linear output units (linear activation
functions). Given a training sample set X = {(xn, cn)|n = 1, . . . , N} (cn denotes the
class label of sample xn), the output values of each sample, yk(xn), k = 1, . . . , M,
are computed by Eq. (4.50). The desired outputs (target values) of a labeled sample

ARTIFICIAL NEURAL NETWORKS
145
(xn, cn) are set to
tn
k =

1,
k = cn,
−1,
otherwise.
(4.51)
The sum of squared error on the training set is
E = 1
2
N

n=1
M

k=1
[yk(xn) −tn
k ]2.
(4.52)
The weights of network are optimized with the aim of minimizing the sum of squared
error.
Representing all the weight vectors in a M × (d + 1) matrix W (each row cor-
responding to a class) and all the data (enhanced feature vectors) in a N × (d + 1)
matrix X (each row corresponding to a sample), the outputs of all samples can be
represented in a N × M matrix Y (each row contains the output values of a sample):
Y = XWT .
(4.53)
RepresentingthetargetvaluesofallsamplesinaN × M matrixT (eachrowcomposed
of the target values of a sample), the sum of squared error of Eq. (4.52) can be rewritten
as
E = 1
2∥XWT −T∥2.
(4.54)
For minimizing E, take
∂E
∂W = 0,
(4.55)
which results in
WT = (XT X)−1XT T = X†T,
(4.56)
where X† = (XT X)−1XT is the pseudo inverse of X.
The advantage of pseudo inverse is that the weights can be solved analytically.
The disadvantage is that the objective function is based on linear activation function
of neurons, which does not explain a posteriori probability and cannot ﬁt the 0–1
target values very well, though the objective function is minimized globally. Hence,
the weights thus estimated do not necessarily give high classiﬁcation performance.

146
PATTERN CLASSIFICATION METHODS
4.3.1.2
Gradient Descent
If the output neurons of SLNN take the sigmoid
activation function:
yk(x) = g(vk) =
1
1 + e−wT
k x′ ,
k = 1, . . . , M,
(4.57)
and accordingly, the desired outputs are set equal to
tn
k =

1,
k = cn,
0,
otherwise,
(4.58)
then the object function of Eq. (4.52) is a nonlinear function of weights and can no
longerbeminimizedbyanalyticalsolutionoflinearequations.Nonlinearoptimization
problems can be solved generally by gradient search, provided that the objective
function is derivable with respect to the free parameters. For the present problem of
minimizing (4.52), the weights and biases are adjusted iteratively by searching along
the negative direction of gradient in the space of parameters:
wk(t + 1) = wk(t) −η(t) ∂E
∂wk
,
k = 1, . . . , M,
(4.59)
where η(t) is the learning step, or called learning rate. The partial derivatives are
computed by
∂E
∂wk
=
N

n=1
M

k=1
[yk(xn) −tn
k ]yk(xn)[1 −yk(xn)]x′n,
k = 1, . . . , M,
(4.60)
where x′n is the enhanced vector of xn.
The complete gradient learning procedure is as follows. First, the parameters are
initialized to be small random values, wki(0) and wk0(0). Then the training samples
are fed to the network iteratively. Once all the samples have been fed, compute the
partial derivatives according to Eq. (4.60) and update the parameters according to
Eq. (4.59). This cycle of feeding samples and updating parameters is repeated until
the objective function does not change.
The updating rule of Eq. (4.59) updates the parameter values only once in a cycle
of feeding all samples. Thus, the samples need to be fed for many cycles until the
parameters converge. The convergence of learning can be accelerated by stochas-
tic approximation [72], which updates the parameters each time a sample is fed.
Stochastic approximation was originally proposed to ﬁnd the root of univariate func-
tional, and then extended to the optimization of univariate and multivariate objective
functions by ﬁnding the roots of derivatives. By stochastic approximation, the squared

ARTIFICIAL NEURAL NETWORKS
147
error is calculated on each input pattern:
En = 1
2
M

k=1
[yk(xn) −tn
k ]2.
(4.61)
Accordingly, the partial derivatives are computed by
∂En
∂wk
=
M

k=1
[yk(xn) −tn
k ]yk(xn)[1 −yk(xn)]x′n.
(4.62)
The parameters are then updated on each input pattern by
wk(t + 1) = wk(t) −η(t)∂En
∂wk
= wk(t) + η(t)δk(xn)x′n,
(4.63)
where δk(xn) = [tn
k −yk(xn)]yk(xn)[1 −yk(xn)] is a generalized error signal.
It has been proved that the stochastic approximation algorithm converges to a local
minimum of E (where ∇E = 0) with probability 1 under the following conditions
[72, 23]:
⎧
⎪
⎨
⎪
⎩
limt→∞η(t) = 0
∞
t=1 η(t) = ∞
∞
t=1 η(t)2 < ∞.
A choice of η(t) = 1/t satisﬁes the above conditions. In practice, the parameters are
updated in ﬁnite sweeps of training samples, and generally, setting the learning rate as
a sequence starting with a small value and vanishing gradually leads to convergence
if the objective function is continuously derivable.
Besides the squared error criterion, another criterion often used in neural network
learning is the cross-entropy (CE):
CE = −
N

n=1
M

k=1
%
tn
k log yk(xn) + (1 −tn
k ) log[1 −yk(xn)]
&
,
(4.64)
which can be minimized as well by gradient descent for learning the network weights.
Some researchers have proved that the minimization of either squared error or cross-
entropy criterion leads to the estimation of Bayesian a posteriori probabilities by
the output units of neural networks [71]. This is a desirable property for pattern
recognition tasks.
The gradient descent optimization method with stochastic approximation, also
referred to as stochastic gradient descent, can be used to estimate the parameters of
any other classiﬁer structures provided that the objective function is derivable with

148
PATTERN CLASSIFICATION METHODS
respect to the parameters. We will extend it to the training of multilayer and higher-
order networks under the objective of squared error.
4.3.2
Multilayer Perceptron
The neuronal output of SLNN performs as a linear discriminant function (the activa-
tion function does not affect classiﬁcation). No matter how the weights are learned,
the SLNN provides a linear hyperplane decision boundary between any two classes.
Hence, the SLNN is unable to separate classes with complicated distributions, for
which the decision boundary is generally nonlinear. A way to enhance the separat-
ing ability is to use nonlinear functions of pattern features as the inputs of a linear
combiner such that the linear combination, called generalized linear discriminant
function (generalized LDF), is a nonlinear function of the original features. The in-
termediate nonlinear functions to combine can be either predeﬁned or adaptively
formed.
The neural network classiﬁers that we are going to describe hereof, namely, multi-
layer perceptron (MLP), RBF network, and higher order (polynomial) network, all
fall in the range of generalized LDF. The intermediate functions of MLP are adaptive,
those of higher order network are ﬁxed, and those of RBF network can be either ﬁxed
or adaptive.
The MLP has an output layer of linear or sigmoidal neurons and one or more hidden
layers of sigmoidal neurons. The sigmoid activation function at the output layer does
not affect the decision of classiﬁcation, but it helps in gradient descent learning of
weights. Disregarding the sigmoid function at the output layer, the output values of
MLP can be viewed as generalized LDFs because the outputs of hidden neurons are
nonlinear. An example of MLP with one hidden layer (in total two layers) is shown
in Figure 4.4.
Assume that a two-layer MLP has d input signals, m hidden units, and M output
units, denoting the weights of the jth hidden unit by wji, its output as hj, and the
FIGURE 4.4
Multilayer perceptron. For simplicity, the bias terms are not shown.

ARTIFICIAL NEURAL NETWORKS
149
weights of the kth output unit by wkj, the output values are computed by
yk(x) = g[vk(x)] = g
" m
j=1 wkjhj + wk0
#
= g
" m
j=1 wkjg
 d
i=1 wjixi + wj0

+ wk0
#
= g
" m
j=1 wkjg[uj(x)] + wk0
#
,
k = 1, . . . , M,
(4.65)
where g(·) is the sigmoid activation function, vk(x) = m
j=1 wkjhj + wk0, and
uj(x) = d
i=1 wjixi + wj0.
4.3.2.1
Back-Propagation Learning
The weights of MLP are generally
trained on a labeled sample set to minimize the squared error, as in Eq. (4.52). This
learning algorithm is commonly referred to as back propagation (BP). Using stochas-
tic gradient descent, the samples are fed iteratively and the weights are adjusted on
each input pattern:
wi(t + 1) = wi(t) + 	wi(t),
	wi(t) = −η(t)∂En
∂wi
,
(4.66)
where wi represents any of the weights or biases. The problem now amounts to
computing the partial derivatives of En with respect to adjustable weights:
∂En
∂wi
= M
k=1[yk(xn) −tn
k ]∂yk(xn)
∂wi
= M
k=1[yk(xn) −tn
k ]yk(xn)[1 −yk(xn)]∂vk(xn)
∂wi
= −M
k=1 δk(xn)∂vk(xn)
∂wi
.
(4.67)
Speciﬁc to different weight terms, ∂vk(xn)
∂wi
is computed by (denoting h0 = 1 and
x0 = 1)
∂vk(x)
∂wkj
= hj,
j = 0, . . . , m,
(4.68)
and
∂vk(x)
∂wji
= wkj
∂hj
∂wji
= wkjhj(x)[1 −hj(x)]xi,
j = 1, . . . , m, i = 0, . . . , d.
(4.69)

150
PATTERN CLASSIFICATION METHODS
Combining Eqs. (4.66)–(4.69), the increments of weights on a training pattern x
are
	wkj(t) = η(t)δk(x)hj,
k = 1, . . . , M, j = 0, . . . , m,
	wji(t) = η(t) M
k=1 wkjδk(x)hj(x)[1 −hj(x)]xi,
= η(t)δj(x)xi,
j = 1, . . . , m, i = 0, . . . , d,
(4.70)
where δj(x) = M
k=1 wkjδk(x)hj(x)[1 −hj(x)] is the error signal back-propagated
from the output layer to the hidden layer. For a network with more than one hidden
layer, the error signal can be similarly propagated to every hidden layer, and the
increments of weights of hidden layers can be computed accordingly.
4.3.2.2
Acceleration to BP
The gradient descent learning algorithm guaran-
tees that the weights converge to a local minimum of squared error criterion. In the
parameter space, however, the surface of the squared error of MLP is complicated.
It may have many local minima and plateaus. A way to overcome local minima is
to train the network multiple times with different initial weight sets, which are ex-
pected to converge to different local minima, and then select the trained weights that
have the lowest squared error. At a point on the plateau, the weights cannot move
because the gradient of error surface is zero. An effective method to get rid of the
plateau is to use a momentum in weight updating, which also results in skipping
insigniﬁcant local minima.
The motivation of using momentum is to smooth the current increment of weights
to the past increments such that in the parameter space, the weights move partially
along the direction of past increments. By doing so, even if the gradient at current
weight values is zero, the weights still move along past directions. With a momentum
coefﬁcient 0 < γ < 1, the increment of a weight term is smoothed to
	wi(t) = −η(t)∂En
∂wi
+ γ	wi(t −1).
(4.71)
The stochastic gradient descent needs many iterations of feeding training sam-
ples and updating weights to guarantee converging to a local minimum. The number
of iterations can be reduced signiﬁcantly by using second-order derivatives, as in
Newton’s method:
w(t + 1) = w(t) −η(t)H−1∇E,
(4.72)
where H is the Hessian matrix, composed of the second-order derivatives of error
function with respect to the weight parameters:
Hij =
∂2E
∂wi∂wj
.
(4.73)
The second-order derivative reﬂects the curvature of error surface at a point of weight
values. The vector deﬁned by −H−1∇E is called Newton direction. When the error

ARTIFICIAL NEURAL NETWORKS
151
function is quadratic, the Newton direction points exactly to the minimum in the
parameter space. Assuming that the error function is quadratic in a local region,
updating according to Eq. (4.72) can draw the weights closer to the local minimum
of error surface than the negative gradient direction.
Though the Newton’s method can reduce the number of iterations for converg-
ing, the computation of the Hessian matrix is expensive. Approximating the Hessian
matrix to diagonal can signiﬁcantly simplify computation and the convergence can
still be accelerated to some extent. By doing this, only the diagonal elements of
Hessian, ∂2E
∂w2
i , need to be computed, and the inversion of diagonal matrix is trivial.
To compute the increments of weights, a constant λ is added for overcoming zero
curvature:
	wi = −η
∂2E
∂w2
i
+ λ
−1 ∂E
∂wi
.
(4.74)
In this formula, η

∂2E
∂w2
i + λ
−1
can be viewed as a learning rate adaptive to weight
terms. The adaptation of learning rate is important for accelerating the learning of
weights or parameters that have appreciably different magnitude of second-order
derivatives. This situation is not obvious for the weights of MLP, however.
Mostly, BP (stochastic gradient descent) learning with momentum works satisfac-
torily for MLP if the training samples are fed in a ﬁnite number of cycles and the
learning rate gradually vanishes.
4.3.2.3
Discussions
Many papers have discussed the approximation ability of
two-layer (one hidden layer) and three-layer (two hidden layer) MLPs. It has been
shown that a three-layer network can map any continuous function exactly from
d input variables to an output variable; and for classiﬁcation, a two-layer network
can approximate any decision boundary to arbitrary accuracy [4]. For a mapping
or classiﬁcation problem, the number of hidden nodes reﬂects the complexity and
approximation ability of the network.
For practical pattern recognition problems, the complexity of neural network
should be tuned to an appropriate size so as to generalize well to unseen new data.
The appropriate size depends on the inherent separability of pattern classes and the
number of training samples. A simple network will not be able to approximate the
training data to sufﬁcient accuracy and hence yields a large bias, whereas a complex
network will ﬁt the training data to an unnecessarily high accuracy and yields large
variance to noisy data. The bias and variance should be suitably compromised for
good generalization performance.
There are several ways to control the complexity of network or the freedom of
weights for good generalization. A straightforward way is to try networks of different
sizes and select the network that performs best on a validation data set disjoint from
the training set. This strategy is called cross-validation. The validation set can be a
holdout set extracted from the whole data set. Alternatively, the data set is partitioned

152
PATTERN CLASSIFICATION METHODS
into a number of subsets, and each subset is used for validation in rotation with the
network trained on the union of the remaining subsets. This is called rotation method
or leave-one-out (LOO).
The second strategy of tuning network complexity is to construct the network
dynamically by adding or removing nodes until the network approximates the training
data to an appropriate precision. Removing nodes, also referred to as pruning, is often
adopted to reduce the network complexity to avoid overﬁtting.
An alternative way for avoiding overﬁtting is to constrain the freedom of weights
instead of tuning the number of nodes. This can be achieved by two methods: weight
sharing and weight decay. By binding the weights for some nodes, the number of free
weights is reduced. Weight decay is to constrain the abstract values of weights. It is
equivalent to minimizing a regularized error function:
E =
N

n=1
En + β
2

i
w2
i ,
(4.75)
where β > 0 is a coefﬁcient of regularization, and wi is a connecting weight (not
including the bias terms). In weight updating by stochastic gradient descent, the
increment of weight is replaced by
	wi(t) = −η(t)
∂En
∂wi
+ β
N wi

.
(4.76)
The regularized squared error with weight decay in Eq. Eq. (4.75) can be applied
to other networks (SLNN, RBF network, polynomial network, etc.) as well, and the
weights are updated similarly.
The MLP has been frequently used in character recognition experiments and ap-
plications, and it is often used as a benchmark in pattern recognition research. In the
character recognition ﬁeld, some modiﬁcations to the structure of MLP have led to
special success. In a general scenario, a number of predeﬁned discriminating features
are extracted from character patterns and input to a network for classiﬁcation. In this
way, a fully connected two-layer MLP performs fairly well. Using a modular network,
with a subnet for each class, can further improve the accuracy [65]. A network work-
ing directly on character images, called convolutional neural network, has reported
great success [51]. It has multiple hidden layers with local connection and shared
weights. The hidden nodes, each connected to a local receptive ﬁeld in the preceding
layer, can be viewed as trainable feature extractors.
4.3.3
Radial Basis Function Network
The RBF network has one hidden layer, with each hidden node performing a localized
nonlinear function, mostly a Gaussian function. The response values of basis functions
are linearly combined by the output nodes. The activation function of output nodes can
be either linear or sigmoidal. To facilitate minimum error training by gradient descent

ARTIFICIAL NEURAL NETWORKS
153
FIGURE 4.5
Radial basis function network. The bias terms of output nodes are not shown.
and make the output values approximate a posteriori probabilities, we use sigmoid
functions in the output layer. A diagram of RBF network is shown in Figure 4.5.
For an RBF network with d input signals, m hidden units, and M output units,
assume spherical Gaussian functions for the hidden nodes
φj(x) = exp

−∥x −μj∥2
2σ2
j

,
j = 1, . . . , m,
(4.77)
where μj is the center of Gaussian, and σ2
j is the variance. The linear combination of
Gaussian functions
yk(x) = vk(x) =
m

j=1
wkjφj(x) + wk0 =
m

j=0
wkjφj(x),
k = 1, . . . , M
(4.78)
(denoting φ0(x) = 1) can be viewed as generalized linear discriminant functions.
On a training sample set X = {(xn, cn)|n = 1, . . . , N}, the parameters of RBF
network (center parameters and weights) can be estimated by minimizing the sum of
squared error, as in Eq. (4.52). By an early method, the parameters are estimated in two
stages. First, the Gaussian centers are estimated as the cluster centers on clustering
the training samples (either altogether or classwise), and the variances are averaged
over the samples in each cluster. The centers and variances are then ﬁxed and the
weights of output layer are estimated by squared error minimization. In the case of
linear outputs, the weights can be estimated by pseudo inverse, with the rows of data
matrix storing the Gaussian values on all training samples.
The outputs of RBF network can better approximate a posteriori class probabilities
by using the sigmoid activation function:
yk(x) = g(vk(x)) =
1
1 + e−vk(x) .
(4.79)

154
PATTERN CLASSIFICATION METHODS
In this situation, the weights of output layer can no longer be estimated by pseudo
inverse. On ﬁxed Gaussian functions in hidden layer, the weights can be efﬁciently
estimated by stochastic gradient descent:
wkj(t + 1) = wkj(t) −η(t) ∂En
∂wkj
,
(4.80)
where
∂En
∂wkj
= [yk(xn) −tn
k ]yk(xn)[1 −yk(xn)]φj(xn),
= −δk(xn)φj(xn).
(4.81)
The two-stage training method for RBF network is fast but will not necessarily
lead to a high classiﬁcation performance because the estimation of center parameters
via clustering does not consider the separability of patterns of different classes. An
alternative is to adjust all the parameters simultaneously in supervised learning by
error minimization [4]. Under supervised training of all parameters, the RBF network
can achieve a higher classiﬁcation performance with much fewer hidden nodes. By
doing this, the performance of RBF network can compete or even exceed the MLP.
The training algorithm is described below.
As in BP training for MLP, the parameters of RBF network are updated simulta-
neously to minimize the squared error on training samples (Eq. (4.52)). The center
parameters can be initialized by clustering as in two-stage training. By stochastic
gradient descent, the weights of output layer are updated on each input pattern as in
Eq. (4.80), and meanwhile, the center parameters (denoting τj = σ2
j ) are updated by
μj(t + 1) = μj(t) −η(t)∂En
∂μj
,
τj(t + 1) = τj(t) −η(t)∂En
∂τj
,
(4.82)
where
∂En
∂μj
= −
M

k=1
wkjδk(xn)∂φj(xn)
∂μj
,
∂En
∂τj
= −
M

k=1
wkjδk(xn)∂φj(xn)
∂τj
,
(4.83)

ARTIFICIAL NEURAL NETWORKS
155
and further,
∂φj(xn)
∂μj
= φj(xn)
2τj
(xn −μj),
∂φj(xn)
∂τj
= φj(xn)
2τ2
j
∥xn −μj∥2.
(4.84)
4.3.4
Polynomial Network
The higher order neural network (HONN) is also called as polynomial network or
polynomial classiﬁer [80]. Its output is a weighted combination of pattern features as
well as their polynomial expansions. For example, the output of a third-order neuron
(with linear activation function) is
y =

i

j≥i

k≥j
w(3)
ijkxixjxk +

i

j≥i
w(2)
ij xixj +

i
w(1)
i xi + w0.
(4.85)
The polynomial terms can be viewed as predeﬁned nonlinear features that, together
with the original pattern features, are the inputs to a single-layer neural network. Thus,
the pattern features and the polynomial terms form an enhanced feature vector, and
the outputs of polynomial network can be viewed as generalized linear discriminant
functions.
A major concern with polynomial networks is the huge number of polynomial
terms on high-dimensional features. For d features, the total number of polynomial
terms up to rth order is [82]
D =
r

i=0
d + i −1
i

=
d + r
r

.
(4.86)
With large d, the network will suffer from high computation complexity and will
give degraded generalization performance. The complexity can be reduced by either
reducing the number of input features or selecting expanded polynomial terms [80].
The former way (feature section or dimensionality reduction) is more computationally
efﬁcient and performs fairly well in practice. On the contrary, constrained polyno-
mial structures with moderate complexity have been proposed, such as the pi-sigma
network and the ridge polynomial network (RPN) [82]. They actually involve all the
polynomial terms of input features up to a certain order, but the weights of polynomi-
als are highly correlated. Therefore, they need polynomials of fairly high order (say,
5 or 6) to approximate complicated functions and cannot guarantee the precision of
approximation in difﬁcult cases.
For character recognition problems, which usually have hundreds of features,
a second-order polynomial (binomial) classiﬁer performs sufﬁciently. A binomial
network with dimensionality reduction by principal component analysis (PCA)
has shown superior classiﬁcation performance in character recognition experiments

156
PATTERN CLASSIFICATION METHODS
[46, 56]. In the following, we describe this type of network with sigmoidal output
nodes and gradient descent learning.
We assume that the d original features are transformed to m (m < d) subspace fea-
tures zj(x), j = 1, . . . , m. For M-class classiﬁcation, the outputs of binomial network
are
yk(x) = g(

i

j≥i
w(2)
kijzizj +

i
w(1)
ki zi + wk0),
k = 1, . . . , M,
(4.87)
where g(·) is the sigmoid activation function. By PCA, the subspace features are
computed by
zj = c · φT
j (x −μ),
j = 1, . . . , M,
(4.88)
where μ is the global mean of training vectors, φj, j = 1, . . . , M, are the eigenvectors
of the sample covariance matrix corresponding to the largest eigenvalues. A scaling
factor c is used to normalize the size of feature values by, for example, scaling the
largest variance of single subspace feature to one.
The connecting weights of binomial network (Eq. (4.87)) are optimized in
supervised learning to minimize the squared error of training samples (Eq. (4.52)). By
stochastic gradient descent, the weights are updated iteratively on each input pattern:
w(t + 1) = w(t) −η(t)∂En
∂w ,
(4.89)
where w represents w(2)
kij, w(1)
ki , or wk0. The partial derivatives are specialized as
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
∂En
∂w(2)
kij
= −δk(xn)zi(xn)zj(xn),
∂En
∂w(1)
ki
= −δk(xn)zi(xn),
∂En
∂wk0
= −δk(xn),
(4.90)
where δk(xn) = [tn
k −yk(xn)]yk(xn)[1 −yk(xn)].
On ﬁxing the polynomial terms, the polynomial network has one single layer of
adjustable weights, so the gradient descent training is fast and mostly converges to
a good solution though global optimum is not guaranteed in the case of sigmoid
activation function.
4.3.5
Unsupervised Learning
Unsupervised learning networks, including autoassociation networks, competitive
learning (CL) networks, and self-organizing map (SOM), have also been applied

ARTIFICIAL NEURAL NETWORKS
157
to pattern recognition. Whereas autoassociation learning aims to ﬁnd a subspace or
manifold of samples, competitive learning aims to ﬁnd representative points from
samples. Both result in a condensed representation of sample distribution, without
considering the class labels of samples in learning. For classiﬁcation, the training
samples of each class are learned separately to give a class-speciﬁc representation,
and a test pattern is then ﬁtted to the representation of each class and classiﬁed to the
class of minimum ﬁtting error.
The autoassociation network is a multilayer feedforward network and can be
viewed as a special structure of MLP. It has the same number of linear output nodes
as input nodes. In training, the input values are used as target values such that the
hidden nodes give a minimum error representation of samples and the output nodes
give a reconstruction of input values. When the hidden nodes have linear activation
functions, the network learns a linear subspace of samples just like PCA, no matter
how many hidden layers and nodes are used. With nonlinear (sigmoidal or hyper-
bolic) activation functions in hidden layers, the network learns a nonlinear subspace
or manifold of samples. Some results of character recognition using autoassociation
networks can be seen in [40].
In the following, we go into some details of competitive learning and SOM.
4.3.5.1
Competitive Learning
The network structure underlying competitive
learning has a single layer of output nodes. The weights of each output node con-
nected with input features give a representative vector (called reference vector or
codevector) of samples. The approximation of a set of samples with codevectors falls
in the classical vector quantization (VQ) problem [26]. VQ is actually a clustering
problem, where the codevectors are cluster centers. A sample vector is approximated
by the closest codevector from a codebook {mi|i = 1, . . . , L}. Assume that the sam-
ple vectors undergo distribution p(x), then the aim of VQ is to ﬁnd the codevectors
such that the reconstruction error is minimized:
min E =

∥x −mc∥2p(x)dx,
(4.91)
where mc, with c = c(x), is the closest codevector to x. On a ﬁnite set of training
samples {xn|n = 1, . . . , N}, the reconstruction error is computed by
E =
N

n=1
∥xn −mc∥2,
(4.92)
where c = c(xn).
For learning codevectors on samples to minimize the error (4.92), the codevectors
are initially set to random values or randomly selected from sample vectors. Then
in competitive learning, the codevectors are updated by stochastic gradient descent.
At time t, the closest codevector mc to sample x(t) is found, and the codevectors are

158
PATTERN CLASSIFICATION METHODS
updated by
mc(t + 1) = mc(t) + α(t)[x(t) −mc(t)],
mi(t + 1) = mi(t),
i ̸= c
(4.93)
where α(t) is a monotonically decreasing learning step. We can see that only the
closest codevector (the winner of competition) to the input vector is adjusted. Thus,
this competitive learning scheme is also called winner-take-all (WTA) learning or
hard competitive learning.
The WTA competitive learning can be viewed as the online version of a classical
partitional clustering algorithm, the k-means algorithm [35]. Both competitive
learning and k-means clustering are susceptible to local minimum of error function.
Competitive learning may also suffer from the node underutilization problem (some
codevectors are never adjusted in training). The so-called soft competitive learning
(SCL) algorithms can alleviate these two problems. Generally, SCL adjusts the
codevectors in different degrees according to the distance of each codevector to the
input vector. In the SCL scheme of Yair et al. [99], the probability of each codevector
on an input vector x(t) is computed based on soft-max:
Pi(t) =
e−γ(t)∥x(t)−mi∥2
L
j=1 e−γ(t)∥x(t)−mj∥2 ,
(4.94)
where γ(t) is a monotonically increasing parameter to control the hardness. The
codevectors are then adjusted by
mi(t + 1) = mi(t) + α(t)Pi(t)[x(t) −mi(t)].
(4.95)
Another soft competition scheme, called “neural-gas” [60], adjusts the codevectors
according to the ranks of distances. On an input vector x(t), the codevectors are
assigned ranks ki(x(t)) ∈{0, . . . , L −1}, i = 1, . . . , L, with 0 for the closest one and
L −1 for the farthest one. The codevectors are then adjusted by
mi(t + 1) = mi(t) + α(t)e−ki(x(t))/λ[x(t) −mi(t)],
(4.96)
where λ is a constant. The “neural-gas” is more computationally efﬁcient than (4.95)
as the probability-like coefﬁcient e−ki(x(t))/λ has L ﬁxed values. The experimental
results of ref. [60] also show that “neural-gas” has better convergence toward the
minimum of quantization error.
4.3.5.2
Self-Organizing Map
The SOM of Kohonen is a topology-preserving
VQ algorithm. The nodes corresponding to codevectors are spatially ordered in a
topology structure (a lattice, a graph, or a net). Unlike in soft competitive learning, the
codevectors are adjusted according to degrees depending on the distance of codevector

ARTIFICIAL NEURAL NETWORKS
159
FIGURE 4.6
Two-dimensional array of nodes for SOM.
to input vector, in SOM, the degree of adjustment depends on the adjacency of the node
with the winning node (corresponding to the winning codevector) in the topology.
Figure 4.6 shows a two-dimensional array of nodes for approximating high-
dimensional sample vectors. The ﬁlled circle denotes the winning node, and the
rectangles denote the neighborhood areas of three sizes. The learning rule of SOM is
outlined as follows. On an input vector x(t), denoting the winning codevector as mc,
the codevectors are adjusted by
mi(t + 1) = mi(t) + α(t)[x(t) −mi(t)],
i ∈Nc(t),
mi(t + 1) = mi(t),
otherwise,
(4.97)
where Nc(t) denotes the neighborhood of the winning node in the topology. Initially,
the size of neighborhood is large such that the neighboring nodes are drawn to
represent proximate points in the feature space. The neighborhood shrinks gradually
in learning such that the codevectors are adjusted to minimize the quantization error.
An alternative to the learning rule (4.97) uses “soft” neighborhood. In this scheme,
the nodes are weighted according to the topological distance to the winning node.
hci(t) = e−∥ri−rc∥/λ, where ri and rc denote the positions of the node and the winning
node in the topology, and λ decreases gradually. Accordingly, the codevectors are
adjusted by
mi(t + 1) = mi(t) + α(t)hci(t)[x(t) −mi(t)].
(4.98)
TheSOMyieldstheeffectthatthecodevectorsofadjacentnodesinthetopologyare
also proximate in the feature space. It can be used for VQ as hard and soft competitive
learning but is more suitable for exploring the adjacency structure of data. When used
for classiﬁcation, the SOM or competitive learning algorithm generates a number
of codevectors (to serve prototypes) for each class. The input pattern is compared
with the prototypes of all classes and is classiﬁed according to the k-NN or 1-NN
rule. As the prototypes are generated class by class without considering the boundary
between classes, they do not necessarily lead to high classiﬁcation accuracy. The
LVQ algorithm, to be described in the following, adjusts the prototypes with the
aim of improving classiﬁcation accuracy. The initial class prototypes of LVQ can be
generated by SOM, competitive learning, or k-means clustering.

160
PATTERN CLASSIFICATION METHODS
4.3.6
Learning Vector Quantization
The LVQ algorithm of Kohonen [43, 44] adjusts class prototypes with the aim
of separating the samples of different classes. The 1-NN rule with Euclidean
distance metric is generally taken for classiﬁcation. Kohonen proposed several
versions of LVQ that adjusts selected prototypes heuristically. Some improve-
ments of LVQ learn prototypes by minimizing classiﬁcation or regression error
[55]. In the following, we will review some representative algorithms of LVQ,
including LVQ2.1, LVQ3, the minimum classiﬁcation error (MCE) method of
Juang and Katagiri [37], and the generalized LVQ (GLVQ) of Sato and Yamada
[77].
As for neural training, the training patterns are fed to the classiﬁer repeatedly to
update the prototypes. For an input pattern x(t), ﬁnd the two closest prototypes mi
and mj to it, with distances di = ∥x(t) −mi∥2 and dj = ∥x(t) −mj∥2. If mi belongs
to the genuine class of x(t) whereas mj belongs to an incorrect class, and further the
two distances are comparable:
min
 di
dj
, dj
di

> 1 −w
1 + w,
(4.99)
where 0 < w < 1, the prototypes are updated by
mi(t) = mi(t) + α(t)[x(t) −mi(t)],
mj(t) = mj(t) −α(t)[x(t) −mj(t)],
(4.100)
where α(t) is a monotonically decreasing learning step. By updating, mi is drawn
toward x(t) and mj is pushed away from x(t).
The above algorithm is referred to as LVQ2.1. LVQ3 is enhanced based on LVQ2.1
with one more updating rule that when both the two closest prototypes mi and mj
belong to the genuine class of x(t), they are updated by
mk = mk + ϵα(t)(x −mk),
k = i, j,
(4.101)
where 0 < ϵ < 1. This rule enables the prototypes to be more representative of the
training samples.
Juang and Katagiri proposed a MCE training method for the parameter learning of
neural networks and statistical classiﬁers [37]. They deﬁned a loss function based on
discriminant functions, and the empirical loss on a training sample set is minimized
by gradient descent to optimize the classiﬁer parameters. For prototype classiﬁers,
the discriminant function of a class is the negative of the minimum distance from the
input pattern to this class (we now attach class label k to prototypes):
gk(x) = −min
j
∥x −mkj∥2.
(4.102)

ARTIFICIAL NEURAL NETWORKS
161
The misclassiﬁcation measure of a pattern from class k is given by
μk(x) = −gk(x) +
⎡
⎣
1
M −1

j̸=k
gj(x)η
⎤
⎦
1/η
,
(4.103)
which, as η →∞, becomes
μk(x) = −gk(x) + gr(x),
(4.104)
where gr(x) = maxi̸=k gi(x).
For prototype classiﬁers, the misclassiﬁcation measure is speciﬁed as
μk(x) = ∥x −mki∥2 −∥x −mrj∥2,
(4.105)
where mki is the closest prototype from the genuine class and mrj is the closest
prototype from incorrect classes. The misclassiﬁcation measure is transformed to
loss by
lk(x) = lk(μk) =
1
1 + e−ξμk ,
(4.106)
where ξ controls the hardness of 0–1 loss.2 On a training sample set, the empirical
loss is
L0 = 1
N
N

n=1
M

k=1
lk(xn)I(xn ∈ωk),
(4.107)
where I(·) is an indicator function that takes value 1 when the condition in the paren-
theses holds and 0 otherwise.
By stochastic gradient descent, the prototypes are updated on each input training
pattern. It is noteworthy that only two selected prototypes are involved in the loss on
a training pattern x(t). Their increments are computed by
	mki(t) = −α(t)∂lk(x)
∂mki
= 2α(t)ξlk(1 −lk)(x −mki),
	mrj(t) = −α(t)∂lk(x)
∂mrj
= −2α(t)ξlk(1 −lk)(x −mrj).
(4.108)
On an input pattern x, the GLVQ also updates the closest genuine prototype mki
and the closest incorrect prototype mrj, but the misclassiﬁcation measure is slightly
2In implementation, ξ should be inversely proportional to the average magnitude of |μk|, which can be
estimated on training samples with the initial prototypes. Whether ξ is constant or decreasing appears to
be not inﬂuential if the initial value is set appropriately.

162
PATTERN CLASSIFICATION METHODS
different:
lk(x) = dki(x) −drj(x)
dki(x) + drj(x),
(4.109)
where dki(x) and drj(x) are the distances from the input pattern to the two prototypes.
The loss and the empirical loss are computed in the same way as in MCE and the
empirical loss is minimized by stochastic gradient search, wherein the prototypes are
updated on feeding a pattern as
mki = mki + 4α(t)lk(1 −lk)
drj
(dki + drj)2 (x −mki),
mrj = mrj −4α(t)lk(1 −lk)
dki
(dki + drj)2 (x −mrj).
(4.110)
In practical implementation, the denominator (dki + drj)2 of Eq. (4.110) is replaced
by (dki + drj) for simpliﬁcation.
An experimental comparison of LVQ and other prototype learning algorithms
in character recognition shows that the MCE and GLVQ algorithms are among the
best performing ones [55]. These algorithms, updating only a few prototypes on a
training sample, are feasible for large category set problems like Chinese character
recognition.
The MCE learning method is applicable to any discriminant function-based
classiﬁers for parameter optimization. The adoption of MCE learning to the
modiﬁed quadratic discriminant function (MQDF) classiﬁer, resulting the so-called
discriminative learning quadratic discriminant function (DLQDF) [57], has shown
superior performance in handwritten character recognition.
4.4
SUPPORT VECTOR MACHINES
The SVM [7, 10] is a new type of hyperplane classiﬁer,3 developed based on the
statistical learning theory of Vapnik [92], with the aim of maximizing a geometric
margin of hyperplane, which is related to the error bound of generalization. The
research of SVMs has seen a boom from the mid-1990s, and the application of SVMs
to pattern recognition has yielded state-of-the-art performance.
Generally, an SVM classiﬁer is a binary (two-class) linear classiﬁer in kernel-
induced feature space and is formulated as a weighted combination of kernel func-
tions on training examples.4 The kernel function represents the inner product of two
vectors in linear/nonlinear feature space. In linear feature space, the decision function,
primarily as a weighted combination of kernel functions, can be converted to a linear
combination of pattern features. Thus, it has the same form as LDF and single-layer
3SVM can be used for both classiﬁcation and regression. We consider only SVM classiﬁcation in this book.
4In machine learning literature, a training pattern is usually called an example, an instance, or a point.

SUPPORT VECTOR MACHINES
163
neural network, but the weights are estimated in a totally different way. The nonlinear
feature space (possibly of inﬁnite dimensionality) is implicitly represented by the ker-
nel function, and it is not necessary to access the nonlinear features explicitly in both
learning and classiﬁcation. Multiclass classiﬁcation is accomplished by combining
multiple binary SVM classiﬁers.
As there have been many papers and some textbooks (e.g., [10, 79]) for describing
the details of SVMs, we will give only a brief introduction to the principle and learning
methodofmaximalmarginhyperplaneclassiﬁeranditsextensiontononlinearkernels,
and summarize some issues related to practical implementation.
4.4.1
Maximal Margin Classiﬁer
For binary classiﬁcation in a d-dimensional feature space, a linear decision function
is used:
f(x) = w · x + b,
(4.111)
where w is the weight vector, and b is a bias (−b is also called threshold). Classiﬁcation
is given by sgn[f(x)] (+1 and −1 correspond to two classes). In linearly separable
case, the decision function speciﬁes a hyperplane separating the points of two classes
(Fig. 4.7). To obtain a canonical form of the hyperplane, w and b are rescaled such
that the points of two classes closest to the hyperplane satisfy |w · x + b| = 1. Thus,
for all points xi, i = 1, . . . , ℓ, with labels yi ∈{+, −1}, yi(w · x + b) ≥1 holds. The
distance between the points of two classes closest to the hyperplane, 2/∥w∥, is called
the margin of the hyperplane.
FIGURE 4.7
Hyperplane classiﬁer for binary classiﬁcation and the margin.

164
PATTERN CLASSIFICATION METHODS
The decision function of SVM estimated from a training data set {(xi, yi)|i =
1, . . . , ℓ} is formulated by
f(x) =
ℓ

i=1
yiαi · k(x, xi) + b,
(4.112)
where k(x, xi) is a kernel function that implicitly deﬁnes an expanded feature space:
k(x, xi) = (x) · (xi),
(4.113)
where (x) is the feature vector in the expanded feature space and may have inﬁnite
dimensionality. In the linear feature space, k(x, xi) = x · xi = xT xi, and Eq. (4.112)
is equivalent to
f(x) =
ℓ

i=1
yiαi · (x · xi) + b =

ℓ

i=1
yiαixi

· x + b = w · x + b,
(4.114)
where w = ℓ
i=1 yiαixi.
The coefﬁcients αi, i = 1, . . . , ℓ, are estimated from the training points with the
aim of maximizing the margin with constraints of linear separation:
minimize τ(w) = 1
2∥w∥2,
subject to yi(w · xi + b) ≥1,
i = 1, . . . , ℓ.
(4.115)
This is a quadratic programing (QP) problem (primal problem) and can be converted
to the following dual problem by introducing Lagrange multipliers:
maximize W(α) = ℓ
i=1 αi −1
2
ℓ

i,j=1
αiαjyiyj · (xi · xj),
subject to αi ≥0, i = 1, . . . , ℓ,
and ℓ
i=1 αiyi = 0.
(4.116)
The coefﬁcients (multipliers) αi, i = 1, . . . , ℓ, are obtained by solving the above QP
problem. The bias b is determined by satisfying the Karush–Kuhn–Tucker (KKT)
complementarity conditions:
αi[yi(w · xi + b) −1] = 0,
i = 1, . . . , ℓ.
(4.117)
From the conditions, the coefﬁcient αi must be zero for those points with yi(w ·
xi + b) > 1 (nonboundary points). The boundary points, with yi(w · xi + b) = 1 and
αi > 0, are called support vectors. The value of b is actually determined by the KKT
conditions on the support vectors.

SUPPORT VECTOR MACHINES
165
4.4.2
Soft Margin and Kernels
The assumption that all the training points are separable (in linear or nonlinear feature
space) is often violated in practice. To deal with nonseparable points, the margin
constraints of Eq. (4.115) is relaxed to
subject to
yi(w · xi + b) ≥1 −ξi,
ξi ≥0, i = 1, . . . , ℓ.
(4.118)
The slack variable ξi is nonzero for nonseparable points. The objective of maximizing
margin is accordingly modiﬁed to a soft margin:
minimize
τ(w, ξ) = 1
2∥w∥2 + C
ℓ

i=1
ξi.
(4.119)
On the contrary, for classiﬁcation in nonlinear feature space, nonlinear kernel
functions k(x, xi) are used to replace the inner product x · xi. The learning problem
of SVM in a general kernel-induced feature space is then formulated as
minimize τ(w, ξ) = 1
2∥w∥2 + C
ℓ

i=1
ξi,
subject to yi(w · (xi) + b) ≥1 −ξi,
ξi ≥0, i = 1, . . . , ℓ,
(4.120)
where
w =
ℓ

i=1
yiαi(xi).
(4.121)
By introducing Lagrange multipliers, this QP problem is converted to a dual problem:
maximize W(α) =
ℓ

i=1
αi −1
2
ℓ

i,j=1
αiαjyiyj · k(xi, xj),
subject to 0 ≤αi ≤C, i = 1, . . . , ℓ,
and ℓ
i=1 αiyi = 0.
(4.122)
In this formula, the constant C (also called noise parameter) serves the upper bound
of multipliers and controls the tolerance of classiﬁcation errors in learning.
For learning SVM from examples, the parameter C and the kernel function k(x, xi)
are speciﬁed a priori, and the multipliers αi, i = 1, . . . , ℓ, are estimated by solving
the optimization problem of Eq. (4.122) using QP algorithms. The value of b is then
determined from the KKT conditions on the support vectors. How to select parameter
C and kernel functions and how to solve the QP problem are to be discussed in the
next subsection.

166
PATTERN CLASSIFICATION METHODS
The SVM has some attractive properties. First, the multipliers of training points
are estimated in QP, which guarantees to ﬁnd the global optimum because the convex
quadratic objective function of Eq. (4.122), constrained in the region of the parameter
space, has no local optimum. Second, the SVM realizes nonlinear classiﬁcation via
introducing kernel functions, which can be designed with some ﬂexibility and can
be generalized to nonvector representation of patterns. Third, the QP optimization of
SVM results in a sparse representation of patterns because only a fraction of training
points have nonzero multipliers and are used in decision. Further, the number of
support vectors (the complexity of SVM classiﬁer) is adaptable to the difﬁculty of
separation of training points. This property leads to good generalization performance
in classiﬁcation.
Besides the inner product of vectors (linear kernel, which realizes linear classiﬁ-
cation), the frequently used nonlinear kernel functions include
• Sigmoid kernel:
k(x, xi) = tanh

κ · (x · xi) + θ

.
(4.123)
• Polynomial kernel:
k(x, xi) =

κ · (x · xi) + 1
p.
(4.124)
• RBF kernel:
k(x, xi) = exp

−∥x −xi∥2
2σ2

.
(4.125)
Incorporating the kernel functions into the decision function (Eq. (4.112)), the SVM
with sigmoid kernel performs like a multilayer neural network, the one with polyno-
mial kernel performs like a polynomial neural network, and the one with RBF kernel
performs like an RBF neural network. The parameters of SVM are estimated in a
different way from neural networks, however.
The selection of kernel parameters (κ, θ, p, and σ2) will be discussed in the next
subsection. The polynomial and RBF kernels have reported superior performance in
pattern recognition applications.
4.4.3
Implementation Issues
For implementing SVMs for pattern classiﬁcation of practical problems, we discuss
four important issues: the implementation of QP, the selection of hyperparameters
(model selection), complexity reduction, and multiclass classiﬁcation. All these issues
have attracted much attention in research. We will not describe the details but will
summarize only some useful techniques.

SUPPORT VECTOR MACHINES
167
4.4.3.1
Implementation of QP
The objective of QP in Eq. (4.122) can be
maximized using standard optimization techniques like gradient ascent. The com-
plexity of storing the kernel matrix k(xi, xj), i, j = 1, . . . , ℓ, and the complexity of
computation increase quadratically with the size of the training data set. The storage
of kernel matrix is infeasible for large-size problems with over tens of thousands of ex-
amples. To solve the particular QP problem of SVM learning, many special techniques
have been developed. Mostly, the global optimization problem is decomposed into
many local ones on subsets of data such that the storage of the kernel matrix on a subset
is trivial and the optimization on a subset can be solved efﬁciently, sometimes ana-
lytically. The subset of data points (called “working set” or “active set”) is iteratively
selected and solved until some stopping conditions on the whole data set are satisﬁed.
Two useful criteria of stopping are the KKT conditions of the primal prob-
lem and the difference between the primal and dual objective functions (feasibil-
ity gap). Only at the optimum of multipliers, the KKT conditions are satisﬁed on
all training points, and the feasibility gap vanishes. Certain level of error is tol-
erated in checking the stopping condition such that a nearly optimal solution is
obtained in moderate computing time and the generalization performance is not
inﬂuenced.
A seminal algorithm of SVM learning is the sequential minimal optimization
(SMO) method of Platt [67]. The working set in SMO contains only two examples,
on which the two multipliers that maximize the objective function can be computed
analytically.Thetwoexamplesareheuristicallyselectedintwoloops:theﬁrstexample
is selected from nonbound (0 < αi < C) points that violate KKT conditions, and the
second one is selected such that the difference of decision error is maximized. Caches
of decision error and kernel functions are used to reduce repeated computation.
The successive overrelaxation (SOR) algorithm of Mangasarian and Musicant [58]
is even simpler than the SMO in that the working set contains only one example. This
is done by reformulating the SVM problem as
minimize
1
2∥w∥2 + C
ℓ

i=1
ξi + b2,
subject to
yi(w · (xi) + b) ≥1 −ξi,
ξi ≥0, i = 1, . . . , ℓ.
(4.126)
Its dual problem is
maximize
ℓ
i=1 αi −1
2
ℓ

i,j=1
αiαjyiyj · k(xi, xj) −1
2(
ℓ

i=1
yiαi)2,
subject to
0 ≤αi ≤C, i = 1, . . . , ℓ,
(4.127)
where b = ℓ
i=1 yiαi. We can see that the major difference between Eqs. (4.127) and
(4.122) is that the former removes the equality constraint ℓ
i=1 αiyi = 0. Without
the equality constraint, the objective can be maximized iteratively by solving one

168
PATTERN CLASSIFICATION METHODS
multiplier each time. The SOR algorithm is argued to have linear complexity with the
number of data points and is able to learn with as many as millions of points.
Another method for learning SVM with huge data set, proposed by Dong et al. [13],
follows the general SVM formulation with equality constraint. It quickly removes
most of the nonsupport vectors and approximates the kernel matrix with block diag-
onal matrices, with each solved efﬁciently.
For practitioners who are interested in applying SVMs more than theoretical and
algorithmic research, some publicly available softwares are very useful. The fre-
quently used softwares include the SVMlight [36] and the LIBSVM [8], and many
softwares are available at the public Web site for kernel machines [39].
4.4.3.2
Model Selection
The selection of hyperparameters (constant C, pa-
rameters of kernel function) in SVM learning is very important to the generalization
performance of classiﬁcation. The linear kernel turns out to have no parameter, but
actually the scaling of data vectors (equivalently, the scaling of inner product) is
important. The scaling problem is also with the sigmoid and polynomial kernels (κ
in Eqs. (4.123) and (4.124)). The width σ2 of RBF kernel is also closely related
to the scale of data. The value of C inﬂuences the convergence of learning and the
generalization performance.
As for designing neural networks, the hyperparameters of SVM can be selected in
the general strategy of cross-validation: LOO or holdout. On partitioning the avail-
able data into learning set and validation set, the SVM is learned multiple times with
different choices of hyperparameters, and the choice that leads to the highest classi-
ﬁcation accuracy on the validation set is selected. As the number of combinations of
hyperparameters (for a continuous parameter, multiple sampled values are taken) is
large, this strategy is very time-consuming.
Unlike cross-validation that checks hyperparameters exhaustively, greedy search
methods have been proposed to select hyperparameters by climbing up a classiﬁcation
performance measure. Chapelle et al. use an estimate of generalization error of SVM,
which is a function of support vector multipliers, as the objective of optimization and
search the hyperparameters by gradient descent [9]. Ayat et al. proposed an empirical
error minimization method [1]. In SVM learning with a choice of hyperparameters, the
empirical classiﬁcation error is estimated on training points with the output of SVM
transformed to a posteriori probability [68]. The hyperparameters are then updated
toward the negative direction of gradient. It is noteworthy that in model selection by
gradient search, the SVM is still learned multiple times.
In many cases, appropriate kernel models can be selected empirically via data
rescaling. Consider that the hyperparameters κ and σ2 are related to the scale of data
vectors, normalizing the scale of data from the statistics of training points may help to
ﬁnd appropriate model easily, or largely reduce the range of hyperparameter search.
The data vectors are rescaled uniformly: x′ = βx. For the sigmoid or polynomial
kernel, if the training vectors are rescaled such that the average self-inner product
of all vectors is normalized to one, a value of κ = 2i with i selected from −4 to 4
will lead to reasonably good convergence and generalization. For the RBF kernel,
consider the vector-to-center (square Euclidean) distance of each class. If the average

SUPPORT VECTOR MACHINES
169
distance over all classes is normalized to one by rescaling, a value of σ2 = 0.5 × 2i
with i selected from −4 to 4 will perform well. If the data distribution of each class
is multimodal, we can instead partition the data into clusters and estimate the scale
from the within-cluster distances.
The order p of polynomial kernel has only a few choices: it is an integer and usually
selected from {2, 3, 4, 5}. Empirically, the value of C can be sampled very sparsely
without inﬂuencing the performance of SVM: It usually increases by a factor of ten:
C = 10c, and usually, c ∈{−2, −1, 0, 1, 2}. From a few values of hyperparameters,
the optimal one can be selected by cross-validation.
4.4.3.3
Complexity Reduction
Though SVM learning by constrained QP
results in a sparse representation of patterns, the number of support vectors can still
be very large in some cases. Depending on the distribution of data points, the fraction
of support vectors in the training data set may be as small as 1% or as large as over
50%. In the latter case, the operation complexity of SVM classiﬁcation is compa-
rable to nonparametric (Parzen window or k-NN) classiﬁers. There have been some
efforts toward the reduction of operation complexity, which can be categorized into
three classes: sample reduction before SVM learning, complexity control in SVM
learning, and support vector reduction after learning.
Reducing the number of training examples can both accelerate SVM learning
and reduce the number of support vectors. This can be done in two ways: sample
compression and sample selection. By sample compression, the data points are
generally clustered (e.g., by k-means or vector quantization algorithms) and the
cluster centers are taken as the examples of SVM learning. There is a trade-off
between the sample reduction ratio and the generalization performance of SVM
classiﬁer. Examples can be selected using techniques like the editing of k-NN or
1-NN classiﬁers [12]. Consider that support vectors are mostly the points near
classiﬁcation boundary, it is reasonable to learn SVM with the points near the k-NN
or 1-NN classiﬁcation boundary. A method removes the examples of high conﬁdence
and of misclassiﬁcation after fusing multiple SVMs learned on subsets of data [2].
The complexity can be controlled in SVM learning via directly modifying the
objective of optimization. In the method of [15], the objective is formed by insert-
ing Eq. (4.121) into Eq. (4.119) and adding a term controlling the magnitude of
multipliers:
minimize
L(α) = 1
2
ℓ

i,j=1
αiαjyiyj · k(xi, xj) + C
ℓ

i=1
ξi + D
ℓ

i=1
αi.
(4.128)
Another method [59] directly uses the number of support vectors as a regularizing
term of objective:
minimize
∥α∥0 + C
ℓ

i=1
φ(yif(xi)),
(4.129)

170
PATTERN CLASSIFICATION METHODS
where ∥α∥0 denotes the number of nonzero multipliers, φ(z) = 1 −z if z ≤1 and 0
otherwise.
After SVM learning, the support vectors can be compressed by synthesizing
another smaller vector set or selecting a subset under the constraint that the classi-
ﬁcation performance of SVM is not sacriﬁced considerably. A method synthesizes a
vector set with the aim that the squared error between the new weight vector and the
weight vector before reduction is minimized [6]. The resulting vectors are not really
support vectors but are used in the decision function for approximation. Another
method exploits the linear dependency of support vectors in the feature space and
removes the redundant ones without affecting the precision of decision [14]. By this
method, the ratio of reduction is largely variable depending on the redundancy.
4.4.3.4 Multiclass Classiﬁcation
SVMsaremostlyconsideredforbinaryclas-
siﬁcation. For multiclass classiﬁcation, multiple binary SVMs, each separating two
classes or two subsets of classes, are combined to give the multiclass decision. Two
general combination schemes are the one-versus-all and the one-versus-one. For an
M-class problem, the one-versus-all scheme uses M SVMs, each separating one class
from the union of the remaining ones. The SVM for class ωi is learned with the exam-
ples of ωi labeled as +1, and the examples of the other classes labeled as −1. After
learning, it gives a decision function fi(x). The test pattern x is classiﬁed to the class
of maximum decision fi(x).
In more sophisticated decision, the decision functions fi(x) are transformed to a
posteriori probabilities by ﬁtting sigmoid functions [67]:
Pi(y = +1|x) =
1
1 + e−(Afi(x)+B) ,
(4.130)
where A and B are estimated in logistic regression5 by minimizing a negative log-
likelihoodonatrainingdatasetoftwoclasses{+1, −1}.Thepatternx isthenclassiﬁed
to the class of maximum a posteriori probability.
In the one-versus-one scheme, each SVM is used to discriminate between a pair
of classes. Hence, for M classes, there should be M(M −1)/2 SVMs. Each SVM,
learned with the examples of two classes (ωi labeled as +1 and ωj labeled as −1),
gives a decision function fij(x). The multiclass decision based on pairwise binary
classiﬁers can be made by three rules. The ﬁrst rule is the maximum votes of wins
[45]. The decision function fij(x) assigns a vote to ωi when fij(x) > 0 or to ωj
otherwise. The pattern x is classiﬁed to the class of maximum number of votes.
The directed acyclic graph (DAG) approach of Platt [69] does not activate the
M(M −1)/2 binary classiﬁers simultaneously, but rather organize them in a DAG
structure and calls them dynamically. The DAG has a triangular structure, with the
number of nodes in each layer increasing by one, from one root node to M −1 leaf
nodes. Each node (binary classiﬁer) fij(x) excludes the class ωj if fij(x) > 0 or
5Logistic regression will be described in the context of combining multiple classiﬁers at the measurement
level.

STRUCTURAL PATTERN RECOGNITION
171
excludes ωi otherwise. On traversing from the root node to a leaf node, M −1 classes
are excluded and the pattern x is classiﬁed to the remaining class. The DAG approach
activates only M −1 binary classiﬁers for classifying a pattern, and hence is more
efﬁcient than the maximum votes decision. Its disadvantage is that it does not give a
conﬁdence score to the ﬁnal decision.
The third rule of one-versus-one combination is based on MAP probability, via
deriving M class probabilities from M(M −1)/2 binary a posteriori probabilities.
Denote
the
a
posteriori
probability
of
binary
classiﬁer
fij(x)
as
rij =
P(ωi|x, {ωi, ωj}), which can be transformed by, for example, Eq. (4.130). The class
probabilities pi = P(ωi|x), i = 1, . . . , M, can be derived from rij by pairwise cou-
pling [28], which is outlined as follows. Assume that there are nij examples for a
pair of classes {ωi, ωj}. Start with initial guess of class probabilities ˆpi (normalized
to unity sum) and compute ˆμij = ˆpi/(ˆpi + ˆpj). Repeat updating probabilities until
convergence:
ˆpi ←ˆpi ·

j̸=i nijrij

j̸=i nij ˆμij
,
(4.131)
renormalize ˆpi and recompute ˆμij. The convergence condition is to maximize the
average Kullback–Leibler divergence between rij and μij:
KL(p) =

i<j
nij

rij log rij
μij
+ (1 −rij) log 1 −rij
1 −μij

.
(4.132)
4.5
STRUCTURAL PATTERN RECOGNITION
Structural pattern recognition methods are used more often in online character recog-
nition [85, 53] than in ofﬂine character recognition. Unlike statistical methods and
neural networks that represent the character pattern as a feature vector of ﬁxed dimen-
sionality, structural methods represent a pattern as a structure (string, tree, or graph) of
ﬂexible size. The structural representation records the stroke sequence or topological
shape of the character pattern, and hence resembles well to the mechanism of human
perception. In recognition, each class is represented as one or more structural tem-
plates, the structure of the input pattern is matched with the templates and is classiﬁed
to the class of the template of minimum distance or maximum similarity. The struc-
tural matching procedure not only provides an overall similarity but also interprets
the structure of the input pattern and indicates the similarities of the components.
Despite the above merits of structural recognition, statistical methods and neural
networks are more often adopted for the ease of feature extraction and learning
from samples. Structural methods face two major difﬁculties: extracting structural
primitives (strokes or line segments) from input patterns, and learning templates
from samples. Primitive extraction from online character patterns (sequences of
pen-down points) is much easier than from ofﬂine character images. Structural

172
PATTERN CLASSIFICATION METHODS
template learning from samples is undergoing study and has gained some progress.
In practice, the templates are often selected from samples, constructed artiﬁcially or
interactively.
Structural pattern recognition is often mentioned together with syntactic pattern
recognition, which represents patterns and classes using formal linguistics and
recognizes via grammatical parsing. Extracting linguistic representation from
patterns is even more difﬁcult than structural representation. This is why syntactic
methods have not been widely used in practical recognition systems.
In the following, we describe two important types of structural recognition
techniques that are useful for character recognition: attributed string matching and
attributed graph matching. String matching techniques are often used in character
string recognition as well, for matching string patterns with lexicon entries (see
Chapter 5). In this section, we focus on the basic algorithms and take single character
recognition as an example.
4.5.1
Attributed String Matching
When a string of symbols (e.g., letters from an alphabet) is to be matched (aligned)
with a reference string, it is deleted symbols from, inserted symbols into, or substituted
symbols such that the edited string is identical to the reference string. The total number
of edit operations (deletions, insertions, and substitutions) is called edit distance
or Levenstein distance. The edit distance can be efﬁciently computed by dynamic
programing (DP) search, using an array (lattice) for representing the search space
[94].
A pattern shape can be represented as a string of primitives, say, the sequence of
stroke points, contour points, or line segments. When a symbol (pattern primitive) is
represented by a number of attributes (features), the string is called an attributed string.
The attributes are used to compute the cost of edit operations in string matching, and
the matching distance is also called as weighted edit distance.
Consider two strings A = a1 · · · am and B = b1 · · · bn. The cost of edit operations
d(ai, bj) ≥0, i = 0, 1, . . . , m, j = 0, 1, . . . , n (i + j > 0), is deﬁned as either ﬁxed
or attribute-dependent values. Let us take A as reference. When i = 0, d(ai, bj) de-
ﬁnes deletion cost; when j = 0, d(ai, bj) is insertion cost; and otherwise, d(ai, bj)
is substitution cost. All the possible edit operations of B matching with A can be
represented in the array M(i, j), i = 0, 1, . . . , m, j = 0, 1, . . . , n. Each point (i, j)
in the array can be viewed as a node in a network. An example of array is shown in
Figure 4.9, where the vertical axis denotes index i and the horizontal axis denotes j.
In the array, every forward path6 from (0, 0) through (m, n) corresponds to a match.
For describing the algorithm for ﬁnding the optimal path (optimal sequence of
edit operations) of string matching, two arrays of accumulated distance D(i, j) and
backpointer T(i, j) are deﬁned. D(i, j) records the accumulated cost of optimal
path from (0, 0) to (i, j). T(i, j) records the coordinates t ∈{(i −1, j −1), (i −
1, j), and (i, j −1)} that D(i, j) is derived from. t = (i −1, j −1) indicates that bj
6In a forward path, every step has an increment 	i > 0 or 	j > 0 or both.

STRUCTURAL PATTERN RECOGNITION
173
is substituted by (matched with) ai, t = (i −1, j) indicates that ai is inserted, and
t = (i, j −1) indicates that bj is deleted. On updating all the values of D(i, j) and
T(i, j), D(m, n) gives the value of edit distance, and T(m, n)T(T(m, n)) · · · (0, 0)
gives the backtrack path of editing. The algorithm is as follows:
Algorithm: Computing string edit distance
1. Initialization.
• Set D(0, 0) = 0.
• For i = 1, . . . , m, update D(i, 0) = D(i −1, 0) + d(i, 0), T(i, 0) = (i −
1, 0).
• For
j = 1, . . . , n,
update
D(0, j) = D(0, j −1) + d(0, j),
T(0, j) =
(0, j −1).
2. Forward updating. For i = 1, . . . , m, j = 1, . . . , n,
• Update D(i, j) by
D(i, j) = min
⎧
⎪
⎨
⎪
⎩
D(i −1, j) + d(i, 0),
D(i, j −1) + d(0, j),
D(i −1, j −1) + d(i, j)
⎫
⎪
⎬
⎪
⎭
.
(4.133)
• Corresponding to the minimum of the three rows of Eq. (4.133), update
T(i, j) by
T(i, j) = select
⎧
⎪
⎨
⎪
⎩
(i −1, j),
(i, j −1),
(i −1, j −1)
⎫
⎪
⎬
⎪
⎭
.
(4.134)
3. Backtracking. Backtrack the editing path from (m, n) to (0, 0) as
T(m, n)T(T(m, n))(T(T(m, n))) · · · (0, 0).
In the algorithm, we can see that the main computation is to update the accumu-
lated cost and the trace at each point of the array, so the complexity of computation is
O((m + 1) · (n + 1)) = O(m · n). The updating of D(i, j) in Eq. (4.133) follows DP,
which relies on the Bellman principle of optimality, that is, the minimum cost of all
paths from the start node to the terminal node through an intermediate node equals the
sumoftwominimumcosts,fromthestarttotheintermediateandfromtheintermediate
to the terminal. This is why every point in the array retains only the minimum accu-
mulated cost and traces the point that the minimum accumulated cost is derived from.
When a character is represented as a string of sampled points, the DP matching of
two strings is also referred to as dynamic time warping (DTW), as have been used in
speech recognition. The substitution cost between two points can be computed from
the geometric distance between them, the difference of direction of adjacent line
segments, and so on. As an example, two online shapes of character “2” are shown in
Figure 4.8. They are encoded in 11 and 12 sampled points, respectively. The matching

174
PATTERN CLASSIFICATION METHODS
FIGURE 4.8
Two shapes of character “2” encoded as strings of sampled points.
distance between them is computed by DP as in Figure 4.9, where the editing path
is shown by the thick line, and the correspondence between points is shown on the
right.
For shape matching for sequence of line segments, Tsai and Yu proposed an
attributed string matching algorithm with merging [88], where the editing path at
node (i, j) is extended not only from {(i −1, j −1), (i −1, j), (i, j −1)} but also
from (i −k, j −l) (k > 1, l > 1). Tsay and Tsai have applied this kind of matching
algorithm to online Chinese character recognition [89].
Another extension of string matching is the normalized edit distance [61], which
is deﬁned as the ratio between the weighted edit distance and the length of editing
path (number of edit operations). The normalization of edit distance is important to
overcome the variability of string length in shape recognition. As the length of editing
path is unknown a priori, the minimum normalized edit distance must be computed by
an algorithm of higher complexity. If we simply normalize w.r.t. the length of strings
(m, n, or any combination of them), the weighted edit distance is computed by DP
and then divided by the length.
4.5.2
Attributed Graph Matching
A graph comprises a set of nodes (vertices) and a set of arcs (edges) for deﬁning
the relationship between nodes. An attributed relational graph (ARG) is a graph with
nodes and edges described by attributes (features) and can be denoted by a triple
G = (V, E, A) (V, E, A denote the sets of nodes, edges, and attributes, respectively).
A character pattern can be represented as an ARG, generally with nodes denoting
strokes and edges denoting interstroke relationships. A simple example of ARG is
FIGURE 4.9
Editing path of the strings in Figure 4.8 and the correspondence of points.

STRUCTURAL PATTERN RECOGNITION
175
FIGURE 4.10
A character shape and its ARG representation.
shown in Figure 4.10, where a character pattern has four strokes and the closely
dependent strokes are related by edges in the graph. For character recognition using
ARGs, each class is modeled as one or more template graphs, and the graph of the
input pattern is matched with the templates and classiﬁed to the class of the template
of minimum matching distance.
Similar to string matching, the matching distance between two ARGs can be
formulated as an edit distance, which is the sum of costs in transforming the input
pattern graph to a model graph by node/edge deletion, insertion, and substitution
[76]. However, to ﬁnd the transformation of minimum graph edit distance is much
more complicated than that of string edit distance. Graph matching can also be
viewed as a correspondence, assignment, or consistent labeling problem. Consider a
graph with m nodes V1 = {u1, . . . , um}, to match it with another graph with n nodes
V2 = {v1, . . . , vn}, the nodes in V2 are actually assigned labels from V1 ∪{φ} (φ is
a null label) under the constraint that when two nodes vi2 and vj2 are assigned labels
ui1 and uj1, the relationships (edges) (vi2, vj2) and (ui1, uj1) should be consistent
(compatible).
The 1:1 correspondence of nodes between two graphs is generally called graph
isomorphism or subgraph isomorphism. The former refers to the 1:1 correspondence
between all nodes of two graphs, whereas the latter refers to the correspondence
between one graph and a subgraph of another. Subgraph isomorphism is also called
as graph monomorphism or homomorphism. It can be generalized to the case of
error-correcting or inexact isomorphism, in which approximate node/edge matching
and deletion/insertion operations are allowed. Most of the practical shape or vision
matching problems can be formulated as inexact subgraph isomorphism.
For character recognition using ARG matching, the most important task is to ﬁnd
the correspondence between the nodes of two graphs. Based on node correspondence,
the distance or similarity between two graphs is computed from the between-node and
between-edge distances and is then used for recognition. For example, the distance
between two strokes can be computed from the geometric distance between two center
points, the difference in lengths and orientations, and so on. The relationship between
a pair of strokes may have the attributes of difference of orientations, the distance
between two center points, and the orientation of the line connecting the center points
of two strokes, and accordingly, the between-edge distance can be computed from
the difference of these attributes.
The correspondence between two sets of graph nodes can be found by two methods:
heuristic tree search and relaxation labeling. In the following, we describe the basic

176
PATTERN CLASSIFICATION METHODS
algorithms of the two methods. It is noteworthy that graph matching is still a research
issue in pattern recognition and computer vision ﬁelds. The readers should consult
the literature for the recent advances.
4.5.2.1
Tree Search
Graph matching is a combinatorial optimization problem,
and it is natural to use a tree to represent the search space of optimization [91]. The
efﬁciency of search depends on the order of node7 expansion in the tree: Breadth-ﬁrst
search and depth-ﬁrst search are brute-force (exhaustive), whereas heuristic search
expands those nodes that are likely to approach the solution.
For inexact graph matching, usually an objective function of matching distance
is formulated and minimized using combinatorial optimization algorithms, including
tree search. Tsai and Fu [87] and Wong et al. [96] proposed to solve this problem
using an efﬁcient heuristic search algorithm, the A* algorithm [64]. In the following,
we outline the method of Wong et al.
Consider two ARGs G1 = (V1, E1, A1) and G2 = (V2, E2, A2), with m nodes
and n nodes, respectively. G2 is to be transformed for optimally matching with G1.
Accordingly, the nodes of G2 are assigned labels from V1 ∪{φ}, or the nodes of G1 are
assigned labels from V2 ∪{φ}. Assume that the nodes of G1, V1 = {u1, u2, . . . , um},
correspond to the nodes in G2, {vl1, vl2, . . . , vlm} ∈V2 ∪{φ}, the matching distance
between these two graphs can be deﬁned as
D(G1, G2) =
m

i=1
d(ui, vli) +
m

i=1
i−1

j=1
d(ui, uj, vli, vlj),
(4.135)
where d(ui, vli) and d(ui, uj, vli, vlj) denote node-matching cost and edge-matching
cost, respectively; and if vli = φ or vlj = φ, they are the costs of node/edge deletion.
The cost functions can be deﬁned incorporating the attributes of nodes and edges and
the compatibility between edges.
To minimize the distance of Eq. (4.135), the combinations of labels from V2 ∪{φ}
are represented in the search tree. The root node of the tree denotes an empty match,
that is, no node of G1 is matched with node of G2. Then, each node in the depth-k
layer (uk, vlk) indicates that the kth node of G1, uk, is matched with a node vlk of
G2. When a node (uk, vlk) is expanded, the next node of G1, uk+1, is matched with
a node from (V2 −{vl1, . . . , vlk}) ∪{φ}. The accumulated cost from the root node to
(uk, vlk) (a partial match in the search tree) is computed by
g(uk, vlk) =
k

i=1
d(ui, vli) +
k

i=1
i−1

j=1
d(ui, uj, vli, vlj) =
k

i=1
d′(ui, vli),
(4.136)
7Note the context of “node”: whether it is the node of a graph or the node of the search space (tree). A
node of the search tree represents a state of search.

STRUCTURAL PATTERN RECOGNITION
177
where
d′(ui, vli) = d(ui, vli) +
i−1

j=1
d(ui, uj, vli, vlj).
(4.137)
The order of node expansion follows branch-and-bound (BAB); that is, every time
the node of minimum accumulated cost is expanded to generate successors. BAB
guarantees that the globally optimal solution, that is, the complete match of minimum
distance, is found. However, the search efﬁciency of BAB is low because no heuristics
about the remaining path is utilized.
In heuristic search, the node selected to be expanded is the one of minimum heuris-
tic function
f(N) = g(N) + h(N),
(4.138)
where N is the current node of the search tree, N = (uk, vlk), g(N) = g(uk, vlk), and
h(N) is an estimate of the remaining cost of the path from N to the goal (complete
match). If h(N) is a lower bound of the real remaining cost, BAB search guarantees
ﬁnding the globally optimal solution, and the tighter the bound is, the more efﬁcient
(fewer nodes are expanded before the goal is reached) the search is. Hence, to ﬁnd
a tight lower bound estimate of the remaining cost is the key to achieve optimal and
efﬁcient search.
Wong et al. compute the estimate of the remaining cost as follows. At the tree
node N = (uk, vlk) denotes the sets of unmatched nodes of G1 and G2 as M1 and M2,
respectively. We have M1 = {uk+1, . . . , um}, M2 = (V2 −{vl1, . . . , vlk}) ∪{φ}. For
a node in M1 matched with a label from M2, deﬁne a cost function
d′′(ui, vli) = d(ui, vli) +
k

j=1
d(ui, uj, vli, vlj).
(4.139)
Finding for each node in M1 an optimal label from M2, we obtain the minimum cost
a(N) =
m

i=k+1
min
vli∈M2 d′′(ui, vli).
(4.140)
Similarly, for each edge (pair of nodes) in M1, ﬁnd an optimal pair from M2. The
minimum cost is
b(N) =
m

i=k+1
m

j=i+1
min
vli,vlj ∈M2 d(ui, uj, vli, vlj).
(4.141)
It can be proved that h(N) = a(N) + b(N) is an estimate of the lower bound of the real
remaining cost. Its use in heuristic search was shown to ﬁnd optimal graph matching
efﬁciently [96].

178
PATTERN CLASSIFICATION METHODS
4.5.2.2
Relaxation Labeling
Relaxation labeling techniques utilize contex-
tual information to reduce the uncertainty of labels in assignment or labeling prob-
lems. They have been widely applied to image analysis since Rosenfeld et al. [74].
Yamamoto and Rosenfeld ﬁrst applied relaxation labeling to Chinese character
recognition and obtained impressive performance [100].
Consider a set of objects or elements (can be viewed as the nodes of a graph)
B = {b1, . . . , bn} and a set of labels (can be viewed as the nodes of another graph)
 = {1, . . . , m}. The aim is to assign a distinct label to each object such that under
a relational structure, each pair of objects is compatible with the labels assigned to
them. In character recognition, the objects can be the strokes of the input pattern,
and the labels are the strokes of a template structure. Relaxation labeling techniques
solve the labels by iteratively updating the assignment probabilities until each object
is unambiguously assigned a unique label.
Denote the probability (certainty) of assigning label λ to object bi as piλ (0 ≤piλ ≤
1). The label probabilities of an object can be represented in a vector pi that satisﬁes
|pi| = m
λ=1 piλ = 1. The initial probability, p(0)
iλ , is set at random or computed from
the matching similarity of attributes between the object bi and its label λ. For updating
the probabilities, compatibility functions, between pairs of objects and pairs of labels,
are deﬁned: rij(λ, μ) = r(bi, bj, λ, μ). In character recognition, the compatibility
functions can be computed according to the differences between relative geometric
features of stroke pairs.
For updating the assignment probabilities at time t, a support function for each
label on an object is computed:
s(t)
iλ =

bj∈N(bi)
m

μ=1
rij(λ, μ)p(t)
jμ,
(4.142)
where N(bi) denotes a neighborhood of object bi. The support functions are then used
to update the probabilities by
p(t+1)
iλ
=
p(t)
iλ s(t)
iλ
m
μ=1 p(t)
iμs(t)
iμ
,
(4.143)
wherein the probabilities of an object have been normalized. After a number of
iterations, the probabilities are expected to converge to the state that for each ob-
ject, only one label probability remains large whereas the other probabilities tend to
vanish. As a result, the object is assigned the label with the maximum probability.
In a modiﬁed updating rule called radial projection [66] both the probabilities and
the support values of an object are viewed as vectors in m-dimensional space. The
normalized vector of support, s(t)
i /|s(t)
i |, is used to update the probability vector by
p(t+1)
i
= p(t)
i + α(t)
i

s(t)
i
|s(t)
i |
−p(t)
i

,
(4.144)

COMBINING MULTIPLE CLASSIFIERS
179
where
α(t)
i
=
|s(t)
i |
1 + |s(t)
i |
.
(4.145)
The updated probability vector satisﬁes |p(t+1)
i
| = 1 automatically.
The relaxation labeling algorithm has polynomial complexity. The complexity
of updating assignment probabilities is O(m · n), and the complexity of computing
support functions is O((mn)2). The convergence of relaxation, however, depends
on the computing of initial probabilities and the compatibility functions. Designing
properinitialprobabilitiesandcompatibilityfunctionsreliesonthedomainknowledge
of the application problem.
Graph matching and relaxation labeling methods have been widely applied to
the structural matching of character patterns. An example of graph-matching-based
ofﬂine character recognition can be found in [54]. For online character recognition,
many structural matching methods are reviewed in [53].
4.6
COMBINING MULTIPLE CLASSIFIERS
The Bayes decision classiﬁer is optimal but the conditions to achieve the optimality
are rather unrealistic: sufﬁciently large (nearly inﬁnite) training data set, ﬂexible
enough density model, and powerful learning tool for parameter estimation from
data. Practical classiﬁers instead learn constrained density models or discriminant
functions from ﬁnite data set and make approximate decisions. Different classiﬁers
approximate the Bayes decision in different ways and show varying performance:
varying classiﬁcation accuracy and speed, and different errors on concrete patterns.
It is hence natural to combine the strengths of different classiﬁers to achieve higher
performance. This is usually accomplished by combining the decisions (outputs) of
multiple classiﬁers.
Classiﬁcation with multiple classiﬁers (also called an ensemble of classiﬁers) has
been an active research area since early 1990s. Many combination methods have been
proposed, and the applications to practical problems have proven the advantage of
ensemble over individual classiﬁers [84, 98, 31, 42, 83]. A recent survey [70] catego-
rizes the methods into parallel (horizontal) and sequential (vertical, cascaded) ones.
Parallel combination is more often adopted for improving the classiﬁcation accuracy,
whereas sequential combination is mainly used for accelerating the classiﬁcation of
large category set (coarse classiﬁcation in Chinese character recognition [53]).
In this section, we mainly address parallel combination of classiﬁers, but when
discussing ensemble generation, there will be a method that sequentially gener-
ates individual classiﬁers (boosting). All the issues concerned have been studied
intensively and we summarize only some selected methods. A comprehensive treat-
ment of combining classiﬁers has been given in the textbook of Kuncheva [47].

180
PATTERN CLASSIFICATION METHODS
4.6.1
Problem Formulation
Assume the classiﬁcation of a pattern X into M classes {ω1, . . . , ωM} using K
individual classiﬁers {E1, . . . , EK}, each using a feature vector xk, k = 1, . . . , K.
Each feature vector can be viewed as a subspace of the entire space occupied by the
union of all features (united feature vector x). On an input pattern, each classiﬁer Ek
outputs decision scores to all classes: dki(xk), j = 1, . . . , M. The classiﬁer outputs
can be categorized into three levels: abstract level (unique class), rank level (rank
order of classes), and measurement level (conﬁdence scores of classes) [98].
Wecanrepresenttheoutputsofaclassiﬁeratdifferentlevelsuniformlyinadecision
vector dk = (dk1, . . . , dkM)T . At the abstract level, dk has only one nonzero element
corresponding to the decided class. The rank order can be converted to class scores
such that dki is the number of classes ranked below ωi. At the measurement level, dki
is the discriminant value (similarity or distance) or probability-like conﬁdence of ωi.
The measurement-level outputs can be easily reduced to rank level and abstract level.
The combination of K classiﬁers assigns fused scores to M classes, which are the
functions of the scores of individual classiﬁers:
yi(X) = Fi
⎛
⎜
⎜
⎝
d11,
· · · ,
d1M,
...
...
...
dK1,
· · · ,
dKM
⎞
⎟
⎟
⎠,
i = 1, . . . , M,
(4.146)
In practice, the fused score of one class often considers the individual classiﬁer outputs
of the same class only:
yi(X) = Fi(d1i, . . . , dKi),
i = 1, . . . , M.
(4.147)
The formulas of (4.146) and (4.147) are referred to as class-indifferent combination
and class-conscious combination, respectively [47].
The class-indifferent combiner can be viewed as a new classiﬁer (called meta-
classiﬁer) in a K × M-dimensional feature space. Class-conscious combination rules
canbedividedintoﬁxedonesandtrainedones[18].Asameta-classiﬁerdoes,atrained
rule has some tunable parameters that are estimated on a training data set. Desirably,
the data set for training the combiner consists of classiﬁer outputs on samples that are
not used in training the individual classiﬁers. If an extra sample set disjoint from the
training set is available, the classiﬁer outputs on the extra set are good for combiner
training. The individual classiﬁers can also be trained with different sample sets. If
only one sample set is available, it can be partitioned for training both individual
classiﬁers and the combiner by a strategy called stacking [95, 86], which we outline
below.
By stacking, the sample set X is partitioned into two subsets, by either rotation or
holdout: one for temporarily training individual classiﬁers and the other (validation
set) for collecting classiﬁer outputs. The output data on validation samples are used
to train the combiner. Yet for classifying new data, the individual classiﬁers are still

COMBINING MULTIPLE CLASSIFIERS
181
trained with the whole sample set. Speciﬁcally, let us partition X into J subsets
Xj, j = 1, . . . , J. We rotationally hold out Xj and use X −Xj to train classiﬁers
E(j)
k , k = 1, . . . , K, whose outputs on the samples in Xj form a data subset D(j).
The union D(1) ∪· · · ∪D(J) = D is used for training the combiner. The classiﬁers
E(j)
k , j = 1, . . . , J and k = 1, . . . , K, are used only for collecting output data. For
classiﬁcation, the classiﬁers Ek, k = 1, . . . , K, are still trained with X.
Holdout partition is different from rotation in that for collecting output data, the
individual classiﬁers are trained only once with a subset X1 and their outputs on
X2 = X −X1 form the data for combiner training (usually, X1 contains more samples
than X2). The individual classiﬁers are then trained with X for classiﬁcation. It is also
a practice that the classiﬁers trained with X1 are directly used for classiﬁcation.
4.6.2
Combining Discrete Outputs
Some particular methods are used for combining discrete (abstract-level and rank-
level) outputs of classiﬁers. If the discrete outputs of a classiﬁer on a pattern is
represented as a vector dk = (dk1, . . . , dkM)T , they can also be combined in the same
way as continuous scores.
4.6.2.1
Majority Vote
For combining abstract-level decisions, the most
frequently considered rules are the majority vote and weighted majority vote. If the in-
put pattern is classiﬁed to ωj by classiﬁer Ek, the class ωj is assigned a vote dkj = 1,
and the other classes are assigned vote dki = 0 (i ̸= j). The combined votes are
computed by
yi =
K

k=1
dki,
i = 1, . . . , M.
(4.148)
By majority vote, the input pattern is classiﬁed to the class of maxi=1,...,M yi
if maxi=1,...,M yi > K/2, and rejected otherwise. According to this rule, the
combination gives correct classiﬁcation if at least ⌊K/2⌋+ 1 classiﬁers are correct.
The rule that always classiﬁes to the class of maxi=1,...,M yi without rejection is also
called plurality.
It is shown that by majority vote, combining K independent classiﬁers of the same
accuracy (p > 0.5) can always improve the accuracy [47]. The probability that the
ensemble makes correct decision (at least ⌊K/2⌋+ 1 classiﬁers are correct) is
Pens =
K

k=⌊K/2⌋+1
K
k

pk(1 −p)K−k.
(4.149)
If p > 0.5, Pens is monotonically increasing with K, and when K →∞, Pens →1.
In practice, however, the individual classiﬁers are more or less dependent, and the
accuracy of ensemble is dependent on the dependence. A more detailed analysis of
the performance of majority vote can be found in [50].

182
PATTERN CLASSIFICATION METHODS
In the case of imbalanced or dependent classiﬁers, better combination performance
can be achieved by weighted majority vote (or weighted plurality):
yi =
K

k=1
wkdki,
i = 1, . . . , M,
(4.150)
where wk is the weight for classiﬁer Ek. The constraints 0 ≤wk ≤1 and K
k=1 wk =
1 are recommended but not compulsory. It is shown that for combining K independent
classiﬁers with accuracies p1, . . . , pK, the ensemble accuracy of weighted majority
vote is maximized by assigning weights [47]
wk ∝log
pk
1 −pk
.
(4.151)
For combining dependent classiﬁers, the weights can be optimized by training with
a validation data set. This is handled in the same way as combining continuous
outputs. Besides, Lam and Suen have proposed to optimize the weights using a genetic
algorithm [49].
4.6.2.2
Bayesian Combination
Consider the decided class of classiﬁer Ek as
a discrete variable ek ∈{ω1, . . . , ωM}, the combined decision of K classiﬁers can be
made by the Bayes decision rule on K variables e1, . . . , eK. To do this, the a posteriori
probabilities of M classes on an input pattern are computed by the Bayes formula:
P(ωi|e1, . . . , eK) = P(e1, . . . , eK|ωi)P(ωi)
P(e1, . . . , eK)
,
i = 1, . . . , M.
(4.152)
The input pattern is classiﬁed to the class of maximum a posteriori probability,
equivalently, the class of maximum likelihood:
max
i
P(e1, . . . , eK|ωi)P(ωi).
(4.153)
P(ωi) and P(e1, . . . , eK|ωi), the a priori probability and the joint conditional
probability, can be estimated from a validation sample set. If the validation set con-
tains N samples in total, P(ωi) is approximated by Ni/N (Ni is the number of samples
from class ωi). Denote nk
ij as the number of samples from ωi that are classiﬁed to ωj
by classiﬁer Ek, then nk
ij/Ni approximates the probability P(ek = ωj|ωi). A matrix
composed of the elements P(ek = ωj|ωi), i, j = 1, . . . , M, is called the confusion
matrix of classiﬁer Ek. Assume that the K decisions are independent, the joint con-
ditional probability is reduced to
P(e1 = ωj1, . . . , eK = ωjK|ωi) =
K
$
k=1
P(ek = ωjk|ωi).
(4.154)

COMBINING MULTIPLE CLASSIFIERS
183
Inserting Eq. (4.154) into (4.153), this Naive Bayes formulation of combination can
perform fairly well, even in combining dependent classiﬁers.
Bayesian combination by estimating the joint probability P(e1, . . . , eK|ωi) is also
called behavior knowledge space (BKS) method [33], where the joint confusion of
K classiﬁers is represented by a lookup table with MK+1 cells. A cell nj1,...,jK(i)
stores the number of samples from class ωi that are classiﬁed to ωjk by classiﬁer Ek,
k = 1, . . . , K. The conditional probability is then estimated by
P(e1 = ωj1, . . . , eK = ωjK|ωi) = nj1,...,jK(i)
Ni
.
(4.155)
The BKS method is expected to yield higher combination performance than
the Naive Bayes if the joint probability P(e1, . . . , eK|ωi) is precisely estimated. In
practice, however, the number of samples for estimating probabilities is limited com-
pared to the number of cells MK+1, and so the probabilities cannot be estimated with
sufﬁcient precision. A way to alleviate this problem is to approximate high-order
probability (K + 1th order in combining K classiﬁers) with the product of lower or-
der probabilities [38]. The storage complexity of this method is intermediate between
the BKS method and the Na¨ıve Bayes.
4.6.2.3
Combination at Rank Level
A simple rule for combining rank-level
outputs is the Borda count [31]. Denote dki as the number of classes ranked below
ωi by classiﬁer Ek, the total count of each class given by K classiﬁers is computed
in the same way as Eq. (4.148). The input pattern is classiﬁed to the class of max-
imum count. For a two-class problem, the Borda count is equivalent to the sum of
votes.
The Borda counts of multiple classiﬁers can also be combined by weighted
sum, as in Eq. (4.150). The weights are estimated by optimizing an objective on
a validation data set, particularly by logistic regression [31]. The same way is
also used for combining abstract-level and measurement-level outputs. We will
go into the details of weight estimation in the context of combining continuous
outputs.
In weighted combination, the weights of classiﬁers are static once trained on
validation data. For better performance, it is desirable to use dynamic weights be-
cause the strength of each individual classiﬁer varies in different regions of a feature
space (the features in this space can be those used by the individual classiﬁers, the
outputs of classiﬁers, or any measures related to the behaviors of classiﬁers). As this
philosophy also applies to combination at the abstract level and the measurement
level, we will describe the details later in Section 4.6.4.
4.6.3
Combining Continuous Outputs
For combining continuous outputs of classiﬁers, we will focus on class-conscious
combination rules, whereas class-indifferent combination can be accomplished us-
ing a generic classiﬁer with parameters estimated on a validation data set. The

184
PATTERN CLASSIFICATION METHODS
class-conscious combination rules, especially weighted combination, perform suf-
ﬁciently well in practice.
The transformation of classiﬁer outputs is important because different classiﬁers
often output scores with much different scales and physical meanings. The outputs
should be at least normalized to the same scale, and, preferably, transformed to
probability-like conﬁdence scores (conﬁdence transformation). On conﬁdence scores,
it is possible to achieve high performance using simple combination rules.
4.6.3.1
Conﬁdence
Transformation
Classiﬁer outputs are commonly
transformed to a posteriori probabilities using soft-max or sigmoid functions. We
describe some details of sigmoid-based conﬁdence transformation. We have shown
in Section 4.3 that the a posteriori probability of a two-class Gaussian classiﬁer is a
sigmoid function. The outputs of most classiﬁers (for neural networks, the weighted
outputs without sigmoid activation) can be viewed as linear or generalized linear dis-
criminant functions and can be similarly transformed to a posteriori probabilities by
sigmoid function.
The outputs of a generic classiﬁer8 can be transformed to conﬁdence values via a
scaling function [52]:
fi(d) =
M

j=1
βijdj + βi0,
i = 1, . . . , M.
(4.156)
Sigmoid scores are obtained by
zs
i(d) =
1
1 + e−fi(d) .
(4.157)
The sigmoid function, like that in Eqs. (4.49) and (4.130), primarily functions as a
two-class a posteriori probability. In multiclass classiﬁcation, zs
i(d) is actually the
probability of ωi from two classes ωi and ωi. The two-class sigmoid probabilities
can be combined to multiclass probabilities according to the Dempster–Shafer theory
of evidence [81, 3]. In the framework of discernment, we have 2M focal elements
(singletons and negations) {ω1, ω1, . . . , ωM, ωM} with basic probability assignments
(BPAs) mi(ωi) = zs
i, mi(ωi) = 1 −zs
i, then the combined evidence of ωi is
zc
i = m(ωi) = A · mi(ωi)
M
$
j=1,j̸=i
mj(ωj) = A · zs
i
M
$
j=1,j̸=i
(1 −zs
j),
(4.158)
8We drop off the classiﬁer index k when considering only one classiﬁer in conﬁdence transformation.

COMBINING MULTIPLE CLASSIFIERS
185
where
A−1 =
M

i=1
zs
i
M
$
j=1,j̸=i
(1 −zs
j) +
M
$
i=1
(1 −zs
i).
The multiclass a posteriori probabilities P(ωi|d) = zc
i , i = 1, . . . , M, satisfy
M
i=1 zc
i ≤1. The complement to one is the probability of input pattern being out
of the M deﬁned classes.
The scaling function is often reduced to the one taking only one input variable:
fi(d) = βi1di + βi0,
i = 1, . . . , M.
(4.159)
The weight parameters can even be shared by M classes: βi1 = β1, βi0 = β0, i =
1, . . . , M. The parameters can be estimated using two techniques: Bayesian formula
assuming one-dimensional Gaussian densities on two classes (ωi and ωi) [80] and
logistic regression [31, 67]. We introduce the two techniques in the following.
Similar to Eq. (4.49), assuming one-dimensional Gaussian densities for two classes
(ωi and ωi) with means μ+
i and μ−
i and equal variance σ2
i , the a posteriori probability
of ωi versus ωi is
zs
i(d) =
P(ωi)p(di|ωi)
P(ωi)p(di|ωi) + P(ωi)p(di|ωi)
=
P(ωi) exp
"
−(di−μ+
i )2
2σ2
i
#
P(ωi) exp
"
−(di−μ+
i )2
2σ2
i
#
+ P(ωi) exp
"
−(di−μ−
i )2
2σ2
i
#
=
1
1 + e−α[di−(β+γ/α)] ,
(4.160)
where α = μ+
i −μ−
i
σ2
i
, β = μ+
i +μ−
i
2
, and γ = log P(ωi)
P(ωi). The scaling function is extracted
as
fi(d) = α

di −

β + γ
α

.
(4.161)
The a priori probabilities and the parameters {μ+
i , μ−
i , σ2
i } are estimated on a valida-
tion data set by maximum likelihood (ML).
In logistic regression, the parameters of scaling functions fi(d), i = 1, . . . , M, are
optimized by gradient descent to maximize the likelihood on a validation data set
{(dn, cn)|n = 1, . . . , N} (cn is the class label of sample dn):
max L =
N
$
n=1
M
$
i=1
[zs
i(dn)]tn
i [1 −zs
i(dn)]1−tn
i ,
(4.162)

186
PATTERN CLASSIFICATION METHODS
where the target probability is tn
i = 1 for i = cn and tn
i = 0 otherwise. Maximizing the
likelihood is equivalent to minimizing the negative log-likelihood, which is actually
the cross-entropy (CE) of Eq. (4.64). The parameters of scaling functions can also be
optimized by minimizing the sum of the squared error of Eq. (4.52). Both the squared
error and the cross-entropy can be regularized by weight decay, as that in Eq. (4.75).
On scaling parameter estimation by either one-dimensional Gaussian density mod-
eling or logistic regression, the sigmoid scores are computed by Eq. (4.157) and are
combined to multiclass probabilities by Eq. (4.158).
In the following description of combination rules for continuous outputs, dki is
replaced by zki, k = 1, . . . , K, i = 1, . . . , M, which represent either transformed
conﬁdence scores or raw classiﬁer outputs.
4.6.3.2
Fixed Rules
The conﬁdence scores of a class given by K classiﬁers, zki,
k = 1, . . . , K, can be combined using ﬁxed rules that have no tunable parameters to
be estimated on validation data. The frequently considered rules are
• Sum rule:
yi =
K

k=1
zki.
Regarding the decision of classiﬁcation, the sum rule is equivalent to the so-
called simple average: yi = 1
K
K
k=1 zki.
• Product rule:
yi =
K
$
k=1
zki.
• Max rule:
yi =
K
max
k=1 zki.
• Min rule:
yi =
K
min
k=1 zki.
• Median rule: Reorder the K scores of class ωi: z(1)
i
≥z(2)
i
≥· · · ≥z(K)
i
, then if
K is odd,
yi = z
( K+1
2 )
i
,

COMBINING MULTIPLE CLASSIFIERS
187
and otherwise,
yi = 1
2

z
( K
2 )
i
+ z
( K
2 −1)
i

.
The input pattern is classiﬁed to the class of maxi yi.
If zki is an estimate of a posteriori probability zki = P(ωi|xk), it is shown that
the decision of input pattern to the maximum product of a posteriori probabilities
is equivalent to the decision to the maximum joint a posteriori probability on K
conditionally independent feature vectors [42]. Further, assume that the a posteriori
probability does not deviate from the a priori probability signiﬁcantly, the product rule
is approximated to the sum rule. In practice, the sum rule performs fairly well even
in combining dependent classiﬁers. It often outperforms the product rule because it
is shown to be less sensitive to the estimation error of a posteriori probabilities [42].
4.6.3.3
Weighted Combination
In weighted combination, the fused score of
a class is a weighted average of K individual scores:
yi =
K

k=1
wkizki,
i = 1, . . . , M.
(4.163)
Usually, the weights are required to be non-negative, and, more strictly, sum to one:
K
k=1 wki = 1. Class-independent weights can be used instead of class-dependent
ones: wki = wk.
To show that weighted average (WA) is superior to simple average (SA), let us
review some theoretical implications of Fumera and Roli [24]. Assume that the
classiﬁer outputs or transformed scores are the estimates of a posteriori probabili-
ties with random prediction error:
zki(x) = P(ωi|x) + ϵki(x),
(4.164)
where ϵki(x) is a random variable with mean μki and variance σ2
ki. ϵki(x) and
ϵlj(x) (i ̸= j) are assumed to be uncorrelated. Using class-independent weights, the
weighted average of a class is
yWA
i
=
K

k=1
wkzki(x) = P(ωi|x) + ϵWA
ki (x),
where ϵWA
ki (x) = K
k=1 wkϵki(x).
Compared to the Bayes error rate, the expected added error of WA, EWA
add, is
shown to depend on the a posteriori probability at the decision boundary, μki, σ2
ki,
and the correlation coefﬁcient ρi(k, l) between ϵki(x) and ϵli(x). Smaller correla-
tion coefﬁcients lead to smaller added error of ensemble. With optimal weights that

188
PATTERN CLASSIFICATION METHODS
minimize EWA
add, the overall error rate of WA is not higher than the best individual
classiﬁer, whereas the SA is guaranteed only to be better than the worst individual
classiﬁer.
Assume unbiased and uncorrelated errors μki = 0, ρi(k, l) = 0, ∀k, l, i, the optimal
weights can be solved analytically:
wk =
1
Eadd(k)
 K

l=1
1
Eadd(l)
−1
,
(4.165)
where Eadd(k) is the expected added error of individual classiﬁer k. It is easy to
see that on K classiﬁers of equal added error, the optimal weights are equal. Under
optimal weights, the difference of added error between WA and SA ensembles, 	E =
ESA
add −	EWA
add ≥0 and increases when the added errors of individual classiﬁers are
imbalanced.
It is also shown in [24] that assuming class-independent variances and correlations
for ϵki(x) , 	E is greater than zero even when the added errors of individual classiﬁers
are equal, but the correlation coefﬁcients are imbalanced.
In practice, to compute the optimal weights is not trivial. Even the formula (4.165)
is not practical because the Bayes error rate is unknown. Empirically, the weights are
estimated on a validation data set to optimize an objective. In combining neural net-
works for function approximation, Hashem solves the optimal weights by regression
under the mean squared error (MSE) criterion [27]. This can be applied to classiﬁ-
cation as well by adding sigmoid function to the combined scores. Ueda optimizes
the weights under the minimum classiﬁcation error (MCE) criterion [90], where the
combined scores need not be transformed by sigmoid.
Solving optimal weights to minimize the CE criterion (Eq. (4.64)) is generally
called as logistic regression, where the combined class score is a sigmoid function of
weighted average with a bias:
yi =
1
1 + e−(K
k=1 wkizki+w0i) .
(4.166)
In this formulation, the weights can also be estimated under the MSE criterion. Either
the MCE and CE or MSE criterion can be regularized by weight decay and can be
optimized to update the weights by gradient descent.
The MCE method for weight estimation deserves some details as it had shown
superior performance in [90]. Given N validation samples (zn, cn), n = 1, . . . , N,
where zn is the vector of classiﬁer outputs or conﬁdence values, and cn is the class
label. On each sample, the combined class scores are computed as in Eq. (4.163). The
empirical loss on the validation set is similar to that for learning vector quantization

COMBINING MULTIPLE CLASSIFIERS
189
(LVQ) as in Eq. (4.107), on which we add a regularization term:
L1 = 1
N
N

n=1
lcn(zn) + β
2
K

k=1
M

i=1
w2
ki
= 1
N
N

n=1

lcn(zn) + β
2N
K

k=1
M

i=1
w2
ki

= 1
N
N

n=1
E(zn),
(4.167)
where lc(z) = lc(μc) =
1
1+e−μc , μc = yr −yc, and yr = maxi̸=c yi (c is the class label
of pattern z).
By stochastic gradient descent, the weights are iteratively updated on each input
pattern.Speciﬁcally,theweightsaremovedalongthenegativedirectionofthegradient
of E(zn): 	wki = −η ∂E(zn)
∂wki , where η is the learning rate, and
∂E(z)
∂wki
= lc(1 −lc)∂(yr −yc)
∂wki
+ β
N wki
=
⎧
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎩
−lc(1 −lc)zki + β
N wki,
if i = c,
lc(1 −lc)zki + β
N wki,
if i = r,
β
N wki,
otherwise.
In the case of class-independent weights, yi = K
k=1 wkzki, i = 1, . . . , M, and
∂E(z)
∂wk
= lc(1 −lc)(zkr −zkc) + β
N wk,
k = 1, . . . , K.
If the weights are constrained to be non-negative, each weight is checked on updating:
if it is negative, it is modiﬁed to 0.
As to the effects of non-negativity constraint of weights and the bias, the
experiments of Ting and Witten, using logistic regression, show that non-negative
weights and free weights yield comparable performance [86]. On the contrary, the ex-
periments of Ueda show that class-dependent weights outperform class-independent
ones [90]. In practice, it is recommended to try the choices of both with and without
non-negativity constraint, with and without bias, and both class-dependent and
class-independent weights.

190
PATTERN CLASSIFICATION METHODS
4.6.4
Dynamic Classiﬁer Selection
Assume that the imbalance of individual classiﬁer performances and the correlation
betweenpredictionerrorsremainconstantinthefeaturespaceoccupiedbytheunionof
features of the ensemble or the decision outputs of individual classiﬁers, the weighted
combination scheme, using data-independent weights, is optimal in all regions of
the feature space. This assumption, however, does not hold true in practice. The
strengths of individual classiﬁers are generally variable in the feature space. It is hence
desirable to use different (data-dependent) weights in different regions and select
active individual classiﬁer or combiner dynamically. This strategy, called dynamic
classiﬁer selection or dynamic combiner selection, has shown superior combination
performance over constant combination in previous works [31, 97, 93].
In a dynamic classiﬁer selection scheme proposed by Woods et al. [97], the local
accuracies of individual classiﬁers are estimated in a region of the pattern feature space
surrounding the unknown test sample x. The local region is deﬁned by the k-nearest
neighbors of training samples to x, and the local accuracies of individual classiﬁers
are the correct rates on the nearest neighbors. The local accuracy is dichotomized
into overall local accuracy and local class accuracy. The local class accuracy of a
classiﬁer Ei is estimated as the fraction of correctly classiﬁed training samples in the
neighborhood whose assigned class labels by Ei are the same as x. The decision of
the individual classiﬁer of the highest local accuracy is assigned to the test sample.
Ties of multiple equally accurate classiﬁers are broken by majority vote.
For dynamic combiner selection, the feature space of pattern features or classiﬁer
outputs are more often partitioned into local regions by clustering the training or
validation samples. Each region is represented by a cluster center, and in classiﬁcation,
the region of the test sample is decided by the nearest distance to the cluster center.
In the decided region, the decision of the most accurate individual classiﬁer is taken,
or region-dependent weighted combination or meta-classiﬁer is activated. Region
partitioning by clustering is computationally much less expensive in operation than
the k-nearest neighbor partitioning, as only the cluster centers are to be stored.
In combining rank-level decisions, Ho et al. proposed a dynamic combiner selec-
tion approach [31], where the training samples are partitioned into subsets (regions)
according to the agreement on top choices of individual classiﬁers. Speciﬁcally, for
K classiﬁers, there are
K
K

+
 K
K−1

+ · · · +
K
2

+
K
0

states (and, accordingly, re-
gions) of agreement on the top choices from “all agree” to “all disagree.” The test
sample is ﬁrst assigned to a region according to the agreement of individual decisions
on it, and a locally optimal individual classiﬁer is selected or the region-dependent
logistic regression model is activated.
4.6.5
Ensemble Generation
The performance of multiple classiﬁer systems not only depend on the combination
scheme, but also rely on the complementariness (also referred to as independence or
diversity) of the individual classiﬁers. Compared to the best individual classiﬁer, the
combination of independent classiﬁers can give maximum performance gain, but it is

COMBINING MULTIPLE CLASSIFIERS
191
hard to make the individual classiﬁers totally independent. In practice, complemen-
tary classiﬁers can be generated by training with different sample sets, using different
features, classiﬁer structures, learning algorithms, and so on, for the individual classi-
ﬁers. In character recognition, combining classiﬁers based on different pre-processing
techniques is also effective.
Some approaches have been proposed to generate multiple classiﬁers system-
atically with the aim of optimizing the ensemble performance. We will describe
some representatives of these: overproduce-and-select, bagging, random subspace,
and boosting. The ﬁrst approach selects a subset of individual classiﬁers from a large
set of candidate classiﬁers. Bagging and random subspace methods generate classi-
ﬁers in parallel, by randomly selecting training data and features, respectively. By
boosting, the individual classiﬁers are generated sequentially, with each successive
classiﬁer designed to optimally complement the previous ones. Bagging, random
subspace, and boosting methods are usually used to generate classiﬁers of the same
structure on the same pattern features, wherein the underlying classiﬁer structure is
called base classiﬁer or base learner.
4.6.5.1
Overproduce and Select
For character recognition, a very large num-
ber of classiﬁers can be generated by varying the techniques of preprocessing, feature
extraction, classiﬁer structure and learning algorithm, and so on. To combine a large
number (say, 100) of classiﬁers is neither efﬁcient in computation nor effective in per-
formance. As the candidate classiﬁer set may contain many correlated and redundant
classiﬁers, it is possible to achieve a higher combination performance by selecting a
subset from overly produced classiﬁers.
To select classiﬁers that are independent, one method is to partition the candidate
classiﬁers into clusters such that the classiﬁers within a cluster are more dependent
than those in different clusters. By deﬁning a dependence measure (like a similarity)
between two classiﬁers, the classiﬁers can be partitioned using a hierarchical agglom-
erative clustering algorithm [35]. On clustering, the ensemble is formed by drawing
one classiﬁer from each cluster [25]. The dependence measure is the rate of double
fault, also called compound error rate, which is deﬁned as the percentage of samples
that are misclassiﬁed by both the two classiﬁers. A low compound error rate indicates
that two classiﬁers complement well. The classiﬁers drawn from different clusters
thus form an ensemble of good complementariness.
The problem of classiﬁer subset selection is very similar to that of feature
selection. On deﬁning an evaluation criterion (called diversity [73]) of classi-
ﬁer subsets, the search methods for feature selection in Chapter 3, like forward
search, backward search, genetic algorithm, and so on, can be used to ﬁnd the
optimal or near-optimal subset of classiﬁers. The diversity of a classiﬁer sub-
set is measured on a validation data set. The accuracy of classiﬁer combination
(e.g., by majority vote or conﬁdence average) is immediately a diversity mea-
sure, but the selected classiﬁers maximizing the ensemble accuracy on validation
data do not necessarily generalize well to unseen test data. A well-performing
diversity measure is the compound diversity (CD), which is actually the com-
plement of the compound error rate: CD = 1 −Prob(both the classiﬁers fail). The

192
PATTERN CLASSIFICATION METHODS
diversity of multiple classiﬁers can be measured as the average CD of all pairs of
classiﬁers.
4.6.5.2
Bagging and Random Subspace
The term bagging is the acronym
of bootstrap aggregating [5]. In this method, each individual classiﬁer is trained with
a bootstrap replicate of the original training data set. Assume that the training data
set contains N vectors, a bootstrap replicate is formed by randomly sampling the
vectors with replacement for N times. The classiﬁers trained with different replicates
of training data are combined by plurality vote or simple average.
Baggingcanimprovetheaccuracyoverthesingleclassiﬁertrainedwiththeoriginal
training data set when the base classiﬁer is unstable: A small change of training data
causes a large change to the output of prediction. Examples of unstable classiﬁers
include neural networks and decision trees, whereas parametric Gaussian classiﬁers
and nonparametric k-NN classiﬁers are fairly stable. From the statistical viewpoint,
unstable classiﬁers have high variance of prediction error, and the average of multiple
bagged predictors can reduce the variance.
The random subspace method generates multiple classiﬁers in parallel by sampling
features instead of training vectors. Each individual classiﬁer is trained with all the
training samples, each represented by a randomly selected subset of features. A subset
of features spans a subspace of the original feature space occupied by all the features.
The random subspace method was experimented with a decision tree classiﬁer in [30],
but it is applicable to other classiﬁer structures as well. According to the observations
therein, combining random subspace ensemble is beneﬁcial when the data set has
a large number of features and samples and is not good when there are very few
features coupled with a small number of samples. It is expected that the random
subspace method is good when there is certain redundancy in the data set, especially
for the collection of features.
4.6.5.3
Boosting
Boosting is a general method for improving the classiﬁcation
performance by combining multiple classiﬁers generated sequentially. The theoreti-
cal work of Schapire [78] shows that by boosting it is possible to convert a learning
machine with error rate less than 0.5 to an ensemble with arbitrarily low error rate.
The underlying learning machine (base classiﬁer) is called a weak learner or weak
classiﬁer. On training the ﬁrst classiﬁer with the original training data set, each suc-
cessive classiﬁer is trained with reweighted training data (misclassiﬁed samples by
the preceding classiﬁer are assigned larger weights than correct ones) and is used to
update the weights. After a number of iterations, the trained classiﬁers are combined
by weighted plurality vote (can be extended to conﬁdence-based combination).
The ﬁrst successful implementation of boosting for pattern recognition was
reported by Drucker et al. [16]. For handwritten digit recognition, they used a
convolutional neural network as the base classiﬁer and selected some deformed
images to form the second and third training sets. Using only three classiﬁers, they
could achieve a very high recognition accuracy.
Freund and Schapire proposed the AdaBoost (adaptive boosting) algo-
rithm [20], which is the ﬁrst principled algorithm and has triggered the

COMBINING MULTIPLE CLASSIFIERS
193
multitude of theoretical and algorithmic research since the mid-1990s. The
AdaBoost assumes that the base classiﬁer gives binary hypothesis. Two extensions
of AdaBoost, called AdaBoost.M1 and AdaBoost.M2, were proposed for training
multiclass classiﬁers [19]. The AdaBoost.M1, covering AdaBoost as a special case,
is applicable to a wide range of base classiﬁers. Assume that a weak classiﬁer
ft(x) ∈{1, . . . , M} assigns an input pattern to one of M classes. Given a training
data set {(xi, yi)|i = 1, . . . , N}, yi ∈{1, . . . , M}, AdaBoost.M1 generates weak clas-
siﬁers successively in a procedure, as described below.
Algorithm: AdaBoost.M1
1. Initialize data weights D1(i) = 1/N, i = 1, . . . , N.
2. Iterate for t = 1, . . . , T,
• Train weak classiﬁer ft(x) with data distribution Dt.
• Calculate the weighted error rate
ϵt =
m

i=1
Dt(i)I[ft(xi) ̸= yi].
If ϵt > 0.5, set T = t −1 and terminate iteration.
• Choose βt =
ϵt
1−ϵt .
• Update weights:
Dt+1(i) = Dt(i)
Zt
·

βt
if ft(xi) = yi,
1
otherwise,
where Zt is used to normalize the weights to unity sum.
3. Output the ﬁnal hypothesis by weighted plurality vote:
F(x) = arg
max
y=1,...,M
T

t=1
αtI[ft(x) = y],
where αt = log 1
βt .
On a data distribution Dt, the weak classiﬁer ht(x) can be trained in two ways.
One way is to directly incorporate the weights into classiﬁer parameter estimation.
This is possible for parametric statistical classiﬁers and the Naive Bayes classiﬁer
that approximate the density of each dimension with a histogram. Another way is to
generate a new data set of N samples by resampling with replacement according to
the distribution (the weight of each sample serves the probability of being drawn).
This is applicable to any classiﬁer structure. In practice, the two ways do not differ
signiﬁcantly in classiﬁcation performance.

194
PATTERN CLASSIFICATION METHODS
4.7
A CONCRETE EXAMPLE
To demonstrate the effects of some classiﬁers and combination schemes described
in this chapter, we show some experimental results in a problem of handwritten
digit recognition on a publicly available database. The database, called USPS, can
be downloaded from the kernel machines Web site [39]. The digit images in the
database were originally segmented from the envelop images of US Postal Service
and were normalized to 16 × 16 images with 256-level gray values. The images were
partitioned into two sets: 7291 images for training and 2007 images for testing. Some
examples of the test images are shown in Figure 4.11.
We trained ﬁve individual classiﬁers, all using the 256 pixel values of a digit image
as features. The individual classiﬁers are the MLP with one hidden layer of 50 units,
the RBF network with 120 hidden units, the polynomial network classiﬁer (PNC) with
60 principal components of features, the LVQ classiﬁer with four prototypes per class
trained under the MCE criterion, the MQDF classiﬁer with 30 eigenvectors per class,
and the minor eigenvalue estimated by ﬁvefold cross-validation. The coefﬁcient of
weight decay was set equal to 0.05, 0.02, 0.1 for MLP, RBF, and PNC, respectively.
The validation data for conﬁdence transformation and combiner training was gen-
erated by stacking with ﬁvefold partitioning of training data. The outputs of neural
classiﬁers are linear outputs without sigmoidal squashing. The output score of LVQ
classiﬁer is the nearest square Euclidean distance to each class, and the class score of
MQDF is as in Eq. (4.18).
For combining the decisions of ﬁve classiﬁers at abstract level, we tested the
plurality vote (PV), dynamic classiﬁer selection with overall local accuracy (DCS-
ov) and local class accuracy (DCS-lc) in three neighboring training points, nearest
mean (NMean, also called as decision template in [48]), linear SVM, and weighted
average (WA) with the weights estimated under the MCE criterion. The WA has four
FIGURE 4.11
Examples of handwritten digits in USPS database.

A CONCRETE EXAMPLE
195
TABLE 4.1
Test accuracies (%) of individual classiﬁers, oracle, and DCS.
Classiﬁer
MLP
RBF
PNC
LVQ
MQDF
Oracle
DCS-ov
DCS-lc
Top one
93.52
94.17
95.12
94.22
94.72
97.31
95.27
95.12
Top two
96.81
97.26
98.01
97.71
97.26
variants: WA00, WA01, WA10, and WA11 (the ﬁrst “1” denotes class-independent
weights, and the second “1” denotes non-negative constraint). The weight decay
coefﬁcient in MCE-based estimation was set equal to 0.5.
For measurement-level combination, the classiﬁer outputs are transformed to sig-
moid scores, where the class-independent scaling parameters (Eq. (4.159)) were es-
timated by logistic regression with the coefﬁcient of weight decay set equal to 1.0.
The accuracies of individual classiﬁers on the test digits are shown in Table 4.1,
which also includes the accuracies of Oracle (at least one classiﬁer assigns correct
label) and DCS. The last row of the table shows the cumulative accuracies of top two
ranks. We can see that the PNC yields the highest accuracy, 95.12%. The accuracy
of Oracle indicates that 2.69% of test samples are misclassiﬁed by all the individ-
ual classiﬁers. Combining the decisions of ﬁve classiﬁers by DCS-ov (overall local
accuracy) yields a higher accuracy (95.27%) than the best individual classiﬁer.
The accuracies of abstract-level combination by plurality vote (PV) and trained
rules are shown in Table 4.2. The PV yields an accuracy of 94.92%, which is lower
than the PNC but higher than the other individual classiﬁers. The class-indifferent
combiners, NMean and SVM, yield accuracies comparable to the best individual
classiﬁers. The comparable accuracies of four variants of WA indicate that the shar-
ing of class weights and the non-negative constraint are not inﬂuential. The highest
accuracy is obtained by WA00, with free class-speciﬁc weights.
The accuracies of measurement-level combination by ﬁxed rules (sum, product,
and median) and trained rules (NMean, SVM, and four variants of WA) are shown in
Table 4.3. By fusing conﬁdence measures, the ﬁxed rules yield lower accuracies than
the abstract-level combination (PV). However, it is evident that measurement-level
combination (except the NMean) maintains a higher cumulative accuracy of two ranks
than abstract-level combination. The trained combiners, SVM and WA, yield higher
accuracies than the best individual classiﬁer, and again the four variants of WA do
not differ signiﬁcantly. The highest accuracy, 95.32%, is obtained by WA00.
It is interesting to look into the classiﬁer weights of WA. On normalizing the non-
negative weights of WA11 into unity sum, the weights corresponding to ﬁve classiﬁers
TABLE 4.2
Accuracies (%) of combination at abstract level: plurality vote and trained
rules.
Method
PV
NMean
SVM
WA00
WA01
WA10
WA11
Top one
94.92
95.02
95.07
95.22
95.12
95.07
95.17
Top two
97.16
97.36
96.96
97.01
96.91
97.16
97.21

196
PATTERN CLASSIFICATION METHODS
TABLE 4.3
Accuracies (%) of combination at measurement level: ﬁxed and trained
rules.
Method
Sum
Prod
Median
NMean
SVM
WA00
WA01
WA10
WA11
Top one
94.62
94.62
94.52
94.57
94.97
95.32
95.17
95.17
95.17
Top two
97.71
97.51
97.46
97.31
97.26
97.71
97.56
97.81
97.91
are {0.23, 0, 0.43, 0.12, 0.22} for the abstract level and {0.06, 0, 0.42, 0, 0.53} for the
measurement level. The best two individual classiﬁers, PNC and MQDF, are assigned
large weights. The RBF classiﬁer, though performing better than MLP, is totally
ignored in combination. The decision of measurement-level combination is almost
made by only two classiﬁers: PNC and MQDF.
In another experiment, we combined only two classiﬁers, PNC and MQDF. The
test accuracies of plurality vote and sum rule (average of sigmoidal conﬁdence) are
95.17 and 95.37%, respectively. The higher accuracy of averaging two classiﬁers
than the weighted combination of a larger set of ﬁve classiﬁers can be explained
that the validation data set is not representative enough for estimating the combining
weights. We observed that by conﬁdence averaging of two classiﬁers, eight test digits
misclassiﬁed by PNC were corrected, as shown in Figure 4.12.
FIGURE 4.12
Test digits corrected by combining PNC and MQDF. The second and third
columns show the conﬁdence values of two classes by PNC, the fourth and ﬁfth columns show
the conﬁdence values of two classes by MQDF, and the rightmost column shows the ﬁnal
decision.

REFERENCES
197
4.8
CHAPTER SUMMARY
Oriented to character recognition applications, this chapter ﬁrst gives an overview
of pattern classiﬁcation methods, and then goes into details for some representative
methods, including those that have been long proven effective and some emerging
ones that are still under research. Some classiﬁers and multiple classiﬁer combination
schemes are tested in an experiment of handwritten digit recognition.
Statistical pattern recognition is based on the Bayes decision theory and is in-
stantiated by classiﬁers based on parametric and nonparametric density estimation.
Its principles are also important for better understanding and implementing neural
networks, support vector machines (SVMs), and multiple classiﬁer systems. Unlike
statistical methods that are based on classwise density estimation, neural networks,
SVMs, and boosting are based on discriminative learning, that is, their parameters are
estimated with the aim of optimizing a classiﬁcation objective. Discriminative clas-
siﬁers can yield higher generalization accuracies when trained with a large number
of samples.
For structural pattern recognition, we describe only two methods that have been
widely used, especially in online character recognition: attributed string matching and
attributed graph matching. Despite that the automatic learning of structural models
from samples is not well solved, structural recognition methods have some advan-
tages over statistical methods and neural networks: They interpret the structure of
characters, store less parameters, and are sometimes more accurate.
Neural networks are considered to be pragmatic and somewhat obsolete com-
pared to SVMs and boosting, but, actually, they yield competitive performance at
much lower (training and operation) complexity. Nevertheless, for neural classiﬁers
to achieve good performance, skilled implementation of model selection and nonlin-
ear optimization are required. Potentially higher accuracies can be obtained by SVMs
and multiple classiﬁer methods.
Though the classiﬁers described in this chapter assume single character recogni-
tion, they are all applicable to integrated segmentation and recognition of character
strings. For string recognition, the objective is no longer the accuracy of single
characters, but the holistic accuracy of string segmentation-recognition. Accordingly,
some classiﬁer structures and learning algorithms need to be customized. Classiﬁer
design strategies oriented to string recognition have been addressed in Chapter 5.
REFERENCES
1. N. E. Ayat, M. Cheriet, and C. Y. Suen. Automatic model selection for the optimization
of SVM kernels. Pattern Recognition. 38(10), 1733–1745, 2005.
2. G. H. Bakir, L. Bottou, and J. Weston. Breaking SVM complexity with cross-training. In
Advances in Neural Information Processing Systems 18. Vancouver, Canada, 2004.
3. J. A. Barnett. Computational methods for a mathematical theory of evidence. In Pro-
ceedings of the 7th International Joint Conference on Artiﬁcial Intelligence. Vancouver,
Canada, 1981, pp. 868–875.

198
PATTERN CLASSIFICATION METHODS
4. C. M. Bishop. Neural Networks for Pattern Recognition. Oxford University Press, 1995.
5. L. Breiman. Bagging predictors. Machine Learning. 24(2), 123–140, 1996.
6. C. J. C. Burges. Simpliﬁed support vector decision rules. In Proceedings of the 13th
International Conference on Machine Learning. Bari, Italy, 1996, pp. 71–77.
7. C. J. C. Burges. A tutorial on support vector machines for pattern recognition. Knowledge
Discovery and Data Mining. 2(2), 1–43, 1998.
8. C. C. Chang and C. J. Lin. LIBSVM: A library for support vector machines. Technical
Report, Department of Computer Science and Information Engineering, National Taiwan
University, 2003.
9. O. Chapelle, V. Vapnik, O. Bousquet, and S. Mukherjee. Choosing multiple parameters
for support vector machines. Machine Learning. 46(1–3), 131–159, 2002.
10. N. Cristianini and J. Shawe-Taylor. An Introduction to Support Vector Machines and
Other Kernel-Based Learning Methods. Cambridge University Press, 2000.
11. A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incom-
plete data via the EM algorithm. Journal of Royal Statistical Society B. 39(1), 1–38,
1977.
12. P. A. Devijver and J. Kittler. Pattern Recognition: A Statistical Approach. Prentice Hall,
Englewood Cliffs, NJ, 1982.
13. J.-X. Dong, A. Krzyzak, and C. Y. Suen. Fast SVM training algorithm with decomposition
on very large data sets. IEEE Transactions on Pattern Analysis and Machine Intelligence.
27(4), 603–618, 2005.
14. T. Downs. Exact simpliﬁcation of support vector solutions. Journal of Machine Learning
Research. 2, 293–297, 2001.
15. P. M. L. Drezet and R. F. Harrison. A new method for sparsity control in support vector
classiﬁcation and regression. Pattern Recognition. 34(1), 111–125, 2001.
16. H. Drucker, R. Scahpire, and P. Simard. Boosting performance in neural networks.
International Journal on Pattern Recognition and Artiﬁcial Intelligence. 7(4), 705–719,
1993.
17. R. O. Duda, P. E. Hart, and D. G. Stork. Pattern Classiﬁcation. 2nd edition, Wiley Inter-
science, New York, 2001.
18. R. P. W. Duin. The combining classiﬁers: To train or not to train. In Proceedings of the
16th International Conference on Pattern Recognition. Quebec, Canada, 2002, Vol. 2,
pp. 765–770.
19. Y. Freund and R. E. Schapire. Experiments with a new boosting algorithm, In Proceedings
of the 13th International Conference on Machine Learning. Bari, Italy, 1996, pp. 148–156.
20. Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and
an application to boosting. Journal of Computer and System Sciences. 55(1), 119–139,
1997.
21. H. Friedman. Regularized discriminant analysis. Journal of the American Statistical
Association. 84(405), 166–175, 1989.
22. K. S. Fu and B. K. Bhargava. Syntactic Methods for Pattern Recognition. Academic Press,
1974.
23. K. Fukunaga. Introduction to Statistical Pattern Recognition. 2nd edition, Academic
Press, 1990.

REFERENCES
199
24. G. Fumera and F. Roli. A theoretical and experimental analysis of linear combiners
for multiple classiﬁer systems. IEEE Transactions on Pattern Analysis and Machine
Intelligence. 27(6), 942–856, 2005.
25. G.GiacintoandF.Roli.Anapproachtotheautomaticdesignofmultipleclassiﬁersystems.
Pattern Recognition Letters. 22(1), 25–33, 2001.
26. R. M. Gray. Vector quantization. IEEE ASSP Magazine. 1(2), 4–29, 1984.
27. S. Hashem. Optimal linear combinations of neural networks. Neural Networks. 10(4),
599–614, 1997.
28. T. Hastie and R. Tibshirani. Classiﬁcation by pairwise coupling. The Annals of Statistics.
26(2), 451–471, 1998.
29. S. Haykin. Neural Networks: A Comprehensive Foundation. Prentice-Hall, Inc., 1999.
30. T. K. Ho. The random subspace method for constructing decision forests. IEEE Transac-
tions on Pattern Analysis on Machine Intelligence. 20(8), 832–844, 1998.
31. T. K. Ho, J. Hull, and S. N. Srihari. Decision combination in multiple classiﬁer systems.
IEEE Transactions on Pattern Analysis on Machine Intelligence. 16(1), 66–75, 1994.
32. L. Holmstr¨om, P. Koistinen, J. Laaksonen, and E. Oja. Neural and statistical classiﬁers—
taxonomy and two case studies. IEEE Transactions on Neural Networks. 8(1), 5–17,
1997.
33. Y. S. Huang and C. Y. Suen. A method of combining multiple experts for the recognition of
unconstrained handwritten numerals. IEEE Transactions on Pattern Analysis on Machine
Intelligence. 17(1), 90–94, 1995.
34. A. K. Jain, R. P. W. Duin, and J. Mao. Statistical pattern recognition: A review. IEEE
Transactions on Pattern Analysis on Machine Intelligence. 22(1), 4–37, 2000.
35. A. K. Jain, M. N. Murty, and P. J. Flynn. Data clustering: A review. ACM Computing
Survey. 31(3), 264–323, 1999.
36. T. Joachims. Making large-scale support vector machine learning practical. In B.
Sch¨olkopf, C. J. C. Burges, and A. J. Smola, editors, Advances in Kernel Methods: Support
Vector Learning. MIT Press, 1999, pp. 169–184.
37. B.-H. Juang and S. Katagiri. Discriminative learning for minimum error classiﬁcation.
IEEE Transactions on Signal Processing. 40(12), 3043–3054, 1992.
38. H.-J. Kang, K. Kim, and J. H. Kim. Optimal approximation of discrete probability distri-
bution with kth-order dependency and its application to combining multiple classiﬁers.
Pattern Recognition Letters. 18(6), 515–523, 1997.
39. Kernel machines web site. http://www.kernel-machines.org/
40. F. Kimura, S. Inoue, T. Wakabayashi, S. Tsuruoka, and Y. Miyake. Handwritten numeral
recognition using autoassociative neural networks. In Proceedings of the 14th Interna-
tional Conference on Pattern Recognition, Brisbane, Australia, 1998, Vol. 1, pp. 166–171.
41. F. Kimura, K. Takashina, S. Tsuruoka, and Y. Miyake. Modiﬁed quadratic discriminant
functions and the application to Chinese character recognition. IEEE Transactions on
Pattern Analysis on Machine Intelligence. 9(1), 149–153, 1987.
42. J. Kittler, M. Hatef, R. P. W. Duin, and J. Matas. On combining classiﬁers. IEEE Trans-
actions on Pattern Analysis on Machine Intelligence. 20(3), 226–239, 1998.
43. T. Kohonen. The self-organizing map. Proceedings of the IEEE. 78(9), 1464–1480,
1990.

200
PATTERN CLASSIFICATION METHODS
44. T. Kohonen. Improved versions of learning vector quantization. In Proceedings of the
1990 International Joint Conference on Neural Networks. San Diego, CA, Vol. 1,
pp. 545–550.
45. U. Kreßel. Pairwise classiﬁcation and support vector machines. In B. Sch¨olkopf, C. J. C.
Burges, and A. J. Smola, editors, Advances in Kernel Methods: Support Vector Learning.
MIT Press, 1999, pp. 255–268.
46. U. Kreßel and J. Sch¨urmann. Pattern classiﬁcation techniques based on function approx-
imation. In H. Bunke and P. S. P. Wang, editors, Handbook of Character Recognition and
Document Image Analysis. World Scientiﬁc, Singapore, 1997, pp. 49–78.
47. L. I. Kuncheva. Combining Pattern Classiﬁers: Methods and Algorithms. Wiley Inter-
science, 2004.
48. L. I. Kuncheva, J.C. Bezdek, and R.P.W. Duin. Decision templates for multiple
classiﬁer fusion: an experimental comparison. Pattern Recognition. 34(2), 299–314,
2001.
49. L. Lam and C. Y. Suen. Optimal combinations of pattern classiﬁers. Pattern Recognition
Letters. 16, 945–954, 1995.
50. L. Lam and C. Y. Suen. Application of majority voting to pattern recognition: an analysis
of its behavior and performance. IEEE Transactions on System Man Cybernet. Part A.
27(5), 553–568, 1997.
51. Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to doc-
ument recognition. Proceedings of the IEEE. 86(11), 2278–2324, 1998.
52. C.-L. Liu. Classiﬁer combination based on conﬁdence transformation. Pattern Recogni-
tion. 38(1), 11–28, 2005.
53. C.-L.Liu,S.Jaeger,andM.Nakagawa.OnlinehandwrittenChinesecharacterrecognition:
Thestateoftheart.IEEETransactionsonPatternAnalysisonMachineIntelligence.26(2),
198–213, 2004.
54. C.-L. Liu, I.-J. Kim, and J. H. Kim. Model-based stroke extraction and matching for hand-
written Chinese character recognition. Pattern Recognition. 34(12), 2339–2352, 2001.
55. C.-L. Liu and M. Nakagawa. Evaluation of prototype learning algorithms for nearest
neighbor classiﬁer in application to handwritten character recognition. Pattern Recogni-
tion. 34(3), 601–615, 2001.
56. C.-L. Liu, K. Nakashima, H. Sako, and H. Fujisawa. Handwritten digit recognition:
Benchmarking of state-of-the-art techniques. Pattern Recognition. 36(10), 2271–2285,
2003.
57. C.-L. Liu, H. Sako, and H. Fujisawa. Discriminative learning quadratic discriminant
function for handwriting recognition. IEEE Transactions on Neural Networks. 15(2),
430–444, 2004.
58. O. L. Mangasarian and D. R. Musicant. Successive overrelaxation for support vector
machines. IEEE Transactions on Neural Networks. 10(5), 1032–1037, 1999.
59. S. Mannor, D. Peleg, and R. Y. Rubinstein. The cross entropy method for classiﬁcation.
In L. De Raedt and S. Wrobel, editors, Proceedings of the 22nd International Conference
on Machine Learning. Bonn, Germany, ACM Press, 2005.
60. T. M. Martinetz, S. G. Berkovich, and K. J. Schulten. “Neural-gas” network for vector
quantization and its application to time-series prediction. IEEE Transactions on Neural
Networks. 4(4), 558–569, 1993.

REFERENCES
201
61. A. Marzal and E. Vidal. Computation of normalized edit distance and applications.
IEEE Transactions on Pattern Analysis on Machine Intelligence. 15(9), 926–932,
1993.
62. W. S. McCulloch and W. Pitts. A logical calculus of the ideas immanent in nervous
activity. Bulletin of Mathematical Biophysics. 5, 115–133, 1943.
63. B. Moghaddam and A. Pentland. Probabilistic visual learning for object representa-
tion. IEEE Transactions on Pattern Analysis on Machine Intelligence. 19(7), 696–710,
1997.
64. N. J. Nilsson. Principles of Artiﬁcial Intelligence. Springer-Verlag, 1980.
65. I.-S. Oh and C. Y. Suen. A class-modular feedforward neural network for handwriting
recognition. Pattern Recognition. 35(1), 229–244, 2002.
66. P. Parent and S. W. Zucher. Radial projection: an efﬁcient update rule for relaxation
labeling. IEEE Transactions on Pattern Analysis on Machine Intelligence. 11(8), 886–
889, 1989.
67. J.C. Platt. Fast training of support vector machines using sequential minimal optimization.
In B. Sch¨olkopf, C. J. C. Burges, and A. J. Smola, editors, Advances in Kernel Methods:
Support Vector Learning. MIT Press, 1999, pp. 185–208.
68. J. Platt. Probabilistic outputs for support vector machines and comparisons to regularized
likelihood methods. In A. J. Smola, P. Bartlett, D. Scholkopf, and D. Schuurmanns,
editors, Advances in Large Margin Classiﬁers. MIT Press, 1999.
69. J. Platt, N. Cristianini, and J. Shawe-Taylor. Large margin DAGs for multiclass classi-
ﬁcation. In Advances in Neural Information Processing Systems 12. MIT Press, 2000,
pp. 547–553.
70. A. F. R. Rahman and M. C. Fairhurst. Multiple classiﬁer decision combination strategies
for character recognition: A review. International Journal on Document Analysis and
Recognition. 5(4), 166–194, 2003.
71. M. D. Richard, R. P. Lippmann. Neural network classiﬁers estimate Bayesian a posteriori
probabilities. Neural Computation. 4, 461–483, 1991.
72. H. Robbins and S. Monro. A stochastic approximation method. Annals of Mathematical
Statistics. 22, 400–407, 1951.
73. F. Roli, G. Giacinto, and G. Vernazza. Methods for designing multiple classiﬁers systems.
In J. Kittler and F. Roli, editors, Multiple Classiﬁer Systems. LNCS Vol. 2096, Springer,
2001, pp. 78–87.
74. A. Rosenfeld, R. A. Hummel, and S. W. Zucker. Scene labeling by relaxation operations.
IEEE Transactions on System, Man, and Cybernetics. 6(6), 420–433, 1976.
75. D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning internal representations by
error propagation. In D. E. Rumelhart, J. L. McClelland, and the PDP Research Group,
editors, Parallel Distributed Processing: Explorations in the Microstructure of Cognition.
MIT Press, Cambridge, MA, 1986, Vol. 1, pp. 318–362.
76. A. Sanfeliu and K.-S. Fu. A distance measure between attributed relational graphs
for pattern recognition. IEEE Transactions on System, Man, and Cybernetics. 13(3),
353–362, 1983.
77. A. Sato and K. Yamada. A formulation of learning vector quantization using a new
misclassiﬁcation measure. In Proceedings of the 14th International Conference on Pattern
Recognition. Brisbane, 1998, Vol. I, pp. 322–325.

202
PATTERN CLASSIFICATION METHODS
78. R. E. Schapire. The strength of weak learnability. Machine Learning. 5(2), 197–227,
1990.
79. B. Sch¨olkopf and A. J. Smola. Learning with Kernels. MIT Press, Cambridge, MA,
2002.
80. J. Sch¨urmann. Pattern Classiﬁcation: A Uniﬁed View of Statistical and Neural Ap-
proaches. Wiley Interscience, New York, 1996.
81. G. Shafer. A Mathematical Theory of Evidence. Princeton University Press, 1976.
82. Y. Shin and J. Ghosh. Ridge polynomial networks. IEEE Transactions on Neural
Networks. 6(3), 610–622, 1995.
83. C. Y. Suen and L. Lam. Multiple classiﬁer combination methodologies for different output
levels. In J. Kittler and F. Roli, editors, Multiple Classiﬁer Systems. LNCS Vol. 1857,
Springer, 2000, pp. 52–66.
84. C. Y. Suen, C. Nadal, R. Legault, T. A. Mai, and L. Lam. Computer recognition of
unconstrained handwritten numerals. Proceedings of the IEEE. 80(7), 1162–1180, 1992.
85. C. C. Tappert, C. Y. Suen, and T. Wakahara. The state of the art in on-line handwriting
recognition. IEEE Transactions on Pattern Analysis on Machine Intelligence. 12(8), 787–
808, 1990.
86. K. M. Ting and I. H. Witten. Issues in stacked generalization. Journal on Artiﬁcial Intel-
ligence Research. 10, 271–289, 1999.
87. W.-H. Tsai and K.-S. Fu. Subgraph error-correcting isomorphisms for syntactic pat-
tern recognition. IEEE Transactions on System, Man, and Cybernetics. 13(1), 48–62,
1983.
88. W.-H. Tsai and S.-S. Yu. Attributed string matching with merging for shape recogni-
tion. IEEE Transactions on Pattern Analysis on Machine Intelligence. 7(4), 453–462,
1985.
89. Y.-T. Tsay and W.-H. Tsai. Attributed string matching by split-and-merge for on-line
Chinese character recognition. IEEE Transactions on Pattern Analysis on Machine Intel-
ligence. 15(2), 180–185, 1993.
90. N. Ueda. Optimal linear combination of neural networks for improving classiﬁcation
performance. IEEE Transactions on Pattern Analysis on Machine Intelligence. 22(2),
207–215, 2000.
91. J. R. Ullmann. An algorithm for subgraph isomorphism. Journal of ACM. 23(1), 31–42,
1976.
92. V. Vapnik. The Nature of Statistical Learning Theory. Springer, New Work, 1995.
93. A. Verikas, A. Lipnickas, K. Malmqvist, M. Bacauskiene, and A. Gelzinis. Soft com-
bination of neural classiﬁers: A comparative study. Pattern Recognition Letters. 20(4),
429–444, 1999.
94. R. A. Wagner and M. J. Fischer. The string-to-string correction problem. Journal of ACM.
21(1), 168–173, 1974.
95. D. H. Wolpert. Stacked generalization. Neural Networks. 5, 241–259, 1992.
96. A. K. C. Wong, M. You, and S. C. Chan. An algorithm for graph optimal monomorphism,
IEEE Transactions on System, Man, and Cybernetics. 20(3), 628–636, 1990.
97. K. Woods, W. P. Kegelmeyer, and K. Bowyer. Combination of multiple classiﬁers using
local accuracy estimates. IEEE Transactions on Pattern Analysis on Machine Intelligence.
19(4), 405–410, 1997.

REFERENCES
203
98. L. Xu, A. Krzyzak, and C. Y. Suen. Methods of combining multiple classiﬁers and their
applications to handwriting recognition. IEEE Transactions on System, Man, and Cyber-
netics. 22(3), 418–435, 1992.
99. E. Yair, K. Zeger, and A. Gersho. Competitive learning and soft competition vector quan-
tizer design. IEEE Transactions on Signal Processing. 40, 294–309, 1992.
100. K. Yamamoto and A. Rosenfeld. Recognition of handprinted Kanji characters by a relax-
ation method. In Proceedings of the 6th International Conference on Pattern Recognition.
Munich, Germany, 1982, pp. 395–398.

CHAPTER 5
WORD AND STRING RECOGNITION
Practical documents consist of words or character strings rather than isolated charac-
ters as elementary units. Very often, the characters composing a word or string cannot
be reliably segmented before they are recognized. Although the character feature ex-
traction and classiﬁcation techniques described in the previous chapters are integral for
word/string recognition, special strategies are required to integrate segmentation and
classiﬁcation. This chapter addresses such kind of strategies, including explicit seg-
mentation, implicit segmentation, classiﬁer design, path search, lexicon organization,
holistic recognition, and so on. It is organized in an analytical-to-holistic fashion: starts
from oversegmentation, then describes the techniques involved in classiﬁcation-based
(character-model-based) recognition, hidden Markov models (HMMs)-based recogni-
tion, and ends with holistic recognition.
5.1
INTRODUCTION
The techniques of feature extraction described in Chapter 3 and those of classiﬁcation
in Chapter 4 are primarily aimed for isolated character recognition. In practical doc-
ument images and online handwritten texts, however, characters are not in isolation.
In English texts, words are separated by apparent space, but the letters within a word
are not well separated, so words can be considered as natural units to recognize. In
Chinese and Japanese texts, a sentence separated by punctuation marks is an inte-
gral unit because there is no difference between the interword and intercharacter
Character Recognition Systems: A Guide for Students and Practitioner, by M. Cheriet, N. Kharma,
C.-L. Liu and C. Y. Suen
Copyright © 2007 John Wiley & Sons, Inc.
204

INTRODUCTION
205
gaps. Word or sentence recognition, or generally, character string recognition, faces
the difﬁculty of character segmentation: the constituent characters cannot be reli-
ably segmented before they are recognized (Sayre’s paradox [82]). Viewing a whole
word/string as a single pattern circumvents segmentation, but then, the number of
pattern classes will be huge!
This chapter describes strategies and techniques for word/string recognition. As a
recognition method, especially segmentation-based one, can apply to both word and
string recognition, we will refer to word and string interchangeably. The context of
language, available in most environments, can greatly reduce the ambiguity in the
segmentation and recognition of characters. The language context can be deﬁned by
a set of legal words, called a lexicon, or a statistical form (often, the n-gram [85]) in
an open vocabulary. It can be applied while segmentation and recognition are being
performed or in a postprocessing stage after lexicon-free recognition.
Numerous word/string recognition methods have been developed since the 1970s,
and a major progress was made in the 1990s [1, 75, 83, 89]. Many recognition systems
have reported fairly high recognition accuracies, sometimes for very large vocabu-
laries with tens of thousands of words [51]. According to the underlying techniques
of segmentation, character recognition, and linguistic processing, we categorize the
string recognition methods as in Figure 5.1, which is similar to the hierarchy of
segmentation methods of Casey and Lecolinet [13].
Depending on whether the characters are segmented or not, the word recogni-
tion methods can be categorized into two major groups: analytical (segmentation-
based) and holistic (segmentation-free). Holistic recognition, where each lexicon en-
try (word class) should be built in a model, generally applies to small and ﬁxed
lexicons. Segmentation-based methods, which form string classes by concatenating
character classes, apply to both lexicon-driven and lexicon-free recognition and can
accommodate a very large vocabulary. Lexicon-free recognition is typically applied
to numeral strings, where little linguistic context is available, or to word/string recog-
nition followed by linguistic postprocessing.
FIGURE 5.1
Overview of word/string recognition methods.

206
WORD AND STRING RECOGNITION
Segmentation methods can be generally divided into explicit ones and implicit
ones. Explicit segmentation tries to separate the string image at character boundaries
and usually results in an over list of candidate segmentation points. So, explicit seg-
mentation is also called oversegmentation. Implicit segmentation blindly slices the
string image into frames of equal size (width). The frames, each represented by a fea-
ture vector, are grouped into characters by recognition. Explicit oversegmentation can
be similarly performed by slicing (merging slices into candidate character patterns)
or windowing (e.g., [68]). The process of oversegmentation and candidate pattern
formation is also called presegmentation.
The character recognizer for segmentation-based string recognition can be any
classiﬁer or a multiple classiﬁer system as described in Chapter 4. A mathematical
tool, hidden Markov models (HMM) [76], is often used with implicit segmentation,
but can be used for modeling character classes in explicit segmentation and modeling
word classes in holistic recognition as well. Character-model HMM for segmentation-
based recognition is also called path discriminant, whereas holistic word-model HMM
is also called model discriminant. Holistic recognition can also be performed by
dynamic programming (DP) for word template matching and classiﬁcation based on
vector representation of global features.
The preprocessing techniques for word recognition (image smoothing, deskew-
ing, slant correction, baseline detection, etc.) have been discussed in Chapter
2. In the remainder of this chapter, we ﬁrst describe the segmentation tech-
niques, focusing on presegmentation. We then describe explicit segmentation-
based methods under a statistical string classiﬁcation model, which uniﬁes char-
acter classiﬁcation and context analysis based on oversegmentation. We introduce
HMM-based recognition in a separate section due to the special characteristics
of HMMs compared to the other classiﬁcation tools. Holistic recognition meth-
ods are described with emphasis on global feature extraction and word template
matching.
We assume that the object to recognize is an ofﬂine word/string image, but except
preprocessing and presegmentation, most strategies are common to online and ofﬂine
handwriting recognition.
5.2
CHARACTER SEGMENTATION
Machine-printed or hand-drawn scripts can have various font types or writing styles.
The writing styles can be roughly categorized into discrete style (handprint or boxed
style), continuous style (cursive style), and mixed style. Figure 5.2 shows some
character string images of different languages and various styles. We can see that
the ambiguity of character segmentation has three major sources: (1) variability of
character size and intercharacter space; (2) confusion between intercharacter and
within-character space (some characters have multiple components); and (3) touch-
ing between characters. To solve the ambiguity, segmentation is often integrated with
character recognition: character patterns are hypothesized and veriﬁed by recognition.
Linguistic context is effective to disambiguate both the classes and the boundaries of

CHARACTER SEGMENTATION
207
FIGURE 5.2
Character strings of different languages and styles (English words and numeral
strings are separated by white space).
characters (it is the case that a composite character can be split into multiple legal
characters).
The problem of segmentation has been attacked by many works, among which
are many techniques of image dissection, integrated segmentation and recognition,
and lexicon manipulation [13, 62]. As string recognition methods will be specially
addressed later, in this section we describe only presegmentation (oversegmentation)
techniques, which decompose a string image into subunits (primitive segments).
Slicing and windowing techniques do not consider the shape features in string
image. To guarantee that all the characters are separated apart, the number of candi-
date character patterns will be very large. This brings heavy burden to the succeeding
character recognition and lexical matching procedures, and may deteriorate the string
recognition accuracy because of the imperfection of character veriﬁcation and recog-
nition. In the following, we consider only the dissection techniques that take into
account shape cues to detect character-like components. We ﬁrst give an overview
of dissection techniques, and then describe an approach specially for segmenting
handwritten digits.
5.2.1
Overview of Dissection Techniques
In oversegmentation, a character can be split into multiple primitive segments, but
different characters should be contained in different segments. Thus, the candidate
patterns formed by merging successive segments will contain the true characters,
which are to be detected by recognition. The ideal case that each character is contained
in a unique segment is hard to achieve, but it is hoped that the number of segments is
as small as possible.
Projection analysis (PA) and connected component analysis (CCA) are two popular
segmentation techniques. If the characters in a string image are not connected, all the
characters can be formed by merging connected components (CCs). PA succeeds if
each pair of neighboring characters can be separated by a white space (a straight
line in background). For touching characters (multiple characters contained in one
connected component), special techniques are needed to split at the touching points.

208
WORD AND STRING RECOGNITION
FIGURE 5.3
Character segmentation by projection analysis. (a) Characters separable by
vertical projection; (b) separation at points of small projection; (c) separation by projection in
the middle zone.
In the following, we brieﬂy describe PA, CCA, and a ligature analysis technique for
segmenting cursive words.
5.2.1.1
Projection Analysis
If the characters in a string image of horizontal
writing are separated by vertical straight lines, the image can be decomposed into seg-
ments at the breaks of horizontal projection proﬁle (vertical projection) (Fig. 5.3(a)).
If the characters can be separated by slanted lines but not vertical lines, then projection
at an angle can solve, but this can be converted to vertically separable case by slant
correction. Further, for string image separable by projection, if the within-character
gap is apparently smaller than the intercharacter gap, then the characters can be un-
ambiguously segmented before recognition. Nevertheless, this only applies to very
limited cases.
PA can also separate touching characters in some special touching situations (usu-
ally for machine-printed characters). If the border between two characters has only
one touching point and the projection at this point is small, the string image can be
oversegmented at points of small projection below a threshold (Fig. 5.3 (b)). For some
scripts, characters cannot be separated by projecting the whole string image, but are
separable in a zone of the image (Fig. 5.3 (c)).
5.2.1.2
Connected Component Analysis
CCs are generally considered in
binary images. Two pixels are said to be 8-connected (4-connected) if they are con-
nected by a chain of 8-connected (4-connected) pixels. A CC is a set of pixels in which
each pixel is connected to the rest. Usually, CCs in foreground (pixels valued 1) are
deﬁned under 8-connectivity and those in background (pixels valued 0) are deﬁned
under 4-connectivity. In document images, the stroke pixels of the same character are
connected more often than those between different characters. If there are no touching
characters, then all the characters can be separated by CCA. Touching characters that
have stroke pixels in a common CC should then be split at the points of touching.
Character segmentation by CCA is accomplished in several steps: CC labeling,
overlapping components merging, and touching character splitting. By CC labeling,
the pixels of different components are stored in different sets or labeled with different
pixel values. There have been many effective algorithms for labeling CCs, which can
be roughly divided into two categories: raster scan [37, 86] and contour tracing [14].

CHARACTER SEGMENTATION
209
FIGURE 5.4
Character segmentation by connected component analysis. Upper: connected
components (left) and segments after merging horizontally overlapping components.
By raster scan, all the CCs can be found in two passes (forward and background) or
a forward scan with local backtracking. By contour tracing, the pixels enclosed by
different outer contours (outer and inner contours are traced in different directions)
belong to different CCs.
For multicomponent characters like Chinese characters, the components of the
same character largely overlap on the axis of writing direction. The heavily overlap-
ping components can be safely merged because the components of different characters
overlap less frequently and more slightly. The degree of overlapping can be quanti-
tatively measured (e.g., [56, 60]). Figure 5.4 shows some examples of segmentation
by CCA. Each component in Figure 5.4 is enclosed within a rectangular box. The
ﬁve characters of the upper string are decomposed into 18 CCs, which are merged
into eight components (primitive segments). The lower left string has a character dis-
connected into two CCs, and the lower right one has a CC containing two touching
characters. All the three string images have overlapping components between dif-
ferent characters. This can be easily separated by CCA but cannot be separated by
projection.
5.2.1.3
Ligature Analysis
Handwritten characters in cursive scripts are
connected frequently. Very often, the letters of a word form one single connected
component (see Figure 5.5). The stroke connecting two neighboring characters is
called a ligature. Generally, the ligature turns out to be a horizontal sequence of verti-
cal black runs in binary images. In the region of ligature, most columns of the image
have only one black run, and the height of the run is small (corresponding to a hor-
izontal or nearly horizontal stroke). These columns are called single-run columns,
and the corresponding ligature is “visible” from both the top and the bottom of the
image. To separate the word image into primitive segments at visible ligatures, ﬁrst
the sequences of single-run columns are found. For each sequence, if the upper or
lower contour has an apparent valley (the lowest point signiﬁcantly lower than the
other points), the word image is separated at the valley, otherwise separated at the
middle of the horizontal sequence of runs.
After separating the word image at visible ligatures, if any primitive segment is
wide enough (compared to the height of middle zone of word image), it is a potential
touching pattern, in which two or more characters are connected by “invisible” liga-
tures. Invisible ligatures can be detected in two cases. In one case, the lower contour
of word image has an apparent valley, at which the image can be separated forcedly
(left word of Fig. 5.5). In another case, the ligature can be traced on the outer contour.

210
WORD AND STRING RECOGNITION
FIGURE 5.5
Cursive word segmentation at visible ligatures (denoted by short vertical lines)
and invisible ligatures (denoted by long vertical lines).
On detecting the ligature in this case, the word image is separated at the valley or the
middle of the ligature (right word of Fig. 5.5).
5.2.2
Segmentation of Handwritten Digits
Thesegmentationandrecognitionofunconstrainedhandwrittennumeralstringsisone
of the most difﬁcult problems in the area of optical character recognition (OCR). It has
been a popular topic of research for many years and has many potential applications
such as postal code recognition, bank check processing, tax form reading, and reading
of many other special forms. Generally, numeral strings contain isolated (with a lot of
variations), touched, overlapped, and noisy or broken digits. Some examples of these
cases are shown in Figure 5.6. The segmentation of touching digits is one of the main
challenges in handwritten numeral recognition systems. Even though handwritten
digits are not so frequently touched as cursive words, they show different features of
touching points, and hence need special techniques to separate.
Many algorithms have been proposed for touching digits segmentation, and gen-
erally they can be classiﬁed into three categories. The ﬁrst uses foreground features
extracted from the black pixels in the image, frequently, by contour shape analysis
[84]. The second uses background features extracted from the white pixels in the
image [20, 61], and the third category uses a combination of both foreground and
background features [19, 80]. Normally, approaches of the third category yield bet-
ter results. For numeral string recognition, the segmentation module ﬁrst provides a
list of candidate character patterns, and each of them is evaluated by the character
FIGURE 5.6
Some examples of handwritten numeral strings.

CHARACTER SEGMENTATION
211
FIGURE 5.7
Numeral string image preprocessing (smoothing and slant correction). Left:
original; right: preprocessed.
recognition module. The character recognition scores are postprocessed to ﬁnd the
best partitioning of character patterns.
Before segmentation and recognition, the numeral string image is preprocessed
using the techniques introduced in Chapter 2. First, the string image is smoothed
in order to remove small spots of noise and to smooth the contour of characters.
Slant correction is then performed to make the segmentation task easier and the
segmentation paths as straight as possible. Some results of preprocessing operations
are shown in Figure 5.7.
If the digits in a string image are not touched, the digits can be easily separated
by CCA. Touching digits in one connected component need to be separated at the
character boundary by shape analysis. In the following, we focus on touching digits
separation utilizing foreground and background features.
5.2.2.1
Feature Points’ Extraction
To ﬁnd feature points on the foreground
(black pixels) of each connected component, an algorithm based on skeleton tracing
is introduced. First, by using the Zhang–Suen thinning algorithm introduced in Chap-
ter 2, the skeleton of each connected component is extracted (see Figure 5.8.) On
the skeleton, a starting point and an ending point (denoted by S and E, respectively)
are found. The starting point is the leftmost skeleton point (if there are multiple
skeleton points in the leftmost column, the top point), and the ending point is the
rightmost skeleton point (top point in the rightmost column). From the starting point,
the skeleton is traced to the ending point in two different directions: clockwise and
counterclockwise. In Figure 5.8(c), the trace in clockwise direction is denoted as top
skeleton, and the trace in counterclockwise direction is denoted as bottom skeleton.
In clockwise and counterclockwise tracing, the intersection points (IPs) on the
skeleton are detected. The IPs are the points that have more than two 8-connected
branches on the skeleton. Some intersection points on the top/bottom skeleton may
be visited more than once. Corresponding to each visit of an intersection point at the
skeleton, the bisector of the angle formed by two branches of the skeleton intersects
with the outer contour of the connected component. This intersection point at the
outer contour is called a foreground feature point (denoted by in  Figure 5.8(d).

212
WORD AND STRING RECOGNITION
FIGURE 5.8
Foreground feature points’ extraction. (a) Original image; (b) skeleton after
thinning, starting point (S) and ending point (E) are denoted by ; (c) the skeleton is traversed
from left to right in two directions: clockwise and counterclockwise; (d) mapping intersection
points to the foreground features on the outer contour (denoted by ).
These feature points can be divided into two subsets: top-foreground features
corresponding to the top skeleton and bottom-foreground features corresponding
to the bottom skeleton. These feature points carry important information about the
location of touching strokes and segmentation regions.
For background feature extraction, ﬁrst the vertical top and bottom projection pro-
ﬁles of the connected component image are found, as seen in Figure 5.9 (b) and (c).
On thinning these proﬁle images (black regions), the skeletons, as seen in Figure 5.9
(e) and (f)), are called top-background skeleton and bottom-background skeleton,
respectively. The branch end points of these skeletons sometimes do not reach the
segmentation regions (an example is shown in Figure 5.10(a)). As shown in Fig-
ure 5.10(b), all parts of the top-background skeleton that are below the middle line
(the line divides the image by two equal halves) and all parts of bottom-background
skeleton above the middle line are removed. Then, the end points on the top- and
bottom-background skeletons (denoted by  in Figure 5.10(b)) are found for later
use in segmentation path construction (wherein the ﬁrst and last end points on each
skeleton are not considered).
5.2.2.2
Constructing Segmentation Paths
The feature points extracted
from foreground and background are divided into four subsets: top- and bottom-
FIGURE5.9
Backgroundskeleton extraction. (a) Preprocessed connected component image;
(b) top projection proﬁle; (c) bottom projection proﬁle; (d) background region (white pixels
outside the black object); (e) top-background skeleton; (f) bottom-background skeleton.

CHARACTER SEGMENTATION
213
FIGURE 5.10
Background feature points’ extraction. (a) Original top/bottom-background
skeletons; (b) after removing the parts crossing the middle line, the end points are background
features.
foreground features, and top- and bottom-background features. From these points,
two paths of search are conducted: upward search and downward search. If two fea-
ture points in two different subsets match each other, they are connected to construct
a segmentation path. Two feature points, A and B are said to match each other if
|x A −xB| ≤α · H,
α ∈[0.25, 0.5],
where xA and xB are the horizontal coordinates of A and B, respectively. The constant
α is taken to be 0.4, and H is the vertical height of the string image. The detailed pro-
cedure of segmentation path construction is depicted in the ﬂowchart of Figure 5.11,
and some examples are shown in Figure 5.12.
FIGURE 5.11
Flowchart of downward/upward search for constructing segmentation paths.

214
WORD AND STRING RECOGNITION
FIGURE 5.12
Feature points on the background and foreground (from the top and bottom)
are matched, and connected to construct segmentation paths.
5.3
CLASSIFICATION-BASED STRING RECOGNITION
We call a string recognition method based on explicit segmentation and character
recognition as classiﬁcation-based, as a character classiﬁer is underlying for verifying
and classifying the candidate character patterns hypothesized in presegmentation. It
is also called character-model-based or, in the case of word recognition, letter-model-
based. By the character classiﬁer, the candidate patterns are assigned conﬁdence
(probability or similarity/dissimilarity) scores to deﬁned classes (characters in an
alphabet), which are combined to measure the conﬁdence of segmentation hypothesis
and string class. A string class is a sequence of character classes assigned to the
segmented character patterns. The combination of a segmentation hypothesis (or
called segmentation candidate) and its string class is also called a segmentation-
recognition hypothesis (or segmentation-recognition candidate).
If segmentation is stressed, classiﬁcation-based string recognition is also called
classiﬁcation-based or recognition-based segmentation. On segmentation, the seg-
mented character patterns can be reclassiﬁed using a more accurate classiﬁer (some
classiﬁers are more suitable for classiﬁcation than for segmentation) or combining
multiple classiﬁers.
The ﬁnal goal of string recognition is to classify the string image (or online string
pattern) to a string class. Character segmentation, character recognition, and linguistic
processing can be formulated into a uniﬁed string classiﬁcation model. Under this
model, the optimal string class (associated with a segmentation hypothesis) of max-
imum conﬁdence is selected. How to determine the optimal string class involves
sophisticated operations of presegmentation, character classiﬁcation, linguistics rep-
resentation, path search, and so on. In the following, we ﬁrst describe the uniﬁed
string classiﬁcation model, and then go into some details of the operations.
5.3.1
String Classiﬁcation Model
A string image can be partitioned into character patterns in many ways, and each seg-
mented pattern can be assigned to different character classes. Figure 5.13 shows

CLASSIFICATION-BASED STRING RECOGNITION
215
FIGURE 5.13
A word image (left), its segmented pattern sequences (middle), and string
classes (right).
the segmentation of a simple word image into different pattern sequences (seg-
mentation hypotheses) and the string classes assigned to the pattern sequences.
The task of string recognition is to ﬁnd the optimal segmentation and its opti-
mal string class, incorporating character recognition scores, character likeliness
(geometric context), and optionally, linguistic context. To do this, the segmenta-
tion candidate and the string class should be ﬁrst evaluated by a string conﬁdence
measure.
Many works have tried to score segmentation candidate and string class prob-
abilistically [10, 60, 72, 88]. Viewing string segmentation-recognition as a string
classiﬁcation problem, that is, classifying the string image to one of all string classes,
according to the Bayes decision rule (see Chapter 4), the minimum classiﬁcation error
rate will be achieved by classifying the string image to the string class of maximum a
posteriori (MAP) probability. The computation of a posteriori probabilities (or other
types of scores) of string classes on a string image encounters two difﬁculties. First,
the number of string classes is very large. In lexicon-free recognition, the number
of string classes is as many as L
n=1 Mn (L is the maximum string length and M
is the number of character classes). Even in lexicon-driven recognition, it is hard to
compute the scores of all string classes in the lexicon. Second, for scoring a string
class, the string image needs to be segmented into characters, wherein segmentation
and character recognition interact. The number of segmentation candidates can be
fairly large.
5.3.1.1
Probabilistic Models
An early string class probability model can be
found in [10]. Assume that a string image X is segmented into n candidate patterns
S = s1, . . . , sn (note that there are many segmentation candidates even with the same
string length) and is assigned to a string class W = w1 · · · wn (character wi is assigned
to si), the a posteriori probability of the string class is
P(W|X) =

S
P(W, S|X).
(5.1)

216
WORD AND STRING RECOGNITION
The segmentation candidate is constrained to have the same length as W: |S| = |W| =
n, and the candidate patterns are represented by feature vectors1 X = x1, . . . , xn. To
avoid summing over multiple segmentation candidates in Eq. (5.1), the optimal string
class can instead be decided by
W∗= arg max
W max
S
P(W, S|X).
(5.2)
This is to search for the optimal segmentation candidate S for each string class.
P(W, S|X) is decomposed into
P(W, S|X) = P(X|W, S)P(W, S)
P(X)
= P(X|W, S)P(S|W)P(W)
P(X)
,
(5.3)
and by assuming context independence of character shapes, can be approximated as
P(W, S|X) ≈P(W)
n
$
i=1
p(xi|wi, si)P(si|wi)
p(xi)
= P(W)
n
$
i=1
P(wi, si|xi)
P(wi)
.
(5.4)
The a priori probability of string class P(W) represents the linguistic context. It is
assumed to be a constant for lexicon-free recognition. For lexicon-driven recognition,
P(W) can be assumed to be equal among the string classes in the lexicon. For an open
vocabulary, it is often approximated by second-order statistics (bigram):
P(W) = P(w1)
n
$
i=2
P(wi|wi−1).
(5.5)
P(wi, si|xi) in Eq. (5.4) can be decomposed into
P(wi, si|xi) = P(wi|si, xi)P(si|xi).
(5.6)
Breuel used a multilayer neural network to directly output the a posteriori probability
P(wi|xi) (approximating P(wi|si, xi)) but did not specify how to estimate P(si|xi).
We will see that P(si|xi) measures the plausibility of character in geometric context
(character likeliness considering the relative width, height, and position in string,
and the relationship with neighboring characters). Ignoring the geometric context,
P(si|xi) can be viewed as a constant, and the string class probability of Eq. (5.4) is
approximately
P(W, S|X) ≈P(W)
n
$
i=1
P(wi|si, xi)
P(wi)
= P(W)
n
$
i=1
P(wi|xi)
P(wi) .
(5.7)
1For describing character patterns in feature vectors, please refer to Chapter 3.

CLASSIFICATION-BASED STRING RECOGNITION
217
In the context of Japanese online handwriting recognition, Nakagawa et al. pro-
posed a string class probability model incorporating the geometry of intercharacter
gap [72]. The candidate pattern sequence is denoted by S = s1g1s2g2 · · · sngn, where
si represents the geometric features of the ith pattern, including the width and height
of bounding box, the within-character gaps, and so on, and gi represents the between-
character gap, and possibly, the relative size and position between characters. In the
formula of (5.3), P(X) is omitted because it is independent of string class. Hence,
the pattern sequence is classiﬁed to the string class of max P(X|W, S)P(S|W)P(W).
P(W) is estimated using a bigram as in Eq. (5.5). Assuming context-independent
character shapes, P(X|W, S) is approximated by
P(X|W, S) ≈
n
$
i=1
p(xi|wi, si) ≈
n
$
i=1
p(xi|wi),
(5.8)
where p(xi|wi) is the likelihood of pattern xi with respect to class wi, which will be
estimated by a character classiﬁer.
The likelihood of geometric context, P(S|W), plays an important role in string
segmentation. Different characters or characters versus noncharacters differ in the dis-
tribution of geometric features (size and position in the string, within-character gap),
and two patterns of classes wiwi+1 in the same string observe special distributions
of relative size/position and gap between them. These features can be incorporated
into the probability P(S|W) (character likeliness or compatibility score) to improve
string segmentation and recognition.
In [72], P(S|W) is estimated by
P(S|W) ≈
n
$
i=1
P(si|wi)P(gi|wiwi+1),
(5.9)
where P(si|wi) and P(gi|wiwi+1) can be seen as character likeliness and between-
character compatibility, respectively.
On extracting single-character and between-character geometric features, the
scores of likeliness and compatibility can be estimated by parametric density func-
tions or artiﬁcial neural networks, both trained with character samples segmented
from strings. The samples can be labeled as some superclasses instead of charac-
ter classes, such as character versus noncharacter (two classes), core (C), ascender
(A), and descender (D) (three classes [32]). The compatibility between two patterns
can similarly be labeled as “compatible” and “incompatible” (two classes) or the
combinations of character superclasses.
5.3.1.2
Simpliﬁed and Heuristic Models
The geometric context is often
ignored in integrated segmentation-recognition of strings. Rather, it is used in pre-
segmentation to preclude candidate character patterns, or in postprocessing to com-
pare a few selected segmentation candidates. Even without geometric context, if the
character classiﬁer is trained to be resistant to noncharacters (all deﬁned classes are

218
WORD AND STRING RECOGNITION
assigned low conﬁdence values on noncharacter patterns), it can still give high string
recognition accuracy [60]. Without geometric context score, the string class proba-
bility model is simpliﬁed to
P(W|X) = P(X|W)P(W)
P(X)
≈P(W)
n
$
i=1
p(xi|wi)
p(xi) .
(5.10)
The right-hand side of Eq. (5.10) is equivalent to that of Eq. (5.7). By omitting the
string-class-independent term P(X), the string pattern is classiﬁed to
W∗= arg max
W P(W)P(X|W) = arg max
W P(W)
n
$
i=1
p(xi|wi).
(5.11)
When P(W) is equal (lexicon-free or closed-lexicon-driven), the classiﬁcation rule is
further simpliﬁed to
W∗= arg max
W P(X|W) = arg max
W
n
$
i=1
p(xi|wi).
(5.12)
Care should be taken when using the rule of (5.12) for string recognition. A string
pattern can be segmented into character pattern sequences of variable lengths. For any
pattern xi and character class wi, the likelihood p(xi|wi) < 1 mostly, and the score
4n
i=1 p(xi|wi) decreases as the string length n increases. Thus, when comparing
the scores of string classes of different lengths, the scores of shorter strings will be
favored. This will raise the segmentation error of merging multiple characters into
one pattern. To overcome this bias, Tulyakov and Govindaraju proposed a normalized
string probability score:
W∗= arg max
W
 n
$
i=1
p(xi|wi)
 1
n .
(5.13)
To explain the poor performance of the simpliﬁed rules (5.11) and (5.12) in string
recognition, let us examine whether omitting P(X) is reasonable or not. When the
segmented pattern sequence x1 · · · xn is ﬁxed, P(X) = P(x1 · · · xn) is really string
class independent and the omission is reasonable. When comparing two string classes
W1 and W2 on two different sequences X1 and X2 (segmented from the same string
pattern), however, P(X1) and P(X2) are different and should not be omitted. On the
contrary, the decision rule of Eq. (5.13) performs fairly well in practice.
In contrast to Eq. (5.12), the bias to short strings does not happen to the right-hand
formula of (5.7). Though P(wi|xi) is proportional to p(xi|wi) (P(wi|xi) is obtained
by normalizing p(xi|wi)p(xi) to unity sum for M classes) and P(wi|xi) ≤1, the
denominator P(wi) prevents the product from shrinking with increased n. String
classiﬁcation according to (5.7), however, is sensitive to the precision of a posteriori

CLASSIFICATION-BASED STRING RECOGNITION
219
probabilities estimation, whereas the estimation of conditional probability density in
(5.13) can be circumvented by the proportional relationship between classiﬁer outputs
and log-likelihood [60] as follows.
For parametric statistical classiﬁers, such as linear discriminant function (LDF)
and quadratic discriminant function (QDF), the discriminant score of a class ωi equals
log P(ωi)p(x|ωi) + CI (CI is a class-independent term), and when assuming equal a
priori probabilities, is equivalent to log p(x|ωi) + CI . LDF and QDF are derived from
the logarithm of multivariate Gaussian density function, and the negative of QDF can
be viewed as a distance metric (a special case is the square Euclidean distance). For
neural network classiﬁers, considering the analogy of output layer to LDF, the output
score (weighted sum without nonlinear squashing) can be regarded to be proportional
to the log-likelihood:
yi(x) = y(x, ωi) ∝log p(x|ωi).
(5.14)
For distance-based classiﬁers, taking the analogy of distance metric with the negative
of QDF, the class distance can be regarded to be proportional to the minus log-
likelihood:
d(x, ωi) ∝−log p(x|ωi).
Viewing d(x, ωk) = −yk(x), the output scores of all classiﬁers can be related to
conditional probabilities as (5.14). The string classiﬁcation rule of (5.12) is then
equivalent to
W∗= arg max
W
n

i=1
y(xi, wi) = arg min
W
n

i=1
d(xi, wi),
and (5.13) is equivalent to
W∗= arg max
W
1
n
n

i=1
y(xi, wi) = arg min
W
1
n
n

i=1
d(xi, wi).
(5.15)
By this formulation, the classiﬁer outputs, regarded as generalized similarity or dis-
tance measures, can be directly used to score string classes.
The geometric context likelihood in (5.9) can be similarly converted to
log(P(S|W))
1
n = 1
n
n

i=1
[log P(si|wi) + log P(gi|wiwi+1)],
(5.16)
where log P(si|wi) and log P(gi|wiwi+1) are, respectively, proportional to y(si, wi)
and y(gi, wiwi+1), the output scores of classiﬁers on geometric features.

220
WORD AND STRING RECOGNITION
The transformed heuristic scores of character classiﬁcation, geometric context and
linguistic context can be combined to a normalized string class score as a weighted
sum:
NL(X, W) = 1
n
n

i=1
[y(xi, wi) + β1y(si, wi) + β2y(gi, wiwi+1) + γ log P(W)],
(5.17)
or equivalently, a distance measure of string class:
ND(X, W) = 1
n
n

i=1
[d(xi, wi) + β1d(si, wi) + β2d(gi, wiwi+1) −γ log P(W)].
(5.18)
Assuming equal a priori string class probabilities, the term log P(W) can be omit-
ted. The weights {β1, β2, γ} can be adjusted empirically for customizing to different
applications.
In(5.17)and(5.18),y(si, wi)andd(si, wi)measurethegeometriccontextplausibil-
ity of single characters, and y(gi, wiwi+1) and d(gi, wiwi+1) measure the plausibility
of pairs of characters, usually pairs of neighboring characters only.
Some researchers have utilized the geometric context in string recognition in dif-
ferent ways, but the obtained scores can be generally incorporated by the framework
of (5.17) or (5.18). Xue and Govindaraju compute for a character pattern the distance
between its geometric features and the class expectation [92]. Gader et al. train a neu-
ral network (called intercharacter compatibility network) to output the probabilities
of a number of superclasses on a pair of neighboring characters [32].
The segmentation statistics of Kim and Govindaraju [47] can be viewed as a
geometric context measure. Based on oversegmentation, the distribution of number of
primitive segments (upper bounded by a deﬁned maximum number) for each character
class is estimated on training string samples. In string recognition, the probability of
the number of segments of a candidate pattern for a character class is combined with
the classiﬁcation score for evaluating the string class.
5.3.2
Classiﬁer Design for String Recognition
In Eq. (5.17) or (5.18), y(xi, wi) or d(xi, wi) is given by a character classiﬁer, to
represent the measure of similarity or distance of the candidate character pattern
(represented by a feature vector xi) to a character class wi. The classiﬁer structure
can be selected from those described in Chapter 4 and the HMM. On estimating
the classiﬁer parameters on a set of character samples (character-level training), the
classiﬁer can then be inserted into the string recognition system to perform candidate
pattern classiﬁcation.
A classiﬁer trained on segmented character samples does not necessarily per-
form well in string recognition. This is because the candidate patterns gener-
ated in pre-segmentation include both characters and noncharacters. The charac-
ter class scores on noncharacter patterns are somewhat arbitrary and can mislead

CLASSIFICATION-BASED STRING RECOGNITION
221
the comparison of segmentation candidates: It is probable that a segmentation with
noncharacters has a higher conﬁdence than the one composed of correctly seg-
mented characters. This can be overcome by two ways: enhancing the noncharac-
ter resistance in character-level training or training the classiﬁer directly on string
samples.
5.3.2.1
Character-Level Training
Most classiﬁers are trained to separate the
patterns of M deﬁned classes regardless of outliers (patterns that do not belong to
the deﬁned classes). Typically, a classiﬁer partitions the feature space into disjoint
decision regions corresponding to different classes. If the whole feature space is
partitioned into regions of the deﬁned classes, an outlier falling in a region and distant
from decision boundaries will be classiﬁed to the corresponding class with high
conﬁdence.
Rejection has been considered in pattern recognition: When it is not sure enough
to which class the input pattern belongs, the pattern is rejected. There are two types
of rejects: ambiguity reject and distance reject [25]. A pattern is ambiguous when its
membership degrees to multiple classes are comparable. Normalizing the a posteriori
probabilities of M deﬁned classes to unity sum, ambiguity can be judged when the
maximum a posteriori probability is below a threshold [22]. Distance reject is the
case that the input pattern does not belong to any of the deﬁned classes; that is, it
is an outlier. It can be judged by the mixture probability density or the maximum
conditional probability density below a threshold.
In character-level classiﬁer training, noncharacter resistance can be obtained
by three methods: density-based classiﬁer design, hybrid generative–discriminative
training, and training with noncharacter samples [11, 60]. In the following, we brieﬂy
discuss the mathematical consideration of outlier rejection, and then describe the
classiﬁer design methods.
Consider classiﬁcation in a feature space, the patterns of M deﬁned classes inhabit
only a portion of the space, and the complement is inhabited by outliers, which we
include into an “outlier” class ω0. The closed world assumption that the a posteriori
probabilities sum up to unity is now replaced by an open class world:
M

i=0
P(ωi|x) = 1.
Apparently, M
i=1 P(ωi|x) ≤1, and the complement is the probability of outlier. The
a posteriori probabilities are computed according to the Bayes formula:
P(ωi|x) = P(ωi)p(x|ωi)
p(x)
=
P(ωi)p(x|ωi)
M
j=0 P(ωj)p(x|ωj)
.
As p(x|ω0) is hard to estimate due to the absence or insufﬁciency of outlier samples,
outlier can be judged according to M
i=1 P(ωi)p(x|ωi) or maxM
i p(x|ωi) below a
threshold.

222
WORD AND STRING RECOGNITION
When using the string class score of (5.7) or (5.13) for string recognition, the a
posteriori probability P(ωi|x) and the conditional probability p(x|ωi) are required to
be estimated in the open class world.
Assume that the regions of M-class patterns and outliers are disjoint or overlap
slightly in the feature space, classiﬁers that approximate the class-conditional proba-
bility density or the log-likelihood are inherently resistant to outliers, because in the
region of outliers, the density of all the deﬁned classes is low. The density functions of
deﬁned classes can be estimated from training samples by parametric methods (like
the Gaussian density), semiparameter methods (like mixture of Gaussians), or non-
parametric methods (like the Parzen window). In character recognition, the QDF and
the modiﬁed QDF (MQDF) [50], derived from multivariate Gaussian density, are both
accurate in classiﬁcation and resistant to outliers, and have shown good performance
in string segmentation and recognition [48, 49, 81].
Probability density functions can be viewed as generative models, which include
HMMs and structural descriptions as well. In contrast, discriminative classiﬁers, like
neural networks and support vector machines (SVMs), have discriminant functions
connected to the a posteriori probabilities more than to the probability density. The
parameters of discriminant functions are adjusted such that the training samples of
different classes are maximally separated without regarding the boundary between
in-class patterns and outliers. For obtaining outlier resistance, the classiﬁer can be
trained discriminatively with outlier (noncharacter) samples. Noncharacter samples
can be collected from the candidate patterns generated in presegmentation of string
images.
In training with outlier samples, the classiﬁer can have either M or M + 1 output
units (discriminant functions), corresponding to M-class or (M + 1)-class classiﬁca-
tion. When outliers are treated in an extra class, the classiﬁer is trained in the same
way as an M-class classiﬁer is trained. After training the (M + 1)-class classiﬁer,
the class output of outliers is abandoned in string recognition. That is to say, only
the output scores of M deﬁned classes (have been trained to be low in the region of
outliers) are used in scoring string classes.
For training an M-class classiﬁer with outlier samples, if the classiﬁer is a neural
network with weights estimated under the mean squared error (MSE) or cross-entropy
(CE) criterion, the target values for M classes are all set to zero on an outlier sample,
unlike that on an in-class sample where one of target values is set to 1. For a one-
versus-all support vector classiﬁer, the outlier samples are added to the negative
samples of each class such that each class is trained to be separated from outliers.
For classiﬁers trained under the minimum classiﬁcation error (MCE) criterion [43,
44], a threshold is set to be the discriminant function of the “outlier” class and is
adjusted together with the classiﬁer parameters [60]. The threshold is abandoned after
training.
In a hybrid generative–discriminative classiﬁer, the discriminant functions are
tied with probability density functions or structural descriptions. The parameters
are initially estimated to optimize a maximum likelihood (ML)-like criterion, and
then adjusted discriminatively to further improve the classiﬁcation accuracy. Two
classiﬁer structures of this kind have been tested in [60]: regularized learning vector

CLASSIFICATION-BASED STRING RECOGNITION
223
quantization (LVQ) and regularized discriminative learning QDF (DLQDF) [59]. The
general learning method is as follows.
The class output (discriminant score) of either LVQ or DLQDF classiﬁer is a dis-
tance measure. The class prototypes of LVQ classiﬁer and the class parameters (mean,
eigenvalues, and eigenvectors) of DLQDF classiﬁer are initialized by ML estimation.
In MCE training [43, 44], the misclassiﬁcation loss on a sample is computed based on
the difference of discriminant score between the genuine class and competing classes,
and is transformed to soft (sigmoid) 0–1 loss lc(x). The empirical loss on the training
sample set is regularized to
L1 = 1
N
N

n=1
[lc(xn) + αd(xn, ωc)],
(5.19)
where N is the number of training samples and ωc denotes the genuine class of xn. The
regularization term αd(xn, ωc) is effective to attract the classiﬁer parameters to the
ML estimate and hence helps preserve the resistance to outliers. The coefﬁcient α is
empirically set to compromise between classiﬁcation accuracy and outlier resistance.
The classiﬁer parameters are updated by stochastic gradient descent to optimize the
regularized MCE criterion (5.19).
5.3.2.2
String-Level Training
String-level training aims to optimize the
performance of string recognition. A performance criterion involving character classi-
ﬁcation scores is optimized on a string sample set, wherein the classiﬁer parameters
are adjusted. Among various criteria, the MCE criterion of Juang et al. [43, 44], which
was originally proposed in speech recognition [21, 46], is popularly used. The MCE
criterion applies to arbitrary forms of discriminant functions, unlike the maximum
mutual information (MMI) criterion (which is usually applied to HMM models, e.g.,
[87]) that requires the class probability estimates. The application of MCE-based
string-level training to handwriting recognition has reported superior performance
[7, 8, 16, 57].
Classiﬁcation-based string recognition involves two levels of classiﬁers, namely
a character classiﬁer and a string classiﬁer. The string classiﬁer combines character
classiﬁcation scores to form string class scores and computes explicitly the scores of a
small portion of string classes only. Assume that a character pattern (represented by a
feature vector x) is classiﬁed into M deﬁned classes {ω1, . . . , ωM}, the string class of
a sequence of candidate patterns X = x1 · · · xn is denoted by a sequence of character
classes W = w1 · · · wn = ωi1 · · · ωin. The parameters  of the embedded character
classiﬁer include a subset of shared parameters θ0 and M subsets of class-speciﬁc
parameters θi, i = 1, . . . , M. The classiﬁer assigns similarity/dissimilarity scores to
the deﬁned classes:
yi(x) = −d(x, ωi) = f(x, θ0, θi).
In string-level training, the classiﬁer parameters are estimated on a data set of string
samples DX = {Xn, Wn)|n = 1, . . . , NX} (Wn denotes the string class of sample Xn)

224
WORD AND STRING RECOGNITION
by optimizing an objective function, for example, the log-likelihood:
max LLX =
NX

n=1
log p(Xn|Wn).
In ML estimation of classiﬁer parameters, the string samples are dynamically seg-
mented into character patterns Xn = xn
1 · · · xn
m, which best match the ground-truth
string class Wn = wn
1 · · · wn
m on the current parameters. The parameters are iteratively
updated on segmented characters with the aim of maximizing the log-likelihood, in
a way similar to expectation maximization (EM) [23].
In discriminative string-level training, the string sample is matched with the gen-
uine string class and a number of competing classes. The contrast between the scores
of genuine string and competing strings corresponds to the string recognition error.
The MCE method transforms this contrast of scores into a probabilistic loss and
updates the classiﬁer parameters with the aim of minimizing the empirical string
recognition error.
The MCE criterion can be applied to either character-level training or string-level
training. Assume to classify an object X into M classes {C1, . . . , CM} (regardless
of character or string classes), each class has a discriminant function (class score)
gi(X, ). A sample set DX = {Xn, cn)|n = 1, . . . , NX} (cn denotes the class label of
sample Xn) is used to estimate the parameters .
Following Juang et al. [43], the misclassiﬁcation measure on a pattern from class
Cc is deﬁned by
hc(X, ) = −gc(X, ) + log
⎡
⎣
1
M −1

i̸=c
eηgi(X,)
⎤
⎦
1/η
,
(5.20)
where η is a positive number. When η →∞, the misclassiﬁcation measure becomes
hc(X, ) = −gc(X, ) + gr(X, ),
(5.21)
where gr(X, ) is the discriminant score of the closest rival class:
gr(X, ) = max
i̸=c gi(X, ).
This simpliﬁcation signiﬁcantly saves the computation of training by stochastic gra-
dient descent (also called generalized probabilistic descent (GPD) [46]), where only
the parameters involved in the loss function are updated on a training pattern.
The misclassiﬁcation measure is transformed to loss function by
lc(X, ) = lc(hc) =
1
1 + e−ξhc ,
(5.22)

CLASSIFICATION-BASED STRING RECOGNITION
225
where ξ is a parameter that controls the hardness of sigmoid nonlinearity. On the
training sample set, the empirical loss is computed by
L0 =
1
NX
NX

n=1
M

i=1
li(Xn, )I(Xn ∈Ci)
=
1
NX
NX

n=1
lc(Xn, ),
(5.23)
where I(·) is the indicator function.
In minimizing L0 by stochastic gradient descent, the training patterns are fed into
the classiﬁer repeatedly. On a training pattern, the classiﬁer parameters are updated
by
(t + 1) = (t) −ϵ(t)U∇lc(X, )|=(t),
(5.24)
where U is a positive deﬁnite matrix and ϵ(t) is the learning step. U is related to
the inverse of Hessian matrix and is usually approximated to a diagonal matrix. In
this case, the diagonal elements U are absorbed into the learning step, which is thus
parameter dependent. The parameters converge to a local minimum of L0 (where
∇lc(X, ) = 0) under the following conditions:
⎧
⎪
⎨
⎪
⎩
limt→∞ϵ(t) = 0,
∞
t=1 ϵ(t)
= ∞,
∞
t=1 ϵ2(t)
< ∞.
In practice, the parameters are updated in ﬁnite iterations, and setting the learning
rate as a gradually vanishing sequence leads to convergence approximately.
To specify the MCE training method to string-level classiﬁer training, we use
the string class score of Eq. (5.15). If geometric context and linguistic context are
incorporated in training, then the score of (5.17) or (5.18) is used instead.
In MCE training, each training sample is segmented by candidate pattern classi-
ﬁcation and path search using the current classiﬁer parameters (t). By path search,
we obtain a number of string classes matched with the string sample with high scores,
each corresponding to a segmented pattern sequence. The search techniques will be
described in the next subsection. Denote the genuine string class by Wc = ωi1 . . . ωin
andtheclosestcompetingstringbyWr = ωj1 . . . ωjm,correspondingtotwosequences
of segmented patterns xc
1 . . . xc
n and xr
1 . . . xr
m, respectively. The scores of two string
classes are computed by
NL(X, Wc) = 1
n
n

k=1
y(xc
k, ωik)

226
WORD AND STRING RECOGNITION
and
NL(X, Wr) = 1
m
m

k=1
y(xr
k, ωjk),
respectively. The misclassiﬁcation measure is then
hc(X, ) = 1
m
m

k=1
y(xr
k, ωjk) −1
n
n

k=1
y(xc
k, ωik),
(5.25)
which is transformed to loss by (5.22).
By stochastic gradient descent, the classiﬁer parameters are updated on a string
sample at time step t by
(t + 1) = (t) −ϵ(t)∂lc(X, )
∂
555
=(t)
= (t) −ϵ(t)ξlc(1 −lc)∂hc(X, )
∂
555
=(t)
= (t) −ϵ(t)ξlc(1 −lc)
 1
m
m

k=1
∂y(xr
k, ωjk)
∂
−1
n
n

k=1
∂y(xc
k, ωik)
∂
555
=(t).
(5.26)
In practice, the pattern sequence of genuine string and the sequence of competing
string have many shared patterns. This can largely simplify the parameter updating
of (5.26).
We partition the candidate patterns in two sequences (corresponding to Wc and
Wr) into three disjoint subsets: Pc ∪Pr ∪Ps. Ps contains the patterns shared by the
two string classes, Pc contains the patterns of Wc that are not shared, and Pr contains
the patterns of Wr that are not shared. Ps is further divided into two subsets: Ps =
Ps1 ∪Ps2. The patterns in Ps1 are assigned different character classes, whereas the
patterns in Ps2 are assigned the same class in the two string classes. The partial
derivative of the second line of (5.26) is speciﬁed to
∂hc(X, )
∂
= 1
m

xr
k∈Pr
∂y(xr
k, ωjk)
∂
−1
n

xc
k∈Pc
∂y(xc
k, ωik)
∂
+ 
xk∈Ps1
 1
m
∂y(xk, ωjk)
∂
−1
n
∂y(xk, ωik)
∂

+
 1
m −1
n
 
xk∈Ps2
∂y(xk, ωijk)
∂
,
(5.27)
where ωijk denotes the common class of a shared pattern in Ps2. When n = m, we
can see that the partial derivative on a pattern in Ps2 vanishes, and consequently, the
classiﬁer parameters are not updated on such patterns.

CLASSIFICATION-BASED STRING RECOGNITION
227
Thelearningprocedureiscustomizedtovariousclassiﬁerstructuresbyspecializing
∂y(x,ωi)
∂
. By partitioning the classiﬁer parameters into shared parameters θ0 and class-
speciﬁc parameters θi, i = 1, . . . , M, the partial derivative of a class output yi(x) or
d(x, ωi) is computed only with respect to θ0 and θi.
The above learning algorithm was applied to lexicon-free numeral string recog-
nition [57], but is applicable to word recognition as well. If a closed lexicon is used
in training, the competing string class on a string sample is selected from the in-
lexicon entries, and the classiﬁer parameters are adjusted to optimize the accuracy
of in-lexicon string classiﬁcation. For an open vocabulary, the a priori probability
of string class from the n-gram model is incorporated into the string class score in
matching the string image with genuine string class and competing classes.
For lexicon-driven word recognition, Chen and Gader [16] and Biem [7, 8] have
used different string class scoring functions in the same MCE training framework
and have investigated the effect of lexicon on training. Lexicon-driven training may
sufferfromthesparsityandimbalance(bothimbalancedstringclassesandimbalanced
character classes) of string samples and generalize poorly to off-lexicon string classes.
Chen and Gader generated an artiﬁcial lexicon for training by randomly replacing
characters in the lexicon entries. The experimental results of Biem show that lexicon-
free training yields higher generalized word recognition accuracy than lexicon-driven
training.
5.3.3
Search Strategies
On deﬁning a matching score between the string image and string classes, the task
of string recognition is to search for the optimal string class of maximum score or
minimum cost (distance). The string image can be segmented into many candidate
pattern sequences, and each can be assigned different string classes. The exhaustive
search strategy that computes the scores of all segmentation-recognition candidates
and then selects the optimal is computationally expensive.
Heuristic search algorithms that evaluate only a portion of segmentation-
recognition candidates have been commonly used in string recognition. According
to the manner of incorporating lexicon, the search techniques can be divided into
two groups: matching string image with one lexicon entry and matching with all
string classes simultaneously. We will describe two important search algorithms,
dynamic programing and beam search, for accomplishing the two kinds of matching,
respectively.
5.3.3.1
Segmentation-Recognition Candidates’ Representation
The
candidates of string segmentation can be represented by a hypothesis network, called
segmentation candidate lattice [71]. An example is shown in Figure 5.14. Each node
in the lattice corresponds to a separation point. One or more consecutive segments
form candidate patterns, which are denoted by edges. All the candidate patterns can
be imposed constraints like the maximum number of segments, least and maximum
width, height, vertical position in string image, and so on. The candidate patterns that
do not meet the constraints are abandoned before being recognized. The maximum

228
WORD AND STRING RECOGNITION
FIGURE 5.14
Segmentation candidate lattice of a word image. The optimal segmentation
candidate is denoted by thick lines.
number of segments is usually set to a constant, typically from 3 to 5, but a variable
number speciﬁc to character class is beneﬁcial [47].
Each candidate pattern is recognized by a character classiﬁer and assigned scores
to (all or selected) character classes. On pairing a candidate pattern with character
classes, the corresponding edge in the segmentation lattice is expanded into mul-
tiple edges, each corresponding to a pattern-class pair. The expanded candidate
lattice is called a segmentation-recognition lattice (see Fig. 5.15). In this lattice,
each path (segmentation-recognition path or segmentation-recognition candidate)
from the leftmost node to the rightmost node denotes a sequence of pattern-
class pairs ((x1, w1), . . . , (xn, wn)) or a pair of pattern sequence and string class
((x1 · · · xn), (w1 · · · wn)) = (X, W).
The segmentation-recognition lattice can be formed either before string recogni-
tion or dynamically during string recognition. In the former way, the string image is
oversegmented and candidate patterns are formed and recognized to assign charac-
ter classes without considering linguistic context. In lexicon-driven recognition, the
candidate patterns and their corresponding classes are dependent on the context. For
example, if a candidate pattern xi has been assigned to a character class wi and the
character following wi is constrained to be from two classes {vi1, vi2}, the candidate
pattern succeeding xi is then only paired with two class vi1 and vi2 and is subject to
geometric constraints speciﬁc to class vi1 or vi2.
FIGURE 5.15
Segmentation-recognition candidate lattice of the word image of Figure 5.13.
Each edge denotes a candidate pattern and its character class.

CLASSIFICATION-BASED STRING RECOGNITION
229
The segmentation lattice can be stored in a two-dimensional array S(j, k), with
j denoting the index of node (separation point) at the right boundary of candidate
pattern and k denoting the number of primitive segments in the candidate pattern.
Assume that the string image is oversegmented into n primitive segments t1 · · · tn,
then the element S(j, k) denotes a candidate pattern composed of k primitive segments
⟨tj−k+1 · · · tj⟩.
The segmentation-recognition lattice inherits the array S(j, k) of segmentation
lattice, and each element is enriched with a list of character classes and their scores:
S(j, k, c), c ∈{ω1, . . . , ωM}. Hence, the segmentation-recognition lattice is stored in
a three-dimensional array S(j, k, c).
For describing the search algorithms, we assume that a segmentation-recognition
path is measured by a distance (cost of path) like that in Eq. (5.18), between a se-
quence of primitive segments X = t1 · · · tn and a string class W = w1 · · · wm. We
re-formulate the normalized distance as
ND(X, W) = 1
m
m

i=1
[αd1(si, wi) + βd2(si, wi−1wi)],
(5.28)
where si denotes a candidate pattern ⟨tei−ki+1 · · · tei⟩. d1(si, wi) covers the character
classiﬁcation score and single-character geometric plausibility, and d2(si, wi−1wi)
covers the character-pair geometric compatibility and linguistic bigram. A partial
segmentation-recognition path is scored by a partial cost (i < m)
ND(s1 · · · si, w1 · · · wi) = 1
i
i

j=1
[αd1(sj, wj) + βd2(sj, wj−1wj)]
= 1
i D(s1 · · · si, w1 · · · wi),
(5.29)
where D(s1 · · · si, w1 · · · wi) denotes the accumulated cost, which can be reformulated
as
D(s1 · · · si, w1 · · · wi) = i
j=1[αd1(sj, wj) + βd2(sj, wj−1wj)]
= D(s1 · · · si−1, w1 · · · wi−1) + dc(si, wi),
(5.30)
where dc(si, wi) = αd1(si, wi) + βd2(si, wi−1wi) is the added cost when matching
a new pattern si with a character wi. The partial cost is normalized with respect
to the substring length i because the true string length m is unknown before string
recognition is completed.
5.3.3.2 Matching with One Lexicon Entry
When the number of string classes
(words or phrases) is not very large (say, less than 1000), string recognition can be
performed by matching the string image with each lexicon entry one by one. To
match with a string class, the string image is segmented into character patterns that
best match the characters. The match between the string image (represented as a

230
WORD AND STRING RECOGNITION
sequence of primitive segments) and a string class is similar to the dynamic time
warping (DTW) in speech recognition and is commonly solved by DP.
For matching with a given string class, as the string length m is known, the
segmentation-recognition path and partial paths can be evaluated using the accu-
mulated cost. Denote the number of primitive segments in candidate pattern si as ki,
si = ⟨tei−ki+1 · · · tei⟩, then s1 · · · si = t1 · · · tei, and the accumulated partial cost is
D(t1 · · · tei, w1 · · · wi) = D(t1 · · · tei−ki, w1 · · · wi−1)
+dc(⟨tei−ki+1 · · · tei⟩, wi).
(5.31)
Thus, the full path cost D(t1 · · · tn, w1 · · · wm) can be minimized stagewise: the
optimal sequence of candidate patterns ending with si = ⟨tei−ki+1 · · · tei⟩corre-
sponding to wi, arg mine1···ei−1 D(t1 · · · tei, w1 · · · wi), comprises a subsequence of
arg mine1···ei−2 D(t1 · · · tei−ki, w1 · · · wi−1).
To search for the optimal match of mine1,...,em−1 D(t1 · · · tn, w1 · · · wm) by DP, an
array D(j, i) is used to record the minimum partial cost up to a primitive segment tj:
mine1,...,ei−1 D(t1 · · · tj, w1 · · · wi), and a backpointer array T(j, i) records the index
of primitive segment j −ki that the candidate pattern si−1 ends with:
D(j, i) = min
ki [D(j −ki, i −1) + dc(⟨tj−ki+1 · · · tj⟩, wi)].
(5.32)
Based on the above formulations, the DP algorithm for matching a string image
with a string class is as below.
Algorithm: DP for matching with a string class
1. Initialization. Set D(0, 0) = 0, D(j, 0) = ∞and D(0, i) = ∞, j = 1, . . . , n,
i = 1, . . . , m.
2. Forward updating. For j = 1, . . . , n, i = 1, . . . , m, update D(j, i) by
Eq. (5.32), denote the resulting number of primitive segments as arg minki,
and update T(j, i) = j −arg minki.
3. Backtracking. The sequence of primitive segments is partitioned into char-
acters at the separation points J1 · · · Jm−1, where Jm−1 = T(n, m), Ji−1 =
T(Ji, i), i = m, . . . , 2.
Figure 5.16 shows an example of DP matching: the string image of Figure 5.14 is
matched with two word classes separately. The search space is represented as a two-
dimensional array D(j, i), and the optimal segmentation is deﬁned by the separation
points J0J1 · · · Jm−1Jm (J0 = 0, Jm = n) and is denoted by the diagonal lines in the
array.
In string matching by the above DP algorithm, the primitive segments of string
image are forced to match with the characters, so the matching path corresponds to
diagonal lines in the array of search space. If, more generally, not only primitive seg-
ments merging but also segment deletion (character insertion) and segment insertion
(character deletion) are considered in matching, the partial cost of (5.32) is minimized

CLASSIFICATION-BASED STRING RECOGNITION
231
FIGURE 5.16
Match the string image of Figure 5.14 with two word classes. Upper: matching
path in search space (array); lower: segmented character patterns.
with respect to operations of deleting segment tj and deleting character wi as well
as merging ki segments, and the matching path comprises of vertical and horizontal
lines as well as diagonal lines.
5.3.3.3
Matching with All String Classes
In string recognition with large
vocabulary, as the objective is to search for only one or a few string classes with
maximum scores, many computation efforts on the low-score string classes can be
saved. To do this, all the string classes are considered simultaneously in matching
with the string image and those classes that are unlikely to give the maximum score
are abandoned before the matching is completed.
Inlexicon-freerecognition,eachcandidatepatternsi = ⟨tei−ki+1 · · · tei⟩isassigned
to all character classes {ω1, . . . , ωM}. On rejecting some characters according to the
classiﬁcation scores, si is paired with a subset of characters, and the corresponding
edge in the segmentation lattice is expanded into multiple segmentation-recognition
edges accordingly. For lexicon-driven recognition, the lexicon is often represented in
a tree structure called trie (to be described in the next subsection). When a candidate
pattern ending with segment tj is paired with a character wi, a candidate pattern
starting from segment tj+1 will be paired with the characters that are the offspring of
wi in the lexicon tree.
A segmentation-recognition solution is a pair of candidate pattern sequence
s1 · · · sm (partitioned by separation points e1 · · · em−1) and its corresponding string
class W = w1 · · · wm. A partial solution, corresponding to a partial path in the
segmentation-recognition lattice, is a pair of pattern sequence s1 · · · si and a substring
w1 · · · wi. To avoid evaluating all the full paths, the optimal full path is searched
heuristically via selectively extending partial paths.
A partial segmentation-recognition path (s1 · · · si, w1 · · · wi) is viewed as a state
in a search space and is represented as a node in a tree. The root of the search tree
denotes an empty match, an intermediate node corresponds to a partial match, and a
terminal node gives a segmentation-recognition result. Each edge in the segmentation-
recognition lattice is converted to one or more nodes in the search tree. Speciﬁcally, an
edge S(j, k, c), which denotes a candidate pattern ⟨tj−k+1 · · · tj⟩paired with character

232
WORD AND STRING RECOGNITION
FIGURE 5.17
Character-synchronous (left) and frame-synchronous (right) node generation
in search tree. The nodes in a column are generated simultaneously.
c, is converted to a search node SN(j, k, c, p), where p denotes the pointer to the parent
node of SN.
Search algorithms vary in the order of node generation. In character-synchronous
search, the offspring of a node, which have the same depth, are generated simultane-
ously. The candidate patterns of these nodes start from the same primitive segment
but may end with different primitive segments. In frame-synchronous search, the
nodes whose candidate patterns end with the same primitive segment are generated
simultaneously, and each node is linked to the parent node whose candidate pattern
immediately precedes the pattern of the new node. If there are multiple such parent
nodes, the new node is generated with multiple copies, each linked to a respective
parent node. Figure 5.17 shows the order of node generation for the segmentation-
recognition lattice in Figure 5.15.
In either character-synchronous or frame-synchronous search, the nodes are gen-
erated by pairing a candidate pattern with character classes via classiﬁcation and
rejection. On computing the class similarity measure y(x, ωi) or distance d(x, ωi),
some classes can be rejected by setting an abstract threshold T1 and a relative thresh-
old T2 [58]. Speciﬁcally, if y(x, ωi) < T1 or d(x, ωi) > T1, ωi is rejected; denote the
top-rank class as ωm, if y(x, ωm) −y(x, ωi) > T2 or d(x, ωi) −d(x, ωm) > T2, and
ωi is rejected. 2
In matching with multiple string classes, as the length of the optimal string class is
unknownapriori,thesearchnodes(correspondingtopartialsegmentation-recognition
paths) should be evaluated using a normalized score like that in Eq. (5.29). The partial
cost is now nonmonotonic in the sense that the cost of a longer path may be smaller
than the cost of its partial path.
In the following, we ﬁrst describe the algorithms for character-synchronous search,
including best-ﬁrst search [9, 15] and beam search [56, 66, 78], and then a beam search
algorithm for frame-synchronous search [60].
Incharacter-synchronoussearch,theoperationofgeneratingtheoffspringofanode
in the search tree is called expansion. The search algorithm is variable depending on
2These simple heuristic rejection rules perform fairly well. More accurate rejection can be achieved by
estimating probabilistic conﬁdence precisely.

CLASSIFICATION-BASED STRING RECOGNITION
233
the order of node expansion [74, 91]. By breadth-ﬁrst search, the nodes of a same
depth are expanded synchronously, whereas by depth-ﬁrst search, the deepest node
is expanded ﬁrst. By best-ﬁrst search, the node of minimum partial cost is expanded
recursively until the minimum cost node is a terminal node. Best-ﬁrst search is still
not efﬁcient enough in the sense that the optimal terminal node cannot be found
before many intermediate nodes are expanded. The heuristic search, A* algorithm,
accelerates best-ﬁrst search by adding an underestimate of remaining path cost. The
remaining cost, however, is not trivial to estimate.
In the case of nonmonotonic path cost, the ﬁrst found terminal node does not
necessarily give an optimal string match though its cost is minimum compared to
the nonterminal nodes because the nonterminal nodes can achieve even lower cost
after expansion. For better solution, node expansion should continue until multiple
terminal nodes are found. Because many nonterminal nodes have comparable partial
costs, a large number of nodes are expanded before terminal nodes are reached.
A good trade-off between the optimality of solution and the efﬁciency of search
can be achieved using a beam search strategy: It improves the optimality by expanding
multiple nodes each time and saves computation by discarding intermediate nodes
of low scores. For character-synchronous search, the beam search expands nodes in
breadth-ﬁrst manner: selected nodes of the same depth (corresponding to substrings
of the same length) are expanded synchronously. To describe the search algorithm
formally, we ﬁrst formulate the operation of node expansion.
DenoteanodeinthesearchtreeasaquadrupleSN(j, ki, wi, pi),whichcorresponds
to a sequence of i candidate patterns with the last one si = ⟨tj−ki+1 · · · tj⟩paired
with a character wi. Each node is evaluated by a normalized partial path cost as
in (5.29). To expand SN(j, ki, wi, pi), the primitive segments starting with tj+1 are
merged into candidate patterns, each paired with character classes following wi (either
lexicon-free or lexicon-driven). Each pattern-character pair gives a new node SN
(j + ki+1, ki+1, wi+1, pi+1), where pi+1 links to its parent node SN(j, ki, wi, pi).
When j + ki+1 = n (n is the total number of primitive segments), the new node is
a terminal node. In this case, if wi+1 is also the last character of a string class, the
terminal node gives a complete string match; otherwise, the string match is incomplete
and abandoned.
The root node of the search tree is SN(0, 0, 0, 0), with accumulated path cost
being zero. When a node SN(j, ki, wi, pi) is expanded to generate a new node
SN(j + ki+1, ki+1, wi+1, pi+1), the accumulated path cost of the new node is up-
dated according to (5.30) and is normalized with respect to the substring length i + 1.
As SN corresponds to a partial segmentation-recognition path, the accumulated cost
and normalized cost are also referred to as D(SN) and ND(SN), respectively.
The search nodes are stored in two lists. All the newly generated nodes (not yet
expanded) are stored in OPEN, and those expanded are stored in CLOSED. Initially,
the OPEN list contains the root node and the CLOSED list is empty. The number of
nodes in OPEN is upper bounded by a number BN. If there are more than BN new
nodes, the nodes with high ND(SN) are abandoned. The nodes in OPEN can be further
prunedbycomparingND(SN)withanabstractthresholdandcomparingthedifference
of ND(SN) from the minimum with a relative threshold, as for character rejection in

234
WORD AND STRING RECOGNITION
candidate pattern classiﬁcation. The nodes in OPEN have the same depth and are
expanded synchronously to generate new OPEN nodes. Node expansion proceeds
until one or more terminal nodes are found or there is no OPEN nodes to expand. The
search algorithm is described below.
Algorithm: Character-synchronous beam search
1. Initialization. Store root node SN(0, 0, 0, 0) in OPEN.
2. Node expansion. Move all the OPEN nodes into CLOSED and expand them
to generate at most BN new OPEN nodes. Repeat until one or more terminal
nodes are found or there is no OPEN node to expand.
3. Backtracking. Corresponding to a terminal node SN(n, km, wm, pm) (match
with a string of m characters), the ending segment indices e1 · · · em−1 that
partition the string image into m character patterns and the character classes are
backtracked from the parent node pointers: ei−1 = ji−1 and wi−1 are deﬁned
by the parent node of SN(ji, ki, wi, pi).
Node expansion is not available in frame-synchronous search because the offspring
of a node are not generated synchronously, but rather the nodes with corresponding
candidate patterns ending with the same primitive segment tj are generated simulta-
neously. The nodes are not differentiated into OPEN and CLOSED, but stored in a
single list. Formally, for j = 1, . . . , n, each candidate pattern ending with tj, referred
to as S(j, k) in segmentation lattice, is linked to all the existing nodes (which serve the
parents of new nodes) with candidate patterns ending with tj−k. The candidate pattern
S(j, k) is paired with some character classes (either lexicon-free or lexicon-driven),
and each pattern-character pair is linked to a parent to generate a new node. By beam
search, all the new nodes at j are sorted to store at most BN of them. The nodes
generated at j = n are terminal nodes and those of complete match are backtracked
to give segmentation-recognition results.
5.3.4
Strategies for Large Vocabulary
The number of string classes in string recognition is often very large. In English word
recognition, for example, the number of daily used words is tens of thousands. In a
Japanese address phrase recognition system, the number of phrases considered is as
many as 111,349 [56]. Obviously, matching a string image with a large number of
string classes one by one is computationally expensive, and the recognition speed is
not acceptable in practical applications. To speed up string recognition, the problem
of large lexicon can be dealt with in two respects: lexicon reduction and structured
lexicon organization.
5.3.4.1 Lexicon Reduction
Lexicon reduction refers to the selection of a small-
size subset from a large number of string classes. The string image to recognize is
then matched only with the string classes in the subset, such that the computation of
matching with the other string classes is saved. Various lexicon reduction techniques

CLASSIFICATION-BASED STRING RECOGNITION
235
have been reviewed in [51, 89]. The techniques can be roughly grouped into three
categories: context knowledge, string length estimation, and shape feature matching.
We brieﬂy outline these techniques, whereas their details can be found in the literature.
For speciﬁc applications, string recognition is often considered in a dynamic
small-size lexicon deﬁned by related knowledge source. For example, in postal mail
address reading, the lexicon of address phrases can be restricted by the result
of postal code recognition. In word recognition of ordinary text, the lexicon for
recognizingawordimagecanbereducedbylinguisticcontextrepresentedasbetween-
word dependency statistics or a grammar.
String recognition can be speeded up by matching with the string classes of a
speciﬁed length or several lengths only. String length or a range of lengths can be
estimatedfromthewidthandheightofstringimage,thenumberofprimitivesegments,
the number of near-vertical strokes, the number of contour peak/valley points, and
so on.
Some holistic string recognition methods using global shape features can quickly
match a string image with a large number of string prototypes, based on which the
string classes of low scores are excluded from detailed matching. Some string shape
features,likethecontourenvelopeofstringimage,letterascenders/descenders,promi-
nent strokes and contour points, and so on, can be easily extracted and can ﬁlter out
a fraction of string classes that are unlikely to match the string image.
5.3.4.2
Lexicon Organization
Lexicon-driven string recognition can be
performed more efﬁciently by matching with all string classes simultaneously than by
matching one by one. In matching with all string classes, the strings with low-score
partial match are abandoned because they are unlikely to give the optimal complete
match. The search strategy, like character-synchronous beam search, usually requires
the lexicon to be organized in a lexical tree (trie) or graph structure such that common
substrings are matched with the string image only once.
The trie structure of lexicon has been widely used in word and string recognition
[8, 9, 15, 24, 56, 62, 66]. In the trie, the strings with a common preﬁx of substring,
such as {ab-normal, ab-stract, ab-use}, have a common path of parent nodes for the
preﬁx and the remaining substrings in a subtree. Each path from the root node to a
terminal node corresponds to a string. So, the number of terminal nodes equals the
number of lexicon entries. This structure can save both the storage of lexicon and the
computation in string recognition because the common preﬁx is stored only once and
matched only once in search. Figure 5.18 shows a trie for 12 month words, which
have 74 characters in total and 69 nodes in the trie.
Assume that the average string length is L and the average branching factor
of nonterminal nodes is b, then the number of nodes in the trie is approximately
(L
l=0 bl = bL+1−1
b−1
≈
b
b−1bL), compared to L · bL, the total number of characters.
This implies, when
b
b−1 < L, the number of trie nodes is smaller than the total number
of characters in the lexicon.
In string recognition by character-synchronous search, a search node is associated
with a node in the trie, which corresponds to the character paired with the candidate

236
WORD AND STRING RECOGNITION
FIGURE 5.18
Trie for a small lexicon of 12 month words.
pattern. When the search node is expanded, a succeeding candidate pattern is paired
with the characters given by the offspring of the trie node.
Compared to lexicon-free recognition, trie structure of lexicon also helps im-
prove the accuracy of character recognition. This is because each candidate pattern is
classiﬁed only in a restrictive subset (deﬁned by the offspring of a trie node) of
character classes, which is mostly very small compared to the whole character set.
Classiﬁcation in a small character set is both more efﬁcient and more accurate than
in a large character set.
The trie structure, though facilitates character-synchronous search, is not econom-
ical enough in storage. If there are many strings sharing middle substrings or sufﬁx
but differing in preﬁx, the trie structure cannot save much nodes compared to the ﬂat
lexicon because the common substrings and sufﬁx are stored in different paths for
multiple times. For example, in the trie of Figure 5.18, there are four words sharing
sufﬁx “-ber” and two words sharing sufﬁx “-uary.” The common substrings can be
better explored using a graph structure called directed acyclic word graph (DAWG)
[54]. In DAWG, a common nonpreﬁx substring is stored in a unique path of nodes
and linked to multiple parent nodes corresponding to multiple strings. Figure 5.19
shows a DAWG for four month words with common sufﬁx.
FIGURE 5.19
Directed acyclic word graph for four words.

HMM-BASED RECOGNITION
237
Ikeda et al. proposed to represent a large variety of strings using a context-free
grammar[40].Thisstructureisespeciallyusefulforthesituationwherealargenumber
of strings differ only in a local substring. This is the case for Japanese address phrases,
where a standard phrase is often written in variations differing in substrings. For
lexicon-driven string recognition, the context-free grammar can be transformed to a
directed acyclic graph for being used in search.
Forstringrecognitionwithgraph-structuredlexicon,character-synchronoussearch
proceeds in the same way as for trie lexicon: the succeeding characters of a trie/graph
node are given by its offspring. In this sense, graph structure of lexicon does not save
search time compared to trie structure, but is much more efﬁcient in saving storage.
The generation, storage, and maintenance of graph lexicon, however, is complicated.
5.4
HMM-BASED RECOGNITION
HMMs [38, 41, 76, 77] have become a mainstream technique and a ubiquitous compo-
nent in current systems developed for recognizing machine-print, online, and ofﬂine
handwriting data. One of the main features of these data is that, owing to noise and
diversity of writing styles, the same transcription word usually takes on variable-
length signal forms. This fact has motivated the use, ﬁrst, of the DP paradigm whose
main strength is the ability to match signals of variable lengths. Although DP tech-
niques have proven to be effective for simple applications with small vocabularies,
they turned out to be less efﬁcient for complex applications involving large vocabu-
laries. The reason is twofold: First, DP robustness requires a quite large number of
prototypes or templates per modeling unit in order to accommodate the increasing
variability in large vocabulary applications. Second, DP suffers from the lack of efﬁ-
cient algorithms to automatically estimate the prototype parameters. By overcoming
these two limitations and featuring other desirable properties, HMMs become the
natural successors of DP and have been adopted as the predominant approach for
recognizing discrete sequential variable signals. These models derive single com-
pact (average) models explaining the variability of data, and besides, automatically
estimate the associated parameters using efﬁcient algorithms.
5.4.1
Introduction to HMMs
HMMs are an extension of Markov chains, for which symbol (or observation)
production along states (symbol generation along transitions is another variant
that is widely used) is no longer deterministic but occurs according to an output
probabilistic function, hence their description as a double stochastic process. Such
a scheme, while avoiding a signiﬁcant increase in the complexity of the Markov
chain, offers much more freedom to the random process, enabling it to capture the
underlying variability of discrete sequences. The Markov chain serves as an abstract
representation of structural constraints on data causality. Such constraints are usually
derived from our knowledge about the problem and the data as well as from taking
into account the way data (1D signals such as online handwriting data or images such
as ofﬂine handwriting data) are transformed into sequential strings.

238
WORD AND STRING RECOGNITION
Theoutputprobabilisticfunctionsembodythesecondstochasticprocessandmodel
the inherent variability of characters or of any basic unit of the language we are dealing
with. The variability results from various distortions inherent to image acquisition,
binarization, intra- and interscriptor diversity of writing styles, and so on.
The underlying distribution of these functions deﬁnes the HMM type. If it is
discrete (nonparametric), the model is called discrete HMM. The discrete alphabet
(codebook), in this case, is usually obtained through vector quantization [35, 64].
Continuous HMMs [55] model the output function by a continuous probability
density function, usually a mixture of Gaussians. Semicontinuous HMMs [39] can
be thought of as a compromise of these two extremes and have the output functions
sharing the same set of Gaussians (but model Gaussian coefﬁcients as state-dependent
parameters).
One of the main limitations of HMMs is the assumption that every observation does
not depend on the previous ones but only on the current state. Such an assumption
clearly does not handle the correlation between contiguous observations, which is
inherent to the diversity of writing styles. However, this very limitation is the main
reason behind the success of HMMs. As will be shown in the next subsection, the
conditional independence assumption is behind the existence of efﬁcient algorithms
for automatic training of the model parameters and decoding of the data.
For HMM-based text recognition, there are two main approaches: the ﬁrst relies
on an implicit segmentation [26, 33, 36, 45, 90], where the handwriting data are
sampled into a sequence of tiny frames (overlapped or not). The second uses a more
sophisticated explicit segmentation technique [17, 28, 31, 87] to cut the text into
more meaningful units or graphemes, which are larger than the frames. In either case,
the ultimate segmentation of the words into letters is postponed and obtained as a
by-product of recognition itself. This capability is one of the most powerful features
of HMMs, which make them the preferred method for word recognition.
5.4.2
Theory and Implementation
In the discrete case, an HMM is formally deﬁned by the following parameters:
• A = {aij}, the Markov chain matrix, where aij is the probability of transition
from state i to state j, with i, j ∈{1, 2, . . . , N}, N being the number of states.
• B = {bj(k)}, the output distribution matrix, where bj(k) is the probability of
producing symbol k, when the Markov process is in state j. k ∈{1, 2, . . . , M},
M being the size of the symbol alphabet.
•  = {πi}, the probability that the Markov process starts in state i. Without loss
of generality, it will be assumed in the remaining of this section that state 1 is the
only initial state. Thus, π1 = 1 and πi = 0 for i ̸= 1. In the same way, we assume
that N is the only terminating state. λ = (A, B) is a compact representation of
the HMM.

HMM-BASED RECOGNITION
239
Before HMMs can be used in actual applications, the development of a hidden
Markov model approach entails addressing three problems. First, given an observation
sequence O and a model λ, how do we compute P(O|λ)? This is known as the
evaluation problem. Second, given an observation sequence O and a model λ, what
is the optimal state sequence in λ accounting for O (decoding problem)? Third, given
a set of observation sequences, how do we estimate the parameters of the model
(learning problem)?
5.4.2.1
The Evaluation Problem
Given an observation sequence O =
o1 · · · oT , the computation of P(O|λ) is straightforward:
P(O|λ) =

S
P(O|S, λ)P(S|λ),
(5.33)
where the sum runs over all state sequences S.
By using HMM properties regarding conditional independence of observations
and ﬁrst-order Markov chains, this sum becomes
P(O|λ) =

S
as0s1bs1(o1)as1s2bs2(o2) · · · asT−1sT bsT (oT ).
(5.34)
As there are NT possible state sequences, this computation is clearly intractable.
Nonetheless, because of the properties mentioned above, it is easy to see that the
paths shared across state sequences S need to be computed only once. Hence, P(O|λ)
can be inductively obtained by introducing a forward probability αt(i) : αt(i) =
P(o1, . . . , ot, st = i|λ), which leads to the Forward algorithm below.
Algorithm: Forward procedure
1. Initialization
α1(1) = 1.
2. Recursion
αt(j) =
6 N

i=1
αt−1(i)aij
7
bj(ot).
3. Termination
P(O|λ) = αT (N).
5.4.2.2
The Decoding Problem
Sometimes, we are interested, for many
reasons, in ﬁnding the optimal state sequence accounting for an observation sequence
O. In the maximum likelihood sense, this amounts to searching for a state sequence

240
WORD AND STRING RECOGNITION
S∗such that
S∗= arg max
S
P(O, S|λ).
(5.35)
Using the properties of HMMs discussed above, this turns out to be a straightforward
task, if we deﬁne the partial Viterbi probability in this way:
δt(i) =
max
s1,...,st−1 P(o1, . . . , ot, s1, . . . , st = i|λ).
(5.36)
This is the probability of the best partial state sequence, generating the t ﬁrst obser-
vations and leading to state i at time t. This leads to the following Viterbi algorithm.
Algorithm: Viterbi decoding
1. Initialization
δ1(1) = 1,
B1(1) = 1.
2. Recursion
δt(j) = max1≤i≤N[δt−1(i)aij]bj(ot),
Bt(j) = arg max1≤i≤N[δt−1(i)aij].
3. Termination
P(S∗) = δT (N),
BT (N) = arg max1≤i≤N[δt−1(i)aiN],
where S∗is the sequence associated with the best path.
4. Backtracking
s∗
t = Bt+1(s∗
t+1).
Bt(i) is a backpointer keeping track of the best partial path leading to state i at
time t. Once reaching the end state N, we recursively apply the backpointer
B to retrieve the entire optimal sequence S∗.
5.4.2.3
The Training Problem
Before using an HMM in evaluation or decod-
ing, we ﬁrst need to estimate its parameters. That is, given a set of training data, we
want to derive a model that is able to perform reasonably well on future unseen data.
Unfortunately, no satisfactory and efﬁcient method has been devised so far to simul-
taneously optimize the model structure (topology) and the model parameters. The
usual technique, to overcome this limitation, is to make use of a priori knowledge of

HMM-BASED RECOGNITION
241
the problem to design a suitable model topology and then to optimize the parameters
given this topology.
(1) ML estimation
Training HMM parameters is a typical example of unsupervised learning where
data are incomplete. The incompleteness here stems from the availability of the
observation sequences (observable) but not of their generating state sequences (hid-
den). The most widely used technique to solve this problem is the ML-based Baum–
Welch algorithm, which is an implementation of the EM algorithm [23, 69] in the
case of HMMs. The objective of ML is to seek the model λ that maximizes P(O|λ).
The Baum–Welch (EM) algorithm achieves this optimization in an elegant way: After
guessing an initial estimate for the model (possibly randomly), the ﬁrst step consists
of replacing P(O|λ) by the Q function Q(λ, λ′):
Q(λ, λ′) =

S
P(O, S|λ) log P(O, S|λ′),
(5.37)
which is essentially the expectation of log P(O, S|λ). The rationale behind this
transformation is that
Q(λ, λ′) ≥Q(λ, λ) ⇒P(O|λ′) ≥P(O|λ).
(5.38)
Thus, maximizing Q(λ, λ′) over λ is equivalent to maximizing P(O|λ). This
maximization constitutes the second step. The algorithm by iteratively running
through the two steps guarantees the convergence toward a local maximum of P(O|λ).
In spite of the nonglobal character of the optimization, the solutions reached are usu-
ally satisfactory, if the assumptions about the model are suitable for the problem at
hand.
For real tasks, an HMM is usually trained from a set of Y independent observation
sequences by looking for the model λ that maximizes the likelihood
L =
Y
$
y=1
P(Oy|λ),
(5.39)
where Oy is the yth observation sequence. Fortunately, because of the assump-
tion of independence between observation sequences as stated above, this leads to
essentially the same calculations. By using the deﬁnition of αt(i) and introducing a
backward probability variable βt(i), βt(i) = P(ot+1, . . . , oT |st = i, λ), we eventually
obtain these quite intuitive reestimation formulas:
aij =
Y
y
1
P(Oy|λ)
T
t=1
N
i=1 αy
t−1(i)aijbj(ot)βy
t (j)
Y
y
1
P(Oy|λ)
T
t=1
N
i=1 αy
t (i)βy
t (i)
,
(5.40)

242
WORD AND STRING RECOGNITION
bj(k) =
Y
y
1
P(Oy|λ)
T
t=1
N
i=1 δ(oy
t = vk)αy
t−1(i)aijbj(ot)βy
t (j)
Y
y
1
P(Oy|λ)
T
t=1
N
i=1 αy
t−1(i)aijbj(ot)βy
t (j)
,
(5.41)
where
δ(a, b) =

1, if a = b,
0, otherwise.
aij can be interpreted as the expected number of transitions from state i to state j over
the expected number of times the Markov chain is in state i. In the same way, bj(k)
is the expected number of times the state j produces observation symbol k over the
expected number of times the Markov chain is in state j.
For continuous HMMs, bj(ot) is no longer represented by a nonparametric discrete
distribution bj(k) but by (usually) a mixture of Gaussians:
bj(ot) =
M

m=1
cjmNjm(ot, μjm, 
jm) =
M

m=1
cjmbjm(ot),
(5.42)
where M is the number of Gaussians under consideration, cjm, μjm, and 
jm are,
respectively, the weight, the mean, and the covariance matrix associated with state j
and mixture m. Likewise, the reestimation formulas for these parameters are intuitive
and similar to the discrete case:
cjm =
Y
y=1
1
P(Oy|λ)
T
t=1
N
i=1 αy
t−1(i)aijcjmbjm(ot)βy
t (j)
Y
y
1
P(Oy|λ)
T
t=1
N
i=1
M
m=1 αy
t−1(i)aijcjmbjm(ot)βy
t (j)
,
(5.43)
μjm =
Y
y=1
1
P(Oy|λ)
T
t=1
N
i=1 αy
t−1(i)aijcjmbjm(ot)βy
t (j)ot
Y
y
1
P(Oy|λ)
T
t=1
N
i=1
M
m=1 αy
t−1(i)aijcjmbjm(ot)βy
t (j)
,
(5.44)

jm =
Y
y=1
1
P(Oy|λ)
T
t=1
N
i=1 αy
t−1(i)aijcjmbjm(ot)βy
t (j)Ejmt
Y
y
1
P(Oy|λ)
T
t=1
N
i=1
M
m=1 αy
t−1(i)aijcjmbjm(ot)βy
t (j)
,
(5.45)
where,
Ejmt = (ot −μjm)(ot −μjm)T .
(2) Other approaches
One of the requirements for the success of EM training is the correctness of the
model. This is hardly met in practice because of some HMM weaknesses, such as the
conditional independence assumption and the Markov chain assumption, which are
clearly inaccurate when studying the production of complex signals like handwriting
(or speech) (actually, the underlying Markov chain may be able to take into account

HMM-BASED RECOGNITION
243
some dependencies between the observations, but only partially). Therefore, other
alternatives have been proposed as a substitute to EM training. One of the most
popular techniques is the MMI estimation [2], which not only tries to increase the
probabilities of the parameters of the correct model (corresponding to current label)
but also aims at decreasing the probabilities of the parameters of competing models.
This optimization usually involves the use of gradient descent techniques. Although
MMI and other discriminative approaches have sound theoretical foundations, EM is
still one of the most widely used estimation approaches as the difference in recognition
performance has not been, in general, signiﬁcant in practice [38].
5.4.3
Application of HMMs to Text Recognition
HMMs are typically used in the framework of the pattern recognition approach to
text recognition [4]. Given an observation sequence O, our aim is to seek the class
W* (W may correspond to characters, words, sentences, etc.) that maximizes the a
posteriori probability P(W|O). Using the Bayes rule,
W∗= arg max
W P(W|O) = arg max
W
P(O|W)P(W)
P(O)
= arg max
W P(O|W)P(W),
(5.46)
as P(O) is independent of W. P(W) is the a priori probability of W, and depending
on the application, it might be either dropped if no a priori knowledge is available
or rather introduced to guide recognition. For example, for sentence recognition,
n-grams are usually used to estimate P(W) = P(W1W2 · · · Wn), where Wi are the
words of the sentence [67, 90].
P(O|W) is the probability of O (likelihood of the data) given class W. In the case
of HMMs, W is implicitly represented by its underlying model: P(O|W) = P(O|λ).
P(O|λ) can be efﬁciently estimated in several ways depending on the task at hand.
For applications involving small vocabularies (for instance, digit recognition [12],
words belonging to literal amounts of checks [34, 36, 87], etc.), we can build an
HMM model λi for each class and use the forward algorithm to estimate P(O|λi) for
i = 1, . . . , Nw, where Nw is the number of classes. In such a case, the lexicon is said
to be linear or ﬂat.
For large vocabularies (for instance, city or street name recognition and sentence
or continuous word recognition [17, 27, 28, 31, 34, 65, 90]), however, this approach is
no longer feasible. The reason is threefold. First, considering one model for each class
means that running the forward algorithm for each class separately could take a huge
amount of time, which might not be affordable. Second, such a scheme may require
a large amount of memory to store as much models as the size of the vocabulary.
For vocabularies of thousands of entries this can be a serious issue. Third, parameter
estimation might get seriously degraded as a consequence of the lack of sufﬁcient
training data to adequately represent the richness of variations inherent to such a
large number of classes.

244
WORD AND STRING RECOGNITION
Coping with these weaknesses usually involves two steps. The ﬁrst tries to ﬁnd
subclasses or the natural basic units of the language in consideration, which can be
shared by the classes of interest. For instance, when we are dealing with words, the
natural basic units are characters. Then by building characters HMMs, HMMs at the
word level can be easily obtained by concatenating the elementary HMMs associated
with the word transcription. One of the most important strengths of HMMs is that no
segmentation of words into characters is needed in order to train the character models.
HMMs are able to train the models while at the same time optimizing the segmentation
of the word into its characters. The second step has to do with the structure of the
lexicon. Instead of using a ﬂat structure, the lexicon could be organized as a trie (or
a lexical tree) by sharing preﬁxes across the words of the vocabulary. In this way,
a preﬁx shared by K words will have its HMM evaluated only once instead of K
times. By avoiding this kind of redundant computation, a signiﬁcant speed up could
be gained, although this depends on the compression ratio that the vocabulary of the
application allows for.
Considering a tree-structured lexicon and the associated HMMs means that to
retrieve the optimal class, the forward algorithm can no longer be used because it
cannot give any clue as to the optimal sequence of characters. The Viterbi algorithm
becomes the natural solution in this case as the optimal path it generates can be directly
associated with the optimal character sequence, and hence the optimal word. How-
ever, one should keep in mind that Viterbi calculation is only an approximation as it
computes the most probable sequence of states rather than the most probable sequence
of characters, though this is usually satisfactory in practice. The Viterbi algorithm,
moreover, makes it possible to use pruning heuristics to discard nonpromising paths.
One popular technique for this purpose is the Viterbi beam search procedure [73].
There exists, however, another decoding scheme, making use of a trie, for which
the forward algorithm can be used. In such a scheme, a segmentation graph is ﬁrst
built out of the observation sequence to generate all plausible segmentations of the
sequence into character subsequences. Every character segmentation hypothesis is
then submitted to the recognizer, and the accumulated scores are propagated forward
using a DP-based procedure. As every character segmentation hypothesis is ﬁrst fed
to the HMM, the latter can be seen here as a static recognizer. Hence, one is free in this
case to use the forward algorithm rather than the Viterbi algorithm. Such an approach
does impede the retrieval of the most likely state sequence, but it provides the most
likely character sequence. A similar technique has been used in [18], although here
the “static” recognizer was a neural network.
5.4.4
Implementation Issues
In implementing HMMs for text recognition, some practical issues should be
considered: the topology of HMM models, initial estimates of parameters, smoothing
of parameters, and the underﬂow of probabilities.
5.4.4.1
Model Topology
As stated in Section 5.4.2, no straightforward method
is available for training both HMM topology and parameters. Therefore, it is important

HMM-BASED RECOGNITION
245
FIGURE 5.20
A left-to-right HMM.
to devise a suitable topology before the training is carried out. This is particularly
important if we bear in mind that the EM method assumes that the model is correct.
The topology of the model is usually built by using our a priori knowledge on the data
and the way we are processing them. For instance, even though the ofﬂine handwriting
or machine-print signal is two dimensional, it evolves, nonetheless, from left to right,
as speech evolves over time. This is the reason why the so-called left-to-right HMMs
are used for speech and text recognition. For such models, no back transitions from
right to left are allowed. Figure 5.20 shows an example of a left-to-right HMM.
When an implicit segmentation approach is considered, the text signal is usually
segmented into tiny frames and observation features (either discrete or continuous) are
extracted from each frame. For such frame widths, observations will evolve slowly
from left to right when spanning the same stroke and will abruptly change when
there is a transition to another stroke. Under this process, states with self-loops can
naturally represent the quasi-stationary evolution through the same stroke whereas a
transition out of this state will model a transition to the following stroke. Thus, an
HMM for modeling this process will have the simple topology shown in Figure 5.21
Note that for implicit segmentation, the number of states can be signiﬁcantly lower
than the average number of observations, as most of contiguous observations would
be generated through self-loops.
In practice, this kind of topology might not be sufﬁcient to adequately represent the
variations of the data. For instance, we might consider one model for both uppercase
and lowercase letters. Besides, each of these two categories usually consists of many
prototypes or allographs, for instance, letter l (or h or k) with a loop versus that without
a loop. For a better categorization of these well-deﬁned shape variations, it is more
convenient to have a topology with multiple branches as shown in Figure 5.22(a). Such
a topology, moreover, may relax the heavy constraint of observations’ conditional
independence, as each parallel branch (or path) implicitly model, to some extent, the
dependence between contiguous observations belonging to the same prototype.
When explicit segmentation is considered, the frames usually consist of graphemes
and the sequence of graphemes or the observations extracted from can no longer be
quasi-stationary. In such a case, the topology shown in Figure 5.22(a) might be used
FIGURE 5.21
A simple left-to-right HMM with no skipping transitions.

246
WORD AND STRING RECOGNITION
FIGURE 5.22
(a) A multiple-branch simple left-to-right HMM; (b) a multiple-branch left-
to-right HMM with the possibility of skipping transitions.
without loops. (It might work with loops as well if the number of branches and the
number of states are adequately chosen. In such a case, the probability of loops’
transitions should be negligible after training.) Note that for explicit segmentation,
the number of states is usually about the average number of observations.
One of the most popular topologies of text recognition is the one augmenting that
of Figure 5.22(a) with transitions skipping one state. The rationale behind such a
topology is that, in practice, the variations of the data and the imperfection of the
feature extraction process might lead to missing observations. This phenomenon is
conveniently modeled through transitions skipping one state (Fig. 5.22(b)). One might
suggest skipping more than one state, but we should keep in mind that increasing the
number of transitions leads to the increase in the number of parameters to estimate.
Besides, having a sparse Markov chain rather than a dense one often leads to much
more efﬁcient calculations.
5.4.4.2
Initial Estimates
Initial estimation of the parameters serves two
purposes. The ﬁrst is to set HMM topology by zeroing the probabilities of the transi-
tions that are not allowed in the model. The second is to initialize the other HMM
parameters with suitable values with the requirement of avoiding null values as the
nature of the reestimation formulas clearly shows that once a parameter is null, it
will remain so until the end of the training process. Transition probabilities can be
initialized randomly or uniformly with little effect on the overall performance. The
same goes for discrete output probabilities. For continuous output distributions, how-
ever, a special care should be given to initialization as Baum–Welch training heavily
depends on the initial values. One suitable method is to run the EM algorithm for
Gaussian mixture distribution on the nonlabeled observation vectors and initialize
the distribution of all the states with the same set of trained Gaussians. A more
sophisticated method is to use the k-segmental or (Viterbi training) algorithm to get a
different estimation for every state according to the implicit segmentation produced
by the Viterbi algorithm [77].
5.4.4.3
Parameter Smoothing
Independently of the algorithm used for
training, a robust estimation of HMM parameters requires the availability of a large
training data set so that all events (observations) can be sufﬁciently represented for
each state. Such amount of data, however, is rarely available and we usually end
up with having some parameters poorly estimated. This constitutes a serious issue,

HMM-BASED RECOGNITION
247
which, if not correctly handled, could lead to a serious mismatch between the behavior
of the HMM on training data on the one hand and on testing data on the other hand,
with a signiﬁcant drop in performance in the latter case. There are various strategies
that have been adopted to alleviate this problem:
 Flooring: The simplest strategy is to ﬂoor all HMM parameters with some
predeﬁned threshold. The threshold can be parameter dependent (it may depend
on whether we are ﬂooring transition probabilities aij, discrete output proba-
bilities bj(k), mixture coefﬁcients cjm, or covariance matrix elements 
mij).
 Tying: A more sophisticated strategy is to tie states that are used to model rather
rare events. Parameter tying consists of sharing some parameters across different
statesthatarebelievedtomodelsimilarevents.Parametertyingoffers,ingeneral,
a good trade-off between discrimination and reliable parameter estimation. The
choice of the states to tie can be obtained either by a priori knowledge or by
theoretical methods like decision trees [93].
 Interpolation: Another strategy is to interpolate two models with the same
structure, one that is more detailed and the other that is less detailed. For
instance, one can obtain a less detailed model from a detailed model by
extensive parameter tying. The ﬁrst model can be very discriminative but may
have some of its parameters poorly estimated. The second will be less discrim-
inative but will have its parameters better estimated. The interpolated model
assigns higher weights to states of the ﬁrst model when they are correctly esti-
mated (meaning sufﬁcient training data were available for that state) and lower
weights when they are not. Implementation details for the interpolation model
are straightforward and may be found in [42], and an application for handwriting
recognition is reported in [29].
5.4.4.4
Underﬂow
One serious consequence of using high sampling rates in
implicit-based segmentation approaches or of using multiple alphabet sets (see
Section 5.4.5) is that the probabilities involved in the DP calculations (αt(i), βt(i))
may quickly reach the null value. This is because today’s machines are unable to
represent the range spanned by probabilities obtained by recursive multiplications.
The usual solution to overcome this problem is to multiply αt(i) and βt(i) by a scaling
factor (usually this factor is 1|  αt(i)) so that these parameters remain sufﬁciently
large for adequate representation in the computer [76]. However, if the Viterbi algo-
rithm is used, no such burden is necessary as the calculations required can be done in
the log domain by adding log probabilities rather than multiplying probabilities.
5.4.5
Techniques for Improving HMMs’ Performance
In the following are some strategies for improving the recognition performance of
HMMs in practice.

248
WORD AND STRING RECOGNITION
5.4.5.1
Multiple Codebooks
So far, we have considered the standard HMM
framework where one observation is output each time a transition is taken. This is by
no means an HMM-speciﬁc constraint. One is free to consider the multiple-codebook
scheme where the HMM outputs more than one observation at a time. The need for
such an approach arises when we want to augment our observation features with
additional ones. The latter might consist of the derivatives of the former or might
be even independent new features extracted by a new feature extractor. Using this
strategy, more discrimination could be obtained locally at each transition. Moreover,
experiments showed that combining features inside the same HMM can be much
more robust and efﬁcient (in terms of both accuracy and speed) than combining
different HMMs, each associated with a different feature set. In other words, com-
bining evidence earlier proves more robust that combing it later.
The multiple-codebook paradigm is usually used by assuming that the different
feature sets are independent. This allows us to express bj(ot) as the product of the
output probabilities for every feature set: bj(ot) = 4F
f=1 bf
j (ot), where bf
j (ot) is the
output probability associated with the fth feature set and F is the number of feature
sets under consideration.
The independence assumption also makes it possible to express the Q function
as a summation over independent terms, each associated with a different feature set
(in addition to the term associated with transition probabilities). We end up, in this
way, with reestimation formulas [30, 38] that are similar to the standard case of one
feature set. For discrete HMM, the reestimation formula for bf
j (ot) is expressed in
the following way:
bf
j (k) =
Y
y
1
P(Oy|λ)
T
t=1
N
i=1 δ(

oy
t
f = vf
k )αy
t−1(i)aijbj(ot)βy
t (j)
Y
y
1
P(Oy|λ)
T
t=1
N
i=1 αy
t−1(i)aijbj(ot)βy
t (i)
,
(5.47)
where

oy
t
f is observation at time t, associated with observation sequence of index y
and the fth feature set, vf
k is the kth symbol associated with the fth alphabet, and F
is the number of alphabets (number of feature sets). Similar formulas can be obtained
for continuous HMM parameters.
5.4.5.2
Null Transitions and Noise Models
In some applications, build-
ing accurate models for characters might not be sufﬁcient. For instance, a character
pertaining to some word may not be observed at all, either because the word was
misspelled or because of the imperfections of the preprocessing or the segmenta-
tion stages. On more complicated tasks, much more variability can be manifested.
For real-life address recognition tasks, for instance, the location of the bloc to be
recognized is not perfect and may cause the input text to be either truncated or aug-
mented with additional text. Even when the bloc location is correct, the transcription
on the image may not match exactly its corresponding entry in the lexicon because
of some rare abbreviations or because of insertions of additional information inside
the relevant transcription. All these factors and others lead to the conclusion that to

HMM-BASED RECOGNITION
249
be able to perform robust recognition, our models must be as ﬂexible as possible by
allowing skipping and inserting characters [3, 28]. Without this feature, any model,
whatever robust at the character level it might be, will inevitably be mismatched with
a transcription involving character omission/insertion. This is because in this case,
one observation must be consumed each time the HMM makes a transition.
A convenient way to allow omission by an HMM is to add null transitions
(transitions not consuming any observation) between states where an event is thought
to be sometimes missing. For instance, to model a missing character in a word, every
character HMM model can be augmented by a null transition between the ﬁrst and last
states (assuming that observations are emitted along transitions). To model insertion,
we might add a noise model to take account of additional text. In this case, this noise
model must be augmented by a null transition between its ﬁrst and last states to allow
the model to skip it in the more likely situation where no such “noise” text appears.
This mix between noise models and null transitions is the key to getting as ﬂexible
HMM models as possible in highly adverse conditions. Note that as this noise data
are quite rare in practice, it becomes clear that considering a noise model for every
preceding character will ultimately lead to poor estimation of the noise models. One
solution to overcome this problem is to tie the states of all the noise models, which is
the same as considering one noise model regardless of the preceding character.
5.4.5.3
Hybrid HMM/Neural Network
An alternative approach to discrete or
continuous HMM is the so-called hybrid HMM–NN [70] or hybrid HMM–MLP
as usually the neural network is a multilayer perceptron (MLP), even though other
neural network architectures have also been used. It consists of an HMM for which the
distributions of the output probabilities are no longer estimated by a nonparametric
discrete distribution or by a mixture of Gaussians, but directly by an MLP. This is
achieved by ﬁrst designing an MLP architecture for which the outputs are associated
with the states of HMMs. Thus, the classes for the MLP are s = 1, 2, . . . , N. The
inputs to the MLP are the same observation frames as before, possibly augmented by
neighboring frames, so as to take into account the context. It has been shown that under
certain conditions (large training data, model topology sufﬁciently complex, etc.), the
outputs of an MLP can be interpreted as estimates of a posteriori probabilities of the
classes given the input [79]. Thus, the output of the neural network given the frame
at time t is the probability of the state s = j given ot : P(s = j|ot), j = 1, . . . , N. As
HMMs rather need the state conditional likelihoods, the Bayes rule is used to perform
the required transformation:
P(ot|s = j) = P(s = j|ot)P(ot)
P(s = j)
.
(5.48)
As P(ot) is constant during recognition, we can safely drop it and use directly the
scaled likelihood:
P(ot|s = j)
P(ot)
= P(s = j|ot)
P(s = j) ,
(5.49)

250
WORD AND STRING RECOGNITION
where P(s = j) is readily obtained by
P(s = j) =

y

t P(st = j|Oy)

y P(Oy)
=

y

t αy
t (j)βy
t (j)

y P(Oy)
.
(5.50)
As MLP training requires the label for each input, a forced Viterbi alignment is carried
out to map the feature vectors from a training input to its producing states. This is
achieved by retrieving the best state sequence for every training observation sequence.
Then the MLP training is run on the state-labeled feature vectors. This procedure is
iterated until convergence is reached.
The main argument for using the MLP to obtain output probabilities is that the
latter are trained discriminatively and that no assumption on their distribution is made
as opposed to mixtures of Gaussians.
It is worth noting that discrimination brought by the neural net is local (at the state
level) as the HMM and the neural network are trained alternatively and separately.
Other approaches attempt to obtain discrimination at the world level by considering
a global optimizing scheme [5, 6, 87].
Another way of using hybrid HMM–NN is to use the neural network to label HMM
states (or transitions) with class labels [52]. In other words, instead of having the NN
as the estimator of output probabilities, the winning class of the latter can be used
as a discrete observation output by the state. A contextual approach based on this
technique has been used in [31] for handwritten word recognition.
5.4.6
Summary of HMM-Based Recognition
This chapter has discussed an introduction to the theory of hidden Markov models
and its use for text recognition. As shown in the previous discussions, HMMs offer
many interesting features, which make them the technique of preferred choice for
text recognition. HMMs are based on a sound theoretical probabilistic framework
that makes them suitable for modeling highly variable data such as handwriting or
degraded machine-printed text. HMM assumptions, although they lead to limitations
in terms of modeling and accuracy, are behind the existence of efﬁcient algorithms
for parameter estimation and for recognition. To achieve optimal performance, many
issues should be carefully addressed, such as the design of model topology, the
choice between different kinds of models (discrete, continuous, and hybrid), the
consideration of multiples feature alphabets, smoothing of unreliable parameters
as well as coping with numerical limitations. Besides these considerations, overall
robustness and efﬁciency heavily depend on preprocessing and feature extraction as
well as on the recognition scheme and vocabulary representation.
5.5
HOLISTIC METHODS FOR HANDWRITTEN WORD RECOGNITION
As words are normally complex patterns and they contain great variability in hand-
writing styles, handwritten word segmentation is a difﬁcult task. Handwritten word

HOLISTIC METHODS FOR HANDWRITTEN WORD RECOGNITION
251
FIGURE 5.23
Handwritten legal amount recognition involves the recognition of each word
in the phrase matched against a static lexicon of 33 words.
recognition can be greatly aided by a lexicon of valid words, which is usually
dependent on the application domain. For example, as shown in Figure 5.23 there
are 33 different words that may appear in the so-called legal amounts on bank checks.
Hence, the lexicon for this application is small and static (it is constant in all the in-
stances). As another example, the lexicon used for street names in handwritten address
reading generally comprises street name candidates generated from knowledge of the
zip code and the street number. As for each zip code we have different street names,
this is an example of an application where the lexicon is small but dynamic (varying
from one instance to the next). In some other applications, the size of the lexicon
may be very large, for example, over 10,000 words for ordinary text. In any case, the
nature of the lexicon is crucial to the design of the algorithm for handwritten word
recognition in a particular application. A lexicon-driven word recognizer outputs, on
inputting a word image, a reduced lexicon sorted by conﬁdence measure [63].
5.5.1
Introduction to Holistic Methods
From the earliest days of research in handwritten word recognition, two approaches
to this problem have been identiﬁed. The ﬁrst approach, often called segmentation-
based or analytical approach, treats a word as a collection of simpler subunits such as
characters and proceeds by segmenting the word into these units, identifying the units,
and building a word-level interpretation using a lexicon. The other approach treats
the word as a single, indivisible entity and attempts to recognize the word as a
whole. The latter approach is referred to as word-based or holistic approach. As
opposed to the analytical approach, the holistic paradigm in handwritten word recog-
nition treats the word as a single, indivisible object and attempts to recognize words
based on features from the overall shape. The holistic paradigm was inspired in part

252
WORD AND STRING RECOGNITION
FIGURE 5.24
A word image (a) and its shape features (b) including “length,” “ascenders,”
“descenders,” “loops,” and so on.
by psychological studies of human reading that indicate that humans use features of
word shapes such as length, ascenders, and descenders in reading (see Fig. 5.24 for
some of these features). A large body of evidence from psychological studies of read-
ing points out that humans do not, in general, read words letter by letter. Therefore,
a computational model of reading should include the holistic method.
Because analytical approaches decompose handwritten word recognition into the
problem of identifying a sequence of smaller subunits of individual characters, the
main problems they face are
 Segmentation ambiguity: deciding the boundaries of individual segments in the
word image (see Fig. 5.25)
 Variability of segment shape: determining the identity of each segment (see
Fig. 5.26)
FIGURE 5.25
Ambiguities in segmentation: The letter(s) following “H” can be recognized
as “w,” “ui,” “iu,” or “iii.”

HOLISTIC METHODS FOR HANDWRITTEN WORD RECOGNITION
253
FIGURE 5.26
Large variability in shapes of handwritten characters (“O” and “P” in this
example).
Holistic approaches circumvent these problems because they make no attempt
to segment the word into subunits. Actually, holistic methods follow a two-step
process: the ﬁrst step performs feature extraction and the second step performs global
recognition by comparing the representation of the unknown word with those of
the references stored in the lexicon. This scheme leads to two important practical
consequences: First, as letter segmentation is avoided and recognition is performed
in a global way, these methods are usually considered to be tolerant to the dramatic
deformations that affect unconstrained cursive scripts. Second, as they do not deal
directly with letters but only with words, recognition is necessarily constrained to a
speciﬁc lexicon of words. The second point is especially critical when training on
word samples is required. In this case, the lexicon cannot be automatically updated
from letter information. A training stage is thus mandatory to expand or modify the
lexicon of possible words. This property makes this kind of method more suitable for
applications where the lexicon is static (and not likely to change), like check recog-
nition. For dynamic lexicons, the recognition system must have the reference models
for all the words in the union of lexicons such that an unknown word can be compared
with any subset of word references.
The term “holistic approach” has been used in the literature at least in two different
senses: (1) an approach that matches words as a whole and (2) an approach that
uses word shape features. It is important to distinguish holistic features from holistic
approaches. A holistic approach may or may not use holistic word shape features. For
example, it may use pixel direction distribution features. Conversely, a classiﬁer may
use word shape features in an approach that is not holistic to perform segmentation
and/or character recognition. The term “global features” has been used by some
researchers to refer to simpler aspects of word shapes that can be easily and reliably
measured. Often, this refers to estimates of word length and counts of perceptual
features such as ascenders and descenders (see Fig. 5.27).
If the size of the lexicon is small enough, the word shape or length of cursive words
alone contains sufﬁcient information to classify the image as one of the lexicon words
with a very high conﬁdence (see Fig. 5.28).
The process of constructing a lexicon in which each lexicon entry is represented
by its holistic features, or statistics about holistic features (in the case of probabilistic
methods), is sometimes referred to as “inverting the lexicon” [63]. Holistic methods
described in the literature have used a variety of holistic features and representations.
As an example, assume that the holistic features are [length, number of ascenders,
number of descenders]. The lexicon shown in Figure 5.29 can be inverted based on
these three features as follows:

254
WORD AND STRING RECOGNITION
FIGURE 5.27
Ascenders and descenders are perceptual features. Features such as the
proportion of pixels in the left and right segments, the number of extrema, the perimeter
of the word are examples of holistic features.
FIGURE 5.28
If the size of the lexicon is small, the word shape or length of cursive words
alone contains sufﬁcient information to classify the image as one of the lexicon words.
beautiful [9, 4, 1]
Montreal [8, 3, 0]
Canada [6, 2, 0]
Beijing [7, 4, 2]
For training the word recognition system, when the lexicon is small and static,
it becomes possible to collect a large number of training samples of each class
(word). Training may then be performed in the traditional sense for estimating class-
FIGURE 5.29
A lexicon of four words that are inverted based on three holistic features. Each
word is represented by a vector of three elements: [length, number of ascenders, number of
descenders].

HOLISTIC METHODS FOR HANDWRITTEN WORD RECOGNITION
255
conditional densities of features from the training samples or storing prototypical fea-
ture vector exemplars for each class. Discriminative classiﬁers like neural networks
and SVMs are also feasible for holistic word recognition of small static lexicons. In
some methods (mainly in the online handwriting and printed domains), feature repre-
sentation shown in the above example has been used for reduction of large lexicons.
This feature representation has also been used internally by some analytical classiﬁers
to rapidly discard dissimilar lexicon entries.
5.5.2
Overview of Holistic Methods
Holistic word recognition methods can be categorized from different perspectives
[63]: the nature of application and lexicon, the level and data structure of feature
representation, and so on.
1. Application domain: online or ofﬂine. Some holistic methods have been
applied for online recognition of handwritten words, where words were
written on an electronic tablet or with a light pen. Some other holistic meth-
ods deal with the problem of reading ofﬂine handwritten words. In such
cases, handwritten words are typically scanned from a paper document and
made available in the form of binary or gray-scale images to the recognition
algorithm.
2. Lexicon: static or dynamic. In some applications like check recognition, the
lexicon is static and ﬁxed and each entry of the incoming words is compared
with the same static lexicon. In the dynamic case, the lexicon varies from one
instance to another instance, such as address recognition, where for each zip
code there is a different lexicon for the names of the streets that exist in that
zip code (region).
3. Feature extraction: low level, intermediate level, and high level. Local and
low-level structural features such as stroke direction distributions have
been applied successfully for holistic recognition of machine-printed words.
Structural features at the intermediate level include edges, end points, concav-
ities, diagonal and horizontal strokes, and they exhibit a greater abstraction
from the image (pixel or trace) level. Perceptual features such as ascenders,
descenders, loops, and length are easily perceived by the human eye, and we
call them high-level features. There are evidences that these features are used
in human reading. They are by far the most popular for holistic recognition of
handwritten words.
4. Feature representation: vectors, assertions, sequences, or graphs. The
representation scheme of features depends on the nature of features: whether
they are low level, medium level, or high level. For example, feature vectors
and matrix representations are commonly used to represent low level features.
Counts and assertions, for example, the counts of ascenders and descenders,
are the simplest representation of high-level features. Such simple features are
also called “global features” and they are often used to discard dissimilar word

256
WORD AND STRING RECOGNITION
candidates from the lexicon. Sequences are also used for the representation
of a whole word as a sequence of symbols representing a set of structural
primitives, which correspond to intermediate- or high-level features or a com-
bination of such features. With respect to intermediate-level features or higher
level structural features such as edges, end points, and perceptual features, the
presence or absence of each of these features is important. The presence of
such features and their interrelationship can be represented by a graph, which
denotes features as nodes and relationships as edges.
5. Hybrid methods. These refer to the methods that explicitly use a combination
of other methodologies; for example, some methods adopt both analytical and
holistic features. Although holistic and analytical approaches are commonly
distinguishedbytheobservationthatthelatteraresegmentation-based,holistic
and analytical paradigms comprise a continuum of approaches to word recog-
nition. As noted by Casey and Lecolinet in [13], some form of segmentation
is involved in all pattern recognition methods, for which holistic method is
their feature extraction phase:
The main difference lies in the level of abstraction of the segmented elements:
features (that is to say low-level elements) in the case of holistic methods, versus
pseudo-letters in the case of analytical methods.
5.5.3
Summary of Holistic Methods
The holistic paradigm in handwritten word recognition treats the word as a single,
indivisibleentityandattemptstorecognizeitusingfeaturesofthewordasawhole,and
is inspired by psychological studies of human reading, which indicates that humans
use features of word shapes such as length, ascenders, and descenders in reading.
Holistic approaches circumvent the issues of segmentation ambiguity and character
shape variability that are primary concerns for analytical approaches, and they may
succeed on poorly written words where analytical methods fail to identify character
content. Their treatment of lexicon words as distinct pattern classes has traditionally
limited their application to recognition scenarios involving small, static lexicons.
Nevertheless, the complexity of holistic recognition methods varies depending on the
representation level of features and the efﬁciency of matching algorithm. The methods
using simple global features and fast matching algorithms can be used for efﬁcient
lexicon reduction of large lexicons.
5.6
CHAPTER SUMMARY
This chapter ﬁrst gives an overview of recognition methods for words and char-
acter strings, then goes into the details of oversegmentation, classiﬁcation-based
(character-model-based) recognition, HMM-based recognition, and ends with holis-
tic word recognition. In classiﬁcation-based recognition, we start with a probabilis-
tic string classiﬁcation model, under which the scores of character segmentation,

REFERENCES
257
character recognition, and linguistic context are uniﬁed. String-recognition-oriented
classiﬁer design techniques, including character-level training and string-level train-
ing techniques, are discussed. Path search and lexicon organization strategies, which
both affect the time efﬁciency of string recognition, are also discussed in depth. In
HMM-basedrecognition,weintroducethemathematicalformulasofHMMs,describe
the paradigms of applying HMMs to text recognition, and discuss the strategies for
improving the recognition performance. Holistic recognition methods are reviewed
from the perspectives of application domain, the nature of lexicon, the level and data
structure of feature representation, and so on.
Although the classiﬁcation-based recognition methods are primarily used for
explicit segmentation, the HMM-based methods can be used for either explicit
segmentation, implicit segmentation, or holistic recognition. Classiﬁcation-based
methods are particularly useful to applications where the linguistic context is weak,
like numeral string recognition, or the number of character classes is large, like
Chinese/Japanese character string recognition. They are, of course, applicable to other
lexicon-driven recognition cases as well. In applications where the shapes of single
characters are not discernable, like cursive word recognition, holistic recognition
methods are very useful. Holistic methods, however, perform well only for small and
static lexicons. For large lexicons, they can be used for lexicon reduction. In prac-
tice, no single word/string recognition method performs sufﬁciently, and it is often
necessary to combine different methods to solve this difﬁcult problem.
REFERENCES
1. N. Arica and F. T. Yarman-Vural. An overview of character recognition focused on off-line
handwriting. IEEE Transactions on Systems, Man and Cybernetics Part C. 31(2), 216–233,
2001.
2. L. R. Bahl, P. F. Brown, P. V. de Souza, and R. L. Mercer. Maximum mutual information
estimation of hidden Markov model parameters for speech recognition. In Proceedings of
the IEEE International Conference on Acoustics, Speech and Signal Processing, Tokyo,
Japan, 1986, pp. 49–52.
3. L.R.BahlandF.Jelinek.Decodingforchannelswithinsertions,deletions,andsubstitutions
with applications to speech recognition. IEEE Transactions on Information Theory, 21(4),
404–411, 1975.
4. L. Bahl, F. Jelinek, and R. Mercer. A maximum likelihood approach to speech recognition.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 5(2), 179–190, 1983.
5. Y. Bengio, R. De Mori, G. Flammia, and R. Kompe. Global optimization of a neural
network-hidden Markov model hybrid. IEEE Transactions on Neural Networks, 3(2), 252–
259, 1992.
6. Y. Bengio, Y. LeCun, C. Nohl, and C. Burges. LeRec: a NN/HMM hybrid for online
handwriting recognition. Neural Computation, 7(5), 1289–1303, 1995.
7. A. Biem. Minimum classiﬁcation error training for online handwritten word recognition. In
Proceedings of the 8th International Workshop on Frontiers in Handwriting Recognition.
Ontario, Canada, 2002, pp. 61–66.

258
WORD AND STRING RECOGNITION
8. A. Biem. Minimum classiﬁcation error training for online handwriting recognition. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 28(7), 1041–1051, 2006.
9. R. M. Bozinovic and S. N. Srihari. Off-line cursive script word recognition. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence, 11(1), 68–83, 1989
10. T. M. Breuel. A system for the off-line recognition of handwritten text. In Proceedings of
the 12th International Conference on Pattern Recognition. Jerusalem, Israel, 1994, Vol. 2,
pp. 129–134.
11. J. Bromley and J. S. Denker. Improving rejection performance on handwritten digits by
training with rubbish. Neural Computation, 5, 367–370, 1993.
12. J. Cai and Z.-Q. Liu. Integration of structural and statistical information for unconstrained
handwritten numeral recognition. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 21(3), 263–270, 1999.
13. R. G. Casey and E. Lecolinet. A survey of methods and strategies in character segmentation.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 18(7), 690–706, 1996.
14. F. Chang, C.-J. Chen, and C.-J. Lu. A linear-time component-labeling algorithm using
contour tracing technique. Computer Vision and Image Understanding, 93(2), 206–220,
2004.
15. C.-H. Chen. Lexicon-driven word recognition. In Proceedings of the 3rd International
Conferene on Document Analysis and Recognition. Montreal, Canada, 1995, pp. 919–922.
16. W.-T. Chen and P. Gader. Word level discriminative training for handwritten word recog-
nition. In Proceedings of the 7th International Workshop on Frontiers of Handwriting
Recognition, Amsterdam, The Netherlands, 2000, pp. 393–402.
17. M. Y. Chen, A. Kundu, and S. N. Srihari. Variable duration hidden Markov model and mor-
phological segmentation for handwritten word recognition. IEEE Transactions on Image
Processing, 4(12), 1675–1688, 1995.
18. D. Y. Chen, J. Mao, and K. M. Mohiuddin. An efﬁcient algorithm for matching a
lexicon with a segmentation graph. In Proceedings of the 5th International Conference
on Document Analysis and Recognition. Bangalore, India, 1999, pp. 543–546.
19. Y. K. Chen and J. F. Wang. Segmentation of single or multiple touching handwritten
numeral string using background and foreground analysis. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 22, 1304–1317, 2000.
20. M. Cheriet, Y. S. Huang, and C. Y. Suen. Background region based algorithm for the
segmentation of connected digits. In Proceedings of the International Conference on
Pattern Recognition, The Hague, 1992, Vol. 2, pp. 619–622.
21. W. Chou. Discriminant-function-based minimum recognition error pattern-recognition
approach to speech recognition. Proceedings of IEEE, 88(8), 1201–1223, 2000.
22. C. K. Chow. On optimal recognition error and reject tradeoff. IEEE Transactions on
Information Theory, 16, 41–46, 1970.
23. A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data
via the EM algorithm. Journal of the Royal Statistical Society B, 39, 1–38, 1977.
24. A. Dengel, R. Hoch, F. H¨ones, T. J¨ager, M. Malburg, and A. Weigel. Techniques for
improving OCR results. In H. Bunke and P. S. P. Wang, editors, Handbook of Character
Recognition and Document Image Analysis. World Scientiﬁc, Singapore, 1997, pp. 227–
254.

REFERENCES
259
25. B. Dubuisson, M. Masson. A statistical decision rule with incomplete knowledge about
classes. Pattern Recognition, 26(1), 155–165, 1993.
26. A. J. Elms, S. Procter, and J. Illingworth. The advantage of using an HMM-based approach
for faxed word recognition. International Journal of Document Analysis and Recognition.
1(1), 18–36, 1998.
27. M. A. El-Yacoubi, M. Gilloux, and J.-M. Bertille. A statistical approach for phrase
location and recognition within a text line: an application to street name recognition.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 24(2), 172–188, 2002.
28. M. A. El-Yacoubi, M. Gilloux, R. Sabourin, and C.Y. Suen. An HMM based approach for
ofﬂine unconstrained handwritten word modeling and recognition. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 21(8), 752–760, 1999.
29. A. El-Yacoubi, R. Sabourin, M. Gilloux, and C.Y. Suen. Improved model architecture
and training phase in an ofﬂine HMM-based word recognition system. In Proceedings of
the 13th International Conference on Pattern Recognition. Brisbane, Australia, 1998, pp.
1521–1525.
30. A. El-Yacoubi, R. Sabourin, M. Gilloux, and C.Y. Suen. Off-line handwritten word recogni-
tion using hidden Markov models. In L. C. Jain and B. Lazzerini, editors, Knowledge-Based
Intelligent Techniques in Character Recognition. CRC Press LLC, 1999, pp. 191–229.
31. C. Farouz, M. Gilloux, and J.-M. Bertille. Handwritten word recognition with contextual
hidden Markov models. In S.-W. Lee, editor, Advances in Handwriting Recognition. World
Scientiﬁc, Singapore, 1999, pp. 183–192.
32. P. D. Gader, M. Mohamed, and J.-H. Chiang. Handwritten word recognition with character
and inter-character neural networks. IEEE Transactions on Systems, Man and Cybernetics
Part B, 27(1), 158–164, 1997.
33. S. Garcia-Salicetti, B. Dorizzi, P. Gallinari, A. Mellouk, and D. Fanchon. A hidden
Markov model extension of a neural predictive system for on-line character recognition. In
Proceedings of the 3rd International Conference on Document Analysis and Recognition,
Montreal, Canada, 1995, Vol. 1, pp. 50–53.
34. M. Gilloux, M. Leroux, and J. M. Bertille. Strategies for cursive script recognition using
hidden Markov models. Machine Vision and Applications, 8(4), 197–205, 1995.
35. R. M. Gray. Vector quantization. IEEE ASSP Magazine, 4–29, 1984.
36. D. Guillevic and C. Y. Suen. HMM word recognition engine. In Proceedings of the 4th
International Conference on Document Analysis and Recognition. Ulm, Germany, 1997,
pp. 544–547.
37. R. M. Haralick and L. G. Shapiro. Computer and Robot Vision. Addison-Wesley, 1992,
Vol. 1, pp. 28–48.
38. X. Huang, A. Acero, and H.-W. Hon. Spoken Language Processing: A Guide to Theory,
Algorithm and System Development. Prentice-Hall, 2001.
39. X. D. Huang and M. A. Jack. Semi-continuous hidden Markov models for speech signals.
Computer Speech and Language, 3, 239–251, 1989.
40. H. Ikeda, N. Furukawa, M. Koga, H. Sako, and H. Fujisawa. A context-free grammar-based
language model for string recognition. International Journal of Computer Processing of
Oriental Languages, 15(2), 149–163, 2002.
41. F. Jelinek. Statistical Methods for Speech Recognition. MIT Press, Cambridge, MA, 1998.

260
WORD AND STRING RECOGNITION
42. F. Jelinek and R. Mercer. Interpolated estimation of Markov source parameters from sparse
data. In Proceedings of the Workshop on Pattern Recognition in Practice. Amsterdam, The
Netherlands, 1980.
43. B.-H. Juang, W. Chou, and C.-H. Lee. Minimum classiﬁcation error rate methods for
speech recognition. IEEE Transactions on Speech and Audio Processing, 5(3), 257–265,
1997.
44. B.-H. Juang and S. Katagiri. Discriminative learning for minimum error classiﬁcation.
IEEE Transactions on Signal Processing, 40(12), 3043–3054, 1992.
45. A. Kaltenmeier, T. Caesar, J. M. Gloger, and E. Mandler. Sophisticated topology of hidden
Markov models for cursive script recognition. In Proceedings of the 2nd International
Conference on Document Analysis and Recognition. Tsukuba, Japan, 1993, pp. 139–142.
46. S. Katagiri, B.-H. Juang, and C.-H. Lee. Pattern recognition using a family of design
algorithms based upon the generalized probabilistic descent method. Proceedings of the
IEEE, 86(11), 2345–2375, 1998.
47. G. Kim and V. Govindaraju. A lexicon driven approach to handwritten word recognition for
real-time applications. IEEE Transactions on Pattern Analysis and Machine Intelligence,
19(4), 366–379, 1997.
48. F. Kimura, Y. Miyake, and M. Sridhar. Handwritten ZIP code recognition using lexicon
free word recognition algorithm. In Proceedings of the 3rd International Conference on
Document Analysis and Recognition, Montreal, 1995, pp. 906–910.
49. F. Kimura, M. Sridhar, and Z. Chen. Improvements of a lexicon directed algorithm for
recognition of unconstrained handwritten words. In Proceedings of the 2nd International
Conference on Document Analysis and Recognition. Tsukuba, Japan, 1993, pp. 18–22.
50. F. Kimura, K. Takashina, S. Tsuruoka, and Y. Miyake. Modiﬁed quadratic discriminant
functions and the application to Chinese character recognition. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 9(1), 149–153, 1987.
51. A. L. Koerich, R. Sabourin, and C. Y. Suen. Large vocabulary off-line handwriting recog-
nition: a survey. Pattern Analysis and Applications, 6(2), 97–121, 2003.
52. P. Le Cerf, W. Ma, and D. Van Compernolle. Multilayer perceptrons as labelers for hidden
Markov models. IEEE Transactions on Speech and Audio Processing, 2(1), 185–193, 1994.
53. Y. LeCun and Y. Bengio. Word-level training of a handwritten word recognizer based on
convolutional neural networks. In Proceedings of the 12th International Conference on
Pattern Recognition, Jerusalem, Israel, 1994, Vol. 2, pp. 88–92.
54. A. Lifchitz and F. Maire. A fast lexically constrained Viterbi algorithm for on-line hand-
writing recognition. In Proceedings of the 7th International Workshop on Frontiers of
Handwriting Recognition, Amsterdam, The Netherlands, pp. 313–322, 2000.
55. L. A. Liporace. Maximum likelihood estimation for multivariate observation of Markov
sources. IEEE Transactions on Information Theory, 28(5), 729–734, 1982.
56. C.-L. Liu, M. Koga, and H. Fujisawa. Lexicon-driven segmentation and recognition of
handwritten character strings for Japanese address reading. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 24(11), 1425–1437, 2002.
57. C.-L. Liu and K. Marukawa. Handwritten numeral string recognition: character-level train-
ing vs. string-level training. In Proceedings of the 17th International Conference on Pattern
Recognition, Cambridge, UK, 2004, Vol. 1, pp. 405–408.

REFERENCES
261
58. C.-L. Liu and M. Nakagawa. Precise candidate selection for large character set recognition
by conﬁdence evaluation. IEEE Transactions on Pattern Analysis and Machine Intelli-
gence, 22(6), 636–642, 2000.
59. C.-L. Liu, H. Sako, and H. Fujisawa. Discriminative learning quadratic discriminant
function for handwriting recognition. IEEE Transactions on Neural Networks, 15(2),
430–444, 2004.
60. C.-L. Liu, H. Sako, and H. Fujisawa. Effects of classiﬁer structures and training regimes
on integrated segmentation and recognition of handwritten numeral strings. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence, 26(11), 1395–1407, 2004.
61. Z. Lu, Z. Chi, W. Siu, and P. Shi. A background-thinning-based approach for separating
and recognizing connected handwriting digit strings. Pattern Recognition, 32, 921–933,
1999.
62. Y. Lu and M. Sridhar. Character segmentation in handwritten words: an overview, Pattern
Recognition, 29(1), 77–96, 1996.
63. S. Madhvanath and V. Govindaraju. The role of holistic paradigms in handwritten word
recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 23(2),
149–164, 2001.
64. J. Makhoul, S. Roucos, and H. Gish. Vector quantization in speech coding. Proceedings
of IEEE, 73, 1551–1588, 1985.
65. J. Makhoul, R. Schwartz, C. Lapre, and I. Bazzi. A script-independent methodology for
optical character recognition. Pattern Recognition, 31(9), 1285–1294, 1998.
66. S. Manke, M. Finke, and A. Waibel. A fast search technique for large vocabulary online
handwriting recognition. In Proceedings of the 5th International Workshop on Frontiers
of Handwriting Recognition, Colchester, UK, 1996, pp. 183–188.
67. U. V. Marti and H. Bunke. Using a statistical language model to improve the performance of
an HMM-based cursive handwriting recognition system. International Journal of Pattern
Recognition and Artiﬁcial Intelligence, 15(1), 65–90, 2001.
68. C. L. Martin. Centered-object integrated segmentation and recognition of overlapping
handprinted characters. Neural Computation, 5, 419–429, 1993.
69. T. K. Moon. The expectation-maximization algorithm. IEEE Signal Processing Magazine,
13(6), 47–60, 1996.
70. N. Morgan and H. Bourlard. Continuous speech recognition: an introduction to the hybrid
HMM/connectionist approach. IEEE Signal Processing Magazine, 12(3), 25–42, 1995.
71. H. Murase. Online recognition of free-format Japanese handwritings. Proceedings of the
9th International Conference on Pattern Recognition, Rome, Italy, 1988, pp. 1143–1147.
72. M. Nakagawa, B. Zhu, and M. Onuma. A model of on-line handwritten Japanese text
recognition free from line direction and writing format constraints. IEICE Transactions on
Information and Systems, E-88D(8), 1815–1822, 2005.
73. H. Ney and S. Ortmanns. Dynamic programing search for continuous speech recognition.
IEEE Signal Processing Magazine, 16(5), 64–83, 1999.
74. N. J. Nilsson. Principles of Artiﬁcial Intelligence. Springer, 1980.
75. R. Plamondon and S. N. Srihari. On-line and off-line handwriting recognition: a compre-
hensive survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 22(1),
63–84, 2000.

262
WORD AND STRING RECOGNITION
76. L. R. Rabiner. A tutorial on hidden Markov models and selected applications in speech
recognition. Proceedings of IEEE, 77(2), 257–286, 1989.
77. L. Rabiner and B.-H. Juang. Fundamentals of Speech Recognition. Prentice-Hall, 1993.
78. E. H. Ratzlaff, K. S. Nathan, and H. Maruyama. Search issues in IBM large vocabulary
unconstrained handwriting recognizer, In Proceedings of the 5th International Workshop
on Frontiers of Handwriting Recognition, Colchester, UK, 1996, pp. 177–182.
79. M. D. Richard and R. P. Lippmann. Neural network classiﬁers estimate Bayesian a poste-
riori probabilities. Neural Computation, 3(4), 461–483, 1991.
80. J. Sadri, C. Y. Suen, and T. D. Bui. Automatic segmentation of unconstrained handwrit-
ten numeral strings. In Proceedings of the 9th International Workshop on Frontiers in
Handwriting Recognition, Tokyo, Japan, 2004, pp. 317–322.
81. Y. Saifullah and M. T. Manry. Classiﬁcation-based segmentation of ZIP codes. IEEE Trans-
actions on Systems, Man and Cybernetics, 23(5), 1437–1443, 1993.
82. K. M. Sayre. Machine recognition of handwritten words: a project report. Pattern Recog-
nition, 5(3), 213–228, 1973.
83. T. Steinherz, E. Rivlin, and N. Intrator. Ofﬂine cursive script word recognition—a survey.
International Journal of Document Analysis and Recognition, 2(2), 90–110, 1999.
84. N. W. Strathy, C. Y. Suen, and A. Krzyzak. Segmentation of handwritten digits using con-
tour features. In Proceedings of the 2nd International Conference on Document Analysis
and Recognition, Tsukuba, Japan, 1993, pp. 577–580.
85. C. Y. Suen. N-gram statistics for natural language understanding and text processing. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 1(2), 164–172, 1979.
86. K. Suzuki, I. Horiba, and N. Sugie. Linear-time connected-component labeling based on
sequential local operations. Computer Vision and Image Understanding, 89(1), 1–23, 2003.
87. Y. H. Tay, P. M. Lallican, M. Khalid, C. Viard-Gaudin, and S. Knerr. An analytical hand-
written word recognition system with word-level discriminant training. In Proceedings of
the 6th International Conference on Document Analysis and Recognition, Seattle, WA,
2001, pp. 726–730.
88. S. Tulyakov and V. Govindaraju. Probabilistic model for segmentation based word recogni-
tion with lexicon. Proceedings of the 6th International Conference on Document Analysis
and Recognition, Seattle, WA, 2001, pp. 164–167.
89. A. Vinciarelli. A survey on off-line cursive word recognition. Pattern Recognition, 35(7),
1433–1446, 2002.
90. A. Vinciarelli, S. Bengio, and H. Bunke. Ofﬂine recognition of unconstrained handwritten
texts using HMMs and statistical language models. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 26(6), 709–720, 2004.
91. P. H. Winston. Artiﬁcial Intelligence, 3rd edition. Addison-Wesley, 1992.
92. H. Xue and V. Govindaraju. Incorporating contextual character geometry in word recog-
nition. In Proceedings of the 8th International Workshop Frontiers in Handwriting Recog-
nition, Ontario, Canada, 2002, pp. 123–127.
93. S. J. Young, J. J. Odell, and P. C. Woodland. Tree-based state tying for high accuracy
acoustic modeling. In Proceedings of the ARPA Human Language Technology Workshop,
1994, pp. 307–312.

CHAPTER 6
CASE STUDIES
A complete document analysis system involves multiple techniques in different process-
ing steps. Even a character recognition or word recognition module (subsystem) needs
techniques traversing different chapters of this book. To exemplify how the techniques
are practically used and how they are performing, this chapter presents some concrete
examples of using multiple techniques to build practical recognition systems, and shows
experimental results on real image data. As described there are three cases: evolutionary
generation of pattern recognizers, ofﬂine handwritten Chinese character recognition,
date image segmentation and recognition on Canadian bank checks.
6.1
AUTOMATICALLY GENERATING PATTERN RECOGNIZERS
WITH EVOLUTIONARY COMPUTATION
In this section, we describe the CellNet project. The CellNet project is an ongoing
attempt for creating an autonomous pattern recognizer—a system capable of self-
adapting to a pattern recognition task, in a given database with minimal human in-
tervention. The CellNet system is an Evolutionary Algorithm that, beginning with a
set of preprogramed features, evolves binary classiﬁers capable of recognizing hand-
written characters. To date, CellNet has been applied to several different sorts of
handwritten characters with minimal human adaptation, obtaining validation accura-
cies comparable to current technologies. We describe work originally published in
[17, 25].
The essential metaphor behind the CellNet system is that of hunters and prey. The
classiﬁers are hunters who must decide whether or not to accept images (prey). The
Character Recognition Systems: A Guide for Students and Practitioner, by M. Cheriet, N. Kharma,
C.-L. Liu and C. Y. Suen
Copyright © 2007 John Wiley & Sons, Inc.
263

264
CASE STUDIES
hunters that correctly identify images (of a particular class) are declared more “ﬁt,”
and hence are used to create new and ideally better hunters. These hunters evolve
in cooperative coevolution, allowing hunters to merge, and operate as a single, more
complex agent. Additionally, in later trials, a system of competitive coevolution was
introduced. The images (prey) were allowed to apply a set of camouﬂage functions to
themselves, in an attempt to fool the hunters. Prey are declared more “ﬁt” by managing
to fool as many hunters as possible, hence making the hunters’ jobs more difﬁcult.
Through the use of cooperative and competitive coevolution, the CellNet system is
able to overcome several initial difﬁculties in the context of handwritten character
recognition, especially in combating overﬁtting and extending sparse training data.
At the end of the chapter, we describe the potential of the system, and how it may be
extended to other kinds of image recognition.
6.1.1
Motivation
The ﬁeld of pattern recognition is vast. The shear volume of possible frameworks,
conceptualizations, and systems is enormous. The amount of literature available to a
practitionerinanysubﬁeldwillspanawideberthofconceptualframeworksandhighly
specialized cases. A valuable tool in this environment is that of an autonomous pattern
recognizer (APR). We envision a “black box” approach to pattern recognition in which
the APR’s operator need not be privy to the details of the mechanism used in order to
generate a reliable recognition tool. A black box of this sort needs to accommodate
many differing classes of input and output. Nearly no assumptions regarding the space
of input can be made, nor any assumptions regarding clustering, features, and so on.
The immediate trade-off found in this setting is the trade-off between the generality of
the system and the so-called “curse of dimensionality.” Any system that operates on a
space of possible recognizers this large quickly confronts a computationally infeasible
task: the space of all possible recognizers in any sufﬁciently general system is simply
too huge. Hence, an APR needs to implement a means to “canalize development,” a
method of search through the space of possible recognizers that is limiting in terms
of the subspace considered, but robust in terms of ﬁnding optima. In addition to these
difﬁculties, such a system would need to be resistant to difﬁcult problems such as
over-ﬁtting, the tendency of learning systems to “memorize data.” In the context of
adaptive systems, overﬁtting means that the system learns a very precise methodology
for classifying a training set, which does not extend to classifying an independent
validation set. An APR search technique needs to deal with the problem of over-
ﬁtting intrinsically in its methodology of subspace search, in addition to other global
strategies.
6.1.2
Introduction
The aim of the CellNet project is to create a software system capable of automati-
cally evolving complete pattern recognizers from arbitrary pattern sets, that is, to
minimize the amount of expert input required for the creation of pattern recognizers
given some set of target data. This is an ambitious goal, requiring much additional

AUTOMATICALLY GENERATING PATTERN
265
work. This chapter describes our initial successes in the context of handwritten char-
acter recognition. A pattern recognition system is almost always deﬁned in terms
of two functionalities: the description of patterns and the identiﬁcation of those
patterns. The ﬁrst functionality is called feature extraction and the second, pattern
classiﬁcation. As there are many types of patterns in this world, ranging from the
images of fruit ﬂies to those of signatures and thumbprints, the focus of most research
endeavors in pattern recognition has rightfully been directed toward the invention of
features that can capture what is most distinctive about a pattern. This leads to two
overlapping areas of research associated with features: feature selection and feature
creation. Feature selection has to do with choosing a small set of features from a
larger set, in order to maximize the rate of recognition of an associated classiﬁer, and
simultaneously reducing the (computational) cost of classiﬁcation. Feature creation,
on the contrary, has to do with the creation or growth of complex features from a
ﬁnite set of simpler ones, also for the beneﬁt of an associated classiﬁer. CellNet blurs
this distinction—beginning from a preprogramed set of functions, CellNet allows for
the simultaneous selection and recombination of features into classiﬁers, evaluated
at the highest level (largely accuracy). Hence, simple preprogramed features are both
reﬁned and combined into more complex features as a natural part of the functioning
of the system.
In its current form, CellNet is capable of evolving classiﬁers for a given set of
patterns. To achieve this it uses a specialized genetic operator: Merger. Merger is
an operator, somewhat similar to that used in MessyGAs [8], designed to allow the
algorithm to search increasingly larger feature spaces in an incremental manner. This
is different from a normal GA, where the dimensionality of the search space is ﬁxed
at the start of the run, or, possibly, varies slightly and randomly through the recombi-
nation operator. CellNet is cast as a general system, capable of self-adapting to many
handwritten alphabets, currently having been applied to both Arabic and Indian num-
bers (using the CEDAR database and a second database collected at CENPARMI,
Montreal [1]). To achieve a truly autonomous pattern recognizer, however, several
signiﬁcant challenges need to be addressed.
1. Feature creation (or detection) mechanisms that are suited to large sets of
classiﬁcation problems. This is to eliminate the need for serious reconﬁgura-
tion or worse still, reengineering, every time a different application domain is
targeted.
2. Elimination of parameters, many of which need to be set by the user before
the system is exploited. These parameters include: probability distributions
types, probability values, population size, number of feature extractors, and
so on.
3. Thorough testing of the evolved system against a diverse set of pattern
databases, and in doing so, without subjecting the system to any signiﬁcant
amount of reconﬁguration.
We view our work as a step toward the realization of points 2 and 3.

266
CASE STUDIES
6.1.3
Hunters and Prey
As previously mentioned, the CellNet system is composed of both hunters (classiﬁers)
and prey (images from a database). Here we describe both, with special attention on
representation, as this is critical for an evolutionary algorithm.
6.1.3.1
Hunters
The basis on which Hunters are built is a set of normalized
feature functions. The functions (all applied to thinned ﬁgures) used by CellNet are
the set parameterized histograms, central moments, Fourier descriptors, Zernike mo-
ments, normalized width (of a bounding box), normalized height, normalized length,
number of terminators, number of intersections. However, for the sake of illustration,
we shall assume that our examples use only two: F1 and F2. This assumption is made
for ease of visual representation of a two-dimensional feature space and is easily
generalized to higher-dimensional spaces.
A hunter is a binary classiﬁer—a structure that accepts an image as input and
outputs a classiﬁcation. The ﬁtness function that is used to drive the genetic algorithm
determines which digit the agent classiﬁes. For example, assuming our ﬁtness function
speciﬁes that we are evolving hunters to recognize the digit “one,” a hunter that returns
“yes” given an image will implicitly be stating that the image is a “one,” as opposed
to “not-one,” that is, any other digit. We will refer to these classes as the primary class
and the nonprimary class. Hence, a hunter outputs a value from primary, nonprimary,
uncertain when presented with an image. A hunter consists of cells, organized in a
net. A cell is a logical statement—it consists of the index of a feature function along
with bounds. Every cell is represented by the following format:
Feature function Fi
Bound b1
Bound b2
Provided with an image I, a cell returns true, if b1 < Fi(I) < b2 , and false other-
wise. A net is an overall structure that organizes cells into a larger tri-valued logical
statement. That is, a net is a logical structure that combines the true/false values of
its constituent cells to return a value from primary, nonprimary, uncertain. Chromo-
somes consist of trees that begin with a class bit—this bit determines whether or not
the chromosome votes for “primary” or “nonprimary.” Following the class bit is a
tree of one or more cells, connected into a logical structure by and and not nodes. A
chromosome may be represented as a string as follows:
Class bit C
[Not]
Cell1
And
[Not]
Cell2
And
· · ·
Hence, the latter part is a logical statement, which returns true or false. A chromo-
some will return C if the logical statement returns true, otherwise it will remain silent.
Every chromosome is followed by two afﬁnity bits, which we will describe later. A
net is a collection of such chromosomes, connected via a vote mechanism. The vote
mechanism collects input from each chromosome (although some chromosomes will
remain silent), consisting of a series of zero or more values of “primary” or “nonpri-
mary.” The vote mechanism will tally the number of each, and output the majority,

AUTOMATICALLY GENERATING PATTERN
267
FIGURE 6.1
Tree diagram of an example agent.
or “uncertain” in the case of no input or a tie. For example, consider the following
example agent speciﬁed by the two chromosomes, chromosome1 and chromosome
2:
Chromosome1
C1
Cell1
Chromosome2
C2
Cell2
And
Not
Cell3
This example agent may be drawn as a tree as shown in Figure 6.1
Hence, a hunter is a net, that is, a hunter is an organized collection of one or more
cells, which when presented with an image will return one of “primary,” “nonprimary,”
or “uncertain.” The complexity of a hunter is the number of cells it contains, regardless
of organization.
Examples of Hunters of Complexity One. The following are some examples of
hunters with complexity one and interpretations in a two-dimensional feature space.
Assume the primary class is “one,” and the nonprimary class is “not-one.” Our ﬁrst
hunter, A1, consist of a single cell in a single chromosome—it is illustrated in
Figure 6.2 It is instructive to consider the feature space of all images on the basis
of feature F1 and F2—every image maps to an (x,y) coordinate in this space, and
hence may be drawn in a unit square. Agent A1 may be viewed as a statement that
partitions feature space into three disjoint sets—this is also illustrated in Figure 6.2
A second hunter, A2, is illustrated in Figure 6.3 This agent’s partition of the same
feature space is also illustrated.
Merger. Thus far, we have given examples only of hunters with complexity one—
this is the state of the CellNet system when initialized. What follows is a system of
cooperative coevolution that generates agents of increasing complexity. Cooperative
coevolution is achieved through the inclusion of a new genetic operator, augmenting
the typical choices of crossover and mutation. This new operator, merger, serves to
accept two hunters and produce a single new hunter of greater complexity. The com-

268
CASE STUDIES
FIGURE 6.2
Agent A1 and its partition of feature space.
plexity of the merged hunter will be the sum of the complexities of the parents. Merger
operates at the level of chromosomes—when merging two hunters, chromosomes are
paired randomly and merged either horizontally or vertically. Vertical merger simply
places both chromosomes in parallel under the vote mechanism—they are now in
direct competition to determine the outcome of the vote. Horizontal merger, on the
contrary, combines the two chromosomes to produce a single and more complex chro-
mosome, where the two original chromosomes are connected via a and or and-not
connective. Hence, horizontal merger serves to reﬁne a particular statement in the
vote mechanism.
Table 6.1 shows illustrative examples for both vertical and horizontal merger. The
decision of whether a merger will be horizontal or vertical is decided by comparing
the afﬁnity bits of the chromosomes in question. Consider the attempt to merge two
FIGURE 6.3
Agent A2 and its partition of feature space.

AUTOMATICALLY GENERATING PATTERN
269
TABLE 6.1
Afﬁnity-bits based control of the merger mechanism.
A
B
Result ( afﬁnity bits–chromosome–conjunction–chromosome)
00
00
00 A and[not] B
00
01
01 A and[not] B
00
10
10 B and[not] A
00
11
Chromosomes laid out vertically
01
00
01 B and[not] A
01
01
Chromosomes laid out vertically
01
10
11 B and[not] A
01
11
Chromosomes laid out vertically
10
00
10 A and[not] B
10
01
11 A and[not] B
10
10
Chromosomes laid out vertically
10
11
Chromosomes laid out vertically
11
00
Chromosomes laid out vertically
11
01
Chromosomes laid out vertically
11
10
Chromosomes laid out vertically
11
11
Chromosomes laid out vertically
agents, A and B. Both A and B have chromosomes, each with a two-bit afﬁnity. The
afﬁnity of a chromosome is the means through which it controls how it merges with
another. Initially, all chromosomes in A and B are enumerated, and pairs are chosen,
sequentially matching the chromosomes as they occur in the agent. Once chromo-
somes have been paired, they are merged according to the following mechanism: the
conjunction is set as and or and-not on the basis of whether or not the vote classes
agree.
The above (admittedly rather intricate) process was chosen for the amount of con-
trol that may be exploited genetically. For example, a chromosome may be declared
complete by setting its afﬁnity bits to “11.” Or, it may “choose” to always be the ﬁrst
section of a chromosome (and hence, the section that deﬁnes which class is being
voted for) by setting its afﬁnity bits to “10.”
Examples of Horizontal and Vertical Merger. The cooperative coevolution of
hunters, as realized through Merger is a technical process, more easily explained
visually.WereconsideragentsA1 andA2 ofmerger,consideringtheirchildrenthrough
the merger operator. Consider the horizontal merger of hunters A1 and A2—here, we
produce agent A3 by combining the chromosomes of A1 and A2 into one new one,
linked via an and connective. As it is visible in Figure 6.4, horizontal merger may be
viewed as the reﬁnement of a partition created by two chromosomes.
In contrast, consider the vertical merger of these same two hunters, producing agent
A4—in this case, the chromosomes are combined directly under the vote mechanism.
As shown in Figure 6.5, vertical merger may loosely be viewed as the union of the
partitions generated by two chromosomes.

270
CASE STUDIES
FIGURE 6.4
Agent A3 and its partition of feature space.
6.1.3.2
Prey
Initially, the prey in the CellNet system were simply images from
a database. Hence, in initial experiments, prey had no genetic representation, as they
were static structures. We shall refer to these in the future as unaltered images. In
later experiments, we added a competitive component to our evolution of pattern
recognizers—prey became agents, images that tried to disguise themselves using a
set of camouﬂage functions. Hence, prey are altered images, rewarded genetically for
“fooling” hunters. A prey consists of a simple genome—an image index and a series
of bits.
Image index I
Bit b1
Bit b2
· · ·
Bit bk
The image index points to a particular image in the database. The bits are boolean
parameters, indicating whether a particular camouﬂage function is to be applied or not.
Prey exist to be passed to hunters for classiﬁcation—prior to this, however, all cam-
ouﬂage functions speciﬁed by the series of bits in a prey genome are applied—hence,
a hunter views a transformed version of the original image speciﬁed by the image
index. Camouﬂage functions used by the CellNet Co-Ev system consist of {salt–
pepper, scaling, translation, and rotation}. These particular functions were chosen
FIGURE 6.5
Agent A4 and its partition of feature space.

AUTOMATICALLY GENERATING PATTERN
271
as they were topologically invariant (save salt–pepper, unlikely to have a signiﬁcant
topological effect). Parameters for the functions were chosen such that simultaneous
application of all to an image would still yield a human-readable image.
6.1.4
Genetic Algorithm
The CellNet system’s primary driving force is a GA. It consists of a population of
hunters and another of prey, initialized randomly, as the 0th generation. Following
this begins an iterative procedure of reﬁnement: Hunter agents are sorted according
to an assigned ﬁtness, according to which the next generation is prepared. The next
generation is spawned from the “ﬁttest” hunters of the ﬁrst generation, utilizing some
of the genetic operators: elitism, merger, crossover, and mutation. This process con-
tinues for a set number of generations. The reader interested in GAs is directed to
Mitchell’s text [38]. The GA functions to continuously update a population of hunters
by selecting the best through a ﬁtness function, then generating a new population via
the genetic operators. The functioning of our genetic algorithm may be summarized
as follows:
1. Initialize the initial population of randomly generated hunters;
2. evaluate the members of the population using the ﬁtness function;
3. select the best percentage to go directly to the next generation (according to
the rate of elitism);
4. select a set to be merged (according to the rate of merger);
5. select a set to be subjected to crossover (according to the rate of crossover);
6. apply mutation to all nonelite members according to the rate of mutation;
7. if the generation number is less than the maximum, go back to step 2.
In the case of competitive coevolution, not only did the population of hunters evolve
but also the camouﬂaged images. Below, the steps are described in more detail. At
the beginning of each generation, each hunter agent is evaluated for ﬁtness. This is
not necessarily the agent’s accuracy (deﬁned in Section 6.1.5.1), although for testing
purposes raw accuracy is also computed. Rather, ﬁtness is a function based loosely on
accuracy and is meant to provide a measure that will drive the population’s evolution.
Explicit ﬁtness functions depend on the trial in question and are shown in Sec-
tion 6.1.5.
The ﬁrst stage of the preparatory process is elitism: Here a percentage of the top-
ranking agents are simply copied into the next generation without modiﬁcation. This
is done to provide additional stability in the genetic algorithm by ensuring that the
maximal discovered ﬁtness (although not necessarily accuracy) does not diminish.
The population is then subjected to the merger operator: The mechanics of this op-
erator has already been described in Section 6.1.3. Merger is an operator designed to
select two ﬁt agents and combine them to create a single agent with complexity equal
to the sum of the parents’. The merged agents are then deposited in the new population
as single units. Agents are selected for merger with a set probability, chosen using

272
CASE STUDIES
roulette-wheel selection (a.k.a. ﬁtness-proportional selection). Next, the population
is subjected to crossover. Crossover is always single point, although there are two
variations on this—the ﬁrst is a single point crossover in which indices are chosen
randomly between the two agents (variable length) and the second in which one index
is chosen for both agents (ﬁxed length; it is assumed that parent agents will be of the
same length in this case). In both cases, crossover is bitwise, where function limits
are stored as integers. The ﬁnal stage in the preparatory process, a mutation operator
is applied. The mutation operator skims over all bits in all agents, making random
changes with a set probability. The mutation operator applies to all hunter agents with
the exception of those selected for elitism. The process is nearly identical for prey,
whose genome consists of a simple series of bits. Crossover for a prey is single point
and ﬁxed length, occurring within the series of bits. Mutation ﬂips a single bit in the
series. This scheme was chosen as it appears to be the simplest example of a GA on
a bit string and closest to a “standard” method.
6.1.5
Experiments
A series of experiments have been conducted using the CellNet system, attempting
to automatically classify handwritten characters from two different databases. These
experiments measured the ability of the CellNet system to ﬁnd highly accurate classi-
ﬁers under differing circumstances: different character sets, use of our novel merger
operator, and use of competitive coevolution.
6.1.5.1 Initial Experiments: Merger Versus the “Standard” GA
Intheﬁrst
series of experiments, a standard problem was chosen to evaluate the performance
of CellNet in ﬁnding an effective pattern recognizer. The chosen problem was the
ability of the system to distinguish between handwritten characters—speciﬁcally, to
distinguish between the zero character and anything else. All examples were drawn
from the CEDAR database. Data was drawn from two series of runs. The ﬁrst set of
runs consisted of the CellNet system evaluated without merger (hereby denoted the
standard genetic algorithm or SGA trial). The second set of runs consisted of a similar
run of the CellNet system, this time using the new merger operation (hereby denoted
the merger-enabled or ME trial). In both the SGA and ME trials, a population of 500
agents was initialized randomly. Five hundred images were chosen as prey. In the ME
and SGA trials, the prey were simple unaltered images. Fitness was assigned to each
agent according to the function:
Fitness (A) = 1
|T|

i∈T
⎧
⎪
⎨
⎪
⎩
1.0;
A correctly identiﬁes image i
0.2;
A replies “uncertain” for image i
0.0;
A misidentiﬁes image i.
(6.1)

AUTOMATICALLY GENERATING PATTERN
273
This differs from raw accuracy, deﬁned as
Accuracytrain (A) = 1
|T|

i∈T
⎧
⎪
⎨
⎪
⎩
1.0;
A correctly identiﬁes image i
0.2;
A replies “uncertain” for image i
0.0;
A misidentiﬁes image i
(6.2)
for the training sets (where T is the training set in question) and
Accuracyvalid (A) =
1
|V|

i∈V

1.0;
A correctly identiﬁes image i
0.0;
A does not correctly classify image i
(6.3)
for the purposes of validation (where V is the set of images reserved for validation).
Once ﬁtness was assigned, the agents were sorted in descending order and prepared
for the next generation. Two processes were used, one for the ME trials and one for
the SGA trials. In the ME trials, we used a rate of elitism of 0.1, a rate of merger of
0.01, a rate of ﬁxed-size crossover of 0.5, and a rate of mutation of 0.01. Agents in
the ME trials were initialized with a complexity of one, and limited to a maximum
complexity of 40 cells, meaning that all growth of complexity came from the merger
operator through the generations. In the SGA trials, we used an elitism of 0.1, a
ﬁxed-size crossover of 0.5 and a mutation of 0.01. In the SGA trials, all agents were
initialized with a complexity of 40 cells. For every 10 generations, a validation step
was performed—accuracy was computed, instead using an independent veriﬁcation
set of 300 images. This independent veriﬁcation had no feedback to the system—
instead it was used to measure the progress of the system in a manner resistant to the
effects of overﬁtting. A run in either trial was executed for 1000 generations. Each of
the SGA and ME trials were repeated 15 times with little variance in results. Results
of typical executions are shown in Figure 6.6
The maximum accuracy found on the validation set for the typical ME trial was
89.2%. For the SGA trial, the maximum validation accuracy found was 72.9%.
6.1.5.2
Autocomplexity Experiments
In the second set of experiments, the
Autocomplexity (AC) experiments, the system was conﬁgured to again ﬁnd a binary
classiﬁerinamannersimilartotheMEtrialdescribedabove.IntheACtrials,however,
no maximum complexity was enforced: agents were initialized with a complexity of
one cell and could grow without bound. There were two sets of AC experiments: one
using Latin digits (from the CEDAR database) and another using Indian digits from
the CENPARMI database. These sets of experiments are called the Latin AC and the
Indian AC or LAC and IAC experiments, respectively. For each run of the AC trial,
the CellNet system was executed for 1000 generations, outputting data regarding its
(raw) accuracy on the independent veriﬁcation set and regarding the complexity of
the agents produced. The initial prey population was initialized at 500 images, set to
replace 3% from a total pool of 1000 every 5 generations. Again, prey consisted of
simple, unaltered images. The LAC trial was run with a rate of elitism of 0.1, a rate
of merger of 0.02, a rate of (variable-length) crossover of 0.4, a rate of mutation of

274
CASE STUDIES
FIGURE 6.6
(a) Typical run of the ME trial; (b) typical run of the SGA trial.
0.01, and a complexity penalty (alpha) of 0.0005. The ﬁtness function used was
Fitness (A) = 1
|T|
⎡
⎢⎣

i∈T
⎧
⎪
⎨
⎪
⎩
1.0;
A correctly identiﬁes image i
0.2;
A replies “uncertain” on image i
0.0;
A misidentiﬁes image i
⎤
⎥⎦−α · |A|,
(6.4)
where T is t he (nonstatic) set of training images and |A| is the complexity (number
of cells) of A.
Results of the best (most accurate classiﬁer found) LAC trial run is shown in
Figure 6.7 The maximum validation accuracy found was 97.7%, found around
generation 150. Following generation 250, maximum accuracy dropped slightly, set-
tling around 94.8%, despite continued smooth increases in training accuracy. There

AUTOMATICALLY GENERATING PATTERN
275
FIGURE 6.7
(a) Best run of the LAC trial accuracy; (b) best run of the LAC trial complexity.
was initially a large variance in the complexity measure, which are quickly stabilized.
The majority of the most accurate agents displayed little variation from the mean.
The mean maximum validation accuracy found between runs of the LAC trial was
0.954 (with a standard deviation of 0.0147). Complexity in all runs (for both the most

276
CASE STUDIES
accurate agent and the mean) settled at slightly higher than 30 cells, with very little
variance between runs (approximately ±5 cells). Additional informal LAC trials were
run involving Latin characters other than zero, with similar results. In the second set
of experiments (the IAC trial), the system was conﬁgured identically to the LAC trial
above, save that the prey was restructured to distinguish between “4” and not “4,”
using Indian handwritten characters from the CENPARMI database. The initial prey
population was initialized at 400 images, set to replace 3% from a total pool of 600
every 5 generations. Figure 6.8 shows the results of the best (most accurate maximum
classiﬁer) run of 20.
The maximum validation accuracy found in the IAC trial was 97.4%, found around
generation 380. Following generation 450, the accuracies followed a stable course,
probably indicating convergence in the genetic algorithm. The IAC trial was run 20
times, each time with similar results. The mean maximum validation accuracy found
between runs was 0.956 (with a standard deviation of 0.019), with typical mean
complexities similar to those found in the best run above. A hunter with validation
accuracy within 0.005 of the maximum validation accuracy found was generated in
each trial prior to generation 500—convergence appears to have occurred in all cases
prior to generation 1000.
6.1.5.3 Competitive Coevolutionary Experiments
In the ﬁnal set of experi-
ments, the competitive coevolutionary experiments, we allowed both hunters and prey
to evolve. Both populations were initially spawned randomly. For each generation,
each agent was evaluated against the entirety of the opposing population. Explicitly,
let h be a member of the hunter population H, p a member of the prey population P.
For each generation, each hunter h attempted to classify each prey p: let
ClassAttempt(h, p) =
⎧
⎪
⎨
⎪
⎩
1;
h correctly classiﬁes p
0.5; h responds uncertain
0;
h incorrectly classiﬁes p.
(6.5)
Then the accuracytrain of a hunter h was
Accuracytrain(h) = 1
p

p∈P
classAttempt(h, p).
(6.6)
Fitness of a hunter was deﬁned as
Fitness(h) = accuracy2
train(h) −α · |h|,
(6.7)
where again, α is a system parameter designed to limit hunter complexity and |h| is
the number of cells in hunter h. In contrast, the ﬁtness of a Prey p was deﬁned as
Fitness(p) = 1
H

h∈H
(1 −classAttempt(h, p)),
(6.8)
which is inversely proportional to the ﬁtness for hunters.

AUTOMATICALLY GENERATING PATTERN
277
FIGURE 6.8
(a) Best run of the IAC trial accuracy; (b) best run of the IAC trial complexity.
In these experiments, the system’s ability to recognize all Latin characters was
tested. The system was conﬁgured to generate ﬁve binary hunters for each digit—
these are labeled h.x.y, where x is the digit number and y an index from 0 to 4.
Each hunter was trained using a base set of 250 training images and tested via an
independent set of 150 validation images. Each run was executed for a maximum of

278
CASE STUDIES
FIGURE 6.9
Maximum training (light line) and validation (dark lines) accuracies for the h.0
hunters.
250 generations, outputting data regarding validation accuracy each 10 generations.
A typical run may be seen in the evolution of the h.0 hunters as illustrated in
Figure 6.9 Training and validation accuracies are very close, although validation
accuracy tends to achieve slightly higher levels—this behavior is typical of all digits.
This is in contrast to previous experiments, where overﬁtting of approximately 2–3%
was reported consistently. It is also noted that in initial generations of the runs, over-
ﬁtting is common, as it can clearly be seen that the training plots are more accurate
than the validation plots. This initial bonus, however, disappears by generation 60,
where the validation plots overtake. However, also in contrast to previous experi-
ments, complexity is vastly increased—in the case of the zero digit, mean complexity
jumps from approximately 35 cells to approximately 65 cells, while the complexity
of the most accurate agent jumps from 40 cells to seemingly random oscillations in
the range of 50 cells to 350 cells. Figure 6.10 shows the complexities of the most
accurate agents and the mean for the h.0 runs.
Table 6.2 shows the maximum training and validation accuracies for each binary
hunter. The ﬁnal columns compute the means for the validation and training accuracies
foreachclassofhunterandcomparethedifference.Itisshownthatthemeandifference
between training and validation data is −0.006, implying slight underﬁtting of the
classiﬁers to the training data.

AUTOMATICALLY GENERATING PATTERN
279
FIGURE 6.10
Complexity of most ﬁt agents (dark lines) and mean complexity (light lines)
for the h.0 runs.
Finally, a series of experiments was undertaken regarding the classiﬁcations of
the evolved binary classiﬁers. The scope of these experiments was the determination
of the relative independence of the errors made by the classiﬁers when classifying
images. Hence, our goal was a measure of the variance found between the errors of the
hunters for each particular digit. Each hunter evaluated a set of 300 previously unseen
images—a note was made for each error. Each classiﬁer for any particular digit then
had an associated error list of images. These lists were contrasted, computing the total
TABLE 6.2
Maximum training and validation accuracies for the binary classiﬁers.

280
CASE STUDIES
TABLE 6.3
Percentage agreement in errors made by classiﬁers by digit.
Digit
0
1
2
3
4
#Errors
38
22
62
58
34
Agreement
0.26
0.05
0.40
0.52
0.47
Digit
5
6
7
8
9
Mean
#Errors
80
58
55
73
29
60.9
Agreement
0.19
0.19
0.35
0.25
0.25
0.29
number of errors (for all ﬁve hunters) and the percentage of the list shared by two or
more hunters. These results are shown in Table 6.3 It is evident that there is much
variance between the errors made by the various hunters.
6.1.6
Analysis
In the ME versus SGA trials, a clear bias is shown toward the ME system. The SGA
system shows a slow gradual progression toward its optimal training accuracy and a
poor progression toward a good validation accuracy. The latter is not surprising, as
the SGA does not contain any measures to prevent overﬁtting to the training set, and
the chosen features bear a high potential for precisely that. More interesting is the
ME system’s resilience to overﬁtting. We believe that this is a result of the structural
construction of the agents, as early simple agents do not have the complexity necessary
for “data memorization.” The LAC and IAC trials showed a good progression toward
a powerful pattern recognizer, without sacriﬁcing much efﬁciency relative to the
ME trial. Unlimited potential agent complexity proved a computational burden in the
initial generations, but quickly decreased to a stable and relatively efﬁcient maximum.
Theeffectsofoverﬁttingcanbeseenhereaswell,buttoamuchlesserextentthaninthe
SGA trial. A difference between training and veriﬁcation accuracy of approximately
2% seems to be the norm, with overﬁtting nonexistent or negative in some runs.
Variance existed between the runs within the LAC and IAC trials. There is nearly a
2% difference in validation accuracy between the typical and the best run. Indeed, this
shows that several runs of the CellNet system are needed to guarantee a high-accuracy
classiﬁer.
Additionally, it shows that the CellNet system is capable of discovering several
independent classiﬁers, based solely on the seed chosen for random parameter gen-
eration. This opens up the CellNet system for placement within a larger classiﬁer-
combining framework (e.g., bagging) utilizing agents found between independent
runs of the system. The slowdown in convergence in the LAC and IAC trials relative
to the ME trials is to be expected: the complexity of the ME trial was speciﬁcally
chosen to be appropriate to the task at hand, while the agents in the AC trials had
no such luxury. The AC trial was more computationally expensive in terms of time
required for a single generation initially, but soon stabilized to a more reasonable
range of agent complexities. The presence of large (100+ cells) and small (3 cells)

AUTOMATICALLY GENERATING PATTERN
281
agents throughout the trial is most likely the result of a preparatory process between
generations, which was rather forgiving. With the augmentation of competitive
coevolution, these issues are signiﬁcantly improved. The creation of binary classiﬁers
is accomplished for each Latin digit, showing little variance between trial runs. This
contrasted against the use of several runs in the AC trials to ﬁnd a good classiﬁer. The
inclusion of a genome for patterns and camouﬂage functions for diversiﬁcation has
resulted in an artiﬁcially difﬁcult problem for classiﬁers, increasing overall perfor-
mance, and lowering the system’s propensity for “data memorization”—indeed, the
typical coevolutionary run included underﬁtting rather than overﬁtting.
Finally, it has been demonstrated that although the reliability of the system’s ability
to generate classiﬁers has been improved, the error sets produced by the classiﬁers
are largely independent. This matter is crucial for the creation of multiclassiﬁers
(combinations of the binary classiﬁers to form a single multiple-class recognizer),
a step that a practitioner may wish to take. The independence of the error rates of
the classiﬁers implies that several hunters for each class may be used in a bagging
or bootstrapping technique, methods that are expected to improve the accuracy of
the overall multiclassiﬁer. These results represent a signiﬁcant step forward for the
goal of an autonomous pattern recognizer. Competitive coevolution and camouﬂage
is expected to aid in the problem of overﬁtting and reliability without expert tuning,
and also in the generation of a larger and more diverse data set.
6.1.7
Future Directions
The idea of a truly autonomous pattern recognizer is enticing. A black box that
could learn to recognize an arbitrary visual pattern in the best possible manner is an
ambitious goal, and one not yet achieved by the CellNet system. However, work is
currently under way involving several axis by which the CellNet system expositioned
above may be improved. One very powerful aspect of evolutionary computing, little
exploited thus far, is the ability of a “blind designer” to discover nonintuitive solutions
that a human designer might overlook. Our current work centers on this goal—we
are currently in the process of investigating a means to represent the “seed” patterns
in an image independently of preprogram features. These seed patterns will form the
basis for a means for transforming images, allowing simple patterns of trends to be
exaggerated. Following this, we aim to classify images on the information provided
by a set of descriptors describing the exaggerated images. It is our hope that this
generalization of the CellNet system will allow it to self-adapt to a far wider set of
databases of patterns without redesign of the initial preprogramed base.
Finally, a missing component in the creation of a truly autonomous system is the
capacity of that system to select global parameters at run-time, as well as handling
global strategies for the prevention of overﬁtting. Rather than specify parameters,
such as elitism or crossover rate explicitly, the CellNet research group is involved in
a redesign of the CellNet environment that will facilitate their inclusion intrinsically.
This includes a divergence from the typical paradigm of genetic programing in favor
of an immersive environment inspired by other experiments in simulated ecology. It
is hoped that the natural competitive coevolution between camouﬂaged images and

282
CASE STUDIES
recognizing agents will provide a self-optimizing world in which global parameters
are no longer necessary.
6.2
OFFLINE HANDWRITTEN CHINESE CHARACTER RECOGNITION
Chinese characters are used in daily communications by about one quarter of popula-
tion of the world. Besides the characters in Chinese language, the Kanji characters in
Japanese have similar shapes with original Chinese characters. Chinese characters are
also divided into traditional characters and simpliﬁed characters, which are used in
different areas of Chinese culture. Some traditional characters, which are not complex
originally, have identical shapes with simpliﬁed ones since they were not simpliﬁed
in the history. For recognition, we usually consider a character set of one type, say,
traditional Chinese, simpliﬁed Chinese, or Japanese Kanji. Alphanumeric characters
are often considered together because they are commonly mixed with Chinese text.
Sometimes, traditional Chinese and simpliﬁed Chinese characters are also mixed in
a text.
Compared to the recognition of other alphabets in the world, the recognition of
Chinese characters has some special characteristics. First, the number of Chinese
characters is very large. A standard of simpliﬁed Chinese, GB2312-80, has 3755
characters in the level-1 set and 3,008 characters in the level-2 set, 6763 in total. A
new standard GB18030-2000 has 27,533 characters, including both traditional and
simpliﬁed characters, as well as many other symbols. For ordinary people, the number
of daily used characters is about 5000, but a commercial recognition system needs to
accommodate nearly 10,000 characters. The large number of character classes poses
a challenge to efﬁcient classiﬁcation. Second, many Chinese characters have com-
plicated structures. To our knowledge, the number of strokes in Chinese characters
ranges from 1 to 36. Many characters have hierarchical structures: a complicated char-
acter is composed of common substructures (called radicals) organized in 2D space.
The structural complexity of Chinese characters makes the shape description difﬁ-
cult. On the contrary, the complicated structure provides rich information for shape
identiﬁcation. Third, there are many similar characters in Chinese. Some characters
differ only in a small stroke or dot, such as those in Figure 6.11. Due to the similarity
of shape between different characters, it is hard to achieve high recognition accuracy,
especially in handwritten character recognition.
Since the ﬁrst work of printed Chinese character recognition (PCCR) was pub-
lished in 1966 [3], many research efforts have been contributed to both printed and
FIGURE 6.11
Pairs of similar Chinese characters.

OFFLINE HANDWRITTEN CHINESE CHARACTER RECOGNITION
283
handwritten Chinese character recognition (HCCR). Many effective methods have
been proposed, and the recognition performance has been constantly improved. From
the 1990s, Japanese/Chinese OCR software packages have gained popularity by or-
ganizational and personal users for document entry, online handwritten character
recognizers are widely applied to personal computers, PDAs (personal digital aides)
and mobile phones for character entry, and ofﬂine handwritten character recognizers
are applied to form processors, check readers, mail sorting machines, and so on.
Despite the many effective methods and successful results on Chinese charac-
ter recognition reported in the literature, this section gives a case study of ofﬂine
HCCR using representative methods of character normalization, feature extraction,
and classiﬁcation. Speciﬁcally, we will evaluate in experiments the performance of
ﬁve normalization methods, three feature extraction methods, and three classiﬁca-
tion methods. The classiﬁcation methods have been described in Chapter 4 of this
book, three of the ﬁve normalization methods have been described in Chapter 2,
and two of the three feature extraction methods have been described in Chapter 3.
Those have not been described in previous chapters will be given in this section.
We evaluate the recognition performance on two databases of handprinted charac-
ters, namely, ETL9B (Electro-Technical Laboratory, Japan) and CASIA (Institute of
Automation, Chinese Academy of Sciences), with 3036 classes and 3755 classes,
respectively.
In the rest of this section, we ﬁrst give a brief review of previous works in HCCR,
show the diagram of a Chinese character recognition system, then describe the meth-
ods used in our experiments, and ﬁnally, present the evaluation results.
6.2.1
Related Works
The ﬁrst work of printed Chinese character recognition (PCCR) was reported in 1966
[3]. Research on online HCCR was started as early as PCCR [43], whereas ofﬂine
HCCR was started in late 1970s, and has attracted high attention from the 1980s [39].
Since then, many effective methods have been proposed to solve this problem, and
the recognition performance has advanced signiﬁcantly [12, 48]. This study is mainly
concerned with ofﬂine HCCR, but most methods of ofﬂine recognition are applicable
to online recognition as well [32].
The approaches of HCCR can be roughly grouped into two categories: feature
matching (statistical classiﬁcation) and structure analysis. Based on feature vector
representation of character patterns, feature matching approaches usually computed a
simple distance measure (correlation matching), say, Euclidean or city block distance,
between the test pattern and class prototypes. Currently, sophisticated classiﬁcation
techniques, including parametric and nonparametric statistical classiﬁers, artiﬁcial
neuralnetworks(ANNs),supportvectormachines(SVMs),andsoon,canyieldhigher
recognition accuracies. Nevertheless, the selection and extraction of features remain
an important issue. Structure analysis is an inverse process of character generation:
to extract the constituent strokes and radicals, and compute a structural distance
measure between the test pattern and class models. Due to its resembling of human
cognition and the potential of absorbing large deformation, this approach was pursued

284
CASE STUDIES
intensively in the 1980s and is still advancing [18]. However, due to the difﬁculty of
stroke extraction and structural model building, it is not widely followed.
Statistical approaches have achieved great success in handprinted character recog-
nition and are well commercialized due to some factors. First, feature extraction
based on template matching and classiﬁcation based on vector computation are easy
to implement and computationally efﬁcient. Second, effective shape normalization
and feature extraction techniques, which improve the separability of patterns of dif-
ferent classes in feature space, have been proposed. Third, current machine learning
methods enable classiﬁer training with large set of samples for better discriminating
shapes of different classes.
The methodology of Chinese character recognition has been largely affected by
some important techniques: blurring [15], directional pattern matching [53, 52], non-
linear normalization [47, 51], modiﬁed quadratic discriminant function (MQDF) [21],
and so on. These techniques and their variations or improved versions are still widely
followed and adopted in most recognition systems. Blurring is actually a low-pass
spatial ﬁltering operation. It was proposed in the 1960s from the viewpoint of human
vision and is effective to blur the stroke displacement of characters of the same class.
Directional pattern matching, motivated from local receptive ﬁelds in vision, is the
predecessor of current direction histogram features. Nonlinear normalization, which
regulates stroke positions as well as image size, signiﬁcantly outperforms the con-
ventional linear normalization (resizing only). The MQDF is a nonlinear classiﬁer
suitable for high-dimensional features and large number of classes.
Recent advances in character normalization include curve-ﬁtting-based nonlin-
ear normalization [30], pseudo-two-dimensional (P2D) normalization [13, 34], and
so on. The curve-ﬁtting-based normalization performs as well as line-density-based
nonlinear normalization at lower computational complexity. The P2D normalization,
especially that based on line density projection interpolation [34], outperforms 1D
normalization methods with the computational complexity increased only slightly.
The advances in feature extraction include normalization-cooperated feature extrac-
tion (NCFE) [10] and continuous NCFE [31], gradient direction features on binary
and on gray-scale images [29].
In the history, some extensions of direction feature, such as the peripheral direction
contributivity (PDC) [9] and the reciprocal feature ﬁeld [54], have reported higher ac-
curacy in HCCR when a simple distance metric (correlation, Euclidean, or city block
distance) was used for classiﬁcation. These features, with very high dimensionality
(over 1000), are actually highly redundant. As background features, they are sensi-
tive to noise and connecting strokes. Extending the line element of direction feature
to higher-order feature detectors (e.g., [42, 46]) helps discriminate similar charac-
ters, but the dimensionality also increases rapidly. The Gabor ﬁlter, also motivated
from vision research, promises feature extraction in character recognition [50], but is
computationally expensive compared to chaincode and gradient features, and at best,
perform comparably with the gradient direction feature [35].
As to classiﬁcation for large category set, discriminative prototype-based (learning
vector quantization, LVQ) classiﬁers [28] and the MQDF have been demonstrated
effective. The LVQ classiﬁer yields much higher accuracy than the Euclidean distance

OFFLINE HANDWRITTEN CHINESE CHARACTER RECOGNITION
285
(nearest mean) classiﬁer, whereas the MQDF yields even higher accuracy at heavy
burdenofrun-timecomplexity.Otherpopulardiscriminativeclassiﬁers,suchasANNs
and SVMs, have not been widely applied to Chinese character recognition, because
the straightforward training of ANNs and SVMs for a large category set is highly
expensive. Using ANNs or SVMs to discriminate a subset of characters or training
one-versus-all binary classiﬁers with subsets of samples have been tried by some
researchers with success [4, 6, 23, 41].
The normalization methods evaluated in our experiments are linear normaliza-
tion, line-density-based nonlinear normalization (NLN), moment-based normaliza-
tion (MN), bi-moment normalization (BMN) [30], and modiﬁed centroid-boundary
alignment (MCBA) [33]. We do not evaluate the P2D normalization methods here be-
cause the description of them (see [34]) is involving. For feature extraction, we focus
on the direction features, which have been widely used with high success. We eval-
uate the chaincode direction feature, the NCFE, the continuous NCFE, the gradient
direction features on binary and on gray-scale images. For classiﬁcation, we compare
the Euclidean distance classiﬁer, the MQDF, and an LVQ classiﬁer. In feature extrac-
tion, we will also investigate the effects of blurring and varying dimensionality. The
MQDF and LVQ classiﬁers will be evaluated with various complexity of parameters.
6.2.2
System Overview
The diagram of an ofﬂine Chinese character recognition system is shown in
Figure 6.12. The recognition process typically consists of three stages: preprocessing,
feature extraction, and classiﬁcation. Preprocessing is to reduce the noise in character
image and, more importantly, to normalize the size and shape of character for im-
proving the recognition accuracy. The feature extraction stage is usually followed by
a dimensionality reduction procedure for lowering the computational complexity of
classiﬁcation and, possibly, improving the classiﬁcation accuracy. For classiﬁcation
of a large category set, a single classiﬁer cannot achieve both high accuracy and high
speed. A complicated classiﬁer, such as the MQDF, gives high accuracy with very high
FIGURE 6.12
Diagram of ofﬂine Chinese character recognition system.

286
CASE STUDIES
computational complexity. Using such a complicated classiﬁer for ﬁne classiﬁcation,
a low-complexity coarse classiﬁer is used to select candidate classes for acceleration.
The ﬁne classiﬁer only classiﬁes the selected candidate classes, which are far less
than the whole category set. The output of classiﬁcation is a unique character class
or a ranked list of multiple classes with their conﬁdence scores.
6.2.3
Character Normalization
Normalization is to regulate the size, position, and shape of character images, so as
to reduce the shape variation between the images of same class. Denote the input
image and the normalized image by f(x, y) and g(x′, y′), respectively, normalization
is implemented by coordinate mapping

x′ = x′(x, y)
y′ = y′(x, y).
(6.9)
Most normalization methods use 1D coordinate mapping:

x′ = x′(x)
y′ = y′(x).
(6.10)
Under 1D normalization, the pixels at the same row/column in the input image are
mapped to the same row/column in the normalized image and hence, the shape restora-
tion capability is limited. Nevertheless, 1D normalization methods are easy to imple-
ment, and if the 1D coordinate functions are designed appropriately, they lead to fairly
high recognition accuracy.
Given coordinate mapping functions (6.9) or (6.10), the normalized image g(x′, y′)
is generated by pixel value and coordinate interpolation. In our implementation of
1D normalization, we map the coordinates forwardly from (binary) input image to
normalized image, and use coordinate interpolation to generate the binary normalized
image. For generating gray-scale normalized image, each pixel is viewed as a square
of unit area. By coordinate mapping, the unit square of input image is mapped to a
rectangle in the normalized plane, and each pixel (unit square) overlapping with the
mapped rectangle is assigned a gray level proportional to the overlapping area [31].
In our experiments, the normalized image plane is set to a square of edge length
L, which is not necessarily fully occupied. To alleviate the distortion of elongated
characters, we partially preserve the aspect ratio of the input image by aspect ratio
adaptive normalization (ARAN) using the aspect ratio mapping function
R2 =

sin
π
2 R1

,
(6.11)
where R1 and R2 are the aspect ratio of the input image and that of the normalized
image, respectively (see Section 2.4.4). The normalized image with unequal width
and height is centered on the square normalized plane.

OFFLINE HANDWRITTEN CHINESE CHARACTER RECOGNITION
287
The details of linear normalization, moment normalization, and nonlinear normal-
ization have been described in Chapter 2 (Section 2.4.4). Linear normalization and
nonlinear normalization methods align the bounding box of input character image to
a speciﬁed rectangle of normalized plane. Moment normalization aligns the gravity
center (centroid) of input image to the center of normalized plane and is a linear
transformation of pixel coordinates. Nonlinear normalization is based on the his-
togram equalization of line density projections, and the coordinate transformation
functions are not smooth. In the following, we describe two smooth nonlinear trans-
formation methods based on functional curve ﬁtting: bi-moment normalization [30]
and modiﬁed centroid-boundary alignment (MCBA) [33].
Bi-moment normalization aligns the centroid of input character image to the center
of normalized plane as moment normalization does. Unlike that moment normaliza-
tion resets the horizontal and vertical boundaries of input image to be equally distant
from the centroid, bi-moment normalization sets the boundaries unequally distant
from the centroid to better account for the skewness of centroid. Speciﬁcally, the
second-order central moment μ20 is split into two parts at the centroid:

μ−
x = 
x<xc

y(x −xc)2f(x, y) = 
x<xc(x −xc)2px(x),
μ+
x = 
x>xc

y(x −xc)2f(x, y) = 
x>xc(x −xc)2px(x),
(6.12)
where px(x) is the horizontal projection proﬁle of input image f(x, y). The moment
μ02,calculatedontheverticalprojectionproﬁle,issimilarlysplitintotwopartsμ−
y and
μ+
y . The boundaries of input image are then reset to [xc −b

μ−x , xc + b

μ+x ] and
[yc −b

μ−y , yc + b

μ+y ] (b is empirically set equal to 2). For the x axis, a quadratic
function u(x) = ax2 + bx + c is used to align three points (xc −b

μ−x , xc, xc +
b

μ+x ) to normalized coordinates (0, 0.5, 1), and similarly, a quadratic function v(y)
is used for the y axis. Finally, the coordinate functions are

x′ = W2u(x),
y′ = H2v(y),
(6.13)
where W2 and H2 are the width and the height of normalized image, respectively.
The quadratic functions can also be used to align the bounding box and centroid,
that is, map (0, xc, W1) and (0, yc, H1) to (0, 0.5, 1) (W1 and H1 are the width and the
height of the bounding box of input image, respectively). We call this method centroid-
boundary alignment (CBA). A modiﬁed CBA (MCBA) method [33] further adjusts
the stroke density in central area by combining a sine function x′ = x + ηx sin(2πx)
with the quadratic functions

x′ = W2[u(x) + ηx sin(2πu(x))],
y′ = H2[v(y) + ηy sin(2πv(y))].
(6.14)

288
CASE STUDIES
FIGURE 6.13
Curves of coordinate mapping: (a) Quadratic curve ﬁtting for centroid align-
ment; (b) sine functions for adjusting inner density; (c) combination of quadratic and sine
functions.
Figure 6.13(c) shows four curves of combined mapping functions. The curve
“A*C” combines the effects of compressing the right side and stretching the inner,
the curve “A*D” combines the effects of expanding the left side and compressing the
inner, the curve “B*C” combines the effects of compressing the left side and stretch-
ing the inner, and the curve “B*D” combines the effect of expanding the right side
and compressing the inner.
The amplitudes of sine waves, ηx and ηy, are estimated from the extent of the
central area, which is deﬁned by the centroids of half images divided by the global
centroid (xc, yc). Denote the x coordinate of the centroid of the left half by x1 and
that of the right half by x2. On centroid alignment with quadratic function, they are
mapped to values z1 = ax2
1 + bx1 and z2 = ax2
2 + bx2. The extent of the central area
is estimated by
sx = z2 −z1 = ax2
2 + bx2 −ax2
1 −bx1.
(6.15)
The bounds of the central area is then reset to be equally distant from the aligned
centroid: 0.5 −sx/2 and 0.5 + sx/2. The sine function aims to map these two bounds
to coordinates 0.25 and 0.75 in the normalized plane. Inserting them into sine function
x′ = x + ηx sin(2πx), we obtain
ηx = sx/2 −0.25
sin(πsx)
.
(6.16)
The amplitude ηy can be similarly estimated from the partial centroid coordinates y1
and y2 of half images divided at yc.
The mapped coordinates x′ and y′ must be increasing with x or y such that the
relative position of pixels is not reversed. This monotonicity is satisﬁed by
dx′
dx ≥0,

OFFLINE HANDWRITTEN CHINESE CHARACTER RECOGNITION
289
FIGURE 6.14
Character image normalization by ﬁve methods. The leftmost image is the
original and the other ﬁve are normalized ones.
which leads to
−1
2π ≤η ≤1
2π.
(6.17)
When the value of η computed by (6.16) is beyond this range, it is enforced to be a
marginal value −1/2π or 1/2π, which corresponds to sx = 0.265 or sx = 0.735.
Figure 6.14 shows some examples of Chinese character normalization using ﬁve
methods:linear(LN),line-density-basednonlinear(NLN),moment(MN),bi-moment
(BMN), and MCBA.
6.2.4
Direction Feature Extraction
The implementation of direction feature is varying depending on the directional ele-
ment decomposition, the sampling of feature values, the resolution of direction, and
feature plane, and so on. Considering that the stroke segments of Chinese charac-
ters can be approximated into four orientations: horizontal, vertical, left-diagonal,
and right-diagonal, early works usually decomposed the stroke (or contour) segments
into these four orientations.
Feature extraction from stroke contour has been widely adopted because the con-
tour length is nearly independent of the stroke-width variation. The local direction
of contour, encoded as a chaincode, actually has eight directions (Fig. 3.3). Decom-
posing the contour pixels into eight directions instead of four orientations (a pair of
opposite directions merged into one orientation) was shown to signiﬁcantly improve
the recognition accuracy [29]. This is because separating the two sides of stroke edge
can better discriminate parallel strokes. The direction of stroke edge can also be mea-
sured by the gradient of image intensity, which applies to gray-scale images as well as

290
CASE STUDIES
binary images. The gradient feature has been applied to Chinese character recognition
in 8-direction [36] and 12-direction [37].
Direction feature extraction is accomplished in three steps: image normalization,
directional decomposition, and feature sampling. Conventionally, the contour/edge
pixels of normalized image are assigned to a number of direction planes. The
normalization-cooperated feature extraction (NCFE) strategy [10], instead, assigns
the chaincodes of original image into direction planes. Though the normalized image
is not generated by NCFE, the coordinates of edge pixels in original image are mapped
to a standard plane, and the extracted feature is thus dependent on the normalization
method.
Direction feature is also called direction histogram feature because at a pixel or a
local region in normalized image, the strength values of Nd directions form a local
histogram. Alternatively, we view the strength values of one direction as a directional
image (direction plane).
In Chapter 3 (Section 3.1.3), we have described the blurring and sampling of orien-
tation/direction planes, and the decomposition of chaincode and gradient directions.
Both chaincode and gradient features are extracted from normalized character images.
Whereas the chaincode feature applies to binary images only, the gradient feature
applies to both binary and gray-scale images. In the following, we will describe the
NCFE method that extracts chaincode direction feature from original character image
incorporating coordinate mapping. The NCFE method has two versions: discrete and
continuous, which generate discrete and continuous direction planes, respectively. We
will then extend the chaincode feature, NCFE, and gradient feature from 8-direction
to 12-direction and 16-direction.
To overcome the effect of contour shape distortion caused by character image
normalization (stairs on contour are often generated), for chaincode feature extraction,
the normalized binary image is smoothed using a connectivity-preserving smoothing
algorithm [27]. The NCFE method characterizes the local contour direction of input
image, which need not be smoothed. Though the gradient feature is also extracted
from the normalized image, it does not need smoothing since the gradient is computed
from a neighborhood and is nearly insensitive to contour stairs.
6.2.4.1
Directional Decomposition for NCFE
Directional decomposition
results in a number of direction planes (with the same size as the normalize image
plane), fi(x, y), i = 1, . . . , Nd. In binary images, a contour pixel is a black point with
at least one of its 4-connected neighbors being white. If ignoring the order of tracing,
the 8-direction chaincodes of contour pixels can be decided by raster scan (see Sec-
tion 3.1.3).
In the NCFE method proposed by Hamanaka et al. [10], each contour pixel in
the input character is assigned to its corresponding orientation (or direction) plane at
the position decided by the coordinate mapping functions given by a normalization
method. In an elaborated implementation [31], each chaincode in the original image
is viewed as a line segment connecting two neighboring pixels, which is mapped
to another line segment in a standard direction plane by coordinate mapping. In the
direction plane, each pixel (unit square) crossed by the line segment in the main (x

OFFLINE HANDWRITTEN CHINESE CHARACTER RECOGNITION
291
FIGURE 6.15
NCFE on continuous direction plane.
or y) direction is given a unit of direction contribution. To exploit the continuous
nature of line segment, the strength of line direction falling in a pixel is proportional
to the length of line segment falling in the unit square. This version of improvement
is called continuous NCFE [31]. As in Fig. 6.15, where a line segment mapped from
a chaincode overlaps with four unit squares A, B, C, and D. By discrete NCCF, the
pixels A and C are assigned a direction unit, whereas by continuous NCCF, all the
four pixels are assigned direction strengths proportional to the in-square line segment
length.
Figure 6.16 shows the direction planes of three decomposition schemes: chaincode
on normalized image, discrete NCFE, and gradient on binary normalized image. The
direction planes of continuous NCFE and the gradient on gray-scale normalized im-
age are not shown here because they are very similar to those of discrete NCFE and
the gradient on binary image, respectively. We can see that the planes of chaincode
directions (second row) are very similar to those of gradient directions (bottom row).
The planes of NCFE, describing the local directions of the original image, show some
FIGURE 6.16
Original image and normalized image (top row), 8-direction chaincode planes
of normalized image (second row), discrete NCFE (third row), and gradient on binary normal-
ized image (bottom row).

292
CASE STUDIES
difference. Comparing the original image and the normalized image, the orientation
of the right-hand stroke, near left-diagonal orientation, deforms to near vertical. Con-
sequently, the direction planes of left-diagonal orientation (2nd and 6th columns) of
NCFE are stronger than those of chaincodes and gradient, whereas the planes of ver-
tical orientation (3rd and 7th columns) of NCFE are weaker than those of chaincodes
and gradient.
6.2.4.2
Extension to More Directions
The extension of gradient decompo-
sition into more than eight directions is straightforward: simply setting Nd standard
directions with angle interval 360/Nd and typically, with one direction pointing to the
east, then decompose each gradient vector into two components in standard directions
and assign the component lengths to the corresponding direction planes. We set Nd
equal to 12 ﬁrst and then 16.
To decompose contour pixels into 16 directions, we follow the 16-direction ex-
tended chaincodes, which is deﬁned by two consecutive chaincodes. In the weighted
direction histogram feature of Kimura et al. [22], 16-direction chaincodes are down-
sampled by weighted average to form 8-direction planes.
Again, we can determine the 16-direction chaincode of contour pixels by raster
scan. At a contour pixel (x, y), when its 4-connected neighbor pk = 0 and the coun-
terclockwise successor pk+1 = 1 or p(k+2)%2 = 1, search the neighbors clockwise
from pk until a pj = 1 is found. The two contour pixels, pk+1 or p(k+2)%2 and pj,
form a 16-direction chaincode. For example, in Figure 6.17, the center pixel has
the east neighbor being 0, the north neighbor alone deﬁning the 8-direction chain-
code, and deﬁning a 16-direction chaincode together with the southeast neighbor. The
16-direction chaincode can be indexed from a table of correspondence between the
code and the difference of coordinates of two pixels forming the code, as shown in
Figure 6.18. Each contour pixel has a unique 16-direction code.
For decomposing contour pixels into 12 directions, the difference of coordinates
corresponding to a 16-direction chaincode is viewed as a vector (the dashed line in
Fig. 6.17), which is decomposed into components in 12 standard directions as a
gradient vector is done. In this sense, the 12-direction code of a contour pixel is not
unique. For 12-direction chaincode feature extraction, a contour pixel is assigned
FIGURE 6.17
16-direction chaincode formed from two 8-direction chaincodes.

OFFLINE HANDWRITTEN CHINESE CHARACTER RECOGNITION
293
FIGURE 6.18
Difference of coordinates and the corresponding 16-direction chaincodes.
to two direction planes, with strength proportional to the component length. For
(either discrete or continuous) NCFE, the two corresponding direction planes are
assigned strengths proportional to the overlapping length of the line segment mapped
by coordinate functions, as in Figure 6.15.
6.2.5
Classiﬁcation Methods
Many classiﬁcation methods have been described in Chapter 4. However, only a few
classiﬁcation methods have shown high success in large character set recognition.
The modiﬁed quadratic discriminant function (MQDF) (Section 4.2.2.2) gives high
classiﬁcation accuracy at high complexity of storage/computation. It has the advan-
tage that the parameters of each class are estimated on the training samples of one
class only, so the training time is linear with the number of classes. Learning vec-
tor quantization (LVQ) classiﬁers (Section 4.3.6) yield good trade-off between the
classiﬁcation accuracy and the computational complexity. Though the training time
of LVQ is proportional to the square of number of classes (the parameters of each
class are estimated on the samples of all classes), it can be largely accelerated by
hierarchical rival class search in training.
The complexity of parameters, namely, the number of eigenvectors per class for
MQDF and the number of prototypes per class for LVQ, are variable and are usually
set by trial-and-error or cross-validation. We will evaluate the classiﬁcation perfor-
mance of MQDF and LVQ classiﬁers with variable complexity, and compare with the
performance of a simple classiﬁer, the Euclidean distance (nearest mean) classiﬁer.
6.2.6
Experiments
The normalization, feature extraction, and classiﬁcation methods are evaluated on two
databases of handprinted characters. The ETL9B database, collected by the Electro-
Technical Laboratory (ETL) of Japan1 contains the character images of 3036 classes
1ETL has been reorganized to the National Institute of Advanced Industrial Science and Technology (AIST)
of Japan.

294
CASE STUDIES
FIGURE 6.19
Some test images of ETL9B database (left) and CASIA database (right).
(71 hiragana, and 2965 Kanji characters in the JIS level-1 set), 200 samples per class.
This database has been widely evaluated by the community [16, 22]. The CASIA
database, collected by the Institute of Automation, Chinese Academy of Sciences, in
early 1990s, contains the handwritten images of 3755 Chinese characters (the level-1
set in GB2312-80 standard), 300 samples per class.
In the ETL9B database, we use the ﬁrst 20 and last 20 samples of each class for
testing and the remaining samples for training classiﬁers. In the CASIA database, we
use the ﬁrst 250 samples of each class for training and the remaining 50 samples per
class for testing. Some test images of two databases are shown in Figure 6.19.
In our experiments, a character pattern is represented by a feature vector. The fea-
ture vector undergoes two transformations: variable transformation and dimensional-
ity reduction. Variable transformation [7, 49] is also called as Box-Cox transformation
[11]. By this transformation, each feature variable x is replaced by xα, 0 < α < 1. For
causal variables, this is helpful to make the probability density of features closer to
Gaussian. We set α = 0.5, which was shown to perform satisfactorily. After variable
transformation, the feature vector is projected onto a low-dimensional linear subspace
learned by Fisher linear discriminant analysis (LDA) [7] (see also Section 3.1.10.2 in
Chapter 3). We set the reduced dimensionality to 160 for all feature types.
For classiﬁcation by MQDF, we use 40 principal eigenvectors for each class. The
minor eigenvalues are forced to be a class-independent constant, which is propor-
tional to the average feature variance σ2, with the multiplier β (0 < β ≤1) selected
by ﬁvefold holdout validation on the training data set. This is to say, the class means,
principal eigenvectors and eigenvalues are estimated on 4/5 of training samples, sev-
eral values of β are tested on the remaining 1/5 of training samples to select the
optimal value of β that yields the highest accuracy. On ﬁxing β, all the parameters
are then reestimated on the whole training set.
The classiﬁcation of MQDF is speeded up by selecting 100 candidate classes
using Euclidean distance. The MQDF is then computed on the candidate classes only.
Candidate selection is further accelerated by clustering the class means into groups.
The input feature vector is ﬁrst compared to cluster centers and then compared to the

OFFLINE HANDWRITTEN CHINESE CHARACTER RECOGNITION
295
class means contained in a number of nearest clusters. We set the total number of
clusters to 220 for the ETL9B database and 250 for the CASIA database.
To compare the performance of various normalization and feature extraction
methods, we use the Euclidean distance and MQDF for classiﬁcation. Afterwards, we
evaluate MQDF and LVQ with variable parameter complexity on a selected feature
type.
First, we evaluate the effect of blurring in feature extraction on two normaliza-
tion methods: linear normalization and nonlinear normalization, and compare the
4-orientation and 8-direction versions of chaincode feature. We then justify the advan-
tage of continuous NCFE over discrete NCFE, and the advantage of gradient feature
on gray-scale image over that on binary image. We evaluate three types of direc-
tion features (chaincode, continuous NCFE, gray-scale-based gradient) with variable
sampling resolution, and based on the optimal sampling scheme, we compare the
performance of ﬁve normalization methods. Last, on a selected normalization-
feature combination, we evaluate MQDF with variable number of principal
eigenvectors and LVQ with variable number of prototypes.
6.2.6.1
Effects of Blurring
We compare the recognition performance of chain-
code feature sampled by zoning and by blurring (spatial ﬁltering), and compare the
4-orientation and 8-direction versions. In this experiment, the character image is
normalized by two methods: linear normalization and line-density-based nonlinear
normalization (Tsukumo and Tanaka [47]). The size of normalized plane (and orien-
tation/direction planes) is set to 64 × 64 pixels. Each orientation/direction plane is
sampled 8 × 8 feature values by zoning or blurring. Thus, the original dimensionality
of feature vector is 256 for 4-orientation and 512 for 8-direction. In either case, the
dimensionality is reduced to 160 by LDA.
Table 6.4 shows the test accuracies using Euclidean distance and MQDF classiﬁers
on ETL9B database and Table 6.5 shows the accuracies on CASIA database. In each
table, the left half shows the accuracies of Euclidean distance and the right half of
MQDF. “4-zone” and “4-blur” indicate 4-orientation features sampled by zoning
and blurring, respectively, and “8-zone” and “8-blur” indicate 8-direction features
sampled by zoning and blurring, respectively.
On both ETL9B and CASIA databases, comparing the accuracies of either
Euclidean distance or MQDF, we can see that the accuracies of 8-direction feature
are evidently higher than those of 4-orientation feature. Comparing the features by
TABLE 6.4
Accuracies (%) of 4-orientation and 8-direction chaincode feature by zoning
and by blurring on ETL9B database.
Euclidean
MQDF
Norm.
4-zone
4-blur
8-zone
8-blur
4-zone
4-blur
8-zone
8-blur
Linear
89.15
90.83
90.72
92.65
95.57
96.95
96.20
97.44
Nonlin
95.81
96.60
96.16
97.06
98.18
98.80
98.42
98.95

296
CASE STUDIES
TABLE 6.5
Accuracies (%) of 4-orientation and 8-direction chaincode feature by zoning
and by blurring on CASIA database.
Euclidean
MQDF
Norm.
4-zone
4-blur
8-zone
8-blur
4-zone
4-blur
8-zone
8-blur
Linear
81.60
84.46
83.97
87.08
92.42
94.78
93.52
95.55
Nonlin
91.62
93.06
92.47
93.93
96.20
97.23
96.75
97.63
zoning and by blurring, it is evident that blurring yields higher recognition accura-
cies than zoning. Thus, this experiment justiﬁes the superiority of 8-direction feature
over 4-orientation feature and the superiority of feature blurring over zoning. In the
following, we experiment with 8-direction features sampled by blurring only.
6.2.6.2
Justiﬁcation of NCFE and Gradient
We compare the recognition
performanceofdiscreteNCFEandcontinuousNCFE,andtheperformanceofgradient
features on binary normalized image and on gray-scale normalized image (the input
character image is binary). As in the previous experiment, two normalization methods
(linear and nonlinear) are undertaken, the size of normalized plane is 64 × 64, and
8 × 8 feature values are sampled from each direction plane by blurring. The 512-
dimensional feature vector is reduced to 160 by LDA.
Tables 6.6 and 6.7 show the test accuracies on ETL9B database and CASIA
database, respectively. On both two databases, comparing the accuracies of Euclidean
distance or MQDF, it is true that the continuous NCFE yields higher recognition
accuracy than the discrete NCFE, and the gradient feature on gray-scale normalized
image yields higher accuracy than that on binary normalized image. The difference
of accuracy, however, is not so prominent as that between feature zoning and feature
blurring.
6.2.6.3
Effects of Feature Sampling Resolution
In this experiment, we
compare the performance of three direction features (chaincode, continuous NCFE,
and gradient feature on gray-scale image) with varying direction resolutions with
a common normalization method (line-density-based nonlinear normalization). The
TABLE 6.6
Accuracies (%) of 8-direction NCFE (discrete and continuous, denoted by
ncf-d and ncf-c) and gradient feature (binary and gray, denoted by grd-b and grd-g) on
ETL9B database.
Euclidean
MQDF
Norm.
ncf-d
ncf-c
grd-b
grd-g
ncf-d
ncf-c
grd-b
grd-g
Linear
92.35
92.62
92.98
93.26
97.40
97.51
97.49
97.75
Nonlin
97.27
97.39
97.26
97.42
99.08
99.13
99.02
99.10

OFFLINE HANDWRITTEN CHINESE CHARACTER RECOGNITION
297
TABLE 6.7
Accuracies (%) of 8-direction NCFE (discrete and continuous, denoted by
ncf-d and ncf-c) and gradient feature (binary and gray, denoted by grd-b and grd-g) on
CASIA database.
Euclidean
MQDF
Norm.
ncf-d
ncf-c
grd-b
grd-g
ncf-d
ncf-c
grd-b
grd-g
Linear
86.73
87.22
87.66
88.45
95.73
95.92
95.72
96.30
Nonlin
94.46
94.65
94.18
94.66
97.97
98.08
97.70
97.99
direction resolution of features is set equal to 8, 12, and 16. For each direction reso-
lution, three schemes of sampling mesh are tested. For 8-direction features, the mesh
of sampling is set to 7 × 7 (M1), 8 × 8 (M2), and 9 × 9 (M3); for 12-direction, 6 × 6
(M1), 7 × 7 (M2), and 8 × 8 (M3); and for 16-direction, 5 × 5 (M1), 6 × 6 (M2), and
7 × 7 (M3). We control the size of normalized image (direction planes) to be around
64 × 64, and the dimensionality (before reduction) to be less than 800. The settings
of sampling mesh are summarized in Table 6.8.
On classiﬁer training and testing using different direction resolutions and sampling
schemes, the test accuracies on ETL9B database are listed in Table 6.9, and the
accuracies on CASIA database are listed in Table 6.10. In the tables, the chaincode
direction feature is denoted by chn, continuous NCFE is denoted by ncf-c, and the
gradient feature on gray scale image by grd-g.
We can see that on either database, using either classiﬁer (Euclidean or MQDF),
the accuracies of 12-direction and 16-direction features are mostly higher than those
of 8-direction features. This indicates that increasing the resolution of direction
decomposition is beneﬁcial. The 16-direction feature, however, does not outperform
the 12-direction feature. To select a sampling mesh, let us focus on the results of 12-
direction features. We can see that by Euclidean distance classiﬁcation, the accuracies
of M1 (6 × 6) and M2 (7 × 7) are mostly higher than those of M3 (8 × 8), whereas
by MQDF, the accuracies of M2 and M3 are higher than those of M1. Considering
that M2 and M3 perform comparably while M2 has lower complexity, we take the
sampling mesh M2 with 12-direction features for following experiments. The original
dimensionality of direction features is now 12 × 7 × 7 = 588, which is reduced to
160 by LDA.
TABLE 6.8
Settings of sampling mesh for 8-direction, 12-direction, and 16-direction
features.
Mesh
M1
M2
M3
Zones
Dim
Zones
Dim
Zones
Dim
8-dir
7 × 7
392
8 × 8
512
9 × 9
648
12-dir
6 × 6
432
7 × 7
588
8 × 8
768
16-dir
5 × 5
400
6 × 6
576
7 × 7
784

298
CASE STUDIES
TABLE 6.9
Accuracies (%) of 8-direction, 12-direction, and 16-direction features on
ETL9B database.
Euclidean
MQDF
chn
M1
M2
M3
M1
M2
M3
8-dir
97.09
97.06
96.98
98.92
98.95
98.91
12-dir
97.57
97.44
97.46
98.98
99.00
99.03
16-dir
97.48
97.60
97.48
98.80
99.00
99.00
ncf-c
M1
M2
M3
M1
M2
M3
8-dir
97.39
97.39
97.29
99.07
99.13
99.11
12-dir
97.95
97.94
97.87
99.18
99.23
99.22
16-dir
97.95
97.96
97.89
99.02
99.15
99.21
grd-g
M1
M2
M3
M1
M2
M3
8-dir
97.41
97.42
97.34
99.07
99.10
99.11
12-dir
97.73
97.70
97.69
99.06
99.14
99.14
16-dir
97.71
97.81
97.75
98.92
99.06
99.15
6.2.6.4
Comparison of Normalization Methods
On ﬁxing the direction res-
olution (12-direction) and sampling mesh (7 × 7), we combine the three types of
direction features with ﬁve normalization methods: linear (LN), nonlinear (NLN),
moment (MN), bi-moment (BMN), and MCBA. The accuracies on the test sets of
two databases are listed in Tables 6.11 and 6.12, respectively. It is evident that the
TABLE 6.10
Accuracies (%) of 8-direction, 12-direction, and 16-direction features on
CASIA database.
Euclidean
MQDF
chn
M1
M2
M3
M1
M2
M3
8-dir
94.05
93.93
93.78
97.55
97.63
97.63
12-dir
94.66
94.56
94.47
97.66
97.74
97.79
16-dir
94.56
94.72
94.63
97.28
97.65
97.71
ncf-c
M1
M2
M3
M1
M2
M3
8-dir
94.69
94.65
94.51
98.06
98.08
98.00
12-dir
95.56
95.52
95.45
98.14
98.25
98.27
16-dir
95.47
95.58
95.48
97.83
98.08
98.18
grd-g
M1
M2
M3
M1
M2
M3
8-dir
94.69
94.66
94.59
97.95
97.99
98.03
12-dir
95.06
95.10
95.02
97.91
98.00
98.05
16-dir
95.02
95.15
95.20
97.58
97.91
98.02

OFFLINE HANDWRITTEN CHINESE CHARACTER RECOGNITION
299
TABLE 6.11
Accuracies (%) of various normalization methods on ETL9B database.
Euclidean
MQDF
Norm.
chn
ncf-c
grd-g
chn
ncf-c
grd-g
LN
93.64
94.06
94.03
97.62
97.91
97.89
NLN
97.44
97.94
97.70
99.00
99.23
99.14
MN
97.65
97.93
97.88
99.05
99.17
99.18
BMN
97.67
97.96
97.91
99.08
99.19
99.20
MCBA
97.48
97.81
97.73
99.00
99.16
99.14
linear normalization (LN) is inferior to the other four normalization methods, whereas
the difference of accuracies among the latter four normalization methods is smaller.
Comparing the normalization methods except LN, we can see that on both two
databases, BMN outperforms MN and MCBA. MN and MCBA perform comparably
on ETL9B database, but on CASIA database, MCBA outperforms MN. The perfor-
mance of NLN depends on the feature extraction method. With chaincode feature
and gradient feature, the accuracies of NLN are comparable to those of BMN and
MCBA. However, on both databases, the combination of NLN with continuous NCFE
gives the highest accuracy. This is because with nonsmooth coordinate mapping, NLN
yields larger shape distortion than the other normalization methods. The chaincode
feature and gradient feature, extracted from the normalized image, are sensitive to
the shape distortion. NCFE combines the contour direction feature of the original
image and the coordinate transformation of nonlinear normalization, thus yields the
best recognition performance.
6.2.6.5
Comparison of Classiﬁers
The inferior classiﬁcation performance of
Euclidean distance to MQDF has been manifested in the above experiments. Now,
we compare the performance of MQDF and LVQ classiﬁers with variable parameter
complexity on a selected normalization-feature combination, namely, 12-direction
continuous NCFE with bi-moment normalization (BMN). We experiment on the
CASIA database only. As shown in Table 6.12, the accuracies of Euclidean distance
and MQDF are 95.46 and 98.07%, respectively.
TABLE 6.12
Accuracies (%) of various normalization methods on CASIA database.
Euclidean
MQDF
Norm.
chn
ncf-c
grd-g
chn
ncf-c
grd-g
LN
88.62
89.54
89.69
95.89
96.51
96.46
NLN
94.56
94.52
95.10
97.74
98.25
98.00
MN
94.39
95.11
95.10
97.50
97.96
97.94
BMN
94.70
95.46
95.44
97.65
98.07
98.08
MCBA
94.52
95.27
95.17
97.69
98.13
98.04

300
CASE STUDIES
TABLE 6.13
Accuracies (%) of MQDF with variable number k of principal eigenvectors
and LVQ with variable number p of prototypes on CASIA database.
MQDF (k)
1
2
3
4
5
10
15
Accuracy (%)
96.44
96.92
97.13
97.3
97.40
97.75
97.87
MQDF (k)
20
25
30
35
40
45
50
Accuracy (%)
97.95
97.97
98.02
98.05
98.07
98.09
98.08
LVQ (p)
1
2
3
4
5
Accuracy (%)
96.82
97.15
97.21
97.22
97.12
The number of principal eigenvectors per class for MQDF classiﬁer is set equal
to k = 1, 2, 3, 4, 5, 10, 15, 20, 25, 30, 40, 45, 50, and the number of prototypes per
class for LVQ classiﬁer is set equal to p = 1, 2, 3, 4, 5. The class prototypes are
initialized to be the cluster centers of the training samples of each class and are
adjusted to optimize the minimum classiﬁcation error (MCE) criterion (see details in
Section 4.3.6) on all the training samples.
The test accuracies of MQDF with variable number of principal eigenvectors and
LVQ classiﬁer with variable number of prototypes are shown in Table 6.13. We can
see that as the number of principal eigenvectors increases, the classiﬁcation accuracy
of MQDF increases gradually and almost saturates when k ≥40. As for the LVQ
classiﬁer, the accuracy is maximized when each class has p = 3 prototypes. Using
more prototypes, the training samples are separated better, but the generalization
performance does not improve. When p = 1, the initial prototypes are class means,
which gives the test accuracy 95.46% (Euclidean distance). By prototype optimization
in LVQ, the test accuracy is improved to 96.82%.
Comparing the performance of MQDF and LVQ classiﬁers, it is evident that the
highest accuracy of LVQ is lower than that of MQDF. However, the MQDF needs
a large number of parameters to achieve high accuracies. For M-class classiﬁcation
with k principal eigenvectors per class, the total number of vectors to be stored in
the parameter database is M · (k + 1) (the class means are stored as well). For LVQ
classiﬁer, the total number of vectors is M · p. The computational complexity in
recognition is linearly proportional to the total number of parameters. Thus, the run-
time complexity of MQDF with k principal eigenvectors per class is comparable to that
of LVQ classiﬁer with k + 1 prototypes per class. When we compare two classiﬁers
with similar complexity, say k = 1 versus p = 2, . . . , k = 4 versus p = 5, we can see
that when the parameter complexity is low (k ≤3), the LVQ classiﬁer gives higher
classiﬁcation accuracy than the MQDF. So, we can say that the LVQ classiﬁer provides
better trade-off between the classiﬁcation accuracy and the run-time complexity.
That the MQDF and LVQ classiﬁers have different trade-off between accuracy
and run-time complexity is due to the different training algorithms. The parameters
of MQDF are estimated generatively: each class is estimated independently; whereas
the parameters of LVQ classiﬁer are trained discriminatively: all parameters are ad-
justed to separate the training samples of different classes. Discriminative classiﬁers

SEGMENTATION AND RECOGNITION OF HANDWRITTEN DATES
301
give higher classiﬁcation accuracy at low complexity of parameters, but are time
consuming in training, and needs large number of training samples for good gen-
eralization. In the case of training on CASIA database on personal computer with
3.0 GHz CPU, the training time of MQDF is about 1 h (computation mainly for
diagonalization of covariance matrices), whereas the training time of LVQ with
acceleration by two-level rival prototype search is over 4 h.
6.2.7
Concluding Remarks
We introduced an ofﬂine handwritten Chinese character recognition (HCCR) system
and evaluated the performance of different methods in experiments. For feature ex-
traction, we justiﬁed the superiority of feature blurring over zoning, the advantage
of continuous NCFE over discrete NCFE, and the advantage of gradient feature on
gray-scale image over that on binary image. In comparing direction features with
variable sampling resolutions, our results show that the 12-direction feature has
better trade-off between accuracy and complexity. The comparison of normaliza-
tion methods shows that bi-moment normalization (BMN) performs very well, and
line-density-based nonlinear normalization (NLN) performs well when combined
with NCFE. The comparison of classiﬁers shows that the LVQ classiﬁer gives better
trade-off between accuracy and run-time complexity than the MQDF. The MQDF
yields high accuracies at very high run-time complexity.
The current recognition methods provide fairly high accuracies on handprinted
characters, say, over 99.0% on the ETL9B database and over 98.0% on the CASIA
databases. The recognition of unconstrained handwritten Chinese characters has not
been studied extensively because a public database of such samples is not available.
We can expect that on unconstrained characters, the accuracies will be much lower,
and so, efforts are needed to improve the current methods and design new methods.
Better performance can be expected by using P2D normalization methods [34].
Despite the superiority of direction features, even better features should be automat-
ically selected from a large number of candidate features. Thus, feature selection
techniques will play an important role in future research of character recognition.
For classiﬁcation, many discriminative classiﬁers have not been exploited for large
category set. As a generative classiﬁer, the MQDF is promising for HCCR, particu-
larly for handprinted characters. Recently, even higher accuracies have been achieved
by discriminative learning of feature transformation and quadratic classiﬁer parame-
ters [36, 37]. Discriminative classiﬁer design for large category problems will be an
important direction in the future.
6.3
SEGMENTATION AND RECOGNITION OF HANDWRITTEN DATES
ON CANADIAN BANK CHEQUES
This case study describes a system developed to recognize date information hand-
written on Canadian bank checks. A segmentation-based strategy is adopted in this
system. In order to achieve high performance in terms of efﬁciency and reliability, a

302
CASE STUDIES
knowledge-based module is proposed for the date segmentation and a cursive month
word recognition module is implemented based on a combination of classiﬁers. The
interaction between the segmentation and recognition stages is properly established
by using multihypotheses generation and evaluation modules. As a result, promising
performance is obtained on a test set from a real-life standard check database.
6.3.1
Introduction
Research on OCR began in the 1950s, and it is one of the oldest research areas in
the ﬁeld of pattern recognition. Nowadays, many commercial systems are available
for reliably processing cleanly machine-printed text documents with simple layouts,
and some successful systems have also been developed to recognize handwritten
texts, particularly, isolated handprinted characters and words. However, the analysis
of documents with complex layouts, recognition of degraded machine-printed texts,
and the recognition of unconstrained handwritten texts demand further improvements
through research. In this case study, our research aims at developing an automatic
recognition system for unconstrained handwritten dates on bank checks, which is a
very challenging topic in OCR.
The ability to recognize the date information handwritten on bank checks is very
important in application environments (e.g., in Canada) where checks cannot be pro-
cessed prior to the dates shown. At the same time, date information also appears on
many other kinds of forms. Therefore, there is a great demand to develop reliable
automatic date processing systems.
The main challenge in developing an effective date processing system stems from
the high degree of variability and uncertainty in the data. As shown in Figure 6.20,
people usually write the date zones on check in such free styles that little a priori
knowledge and few reliable rules can be applied to deﬁne the layout of a date image.
For example, the date ﬁelds can contain either only numerals or a mixture of alphabetic
letters (for month) and numerals (for day and year), punctuations, sufﬁxes, and the
article “Le” may also appear. (The dates can be written in French or in English in
Canada, and a “Le” may be written at the beginning of a French date zone.)
Perhaps because of this high degree of variability, there has been no published
work on this topic until the work on the date ﬁelds of machine-printed checks was
reported in 1996 [14]. This reference also considered date processing to be the most
difﬁcult target in check processing, given that it has the worst segmentation and
FIGURE 6.20
Sample dates handwritten on standard Canadian bank checks.

SEGMENTATION AND RECOGNITION OF HANDWRITTEN DATES
303
recognition performance. In 2001, a date processing system for recognizing hand-
written date images on Brazilian checks was presented in [40]. A segmentation-free
method was used in this system, that is, an HMM (Hidden Markov Model) based
approach was developed to perform segmentation in combination with the recognition
process.
The system addressed in this case study (which is an extension of a previous
work [5]) is the only published work on processing of date zones on Canadian bank
checks. In our system, date images are recognized by a segmentation-based method,
that is, a date image is ﬁrst segmented into day, month, and year, the category of
month (alphabetic or numeric) is identiﬁed, and then an appropriate recognizer is
applied for each ﬁeld. In the following, the main modules of the whole system will
be discussed, together with some experimental results.
6.3.2
System Architecture
Since the date image contains ﬁelds that may belong to different categories (alphabetic
or numeric), it is difﬁcult to process the entire date image at the same time efﬁciently.
Therefore, a segmentation-based strategy is employed in our system.
The main procedures in the system consist of segmenting the date image into ﬁelds
through the detection of the separator or transition between the ﬁelds, identifying the
nature of each ﬁeld, and applying an appropriate recognizer for each.
Figure 6.21 illustrates the basic modules in our date processing system. In addi-
tion to the two main modules related to segmentation and recognition, a preprocessing
module has been designed to deal with simple noisy images and to detect and process
possible appearances of “Le.” In the postprocessing stage, in order to further im-
prove the reliability and performance of the system, a two-level veriﬁcation module
is designed to accept valid and reliable recognition results, and to reject the others.
In order to improve the performance and efﬁciency of the system, a knowledge-
based segmentation module is used to solve most segmentation cases in the segmen-
tation stage. Ambiguous cases are handled by a multihypotheses generation module
at this stage, for a ﬁnal decision to be made when more contextual information and
syntactic and semantic knowledge are available, that is, multihypotheses evaluation
is made in the recognition stage.
6.3.3
Date Image Segmentation
Figure 6.22 shows an overview of the date segmentation module. As described above,
the date image segmentation module divides the entire image into three subimages
corresponding to day, month, and year, respectively, and also makes a decision on
how month is written so that it can be processed using an appropriate recognizer.
Since there is no predeﬁned position for each ﬁeld, and no uniform or even obvious
spacing between the ﬁelds, it is difﬁcult to implement the segmentation process with
a high success rate by using simple structural features.
The ﬁrst step of our segmentation module is to separate year from day&month
based on structural features and the characteristics of the Year ﬁeld, and then the

304
CASE STUDIES
FIGURE 6.21
Diagram of date processing system.
knowledge-based segmentation module and the multihypotheses generation and
evaluation modules are applied in the day&month segmentation stage.
6.3.3.1
Year Detection
Based on our database analyses (CENPARMI Check
database and CENPARMI IRIS Check database), the writing styles of date zones
on Canadian bank checks can be grouped into two categories, which are deﬁned as
standard format and free format in this case study. The standard format is used when
“2” and “0” or “1” and “9” are printed as isolated numerals on the date zone indicating
the century and the free format is adopted when the machine-printed “20” or “19” does
not appear on a date zone. Since about 80% of date zones are of the standard format
in our databases, year with the standard format is ﬁrst detected in the year detection
module. If the detection is not successful, year with the free format is detected.
The detection of year for the standard format is based on the detection of the
machine-printed “20” or “19.” For detecting year with the free format, we ﬁrst assume:
(i) year is located at one end of the date zone; (ii) year belongs to one of the two
patterns: 20∗∗(or 19∗∗) and ∗∗, where ∗is a numeral; and (iii) a separator is used by
writers to separate a year ﬁeld from a day&month ﬁeld. Based on these assumptions,
we use a candidate then conﬁrmation strategy to detect the year with the free format.
Year candidates are ﬁrst detected from the two ends of the date zone, and then one of
the candidates is conﬁrmed as the year by using the recognition results from a digit
recognizer and by using the assumption that a year ﬁeld contains either four numerals
starting with “20” (or “19”) or two numerals. Here year candidates are obtained by
detecting the separator (a punctuation or a big gap) between year and day&month

SEGMENTATION AND RECOGNITION OF HANDWRITTEN DATES
305
FIGURE 6.22
Diagram of date image segmentation.
ﬁelds, and these separator candidates are detected from structural features [5, 45].
In the conﬁrmation stage, the conﬁdence value for a year candidate with the pattern
20∗∗(or 19∗∗) is considered to be higher than that of a year candidate with the
pattern ∗∗.
6.3.3.2
Knowledge-Based Day&Month Segmentation
The tasks of this
knowledge-based segmentation module include (i) detecting the separator between
day and month; and (ii) segmenting the day&month ﬁeld into day and month, and
identifying the category of the month.
For the separator detection, the separators can be punctuations, such as slash “/,”
hyphen “-,” comma “,” and period “.,” or big gaps, and a candidate then conﬁrmation
strategy is used here to detect them. Separator candidates are ﬁrst detected by shape

306
CASE STUDIES
and spatial features [5, 45]. While some of the candidates with high conﬁdence
values of the features can be conﬁrmed immediately, others should be evaluated
by considering more information.
Based on our database analyses, some relationship between the type of separator
and the writing style of day&month has been found, for example, slashes or hyphens
usually appear when both day and month are written in numerals. So some separator
candidates are easily conﬁrmed or rejected using a set of rules if the knowledge about
the writing style can be obtained [45]. Furthermore, these rules can be designed in the
training stage based on human knowledge and syntactic constraints, and in general
they can be encoded in a pattern-based grammar. Here pattern means image pattern,
and usually a day&month subimage can appear in any one of the three patterns: NSN,
NSA, and ASN, where S denotes the separator, N denotes a numeric string (day or
month ﬁeld), and A denotes an alphabetic string (month ﬁeld). The pattern-based
grammar used to detect the separator can be expressed as
Given image Pattern:
If < condition > then < action >
< condition >⇒the separator candidate (S) in the image Pattern is P, P ∈
< action >⇒Conﬁdence Adjustment|
Add New Feature|
Adjust Segmentation Method|. . .
Here  represents the set of separator types. Some explanations about this pattern-
based grammar are given in Table 6.14, where day&month image “patterns” are given
as the entries. When we try to conﬁrm a separator candidate and the “condition” is
that the separator candidate is a “/ ” or “−” or . . . , we can take the corresponding
“action.”
For this knowledge-based method, two approaches have been developed in our
system to determine the writing styles. The ﬁrst method adopts a distance to numeral
measure to represent the likelihood of a subimage in day&month ﬁeld being numeric,
and it is based on feedbacks of a digit recognizer and structural features. A system
of combining multiple multilayer perceptron (MLP) networks is the second method
to realize this writing style analysis task. More details about these two methods can
TABLE 6.14
Examples of condition-action rules.
Pattern
Condition
Action
NSN
−
“−” candidate is conﬁrmed.
NSA
/
New “long” features are checked.
ASA
NULL
Separator candidate is not conﬁrmed.
. . .
. . .
. . .

SEGMENTATION AND RECOGNITION OF HANDWRITTEN DATES
307
be found in [45] and [26], respectively. The effectiveness of these two methods has
been proven in the experiments. However, their results may be inconclusive in some
ambiguous segmentation cases, where the multihypotheses generation and evaluation
are introduced.
After the separator detection step, day&month segmentation and identiﬁcation can
be conducted. Based on the separators detected, a set of rules have been developed
to segment the day&month ﬁeld into day and month ﬁelds and to determine whether
the month ﬁeld is written in numeric or alphabetic form [5, 45].
6.3.3.3
Multihypotheses Generation
In the knowledge-based day&month
segmentation module, only the separators with high conﬁdence values and the writing
styles with high conﬁdence values to indicate “common styles” would be conﬁrmed.
Here “common styles” are determined based on database analyses, including that
the gap separator usually occurs at the transition between numeric and alphabetic
ﬁelds, the subimages on both sides of slash or hyphen are often numeric, and a
period separator is usually used in ASN pattern. Otherwise, the multihypotheses
generation module is activated. This module produces and places multiple hypotheses
in a multihypotheses list, where each hypothesis consists of a possible segmentation
of day&month ﬁeld.
6.3.3.4
Multihypotheses Evaluation
Each possible segmentation in the
multihypotheses list includes a separator candidate and segments on both sides of
the separator candidate. For each such hypothesis, the multihypotheses evaluation
module estimates its conﬁdence values for the three writing styles or types (NSN,
NSA, or ASN) as the following weighted sums:
ConfidenceType1 = w1 ∗DigitConfidenceLeft
+ w1 ∗DigitConfidenceRight
+ w3 ∗SeparatorConfidence
ConfidenceType2 = w1 ∗DigitConfidenceLeft
+ w2 ∗WordConfidenceRight
+ w3 ∗SeparatorConfidence
ConfidenceType3 = w2 ∗WordConfidenceLeft
+ w1 ∗DigitConfidenceRight
+ w3 ∗SeparatorConfidence
DigitConfidenceLeft and DigitConfidenceRight are conﬁdence values from a
digit recognizer for the left and right sides of the separator candidate, respectively.
WordConfidenceLeft and WordConfidenceRight are conﬁdence values from a cursive
month word recognizer. SeparatorConfidence is the conﬁdence value of the separa-
tor candidate, which is derived from the segmentation stage. The weights w1, w2, and
w3 are determined in the training stage. For example, w1 = 0.8, and w2 = 1 because
the distribution of conﬁdence values from the digit recognizer is different from that

308
CASE STUDIES
of the word recognizer, and these weights are set to make the conﬁdence values of
the digit and word recognizers comparable.
For each hypothesis in the list, usually the Type with the maximum ConfidenceType
value is recorded to be compared with the corresponding information from other
hypotheses in the list. In the application, some semantic and syntactic constraints
can be used to improve the performance of this multihypotheses evaluation module.
First, based on semantic constraints, if the recognition result from a Type is not a valid
date, the corresponding ConfidenceType would be reduced by a small value (α) before
ConfidenceType values are compared in Type selection. In addition, as we discussed
above, “common styles” have been determined based on database analyses. These
“common styles” can be used as syntactic constraints to modify the Type selection
procedure, that is, if the interpretation of the ﬁrst choice is not a “common style” and
the difference between the conﬁdence values of the top two choices is very small, the
second choice that has the second largest ConfidenceType value should be the ﬁnal
selection if the interpretation of this choice is a “common style” and is a valid date.
6.3.4
Date Image Recognition
In the date recognition stage, if one segmentation hypothesis can be conﬁrmed in the
segmentation stage, an appropriate recognizer (digit recognizer or cursive month word
recognizer) is invoked for each of day, month and year ﬁelds. Otherwise, the multi-
hypotheses evaluation module that makes use of the results from the digit and word
recognizers is invoked. The digit recognizer used in our date processing system was
originally developed for processing the courtesy amount written on bank checks [44],
and a 74% recognition rate (without rejection) was reported for processing these
courtesy amounts. For the recognition of cursive month words, a new combination
method with an effective conditional topology has been implemented in our date pro-
cessing system. More discussions about this cursive month word recognizer are given
below.
6.3.4.1
Introduction to Cursive Month Word Recognizers
The recogni-
tion of month words on bank checks poses some new problems in addition to the
challenges caused by the high degree of variability in unconstrained handwriting.
First, improper binarization and preprocessing can have a great impact on the quality
of month word images. Second, although a limited lexicon is involved, many
similarities exist among the month word classes, and this can give rise to problems in
feature extraction and classiﬁcation. Furthermore, month words on bank checks can
be written in French or in English in Canada, which increases the complexity of the
problem. In addition, most month words have very short abbreviated forms, which
make it more difﬁcult to differentiate between them.
According to our literature review, only two systems have been reported for month
word recognition. In [40], a system was developed for handwritten month word
recognition on Brazilian bank checks by using an explicit segmentation-based HMM
classiﬁer. Another system mentioned in [19] is the previous work of this case
study, which deals with English month word recognition by combining an MLP

SEGMENTATION AND RECOGNITION OF HANDWRITTEN DATES
309
FIGURE 6.23
Month word samples from CENPARMI IRIS check database.
classiﬁer and an HMM classiﬁer. Based on the previous work, some improvements
and modiﬁcations have been made in our current recognition system to recognize both
French and English month words. An effective conditional combination topology is
presented to combine two MLP and one HMM classiﬁers, and a proposed modiﬁed
Product fusion rule gives the best recognition rate so far.
6.3.4.2
Writing Style Analysis for Cursive Month Word Recognition
In
this section, the writing styles of month words are analyzed based on CENPARMI
bank check databases. Some samples extracted from the databases are shown in
Figure 6.23, including both English and French month words.
Altogether, 33 English/French month word classes have been observed in the
databases including full and abbreviated forms. Based on analyses of the databases,
many similarities have been found among the word classes:
 The most similar classes are: “September” and “Septembre”, “October” and
“Octobre”, “November” and “Novembre”, and “December” and “D´ecembre”.
Since each pair of classes are very similar in shape and they represent the same
month, they are assigned to the same class. So only 29 classes will be considered.
 Some other similarities among the words are due to their shapes, for example,
“Jan,” “June” and “Juin,” and “Mar,” “Mai,” and “Mars.” They can also contain
similar subimages, for example, “September,” “November” and “December.”
These similarities can affect the performance of the recognition systems, and
they have been considered in the feature extraction and individual classiﬁer
design.
From analyses of the databases, we also ﬁnd that about one third of the month word
classes consist of only three letters, mostly from abbreviations. In addition, French
month words have been found to have freer writing styles, for example, both capital
and lowercase letters can be used at the beginning of words, and mixtures of capital
and lowercase letters appear more frequently.
These writing style analyses show some of the challenges for month word recog-
nition. Some solutions to this problem will be given in the following sections using
three individual classiﬁers and a combination system.
6.3.4.3
Individual Classiﬁers
In this section, three individual month word
classiﬁers will be discussed, including two MLP classiﬁers and one HMM classiﬁer.

310
CASE STUDIES
(1) MLP classiﬁers
Two MLP classiﬁers have been developed in CENPARMI for month word recog-
nition. MLPA and MLPB were originally designed for the recognition of the legal
amount [20] and the courtesy amount [44], respectively, and both of them have been
modiﬁed for month word recognition. The two MLPs use a holistic approach. Their
architectures and features can be summarized below. More details can be found from
the references cited.
 MLPA is a combination of two networks using different features. The combina-
tion adopts the “fusion” scheme, that is, MLPA is implemented by combining the
two networks at an architectural level, and uses the outputs of the neurons in two
hidden layers as new input features [20]. There are two feature sets designed for
the two networks. The feature set 1 consists of mesh features, direction features,
projection features, and distance features, whereas the feature set 2 consists of
gradient features.
 MLPB is a combination of three MLPs. The combination scheme is a “hybrid”
strategy that combines the output values of the MLPs. The MLPs are
implemented by using three different sets of input features. Feature set
1 consists of pixel distance features (PDF), whereas feature sets 2 and 3 consist
of size-normalized image pixels from different preprocessed images [44].
(2) HMM classiﬁer
A segmentation based grapheme level HMM has been developed using an analytical
feature extraction scheme for month word recognition [19]. This model has the
potential to solve the over- or under-segmentation problem in cursive script
recognition. The features used to obtain pseudotemporal sequences for the HMM
come from shape feature, direction code distribution feature, curvature feature, and
moment feature.
(3) Performances
The training set for implementing the individual month word classiﬁers consists of
6201 month word samples, and the test set consists of 2063 month word samples.
Both the training set and the test set were extracted from CENPARMI databases
(CENPARMI check database and CENPARMI IRIS check database).
The performances of the two MLP classiﬁers and the HMM classiﬁer on the test
set are shown in Table 6.15. In the table, the 12 outputs mean mapping of the month
words into the 12 months of a year. These results indicate that the two MLP classiﬁers
producecomparablerecognitionrates,whereastheHMMclassiﬁerproducestheworst
results. In addition, performance improvements can be noticed from these results
when the number of output classes is changed from 29 to 12. This is a result of the
fact that some month words of different classes, but representing the same month,
can have very similar shapes (e.g., “Mar,” “Mars,” “March,” etc). Samples of these
month words are sometimes recognized as words of different classes but belonging
to the same month, which become correct classiﬁcations when the output classes are
reduced to 12.

SEGMENTATION AND RECOGNITION OF HANDWRITTEN DATES
311
TABLE 6.15
Performances of individual classiﬁers.
Recognition rate (%)
Classiﬁer
29 outputs
12 outputs
MLPA
76.44
78.87
MLPB
75.42
77.27
HMM
66.89
69.70
In order to enhance the recognition performance, combinations of these classiﬁers
are considered in the following section.
6.3.4.4
Combinations
Combination systems are expected to produce better
recognition results. Before discussing the combination topology and combination
rule for month word classiﬁcation, further analyses of the performances of the three
individual classiﬁers are presented while focusing on the correlation of the recognition
results among these classiﬁers. Some useful information has been obtained from these
analyses.
(1) Correlation of recognition results among individual classiﬁers
The correlation of recognition results among the three individual classiﬁers is shown
in Table 6.16. It was obtained from an additional training set (2063 samples extracted
from CENPARMI check databases) separate from the training set for training the
individual classiﬁers. This additional training set is also used in the experiments for
developing the combination topology and combination rules to be discussed later.
In Table 6.16, “correct” means all the classiﬁers are correct on the sample, “error”
means all the classiﬁers are incorrect, and “C/E” means both correct and wrong results
are produced by the classiﬁers. The errors can be further grouped into two classes
ErrorI and ErrorII, which mean the samples are assigned different and the same
incorrect classes by the classiﬁers, respectively. From these results, we can infer the
following:
 Even though combining these three classiﬁers may improve the recognition rate
considerably due to the independence in errors, a combination would have a
lower bound of 1.26% (26/2063) for the error rate without rejections, because
all the three classiﬁers produce the same 26 errors.
TABLE 6.16
Correlation of recognition results among classiﬁers.
Classiﬁers
Correct
Error
C/E
ErrorI
ErrorII
MLPA and MLPB
1297
326
440
213
113
MLPA and HMM
1162
282
619
217
65
MLPB and HMM
1167
295
601
218
77
MLPA, MLPB, and HMM
1032
199
832
173
26

312
CASE STUDIES
FIGURE 6.24
Conditional combination topology for month word recognition.
 If two classiﬁers are to be combined, the combination of MLPA and MLPB
makes the same errors more frequently than the other combinations, which can
be observed from the ErrorIIs produced. Although the performances of both
MLPA and MLPB are much better than that of the HMM classiﬁer, they are
homogeneous classiﬁers and tend to produce the same errors, which is not useful
in combination. The idea that more distinct classiﬁers can better complement
each other has been shown here, and it is very important for designing combina-
tion systems. In addition, three classiﬁers would be less likely to make the same
errors than two of them as observed from the ErrorIIs.
(2) Combination topology
Combination topologies can be broadly classiﬁed as parallel, serial, hybrid, and
conditional. Based on considerations of both speed and accuracy, as well as experi-
mental ﬁndings, a conditional combination topology is proposed here to combine the
three classiﬁers to improve the performance of month word recognition as shown in
Figure 6.24.
In this architecture, MLPA and MLPB are ﬁrst applied using serial strategy in
the ﬁrst stage; for samples rejected by both MLPA and MLPB, the decisions of all
the three classiﬁers are combined in the second stage. The rejection conditions of
both MLPA and MLPB have been set very strictly in the ﬁrst stage (total error rate
introduced in the ﬁrst stage is 0.6% on the test set). A rejection is made in MLPA
when the top conﬁdence value is less than a threshold (0.99), and a rejection is made
in MLPB when the difference between the top two conﬁdence values is less than a
threshold (0.4). This stage proved to be very efﬁcient and effective based on the results

SEGMENTATION AND RECOGNITION OF HANDWRITTEN DATES
313
of our experiments. About 33% of the samples can be recognized in the ﬁrst stage.
More difﬁcult samples are rejected and sent to the three classiﬁers in parallel, and
decisions from these classiﬁers are combined to get better results. Since the samples
have already been processed by MLPA and MLPB by this stage, additional processing
is required only from the HMM classiﬁer. Combination rules used in the second stage
will be discussed below.
(3) Combination rule
Many strategies have been designed and implemented by various researchers to com-
bine parallel classiﬁers with measurement outputs. For example, the sum, product,
maximum, minimum, median, and voting rules have been studied extensively [24].
The main advantages of these rules are their simplicity and that they do not require any
training. Three of these widely used combination rules have been applied in our month
word recognition module. These are the majority vote on decisions, ave (sum) and
product. Since detailed descriptions of these rules can be found in the literature [24],
here we only describe the weighted product rule used in our system. Let x be an input
sample, and Dk(x), where k = 1, 2, 3, be the decision on x by the three classiﬁers
MLPA, MLPB, and HMM. Ck,wj(x), where k = 1, 2, 3 and j = 1, 2, . . . , 29 repre-
sent the conﬁdence values assigned to class wj by classiﬁer k. D(x) and Cwj(x) are
the ﬁnal combined result and conﬁdence, respectively. The weighted product rule can
be described as follows:
Cwj(x) = 43
k=1(Ck,wj(x))Weightk, j = 1, 2, . . . , 29,
D(x) = wj, if Cwj(x) = max29
i=1(Cwi(x)).
Here the weighting factors Weightks are determined in the training stage, and they
are chosen to make the conﬁdence values of the three classiﬁers comparable because
the distributions of the conﬁdence values for the three classiﬁers are different.
Experiments have been conducted to compare these combination rules based on
the conditional topology implemented. The results are shown in Table 6.17 without
rejections.
Among these three combination rules, weighted product rule is superior to the other
two. However, based on error analyses, a problem of product rule is found regarding
the effect of very low output (conﬁdence) values, which is also reported in [2] as veto
effect. A modiﬁed product rule is proposed in this case study to reduce this effect,
which will be discussed below.
TABLE 6.17
Performance of different combination rules on the test set.
Recognition rate (%)
Combination
29 outputs
12 outputs
Majority vote
76.59
79.01
sum
79.69
81.39
Weighted product
84.97
86.67

314
CASE STUDIES
(4) Modiﬁed product rule
The veto effect in the product rule is caused by small classiﬁer measurement output
values dominating the product, that is, by giving close to zero values. The basic idea
is to modify the output of a classiﬁer if it falls below a speciﬁed threshold. A modiﬁed
product (Mproduct) has been presented in [2]. They suggested that if the outputs of
more than half classiﬁers are larger than a threshold for a class, then the output of a
classiﬁer which is less than the threshold should be modiﬁed by setting the output to
the threshold. Better results have been obtained in their experiments. However, they
did not give the method for selecting the threshold value. In our experiments, we ﬁnd
that the performance is sensitive to this threshold and it is hard to make a decision
except using heuristics.
Based on the above method, a new modiﬁed product rule is proposed in our
combination system. This method can be described as follows:
If two of Ck,wj(x), k = 1, 2, 3 on class wj are larger than a high threshold (thr1)
and the other Cm,wj(x) is less than a low threshold (thr2), j = 1, 2, . . . , 29
Cm,wj(x) = thr2.
else
Ck,wj(x), where k = 1, 2, 3 and j = 1, 2, . . . , 29 remain unaltered.
After the above recomputation of Ck,wj(x), the same steps as shown in the
previous weighted product rule are adopted. Here the modiﬁcation of using two
different thresholds has been made. The method of choosing these two thresholds
in our system is as follows:
 thr1 is larger than thr2, but it is not a real “high” threshold. We set thr1 to a
value such that when a classiﬁer measurement output is larger than this value,
this output is very often among the top three choices of all the classiﬁers in the
system.
 thr2 is not a ﬁxed value. The measurement output value of the ﬁfth choice of the
classiﬁer considered is used as the threshold in our system. (If the value is zero,
then thr2 is set to the small value of 0.05 instead.)
In this modiﬁed product rule, the thresholds are related to the measurement outputs
in the system. It is a reasonable process that provides an automatic setting of the
thresholds. Based on this new modiﬁed product rule, better results have been obtained
for our month word recognition as shown in Table 6.18.
TABLE 6.18
Combination results of modiﬁed product rule on the test set.
Recognition rate (%)
Combination
29 outputs
12 outputs
Modiﬁed product
85.36
87.06

SEGMENTATION AND RECOGNITION OF HANDWRITTEN DATES
315
This is the best result produced by our month word recognition system. It is difﬁcult
to compare this result with the other systems [19, 40] due to the use of different
databases. In [40], a 91% recognition rate for 12 Brazilian month word classes has
been reported on a small test set (402 samples). In another system [19], 21 English
classes are recognized, and an 87.3% recognition rate was obtained on a test set of
2152 samples. Because both French and English month words are processed in our
system, a total of 29 classes are involved, giving rise to greater complexities. The
recognition rate of 85.36% for 29 classes in our system is very promising.
Comparing the combination results obtained here with those of the individual
classiﬁers, signiﬁcant performance improvements have been observed. In addition,
some experiments have been conducted to compare the effect of combining three
or two classiﬁers. Combining the MLPA and HMM classiﬁers using the weighted
product rule produces the best results among the combinations of two classiﬁers,
with recognition rates of 83.18% and 85.26% for 29 and 12 outputs, respectively,
which is not as good as those of combining the three classiﬁers.
6.3.5
Experimental Results
Several experiments have been designed to test the performance of our date processing
system. Since the performance of the cursive month word recognition has been
discussed above, the performances of only the date segmentation module and the
entire system will be given below.
6.3.5.1
Date Image Segmentation
The test set is derived from the
CENPARMI IRIS check database, and it contains 3399 date images, of which 1219
samples are written in English, and the other 2180 samples are written in French. The
segmentation results based on different rejection thresholds are given in Table 6.19
for both English and French samples. Some discussions on these results are given
below.
 The Rejection. Rejection rate 1 is obtained by trying to recognize every date
sample,andarejectionismadewhenatleastoneofthethreeﬁeldscorresponding
to year, month, and day cannot be found, for example, day&month is written or
binarized as one component or the year ﬁeld cannot be found. If strong noise such
TABLE 6.19
Performances of date segmentation system for the English and French sets.
English
Correct(%)
Rejection(%)
Error(%)
Rejection rate 1
90.40
1.81
7.79
Rejection rate 2
74.57
21.16
4.27
French
Correct(%)
Rejection(%)
Error(%)
Rejection rate 1
82.94
4.22
12.84
Rejection rate 2
65.69
26.97
7.34

316
CASE STUDIES
as a big blob of ink (due to improper binarization) or too many components have
been detected in the preprocessing stage, a rejection is also made. For rejection
rate 2, the recognition result should be a valid date and the average conﬁdence
value of the three ﬁelds should exceed a threshold; otherwise, a rejection is
made.
 The Performance. The performance for the English set is better than that for the
French set. Several reasons can account for this difference. First, based on the
database analysis, it was found that more French checks have the free format.
With this format, more variations exist and detecting the handwritten year is
more difﬁcult than detecting the machine-printed “20” or “19.” Therefore, more
errors and rejections occur. In addition, the article “Le” sometimes used at the
beginning of French date zones, together with freer writing styles on French
checks, also increase the difﬁculty of segmentation.
 Error Analysis. Based on our experiments in the training stage, the errors made
can be categorized into two classes. The ﬁrst class contains date images of very
poor quality, and it includes: (i) date images having touching ﬁelds; (ii) strong
noise introduced by improper binarization; and (iii) incomplete dates with one
of the three ﬁelds missing. Our current segmentation module cannot process
this type of date image, so a rejection is often made when rejection rate 2 is
imposed. Based on the experiments, about 40% of the errors belong to this type
when rejection rate 1 is adopted. The second class of errors consist of errors
generated by all segmentation modules in the system.
 The Efﬁciency. The date segmentation is divided into two stages in order to
improve the performance and efﬁciency of the system. The knowledge-based
segmentation module is used in the ﬁrst stage to solve most segmentation cases.
Ambiguous cases are handled by multihypotheses generation and evaluation
modules in a later stage. Experimental results show that 74.19% of the date
images in English and 71.41% of the date images in French are processed in
the knowledge-based segmentation module, and the others are processed by the
multihypotheses generation and evaluation modules.
6.3.5.2
Overall Performances
The overall performances are given in
Table 6.20, where the rejections are made under the same conditions as in Table 6.19.
TABLE 6.20
Performances of date processing system for the English and French sets.
English
Correct (%)
Rejection (%)
Error (%)
Rejection rate 1
62.34
1.81
35.85
Rejection rate 2
61.69
21.16
17.15
French
Correct (%)
Rejection (%)
Error (%)
Rejection rate 1
57.75
4.22
38.03
Rejection rate 2
53.67
26.97
19.36

REFERENCES
317
The errors of the date processing system mainly come from segmentation, month
word misrecognition and/or numeral misrecognition. Currently a more effective
veriﬁcation module in the postprocessing stage is being considered to reduce the
error rates and improve the recognition rates. Also, new checks have been designed
and used in practice to facilitate recognition and reading. They include boxes for
entering the day, month, and year on the check.
6.3.6
Concluding Remarks
This case study proposes a system for automatically recognizing the date informa-
tion handwritten on Canadian bank checks. In the system, the date segmentation is
implemented at different levels using knowledge obtained from different sources.
Simple segmentation cases can be efﬁciently solved by using the knowledge-based
segmentation module, which makes use of some contextual information provided
by writing style analyses. For ambiguous segmentation cases, the multihypotheses
generation and evaluation modules are invoked to make the ﬁnal decision based on
the recognition results and semantic and syntactic constraints. In addition, a new
cursive month word recognizer has been implemented based on a combination of
classiﬁers. The complete system has produced promising performance on a test set
from a real-life standard check database.
REFERENCES
1. Y. Al-Ohali, M. Cheriet, and C. Suen. Databases for recognition of handwritten
Arabic cheques. In Proceedings of the 7th International Workshop on Frontiers of Hand-
writing Recognition, Amsterdam, The Netherlands, 2002, pp. 601–606.
2. F. M. Alkoot and J. Kittler. Improving the performance of the product fusion strategy.
In Proceedings of the 15th International Conference on Pattern Recognition, Barcelona,
Spain, September 2000, Vol. 2, pp. 164–167.
3. R. Casey and G. Nagy. Recognition of printed Chinese characters. IEEE Transactions on
Electronic Computers. 15(1), 91–101, 1966.
4. J. X. Dong, A. Krzyzak, C. Y. Suen. An improved handwritten Chinese character recogni-
tion system using support vector machine. Pattern Recognition Letters. 26(12), 1849–1856,
2005.
5. R. Fan, L. Lam, and C. Y. Suen. Processing of date information on cheques. In A. C.
Downton and S. Impedovo, editors, Progress in Handwriting Recognition, World Scien-
tiﬁc, Singapore, 1997, pp. 473–479.
6. H. C. Fu and Y. Y. Xu. Multilinguistic handwritten character recognition by Bayesian
decision-based neural networks. IEEE Transactions on Signal Processing. 46(10), 2781–
2789, 1998.
7. K. Fukunaga. Introduction to Statistical Pattern Recognition, 2nd edition. Academic Press,
1990.
8. D. E. Goldberg, B. Korb, and K. Deb. Messy genetic algorithms: motivation, analysis, and
ﬁrst results. Complex Systems. 3(5), 493–530, 1989

318
CASE STUDIES
9. N. Hagita, S. Naito, and I. Masuda. Handprinted Chinese characters recognition by periph-
eral direction contributivity feature. Transactions on IEICE Japan. J66-D(10), 1185–1192,
1983 (in Japanese).
10. M. Hamanaka, K. Yamada, and J. Tsukumo. Normalization-cooperated feature extraction
methodforhandprintedKanjicharacterrecognition.InProceedingsofthe3rdInternational
Workshop on Frontiers of Handwriting Recognition, Buffalo, NY, 1993, pp. 343–348.
11. R. V. D. Heiden and F. C. A. Gren. The box-cox metric for nearest neighbor classiﬁcation
improvement. Pattern Recognition. 30(2), 273–279, 1997.
12. T. H. Hildebrandt and W. Liu. Optical recognition of Chinese characters: advances since
1980. Pattern Recognition. 26(2), 205–225, 1993.
13. T. Horiuchi, R. Haruki, H. Yamada, and K. Yamamoto. Two-dimensional extension of non-
linear normalization method using line density for character recognition. In Proceedings of
the 4th International Conference on Document Analysis and Recognition, Ulm, Germany,
1997, pp. 511–514.
14. G. F. Houle, D. B. Aragon, R. W. Smith, M. Shridhar, and D. Kimura. A multi-layered
corroboration-based
check
reader.
In
Proceedings of IAPR Workshop on Document
Analysis Systems, Malvern, USA, October 1996, pp. 495–546.
15. T. Iijima, H. Genchi, and K. Mori. A theoretical study of the pattern identiﬁcation by
matching method. In Proceedings of the First USA-JAPAN Computer Conference, October
1972, pp. 42–48.
16. N. Kato, M. Suzuki, S. Omachi, H. Aso, and Y. Nemoto. A handwritten character recogni-
tion system using directional element feature and asymmetric Mahalanobis distance. IEEE
Transactions on Pattern Analanalysis and Machine Intelligence. 21(3), 258–262, 1999.
17. N. Kharma, T. Kowaliw, E. Clement, C. Jensen, A. Youssef, and J. Yao. Project CellNet:
Evolving an autonomous pattern recognizer. International Journal on Pattern Recognition
and Artiﬁcial Intelligence. 18(6), 1039–1056, 2004.
18. I.-J. Kim and J. H. Kim. Statistical character structure modeling and its application
to handwritten Chinese character recognition. IEEE Transactions on Pattern Analysis
and Machine Intelligence; 25(11), 1422–1436, 2003.
19. J. Kim, K. Kim, C. P. Nadal, and C. Y. Suen. A methodology of combining
HMM
and
MLP
classiﬁers
for
cursive
word
recognition.
In
Proceedings of
the 15th International Conference on Pattern Recognition, Barcelona, Spain, September
2000, Vol. 2, pp. 319–322.
20. J. Kim, K. Kim, and C. Y. Suen. Hybrid schemes of homogeneous and hetero-
geneous
classiﬁers
for
cursive
word
recognition.
In
Proceedings of the 7th
International Workshop on Frontiers in Handwriting Recognition,
Amsterdam,
the
Netherlands, September 2000, pp. 433–442.
21. F. Kimura, K. Takashina, S. Tsuruoka, and Y. Miyake. Modiﬁed quadratic discriminant
functions and the application to Chinese character recognition. IEEE Transactions on
Pattern Analysis and Machine Intelligence; 9(1), 149–153, 1987.
22. F. Kimura, T. Wakabayashi, S. Tsuruoka, and Y. Miyake. Improvement of handwritten
Japanese character recognition using weighted direction code histogram. Pattern Recog-
nition. 30(8), 1329–1337, 1997.
23. Y. Kimura, T. Wakahara, and A. Tomono. Combination of statistical and neural classiﬁers
for a high-accuracy recognition of large character sets. Transactions of IEICE Japan.
J83-D-II(10), 1986–1994, 2000 (in Japanese).

REFERENCES
319
24. J. Kittler, M. Hatef, R. P. W. Duin, and J. Matas. On combining classiﬁers. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence; 20(3), 226–239, 1998.
25. T. Kowaliw, N. Kharma, C. Jensen, H. Mognieh, and J. Yao. Using competitive
co-evolution to evolve better pattern recognizers. International Journal Computa-
tional Intelligence and Applications. 5(3), 305–320, 2005.
26. L. Lam, Q. Xu, and C. Y. Suen. Differentiation between alphabetic and numeric data
using ensembles of neural networks. In Proceedings of the 16th International Conference
on Pattern Recognition, Quebec City, Canada, August 2002, Vol. 4, pp. 40–43.
27. C.-L. Liu, Y-J. Liu, and R-W. Dai. Preprocessing and statistical/structural feature extraction
for handwritten numeral recognition. In A. C. Downton and S. Impedovo, editors, Progress
of Handwriting Recognition, World Scientiﬁc, Singapore, 1997, pp. 161–168.
28. C.-L. Liu and M. Nakagawa. Evaluation of prototype learning algorithms for nearest neigh-
bor classiﬁer in application to handwritten character recognition. Pattern Recognition.
34(3), 601–615, 2001.
29. C.-L. Liu, K. Nakashima, H. Sako, and H. Fujisawa. Handwritten digit recognition: Bench-
marking of state-of-the-art techniques. Pattern Recognition. 36(10), 2271–2285, 2003.
30. C.-L. Liu, H. Sako, and H. Fujisawa. Handwritten Chinese character recognition: alterna-
tives to nonlinear normalization. In Proceedings of the 7th International Conference on
Document Analysis and Recognition, Edinburgh, Scotland, 2003, pp. 524–528.
31. C.-L. Liu, K. Nakashima, H. Sako, and H. Fujisawa. Handwritten digit recognition: inves-
tigation of normalization and feature extraction techniques. Pattern Recognition. 37(2),
265–279, 2004.
32. C.-L. Liu, S. Jaeger, and M. Nakagawa. Online recognition of Chinese characters: the state-
of-the-art. IEEE Transactions on Pattern Analysis Machanic Intelligence; 26(2), 198–213,
2004.
33. C.-L. Liu and K. Marukawa. Global shape normalization for handwritten Chinese character
recognition: a new method. In Proceedings of the 9th International Workshop on Frontiers
of Handwriting Recognition, Tokyo, Japan, 2004, pp. 300–305.
34. C.-L. Liu and K. Marukawa. Pseudo two-dimensional shape normalization methods for
handwritten Chinese character recognition. Pattern Recognition. 38(12), 2242–2255, 2005.
35. C.-L. Liu, M. Koga, and H. Fujisawa. Gabor feature extraction for character recognition:
comparison with gradient feature. In Proceedings of the 8th International Conference on
Document Analysis and Recognition, Seoul, Korea, 2005, pp. 121–125.
36. C.-L. Liu. High accuracy handwritten Chinese character recognition using quadratic
classiﬁers with discriminative feature extraction. In Proceedings of the 18th Inter-
national Conference on Pattern Recognition, Hong Kong, 2006, Vol. 2, pp. 942–
945.
37. H. Liu and X. Ding. Handwritten character recognition using gradient feature and
quadratic classiﬁer with multiple discrimination schemes. In Proceedings of the 8th In-
ternational Conference on Document Analysis and Recognition, Seoul, Korea, 2005,
pp. 19–23.
38. M. Mitchell. An Introduction to Genetic Algorithms. MIT Press, 1998.
39. S. Mori, K. Yamamoto, and M. Yasuda. Research on machine recognition of handprinted
characters. IEEE Transactions on Pattern Analysis Machanic Intelligence; 6(4), 386–405,
1984.

320
CASE STUDIES
40. M. Morita, A. El Yacoubi, R. Sabourin, F. Bortolozzi, and C. Y. Suen. Handwritten
month word recognition on Brazilian bank cheques. In Proceedings of the 6th Inter-
national Conference on Document Analysis and Recognition, Seattle, USA, September
2001, pp. 972–976.
41. K. Saruta, N. Kato, M. Abe, Y. Nemoto. High accuracy recognition of ETL9B using
exclusive learning neural network-II (ELNET-II). IEICE Transactions of Information and
Systems. 79-D(5), 516–521, 1996.
42. M. Shi, Y. Fujisawa, T. Wakabayashi, and F. Kimura. Handwritten numeral recognition
using gradient and curvature of gray scale image. Pattern Recognition. 35(10), 2051–2059,
2002.
43. W. Stalling. Approaches to Chinese character recognition. Pattern Recognition. 8, 87–98,
1976.
44. N. W. Strathy. Handwriting recognition for cheque processing. In Proceedings of the
2nd International Conference on Multimodal Interface,
Hong
Kong,
January
1999,
pp. 47–50.
45. C. Y. Suen, Q. Xu, and L. Lam. Automatic recognition of handwritten data on cheques—
fact or ﬁction? Pattern Recognition Letters. 20(11–13), 1287–1295, 1999.
46. L.-N. Teow and K.-F. Loe. Robust vision-based features and classiﬁcation schemes
for off-line handwritten digit recognition. Pattern Recognition. 35(11), 2355–2364,
2002.
47. J. Tsukumo and H. Tanaka. Classiﬁcation of handprinted Chinese characters using non-
linear normalization and correlation methods. In Proceedings of the 9th International
Conference on Pattern Recognition, Rome, Italy, 1988, pp. 168–171.
48. M. Umeda. Advances in recognition methods for handwritten Kanji characters. IEICE
Trans. Information and Systems. E29(5), 401–410, 1996.
49. T. Wakabayashi, S. Tsuruoka, F. Kimura, and Y. Miyake. On the size and variable trans-
formation of feature vector for handwritten character recognition. Transactions of IEICE
Japan. J76-D-II(12), 2495–2503, 1993 (in Japanese).
50. X. Wang, X. Ding, and C. Liu. Gabor ﬁlter-base feature extraction for character recognition.
Pattern Recognition. 38(3), 369–379, 2005.
51. H. Yamada, K. Yamamoto, and T. Saito. A nonlinear normalization method for hanprinted
Kanji character recognition—line density equalization. Pattern Recognition. 23(9), 1023–
1029, 1990.
52. Y. Yamashita, K. Higuchi, Y. Yamada, and Y. Haga. Classiﬁcation of handprinted Kanji
characters by the structured segment matching method. Pattern Recognition Letters. 1,
475–479, 1983.
53. M. Yasuda and H. Fujisawa. An improvement of correlation method for character recog-
nition. Transactions of IEICE Japan. J62-D(3), 217–224, 1979 (in Japanese).
54. M. Yasuda, K. Yamamoto, H. Yamada, and T. Saito. An improved correlation method for
handprinted Chinese character recognition in a reciprocal feature ﬁeld. Transactions of
IEICE Japan. J68-D(3), 353–360, 1985 (in Japanese).

INDEX
ADF-encoded feature detector, 113
Afﬁnity bit, 268, 269
ASCII, 6, 8
Aspect ratio, 36, 37, 39, 64, 108
Attributed
graph matching, 174–176
string matching, 172–174
Automatic Complexiﬁcation, 118
Background, 5, 8, 8–23, 40, 44, 46, 106,
108, 207–213, 207
Complex, 8, 11, 14, 15, 21
elimination, 8, 100–101
pixel, 9, 12
Bayes, 179, 182, 187, 193, 215, 222, 243,
249
decision theory, 131, 197
error rate, 132, 141, 187
Bellman principle, 173
Bezier Curves, 76–77
Bimodality, 11, 18
Binary
image, 8–9, 11, 19–20, 61–62, 71,
109–110, 208–209, 255, 289–291,
295–297, 302
Character Recognition Systems: A Guide for Students and Practitioner, by M. Cheriet, N. Kharma,
C.-L. Liu and C. Y. Suen
Copyright © 2007 John Wiley & Sons, Inc.
Morphological operators, 21
Bloc location, 248
Blurring, 30, 60, 285, 290
Boolean function-based, 104
Bootstrapping technique, 282
Bounding box, 38, 40, 106, 218, 266,
286–287
B-spline blending function, 78
Camouﬂage function, 115, 265, 269, 280
Canalize development, 265
Capping mechanism, 115
CellNet, 115, 117, 265–275, 272, 281
Centroid distance, 74
Chaincode, 59, 71, 76, 284, 289–292,
295–297, 299
decomposition, 59, 61, 289–292
direction, 61, 64, 285, 290
Character
recognition, 1–3, 5, 8, 20, 32 63, 85, 106,
129, 205, 212
segmentation, 205, 207, 208, 244, 257
separation, 208, 212, 228, 231–232
shape, 3, 21, 28, 36, 74, 78, 76, 171,
217–218, 281–286
321

322
INDEX
Character (Continued)
skeleton, 5, 45–46, 49, 76, 78, 212–214
Character normalization, 36–41, 286–289
Aspect ratio adaptive, 37
Bi-moment, 287
Linear, 38
Modiﬁed CBA, 287
Moment-based, 38–40
Nonlinear, 40–41
Character segmentation, 205, 207, 208, 243
Candidate lattice, 227, 228
Dissection, 207–210
Explicit, 206, 214, 238
Handwritten digits, 210–214
Implicit, 206, 238
Ligature analysis, 209
Over-segmentation, 206, 207, 220
Projection analysis, 208
Character string recognition
Character-model-based, 214
Character-level training, 221–223
Classiﬁcation-based, 214–237
Geometric context, 215–220
HMM-based, 237–250
Holistic, 250–256
Lexicon-driven, 204, 215, 227, 228, 231
Lexicon-free, 205, 215, 218, 227, 231
Linguistic context, 216, 220, 228, 235
Probabilistic model, 215–220
Segmentation-based, 205, 206, 256
String-level training, 223–227
Chinese character recognition, 40, 162, 174,
178, 179, 282
Class separability, 9–11, 18
Classiﬁer
Discriminant function-based, 162
Distance-based, 219
Gaussian, 132–134
Gaussian mixture, 137
Hyperplane, 163
K-nearest neighbor, 140
Maximal margin, 163
Nearest-neighbor, 140
Non-parametric, 138
One-versus-all, 170–171
One-versus-one, 170–171
Parametric, 132–138
Parzen window, 139
Semi-parametric, 136
Clustering, 157, 158
k-means, 158
Coarse scale, 23
Co-evolution, 116, 264
Complexity of a Hunter, 268
Component analysis
Connected, 30, 208
Principal, 80–81, 155
Confusion matrix, 182
Continuity, 28, 75,
Contour, 5, 23, 30, 41
analysis, 30, 41
extraction, 23, 45
tracing, 41
Convex hull, 77
Convolution, 28, 60
Coordinate mapping, 41, 43, 44, 286
Forward, 37
Backward, 37–38
Correlation method, 66
Covariance matrices, 83, 134, 301
Cross, 66, 93, 140, 147, 186
correlation, 66
entropy, 147, 186
validation, 93, 140
Cumulative Angular Function Shapes, 75
Curvature, 76, 78, 311
approximation, 78
signature, 76
Data, 3, 5–9, 21, 24
Enhance, 3–4, 8, 20
extraction, 9, 21, 24
Database, 5–8, 112, 197
CASIA, 291, 295–302
CEDAR, 115, 265, 272
CENPARMI, 273, 276, 304
CENPARMI IRIS, 304, 315
ETL9B, 290, 295–302
USPS, 194
USPS-CEDAR, 34
Decision boundary, 90, 132
Degradation, 22
Dempster-Shafer theory, 184
Dilation, 14, 17
Dimensionality reduction, 80, 83, 90, 285,
293

INDEX
323
Direction feature, 59, 61, 289
Chaincode, 62, 63, 290, 292, 295
Decomposition, 61, 290
Gradient, 61, 290, 296
Normalization-cooperated feature
extraction (NCFE), 284, 285, 290
Discriminant analysis, 10, 83, 90, 96
Fisher, 83, 96
Heteroscedastic, 85
Linear, 83–86, 294
Non-parametric, 86
Regularized, 134
Discriminant function, 132, 135, 143, 144,
148, 155, 160, 162, 184, 219, 293
Discriminative learning quadratic, 162
Linear, 133, 143
Modiﬁed quadratic (MQDF), 134–136,
194, 222, 293
Quadratic, 133, 162, 219
Distance
Bhattacharya, 94, 96
City block, 99
Edit, 173
Euclidean, 94, 223
Kullback-Leibler, 96, 174
Mahalanobis, 96, 133
Minkowski, 95, 96
Distortion, 19, 70, 236, 286
Document image, 15, 136, 205
Double-edge, 17–19
Dynamic time warping (DTW), 173, 233
Edge, 12–13, 42–44, 62, 70, 228, 286
based image analysis, 13
detection, 70
sharp, 12
Eigen, 81, 84, 89, 220, 301, 308
value, 81–84, 89, 220
vector, 81–84, 220, 301, 308
Elastic method, 73
Elitism, 269, 272, 281
Ensemble Generation, 190
Bagging, 192
Boosting, 192, 200
Overproduce-and-select, 191
Random subspace, 192
Erosion, 17
Evolutionary Algorithm, 120, 264,
266
Evolutionary computation, 263, 265, 267,
269, 273 275, 283
Expectation-Maximization (EM), 137, 224,
230, 233, 240, 243, 245
Entropy
Conditional, 96
Cross, 147, 186, 223
Error
quantization, 158
Rate, 91, 131, 318
regression, 80
Squared, 145, 223
Feature
Bottom-foreground, 216
creation, 104, 265, 266
extraction, 5, 8, 30, 36, 45, 50, 54, 202,
205, 213, 245, 250, 255, 289, 293, 295,
310
Intermediate-level, 255–256
Line element, 284
Perceptual, 254–257
Polynomial, 110
points, 78, 212–214, 216
selection, 90, 265, 302
space transformation, 79
Structural, 60, 255–256,
304–307
Topological, 78
Filter, 22, 27, 30, 60, 283, 295
Gaussian, 23, 60
Linear, 23, 30
Low pass, 22–23, 27
Recursive, 26
Spatial, 60
Wiener, 23
Wrapper (and), 93
Foreground, 12, 20, 208, 216
features, 216
feature point, 216
Pixel, 12, 20–22, 208
Form, 5, 7, 19
frame, 7, 19
identiﬁcation, 8
processing, 5–7, 19
structure, 7–9
Form processing, 5–7, 19
Generic, 5–7
Intelligent, 5–7

324
INDEX
Formal linguistics, 175
Fourier, 23, 67, 73, 266
descriptors, 73, 266
Inverse, 67
transform, 23, 67
Function
B-spline blending, 78
Cumulative angular, 74, 75
Logistic, 143
Radial basis, 88, 143, 152
Sigmoid activation, 144, 153,
188
Gain ratio, 97
Gaussian
classiﬁer, 132, 137
ﬁlter, 22–23, 60
mixture, 137, 246
Geometric
context, 216–214, 227
feature, 3, 54, 182, 218
moment, 39, 54
Generative model, 226
Genetic Algorithm (GA), 267, 271–273
Crossover, 272
Flooring, 246
Mutation, 269, 271–274
Vote mechanism, 273, 275
Global
features, 206, 254, 255
optimizing scheme, 249
Thresholding, 9–15
Gradient descent, 146, 241
Stochastic, 146, 220–223
Grammar, 234–235, 305–308
Context-free, 240, 305
Handwritten dates, 302–318
segmentation, 302, 307, 305
recognition, 302–303
HELPR, 115
Hessian matrix, 226
Hidden Markov Model (HMM), 204, 223,
236, 242, 337
Baum-Welch, 241, 246
Character-model, 204, 214, 255
Multiple codebooks, 247
Noise model, 249
parameter smoothing, 247
recognition, 204, 236, 242–249
Semi-continuous, 238
topology, 246
Viterbi decoding, 244, 248, 254
Histogram, 10–12, 20, 40, 58, 260, 284,
287, 292
bimodality, 11, 18
Holistic, 200, 204, 235, 251–258, 311
approach, 253
feature, 254–257
recognition method, 204–207, 250–257,
311
word-model, 206
Hunter, 262–270, 275
Hybrid, 220, 249
ﬁlter wrapper, 93
generative-discriminative training, 221
HMM/Neural Network, 253
Image
acquisition, 6, 8, 25, 237
projection, 37
registration, 64
regularization, 22, 26
smoothing, 33, 206
variations, 66
Interpolation, 38, 247, 292, 286
Isotropic, 23
Kernel, 6, 23, 27, 86, 165, 166
KCS, 26
PCA, 89
Polynomial, 166
RBF, 166
regularizing, 23
smoothing, 27
Sigmoid, 166
Layout analysis, 6, 32
Learning, 157, 200, 217, 234, 241
Competitive, 157
Discriminative, 200
Layered, 119
Soft competitive, 157
vector quantization, 160
Supervised, 245
Unsupervised, 156, 241

INDEX
325
Lexicon
organization, 235
Reduction, 234
Line-Based Representation, 75
Local
background, 9
Direction, 60, 297
Gradient, 62
neighborhood, 11
Orientation, 59
thresholding, 11–15
Logistic regression, 170, 183
Loss matrix, 131
Matching, 137, 145, 182
Graph, 174, 200
String, 172, 200, 234
Template, 206, 283
Merger, 271, 267
Model selection, 168
Moment, 55, 266, 284, 286, 300, 311
Central, 266, 287, 43
discrete orthogonal Tchebichef, 63
Geometric, 55
Legendre, 57
Psuedo-Zernike, 56
Tchebichef, 58
Zernike, 58
Multiple Classiﬁers, 179–193
Bayesian combination, 182
Behavior knowledge space (BKS), 183
Borda counts, 183
Fixed rules, 186
Weighted combination, 187–189
Dynamic classiﬁer selection, 190
Conﬁdence transformation, 184
Majority vote, 181
stacking, 180, 194
Multiple Codebooks, 252
Neural network, 142–162
Back-propagation (BP), 149, 150
Higher-order, 155
Multilayer Perceptron, 148–152
Neural-Gas, 158
Perceptron algorithm, 144
Polynomial, 155
Single-Layer, 144–148
Radial basis function (RBF), 152
Noise, 8, 13, 26
removal, 30
sensitivity, 73
Non-Evolutionary, 105
OTSU, 18
Overﬁtting, 138
Page segmentation, 32
Parameter
estimation, 132
Smoothing, 139
Symptom, 110
Parameter estimation
Cross-entropy (CE), 186
Expectation-maximization (EM), 228
Maximum likelihood (ML), 244
Mean squared error (MSE), 188
Minimum classiﬁcation error (MCE), 188
Maximum mutual information
(MMI), 220
Pseudo inverse, 220
squared error, 146–147
Pattern recognition, 45, 84, 115
Statistical, 137
Structural, 175
Pixel, 5, 8–12, 15, 38, 254
intensity, 55
interpolation, 38, 73, 251
background, 40
Point-mapping, 68
Polygonal Approximation, 76
Polynomial, 61, 82, 93
function-based, 108
Kernel, 166
Network, 155
Post-processing, 14, 17, 205
Pre-Evolutionary Method, 116
Preprocessing, 303, 309, 305, 24, 49
Prey, 115, 265
Projection, 33, 40, 71, 88, 141, 182
analysis, 33, 207
density, 41
horizontal-vertical, 41
oblique, 34
parallel, 33–34
radial, 178

326
INDEX
Quasi-uniform matrix, 87
Raster, 208
Regression, 85
Regularization, 26
Reject, 221
Distance, 221, 231
Ambiguity, 221, 231
Relaxation labeling, 178
Restoration, 21, 294
Rotation, 33–34, 38
Scale space, 15
Search, 44, 227
Beam, 233, 234
Branch-and-bound, 177
Character-synchronous, 231, 232, 234
Dynamic programming (DP), 172, 229
Frame-synchronous, 232
Heuristic, 176, 177
Tree, 176
Self-organizing map (SOM), 158
Seminal algorithm, 167
Sequential Forward Selection, 100
Shape retrieval, 74
Single-run columns, 213
Skeleton, 5, 48, 50
bottom-background
tracing, 208
top-background, 209
Skew, 22, 66, 206, 287
Detection, 32
ﬂat plane, 66
Slant, 30, 34, 41
angle estimation, 35, 41
Correction, 34, 41
Smoothing algorithm, 291
Sobel operator, 61–62
Soft-max, 158, 187
Spatial ﬁlter, 60, 283, 295
Gaussian, 61
Stroke, 11, 14, 54, 66
based model, 8
connectivity, 41
edge direction, 67
model, 8
reconstructing, 20
segment, 64, 290
thickness, 50
Structural template, 171
Structuring element, 14
Support Vector Machine, 162–171
Margin, 163
Sequential minimal optimization
(SMO), 167
Soft margin, 165
Successive overrelaxation (SOR), 167
Syntax analysis, 6
Taxonomy, 103
Tree-based, 103
Thinning, 30, 45
Threshold, 9, 102
Selection, 9
Technique, 9
Transformation
Afﬁne, 64–65
Conﬁdence, 184
Hough, 68–69
Inverse Fourier, 68
Linear, 79–86
Medial axis, 45
Perspective, 65–66
Projective, 66
Rigid, 5
Tying, 247
Underﬂow, 244, 247
Vector quantization, 149, 160
Visual data extraction, 21
Viterbi alignment, 250
Winning codevector, 159
Wrapper algorithm, 90
Zone detection, 106–109

